

\hide{
\begin{figure*}
    \centering
    \includegraphics[width= \linewidth]{grand7.pdf}
    \caption{Basic \model\ Operations. \yd{to remove this figure.} }
    \label{fig:arch1}
\end{figure*}
}


\label{sec:consis}  
\begin{figure*}[t]
	\centering
	\includegraphics[width= 1.0 \linewidth]{pics/grand_consis2.pdf}
	\caption{Illustration of \model\ with DropNode as the perturbation method. \textmd{\model\ designs random propagation (a) to generate multiple graph data augmentations (b), which are further used as consistency regularization (c) for semi-supervised learning. 
	\hide{
	\model\ consists of two mechanisms---random propagation and consistency regularized training. In random propagation, some nodes' feature vectors are randomly dropped with DropNode. 
	The resultant perturbed feature matrix  is then used to perform propagation without parameters to learn. Further, random propagation is used for stochastic graph data augmentation. 
	After that, the augmented feature matrices are fed into a two-layer MLP model for prediction. With applying consistency regularized training, \model\ generates  data augmentations by performing random propagation  times, and leverages both supervised classification loss  and consistency regularization loss  in optimization.
	}}
	}
	\label{fig:arch2}
\end{figure*}





\section{\full}

We present the \full\ (\model) for semi-supervised learning on graphs, as illustrated in Figure  \ref{fig:arch2}. 
The idea is to design a propagation strategy (a) to stochastically generate multiple graph data augmentations (b),  
based on which we present a consistency regularized training (c) for improving the generalization capacity under the semi-supervised setting. 





\subsection{Random Propagation for Graph Data Augmentation}
\label{sec:randpro}



Given an input graph  with its adjacency matrix  and feature matrix , the random propagation module generates multiple data augmentations. 
For each augmentation , it is then fed into the classification model, a two-layer MLP, for predicting node labels. 
The MLP model can also be replaced with more complex and advanced GNN models, such as GCN and GAT.







\vpara{Random Propagation.} 
There are two steps in random propagation. 
First, we generate a perturbed feature matrix  by randomly dropping out elements in .  Second, we leverage  to perform feature propagation for generating the augmented features . 

In doing so, each node's features are randomly mixed with signals from its neighbors. Note that the homophily assumption suggests that adjacent nodes tend to have similar features and labels~\cite{mcpherson2001birds}. 
Thus, the dropped information of a node could be compensated by its neighbors, forming an approximate representation for it in the corresponding augmentation. In other words, random propagation allows us to stochastically generate multiple augmented representations for each node. 










In the first step, there are different ways to perturb the input data . 
Straightforwardly, we can use the dropout strategy ~\cite{srivastava2014dropout}, which has been widely used for regularizing neural networks. 
Specifically, dropout perturbs the feature matrix by randomly setting some elements of  to 0 during training, i.e., , where  draws from . 
In doing so, dropout makes the input feature matrix  noisy by randomly dropping out its elements without considering graph structures.



To account for the structural effect, we can simply remove some nodes' entire feature vectors---referred to as DropNode, instead of dropping out single feature elements. 
In other words, DropNode enables each node to only aggregate information from a subset of its (multi-hop) neighbors by completely ignoring some nodes' features, which reduces its dependency on particular neighbors and thus helps increase the model's robustness (Cf. Section~\ref{sec:robust}). 
Empirically, it generates more stochastic data augmentations and achieves better performance than dropout (Cf. Section~\ref{sec:overall}). 


Formally, in DropNode, we first 
randomly sample a binary mask  for each node .
Second, we obtain the perturbed feature matrix  by multiplying each node's feature vector with its corresponding mask, i.e.,  where  denotes the  row vector of . 
Finally,  we scale  with the factor of  to guarantee the perturbed feature matrix is in expectation equal to . 
Note that the sampling procedure is only performed during training. 
During inference, we directly set  as the original feature matrix . 


In the second step of random propagation, we adopt the mixed-order propagation, i.e., , where   is the average of the power series of  from order 0 to order . 
This propagation rule enables the model to incorporate more local information, reducing the risk of over-smoothing when compared with directly using ~\cite{abu2019mixhop,xu2018representation}. Note that calculating the dense matrix  is computationally inefficient, thus we compute  by iteratively calculating and summing up the product of  sparse matrix  and  () in implementation.


With this propagation rule, we could observe that DropNode (dropping the  row of ) is equivalent to dropping the  column of . This is similar to DropEdge~\cite{YuDropedge}, which aims to address over-smoothing by randomly removing some edges. In practice, DropEdge could also be adopted as the perturbation method here. Specifically, we first generate a corrupted adjacency matrix  by dropping some elements from , and then use  to perform mix-order propagation as the substitute of   at each epoch. We empirically compare the effects of different perturbation methods in Section~\ref{sec:overall}. By default, we use DropNode as the perturbation method. 

\hide{
Differently, DropEdge is originally designed to be directly manipulating the adjacency matrix , and extending it to drop the elements of  is time-consuming since it needs to calculate the exact form of the dense matrix .
}


\hide{
In random propagation,  each node's features are randomly mixed with signals from its neighbors. 
Note that the local smoothness assumption suggests that adjacent nodes often tend to have similar features and labels~\cite{dakovic2019local}. 
Therefore, the information of a dropped node could be compensated by its neighbors, forming an approximate representation for it in the corresponding augmentation. 
In other words, random propagation allows us to stochastically generate multiple augmented representations for each node. 
}





\hide{

\vpara{Dropping Function.} As for the perturbation function, a naive choice is dropout~\cite{srivastava2014dropout}. 

In that, a proportion of feature elements of each nodes are randomly set to  during training. Formally, dropout perturbs the feature matrix by randomly setting some elements of  to 0, i.e., , where  draws from . 
It has been widely-used as a general regularization method for neural network. However, each node still interacts with their deterministic neighbors despite dropout, though with random partial feature information. Inspired by DropBlock applied in CNNs, here we provide a more structured form of perturbation method for graphs---DropNode.

Different with dropout, DropNode is designed to randomly remove some nodes' entire feature vector instead of dropping single feature element. By doing so, DropNode enables each node only aggregates information from a subset of its (multi-hop) neighborhoods in random propagation, which will perform better in reducing the dependency between nodes to increase model's robustness. Meanwhile, it can also generate more stochastic data augmentations and achieves better performance than dropout (Cf. Section~\ref{sec:overall}). 
The formal DropNode operation is shown in Algorithm \ref{alg:dropnode}. First, we randomly sample a binary mask  for each node .
Second, we obtain the perturbed feature matrix  by multiplying each node's feature vector with its corresponding mask, i.e., \footnote{ denotes the  row vector of .} . 
Finally,  we scale  with the factor of  to guarantee the perturbed feature matrix is in expectation equal to . 
Note that the sampling procedure is only performed during training. 
During inference, we directly set  with the original feature matrix . 




\begin{algorithm}[h]
\small
\caption{DropNode}
\label{alg:dropnode}
\begin{algorithmic}[1]
\REQUIRE ~~\\
Feature matrix , DropNode probability 
. \\
\ENSURE ~~\\
Perturbed feature matrix  .
\STATE Randomly sample  masks: .
\STATE Obtain deformity feature matrix by  multiplying each node's feature vector with the corresponding  mask: .
\STATE Scale the deformity features: .
\end{algorithmic}
\end{algorithm}

}









\vpara{Prediction.}
After performing random propagation for  times, we generate  augmented feature matrices . 
Each of these augmented data is fed into a two-layer MLP to get the corresponding outputs:

where  denotes the prediction probabilities on  and  are the model parameters. 


Observing the data flow from random propagation to the prediction module, it can be realized that \model\ actually separates the feature propagation step, i.e., , and transformation step, i.e., . 
Note that these two steps are commonly coupled with each other in standard GNNs, that is, . 
This separation allows us to perform the high-order feature propagation 
without conducting non-linear transformations, reducing the risk of over-smoothing (Cf. Section~\ref{sec:oversmoothing}). 
A similar idea has been adopted by Klicpera et al.~\cite{klicpera2018predict}, with the difference that they first perform the prediction for each node and then propagate the prediction probabilities over the graph. 












\hide{
each of which  can be then fed into any classification models for predicting node labels. 
In \model, we simply employ a two-layer MLP as the classifier, 
where  denotes the model's parameters. 
The MLP classification model can be also replaced with more complex and advanced GNN models, such as GCN and GAT. 
The experimental results show that the replacements result in consistent performance drop across different datasets due to GNNs' over-smoothing problem (Cf. Appendix~~\ref{sec:oversmoothing_grand} for details). 
}


















\subsection{Consistency Regularized Training}
\label{sec:consis}


In graph based semi-supervised learning, the objective is usually to smooth the label information over the graph with regularizations~\cite{zhu2003semi,weston2012deep,kipf2016semi}, i.e., its loss function is a combination of the supervised loss on the labeled nodes and the graph regularization loss. 
Given the  data augmentations generated in random propagation, 
we can naturally design a consistency regularized loss for \model's semi-supervised learning.  





\hide{
Employing random propagation as stochastic graph data augmentation, it's natural to design a consistency regularized training algorithm for \model. 
In specific, we perform the random propagation operation for  times to generate  augmented feature matrices . 
Each of these augmented feature matrices is fed into the MLP prediction module to get the corresponding outputs:

where  denotes the prediction probabilities on . 

  



Feature propagation in existing GNNs and Weisfeiler-Lehman Isomorphism test~\cite{shervashidze2011weisfeiler} has been proven to be an effective method for enhancing node representation by aggregating information from neighborhoods. 
We discuss the additional implications that \model's random propagation brings into feature propagation. 
Random propagation randomly drops some nodes' entire features before propagation. 
As a result, each node only aggregates information from a random subset of its (multi-hop) neighborhood. 
In doing so, we are able to stochastically generate different representations for each node, which can be considered as a stochastic graph augmentation method. 
In addition, random propagation can be seen as injecting random noise into the propagation procedure. 


To empirically examine this data augmentation idea, we generate a set of augmented node representations  with different drop rates in random propagation and use each  to train a GCN for node classification on commonly used datasets---Cora, Citeseer, and Pubmed.  The results show that the decrease in GCN's classification accuracy is less than  even when the drop rate is set to . In other words, with half of rows in the input  removed (set to ), random propagation is capable of generating augmented node representations that are sufficient for prediction. 


Though one single  is relatively inferior to the original  in performance, in practice, multiple augmentations---each per epoch---are utilized for training the \model\ model. 
Similar to bagging~\cite{breiman1996bagging}, \model's random data augmentation scheme makes the final prediction model implicitly assemble models on exponentially many augmentations, yielding much better performance than the deterministic propagation used in GCN and GAT.}



 


\vpara{Supervised Loss.}
With  labeled nodes among  nodes, the supervised objective of the graph node classification task in each epoch is defined as the average cross-entropy loss over  augmentations:







\vpara{Consistency Regularization Loss.} 
In the semi-supervised setting, we propose to optimize the prediction consistency among  augmentations for unlabeled data. 
Considering a simple case of , we can minimize the squared  distance between the two outputs, i.e.,
 .
To extend this idea into the multiple-augmentation situation, 
we first 
calculate the label distribution center by taking the average of all distributions, i.e., 
 . Then we utilize the \textit{sharpening}~\cite{berthelot2019mixmatch} trick to ``guess'' the labels based on the average distributions. Specifically, the  node's guessed probability on the  class is calculated by:
\hide{

}

where  acts as the ``temperature'' that controls the sharpness of the categorical distribution. 
As , the sharpened label distribution will approach a one-hot distribution. 
We minimize the distance between   and  in \model:



Therefore, by setting  as a small value, we can enforce the model to output low-entropy predictions. 
This can be viewed as adding an extra entropy minimization regularization into the model, which assumes that the classifier's decision boundary should not pass through high-density regions of the marginal data distribution~\cite{grandvalet2005semi}. 





\vpara{Training and Inference.}
In each epoch, we employ both the supervised classification loss in Eq. \ref{equ:loss} and the consistency regularization loss in Eq. \ref{equ:consistency} on  augmentations. 
The final loss of \model\ is:

where  is a hyper-parameter that controls the balance between the two losses. 
Algorithm~\ref{alg:2} outlines \model's training process. 
During inference, as mentioned in Section \ref{sec:randpro}, we directly use the original feature   for propagation. 
This is justified because we scale  the perturbed feature matrix  during training to guarantee its expectation to match . Hence the inference formula is .


\vpara{Complexity.}
 The complexity of random propagation is  , where  denotes propagation step,  is the dimension of node feature,  is the number of nodes and  denotes edge count. The complexity of its prediction module (two-layer MLP) is , where  denotes its hidden size and  is the number of classes. 
By applying consistency regularized training, the total computational complexity of \model\ is , which is linear with the sum of node and edge counts. 

\vpara{Limitations.}
\model\ is based on the homophily assumption~\cite{mcpherson2001birds}, i.e., ``birds of a feather flock together'', a basic assumption in the literature of graph-based semi-supervised learning~\cite{zhu2003semi}. Due to that, however, \model\ may not succeed on graphs with less homophily. 



 \begin{algorithm}[t]
\caption{\model}
\small
\label{alg:2}
\begin{algorithmic}[1] \REQUIRE ~~\\
 Adjacency matrix ,
feature matrix , 
times of augmentations in each epoch , DropNode/dropout probability , learning rate , an MLP model: .\\
\ENSURE ~~\\
Prediction . \WHILE{not convergence}
\FOR{} 
\STATE Pertube the input:  . 
\STATE Perform propagation: .
\STATE Predict class distribution using MLP: 
\ENDFOR
\STATE Compute supervised classification loss  via Eq. 1 and consistency regularization loss via Eq. 3.
\STATE Update the parameters  by gradients descending:

\ENDWHILE
\STATE Output prediction  via: .

\end{algorithmic}
\end{algorithm}


















































































\hide{



\section{Graph Random Networks}
\hide{
\begin{figure*}
  		\centering
  		\includegraphics[width=0.8 \linewidth,height =  0.25\linewidth]{grand2.pdf}
 	\caption{Diagram of the training process of \model.  
 	As a general semi-supervised learning framework on graphs, \model\ provides two mechanisms---random propagation and consistency regularization----to enhance the prediction model's robustness and generalization. In each epoch, given the input graph, \model\ first generates  graph data augmentations  via random propagation layer. Then, each  is fed into a prediction model, leading to the corresponding prediction distribution . In optimization, except for optimizing the supervised classification loss  with the given labels , we also minimize the prediction distance among different augmentations of unlabeled nodes via unsupervised consistency loss . }
\label{fig:model}
\end{figure*}
}
To achieve a better model for semi-supervised learning on graphs, we propose Graph Random Networks (\model). In \model , different with other GNNs, each node is allowed to randomly interact with different subsets of neighborhoods in different training epochs. This random propagation mechanism is shown to be an economic way for stochastic graph data augmentation. Based on that, we also design a consistency regularized training method to improve model's generalization capacity by encouraging predictions invariant to different augmentations of the same node. 

\hide{
Graph is complex with highly tangled nodes and edges, while previous graph models, e.g., GCN and GAT, take the node neighborhood as a whole and follow a determined aggregation process recursively. These determined models, where nodes and edges interact with each other in a fixed way, suffer from overfitting and risk of being misguiding by small amount of potential noise, mistakes, and malevolent attacks. To address it, we explore graph-structured data in a random way. In the proposed framework, we train the graph model on massive augmented graph data, generated by random sampling and propagation. In different training epochs, nodes interact with different random subsets of graph, and thus mitigate the risk of being misguiding by specific nodes and edges. On the other hand, the random data augmentation alleviates the overfitting and the implicit ensemble style behind the process improves the capacity of the representation. As the model should generalize well and have a similar prediction on unlabeled data on different data augmentations, despite the potential divergence from sampling randomness, we can further introduce an unsupervised graph-based regularization to the framework.
}

\hide{
In a graph neural model, if any given node's  representation (including unlabeled nodes') has a sufficient and consistent performance in any random subgraphs, that is, any node representation disentangles with specific nodes or edge links, which can be swapped by others, the graph neural model can effectively alleviate the  overfitting to specific graph structure and being misguided by noise, mistakes, and malevolent attacks. One the other hand, the graph neural model trained on exponentially many random subgraphs will also explore graph structures in different levels as sufficiently as possible, while previous models, e.g., GCN and GAT, model the neighborhood as a whole. Based on it, the proposed model, \model, utilizes a series of random sampling strategies to explore graph structures. We found that sampling strategy coupled with graph propagation is an efficient way of graph data augmentation. By leveraging the consistency of abundant unlabeled data on different random augmentations, we can further lower the generalization loss.
}


\hide{
\subsection{Over-smoothing Problem }
The over-smoothing issue of GCNs was first studied in \cite{li2018deeper}, which indicates that node features will converge to a fixed vector as the network depth increases. This undesired convergence heavily restricts the expressive power of deep GCNs. Formally speaking, suppose  has  connected components . Let  denote the indication vectors for the  component , which indicates whether a node belongs to   i.e.,
	
 The over-smoothing phenomenon of GCNs is formulated as the following theorem:
\begin{theorem}
	Given a graph  which has  connected components , for any , we have:
	
	where  is a vector associated with .  

\end{theorem}
From this theorem, we could notice that after repeatedly performing the propagation rule of GCNs many times, node features will converge to a linear combination of . And the nodes in the same connected component only distinct by their degrees. In the above analyses, the activation function used in GCNs is assumed to be a linear transformation, but the conclusion can also be generalized to non-linear case\cite{oono2019graph}.
}

\begin{figure*}
    \centering
    \includegraphics[width= \linewidth]{grand6.pdf}
    \caption{Architecture of \model.}
    \label{fig:arch1}
\end{figure*}

\subsection{Architecture}
The architecture of \model\ is illustrated in Figure~\ref{fig:arch1}. Overall, the model includes two components, i.e., Random propagation module and classification module.


\subsubsection{Random Propagation Module.}
\label{sec:randpro}
\begin{figure}
    \centering
    \includegraphics[width=0.8
    \linewidth]{dropnode_vs_dropout2.pdf}
    \caption{Difference between DropNode and dropout. Dropout drops  elements of  independently. While dropnode drops feature vectors of nodes (row vectors of ) randomly.}
    \label{fig:dropnode_vs_dropout}
\end{figure}



\begin{algorithm}[tb]
\caption{Dropnode}
\label{alg:dropnode}
\begin{algorithmic}[1] 
\REQUIRE ~~\\
Original feature matrix , DropNode probability 
. \\
\ENSURE ~~\\
Perturbed feature matrix  .
\IF{mode == Inference}
\STATE .
\ELSE
\STATE Randomly sample  masks: .
\STATE Obtain deformity feature matrix by  multiplying each node's feature vector with the corresponding  mask: .
\STATE Scale the deformity features: .
\ENDIF
\end{algorithmic}
\end{algorithm}

\label{sec:randpro}


In random propagation, we aim to perform message passing in a random way during model training. To achieve that, we add an extra node sampling operation called ``DropNode'' in front of the propagation layer.


\vpara{DropNode.} In DropNode, feature vector of each node is randomly removed (rows of  are randomly set to ) with a pre-defined probability  at each training epoch. The resultant perturbed feature matrix  is fed into the propagation layer later on.
More formally, we first randomly sample a binary mask  for each node , and obtain the perturbed feature matrix  by multiplying each node's feature vector with the corresponding mask, i.e., . Furthermore, we scale  with a factor of  to guarantee the perturbed feature matrix is equal to  in expectation. Please note that the sampling procedure is only performed during training. In inference time, we directly let  equal to the original feature matrix . The algorithm of DropNode is shown in Algorithm \ref{alg:dropnode}. 

DropNode is similar to dropout\cite{srivastava2014dropout}, a more general regularization method used in deep learning to prevent overfitting. 
In dropout, the elements of  are randomly dropped out independently. While DropNode drops a node's whole features together, serving as a node sampling method. 
Figure \ref{fig:dropnode_vs_dropout} illustrates their differences.
Recent study suggests that a more structured form of dropout, e.g., dropping a contiguous region in DropBlock\cite{ghiasi2018dropblock}, is required for better regularizing CNNs on images. From this point of view, dropnode can be seen as a structured form of dropout on graph data.
We also demonstrate that dropnode is more suitable for graph data than dropout both theoretically (Cf. Section \ref{sec:theory}) and experimentally (Cf. Section \ref{sec:ablation}). 




After dropnode, the perturbed feature matrix  is fed into a propagation layer to perform message passing. Here we adopt mixed-order propagation, i.e.,


Here we define the propagation matrix as , that is, the average of the power series of  from order 0 to order . This kind of propagation rule enables model to incorporate multi-order neighborhoods information, and have a lower risk of over-smoothing compared with using  when  is large. Similar ideas have been adopted in previous works~\cite{abu2019mixhop,abu2018n}. We compute the Equation \ref{equ:kAX} by iteratively calculating the product of sparse  matrix  and . The corresponding time complexity is .


\hide{
In other words, the original features of about  nodes are removed (set as ).
Obviously, this random sampling strategy may destroy the information carried in nodes and the resultant corrupted feature matrix is insufficient for prediction. To compensate it, we try to recover the information in a graph signal propagation process, and get the recovery feature matrix . The propagation recovery process is:


It also offers a general way for implicit model ensemble on exponentially many augmented data.
}
\hide{
In each epoch, given the input graph, \model\ first generates  graph data augmentations  via random propagation layer. Then, each  is fed into a prediction model, leading to the corresponding prediction distribution . In optimization, except for optimizing the supervised classification loss  with the given labels , we also minimize the prediction distance among different augmentations of unlabeled nodes via unsupervised consistency loss .}







\hide{
\subsection{Motivation}
Our motivation is that GCNs can not sufficiently leverage unlabeled data in this task. Specifically,


\subsection{Overview of Graph Random Network}
    In this section, we provide a brief introduction of the proposed graph random network. The basic idea of \model is to promote GCNs' generalization and robustness by taking the full advantage of unlabeled data in the graph.


Random propagation layer consists of graph dropout and propagation.
We first introduce how to generate a subgraph by random sampling nodes.
Formally, we randomly sample the nodes without replacement at a probability   and drop the rest.
The deformity feature matrix  is formed in the following way,


In other words, the original features of about  nodes are removed (set as ).
Obviously, this random sampling strategy may destroy the information carried in nodes and the resultant corrupted feature matrix is insufficient for prediction. To compensate it, we try to recover the information in a graph signal propagation process, and get the recovery feature matrix . The propagation recovery process is:


\hide{
  
\begin{small}
  
\end{small}
where the first term keeps the denoised signal similar to the measurement and the second term enforces the smoothing of the solution. By setting the derivate of  to zero, we can obtain the solution to the Equation \ref{Equ:denoise}:


To avoid the inversion operation in Equation \ref{Equ:denoise2}, we derive an approximate solution of Equation \ref{Equ:denoise2} using Taylor Extension:

}

Combining Equation \ref{equ:samplingX1} and \ref{equ:kAX}, the original feature matrix  is firstly \hide{heavily} corrupted by sampling and then smoothed by graph propagation. In fact, the whole procedure consisting of two opposite operations provides a new representation  for the original . Later we will verify that  is still a good representation substitute with a sufficient prediction ability, while the randomness inside the process helps the model avoid the over-dependence on specific graph structure and 
explore different graph structure in a more robust way.
We treat these opposite operations as a whole procedure and call them \textit{random propagation layer} in the proposed model \model. 
\hide{After filtered by random sampling operation in random propagation layer, the subsequent model parts only involve interaction among nearly half of the original node features and thus graph-structure data are disentangled somehow.}

Here, we generalize the random propagation layer with another sampling strategies. Instead of removing the feature of a node entirely by sampling once, we drop the element of the feature in each dimension one by one by multiple sampling. This is a multi-channel version of node sampling. Furthermore, we scale the remaining elements with a factor of  to 
 guarantees the deformity feature matrix or adjacency matrix are the same as the original in expectation. Similar idea of dropping and rescaling has been also used in Dropout~\cite{srivastava2014dropout}. Hence we called our three graph sampling strategy graph dropout (\textit{dropnode} and \textit{dropout}), and formulate them following:
 
\vpara{Dropnode.} In dropnode, the feature vector of each node is randomly dropped with a pre-defined probability  during the propagation. More formally, we first form a deformity feature matrix  in the following way:

Then let  propagate in the graph via Equation~\ref{equ:kAX}. 

\vpara{Dropout.} In dropout, the element of the feature matrix is randomly dropped with a probability  during the propagation. Formally, we have:

Then we  propagate  in the graph via Equation~\ref{equ:kAX}. 



Note that dropout in graph dropout shares the same definition of Dropout~\cite{srivastava2014dropout}, a more general method widely used in deep learning to prevent parameters from overfitting. However, dropout in graph dropout, as a multi-channel version of dropnode, is mainly developed to explore graph-structured data in the semi-supervised learning framework. The graph dropout strategy, directly applied to graph objects, is also an efficient way of graph data augmentation and  model ensemble on exponentially many subgraphs. As for the common dropout applied to the input units, the optimal probability of drop is usually closer to 0 to prevent the loss of information~\cite{srivastava2014dropout}, which is not the case in our graph dropout. In this paper we will further analyze theoretically that graph dropout help \model\ leverage unlabeled data, and dropnode and dropout play different roles. 
}


\hide{
Note that dropout is applied to neural network activation to prevent overfitting of parameters while our node dropout and edge dropout are coupled with graph propagation, performing on graph objects without parameters. We will reveal that our graph dropout strategy is an efficient way of graph data augmentation and graph models ensemble on exponentially many subgraphs. We will further analyse theoretically that graph dropout help \model\ leverage unlabeled data. \reminder{}
}

\vpara{Stochastic Graph Data Augmentation.}
Feature propagation have been proven to be an effective method for enhancing node representation by aggregating information from neighborhoods, and becomes the key step of Weisfeiler-Lehman Isomorphism test~\cite{shervashidze2011weisfeiler} and GNNs. In random propagation, some nodes are first randomly dropped before propagation. That is, each node only aggregates information from a subset of multi-hop neighborhoods randomly. In this way, random propagation module stochastically generates different representations for a node, suggesting an efficient stochastic augmentation method for graph data. From another point of view, random propagation can also be seen as injecting random noise to the propagation procedure by dropping a portion of nodes . 

We also conduct an interesting experiment to examine the validation of this data augmentation method. In this setting, we first sample a set of augmented node representations  from random propagation module with different drop rates, then use  to train a GCN for node classification. 
By examining the classification accuracy on , we can empirically analyze the influence of the injected noise on model performance. The results with different drop rates have been shown in Figure \ref{fig:redundancy}. It can be seen that the performance only decreases by less than  as the drop rate increases from  to , indicating the augmented node representations is still sufficient for prediction\footnote{In practice, the drop rate is always set to }. 
 Though   is still inferior to  in performance, in practice, multiple augmentations will be utilized for training prediction model since the sampling will be performed in every epoch. From the view of bagging~\cite{breiman1996bagging}, this random data augmentation scheme makes the final prediction model implicitly ensemble models on exponentially many augmentations, yielding much better performances than using deterministic propagation.
 
  


\begin{figure}
  		\centering
  		\includegraphics[width = 0.8 \linewidth, height =  0.58\linewidth]{drop_rate.pdf}
  	\caption{Classification results of GCN with  as input.} 
  	\label{fig:redundancy}
\end{figure}
  






\subsubsection{Prediction Module.}
In prediction module, the augmented feature matrix  is fed into a neural network to predict nodes labels. We employ a two-layer MLP as the classifier:

where  is ReLU function,  is softmax function,  refers to model parameters. Here the classification model can also adopt more complex GNN based node classification models, e.g., GCN, GAT.  But we find the performance decreases when we replace MLP with GNNs because of the over-smoothing problem. We will explain this phenomenon in Section \ref{sec:oversmoothing}.






\hide{
From another perspective, Equation \ref{equ:samplingX1} and \ref{equ:kAX} in the random propagation layer perform a data augmentation procedure by linear interpolation~\cite{devries2017dataset} in a random subset of the multi-hop neighborhood.
}


 


\hide{
In a graph, we random sample a fixed proportion of neighborhoods for each node , and let  only interact with the sampled neighborhoods in propagation. Using this method, we are equivalent to let each node randomly perform linear interpolation with its neighbors. 

However, generating neighborhood samples for each node always require a time-consuming preprocessing. For example, the sampling method used in GraphSAGE~\cite{hamilton2017inductive} requires  times of sampling operations, where  denotes the size of sampled neighbor set. To solve this problem, here we propose an efficient sampling method to perform random propagation --- graph dropout. Graph dropout is inspired from Dropout\cite{srivastava2014dropout}, a widely used regularization method in deep learning. \reminder{graph dropout is an efficient sampling method}
In graph dropout, we randomly drop a set of nodes or edges in propagation, which is introduced separately. \\

\vpara{Edge Dropout.} The basic idea of edge dropout is randomly dropping a fix proportion of edges during each propagation. 
Specifically, we construct a deformity feature matrix  following:

Then we use  as the replacement of  in propagation.  


\vpara{Node Dropout.} In node dropout, the feature vector of each node is randomly dropped with a pre-defined probability  during propagation. More formally, we first form a deformity feature matrix  in the following way:

Then let  propagate in graph as the substitute of , i.e., . 

Actually, node dropout can be seen as a special form of edge dropout:
}



\subsection{Consistency Regularized Training}
\label{sec:consis}
\begin{figure*}
	\centering
	\includegraphics[width= \linewidth]{grand_consis.pdf}
	\caption{Illustration of consistency regularized training for \model. \yd{how about making the random progagation module into two as well?}}
	\label{fig:arch2}
\end{figure*}
As mentioned in previous subsection, random propagation module can be seen as an efficient method of stochastic data augmentation. That inspires us to design a consistency regularized training algorithm for optimizing parameters. 
  In the training algorithm, we generate multiple data augmentations at each epoch by performing dropnode multiple times. Besides the supervised classification loss, we also add a consistency regularization loss to enforce model to give similar predictions across different augmentations. This algorithm is illustrated in Figure \ref{fig:arch2}.


\begin{algorithm}[tb]
\caption{Consistency Regularized Training for \model}
\label{alg:2}
\begin{algorithmic}[1] \REQUIRE ~~\\
 Adjacency matrix ,
labeled node set ,
 unlabeled node set ,
feature matrix , 
times of dropnode in each epoch , dropnode probability .\\
\ENSURE ~~\\
Prediction .
\WHILE{not convergence}
\FOR{} 
\STATE Apply dropnode via Algorithm \ref{alg:dropnode}: . 
\STATE Perform propagation: .
\STATE Predict class distribution using MLP: .
\ENDFOR
\STATE Compute supervised classification loss  via Equation \ref{equ:loss} and consistency regularization loss via Equation \ref{equ:consistency}.
\STATE Update the parameters  by gradients descending:

\ENDWHILE
\STATE Output prediction  via Equation \ref{equ:inference}
\end{algorithmic}
\end{algorithm}

 
\subsubsection{augmentation Prediction}
At each epoch, we aim to generate  different data augmentations for graph data . To achieve that, we adopt sampling dropnode strategy in random propagation module. That is, we performing  times of random sampling in dropnode to generate  perturbed feature matrices . Then we propagate these features according to Equation \ref{equ:kAX} respectively, and hence obtain  data augmentations . Each of these augmented feature matrice are fed into the prediction module to get the corresponding output:


Where  denotes the classification probabilities on the  augmented data. 
\subsubsection{Supervised Classification Loss.}
The supervised objective of graph node classification in an epoch is the averaged cross-entropy loss over  times sampling:


\noindent where  is binary, indicating whether node  has the label . By optimizing this loss, we can also enforce the model to output the same predictions on multiple augmentations of a labeled node.
 However, in the semi-supervised setting, labeled data is always very rare. In order to make full use of unlabeled data, we also employ consistency regularization loss in \model. 
 
\subsubsection{Consistency Regularization Loss.} How to optimize the consistency among  augmentations of unlabeled data? 
 Let's first consider a simple case, where the random propagation procedure is performed only twice in each epoch, i.e., . Then a straightforward method is to minimize the distributional distance between two outputs:
 
where  is the distance function. Then we extend it into multiple augmentation situation. We can first calculate the label distribution center by taking average of all distributions:


And we can minimize the distributional distance between  and , i.e., .
\hide{

}
However, the distribution center calculated in this way is always inclined to have more entropy value, which indicates to be more ``uncertain''. 
Thus this method will bring extra uncertainty into model's predictions. To avoid this problem, We utilize the label sharpening trick in \model. Specifically, we apply a sharpening function onto the averaged label distribution to reduce its entropy:

where  acts as the ``temperature'' which controls the sharpness of the categorical distribution. As , the sharpened label distribution will approach a one-hot distribution. As the substitute of , we minimize the distance between   and  in \model:


By setting  as a small value, we could enforce the model to output low-entroy predictions. This can be seen as adding an extra entropy minimization regularization into the model, which assumes that classifier's decision boundary should not pass through high-density regions of the marginal data distribution\cite{grandvalet2005semi}. As for the  distance function , we adopt squared  loss, i.e., , in our model. It has been proved to be less sensitive to incorrect predictions\cite{berthelot2019mixmatch}, and is more suitable to the semi-supervised setting compared to cross-entropy. 



\subsubsection{Training and Inference}
In each epoch, we employ both supervised classification loss (Cf. Equation \ref{equ:loss}) and consistency regularization loss (Cf. Equation \ref{equ:consistency}) on  times of sampling. Hence, the final loss is:

Here  is a hyper-parameter which controls the balance between supervised classification loss and consistency regularization loss.
In the inference phase, as mentioned in Section \ref{sec:randpro}, we directly use original feature  as the output of dropnode instead of sampling. Hence the inference formula is:

Here  denotes the optimized parameters after training. We summarize our algorithm in Algorithm \ref{alg:2}. 


\hide{Note that  is equal to the expectation of s from data augmentation by multiple sampling. From the view of model bagging~\cite{breiman1996bagging}, the final graph model implicitly aggregates models trained on exponentially many subgraphs, and performs a plurality vote among these models in the inference phase, resulting in a lower generalization loss. }
 
\hide{




Hence the supervised objective of graph node classification in an epoch is the averaged cross-entropy loss over  times sampling:


\noindent where  is binary, indicating whether node  has the label . 


In each epoch, we employ both supervised cross-entropy loss (Cf. Eq.\ref{equ:loss}) and unsupervised consistency loss (Cf. Eq.\ref{equ:consistency}) on  times of sampling. Hence, the final loss is:

Here  is a hyper-parameter which controls the balance between supervised classification loss and unsupervised consistency loss.
In the inference phase, the output is achieved by averaging  over the results on exponentially many augmented test data. This can be economically realized by inputting  without sampling:


Note that  is the average of s from data augmentation by multiple sampling. From the view of model bagging~\cite{breiman1996bagging}, the final graph model implicitly aggregates models trained on exponentially many subgraphs, and performs a plurality vote among these models in the inference phase, resulting in a lower generalization loss.  
}






\hide{
\subsection{graph propagation for smoothing}

In this section, we introduce the random propagation layer, an efficient method to perform stochastic data augmentation on the graph. We first demonstrate the propagation layer in GCNs is actually a special form of data augmentation on graph-structured data. Based on this discovery, we propose random propagation strategy, which augments each node with a part of randomized selected neighborhoods \reminder{}. Finally we show that this strategy can be efficiently instantiated with a stochastic sampling method --- graph dropout.

\subsection{Feature Propagation as Data Augmentation}
\label{sec:aug}
An elegant data augmentation technique used in supervised learning is linear interpolation \cite{devries2017dataset}. Specifically, for each example on training dataset, we first find its nearest neighboring samples in feature space which share the same class label. Then for each pair of neighboring feature vectors, we get the augmented example using interpolation:

where  and  are neighboring pairs with the same label ,  is augmented feature vector, and  controls degree of interpolation\reminder{}. Then the example  is used as extra training sample to facilitate the learning task. In this way, the model is encouraged to more smooth in input space and more robust against small perturbations. \reminder{why?}


As for the problem of semi-supervised learning on graphs, we assume the graph signals are smooth, i.e., \textit{neighborhoods have similar feature vectors and similar class labels}. Thus a straightforward idea of augmenting graph-structured data is interpolating node features with one of its neighborhoods' features. However, in the real network, there exist small amounts of neighboring nodes with different labels and we can't identify that for samples with non-observable labels \reminder{?}. Thus simple interpretation with one neighbor might bring uncontrollable noise into the learning framework. 
An alternative solution is interpolating samples with multiple neighborhoods, which leads to Laplacian Smoothing:

.
Here we follow the definition of GCNs which adding a self-loop for each node in graph\reminder{}. Rewriting Eq.~\ref{equ:lap} in matrix form, we have:
.
As pointed out by Li et.al.\cite{li2018deeper}\reminder{}, when  and replacing  with the symmetric normalized adjacency matrix , Eq.~\ref{equ:lap2} is identical to the propagation layer in GCN, i.e., .

}

\hide{
\subsection{Random Propagation with Graph Dropout}
With the insight from last Section~\ref{sec:aug}, we develop a random propagation strategy for stochastic data augmentation on graph.

During each time of random propagation, we random sample a fixed proportion of neighborhoods for each node , and let  only interact with the sampled neighborhoods in propagation. Using this method, we are equivalent to let each node randomly perform linear interpolation with its neighbors. 

However, generating neighborhood samples for each node always require a time-consuming preprocessing. For example, the sampling method used in GraphSAGE~\cite{hamilton2017inductive} requires  times of sampling operations, where  denotes the size of sampled neighbor set. To solve this problem, here we propose an efficient sampling method to perform random propagation --- graph dropout. Graph dropout is inspired from Dropout\cite{srivastava2014dropout}, a widely used regularization method in deep learning. \reminder{graph dropout is an efficient sampling method}
In graph dropout, we randomly drop a set of nodes or edges in propagation, which is introduced separately. \\

\vpara{Edge Dropout.} The basic idea of edge dropout is randomly dropping a fix proportion of edges during each propagation. 
Specifically, we construct a deformity feature matrix  following:

Then we use  as the replacement of  in propagation.  


\vpara{Node Dropout.} In node dropout, the feature vector of each node is randomly dropped with a pre-defined probability  during propagation. More formally, we first form a deformity feature matrix  in the following way:

Then let  propagate in graph as the substitute of , i.e., . 

Actually, node dropout can be seen as a special form of edge dropout: \reminder{Dropping a node is equivalent to drop all the edges start from the node.} \\

\reminder{connection between graph dropout and feature dropout. dropblock..}




}

\hide{
\section{Consistency Optimization}
\subsection{\drop: Exponential Ensemble of GCNs}
Benefiting from the information redundancy, the input of \shalf gives a relatively sufficient view for node classification.
It is straightforward to generalize \shalf to ensemble multiple \shalf s, as each set  provides a unique network view. In doing so, we can use  to measure the correlation between two network data view  and . In particular,  and  can be considered independent. However, one thing that obstructs the direct application of traditional ensemble methods is that a network can generate exponential data views when sampling  nodes from its  nodes. 


\begin{figure}{
		\centering
\includegraphics[width = 1\linewidth]{fig_NSGCN.pdf}
		\caption{\sdrop and \dm. \drop: In each epoch we randomly drop half of the nodes and do the process of \half. The input in the inference phase is the original feature matrix without dropout, but with a discount factor of . This method can be treated as the exponential ensemble of \half.
			\dm: In each epoch of \drop, we consider the half of the nodes that are dropped are independent to the remaining nodes and input them to another \drop. We minimize the disagreement of these two \drop s via the Jensen-Shannon divergence. This method can be treated as an economical co-training of two independent \drop s.}
		\label{fig:cotrain}
	}
\end{figure}

We propose a dropout-style ensemble model \drop.
In \drop, we develop a new network-sampling ensemble method---graph dropout, which samples the node set  from a network's node set and do the \shalf process in each training epoch. 
In the inference phase, we use the entire feature matrix with a discount factor  as input~\cite{srivastava2014dropout}. 
The data flow in \drop\ is shown in Figure~\ref{fig:cotrain}. 


Specifically, given the recovery feature matrix  in a certain epoch, the softmax output of GCN in the training phase is . Hence the  objective of node classification in the network is




\noindent where  is binary, indicating whether node  has the label .  


To make the model practical, in the inference phase, the output is achieved by using the entire feature matrix  and a discount factor ~\cite{srivastava2014dropout}, that is, 



The discount factor guarantees the input of GCNs in the inference phase is the same as the expected input in the training phase. Similar idea has been also used in~\cite{srivastava2014dropout}. 









\subsection{\dm: Disagreement Minimization}In this section, we present the \dm\ model, which leverages the independency between the chosen node set  and the dropout node set  in each epoch of \drop. 


In \drop, the network data view provided by the chosen set  is trained and the view provided by the remaining nodes  at each epoch is completely ignored. 
The idea behind \drop\ aims to train over as many as network views as possible. 
Therefore, in \dm, we propose to simultaneously train two \drop s with complementary inputs--- and ---at every epoch. 
In the  training phase, we improve the two GCNs' prediction ability separately by minimizing the disagreement of these two models. 




The objective to minimize the disagreement of the two complementary \sdrop at a certain epoch can be obtained by Jensen-Shannon-divergence, that is, 



\noindent where  and  are the outputs of these two \drop s in the training phase, respectively. 


The final objective of \sdm is



In the inference phase, the final prediction is made by the two GCNs together, i.e., 



Practically, the correlation of two \drop s and their individual prediction ability jointly affect the performance of \dm. 
The inputs to the two models are complementary  and usually uncorrelated, empowering \dm\ with more representation capacity than \drop, which is also evident from the empirical results in Section \ref{sec:exp}.  



\subsection{ Graph Dropout for Network Sampling}
In GraphSAGE~\cite{hamilton2017inductive} and FastGCN~\cite{chen2018fastgcn}, 
nodes or edges are sampled in order to accelerate training, avoid overfitting, and make the network data regularly like grids. However in the inference phase, these models may miss information when the sampling result is not representative. 

Different from previous works, our \drop\ and \dm\ models are based on the network redundancy observation and the proposed dropout-like ensemble enable the (implicit) training on exponential sufficient network data views, making them outperform not only existing sampling-based methods, but also the original GCN with the full single-view~\cite{kipf2016semi}. 
This provides 
insights into the network sampling-based GCNs and offers a new way to sample networks for GCNs.



In addition, there exist connections between the original dropout mechanism in CNNs and our dropout-like network sampling technique. 
Similarly, both techniques use a discount factor in the inference phase. 
Differently, the dropout operation in the fully connected layers in CNNs results in the missing values of the activation (hidden representation) matrix , while our graph dropout sampling method is applied in network nodes, which removes network information in a structured way. 

Notice that very recently, DropBlock~\cite{ghiasi2018dropblock} suggested that a more structured form of dropout is required for better regularizing CNNs, and from this point of view, our graph dropout based network sampling is more in line with the idea of DropBlock. In \drop\ and \dm, dropping network information, instead of removing values from the activation functions, brings a structured way of removing information from the network. 
}






}%
