

\hide{
\begin{figure*}
    \centering
    \includegraphics[width= \linewidth]{grand7.pdf}
    \caption{Basic \model\ Operations. \yd{to remove this figure.} }
    \label{fig:arch1}
\end{figure*}
}


\label{sec:consis}  
\begin{figure*}[t]
	\centering
	\includegraphics[width= 1.0 \linewidth]{pics/grand_consis2.pdf}
	\caption{Illustration of \model\ with DropNode as the perturbation method. \textmd{\model\ designs random propagation (a) to generate multiple graph data augmentations (b), which are further used as consistency regularization (c) for semi-supervised learning. 
	\hide{
	\model\ consists of two mechanisms---random propagation and consistency regularized training. In random propagation, some nodes' feature vectors are randomly dropped with DropNode. 
	The resultant perturbed feature matrix $\widetilde{\mathbf{X}}$ is then used to perform propagation without parameters to learn. Further, random propagation is used for stochastic graph data augmentation. 
	After that, the augmented feature matrices are fed into a two-layer MLP model for prediction. With applying consistency regularized training, \model\ generates $S$ data augmentations by performing random propagation $S$ times, and leverages both supervised classification loss $\mathcal{L}_{sup}$ and consistency regularization loss $\mathcal{L}_{con}$ in optimization.
	}}
	}
	\label{fig:arch2}
\end{figure*}





\section{\full}

We present the \full\ (\model) for semi-supervised learning on graphs, as illustrated in Figure  \ref{fig:arch2}. 
The idea is to design a propagation strategy (a) to stochastically generate multiple graph data augmentations (b),  
based on which we present a consistency regularized training (c) for improving the generalization capacity under the semi-supervised setting. 





\subsection{Random Propagation for Graph Data Augmentation}
\label{sec:randpro}



Given an input graph $G$ with its adjacency matrix $\mathbf{A}$ and feature matrix $\mathbf{X}$, the random propagation module generates multiple data augmentations. 
For each augmentation $\overline{\mathbf{X}}$, it is then fed into the classification model, a two-layer MLP, for predicting node labels. 
The MLP model can also be replaced with more complex and advanced GNN models, such as GCN and GAT.







\vpara{Random Propagation.} 
There are two steps in random propagation. 
First, we generate a perturbed feature matrix $\widetilde{\mathbf{X}}$ by randomly dropping out elements in $\mathbf{X}$.  Second, we leverage $\widetilde{\mathbf{X}}$ to perform feature propagation for generating the augmented features $\overline{\mathbf{X}}$. 

In doing so, each node's features are randomly mixed with signals from its neighbors. Note that the homophily assumption suggests that adjacent nodes tend to have similar features and labels~\cite{mcpherson2001birds}. 
Thus, the dropped information of a node could be compensated by its neighbors, forming an approximate representation for it in the corresponding augmentation. In other words, random propagation allows us to stochastically generate multiple augmented representations for each node. 










In the first step, there are different ways to perturb the input data $\mathbf{X}$. 
Straightforwardly, we can use the dropout strategy ~\cite{srivastava2014dropout}, which has been widely used for regularizing neural networks. 
Specifically, dropout perturbs the feature matrix by randomly setting some elements of $\mathbf{X}$ to 0 during training, i.e., $\widetilde{\mathbf{X}}_{ij} = \frac{\epsilon_{ij}}{1-\delta} \mathbf{X}_{ij}$, where $\epsilon_{ij}$ draws from $Bernoulli(1-\delta)$. 
In doing so, dropout makes the input feature matrix $\mathbf{X}$ noisy by randomly dropping out its elements without considering graph structures.



To account for the structural effect, we can simply remove some nodes' entire feature vectors---referred to as DropNode, instead of dropping out single feature elements. 
In other words, DropNode enables each node to only aggregate information from a subset of its (multi-hop) neighbors by completely ignoring some nodes' features, which reduces its dependency on particular neighbors and thus helps increase the model's robustness (Cf. Section~\ref{sec:robust}). 
Empirically, it generates more stochastic data augmentations and achieves better performance than dropout (Cf. Section~\ref{sec:overall}). 


Formally, in DropNode, we first 
randomly sample a binary mask $\epsilon_i \sim Bernoulli(1-\delta)$ for each node $v_i$.
Second, we obtain the perturbed feature matrix $\widetilde{\mathbf{X}}$ by multiplying each node's feature vector with its corresponding mask, i.e., $\widetilde{\mathbf{X}}_i=\epsilon_i \cdot \mathbf{X}_i$ where $\mathbf{X}_i$ denotes the $i^{th}$ row vector of $\mathbf{X}$. 
Finally,  we scale $\widetilde{\mathbf{X}}$ with the factor of $\frac{1}{1-\delta}$ to guarantee the perturbed feature matrix is in expectation equal to $\mathbf{X}$. 
Note that the sampling procedure is only performed during training. 
During inference, we directly set $\widetilde{\mathbf{X}}$ as the original feature matrix $\mathbf{X}$. 


In the second step of random propagation, we adopt the mixed-order propagation, i.e., $\overline{\mathbf{X}} = \overline{\mathbf{A}} \widetilde{\mathbf{X}}$, where  $\overline{\mathbf{A}} =  \sum_{k=0}^K\frac{1}{K+1}\hat{\mathbf{A}}^k$ is the average of the power series of $\hat{\mathbf{A}}$ from order 0 to order $K$. 
This propagation rule enables the model to incorporate more local information, reducing the risk of over-smoothing when compared with directly using $\hat{\mathbf{A}}^K$~\cite{abu2019mixhop,xu2018representation}. Note that calculating the dense matrix $\overline{\mathbf{A}}$ is computationally inefficient, thus we compute $\overline{\mathbf{X}}$ by iteratively calculating and summing up the product of  sparse matrix $\hat{\mathbf{A}}$ and $\hat{\mathbf{A}}^{k}\widetilde{\mathbf{X}}$ ($0\leq k \leq K-1$) in implementation.


With this propagation rule, we could observe that DropNode (dropping the $i^{th}$ row of $\mathbf{X}$) is equivalent to dropping the $i^{th}$ column of $\overline{\mathbf{A}}$. This is similar to DropEdge~\cite{YuDropedge}, which aims to address over-smoothing by randomly removing some edges. In practice, DropEdge could also be adopted as the perturbation method here. Specifically, we first generate a corrupted adjacency matrix $\tilde{\mathbf{A}}$ by dropping some elements from $\hat{\mathbf{A}}$, and then use $\tilde{\mathbf{A}}$ to perform mix-order propagation as the substitute of $\hat{\mathbf{A}}$  at each epoch. We empirically compare the effects of different perturbation methods in Section~\ref{sec:overall}. By default, we use DropNode as the perturbation method. 

\hide{
Differently, DropEdge is originally designed to be directly manipulating the adjacency matrix $\mathbf{A}$, and extending it to drop the elements of $\overline{\mathbf{A}}$ is time-consuming since it needs to calculate the exact form of the dense matrix $\overline{\mathbf{A}}$.
}


\hide{
In random propagation,  each node's features are randomly mixed with signals from its neighbors. 
Note that the local smoothness assumption suggests that adjacent nodes often tend to have similar features and labels~\cite{dakovic2019local}. 
Therefore, the information of a dropped node could be compensated by its neighbors, forming an approximate representation for it in the corresponding augmentation. 
In other words, random propagation allows us to stochastically generate multiple augmented representations for each node. 
}





\hide{

\vpara{Dropping Function.} As for the perturbation function, a naive choice is dropout~\cite{srivastava2014dropout}. 

In that, a proportion of feature elements of each nodes are randomly set to $0$ during training. Formally, dropout perturbs the feature matrix by randomly setting some elements of $\mathbf{X}$ to 0, i.e., $\widetilde{\mathbf{X}}_{ij} = \frac{\epsilon_{ij}}{1-\delta} \mathbf{X}_{ij}$, where $\epsilon_{ij}$ draws from $Bernoulli(1-\delta)$. 
It has been widely-used as a general regularization method for neural network. However, each node still interacts with their deterministic neighbors despite dropout, though with random partial feature information. Inspired by DropBlock applied in CNNs, here we provide a more structured form of perturbation method for graphs---DropNode.

Different with dropout, DropNode is designed to randomly remove some nodes' entire feature vector instead of dropping single feature element. By doing so, DropNode enables each node only aggregates information from a subset of its (multi-hop) neighborhoods in random propagation, which will perform better in reducing the dependency between nodes to increase model's robustness. Meanwhile, it can also generate more stochastic data augmentations and achieves better performance than dropout (Cf. Section~\ref{sec:overall}). 
The formal DropNode operation is shown in Algorithm \ref{alg:dropnode}. First, we randomly sample a binary mask $\epsilon_i \sim Bernoulli(1-\delta)$ for each node $v_i$.
Second, we obtain the perturbed feature matrix $\widetilde{\mathbf{X}}$ by multiplying each node's feature vector with its corresponding mask, i.e., $\widetilde{\mathbf{X}}_i=\epsilon_i \cdot \mathbf{X}_i$\footnote{$\mathbf{M}_i$ denotes the $i^{th}$ row vector of $\mathbf{M}$.} . 
Finally,  we scale $\widetilde{\mathbf{X}}$ with the factor of $\frac{1}{1-\delta}$ to guarantee the perturbed feature matrix is in expectation equal to $\mathbf{X}$. 
Note that the sampling procedure is only performed during training. 
During inference, we directly set $\widetilde{\mathbf{X}}$ with the original feature matrix $\mathbf{X}$. 




\begin{algorithm}[h]
\small
\caption{DropNode}
\label{alg:dropnode}
\begin{algorithmic}[1]
\REQUIRE ~~\\
Feature matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, DropNode probability 
$\delta \in (0,1)$. \\
\ENSURE ~~\\
Perturbed feature matrix  $\mathbf{\widetilde{X}}\in \mathbb{R}^{n \times d}$.
\STATE Randomly sample $n$ masks: $\{\epsilon_i \sim Bernoulli(1-\delta)\}_{i=0}^{n-1}$.
\STATE Obtain deformity feature matrix by  multiplying each node's feature vector with the corresponding  mask: $\widetilde{\mathbf{X}}_{i} = \epsilon_i \cdot \mathbf{X}_{i} $.
\STATE Scale the deformity features: $\widetilde{\mathbf{X}} = \frac{\widetilde{\mathbf{X}}}{1-\delta}$.
\end{algorithmic}
\end{algorithm}

}









\vpara{Prediction.}
After performing random propagation for $S$ times, we generate $S$ augmented feature matrices $\{\overline{\mathbf{X}}^{(s)}|1\leq s \leq S\}$. 
Each of these augmented data is fed into a two-layer MLP to get the corresponding outputs:
$$
 \small
     \widetilde{\mathbf{Z}}^{(s)} = f_{mlp}(\overline{\mathbf{X}}^{(s)}, \Theta), 
$$
where $\widetilde{\mathbf{Z}}^{(s)} \in [0,1]^{n\times C}$ denotes the prediction probabilities on $\overline{\mathbf{X}}^{(s)}$ and $\Theta$ are the model parameters. 


Observing the data flow from random propagation to the prediction module, it can be realized that \model\ actually separates the feature propagation step, i.e., $\overline{\mathbf{X}} = \overline{\mathbf{A}} \widetilde{\mathbf{X}}$, and transformation step, i.e., $f_{mlp}(\overline{\mathbf{X}} \mathbf{W}, \Theta)$. 
Note that these two steps are commonly coupled with each other in standard GNNs, that is, $\sigma(\mathbf{AX} \mathbf{W})$. 
This separation allows us to perform the high-order feature propagation 
without conducting non-linear transformations, reducing the risk of over-smoothing (Cf. Section~\ref{sec:oversmoothing}). 
A similar idea has been adopted by Klicpera et al.~\cite{klicpera2018predict}, with the difference that they first perform the prediction for each node and then propagate the prediction probabilities over the graph. 












\hide{
each of which $\overline{\mathbf{X}}$ can be then fed into any classification models for predicting node labels. 
In \model, we simply employ a two-layer MLP as the classifier, \begin{equation}
\small
\label{equ:mlp}
    p(\mathbf{Y}|\overline{\mathbf{X}};\Theta) = f_{mlp}(\overline{\mathbf{X}}, \Theta),\end{equation}
where $\Theta$ denotes the model's parameters. 
The MLP classification model can be also replaced with more complex and advanced GNN models, such as GCN and GAT. 
The experimental results show that the replacements result in consistent performance drop across different datasets due to GNNs' over-smoothing problem (Cf. Appendix~~\ref{sec:oversmoothing_grand} for details). 
}


















\subsection{Consistency Regularized Training}
\label{sec:consis}


In graph based semi-supervised learning, the objective is usually to smooth the label information over the graph with regularizations~\cite{zhu2003semi,weston2012deep,kipf2016semi}, i.e., its loss function is a combination of the supervised loss on the labeled nodes and the graph regularization loss. 
Given the $S$ data augmentations generated in random propagation, 
we can naturally design a consistency regularized loss for \model's semi-supervised learning.  





\hide{
Employing random propagation as stochastic graph data augmentation, it's natural to design a consistency regularized training algorithm for \model. 
In specific, we perform the random propagation operation for $S$ times to generate $S$ augmented feature matrices $\{\overline{\mathbf{X}}^{(s)}|1\leq s \leq S\}$. 
Each of these augmented feature matrices is fed into the MLP prediction module to get the corresponding outputs:
$
 \small
     \widetilde{\mathbf{Z}}^{(s)} = f_{mlp}(\overline{\mathbf{X}}^{(s)}, \Theta), 
$
where $\widetilde{\mathbf{Z}}^{(s)} \in [0,1]^{n\times C}$ denotes the prediction probabilities on $\overline{\mathbf{X}}^{(s)}$. 

  



Feature propagation in existing GNNs and Weisfeiler-Lehman Isomorphism test~\cite{shervashidze2011weisfeiler} has been proven to be an effective method for enhancing node representation by aggregating information from neighborhoods. 
We discuss the additional implications that \model's random propagation brings into feature propagation. 
Random propagation randomly drops some nodes' entire features before propagation. 
As a result, each node only aggregates information from a random subset of its (multi-hop) neighborhood. 
In doing so, we are able to stochastically generate different representations for each node, which can be considered as a stochastic graph augmentation method. 
In addition, random propagation can be seen as injecting random noise into the propagation procedure. 


To empirically examine this data augmentation idea, we generate a set of augmented node representations $\overline{\mathbf{X}}$ with different drop rates in random propagation and use each $\overline{\mathbf{X}}$ to train a GCN for node classification on commonly used datasets---Cora, Citeseer, and Pubmed.  The results show that the decrease in GCN's classification accuracy is less than $3\%$ even when the drop rate is set to $0.5$. In other words, with half of rows in the input $\mathbf{X}$ removed (set to $\vec{0}$), random propagation is capable of generating augmented node representations that are sufficient for prediction. 


Though one single $\overline{\mathbf{X}}$ is relatively inferior to the original $\mathbf{X}$ in performance, in practice, multiple augmentations---each per epoch---are utilized for training the \model\ model. 
Similar to bagging~\cite{breiman1996bagging}, \model's random data augmentation scheme makes the final prediction model implicitly assemble models on exponentially many augmentations, yielding much better performance than the deterministic propagation used in GCN and GAT.}



 


\vpara{Supervised Loss.}
With $m$ labeled nodes among $n$ nodes, the supervised objective of the graph node classification task in each epoch is defined as the average cross-entropy loss over $S$ augmentations:
\begin{equation}
\small
\label{equ:loss}
	\mathcal{L}_{sup} = -\frac{1}{S}\sum_{s=1}^{S}\sum_{i=0}^{m-1}\mathbf{Y}_{i}^\top \log \widetilde{\mathbf{Z}}_{i}^{(s)} .
\end{equation}






\vpara{Consistency Regularization Loss.} 
In the semi-supervised setting, we propose to optimize the prediction consistency among $S$ augmentations for unlabeled data. 
Considering a simple case of $S=2$, we can minimize the squared $L_2$ distance between the two outputs, i.e.,
 $
\min \sum_{i=0}^{n-1} \|\widetilde{\mathbf{Z}}^{(1)}_i - \widetilde{\mathbf{Z}}^{(2)}_i\|_2^2$.
To extend this idea into the multiple-augmentation situation, 
we first 
calculate the label distribution center by taking the average of all distributions, i.e., 
 $ \overline{\mathbf{Z}}_i = \frac{1}{S}\sum_{s=1}^{S} \widetilde{\mathbf{Z}}_i^{(s)}$. Then we utilize the \textit{sharpening}~\cite{berthelot2019mixmatch} trick to ``guess'' the labels based on the average distributions. Specifically, the $i^{th}$ node's guessed probability on the $j^{th}$ class is calculated by:
\hide{
\begin{equation}
    \min \sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{\mathbf{Z}}_i, \widetilde{\mathbf{Z}}^{(s)}_i).
\end{equation}
}
\begin{equation}
\small
\label{equ:sharpen}
\overline{\mathbf{Z}}^{'}_{ij} = \overline{\mathbf{Z}}_{ij}^{\frac{1}{T}} ~\bigg/~\sum_{c=0}^{C-1}\overline{\mathbf{Z}}_{ic}^{\frac{1}{T}}, (0\leq j \leq C-1),
\end{equation}
where $0< T\leq 1$ acts as the ``temperature'' that controls the sharpness of the categorical distribution. 
As $T \to 0$, the sharpened label distribution will approach a one-hot distribution. 
We minimize the distance between  $\widetilde{\mathbf{Z}}_i$ and $\overline{\mathbf{Z}}^{'}_i$ in \model:
\begin{equation}
\small
\label{equ:consistency}
    \mathcal{L}_{con} =   \frac{1}{S}\sum_{s=1}^{S}\sum_{i=0}^{n-1} \|\overline{\mathbf{Z}}^{'}_i- \widetilde{\mathbf{Z}}^{(s)}_i\|_2^2.
\end{equation}


Therefore, by setting $T$ as a small value, we can enforce the model to output low-entropy predictions. 
This can be viewed as adding an extra entropy minimization regularization into the model, which assumes that the classifier's decision boundary should not pass through high-density regions of the marginal data distribution~\cite{grandvalet2005semi}. 





\vpara{Training and Inference.}
In each epoch, we employ both the supervised classification loss in Eq. \ref{equ:loss} and the consistency regularization loss in Eq. \ref{equ:consistency} on $S$ augmentations. 
The final loss of \model\ is:
\begin{equation}
\small
\label{equ:inf}
	\mathcal{L} = \mathcal{L}_{sup} + \lambda \mathcal{L}_{con},
\end{equation}
where $\lambda$ is a hyper-parameter that controls the balance between the two losses. 
Algorithm~\ref{alg:2} outlines \model's training process. 
During inference, as mentioned in Section \ref{sec:randpro}, we directly use the original feature $\mathbf{X}$  for propagation. 
This is justified because we scale  the perturbed feature matrix $\widetilde{\mathbf{X}}$ during training to guarantee its expectation to match $\mathbf{X}$. Hence the inference formula is $\mathbf{Z} = f_{mlp}(\overline{\mathbf{A}}\mathbf{X}, \Theta)$.


\vpara{Complexity.}
 The complexity of random propagation is  $\mathcal{O}(Kd(n+|E|))$, where $K$ denotes propagation step, $d$ is the dimension of node feature, $n$ is the number of nodes and $|E|$ denotes edge count. The complexity of its prediction module (two-layer MLP) is $\mathcal{O}(nd_h(d+ C))$, where $d_h$ denotes its hidden size and $C$ is the number of classes. 
By applying consistency regularized training, the total computational complexity of \model\ is $\mathcal{O}(S(Kd(n + |E|)+ nd_h(d + C)))$, which is linear with the sum of node and edge counts. 

\vpara{Limitations.}
\model\ is based on the homophily assumption~\cite{mcpherson2001birds}, i.e., ``birds of a feather flock together'', a basic assumption in the literature of graph-based semi-supervised learning~\cite{zhu2003semi}. Due to that, however, \model\ may not succeed on graphs with less homophily. 



 \begin{algorithm}[t]
\caption{\model}
\small
\label{alg:2}
\begin{algorithmic}[1] \REQUIRE ~~\\
 Adjacency matrix $\hat{\mathbf{A}}$,
feature matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, 
times of augmentations in each epoch $S$, DropNode/dropout probability $\delta$, learning rate $\eta$, an MLP model: $f_{mlp}(\mathbf{X}, \Theta)$.\\
\ENSURE ~~\\
Prediction $\mathbf{Z}$. \WHILE{not convergence}
\FOR{$s=1:S$} 
\STATE Pertube the input:  $\widetilde{\mathbf{X}}^{(s)} \sim \text{DropNode}(\mathbf{X},\delta)$. 
\STATE Perform propagation: $\overline{\mathbf{X}}^{(s)} = \frac{1}{K+1}\sum_{k=0}^K\hat{\mathbf{A}}^k \widetilde{\mathbf{X}}^{(s)}$.
\STATE Predict class distribution using MLP: $\widetilde{\mathbf{Z}}^{(s)} = f_{mlp}(\mathbf{\overline{X}}^{(s)}, \Theta)$
\ENDFOR
\STATE Compute supervised classification loss $\mathcal{L}_{sup}$ via Eq. 1 and consistency regularization loss via Eq. 3.
\STATE Update the parameters $\Theta$ by gradients descending:
$\Theta = \Theta - \eta \nabla_\Theta (\mathcal{L}_{sup} + \lambda \mathcal{L}_{con})$
\ENDWHILE
\STATE Output prediction $\mathbf{Z}$ via: $\mathbf{Z}= f_{mlp}(\frac{1}{K+1}\sum_{k=0}^K\hat{\mathbf{A}}^k \mathbf{X}, \Theta)$.

\end{algorithmic}
\end{algorithm}


















































































\hide{



\section{Graph Random Networks}
\hide{
\begin{figure*}
  		\centering
  		\includegraphics[width=0.8 \linewidth,height =  0.25\linewidth]{grand2.pdf}
 	\caption{Diagram of the training process of \model.  
 	As a general semi-supervised learning framework on graphs, \model\ provides two mechanisms---random propagation and consistency regularization----to enhance the prediction model's robustness and generalization. In each epoch, given the input graph, \model\ first generates $S$ graph data augmentations $\{\widetilde{X}^{(s)}| 1\leq s \leq S\}$ via random propagation layer. Then, each $\widetilde{X}^{(s)}$ is fed into a prediction model, leading to the corresponding prediction distribution $\widetilde{Z}^{(s)}$. In optimization, except for optimizing the supervised classification loss $\mathcal{L}^l$ with the given labels $Y^L$, we also minimize the prediction distance among different augmentations of unlabeled nodes via unsupervised consistency loss $\mathcal{L}^u$. }
\label{fig:model}
\end{figure*}
}
To achieve a better model for semi-supervised learning on graphs, we propose Graph Random Networks (\model). In \model , different with other GNNs, each node is allowed to randomly interact with different subsets of neighborhoods in different training epochs. This random propagation mechanism is shown to be an economic way for stochastic graph data augmentation. Based on that, we also design a consistency regularized training method to improve model's generalization capacity by encouraging predictions invariant to different augmentations of the same node. 

\hide{
Graph is complex with highly tangled nodes and edges, while previous graph models, e.g., GCN and GAT, take the node neighborhood as a whole and follow a determined aggregation process recursively. These determined models, where nodes and edges interact with each other in a fixed way, suffer from overfitting and risk of being misguiding by small amount of potential noise, mistakes, and malevolent attacks. To address it, we explore graph-structured data in a random way. In the proposed framework, we train the graph model on massive augmented graph data, generated by random sampling and propagation. In different training epochs, nodes interact with different random subsets of graph, and thus mitigate the risk of being misguiding by specific nodes and edges. On the other hand, the random data augmentation alleviates the overfitting and the implicit ensemble style behind the process improves the capacity of the representation. As the model should generalize well and have a similar prediction on unlabeled data on different data augmentations, despite the potential divergence from sampling randomness, we can further introduce an unsupervised graph-based regularization to the framework.
}

\hide{
In a graph neural model, if any given node's  representation (including unlabeled nodes') has a sufficient and consistent performance in any random subgraphs, that is, any node representation disentangles with specific nodes or edge links, which can be swapped by others, the graph neural model can effectively alleviate the  overfitting to specific graph structure and being misguided by noise, mistakes, and malevolent attacks. One the other hand, the graph neural model trained on exponentially many random subgraphs will also explore graph structures in different levels as sufficiently as possible, while previous models, e.g., GCN and GAT, model the neighborhood as a whole. Based on it, the proposed model, \model, utilizes a series of random sampling strategies to explore graph structures. We found that sampling strategy coupled with graph propagation is an efficient way of graph data augmentation. By leveraging the consistency of abundant unlabeled data on different random augmentations, we can further lower the generalization loss.
}


\hide{
\subsection{Over-smoothing Problem }
The over-smoothing issue of GCNs was first studied in \cite{li2018deeper}, which indicates that node features will converge to a fixed vector as the network depth increases. This undesired convergence heavily restricts the expressive power of deep GCNs. Formally speaking, suppose $G$ has $P$ connected components $\{C_i\}^{P}_{i=1}$. Let $\mathbbm{1}^{(i)} \in \mathbb{R}^n$ denote the indication vectors for the $i^{th}$ component $C_i$, which indicates whether a node belongs to $C_i$  i.e.,
	\begin{equation*}
\mathbbm{1}_j^{(i)} = \left\{ \begin{aligned}
1, &v_j \in C_i \\
0, &v_j \notin C_i  \end{aligned}\right..
\end{equation*}
 The over-smoothing phenomenon of GCNs is formulated as the following theorem:
\begin{theorem}
	Given a graph $G$ which has $P$ connected components $\{C_i\}^{P}_{i=1}$, for any $\mathbf{x} \in \mathbb{R}^n$, we have:
	\begin{equation*}
		\lim_{k\rightarrow +\infty} \hat{A}^k x = \widetilde{D}^{\frac{1}{2}}[\mathbbm{1}^{(1)}, \mathbbm{1}^{(2)}, ..., \mathbbm{1}^{(P)}] \hat{x}
	\end{equation*}
	where $\hat{x} \in \mathbb{R}^P$ is a vector associated with $x$.  

\end{theorem}
From this theorem, we could notice that after repeatedly performing the propagation rule of GCNs many times, node features will converge to a linear combination of $\{D^{\frac{1}{2}}\mathbbm{1}^{(i)}\}$. And the nodes in the same connected component only distinct by their degrees. In the above analyses, the activation function used in GCNs is assumed to be a linear transformation, but the conclusion can also be generalized to non-linear case\cite{oono2019graph}.
}

\begin{figure*}
    \centering
    \includegraphics[width= \linewidth]{grand6.pdf}
    \caption{Architecture of \model.}
    \label{fig:arch1}
\end{figure*}

\subsection{Architecture}
The architecture of \model\ is illustrated in Figure~\ref{fig:arch1}. Overall, the model includes two components, i.e., Random propagation module and classification module.


\subsubsection{Random Propagation Module.}
\label{sec:randpro}
\begin{figure}
    \centering
    \includegraphics[width=0.8
    \linewidth]{dropnode_vs_dropout2.pdf}
    \caption{Difference between DropNode and dropout. Dropout drops  elements of $X$ independently. While dropnode drops feature vectors of nodes (row vectors of $X$) randomly.}
    \label{fig:dropnode_vs_dropout}
\end{figure}



\begin{algorithm}[tb]
\caption{Dropnode}
\label{alg:dropnode}
\begin{algorithmic}[1] 
\REQUIRE ~~\\
Original feature matrix $X \in R^{n \times d}$, DropNode probability 
$\delta \in (0,1)$. \\
\ENSURE ~~\\
Perturbed feature matrix  $X\in R^{n \times d}$.
\IF{mode == Inference}
\STATE $\widetilde{X} = X$.
\ELSE
\STATE Randomly sample $n$ masks: $\{\epsilon_i \sim Bernoulli(1-\delta)\}_{i=1}^n$.
\STATE Obtain deformity feature matrix by  multiplying each node's feature vector with the corresponding  mask: $\widetilde{X}_{i} = \epsilon_i \cdot X_{i} $.
\STATE Scale the deformity features: $\widetilde{X} = \frac{\widetilde{X}}{1-\delta}$.
\ENDIF
\end{algorithmic}
\end{algorithm}

\label{sec:randpro}


In random propagation, we aim to perform message passing in a random way during model training. To achieve that, we add an extra node sampling operation called ``DropNode'' in front of the propagation layer.


\vpara{DropNode.} In DropNode, feature vector of each node is randomly removed (rows of $X$ are randomly set to $\vec{0}$) with a pre-defined probability $\delta \in (0,1)$ at each training epoch. The resultant perturbed feature matrix $\widetilde{X}$ is fed into the propagation layer later on.
More formally, we first randomly sample a binary mask $\epsilon_i \sim Bernoulli(1-\delta)$ for each node $v_i$, and obtain the perturbed feature matrix $\widetilde{X}$ by multiplying each node's feature vector with the corresponding mask, i.e., $\widetilde{X}_i=\epsilon_i \cdot X_i$. Furthermore, we scale $\widetilde{X}$ with a factor of $\frac{1}{1-\delta}$ to guarantee the perturbed feature matrix is equal to $X$ in expectation. Please note that the sampling procedure is only performed during training. In inference time, we directly let $\widetilde{X}$ equal to the original feature matrix $X$. The algorithm of DropNode is shown in Algorithm \ref{alg:dropnode}. 

DropNode is similar to dropout\cite{srivastava2014dropout}, a more general regularization method used in deep learning to prevent overfitting. 
In dropout, the elements of $X$ are randomly dropped out independently. While DropNode drops a node's whole features together, serving as a node sampling method. 
Figure \ref{fig:dropnode_vs_dropout} illustrates their differences.
Recent study suggests that a more structured form of dropout, e.g., dropping a contiguous region in DropBlock\cite{ghiasi2018dropblock}, is required for better regularizing CNNs on images. From this point of view, dropnode can be seen as a structured form of dropout on graph data.
We also demonstrate that dropnode is more suitable for graph data than dropout both theoretically (Cf. Section \ref{sec:theory}) and experimentally (Cf. Section \ref{sec:ablation}). 




After dropnode, the perturbed feature matrix $\widetilde{X}$ is fed into a propagation layer to perform message passing. Here we adopt mixed-order propagation, i.e.,

\begin{equation}
\label{equ:kAX}
	\overline{X} = \overline{A} \widetilde{X}.
\end{equation}
Here we define the propagation matrix as $\overline{A} =  \frac{1}{K+1}\sum_{k=0}^K\hat{A}^k$, that is, the average of the power series of $\hat{A}$ from order 0 to order $K$. This kind of propagation rule enables model to incorporate multi-order neighborhoods information, and have a lower risk of over-smoothing compared with using $\hat{A}^K$ when $K$ is large. Similar ideas have been adopted in previous works~\cite{abu2019mixhop,abu2018n}. We compute the Equation \ref{equ:kAX} by iteratively calculating the product of sparse  matrix $\hat{A}^k$ and $\widetilde{X}$. The corresponding time complexity is $\mathcal{O}(Kd(n+|E|))$.


\hide{
In other words, the original features of about $\delta |V|$ nodes are removed (set as $\vec{0}$).
Obviously, this random sampling strategy may destroy the information carried in nodes and the resultant corrupted feature matrix is insufficient for prediction. To compensate it, we try to recover the information in a graph signal propagation process, and get the recovery feature matrix $\widetilde{X}$. The propagation recovery process is:
\begin{equation}
\label{equ:kAX}
	\widetilde{X} = \frac{1}{K+1}\sum_k^K\hat{A}^k X^{'}.
\end{equation}

It also offers a general way for implicit model ensemble on exponentially many augmented data.
}
\hide{
In each epoch, given the input graph, \model\ first generates $S$ graph data augmentations $\{\widetilde{X}^{(s)}| 1\leq s \leq S\}$ via random propagation layer. Then, each $\widetilde{X}^{(s)}$ is fed into a prediction model, leading to the corresponding prediction distribution $\widetilde{Z}^{(s)}$. In optimization, except for optimizing the supervised classification loss $\mathcal{L}^l$ with the given labels $Y^L$, we also minimize the prediction distance among different augmentations of unlabeled nodes via unsupervised consistency loss $\mathcal{L}^u$.}







\hide{
\subsection{Motivation}
Our motivation is that GCNs can not sufficiently leverage unlabeled data in this task. Specifically,


\subsection{Overview of Graph Random Network}
    In this section, we provide a brief introduction of the proposed graph random network. The basic idea of \model is to promote GCNs' generalization and robustness by taking the full advantage of unlabeled data in the graph.


Random propagation layer consists of graph dropout and propagation.
We first introduce how to generate a subgraph by random sampling nodes.
Formally, we randomly sample the nodes without replacement at a probability $1-\delta$  and drop the rest.
The deformity feature matrix $X^{'}$ is formed in the following way,
\begin{align}
\label{equ:samplingX1}
\left\{
\begin{aligned}
	& Pr(X^{'}_{i}=\vec{0}) = \delta, \\&Pr(X^{'}_{i}=X_i) = 1-\delta. \end{aligned}
\right.
\end{align}

In other words, the original features of about $\delta |V|$ nodes are removed (set as $\vec{0}$).
Obviously, this random sampling strategy may destroy the information carried in nodes and the resultant corrupted feature matrix is insufficient for prediction. To compensate it, we try to recover the information in a graph signal propagation process, and get the recovery feature matrix $\widetilde{X}$. The propagation recovery process is:
\begin{equation}
\label{equ:kAX}
	\widetilde{X} = \frac{1}{K}\sum_k^K\hat{A}^k X^{'}.
\end{equation}

\hide{
\begin{equation}
\label{Equ:denoise}
\widetilde{X} = \arg\min_{\widetilde{X}} \frac{1}{2}\left\| \widetilde{X}- X^{'}\right\|^2_2 + \alpha \frac{1}{2}\left \| \widetilde{X} - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} \widetilde{X}\right\|^2_2
\end{equation}  
\begin{small}
\begin{equation}
\label{Equ:denoise}
\widetilde{X}=\arg\min_{\widetilde{X}} 
   \frac{1}{2}\left(\sum_{i, j=1}^{n} A_{i j}\left\|\frac{1}{\sqrt{D_{i i}}} \widetilde{X}_{i}-\frac{1}{\sqrt{D_{j j}}} \widetilde{X}_{j}\right\|^{2}+\mu \sum_{i=1}^{n}\left\|\widetilde{X}_{i}-X^{'}_{i}\right\|^{2}\right)
\end{equation}  
\end{small}
where the first term keeps the denoised signal similar to the measurement and the second term enforces the smoothing of the solution. By setting the derivate of $\widetilde{X}$ to zero, we can obtain the solution to the Equation \ref{Equ:denoise}:
\begin{equation}
\label{Equ:denoise2}
\widetilde{X} = (I+\alpha \tilde{L})^{-1}X^{'}
\end{equation}

To avoid the inversion operation in Equation \ref{Equ:denoise2}, we derive an approximate solution of Equation \ref{Equ:denoise2} using Taylor Extension:
\begin{equation}
\label{Equ:denoise3}
\widetilde{X} = (I + \alpha \tilde{L}^2)^{-1}X^{'} \approx \sum_{i=0}^K (-\alpha\tilde{L}^2)^i X^{'} 
\end{equation}
}

Combining Equation \ref{equ:samplingX1} and \ref{equ:kAX}, the original feature matrix $X$ is firstly \hide{heavily} corrupted by sampling and then smoothed by graph propagation. In fact, the whole procedure consisting of two opposite operations provides a new representation $\widetilde{X}$ for the original $X$. Later we will verify that $\widetilde{X}$ is still a good representation substitute with a sufficient prediction ability, while the randomness inside the process helps the model avoid the over-dependence on specific graph structure and 
explore different graph structure in a more robust way.
We treat these opposite operations as a whole procedure and call them \textit{random propagation layer} in the proposed model \model. 
\hide{After filtered by random sampling operation in random propagation layer, the subsequent model parts only involve interaction among nearly half of the original node features and thus graph-structure data are disentangled somehow.}

Here, we generalize the random propagation layer with another sampling strategies. Instead of removing the feature of a node entirely by sampling once, we drop the element of the feature in each dimension one by one by multiple sampling. This is a multi-channel version of node sampling. Furthermore, we scale the remaining elements with a factor of $\frac{1}{1-\delta}$ to 
 guarantees the deformity feature matrix or adjacency matrix are the same as the original in expectation. Similar idea of dropping and rescaling has been also used in Dropout~\cite{srivastava2014dropout}. Hence we called our three graph sampling strategy graph dropout (\textit{dropnode} and \textit{dropout}), and formulate them following:
 
\vpara{Dropnode.} In dropnode, the feature vector of each node is randomly dropped with a pre-defined probability $\delta \in (0,1)$ during the propagation. More formally, we first form a deformity feature matrix $X^{'}$ in the following way:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{i}=\vec{0}) = \delta,& \\
&Pr(X^{'}_{i}= \frac{X_i}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then let $X^{'}$ propagate in the graph via Equation~\ref{equ:kAX}. 

\vpara{Dropout.} In dropout, the element of the feature matrix is randomly dropped with a probability $\delta \in (0,1)$ during the propagation. Formally, we have:
\begin{align}
\label{equ:featuredropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{ij}=0) = \delta,& \\
&Pr(X^{'}_{ij}= \frac{X_{ij}}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then we  propagate $X^{'}$ in the graph via Equation~\ref{equ:kAX}. 



Note that dropout in graph dropout shares the same definition of Dropout~\cite{srivastava2014dropout}, a more general method widely used in deep learning to prevent parameters from overfitting. However, dropout in graph dropout, as a multi-channel version of dropnode, is mainly developed to explore graph-structured data in the semi-supervised learning framework. The graph dropout strategy, directly applied to graph objects, is also an efficient way of graph data augmentation and  model ensemble on exponentially many subgraphs. As for the common dropout applied to the input units, the optimal probability of drop is usually closer to 0 to prevent the loss of information~\cite{srivastava2014dropout}, which is not the case in our graph dropout. In this paper we will further analyze theoretically that graph dropout help \model\ leverage unlabeled data, and dropnode and dropout play different roles. 
}


\hide{
Note that dropout is applied to neural network activation to prevent overfitting of parameters while our node dropout and edge dropout are coupled with graph propagation, performing on graph objects without parameters. We will reveal that our graph dropout strategy is an efficient way of graph data augmentation and graph models ensemble on exponentially many subgraphs. We will further analyse theoretically that graph dropout help \model\ leverage unlabeled data. \reminder{}
}

\vpara{Stochastic Graph Data Augmentation.}
Feature propagation have been proven to be an effective method for enhancing node representation by aggregating information from neighborhoods, and becomes the key step of Weisfeiler-Lehman Isomorphism test~\cite{shervashidze2011weisfeiler} and GNNs. In random propagation, some nodes are first randomly dropped before propagation. That is, each node only aggregates information from a subset of multi-hop neighborhoods randomly. In this way, random propagation module stochastically generates different representations for a node, suggesting an efficient stochastic augmentation method for graph data. From another point of view, random propagation can also be seen as injecting random noise to the propagation procedure by dropping a portion of nodes . 

We also conduct an interesting experiment to examine the validation of this data augmentation method. In this setting, we first sample a set of augmented node representations $\overline{X}$ from random propagation module with different drop rates, then use $\overline{X}$ to train a GCN for node classification. 
By examining the classification accuracy on $\overline{X}$, we can empirically analyze the influence of the injected noise on model performance. The results with different drop rates have been shown in Figure \ref{fig:redundancy}. It can be seen that the performance only decreases by less than $3\%$ as the drop rate increases from $0$ to $0.5$, indicating the augmented node representations is still sufficient for prediction\footnote{In practice, the drop rate is always set to $0.5$}. 
 Though $\overline{X}$  is still inferior to $X$ in performance, in practice, multiple augmentations will be utilized for training prediction model since the sampling will be performed in every epoch. From the view of bagging~\cite{breiman1996bagging}, this random data augmentation scheme makes the final prediction model implicitly ensemble models on exponentially many augmentations, yielding much better performances than using deterministic propagation.
 
  


\begin{figure}
  		\centering
  		\includegraphics[width = 0.8 \linewidth, height =  0.58\linewidth]{drop_rate.pdf}
  	\caption{Classification results of GCN with $\overline{X}$ as input.} 
  	\label{fig:redundancy}
\end{figure}
  






\subsubsection{Prediction Module.}
In prediction module, the augmented feature matrix $\overline{X}$ is fed into a neural network to predict nodes labels. We employ a two-layer MLP as the classifier:
\begin{equation}
\label{equ:mlp}
    P(\mathcal{Y}|\overline{X};\Theta) = \sigma_2(\sigma_1(\overline{X}W^{(1)})W^{(2)})
\end{equation}
where $\sigma_1$ is ReLU function, $\sigma_2$ is softmax function, $\Theta=\{W^{(1)}, W^{(2)}\}$ refers to model parameters. Here the classification model can also adopt more complex GNN based node classification models, e.g., GCN, GAT.  But we find the performance decreases when we replace MLP with GNNs because of the over-smoothing problem. We will explain this phenomenon in Section \ref{sec:oversmoothing}.






\hide{
From another perspective, Equation \ref{equ:samplingX1} and \ref{equ:kAX} in the random propagation layer perform a data augmentation procedure by linear interpolation~\cite{devries2017dataset} in a random subset of the multi-hop neighborhood.
}


 


\hide{
In a graph, we random sample a fixed proportion of neighborhoods for each node $v_i \in \mathcal{V}$, and let $v_i$ only interact with the sampled neighborhoods in propagation. Using this method, we are equivalent to let each node randomly perform linear interpolation with its neighbors. 

However, generating neighborhood samples for each node always require a time-consuming preprocessing. For example, the sampling method used in GraphSAGE~\cite{hamilton2017inductive} requires $k \times n$ times of sampling operations, where $k$ denotes the size of sampled neighbor set. To solve this problem, here we propose an efficient sampling method to perform random propagation --- graph dropout. Graph dropout is inspired from Dropout\cite{srivastava2014dropout}, a widely used regularization method in deep learning. \reminder{graph dropout is an efficient sampling method}
In graph dropout, we randomly drop a set of nodes or edges in propagation, which is introduced separately. \\

\vpara{Edge Dropout.} The basic idea of edge dropout is randomly dropping a fix proportion of edges during each propagation. 
Specifically, we construct a deformity feature matrix $\hat{A}^{'}$ following:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(\hat{A}^{'}_{ij}=\vec{0}) = \delta.& \\
&Pr(\hat{A}^{'}_{ij}= \frac{\hat{A}^{'}_{ij}}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then we use $\hat{A}^{'}$ as the replacement of $\hat{A}$ in propagation.  


\vpara{Node Dropout.} In node dropout, the feature vector of each node is randomly dropped with a pre-defined probability $\delta \in (0,1)$ during propagation. More formally, we first form a deformity feature matrix $X^{'}$ in the following way:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{i}=\vec{0}) = \delta.& \\
&Pr(X^{'}_{i}= \frac{X_i}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then let $X^{'}$ propagate in graph as the substitute of $X$, i.e., $\tilde{X} = \hat{A}X$. 

Actually, node dropout can be seen as a special form of edge dropout:
}



\subsection{Consistency Regularized Training}
\label{sec:consis}
\begin{figure*}
	\centering
	\includegraphics[width= \linewidth]{grand_consis.pdf}
	\caption{Illustration of consistency regularized training for \model. \yd{how about making the random progagation module into two as well?}}
	\label{fig:arch2}
\end{figure*}
As mentioned in previous subsection, random propagation module can be seen as an efficient method of stochastic data augmentation. That inspires us to design a consistency regularized training algorithm for optimizing parameters. 
  In the training algorithm, we generate multiple data augmentations at each epoch by performing dropnode multiple times. Besides the supervised classification loss, we also add a consistency regularization loss to enforce model to give similar predictions across different augmentations. This algorithm is illustrated in Figure \ref{fig:arch2}.


\begin{algorithm}[tb]
\caption{Consistency Regularized Training for \model}
\label{alg:2}
\begin{algorithmic}[1] \REQUIRE ~~\\
 Adjacency matrix $\hat{A}$,
labeled node set $V^L$,
 unlabeled node set $V^U$,
feature matrix $X \in R^{n \times d}$, 
times of dropnode in each epoch $S$, dropnode probability $\delta$.\\
\ENSURE ~~\\
Prediction $Z$.
\WHILE{not convergence}
\FOR{$s=1:S$} 
\STATE Apply dropnode via Algorithm \ref{alg:dropnode}: $
\widetilde{X}^{(s)} \sim \text{dropnode}(X,\delta)$. 
\STATE Perform propagation: $\overline{X}^{(s)} = \frac{1}{K}\sum_{k=0}^K\hat{A}^k \widetilde{X}^{(s)}$.
\STATE Predict class distribution using MLP: $\widetilde{Z}^{(s)} = P(\mathcal{Y}|\overline{X}^{(s)};\Theta)$.
\ENDFOR
\STATE Compute supervised classification loss $\mathcal{L}_{sup}$ via Equation \ref{equ:loss} and consistency regularization loss via Equation \ref{equ:consistency}.
\STATE Update the parameters $\Theta$ by gradients descending:
$$\nabla_\Theta \mathcal{L}_{sup} + \lambda \mathcal{L}_{con}$$
\ENDWHILE
\STATE Output prediction $Z$ via Equation \ref{equ:inference}
\end{algorithmic}
\end{algorithm}

 
\subsubsection{$S-$augmentation Prediction}
At each epoch, we aim to generate $S$ different data augmentations for graph data $X$. To achieve that, we adopt $S-$sampling dropnode strategy in random propagation module. That is, we performing $S$ times of random sampling in dropnode to generate $S$ perturbed feature matrices $\{\widetilde{X}^{(s)}|1\leq s \leq S\}$. Then we propagate these features according to Equation \ref{equ:kAX} respectively, and hence obtain $S$ data augmentations $\{\overline{X}^{(s)}|1 \leq s \leq S\}$. Each of these augmented feature matrice are fed into the prediction module to get the corresponding output:

\begin{equation}
     \widetilde{Z}^{(s)} = P(\mathcal{Y}|\overline{X}^{(s)}; \Theta).
 \end{equation}
Where $\widetilde{Z}^{(s)} \in (0,1)^{n\times C}$ denotes the classification probabilities on the $s^{th}$ augmented data. 
\subsubsection{Supervised Classification Loss.}
The supervised objective of graph node classification in an epoch is the averaged cross-entropy loss over $S$ times sampling:
\begin{equation}
\label{equ:loss}
	\mathcal{L}_{sup} = -\frac{1}{S}\sum_{s=1}^{S}\sum_{i=1}^m \sum_{j=1}^{C}Y_{i,j} \ln \widetilde{Z}_{i,j}^{(s)} ,
\end{equation}

\noindent where $Y_{i,l}$ is binary, indicating whether node $i$ has the label $l$. By optimizing this loss, we can also enforce the model to output the same predictions on multiple augmentations of a labeled node.
 However, in the semi-supervised setting, labeled data is always very rare. In order to make full use of unlabeled data, we also employ consistency regularization loss in \model. 
 
\subsubsection{Consistency Regularization Loss.} How to optimize the consistency among $S$ augmentations of unlabeled data? 
 Let's first consider a simple case, where the random propagation procedure is performed only twice in each epoch, i.e., $S=2$. Then a straightforward method is to minimize the distributional distance between two outputs:
 \begin{equation}
 \label{equ:2d}
     \min \sum_{i=1}^n \mathcal{D}(\widetilde{Z}^{(1)}_i, \widetilde{Z}^{(2)}_i),
 \end{equation}
where $ \mathcal{D}(\cdot,\cdot)$ is the distance function. Then we extend it into multiple augmentation situation. We can first calculate the label distribution center by taking average of all distributions:
\begin{equation}
    \overline{Z} = \frac{1}{S}\sum_{s=1}^{S} \widetilde{Z}^{(s)}.
\end{equation}

And we can minimize the distributional distance between $\widetilde{Z}^{(s)}$ and $\overline{Z}$, i.e., $\min \sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{Z}_i, \widetilde{Z}^{(s)}_i)$.
\hide{
\begin{equation}
    \min \sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{Z}_i, \widetilde{Z}^{(s)}_i).
\end{equation}
}
However, the distribution center calculated in this way is always inclined to have more entropy value, which indicates to be more ``uncertain''. 
Thus this method will bring extra uncertainty into model's predictions. To avoid this problem, We utilize the label sharpening trick in \model. Specifically, we apply a sharpening function onto the averaged label distribution to reduce its entropy:
\begin{equation}
 \overline{Z}^{'}_{i,l} = \frac{\overline{Z}_{i,l}^{\frac{1}{T}}}{\sum_{j=1}^{|\mathcal{Y}|}\overline{Z}_{i,j}^{\frac{1}{T}}},
\end{equation}
where $0< T\leq 1$ acts as the ``temperature'' which controls the sharpness of the categorical distribution. As $T \to 0$, the sharpened label distribution will approach a one-hot distribution. As the substitute of $\overline{Z}$, we minimize the distance between  $\widetilde{Z}^i$ and $\overline{Z}^{'}$ in \model:
\begin{equation}
\label{equ:consistency}
    \mathcal{L}_{con} =   \frac{1}{S}\sum_{s=1}^{S}\sum_{i=1}^n \mathcal{D}(\overline{Z}^{'}_i, \widetilde{Z}^{(s)}_i).
\end{equation}

By setting $T$ as a small value, we could enforce the model to output low-entroy predictions. This can be seen as adding an extra entropy minimization regularization into the model, which assumes that classifier's decision boundary should not pass through high-density regions of the marginal data distribution\cite{grandvalet2005semi}. As for the  distance function $\mathcal{D}(\cdot, \cdot)$, we adopt squared $L_2$ loss, i.e., $\mathcal{D}(x, y)=\|x-y\|^2$, in our model. It has been proved to be less sensitive to incorrect predictions\cite{berthelot2019mixmatch}, and is more suitable to the semi-supervised setting compared to cross-entropy. 



\subsubsection{Training and Inference}
In each epoch, we employ both supervised classification loss (Cf. Equation \ref{equ:loss}) and consistency regularization loss (Cf. Equation \ref{equ:consistency}) on $S$ times of sampling. Hence, the final loss is:
\begin{equation}
\label{equ:inf}
	\mathcal{L} = \mathcal{L}_{sup} + \lambda \mathcal{L}_{con}.
\end{equation}
Here $\lambda$ is a hyper-parameter which controls the balance between supervised classification loss and consistency regularization loss.
In the inference phase, as mentioned in Section \ref{sec:randpro}, we directly use original feature $X$ as the output of dropnode instead of sampling. Hence the inference formula is:
\begin{equation}
\label{equ:inference}
Z= P\left(\mathcal{Y}~\bigg|~\frac{1}{K+1}\sum_{k=0}^K\hat{A}^k X;\hat{\Theta}\right).
\end{equation}
Here $\hat{\Theta}$ denotes the optimized parameters after training. We summarize our algorithm in Algorithm \ref{alg:2}. 


\hide{Note that $X$ is equal to the expectation of $\widetilde{X}$s from data augmentation by multiple sampling. From the view of model bagging~\cite{breiman1996bagging}, the final graph model implicitly aggregates models trained on exponentially many subgraphs, and performs a plurality vote among these models in the inference phase, resulting in a lower generalization loss. }
 
\hide{
\begin{equation}
\label{equ:model}
	\widetilde{Z}^{(s)}=p_{\text{model}}\left(\mathcal{Y}~\bigg|~\frac{1}{K}\sum_k^K\hat{A}^k X^{'}\right) \in R^{n \times |\mathcal{Y}|}.
\end{equation}



Hence the supervised objective of graph node classification in an epoch is the averaged cross-entropy loss over $S$ times sampling:
\begin{equation}
\label{equ:loss}
	\mathcal{L}^l = -\frac{1}{S}\sum_{s}^{S}\sum_{i\in V^L } \sum_l^{|\mathcal{Y}|}Y_{i,l} \ln \widetilde{Z}_{i,l}^s ,
\end{equation}

\noindent where $Y_{i,l}$ is binary, indicating whether node $i$ has the label $l$. 


In each epoch, we employ both supervised cross-entropy loss (Cf. Eq.\ref{equ:loss}) and unsupervised consistency loss (Cf. Eq.\ref{equ:consistency}) on $S$ times of sampling. Hence, the final loss is:
\begin{equation}
\label{equ:inf}
	\mathcal{L} = \mathcal{L}^l + \lambda \mathcal{L}^u.
\end{equation}
Here $\lambda$ is a hyper-parameter which controls the balance between supervised classification loss and unsupervised consistency loss.
In the inference phase, the output is achieved by averaging  over the results on exponentially many augmented test data. This can be economically realized by inputting $X$ without sampling:
\begin{equation}
\label{equ:inference}
Z= p_{\text{model}}\left(\mathcal{Y}~\bigg|~\frac{1}{K}\sum_k^K\hat{A}^k X\right).
\end{equation}

Note that $X$ is the average of $X^{'}$s from data augmentation by multiple sampling. From the view of model bagging~\cite{breiman1996bagging}, the final graph model implicitly aggregates models trained on exponentially many subgraphs, and performs a plurality vote among these models in the inference phase, resulting in a lower generalization loss.  
}






\hide{
\subsection{graph propagation for smoothing}

In this section, we introduce the random propagation layer, an efficient method to perform stochastic data augmentation on the graph. We first demonstrate the propagation layer in GCNs is actually a special form of data augmentation on graph-structured data. Based on this discovery, we propose random propagation strategy, which augments each node with a part of randomized selected neighborhoods \reminder{}. Finally we show that this strategy can be efficiently instantiated with a stochastic sampling method --- graph dropout.

\subsection{Feature Propagation as Data Augmentation}
\label{sec:aug}
An elegant data augmentation technique used in supervised learning is linear interpolation \cite{devries2017dataset}. Specifically, for each example on training dataset, we first find its $K-$nearest neighboring samples in feature space which share the same class label. Then for each pair of neighboring feature vectors, we get the augmented example using interpolation:
\begin{equation}
    x^{'}_i = \lambda x_i + (1-\lambda)x_j
\end{equation}
where $x_j$ and $x_i$ are neighboring pairs with the same label $y$, $x^{'}_i$ is augmented feature vector, and $\lambda \in [0,1]$ controls degree of interpolation\reminder{}. Then the example $(x^{'}_i, y)$ is used as extra training sample to facilitate the learning task. In this way, the model is encouraged to more smooth in input space and more robust against small perturbations. \reminder{why?}


As for the problem of semi-supervised learning on graphs, we assume the graph signals are smooth, i.e., \textit{neighborhoods have similar feature vectors and similar class labels}. Thus a straightforward idea of augmenting graph-structured data is interpolating node features with one of its neighborhoods' features. However, in the real network, there exist small amounts of neighboring nodes with different labels and we can't identify that for samples with non-observable labels \reminder{?}. Thus simple interpretation with one neighbor might bring uncontrollable noise into the learning framework. 
An alternative solution is interpolating samples with multiple neighborhoods, which leads to Laplacian Smoothing:

\begin{equation}
\label{equ:lap}
x^{'}_i = (1-\lambda) x_i + \lambda \sum_j \frac{\widetilde{A}_{ij}}{\widetilde{D}_{ii}} x_j
\end{equation}.
Here we follow the definition of GCNs which adding a self-loop for each node in graph\reminder{}. Rewriting Eq.~\ref{equ:lap} in matrix form, we have:
\begin{equation}
\label{equ:lap2}
    X^{'}=  (I-\lambda I)X + \lambda \widetilde{D}^{-1} \widetilde{A}X
\end{equation}.
As pointed out by Li et.al.\cite{li2018deeper}\reminder{}, when $\lambda = 1$ and replacing $\Tilde{D}^{-1} \Tilde{A}$ with the symmetric normalized adjacency matrix $\hat{A}$, Eq.~\ref{equ:lap2} is identical to the propagation layer in GCN, i.e., $X^{'}= \hat{A}X$.

}

\hide{
\subsection{Random Propagation with Graph Dropout}
With the insight from last Section~\ref{sec:aug}, we develop a random propagation strategy for stochastic data augmentation on graph.

During each time of random propagation, we random sample a fixed proportion of neighborhoods for each node $v_i \in \mathcal{V}$, and let $v_i$ only interact with the sampled neighborhoods in propagation. Using this method, we are equivalent to let each node randomly perform linear interpolation with its neighbors. 

However, generating neighborhood samples for each node always require a time-consuming preprocessing. For example, the sampling method used in GraphSAGE~\cite{hamilton2017inductive} requires $k \times n$ times of sampling operations, where $k$ denotes the size of sampled neighbor set. To solve this problem, here we propose an efficient sampling method to perform random propagation --- graph dropout. Graph dropout is inspired from Dropout\cite{srivastava2014dropout}, a widely used regularization method in deep learning. \reminder{graph dropout is an efficient sampling method}
In graph dropout, we randomly drop a set of nodes or edges in propagation, which is introduced separately. \\

\vpara{Edge Dropout.} The basic idea of edge dropout is randomly dropping a fix proportion of edges during each propagation. 
Specifically, we construct a deformity feature matrix $\hat{A}^{'}$ following:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(\hat{A}^{'}_{ij}=\vec{0}) = \delta.& \\
&Pr(\hat{A}^{'}_{ij}= \frac{\hat{A}^{'}_{ij}}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then we use $\hat{A}^{'}$ as the replacement of $\hat{A}$ in propagation.  


\vpara{Node Dropout.} In node dropout, the feature vector of each node is randomly dropped with a pre-defined probability $\delta \in (0,1)$ during propagation. More formally, we first form a deformity feature matrix $X^{'}$ in the following way:
\begin{align}
\label{equ:nodedropout}
\left\{
\begin{aligned}
& Pr(X^{'}_{i}=\vec{0}) = \delta.& \\
&Pr(X^{'}_{i}= \frac{X_i}{1-\delta} ) = 1- \delta. &
\end{aligned}
\right.
\end{align}
Then let $X^{'}$ propagate in graph as the substitute of $X$, i.e., $\tilde{X} = \hat{A}X$. 

Actually, node dropout can be seen as a special form of edge dropout: \reminder{Dropping a node is equivalent to drop all the edges start from the node.} \\

\reminder{connection between graph dropout and feature dropout. dropblock..}




}

\hide{
\section{Consistency Optimization}
\subsection{\drop: Exponential Ensemble of GCNs}
Benefiting from the information redundancy, the input of \shalf gives a relatively sufficient view for node classification.
It is straightforward to generalize \shalf to ensemble multiple \shalf s, as each set $\mathcal{C}$ provides a unique network view. In doing so, we can use $|\mathcal{C}_i \bigcap \mathcal{C}_j|$ to measure the correlation between two network data view $i$ and $j$. In particular, $\mathcal{C}$ and $V-\mathcal{C}$ can be considered independent. However, one thing that obstructs the direct application of traditional ensemble methods is that a network can generate exponential data views when sampling $n/2$ nodes from its $n$ nodes. 


\begin{figure}{
		\centering
\includegraphics[width = 1\linewidth]{fig_NSGCN.pdf}
		\caption{\sdrop and \dm. \drop: In each epoch we randomly drop half of the nodes and do the process of \half. The input in the inference phase is the original feature matrix without dropout, but with a discount factor of $0.5$. This method can be treated as the exponential ensemble of \half.
			\dm: In each epoch of \drop, we consider the half of the nodes that are dropped are independent to the remaining nodes and input them to another \drop. We minimize the disagreement of these two \drop s via the Jensen-Shannon divergence. This method can be treated as an economical co-training of two independent \drop s.}
		\label{fig:cotrain}
	}
\end{figure}

We propose a dropout-style ensemble model \drop.
In \drop, we develop a new network-sampling ensemble method---graph dropout, which samples the node set $\mathcal{C}$ from a network's node set and do the \shalf process in each training epoch. 
In the inference phase, we use the entire feature matrix with a discount factor $0.5$ as input~\cite{srivastava2014dropout}. 
The data flow in \drop\ is shown in Figure~\ref{fig:cotrain}. 


Specifically, given the recovery feature matrix $\widetilde{X}$ in a certain epoch, the softmax output of GCN in the training phase is $Z = GCN(\widetilde{X}) \in R^{n \times |\mathcal{Y}|}$. Hence the  objective of node classification in the network is


\begin{equation}
\label{equ:loss}
	Loss_{c1} = -\sum_{i\in V^L } \sum_l^{|\mathcal{Y}|}Y_{i,l} \ln Z_{i,l}
\end{equation}

\noindent where $Y_{i,l}$ is binary, indicating whether node $i$ has the label $l$.  


To make the model practical, in the inference phase, the output is achieved by using the entire feature matrix $X$ and a discount factor $0.5$~\cite{srivastava2014dropout}, that is, 

\begin{equation}
\label{equ:inf}
	\hat{Z} = GCN\left(\frac{0.5}{k}\sum_i^k\hat{A}^i X\right)
\end{equation}

The discount factor guarantees the input of GCNs in the inference phase is the same as the expected input in the training phase. Similar idea has been also used in~\cite{srivastava2014dropout}. 









\subsection{\dm: Disagreement Minimization}In this section, we present the \dm\ model, which leverages the independency between the chosen node set $\mathcal{C}$ and the dropout node set $V-\mathcal{C}$ in each epoch of \drop. 


In \drop, the network data view provided by the chosen set $\mathcal{C}$ is trained and the view provided by the remaining nodes $V-\mathcal{C}$ at each epoch is completely ignored. 
The idea behind \drop\ aims to train over as many as network views as possible. 
Therefore, in \dm, we propose to simultaneously train two \drop s with complementary inputs---$\mathcal{C}$ and $V-\mathcal{C}$---at every epoch. 
In the  training phase, we improve the two GCNs' prediction ability separately by minimizing the disagreement of these two models. 




The objective to minimize the disagreement of the two complementary \sdrop at a certain epoch can be obtained by Jensen-Shannon-divergence, that is, 

\begin{equation}
\label{equ:jsloss}
	Loss_{JS} = \frac{1}{2}\sum_{i\in V} \sum_l^{|\mathcal{Y}|} \left(Z'_{i,l} \ln \frac{Z'_{i,l}}{Z''_{i,l}} + Z''_{i,l} \ln \frac{Z''_{i,l}}{Z'_{i,l}}\right)
\end{equation}

\noindent where $Z'$ and $Z''$ are the outputs of these two \drop s in the training phase, respectively. 


The final objective of \sdm is

\begin{equation}
\label{equ:loss}
	Loss_{dm} = (Loss_{c1} + Loss_{c2})/2 + \lambda Loss_{JS}
\end{equation}

In the inference phase, the final prediction is made by the two GCNs together, i.e., 

\begin{equation}
\label{equ:inf}
	\hat{Z} = (\hat{Z}^{'} + \hat{Z}{''})/2
\end{equation}

Practically, the correlation of two \drop s and their individual prediction ability jointly affect the performance of \dm. 
The inputs to the two models are complementary  and usually uncorrelated, empowering \dm\ with more representation capacity than \drop, which is also evident from the empirical results in Section \ref{sec:exp}.  



\subsection{ Graph Dropout for Network Sampling}
In GraphSAGE~\cite{hamilton2017inductive} and FastGCN~\cite{chen2018fastgcn}, 
nodes or edges are sampled in order to accelerate training, avoid overfitting, and make the network data regularly like grids. However in the inference phase, these models may miss information when the sampling result is not representative. 

Different from previous works, our \drop\ and \dm\ models are based on the network redundancy observation and the proposed dropout-like ensemble enable the (implicit) training on exponential sufficient network data views, making them outperform not only existing sampling-based methods, but also the original GCN with the full single-view~\cite{kipf2016semi}. 
This provides 
insights into the network sampling-based GCNs and offers a new way to sample networks for GCNs.



In addition, there exist connections between the original dropout mechanism in CNNs and our dropout-like network sampling technique. 
Similarly, both techniques use a discount factor in the inference phase. 
Differently, the dropout operation in the fully connected layers in CNNs results in the missing values of the activation (hidden representation) matrix $H^{(l)}$, while our graph dropout sampling method is applied in network nodes, which removes network information in a structured way. 

Notice that very recently, DropBlock~\cite{ghiasi2018dropblock} suggested that a more structured form of dropout is required for better regularizing CNNs, and from this point of view, our graph dropout based network sampling is more in line with the idea of DropBlock. In \drop\ and \dm, dropping network information, instead of removing values from the activation functions, brings a structured way of removing information from the network. 
}






}%
