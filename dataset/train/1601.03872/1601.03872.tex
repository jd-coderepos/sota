In this section, three scientific case study applications are used to evaluate the benchmarking methods. An evaluation to validate the hypothesis of this research is considered by comparing the time taken to execute the benchmark on the VMs and comparing VM rankings generated by an empirical analysis, a heavyweight method and the two benchmarking methods. 

\begin{figure*}[ht]
	\centering
	\includegraphics[width=\textwidth]{figure4.png}
	\caption{Time taken for executing the benchmarks using 100MB, 500MB and 1000MB containers and on the whole VM}
	\label{evaluation:feasibility}
\end{figure*}

\subsection{Case Study Applications}
\label{studies:casestudy}
Three high-performance computing applications are chosen to evaluate the benchmarking methods. These applications are executed on VMs as shown in Table \ref{table1} with at least 15 GiB memory so that the applications have sufficient memory on the VM. 

The first case study is a molecular dynamics simulation of a system comprising 10,000 particles in a three dimensional space used by theoretical physicists \cite{md-1}. The simulation solves differential equations to model particles for different time steps. The simulation is memory intensive with numerous read and write operations and computationally intensive requiring a large number of float operations. Local communication between processes are less relevant and the application does not require file operations. 

The second case study is a risk simulation that generates probable maximum losses due to catastrophic events \cite{risk-1}. The simulation considers over a million alternate views of a given year and a number of financial terms to estimate losses. The simulation is memory intensive with numerous read and write operations and at the same time computationally intensive requiring a large number of float operations to be performed both to compute the risk metrics. The local communication between processes are less relevant and the application does not require file operations.

The third case study is a block triagonal solver, which is a NASA Parallel Benchmark (NPB), version 3.3.1
\footnote{https://www.nas.nasa.gov/publications/npb.html}
\cite{npb-1}. This mathematical solver is used on a grid size of $162 \times 162 \times 162$ for 200 iterations. The solver is numerically intensive and memory and processor related operations are relevant, but does not take precedence over computations. Local communications and file operations have little effect on the solver. 

\subsection{Evaluation}
\label{studies:evaluation}

The aims of the experimental evaluation are to address two important research questions related to lightweight benchmarking. They are: 1) how fast can lightweight benchmarking execute compared to a heavyweight technique that benchmarks the entire VM? and 2) how accurate will the generated lightweight benchmarks be?

\subsubsection{Execution Time of Benchmarks}
\label{eval:time}
The first question related to speed is addressed by demonstrating the feasibility of the proposed lightweight benchmarking methods in real-time on the cloud. For this, the time taken to execute the lightweight and heavyweight benchmarking techniques are compared as shown in Figure \ref{evaluation:feasibility}. On an average the 100 MB, 500 MB, and 1000 MB containers take 8 minutes, 13 minutes and 18 minutes to complete benchmarking on all the VMs. Benchmarking the whole VM takes up to 822 minutes for \texttt{hs1.4xlarge}.  
It is immediately evident that container-based benchmarking is between 19-91 times faster than the benchmarking the entire VM.  

\subsubsection{Accuracy of Benchmarks}
\label{evaluation:empiricalanalysis}
The second question related to accuracy is addressed by evaluating the lightweight methods against three real-world case study applications. For this, ranks obtained from DocLite are compared against actual ranks of VMs when the application is executed. The following steps are used to evaluate the accuracy of the benchmarks:
\begin{itemize}
\item \textit{Step 1} - Execute case study application on all VMs.
\item \textit{Step 2} - Generate empirical ranks for the case study.
\item \textit{Step 3} - Provide weights of the application to DocLite.
\item \textit{Step 4} - Obtain benchmark ranks for the application.
\item \textit{Step 5} - Find correlation of benchmark and empirical ranks.
\end{itemize}

\begin{figure*}[ht]
\centering
	\subfloat[Case study 1 - sequential]{\label{figure1-1}\includegraphics[width=0.325\textwidth]{figure1-2.png}} \hfill
	\subfloat[Case study 2 - sequential]{\label{figure1-2}\includegraphics[width=0.325\textwidth]{figure1-1.png}} \hfill
	\subfloat[Case study 3 - sequential]{\label{figure1-3}\includegraphics[width=0.325\textwidth]{figure1-3.png}}\\
	\subfloat[Case study 1 - parallel]{\label{figure2-1}\includegraphics[width=0.325\textwidth]{figure2-2.png}} \hfill
	\subfloat[Case study 2 - parallel]{\label{figure2-2}\includegraphics[width=0.325\textwidth]{figure2-1.png}} \hfill
	\subfloat[Case study 3 - parallel]{\label{figure2-3}\includegraphics[width=0.325\textwidth]{figure2-3.png}}
\caption{Sequential and parallel execution times for the case study applications}
\label{figure11}
\end{figure*}

In \textit{Step 1}, the three case study applications were executed on the VMs. The time taken to execute the application sequentially is presented in Figure \ref{figure1-1} to Figure \ref{figure1-3} and to execute the application in parallel using all available vCPUs is presented in Figure \ref{figure2-1} to Figure \ref{figure2-2}. In all case studies, the \texttt{cr1.8xlarge} and \texttt{cc2.8xlarge} have best performance; these VMs show good performance in memory and process and computation groups. The \texttt{m3} VMs are close competitors for sequential execution and \texttt{hi1.4xlarge} and \texttt{hs1.8xlarge} perform well for parallel execution. 
The results from parallel execution depend on the number of vCPUs available on the VM.

In \textit{Step 2}, the empirical ranks are generated using the standard competition ranking approach. The lowest time translates to the highest rank. If there are two VMs with the same program execution time they get the same rank and the ranking is continued with a gap. For example, in Figure \ref{figure1-1}, \texttt{m3.2xlarge} and \texttt{m3.xlarge} have the same program execution time. Both VMs have third rank and the next best performing VM, \texttt{hs1.8xlarge} obtains the fifth rank. 

In \textit{Step 3}, to generate the rankings from DocLite, a user provides the set of weights $W$ that characterise the case study applications, this is further explained in Section \cite{lightweight}. In consultation with domain scientists and practitioners, the weights for the three case studies are $\{4, 3, 5, 0\}$, $\{5, 3, 5, 0\}$ and $\{2, 0, 5, 0\}$ respectively. The above ranks were provided as input to the two benchmarking methods. 

Tables \ref{table2-1} to \ref{table2-3} show the empirical and benchmarking ranks for the three case studies using the lightweight container method as obtained in \textit{Step 4}. Tables \ref{table3-1} to \ref{table3-3} show the ranks for the case studies using the hybrid method (data from the lightweight container method along with data from the heavyweight method were considered). The historic benchmark data used in this paper was less than one month old when the hybrid method was executed.

Sequential and parallel ranks are generated for each case study using the weights. The empirical ranks are obtained from the timing results. The ranks obtained when using different sizes of the container are also reported in the tables. 

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|  }
\hline
\multirow{2}{*}{Amazon VM} & \multicolumn{4}{|c|}{Sequential Ranking} &
\multicolumn{4}{|c|}{Parallel Ranking}\\ \cline{2-9}
& Emp-irical & 100 MB & 500 MB & 1000 MB & Emp-irical & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
m1.xlarge   & 9    &10     &10     &10     &9      &10     &10     &10 \\
m2.xlarge   & 7     &4      &4      &5      &10     &8      &8      &8 \\
m2.2xlarge  & 6     &7      &6      &7      &7      &9      &9      &9 \\
m2.4xlarge  & 5     &6      &7      &6      &5      &6      &6      &6 \\
m3.xlarge   & 4     &3      &3      &3      &8      &7      &7      &7 \\
m3.2xlarge  & 3     &5      &5      &5      &6      &4      &3      &4 \\
cr1.8xlarge & 1     &1      &1      &1      &1      &1      &1      &1 \\
cc2.8xlarge & 2     &2      &2      &2      &2      &2      &2      &2 \\
hi1.4xlarge & 8     &8      &8      &8      &3      &3      &4      &3 \\
hs1.8xlarge & 10     &9      &9      &9      &4      &5      &5      &5 \\
\hline
\end{tabular}
\caption{Case Study 1: Empirical and benchmark rankings for lightweight container benchmarking}
\label{table2-1}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|  }
\hline
\multirow{2}{*}{Amazon VM} & \multicolumn{4}{|c|}{Sequential Ranking} &
\multicolumn{4}{|c|}{Parallel Ranking}\\ \cline{2-9}
& Emp-irical & 100 MB & 500 MB & 1000 MB & Emp-irical & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
m1.xlarge 	&	10	&	10	&	10	&	10	&	8	&	10	&	10	&	10\\
m2.xlarge 	&	6	&	5	&	5	&	4	&	10	&	8	&	8	&	8\\
m2.2xlarge 	&	6	&	7	&	6	&	7	&	7	&	9	&	9	&	9\\
m2.4xlarge 	&	6	&	6	&	7	&	6	&	5	&	6	&	6	&	6\\
m3.xlarge 	&	3	&	3	&	3	&	3	&	9	&	7	&	7	&	7\\
m3.2xlarge 	&	3	&	4	&	4	&	5	&	6	&	4	&	4	&	4\\
cr1.8xlarge 	&	1	&	1	&	1	&	1	&	2	&	1	&	1	&	1\\
cc2.8xlarge 	&	2	&	2	&	2	&	2	&	1	&	2	&	2	&	2\\
hi1.4xlarge 	&	9	&	8	&	8	&	8	&	4	&	3	&	3	&	3\\
hs1.8xlarge 	&	5	&	9	&	9	&	9	&	3	&	5	&	5	&	5\\
\hline
\end{tabular}
\caption{Case Study 2: Empirical and benchmark rankings for lightweight container benchmarking method}
\label{table2-2}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|  }
\hline
\multirow{2}{*}{Amazon VM} & \multicolumn{4}{|c|}{Sequential Ranking} &
\multicolumn{4}{|c|}{Parallel Ranking}\\ \cline{2-9}
& Emp-irical & 100 MB & 500 MB & 1000 MB & Emp-irical & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
m1.xlarge 	&	10	&	10	&	10	&	10	&	8	&	10	&	10	&	10\\
m2.xlarge 	&	6	&	5	&	5	&	5	&	10	&	8	&	8	&	8\\
m2.2xlarge 	&	7	&	7	&	7	&	7	&	7	&	9	&	9	&	9\\
m2.4xlarge 	&	5	&	6	&	6	&	6	&	5	&	6	&	6	&	6\\
m3.xlarge 	&	2	&	3	&	3	&	3	&	9	&	7	&	7	&	7\\
m3.2xlarge 	&	3	&	4	&	4	&	4	&	6	&	5	&	5	&	5\\
cr1.8xlarge 	&	1	&	1	&	1	&	1	&	1	&	2	&	2	&	2\\
cc2.8xlarge 	&	4	&	2	&	2	&	2	&	2	&	1	&	1	&	1\\
hi1.4xlarge 	&	8	&	8	&	8	&	8	&	3	&	3	&	3	&	3\\
hs1.8xlarge 	&	9	&	9	&	9	&	9	&	3	&	4	&	4	&	4\\
\hline
\end{tabular}
\caption{Case Study 3: Empirical and benchmark rankings for lightweight container benchmarking}
\label{table2-3}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|  }
\hline
\multirow{2}{*}{Amazon VM} & \multicolumn{4}{|c|}{Sequential Ranking} &
\multicolumn{4}{|c|}{Parallel Ranking}\\ \cline{2-9}
& Emp-irical & 100 MB & 500 MB & 1000 MB & Emp-irical & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
m1.xlarge 	&	9	&	10	&	10	&	10	&	9	&	10	&	10	&	10\\
m2.xlarge 	&	7	&	5	&	5	&	5	&	10	&	9	&	9	&	9\\
m2.2xlarge 	&	6	&	7	&	7	&	7	&	7	&	8	&	8	&	8\\
m2.4xlarge 	&	5	&	6	&	6	&	6	&	5	&	6	&	6	&	6\\
m3.xlarge 	&	4	&	3	&	3	&	3	&	8	&	7	&	7	&	7\\
m3.2xlarge 	&	3	&	4	&	4	&	4	&	6	&	4	&	4	&	4\\
cr1.8xlarge 	&	1	&	1	&	1	&	1	&	1	&	1	&	1	&	1\\
cc2.8xlarge 	&	2	&	2	&	2	&	2	&	2	&	2	&	2	&	2\\
hi1.4xlarge 	&	8	&	8	&	8	&	8	&	3	&	3	&	3	&	3\\
hs1.8xlarge 	&	10	&	9	&	9	&	9	&	4	&	5	&	5	&	5\\
\hline
\end{tabular}
\caption{Case Study 1: Empirical and benchmark rankings for hybrid benchmarking}
\label{table3-1}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|  }
\hline
\multirow{2}{*}{Amazon VM} & \multicolumn{4}{|c|}{Sequential Ranking} &
\multicolumn{4}{|c|}{Parallel Ranking}\\ \cline{2-9}
& Emp-irical & 100 MB & 500 MB & 1000 MB & Emp-irical & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
m1.xlarge 	&	10	&	10	&	10	&	10	&	8	&	10	&	10	&	10\\
m2.xlarge 	&	6	&	5	&	5	&	5	&	10	&	9	&	9	&	9\\
m2.2xlarge 	&	6	&	7	&	7	&	7	&	7	&	8	&	8	&	8\\
m2.4xlarge 	&	6	&	6	&	6	&	6	&	5	&	6	&	6	&	6\\
m3.xlarge 	&	3	&	3	&	3	&	3	&	9	&	7	&	7	&	7\\
m3.2xlarge 	&	3	&	4	&	4	&	4	&	6	&	4	&	4	&	4\\
cr1.8xlarge 	&	1	&	1	&	1	&	1	&	2	&	1	&	1	&	1\\
cc2.8xlarge 	&	2	&	2	&	2	&	2	&	1	&	2	&	2	&	2\\
hi1.4xlarge 	&	9	&	8	&	8	&	8	&	4	&	3	&	3	&	3\\
hs1.8xlarge 	&	5	&	9	&	9	&	9	&	3	&	5	&	5	&	5\\
\hline
\end{tabular}
\caption{Case Study 2: Empirical and benchmark rankings for hybrid benchmarking}
\label{table3-2}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|p{0.4cm}|  }
\hline
\multirow{2}{*}{Amazon VM} & \multicolumn{4}{|c|}{Sequential Ranking} &
\multicolumn{4}{|c|}{Parallel Ranking}\\ \cline{2-9}
& Emp-irical & 100 MB & 500 MB & 1000 MB & Emp-irical & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
m1.xlarge 	&	10	&	10	&	10	&	10	&	8	&	10	&	10	&	10\\
m2.xlarge 	&	6	&	5	&	5	&	5	&	10	&	9	&	9	&	9\\
m2.2xlarge 	&	7	&	7	&	7	&	7	&	7	&	8	&	8	&	8\\
m2.4xlarge 	&	5	&	6	&	6	&	6	&	5	&	6	&	6	&	6\\
m3.xlarge 	&	2	&	3	&	3	&	3	&	9	&	7	&	7	&	7\\
m3.2xlarge 	&	3	&	4	&	4	&	4	&	6	&	4	&	4	&	4\\
cr1.8xlarge 	&	1	&	1	&	1	&	1	&	1	&	1	&	1	&	1\\
cc2.8xlarge 	&	4	&	2	&	2	&	2	&	2	&	2	&	2	&	2\\
hi1.4xlarge 	&	8	&	8	&	8	&	8	&	3	&	3	&	3	&	3\\
hs1.8xlarge 	&	9	&	9	&	9	&	9	&	3	&	5	&	5	&	5\\
\hline
\end{tabular}
\caption{Case Study 3: Empirical and benchmark rankings for hybrid benchmarking}
\label{table3-3}
\end{table}

Given the rank tables for each case study it is important to determine the accuracy (or quality) of the ranks. In this paper, the accuracy of results is the correlation between the empirical ranks and the benchmark ranks. This quality measure validates the feasibility of using lightweight benchmarks and guarantees results obtained from benchmarking correspond to reality. 

\begin{table}[h]
\centering
\begin{tabular}{ |c|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|  }
\hline
\multirow{2}{*}{Case study} & \multicolumn{3}{|c|}{Sequential Ranking} &
\multicolumn{3}{|c|}{Parallel Ranking}\\ \cline{2-7}
& 100 MB & 500 MB & 1000 MB & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
1 & 89.1 & 87.9 & 92.1 & 90.3 & 86.7 & 90.3\\
2 & 88.5 & 88.5 & 84.7 & 83.0 & 83.0 & 83.0\\
3 & 95.2 & 95.2 & 95.2 & 87.6 & 87.6 & 87.6\\
\hline
\end{tabular}
\caption{Correlation (in \%) between empirical and benchmarking ranks for the lightweight benchmarking method}
\label{table4-1}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ |c|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|p{0.6cm}|  }
\hline
\multirow{2}{*}{Case study} & \multicolumn{3}{|c|}{Sequential Ranking} &
\multicolumn{3}{|c|}{Parallel Ranking}\\ \cline{2-7}
& 100 MB & 500 MB & 1000 MB & 100 MB & 500 MB & 1000 MB \\
\hline
\hline
1 & 93.9 & 93.9 & 93.9 & 93.9 & 93.9 & 93.9\\
2 & 88.5 & 88.5 & 88.5 & 86.7 & 86.7 & 86.7\\
3 & 95.2 & 95.2 & 95.2 & 88.8 & 88.8 & 88.8\\
\hline
\end{tabular}
\caption{Correlation (in \%) between empirical and benchmarking ranks for the hybrid benchmarking method}
\label{table4-2}
\end{table}

In \textit{Step 5}, the correlation of the benchmark ranks using different containers and the empirical ranks for benchmarking is determined and shown in Table \ref{table4-1} and Table \ref{table4-2}; the percentage value shows the degree of correlation. Higher the correlation value the more robust is the benchmarking method since it corresponds more closely to the empirical ranks.

Consider Table \ref{table4-1}, on an average there is over 90\% and 86\% correlation between the empirical and benchmarked sequential and parallel ranks respectively. It is observed that increasing the size of the container does not generally increase the correlation between the ranks. The smallest container of 100 MB performs as well as the other containers. 

There is an average improvement of 1\%-2\% in the correlation between the ranks (Table \ref{table4-2}). While the hybrid method can improve the ranks, it is observed that the position of the top three ranks are not affected. Again, using the smallest container does not change the quality of results.   

\subsection{Summary} 
The experimental studies considered container benchmarking both in the context of varying the memory size and number of virtual cores of the VM. Variation of the number of cores is evaluated in the sequential (1 virtual core) and parallel (maximum number of virtual cores available) execution of the benchmarks. The results indicate that real-time benchmarking can be achieved which in turn will be useful for decision making for real-time deployments of applications on the cloud. This is substantiated by Figure \ref{evaluation:feasibility}; nearly 14 hours are required to benchmark a large VM entirely, however, using containers it can be done in just 8 minutes. This is significant improvement. 

The following three key observations are summarised from the experimental studies:

\begin{itemize}
\item[i.] Small containers using lightweight benchmarks perform similar to large containers. No improvement is observed in the quality of results with larger containers. On average, there is over 90\% and 86\% correlation when comparing ranks obtained from the empirical analysis and the 100 MB container.

\item[ii.] The hybrid method can slightly improve the quality of the benchmark rankings, although the position of the top three ranks do not change. The lightweight method is sufficient to maximise the performance of an application on the cloud. Implementing hybrid methods will require the storage of historic benchmark data and its maintenance over time.

\item[iii.] Since container-based benchmarks takes lower execution time compared to executing them directly on the VM they can be used for real-time deployment of applications.

\end{itemize}
 
