









\documentclass[journal]{IEEEtran}











\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{color}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{threeparttable}
\usepackage{ulem}
\usepackage[switch]{lineno}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{cancel} 
\usepackage{caption}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=blue,      
	urlcolor=blue,
	citecolor=cyan,
}

\usepackage{array}
\usepackage{colortbl}

\newtheorem{theorem}{Theorem}{}
\newtheorem{corollary}{Corollary}{}
\newtheorem{remark}{Remark}{}









\ifCLASSINFOpdf
\else
\fi



































	
	
	


	
	
	


	
	
	


	
\hyphenation{op-tical net-works semi-conduc-tor}
	
	
	\begin{document}
\title{Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval}


\author{Xianghao~Zang,
			Ge~Li,
			and~Wei~Gao\thanks{This work was supported by the National Key R\&D Program of China (2020AAA0103501). (\textit{Corresponding author: Wei Gao.})}
			\thanks{Xianghao Zang, Ge Li and Wei Gao are with School of Electronic and Computer Engineering, Peking University, Shenzhen 518055, China (e-mail: zangxh@pku.edu.cn; geli@ece.pku.edu.cn; gaowei262@pku.edu.cn).}}





\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}





\IEEEpubid{\begin{minipage}{\textwidth}\ \\ \\ \\ \\
0000--0000/00\D_vD_hD_pD_vD_hD_p_+\{V_1, V_2, \cdots\}\{y_1, y_2, \cdots\}VKIV = \{I_1, I_2, \cdots, I_K\}f \in \mathbb{R}^{h \times w \times c}NN = h \cdot w1\times cksIp\phi_\text{cls}1 \times c\{\phi_\text{cls}; p_1; \dots; p_N\}\phi_\text{pos} \in \mathbb{R}^{(N+1)\times c}\hat{\phi}_\text{view}\hat{\phi}_\text{view}1\times c\hat{\phi}_\text{view}N+1\phi_\text{view} \in \mathbb{R}^{(N+1)\times c}z^0I\lambda_1\lambda_2mmz^mN\{p_i^m\}_{i=1}^N\mathrm{P}^m\mathrm{P}^m\widehat{\mathrm{P}}I\widehat{\mathrm{P}}f\in \mathbb{R}^{h\times w \times c}\widehat{\mathrm{P}}\widehat{\mathrm{P}}\widehat{\mathrm{P}}\phi_\text{cls}^m\widehat{\mathrm{P}}z_\text{global} \in \mathbb{R}^{(N+1)\times c}z^mL_{g}z'_\text{global} \in \mathbb{R}^{(N+1)\times c}\phi_\text{cls}^\text{global} \in \mathbb{R}^{1\times c}I\widehat{\mathrm{P}}D_vN/D_vD_vz_\text{vertical} \in \mathbb{R}^{(N/D_v+1)\times c}D_vz_{\text{vertical},1}z_{\text{vertical},i}L_{v}L_{v}D_v\{\phi_{\text{cls},i}^\text{vertical}\}_{i=1}^{D_v}\phi_\text{cls}^\text{vertical}\widehat{\mathrm{P}}D_hN/D_hD_hz_\text{horizontal} \in \mathbb{R}^{(N/D_h+1)\times c}z_{\text{horizontal},1}\{z_{\text{horizontal},i}\}_{i=1}^{D_h}L_{h}D_h\{\phi_{\text{cls},i}^\text{horizontal}\}_{i=1}^{D_h}\phi_\text{cls}^\text{horizontal}\widehat{\mathrm{P}}N/D_pD_pz_\text{patch} \in \mathbb{R}^{(N/D_p+1)\times c}D_p = D_v\times D_hz_{\text{patch},1}z_{\text{patch},i}L_{p}D_p\{\phi_{\text{cls},i}^\text{patch}\}_{i=1}^{D_p}\phi_\text{cls}^\text{patch}\widehat{\mathrm{P}}z_\phi\in\mathbb{R}^{(1+D_v+D_h+D_p)\times c}I\phi_\text{cls}^\text{global}\phi_\text{cls}^\text{vertical}\phi_\text{cls}^\text{horizontal}\phi_\text{cls}^\text{patch}\mathbb{R}^{1\times c}\mathbb{R}^{D_v\times c}\mathbb{R}^{D_h\times c}\mathbb{R}^{D_p\times c}KVz_\phi\bar{z}_\phiVV\bar{\phi}_\text{cls}^\text{global}\bar{z}_\phi\in\mathbb{R}^{(1\mathcal{+}D_v\mathcal{+}D_h\mathcal{+}D_p)\times c}VV\bar{z}_\phi\in\mathbb{R}^{(1\mathcal{+}D_v\mathcal{+}D_h\mathcal{+}D_p)\times c}\phi^\theta_\text{cls} \in \mathbb{R}^{1\times c}\bar{z}_\phi\phi^\theta_\text{cls}p^\theta1\mathcal{+}D_v\mathcal{+}D_h\mathcal{+}D_p\phi^\theta_\text{cls}\mathcal{L}_{cls}\mathcal{L}_{tri}T\mathcal{=}1\mathcal{+}D_v\mathcal{+}D_h\mathcal{+}D_pN_cb_i\text{i}^{\text{th}}Ba,p,nd(\cdot)\mathcal{L}_+\bar{z}_\phi \in\mathbb{R}^{(1\mathcal{+}D_v\mathcal{+}D_h\mathcal{+}D_p)\times c}VKKK256\times 128mksh\times w \times c21\times 10\times 768\lambda_1\lambda_21.01.5D_vD_hD_p1\mathcal{\times}210105\mathtt{\times}270\mathtt{\times}342\mathtt{\times}535\mathtt{\times}630\mathtt{\times}72\mathtt{\times}1053\mathtt{\times}705\mathtt{\times}426\mathtt{\times}357\mathtt{\times}306\mathtt{p}14\mathtt{p}15\mathtt{p}1\mathcal{\times}210\_105\mathcal{\times}2\_42\mathcal{\times}5\_30\mathcal{\times}71\mathcal{\times}210\_2\mathcal{\times}105\_5\mathcal{\times}42\_7\mathcal{\times}301\mathcal{\times}210\_6\mathtt{p}\_14\mathtt{p}\_15\mathtt{p}1\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}70\_6\mathtt{p}2, 3, 5, 6, 7D_vD_h2, 3, 5, 6, 7105\mathtt{\times}270\mathtt{\times}342\mathtt{\times}535\mathtt{\times}630\mathtt{\times}72\mathtt{\times}1053\mathtt{\times}705\mathtt{\times}426\mathtt{\times}357\mathtt{\times}30105\mathtt{\times}23\mathtt{\times}706105\mathtt{\times}27\mathtt{\times}301442\mathtt{\times}53\mathtt{\times}7015D_p6,14,156\mathtt{p}, 14\mathtt{p}, 15\mathtt{p}1\mathtt{\times}2101\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}70\_6\mathtt{p}1\mathcal{\times}2101\mathcal{\times}210\_105\mathcal{\times}21\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}701\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}70\_6\mathtt{p}2, 5, 76\mathtt{p}, 14\mathtt{p}, 15\mathtt{p}1\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}70\_6\mathtt{p}1\mathcal{\times}210\_105\mathcal{\times}2\_7\mathcal{\times}30\_14\mathtt{p}1\mathcal{\times}210\_42\mathcal{\times}5\_3\mathcal{\times}70\_15\mathtt{p}1\mathcal{\times}2101\mathcal{\times}210\_105\mathcal{\times}21\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}701\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}70\_6\mathtt{p}1\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}70\_6\mathtt{p}1\mathcal{\times}210105\mathcal{\times}23\mathcal{\times}706\mathtt{p}1\mathcal{\times}210\_105\mathcal{\times}2\_3\mathcal{\times}70\_6\mathtt{p}1\mathcal{\times}210\_105\mathcal{\times}2\_7\mathcal{\times}30\_14\mathtt{p}1\mathcal{\times}210\_42\mathcal{\times}5\_3\mathcal{\times}70\_15\mathtt{p}$ and achieves better performance. Although their fourth layers have a close number of parts, more horizontal parts introduce more performance improvement.

\subsubsection{Qualitative Analysis} \label{qualitative}
The retrieval examples are illustrated in Fig. \ref{fig:retrievalResults}. One pedestrian image is selected to represent the video for convenience, and the top eight retrieval results for each query are illustrated in this figure. 
We select the query pedestrian video from MARS benchmark according to the Average Precision (AP) value. Fig. \ref{fig:retrievalResults}(a)(b)(c)(d) show the successful cases where the proposed PiT has a better AP than the baseline method, and Fig. \ref{fig:retrievalResults}(e)(f) show the failed cases where the baseline method performs better.

For the successful case in Fig. \ref{fig:retrievalResults}(a)(b), the proposed PiT retrieves many correct videos in the gallery. In contrast, the baseline method retrieves many incorrect results, including a man in a blue shirt in Fig. \ref{fig:retrievalResults}(a) and a road sign in the second and third places in Fig. \ref{fig:retrievalResults}(b). For the failed case in Fig. \ref{fig:retrievalResults}(d), the baseline method puts the correct videos in the top two places. However, the proposed PiT also retrieves pedestrians with similar appearances. 
In Fig. \ref{fig:retrievalResults}, we employ the AP value to determine whether the proposed method is successful or not. For practical application, people usually look for the person of interest from the top-k results, not just the top-1 result. Therefore, the proposed PiT can be utilized effectively for Fig. \ref{fig:retrievalResults}(e)(f).

To explore the difference between the proposed PiT and the baseline method, the attention maps of the query pedestrian image are illustrated in Fig. \ref{attentions}. 
With the same input image, the proposed PiT and the baseline method have different attention maps. The baseline method cannot extract the fine-grained local features. Therefore, it cannot reduce the unfavorable impacts from the man in a blue shirt in Fig. \ref{attentions}(a) and the road sign in Fig. \ref{attentions}(b).
Therefore, more fine-grained feature representation can help the model recognize the pedestrian of interest.

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.48\textwidth]{FIG5_TII-21-4985.R1.pdf}
	\caption{Attention map examples. Three images in each group are the input image, the attention map of this image, and the product result between input image and its attention map.}
	\label{attentions}
\end{figure}

\subsubsection{Computation Complexity and Running Time} \label{sec:computation}
Table \ref{computation} adopts Multiply-ACcumulate operations (MACs) and trainable parameters to show the computation complexity. We use one 24G NVIDIA TITAN RTX GPU to conduct the experiments. For the MARS benchmark, the training and testing processes of the proposed PiT take 5.00 hours, including training 8,298 videos from 625 pedestrian IDs and testing another 11,310 videos for evaluation. For the iLIDS-VID benchmark, the experiments include ten trials to ensure statistical stability, and the total running time takes 10.70 hours. Each trial contains training 300 videos from 150 IDs and testing another 300 videos. In Table \ref{computation}, the proposed PiT has acceptable MACs, trainable parameters, and running time, which have the same order of magnitude as the baseline method. The superiority of the proposed method can be seen from the performance improvement in terms of the Rank-1 metric.

\begin{figure*}[htp]
	\centering
	\includegraphics[width=1\textwidth]{FIG6_TII-21-4985.R1.pdf}
	\caption{Retrieval examples. Each video is represented by one pedestrian image within it. The query video is selected from MARS benchmark, and the top eight retrieval results for each query are illustrated in this figure. (a)(b)(c)(d) show the successful cases, and (e)(f) show the failed cases. The correct results are in green boxes.}
	\label{fig:retrievalResults}
\end{figure*}

\section{Conclusion} \label{conlusion}
This paper proposes a multi-direction and multi-scale Pyramid in Transformer (PiT) for video-based pedestrian retrieval.
The proposed PiT contains four layers, and each layer applies different division strategies on the patch tokens to generate different-direction parts. The class token and the patch tokens in each generated part are fed to the corresponding transformer layer. In this way, the class token perceives the fine-grained, part-informed features. Then multi-direction and multi-scale features are combined to form a feature pyramid for each pedestrian image. The feature pyramids of pedestrian images belonging to the same video are fused to generate the final feature pyramid. Experimental results on two challenging benchmarks, MARS and iLIDS-VID, show the proposed PiT achieves state-of-the-art results. The comprehensive ablation studies demonstrate the superiority of the proposed multi-direction and multi-scale pyramid structure.







































\ifCLASSOPTIONcaptionsoff
\newpage
\fi







\normalem
\bibliographystyle{IEEEtran}
\bibliography{BIB_TII-21-4985.R1}
























	
	
\end{document}
