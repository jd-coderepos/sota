\documentclass{article} \usepackage{iclr2021_conference,times}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[pdftex]{graphicx}
\usepackage{euscript}
\usepackage{amsmath}
\usepackage{bbm} 
\usepackage{caption}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{maintheorem}[theorem]{Main Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}

\DeclareMathOperator{\diag}{diag}


\newcommand{\cW}{\EuScript{W}}
\newcommand{\by}{{\bf y}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bW}{{\bf W}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bV}{{\bf V}}
\newcommand{\bA}{{\bf A}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bB}{{\bf B}}
\newcommand{\bC}{{\bf C}}
\newcommand{\bo}{{\bf \omega}}
\newcommand{\ep}{\epsilon}
\newcommand{\ord}{{\mathcal O}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Dt}{{\Delta t}}
\newcommand{\E}{\EuScript{E}}
\newcommand{\ind}{\mathrm{I}}
\newcommand{\fref}[1] {Fig.~\ref{#1}}
\newcommand{\Tref}[1]{Table~\ref{#1}}

\title{Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies}




\author{T. Konstantin Rusch \\
  Seminar for Applied Mathematics (SAM)\\
  Department of Mathematics \\
  ETH Z\"urich\\
  Z\"urich, 8092, Switzerland \\
  \texttt{trusch@ethz.ch} \\
  \And
  Siddhartha Mishra \\
  Seminar for Applied Mathematics (SAM)\\
  Department of Mathematics \\
  ETH Z\"urich\\
  Z\"urich, 8092, Switzerland \\
  \texttt{smishra@ethz.ch} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data. 
\end{abstract}

\section{Introduction}
Recurrent neural networks (RNNs) have achieved tremendous success in a variety of tasks involving sequential (time series) inputs and outputs, ranging from speech recognition to computer vision and natural language processing, among others. However, it is well known that training RNNs to process inputs over long time scales (input sequences) is notoriously hard on account of the so-called \emph{exploding and vanishing gradient problem (EVGP)} \citep{vanish_grad}, which stems from the fact that the well-established BPTT algorithm for training RNNs requires computing products of gradients (Jacobians) of the underlying hidden states over very long time scales. Consequently, the overall gradient can grow (to infinity) or decay (to zero) exponentially fast with respect to the number of recurrent interactions.

A variety of approaches have been suggested to mitigate the exploding and vanishing gradient problem. These include adding \emph{gating mechanisms} to the RNN in order to control the flow of information in the network, leading to architectures such as \emph{long short-term memory} (LSTM) \citep{lstm} and \emph{gated recurring units} (GRU) \citep{gru}, that can overcome the vanishing gradient problem on account of the underlying additive structure. However, the gradients might still explode and learning very long term dependencies remains a challenge \citep{indrnn}. Another popular approach for handling the EVGP is to \emph{constrain} the structure of underlying recurrent weight matrices by requiring them to be orthogonal (unitary), leading to the so-called \emph{orthogonal RNNs} \citep{orthornn,urnn,eurnn,nnRNN} and references therein. By construction, the resulting Jacobians have eigen- and singular-spectra with unit norm, alleviating the EVGP. However as pointed out by \cite{nnRNN}, imposing such constraints on the recurrent matrices may lead to a significant loss of \emph{expressivity} of the RNN resulting in inadequate performance on realistic tasks.

In this article, we adopt a different approach, based on observation that \emph{coupled networks of controlled non-linear forced and damped oscillators}, that arise in many physical, engineering and biological systems, such as networks of biological neurons, do seem to ensure expressive representations while constraining the dynamics of state variables and their gradients. This motivates us to propose a novel architecture for RNNs, based on time-discretizations of second-order systems of non-linear ordinary differential equations (ODEs) \eqref{eq:ode1} that model coupled oscillators. Under verifiable hypotheses, we are able to \emph{rigorously prove precise bounds on the hidden states of these RNNs and their gradients, enabling a possible solution of the exploding and vanishing gradient problem}, while demonstrating through benchmark numerical experiments, that the resulting system still retains sufficient expressivity, i.e. ability to process complex inputs, with a competitive performance, with respect to the state of the art, on a variety of sequential learning tasks.  


\section{The proposed RNN}
Our proposed RNN is based on the following second-order system of ODEs, 
\begin{equation}
\label{eq:ode1}
\by^{\prime \prime} = \sigma\left(\bW \by +  {\bf \cW} \by^{\prime} + \bV \bu + \bb \right) -\gamma \by - \ep \by^{\prime}.
\end{equation}
Here, $t \in [0,1]$ is the (continuous) time variable, $\bu = \bu(t) \in  \R^d$ is the time-dependent \emph{input signal}, $\by = \by (t) \in \R^m$ is the \emph{hidden state} of the RNN with $\bW, {\bf \cW} \in \R^{m \times m}$, $\bV \in \R^{m \times d}$ are weight matrices,
$\bb \in \R^m$ is the bias vector and $0 < \gamma,\ep$ are parameters, representing oscillation frequency and the amount of damping (friction) in the system, respectively. $\sigma: \R \mapsto \R$ is the \emph{activation function}, set to $\sigma (u) = \tanh(u)$ here. By introducing the so-called \emph{velocity} variable $\bz = \by^{\prime}(t) \in \R^m$, we rewrite \eqref{eq:ode1} as the first-order system:
\begin{equation}
\label{eq:ode}
\by^{\prime} = \bz, \quad
\bz^{\prime}= \sigma\left(\bW \by +  \bf{\cW} \bz + \bV \bu + \bb \right)  - \gamma \by - \ep \bz.
\end{equation}
We fix a timestep $0 < \Dt < 1$ and define our proposed RNN hidden states at time $t_n = n \Dt \in [0,1]$ (while omitting the affine output state) as the following IMEX (implicit-explicit) discretization of the first order system \eqref{eq:ode}:
\begin{equation}
\label{eq:brnn}
\begin{aligned}
\by_n &= \by_{n-1} + \Dt \bz_n,\\
\bz_n &= \bz_{n-1} + \Dt \sigma\left(\bW\by_{n-1} +  {\bf\cW} \bz_{n-1} + \bV \bu_{n} + \bb \right) -\Dt \gamma \by_{n-1} -  \Dt \ep \bz_{\bar{n}},  
\end{aligned}
\end{equation}
with either $\bar{n} = n$ or $\bar{n}=n-1$. Note that the only difference in the two versions of the RNN \eqref{eq:brnn} lies in the implicit ($\bar{n}=n$) or explicit ($\bar{n}=n-1$) treatment of the damping term $-\ep\bz$ in \eqref{eq:ode}, whereas both versions retain the implicit treatment of the first equation in \eqref{eq:ode}.
\paragraph{Motivation and background.} To see that the underlying ODE \eqref{eq:ode} models a \emph{coupled network of controlled forced and damped nonlinear oscillators}, we start with the single neuron (scalar) case by setting $d=m=1$ in \eqref{eq:ode1} and assume an identity activation function $\sigma(x) = x$. Setting $\bW=\cW=\bV=\bb=\ep=0$ leads to the simple ODE, $\by^{\prime \prime} + \gamma \by = 0$, which exactly models \emph{simple harmonic motion} with \emph{frequency} $\gamma$, for instance that of a mass attached to a spring \citep{GHbook}. Letting $\ep > 0$ in \eqref{eq:ode1} adds \emph{damping} or friction to the system \citep{GHbook}. Then, by introducing non-zero $\bV$ in (\ref{eq:ode1}), we drive the system with a driving force proportional to the input signal $\bu(t)$. The parameters $\bV,\bb$ modulate the effect of the driving force, $\bW$ controls the frequency of oscillations and $\cW$ the amount of damping in the system. Finally, the $\tanh$ activation mediates a non-linear response in the oscillator. In the coupled network \eqref{eq:ode} with $m>1$, each neuron updates its hidden state based on the input signal as well as information from other neurons. The diagonal entries of $\bW$ (and the scalar hyperparameter $\gamma$) control the frequency whereas the diagonal entries of $\cW$ (and the hyperparameter $\epsilon$) determine the amount of damping for each neuron, respectively, whereas the non-diagonal entries of these matrices modulate interactions between neurons. Hence, given this behavior of the underlying ODE \eqref{eq:ode}, we term the RNN \eqref{eq:brnn} as a \emph{coupled oscillatory Recurrent Neural Network} (coRNN).

The dynamics of the ODE \eqref{eq:ode} (and the RNN \eqref{eq:brnn}) for a single neuron are relatively straightforward. As we illustrate in \fref{fig:1} of supplementary material {\bf SM}\S\ref{app:heuristics}, input signals drive the generation of (superpositions of) oscillatory wave-forms, whose amplitude and (multiple) frequencies are controlled by the tunable parameters $\bW,\cW,\bV,\bb$. Adding a $\tanh$ activation does not change these dynamics much. This is in contrast to truncating $\tanh$ to leading non-linear order by setting $\sigma(x) = x - x^3/3$, which yields a Duffing type oscillator that is characterized by chaotic behavior \citep{GHbook}. Adding interactions between neurons leads to further accentuation of this generation of superposed wave forms (see \fref{fig:1} in {\bf SM}\S\ref{app:heuristics}) and even with very simple network topologies, one sees the emergence of non-trivial non-oscillatory hidden states from oscillatory inputs. In practice, a network of a large number of neurons is used and can lead to extremely rich global dynamics. Hence, we argue that the ability of a network of (forced, driven) oscillators to access a very rich set of output states may lead to high expressivity of the system, allowing it to approximate outputs from complicated sequential inputs. 

Oscillator networks are ubiquitous in nature and in engineering systems \citep{GHbook,stgz2} with canonical examples being pendulums (classical mechanics), business cycles (economics), heartbeat (biology) for single oscillators and electrical circuits for networks of oscillators. Our motivating examples arise in neurobiology, where individual biological neurons can be viewed as oscillators with periodic spiking and firing of the action potential. Moreover, functional circuits of the brain, such as cortical columns and prefrontal-striatal-hippocampal circuits, are being increasingly interpreted by networks of oscillatory neurons, see \cite{ermen1} for an overview. Following well-established paths in machine learning, such as for convolutional neural networks \citep{DLnat}, our focus here is to abstract the essence of functional brain circuits being networks of oscillators and design an RNN based on much simpler mechanistic systems, such as those modeled by \eqref{eq:ode}, while ignoring the complicated biological details of neural function.

\paragraph{Related work.} There is an increasing trend of basing RNN architectures on ODEs and dynamical systems. These approaches can roughly be classified into two branches, namely RNNs based on discretized ODEs and continuous-time RNNs. Examples of continuous-time approaches include neural ODEs \citep{neuralODE} with ODE-RNNs \citep{ode_rnn} as its recurrent extension as well as \cite{E} and references therein, to name just a few.
We focus, however, in this article on an ODE-inspired discrete-time RNN, as the proposed coRNN is derived from a discretization of the ODE \eqref{eq:ode1}. A good example for a discrete-time ODE-based RNNs is the so-called \emph{anti-symmetric} RNN of \cite{anti}, where the RNN architecture is based on a stable ODE resulting from a skew-symmetric hidden weight matrix, thus constraining the stable (gradient) dynamics of the network. This approach has much in common with previously mentioned unitary/orthogonal/non-normal RNNs in constraining the structure of the hidden-to-hidden layer weight matrices. However, adding such strong constraints might reduce expressivity of the resulting RNN and might lead to inadequate performance on complex tasks. In contrast to these approaches, our proposed coRNN does not explicitly constrain the weight matrices but relies on the dynamics of the underlying ODE (and the IMEX discretization \eqref{eq:brnn}), to provide gradient stability. Moreover, no gating mechanisms as in LSTMs/GRUs are used in the current version of coRNN. There is also an increasing interest in designing \emph{hybrid} methods, which use a discretization of an ODE (in particular a Hamiltonian system) in order to learn the continuous representation of the data, see for instance \cite{hnn,srnn}. Overall, our approach here differs from these papers in our use of networks of oscillators to build the RNN.

\section{Rigorous analysis of the proposed RNN}
An attractive feature of the underlying ODE system \eqref{eq:ode} lies in the fact that the resulting hidden states (and their gradients) are bounded (see 
{\bf SM}\S\ref{sec:bd} for precise statements and proofs). Hence, one can expect that a suitable discretization of the ODE \eqref{eq:ode} that preserves these bounds will not have exploding gradients. We claim that one such \emph{structure preserving discretization} is given by the IMEX discretization that results in the RNN \eqref{eq:brnn} and proceed to derive bounds on this RNN below. 


Following standard practice we set $\by(0) = \bz(0) = 0$ and purely for the simplicity of exposition, we set the control parameters, $\ep=\gamma=1$ and $\bar{n}=n$ in \eqref{eq:brnn} leading to,
\begin{equation}
\label{eq:brnn1}
\begin{array}{ll}
\by_n &= \by_{n-1} + \Dt \bz_n,  \\
\bz_n &= \frac{\bz_{n-1}}{1+ \Dt}  + \frac{\Dt}{1 + \Dt}  \sigma(\bA_{n-1}) - \frac{\Dt}{1+ \Dt} \by_{n-1},~\bA_{n-1} :=  \bW\by_{n-1} +  {\bf \cW} \bz_{n-1} + \bV \bu_{n} + \bb.  
\end{array}
\end{equation}
Analogous results and proofs for the case where $\bar{n}=n-1$ and for general values of $\ep,\gamma$ are provided in {\bf SM}\S\ref{sec:exp}.
\paragraph{Bounds on the hidden states.} As with the underlying ODE \eqref{eq:ode}, the hidden states of the RNN \eqref{eq:brnn} are bounded, i.e.
\begin{proposition}
\label{prop:1}
Let $\by_n,\bz_n$ be the hidden states of the RNN \eqref{eq:brnn1} for $1\leq n \leq N$, then the hidden states satisfy the following (energy) bounds:
\begin{equation}
    \label{eq:enb}
    \by_n^\top \by_n + \bz_n^\top \bz_n \leq n m\Dt = mt_n \leq m.
\end{equation}
\end{proposition}
The proof of the \emph{energy} bound \eqref{eq:enb} is provided in {\bf SM}\S\ref{app:proof_energy_bound} and a straightforward variant of the proof (see {\bf SM}\S\ref{app:input_stable}) yields an estimate on the sensitivity of the hidden states to changing inputs. As with the underlying ODE (see {\bf SM}\S\ref{sec:bd}) , this bound \emph{rules out chaotic behavior of hidden states}.
\paragraph{Bounds on hidden state gradients.} We train the RNN \eqref{eq:brnn} to minimize the loss function,
\begin{equation}
\label{eq:lf1}
\E := \frac{1}{N}\sum\limits_{n=1}^N \E_n, \quad \E_n = \frac{1}{2} \|\by_n - \bar{\by}_n\|_2^2,
\end{equation}
with $\bar{\by}$ being the underlying ground truth (training data). During training, we compute gradients of the loss function \eqref{eq:lf1} with respect to the weights and biases $\bf \Theta = [W, \cW, V, b]$, i.e.
\begin{equation}
\label{eq:grad1}
\frac{\partial \E}{\partial \theta} = \frac{1}{N}\sum_{n=1}^N \frac{\partial \E_n}{\partial \theta}, \quad \forall ~\theta \in {\bf \Theta}.
\end{equation}
\begin{proposition}
\label{prop:3}
Let $\by_n,\bz_n$ be the hidden states generated by the RNN \eqref{eq:brnn1}. We assume that the time step $\Dt << 1$ can be chosen such that, 
\begin{equation}
\label{eq:assm}
\max \left\{ \frac{\Dt (1 + \|\bW\|_{\infty})}{1  + \Dt},  \frac{\Dt  \|{\bf \cW}\|_{\infty}}{1  + \Dt } \right \}  = \eta \leq \Dt^r, \quad \frac{1}{2} \leq r \leq 1.  
\end{equation}
Denoting $\delta = \frac{1}{1 + \Dt}$, the gradient of the loss function $\E$ \eqref{eq:lf1} with respect to any parameter $\theta \in {\bf \Theta}$ is bounded as,
\begin{equation}
    \label{eq:gbd}
    \left| \frac{\partial \E}{\partial \theta} \right| \leq \frac{3}{2}\left(m + \bar{Y} \sqrt{m}\right),
\end{equation}
with $\bar{Y} = \max\limits_{1 \leq n \leq N} \|\bar{\by}_n\|_{\infty}$ be a bound on the underlying training data.
\end{proposition}
\emph{Sketch of the proof.} Denoting $\bX_n = [\by_n, \bz_n]$, we can apply the chain rule repeatedly (for instance as in \cite{vanish_grad}) to obtain,
\begin{equation}
\label{eq:grad2}
\frac{\partial \E_n}{\partial \theta} = \sum\limits_{1 \leq k \leq n} \underbrace{\frac{\partial \E_n}{\partial \bX_n} \frac{\partial \bX_n}{\partial \bX_k} \frac{\partial^{+} \bX_k}{\partial \theta}}_{\frac{\partial \E^{(k)}_n}{\partial \theta}}.
\end{equation}
Here, the notation $\frac{\partial^{+} \bX_k}{\partial \theta}$ refers to taking the partial derivative of $\bX_k$ with respect to the parameter $\theta$, while keeping the other arguments constant. This quantity can be readily calculated from the structure of the RNN \eqref{eq:brnn1} and is presented in the detailed proof provided in {\bf SM}\S\ref{app:proof_explod}. From \eqref{eq:lf1}, we can directly compute that $\frac{\partial \E_n}{\partial \bX_n} = \left[\by_n - \bar{\by}_n, 0 \right].$ 

Repeated application of the chain rule and a direct calculation with \eqref{eq:brnn1} yields,
\begin{equation}
\label{eq:grad5}
 \frac{\partial \bX_n}{\partial \bX_k} = \prod\limits_{k < i \leq n} \frac{\partial \bX_i}{\partial \bX_{i-1}}, \quad \frac{\partial \bX_i}{\partial \bX_{i-1}} = 
 \begin{bmatrix}
 \ind + \Dt \bB_{i-1} & \Dt \bC_{i-1} \\
 \bB_{i-1} & \bC_{i-1} 
 \end{bmatrix},
\end{equation}
where $\ind$ is the identity matrix and 
\begin{equation}
\label{eq:grad7}
\bB_{i-1} = \delta\Dt\left(\diag(\sigma^{\prime}(\bA_{i-1})) \bW-\ind\right), \quad \bC_{i-1} = \delta\left(\ind +  \Dt\diag(\sigma^{\prime}(\bA_{i-1}))\cW\right).
\end{equation}
It is straightforward to calculate using the assumption \eqref{eq:assm} that $\|\bB_{i-1}\|_{\infty} < \eta$ and $\|\bC_{i-1}\|_{\infty} \leq \eta + \delta$. Using the definitions of matrix norms and \eqref{eq:assm}, we obtain:
\begin{equation}
\label{eq:norm1}
\begin{aligned}
\left \| \frac{\partial \bX_i}{\partial \bX_{i-1}} \right\|_\infty &\leq \max \left( 1 + \Dt (\|\bB_{i-1}\|_\infty + \|\bC_{i-1}\|_\infty), \|\bB_{i-1}\|_\infty + \|\bC_{i-1}\|_\infty \right) \\
&  \leq \max \left( 1+\Dt(\delta + 2 \eta), \delta + 2\eta \right) \leq  1 + 3\Dt^r.
\end{aligned}
\end{equation}
Therefore, using \eqref{eq:grad5}, we have 
\begin{equation}
\label{eq:norm2}
\left \| \frac{\partial \bX_n}{\partial \bX_{k}} \right\|_\infty \leq \prod\limits_{k < i \leq n} \left \| \frac{\partial \bX_i}{\partial \bX_{i-1}} \right\|_\infty \leq (1 + 3\Dt^r)^{n-k} \approx 1 + 3(n-k)\Dt^r.
\end{equation}
Note that we have used an expansion around $1$ and neglected terms of ${\mathcal O}(\Dt^{2r})$ as $\Dt << 1$. We remark that the bound \eqref{eq:norm1} is the \emph{crux of our argument} about gradient control as we see from the structure of the RNN that the recurrent matrices have close to unit norm. The detailed proof is presented in {\bf SM}\S\ref{app:proof_explod}. As the entire gradient of the loss function \eqref{eq:lf1}, with respect to the weights and biases of the network, is bounded above in \eqref{eq:gbd}, the \emph{exploding gradient problem} is mitigated for this RNN.
\paragraph{On the vanishing gradient problem.} The vanishing gradient problem \citep{vanish_grad} arises if $\left | \frac{\partial \E^{(k)}_n}{\partial \theta} \right |$, defined in \eqref{eq:grad2}, $\rightarrow 0$ exponentially fast in $k$, for $k << n$ (long-term dependencies). In that case, the RNN does not have long-term memory, as the contribution of the $k$-th hidden state to error at time step $t_n$ is infinitesimally small. We already see from \eqref{eq:norm2} that $\left \| \frac{\partial \bX_n}{\partial \bX_{k}} \right\|_\infty \approx 1$ (independently of k). Thus, we should not expect the products in \eqref{eq:grad2} to decay fast. In fact, we will provide a much more precise characterization of this gradient. To this end, we introduce the following \emph{order}-notation,
\begin{equation}
    \label{eq:ord}
    \begin{aligned}
   {\bf \beta} &= \ord(\alpha), {\rm for}~\alpha,\beta \in \R_+ \quad {\rm if~there~exists~constants}~ \overline{C},\underline{C}~{\rm such~that}~\underline{C} \alpha \leq \beta \leq \overline{C} \alpha. \\
   {\bf M} &= \ord(\alpha), {\rm for}~{\bf M} \in \R^{d_1 \times d_2}, \alpha \in \R_+ \quad {\rm if~there~exists~constant}~ \overline{C}~{\rm such~that}~\|{\bf M}\| \leq \overline{C} \alpha.
   \end{aligned}
\end{equation}
For simplicity of notation, we will also set $\bar{\by}_n = \bu_n \equiv 0$, for all $n$, $\bb=0$ and $r=1$ in \eqref{eq:assm} and we will only consider $\theta = \bW_{i,j}$ for some $1\leq i,j \leq m$ in the following proposition.
\begin{proposition}
\label{prop:4}
Let $\by_n$ be the hidden states generated by the RNN \eqref{eq:brnn1}. Under the assumption that $\by^i_n = \ord(\sqrt{t_n})$, for all $1 \leq i \leq m$ and \eqref{eq:assm}, the gradient for long-term dependencies satisfies,
\begin{equation}
    \label{eq:gulb}
    \frac{\partial \E^{(k)}_n}{\partial \theta} = \ord\left(\hat{c}\delta \Dt^{\frac{3}{2}}\right) + \ord\left(\hat{c}\delta(1+\delta) \Dt^{\frac{5}{2}}\right) + \ord(\Dt^3),~ \hat{c} = sech^2\left(\sqrt{k\Dt}(1+\Dt)\right),
     ~k << n. 
\end{equation}
\end{proposition}
This precise bound \eqref{eq:gulb} on the gradient shows that although the gradient can be small, i.e $\ord(\Dt^{\frac{3}{2}})$, it is in fact \emph{independent of $k$}, ensuring that long-term dependencies contribute to gradients at much later steps and mitigating the vanishing gradient problem. The detailed proof is presented in   {\bf SM}\S\ref{app:proof_vanish}.

Summarizing, we see that the RNN \eqref{eq:brnn} indeed satisfied similar bounds to the underlying ODE \eqref{eq:ode} that resulted in upper bounds on the hidden states and its gradients. However, the lower bound on the gradient \eqref{eq:gulb} is due to the specific choice of this discretization and does not appear to have a continuous analogue, making the specific choice of discretization of \eqref{eq:ode} crucial for mitigating the vanishing gradient problem. 
\section{Experiments}
We present results on a variety of learning tasks with coRNN \eqref{eq:brnn} with $\bar{n} = n-1$, as this version resulted in marginally better performance than the version with $\bar{n}=n$. Details of the training procedure for each experiment can be found in {\bf SM}\S\ref{app:training_details}. We wish to clarify here that we use a straightforward hyperparameter tuning protocol based on a validation set and do not use additional performance enhancing tools, such as dropout \citep{dropout}, gradient clipping \citep{vanish_grad} or batch normalization \citep{batch_norm}, which might further improve the performance of coRNNs. 

\begin{figure}[ht!]
\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_500.pdf}
\end{minipage}\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_2000.pdf}
\end{minipage}\begin{minipage}{.33\textwidth}
\includegraphics[width=1.\textwidth]{figures/adding_5000.pdf}
\end{minipage}
\caption{Results of the adding problem for coRNN, expRNN, FastRNN, anti.sym. RNN and tanh RNN based on three different sequence lengths $T$, i.e. $T=500$, $T=2000$ and $T=5000$.}
\label{fig:adding_results}
\end{figure}
\paragraph{Adding problem.} We start with the well-known adding problem \citep{lstm}, proposed to test the ability of an RNN to learn (very) long-term dependencies. The input is a two-dimensional sequence of length $T$, with the first dimension consisting of random numbers drawn from $\mathcal{U}([0,1])$ and with two non-zero entries (both set to $1$) in the second dimension, chosen at random locations, but one each in both halves of the sequence. The output is the sum of two numbers of the first dimension at positions, corresponding to the two 1 entries in the second dimension. We compare the proposed coRNN to three recently proposed RNNs, which were explicitly designed to learn LTDs, namely the FastRNN \citep{fastrnn}, the antisymmetric (anti.sym.) RNN \citep{anti} and the expRNN \citep{expRNN}, and to a plain vanilla tanh RNN, with the goal of beating the baseline mean square error (MSE) of $0.167$ (which stems from the variance of the baseline output $1$). All methods have 128 hidden units (dimensionality of the hidden state $\by$) and the same training protocol is used in all cases. \fref{fig:adding_results} shows the results for different lengths $T$ of the input sequences. We can see that while the tanh RNN is not able to beat the baseline for any sequence length, the other methods successfully learn the adding task for $T=500$. However, in this case, coRNN converges significantly faster and reaches a lower test MSE than other tested methods. When setting the length to the much more challenging case of $T=2000$, we see that only coRNN and the expRNN beat the baseline. However, the expRNN fails to reach a desired test MSE of $0.01$ within training time. In order to further demonstrate the superiority of coRNN over recently proposed RNN architectures for learning LTDs, we consider the adding problem for $T=5000$ and observe that coRNN converges very quickly even in this case, while expRNN fails to consistently beat the baseline. We thus conclude that the coRNN mitigates the vanishing/exploding gradient problem even for very long sequences.

\begin{table}[h!]
\caption{Test accuracies on sMNIST and psMNIST (we provide our own psMNIST result for the FastGRNN, as no official result for this task has been published so far).}
\label{tab:mnist}
\begin{center}
\begin{tabular}{lllll}
Model & sMNIST &  psMNIST &  \# units & { \# params}
\\ \hline \\
uRNN \citep{urnn} & 95.1\%& 91.4\% & 512 & 9k\\
LSTM \citep{scornn} &98.9\% & 92.9\% & 256 & 270k \\
GRU \citep{GRU_results} & 99.1\%&94.1\% & 256& 200k\\
anti.sym. RNN \citep{anti}  & 98.0\% & 95.8\% & 128 & 10k\\
DTRIV$\infty$ \citep{dtriv}& 99.0\%& 96.8\% &512 & 137k \\
FastGRNN \citep{fastrnn} & 98.7\% & 94.8\% & 128 & 18k\\
\textbf{coRNN} (128 units) &99.3\% & 96.6\%&128& 34k\\
\textbf{coRNN} (256 units)&\bf{99.4}\%& \bf{97.3}\%&256& 134k\\
\end{tabular}
\end{center}
\end{table}
\paragraph{Sequential (permuted) MNIST.}
Sequential MNIST (sMNIST) \citep{seq_mnist} is a benchmark for RNNs, in which the model is required to classify an MNIST \citep{mnist} digit one pixel at a time leading to a classification task with a sequence length of $T=784$. In permuted sequential MNIST (psMNIST), a fixed random permutation is applied in order to increase the time-delay between interdependent pixels and to make the problem harder. In \Tref{tab:mnist}, we compare the test accuracy for coRNN on sMNIST and psMNIST with recently published best case results for other recurrent models, which were explicitly designed to solve long-term dependencies together with baselines corresponding to gated and unitary RNNs. To the best of our knowledge the proposed coRNN outperforms all single-layer recurrent architectures, published in the literature, for both the sMNIST and psMNIST. 
Moreover in \fref{fig:mnist}, we present the performance (with respect to number of epochs) of different RNN architectures for psMNIST with the same fixed random permutation and the same number of hidden units, i.e. 128. As seen from this figure, coRNN clearly outperforms the other architectures, some of which were explicitly designed to learn LTDs, handily for this permutation.

\begin{figure}[ht!]
\centering
\begin{minipage}[t]{.32\textwidth}
\includegraphics[width=1.\textwidth]{figures/mnist.pdf}
\caption{Performance on psMNIST for different models, all with 128 hidden units and the same fixed random permutation.}
\label{fig:mnist}
\end{minipage}\hspace{0.01\textwidth}
\begin{minipage}[t]{.32\textwidth}
\includegraphics[width=1.\textwidth]{figures/weight_norm_bounds.pdf}
\caption{Weight assumptions \eqref{eq:assm}, with $r=\frac{1}{2}$, evaluated during training for all LTD experiments (mean and standard deviation of 10 different runs for each task).}
\label{fig:weight_assumptions}
\end{minipage}\hspace{0.01\textwidth}
\begin{minipage}[t]{.32\textwidth}
\includegraphics[width=1.\textwidth]{figures/ablation.pdf}
\caption{Ablation study on the hyperparameters $\epsilon,\gamma$ in \eqref{eq:brnn} using the noise padded CIFAR-10 experiment.}
\label{fig:ablation}
\end{minipage}
\end{figure}

\paragraph{Noise padded CIFAR-10.}
Another challenging test problem for learning LTDs is the recently proposed noise padded CIFAR-10 experiment by \cite{anti}, in which CIFAR-10 data points \citep{cifar} are fed to the RNN row-wise and flattened along the channels resulting in sequences of length 32. To test the long term memory, entries of uniform random numbers are added such that the resulting sequences have a length of 1000, i.e. the last 968 entries of each sequence are only noise to distract the network. 
\Tref{tab:cifar} shows the result for coRNN together with other recently published best case results. We observe that coRNN readily outperforms other RNN architectures on this benchmark, while requiring only 128 hidden units.

\begin{table}[h!]
\caption{Test accuracies on noise padded CIFAR-10.}
\label{tab:cifar}
\begin{center}
\begin{tabular}{llll}
{ Model} &  test accuracy & \# units & { \# params}
\\ \hline \\
LSTM \citep{inc_rnn}&  11.6\% & 128 & 64k\\
Incremental RNN \citep{inc_rnn}& 54.5\% & 128 & 12k \\
FastRNN \citep{inc_rnn}& 45.8\% & 128 & 16k\\
anti.sym. RNN \citep{anti} & 48.3\% & 256 & 36k\\
Gated anti.sym. RNN \citep{anti}& 54.7\% &256 & 37k \\
Lipschitz RNN \citep{lip_rnn}& 55.2\% & 256 & 134k\\
\textbf{coRNN} & \textbf{59.0}\% & 128 & 46k\\
\end{tabular}
\end{center}
\end{table}

\paragraph{Human activity recognition.}
This experiment is based on the human activity recognition data set provided by \cite{har}. The data set is a collection of tracked human activities, which were measured by an accelerometer and gyroscope on a Samsung Galaxy S3 smartphone. Six activities were binarized to obtain two merged classes \{Sitting, Laying, Walking\_Upstairs\} and \{Standing, Walking, Walking\_Downstairs\}, leading to the HAR-2 data set, which was first proposed in \cite{fastrnn}. \Tref{tab:har2} shows the result for coRNN together with other very recently published best case results on the same data set. We can see that coRNN readily outperforms all other methods. We also ran this experiment on a \textit{tiny coRNN} with very few parameters, i.e. only 1k. We can see that even in this case, the tiny coRNN beats all baselines. We thus conclude that coRNN can efficiently be used on resource-constrained IoT micro-controllers.
\begin{table}[htbp]
\caption{Test accuracies on HAR-2.}
\label{tab:har2}
\begin{center}
\begin{tabular}{llll}
{ Model} &  test accuracy & \# units & { \# params}
\\ \hline \\
GRU \citep{fastrnn}&  93.6\% & 75 & 19k\\
LSTM \citep{inc_rnn}& 93.7\%  & 64 & 16k \\
FastRNN \citep{fastrnn}& 94.5\%  & 80 & 7k \\
FastGRNN \citep{fastrnn}& 95.6\% & 80 &  7k\\
anti.sym. RNN \citep{inc_rnn}& 93.2\% & 120 & 8k \\
incremental RNN \citep{inc_rnn}& 96.3\% & 64 & 4k \\
\textbf{coRNN} & \textbf{97.2}\% & 64 & 9k\\
\textit{tiny coRNN} & 96.5\% & 20 & 1k \\
\end{tabular}
\end{center}
\end{table}
\paragraph{IMDB sentiment analysis.}
The IMDB data set \citep{imdb} is a collection of 50k movie reviews, where 25k reviews are used for training (with 7.5k of these reviews used for validating) and 25k reviews are used for testing. The aim of this binary sentiment classification task is to decide whether a movie review is positive or negative. We follow the standard procedure by initializing the word embedding with pretrained 100d GloVe \citep{glove} vectors and restrict the dictionary to 25k words. \Tref{tab:imdb} shows the results for coRNN and other recently published models, which are trained similarly and have the same number of hidden units, i.e. 128. We can see that coRNN compares favorable with gated baselines (which are known to perform very well on this task), while at the same time requiring significantly less parameters.
\begin{table}[htbp]
\caption{Test accuracies on IMDB.}
\label{tab:imdb}
\begin{center}
\begin{tabular}{llll}
{ Model} &  test accuracy & \# units &  \# params
\\ \hline \\
LSTM \citep{imdb_base}&  86.8\% & 128 & 220k\\
Skip LSTM\citep{imdb_base}& 86.6\%  & 128 & 220k\\
GRU \citep{imdb_base}& 86.2\% & 128 & 164k\\
Skip GRU \citep{imdb_base}& 86.6\% & 128 & 164k \\
ReLU GRU \citep{imdb_gru}& 84.8\% & 128 & 99k \\
\textbf{coRNN} & \textbf{87.4}\% & 128 & 46k\\
\end{tabular}
\end{center}
\end{table}
\paragraph{Further experimental results.}
To shed further light on the performance of coRNN, we consider the following issues. First, the theory suggested that coRNN mitigates the exploding/vanishing gradient problem as long as the assumptions \eqref{eq:assm} on the time step $\Dt$ and weight matrices $\bW,\cW$ hold. Clearly one can choose a suitable $\Dt$ to enforce \eqref{eq:assm} before training, but do these assumptions remain valid during training? In {\bf SM}\S\ref{app:explod_training}, we argue, based on worst-case estimates, that the assumptions will remain valid for possibly a large number of training steps. More pertinently, we can verify experimentally that \eqref{eq:assm} holds during training. This is demonstrated in \fref{fig:weight_assumptions}, where we show that \eqref{eq:assm} holds for all LTD tasks during training. Thus, the presented theory applies and one can expect control over hidden state gradients with coRNN. Next, we recall that the frequency parameter $\gamma$ and damping parameter $\ep$ play a role for coRNNs (see {\bf SM}\S\ref{sec:exp} for the theoretical dependence and \Tref{tab:hyperparameters_rounded} for best performing values of $\ep,\gamma$ for each numerical experiment within the range considered in \Tref{tab:hyperparameters}). How sensitive is the performance of coRNN to the choice of these 2 parameters? To investigate this dependence, we focus on the noise padded CIFAR-10 experiment and show the results of an \emph{ablation study} in \fref{fig:ablation}, where the test accuracy for different coRNNs based on a two dimensional hyperparameter grid $(\epsilon,\gamma) \in [0.8,1.8]\times[5.7,17,7]$ (i.e., sufficiently large intervals around the best performing values of $\ep,\gamma$ from \Tref{tab:hyperparameters_rounded}) is plotted. We observe from the figure that although there are reductions in test accuracy for non-optimal values of $(\ep,\gamma)$, there is no large variation and the performance is rather robust with respect to these hyperparameters. Finally, note that we follow standard practice and present best reported results with coRNN as well as other competing RNNs in order to compare the relative performance. However, it is natural to investigate the dependence of these \emph{best} results on the random initial (before training) values of the weight matrices. To this end, in \Tref{tab:distr_results} of {\bf SM}, we report the mean and standard deviation (over $10$ retrainings) of the test accuracy with coRNN on various learning tasks and find that the mean value is comparable to the best reported value, with low standard deviations. This indicates further robustness of the performance of coRNNs. 
\begin{table}[h!]
  \caption{Distributional information (mean and standard deviation) on the results for each classification experiment presented in the paper based on 10 re-trainings of the best performing coRNN using random initialization of the trainable parameters.}
  \label{tab:distr_results}
  \centering
  \begin{tabular}{lllll}
    \toprule
    \cmidrule(r){1-3}
    Experiment & Mean & Standard deviation \\
        \midrule
    sMNIST (256 units) & 99.17\% & 0.07\% \\
    psMNIST (256 units) & 96.10\%& 1.20\% \\
    Noise padded CIFAR-10 & 58.56\% & 0.35\% \\
    HAR-2 (64 units) & 96.01\% & 0.53\%\\
    IMDB & 86.65\% & 0.31\% \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Discussion}
Inspired by many models in physics, biology and engineering, we proposed a novel RNN architecture \eqref{eq:brnn} based on a model \eqref{eq:ode1} of a \emph{network of controlled forced and damped oscillators}. For this RNN, we rigorously showed that under verifiable hypotheses on the time step and weight matrices, the hidden states are bounded \eqref{eq:enb} and obtained precise bounds on the gradients (Jacobians) of the hidden states, \eqref{eq:gbd} and \eqref{eq:gulb}. Thus by design, this architecture can mitigate the exploding and vanishing gradient problem (EVGP) for RNNs. We present a series of numerical experiments that include sequential image classification, activity recognition and sentiment analysis, to demonstrate that the proposed coRNN keeps hidden states and their gradients under control, while retaining sufficient expressivity to perform complex tasks. Thus, we provide a novel and promising strategy for designing RNN architectures that are motivated by the functioning of natural systems, have rigorous bounds on hidden state gradients and are robust, accurate, straightforward to train and cheap to evaluate. 

This work can be extended in different directions. For instance in this article, we have mainly focused on the learning of tasks with long-term dependencies and observed that coRNNs are comparable in performance to the best published results in the literature. Given  that coRNNs are built with networks of oscillators, it is natural to expect that they will perform very well on tasks with oscillatory inputs/outputs, such as the time series analysis of high-resolution biomedical data, for instance EEG (electroencephalography) and EMG (electromyography) data and seismic activity data from geoscience. This will be pursued in a follow-up article. Similarly, applications of coRNN to language modeling will be covered in future work.

However, it is essential to point out that coRNNs might not be suitable for every learning task involving sequential inputs/outputs. As a concrete example, we consider the problem of predicting time series corresponding to a chaotic dynamical system. We recall that by construction, the underlying ODE \eqref{eq:ode} (and the discretization \eqref{eq:brnn}) do not allow for super-linear (in time) separation of trajectories for nearby inputs. Thus, we cannot expect that coRNNs will be effective at predicting chaotic time series and it is indeed investigated and demonstrated for a Lorenz-96 ODE in {\bf SM}\S\ref{sec:ctsp}, where we observe that the coRNN is outperformed by LSTMs in the chaotic regime.

Our main theoretical focus in this paper was to demonstrate the possible mitigation of the exploding and vanishing gradient problem. On the other hand, we only provided some heuristics and numerical evidence on why the proposed RNN still has sufficient expressivity. A priori, it is natural to think that the proposed RNN architecture might introduce a strong bias towards oscillatory functions. However, as we argue in {\bf SM}\S\ref{app:heuristics}, the proposed coRNN can be significantly more expressive, as the damping, forcing and coupling of several oscillators modulates nonlinear response to yield a very rich and diverse set of output states. This is also evidenced by the ability of coRNNs to deal with many tasks in our numerical experiments, which do not have an explicit oscillatory structure. This sets the stage for a rigorous investigation of \emph{universality} of the proposed coRNN architecture, as in the case of echo state networks in \cite{ort1}. A possible approach would be to leverage the ability of the proposed RNN to convert general inputs into a rich set of superpositions of harmonics (oscillatory wave forms). Moreover, the proposed RNN was based on the simplest model of coupled oscillators \eqref{eq:ode1}. Much more detailed models of oscillators are available, particularly those that arise in the modeling of biological neurons, \cite{ermen1} and references therein. An interesting variant of our proposed RNN would be to base the RNN architecture on these more elaborate models, resulting in analogues of the spiking neurons model of \cite{maass1} for RNNs.

\bibliography{refs}
\bibliographystyle{iclr2021_conference}

\appendix
\newpage
\begin{center}
{\bf Supplementary Material for:}\\
Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies
\end{center}

\section{Chaotic time-series prediction.}
\label{sec:ctsp}
According to proposition \ref{prop:2}, coRNN does not exhibit chaotic behavior by design. While this property is highly desirable for learning long-term dependencies (a slight perturbation of the input should not result in an unbounded perturbation of the prediction), it impairs the performance on tasks, where the network has to learn actual chaotic dynamics. To test this numerically, we consider the following version of the Lorenz 96 system: \citep{lorenz96}:
\begin{align}
\label{eq:lorenz}
    x^\prime_j = (x_{i+1} - x_{i-2})x_{i-1} - x_i + F,
\end{align}
where $x_j\in \mathbb{R}$ for all $j=1,\dots,5$ and $F$ is an external force controlling the level of chaos in the system.
\begin{figure}[ht!]
\centering
\begin{minipage}[t]{0.49\textwidth}	
\includegraphics[width=1.\textwidth]{figures/lorenz96_0_9.pdf}
\end{minipage}\hspace{0.01\textwidth}
\begin{minipage}[t]{.49\textwidth}
\includegraphics[width=1.\textwidth]{figures/lorenz96_8.pdf}
\end{minipage}
\caption{Exemplary $(x_1,x_2)$-trajectories of the Lorenz 96 system \eqref{eq:lorenz} for different forces $F$.}
\label{fig:lorenz}
\end{figure}
\fref{fig:lorenz} shows a trajectory of the system \eqref{eq:lorenz} plotted on the $x_1x_2$-plane for a small external force of $F=0.9$ as well as a trajectory for a large external force of $F=8$. We can see that while for $F=0.9$ the system does not exhibit chaotic behavior, the dynamics for $F=8$ is already highly chaotic.

Our task consists of predicting the $25$-th next state of a trajectory of the system \eqref{eq:lorenz}. 
We provide $128$ trajectories of length $2000$ for each of the training, validation and test sets. The trajectories are generated by numerically solving the system \eqref{eq:lorenz} and evaluating it at $2000$ equidistantly distributed discrete time points with distance $0.01$. The initial value for each trajectory is chosen uniform at random on $[F-1/2,F+1/2]^5$ around the equilibrium point $(F,\dots,F)$ of the system \eqref{eq:lorenz}. 

Since LSTMs are known to be able to produce chaotic dynamics, even in the autonomous (zero-entry) case \citep{chaotic_lstm}, we expect them to perform significantly better than coRNN if the underlying system exhibits strong chaotic behavior. 
\Tref{tab:lorenz} shows the normalized root mean square error (NRMSE) (RMSE divided by the root mean square of the target trajectory) on the test set for coRNN and LSTM. We can see that indeed for the non-chaotic case of using an external force of $F=0.9$ LSTM and coRNN perform similarly. However, when the dynamics get chaotic (in this case using an external force of $F=8$), the LSTM clearly outperforms coRNN. 
\begin{table}[h!]
  \caption{Test NRMSE on the Lorenz 96 system \eqref{eq:lorenz} for coRNN and LSTM.}
  \label{tab:lorenz}
  \centering
  \begin{tabular}{lllll}
    \toprule
    \cmidrule(r){1-5}
    Model & $F=0.9$ & $F=8$ & \# units & \# params \\
        \midrule
    LSTM & $2.0\times 10^{-2}$& $6.8\times 10^{-2}$ & 44 & 9k\\
    coRNN & $2.0\times 10^{-2}$ & $9.8\times 10^{-2}$ & 64 & 9k\\
    \bottomrule
  \end{tabular}
\end{table}



\section{Training details}
\label{app:training_details}
The IMDB task was conducted on an NVIDIA GeForce GTX 1080 Ti GPU, while all other experiments were run on a Intel Xeon E3-1585Lv5 CPU.
The weights and biases of coRNN are randomly initialized according to $\mathcal{U}(-\frac{1}{\sqrt{n_{in}}},\frac{1}{\sqrt{n_{in}}})$, where $n_{in}$ denotes the input dimension of each affine transformation. Instead of treating the parameters $\Dt, \gamma$ and $\epsilon$ as fixed hyperparameters, we can also treat them as trainable network parameters by constraining $\Dt$ to $[0,1]$ by using a sigmoidal activation function and $\epsilon, \gamma >0$ by the use of ReLU for instance. However, in this case no major difference in performance is obtained.
The hyperparameters are optimized with a random search algorithm, where the results of the best performing coRNN (based on the validation set) are reported. The ranges of the hyperparameters for the random search algorithm are provided in \Tref{tab:hyperparameters}. \Tref{tab:hyperparameters_rounded} shows the rounded hyperparameters of the best performing coRNN architecture resulting from the random search algorithm for each learning task. We used 100 training epochs for sMNIST, psMNIST and noise padded CIFAR-10 with additional 20 epochs in which the learning rate was reduced by a factor of $10$. Additionally, we used 100 epochs for the IMDB task and 250 epochs for the HAR-2 task.

\begin{table}[h!]
  \caption{Setting for the hyperparameter optimization of coRNN. Intervals denote ranges of the corresponding hyperparameter for the grid search algorithm, while fixed numbers mean that no hyperparameter optimization was done in this case.}
  \label{tab:hyperparameters}
  \centering
  \begin{tabular}{llllll}
    \toprule
    \cmidrule(r){1-6}
    task & learning rate & batch size  & $\Delta t$ & 
    $\gamma$ & $\epsilon$ \\
    \midrule
Adding & $2\times 10^{-2}$ &50& $[10^{-2},10^{-1}]$ & $[1,100]$ & $[1,100]$\\
sMNIST ($n_{hid}=128$) & $[10^{-4},10^{-1}]$ & 120 & $[10^{-2},10^{-1}]$ & $[10^{-1},10]$ & $[10^{-1},10]$ \\
sMNIST ($n_{hid}=256$) & $[10^{-4},10^{-1}]$ & 120 &  $[10^{-2},10^{-1}]$  &$[10^{-1},10]$ & $[10^{-1},10]$\\
psMNIST ($n_{hid}=128$) & $[10^{-4},10^{-1}]$ & 120 & $[10^{-2},10^{-1}]$ & $[10^{-1},10]$ & $[10^{-1},10]$ \\
psMNIST ($n_{hid}=256$) & $[10^{-4},10^{-1}]$ & 120 &  $[10^{-2},10^{-1}]$ & $[10^{-1},10]$ & $[10^{-1},10]$ \\
Noise padded CIFAR-10 & $[10^{-4},10^{-1}]$ & 100 & $[10^{-2},10^{-1}]$ & $[1,100]$ & $[1,100]$ \\
HAR-2 & $[10^{-4},10^{-1}]$ & 64 & $[10^{-2},10^{-1}]$ & $[10^{-1},10]$ & $[10^{-1},10]$ \\
IMDB &$[10^{-4},10^{-1}]$ & 64  & $[10^{-2},10^{-1}]$ & $[10^{-1},10]$ & $[10^{-1},10]$ \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{table}[h!]
  \caption{Rounded hyperparameters of the best performing coRNN architecture.}
  \label{tab:hyperparameters_rounded}
  \centering
  \begin{tabular}{llllll}
    \toprule
    \cmidrule(r){1-6}
    task & learning rate & batch size  & $\Delta t$ & 
    $\gamma$ & $\epsilon$ \\
    \midrule
Adding ($T=5000$) & $2\times 10^{-2}$ &50& $1.6\times10^{-2}$ & $94.5$ & $9.5$\\
sMNIST ($n_{hid}=128$) & $3.5\times10^{-3}$ & 120 & $5.3\times10^{-2}$ & $1.7$  & $4$\\
sMNIST ($n_{hid}=256$) & $2.1\times10^{-3}$&  120& $4.2\times10^{-2}$ & $2.7$& $4.7$\\
psMNIST ($n_{hid}=128$) &$3.7\times10^{-3}$ & 120 & $8.3\times10^{-2}$ & $1.3\times10^{-1}$ & $4.1$ \\
psMNIST ($n_{hid}=256$) & $5.4\times10^{-3}$& 120 & $7.6\times10^{-2}$ &$4\times10^{-1}$  & $8.0$ \\
Noise padded CIFAR-10 & $7.5\times 10^{-3}$ &100& $3.4\times10^{-2}$ & $1.3$ & $12.7$\\
HAR-2 & $1.7\times 10^{-2}$ &64& $10^{-1}$ & $2\times 10^{-1}$ & $6.4$\\
IMDB & $6.0\times 10^{-4}$ & 64 & $5.4\times 10^{-2}$& $4.9$ & $4.8$\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Heuristics of network function}
\label{app:heuristics}
At the level of a single neuron, the dynamics of the RNN is relatively straightforward. We start with the scalar case, i.e. $m=d =1$ and illustrate different hidden states $\by$ as a function of time, for different input signals, in \fref{fig:1}. In this figure, we consider two different input signals, one oscillatory signal given by $\bu(t) = \cos(4t)$ and another is a combination of step functions. First, we plot the solution $\by(t)$ of \eqref{eq:ode1}, with the parameters $\bV,\bb,\bW,\cW,\ep=0$ and $\gamma=1$. This simply corresponds to the case of a simple harmonic oscillator (SHO) and the solution is described by a sine wave with the natural frequency of the oscillator. Next, we introduce forcing by the input signal by setting $\bV=1$ and the activation function is the identity $\sigma(x) =x $, leading to a forced damped oscillator (FDO). As seen from \fref{fig:1}, in the case of an oscillatory signal, this leads to a very minor change over the SHO, whereas for the step function, the change is only in the amplitude of the wave. Next, we add damping by setting $\ep=0.25$ and see that the resulting forced damped oscillator (FDO), merely damps the amplitude of the waves, without changing their frequency. Then, we consider the case of \emph{controlled oscillator} (CFDO) by setting $\bW=-2,\bV = 2, \bb = 0.25,\cW=0.75$. As seen from \fref{fig:1}, this leads to a significant change in the wave form in both cases. For the oscillatory input, the output is now a superposition of many different forms, with different amplitudes and frequencies (phases) whereas for the step function input, the phase is shifted. Already, we can see that for a linear controlled oscillator, the output can be very complicated with the superposition of different waves. This holds true when the activation function is set to $\sigma(x) = \tanh(x)$ (which is our proposed coRNN). For both inputs, the output is a modulated version of the one generated by CFDO, expressed as a superposition of waves. On the other hand, we also plot the solution with a Duffing type oscillator (DUFF) by setting the activation function as,
\begin{equation}
    \sigma(x) = x - \frac{x^3}{3}.
\end{equation}
In this case, the solution is very different from the CFDO and coRNN solutions and is heavily damped (either in the output or its derivative). On the other hand, given the chaotic nature of the dynamical system in this case, a slight change in the parameters led to the output blowing up. Thus, a bounded nonlinearity seems essential in this context. 



Coupling neurons together further accentuates this generation of superpositions of different wave-forms, as seen even with the simplest case of a network with two neurons, shown in \fref{fig:1} (Bottom row). For this figure, we consider two neurons, i.e $m=2$ and two different network topologies. For the first, we only allow the first neuron to influence the second one and not vice versa. This is enforced with the weight matrices,
$$
\bW = \begin{bmatrix}
        - 2 & 0 \\      
        3 & -2 
      \end{bmatrix}, \quad
    \cW =   \begin{bmatrix}
        0.75 & 0 \\
        -1 & 0.75 
      \end{bmatrix}.  
$$
We also set $\bV = [2,2]^\top, \bb = [0.25,0.25]^\top$. Note that in this case (we name as ORD (for ordered connections)), the output of the first neuron should be exactly the same as in the uncoupled (UC) case, whereas there is a distinct change in the output of the second neuron and we see that the first neuron has modulated a sharp change in the resulting output wave form. It is well illustrated by the emergence of an approximation to the step function (Bottom Right of \fref{fig:1}), even though the input signal is oscillatory. 

Next, we consider the case of fully connected (FC) neurons by setting the weight matrices as,
$$
\bW = \begin{bmatrix}
        - 2 & 1 \\      
        3 & -2 
      \end{bmatrix}, \quad
    \cW =   \begin{bmatrix}
        0.75 & 0.3 \\
        -1 & 0.75 
      \end{bmatrix}.  
$$
The resulting outputs for the first neuron are now slightly different from the uncoupled case. On the the other hand, the approximation of step function output for the second neuron is further accentuated. 

Even these simple examples illustrate the functioning of a network of controlled oscillators well. The input signal is converted into a superposition of waves with different frequencies and amplitudes, with these quantities being controlled by the weights and biases in \eqref{eq:ode1}. Thus, very complicated outputs can be generated by modulating the number, frequencies and amplitudes of the waves. In practice, a network of a large number of neurons is used and can lead to extremely rich global dynamics, along the lines of emergence of synchronization or bistable heterogeneous behavior seen in systems of idealized oscillators and explained by their mean field limit, see \cite{KUR,WIN,stgz1}. Thus, we argue that the ability of the network of (forced, driven) oscillators to access a very rich set of output states can lead to high expressivity of the system. The training process selects the weights that modulate frequencies, phases and amplitudes of individual neurons and their interaction to guide the system to its target output. 
\begin{figure}[ht!]
\begin{minipage}{.5\textwidth}
\includegraphics[width=1.\textwidth]{figures/fig3.pdf}
\end{minipage}\begin{minipage}{.5\textwidth}
\includegraphics[width=1.\textwidth]{figures/fig1.pdf}
\end{minipage} \\
\begin{minipage}{.5\textwidth}
\includegraphics[width=1.\textwidth]{figures/fig6.pdf}
\end{minipage}\begin{minipage}{.5\textwidth}
\includegraphics[width=1.\textwidth]{figures/fig4.pdf}
\end{minipage}
 \\
\begin{minipage}{.5\textwidth}
\includegraphics[width=1.\textwidth]{figures/fig8.pdf}
\end{minipage}
\begin{minipage}{.5\textwidth}
\includegraphics[width=1.\textwidth]{figures/fig9.pdf}
\end{minipage} 
\caption{Illustration of the hidden state $\by$ of coRNN \eqref{eq:brnn} with a scalar input signal $\bu$ (Top, Middle, Left) with one neuron with state $\by$ (Top and Middle, Right) and two neurons with states $\by_1$ (Bottom left), and $\by_2$ (Bottom right), corresponding to scalar input signal, shown in Top Left. Legend is SHO (simple harmonic oscillator), FHO (forced oscillator), FDO (forced and damped oscillator), CFDO (controlled forced and damped oscillator), DUFF (Duffing type) UC (Uncoupled), Ord (ordered coupling) and FC (fully coupled). Legend explained in the text.}
\label{fig:1}
\end{figure}
\section{Bounds on the dynamics of the Ordinary Differential Equation \eqref{eq:ode1}}
\label{sec:bd}
In this section, we present bounds that show how the continuous time dynamics of the ordinary differential equation \eqref{eq:ode}, modeling non-linear damped and forced networks of oscillators, is constrained. We start with the following estimate on the \emph{energy} of the solutions of the system \eqref{eq:ode}. 
\begin{proposition}
\label{prop:n1}
Let $\by(t),\bz(t)$ be the solutions of the ODE system \eqref{eq:ode} at any time $t \in [0,T]$ and assume that the damping parameter $\epsilon \geq \frac{1}{2}$ and the initial data for \eqref{eq:ode} is given by,
$$
\by(0) = \bz(0) \equiv 0.
$$
Then, the solutions are bounded as,
\begin{equation}
\label{eq:nbd1}
\by(t)^{\top}\by(t) \leq \frac{mt}{\gamma}, \quad \bz(t)^{\top}\bz(t) \leq mt, \quad \forall t \in (0,T].
\end{equation}
\end{proposition}
To prove this proposition, we multiply the first equation in \eqref{eq:ode} with $y(t)^\top$ and the second equation in \eqref{eq:ode} with $\frac{1}{\gamma}z(t)^\top$ to obtain,
\begin{equation}
    \label{eq:pbd1}
    \frac{d}{dt}\left(\frac{\by(t)^\top \by(t)}{2} + \frac{\bz(t)^\top \bz(t)}{2\gamma} \right) = \frac{\bz(t)^\top \sigma(\bA(t))}{\gamma} -\frac{\epsilon}{\gamma} \bz(t)^\top \bz(t),
\end{equation}
with 
$$
\bA(t) = \bW \by(t) + \cW \bz(t) + \bV \bu(t) + \bb.
$$
Using the elementary Cauchy's inequality repeatedly in \eqref{eq:pbd1} results in,
\begin{align*}
     \frac{d}{dt}\left(\frac{\by(t)^\top \by(t)}{2} + \frac{\bz(t)^\top \bz(t)}{2\gamma} \right) &\leq \frac{\sigma(A)^\top \sigma(A)}{2\gamma} + \frac{1}{\gamma}\left(\frac{1}{2} -\epsilon\right) \bz^{\top}\bz \\
     &\leq \frac{m}{2\gamma} \quad ({\rm as}~|\sigma| \leq 1 ~~{\rm and}~~\epsilon \geq \frac{1}{2}).
\end{align*}
Integrating the above inequality over the time interval $[0,t]$ and using the fact that the initial data are $\by(0) = \bz(0) \equiv 0$, we obtain the bounds \eqref{eq:nbd1}.

The above proposition and estimate \eqref{eq:nbd1} clearly demonstrate that the dynamics of the network of coupled non-linear oscillators \eqref{eq:ode1} is bounded. The fact that the nonlinear activation function $\sigma = \tanh$ is uniformly bounded in its arguments played a crucial role in deriving the energy bound \eqref{eq:nbd1}. A straightforward adaptation of this argument leads to the following proposition about the sensitivity of the system to inputs,
\begin{proposition}
\label{prop:n2}
Let $\by(t),\bz(t)$ be the solutions of the ODE system \eqref{eq:ode} with respect to the input signal $\bu(t)$. Let  $\bar{\by}(t),
\bar{\bz}(t)$ be the solutions of the ODE system \eqref{eq:ode}, but with respect to the input signal $\bar{\bu}(t)$. Assume that the damping parameter $\epsilon \geq \frac{1}{2}$ and the initial data are given by,
$$
\by(0) = \bz(0) = \bar{\by}(0) = \bar{\bz}(0) \equiv 0.
$$
Then we have the following bound,
\begin{equation}
\label{eq:nbd2}
\left(\by(t)-\bar{\by}(t)\right)^{\top}\left(\by(t)-\bar{\by}(t)\right) \leq \frac{4mt}{\gamma}, \quad \left(\bz(t)-\bar{\bz}(t)\right)^{\top}\left(\bz(t)-\bar{\bz}(t)\right) \leq 4mt, \quad \forall t \in (0,T].
\end{equation}
\end{proposition}
Thus from the bound \eqref{eq:nbd2}, there can be \emph{atmost} linear separation (in time) with respect to the trajectories of the ODE \eqref{eq:ode} for different input signals. Hence, chaotic behavior, which is characterized by the (super-)exponential separation of trajectories is ruled out by the structure of the ODE system \eqref{eq:ode}. Note that this property of the ODE system was primarily a result of the uniform boundedness of the activation function $\sigma$. Using a different activation function such as ReLU might enable to obtain an exponential separation of trajectories that is a prerequisite for a chaotic dynamical system.  
\subsection{Gradient dynamics for the ODE system \eqref{eq:ode}}
\label{sec:gd}
Let $\theta$ denote the $i,j$-th entry of the Weight matrices $\bW,\cW,\bV$ or the $i$-th entry of the bias vector $\bb$. We are interested in finding out how the gradients of the hidden state $\by$ (and the auxiliary hidden state $\bz$) with respect to parameter $\theta$, vary with time. Note that these gradients are precisely the objects of interest in the training of an RNN, based on a discretization of the ODE system \eqref{eq:ode}. To this end, we differentiate \eqref{eq:ode} with respect to the parameter $\theta$ and denote 
$$
\by_{\theta}(t) = \frac{\partial \by}{\partial \theta}(t),~ \bz_{\theta}(t) = \frac{\partial \bz}{\partial \theta}(t),
$$
to obtain,
\begin{equation}
    \label{eq:ngd1}
    \begin{aligned}
    \by^{\prime}_{\theta} &= \bz_{\theta}, \\
    \bz^{\prime}_{\theta} &= \diag(\sigma^{\prime}(\bA))\left[\bW\by_{\theta} + \cW\bz_{\theta}\right] +  {\bf Z}_{m,\bar{m}}^{i,j}(\bA){\bf \rho} - \gamma \by_{\theta} -\epsilon \bz_{\theta}.
    \end{aligned}
\end{equation}
As introduced before, ${\bf Z}_{m,\bar{m}}^{i,j}(\bA) \in \mathbb{R}^{m \times \bar{m}}$ is a matrix with all elements are zero except for the $(i,j)$-th entry which is set to $\sigma^{\prime}(\bA(t))_i$, i.e. the $i$-th entry of $\sigma^{\prime}(\bA)$, and we have,
\begin{align*}
\rho &= \by, \quad \bar{m} =m, \quad {\rm if}~\theta = \bW_{i,j}, \\
\rho &= \bz, \quad \bar{m} =m, \quad {\rm if}~\theta = \cW_{i,j}, \\
\rho &= \bu, \quad \bar{m} =d, \quad {\rm if}~\theta = \bV_{i,j}, \\
\rho &= {\bf 1}, \quad \bar{m} =1, \quad {\rm if}~\theta = \bb_{i}.
\end{align*}
We see from \eqref{eq:ngd1} that the ODEs governing the gradients with respect to the parameter $\theta$ also represent a system of oscillators but with additional coupling and forcing terms, proportional to the hidden states $\by,\bz$ or input signal $\bu$. As we have already proved with estimate \eqref{eq:nbd1} that the hidden states are always bounded and the input signal is assumed to be bounded, it is natural to expect that the gradients of the states with respect to $\theta$ are also bounded. We make this statement explicit in the following proposition, which for simplicity of exposition, we consider the case of $\theta = \bW_{i,j}$, as the other values of $\theta$ are very similar in their behavior.
\begin{proposition}
\label{prop:n3_cont}
Let $\theta = \bW_{i,j}$ and $\by,\bz$ be the solutions of the ODE system \eqref{eq:ode}. Assume that the weights and the damping parameter satisfy, 
$$
\|\bW\|_{\infty} + \|\cW\|_{\infty} \leq \epsilon,
$$
then we have the following bounds on the gradients,
\begin{equation}
    \label{eq:nbd3}
    \begin{aligned}
    \by_{\theta}(t)^\top \by_{\theta}(t) + \frac{1}{\gamma}\left(\bz_{\theta}(t)^\top \bz_{\theta}(t)\right) &\leq \left[\by_{\theta}(0)^\top \by_{\theta}(0) + \frac{1}{\gamma}\left(\bz_{\theta}(0)^\top \bz_{\theta}(0)\right)\right]e^{Ct} + \frac{m t^2}{2\gamma^2},  
    \quad t \in (0,T], \\
    C &= \max\left\{ \frac{\|\bW\|_{1}}{\gamma}, 1 + \|\cW\|_{1}\right\}.
    \end{aligned}
\end{equation}
\end{proposition}
The proof of this proposition follows exactly along the same lines as the proof of proposition \ref{prop:n1} and we skip the details, while noting the crucial role played by the energy bound \eqref{eq:nbd1}. 

We remark that the bound \eqref{eq:nbd3} indicates that as long as the initial gradients with respect to $\theta$ are bounded and the weights are controlled by the damping parameter, the hidden state gradients remain bounded in time. 


\section{Supplement to the rigorous analysis of coRNN}
In this section, we supplement the section on the rigorous analysis of the proposed RNN \eqref{eq:brnn1}. We start with 
\subsection{Proof of Proposition \ref{prop:1}}
\label{app:proof_energy_bound}
We multiply $({\by}_{n-1}^\top, {\bz}^\top_n)$ to \eqref{eq:brnn} and use the elementary identities,
\begin{align*}
{\bf a}^\top({\bf a}-\bb) = \frac{{\bf a}^\top {\bf a}}{2} - \frac{\bb^\top \bb}{2} + \frac{1}{2}({\bf a}-\bb)^\top({\bf a}-\bb), \quad
\bb^\top({\bf a}-\bb) = \frac{{\bf a}^\top {\bf a}}{2} - \frac{\bb^\top \bb}{2} - \frac{1}{2}({\bf a}-\bb)^\top({\bf a}-\bb),
\end{align*}
to obtain the following,
\begin{align*}
\frac{\by_n^\top \by_n + \bz_n^\top \bz_n}{2} &= \frac{\by_{n-1}^\top \by_{n-1} + \bz_{n-1}^\top \bz_{n-1}}{2} + \frac{(\by_n- \by_{n-1})^\top (\by_n- \by_{n-1})}{2} \\
&- \frac{(\bz_n- \bz_{n-1})^\top (\bz_n- \bz_{n-1})}{2}+  \Dt \bz_n^\top \sigma(\bA_{n-1}) -  \Dt \bz_n^\top\bz_n \\
&\leq  \frac{\by_{n-1}^\top \by_{n-1} + \bz_{n-1}^\top \bz_{n-1}}{2} + \Dt \left(1/2+ \Dt/2 - 1\right) \bz_n^\top\bz_n + \frac{\Dt}{2}\sigma^\top(\bA_{n-1})\sigma(\bA_{n-1}) \\
&\leq  \frac{\by_{n-1}^\top \by_{n-1} + \bz_{n-1}^\top \bz_{n-1}}{2} + \frac{m\Dt}{2} \quad {\rm as}~\sigma^2 \leq 1~{\rm and }~\ep > \Dt << 1.
\end{align*} 
Iterating the above inequality $n$ times leads to the energy bound,
\begin{equation}
\label{eq:enb10}
\by_n^\top \by_n + \bz_n^\top \bz_n \leq \by_0^\top \by_0 + \bz_0^\top \bz_0 + n m\Dt = mt_n,
\end{equation}
as $\by_0=\bz_0 = 0$.
\subsection{Sensitivity to inputs}
\label{app:input_stable}
Next, we examine how changes in the input signal $\bu$ affect the dynamics. We have the following proposition:
\begin{proposition}
\label{prop:2}
Let $\by_n,\bz_n$ be the hidden states of the trained RNN \eqref{eq:brnn1} with respect to the input $\bu = \left\{\bu_n\right\}_{n=1}^N$ and let $\overline{\by}_n,\overline{\bz}_n$ be the hidden states of the same RNN \eqref{eq:brnn1}, but with respect to the input $\overline{\bu} = \left\{\overline{\bu}_n\right\}_{n=1}^N$, then the differences in the hidden states are bounded by,
\begin{equation}
    \label{eq:enb1}
    \left(\by_n-\overline{\by}_n\right)^\top \left(\by_n - \overline{\by}_n \right) + \left(\bz_n-\overline{\bz}_n\right)^\top \left(\bz_n - \overline{\bz}_n \right) \leq 4mt_n.
\end{equation}
\end{proposition}
The proof of this proposition is completely analogous to the proof of proposition \ref{prop:1}, we subtract 
\begin{equation}
\label{eq:brnn2}
\begin{array}{ll}
\overline{\by}_n &= \overline{\by}_{n-1} + \Dt \overline{\bz}_n,  \\
\overline{\bz}_n &= \frac{\overline{\bz}_{n-1}}{1+ \Dt}  + \frac{\Dt}{1 + \Dt}  \sigma(\overline{\bA}_{n-1}) - \frac{\Dt}{1+ \Dt} \overline{\by}_{n-1},~\overline{\bA}_{n-1} :=  \bW\overline{\by}_{n-1} +  {\bf \cW} \overline{\bz}_{n-1} + \bV \overline{\bu}_{n} + \bb.  
\end{array}
\end{equation}
from \eqref{eq:brnn1} and multiply $\left(\left(\by_n-\overline{\by}_n\right)^\top,\left(\bz_n-\overline{\bz}_n\right)^\top \right)$ to the difference. The estimate \eqref{eq:enb1} follows identically to the proof of \eqref{eq:enb} (presented above) by realizing that $\sigma(\bA_{n-1}) - \sigma(\overline{\bA}_{n-1}) \leq 2$.

Note that the bound \eqref{eq:enb1} ensures that the hidden states can only separate linearly in time for changes in the input. Thus, chaotic behavior, such as for Duffing type oscillators, characterized by at least exponential separation of trajectories, is ruled out for this proposed RNN, showing that it is stable with respect to changes in the input. This is largely on account of the fact that the activation function $\sigma$ in \eqref{eq:brnn} is globally bounded. 
\subsection{Proof of Proposition \ref{prop:3}}
\label{app:proof_explod}
From \eqref{eq:lf1}, we readily calculate that,
\begin{equation}
\label{eq:grad3}
 \frac{\partial \E_n}{\partial \bX_n} = \left[\by_n - \bar{\by}_n, 0 \right].
 \end{equation}
Similarly from \eqref{eq:brnn}, we calculate,
\begin{equation}
\label{eq:grad4}
\frac{\partial^{+} \bX_k}{\partial \theta} = \begin{cases} 
  \left[\left(\frac{\Dt^2}{1+ \Dt}{\bf Z}_{m,m}^{i,j}(\bA_{k-1}) \by_{k-1}\right)^\top, \left(\frac{\Dt}{1+ \Dt}{\bf Z}_{m,m}^{i,j}(\bA_{k-1}) \by_{k-1}\right)^\top\right]^\top&{\rm if}\quad \theta = (i,j){\rm -th} \> {\rm entry} \> {\rm of} \> \bW, \\
    \left[\left(\frac{\Dt^2}{1+ \Dt}{\bf Z}_{m,m}^{i,j}(\bA_{k-1}) \bz_{k-1}\right)^\top, \left(\frac{\Dt}{1+ \Dt}{\bf Z}_{m,m}^{i,j}(\bA_{k-1}) \bz_{k-1}\right)^\top\right]^\top&{\rm if}\quad \theta = (i,j){\rm -th} \> {\rm entry} \> {\rm of} \> {\bf \cW}, \\
    \left[\left(\frac{\Dt^2}{1+ \Dt}{\bf Z}_{m,d}^{i,j}(\bA_{k-1}) \bu_{k}\right)^\top, \left(\frac{\Dt}{1+ \Dt}{\bf Z}_{m,d}^{i,j}(\bA_{k-1}) \bu_{k}\right)^\top\right]^\top&{\rm if}\quad \theta = (i,j){\rm -th} \> {\rm entry} \> {\rm of} \> \bV, \\
  \left[\left(\frac{\Dt^2}{1+ \Dt}{\bf Z}_{m,1}^{i,1}(\bA_{k-1}) \right)^\top, \left(\frac{\Dt}{1+ \Dt}{\bf Z}_{m,1}^{i,1}(\bA_{k-1}) \right)^\top\right]^\top&{\rm if}\quad \theta = i{\rm -th} \> {\rm entry} \> {\rm of} \> \bb,
\end{cases} 
\end{equation}
where ${\bf Z}_{m,\bar{m}}^{i,j}(\bA_{k-1}) \in \mathbb{R}^{m \times \bar{m}}$ is a matrix with all elements are zero except for the $(i,j)$-th entry which is set to $\sigma^{\prime}(\bA_{k-1})_i$, i.e. the $i$-th entry of $\sigma^{\prime}(\bA_{k-1})$. We easily see that $\|{\bf Z}^{i,j}_{m,\bar{m}}(\bA_{k-1})\|_\infty \leq 1$ for all $i,j,m,\bar{m}$ and all choices of $\bA_{k-1}$. 

Now, using definitions of matrix and vector norms and applying \eqref{eq:norm2} in \eqref{eq:grad2}, together with \eqref{eq:grad3} and \eqref{eq:grad4}, we obtain the following estimate on the norm:
\begin{equation}
\label{eq:norm20}
\left | \frac{\partial \E^{(k)}_n}{\partial \theta} \right | \leq \begin{cases} 
                                     (\|\by_n\|_\infty+\|\bar{\by}_n\|_\infty)(1 + 3(n-k)\Dt^r) \delta \Dt \|\by_{k-1}\|_\infty, &{\rm if} \quad \theta \> {\rm is} \> {\rm entry} \> {\rm of} \> \bW, \\    (\|\by_n\|_\infty+\|\bar{\by}_n\|_\infty)(1 + 3(n-k)\Dt^r) \delta \Dt \|\bz_{k-1}\|_\infty, &{\rm if} \quad \theta \> {\rm is} \> {\rm entry} \> {\rm of} \> {\bf \cW}, \\ (\|\by_n\|_\infty+\|\bar{\by}_n\|_\infty)(1 + 3(n-k)\Dt^r) \delta \Dt \|\bu_k\|_\infty, &{\rm if} \quad \theta \> {\rm is} \> {\rm entry} \> {\rm of} \> \bV, \\    (\|\by_n\|_\infty+\|\bar{\by}_n\|_\infty)(1 + 3(n-k)\Dt^r) \delta \Dt , &{\rm if} \quad \theta \> {\rm is} \> {\rm entry} \> {\rm of} \> \bb. 
                                     \end{cases}
\end{equation}
We will estimate the above term, just for the case of $\theta$ is an entry of $\bW$, the rest of the terms are very similar to estimate. 



For simplicity of notation, we let $k-1 \approx k$ and aim to estimate the term,
\begin{equation}
\label{eq:norm3}
\begin{aligned}
\left | \frac{\partial \E^{(k)}_n}{\partial \theta} \right | &\leq \|\by_n\|_\infty\|\by_k\|_\infty(1 + 3(n-k)\Dt^r) \delta \Dt + \|\bar{\by}_n\|_\infty\|\by_k\|_\infty(1 + 3(n-k)\Dt^r) \delta \Dt\\
&\leq m\sqrt{nk}\Dt (1 + 3(n-k)\Dt^r) \delta \Dt + \|\bar{\by}_n\|_\infty\sqrt{mk}\sqrt{\Dt} (1 + 3(n-k)\Dt^r) \delta \Dt\quad ({\rm by}~ \eqref{eq:enb}) \\
&\leq m\sqrt{nk}\delta \Dt^2 + 3m\sqrt{nk}(n-k)\delta \Dt^{r+2}+\|\bar{\by}_n\|_\infty\sqrt{mk}\sqrt{\Dt} (1 + 3(n-k)\Dt^r) \delta \Dt.
\end{aligned}
\end{equation}
To further analyze the above estimate, we recall that $n\Dt = t_n \leq 1$ and consider two different regimes. Let us start by considering \emph{short-term dependencies} by letting $k \approx n$, i.e $n-k = c$ with constant $c \sim \ord(1)$, independent of $n,k$. In this case, a
straightforward application of the above assumptions in the bound \eqref{eq:norm3} yields,
 \begin{equation}
\label{eq:norm30}
\begin{aligned}
\left | \frac{\partial \E^{(k)}_n}{\partial \theta} \right | &\leq m\sqrt{nk}\delta \Dt^2 + 3m\sqrt{nk}(n-k)\delta \Dt^{r+2}+\|\bar{\by}_n\|_\infty\sqrt{m}\sqrt{t_n}\delta \Dt +\|\bar{\by}_n\|_\infty\sqrt{m}\sqrt{t_n}c\delta \Dt^{r+1}  \\
&\leq mt_n\delta \Dt + mct_n\delta \Dt^{r+1} + \|\bar{\by}_n\|_\infty\sqrt{m}\sqrt{t_n}\delta \Dt +\|\bar{\by}_n\|_\infty\sqrt{m}\sqrt{t_n}c\delta \Dt^{r+1}  \\
&\leq  t_n m \delta \Dt +  \|\bar{\by}_n\|_\infty\sqrt{m}\sqrt{t_n}\delta \Dt\quad ~({\rm for}~\Dt << 1~{\rm as}~r \geq 1/2) \\
&\leq  m \delta \Dt + \|\bar{\by}_n\|_\infty\sqrt{m}\delta \Dt.
\end{aligned}
\end{equation}


Next, we consider \emph{long-term dependencies} by setting $k << n$ and estimating,
 \begin{equation}
\label{eq:norm4}
\begin{aligned}
\left | \frac{\partial \E^{(k)}_n}{\partial \theta} \right | &\leq m\sqrt{nk}\delta \Dt^2 + 3m\sqrt{nk}(n-k)\delta \Dt^{r+2} + \|\bar{\by}_n\|_\infty\sqrt{m}\delta \Dt^{\frac{3}{2}} + 3\|\bar{\by}_n\|_\infty\sqrt{m}n\delta \Dt^{r+\frac{3}{2}}\\
&\leq m\sqrt{t_n} \delta \Dt^{\frac{3}{2}} + 3m t_n^{\frac{3}{2}} \delta \Dt^{r + \frac{1}{2}} + \|\bar{\by}_n\|_\infty\sqrt{m}\delta \Dt^{\frac{3}{2}} + 3\|\bar{\by}_n\|_\infty\sqrt{m}t_n\delta \Dt^{r+\frac{1}{2}} \\
&\leq  m \delta \Dt^{\frac{3}{2}} + 3m \delta \Dt^{r + \frac{1}{2}}  +\|\bar{\by}_n\|_\infty\sqrt{m}\delta \Dt^{\frac{3}{2}} + 3\|\bar{\by}_n\|_\infty\sqrt{m}\delta \Dt^{r+\frac{1}{2}}  \quad ~({\rm as}~t_n < 1)\\
&\leq 3 m  \delta \Dt^{r + \frac{1}{2}} + 3\|\bar{\by}_n\|_\infty \sqrt{m} \delta \Dt^{r+\frac{1}{2}}\quad ~({\rm as}~r\leq 1~{\rm and}~\Dt << 1).
\end{aligned}
\end{equation}
Thus, in all cases, we have that,
 \begin{equation}
\label{eq:norm5}
\left | \frac{\partial \E^{(k)}_n}{\partial \theta} \right | \leq 3 \delta \Dt\left(m + \sqrt{m}\|\bar{\by}_n\|_\infty\right) \quad ~({\rm as}~ r\geq 1/2).
\end{equation}

Applying the above estimate in \eqref{eq:grad2} allows us to bound the gradient by,
\begin{equation}
\label{eq:norm6}
\left | \frac{\partial \E_n}{\partial \theta} \right | \leq \sum\limits_{1 \leq k \leq n} \left | \frac{\partial \E^{(k)}_n}{\partial \theta} \right | \leq  3\delta t_n\left(m + \sqrt{m}\|\bar{\by}_n\|_\infty\right)  . 
\end{equation}
Therefore, the gradient of the loss function \eqref{eq:lf1} can be bounded as,
\begin{equation}
\label{eq:norm}
\begin{aligned}
\left | \frac{\partial \E}{\partial \theta} \right | &\leq \frac{1}{N} \sum_{n=1}^N  \left | \frac{\partial \E_n}{\partial \theta} \right | \\
&\leq 3 \delta \left[ \frac{m\Dt}{N}\sum_{n=1}^N n + \frac{\sqrt{m}\Dt}{N}\sum_{n=1}^N\|\bar{\by}_n\|_{\infty}n\right] \\
&\leq 3 \delta \left[ \frac{m\Dt}{N}\sum_{n=1}^N n + \frac{\sqrt{m}\bar{Y}\Dt}{N}\sum_{n=1}^N n\right] \\
&\leq  \frac{3}{2} \delta (N+1)\Dt \left(m + \bar{Y} \sqrt{m}\right)  \\
&\leq \frac{3}{2} \delta (t_N + \Dt) \left(m + \bar{Y} \sqrt{m}\right) \\
&\leq \frac{3}{2} \delta (1 + \Dt) \left(m + \bar{Y} \sqrt{m}\right)\quad~({\rm as}~t_N=1) \\
&\leq \frac{3}{2}\left(m + \bar{Y} \sqrt{m}\right),
\end{aligned}
\end{equation}
which is the desired estimate \eqref{eq:gbd}. 
\subsection{On the assumption \eqref{eq:assm} and training}
\label{app:explod_training}
Note that all the estimates were based on the fact that we were able to choose a time step $\Dt$ in \eqref{eq:brnn} that enforces the condition \eqref{eq:assm}. For any fixed weights $\bW, \cW$, we can indeed choose such a value of $\ep$ to satisfy \eqref{eq:assm}. However, we \emph{train} the RNN to find the weights that minimize the loss function \eqref{eq:lf1}. Can we find a hyperparameter $\Dt$ such that \eqref{eq:assm} is satisfied at every step of the stochastic gradient descent method for training?

To investigate this issue, we consider a simple gradient descent method of the form:
\begin{equation}
\label{eq:gd1}
\theta_{\ell+1} = \theta_\ell - \zeta \frac{\partial \E}{\partial \theta} (\theta_\ell).
\end{equation}
Note that $\zeta$ is the constant (non-adapted) learning rate. We assume for simplicity that $\theta_0 = 0$ (other choices lead to the addition of a constant). Then, a straightforward estimate on the weight is given by,
\begin{equation}
\label{eq:gd2}
\begin{aligned}
|\theta_{\ell +1}| &\leq |\theta_{\ell}| + \zeta \left | \frac{\partial \E}{\partial \theta} (\theta_\ell) \right | \\
&\leq  |\theta_{\ell}|  + \zeta \frac{3}{2}\left(m + \bar{Y} \sqrt{m}\right) \quad ({\rm by}~\eqref{eq:norm}) \\
&\leq |\theta_0| + \ell \zeta \frac{3}{2}\left(m + \bar{Y} \sqrt{m}\right)   = \ell \zeta \frac{3}{2}\left(m + \bar{Y} \sqrt{m}\right) .
\end{aligned}
\end{equation}
In order to calculate the minimum number of steps $L$ in the gradient descent method \eqref{eq:gd1} such that the condition \eqref{eq:assm} is satisfied, we set $\ell = L$ in \eqref{eq:gd2} and applying it to the condition \eqref{eq:assm} leads to the straightforward estimate,
\begin{equation}
\label{eq:gd3}
L \geq \frac{1}{\zeta \frac{3}{2}\left(m + \bar{Y} \sqrt{m}\right) m \Dt^{1-r} \delta}.
\end{equation}
Note that the parameter $\delta < 1$, while in general, the learning rate $\zeta << 1$. Thus, as long as $r \leq 1$, we see that the assumption \eqref{eq:assm} holds for a large number of steps of the gradient descent method. We remark that the above estimate \eqref{eq:gd3} is a large underestimate on $L$. In the experiments presented in this article, we are able to take a very large number of training steps, while the gradients remain within a range (see \fref{fig:weight_assumptions}).
\subsection{Proof of Proposition \ref{prop:4}}
\label{app:proof_vanish}
We start with the following decomposition of the recurrent matrices:
\begin{align*}
     \frac{\partial \bX_i}{\partial \bX_{i-1}} &= M_{i-1} + \Dt \tilde{M}_{i-1}, \\
 M_{i-1}&:= \begin{bmatrix}
                                                           \ind  & \Dt \bC_{i-1} \\ 
                                                              \bB_{i-1} & \bC_{i-1} 
                                                              \end{bmatrix},  \quad
 \tilde{M}_{i-1}:= \begin{bmatrix}
                                                           \bB_{i-1}  & {\bf 0} \\ 
                                                              {\bf 0} & {\bf 0} 
                                                              \end{bmatrix},                                                               
\end{align*}
with $\bB,\bC$ defined in \eqref{eq:grad7}. By the assumption \eqref{eq:assm}, one can readily check that $\|\tilde{M}_{i-1}\|_{\infty} \leq \Dt$, for all $k \leq i \leq n-1$.

We will use an induction argument to show the following representation formula for the product of Jacobians,
\begin{equation}
\label{eq:vg2}
 \frac{\partial \bX_n}{\partial \bX_k} = \prod\limits_{k < i \leq n}  \frac{\partial \bX_i}{\partial \bX_{i-1}} =   \begin{bmatrix}
                                                           \ind   & \Dt \sum\limits_{j=k}^{n-1} \prod\limits_{i=j}^k \bC_i \\ 
                    \bB_{n-1}+  \sum\limits_{j=n-2}^{k}\left(\prod\limits_{i=n-1}^{j+1}\bC_i\right)\bB_j  & \prod\limits_{i=n-1}^k \bC_i
                                                              \end{bmatrix} 
                                                  + \ord(\Dt)            .\end{equation}



We start by the outermost product and calculate,
\begin{align*}
    \frac{\partial \bX_n}{\partial \bX_{n-1}}\frac{\partial \bX_{n-1}}{\partial \bX_{n-2}} &= 
    \left(M_{n-1} + \Dt \tilde{M}_{n-1}\right)\left(M_{n-2} + \Dt \tilde{M}_{n-2}\right) \\
    &= M_{n-1}M_{n-2} + \Dt(\tilde{M}_{n-1}M_{n-2} + M_{n-1}\tilde{M}_{n-2}) + \ord(\Dt^2).
\end{align*}
By direct multiplication, we obtain,
\begin{align*}
 M_{n-1}M_{n-2} &= \begin{bmatrix}
                                                           \ind  & \Dt \left(\bC_{n-2}+ \bC_{n-1} \bC_{n-2} \right)\\ 
        \bB_{n-1}+\bC_{n-1}\bB_{n-2} & \bC_{n-1} \bC_{n-2}
        \end{bmatrix}  \\
        &+ \Dt \begin{bmatrix}
                                               \bC_{n-1}\bB_{n-2}              & {\bf 0} \\
        {\bf 0} & \bB_{n-1}\bC_{n-2}
        \end{bmatrix}.
\end{align*}
Using the definitions in \eqref{eq:grad7} and \eqref{eq:assm}, we can easily see that
$$
\begin{bmatrix}
                                               \bC_{n-1}\bB_{n-2}              & {\bf 0} \\
        {\bf 0} & \bB_{n-1}\bC_{n-2}
        \end{bmatrix}  = \ord(\Dt).
$$
Similarly, it is easy to show that
$$
\tilde{M}_{n-1}M_{n-2}, M_{n-1}\tilde{M}_{n-2} \sim \ord(\Dt).
$$
Plugging all the above estimates yields,
$$
\frac{\partial \bX_n}{\partial \bX_{n-1}}\frac{\partial \bX_{n-1}}{\partial \bX_{n-2}}  
    = \begin{bmatrix}
                                                           \ind  & \Dt \left(\bC_{n-2}+ \bC_{n-1} \bC_{n-2} \right)\\ 
        \bB_{n-1}+\bC_{n-1}\bB_{n-2} & \bC_{n-1} \bC_{n-2}
        \end{bmatrix} + \ord(\Dt^2),
$$
which is exactly the form of the leading term \eqref{eq:vg2}.

Iterating the above calculations $(n-k)$ times and realizing that $(n-k)\Dt^2 \approx n\Dt^2 = t_n \Dt$ yields the formula \eqref{eq:vg2}. 

Recall that we have set $\theta = \bW_{i,j}$, for some $1 \leq i,j \leq m$ in proposition \ref{prop:4}. Directly calculating with \eqref{eq:grad3}, \eqref{eq:grad4} and the representation formula \eqref{eq:vg2} yields the formula, 
\begin{equation}
\label{eq:vg4}
\begin{aligned}
\frac{\partial \E^{(k)}_n}{\partial \theta} =  \by_n^\top \Dt^2\delta {\bf Z}^{i,j}_{m,m}(\bA_{k-1}) \by_{k-1} 
+\by_n^\top \Dt^2\delta {\bf C}^{\ast}{\bf Z}^{i,j}_{m,m}(\bA_{k-1}) \by_{k-1} + \ord(\Dt^3),
\end{aligned}
\end{equation}
with matrix ${\bf C}^\ast$ defined as,
$$
{\bf C}^{\ast} := \sum\limits_{j=k}^{n-1} \prod\limits_{i=j}^k \bC_i,
$$
and ${\bf Z}_{m,m}^{i,j}(\bA_{k-1}) \in \mathbb{R}^{m \times m}$ is a matrix with all elements are zero except for the $(i,j)$-th entry which is set to $\sigma^{\prime}(a_{k-1}^i)$, i.e. the $i$-th entry of $\sigma^{\prime}(\bA_{k-1})$.


Note that the formula \eqref{eq:vg4} can be explicitly written as,
\begin{equation}
    \label{eq:vg8}
    \frac{\partial \E^{(k)}_n}{\partial \theta} = \delta \Dt^2 \sigma^{\prime}(a_{k-1}^i) \by^i_n \by^j_{k-1} +  \delta \Dt^2 \sigma^{\prime}(a_{k-1}^i) \sum\limits_{\ell=1}^m \bC^{\ast}_{\ell i}\by^\ell_n \by^j_{k-1}  + \ord(\Dt^3),
\end{equation}
with $\by^j_n$ denoting the $j$-th element of vector $\by_n$, and
\begin{equation}
    \label{eq:vg9}
    a_{k-1}^i:= \sum\limits_{\ell=1}^m \bW_{i\ell}\by^\ell_{k-1} + \sum\limits_{\ell=1}^m \cW_{i\ell}\bz^\ell_{k-1}. 
\end{equation}
By the assumption \eqref{eq:assm}, we can readily see that 
$$
\|\bW\|_{\infty},\|\cW\|_{\infty} \leq 1 + \Dt.
$$
Therefore by the fact that $\sigma^{\prime} = sech^2$, the assumption $\by_k^i = \ord(\sqrt{t_k})$ and \eqref{eq:vg9}, we obtain,
\begin{equation}
    \label{eq:vg10}
    \hat{c} = sech^2(\sqrt{k\Dt}(1+\Dt) \leq \sigma^{\prime}(a^{k-1}_i) \leq 1.
    \end{equation}
Using \eqref{eq:vg10} in \eqref{eq:vg8}, we obtain,
\begin{equation}
    \label{eq:vg11}
    \delta \Dt^2 \sigma^{\prime}(a_{k-1}^i) \by^i_n \by^j_{k-1} = \ord\left(\hat{c}\delta \Dt^{\frac{5}{2}}\right).
\end{equation}
Using the definition of $\bC_i$, we can expand the product in $\bC^{\ast}$ and neglect terms of order $\ord(\Dt^4)$, to obtain
$$
\prod\limits_{i=j}^k \bC_i = (\ord(1) + \ord((j-k+1)\delta \Dt^{2})) \ind.
$$
Summing over $j$ and using the fact that $k << n$, we obtain that
\begin{equation}
    \label{eq:vg12}
    \bC^{\ast} = (\ord(n) + \ord(\delta \Dt^0)) \ind.
\end{equation}
Plugging \eqref{eq:vg12} and \eqref{eq:vg10} into \eqref{eq:vg8} leads to,
\begin{equation}
    \label{eq:vg110}
\delta \Dt^2 \sigma^{\prime}(a_{k-1}^i) \sum\limits_{\ell=1}^m \bC^{\ast}_{\ell i}\by^\ell_n \by^j_{k-1} = \ord\left(\hat{c}\delta \Dt^{\frac{3}{2}}\right) + \ord\left(\hat{c}\delta^2 \Dt^{\frac{5}{2}}\right).
\end{equation}
Combining \eqref{eq:vg11} and \eqref{eq:vg110} yields the desired estimate \eqref{eq:gulb}.
\paragraph{Remark.} A careful examination of the above proof reveals that the constants hidden in the prefactors of the leading term $\ord\left(\hat{c}\delta \Dt^{\frac{3}{2}}\right)$ of \eqref{eq:gulb} stem from the formula \eqref{eq:vg110}. Here, we have used the assumption that $\by_k^i = \ord(\sqrt{t_k})$. Note that this assumption implicitly assumes that the energy bound \eqref{eq:enb} is \emph{equidistributed} among all the elements of the vector $\by_k$ and results in the obfuscation of the constants in the leading term of \eqref{eq:gulb}. Given that the energy bound \eqref{eq:enb} is too coarse to allow for precise upper and lower bounds on each individual element of the hidden state vector $\by_k$, we do not see any other way of, in general, determining the distribution of energy among individual entries of the hidden state vector. Thus, assuming equidistribution seems reasonable. On the other hand, in practice, one has access to all the terms in formula \eqref{eq:vg110} for each numerical experiment and if one is interested, then one can directly evaluate the precise bound on the leading term of the formula \eqref{eq:gulb}. 
\section{Rigorous estimates for the RNN \eqref{eq:brnn} with $\bar{n}=n-1$ and general values of $\ep,\gamma$}
\label{sec:exp}
In this section, we will provide rigorous estimates, similar to that of propositions \ref{prop:1}, \ref{prop:2} and \ref{prop:3} for the version of coRNN \eqref{eq:brnn} that results by setting $\bar{n}=n-1$ in \eqref{eq:brnn} leading to,
\begin{equation}
\label{eq:ernn}
\begin{aligned}
\by_n &= \by_{n-1} + \Dt \bz_n,\\
\bz_n &= \bz_{n-1} + \Dt \sigma\left(\bW\by_{n-1} +  {\bf\cW} \bz_{n-1} + \bV \bu_{n} + \bb \right) -\Dt \gamma \by_{n-1} -  \Dt \ep \bz_{n-1}.  \end{aligned}
\end{equation}
Note that \eqref{eq:ernn} can be equivalently written as,
\begin{equation}
\label{eq:ernn1}
\begin{aligned}
\by_n &= \by_{n-1} + \Dt \bz_n,\\
\bz_n &= \left(1-\ep \Dt\right) \bz_{n-1} + \Dt \sigma\left(\bW\by_{n-1} +  {\bf\cW} \bz_{n-1} + \bV \bu_{n} + \bb \right) -\Dt \gamma \by_{n-1}.  \end{aligned}
\end{equation}
We will also consider the case of non-unit values of the control parameters $\gamma$ and $\ep$ below.
\paragraph{Bounds on Hidden states.} We start the following bound on the hidden states of \eqref{eq:ernn},
\begin{proposition}
\label{prop:en1}
Let the damping parameter $\ep > \frac{1}{2}$ and the time step $\Dt$ in the RNN \eqref{eq:ernn} satisfy the following condition,
\begin{equation}
    \label{eq:ndt}
    \Dt < \frac{2\ep -1}{\gamma + \ep^2}.
\end{equation}
Let $\by_n,\bz_n$ be the hidden states of the RNN \eqref{eq:ernn} for $1\leq n \leq N$, then the hidden states satisfy the following (energy) bounds:
\begin{equation}
    \label{eq:nenb}
    \by_n^\top \by_n + \frac{1}{\gamma} \bz_n^\top \bz_n \leq \frac{m t_n}{\gamma}.
\end{equation}
\end{proposition}
We set $\bA_{n-1} = \bW \by_{n-1} + \cW \bz_{n-1} + \bV\bu_{n-1} + \bb$ and as in the proof of proposition \ref{prop:1}, we  multiply $({\by}_{n-1}^\top, \frac{1}{\gamma}{\bz}^\top_n)$ to \eqref{eq:ernn} and use elementary identities and rearrange terms to obtain,
\begin{align*}
\frac{\by_n^\top \by_n}{2} + \frac{\bz_n^\top \bz_n}{2\gamma} &= \frac{\by_{n-1}^\top \by_{n-1}}{2} + \frac{\bz_{n-1}^\top \bz_{n-1}}{2\gamma} + \frac{(\by_n- \by_{n-1})^\top (\by_n- \by_{n-1})}{2} \\
&- \frac{(\bz_n- \bz_{n-1})^\top (\bz_n- \bz_{n-1})}{2\gamma} \\
&+  \frac{\Dt}{\gamma} \bz_n^\top \sigma(\bA_{n-1}) -  \frac{\ep\Dt}{\gamma} \bz_n^\top\bz_n + \frac{\ep \Dt}{\gamma}\bz_n^{\top}\left(\bz_n-\bz_{n-1}\right).
\end{align*}
We use a \emph{rescaled version} of the well-known Cauchy's inequality 
$$
ab \leq \frac{c a^2}{2} + \frac{b^2}{2c},
$$
for a constant $c > 0$ to be determined, to rewrite the above identity as,
\begin{align*}
\frac{\by_n^\top \by_n}{2} + \frac{\bz_n^\top \bz_n}{2\gamma} &\leq \frac{\by_{n-1}^\top \by_{n-1}}{2} + \frac{\bz_{n-1}^\top \bz_{n-1}}{2\gamma} + \frac{(\by_n- \by_{n-1})^\top (\by_n- \by_{n-1})}{2} \\
&+\left(\frac{\ep \Dt}{2 c \gamma} - \frac{1}{2 \gamma}\right)(\bz_n- \bz_{n-1})^\top (\bz_n- \bz_{n-1})
+ \frac{\Dt}{2\gamma}\sigma(\bA_{n-1})^\top \sigma(\bA_{n-1}) \\
&+ \left(\frac{\Dt}{2\gamma} + \frac{c \ep \Dt}{2 \gamma} - \frac{\ep \Dt}{\gamma}\right) \bz_n^{\top}\bz_n.  
\end{align*}
Using the first equation in \eqref{eq:ernn}, the above inequality reduces to,
\begin{align*}
\frac{\by_n^\top \by_n}{2} + \frac{\bz_n^\top \bz_n}{2\gamma} &\leq \frac{\by_{n-1}^\top \by_{n-1}}{2} + \frac{\bz_{n-1}^\top \bz_{n-1}}{2\gamma} \\
&+\left(\frac{\ep \Dt}{2 c \gamma} - \frac{1}{2 \gamma}\right)(\bz_n- \bz_{n-1})^\top (\bz_n- \bz_{n-1})
+ \frac{\Dt}{2\gamma}\sigma(\bA_{n-1})^\top \sigma(\bA_{n-1}) \\
&+ \left(\frac{\Dt^2}{2} + \frac{\Dt}{2\gamma} + \frac{c \ep \Dt}{2 \gamma} - \frac{\ep \Dt}{\gamma}\right) \bz_n^{\top}\bz_n.
\end{align*}
As long as,
\begin{equation}
    \label{eq:ndt1}
    \Dt \leq \min \left(\frac{c}{\epsilon}, \frac{(2-c)\ep-1}{\gamma}\right),
\end{equation}
we can easily check that,
\begin{align*}
\frac{\by_n^\top \by_n}{2} + \frac{\bz_n^\top \bz_n}{2\gamma} &\leq \frac{\by_{n-1}^\top \by_{n-1}}{2} + \frac{\bz_{n-1}^\top \bz_{n-1}}{2\gamma} + \frac{\Dt}{2\gamma}\sigma(\bA_{n-1})^\top \sigma(\bA_{n-1}) \\
&\leq \frac{\by_{n-1}^\top \by_{n-1}}{2} + \frac{\bz_{n-1}^\top \bz_{n-1}}{2\gamma} + \frac{m\Dt}{2\gamma}\quad (\sigma \leq 1).
\end{align*}
Iterating the above bound till $n=0$ and using the zero initial data yields the desired \eqref{eq:nenb} as long as we find a $c$ such that the condition \eqref{eq:ndt1} is satisfied. To do so, we equalize the two terms on the right hand side of \eqref{eq:ndt1} to obtain, 
$$
c = \frac{\ep(2\ep-1)}{\gamma + \ep^2}.
$$
From the assumption \eqref{eq:ndt} and the fact that $\ep > \frac{1}{2}$, we see that such a $c > 0$ always exists for any value of $\gamma > 0$ and \eqref{eq:ndt1} is satisfied, which completes the proof.

We remark that the same bound on the hidden states is obtained for both versions of coRNN, i.e. \eqref{eq:brnn} with $\bar{n} = n$ and \eqref{eq:ernn}. However, the difference lies in the constraint on the time step $\Dt$. In contrast to \eqref{eq:ndt}, a careful examination of the proof of proposition \ref{prop:1} reveals that the condition on the time step for the stability of \eqref{eq:brnn} with $\bar{n} = n$ is given by,
\begin{equation}
    \label{eq:dt}
    \Dt < \frac{2\ep -1}{\gamma},
\end{equation}
and is clearly less stringent than the condition \eqref{eq:ndt1} for the stability of \eqref{eq:ernn}. For instance, in the prototypical case of $\gamma = \ep =1$, the stability of \eqref{eq:brnn} with $\bar{n} = n$ is ensured for any $\Dt < 1$. On the other hand, the stability of \eqref{eq:ernn} is ensured as long as $\Dt < \frac{1}{2}$. However, it is essential to recall that these conditions are only sufficient to ensure stability and are by no means necessary. Thus in practice, the coRNN version \eqref{eq:ernn} is found to be stable in the same range of time steps as the version \eqref{eq:brnn} with $\bar{n} = n$. 

\paragraph{On the exploding and vanishing gradient problems for coRNN \eqref{eq:ernn}}
Next, we have the following upper bound on the hidden state gradients for the version \eqref{eq:ernn} of coRNN,
\begin{proposition}
\label{prop:n3}
Let $\by_n,\bz_n$ be the hidden states generated by the RNN \eqref{eq:ernn}. We assume that the damping parameter $\ep > \frac{1}{2}$ and the time step $\Dt$ can be chosen such that in addition to \eqref{eq:ndt1} it also satisfies, 
\begin{equation}
\label{eq:nassm}
\max \left\{\Dt (\gamma + \|\bW\|_{\infty}),  \Dt  \|{\bf \cW}\|_{\infty} \right \}  = \eta \leq \tilde{C}\Dt^r, \quad \frac{1}{2} \leq r \leq 1,  
\end{equation}
and with the constant $\tilde{C}$ independent of the other parameters of the RNN \eqref{eq:ernn}. Then the gradient of the loss function $\E$ \eqref{eq:lf1} with respect to any parameter $\theta \in {\bf \Theta}$ is bounded as,
\begin{equation}
    \label{eq:ngbd}
    \left| \frac{\partial \E}{\partial \theta} \right| \leq \frac{3(\tilde{C})\left(m+\bar{Y}\sqrt{m}\right)}{2\gamma},
\end{equation}
with the constant $\tilde{C}$, defined in \eqref{eq:nassm} and $\bar{Y} = \max\limits_{1 \leq n \leq N} \|\bar{\by}_n\|_{\infty}$ be a bound on the underlying training data
\end{proposition}
The proof of this proposition is completely analogous to the proof of proposition \ref{prop:3} and we omit the details here.

Note that the bound \eqref{eq:ngbd} enforces that hidden state gradients cannot explode for version \eqref{eq:ernn} of coRNN. A similar statement for the vanishing gradient problem is inferred from the proposition below.
\begin{proposition}
\label{prop:n4}
Let $\by_n$ be the hidden states generated by the RNN \eqref{eq:ernn}. Under the assumption that $\by^i_n = \ord(\sqrt{\frac{t_n}{\gamma}})$, for all $1 \leq i \leq m$ and \eqref{eq:nassm}, the gradient for long-term dependencies satisfies,
\begin{equation}
    \label{eq:ngulb}
    \frac{\partial \E^{(k)}_n}{\partial \theta} = \ord\left(\frac{\hat{c}}{\gamma}\Dt^{\frac{3}{2}}\right) + \ord\left(\frac{\hat{c}}{\gamma}\delta(1+\delta)\Dt^{\frac{5}{2}}\right) + \ord(\Dt^3),~~ \hat{c} = sech^2\left(\sqrt{k\Dt}(1+\Dt)\right)
     \quad k << n.
\end{equation}
\end{proposition}
The proof is a repetition of the steps of the proof of proposition \ref{prop:4}, with suitable modifications for the structure of the RNN and non-unit $\epsilon,\gamma$ and we omit the tedious calculations here. Note that \eqref{eq:ngulb} rules out the vanishing gradient problem for the coRNN version \eqref{eq:ernn}. 


\end{document}
