

\documentclass[nohyperref, dvipsnames]{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}
\usepackage{multirow}
\usepackage{subfloat}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{template/icml2023}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{import}




\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\newcommand{\dl}[1]{\textcolor{purple}{derek: #1}}
\newcommand{\lm}[1]{\textcolor{violet}{liheng: #1}}
\newcommand{\ars}[1]{\textcolor{blue}{ars: #1}}
\newcommand{\lc}[1]{\textcolor{ForestGreen}{chen: #1}}

\newcommand{\V}{\mathcal{V}} \newcommand{\E}{\mathcal{E}} \newcommand{\mlp}{\mathrm{MLP}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\rw}{\mathbf{M}}
\newcommand{\spd}{\mathrm{SPD}}
\renewcommand{\P}{\mathbf{P}}
\def\ourmethod {GRIT }










\icmltitlerunning{Graph Inductive Biases in Transformers without Message Passing}

\begin{document}

\twocolumn[
\icmltitle{Graph Inductive Biases in Transformers without Message Passing}





\icmlsetsymbol{equal}{*}
\icmlsetsymbol{ills2}{8}

\begin{icmlauthorlist}
\icmlauthor{Liheng Ma}{equal,mcgill,ills2}
\icmlauthor{Chen Lin}{equal,oxford}
\icmlauthor{Derek Lim}{mit}
\icmlauthor{Adriana Romero-Soriano}{metaai,mcgill,mila,cifar}
\icmlauthor{Puneet K. Dokania}{oxford,fiveai}

\icmlauthor{Mark Coates}{mcgill,ills}
\icmlauthor{Philip H.S. Torr}{oxford}
\icmlauthor{Ser-Nam Lim}{metaai}
\end{icmlauthorlist}

\icmlaffiliation{mcgill}{McGill University}
\icmlaffiliation{oxford}{Department of Engineering Science, University of Oxford}
\icmlaffiliation{mit}{CSAIL, Massachusetts Institute of Technology}
\icmlaffiliation{fiveai}{FiveAI}
\icmlaffiliation{metaai}{MetaAI}
\icmlaffiliation{mila}{Mila - Quebec AI Institute} 
\icmlaffiliation{ills}{International Laboratory on Learning Systems (ILLS)} 
\icmlaffiliation{cifar}{Canada CIFAR AI Chair} 

\icmlcorrespondingauthor{Liheng Ma}{liheng.ma@mail.mcgill.ca}
\icmlcorrespondingauthor{Chen Lin}{chen.lin@eng.ox.ac.uk}

\icmlkeywords{Graph Transformers, Message Passing, Inductive Bias}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}

Transformers for graph data are increasingly widely studied and successful in numerous learning tasks.  
Graph inductive biases are crucial for Graph Transformers, and previous works incorporate them using message-passing modules and/or positional encodings.
However, Graph Transformers that use message-passing inherit known issues of message-passing, and differ significantly from Transformers used in other domains, thus making transfer of research advances more difficult. 
On the other hand, Graph Transformers without message-passing often perform poorly on smaller datasets, where inductive biases are more important.
To bridge this gap, we propose the Graph Inductive bias Transformer (GRIT) --- a new Graph Transformer that incorporates graph inductive biases without using message passing. GRIT is based on several architectural changes that are each theoretically and empirically justified, including: learned relative positional encodings initialized with random walk probabilities, a flexible attention mechanism that updates node and node-pair representations, and injection of degree information in each layer. We prove that GRIT is expressive --- it can express shortest path distances and various graph propagation matrices. GRIT achieves state-of-the-art empirical performance across a variety of graph datasets,
thus showing the power that Graph Transformers without message-passing can deliver. 














\end{abstract}

\section{Introduction}

Following the success of Transformers~\citep{vaswani2017AttentionAllYou} in different modalities like natural language processing (NLP)~\citep{vaswani2017AttentionAllYou} and computer vision~\citep{dosovitskiy2020ImageWorth16x16}, 
developing Transformers for graph data has attracted much interest ~\cite{dwivedi2021GeneralizationTransformerNetworks, kreuzer2021RethinkingGraphTransformers, ying2021TransformersReallyPerform, chen2022StructureAwareTransformerGraph, hussain2022GlobalSelfAttentionReplacement, rampasek2022RecipeGeneralPowerful, zhang2023rethinking}. 
A major motivation of Graph Transformers is to alleviate certain known
limitations of (local) Message-Passing Graph Neural Networks (MPNNs)~\cite{ gilmer2017NeuralMessagePassing}, such as over-smoothing~\cite{li2018DeeperInsightsGraph, oono2020graph}, over-squashing~\cite{alon2020BottleneckGraphNeural, topping2022UnderstandingOversquashingBottlenecks}, and expressive power limitations~\cite{xu2019HowPowerfulAre, loukas2020WhatGraphNeural, morris2019WeisfeilerLemanGo}.










However, it is known that transformers generally \textit{lack strong inductive biases}~\cite{dosovitskiy2020ImageWorth16x16}.
In the graph domain, Graph Transformers aggregate information based on a learned attention matrix, which enjoys a high degree of freedom; in contrast, MPNNs explicitly aggregate the information according to the exact topology of the input graph.
This comes with at least two consequences.
First, Graph Transformers may be prone to over-fitting, and thus they often fail to outperform MPNNs in limited data settings.
Second, learning meaningful attention scores typically requires capturing important positional or structural relationships between nodes, 
and strong positional encodings are challenging to design since the structure and symmetries of graph data are fundamentally different from that of other (Euclidean) domains ~\cite{vaswani2017AttentionAllYou, bronstein2021GeometricDeepLearning}.
For instance, there is no ordering or canonical coordinate system for nodes in a graph, whereas words in a sentence have a sequence structure, and pixels in an image have a grid structure.

To incorporate graph inductive biases, many of the best-performing Graph Transformers explicitly integrate local message-passing mechanisms.
For instance, some works incorporate sparse attention on local neighborhoods \cite{dwivedi2021GeneralizationTransformerNetworks, kreuzer2021RethinkingGraphTransformers}, 
or integrate various other types of MPNN modules into their models~\cite{chen2022StructureAwareTransformerGraph, rampasek2022RecipeGeneralPowerful}.
Thus, such Graph Transformers may at least partially inherit some of the limitations of MPNNs.
Moreover, message-passing modules within Graph Transformers make these models significantly different from the Transformers used in other domains; thus, it becomes more difficult to transfer some of the large quantities of Transformer research from other domains into the graph domain.
This is exacerbated by the fact that integrating message-passing
adds complexity to the design space of the underlying model, necessitating more architectural decisions and extra effort for hyperparameter tuning \cite{masters2022GPSOptimisedHybrid}.


The trade-off between the limitations of message-passing and the importance of graph inductive biases can be seen in the empirical performance of models on competitive benchmarks. For instance, let us consider the results for two popular molecular graph regression benchmarks as of May 2023. On the small ZINC dataset (12,000 graphs)~\citep{dwivedi2020BenchmarkingGraphNeural}, GNNs that rely on message-passing take up the top spots on the leaderboards.\footnote{\url{https://paperswithcode.com/sota/graph-regression-on-zinc-500k} at the time of submission.} For the large PCQM4MV2 dataset (about 3,700,000 graphs)~\citep{hu2021ogblsc}, Graph Transformers take up the top spots.\footnote{\url{https://ogb.stanford.edu/docs/lsc/leaderboards/}}

In this work, we introduce the \underline{Gr}aph \underline{I}nductive bias \underline{T}ransformer (GRIT), a Graph Transformer that incorporates useful graph inductive biases without explicit message-passing modules. Our model is based on three design choices that integrate graph inductive biases, each of which is theoretically justified: (a) we use a learned relative positional encoding initialized with Relative Random Walk Probabilities (RRWP); this learned positional encoding can provably express shortest path distances~\citep{ying2021TransformersReallyPerform} and general classes of message-passing propagations~\citep{gasteiger2019DiffusionImprovesGraph, zhao2021AdaptiveDiffusionGraph, xu2019HowPowerfulAre}. (b) we develop an attention mechanism that jointly updates both node representations and node-pair representations, and can thus learn to update the RRWP positional encodings; a distance based Weisfeiler Leman test~\citep{zhang2023rethinking} shows that certain Graph Transformers with RRWP are strictly stronger than Graph Transformer with shortest path distances like Graphormer~\citep{ying2021TransformersReallyPerform}. (c) we inject degree information into our Transformer update using degree scalers, with batch normalization replacing the standard layer normalization; replacing the layer normalization is provably required to maintain the degree information.



Along with theoretical justification, we provide ample empirical evidence to demonstrate the effectiveness of our design choices. GRIT achieves state-of-the-art empirical performance across a variety of graph learning benchmarks, both small and large-scale. In particular, we bridge the performance gap in which message-passing-based methods do not perform as well in large datasets, and non-message-passing Transformers do not perform as well in small datasets. Ablations and synthetic experiments further justify our design decisions, showing that GRIT indeed integrates graph inductive biases into Transformers in an effective manner.\footnote{The code and models are publicly available at \url{https://github.com/LiamMa/GRIT}.}









\section{Related Work}


\paragraph{Transformers for Euclidean Domains}

Transformers have achieved ground-breaking successes in various domains, including but not limited to natural language processing~\cite{vaswani2017AttentionAllYou, devlin2019BERTPretrainingDeep, dai2019TransformerXLAttentiveLanguage} and computer vision~\cite{dosovitskiy2020ImageWorth16x16, liu2021SwinTransformerHierarchical}.
As can be seen in both domains, Transformers often suffer from a lack of inductive bias compared to Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), and typically require a large amount of training data in order to perform well.
Recent studies have found that introducing more inductive biases can effectively improve the performance of Transformers~\cite{liu2021SwinTransformerHierarchical, park2021HowVisionTransformers}



\paragraph{Positional Encoding and Structural Encoding for Graphs}

In the graph domain, positional encodings (PE) and structural encodings (SE)~\citep{srinivasan2019EquivalencePositionalNode} have been studied for enhancing both Message-Passing Graph Neural Networks (MPNNs) and Graph Transformers
~\cite{you2019PositionawareGraphNeural,ma2021graph,li2020DistanceEncodingDesign,zhang2021EigenGNNGraphStructure, dwivedi2020BenchmarkingGraphNeural, loukas2020WhatGraphNeural, dwivedi2021GraphNeuralNetworks, lim2022SignBasisInvariant, wang2022EquivariantStablePositional}.
Such positional or structural encodings capture various types of graph features, such as shortest-path distances~\cite{li2020DistanceEncodingDesign}, identity-awareness~\cite{you2021IdentityawareGraphNeural}, and spectral information~\cite{dwivedi2020BenchmarkingGraphNeural}.

Many so-called positional or structural encodings contain both
positional and structural information ~\cite{dwivedi2020BenchmarkingGraphNeural, srinivasan2019EquivalencePositionalNode, rampasek2022RecipeGeneralPowerful}; 
thus, researchers use the terms positional encoding and structural encoding interchangeably in related literature. In this work, we use ``positional encodings'' as an umbrella term.

\paragraph{Graph Transformers with Message-Passing}

\citet{dwivedi2021GeneralizationTransformerNetworks} is one of the early works that introduce a Transformer architecture to graph domains with Laplacian Positional Encoding (LapPE).
However, the global attention version of it performs poorly compared to the message-passing-based sparse attention version.
\citet{kreuzer2021RethinkingGraphTransformers} propose the SAN model, which uses both sparse and global attention mechanisms at each layer, 
and introduces an extra transformer to encode LapPE. 
SignNet~\citep{lim2022SignBasisInvariant} is a specialized symmetry-invariant encoder for LapPE that uses message-passing modules to process Laplacian eigenvectors~\citep{lim2022SignBasisInvariant, rampasek2022RecipeGeneralPowerful}.

Several Graph Transformers~\cite{chen2022StructureAwareTransformerGraph, rampasek2022RecipeGeneralPowerful} use random walk structural encodings (RWSE)~\cite{dwivedi2021GraphNeuralNetworks}. 
Due in part to the fact that RWSE focuses on structural information and does not encode as much positional information,
these Graph Transformers each integrate message-passing modules.


\paragraph{Graph Transformers without Message-Passing}

In contrast to the aforementioned works, a number of Graph Transformers without (local) message-passing have been proposed. 
Among them, a series of works propose to use relative positional encodings for each pair of nodes, such as shortest-path distance positional encodings (SPDPE)~\cite{ying2021TransformersReallyPerform, park2022GRPERelativePositional, luo2022your}.
Generalized-Distance Transformers~\cite{zhang2023rethinking}  introduce other distances on graphs (e.g., resistance distances) as relative positional encodings.  
These works are among the most related to our method and typically perform well on large datasets such as the PCQM4Mv2 dataset from the OGB Large Scale Challenge~\cite{hu2021ogblsc}.
However, they perform worse on smaller datasets (e.g., ZINC~\cite{dwivedi2020BenchmarkingGraphNeural}) compared to hybrid Graph Transformers and MPNNs~\cite{rampasek2022RecipeGeneralPowerful}. 
Based on a synthetic experiment (Sec.~\ref{sec:attn2mp}), we give evidence that due to the lack of sufficient inductive bias, they are not as capable in enabling attention mechanisms to perform local message-passing when necessary.


There are also several other Graph Transformers without message-passing proposed in recent years:
TokenGT~\cite{kim2022PureTransformersAre} proposes a Transformer that views both nodes and edges as tokens, and uses LapPE or  orthogonal random features; Relational Transformer~\citep{diao2022relational} also proposes to update both node and edge tokens;
EGT~\cite{hussain2022GlobalSelfAttentionReplacement} proposes to use an SVD-based PE instead of LapPE for directed graphs;
\cite{mialon2021GraphiTEncodingGraph, feldman2022weisfeiler} introduce positional encodings based on heat kernels and other graph kernels.
































































%
 \begin{figure*}[ht]
\newcommand{\wth}{.20}
\centering
{
\hspace{8pt} \includegraphics[width=\wth\textwidth]{Sections/Figures/fluorescein_1.pdf}
\includegraphics[width=\wth\textwidth]{Sections/Figures/fluorescein_2.pdf}
\includegraphics[width=\wth\textwidth]{Sections/Figures/fluorescein_3.pdf}
\includegraphics[width=\wth\textwidth]{Sections/Figures/fluorescein_4.pdf}
\
    \P_{i,j} = [\mathbf{I}, \rw, \rw^2, \dots, \rw^{K-1}]_{i,j} \in \mathbb{R}^K,

   \begin{aligned}
    &
    \hat{\e}_{i,j} = \sigma\Big( \rho\left(\left(\W_\text{Q} \x_i + \W_\text{K}\x_j\right) \odot \W_\text{Ew}\e_{i,j}\right)\\
   & \text{\hspace{5cm}} + \W_\text{Eb}\e_{i,j} \Big) \in \mathbb{R}^{d'}, \\
    &\alpha_{ij} = \text{Softmax}_{j \in \mathcal{V}} (\W_\text{A} \hat{\e}_{i,j}) \in \mathbb{R},\\
    & \hat{\x}_i =  \sum_{j \in \mathcal{V}} \alpha_{ij} \cdot ( \W_\text{V} \x_j + \W_\text{Ev} \hat{\e}_{i,j}) \in \mathbb{R}^d,\\
   \end{aligned}
\label{eq:attention_out}
   \begin{aligned}
    \x^\text{out}_i =  \sum_{h=1}^{N_h} \W^h_\text{O}\hat{\x}^h_i \in \mathbb{R}^d\,,  \\
    \e^\text{out}_{ij} =  \sum_{h=1}^{N_h} \W^h_\text{Eo}\hat{\e}^h_{ij}  \in \mathbb{R}^d \,, \\
   \end{aligned}

    \chi_G^t(v) = \hash(\multiset{ (d_G(v,u),  \chi_G^{t-1}(u)) : u \in \V}  ).

   \begin{aligned}
    \x^{\text{out}'}_i :=  \x^\text{out}_i \odot \btheta_1 + \left( \log(1 + d_i) \cdot \x^\text{out}_i \odot \btheta_2 \right) \in \mathbb{R}^d \,, \\
   \end{aligned}

     \min \{i :\  d_G^{\mathrm{RW}}(v,u)_i \neq 0 \}   = d_G^{\mathrm{SPD}}(v,u) \},   
    
    L = \min_{\A \in \mathbb{G}_n} \  \min_{i,j,t : \P(\A)_{ijt} > 0} \P(\A)_{ijt}.

    f_1(\P_{i, j, :})_t = \begin{cases}
        1 & \text{if  can reach  in  hops}\\
        0 & \text{else}
    \end{cases}.

     (f_2 \circ f_1(\P_{i, j, :}))_t = \begin{cases}
        1 & \text{if }\\
        0 & \text{else}
    \end{cases}.

    f_3 \circ f_2 \circ f_1(\P_{i, j, :}) = \begin{cases}
        \spd(i,j) & \text{if }\\
        n & \text{else}
    \end{cases}.

    \sum_{k=0}^{K-1} \theta_k \rw^k = \sum_{k=0}^{K-1} \theta_k \P_{:, :, k}.

    f_2 \circ f_1(\P_{i,j}) = \sum_{k=0}^{K-1} \theta_k \P_{i,j,k},

    f_3 \circ f_2 \circ f_1(\P_{i,j}) = \theta_0 \mathbf{I} + \theta_1 \A

    \begin{aligned}
    & \mu^\text{sum}_i = \frac{1}{F} \sum_{j=1}^F x^\text{sum}_{ij} = 
    \frac{1}{F} \sum_{j=1}^F d_i \cdot x^\text{mean}_{ij} 
    = \frac{d_i}{F} \sum_{j=1}^F x^\text{mean}_{ij} 
    = d_i \cdot \mu^\text{mean}_i \\
    &\sigma^\text{sum}_i =
    \sqrt{\frac{1}{F}\sum_{j=1}^F (x^\text{sum}_{ij}-\mu^\text{sum})^2}
    = 
    \sqrt{\frac{d_i^2}{F}\sum_{j=1}^F (x^\text{mean}_{ij}-\mu^\text{mean})^2} = d_i \cdot \sigma_i^\text{mean}
    \end{aligned}

    \begin{aligned}
        \tilde{x}_{ij}^\text{sum} = 
        \frac{({x}_{ij}^\text{sum} - \mu_i^\text{sum})}{\sigma^\text{sum}_i} 
        = 
        \frac{(d_i \cdot {x}_{ij}^\text{mean} - d_i\cdot \mu_i^\text{mean})}{d_i \cdot \sigma^\text{mean}_i}
        = 
        \frac{( {x}_{ij}^\text{mean} - \mu_i^\text{mean})}{\sigma^\text{mean}_i} = \tilde{x}_{ij}^\text{mean},
        \quad \forall i \in \mathcal{V}, \forall j=1,\dots, F, 
    \end{aligned}

will be the same for both sum-aggregation and mean-aggregation. 

The same conclusion can be seen for degree scalers, by simply changing  to  in the proof, where .

\end{proof}

 





\end{document}
