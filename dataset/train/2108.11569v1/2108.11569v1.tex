\documentclass{article}


\usepackage{arxiv}







\usepackage{tabularx}
\usepackage{longtable}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}		\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{dblfloatfix}
\usepackage[space]{grffile}
\usepackage{rotating}
\usepackage{xcolor,colortbl}
\usepackage{amsmath}

\usepackage{multirow}
\usepackage{enumerate} 
\usepackage{xcolor,colortbl}
\usepackage{enumitem}
\usepackage{bm}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{caption}
\usepackage{pifont}
\usepackage{amssymb}


\def\c{{\boldsymbol c}}
\def\g{{\boldsymbol g}}
\def\x{{\boldsymbol x}}
\def\y{{\boldsymbol y}}
\def\z{{\boldsymbol z}}
\def\w{{\boldsymbol w}}
\def\W{{\boldsymbol W}}
\def\D{{\boldsymbol D}}
\def\Y{{\boldsymbol Y}}
\def\X{{\boldsymbol X}}
\def\Z{{\boldsymbol Z}}

\def\DD{{\mathcal D}}
\def\II{{\mathcal I}}
\def\LL{{\mathcal L}}
\def\SS{{\mathcal S}}
\def\XX{{\mathcal X}}
\def\YY{{\mathcal Y}}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\definecolor{Gray}{gray}{0.9}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{white}}c}

\newcommand*{\belowrulesepcolor}[1]{\noalign{\kern-\belowrulesep 
    \begingroup 
      \color{#1}\hrule height\belowrulesep 
    \endgroup 
  }} 
\newcommand*{\aboverulesepcolor}[1]{\noalign{\begingroup 
      \color{#1}\hrule height\aboverulesep 
    \endgroup 
    \kern-\aboverulesep 
  }} 


\def\algo{{\textsc{RoLT}}}

\newcommand\mycommfont[1]{\footnotesize\textcolor{gray}{#1}}
\SetCommentSty{mycommfont}



\setlength\heavyrulewidth{0.25ex}

\title{Robust Long-Tailed Learning under Label Noise}




\author{
    Tong Wei\thanks{equal contribution}, \quad Jiang-Xin Shi\footnotemark[1], \quad Wei-Wei Tu, \quad Yu-Feng Li \\
    Nanjing University, China \\
    4Paradigm Inc. \\
    \texttt{\{weit,shijx\}@lamda.nju.edu.cn}\\
}





\begin{document}
\maketitle

\begin{abstract}
Long-tailed learning has attracted much attention recently, with the goal of improving generalisation for tail classes. Most existing works use supervised learning without considering the prevailing noise in the training dataset. To move long-tailed learning towards more realistic scenarios, this work investigates the label noise problem under long-tailed label distribution. We first observe the negative impact of noisy labels on the performance of existing methods, revealing the intrinsic challenges of this problem. As the most commonly used approach to cope with noisy labels in previous literature, we then find that the small-loss trick fails under long-tailed label distribution. The reason is that deep neural networks cannot distinguish correctly-labeled and mislabeled examples on tail classes. To overcome this limitation, we establish a new prototypical noise detection method by designing a distance-based metric that is resistant to label noise. Based on the above findings, we propose a robust framework,~\algo, that realizes noise detection for long-tailed learning, followed by soft pseudo-labeling via both label smoothing and diverse label guessing. Moreover, our framework can naturally leverage semi-supervised learning algorithms to further improve the generalisation. Extensive experiments on benchmark and real-world datasets demonstrate the superiority of our methods over existing baselines. In particular,
our method outperforms DivideMix by 3\% in test accuracy. Source code will be released soon. \end{abstract}



\section{Introduction}

Classification problems in real-world typically exhibit a long-tailed label distribution, where most classes are associated with only a few examples, e.g., visual recognition~\cite{DBLP:conf/cvpr/HornASCSSAPB18,DBLP:conf/cvpr/0002MZWGY19,DBLP:conf/cvpr/TanWLLOYY20}, instance segmentation~\cite{DBLP:conf/cvpr/GuptaDG19}, and text categorization~\cite{weit2020tnnls}. Due to the paucity of training examples, generalisation for tail classes is challenging; moreover, na√Øve learning on such data is susceptible to an undesirable bias towards head classes. Recently, long-tailed learning (LTL) has gained renewed interest in the context of deep neural networks~\cite{DBLP:conf/nips/WangRH17,DBLP:conf/nips/CaoWGAM19ldam,DBLP:conf/cvpr/CuiJLSB19,DBLP:conf/iclr/KangXRYGFK20,DBLP:conf/eccv/WuH0WL20,DBLP:conf/nips/YangX20rethinking,DBLP:journals/corr/abs-2104-02703,wang2021diverse}. Two active strands of work involve normalisation of the classifier's weights, and modification of the underlying loss to account for different class penalties. Each of these strands is intuitive, and has been empirically shown to be effective~\cite{menon2021logitadjust}. 

The above-mentioned LTL methods with remarkable performance are mostly trained on clean datasets with high-quality human annotations. However, in real-world machine learning applications, annotating a large-scale dataset is costly and time-consuming. Some recent works resort to the large amount of web data as a source of supervision for training deep neural networks~\cite{webvision}. While the existing works have shown advantages in various applications~\cite{DBLP:conf/eccv/LiNX14,ma2018dimensionality}, web data is naturally class-imbalanced~\cite{li2020dividemix,li2021mopro} and accompanied with label noise~\cite{xu2019l_dmi,li2020dividemix,yao2020dualt,xia2021robust}. As a result, it is crucial that deep neural networks can harvest noisy and class-imbalanced training data. 


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.268\textwidth}
        \centering
        \includegraphics[width=\linewidth]{problem_setup.pdf}
        \caption{Problem setup} \label{fig:problem_setup}
    \end{subfigure}
    \begin{subfigure}[b]{0.244\textwidth}
        \centering
        \includegraphics[width=\linewidth]{compare_cifar10.pdf}
        \caption{CIFAR-10} \label{fig:dividemix_cifar10}
    \end{subfigure}
    \begin{subfigure}[b]{0.244\textwidth}
        \centering
        \includegraphics[width=\linewidth]{compare_cifar100.pdf}
        \caption{CIFAR-100} \label{fig:dividemix_cifar100}
    \end{subfigure}
    \begin{subfigure}[b]{0.223\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ncm_confusion_matrix.pdf}
        \caption{Confusion matrix} \label{fig:ncm_confusion_matrix}
    \end{subfigure}
    \caption{(a) Illustration of the studied problem setup. (b-c) Comparison of DivideMix and~\algo+ on  datasets with imbalance ratio  and noise level . (d) We show the test accuracy of NCM classifier under different noise levels by disentangling representation and classifier learning.} \end{figure}


Although the LTL and noisy label problems have been extensively studied in previous literature, it is still poorly explored when the training dataset follows a long-tailed label distribution while contains label noise. We provide a simple visualization of the studied problem in Figure~\ref{fig:problem_setup}. Without considering label noise, we show that LTL methods severely degrade their performance in experiments. To address this problem, a direct approach is to apply methods for learning with noisy labels to LTL. One of the most commonly used approaches for learning with noisy labels is DivideMix~\cite{li2020dividemix}, which uses the small-loss criterion to detect label noise. However, we note that using such approach leads to unsatisfactory results in long-tailed label distribution, as shown in Figure~\ref{fig:dividemix_cifar10} and~\ref{fig:dividemix_cifar100}. Therefore, it remains a challenge to obtain models that can cope with LTL under label noise. 

To achieve performance improvement, it is a natural idea to detect noisy data while accommodating class imbalance. It is known that a classifier trained on long-tailed data yields higher accuracy for head classes but hurts tail classes~\cite{DBLP:conf/iclr/KangXRYGFK20}. Thus, to detect label noise, it is not trustworthy to use predictions and training losses produced by the biased classifier. Another commonly used approach for LTL is the nearest class mean (NCM) classifier that computes class prototypes and performs nearest neighbour search in embedding space~\cite{DBLP:conf/iclr/KangXRYGFK20}. In Figure~\ref{fig:ncm_confusion_matrix}, we test NCM by disentangling the representation and classifier learning. We first train a feature extractor, then compute class prototypes by polluting clean labels with different noise level. It depicts that the estimation of class prototypes is robust to label noise. This observation motivates us to explore the geometric information between examples and their class prototypes as a criterion for noise detection. After acquiring the prototypes, we design a class-independent noise detector by treating examples closed to their corresponding prototypes as clean, while others as noisy. Unlike learning from balanced datasets where noisy data can be removed from training~\cite{pleiss2020identifying}, we claim that each example is significant, especially for tail classes. To this end, we introduce a new soft pseudo-labeling mechanism that uses both label smoothing and label guessing to guide the learning of networks. Thanks to the generality of our proposed noise detection method, we can also interpret noisy examples as unlabeled data and incorporate well-established semi-supervised learning techniques to further improve the generalisation.


Our main contributions are: (i) We study the problem of long-tailed learning under label noise, which is less explored and is a significant step towards real-world applications; (ii) We find that the commonly used small-loss trick fails in long-tailed learning. Thus, we establish a novel prototypical noise detection method that overcomes the limitations of small-loss trick; (iii) We propose a robust framework,~\algo. It realizes noise detection that is immune to label distribution, and compensates the problem of data scarcity for tail classes. 
Our framework can be built on top of semi-supervised learning methods without much extra overhead, leading to an improved approach~\algo+. The proposed methods achieve strong empirical performance on benchmark and real-world datasets.



\section{Related Work}
Our work is closely related to the following directions.

\textbf{Long-Tailed Learning.} Recently, many approaches have been proposed to cope with long-tailed learning. Most extant approaches can be categorized into three types by modifying (i) the inputs to a model by re-balancing the training data~\cite{DBLP:conf/eccv/ShenLH16,DBLP:conf/cvpr/0002MZWGY19,DBLP:conf/cvpr/ZhouCWC20bbn}; (ii) the outputs of a model, for example by post-hoc adjustment of the classifier~\cite{DBLP:conf/iclr/KangXRYGFK20,tang2020tde,menon2021logitadjust}; and (iii) the internals of a model by modifying the loss function~\cite{DBLP:conf/nips/CaoWGAM19ldam,DBLP:conf/nips/ShuXY0ZXM19,jamal2020rethinking,DBLP:conf/nips/RenYSMZYL20}. Each of the above methods are intuitive, and have shown strong empirical performance. However, these methods assume the training examples are correctly-labeled, which is often difficult to obtain in many real-world applications. Instead, we study a realistic problem to learn from long-tailed data under label noise. Although the presence of label noise in class-imbalanced dataset has also been mentioned in HAR~\cite{cao2021heteroskedastic}, they only consider a specialized noise setup. In this work, we provide a more general simulation of label noise, as well as systematic studies for long-tailed learning methods. More importantly, many existing methods can be easily integrated into our framework, leading to noticeable performance improvement.


\textbf{Label Noise Detection.} Plenty of methods have been proposed to detect noisy examples~\cite{jiang2018mentornet,han2018co,li2020dividemix,Nguyen2020self}. Many works adopt the small-loss trick, which treats examples with small training losses as correctly-labeled. In particular, MentorNet~\cite{jiang2018mentornet} reweights samples with small loss so that noisy samples contribute less to the loss.  Co-teaching~\cite{han2018co} trains two networks where each network selects small-loss samples in a mini-batch to train the other. DivideMix~\cite{li2020dividemix} fits a Gaussian mixture model on per-sample loss distribution to divide the training data into clean set and noisy set. In addition, AUM~\cite{pleiss2020identifying} introduces a margin statistic to identify noisy samples by measuring the average difference between the logit values for a sample's assigned class and its highest non-assigned class. The above methods only consider training datasets that are class-balanced, thus is not applicable for long-tailed label distribution. Recent work~\cite{li2021mopro} observes the real-world dataset with label noise also has imbalanced number of samples per-class. Nevertheless, they only inspect a particular setup, while we provide a systematic study of learning with noisy labels under various long-tailed scenarios. In contrast to previous works, we propose a class-independent prototypical noise detection method that works well in long-tailed learning.



 
\section{Robust Long-Tailed Learning under Label Noise}\label{sec:method}
In this section, we first introduce the problem setting. Then, we present our method for long-tailed learning under label noise. 

\subsection{Problem Formulation}
Given a training dataset , where  is an instance feature vector and  is the class label assigned to it. We assume that training examples  consists of two types: i) a \textit{correctly-labeled example} whose assigned label matches the ground-truth label, i.e., , where  denotes the ground-truth label of , ii) a \textit{mislabeled example} whose assigned label does not match the ground-truth label, but the input matches one of the classes in , i.e.,  and . The setting of long-tailed learning is where the class prior distribution  is highly skewed, so that many tail labels have a very low probability of occurrence. Specifically, we define the imbalance ratio (IR) as .


In practice, since the data distribution is not known, Empirical Risk Minimization (ERM) uses the training data to achieve an empirical estimate of the data distribution. Typically, one minimizes the softmax cross-entropy as

where  denotes the predictive probability of model  on class . This ubiquitous approach neglects the issue of class imbalance, and makes the model biased toward head classes. Moreover, it assumes training examples are correctly-labeled. In the following, we adapt the ERM to address these problems without introducing much extra training efforts.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.236\textwidth}
        \centering
        \includegraphics[width=\linewidth]{loss_ce_class_0.pdf}
        \caption{ Loss for head class}
        \label{fig:loss_class_0}
    \end{subfigure}
    \begin{subfigure}[b]{0.231\textwidth}
        \centering
        \includegraphics[width=\linewidth]{loss_ce_class_7.pdf}
        \caption{ Loss for tail class}
        \label{fig:loss_class_7}
    \end{subfigure}
    \begin{subfigure}[b]{0.256\textwidth}
        \centering
        \includegraphics[width=\linewidth]{distance_class_0.pdf} 
        \caption{Distance for head class}\label{fig:distance_class_0}
    \end{subfigure}
    \begin{subfigure}[b]{0.256\textwidth}
        \centering
        \includegraphics[width=\linewidth]{distance_class_7.pdf} 
        \caption{Distance for tail class}\label{fig:distance_class_7}
    \end{subfigure}
    \caption{(a-b) Training losses for examples of head class and tail class, respectively. (c-d) Distance distribution between examples and their class prototype for head class and tail class, respectively.}
\end{figure}

\subsection{Class-Independent Prototypical Noise Detection}
A popular method for noise detection is the small-loss trick, however, we find that this commonly used method does not fit well with long-tailed learning. The reason is that the training loss of an example can also be large because it belongs to the tail classes and the small-loss trick is not able to distinguish mislabeled examples from tail classes examples, as shown in Figure~\ref{fig:loss_class_0} and~\ref{fig:loss_class_7}. 
In contrast, we find that the estimate of class prototypes is robust to label noise. More importantly, clean examples tend to be clustered around their prototypes even when training with noisy labels. As a comparison with the small-loss trick, we demonstrate the distance distribution for both head and tail classes in Figure~\ref{fig:distance_class_0} and~\ref{fig:distance_class_7}, which motivates us to detect noisy data using class prototypes. 

Considering the discrepancy of data distribution of each class, we inspect the distance statistics in a class-independent manner. Formally, we model clean examples of class  as if they were distributed around prototype , and the likelihood of an example  belonging to class  decays exponentially with its distance from the prototype , i.e., , which is a common assumption about the data distribution~\cite{DBLP:conf/nips/GoldbergerRHS04,DBLP:journals/corr/abs-2104-03066}. Here,  is a distance measure in the embedding space and is typically set to be the Euclidean distance. 
To separate clean examples from noisy data, we assume that, for training examples of class , the distance statistics follow a mixture of two Gaussians~\cite{DBLP:journals/pr/PermuterFJ06}, i.e., , where  and  denotes weight of the -th component. Note that we have . Without loss of generality, we assume . Since clean examples locate around the prototype while noisy examples spread out, we flag  as clean if and only if .
We thus perform class-independent noise detection by estimating the Gaussians' parameters from distance statistics.
In particular, for each class , we compute its prototype as the normalized average of the embeddings for training examples by

where  denotes the extracted feature representation of . Based on , the distances between  and examples of class  are obtained by

We then fit a two-component Gaussian mixture model to maximize the log-likelihood value by , where  for .

For simplicity, we denote the clean (noisy) data of class  as  (). Note that we have . Therefore, we obtain a subset of clean examples by  and noisy examples by . 
It is also verified that Gaussian mixture model can be used to distinguish clean and noisy data because of its flexibility in the sharpness of distribution in previous literature~\cite{li2020dividemix}.
Recall that  may contain noisy labels, the estimate of  in~\eqref{equ:compute-prototype} is inaccurate and the split of  is problematic. To remedy this, we refine class prototypes using  rather than , and acquire a new split of . By doing this, we believe that the obtained  retains most of correctly-labeled examples of class  as well as less mislabeled examples. 



\begin{algorithm}[htbp]
\DontPrintSemicolon

    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
    \textbf{Input:} training dataset , initial learning rate , number of warm-up iterations   \;
\tcp{Warm-up Stage: run SGD for  iterations}
    \For{}
    {
        Sample  examples  from \;
        , where  \;
    }
    \tcp{Robust Learning Stage: run SGD for  iterations}
    \For{}
    {
         \;
        \For{}
        {
            Compute class prototype  as in \eqref{equ:compute-prototype}\;
            Compute distance between the prototype  and each of  as in \eqref{equ:distance}\;
            Fit GMM and divide  into clean set  and noisy set \;
             \tcp*{collect clean and noisy examples of class }
            Refine class prototype  \tcp*{In practice, class prototype computed from  is more accurate than that from }
        }
        Compute soft pseudo-labels  for  as in \eqref{equ:soft-label} \;
        Compute stochastic gradient  as \;
        Update model parameters using  and learning rate \;
    }
    \caption{Robust Long-Tailed Learning under Label Noise (\algo)}\label{alg:pseudo-code}
\end{algorithm}


\subsection{Soft Pseudo-Labeling via Label Smoothing and Label Guessing}
For each noisy example, we aims to refine its training label by generating a soft pseudo-label. A direct approach is to leverage the prediction of ERM model. However, the ERM is known to be biased toward head classes~\cite{DBLP:journals/corr/abs-2104-00466}. Hence, refining noisy labels using the predictions of ERM may be sub-optimal for examples of tail classes. In contrast, the NCM classifier can yield balanced classification boundary~\cite{DBLP:conf/iclr/KangXRYGFK20}. Specifically, we find that the NCM classifier produces much higher recall on tail classes than the ERM in experiments. By aggregating the predictive information from the ERM and NCM classifiers, we construct diverse soft pseudo-labels for detected noisy examples. To amend the misflag of noisy detector, we also take account of the original labels as a source of soft pseudo-labels. Moreover, since it is not impossible that both ERM and NCM classifiers produce incorrect predictions, we further remedy this by the label smoothing technique~\cite{DBLP:journals/corr/abs-2104-00466}.

Put together, given the predictions , , and original label , we form the guessing label set  and generate soft pseudo-label  of  as follows. For class , we compute
0.2cm]
\frac{1-\sum_{\hat{y} \in \mathcal{G}} \mathbb{P}(\hat{y} = y^*)}{K-|\mathcal{G}|} & \text { otherwise. }
\end{array}\right.

\boldsymbol q_{i}(t)=\alpha \boldsymbol q_{i}(t-1)+(1-\alpha) \boldsymbol{z}_{i}(t),

    \LL_{\XX} = \frac{1}{|\XX|} \sum_{i \in \XX} H (\y_i, f(\x_i)),

    \LL_{\SS} = \frac{1}{|\SS|} \sum_{i \in \SS} H (\tilde{\y}_i, f(\x_i)),
\label{equ:transition-matrix}
    T_{ij}(X) = \mathbb{P}(\bar{Y}=j \mid Y=i, X=x) = \left\{\begin{array}{ll}
1 - \gamma  & i = j \\
\frac{N_j}{N - N_i} \gamma & \text { otherwise. }
\end{array}\right.

Here,  denotes the total number of training examples and  is frequency of class . In contrast to commonly used uniform label noise, we believe that examples are more likely to be mislabeled as frequent ones in real-world situations.



\textbf{Result.} Table~\ref{tab:cifar} and Table~\ref{tab:dividemix} summarize the results for CIFAR-10 and CIFAR-100. We compare our methods with several commonly used baselines for long-tailed learning and learning with noisy labels. As shown in the results, previous methods dreadfully degrade their performance as the noise level and imbalance ratio increase, while our methods retain robust performance. In particular, compared with ERM,~\algo~improves the test accuracy by 8\% on average. It can be observed that the improvement becomes more significant at high noise levels, benefiting from proposed noise detection and soft pseudo-labeling. Further application of Deferred Re-Weighting (DRW)~\cite{DBLP:conf/nips/CaoWGAM19ldam} enhances the performance by favoring the tail classes. Note that, ERM-DRW achieves even lower accuracy than LDAM~\cite{DBLP:conf/nips/CaoWGAM19ldam} in many cases, while~\algo-DRW outperforms LDAM-DRW by 5\% on average. This clearly demonstrates the importance of correcting noisy labels in the training data. Intriguingly, our experiments reveal that NCM~\cite{DBLP:conf/iclr/KangXRYGFK20}, which is mostly overlooked in previous literature on long-tailed learning, performs better than cRT~\cite{DBLP:conf/iclr/KangXRYGFK20} in most cases, especially in scenarios with high noise levels. This also provides evidence 
for us to develop geometry-based noise detection method.

We further compare~\algo+ with DivideMix~\cite{li2020dividemix}, one of the most popular methods for learning with noisy labels. We use the same experimental setups for these two methods. The results are given in Table~\ref{tab:dividemix}. It can be observed that DivideMix performs worse as the training dataset becomes more class-imbalanced. In contrast, our method~\algo+ achieves an improvement in test accuracy by 3\% on average. This validates the superiority of our class-independent prototypical noise detector over the small-loss trick. In the supplementary material, we further show that DivideMix flags most example of tail classes as noisy, which is the main reason accounting for its failure.

\setlength{\tabcolsep}{6pt}
\begin{table}[h]
\small
\begin{center}
\centering
\begin{tabular}{ l | c c c c c | c c c c c  }
\toprule
\belowrulesepcolor{Gray} 
\rowcolor{Gray}
\multicolumn{11}{c}{CIFAR-10} \\
\aboverulesepcolor{Gray}

\midrule
Imbalance Ratio  & \multicolumn{5}{ c| }{10} & \multicolumn{5}{ c }{100} \\
\midrule
Noise Level  & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
\midrule
ERM  & 80.41 & 75.61 & 71.94 & 70.13 & 63.25  & 64.41 & 62.17 & 52.94 & 48.11 & 38.71 \\
ERM-DRW  & 81.72 & 77.61 & 71.94 & 70.13 & 63.25  & 66.74 & 62.17 & 52.94 & 48.11 & 38.71 \\
LDAM  & 84.59 & 82.37 & 77.48 & 71.41 & 60.30  & 71.46 & 66.26 & 58.34 & 46.64 & 36.66 \\
LDAM-DRW  & 85.94 & 83.73 & 80.20 & 74.87 & 67.93  &\bf 76.58 & 72.28 & 66.68 & 57.51 & 43.23 \\
BBN  & 83.59 & 80.35 & 72.94 & 70.04 & 63.63  & 70.05 & 64.51 & 56.86 & 44.30 & 36.72 \\
cRT  & 80.22 & 76.15 & 74.17 & 70.05 & 64.15  & 61.54 & 59.92 & 54.05 & 50.12 & 36.73 \\
NCM  & 82.33 & 74.73 & 74.76 & 68.43 & 64.82  & 68.09 & 66.25 & 60.91 & 55.47 & 42.61 \\
HAR-DRW  & 84.09 & 82.43 & 80.41 & 77.43 & 67.39  & 70.81 & 67.88 & 48.59 & 54.23 & 42.80 \\
\midrule
\textbf{\algo}  & 85.68 & 85.43 & 83.50 & 80.92 & 78.96  & 73.02 & 71.20 & 66.53 & 57.86 & 48.98 \\
\textbf{\algo-DRW}  &\bf 86.24 &\bf 85.49 &\bf 84.11 &\bf 81.99 &\bf 80.05  & 76.22 &\bf 74.92 &\bf 71.08 &\bf 63.61 &\bf 55.06 \\
\specialrule{1pt}{2pt}{3pt}
\belowrulesepcolor{Gray} 
\rowcolor{Gray}
\multicolumn{11}{c}{CIFAR-100} \\
\aboverulesepcolor{Gray}
\midrule
Imbalance Ratio  & \multicolumn{5}{ c| }{10} & \multicolumn{5}{ c }{100} \\
\midrule
Noise Level  &    0.1  &    0.2  &    0.3  &    0.4  &    0.5   &    0.1  &    0.2  &    0.3  &    0.4  &    0.5  \\
\midrule
ERM  &  48.65  &  43.27  &  37.43  &  32.94  &  26.92   &  31.81  &  26.21  &  21.79  &  17.91  &  14.23  \\
ERM-DRW  &  50.38  &  45.24  &  39.02  &  34.78  &  28.50   &  34.49  &  28.67  &  23.84  &  19.47  &  14.76  \\
LDAM  &  51.77  &  48.14  &  43.27  &  36.66  &  29.62   &  34.77  &  29.70  &  25.04  &  19.72  &  14.19  \\
LDAM-DRW  &  54.01  &  50.44  &  45.11  &  39.35  &  32.24   &  37.24  &  32.27  &  27.55  &  21.22  &  15.21  \\
BBN  &  53.50  &  47.91  &  42.81  &  35.17  &  28.60   &  34.39  &  27.84  &  23.38  &  18.20  &  15.47  \\
cRT  &  49.13  &  42.56  &  37.80  &  32.18  &  25.55   &  32.25  &  26.31  &  21.48  &  20.62  &  16.01  \\
NCM  &  50.76  &  45.15  &  41.31  &  35.41  &  29.34   &  34.89  &  29.45  &  24.74  &  21.84  &  16.77  \\
HAR-DRW  & 51.04 & 46.24 & 41.23 & 37.35 & 31.30  & 33.21 & 26.29 & 22.57 & 18.98 & 14.78 \\
\midrule
\textbf{\algo} & 54.11 & 51.00 & 47.42 & 44.63 & 38.64  & 35.21 & 30.97 & 27.60 & 24.73 & 20.14 \\
\textbf{\algo-DRW}  &\bf 55.37 &\bf 52.41 &\bf 49.31 &\bf 46.34 &\bf 40.88  &\bf 37.60 &\bf 32.68 &\bf 30.22 &\bf 26.58 &\bf 21.05 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Test accuracy (\%) on CIFAR datasets with different imbalanced ratio and noise level. }\label{tab:cifar}
\end{table}

\vspace{-0.4cm}

\setlength{\tabcolsep}{3.3pt}
\begin{table}[!h]
\small
\begin{center}
\centering
\begin{tabular}{ l c | c c c | c c c | c c c | c c c }
\toprule
 &  & \multicolumn{6}{ c| }{CIFAR-10} & \multicolumn{6}{ c }{CIFAR-100} \\
\midrule
\multicolumn{2}{ l| }{Noise Level} & \multicolumn{3}{ c| }{0.2} & \multicolumn{3}{ c| }{0.5} & \multicolumn{3}{ c| }{0.2} & \multicolumn{3}{ c }{0.5}\\
\midrule
\multicolumn{2}{ l| }{Imbalance Ratio} & 10 & 50 & 100  & 10 & 50 & 100  & 10 & 50 & 100 & 10 & 50 & 100 \\
\midrule
\multirow{2}{*}{DivideMix}  & Best  & 88.80 & \bf77.95 & 71.54  & 87.26 & 69.09 & 53.96  & 63.41 & 48.77 & 43.34  & 47.60 & 34.15 & 29.92 \\
 & Last  & 88.77 & \bf77.95 & 69.90  & \bf87.04 & 68.17 & 53.25  & 62.44 & 48.29 & 42.80  & 47.23 & 33.25 & 29.59 \\
\midrule
\multirow{2}{*}{\textbf{\algo+}}  & Best  & \bf89.09 & 77.93 &\bf 72.75  & \bf87.51 &\bf 75.57 &\bf 65.72  &\bf 64.24 &\bf 49.90 &\bf 44.39  &\bf 52.27 &\bf 38.96 &\bf 32.50 \\
 & Last  & \bf88.91 & 76.75 &\bf 72.16  & 86.71 &\bf 74.93 &\bf 64.65  &\bf 63.86 &\bf 48.85 &\bf 43.98  &\bf 51.77 &\bf 37.92 &\bf 31.89 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Test accuracy (\%) on CIFAR datasets with different imbalanced ratio and noise level. }\label{tab:dividemix}
\vspace{-0.4cm}
\end{table}


\subsection{Evaluation on Real-World Class-Imbalanced and Noisy Dataset}
We test the performance of our method on a real-world dataset. WebVision~\cite{webvision} contains 2.4 million images collected from Flickr and Google with real noisy and class-imbalanced data. Following previous literature, we train on a subset, mini WebVision, which contains the first 50 classes. In Table~\ref{exp:webvision}, we report results comparing against state-of-the-art approaches, including D2L~\cite{ma2018dimensionality}, MentorNet~\cite{jiang2018mentornet}, Co-teaching~\cite{han2018co}, Iterative-CV~\cite{chen2019understanding}, HAR~\cite{cao2021heteroskedastic}, and DivideMix~\cite{li2020dividemix}. 

\begin{table}[htbp]
\centering
\begin{center}
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\toprule
& & D2L & MentorNet & Co-teaching & Iterative-CV & HAR & DivideMix & \textbf{\algo+} \\
\midrule
\multirow{2}{*}{Webvision} & top1 & 62.68 & 63.00 & 63.58 & 65.24 & 75.5 & 77.32 & \textbf{77.64} \\
                           & top5 & 84.00 & 81.40 & 85.20 & 85.34 & 90.7 & 91.64 & \textbf{92.44}\\
\hline
\rule{0pt}{2.2ex} \multirow{2}{*}{ImageNet} & top1 & 57.80 & 57.80 & 61.48 & 61.60 & 70.3 & \textbf{75.20} & 74.64\\
                           & top5 & 81.36 & 79.92 & 84.70 & 84.98 & 90.0 & 90.84 & \textbf{92.48}\\
\bottomrule
\end{tabular}
\end{center}
\caption{Accuracy (\%) on mini WebVision and ImageNet validation sets.}\label{exp:webvision}
\end{table}



\vspace{-0.55cm}
\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ce_recall_cifar10.pdf}
        \caption{ERM on CIFAR-10}
        \label{fig:ce_recall_cifar10}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ncm_recall_cifar10.pdf} 
        \caption{NCM on CIFAR-10}\label{fig:ncm_recall_cifar10}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ce_recall_cifar100.pdf} 
        \caption{ERM on CIFAR-100}\label{fig:ce_recall_cifar100}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ncm_recall_cifar100.pdf} 
        \caption{NCM on CIFAR-100}\label{fig:ncm_recall_cifar100}
    \end{subfigure}
    \caption{Per-class recall of ERM and NCM classifiers on CIFAR-10 and CIFAR-100 datasets. It can be clearly seen that NCM produces more balanced predictions than ERM across classes.}
    \label{fig:recall-ce-ncm}
\end{figure*}


\vspace{-0.25cm}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{gmm_legend.pdf}
    
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gmm_precision_cifar10.pdf}
        \caption{CIFAR-10 Precision}
        \label{fig:gmm_precision_cifar10}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gmm_recall_cifar10.pdf} 
        \caption{CIFAR-10 Recall}\label{fig:gmm_recall_cifar10}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gmm_precision_cifar100.pdf} 
        \caption{CIFAR-100 Precision}\label{fig:gmm_precision_cifar100}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gmm_recall_cifar100.pdf} 
        \caption{CIFAR-100 Recall}\label{fig:gmm_recall_cifar100}
    \end{subfigure}
    \caption{Precision and Recall of selected clean examples by our method.}
    \label{fig:gmm-precision-recall}
\end{figure*}
\vspace{-0.2cm}




\subsection{Further Analysis and Ablation Studies}
We study the effectiveness of the two main modules of our method.

\textbf{Efficacy of the noise detector.}
To further support our motivation, we compare the performance of the ERM and NCM classifiers in Figure~\ref{fig:recall-ce-ncm}. It can be seen that NCM produces more balanced recall across classes, while ERM tends to predict examples as head classes, resulting in low recall for tail classes. Figure~\ref{fig:gmm-precision-recall} shows the precision and recall of selected clean examples by our method. To better understand~\algo, we construct three groups of classes for CIFAR-100 by: many (more than 100 images), medium (20100 images), and few (less than 20 images) shots; and CIFAR-10 by: many (), medium(), and few () shots according to class indices.~\algo~maintains high precision and recall, which validates the effectiveness of our method. This experiment is conducted under imbalance ratio  and noise level .

\textbf{Efficacy of the soft pseudo-labeling.}
We investigate the effectiveness of soft pseudo-labeling by comparing it with two other methods, i.e., keep the noisy labels or rectify it via the ERM predictions. We report the results in Table~\ref{tab:pseudo-label} with respect to noise level  and imbalance ratio . We observe that ERM and soft pseudo-labeling significantly improve the performance by over 4\% in test accuracy, and the improvement is more significant under high noise levels. Moreover, the soft pseudo-labeling outperforms its ERM counterpart in most cases, demonstrating that label smoothing and label guessing can provide diverse and informative supervision under imperfect training labels. We also investigate the effectiveness of learned representations with NCM for classification. It can be observed that NCM with soft labels outperforms the one using original noisy labels, which confirms that our soft pseudo-labeling facilitates representation learning.

\setlength{\tabcolsep}{3.5pt}
\begin{table}[ht]
\begin{center}
\centering
\begin{tabular}{ c | c | c | c c c c | c c c c }
\toprule
\multirow{2}{*}{DRW} & \multirow{2}{*}{Classifier} & \multirow{2}{*}{ Pseudo-Label }  & \multicolumn{4}{ c| }{} & \multicolumn{4}{ c }{} \\
 &  &  & Many & Medium & Few & All  & Many & Medium & Few & All  \\
\midrule
\xmark & Linear & Noisy Label  & 49.38 & 21.42 & 4.57 & 26.21  & 32.06 & 7.89 & 0.04 & 14.23 \\
\xmark & Linear & ERM  & 58.79 & 26.50 & 4.21 &\bf 31.24  & 38.83 & 12.05 & 0.89 & 18.41 \\
\xmark & Linear & Soft Label   & 56.79 & 26.47 & 5.71 & 30.97  & 39.09 & 16.13 & 1.22 &\bf 20.14 \\
\hline
\rule{0pt}{2.2ex} \xmark & NCM & Noisy Label  & 44.09 & 32.03 & 12.00 & 30.52  & 26.86 & 17.89 & 5.59 & 17.71 \\
\xmark & NCM & ERM  & 49.06 & 34.92 & 13.07 &\bf 33.61  & 31.11 & 21.05 & 5.63 & 20.41 \\
\xmark & NCM & Soft Label  & 48.03 & 32.18 & 14.25 & 32.55  & 31.46 & 21.66 & 5.59 &\bf 20.75 \\
\hline
\rule{0pt}{2.2ex}\cmark & Linear & Noisy Label   & 45.82 & 26.50 & 10.79 & 28.67  & 23.77 & 14.53 & 3.41 & 14.76 \\
\cmark & Linear & ERM  & 50.62 & 31.55 & 11.64 & 32.46  & 32.80 & 17.05 & 2.30 & 18.58 \\
\cmark & Linear & Soft Label  & 49.50 & 31.11 & 14.39 &\bf 32.68  & 32.14 & 21.16 & 6.52 &\bf 21.05 \\
\hline
\rule{0pt}{2.2ex}\cmark & NCM & Noisy Label   & 43.21 & 31.95 & 12.61 & 30.36  & 26.86 & 17.89 & 5.59 & 17.71 \\
\cmark & NCM & ERM  & 43.53 & 33.21 & 11.07 & 30.52  & 26.83 & 19.45 & 5.52 & 18.27 \\
\cmark & NCM & Soft Label  & 46.79 & 31.61 & 13.79 &\bf 31.78  & 29.86 & 21.05 & 6.26 &\bf 20.14 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Ablation studies on pseudo-labeling. Test accuracy on CIFAR-100 is reported.}\label{tab:pseudo-label}
\vspace{-0.4cm}
\end{table}



\subsection{Discussion and Limitations}\label{sec:discussion}
One may be interested in combining the proposed method~\algo~with other loss functions. In particular, we attempt to optimize LDAM loss~\cite{DBLP:conf/nips/CaoWGAM19ldam} during training and the results are reported in the supplementary material. Indeed, LDAM encourages the model to yield balanced classification boundaries. However, it slightly distort these boundaries when applied together with soft pseudo-labeling because too much focus has been put on tail classes. Our experimental finding suggests using the ERM predictions as pseudo-labels leading to more significant improvements.

Additionally, we admit that it is challenging to train networks that consistently performs well under various noise levels in long-tailed learning. Although~\algo~can take both label noise and class imbalance into account, its improvement is less obvious in the event of training on a clean dataset. We report the results in the supplementary material due to limited space. This is because that the noise detector inevitably fits a two-component GMM and flags some examples as noisy, leading to loss of accurate supervision. We believe this concern can be alleviated by estimating the noise proportion in training data, which is another interesting research problem, and leave this for future work.


\section{Conclusion}
We study the long-tailed learning under label noise and a robust framework is proposed to tackle this challenging problem. We reveal the failure of small-loss trick in long-tailed learning, and establish a prototypical noise detection method that is immune to label distribution. We provide systematic studies on benchmark and real-world datasets to verify the superiority of our methods by comparing to state-of-the-art methods in the strands of long-tailed learning and learning with noisy labels. 

\section*{Broader Impact}
This paper introduces a method to learning from noisy and long-tailed data. It can benefit the widespread use of ``weakly-labeled'' data~\cite{webvision,li2020dividemix,li2021mopro}, which are often cheap to acquire but have suffered from data quality issues. The proposed method is simple yet effective, which we believe will broadly benefit practitioners dealing with heavily imbalanced data in realistic applications.

In this work, we only extensively test our strategies on benchmark datasets. In many
real-world applications such as autonomous driving, medical diagnosis, and healthcare, beyond
being naturally noisy and imbalanced, the data may impose additional constraints on learning process and final
models, e.g., being fair or private. We focus on standard accuracy as our measure and largely ignore
other ethical issues in imbalanced data, especially in minor classes. As such, the risk of producing
unfair or biased outputs reminds us to carry rigorous validations in critical, high-stakes applications.

\bibliographystyle{unsrt}
\bibliography{reference}

\newpage
\appendix

\section{Implementation Details}
We develop our core algorithm in PyTorch.

\textbf{Implementation details for CIFAR.} We follow the simple data augmentation used in~\cite{he2016deep} with only random crop and horizontal flip. For experiments of~\algo, we use ResNet-32 as the backbone network and train it using standard SGD with a momentum of 0.9, a weight decay of , a batch size of 128, and an initial learning rate of 0.1. The model is trained for 200 epochs. We perform noise detection and soft pseudo-labeling after a warm up period of 80 epochs, and anneal the learning rate by a factor of 100 at 160 and 180 epochs. For soft pseudo-labeling, we manually set the prior  to be  for the ERM, NCM, and original label. For experiments of~\algo+, we use the same settings as~\cite{li2020dividemix}, which trains two 18-layer PreAct Resnet for 300 epochs. We train each model with 1 NVIDIA GeForce RTX 2070.


\textbf{Implementation details for mini WebVision.} Following previous work~\cite{li2020dividemix}, we use two Inception-Resnet V2 for~\algo+. The model is trained for 100 epochs. We set the initial learning rate as 0.01, and reduce it by a factor of 10 after 50 epochs. The warm up period is 40 epochs. We train each model with 1 NVIDIA Tesla V100 GPUs.

\section{Additional Experimental Results}

\subsection{Comparison with DivideMix with respect to Noise Detection}
To further demonstrate our proposed noise detection that is tailored for long-tailed learning, we compare it with DivideMix and the results are shown in Figure~\ref{fig:detection_acc_cifar10} and~\ref{fig:detection_acc_cifar100}. This experiment is conducted under imbalance ratio  and noise level . We partition classes into three splits, i.e., Many, Medium, and Few-shots, and report the accuracy (recall) of examples that are flagged as clean for each split. It can be observed that DivideMix only flags a small proportion of examples as clean on tail classes, even fewer than those in original data (the green dotted line in figures, about 70\%). This also explains that, DivideMix trains networks that are biased towards head classes, thus leading to poor overall performance. In contrast, \algo+ correctly flags more clean examples of tail classes than DivideMix, demonstrating the superiority of our prototypical noise detection method.


\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{detection_acc_cifar10_many.pdf}
        \caption{CIFAR-10 Many Shot}
        \label{fig:detection_acc_cifar10_many}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{detection_acc_cifar10_medium.pdf} 
        \caption{CIFAR-10 Medium Shot}
        \label{fig:detection_acc_cifar10_medium}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{detection_acc_cifar10_few.pdf} 
        \caption{CIFAR-10 Few Shot}
        \label{fig:detection_acc_cifar10_few}
    \end{subfigure}
    \caption{Comparison of detection accuracy between~\algo+~and DivideMix on CIFAR-10 dataset.}
    \label{fig:detection_acc_cifar10}
\end{figure}


\begin{figure}[!h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{detection_acc_cifar100_many.pdf}
        \caption{CIFAR-100 Many Shot}
        \label{fig:detection_acc_cifar100_many}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{detection_acc_cifar100_medium.pdf} 
        \caption{CIFAR-100 Medium Shot}
        \label{fig:detection_acc_cifar100_medium}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{detection_acc_cifar100_few.pdf} 
        \caption{CIFAR-100 Few Shot}
        \label{fig:detection_acc_cifar100_few}
    \end{subfigure}
    \caption{Comparison of detection accuracy between~\algo+~and DivideMix on CIFAR-100 dataset.}
    \label{fig:detection_acc_cifar100}
\end{figure}


\subsection{Comparison with DivideMix on Balanced Datasets}
We compare the performance of our method with DivideMix on balanced datasets with noise level . The results are reported in Table~\ref{tab:cifar_balanced} and our method is comparable with DivideMix. This shows that the proposed prototypical noise detector also works well on balanced datasets.

\setlength{\tabcolsep}{6pt}
\begin{table}[htbp]
\small
\begin{center}
\centering
\begin{tabular}{ l c | c | c | c | c }
\toprule
 &  & \multicolumn{2}{ c| }{CIFAR-10} & \multicolumn{2}{ c }{CIFAR-100} \\
\midrule
\multicolumn{2}{ l| }{Noise Level} & 0.2 & 0.5  & 0.2 & 0.5  \\
\midrule
\multirow{2}{*}{DivideMix}  & Best  & 97.01 & \bf 97.07  & \bf 81.40 & 80.16 \\
 & Last  & 96.78 & 96.89  & 80.61 & 79.79 \\
\midrule
\multirow{2}{*}{\textbf{\algo+}}  & Best  & \bf 97.14 & 97.03  & 81.07 & \bf 80.50 \\
 & Last  & 96.93 & 96.64  & 80.16 & 79.80 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Test accuracy (\%) on class-balanced CIFAR datasets with different noise level. }\label{tab:cifar_balanced}
\end{table}



\subsection{Additional Results on CIFAR Datasets}
We report the results on CIFAR-10 and CIFAR-100 with simulated imbalance ratio  with noise level  in Table~\ref{tab:cifar_rho_50}. The performance of comparison methods is in line with that of  and  which are reported in the main text. This further justifies that our method can adapt to various class-imbalanced and noisy datasets.


\setlength{\tabcolsep}{6pt}
\begin{table}[!h]
\small
\begin{center}
\centering
\begin{tabular}{ l | c c c c c | c c c c c }
\toprule
 & \multicolumn{5}{ c| }{CIFAR-10} & \multicolumn{5}{ c }{CIFAR-100} \\
\midrule
Noise Level  & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
\midrule
ERM  & 69.33 & 63.60 & 58.69 & 55.85 & 43.38  & 35.10 & 30.27 & 25.11 & 19.49 & 16.97  \\
ERM-DRW  & 71.99 & 65.76 & 58.69 & 55.85 & 43.38  & 37.74 & 32.63 & 27.19 & 21.43 & 17.52  \\
LDAM  & 75.06 & 71.34 & 64.71 & 54.42 & 42.95  & 39.94 & 33.43 & 30.01 & 23.30 & 17.51  \\
LDAM-DRW  & 79.01 & 76.41 & 71.83 & 62.22 & 48.88  &\bf 42.88 & 36.60 & 33.12 & 25.91 & 19.48  \\
BBN  & 72.86 & 68.01 & 60.49 & 52.89 & 46.22  & 39.40 & 36.48 & 26.89 & 21.08 & 16.77  \\
cRT  & 69.22 & 65.02 & 60.64 & 51.90 & 43.26  & 35.70 & 30.23 & 24.37 & 19.90 & 17.47  \\
NCM  & 72.37 & 69.60 & 65.26 & 56.78 & 49.68  & 38.91 & 33.49 & 28.85 & 23.91 & 19.01  \\
HAR-DRW  & 72.12 & 67.44 & 60.73 & 63.04 & 52.35  & 38.46 & 28.86 & 29.33 & 22.06 & 16.75  \\
\midrule
\textbf{\algo}  & 77.49 & 74.91 & 72.43 & 64.37 & 49.32  & 38.94 & 35.75 & 32.55 & 27.62 & 25.01  \\
\textbf{\algo-DRW}  &\bf 80.46 &\bf 78.31 &\bf 76.36 &\bf 69.64 &\bf 54.34  & 40.85 &\bf 38.10 &\bf 34.75 &\bf 29.24 &\bf 26.17  \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Test accuracy (\%) on CIFAR datasets with imbalance ratio  and different noise level. }\label{tab:cifar_rho_50}
\end{table}


\subsection{Results on Clean CIFAR Datasets}
Although our method is particularly designed for long-tailed learning with noisy labels, it is interesting to study its performance on clean datasets. We report the results in Table~\ref{tab:cifar_clean}. Intriguingly,~\algo~consistently outperforms vanilla ERM in all cases, showing the benefit of the proposed soft pseudo-labeling approach. Additionally, our method achieves comparable performance with the popular baseline LDAM-DRW. In comparison with the HAR-DRW, which is also proposed to cope with class imbalance and label noise problems, our method improves the performance by over 2\% on average. This validates the robustness of our method, which does not hurt the performance in the corner case.

\setlength{\tabcolsep}{6pt}
\begin{table}[!h]
\small
\begin{center}
\centering
\begin{tabular}{ l | c c c | c c c }
\toprule
 & \multicolumn{3}{ c| }{CIFAR-10} & \multicolumn{3}{ c }{CIFAR-100} \\
\midrule
Imbalance Ratio  & 10 & 50 & 100  & 10 & 50 & 100 \\
\midrule
ERM  & 86.75 & 77.38 & 71.83  & 56.31 & 44.15 & 38.88  \\
ERM-DRW  & 87.71 & 80.58 & 76.33  & 57.68 & 46.71 & 41.90  \\
LDAM  & 86.38 & 77.62 & 74.31  & 55.66 & 43.61 & 39.25  \\
LDAM-DRW  & 87.29 & 81.25 & 78.78  & 57.21 & \bf 47.30 & \bf 42.93  \\
BBN  & 87.83 & 81.19 & 78.87  & 58.08 & 45.62 & 40.09  \\
cRT  & 86.78 & 77.30 & 71.18  & 56.62 & 43.01 & 39.44  \\
NCM  & \bf 88.14 & \bf 82.75 & 79.59  & 56.05 & 45.13 & 41.73  \\
HAR-DRW  & 87.81 & 79.82 & 75.99  & 56.89 & 43.34 & 40.78  \\
\midrule
\textbf{\algo}  & 87.87 & 79.79 & 76.49  & 57.83 & 44.52 & 40.75  \\
\textbf{\algo-DRW}  & 87.87 & 82.21 & \bf 79.62  & \bf 58.35 & 46.18 & 42.54  \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Test accuracy (\%) on clean CIFAR datasets with different imbalanced ratio. }\label{tab:cifar_clean}
\end{table}



\subsection{Results for Optimizing LDAM Loss}
In the main text, we optimize the cross-entropy loss and report its performance for comparison. One may interested in if other loss functions can be integrated into our framework. To this end, we leverage the LDAM loss, which is particularly designed for long-tailed learning, and report the results in Table~\ref{tab:ablation-ldam}. This indeed produces different results with the cross-entropy. It is known that LDAM can prevent the networks from being biased toward tail classes and yield balanced predictions. Therefore, it is reasonable to use predictions of the ERM for pseudo-labeling. By further applying the soft pseudo-labels, it puts much focus on tail classes and results in performance deterioration.

\setlength{\tabcolsep}{3.5pt}
\begin{table}[!h]
\begin{center}
\centering
\begin{tabular}{ c | c | c | c c c c | c c c c }
\toprule
\multirow{2}{*}{DRW} & \multirow{2}{*}{Classifier} & \multirow{2}{*}{ Pseudo-Label }  & \multicolumn{4}{ c| }{} & \multicolumn{4}{ c }{}  \\
 &  &  & Many & Medium & Few & All  & Many & Medium & Few & All  \\
\midrule
\xmark & Linear & Noisy Label  & 54.06 & 26.53 & 4.43 & 29.70  & 31.03 & 8.42 & 0.48 & 14.19 \\
\xmark & Linear & ERM  & 61.47 & 29.32 & 4.96 & 33.43  & 46.60 & 14.18 & 1.11 &\bf 22.00 \\
\xmark & Linear & Soft Label  & 61.32 & 31.63 & 7.46 &\bf 34.96  & 38.89 & 15.71 & 1.52 & 19.99 \\
\midrule
\xmark & NCM & Noisy Label  & 49.82 & 27.82 & 11.14 & 30.63  & 25.60 & 16.37 & 6.59 & 16.96 \\
\xmark & NCM & ERM  & 58.53 & 30.11 & 12.46 &\bf 34.83  & 42.57 & 16.50 & 4.41 &\bf 22.36 \\
\xmark & NCM & Soft Label  & 56.18 & 30.24 & 10.68 & 33.58  & 28.80 & 15.89 & 5.59 & 17.63 \\
\midrule
\cmark & Linear & Noisy Label  & 49.53 & 30.34 & 13.93 & 32.27  & 24.83 & 13.53 & 5.11 & 15.21 \\
\cmark & Linear & ERM  & 54.41 & 34.00 & 19.61 &\bf 36.91  & 39.34 & 20.08 & 7.04 &\bf 23.30 \\
\cmark & Linear & Soft Label  & 54.15 & 35.84 & 16.79 & 36.73  & 31.80 & 21.00 & 6.93 & 20.98 \\
\midrule
\cmark & NCM & Noisy Label  & 49.50 & 29.45 & 12.00 & 31.38  & 25.60 & 16.37 & 6.59 & 16.96 \\
\cmark & NCM & ERM  & 56.09 & 32.39 & 13.75 &\bf 35.23  & 41.00 & 17.89 & 4.74 &\bf 22.43 \\
\cmark & NCM & Soft Label  & 55.26 & 30.82 & 10.82 & 33.53  & 28.80 & 15.89 & 5.59 & 17.63 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Ablation studies on pseudo-labeling based on models that optimize LDAM loss. Test accuracy on CIFAR-100 dataset with imbalance ratio  is reported. }
\label{tab:ablation-ldam}
\end{table}




\subsection{The Impact of Label Noise on Representation and Classifier Learning}
In Table~\ref{tab:crt-cifar10}\ref{tab:ncm-cifar100}, we study the impact of label noise for two-stage long-tailed learning methods, i.e., Classifier Re-Training (cRT) and Nearest Classifier Mean (NCM), which disentangle the representation and classifier learning. In this setup,  and  are the noise level when performing representation and classifier learning, respectively. 

We have the following observations from the results. In particular, when , the performance of both cRT and NCM drop significantly as  increases, revealing the negative impact of label noise on representation learning. With respect to classifier learning, it can be seen that cRT further suffers from inaccurate supervision. In contrast, NCM classifier retains high performance as  grows. The results validate our finding that NCM is more robust to label noise, which motivates us to investigate distance-based method for noise detection. Moreover, in order to improve the representation learning, one may remove noisy data or rectify noisy labels during training. In this work, we provide two ways of achieving this, by pseudo-labeling using either ERM predictions or soft pseudo-labels. Recall that, NCM computes the classification vectors for each class by taking the mean of all vectors belonging to that
class. Thus, the classification accuracy is directly related to the feature representation quality. By observing considerable performance gains for NCM, it shows the effectiveness of our pseudo-labeling method for representation learning.

\setlength{\tabcolsep}{2pt}
\begin{table}[!h]
\tiny
\begin{center}
\centering
\begin{tabular}{ l | l | c c c c c c | l | l | c c c c c c | l | l | c c c c c c  }
\toprule
\multicolumn{8}{ c| }{} & \multicolumn{8}{ c| }{} & \multicolumn{8}{ c }{} \\
\cmidrule{1-8} \cmidrule{9-16} \cmidrule{17-24} 
\multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c }{} \\
\cmidrule{3-8} \cmidrule{11-16} \cmidrule{19-24} 
\multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
\cmidrule{1-2} \cmidrule{3-8} \cmidrule{9-16} \cmidrule{17-24} 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 93.15 & 92.85 & 92.76 & 92.55 & 92.56 & 92.40  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 86.78 & 85.90 & 85.43 & 85.21 & 83.13 & 81.49  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 71.18 & 68.59 & 66.31 & 65.92 & 61.83 & 57.58 \\
\addlinespace
& 0.1 & 91.43 & 91.37 & 91.36 & 91.31 & 91.33 & 91.41  &
& 0.1 & 81.13 & 80.22 & 78.84 & 77.60 & 77.05 & 75.13  &
& 0.1 & 62.48 & 61.54 & 59.91 & 58.70 & 55.57 & 53.17 \\
 \addlinespace
& 0.2 & 90.40 & 90.42 & 90.31 & 90.33 & 90.35 & 90.24  &
& 0.2 & 76.91 & 76.48 & 76.15 & 75.09 & 75.20 & 73.66  &
& 0.2 & 61.33 & 60.18 & 59.92 & 57.98 & 56.34 & 52.82 \\
 \addlinespace
& 0.3 & 88.74 & 88.80 & 88.77 & 88.58 & 88.73 & 88.54  &
& 0.3 & 75.64 & 74.60 & 74.36 & 74.17 & 72.76 & 71.17  &
& 0.3 & 55.26 & 55.05 & 53.79 & 54.05 & 50.45 & 47.74 \\
 \addlinespace
& 0.4 & 87.00 & 86.91 & 86.82 & 86.89 & 86.85 & 86.75  &
& 0.4 & 72.26 & 71.61 & 70.95 & 69.96 & 70.05 & 67.83  &
& 0.4 & 51.98 & 51.22 & 51.05 & 50.36 & 50.12 & 46.28 \\
 \addlinespace
& 0.5 & 84.57 & 84.53 & 84.46 & 84.38 & 84.29 & 83.95  &
& 0.5 & 67.01 & 67.04 & 66.83 & 64.68 & 64.16 & 64.15  &
& 0.5 & 41.70 & 40.90 & 40.75 & 40.07 & 38.61 & 36.73 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Accuracy (\%) of cRT on CIFAR-10 with different imbalanced ratio  and noise level .}\label{tab:crt-cifar10}
\end{table}


\begin{table}[!h]
\tiny
\begin{center}
\centering
\begin{tabular}{ l | l | c c c c c c | l | l | c c c c c c | l | l | c c c c c c  }
\toprule
\multicolumn{8}{ c| }{} & \multicolumn{8}{ c| }{} & \multicolumn{8}{ c }{} \\
\cmidrule{1-8} \cmidrule{9-16} \cmidrule{17-24} 
\multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c }{} \\
\cmidrule{3-8} \cmidrule{11-16} \cmidrule{19-24} 
\multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
\cmidrule{1-2} \cmidrule{3-8} \cmidrule{9-16} \cmidrule{17-24} 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 92.77 & 92.75 & 92.69 & 92.67 & 92.55 & 92.54  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 88.14 & 88.08 & 87.97 & 87.89 & 87.72 & 87.45  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 79.59 & 79.64 & 79.67 & 79.64 & 79.57 & 78.63 \\
\addlinespace
& 0.1 & 91.29 & 91.29 & 91.28 & 91.31 & 91.25 & 91.24  &
& 0.1 & 82.23 & 82.33 & 82.09 & 82.05 & 81.91 & 81.91  &
& 0.1 & 68.21 & 68.09 & 67.06 & 66.19 & 65.35 & 64.53 \\
 \addlinespace
& 0.2 & 90.20 & 90.24 & 90.23 & 90.26 & 90.31 & 90.24  &
& 0.2 & 75.27 & 75.02 & 74.73 & 74.37 & 73.82 & 73.25  &
& 0.2 & 66.80 & 66.59 & 66.25 & 65.98 & 64.95 & 63.70 \\
 \addlinespace
& 0.3 & 88.51 & 88.51 & 88.48 & 88.55 & 88.53 & 88.53  &
& 0.3 & 74.99 & 75.01 & 74.98 & 74.76 & 74.52 & 74.09  &
& 0.3 & 61.68 & 61.22 & 61.06 & 60.91 & 60.04 & 59.19 \\
 \addlinespace
& 0.4 & 86.77 & 86.80 & 86.78 & 86.80 & 86.79 & 86.76  &
& 0.4 & 70.45 & 69.75 & 69.40 & 69.07 & 68.43 & 67.97  &
& 0.4 & 56.57 & 56.46 & 56.21 & 55.92 & 55.47 & 54.60 \\
 \addlinespace
& 0.5 & 83.78 & 83.78 & 83.78 & 83.77 & 83.79 & 83.77  &
& 0.5 & 66.16 & 65.82 & 65.62 & 65.40 & 65.07 & 64.82  &
& 0.5 & 44.66 & 44.08 & 43.98 & 43.18 & 43.10 & 42.61 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Accuracy (\%) of NCM on CIFAR-10 with different imbalanced ratio  and noise level .}\label{tab:ncm-cifar10}
\end{table}


\begin{table}[!h]
\tiny
\begin{center}
\centering
\begin{tabular}{ l | l | c c c c c c | l | l | c c c c c c | l | l | c c c c c c  }
\toprule
\multicolumn{8}{ c| }{} & \multicolumn{8}{ c| }{} & \multicolumn{8}{ c }{} \\
\cmidrule{1-8} \cmidrule{9-16} \cmidrule{17-24} 
\multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c }{} \\
\cmidrule{3-8} \cmidrule{11-16} \cmidrule{19-24} 
\multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
\cmidrule{1-2} \cmidrule{3-8} \cmidrule{9-16} \cmidrule{17-24} 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 69.73 & 68.90 & 68.09 & 67.49 & 66.61 & 65.70  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 56.62 & 53.55 & 52.21 & 50.52 & 49.09 & 47.34  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 39.44 & 35.43 & 33.93 & 32.34 & 31.32 & 29.68 \\
\addlinespace
& 0.1 & 68.55 & 68.03 & 67.38 & 66.58 & 66.48 & 65.87  &
& 0.1 & 50.55 & 49.13 & 47.89 & 46.23 & 44.55 & 43.36  &
& 0.1 & 33.19 & 32.25 & 30.69 & 28.76 & 27.61 & 25.97 \\
 \addlinespace
& 0.2 & 65.51 & 65.21 & 64.86 & 64.45 & 64.46 & 63.96  &
& 0.2 & 45.31 & 44.22 & 42.56 & 41.73 & 40.27 & 38.61  &
& 0.2 & 27.77 & 27.02 & 26.31 & 24.57 & 23.79 & 22.82 \\
 \addlinespace
& 0.3 & 63.01 & 62.74 & 62.32 & 62.02 & 61.65 & 60.96  &
& 0.3 & 41.72 & 40.82 & 39.77 & 37.80 & 37.84 & 36.38  &
& 0.3 & 24.91 & 23.83 & 23.61 & 21.48 & 21.28 & 19.61 \\
 \addlinespace
& 0.4 & 60.78 & 60.42 & 60.30 & 59.73 & 59.19 & 58.93  &
& 0.4 & 37.33 & 36.76 & 35.31 & 34.46 & 32.18 & 32.68  &
& 0.4 & 23.02 & 22.38 & 22.04 & 21.49 & 20.62 & 19.48 \\
 \addlinespace
& 0.5 & 57.88 & 57.51 & 56.98 & 56.83 & 55.97 & 55.14  &
& 0.5 & 32.07 & 31.09 & 30.29 & 29.90 & 28.58 & 25.55  &
& 0.5 & 19.05 & 18.60 & 17.93 & 17.67 & 16.89 & 16.01 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Accuracy (\%) of cRT on CIFAR-100 with different imbalanced ratio  and noise level .}\label{tab:crt-cifar100}
\end{table}


\begin{table}[!h]
\tiny
\begin{center}
\centering
\begin{tabular}{ l | l | c c c c c c | l | l | c c c c c c | l | l | c c c c c c  }
\toprule
\multicolumn{8}{ c| }{} & \multicolumn{8}{ c| }{} & \multicolumn{8}{ c }{} \\
\cmidrule{1-8} \cmidrule{9-16} \cmidrule{17-24} 
\multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c| }{} & \multicolumn{2}{ c| }{} & \multicolumn{6}{ c }{} \\
\cmidrule{3-8} \cmidrule{11-16} \cmidrule{19-24} 
\multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & \multicolumn{2}{ c| }{} & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\
\cmidrule{1-2} \cmidrule{3-8} \cmidrule{9-16} \cmidrule{17-24} 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 66.94 & 66.71 & 66.37 & 65.79 & 64.84 & 64.07  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 56.05 & 55.63 & 55.22 & 54.14 & 53.18 & 51.78  & 
\multirow{6}{*}[-3ex]{~~~\begin{rotate}{90}\end{rotate}~} & 0  & 41.73 & 41.18 & 40.59 & 39.81 & 38.56 & 37.95 \\
\addlinespace
& 0.1 & 65.72 & 65.84 & 65.66 & 65.27 & 64.83 & 64.16  &
& 0.1 & 50.49 & 50.76 & 50.14 & 49.53 & 49.51 & 48.16  &
& 0.1 & 35.43 & 34.89 & 34.49 & 33.77 & 32.93 & 32.11 \\
 \addlinespace
& 0.2 & 63.08 & 62.92 & 63.26 & 62.61 & 62.67 & 62.15  &
& 0.2 & 45.22 & 45.05 & 45.15 & 44.83 & 44.21 & 43.22  &
& 0.2 & 30.47 & 29.95 & 29.45 & 28.74 & 28.56 & 28.13 \\
 \addlinespace
& 0.3 & 60.82 & 60.64 & 60.62 & 60.81 & 60.29 & 60.16  &
& 0.3 & 41.82 & 41.66 & 41.23 & 41.31 & 40.27 & 39.68  &
& 0.3 & 25.97 & 25.50 & 25.17 & 24.74 & 23.96 & 22.59 \\
 \addlinespace
& 0.4 & 57.87 & 58.00 & 57.81 & 57.82 & 57.91 & 57.55  &
& 0.4 & 36.13 & 36.32 & 36.19 & 35.81 & 35.41 & 34.84  &
& 0.4 & 23.89 & 23.47 & 22.80 & 22.29 & 21.84 & 20.50 \\
 \addlinespace
& 0.5 & 55.24 & 55.25 & 55.05 & 55.01 & 54.64 & 54.95  &
& 0.5 & 30.85 & 30.63 & 30.50 & 30.07 & 29.84 & 29.34  &
& 0.5 & 19.16 & 18.63 & 18.47 & 18.15 & 16.89 & 16.77 \\
\bottomrule
\end{tabular}
\end{center}
\caption{ Accuracy (\%) of NCM on CIFAR-100 with different imbalanced ratio  and noise level .}\label{tab:ncm-cifar100}
\end{table}



\end{document}
