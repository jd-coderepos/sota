\section{Experiments}
\label{sec05:exp}
In the experiment section, we mainly present:
\begin{itemize}\item how we construct multi-relation graphs upon different fraud data (Section~\ref{sec05:graph_build});
    
    \item camouflage evidences in real-world fraud data (Section~\ref{sec05:camoflage});
    
    \item the performance comparison over baselines and CARE-GNN variants (Section~\ref{sec05:overall_eval});
    
    \item the learning process and explanation of the RL algorithm (Section~\ref{sec05:training_analysis});
    
    \item the sensitivity study of hyper-parameters and their effects on model designing (Section~\ref{sec05:sensitivity}).
\end{itemize}



\subsection{Experimental Setup}
\label{sec05:setup}





\begin{table}[h]
\small
\centering
\caption{Dataset and graph statistics.}
\resizebox{0.98\linewidth}{!}{\begin{tabular}{c|lcccc}  
\hline
\multicolumn{2}{r}{\begin{tabular}[c]{@{}c@{}}\textbf{\#Nodes} \\ (\textbf{Fraud\%})\end{tabular}} & \textbf{Relation}  & \textbf{\#Edges}  & \begin{tabular}[c]{@{}c@{}} \textbf{Avg. Feature} \\ \textbf{Similarity}\end{tabular}  & \begin{tabular}[c]{@{}c@{}} \textbf{Avg. Label} \\ \textbf{Similarity}\end{tabular}  \\ 

\hline
 \multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Yelp}}} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}} 45,954 \\ (14.5\%)\end{tabular}}  & \textit{R-U-R} & 49,315   & 0.83 & 0.90 \\
&  & \textit{R-T-R} & 573,616   &0.79 & 0.05\\
&  & \textit{R-S-R} & 3,402,743  & 0.77 & 0.05\\
&  & \textit{ALL} & 3,846,979   & 0.77& 0.07\\
\hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Amazon}}} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}} 11,944 \\ (9.5\%)\end{tabular}}  & \textit{U-P-U} & 175,608   & 0.61 & 0.19 \\
&  & \textit{U-S-U} & 3,566,479   &0.64 & 0.04\\
&  & \textit{U-V-U} & 1,036,737  & 0.71 & 0.03\\
&  & \textit{ALL} & 4,398,392   & 0.65 & 0.05\\
\hline
\end{tabular}}
\label{tab:stat}
\end{table}

\subsubsection{Dataset.}
We use the Yelp review dataset~\cite{Rayana2015} and Amazon review dataset~\cite{mcauley2013amateurs} to study the fraudster camouflage and GNN-based fraud detection problem.
The Yelp dataset includes hotel and restaurant reviews filtered (spam) and recommended (legitimate) by Yelp.
The Amazon dataset includes product reviews under the Musical Instruments category.
Similar to~\cite{zhang2020gcn}, we label users with more than 80\% helpful votes as benign entities and users with less than 20\% helpful votes as fraudulent entities.
Though previous works have proposed other fraud datasets like Epinions~\cite{kumar2018rev2} and Bitcoin~\cite{weber2019anti}, they only contain graph structures and compacted features, with which we cannot build meaningful multi-relation graphs.
In this paper, we conduct a spam review detection (fraudulent user detection resp.) task on the Yelp dataset (Amazon dataset resp.), which is a binary classification task.
We take 32 handcrafted features from~\cite{Rayana2015} (25 handcrafted features from~\cite{zhang2020gcn} resp.) as the raw node features for Yelp (Amazon resp.) dataset.
Table~\ref{tab:stat} shows the dataset statistics.


\subsubsection{Graph Construction}
\label{sec05:graph_build}
\noindent \textbf{Yelp:} based on previous studies~\cite{Rayana2015, Mukherjee:2013uk} which show that opinion fraudsters have connections in user, product, review text, and time,
we take reviews as nodes in the graph and design three relations: 1) \textit{R-U-R}: it connects reviews posted by the same user; 2) \textit{R-S-R}: it connects reviews under the same product with the same star rating (1-5 stars); 3) \textit{R-T-R}: it connects two reviews under the same product posted in the same month.
\textbf{Amazon:} similarly, we take users as nodes in the graph and design three relations: 1) \textit{U-P-U}: it connects users reviewing at least one same product; 2) \textit{U-S-V}: it connects users having at least one same star rating within one week; 3) \textit{U-V-U}: it connects users with top 5\% mutual review text similarities (measured by TF-IDF) among all users.
The number of edges belonging to each relation is shown in Table~\ref{tab:stat}.


\subsubsection{Baselines.}
To verify the ability of CARE-GNN in alleviating the negative influence induced by camouflaged fraudsters, we compare it with various GNN baselines under the semi-supervised learning setting. We select GCN~\cite{kipf2016semi}, GAT~\cite{velivckovic2017graph}, RGCN~\cite{schlichtkrull2018modeling}, and GraphSAGE~\cite{hamilton2017inductive} to represent general GNN models. We choose GeniePath~\cite{liu2019geniepath}, Player2Vec~\cite{zhang2019key}, SemiGNN~\cite{wang2019semi}, and GraphConsis~\cite{liu2020alleviating} as four state-of-the-art GNN-based fraud detectors. Their detailed introduction can be found in Section~\ref{sec02:RW}. We also implement several variants of CARE-GNN: CARE-\textit{Att}, CARE-\textit{Weight}, and CARE-\textit{Mean}, and they differ from each other in Attention~\cite{velivckovic2017graph}, Weight~\cite{liu2018heterogeneous}, and Mean~\cite{hamilton2017inductive} inter-relation aggregator respectively.

Among those baselines, GCN, GAT, GraphSAGE, and GeniePath are run on homogeneous graphs (i.e., Relation \textit{ALL} in Table~\ref{tab:stat}) where all relations are merged together. Other models are run on multi-relation graphs where they handle information from different relations in their approaches.


\begin{table*}[h]
\small
\centering
\caption{Fraud detection performance (\%) on two datasets under different percentage of training data.}
\resizebox{0.95\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c||c|c|c|c|}  
\hline
& \textbf{Metric}  & \textbf{Train\%}  & \textbf{GCN} & \textbf{GAT} & \textbf{RGCN} & \begin{tabular}[c]{@{}c@{}} \textbf{Graph-}\\ \textbf{SAGE} \end{tabular}   & \begin{tabular}[c]{@{}c@{}} \textbf{Genie-}\\ \textbf{Path} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Player-}\\ \textbf{2Vec} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Semi-}\\ \textbf{GNN} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Graph-}\\ \textbf{Consis} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{CARE-}\\ \textbf{\textit{Att}} \end{tabular} &  \begin{tabular}[c]{@{}c@{}} \textbf{CARE-}\\ \textbf{\textit{Weight}} \end{tabular} &  \begin{tabular}[c]{@{}c@{}} \textbf{CARE-}\\ \textbf{\textit{Mean}} \end{tabular} &  \begin{tabular}[c]{@{}c@{}} \textbf{CARE-}\\ \textbf{GNN} \end{tabular} \\ 

\hline
  \multirow{8}{*}{\rotatebox[origin=c]{90}{\textbf{Yelp}}} & \multirow{4}{*}{AUC} & 5\%  & 54.98  & 56.23 & 50.21  & 53.82   & 56.33   & 51.03  & 53.73  & 61.58   & 66.08   & 71.10  & 69.83 & \textbf{71.26}\\
  & & 10\%  & 50.94 & 55.45   & 55.12  & 54.20   & 56.29   & 50.15  & 51.68  & 62.07   & 70.21   & 71.02  & 71.85 & \textbf{73.31}\\
  & & 20\%  & 53.15 & 57.69   & 55.05  & 56.12  & 57.32   & 51.56  & 51.55  & 62.31  & 73.26   & 74.32  & 73.32 & \textbf{74.45}\\
  & & 40\%  & 52.47 & 56.24   & 53.38  & 54.00   & 55.91   & 53.65  & 51.58  & 62.07   & 74.98   & 74.42  & 74.77 & \textbf{75.70}\\
\cline{2-15}
  & \multirow{4}{*}{Recall}& 5\%  & 53.12 & 54.68   & 50.38  & 54.25   & 52.33   & 50.00  & 52.28  & 62.60   & 63.52   & 66.64  & \textbf{68.09} & 67.53\\
  & & 10\%  & 51.10 & 52.34   & 51.75  & 52.23   & 54.35   & 50.00  & 52.57  & 62.08   & 67.38   & 68.35  & \textbf{68.92} & 67.77\\
  & & 20\%  & 53.87 & 53.20   & 50.92  & 52.69   & 54.84   & 50.00  & 52.16  & 62.35   & 68.34   & 69.07  & \textbf{69.48} & 68.60\\
  & & 40\%  & 50.81 & 54.52   & 50.43  & 52.86   & 50.94   & 50.00  & 50.59  & 62.08   & 71.13   & 70.22  & 69.25 & \textbf{71.92}\\
\hline
\hline
  \multirow{8}{*}{\rotatebox[origin=c]{90}{\textbf{Amazon}}} & \multirow{4}{*}{AUC} & 5\%  & 74.44 & 73.89   & 75.12  & 70.71   & 71.56   & 76.86  & 70.25  & 85.46   & 89.49  & 89.36 & 89.35 & \textbf{89.54}\\
  & & 10\%  & 75.25 & 74.55   & 74.13  & 73.97   & 72.23   & 75.73  & 76.21  & 85.29    & \textbf{89.58}  & 89.37 & 89.43 & 89.44\\
  & & 20\%  & 75.13 & 72.10   & 75.58  & 73.97   & 71.89   & 74.55  & 73.98  & 85.50    & 89.58  & \textbf{89.68} & 89.34 & 89.45\\
  & & 40\%  & 74.34  & 75.16 & 74.68 & 75.27   & 72.65  & 56.94 & 70.35 & 85.50   & 89.70  & 89.69 & 89.52 & \textbf{89.73}\\
\cline{2-15}
  & \multirow{4}{*}{Recall}& 5\%  & 65.54 & 63.22   & 64.23  & 69.09   & 65.56   & 50.00  & 63.29  & 85.49   & 88.22  & 88.31 & 88.02 & \textbf{88.34}\\
  & & 10\%  & 67.81 & 65.84   & 67.22  & 69.36   & 66.63   & 50.00  & 63.32  & 85.38   & 87.87  & \textbf{88.36} & 88.12 & 88.29\\
  & & 20\%  & 66.15 & 67.13   & 65.08  & 70.30   & 65.08   & 50.00  & 61.28  & 85.59   & 88.40  & \textbf{88.60} & 88.00 & 88.27\\
  & & 40\%  & 67.45   & 65.51 & 67.68 & 70.16   & 65.41  & 50.00 & 62.89 & 85.53   & 88.41  & 88.45 & 88.22 & \textbf{88.48}\\
  \hline
\end{tabular}}
\label{tab:overall}
\end{table*} 


\subsubsection{Experimental Setting}
\label{sec05:experimental_setting}
From Table~\ref{tab:stat}, we can see that the percentage of fraudsters are small in both datasets.
Meanwhile, real-world graphs usually have great scales.
To improve the training efficiency and avoid overfitting, we employ mini-batch training~\cite{goyal2017accurate} and under-sampling~\cite{liu2008exploratory} techniques to train CARE-GNN and other baselines.
Specifically, under each mini-batch, we randomly sample the same number of negative instances as the number of positive instances.
We also study the sample ratio sensitivity in Section~\ref{sec05:sensitivity}.

We use unified node embedding size (64), batch size (1024 for Yelp, 256 for Amazon), number of layers(1), learning rate (0.01), optimizer (Adam), and L2 regularization weight () for all models.
For CARE-GNN and its variants, we set the RL action step size () as 0.02 and the similarity loss weight () as 2.
In Section~\ref{sec05:sensitivity}, we present the sensitivity study for the number of layers, embedding size, and .   

\subsubsection{Implementation}
For the GCN, GAT, RGCN, GraphSAGE, GeniePath, we use the source code provided by authors.
For Player2Vec, SemiGNN, and GraphConsis, we use the open-source implementations\footnote{\url{https://github.com/safe-graph/DGFraud}}.
We implement CARE-GNN with Pytorch. All models are running on Python 3.7.3, 2 NVIDIA GTX 1080 Ti GPUs, 64GB RAM, 3.50GHz Intel Core i5 Linux desktop.

\subsubsection{Evaluation Metric}
Since the Yelp dataset has imbalanced classes, and we focus more on fraudsters (positive instances),
like previous work~\cite{Rayana2015}, we utilize ROC-AUC (AUC) and Recall to evaluate the overall performance of all classifiers.
AUC is computed based on the relative ranking of prediction probabilities of all instances, which could eliminate the influence of imbalanced classes.


\subsection{Camouflage Evidence}
\label{sec05:camoflage}
We analyze fraudster camouflage using two metrics introduced in~\cite{liu2020alleviating}. 
For the feature camouflage, we compute the feature similarity of neighboring nodes based on their feature vectors' Euclidean distance, ranging from  to .
The average feature similarity is normalized w.r.t. the total number of edges, which is presented in Table~\ref{tab:stat}.
We observe that the averaged similarity scores under all relations are high.
High feature similarity implies that fraudsters camouflage their features in a similar way to benign nodes.
Moreover, the minor feature similarity difference across different relations proves that the unsupervised similarity measure cannot effectively discriminate fraudsters and benign entities.
For instance, the label similarity difference between \textit{R-U-R} and \textit{R-T-R} is 0.85, but the feature similarity difference is only 0.04.  

For the relation camouflage, we study it by calculating the label similarity based on whether two connected nodes have the same label.
The label similarity is normalized w.r.t. the total number of edges.
The average label similarity for each relation is shown in Table~\ref{tab:stat}.
High label similarity score implies that the fraudsters fail to camouflage, and low score implies that fraudsters camouflage successfully.
We observe that only \textit{R-U-R} relation has a high label similarity score, while the other relations have label similarity scores less than 20\%.
It suggests that we need to select different amounts of neighbors for different relations to facilitate the GNN aggregation process.
Meanwhile, we should distinguish relations in order to prevent fraudsters from camouflaging.

\subsection{Overall Evaluation}
\label{sec05:overall_eval}

Table~\ref{tab:overall} shows the performance of proposed CARE-GNN and various GNN baselines under the fraud detection task on two datasets.
We report the best testing results after thirty epochs. 
We observe that CARE-GNN outperforms other baselines under most of the training proportions and metrics.

\vspace{1mm}
\noindent \textbf{Single-relation vs. Multi-relation.}
Among all GNN baselines in Table~\ref{tab:overall}, GCN, GAT, GraphSAGE, and GeniePath run on single-relation (i.e., homogeneous) graph where all relations are merged together (\textit{ALL} in Table~\ref{tab:stat}).
Other baselines are built upon multi-relation graphs.
The performances of single-relation GNNs are better than Player2Vec and SemiGNN, which indicates previously designed fraud detection methods are not suitable for multi-relation graphs.
Among the multi-relation GNNs, GraphConsis outperforms all other multi-relation GNNs.
The reason is that GraphConsis samples the neighbors based on the node features before aggregating them. Better than GraphConsis, CARE-GNN and its variants adopt parameterized similarity measure and adaptive sampling thresholds, which could better identify and filter camouflaged fraudsters. 
It demonstrates that neighbor filtering is critical to GNNs when the graph contains many noises (i.e., dissimilar/camouflaged neighbors).
Also, CARE-GNN has higher scores than all single-relation GNNs, suggesting that a noisy graph undermines the performance of multi-relation GNNs.
A possible reason is the higher complexity of multi-relation GNNs comparing to single-relation ones.


\begin{figure*}
     \centering
     \begin{subfigure}[b]{0.98\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/yelp_rl.pdf}
         \label{fig:yelp_rl}
     \end{subfigure}
    \bigskip
     \begin{subfigure}[b]{0.98\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/amz_rl.pdf}
         \label{fig:amz_rl}
     \end{subfigure}
     \vspace*{-5mm}
        \caption{The training process and testing performance of CARE-\textit{Weight} on Yelp (upper) and Amazon (lower) dataset.} 
        \label{fig:training}
\end{figure*}


\vspace{1mm}
\noindent \textbf{Training Percentage.}
From Table~\ref{tab:overall}, there is little performance gain for GNNs when increasing the training percentages.
It demonstrates the advantage of semi-supervised learning, where a small amount of supervised signals is enough to train a good model.
Meanwhile, with informative handcraft features as inputs for two datasets, GNNs are much easier to learn high-quality embeddings. 


\vspace{1mm}
\noindent \textbf{CARE-GNN Variants.}
The last four columns of Table~\ref{tab:overall} show the performance of CARE-GNN and its variants with different inter-relation aggregators.
It is observed that those four models have similar performances under most training percentages and metrics.
It verifies our assumption in Section~\ref{sec04:aggregation} that the attention coefficients and relation weights will become unnecessary when we select similar neighbors under all relations.
Moreover, for the Yelp dataset, the CARE-\textit{Att} has worse performances under a smaller training percentage (e.g., 5\%).
While for CARE-GNN, since it does not need to train extra attention weights, it attains the best performance against other variants. 
The first column of Figure~\ref{fig:training} presents more evidence that the relation weights finally become equal for all relations under both datasets.
The better performance of CARE-GNN comparing to CARE-\textit{Mean} shows that keeping the filtering threshold as inter-relation aggregation weights could enhance the GNN performance and reduce model complexity.

\vspace{1mm}
\noindent \textbf{GNN vs. Similarity Measure (Figure~\ref{fig:training} Column 4).}
Figure~\ref{fig:training} Column 4 shows the testing performances solely based on the outputs of the GNN module and similarity measure module during training.
For the Yelp dataset, GNN has better AUC and Recall than the similarity measure, which suggests that leveraging the structural information benefits the model to classify fraud and benign entities. 
For Amazon, the performance of GNN and the similarity measure are comparable with each other.
It is because the input features provide enough information to discriminate fraudsters.



\subsection{RL Process Analysis}
\label{sec05:training_analysis}

In this paper, we jointly train the similarity measure and GNN together and employ RL to find the neighbor filtering thresholds adaptively.
To present the RL process from different perspectives, in Figure~\ref{fig:training}, we plot the updating process of three parameters without terminating the RL process during training CARE-\textit{Weight}.
Since CARE-\textit{Weight} learns the aggregation weight for each relation, plotting its training process instead of CARE-GNN could help understand the effects of our proposed GNN enhancement modules.
During training, we also test the model every three epochs for Yelp (four epochs for Amazon) and plot the testing performance for both GNN and similarity measure at the last column of Figure~\ref{fig:training}.


\vspace{1mm}
\noindent \textbf{Relation Weights (Figure~\ref{fig:training} Column 1).}
We observe that the randomly initialized relation aggregation weights gradually converge to the same value as the neighbor selector updates its filtering thresholds and selects more similar neighbors under each relation.
When neighbors under each relation provide similar information, their aggregation weights will be similar as well.


\vspace{1mm}
\noindent \textbf{Relation Distance (Figure~\ref{fig:training} Column 2).}
As the training epoch increases, it is clearly that the differences between neighbor distances under each relation (computed by Eq. (\ref{eq:avg_distance})) become larger and comparable to each other.
The reason is that the GNN projects the node embeddings to a broader range of space and makes them more distinguishable.
As the model filters more noisy neighbors, the average distance across different relations become closer.

\vspace{1mm}
\noindent \textbf{Neighbor Filtering Threshold (Figure~\ref{fig:training} Column 3).}
We take 0.02 as the action step size; all thresholds are updated and converge to different values.
When the filtering threshold oscillates for several rounds, it reaches the terminal condition in Eq. (\ref{eq:terminal}).
For different datasets, the proposed RL algorithm could adaptively find the optimal filtering thresholds.

To demonstrate the advantage of the optimal neighbor filtering thresholds found by RL, in Figure~\ref{fig:adaptive}, we plot the testing performances of three different neighbor selection criteria under two datasets. 
 filters neighbors using converged thresholds found by RL (as shown in Figure~\ref{fig:training} Column 3);  keeps the \textit{top} 50\% similar neighbors under each relation and  keeps all neighbors without filtering.   
It is illustrated that CARE-GNN with adaptive filtering thresholds is optimized faster than the other two neighbor selectors.
Meanwhile, it has a better and smoother performance during training. 
It verifies the effectiveness of the proposed RL algorithm, which is able to find informative neighbors under each relation.


























\begin{figure}
     \centering
         \includegraphics[width=0.485\textwidth]{figures/adaptive.pdf}
         \caption{The testing AUC and Recall for CARE-GNN with different neighbor filtering methods during training.}
        \label{fig:adaptive}
\end{figure}



\subsection{Hyper-parameter Sensitivity}
\label{sec05:sensitivity}


Figure~\ref{fig:sensitivity} shows the testing performance of CARE-GNN regarding four hyper-parameters on the Yelp dataset.
From Figure~\ref{fig:sensitivity}(a), we observe that increasing the number of layers barely improves the performance of CARE-GNN.
For the three-layer model, the CARE-GNN suffers the overfitting problem (Recall = 0.5).
Therefore, the one-layer model is not only able to save the computational cost but also achieve better classification results.
Figure~\ref{fig:sensitivity}(b) presents the CARE-GNN performance under different under-sampling ratios as introduced in Section~\ref{sec05:experimental_setting}.
Note that CARE-GNN is tested on an imbalanced test set.
Moreover, CARE-GNN is overfitted when negative instances are less than positive ones (under 1:0.2 and 1:0.5, Recalls are equal to 0.5).
An equal under-sampling ratio guarantees a good and fair performance of CARE-GNN.
Figure~\ref{fig:sensitivity}(c) shows the influence of different embedding sizes. Embedding sizes with 16, 32, and 64 have comparable performance.
Figure~\ref{fig:sensitivity}(d) illustrates the effects of different weighting values for the similarity loss ( in Eq. (\ref{eq:total_loss})).
When the weight of similarity loss is doubled compared to which of GNN loss, CARE-GNN reaches the best performance.
Therefore, the similarity measure is crucial for GNN training.


\subsection{Discussion}
\label{sec05:discussion}

Since the multi-relation graphs used in the experiments are very dense (average node degree ), one-layer CARE-GNN (which aggregates one-hop neighbors) has already utilized abundant information and thus can achieve excellent performance.
CARE-GNN with more layers is suitable for sparse graphs.
We improve the computational efficiency using multiple approaches: the light-weight similarity measure, the classic and fast RL framework, positive-node based neighbor selector, no attention mechanism, and mini-batch training with under-sampling.
For CARE-GNN, each epoch only takes 17 seconds on Yelp (3 seconds on Amazon), and it has a great performance gain comparing to other baselines.
