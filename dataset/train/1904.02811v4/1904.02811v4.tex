\pdfoutput=1

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabulary}
\usepackage{multirow}

\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\DT}[1]{\textcolor{red}{[DT: #1]}}
\newcommand{\mdf}[1]{\textcolor{blue}{[mdf: #1]}}
\newcommand{\LT}[1]{\textcolor{green}{[LT: #1]}}
\newcommand{\ddd}[1]{#1#1#1}


\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Video Classification with Channel-Separated Convolutional Networks}

\author{Du Tran \qquad Heng Wang \qquad Lorenzo Torresani  \qquad Matt Feiszli\\Facebook AI\\
{\tt\small \{trandu,hengwang,torresani,mdf\}@fb.com}
}

\maketitle



\begin{abstract}

Group convolution has been shown to offer great computational savings in various 2D convolutional architectures for image classification. It is natural to ask: 1) if group convolution can help to alleviate the high computational cost of video classification networks; 2) what factors matter the most in 3D group convolutional networks; and 3) what are good computation/accuracy trade-offs with 3D group convolutional networks.

This paper studies the effects of different design choices in 3D group convolutional networks for video classification. We empirically demonstrate that the amount of channel interactions plays an important role in the accuracy of 3D group convolutional networks. Our experiments suggest two main findings. First, it is a good practice to factorize 3D convolutions by separating channel interactions and spatiotemporal interactions as this leads to improved accuracy and lower computational cost. Second, 3D channel-separated convolutions provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions. These two empirical findings lead us to design an architecture --  Channel-Separated Convolutional Network (CSN) -- which is simple, efficient, yet accurate. On Sports1M, Kinetics, and Something-Something, our CSNs are comparable with or better than the state-of-the-art while being 2-3 times more efficient.

\end{abstract} 
\section{Introduction}
\label{sec:intro}

Video classification has witnessed much good progress in recent years. Most of the accuracy gains have come from the introduction of new powerful architectures~\cite{I3D,Tran18,P3D,xie2017rethinking,XiaolongWang18}. However, many of these architectures are built on expensive 3D spatiotemporal convolutions.  Furthermore,  these convolutions are typically computed across all the channels in each layer. 3D CNNs have complexity  as opposed to the cost of  of 2D CNNs, where  denotes the number of frames,  the spatial dimensions and  the number of channels.  For both foundational and practical reasons, it is natural to ask which parameters in these large 4D kernels matter the most.

Kernel factorizations have been applied in several settings to reduce compute and improve accuracy.  For example, several recent video architectures factor 3D convolution in space and time: examples include P3D~\cite{P3D}, R(2+1)D~\cite{Tran18}, and S3D~\cite{xie2017rethinking}.  In these architectures, a 3D convolution is replaced with a 2D convolution (in space) followed by a 1D convolution (in time). This factorization can be leveraged to increase accuracy and/or to reduce computation. In the still-image domain, separable convolution~\cite{Xception} is used to factorize the convolution of 2D  filters into a pointwise  convolution followed by a depthwise  convolution.  When the number of channels is large compared to , which is usually the case, this reduces FLOPs by  for images. For the case of 3D video kernels, the FLOP reduction is even more dramatic: .


Inspired by the accuracy gains and good computational savings demonstrated by 2D separable convolutions in image classification~\cite{Xception,MobileNet,ShuffleNet}, this paper proposes a set of architectures for video classification -- 3D Channel-Separated Networks (CSN) -- in which all convolutional operations are separated into either pointwise 111 or depthwise 333 convolutions. Our experiments reveal the importance of {\it channel interaction} in the design of CSNs. In particular, we show that excellent accuracy/computational cost balances can be obtained with CSNs by leveraging channel separation to reduce FLOPs and parameters as long as high values of channel interaction are retained.  We propose two factorizations, which we call {\it interaction-reduced} and {\it interaction-preserved}. Compared to 3D CNNs, both our interaction-reduced and interaction-preserved CSNs provide higher accuracy and FLOP savings of about 2.5-3 {\em when} there is enough channel interaction. We experimentally show that the channel factorization in CSNs acts as a regularizer, leading to a higher training error but better generalization. Finally, we show that our proposed CSNs outperform or are comparable with the current state-of-the art methods on Sports1M, Kinetics, and Something-Something while being 2--3 times faster. 
\section{Related Work}
\label{sec:related_work}

\noindent{\bf Group convolution}. Group convolution was adopted in AlexNet~\cite{Krizhevsky12} as a way to overcome GPU memory limitations. Depthwise convolution was introduced in MobileNet~\cite{MobileNet} as an attempt to optimize model size and computational cost for mobile applications. Chollet~\cite{Xception} built an extreme version of Inception~\cite{Inception} based on 2D depthwise convolution, named Xception, where the Inception block was redesigned to include multiple separable convolutions. Concurrently, Xie~\emph{et al.} proposed ResNeXt~\cite{SainingXie17} by equipping ResNet~\cite{KaimingHe16} bottleneck blocks with groupwise convolution.  Further architecture improvements have also been made for mobile applications. ShuffleNet~\cite{ShuffleNet} further reduced the computational cost of the bottleneck block with both depthwise and group convolution. MobileNetV2~\cite{MobileNet2} improved MobileNet~\cite{MobileNet} by switching from a VGG-style to a ResNet-style network, and introducing a ``reverted bottleneck'' block. All of these architectures are based on 2D CNNs and are applied to image classification while our work focuses on 3D group CNNs for video classification.

\noindent{\bf Video classification}. In the last few years, video classification has seen a major paradigm shift, which involved moving from hand-designed features~\cite{Laptev03,Piotr05,ActionBank,Wang2013} to deep network approaches that learn features and classify end-to-end~\cite{Tran15,Karpathy14,SimonyanZ14,FeichtenhoferNIPS16,WangXW0LTG16,Wang_Transformation,FeichtenhoferPZ16}. This transformation was enabled by the introduction of large-scale video datasets~\cite{Karpathy14,kinetics} and massively parallel computing hardware, \ie, GPU. Carreira and Zisserman~\cite{I3D} recently proposed to inflate 2D convolutional networks pre-trained on images to 3D for video classification. Wang~\emph{et al.}~\cite{XiaolongWang18} proposed non-local neural networks to capture long-range dependencies in videos. ARTNet~\cite{LiminWang18} decouples spatial and temporal modeling into two parallel branches. Similarly, 3D convolutions can also be decomposed into a Pseudo-3D convolutional block as in P3D~\cite{P3D} or factorized convolutions as in R(2+1)D~\cite{Tran18} or S3D~\cite{xie2017rethinking}. 3D group convolution was also applied to video classification in ResNeXt~\cite{retrace} and Multi-Fiber Networks~\cite{MFNet} (MFNet). 


Among previous approaches, our work is most closely related to the following architectures. First, our CSNs are similar to Xception~\cite{Xception} in the idea of using channel-separated convolutions. Xception factorizes 2D convolution in channel and space for object classification, while our CSNs factorize 3D convolution in channel and space-time for action recognition. In addition, Xception uses simple blocks, while our CSNs use bottleneck blocks. The variant ir-CSN of our model shares similarities with ResNeXt~\cite{SainingXie17} and its 3D version~\cite{retrace} in the use of bottleneck block with group/depthwise convolution. The main difference is that ResNext~\cite{SainingXie17,retrace} uses group convolution in its \ddd{3} layers with a fixed group size (\eg, ), while our ir-CSN uses depthwise convolutions in all \ddd{3} layers which makes our architecture fully channel-separated. As we will show in section~\ref{sec:csn_factorization}, making our network fully channel-separated helps not only to reduce a significant amount of compute, but also to improve model accuracy by better regularization. We emphasize that our contribution includes not only the design of CSN architectures, but also a systematic empirical study of the role of channel interactions in the accuracy of CSNs. 
\section{Channel-Separated Convolutional Networks}
\label{sec:channel_separated_network}

In this section, we discuss the concept of 3D channel-separated networks. Since channel-separated networks use group convolution as their main building block, we first provide some background about group convolution.

\subsection{Background}
\label{sec:background}




\noindent{\bf Group convolution}. Conventional convolution is implemented with dense connections, i.e., each convolutional filter receives input from all channels of its previous layer, as in Figure~\ref{fig:group_conv}(a). However, in order to reduce the computational cost and model size, these connections can be sparsified by grouping convolutional filters into subsets. Filters in a subset receive signal from only channels within its group (see Figure~\ref{fig:group_conv}(b)). Depthwise convolution is the extreme version of group convolution where the number of groups is equal to the number of input and output channels (see figure~\ref{fig:group_conv}(c)). Xception~\cite{Xception} and MobileNet~\cite{MobileNet} were among the first networks to use depthwise convolutions. Figure~\ref{fig:group_conv} presents an illustration of conventional, group, and depthwise convolutional layers for the case of  input channels and  output channels.

\begin{figure}
\begin{center}
   \includegraphics[width=\linewidth]{group_conv.pdf}
\end{center}
\vspace{-10pt}
   \caption{{\bf Group convolution}. Convolutional filters can be partitioned into groups with each filter receiving input from channels only within its group. (a) A conventional convolution, which has only one group. (b) A group convolution with 2 groups. (c) A depthwise convolution where the number of groups matches the number of input/output filters, i.e., each group contains only one channel.}
\label{fig:group_conv}
\end{figure}

\noindent{\bf Counting FLOPs, parameters, and interactions}. Dividing a conventional convolutional filter into  groups reduces compute and parameter count by a factor of .  These reductions occur because each filter in a group receives input from only a fraction  of the channels from the previous layer.  In other words, channel grouping restricts feature interaction: only channels within a group can interact. If multiple group convolutional layers are stacked directly on top of each other, this feature segregation is further amplified since each channel becomes a function of small channel-subsets in all preceding layers. So, while group convolution saves compute and parameters, it also reduces feature interactions.

We propose to quantify the amount of channel interaction as the number of pairs of two input channels that are connected through any output filter. If the convolutional layer has  channels and  groups, each filter is connected to  input channels. Therefore each filter will have  interacting feature pairs. According to this definition, the example convolutions in Figure~\ref{fig:group_conv}(a)-(c) will have , , and  channel interaction pairs, respectively.

Consider a 3D convolutional layer with spatiotemporal convolutional filters of size \ddd{} and  groups,  input channels, and  output channels. Let   be the total number of voxels in the spatiotemporal tensor provided as input to the layer. Then, the number of parameters, FLOPs (floating-point operations), and number of channel interactions can be measured as:


Recall that . We note that while FLOPs and number of parameters are commonly used to characterize a layer, the ``amount'' of channel interaction is typically overlooked. Our study will reveal the importance of this factor.


\subsection{Channel Separation} 
\label{sec:channel_separated_networks}

We define channel-separated convolutional networks (CSN) as 3D CNNs in which all convolutional layers (except for \texttt{conv1}) are either \ddd{1} conventional convolutions or \ddd{} depthwise convolutions (where, typically, ). Conventional convolutional networks model channel interactions and local interactions (i.e., spatial or spatiotemporal) jointly in their 3D  convolutions. Instead, channel-separated networks decompose these two types of interactions into two distinct layers: \ddd{1} conventional convolutions for channel interaction (but no local interaction) and \ddd{} depthwise convolutions for local spatiotemporal interactions (but not channel interaction). Channel separation may be applied to any \ddd{} traditional convolution by decomposing it into a \ddd{1} convolution and a depthwise \ddd{} convolution.



We introduce the term ``channel-separated'' to highlight the importance of channel interaction; we also point out that the existing term ``depth-separable'' is only a good description when applied to tensors with two spatial dimensions and one channel dimension. We note that channel-separated networks have been proposed in Xception~\cite{Xception} and MobileNet~\cite{MobileNet} for image classification. In video classification, separated convolutions have been used in P3D~\cite{P3D}, R(2+1)D~\cite{Tran18}, and S3D~\cite{xie2017rethinking}, but to decompose 3D convolutions into separate temporal and spatial convolutions. The network architectures presented in this work are designed to separate channel interactions from spatiotemporal interactions.

\subsection{Example: Channel-Separated Bottleneck Block}
\label{sec:lossy_lossless_csn}

Figure~\ref{fig:factorized_blocks} presents two ways of factorizing a 3D bottleneck block using channel-separated convolutional networks. Figure~\ref{fig:factorized_blocks}(a) presents a standard 3D bottleneck block, while Figure~\ref{fig:factorized_blocks}(b) and~\ref{fig:factorized_blocks}(c) present interaction-preserved and interaction-reduced channel-separated bottleneck blocks, respectively.


\noindent {\bf Interaction-preserved channel-separated bottleneck block} is obtained from the standard bottleneck block (Figure~\ref{fig:factorized_blocks}(a) by replacing the \ddd{3} convolution in (a) with a \ddd{1} traditional convolution and a \ddd{3} depthwise convolution (shown in Figure~\ref{fig:factorized_blocks}(b)). This block reduces parameters and FLOPs of the traditional \ddd{3} convolution significantly, but preserves all channel interactions via a newly-added \ddd{1} convolution. We call this an {\it interaction-preserved} channel-separated bottleneck block and the resulting architecture an {\it interaction-preserved channel-separated network} (ip-CSN).

\noindent {\bf Interaction-reduced channel-separated bottleneck block} is derived from the preserved bottleneck block by removing the extra \ddd{1} convolution.  This yields the depthwise bottleneck block shown in Figure~\ref{fig:factorized_blocks}(c). Note that the initial and final \ddd{1} convolutions (usually interpreted respectively as projecting into a lower-dimensional subspace and then projecting back to the original dimensionality) are now the only mechanism left for channel interactions.  This implies that the complete block shown in (c) has a reduced number of channel interactions compared with those shown in (a) or (b). We call this design an {\it interaction-reduced} channel-separated bottleneck block and the resulting architecture an {\it interaction-reduced channel-separated network} (ir-CSN).


\begin{figure}
\begin{center}
   \includegraphics[width=0.8\linewidth]{factorized_models.pdf}
\end{center}
\vspace{-10pt}
   \caption{{\bf Standard vs. channel-separated convolutional blocks}. (a) A standard ResNet bottleneck block. (b) An interaction-preserved bottleneck block: a bottleneck block where the \ddd{3} convolution in (a) is replaced by a \ddd{1} standard convolution and a \ddd{3} depthwise convolution (shown in dashed box). (c) An interaction-reduced bottleneck block, a bottleneck block where the \ddd{3} convolution in (a) is replaced with a depthwise convolution (shown in dashed box). We note that channel interaction is preserved in (b) by the \ddd{1} convolution, while (c) lost all of the channel interaction in its \ddd{3} convolution after factorization. Batch norm and ReLU are used after each convolution layer. For simplicity, we omit the skip connections.}
\label{fig:factorized_blocks}
\end{figure}


\subsection{Channel Interactions in Convolutional Blocks}
\label{sec:group_conv_by_blocks}

The interaction-preserving and interaction-reducing blocks in section~\ref{sec:lossy_lossless_csn} are just two architectures in a large spectrum. In this subsection we present a number of convolutional block designs, obtained by progressively increasing the amount of grouping. The blocks differ in terms of compute cost, parameter count and, more importantly, channel interactions. 

 
\noindent{\bf Group convolution applied to ResNet blocks}. Figure~\ref{fig:simpleblock_transform}(a) presents a ResNet~\cite{KaimingHe16}  {\bf simple} block consisting of two \ddd{3} convolutional layers. Figure~\ref{fig:simpleblock_transform}(b) shows the {\bf simple-G} block, where the \ddd{3} layers now use grouped convolution.  Likewise, Figure~\ref{fig:simpleblock_transform}(c) presents {\bf simple-D}, with two depthwise layers.  Because depthwise convolution requires the same number of input and output channels, we optionally add a \ddd{1} convolutional layer (shown in the dashed rectangle) in blocks that change the number of channels. 

\begin{figure}
\begin{center}
   \includegraphics[width=0.8\linewidth]{simpleblock_transform.pdf}
\end{center}
\vspace{-10pt}
   \caption{{\bf ResNet simple block transformed by group convolution}. (a) Simple block: a standard ResNet simple block with two \ddd{3} convolutional layers. (b) Simple-G block: a ResNet simple block with two \ddd{3} group convolutional layers. (c) Simple-D block: a ResNet simple block with two \ddd{3} depthwise convolutional layers with an optional \ddd{1} convolutional layer (shown in dashed box) added when increasing number of filters is needed. Batch norm and ReLU are used after each convolution layer. For simplicity, we omit the skip connections.}
\label{fig:simpleblock_transform}
\end{figure}


Figure~\ref{fig:bottleneck_transform}(a) presents a ResNet {\bf bottleneck} block consisting of two \ddd{1} and one \ddd{3} convolutional layers. Figures~\ref{fig:bottleneck_transform}(b-c) present {\bf bottleneck-G} and  {\bf bottleneck-D} where the \ddd{3} convolutions are grouped and depthwise, respectively. If we further apply group convolution to the two \ddd{1} convolutional layers, the block becomes a {\bf bottleneck-DG}, as illustrated in Figure~\ref{fig:bottleneck_transform}(d). In all cases, the \ddd{3} convolutional layers always have the same number of input and output channels. 

There are some deliberate analogies to existing architectures here.  First, bottleneck-G (Figure~\ref{fig:bottleneck_transform}(b)) is exactly a ResNeXt block~\cite{SainingXie17}, and bottleneck-D is its depthwise variant. Bottleneck-DG (Figure~\ref{fig:bottleneck_transform}(d)) resembles the ShuffleNet block~\cite{ShuffleNet}, without the channel shuffle and without the downsampling projection by average pooling and concatenation. The progression from simple to simple-D is similar to moving from ResNet to Xception (though Xception has many more \ddd{1} convolutions).  We omit certain architecture-specific features in order to better understand the role of grouping and channel interactions.

\begin{figure}
\begin{center}
   \includegraphics[width=\linewidth]{bottleneck_transform.pdf}
\end{center}
\vspace{-10pt}
   \caption{{\bf ResNet bottleneck block transformed by group convolution}. (a) A standard ResNet bottleneck block. (b) Bottleneck-G: a ResNet bottleneck block with a \ddd{3} group convolutional layer. (c) Bottleneck-D: a bottleneck block with a \ddd{3} depthwise convolution (previously named as ir-CSN, the new name of Bottleneck-D is used here for simplicity and analogy with other blocks). (d) Bottleneck-DG: a ResNet bottleneck block with a \ddd{3} depthwise convolution and two \ddd{1} group convolutions. We note that from (a) to (d), we gradually apply group convolution to the \ddd{3} convolutional layer and then the two \ddd{1} convolutional layers. Batch norm and ReLU are used after each convolution layer. For simplicity, in the illustration we omit to show skip connections.}
\label{fig:bottleneck_transform}
\end{figure}
 
\section{Ablation Experiment}
\label{sec:experiments}

This empirical study will allow us to cast some light on the important factors in the performance of channel-separated network and will lead us to two main findings:
\begin{enumerate}
\item We will empirically demonstrate that within the family of architectures we consider, similar depth and similar amount of channel interaction implies similar accuracy.  In particular, the interaction-preserving blocks reduce compute significantly but preserve channel interactions, with only a slight loss in accuracy for shallow networks and an increase in accuracy for deeper networks.
\item In traditional \ddd{3} convolutions all feature maps interact with each other. For deeper networks, we show that this causes overfitting.
\end{enumerate}


\subsection{Experimental setup}
\label{sec:exp_setup}
\noindent{\bf Dataset}. We use Kinetics-400~\cite{kinetics} for the ablation experiments in this section. Kinetics is a standard benchmark for action recognition in videos. It contains about  videos of  different human action categories. We use the training split ( videos) for training and the validation split ( videos) for evaluating different models.

\noindent{\bf Base architecture}. We use \emph{ResNet3D}, presented in Table~\ref{tab:basic_architecture}, as our base architecture for most of our ablation experiments in this section. More specifically, our model takes clips with a size of T224224 where  is the number of frames,  is the height and width of the cropped frame. Two spatial downsampling layers (122) are applied at \texttt{conv1} and at \texttt{pool1}, and three spatiotemporal downsampling (222) are applied at \texttt{conv3}\_1,  \texttt{conv4}\_1 and  \texttt{conv5}\_1 via convolutional striding. A global spatiotemporal average pooling with kernel size 77 is applied to the final convolutional tensor, followed by a fully-connected (fc) layer performing the final classification.


\newcommand{\blocka}[2]{\multirow{3}{*}{-.1em] \text{333, #1} \end{array}\right]\times\times\times\times\times\times\times\times\times\times\times\times\times\times\times64b_125664b_1\frac{T}{2}\times\times128b_2512128b_2\frac{T}{4}\times\times256b_31024256b_3\frac{T}{8}\times\times512b_42048512b_4\times\timesb_{1,\dots,4}ss256320\timesTT=8485124510350.010.6410102.9\%0.7\%0.420.42 \times 10^90.270.9-1.4\%\times 10^9\times 10^6\times 10^92.9\%0.7\%\times\times\times\times\times\times\times\times\times\times487260K400^2100K174^232T=322^2\times14.4\%9.1\%3.8\%2.2\%\times\times\times\times\times\times\times8.1\%4.9\%4.5\%A^24.6\%3.1\%1.5\%0.3\%0.6\%1.21.3\%3.03.4\times\times\times\times\times\timesA^2\times\times\times\times\times\times\times\times\times\times\times\times\times\times\times\times\times^253.3\%0.5\%1.7\%^2\times\times\times\times\times\times\times\times\times\times\times\times\times\times$3 channel-separated convolutional filters in \texttt{comp\_12} of ir-CSN-152. \texttt{comp\_12} is an arbitrarily chosen block in the third convolutional group.}
\label{fig:csn_conv4}
\vspace{-6pt}
\end{figure*} 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ieeedu_ref}
}


\end{document}
