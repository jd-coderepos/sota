
\documentclass{article} \usepackage{iclr2022_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{tablefootnote}
\usepackage{color}
\usepackage[hidelinks,colorlinks=true,linkcolor=blue,citecolor=cyan]{hyperref}
\usepackage{url}
\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\textcolor{green}{\ding{51}}}\newcommand{\xmark}{\textcolor{red}{\ding{55}}}\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{threeparttable}


\title{Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework}


\author{Xu Ma\textsuperscript{1}, Can Qin\textsuperscript{1}, Haoxuan You\textsuperscript{2}, Haoxi Ran\textsuperscript{1}, Yun Fu\textsuperscript{1}
\\
\textsuperscript{1}Northeastern University, Boston, MA, USA\\
\textsuperscript{2}Columbia University, New York, NY, USA\\
\texttt{\{ma.xu1,qin.ca,ran.h\}@northeastern.edu} \\
\texttt{\{haoxuanyou,ranhaoxi\}@gmail.com} \\
\texttt{yunfu@ece.neu.edu} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Point cloud analysis is challenging due to irregularity and unordered data structure. To capture the 3D geometries, prior works mainly rely on exploring sophisticated local geometric extractors using convolution, graph, or attention mechanisms. These methods, however, incur unfavorable latency during inference, and the performance saturates over the past few years. In this paper, we present a novel perspective on this task. We notice that detailed local geometrical information \emph{probably is not} the key to point cloud analysis -- we introduce a \emph{pure residual MLP} network, called PointMLP, which integrates no ``sophisticated" local geometrical extractors but still performs very competitively. Equipped with a proposed lightweight geometric affine module, PointMLP delivers the new state-of-the-art on multiple datasets. On the real-world ScanObjectNN dataset, our method even surpasses the prior best method by \textbf{3.3}\% accuracy. 
We emphasize that PointMLP achieves this strong performance \emph{without} any sophisticated operations, hence leading to a superior inference speed. Compared to most recent CurveNet, PointMLP \textbf{trains 2 faster, tests 7 faster}, and is more accurate on ModelNet40 benchmark.
We hope our PointMLP may help the community towards a better understanding of point cloud analysis. 
The code is available at \href{https://github.com/ma-xu/pointMLP-pytorch}{https://github.com/ma-xu/pointMLP-pytorch}.

\end{abstract}

\section{Introduction}
Lately, point cloud analysis has emerged as a popular topic in 3D understanding, attracting attention from academia and industry~\citep{qi2017pointnet,shi2019pointrcnn,xu2020squeezesegv3}.
Different from 2D images represented by regular dense pixels, point clouds are composed of unordered and irregular sets of points , making it infeasible to apply image processing methods to point cloud analysis directly. Meanwhile, the nature of sparseness and the presence of noises further restrict the performance. In the past few years, endowing with neural networks, point cloud analysis has seen a great improvement in various applications, including 3D shape classification~\citep{qi2017pointnet}, semantic segmentation~\citep{hu2020randla} and object detection~\citep{shi2020point}, etc. 

Recent efforts have shown promising results for point cloud analysis by exploring local geometric information, using convolution~\citep{li2021deepgcns}, graph~\citep{li2021deepgcns}, or attention mechanism~\citep{guo2021pct} (see Section~\ref{sec:related_work} for details).
These methods, despite their gratifying results, have mainly relied on the premise that an elaborate local extractor is essential for point cloud analysis, leading to the competition for careful designs that explore fine local geometric properties. Nevertheless, sophisticated extractors are not without drawbacks.
On the one hand, due to prohibitive computations and the overhead of memory access, these sophisticated extractors hamper the efficiency of applications in natural scenes. As an example, until now, most 3D point cloud applications are still based on the simple PointNet ( and PointNet++) or the voxel-based methods~\citep{liu2021group,li2021lidar, zhang2021self}. However, applications that employ the aforementioned advanced methods are rare in literature.
On the other hand, the booming sophisticated extractors saturate the performance since they already describe the local geometric properties well. A more complicated design is no longer to improve the performance further. 
These phenomena suggest that we may need to stop the race of local feature extraction designing, rethinking the necessity of elaborate local feature extractors and further revisiting the succinct design philosophy in point cloud analysis.

\begin{wrapfigure}{r}{6.5cm}
    \centering
    \vspace{-2mm}
    \includegraphics[width=1\linewidth]{images/acc_time.pdf}
    \vspace{-6mm}
    \caption{Accuracy-speed tradeoff on ModelNet40. Our PointMLP performs best. Please refer to Section~\ref{sec:experiments} for details.}
    \vspace{-2mm}
    \label{fig:acc_time}
\end{wrapfigure}
In this paper, we aim at the ambitious goal of building a deep network for point cloud analysis using only residual feed-forward MLPs, without any delicate local feature explorations. By doing so, we eschew the prohibitive computations and continued memory access caused by the sophisticated local geometric extractors and enjoy the advantage of efficiency from the highly-optimized MLPs. To further improve the performance and generalization ability,
We introduce a lightweight local geometric affine module that adaptively transforms the point feature in a local region. We term our new network architecture as PointMLP. In the sense of MLP-based design philosophy, our PointMLP is similar to PointNet and PointNet++~\citep{qi2017pointnet, qi2017pointnet++}. However, our model is more generic and exhibits promising performance. Different from the models with sophisticated local geometric extractors (e.g., DeepGCNs~\citep{li2019deepgcns}, RSCNN~\citep{liu2019relation}, RPNet~\citep{ran2021learning}.), our PointMLP is conceptually simpler and achieves results on par or even better than these state-of-the-art methods (see Figure~\ref{fig:acc_time}). Keep in mind that we did not challenge the advantages of these local geometric extractors and we acknowledge their contributions; however, a more succinct framework should be studied considering both the efficiency and accuracy.
In Table~\ref{tab:systematic_comparison}, we systemically compare our PointMLP with some representative methods.

Even though the design philosophy is simple, PointMLP (as well as the elite version) exhibits superior performance on 3D point cloud analysis. Specifically, we achieve the state-of-the-art classification performance, \textbf{94.5\%}, on the ModelNet40 benchmark, and we outperform related works by \textbf{3.3\%} accuracy on the real-world ScanObjectNN dataset, with a significantly higher inference speed.

\section{Related Work}
\label{sec:related_work}

\paragraph{Point cloud analysis.} There are mainly two streams to process point cloud. 
Since the point cloud data structure is irregular and unordered, some works consider projecting the original point clouds to intermediate voxels~\citep{maturana2015voxnet,shi2020pv} or images~\citep{you2018pvnet,li2020end}, translating the challenging 3D task into a well-explored 2D image problem. In this regime, point clouds understanding is largely boosted and enjoys the fast processing speed from 2D images or voxels. Albeit efficient, information loss caused by projection degrades the representational quality of details for point clouds~\citep{yang2019std}. To this end, some methods are proposed to process the original point cloud sets directly. PointNet~\citep{qi2017pointnet} is a pioneering work that directly consumes unordered point sets as inputs using shared MLPs. Based on PointNet, PointNet++~\citep{qi2017pointnet++} further introduced a hierarchical feature learning paradigm to capture the local geometric structures recursively. Owing to the local point representation (and multi-scale information), PointNet++ exhibits promising results and has been the cornerstone of modern point cloud methods~\citep{wang2019dynamic,fan2021scf,xu2021paconv}. Our PointMLP also follows the design philosophy of PointNet++ but explores a simpler yet much deeper network architecture.


\paragraph{Local geometry exploration.} As PointNet++ built the generic point cloud analysis network framework, the recent research focus is shifted to how to generate better regional points representation. Predominantly, the explorations of local points representation can be divided into three categories: convolution-, graph-, and attention-based methods. One of the most distinguished convolution-based methods is PointConv~\citep{wu2019pointconv}. By approximating continuous weight and density functions in convolutional filters using an MLP, PointConv is able to extend the dynamic filter to a new convolution operation. Also, PAConv~\citep{xu2021paconv} constructs the convolution kernel by dynamically assembling basic weight matrices stored in a weight bank. Without modifying network configurations, PAConv can be seamlessly integrated into classical MLP-based pipelines. Unlike convolution-based methods, Graph-based methods investigate mutually correlated relationships among points with a graph. In~\citet{wang2019dynamic}, an EdgeConv is proposed to generate edge features that describe the relationships between a point and its neighbors. By doing so, a local graph is built, and the point relationships are well preserved. In 3D-GCN~\citep{lin2021learning}, authors aim at deriving deformable 3D kernels using a 3D Graph Convolution Network. Closely related to graph-based methods, the attention-based methods exhibit excellent ability on relationship exploration as well, like PCT~\citep{guo2021pct} and Point Transformer~\citep{zhao2021point,engel2020point}.
With the development of local geometry exploration, the performances on various tasks appear to be saturated. Continuing on this track would bring minimal improvements. In this paper, we showcase that even without the carefully designed operations for local geometry exploration, a pure deep hierarchical MLP architecture is able to exhibit gratifying performances and even better results. 



\begin{wraptable}{r}{8cm}
    \centering
    \vspace{-3mm}
    \caption{Systematic comparison among some representative methods. ``Deep" indicates that a model is expandable along depth. ``Opt.'' stands for the principal operator.
    }
    \label{tab:systematic_comparison}
    \vspace{1mm}
    \begin{tabular}{l|cccc}
        \toprule
        Method&hierarchy & locality  & \makecell[c]{deep} & \makecell[c]{opt.} \\
        \midrule
        PointNet &\xmark &\xmark &\xmark &MLP \\
        PointNet++ &\cmark &\cmark &\xmark & MLP\\
        DGCNN&\xmark &\cmark &\xmark &DGCN  \\
        DeepGCNs &\cmark&\cmark&\cmark&GCN\\
        PointConv &\cmark &\cmark &\xmark &Conv.  \\
        Point Trans. &\cmark &\cmark &\cmark &Atten.  \\
        \midrule
        PointMLP&\cmark &\cmark &\cmark &MLP \\
        \bottomrule
    \end{tabular}
\end{wraptable}
\paragraph{Deep network architecture for point cloud.} Interestingly, the development of point cloud analysis is closely related to the evolution of the image processing network. In the early era, works in the image processing field simply stack several learning layers to probe the performance limitations~\citep{alexnet, SimonyanZ14a,dong2014learning}. Then, the great success of deep learning was significantly promoted by deep neural architectures like ResNet~\citep{he2016deep}, which brings a profound impact to various research fields. Recently, attention-based models, including attention blocks~\citep{wang2018non} and Transformer architectures~\citep{dosovitskiy2020image}, further flesh out the community. Most recently, the succinct deep MLP architectures have attracted a lot of attention due to their efficiency and generality. 
Point cloud analysis follows the same develop history as well, from MLP-based PointNet~\citep{qi2017pointnet}, deep hierarchical PointNet++~\citep{qi2017pointnet++}, convolution-/graph-/relation- based methods~\citep{wu2019pointconv, wang2019dynamic, ran2021learning}, to state-of-the-art Transformer-based models~\citep{guo2021pct,zhao2021point}. 
In this paper, we abandon sophisticated details and present a simple yet effective deep residual MLP network for point cloud analysis. Instead of following the tendency in the vision community deliberately, we are in pursuit of an inherently simple and empirically powerful architecture for point cloud analysis.


\section{Deep Residual MLP for Point Cloud}
\label{sec:pointmlp}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{images/overview.pdf}
    \caption{Overview of one stage in PointMLP. Given an input point cloud, PointMLP progressively extracts local features using residual point MLP blocks. In each stage, we first transform the local points using a geometric affine module, then they are extracted before and after the aggregation operation, respectively. PointMLP progressively enlarges the receptive field and models complete point cloud geometric information by repeating multiple stages.}
    \label{fig:architecture}
\end{figure}

We propose to learn the point cloud representation by a simple feed-forward residual MLP network (named PointMLP), which hierarchically aggregates the local features extracted by MLPs, and abandons the use of delicate local geometric extractors. To further improve the robustness and improve the performance, we also introduce a lightweight geometric affine module to transform the local points to a normal distribution. The detailed framework of our method is illustrated in Figure~\ref{fig:architecture}.



\subsection{Revisiting Point-based Methods}
The design of point-based methods for point cloud analysis dates back to the PointNet and PointNet++ papers~\citep{qi2017pointnet, qi2017pointnet++}, if not earlier. The motivation behind this direction is to directly consume point clouds from the beginning and avoid unnecessary rendering processes.  

Given a set of points , where  indicates the number of points in a   Cartesian space, point-based methods aims to directly learn the underlying representation  of  using neural networks. One of the most pioneering works is PointNet++, which learns hierarchical features by stacking multiple learning stages. In each stage,  points are re-sampled by the farthest point sampling (FPS) algorithm where  indexes the stage and  neighbors are employed for each sampled point and aggregated by max-pooling to capture local structures. Conceptually, the kernel operation of PointNet++ can be formulated as:

where  means aggregation function (max-pooling in PointNet++),  denotes the local feature extraction function (MLP in PointNet++), and  is the -th neighbor point feature of -th sampled point. By doing so, PointNet++ is able to effectively capture local geometric information and progressively enlarge the receptive fields by repeating the operation.

In the sense of network architecture design, PointNet++ exhibits a universal pipeline for point cloud analysis. Following this pipeline, some plug-and-play methods have been proposed, mainly focusing on the local feature extractor ~\citep{xu2021paconv,liu2019relation,thomas2019kpconv,zhao2021point}.
Generally, these local feature extractors thoroughly explore the local geometric information using convolution, graph, or self-attention mechanisms. In RSCNN~\citep{liu2019relation}, the extractor is mainly achieved by exploring point relations as follow:

where  is the concatenation operation and MLP is a small network composed of a Fully-connected (FC) layer, Batch Normalization layer, and activation function. Unlike RSCNN, Point Transformer introduces the self-attention mechanism into point cloud analysis and considers the similarities between pair-wise points in a local region. To this end, it re-formulates the extractor as:

where  and  are linear mapping function, ``" is a Hadamard product, and  is a softmax normalization. In particular, Point Transformer introduces a relative position encoding, , where the relative position is encoded by two FC layers with a ReLU non-linearity layer, into both attention weights and features. The lightweight positional encoder largely improves the performance of Point Transformer.


While these methods can easily take the advantage of detailed local geometric information and usually exhibit promising results, two issues limit their development. First, with the introduction of delicate extractors, the computational complexity is largely increased, leading to prohibitive inference latency~\footnote{We emphasize that the model complexity could not be simply revealed by FLOPs or parameters, other metrics like memory access cost (MAC) and the degree of parallelism also significantly affect the speed~\citep{ma2018shufflenet, zhang2020resnest}. However, these important metrics are always ignored in point clouds analysis.}. 
For example, the FLOPs of Equation~\ref{eq:point_transformer} in Point Transformer would be , ignoring the summation and subtraction operations. Compared with the conventional FC layer that enjoys  FLOPs, it increases the computations by times. Notice that the memory access cost is not considered yet.
Second, with the development of local feature extractors, the performance gain has started to saturate on popular benchmarks. Moreover, empirical analysis in ~\citet{liu2020closer} reveals that most sophisticated local extractors make surprisingly similar contributions to the network performance under the same network input.
Both limitations encourage us to develop a new method that circumvents the employment of sophisticated local extractors, and provides gratifying results.







\subsection{Framework of PointMLP}
In order to get rid of the restrictions mentioned above, we present a simple yet effective MLP-based network for point cloud analysis that no sophisticated or heavy operations are introduced. The key operation of our PointMLP can be formulated as:

where  and  are residual point MLP blocks: the shared  is designed to learn shared weights from a local region while the  is leveraged to extract deep aggregated features. In detail, the mapping function can be written as a series of homogeneous residual MLP blocks, , in which  is combined by FC, normalization and activation layers (repeated two times). Following ~\citet{qi2017pointnet}, we consider the aggregation function  as max-pooling operation.  Equation~\ref{eq:pointMLP_stage} describes one stage of of PointMLP. For a hierarchical and deep network, we recursively repeat the operation by  stages. Albeit the framework of PointMLP is succinct, it exhibits some prominent merits. 1) Since PointMLP only leverages MLPs, it is naturally invariant to permutation, which perfectly fits the characteristic of point cloud. 2) By incorporating residual connections, PointMLP can be easily extended to dozens layers, resulting deep feature representations. 3) In addition, since there is no sophisticated extractors included and the main operation is only highly optimized feed-forward MLPs, even we introduce more layers, our PointMLP still performs efficiently.  Unless explicitly stated, the networks in our experiments use four stages, and two residual blocks in both  and . We employ k-nearest neighbors algorithm (kNN) to select the neighbors and set the number  to 24. 


\subsection{Geometric Affine Module}
While it may be easy to simply increase the depth by considering more stages or stacking more blocks in  and , we notice that a simple deep MLP structure will decrease the accuracy and stability, making the model less robust. This is perhaps caused by the sparse and irregular geometric structures in local regions. Diverse geometric structures among different local regions may require different extractors but shared residual MLPs struggle at achieving this.  We flesh out this intuition and develop a lightweight geometric affine module to tackle this problem. Let   be the grouped local neighbors of  containing  points, and each neighbor point  is a -dimensional vector. We transform the local neighbor points by the following formulation:

where  and  are learnable parameters,  indicates Hadamard product, and  is a small number for numerical stability~\citep{ioffe2015batch,wu2018group,dixon1951introduction}. Note that  is a scalar describes the feature deviation across all local groups and channels.  By doing so, we transform the points via a normalization operation while maintaining original geometric properties.



\subsection{Computational Complexity and Elite Version}
Although the FC layer is highly optimized by mainstream deep learning framework, the theoretical number of parameters and computational complexity are still high. To further improve the efficiency, we introduce a lightweight version of PointMLP named as \textit{pointMLP-elite}, \textit{with less than \textbf{0.7M} parameters and prominent inference speed (176 samples/second on ModelNet40 benchmark)}. 

Inspired by~\citet{he2016deep,hu2018squeeze}, we present a bottleneck structure for the mapping function  and . We opt to reduce the channel number of the intermediate FC layer by a factor of  and increase the channel number as the original feature map. This strategy is opposite to the design in ~\citet{vaswani2017attention,touvron2021resmlp} which increases the intermediate feature dimensions. Empirically, we do not observe a significant performance drop. This method reduce the parameters of residual MLP blocks from  to . By default, we set  to 4 in PointMLP-elite.
Besides, we also slightly adjust the network architecture, reducing both the MLP blocks and embedding dimension number (see appendix for details).
Inspired by~\citet{xie2017aggregated}, we also investigated a grouped FC operation in the network that divides one FC layer into  groups of sub-FC layers, like group convolution layer. However, we empirically found that this strategy would largely hamper the performance. As a result, we did not consider it in our implementation.




\begin{table}[]
\small
    \centering
    \caption{Classification results on ModelNet40 dataset. 
 With only 1k points, our method achieves state-of-the-art results on both class mean accuracy (mAcc) and overall accuracy (OA) metrics. We also report the speed of some open-sourced methods by samples/second tested on one Tesla V100-pcie GPU and four cores AMD EPYC 7351@2.60GHz CPU. * For KPConv, we take the results from the original paper. The best is marked in \textbf{bold} and second best is in \textcolor{blue}{blue}.}
 \vspace{1mm}
    \begin{tabular}{l|ccc|ccc}
    \toprule
         Method& Inputs & mAcc(\%) &OA(\%) & Param. &\makecell{ Train\\speed} &\makecell{ Test\\speed} \\
         \midrule
         PointNet~\citep{qi2017pointnet}   & 1k P &86.0 &89.2 & & & \\
         PointNet++~\citep{qi2017pointnet++} & 1k P &-&90.7  & 1.41M&\textbf{223.8} &\textbf{308.5} \\
         PointNet++~\citep{qi2017pointnet++} &5k P+N &-&91.9  & 1.41M& & \\
         \midrule
         PointCNN~\citep{li2018pointcnn}   &1k P &88.1&92.5  & & & \\
         
         PointConv~\citep{wu2019pointconv}  &1k P+N &-&92.5  &18.6M &17.9 &10.2 \\
         KPConv~\citep{thomas2019kpconv}     & 7k P &-&92.9  &15.2M &31.0* &80.0* \\
         DGCNN~\citep{wang2019dynamic}      & 1k P &90.2&92.9  & & &  \\
         RS-CNN~\citep{liu2019relation}     & 1k P &-&92.9  & & & \\
         DensePoint~\citep{liu2019densepoint} &1k P &-&93.2  & & & \\
         PointASNL~\citep{yan2020pointasnl}  & 1k P &-&92.9  & & & \\
         PosPool~\citep{liu2020closer}    &5k P &-&93.2  & & & \\
         Point Trans.~\citep{engel2020point}  &1k P &-&92.8  & & & \\
         GBNet~\citep{qiu2021geometric} &1k P &\textcolor{blue}{91.0} &93.8 &8.39M &16.3 &112 \\
         GDANet~\citep{xu2021learning}& 1k P &-&93.8  &\textcolor{blue}{0.93M} &26.3 & 14.0\\
         PA-DGC~\citep{xu2021paconv}    &1k P &-&93.9  & & & \\
         MLMSPT~\citep{han2021point}  &1k P &-&92.9  & & & \\
         
         PCT~\citep{guo2021pct}        &1k P &-&93.2  & & & \\
         Point Trans.~\citep{zhao2021point}  &1k P &90.6&93.7  & & & \\
        CurveNet~\citep{xiang2021walk} &1k P &-&\textcolor{blue}{94.2}  &2.04M & 20.8&15.0 \\
         \midrule
        
        
        
        
        
        
        PointMLP {\small \textbf{w/o vot.}}&1k P &91.3 &94.1  &12.6M &47.1 &112 \\ PointMLP {\small \textbf{w/ vot.}} &1k P &\textbf{91.4} &\textbf{94.5}  &12.6M &47.1 &112\\ 
        PointMLP-elite {\small \textbf{w/o vot.}} &1k P &90.9 &93.6  &\textbf{0.68M} &\textcolor{blue}{116} &\textcolor{blue}{176} \\
        PointMLP-elite {\small \textbf{w/ vot.}} &1k P &90.7 & 94.0 &\textbf{0.68M} &\textcolor{blue}{116} &\textcolor{blue}{176} \\ 
        
         \bottomrule
    \end{tabular}
    \label{tab:classification-modelnet40}
\end{table}

\section{Experiments}
\label{sec:experiments}
In this section, we comprehensively evaluate PointMLP on several benchmarks. Detailed ablation studies demonstrate the effectiveness of PointMLP with both quantitative and qualitative analysis.


\subsection{Shape classification on ModelNet40}
We first evaluate PointMLP on the ModelNet40~\citep{modelnet40} benchmark, which contains 9,843 training and 2,468 testing meshed CAD models belonging to 40 categories. Following the standard practice in the community, we report the class-average accuracy (mAcc) and overall accuracy (OA) on the testing set. We train all models  for 300 epochs using SGD optimizer.

Experimental results are presented in Table~\ref{tab:classification-modelnet40}. Among these methods, our PointMLP clearly outperforms state-of-the-art  method CurveNet  by 0.3\% (94.5\% \textit{vs.} 94.2\%) overall accuracy with only 1k points. Note that this improvement could be considered as a promising achievement since the results on ModelNet40 recent methods have been saturated around 94\% for a long time.
Even without the voting strategy~\citep{liu2019relation}, our PointMLP still performs on par or even better than other methods that are tested with voting strategy.

Despite having better accuracy, our method is much faster than the methods with sophisticated local geometric extractors. We compare PointMLP to several open-sourced methods and report the parameters, classification accuracy, training, and testing speed. As we stated previously, a key intuition behind this experiment is that model complexity can not directly reflect efficiency. For example, CurveNet is lightweight and delivers a strong result, whereas the inference cost is prohibitive \textbf{(15 samples/second)}. On the contrary, our PointMLP presents a high inference speed \textbf{(112 samples/second)}. To further reduce the model size and speed up the inference, we present a lightweight PointMLP-elite, which significantly reduces the number of parameters to 0.68M, while maintaining high-performance 90.9\% mAcc and 94.0\% OA on ModelNet40. With PointMLP-elite, we further speed up the inference to \textbf{176 samples/second}.





\subsection{Shape Classification on ScanObjectNN}
\begin{figure}[!t]
    \centering
    \subfigure[PointMLP 24-Layers]{
        
        \includegraphics[width=0.31\linewidth]{images/with_without_affine_24.pdf}
    }
    \subfigure[PointMLP 40-Layers]{
        \includegraphics[width=0.31\linewidth]{images/with_without_affine_40.pdf}
    }
    \subfigure[PointMLP 56-Layers]{
        \includegraphics[width=0.31\linewidth]{images/with_without_affine_56.pdf}
    }
    \caption{Four run results (mean  std) of PointMLP with/without our geometric affine module on ScanObjectNN test set. We zoom in on the details of PointMLP40 to show the stability difference.}
    \label{fig:affine_module}
\end{figure}

\begin{wraptable}{r}{8.9cm}
\small
    \centering
    \vspace{-7mm}
    \caption{Classification results on ScanObjectNN dataset. We examine all methods on the most challenging variant (PB\_T50\_RS). For our pointMLP and PointMLP-elite, we train and test for four runs and report mean  std results.
    }
    \label{tab:my_label}
    \vspace{1mm}
    \begin{tabular}{l|cc}
        \toprule
         Method & mAcc(\%) &OA(\%) \\
         \midrule
         3DmFV & 58.1&63\\
         PointNet~\citep{qi2017pointnet} &63.4 &68.2 \\
         SpiderCNN~\citep{xu2018spidercnn} &69.8 &73.7 \\
         PointNet++~\citep{qi2017pointnet++} &75.4 &77.9 \\
         DGCNN~\citep{wang2019dynamic} &73.6 &78.1 \\
         PointCNN~\citep{li2018pointcnn} &75.1 &78.5 \\
         BGA-DGCNN~\citep{uy2019revisiting} &75.7 &79.7 \\
         BGA-PN++~\citep{uy2019revisiting} &77.5 &80.2 \\
         DRNet~\citep{qiu2021dense} &78.0 &80.3 \\
         GBNet~\citep{qiu2021geometric} &77.8 &80.5 \\
         SimpleView~\citep{goyal2021revisiting} & - &80.50.3 \\
         PRANet~\citep{cheng2021net} & 79.1&82.1 \\
         MVTN~\citep{hamdi2021mvtn} & -&82.8 \\
         \midrule
         PointMLP \small{(ours)} &\textbf{83.90.5} &\textbf{85.40.3}\\
         
         PointMLP-elite \small{(ours)} &\textbf{81.80.8} &\textbf{83.80.6}\\
         
         \bottomrule
    \end{tabular}
    \label{tab:cls_scanobjectNN}
    \vspace{-3mm}
\end{wraptable}

While ModelNet40 is the de-facto canonical benchmark for point cloud analysis, it may not meet the requirement of modern methods due to its synthetic nature and the fast development of point cloud analysis. To this end, we also conduct experiments on the ScanObjectNN benchmark~\citep{uy2019revisiting}. 

ScanObjectNN is a recently released point cloud benchmark that contains ~15,000 objects that are categorized into 15 classes with 2,902 unique object instances in the real world. Due to the existence of background, noise, and occlusions, this benchmark poses significant challenges to existing point cloud analysis methods. We consider the hardest perturbed variant (PB\_T50\_RS) in our experiments. We train our model using an SGD optimizer for 200 epochs with a batch size of 32. For a better illustration, we train and test our method for four runs and report the mean  standard deviation in Table~\ref{tab:cls_scanobjectNN}.

Empirically, \textbf{our PointMLP surpasses all methods by a significant improvement on both class mean accuracy (mAcc) and the overall accuracy (OA)}. For example, we outperform PRANet by 4.8\% mAcc and 3.3\% OA. Even compared with the heavy multi-view projection method MVTN (12 views), our PointMLP still performs much better (85.39\% \textit{82.8\%}). \textit{Notice that we achieve this by fewer training epochs and did not consider the voting strategy.}
Moreover, we notice that our method achieves the smallest gap between class mean accuracy and overall accuracy. This phenomenon indicates that PointMLP did not bias to a particular category, showing decent robustness.













\begin{table}
\small
\begin{minipage}{0.44\linewidth}
\caption{Classification accuracy of pointMLP on ScanObjectNN test set using 24, 40, and 56 layers, respectively.}
 \centering
    \vspace{1mm}
    \begin{tabular}{c|cc}
    \toprule
     Depth& mAcc(\%) & OA(\%)  \\
     \midrule
     24 layers&83.40.4 &84.80.5 \\
     40 layers&\textbf{83.90.5} &\textbf{85.40.3} \\
     56 layers&83.20.2 &85.00.1 \\
     \bottomrule
     
    \end{tabular}
    
    \label{tab:depth}


\end{minipage}
\hspace{10mm}
\begin{minipage}{0.48\linewidth}  
\small
\centering
\caption{Component ablation studies on ScanObjectNN test set.}
\vspace{1mm}
\begin{tabular}{ccc|cc}

    \toprule
     &  & Affine & mAcc(\%) & OA(\%) \\
    \midrule
     \xmark&\cmark &\cmark &80.80.4& 82.80.0\\
     \cmark&\xmark &\cmark &83.30.3 & 84.70.2\\
     \cmark&\cmark &\xmark &79.11.7 & 81.51.4\\
     \midrule
     \cmark&\cmark &\cmark & \textbf{83.9}0.5& \textbf{85.40.3}\\
     \bottomrule
\end{tabular}
\label{tab:component}   
\end{minipage}
\end{table}




\subsection{Ablation studies}
\label{sec:ablation}

\paragraph{Network Depth.}
     
    
Network depth has been exploited in many tasks but is rare in point cloud analysis. We first investigate the performance of PointMLP with different depths in Table~\ref{tab:depth}. We vary the network depth by setting the number of homogeneous residual MLP blocks to 1, 2, and 3, respectively, resulting in 24, 40, and 56-layers PointMLP variants. Detailed depth formulation can be found in Appendix~\ref{appendix:depth}. At first glance, we notice that simply increasing the depth would not always bring better performance; an appropriate depth would be a good solution. Additionally, the model gets stable with more layers introduced, as demonstrated by the decreasing standard deviation. When the depth is set to 40, we achieve the best tradeoff between accuracy and stability (85.4\% mean accuracy and 0.3 standard deviations). Remarkably, PointMLP consistently achieves gratifying results that outperform recent methods, regardless of the depth. 

\paragraph{Geometric Affine Module.}  Other work provides sophisticated local geometric extractors to explore geometric structures. Instead, our PointMLP discards these burdensome modules and introduces a lightweight geometric affine module.
Figure~\ref{fig:affine_module} presents the results of PointMLP with/without the geometric affine module. By integrating the module, we systematically improve the performance of PointMLP by about 3\% for all variants. The reasons for this large improvement are two-fold. First, the geometric affine module maps local input features to a normal distribution, which eases the training of PointMLP. Second, the geometric affine module implicitly encodes the local geometrical information by the channel-wise distance to local centroid and variance, remedying the deficiency of geometric information. Besides the gratifying improvements, the geometric affine module also largely boosts the stability of PointMLP, suggesting better robustness.

\begin{wrapfigure}{r}{0.7\textwidth}
    \centering
        
    \vspace{-6mm}
    \subfigure[PointMLP w/o Res.]{
        \includegraphics[width=0.400\linewidth]{images/model31CNoRes_loss_landscape.pdf}
    }
    \hspace{3mm}
    \subfigure[PointMLP]{
        \includegraphics[width=0.40\linewidth]{images/model31C_loss_landscape.pdf}
    }
    \caption{
    Loss landscape along two rand directions. By introducing residual connection, we ease the optimization of PointMLP and achieve a flat landscape like a simple shallow network intuitively.  
    }
    \vspace{-3mm}
    \label{fig:losssurface}
\end{wrapfigure}

\paragraph{Component ablation study.} Table~\ref{tab:component} reports the results on ScanObjectNN of removing each individual component in PointMLP. Consistent with Figure~\ref{fig:affine_module}, geometric affine module plays an important role in PointMLP,
improving the base architecture by 3.9\%. Remarkably, even without this module, which is an unfair setting for PointMLP,  our base network stills achieves  OA, outperforming most related methods (see Table~\ref{tab:cls_scanobjectNN}).
Removing  function (MLPs before aggregator ), the performance drops 2.6\% overall accuracy. Combining all these components together, we achieve the best result 85.4\% OA. See Appendix~\ref{appendix:ablation} for more ablations.


\paragraph{Loss landscape.} We depict the 3D loss landscape~\citep{li2018visualizing} in Figure~\ref{fig:losssurface}. Simply increasing the network depth may not achieve a better representation and even hamper the results. When removing the residual connection in PointMLP, the loss landscape turns sharp, and the performance plummets to 88.1\% (6\% drop) on ModelNet40. With residual connection, we greatly ease the optimization course of PointMLP and make it possible to train a deep network.


         
         






\subsection{Part segmentation}
\begin{wrapfigure}{r}{0.6\textwidth}
  \vspace{-7mm}
  \begin{center}
    \includegraphics[width=0.24\linewidth]{images/part/901-gt.png}
    \includegraphics[width=0.24\linewidth]{images/part/1-gt.png}
    \includegraphics[width=0.24\linewidth]{images/part/1531-gt.png}
    \includegraphics[width=0.24\linewidth]{images/part/1005-gt.png}
    
    
    \includegraphics[width=0.24\linewidth]{images/part/901-predict.png}
     \includegraphics[width=0.24\linewidth]{images/part/1-predict.png}
    \includegraphics[width=0.24\linewidth]{images/part/1531-predict.png}
    \includegraphics[width=0.24\linewidth]{images/part/1005-predict.png}
  \end{center}
  \vspace{-2mm}
  \caption{Part segmentation results on ShapeNetPart. Top line is ground truth and bottom line is our prediction.}
  \label{fig:partseg}
  \vspace{-2mm}
\end{wrapfigure}
Our PointMLP can also be generalized to other 3D point cloud tasks. We next test PointMLP for 3D shape part segmentation task on the ShapeNetPart benchmark~\citep{yi2016scalable}. 
The shapeNetPart dataset consists of 16,881 shapes with 16 classes belonging to 50  parts labels in total. In each class, the number of parts is between 2 and 6. We follow the settings from~\citet{qi2017pointnet++} that randomly select 2048 points as input for a fair comparison. We compare our methods with several recent works, including SyncSpecCNN~\citep{yi2017syncspeccnn}, SPLATNet~\citep{su2018splatnet},  etc. 
We also visualize the segmentation ground truths and predictions in Figure~\ref{fig:partseg}. Intuitively, the predictions of our PointMLP are close to the ground truth. Best viewed in color.



\begin{table*}[]
    \centering
    \caption{Part segmentation results on the ShapeNetPart dataset. Empirically, our method is much faster than the best method KPConv, and presents a competitive performance.}
    \label{tab:part_segmentation}
    \vspace{1mm}
    \begin{adjustbox}{width=1\textwidth}
    \begin{tabular}{l|cc|cccccccccccccccc}
        \toprule 
         Method& \makecell{ Cls.\\mIoU}& \makecell{ Inst.\\mIoU}&aero &bag &cap &car &chair &\makecell{ aerp-\\hone} &guitar &knife &lamp &laptop &\makecell{ motor-\\bike} &mug &pistol &rocket  &\makecell{ skate-\\board} & table \\
         \midrule
         PointNet&80.4&83.7 &83.4 &78.7 &82.5 &74.9 &89.6 &73.0 &91.5 &85.9 &80.8 &95.3 &65.2 &93.0 &81.2 &57.9 &72.8 &80.6 \\
         
         PointNet++&81.9&85.1 &82.4 &79.0 &87.7 &77.3 &90.8 &71.8 &91.0 &85.9 &83.7 &95.3 &71.6 &94.1 &81.3 &58.7 &76.4  &82.6 \\
         
         Kd-Net&-&82.3 &80.1 &74.6  &74.3 &70.3 &88.6 &73.5 &90.2 &87.2 &81.0 &94.9 &57.4 &86.7 &78.1 &51.8 &69.9 &80.3 \\
         SO-Net&-&84.9 &82.8 &77.8 &88.0 &77.3 &90.6 &73.5 &90.7 &83.9 &82.8 &94.8 &69.1 &94.2 &80.9 &53.1 &72.9 &83.0 \\
         PCNN&81.8&85.1 &82.4 &80.1 &85.5 &79.5 &90.8 &73.2 &91.3 &86.0 &85.0 &95.7 &73.2 &94.8 &83.3 &51.0 &75.0 &81.8 \\
         DGCNN&82.3&85.2 &84.0 &83.4 &86.7 &77.8 &90.6 &74.7 &91.2 &87.5 &82.8 &95.7 &66.3 &94.9 &81.1 &63.5 &74.5 &82.6 \\
         P2Sequence&-&85.2 &82.6 &81.8 &87.5 &77.3 &90.8 &77.1 &91.1 &86.9 &83.9 &95.7 &70.8 &94.6 &79.3 &58.1 &75.2 &82.8 \\
         PointCNN&84.6&86.1&84.1 &86.5 &86.0 &80.8 &90.6 &79.7 &92.3 &88.4 &85.3 &96.1 &77.2 &95.2 &84.2 &64.2 &80.0 &83.0 \\
         PointASNL&-&86.1 &84.1 &84.7 &87.9 &79.7 &92.2 &73.7 &91.0 &87.2 &84.2 &95.8 &74.4 &95.2 &81.0 &63.0 &76.3 &83.2 \\
         RS-CNN&84.0&86.2 &83.5 &84.8 &88.8 &79.6 &91.2 &81.1 &91.6 &88.4 &86.0 &96.0 &73.7 &94.1 &83.4 &60.5 &77.7 &83.6 \\
         SynSpec&82.0&84.7 &81.6 &81.7 &81.9 &75.2 &90.2 &74.9 &93.0 &86.1 &84.7 &95.6 &66.7 &92.7 &81.6 &60.6 &82.9 &82.1 \\
         SPLATNet&83.7&85.4&83.2 &84.3 &89.1 &80.3 &90.7 &75.5 &92.1 &87.1 &83.9 &96.3 &75.6 &95.8 &83.8 &64.0 &75.5 &81.8 \\
         SpiderCNN&82.4&85.3 &83.5 &81.0 &87.2 &77.5 &90.7 &76.8 &91.1 &87.3 &83.3 &95.8 &70.2 &93.5 &82.7 &59.7 &75.8 &82.8 \\
         KPConv&85.1&86.4 &84.6 &86.3 &87.2 &81.1 &91.1 &77.8 &92.6 &88.4 &82.7 &96.2 &78.1 &95.8 &85.4 &69.0 &82.0 &83.6 \\
         PA-DGC&84.6&86.1 &84.3 &85.0 &90.4 &79.7 &90.6 &80.8 &92.0 &88.7 &82.2 &95.9 &73.9 &94.7 &84.7 &65.9 &81.4 &84.0 \\
         \midrule
        PointMLP&84.6  &86.1  &83.5  &83.4 &87.5 &80.54  &90.3 &78.2 &92.2  & 88.1 &82.6  & 96.2  &77.5  &95.8  &85.4  &64.6  & 83.3 &84.3 \\
         \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table*}





\section{Conclusion}
In this paper, we propose a simple yet powerful architecture named PointMLP for point cloud analysis. The key insight behind PointMLP is that a sophisticated local geometric extractor may not be crucial for performance. We begin with representing local points with simple residual MLPs as they are permutation-invariant and straightforward. Then we introduce a lightweight geometric affine module to boost the performance. To improve efficiency further, we also introduce a lightweight counterpart, dubbed as PointMLP-elite. Experimental results have shown that PointMLP outperforms related work on different benchmarks beyond simplicity and efficiency. We hope this novel idea will inspire the community to rethink the network design and local geometry in point cloud.

\newpage



\bibliography{reference}
\bibliographystyle{iclr2022_conference}

\newpage
\appendix
\section{PointMLP Detail}
We detail the architecture of PointMLP in Figure~\ref{fig:detail_pointmlp} (as well as PointMLP-elite in Figure~\ref{fig:detail_pointmlp_elite}) for a better understanding. Compared with PointMLP, the elite version mainly adjusts three configurations: 1) it reduces the number of residual point (Resp) MLP blocks; 2) it reduces the embedding dimension from 64 to 32, hence the overall model overhead is significantly alleviated; 3) by introducing a bottleneck structure, PointMLP further reduces the parameters by four times.

For part segmentation task, we use the framework presented in PointNet~\citep{qi2017pointnet} and replace the backbone to our PointMLP. With the only modification, we improve the performance from 85.1 to 86.1 Instance mIoU.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{images/pointMLP_detail.pdf}
    \caption{Detail architecture of PointMLP for classification.}
    \label{fig:detail_pointmlp}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{images/PointMLP-elite-details.pdf}
    \caption{Detail architecture of PointMLP-elite for classification.}
    \label{fig:detail_pointmlp_elite}
\end{figure}

\section{Detail Experimental Setting}
\subsection{ModelNet40 and ScanObjectNN}
Our implementations are based on PyTorch. For ModelNet40, we train models for 300 epochs on one Tesla V100 GPU with a batch size of 32. All our models are trained using synchronous SGD with a Nesterov momentum of 0.9 and a weight decay of 0.0002. The learning rate is set to  0.1  initially. We use the cosine annealing scheduler~\citep{loshchilov2016sgdr} to adjust the learning rate. For each sample, we randomly select 1024 points and consider the same augmentation strategy as~\citet{qi2017pointnet++}. The setting for ScanObjectNN is similar to ModelNet40, except we train all models for only 200 epochs.

For the reported speed in Table~\ref{tab:classification-modelnet40}, we test the open-source code on a Tesla V100-pcie GPU. All the source codes we used are listed\footnote{
all tested methods are listed bellow\\
PointNet++: https://github.com/erikwijmans/Pointnet2\_PyTorch \\
CurveNet: https://github.com/tiangexiang/CurveNet \\
GBNet: https://github.com/ShiQiu0419/GBNet \\
GDANet: https://github.com/mutianxu/GDANet \\
PointConv: https://github.com/DylanWusee/pointconv \\
KPConv: https://github.com/HuguesTHOMAS/KPConv-PyTorch \\
}
in the footnote.



\subsection{ShapeNetPart}
Our setting for part segmentation task is following PointNet~\citep{qi2017pointnet}. We randomly sample 2048 points for each sample and re-scale the input in a range of . Note that we did not test the result using a multi-scale testing strategy, which could further improve the performance, but is not realizable in real-world applications. Hence, we only report the single-scale results. Even the comparison is unfair, we still achieve competitive performance.



\section{More Detailed Ablation Studies}
\label{appendix:ablation}
\textit{Skip connection.} Figure~\ref{fig:losssurface} shows the loss landscapes of our PointMLP with and without skip connections. We also consider adding skip connections to PointNet++ to validate the effectiveness of skip connections. Due to the structure of PointNet++, only two skip connections could be added without modifying the original architecture of PointNet++. By adding the skip connections, we achieve a classification accuracy of 92.7\% on ModelNet40 in our re-implementation.


\textit{Pre-MLP block vs. Pos-MLP block.} 
we also modified the configuration of our PointMLP and retrained the model to investigate the importance of Pre-MLP and Pos-MLP blocks. In our original implementation, we set the pre-MLP block list to [2, 2, 2, 2] and the pos-MLP blocks list to . Here, we remove the pos-MLP blocks and change the pre-MLP blocks to  to match the block number. The 3-layer classifier can be considered as the MLP at the end of the last stage. We trained the models two times and got an average OA of 84.13\% (83.87\% and 84.39\%), which is lower than vanilla PointMLP 85.4\%, and even the result in Table~\ref{tab:component} second-row 84.7\%. This result indicates that pos-MLP does benefit our PointMLP, and simply adding more pre-MLP blocks does not help. We acknowledge that the effect of pos-MLP is not as strong as other components and believe that a detailed fine-tuning of the configurations would deliver an even better performance-efficiency balance.


\textit{Geometric Affine Module Applications.} Geometric affine module plays an essential role in our PointMLP, exhibiting promising performance improvements. While this module can be considered as a plug-and-play method, the overlap with some local geometric extractors in other methods may limit its application. Here we integrate the module to two popular methods, PointNet++ and DGCNN, for illustration and experiment on the ModelNet40 benchmark. By integrating the geometric affine module, we improve the performance of PointNet++ to 93.3\%,  achieving an improvement of 1.4\%. However, when integrating the module to DGCNN, we get a performance of 92.8\%, which is slightly lower than the original results (92.9\%). Note that both results are tested without voting. 


\section{PointMLP Depth}
\label{appendix:depth}
Here we format the detailed formulation of layer number in our PointMLP. For the sake of clarity, we ignore Batch Normalization layers and activation functions.
Let   and   indicate the repeating number of the  block (which includes 3 layers) and  block (which includes 2 layers) in -th stage, respectively. Note that we have one layer in feature embedding in the beginning, one layer for channel number matching in each stage, and three layers in the classifier. Hence, the total number of learnable layers  would be 

As a result, the depth configuration of our network (24, 40, and 56) can be summarized as:
\begin{table}[!h]
    \centering
    \begin{tabular}{c|c|c}
    \toprule
       Depth  &  &\\
       \midrule
         24&&  \\
         40& &   \\
         56& & \\
        \bottomrule 
    \end{tabular}
\end{table}

\end{document}
