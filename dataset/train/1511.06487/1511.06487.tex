\documentclass[a4paper,11pt]{article}   \usepackage{authblk} \usepackage[top=1.9cm,bottom=1.9cm,left=1.9cm,right=1.9cm]{geometry}

\usepackage{xspace}
\usepackage{tikz}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{threeparttable}
\usetikzlibrary{mindmap}

\newcommand{\progname}{\textsf}
\newcommand{\compname}{\emph} 

\newcommand{\HTCondor}{\progname{HTCondor}\xspace}
\newcommand{\MW}{\progname{MW}\xspace}
\newcommand{\zram}{\progname{ZRAM}\xspace}
\newcommand{\tbb}{\progname{TBB}\xspace}
\newcommand{\bpp}{\progname{Bob++}\xspace}
\newcommand{\pth}{\progname{Pthread}\xspace}
\newcommand{\prs}{\progname{prs}\xspace}
\newcommand{\lrs}{\progname{lrs}\xspace}
\newcommand{\plrs}{\progname{plrs}\xspace}
\newcommand{\mplrs}{\progname{mplrs}\xspace}
\newcommand{\mts}{\progname{mts}\xspace}
\newcommand{\lrsa}{\progname{lrs1}\xspace}
\newcommand{\mplrsa}{\progname{mplrs1}\xspace}
\newcommand{\norm}{\progname{normaliz}\xspace}
\newcommand{\normaliz}{\progname{normaliz}\xspace}
\newcommand{\mai}{\compname{mai12}\xspace}
\newcommand{\mait}{\compname{mai20}\xspace}
\newcommand{\maicl}{\compname{mai}\xspace}
\newcommand{\maitt}{\compname{mai32}\xspace}
\newcommand{\mais}{\compname{mai64}\xspace}
\newcommand{\mainew}{\compname{mai32abcd}\xspace}
\newcommand{\maitf}{\compname{mai24}\xspace}
\newcommand{\maief}{\compname{mai32ef}\xspace}
\newcommand{\cdd}{\progname{cddr+}\xspace}
\newcommand{\porta}{\progname{PORTA}\xspace}
\newcommand{\ppl}{\progname{ppl\_lcdd}\xspace}
\newcommand{\lrslib}{\progname{lrslib}\xspace}
\newcommand{\tsubame}{\compname{Tsubame2.5}\xspace}
\newcommand{\gnuplot}{\progname{gnuplot}\xspace}

\newcommand{\polytope}{\emph}
\newcommand{\bvseven}{\polytope{bv7}\xspace}
\newcommand{\cthirty}{\polytope{c30}\xspace}
\newcommand{\cforty}{\polytope{c40}\xspace}
\newcommand{\fqfour}{\polytope{fq48}\xspace}
\newcommand{\msix}{\polytope{m6}\xspace}
\newcommand{\mitseven}{\polytope{mit71}\xspace}
\newcommand{\mitine}{\polytope{mit}\xspace}
\newcommand{\permten}{\polytope{perm10}\xspace}
\newcommand{\cpsix}{\polytope{cp6}\xspace}
\newcommand{\vffive}{\polytope{vf500}\xspace}
\newcommand{\kmtwo}{\polytope{km22}\xspace}
\newcommand{\vfnine}{\polytope{vf900}\xspace}
\newcommand{\zfw}{\polytope{zfw91}\xspace}


\newcommand{\Adj}{\textrm{Adj}}
\newcommand{\initdepth}{\ensuremath{\mathit{init\_depth}}\xspace}
\newcommand{\mymaxdepth}{\ensuremath{\mathit{max\_depth}}\xspace}
\newcommand{\maxcobases}{\ensuremath{\mathit{max\_cobases}}\xspace}
\newcommand{\lmin}{\ensuremath{\mathit{lmin}}\xspace}
\newcommand{\lmax}{\ensuremath{\mathit{lmax}}\xspace}
\newcommand{\myscale}{\ensuremath{\mathit{scale}}\xspace}
\newcommand{\startvertex}{\ensuremath{\mathit{start\_vertex}}\xspace}
\newcommand{\mydepth}{\ensuremath{\mathit{depth}}\xspace}
\newcommand{\maxthreads}{\ensuremath{\mathit{max\_threads}}\xspace}
\newcommand{\numworkers}{\ensuremath{\mathit{num\_workers}}\xspace}
\newcommand{\mysize}{\ensuremath{\mathit{size}}\xspace}
\newcommand{\maxd}{\ensuremath{\mathit{maxd}}\xspace}
\newcommand{\maxc}{\ensuremath{\mathit{maxc}}\xspace}
\newcommand{\mystart}{\ensuremath{\mathit{start}}\xspace}

\newcommand{\unexplored}{\ensuremath{\mathit{unexplored}}\xspace}
\newcommand{\myfalse}{\ensuremath{\textrm{false}}\xspace}
\newcommand{\mytrue}{\ensuremath{\textrm{true}}\xspace}
\newcommand{\mycount}{\ensuremath{\mathit{count}}\xspace}
\newcommand{\putoutput}{\ensuremath{\textrm{put\_output}}\xspace}
\newcommand{\unfinished}{\ensuremath{\mathit{unfinished}}\xspace}

\newcommand{\numthreads}{\ensuremath{\mathit{mt}}\xspace}

\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{slashbox}
\usepackage{algorithm}
\usepackage{algpseudocode}

\definecolor{darkblue}{rgb}{0,0,0.6}

\clearpage{}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\NNZ}{\NN_0}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\QQP}{\mathbb{Q}^+}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\RRP}{\mathbb{R}^+}
\newcommand{\CC}{\mathbb{C}}




\newcommand{\twocasefunction}[4]{
	\begin{cases}
		#1 & \text{if #3} \\
		#2 & \text{if #4}
	\end{cases}
}
\newcommand{\twocaseotherwisefunction}[3]{
	\begin{cases}
		#1 & \text{if #3} \\
		#2 & \text{otherwise}
	\end{cases}
}




\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\posminus}{{\dot -}}
\newcommand{\id}{1\hspace{-1,5ex}1}






\newcommand{\NP}{\mathtt{NP}}
\newcommand{\coNP}{\mathtt{coNP}}
\newcommand{\UP}{\mathtt{UP}}
\newcommand{\coUP}{\mathtt{coUP}}
\newcommand{\PLS}{\mathtt{PLS}}
\newcommand{\PP}{\mathtt{P}}
\newcommand{\LP}{\mathtt{LP}}
\newcommand{\SAT}{\mathtt{SAT}}
\newcommand{\UNSAT}{\mathtt{UNSAT}}
\newcommand{\EXPEXP}{\mathtt{EXP}}




\newcommand{\ran}[1]{\mathtt{ran}(#1)}
\newcommand{\dom}[1]{\mathtt{dom}(#1)}
\newcommand{\img}[1]{\mathtt{img}(#1)}




\newcommand{\BigO}[1]{\mathcal{O}(#1)}

\clearpage{}
\clearpage{}\newcommand{\bit}{\mathfrak{b}}
\newcommand{\nsb}{\nu}
\newcommand{\nub}{\mu}
\newcommand{\bitset}{\mathcal{B}}
\newcommand{\boldone}{\mathbf{1}}
\newcommand{\boldzero}{\mathbf{0}}

\newcommand{\rew}{\mathtt{rew}}
\newcommand{\rel}{\mathtt{rel}}

\newcommand{\succedge}[3]{\mathrm{succ}_{#1}(#2,#3)}
\newcommand{\mdpval}[1]{\mathtt{val}_{#1}}
\newcommand{\mdppot}[1]{\mathtt{pot}_{#1}}
\newcommand{\pri}[1]{{\langle #1\rangle}}\clearpage{}

\begin{document}

\title{\mplrs: A scalable parallel vertex/facet enumeration code\thanks{This work was partially supported by JSPS 
Kakenhi Grants 16H02785, 23700019 and 15H00847, Grant-in-Aid for Scientific Research 
on Innovative Areas, `Exploring the Limits of Computation (ELC)'.}}

\author[1]{David Avis}
\author[2]{Charles Jordan}
\affil[1]{School of Informatics, Kyoto University, Kyoto, Japan and School of Computer Science,
          McGill University, Montr{\'e}al, Qu{\'e}bec, Canada\\
          \texttt{avis@cs.mcgill.ca}}
\affil[2]{Graduate School of Information Science and Technology, 
          Hokkaido University, Japan\\
          \texttt{skip@ist.hokudai.ac.jp}}

\maketitle

\begin{abstract}
We describe a new parallel implementation, \mplrs, of the vertex enumeration code \lrs that 
uses the MPI parallel environment and can be run on a network of computers. 
The implementation makes use of a C wrapper that essentially uses the existing \lrs code
with only minor modifications. \mplrs was derived from the earlier parallel implementation \plrs,
written by G. Roumanis in C++ which runs on a shared memory machine.
By improving load balancing we are able to greatly improve performance for medium to large
scale parallelization of \lrs.
We report computational results comparing parallel and sequential codes for vertex/facet enumeration problems for convex polyhedra. 
The problems chosen span the range from simple to highly degenerate polytopes. 
For most problems tested, the results clearly show the advantage of using the parallel 
implementation \mplrs of the reverse search based code \lrs, even when as few as 8 cores are available. 
For some problems almost linear speedup was observed up to 1200 cores, the largest number of cores tested.

\noindent{}Keywords: vertex enumeration, reverse search, parallel processing\\
Mathematics Subject Classification (2000) 90C05
\end{abstract}

\section{Introduction}
\label{sec:intro}
The vast majority of mathematical programming software was designed and implemented for the prevalent
computers of the last century, which generally had single processors. Improvements in algorithmic design
and processor speed, in roughly equal measure, led to enormous speed-ups allowing increasingly
large problems to be solved in reasonable time. These legacy computer codes are sophisticated and extremely robust,
having been extensively tested on a wide range of platforms and applications. Previously,  
parallel processing was limited largely to expensive supercomputers. In recent years the situation
has changed radically;  increases in processor speed have been replaced by the ubiquitousness of multicore
processors. Desktop computers usually include at least four CPU cores and relatively inexpensive compute servers provide 
64 cores in a shared memory machine. Networks of such computers readily provide hundreds of available cores.
Unfortunately, very little legacy software for mathematical programming can make effective use of this hardware.

Some algorithms, such as those based on the simplex method, seem inherently sequential and will require new ideas
to exploit large scale parallel processing.
Others, such as integer programming via branch and cut, are basically tree searches that should benefit
greatly from parallelism. For example, the Concorde code for the travelling salesman problem~\cite{concorde,ABCC}
used large scale parallelism to solve extremely large problems to optimality over a distributed network.
General integer programming solvers such as CPLEX~\cite{cplex} and Gurobi~\cite{gurobi} also make use of multicore and distributed computing.
A computational study using Gurobi is contained in Koch et al.~\cite{Koch2012}. Using a shared memory machine, they report
speedups of roughly 9 times with 32 cores and 25 times with 128 cores for integer programming instances tested.
Using a distributed system with 8000 cores they report an estimated speedup of approximately 800 times.
However,
tests by Carle~\cite{Carle} show that results may be very disappointing if some processors are considerably slower than others, even with only 4 or 8 processors\footnote{See posts for December 9, 2014 (Gurobi) and
February 25, 2015 (CPLEX).}. Much work clearly needs to be done in this area.

In this paper we report on the parallelization of \lrs, a tree search algorithm for the {\em vertex/facet enumeration problem}.
The method we developed has the following features which are discussed in detail below:
(a) there is little modification to a complex legacy code;
(b) the parallelism is applied only in a wrapper;
(c) the subproblems are not interrupted;
(d) there is no communication between these threads;
and (e) it works on both shared-memory and distributed systems with essentially no user intervention required.
We also report computational results on a variety of problems and hardware that show near linear speedups, in some cases up to 1200 processors.


Vertex/facet enumeration problems find applications in many areas, of which
we list a few here\footnote{John White prepared a long list of applications
which is available at~\cite{lrs}.}.
Early examples include computing the facets of correlation/cut polyhedra by 
physicists (see, e.g.,~\cite{ceder1994,DL97}) and current research in 
this area relates to detecting quantum behaviour in computers such as D-Wave.
Research on facets of travelling salesman polytopes 
leads to important advances in branch-and-cut algorithms, see, e.g.,~\cite{ABCC}.
For example, Chv\'{a}tal local cuts are derived from
facets of small TSPs and this idea is also seen in the small instance relaxations
of Reinelt and Wenger~\cite{RW2003}.
Vertex enumeration is used to compute all Nash equilibria of bimatrix games and a code
for this based on \lrs is found at~\cite{lrs}. 
Vertex enumeration may be a last resort for minimizing extremely
complicated concave functions.
See, for example, Chapter 3 of Horst et al.~\cite{HPT02}.
This application shows the advantage of getting the output as a stream, most
of which can be immediately discarded.
When doing facet enumeration \lrs automatically computes the volume of the polytope
using much less memory than other methods, such as those described in \cite{FP16}.

The remainder of the paper is organised as follows.
We begin by introducing related work in Section~\ref{sec:relwork} and then proceed to
background on vertex enumeration, reverse search, \lrs and \plrs in Section~\ref{sect:back}.
This is followed in Section~\ref{sec:par} by a discussion of
various parallelization strategies that could be employed to manage the load balancing
problem. In Section~\ref{sec:mplrs} we discuss the implementation of \mplrs and
describe its features.
In Section~\ref{sec:experiments} we give some test results on a wide range of inputs,
comparing 7 codes: \cdd, \norm, \porta, \ppl, \lrs, \plrs and \mplrs and present an
analysis of our findings where we see that \mplrs scales further than other vertex
enumeration codes. Finally in the conclusion we compare our results with
those obtained for parallel integer programming and discuss the wider applicability of
our research.

\section{Related Work}
\label{sec:relwork}

We begin by reviewing the available algorithms and codes for vertex enumeration, focusing in particular on
parallel codes.  Codes for this problem were recently compared by Assarf et al.~\cite{polymake17},
however they focus primarily on sequential codes and therefore utilize comparatively
easy instances.  Then we introduce work on parallel reverse search, and also work on other parallel
search problems that may appear related to \mplrs.

There are basically two algorithmic approaches to the vertex enumeration problem: the Fourier-Motzkin double description method
(see, e.g.,~\cite{Ziegler})
and pivoting methods such as Avis-Fukuda reverse search~\cite{AF92} which enumerates all nodes of a tree.
The double description method involves inserting the half spaces from the H-representation sequentially and
updating the list of vertices that they span. 
Readily available codes for this method include \cdd~\cite{cdd},  
\norm~\cite{norm}, \ppl~\cite{ppl} and PORTA~\cite{porta}.
Although this sequential method did not seem easy to parallelize, it was recently achieved
and implemented in \norm. This breakthrough for the double description method involves
a new technique called pyramidal decomposition \cite{BIS16}. This decomposition is not equivalent to a standard
polyhedral decomposition and is much less costly to compute. We include experimental results
for \norm in Section \ref{sec:experiments}.

\subsection{\lrs and parallelization}

The reverse search method for vertex enumeration was implemented as \lrs~\cite{lrs,lrs2}.
From the outset it was realized that reverse search was eminently suitable for parallelization.
Marzetta developed the first parallel reverse search code using his \zram parallelization
platform~\cite{BMFN99,ZRAMthesis}, and implemented the first parallel vertex enumeration code, \prs, using
this generic reverse search framework. 
Load balancing is performed using a variant of what is now known as job stealing.
Application codes, such as \lrs, were embedded into \zram itself leading
to problems of maintenance as the underlying codes evolved.                     
Although \prs is no longer distributed and was based on a now obsolete version of \lrs,
it clearly showed the potential for large speedups of reverse search algorithms. 
Some limited experimental results for vertex enumeration are given in \cite{BMFN99} 
and these are discussed in Section \ref{parallel}.

The \lrs code is rather complex and has been under development for over twenty
years incorporating a multitude of different functions. It has been used
extensively and its basic functionality is very stable. Directly adding parallelization code
to such legacy software is extremely delicate and can easily produce bugs that
are difficult to find. A high level wrapper avoids this problem by implementing the parallelization
as a separate layer with very few changes to \lrs itself.
This allows the independent development of both parallelization
ideas and basic improvements in the underlying code, both of which stay up to date.
In return for this flexibility there are certain overheads that we discuss later.
However, the focus on \lrs and reverse search minimizes the number of modifications required
compared to using a general framework like \zram, and also allows the use of a load balancing technique
that is both simple and efficient for such codes.

The concept of a high level wrapper along these lines was tested by a shell
script, \progname{tlrs}, developed by White in 2009. Here the parallelization was achieved
by scheduling independent \lrs processes for subtrees via the shell. Although good speedups were obtained,
several limitations of this approach materialized as the number of processors available increased.
In particular job control becomes a major issue: there is no single controlling process.

To overcome these limitations the first author and Roumanis developed \plrs~\cite{AR13}.
This code is a C++ wrapper that compiles in the original \lrslib library essentially maintaining
the integrity of the underlying \lrs code. The parallelization was achieved by multithreading
using the Boost library and was designed to run on shared memory machines with little user interaction.
Experience with the \plrs
code showed good speedups with up to about 16 cores, then reduced
performance after that. 
The goal of \mplrs was to solve this load balancing problem and to move to a distributed
environment which could contain hundreds or thousands of processors.

The differences between \mplrs and \plrs are described in Section \ref{sec:plrs}.  While \prs
is able to run on distributed systems using the MPI layer in \zram, there are many differences
between \prs and \mplrs.  In particular, \prs uses a very different strategy for load balancing
where splitting work is distinct from performing work, splitting is computationally expensive, and
is targeted at cases with comparatively regular search trees (see Section 6.3.2 of \cite{ZRAMthesis}).
This is because the node descriptions are quite large (see Section 4.3.1 of \cite{ZRAMthesis}), and so
it tries to minimize the number of subproblems stored in memory.  \mplrs uses much smaller node descriptions (the cobasis)
and a very different strategy for load balancing, where splitting and performing work are not distinct.
This budgeted tree search results in the much better scaling and performance of \mplrs.
Many other differences between \prs and \mplrs are due to the age of \prs and 
the fact that it is no longer available or maintained.

\subsection{Other parallel codes}

The reverse search framework in \zram was also used to implement a parallel code for
certain quadratic maximization problems~\cite{FKL05}.
In a separate project, 
Weibel \cite{Weibel10} developed a parallel reverse search code to compute Minkowski sums.
This C++ implementation runs on shared memory machines and he obtains linear speedups with up to 8 processors,
the largest number reported.

\zram is a general-purpose framework that is able to handle a number of other applications, such
as branch-and-bound and backtracking, for which there are by now a large number of competing frameworks.
Recent papers by Crainic et al.~\cite{CLR06}, McCreesh et al.~\cite{MP15} and Herrera et al.~\cite{He17}
describe over a dozen such systems. While branch-and-bound may seem similar to reverse search enumeration, there
are fundamental differences. In enumeration it is required to explore the entire
tree whereas in branch-and-bound the goal is to explore as little of the tree as possible
until a desired node is found. The bounding step removes subtrees from consideration and
this step depends critically on what has already been discovered. Hence the order of
traversal is crucial and the number of nodes evaluated varies dramatically depending on this
order. Sharing of information is critical to the success of parallelization.
Similar complications exist in parallel SAT solving~\cite{HW12} and
parallel game tree search~\cite{HSN88}.
These issues do not occur in reverse search enumeration, and so a much lighter wrapper is possible.

Relevant to the heaviness of the wrapper and amount of programming effort required,
a comparison of three frameworks is given in \cite{He17}. The first, \bpp  \cite{Dj06},
is a high level abstract framework, similar in nature to \zram, on top of which the application sits.
This framework provides parallelization with relatively little programming effort
on the application side and can run on a distributed network. 
The second, Threading Building Blocks (\tbb) \cite{Re07},
is a lower level interface providing more control but also 
considerably more programming effort. It runs on a shared memory machine. 
The third framework is the \pth model \cite{Ca08} in which parallelization
is deep in the application layer and migration of threads is done by the operating system.
It also runs on a shared memory machine.
All of these methods use job stealing for load balancing \cite{BL99}. 
In \cite{He17} these three approaches are applied to a global optimization algorithm.
They are compared on a rather small
setup of 16 processors, perhaps due to the shared memory limitation of the last two approaches. 
The authors found that \bpp achieved a disappointing speedup of about 3 times, 
considerably slower than the other two approaches
which achieved near linear speedup. 
Other frameworks include CHiPPS~\cite{YanXu07} for parallel tree search
and \MW~\cite{Goux01}, which uses the \HTCondor framework.
\MW can be used to parallelize existing applications using the master-worker paradigm; one
such application was to quadratic assignment problems~\cite{ABGL02}.


Computational tasks that can be divided into subproblems which can be solved 
independently with no communication
are often called embarrassingly parallel~\cite{WA}.  Many such problems
involve processing an enormous amount of data that can easily be divided,
one prominent example being the SETI@home project~\cite{SETI}.
A recent approach to parallel constraint solvers~\cite{MRR16} (where the
input and output are comparatively small) uses this
as inspiration and initially creates a large number of 
subproblems that are then solved in parallel.  Other approaches to
creating an initial (hopefully balanced) decomposition of the input
include 
cube-and-conquer~\cite{HKWB11},
which uses a lookahead SAT solver to split the original problem into
many subproblems that are solved in parallel by CDCL solvers, and
applying machine learning techniques to parallel AND/OR 
branch-and-bound~\cite{OD17}.  Self-splitting~\cite{FMS14} is a technique
for minimizing communication when the subproblem descriptions are large.
There, each worker performs an identical split of the original problem and
then follows some deterministic rule to decide which portions belong to it.
This is not particularly appropriate in our case, where subproblem descriptions
are small and the major concern is that subproblem difficulty is
highly unbalanced.

Another way to deal with the problem posed by subproblems 
of varying difficulties is dynamic load balancing, where one
can split difficult subproblems during the computation.  Work
stealing~\cite{BL99} is one well-known approach where free workers
can steal portions of work from busy workers.

Parallel search has a long history and many applications~\cite{GK99}.
Topics related to this paper include load balancing techniques~\cite{KGR94,KR87} and estimating the difficulty of
subproblems.  The general idea of developing a lightweight parallel wrapper and
reusing sequential code with minimal changes has been applied in many areas,
including mixed integer programming~\cite{SABHK12} and SAT solving~\cite{BSS15}.

Parallel programming is almost as old as programming itself and there is a wealth of literature
on the subject which we can not cover here. For a modern introduction the reader is referred to Mattson et al.~\cite{MSM}.
Generally, much attention is given to machine architecture, communications between processes,
data sharing, synchronization, interrupts, load balancing and so on. This is essential knowledge for building and
implementing a parallel algorithm from scratch. However our aim was essentially different.
In return for some computational overhead, we would like to use existing sequential code with only
minor modifications. In particular, this eliminates the need for considering most of these topics.
The main issue that remains is load balancing, a topic we discuss in detail throughout the paper.

\section{Background}
\label{sect:back}
\subsection{The vertex/facet enumeration problem}
\label{sect:ve}
The vertex enumeration problem is described as follows.
Given an  matrix  and an  dimensional vector
, a
\emph{convex polyhedron}, or simply \emph{polyhedron},  is defined as:

This description of a polyhedron is known as an \emph{H-representation}.
A \emph{polytope} is a bounded polyhedron. For simplicity in this article
we will assume that the input data  defines 
a polytope which has dimension , i.e.\ it is
full dimensional.
For this it is necessary that . 
A point  is a \emph{vertex} of  if and only if
it is the unique solution to a subset of  inequalities from~(\ref{polyhedron})
solved as equations. Such a subset of inequalities is called a {\em basis}.

The \emph{vertex enumeration problem} is to output all vertices of a
polytope . This list of vertices gives us a \emph{V-representation} of
. The reverse transformation, called the \emph{facet enumeration problem},
takes a V-representation and computes its
H-representation. The two problems are computationally equivalent via polarity.
A polytope is called {\em simple} if each vertex is described by a single basis
and {\em simplicial} if each facet contains exactly  vertices.
A vertex enumeration problem for a simple polytope is called {\em non-degenerate}
as is a facet enumeration problem for a simplicial polytope. Other
such problems are called {\em degenerate}.
Since the two problems are equivalent, we will consider only the vertex enumeration
problem in what follows.

One of the features of these types of enumeration problems is that the output
size varies widely for given
parameters  and . 
It is known that up to scaling by constants, each full dimensional polytope
has a unique  non-redundant  and  representation.
For the bounds given next we assume such representations.
For positive integers  let
\def\lf{\left\lfloor}
\def\rf{\right\rfloor}

McMullen's Upper Bound Theorem (see, e.g.,~\cite{Ziegler})
states that for a polytope whose -representation has parameters  
the maximum number of vertices it can have is . This bound is tight
and is achieved by the class of cyclic polytopes.
By inverting the formula and using polarity we can get lower bounds on the number of vertices of a polytope.
We have:

The first inequality follows because a polytope with fewer than this number of vertices must have less than  facets.
For example, suppose  and . Then we have . 

Pivoting methods compute the bases of a polytope and this number can be much larger than the upper bound in (\ref{bt}). 
However, as described in the next subsection, \lrs uses lexicographic pivoting which is equivalent to a symbolic perturbation
of the polytope into a simple polytope. Hence  is a tight upper bound on the number of
bases computed. Since we only require each vertex once, highly degenerate polytopes will
cause large overhead for pivoting methods. 

\subsection{Reverse search and \lrs}
\label{sec:lrs}

Reverse search is a technique for generating large, relatively unstructured, sets of discrete
objects. We give an outline of the method here and refer the reader to~\cite{AF92,AF93}
for further details.
In its most basic form, reverse search can be viewed as the traversal of a spanning tree, called the reverse
search tree , of a graph  whose nodes are the objects to be generated. Edges in the graph are
specified by an adjacency oracle, and the subset of edges of the reverse search tree are
determined by an auxiliary function, which can be thought of as a local search function  for an
optimization problem defined on the set of objects to be generated. One vertex, , is designated
as the \emph{target} vertex. For every other vertex ,
repeated application of  must generate a
path in  from  to . The set of these paths defines the reverse search tree , which has root .

A reverse search is initiated at , and only edges of the reverse search tree are traversed.
When a node is visited the corresponding object is output.  Since there is no possibility of
visiting a node by different paths, the nodes are not stored.  Backtracking can be performed in the
standard way using a stack, but this is not required as the local search function can be used for
this purpose. This implies two critical features that are essential for effective parallelization.
Firstly, it is not necessary to store more than one node of the tree at any
given time and no database is required for visited nodes. 
Secondly, it is possible to \emph{restart} the enumeration process from
any given node in the tree using only a description of this one node.
This contrasts with standard depth first search algorithms for which restart
is only possible with a complete database of visited nodes as well as the backtrack stack
to the root of the search tree.

In the basic setting described here a few properties are required. Firstly, the
underlying graph  must be connected and an upper bound on the maximum vertex degree, , must
be known.  The performance of the method depends on  having  as low as
possible.  The adjacency oracle must be capable of generating the adjacent vertices of some given
vertex  sequentially and without repetition.  This is done by specifying a function  
, where  is a vertex of  and .  Each value of  is
either a vertex adjacent to  or null. Each vertex adjacent to  appears precisely once as  ranges
over its possible values.  For each vertex 
the local search function  returns the tuple  where  such that 
is 's parent in .
Pseudocode is given in Algorithm~\ref{rsalg1}.
Note that the vertices are output as a continuous stream.
For convenience later, we do not output the root vertex  in the pseudocode shown.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{rs}{, , , }
        \State ~~~~~~
        \Repeat
       	\While {}
                \State 
		\If {}  \Comment{forward step}
			\State   
			\State  
                        \State          
			\State {\bf output }         
                \EndIf
        \EndWhile
        \If {}   \Comment{backtrack step}                 
		\State 
                \State        
        \EndIf
        \Until { {\bf and} }
\EndProcedure
\end{algorithmic}
\caption{Generic Reverse Search}
\label{rsalg1}
\end{algorithm}


To apply reverse search to vertex enumeration we first
make use of \emph{dictionaries}, as is done for the
simplex method of linear programming.
To get a dictionary for (\ref{polyhedron}) we
add one new nonnegative variable for each inequality:

These new variables are called \emph{slack variables} and the
original variables are called
\emph{decision variables}.

In order to have any vertex at all we must
have , and normally  is significantly larger than~,
allowing us to solve the equations for various sets of variables on
the left hand side.
The variables on the left hand side of a dictionary are called
\emph{basic}, and those on the right hand side are called \emph{non-basic}
or, equivalently, \emph{co-basic}.
We use the notation
 and
.

A \emph{pivot} interchanges one index from  and  and solves the equations
for the new basic variables.
A \emph{basic solution} from a dictionary is obtained by setting
 for all .
It is a \emph{basic feasible solution (BFS)} if  for every slack
variable . A dictionary is called
\emph{degenerate} if it has a slack basic variable .
As is well known, each BFS defines a vertex of  and each vertex of  can
be represented as one or more (in the case of degeneracy) BFSs. 

Next we define the relevant graph  to be used in Algorithm~\ref{rsalg1}.
Each node in  corresponds to a BFS and is labelled with the cobasic set .
Each edge in  corresponds to a pivot between two BFSs. 
Formally we may define the adjacency
oracle as follows. Let  and  be index sets for the current dictionary.
For  and 


A target  for the
reverse search is found by solving a linear program over this dictionary
with any objective function. A new objective
function is then chosen so that
the optimum dictionary is unique and represents .
\lrs uses Bland's least subscript rule for selecting the variable which
enters the basis and a lexicographic ratio test to select the leaving variable.
The lexicographic rule simulates a simple polytope which greatly reduces the number
of bases to be considered. We initiate the reverse search from the unique optimum dictionary.
For more details see the technical description at \cite{lrs}.
\lrs is an implementation of Algorithm~\ref{rsalg1} in exact rational arithmetic
using  as just described.

\subsection{Parallelization and \plrs}
\label{sec:plrs}

The development of \mplrs started from our experiences with \plrs, with
the goal of scaling past the limits of other vertex enumeration codes
while using the existing \lrs code with only minor modifications.
The details of \plrs are described in~\cite{AR13};
here we give a generic description of the parallelization
which is by nature somewhat oversimplified. We will use as an example the tree shown
in Figure~\ref{fig:mitfig} which shows the first two layers of the
reverse search tree for the problem \mitine, an 8-dimensional polytope with
729 facets that will be described in Section~\ref{subsec:expsetup}. 
The weight on each node is
the number of nodes in the subtree 
that it roots. The root of the tree is in the centre and its weight
shows that the tree contains 1375608 nodes, the number of cobases generated
by \lrs. At depth 2 there are 35 nodes but of these, just the four 
underlined nodes
contain collectively about 58\% of the total number of tree nodes.


\begin{figure}[htbp]
\centering
\begin{tikzpicture}[grow cyclic, align=flush center,
    level 1/.style={level distance=3cm,sibling angle=45},
    level 2/.style={level distance=1.6cm,sibling angle=30}]
\node{}
 child { node {}
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
       }
 child { node {}
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
       }
 child { node {}
         child { node {} }
	 child { node {} }
	 child { node {} }
         child { node {} }
         child { node { } }
       }
 child { node {}
         child { node {} }
         child { node { } }
         child { node { } }
       }
 child { node {}
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
       }
 child { node {}
         child { node {} }
         child { node {} }
         child { node {} }
	 child { node {} }
         child { node {} }
       }
 child { node {}
         child { node {} }
         child { node {} }
       }
 child { node {}
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
         child { node {} }
       };
\end{tikzpicture}
\caption{Number of nodes in subtrees at depth 2 for \mitine}
\label{fig:mitfig}
\end{figure}
The method implemented in \plrs proceeds in three phases. 
In the first phase, sometimes called ramp-up in the parallel processing literature,
we generate the reverse search tree  down to a fixed
depth, \initdepth, reporting all nodes to the output stream. In addition, the nodes of the tree 
with depth equal to  \initdepth which are not leaves of 
are stored in a list . 

In the second phase we schedule subtree enumeration for nodes in 
using a user-specified parameter \maxthreads to limit the number of parallel processes. 
For subtree enumeration we use \lrs with a slight
modification to its earlier described restart feature.
Normally, in a restart, \lrs starts at a given restart node at its given 
depth and computes all remaining nodes in the tree .
The simple modification is to supply a depth of zero with the restart node so that the search
terminates when trying to backtrack from this node. 

When the list  becomes empty we move to Phase 3, sometimes called ramp-down,
in which the threads
terminate one by one until there are no more running and the procedure terminates.
In both Phase 2 and Phase 3 we make use of a \emph{collection process} which concatenates the output from the threads
into a single output stream.
It is clear that the only interaction between the parallel threads is the common output
collection process. The only signalling required is when a thread initiates or terminates
a subtree enumeration. 

Let us return to the example in Figure~\ref{fig:mitfig}.
Suppose we set  and . 
A total of 35 nodes are found at this depth. 34 are stored in  and the other, being a leaf, is ignored.
The first 12 nodes are removed from  and scheduled on the 12 threads.
Each time a subtree is completely enumerated the associated thread receives
another node from  and starts again. When  is empty the thread is idle
until the entire job terminates.
To visualize the process refer to Figure~\ref{fig:mit_L_12}.
In this case we have set  to obtain a larger .
The vertical axis shows thread usage and the horizontal axis shows time.
Phase 1 is so short - less than one second - that it does not appear.
Phase 2 lasts about 50 seconds, when all 12 threads are busy.
Phase 3 lasts the remaining 70 seconds as more and more threads become idle.
If we add more cores, only Phase 2 will profit. Even with very many
cores the running time will not drop below 70 seconds and so this technique does not
scale well.
In comparing  Figures~\ref{fig:mitfig} and~\ref{fig:mit_L_12} we see that
the few large subtrees create an undesirably long Phase 3.
Going to a deeper initial depth helps to some extent, but this eventually 
creates an extremely long list  with subsequent increase in overhead
(see~\cite{AR13} for more details). 
Nevertheless \plrs performs very well with up to about 32 parallel threads,
as we will see in Section~\ref{sec:experiments}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figs/mit_L_12.png}
\caption{Processor usage by \plrs on problem \mitine on a 12 core machine, }                          
\label{fig:mit_L_12}
\end{figure}     

In analyzing this method we observe that in Phase 1 there is 
no parallelization, in Phase 2 all available cores are used, and in Phase 3
the level of parallelization drops monotonically as threads terminate.
Looking at the overhead compared with \lrs we see that
this almost entirely consists of the amount of 
time required to restart the reverse
search process. In this case it requires the time to pivot the input matrix to a given 
cobasis, which is not negligible.
However a potentially greater cost occurs when  is empty and threads are idle.
As the number of available processors increase this cost goes up, but the overhead of restarting
remains the same, for given fixed \initdepth.
This leads to conflicting issues in setting the 
critical \initdepth parameter. A larger value implies that: 
\begin{itemize}
\item
only a single thread is working for a longer
time,
\item
the list  will typically be larger requiring more overhead in restarts but,
\item
the time spent in Phase 3 will typically be reduced.
\end{itemize}

The success in parallelization clearly depends on the structure of the tree .
In the worst case it is a path and no parallelization occurs in Phase 2.
Therefore in the worst case we have no improvement  
over \lrs.
In the best case the tree is balanced so that the list  can be
short reducing overhead and all threads terminate at more or less the same time.
Success therefore heavily depends on the structure of the underlying enumeration
problem.  

\section{Load Balancing Strategies}

\label{sec:par}
\plrs generates subproblems
in an initial phase 
based on a user supplied \initdepth parameter.  This tends to perform best
on balanced trees which, in practice, seem rather rare.
In \plrs, workers (except the initial Phase 1 worker) always
finish the subproblem that they are assigned.  However, there is
no guarantee that subproblems have similar sizes and as we have seen 
they can differ dramatically.  As we saw earlier, this can lead to a major loss of parallelism
after the queue  becomes empty. 
Load balancing is the efficient distribution of work among a number of processors and is 
a well-studied area of parallel computation, see for example Shirazi et al.~\cite{SLB}.
The constraints of our parallelization approach described in the Introduction, such as no
interrupts or communication between subprocesses, greatly limits the methods available.
In this section we discuss various strategies we tried in developing \mplrs.
In particular, we focus~on:
\begin{itemize}
 \item estimating the size of subproblems to 
improve scheduling and create reasonably-sized problems,
 \item dynamic creation of subproblems, where we
can split subproblems at any time instead of only during the initial phase,
 \item using budgets for workers, who return after exploring
a budgeted number of nodes adding unfinished subproblems to .
\end{itemize}

\subsection{Subtree Estimation}
\label{subsec:subtree}
A glance at Figure~\ref{fig:mitfig} shows the problem with
using a fixed initial depth to generate the subtrees for :
the tree mass is concentrated on very few nodes. Of course,
increasing \initdepth would decrease the size of the large subtrees.
However, the subtrees can still be unbalanced at the new depth and
this also increases the number of jobs in 
, increasing the restart overhead. Since \lrs has
the capability to estimate subtree size we tried two approaches using that: 
priority scheduling and iterative deepening.

Estimation is possible for
vertex enumeration by reverse
search using Hall-Knuth estimation~\cite{HK}.
From any node a child can be chosen at random and by continuing
in the same way a random path to a leaf is constructed. This leads
to an unbiased estimate of the subtree size from the initial node.
Various methods lead to lower variance, see~\cite{AD00}. 

The first use of estimation we tried was in priority scheduling.
Although finding a schedule that minimizes the total time to complete all work is NP-hard,
good heuristics are available.
One such heuristic is the list decreasing heuristic, analyzed by  Graham \cite{Gr69},
that schedules the jobs in decreasing order by their execution time.
Referring again to 
Figure~\ref{fig:mitfig}
we see that we should schedule those four heaviest subtrees at the start of
Phase 2. Since we do not have the exact values of the subtree sizes we
decided to use the estimation function as a proxy. We then scheduled jobs
from  in a list decreasing manner by estimated tree~size.

A second idea we tried was iterative deepening.
We start by 
setting a threshold value, say , for maximum estimated subtree size.  
Once a node at \initdepth is encountered an estimate of its subtree
size is made. If this exceeds  then we continue to the next layer
of the tree and estimate the subtree sizes again, repeatedly going deeper
in the tree for subtrees whose estimates
exceed . In this way all nodes returned to  will have estimated
subtree sizes smaller than .

The results from these two approaches were mixed. There are two negative
points. One is that Hall-Knuth estimates have very high variance, and
the true value tends to be larger than the estimate in probability.
So very large subtrees receiving small estimates would not be scheduled
first in priority scheduling and would not
be broken up by iterative deepening.
Secondly, the nodes visited during the random probes
represent overhead, as these nodes will all be visited again later.
In order to improve the quality of the estimate a large number of probes
need to be made, increasing this overhead.

Nevertheless this seems to be an interesting area of research. Newer
more reliable estimation techniques that
do not result in much overhead, such as the on-the-fly methods of~\cite{CKL}
and~\cite{KSW}, may greatly improve the effectiveness of these approaches.

\subsection{Dynamic Creation of Subproblems}
\label{subsec:dynamic}

As we saw in Section~\ref{sec:plrs}, \plrs creates new subproblems only
during the initial phase.  We can think in terms of one boss, 
who creates subproblems in Phase 1, and a set of workers who 
start work in Phase~2 and 
each works on a single subproblem until it is completed.
However, there is no reason why an individual worker cannot
send some parts of its search tree back to  without exploring them.

A simple example of this is to implement a  parameter. 
This is set at some integer value   and subtrees rooted
at every -th node explored are 
sent back to  without exploration. The boss
can set the  parameter dynamically when allocating work from .
If  is getting dangerously small, then a small value is set.
Conversely if  is very large an extremely large value is set.

We implemented this idea but did not get good results. 
When the 
parameter is set then all subtrees are split into smaller pieces,
even the small subtrees, which is undesirable.
When 
is too small, the list  quickly becomes unmanageably large
with very high overhead. 
It seemed hard for the boss to control the size of 
by varying the size of the parameter, due to the
delay incurred before the new parameter propagated to all the workers.

\subsection{Budgeted Subproblems}
\label{subsec:budg}

The final and 
most successful approach involved limiting
the amount of work a worker could do before being
required to quit. Each worker is given a \emph{budget}
which is the maximum number of nodes that can be visited.
Once this budget is exceeded the worker backtracks to the root
of its subtree returning all unfinished subproblems. These consist of all unexplored
children of nodes in the backtrack path.  This has several advantages.
Firstly, if the subtree has size less than the budget (typically 5000 nodes in practice)
then the entire subtree is evaluated without additional creation of overhead.
Secondly, each large subtree automatically gets split up. By including all
unexplored subtrees back to the root a variable number of jobs will be added to .
A giant subtree will be split up many times. For example, the
subtree with 308626 nodes in Figure~\ref{fig:mitfig} will be split over 600 times, providing
work for idle workers. We can also change the budget dynamically to obtain different effects.
If the budget is set to be small we immediately create many new jobs for . If
 grows large we can increase the budget: since most subtrees will be below
the threshold the budget is not used up and  new jobs are not created. 

Budgeting can be introduced to the generic reverse search procedure of Algorithm~\ref{rsalg1}
as follows.
When calling the reverse search procedure we now supply three additional parameters: 
\begin{itemize}
\item
\startvertex is the vertex from which the reverse search should be initiated and replaces ,
\item
\mymaxdepth is the depth at which forward steps are terminated,
\item
\maxcobases is the number of nodes to generate before terminating and reporting unexplored subtrees.
\end{itemize}
Both \mymaxdepth and \maxcobases are assumed to be positive, for otherwise there is no work to do.
The modified algorithm is shown in Algorithm~\ref{brs}. 

\begin{algorithm}[htb]
\begin{algorithmic}[1]
\Procedure{brs}{\startvertex, , , , \mymaxdepth, \maxcobases }
        \State 
        \Repeat
        \State 
        \While { {\bf and}  }
                \State 
                \If {}  \Comment{forward step}
                        \State 
                        \State  
                        \State 
                        \State 
                        \If { {\bf or} } \Comment{budget is exhausted}
                            \State 
                        \EndIf
                        \State 
                \EndIf
        \EndWhile
        \If {}   \Comment{backtrack step}
                \State 
                \State 
        \EndIf
        \Until { {\bf and} }
\EndProcedure
\end{algorithmic}
\caption{Budgeted Reverse Search}
\label{brs}
\end{algorithm}
Comparing Algorithm~\ref{rsalg1} and Algorithm~\ref{brs} we note several changes. 
Firstly, an integer variable \mycount is introduced to keep track
of how many tree nodes have been generated. 
Secondly, a flag \unexplored is introduced to distinguish the tree nodes which have
not been explored and which are to be placed on . It is initialized as \myfalse on line 4.
The flag is set to \mytrue in line 13 if either 
the budget of \maxcobases has been exhausted
or a depth of \mymaxdepth has been reached. 
In any case, each node encountered on a forward step is output via the routine \putoutput
on line 15.
In single-processor mode the output is simply sent to the output file with a flag added
to unexplored nodes. In multi-processor mode, the output is synchronized and
unexplored nodes are returned to  (cf. Section~\ref{sec:mplrs}).

Backtracking is as in Algorithm~\ref{rsalg1}.
After each backtrack step the \unexplored flag is set to \myfalse in line 4.
If the budget constraint has been exhausted then \unexplored will again be set
to \mytrue in line 13 after the first forward step.
In this way all unexplored siblings of nodes on the backtrack path to the root are flagged
and placed on . If the budget is not yet exhausted, forward steps continue until the budget is exhausted, \mymaxdepth is reached, or we reach a leaf.

To output all nodes in the subtree of  rooted at  we set  
,  and .
So if  this reduces to Algorithm~\ref{rsalg1}.
For budgeted subtree enumeration we set \maxcobases to be the worker's budget.
To initialize the parallelization process we will generate the tree  down to a 
a small fixed depth with a small budget constraint in order to generate a
lot of subtrees. We then increase the budget constraint 
and remove the depth constraint so that most workers will finish the tree they
are assigned without returning any new subproblems for .
Since subproblems are dynamically created, it is not necessary to have a long
Phase 1.  By default, \mplrs logs the time spent in Phase 1 and this time was
insignificant in all runs considered in this paper.
The details are given in Section~\ref{subsec:mplrs_master}.


\section{Implementation of \mplrs}
\label{sec:mplrs}

The primary goals of \mplrs were to move beyond single, shared-memory
systems to clusters and improve load balancing when a large number of cores is available.
The implementation uses MPI, and starts a user-specified number of
processes on the cluster.  One of these processes becomes the \emph{master},
another becomes the \emph{consumer}, and the remaining processes are
\emph{workers}.

The master process is responsible for distributing the input file and
parametrized subproblems to the workers, informing the other processes to exit
at the appropriate time, and handling checkpointing.  The consumer
receives output from the workers and produces the output file.  The workers
receive parametrized subproblems from the master, run the \lrs
code,  send output to the consumer, and return unfinished subproblems 
to the master if the budget has expired.

\subsection{Master Process}
\label{subsec:mplrs_master}

The master process begins by sending the input to all workers,
which may not have a shared file system.
In \mplrs,  and  are defined as in
Section~\ref{sec:lrs} and so it suffices to send the input polyhedron.
Pseudocode for the master is given
in Algorithm~\ref{alg:mplrs_master}.

Since we begin from a single \startvertex, the master chooses an
initial worker and sends it the initial subproblem.  We cannot yet
proceed in parallel, so the master uses user-specified (or
very small default) 
initial parameters
\initdepth and \maxcobases to ensure that this worker will
return (hopefully many) unfinished subproblems quickly.
The master then executes its main loop, which it continues until
no workers are running and the master has no unfinished subproblems.
Once the main loop ends, the master informs all processes to finish.
The main loop performs the following tasks:
\begin{itemize}
 \item if there is a free worker and the master has a subproblem, subproblems
       are sent to workers;
 \item we check if any workers are finished, mark them as free and receive 
       their unfinished subproblems.
\end{itemize}
\begin{algorithm}[htb]
 \caption{Master process}
 \label{alg:mplrs_master}
 \begin{algorithmic}[1]
  \Procedure{mprs}{\startvertex, , , ,
   \initdepth, \mymaxdepth, \maxcobases, \lmin, \lmax, \myscale, \numworkers}
   \State {\bf Send} (, , ) to each worker
   \State {\bf Create empty list} 
   \State 
   \State {\bf Send} (\startvertex, \initdepth,
                      \maxcobases) to worker 
   \State {\bf Mark}  as working

   \While { is not empty or some worker is marked as working}
          \While { is not empty and some worker not marked as working}
             \If {} 
                \State 
             \Else \State 
             \EndIf
             \If {} 
                \State 
             \Else \State 
             \EndIf
          \State {\bf Remove} next element \mystart from 
          \State {\bf Send} (\mystart, \maxd, \maxc)
                            to first free worker 
          \State {\bf Mark}  as working
          \EndWhile

          \For {each marked worker }
            \State {\bf Check} for new message \unfinished from 
            \If {incoming message \unfinished from }
               \State{\bf Join} list \unfinished to 
               \State{\bf Unmark}  as working
            \EndIf
          \EndFor
     \EndWhile
     \State {Send} \texttt{terminate} to all processes
  \EndProcedure
 \end{algorithmic}
\end{algorithm}

Using reasonable parameters is critical to achieving good parallelization.
As described in Section~\ref{subsec:budg}, this is done dynamically
by observing the size of . We use the parameters \lmin, \lmax and \myscale.
Initially, to create a reasonable size list , we set   and .
Therefore the initial worker will generate subtrees at depth 2 until 50 nodes have
been visited and then backtrack. Additional workers are given the same aggressive parameters
until  grows larger than \lmax times the number of processors. We now multiply
the budget by \myscale and remove the \mymaxdepth constraint. Currently  so
workers will
not generate any new subproblems unless their tree has at least 5000 nodes.
If the length of  drops below this bound we return to the earlier value of
 and if it drops below \lmin times the size of  we
reinstate the \mymaxdepth constraint. The current default is to set .
In Section~\ref{subsec:histo} we show an example of how the length of 
typically behaves with these parameter settings. 

\subsection{Workers}

The worker processes are simpler -- they receive the problem at
startup, and then repeat their main loop: receive a
parametrized subproblem from the master, work on it subject to the
parameters, send the output to the consumer, and send unfinished
subproblems to the master if the budget is exhausted.  

\begin{algorithm}[ht!]
 \caption{Worker process}
 \label{alg:mplrs_worker}
 \begin{algorithmic}[1]
  \Procedure{worker}{}
   \State{\bf Receive} (, , ) from master
   \While {\mytrue}
     \State{Wait for} message from master
     \If {message is \texttt{terminate}}
       \State {\bf Exit}
     \EndIf
     \State{\bf Receive} (\startvertex, \mymaxdepth, \maxcobases) 
     \State{\bf Call} BRS(\startvertex, , , , \mymaxdepth,  \maxcobases)
     \State{\bf Send} list of unfinished vertices to master
     \State{\bf Send} output list to consumer
   \EndWhile
  \EndProcedure
 \end{algorithmic}
\end{algorithm}

\subsection{Consumer Process}

The consumer process in \mplrs is the simplest.  The workers send output
to the consumer in exactly the format it should be output (i.e., this
formatting is done in parallel).  The consumer simply sends it to an
output file, or prints it if desired.  By synchronizing output to a
single destination, the consumer delivers a continuous output
stream to the user in the same way as 
\lrs does.

\begin{algorithm}
 \caption{Consumer process}
 \label{alg:mplrs_consumer}
 \begin{algorithmic}[1]
  \Procedure{consumer}{}
   \While {\mytrue}
    \State{\bf Wait} for incoming message
    \If {message is \texttt{terminate}}
     \State {\bf Exit}
    \EndIf
    \State{\bf Output} this message
   \EndWhile
  \EndProcedure
 \end{algorithmic}
\end{algorithm}

\subsection{Histograms}
\label{subsec:histo}

There are additional features supported by \mplrs that
are minor additions to 
Algorithms~\ref{alg:mplrs_master}--\ref{alg:mplrs_consumer}.
We introduce \emph{histograms} in this subsection, before proceeding
to checkpoints in Section~\ref{subsec:checkp}.

When desired, \mplrs can provide a variety of information in a
histogram file.  Periodically, the master process
adds a line to this file, containing the following information:
\begin{itemize}
 \item real time in seconds since execution began,
 \item the number of workers marked as working,
 \item the current size of  (number of subproblems the master has).
\end{itemize}

We use this histogram file with \gnuplot to produce
plots that help understand how much parallelization is achieved over
time, which helps when tuning parameters.  Examples of the resulting output
are shown in Figure~\ref{fig:hist_sample}. The problem, \mitseven, 
is a degenerate 60-dimensional polytope with 71 facets
and is described in
Section \ref{subsec:expsetup}. 

\begin{figure}[h!tb]
\centering
\begin{subfigure}[b]{0.47\textwidth}
 \centering
 \resizebox{\textwidth}{!}{
  \begingroup
  \makeatletter
  \providecommand\color[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package color not loaded in conjunction with
      terminal option `colourtext'}{See the gnuplot documentation for explanation.}{Either use 'blacktext' in gnuplot or load the package
      color.sty in LaTeX.}\renewcommand\color[2][]{}}\providecommand\includegraphics[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package graphicx or graphics not loaded}{See the gnuplot documentation for explanation.}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}\renewcommand\includegraphics[2][]{}}\providecommand\rotatebox[2]{#2}\@ifundefined{ifGPcolor}{\newif\ifGPcolor
    \GPcolortrue
  }{}\@ifundefined{ifGPblacktext}{\newif\ifGPblacktext
    \GPblacktexttrue
  }{}\let\gplgaddtomacro\g@addto@macro
\gdef\gplbacktext{}\gdef\gplfronttext{}\makeatother
  \ifGPblacktext
\def\colorrgb#1{}\def\colorgray#1{}\else
\ifGPcolor
      \def\colorrgb#1{\color[rgb]{#1}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color[rgb]{1,0,0}}\expandafter\def\csname LT1\endcsname{\color[rgb]{0,1,0}}\expandafter\def\csname LT2\endcsname{\color[rgb]{0,0,1}}\expandafter\def\csname LT3\endcsname{\color[rgb]{1,0,1}}\expandafter\def\csname LT4\endcsname{\color[rgb]{0,1,1}}\expandafter\def\csname LT5\endcsname{\color[rgb]{1,1,0}}\expandafter\def\csname LT6\endcsname{\color[rgb]{0,0,0}}\expandafter\def\csname LT7\endcsname{\color[rgb]{1,0.3,0}}\expandafter\def\csname LT8\endcsname{\color[rgb]{0.5,0.5,0.5}}\else
\def\colorrgb#1{\color{black}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color{black}}\expandafter\def\csname LT1\endcsname{\color{black}}\expandafter\def\csname LT2\endcsname{\color{black}}\expandafter\def\csname LT3\endcsname{\color{black}}\expandafter\def\csname LT4\endcsname{\color{black}}\expandafter\def\csname LT5\endcsname{\color{black}}\expandafter\def\csname LT6\endcsname{\color{black}}\expandafter\def\csname LT7\endcsname{\color{black}}\expandafter\def\csname LT8\endcsname{\color{black}}\fi
  \fi
    \setlength{\unitlength}{0.0500bp}\ifx\gptboxheight\undefined \newlength{\gptboxheight}\newlength{\gptboxwidth}\newsavebox{\gptboxtext}\fi \setlength{\fboxrule}{0.5pt}\setlength{\fboxsep}{1pt}\begin{picture}(5040.00,3600.00)\gplgaddtomacro\gplbacktext{\csname LTb\endcsname \put(814,704){\makebox(0,0)[r]{\strut{}}}\put(814,1023){\makebox(0,0)[r]{\strut{}}}\put(814,1343){\makebox(0,0)[r]{\strut{}}}\put(814,1662){\makebox(0,0)[r]{\strut{}}}\put(814,1981){\makebox(0,0)[r]{\strut{}}}\put(814,2300){\makebox(0,0)[r]{\strut{}}}\put(814,2620){\makebox(0,0)[r]{\strut{}}}\put(814,2939){\makebox(0,0)[r]{\strut{}}}\put(946,484){\makebox(0,0){\strut{}}}\put(1562,484){\makebox(0,0){\strut{}}}\put(2178,484){\makebox(0,0){\strut{}}}\put(2795,484){\makebox(0,0){\strut{}}}\put(3411,484){\makebox(0,0){\strut{}}}\put(4027,484){\makebox(0,0){\strut{}}}\put(4643,484){\makebox(0,0){\strut{}}}}\gplgaddtomacro\gplfronttext{\csname LTb\endcsname \put(176,1821){\rotatebox{-270}{\makebox(0,0){\strut{}cores}}}\put(2794,154){\makebox(0,0){\strut{}time (secs)}}\put(2794,3269){\makebox(0,0){\strut{}Number of cores vs time}}}\gplbacktext
    \put(0,0){\includegraphics{plots/phist-mit71_128-activeworkers}}\gplfronttext
  \end{picture}\endgroup
  }
\caption{Active workers}
 \label{subfig:actwork}
\end{subfigure}
\begin{subfigure}[b]{0.47\textwidth}
 \centering
 \resizebox{\textwidth}{!}{
  \begingroup
  \makeatletter
  \providecommand\color[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package color not loaded in conjunction with
      terminal option `colourtext'}{See the gnuplot documentation for explanation.}{Either use 'blacktext' in gnuplot or load the package
      color.sty in LaTeX.}\renewcommand\color[2][]{}}\providecommand\includegraphics[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package graphicx or graphics not loaded}{See the gnuplot documentation for explanation.}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}\renewcommand\includegraphics[2][]{}}\providecommand\rotatebox[2]{#2}\@ifundefined{ifGPcolor}{\newif\ifGPcolor
    \GPcolortrue
  }{}\@ifundefined{ifGPblacktext}{\newif\ifGPblacktext
    \GPblacktexttrue
  }{}\let\gplgaddtomacro\g@addto@macro
\gdef\gplbacktext{}\gdef\gplfronttext{}\makeatother
  \ifGPblacktext
\def\colorrgb#1{}\def\colorgray#1{}\else
\ifGPcolor
      \def\colorrgb#1{\color[rgb]{#1}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color[rgb]{1,0,0}}\expandafter\def\csname LT1\endcsname{\color[rgb]{0,1,0}}\expandafter\def\csname LT2\endcsname{\color[rgb]{0,0,1}}\expandafter\def\csname LT3\endcsname{\color[rgb]{1,0,1}}\expandafter\def\csname LT4\endcsname{\color[rgb]{0,1,1}}\expandafter\def\csname LT5\endcsname{\color[rgb]{1,1,0}}\expandafter\def\csname LT6\endcsname{\color[rgb]{0,0,0}}\expandafter\def\csname LT7\endcsname{\color[rgb]{1,0.3,0}}\expandafter\def\csname LT8\endcsname{\color[rgb]{0.5,0.5,0.5}}\else
\def\colorrgb#1{\color{black}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color{black}}\expandafter\def\csname LT1\endcsname{\color{black}}\expandafter\def\csname LT2\endcsname{\color{black}}\expandafter\def\csname LT3\endcsname{\color{black}}\expandafter\def\csname LT4\endcsname{\color{black}}\expandafter\def\csname LT5\endcsname{\color{black}}\expandafter\def\csname LT6\endcsname{\color{black}}\expandafter\def\csname LT7\endcsname{\color{black}}\expandafter\def\csname LT8\endcsname{\color{black}}\fi
  \fi
    \setlength{\unitlength}{0.0500bp}\ifx\gptboxheight\undefined \newlength{\gptboxheight}\newlength{\gptboxwidth}\newsavebox{\gptboxtext}\fi \setlength{\fboxrule}{0.5pt}\setlength{\fboxsep}{1pt}\begin{picture}(5040.00,3600.00)\gplgaddtomacro\gplbacktext{\csname LTb\endcsname \put(946,704){\makebox(0,0)[r]{\strut{}}}\put(946,983){\makebox(0,0)[r]{\strut{}}}\put(946,1263){\makebox(0,0)[r]{\strut{}}}\put(946,1542){\makebox(0,0)[r]{\strut{}}}\put(946,1822){\makebox(0,0)[r]{\strut{}}}\put(946,2101){\makebox(0,0)[r]{\strut{}}}\put(946,2380){\makebox(0,0)[r]{\strut{}}}\put(946,2660){\makebox(0,0)[r]{\strut{}}}\put(946,2939){\makebox(0,0)[r]{\strut{}}}\put(1078,484){\makebox(0,0){\strut{}}}\put(1672,484){\makebox(0,0){\strut{}}}\put(2266,484){\makebox(0,0){\strut{}}}\put(2861,484){\makebox(0,0){\strut{}}}\put(3455,484){\makebox(0,0){\strut{}}}\put(4049,484){\makebox(0,0){\strut{}}}\put(4643,484){\makebox(0,0){\strut{}}}}\gplgaddtomacro\gplfronttext{\csname LTb\endcsname \put(176,1821){\rotatebox{-270}{\makebox(0,0){\strut{}}}}\put(2860,154){\makebox(0,0){\strut{}time (secs)}}\put(2860,3269){\makebox(0,0){\strut{}Size of job list  vs time}}}\gplbacktext
    \put(0,0){\includegraphics{plots/phist-mit71_128-sizeL}}\gplfronttext
  \end{picture}\endgroup
  }
\caption{Size of }
 \label{subfig:size_L}
\end{subfigure}
\begin{subfigure}[b]{0.47\textwidth}
 \centering
 \resizebox{\textwidth}{!}{
  \begingroup
  \makeatletter
  \providecommand\color[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package color not loaded in conjunction with
      terminal option `colourtext'}{See the gnuplot documentation for explanation.}{Either use 'blacktext' in gnuplot or load the package
      color.sty in LaTeX.}\renewcommand\color[2][]{}}\providecommand\includegraphics[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package graphicx or graphics not loaded}{See the gnuplot documentation for explanation.}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}\renewcommand\includegraphics[2][]{}}\providecommand\rotatebox[2]{#2}\@ifundefined{ifGPcolor}{\newif\ifGPcolor
    \GPcolortrue
  }{}\@ifundefined{ifGPblacktext}{\newif\ifGPblacktext
    \GPblacktexttrue
  }{}\let\gplgaddtomacro\g@addto@macro
\gdef\gplbacktext{}\gdef\gplfronttext{}\makeatother
  \ifGPblacktext
\def\colorrgb#1{}\def\colorgray#1{}\else
\ifGPcolor
      \def\colorrgb#1{\color[rgb]{#1}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color[rgb]{1,0,0}}\expandafter\def\csname LT1\endcsname{\color[rgb]{0,1,0}}\expandafter\def\csname LT2\endcsname{\color[rgb]{0,0,1}}\expandafter\def\csname LT3\endcsname{\color[rgb]{1,0,1}}\expandafter\def\csname LT4\endcsname{\color[rgb]{0,1,1}}\expandafter\def\csname LT5\endcsname{\color[rgb]{1,1,0}}\expandafter\def\csname LT6\endcsname{\color[rgb]{0,0,0}}\expandafter\def\csname LT7\endcsname{\color[rgb]{1,0.3,0}}\expandafter\def\csname LT8\endcsname{\color[rgb]{0.5,0.5,0.5}}\else
\def\colorrgb#1{\color{black}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color{black}}\expandafter\def\csname LT1\endcsname{\color{black}}\expandafter\def\csname LT2\endcsname{\color{black}}\expandafter\def\csname LT3\endcsname{\color{black}}\expandafter\def\csname LT4\endcsname{\color{black}}\expandafter\def\csname LT5\endcsname{\color{black}}\expandafter\def\csname LT6\endcsname{\color{black}}\expandafter\def\csname LT7\endcsname{\color{black}}\expandafter\def\csname LT8\endcsname{\color{black}}\fi
  \fi
    \setlength{\unitlength}{0.0500bp}\ifx\gptboxheight\undefined \newlength{\gptboxheight}\newlength{\gptboxwidth}\newsavebox{\gptboxtext}\fi \setlength{\fboxrule}{0.5pt}\setlength{\fboxsep}{1pt}\begin{picture}(5040.00,3600.00)\gplgaddtomacro\gplbacktext{\csname LTb\endcsname \put(946,704){\makebox(0,0)[r]{\strut{}}}\put(946,952){\makebox(0,0)[r]{\strut{}}}\put(946,1201){\makebox(0,0)[r]{\strut{}}}\put(946,1449){\makebox(0,0)[r]{\strut{}}}\put(946,1697){\makebox(0,0)[r]{\strut{}}}\put(946,1946){\makebox(0,0)[r]{\strut{}}}\put(946,2194){\makebox(0,0)[r]{\strut{}}}\put(946,2442){\makebox(0,0)[r]{\strut{}}}\put(946,2691){\makebox(0,0)[r]{\strut{}}}\put(946,2939){\makebox(0,0)[r]{\strut{}}}\put(1078,484){\makebox(0,0){\strut{}}}\put(1672,484){\makebox(0,0){\strut{}}}\put(2266,484){\makebox(0,0){\strut{}}}\put(2861,484){\makebox(0,0){\strut{}}}\put(3455,484){\makebox(0,0){\strut{}}}\put(4049,484){\makebox(0,0){\strut{}}}\put(4643,484){\makebox(0,0){\strut{}}}}\gplgaddtomacro\gplfronttext{\csname LTb\endcsname \put(176,1821){\rotatebox{-270}{\makebox(0,0){\strut{}Frequency}}}\put(2860,154){\makebox(0,0){\strut{}Size of subtree}}\put(2860,3269){\makebox(0,0){\strut{}Subtree frequency vs subtree size}}}\gplbacktext
    \put(0,0){\includegraphics{plots/pplotD-mit71_128}}\gplfronttext
  \end{picture}\endgroup
  }
\caption{Distribution of subproblem sizes}
 \label{subfig:plotD}
\end{subfigure}
\begin{subfigure}[b]{0.47\textwidth}
 \centering
 \resizebox{\textwidth}{!}{
  \begingroup
  \makeatletter
  \providecommand\color[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package color not loaded in conjunction with
      terminal option `colourtext'}{See the gnuplot documentation for explanation.}{Either use 'blacktext' in gnuplot or load the package
      color.sty in LaTeX.}\renewcommand\color[2][]{}}\providecommand\includegraphics[2][]{\GenericError{(gnuplot) \space\space\space\@spaces}{Package graphicx or graphics not loaded}{See the gnuplot documentation for explanation.}{The gnuplot epslatex terminal needs graphicx.sty or graphics.sty.}\renewcommand\includegraphics[2][]{}}\providecommand\rotatebox[2]{#2}\@ifundefined{ifGPcolor}{\newif\ifGPcolor
    \GPcolortrue
  }{}\@ifundefined{ifGPblacktext}{\newif\ifGPblacktext
    \GPblacktexttrue
  }{}\let\gplgaddtomacro\g@addto@macro
\gdef\gplbacktext{}\gdef\gplfronttext{}\makeatother
  \ifGPblacktext
\def\colorrgb#1{}\def\colorgray#1{}\else
\ifGPcolor
      \def\colorrgb#1{\color[rgb]{#1}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color[rgb]{1,0,0}}\expandafter\def\csname LT1\endcsname{\color[rgb]{0,1,0}}\expandafter\def\csname LT2\endcsname{\color[rgb]{0,0,1}}\expandafter\def\csname LT3\endcsname{\color[rgb]{1,0,1}}\expandafter\def\csname LT4\endcsname{\color[rgb]{0,1,1}}\expandafter\def\csname LT5\endcsname{\color[rgb]{1,1,0}}\expandafter\def\csname LT6\endcsname{\color[rgb]{0,0,0}}\expandafter\def\csname LT7\endcsname{\color[rgb]{1,0.3,0}}\expandafter\def\csname LT8\endcsname{\color[rgb]{0.5,0.5,0.5}}\else
\def\colorrgb#1{\color{black}}\def\colorgray#1{\color[gray]{#1}}\expandafter\def\csname LTw\endcsname{\color{white}}\expandafter\def\csname LTb\endcsname{\color{black}}\expandafter\def\csname LTa\endcsname{\color{black}}\expandafter\def\csname LT0\endcsname{\color{black}}\expandafter\def\csname LT1\endcsname{\color{black}}\expandafter\def\csname LT2\endcsname{\color{black}}\expandafter\def\csname LT3\endcsname{\color{black}}\expandafter\def\csname LT4\endcsname{\color{black}}\expandafter\def\csname LT5\endcsname{\color{black}}\expandafter\def\csname LT6\endcsname{\color{black}}\expandafter\def\csname LT7\endcsname{\color{black}}\expandafter\def\csname LT8\endcsname{\color{black}}\fi
  \fi
    \setlength{\unitlength}{0.0500bp}\ifx\gptboxheight\undefined \newlength{\gptboxheight}\newlength{\gptboxwidth}\newsavebox{\gptboxtext}\fi \setlength{\fboxrule}{0.5pt}\setlength{\fboxsep}{1pt}\begin{picture}(5040.00,3600.00)\gplgaddtomacro\gplbacktext{\csname LTb\endcsname \put(946,704){\makebox(0,0)[r]{\strut{}}}\put(946,952){\makebox(0,0)[r]{\strut{}}}\put(946,1201){\makebox(0,0)[r]{\strut{}}}\put(946,1449){\makebox(0,0)[r]{\strut{}}}\put(946,1697){\makebox(0,0)[r]{\strut{}}}\put(946,1946){\makebox(0,0)[r]{\strut{}}}\put(946,2194){\makebox(0,0)[r]{\strut{}}}\put(946,2442){\makebox(0,0)[r]{\strut{}}}\put(946,2691){\makebox(0,0)[r]{\strut{}}}\put(946,2939){\makebox(0,0)[r]{\strut{}}}\put(1078,484){\makebox(0,0){\strut{}}}\put(1791,484){\makebox(0,0){\strut{}}}\put(2504,484){\makebox(0,0){\strut{}}}\put(3217,484){\makebox(0,0){\strut{}}}\put(3930,484){\makebox(0,0){\strut{}}}\put(4643,484){\makebox(0,0){\strut{}}}}\gplgaddtomacro\gplfronttext{\csname LTb\endcsname \put(176,1821){\rotatebox{-270}{\makebox(0,0){\strut{}Frequency}}}\put(2860,154){\makebox(0,0){\strut{}Size of subtree}}\put(2860,3269){\makebox(0,0){\strut{}Subtree frequency vs subtree size}}}\gplbacktext
    \put(0,0){\includegraphics{plots/pplotD-mit71_128-small}}\gplfronttext
  \end{picture}\endgroup
  }
\caption{Distribution of \emph{small} subproblem sizes}
\label{subfig:plotDa}
\end{subfigure}
\caption{Histograms for \mitseven with 128 processes}
\label{fig:hist_sample}
\end{figure}

It is useful to compare Figure~\ref{fig:hist_sample}(a)
to Figure \ref{fig:mit_L_12}
showing a typical \plrs run. The long Phase 3 ramp-down
time of \plrs no longer appears. This is due to the budget constraint
automatically breaking up large subtrees and the master redistributing this 
new work to other workers.  The fact that workers are generally not idle  
is necessary for efficient parallelization, but it is
not sufficient: if the job queue is very large the overhead required to start
jobs will dominate and performance is lost. To get information on
this the second histogram,   
Figure~\ref{fig:hist_sample}(b), is of use.
This plot gives the size of , the number of subproblems held by the
master. This histogram is useful to visualize the overall progress of the run
in real time
to see if the parameters are reasonable.
In \mplrs,  is implemented as a stack.
When  falls to a value for the first time, a new (relatively high in
the tree) subproblem is examined for the first time.  If this new subproblem
happens to be large, the size of  can grow dramatically due
to the budget being exhausted by the assigned worker.  The choice of
parameters greatly affects the rate at which new subproblems are created.

A third type of histogram, subtree size, can also be produced as shown
in Figure~\ref{fig:hist_sample}(c). 
This gives the frequency of the sizes of all subtrees whose roots were stored in
the list , which in this case contained a total of 116,491 subtree roots.
We see that for this problem the vast
majority of subtrees are extremely small. The detail of this is
shown in Figure~\ref{fig:hist_sample}(d). These small subtrees
could have been enumerated more quickly than their restart cost alone
 -- if they could have been identified quickly.
This is an interesting research problem.  After about 60 nodes the
distribution is quite flat until the small hump occurring at 5000 nodes.
This is due to the budget limit of 5000 causing a worker to terminate.
The hump continues slightly past 5000 nodes reflecting the additional
nodes the worker visits on the backtrack path back to the root.
It is interesting that most workers completely finish their
subtrees and only very few actually hit the budget constraint.
Histograms such as these may be of interest for
theoretical analysis of the budgeting method. For example, the shape of
the histogram may suggest an appropriate random tree model to study
for this type of problem.

\subsection{Checkpointing}
\label{subsec:checkp}

An important feature of \mplrs is the ability to
checkpoint and restart execution with potentially different
parameters or number of processes.  This allows, for example, users
to tune parameters over time using the histogram file, without discarding
initial results. It is also very useful for very large jobs
if machines need to be
turned off for any reason or if new machines become available.

Checkpointing is easy to implement in \mplrs but to be effective
it depends heavily on the \maxcobases option being set.  
Workers are never aware of
checkpointing or restarting -- as in Algorithm~\ref{alg:mplrs_worker} they 
simply use \lrs to solve given subproblems until their budget runs out.
When the master wishes to checkpoint, it ceases distribution of new subproblems
and tells workers to terminate.  Once all workers have finished and returned
any unfinished subproblems, the master informs the consumer of a checkpoint.
The consumer then sends various counting statistics to the master, which
saves these statistics and  in a \emph{checkpoint file}.
Note that if \maxcobases is not set then each worker must completely
finish the subtree assigned, which may take a very long time.

When restarting from a checkpoint file, the master reloads  from the
file instead of
distributing the initial subproblem.  It informs the consumer of the counting
statistics and then proceeds normally.  Previous output is not re-examined: 
\mplrs assumes that the checkpoint file is correct.

\section{Performance}
\label{sec:experiments}

We describe here some experimental results for the
three codes described in this paper and 4 codes based on the double description method:
\cdd~\cite{cdd}, \norm~\cite{norm}, \porta~\cite{porta} and \ppl~\cite{ppl}.

\subsection{Experimental Setup}
\label{subsec:expsetup}

The tests were performed using the following computers:
\begin{itemize}
\item
\mait: 2x Xeon E5-2690v2 (10-core 3.0GHz), 20 cores, 128GB memory, 3TB hard drive,
\item
\maief: 4x Opteron 6376 (16-core 2.3GHz), 64 cores, 256GB memory, 4TB hard drive,
\item
\mainew: 4 nodes, each containing: 2x Opteron 6376 (16-core 2.3GHz), 32GB memory, 500GB hard drive (128 cores in total),
\item
\mais: 4x Opteron 6272 (16-core 2.1GHz), 64 cores, 64GB memory, 500GB hard drive,
\item
\mai: 2x Xeon X5650 (6-core 2.66GHz), 12 cores, 24GB memory, 60GB hard drive,
\item
\maitf: 2x Opteron 6238 (12-core 2.6GHz), 24 cores, 16GB memory, 600GB RAID5 array,
\item
\tsubame: supercomputer located at Tokyo Institute of Technology, nodes containing: 2x Xeon X5670 (6-core 2.93GHz), 12 cores, 54GB memory, large file systems, dual-rail QDR Infiniband.
\end{itemize}
The first six machines total 312 cores, are located at Kyoto University and connected with gigabit ethernet. They were purchased between 2011-15 for a combined
total of 3.9 million yen (\mnn=d+1i=1,\ldots,mib_idiAdmn=d+1d10!(1,2,3,\ldots,10)p2^p -2p!01\pm 100p^23p-1p^2+pp!0,\pm 1d=22\pm1000,\pm 10, \pm1K_6K_60, \pm 1, \pm 2mn>>>>>>m=134n=1100.20.40.60.8148163264128256512102400.20.40.60.8148163264128256512102400.20.40.60.8148163264128256512102400.20.40.60.81481632641282565121024LLLLL020004000600080001000012000020040060080010001200|L|L\maxcobases=1|L|=\mbox{2,157,153}0200040006000800010000120000100200300400500600700|L|L\maxcobases=10|L|=\mbox{417,272}0100020003000400050006000050100150200250300350400450500550|L|L\maxcobases=100|L|=\mbox{69,422}0200400600800100012001400160018000100200300400500600|L|L\maxcobases=1000|L|=\mbox{30,088}\maxcobases\myscale=100\maxcobases=1,10,100,1000\maxcobases=50\maxcobases=1\maxcobases=1000\maxcobases=100010002000300040005000600070008000900002004006008001000120014001600|L|L\myscale=1|L|=\mbox{3,315,660}01000200030004000500060007000800090000100200300400500600700|L|L\myscale=10|L|=\mbox{683,870}05001000150020002500300035000100200300400500600|L|L\myscale=1000|L|=\mbox{32,721}05001000150020002500300035000100200300400500600700800900|L|L\myscale=10000|L|=\mbox{54,555}\maxcobases=50\maxcobases=50\myscale=1,10,1000,10000\myscale=100\myscale=1\myscale=1000\myscale=10000\maxcobases=50$ 
whenever the job list becomes nearly empty,
which happens frequently in this case.

It would be nice to get a formal relationship between job list size and the budget.
This is likely to be very difficult for the vertex enumeration problem due to vast
differences in search tree shapes. However such results are possible for random
search trees.
In recent work Avis and Devroye \cite{AD17a} analyzed this relationship for 
very large randomly generated Galton-Watson trees.
They showed that, in probability,
the job list size declines as the square root of the increase in budget.

\section{Conclusions}
\label{conclusions}
It is natural to ask what is the limit of the scalability of the current \mplrs?
Very preliminary experiments with \tsubame using up to 2400 cores indicate that this limit may be 
at about 1200 cores. Although budgeting seemed to produce nicely scaled job queue sizes,
there was a limit to the ability of the single producer (and consumer) to keep up with the workers.
While small modifications can perhaps push this limit somewhat further,
this indicates that a higher level `depot' system may be required, where each depot
receives a part of the job queue and acts as a producer with a subset of the available cores.
This could also help avoid overhead related to the interconnect latency, since
many jobs would be available locally and even remote jobs would be transferred 
in blocks.
Similarly the output may need to be collected by several consumers, especially when it is 
extremely large as in \cforty and \mitseven. These are topics for future research.

Finally one may ask if the parallelization method used here could be used
to obtain similar results for other tree search applications.
Indeed we believe it can. In ongoing work \cite{AJ16a} we have prepared a generic framework
called \mts that can be used to parallelize legacy reverse search enumeration codes.
The results presented there for two other reverse search applications
give comparable speedups to the ones we obtained for \mplrs.
We are also extending the range of possible applications by allowing
in \mts a certain amount of shared
information between workers. This allows the possibility of trying this approach
on branch and bound algorithms, game trees, satisfiability solvers, and the like.

\subsubsection*{Acknowledgements}
We thank Kazuki Yoshizoe for
helpful discussions concerning the MPI library
which improved \mplrs' performance.


\bibliographystyle{spmpsci}
\bibliography{paper}

\end{document}
