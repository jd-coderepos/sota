\documentclass[sigconf]{acmart}

\usepackage{multirow}
\usepackage{url}
\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}






\copyrightyear{2022}
\acmYear{2022}
\setcopyright{acmcopyright}\acmConference[MM '22]{Proceedings of the 30th ACM
International Conference on Multimedia}{October 10--14, 2022}{Lisboa, Portugal}
\acmBooktitle{Proceedings of the 30th ACM International Conference on Multimedia
(MM '22), October 10--14, 2022, Lisboa, Portugal}
\acmPrice{15.00}
\acmDOI{10.1145/3503161.3548329}
\acmISBN{978-1-4503-9203-7/22/10}










\begin{document}

\title{A Deep Learning based No-reference Quality Assessment Model for UGC Videos}



\author{Wei Sun}
\affiliation{\institution{Shanghai Jiao Tong University}
  \city{Shanghai}
  \country{China}}
\email{sunguwei@sjtu.edu.cn}

\author{Xiongkuo Min}
\affiliation{\institution{Shanghai Jiao Tong University}
  \city{Shanghai}
  \country{China}}
\email{minxiongkuo@sjtu.edu.cn}

\author{Wei Lu}
\affiliation{\institution{Shanghai Jiao Tong University}
  \city{Shanghai}
  \country{China}}
\email{SJTU-Luwei@sjtu.edu.cn}


\author{Guangtao Zhai}
\affiliation{\institution{Shanghai Jiao Tong University}
  \city{Shanghai}
  \country{China}}
\email{zhaiguangtao@sjtu.edu.cn}






\thanks{Corresponding author: Guangtao Zhai.}















\begin{abstract}
Quality assessment for User Generated Content (UGC) videos plays an important role in ensuring the viewing experience of end-users. Previous UGC video quality assessment (VQA) studies either use the image recognition model or the image quality assessment (IQA) models to extract frame-level features of UGC videos for quality regression, which are regarded as the sub-optimal solutions because of the domain shifts between these tasks and the UGC VQA task. In this paper, we propose a very simple but effective UGC VQA model, which tries to address this problem by training an end-to-end spatial feature extraction network to directly learn the quality-aware spatial feature representation from raw pixels of the video frames. We also extract the motion features to measure the temporal-related distortions that the spatial features cannot model. The proposed model utilizes very sparse frames to extract spatial features and dense frames (i.e. the video chunk) with a very low spatial resolution to extract motion features, which thereby has low computational complexity. With the better quality-aware features, we only use the simple multilayer perception layer (MLP) network to regress them into the chunk-level quality scores, and then the temporal average pooling strategy is adopted to obtain the video-level quality score. We further introduce a multi-scale quality fusion strategy to solve the problem of VQA across different spatial resolutions, where the multi-scale weights are obtained from the contrast sensitivity function of the human visual system. The experimental results show that the proposed model achieves the best performance on five popular UGC VQA databases, which demonstrates the effectiveness of the proposed model. The code is available at \url{https://github.com/sunwei925/SimpleVQA}.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010341.10010342.10010343</concept_id>
       <concept_desc>Computing methodologies~Modeling methodologies</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Modeling methodologies}

\keywords{video quality assessment, UGC videos, deep learning, feature fusion}



\maketitle

\section{Introduction}
\label{introduction}
With the proliferation of mobile devices and wireless networks in recent years, User Generated Content (UGC) videos have exploded over the Internet. It has become a popular daily activity for the general public to create, view, and share UGC videos through various social media applications such as YouTube, TikTok, etc. 
However, UGC videos are captured by a wide variety of consumers, ranging from professional photographers to amateur users, which makes the visual quality of UGC videos vary greatly. In order to ensure the Quality of Experience (QoE) of end-users, the service providers need to monitor the quality of UGC videos in the entire streaming media link, including but not limited to video uploading, compressing, post-processing, transmitting, etc. Therefore, with billions of video viewing and millions of newly uploaded UGC videos every day, an effective and efficient video quality assessment (VQA) model is needed to measure the perceptual quality of UGC videos.

Objective VQA can be divided into full-reference (FR), reduced-reference (RR), and no-reference (NR) according to the amount of pristine video information needed. Since there is no reference video for in-the-wild UGC videos, only NR VQA models are qualified for evaluating their quality. Although NR VQA algorithms \cite{saad2014blind,mittal2015completely,min2020study} have been studied for many years, most of them were developed for Professionally Generated Content (PGC) videos with synthetic distortions, where the pristine PGC videos are shot by photographers using professional devices and are normally of high quality, and the distorted PGC videos are then degraded by specific video processing algorithms such as video compression, transmission, etc. So, previous VQA studies mainly focus on modeling several types of distortions caused by specific algorithms, which makes them less effective for UGC videos with in-the-wild distortions. To be more specific, the emerging UGC videos pose the following challenges to the existing VQA algorithms for PGC videos:

First, the distortion types of UGC videos are diverse. A mass of UGC videos are captured by amateur users, which may suffer various distortion types such as under/over exposure, low visibility, jitter, noise, color shift, etc. These authentic distortions are introduced in the shooting processing and cannot be modeled by the single distortion type, which thereby requires that the VQA models have a more strong feature representation ability to qualify the authentic distortions.
Second, the content and forms of UGC videos are extremely rich. UGC videos can be natural scenes, animation \cite{wang2022subjective}, games \cite{zadtootaghaj2020quality, zadtootaghaj2018nr}, screen content, etc. Note that the statistics characteristics of different video content vary greatly. For example, the natural scenes statistics (NSS) features \cite{mittal2012making,mittal2012no,saad2014blind,mittal2015completely} are commonly used in the previous VQA studies to measure the distortions of natural scene content, but they may be ineffective for computer-generated content like animation or games. In addition, live videos, videoconferencing, etc. are also ubiquitous for UGC videos nowadays, whose quality is severely affected by the network bandwidth. 
Third, due to the advancement of shooting devices, more high resolution \cite{lu2022deep} and high frame rate \cite{madhusudana2021subjective, zheng2022no, zheng2022faver} videos have emerged on the Internet. The various kinds of resolutions and frame rates are also important factors for video quality. What's more, users can view the UGC videos through mobile devices anywhere and at any time, so the display \cite{rehman2015display} and the viewing environment such as ambient luminance \cite{sun2020dynamic}, etc. also affect the perceptual quality of UGC videos to a certain extent. However, these factors are rarely considered by previous studies.

The recently released large-scale UGC VQA databases such as KoNViD-1k \cite{hosu2017konstanz}, YouTube UGC \cite{wang2019youtube}, LSVQ \cite{ying2021patch}, etc. have greatly promoted the development of UGC VQA. Several deep learning based NR VQA models \cite{li2019quality,ying2021patch,wang2021rich,xu2021perceptual,li2021blindly} have been proposed to solve some challenges mentioned above and achieve pretty good performance. However, there are still some problems that need to be addressed. First, the previous studies either use the image recognition model \cite{li2019quality}\cite{ying2021patch} or the pretrained image quality assessment (IQA) models \cite{wang2021rich}\cite{xu2021perceptual}\cite{li2021blindly} to extract frame-level features, which lacks an end-to-end learning method to learn the quality-aware spatial feature representation from raw pixels of video frames. 
Second, previous studies usually extract the features from all video frames and have a very high computational complexity, making them difficult to apply to real-world scenarios. Since there is much redundancy spatial information between the adjacent frames, we argue that there is not necessary to extract the features from all frames. Third, the spatial resolution and frame rate of UGC videos as well as other factors such as the display, viewing environment, etc. are still rarely considered by these studies. However, these factors are very important for the perceptual quality of UGC videos since the contrast sensitivity of the human visual system (HVS) is affected by them.




















\begin{figure*}[!t]
	\centering
	\includegraphics[height=2.2in]{framework.pdf}
	\caption{The network architecture of the proposed model. The proposed model contains the feature extraction module, the quality regression module, and the quality pooling module. The feature extraction module extracts two kinds of features, the spatial features and the motion features.}
	\label{model_framework}
\end{figure*}

In this paper, to address the challenges mentioned above, we propose a very simple but effective deep learning based VQA model for UGC videos. The proposed framework is illustrated in Figure \ref{model_framework}, which consists of the feature extraction module, the quality regression module, and the quality pooling module.
For the feature extraction module, we extract quality-aware features from the spatial domain and the spatial-temporal domain to respectively measure the spatial distortions and motion distortions. Instead of using the pretrained model to extract the spatial features in the previous studies, we propose to train an end-to-end spatial feature extraction network to learn quality-aware feature representation in the spatial domain, which thereby makes full use of various video content and distortion types in current UGC VQA databases.
We then utilize the action recognition network to extract the motion features, which can make up the temporal-related distortions that the spatial features cannot model. Considering that the spatial features are sensitive to the resolution while the motion features are sensitive to the frame rate, we first split the video into continuous chunks and then extract the spatial features and motion features by using a key frame of each chunk and all frames of each chunk but at a low spatial resolution respectively. So, the computational complexity of the proposed model can be greatly reduced.

For the quality regression module, we use the multilayer perception (MLP) network to map the quality-aware features into the chunk-level quality scores, and the temporal average pooling strategy is adopted to obtain the final video quality. In order to solve the problem of quality assessment across different resolutions, we introduce a multi-scale quality fusion strategy to fuse the quality scores of the videos with different resolutions, where the multi-scale weights are obtained from the contrast sensitivity function (CSF) of HVS by considering the viewing environment information. The proposed models are validated on five popular UGC VQA databases and the experimental results show that the proposed model outperforms other state-of-the-art VQA models by a large margin. What's more, the proposed model trained on a large-scale database such as LSVQ \cite{ying2021patch} achieves remarkable performance when tested on the other databases without any fine-tuning, which further demonstrates the effectiveness and generalizability of the proposed model.



In summary, this paper makes the following contributions:
\begin{enumerate}
	\item We propose an effective and efficient deep learning based model for UGC VQA, which includes the feature extraction module, the quality regression module, and the quality pooling module. The proposed model not only achieves remarkable performance on the five popular UGC VQA databases but also has a low computational complexity, which makes it very suitable for practical applications.
	\item The feature extraction module extracts two kinds of quality-aware features, the spatial features for spatial distortions and the spatial-temporal features for motion distortions, where the spatial features are learned from raw pixels of video frames via an end-to-end manner and the spatial-temporal features are extracted by a pretrained action recognition network. 
	\item We introduce a multi-scale quality fusion strategy to solve the problem of quality assessment across different resolutions, where the multi-scale weights are obtained from the contrast sensitivity function of the human visual system by considering the viewing environment information.
\end{enumerate}




\section{Related Work}
\label{related_work}
\subsection{Handcrafted feature based NR VQA Models}

A naive NR VQA method is to compute the quality of each frame via popular NR IQA methods such as NIQE \cite{mittal2012making}, BRISQUE \cite{mittal2012no}, CORNIA \cite{ye2012unsupervised} etc., and then pool them into the video quality score. A comparative study of various temporal pooling strategies on popular NR IQA methods can refer to \cite{tu2020comparative}. The temporal information is very important for VQA. V-BLIINDS \cite{saad2014blind} is a spatio-temporal natural scene statistics (NSS) model for videos by quantifying the NSS feature of frame-differences and motion coherency characteristics. 
Mittal \textit{et al.} \cite{mittal2015completely} propose a training-free blind VQA model named VIIDEO that exploits intrinsic statistics regularities of natural videos to quantify disturbances introduced due to distortions. 
TLVQM \cite{korhonen2019two} extracts abundant spatio-temporal features such as motion, jerkiness, blurriness, noise, blockiness, color, etc. at two levels of high and low complexity. VIDEVAL \cite{tu2021ugc} further combines the selected features from typical NR I/VQA methods to train a SVR model to regress them into the video quality. Since video content also affects its quality, especially for UGC videos, understanding the video content is beneficial to NR VQA. Previous handcrafted feature based methods are difficult to understand semantic information. Hence, some studies \cite{tu2021rapique, korhonen2020blind} try to combine the handcrafted features with the semantic-level features extracted by the pretrained CNN model to improve the performance of NR VQA models. For example, CNN-TLVQM \cite{korhonen2020blind} combines the handcrafted statistical temporal features from TLVQM and spatial features extracted by 2D-CNN model trained for IQA. RAPIQUE \cite{tu2021rapique} utilizes the quality-aware scene statistics features and semantics-aware deep CNN features to achieve a rapid and accurate VQA model for UGC videos.

\subsection{Deep learning based NR VQA Models}
With the release of several large-scale VQA databases \cite{hosu2017konstanz,wang2019youtube,ying2021patch}, deep learning based NR VQA models \cite{kim2018deep, li2019quality, ying2021patch, wang2021rich, xu2021perceptual, li2021blindly, sun2021deep, yi2021attention, cao2021deep} attract many researchers' attention. Liu \textit{et al.} \cite{liu2018end} propose a multi-task BVQA model V-MEON by jointly optimizing the 3D-CNN for quality assessment and compression distortion classification. VSFA \cite{li2019quality} first extracts the semantic features from a pre-trained CNN model and then uses a gated recurrent unit (GRU) network to model the temporal relationship between the semantic features of video frames. 
The authors of VSFA further propose MDVSFA \cite{li2021unified}, which trains the VSFA model on the multiple VQA databases to improve its performance and generalization. 
RIRNet \cite{chen2020rirnet} exploits the effect of motion information extracted from the multi-scale temporal frequencies for video quality assessment. 
Ying \textit{et al.} \cite{ying2021patch} propose a local-to-global region-based NR VQA model that combines the spatial features extracted from a 2D-CNN model and the spatial-temporal features from a 3D-CNN network. Wang \textit{et al.} \cite{wang2021rich} propose a feature-rich VQA model for UGC videos, which measures the quality from three aspects, compression level, video content, and distortion type and each aspect is evaluated by an individual neural network. 
Xu \textit{et al.} \cite{xu2021perceptual} first extract the spatial feature of the video frame from a pre-trained IQA model and use the graph convolution to extract and enhance these features, then extract motion information from the optical flow domain, and finally integrated the spatial feature and motion information via a bidirectional long short-term memory network. 
Li \textit{et al.} \cite{li2021blindly} also utilize the IQA model pre-trianed on multiple databases to extract quality-aware spatial features and the action recognition model to extract temporal features, and then a GRU network is used to model spatial and temporal features and regress them into the quality score.  Wen and Wang \cite{wen2021strong} propose a baseline I/VQA model for UGC videos, which calculates the video quality by averaging the scores of each frame and frame-level quality scores are obtained by a simple CNN network.





\section{Proposed Model}
\label{proposed_model}

The framework of the proposed NR VQA model is shown in Figure \ref{model_framework}, which consists of the feature extraction module, the quality regression module, and the quality pooling module. First, we extract the quality-aware features from the spatial domain and the spatial-temporal domain via the feature extraction module, which are utilized to evaluate the spatial distortions and motion distortions respectively. Then, the quality regression module is used to map the quality-aware features into chunk-level quality scores. Finally, we perform the quality pooling module to obtain the video quality score.

\subsection{Feature Extraction Module}
\label{feature_extraction_module}
In this section, we expect to extract the quality-aware features that can represent the impact of various distortion types and content on visual quality. The types of video distortion can be roughly divided into two categories: the spatial distortions and the motion distortions. The spatial distortions refer to the artifacts introduced in the video frames, such as noise, blur, compression, low visibility, etc. The motion distortions refer to the jitter, lagging due, etc., which are mainly caused by unstable shooting equipment, fast-moving objects, the low network bandwidth, etc. Therefore, we need to extract the quality-aware features from these two aspects.

Note that the characteristics of the spatial features and motion features are quite different. The spatial features are sensitive to the video resolution but insensitive to the video frame rate since the adjacent frames of the video contain lots of redundancy spatial information and higher resolution can represent more abundant high-frequency information, while motion features are the opposite because the motion distortions are reflected on the temporal dimension and these features are usually consistent for local regions of the frames.

Therefore, considering these characteristics, given a video , whose number of frames and frame rate are  and  respectively, we first split the video  into  continuous chunks  at an time interval , where , and there are  frames in each chunk , which is denoted as . Then we only choose a key frame  in each chunk to extract the spatial features and the motion features of each chunk are extracted using all frames in  but at a very low spatial resolution. As a result, we can greatly reduce the computation complexity of the VQA model with little performance degradation.

\subsubsection{Spatial Feature Extraction Module}
\label{spatial_feature_extraction_module}














Given a frame , we denote  as the output of the CNN model  with trainable parameters  applied on the frame . Assume that there are  stages in the CNN model, and  is the output feature maps extracted from the -th stage, where , and , , and  are the height, width, and the number of channels of the feature maps  respectively. In the following, we use the  to replace the  for simplicity.

It is well known that the features extracted by the deep layers of the CNN model contain rich semantic information, and are suitable for representing content-aware features for UGC VQA. Moreover, previous studies indicate that the features extracted by the shallow layers of the CNN models contain low-level information \cite{zeiler2014visualizing, sun2019mc360iqa}, which responds to low-level features such as edges, corners, textures, etc. The low-level information is easily affected by the distortion and is therefore distortion-aware. Hence, we extract the quality-aware features via calculating the global mean and stand deviation of feature maps extracted from all stages of the CNN model.
Then, we apply global average and stand deviation pooling operations on the feature maps :


where  and  are the global means and stand deviation of feature maps  respectively.
Finally, we concatenate the  and  to derive the spital feature representation of our NR VQA model:


\begin{table*}
\small
	\centering
	\renewcommand{\arraystretch}{1}
	\caption{Summary of the benchmark UGC VQA databases. Time duration: Seconds.}
	\label{the_database}
	\begin{tabular}{c|cccccccc}
		\hline
		\hline
		Database & Videos & Scenes & Resolution & Time Duration & Format & Distortion Type & DATA & Environment \\
		\hline
		KoNViD-1k \cite{hosu2017konstanz} & 1,200 & 1,200 & 540p & 8 & MP4 & Authentic & MOS +  & Crowd \\
		YouTube-UGC \cite{wang2019youtube} & 1500 & 1500 & 360p-4K & 20 & YUV, MP4 & Authentic & MOS +  & Crowd \\
		LSVQ \cite{ying2021patch} & 38,811 & 38,811 & 99p-4K & 5-12 & MP4 & Authentic & MOS +  & Crowd \\
		LBVD \cite{chen2019qoe} & 1,013 & 1,013 & 240p-540p & 10 & MP4 & Authentic, Transmission & MOS +  & In-lab \\
		LIVE-YT-Gaming \cite{yu2022subjective} & 600& 600& 360p-1080p& 8-9& MP4& Authentic& MOS & Crowd \\
		\hline
		\hline
	\end{tabular}
	
\end{table*}

\subsubsection{Motion Feature Extraction Module}
We extract the motion features as the complementary quality-aware features since UGC videos are commonly degraded by the motion distortions caused by the unstable shooting equipment or low bit rates in the living streaming or videoconferencing. The spatial features are difficult to handle these distortions because they are extracted by the intra-frames while motion distortions occur in the interframes. Therefore, the motion features are also necessary for evaluating the quality of UGC videos. Here, we utilize the pretrained action recognition model as the motion feature extractor to obtain the motion features of each video chunk. The action recognition model is designed to detect different kinds of action classes, so the feature representation of the action recognition network can reflect the motion information of the video to a certain extent. Therefore, given the video chunk  and the action recognition network , we can obtain the motion features:

where  represents the motion features extract by the action recognition network.

Therefore, given the video chunk , we first select a key frame in the chunk to calculate the spatial features . Then, we calculate the motion features  using the whole frames but at a low spatial resolution in the video chunk. Finally, we obtain the quality-aware features for the video chunk  by concatenating the spatial features and motion features:


\subsection{Quality Regression Module}
After extracting quality-aware feature representation by the feature extraction module, we need to map these features to the quality scores via a regression model. In this paper, we use the multi-layer perception (MLP) as the regression model to obtain the chunk-level quality due to its simplicity and effectiveness. The MLP consists of two fully connected layers and there are 128 and 1 neuron in each layer respectively. Therefore, we can obtain the chunk-level quality score via

where  denotes the function of the two FC layers and  is the quality of the video chunk.



\subsection{Quality Pooling Module}
As stated in Section \ref{feature_extraction_module}, we split the video  into  continuous chunks . For the chunk , we can obtain its chunk-level quality score  via the feature extraction module and the quality regression module. Then, it is necessary to pool the chunk-level scores into the video level. Though many temporal pooling methods have been proposed in literature \cite{tu2020comparative}\cite{li2019quality}, we find that the temporal averaging pooling achieves the best performance from Section \ref{quality_regression_module}. Therefore, the video-level quality is calculated as:

where  is the quality of the -th chunk and  is the video quality evaluated by the proposed model.


\subsection{Loss Function}
The loss function used to optimize the proposed models consists of two parts: the mean absolute error (MAE) loss and rank loss \cite{wen2021strong}. The MAE loss is used to make the evaluated quality scores close to the ground truth, which is defined as:

where the  is the ground truth quality score of the -th video in a mini-batch and  is the number of videos in the mini-batch.

The rank loss is further introduced to make the model distinguish the relative quality of videos better, which is very useful for the model to evaluate the videos with similar quality. Since the rank value between two video quality is non-differentiable, we use the following formula to approximate the rank value:

where  and  are two video indexes in a mini-batch, and  is formulated as:


Then,  is calculated via:



Finally, the loss function can be obtained by:

where  is a hyper-parameter to balance the MAE loss and the rank loss.










\begin{table*}
\centering
\renewcommand{\arraystretch}{1}
\caption{Performance of the SOTA models and the proposed model on the KoNViD-1k, YouTube-UGC, LBVD, and LIVE-YT-Gaming databases. W.A. means the weight average results. The best performing model is highlighted in each column.}
\label{performance}
\begin{tabular}{c|c|cc|cc|cc|cc|cc}
\toprule[.15em]
\multirow{2}{*}{Type} & Database & \multicolumn{2}{c|}{KoNViD-1k} & \multicolumn{2}{c|}{YouTube-UGC} & \multicolumn{2}{c|}{LBVD} & \multicolumn{2}{c|}{LIVE-YT-Gaming} & \multicolumn{2}{c}{W.A.}\\
 & Criterion & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC& SRCC & PLCC \\
\hline
\multirow{6}{*}{IQA}&NIQE & 0.542 & 0.553& 0.238& 0.278& 0.327& 0.387 & 0.280& 0.304& 0.359 & 0.393 \\
&BRISQUE & 0.657& 0.658& 0.382& 0.395& 0.435& 0.446& 0.604&0.638 & 0.513 &0.525 \\
&GM-LOG  & 0.658& 0.664& 0.368& 0.392& 0.314& 0.304& 0.312& 0.317& 0.433 &0.440 \\
&VGG19 & 0.774& 0.785& 0.703& 0.700& 0.676& 0.673& 0.678&0.658 & 0.714 &0.712 \\
&ResNet50 & 0.802& 0.810& 0.718& 0.710& 0.715& 0.717& 0.729& 0.768& 0.744 &0.751 \\
&KonCept512 & 0.735& 0.749& 0.587& 0.594& 0.626& 0.636& 0.643&0.649 & 0.650 &0.660 \\
\hline
\multirow{6}{*}{VQA}&V-BLIINDS  & 0.710& 0.704& 0.559& 0.555& 0.527& 0.558& 0.357& 0.403& 0.566 &0.578 \\
& TLVQM& 0.773& 0.769& 0.669& 0.659& 0.614& 0.590& 0.748& 0.756&  0.699 & 0.689 \\
&VIDEVAL & 0.783& 0.780& 0.779& 0.773& 0.707& 0.697& 0.807& 0.812& 0.766 &0.762 \\
&RAPIQUE & 0.803& 0.818& 0.759& 0.768& 0.712& 0.725& 0.803&0.825 & 0.767 &0.781 \\
&VSFA & 0.773& 0.775& 0.724& 0.743& 0.622& 0.642& 0.776&0.801& 0.721 &0.736  \\
&Li \textit{el al.} & 0.836& 0.834& 0.831& 0.819& -& -& -&- &- &-\\
& Pro. & \textbf{0.856} & \textbf{0.860}& \textbf{0.847} & \textbf{0.856} & \textbf{0.844}& \textbf{0.846}& \textbf{0.861} & \textbf{0.866} & \textbf{0.851}&\textbf{0.856} \\
\bottomrule[.15em]
\end{tabular}
\end{table*}


\subsection{Multi-scale Quality Fusion Strategy}
Previous studies evaluate the video quality either using the original spatial resolution or a fixed resized spatial resolution, which ignore that videos are naturally multi-scale \cite{zheng2022faver}. Some existing work \cite{wang2003multiscale}\cite{rehman2015display}\cite{min2017unified} shows that considering the multi-scale characteristics can improve the performance of image quality assessment. So, we propose a multi-scale quality fusion strategy to further improve the evaluation accuracy of the VQA model and this strategy is very useful to compare the quality of videos with different spatial resolutions.
\subsubsection{Multi-scale Video Quality Scores}
We first resize the resolution of the video into three fixed spatial scales, which are 540p, 720p, and 1080p, respectively. We do not downscale the video from the original scale to several lower resolution scales, which is a more common practice in previous studies. That is because when users watch videos in an application, the resolution of videos is actually adapted to the resolution of the playback device, and the modern display resolution is normally larger than 1080p. So, the perceptual quality of the low-resolution videos is also affected by the up-sampling artifacts, which also need to be considered by VQA models. Therefore, given a VQA model, we can derive three quality of videos at three scales, which are denoted as , , and  respectively. 
\subsubsection{Adaptive Multi-scale Weights}
The weight of each scale is obtained by considering the human psychological behaviors and the visual sensitivity characteristics. It is noted that the contrast perception ability of the HVS depends on the spatial frequency of the visual signal, which is modeled by the contrast sensitivity function (CSF). Specifically, we first define a viewing resolution factor  as:

where the unit of  is cycles per degree of visual angle (cpd),  is the viewing distance (inch),  is the height of the screen (inch), and  denotes the number of pixels in the vertical direction of the screen. For the above three spatial scales of video, we can obtain the corresponding , which are denoted as , , and  respectively. We use  to divide the spatial frequency range of the corresponding scale, which covers one section of the CSF formulated by:

where , , and  indicate spatial frequency (cpd), luminance (), and angular object area (squared degrees), respectively.

The weight of each scale is calculated as the area under the CSF within the corresponding frequency covering range:

where  from 1 to 3 corresponds the finest to coarsest scale respectively, and  corresponds the viewing resolution factor of 0.  is a normalization factor such that .


Therefore, the multi-scale fusion quality score  is calculated as:



\section{Experimental Validation}




















\begin{table}
\centering
\renewcommand{\arraystretch}{1}
\caption{Performance of the SOTA models and the proposed models on the LSVQ database. Pro. M.S. refers to the proposed model implemented by the multi-scale quality fusion strategy. W.A. means the weighted average results. The best performing model is highlighted in each column.}
\label{performance_lsvq}
\begin{tabular}{c|cc|cc|cc}
\toprule[.15em]
 Database & \multicolumn{2}{c|}{Test} & \multicolumn{2}{c|}{Test-1080p} & \multicolumn{2}{c}{W.A.} \\
  Criterion & SRCC & PLCC & SRCC & PLCC& SRCC & PLCC  \\
\hline
TLVQM & 0.772&0.774& 0.589& 0.616& 0.712 & 0.722 \\
VIDEVAL & 0.794& 0.783& 0.545& 0.554& 0.712 &0.707 \\
VSFA & 0.801& 0.796& 0.675& 0.704 &0.759  &0.766 \\
PVQ & 0.827& 0.828& 0.711& 0.739 & 0.789 &0.799 \\
Li \textit{el al.} & 0.852& 0.854& \textbf{0.772}& 0.788 & 0.825 &0.832 \\
Pro. &  0.864& 0.861&  0.756&  0.801& 0.829 &0.841 \\
Pro. M.S. & \textbf{0.867}  &\textbf{0.861}  & 0.764  & \textbf{0.803}  & \textbf{0.833} & \textbf{0.842}  \\
\bottomrule[.15em]
\end{tabular}
\end{table}



\subsection{Experimental Protocol}
\subsubsection{Test Databases}
We test the proposed model on the five UGC VQA database: KoNViD-1k \cite{hosu2017konstanz}, YouTube-UGC \cite{wang2019youtube}, LSVQ \cite{ying2021patch},  LBVD \cite{chen2019qoe}, and LIVE-YT-Gaming \cite{yu2022subjective}. We summarize the main information of the databases in Table \ref{the_database}. The LSVQ database is the largest UGC VQA database so far, and there are 15 video categories such as animation, gaming, HDR, live music, sports, etc. in the YouTube-UGC database, which is more diverse than other databases. The LBVD database focuses on the live broadcasting videos, of which the videos are degraded by the authentic transmission distortions. The LIVE-YT-Gaming database consists of streamed gaming videos, where the video content is generated by computer graphics.

\subsubsection{Implementation Details}
We use the ResNet50 \cite{he2016deep} as the backbone of the spatial feature extraction module and the SlowFast R50 \cite{feichtenhofer2019slowfast} as the motion feature extraction model for the whole experiments. The weights of the ResNet50 are initialized by training on the ImageNet dataset \cite{deng2009imagenet}, the weights of the SlowFast R50 are fixed by training on the Kinetics 400 dataset \cite{kay2017kinetics}, and other weights are randomly initialized. For the spatial feature extraction module, we resize the resolution of the minimum dimension of key frames as 520 while maintaining their aspect ratios. In the training stage, the input frames are randomly cropped with the resolution of 448448. If we do not use the multi-scale quality fusion strategy, we crop the center patch with the same resolutions of 448448 in the testing stage. Note that we only validate the multi-scale quality fusion strategy on the model trained by the LSVQ database since there are enough videos with various spatial resolutions in it. For the motion feature extraction module, the resolution of the videos is resized to 224224 for both the training and testing stages. We use PyTorch to implement the proposed models. The Adam optimizer with the initial learning rate 0.00001 and batch size 8 are used for training the proposed model on a server with NVIDIA V100. The hyper-parameter  is set as 1. For simplicity, we select the first frame in each chunk as the key frame. For the multi-scale quality fusion strategy, there are , , , , and , and the final multi-scale weights for UGC videos are , , and .

\subsubsection{Comparing Algorithms}
We compare the proposed method with the following no-reference models:
\begin{itemize}
    \item IQA models: NIQE \cite{mittal2012making}, BRISQUE \cite{mittal2012no}, GM-LOG \cite{xue2014blind}, VGG19 \cite{simonyan2014very}, ResNet50 \cite{he2016deep}, and KonCept512 \cite{hosu2020koniq}.
    \item VQA models:  V-BLIINDS \cite{saad2014blind}, TLVQM \cite{korhonen2019two}, VIDEAL \cite{tu2021ugc}, RAPIQUE \cite{tu2021rapique}, VSFA \cite{li2019quality}, PVQ \cite{ying2021patch}, and Li \textit{et al.} \cite{li2021blindly}.
\end{itemize}

Since the number of videos in the LSVQ database is relatively large, we only compare some representative VQA models on the LSVQ database and omit the methods which perform poorly on the other four UGC databases.

\subsubsection{Evaluation Criteria}
We adopt two criteria to evaluate the performance of VQA models, which are Pearson linear correlation coefficient (PLCC) and Spearman rank-order correlation coefficient (SRCC).
PLCC reflects the prediction linearity of the VQA algorithm and SRCC indicates the prediction monotonicity. An excellent VQA model should obtain the value of SRCC and PLCC close to 1.
Before calculating the PLCC, we follow the same procedure in \cite{antkowiak2000final} to map the objective score to the subject score using a four-parameter logistic function.

For KoNViD-1k, YouTube-UGC, LBVD, and LIVE-YT-Gaming databases, we randomly split these databases into the training set with 80\% videos and the test set with 20\% videos for 10 times, and report the median values of SRCC and PLCC. For the LSVQ database, we follow the same training and test split suggested by \cite{ying2021patch} and report the performance on the test and test-1080p subsets.

\subsection{Performance Comparison with the SOTA Models}











The performance results of the VQA models on the KoNViD-1k, YouTube-UGC, LBVD, and LIVE-YT-Gaming databases are listed in Table \ref{performance}, and on the LSVQ database are listed in Table \ref{performance_lsvq}. From Table \ref{performance} and Table \ref{performance_lsvq}, we observe that the proposed model achieves the best performance on all five UGC VQA databases and leads by a large margin, which demonstrates that the proposed model does have a strong ability to measure the perceptual quality of various kinds of UGC videos. For the test-1080p subset of the LSVQ database, the proposed model is inferior to Li \textit{et al.}, which may be because the spatial resolution of most videos in the test-1080p subset is larger than 1080p while the proposed model resizes the spatial resolution of test videos into 448448, so the proposed model has a relatively poor ability to represent the characteristics of high-resolution videos. Through the multi-scale quality weighting fusion strategy, the proposed model can significantly improve the performance on the test-1080p subset. 

Then, most handcrafted feature based IQA models perform poorly on these UGC VQA databases especially for the LBVD and LIVE-YT-Gaming databases since they are designed for natural scene images with synthetic distortions and are difficult to handle the complex in-the-wild distortions and other video types such gaming, videoliving, etc. It is worth noting that through fine-tuning the deep CNN baseline i.e. ResNet50 on the VQA databases, it can achieve a pretty good performance, which also indicates that spatial features are very important for VQA tasks. For the NR VQA methods, the hand-crafted feature based NR VQA methods such as TLVQM and VIDEVAL achieve pretty well performance by incorporating the rich spatial and temporal quality features, such as NSS features, motion features, etc., but they are inferior to the deep learning based NR VQA methods due to the strong feature representation ability of CNN. VSFA extracts the spatial features from the pretrained image recognition model, which are not quality-aware, and achieves relatively poor performance when compared with other deep learning based methods. PVQ and Li \textit{et al.} methods both utilize the pretrained IQA model and ptretrained action recognition model to extract spatial and motion features respectively, and they perform better than other compared NR I/VQA methods but are inferior to the proposed model. 
Through training an end-to-end spatial feature extractor, the proposed model can take advantage of various video content and distortion types in the UGC databases and learn a better quality-aware feature representation. As a result, the proposed model achieves the best performance on all five UGC VQA databases.














































































\begin{table}
\centering
\renewcommand{\arraystretch}{1}
\caption{The results of ablation studies on the LSVQ database. S and M means the spatial features and motion features respectively, and S means that the spatial features are extracted by the pretrained image classification network.}
\label{ablation_study}
\begin{tabular}{c|c|cc|cc}
\toprule[.15em]
\multirow{2}{*}{} & Database & \multicolumn{2}{c|}{Test} & \multicolumn{2}{c}{Test-1080p}  \\
 & Criterion & SRCC & PLCC & SRCC & PLCC  \\
\hline
\multirow{3}{*}{Feature}
&S+M& 0.847 & 0.841 & 0.732 &  0.774 \\
&S& 0.827 & 0.829 & 0.702 & 0.757  \\
&M& 0.660 & 0.669 & 0.569 & 0.621  \\
\hline
\multirow{2}{*}{Regression}
&GRU& 0.858 & 0.855 & 0.735 & 0.788  \\
&Transformer& 0.860 & 0.861 & 0.753 &0.799   \\
\hline
\multirow{2}{*}{Pooling}
& Method in \cite{li2019quality}& 0.860 & 0.858 & 0.733 & 0.786  \\
&1D CNN based& 0.864 & 0.862 & 0.739 &  0.790 \\
\bottomrule[.15em]
\end{tabular}
\end{table}


\subsection{Ablation Studies}
In this section, we conduct several ablation studies to investigate the effectiveness of each module in the proposed model, including the feature extraction module, and the quality regression module. All the experiments are tested on the LSVQ database since it is the largest UGC VQA model and is more representative.


\begin{table}
\small
\centering
\renewcommand{\arraystretch}{1}
\caption{The SRCC results of cross-database evaluation. The model is trained on the LSVQ database. }
\label{cross_database_evaluation}
\begin{tabular}{c|cccc}
\toprule[.15em]
 Database & KoNViD-1k & YouTube-UGC & LBVD & LIVE-YT-Gaming  \\
\hline
Pro.&  \textbf{0.860}&  0.789 &  0.689& 0.642  \\
Pro. M.S.&  0.859&  \textbf{0.822}&  \textbf{0.711}&  \textbf{0.683} \\
\bottomrule[.15em]
\end{tabular}
\end{table}





\begin{table*}
\centering
\renewcommand{\arraystretch}{1}
\caption{Comparison of computational complexity for the six VQA models and two proposed models. Time: Second.}
\label{computationsl_complexity}
\begin{tabular}{c|cccccccc}
\toprule[.15em]
 Methods & V-BLIINDS & TLVQM & VIDEVAL & VSFA& RAPIQUE & Li \textit{et al.} & Pro. & Pro. M.S. \\
\hline
Time & 61.982  & 219.992   & 561.408  &56.424  & 38.126  & 61.971 & \textbf{6.929}  & 8.448  \\
\bottomrule[.15em]
\end{tabular}
\end{table*}




\subsubsection{Feature Extraction Module}
The proposed model consists of the spatial feature extractor that learns the end-to-end spatial quality-aware features and the motion feature extractor that utilizes a pretrained action recognition model to represent motion information. 
Therefore, we first do not train the spatial feature extractor and directly use the weights trained on the ImageNet database to study the effect of the end-to-end training strategy for the spatial feature extractor. Then, we only use the end-to-end trained spatial features or the pretrained motion features to evaluate the quality of UGC videos to investigate the effect of these two kinds of features. The results are listed in Table \ref{ablation_study}. First, it is observed that the model using the motion features is inferior to the model using the spatial features and both of them are inferior to the proposed model, which indicates that both spatial and motion features are beneficial to the UGC VQA task and the spatial features are more important. Then, we find that end-to-end training for the spatial feature extractor can significantly improve the evaluation performance, which demonstrates that end-to-end trained spatial features represent better than that extracted by the pretrained image classification model.  


\subsubsection{Quality Regression Module}
\label{quality_regression_module}
In this paper, we use the MLP as the regression model to derive the chunk-level quality scores. However, in previous studies, some sequential models such as GRU \cite{li2019quality}, Transformer \cite{li2021blindly}, etc. are also adopted to further consider the influence of the features extracted from adjacent frames. Here, we also adopt these methods as a comparison to investigate whether sequential models can improve the performance of the proposed models. Specifically, we replace the MLP module with the GRU and Transformer and keep other experimental setups the same. The results are listed in Table \ref{ablation_study}. We observe that models using GRU and Transformer are both inferior to the proposed model, which means that the MLP module is enough to regress the quality-aware features to quality scores though it is very simple. This conclusion is also consistent with \cite{wang2021rich}. The reason is that the proposed model and the model in \cite{wang2021rich} calculate the chunk-level quality score and the effect of adjacent frames are considered in the quality-aware features (i.e. motion features), while other VQA models \cite{li2019quality} \cite{li2021blindly} calculate the frame-level quality scores, which may need to consider the effect of adjacent frames in the quality regression module.









\subsubsection{Quality Pooling Module}
The proposed model uses the temporal average pooling method to fuse the chunk-level quality scores into the video level. It is noted that previous studies also propose several temporal pooling methods for VQA. 
In this section, we test two temporal pooling methods, which are the subjectively-inspired method introduced in \cite{li2019quality} and a learning based temporal pooling method using the 1D CNN. The results are listed in Table \ref{ablation_study}. From Table \ref{ablation_study}, we observe that the average pooling strategy achieves similar performance to the learning based pooling method, and both of them are superior to the subjectively-inspired methods. Since the average pooling strategy is simpler and does not increase the extra parameters, we use the temporal average pooling method in this paper.








\subsection{Cross-Database Evaluation}
UGC videos may contain various kinds of distortions and content, most of which may not exist in the training set. Hence, the generalization ability of the UGC VQA model is very important. In this section, we use the cross-database evaluation to test the generalization ability of the proposed model. Specifically, we train the proposed model on the LSVQ database and test the trained model on the other four UGC VQA databases. We list the results in Table \ref{cross_database_evaluation}. It is observed that the proposed model achieves excellent performance in cross-database evaluation. The SRCC results on the KoNViD-1k and YouTube-UGC databases both exceed 0.8, which have surpassed most VQA models trained on the corresponding database. We find that the multi-scale quality fusion strategy can significantly improve the performance on the databases containing videos with different spatial resolutions (YouTube-UGC, LBVD, and LIVE-YT-Gaming), which further demonstrates its effectiveness. It is also observed that the performance on the LBVD and LIVE-YT-Gaming databases is not good as the other two databases. The reason is that the LBVD and LIVE-YT-Gaming databases contain live broadcasting and gaming videos respectively, which may rarely exist in the LSVQ database. Since the single database can not cover all kinds of video types and distortions, we may further improve the generalization ability of the proposed model via the multiple database training strategy \cite{sun2021blind} \cite{zhang2021uncertainty} or the continual learning manner \cite{zhang2021continual} \cite{zhang2021task}.



\subsection{Computational Complexity}
The computational complexity is a very important factor that needs to be considered in practical applications. Hence, we test the computational complexity in this section. All models are tested on a computer with i7-6920HQ CPU, 16G RAM, and NVIDIA Quadro P400. The deep learning based models and the handcrafted based models are tested using the GPU and CPU respectively. We report the running time for a video with the resolution of 19201080 and time duration of eight seconds in Table \ref{computationsl_complexity}. It is seen that the proposed model has a considerably low running time compared with other VQA models. The reason is that we use very sparse frames to calculate the spatial features while other deep learning based methods need dense frames. Moreover, we extract the motion features at a very low resolution, which only adds little computational complexity to the proposed model. The very low computational complexity makes the proposed model suitable for practical applications.

\section{Conclusion}
In this paper, we propose an effective and efficient NR VQA model for UGC videos. The proposed model extracts the quality-aware features from the spatial domain and the spatial-temporal domain to measure the spatial distortions and motion distortions respectively. We train the spatial feature extractor in an end-to-end training manner, so the proposed model can make full use of the various spatial distortions and content in the current VQA database. Then, the quality-aware features are regressed into the quality scores by the MLP network, and the temporal average pooling is used to obtain the video-level quality scores. We further introduce the multi-scale quality fusion strategy to address the problem of quality assessment across different spatial resolutions. The experimental results show that the proposed model can effectively measure the quality of UGC videos.






\begin{acks}
This work was supported by the National Natural Science Foundation of China (61831015, 61901260) and the National Key R\&D Program of China 2021YFE0206700.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
