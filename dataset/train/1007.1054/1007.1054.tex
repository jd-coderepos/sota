\documentclass[runningheads]{llncs}
\clearpage{}\usepackage{subfigure}
\usepackage{amscd}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\DeclareMathSymbol\RRightarrow{\mathrel}{AMSa}{"56}
\DeclareMathSymbol\LLeftarrow{\mathrel}{AMSa}{"57}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{color}
\usepackage[normalem]{ulem} \usepackage{wrapfig}
\newcommand\Supp[1] {\lceil#1\rceil}
\newcommand\Defs {\mathord{:=}\,}
\newcommand\Wp {\textit{wp}}
\newcommand\Expt{\mathsf{Expt}}
\newcommand\bft{\mathsf{bft}}
\newcommand{\ct}{\mathsf{ct}}
\newcommand{\Ct}{\mathsf{ct}}
\newcommand{\Et}{\mathsf{bv}}
\newcommand{\Etv}{\mathsf{brv}}
\newcommand{\Etp}{\mathsf{brp}}
\newcommand{\Ft}{\mathsf{ft}}
\newcommand{\Ht}{\mathsf{H}}
\newcommand{\Bt}{\mathsf{B}}
\newcommand\Gt[1]{\mathsf{W}_{#1}}

\newcommand\CCL {\mathsf{ccl}} 

\newcommand\app{\mathsf{app}}
\newcommand{\Apply}{\mathsf{app}}
\newcommand\Vh {\mathsf{h}}
\newcommand\Vv {\mathsf{v}}
\newcommand\Va {\mathsf{a}}
\newcommand\Vb {\mathsf{b}}
\newcommand\Vc {\mathsf{c}}
\newcommand\Vd {\mathsf{d}}
\newcommand\Vj {\mathsf{j}}
\newcommand\Vx {\mathsf{x}}
\newcommand\Vy {\mathsf{y}}
\newcommand\Vm {\mathsf{m}}
\newcommand\Vn {\mathsf{n}}

\newcommand\BftMin{\mathbin{\oslash}}

\newcommand\Point[1] {\PSet{}{}{#1}}
\newcommand{\Peq}{\equiv}
\newcommand{\Pge}{\mathrel\LLeftarrow}
\newcommand{\Ple}{\mathrel\RRightarrow}
\newcommand\Sec[1] {Sec.~\ref{#1}}
\newcommand\App[1] {App.~\ref{#1}}
\renewcommand\Sec[1] {\S\ref{#1}}
\renewcommand\App[1] {\S\ref{#1}}
\newcommand\Secs[1] {\S\S#1}
\newcommand\Wlog {\textit{wlog}}
\newcommand\Eqn[1] {(\ref{#1})}
\newcommand\Cite[1]{[#1]}
\newcommand\Spot {\raisebox{-.1em}{\Large\boldmath}}
\newcommand\st {\mathrel{\Spot}}
\newcommand\VV {{\cal V}}
\newcommand\NLg {\textrm{l}\overline{\textrm{g}}}
\renewcommand\NLg {\overline{\lg}}
\newcommand\CC {{\cal C}}
\newcommand\DD {{\cal D}}
\newcommand\SeqCC[1] {(-\,;\,#1)}
\newcommand\HH {{\cal H}}
\newcommand\Atomic[1] {\AtomicOpen#1\AtomicClose}
\newcommand\AtomicOpen {\langle\!\langle}
\newcommand\AtomicClose {\rangle\!\rangle}
\newcommand\Fun {\mathbin{\rightarrow}}
\newcommand\Rel {\mathbin{\leftrightarrow}}
\newcommand\Dom {\mathsf{dom}}
\newcommand\Ran {\mathsf{ran}}
\newcommand\Pow {{\mathbb P}}
\newcommand\NPow {\mathsf P}
\newcommand\Real {{\mathbb R}}
\newcommand\Nat {{\mathbb N}}
\newcommand\Dist {{\mathbb D}}
\newcommand\TDist {\mathsf{D}}
\newcommand\Uniform[1] {\PSet{}{}{#1}}
\newcommand\Support[1] {{\lceil #1 \rceil}}
\newcommand\Norm[1]{[#1]}
\newcommand{\Transpose}[1]{{#1}^{\mathbf{T}}} \newcommand\Above {\mathbin{\raisebox{-.15em}{+}\kern-.39em\raisebox{.15em}{\makebox[0pt]{}}\kern.45em}}
\newcommand\AboveRel {~\mathrel{\Above}~}
\newcommand\Beside {{+}\kern-.5em{+}}
\newcommand\BesideRel {\mathrel{\Beside}}
\newcommand\IdM[1] {{\mathsf 1}_{#1}}
\newcommand\DiagM[1] {\backslash\kern-.35em\backslash\kern-.05em\mbox{\small}\kern-.05em\backslash\kern-.35em\backslash}
\newcommand{\ISeq}{{\mathsf{iseq}}}



\newcommand\HMM {\textit{HMM}}
\newcommand\BF {Big Fat}
\newcommand\GGA {\textsf{gg}}
\newcommand\Min[1] {\sqcap_{#1}}
\newcommand\MaxW {\mathbin{\textbf{max}}}
\newcommand\MAX {\sqcup}
\newcommand\MIN {\sqcap}
\newcommand\UMax[1] {{\def\Cond{#1}\ifx\Cond\empty {\sqcup} \else {\sqcup}^{#1} \fi}}
\newcommand\UMin[1] {{\def\Cond{#1}\ifx\Cond\empty {\sqcup} \else {\sqcap}^{#1} \fi}}
\newcommand\Max[1] {\MAX_{#1}}
\newcommand\In {{:}\,}
\newcommand\List[1] {\langle#1\rangle}
\newcommand\Ref {\mathrel{\sqsubseteq}}
\newcommand\Finer {\mathrel{\raisebox{0.2ex}{}\kern-.8em\raisebox{-1.1ex}{}}}
\newcommand\StrictFiner {\mathrel{\raisebox{0.2ex}{}\kern-.8em\raisebox{-1.1ex}{/}\kern.4em}}
\newcommand\Coarser {\mathrel{\raisebox{0.2ex}{}\kern-.8em\raisebox{-1.1ex}{}}}
\newcommand\SimFiner {\mathrel{\approx}}
\newcommand\Similar {\mathrel{\approx}}
\newcommand\NotSimilar {\mathrel{\not\approx}}
\newcommand\StrictRef {\mathrel{\sqsubset}}
\newcommand\EquivRef {\mathrel{\equiv}}
\newcommand\NotRef {\mathrel{\not\sqsubseteq}}
\newcommand\ORef[1] {\mathrel{\preceq_{#1}}}
\newcommand\StrictORef[1] {\mathrel{\prec_{#1}}}
\newcommand\ORefB[1] {\mathrel{\succeq_{#1}}}
\newcommand\NotORef[1] {\mathrel{\not\preceq_{#1}}}
\newcommand\ERef {\mathrel{\preceq}}
\newcommand\ERefB {\mathrel{\succeq}}
\newcommand\NotERef {\mathrel{\not\preceq}}
\newcommand\BRef {\mathrel{\preceq_\Bt}}
\newcommand\NotBRef {\mathrel{\not\preceq_\Bt}}
\newcommand\StrictBRef {\mathrel{\prec_\Bt}}
\newcommand\HRef {\mathrel{\preceq_\Ht}}
\newcommand\NotHRef {\mathrel{\not\preceq_\Ht}}
\newcommand\StrictHRef {\mathrel{\prec_\Ht}}
\newcommand\GRef[1] {\mathrel{\preceq_{\Gt{#1}}}}
\newcommand\NotGRef[1] {\mathrel{\not\preceq_{\Gt{#1}}}}
\newcommand\TRef {\mathrel{\sqsubseteq}_{T}}
\newcommand\ARef {\sqsupseteq}


\newcommand{\RefMatrices}{{\cal R}}
\newcommand{\FinerMatrices}{{\cal M}} \newcommand{\StratMatrices}{{\cal G}}
\newcommand{\GStratMatrices}{{\cal K}} \newcommand{\UnitMatrix}{{\bf 1}}
\newcommand{\ZeroMatrix}{{\bf 0}}

\newcommand{\DRef}{\Ref_{\Dist{}}} \newcommand{\QDRef}{\Ref_{R}}      \newcommand{\NotQDRef}{\mathrel{\not{\QDRef}}}
\newcommand{\TQDRef}{\Ref_{T}}      \newcommand{\NotTQDRef}{\mathrel{\not{\TQDRef}}}

\newcommand{\EDRef}{\Ref_{ET}}
\newcommand{\NotEDRef}{\mathrel{\not{\EDRef}}}
\newcommand{\FDRef}{\Ref_{FT}}
\newcommand{\NotFDRef}{\mathrel{\not{\FDRef}}}
\newcommand{\FRef}{\Ref_{FT}}
\newcommand{\NotFRef}{\mathrel{\not{\FRef}}}
\newcommand{\CRef}{\Ref_{C}}
\newcommand{\NotCRef}{\mathrel{\not{\FRef}}}
\newcommand{\SSRef}{\Ref_{S}}      \newcommand{\Fracs}{\textsf{fracs}}    \newcommand\From[1][*] {{\def\z{#1}\if*\z {:}{\in}\,\else {:}{\in}_{\kern-.1em{#1}}\,\fi}}
\newcommand\Gets {{:}{=}\,}
\newcommand\NGets {{:}{\in}\,}
\newcommand\PGets {{\NGets}}
\newcommand\Implies {\mathop{\Rightarrow}}
\newcommand\LHS {\textit{lhs}}
\newcommand\RHS{\textit{rhs}}
\newcommand\If {\textbf{if}}
\newcommand\Then {\textbf{then}}
\newcommand\Else {\textbf{else}}
\newcommand\Elseif {\textbf{else if}}
\newcommand\Fi {\textbf{fi}}
\newcommand\Reveal {\textbf{reveal}}
\newcommand\Sem[1] {[\![#1]\!]}
\newcommand\Vpk {v'\kern-.25em}
\newcommand\MatSem[2][*] {\def\z{#1}\if*\z (\!(#2)\!) \else \left(\kern-.3em\left(\rule{0pt}{#1}#2\right)\kern-.3em\right) \fi}
\newcommand\MatSemC[1] {(\kern-.21em\raisebox{.025em}{\small}#1\raisebox{.025em}{\small}\kern-.22em)}
\newcommand\SemC[1] {[\![#1]\!]_C}
\newcommand\Or {\mathbin{\sqcap}}
\newcommand\PC[1] {\mathbin{_{#1}\oplus}} \newcommand\UC[1] {\mathbin{_{#1}\uplus}} \newcommand{\ITE}[3]{#1\,\IF\,#2\,\ELSE\,#3}
\newcommand\True {\textsf{true}}
\newcommand\False {\textsf{false}}
\newcommand\TT {\textsf{T}}
\newcommand\FF {\textsf{F}}
\newcommand\Xor {\mathbin{\nabla}}
\newcommand\Spec[2]{#1{:}[#2]}
\newcommand\Vis {\textbf{vis}}
\newcommand\Hid {\textbf{hid}}
\newcommand\While {\textbf{while}}
\newcommand\Do {\textbf{do}}
\newcommand\Od {\textbf{od}}
\newcommand{\Begin}{\mbox{\boldmath\textbf{}}}
\newcommand{\End}{\mbox{\boldmath\textbf{}}}

\newcommand{\ABegin}{\langle \!\langle}
\newcommand{\AEnd}{\rangle \! \range}

\newcommand{\Quantum}[1]{\QBegin #1 \QEnd} \newcommand{\Prob}[1]{\QBegin #1 \QEnd} \newcommand{\Shadow}[1]{\QBegin #1 \QEnd} \newcommand{\Std}[1]{\QBegin #1 \QEnd} \newcommand{\QBegin}{{ [\!\!\mid }}
\newcommand{\QEnd}{{ \mid\!\!] }}
\newcommand{\LeftPS}{ \{\!\!\{ }
\newcommand{\RightPS}{ \}\!\!\} }

\newcommand{\AddShadow}{\mathsf{atomic}} \newcommand{\Hide}{\mathsf{embed}} 


\newcommand\Skip {\textbf{skip}}
\newcommand\Abort {\textbf{abort}}
\newcommand\HangRight[1] {\makebox[0pt][l]{#1}}
\newcommand\HangLeft[1] {\makebox[0pt][r]{#1}}
\newcommand\InQuadL[1] {\makebox[0pt][l]{}\quad}
\newcommand\InQuadR[1] {\quad\makebox[0pt][r]{}}
\newcommand\IF {\textbf{if}}
\newcommand\Mod {\mathbin{\bf mod}}
\newcommand\Rnd {\mathbin{\bf rnd}}
\newcommand\Module {\textbf{module}}
\newcommand\Int {\textbf{int}}
\newcommand\Public {\textbf{public}}
\newcommand\Private {\textbf{private}}
\newcommand\Init {\textbf{init}}
\newcommand\Return {\textbf{return}}
\newcommand\PP {{{+}{+}}}
\newcommand\MM {{-}{-}}
\newcommand\Pp[1] {[#1]}
\newcommand\Tr {\textsf{tr}}
\newcommand\TBC {\medskip\noindent\dotfill~To be continued~\dotfill}
\newcommand\ELSE {\textbf{else}}
\newcommand\Wide[1] {~~~#1~~~}
\newcommand\WideRm[1] {~~~\textrm{#1}~~~}
\newcommand\WIDE[1] {~~~~~~#1~~~~~~}
\newcommand\WIDERM[1] {~~~~~~\textrm{#1}~~~~~~}
\newcommand\Bool {\textsf{Bool}}

\newcommand\POINT {\textsf{U}}
\newcommand\LIFT {\textsf{L}}
\newcommand\SQUASH {\textsf{Q}}
\newcommand\MAP {\textsf{M}}
\newcommand{\HIDE}{\textsf{E}} \newcommand{\PART}{\textsf{P}}
\newcommand{\OutV}{\textsf{outV}}
\newcommand\Comp {\mathbin{\circ}}
\newcommand\Id {\textsf{1}}
\newcommand\AT[1] {{@#1}}
\newcommand\At[1] {^{@#1}}
\newcommand\Att[2] {^{@\frac{#1}{#2}}}
\newcommand\IIFrac[2] {\mbox{}} \newcommand\IFrac[2] {\mbox{\small}} \newcounter{enumSaver}

\newenvironment{NumMini}[1]{}
\newenvironment{Reason}{\vspace{-.0em}\begin{tabbing}\hspace{2em}\= \hspace{1cm} \= \kill}{\end{tabbing}\vspace{-1em}}
\newcommand\Step[2] {#1 \>  \\}
\newcommand\StepR[3] {#1 \>  \` {\RF \makebox[0pt][r]{\begin{tabular}[t]{r}``#2''\end{tabular}}} \\}
\newcommand\BStep[2] {#1 \> \> \\}
\newcommand\BStepR[3] {#1 \> \> \` {\RF \makebox[0pt][r]{\begin{tabular}[t]{r}``#2''\end{tabular}}} \\}
\newcommand{\StepL}[3]{
#1 \>  \` {\RF \makebox[0pt][r]{\begin{tabular}[t]{r}``#2''\end{tabular}}} \\
{} \>  \\
}
\newcommand\WideStepR[3] {#1 \>  \` {\RF \makebox[0pt][r]{\begin{tabular}[t]{r}``#2''\end{tabular}}} \\} \newcommand\BWideStepR[3] {#1 \> \>  \` {\RF \makebox[0pt][r]{\begin{tabular}[t]{r}``#2''\end{tabular}}} \\} \newcommand\Space {~ \\}
\newcommand\RF {\small}
\newenvironment{EQN}[1]{\def\z{#1}\medskip\par\noindent\hfill}{\hfill\makebox[0pt][r]{(\z)}\medskip\par\noindent}
\newcommand\ImageInText[4]{\makebox[0pt][l]{{\def\Up{#1}\def\Right{#2}\hfill \raisebox{\Up}[0pt][0pt]{\makebox[0pt][l]{\hspace{\Right}\includegraphics[scale=#3]{#4}}}}}}
\newcommand\Bag[3]{
 \bgroup\def\Bound{#1}\{\ifx\Bound\empty\egroup #3 \else\egroup #1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi
 \fi\}
}
\newcommand\Set[3]{
 \bgroup\def\Bound{#1}\{\ifx\Bound\empty\egroup #3 \else\egroup #1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi
 \fi\}
}
\newcommand\SetBig[3]{
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup
   \begin{array}[t]{l}
	\{#1 \mathrel{\Spot}\\
    \hspace{3em}#3\}
   \end{array}
  \else\egroup
   \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup
    \begin{array}[t]{l}
     \{#1 \mathrel{\mid} \\
     \hspace{3em}#2\}
   \end{array}
   \else\egroup
    \begin{array}[t]{l}
     \{#1 \\
     \hspace{3em} \mathrel{\mid}#2 \\
     \hspace{3em} \mathrel{\Spot}#3\}
    \end{array}
   \fi
  \fi
}
\newcommand\PSet[3]{
 \bgroup\def\Bound{#1}\LeftPS \ifx\Bound\empty\egroup #3 \else\egroup #1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi
 \fi \RightPS
}
\newcommand\PSetBig[3]{
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup
   \begin{array}[t]{l}
	\LeftPS#1 \mathrel{\Spot}\\
    \hspace{3em}#3\RightPS
   \end{array}
  \else\egroup
   \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup
    \begin{array}[t]{l}
     \LeftPS#1 \mathrel{\mid} \\
     \hspace{3em}#2\RightPS
   \end{array}
   \else\egroup
    \begin{array}[t]{l}
     \LeftPS#1 \\
     \hspace{3em} \mathrel{\mid}#2 \\
     \hspace{3em} \mathrel{\Spot}#3\RightPS
    \end{array}
   \fi
  \fi
}
\newcommand\Exists[3]{
 (\begin{array}[c]{l}
  \exists#1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi
 \end{array})
}
\newcommand\ForAll[3]{
 (\begin{array}[c]{l}
  \forall#1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi
 \end{array})
}
\newcommand\Union[3]{
(\begin{array}[c]{l}
  \bigcup#1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi
 \end{array})
}
\newcommand\Sum[3]{
 \begin{array}[c]{l}
  (\sum#1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi )
 \end{array}
}
\newcommand\SumSmall {\mbox{}}
\newcommand{\EXP}{\mathop{\sum\kern-.45cm\sum}}
\renewcommand{\EXP}{+}
\renewcommand{\EXP}{{\cal E}}
\renewcommand{\EXP}{\otimes}
\renewcommand{\EXP}{\odot}
\newcommand\Exp[2]{(\kern-.25ex\EXP\,#1\bgroup\def\Elem{#2}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#2\fi)}
\newcommand\ExpNorm[2]{[\kern-.1ex\EXP\,#1\bgroup\def\Elem{#2}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#2\fi]}
\newcommand\CExp[3]{
 (\begin{array}[c]{l}
  \EXP#1
  \bgroup\def\Cond{#2}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#2\fi
  \bgroup\def\Elem{#3}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#3\fi
 \end{array})
}
\newcommand\General[4]{
 \begin{array}[c]{l}
  (#1#2
  \bgroup\def\Cond{#3}\ifx\Cond\empty\egroup\else\egroup\mathrel{\mid}#3\fi
  \bgroup\def\Elem{#4}\ifx\Elem\empty\egroup\else\egroup\mathrel{\Spot}#4\fi)
 \end{array}
}
\newenvironment{Figure}[2][t]{\begin{figure}[#1]\def\Label{#2}\small}{\label{\Label}\end{figure}}
\newenvironment{Theorem}[2]{\begin{theorem}\label{#2}\textit{#1}\rm\quad}{\hfill\end{theorem}}
\newenvironment{Lemma}[2]{\begin{lemma}\label{#2}\textit{#1}\rm\quad}{\hfill\end{lemma}}
\newenvironment{Definition}[2]{\begin{definition}\label{#2}\textit{#1}\rm\quad}{\hfill\end{definition}}

\newcommand\Proof{\par\textit{Proof:}\quad}
\newcommand\Itm[1] {(\ref{#1})}
\newcommand\Thm[1] {Thm.~\ref{#1}}
\newcommand\Lem[1] {Lem.~\ref{#1}}
\newcommand\Def[1] {Def.~\ref{#1}}
\newcommand\Fig[2][*] {{\def\z{#1}\if*\z Fig.~\ref{#2}\else Fig.~\ref{#2}(#1)\fi}}
\clearpage{}

\setcounter{tocdepth}{1}
\begin{document}
\title{Compositional closure \\
for Bayes Risk \\
in probabilistic noninterference}
\author{Annabelle McIver\inst{1} \and Larissa Meinicke\inst{1} \and Carroll Morgan\inst{2}\thanks{We acknowledge the support of the Australian Research Council Grant {DP0879529}.}}
\institute{Dept.~Computer Science, Macquarie University, NSW 2109 Australia
\and School of Comp.\ Sci.\ and Eng., Univ.~New South Wales, NSW 2052 Australia
}
\maketitle

\begin{abstract}
We give a sequential model for noninterference security including probability (but not demonic choice), thus supporting reasoning about the \emph{likelihood} that high-security values might be revealed by observations of low-security activity. Our novel methodological contribution is the definition of a \emph{refinement} order  and its use to \emph{compare} security measures between specifications and (their supposed) implementations. This contrasts with the more common practice of evaluating the security of individual programs in isolation.

\medskip
The appropriateness of our model and order is supported by our showing that  is the \emph{greatest} compositional relation --the compositional closure-- with respect to our semantics and an ``elementary'' order based on Bayes Risk --- a security measure already in widespread use. We also relate refinement to other measures such as Shannon Entropy.

\medskip
By applying the approach to a non-trivial example, the anonymous-majority \emph{Three-Judges} protocol, we demonstrate by example that correctness arguments can be simplified by the sort of layered developments --through levels of increasing detail-- that are allowed and encouraged by compositional semantics.
\end{abstract}

\tableofcontents

\newpage
\section{Introduction}
We apply notions of testing equivalence and refinement, based on \emph{Bayes Risk}, to the topic of \emph{noninterference security}~\cite{Goguen:84} \emph{with probability} but without demonic choice. Previously, we have studied noninterference for demonic systems without probabilistic choice \cite{Morgan:06,Morgan:07}, and we have studied probability and demonic choice without noninterfence \cite{Morgan:96d,McIver:05a}. Here thus we are completing a programme of treating these features ``pairwise.''

Our long-term aim --as we explain in the conclusion-- is to treat all three features together, based on the lessons we have learned by treating strict subsets of them. The benefit (should we succeed) would apply not only to security, but also to conventional program development where, in the presence of both probabilistic and demonic choice, the technique of data-transformation (aka.\ data \emph{refinement} or data \emph{reification}) becomes unexpectedly complex: variables inside local scopes must be treated analogously to ``high security'' variables in noninterference security.

We take the view, learned from others, that program/system development benefits from a comparison of specification programs with (putative) implementations of them, wherever this is possible, via a mathematically defined ``refinement'' relation whose formulation depends ultimately on a notion of testing that is agreed-to subjectively by all parties concerned \cite{Nicola:84}.\,\footnote{We say ``wherever this is possible'' since there are many aspects of system development that cannot be pinned down mathematically. But --we argue-- those that can be, should be.}
To explain our position unambiguously, we begin by recalling the well known effects of this approach for conventional, sequential programming.

\subsection{Elementary testing and refinement for conventional programs}\label{s1309}
Consider sequential programs operating over a state-space of named variables with fixed types, including a program \Abort\ that diverges (such as an infinite loop). We allow demonic nondeterminsm, statements such as , in the now-conventional way in which they represent equally abstraction (we do not care whether  is assigned 0 or 1, as long as it is one or the other), on the one hand, or unpredictable and arbitrary run-time choice on the other.

Having determined a ``specification'' program , we address the question of whether we are prepared to accept some program  that purports to ``implement'' it. Although there is nowadays a widely accepted answer to this, we imagine that we are considering the question for the first time and that we are hoping to find an answer that everybody will accept. For that we search for a test on programs that is ``elementary'' in the sense that it is conceptually simple and that no ``reasonable'' person could ever argue that  is implemented by  if it is the case that  always passes the test but  might fail it.\,\footnote{There is a possibly dichotomy here between ``may testing'' and ``must testing,'' and we are taking the latter in this example: if  must pass a certain test, then so must  if it is to be considered an implementation.}

A common choice for such an \emph{elementary test} is ``can diverge,'' where divergence is considered to be a bad thing: using it, our criterion becomes ``if  indeed implements  and  can diverge, then it must be possible for  to diverge also.'' We note that the elementary test cannot be objectively justified: it is an ``axiom'' of the approach that will be built on it; and it is via the subjective axioms (in any approach) that we touch reality, where we avoid an infinite definitional regress.

The elementary test provides an ``only if'' answer to the implementation question, but not an ``if.'' That is, we do not say that  implements  \emph{if} either  never fails the test or  might fail it: this is not practical, because of context. For an example, let  be  and let  be simply . Then indeed  passes the test if  does (because they both fail); but we cannot accept  generally as a replacement for  because context  ``protects'' , and passes the test as a whole; but the same context does not protect , since  (still) fails. This illustrates the inutility of the elementary view taken on its own, and it shows that we need a more sophisticated comparison in order to have a practical tool that respects contexts. (Thus it is clear above that we must add ``if executed from the same initial state.'') The story leads on from here to a definition, ultimately, of sequential-program refinement  as the unique relation such that\,\footnote{We say ``a'' rather thean ``the'' definition of refinement because this is just an example: other elementary tests, and other possible contexts, lead naturally to other definitions.}
\begin{enumerate}[(i)]
\item\label{i1253}\makebox[6em][l]{\textit{soundness}} If  then for all contexts  we have that  passes the elementary test if  does, and
\item\label{i1254}\makebox[6em][l]{\textit{completeness}} If  then there is some context  such that  fails the elementary test although  passes it.
\end{enumerate}
That relation turns out to have the \emph{direct} definition that  just when, for all initial states ,
if executing  from  can deliver some final state  then --from  again-- either  can deliver , as well, or  can diverge. Crucially, it is the direct definition that allows  to be determined without examining all possible contexts.

\subsection{Elementary testing and refinement for probabilistic noninterference-secure programs}\label{s1028}
In attempting to follow the trajectory of \Sec{s1309} into the modern context of noninterference and probability, we immediately run into the problem that there are competing notions of elementary test. Here are just four of them:
\begin{description}
\item[Bayes Risk] \cite{Smith:07,Chatzikokolakis:07b,Braun:08,Braun:09} is based on the probability an attacker can reveal a high-security, ``hidden'' variable  using a single query ``Is  equal to ?''\ where  is some value in 's type. Here (and below) the elementary testing of  wrt.\  requires that the probability of revealing  in  cannot be higher than it is in .
\item[marginal guesswork] \cite{Pliam:00,Kopf:07} is measured in terms of how many queries of the form ``Is  equal to ?''\ are needed to determine 's value with a given probability.
\item[Shannon Entropy] \cite{Shannon:48} is related to the use of multiple queries of the form ``Is  in some set ?'' where  is a subset of 's type.
\item[guessing entropy]~\cite{Massey:94,Kopf:07} is the average number of ``Is  equal to ?''\ guesses necessary to determine 's value.
\end{description}
Not only do these criteria compete for popularity, it turns out that on their own they are not even objectively comparable. For instance,  Pliam~\cite{Pliam:00} finds that there can be no general ordering between marginal guesswork and Shannon Entropy: that is, from a marginal-guesswork judgement of whether  passes all tests that  does, there is no way to determine whether the same would hold for Shannon-entropy judgements, nor vice versa.
Similarly, Smith has compared Bayes Risk and Shannon Entropy, and claims that these measures are inconsistent in the same sense~\cite{Smith:07}. The general view seems to be that none of these (four) methods can be said to be generally more- or less discriminating than any of the others.

In spite of the above, \emph{one of our contributions here} is to show that Bayes Risk is maximally discriminating among those four if context is taken into account.

\subsection{Features of our approach: a summary}

Our most significant deviation from traditional noninterference is that, rather than calculating security measures of programs in isolation, instead we focus on comparing security measures \emph{between} programs: typically one is supposed to be a specification, and another is supposed to be an implementation of it. What we are looking for is an implementation that is at least as secure as its specification. 

Since we never consider the security of programs in isolation, an advantage is that it is possible easily to arrange certain kinds of permissible information flow. For example whenever  holds, a program  that leaks only the  low-order bits of a hidden integer  is secure with respect to a specification  that leaks the  low-order bits of  --- that is, for any implementation of , the leaking of up to  low-order bits of  is allowed but no more. This way we sometimes can avoid separate tools for declassification: to allow an implementation to release (partial) information, we simply arrange that its specification does so.

Typically it is both functional- and security properties (however we measure them) that are of interest. As such, we would like to define a relation  between these programs so that  just when \emph{implementation}  has all the functional and the security properties that \emph{specification}  does, where ``all'' is interpreted within our terms of reference. For incremental, compositional reasoning with such an order, it has been known from the very beginning~\cite{Wirth:71} that the \emph{refinement} relation  must satisfy two key technical properties:
\begin{description}
\item[Transitivity] If  then also . Because of this a comparison between two large programs  can be carried out via  through many small steps over a long time.
\item[Monotonicity of contexts] If  then also , where  is any program context. Because of this, a large comparison can be carried out via many small steps \emph{independently} by a large programming team working in parallel. 
\end{description}
As argued above, since our comparisons rest ultimately on subjective criteria for failure, we reduce that dependency on what is essentially an arbitrary choice by making those criteria as elementary as possible: when can you be \emph{absolutely sure} that , that refinement should fail? For this purpose we identify an elementary testing relation  based on Bayes Risk, such that if  then  ``certainly'' (but still subjectively) does not satisfy the specification  in terms of ``reasonable'' functional- and probabilistically secure properties. 

Because our  is not respected by all contexts (there exist programs  and context  such that , yet  in spite of that) our relation  is chosen so that it is smaller --i.e. more restrictive-- than , so that it excludes just those ``apparent'' refinements that can be voided by context.

Our refinement relation is the \emph{compositional closure} of , the largest relation  such that  implies  for all possible contexts . Abusing terminology slightly, we will for simplicity say that  is \emph{compositional} just when it is respected by all possible contexts  (whereas strictly speaking we should say that all such 's are -monotonic). Further, we note that if we define equivalence  to be ``bi-refinement''  and  then monotonicity of  implies that  is is a congruence for all contexts .

There are two further, smaller idiosyncracies of our approach. The first is that we allow the high-security, ``hidden'' variables to be assigned-to by the program, so that it is the secrecy of the \emph{final} value  of  that is of concern to us, not the initial value . This is because we could not otherwise meaningfully compare functional properties, nor would we be able to treat (sequential) compositional contexts. The other difference, more a position we take, is that we allow an attacker both \emph{perfect recall} and an awareness of \emph{implicit flow}: that the intermediate values of low-security ``visible'' program variables are observable, even if subsequently overwritten; and that the control-flow of non-atomic program statements is observable. 
As shown in our case study (\Sec{s1324}) it is this which allows us to model distributed applications: there, the values of intermediate variables can be observed (and recalled) if they are sent on an insecure channel, and the control flow of a program may be witnessed (for example) by observing which 
request an agent is instructed to fulfill.

\smallskip
In summary, our \underline{\textsc{technical contribution}} is that we (i) give a sequential semantics for probabilistic noninterference, (ii) define the above order  based on Bayes Risk, (iii) show it is \emph{not} compositional, (iv) identify a \textbf{compositional} subset of it, a \emph{refinement} order  such that  implies  for all contexts  and (v) show that  is in fact the compositional \textbf{closure} of , so that in fact we have  \emph{only} when  for some .

Finally, we note (vi) that  is sound for the other three, competing notions of elementary test and that therefore Bayes-Risk testing, with context, is maximally discriminating among them.

These technical contributions further our general goal of structuring secure protocols hierarchically and then designing/verifying them in separate pieces, a claim that we illustrate by showing how our model and our secure-program ordering may be used to give an incremental development of \emph{The Three Judges}, an ``anonymous majority'' protocol we constructed precisely to make this point.

\section{A probabilistic, noninterference sequential semantics}\label{s1121}
We identify \emph{visible} variables (low-security), typically  in  some finite type , and \emph{hidden} variables (high-security), typically  in finite . Variables are in \textsf{sans serif} to distinguish them from (decorated) values  they might contain.\,\footnote{We say hidden and visible, rather than high- and low security, because of the connection with data refinement where the same technical issues occur but there are no security implications.}

As an example, let hidden  represent one of three boxes: Box 0 has two black balls; Box 1 has one black- and one white ball; and Box 2 has two white balls. Then let  represent a ball colour: \emph{white}, \emph{black} or \emph{unknown}. Our first experiment in this system is Program , informally written , that chooses box  uniformly, and then draws a ball  from that Box~: from the description above (and the code) we can see that with probability   the ball is white, and with probability  it is black. Then the ball is replaced. A typical security concern is ``How much information about  is revealed by its assignments to ?''

We use this program, and that question, to motivate our program syntax and semantics, to make Program  the above program precise and to provide the framework for asking --and answering-- such security questions.

We begin by introducing \emph{distribution} notation, generalising the notations for na{\"i}ve set theory.


\subsection{Distributions: explicit, implicit and expected values over them}\label{s1729}

We write function application as , with ``'' associating to the left. Operators without their operands are written between parentheses, as  for example. 
\emph{Set} comprehensions are written as  meaning the set formed by instantiating bound variable  in the expression  over those elements of  satisfying formula .\,\footnote{This is a different order from the usual notation , but we have good reasons for using it: calculations involving both sets and quantifications are made more reliable by a careful treatment of bound variables and by arranging that the order  is the same in both comprehensions and quantifications (as in  and ).}

By  we mean the set of \emph{discrete sub-distributions} on set  that sum to no more than one, and  means the \emph{full} distributions that sum to one exactly. The \emph{support}  of (sub-)distribution  is those elements  in  with , and the \emph{weight}  of a distribution is , so that full distributions have weight 1. Distributions can be scaled and summed according to the usual pointwise extension of arithmetic to real-valued functions, so that  is  for example; the \emph{normalisation} of a (sub-)distribution  is defined .

Here are our notations for \emph{explicit} distributions (cf.\ set enumerations):
\begin{description}
\item[multiple] We write  for the distribution assigning probabilities  to elements  respectively, with .
\item[uniform] When explicit probabilities are omitted they are uniform: thus  is the point distribution , and  is . And  is .
\end{description}

In general, we write  for the \emph{expected value}  of expression  interpreted as a random variable in  over distribution .\footnote{It is a dot-product between the distribution and the random variable as state-vectors.}
If however  is Boolean, then it is taken to be 1 if  holds and 0 otherwise: thus in that case  is the combined probability in  of all elements  that satisfy .

We write \emph{implicit} distributions (cf.\ set comprehensions) as , for distribution , real expression  and expression , meaning

where, first, an expected value is formed in the numerator by scaling and adding point-distribution  as a real-valued function: this gives another distribution. The scalar denominator then normalises to give a distribution yet again. A missing  is implicitly  itself. If  is missing, however, then  is just  --- in that case we do not multiply by  in the numerator, nor do we divide (by anything).

Thus  \emph{maps} expression  in  over distribution  to make a new distribution on 's type.  When  is present, and Boolean, it is converted to 0,1; thus in that case  is 's \emph{conditioning} over formula  as predicate on .

Finally, for \emph{Bayesian belief revision} we let  be an a-priori distribution over some , and we let expression  for each  in  be the probability of a certain subsequent result if that  is chosen. Then  is the a-posteriori distribution over  when that result actually occurs. Thus in the three-box program  let the value first assigned to  be . The a-priori distribution over  is uniform, and the probability that the chosen ball is white, that , is therefore . But the a-posteriori distribution of  \emph{given that } is , which from \Eqn{e1221} we can evaluate
{\small}
that is , to calculate our way to the conclusion that if a white ball is drawn () then the chance it came from Box 2 is , the probability of  in the a-posteriori distribution.

\subsection{Program denotations over a visible/hidden ``split'' state-space}
We account for the \emph{visible} and \emph{hidden} partitioning of the finite state space  in our new model by building \emph{split-states} of type , whose typical element  indicates that we know  exactly, but that all we know about  --which is not directly observable-- is that it takes value  with probability . 

Programs become functions  from split-states to distributions over them, called \emph{hyper-distributions} since they are distributions with other distributions inside them: the outer distribution is directly visible but the inner distribution(s) over  are not. Thus for a program  with semantics , the application  is the distribution of final split-states produced from initial . Each  in the support of that outcome, with probability  say in the outer- (left-hand)  in , means that with probability  an attacker will observe that  is  and simultaneously will be able to deduce (via the explicit observation of  and  and other implicit observations) that  has distribution .

When applied to hyper-distributions, addition, scaling and probabilistic choice () are to be interpreted as operations on the outer distributions (as explained in \Sec{s1729}).

\subsection{Program syntax and semantics}\label{s47563}

The programming language semantics is given in \Fig{f1228}. In this presentation we do not treat loops and, therefore, all our programs are terminating. 

When we refer to \emph{classical} semantics, we mean the interpretation of a program without distinguishing its visible and hidden variables, thus as a ``relation'' of type .\,\footnote{Classical relational and \emph{non-probabilistic} semantics over a state-space  is strictly speaking  or equivalently . Further formulations include however both  and . Because all these are essentially the same, we call  a ``relational'' semantics.}
\begin{Figure}[ht!]{f1228} 
1.5ex]
Composition & P_1; P_2 & 
   \Exp{(v',\delta')\In\Sem{P_1}.(v,\delta)}{~\Sem{P_2}.(v',\delta')} \\  
  General prob.\ choice & P_1 \PC{q.\Vv.\Vh} P_2
                     & \hspace{3em} p*\Sem{P_1}.(v,\PSet{h\In\delta}{q.v.h}{})
                     \hspace{1em}\textrm{\tiny  is } \\
                     & 
                     & \makebox[3em][r]{~} 
                                   (1{-}p)*\Sem{P_2}.(v,\PSet{h\In\delta}{1{-}q.v.h}{}) 
                    \1.5ex]
Conditional choice & \makebox[0pt][l]{} 
                     & \hspace{3em} p*\Sem{P_t}.(v,\PSet{h\In\delta}{G.v.h}{})
                     \hspace{1em}\textrm{\tiny  is } \\
                     & \makebox[0pt][l]{}
                     & \makebox[3em][r]{~} (1{-}p)*\Sem{P_f}.(v,\PSet{h\In\delta}{\neg G.v.h}{})
\end{array}

 \Hide.\delta' \Wide{\Defs} \PSet{v'\In\VProj.\delta'~}{}{~(v',\VCond.\delta'.v')} ~,
\label{e1028}
 \Sem{A}.(v,\delta) \Wide{\Defs} \Hide.\Exp{h\In \delta~}{~\SemC{A}.(v,h)}~.

\Vh\Gets 0{\PC{}}1{\PC{}}2; \Vv\From \PSet{}{}{w\Att{\Vh}{2},b\At{1{-}\frac{\Vh}{2}}}; \Vv\Gets \bot ~,

 \PSet{}{}{~(\bot,\PSet{}{}{1\Att{1}{3},2\Att{2}{3}}),~(\bot,\PSet{}{}{0\Att{2}{3},1\Att{1}{3}})~}

 \makebox[0pt][r]{and\hspace{7em}}
 \PSet{}{}{~(\bot,\PSet{}{}{0,1,2})~}\makebox[0pt][l]{~.}

 \Sem{\Skip \PC{\Vh} \Skip}.(v, \Uniform{\IIFrac{1}{4}, \IIFrac{1}{2}})
 \Wide{=} 
 \PSet{}{}{
  (v, \PSet{}{}{\IIFrac{1}{4}\Att{1}{3}, \IIFrac{1}{2}\Att{2}{3}})\Att{3}{8},
  (v, \PSet{}{}{\IIFrac{1}{4}\Att{3}{5}, \IIFrac{1}{2}\Att{2}{5}})\Att{5}{8}
 }~,
\label{e1930}
 \begin{array}{lrl}
                    & P_2 \Defs~~~ & \Vh\Gets1{\PC{}}2{\PC{}}3;~ \Vv \From \Vh\Rnd2;~ \Vv \Gets \Vh\Mod2 \\                    
  \textrm{and}\quad 
                    & P_4 \Defs~~~ & \Vh\Gets1{\PC{}}2{\PC{}}3;~ \Vv\From \Vh\Rnd4;~\Vv \Gets \Vh\Mod2 ~.
\end{array}

 \PSet{}{}{~~(0,\PSet{}{}{2})\Att{1}{3},~
       (1,\PSet{}{}{1})\Att{1}{6},
       (1,\PSet{}{}{1,3})\Att{1}{3},
       (1,\PSet{}{}{3})\Att{1}{6}~~}

 \PSet{}{}{~~(0,\PSet{}{}{2})\Att{1}{3},
       \underbrace{(1,\PSet{}{}{1\Att{3}{4},3\Att{1}{4}})\Att{1}{3}}
       _{\makebox[0pt]{\hspace{3em}\parbox{33em}{\scriptsize
         With overall probability  the final  will be 1 and  will be 0;
         since  is 1 then  must be 1 or 3;
         but if  was 0 that  is three times
         as likely to have been 1.}}},
       (1,\PSet{}{}{1\Att{1}{4},3\Att{3}{4}})\Att{1}{3}~~}
\label{e1948}
 \textrm{when }\quad\left\{\quad\quad
 \begin{array}{r@{\hspace{3em}}l}
  \Pi'_{P_2}\In&\List{\PSet{}{}{1\Att{1}{6}}, \PSet{}{}{1\Att{1}{6},3\Att{1}{6}}, \PSet{}{}{3\Att{1}{6}}} \\
  \Pi'_{P_4}\In&\List{\PSet{}{}{1\Att{1}{4},3\Att{1}{12}}, \PSet{}{}{1\Att{1}{12},3\Att{1}{4}}} ~.
 \end{array}
 \right.

 \sum\List{\PSet{}{}{1\Att{1}{6}},\PSet{}{}{1\Att{1}{6},3\Att{1}{6}}}
 \WIDERM{is}
 \List{\PSet{}{}{1\Att{1}{3},3\Att{1}{6}}}~.

 \List{\PSet{}{}{},\PSet{}{}{0\Att{1}{3}},\PSet{}{}{1\Att{1}{6},2\Att{1}{6}},\PSet{}{}{1\Att{1}{6},2\Att{1}{6}}}
 \Wide{\Similar}
 \List{\PSet{}{}{0\Att{1}{3}},\PSet{}{}{1\Att{1}{9},2\Att{1}{9}},\PSet{}{}{1\Att{2}{9},2\Att{2}{9}}}~,

 \List{\PSet{}{}{0\Att{1}{3}},\PSet{}{}{1\Att{1}{9},2\Att{2}{9}},\PSet{}{}{1\Att{2}{9},2\Att{1}{9}}}
 \Wide{\Finer}
 \List{\PSet{}{}{0\Att{1}{3}},\PSet{}{}{1\Att{1}{3},2\Att{1}{3}}}

 \Delta_{S}\Ref\Delta_{I}
 \WideRm{~~iff~~}
 \Fracs.\Delta_{S}.v ~\Similar~ \Pi ~\Finer~ \Fracs.\Delta_{I}.v
 \quad\textrm{for some partition .}
\label{e1520}
 \Pi_{P_2}'{:}\quad
  \left (\begin{array}{c@{~}c@{~}c}
          \IFrac{1}{6} & 0 &  0 \\
          \IFrac{1}{6} & 0 & \IFrac{1}{6} \\
          0 & 0 & \IFrac{1}{6}
         \end{array}
  \right)
 \hspace{5em}
 \Pi_{P_4}'{:}\quad
  \left (\begin{array}{c@{~}c@{~}c}
          \IFrac{1}{4}  & 0 & \IFrac{1}{12} \\
          \IFrac{1}{12} & 0 & \IFrac{1}{4}\\
         \end{array}
  \right)~.
\label{e64957}
 \Pi_{P_4}'{:}\quad
  \left (\begin{array}{c@{~}c@{~}c}
          \IFrac{1}{4}  & 0 & \IFrac{1}{12} \\
          \IFrac{1}{12} & 0 & \IFrac{1}{4} \\
          0 & 0 & 0
         \end{array}
  \right)~.
\label{e1329}
 \Vh\From~(\,\PSet{}{}{1\Att{1}{2}, 2\Att{1}{4}, 3\Att{1}{4}}
           ~~\If~\Vh{=}1~\Else~~
           \PSet{}{}{2\Att{1}{2}, 3\Att{1}{2}}\,)~,           

 \left(
  \begin{array}{c@{~}c@{~}c}
   \IFrac{1}{2} & \IFrac{1}{4} & \IFrac{1}{4} \\
   0 & \IFrac{1}{2} & \IFrac{1}{2} \\
   0 & \IFrac{1}{2} & \IFrac{1}{2}
  \end{array}
 \right)~.

  \left (\begin{array}{c@{~}c@{~}c}
          \IFrac{1}{6} & 0 &  0 \\
          \IFrac{1}{6} & 0 & \IFrac{1}{6} \\
          0 & 0 & \IFrac{1}{6}
         \end{array}
  \right)
 \times
 \left(
  \begin{array}{c@{~}c@{~}c}
   \IFrac{1}{2} & \IFrac{1}{4} & \IFrac{1}{4} \\
   0 & \IFrac{1}{2} & \IFrac{1}{2} \\
   0 & \IFrac{1}{2} & \IFrac{1}{2}
  \end{array}
 \right)
 \Wide{=}
  \left(\begin{array}{c@{~\,}c@{\,~}c}
        \IFrac{1}{12} & \IFrac{1}{24} &  \IFrac{1}{24} \\
        \IFrac{1}{12} & \IFrac{1}{8} & \IFrac{1}{8} \\
        0 & \IFrac{1}{12} & \IFrac{1}{12}
       \end{array}
  \right)~.

  \left(\begin{array}{c@{~}c@{~}c}
        1 & \IFrac{1}{2} & 0 \\
        0 & \IFrac{1}{2} & 1 \\
        0 & 0 & 0
  \end{array}\right)
 \times
  \left (\begin{array}{c@{~}c@{~}c}
          \IFrac{1}{6} & 0 &  0 \\
          \IFrac{1}{6} & 0 & \IFrac{1}{6} \\
          0 & 0 & \IFrac{1}{6}
         \end{array}
  \right)
 \Wide{=}
  \left (\begin{array}{c@{~}c@{~}c}
         \IFrac{1}{4}  & 0 & \IFrac{1}{12}\\
         \IFrac{1}{12} & 0 & \IFrac{1}{4} \\
         0 & 0  & 0
        \end{array}
  \right)~.

  \left (\begin{array}{c@{~}c@{~}c}
          \mbox{\boldmath}  & 0 & \IFrac{1}{12} \\
          \IFrac{1}{12} & 0 & \mbox{\boldmath} \\
          0 & 0 & 0
         \end{array}
  \right)
\WideRm{with maxima selected by the \emph{strategy} matrix :}
  \left (\begin{array}{c@{~}c@{~}c}
         1  & 0 & 0\\
         0 & 0 & 1\\
         0 & 0 & 1
        \end{array}
  \right)
\label{e1605}
 \General{\MAX}{\,\textrm{strategy matrices }}{}{\Tr.(\Transpose{G}{\times}\Pi)}

  \left (\begin{array}{c@{~}c@{~}c}
         1 & 0 & 0 \\
         0 & 0 & 0 \\
         0 & 1 & 1
        \end{array}
  \right)
  \times
  \left (\begin{array}{c@{~}c@{~}c}
          \mbox{\boldmath}  & 0 & \IFrac{1}{12} \\
          \IFrac{1}{12} & 0 & \mbox{\boldmath} \\
          0 & 0 & 0
         \end{array}
  \right)
 \Wide{=}
  \left (\begin{array}{c@{~}c@{~}c}
         \mbox{\boldmath} & 0 & \IFrac{1}{12} \\
         0 & \mbox{\boldmath} & 0 \\
         \IFrac{1}{12} & 0 & \mbox{\boldmath} 
        \end{array}
  \right) ~,
 
\Set{G\In \StratMatrices_N}{}{\Transpose{G}} 
\Wide{=} \FinerMatrices_N 
\Wide{\subseteq} \RefMatrices_N ~.
\label{e29374}
\RefMatrices_{N} \Wide{=} \CCL.(\FinerMatrices_N)~.
\label{e8564}
\Et.\Pi
\Wide{=}
\General{\MAX}{G\In\StratMatrices_N}{}{\Tr.(\Transpose{G}{\times}\Pi)}
\Wide{=}
\General{\MAX}{R\In\RefMatrices_N}{}{\Tr.(R{\times}\Pi)} ~,
\label{e58762}
(\RefMatrices_{N}, {\times}, \UnitMatrix_{N}) \Wide{\textrm{is a monoid,}}

\If~\Vv{=}v'~\Then~\Vh\From D.\Vh~\Else~\Vh\Gets0~\Fi

 \Tr.(\widehat{\Pi}{\times}\Transpose{X})
 \Wide{<}
 \Tr.(\Pi_I{\times}\Transpose{X})
 \hspace{3em}
 \textrm{for all  refining }
\label{e1253}
 \Begin~\Vis~\Vv;~ \Vv\Gets E~\End ~,

 \Reveal~\Va{\Xor}\Vb;~\Reveal~\Vb{\Xor}\Vc
 \Wide{=}
 \Reveal~\Vb{\Xor}\Vc;~\Reveal~\Vc{\Xor}\Va ~,
\label{e1028a}
 \Sem{\Atomic{P}}.(v,\delta) \Wide{\Defs} \Hide.\Exp{h\In \delta~}{~\SemC{P}.(v,h)}~.

 \Atomic{P;Q} \Wide{=} \Atomic{P}; \Atomic{Q} ~.
\label{e1105}
\Skip
\Wide{=}
\Begin~\Vis~\Vv; \Hid~\Vh;~(\Vv{\Xor}\Vh)\Gets E ~\End ~.

 \Begin~\Vis~\Vv;\Hid~\Vh;~ (\Vv{\Xor}\Vh)\Gets E~\End~.
\label{e1359}
 (\Vv{\Xor}\Vh)\Gets E
 \WIDE{=}
 \Vh\Gets\True{\PC{}}\False;~\Vv\Gets E{\Xor}\Vh ~,

 \Vv\Gets\neg E{\PC{p}}E;~~\Vh\Gets \Vv{\Xor}E

 \Vv\Gets\neg E\PC{1/2}E \WIDE{=} \Vv\Gets\True\PC{1/2}\False
0.5em]
 \hspace{-1em}\Vis_{B}~\Vb; \Vis_{C}~\Vc;
  & \hspace{3em}\textrm{\small\ These are global variables.} \\
 \Reveal~\Vb\land \Vc~,
}
\end{Reason}\end{NumMini}and its similarity to \Eqn{e1035} is clear: a compound outcome  is published without revealing the components  --- except that, just as before, if for example the revealed outcome is \False\ but  is \True, then  can deduce that  must have been \False\ (and similar).

We develop an implementation of \Eqn{e1058} in several steps, as follows. Note that for some steps the \emph{justification} varies depending on the agent although we have arranged that the claimed equality is valid for all of them. We have
\begin{Reason}
\StepR{\HangLeft{\Eqn{e1058}} }{identity}{
 \Skip; \\
 \Reveal~\Vb\land \Vc
}
\Space
\StepR{}{Encryption Lemma for ; \\ obvious for ; see below ().}{
 \Begin~\Vis_{B}~\Vb_{0},\Vb_{1};~ (\Vb_{0}{\Xor}\Vb_{1})\Gets \Vb;~ \Reveal~\Vb_{0} \End; \\
 \Reveal~\Vb\land \Vc
}
\Space
\StepR{}{Revelation algebra: in this context \HangRight{\quad} \\where .}{
 \InQuadL{\Begin}\Vis_{B}~\Vb_{0},\Vb_{1}; \\
 \quad (\Vb_{0}{\Xor}\Vb_{1})\Gets \Vb;~ \Reveal~\Vb_{0}; \\
 \quad \Reveal~\Vb_{\Vc} \\
 \End
}
\Space
\StepR{}{Delegate second revelation to Agent .}{
 \InQuadL{\Begin}\Vis_{B}~\Vb_{0},\Vb_{1}; \\
 \quad (\Vb_{0}{\Xor}\Vb_{1})\Gets \Vb;~ \Reveal~\Vb_{0}; \\
 \quad\InQuadL{\Begin}\Vis_{C}~\Vc_{0};~ \Vc_{0}\Gets \Vb_{\Vc};~ \Reveal~\Vc_{0}~\End \\
 \End
}
\Space
\StepR{}{Rearrange declarations; clean up.}{
 \InQuadL{\Begin}\Vis_{B}~\Vb_{0},\Vb_{1};~ \Vis_{C}~\Vc_{0}; \
 \Begin~\Hid~\Vb_{0},\Vb_{1};~ (\Vb_{0}{\Xor}\Vb_{1})\Gets \Vb;~ \Reveal~\Vb_{0} \End~,

 \Begin~\Vis~\Vb_{0},\Vb_{1};~ (\Vb_{0}{\Xor}\Vb_{1})\Gets \Vb;~ \Reveal~\Vb_{0} \End

 \Vb_{0}{\Xor}\Vb_{\Vc}
 ~\equiv~
 (\Vb_{0}{\Xor}\Vb_{1}~\If~\Vc~\Else~\Vb_{0}{\Xor}\Vb_{0})
 ~\equiv~
 (\Vb~\If~\Vc~\Else~\False)
 ~\equiv~
 \Vb\land \Vc ~.

 (\Va{+}\Vb{+}\Vc \geq 2)
 \Wide{\equiv}
 \Va\land(\Vb\lor \Vc) ~\lor~ \Vb\land \Vc
 \Wide{\equiv}
 (\Vb\lor \Vc ~\If~\Va~\Else~ \Vb\land \Vc) ~,
0.5em]
 \quad\If~\Va \\
 \quad\quad\makebox[2.3em][l]{\Then}~ \Reveal~ \Vb\lor \Vc \\
 \quad\quad\makebox[2.3em][l]{\Else}
                          (\Vb_{0}{\Xor}\Vb_{1})\Gets \Vb;
                           & \hspace{1em}\textrm{\small\ Done privately by Agent .} \\
 \quad\quad\hspace{2.3em} \Va_{B}\Gets \Vb_{0};
                           & \hspace{1em}\textrm{\small\ Message .} \\
 \quad\quad\hspace{2.3em} \Vc_{0}\Gets \Vb_{\Vc};
                           & \hspace{1em}\textrm{\small\ Oblivious Transfer .} \\
 \quad\quad\hspace{2.3em} \Va_{C}\Gets \Vc_{0};
                           & \hspace{1em}\textrm{\small\ Message .} \\
 \quad\quad\hspace{2.3em} \Reveal~\Va_{B}{\Xor}\Va_{C}
                           & \hspace{1em}\textrm{\small\ Agent  announces majority verdict.} \\
 \quad\Fi \\
 \End\ldots
}
\end{Reason}

For the \Then-part we write  as  and adapt the \Else-part accordingly; the effect overall turns out to be replacing the initial  by  and changing the following assignment. Once we factor out the common portion of the conditional, we have
\begin{Reason}
\StepR{\HangLeft{\ldots}}{Using \emph{de Morgan}}{
 \multicolumn{2}{@{}l}{\InQuadL{\Begin} \Vis_{A}~\Va_{B},\Va_{C};~\Vis_{B}~\Vb_{0},\Vb_{1};~\Vis_{C}~\Vc_{0};} \.5em]
 \quad (\Vb_{0}{\Xor}\Vc_{0})\Gets \Vb\land \Vc; \\
 \quad (\Vb_{1}{\Xor}\Vc_{1})\Gets \Vb\lor \Vc; \\
 \quad \Reveal~(\Va{+}\Vb{+}\Vc \geq 2) \\
 \End
}
\Space
\StepR{}{Boolean algebra}{
 \InQuadL{\Begin} \Vis_{B}~\Vb_{0},\Vb_{1}; \Vis_{C}~\Vc_{0}, \Vc_{1}; \.5em]
 \quad (\Vb_{0}{\Xor}\Vc_{0})\Gets \Vb\land \Vc; \\
 \quad (\Vb_{1}{\Xor}\Vc_{1})\Gets \Vb\lor \Vc; \\
 \quad (\Va_{B}{\Xor}\Va_{C})\Gets \Vb_a{\Xor}\Vc_a; \\
 \quad \Reveal~\Va_{B}{\Xor}\Va_{C} \\
 \End\ldots
}
\end{Reason}
The point of using two variables  rather than one is to be able to split the transmission of information  into two separate oblivious transfers  and .

Thus the protocol boils down to three two-party computations: a conjunction , a disjunction  and an exclusive-or . The \RHS\ of the last is actually within an atomic conditional on , that is .

\subsection{Two-party exclusive-or}\label{s1045}

Our final step is to split the two-party exclusive-or into two separate assignments. This is achieved by introducing a local shared variable  that is visible to  only, i.e.\ not to , and encrypting both hidden variables with it. Thus we take the step
\begin{Reason}
\Step{}{
 (\Va_{B}{\Xor}\Va_{C})\Gets \Vb_{\Va}{\Xor}\Vc_{\Va}
}
\Space
\Step{}{
  \InQuadL{\Begin}\Vis_{B,C}~\Vh; \\
  \quad \Vh\Gets\True\PC{}\False; \\
  \quad \Va_B\Gets \Vb_a {\Xor}\Vh; \\
  \quad \Va_C\Gets \Vc_a {\Xor}\Vh\\
  \End ~,
}
\end{Reason}
justified trivially for  since the only assignments of non-constants are to variables visible only to . For  the justification comes from the the use of classical equality reasoning within a temporary atomicity block (refer \Lem{l1222}): the effect of the two fragments above on  is identical, and there are no overwritten visible values.

We will now show that in fact the extra variable  is not necessary: by absorbing it into earlier statements, and with some rearrangement of scopes we can rewrite our code at the end of \Sec{s1323} as
\begin{Reason}
\Step{\HangLeft{\ldots}}{
 \InQuadL{\Begin} \Vis_{A}~\Va_{B}, \Va_{C}; ~\Vis_{B}~\Vb_{0},\Vb_{1}; ~\Vis_{C}~\Vc_{0}, \Vc_{1}; ~\Vis_{B,C}~ \Vh; \0.5em]
 \quad \Va_B\Gets \Vb_a{\Xor}\Vh; \\
 \quad \Va_C\Gets \Vc_a{\Xor}\Vh; \\
 \quad \Reveal~\Va_{B}{\Xor}\Va_{C} \\
 \End\ldots
}
\end{Reason}
where in fact we have moved the declaration and initialisation of  right to the beginning. We now absorb it into the earlier two-party computations by introducing temporarily variables  and  which correspond to their unprimed versions except that they, too, are encrypted with . That gives
\begin{Reason}
\StepR{\HangLeft{\ldots}}{Boolean reasoning}{
 \InQuadL{\Begin} \Vis_{A}~\Va_{B}, \Va_{C}; \\
 \quad \Vis_{B}~\Vb_{0},\Vb_{1},\Vb'_{0},\Vb'_{1}; \\
 \quad \Vis_{C}~\Vc_{0}, \Vc_{1},\Vc'_{0}, \Vc'_{1}; \\
 \quad \Vis_{B,C}~ \Vh; \0.5em]
 \quad \Va_B\Gets \Vb'_a;
   & \hspace{1em}\textrm{\small\ Note primes, justified by earlier.} \\
 \quad \Va_C\Gets \Vc'_a;
   & \hspace{1em}\textrm{\small\ Assignments to .} \\
 \quad \Reveal~\Va_{B}{\Xor}\Va_{C}~\End\ldots
}
\end{Reason}
where we have replaced the  and  at the end of the code with their simpler, primed versions where the encryption in built-in. Now we can rearrange the statements using  so that not only  but also the unprimed  and  become auxiliary; that is, we have for the conjunction
\begin{Reason}
\Step{}{
  (\Vb_{0}{\Xor}\Vc_{0})\Gets \Vb\land \Vc; ~~
   \Vb'_0,\Vc'_0\Gets \Vb_0{\Xor}\Vh, \Vc_0{\Xor}\Vh
}
\StepR{}{introduce atomicity}{
  \InQuadL{\AtomicOpen}(\Vb_{0}{\Xor}\Vc_{0})\Gets \Vb\land \Vc; ~~
   \Vb'_0,\Vc'_0\Gets \Vb_0{\Xor}\Vh, \Vc_0{\Xor}\Vh ~\AtomicClose
}
\StepR{}{classical equality}{
  \InQuadL{\AtomicOpen}(\Vb'_{0}{\Xor}\Vc'_{0})\Gets \Vb\land \Vc; ~~
   \Vb_0,\Vc_0\Gets \Vb'_0{\Xor}\Vh, \Vc'_0{\Xor}\Vh ~\AtomicClose
}
\StepR{}{remove atomicity}{
  (\Vb'_{0}{\Xor}\Vc'_{0})\Gets \Vb\land \Vc; ~~
  \Vb_0,\Vc_0\Gets \Vb'_0{\Xor}\Vh, \Vc'_0{\Xor}\Vh ~,
}
\end{Reason}
and similarly for the disjunction. Removing the auxiliaries, and then applying a trivial renaming to get rid of the primes, we end up with
\begin{Reason}
\StepR{\HangLeft{\ldots}}{Consolidating the above}{
 \multicolumn{2}{l}{\InQuadL{\Begin} \Vis_{A}~\Va_{B}, \Va_{C};~ \Vis_{B}~\Vb_{0},\Vb_{1};~ \Vis_{C}~\Vc_{0}, \Vc_{1};} \0.5em]
 \quad \Reveal~\Va_{B}{\Xor}\Va_{C}~\End
}
\end{Reason}
which is precisely what we sought.

In \Fig{f1640} we give the code with the (two) two-party computations instantiated. In \Fig{f1641} we instantiate one of the (four) oblivious transfers.
\begin{Figure}{f1640}


We replace the two Two-Party 'junctions by their implementations as oblivious transfers: each becomes two statements instead of one. The random flipping of bits  is then collected at the start.

\medskip
The preservation of correctness is guaranteed by the compositionality of the security semantics.
\caption{Three-Judges Protocol assuming Oblivious Transfers as primitives}
\end{Figure}
\begin{Figure}[ht!]{f1641}
Starting from \Fig{f1640}, we replace the specification of the first of its four oblivious transfers  by an implementation in elementary terms \cite{McIver:09b}:



Each of the other three transfers would expand to a similar block of code, making about 40 lines of code in all. The Oblivious Transfer is formally derived elsewhere \cite{Morgan:07}; an informal explanation is given in \App{a1446}.

\medskip
The preservation of correctness, under expansion, is again guaranteed by the compositionality of the security semantics.

\medskip
Note that aside from the statement marked  (and its three other instances within the three other, unexpanded oblivious transfers), all messages are wholly public because of the declarations ; that is, all the privacy needed is provided already by the exclusive-or'ing with hidden Booleans, as  shows. 

The only private communications () are done with the aid of a trusted third party. As explained by Rivest \cite{Rivest:99,Morgan:07} this party's involvement occurs only \emph{before} the protocol begins, and it is trusted not to observe any data exchanged subsequently between the agents; alternatively, the subsequent transfers can themselves be encrypted without affecting the protocol's correctness. (A trusted third party without these limitations would implement trivially any protocol of this kind, simply by collecting the secret data, processing it and then distributing the result.)

\caption{Three-Judges Protocol in elementary terms}
\end{Figure}


\section{Conclusion: a challenge and an open problem}\label{s1731}
We have investigated the foundations for probabilistic non-interference security by proposing a semantics, and a refinement order between its programs, which we have demonstrated has connections with existing entropy-based measures. Especially it is related to Bayes Risk and we have given a soundness and completeness result that establishes compositional closure.

Our approach has a  general goal: to justify practical methods which support accurate analysis of programs operating in a context of probabilistic uncertainty. Abstraction underlies tractable analysis, but the results of such analyses become relevant \emph{only if} the method of abstraction aptly preserves the properties intended for examination. The impact of this research is to show firstly that our refinement order aptly characterises Bayes Risk, and secondly that the former discrepancies between Bayes Risk and other information orders can be rationalised by taking contexts into account.

By taking a fresh point of view, we have related entropies that were formerly thought to be inconsistent.
Furthermore, we highlight the \emph{similarities} between non-interference (as defence against an adversary) and large-scale structuring techniques (such as stepwise refinement and its associated information hiding \cite{Parnas:72}) for probabilistic systems. Both require a careful distinction between what data can be observed and what data must be protected; by observing that distinction in the definition of abstraction, we allow the tractable analysis of properties which rely on ``secrecy" (on the one hand) or ``probabilistic local state'' (on the other). This unified semantic foundation opens up the possibility for a uniform approach to the specification of security properties, along with other safety-critical features, during system design \cite{McIver:09f}.

These positive results now present a challenge and an open problem. The \underline{challen}g\underline{e} is to find a model where all three features, probability, nondeterminism and hidden state, can reside together, and then an equivalence between semantic objects which respects an appropriate definition of testing. The presence of nondeterminism would then include a treatment of distributed systems with schedulers having a restricted view of the state \cite{Chatzikokolakis:10}; that is because nondeterminism can be interpreted either as underspecification, or as a range of decisions presented to a scheduler.  Within such a model we would be increasing the power of the adversary  to harvest information about the hidden state by increasing the expressivity of the contexts she can create. It is an \underline{o}p\underline{en} p\underline{roblem} whether that increased power is sufficient to  make the various information-theoretic orders (Bayes Risk, Shannon Entropy, Marginal Guesswork etc.) equivalent or whether they remain truly distinct. 



\subsection*{Related techniques}

The use of information orders, such as those summarised in \Sec{s1028}, to determine the extent to which programs leak their secrets is widespread. Early work that took this approach includes  \cite{Millen:87,Wittbold:90,Gray:91}, and more recently it has been employed in \cite{Smith:07,Kopf:07,Braun:08,Clarkson:09,Malacaria:10}.
One of the contributions of this paper is to show how those evaluations can be related by taking a refinement-oriented perspective. Compositionality plays a major role in our definition of refinement and we note that other orders between probability distributions such as  the ``peakedness'' introduced by Dubois and H\"ullermeir  \cite{Dubois:05} appear not to be compositional when generalised to our hyperdistributions.

More significant than the particular information order is the way that it is used in the analysis of programs. Our approach uses specifications to characterise permitted leaks, and a refinement order which ensures that for our chosen information order (i.e. Bayes Risk), the implementation is at least as secure as its specification. An alternative mode is taken by Braun et al.\ \cite{Braun:08}. Rather than restricting  the elementary \emph{testing-relation}  to a compositional subset , they identify the  \emph{safe} contexts  such that \ERef. With our emphasis on implementations  \emph{and} their specifications , by analogy we would be looking for  implies .

Building on the theoretical approaches, others have investigated the use of automation to evaluate the quantitative weaknesses in programs. Heusser and Malacaria \cite{Heusser:10}, for example, have automated a technique based on Shannon entropy. Andr{\'e}s et al. \cite{Andres:10} similarly consider efficient calculation of information leakage, which can provide diagnostic feedback to the designer.

In some ways our semantics is related in structure to \emph{Hidden Markov Models} \cite{Jurafsky:00} suggesting that, in the future, the algorithmic methods developed in that field might apply to the special concerns of program development. A Hidden Markov Model considers a system partitioned into hidden states (our ) and observable outputs (similar to our ). The -state evolves according to a Markov Chain, in our terms repeated execution of a fragment  in which the probability of the next state  is given by a fixed ``matrix''  as  where  is the current state. Associated with each transition is an observation, in our terms execution of a fragment . Put together, therefore, the \HMM\ evolves according to repeated executions of the fragment

which fragment is a special case of our probabilistic-choice statements since the distributions on the right in \Eqn{e1411} do not depend on , whereas in \Fig{f1228} they can.

The canonical problems associated with \HMM's are (in the terms above)
\begin{enumerate}
\item Given the source code (that is, the matrices ), compute the probability of observing a given sequence of values assigned to .
\item Given a sequence of output values, determine the most likely values of .
\item Given the source code and a particular sequence of values assigned to , calculate the sequence of values assigned to  that was most likely to have occurred.
\end{enumerate}

The first of those is basically the classical semantics \cite{Kozen:85,McIver:05a}, but projected onto  since we are not interested in 's values. The second we do not treat at all --- it is tantamount to trying to guess a program's source code (in a limited repertoire) given the outputs it produces. The third is closest to our security concerns, since it is in a sense trying to guess  from observation of .

But in fact we address none of the three problems directly, since even in the third case we have a different concern: in \HMM\ terms we are comparing \emph{two} systems   and , asking  whether --according to certain entropy measures-- the entropy of the a-posteriori distribution of the final value of  is at least as secure in system  as it is in . Furthermore, our concern with compositionality would in \HMM\ terms relate to the question of embedding each of  and  ``inside'' another system .

The application of \HMM\ techniques to our work would in the first instance probably be in the efficient calculation of whether , the specification, was secure enough for our purposes: once that was done, the refinement relation would ensure that the implementation  was also secure enough, without requiring a second calculation. The advantage of this is that the first calculation, over a smaller and more abstract system, is likely to be much simpler than the second would have been.

\bibliographystyle{plain}
\bibliography{ProbsNew}

\newpage
\appendix

\section{Proofs for partition-based matrix representations}\label{a39475}

We give here the proofs for properties we relied on in \Sec{s1724}.

\subsubsection{Property \ref{e29374} (in \Sec{s1552}): Convex closure of refinement matrices}

To show that the set of  refinement matrices is the convex closure of the transpose of the set of  strategy matrices, i.e.\ that

we first observe  that every element in  is trivially non-negative and column-one-summing (that is, it is a refinement matrix). It remains to show  that any refinement matrix  in  can be expressed as an interpolation of matrices from .

We argue as follows. Fix , and identify a non-zero minimum element
in each of its columns; let  be the minimum of those column-minima;
select the  in  that has 1's in the column-minima
positions exactly; and subtract  from  to give some .

Now  has at least one more 0 entry than  did, and yet the columns of  still have equal sums, now . Continue this process from  onwards: it must stop, since the number of 0's increases each time; and when it does stop it must be because there is an all-0 column, in which case \emph{all} columns must be all-0, since the column sums have remained the same all the way through.\par
The collection of 's and their associated 's is the interpolation we had to find: for example, in three steps the procedure generates the interpolation


\subsubsection{Property \Eqn{e58762} (in \Sec{s1552}): refinement matrices form a monoid}

Since matrix multiplication is associative and the identity  is an element of , we need only demonstrate that  is closed under multiplication. That can be checked by direct calculation.

\section{Secure semantics via matrices} \label{a394759}

In \Sec{s1724} we appealed to matrix representations of partitions to construct our proof that  is the compositional closure of . Here we we project the rest of our semantics into matrix algebra, giving matrix representations of split-states, hyper-distributions, programs, and refinement. These representations are used to verify both monotonicity (\Thm{t1120} from \Sec{s9345}) in \App{a9475} and the Atomicity Lemma (\Lem{l1222} from \Sec{s9374}) in \App{a29375}.

\subsection{Notation}

For  taken from some ordered index set  we will write 
 
for the vertical concatenation of matrices  for those  satisfying , taken in -order: for this to be well defined, the column-count must be the same for all 's; but their row-counts may differ.  In the same way we will write 
 
for horizontal matrix concatenation (in which case the row-counts must agree).

For a given dimension  and expression  we write  for the  diagonal matrix whose value at the element (doubly) indexed  is . Thus for example we have .

\subsection{Split-states as single-column matrices}

Let  be the cardinality of . A split-state has type  and can be written as an  matrix, a row of probabilities in some agreed-upon index order of  where the element at (column) index  gives the probability  associated with that pair.

Naturally the row sums to 1 but --more than that-- each such representation of a split-state will have nonzero entries only in columns whose first index-component is the  appearing in . We say that such a row is \emph{-unique} and that it has \emph{characteristic} .

Write  for the  diagonal matrix  having ones only at positions whose row- (or equivalently column-) index has that  as its first component; elsewhere in the diagonal (and everywhere off the diagonal) the entries are zero. The row-matrix representation  of split-state  then satisfies  because it has characteristic , so that the multiplication by  sets to zero only elements that were zero already.


\subsection{Hyper-distributions as matrices}

In \Sec{s1552} we interpreted whole partitions as matrices, with each row (fraction) giving a possible distribution over  for some fixed . Here we proceed similarly, but we do not fix , so that a hyper-distribution  whose support has cardinality  is represented as an  matrix  each of whose rows is -unique, as above, thus independently representing some split-state . 
Extending the matrix representation of individual split-states, we can represent whole hyper-distributions according to

where, as in \Sec{s1552}, with the multiplier  we are scaling the rows so that the total weight of each gives the probability of that split-state in the hyper-distribution overall; the distribution the split-state actually contains (the  in the  that the row represents) is as usual recoverable by normalising. Because each of the rows is -unique we say that the matrix as a whole, also, is -unique; but note that it is possible to have several rows with the same characteristic . -uniqueness means that no two distinct 's appear with non-zero probability in the \emph{same} row.

As for partitions, in such matrices we define \emph{similarity} between rows and say that a hyper-distribution is in \emph{reduced} matrix representation if all its similar rows have been added together, and all its all-zero rows have been removed. We say that two hyper-distribution matrices are \emph{similar}  if their reductions are equivalent up to a reordering of rows. Similarity is a congruence for matrix multiplication on the right, but not on the left; vertical concatenation  respects similarity on both sides.

While the column-order of  is fixed by our (arbitrary) ordering of , the row-order might vary since there is no intrinsic order on fractions. We therefore regard  as determined only up to similarity, and our reasoning below will be restricted to operations for which similarity is a congruence. In particular we have that  implies , i.e.\ that  is injective up to similarity.


The operation  on (sub-)hyper-distributions is linear in the sense that


\subsection{Classical commands as matrices}

We recall from \Sec{s47563} that the classical ``relational'' semantics   of a program  is a function  and may hence be treated (just as  from \Sec{s1552} was) as an  matrix written  whose value in row  and  column  is just . 
\footnote{Note that operation  applies to texts, i.e.\ syntax but  applies to hyper-distributions, i.e.\ semantics.}

Sequential composition between classical commands is then represented by matrix multiplication, in the usual Markov style, so that we have


\subsection{Secure commands as matrices}

We will establish that for any secure program  there is an -indexed set of  matrices such that 

for any split-state . We think of the matrices as giving a \emph{normal form} for . Using the normal form, we will be able to represent the lifting of 's secure semantics using matrix operations, since then

can be established by the calculation
\begin{Reason}
\Step{}{
  \MatSem{\,\Exp{(v,\delta)\In \Delta}{\Sem{P}.(v,\delta)}\,}
}
\StepR{}{definition expected value \Sec{s1729}}{
  \MatSem{\,\General{\sum}{(v,\delta)\In \Support{\Delta}}{}{
    \Delta.(v,\delta) * \Sem{P}.(v,\delta)\,}
  }
}
\StepR{}{from \Eqn{e1503}}{
  \General{\Above}{(v,\delta)\In \Support{\Delta}}{}{
    \Delta.(v,\delta) * \MatSem{\Sem{P}.(v,\delta)}
  }
}
\StepR{}{normal form \Eqn{e1612}}{
  \General{\Above}{(v,\delta)\In \Support{\Delta}}{}{
    \Delta.(v,\delta) * 
    \General{\Above}{i\In I}{}{\MatSem{v,\delta}\times M_i}
  }
}
\StepR{}{distribute multiplication}{
  \General{\Above}{(v,\delta)\In \Support{\Delta}; i\In I}{}{ 
    (\Delta.(v,\delta) * \MatSem{v,\delta}) \times M_i
  }
}
\Space
\StepR{}{rearrange rows; distribute\\ post-multiplication}{
  \General{\Above}{i\In I}{}{ 
    \General{\Above}{(v,\delta)\In \Support{\Delta}}{}{
        \Delta.(v,\delta) * \MatSem{v,\delta}
    }
    \times M_i
  }
}
\Space
\StepR{}{from \Eqn{e1729} defining }{
  \General{\Above}{i\In I}{}{ 
     \MatSem{\Delta} \times M_i
  } ~.
}
\end{Reason}
We now show by structural induction how embedded classical commands, general choice, sequential composition (and hence all of our secure commands) can be translated into this normal form.

\subsubsection{Embedded classical commands}

In \Def{d0855a} from \Sec{s1044} we gave the semantics  of a program  considered as an atomic unit; we now do the same here in matrix style.

If we were to execute an atomic program  from a split-state , in the matrix style we would begin by calculating , giving again a single row; but that row might not be -unique, in which case a further step would be needed. We'd split its possibly non-unique rows into (maximally) -unique portions, an operation that corresponds roughly to the  funtion used in \Def{d0855}.

Given a row matrix  that is -indexed by column (such as the output  from just above) the splitting of its possibly non-unique row is achieved via

in which each of the values  in  is used, in turn, to construct a row matrix of characteristic  projected from  by zeroing all other entries: those characteristic- projections are then stacked on top of each other with ) to make a single (possibly quite tall!)\ matrix that is derived from  but now is -unique.\,\footnote{For example, if the row  is -unique already then  will stack up a great many all-zero rows. But still we will have , so no damage is done.}
With that apparatus, we have

thereby giving the -unique matrix representation (up to similarity) of the hyper-distribution output by  if executed from incoming split-state .\,\footnote{Note the algebra of similarity here: if we have  for some , then  is similar to the right-hand side above.}

\subsubsection{General choice}

For both general choice and sequential composition we assume inductively that the semantics of subprograms  and  can be written in matrix normal form so that for each split-state  we have


To show that general choice can be expressed in matrix normal form, we use the following identity which expresses the conditioning of a split-state  by expression  in terms of matrix operations:



We then have
\begin{Reason}
\Step{}{
  \MatSem{\,\Sem{P_1 \PC{q.\Vv.\Vh} P_2}.(v,\delta)\,}
}
\Space
\WideStepR{}{general choice from \Fig{f1228}; }{
  \MatSem{\,p*\Sem{P_1}.(v,\PSet{h\In\delta}{q.v.h}{}) + {}
  (1{-}p)*\Sem{P_2}.(v,\PSet{h\In\delta}{1{-}q.v.h}{})\,}
}
\Space
\WideStepR{}{from \Eqn{e1503}}{
  p * \MatSem{\,\Sem{P_1}.(v,\PSet{h\In\delta}{q.v.h}{})\,} \AboveRel {}
  (1{-}p)*\MatSem{\,\Sem{P_2}.(v,\PSet{h\In\delta}{1{-}q.v.h}{})\,}
}
\Space
\WideStepR{}{inductive assumption: matrix normal form of  and }{
\begin{array}{lrl}
  &
  p*{}&
  \General{\Above}{j_1\In J_{1}}{}{
    \MatSem{v,\PSet{h\In\delta}{q.v.h}{}} \times M_{1,j_1} 
  }\\
  \AboveRel &
  (1{-}p)*{}&
  \General{\Above}{j_2\In J_{2}}{}{     
    \MatSem{v,\PSet{h\In\delta}{1{-}q.v.h}{}} \times M_{2,j_2} 
  }
\end{array}
}
\Space
\WideStepR{}{distribute scalar multiplications}{
  &
  \General{\Above}{j_1\In J_{1}}{}{
    (p * \MatSem{v,\PSet{h\In\delta}{q.v.h}{}}) \times M_{1,j_1} 
  }\\
  \AboveRel &
  \General{\Above}{j_2\In J_{2}}{}{
    ((1{-}p)*\MatSem{v,\PSet{h\In\delta}{1{-}q.v.h}{}}) \times M_{2,j_2} 
  }
}
\Space
\WideStepR{}{recall ; from \Eqn{e4573}}{
  &
  \General{\Above}{j_1\In J_{1}}{}{
    \MatSem{v,\delta} \times \DiagM{q.\Vv.\Vh} \times M_{1,j_1}
  }\\
  \AboveRel &
  \General{\Above}{j_2\In J_{2}}{}{
    \MatSem{v,\delta} \times \DiagM{1{-}q.\Vv.\Vh} \times  M_{2,j_2}
  }
}
\Space
\WideStepR{}{Let  and 
                    }{
  \General{\Above}{i\In\{1,2\}; j\In J_i}{}{
     \MatSem{v,\delta} \times \DiagM{p_i.\Vv.\Vh} \times M_{i,j}
   } ~.
}
\end{Reason}

\subsubsection{Sequential composition}

For sequential composition of  and  we have
\begin{Reason}
\Step{}{
  \MatSem{\,\Sem{P_1;P_2}.(v,\delta)\,}
}
\StepR{}{Composition from \Fig{f1228}}{
  \MatSem[.9em]{\,\Exp{(\Vpk,\delta')\In\Sem{P_1}.(v,\delta)}{\Sem{P_2}.(\Vpk,\delta')}\,}
}
\StepR{}{\Eqn{l9745}; matrix normal form of }{
  \General{\Above}{j_2\In J_2}{}{
    \MatSem{\Sem{P_1}.(v,\delta)}
    \times M_{2,j_2}
  }
}
\StepR{}{matrix normal form of }{
  \General{\Above}{j_1\In J_1; j_2\In J_2}{}{
     \MatSem{v,\delta} \times M_{1,j_1}\times M_{2,j_2}
  } ~.
}
\end{Reason}

\subsection{Refinement as matrix multiplication}

In \Sec{s1552} we showed how refinement between partitions could be defined using matrix multiplication. We can promote this to hyper-distributions by dealing with each  separately: we have that hyper-distribution  is refined by  just when for each  there exists a refinement matrix (i.e. a non-negative, column one-summing matrix)  such that

The effect of requiring similarity for each  separately is to prevent rows with differing 's from being added together.

\section{Proofs for the refinement relation}

\subsection{Secure programs are partially ordered by }\label{a2937}

We show (\Thm{t1042} in \Sec{s9345}) that the refinement relation  defines a partial order over hyper-distribut\-ions. It follows by extension that it is a partial order over secure programs.


\subsubsection{Reflexivity} For any hyper-distribution  reflexivity holds trivially since, for each , the intermediate partition  is both similar to and as fine as itself.

\subsubsection{Transitivity} Assume that  and . It is enough to show that for each  we have .
For each  let  be the  matrix representation (\Sec{s1552}) of  for some .
To prove , we need to find a refinement matrix  such that  is .

From above there are refinement matrices  with  and . Thus  defined  satisfies , and it is a refinement matrix by Property \Eqn{e58762} from \Sec{s1552}.

\subsubsection{Antisymmetry}

Assume that both  and  but . From the first and third assumptions, with \Lem{l2121} (\App{s3628}) we have that the Shannon Entropy 
of  is strictly less than that of ; from the second and third, we have the opposite --- thus a contradiction.


\subsection{Monotonicity of secure programs w.r.t.\ } \label{a9475}

We use the following technical results to verify that  is monotonic with respect to secure program contexts (\Thm{t1120} from \Sec{s9345}). They are verified using the matrix algebra from \App{a394759} above.

\begin{Lemma}{}{l35679}
For any indexed set of matrices  each of dimension  and corresponding refinement matrices  each having  rows and  columns, there exists a refinement matrix  such that 

\begin{Proof}
Refinement matrix  can be given directly as

That  is a refinement matrix (i.e. it has non-negative entries and is column-one-summing) follows from its definition and the fact that each  is a refinement matrix. It can be established by matrix multiplication that \Eqn{l59867} holds.\,\footnote{A sketch of the block matrices helps to see the pattern.}
\end{Proof}
\end{Lemma}

\begin{Lemma}{Additive monotonicity of hyper-distributions}{l9237}
For probability , and hyper-distributions  we have that   implies 

\begin{Proof}
From \Eqn{e0231} it is enough for each  to find a refinement matrix  such that 

We have:
\begin{Reason}
\Step{}{
 \MatSem{\,\Delta_{I_1} \PC{p} \Delta_{I_2}\,} \times \IdM{v}
}
\StepR{}{from \Eqn{e1503}}{
 (\,p{*}\MatSem{\Delta_{I_1}} \AboveRel 
 (1{-}p){*}\MatSem{\Delta_{I_2}}\,) 
\times \IdM{v}
}
\StepR{}{distribute post-multiplication}{
                p{*}\MatSem{\Delta_{I_1}} \times \IdM{v} 
\AboveRel  (1{-}p){*}\MatSem{\Delta_{I_2}} \times \IdM{v}
}
\Space
\WideStepR{}{ implies 
 for some refinement \\matrix }{
                p * R_1 \times \MatSem{\Delta_{S_1}} \times \IdM{v} 
\AboveRel  (1{-}p)* R_2 \times \MatSem{\Delta_{S_2}} \times \IdM{v}
}
\Space
\WideStepR{}{commute scalar multiplication; distribute post-multiplication}{
 (\,        R_1 \times      p{*}\MatSem{\Delta_{S_1}} 
 \AboveRel  R_2 \times (1{-}p){*}\MatSem{\Delta_{S_2}} \,)
 \times \IdM{v} 
}
\Space
\WideStepR{}{\Lem{l35679} for some refinement matrix }{
 R \times
 (\,              p * \MatSem{\Delta_{S_1}} 
 \AboveRel   (1{-}p)* \MatSem{\Delta_{S_2}} \,)
 \times \IdM{v} 
}
\StepR{}{from \Eqn{e1503}}{
  R \times \MatSem{\,\Delta_{S_1} \PC{p} \Delta_{S_2}\,} \times \IdM{v} ~.
}
\end{Reason}
\end{Proof}
\end{Lemma}


\begin{Lemma}{Pointwise monotonicity}{l02834}
For all program texts  and hyper-distribut\-ions  and  such that , we have

\begin{Proof}
Let  be a set of  matrices giving the normal form for  as at \Eqn{e1612} above, so that for any  we have

From \Eqn{e0231} and \Eqn{l9745}, it is enough to show that for each  there exists a refinement matrix  such that
. 
We have 
\begin{Reason}
\Step{}{
 \General{\Above}{i\In I}{}{\MatSem{\Delta_I}\times M_i}
 \times \IdM{v'} 
}
\StepR{}{ is -unique}{
  \General{\Above}{i\In I}{}{
    \General{\Above}{v\In \VV}{}{\MatSem{\Delta_I} \times \IdM{v}}
    \times M_i 
  }
  \times \IdM{v'} 
}
\StepR{}{distribute post-multiplication}{
  \General{\Above}{i\In I; v\In \VV}{}{
    \MatSem{\Delta_I} \times \IdM{v} \times M_i
  }
  \times \IdM{v'} 
}
\Space
\WideStepR{}{ implies 
 for some\\ refinement matrix }{
  \General{\Above}{i\In I; v\In \VV}{}{
    R_v \times \MatSem{\Delta_S} \times \IdM{v} \times M_i
  }
  \times \IdM{v'} 
}
\Space
\StepR{}{\Lem{l35679} \\ for some refinement matrix }{
  R \times 
  \General{\Above}{i\In I; v\In \VV}{}{
    \MatSem{\Delta_S} \times \IdM{v} \times M_i
  }
  \times \IdM{v'} 
}
\Space
\StepR{}{distribute post-multiplication; \\
 is -unique}{
  R\times \General{\Above}{i\In I}{}{\MatSem{\Delta_S}\times M_i} \times \IdM{v'} ~.
}
\end{Reason}
\end{Proof}
\end{Lemma}

\subsubsection{Monotonicity of secure programs w.r.t.\ } 
Using \Lem{l9237} and \Lem{l02834} we now prove \Thm{t1120} from \Sec{s9345}. We must show that if  then  for all contexts  built from programs as defined in \Fig{f1228}.

We use structural induction. For the base case, context  is trivially monotonic. 

General probabilistic choice (and hence probabilistic and conditional choice) is trivially monotonic in either argument from monotonicity of addition over hyper-distributions (\Lem{l9237}). For example, for monotonicity in the first argument we have
\begin{Reason}
\Step{}{
  \Sem{S \PC{q.\Vv.\Vh} R}.(v,\delta)
}
\WideStepR{}{Let ; General choice from \Fig{f1228}}{
(\Sem{S}.(v, \PSet{h\In \delta}{q.v.h}{}) 
 \PC{q_{\delta}} 
 \Sem{R}.(v, \PSet{h\In \delta}{1{-}q.v.h}{}) )
}
\Space
\StepR{}{; \Lem{l9237}}{
(\Sem{I}.(v, \PSet{h\In \delta}{q.v.h}{}) 
 \PC{q_{\delta}} 
 \Sem{R}.(v, \PSet{h\In \delta}{1{-}q.v.h}{}) )
}
\StepR{}{General choice from \Fig{f1228}}{
  \Sem{I \PC{q.\Vv.\Vh} R}.(v,\delta) ~.
}
\end{Reason}

To show monotonicity of sequential composition in its right-hand argument we have for any programs  and  and initial state  that
\begin{Reason}
\Step{}{
  \Sem{R;S}.(v,\delta)
}
\StepR{}{Composition from \Fig{f1228}}{
  \Exp{(v',\delta')\In \Sem{R}.(v,\delta)}{\Sem{S}.(v',\delta')}
}
\StepR{}{; \Lem{l9237}}{
  \Exp{(v',\delta')\In \Sem{R}.(v,\delta)}{\Sem{I}.(v',\delta')}
}
\StepR{}{Composition from \Fig{f1228}}{
  \Sem{R;I}.(v,\delta) ~.
}
\end{Reason}
For monotonicity in the first argument we have
\begin{Reason}
\Step{}{
  \Sem{S;R}.(v,\delta)
}
\StepR{}{Composition from \Fig{f1228}}{
  \Exp{(v',\delta')\In \Sem{S}.(v,\delta)}{\Sem{R}.(v',\delta')}
}
\StepR{}{ and \Lem{l02834}}{
  \Exp{(v',\delta')\In \Sem{I}.(v,\delta)}{\Sem{R}.(v',\delta')}
}
\StepR{}{Composition from \Fig{f1228}}{
    \Sem{I;R}.(v,\delta)~.
}
\end{Reason}



\section{Example of completeness construction}\label{s9347} 


Here we illustrate the completeness proof set out in \Sec{s1634} by applying it to the example of \Sec{s1457}, where we claimed that . We use \Sec{s1634} to find a  such that indeed .

\medskip
Our  is 1, since that is where we find the difference between  and  in the residual uncertainties of ; with that, we extract the fractions

and find that there are two values of , two fractions in  and three fractions in . Accordingly we set  to 3 and include an extra column for  and an extra, zero fraction in . Note that  and that the total weight (of each) is .

\parshape=5  0pt 1\linewidth 0pt .75\linewidth 0pt .75\linewidth 0pt .75\linewidth 0pt 1\linewidth
The  matrix corresponding to  is then as at right
\hfill
\raisebox{-1.5em}[0pt][0pt]{

}\\
with its columns corresponding to values  of  and the rows to 's three fractions. The point obtained by concatenating the rows is , and is in 9-dimensional space; but to avoid a proliferation of fractions, we scale everything up from now on by a factor of 12, and so take  to be the point .

Now the scaled-up (and extended) matrix corresponding to , with a selection of refinement-forming matrices  in , is given by

Carrying out the matrix multiplications gives us these four possible refinements of :

Doing all of them for  in , and concatenating their rows to make points in 9-dimensional space, gives us this collection of refinements altogether:

Our claim that  suggests that the point  (corresponding to the matrix derived from ) should not lie in the convex closure of the points \Eqn{e93475} above.

We can see this easily by concentrating on the first and third dimensions only: for  we get ; and for , that is \Eqn{e93475}, after removing duplicates we get , ,  and  The -point  is not in the convex closure of the other four because all of them have their two coordinates both positive or both zero, a property preserved by any convex combination but not shared by .

\begin{wrapfigure}{l}{.5\textwidth}
\setlength{\unitlength}{.9cm}
\begin{picture}(5.5,4.8)(-0.5,-1)
\put(0,-0.5){\line(0,1){5}} \put(-0.5,0){\line(1,0){5}} \multiput(1,-.25)(1,0){4}{\line(0,1){0.5}} \multiput(-.25,1)(0,1){4}{\line(1,0){0.5}} \put(2,0){\circle{.2}} \put(0,0){\circle*{.2}} \put(1,3){\circle*{.2}} \put(3,1){\circle*{.2}} \put(4,4){\circle*{.2}} \put(0,0){\line(3,1){3}} \put(0,0){\line(1,3){1}} \put(3,1){\line(1,3){1}} \put(1,3){\line(3,1){3}} \thicklines
\put(-0.5,-0.5){\vector(3,1){5}} \put(4.5,1.25){\makebox(0.25,.5){\small}}
\end{picture}


\small We insert a hyperplane (just a line, in 2-space) midway between the separated point and the convex shape, parallel to the boundary of the latter.
\caption{Finding a separating hyperplane  in 2-space.}\label{f1446}
\end{wrapfigure}

Now that we can concentrate on just two dimensions, it's easy to find a separating hyperplane with a picture. \Fig{f1446} shows the -point as an open circle at , while the filled circles give the vertices of the diamond-shaped convex closure corresponding to refinements of . Clearly the (degenerate) hyperplane  separates the point from the diamond. The normal of that hyperplane (up and left, perpendicular to the line) has direction , and when we fill in the other seven dimensions as zero --since they're not needed for separation-- that gives us a candidate normal of  in 9-space. By translating  to matrix form and transposing it, we can then give a tentatitive definition of  as shown at right. However this is not quite our final value for it. 
\hspace{\fill}
\raisebox{-1.8em}[0pt][0pt]{

}

\parshape=4  0pt .75\linewidth 0pt .75\linewidth 0pt .75\linewidth 0pt 1\linewidth
The dot-product of  with , that is , turns out to be ; and with the refinements of  shown at \Eqn{e93475} we get the dot-products  and  (multiple times), showing indeed the separation we expect, but in the wrong direction: the values 0 and 8 for  are both strictly greater than the value  for , and we want them to be strictly less. Accordingly we multiply the tentative  above by 
\footnote{The fact we can simply multiply by  to reverse the sense of the comparison does not mean we can just as easily construct a context to show  --- which would indeed be a worry. In fact for  we'd need a shape  and a point ; and then we would find  \emph{inside} the shape , thus unable to be separated from it.}
and then add 3 to all its elements to make them non-negative; finally we divide everything by 10 to make its rows sum to no more than 1. To get the final  from this we must then add a new ``zero-th column'' to make each row one-summing exactly. That gives
\vspace{1em}
The distinguishing context , say, must then overwrite  according to the distribution given by a row of , the one selected by the value  of  incoming to ;
thus we construct  to be

with the outer \If\ effectively restricting our attack to occur only when . (That allows us to ignore the  case in , as well.) Thus we have our context . Let us now check that it actually works.

We begin with . Its output hyper-distribution is (after some calculation) given by

whose Bayes Vulnerability is .
On the other hand, for  the output hyper-distribution is

and here the vulnerability is . Note that in the third summand we took  rather than the larger  associated with 0, since as part of our construction we exclude guesses that  is 0.\,\footnote{\label{fn1223}Dealing with this detail would split the 0-case in half, uniformly distributed over , since the resulting probability  for each would then be small enough that a Bayes-Vulnerability attack would never choose it. The adjusted context  would contain

and the resulting output for  would be

in which neither  nor  would ever be chosen for a Bayes-Vulnerability attack.
}


\begin{wrapfigure}[18]{R}{.58\textwidth}
\includegraphics[scale=.32]{hyperplaneOmnigraffle.png}
\caption{Finding a separating hyperplane, with (2,1,1) as normal, in 3 of the 9 dimensions.}\label{f1854}
\end{wrapfigure}
Thus we have established that  (for the adjusted  --- see Footnote \ref{fn1223}), because the vulnerability of the former is  but for the latter the vulnerability is the greater . Hence when our refinement relation insists that  --as we argued earlier above-- in fact it is not being too severe, but rather it is acting just as a compositional closure should. It protects us not only against the context  we just made, but all other contexts too --- in spite of the fact that in isolation  and  are not distinguished by elementary testing.

Finally, in this example there are many hyperplanes with distinct normals that achieve the separation we need, and each of these may be used to construct different distinguishing contexts. For example, since there exists a separating hyperplane with normal

we can use it to define another distribution matrix

from which we can specify the distinguishing context , where  is

which requires no   case for  since the rows of its defining normal just happen to have the same sum. (That's not true for the middle row; but as before we can ignore it since, in the  case we are considering, that row is never used.)
\footnote{It can be shown that this is a legitimate counter-example by using  \Eqn{e1948} and \Eqn{e1947} to calculate the partitions

and observing that the vulnerability of 
 is , which is just smaller than the vulnerability of  at . 
} 

Finding the normal that generates \Eqn{e1947}, however, is harder if done geometrically: it turns out that we would have had to specialise to three coordinate indices 3, 4 and 7 rather than just 1 and 3. 
The resulting inspection --to see just where to slip the hyperplane in between-- would then have had to be done in three- rather than two dimensions, as \Fig{f1854} illustrates (in a side view). In general such hyperplanes can of course be found, without drawing pictures, by using constraint solvers to deal with the linear inequalities symbolically.\par



\section{Proof of the Atomicity Lemma} \label{a29375}

To prove the atomicity distribution lemma (\Lem{l1222} from \Sec{s9374}) we use the matrix algebra from \App{a394759}.

Suppose we have matrix representations  for the classical program texts  and  and a row-matrix representation  of an incoming split-state.

If from every initial and final -state of  it is possible to determine the intermediate value of  (after  and before ) then there must exist a total function  such that for all  we have

from which we have for all  that

where  is the  matrix of zeros.

Assuming such an  with properties \Eqn{e9735} and \Eqn{e57647}, we can calculate
\begin{Reason}
\Step{}{
 \MatSem[.9em]{\,\Sem{\Atomic{P_1;P_2}}.(v,\delta)\,}
}
\StepR{}{embedding}{
  \General{\Above}{v'\In\VV}{}{
   \MatSem{v,\delta}\times\MatSemC{P_1;P_2}\times\IdM{v'}}
}
\StepR{}{classical composition}{
  \General{\Above}{v'\In\VV}{}{
   \MatSem{v,\delta}\times\MatSemC{P_1}\times\MatSemC{P_2}\times\IdM{v'}}
}
\StepR{}{}{
  \General{\Above}{v'\In\VV}{}{
   \MatSem{v,\delta}\times\IdM{v}\times\MatSemC{P_1}\times\MatSemC{P_2}\times\IdM{v'}}
}
\StepR{}{\Eqn{e9735}}{
  \General{\Above}{v'\In\VV}{}{
   \MatSem{v,\delta}\times\IdM{v}\times\MatSemC{P_1}\times\IdM{f.v.v'}\times\MatSemC{P_2}\times\IdM{v'}}
}
\Space
\StepR{}{one-point rule\\ for }{
  \General{\Above}{\Vpk,\hat{v}\In\VV}{\hat{v}{=}f.v.v'}{
   \MatSem{v,\delta}\times\IdM{v}\times\MatSemC{P_1}\times\IdM{\hat{v}}\times\MatSemC{P_2}\times\IdM{v'}}
}
\Space
\WideStepR{}{ is unit of concatenation, up to similarity}{
  &\General{\Above}{\Vpk,\hat{v}\In\VV}{\hat{v}{=}f.v.v'}{
   \MatSem{v,\delta}\times\IdM{v}\times\MatSemC{P_1}\times\IdM{\hat{v}}\times\MatSemC{P_2}\times\IdM{v'}} \\
  \Above& \General{\Above}{\Vpk,\hat{v}\In\VV}{\hat{v}{\neq}f.v.v'}{\ZeroMatrix_{1{\times}N}}
}
\Space
\WideStepR{}{, \Eqn{e57647}}{
  &\General{\Above}{\Vpk,\hat{v}\In\VV}{\hat{v}{=}f.v.v'}{
   \MatSem{v,\delta}\times\IdM{v}\times\MatSemC{P_1}\times\IdM{\hat{v}}\times\MatSemC{P_2}\times\IdM{v'}} \\
  \Above& \General{\Above}{\Vpk,\hat{v}\In\VV}{\hat{v}{\neq}f.v.v'}{\MatSem{v,\delta}\times\IdM{v}\times\MatSemC{P_1}\times\IdM{\hat{v}}\times\MatSemC{P_2}\times\IdM{v'}}
}
\Space
\WideStepR{}{ and  are disjoint and exhaustive;\\
 is commutative and associative up to similarity}{~\\
  \General{\Above}{\Vpk,\hat{v}\In\VV}{}{
   \MatSem{v,\delta}\times\IdM{v}\times\MatSemC{P_1}\times\IdM{\hat{v}}\times\MatSemC{P_2}\times\IdM{v'}} \\
}
\Space
\StepR{}{}{
  \General{\Above}{\Vpk,\hat{v}\In\VV}{}{
   \MatSem{v,\delta}\times\MatSemC{P_1}\times\IdM{\hat{v}}\times\MatSemC{P_2}\times\IdM{v'}} \\
}
\StepR{}{distribute }{
  \General{\Above}{v'\In\VV}{}{
   \General{\Above}{\hat{v}}{}{\MatSem{v,\delta}\times\MatSemC{P_1}\times\IdM{\hat{v}}}\times\MatSemC{P_2}\times\IdM{v'}} \\
}
\StepR{}{composition, embedding}{
 \MatSem[.9em]{\Sem{\Atomic{P_1};\Atomic{P_2}}.(v,\delta)}~,
}
\end{Reason}
whence our result follows because  is injective up to similarity and  was arbitrary.


\section{Informal description of the Oblivious Transfer implementation\,\protect\footnotemark}\label{a1446}
\footnotetext{An even more informal description is this fairy tale. An Apprentice magician is about to graduate, and he must now choose between black- or white magic. His Sorcerer will allow him to read either the \emph{Black Tome} or the \emph{White Tome}, not both; and his choice must be his own, uncoerced, thus never revealed to the Academy.

The Sorcerer summons a \emph{trusted third party} Djinn who gives him two locks, one black and one white; and the Djinn gives a single, golden key to the Apprentice. On the key is a small dot that only the Apprentice can see: it is the colour of the matching lock. The Djinn then returns to his own dimension.

The Apprentice tells the Sorcerer to match the lock colours to the Tomes, or to reverse them: it depends on whether his choice matches the colour of the dot. The Sorcerer then leaves; the Apprentice can unlock only the Tome of his choice; and --provided he locks it again-- no one afterwards will know which one he read.} 

Given are two agents ; Agent  has two messsages , bit-strings of the same length; Agent  has a message variable  and a choice  of which of  is to be assigned to . The specification is thus

Note that  does not discover  and that  does not discover .

\medskip
The implementation is, informally, as follows:
\begin{enumerate}
\item[]~
\item[] \hrulefill\textit{This is the prelude of the protocol}\hrulefill
\item\label{i1620} Agent  chooses privately two random bit-strings  to be used for -encrypting  respectively.
\item\label{i1621} Agent  chooses privately in  which of the encrypting strings  will be revealed to her.
\item\label{i1619} A trusted third party collects both  from , collects  from  and then reveals (only)  to . She throws  away, and then leaves.
\item[]~
\item[] \hrulefill\textit{From here is the main part of the protocol}\hrulefill
\item\label{i1622} Agent  then tells  to encrypt and send messages in the following way:
\begin{enumerate}
\item[]~
\item\label{i1610} If Agent  wants  and has , then she instructs  to send her both  and . Because she has  she can recover  via ; but she cannot recover .
\item\label{i1609} Similarly, if Agent  wants  but has  instead (of ), then she simply instructs  to send her both  and , i.e.\ with the encryption the other way around.
\item If Agent  wants  and has  --- as for \Itm{i1609}.
\item\label{i1630} If Agent  wants  and has  --- as for \Itm{i1610}.
\end{enumerate}

The four cases (\ref{i1610}--\ref{i1630}) can be described succinctly --if cryptically-- simply by instructing  to send  for .
\end{enumerate}
Note that only Step \Itm{i1619} involves private messages (first between  and the third party, and then between the third party and ), and that is only in the prelude, before any of the actual data  has appeared. Steps \Itm{i1620} and \Itm{i1621} involve no messages at all; and the messages occurring in Step \Itm{i1622} are -encrypted already. In effect the prelude has created a one-time pad.

A formal derivation of this implementation is given elsewhere \cite{Morgan:07}.

\section{Alternative uncertainty measures}\label{s3448}

\subsection{Shannon Entropy} \label{s3628}
The \emph{Shannon Entropy} of a (full) distribution  is , that is the weighted average of the negated base-2 logarithms of its constituent probabilities~\cite{Shannon:48}. By extension, for any hyper-distribution  we define the \emph{conditional} Shannon Entropy  to be , the expected value of the entropies of its support~\cite{Cachin:97}.

Going further, if we split up our hyper-distribution by  into its partitions, we have an equivalent presentation of entropy as the sum of individual partition-entropies , provided we define the entropy of a single partition, and of a single fraction, as follows:

where we write  for  to avoid a proliferation of minus signs, and  is normalisation of the fraction , scaling it up (if necessary) to give a distribution again.

The ordering  based on hyper-distributions

is then specified, as for the Bayes order , so that  if they are functionally equivalent and the uncertainty (the Shannon Entropy in this case) of  is no less than that of . It extends pointwise to secure programs. Furthermore we write that  when  but .

\subsubsection{Non-compositionality}
Consider again two functionally-equivalent programs from our three-box puzzle example from \Sec{s1121} and \Sec{s1201}:

with final hyper-distributions
\medskip\par\noindent
\hfill\hfill\makebox[0pt][r]{()}
\par\noindent
\hfill\hfill\makebox[0pt][r]{()}\medskip\par\noindent

The Shannon entropy of , calculated , is slightly more than , exceeding the entropy  that, by the simpler calculation given by , turns out to be exactly ; and so  .

However if we define context  to be 
 
then the entropy of  is the same as before at ; but the entropy of  is now only a half of what it was, at . Hence .

\subsubsection{Soundness}

We follow initially the structure of the soundness proof for Bayes Risk. Fix an initial split-state and construct the output hyper-distributions  that result from  respectively. Then since we assume  we must have . We now show that this implies .

Since  trivially guarantees that , we need to show that the Shannon Entropy condition in  is satisfied. Since we have that  is , it is enough to show that for each  the entropy of  is no less than the entropy of , provided that  for some partition  depending on .

For  we consider the unique  that is the reduction of both: it is formed in each case by adding together groups of similar fractions. From \Eqn{e2058} and arithmetic, we obtain immediately that .\,\footnote{If  then  and so the line marked  below becomes an equality.}

For  we know that the fractions of  are sums of groups of not-necessarily-similar fractions in . We consider the special case of just two fractions  in  summing to a single fraction  in , and look at their relative contributions to the sum \Eqn{e2058}; we have
\begin{Reason}
\Step{}{
 \Ht.\pi
}
\Step{}{
 \Ht.(\pi_1{+}\pi_2)
}
\Step{}{
 \Exp{d\In(\pi_1{+}\pi_2)}{\NLg(\Norm{\pi_1{+}\pi_2}.d)}
}
\Step{}{
 \Exp{d\In\pi_1}{\NLg(\Norm{\pi_1{+}\pi_2}.d)} + \Exp{d\In\pi_2}{\NLg(\Norm{\pi_1{+}\pi_2}.d)}
}
\StepR{\dagger}{see below}{
 \Exp{d\In\pi_1}{\NLg(\Norm{\pi_1}.d)} + \Exp{d\In\pi_2}{\NLg(\Norm{\pi_2}.d)}
}
\Step{}{
 \Ht.\pi_1 + \Ht.\pi_2 ~,
}
\end{Reason}
that is that the contribution to the conditional entropy of  on its own is at least as great as it was when was separated into .

For ``see below'' we refer to the \emph{Key Lemma} \cite[p5]{Welsh:88} which states that for two total distributions  of equal support, the weighted sum  attains its minimum over  when .

Extending the argument similarly to multiple additions gives  as required and thus we have  overall. We note that the inequality at  is strict when , because then e.g.\ .\,\footnote{This follows from a strengthening of the Key Lemma to ``\ldots\emph{only} when ,'' which is implied by the proof of Thm.~1 [\textit{op.\ cit.}]\ immediately before.}
We have established
\begin{Theorem}{Soundness of  w.r.t. }{t2112}
For all secure programs  and  and contexts , we have that  implies .
\end{Theorem}

Finally, when  but  so that  for some initial split-state we must have  for some final , since both those partitions are in reduced form: that is, reduced partitions cannot be similar without actually being equal. Thus also , and so we can find particular  to realise the strict inequality at . That gives us
\begin{Lemma}{Strict soundness}{l2121}
For all hyper-distributions  we have that  but  implies .
\end{Lemma}


\subsection{Marginal guesswork}

The \emph{Marginal guesswork}~\cite{Pliam:00} of a distribution  is the least number of guesses an attacker requires to be sure that her chance of guessing some  chosen according to  is at least a given probability . We define it

where we write , or more generally  for fraction  to mean the sum of the  greatest probabilities in , and  is the cardinality of . Note that by super-distribution of maximum over addition we have  for any  in the proper range. To avoid clutter, we will omit the range  for  from here on.

For a hyper-distribution  we define

which is the least value  such that if an attacker is allowed to make that many guesses then she can discover the value of  with probability at least . 


Observe that our definition of  is \emph{not} the same as the \emph{conditional marginal guesswork}  as conventionally defined~\cite{Kopf:07}. We argue that conditional marginal guesswork \emph{is not} a reasonable measure of the number of guesses required by an attacker to ensure that the probability of guessing  in  is greater than . Consider for example the hyper-distributions

\footnotetext{Alternatively we could write out  with its explicit probabilities as , but we prefer to avoid the superscripts.}Note that  since the latter is obtained by merging the two split-states of the former.

Now an attacker has more information about how  was chosen in  than in : for  she knows not only that  is distributed according to the distribution  overall (as for ), but as well she knows when  was chosen from  and when  was chosen from . However, when we set  the conditional marginal guesswork of  is , that is  --- which is higher than for , which gives only . This suggests that it is \emph{harder} for an attacker to guess  in  than in , in spite of the fact that the attacker knows more about the final -distribution in  when launching an attack.


Using our  we have , that is 1 in both cases: with just one guess at her disposal an attacker is guaranteed to guess  at least half the time. Applying her one guess to , half the time she can guess 0 and is sure to be right; in  she guesses 0 and will be right half the time.

The ordering between hyper-distributions based on marginal guesswork is 

which extends pointwise to programs.

\subsubsection{Non-compositionality}

When  is not zero, marginal guesswork --like the other measures-- is non-compositional for our subset of programs. For such an  take, for example, functionally equivalent programs

such that if  then  else . These programs have the final output distributions
\medskip\par\noindent
\hfill\hfill\makebox[10pt][r]{()}
\par\noindent
and 
\hfill\hfill\makebox[10pt][r]{()}\medskip\par\noindent
We can calculate that both  and  are , and so , but that for context  defined as  we have  is only , while  remains at  --- and so . 

\subsubsection{Soundness}

\begin{Lemma}{ implies }{l9734}
For all hyper-distributions  and  and probabilities , if  then also ; consequently  implies.
\begin{Proof}
From \Eqn{e9874} and the definition of refinement (\Def{d1424}, \Sec{s1213}) it is enough to show that for any partition  and  in range that (i) if the fractions in  are similar then  else (ii) . To show (ii) we have by generalising  that indeed

and for (i) we can replace inequality by equality since  distributes over summation in that case. 
\end{Proof}
\end{Lemma}


\begin{Theorem}{Soundness of  w.r.t. }{l9543}
For all probabilities , secure programs ,  and contexts  we have that  implies .
\begin{Proof}
\Lem{l9734} and monotonicity of  (\Thm{t1120} from \Sec{s9345}).
\end{Proof}
\end{Theorem}

\subsection{Guessing entropy}

The \emph{guessing entropy}~\cite{Massey:94} of a distribution  is the (least) average number of guesses required to guess  in . It is equivalent to the average -marginal guesswork over all values of  \cite{Pliam:00}, and we define it 

where  is the sum of the  smallest probabilities in .\,\footnote{If \Wlog\ the four probabilities  are ordered greatest to least, then the best strategy is to guess (the value associated with)  first, and then to go on to guess  in order as necessary. The average number of guesses needed overall is then , that is .}
Note that by subdistribution of minimum over addition we have  for any  in range.
For hyper-distribution  we define the conditional guessing entropy, thus

where  is defined in the same way as . We define the ordering by

which extends pointwise to secure programs.

\subsubsection{Non-compositionality}

To show non-compositionality of ordering  we refer again (as we did for Shannon entropy in \App{s3628}) to the functionally equivalent programs  and . First we calculate that 
.5ex]
\textrm{and} \quad  
& \Gt.\Delta'_{I_2} &=&
\frac{1}{3}(1) + \frac{2}{3}(\frac{1}{2} + (\frac{1}{2}{+}\frac{1}{2})) &=& \frac{4}{3} ~, 
\end{array}

so that we have .
Again taking context  to be  we get that the guessing entropy of  is reduced to 
 while that of  is still , and hence .


\subsubsection{Soundness}

\begin{Lemma}{ implies }{l5433}
For all hyper-distributions  and , we have that  implies ; and consequently  implies .
\begin{Proof}
As in the proof of soundness for marginal guesswork, it is enough to show
that for any partition  (i) if the fractions in  are similar then  else (ii) . For (ii) we reason:
\begin{Reason}
\Step{}{
 \Sum{\pi\In\Pi}{}{\Gt{}.\pi}
}
\StepR{}{definition  for a partition}{
 \Sum{\pi\In\Pi}{}{\Sum{i}{}{\UMin{i}\pi}}
}
\StepR{}{swap summations}{
 \Sum{i}{}{\Sum{\pi\In\Pi}{}{\UMin{i}\pi}}
}
\StepR{}{subdistribute minimisation}{
 \Sum{i}{}{\UMin{i}\Sum{\pi\In\Pi}{}{\pi}}
}
\StepR{}{definition  for partition}{
 \Gt{}.\Sum{\pi\In\Pi}{}{}~.
}
\end{Reason}
When all the fractions  in  are similar, we can replace the inequality in the second-last step with equality, establishing (i).
\end{Proof}
\end{Lemma}


\begin{Theorem}{Soundness of  w.r.t. }{l6855}
For all programs  and  and contexts  we have that  implies .
\begin{Proof}
Immediate from \Lem{l5433} and monotonicity of  (\Thm{t1120} in \Sec{s9345}).
\end{Proof}
\end{Theorem}

\end{document}
