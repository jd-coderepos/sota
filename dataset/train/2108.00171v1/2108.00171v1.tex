\documentclass[10pt,twocolumn,letterpaper]{article}



\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{3713} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Learning Instance-level Spatial-Temporal Patterns for Person Re-identification}

\author{Min Ren\textsuperscript{1 2}\thanks{This work is done when Min Ren is an intern at JD AI Research.}, Lingxiao He\textsuperscript{3}, Xingyu Liao\textsuperscript{3},Wu Liu\textsuperscript{3}, Yunlong Wang\textsuperscript{2}, Tieniu Tan\textsuperscript{2}\\
\textsuperscript{1}University of Chinese Academy of Sciences\\
\textsuperscript{2}CRIPAC NLPR, Institute of Automation Chinese Academy of Sciences\\
\textsuperscript{3}JD AI Research\\
\{min.ren, yunlong.wang\}@cripac.ia.ac.cn, \{helingxiao3, liaoxingyu5, liuwu1\}@jd.com, tnt@nlpr.ia.ac.cn
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Person re-identification (Re-ID) aims to match pedestrians under dis-joint cameras.
Most Re-ID methods formulate it as visual representation learning and image search, and its accuracy is consequently affected greatly by the search space.
Spatial-temporal information has been proven to be efficient to filter irrelevant negative samples and significantly improve Re-ID accuracy.
However, existing spatial-temporal person Re-ID methods are still rough and do not exploit spatial-temporal information sufficiently.
In this paper, we propose a novel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD), to improve Re-ID accuracy.
In our proposed framework, personalized information such as moving direction is explicitly considered to further narrow down the search space.
Besides, the spatial-temporal transferring probability is disentangled from joint distribution to marginal distribution, so that outliers can also be well modeled.
Abundant experimental analyses are presented, which demonstrates the superiority and provides more insights into our method.
The proposed method achieves mAP of~ 90.8\% on Market-1501 and 89.1\% on DukeMTMC-reID, improving from the baseline 82.2\% and 72.7\%, respectively. 
Besides, in order to provide a better benchmark for person re-identification, we release a cleaned data list of DukeMTMC-reID with this paper: \url{https://github.com/RenMin1991/cleaned-DukeMTMC-reID/}
\end{abstract}

\section{Introduction}

Person re-identification aims to retrieve pedestrians across non-overlapping camera views. Most existing person re-identification methods focus on the visual feature representations of pedestrian images~\cite{2015An, 2018Learning, he2020fastreid, 2015Learning, 2016Person, Lin2017Deep, 2017Beyond, Chen2020Salience, 2016Deep}, such as appearance, clothes, and textures.
The auxiliary information of person images is also adopted recently, such as parsing information~\cite{ Song2018Mask,He2018CVPR, 2020Foreground, 2018MaskReID, Kalayeh2018Human}, pose of the pedestrians~\cite{Su2017Learning, 2019Pose, 2018Pose}, or human body key points~\cite{Wang2020High}. 
However, the performances of these methods are still far from the requirements of real-world situations.
Because it is hard for visual representations to discriminate pedestrian with similar appearance and clothes.


\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/pipeline.png}
\end{center}
   \caption{For each pair of pedestrian images, instance-level spatial and temporal constraints are provided separately by the proposed framework. Then they are adaptively combined with the visual feature similarity for matching.}
\label{fig:pipeline}
\vspace{-0.3cm}
\end{figure}





Recent methods model spatial-temporal patterns ~\cite{guangcong2019aaai, 2018Unsupervised, 2017Joint, 2016Camera} to filter out the irrelevant candidates and narrow down the search space.
Specifically, these methods mainly formulate spatial-temporal pattern as a joint distribution , where  means moving from \textit{camera i} to \textit{camera j},  means time interval.
It has been proven to be efficient to significantly improve re-identification accuracy.
However, there are two problems of the existing methods.
Firstly, the existing spatial-temporal methods only consider camera-level but neglect instance-level information.
The state information of each pedestrian is neglected while it is essential for spatial-temporal patterns of the person.
Secondly, existing methods formulate spatial-temporal patterns as a joint distribution, meaning that only those candidates matching both spatial and temporal priors can be matched. 
They are not robust to the outliers.



\iffalse
Some methods pay attention to the spatial-temporal constraints, which are implicit in the topology of cameras, to enhance the person re-identification performance~\cite{2016Camera, 2017Joint, 2018Unsupervised, guangcong2019aaai}. 
Compared with the structure information, the spatial-temporal information is much more economical but useful.
In these methods, hard or soft constraints are utilized to filter out the irrelevant gallery images so as to simplify the retrieval of pedestrians.
However, the existing spatial-temporal methods model the patterns according to the camera-level information, which may not provide reasonable constraints for each instance. 
The state information of each pedestrian are neglected while it is essential for spatial-temporal patterns of the person.
Besides, the spatial and temporal patterns are coupled together in most of the current methods~\cite{2017Joint, 2018Unsupervised, guangcong2019aaai}. This coupling is somehow harmful to person re-identification. For example, a runner, who is moving faster than most pedestrians, is an outlier from the view of temporal patterns. But the person can be quite normal from terms of spatial transmission perspective. The existing methods are likely to filter out this runner because the spatial and temporal patterns are coupled together.
It can be worse in complex scenarios.
\fi


\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.87\linewidth]{figures/idea.png}
\end{center}
\vspace{-0.3cm}
   \caption{Spatial-temporal patterns are implied in the topology of cameras. The spatial-temporal pattern between two cameras of a pedestrian are highly correlated with his/her moving direction. Pedestrians in the view of camera 2 may appear in camera 1, camera 3, or camera 5  after a certain time lapse. However, pedestrians with different states will appear in different cameras at different times. For example, the pedestrian in the red bounding box is much more likely to appear in Camera 3 than Camera 1, because he is moving towards the field of view of Camera 3. In the proposed method, the instance-level state information is adopted rather than modeling the spatial-temporal patterns on camera-level as the existing methods.}
\label{fig:intro1}
\vspace{-0.3cm}
\end{figure*}


To solve these problems, we propose a novel method named Instance-level and Spatial-Temporal Disentangled Re-ID (InSTD) to model the instance-level and spatial-temporal disentangled patterns.
Firstly, the traditional spatial-temporal pattern is updated to be conditional on instance-level state information.
Its formulation looks like , where  is instance-level pedestrian information.
The walking direction of the pedestrian, which is the key instance-level state information, is taken into consideration in this paper.
The walking direction of a pedestrian is complimentary information of pedestrian detection and tracking.
It is useful because it is highly correlated with spatial-temporal patterns.
For example, a pedestrian, who is walking towards the west in the view of a camera, is more probable to appear in the view of the western cameras later, rather than the eastern cameras.
Meanwhile, it is economical because pedestrian detection and tracking are necessary steps before person re-identification in practice.












Secondly, we disentangle the spatial-temporal pattern by constructing their marginal distribution, \ie transmission probability  and time interval distribution .
They are modeled separately and adaptively combined to handle outliers.
If the temporal (spatial) pattern of a pedestrian is unusual, the person may be normal in the term of spatial (temporal) patterns. The similarity metric should focus on the spatial (temporal) pattern.
For example, a runner, who is moving faster than most pedestrians, is an outlier from the view of temporal pattern. But the runner can be quite normal in terms of spatial transmission perspective.
It is harmful to model this runner by joint distribution of spatial and temporal patterns.
To this end, we propose a novel fusion approach to adaptively combine the spatial and temporal patterns.
The spatial patterns and temporal patterns are complementary, rather than in conflict as existing methods, so that outliers can also be well modeled.






The contributions of this paper can be summarized as follows:

\begin{itemize}

\item We present a novel instance-level method to model spatial-temporal patterns for person re-identification. The proposed method provides personalized predictions by leveraging the instance-level state information of each pedestrian.

\item The instance-level spatial-temporal patterns are decoupled into transmission probabilities and time interval distributions between cameras in the proposed method. The spatial and temporal patterns become complementary rather than in conflict as existing methods.

\item Without bells and whistles, the proposed method surpasses the baseline model based on visual features by 16.9\% on DukeMTMC-reID and 8.6\% on Market-1501 in the term of mAP, and outperforms the state-of-the-art method based on spatial-temporal patterns by 4.8\% on DukeMTMC-reID and 2.2\% on Market-1501.

\end{itemize}






\section{Related Work}
\label{sec:related_work}








\subsection{Visual Features based Re-ID}


Person re-identification addresses the problem of matching pedestrian images across non-overlapping camera views~\cite{Gong2014Person}. Many studies exploit discriminative visual features~\cite{Apurva2011Multiple, Ma2018Covariance, Yang2014Salient}. 


Deep learning algorithms foster significant improvements in the field of person re-identification.
Some researchers attempt to explore effective convolutional neural networks~\cite{Hermans2017In, 2015An, Wang2017P2SNet, Wang2016DARI, Shen2018PersonRW, Ding2015Deep, Lin2017Deep, Chen2020Salience, Zhang2020Relation, Zheng2018Person}. 
Some studies explore training strategies and loss functions for person re-identification~\cite{Hermans2017In, Wang2017P2SNet, Wang2016DARI, Ding2015Deep}. 
Recently, some studies leverage the structure information of person images, such as parsing information~\cite{Song2018Mask, 2020Foreground, 2018MaskReID, Kalayeh2018Human}, pose of the pedestrians~\cite{Su2017Learning, 2019Pose, 2018Pose}, or human body key points~\cite{Wang2020High}.



However, appearance-based methods are still far from practical applications.
They are not discriminative enough in complex scenarios where pedestrians may exhibit similar appearance and clothes.
It is hard to further improve the performance using only appearance-based features.







\subsection{Spatial-temporal Person Re-ID}

 
There are some researchers who have paid attention to the topology of cameras since the spatial-temporal patterns implied in the topology are essential for cross-camera retrieval. 
The spatial-temporal constraints are utilized to filter out the irrelevant gallery images~\cite{guangcong2019aaai, 2018Unsupervised, 2017Joint, 2016Camera}. 
Huang \etal~\cite{2016Camera} propose a method to take both visual feature representation and spatial-temporal constraints into consideration for person re-identification. However, this method makes a strong assumption that the time intervals between cameras follow Weibull distribution. This assumption is invalid for complex scenarios.  
Cho \etal ~\cite{2017Joint} propose a framework to integrate camera network topology into person re-identification. However, the temporal constraints are simply realized by time thresholds, which cannot handle massive gallery images and complex cases in practice.
Lv \etal~\cite{2018Unsupervised} propose a method that leverage the spatial-temporal constraints for cross-dataset person re-identification. The spatial-temporal constraints improve the performance on the target dataset by enhancing the pseudo label during training. 
It is not proper to be directly applied to general person re-identification tasks.
Wang \etal~\cite{guangcong2019aaai} propose a two-stream architecture to apply spatial-temporal constraints to person re-identification. However, the spatial-temporal constraints are coupled together in this method, which is harmful to recalling positive samples. 
All these spatial-temporal person Re-ID methods establish the patterns based on the camera-level information, which means they can not provide fine-grained constraints for each instance.

Different from existing spatial-temporal methods, our method models spatial-temporal patterns at the instance-level to filter out more irrelevant gallery images and provide personalized predictions. And the spatial-temporal patterns are decoupled into transmission probabilities and time interval distributions to make them mutually beneficial rather than in conflict. 





\section{Method}

The instance-level spatial constraint \ie transmission probabilities, and temporal constraint \ie time interval distributions are detailed separately in this section.
Then the adaptive combined metric is presented.



\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/cam1.png}
\end{center}
   \caption {View of the first camera of DukeMTMC-reID. The state set of this camera contains two states: walking towards the red zone and walking towards the blue zone.}
\label{fig:cam1}
\vspace{-0.3cm}
\end{figure}


\subsection{Instance-level Spatial Constraint}

The spatial constraint between two cameras is described by the transmission probability of the cameras, which means how tightly the two cameras correlate. Formally, we model the transmission probability by a conditional probability:

where  and  are the indexes of cameras,  is the camera that a person appears earlier,  is the camera that the same person appears later.
It is the probability that a person appears in the view of camera  later on the condition that this person has appeared in the view of camera .
The conditional probability in Eq.~\ref{equ:spatial1} can be easily calculated:


The higher the conditional probability means the person in the view of camera  is more likely to appear in the view of camera  later. The time interval between camera  and  is not involved here. Note that  in most cases.

However, the spatial patterns of persons appear in the same camera can be different, as shown in Fig.~\ref{fig:intro1} . To address this problem, we introduce the instance-level state information of a person into the conditional probability:

where  is the state of a person in the view of , ,  is the set of states: 

where  is the number of states of camera .

The instance-level states are represented by walking directions of pedestrians. 
For example, the view of the first camera of DukeMTMC-reID~\cite{Ergys2016Performance} is shown in Fig.~\ref{fig:cam1}. The state set of this camera contains two states: walking towards the red zone and walking towards the blue zone. The state sets of the rest cameras are defined similarly, and the illustrations of other cameras can be found in the supplementary material.

Hence, the instance-level transmission probability can be calculated: 






\subsection{Instance-level Temporal Constraint}

The temporal constraint is described by the time interval distribution, which represents the time lapse for a pedestrian to transfer between two cameras. Formally, we model the time interval distribution by a conditional probability density function:

where  is the transfer time,   is the cumulative distribution function, which is a conditional probability:



It can be harmful to recalling positive samples that fitting  or  into a closed-form probability distribution.
Hence, a non-parameter estimation method is adopted in our method. Specially, we use Parzen window with Gaussian kernel to estimate :



where  is the kernel function,  is a normalized factor,  is the time interval set between camera  and camera  of training samples.


However, the time interval distribution of persons appear in the same camera can be different, as we have mentioned before. Similar to the instance-level transmission probabilities, the instance-level state information of a person is introduced into the conditional probability to address this problem:


where  is the instance-level state of a person in the view of . The moving direction is also considered as the key state information.
Similar with Eq.~\ref{equ:parzen1} , instance-level time interval distribution  can be estimated:



where the normalized factor ,  is a subset of , it contains the samples subject to .




\subsection{Joint Metric}

Given the transmission probability and time interval distribution affiliated to instance-level state information, the spatial-temporal probability is the fusion of them:

where  is the instance-level transmission probability of two images in Eq.~\ref{equ:spatial3}, and  is the instance-level time interval probability in Eq.~\ref{equ:temporal3}. And the final joint metric of two images is:

where  is the visual feature similarity.

A straightforward way to fuse both components is multiplying  and  together. 
However, the constraint realized by directly multiplying is too strict for person re-identification.
For example, the spatial constraint of a gallery image given by transmission probability is 0.9, the temporal constraint given by time interval distribution is 0.01; the spatial constraint of another gallery image is 0.1, the temporal constraint is 0.1. 
The first gallery image should be ranked higher than the second one because their temporal constraints are similar actually, while the spatial constraints have a significant gap.
However, if we fuse the spatial constraint and temporal constraint by multiplying, the second image will be ranked higher instead.
If a spatial/temporal pattern of a pedestrian is unusual, the person may be normal in terms of temporal/spatial patterns. This kind of samples should not be removed recklessly.
Hence, the spatial-temporal probability  should be fairly high when only one of them is high. Fusion by multiplying directly is not proper here obviously.

The spatial-temporal probability in our method is defined as:

where  and  are scaling parameters of similarity fusion. The spatial and temporal constraints are adjusted by  and  separately.

The spatial-temporal factor is scaled into . The constraint is relaxed properly when the spatial-temporal probability is low. And the value of  stays stable when  or  is low.




\subsection{Implementation Details}

More details are presented in this subsection.
The moving direction of a pedestrian is complimentary information of pedestrian detection and tracking, which is a necessary step before person re-identification in practice.
There is no need to predict the moving direction by an extra model or manual annotations.
In our experiments, the moving directions of samples in the field of one camera are confirmed by tracking them in the original video of this camera.
Actually, the moving direction of a pedestrian can be confirmed within five consecutive frames in most cases, which can be easily derived from existing tracking methods.

A pretrained ResNet-50 is adopted as baseline for feature extraction.
We set the standard deviation of the Gaussian kernel for distribution estimation to 100.
As for the scaling parameters, ,  in Eq.~\ref{equ:fusion3} are set to 0.15 and 1 respectively.





\section{Experiments}

In this section, we evaluate our method on two large scale person re-identification benchmark datasets, \ie Market-1501~\cite{Zheng2015Scalable} and DukeMTMC-reID~\cite{Ergys2016Performance}. Then, more experimental analysis is presented.



\subsection{Datasets and Evaluation Protocol}

Market-1501 dataset~\cite{Zheng2015Scalable} is collected at a university campus. A total of six cameras are used, including 5 high-resolution cameras, and one low-resolution camera. It contains 32,668 annotated bounding boxes of 1,501 identifies, plus a distractor set of over 500K images. The pedestrians are detected by  Deformable Part Model (DPM). Among them, the training set consists of 12,936 images from 751 identities, the gallery set contains 19,732 images from other 750 identities and all the distractors. 3,368 hand-drawn bounding boxes from 750 identities are used as the query images. In this dataset, each image contains its camera index and time stamp.

DukeMTMC-reID~\cite{Ergys2016Performance} is a subset of DukeMTMC dataset for image-based person re-identification. There are eight cameras in total. 1,404 identities appear in more than one camera and 408 identities (distractor) appear in only one camera. 702 identities are used for training, and the other 702 identities plus distractors are used for testing. One image for each identity in each camera is picked as a query, and the other images are put into the gallery. Each image contains its camera index and time stamp.

We use two performance indexes as in most person re-identification literature.
The first is mean average precision (mAP). The average precision (AP) of a query is the area under the Precision-Recall curve, which means both precision and recall rate is taken into consideration. Hence, the mean average precision among all query images is a comprehensive performance index for person re-identification.
The second is the cumulative matching characteristic (CMC) \ie the top-k accuracy. Hence, the cumulative matching characteristic emphasizes precision rather than recall rate.




\subsection{Comparisons to the State-of-the-Art}


The proposed method is compared with fourteen existing state-of-the-art methods, which can be categorized into four groups.
The first group of methods extract visual features directly from the person images, including PCB~\cite{2017Beyond}, VPM~\cite{Perceive2020Perceive}, and BOT~\cite{Luo2019Bag}. These methods explore various aspects of visual feature extraction, including the structure of convolutional neural networks, training strategy, data augmentation, and loss function.
The second group of methods adopt human parsing information for person re-identification, including SPReID~\cite{Kalayeh2018Human}, MGCAM~\cite{Song2018Mask}, MaskReID~\cite{2018MaskReID} and FPR~\cite{2020Foreground}. These methods introduce semantic information to the person images for better image alignment and feature matching.
The third group of methods leverage the pose or key points of person images, including PDC~\cite{Su2017Learning}, Pose-transfer~\cite{2018Pose}, PSE~\cite{2017A}, PGFA~\cite{2019Pose} and HOReID~\cite{Wang2020High}. These methods attempt to overcome the various human pose by taking the pose information or key points of the human body into consideration.
The fourth group of methods utilize the spatial-temporal information to enhance the person re-identification, including TFusion-sup~\cite{2018Unsupervised} and st-ReID~\cite{guangcong2019aaai}. These methods use hard or soft constraints to narrow the number of gallery images.



\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{1mm}
{
\begin{tabular}{l | c | c | c | c}
\hline
\bf{Methods} & \bf{mAP} & \bf{Rank-1} & \bf{Rank-5} & \bf{Rank-10}  \\
\hline
PCB~\cite{2017Beyond} & 77.4\% & 92.3\% & 97.2\% & 98.2\% \\
VPM~\cite{Perceive2020Perceive} & 80.8\% & 93.0\% & 97.8\% & 98.8\% \\
BOT~\cite{Luo2019Bag} & 85.9\% & 94.5\% & - & - \\


\hline
SPReID~\cite{Kalayeh2018Human} & 81.3\% & 92.5\% & 97.15\% & 98.1\% \\ 
MGCAM~\cite{Song2018Mask} & 74.3\% & 83.8\% & - & - \\
MaskReID~\cite{2018MaskReID} &75.4\% & 90.4\% & - & - \\
FPR~\cite{2020Foreground} & 86.6\% & 95.4\% & - & - \\

\hline
PDC~\cite{Su2017Learning} & 63.4\% & 84.1\% & - & - \\
Pose-transfer~\cite{2018Pose} & 68.9\% & 87.7\% & - & - \\
PSE~\cite{2017A} & 69.0\% & 87.7\% & 94.5\% & 96.8\% \\
PGFA~\cite{2019Pose} & 76.8\% & 91.2\% & - & - \\
HOReID~\cite{Wang2020High} & 84.9\% & 94.2\% & - & - \\

\hline
Baseline & 82.2\% & 93.6\% & 98.4\% & 99.0\% \\
Baseline+st-ReID~\cite{guangcong2019aaai} & 88.6\% & 96.9\% & 99.2\% & 99.5\% \\



\textbf{Baseline+InSTD} & \bf{90.8\%} & \bf{97.6\%}  & \bf{99.5\%} & \bf{99.7\%} \\
\hline
\end{tabular}}
\end{center}
\caption{Comparison with the state-of-the-arts methods on Market-1501. Group 1: vanilla deep learning based methods. Group 2: human-parsing information based methods. Group 3: pose or key points based methods. Group 4: spatial-temporal methods.}
\label{tab:market}
\vspace{-0.3cm}
\end{table}

The experiment results on Market-1501 are shown in Tab.~\ref{tab:market}, and the results on DukeMTMC-reID are shown in Tab.~\ref{tab:duke}.
Our method outperforms all of the existing methods on both datasets. Comparing to the baseline model, which is a ResNet-50, our method improves the mAP by 8.6\% on Market-1501 and 16.5\% on DukeMTMC-reID, improve the Rank-1 accuracy by 4\% on Market-1501 and 10\% on DukeMTMC-reID. Our method achieves significant improvements especially in terms of mAP.

Comparing to the methods in the first three groups, the advantage of our method is obvious. Besides,  the compared methods in the second and third groups need expensive annotations, such as key points, pixel-wise parsing maps, and masks, to match the query and gallery images. Our method adopts economical information \ie camera ID, timestamp, and state information.

The disadvantages of the methods in the fourth group have been interpreted in Sec.~\ref{sec:related_work}. And the interpretations have been demonstrated by the results of experiments in this subsection.
Given the same baseline model, our method outperforms the st-ReID by a remarkable margin, especially in terms of mAP (2.2\% on Market-1501 and 4.8\% on DukeMTMC-reID). These results indicate that our method has evident advantages over existing spatial-temporal methods.



\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{1mm}
{
\begin{tabular}{l | c | c | c | c}
\hline
\bf{Methods} & \bf{mAP} & \bf{Rank-1} & \bf{Rank-5} & \bf{Rank-10}  \\
\hline


PCB~\cite{2017Beyond} & 66.1\% & 81.7\% & 89.7\% & 91.9\% \\
VPM~\cite{Perceive2020Perceive} & 72.6\% & 83.6\% & 91.7\% & 94.2\% \\
BOT~\cite{Luo2019Bag} & 76.4\% & 86.4\% & - & - \\


\hline
SPReID~\cite{Kalayeh2018Human} & 70.9\% & 84.4\% & 91.8\% & 93.7\% \\ 
MGCAM~\cite{Song2018Mask} & 46.0\% & 46.7\% & - & - \\
MaskReID~\cite{2018MaskReID} &61.89\% & 78.86\% & - & - \\
FPR~\cite{2020Foreground} & 78.4\% & 88.6\% & - & - \\

\hline
Pose-transfer~\cite{2018Pose} & 56.9\% & 78.5\% & - & - \\
PSE~\cite{2017A} & 62.0\% & 79.8\% & 89.7\% & 92.2\% \\
PGFA~\cite{2019Pose} & 65.5\% & 82.6\% & - & - \\
HOReID~\cite{Wang2020High} & 75.6\% & 86.9\% & - & - \\

\hline
Baseline & 72.7\% & 85.7\% & 90.9\% & 93.5\% \\
Baseline+st-ReID~\cite{guangcong2019aaai} & 84.3\% & 94.1\% & 96.3\% &97.2\% \\



\textbf{Baseline+InSTD} & \bf{89.1\%} & \bf{95.7\%}  & \bf{97.2\%} & \bf{98.0\%} \\
\hline
\end{tabular}}
\end{center}
\caption{Comparison with state-of-the-arts for person re-identification on DukeMTMC-reID~\cite{Ergys2016Performance}. Group 1: vanilla deep learning based methods. Group 2: human-parsing information based methods. Group 3: pose or key points based methods. Group 4: spatial-temporal methods.}
\label{tab:duke}
\vspace{-0.3cm}
\end{table}

To show the effect of spatial-temporal constraint, an example from DukeMTMC-reID is presented in Fig.~\ref{fig:suc1}.
The appearance of the pedestrian in the red bounding box, who is mistakenly ranked first, is similar to the query image. The visual representation cannot distinguish it from the correct identifies as shown in Fig.~\ref{fig:suc1} (a).
The incorrect pedestrian, which is difficult to discriminate for the visual representation, is filtered out by the spatial-temporal constraint as shown in Fig.~\ref{fig:suc1} (b).


\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/suc1.png}
\end{center}
\vspace{-0.3cm}
   \caption {\textbf{(a)}: The appearance of the pedestrian in the red bounding box, who is mistakenly ranked first, is similar to the query image. The visual representation cannot distinguish it from the correct identifies. \textbf{(b)}: The incorrect pedestrian is filtered out by the spatial-temporal constraint.}
\label{fig:suc1}
\end{figure}

The effect of instance-level information are shown in Fig.~\ref{fig:suc2}.
The spatial-temporal constrains may be misguided in complex scenarios, as shown in Fig.~\ref{fig:suc2} (a).
The instance-level information can make the spatial-temporal constrains more reliable for person re-identification.
The incorrect pedestrians are filtered out by the instance-level state as shown in Fig.~\ref{fig:suc2} (b).

\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/suc2.png}
\end{center}
\vspace{-0.3cm}
   \caption {\textbf{(a)}: The top three of the ranked list are wrong samples because the spatial-temporal constraints are misguided without instance-level information. \textbf{(b)}: The spatial-temporal constraints are more reliable because of the instance-level state.}
\label{fig:suc2}
\vspace{-0.3cm}
\end{figure}





\subsection{Experiments on Different Feature Extractors}
\vspace{-0.15cm}
The proposed method can be applied to different feature extractors.
To verify its effectiveness, we evaluate the proposed method based on other two feature extractors: PCB~\cite{2017Beyond}, and VPM~\cite{Perceive2020Perceive}.

The results are shown in Tab.~\ref{tab:diff_feat}.
Our method consistently improves the performance of all feature extractors.
Our method gains significant 20\%/11\% improvement in mAP/rank-1 accuracy for PCB~\cite{2017Beyond}, and 16\%+/10\%+ improvement for the other two feature extractors.
Comparing to st-ReID~\cite{guangcong2019aaai}, which is also based on spatial-temporal constraints, our method achieves consistent improvements too.
Our method outperforms st-ReID~\cite{guangcong2019aaai} by 4\%+/0.6\%+ improvement in mAP/rank-1 accuracy for all of the feature extractors.

The results show that our method can be generalized to different feature extractors.
Moreover, the results demonstrate the advantages of our method comparing to the existing spatial-temporal based method.



\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{0.5mm}
{
\begin{tabular}{l | c | c | c | c}
\hline
\bf{Methods} & \bf{mAP} & \bf{Rank-1} & \bf{Rank-5} & \bf{Rank-10}  \\
\hline


PCB~\cite{2017Beyond} & 66.1\% & 81.7\% & 89.7\% & 91.9\% \\
PCB~\cite{2017Beyond}+st-ReID~\cite{guangcong2019aaai} & 80.9\% & 92.1\% & 95.4\% & 96.6\% \\
\textbf{PCB~\cite{2017Beyond}+InSTD} & 86.1\% & 92.7\% & 96.5\% & 97.6\% \\

\hline

VPM~\cite{Perceive2020Perceive} & 72.6\% & 83.6\% & 91.7\% & 94.2\% \\
VPM~\cite{Perceive2020Perceive}+st-ReID~\cite{guangcong2019aaai} & 84.9\% & 94.2\% & 96.1\% & 96.9\% \\
\textbf{VPM~\cite{Perceive2020Perceive}+InSTD} & 89.3\% & 95.1\% & 97.0\% & 97.9\% \\





\hline
\end{tabular}}
\end{center}
\caption{Effects on different feature extractors. The experiments are conducted on DukeMTMC-reID~\cite{Ergys2016Performance}}
\label{tab:diff_feat}
\end{table}








\subsection{Analysis of Scaling Parameters}
\vspace{-0.15cm}
To investigate the impact of two scaling parameters,  and  in Eq.~\ref{equ:fusion3}, we conduct two sensitivity analysis experiments on  and .
The results are shown in Fig.~\ref{fig:param}.
When analyzing one of them, the other one is fixed as its optimal value: , .
As we can observe, our method nearly keeps the best performance when  is in the range of 0.1 to 0.3 or  is in the range of 1 to 1.7. The results show that our method is insensitive to fusion parameters.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/param.png}
\end{center}
\vspace{-0.4cm}
   \caption {Result of sensitivity analysis experiments on  and  in Eq.~\ref{equ:fusion3}. When analyzing one of them, the other one is fixed as its optimal value. The experimental results show that our method is is insensitive to fusion parameters.}
\label{fig:param}
\vspace{-0.3cm}
\end{figure}






\subsection{Ablation Study}
\vspace{-0.15cm}
The ablation study on the instance-level state information of pedestrians and the decoupling of spatial-temporal patterns is presented in this part.

Four protocols are taken into consideration.
The first one is the proposed method itself.
In the second protocol,  and  in Eq.~\ref{equ:fusion1} are replaced by  (Eq.~\ref{equ:spatial2}) and  (Eq.~\ref{equ:parzen1}). The instance-level state information is excluded in this protocol.

In the third protocol, the spatial pattern and temporal pattern are coupled together. The normalized factor in Eq.~\ref{equ:parzen3} is replaced by :


which means all time interval distributions share an identical denominator. 
The numerical relations of the area under curves indicate the transmission probabilities between cameras. 
And  and  are replaced by  Eq.~\ref{equ:fusion3}:




In the fourth protocol, the instance-level state information is excluded based on the third protocol:



The results of these four protocols are shown in Tab.~\ref{tab:abl}. The results show that the instance-level state information of pedestrians and the decoupling of spatial and temporal are both useful to improve the performance of person re-identification.

\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{1mm}
{
\begin{tabular}{c | c | c | c | c}
\hline
\bf{Protocol} & \bf{Instance Info.} & \bf{ST Decouple} & \bf{mAP} & \bf{Rank-1}  \\
\hline

1 &\checkmark & \checkmark & \bf{89.1\%} & \bf{95.7\%} \\
2 &  & \checkmark & 87.1\% & 94.3\% \\
3 & \checkmark &  & 86.9\% & 95.0\% \\
4 &  &  & 83.4\% & 93.8\% \\

\hline
\end{tabular}}
\end{center}
\caption{Ablation Study results on DukeMTMC-reID~\cite{Ergys2016Performance}.}
\label{tab:abl}
\vspace{-0.3cm}
\end{table}

Comparing the second and the fourth protocol, the mAP and Rank-1 accuracy are improved by 3.7\% and 0.5\%.
Comparing the third and the fourth protocol, the mAP and Rank-1 accuracy are improved by 3.5\% and 1.2\%.
The instance-level state information of pedestrians is more helpful to Rank-1 accuracy, and the decoupling of spatial and temporal patterns is more contributive to mAP.
These results indicate that the decoupling of spatial and temporal patterns is more helpful to improve mAP by recalling more hard positive samples. 
The instance-level state information of pedestrians is more helpful to improve precision by narrow the number of gallery images.
The combination of these two strategies achieves the best performance.


To demonstrate the effect of introducing instance-level state information, the time interval distributions between \textit{camera 1} and \textit{camera 2} of DukeMTMC-reID are shown in Fig.~\ref{fig:time_dis2}. The instance-level states are not taken into consideration in Fig.~\ref{fig:time_dis2} (a). On the other hand, distributions of two states are shown separately in Fig.~\ref{fig:time_dis2} (b). 
The distribution in Fig.~\ref{fig:time_dis2} (a) is split into two distributions, which means more irrelevant gallery images can be filtered out according to the instance-level state information.



\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/time_dis2.png}
\end{center}
\vspace{-0.3cm}
   \caption {The time interval distributions between \textit{camera 1} and \textit{camera 2} of DukeMTMC-reID. \textbf{(a)}: Time interval distribution without state information. \textbf{(b)}: Time interval distributions with instance-level state information. The distribution in \textbf{(a)} is split into two distributions in \textbf{(b)}, which means more irrelevant gallery images can be filtered out according to the instance-level state information.}
\label{fig:time_dis2}
\vspace{-0.3cm}
\end{figure}

To show the difference between spatial-temporal coupled constraint and spatial-temporal decoupled constraint, time interval distributions of \textit{camera 1} to \textit{camera 2} and \textit{camera 1} to \textit{camera 5} are shown in Fig.~\ref{fig:time_dis1}. In the coupled case, the spatial pattern is conveyed by the areas under distribution curves as shown in Fig.~\ref{fig:time_dis1} (b). On the contrary, in the decoupled case, the areas under distribution curves are the same  as shown in Fig.~\ref{fig:time_dis1} (c), and the spatial pattern is decoupled from the time interval distributions as transmission probabilities.


\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/time_dis1.png}
\end{center}
\vspace{-0.3cm}
   \caption {The time interval distributions of \textit{camera 1} to \textit{camera 2} and \textit{camera 1} to \textit{camera 5} of DukeMTMC-reID. \textbf{(a)}: Time interval frequencies. \textbf{(b)}: Time interval distributions without spatial-temporal decouple. The spatial pattern is conveyed by the areas under distribution curves. \textbf{(c)}: Time interval distributions with spatial-temporal decouple. The areas under distribution curves are the same. The spatial pattern is decoupled from the time interval distributions as transmission probabilities. (Instance-level state information is not shown for simplicity.)}
\label{fig:time_dis1}
\vspace{-0.4cm}
\end{figure}



\subsection{Failure Analysis}
\vspace{-0.15cm}
In this part, we analyze the failure cases of the proposed method on DukeMTMC-reID.
We find that the failures can be categorized as four cases:

Firstly, there are incorrect labels in DukeMTMC-reID. 
The proportion of failure cases caused by incorrect labels is 16.2\%.
It is harmful to keep the incorrect labels in the database. \textbf{Hence, we release a cleaned data list of DukeMTMC-reID with this paper: \url{https://github.com/RenMin1991/cleaned-DukeMTMC-reID/}}.







Secondly, the feature extractor is fooled because of serious occlusions. For example, the upper part of two individuals is quite similar while the lower part is occluded.
The proportion of this case in all failures is 56.9\%.

In the third case, the visual feature is not discriminative enough to distinguish the hard negative samples.
The proportion of this case in all failures is 23.5\%.




In the last case, the proposed method outputs high probabilities due to spatial-temporal patterns. However, it improperly pushes up the final joint metric.
The proportion of this case in all failures is 3.4\%.

The failure analysis shows that serious occlusion is the main cause of mismatching (56.9\%).
The second important reason is that the feature extracted by the recognition model is not discriminative enough for some similar images (23.5\%).
Incorrect labels also degrade the performance (16.2\%).
The proportion of failure samples caused by the improper spatial-temporal probability in all failures is quite small (3.4\%).





\section{Conclusion}
\vspace{-0.1cm}
In this paper, we propose a method to exploit spatial-temporal patterns for person re-identification.
Different from the existing spatial-temporal person re-identification methods, the proposed method adopts the walking direction of each pedestrian, as key instance-level state information, to provide personalized predictions.
In addition, the spatial-temporal patterns are decoupled into transmission probabilities and time interval distributions between cameras. The spatial-temporal patterns become mutually beneficial rather than in conflict with each other as current methods.
A novel joint metric is proposed to fuse the instance-level spatial constraint, temporal constraint, and visual feature similarity.
The superiority of our method is demonstrated by extensive contrast experiments.
And adequate experimental analyses provide more insights into our method.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper_for_review}
}

\end{document}
