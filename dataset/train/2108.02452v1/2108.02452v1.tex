\subsection{Implementation Details}
The training and testing images are resized to . The resulting D heatmaps and Re-ID feature maps have the resolution of . We use DLA-34 \cite{zhou2019objects} as our backbone which is pre-trained on the ImageNet classification dataset. The number of body joints  is set to be  in accordance with the COCO dataset. The dimension of Re-ID features is set to be  following FairMOT \cite{zhang2020simple}.

The motion capture space is set to be  for the three datasets. We divide the space into  bins. So each bin is approximately of size . Note that since we compute expectation of joint locations over D heatmaps, the actual error is much smaller than mm. Recall that we apply sparse D convolution to feature volume to estimate D heatmaps. To promote sparsity, we set features in the volume to zero if their original values is smaller than . For ARN, we pool features from the space of size  (about  voxels) around estimated pelvis joints. 


We train \emph{VoxelTrack} in three separate stages. We use Adam optimizer in all stages. In the first stage, we train the D model for estimating heatmaps and Re-ID feature maps for  epochs with a start learning rate of . The learning rate decreases to  after the  epoch. Next, we fix the D model and train the D joint estimation network for  epochs. The learning rate is set to be . Finally, we train ARN to estimate D poses of all instances for  epochs with learning rate set to be . Note that we can also jointly train the three models if we have access to a large number of paired (image, D pose) training data. There are several reasons why we choose separate training: (1) when we apply our model to a new environment, it may be impossible to obtain a large number of (image, D pose) pairs for training the model. In this case, we can train the D model on public datasets and train the D model by generating a large number of D pose and D heatmap pairs; (2) separate training allows us to use larger batch size which helps stabilize training. 

For training on the Campus and Shelf dataset \cite{belagiannis20143d}, we do not use the images or poses and only use the camera parameters to avoid over-fitting to these small datasets. We use \cite{cheng2020bottom} as our D backbone network and train on the COCO Keypoint dataset \cite{lin2014microsoft}. It is worth noting that COCO does not have identity annotations and we cannot directly train the Re-ID branch on it. We propose a weakly supervised learning approach to train the Re-ID part on the COCO dataset. We assign each D pose a unique identity and thus regard each object instance in the dataset as a separate class. We apply different transformations to the whole image including flipping, rotation, scaling and translation to help create different appearances of the same instance. 

For training the D part on the Campus and Shelf dataset, we generate many synthetic heatmaps using the camera parameters. we place a number of 3D poses (sampled from the motion capture datasets such as Panoptic \cite{Joo_2017_TPAMI}) at random locations in the space and project them to all views to get the respective 2D locations. Then we generate 2D heatmaps from the locations
to train the D part. This has significant practical values as we can easily apply our model to new environments such as a retail store with the camera parameters available. 


\begin{table*}[]
    \setlength{\tabcolsep}{8pt}
    \centering
    \begin{tabular}{r|cccc|ccc}
        \toprule
         Campus &  Actor 1 & Actor 2 & Actor 3 & Average PCPD & MOTA & ID Switch & IDF1\\
         \hline
         Belagiannis \etal \cite{belagiannis20143d} & 82.0 & 72.4 & 73.7 & 75.8 & - & - & -\\
         Belagiannis \etal \cite{belagiannis2014multiple} & 83.0 & 73.0 & 78.0 & 78.0 & - & - & -\\
         Belagiannis \etal \cite{belagiannis20153d} & 93.5 & 75.7 & 84.4 & 84.5 & - & - & -\\
         Ershadi-Nasab \etal \cite{ershadi2018multiple} & 94.2 & 92.9 & 84.6 & 90.6 & - & - & -\\
         Dong \etal \cite{dong2019fast} & 97.6 & 93.3 & 98.0 & 96.3 & - & - & -\\
         Chen \etal \cite{chen2020cross} & 97.1 & \textbf{94.1} & \textbf{98.6} & 96.6 & - & - & -\\
         Ours & \textbf{98.1} & 93.7 & 98.3 & \textbf{96.7} & 89.3 & 0 & 94.6\\
         \hline
         \hline
          Shelf & Actor 1 & Actor 2 & Actor 3 & Average & MOTA & ID Switch & IDF1\\
          \hline
          Belagiannis \etal \cite{belagiannis20143d} & 66.1 & 65.0 & 83.2 & 71.4 & - & - & -\\
          Belagiannis \etal \cite{belagiannis2014multiple} & 75.0 & 67.0 & 86.0 & 76.0 & - & - & -\\
          Belagiannis \etal \cite{belagiannis20153d} & 75.3 & 69.7 & 87.6 & 77.5 & - & - & -\\
          Ershadi-Nasab \etal \cite{ershadi2018multiple} & 93.3 & 75.9 & 94.8 & 88.0 & - & - & -\\
         Dong \etal \cite{dong2019fast} & 98.8 & 94.1 & \textbf{97.8} & 96.9 & - & - & -\\
         Chen \etal \cite{chen2020cross} & \textbf{99.6} & 93.2 & 97.5 & 96.8 & - & - & -\\
         Ours & 98.6 & \textbf{94.9} & 97.7 & \textbf{97.1} & 94.4 & 0 & 97.2\\
         \bottomrule
    \end{tabular}
    \caption{Comparison to the state-of-the-art methods on the Campus and the Shelf datasets. The metric is PCP3D and AP. }
    \label{tab:campus_and_shelf}
\end{table*}

\subsection{Comparison to the State-of-the-art Methods}
\textbf{D Pose Estimation } Table \ref{tab:campus_and_shelf} shows the D pose estimation results of the state-of-the-art methods on the Campus and the Shelf datasets in the top and bottom sections, respectively. We can see that our approach improves PCP3D from  of \cite{chen2020cross} to  on the Campus dataset and  of \cite{dong2019fast} to  on the Shelf dataset, which is a decent improvement considering the already very high accuracy. As discussed in Section \ref{sec:metrics}, the PCP3D metric does not penalize false positive estimates. However, it is also meaningless to report AP scores because the GT pose annotations in this dataset are incomplete. So we propose to visualize and publish all of our estimated poses of the Shelf dataset\footnote{https://youtu.be/hwdk3sQEdY8} and the Campus datset\footnote{https://youtu.be/mXSQLDh953E}. We find that our approach usually gets accurate estimates as long as joints are visible in at least two views. The previous works \cite{belagiannis20143d,belagiannis20153d,belagiannis2014multiple,dong2019fast,chen2020cross} did not report numerical results on the large scale Panoptic dataset. We encourage future works to do so as in Table \ref{tab:panoptic_3d} and Table \ref{tab:panoptic_2d}. 


\begin{figure*}
	\centering
	\includegraphics[width=7in]{imgs/shelf_vis.pdf}
	\caption{Visualization results on the Shelf dataset. The top is the D images captured by 5 cameras and the bottom is the D pose tracking results. Different numbers and colors represent different person identities. 
	}
	\label{fig:shelf}
\end{figure*}


\vspace{1em}
\noindent
\textbf{D Pose Tracking } Table \ref{tab:campus_and_shelf} shows the D pose tracking results of our method on the Campus and the Shelf datasets. The MOTA metric is computed by FP, FN and ID Switch, which jointly consider the person detection, pose estimation and pose tracking performance. The ID Switch and IDF1 metrics can better reveal the tracking performance. We achieve 0 ID Switch and high IDF1 score (94.6 on Campus and 97.2 on Shelf) on both of the datasets with severe occlusion in each single view, which indicates that our multi-view D tracking method can achieve accurate tracking results. We achieve higher MOTA on the Shelf dataset than on the Campus dataset (94.4 vs 89.3) mainly because the pose estimation results on the Shelf dataset is better. 


\vspace{1em}
\noindent
\textbf{Qualitative Study } We show some D pose tracking results on the Shelf dataset in Figure \ref{fig:shelf}. We can see that there is severe occlusion in the images of all camera views. However, by fusing heatmaps from multiple cameras, our approach obtains more robust features which allows us to successfully estimate the 3D poses without bells and whistles. It is noteworthy that we do not need to associate 2D poses in different views based on noisy 2D poses by combining a number of sophisticated techniques. This significantly improves the robustness of the approach. We can see that people with identity 2, 3 and 4 are walking around the shelf with lots of occlusion. The identities of the four people keep the same across the frames, which indicates that our approach has stable tracking performance in severe occlusion cases. 

\subsection{Factors that Impact Estimation Accuracy}
We conduct ablation studies to evaluate a variety of factors of our approach. The results on the Panoptic dataset \cite{Joo_2017_TPAMI} are shown in Table \ref{tab:panoptic_3d} and Table \ref{tab:panoptic_2d}. We evaluate both the D pose estimation accuracy, the D tracking accuracy and the computation time. Table \ref{tab:panoptic_3d} shows the D factors and Table \ref{tab:panoptic_2d} shows the D factors.

\vspace{0.5em}
\noindent
\textbf{Voxel Size }
We evaluate three different voxel sizes: ,  and . The motion capture space is set to be . By comparing the first three lines in Table \ref{tab:panoptic_3d}, we can see that increasing the voxel size from  to  significantly improves the D pose estimation accuracy as the AP metric improves from 39.74 to 71.00 and the MPJPE metric decreases from 26.16mm to 19.83mm. When further increasing the size from  to , the improvement is not that large, which is also reasonable because the accuracy is more difficult to be increased as the grids become more fine-grained. By comparing the MOTA, IDF1 and ID Switch metrics, we can see that the voxel size does not influence the tracking accuracy much. The computation time of JEN increases as the voxel size increases. To strike a good balance between accuracy and speed, we use  for the rest of the experiments. 


\begin{table*}[]
    \setlength{\tabcolsep}{7pt}
    \centering
    \begin{tabular}{cc|cccc|ccc|cc}
        \toprule
         Voxel Size & JEN Type &  &  &  & MPJPE & MOTA & IDF1 & ID Switch & JEN Time & ARN Time\\
         \midrule
          & SP Conv & 79.34 & 96.83 & 99.58 & 18.49 mm & 98.45 & 98.67 & 0 & 48.46 ms &  ms\\
          & SP Conv & 71.00 & 97.04 & 99.45 & 19.83 mm & 98.27 & 98.52 & 0 & 30.93 ms &  ms\\
          & SP Conv & 39.74 & 94.27 & 99.10 & 26.16 mm & 97.62 & 95.13 & 0 & 22.01 ms &  ms\\
          & Conv & 74.09 & 96.87 & 99.55 & 19.05 mm & 98.32 & 98.39 & 0 & 132.64 ms &  ms\\
          & Conv & 68.89 & 97.06 & 99.51 & 20.28 mm & 98.16 & 98.21 & 0 & 57.30 ms &  ms\\
          & Conv & 38.66 & 94.40 & 99.17 & 25.93 mm & 98.27 & 98.56 & 0 & 18.75 ms &  ms\\
         \bottomrule
    \end{tabular}
    \caption{Ablation study of voxel size and sparse convolution on the Panoptic dataset. }
    \label{tab:panoptic_3d}
\end{table*}

\vspace{0.5em}
\noindent
\textbf{Sparse Convolution} We use the sparse convolution to replace the standard convolution as the D feature volume only has a small number of non-zero values and the sparse convolution only computes for the non-zero values. By comparing the sparse convolution to the standard convolution with the same voxel size in Table \ref{tab:panoptic_3d}, we can see that the computation time of JEN significantly decreases when using sparse convolution, especially under large voxel sizes such as  and . For the small voxel size such as , the sparse convolution is a little slower than the standard convolution. This is because the sparse convolution needs to find the index of the non-zero values and it takes considerable time. Thus, we do not apply sparse convolution to ARN because the size of the input D heatmap to ARN is much smaller. We can also see that the pose estimation accuracy of the sparse convolution is a little higher than the standard convolution under all voxel sizes. This is because we set the values of the feature volume to zero if their original values is smaller than 0.15 when applying the sparse convolution. The sparsity of the feature volume may reduce some ambiguity and thus increase the pose estimation accuracy. 


\begin{table*}[]
    \setlength{\tabcolsep}{8pt}
    \centering
    \begin{tabular}{ccc|cccc|ccc|c}
        \toprule
         Views & Backbone & Image Size &  &  &  & MPJPE & MOTA & IDF1 & ID Switch & D Time\\
         \midrule
         5 & DLA-34 &  & 79.34 & 96.83 & 99.58 & 18.49 mm & 98.45 & 98.67 & 0 & 85.71 ms\\
         4 & DLA-34 &  & 66.20 & 96.34 & 99.47 & 20.35 mm & 98.37 & 98.46 & 0 & 66.93 ms\\
         3 & DLA-34 &  & 49.09 & 92.44 & 97.62 & 24.93 mm & 95.77 & 93.08 & 0 & 54.99 ms\\
         5 & DLA-34 &  & 70.66 & 97.26 & 99.70 & 19.99 mm & 98.61 & 98.99 & 0 & 65.20 ms\\
         5 & DLA-34 &  & 55.96 & 96.78 & 99.65 & 21.67 mm & 98.37 & 98.45 & 0 & 45.37 ms\\
         5 & MobileNet-V2 &  & 42.42 & 94.09 & 99.33 & 24.38 mm & 97.61 & 97.82 & 0 & 27.50 ms\\
         5 & Higher-HRNet-W32 &  & 85.88 & 98.31 & 99.54 & 16.97 mm & 98.51 & 98.73 & 0 & 128.95 ms\\
         \bottomrule
    \end{tabular}
    \caption{Ablation study of number of views, 2D backbone and input image size on the Panoptic dataset. }
    \label{tab:panoptic_2d}
\end{table*}

\begin{table*}[]
    \setlength{\tabcolsep}{9pt}
    \centering
    \begin{tabular}{ccc|ccc|c}
        \toprule
         Re-ID Features & Occlusion Mask & 3D Poses & MOTA & IDF1 & ID Switch & Tracking Time\\
         \midrule
          &  &  & 98.42 & 93.82 & 90 & 0.92 ms\\
          &  &  & 98.44 & 94.38 & 15 & 0.96 ms\\
          &  &  & 98.45 & 98.67 & 0 & 2.10 ms\\
          &  &  & 98.45 & 98.67 & 0 & 2.16 ms\\
         \bottomrule
    \end{tabular}
    \caption{Ablation study of Re-ID features, occlusion mask and 3D poses on the Panoptic dataset. }
    \label{tab:panoptic_tracking}
\end{table*}

\vspace{0.5em}
\noindent
\textbf{Number of Cameras }
As shown in the first three lines of Table \ref{tab:panoptic_2d}, reducing the number of cameras generally increases the D pose estimation error because the information in the feature volume becomes less complete. The tracking accuracy is less affected by the number of cameras as the ID Switch is always 0. We can see that using 3 cameras can already accurately track the D poses. The computation time also decreases as the number of cameras decreases. 

\vspace{0.5em}
\noindent 
\textbf{Backbone Networks}
We evaluate three different backbone networks including DLA-34 \cite{zhou2019objects}, MobileNet-V2 \cite{sandler2018mobilenetv2} and Higher-HRNet-W32 \cite{cheng2020bottom}. For the MobileNet-V2, we add several de-convolution layers after the backbone network following \cite{xiao2018simple}. The results are shown in Table \ref{tab:panoptic_2d}. Higher-HRNet-W32 achieves the highest AP and the lowest MPJPE. MobileNet-V2 achieves the highest running speed. DLA-34 achieves a good balance between accuracy and speed. 

\vspace{0.5em}
\noindent
\textbf{Image Sizes}
We also evaluate three different image sizes including , , . As shown in Table \ref{tab:panoptic_2d}, we use DLA-34 as the backbone and evaluate different image sizes. Reducing the image sizes generally increases the D pose estimation error because large size images provide more detailed information. The tracking performance is hardly affected by image sizes. We can see that getting accurate D heatmaps is critical to the D accuracy. 


\begin{figure*}
	\centering
	\includegraphics[width=7in]{imgs/panoptic_vis.pdf}
	\caption{Visualization results on the ``160906\_pizza1'' sequence of the Panoptic dataset. The top is the D images captured by 5 cameras and the bottom is the D pose tracking results. Different numbers and colors represent different person identities. 
	}
	\label{fig:panoptic}
\end{figure*}


\subsection{Factors that Impact Tracking Accuracy }
There are three main components in our tracking procedure: 1) 3D poses, 2) Re-ID features, 3) occlusion-mask. We evaluate the impact of each of these components. The tracking results are shown in Table \ref{tab:panoptic_tracking}. The MOTA and IDF1 metrics are the average of all keypoints. The ID Switch metric is the switches of all the keypoints and it is a multiple of the number of joints (\ie 15). 

\vspace{0.5em}
\noindent
\textbf{D Poses }
We only use the normalized Euclidean distances of the D poses to link the detections to the tracklets. The result is shown in the first line of Table \ref{tab:panoptic_tracking}. We can only get 93.82 IDF1 score and 90 ID switches when only using the D poses. We find that the ID switches often occur when FP appears. In general, the D poses are reliable because there is almost no occlusion in the D space.   

\vspace{0.5em}
\noindent
\textbf{Re-ID Features }
We only use the cosine distance of Re-ID features to perform linking. For each D person, we fuse the Re-ID features in each view by just adding them without reasoning about the occlusion. The result is shown in the second line of Table \ref{tab:panoptic_tracking}. The IDF1 score (94.38 vs 93.82) and ID Switch (15 vs 90) are better than only using D poses. There still exists some ID Switches because some views with heavily occluded people provide some unreliable Re-ID features which cause some ambiguity.

\vspace{0.5em}
\noindent
\textbf{Occlusion Mask}
We use the occlusion mask mentioned in Section \ref{sec:occlusion_m} computed by each person's depth to fuse the Re-ID features of all the views. If the person in the view is heavily occluded, we do not use the Re-ID features of the person in this view. The result is shown in the third line of Table \ref{tab:panoptic_tracking}. We can see that using the occlusion mask to fuse Re-ID features achieves the highest IDF1 score 98.67 and does not have ID Switch which agrees with our expectation. When we further use the Re-ID features with occlusion mask and the D poses together, the tracking results keep the same, which indicates that our multi-view fused Re-ID features have powerful discriminative ability.  

\vspace{0.5em}
\noindent
\textbf{Qualitative Study }
We show the D pose tracking results of the Panoptic dataset in Figure \ref{fig:panoptic}. We can see that there are severe occlusions in the images of all camera views. The person with identity 6 has the most obvious movement. He comes to the table and then leaves. He is occluded in most of the camera views and the Re-ID features of the person are not reliable in most views. Thus, we need to use the occlusion mask to choose the Re-ID features in the specific views where the person is not occluded. Our approach can keep the identities of the six people the same and has stable tracking performance.  


\subsection{Whole System Running Time }
We divide our whole system into the D part, the D part and the tracking part and compute the running time of each of them. The running time of the D part is shown in Table \ref{tab:panoptic_3d} which is the sum of the ``JEN Time'' and the ``ARN Time''. The running time of the D part and the tracking part is shown in Table \ref{tab:panoptic_2d} and Table \ref{tab:panoptic_tracking}, respectively. From Table \ref{tab:panoptic_3d} we can see that the sparse convolution can reduce a large amount of running time of JEN. The voxel size also notably affects the running time. It is worth noting that the running time of ARN is hardly affected by the number of people (\ie 1 \emph{ms} for 1 person). From Table \ref{tab:panoptic_2d} we can see that the number of views, the backbone network, and the image size together determine the running time of the D part. From Table \ref{tab:panoptic_tracking} we can see that the running time of the tracking part can almost be ignored (\ie 2 \emph{ms}), which indicates the simplicity of our tracking algorithm. The light version of our system using MobileNet-V2 \cite{sandler2018mobilenetv2} as the D backbone and  JEN with the sparse convolution can run at 15 FPS with 5 camera views as input, which dramatically enhances the practical values of our approach. 






