



\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers,sort]{natbib}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks=true,linkcolor=red, citecolor=green]{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{amssymb}
\usepackage{nicefrac}       \usepackage{microtype}      \usepackage{adjustbox}
\usepackage{multirow}
\usepackage[flushleft]{threeparttable}
\usepackage{makecell}
\usepackage{tabulary}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{kotex}
\usepackage{bbold}
\usepackage{wrapfig}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{scalerel}
\usepackage{caption}
\usepackage{tabularx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{stfloats}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}

\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\xmark}{\text{\ding{55}}}
\newcommand{\cmark}{\text{\ding{51}}}





\def\cvprPaperID{4016} \def\confYear{CVPR 2021}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\begin{document}

\title{Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation}


\author{Jungbeom Lee ~~~~~~~ Eunji Kim ~~~~~~~  Sungroh Yoon\thanks{Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>.}\\
 Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea\\
 ASRI, INMC, ISRC, and Institute of Engineering Research, Seoul National University\\
{\tt\small \{jbeom.lee93, kce407, sryoon\}@snu.ac.kr}}



\maketitle


\begin{abstract}
\vspace{-0.3em}
Weakly supervised semantic segmentation produces a pixel-level localization from a classifier, but it is likely to restrict its focus to a small discriminative region of the target object. AdvCAM is an attribution map of an image that is manipulated to increase the classification score. This manipulation is realized in an anti-adversarial manner, which perturbs the images along pixel gradients in the opposite direction from those used in an adversarial attack. It forces regions initially considered not to be discriminative to become involved in subsequent classifications, and produces attribution maps that successively identify more regions of the target object. 
In addition, we introduce a new regularization procedure that inhibits the incorrect attribution of regions unrelated to the target object and limits the attributions of the regions that already have high scores.
On PASCAL VOC 2012 test images, we achieve mIoUs of 68.0 and 76.9 for weakly and semi-supervised semantic segmentation respectively, which represent a new state-of-the-art.
The code is available at: \url{https://github.com/jbeomlee93/AdvCAM}.

\end{abstract}

\vspace{-0.8em}
\section{Introduction}\label{intro}
Semantic segmentation involves the allocation of a semantic label to each pixel of an image.
It is an essential task in image recognition and scene understanding.
Deep neural networks (DNNs) have facilitated tremendous progress in semantic segmentation~\cite{chen2017deeplab, huang2019ccnet}; but they require a large number of training images annotated with pixel-level labels. Preparing such a training dataset is very expensive: pixel-level annotation of images containing an average of 2.8 objects takes about 4 minutes~\cite{bearman2016s} per image, and a single large (20481024) image depicting a complicated scene requires more than 90 minutes for pixel-level annotation~\cite{cordts2016cityscapes}. 



The need for pixel-level annotation is addressed by weakly supervised learning, in which a segmentation network is trained on images with less comprehensive annotations that are cheaper to obtain than pixel-level labels. 
Weakly supervised methods can use scribbles~\cite{tang2018normalized}, points~\cite{bearman2016s}, bounding boxes~\cite{khoreva2017simple, song2019box}, and class labels~\cite{lee2019ficklenet, Shimoda_2019_ICCV, ahn2018learning, chang2020weakly} as annotations. 
Labeling an image with class labels takes about 20 seconds~\cite{bearman2016s}, making class labels the cheapest option. 
In addition, many public datasets are already annotated with class labels~\cite{deng2009imagenet, everingham2010pascal}, and automated web searches can also provide images with class labels~\cite{lee2019frame, hong2017weakly, shen2018bootstrapping} although the accuracy of such labels may be low.
These considerations make class labels the most popular form of weak supervision.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{latex/Figures/overview_final_v2.pdf}
\vspace{-1.5em}
\caption{\label{overview}  Conceptual description of image manipulation methods for weakly supervised semantic segmentation: (a) erasure~\cite{wei2017object, hou2018self, zhang2018adversarial}; (b) FickleNet~\cite{lee2019ficklenet}; and (c) AdvCAM. (d) Examples of successive attribution maps obtained from iteratively manipulated images.}
\vspace{-1.3em}
\end{figure} Most weakly supervised segmentation methods that use class labels depend on attribution maps obtained from a trained classifier~\cite{zhou2016learning, selvaraju2017grad}.
Such a map identifies the image regions on which the classifier concentrated.
However, these important, or discriminative, regions are relatively small, and most attribution maps do not represent the whole region occupied by a target object, which makes those attribution maps unsuitable for training a semantic segmentation network.
Therefore, many researchers have tried to extend regions to cover more of a target object, by manipulating images~\cite{wei2017object, li2018tell, singh2017hide} or feature maps~\cite{lee2019ficklenet, zhang2018adversarial, hou2018self}.


One popular method for manipulation is erasure: the classifier is forced to find new regions of the target object from which discriminative regions previously located have been removed.
Erasure is effective, but it requires modification of the network, often by adding additional layers~\cite{hou2018self, zhang2018adversarial}, or additional training steps~\cite{wei2017object}.
Another difficulty is the provision of a reliable termination condition for the iterative erasure; the erasure of discriminative region of an image can cause the DNN to misclassify that image.
If the image from which the discriminative region has been erased crosses the decision boundary as shown in Figure~\ref{overview}(a), an erroneous attribution map may be generated.
An alternative method for manipulation is a stochastic perturbation shown in Figure~\ref{overview}(b). 
FickleNet~\cite{lee2019ficklenet} diversifies attribution maps from an image by applying random dropout to the feature maps of a DNN and aggregates them into a unified map. 



We propose a new manipulation method for extending the discriminative regions of a target object.
Our method is based on adversarial attack~\cite{goodfellow2014explaining, kurakin2016adversarial}, but with a benign purpose. Adversarial attack finds a small perturbation of an image that pushes it across the decision boundary to change the classification result.
By contrast, our method operates in an anti-adversarial manner , which is the reversal of adversarial attack.
It aims to find a perturbation that pushes the manipulated image away from the decision boundary, as shown in Figure~\ref{overview}(c).
This manipulation is realized by adversarial climbing, in which an image is perturbed along pixel gradients which increase the classification score of the target class.
The result is that non-discriminative regions, which are nevertheless relevant to that class, gradually become involved in the classification, so that the CAM of the manipulated image identifies more regions of the object. Figure~\ref{overview}(d) shows examples of CAMs obtained by applying this manipulation technique iteratively.


Ascending the gradient ensures that classification score increases, but the repetitive ascending may cause irrelevant areas, such as parts of the backgrounds or regions of other objects, to be activated together or the attribution scores of some part of the target object to be increased dramatically.
We can address these problems by introducing regularization terms that suppress the scores of other classes and limit the attribution scores of the regions that already have high scores. 
The attribution maps obtained from images that have been iteratively manipulated in this way can be used as pseudo ground-truth masks to train a semantic segmentation network in a weakly and semi-supervised manner.


Our method is a post-hoc analysis of the trained classifier, and can be used to improve the performance of existing methods without modification, resulting in new state-of-the-art performance on the PASCAL VOC 2012 benchmark in both weakly and semi-supervised semantic segmentation.

The main contributions of this paper are three-fold:
\begin{itemize}
\vspace{-5pt}
	\item[] We propose AdvCAM, an attribution map of an image that is manipulated to increase the classification score, allowing it to identify more regions of an object.
	\vspace{-5pt}
	\item[] We empirically demonstrate that our method improves the performance of several methods of weakly supervised semantic segmentation without modification or re-training of their networks.
	\vspace{-5pt}
	\item[] Our technique produces significantly better performance on the Pascal VOC 2012 benchmark than existing methods, in both weakly and semi-supervised semantic segmentation.
\vspace{-3pt}
\end{itemize}


\section{Related Work}
\subsection{Weakly Supervised Learning}\label{re_weak}
Existing weakly supervised semantic segmentation methods aim to find the whole region occupied by a target object by obtaining an improved initial seed which contains a good approximation of the region occupied by the object, and growing that region so that more of the object is identified.

\textbf{Obtaining a High Quality Seed:}
Several methods have been proposed to improve the quality of the initial seeds obtained from classifiers. 
Wang \textit{et al.}~\cite{wang2020self} use equivariance regularization during the training of their classifier so that the attribution maps obtained from differently transformed images are equivariant to those transformations.
Chang \textit{et al.}~\cite{chang2020weakly} improve feature learning by using latent semantic classes that are sub-categories of annotated parent classes, which can be pseudo-labeled by clustering image features.
Fan \textit{et al.}~\cite{fan2018cian} and Sun \textit{et al.}~\cite{sun2020mining} capture information shared between several images by considering cross-image semantic similarities and differences. 
Wei \textit{et al.}~\cite{wei2018revisiting} and Lee \textit{et al.}~\cite{lee2018robust} consider the target object in several contexts by combining multiple attribution maps from differently dilated convolutions or from different layers of a DNN.


\textbf{Growing the Object Region:}
Some researchers expand an initial CAM~\cite{zhou2016learning} seed using a method analogous to region growing by examining the neighborhood of each pixel.
Semantic labels are propagated from regions which can confidently be associated with the target object to regions which were initially ambiguous.
SEC~\cite{kolesnikov2016seed} and DSRG~\cite{huang2018weakly} start with a initial CAM seed containing ambiguous regions, and allocates pseudo labels to those ambiguous region during the training of the segmentation network. 
PSA~\cite{ahn2018learning} and IRN~\cite{ahn2019weakly} extend the object region to semantically similar areas by a random walk.
BEM~\cite{chenweakly} synthesizes a pseudo boundary from a CAM and then uses a similar propagation with PSA~\cite{ahn2018learning}.


\subsection{Semi-Supervised Learning}\label{re_semi}
In semi-supervised learning, a segmentation network is trained using a small number of images with pixel-level annotations, together with a much larger number of images with weak annotations or none at all.
Cross-consistency training (CCT)~\cite{ouali2020semi} involves the training of a segmentation network with unlabeled, or weakly labeled, images by enforcing an invariance of the predictions over different perturbations, such as injecting random noise.
Souly \textit{et al.}~\cite{souly2017semi} improve feature learning by using images synthesized by generative adversarial network~\cite{goodfellow2014generative}.
Hung \textit{et al.}~\cite{hung2019adversarial} adopt adversarial training scheme that reduces the distribution gap between predicted segmentation maps and ground-truth maps.




\subsection{Adversarial Attack}\label{adv_manipulation}
Methods of adversarial attack attempt to fool a DNN by presenting it with manipulated input with the intent to deceive.
Adversarial attack can be applied to classification~\cite{goodfellow2014explaining, moosavi2016deepfool}, semantic segmentation~\cite{arnab2018robustness}, and object detection~\cite{xie2017adversarial}.
Deceptive attribution maps can also be produced by adversarial image manipulation~\cite{dombrowski2019explanations} or model parameter manipulation~\cite{heo2019fooling}.
The aim of such attacks is to replace an attribution map with a spurious map, which highlights another location in the same image, without significantly changing the output of the DNN.
Those methods are interested in manipulating the image to cause the neural network's unintended behavior. 
By contrast, we are interested in finding the proper manipulation of the input image, so the resulting attribution map can cover the target object better.



\section{Proposed Method}
We look more closely at adversarial attack methods and class activation map in Section~\ref{adv_attack_method}.
In Sections~\ref{Advcam_method} and~\ref{reg_sec}, we introduce AdvCAM and explain how we generate pseudo ground truth for weakly supervised semantic segmentation. Finally, we show how to train a semantic segmentation network with generated pseudo ground-truth in Section~\ref{train_segnet}. 



\subsection{Preliminaries}\label{adv_attack_method}
\noindent\textbf{Adversarial Attack in more detail:} An adversarial attack aims to find a small pixel-level perturbation that can change the output from a DNN.
In other words, given an input , it finds the perturbation  satisfying , where  is the output of the neural network.
A representative method~\cite{goodfellow2014explaining} of constructing  is to consider the normal vector to the decision boundary of , which can be realized by finding the gradients of  with respect to . 
A manipulated image  can then be obtained as follows:
\vspace{-0.2em}

where  determines the extent of the change to the image. This process can be understood as gradient descent.



\noindent\textbf{Class Activation Map (CAM):}
It identifies the region of an image which a classifier has used.
A CAM is computed from the class-specific contribution of each channel of the feature map to the classification score. It is based on a convolutional neural network that has global average pooling (GAP) before the last classification layer. 
A class activation map  from an image  can be computed as follows:
\vspace{-0.4em}

where  is the weights of the final classification layer for class , and  is the feature map of  prior to GAP.



A CAM bridges the gap between image-level and pixel-level annotations. However, the regions obtained by a CAM are usually much smaller than the full extent of the target object, since the small discriminative regions provide sufficient information for classification.

\subsection{AdvCAM}\label{Advcam_method}
\subsubsection{Adversarial Climbing}
AdvCAM is an attribution map obtained through adversarial climbing, which is an anti-adversarial technique that manipulates the image so as to increase the classification score of that image, with the result that the classifier identifies more regions of objects.
This is the reverse of an adversarial attack based on Eq.~\ref{adv_attack}, which manipulates the image to reduce the classification score. 
Inspired by PGD~\cite{kurakin2016adversarial}, iterative adversarial climbing of the initial image  can be performed using the following relation:
\vspace{-0.1em}

where  () is the adversarial step index,  is the manipulated image at the th step, and  is the classification logit of  for class . 

This process makes the previously non-discriminative yet relevant features become more involved in the classification.
Thus, the CAMs obtained from successive images manipulated by the iteration can be expected to identify an increasing amount of the region of the target object.
We produce a localization map  which encapsulates the results of the iteration by aggregating the CAMs obtained from the manipulated images at each iteration , as follows: 
\vspace{-0.3em}



\subsubsection{How can Adversarial Climbing Improve CAMs?}\label{how_advcam}

The connection between a classification logit  and a CAM, \textit{i.e.} ~\cite{zhang2018adversarial}, infers that adversarial climbing increases , and thus the CAM. 
In this process, features involved in classification are enhanced.
To provide a better understanding how adversarial climbing generates a denser CAM, we consider two questions: \textcircled{\raisebox{-0.9pt}{1}} Can non-discriminative features be enhanced? \textcircled{\raisebox{-0.9pt}{2}} Are those enhanced features class-relevant from a human point of view?



\textbf{\textcircled{\raisebox{-0.9pt}{1}} Can non-discriminative features be enhanced?:} One might think that changing a pixel with a large gradient primarily enhances discriminative features. 
This pixel change affects many features due to the receptive field. However, not all the affected features are necessarily discriminative.
We support this analysis empirically.
We define the discriminative region  and the non-discriminative region , where  is the location index. 
The pixel amplification ratio  is  at location  and step .
Figure~\ref{fig_amp}(a) shows that adversarial climbing makes both  and  grow, but enhances non-discriminative features more than discriminative ones, resulting in a denser CAM.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{latex/Figures/fig_merge_v5.pdf}
\vspace{-2em}
\caption{\label{fig_amp} Distributions of the pixel amplification ratio  for  and  for 100 images, (a) without regularization and (b) with regularization.}
\vspace{-1em}
\end{figure}
 
\textbf{\textcircled{\raisebox{-0.9pt}{2}} Are those enhanced features class-relevant from a human point of view?}
We now consider whether the highlighted non-discriminative features are class-relevant from a human point of view.
Moosavi \textit{et al.}~\cite{moosavi2019robustness} argued that a loss landscape that is sharply curved with respect to input makes a NN vulnerable to adversarial attack.
Researchers have subsequently shown that a flattened loss landscape, obtained by reducing the curvature of the loss surface~\cite{moosavi2019robustness} or encouraging the loss to behave linearly~\cite{qin2019adversarial}, can improve the robustness of a NN.
Systems which are robust in this sense have been shown to produce features that align better with human perception and operate in a easier way to understand~\cite{santurkar2019image, tsipras2018robustness, ilyas2019adversarial}.




By the same token, we can expect that images manipulated by adversarial climbing will produce features that align with human perception well because the curvature of loss surface affected by adversarial climbing is small.
To support this, we visualize the loss landscape of our trained classifier, following Moosavi \textit{et al.}~\cite{moosavi2019robustness}: we obtain a manipulation vector  and a random vector  from the classification loss  computed from an image. 
We determine the surfaces of classification loss values computed from images, manipulated by a vector which is interpolated between  and  using a range of interpolation ratios.
The loss landscape obtained by adversarial climbing (Figure~\ref{landscape}(a)) is much more flatten than that obtained by adversarial attacking (Figure~\ref{landscape}(b)).
Therefore, we can legitimately expect it to increase the attribution of features relevant to the class from a human point of view, resulting in a better CAM.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{latex/Figures/landscapes_done_pdf_v12.pdf}
\vspace{-.7em}
\caption{\label{landscape} Loss landscapes by manipulating images with weighted sums of the normal vector  and a random vector  for (a) adversarial climbing and (b) adversarial attack. The yellow star corresponds to the original image.}
\vspace{-.6em}
\end{figure}
 

\newcommand{\RNum}[1]{\lowercase\expandafter{\romannumeral #1\relax}}


\subsection{Regularization}\label{reg_sec}
Even if the loss surface obtained by adversarial climbing is reasonably flat, too much repetitive adversarial manipulation may cause regions corresponding to objects in the wrong class to be activated, or increase the attribution scores of the regions that already have high scores.
We address this by (\RNum{1}) suppressing the logit values associated with other classes and (\RNum{2}) restricting high attributions on discriminative regions of the target object.

\textbf{Suppressing Other Classes:}
In an image, objects of different classes can mutually increase logit values.
For example, since a chair and a dining table mainly occur together in an image, a NN may infer an increased logit value for the chair from the region of the table.
We thus add regularization that reduces logit values for all classes except .

\textbf{Restricting High Attributions:}
As mentioned in Section~\ref{how_advcam}, adversarial climbing increases the attribution scores for both discriminative and non-discriminative regions in the feature map.
However, the growth of attribution scores for discriminative regions is problematic for two reasons: 1) it prevents new regions from being additionally attributed to the classification score, and 2) if the maximum value of the attribution score increases during adversarial climbing, the normalized scores of the remaining area may decrease. Please see the blue boxes in Figure~\ref{mask_ex}(b).


Therefore we limit the attribution scores in regions that already have high scores during adversarial climbing, so the attribution scores of those regions remain similar to that of .
We realize this scheme by introducing a restricting mask  that contains the regions whose attribution scores of  are higher than the threshold . 
More specifically,  can be represented as follows:
\vspace{-0.1em}

where  is an indicator function. An example mask  is shown in Figure~\ref{mask_ex}(a). 

We add the regularization term so that the values of the CAM corresponding to the regions of  are forced to equal to that of . 
With this regularization,  remains fairly constant but  still grows during adversarial climbing (Figure~\ref{fig_amp}(b)).
Figure~\ref{fig_amp} shows that, adversarial climbing enhances non-discriminative features more than discriminative features (< 2), and regularization makes this difference even
larger (> 2.5).
Thus, new regions of the target object are found more effectively, resulting in a denser CAM (Figure~\ref{mask_ex}(b)).



To apply regularization, we modify Eq.~\ref{sgd} as follows:
\vspace{-0.3em}

\vspace{-2.2em}
-2em] &-  \lambda \left\lVert \mathcal{M} \odot |\texttt{CAM}(x^{t-1}) -  \texttt{CAM}(x^{0})|\right\lVert_1.
\vspace{-0.5em}
\end{split}
-0.95em]
    Method  &  Seed  & + CRF & Mask \\
    \hline\hline \-0.9em]
    \multicolumn{4}{l}{Seed Refine with IRN~\cite{ahn2019weakly}:} \\
    ~\cite{ahn2019weakly} & 48.8  & 54.3 & 66.3 \\
    ~\cite{liu2020weakly} & 50.2 & - & 66.8 \\
    ~\cite{zhang2020causal} & 48.8 & - & 67.9 \\

    AdvCAM (Ours) & \textbf{55.6}  & \textbf{62.1} & \textbf{69.9} \\
    \Xhline{1pt}
    \vspace{-2em}
    \end{tabular}\label{table_seed}\end{table}% 




{\textbf{Weakly Supervised Semantic Segmentation:}} Table~\ref{table_semantic} compares our method with other recently introduced weakly supervised semantic segmentation methods with various levels of supervision: fully supervised pixel-level masks (), bounding boxes () or image class labels (), with and without salient object masks (). All the results in Table~\ref{table_semantic} were obtained using a ResNet-based backbone~\cite{he2016deep}.
With image-level annotation alone, our method achieves mIoU values of 68.1 and 68.0 for the PASCAL VOC 2012 validation and test images respectively. This is significantly better than the other methods under the same level of supervision. In particular, the mIoU value for validation images is 4.6\% higher than that for IRN~\cite{ahn2019weakly}, which is our baseline.
CONTA~\cite{zhang2020causal}, the best-performing method among our competitors, achieves an mIoU value of 66.1; but their method depends upon SEAM~\cite{wang2020self}, which is known to outperform IRN~\cite{ahn2019weakly}. If CONTA is implemented with IRN, the resulting mIoU value is 65.3, which is 2.8\% worse than our method. Figure~\ref{segsample} presents examples of semantic masks produced by FickleNet~\cite{lee2019ficklenet}, IRN~\cite{ahn2019weakly}, and our method.


Our method also outperforms other methods using auxiliary salient object mask supervision~\cite{li2014secrets, liu2010learning} that provides exact boundary information of salient objects in an image, or extra web images or videos~\cite{sun2020mining, lee2019frame}.
The performance of our method is also comparable with that of methods~\cite{song2019box, khoreva2017simple} that use bounding box supervision.



{\textbf{Semi-Supervised Semantic Segmentation:}} 
Table~\ref{tabsemi} compares the mIoU scores of our method on the PASCAL VOC validation and test images with those of other recent semi-supervised segmentation methods, which use 1.5K images with fully supervised masks and 9.1K images with weak annotations.
All the methods in Table~\ref{tabsemi} were implemented on the ResNet-based backbone~\cite{he2016deep}, except that daggered () methods which used the VGG-based backbone~\cite{simonyan2014very}.
We achieve mIoU values of 77.8 and 76.9 for the PASCAL VOC 2012 validation and test images respectively, which is better than the other methods under the same level of supervision. 
Specifically, the performance of our method on the validation images was 4.6\% better than that of CCT~\cite{ouali2020semi}, which is our baseline. Our method even outperforms Song \textit{et al.}~\cite{song2019box} which uses bounding box labels for 9.1K images, instead of class labels.
Figure~\ref{segsample} presents examples of semantic masks produced by CCT~\cite{ouali2020semi} and our method.





\begin{table}[t]
\renewcommand{\arraystretch}{0.95}
\centering
  \caption{Weakly supervised semantic segmentation performance on PASCAL VOC 2012 \textit{val} and \textit{test} images.}\label{table_semantic}
\vspace{-0.7em}
\begin{threeparttable}
\begin{tabular}{l@{\hskip 0.3in}c@{\hskip 0.3in}cc}
    \Xhline{1pt}\-0.9em]
    
    \multicolumn{4}{l}{Supervision: Stronger than image labels} \\
    ~\cite{chen2017deeplab}  &   & 76.8  & 76.2 \\
    ~\cite{khoreva2017simple}   &  & 69.4  & -  \\
    ~\cite{song2019box}   &  & 70.2  & - \\
\-0.9em]
    \multicolumn{3}{l}{Supervision: Image-level tags}\\
~\cite{li2019attention}   &   ,  & 62.1  & 63.0  \\
~\cite{lee2019ficklenet}  & ,  & 64.9 & 65.3\\
    ~\cite{lee2019frame}   & , ,   & 66.5  & 67.4  \\
~\cite{fan2018cian}    & ,  & 64.3  & 65.3 \\
    ~\cite{zhangsplitting}   & ,   & 66.6  & 66.7  \\
    ~\cite{sun2020mining}   & ,   & 66.2  & 66.9  \\
    ~\cite{fanemploying}   & ,   & 67.2  & 66.7  \\
    ~\cite{sun2020mining}   & , ,   & 67.7  & 67.5  \\
    
    ~\cite{ahn2019weakly}  &   & 63.5 & 64.8 \\
    ~\cite{Shimoda_2019_ICCV}    &    & 64.9  & 65.5\\
    ~\cite{wang2020self}    &  & 64.5  & 65.7 \\
~\cite{chenweakly}   &    & 65.7  & 66.6  \\

    ~\cite{chang2020weakly}   &   & 66.1  & 65.9\\


    ~\cite{zhang2020causal}   &   & 66.1  & 66.7  \\

    AdvCAM (Ours) &  & \textbf{68.1} & \textbf{68.0}  \\
\Xhline{1pt}
    
    \end{tabular}\begin{tablenotes}
  \footnotesize
\item pixel-level mask, image class, box, saliency, web\\
\scriptsize
\end{tablenotes}
     \end{threeparttable}
\vspace{-1em}

      \end{table} \begin{table}[tbp]
\renewcommand{\arraystretch}{0.95}
  \centering  \caption{Comparison of semi-supervised semantic segmentation methods on the PASCAL VOC 2012 \textit{val} and \textit{test} images.}
\vspace{-0.7em}
\begin{threeparttable}
    \begin{tabular}{l@{\hskip 0.1in}c@{\hskip 0.15in}cc}
    \Xhline{1pt}\-0.95em]
WSSL~\cite{papandreou2015weakly}  & 1.5K  + 9.1K  & 64.6 & 66.2 \\
MDC~\cite{wei2018revisiting}  & 1.5K  + 9.1K  & 65.7 & 67.6\\
    
Souly \textit{et al.}~\cite{souly2017semi} & 1.5K  + 9.1K  &   65.8  & - \\
    FickleNet~\cite{lee2019ficklenet} & 1.5K  + 9.1K  &   65.8  & - \\
    Song \textit{et al.}~\cite{song2019box}& 1.5K  + 9.1K  &   71.6 & -  \\

    
    Luo \textit{et al.}~\cite{luosemi}& 1.5K  + 9.1K  &   76.6 & -  \\
    CCT~\cite{ouali2020semi} (baseline)& 1.5K  + 9.1K   &   73.2 & -  \\
    AdvCAM (Ours) & 1.5K  + 9.1K  &   \textbf{77.8}  &  \textbf{76.9}\\


\Xhline{1pt}
    \end{tabular}\begin{tablenotes}
  \footnotesize
\item pixel-level mask, image class label, box,  VGG backbone \\
\scriptsize
\end{tablenotes}
     \end{threeparttable}
  \label{tabsemi}
\vspace{-1.3em}

\end{table}% 




\begin{figure*}[t]
\centering
\includegraphics[width=0.96\linewidth]{latex/Figures/seg_samples.pdf}
\vspace{-.7em}
\caption{\label{segsample} Examples of predicted semantic masks for PASCAL VOC \textit{val} images in weakly and semi-supervised manner.}
\vspace{-1em}
\end{figure*}
 \begin{figure*}[t]
\centering
\includegraphics[width=0.96\linewidth]{latex/Figures/fig1_stepsize_1114_pdf.pdf}
\vspace{-.7em}
\caption{\label{eachiter} 
Effect of adversarial climbing and regularization on (a) the seed quality and (b) the proportion of noise. (c) Effect of the regularization coefficient . (d) Effect of the masking threshold . (d) Effect of the step size .}
\vspace{-1em}
\end{figure*}
 
\section{Discussion}



\subsection{Iterative Adversarial Climbing}\label{iterative}
We analyzed the effectiveness of the iterative adversarial climbing and regularization technique introduced in Section~\ref{reg_sec} by evaluating the initial seed in terms of mIoU. Figure~\ref{eachiter}(a) shows the mIoU of the initial seed for each adversarial iteration. Initially, the mIoU rises steeply, with or without regularization; but without regularization the curves peaks around iteration 8.

To analyze this, we evaluate the truthfulness of the newly localized region at each adversarial climbing iteration in terms of the proportion of noise, which we define to be the proportion of pixels that are classified as foreground but are actually background. Without regularization, the proportion of noise rises steeply after some iterations as shown in Figure~\ref{eachiter}(b), which means that new regions tend to be in the regions of background.
Regularization allows new regions of the target object to be found in as many as 30 adversarial steps, keeping the proportion of noise much lower than that of initial CAM.
Figure~\ref{eachiter_ex} shows examples of attribution maps at each adversarial iteration with and without regularization.



\begin{figure*}[t]
\centering
\vspace{-0.2em}
\includegraphics[width=\linewidth]{latex/Figures/each_iter_example_v3.pdf}
\vspace{-2em}
\caption{\label{eachiter_ex} Examples of initial CAMs (the blue boxes) and successive localization maps obtained from images manipulated by iterative adversarial climbing, with the regularization procedure (\textit{top}) and without (\textit{bottom}).}
\vspace{-1.em}
\end{figure*}
 













\begin{table}[tbp]
\renewcommand{\arraystretch}{0.92}
  \centering
  \caption{Effects of AdvCAM on different methods of generating the initial seed: mIoU of the initial seed (Seed) and of the pseudo ground truth mask (Mask), for the PASCAL VOC 2012 training images.}
  \vspace{-0.7em}
    \begin{tabular}{l@{\hskip 0.25in}l@{\hskip 0.25in}l}
     \Xhline{1pt}\-0.9em]
    ~\cite{chang2020weakly} & 50.9 & 63.4 \\
    + AdvCAM  &53.7 & 67.5  \\
    \hline \-0.9em]
    ~\cite{ahn2018learning} & 48.8  & 66.3  \\
    + AdvCAM  & 55.6  & 69.9   \\
\Xhline{1pt}
    \vspace{-1.5em}
    \end{tabular}\label{tab:baselines}\end{table}


%
 
\subsection{Hyper-Parameter Analysis}\label{hyperparam}
In the previous section, we looked at the effect of the number of adversarial iterations (Figures~\ref{eachiter}(a) and (b)).
We also analyzed the sensitivity of the mIoU of the initial seed to the other three hyper-parameters used by AdvCAM. 


\textbf{Regularization Coefficient :} It controls the influence of the masking technique that limits the attribution scores of the regions that already have high scores during adversarial climbing, in Eq.~\ref{reg_loss}.
Figure~\ref{eachiter}(c) shows the mIoU of the initial seed for different values of . When , there is no regularization.
Masking technique improves performance by more than 5\% (50.43 for  \textit{vs.} 55.55 for ).
The flattening of the curve after  suggests that it is not difficult to select a good value of .

\textbf{Masking Threshold }: It controls the size of the restricting mask  in Eq.~\ref{mask}, determining how many pixels' attribution values will remain similar to that of the original CAM during adversarial climbing.
Figure~\ref{eachiter}(d) shows the mIoU of the initial seed for different values of . This parameter is even less sensitive than : varying  between 0.3 and 0.7 produces less than 1\% change in mIoU.

\textbf{Step Size }: It determines the extent of the manipulation to the image in Eq.~\ref{sgd_reg}. Figure~\ref{eachiter}(e) shows the mIoU of the initial seed for different values of . In our system, changes in step size  are not particularly significant.



\subsection{Generality of Our Method}\label{generality}
In addition to IRN~\cite{ahn2019weakly}, we experimented with two state-of-the-art methods of generating an initial seed for weakly supervised semantic segmentation, namely Chang \textit{et al.}~\cite{chang2020weakly} and SEAM~\cite{wang2020self}. 
We used the authors' pre-trained classifier where possible, but we re-trained the classifier of IRN~\cite{ahn2019weakly} since the authors do not provide pre-trained one. We also followed their experimental settings including the backbone networks and mask refinement methods, \textit{i.e.,} we used PSA~\cite{ahn2018learning} to refine the initial seed from ``Chang \textit{et al.} + AdvCAM" or ``SEAM + AdvCAM".
Table~\ref{tab:baselines} gives mIoU values for the initial seed and the pseudo ground truth mask obtained by combining each method with adversarial climbing. The use of AdvCAM improves the quality of the initial seed by an average of over 4\%. 
Our approach does not require those initial seed generators to be modified or retrained.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{latex/Figures/tsne_with_samples_smallsmall_v4_print.pdf}
\caption{\label{fig_tsne} Feature manifold of images with ``bird" (blue) and ``cat" (green), and a trajectory of adversarial climbing for an image of each class. The dimensionality of the feature was reduced by t-SNE~\cite{maaten2008visualizing}.
}
\vspace{-1em}
\end{figure}
 
\subsection{Manifold Visualization}
For visualizing a trajectory of adversarial climbing at a feature-level, we used t-SNE dimensional reduction~\cite{maaten2008visualizing}. 
We collect images that contain a single class of a cat or a bird and that are predicted by the classifier correctly. 
We then construct a set  containing the features of those images, before the final classification layer. 
We also choose a representative image of a cat, and another of a bird, and construct a set  containing the features of those two images and their 20 manipulated images by adversarial climbing.
Figure~\ref{fig_tsne} presents t-SNE visualization of features in .
We can see that adversarial climbing actually pushes the features away from the decision boundary boundary that separates the blue and green areas.
In addition, despite 20 adversarial climbing steps, the manipulated features did not deviate significantly from the feature manifold of each class.






\section{Conclusion}
We have shown how adversarial manipulation can be used to expand the small discriminative regions of a target object, so as to obtain a better localization of that object.
We manipulate images with a pixel-level perturbation, which is obtained from the gradient computed from the output of classifier with respect to the input image, which increase the classification score of the perturbed image. The attribution map of the manipulated image covers more of the target object.
This is a post-hoc analysis of a trained classifier, and therefore no modification or re-training of the classifier is required.
This allows AdvCAM to be readily integrated into existing methods. We have shown that AdvCAM can indeed be combined with recent weakly supervised semantic segmentation networks, and achieved new state-of-the-art performance on both weakly and semi-supervised semantic segmentation. 

\bigskip
\noindent\textbf{Acknowledgements:}
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) [2018R1A2B3001628], AIR Lab (AI Research Lab) in Hyundai \& Kia Motor Company through HKMC-SNU AI Consortium Fund, and the Brain Korea 21 Plus Project in 2021.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}





\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\clearpage

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{latex/Figures/supple_together_pdf.pdf}
\vspace{-1em}
\caption{\label{appendix_eachiter_ex} (a) Threshold analysis. (b) Effect of suppressing other classes.}
\vspace{-1em}
\end{figure*}
 
\renewcommand{\tabcolsep}{2pt}

\begin{table*}[t]
  \caption{Comparison of per-class mIoU scores.}
  \vspace{-1em}
\centering
  \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lccccccccccccccccccccc|c}
    
        \Xhline{1pt}

        \-0.9em]
\multicolumn{22}{l}{Results on PASCAL VOC 2012 validation images:}\\










    AdvCAM (Ours, weak) &   90.0    &   79.8    &   34.1    &   82.6    &   63.3   &   70.5    &    89.4  &   76.0   &    87.3   &  31.4     &   81.3    &   33.1    &    82.5 &    80.8   &   74.0    &  72.9 &  50.3   &    82.3  & 42.2  &  74.1 &   52.9  & 68.1\\
    AdvCAM (Ours, semi) &   94.4    &   91.7    &   65.6    &   89.1    &   72.4   &   72.8    &    93.4  &   86.0   &    90.4   &  37.5     &   90.6    &   58.6    &    84.5 &    88.9   &   83.3    &  84.9  &  62.0   &    81.6  & 49.5  &  85.9 &   71.8  & 77.8\\
    \hline\-0.9em]
    \multicolumn{22}{l}{Results on PASCAL VOC 2012 test images:}\\
AdvCAM (Ours, weak) &   90.1  &   81.2    &   33.6    &   80.4    &   52.4   &   66.6    &    87.1  &   80.5   &    87.2   &  28.9     &   80.1    &   38.5    &    84.0 &  83.0 &  79.5   &   71.9    &  47.5 &  80.8   &    59.1  & 65.4  &  49.7  & 68.0\\
    AdvCAM (Ours, semi) &   94.3  &   93.6    &   65.7    &   90.3    &   54.2   &   74.4    &    91.7  &   85.6   &    91.7  &  28.2     &   88.1    &   67.4    &    86.2 &    88.5   &   89.4    &  82.6 &  62.2   &    87.2  & 47.6  &  80.5 &   65.3  & 76.9\\
        \Xhline{1pt}
    \vspace{-2em}
    \end{tabular}\end{adjustbox}\label{class-specific-results}\end{table*} 
\section{Appendix}
\subsection{Implementation Details}
\textbf{Details for Adversarial Climbing:}
Many recent studies~\cite{wang2020self, chang2020weakly, zhang2020causal} rely on the procedure of PSA~\cite{ahn2018learning} and IRN~\cite{ahn2019weakly} for generating a CAM: a single image is flipped and resized with four different scales of \{0.5, 1.0, 1.5, 2.0\}, and the CAMs are extracted from those eight images. Those CAMs are aggregated into a single map by pixel-wise sum pooling.
We manipulate those eight images independently for adversarial climbing.


\textbf{Details for Semantic Segmentation:} 
We used the PyTorch implementation of DeepLab-v2-ResNet101\footnote{\url{https://github.com/kazuto1011/deeplab-pytorch}} to train our segmentation network.
We used multi-scale testing during inference time following~\cite{wang2020self, ahn2019weakly, ahn2018learning, lee2019ficklenet, lee2019frame}. Specifically, an input image is resized with four different scales of \{0.5, 0.75, 1.0, 1.25\}. These images are fed into the segmentation network independently, and the outputs are aggregated into a single map by pixel-wise max pooling, resulting in the final segmentation map. The experiments were performed on NVIDIA Tesla V100 GPUs.


\subsection{Additional Analysis}

\textbf{Threshold analysis:} 
As mentioned in Section \textcolor{red}{4.2} of the main paper, we report the best initial seed performance by applying a range of thresholds to separate the foreground and background in the map . 
We present the effectiveness of this threshold by evaluating the initial seed, separated by a range of thresholds, in terms of mIoU. Figure~\ref{appendix_eachiter_ex}(a) shows the mIoU of the initial seed obtained from the `CAM', `AdvCAM without regularization', and `AdvCAM with regularization'. We select  for `AdvCAM without regularization' and  for `AdvCAM with regularization', which are the best values of  for each setting according to Figure \textcolor{red}{6}(a) in the main paper.

\textbf{Effects of suppressing other classes:} Section \textcolor{red}{3.4} in the main paper has proposed two regularization terms: 1) suppressing other classes and 2) inhibiting excessive concentration.
The effectiveness of the latter was dealt with in-depth in the main paper (please see Section \textcolor{red}{5}). We will now focus on the effectiveness of suppressing other classes.
To isolate the effect of this regularization procedure, we exclude the masking technique in all experiments here.


Figure~\ref{appendix_eachiter_ex}(b) shows the mIoU of the initial seed for each adversarial iteration with and without the regularization of suppressing other classes. We can see that using this regularization technique provides better adversarial manipulation.



\textbf{Comparison of per-class mIoU scores:}
Table~\ref{class-specific-results} shows the per-class mIoU of our method and recently produced methods.








\textbf{Additional mask examples on semantic segmentation.}
Figure~\ref{appendix_segsample} shows more examples of the semantic masks from FickleNet~\cite{lee2019ficklenet}, IRN~\cite{ahn2019weakly}, CCT~\cite{ouali2020semi}, and our method.



\textbf{Additional examples of localization maps by adversarial climbing.}
Figure~\ref{appendix_eachiter_ex_successive} shows additional examples of successive attribution maps obtained from images manipulated by iterative adversarial climbing.




\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{latex/Figures/seg_ex_print.pdf}
\caption{\label{appendix_segsample} Examples of predicted semantic masks for PASCAL VOC \textit{val} images in weakly and semi-superivsed manner.}
\end{figure*}
 \begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{latex/Figures/heatmap_ex_v3_print.pdf}
\caption{\label{appendix_eachiter_ex_successive} Examples of initial CAMs and successive localization maps obtained from images manipulated by iterative adversarial climbing.}
\end{figure*}
 
\end{document}
