





\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{subcaption}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{53}}\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{ragged2e}
\usepackage{booktabs}       






\title{Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings}











\author{Ilyass Hammouamri \\
CerCo UMR 5549\\
CNRS – Université Toulouse, France\\
\texttt{ilyass.hammouamri@cnrs.fr} \\
\And
Ismail Khalfaoui-Hassani \\
Artificial and Natural Intelligence Toulouse Institute (ANITI) \\
Université de Toulouse, France \\
\texttt{ismail.khalfaoui-hassani@univ-tlse3.fr} \\
\AND
Timothée Masquelier \\
CerCo UMR 5549\\
CNRS – Université Toulouse, France\\
\texttt{timothee.masquelier@cnrs.fr}
}






\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights – one per synapse – whose positions correspond to the delays. These positions are learned together with the weights using the recently proposed Dilated Convolution with Learnable Spacings (DCLS). We evaluated our method on three datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC) and its non-spiking version Google Speech Commands v0.02 (GSC) benchmarks, which require detecting temporal patterns. We used feedforward SNNs with two or three hidden fully connected layers, and vanilla leaky integrate-and-fire neurons. We showed that fixed random delays help and that learning them helps even more. Furthermore, our method outperformed the state-of-the-art in the three datasets without using recurrent connections and with substantially fewer parameters. Our work demonstrates the potential of delay learning in developing accurate and precise models for temporal data processing. Our code is based on PyTorch / SpikingJelly and available at: \url{https://github.com/Thvnvtos/SNN-delays} 
\end{abstract}

\section{Introduction}

Spiking neurons are coincidence detectors \citep{Konig1996,Rossant2011}: they respond more when receiving synchronous, rather than asynchronous, spikes. Importantly, it is the spike arrival times that should coincide, not the spike emitting times --  these times are different because propagation is usually not instantaneous. There is a delay between spike emission and reception, called delay of connections, which can vary across connections. Thanks to these heterogeneous delays, neurons can detect complex spatiotemporal spike patterns, not just synchrony patterns \citep{Izhikevich2006} (see Figure \ref{fig:coincidence_detection}).

In the brain, the delay of a connection corresponds to the sum of the axonal, synaptic, and dendritic delays. It can reach several tens of milliseconds, but it can also be much shorter (1 ms or less) \citep{Izhikevich2006}. For example, the axonal delay can be reduced with myelination, which is an adaptive process that is required to learn some tasks (see \citet{Bowers2017a} for a review). In other words, learning in the brain can not be reduced to synaptic plasticity. Delay learning is also important.

A certain theoretical work has led to the same conclusion: Maass and Schmitt demonstrated, using simple spiking neuron models, that a SNN with k adjustable delays can compute a much richer class of functions than a threshold circuit with k adjustable weights \citep{Maass1999}.

Finally, on most neuromorphic chips, synapses have a programmable delay. This is the case for Intel Loihi \citep{Davies2018}, IBM TrueNorth \citep{Akopyan2015}, SpiNNaker \citep{Furber2014} and SENeCA \citep{Yousefzadeh2022}.

All these points have motivated us and others (see related works in the next section) to propose delay learning rules. Here, we show that delays can be learned together with the weights, using backpropagation, in arbitrarily deep SNNs. More specifically, we first show that there is a mathematical equivalence between 1D temporal convolutions and connection delays. Thanks to this equivalence, we then demonstrate that the delays can be learned using Dilated Convolution with Learnable Spacings \citep{hassani2023dilated, khalfaouihassani2023dilated}, which was recently proposed for another purpose, namely to increase receptive field sizes in non-spiking 2D CNNs for computer vision. In practice, the method is fully integrated with PyTorch and leverages its automatic differentiation engine.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.92\textwidth]{Figures/Fig1.drawio.pdf}
  \caption{Coincidence detection: we consider two neurons  and  with the same positive synaptic weight values.  has a delayed synaptic connection denoted  of ms, thus both spikes from spike train  and  will reach  quasi-simultaneously. As a result, the membrane potential of  will reach the threshold  and  will emit a spike. On the other hand,  will not react to these same input spike trains. }
\label{fig:coincidence_detection}
\end{figure}




















\section{Related Work}
\subsection{Deep Learning for Spiking Neural Networks}

Recent advances in SNN training methods like the surrogate gradient method \citep{neftci2018surrogate, slayer} and the ANN2SNN conversion methods \citep{ann2snn_1, ann2snn_2, ann2snn_3} made it possible to train increasingly deeper spiking neural networks. The surrogate gradient method defines a continuous relaxation of the non-smooth spiking nonlinearity: it replaces the gradient of the Heaviside function used in the spike-generating process with a smooth surrogate gradient that is suitable for optimization. On the other hand, the ANN2SNN methods convert conventional artificial neural networks (ANNs) into SNNs by copying the weights from ANNs while trying to minimize the conversion error. 

Other works have explored improving the spiking neurons using inspiration from biological mechanisms or techniques used in ANNs. The Parametric Leaky Integrate-and-Fire (PLIF) \citep{plif} incorporates learnable membrane time constants that could be trained jointly with synaptic weights. \citet{bellec2018} were the first to propose a method for dynamically adapting firing thresholds in deep (recurrent) SNNs, \citet{hammouamri2022mitigating} also proposes a method to dynamically adapt firing thresholds in order to improve continual learning in SNNs. Spike-Element-Wise ResNet \citep{sew} addresses the problem of vanishing/exploding gradient in the plain Spiking ResNet caused by sigmoid-like surrogate functions and successfully trained the first deep SNN with more than 150 layers. Spikformer \citep{spikformer} adapts the softmax-based self-attention mechanism of Transformers \citep{transformer} to a spike-based formulation. Other recent works like SpikeGPT \citep{spikegpt} and Spikingformer \citep{spikformer} also proposes spike-based transformer architectures. These efforts have resulted in closing the gap between the performance of ANNs and SNNs on many widely used benchmarks.

\subsection{Delays in SNNs}

Few previous works considered learning delays in SNNs. \citet{Delay_Learning_Kernel} proposed a similar method to ours in which they convolve spike trains with an exponential kernel so that the gradient of the loss with respect to the delay can be calculated. However, their method is used only for a shallow SNN with no hidden layers. 

Other methods like \citet{related_delays1,related_delays1bis, related_delays2, related_delays3} also proposed learning rules developed specifically for shallow SNNs with only one layer. \citet{related_delays4} proposed to learn temporal delays with Spike Timing Dependent Plasticity (STDP) in weightless SNNs. \citet{DW_photonic} proposed a method for delay-weight supervised learning in optical spiking neural networks. \citet{iscas} proposed a method for deep feedforward SNNs that uses a set of multiple fixed delayed synaptic connections for the same two neurons before pruning them depending on the magnitude of the learned weights.

To the best of our knowledge, SLAYER \citep{slayer} and \citet{sun22, sun23, sun23-2} (which are based on SLAYER) are the only ones to learn delays and weights jointly in a deep SNN. However, unless a Spike Response Model (SRM) \citep{srm} is used, the gradient of the spikes with respect to the delays is numerically estimated using finite difference approximation, and we think that those gradients are not precise enough as we achieve similar performance in our experiments with fixed random delays (see Table~\ref{table:results} and Figure~\ref{fig:barplots}).

We propose a control test that was not considered by the previous works and that we deem necessary: the SNN with delay learning should outperform an equivalent SNN with fixed random and uniformly distributed delays, especially with sparse connectivity.




















\section{Methods}
\subsection{Spiking Neuron Model}
\label{methods:spiking}
The spiking neuron, which is the fundamental building block of SNNs, can be simulated using various models. In this work, we use the Leaky Integrate-and-Fire model \citep{gerstnerkistler2002}, which is the most widely used for its simplicity and efficiency. The membrane potential  of the -th neuron in layer  follows the differential equation: 



where  is the membrane time constant,  the potential at rest,  the input resistance and  the input current of the neuron at time . In addition to the sub-threshold dynamics, a neuron emits a unitary spike  when its membrane potential exceeds the threshold , after which it is instantaneously reset to . Finally, the input current  is stateless and represented as the sum of afferent weights  multiplied by spikes :



We formulate the above equations in discrete time using Euler's method approximation, and using  and .



We use the surrogate gradient method \citep{neftci2018surrogate} and define  during the backward step, where  is the surrogate arctangent function \citep{plif}.
\begin{figure}[!ht]
    \centering
        \includegraphics[width=0.7\textwidth]{Figures/Figure2.drawio.pdf}
    \caption{Example of one neuron with 2 afferent synaptic connections, convolving  and  with the zero left-padded  and  is equivalent to following Equation \ref{eq:Input_ff_delayd} }
    
    \label{fig:methods_fig1}
\end{figure}

\subsection{Synaptic Delays as a Temporal Convolution}
\label{methods:conv}
In the following, for clarity, we assume one synapse only between pairs of neurons (modeled with a kernel containing only one non-zero element). Generalization to multiple synapses (kernels with multiple non-zero elements) is trivial and will be explored in the experiments.

A feed-forward SNN model with delays is parameterized with  and , where the input of neuron  at layer  is 


We model a synaptic connection from neuron  in layer  to neuron  in layer  which have a synpatic weight  and delay  as a one dimensional temporal convolution (see Figure \ref{fig:methods_fig1}) with kernel  as follows:






where  is the kernel size or maximum delay + 1. Thus we redefine the input  in Equation \ref{eq:Input_ff_delayd} as a sum of convolutions:


We used a zero left-padding with size  on the input spike trains  so that  does correspond to . Moreover, a zero right-padding could also be used, but it is optional; it could increase the expressivity of the learned delays with the drawback of increasing the processing time as the number of time-steps after the convolution will increase.




To learn the kernel elements positions (i.e., delays), we use the 1D version of DCLS \citep{hassani2023dilated}  with a Gaussian kernel \citep{khalfaouihassani2023dilated} centered at , where , and of standard deviation , thus we have:




With  a normalization term and  to avoid division by zero, assuming that the tensors are in \texttt{float32} precision. During training,  are clamped after every batch to ensure their value stays in .

The learnable parameters of the 1D DCLS layer with Gaussian interpolation are the weights , the corresponding delays , and the standard deviations . However, in our case,  are not learned, and all kernels in our model share the same decreasing standard deviation, which will be denoted as . Throughout training, we exponentially decrease  as our end goal is to have a sparse kernel where only the delay position is non-zero and corresponds to the weight.

The Gaussian kernel transforms the discrete positions of the delays into a smoother kernel (see Figure \ref{fig:methods_fig2}), which enables the calculation of the gradients .

\begin{figure}[!ht]

  \centering
  \includegraphics[width=0.7\textwidth]{Figures/fig4.pdf}
  \caption{ This figure illustrates the evolution of the same delay kernels for an example of eight synaptic connections of one neuron throughout the training process. The x-axis corresponds to time, and each kernel is of size . And the y-axis is the synapse id. (a) corresponds to the initial phase where the standard deviation of the Gaussian  is large (), allowing to take into consideration long temporal dependencies. (b) corresponds to the intermediate phase, (c) is taken from the final phase where  is at its minimum value (0.5) and weight tuning is more emphasized. Finally, (d) represents the kernel after converting to the discrete form with rounded positions.}
  \label{fig:methods_fig3}
\end{figure}


By adjusting the parameter , we can regulate the temporal scale of the dependencies. A small value for  enables the capturing of variations that occur within a brief time frame. In contrast, a larger value of  facilitates the detection of temporal dependencies that extend over longer durations. Thus,  tuning is crucial to the trade-off between short-term precision and long-term dependencies.

We start with a high  value and exponentially reduce it throughout the training process, after each epoch, until it reaches its minimum value of 0.5 (Fig.~\ref{fig:methods_fig3}). This approach facilitates the learning of distant long-term dependencies at the initial time. Subsequently, when  has a smaller value, it enables refining both weights and delays with more precision, making the Gaussian kernel more similar to the discrete kernel that is used at inference time. As we will see later in our ablation study (Section~\ref{sec:ablation}), this approach outperforms a constant .

Indeed, the Gaussian kernel is only used to train the model; when evaluating on the validation or test set, it is converted to a discrete kernel as described in Equation \ref{eq:discrete_delays} by rounding the delays. This permits to implement sparse kernels for inference which are very useful for uses on neuromorphic hardware, for example, as they correspond to only one synapse between pairs of neurons, with the corresponding weight and delay.












\section{Experiments}

\subsection{Experimental Setup}



We chose to evaluate our method on the SHD (Spiking Heidelberg Digits) and SSC (Spiking Speech Commands)/GSC (Google Speech Commands v0.02) datasets \citep{shd}, as they require leveraging temporal patterns of spike times to achieve a good classification accuracy, unlike most computer vision spiking benchmarks. Both spiking datasets are constructed using artificial cochlear models to convert audio speech data to spikes; the original audio datasets are the Heidelberg Dataset (HD) and the GSC v0.02 Dataset (SC) \citep{SC} for SHD and SSC, respectively. 

The SHD dataset consists of 10k recordings of 20 different classes that consist of spoken digits ranging from zero to nine in both English and German languages. SSC and GSC are much larger datasets that consist of 100k different recordings. The task we consider on SSC and GSC is the top one classification on all 35 different classes (similar to \citet{shd, baseline}), which is more challenging than the original key-word spotting task on 12 classes, proposed in \citet{SC}.

For the two spiking datasets, we used spatio-temporal bins to reduce the input dimensions. 
Input neurons were reduced from 700 to 140 by binning every 5 neurons; as for the temporal dimension, we used a discrete time-step  ms and a zero right-padding to make sure all recordings in a batch have the same time duration. As for the non-spiking GSC, we used the Mel Spectrogram representation of the waveforms with 140 frequency bins and approximately 100 timesteps to remain consistent to the input sizes used in SSC.



We used a very simple architecture: a feedforward SNN with two or three hidden fully connected layers. Each feedforward layer is implemented using a DCLS module where each synaptic connection is modeled as a 1D temporal convolution with one Gaussian kernel element (as described in Section~\ref{methods:conv}), followed by batch normalization, a LIF module (as described in Section~\ref{methods:spiking}) and dropout. Table \ref{NetworkParams} lists the values of some hyperparameters used for the three datasets (for more details, refer to the code repository). 

\begin{table}[ht]
    \caption{Network parameters for different datasets}
    \label{NetworkParams}
    \centering
    \begin{tabular}{cccccc}
        \toprule
            Dataset  & \# Hidden Layers & \# Hidden size & (ms) &Maximum Delay(ms) & Dropout rate  \\
        \midrule
            SHD & 2 & 256 &    & 250 & 0.4\\
            SSC/GSC & 2 or 3 & 512 & 15 & 300 & 0.25\\
        \bottomrule
  \end{tabular}
{\raggedright\footnotesize
*We found that a LIF with quasi-instantaneous leak  (since  ) is better than using a Heaviside function for SHD. \par}
  \vspace{1ex}
\end{table}
   

The readout layer consists of  LIF neurons with an infinite threshold (where  is 20 or 35 for SHD and SSC/GSC, respectively). Similar to \citet{baseline}, the output  for every neuron  at time  is

where  is the membrane potential of neuron  in the readout layer  at time .\\
The final output of the model after  time-steps is defined as


We denote the batch size by  and the ground truth by . We calculate the cross-entropy loss for one batch as






The Adam optimizer \citep{adam} is used for all models and groups of parameters with base learning rates  for synaptic weights and   for delays. We used a one-cycle learning rate scheduler \citep{one_cycle} for the weights and cosine annealing \citep{cosine_annealing} without restarts for the delays learning rates.
Our work is implemented\footnote{Our code is available at: \url{https://github.com/Thvnvtos/SNN-delays} 
} using the PyTorch-based SpikingJelly\citep{SpikingJelly,Fang2023a} framework.

\subsection{Results}

We compare our method (DCLS-Delays) in Table \ref{table:results} to previous works on the SHD, SSC, and GSC-35 (35 denoting the 35 classes harder version) benchmark datasets in terms of accuracy, model size, and whether recurrent connections or delays were used.

The reported accuracy of our method corresponds to the accuracy on the test set using the best-performing model on the validation set. However, since there is no validation set provided for SHD we use the test set as the validation set (similar to \citet{baseline}). The margins of error are calculated at a 95\% confidence level using a t-distribution (we performed ten and five experiments using different random seeds for SHD and SSC/GSC, respectively).

\begin{table}[ht]
    \caption{Classification accuracy on SHD, SSC and GSC-35 datasets}
    \label{table:results}
    \centering
    \begin{tabular}{llcccl}
        \toprule
            Dataset  &   Method & Rec. & Delays &  \#Params  & Top1 Acc.  \\
        \midrule 
            \multirow{8}{4em}{\textbf{SHD}}
            & \small EventProp-GeNN \footnotesize \citep{eventprop-genn}  & \checkmark & \xmark & N/a & 84.801.5\%     \\
            & \small Cuba-LIF \footnotesize \citep{spikGRU} & \checkmark & \xmark  & 0.14M & 87.801.1\% \\
            & \small Adaptive SRNN \footnotesize \citep{Adaptive-SRNN} & \checkmark & \xmark  & N/a & 90.40\% \\
            & \small SNN+Delays \footnotesize \citep{iscas} & \xmark & \checkmark & 0.1M & 90.43\% \\
            & \small TA-SNN \footnotesize \citep{TA-SNN} & \xmark & \xmark  & N/a &  91.08\%     \\
            & \small STSC-SNN \footnotesize \citep{FFSNNattention} & \xmark & \xmark & 2.1M &  92.36\%     \\
            & \small Adaptive Delays \footnotesize \citep{sun23} & \xmark & \checkmark & 0.1M &  92.45\%      \\
            & \small DL128-SNN-Dloss \footnotesize \citep{sun23-2} & \xmark & \checkmark & 0.14M &  92.56\%      \\
            & \small Dense Conv Delays (ours)  & \xmark & \checkmark   & 2.7M &  93.44\%      \\
            & \small RadLIF \footnotesize \citep{baseline} & \checkmark & \xmark   & 3.9M &  94.62\%      \\
            & \small \textbf{DCLS-Delays (2L-1KC)}  & \xmark & \checkmark & \textbf{0.2M} &  \textbf{95.070.24\%} \\
        \midrule
            \multirow{3}{4em}{\textbf{SSC}}
            & \small Recurrent SNN \footnotesize \citep{shd} & \checkmark & \xmark & N/a &  50.90  1.1\%      \\     
            & \small Heter. RSNN \footnotesize \citep{heterogeneity}  & \checkmark & \xmark & N/a &  57.30\% \\
            & \small SNN-CNN \footnotesize \citep{ieee_cnn} & \xmark & \checkmark  & N/a &  72.03\%     \\
            & \small Adaptive SRNN \footnotesize \citep{Adaptive-SRNN} & \checkmark & \xmark  & N/a &  74.20\%     \\
            & \small SpikGRU \footnotesize \citep{spikGRU} & \checkmark & \xmark  & 0.28M &  77.000.4\%     \\
            & \small RadLIF \footnotesize \citep{baseline} & \checkmark & \xmark  & 3.9M &  77.40\%      \\
            & \small Dense Conv Delays 2L (ours)  & \xmark & \checkmark   & 10.9M &  77.86\%      \\
            & \small Dense Conv Delays 3L (ours)  & \xmark & \checkmark   & 19M &  78.44\%      \\
            & \small \textbf{DCLS-Delays (2L-1KC)} & \xmark & \checkmark & \textbf{0.7M} &  \textbf{79.770.09\%} \\
            & \small \textbf{DCLS-Delays (2L-2KC)} & \xmark & \checkmark & \textbf{1.4M} &  \textbf{80.160.09\%} \\
            & \small \textbf{DCLS-Delays (3L-1KC)} & \xmark & \checkmark & \textbf{1.2M} &  \textbf{80.290.06\%} \\
            & \small \textbf{DCLS-Delays (3L-2KC)} & \xmark & \checkmark & \textbf{2.5M} &  \textbf{80.690.21\%} \\
        \midrule
            \multirow{3}{4em}{\textbf{GSC-35}}
            & \small MSAT \footnotesize \citep{msat} & \xmark & \xmark & N/a &  87.33\%      \\
            & \small Dense Conv Delays 2L (ours)  & \xmark & \checkmark   & 10.9M &  92.97\%      \\
            & \small Dense Conv Delays 3L (ours)  & \xmark & \checkmark   & 19M &  93.19\%      \\
            & \small RadLIF \footnotesize \citep{baseline} & \checkmark & \xmark  & 1.2M &  94.51\%      \\
            & \small \textbf{DCLS-Delays (2L-1KC)} & \xmark & \checkmark & \textbf{0.7M} &  \textbf{94.910.09\%} \\
            & \small \textbf{DCLS-Delays (2L-2KC)} & \xmark & \checkmark & \textbf{1.4M} &  \textbf{95.000.06\%} \\
            & \small \textbf{DCLS-Delays (3L-1KC)} & \xmark & \checkmark & \textbf{1.2M} &  \textbf{95.290.11\%} \\
            & \small \textbf{DCLS-Delays (3L-2KC)} & \xmark & \checkmark & \textbf{2.5M} &  \textbf{95.350.04\%} \\
        \bottomrule
  \end{tabular}
  {\raggedright\footnotesize nL-mKC stands for a model with n hidden layers and kernel count m, where kernel count denotes the number of non-zero elements in the kernel. ``Rec.'' denotes recurrent connections. \par}
\end{table}

Our method outperforms the previous state-of-the-art accuracy on the three benchmarks (with a significant improvement on SSC and GSC) without using recurrent connections (apart from the self-recurrent connection of the LIF neuron), with a substantially lower number of parameters, and using only vanilla LIF neurons. Other methods that use delays do have a slightly lower number of parameters than we do, yet we outperform them significantly on SHD, while they didn't report any results on the harder benchmarks SSC/GSC. Finally, by increasing the number of hidden layers, we found that the accuracy plateaued after two hidden layers for SHD and three for SSC/GSC.
Furthermore, we also evaluated a model (Dense Conv Delay) that uses standard dense convolutions instead of the DCLS ones. This corresponds conceptually to having a fully connected SNN with all possible delay values as multiple synaptic connections between every pair of neurons in successive layers. This led to worse accuracy (partly due to overfitting) than DCLS. The fact that DCLS outperforms a standard dense convolution, although DCLS is more constrained and has fewer parameters, is remarkable. 


\subsection{Ablation study}
\label{sec:ablation}

In this section, we conduct control experiments aimed at assessing the effectiveness of our delay learning method. The model trained using our full method will be referred to as \emph{Decreasing } (specifically, we use the 2L-1KC version), while \emph{Constant } will refer to a model where the standard deviation  is constant and equal to the minimum value of  throughout the training. Additionally, \emph{Fixed random delays} will refer to a model where delays are initialized randomly and not learned, while only weights are learned. Meanwhile, \emph{Decreasing  - Fixed weights} will refer to a model where the weights are fixed and only delays are learned with a decreasing . Finally, \emph{No delays} denotes a standard SNN without delays. To ensure equal parameter counts across all models (for fair comparison), we increased the number of hidden neurons in the \emph{No delays - wider} case, and increased the number of layers instead in the \emph{No delays - deeper} case. Moreover, to make the comparison even fairer, all models have the same initialization for weights and, if required, the same initialization for delays.

\begin{figure}[!ht]
  \centering
  \begin{subfigure}[b]{0.43\textwidth}
    \includegraphics[width=\textwidth]{Figures/ablation_barplot_FC.pdf}
    \caption{FC: Fully Connected}
    \label{fig:fc}
  \end{subfigure}
\begin{subfigure}[b]{0.43\textwidth}
    \includegraphics[width=\textwidth]{Figures/ablation_barplot_S.pdf}
    \caption{S: Sparse connections}
    \label{fig:S}
  \end{subfigure}
  \caption{Barplots of test accuracies on SHD and SSC datasets for different models. With (a): fully connected layers (FC) and (b): sparse synaptic connections (S). Reducing the number of synaptic connections of each neuron to ten for both SHD and SSC. }
  \label{fig:barplots}
\end{figure}

We compared the five different models as shown in Figure \ref{fig:fc}. The models with delays (whether fixed or learned) significantly outperformed the \textit{No delays} model both on SHD (FC) and SSC (FC); for us, this was an expected outcome given the temporal nature of these benchmarks, as achieving a high accuracy necessitates learning long temporal dependencies. However, we didn't expect the Fixed random delays model to be almost on par with models where delays were trained, with Decreasing  model only slightly outperforming it. 

To explain this, we hypothesized that a random uniformly distributed set of delay positions will likely cover the whole temporal range. This hypothesis is plausible given the fact that the number of synaptic connections vastly outnumbers the total possible discrete delay positions for each kernel. Therefore, as the number of synaptic connections within a layer grows, the necessity of moving delay positions away from their initial state diminishes. And only tuning the weights of this set of fixed delays is enough to achieve comparable performance to delay learning.

In order to validate this hypothesis, we conducted a comparison using the same models with a significantly reduced number of synaptic connections. We applied fixed binary masks to the network's synaptic weight parameters. Specifically, for each neuron in the network, we reduced the number of its synaptic connections to ten for both datasets (except for the No delays model, which has more connections to ensure equal parameter counts). This corresponds to 96\% sparsity for SHD and 98\% sparsity for SSC. 
With the number of synaptic connections reduced, it is unlikely that the random uniform initialization of delay positions will cover most of the temporal range. Thus, specific long-term dependencies will need to be learned by moving the delays. 

The test accuracies corresponding to this control test are shown in Figure \ref{fig:S}. It illustrates the difference in performance between the Fixed random delays model and the Decreasing/Constant  models in the sparse case. This enforces our hypothesis and shows the need to perform this control test for delay learning methods. Furthermore, it also indicates the effectiveness of our method.

In addition, we also tested a model where only the delays are learned while the synaptic weights are fixed (Decreasing  - Fixed weights). It can be seen that learning only the delays gives acceptable results in the fully connected case (in agreement with \citet{beyondweights}) but not in the sparse case. To summarize, it is always preferable to learn both weights and delays (and decreasing  helps). If one has to choose, then learning weights is preferable, especially with sparse connectivity.












\section{Conclusion}



In this paper, we propose a method for learning delays in feedforward spiking neural networks using dilated convolutions with learnable spacings (DCLS). Every synaptic connection is modeled as a 1D Gaussian kernel centered on the delay position, and DCLS is used to learn the kernel positions (i.e. delays). The standard deviation of the Gaussians is decreased throughout training, such that at the end of training, we obtain a SNN model with one discrete delay per synapse, which could potentially be compatible with neuromorphic implementations. We show that our method outperforms the state-of-the-art in the temporal spiking benchmarks SHD and SSC and the non-spiking benchmark GSC-35 while using fewer parameters than previous proposals. Finally, we also perform a rigorous control test that demonstrates the effectiveness of our delay learning method. Future work will investigate the use of other kernel functions than the Gaussian or applying our method to other network architectures like convolutional networks.


\begin{comment}
\subsubsection*{Limitations}
The primary limitations of our work revolve around the compatibility and constraints of our delay learning method. Specifically, our method is limited to offline training conducted in discrete-time simulations, and it cannot handle recurrent connections. Additionally, a maximum delay limit, which corresponds to the size of the kernel, must be predetermined and fixed before the learning process.

\subsubsection*{Computational resources}
This project required about 500 GPU hours on a single Nvidia Tesla T4 GPU with two Intel(R) Xeon(R) CPUs @ 2.20 GHz threads. Given this hardware configuration, a single training session lasted approximately 1 hour for the SHD runs, while for the SSC/GSC runs, a single training session lasted around 7 hours. The available computing resources allowed us to perform the required calculations efficiently, leading to accurate and competitive outcomes within a reasonable time.
\end{comment}

\subsubsection*{Acknowledgment}

This research was supported in part by the Agence Nationale de la Recherche under Grant ANR-20-CE45-0005 BRAIN-Net. This work was granted access to the HPC resources of CALMIP supercomputing center under the allocation 2023-[P22021]. Support from the ANR-3IA Artificial and Natural Intelligence Toulouse Institute is gratefully acknowledged. We also want to thank Wei Fang for developing the SpikingJelly framework that we used in this work.


\bibliography{biblio}
\bibliographystyle{iclr2024_conference}

\newpage
\appendix
\section{Appendix}
\subsection{Supplementary figure}
\begin{figure}[!ht]

  \centering
  \includegraphics[width=0.9\textwidth]{Figures/Figure3.drawio.pdf}
  \caption{Gaussian convolution kernels for  synaptic connections. The Gaussians are centered on the delay positions, and the area under their curves corresponds to the synaptic weights . On the right, we see the delayed spike trains after being convolved with the kernels. (the  was omitted for figure clarity).}
  \label{fig:methods_fig2}
\end{figure}

\end{document}
