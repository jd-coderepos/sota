\clearpage

\section*{Appendix}
In Sec.~\ref{sec:dataset_stats} we report the summary of datasets used for the experiments. Sec.~\ref{sec:agem:algorithm} details our \sgem algorithm and Sec.~\ref{sec:supp_sgem_update_rule} provides the proof of update rule of \sgem discussed in Sec.~\ref{sec:approach} of the main paper. In Sec.~\ref{sec:gem_vs_sgem_abal}, we analyze the differences between \sgem and \gem, and describe another variation of \gem, dubbed Stochastic \gem (\mgem). The detailed results of the experiments which were used to generate Fig~\ref{fig:grouped_cifar_mnist} and \ref{fig:grouped_cub_awa} in the main paper are given in Sec.~\ref{sec:detailedresults}. In Sec.~\ref{sec:supp_increasing_epochs_and_arch}, we provide empirical evidence to the conjecture that regularization-based approaches like \ewc require over-parameterized architectures and multiple passes over data in order to perform well as discussed in the Sec.~\ref{subsec:results} of the main paper. In Sec.~\ref{sec:supp_hyperparam_selec}, we provide the grid used for the cross-validation of different hyper-parameters and report the optimal values for different models. Finally, in Sec.~\ref{sec:je_pict_desc}, we pictorially describe the joint embedding model discussed in Sec.~\ref{sec:joint_embed}. 

\appendix

\section{Dataset Statistics}
\label{sec:dataset_stats}
\begin{table}[bh]
\small
\centering
\caption{\em \textbf{Dataset statistics}.}
\label{tab:datasets}
\begin{tabular}{l|cccc}
\toprule
  & \textbf{Perm. MNIST} &\textbf{Split CIFAR} & \textbf{Split CUB} & \textbf{Split AWA} \\
\hline
num. of tasks & 20 & 20 & 20 & 20 \\
input size & 12828 & 33232 & 3224224 & 3224224 \\ 
num. of classes per task & 10 & 5 & 10 & 5 \\
num. of training images per task & 60000 & 2500 & 300 & - \\ 
num. of test images per task & 10000 & 500 & 290 & 560 \\  
\bottomrule
\end{tabular}
\end{table}

\section{\sgem Algorithm}
\label{sec:agem:algorithm}
\begin{algorithm}[h!]
\caption{Training and evaluation of \sgem\ on sequential data } \label{alg:sgem}
\footnotesize
\begin{multicols}{2}
\begin{algorithmic}[1]
  \Procedure{TRAIN}{}
    \State 
    \State 
    \For{}
                \For{}
                        \State 
                        \State 
                        \State 
                                                      \If{}
                                                                               \State 
                                                                                                      \Else
                                                                                                      \State 
                        \EndIf
                        \State 
                \EndFor
                \State 
                \State 
        \EndFor
    \State \textbf{return} 
  \EndProcedure
\end{algorithmic}
\columnbreak
\begin{algorithmic}[1]
   \Procedure{EVAL}{}
                \State 
       \For{}
                \State 
            \For{}
                          \State 
            \EndFor
            \State 
       \EndFor
       \State \textbf{return} 
   \EndProcedure
\end{algorithmic}
\vspace{2mm}
\begin{algorithmic}[1]
   \Procedure{UpdateEpsMem}{}
                \State 
        \For{}
                \State 
            \State 
        \EndFor
        \State \textbf{return} 
   \EndProcedure
\end{algorithmic}
\end{multicols}
\end{algorithm}

\section{\sgem Update Rule}
\label{sec:supp_sgem_update_rule}

Here we provide the proof of the update rule of \sgem (Eq.~\ref{eq:agem_update_rule}), , stated in Sec.~\ref{sec:approach} of the main paper.
\begin{proof}
The optimization objective of \sgem\, as described in the Eq.~\ref{eq:s_gem_primal_obj} of the main paper, is:

Replacing  with  and rewriting Eq.~\ref{eq:supp_sgem_obj} yields:

Note that we discard the term  from the objective and change the sign of the inequality constraint. The Lagrangian of the constrained optimization problem defined above can be written as:


Now, we pose the dual of Eq.~\ref{eq:supp_sgem_lagrang} as:


Lets find the value  that minimizes the  by setting the derivatives of  \wrt to  to zero:


The simplified dual after putting the value of  in Eq.~\ref{eq:supp_sgem_dual} can be written as:


The solution  to the dual is given by:


By putting  in Eq.~\ref{eq:supp_sgem_z_star}, we recover the \sgem\ update rule:

\end{proof}

\section{Analysis of gem\ and \sgem} \label{sec:gem_vs_sgem_abal}

\begin{figure}[tb]
	\begin{subfigure}{0.5\linewidth}
	\begin{center}
		\includegraphics[scale=0.4]{figs/gem_sgem_constraint_violations_mnist.pdf}
		\caption{\small MNIST}
	\end{center}
	\end{subfigure}\begin{subfigure}{0.5\linewidth}
	\begin{center}
		\includegraphics[scale=0.4]{figs/gem_sgem_constraint_violations_cifar.pdf}
		\caption{\small CIFAR}
	\end{center}
	\end{subfigure}\vspace{-3mm}
\caption{\em Number of constraint violations in \textbf{\gem} and \textbf{\sgem} on \textbf{Permuted MNIST} and \textbf{Split CIFAR} as new tasks are learned.}
	\label{fig:gem_vs_sgem_constrnt_violtn}
\end{figure}

In this section, we empirically analyze the differences between \sgem\ and \gem, and report experiments with 
another computationally efficient but worse performing version of \gem.
\subsection{Frequency of Constraint Violations}
\label{sec:supp_freq_const_violtn}
Fig.~\ref{fig:gem_vs_sgem_constrnt_violtn} shows the frequency of constraint violations (see Eq.~\ref{eq:gem_dual_obj} and \ref{eq:s_gem_primal_obj}) 
on Permuted MNIST and Split CIFAR datasets. 
Note that, the number of gradient updates (training steps) per task on MNIST and CIFAR are  and , respectively. 
As the number of tasks increase, \gem\ violates the optimization constraints at almost each training step, whereas \sgem\ plateaus to a much lower value. 
Therefore, the computational efficiency of \sgem\ not only stems from the fact that it avoids solving a QP at each training step (which is much more expensive than a simple
inner product) but also from the fewer number of constraint violations. From the figure, we can also infer that as the number of tasks grows the gap between \gem\ 
and \sgem\ would grow further. Thus, the computational and memory overhead of \gem\ over \sgem, see also Tab.~\ref{tab:gem_vs_sgem_time}, 
gets worse as the number of tasks increases.


\subsection{Average Accuracy and Worst-Case Forgetting}
\label{sec:avg_Acc_wst_fgt}

In Tab.~\ref{tab:avg_acc_wst_fgt}, 
we empirically demonstrate the different properties induced by the objective functions of \gem\ and \sgem. \gem\ enjoys lower worst-case task forgetting
while \sgem\ enjoys better overall average accuracy. This is particularly true on the training examples stored in memory, as on the test set the result is confounded
by the generalization error.


\begin{table}[bh]
\centering
\caption{\em Comparison of average accuracy () and worst-case forgetting () on the \textbf{Episodic Memory ()} and \textbf{Test Set ()}.}
\label{tab:avg_acc_wst_fgt}
\begin{tabular}{@{}c@{\hspace{14pt}}c@{\hspace{4pt}}c@{\hspace{10pt}}c@{\hspace{4pt}}c@{\hspace{12pt}}c@{\hspace{4pt}}c@{\hspace{10pt}}c@{\hspace{4pt}}c@{}}
\toprule
\multicolumn{1}{c}{\textbf{Methods}} & \multicolumn{4}{c}{\textbf{MNIST}}                                             & \multicolumn{4}{c}{\textbf{CIFAR}}                                             \\ 
\midrule
        & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\  \cmidrule(r){2-5} \cmidrule(l){6-9}
        & \multicolumn{1}{c}{}                & \multicolumn{1}{c}{}               & \multicolumn{1}{c}{}               & \multicolumn{1}{c}{}              & \multicolumn{1}{c}{}                & \multicolumn{1}{c}{}               & \multicolumn{1}{c}{}               & \multicolumn{1}{c}{}              \\
        \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(l){6-7} \cmidrule(l){8-9}
\gem     & \multicolumn{1}{c}{99.5}                 & \multicolumn{1}{c}{0}                & \multicolumn{1}{c}{89.5}                &  \multicolumn{1}{c}{0.10}              & \multicolumn{1}{c}{97.1}                 & \multicolumn{1}{c}{0.05}                & \multicolumn{1}{c}{61.2}                & \multicolumn{1}{c}{0.14}                \\
\sgem   & \multicolumn{1}{c}{99.3}                 & \multicolumn{1}{c}{0.008}                & \multicolumn{1}{c}{89.1}                 & \multicolumn{1}{c}{0.13}               &  \multicolumn{1}{c}{72.1}                &  \multicolumn{1}{c}{0.15}               & \multicolumn{1}{c}{62.3}                & \multicolumn{1}{c}{0.15}                \\ 
\bottomrule
\end{tabular}
\end{table}
\SKIP{
Here we provide a formal proof that the objective of \sgem lower bounds the objective of \gem thereby giving better guarantees for average accuracy.
\begin{proof}
The objective of \gem as defined in the Eq.~\ref{eq:gem_main_obj} of the main paper can be rewritten as:

The generalized Lagrangian of Eq.~\ref{eq:supp_gem_main_obj} can be written as:



Similarly, the Lagrangian of \sgem is given by:

where . Rewriting above with the value of  would yield:


Since the first term is common in both the  and , it can be ignored. Let , the simplified Lagrangian for both \gem and \sgem would then become:


Let  be a convex function. The equations above will then reduce to:


For any convex function , Jensen's inequality state that . Therefore, Eq.~\ref{eq:supp_jens_form} implies:


which completes the proof that the objective of \sgem is a strict lower bound of the objective of \gem.
\end{proof}}


\subsection{Stochastic \gem (\mgem)}
\label{sec:supp_stochstc_gem}
In this section we report experiments with another variant of \gem, dubbed Stochastic \gem\ (\mgem). The main idea in \mgem\ is to randomly sample one constraint, 
at each training step, from the possible  constraints of \gem. If that constraint is violated, the gradient is projected only taking into account that 
constraint. Formally, the optimization objective of \mgem is given by:
 

In other words, at each training step, \mgem\ avoids the increase in loss of one of the previous tasks sampled randomly. 
In Tab.~\ref{tab:agem_sgem_comp} we report the comparison of \gem, \mgem\ and \sgem\ on Permuted MNIST and Split CIFAR. 

Although, \mgem is closer in spirit to \gem, as it requires randomly sampling one of the \gem constraints to satisfy, compared to \sgem, which defines the constraint as the average gradient of the previous tasks, it perform slightly worse than \gem, as can be seen from Tab.~\ref{tab:agem_sgem_comp}.

\begin{table}[H]
\centering
\small
\caption{\em Comparison of different variations of \gem on MNIST Permutations and Split CIFAR.}
\label{tab:agem_sgem_comp}
\begin{tabular}{lcc|cc}
\toprule
\multicolumn{1}{l}{\textbf{Methods}} &\multicolumn{2}{c}{\textbf{Permuted MNIST}} &\multicolumn{2}{c}{\textbf{Split CIFAR}} \\
\hline
                              & (\%)        &      & (\%)      &   \\
  \cmidrule(r){2-3} \cmidrule(l){4-5}
{\gem}                  & 89.5          & 0.06      & 61.2  &  0.06        \\
{\mgem} 		        & 88.2        & 0.08  		   & 56.2   &  0.12   			 \\
{\sgem}            	    & 89.1 	    & 0.06       	   & 62.3  & 0.07        \\
  \bottomrule
\end{tabular}
\end{table}


\SKIP{
\section{Zero-shot Performance Evolution over 50 Tasks}
\label{sec:supp_zst_perf} 


In Fig.~\ref{fig:zst_perf_evol2}, we extend the scale of Split AWA experiment and report the -shot performance over a sequence of  tasks. Similar to Split AWA (having 23 tasks) discussed in the main paper, the zero-shot transfer of joint embedding models increases with time and \sgemz perform the best among all the baselines. \pnn ran out of memory while running this experiment confirming again that it does not scale in bigger setups as discussed in Sec.~\ref{subsec:results} of the main paper.  

\begin{figure}[H]
       \includegraphics[scale=0.4]{figs/zst_perf_awa_50tasks.pdf}
\caption{\em \textbf{AWA}: Evolution of zero-shot performance as the learner sees new tasks. }
\label{fig:zst_perf_evol2}
\end{figure}}


\section{Result Tables} \label{sec:detailedresults}
In Tab.~\ref{tab:mnist_cifar_comp},~\ref{tab:cub_comp},~\ref{tab:awa_comp} and~\ref{tab:gem_vs_sgem_time} we report the detailed results which were used to generate Fig.\ref{fig:grouped_cifar_mnist} and \ref{fig:grouped_cub_awa}.
\begin{table}[h]
\centering
\small
\caption{\em Comparison with different baselines on Permuted MNIST and Split CIFAR. The value of  is assigned to a metric when the model fails to train with the cross-validated values of hyper-parameters found on the subset of the tasks as discussed in Sec.~\ref{sec:setup} of the main paper. The numbers are averaged across 5 runs using a different seed each time. The results from this table are used to generate Fig~\ref{fig:grouped_cifar_mnist} in Sec.~\ref{subsec:results} of the main paper.}
\label{tab:mnist_cifar_comp}
\resizebox{\textwidth}{!}{\begin{tabular}{lccc|ccc}\toprule
\multicolumn{1}{l}{\textbf{Methods}} &\multicolumn{3}{c}{\textbf{Permuted MNIST}} &\multicolumn{3}{c}{\textbf{Split CIFAR}} \\
\hline
        & (\%)  &  &   & (\%) &   & \\
\cmidrule(r){2-4} \cmidrule(l){5-7}
{\van}                      & 47.9 (\textpm\ 1.32)      & 0.51 (\textpm\ 0.01) & 0.26 (\textpm\ 0.006)          & 42.9 (\textpm\ 2.07)  & 0.25 (\textpm\ 0.03) & 0.30 (\textpm\ 0.008)        \\
{\icarl}                    & -        & -  & -  & 50.1  & 0.11 & -                            \\
{\ewc}                      & 68.3 (\textpm\ 0.69)     & 0.29 (\textpm\ 0.01) & 0.27 (\textpm\ 0.003)          & 42.4 (\textpm\ 3.02 ) & 0.26 (\textpm\ 0.02) & 0.33 (\textpm\ 0.01)     \\
{\pii}                      &   &   &     & 47.1 (\textpm\ 4.41)   & 0.17 (\textpm\ 0.04) & 0.31 (\textpm\ 0.008)    \\
{\mas}                      & 69.6 (\textpm\ 0.93)      & 0.27 (\textpm\ 0.01)    & 0.29 (\textpm\ 0.003)      & 44.2 (\textpm\ 2.39) & 0.25 (\textpm\ 0.02)  & 0.33 (\textpm\ 0.009)  \\
{\rwalk}                    & 85.7 (\textpm\ 0.56)       & 0.08 (\textpm\ 0.01)  & \textbf{0.31} (\textpm\ 0.005)        & 40.9 (\textpm\ 3.97)   & 0.29 (\textpm\ 0.04) & 0.32 (\textpm\ 0.005)   \\
{\pnn}                      & \textbf{93.5} (\textpm\ 0.07)  & \textbf{0}    & 0.19 (\textpm\ 0.006)         & 59.2 (\textpm\ 0.85) & \textbf{0}    & 0.21 (\textpm\ 0.001)    \\
{\gem}                      & 89.5 (\textpm\ 0.48)     & 0.06 (\textpm\ 0.004) & 0.23 (\textpm\ 0.005) & 61.2 (\textpm\ 0.78) & 0.06 (\textpm\ 0.007)& \textbf{0.36} (\textpm\ 0.007) \\
\textbf{\sgem (Ours)}       & 89.1 (\textpm\ 0.14) & 0.06 (\textpm\ 0.001) & 0.29 (\textpm\ 0.004) & \textbf{62.3} (\textpm\ 1.24)     & 0.07 (\textpm\ 0.01) & 0.35 (\textpm\ 0.01) \\
\cmidrule(r){2-4} \cmidrule(l){5-7}
{\mtask}                    & 95.3      & - & -             & 68.3  & - & - \\
\bottomrule
\end{tabular}}
\end{table}


\begin{table}[h]
\small
\centering
\caption{\em Average accuracy and forgetting of standard models (left) and joint embedding models (right) on Split CUB. The value of `OoM' is assigned to a metric when the model fails to fit in the memory. The numbers are averaged across 10 runs using a different seed each time. The results from this table are used to generate Fig~\ref{fig:grouped_cub_awa} in Sec.~\ref{subsec:results} of the main paper. }
\label{tab:cub_comp}
\resizebox{\textwidth}{!}{\begin{tabular}{lccc}
\toprule
\textbf{Methods} & \multicolumn{3}{c}{\textbf{Split CUB}}  \\
\cmidrule{1-4}
            & (\%)        &    &   \\
\cmidrule{2-4}
{\van}                        & 54.3 (\textpm\ 2.03) /\ 67.1 (\textpm\ 4.77)                   & 0.13 (\textpm\ 0.02)/\ 0.10 (\textpm\ 0.04) & 0.29 (\textpm\ 0.009) /\ 0.52 (\textpm\ 0.01) \\
{\ewc}                        & 54 (\textpm\ 1.08) /\ 68.4 (\textpm\ 4.08)                   & 0.13 (\textpm\ 0.02) /\ 0.09 (\textpm\ 0.03) & 0.29 (\textpm\ 0.007) /\ 0.52 (\textpm\ 0.01) \\
{\pii}                        & 55.3 (\textpm\ 2.28) /\ 66.6 (\textpm\ 5.18)                   & 0.12 (\textpm\ 0.02)/\ 0.10 (\textpm\ 0.04)  & 0.29 (\textpm\ 0.008) /\ 0.52 (\textpm\ 0.01) \\
{\rwalk}                      & 54.4 (\textpm\ 1.82) /\ 67.4 (\textpm\ 3.50)                   & 0.13 (\textpm\ 0.01) /\ 0.10 (\textpm\ 0.03)  & 0.29 (\textpm\ 0.008) /\ 0.52 (\textpm\ 0.01) \\
{\pnn}                        & OoM /\ OoM  & OoM /\ OoM & OoM /\ OoM \\
\textbf{\sgem (Ours)}         & \textbf{62} (\textpm\ 3.5) /\ \textbf{71} (\textpm\ 2.83)  & \textbf{0.07} (\textpm\ 0.02) /\ \textbf{0.07} (\textpm\ 0.01)  & \textbf{0.30} (\textpm\ 0.008) /\ \textbf{0.54} (\textpm\ 0.02) \\
\cmidrule{2-4}
{\mtask}                      & 65.6 /\ 73.8 & - /\ - & - /\ - \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[h]
\small
\centering
\caption{\em Average accuracy and forgetting of standard models (left) and joint embedding models (right) on Split AWA. The value of `OoM' is assigned to a metric when the model fails to fit in the memory. The numbers are averaged across 10 runs using a different seed each time. The results from this table are used to generate Fig~\ref{fig:grouped_cub_awa} in Sec.~\ref{subsec:results} of the main paper. }
\label{tab:awa_comp}
\resizebox{\textwidth}{!}{\begin{tabular}{lccc}
\toprule
\textbf{Methods} & \multicolumn{3}{c}{\textbf{Split AWA}}  \\
\cmidrule{1-4}
            & (\%)        &    &   \\
\cmidrule{2-4}
{\van}  & 30.3 (\textpm\ 2.84) /\ 42.8 (\textpm\ 2.86)         & \textbf{0.04} (\textpm\ 0.01) /\ 0.07 (\textpm\ 0.02) & 0.21 (\textpm\ 0.008) /\ 0.37 (\textpm\ 0.02) \\
{\ewc}  & 33.9 (\textpm\ 2.87) /\ 43.3 (\textpm\ 3.71)         & 0.08 (\textpm\ 0.02) /\ 0.07 (\textpm\ 0.03) & 0.26 (\textpm\ 0.01) /\ 0.37 (\textpm\ 0.02) \\
{\pii}  & 33.9 (\textpm\ 3.25) /\ 43.4 (\textpm\ 3.49)         & 0.08 (\textpm\ 0.02) /\ 0.06 (\textpm\ 0.02) & 0.26 (\textpm\ 0.01) /\ 0.37 (\textpm\ 0.02)  \\
{\rwalk} & 33.9 (\textpm\ 2.91) /\ 42.9 (\textpm\ 3.10)         & 0.08 (\textpm\ 0.02) /\ 0.07 (\textpm\ 0.02) & 0.26 (\textpm\ 0.01) /\ 0.37 (\textpm\ 0.02)  \\
{\pnn} & OoM /\  OoM & OoM /\  OoM & OoM /\  OoM \\
\textbf{\sgem (Ours)} & \textbf{44} (\textpm\ 4.10) /\ \textbf{50} (\textpm\ 3.25)  &
0.05 (\textpm\ 0.02) /\ \textbf{0.03} (\textpm\ 0.02) & \textbf{0.29} (\textpm\ 0.01) /\ \textbf{0.39} (\textpm\ 0.02) \\
\cmidrule{2-4}
{\mtask} & 64.8 /\ 66.8         & - /\ - & - /\ -  \\
\bottomrule
\end{tabular}}
\end{table}

\SKIP{
\begin{table}[h]
\small
\centering
\caption{\em Average accuracy and forgetting of standard models (left) and joint embedding models (right) on Split CUB and AWA datasets. The value of  is assigned to a metric when the model fails to train with the cross-validated values of hyper-parameters found on the subset of the tasks as discussed in Sec.~\ref{sec:setup} of the main paper. The value of `OoM' is assigned to a metric when the model fails to fit in the memory. The numbers are averaged across 5 runs using a different seed each time. The results from this table are used to generate Fig~\ref{fig:grouped_cub_awa} in Sec.~\ref{subsec:results} of the main paper. }
\label{tab:cub_awa_comp}
\resizebox{\textwidth}{!}{\begin{tabular}{lcc|cc}
\toprule
\textbf{Methods} & \multicolumn{2}{c}{\textbf{Split CUB}} & \multicolumn{2}{c}{\textbf{Split AWA}} \\
\cmidrule{1-5}
                              & (\%)        &    & (\%)      &  \\
\cmidrule(r){2-3} \cmidrule(l){4-5}
{\van}                        & 54.9 (\textpm\ 2.11) /\ 66.2 (\textpm\ 4.72)                   & 0.12 (\textpm\ 0.01)/\ 0.11 (\textpm\ 0.04)                  & 28.8 (\textpm\ 2.87) /\ 41.9 (\textpm\ 3.49)         & 0.04 (\textpm\ 0.01) /\ 0.06 (\textpm\ 0.02) \\
{\ewc}                        & 55.4 (\textpm\ 2.25) /\ 65.6 (\textpm\ 4.45)                   & 0.12 (\textpm\ 0.01) /\ 0.12 (\textpm\ 0.04)                  & 33.7 (\textpm\ 2.87) /\ 42.4 (\textpm\ 2.75)         & 0.06 (\textpm\ 0.01) /\ 0.06 (\textpm\ 0.02) \\
{\pii}                        & 55.3 (\textpm\ 2.48) /\ 61 (\textpm\ 5.71)                   & 0.12 (\textpm\ 0.01)/\ 0.17 (\textpm\ 0.05)                  & 33.4 (\textpm\ 3.26) /\ 42.7 (\textpm\ 3.94)         & 0.07 (\textpm\ 0.02) /\ 0.05 (\textpm\ 0.02)  \\
{\mas}                        & 55.6 (\textpm\ 1.87) /\ 64.2 (\textpm\ 4.08)               & 0.12 (\textpm\ 0.01) /\ 0.12 (\textpm\ 0.05)              & - /\ -         & - /\ - \\
{\rwalk}                      & 54.6 (\textpm\ 2.23) /\ 65.6 (\textpm\ 4.13)                   & 0.13 (\textpm\ 0.01) /\ 0.11 (\textpm\ 0.04)                  & 33.6 (\textpm\ 3.28) /\ 42.2 (\textpm\ 2.55)         & 0.07 (\textpm\ 0.02) /\ 0.06 (\textpm\ 0.02)  \\
{\pnn}                        & OoM /\ OoM                     & OoM /\ OoM                    & OoM /\  OoM          & OoM /\  OoM \\
\textbf{\sgem (Ours)}         & \textbf{61} (\textpm\ 2.45) /\ \textbf{70.2} (\textpm\ 3.04)  & \textbf{0.09} (\textpm\ 0.02) /\ \textbf{0.09} (\textpm\ 0.02)    & \textbf{40} (\textpm\ 3.12) /\ \textbf{48.2} (\textpm\ 1.96)  &
0.06 (\textpm\ 0.02) /\ \textbf{0.03} (\textpm\ 0.01) \\
\cmidrule(r){2-3} \cmidrule(l){4-5}
{\mtask}                      & 65.6 /\ 73.8                   & - /\ -                        & 64.8 /\ 66.8         & - /\ -   \\
\bottomrule
\end{tabular}}
\end{table}}


\begin{table}[H]
\small
\centering
\caption{\em Computational cost and memory complexity of different LLL approaches. The timing refers to training time on a GPU device.
Memory cost is provided in terms of the total number of parameters P, the size of the minibatch B, the total size of the network hidden state H (assuming
all methods use the same architecture), the size of the episodic memory M per task. The results from this table are used to generate Fig.~\ref{fig:grouped_cifar_mnist} and \ref{fig:grouped_cub_awa} in Sec.~\ref{subsec:results} of the main paper. 
}
\label{tab:gem_vs_sgem_time}
\begin{tabular}{l cccc || cc} 
\toprule
\textbf{Methods} & \multicolumn{4}{c}{\textbf{Training Time [s]}} & \multicolumn{2}{c}{\textbf{Memory}} \\ \hline
        & MNIST         & CIFAR  & CUB & AWA    & Training & Testing         \\
\cmidrule(r){2-5} \cmidrule(l){6-7}
\van     & 186          & 105 & 54 & 4123 & P + B*H  & P + B*H               \\
\ewc     & 403          & 250 & 72 & 4136 & 4*P + B*H & P + B*H               \\
\pn      & 510 & 409          &  &  & 2*P*T + B*H*T & 2*P*T + B*H*T  \\
\gem     & 3442    & 5238     & - & - & P*T + (B+M)*H & P + B*H              \\
\textbf{\sgem (Ours)}  & 477  & 449 & 420 & 5221  & 2*P + (B+M)*H & P + B*H \\
\bottomrule
\end{tabular}
\end{table}



\section{Analysis of EWC}
\label{sec:supp_increasing_epochs_and_arch}
In this section we provide empirical evidence to the conjecture that regularization-based approaches like \ewc need over-parameterized architectures and
multiple passes over the samples of each task in order to perform well. The intuition as to why models need to be over-parameterized is 
because it is easier to avoid cross-task interference when the model has additional capacity. 
In the single-pass setting and when each task does not have very many training samples, 
regularization-based appraches also suffer because regularization parameters cannot be estimated well from a model 
that has not fully converged. Moreover, for tasks that do not have much data, rgularization-based
approaches do not enable any kind of positive backward transfer~\citep{lopez2017gradient} which further hurts performance as the predictor cannot leverage knowledge
acquired later to improve its prediction on past tasks. Finally, regularization-based approaches perform much better in the multi-epoch setting simply because
in this setting the baseline un-regularized model performs much worse, as it overfits much more to the data of the current task, every time unlearning what it 
learned before.

We consider Permuted MNIST and Split CIFAR datasets as described in Sec.~\ref{sec:experiments} of the main paper. 
For MNIST, the two architecture variants that we experiment with are; ) two-layer fully-connected network with  units in each layer (denoted by  suffix), and ) 
two-layer fully-connected network with  units in each layer (denoted by  suffix). 

For CIFAR, the two architecture variants are; ) ResNet-18 with  times less feature maps in all the layers (denoted by  suffix), and 
) Standard ResNet-18 (denoted by  token).

We run the experiments on \van and \ewc\ with increasing the number of epochs from  to  for Permuted MNIST and from  to  for CIFAR. 
For instance, when epoch is set to , it means that the training samples of task  are presented  times before showing examples from task .
In Fig.~\ref{fig:supp_mnist_inc_epoch} and~\ref{fig:supp_cifar_inc_epoch} we plot the Average Accuracy (Eq.~\ref{eq:avg_acc}) and Forgetting (Eq.~\ref{eq:fgt}) 
on Permuted MNIST and Split CIFAR, respectively. 

We observe that the average accuracy significantly improves with the number of epochs only when \ewc\ is applied
to the big network. In particular, in the single epoch setting, \ewc peforms similarly to the baseline \van on Split CIFAR which has fewer number of training examples per 
task.

\begin{figure}[H]
	\begin{subfigure}{0.45\linewidth}
	\begin{center}
		\includegraphics[scale=0.45]{figs/mnist_increasing_epochs_and_arch_acc_crop.pdf}
		\caption{\small Accuracy}
	\end{center}
	\end{subfigure}\begin{subfigure}{0.55\linewidth}
	\begin{center}
		\includegraphics[scale=0.45]{figs/mnist_increasing_epochs_and_arch_fgt_crop.pdf}
		\caption{\small Forgetting}
	\end{center}
	\end{subfigure}\vspace{-3mm}
\caption{\em \textbf{Permuted MNIST}: Change in average accuracy and forgetting as the number of epochs are increased. Tokens '-S' and '-B' denote smaller and bigger networks, respectively.}
	\label{fig:supp_mnist_inc_epoch}
\end{figure}

\begin{figure}[H]
	\begin{subfigure}{0.45\linewidth}
	\begin{center}
		\includegraphics[scale=0.45]{figs/cifar_increasing_epochs_and_arch_acc_crop.pdf}
		\caption{\small Accuracy}
	\end{center}
	\end{subfigure}\begin{subfigure}{0.55\linewidth}
	\begin{center}
		\includegraphics[scale=0.45]{figs/cifar_increasing_epochs_and_arch_fgt_crop.pdf}
		\caption{\small Forgetting}
	\end{center}
	\end{subfigure}\vspace{-3mm}
\caption{\em  \textbf{Split CIFAR}: Change in average accuracy and forgetting as the number of epochs are increased. Tokens '-S' and '-B' denote smaller and bigger networks, respectively.}
	\label{fig:supp_cifar_inc_epoch}
\end{figure}

\SKIP{
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{figs/increasing_epochs_and_arch_acc.pdf}
\end{figure}
}



\section{Hyper-parameter Selection}
\label{sec:supp_hyperparam_selec}
Below we report the hyper-parameters grid considered for different experiments. Note, as described in the Sec.~\ref{sec:experiments} of the main paper, to satisfy the requirement that a learner does not see the data of a task more than once, first  tasks are used to cross-validate the hyper-parameters. In all the datasets, the value of  is set to `'. The best setting for each experiment is reported in the parenthesis.
\begin{itemize}
        \item {\mtask}
                \begin{itemize}
                        \item learning rate: [, ,  (MNIST perm, Split CIFAR, Split CUB, Split AWA), , , , , ]
                \end{itemize}
        \item {\mtaskz} 
                \begin{itemize}
                        \item learning rate: [, ,  (Split CUB, Split AWA), , , , , ]
                \end{itemize}
        \item {\van}
                \begin{itemize}
                        \item learning rate: [, ,  (MNIST perm, Split CUB),  (Split CIFAR), ,  (Split AWA), , ]
                \end{itemize}
        \item {\vanz}
                \begin{itemize}
                                \item learning rate: [, ,  (Split CUB), ,  (Split AWA), , , ]
                \end{itemize}
        \item {\pnn}
                \begin{itemize}
                        \item learning rate: [,  (MNIST perm, ),  (Split CIFAR, Split AWA),  (Split CUB), , , , ]
                \end{itemize}
        \item {\ewc}
                \begin{itemize}
                        \item learning rate: [, ,  (MNIST perm, Split CIFAR, Split CUB), ,  (Split AWA), , , ]
                        \item regularization: [ (Split CUB),  (MNIST perm, Split CIFAR),  (Split AWA), , ]
                \end{itemize}
        \item {\ewcz}
                \begin{itemize}
                        \item learning rate: [, ,  (Split CUB), ,  (Split AWA), , , ]
                        \item regularization: [,  (Split CUB),  (Split AWA), , ]
                \end{itemize}
        \item {\pii}
                \begin{itemize}
                        \item learning rate: [,  (MNIST perm),  (Split CUB),  (Split CIFAR),  (Split AWA), , , ]
                        \item regularization: [, ,  (MNIST perm, Split CIFAR, Split CUB),  (Split AWA), ]
                \end{itemize}
        \item {\piiz}
                \begin{itemize}
                        \item learning rate: [, ,  (Split CUB), ,  (Split AWA), , , ]
                        \item regularization: [, ,  (Split CUB), ,  (Split AWA)]
                \end{itemize}
        \item {\mas}
                \begin{itemize}
                        \item learning rate: [,  (MNIST perm),  (Split CIFAR, Split CUB), ,  (Split AWA), , , ]
                        \item regularization: [,  (MNIST perm, Split CIFAR, Split CUB),  (Split AWA), ]
                \end{itemize}
        \item {\masz}
                \begin{itemize}
                        \item learning rate: [, ,  (Split CUB), , ,  (Split AWA), , ]
                        \item regularization: [,  (Split CUB, Split AWA), , ]
                \end{itemize}
        \item {\rwalk}
                \begin{itemize}
                        \item learning rate: [,  (MNIST perm),  (Split CIFAR, Split CUB), ,  (Split AWA), , , ]
                        \item regularization: [,  (MNIST perm, Split CIFAR, Split CUB),  (Split AWA), , ]
                \end{itemize}
        \item {\rwalkz}
                \begin{itemize}
                        \item learning rate: [, ,  (SPLIT CUB), ,  (Split AWA), , , ]
                                       \item regularization: [,  (Split CUB),  (Split AWA), , ]
                \end{itemize}
        \item {\sgem}
                \begin{itemize}
                        \item learning rate: [,  (MNIST perm),  (Split CIFAR, Split CUB),  (Split AWA), , , , ]
                \end{itemize}
        \item {\sgemz}
                \begin{itemize}
                        \item learning rate: [, ,  (SPLIT CUB), ,  (Split AWA), , , ]
                \end{itemize}
\end{itemize}

\section{Pictorial Description of Joint Embedding Model }
In Fig.~\ref{fig:je_pict_desc} we provide a pictorial description of the joint embedding model discussed in the Sec.~\ref{sec:joint_embed} of the main paper. 
\label{sec:je_pict_desc}
\begin{figure}[t]
    \centering
    \includegraphics[scale=1.0]{figs/je.pdf}
    \caption{Pictorial description of the joint embedding model discussed in the Sec.~\ref{sec:joint_embed} of the main paper. Modules;  and  are implemented as feed-forward neural networks with  and  parameters, respectively. The descriptor of task  ( ) is a matrix of dimensions , shared among all the examples of the task, constructed by concatenating the -dimensional class attribute vectors of  classes in the task.}
    \label{fig:je_pict_desc}
\end{figure}