\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}




\title{Cache me if you Can: an Online Cost-aware Teacher-Student Framework to Reduce the Calls to Large Language Models}

\author{Ilias Stogiannidis\textsuperscript{, } 
  {\bf Stavros Vassos}\textsuperscript{}
  {\bf Prodromos Malakasiotis}\textsuperscript{, }
  {\bf Ion Androutsopoulos}\textsuperscript{, }\\
  \textsuperscript{}Department of Informatics, Athens University of Economics and Business, Greece \\
  \textsuperscript{}Helvia.ai \\
  \textsuperscript{}Archimedes Unit, Athena Research Center, Greece \\
  \textsuperscript{} Workable \\
  \{stoyianel, rulller, ion\}@aueb.gr,\space stavros@helvia.ai}


\begin{document}
\maketitle 

\begin{abstract}
Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a -NN classifier or a Multi-Layer Perceptron, using two common business tasks, intent recognition and sentiment analysis. Experimental results indicate that significant OpEx savings can be obtained with only slightly lower performance. 
\end{abstract}


\section{Introduction}

Prompting pre-trained Large Language Models (LLMs) aligned to follow instructions \cite{InstructGPT,OpenAssistant} performs impressively well in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets for model fine-tuning, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. For example, SMEs that provide customer support chatbots prompt LLMs like GPT-4 \cite{OpenAI2023GPT4TR} to detect user intents and drive the chatbot-customer interaction \cite{GPT2Dialogues}. The best LLMs, however, currently require a payment per prompting call, and these payments become a significant operating expense (OpEx) for SMEs. Furthermore, customer inputs (e.g., dialog turns) are often very similar over time, hence SMEs end up calling LLMs to handle inputs that may be very similar to inputs already handled by the LLMs in previous (already paid) calls. 

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{img/design/architecture_new.pdf}
    \caption{OCaTS architecture.}
    \label{fig:architecture}
    \vspace*{-4mm}
\end{figure}

We introduce the \emph{Online Cost-aware Teacher-Student} (OCaTS) framework that allows reducing the calls to a commercial LLM, treated as a teacher model, by caching its previous responses and using them to train a local inexpensive student model. OCaTS includes criteria for deciding when to trust the student or call the teacher, and a methodology to tune the criteria and measure the tradeoff between performance and cost. Unlike common teacher-student training for knowledge distillation \cite{DistillationHinton,10.1007/s11263-021-01453-z}, here the teacher does not train the student on all the available instances (in our case, all the incoming customer inputs). Also, unlike teacher-student approaches to self-training \cite{SelfTrainingForDialogues,SelfTrainingAndPreTrainingComplementary}, the teacher is already reasonably effective (but expensive). In that sense, our work is closer to active learning \cite{ActiveLearningBook,HumanInTheLoopBook}, but OCaTS trains the student on labels provided by a teacher LLM, not humans, and there is initially no large pool of unlabeled instances (customer inputs) to select from, as instances arrive online. 

OCaTS can be used with any service that allows prompting LLMs, and any kind of local student model. For experimental purposes, we instantiate OCaTS with GPT-3.5 or GPT-4 as the teacher, and a -NN or Multi-Layer Perceptron (MLP) classifier as the student, using an intent recognition dataset from the banking domain or a sentiment analysis dataset. Experimental results indicate that significant OpEx savings can be obtained with only slightly lower performance. For example, the -NN student can handle approximately two-thirds of the incoming instances (customer inputs) of the intent recognition task without calling the GPT-4 teacher (Fig.~\ref{fig:acc_calls}, left, red line) for a decrease of less than 0.5 percentage points in accuracy (Fig.~\ref{fig:acc_calls}, middle, red and black lines). OCaTS introduces discounted versions of common evaluation measures (e.g., accuracy) that allow an SME to quantify how much it prefers to lean towards fewer calls or less user frustration (different  values in Fig.~\ref{fig:acc_calls}).

Our main contributions are: (i) We introduce a general teacher-student framework that helps SMEs reduce the prompting calls to commercial LLMs and the corresponding OpEx costs by caching the responses of the LLMs and training inexpensive local student models. (ii) We introduce discounted versions of common evaluation measures that allow the SMEs to quantify how much they prefer fewer LLM calls vs.\ increased user frustration (e.g., caused by lower accuracy) and tune the framework's criteria that decide when to trust the local student model or call the LLM teacher accordingly. (iii) We instantiate the framework with GPT-3.5 or GPT-4 as teachers, and a -NN or MLP classifier as students. (iv) We perform experiments on two well-known tasks for SMEs, intent recognition and sentiment analysis, and show that significant cost savings can be obtained with only slightly lower performance. This is a first step towards exploring the benefits of the proposed framework with more datasets, models, and business scenarios.

\section{Framework} \label{sec:framework}

\noindent\textbf{Architecture:} The proposed framework (OCaTS) consists of three main components (Fig.~\ref{fig:architecture}): a \emph{teacher}, typically a resource-intensive model offering premium results; a \emph{student}, a cost-effective model that is typically much smaller and simpler than the teacher; a \emph{cache}, a repository of incoming instances (e.g., customer requests) that have already been processed by the teacher. We assume that the framework is employed to handle a task for which there is no available large dataset for supervised training, apart from a few incoming instances (possibly a handful per class) annotated with the ground truth (e.g., correct labels). This is a very common case for SMEs that cannot afford the cost of creating large task-specific training datasets, but can easily construct small numbers of demonstration instances. The teacher-student setting is \emph{online}, as every incoming instance is handled at inference time as follows. First, the student is called to handle the instance. Then some student- and task-specific \emph{criteria}, which assess the reliability of the student's output, indicate if the student's output (e.g., label) should be used or if the teacher should be consulted. If the student's output is selected, it is returned as the response to the incoming instance. Otherwise, the teacher is called to handle the instance. In the latter case, the instance along with the teacher's result are stored in the cache. Depending on the type of student, periodic re-training takes place, to update the student with the cached instances.

\vspace{1mm}\noindent\textbf{Instantiations:}
In the experiments of this paper, we instantiate OCaTS with a GPT-3.5 or GPT-4 teacher, a distance-weighted -NN or MLP classifier as the student, for a single-label classification task (intent recognition or sentiment analysis). In all cases, we represent each incoming instance (customer request) by its MPNet-based \cite{song2020mpnet} vector representation (text embedding) and we use two criteria (Fig.~\ref{fig:architecture}) to decide when to use the student's response or invoke the teacher: (i) the entropy of the probability distribution (over the label set) produced by the student (-NN or MLP) for the incoming instance, and (ii) the distance of the vector representation of the incoming instance from the centroid of the vector representations of the  most similar  cached instances. Consult \citet{10.1007/s10994-021-06003-9} for other possible criteria. We leave other instantiations of OCaTS (other teachers, students, tasks, representations) for future work. 

\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=\textwidth]{img/plots/knn-banking.pdf}
    \vspace*{-7mm}
    \caption{Number of calls to the teacher (left), accuracy (middle), discounted accuracy (right), using a GPT-4 teacher and a -NN student, for various  values, on Banking77 data. The larger the  the more the SME prefers fewer calls at the expense of increased user frustration. Dashed lines show the discounted accuracy when calling GPT-4 for all incoming instances. OCaTS has a better discounted accuracy than always calling the GPT-4 teacher.} 
    \label{fig:acc_calls}
    \vspace*{-3mm}
\end{figure*}

\vspace{1mm}\noindent\textbf{Discounted evaluation measures:}
The main goal of the proposed architecture is to reduce the number of calls to the expensive teacher model by caching previous teacher responses and using them to train a local inexpensive student model on the SME side. This introduces a tradeoff between the OpEx cost of calling the teacher and the frustration of the end-users when the less accurate student model is used instead. To quantify this tradeoff, we introduce a \emph{discounted} variant  of any common evaluation measure  (e.g., accuracy, F1), as follows:

where  is the number of incoming instances that have been processed (on which  is measured),  is the number of calls made to the teacher while processing the  instances,  shows for what percentage of the incoming instances we call the teacher, and  is a scalar specifying how intensively the measure should be discounted. Assume, for example, that the accuracy of the teacher-student combination is , but that this accuracy is achieved with . If the SME considers this  value (which would translate, e.g., to a monthly cost) as costly as a loss of five percentage points of accuracy, then , and Eq.~\ref{eq:discountedmeasure} becomes , from which we obtain . Larger (or smaller)  values correspond to cases where the SME considers the same  value more (or less) costly in terms of loss of accuracy points. We can also reformulate Eq.~\ref{eq:discountedmeasure} as , where  shows how much  gets discounted to account for the cost of . Then  can intuitively be thought of as a currency exchange rate, showing how expensive  is in terms of  (e.g., loss of accuracy in percentage points).\footnote{We implicitly assume that the exchange rate  is constant for all the values of  and . In practice, it may be different for different ranges of  and , but we leave this for future work.} 

\section{Main experiments} \label{sec:main_experiments}
\label{sec:exp_setup}

Here we discuss the experiments we conducted with the GPT-4 teacher, the -NN student, and the banking intent recognition dataset. In the Appendix, we report two additional sets of experiments, one where we replaced the -NN student by an MLP (Appendix~\ref{sec:mlp}) keeping the rest of the setup unchanged, and one where we replaced the task/dataset (by sentiment analysis) and the teacher (by the cheaper GPT-3.5) otherwise keeping the setup of the initial experiments (Appendix~\ref{sec:sentiment}). The additional experiments verify the conclusions of the experiments of this section. 

\vspace{1mm}\noindent\textbf{Intent recognition dataset:} 
In this section, we use Banking77 \cite{banking77}, an intent recognition dataset from the banking customer service domain. It includes 13,083 customer messages. The ground truth assigns to each message a single label (intent) from the 77 available. The dataset is divided into training (10,003 instances) and test (3,080) subsets. Appendix~\ref{sec:datastats} shows more statistics. 


\vspace{1mm}\noindent\textbf{Few-shot training and development sets:} 
Assuming that an SME can only afford to construct a small number of training instances per class, we use only  instances from the original training set of Banking77, three per class, as a few-shot version of the training set. The 231 instances were manually selected to avoid unclear cases, e.g., similar instances with different ground truth labels. Similarly, we created a few-shot development set of  instances from the original training set, for hyperparameter tuning.


\vspace{1mm}\noindent\textbf{Incoming instances and evaluation measure:} We use the original test set of Banking77 as the incoming instances. We repeat each experiment with five random shufflings of the test set (to obtain five different streams of input instances) and report average scores over the shufflings. We set  to accuracy, since the test set is balanced (Appendix~\ref{sec:datastats}). 

\vspace{1mm}\noindent\textbf{Teacher:} 
In this section, we use GPT-4 \cite{OpenAI2023GPT4TR} as the teacher, the most capable LLM for few-shot in-context learning tasks at the time. Each prompt includes instructions, demonstrators (in-context few-shot examples), and the incoming instance to be classified; see Appendix~\ref{sec:prompt} for details.

\vspace{1mm}\noindent\textbf{Student:} 
In this section, a distance-weighted -NN classifier is used as the student. Vector representations of the incoming instances are generated with a Sentence-Transformer \cite{reimers-gurevych-2019-sentence} variation of MPNet \cite{song2020mpnet}.\footnote{We used \texttt{gpt-4-0314} and \texttt{all-mpnet-base-v2}, in particular, for the teacher and student, respectively.} Appendix~\ref{sec:learning_curves} provides more information on the distance weighting used. It also shows (Fig.~\ref{fig:lrc}) that in a more conventional setting, where a large manually labeled training set is available, the -NN classifier clearly outperforms GPT-4 in accuracy (92\% vs.\ 82\%). Note that for the -NN student, no retraining (Fig.~\ref{fig:architecture}) is necessary, since the cache coincides with the memory of the -NN classifier. The cache is initialized with the 3-shot training examples of the classes (231 instances in total). 

\vspace{1mm}\noindent\textbf{Criteria:} We instantiate the criteria of Fig.~\ref{fig:architecture} with two conditions. Both have to be satisfied for the student's response to be used; otherwise, we call the teacher. The first condition is that the cosine distance between the (MPNet-based) vector representation of the incoming message and the \emph{weighted centroid vector}  of the  nearest neighbors should be less than a threshold . Here
, 
and , where  is the weight assigned by distance weighting (Appendix~\ref{sec:knn_details}) to the -th neighbour, and  is the (MPNet-based) vector representation of the neighbour. Intuitively, this condition ensures that the incoming instance is sufficiently close to cached instances.

To define the second condition, let  be the set of the labels (classes) of the  nearest neighbors (hereafter simply neighbors). Let  be the weight (assigned by distance weighting) to the -th neighbour belonging in class , and let  be the sum of all weights of neighbors of class , i.e., . We define the probability  of each  as:
 The \emph{entropy}  of the probabilities  of the labels of the neighbors is:

The second criterion requires  to be less than a threshold . Intuitively, it requires the neighbors to agree on the label of the incoming instance.

\vspace{1mm}\noindent\textbf{Hyperparameter tuning:} 
There are three hyperparameters here, the number of neighbors , and the thresholds , . We fix  as a practical choice considering that there are 3 examples per class initially. For each indicative  value (0.05, 0.1, 0.2, 0.3), we employ Bayesian optimization on the few-shot development set (Section~\ref{sec:exp_setup}) to determine the optimal combination of the two thresholds that maximize  (discounted accuracy). We let  range in , and  in .\footnote{The maximum  value of  with 77 classes is 4.34, when using natural logarithms. The upper bound of  was chosen based on initial experiments on development data.} We use Optuna's \cite{optuna_2019} implementation of the Tree-Structured Parzen Estimator (TSPE) algorithm \cite{NIPS2011_86e8f7ab} after first performing a  grid search on the range of values of the two thresholds as a head start. The resulting contour maps and the optimal values of the two thresholds per  value can be found in Appendix~\ref{sec:tuning}. 


\label{section:results}

\vspace{1mm}\noindent\textbf{Results:} We evaluate OCaTS for each of the four indicative  values, using the same incoming instances (original test set of Banking 77), and the -specific tuned thresholds , . As illustrated in Fig.~\ref{fig:acc_calls}, OCaTS succeeds in managing the tradeoff between calls to the teacher vs.\ accuracy. Figure~\ref{fig:acc_calls} (left) shows that as the discount factor  increases, fewer calls to the teacher are made. In Fig.~\ref{fig:acc_calls} (middle), we see how much accuracy is sacrificed for this OpEx relief. In particular, for  the accuracy of OCaTS is very close to the accuracy of the GPT-4 teacher, within a margin of 0.37 percentage points (83.05\% vs.\ 82.68\% for the entire test set), while calling the teacher for only 1/3 of the incoming instances (1050 out of 3080). For higher values of , we see the intended drop in accuracy to achieve an increasingly smaller number of calls to the teacher. Figure~\ref{fig:acc_calls} (right) shows that the discounted accuracy  of OCaTS (solid lines, one per  value) is always clearly higher than the corresponding discounted accuracy of always calling the GPT-4 teacher (dashed lines). Hence, OCaTS is clearly better than always calling the teacher, if OpEx costs are taken into account. The difference increases (in favor of OCaTS) as  increases, i.e., as  reducing OpEx costs becomes more important.


\section{Conclusions}
We introduced an Online Cost-aware Teacher-Student framework (OCaTS) to help SMEs reduce OpEx costs by caching the responses of commercial LLMs and training inexpensive local students. We also introduced discounted versions of common evaluation measures, allowing SMEs to quantify the trade-off between LLM calls and user frustration. By instantiating OCaTS with a GPT-4 teacher and a -NN student and experimenting with an intent recognition dataset from the banking domain (Banking77), we showed that the calls to the teacher can be significantly reduced (by 1/3) with only a slight performance drop (0.37 percentage points). Additional experiments with an MLP student  on the same dataset led to the same findings (Appendix~\ref{sec:mlp}). Further experiments with a GPT-3.5 teacher, the initial -NN student, and a sentiment analysis dataset (LMR) also confirmed the conclusions of the previous experiments (Appendix~\ref{sec:sentiment}).    

In future work, we plan to experiment with more datasets and tasks (e.g., question answering), and suggest adaptive policies for  to allow higher OpEx costs (more frequent calls to the teacher) when the cache is cold and be more selective (calling the teacher less frequently) later on. We also plan to enhance OCaTS with indicators of how much we can trust the teacher responses (e.g., confidence of the teacher). Finally, we intend to incorporate more financial metrics (e.g., student costs) in the discounted versions of the evaluation measures and study more complex strategies (e.g., game-theoretic, reinforcement learning) to select the thresholds that determine when to trust the student or call the teacher.

\section{Limitations}
The main scope of this work was to propose a flexible framework (OCaTS) that will allow SMEs to reduce the OpEx costs when incorporating commercial LLMs in their solutions. We considered only two instantiations of the teacher (GPT-4, GPT-3.5) and two instantiations of the student (-NN, MLP) in two tasks (intent recognition, sentiment analysis), leaving further instantiations for future work. 
Although LLMs like GPT-4 and GPT-3.5 can in principle be used for zero-shot inference, we considered in-context learning with
a few demonstrator examples per class. These examples where manually selected to be diverse and indicative of the corresponding classes. This is realistic to some extent; SMEs often request a small number of examples from their customers, but the quality of these examples is not always guaranteed.  In addition, the test sets we used (from Banking77 and LMR) 
were balanced and thus not entirely realistic. However, we shuffle the stream of incoming (test) instances which, hence, do not arrive in a uniform way with the respect to their classes. Also, to tune  and , we used a development set, extracted from the original training data. Such a development set is not always available in practice, but we used it for the sake of the analysis. Interested SMEs can use our analysis, as a starting point for their applications and reduce the number of trials needed to find suitable values for  and . 

Another limitation is that  takes into consideration only the cost to call the teacher (), and indirectly the frustration of the user, as implied by the performance drop. A more detailed analysis would also incorporate the student cost and other financial metrics possibly with different weights; OCaTS can be easily extended in that direction. Finally, we did not compare against existing caching libraries, e.g., GPTCache.\footnote{\url{https://github.com/zilliztech/GPTCache}} These libraries are quite simplistic and less flexible than OCaTS, which can be used with a variety of teacher-student settings.

\section{Ethics statement}
Constantly querying LLMs to solve everyday tasks is not only costly; it has a large energy footprint as well. Our framework aims to alleviate both phenomena. Nonetheless, our study required a significant amount of resources. We believe, however, that by making the framework and the analysis publicly available, we can pave the way towards reducing the resources required by SMEs to handle their day-to-day tasks in the long run.

\section*{Acknowledgements}
This work was supported by Googleâ€™s TPU Research Cloud (TRC) program.\footnote{\url{https://sites.research.google/trc/about/}}


\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section*{Appendix}

\section{Statistics of the Banking77 dataset}
\label{sec:datastats}
Figure~\ref{fig:distributions} shows the label distribution of the original training and test subsets of the Banking77 intent recognition dataset. The training subset exhibits a significant class imbalance (Fig.~\ref{fig:distributions}, left), whereas the test subset is balanced (right). In Table~\ref{tab:banking77-stats}, we provide further statistics which, along with the label distribution, support the selection of the dataset as a realistic case to perform our experiments.

\begin{figure*}[!hbtp]
    \centering
    \includegraphics[width=\columnwidth]{img/dataset/train_subset_label_distribution.pdf}
    \includegraphics[width=\columnwidth]{img/dataset/test_subset_label_distribution.pdf}
    \vspace{-1mm}
    \caption{Label distribution of the original train (left) and test (right) subsets of Banking77.}
    \label{fig:distributions}
\end{figure*}

\begin{table}[!htpb]
\centering
\small
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Statistics} & \textbf{Train} & \textbf{Test} \\
    \hline
        Number of examples & 10,003 & 3,080 \\
        \hline
        Minimum length in characters & 13 & 13 \\
        Average length in characters & 59.5 & 54.2 \\
        Maximum length in characters & 433 & 368 \\
        \hline
        Minimum length in words & 2 & 2 \\
        Average length in words & 11.9 & 10.9 \\
        Maximum length in words & 79 & 69 \\
        \hline
        Number of intents & 77 & 77 \\
    \hline
    \end{tabular}
    \vspace{-1mm}
\caption{Statistics of Banking77.}
\label{tab:banking77-stats}
\end{table}


\section{More details about the LLM teachers}
\label{sec:prompt}

\begin{figure}[ht]
    \begin{minipage}{\columnwidth}
    {\small\texttt{You are an expert assistant in the field of customer service. Your task is to help workers in the customer service department of a company.\\Your task is to classify the customer's question in order to help the customer service worker to answer the question. In order to help the worker you MUST respond with the number and the name of one of the following classes you know.\\In case you reply with something else, you will be penalized.\\The classes are:\\- activate\_my\_card\\- age\_limit\\...}}
    \end{minipage}
    \caption{System message used in the GPT-4 teacher.}
    \label{fig:gpt-system}
    \vspace{-1em}
\end{figure}

\begin{figure}[ht]
    \begin{minipage}{\columnwidth}
    {\small\texttt{\textbf{User}: My new card is here, what's the process for activating it?}}\\
    {\small\texttt{\textbf{Assistant}: activate\_my\_card}}\\{\small\texttt{\textbf{User}: I am unable to activate my card, it won't let me.}}\\
    {\small\texttt{\textbf{Assistant}: activate\_my\_card}}\\{\small\texttt{\textbf{User}: Can you help me activate my card}}\\
    {\small\texttt{\textbf{Assistant}: activate\_my\_card}}\\
    {\small\texttt{\textbf{User}: What is the youngest age for an account?}}\\
    {\small\texttt{\textbf{Assistant}: age\_limit}}\\{\small\texttt{\textbf{User}: What is the appropriate age for my child to be able to open an account?}}\\
    {\small\texttt{\textbf{Assistant}: age\_limit}}\\{\small\texttt{\textbf{User}: How do I set up an account for my children?}}\\
    {\small\texttt{\textbf{Assistant}: age\_limit}}\\
    ...
    \end{minipage}
    \caption{Demonstrators (in-context few-shot examples) used in the GPT-4 teacher as conversation history.}
    \label{fig:gpt-history}
\end{figure}

To prompt the LLMs, we used the chat completion API provided by OpenAI, which takes as input a \emph{system} message (instructions) along with a history of \emph{user} and \emph{assistant} messages, and generates an assistant message as output. The system message specifies the model's behavior as a chat assistant (Fig.~\ref{fig:gpt-system}). For our few-shot setting, we add to the system message a few pairs of user-assistant messages from the training set as demonstrators (Fig.~\ref{fig:gpt-history}). The incoming instance to be classified is added as the last user message in the history.

\section{Distance weighting in the k-NN student}
\label{sec:knn_details}
\label{sec:learning_curves}

The -NN classifier assigns a weight to each one of the  nearest neighbors of an incoming instance. The weight is inversely proportional to the square of the cosine distance between the vectors of the incoming instance and the neighbor,  . The class with the largest sum of neighbor weights is then assigned to the incoming instance. 

Figure~\ref{fig:lrc} shows the learning curve of a distance-weighted -NN classifier that is trained on the original training set of Banking77  and evaluated on the few-shot development set (Section~\ref{sec:exp_setup}). We observe that with sufficient training data (approx.\ 1000 instances) the -NN classifier clearly outperforms GPT-4 in accuracy (92\% vs.\ 82\%), which justifies our choice to use it as a student in our experiments. 

\begin{figure*}[tb]
\centering 
    \includegraphics[width=\columnwidth]{img/contours/contour0.05_new.pdf}
    \includegraphics[width=\columnwidth]{img/contours/contour0.1_new.pdf}
    \includegraphics[width=\columnwidth]{img/contours/contour0.2_new.pdf}
    \includegraphics[width=\columnwidth]{img/contours/contour0.3_new.pdf}
    \vspace{-2mm}    
    \caption{Contour plots (discounted accuracy) obtained during threshold tuning in the main experiments (GPT-4 teacher, -NN student, Banking 77 data), for various  values.}
    \label{fig:contours}
\end{figure*}


\begin{figure}[ht]
    \centering
    \vspace{-1pt} 
    
    \includegraphics[width=\columnwidth]{img/learn_curves/learning_curve.pdf}
    \caption{Learning curve of the distance-weighted -NN student (solid line) when trained on the original training set of Banking77 and evaluated (accuracy) on the few-shot development set. Accuracy of GPT-4 on the few-shot development set also shown (dashed line).}
    \label{fig:lrc}
    \vspace{-1em}
\end{figure}

\begin{figure}[!htb]
\scriptsize
\begin{minipage}{\columnwidth}
\small\texttt{{\textbf{System message:} Analyze the sentiment of the following reviews and classify them as either `positive' or `negative'.
}}\\

{\small\texttt{\textbf{User:} When I started watching this movie I saw the dude 
from Buffy, Xander, and figured ah how nice that he's still making a living acting in movies. Now a weird movie I can stand, given that it's a good dose of weird like for example David Lynch movies, twin peaks, lost highway etc.  [...] It wasn't his acting though, that was alright, but the script just didn't make any sense. Sorry.}}\\
{\small\texttt{\textbf{Assistant}: negative}}\\

{\small\texttt{\textbf{User}: As part of the celebration of the release of Casino Royale,v this film with the new Bond starring in it was shown, from director Roger Michell (Notting Hill). [...] Also starring Peter Vaughan as Toots, Danira Govich as Au Pair,Harry Michell as Harry, Rosie Michell as Rosie and Johnny English's Oliver Ford Davies as Bruce. 
Very good!}}\\
{\small\texttt{\textbf{Assistant}: positive}}\\
...
\end{minipage}
\vspace*{-2mm}
\caption{System message and demonstrators when using GPT-3.5 in the \textbf{sentiment analysis} task.}
\vspace*{-6mm}
\label{fig:sentiment_prompt}
\end{figure}


\section{Hyperparameter tuning} \label{sec:tuning}
We tuned the thresholds  and   anew for each  value. For each  value, we first performed a  grid search on the ranges of values of the two thresholds, to evaluate indicative combinations for the thresholds. These are considered as starting points for the Bayesian optimization (Section~\ref{sec:exp_setup}). 
Figure \ref{fig:contours} illustrates the presence of several good points adjacent to the best point in the main experiments (Section~\ref{section:results}), all of which maximize the discounted metric to a significant extent. This shows that several threshold combinations may be considered optimal. Moreover there are large areas often with a wide range of values for one threshold that are comparable in terms of maximizing the discounted accuracy measure. Table \ref{tab:threshold_pel_l} provides the optimal threshold combination as selected by the optimization process for each  value. We can observe that the tuned value for  decreases from  to , instead of increasing, which can be accounted for by the aforementioned observation that multiple points maximize . However, we also notice the general increasing trend for both thresholds, which leads to fewer calls to the GPT-4 teacher, as one would expect, since higher  values mean that calls to the teacher are more costly.

\begin{table}[!h]
\centering
\small
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{} & \textbf{} &  \\
    \hline
        0.05 & 0.8359 & 0.2269\\
        0.1 & 1.324 & 0.5656 \\
        0.2 & 1.558 & 0.76 \\
        0.3 & 2.336 & 0.4993 \\
    \hline
    \end{tabular}
    \vspace{-1mm}
\caption{Tuned thresholds per  in the main experiments (GPT-4 teacher, -NN student, Banking77 dataset).}
\label{tab:threshold_pel_l}
\vspace{-1em}
\end{table}


\begin{figure*}[!htbp]

\centering
{
    \includegraphics[width=\textwidth]{img/plots/mpnet-banking.pdf}
    \caption{
    Number of calls to the teacher (left),  accuracy (middle), and discounted accuracy (right), using a GPT-4 teacher and an \textbf{MLP student}, for various  values, on Banking77 data. In the left sub-figure, the green line (OCaTS, ) is not visible, because it overlaps with the purple one (OCaTS, ). The results are very similar to those of the main experiments (cf.\ Fig.~\ref{fig:lrc}). Again, OCaTS (right, solid lines) has better discounted accuracy than always calling the teacher (right, dashed lines) for all four indicative  values. The larger the , the fewer the calls to the teacher (left), at the expense of reduced accuracy (middle).}
    \label{fig:mpnet}
    
}
\end{figure*}

\section{MLP student instead of k-NN student} \label{sec:mlp}

As a first step to test the generality of the conclusions of our main experiments (Section~\ref{section:results}), we repeated the main experiments this time using a Multi-layer Perceptron (MLP) student, instead of the -NN student. The MLP has one hidden 
layer with a ReLU activation function, followed by a dropout layer, then an output layer with 77 neurons (number of classes) and a softmax. We use cross-entropy loss. 
Again, we initially filled the cache with 3 few-shot training instances per class ( in total) from the original training set (the same ones as in the main experiments), and the MLP was initially trained on them. 
We retrain the MLP every 100 calls to the teacher, i.e., whenever 100 new training instances have been added to the cache. We used multi-objective Bayesian optimization on the few-shot development set (treated as an incoming stream of user requests), optimizing for both loss and accuracy (no cost factors), to find the optimal hyperparameters for the MLP architecture. Subsequently, we tuned the threshold values (, ) for each lambda again using Bayesian optimization, as we did in the main experiments. The tuned hyperparameters and thresholds are shown in Table~\ref{tab:params} and Table~\ref{tab:threshold_pel_l_mlp}, respectively.
For every incoming test instance, the entropy  (Section~\ref{section:results}) is now computed using the output probabilities of the MLP. The other criterion (Fig.~\ref{fig:architecture}) remains unchanged, i.e., it requires the cosine distance of the MPNet-based vector of the incoming instance to the centroid of the  most similar cached instances to be less than ; we use , as in the main experiments. 
As shown in Fig.~\ref{fig:mpnet}, the experimental results with the MLP student are very similar to those of the main experiments (cf.\ Fig.~\ref{fig:lrc}). 

\begin{table}[h]
\centering
{\small
  \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Hyper-parameter} & \textbf{Range} & \textbf{Value} \\
    \hline
    learning rate &  &  \\
    hidden layer neurons &  &  \\
    dropout rate &  &  \\
    \hline
  \end{tabular}
  }
\vspace{-1mm}
\caption{Tuned hyper-parameters in the Banking77 experiments with the \textbf{MLP student} and GPT-4 teacher.}
\label{tab:params}
\end{table}

\begin{table}[!h]
\centering
\small
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{} & \textbf{} &  \\
    \hline
        0.05 & 0.48 & 0.896\\
        0.1 & 0.479 & 1.553 \\
        0.2 & 0.583 & 1.464 \\
        0.3 & 0.646 & 1.882 \\
    \hline
    \end{tabular}
    \vspace{-1mm}
\caption{Tuned thresholds per  in the Banking77 experiments with the \textbf{MLP student} and GPT-4 teacher.}
\label{tab:threshold_pel_l_mlp}
\vspace{-1em}
\end{table}


\begin{table}[tb]
\centering
\small

    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Statistics} & \textbf{Train} & \textbf{Test} \\
    \hline
        Number of examples & 25,000 & 25.000 \\
        \hline
        Minimum length in characters & 52 & 32 \\
        Average length in characters & 1325.06 & 1293.7 \\
        Maximum length in characters & 13704 & 12988 \\
        \hline
        Minimum length in words & 10 & 4 \\
        Average length in words & 233.7 & 228.5 \\
        Maximum length in words & 2470 & 2278 \\
        \hline
        Number of sentiments & 2 & 2 \\
    \hline
    \end{tabular}
    \vspace{-1mm}
\caption{Statistics of Large Movie Review (LMR).}
\label{tab:lmr-stats}
\vspace{-1em}
\end{table}

\begin{figure*}[!t]
\centering
{
    \includegraphics[width=\textwidth]{img/plots/knn-imdb.pdf}
    \vspace{-4mm}
    \caption{
    Number of calls to the teacher (left),  accuracy (middle), and discounted accuracy (right), using a GPT-3.5 teacher and a -NN student, for various  values, on \textbf{sentiment analysis} (LMR) data. The results are very similar to those of the previous experiments (cf.\ Figures~\ref{fig:acc_calls} and \ref{fig:mpnet}).}
    \label{fig:knn-imdb}
}
\end{figure*}

\section{Sentiment analysis experiments}
\label{sec:sentiment}

As a further step to confirm the conclusions of the previous experiments (Section~\ref{sec:main_experiments}, Appendix~\ref{sec:mlp}), 
we conducted an additional set of experiments, now using a sentiment analysis task.
We used the Large Movie Review (LMR) dataset \cite{maas-EtAl:2011:ACL-HLT2011}, which consists of 50,000 IMDB reviews, and two labels (positive, negative review). Table~\ref{tab:lmr-stats} shows more statistics for this dataset.

Again, we assume that an SME can afford to create only a small number of training instances per class. To simulate this limited data setting, we created few-shot versions of the training and development sets of LMR, much as in Section~\ref{sec:main_experiments}. For the few-shot training set, we randomly selected 10 instances from the original training set, 5 for each sentiment, as follows. We employed the GPT-3.5 tokenizer from OpenAI\footnote{\url{https://github.com/openai/tiktoken}} and computed the token size for each review. Subsequently, we randomly chose reviews with token sizes close to the median. Similarly, for the few-shot development set, we randomly selected 1,000 instances from the original training set, 500 for each sentiment. Finally, due to limited resources, we used only 5,000 instances of the original test set as the stream of incoming instances. As in the previous experiments, we repeat each experiment with five random shufflings of the incoming stream, and report the average scores. We use the distance weighted -NN classifier, as in Section~\ref{sec:exp_setup}. Again, we set , and we employ Bayesian optimization on the few-shot development set to determine the optimal combination of the two thresholds that maximize . We let  range in , and  in .\footnote{The maximum  value of  with 2 classes is 0.69 when using the natural logarithm.} For the teacher, we used the cheaper GPT-3.5 in these experiments.\footnote{We used version \texttt{gpt-3.5-turbo-0301}.} We prompted GPT-3.5 using the same in-context learning approach outlined in Section~\ref{sec:exp_setup}; we provided the few-shot training set we created as demonstrators and asked GPT-3.5 to classify the review as either positive or negative (Figure~\ref{fig:sentiment_prompt}). Figure \ref{fig:knn-imdb} shows that the results were very similar to those of Section~\ref{sec:main_experiments} and Appendix~\ref{sec:mlp}.



\end{document}
