\def\year{2019}\relax
\documentclass[letterpaper]{article} \usepackage{aaai19}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{graphicx}  \frenchspacing  \usepackage{comment}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm,bbm}
\usepackage{color}
\usepackage[para,online,flushleft]{threeparttable}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits}
\newcommand{\indep}{\mathop{\perp\!\!\!\!\perp}}
\newcommand{\citet}[1]
{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}{\cite}
\newcommand{\citealp}[1]
{\citeauthor{#1}, \citeyear{#1}}
\allowdisplaybreaks
\setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \pdfinfo{
/Title (Efficient Counterfactual Learning from Bandit Feedback)
/Author (Yusuke Narita, Shota Yasui, Kohei Yata)
/Keywords (Machine Learning, AAAI) }
\setcounter{secnumdepth}{2}

\title{Efficient Counterfactual Learning from Bandit Feedback}
\author{Yusuke Narita\\ Yale University \\yusuke.narita@yale.edu 
\And Shota Yasui\\ CyberAgent Inc. \\yasui\_shota@cyberagent.co.jp 
\And Kohei Yata\\ Yale University \\kohei.yata@yale.edu
}
\begin{document}
\maketitle
\begin{abstract}
\noindent What is the most statistically efficient way to do off-policy optimization with batch data from bandit feedback? 
For log data generated by contextual bandit algorithms, we consider offline estimators for the expected reward from a counterfactual policy. 
Our estimators are shown to have lowest variance in a wide class of estimators, achieving variance reduction relative to standard estimators. 
We then apply our estimators to improve advertisement design by a major advertisement company. 
Consistent with the theoretical result, our estimators allow us to improve on the existing bandit algorithm with more statistical confidence compared to a state-of-the-art benchmark.
\end{abstract}
\section{Introduction}

Interactive bandit systems (e.g. personalized education and medicine, ad/news/recommendation/search platforms) produce log data valuable for evaluating and redesigning the systems. 
For example, the logs of a news recommendation system record which news article was presented and whether the user read it, giving the system designer a chance to make its recommendation more relevant. 
Exploiting log data is, however, more difficult than conventional supervised machine learning: 
the result of each log is only observed for the action chosen by the system (e.g. the presented news) but not for all the other actions the system could have taken. 
Moreover, the log entries are biased in that the logs over-represent actions favored by the system. 

A potential solution to this problem is an A/B test that compares the performance of counterfactual systems. 
However, A/B testing counterfactual systems is often technically or managerially infeasible, since deploying a new policy is time- and money-consuming, and entails a risk of failure.

This leads us to the problem of \textit{counterfactual (off-policy) evaluation and learning}, where one aims to use batch data collected by a logging policy to estimate the value of a counterfactual policy or algorithm without employing it \citep{li2010contextual,Strehl2010,li2011unbiased,Li2012,bottou2013counterfactual,Swaminathan2015,Swaminathan2015b,wang2016optimal,swaminathan2017off}. 
Such evaluation allows us to compare the performance of counterfactual policies to decide which policy should be deployed in the field. 
This alternative approach thus solves the above problem with the naive A/B test approach. \\

\textbf{Method.} 
For off-policy evaluation with log data of bandit feedback, this paper develops and empirically implements a variance minimization technique. 
Variance reduction and statistical efficiency are important for minimizing the uncertainty we face in decision making.  
Indeed, an important open question raised by \citet{li2015offline} is how to achieve ``statistically more efficient (even optimal) offline estimation" from batch bandit data. 
This question motivates a set of studies that bound and characterize the variances of particular estimators \citep{Dudik2014,li2015toward,thomas2015high,munos2016safe,thomas2016data,agarwal2017effective}. 

We study this statistical efficiency question in the context of offline policy value estimation with log data from a class of contextual bandit algorithms. 
This class includes most of the widely-used algorithms such as contextual -Greedy and Thompson Sampling, as well as their non-contextual analogs and random A/B testing. 
We allow the logging policy to be unknown, degenerate (non-stochastic), and time-varying, all of which are salient in real-world bandit applications. 
We also allow the evaluation target policy to be degenerate, again a key feature of real-life situations. 

We consider offline estimators for the expected reward from a counterfactual policy. 
Our estimators can also be used for estimating the average treatment effect. 
Our estimators are variations of well-known inverse probability weighting estimators (\citet{horvitz1952generalization}, \citet{rosenbaum1983central}, and modern studies cited above) except that we use an \textit{estimated} propensity score (logging policy) even if we know the true propensity score. 
We show the following result, building upon \citet{Bickel1993}, \citet{HIR2003}, and \citet{Ackerberg2014} among others:
\begin{quote}
\textbf{Theoretical Result 1.} Our estimators minimize the variance among all reasonable estimators. 
More precisely, our estimators minimize the asymptotic variance among all ``asymptotically normal" estimators (in the standard statistical sense defined in Section \ref{estimation}). 
\end{quote}
We also provide estimators for the asymptotic variances of our estimators, thus allowing analysts to calculate the variance in practice. 
In contrast to Result 1, we also find: 
\begin{quote}
\textbf{Theoretical Result 2.} Standard estimators using the true propensity score (logging policy) have larger asymptotic variances than our estimators. 
\end{quote}

\noindent Perhaps counterintuitively, therefore, the policy-maker should use an estimated propensity score even when she knows the true one. \\

\textbf{Application.} We empirically apply our estimators to evaluate and optimize the design of online advertisement formats. 
Our application is based on proprietary data provided by CyberAgent Inc., the second largest Japanese advertisement company with about 6 billion USD market capitalization (as of November 2018). 
This company uses a contextual bandit algorithm to determine the visual design of advertisements assigned to users. 
Their algorithm produces logged bandit data. 

We use this data and our estimators to optimize the advertisement design for maximizing the click through rates (CTR). 
In particular, we estimate how much the CTR would be improved by a counterfactual policy of choosing the best action (advertisement) for each context (user characteristics). 
We first obtain the following result: 

\begin{quote}
\textbf{Empirical Result A.} Consistent with Theoretical Results 1-2, our estimators produce narrower confidence intervals about the counterfactual policy's CTR than a benchmark using the known propensity score \cite{Swaminathan2015b}. 
\end{quote}

\noindent This result is reported in Figure \ref{figure}, where the confidence intervals using ``True Propensity Score (Benchmark)" are wider than other confidence intervals using propensity scores estimated either by the Gradient Boosting, Random Forest, or Ridge Logistic Regression. 

Thanks to this variance reduction, we conclude that the logging policy's CTR is below the confidence interval of the hypothetical policy of choosing the best advertisement for each context. 
This leads us to obtain the following bottomline: 

\begin{quote}
\textbf{Empirical Result B.} Unlike the benchmark estimator, our estimator predicts the hypothetical policy to statistically significantly improve the CTR by 10-15\% (compared to the logging policy). 
\end{quote}

\noindent Empirical Results A and B therefore show that our estimator can substantially reduce uncertainty we face in real-world policy-making. 

\begin{figure}
\begin{center}
 \includegraphics[width=85mm]{plot1.pdf}
\end{center}
\caption{Improving Ad Design with Lower Uncertainty}\label{figure}\par
~\par
         \fontsize{9.0pt}{10.0pt}\selectfont \textit{Notes}: This figure shows estimates of the expected CTRs of the logging policy and a counterfactual policy of choosing the best action for each context. 
         CTRs are multiplied by a constant for confidentiality reasons. 
         We obtain these estimates by the ``self-normalized inverse probability weighting estimator" using the true propensity score (benchmark thanks to \citet{Swaminathan2015b}) or estimated propensity scores (our proposal), both of which are defined and analyzed in Section \ref{estimation}. 
         Bars indicate 95\% confidence intervals based on our asymptotic variance estimators developed in Section \ref{section:variance}.
\end{figure} 




\section{Setup}\label{model}

\subsection{Data Generating Process}\label{dgp}

We consider a general multi-armed contextual bandit setting. 
There is a set of  \textit{actions} (equivalently, \textit{arms} or \textit{treatments}), , that the decision maker can choose from.
Let  denote a potential reward function that maps actions into rewards or outcomes, where  is the reward when action  is chosen (e.g., whether an advertisement as an action results in a click).
Let  denote \textit{context} or \textit{covariates} (e.g., the user's demographic profile and browsing history) that the decision maker observes when picking an action. 
We denote the set of contexts by .
We think of  as a random vector with unknown distribution .

We consider log data coming from the following data generating process (DGP), which is similar to those used in the literature on the offline evaluation of contextual bandit algorithms \citep{li2010contextual,Strehl2010,li2011unbiased,Li2012,Swaminathan2015,Swaminathan2015b,swaminathan2017off}.
We observe data  with  observations. 
 where  is a binary variable indicating whether action  is chosen in round . 
 denotes the reward observed in round , i.e., . 
 denotes the context observed in round .


A key feature of our DGP is that the data  are divided into  batches, where different batches may use different choice probabilities (propensity scores). 
Let  denote a random variable indicating the batch to which round  belongs.
We treat this batch number as one of context variables and write , where  is the vector of context variables other than . 


Let  denote the potentially unknown probability vector indicating the probability that each action is chosen in round . 
Here  with  being the probability that action  is chosen. 
A {\it contextual bandit algorithm} is a sequence  of distribution functions of choice probabilities  conditional on , where  for  and  is the support of , where  is the set of distributions over .
 takes context  as input and returns a distribution of probability vector  in rounds of batch .
 can vary across batches but does not change across rounds within batch . 
We assume that the log data are generated by a contextual bandit algorithm  as follows:

\begin{itemize}
\item In each round ,  is i.i.d. drawn from distribution . 
Re-order round numbers so that they are monotonically increasing in their batch numbers . 
\item In each round  within batch  and given , probability vector  is drawn from . 
Action is randomly chosen based on probability vector , creating the action choice  and the associated reward . 
\end{itemize}

\noindent Here, the contextual bandit algorithm  and the realized probability vectors  may or may not be known to the analyst. 
We also allow for the realization of  to be degenerate, i.e., a certain action may be chosen with probability  at a point in time.\\




\textbf{Examples.} This DGP allows for many popular bandit algorithms, as the following examples illustrate. 
In each of the examples below, the contextual bandit algorithm  is degenerate and produces a particular probability vector  for sure. 

\begin{example}[Random A/B testing]\label{ex:A/B}
	We always choose each action uniformly at random:  always holds for any  and any .
\end{example}

In the remaining examples, at every batch , the algorithm uses the history of observations from the previous  batches to estimate the mean and the variance of the potential reward under each action conditional on each context:  and .
We denote the estimates using the history up to batch  by  and . 
See \citet{Li2012} and \citet{Dimakopoulou2017} for possible estimators based on generalized linear models and generalized random forest, respectively.
The initial estimates,  and , are set to any values. 


\begin{example}[-Greedy]\label{ex:e-greedy}
	In each round within batch , we choose the best action based on  with probability  and choose the other actions uniformly at random with probability :
	
\end{example}



\begin{example}[Thompson Sampling using Gaussian priors]\label{ex:Thompson}
	In each round within batch , we sample the potential reward  from distribution  for each action, and choose the action with the highest sampled potential reward, .
	As a result, this algorithm chooses actions with the following probabilities:
	
	where , , and
	
\end{example}

In Examples \ref{ex:e-greedy} and \ref{ex:Thompson},  depends on the random realization of the estimates  and , and so does the associated .
If the data are sufficiently large, the uncertainty in the estimates vanishes:  and  converge to  and , respectively. 
In this case,  becomes nonrandom since it depends on the fixed realizations  and .
In the following analysis, we consider this large-sample scenario and assume that  is nonrandom.


To make the notation simpler, we put  together into a single distribution  obtained by  for each .
We use this to rewrite our DGP as follows:

\begin{itemize}
	\item In each round ,  is i.i.d. drawn from distribution .
	Given , probability vector  is drawn from . 
	Action is randomly chosen based on probability vector , creating the action choice  and the associated reward . 
\end{itemize}

Define

for each , and let .
This is the choice probability vector conditional on each context.
We call  the {\it logging policy} or the {\it propensity score}.

 is common for all rounds regardless of the batch to which they belong. 
Thus  and  are i.i.d. across rounds.
Because  is i.i.d. and , each observation  is i.i.d..
Note also that  is independent of  conditional on .
We use  to denote a random vector that has the same distribution as .



\subsection{Parameters of Interest}



We are interested in using the log data to estimate the expected reward from any given {\it counterfactual policy} , which chooses a distribution of actions given each context:

where the last equality uses the independence of  and  conditional on  and the definition of .
Here, . 
We allow the counterfactual policy  to be degenerate, i.e.,  may choose a particular action with probability 1. 

Depending on the choice of ,  represents a variety of parameters of interest.
When we set  for a particular action  and  for all  for all ,  equals , the expected reward from action .
When we set ,  and  for all  for all ,  equals , the average treatment effect of action  over action .
Such treatment effects are of scientific and policy interests in medical and social sciences.  
Business and managerial interests also motivate treatment effect estimation. 
For example, when a company implements a bandit algorithm using a particular reward measure like an immediate purchase, the company is often interested in treatment effects on other outcomes like long-term user retention.


\section{Efficient Value Estimation}\label{estimation}

We consider the efficient estimation of the expected reward from a counterfactual policy, . 
We consider an estimator consisting of two steps.
In the first step, we nonparametrically estimate the propensity score vector  by a consistent estimator.
Possible estimators include machine learning algorithms such as gradient boosting, as well as nonparametric sieve estimators and kernel regression estimators, as detailed in Section \ref{section:pscore}.
In the second step, we plug the estimated propensity  into the sample analogue of expression (\ref{eq:V^pi}) to estimate  (in practice, some trimming or thresholding may be desirable for numerical stability):
 
Alternatively, one can use a ``self-normalized" estimator inspired by \citet{Swaminathan2015b} when  for all :

\citet{Swaminathan2015b} suggest that  tends to be less biased than  in small sample. 
Unlike \citet{Swaminathan2015b}, however, we use the estimated propensity score rather than the true one.


The above estimators estimate a scalar parameter  defined as a function of the distribution of , on which we impose no parametric assumption. 
Our estimators therefore attempt to solve a semiparametric estimation problem, i.e., a partly-parametric and partly-nonparametric estimation problem.
For this semiparametric estimation problem, we first derive the \textit{semiparametric efficiency bound} on how efficient and precise the estimation of the parameter can be, which is a semiparametric analog of the Cramer-Rao bound \citep{Bickel1993}.
The asymptotic variance of any asymptotically normal estimator is no smaller than the semiparametric efficiency bound. 
Following the standard statistics terminology, we say that estimator  for parameter  is \textit{asymptotically normal} if  as , where  denotes convergence in distribution, and  denotes a normally distributed random variable with mean  and variance .
We call  the \textit{asymptotic variance} of .
The semiparametric efficiency bound for  is a lower bound on the asymptotic variance of asymptotically normal estimators; Appendix \ref{SEB} provides a formal definition of the semiparametric efficiency bound. 












We show the above estimators achieve the semiparametric efficiency bound, i.e., they minimize the asymptotic variance among all asymptotically normal estimators. Our analysis uses a couple of regularity conditions. 
We first assume that the logging policy  ex ante chooses every action with a positive probability for every context.


\begin{assumption}\label{nondegeneracy}
	There exists some  such that  for any  and for .
\end{assumption}

\noindent Note that Assumption \ref{nondegeneracy} is consistent with the possibility that the realization of  takes on value  or  (as long as it takes on positive values with a positive probability).

We also assume the existence of finite second moments of potential rewards. 

\begin{assumption}\label{finite_variance}
	 for .
\end{assumption}

The following proposition provides the semiparametric efficiency bound for . 
All the proofs are in Appendix \ref{proofs}. 

\begin{lemma}[Semiparametric Efficiency Bound]\label{prop:bound:stationary}
	Under Assumptions \ref{nondegeneracy} and \ref{finite_variance}, the semiparametric efficiency bound for , the expected reward from counterfactual policy , is
	
	where  is the expected reward from policy  conditional on .
\end{lemma}

Lemma \ref{prop:bound:stationary} implies the semiparametric efficiency bounds for the expected reward from each action and for the average treatment effect, since they are special cases of .

\begin{corollary}\label{corollary:bound:action}
	Suppose that Assumptions \ref{nondegeneracy} and \ref{finite_variance} hold.
	Then, the semiparametric efficiency bound for the expected reward from each action, , is
	
	The semiparametric efficiency bound for the average treatment effect, , is
	
\end{corollary}

Our proposed estimators are two-step generalized-method-of-moment estimators and are asymptotically normal under some regularity conditions, one of which requires that the convergence rate of  be faster than  \cite{Newey1994,chen2007large}.
Given the asympotic normality of the estimators, we find that they achieve the semiparametric efficiency bound, building upon \citet{Ackerberg2014} among others.

\begin{theorem}[Efficient Estimators]\label{prop:estimator:stationary}
	Suppose that Assumptions \ref{nondegeneracy} and \ref{finite_variance} hold and that  is a consistent estimator for .
	Then, the variance of  and  achieves the semiparametric efficiency bound for  (provided in Lemma \ref{prop:bound:stationary}).
\end{theorem}



\subsection{Inefficient Value Estimation}

In some environments, we know the true  or observe the realization of the probability vectors .
In this case, an alternative way to estimate  is to use the sample analogue of the expression (\ref{eq:V^pi}) without estimating the propensity score. 
If we know , a possible estimator is

If we observe the realization of , we may use

When  for all , it is again possible to use their self-normalized versions: 



These intuitive estimators turn out to be less efficient than the estimators with the estimated propensity score, as the following result shows.

\begin{theorem}[Inefficient Estimators]\label{prop:true_pscore:stationary}
	Suppose that the propensity score  is known and we observe the realization of .
	Suppose also that Assumptions \ref{nondegeneracy} and \ref{finite_variance} hold and that  is a consistent estimator for .
	Then, the asymptotic variances of  and  are no smaller than that of  and . 
	Generically,  and  are strictly less efficient than  and  in the following sense.
	\begin{enumerate}
	\item If , then the asymptotic variances of , ,  and  are strictly larger than that of  and .
	\item If , then the asymptotic variance of  and  is strictly larger than that of  and .
	\end{enumerate}
\end{theorem}

\noindent The condition in Part 1 of Theorem \ref{prop:true_pscore:stationary} is about the dominating term in the difference between  and .
The proofs of Theorems \ref{prop:estimator:stationary} and \ref{prop:true_pscore:stationary} show that the asymptotic variance of  is the asymptotic variance of

Part 1 of Theorem \ref{prop:true_pscore:stationary} requires that the second term be not always zero so that the asymptotic variance of  is different from that of . As long as the two variances are not the same,  achieves variance reduction.

Part 2 of Theorem \ref{prop:true_pscore:stationary} requires that  with a positive probability.
This means that  is not always the same as the true propensity score , i.e.,  is not degenerate (recall that  is drawn from  whose expected value is ).
Under this condition,  has a strictly larger asymptotic variance than  and .

Theorems \ref{prop:bound:stationary} and \ref{prop:true_pscore:stationary} suggest that we should use an estimated score regardless of whether the propensity score is known. To develop some intuition for this result, consider a simple situation where the context  always takes some constant value .
Suppose that we are interested in estimation of the expected reward from action , .
Since  is constant across rounds, a natural nonparametric estimator for  is the proportion of rounds in which action  was chosen: .
The estimator using the estimated propensity score is

The estimator using the true propensity score is

When action  happens to be chosen frequently in a sample so that  is larger, the absolute value of  tends to be larger in the sample.
Because of this positive correlation between  and the absolute value of ,  has a smaller variance than , which produces no correlation between the numerator and the denominator.
Similar intuition applies to the comparison between  and . 



\subsection{How to Estimate Propensity Scores?}\label{section:pscore}
There are several options for the first step estimation of the propensity score.
\begin{enumerate}
	\item A sieve Least Squares (LS) estimator:
	
	where  and  as . 
	Here  is some known basis functions defined on  and .
	\item A sieve Logit Maximum Likelihood estimator:
	
	where
	.
	Here  and  is the set of some basis functions.
	\item Prediction of  by  using a modern machine learning algorithm like random forest, ridge logistic regression, and gradient boosting. 
\end{enumerate}

The above estimators are known to satisfy consistency with a convergence rate faster than  under regularity conditions \citep{Newey1997,cattaneo2010efficient,knight2000asymptotics,blanchard2003rate,buhlmann2011statistics,wager2017estimation}. 

How should one choose a propensity score estimator?
We prefer an estimated score to the true one because it corrects the discrepancy between the realized action assignment in the data and the assignment predicted by the true score.
To achieve this goal, a good propensity score estimator should fit the data better than the true one, which means that the estimator should overfit to some extent.
As a concrete example, in our empirical analysis, random forest produces a larger (worse) variance than gradient boosting and ridge logistic regression (see Figure \ref{figure} and Table \ref{table}).
This is because random forest fits the data worse, which is due to its bagging aspect preventing random forest from overfitting.
In general, however, we do not know which propensity score estimator achieves the best degree of overfitting.
We would therefore suggest that the analyst try different estimators to determine which one is most efficient.




\section{Estimating Asymptotic Variance}\label{section:variance}
We often need to estimate the asymptotic variance of the above estimators. 
For example, variance estimation is crucial for determining whether a counterfactual policy is statistically significantly better than the logging policy. 
We propose an estimator that uses the sample analogue of an expression of the asymptotic variance.
As shown in the proof of Theorem \ref{prop:estimator:stationary}, the asymptotic variance of  and  is , where  such that  for each  and ,

and


We estimate this asymptotic variance in two steps.
In the first step, we obtain estimates of  and  using the method in Section \ref{estimation}.
In addition, we estimate  by nonparametric regression of  on  using the subsample with  for each .
Denote the estimate by .
For this regression, one may use a sieve Least Squares estimator and machine learning algorithms.
In the empirical application below, we use ridge logistic regression.

In the second step, we plug the estimates of ,  and  into the sample analogue of  to estimate the asymptotic variance: when we use :

When we use , its asymptotic variance estimator is obtained by replacing  with  in the above expression.


This asymptotic variance estimator is a two-step generalized-method-of-moment estimator, and is shown to be a consistent estimator under the condition that the first step estimator of  is consistent and some regularity conditions \cite{Newey1994}.

It is easier to estimate the asymptotic variance of  and  with the true propensity score. 
Their asymptotic variance is  by the standard central limit theorem.
When we use , we estimate this asymptotic variance by

When we use , its asymptotic variance estimator is obtained by replacing  with  in the above expression.




\section{Real-World Application}

\begin{table*}[htb]\centering
	\begin{threeparttable}
		\begin{tabular}{l |cc | cc}\hline
&\multicolumn{2}{c|}{Existing Logging Policy} & \multicolumn{2}{c}{Policy of Choosing Best Action by Context}\-0.3em]
			True Score (Benchmark) & &  &  &  \0.2cm]
			Ridge Logistic Regression & &  & & \0.2cm] \hline
			Sample Size & \multicolumn{4}{c}{}\\\hline
		\end{tabular}
		\caption{Improving Ad Design with Lower Uncertainty}\label{table}\par
		~\par
		\fontsize{9.0pt}{10.0pt}\selectfont \textit{Notes}: The first and third columns of this table show 95\% confidence intervals of the expected CTRs  of the logging policy and a hypothetical policy of choosing the best action (ad) for each context. 
			CTRs are multiplied by a constant for confidentiality reasons. 
			We obtain the CTR estimates by the self-normalized inverse probability weighting estimator  using the true propensity score \citep{Swaminathan2015b} or the estimated propensity score ( in Section \ref{estimation}). 
			We estimate standard errors and confidence intervals based on the method described in Section \ref{section:variance}. 
			The second and fourth columns show the size of reductions in confidence interval length, i.e., value  such that the length of the confidence interval is equal to  of the length of the confidence interval using the true propensity score.\\
	\end{threeparttable}
\end{table*}

We apply our estimators described in Sections \ref{estimation} and \ref{section:variance} to empirically evaluate and optimize the design of online advertisements. 
This application uses proprietary data provided by CyberAgent Inc., which we described in the introduction. 
This company uses a contextual bandit algorithm to determine the visual design of advertisements assigned to user impressions (there are four design choices). 
This algorithm produces logged bandit data. 
We use this logged bandit data and our estimators to improve their advertisement design for maximizing the click through rates (CTR). 
In the notation of our theoretical framework, reward  is a click, action  is one of the four possible individual advertisement designs, and context  is user and ad characteristics used by the company's logging policy. 

The logging policy (the company's existing contextual bandit algorithm) works as follows. 
For each round, the logging policy first randomly samples each action's predicted reward from a beta distribution. 
This beta distribution is parametrized by the predicted CTR for each context, where the CTR prediction is based on a Factorization Machine \citep{rendle2010factorization}. 
The logging policy then chooses the action (advertisement) with the largest sampled reward prediction. 
The logging policy and the underlying CTR prediction stay the same for all rounds in each day. 
Each day therefore performs the role of a batch in the model in Section 2. 
This somewhat nonstandard logging policy and the resulting log data are an example of our DGP in Section \ref{model}. 

This logging policy may have room for improvement for several reasons. 
First, the logging policy randomly samples advertisements and does not necessarily choose the advertisement with the best predicted CTR. 
Also, the logging policy uses a predictive Factorization Machine for its CTR prediction, which may be different from the causal CTR (the causal effect of each advertisement on the probability of a click). 

To improve on the logging policy, we first estimate the propensity score by random forest, ridge logistic regression, or gradient boosting (implemented by XGBoost). 
These estimators are known to satisfy the regularity conditions (e.g. consistency) required for our theoretical results, as explained in Section \ref{section:pscore}. 

With the estimated propensity score, we then use our estimator  to estimate the expected reward from two possible policies: 
(1) the logging policy and (2) a counterfactual policy that chooses the best action (advertisement) that is predicted to maximize the CTR conditional on each context. 
To implement this counterfactual policy, we estimate  by ridge logistic regression for each action  and context  used by the logging policy (we apply one-hot encoding to categorical variables in ). 
Given each context , the counterfactual policy then chooses the action with the highest estimated value of .

Importantly, we use separate data sets for the two estimation tasks (one for the best actions and the other for the expected reward from the hypothetical policy). 
Specifically, we use data logged during April 20-26, 2018 for estimating the best actions and data during April 27-29 for estimating the expected reward. 
This data separation allows us to avoid overfitting and overestimation of the CTR gains from the counterfactual policy. 

As a benchmark, we also estimate the same expected rewards based on \citet{Swaminathan2015b}'s self-normalized estimator , which uses the true propensity score. 
The resulting estimates show the following result: 

\begin{quote}
	\textbf{Empirical Result A.} Consistent with Theorems \ref{prop:estimator:stationary}-\ref{prop:true_pscore:stationary}, our estimator  with the estimated score is statistically more efficient than the benchmark  with the true score. 
\end{quote}

This result is reported in Figure \ref{figure} and Table \ref{table}, where the confidence intervals about the predicted CTR using ``True Propensity Score (Benchmark)" are less precise (wider) than those using estimated propensity scores (regardless of which one of the three score estimators to use). 
The magnitude of this shrinkage in the confidence intervals and standard errors is 6-34\%, depending on how to estimate the propensity score. 

This variance reduction allows us to conclude that the logging policy is below the lower bound of the confidence interval of the hypothetical policy, giving us confidence in the following implication: 

\begin{quote}
	\textbf{Empirical Result B.} Compared to the logging policy, the hypothetical policy (choosing the best advertisement given each context) improves the CTR by 10-15\% statistically significantly at the 5\% significance level. 
\end{quote}

\section{Conclusion} 

We have investigated the most statistically efficient use of batch bandit data for estimating the expected reward from a counterfactual policy. 
Our estimators minimize the asymptotic variance among all asymptotically normal estimators (Theorem \ref{prop:estimator:stationary}). 
By contrast, standard estimators have larger asymptotic variances (Theorem \ref{prop:true_pscore:stationary}). 

We have also applied our estimators to improve online advertisement design. 
Compared to the frontier benchmark , our reward estimator  provides the company with more statistical confidence in how to improve on its existing bandit algorithm (Empirical Results A and B). 
The hypothetical policy of choosing the best advertisement given user characteristics would improve the click through rate by 10-15\% at the 5\% significance level. 
These empirical results thus highlight the practical values of Theorems \ref{prop:estimator:stationary}-\ref{prop:true_pscore:stationary}. \\

\noindent \textbf{Acknowledgments.} 
We are grateful to seminar participants at ICML/IJCAI/AAMAS Workshop ``Machine Learning for Causal Inference, Counterfactual Prediction, and Autonomous Action (CausalML)" and RIKEN Center for Advanced Intelligence Project, especially Junya Honda, Masaaki Imaizumi, Atsushi Iwasaki, Kohei Kawaguchi, and Junpei Komiyama. 



\bibliography{aaai2019-paper}
\bibliographystyle{aaai}

\appendix

\section*{Appendices}

\section{Defining Semiparametric Efficiency Bound}\label{SEB}
We present the definition of semiparametric efficiency bound based on \citet{Bickel1993}.
Let  be an i.i.d. sample from the probability distribution  on , where  is some Euclidean sample space and  is its Borel -field.
Let  be a fixed -finite measure on , and let  be the collection of all probability measures dominated by .
Consider a subset  of  such that , and a parameter .

We first define a {\it regular parametric model}.
Consider a subset  of  that has a parametrization  such that

where  is a subset of .
Let , a density of , and .
In the following,  is the Hilbert space of -square integrable functions,  is the Euclidean norm, and  is the Hilbert norm in : .

\begin{definition}[Definition 2.1.1 in \citet{Bickel1993}]
	 is a {\it regular point} of the parametrization  if  is an interior point of , and
	\renewcommand\labelenumi{(\roman{enumi})}
	\begin{enumerate}
		\setlength{\itemindent}{10pt}
		\item The map  from  to  is Fr\'echet differentiable at : there exists a vector  such that
		
		\item The  matrix  is nonsingular.
	\end{enumerate}
\end{definition}

\begin{definition}[Definition 2.1.2 in \citet{Bickel1993}]
	A parametrization  is {\it regular} if:
	\renewcommand\labelenumi{(\roman{enumi})}
	\begin{enumerate}
		\setlength{\itemindent}{10pt}
		\item Every point of  is regular.
		\item The map  is continuous from  to  for .
	\end{enumerate}
\end{definition}
\noindent
We call  a {\it regular parametric model} if it has a regular parametrization.

Now let .
Fix  and suppose  has a  total differential vector  at .
Define

where

Suppose that there exists a regular parametric model  that contains .

\begin{definition}
	The {\it semiparametric efficiency bound} for  is defined by
	
\end{definition}


\section{Proofs}\label{proofs}
\noindent \textit{Proof of Lemma \ref{prop:bound:stationary}.}
The derivation of the semiparametric efficiency bound follows the approach of \citet{Hahn1998}, \citet{HIR2003}, \citet{chen2008semiparametric}, \citet{cattaneo2010efficient} and \citet{Newey1990}.
The proof proceeds in four steps: (i) characterize the tangent set for all regular parametric submodels, (ii) verify that the parameter of interest is pathwise differentiable, (iii) verify that the efficient influence function lies in the tangent set, and (iv) calculate the expected square of the influence function.

Consider a regular parametric submodel of the joint distribution of  with parameter  and the likelihood given by

where  is the conditional density of  given , , and  is the density of .
The log-likelihood function is


The corresponding score is

where , , and .
The tangent set of this model is therefore given by

where  for all  and ,  for all , and .

Now let  be our parameter of interest as a function of :

Differentiation of this under the integral gives

where .

 is pathwise differentiable if there exists a function  such that  and for all regular parametric submodels

Let

We first verify that .
Since  by the independence of  and  conditional on , we have that  for all . Also,  because  for any .
It then follows that

where for the last equality, we use the independence of  and  conditional on .
Under Assumptions \ref{nondegeneracy} and \ref{finite_variance}, .

We next verify that equation (\ref{eq:pathwise_dif}) holds.
The RHS is

where the first equality holds because  for , , and , the second equality holds because , and , and the last equality holds because  and .
We therefore conclude that  is pathwise differentiable.

Finally, we can verify that , since  is the score of a parametric submodel such that , , and  for all  and .

By Theorem 3.1 of \citet{Newey1990}, the semiparametric efficiency bound is the expected square of the projection of  on .
Since , the projection on  is itself, and the semiparametric efficiency bound is .
\qed
\par
\bigskip
\noindent
\noindent \textit{Proof of Theorem \ref{prop:estimator:stationary}.}
We first show that  achieves the semiparametric efficiency bound.
Let  denote a candidate propensity vector, and let .
Let  and  be the following scalar valued function and  vector valued function:

and

where .
Then,  is identified by the unconditional moment restriction

and  is identified by the conditional moment restriction

In addition,  is characterized by the solution to

where  is a nonparametric consistent estimator.
We have that


We use \citet{Ackerberg2014}'s results to show that the asymptotic variance of  is equal to the semiparametric efficiency bound given by Lemma \ref{prop:bound:stationary}.
Let

Here, the -th element of  is given by

for , where , and the second equality follows from the fact that  and  are independent conditional on .
Let , where .
We have that , where  is an  identity matrix.

Condition 1.(i)-(iii) of \citet{Ackerberg2014} trivially hold.
To see that Condition 1.(iv) holds, note that

for , where  is a linear subspace of the space of -dimensional square integrable functions with respect to .
 is a linear functional on .
We have

where , the first equality uses the linearity of , the second equality uses the definition of , the first inequality uses the triangle inequality, the second inequality uses the Cauchy-Schwarz inequality, and the last inequality holds because  for all  and all .
Under Assumptions \ref{nondegeneracy} and \ref{finite_variance},  for all , and thus  is a bounded linear functional.



Now note that the function  depends on  only through .
By Lemma 2 and Proposition 1 of \citet{Ackerberg2014}, the asymptotic variance of  is equal to , where

By the moment restrictions (\ref{eq:moment_g}) and (\ref{eq:moment_rho}), .
It then follows that

where we have shown the last equality in the proof of Lemma \ref{prop:bound:stationary}.

To show that  has the same asymptotic variance as , it suffices to show that the denominator of , , converges in probability to one.
Denote this denominator by .
Let  be the following scalar valued function:

Also, let  be the solution to the following moment condition:

i.e., .
 is characterized by the solution to

 is a semiparametric two-step GMM estimator, and it is shown that  is consistent for  under the condition that  is a consistent estimator for  and regularity conditions \cite{Newey1994}.
Since ,  converges in probability to one.
\qed
\par
\noindent
\noindent \textit{Proof of Theorem \ref{prop:true_pscore:stationary}.}
We use the same notations as those in the proof of Theorem \ref{prop:estimator:stationary}.
We first compare  to .
We have that

By the central limit theorem, the asymptotic variance of  is .
We compare this to .
From the proof of Theorem \ref{prop:estimator:stationary}, we can write:

It follows that

The first term equals

The second term equals

where the last equality uses the facts that  and that .
Therefore,

which is nonnegative.

Since , this is equal to

where we use the definition of expectation for the last equality.
Under Assumption \ref{nondegeneracy}, this is greater than zero if .

We next compare  to .
Recall that  is the probability vector indicating the probability that each action is chosen in round  and that  is a probability vector that has the same distribution as .
We have that

It follows that

where the second equality uses  and , the third equality uses , and the last equality uses .
By the central limit theorem, the asymptotic variance of  is

The difference between the asymptotic variance of  and that of  is

The second term equals

where the second equality uses  for , the third equality uses  and , and the last equality uses .
The first term is nonnegative, and equals

where the first line uses  for , the first equality uses  and , and the last equality uses .
By Jensen's inequality, .
Expression (\ref{eq:varddot}) is greater than zero if .

To show that  and  have the same asymptotic variance as  and , respectively, it suffices to show that the denominators of  and  converge in probability to one.
We have that

and that

The law of large numbers implies the convergence in probability of  and  to one.
\qed

\end{document}