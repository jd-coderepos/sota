\documentclass[11pt,letterpaper]{article}
\usepackage{typearea}
\typearea{15}

\newif\iffullver
\fullverfalse
\fullvertrue 


\newcommand{\full}[1]{\iffullver#1\fi}
\newcommand{\short}[1]{\iffullver\else#1\fi}
\newcommand{\defertext}{(Proof in full version.)}
\newcommand{\defer}[2][(Proof in full version.)]{\full{#2}\short{#1}}
\newcommand{\tfv}[1]{\short{the full version}\full{#1}}

\usepackage{theorem,latexsym,graphicx}
\usepackage{amsmath,amssymb,enumerate}
\usepackage{xspace}
\usepackage{bm,fullpage}
\usepackage{ifpdf}
\usepackage{shadow,shadethm,color}
\usepackage[compact]{titlesec}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage{times}
\usepackage{paralist}

\definecolor{Darkblue}{rgb}{0,0,0.4}
\definecolor{Brown}{cmyk}{0,0.81,1.,0.60}
\definecolor{Purple}{cmyk}{0.45,0.86,0,0}
\newcommand{\mydriver}{hypertex}
\ifpdf
 \renewcommand{\mydriver}{pdftex}
\fi
\usepackage[breaklinks,\mydriver]{hyperref}
\hypersetup{colorlinks=true,citebordercolor={.6 .6 .6},linkbordercolor={.6 .6 .6},citecolor=Darkblue,urlcolor=black,linkcolor=Darkblue,pagecolor=black}
\newcommand{\lref}[2][]{\hyperref[#2]{#1~\ref*{#2}}}

\makeatletter
 \setlength{\parindent}{0pt}
 \addtolength{\partopsep}{-2mm}
 \setlength{\parskip}{5pt plus 1pt}
 \addtolength{\theorempreskipamount}{-1mm}
 \addtolength{\theorempostskipamount}{-1mm}
 \addtolength{\abovedisplayskip}{-3mm}
 \addtolength{\textheight}{45pt}
 \addtolength{\footskip}{-10pt}
\makeatother

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newshadetheorem{lemmashaded}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}[theorem]{Example}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{reduction}[theorem]{Reduction}
\newtheorem{invariant}{Invariant}
\newtheorem{extension}[theorem]{Extension}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\numberwithin{algorithm}{section}

\newenvironment{proof}{

\noindent{\bf Proof:}}
{\hfill


}




\newenvironment{proofof}[1]{

\noindent{\bf Proof of {#1}:}}
{\hfill


}

\newcommand{\junk}[1]{}
\newcommand{\ignore}[1]{}

\newcommand{\R}[0]{{\ensuremath{\mathbb{R}}}}
\newcommand{\Z}[0]{{\ensuremath{\mathbb{Z}}}}

\newcommand{\A}[0]{{\ensuremath{\mathcal{A}}}\xspace}
\newcommand{\lm}[0]{{\ensuremath{\mathcal{L}}}\xspace}
\newcommand{\N}[0]{{\ensuremath{\mathcal{N}}}\xspace}


\def\floor#1{\lfloor #1 \rfloor}
\def\ceil#1{\lceil #1 \rceil}
\def\seq#1{\langle #1 \rangle}
\def\set#1{\{ #1 \}}
\def\abs#1{\mathopen| #1 \mathclose|}   \def\norm#1{\mathopen\| #1 \mathclose\|}

\newcommand{\poly}{\operatorname{poly}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\polylog}{\operatorname{polylog}}

\newcommand{\sse}{\subseteq}
\newcommand{\I}{{\mathcal{I}}}
\newcommand{\Larg}{{\mathcal{L}}}
\newcommand{\Smal}{{\mathcal{S}}}
\newcommand{\rko}{\widehat{r}}
\newcommand{\sko}{\widehat{s}}
\newcommand{\Si}{{\mathcal{S}_i}}
\newcommand{\B}{{\mathcal{B}}}
\newcommand{\T}{\ensuremath{\mathcal{T}}\xspace}
\newcommand{\Ti}{{\mathcal{T}_i}}
\newcommand{\tend}{t_{end}}
\newcommand{\nbands}{\lceil \log B \rceil}
\newcommand{\Problem}{{\mathcal{P}}}
\newcommand{\e}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\newcommand{\ts}{\textstyle}

\newcommand{\ALG}{\ensuremath{{\sf alg}}\xspace}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\pr}[1]{{\rm Pr} \left[ #1 \right]}
\newcommand{\ex}[1]{{\rm E} \left[ #1 \right]}

\newcommand{\opt}[1]{\ensuremath{\mathsf{Opt(#1)}}\xspace}
\newcommand{\tmean}[2]{\ensuremath{\mu_{#1}\left({#2}\right)}\xspace}
\newcommand{\Opt}{\ensuremath{\mathsf{Opt}}\xspace}
\newcommand{\algko}{\ensuremath{\mathsf{AlgKO}}\xspace}
\newcommand{\algso}{\ensuremath{\mathsf{AlgSO}}\xspace}
\newcommand{\algdo}{\ensuremath{\mathsf{AlgDO}}\xspace}
\newcommand{\Opti}{\ensuremath{\mathsf{Opt(i)}}\xspace}
\newcommand{\LPOpt}{\ensuremath{\mathsf{LPOpt}\xspace}}
\newcommand{\LPLargeOpt}{\ensuremath{\mathsf{LP2Opt}\xspace}}

\newcommand{\degree}{\ensuremath{\mathsf{degree}}\xspace}
\newcommand{\cost}{\ensuremath{\mathsf{cost}}\xspace}
\newcommand{\alg}{\ensuremath{\mathsf{Alg}}\xspace}
\newcommand{\approxalg}{\ensuremath{\mathsf{ApproxAlg}\xspace}}

\newcommand{\so}{\ensuremath{\mathsf{StocOrient}}\xspace}
\newcommand{\ko}{\ensuremath{\mathsf{KnapOrient}}\xspace}
\newcommand{\co}{\ensuremath{\mathsf{CorrOrient}}\xspace}
\newcommand{\kdo}{\ensuremath{\mathsf{KDO}}\xspace}
\newcommand{\ddo}{\ensuremath{\mathsf{DeadlineOrient}}\xspace}
\newcommand{\lp}{\ensuremath{\mathsf{LP}}\xspace}
\newcommand{\dlp}{\ensuremath{\mathsf{DLP}}\xspace}

\newcommand{\kb}{{W}}
\newcommand{\waitbud}{{W}}
\newcommand{\size}{{S}}

\newcommand{\depth}{\mathsf{depth}}
\newcommand{\level}{\mathsf{level}}


\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\F}{{\ensuremath{\mathcal{F}}}}

\newcommand{\q}[1]{e\left(#1\right)}

\newcounter{myLISTctr}
\newcommand{\initOneLiners}{\setlength{\itemsep}{0pt}
    \setlength{\parsep }{0pt}
    \setlength{\topsep }{0pt}
}
\newenvironment{OneLiners}[1][\ensuremath{\bullet}]
    {\begin{list}
        {#1}
        {\initOneLiners}}
    {\end{list}}

\newenvironment{MyEqn}[1]{\setlength\arraycolsep{2pt}}






\begin{document}

\title{On the Adaptivity Gap of Stochastic Orienteering}
\author{
Nikhil Bansal\thanks{Eindhoven University of Technology. Supported by the Dutch NWO Grant 639.022.211.}
\and
Viswanath Nagarajan\thanks{IBM T.J. Watson Research Center}
}
\date{}

\maketitle

\begin{abstract}
The input to the {\em stochastic orienteering} problem~\cite{GKNR12} consists of a budget  and metric  where each vertex  has a job with a deterministic reward and a \emph{random} processing time (drawn from a known distribution). The processing times are independent across vertices. The goal is to obtain a non-anticipatory policy  (originating from a given root vertex)
to run jobs at different vertices, that maximizes expected reward, subject to the total distance traveled plus processing times being at most . An {\em adaptive} policy is one that can choose the next vertex to visit based on observed random instantiations. Whereas, a {\em non-adaptive} policy is just given by a fixed ordering of vertices.
The {\em adaptivity gap} is the worst-case ratio of the expected rewards of the optimal adaptive and non-adaptive policies.

\smallskip

We prove an  lower bound on the adaptivity gap of stochastic orienteering. This provides a negative answer to the -adaptivity gap conjectured in~\cite{GKNR12}, and comes close to the  upper bound proved there. This result holds even on a line metric.


\smallskip

We also show an  upper bound on the adaptivity gap for the {\em correlated} stochastic orienteering problem, where the reward of each job is random and possibly correlated to its processing time. Using this, we obtain an improved quasi-polynomial time -approximation algorithm for correlated stochastic orienteering.

\end{abstract}

\section{Introduction}

In the {\em orienteering} problem~\cite{GLV87}, we are given a metric  with a starting vertex  and a budget  on length. The objective is to compute a path originating from  having length at most , that maximizes the number of vertices visited. This is a basic vehicle routing problem (VRP) that arises as a subroutine in algorithms for a number of more complex variants, such as VRP with
time-windows, discounted reward TSP and distance constrained VRP.


The stochastic variants of orienteering and related problems such as traveling salesperson and vehicle routing have also been extensively studied. In particular, several dozen variants have been considered depending on which parameters are stochastic, the choice of the objective function, the probability distributions, and optimization models such as {\em a priori} optimization, stochastic optimization with recourse, probabilistic settings and so on.
For more details we refer to a recent survey \cite{Weyland} and references therein.

Here, we consider the following stochastic version of the orienteering problem defined by~\cite{GKNR12}. Each vertex contains a job with a deterministic reward and random processing time (also referred to as size); these processing times are independent across vertices. The processing times  model the random delays encountered at the node, say due to long queues or activities such as filling out a form, before the reward can be collected. The distances in the metric correspond to travel times between vertices, which are deterministic. The goal is to compute a {\em policy}, which describes a path originating from the root  that visits vertices and runs the respective jobs, so as to maximize the total expected reward subject to the total time (for travel plus processing) being at most .
Stochastic orienteering also generalizes the well-studied stochastic knapsack problem~\cite{DeanGV08,BGK11,Bhalgat11} (when all distances are zero).
We also consider a further generalization, where the reward at each vertex is also random and possibly {\em correlated} to its processing time. 



A feasible solution (policy) for the stochastic orienteering problem is represented by a decision tree, where nodes encode the ``state'' of the solution (previously visited vertices and the residual budget), and branches denote random instantiations. Such solutions are called {\em adaptive} policies, to emphasize the fact that their actions may depend on previously observed random outcomes.
Often, adaptive policies can be very complex and hard to reason about.
For example, even for the stochastic knapsack problem an optimal adaptive strategy may have exponential size (and
several related problems are PSPACE-hard) \cite{DeanGV08}.



Thus a natural approach for designing algorithms in the stochastic setting is to:
(i) restrict the solution space to the simpler class of {\em non adaptive} policies (eg.~in our stochastic orienteering setting, such a policy is described by a fixed permutation to visit vertices in, until the budget  is exhausted), and (ii) design an efficient algorithm to find a
(close to) optimum non-adaptive policy.


While non-adaptive policies are often easier to optimize over, the drawback is that they could be much worse than the optimum adaptive policy. Thus, a key issue is to bound the {\em adaptivity gap}, introduced by \cite{DeanGV08} in their seminal paper,  which is the worst-case ratio (over all problem instances) of the optimal adaptive value to the optimal non-adaptive value. 

In recent years, increasingly sophisticated techniques have been developed for designing good non-adaptive policies and for proving small adaptivity gaps~\cite{DeanGV08,GuhaM09,chenetal,BGLMNR10,GKMR11,GKNR12}.
For stochastic orienteering, \cite{GKNR12} gave an  bound on the adaptivity gap, using an elegant probabilistic argument (previous approaches only gave a  bound).
More precisely, they considered certain  correlated probabilistic events and used martingale tails bounds on suitably defined stopping times to bound the probability that none of these events happen.
In fact, \cite{GKNR12} conjectured that the adaptivity gap for stochastic orienteering was , suggesting that the  factor was an artifact of their analysis.



\subsection{Our Results and Techniques}


\noindent {\bf Adaptivity gap for stochastic orienteering:}
Our main result is the following lower bound.
\begin{theorem}\label{thm:ad-gap}
The adaptivity gap of stochastic orienteering is , even on a line metric.
\end{theorem}
This answers negatively the -adaptivity gap conjectured in~\cite{GKNR12}, and comes close to the  upper bound proved there. To the best of our knowledge, this gives the first non-trivial  adaptivity gap for a natural problem.


The lower bound proceeds in three steps and is based on a somewhat intricate construction.
 We begin with a basic instance described by a directed binary tree of height  that essentially represents the optimal adaptive policy. Each processing time is a Bernoulli random variable: it is either zero, in which case the optimal policy goes to its left child, or a carefully set positive value, in which case the optimal policy goes to its right child.
The edge distances and processing times are chosen so that when a non-zero size instantiates, it is always possible to take a right edge, while the left edges can only be taken a few times. On the other hand, if the non-adaptive policy chooses a path with mostly right edges, then it cannot collect too much reward.

In the first step of the proof, we show that this directed tree instance has an  adaptivity gap.
The main technical difficulty here is to show that every fixed path (which may possibly skip vertices, and gain advantage over the adaptive policy) either runs out of budget  or collects low expected reward.
In the second step, we drop the directions on the edges and show that the adaptivity gap continues to hold (up to constant factors). The optimum adaptive policy that we compare against remains the same as in the directed case, and the key issue here is to show that
the non-adaptive policy cannot gain too much by backtracking along the edges.
To this end, we use some properties of the distances on edges in our instance.
In the final step, we embed the undirected tree onto a line at the expense of losing another  factor in  the adaptivity gap.
The problem here is that pairs of nodes that are far apart on the tree may be very close on the line.
To get around this, we exploit the asymmetry of the tree distances and some other structural properties to show that this has limited effect.


\medskip

\noindent  {\bf Correlated Stochastic Orienteering:}
Next, we consider the correlated stochastic orienteering problem, where the reward at each vertex is also random and possibly correlated with its processing time (the distributions are still independent across vertices). In this setting, we prove the following.\begin{theorem}\label{thm:corr-loglog-UB}
The adaptivity gap of correlated stochastic orienteering is .
\end{theorem}
 This improves upon the -factor adaptivity gap that is implicit in~\cite{GKNR12}, and matches the adaptivity gap upper bound known for uncorrelated stochastic orienteering. The  proof makes use of a martingale concentration inequality~\cite{Zhang05} (as~\cite{GKNR12} did for the uncorrelated problem), but dealing with the reward-size correlations requires a different definition of the stopping time. For the uncorrelated case, the stopping time~\cite{GKNR12} used a single ``truncation threshold'' (equal to  minus the travel time)  to compare the instantiated sizes and their expectation.
In the correlated setting, we use  different
truncation thresholds (all powers of ), irrespective of the travel time, to determine the stopping criteria.




\medskip

\noindent{\bf Algorithm for Correlated Stochastic Orienteering:}
Using some structural properties in the proof of the adaptivity gap upper bound above, we obtain an improved {\em quasi-polynomial}\footnote{A quasi-polynomial time algorithm runs in  time on inputs of size , where  is some constant.} time algorithm for correlated stochastic orienteering.
\begin{theorem}\label{thm:corr-NA}
There is an -approximation algorithm for correlated stochastic orienteering, running in time . Here  denotes the best approximation ratio for the orienteering with deadlines problem.
\end{theorem}
The {\em orienteering with deadlines} problem is defined formally in Section~\ref{subsec:defn}.
Previously,~\cite{GKNR12} gave a  polynomial time -approximation algorithm for correlated stochastic orienteering. They also showed that this problem is at least as hard to approximate as the deadline orienteering problem, i.e.~an -hardness of approximation (this result also holds for quasi-polynomial time algorithms). Our algorithm improves the approximation ratio to , but at the expense of quasi-polynomial running time. We note that the running time in Theorem~\ref{thm:corr-NA} is quasi-polynomial for general inputs where probability distributions are described {\em explicitly}, since the input size is . If probability distributions are specified implicitly, the runtime is quasi-polynomial only for .


The algorithm in Theorem~\ref{thm:corr-NA} is based on finding an approximate non-adaptive policy, and losing an -factor on top by Theorem~\ref{thm:corr-loglog-UB}. There are three main steps in the algorithm: (i) we enumerate over  many ``portal'' vertices (suitably defined) on the optimal policy; (ii) using these portal vertices, we solve (approximately) a {\em configuration LP relaxation} for paths between portal vertices; (iii) we randomly round the LP solution. The quasi-polynomial running time is only due to the enumeration. In formulating and solving the configuration LP relaxation, we also use some ideas from the earlier -approximation algorithm~\cite{GKNR12}. Solving the configuration LP requires an algorithm for deadline orienteering (as the dual separation oracle), and incurs an -factor loss in the approximation ratio.
This configuration LP is a ``packing linear program'', for which we can use fast combinatorial algorithms~\cite{PST91,GK07}. The final rounding step involves randomized rounding with alteration, and loses an extra  factor.


\subsection{Related Work}
\label{s:prev-work}
The deterministic orienteering problem was introduced by Golden et al.~\cite{GLV87}. It has several applications, and many exact approaches and heuristics have been applied to this problem, see eg. the survey~\cite{VSO11}. The first constant-factor approximation algorithm was due to Blum et al.~\cite{BCKLMM07}. The approximation ratio has been improved~\cite{BBCM04,CKP08} to the current best .



Dean et al.~\cite{DeanGV08} were the first to consider stochastic packing problems in this adaptive optimization framework: they introduced the {\em stochastic knapsack} problem (where items have random sizes), and obtained a constant-factor approximation algorithm and adaptivity gap. The approximation ratio has subsequently been improved to , due to~\cite{BGK11,Bhalgat11}. The stochastic orienteering problem~\cite{GKNR12} is a common generalization of both deterministic orienteering and stochastic knapsack.



Gupta et al.~\cite{GKMR11} studied a generalization of the stochastic knapsack problem, to the setting where the reward and size of each item  may be correlated, and gave an -approximation algorithm and adaptivity gap for this problem.
Recently, Ma~\cite{M14} improved the approximation ratio to .


The correlated stochastic orienteering problem was studied in~\cite{GKNR12}, where the authors obtained an -approximation algorithm and an  adaptivity gap. They also showed the  problem to be at least as hard to approximate as the deadline orienteering problem, for which the best approximation ratio known is ~\cite{BBCM04}.


A related problem to stochastic orienteering was considered by Guha and Munagala~\cite{GuhaM09} in the context of the {\em multi-armed bandit} problem. As observed in~\cite{GKNR12}, the approach in~\cite{GuhaM09} yields an -approximation algorithm (and adaptivity gap) for the variant of stochastic orienteering with two {\em separate} budgets for the travel and processing times. In contrast, our result shows that stochastic orienteering (with a single budget) has super-constant adaptivity gap.

\subsection{Problem Definition}\label{subsec:defn}
An instance of stochastic orienteering (\so) consists of a metric space
 with vertex-set  and symmetric integer distances  (satisfying the
triangle inequality) that represent travel times.  Each vertex  is associated with a stochastic job, with a deterministic
reward  and a random processing time (also called size)  distributed according to a
known probability distribution. The processing times are independent across vertices. We are also given a starting ``root'' vertex , and
a budget  on the total time available. A solution (policy) must start from , and visit a sequence of vertices (possibly adaptively). Each job is executed non-preemptively, and the solution knows the precise processing time only upon completion of the job. The objective is to maximize the expected reward from jobs that are completed before the horizon ; note that there is no reward for partially completing a job.
The approximation ratio of an
algorithm is the ratio of the expected reward of an optimal policy to that of the algorithm's policy.


We assume that all times (travel and processing) are integer valued and lie in .
In the {\em correlated} stochastic orienteering problem (\co), the job sizes and rewards are both random, and correlated with each other.
The distributions across different vertices are still independent.  For each vertex , we use  and  to denote its random size and reward, respectively. We assume an explicit representation of the distribution of each job : for each , job  has size  and reward  with probability . Note that the input size is .

An \emph{adaptive policy} is a decision tree where each node is labeled by a job/vertex of , with the outgoing arcs
from a node labeled by  corresponding to the possible sizes in the support of .  A \emph{non-adaptive
policy} is simply given by a path  starting at : we just traverse this path, processing
the jobs that we encounter, until the total (random) size of the jobs plus the distance traveled reaches . A
randomized non-adaptive policy may pick a path  at random from some distribution before it knows any of the
size instantiations, and then follows this path as above. Note that in a non-adaptive policy, the order in which jobs
are processed is independent of their processing time instantiations.





In our algorithm for \co, we use the {\em deadline orienteering} problem as a subroutine.
The input to this problem is a metric  denoting travel times, a reward and deadline at each vertex, start () and end () vertices, and length bound . The objective is to compute an  path of length at most  that maximizes the reward from vertices visited before their deadlines.
The best approximation ratio for this problem is   due to~\cite{BBCM04,CKP08}.


\subsection{Organization}
The adaptivity gap lower bound appears in Section~\ref{sec:ad-gap}, where we prove Theorem~\ref{thm:ad-gap}. In Section~\ref{sec:corr-ad-gap}, we consider the correlated stochastic orienteering problem and prove the upper bound on its adaptivity gap (Theorem~\ref{thm:corr-loglog-UB}). Finally, the improved quasi-polynomial time algorithm (Theorem~\ref{thm:corr-NA}) for correlated stochastic orienteering appears in Section~\ref{sec:corr-alg}

\section{Lower Bound on the Adaptivity Gap}\label{sec:ad-gap}
Here we describe our lower bound instance which shows that the adaptivity gap is  even for an undirected line metric. The proof and the description of the instance is divided into three steps. First we describe an instance where the underlying graph is a directed complete binary tree, and prove the lower bound for it. The directedness ensures that all policies follow a path from root to a leaf (possibly with some nodes skipped) without any backtracking. Second, we show that the directed assumption can be removed at the expense of an additional  factor in the adaptivity gap. In particular this means that the nodes on the tree can be visited in any order starting from the root. Finally, we ``embed" the undirected tree into a line metric, and show that the adaptivity gap stays the same up to a constant factor.

\subsection{Directed Binary Tree}

Let  be an integer and . We define a complete binary tree \T of height  with root .
All the edges are directed from the root towards the leaves.
The {\em level}  of any node  is the number of nodes on the shortest path from  to any leaf. So all the leaves are at {\em level} one and the root  is at level . We refer to the two children of each internal node as the left and right child, respectively. Each node  of the tree has a job with some deterministic reward  and a random size . Each random variable  is Bernoulli, taking value zero with probability  and some positive value  with the remaining probability . The budget for the instance is .

To complete the description of the instance, we need to define the values of the rewards , the job sizes , and the distances  on edges .

{\bf Defining rewards.} For any node , let  denote the number of right-branches taken on the path from the root to . We define the reward of each node  to be .

{\bf Defining sizes.} Let  for any . The size at the root, . The rest of the sizes are defined recursively. For any non-root node  at level  with  denoting its parent, the size is:


In other words, for a node  at level , consider the path   from  to . Let 
where  if  is the left child of its parent , and  otherwise (we assume ).
Then .

Observe that for a node , each node  in its left (resp. right) subtree has  (resp. ).

It remains to define distances on the edges. This will be done in an indirect way, and it is instructive to first consider the adaptive policy that we will work with. In particular, the distances will be defined in such a way that the adaptive policy can always continue till it reaches a leaf node.



{\bf Adaptive policy \A.} Consider the policy \A  that goes left at node  whenever it observes size zero at , and goes right otherwise.

Clearly, the {\em residual budget}  at node  under   will satisfy the following:
, and


{\bf Defining distances.} We will define the distances so that the residual budgets  under  satisfy the following:
, and for any node  with parent ,


In particular, this implies the following lengths on edges. For any node  with parent ,


In Claim \ref{cl:well-defn} below we will show that the distances are non-negative, and hence well-defined.


Figure \ref{fig:tree-def} gives a pictorial view of the instance.

\begin{figure}[ht]
  \begin{centering}
    \includegraphics[scale=0.8]{tree-defn}
    \caption{The binary tree \T. \label{fig:tree-def}}
  \end{centering}
\end{figure}


\paragraph{Basic properties of the instance}

Let  denote the distance traveled by the adaptive strategy \cal{A} to reach , and let  denote the total size instantiation before reaching . By the definition of the budgets, and as \cal{A} takes the right branch at  iff the size at  instantiates, we have the following.

\begin{claim}
\label{lem:bv}
For any node , the budget  satisfies .
\end{claim}

\begin{claim}
\label{size:val}
If a node  is a left child of its parent, then .
\end{claim}
\begin{proof}
Let  be the parent of .
By definition of sizes, .
As  by the definition of residual budgets, the claim follows.
\end{proof}


\begin{claim}\label{cl:well-defn}
For any node , we have . This implies that all the residual budgets and distances are non-negative.
\end{claim}
\begin{proof}
Let  denote the lowest level node on the path from  to  that is the left child of its parent (if  is the left child of its parent, then ); if there is no such node, set .
Note that by Claim~\ref{size:val} and the definition of  and , in either case it holds that .

Let  denote the path from  to  (including  but not ; so  if ).
Since  contains only right-branches,  and hence . Thus to prove  it  suffices to show . For brevity, let  and . Using the definition of sizes,

as desired.  Here the right hand side of the first inequality is simply the total size of nodes in the  to leaf path using all right branches. The inequality in the second line follows as  for all .

Thus we always have .

As  if  is the right child of , or  otherwise, this implies that all the residual-budgets are non-negative.

Similarly, as  is either  or  (and hence at least ), this implies that all edge lengths are non-negative.
\end{proof}

This claim shows that the above instance is well defined, and that  \A is a feasible adaptive policy that always continues for  steps until it reaches a leaf. Next, we show that \A obtains large expected reward.

\begin{lemma}\label{lem:ad-profit}
The expected reward of policy \A is .
\end{lemma}
\begin{proof}
Notice that \A accrues reward as follows: it keeps getting reward  (and going left) until the first positive size instantiation, then it goes right for a single step and keeps going left and getting reward  till the next positive size instantiation and so on. This continues  for a total of  steps.
In particular, at any time  it collects reward , if exactly  nodes have positive sizes among the  nodes seen.

Let  denote the Bernoulli random variable that is  if the  node in \A has a positive size instantiation, and  otherwise. So , and
. By Markov's inequality, the probability that more than 
nodes in \A have positive sizes is at most half. Hence, with probability at least  the reward collected in the last node of \A is at least . That is, the total expected reward of \A is at least .
\end{proof}

\subsection{Bounding Directed Non-adaptive Policies}\label{subsec:dir-gap}
We will first show that any non-adaptive policy \N
that is constrained to visit vertices according to the partial order given by the tree \T
gets reward at most . 
Notice that these correspond precisely to non-adaptive policies on the directed tree \T.

The key property we need from the size construction is the following.
\begin{lemma}\label{lem:prefix-size}
For any node , the total size instantiation observed under the adaptive policy \A before  is strictly less than .
\end{lemma}
\begin{proof}
Consider the path  from the root to , and let  denote the levels at which  ``turns left''. That is, for each , the node  at level  in path  satisfies (a)  is the right child of its parent, and (b)  contains the left child of  if it goes below level . (If  is the right child of its parent then  and .) Let  denote the size of , the level  node in . Also, set  corresponding to the root. Below we use .

\begin{figure}[ht]
  \begin{centering}
    \includegraphics[scale=0.7]{prefix-size}
    \caption{The path  in proof of Lemma~\ref{lem:prefix-size}.    \label{fig:??} }
  \end{centering}
\end{figure}


We first bounds the size instantiation between levels  and  in terms of .
Observe that a positive size instantiation is seen in \A only along right branches. So for any , the total size instantiation seen in  between levels  and  is at most:


Now, note that for any , the sizes  and  are related as follows:

The first inequality uses the fact that the path from  to  is a sequence of (at least one) left-branches followed
by a sequence of (at least one) right-branches. As the size decreases along left-branches and increases along right branches, it follows that conditional on the values of  and , the  ratio  is maximized for the path with a sequence of left branches followed by a single right branch (at level ).


Using~\eqref{eq:prefix-size2}, we obtain inductively that:



Using~\eqref{eq:prefix-size1} and~\eqref{eq:prefix-size3}, the total size instantiation seen in  (this does not include the size at ) is at most:


Finally, observe that the size at the level  node , since  is the lowest level at which  turns left (i.e.  keeps going left below level  until ). Together with \eqref{eq:prefix-size4}, it follows that the total size instantiation seen before  is strictly less than

This completes the proof of Lemma~\ref{lem:prefix-size}. \end{proof}





We now show that any non-adaptive policy on the directed tree \T achieves reward . Note that any such solution \N is just a root-leaf path in \T that skips some subset of vertices. A node  in \N is an {\em L-branching} node if the path \N goes left after . {\em R-branching} nodes are defined similarly.

\begin{claim}\label{cl:na-Rbranch}
The total reward from R-branching nodes is at most .
\end{claim}
\begin{proof}
As the reward of a node decreases by a factor of  upon taking a right branch,
the total reward of such nodes is at most .
\end{proof}

\begin{claim}\label{cl:na-Lbranch}
\N can not get any reward after two L-branching nodes instantiate to positive sizes.
\end{claim}
\begin{proof}
For any node  in tree \T, let  (resp. ) denote the distance traveled (resp. size instantiated) in the adaptive policy \A until ; here  does not include the size of .  Observe that Lemma~\ref{lem:prefix-size} implies that  for all nodes .

In the non-adaptive solution \N, let  and  be any two L-branching nodes that instantiate to positive sizes  and ; say  appears before . Under this outcome, we will show that \N exhausts its budget after . Note that the distance traveled to node  in \N is exactly , the same as that under \A. So the total distance plus size instantiated in \N is at least , which (as we show next) is more than the budget .

By Claim  \ref{lem:bv},  . Moreover, the residual budget  at the left child  of  equals . Since the residual budgets are non-increasing down the tree \T, we have , i.e. . Hence, the total distance plus size in \N is at least

 where the last inequality follows from Lemma~\ref{lem:prefix-size}. So \N can not obtain reward from any node after .
\end{proof}

Combining the above two claims, we obtain:
\begin{claim}\label{cl:mon-na}
The expected reward of any directed non-adaptive policy is at most .
\end{claim}
\begin{proof}
Using Claim~\ref{cl:na-Lbranch}, the expected reward from L-branching nodes is at most the expected number of L-branching nodes until two positive sizes instantiate, i.e. at most . Claim~\ref{cl:na-Rbranch} implies that the expected reward from R-branching nodes is at most . Adding the two types of rewards, we obtain the claim.
\end{proof}

This proves an  adaptivity gap for stochastic orienteering on directed metrics. We remark that the  upper bound in~\cite{GKNR12} also holds for directed metrics.

\subsection{Adaptivity Gap for Undirected Tree}

We now show that the adaptivity gap does not change much even if we make the edges of the tree undirected.
In particular, this has the effect of allowing the non-adaptive policy \N to backtrack along the (previously directed) edges, and visit any collection of nodes in the tree.
Recall that in the directed instance of the previous subsection, the non-adaptive policy could not try too many -branching nodes (Claim \ref{cl:na-Lbranch}) and hence was forced to choose mostly -branching nodes, in which case the rewards decreased rapidly.
However, in the undirected case, the non-adaptive policy can move along some right edges to collect rewards and then backtrack to high-reward nodes.

The main idea of the analysis below will be to show that  non-adaptive policies cannot gain much more from backtracking (Claims \ref{cl:na-noLback} and \ref{cl:na-back}).

The adaptive policy we compare against is the same as in the directed case. Let  denote some fixed non-adaptive policy.
Using the definition of edge-lengths,
\begin{claim} \label{cl:na-noLback}
 can not backtrack over any left-branching edge.
\end{claim}
\begin{proof}
As in the proof of Claim~\ref{cl:na-Lbranch}, for any , let  (resp. ) denote the distance traveled (resp. size instantiated) in the adaptive policy \A until node ; recall  does not include the size of . If  backtracks over the left-edge  out of some node  then the distance traveled is at least:

The first equality follows as  and the second equality follows as  by Claim 
\ref{lem:bv}.
The first inequality follows as ,  by Lemma~\ref{lem:prefix-size}. The second inequality follows as  by the definition of sizes. Finally, the last inequality follows by Claim~\ref{cl:well-defn}.
Since the distance traveled by \N' is more than , the claim follows.
\end{proof}

We now focus on bounding the contribution due to backtracking on the right edges.

Let  be the left-edges traversed by ; we denote  where  is the left child of . We now partition the nodes visited in  as follows. For each , group  consists of nodes visited after traversing  and before traversing ; and  is the set of nodes visited after . Note that the nodes in  are visited contiguously using only right edges (they need not be visited in the order given by tree \T, as the algorithm may bactrack). See Figure~\ref{fig:back-NA} for a pictorial view.

For each , let  denote the nodes at level more than  (the parent node of left-edge ); and  let . We also set .

By using exactly the argument in Claim~\ref{cl:na-Rbranch}, the total reward in  is at most
.

 Let us modify  by dropping all nodes in .
 Each remaining node  of  is either (i) an L-branching node, where  goes left after  (these are the end-points s of left-edges), or (ii) a ``{\em backtrack} node'' where  backtracks on the edge from  to its parent (these are nodes in s). By Claim~\ref{cl:na-Lbranch}, the expected reward from L-branching nodes is at most . In order to bound the total expected reward, it now suffices to bound the reward from the backtrack nodes.

\begin{claim}\label{cl:na-back}
The expected reward of  from the backtrack nodes is at most 
\end{claim}
\begin{proof}
Consider the partition (defined above) of backtrack nodes of  into groups . Recall that  visits each group  contiguously (perhaps not in the order given by \T) and then traverses left-edge  to go to the next group . Moreover,  (the parent end-point of left-edge ) is an ancestor of all -nodes. See also Figure~\ref{fig:back-NA}.

Note also that the walk visiting each group  consists only of right-edges: so the total reward in any single group  is at most  (see Claim~\ref{cl:na-Rbranch}). Define  for each .

\begin{figure}[ht]
  \begin{centering}
    \includegraphics[scale=0.75]{back-NA}
    \caption{The walk corresponding to non-adaptive policy  in Claim~\ref{cl:na-back}. \label{fig:back-NA} }
  \end{centering}
\end{figure}

Let  denote the (random) index of the first group where a positive size is  instantiated. We now show that  can not visit any group indexed more than . Let  and  denote the end-points of the left-edge . Note that  must traverse the left edge  out of  to reach groups . If  is the node with positive size instantiation and  its level, then  (since  is an ancestor of all -nodes). The distance traveled by \N' till  is 
 where the last inequality follows by Lemma~\ref{lem:prefix-size}. Thus the total distance plus size seen in  (till ) is at least  , which is at least  and hence . Thus  can not visit any higher indexed group.

Using the above observation, the expected reward from backtrack nodes is at most:

Above we used the fact that .
\end{proof}

Altogether, it follows that any non-adaptive policy  has expected reward at most . Finally, using Lemma~\ref{lem:ad-profit}, we obtain an  adaptivity gap.

\subsection{Adaptivity Gap on Line Metric}
We now show that the previous instance on a tree metric can also be embedded into a line metric such that the adaptivity gap does not change much. This gives an  adaptivity gap for stochastic orienteering even on line metrics.

The line metric \lm is defined as follows. Each node  of the tree \T is mapped (on the real line) to the coordinate  which is the distance in \T from the root  to . Since all distances in our construction are integers, each node lies at a non-negative integer coordinate. Note that multiple nodes may be at the same coordinate (for example, as all right-edges in \T have zero length). Below,  will denote distances in the tree metric \T, and  denotes distances in the line metric \lm.

Note that  for all nodes . Moreover, the distance  between two nodes  and  in the line metric is , which is at most the distance  in the tree metric. Thus the adaptive policy \A for the tree \T is also valid for the line, which (by Lemma~\ref{lem:ad-profit}) has expected reward .
However, the distances  on the line could be arbitrarily smaller than , and thus the key issue is to show that non-adaptive policies cannot do much better. To this end, we begin by observing some more properties of the distances  and the embedding on the line.



\begin{lemma}
\label{lem:dist-lr}
 For any internal node , let  (resp. ) denote the subtree rooted at the left (resp. right) child of .
Then, for any node ,  and
for any node ,

\end{lemma}
\begin{proof}
For any node , recall that its residual budget , where  is the total size instantiated in the adaptive policy \A before node .
Suppose , and let  be the left child of . Then
 
 where we use that ,  and the last inequality follows from Lemma \ref{lem:prefix-size}.

 Now consider .
  We have , as  lies in the right subtree under  and so  must have instantiated to a positive size before reaching .
By Claim~\ref{cl:well-defn},  which is at least  since  for each . Thus .
\end{proof}

This implies the following useful fact.
 \begin{corollary}
 \label{cor:left-right}
 In the line embedding, for any node , all nodes in the left-subtree  appear after all nodes in the right-subtree .
 \end{corollary}

We will now show that any non-adaptive policy has reward at most . This requires more work than in the tree metric case, but the high level idea is quite similar:
we restrict how any non-adaptive policy can look like by using the properties of distances, and show that such policies cannot obtain too much profit.
Observe that a non-adaptive policy  is just a walk on \lm, originating from  and  visiting a subset of vertices.


\begin{lemma}\label{lem:line-na-ordered}
Any non-adaptive policy on \lm must visit vertices ordered by non-decreasing distance from .
\end{lemma}
\begin{proof}
We will show that if vertex  is visited before  and  then the walk to  has length more than ; this would prove the lemma.

Let  denote the least common ancestor of  and .
There are two cases depending on whether  or ; note that the ancestor  cannot be  as .

If , since , it must be that  and  by Corollary \ref{cor:left-right}.
Moreover, the total distance traveled by the path is at least
  where the second inequality is  by Lemma \ref{lem:dist-lr}.


If , since , there must be at least one left edge  on the path from  to  in the tree (as the length of the right edges is 0).  Then,
the distance traveled by the path is at least . As  by Lemma \ref{lem:prefix-size},
 and as  (by definition of distances on left edges), we have
  
where the last inequality follows from Claim \ref{cl:well-defn}.
\end{proof}

By Lemma~\ref{lem:line-na-ordered}, any non-adaptive policy  visits vertices in non-decreasing coordinate order. For vertices at the same coordinate, we can break ties and assume that these nodes are visited in decreasing order of their level in \T. This does not decrease the expected reward due to the following exchange argument.

\def\oN{\ensuremath{\overline{\cal N}}\xspace}

\begin{claim}\label{cl:line-na-ties}
If  visits two vertices  consecutively that have the same coordinate in \lm and have levels , then  must be visited before .
\end{claim}
\begin{proof}
Since  and  have the same coordinate in \lm, by Lemma~\ref{lem:dist-lr} it must be that  one is an ancestor of the other, and the  path in \T consists only of right-edges. Since , node  is an ancestor of  in \T. Suppose that  chooses to visit  before . We will show that the alternate solution \oN that visits  before  has larger expected reward. This is intuitively clear since  stochastically dominates  in our setting: the probabilities are identical, size of  is less than , and reward of  is more than . The formal proof also requires independence of  and , and is by a case analysis.

Let us {\em condition} on all instantiations other than  and : we will show that \oN has larger conditional expected reward than . This would also show that the (unconditional) expected reward of \oN is more than . Let  denote the total distance plus size in  (resp. \oN) when it reaches  (resp. ). Irrespective of the outcomes at  and , the residual budgets in  and  before/after visiting  will be identical. So the only difference in (conditional) expected reward is at  and . The following table lists the different possibilities for  rewards from  and , as  varies (recall that  is the budget).
\begin{center}
{\renewcommand{\arraystretch}{1.5}
\renewcommand{\tabcolsep}{0.2cm}\begin{tabular}{|c|c|c|}
\hline  Cases & Reward  & Reward  \\
\hline  &  &  \\
\hline  &  &  \\
\hline  &  &  \\
\hline  &  &  \\
\hline  &  &  \\
\hline
\end{tabular}}
\end{center}
In each case,  gets at least as much reward as  since . This completes the proof.
\end{proof}

For any node  in , let  denote the set of nodes  satisfying (i)  appears before  in , and (ii)  is {\em not} an ancestor of  in tree \T. We refer to  as the ``blocking set'' for node . We first prove a useful property of the   sets.
\begin{claim}\label{cl:line-E-prop}
For any  and , we must have  right-subtree and  left-subtree at the lowest common ancestor  of  and . Moreover  can not get reward from  if any vertex in its blocking set  instantiates to a positive size.
\end{claim}
\begin{proof}
Observe that  and  are incomparable in \T because:
\begin{itemize}
\item  is not an ancestor of  by definition of .
\item  is not a descendant of . Suppose (for a contradiction) that  is a descendant of . Note that  and  are not co-located in \lm: if it were, then by  Claim~\ref{cl:line-na-ties} and the fact that  visits  before ,  must be an ancestor of , which contradicts the definition of . So the only remaining case is that  is located further from  than : but this contradicts Lemma~\ref{lem:line-na-ordered} as  visits  before .
\end{itemize}
So the lowest common ancestor  of  and  is distinct from both . Since , we must have  and . This proves the first part of the claim.

Since , its size .
As , by Lemma \ref{lem:dist-lr}   and hence if  has non-zero size, the total distance plus size until  is more than , i.e.~ can not get reward from .
\end{proof}

The next key claim shows that the sets  are increasing along .
\begin{claim}\label{cl:inc-E}
If node  appears before  in  then .
\end{claim}
\begin{proof}
Consider nodes  and  as in the claim, and suppose (for contradiction) that there is some . Since , by Claim~\ref{cl:line-E-prop},  right-subtree and  left-subtree, where  is the lowest common ancestor of  and . Clearly  appears before  in  ( is before  which is before ). And since ,  must be an ancestor of . Hence  is also in the right-subtree, and ;  recall that  left-subtree. This contradicts with Lemma~\ref{lem:line-na-ordered} since  is visited before . Thus .
\end{proof}

Based on Claim~\ref{cl:inc-E}, the blocking sets in  form an increasing sequence. So we can partition  into contiguous segments  with  (resp. ) denoting the first (resp. last) vertex of ,  so that the following hold for each .
\begin{itemize}
\item The first vertex  of  has , and
\item the increase in the blocking set .
\end{itemize}

{\bf Defining directed non-adaptive policies from .} For each  consider the non-adaptive policy  that traverses segment  and visits only vertices in ; note that  since . Notice that the blocking set is always empty in : this means that nodes in  are visited in the order of some root-leaf path in tree \T, i.e.  is a directed non-adaptive policy (as considered in Section~\ref{subsec:dir-gap}). So by Claim~\ref{cl:mon-na}, the expected reward in each  is at most . That is,


Now we can upper bound the reward in the original non-adaptive policy .

Inequality~\eqref{eq:line-na-fin2} uses  and the second part of Claim~\ref{cl:line-E-prop} which implies . Inequality~\eqref{eq:line-na-fin3} uses the definition of  which is obtained by skipping nodes  in , and also the independence of the sizes (which allows us to drop the conditioning). Finally,~\eqref{eq:line-na-fin4} uses the property  and~\eqref{eq:line-mon-na}. This completes the proof of Theorem~\ref{thm:ad-gap}.



\section{Adaptivity Gap Upper Bound for Correlated  Orienteering} \label{sec:corr-ad-gap}
Here we show that the adaptivity gap of stochastic orienteering is  even in the setting of {\em correlated} sizes and rewards. This is an extension of the result proved in~\cite{GKNR12} for the uncorrelated case.

The correlated stochastic orienteering problem again consists of a budget  and metric  with each vertex  denoting a random job. Each vertex has a joint distribution over its reward and processing time (size); so the reward and size at any vertex are correlated. These distributions are still independent across vertices. The basic stochastic orienteering problem is the special case when vertex rewards are deterministic. We prove Theorem~\ref{thm:corr-loglog-UB} in this section.


{\bf Notation.} Recall that for each vertex ,  and  denote its (random) size and reward. As before, all sizes are integers in the range . We assume an explicit representation of each job's distribution: the job at vertex  has size  and reward  with probability .

We represent the optimal adaptive policy naturally as a decision tree \T. Nodes in \T are labeled by vertices in  and branches correspond to size (and reward) instantiations. Note that the same vertex of  may appear at multiple nodes of \T (note the distinction between nodes and vertices). However, any root-leaf path in \T contains each vertex at most once. For nodes , we use  to denote  being an ancestor of  in \T, where .
For any node , let  (resp. ) denote the total distance traveled (resp. size observed) in \T before . For node , we overload notation and use ,  etc to denote the respective term for the vertex labeling .

\def\rr{\ensuremath{\overline{r}}\xspace}

Note that at any node , only size instantiations  contribute reward (any larger size violates the budget).
Define the expected reward at node  as . Observe that the optimal adaptive reward is .

For each integer  and node , define .


Let  be a parameter that will be fixed later.

We are now ready to prove Theorem~\ref{thm:corr-loglog-UB}. It is along similar lines as the proof for uncorrelated stochastic orienteering in~\cite{GKNR12}, and also makes use of the following concentration inequality.

\begin{theorem}[Theorem~1 in~\cite{Zhang05}]\label{thm:zhang}
Let  be a sequence of possibly dependent random variables; for each  variable  depends
only on . Consider also a sequence of random functionals  that lie in . Let
 denote the expectation of  with respect to , conditional on . Furthermore, let
 denote any stopping time. Then,

\end{theorem}




This result is used in proving the following important property.

\begin{lemma}\label{lem:corr-prob}
Assume , and fix any . Then, the probability of reaching a node  with:\begin{itemize}
\item , and
\item 
\end{itemize}
is at most .
\end{lemma}
\begin{proof}
For each , set  to be the  node seen in \T, and

Observe that this sequence satisfies the condition in Theorem~\ref{thm:zhang}, since the identity of the  node in \T depends only on the outcomes of the previous  nodes. Moreover, each  has a value in the range . Note that the conditional expectation . Define stopping time  to be the first node  (if one exists) at which the following two conditions hold:


If there is no such node, then  stops when \T ends (i.e. after a leaf-node of \T). Clearly, if  stops before \T ends, then


Now, setting  in Theorem~\ref{thm:zhang}, the probability that  stops before \T ends (i.e. we reach a node  satisfying the two conditions stated in the lemma) is at most    using . \end{proof}


\begin{lemma}\label{lem:corr-NAfromA}
Assume . There is some node  such that the path  from the root to  satisfies:
\begin{itemize}
\item Total reward: .
\item Prefix size: For each  and , either  or .
\end{itemize}
\end{lemma}
\begin{proof}
For each , define {\em band  ``star nodes''} to be those  that satisfy the two conditions in Lemma~\ref{lem:corr-prob}. Using Lemma~\ref{lem:corr-prob} and a union bound over  values of , the probability  of reaching any star node is at most  since .

Observe that for any node , the conditional expected reward from the subtree of \T under  is at most : otherwise, the alternate policy that visits  directly from the root and follows the subtree below  would be a feasible policy of value more than , contradicting the optimality of \T.

Consider tree \T truncated just before all the star nodes. By the above two properties, the expected reward lost is at most . So the remaining reward is at least . By averaging, there is some leaf node  in truncated \T, at which ; this proves the ``total reward'' property. Let  denote the path from root to  in \T. Since  does not contain a star node (of any band), every   violates one of the conditions in Lemma~\ref{lem:corr-prob} for each value of . This proves the ``prefix size'' property.
\end{proof}

{\bf The non-adaptive policy.} We now complete the proof of Theorem~\ref{thm:corr-loglog-UB} by implementing the path  from Lemma~\ref{lem:corr-NAfromA} as a non-adaptive solution \N. The policy \N simply involves visiting the vertices on  (in that order) and attempting each job independently with probability . Observe that the distance traveled by \N until any  is exactly .

We will show that the expected reward of policy \N is at least .

\begin{claim}\label{cl:corr-NA}
For any , \le i_v.
\end{claim}
\begin{proof}
Let  be the value for which . So,


Note that the claim is trivially true for  being the root. For any other , let  denote the node in  (and \T) immediately preceding .
Using the ``prefix size'' property  (with  and ) in Lemma~\ref{lem:corr-NAfromA}, it follows that . Since policy \N tries each job only with probability ,  by Markov's inequality,
 Since  and sizes are integral, the claim follows.
\end{proof}
We can now bound the reward in \N.

The first inequality uses independence across vertices. The second inequality uses the sampling probability of jobs in \N, Claim~\ref{cl:corr-NA} and the definition of . The last inequality uses the ``total reward'' property from Lemma~\ref{lem:corr-NAfromA}. This completes the proof of Theorem~\ref{thm:corr-loglog-UB}.

\paragraph{Defining portals on non-adaptive policy}
We now define some special nodes on the path  from Lemma~\ref{lem:corr-NAfromA}, which will be useful in the approximation algorithm given in the next section. First, some notation.
\begin{definition}[Capped sizes and rewards]\label{def:cap-size-rew}
For any vertex  and integer , let  be the mean size of  capped at . For any vertex  and integer , let  be the expected reward from  under size instantiation at most .
\end{definition}

Note that the expected reward of any  in \T is .

Also, capped sizes satisfy the following property which will play a crucial role  in our algorithm:

This inequality follows from the fact that .



Let , and .  
Recall that  starts at the root ; for notational convenience set . Clearly, .
For any , define {\em segment}  to consist of the vertices  in path . The following lemma shows that
we can round size instantiations in  to powers of two, and still retain the two properties in Lemma~\ref{lem:corr-NAfromA}.



\begin{lemma}\label{lem:corr-portals}
Given any instance of correlated stochastic orienteering, there exist ``portal vertices''  and path  originating from  and visiting the portals in that order, such that:
\begin{itemize}
\item Reward: . For each ,  consists of the vertices in  between  and . For any ,  is the distance to vertex  along .
\item Prefix mean size: , for all . Here .
\end{itemize}
\end{lemma}
\begin{proof}
The path  is from Lemma~\ref{lem:corr-NAfromA}, and portals  are as in~\eqref{eq:NA-portals}. For the first property, consider any segment  and vertex . By the definition of portals, we have ; so . Using the ``total reward'' property in Lemma~\ref{lem:corr-NAfromA}, we have:

We used the fact that for any vertex  with ,  since the total size observed before  is at least .

To see the second property consider any . Let  be the vertex immediately preceding , and let  be the immediate predecessor of . By definition of portal , we have ; i.e. . Using the ``prefix size'' property in Lemma~\ref{lem:corr-NAfromA} with  and , we obtain that . So .
\end{proof}

\section{New Approximation Algorithm for Correlated Orienteering} \label{sec:corr-alg}
In this section, we present an improved quasi-polynomial time approximation algorithm for correlated stochastic orienteering, and prove Theorem~\ref{thm:corr-NA}.

An important subroutine in our algorithm is the {\em deadline orienteering} problem~\cite{BBCM04}. The input to deadline-orienteering is a metric  denoting travel times, rewards  and deadlines  at all vertices, start () and end () vertices, and length bound . The objective is to compute an  path of length at most  that maximizes the reward from vertices visited before their deadlines. The best approximation ratio for this problem is   due to Bansal et al.~\cite{BBCM04}; see also Chekuri et al.~\cite{CKP08}. (Strictly speaking, this definition is slightly more general than the usual one where there is no end-vertex or length bound; but all known approximation algorithms also work for the version we use here.) We actually need an algorithm for a generalization of this problem, in the presence of an additional knapsack constraint. The input to {\em knapsack deadline orienteering} (\kdo) is the same as deadline-orienteering, along with a knapsack constraint given by sizes  and capacity . The objective is an   path of length at most , having total knapsack size at most , that maximizes the reward from vertices visited before their deadlines. We will use the following known result for \kdo.
\begin{theorem}[\cite{GKNR12}] \label{thm:kdo} There is an -approximation algorithm for knapsack deadline orienteering, where  denotes the best approximation ratio for the deadline orienteering problem.
\end{theorem}



Previously, a polynomial time -approximation algorithm and -hardness of approximation were known for correlated stochastic orienteering~\cite{GKNR12}, where  is the best approximation ratio for deadline orienteering. Our result improves the approximation ratio to , at the expense of quasi-polynomial running time.





{\bf Outline:} The algorithm involves three main steps. First, it guesses (by enumeration)  many ``portal vertices'' corresponding to the near-optimal structure in Lemma~\ref{lem:corr-portals}, as well as the distances traveled between consecutive portal vertices. (This is the only step that requires quasi-polynomial time.) Next, based on this information, the algorithm solves a configuration LP-relaxation for paths between the portal vertices. This step also makes use of some results/ideas from the previous algorithm~\cite{GKNR12}. Finally, the algorithm uses randomized rounding with alterations to compute the non-adaptive policy from the LP solution.





\paragraph{Portals.}
The simple enumeration algorithm guesses the  portal vertices  as in Lemma~\ref{lem:corr-portals}; recall that . It also guesses the lengths  of the segments s in Lemma~\ref{lem:corr-portals}. This requires running time .

We can reduce the enumeration of lengths using a somewhat stronger property than in Lemma~\ref{lem:corr-portals}.

\begin{lemma}\label{lem:corr-enum}
Given any instance of correlated stochastic orienteering, there exist portal vertices , auxiliary vertices , integers , and for each  a - path , such that:
\begin{itemize}
\item Path length: for any , path  has  length at most .
\item Reward: . Here,  is the distance to vertex  from  along path .
\item Prefix mean size: , for all . Here .
\end{itemize}
\end{lemma}
\begin{proof}
Consider the path  and portal vertices  satisfying the properties in Lemma~\ref{lem:corr-portals}; recall that . For each , let  denote the subpath of  from vertex  to . Recall that for any ,  distance from  to  along path .

For each , let  denote the length of subpath . Also define for each  and vertex ,  distance to vertex  from  along path ; note that .

We now modify each subpath  to obtain a new - subpath  as follows. For each vertex  define its profit , and let  denote the total profit of vertices in . Let  denote any vertex on  such that:

Here  denotes the order in which vertices appear on . So  can be viewed as a ``mid point'' of , containing half the total profit on either side of it.

Let  length of  from  to , and . Similarly, let  length of  from  to , and . Also, let ,  and ; clearly . Note that the length of  is . Let  denote the subpath   of  that shortcuts over vertices between  and . Similarly,  denotes the subpath   of  that shortcuts over vertices between  and . By~\eqref{eq:portal-mid} the total profit on each of  and  is at least .
The length of subpath  (resp. ) is  (resp. ). We have:


Let  denote the unique integer such that ; note that  since  is an integer between  and . Based on the above inequality, the shorter of  and  has length at most . We set subpath  to be the shorter one among  and . This choice ensures that:

Note also that .


We set the portal vertices , auxiliary vertices , integers , and path  as defined above.  The path length property follows from the first condition in~\eqref{eq:enum-subpath} and the definition of . The prefix size property is immediate from that in Lemma~\ref{lem:corr-portals} since each .
We now show the reward property:

Inequality~\eqref{eq:enum-profit1} uses the fact that  for all . The  equality~\eqref{eq:enum-profit2} uses  for any  and . Inequality~\eqref{eq:enum-profit3} is by the definition of  and the second condition from~\eqref{eq:enum-subpath}. Inequality~\eqref{eq:enum-profit4} uses the reward property in Lemma~\ref{lem:corr-portals}.
\end{proof}
Based on Lemma~\ref{lem:corr-enum}, our enumeration algorithm guesses the  portals  and segment lengths  in time  (by 
\eqref{eq:enum-subpath} there are at most  choices for , as it is determined by   and ). 


\paragraph{Knapsack Deadline Orienteering Instances.} Our goal is to find a  path  for each , that have properties similar to the segments in Lemma~\ref{lem:corr-enum}. To this end, we define a knapsack-deadline-orienteering instance  for each .


Instance  is defined on metric  with start vertex , end vertex  and length bound . The rewards, sizes and deadlines will be defined shortly. As in~\cite{GKNR12}, we will introduce a suitable set of {\em copies} of each vertex , with deadlines that correspond to visiting  at different possible times.
 the expected reward from  under instantiation at most . Intuitively,  is the expected reward obtained by visiting vertex  in segment  at distance  along  (so the total distance to  is ) and having observed total size  until .

In the \kdo instance , we would like to introduce a copy of vertex  for each distance , in order to permit all possibilities for visiting  in segment .
However, solutions to such an instance may obtain a large reward just by visiting multiple copies of the same vertex. In order to control the total reward obtainable from multiple copies of a vertex, we introduce copies of each vertex corresponding only to a suitable subset of . This relies on the following construction.
\begin{claim}[\cite{GKNR12}] \label{cl:gknr-count}
For each  and , we can compute in polynomial time, a subset  s.t.

\end{claim}
Roughly speaking, the copies  of vertex  are those times  at which its expected reward  doubles.

We now formally define the entire instance .
\begin{definition}[\kdo instance ]
The metric is , start-vertex , end-vertex , length bound  and knapsack capacity . For each  and , a job  with reward , deadline  and knapsack-size  is located at vertex .
\end{definition}

To reduce notation, when it is clear from context, we will use  etc. to refer to jobs as well. A feasible solution  to  is a  path of length  and total size ; the objective value is the total reward of jobs visited by  before their deadlines. We will also use  to denote the set of all feasible solutions to this \kdo instance.

{\bf Remark:} A solution  to this instance  corresponds to the  segment in the correlated orienteering solution (between portal  and ). Note that  imposes no upper bound on
's mean size capped at  (i.e. ) for any , which however will be required in the the expected reward analysis (this corresponds to the ``prefix mean size'' property
in Lemma~\ref{lem:corr-enum}). Although there is no explicit bound on  for , we can infer this using the capped mean properties, as follows.
\begin{itemize}
\item For any , we do not need a bound on  since  does not affect segment .
\item For any  , we can use~\eqref{eq:capped-mu} to obtain

So the knapsack constraint in  corresponding to  also implicitly bounds .
\end{itemize}
The fact that we introduce only a {\em single} knapsack constraint in  turns out to be important since the best algorithm (that we are aware of) for deadline-orienteering with multiple knapsack constraints has an approximation factor that grows linearly with the number of knapsacks.

\paragraph{Configuration LP relaxation.} We now give an LP
relaxation to the near-optimal structure in Lemma~\ref{lem:corr-enum}. We make use of the guessed portals  and lengths . The segments s in Lemma~\ref{lem:corr-enum} will correspond to solutions of \kdo instances s. The LP relaxation is given below:


Above,  is intended to be the indicator variable that chooses a solution path for the \kdo instance . Variables  indicate whether/not job  is selected in . These are enforced by constraints~\eqref{LP:1} and~\eqref{LP:2}. Constraint~\eqref{LP:3} requires at most one copy of each vertex to be chosen, over all segments. And constraint~\eqref{LP:4} bounds the ``prefix mean size'' as in Lemma~\ref{lem:corr-enum}. The objective corresponds to the reward in  Lemma~\ref{lem:corr-enum}.

First, observe that we can ensure equality in~\eqref{LP:2}.
\begin{claim}\label{cl:xy-equality}
Any \lp solution  can be modified to another solution that satisfies~\eqref{LP:2} with equality and has the same objective value.
\end{claim}
\begin{proof} This follows immediately from the fact that each solution-set  is ``down monotone'', i.e.  and  implies . Formally, consider the constraints~\eqref{LP:2} in any order. For each , order arbitrarily the  variables with  as . Let  denote the unique index with . Now perform the following modification:
\begin{itemize}
\item For each index , drop  from the solution .
\item For index , consider the solutions  and . Set  and .
\end{itemize}
After this change, it is clear that we have equality for
 in constraint~\eqref{LP:2}. Also, constraint~\eqref{LP:1} remains feasible. Finally, since the  variables remain unchanged, constraints~\eqref{LP:3}-\eqref{LP:4} remain feasible and the objective value stays the same.
\end{proof}



\begin{claim}\label{cl:LP-obj}
The optimal value of \lp is at least .
\end{claim}
\begin{proof}
We will show that the subpaths  in Lemma~\ref{lem:corr-enum} correspond to a feasible integral solution to \lp. 
Let  denote the concatenated path .
Based on our guess of the portals and distances, we have . For any vertex  let  denote the distance to  along  (this also appears in Lemma~\ref{lem:corr-enum}); note that the distance to  along  is then . For each  let  be the deadline of the earliest copy of  that path  can visit in the \kdo instance . Consider  as a solution to  which visits jobs ; in order to verify feasibility, we only need to check that , which follows from the third property in Lemma~\ref{lem:corr-enum}.

Consider now the solution to \lp with  for all ;  for each , ; and all other variables set to zero. Constraints~\eqref{LP:1},~\eqref{LP:2},~\eqref{LP:3} and ~\eqref{LP:5} are clearly satisfied. Observe that the left-hand-side of~\eqref{LP:4} is exactly  which is at most  by the third property in Lemma~\ref{lem:corr-enum}. So  is a feasible (integral) solution to \lp. We now bound the objective value:

The first inequality is by definition of  and Claim~\ref{cl:gknr-count}; the next two equalities use the definitions  and ; and the final inequality is by the first property in Lemma~\ref{lem:corr-enum}.
\end{proof}

\paragraph{Solving the configuration LP.} We show that \lp  can be solved approximately using an approximation algorithm for \kdo. This is based on applying the Ellipsoid algorithm to the dual of \lp, which is given below:

In order to solve \dlp using the Ellipsoid method, we need to provide a separation oracle that tests feasibility. Observe that constraints~\eqref{DLP:1} are polynomial in number, and can be checked explicitly. Constraint~\eqref{DLP:2} for any  is equivalent to asking whether the optimal value of \kdo instance  with rewards  is at most . Using an -approximate separation oracle (the knapsack deadline orienteering algorithm from Theorem~\ref{thm:kdo}) within the Ellipsoid algorithm, we obtain an -approximation algorithm for \dlp and hence \lp.

{\bf Remark:} Alternatively, we can solve \lp using a faster combinatorial algorithm that is based on multiplicative weight updates. Note that we can eliminate -variables in \lp by setting constraints~\eqref{LP:2} to equality (Claim~\ref{cl:xy-equality}). This results in a {\em packing LP}, consisting of  non-negative variables with each constraint of the form  where all entries in  and  are non-negative. So we can solve \lp using faster approximation algorithms~\cite{PST91,GK07} for packing LPs, that also require only an approximate dual separation oracle (which is \kdo in our setting).


\paragraph{Rounding the LP solution.} Let  denote the -approximate solution to \lp. By Claim~\ref{cl:LP-obj} the objective value is . The rounding algorithm below describes a (randomized) non-adaptive policy.
\begin{enumerate}
\item \label{step:corr1} For each , independently select solution  as:



\item \label{step:corr2} If any vertex  appears in more than one solution , then drop  from all of them.

\item \label{step:corr3} If solution  exceeds any constraint~\eqref{LP:4} by a factor more than  then return an empty solution.

\item \label{step:corr4} For each , if  contains multiple copies of any vertex  then retain only the copy with earliest deadline (i.e. highest reward).

\item \label{step:corr5} Return the non-adaptive policy that traverses the path   and attempts each vertex independently with probability .
\end{enumerate}

\paragraph{Analysis.} We now show that the expected reward of this non-adaptive policy is , which would prove Theorem~\ref{thm:corr-NA}; recall that  and .



\begin{lemma} \label{lem:RRalt}
For any ,  and ,
.
\end{lemma}
\begin{proof}
Let ,  and  denote the solution  after Step 1, 2 and 3 respectively. Clearly,
\vspace{-2mm}
\vspace{-3mm}

The last equality uses Claim~\ref{cl:xy-equality}. Note that  gets dropped in Step~\ref{step:corr2} exactly when there is some other solution  that contains a copy of . By union bound,~\eqref{eq:prob-tau1} and~\eqref{LP:3},
\vspace{-2mm}\vspace{-5mm}

In Step~\ref{step:corr3}, the entire solution is declared empty if any constraint~\eqref{LP:4} is violated by more than a factor of ; otherwise, . Consider the constraint~\eqref{LP:4} with index . This reads , where . The key observation is the following:

This uses the fact that  satisfies the knapsack constraint  in \kdo instance ; and by the observation~\eqref{eq:capped-mu} on capped sizes,  since .

Using~\eqref{eq:main-RR} it follows that  is the sum of independent  bounded random variables.  Using~\eqref{eq:prob-tau1}, and \lp constraint~\eqref{LP:4} we have:

Hence, by a Chernoff bound, {\small }.

Taking a union bound over all  constraints~\eqref{LP:4}, it follows that with probability at least half, none of them is violated by a factor more than . That is, .

Combined with~\eqref{eq:prob-tau1} and \eqref{eq:prob-tau2}, we obtain the lemma.
\end{proof}


\begin{claim}\label{cl:corr-exp}
The expected \lp objective value of solution  after Step~\ref{step:corr4} is .
\end{claim}
\begin{proof}
By Lemma~\ref{lem:RRalt} it follows that expected \lp objective value of solution  after Step~\ref{step:corr3} is at least:
{\small }
The last inequality uses Claim~\ref{cl:LP-obj} and the fact that we have an -approximately optimal \lp solution.

In Step~\ref{step:corr4}, we retain only one copy of each vertex in each . Using Claim~\ref{cl:gknr-count}, since we retain the most profitable copy of each vertex, this decreases the total reward of each  by at most a factor of . \end{proof}

Consider now the non-adaptive policy in Step~\ref{step:corr5} and {\em condition} on any solution . Fix any  and  . The distance traveled until  is at most . By Step~\ref{step:corr4} and \lp constraint~\eqref{LP:4}, the total -size of vertices in  is at most . Since each vertex is attempted only with probability , we have
 where the summation ranges over all vertices visited before . Thus with probability at least half, vertex  is visited  by time .

So the expected reward from vertex  is .
Adding this contribution over all vertices, the total reward is  times the \lp objective of solution  after Step~\ref{step:corr4}. Taking expectations over Steps~\ref{step:corr1}-\ref{step:corr4}, and using Claim~\ref{cl:corr-exp}, it follows that our non-adaptive policy has expected reward . This completes the proof of Theorem~\ref{thm:corr-NA}.


\section{Conclusion}
In this paper, we proved an  lower bound on the adaptivity gap of stochastic orienteering. The best known upper bound  is ~\cite{GKNR12}. Closing this gap is an interesting open question. For the {\em correlated} stochastic orienteering problem, we gave a quasi-polynomial time -approximation algorithm, where  denotes the best approximation ratio for the deadline-orienteering problem. It is known that correlated stochastic orienteering can not be approximated to a factor better than ~\cite{GKNR12}. Finding an  approximation algorithm for correlated stochastic orienteering is another interesting direction.


\bibliographystyle{plain}
\bibliography{stoc-ks}


\end{document}
