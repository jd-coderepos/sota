\section{Experimental Evaluation}
We conduct experiments on the COCO dataset \cite{lin2014microsoft}. Our proposed model is implemented in TensorFlow \cite{abadi2016tensorflow} on top of the object detection library developed by \cite{huang2016speed}.

\subsection{Implementation Details}
We employ the same hyper-parameter settings as in \cite{huang2016speed, sun2017revisiting}, and only discuss the main difference below.

\textbf{Atrous convolution:} We apply the atrous convolution \cite{holschneider1989real, giusti2013fast, sermanet2013overfeat, papandreou2014untangling}, which has been successfully explored in semantic segmentation \cite{chen2015attention, zhao2017pyramid, chen2017rethinking}, object detection \cite{dai2016rfcn, huang2016speed} and instance segmentation \cite{zhang2015monocular, dai2017fully}, to extract denser feature maps. Specifically, we extract features with  (\emph{output\_stride} denotes the ratio of input image spatial resolution to final output resolution).

\textbf{Weight initialization:} For the  convolution applied to the concatenation of semantic and direction features, we found that the training converges faster by initializing the convolution weights to be , putting a slightly larger weight on the direction features, which is more important in instance segmentation, as shown in the experimental results.

\textbf{Mask training:} During training, only groundtruth boxes are used to train the branches that predict semantic segmentation logits and direction logits, since direction logits may not align well with instance center if boxes are jittered. We employ sigmoid function to estimate both the coarse and refined mask results. Our proposed model is trained end-to-end without piecewise pretraining of each component.

\subsection{Quantitative Results}
We first report the ablation studies on a \textit{minival} set and then evaluate the best model on \textit{test-dev} set, with the metric mean average precision computed using mask IoU.

\textbf{Mask crop size:} The TensorFlow operation, ``crop and resize'', is used at least in two places: one for box classification and one for cropping semantic and direction features for foreground/background segmentation (another one for deformed sub-boxes if ``deformable crop and resize'' is used). In the former case, we use the same setting as in \cite{huang2016speed, sun2017revisiting}, while in the latter case, the crop size determines the mask segmentation resolution. Here, we experiment with the effect of using different crop size in \tabref{tab:mask_crop_size} and observe that using crop size more than 41 does not change the performance significantly and thus we use 41 throughout the experiments.

\begin{table}[!t]
  \centering
  \scalebox{1}{
  \begin{tabular}{c | c}
    \toprule[0.2em]
    Mask crop size & mAP@0.5 \\
    \toprule[0.2em]
    21 & 50.92\% \\
    41 & 51.29\% \\
    81 & 51.17\% \\
    161 & 51.36\% \\
    321 & 51.24\% \\
    \bottomrule[0.1em]
  \end{tabular}
  }
  \caption{Using crop size = 41 is sufficient for mask segmentation.}
  \label{tab:mask_crop_size}
\end{table}

\textbf{Effect of semantic and direction features:} In \tabref{tab:sem_dir}, we experiment with the effect of semantic and direction features. Given only semantic segmentation features, the model attains an mAP@0.75 performance of 24.44\%, while using only direction features the performance improves to 27.4\%, showing that direction feature is more important than the semantic segmentation feature. When employing both features, we achieve 29.72\%. We observe that the performance can be further improved if we also quantize the distance in the direction pooling. As illustrated in \figref{fig:boxes}, we also quantize the distance with different number of bins. For example, when using 2 bins, we split the same direction region into 2 regions. We found that using 4 bins can further improves performance to 30.57\%. Note that quantizing the distance bins improves more at high mAP threshold (\cf mAP@0.5 and mAP@0.75 in \tabref{tab:sem_dir}). In the case of using  distance bins, the channels of direction logits become , since we use 8 directions by default (\ie, 360 degree is quantized into 8 directions). Thus, our model generates  channels for direction pooling in the end.

\begin{figure}[!t]
  \centering
  \begin{tabular}{c c c}
    \includegraphics[width=0.3\linewidth]{fig/box1_direction8} &
    \includegraphics[width=0.3\linewidth]{fig/box2_direction8} &
    \includegraphics[width=0.3\linewidth]{fig/box4_direction8} \\
    (a) 1 bin &
    (b) 2 bins &
    (c) 4 bins \\
  \end{tabular}
  \caption{We quantize the distance within each direction region. In (b), we split each original direction region into 2 regions. Our final model uses 4 bins for distance quantization as shown in (c).}
  \label{fig:boxes}
\end{figure}

\begin{table}[!t]
  \centering
  \begin{tabular}{c c | c c}
    \toprule[0.2em]
    Semantic & Direction & mAP@0.5 & mAP@0.75 \\
    \toprule[0.2em]
    \checkmark &            & 48.41\% & 24.44\% \\
               & \checkmark (1) & 50.21\% & 27.40\% \\
    \checkmark & \checkmark (1) & 51.83\% & 29.72\% \\
    \toprule[0.1em]
    \checkmark & \checkmark (4) & {\bf 52.26\%} & {\bf 30.57\%} \\
    \bottomrule[0.1em]
  \end{tabular}
  \caption{Effect of semantic and direction features. Direction features are more important than semantic segmentation features in the model, and the best performance is obtained by using both features and adopting 4 bins to quantize the distance in direction pooling. We show number of bins for distance quantization in parentheses.}
  \label{tab:sem_dir}
\end{table}


\textbf{Number of directions:} In \tabref{tab:direction_effect}, we explore the effect of different numbers of directions for quantizing the 360 degree. We found that using 8 directions is sufficient to deliver good performance, when adopting 4 bins for distance quantization. Our model thus uses  (8 for direction and 4 for distance quantization) channels for direction pooling throughout the experiments.

\begin{table}[!t]
  \centering
  \scalebox{1}{
  \begin{tabular}{c c | c c}
    \toprule[0.2em]
    Distance bins & Directions & mAP@0.5 & mAP@0.75\\
    \toprule[0.2em]
    4 & 2 & 53.51\% & 33.80\% \\
    4 & 4 & 53.85\% & 34.39\% \\
    4 & 6 & 54.10\% & 34.86\% \\
    4 & 8 & 54.13\% & 34.82\% \\
\bottomrule[0.1em]
  \end{tabular}
  }
  \caption{Effect of different numbers of directions (\ie, how many directions for quantizing the 360 degree) when using four bins for distance quantization.}
  \label{tab:direction_effect}
\end{table}






\textbf{Mask refinement:} We adopt a small ConvNet consisting of three  convolution layers with 64 filters. We have experimented with replacing the small ConvNet with other structures (\eg, more layers and more filters) but have not observed any significant difference. In \tabref{tab:refine_mask}, we experiment with different features from lower-level of ResNet-101. Using conv1 (the feature map generated by the first convolution) improves the mAP@0.75 performance to 32.92\% from 30.57\%, while using both conv1 and conv2 (\ie, the last feature map in res2x block) obtains the best performance of 33.89\%. We have observed no further improvement when adding more lower-level features.

\begin{table}[!t]
  \centering
  \scalebox{1}{
  \begin{tabular}{c c c | c c}
    \toprule[0.2em]
    conv1 & conv2 & conv3 & mAP@0.5 & mAP@0.75\\
    \toprule[0.2em]
    &        &        & 52.26\% & 30.57\% \\
    \checkmark &      &   & 52.68\% & 32.92\% \\
    \checkmark & \checkmark &  & \bf{53.26\%} & \bf{33.89\%} \\
    \checkmark & \checkmark & \checkmark & 52.55\% & 32.88\% \\ \bottomrule[0.1em]
  \end{tabular}
  }
  \caption{Mask refinement. The best performance is obtained when using features from conv1 and conv2 (\ie, last feature map in res2x block). Note conv3 denotes the last feature map in res3x block.}
  \label{tab:refine_mask}
\end{table}

\textbf{Multi-grid:} Motivated by the success of employing a hierarchy of different atrous rates in semantic segmentation \cite{wang2017understanding, dai2017deformable, chen2017rethinking}, we modify the atrous rates in (1) the last residual block shared for predicting both semantic and direction features, and (2) the block for box classifier. Note that there are only three convolutions in those blocks. As shown in \tabref{tab:multi_grid}, it is more effective to apply different atrous rates for the box classifier. We think current evaluation metric (mAP) favors detection-based methods (as also pointed out by \cite{bai2017deep}) and thus it is more effective to improve the detection branch over the segmentation branch in our proposed model.

\begin{table}[!t]
  \centering
  \scalebox{1.}{
  \begin{tabular}{c c | c c c}
    \toprule[0.2em]
    & & \multicolumn{3}{c}{Box Classifier} \\
    & & (1, 1, 1)  & (1, 2, 1) & (1, 2, 4)\\
    \toprule[0.1em]
    \multirow{3}{*}{Sem/Dir} &  (4, 4, 4) & 34.82\% & 35.59\% & 35.35\% \\
    &  (4, 8, 4) & 35.07\% & 35.60\% & \bf{35.78\%} \\
    &  (4, 8, 16) & 34.89\% & 35.43\% & 35.51\% \\
    \bottomrule[0.1em]
  \end{tabular}
  }
  \caption{Multi-grid performance (mAP@0.75). Within the parentheses, we show the three atrous rates used for the three convolutions in the residual block. It is effective to adopt different atrous rates for the box classifier. Further marginal improvement is obtained when we also change the atrous rates in the last block that is shared by semantic segmentation and direction prediction logits.}
  \label{tab:multi_grid}
\end{table}



\textbf{Pretrained network:} We experimentally found that it is beneficial to pretrain the network. Recall that we duplicate one extra conv5 (or res5x) block in original ResNet-101 for box classification. As shown in \tabref{tab:ablation}, initializing the box classifier in Faster R-CNN with the ImageNet pretrained weights improves the performance from 33.89\% to 34.82\% (mAP@0.75). If we further pretrain ResNet-101 on the COCO semantic segmentation annotations and employ it as feature extractor, the model yields about 1\% improvement. This finding bears a similarity to \cite{bell2016inside} which adopts the semantic segmentation regularizer.

\textbf{Putting everything together:} We then employ the best multi-grid setting from \tabref{tab:multi_grid} and observe about 0.7\% improvement (mAP@0.75) over the one pretrained with segmentation annotations, as shown in \tabref{tab:ablation}. Following \cite{lin2016feature, he2017mask}, if the input image is resized to have a shortest side of 800 pixels and the Region Proposal Network adopts 5 scales, we observe another 1\% improvement. Using the implemented ``deformable crop and resize'' brings extra 1\% improvement. Additionally, we employ scale augmentation, specifically random scaling of inputs during training (with shortest side randomly selected from ), and attain performance of 40.41\% (mAP@0.75). Finally, we exploit the model that has been pretrained on the JFT-300M dataset \cite{hinton2015distilling, chollet2016xception, sun2017revisiting}, containing 300M images and more than 375M noisy image-level labels, and achieve performance of 41.59\% (mAP@0.75). 

\textbf{Atrous convolution for denser feature maps:} We employ atrous convolution, a powerful tool to control output resolution, to extract denser feature maps with . We have observed that our performance drops from 40.41\% to 38.61\% (mAP@0.75), if we change .

\begin{table}[!t]
  \centering
  \scalebox{0.75}{
  \begin{tabular}{c c c c c c c | c c}
    \toprule[0.2em]
    \textbf{BC} & \textbf{Seg} & \textbf{MG} & \textbf{Anc} & \textbf{DC} & \textbf{RS} & \textbf{JFT} & mAP@0.5 & mAP@0.75\\
    \toprule[0.2em]
               &            &            &            &        &      &  & 53.26\% & 33.89\% \\
    \checkmark &            &            &            &        &      &  & 54.13\% & 34.82\% \\
    \checkmark & \checkmark &            &            &        &      &  & 55.03\% & 35.91\% \\
    \checkmark & \checkmark & \checkmark &            &        &      &  & 55.64\% & 36.65\% \\
    \checkmark & \checkmark & \checkmark & \checkmark &        &      &  & 57.44\% & 37.57\% \\
    \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  &      &  & 58.69\% & 38.61\% \\
    \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  &  \checkmark  &  & 60.55\% & 40.41\% \\
    \checkmark & \checkmark & \checkmark & \checkmark & \checkmark  &  \checkmark  & \checkmark & 61.80\% & 41.59\% \\
    \bottomrule[0.1em]
  \end{tabular}
  }
  \caption{\textbf{BC}: Initialize the Box Classifier branch with ImageNet pretrained model. \textbf{Seg}: Pretrain the whole model on COCO semantic segmentation annotations. \textbf{MG}: Employ multi-grid in last residual block. \textbf{Anc}: Use (800, 1200) and 5 anchors. \textbf{DC}: Adopt deformable crop and resize. \textbf{RS}: Randomly scale inputs during training. \textbf{JFT}: Further pretrain the model on JFT dataset.}
  \label{tab:ablation}
\end{table}



\begin{table*}[!t]
  \centering
  \scalebox{1.}{
  \begin{tabular}{c c | c c c | c c c}
    \toprule[0.2em]
    Method & Feature Extractor & mAP & mAP@0.5 & mAP@0.75 & mAP & mAP & mAP \\
    \toprule[0.2em]
    FCIS \cite{dai2017fully} & ResNet-101 & 29.2\% & 49.5\% & - & - & - & - \\
    FCIS+++ \cite{dai2017fully} & ResNet-101 & 33.6\% & 54.5\% & - & - & - & - \\
    Mask R-CNN \cite{he2017mask} & ResNet-101 & 33.1\% & 54.9\% & 34.8\% & 12.1\% & 35.6\% & 51.1\% \\
    Mask R-CNN \cite{he2017mask} & ResNet-101-FPN & 35.7\% & 58.0\% & 37.8\% & 15.5\% & 38.1\% & 52.4\% \\
    Mask R-CNN \cite{he2017mask} & ResNeXt-101-FPN & 37.1\% & 60.0\% & 39.4\% & 16.9\% & 39.9\% & 53.5\% \\
    \toprule[0.1em]
    MaskLab  & ResNet-101 & 35.4\% & 57.4\% & 37.4\% & 16.9\% & 38.3\% & 49.2\% \\
    MaskLab+ & ResNet-101       & 37.3\% & 59.8\% & 39.6\% & 19.1\% & 40.5\% & 50.6\% \\
    MaskLab+ & ResNet-101 (JFT) & 38.1\% & 61.1\% & 40.4\% & 19.6\% & 41.6\% & 51.4\% \\
    \bottomrule[0.1em]
  \end{tabular}
  }
  \caption{Instance segmentation single model \textit{mask} mAP on COCO test-dev. \textbf{MaskLab+}: Employ scale augmentation during training.}
  \label{tab:test_dev_mask}
\end{table*}

\begin{table*}[!t]
  \centering
  \scalebox{1.}{
  \begin{tabular}{c c | c c c | c c c}
    \toprule[0.2em]
    Method & Feature Extractor & mAP & mAP@0.5 & mAP@0.75 & mAP & mAP & mAP \\
    \toprule[0.2em]
    G-RMI \cite{huang2016speed} & Inception-ResNet-v2 & 34.7\% & 55.5\% & 36.7\% & 13.5\% & 38.1\% & 52.0\% \\
    TDM \cite{shrivastava2016beyond} & Inception-ResNet-v2 & 37.3\% & 57.8\% & 39.8\% & 17.1\% & 40.3\% & 52.1\% \\
    Mask R-CNN \cite{he2017mask} & ResNet-101-FPN & 38.2\% & 60.3\% & 41.7\% & 20.1\% & 41.1\% & 50.2\% \\
    Mask R-CNN \cite{he2017mask} & ResNeXt-101-FPN & 39.8\% & 62.3\% & 43.4\% & 22.1\% & 43.2\% & 51.2\% \\
    \toprule[0.1em]
    MaskLab  & ResNet-101 & 39.6\% & 60.2\% & 43.3\% & 21.2\% & 42.7\% & 52.4\% \\
    MaskLab+ & ResNet-101       & 41.9\% & 62.6\% & 46.0\% & 23.8\% & 45.5\% & 54.2\% \\
    MaskLab+ & ResNet-101 (JFT) & 43.0\% & 63.9\% & 47.1\% & 24.8\% & 46.7\% & 55.2\% \\
    \bottomrule[0.1em]
  \end{tabular}
  }
  \caption{Object detection single model \textit{box} mAP on COCO test-dev. \textbf{MaskLab+}: Employ scale augmentation during training.}
  \label{tab:test_dev_box}
\end{table*}


\textbf{Test-dev mask results:} After finalizing the design choices on the \textit{minival} set, we then evaluate our model on the \textit{test-dev} set. As shown in \tabref{tab:test_dev_mask}, our MaskLab model outperforms FCIS+++ \cite{dai2017fully}, although FCIS+++ employs scale augmentation and on-line hard example mining \cite{shrivastava2016training} during training as well as multi-scale processing and horizontal flip during test. Our ResNet-101 based model performs better than the ResNet-101 based Mask R-CNN \cite{he2017mask}, and attains similar performance as the ResNet-101-FPN based Mask R-CNN. Our ResNet-101 based model with scale augmentation during training, denoted as MaskLab+ in the table, performs 1.9\% better, attaining similar mAP with Mask R-CNN built on top of the more powerful ResNeXt-101-FPN \cite{lin2016feature, xie2017aggreated}. Furthermore, pretraining MaskLab+ on the JFT dataset achieves performance of 38.1\% mAP.

\textbf{Test-dev box results:} We also show box detection results on COCO test-dev in \tabref{tab:test_dev_box}. Our ResNet-101 based model even without scale augmentation during training performs better than G-RMI \cite{huang2016speed} and TDM \cite{shrivastava2016beyond} which employ more expensive yet powerful Inception-ResNet-v2 \cite{szegedy2017inception} as feature extractor. All our model variants perform comparably or better than Mask R-CNN variants in the box detection task. Our best single-model result is obtained with scale augmentation during training, 41.9\% mAP with an ImageNet pretrained network and 43.0\% mAP with a JFT pretrained network.

\subsection{Qualitative Results}

\textbf{Semantic and direction features:} In \figref{fig:sem_seg_logits}, we visualize the `person' channel in the learned semantic segmentation logits. We have observed that there can be some high activations in the non-person regions (\eg, regions that are near elephant's legs and kite), since the semantic segmentation branch is only trained with groundtruth boxes without any negative ones. This, however, is being handled by the box detection branch which filters out wrong box predictions. More learned semantic segmentation and direction prediction logits are visualized in \figref{fig:model_illustration}.

\textbf{Deformable crop and resize:} In \figref{fig:vis_deformed_crop_and_resize}, we visualize the learned deformed sub-boxes. Interestingly, unlike the visualization results of deformable pooling in \cite{dai2017deformable} which learns to focus on object parts, our sub-boxes are deformed in a circle-shaped arrangement, attempting to capture longer context for box classification. We note that incorporating context to improve detection performance has been used in, \eg, \cite{gidaris2015object, zhu2015segdeepm,Zagoruyko2016Multipath}, and our model is also able to learn this.

\textbf{Predicted masks:} We show some qualitative results produced by our proposed model in \figref{fig:vis_val}. We further visualize our failure mode in the last row, mainly resulting from detection failure (\eg, missed-detection and wrong class prediction) and segmentation failure (\eg, coarse boundary result).

\begin{figure}[!t]
  \centering
  \begin{tabular}{c c}
    \includegraphics[width=0.45\linewidth]{fig/model_illustration/image/000000000318.png} &
    \includegraphics[width=0.45\linewidth]{fig/model_illustration/person_logits/000000000318.png} \\
    \includegraphics[width=0.45\linewidth]{fig/model_illustration/image/000000000458.png} &
    \includegraphics[width=0.45\linewidth]{fig/model_illustration/person_logits/000000000458.png} \\
    (a) Image &
    (b) `Person' Logits\\
  \end{tabular}
  \caption{`Person' channel in the predicted semantic segmentation logits. Note the high activations on non-person regions, since the semantic segmentation branch is only trained with groundtruth boxes. This, however, is being handled by the box detection branch which filters out wrong box predictions.}
  \label{fig:sem_seg_logits}
\end{figure}

\begin{figure}[!t]
  \centering
  \begin{tabular}{c c c}
    \includegraphics[height=0.22\linewidth]{fig/deformable_crop_and_resize/motor.png} &
    \includegraphics[height=0.22\linewidth]{fig/deformable_crop_and_resize/airplane.png} &
    \multicolumn{1}{|c}{
    \includegraphics[height=0.22\linewidth]{fig/deformable_crop_and_resize/subbox_colormap.png}} \\
    \includegraphics[height=0.35\linewidth]{fig/deformable_crop_and_resize/zebra.png} &
    \includegraphics[height=0.35\linewidth]{fig/deformable_crop_and_resize/skate.png} &
    \includegraphics[height=0.35\linewidth]{fig/deformable_crop_and_resize/person.png} \\
  \end{tabular}
  \caption{Visualization of learned deformed sub-boxes. The 49 (arranged in a  grid) sub-boxes (each has size ) are color-coded w.r.t. the top right panel (\eg, the top-left sub-box is represented by light blue color). Our ``deformable crop and resize'' tend to learn circle-shaped context for box classification.}
  \label{fig:vis_deformed_crop_and_resize}
\end{figure}

\begin{figure*}[!t]
  \centering
  \scalebox{1}{
  \begin{tabular}{c}
    \includegraphics[width=0.95\linewidth]{fig/all_results_1} \\
  \end{tabular}
  }
  \caption{Visualization results on the \textit{minival} set. As shown in the figure (particularly, last row), our failure mode comes from two parts: (1) detection failure (missed-detection and wrong classification), and (2) failure to capture sharp object boundary.}
  \label{fig:vis_val}
\end{figure*}
