\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{upgreek} 
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{pifont}\usepackage{mathtools}\usepackage{caption}



\newcommand{\hjk}[1]{{\color{black}#1}}
\newcommand{\jhp}[1]{{\color{black}#1}}

\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}






\def\cvprPaperID{*****} \def\confName{CVPR}
\def\confYear{2022}


\newcommand{\posE}{\text{p}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\enc}{\textbf{enc}}
\newcommand{\dec}{\textbf{dec}}

\newcommand{\paths}{\mathcal{P}}

\newcommand{\perm}{\sigma}
\newcommand{\perminv}{\widetilde{\sigma}}
\newcommand{\feature}{\boldsymbol{F}}

\newcommand{\comb}[2]{{}_{#1}\mathrm{C}_{#2}}

\newcommand{\xmark}{\ding{55}}
\newcommand{\scone}{{AP}${}^{}_{\text{role1}}$}
\newcommand{\sctwo}{{AP}${}^{}_{\text{role2}}$} \begin{document}

\title{Consistency Learning via Decoding Path Augmentation \\for Transformers in Human Object Interaction Detection}  

\maketitle
\thispagestyle{empty}
\appendix
\hjk{We thank all three reviewers for their constructive comments and  supporting the novelty of our paper. 
We will address all the concerns raised by the reviewers below.


\jhp{\noindent\textbf{[R1] Q1. The paper can be clearer if additional examples generated by the proposed method is provided.}}
\newline
Section 3.3 of the main paper discussed how to calculate the consistency loss between the main path $\mathcal{P}_1$ and an augmented path $\mathcal{P}_2$.
We here provide another example of the consistency loss between augmented paths $\mathcal{P}_3: x\rightarrow\text{HI}\rightarrow\text{O}$ and $\mathcal{P}_4: x\rightarrow\text{OI}\rightarrow\text{H}$.
Our framework generates $e_{3,1}$ at the first decoding stage on $\mathcal{P}_3$, and applies a readout function to predict human and interaction, \ie, $\hat{y}_3^h=\text{FFN}_h^{\mathcal{P}_3}(e_{3,1})$ and $\hat{y}_3^i=\text{FFN}_i^{\mathcal{P}_3}(e_{3,1})$. 
Then our model decodes $e_{3,1}$ again at the second decoding stage and produces $e_{3,2}$. 
The prediction for a corresponding object is obtained by another readout function \ie, $\hat{y}_3^o=\text{FFN}_2^{\mathcal{P}_3}(e_{3,2})$.
Similarly, for $\mathcal{P}_4$, $(\hat{y}_4^o, \hat{y}_4^{act})$ and $\hat{y}_4^h$ are obtained at the first and second decoding stages, respectively.
Using predictions from $\mathcal{P}_3$ and $\mathcal{P}_4$, the consistency loss is calculated as follows:
\setlength{\abovedisplayskip}{-1pt}
\begin{equation}
\begin{split}
     \mathcal{L}_{\paths_{3}\paths_4}=\lambda_h\cdot\mathcal{L}_{h}\big(\hat{y}_3^{h},\hat{y}_4^{h}\big)  +\lambda_o\cdot
     \mathcal{L}_{o}\big(\hat{y}_3^{o},\hat{y}_4^{o}\big) \\ +\lambda_{act}\cdot\mathcal{L}_{act}\big(\hat{y}_3^{act},\hat{y}_4^{act}\big). 
\end{split}
\end{equation}
In the final version, we will include the details and (qualitative) detection results of all paths in the supplement.}





\hjk{\noindent\textbf{[R1] Q2. Relatively small improvements in HICO-DET compared to V-COCO.}\newline
The proposed method is a novel training strategy that boosts the performance of transformer networks without any additional labels and architectural changes.
Since the augmented decoding paths and task-specific readout functions are only used during training, at test time the model architecture and size are exactly the same as the original model. In short, no computation and memory overhead at test time.  
Considering this fact, the consistent performance gain across multiple architectures is surprising and significant. 
In the HICO-DET dataset, the improvement in the Full setting seems relatively small, but in the Rare classes the proposed method significantly improved HOTR and QPIC by 5.49 mAP and 1.29 mAP, respectively.}






\hjk{
\noindent\textbf{[R2] Q1. Merge contributions in introduction section.}\newline
Thank you for the suggestion. As suggested, we will polish the summary of our contributions in the final version.

\noindent\textbf{[R2] Q2. Will the code be publicly  available?}\newline
Yes, the code will be released if accepted.



\noindent\textbf{[R2] Q3. Limitations described in conclusion section.}\newline
We will discuss limitations in a separate section or subsection in the final version.

\noindent\textbf{[R3] Q1. Insufficient theoretical analysis.}\newline
Our proposed method is inspired by cross-task consistency~\cite{zamir2020robust} using taskonomy~\cite{zamir2018taskonomy}, which received the best paper award in CVPR.
Although those papers and our paper do not have theoretical analysis or mathematical proofs, we believe that experimental results may demonstrate the effectiveness of the proposed method and support the value of new findings. 
To our best knowledge, our framework is the first work that empirically proves that the consistency between various decoding paths boosts learning of transformers.
As suggested, theoretical analysis of our framework is an interesting future direction of this work.}


\hjk{
\noindent\textbf{[R3] Q2. Why does QPIC with a shared decoder underperform the vanialla QPIC?}\newline
We found that decoder sharing is effective when the encoder is frozen. 
In the main paper, we followed the training procedure of vanilla models; HOTR is trained with a frozen encoder and QPIC trains both encoder and decoder together. 
So, in Table 4 of the main paper, decoder sharing improves the performance of HOTR whereas QPIC shows degradation. 
We additionally conducted ablation studies for QPIC on V-COCO with frozen backbone and encoder networks. 
\vspace{-1.8mm}

\begin{table}[h]
    \centering
\footnotesize
\captionsetup{aboveskip=2pt}
    \begin{tabular}{c c  c  c}
         \toprule
          
\multirow{1}{*}{\textit{Share Dec.}} &
         \multirow{1}{*}{$\  $\textit{CPC}$\  $}& 
         \multirow{1}{*}{\textbf{QPIC} ($\mathcal{P}_1$)} &
        
         \multirow{1}{*}{\textbf{HOTR} ($\mathcal{P}_1$)} 
         
         \\
         
         \midrule    
      
         
      
         
         
         \checkmark &\checkmark &
         \textbf{65.0} \color{blue}(+1.4)&\textbf{61.6 } \color{blue}(+1.8) \\
         
        
          &\checkmark & 
         64.1 \color{blue}(+0.5) & 61.2 \color{blue}(+1.4) \\
         
        
         \checkmark  & &
         64.0 \color{blue}(+0.4)&60.6 \color{blue}(+0.8)
         \\
         
           & &
         63.6 \color{blue}(+0.0)&59.8 \color{blue}(+0.0)
         \\
         
        
         
         \bottomrule
    \end{tabular}
    \caption{
Ablations for decoder sharing and CPC in $\mathcal{P}_1$.}
    \label{table:abl_rebuttal}
\end{table}









         


        


        






         
        


         


         




         
 \vspace{-4mm}
QPIC with a frozen encoder network shows improvement (+0.4 AP) solely by decoder sharing. 
CPC improves QPIC by (+0.5 AP) and when both decoder sharing and CPC are applied, QPIC is improved by (+1.4 AP).
We conjecture that the competition between different paths (or tasks) may hurt performance of a transformer with a shared decoder when its encoder is not frozen.}
\newline




\vspace{-4mm}
\hjk{
\noindent\textbf{[R3] Q3. Why is it reasonable to share the decoder parameters between other subtasks?} 
\newline
Transformers with a shared decoder for multiple tasks have been studied in the literature. 
For instance, MDETR~\cite{Kamath_2021_ICCV} used one shared decoder for multiple tasks: object detection, and visual question answering. UniT~\cite{Hu_2021_ICCV} studied a Unified Transformer model with a shared decoder to learn multiple tasks across different domains: object detection, natural language understanding, and multimodal reasoning.
More importantly, Table~\ref{table:abl_rebuttal} clearly shows 
that sharing decoder is reasonble and beneficial in our setting.
}







\vspace{-7pt}
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
