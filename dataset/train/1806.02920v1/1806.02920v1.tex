

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2018}



\icmltitlerunning{GAIN: Missing Data Imputation using Generative Adversarial Nets}

\begin{document}

\twocolumn[
\icmltitle{GAIN: Missing Data Imputation using Generative Adversarial Nets}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jinsung Yoon}{to,equal}
\icmlauthor{James Jordon}{goo,equal}
\icmlauthor{Mihaela van der Schaar}{to,goo,ati}
\end{icmlauthorlist}

\icmlaffiliation{to}{University of California, Los Angeles, CA, USA}
\icmlaffiliation{goo}{University of Oxford, UK}
\icmlaffiliation{ati}{Alan Turing Institute, UK}


\icmlcorrespondingauthor{Jinsung Yoon}{jsyoon0823@gmail.com}



\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator () observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator () then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that  forces  to learn the desired distribution, we provide  with some additional information in the form of a {\em hint} vector. The hint reveals to  {\em partial} information about the missingness of the original sample, which is used by  to focus its attention on the imputation quality of particular components. This hint ensures that  does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.

\end{abstract}

\section{Introduction}\label{sec:introduction}
Missing data is a pervasive problem. Data may be missing because it was never collected, records were lost or for many other reasons. In the medical domain, the respiratory rate of a patient may not have been measured (perhaps because it was deemed unnecessary/unimportant) or accidentally not recorded \cite{yoon_jbhi,ahmed_tbme}. It may also be the case that certain pieces of information are difficult or even dangerous to acquire (such as information gathered from a biopsy), and so these were not gathered for those reasons \cite{yoon_plosone}. An imputation algorithm can be used to estimate missing values based on data that was observed/measured, such as the systolic blood pressure and heart rate of the patient \cite{yoon_deep}. A substantial amount of research has been dedicated to developing imputation algorithms for medical data \cite{Medimpute1,Medimpute2,Medimpute3,Medimpute4}. Imputation algorithms are also used in many other applications such as image concealment, data compression, and counterfactual estimation \cite{Rubin,Missing_Book,yoon_ganite}. 

Missing data can be categorized into three types: (1) the data is missing completely at random (MCAR) if the missingness occurs entirely at random (there is no dependency on any of the variables), (2) the data is missing at random (MAR) if the missingness depends only on the {\em observed} variables\footnote{A formal definition of MAR can be found in the Supplementary Materials.}, (3) the data is missing not at random (MNAR) if the missingness is neither MCAR nor MAR (more specifically, the data is MNAR if the missingness depends on {\em both observed} variables and the {\em unobserved} variables; thus, missingness cannot be fully accounted for by the observed variables). In this paper we provide theoretical results for our algorithm under the MCAR assumption, and compare to other state-of-the-art methods in this setting\footnote{Empirical results for the MAR and MNAR settings are shown in the Supplementary Materials.}.


State-of-the-art imputation methods can be categorized as either discriminative or generative. Discriminative methods include MICE \cite{MICE,MICE-R}, MissForest \cite{missforest}, and matrix completion \cite{Mat-0,Mat-1,Mat-2,Mat-3}; generative methods include algorithms based on Expectation Maximization \cite{EM} and algorithms based on deep learning (e.g. denoising autoencoders (DAE) and generative adversarial nets (GAN)) \cite{DAE,autoencoder,GAN-imagecomplete}. However, current generative methods for imputation have various drawbacks. For instance, the approach for data imputation based on \cite{EM} makes assumptions about the underlying distribution and fails to generalize well when datasets contain mixed categorical and continuous variables. In contrast, the approaches based on DAE \cite{DAE} have been shown to work well in practice but require complete data during training. In many circumstances, missing values are part of the inherent structure of the problem so obtaining a complete dataset is impossible. Another approach with DAE \cite{autoencoder} allows for an incomplete dataset; however, it only utilizes the observed components to learn the representations of the data. \cite{GAN-imagecomplete} uses Deep Convolutional GANs for image completion; however, it also requires complete data for training the discriminator.

In this paper, we propose a novel imputation method, which we call Generative Adversarial Imputation Nets (GAIN), that generalizes the well-known GAN \cite{GAN} and is able to operate successfully even when complete data is unavailable. In GAIN, the generator's goal is to accurately impute missing data, and the discriminator's goal is to distinguish between observed and imputed components. The discriminator is trained to minimize the classification loss (when classifying which components were observed and which have been imputed), and the generator is trained to maximize the discriminator's misclassification rate. Thus, these two networks are trained using an adversarial process. To achieve this goal, GAIN builds on and adapts the standard GAN architecture. To ensure that the result of this adversarial process is the desired target, the GAIN architecture provides the discriminator with additional information in the form of ``hints". This hinting ensures that the generator generates samples according to the true underlying data distribution.

    \begin{figure}[t!]
        \centering
        \includegraphics[width=0.48\textwidth]{GAN_Model.pdf}
        \caption{The architecture of GAIN}
        \label{fig:model}
    \end{figure}
    
\section{Problem Formulation} \label{sect:problem_formulate}
Consider a -dimensional space . Suppose that  is a random variable (either continuous or binary) taking values in , whose distribution we will denote . Suppose that  is a random variable taking values in . We will call  the data vector, and  the mask vector.  

For each , we define a new space  where  is simply a point not in any , representing an unobserved value. Let .  We define a new random variable  in the following way:

so that  indicates which components of  are observed. Note that we can recover  from .

Throughout the remainder of the paper, we will often use lower-case letters to denote realizations of a random variable and use the notation  to denote a vector of s, whose dimension will be clear from the context (most often, ).

\subsection{Imputation}
In the imputation setting,  i.i.d. copies of  are realized, denoted  and we define the dataset , where  is simply the recovered realization of  corresponding to . 

Our goal is to {\em impute} the unobserved values in each . Formally, we want to generate samples according to , the conditional distribution of  given , for each , to fill in the missing data points in . By attempting to model the {\em distribution} of the data rather than just the expectation, we are able to make multiple draws and therefore make {\em multiple imputations} allowing us to capture the uncertainty of the imputed values \cite{MICE, MICE-R, Rubin}.

\section{Generative Adversarial Imputation Nets}\label{sect:gain}
In this section we describe our approach for simulating  which is motivated by GANs. We highlight key similarities and differences to a standard (conditional) GAN throughout. Fig. \ref{fig:model} depicts the overall architecture.
\subsection{Generator}
The generator, , takes (realizations of) ,  and a noise variable, , as input and outputs , a vector of imputations. Let  be a function, and  be d-dimensional noise (independent of all other variables).

Then we define the random variables  by

where  denotes element-wise multiplication.  corresponds to the vector of {\em imputed} values (note that  outputs a value for every component, even if its value was observed) and  corresponds to the completed data vector, that is, the vector obtained by taking the partial observation  and replacing each  with the corresponding value of .

This setup is very similar to a standard GAN, with  being analogous to the noise variables introduced in that framework. Note, though, that in this framework, the target distribution, , is essentially -dimensional and so the noise we pass into the generator is , rather than simply , so that its dimension matches that of the targeted distribution.

\subsection{Discriminator}
As in the GAN framework, we introduce a discriminator, , that will be used as an adversary to train . However, unlike in a standard GAN where the output of the generator is either {\em completely} real or {\em completely} fake, in this setting the output is comprised of some components that are real and some that are fake. Rather than identifying that an entire vector is real or fake, the discriminator attempts to distinguish which {\em components} are real (observed) or fake (imputed) - this amounts to predicting the mask vector, . Note that the mask vector  is pre-determined by the dataset.

Formally, the discriminator is a function  with the -th component of  corresponding to the probability that the -th component of  was observed.

\subsection{Hint} \label{subsec:hint}
As will be seen in the theoretical results that follow, it is necessary to introduce what we call a hint mechanism. A hint mechanism is a random variable, , taking values in a space , both of which {\em we define}. We allow  to depend on  and for each (imputed) sample , we draw  according to the distribution . We pass  as an additional input to the discriminator and so it becomes a function , where now the -th component of  corresponds to the probability that the -th component of  was observed conditional on  {\em and} .

By defining  in different ways, we control the amount of information contained in  about  and in particular we show (in Proposition \ref{prop:nonunique}) that if we do not provide ``enough" information about  to  (such as if we simply did not have a hinting mechanism), then there are several distributions that  could reproduce that would all be optimal with respect to .

\subsection{Objective}
We train  to {\em maximize} the probability of correctly predicting . We train  to {\em minimize} the probability of  predicting . We define the quantity  to be


where  is element-wise logarithm and dependence on  is through .

Then, as with the standard GAN, we define the objective of GAIN to be the minimax problem given by


We define the loss function  by

Writing , we can then rewrite (\ref{eq:obj}) as


\section{Theoretical Analysis} \label{sec:theory}
In this section we provide a theoretical analysis of (\ref{eq:obj}). Given a d-dimensional space , a (probability) density\footnote{For ease of exposition, we use the term density even when referring to a probability mass function.}  over  corresponding to a random variable , and a vector  we define the set , the projection  by  and the density  to be the density of .

Throughout this section, we make the assumption that  is independent of , i.e. that the data is MCAR.

We will write  to denote the density of the random variable  and we will write ,  and  to denote the marginal densities (of ) corresponding to ,  and , respectively. When referring to the joint density of two of the three variables (potentially conditioned on the third), we will simply use , abusing notation slightly.

It is more intuitive to think of this density through its decomposition into densities corresponding to the true data generating process, and to the generator defined by (\ref{eq:xbar}),

The first two terms in (\ref{eq:decomp}) are both defined by the data, where  is the density of  which corresponds to the density of  (i.e. the true data distribution), since conditional on ,  (see equations \ref{eq:xtilde} and \ref{eq:xhat}). The third term, , is determined by the generator, , and is the density of the random variable  where  is determined by  and . The final term is the conditional density of the hint, which we are free to define (its selection will be motivated by the following analysis).

Using this decomposition, one can think of drawing a sample from  as first sampling  according to , then sampling the ``observed" components, , according to  (we can then construct  from  and ), then {\em generating} the imputed values, , from the generator according to  and finally sampling the hint according to .

\begin{lemma} \label{lem:D}
	Let . Let  be a fixed density over the hint space  and let  be such that .	Then for a fixed generator, , the -th component of the optimal discriminator,  is given by
	
	for each .
\end{lemma}
\begin{proof}
	All proofs are provided in Supplementary Materials.
\end{proof}

We now rewrite (\ref{eq:V}), substituting for , to obtain the following minimization criterion for :

where dependence on  is through .

\begin{theorem} \label{thm:main}
 	A global minimum for  is achieved if and only if the density  satisfies
	
	for each ,  and  such that .
\end{theorem}

The following proposition asserts that if  does not contain ``enough" information about , we cannot guarantee that  learns the desired distribution (the one uniquely defined by the (underlying) data).

\begin{proposition} \label{prop:nonunique}
	There exist distributions of ,  and  for which solutions to (\ref{eq:phat}) are not unique. In fact, if  is independent of , then (\ref{eq:phat}) does not define a unique density, in general.
\end{proposition}

Let the random variable  be defined by first sampling  from  uniformly at random and then setting 

Let  and, given , define

Observe first that  is such that  for  but that  implies nothing about . In other words,  reveals all but one of the components of  to . Note, however, that  does contain some information about  since  is not assumed to be independent of the other components of .

The following lemma confirms that the discriminator behaves as we expect with respect to this hint mechanism.

\begin{lemma} \label{lem:DH}
	Suppose  is defined as above. Then for  such that  we have  and for  such that  we have , for all , .
\end{lemma}

The final proposition we state tells us that  as specified above ensures the generator learns to replicate the desired distribution.

\begin{proposition}	\label{prop:h}
	Suppose  is defined as above. Then the solution to (\ref{eq:phat}) is unique and satisfies
	
	for all . In particular,  and since  is independent of ,  is the density of . The distribution of  is therefore the same as the distribution of .
\end{proposition}

For the remainder of the paper,  and  will be defined as in equations (\ref{eq:bdef}) and (\ref{eq:hdef}).

\section{GAIN Algorithm}\label{sect:gain_algorithm}

Using an approach similar to that in \cite{GAN}, we solve the minimax optimization problem (\ref{eq:obj}) in an iterative manner. Both  and  are modeled as fully connected neural nets.

We first optimize the discriminator  with a fixed generator  using mini-batches of size \footnote{Details of hyper-parameter selection can be found in the Supplementary Materials.}. For each sample in the mini-batch, \footnote{The index  now corresponds to the -th sample of the mini-batch, rather than the -th sample of the entire dataset.}, we draw  independent samples,  and , of  and  and compute  and  accordingly. Lemma \ref{lem:DH} then tells us that the only outputs of  that depend on  are the ones corresponding to  for each sample. We therefore only train  to give us these outputs (if we also trained  to match the outputs specified in Lemma \ref{lem:DH} we would gain no information about , but  would overfit to the hint vector). We define  by

 is then trained according to

recalling that .

Second, we optimize the generator  using the newly updated discriminator  with mini-batches of size . We first note that  in fact outputs a value for the {\em entire} data vector (including values for the components we observed). Therefore, in training , we not only ensure that the imputed values for missing components () successfully fool the discriminator (as defined by the minimax game), we also ensure that the values outputted by  for observed components () are close to those actually observed. This is justified by noting that the conditional distribution of  given  obviously fixes the components of  corresponding to  to be . This also ensures that the representations learned in the hidden layers of  suitably capture the information contained in  (as in an auto-encoder).


\begin{algorithm}[h!]
	\caption{Pseudo-code of GAIN}\label{alg:pseudo}
	\begin{algorithmic} 
		\WHILE {training loss has not converged}
		\STATE \textbf{(1) Discriminator optimization} 
		\STATE Draw  samples from the dataset 
		\STATE Draw  i.i.d. samples, , of 
		\STATE Draw  i.i.d. samples, , of 
		\FOR {}
		\STATE 
		\STATE 
		\STATE 
		\ENDFOR
		\STATE Update  using stochastic gradient descent (SGD) 
		
		\STATE \textbf{(2) Generator optimization} 
		\STATE Draw  samples from the dataset 
		\STATE Draw  i.i.d. samples,  of 
		\STATE Draw  i.i.d. samples,  of 
		\FOR {}
		\STATE 
		\ENDFOR
		\STATE Update  using SGD (for fixed ) 
		
		\ENDWHILE
	\end{algorithmic} 
\end{algorithm}



    \begin{table*}[t!]
        \renewcommand{\arraystretch}{1.3}
        \caption{Source of gains in GAIN algorithm (Mean  Std of RMSE (Gain (\%)))}    
        \label{table:sourceofgain}
        \centering
        \begin{tabular}{ |c|| c | c  |  c| c | c |  }
            \toprule
            \textbf{Algorithm}    & \textbf{Breast} &  \textbf{Spam} & \textbf{Letter}& \textbf{Credit} & \textbf{News}   \\ \midrule
            \textbf{GAIN} & \textbf{.0546  .0006} & \textbf{.0513 .0016} & \textbf{.1198 .0005}  & \textbf{.1858  .0010} & \textbf{.1441  .0007}  \\   \midrule
            GAIN w/o  & .0701  .0021  & .0676  .0029 & .1344  .0012  & .2436  .0012 & .1612  .0024   \\ 
             & \textbf{(22.1\%)} & \textbf{(24.1\%) }& \textbf{(10.9\%) }& \textbf{(23.7\%)} & \textbf{(10.6\%)} \\  \midrule
            GAIN w/o   & .0767  .0015 & .0672  .0036 & .1586  .0024 & .2533  .0048 & .2522  .0042  \\
            & \textbf{(28.9\%)} & \textbf{(23.7\%) }& \textbf{(24.4\%) }& \textbf{(26.7\%)} & \textbf{(42.9\%)}\\   \midrule
            GAIN w/o  & .0639  .0018 & .0582  .0008 & .1249  .0011  & .2173  .0052 & .1521  .0008   \\ 
            Hint & \textbf{(14.6\%)} & \textbf{(11.9\%)} & \textbf{(4.1\%)} & \textbf{(14.5\%)} & \textbf{(5.3\%)} \\  \midrule
            GAIN w/o   &.0782  .0016  & .0700  .0064& .1671  .0052 & .2789  .0071& .2527  .0052  \\
            Hint \&  & \textbf{(30.1\%)} & \textbf{(26.7\%)} & \textbf{(28.3\%)} & \textbf{(33.4\%)} & \textbf{(43.0\%)}\\
            \bottomrule
        \end{tabular}
    \end{table*}

To achieve this, we define two different loss functions. The first, , is given by

and the second, , by

where

As can be seen from their definitions,  will apply to the missing components () and  will apply to the observed components ().

 is smaller when  is closer to  for  such that . That is,  is smaller when  is less able to identify the imputed values as being imputed (it falsely categorizes them as observed).  is minimized when the reconstructed features (i.e. the values  outputs for features that were observed) are close to the actually observed features.

 is then trained to minimize the weighted sum of the two losses as follows:

where  is a hyper-parameter. 

The pseudo-code is presented in Algorithm \ref{alg:pseudo}.


\section{Experiments}\label{sect:experiments}
In this section, we validate the performance of GAIN using multiple real-world datasets. In the first set of experiments we qualitatively analyze the properties of GAIN. In the second we quantitatively evaluate the imputation performance of GAIN using various UCI datasets \cite{UCI}, giving comparisons with state-of-the-art imputation methods. In the third we evaluate the performance of GAIN in various settings (such as on datasets with different missing rates). In the final set of experiments we evaluate GAIN against other imputation algorithms when the goal is to perform prediction on the imputed dataset.

We conduct each experiment 10 times and within each experiment we use 5-cross validations. We report either RMSE or AUROC as the performance metric along with their standard deviations across the 10 experiments. Unless otherwise stated, missingness is applied to the datasets by randomly removing 20\% of all data points (MCAR).

\subsection{Source of gain}
The potential sources of gain for the GAIN framework are: the use of a GAN-like architecture (through ), the use of reconstruction error in the loss (), and the use of the hint (). In order to understand how each of these affects the performance of GAIN, we exclude one or two of them and compare the performances of the resulting architectures against the full GAIN architecture.

Table \ref{table:sourceofgain} shows that the performance of GAIN is improved when all three components are included. More specifically, the full GAIN framework has a 15\% improvement over the simple auto-encoder model (i.e. GAIN w/o ). Furthermore, utilizing the hint vector additionally gives improvements of 10\%.


    \begin{table*}[t!]
        \renewcommand{\arraystretch}{1.3}
        \caption{Imputation performance in terms of RMSE (Average  Std of RMSE)}    
        \label{table:impute}
        \centering
        \begin{tabular}{ |c|| c | c  |  c| c | c |   }
            \toprule
            \textbf{Algorithm}    & \textbf{Breast} &  \textbf{Spam} & \textbf{Letter}& \textbf{Credit} & \textbf{News}   \\ \midrule
            \textbf{GAIN} & \textbf{.0546  .0006} & \textbf{.0513 .0016} & \textbf{.1198 .0005}  & \textbf{.1858  .0010} & \textbf{.1441  .0007}  \\  \midrule
            MICE  & .0646  .0028 & .0699  .0010 & .1537  .0006  & .2585  .0011 & .1763  .0007  \\ 
            MissForest  & .0608  .0013 & .0553  .0013 & .1605  .0004  & .1976  .0015 & .1623  0.012
             \\ 
            Matrix  & .0946  .0020 & .0542  .0006 & .1442  .0006  & .2602  .0073 & .2282  .0005 \\
            Auto-encoder  & .0697  .0018 & .0670  .0030 & .1351  .0009  & .2388  .0005 & .1667  .0014   \\
            EM & .0634  .0021 & .0712  .0012 & .1563  .0012  & .2604  .0015 & .1912  .0011   \\ \bottomrule
        \end{tabular}
    \end{table*}
    
\subsection{Quantitative analysis of GAIN}
We use five real-world datasets from UCI Machine Learning Repository
\cite{UCI} (Breast, Spam, Letter, Credit, and News) to quantitatively evaluate the imputation performance of GAIN. Details of each dataset can be found in the Supplementary Materials.

In table \ref{table:impute} we report the RMSE (and its standard deviation) for GAIN and 5 other state-of-the-art
imputation methods: MICE \cite{MICE,MICE-R}, MissForest \cite{missforest},
Matrix completion (Matrix) \cite{Mat-0}, Auto-encoder \cite{autoencoder}
and Expectation-maximization (EM) \cite{EM}. As can be seen from the table, GAIN significantly outperforms each benchmark. Results for the imputation quality of categorical variables in this experiment are given in the Supplementary Materials.

\subsection{GAIN in different settings}
    \begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Various.pdf}
    \caption{RMSE performance in different settings: (a) Various missing rates, (b) Various number of samples, (c) Various feature dimensions}
    \label{fig:setting}
    \end{figure*}
To better understand GAIN, we conduct several experiments in which we vary the
missing rate, the number of samples, and the number of dimensions
using Credit dataset. Fig. \ref{fig:setting} shows the performance
(RMSE) of GAIN within these different settings in comparison to the two most competitive benchmarks (MissForest and Auto-encoder). Fig. \ref{fig:setting} (a) shows that, even though the performance of each algorithm decreases as missing rates increase, GAIN consistently outperforms the benchmarks across the entire range of missing rates.

Fig. \ref{fig:setting} (b) shows that as the number of samples increases, the performance improvements of GAIN over the benchmarks also increases. This is due to the large number of parameters in GAIN that need to be optimized, however, as demonstrated on the Breast dataset (in Table \ref{table:impute}), GAIN is still able to outperform the benchmarks even when the number of samples is relatively small.

Fig. \ref{fig:setting} (c) shows that GAIN is also robust to the number of feature dimensions. On the other hand, the discriminative model (MissForest) cannot as easily cope when the number of feature dimensions is small.

\subsection{Prediction Performance}
We now compare GAIN against the same benchmarks with respect to the accuracy of post-imputation prediction. For this purpose, we use Area Under the Receiver Operating Characteristic Curve (AUROC) as the measure of performance. To be fair to all methods, we use the same predictive model (logistic regression) in all cases.

Comparisons are made on all datasets except Letter (as it has multi-class labels) and the results are reported in Table \ref{tab:Prediction}.

\begin{table*}[t!]
	\renewcommand{\arraystretch}{1.3}
	\caption{Prediction performance comparison}
	\label{tab:Prediction}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\toprule
		\multirow{2}{*}{\textbf{Algorithm}}&\multicolumn{4}{c|}{\textbf{AUROC (Average  Std)}}  \\
		\cmidrule{2-5}
		& \textbf{Breast} & \textbf{Spam} &  \textbf{Credit} & \textbf{News} \\
		\midrule
		\textbf{GAIN}& \textbf{.9930  .0073} & \textbf{.9529  .0023}  & \textbf{.7527  .0031} & \textbf{.9711  .0027}  \\
		\midrule
		MICE & .9914  .0034  & .9495  .0031 & .7427  .0026& .9451  .0037\\
		MissForest  & .9860  .0112&.9520  .0061&.7498  .0047& .9597  .0043 \\
		Matrix   & .9897  .0042 & .8639  .0055 & .7059  .0150 & .8578  .0125  \\
		Auto-encoder& .9916  .0059 & .9403  .0051 & .7485  .0031 & .9321  .0058 \\
		EM& .9899  .0147 & .9217  .0093 & .7390  .0079 & .8987  .0157 \\
		\bottomrule
	\end{tabular}
\end{table*}

As Table \ref{tab:Prediction} shows, GAIN, which we have already shown to achieve the best imputation accuracy (in Table \ref{table:impute}), yields the best post-imputation prediction accuracy. However, even in cases where the improvement in imputation accuracy is large, the  improvements in prediction accuracy are not always significant. This is probably due to the fact that there is sufficient information in the (80\%) observed data to predict the label.

\begin{figure}[t!]
	\center
	\includegraphics[width=0.5\textwidth]{Prediction_Missing_Rate.pdf}
	\caption{The AUROC performance with various missing rates with Credit dataset}
	\label{fig:prediction_missing_rate}
\end{figure}

\textbf{Prediction accuracy with various missing rates: }In this experiment, we evaluate the post-imputation prediction performance when the missing rate of the dataset is varied. Note that every dataset (except Letter) has their own binary label.

The results of this experiment (for GAIN and the two most competitive benchmarks) are shown in Fig. \ref{fig:prediction_missing_rate}. In particular, the performance of GAIN is significantly better than the other two for higher missing rates, this is due to the fact that as the information contained in the observed data decreases (due to more values being missing), the imputation quality becomes more important, and GAIN has already been shown to provide (significantly) better quality imputations.

\subsection{Congeniality of GAIN}



The congeniality of an imputation model is its ability to impute values that respect the feature-label relationship \cite{congeniality_meng, reason_of_imputation1, reason_of_imputation2}. The congeniality of an imputation model can be evaluated by measuring the effects on the feature-label relationships after the imputation. We compare the logistic regression parameters, , learned from the complete Credit dataset with the parameters, , learned from an incomplete Credit dataset by first imputing and then performing logistic regression.

We report the mean and standard deviation of both the mean bias  and the mean square error  for each method in Table \ref{tab:congeniality}. These quantities being lower indicates that the imputation algorithm better respects the relationship between feature and label. As can be seen in the table, GAIN achieves significantly lower mean bias and mean square error than other state-of-the-art imputation algorithms (from 8.9\% to 79.2\% performance improvements).

\begin{table}[t!]
	\renewcommand{\arraystretch}{1.3}
	\caption{Congeniality of imputation models}
	\label{tab:congeniality}
	\centering
	\begin{tabular}{|c|c|c|}
		\toprule
		\multirow{2}{*}{\textbf{Algorithm}} & \textbf{Mean Bias} & \textbf{MSE} \\
		&  & \\
		\midrule
		\textbf{GAIN} & \textbf{0.3163 0.0887} & \textbf{0.5078 0.1137} \\ \midrule
		MICE   & 0.8315  0.2293 & 0.9467  0.2083 \\
		MissForest & 0.6730  0.1937 & 0.7081  0.1625 \\
		Matrix & 1.5321  0.0017 & 1.6660  0.0015 \\
		Auto-encoder & 0.3500  0.1503 & 0.5608 0.1697  \\
		EM  & 0.8418  0.2675 & 0.9369  0.2296 \\
		\bottomrule
	\end{tabular}
\end{table}

\section{Conclusion}\label{sect:conclusion}
We propose a generative model for missing data imputation, GAIN. This novel architecture generalizes the well-known GAN such that it can deal with the unique characteristics of the imputation problem. Various experiments with real-world datasets show that GAIN significantly outperforms state-of-the-art imputation techniques. The development of a new, state-of-the-art technique for imputation can have transformative impacts; most datasets in medicine as well as in other domains have missing data. Future work will investigate the performance of GAIN in recommender systems, error concealment as well as in active sensing \cite{activesensing}. Preliminary results in error concealment using the MNIST dataset \cite{mnist} can be found in the Supplementary Materials - see Fig. 4 and 5. 

\newpage
\section*{Acknowledgement}
The authors would like to thank the reviewers for their helpful comments. The research presented in this paper was supported by the Office of Naval Research (ONR) and the NSF (Grant number: ECCS1462245, ECCS1533983, and ECCS1407712).

\begin{thebibliography}{29}
	\providecommand{\natexlab}[1]{#1}
	\providecommand{\url}[1]{\texttt{#1}}
	\expandafter\ifx\csname urlstyle\endcsname\relax
	\providecommand{\doi}[1]{doi: #1}\else
	\providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi
	
	\bibitem[Alaa et~al.(2018)Alaa, Yoon, Hu, and van~der Schaar]{ahmed_tbme}
	Alaa, A.~M., Yoon, J., Hu, S., and van~der Schaar, M.
	\newblock Personalized risk scoring for critical care prognosis using mixtures
	of gaussian processes.
	\newblock \emph{IEEE Transactions on Biomedical Engineering}, 65\penalty0
	(1):\penalty0 207--218, 2018.
	
	\bibitem[Allen \& Li(2016)Allen and Li]{GAN-imagecomplete}
	Allen, A. and Li, W.
	\newblock \emph{Generative Adversarial Denoising Autoencoder for Face
		Completion}, 2016.
	\newblock URL
	\url{https://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/}.
	
	\bibitem[Barnard \& Meng(1999)Barnard and Meng]{Medimpute1}
	Barnard, J. and Meng, X.-L.
	\newblock Applications of multiple imputation in medical studies: from aids to
	nhanes.
	\newblock \emph{Statistical methods in medical research}, 8\penalty0
	(1):\penalty0 17--36, 1999.
	
	\bibitem[Burgess et~al.(2013)Burgess, White, Resche-Rigon, and
	Wood]{reason_of_imputation1}
	Burgess, S., White, I.~R., Resche-Rigon, M., and Wood, A.~M.
	\newblock Combining multiple imputation and meta-analysis with individual
	participant data.
	\newblock \emph{Statistics in medicine}, 32\penalty0 (26):\penalty0 4499--4514,
	2013.
	
	\bibitem[Buuren \& Groothuis-Oudshoorn(2011)Buuren and
	Groothuis-Oudshoorn]{MICE-R}
	Buuren, S. and Groothuis-Oudshoorn, K.
	\newblock mice: Multivariate imputation by chained equations in r.
	\newblock \emph{Journal of statistical software}, 45\penalty0 (3), 2011.
	
	\bibitem[Buuren \& Oudshoorn(2000)Buuren and Oudshoorn]{MICE}
	Buuren, S.~v. and Oudshoorn, C.
	\newblock Multivariate imputation by chained equations: Mice v1. 0 user's
	manual.
	\newblock Technical report, TNO, 2000.
	
	\bibitem[Deng et~al.(2016)Deng, Chang, Ido, and Long]{reason_of_imputation2}
	Deng, Y., Chang, C., Ido, M.~S., and Long, Q.
	\newblock Multiple imputation for general missing data patterns in the presence
	of high-dimensional data.
	\newblock \emph{Scientific reports}, 6:\penalty0 21689, 2016.
	
	\bibitem[Garc{\'\i}a-Laencina et~al.(2010)Garc{\'\i}a-Laencina,
	Sancho-G{\'o}mez, and Figueiras-Vidal]{EM}
	Garc{\'\i}a-Laencina, P.~J., Sancho-G{\'o}mez, J.-L., and Figueiras-Vidal,
	A.~R.
	\newblock Pattern classification with missing data: a review.
	\newblock \emph{Neural Computing and Applications}, 19\penalty0 (2):\penalty0
	263--282, 2010.
	
	\bibitem[Gondara \& Wang(2017)Gondara and Wang]{autoencoder}
	Gondara, L. and Wang, K.
	\newblock Multiple imputation using deep denoising autoencoders.
	\newblock \emph{arXiv preprint arXiv:1705.02737}, 2017.
	
	\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
	Warde-Farley, Ozair, Courville, and Bengio]{GAN}
	Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
	S., Courville, A., and Bengio, Y.
	\newblock Generative adversarial nets.
	\newblock In \emph{Advances in Neural information processing systems}, pp.\
	2672--2680, 2014.
	
	\bibitem[Kreindler \& Lumsden(2012)Kreindler and Lumsden]{Missing_Book}
	Kreindler, D.~M. and Lumsden, C.~J.
	\newblock The effects of the irregular sample and missing data in time series
	analysis.
	\newblock \emph{Nonlinear Dynamical Systems Analysis for the Behavioral
		Sciences Using Real Data}, pp.\  135, 2012.
	
	\bibitem[LeCun \& Cortes(2010)LeCun and Cortes]{mnist}
	LeCun, Y. and Cortes, C.
	\newblock {MNIST} handwritten digit database.
	\newblock 2010.
	\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.
	
	\bibitem[Lichman(2013)]{UCI}
	Lichman, M.
	\newblock {UCI} machine learning repository, 2013.
	\newblock URL \url{http://archive.ics.uci.edu/ml}.
	
	\bibitem[Mackinnon(2010)]{Medimpute2}
	Mackinnon, A.
	\newblock The use and reporting of multiple imputation in medical research--a
	review.
	\newblock \emph{Journal of internal medicine}, 268\penalty0 (6):\penalty0
	586--593, 2010.
	
	\bibitem[Mazumder et~al.(2010{\natexlab{a}})Mazumder, Hastie, and
	Tibshirani]{Mat-0}
	Mazumder, R., Hastie, T., and Tibshirani, R.
	\newblock Spectral regularization algorithms for learning large incomplete
	matrices.
	\newblock \emph{Journal of machine learning research}, 11\penalty0
	(Aug):\penalty0 2287--2322, 2010{\natexlab{a}}.
	
	\bibitem[Mazumder et~al.(2010{\natexlab{b}})Mazumder, Hastie, and
	Tibshirani]{Mat-3}
	Mazumder, R., Hastie, T., and Tibshirani, R.
	\newblock Spectral regularization algorithms for learning large incomplete
	matrices.
	\newblock \emph{Journal of machine learning research}, 11\penalty0
	(Aug):\penalty0 2287--2322, 2010{\natexlab{b}}.
	
	\bibitem[Meng(1994)]{congeniality_meng}
	Meng, X.-L.
	\newblock Multiple-imputation inferences with uncongenial sources of input.
	\newblock \emph{Statistical Science}, pp.\  538--558, 1994.
	
	\bibitem[Purwar \& Singh(2015)Purwar and Singh]{Medimpute4}
	Purwar, A. and Singh, S.~K.
	\newblock Hybrid prediction model with missing value imputation for medical
	data.
	\newblock \emph{Expert Systems with Applications}, 42\penalty0 (13):\penalty0
	5621--5631, 2015.
	
	\bibitem[Rubin(2004)]{Rubin}
	Rubin, D.~B.
	\newblock \emph{Multiple imputation for nonresponse in surveys}, volume~81.
	\newblock John Wiley \& Sons, 2004.
	
	\bibitem[Schnabel et~al.(2016)Schnabel, Swaminatan, Singh, Chandak, and
	Joachims]{Mat-2}
	Schnabel, T., Swaminatan, A., Singh, A., Chandak, N., and Joachims, T.
	\newblock Recommendations as treatments: debiasing learning and evolution.
	\newblock \emph{ICML}, 2016.
	
	\bibitem[Stekhoven \& B{\"u}hlmann(2011)Stekhoven and B{\"u}hlmann]{missforest}
	Stekhoven, D.~J. and B{\"u}hlmann, P.
	\newblock Missforestâ€”non-parametric missing value imputation for mixed-type
	data.
	\newblock \emph{Bioinformatics}, 28\penalty0 (1):\penalty0 112--118, 2011.
	
	\bibitem[Sterne et~al.(2009)Sterne, White, Carlin, Spratt, Royston, Kenward,
	Wood, and Carpenter]{Medimpute3}
	Sterne, J.~A., White, I.~R., Carlin, J.~B., Spratt, M., Royston, P., Kenward,
	M.~G., Wood, A.~M., and Carpenter, J.~R.
	\newblock Multiple imputation for missing data in epidemiological and clinical
	research: potential and pitfalls.
	\newblock \emph{BMJ}, 338:\penalty0 b2393, 2009.
	
	\bibitem[Vincent et~al.(2008)Vincent, Larochelle, Bengio, and Manzagol]{DAE}
	Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A.
	\newblock Extracting and composing robust features with denoising autoencoders.
	\newblock In \emph{Proceedings of the 25th International conference on Machine
		learning}, pp.\  1096--1103. ACM, 2008.
	
	\bibitem[Yoon et~al.(2017)Yoon, Davtyan, and van~der Schaar]{yoon_jbhi}
	Yoon, J., Davtyan, C., and van~der Schaar, M.
	\newblock Discovery and clinical decision support for personalized healthcare.
	\newblock \emph{IEEE journal of biomedical and health informatics}, 21\penalty0
	(4):\penalty0 1133--1145, 2017.
	
	\bibitem[Yoon et~al.(2018{\natexlab{a}})Yoon, Jordon, and van~der
	Schaar]{yoon_ganite}
	Yoon, J., Jordon, J., and van~der Schaar, M.
	\newblock {GANITE}: Estimation of individualized treatment effects using
	generative adversarial nets.
	\newblock In \emph{International Conference on Learning Representations},
	2018{\natexlab{a}}.
	\newblock URL \url{https://openreview.net/forum?id=ByKWUeWA-}.
	
	\bibitem[Yoon et~al.(2018{\natexlab{b}})Yoon, Zame, Banerjee, Cadeiras, Alaa,
	and van~der Schaar]{yoon_plosone}
	Yoon, J., Zame, W.~R., Banerjee, A., Cadeiras, M., Alaa, A.~M., and van~der
	Schaar, M.
	\newblock Personalized survival predictions via trees of predictors: An
	application to cardiac transplantation.
	\newblock \emph{PloS one}, 13\penalty0 (3):\penalty0 e0194985,
	2018{\natexlab{b}}.
	
	\bibitem[Yoon et~al.(2018{\natexlab{c}})Yoon, Zame, and van~der
	Schaar]{yoon_deep}
	Yoon, J., Zame, W.~R., and van~der Schaar, M.
	\newblock Deep sensing: Active sensing using multi-directional recurrent neural
	networks.
	\newblock In \emph{International Conference on Learning Representations},
	2018{\natexlab{c}}.
	\newblock URL \url{https://openreview.net/forum?id=r1SnX5xCb}.
	
	\bibitem[Yu et~al.(2016)Yu, Rao, and Dhillon]{Mat-1}
	Yu, H.-F., Rao, H., and Dhillon, I.~S.
	\newblock Temporal regularized matrix factorization for high-dimensional time
	series prediction.
	\newblock \emph{NIPS}, 2016.
	
	\bibitem[Yu et~al.(2009)Yu, Krishnapuram, Rosales, and Rao]{activesensing}
	Yu, S., Krishnapuram, B., Rosales, R., and Rao, R.~B.
	\newblock Active sensing.
	\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  639--646,
	2009.
	
\end{thebibliography}


\end{document}
