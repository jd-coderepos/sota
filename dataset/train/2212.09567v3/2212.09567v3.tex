\appendix
\onecolumn

\section{Conversion Between FOL Expression and Query Computation Tree}
\label{app:conversion}
The conversion from a FOL expression (disjunctive normal form) to its query computation tree involves three steps: dependency graph generation, union branches merging, and variable separation.

\xhdr{Dependency Graph Generation}
Given a FOL expression, we first assign a unique node to each of the variables, and a unique node to the constant entity in each of the one-hop atoms. Note that there might be several nodes that correspond to the same constant entity, since they appear in different one-hop atoms.
Then we use undirected edges to connect the nodes according to the one-hop atoms.
Specifically, if $e^i_j=r(v', v)$ (or $r(c, v)$), then we connect the nodes of $v'$ (or $c$) and $v$ by an edge $r_i$.
Similarly, if $e^i_j=\lnot r(v', v)$ (or $\lnot r(c, v)$), then we connect the nodes of $v'$ (or $c$) and $v$ by an edge $\lnot r_i$.
The notation $i$ here will be used to distinguish the edges from different conjunctions.
The constructed undirected dependency multigraph has to be a tree, in other words, it is a connected acyclic graph.
We take the node $v_?$ as root, and assign directions for the edges such that they all point from child nodes to their parent nodes, during which we have to handle the inverse of relations.
The constant entities are naturally leaf nodes in the tree, since each entity node is only connected to one variable node.

\xhdr{Union Branches Merging}
Then we handle the union structures in the query computation tree.
On the path $\tau$ from root to every leaf node, if exists, we find the first node $v_i$ such that the edges between $v_i$ and its child node $v_j$ are all of the same relation, but in different conjunctions: $r_{t_1}, r_{t_2}, \dots, r_{t_p}$.
We merge these edges into a single edge $r_{t_1, t_2, \dots t_p}$, since they all correspond to the same one-hop atom but in different conjunctions, they can be merged by the distributive law:
\begin{equation}
    (P\land Q)\lor(P\land R) \Leftrightarrow P\land(Q\lor R)
\end{equation}
We assert that there is a subpath from $v_i$ to some $v_k$ within the path $\tau$ that only consists of edges $r_{t_1, t_2, \dots t_p}$, and $v_k$ is connected to different child nodes by relations from conjunctions $t_1, t_2, \dots t_p$.
We mark these edges as union, while the rest of one-to-many structures are marked as intersection.
Now the multigraph becomes a simple graph (no multiple edges).

\xhdr{Variable Separation}
For the one-to-many intersection/union structures in the tree, we separate the parent node $v_k$ into $v^1_k, v^2_k, \dots$ for each of the child nodes, and connect the $i$th child node with $v^i_k$ by its original relational edge, while all $v^1_k, v^2_k, \dots$ connect to $v_k$ by intersection/union edges.
Note that the union branches merging step may create one-to-many structures that consist of both intersection and union edges, take Figure~\ref{fig:conversion} for an example.
This can be taken care of by first separating $v_k$ into a union structure ($v^3_1$ and $v^4_1$ in the example), and then separating the child node into an intersection structure ($v^1_1$ and $v^2_1$ in the example).

We show an example of converting a FOL expression to its query computation tree in Figure~\ref{fig:conversion}. One can verify that the logic expression represented by the query computation tree is equivalent to the FOL expression.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth,trim=0 60 0 20,clip]{figs/conversion.pdf}
\caption{Conversion from a FOL expression to its query computation tree.}
\label{fig:conversion}
\end{figure}

Moreover, we believe that \textbf{query computation tree actually serves as a more appropriate form to represent complex logical questions on KGs}.
Although the FOL expression is more general, most kinds of the FOL expressions are rarely asked in practice.
In fact, query computation tree is more in line with human's multi-hop questioning style --- where an intermediate variable is usually described by its relationships with other entities or their logical combinations, which corresponds to a subtree rooted at the variable in the query computation tree.

\section{Proof of \cref{prop:eq}: Equivalence of Optimization on the Query Computation Tree}
\label{app:prop}

\prop*
\begin{proof}
    Recall that $T(v_?)$ denotes the truth value of the query computation tree rooted at $v_?$, thus to prove the equivalence between the two optimization problems, it suffices to show that $T(v_?)=T(Q(v_?))$ where $T(q(v_?))$ is the truth value of the FOL expression corresponding to the query computation tree rooted at $v_?$.
    We prove it by induction. Assume the equation holds for the child nodes of $v_?$, then for $v_?$, consider the 4 types of edges connecting it to its child nodes.
    For type \Roman{1} (intersection), then according to Eq.~\ref{eq:qt1}, it holds that
    \begin{equation}
        Q(v_?) = \land_{1\leq i\leq K}Q(v^i_?)\ \Rightarrow\ 
        T(Q(v_?)) = \top_{1\leq i\leq K}T(v^i_?) = T(v_?)
    \end{equation}
    For type \Roman{2} (union), according to Eq.~\ref{eq:qt1}, we also have
    \begin{equation}
        Q(v_?) = \lor_{1\leq i\leq K}Q(v^i_?)\ \Rightarrow\ 
        T(Q(v_?)) = \bot_{1\leq i\leq K}T(v^i_?) = T(v_?)
    \end{equation}
    For type \Roman{3} (relational projection), according to Eq.~\ref{eq:qt2},
    \begin{equation}
        Q(v_?) = r(c, v_?)\text{ or }Q(v_k)\land r(v_k, v_?) \ \Rightarrow\ 
        T(Q(v_?)) = r(c, v_?)\text{ or }T(Q(v_k))\ \top\ r(v_k, v_?) = T(v_?)
    \label{eq:app3}
    \end{equation}
    Similarly, for type \Roman{4} (anti-relational projection), according to Eq.~\ref{eq:qt2},
    \begin{equation}
        Q(v_?) = \lnot r(c, v_?)\text{ or }Q(v_k)\land \lnot r(v_k, v_?) \ \Rightarrow\ 
        T(Q(v_?)) = \lnot r(c, v_?)\text{ or }T(Q(v_k))\ \top\ \lnot r(v_k, v_?) = T(v_?)
    \label{eq:app4}
    \end{equation}
The trivial case is when the child node of $v_?$ is a constant entity, and it also holds true according to Eq.~\ref{eq:app3}, \ref{eq:app4}.
Therefore, $T(v_?)=T(Q(v_?))$ is always true, indicating that the optimization on query computation tree is equivalent to the optimization on its FOL expression.
\end{proof}

\section{Query Computation Tree Optimization Algorithm}
\label{app:qto}

\begin{minipage}[!h]{\textwidth}
\begin{minipage}{.48\textwidth}
\centering
\begin{algorithm}[H]
\small
\caption{Forward Propagation Function}
\label{alg:forward}
\begin{algorithmic}
   \Function{Forward}{$v$, $M$, $T$}
   \State $V\leftarrow 1^{|\mathcal{V}|}$
   \If {$v$.type = \emph{intersection}}
   \For {each node $u$ in $v$.child}
   \State $U, T\leftarrow$ \Call{Forward}{$u$, $M$, $T$}
   \State $V\leftarrow V * U$
   \EndFor
   \EndIf
   \If {$v$.type = \emph{union}}
   \For {each node $u$ in $v$.child}
   \State $U, T\leftarrow$ \Call{Forward}{$u$, $M$, $T$}
   \State $V\leftarrow V * (1^{|\mathcal{V}|}-U)$
   \EndFor
   \State $V\leftarrow 1^{|\mathcal{V}|}-V$
   \EndIf
   \If {$v$.type = \emph{relational}}
   \If {$v$.child is a constant entity $i$}
   \State $V\leftarrow M[v.r][i][:]$
   \Else
   \State $U, T\leftarrow$ \Call{Forward}{$v$.child, $M$, $T$}
   \State $V\leftarrow \max_j\{U^T * M[v.r][:][:]\}$
   \EndIf
   \EndIf
   \If {$v$.type = \emph{anti-relational}}
   \If {$v$.child is a constant entity $i$}
   \State $V\leftarrow 1^{\mathcal{V}}-M[v.r][i][:]$
   \Else
   \State $U, T\leftarrow$ \Call{Forward}{$v$.child, $M$, $T$}
   \State $V\leftarrow \max_j\{U^T * (1^{\mathcal{V}\times\mathcal{V}} - M[v.r][:][:])\}$
   \EndIf
   \EndIf
   \State $T(v)\leftarrow V$ \\
   \Return $V, T$
   \EndFunction
\end{algorithmic}
\end{algorithm}
\vfill
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
\centering
\begin{algorithm}[H]
\small
\caption{Backward Propagation Function}
\label{alg:backward}
\begin{algorithmic}
   \Function{Backward}{$v$, $M$, $T$, $E$, $t$}
   \State $E(v)\leftarrow t$
   \If {$v$.type = \emph{intersection} or \emph{union}}
   \For {each node $u$ in $v$.child}
   \State $E\leftarrow$\Call{Backward}{$u$, $M$, $T$, $E$, $t$}
   \EndFor
   \EndIf
   \If {$v$.type = \emph{relational}}
   \If {$v$.child is not a constant node}
   \State $u\leftarrow v$.child
   \State $t'\leftarrow \argmax\{T(u)^T * M[v.r][:][t]\}$
   \State $E\leftarrow$\Call{Backward}{$u$, $M$, $T$, $E$, $t'$}
   \EndIf
   \EndIf
   \If {$v$.type = \emph{anti-relational}}
   \If {$v$.child is not a constant node}
   \State $u\leftarrow v$.child
   \State $t'\leftarrow \argmax\{T(u)^T * (1^{|\mathcal{V}|\times1}-M[v.r][:][t])\}$
   \State $E\leftarrow$\Call{Backward}{$u$, $M$, $T$, $E$, $t'$}
   \EndIf
   \EndIf
   \Return $E$
   \EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\small
\caption{Query Computation Tree Optimization}
\label{alg:qto}
\begin{algorithmic}
   \State \textbf{Input:} neural adjacency matrix $M$, nodes in the query computation tree (each node has \emph{child} and \emph{type} members), and root $v_?$
   \State \textbf{Output: } optimal truth values $T$ on root (our defined $\mathbf{T}^*(v_?)$ vector), and the optimal assignments $E$ for $v_?=t$
   \State $T, E\leftarrow \{\}$
   \State $V, T\leftarrow$\Call{Forward}{$v_?$, $M$, $T$}
   \State $E\leftarrow$\Call{Backward}{$v_?$, $M$, $T$, $E$, $t$} \\
   \Return $T, E$
\end{algorithmic}
\end{algorithm}
\vfill
\end{minipage}
\end{minipage}

Alg.~\ref{alg:forward} shows the forward propagation function, which recursively calls itself on the child nodes to obtain the optimal truth values of the child subtrees, until reaching a leaf node (constant entity).
Alg.~\ref{alg:backward} shows the backward propagation function, which also recursively calls itself on the child nodes to compute the optimal assignments for the child subtrees, until reaching a leaf node.
Our QTO algorithm is shown in Alg.~\ref{alg:qto}, which calls the forward-propagation procedures to solve the optimization problem.

\section{Proof of \cref{thm:main}: Optimality of QTO}
\label{app:thm}

\thm*
\begin{proof}
    First, we show the optimality of the forward procedure by proving that the derived $\mathbf{T}^*(v)$ is indeed the maximum truth value of the subquery rooted at $v$.
    We prove it by induction: suppose it holds for the subquery rooted at child nodes of $v_?$, then according to the derivation process in Eq.~\ref{eq:f1}-\ref{eq:f4c}, it still holds for $v_?$.
    The trivial case is when the child node of $v_?$ is a constant entity, which also holds true according to Eq.~\ref{eq:f3c}, \ref{eq:f4c}.
    Therefore, according to the definition of $\mathbf{T}^*(v_?)$, the maximum truth value for the query can be obtained by Eq.~\ref{eq:opt_root}.

    Next, we show that the backward procedure can find a set of assignments that obtains the optimal value.
    We prove it by induction on the root of the query subtree.
    Suppose our backward procedure can find the optimal set of assignments for the subquery rooted at every child node $v$ of $v_?$, when the child node is assigned an arbitrary $e\in\mathcal{V}$. In other words, the truth value of such a set of assignments is $T(v)=T^*(v=e)$.
    Then we consider the procedure on the subquery rooted at $v_?$ when $v_?$ is assigned an arbitrary $e_t\in\mathcal{V}$. For type \Roman{1} (intersection), by Eq.~\ref{eq:qt1}, \ref{eq:b12}, the truth value $T(v_?)$ under the returned assignments is:
    \begin{equation}
        T(v_?) = \top_{1\leq i\leq K}(T(v_?^i)) = \top_{1\leq i\leq K}(T^*(v_?^i=e_t))
    \end{equation}
    while
    \begin{equation}
        T^*(v_?=e_t) = \max\{\top_{1\leq i\leq K}(T(v_?^i=e_t))\} = \top_{1\leq i\leq K}(T^*(v_?^i=e_t))
    \end{equation}
    Hence $T(v_?)=T^*(v_?=e_t)$. Similarly, it also holds for type \Roman{2} (union):
    \begin{equation}
        T(v_?) = \bot_{1\leq i\leq K}(T(v_?^i)) = \bot_{1\leq i\leq K}(T^*(v_?^i=e_t)) = T^*(v_?=e_t)
    \end{equation}
    For type \Roman{3} (relational projection), by Eq.~\ref{eq:qt2}, \ref{eq:b3}, and the induction hypothesis, we have
    \begin{equation}
    \begin{aligned}
        &T(v_?) = T^*(v_k=e)\ \top\ r(e, e_t),\ e=\argmax_{e\in\mathcal{V}}\{T^{*}(v_k=e)\ \top\ r(e, e_t)\} \\
        &\Rightarrow T(v_?) = \max_{e\in\mathcal{V}}\{T^*(v_k=e)\ \top\ r(e, e_t)\}
    \end{aligned}
    \end{equation}
    while by definition of $T^*$, it holds that
    \begin{equation}
        T^*(v_?=e_t) = \max\{T(v_k)\ \top\ r(v_k, e_t)\}
        = \max_{e\in\mathcal{V}}\{T^*(v_k=e)\ \top\ r(e, e_t)\}
    \end{equation}
    Thus $T(v_?)=T^*(v_?=e_t)$. Similarly, for type \Roman{4} (anti-relational projection), by Eq.~\ref{eq:qt2}, \ref{eq:b4}, it still holds:
    \begin{equation}
    \begin{aligned}
        &T(v_?) = T^*(v_k=e)\ \top\ (1-r(e, e_t)),\ e=\argmax_{e\in\mathcal{V}}\{T^{*}(v_k=e)\ \top\ (1-r(e, e_t))\} \\
        &\Rightarrow T(v_?) = \max_{e\in\mathcal{V}}\{T^*(v_k=e)\ \top\ (1-r(e, e_t))\}=T^*(v_?=e_t)
    \end{aligned}
    \end{equation}
    Hence, the induction hypothesis holds for $v_?$.
    The trivial case is when $v_?=c$ is a constant entity, and its truth value is simply $T(c)=T^*(c)=1$.
    Therefore, by Eq.~\ref{eq:opt_root}, the backward procedure from root $v_?$ returns a set of assignments that obtains a truth value of 
    \begin{equation}
        T(v_?)=T^*(v_?=e),\ e=\argmax_{e\in\mathcal{V}}\{T^*(v_?=e)\}
        \Rightarrow T(v_?)=\max_{e\in\mathcal{V}}\{T^*(v_?=e)\}
    \end{equation}
    which is the optimal truth value.
\end{proof}

\section{Discussions on Neural Adjacency Matrix}
\label{app:discussion}
\xhdr{Neural Adjacency Matrix}
An intriguing question is, can our method learn the parameters in the neural adjacency matrix?
In fact, our method supports optimization on the parameters, since each step is differentiable.
However, we should not directly optimize the neural adjacency matrix, since it does not generalize to missing links.
More specifically, during the learning procedure, the entries corresponding to already existing edges are pushed to 1 while other entries to 0, since such a matrix would induce a correct answer.
A more promising direction is to learn the calibration function that transforms the KGE scores to probabilities between $[0, 1]$ for the neural adjacency matrix, also known as KGE calibration~\cite{safavi2020evaluating}.
In our work, the calibration function is fixed, as shown in Eq.~\ref{eq:norm}, \ref{eq:nam}, but more functionals can be defined to learn soft ``threshold'' or ``stretching'' to better calibrate KGE scores to complex query answering.

\xhdr{Storage}
We store the $|\mathcal{R}|\times|\mathcal{V}|\times|\mathcal{V}|$ neural adjacency matrix as a sparse tensor since most of its entries are 0 (filtered out by threshold $\epsilon$). For example, with an $\epsilon$ of $0.0002$ on FB15k-237, only 1\% of all entries are nonzero. We study the overlap between the neural adjacency matrix and the truth adjacency matrix. On triplets in the training graph, the faithful rounding (Eq.~\ref{eq:nam}) ensures the corresponding entries in the neural adjacency matrix are 1. On triplets in valid+test graph, 99.1\% of the corresponding entries have a nonzero value (after filtering with $\epsilon=0.0002$), and the mean value of all the entries is 0.19. Meanwhile, on false triplets (no such edges in the full graph), only 1.0\% of the corresponding entries have a nonzero value, and the mean value of all the entries is 1e-5. The gap between the corresponding entries in the neural adjacency matrix for existing edges and non-existing edges is sufficient to distinguish between true and false triplets.

\xhdr{Scalability}
For a given KG, its neural adjacency matrix $\mathbf{M}$ is pre-computed by a pretrained KGE model and then saved for query answering.
The pre-computing step on FB15k, FB15k-237, and NELL995 take 40mins, 7mins, and 7hrs, which are all carried out on one RTX-3090 GPU.
We see that the pre-computing time scales to the number of entities and relations in the KG.
A possible way to mitigate this problem may be to first predict what relations each entity possesses based on the concept of the entity or other learnable property of the entity.
Another way is to simplify $\mathbf{M}$ as a block matrix, and compute each block in the corresponding subgraph.
We leave the scalability of QTO to large KGs for future work.

\section{Experiment Details}

\subsection{Dataset Statistics}
\label{app:stat}

\begin{table*}[!h]
\centering
\begin{tabular}{lccccc}
\toprule
\bf{Dataset} & \bf{\#Entities} & \bf{\#Relations} & \bf{\#Training edges} & \bf{\#Valid edges} & \bf{\#Test edges} \\
\midrule
FB15k & 14,951 & 1,345 & 483,142 & 50,000 & 59,071 \\
FB15k-237 & 14,505 & 237 & 272,115 & 17,526 & 20,438 \\
NELL995 & 63,361 & 200 & 114,213 & 14,324 & 14,267 \\
\toprule
\end{tabular}
\caption{Statistics of the three knowledge graph datasets.}
\label{tb:stat}
\end{table*}

Table~\ref{tb:stat} summarizes the statistics of the three datasets in our experiments.
Note that the inverse of each relation is also added to the graph, and can appear in the queries.

\subsection{Query Structures}
\label{app:query}

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth,trim=0 0 0 20,clip]{figs/qtype.pdf}
\caption{Query structures, illustrated in their query computation tree representations.}
\label{fig:qtype}
\end{figure}

We use the 14 types of complex queries generated in~\cite{ren2019query2box, ren2020beta}. We show the query computation tree for each type of query in Figure~\ref{fig:qtype}.

\subsection{Implementation Details}
\label{app:detail}

\begin{table*}[!h]
\centering
\begin{tabular}{lcccccccc}
\toprule
 & \multicolumn{6}{c}{KGE (ComplEx)} & \multicolumn{2}{c}{QTO} \\
\cmidrule(l){2-7} \cmidrule(l){8-9}
& $d$ & $lr$ & $b$ & $\lambda$ & $w$ & epoch & $\epsilon$ & $\alpha$ \\
\midrule
FB15k & 1000 & 0.1 & 100 & 0.01 & 0.1 & 100 & 0.001 & 6 \\
FB15k-237 & 1000 & 0.1 & 1000 & 0.05 & 4 & 100 & 0.0002 & 3 \\
NELL995 & 1000 & 0.1 & 1000 & 0.05 & 0 & 100 & 0.0002 & 6 \\
\toprule
\end{tabular}
\caption{Hyperparameters of pretrained KGE and QTO.}
\label{tb:hyper}
\end{table*}

We provide the best hyperparameters of the pretrained KGE~\footnote{We utilize the KGE implementation from \href{https://github.com/facebookresearch/ssl-relation-prediction}{https://github.com/facebookresearch/ssl-relation-prediction}~\cite{chen2021relation}.} and QTO in Table~\ref{tb:hyper}.
The hyperparameters for KGE, which is a ComplEx model~\cite{trouillon2016complex} trained with N3 regularizor~\cite{lacroix2018canonical} and auxiliary relation prediction task~\cite{chen2021relation}, include embedding dimension $d$, learning rate $lr$, batch size $b$, regularization strength $\lambda$, auxiliary relation prediction weight $w$, and the number of epochs.
We recall that the hyperparameters in our QTO method include the threshold $\epsilon$ and the negation scaling coefficient $\alpha$.

\section{More Experimental Results}

\subsection{Hits@1 on Complex Query Answering}
\label{app:h@1}

\begin{table*}[!h]
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{lccccccccccccccccc}
        \toprule
        \bf{Model} & \bf{avg$_p$} & \bf{avg$_{ood}$} & \bf{avg$_n$} & \bf{1p} & \bf{2p} & \bf{3p} & \bf{2i} & \bf{3i} & \bf{pi} & \bf{ip} & \bf{2u} & \bf{up} & \bf{2in} & \bf{3in} & \bf{inp} & \bf{pin} & \bf{pni} \\
        \midrule
        \multicolumn{18}{c}{FB15k} \\
        \midrule
        GQE & 16.6 & 11.0 & - & 34.2 & 8.3 & 5.0 & 23.8 & 34.9 & 15.5 & 11.2 & 11.5 & 5.6 & - & - & - & - & - \\
        Query2Box & 26.8 & 18.7 & - & 52.0 & 12.7 & 7.8 & 40.5 & 53.4 & 26.7 & 16.7 & 22.0 & 9.4 & - & - & - & - & - \\
        BetaE & 31.3 & 24.2 & 5.2 & 52.0 & 17.0 & 16.9 & 43.5 & 55.3 & 32.3 & 19.3 & 28.1 & 16.9 & 6.4 & 6.7 & 5.5 & 2.0 & 5.3 \\
        CQD-CO & 39.7 & 26.4 & - & 85.8 & 17.8 & 9.0 & 67.6 & 71.7 & 34.5 & 24.5 & 30.9 & 15.5 & - & - & - & - & - \\
        CQD-Beam & 51.9 & 42.7 & - & 85.8 & 48.6 & 22.5 & 67.6 & 71.7 & 51.7 & 62.3 & 31.7 & 25.0 & - & - & - & - & - \\
        ConE & 39.6 & 33.0 & 7.3 & 62.4 & 23.8 & 20.4 & 53.6 & 64.1 & 39.6 & 25.6 & 44.9 & 21.7 & 9.4 & 9.1 & 6.0 & 4.3 & 7.5 \\
        GNN-QE & 67.3 & 62.2 & 28.6 & 86.1 & \bf{63.5} & \bf{52.5} & 74.8 & \bf{80.1} & 63.6 & 65.1 & 67.1 & 53.0 & 35.4 & 33.1 & 33.8 & 18.6 & \bf{21.8} \\
        \midrule
        QTO & \bf{68.6} & \bf{66.0} & \bf{39.1} & \bf{86.6} & 60.9 & 51.9 & \bf{75.3} & 79.0 & \bf{69.2} & \bf{68.8} & \bf{71.8} & \bf{54.3} & \bf{50.3} & \bf{50.3} & \bf{39.1} & \bf{36.9} & 18.8 \\
        \midrule[0.08em]
        \multicolumn{18}{c}{FB15k-237} \\
        \midrule
        GQE & 8.8 & 4.9 & - & 22.4 & 2.8 & 2.1 & 11.7 & 20.9 & 8.4 & 5.7 & 3.3 & 2.1 & - & - & - & - & - \\
        Query2Box & 12.3 & 7.0 & - & 28.3 & 4.1 & 3.0 & 17.5 & 29.5 & 12.3 & 7.1 & 5.2 & 3.3 & - & - & - & - & - \\
        BetaE & 13.4 & 7.9 & 2.8 & 28.9 & 5.5 & 4.9 & 18.3 & 31.7 & 14.0 & 6.7 & 6.3 & 4.6 & 1.5 & 7.7 & 3.0 & 0.9 & 0.9 \\
        CQD-CO & 14.7 & 9.5 & - & 36.6 & 4.7 & 3.0 & 20.7 & 29.6 & 15.5 & 9.9 & 8.6 & 4.0 & - & - & - & - & - \\
        CQD-Beam & 15.1 & 9.7 & - & 36.6 & 6.3 & 4.3 & 20.7 & 29.6 & 13.5 & 12.1 & 8.7 & 4.3 & - & - & - & - & - \\
        ConE & 15.6 & 9.5 & 2.2 & 31.9 & 6.9 & 5.3 & 21.9 & 36.6 & 17.0 & 7.8 & 8.0 & 5.3 & 1.8 & 3.7 & 3.4 & 1.3 & 1.0 \\
        GNN-QE & 19.1 & 13.0 & 4.3 & 32.8 & 8.2 & 6.5 & 27.7 & 44.6 & 22.4 & 12.3 & 9.8 & 7.6 & 4.1 & 8.1 & 4.1 & 2.5 & \bf{2.7} \\
        \midrule
        QTO & \bf{25.4} & \bf{19.9} & \bf{8.3} & \bf{39.5} & \bf{14.3} & \bf{14.7} & \bf{33.2} & \bf{47.2} & \bf{29.0} & \bf{20.6} & \bf{15.1} & \bf{14.8} & \bf{8.6} & \bf{15.9} & \bf{8.5} & \bf{6.4} & 2.0 \\
        \midrule[0.08em]
        \multicolumn{18}{c}{NELL-995} \\
        \midrule
        GQE & 9.9 & 6.9 & - & 15.4 & 6.7 & 5.0 & 14.3 & 20.4 & 10.6 & 9.0 & 2.9 & 5.0 & - & - & - & - & - \\
        Query2Box & 14.1 & 8.8 & - & 23.8 & 8.7 & 6.9 & 20.3 & 31.5 & 14.3 & 10.7 & 5.0 & 6.0 & - & - & - & - & - \\
        BetaE & 17.8 & 9.6 & 2.1 & 43.5 & 8.1 & 7.0 & 27.2 & 36.5 & 17.4 & 9.3 & 6.9 & 4.7 & 1.6 & 2.2 & 4.8 & 0.7 & 1.2 \\
        CQD-CO & 21.3 & 13.9 & - & 51.2 & 11.8 & 9.0 & 28.4 & 36.3 & 22.4 & 15.5 & 9.9 & 7.6 & - & - & - & - & - \\
        CQD-Beam & 21.0 & 13.2 & - & 51.2 & 14.3 & 6.3 & 28.4 & 36.3 & 18.1 & 17.4 & 10.2 & 7.2 & - & - & - & - & - \\
        ConE & 19.8 & 11.6 & 2.2 & 43.6 & 10.7 & 9.0 & 28.6 & 39.8 & 19.2 & 11.4 & 9.0 & 6.6 & 1.4 & 2.6 & 5.2 & 0.8 & 1.2 \\
        GNN-QE & 21.5 & 13.2 & 3.6 & 43.5 & 12.9 & 9.9 & \bf{32.5} & \bf{42.4} & \bf{23.5} & 12.9 & 8.8 & 7.4 & 3.2 & 5.9 & 5.4 & 1.6 & 2.0 \\
        \midrule
        QTO & \bf{24.8} & \bf{16.7} & \bf{6.1} & \bf{51.6} & \bf{17.1} & \bf{15.3} & 32.1 & 40.8 & 23.1 & \bf{19.9} & \bf{12.3} & \bf{11.3} & \bf{5.6} & \bf{9.4} & \bf{9.9} & \bf{3.5} & \bf{2.1} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Test Hits@1 results (\%) on complex query answering across all query types. avg$_p$ is the average on EPFO queries; avg$_{ood}$ is the average on out-of-distribution (OOD) queries; avg$_n$ is the average on queries with negation.}
    \label{tb:h@1}
\end{table*}

Table~\ref{tb:h@1} reports the Hits@1 result on complex query answering.
On Hits@1 metric, QTO outperforms previous Sota method GNN-QE by an average of 8.5\%, 39.8\%, and 20\% over all query types on the three datasets.

\subsection{More Plots on Complex Query Answering w.r.t. 1-hop Query Answering}
\label{app:plot}

\begin{figure}[htbp]
    \centering
    \subfigure[2p-1p]{
        \includegraphics[width=2.1in]{figs/2p-1p.pdf}
    }
    \subfigure[3p-1p]{
	\includegraphics[width=2.1in]{figs/3p-1p.pdf}
    }
    \subfigure[2i-1p]{
    	\includegraphics[width=2.1in]{figs/2i-1p.pdf}
    }
    \quad
    \subfigure[3i-1p]{
    	\includegraphics[width=2.1in]{figs/3i-1p.pdf}
    }
    \subfigure[pi-1p]{
	\includegraphics[width=2.1in]{figs/pi-1p.pdf}
    }
    \subfigure[ip-1p]{
	\includegraphics[width=2.1in]{figs/ip-1p.pdf}
    }
    \quad
    \subfigure[2u-1p]{
	\includegraphics[width=2.1in]{figs/2u-1p.pdf}
    }
    \subfigure[up-1p]{
	\includegraphics[width=2.1in]{figs/up-1p.pdf}
    }
    \subfigure[2in-1p]{
	\includegraphics[width=2.1in]{figs/2in-1p.pdf}
    }
    \quad
    \subfigure[3in-1p]{
	\includegraphics[width=2.1in]{figs/3in-1p.pdf}
    }
    \subfigure[inp-1p]{
	\includegraphics[width=2.1in]{figs/inp-1p.pdf}
    }
    \subfigure[pin-1p]{
	\includegraphics[width=2.1in]{figs/pin-1p.pdf}
    }
    \caption{Test MRR on each type of queries w.r.t. MRR on 1p (one-hop) queries, evaluated on FB15k-237.}
    \label{fig:all-1p}
\end{figure}

Figure~\ref{fig:all-1p} reports the Test MRR on each type of complex queries w.r.t. the MRR performance on one-hop queries. We observe a similar trend across all types of queries, verifying our conclusion in Sec.~\ref{sec:result}.

\subsection{Hyperparameter Analysis}
\label{app:hyper}
Table~\ref{tb:eps}, \ref{tb:alpha} shows the effect of $\epsilon$ and $\alpha$ on FB15k-237.

\begin{table}[htbp]
    \centering
    \begin{minipage}{0.4\linewidth}
        \centering
        \begin{tabular}{lcccc}
            \toprule
            $\epsilon$ & $0.1$ & $0.01$ & $0.001$  & $0.0002$ \\
            \midrule
            memory usage & 19M & 82M & 817M & 20G \\
            {\bf avg$_p$} (\%) & 24.5 & 31.5 & 33.1 & 33.5 \\
            \toprule
        \end{tabular}
        \caption{Effect of $\epsilon$ on memory usage and avg$_p$ MRR.}
        \label{tb:eps}
    \end{minipage}
    \qquad
    \begin{minipage}{0.5\linewidth}
        \centering
        \begin{tabular}{lcccccc}
            \toprule
            $\alpha$ & $0.5$ & $1$ & $3$ & $5$ & $7$ & $9$ \\
            \midrule
            {\bf avg$_n$} (\%) & 7.4 & 13.8 & 15.5 & 14.9 & 13.8 & 13.2 \\
            \toprule
        \end{tabular}
        \caption{Effect of $\alpha$ on avg$_n$ MRR.}
        \label{tb:alpha}
    \end{minipage}
\end{table}

\subsection{QTO with Different One-hop Link Predictors}
\label{app:kge}
Table~\ref{tb:kge} reports the performance of QTO with different KGE models on FB15k-237.

\begin{table}[t]
    \centering
    \begin{tabular}{lcccccccccc}
        \toprule
        & {\bf avg$_p$} & {\bf 1p} & {\bf 2p} & {\bf 3p} & {\bf 2i} & {\bf 3i} & {\bf pi} & {\bf ip} & {\bf 2u} & {\bf up} \\
        \midrule
        \multicolumn{11}{c}{QTO's performance under different KGE models} \\
        \midrule
        QTO+TransE & 22.1 & 43.2 & 11.5 & 9.7 & 28.1 & 41.5 & 23.3 & 17.8 & 13.4 & 9.7 \\
        QTO+RotatE & 22.4 & 43.8 & 11.7 & 9.8 & 28.7 & 42.2 & 23.6 & 18.0 & 14.1 & 9.7 \\
        QTO+ComplEx (w/ N3\&RP) & 33.5 & 49.0 & 21.4 & 21.2 & 43.1 & 56.8 & 38.1 & 28.0 & 22.7 & 21.4 \\
        \midrule
        \multicolumn{11}{c}{Comparison with prior methods under the same KGE model} \\
        \midrule
        CQD-Beam & 22.3 & 46.7 & 11.6 & 8.0 & 31.2 & 40.6 & 21.2 & 18.7 & 14.6 & 8.4 \\
        QTO+ComplEx (w/ N3) & 28.7 & 46.7 & 16.0 & 15.1 & 38.7 & 51.2 & 32.8 & 23.6 & 18.1 & 15.6 \\
        GNN-QE & 26.8 & 42.8 & 14.7 & 11.8 & 38.3 & 54.1 & 31.1 & 18.9 & 16.2 & 13.4 \\
        QTO+NBFNet & 27.3 & 51.7 & 17.5 & 13.3 & 31.8 & 43.0 & 28.9 & 24.2 & 22.3 & 12.6 \\
        \bottomrule
    \end{tabular}
    \caption{MRR (\%) on FB15k-237, RP is short for the relation prediction task~\cite{chen2021relation} during KGE training.}
    \label{tb:kge}
\end{table}

\subsection{More Results on Interpretation Study}
\label{app:inter}

\begin{table}[t]
    \centering
    \begin{tabular}{lcccccccc}
        \toprule
        & \bf{2p} & \bf{3p} & \bf{pi} & \bf{ip} & \bf{up} & \bf{inp} & \bf{pin} & \bf{pni} \\
        \midrule
        \multicolumn{9}{c}{FB15k} \\
        \midrule
        on Hits@1 & 98.2 & 96.3 & 98.2 & 98.2 & 98.2 & 96.2 & 98.6 & 97.8 \\
        on Hits@3 & 97.4 & 94.8 & 97.3 & 97.1 & 97.3 & 94.6 & 97.7 & 98.3 \\
        on Hits@10 & 96.0 & 92.7 & 96.6 & 96.0 & 95.7 & 92.8 & 96.3 & 98.7 \\
        on All & 90.5 & 85.3 & 94.1 & 91.7 & 89.2 & 81.8 & 90.5 & 97.7 \\
        \midrule
        \multicolumn{9}{c}{NELL995} \\
        \midrule
        on Hits@1 & 90.7 & 81.8 & 92.3 & 92.2 & 91.1 & 74.9 & 89.7 & 91.5 \\
        on Hits@3 & 88.3 & 77.2 & 91.6 & 89.9 & 88.8 & 71.5 & 86.4 & 93.8 \\
        on Hits@10 & 86.1 & 75.2 & 90.8 & 88.3 & 85.6 & 68.6 & 84.5 & 95.2 \\
        on All & 76.4 & 64.4 & 81.5 & 82.4 & 77.2 & 54.5 & 74.3 & 95.1 \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy (\%) of intermediate variable interpretation given the Hits@K answers that QTO predicts and all true answers, evaluated on FB15k and NELL995.}
    \label{tb:interpretability1}
\end{table}

Table~\ref{tb:interpretability1} reports the accuracy on intermediate variable interpretation on FB15k and NELL995.
We observe that QTO can provide valid explanations for over 90\% of the Hits@1 answers it predicts.

\subsection{Case Study on QTO's Interpretation}
\label{app:case}

\begin{figure}[htbp]
    \centering
    \subfigure[Case study on 2p query]{
        \includegraphics[width=1\linewidth,trim=50 120 50 120,clip]{figs/case-1.pdf}
    }
    \quad
    \subfigure[Case study on pi query]{
        \includegraphics[width=1\linewidth,trim=50 120 50 120,clip]{figs/case-2.pdf}
    }
    \quad
    \subfigure[Case study on ip query]{
        \includegraphics[width=1\linewidth,trim=50 120 50 120,clip]{figs/case-3.pdf}
    }
    \quad
    \subfigure[Case study on pni query]{
        \includegraphics[width=1\linewidth,trim=50 120 50 120,clip]{figs/case-4.pdf}
    }
\caption{Case study of QTO's interpretation.}
\label{fig:case}
\end{figure}

We report case studies on QTO's interpretation in Figure~\ref{fig:case}. We include both success cases, where the assignments can be verified by the full graph, and failure cases where the assignments are not viable.
For each set of assignments, we provide the rank of its answer in QTO's prediction, the correctness of the assignments, and the missing link in the training graph that need to be predicted for deducing the answer.

\subsection{More Results on Cardinality Prediction}
\label{app:card}

\begin{table}[!h]
    \centering
    \begin{tabular}{lcccccccccc}
        \toprule
        \bf{Method} & \bf{avg} & \bf{1p} & \bf{2p} & \bf{3p} & \bf{2i} & \bf{3i} & \bf{pi} & \bf{ip} & \bf{2u} & \bf{up} \\
        \midrule
        \multicolumn{11}{c}{FB15k} \\
        \midrule
        GNN-QE & 37.1 & 34.4 & 29.7 & 34.7 & 39.1 & 57.3 & 47.8 & 34.6 & 13.5 & 26.5 \\
        QTO & \bf{23.1} & \bf{11.7} & \bf{24.1} & \bf{29.5} & \bf{25.5} & \bf{27.2} & \bf{29.3} & \bf{25.2} & \bf{12.4} & \bf{22.7} \\
        \midrule
        \multicolumn{11}{c}{NELL995} \\
        \midrule
        GNN-QE & 44.0 & 61.9 & 38.2 & 47.1 & 56.6 & 72.3 & 49.5 & 45.8 & 19.9 & 36.2 \\
        QTO & \bf{37.6} & \bf{40.1} & \bf{32.3} & \bf{39.3} & \bf{48.2} & \bf{60.7} & \bf{37.7} & \bf{42.4} & \bf{16.8} & \bf{20.5} \\
        \bottomrule
    \end{tabular}
    \caption{MAPE (\%, $\downarrow$) on answer set cardinality prediction, evaluated on FB15k and NELL995.}
    \label{tb:mape1}
\end{table}

Table~\ref{tb:mape1} reports the mean absolute percentage error (MAPE) on cardinality prediction on FB15k and NELL995.
We see that QTO outperforms GNN-QE by a large margin on these two datasets, and the advantage is consistent across all types of queries.