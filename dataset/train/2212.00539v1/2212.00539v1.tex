\section{EXPERIMENTS AND IMPLEMENTATION DETAILS}
\subsection{Implementation details}
We start with extracting the active voice regions for a video under consideration using an open-source tool, \emph{pyannote}~\cite{pyannote}. We then segment the obtained voiced parts into approximated speaker-homogeneous speech segments, , by partitioning at the shot boundaries~\cite{pardo2021moviecuts} and keeping the maximum duration to be 1sec each~\cite{sharma2022unsupervised}. We gather the set of temporally overlapping face tracks, , for all the obtained speech segments, using the RetinaFace~\cite{deng2020retinaface} face detector, and SORT~\cite{Bewley2016_sort} tracker. As a trade-off between the computational time and the context length~\cite{sharma2022unsupervised}, we partition the set of speech segments, , to contain  segments each. Next, we compute each partition's speech-identity and face-identity distance matrix and solve the speech-face association by separately maximizing each partition's objective function . For \emph{SCMIA} we use . 

For \emph{GSCMIA} we employ TalkNet~\cite{tao2021someone} to gather AV-activity ASD predictions. We construct the positive guidance set of speech segments, , using  (Eqn.~\ref{eq:PG}) for AVA dataset and  for VPCD videos, where TalkNet prediction scores lie in the range . While constructing the negative guidance set of face tracks, , we use  (Eqn.~\ref{eq:NG}) for both the datasets. These thresholds are derived using a subset of training sets for both the datasets. In contrast to ~\emph{SCMIA}, we use  for \emph{GSCMIA}.

For evaluation purposes we use two benchmark datasets: i) the AVA-active speaker dataset~\cite{roth2020ava} and ii) the Visual person clustering dataset~\cite{brown2021face}. The AVA-active speaker dataset consists of 160 international movies, publicly available, and face-boxwise annotations for 15 min duration of each film. We report performance for the publicly available and widely used validation split of the AVA-active speaker dataset consisting of 33 movies. To incorporate diversity, we use a part of the VPCD consisting of active speaker annotations and character identities for several Hollywood TV shows and movies. We observed that the available annotations in VPCD are exhaustive only for TV shows: Friends and TBBT, primarily due to the limited number of characters in the videos. So we restrict our evaluation to TV shows: 25 episodes of Friends from season 3 and 6 episodes of TBBT from season 1. We use the widely used mean average precision (mAP) metric to report performances.

\subsection{Performance Evaluation}
In Table~\ref{tab:mAP}, we report the performance of various strategies described in Section~\ref{sec:methods} for the three datasets. We present the performance of \emph{SCMIA} and TalkNet~\cite{tao2021someone}, computed as an average over mAP of the constituting videos for each dataset. We compute the mAP using the official tool provided by AVA-active speaker. We observe that TalkNet performs significantly better than \emph{SCMIA} for the AVA dataset. At the same time, the performance is comparable for both strategies in the case of the TV shows: Friends and TBBT. A reason for such observed behavior can be the fully-supervised setup of TalkNet, trained on AVA training set. Table~\ref{tab:mAP} also shows the performance of \emph{GSCMIA}, following Algorithm~\ref{algo:optimzation}. The av-activity guidance enhances the performance of \emph{SCMIA} across all datasets. We note that the performance enhancement for the AVA dataset is more significant than for others.

Since the sources of information for audio-visual activity in the case of TalkNet and \emph{SCMIA} is independent, we expect them to have some exclusive components. Here we investigate the validity of this hypothesis by combining the two methods at the score level. We score each face box as the weighted aggregate of the individual scores from the two methods: i) \emph{GSCMIA} and ii) TalkNet, in a ratio of .
We report the obtained mAP for the three datasets in Table~\ref{tab:mAP}. We observe that a simple late fusion of the two methods improved the performance for all three datasets, showing performance better than the individual performances of the involved methods.
\begin{table}[]
\centering
\caption{Performance of various strategies for ASD when guided with Talknet~\cite{talknet}. The reporting metric is mean average precision (\%).}
\label{tab:mAP}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{@{}c|cccc@{}}
\toprule
Datasets & \emph{SCMIA} & TalkNet & \emph{GSCMIA} & Late-fusion \\ \midrule
\begin{tabular}[c]{@{}c@{}}AVA\\ active speaker\end{tabular}    & 68.84 & 91.96 & 80.9  & \textbf{92.86} \\ \midrule
\begin{tabular}[c]{@{}c@{}}Friends\\ (25 episodes)\end{tabular} & 81.45 & 85.42 & 84.13 & \textbf{87.54}                         \\ \midrule
\begin{tabular}[c]{@{}c@{}}TBBT\\ (6 episodes)\end{tabular}     & 85.74 & 83.84 & 86.39 & \textbf{87.19}                     \\ \bottomrule
\end{tabular}}
\end{table}

We further evaluate the \emph{GSCMIA} guided with another av-activity model: syncnet~\cite{chung2016out}. Syncnet is trained for audio-visual synchronization in a self-supervised framework, unlike fully-supervised TalkNet. In Table~\ref{tab:syncnet}, we show the performance of Syncnet and \emph{GSCMIA} assisted with Syncnet, for the three datasets. We note the weaker performance of Syncnet compared to TalkNet. We observe that the assistance from Syncnet enhances the performance for \emph{GSCMIA}, although marginally for TV shows, attributing to the weaker performance of SyncNet. The two methods' late fusion of posterior scores performs improves the overall performance. The observed trends in performance enhancement with SyncNet and TalkNet guidance are consistent. 


\begin{table}[]
\centering
\caption{Performance of various strategies for ASD when guided with Syncnet~\cite{chung2016out}. The reporting metric is mean average precision (\%).}
\label{tab:syncnet}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{@{}c|cccc@{}}
\toprule
Datasets & \emph{SCMIA} & SyncNet & \emph{GSCMIA} & Late-fusion \\ \midrule
\begin{tabular}[c]{@{}c@{}}AVA\\ active speaker\end{tabular}    & 68.84 & 58.70 & 71.74  & \textbf{73.74} \\ \midrule
\begin{tabular}[c]{@{}c@{}}Friends\\ (25 episodes)\end{tabular} & 81.45 & 77.14 & 81.98 & \textbf{83.90}                         \\ \midrule
\begin{tabular}[c]{@{}c@{}}TBBT\\ (6 episodes)\end{tabular}     & 85.74 & 80.00 & 85.87 & \textbf{86.14}                     \\ \bottomrule
\end{tabular}}
\end{table}
\subsection{Ablation studies}
\subsubsection{Error Analysis}
In this work, we considered two sources of information concerning active speaker detection in videos: i) the audio-visual activity modeling and ii) the speaker identity co-occurrences across modalities. The fundamental hypothesis underlying this work is that the information captured by the two sources may have complementary components. Here we investigate this further by comparing the predictions of the two systems. 



\begin{figure}[]
    \centering
    \includegraphics[width=0.5\textwidth,keepaspectratio]{complementary_part.pdf}
    \caption{Comparison of \emph{SCMIA} and TalkNet~\cite{tao2021someone} perdictions for positive (top row) and negative samples (bottom row).} 
    \label{fig:error_analysis}
\end{figure}
We generate face box-wise scores using i) \emph{SCMIA} and ii) TalkNet for the three datasets. We then generate the corresponding predictions by thresholding the obtained scores such that the systems' precision and recall are nearly equivalent. We divide the samples into two groups: i) positive samples and ii) negative samples, using ground truth. For both groups, we construct the confusion matrices, comparing the predictions of the two systems. In Figure~\ref{fig:error_analysis} we show the confusion matrices for the positive and negative samples for three datasets (AVA-active speaker, Friends, and TBBT). 

The off-diagonal cells in confusion matrices shown in Figure~\ref{fig:error_analysis} represent the exclusive correctness of one of the methods. The cells marked in green shows the fraction of samples where the \emph{SCMIA} is exclusively correct. We note that the fraction of samples where one method is solely accurate is significant, especially for the positive samples. From Table~\ref{tab:mAP}, we note that the performance of TalkNet on AVA is significantly higher than the \emph{SCMIA}, precisely 92.0 mAP score, and has marginal room for further improvement. Even then, the fraction of exclusive correctness of the \emph{SCMIA} is significant. 
This verifies the hypothesis that the information comprising the audio-visual activity and \emph{SCMIA} have some exclusive components, which motivates us to explore the use of two sources of information in a complementary fashion towards enhancing ASD performance. 



\subsubsection{Role of Positive guidance  in \emph{GSCMIA}}
The Positive guides intend to provide explicit disambiguating information to the \emph{SCMIA} by acquiring high-confidence positive class (speaking) predictions from the audio-visual activity information (TalkNet). The speech segments in  and their corresponding active-speaker face tracks readily provide the speech-face associations to the \emph{SCMIA}. To quantify the role of , we report, in Table~\ref{tab:pg}, the fraction of all speech segments which provide positive guidance: , where  denotes the cardinality of a set. We also report the accuracy of the positive guides toward predicting the positive class (speaking). We observe that the accuracy of the PG is high for all the datasets. This validates the usage of the  as the ground truth, adding a marginal amount of noise in the form of a small number of false positives. We note that the  fraction is larger and accuracy is smaller for TV shows compared to AVA. This is due to the lower value of  for VPCD.



For some of the speech segments in , speech-face associations are correctly deduced even by the \emph{SCMIA} (no-guidance). A simple example is a case when only the speaker's face is visible in the frames. Effectively the speech segments in , for which the \emph{SCMIA} can establish the same speech-face association, do not help other than reducing the computational load. Removing such speech segments from the , we report the fraction of the  that presents effectively helpful information in Table~\ref{tab:pg} as Effective positive guides.  We observe that a much smaller fraction of  constitutes the effective positive guides.
We also tabulate the accuracy of the effective positive guides and observe that they are less accurate than the bigger set . The observed smaller accuracy is notable for TV shows, attributed to the lower value of  for VPCD. 
\begin{table}[]
\centering
\caption{Constituents of positive and effective positive guides and their exactness. All values are shown in \%.}
\label{tab:pg}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{@{}c|cc|cc@{}}
\toprule
    \multirow{2}{*}{Videos}               & \multicolumn{2}{c|}{Positive guides (PG)} & \multicolumn{2}{c}{Effective PG} \\ \cmidrule(l){2-5} 
             & Fraction         & Accuracy         & Fraction              & Accuracy             \\ \midrule
AVA & 24.65            & 91.67             & 4.41                 & 82.03                 \\
Friends            & 73.42            & 79.69             & 12.09                  & 47.73                 \\
TBBT               & 66.59            & 83.13             & 7.81                  & 65.82                 \\ \bottomrule
\end{tabular}}
\end{table}


\subsubsection{Positive guides at work}
Here we display the advantage of the positive guidance from the audio-visual activity to solve the speech-face association using \emph{GSCMIA}. We consider an adverse case where the speaker identity-based method fails due to insufficient disambiguating information. One such scenario is where all the speaker faces are always visible in frames; a panel discussion is a simple example. The Columbia dataset, consisting of an 85 min long panel discussion video, is the closest, and it consists of active speaker annotations for 35 min duration of the video comprising six speakers.

We evaluate the performance of the \emph{SCMIA} (no guidance), on the Columbia dataset and report the widely used speaker-wise weighted F1 score in Table~\ref{tab:columbia}. We also report the performance of several state-of-the-art methods for comparison. We observe that the performance is terrible, particularly for three speakers: Lieb, Long, and Sick, while comparable to other methods for the remaining speakers. By visually inspecting the system's active speaker predictions, we noted that the system incorrectly associates Long's speech with Sick's face and Lieb's with Long's face. The relevant fragments of the video output are present in the supplementary material. These erroneous associations can be attributed to the fact that Lieb and Sick always appear together in the video, providing the system with insufficient information to resolve the speech-face association. 
\begin{table}[tb]
\centering
\caption{Comparison of the speaker-wise weighted F1 scores for all the speakers in Columbia dataset.}
\label{tab:columbia}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{@{}ccccccc|c@{}}
\toprule
Methods             &Abbas  & Bell & Boll  & Lieb & Long & Sick & Avg   \\ \midrule
Chakravarty et al.~\cite{chakravarty2016cross}  &-      & 82.9 & 65.8  & 73.6 & 86.9 & 81.8 & 78.2  \\
Shahid et al~\cite{shahid_columbia}          &-      & 89.2 & 88.8  & 85.8 & 81.4 & 86   & 86.2  \\
Chung et al.~\cite{chung2016out}             &-      & 93.7 & 83.4  & 86.8 & 97.7 & 86.1 & 89.5  \\
Afouras et al.~\cite{afouras2020self}     &-      & 92.6 & 82.4  & 88.7 & 94.4 & 95.9 & 90.8  \\
S-VVAD~\cite{svvad}           &-      & 92.4 & 97.2  & 92.3 & 95.5 & 92.5 & 94    \\
RealVAD~\cite{realvad}           &-      & 92   & 98.9  & 94.1 & 89.1 & 92.8 & 93.4  \\
TalkNet ~\cite{talknet}           &-      & 97.1 & 90.0    & 99.1 & 96.6 & 98.1 & 96.2  \\ \midrule
\emph{SCMIA}           &97.0  & 95.0 & 91.0 & 65.3  & 0.1 & 34.6  & 65.0 \\
\emph{GSCMIA} &97.3  & 96.3 & 89.4  & 98.7 & 98.7 & 96.8 & \textbf{96.2}  \\ \bottomrule
\end{tabular}}
\end{table}

We inject the explicit disambiguating information in form of positive guides from TalkNet. We use just the positive guides, no negative guides, while employing Algorithm~\ref{algo:optimzation} to quantify the impact of the positive guides. We report the performance of the positively guided \emph{GSCMIA} for all the speakers in Table~\ref{tab:columbia}. We observe that the guided system restores the earlier incorrect speech-face associations and performs at par with other methods for all the speakers. The relevant video fragments with enhanced predictions are present in the supplementary material. The improved performance highlights the importance of positive guides in providing the required disambiguating information to establish accurate speech-face associations. 

\subsubsection{Role of Negative guides ()}
The visual information modeling the activity in the lip region and its coordination with the underlying audio waveform can reliably predict which of the visible faces are not speaking. The negative guides are introduced to incorporate the information concerning the non-speaking faces to help the speech-face association using the speaker identity contextual information. The face tracks classified by the TalkNet as not speaking with high enough confidence are used as ground truth. We remove these faces from the set of candidate active speaker faces while establishing speech-face association using the \emph{GSCMIA}.

Although not explicitly as in the case of positive guides, the negative guides also introduce disambiguating information by eliminating some of the candidate active speaker faces. An example can be a scenario with two faces visible in the frames, and one of them is eliminated by the negative guides classifying as not speaking: establishing a direct speech-face correspondence. In Table~\ref{tab:ng}, we quantify the fraction of all face tracks that constitute the negative guides: , along with their accuracy predicting the negative class (not speaking). We observe high accuracy values for all datasets, attributed to selecting only high-confidence predictions from TalkNet. We also note that the fraction of face tracks that negative guides constitutes is much higher for AVA than others. The reason can be that TalkNet performs better for AVA than others, primarily due to training on the AVA train set. 
\begin{table}[]
\centering
\caption{Constituents of negative and effective negative guides and their exactness. All values are shown in \%.}
\label{tab:ng}
\resizebox{0.45\textwidth}{!}{\begin{tabular}{@{}c|cc|cc@{}}
\toprule
     \multirow{2}{*}{Videos}               & \multicolumn{2}{c|}{Negative guides (NG)} & \multicolumn{2}{c}{Effective NG} \\ \cmidrule(l){2-5} 
           & Fraction            & Accuracy           & Fraction       & Accuracy       \\ \midrule
AVA & 46.69               & 96.06               & 20.54          & 89.57           \\
Friends            & 22.00               & 98.43               & 2.98           & 86.72           \\
TBBT               & 17.87               & 93.87               & 3.61           & 56.59           \\ \bottomrule
\end{tabular}}
\end{table}

As in the case of , only some of the face tracks in negative guides provide new information to the \emph{GSCMIA}. Even with no guidance, the \emph{SCMIA} can correctly deduce some of the face tracks in negative guides as not speaking. Discarding such face tracks, we report the remaining fraction of all face tracks in Table~\ref{tab:ng} as effective negative guides, along with their precision for negative class. We observe that only a small fraction of face tracks in  effectively add value to \emph{GSCMIA} for TV shows while a notable fraction for the AVA dataset. Surprisingly, we note that the accuracy of the effective negative guides is low for TBBT, adding noisy information. The noisy guides lead to only marginal increase in the overall performance with the av-guidance, \emph{GSCMIA}, for TBBT compared to others, as shown in Table~\ref{tab:mAP}

\subsubsection{Effects on offscreen speakers}
One of the limitations of \emph{SCMIA} is the inability to explicitly address the speech segments where faces are visible in the frames, but the speaker is off-screen. However, audio-visual activity methods can handle such cases reliably. The negative guides are intended to fuse this ability of the AV-activity information with the \emph{SCMIA}. Here we explore the effect of AV-activity guidance on the speech segments with off-screen speakers.

We tabulate the fraction of speech segments with off-screen speakers in Table~\ref{tab:offscreen} for all three datasets. We observe that the AVA has much higher fraction of speech segments with off-screen speakers than others. It is due to the nature of videos in AVA which are more in the wild compared to structured TV shows where the video primarily captures the speaker. The higher fraction of off-screen speakers in AVA is one of the reasons for the lower performance of the \emph{SCMIA} for AVA compared to others (Table~\ref{tab:mAP}). We compare the errors of \emph{SCMIA} and \emph{GSCMIA}, particular to speech segments with off-screen speakers. We report the fraction of speech segments with off-screen speakers for which the systems incorrectly assign an active speaker face in Table~\ref{tab:offscreen}.
\begin{table}[]
\centering
\caption{Effect of using the Negative guides: performance enhancement for speech segments with off-screen speakers.}
\label{tab:offscreen}
\resizebox{0.4\textwidth}{!}{\begin{tabular}{@{}c|ccc@{}}
\toprule
Videos  & off-screen (\%) & \emph{SCMIA} & \emph{GSCMIA} \\ \midrule
AVA     & 33.30           & 16.67       & 9.51                 \\
Friends & 13.31           & 19.64       & 17.28                \\
TBBT    & 12.21           & 10.32       & 10.02                \\ \bottomrule
\end{tabular}}
\end{table}

We observe that the errors in the form of false positives by assigning active speaker faces to speech segments with off-screen speakers reduce consistently for all datasets when guided with av-activity. We point out that the improvement in the AVA dataset is greater than TV shows. The larger fraction of effective negative guides for AVA dataset, observed in Table~\ref{tab:ng}, which in turn are due to large fraction of off-screen speakers, is the reason for the observed substantial improvement.

\subsubsection{Effects on required temporal context}
\begin{figure}[]
    \centering
    \includegraphics[width=0.5\textwidth,keepaspectratio]{context_part.png}
    \caption{Variation in performance, reported in mAP, of \emph{SCMIA} and \emph{GSCMIA} with the context length }
    \label{fig:context}
\end{figure}
The \emph{SCMIA} studies the contextual information concerning the speakers' identity appearing in the speech and faces and establishes the speech-face associations. By formulation, it relies on the observed speakers' identities in temporal context to have enough disambiguating information to solve the required speech-face associations. Using all the available speech segments, i.e., the entire video length, is ideal. However, as described in ~\cite{sharma2022unsupervised}, the complexity of finding the speech-face association, using the Algorithm~\ref{algo:optimzation}, is a quadratic function of the context length (); thus, a lower value of  is desirable. The value of  acts as a tradeoff between the system's performance and time complexity.

As mentioned in the implementation details, we use  for \emph{SCMIA}. The system observes the context of identities from speech and faces appearing during the 500 speech segments and assumes that it consists of enough disambiguating information. At the same time, we use a context of only 50 segments  while employing \emph{GSCMIA}. Here we explore the role of required context of speech segments to solve speech-face associations.

In Figure~\ref{fig:context}, we show the variation in the performance of each system (\emph{SCMIA} \& \emph{GSCMIA}) with the length of context (). We observe a notable increase in the performance (reported in mAP) of the \emph{SCMIA} with the increase in . This increased performance follows from the fact that the system observes more disambiguating information with a larger context length () and thus better solves the speech-face association. In contrast, the system's performance with av-activity guidance (\emph{GSCMIA}) is robust to the variations in the context length. It validates the hypothesis that the AV-activity guides, positive and negative, contribute to the disambiguating information making even the smaller context length enough to solve the speech-face associations. We also observe that the performance of the guided system with just 25 speech segments is significantly better than the unguided system, even with a context of 500 speech segments. Thus guiding the speaker identity-based system with AV-activity information shows a 400-times improvement in the time complexity along with the noted improvement in performance. 
