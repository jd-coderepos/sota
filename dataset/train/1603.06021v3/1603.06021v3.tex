\documentclass[11pt]{article}
\usepackage{acl2016}

\aclfinalcopy \def\aclpaperid{265} 

\usepackage[leqno, fleqn]{amsmath}
\usepackage[breaklinks, colorlinks, linkcolor=black, urlcolor=black, citecolor=black, draft]{hyperref}
\usepackage{natbib}
\usepackage{times}
\usepackage{txfonts}
\usepackage{latexsym}


\usepackage{url}
\usepackage{qtree}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{stmaryrd}
\usepackage{gb4e}

\newcommand\todo[1]{\textcolor{blue}{\textbf{TODO:} #1}}
\newcommand\result[1]{\textcolor{red}{\textbf{RESULT NEEDED:} #1}}
\newcommand\question[1]{\textcolor{orange}{\textbf{OPEN QUESTION:} #1}}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{bmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{bmatrix}
        \fi
}

\newcommand{\shift}{\textsc{shift}}
\newcommand{\reduce}{\textsc{reduce}}

\def\ii#1{\textit{#1}}
\newcommand{\word}[1]{\emph{#1}}
\newcommand{\fulllabel}[2]{\b{#1}\newline\textsc{#2}}

\newcommand{\snli}[3]{{\vspace{0.25em}
{\small \setlength{\parindent}{0.6em} \hangindent=1.2em  \textbf{Premise:} #1\par}\vspace{0.25em}
{\small \setlength{\parindent}{0.6em} \hangindent=1.2em   \textbf{Hypothesis:} #2\par}\vspace{0.25em}
{\small \setlength{\parindent}{0.6em}  \textbf{Label:} #3\par}
}}

\providecommand{\norm}[1]{\lVert#1\rVert}

\hyphenation{Image-Net}

\noautomath

\title{A Fast Unified Model for Parsing and Sentence Understanding}

\author{
Samuel R.\ Bowman\thanks{~\,The first two authors contributed equally.} \\
\texttt{\small sbowman@stanford.edu} \\
\And
Jon Gauthier \\
\texttt{\small jgauthie@stanford.edu} \\
\And
Abhinav Rastogi \\
\texttt{\small arastogi@stanford.edu} \\
\AND
Raghav Gupta \\
\texttt{\small rgupta93@stanford.edu} \\
\And
Christopher D.\ Manning\\
\texttt{\small manning@stanford.edu}\\
\And
Christopher Potts\\
\texttt{\small cgpotts@stanford.edu}
\AND\
\colvec{5}
    {\vec{i}}
    {\vec{f}_l}
    {\vec{f}_r}
    {\vec{o}}
    {\vec{g}}
= \colvec{5}
     {\sigma\vphantom{\vec{i}}}
     {\sigma\vphantom{\vec{f}_l}}
     {\sigma\vphantom{\vec{f}_r}}
     {\sigma\vphantom{\vec{o}}}
    {\text{tanh}\vphantom{\vec{g}}}
\left(
W_{\text{comp}}
\colvec{3}
    {\vec{h}_s^1}
    {\vec{h}_s^2}
    {\vec{e}}
+ \vec{b}_{\text{comp}}
\right) \label{eqn:lstm1}
\\
\vec{c} = \vec{f}_l \odot \vec{c}_s^{\,2} + \vec{f}_r \odot \vec{c}_s^{\,1} + \vec{i} \odot \vec{g}
\\
\vec{h} = \vec{o} \odot \tanh(\vec{c})

\colvec{2}
    {\vec{h}}
    {\vec{c}}
= W_{\text{wd}} \vec{x}_{\text{GloVe}} + \vec{b}_{\text{wd}}

\vec{p}_{\text{a}} = \text{softmax}(W_{\text{trans}}\vec{h}_{\text{tracking}} + \vec{b}_{\text{trans}})

\vec{x}_{\text{classifier}} =
\colvec{4}
    {\vec{h}_{\text{premise}}}
    {\vec{h}_{\text{hypothesis}}}
    {\vec{h}_{\text{premise}} - \vec{h}_{\text{hypothesis}}}
    {\vec{h}_{\text{premise}} \odot \vec{h}_{\text{hypothesis}}}

\begin{split}
\mathcal{L}_{\text{m}} = &\mathcal{L}_{\text{s}} + \alpha \sum_{t=0}^{T-1} (\mathcal{L}^t_{\text{p}} + \mathcal{L}^t_{\text{h}}) + \lambda \norm{\theta}^2_2
\end{split}


\paragraph{Initialization, optimization, and tuning}

We initialize the model parameters using the nonparametric strategy of \citet{DBLP:journals/corr/HeZR015}, with the exception of the softmax classifier parameters, which we initialize using random uniform samples from .
 
We use minibatch SGD with the RMSProp optimizer \citep{tieleman2012lecture} and a tuned starting learning rate that decays by a factor of 0.75 every 10k steps. We apply both dropout \citep{srivastava2014dropout} and batch normalization \citep{2015SIoffeCSzegedy} to the output of the word embedding projection layer and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.

We train each model for 250k steps in each run, using a batch size of 32. We track each model's performance on the development set during training and save parameters when this performance reaches a new peak. We use early stopping, evaluating on the test set using the parameters that perform best on the development set.

We use random search to tune the hyperparameters of each model, setting the ranges for search for each hyperparameter heuristically (and validating the reasonableness of the ranges on the development set), and then launching eight copies of each experiment each with newly sampled hyperparameters from those ranges. Table~\ref{tab:hyperparams} shows the hyperparameters used in the best run of each model.


\begin{table*}[t]
  \centering\small
  \begin{tabular}{lrrrr}
    \toprule
Model                   & Params.    & Trans. acc. (\%)  &   Train acc. (\%)  &   Test acc. (\%) \\
\midrule
\multicolumn{5}{c}{\textbf{Previous non-NN results}}\\
Lexicalized classifier \citep{snli:emnlp2015}
                        & ---                & ---                    &   99.7   &   78.2      \\
\midrule
\multicolumn{5}{c}{\textbf{Previous sentence encoder-based NN results}}\\
100D LSTM encoders \citep{snli:emnlp2015}
                        & 221k               & ---               &   84.8   &   77.6      \\
1024D pretrained GRU encoders \citep{DBLP:journals/corr/VendrovKFU15}
                        & 15m                & ---              &   98.8   &   81.4       \\
300D Tree-based CNN encoders \citep{mou2015recognizing}
                        & 3.5m                & ---             &   83.4   &   82.1       \\
\midrule
\multicolumn{5}{c}{\textbf{Our results}}\\
300D LSTM RNN encoders         & 3.0m                  & ---                &   83.9      &   80.6       \\
300D SPINN-PI-NT (\ii{parsed input, no tracking}) encoders
                        & 3.4m                  & ---                &   84.4      &   80.9       \\
300D SPINN-PI (\ii{parsed input}) encoders
                        & 3.7m                  & ---                &   89.2      &   \textbf{83.2}       \\
300D SPINN (unparsed input) encoders
                        & 2.7m                  & 92.4           &   87.2    &   82.6      \\
    \bottomrule
  \end{tabular}
\caption{\protect\label{tab:results}Results on SNLI 3-way inference classification. Params.\ is the approximate number of trained parameters (excluding word embeddings for all models). Trans.\ acc.\ is the model's accuracy in predicting parsing transitions at test time. Train and test are SNLI classification accuracy.}
\end{table*}

\subsection{Models evaluated}

We evaluate four models. The four all use the sentence-pair classifier architecture described in Section~\ref{sec:classifier}, and differ only in the function computing the sentence encodings. First, a single-layer LSTM RNN \citep[similar to that of][]{snli:emnlp2015} serves as a baseline encoder. Next, the minimal SPINN-PI-NT model (equivalent to a TreeLSTM) introduces the SPINN model design. SPINN-PI adds the tracking LSTM to that design. Finally, the full SPINN adds the integrated parser.

We compare our models against several baselines, including the strongest published non-neural network-based result from \citet{snli:emnlp2015} and previous neural network models built around several types of sentence encoders.

\subsection{Results}

Table~\ref{tab:results} shows our results on SNLI. For the full SPINN, we also report a measure of agreement between this model's parses and the parses included with SNLI, calculated as classification accuracy over transitions averaged across timesteps.

We find that the bare SPINN-PI-NT model performs little better than the RNN baseline, but that SPINN-PI with the added tracking LSTM performs well. The success of SPINN-PI, which is the hybrid tree-sequence model, suggests that the tree- and sequence-based encoding methods are at least partially complementary, with the sequence model presumably providing useful local word disambiguation. The full SPINN model with its relatively weak internal parser performs slightly less well, but nonetheless robustly exceeds the performance of the RNN baseline.

Both SPINN-PI and the full SPINN significantly outperform all previous sentence-encoding models. Most notably, these models outperform the tree-based CNN of \citet{mou2015recognizing}, which also uses tree-structured composition for local feature extraction, but uses simpler pooling techniques to build sentence features in the interest of efficiency. Our results show that a model that uses tree-structured composition fully (SPINN) outperforms one which uses it only partially (tree-based CNN), which in turn outperforms one which does not use it at all (RNN).

The full SPINN performed moderately well at reproducing the Stanford Parser's parses of the SNLI data at a transition-by-transition level, with 92.4\% accuracy at test time.\footnote
  {Note that this is scoring the model against automatic parses, not a human-judged gold standard.}
However, its transition prediction errors are fairly evenly distributed across sentences, and most sentences were assigned partially invalid transition sequences that either left a few words out of the final representation or incorporated a few padding tokens into the final representation.

\subsection{Discussion}

The use of tree structure improves the performance of sentence-encoding models for SNLI. We suspect that this improvement is largely due to the more efficient learning of accurate generalizations overall, and not to any particular few phenomena. However, some patterns are identifiable in the results. 

While all four models under study have trouble with negation, the tree-structured SPINN models do quite substantially better on these pairs. This is likely due to the fact that parse trees make the scope of any instance of negation (the portion of the sentence's content that is negated) relatively easy to identify and separate from the rest of the sentence. For test set sentence pairs like the one below where negation (\word{not} or \word{n't}) does not appear in the premise but does appear in the hypothesis, the RNN shows  accuracy, while all three tree-structured models exceed . Only the RNN got the below example wrong:

\snli
{The rhythmic gymnast completes her floor exercise at the competition.}
{The gymnast cannot finish her exercise.}
{contradiction}
\noindent Note that the presence of negation in the hypothesis is correlated with a label of \textit{contradiction} in SNLI, but not as strongly as one might intuit---only  of these examples in the test set are labeled as contradictions.

In addition, it seems that tree-structured models, and especially the tree-sequence hybrid models, are more effective than RNNs at extracting informative representations of long sentences. The RNN model falls off in test accuracy more quickly with increasing sentence length than SPINN-PI-NT, which in turn falls of substantially faster than the two hybrid models, repeating a pattern seen more dramatically on artificial data in \citet{bowman2015trees}. On pairs with premises of 20 or more words, the RNN's 76.7\% accuracy, while SPINN-PI reaches 80.2\%. All three SPINN models labeled the following example correctly, while the RNN did not:

\snli
{A man wearing glasses and a ragged costume is playing a Jaguar electric guitar and singing with the accompaniment of a drummer.}
{A man with glasses and a disheveled outfit is playing a guitar and singing along with a drummer.}
{entailment}

We suspect that the hybrid nature of the full SPINN model is also responsible for its surprising ability to perform better than an RNN baseline even when its internal parser is relatively ineffective at producing correct full-sentence parses. It may act somewhat like the tree-based CNN, only with access to larger trees: using tree structure to build up local phrase meanings, and then using the tracking LSTM, at least in part, to combine those meanings.

Finally, as is likely inevitable for models evaluated on SNLI, all four models under study did several percent worse on test examples whose ground truth label is \textit{neutral} than on examples of the other two classes. \textit{Entailment}--\textit{neutral} and \textit{neutral}--\textit{contradiction} confusions appear to be much harder to avoid than \textit{entailment}--\textit{contradiction} confusions, where relatively superficial cues might be more readily useful.

\section{Conclusions and future work}

We introduce a model architecture (SPINN-PI-NT) that is equivalent to a TreeLSTM, but an order of magnitude faster at test time. We expand that architecture into a tree-sequence hybrid model (SPINN-PI), and show that this yields significant gains on the SNLI entailment task. Finally, we show that it is possible to exploit the strengths of this model without the need for an external parser by integrating a fast parser into the model (as in the full SPINN), and that the lack of external parse information yields little loss in accuracy.

Because this paper aims to introduce a general purpose model for sentence encoding, we do not pursue the use of soft attention \citep{bahdanau2014neural,rocktaschel2015reasoning}, despite its demonstrated effectiveness on the SNLI task.\footnote{Attention-based models like \citet{rocktaschel2015reasoning}, \citet{wang2015learning}, and the unpublished \citet{cheng2016long} have shown accuracies as high as 86.3\% on SNLI, but are more narrowly engineered to suit the task and do not yield sentence encodings.} However, we expect that it should be possible to productively combine our model with soft attention to reach state-of-the-art performance. 

Our tracking LSTM uses only simple, quick-to-compute features drawn from the head of the buffer and the head of the stack. It is plausible that giving the tracking LSTM access to more information from the buffer and stack at each step would allow it to better represent the context at each tree node, yielding both better parsing and better sentence encoding. One promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by \citet{dyer-EtAl:2015:ACL-IJCNLP}.

For a more ambitious goal, we expect that it should be possible to implement a variant of SPINN on top of a modified stack data structure with differentiable \textsc{push} and \textsc{pop} operations \citep[as in][]{grefenstette2015learning,joulin2015inferring}. This would make it possible for the model to learn to parse using guidance from the semantic representation objective, which currently is blocked from influencing the key parsing parameters by our use of hard \shift/\reduce\ decisions. This change would allow the model to learn to produce parses that are, in aggregate, better suited to supporting semantic interpretation than those supplied in the training data.


\subsubsection*{Acknowledgments}

We acknowledge financial support from a Google Faculty Research Award, the Stanford Data Science Initiative, and the National Science Foundation under grant nos.~BCS 1456077 and IIS 1514268. Some of the Tesla K40s used for this research were donated by the NVIDIA Corporation. We also thank Kelvin Guu, Noah Goodman, and many others in the Stanford NLP group for helpful comments.

\bibliographystyle{acl_natbib}
\bibliography{MLSemantics}

\end{document}
