
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}



\usepackage{amsfonts}       \usepackage{amsmath}

\usepackage{appendix}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{blindtext}
\usepackage{makecell}



\usepackage{soul}
\usepackage{booktabs}
\usepackage{nicefrac}       \usepackage{lipsum} 
\usepackage{stmaryrd}



\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{mathtools, cuted}
\usepackage{rotating}
\usepackage{adjustbox}  

\usepackage{wrapfig}  


\title{KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs}






\author{Yinquan Lu\textsuperscript{\rm 1,}\textsuperscript{\rm 5}\thanks{This work is done when Yinquan Lu, Haonan Lu and
Guirong Fu work at Huawei Technologies Co., Ltd.}\quad
Haonan Lu\textsuperscript{\rm 2}\footnotemark[1]\ \ \thanks{Corresponding author}\quad
Guirong Fu\textsuperscript{\rm 3}\footnotemark[1]\quad
Qun Liu\textsuperscript{\rm 4}\\
Huawei Technologies Co., Ltd.\textsuperscript{\rm 1} \quad
OPPO Guangdong Mobile Telecommunications Co., Ltd.\textsuperscript{\rm 2} \\
ByteDance\textsuperscript{\rm 3} \quad
Huawei Noah’s Ark Lab\textsuperscript{\rm 4} \quad
Shanghai AI Laboratory\textsuperscript{\rm 5} \\
\texttt{luyinquan@pjlab.org.cn,}\quad
\texttt{luhaonan@oppo.com}\\
\texttt{fuguirong@bytedance.com,}\quad
\texttt{qun.liu@huawei.com}
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Incorporating factual knowledge into pre-trained language models (PLM) such as BERT is an emerging trend in recent NLP studies. However, most of the existing methods combine the external knowledge integration module with a modified pre-training loss and re-implement the pre-training process on the large-scale corpus. Re-pretraining these models is usually resource-consuming, and difficult to adapt to another domain with a different knowledge graph (KG). Besides, those works either cannot embed knowledge context dynamically according to textual context or struggle with the knowledge ambiguity issue. In this paper, we propose a novel knowledge-aware language model framework based on fine-tuning process, which equips PLM with a unified knowledge-enhanced text graph that contains both text and multi-relational sub-graphs extracted from KG. We design a hierarchical relational-graph-based message passing mechanism, which allows the representations of injected KG and text to mutually update each other and can dynamically select ambiguous mentioned entities that share the same text. Our empirical results show that our model can efficiently incorporate world knowledge from KGs into existing language models such as BERT, and achieve significant improvement on the machine reading comprehension (MRC) tasks compared with other knowledge-enhanced models.
\end{abstract}

\section{Introduction}
Pre-trained language models benefit from the large-scale corpus and can learn complex linguistic representation~\citep{devlin-etal-2019-bert, liu2019roberta, yang2020xlnet}. Although they have achieved promising results in many NLP tasks, they neglect to incorporate structured knowledge for language understanding.
Limited by implicit knowledge representation, existing PLMs are still difficult to learn world knowledge efficiently~\citep{Poerner2019BERTIN, yu2020jaket}. For example, hundreds of related training samples in the corpus are required to understand the fact `` means an official prohibition or edict against something'' for PLMs. 

By contrast, knowledge graphs (KGs) explicitly organize the above fact as a triplet \textit{``(ban, hypernyms, prohibition)''}. Although domain knowledge can be represented more efficiently in KG form, entities with different meanings share the same text may happen in a KG (knowledge ambiguity issue). For example, one can also find \textit{``(ban, hypernyms, moldovan monetary unit)''} in WordNet~\citep{Miller95wordnet:a}. 
Recently, many efforts have been made on leveraging heterogeneous factual knowledge in KGs to enhance PLM representations. These models generally adopt two methods: (1). Injecting pre-trained entity embeddings into PLM explicitly, such as ERNIE~\citep{zhang2019ernie}, which injects entity embeddings pre-trained on a knowledge graph by using TransE~\citep{Bordes2013TranslatingEF}. 
(2). Implicitly learning factual knowledge by adding extra pre-training tasks such as entity-level mask, entity-based replacement prediction, etc.~\citep{wang2020kadapter, sun2020colake}. Some studies use both of the above two methods such as CokeBERT~\citep{su2020cokebert}. 

However, as summarized in Table~\ref{model comparison v1} of Appendix, most of the existing knowledge-enhanced PLMs need to re-pretrain the models based on an additional large-scale corpus, they mainly encounter two problems below: 
(1) Incorporating external knowledge during pretraining is usually resource-consuming and difficult to adapt to other domains with different KGs. By checking the third column of Table~\ref{model comparison v1} in Appendix, one can see that most of the pretrain-based models use Wiki-related KG as their injected knowledge source. These models also use English Wikipedia as pre-training corpus. They either use an additional entity linking tool (e.g. TAGME~\citep{Ferragina2010TAGMEOA}) to align the entity mention in the text to a single mentioned entity in a Wiki-related KG uniquely or directly treat hyperlinks in Wikipedia as entity annotations. These models depend heavily on the one-to-one mapping relationship between Wikipedia corpus and Wiki-related KG, thus they never consider handling knowledge ambiguity issue.
(2) These models with explicit knowledge injection usually use algorithms like BILINEAR~\citep{yang2015embedding} to obtain pre-trained KG embeddings, which contain information about graph structure. Unfortunately, their knowledge context is usually static and cannot be embedded dynamically according to textual context.

Several works~\citep{qiu-etal-2019-machine, yang-etal-2019-enhancing-pre} concentrate on injecting external knowledge based on fine-tuning PLM on downstream tasks, which is much easier to change the injected KGs and adapt to relevant domain tasks. They either cannot consider multi-hop relational information, or struggle with knowledge ambiguity issue. How to fuse heterogeneous information dynamically based on the fine-tuning process on the downstream tasks and use the information of injected KGs more efficiently remains a challenge.



\begin{wrapfigure}{r}{8.3cm}
\centering  
\includegraphics[height=38mm]{UKET_v3.jpeg} 

\caption{Unified Knowledge-enhanced Text Graph (UKET) consists of three parts corresponding to our model: (1) KG only part, (2) Entity link to token graph, (3) Text only graph.}
\label{KWG}
\end{wrapfigure}

To overcome the challenges mentioned above, we propose a novel framework named \textbf{KELM}, which injects world knowledge from KGs during the fine-tuning phase by building a \textbf{Unified Knowledge-enhanced Text Graph (UKET)} that contains both injected sub-graphs from external knowledge and text. The method extends the input sentence by extracting sub-graphs centered on every mentioned entity from KGs. In this way, we can get a Unified Knowledge-enhanced Text Graph as shown in Fig.~\ref{KWG}, which is made of three kinds of graph: (1) The injected knowledge graphs, referred to as the ``\textbf{KG only}'' part; (2) The graph about entity mentions in the text and mentioned entities in KGs, referred to as the ``\textbf{entity link to token}'' part. Entity mentions in the text are linked with mentioned entities in KGs by string matching, so one entity mention may trigger several mentioned entities that share the same text in the injected KGs (e.g. “Ford” in Fig.~\ref{KWG}); (3) The ``\textbf{text only}'' part, where the input text sequence is treated as a fully-connected word graph just like classical Transformer architecture~\citep{vaswani2017attention}.

\begin{figure*}[!htb]
\centering  
\includegraphics[width=0.7\linewidth]{main_arch_v2.jpeg}
\caption{Framework of KELM (left) and illustrates how to generate knowledge-enriched token embeddings (right).}
\label{module}
\end{figure*}

Based on this unified graph, we design a novel Hierarchical relational-graph-based Message Passing (HMP) mechanism to fuse heterogeneous information on the output layer of PLM. The implementation of HMP is via a Hierarchical Knowledge Enhancement Module as depicted in Fig.~\ref{module}, which also consists of three parts, and each part is designed for solving the different problems above: 
(1) For reserving the structure information and dynamically embedding injected knowledge, we utilize a relational GNN (e.g. rGCN~\citep{schlichtkrull2017modeling}) to aggregate and update representations of extracted sub-graphs for each injected KG (corresponding to the ``KG only'' part of UKET). All mentioned entities and their K-hop neighbors in sub-graphs are initialized by pre-trained vectors obtained from the classical knowledge graph embedding (KGE) method (we adopt BILINEAR here). In this way, knowledge context can be dynamically embedded, the structural information about the graph is also kept;
(2) For handling knowledge ambiguity issue and selecting relevant mentioned entities according to the input context, we leverage a specially designed attention mechanism to weight these ambiguous mentioned entities by using the textual representations of words/tokens to query the representations of their related mentioned entities in KGs (corresponding to the ``entity link to token'' graph of UKET). The attention score can help to select knowledge according to the input sentence dynamically. By concatenating the outputs of this step with the original outputs of PLM, we can get a knowledge-enriched representation for each token;
(3) For further interactions between knowledge-enriched tokens, we employ a self-attention mechanism that operates on the fully-connected word graph (corresponding to the ``text only'' graph of UKET) to allow the knowledge-enriched representation of each token to further interact with others.

We conduct experiments on the MRC task, which requires a system to comprehend a given text and answer questions about it. 
In this paper, to prove the generalization ability of our method, we evaluate KELM on both the extractive-style MRC task (answers can be found in a span of the given text) and the multiple-response-items-style MRC task (each question is associated with several choices for answer-options, the number of correct answer-options is not pre-specified).
MRC is a challenging task and represents a valuable path towards natural language understanding (NLU). With the rapid increment of knowledge, NLU becomes more difficult since the system needs to absorb new knowledge continuously. Pre-training models on large-scale corpus is inefficient. Therefore, fine-tuning the knowledge-enhanced PLM on the downstream tasks directly is crucial in the application.

\section{Related Work}
\subsection{Knowledge Graph Embedding}
We denote a directed knowledge graph as , where  and  are sets of entities and relations, respectively. We also define  as a set of facts, a fact stored in a KG can be expressed as a triplet , which indicates a relation  pointing from the head entity  to tail entity , where  and . KGE aims to extract topological information in KG and to learn a set of low-dimensional representations of entities and relations by knowledge graph completion task~\citep{yang2015embedding, lu2020dense}. 

\subsection{Multi-relational Graph Neural Network}
Real-world KGs usually include several relations. However, traditional GNN models such as GCN~\citep{kipf2017semisupervised}, and GAT~\citep{GAT} can only be used in the graph with one type of relation. ~\citep{schlichtkrull2017modeling, haonan2019graph} generalizes traditional GNN models by performing relation-specific aggregation, making it possible to encode relational graphs. The use of multi-relational GNN makes it possible to encode injected knowledge embeddings dynamically in SKG and CokeBERT.

\subsection{Joint Language and Knowledge Models}
Since BERT was published in 2018, many efforts have been made for further optimization, basically focusing on the design of the pre-training process and the variation of the encoder. For studies of knowledge-enhanced PLMs, they also fall into the above two categories or combine both of them sometimes. Despite their success in leveraging external factual knowledge, the gains are limited by computing resources, knowledge ambiguity issue, and the expressivity of their methods for the fusion of heterogeneous information, as summarized in Table~\ref{model comparison v1} of Appendix and the introduction part.

Recent studies notice that the architecture of Transformer treats input sequences as fully-connected word graphs, thus some of them try to integrate injected KGs and textual context into a unified data structure. Here we argue that UKET in our KELM is different from the WK graph proposed in CoLAKE/K-BERT. These two studies heuristically convert textual context and entity-related sub-graph into input sequences, both entities and relations are treated as input words of the PLM, then they leverage a Transformer with a masked attention mechanism to encode those sequences from the embedding layer and pre-train the model based on the large-scale corpus. Unfortunately, it is not trivial for them to convert the second or higher order neighbors related to textual context~\citep{su2020cokebert}, the structural information about the graph is lost. UKET differs from the WK graph of CoLAKE/K-BERT in that, instead of converting mentioned entities, relations, and text into a sequence of words and feeding them together into the input layer of PLM (they unify text and KG into a sequence), UKET unifies text and KG into a graph. Besides, by using our UKET framework, the knowledge fusion process of KELM is based on the representation of the last hidden layer of PLM, making it possible to directly fine-tune the PLM on the downstream tasks without re-pretraining the model. SKG also utilizes relational GNN to fuse information of KGs and text representation encoded by PLM. However, SKG only uses GNN to dynamically encode the injected KGs, which corresponds to part one of Fig.~\ref{KWG}. Outputs of SKG are made by directly concatenating outputs of graph encoder with the outputs of PLM. It cannot select ambiguous knowledge and forbids the interactions between knowledge-enriched tokens corresponding to part two and part three of Fig.~\ref{KWG}, respectively. KT-NET uses a specially designed attention mechanism to select relevant knowledge from KGs. For example, it treats all synsets of entity mentions within the WN18 as candidate KB concepts.
This limits the ability of KT-NET to select the most relevant mentioned entities. Moreover, the representations of injected knowledge are static in KT-NET, they cannot dynamically change according to textual context, the information about the original graph structure in KG is also lost.

\section{Methodology}

The architecture of KELM is shown in Fig.~\ref{module}. It consists of three main modules: (1) PLM Encoding Module; (2) Hierarchical Knowledge Enhancement Module; (3) Output Module. 

\subsection{PLM Encoding Module}

This module utilizes PLM (e.g.BERT) to encode text to get textual representations for passages and questions. An input example of the MRC task includes a paragraph and a question with a candidate answer, represented as a single sequence of tokens of the length : ==, where ,  and  represent all tokens for question, candidate answer and paragraph, respectively.  and  are special tokens in BERT and defined as a sentence separator and a classification token, respectively. -th token in the sequence is represented by , where  is the last hidden layer size of used PLM.

\subsection{Hierarchical Knowledge Enhancement Module}
This module is the implementation of our proposed HMP mechanism to fuse information of textual and graph context. We will formally introduce graph construction for UKET, and the three sub-processes of HMP in detail in the following sections. 

\subsubsection{Construction of UKET Graph}
(1) Given a set with  elements:  and input text, where  is the total number of injected KGs, and  indicates the -th KG. We denote the set of entity mentions related to the -th KG as =, where  is the number of entity mentions in the text. 
The corresponding mentioned entities are shared by all tokens in the same entity mention. All mentioned entities = are linked with their relevant entity mentions in the text, where  is the number of mentioned entities in the -th KG. We define this "entity link to token graph" in Fig.~\ref{KWG} as , where = is the union of entity mentions and their relevant mentioned entities,  is a set with only one element that links mentioned entities and their relevant entity mentions. 
(2) For -th mentioned entity  in , we retrieve all its K-hop neighbors  from the -th knowledge graph, where  is a set of -th mentioned entity's x-hop neighbors, hence we have =. We define "KG-only graph": , where = is the union of all mentioned entities and their neighbors within the K-hops sub-graph, and  is a set of all relations in the extracted sub-graph of -th KG. 
(3) The text sequence can be considered as a fully-connected word graph as pointed out previously. This ``text-only graph'' can be denoted as , where  is all tokens in text and  is a set with only one element that connects all tokens. 
Finally, we define the full hierarchical graph consisting of all three parts , , and , as Unified Knowledge-enhanced Text Graph (UKET).

\subsubsection{Dynamically Embedding Knowledge Context}
We use pre-trained vectors obtained from the KGE method to initialize representations of entities in . Considering the structural information of injected knowledge graph forgotten during training, we utilize  independent GNN encoders (i.e. ,  in Fig.~\ref{module}, which is the case of injecting two independent KGs in our experiment setting) to dynamically update entity embeddings of  injected KGs. We use rGCN to model the multi-relational nature of the knowledge graph. To update -th node of -th KG in -th rGCN layer:


Where  is a set of neighbors of -th node under relation .  is trainable weight matrix at -th layer and  is the hidden state of -th node at (1)-th layer.
After  updates,  sets of node embeddings are obtained. The output of the -th KG can be represented as , where  and  are the numbers of nodes of extracted sub-graph and the dimension of pre-trained KGE, respectively.

\subsubsection{Dynamically Selecting Semantics-Related Mentioned Entities}
To handle the knowledge ambiguity issue, we introduce an attention layer to weight these ambiguous mentioned entities by using the textual representations of tokens (outputs of Section 3.1) to query their semantics-related mentioned entities representations in KGs. Here, we follow the attention mechanism of GAT to update each entity mention embedding in : 

\noindent Where  is the output embeddings from the -th rGCN in the previous step.  is the hidden state of -th entity mention  in , and  is a set of neighbors of  in .  is a trainable weight matrix, we set == (thus .  is a nonlinear activation function.  is the attention score that weights ambiguous mentioned entities in the -th KG:



\noindent The representation  with a dimension of  is projected to the dimension of , before using it to query the related mentioned entity embeddings of : , where .  is a trainable weight vector.  is the transposition operation and  is the concatenation operation.

Finally, we concatenate outputs of  KGs with textual context representation to get final knowledge-enriched representation:

If token  can't match any entity in -th KG (say , we fill  in Eq.\ref{(7)} with zeros. Note that mentioned entities in KGs are not always useful, to prevent noise, we follow ~\citep{yang-mitchell-2017-leveraging}'s work and add an extra sentinel node linked to each entity mention in . The sentinel node is initialized by zeros and not trainable, which is the same as the case of no retrieved entities in the KG. In this way, according to the textual context, KELM can dynamically select mentioned entities and avoid introducing knowledge noise.

\subsubsection{Interaction Between Knowledge-enriched Token Embeddings}
To allow knowledge-enriched tokens' representations to propagate to each other in the text, we use a fully-connected word graph , with knowledge-enriched representations from outputs of the previous step, and employ the self-attention mechanism similar to KT-NET to update token's embedding. The final representation for -th token in the text is .

\subsection{Output Module}
\subsubsection{Extractive-style MRC task} 
A simple linear transformation layer and softmax operation are used to predict start and end positions of answers. For -th token, the probabilities to be the start and end position of answer span are:
, where  are trainable vectors and  is the number of tokens. The training loss is calculated by the log-likelihood of the true start and end positions: ,
\noindent where  is the total number of examples in the dataset,  and  are the true start and end positions of -th query's answer, respectively. During inference, we pick the span  with maximum  where  as predicted anwser. 

\subsubsection{Multiple-response-items-style MRC task}
Since answers to a given question are independent of each other, to predict the correct probability of each answer, a fully connected layer followed by a sigmoid function is applied on the final representation of  token in BERT. 

\section{Experiments}
\subsection{Datasets}
In this paper, we empirically evaluate KELM on both two types of MRC benchmarks in SuperGLUE~\citep{wang2020superglue}: \textbf{ReCoRD}~\citep{zhang2018record} (extractive-style) and \textbf{MultiRC}~\citep{MultiRC2018} (multiple-response-items-style). Detailed descriptions of the two datasets can be found in Appendix B. On both datasets, the test set is not public, one has to submit the predicted results to the organization to get the final test score. Since frequent submissions to probe the unseen test set are not encouraged, we only submit our best model once for each of the datasets, thus the statistics of the results (e.g., mean, variance, etc.) are not applicable. We use Exact Match (EM) and (macro-averaged) F1 as the evaluation metrics.



\noindent\textbf{External Knowledge} We adopt knowledge sources the same as used in KT-NET: WordNet and NELL~\citep{Carlson10}. Representations of injected knowledge are initialized by resources provided by~\citep{yang-mitchell-2017-leveraging}. The size of these embeddings is 100. We retrieve related knowledge from the two KGs in a given sentence and construct UKET graph (as shown in Section 3.2.1). More details about entity embedding and concepts retrieval are available in Appendix B.

\subsection{Experimental Setups}

\textbf{Baselines and Comparison Setting} 
Because we use  as the base model in our method, we use it as our primary baseline for all tasks. 
For fair comparison, we mainly compare our results with two fine-tune-based knowledge-enhanced models: KT-NET and SKG, which also evaluate their results on ReCoRD with \textbf{} as the encoder part. 
As mentioned in the original paper of KT-NET, KT-NET mainly focuses on the extractive-style MRC task. We also evaluate KT-NET on the multiple-response-items-style MRC task and compare the results with KELM.
We evaluate our approach in three different KB settings: , , and , to inject KG from WordNet, NELL, and both of the two, respectively (The same as KT-NET). 
Implementation details of our model are presented in Appendix C.

\begin{table}[!ht]
\begin{minipage}{0.48\linewidth}

\centering
\scriptsize
\setlength{\tabcolsep}{1.0mm}
 \begin{tabular}{clcccc}
  \toprule
    \multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Dev}} & \multicolumn{2}{c}{\textbf{Test}}  \\
  	&	\textbf{Model} & \textbf{EM}	&	\textbf{F1} &	\textbf{EM}	&	\textbf{F1} \\
  \midrule
~	&		&	70.2	&	72.2	&	71.3	&	72.0	\\
\midrule															
~	&	SKG+	&	70.9	&	71.6	&	72.2	&	72.8	\\
    ~	&		&	70.6	&	72.8	&	-	&	-	\\
    ~	&		&	70.5	&	72.5	&	-	&	-	\\
    ~	&		&	71.6	&	73.6	&	73.0	&	74.8	\\
    \midrule	
    
~	&		&	\textbf{75.4}	&	\textbf{75.9}	&	\underline{75.9}	&	\underline{76.5}	\\
~	&		&	74.8	&	75.3	&	\underline{75.9}	&	76.3	\\
    ~	&		&	\underline{75.1}	&	\underline{75.6}	&	\textbf{76.2}	&	\textbf{76.7}	\\
\bottomrule
\end{tabular}
\caption{Result on ReCoRD.}
\label{main results table extractive}

\end{minipage}\begin{minipage}{0.48\linewidth}  


\setlength{\belowcaptionskip}{-5mm}
\centering
\scriptsize
\setlength{\tabcolsep}{1.0mm}
 \begin{tabular}{clcccc}
  \toprule
    \multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Dev}} & \multicolumn{2}{c}{\textbf{Test}}  \\
  	&	\textbf{Model} & \textbf{EM}	&	\textbf{F1} &	\textbf{EM}	&	\textbf{F1} \\
\midrule
~	&		&	-	&	-	&	24.1	&	70.0	\\
\midrule															
~	&		&	26.7	&	\textbf{71.7}	&	25.4 & \textbf{71.1}	\\
    \midrule	
    
~	&		&	\underline{29.2}	&	70.6	&	25.9	&	69.2	\\
~	&		&	27.3	&	70.4	&	\underline{26.5}	&	70.6	\\
~	&		&	\textbf{30.3}	&	\underline{71.0}	&	\textbf{27.2}	&	\underline{70.8}	\\

\bottomrule
\end{tabular}
\caption{Result on MultiRC. [*] are from our implementation.}
\label{main results table multiple}
\end{minipage}

\end{table}

\subsection{Results}
The results for the extractive-style MRC task and multiple-response-items-style MRC task are given in Table~\ref{main results table extractive} and Table~\ref{main results table multiple}, respectively. The scores of other models are taken directly from the leaderboard of SuperGLUE and literature~\citep{qiu-etal-2019-machine, yang-etal-2019-enhancing-pre}. 
In this paper, our implementation is based on a single model, and hence comparing with ensemble based models is not considered. 
Best results are labeled in bold and the second best are underlined.

Results on the \textbf{dev set} of ReCoRD show that: (1) KELM outperforms , irrespective of which external KG is used. Our best KELM offers a \textbf{5.2/3.7} improvement in EM/F1 over . (2) KELM outperforms previous SOTA knowledge-enhanced PLM (KT-NET) by \textbf{+3.8 EM/+2.3 F1}. In addition, KELM outperforms KT-NET significantly in all three KB settings. On the \textbf{dev set} of MultiRC, the best KELM offers a \textbf{3.6} improvement in EM over KT-NET. Although the performance on F1 drop a little compared with KT-NET, we still get a gain of \textbf{+2.9 (EM+F1)} over the former SOTA model. 

Results on the \textbf{test set} further demonstrate the effectiveness of KELM and its superiority over the previous works. 
On ReCoRD, it significantly outperforms the former SOTA knowledge-enhanced PLM (finetuning based model) by \textbf{+3.2 EM/+1.9 F1}. And on MultiRC, KELM offers a \textbf{3.1/0.8} improvement in EM/F1 over , and achieves a gain of \textbf{+1.5 (EM+F1)} over KT-NET.

\section{Case Study}
This section uses an example in ReCoRD to show how KELM avoids knowledge ambiguity issue and selects the most relevant mentioned entities adaptively w.r.t the textual context. Recall that given a token , the importance of a mentioned entity  in -th KG is scored by the attention weight  in Eq.\ref{hierarchy}. To illustrate how KELM can select the most relevant mentioned entities, we analyze the example that was also used in the case study part of KT-NET. The question of this example is ``\textit{Sudan remains a XXX-designated state sponsor of terror and is one of six countries subject to the Trump administration’s \textbf{ban}}'', where the ``XXX'' is the answer that needs to be predicted. The case study in KT-NET shows the top 3 most relevant concepts from WordNet for the word ``ban'' are ``forbidding.n.01'', ``proscription.n.01'', and ``ban.v.02'', with the weights of 0.861, 0.135, and 0.002, respectively. KT-NET treats all synsets of a word as candidate KG concepts, both ``forbidding.n.01'' and ``ban.v.02'' will be the related concepts of the word ``ban'' in the text. Although KT-NET can select relevant concepts and suppress the knowledge noise through its specially designed attention mechanism, we still observe two problems from the previous case study: (1) KT-NET cannot select the most relevant mentioned entities in KG that share the same string in the input text. (2) Lack of ability to judge the part of speech (POS) of the word (e.g. ``ban.v.02'' gets larger weights than ``ban.n.04'').

\begin{wraptable}{r}{8.3cm}
\setlength{\belowcaptionskip}{-6mm}
\centering
\scriptsize
 \begin{tabular}{ccc}
  \toprule
  \makecell[c]{\textbf{Word in text}\\\textbf{(prototype)}} & \makecell[c]{\textbf{The most relevant}\\ \textbf{mentioned entity in}\\ \textbf{WordNet (predicted)}} & \textbf{Golden mentioned entity}\\ \hline
ford &  ford.n.05 (0.56)	&  ford.n.05	\\ \hline
  pardon &  pardon.v.02 (0.86)	& pardon.v.02	\\ \hline
  nixon &  nixon.n.01 (0.74) & nixon.n.01	\\ \hline
  lead &  lead.v.03 (0.73) & lead.v.03		\\ \hline
  outrage &  outrage.n.02 (0.62) & outrage.n.02 	\\ 
\bottomrule
\end{tabular}
\caption{Case study. Comparisons between the golden label with the most relevant mentioned entity in WordNet. The importance of selected mentioned entities is provided in the parenthesis.}
\label{case study v2}
\end{wraptable}

For KELM, by contrast, we focus on selecting the most relevant mentioned entities to solve the knowledge ambiguity issue (based on the ``entity link to token graph'' part of UKET). For injecting WordNet, by allowing message passing on the extracted sub-graphs (``KG only'' part of UKET), knowledge context can be dynamically embedded according to the textual context. Thus the neighbors' information of mentioned entities in WordNet can be used to help the word in a text to correspond to a particular POS based on its context. The top 3 most relevant mentioned entities in WordNet for the word ``ban'' in the above example are ``ban.n.04'', ``ban.v.02'', and ``ban.v.01'', with the weights of 0.715, 0.205, and 0.060, respectively.

To vividly show the effectiveness of KELM, we analyze ambiguous words in the motivating example show in Fig.~\ref{KWG} (The example comes from ReCoRD): 

``\textit{President Ford then pardoned Richard Nixon, leading to a further firestorm of outrage.}'' 

Table.~\ref{case study v2} presents 5 words in the above passage. For each word, the most relevant mentioned entity in WordNet with the highest score is given. The golden mentioned entity for each word is labeled by us.
Definitions of mentioned entities in WordNet that correspond to the word examples are listing in Table~\ref{case study def} of Appendix.


\section{Conclusion}

In this paper, we have proposed KELM for MRC, which enhances PLM representations with structured knowledge from KGs based on the fine-tuning process. Via a unified knowledge-enhanced text graph, KELM can embed the injected knowledge dynamically, and select relevant mentioned entities in the input KGs. In the empirical analysis, KELM shows the effectiveness of fusing external knowledge into representations of PLM and demonstrates the ability to avoid knowledge ambiguity issue. 
Injecting emerging factual knowledge into PLM during finetuning without re-pretraining the whole model is quite important in the application of PLMs and is still barely investigated. Improvements achieved by KELM over vanilla baselines indicate a potential direction for future research.

\section*{Acknowledgements}
The authors thank Ms. X. Lin for insightful comments on the manuscript. We also thank Dr. Y. Guo for helpful suggestions in parallel training settings. We also thank all the colleagues in AI Application Research Center (AARC) of Huawei Technologies for their supports.
\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}


\appendix
\section{Summary and Comparison of Recent Knowledge-enhanced PLMs}
\label{sec_pre:appendix}

Table~\ref{model comparison v1} shows a brief summary and comparison of recent knowledge-enhanced PLMs. Most of recent work concentrated on injecting external knowledge graphs during pre-training phase, which makes them inefficient in injecting external knowledge (e.g. LUKE takes about 1000 V100 GPU days to re-pretraining the RoBERTa based PLM model). Also, nearly all of them uses an additional entity linking tool to align the mentioned entities in the Wikidata to the entity mentions in the pre-trained corpus (English Wikipedia) uniquely. These methods never consider to resolve the knowledge ambiguity problem.


\begin{table*}[!ht]
\setlength{\belowcaptionskip}{-4mm}
\centering
\resizebox{\textwidth}{38mm}{
\begin{tabular}{lccccccccc}
     \toprule
  \textbf{Model} & \textbf{Downstream Task} & \textbf{Used KGs} & \makecell[c]{ \textbf{Need} \\ \textbf{Pre-train}}  &  \makecell[c]{ \textbf{Dynamically} \\ \textbf{Embedding KG} \\ \textbf{Context}}   & \makecell[c]{\textbf{Inject external} \\ \textbf{KG's Representations}} & \makecell[c]{\textbf{Support} \\ \textbf{Multi-relational}}  & \makecell[c]{ \textbf{Support} \\ \textbf{Multi-hop}}  & \makecell[c]{ \textbf{Handle Knowledge} \\ \textbf{Ambiguity Issue}} & \makecell[c]{ \textbf{Base Model}}\\ \hline 

 ERNIE~\citep{zhang2019ernie}  & \makecell[c]{Glue, Ent Typing \\ Rel CLS}  & Wikidata  & \makecell[c]{Yes\only entity embedding)}  & No  & \makecell[c]{No \\text{BERT}_{\text{base}}MLM, NSP)}  & No  & No  & \textbf{\makecell[c]{Yes \designed ATT mechanism\\ can solve KN issue)} &   \\ \hline
 
 KnowBERT~\citep{peters2019knowledge}  & \makecell[c]{Rel Extraction \\ Ent Typing}   & \makecell[c]{CrossWikis, \\WordNet}  & \makecell[c]{Yes\only entity embedding)}  & No  & \textbf{\makecell[c]{Yes \\text{BERT}_{\text{base}}MLM, Ent \\replacement task)}  & No  & No  & No  & No  & \makecell[c]{No \\text{BERT}_{\text{base}}MLM, \\Rel predition task)}  & No  & No  & \textbf{\makecell[c]{Yes\anchored entity mention to\\the unique id of Wikidata)}  &  \\ \hline
 
 KEPLER~\citep{wang2020kepler}  & \makecell[c]{Ent Typing \\ Glue, Rel CLS \\ Link Prediction}   & Wikidata  & \makecell[c]{Yes\Via link prediction task\\during pretraining)}}  & No  & \makecell[c]{No \\text{RoBERTa}_{\text{base}}MLM, Ent Mask task, \\ Ent category prediction, \\ Rel type prediction)}  &  \textbf{Yes} & \makecell[c]{Inject embeddings of \\ entity descriptions}  & \textbf{\makecell[c]{Yes\anchored entity mention to\\the unique id of Wikidata)} &   \\ \hline
 
 CoLAKE~\citep{sun2020colake} & \makecell[c]{Glue, Ent Typing \\ Rel Extraction}  & Wikidata  & \makecell[c]{Yes\treat relations as words)}}  & No  & \makecell[c]{No \\text{RoBERTa}_{\text{base}}MLM, Ent Mask task)}  & No  & No  & No & No  & \makecell[c]{No \\text{RoBERTa}_{\text{large}}MLM, NSP, \\Ent Mask task)}  & \textbf{Yes}  & \makecell[c]{Inject pretrained\\ entity embeddings\\ (TransE) explicitly}  & \textbf{\makecell[c]{Yes\anchored entity mention to\\the unique id of Wikidata)}  & \\ \hline
 
 SKG~\citep{qiu-etal-2019-machine}   & MRC  & \makecell[c]{WordNet, ConceptNet}  & \textbf{No} & \textbf{Yes}  & \makecell[c]{Inject pretrained\\ entity embeddings\\ (BILINER) explicitly}  & \textbf{\makecell[c]{Yes\\text{BERT}_{\text{large}}only entity embedding)}  & No  & \textbf{\makecell[c]{Yes \\text{BERT}_{\text{large}}Via multi-relational \\GNN to encode KG\\ context dynamically)}}  & \textbf{Yes}  & \textbf{\makecell[c]{Yes \\text{BERT}_{\text{large}}\text{BERT}_{\text{large}}\text{BERT}_{\text{large}}T\left\{4, 6, 8\right\}6\%TQAP\left\{10000, 15000, 20000\right\}10\%\text{BERT}_{\text{large}}\text{BERT}_{\text{large}}\text{BERT}_{\text{large}}\text{KELM}^{\text{BERT}_{\text{large}}}_{\text{WordNet}}\text{RoBERTa}_{\text{large}}\text{BERT}_{\text{large}}\text{BERT}_{\text{large}}\text{RoBERTa}_{\text{large}}\text{BERT}_{\text{large}}\text{RoBERTa}_{\text{large}}\text{BERT}_{\text{large}}\text{RoBERTa}_{\text{large}}^{*}\text{KELM}^{\text{BERT}_{\text{large}}}_{\text{Both}}\text{KELM}^{\text{RoBERTa}_{\text{large}}}_{\text{Both}}\text{BERT}_{\text{large}}\text{BERT}_{\text{large}}\text{KT-NET}^{\text{BERT}_{\text{large}}}_{\text{WordNet}}\text{KELM}^{\text{BERT}_{\text{large}}}_{\text{WordNet}}71.3\rightarrow 88.484.1\rightarrow88.9\text{BERT}_{\text{large}}\text{RoBERTa}_{\text{large}}\text{BERT}_{\text{large}}71.3\rightarrow 88.484.1\rightarrow88.9. A similar analysis can be also found in T5~\citep{raffel2020exploring}.
From our empirical results, we can summarize that general KG (e.g. WordNet) can not help too much for the PLMs pretrained on in-domain data. But it can still improve the performance of the model when the downstream tasks are out-of-domain. Further detailed analysis can be found in our appendix.

Finding a popular NLP task/dataset that is not related to the training corpus of modern PLMs is difficult. Pre-training on large-scale corpus is always good if we have unlimited computational resources and plenty of in-domain corpus. It has been evident that the simple finetuning of PLM is not sufficient for domain-specific applications. KELM can provide people another choice when they do not have such a large-scale in-domain corpus and want to incorporate incremental domain-related structural knowledge into the domain-specific applications.


\end{document}
