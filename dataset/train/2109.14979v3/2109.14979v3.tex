\documentclass{article}
\usepackage{arxiv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{caption}
\usepackage{mathtools}

\usepackage{array}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage[accsupp]{axessibility}
\usepackage{url}
\usepackage{fullpage}
\usepackage{times}
\usepackage{fancyhdr}
\usepackage[dvipsnames]{xcolor}
\usepackage[ruled,vlined]{algorithm2e}
\include{pythonlisting}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage[T1]{fontenc}
\usepackage{enumitem}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}		\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Moving Object Detection for Event-based Vision using Graph Spectral Clustering}

\author{Anindya Mondal\thanks{Authors have equal contributions.} , Shashant R\footnotemark[1] , Jhony H. Giraldo, Thierry Bouwmans, Ananda S. Chowdhury \\
 Department of Electronics and Telecommunication Engineering, Jadavpur University, India\\
 
Laboratoire Mathématiques, Image et Applications (MIA), La Rochelle Université, France \\
{\tt\small \{anindyam.jan, shashant7699\}@gmail.com, \{jgiral01, tbouwman\}@univ-lr.fr,} \\
{\tt\small as.chowdhury@jadavpuruniversity.in}}


\date{}

\renewcommand{\headeright}{ICCVW 2021}
\renewcommand{\undertitle}{Accepted in IEEE/CVF ICCVW 2021 (Virtual)}


\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	Moving object detection has been a central topic of discussion in computer vision for its wide range of applications like in self-driving cars, video surveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are bio-inspired sensors that mimic the working of the human eye. Unlike conventional frame-based cameras, these sensors capture a stream of asynchronous ‘events’ that pose multiple advantages over the former, like high dynamic range, low latency, low power consumption, and reduced motion blur. However, these advantages come at a high cost, as the event camera data typically contains more noise and has low resolution. Moreover, as event-based cameras can only capture the relative changes in brightness of a scene, event data do not contain usual visual information (like texture and color) as available in video data from normal cameras. So, moving object detection in event-based cameras becomes an extremely challenging task. In this paper, we present an unsupervised Graph Spectral Clustering technique for Moving Object Detection in Event-based data (GSCEventMOD). We additionally show how the optimum number of moving objects can be automatically determined. Experimental comparisons on publicly available datasets show that the proposed GSCEventMOD algorithm outperforms a number of state-of-the-art techniques by a maximum margin of 30. 
\end{abstract}


\keywords{Event-based Vision \and Moving Object Detection \and Spectral Clustering}


\section{Introduction}
Recent developments in materials engineering, fabrication technology, VLSI (very-large-scale-integration) design techniques, and neuro-science have facilitated the newly discovered concept of bio-inspired visual sensors and processors (\cite{chen2020event}). Event-based cameras are such neuromorphic bio-inspired visual sensors that mimic the mode of action of a biological retina, bringing a paradigm shift in computer vision technology (\cite{chen2020event}). One of the earliest contributions to put the concept of event-based cameras in action was made by Lichtsteiner \textit{et al.} (\cite{lichtsteiner2006128}), where they  developed an event-based neuromorphic sensor based on biological principles. These bio-inspired cameras capture a stream of asynchronous events in contrast to the traditional RGB cameras which acquire images at a fixed-frame rate as specified by an external clock (\cite{gallego2020event}). In event cameras, each pixel memorizes the log intensity each time an event is sent and incessantly monitors for a sufficient change in magnitude from this memorized value. When this change crosses a threshold value, an event is recorded by the camera, which is then transmitted by the sensor in form of its location , its timestamp  (in microseconds), and its polarity  (\textit{i.e.}, whether the pixel had become brighter or darker). In Fig. \ref{fig:evcam}, we illustrate how an event camera works following (\cite{gallego2020event}). In event-based cameras,  sensors capture the per-pixel brightness changes (called events) asynchronously instead of measuring the absolute brightness of all pixels at a constant rate. As a result, traditional vision algorithms designed to process video data from frame-based cameras often become unsuitable for processing event data. 

\begin{figure*}
\begin{center}
\includegraphics[width=0.88\textwidth]{ev_cam_diag.png}
\end{center}
   \caption{Visualization of the output from a neuromorphic vision sensor and a standard frame-based camera when facing a rotating disk with a black dot. Inspired by \cite{gallego2020event}}.
    \label{fig:evcam}
\label{fig:evcam}
\end{figure*}

Moving object detection is an important task in the field of computer vision which appears in autonomous vehicles, surveillance systems and alike. In moving object detection, we identify the physical movement of an object in a given region or area (\cite{kulchandani2015moving}). However, event-based moving object detection systems are far inferior to their frame-based counterparts (\cite{piccardi2004background,agarwal2016review}), because only a limited amount of reliable labels are available for training (\cite{bi2020graph}). In fact, this limitation restricts the use of deep learning in solving motion detection problems (from event data).   

Moving object detection in event-based cameras becomes further challenging as event cameras can only capture the relative changes in brightness of a scene and not usual visual features like color and texture (\cite{pikatkowska2012spatiotemporal}). 

Recently few attempts have been made to extend the task of moving object detection in neuromorphic vision, either by using parametric models (\cite{mitrokhin2018event,stoffregen2019event}) or by using traditional clustering algorithms (\cite{pikatkowska2012spatiotemporal,hinz2017online,chen2018neuromorphic}). Unfortunately, those parametric models are complex and require several assumptions to hold for their proper working. Furthermore, traditional clustering algorithms are sensitive to noise generated from the motion of the objects and sensor defects (temporal and shot noise) (\cite{chen2020event}).
Widespread use of graph-based representations have been noticed in recent computer vision and machine learning applications (\cite{giraldo2020graph,giraldo2021graphbgs,xia2021graph}). In some cases (like financial and banking data, social networks, mobility and traffic patterns, marketing preferences, fads, etc.), the data resides on irregular and complex structures, which can be efficiently tackled with graph-based methods (\cite{ortega2018graph}). Note that neuromorphic-based sensors activate asynchronously in time. So, the data streams are produced at irregular space-time coordinates which depends upon the scene activity (\cite{bi2020graph}). Therefore, by representing the events as graphs, one can maintain the event asynchronicity and sparsity and exploit their advantages (\cite{bi2020graph}).

Our method improves the previous works (\cite{pikatkowska2012spatiotemporal,chen2018neuromorphic,hinz2017online}) in multiple ways. Firstly, we do not need any prior knowledge about the actual number of moving objects in a scene. Secondly, our model is successful in detecting the moving objects from the noisy event data. We construct a similarity graph using k-Nearest Neighbors (k-NN). Then, graph spectral clustering is applied for detecting the moving objects. Our contributions can be summarized as follows:

\begin{itemize}[leftmargin=*]
    \itemsep0em
    \item We introduce graph-spectral clustering (\cite{martin2018robust,luo2003spectral,von2007tutorial,ng2001spectral,panda2017nystrom}) for detecting moving objects in event data. We use this method because it can handle clusters of arbitrary (including non-convex) shapes and it does not make any prior assumptions about the cluster shapes (\cite{meila2016spectral}).
    \item We show, using silhouette analysis (\cite{shutaywi2021silhouette}), how the actual number of moving objects in the event-based data can be automatically determined.
    \item We show that our method (GSCEventMOD) allows successful detection of moving objects in a scene. Experimental results show that GSCEventMOD performs better than previous state-of-the-art techniques on a publicly available dataset (\cite{almatrafi2020distance}). We also demonstrate the versatility of our method by testing it on a synthetically generated dataset.
\end{itemize}
The rest of the paper is organized as follows:
In Section 2 we briefly discuss the related works. In Section 3, we describe our proposed method. The details of our experiments and comparisons with the state-of-the-art are presented in Section 4. Finally, in Section 5 we conclude the paper with outlines for direction of future work.

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{pipeline.png}
\end{center}
   \caption{An illustrative overview of the proposed GSCEventMOD. First, we sample the event space-time volume on the basis of the timestamps of the corresponding grayscale images. Secondly, we construct a similarity graph using k-NN. We also perform eigendecomposition on the Graph Laplacian and take the first  eigenvectors to get the moving objects as clusters. We use silhouette analysis for determining the optimal value of .}
    \label{fig:pipeline}
\label{fig:short}
\end{figure*}


\section{Related work}
In this section, we briefly discuss the available techniques for moving object detection in frame-based cameras and see how they are extended to neuromorphic vision. Many classical approaches that are proposed for moving object detection are based on the geometrical understanding of the scene like in (\cite{menze2015object}). There are several deep learning-based approaches too, like in (\cite{huang2019deep,zhu2020moving}), where they have used multi-layered convolutional neural networks for detecting moving objects. However, using deep learning has its own disadvantages, for example, the models are very complex and they require a large amount of labeled data to avoid over-fitting. Indeed, there are no general answers in the literature about the sample complexity required in the deep learning regimen (\cite{giraldo2020graph}).

Also during high-speed motion, traditional RGB cameras suffer from high motion blur and perform badly in challenging situations (like abrupt variation in ambient brightness, remote areas with power scarcity, etc.). Here, event-based cameras (\cite{chen2020event,gallego2020event}) can be useful (\cite{rebecq2019high}). This is because these neuromorphic sensors capture the scene dynamics only (as the change in brightness occurs only when there is motion in the scene) and detection of the moving objects from the static background is done by the sensor itself (\cite{pikatkowska2012spatiotemporal}). So, the task of moving object detection may seem trivial if there is only one object in a scene as all the generated events correspond to the motion of the object (ignoring noise). However, when there are multiple moving objects in a scene, the task of detecting them all becomes really difficult. This is because unlike the frame-based cameras, neuromorphic vision sensors capture only the binary changes in brightness, which do not contain much visual information (\cite{pikatkowska2012spatiotemporal}).

In 2012, Piątkowska \textit{et al.} (\cite{pikatkowska2012spatiotemporal}) have studied the possibility of using Gaussian Mixture Models (GMMs) (\cite{reynolds2009gaussian}) for multiple persons tracking using event-based cameras. However, a model using GMM is too sensitive to noise, as it assumes that each data point (here the events) is independent of its neighbors, thus ignoring the similarity relations among those points (\cite{nguyen2011gaussian}).
 
In 2018, Chen \textit{et al.} (\cite{chen2018neuromorphic}) and Hinz \textit{et al.} (\cite{hinz2017online}) have performed a preliminary multi-vehicle detection and tracking using classical clustering approaches (like DBSCAN (\cite{khan2014dbscan}), MeanShift (\cite{derpanis2005mean}), etc.).
However, these methods also perform poorly as they are sensitive to noise and require tuning of quite a few parameters (\cite{feng2018robust}). Here graph spectral clustering performs better, because it requires the tuning of a single parameter (i.e. the number of clusters). In this work, we show how the number of clusters can be determined automatically using silhouette analysis.

\section{Proposed Method}
In our work, we use graph-spectral clustering (\cite{luo2003spectral,martin2018robust}) for the task of detecting moving objects in event-based data. We show that the application of graph spectral clustering can find meaningful clusters of arbitrary shapes under realistic separations. The schematic of the proposed unsupervised model, termed as, Graph Spectral Clustering technique in Event data for Moving Object Detection (GSCEventMOD) is shown in Fig. \ref{fig:pipeline}. A sampling strategy is first employed for obtaining a small set of events to facilitate computationally efficient processing. The sampled events are then used to construct a k-NN graph. Then we apply spectral clustering on the k-NN graph. The clusters represent the moving objects. 


\subsection{Sampling and Graph Construction}
The events generated due to the moving objects are treated as sparse point-cloud data in the 3-dimensional space-time volume (\cite{chen2020event}). Let us consider there are  event points and corresponding  grayscale images captured at uniform timestamps  ( ranging from 1 to ). To generate meaningful samples, we divide the whole space-time event volume into  partitions. Each of these individual partitions consists of  events that occurred before timestamp . Thus, . 

We adopt a uniform sampling strategy for obtaining a small set of neuromorphic events. Let  events are selected from each of these partitions. Each event-point in  is represented as a tuple sequence:

where  indicates the spatial address at which the spike event had occurred,  is the timestamp indicating when the event was generated and  represents the total number of events. 

As the events are sparse in the spatio-temporal domain (image plane evolving in time), the underlying graph is generally unstructured (as opposed to the graph of pixels in an image, which is regular) (\cite{zhou2020event}). To address the irregularity, a graph is constructed with  events using the popular k-NN strategy (\cite{ortega2018graph}). The neighborhood is based on the spatio-temporal similarity between the event points in the point cloud. Let us define a graph , where   is the set of nodes or vertices and  is the set of edges. Here we represent each event  as a node  in the graph (). We connect  and  with an edge  () if either  is among the  nearest neighbors of  or  is among the  nearest neighbors of . 

The value of  in k-NN has been chosen carefully. A small value of  would make the result sensitive to noise whereas a large value of  would make the process computationally expensive. As there is no definite statistical method to find an optimal  and there is abundance of noise in the event data (due to current limitations of neuromorphic vision sensors (\cite{chen2018neuromorphic}), we have experimentally set its value for each sequence. 

\subsection{Graph Laplacian}
Let  be the adjacency matrix of the graph  with , where the set of vertices is  and . Note that  indicates whether node  is connected to the node . As  is unweighted and undirected,  and  = . The degree of a vertex  is given by . We construct the degree matrix  which is a diagonal matrix with the degrees  of the respective vertices  as the diagonal elements. With the adjacency matrix  and the degree matrix , the graph Laplacian  is given by: 


Here,  is the unnormalized Laplacian matrix of the graph  and . 
The eigenvectors of  are calculated next. Let  has  eigenvalues denoted by:  (\cite{von2007tutorial}). Further, let  be the corresponding eigenvectors, which can be obtained by solving the generalized eigenproblem  = , where . 

\subsection{Graph Spectral Clustering}
In this work, the moving objects are determined as the connected components (clusters) in the graph. Let us consider there are  () moving objects which means there are  clusters. The clusters are obtained by the spectral clustering algorithm following (\cite{shi2000normalized}):

\begin{itemize}[leftmargin=*]
    \itemsep0em
    \item Let  be the matrix whose columns are the first  eigenvectors  where .
    \item For , let  be the vector corresponding to th row of .
    \item Now the vectors  are clustered using -means algorithm into  clusters . Note that . 
    \item This induces a set of clusters  on the original data, in which  is assigned to  if  is assigned to  (\cite{tung2010enabling}).
    
\end{itemize}

\begin{algorithm}[]
\KwIn{Sampled events in space-time (from the user).}
\KwOut{Moving objects as clusters.}

\nl  Construct a similarity graph from each event sample by using k-NN.

\nl Compute the adjacency matrix for the graph.

\nl Compute the unnormalized Laplacian.

\nl Compute the eigenvalues and eigenvectors.

\nl \SetKwFor{}{}{}{}\textbf{for} a range of values of :

\hspace{\parindent}  Take the first  eigenvectors and construct another matrix taking those eigenvectors as
columns.

\hspace{\parindent}   Take the first  rows from that newly formed matrix.

\hspace{\parindent}   Obtain the clusters. 

\hspace{\parindent}   Compare and Update the maximum Silhouette Coefficient (SC) obtained so far.

\nl Get the moving objects as clusters and their corresponding labels for which SC is maximum.

    \caption{{\bf GSCEventMOD} \label{GSCEventMOD}}

\end{algorithm}
    


\subsection{Optimal Number of Clusters}
Determining the optimal number of clusters in data is a crucial task. Even though there are no foolproof methods available for determining its value, there are few widely used methods like the elbow method (\cite{syakur2018integration}), Rand Index (\cite{santos2009use}), adjusted Rand Index (\cite{hubert1985comparing}) and silhouette analysis (\cite{shutaywi2021silhouette}). In our work, we use the silhouette analysis strategy to find the actual number of clusters in the data, which, in our case, is the actual number of moving objects in a particular scene. With this strategy we do not need to have any prior knowledge about the exact number of clusters. A range of values for the number of clusters is inputted instead and the optimal value of the number of cluster is automatically determined. 

Silhouette value measures the difference between the within-cluster tightness and separation from the rest (\cite{rousseeuw_2002}). It is defined for each sample point and is composed of two scores:
\begin{itemize}[leftmargin=*]
    \itemsep0em
    \item : The mean distance between a sample point  and all other points in the same cluster.
    \item : The mean distance between a sample point  and all other points in all other clusters of which it is not a member.
\end{itemize}
Now the silhouette value  for that single sample is given as:


Kauffman \textit{et al.} (\cite{kaufman_rousseeuw_1990}) proposed the term Silhouette Coefficient , which returns the maximum value of the mean  over all data points of a specific sequence.  is defined as:

where  represents the mean  over the entire data of a specific sequence for a definite number of clusters .   
The value of  is bounded in between  and , where a score near to  shows greater intra-cluster tightness and greater inter-cluster distance. A higher value means that the clusters are dense and well separated, leading to a meaningful representation of the clusters.

\begin{figure}
\begin{center}
\includegraphics[width = 0.6\textwidth]{sil_comp.png}
\end{center}
   \caption{Few outputs from our framework and their corresponding Silhouette plots. Note how the maxima in the two plots are correctly representing the number of moving objects in the scenes (best viewed in color).}
    \label{fig:sample and sil plots}
\label{fig:sample and sil plots}
\end{figure}

In our work, we use  to determine the optimal value of . We take some values of , ranging from 2 to 10 (by the knowledge from the ground truth about the maximum number of moving objects) and plot their respective s. From those plots, we take the value of  for which the magnitude of  is maximum. This  is the required optimal number of clusters. This is how we get the actual number of moving objects in a scene. We show some sample  vs  plots from our work in Fig. \ref{fig:sample and sil plots}. The whole methodology is summarized in Algorithm \ref{GSCEventMOD}.


\section{Experimental Results}
In this section, we evaluate our proposed method. First, we show the datasets used and metrics in Sections 4.1 and 4.2 respectively. In Section 4.2, we also compare our method against the state-of-the-art baselines (\cite{pikatkowska2012spatiotemporal,chen2018neuromorphic,hinz2017online}) and show how it is performing better than theirs.

We experimentally set the value of nearest neighbors at 30 for hands, 100 for cars and 25 for street sequences. 
All the experiments were performed on a computer with 2 Intel\textsuperscript{\tiny\textregistered} Xenon\textsuperscript{\tiny\textregistered} CPUs, each with a clock frequency of  GHz.

\begin{figure*}
\begin{center}
\includegraphics[width = 0.81\textwidth]{diag_comp.png}
\end{center}
   \caption{Visual results for GSCEventMOD and other SOTA methods on the DistSurf (\cite{almatrafi2020distance}) (marked with ) and the synthetic dataset (marked with ). We show that our method performs significantly better than the other compared methods (best viewed in color).}
    \label{fig:comparison diagram}
\label{fig:comparison diagram}
\end{figure*}



\subsection{Datasets}

As the datasets used in the baseline methods (\cite{pikatkowska2012spatiotemporal,chen2018neuromorphic}) are not publicly available, we evaluate our algorithm on the DistSurf\footnote{\url{https://sites.google.com/a/udayton.edu/issl/software/dataset}} (\cite{almatrafi2020distance}) dataset, where the events are recorded using the IniVation DAViS346 camera. This camera has a  spatial resolution and outputs frames up to  frames per second for RGB/ grayscale images and in microsecond resolution for the events. This dataset contains sequences (both events and their corresponding grayscale images) captured in both indoor and outdoor environments, involving multiple moving objects (cars, pedestrians, hands, etc.). Few sequences from this dataset (\cite{almatrafi2020distance}) are shown in Fig. \ref{fig:comparison diagram} (hands and cars). 

We further evaluate our method on a synthetic dataset generated using the v2e framework\footnote{\url{https://sites.google.com/view/video2events/home}} (\cite{hu2021v2e}). Using (\cite{hu2021v2e}), we convert the grayscale frames from a video to realistic events. The synthetic sequences are shown in Fig. \ref{fig:comparison diagram} (street).


\begin{table}[]
\centering
\caption{Evaluation metrics}
\label{tab:metric}
\begin{tabular}{|c|c|}
\hline
\textbf{Metrics} & \textbf{Formula} \\ \hline
True positive (TP) &  \\ \hline
False positive (FP) &  \\ \hline
False negative (FN) & No ground truth detected \\ \hline
Precision (P) &  \\ \hline
Recall &  \\ \hline
F measure &  \\ \hline
\end{tabular}
\end{table}

\begin{table*}[]
\centering
\caption{Comparison of GSCEventMOD with the state-of-the-art methods. Note that the precision and recall scores are in percentage. The best results are in bold.}
\label{tab:metric table}
\makebox[\linewidth]{
\scalebox{0.75}{
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Sequence} & \multicolumn{3}{c|}{{DBSCAN (\cite{chen2018neuromorphic})}} & \multicolumn{3}{c|}{{Meanshift (\cite{chen2018neuromorphic})}} & \multicolumn{3}{c|}{{GMM (\cite{pikatkowska2012spatiotemporal})}} & \multicolumn{3}{c}{\textbf{GSCEventMOD (Ours)}}       \\
                          & Recall   & Precision   & F measure  & Recall    & Precision    & F measure   & Recall  & Precision  & F measure & Recall & Precision & F measure \\ \hline
Hands (\cite{almatrafi2020distance})                     & 66.57    & 77.33       & 71.55      & 71.56     & 79.30        & 75.23       & 87.14   & 88.67      & 87.90     & \textbf{90.68}  & \textbf{91.56}     & \textbf{91.12}     \\ Cars (\cite{almatrafi2020distance})                      & 48.58    & 31.50       & 38.22      & 41.68     & 50.00           & 45.46       & 50.26   & 77.60      & 61.00     & \textbf{56.40}  & \textbf{82.29}     & \textbf{66.93}     \\ Street (\cite{hu2021v2e})                    & 44.94    & 34.87       & 39.27      & 38.95     & 52.00           & 44.53       & 43.78   & 67.59      & 53.14     & \textbf{58.64}  & \textbf{84.49}     & \textbf{69.23}     \\ \hline
\end{tabular}}
}
\end{table*}






\subsection{Evaluation Metrics and Comparison}

Multiple protocols are available for the detection of moving objects in frame-based cameras. We can apply many of them in event-based data too. As in (\cite{chen2018neuromorphic}), we accumulate the events corresponding to their grayscale frames in different time intervals. Now to decide how well the detected objects (in event-data) are located with respect to the ground truth (in frame-based data), we perform a coverage test over the whole dataset (\cite{pikatkowska2012spatiotemporal}) for the correctly detected objects  (area of bounding box around moving objects in event-data) and the ground truth  (area of bounding box around moving objects in ground truth). As the Distsurf (\cite{almatrafi2020distance}) dataset does not have ground-truths for moving object detection, we have labeled them manually. The metrics used are shown in Table: \ref{tab:metric}. 

Visual comparisons of our method with the state-of-the-art approaches are shown in Fig. \ref{fig:comparison diagram} and the quantitative comparisons are shown in Table: \ref{tab:metric table}. From Fig. \ref{fig:comparison diagram}, we can see that the clusters represent the moving objects. However, we have noise surrounding those moving objects, which is generated mainly due to ambient disturbances and sensor defects. Despite that noise, our model could successfully detect the moving objects in comparison to other state-of-the-art methods (\cite{pikatkowska2012spatiotemporal,chen2018neuromorphic,hinz2017online}). We can also see that at the time of occlusions, while the other models fail to distinguish between the moving objects in the scene, our model is performing significantly better. Note that as the number of moving objects in a scene increases, the difference between the performance of our model and the previous methods becomes more prominent. 

Nevertheless, there are some failure cases. For example, Fig. \ref{fig:fail cases} shows few samples, where the moving cars are so close to each other, that our model merges them into a single cluster (Case 1). Also, we could see that while a moving object is significantly larger than the others, the model is breaking it into multiple clusters (Case 2). However, even in such situations, our model performs better than the other methods, as can be seen from Fig. \ref{fig:fail cases}.

\begin{figure*}
\begin{center}
\includegraphics[width = 0.9\textwidth]{fail_cases.png}
\end{center}
   \caption{Some failure cases in GSCEventMOD and other SOTA methods. Case 1: When the moving objects are too close to each other.  Case 2: When one of the moving objects is significantly larger than all the others.}
   \label{fig:fail cases}
\end{figure*}

\section{Conclusions}
In this paper, we proposed a novel method, termed as GSCEventMOD for event-based moving object detection using graph spectral clustering. We demonstrated that detecting moving objects using neuromorphic vision sensors can perform well in the challenging situations like fast motions and abrupt changes in the lighting conditions. GSCEventMOD requires minimal pre-processing as scene dynamics is captured by the sensor itself. GSCEventMOD is shown to outperform some of the previous approaches in event-based vision. We validated GSCEventMOD with synthetic data and real-world data captured under varied environments to demonstrate its flexibility. Overall, we believe our method will be suitable for multiple computer vision applications like autonomous vehicles, robotics, remote surveillance, and others.

In future, we plan to incorporate suitable motion models. Another direction of future research would be to explore semi-supervised learning to improve the solution.  


\bibliographystyle{unsrtnat}
\bibliography{references}  













\end{document}
