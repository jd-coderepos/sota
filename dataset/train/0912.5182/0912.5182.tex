\documentclass[11pt]{article}
\usepackage{wide, amsmath, amssymb, sariel, graphicx, euscript, ifthen}
\usepackage{times}



\def\mathbi#1{\textbf{\em #1}}
\def\tt{{\text{\boldmath }}}
\def\ss{{\text{\boldmath }}}
\def\bb{b}
\def\tvec{c}
\def\amat{M}
\def\rr{{\text{\boldmath }}}
\def\qq{q}
\def\vv{v}
\def\uu{u}
\def\xx{x}
\def\yy{{\text{\boldmath }}}
\def\zz{{\text{\boldmath }}}
\def\ww{{\text{\boldmath }}}
\def\llambda{{\text{\boldmath }}}
\def\ttau{{\text{\boldmath }}}
\def\pred{\mathop{\text{\sc Predecessor}}}
\def\succ{\mathop{\text{\sc Sucessor}}}
\def\TT{\EuScript{T}}
\def\MM{\mathbb{M}}
\def\Et{\tilde{E}}
\def\reals{\mathbb{R}}
\def\ssigma{{\text{\boldmath }}}
\def\zz{z}

\hyphenation{Aff-ine-Trans-form In-ter-val-Trans-form Thresh-old-Trans-form}

\def\eval{\mathop{\mbox{\textsc{Evaluate}}}}
\def\evali{\mathop{\mbox{\textsc{Evaluate}}}^{-1}}
\def\ins{\mathop{\mbox{\textsc{Insert}}}}
\def\delete{\mathop{\mbox{\textsc{Delete}}}}
\def\atrans{\mathop{\textsc{Affine}}}
\def\itrans{\mathop{\textsc{Interval}}}
\def\ttrans{\mathop{\textsc{Threshold}}}
\def\Update{\mathop{\textsc{Update}}}
\def\UnUpdate{\mathop{\textsc{unUpdate}}}
\def\integrate{\mathop{\textsc{Integrate}}}

\def\merge{\mathop{\textsc{Merge}}}
\def\include{\mathop{\textsc{Include}}}
\def\unmerge{\mathop{\textsc{unMerge}}}
\def\uninclude{\mathop{\textsc{unInclude}}}
\def\pred{\mathop{\textsc{Pred}}}
\def\succ{\mathop{\textsc{Succ}}}

\newcommand\myCaption[1]{\small\refstepcounter{figure}\figurename\ \thefigure :\ #1}

\newcommand{\lir}{LIR }
\newcommand{\ir}{IR }
\newcommand{\ur}{UR }
\newcommand{\hF}{\hat{F}}
\renewcommand{\c}[1]{\ensuremath{\EuScript{#1}}}
\renewcommand{\b}[1]{\ensuremath{\mathbb{#1}}}

\def\argmin{\mathop{\text{argmin}}}




\title{Lipschitz Unimodal and Isotonic Regression on Paths and Trees\footnote{The work was primarily done when the second and third authors were at Duke University. Research supported by NSF under grants
    CNS-05-40347, CFF-06-35000, and DEB-04-25465, by ARO grants
    W911NF-04-1-0278 and W911NF-07-1-0376, by an NIH grant
    1P50-GM-08183-01, by a DOE grant OEG-P200A070505, by a grant
    from the U.S.--Israel Binational Science Foundation, and a subaward to the University of Utah under NSF Award 0937060 to Computing Research Association.}}
\author{\begin{tabular}{c}
	Pankaj K. Agarwal\\
	\small Duke University\\
	\small Durham, NC 27708\\
	\small pankaj@cs.duke.edu
	\end{tabular}
	\and 
	\begin{tabular}{c}
	Jeff M. Phillips\\
	\small University of Utah\\
	\small Salt Lake City, UT 84112\\
	\small jeffp@cs.utah.edu
	\end{tabular}
	\and 
	\begin{tabular}{c}
	Bardia Sadri\\
	\small University of Toronto\\
	\small Toronto, ON M5S 3G4\\
	\small sadri@cs.toronto.edu
	\end{tabular}
}

\begin{document}
\begin{titlepage}
\maketitle
\thispagestyle{empty}
\begin{abstract}
We describe algorithms for finding the regression of , a sequence of values, to the closest sequence  by mean squared error, so that  is always increasing (isotonicity) and so the values of two consecutive points do not increase by too much (Lipschitz).  The isotonicity constraint can be replaced with a unimodular constraint, where there is exactly one local maximum in .  These algorithm are generalized from sequences of values to trees of values.  For each scenario we describe near-linear time algorithms.  
\end{abstract}
\end{titlepage}




\section{Introduction}\label{sec:intro}

Let  be a triangulation of a polygonal region  in which each vertex is 
associated with a real valued height (or elevation). Linear interpolation of vertex heights in the interior of 
each triangle of  defines a piecewise-linear
function , called a \emph{height function}. 
A height function (or its graph) is widely used to model a 
two-dimensional surface in numerous applications (e.g. modeling the 
terrain of a geographical area).  With recent advances in sensing
and mapping technologies, such models are being generated
at an unprecedentedly large scale. These models are then used to 
analyze the surface and to compute various geometric and 
topological properties of the surface. For example, researchers in GIS
are interested in extracting river networks or computing visibility 
or shortest-path maps on terrains modeled as height functions. These structures depend heavily on 
the topology of the level sets of the surface and in particular on the topological relationship between its critical 
points (maxima, minima, and saddle points). 
Because of various factors such as measurement or sampling errors or the nature of the sampled surface, 
there is often plenty of noise in these surface models which introduces spurious 
critical points. This in turn leads to misleading or 
undesirable artifacts in the computed structures, e.g.,
artificial breaks in river networks. These difficulties have motivated
extensive work on topological simplification and noise removal through modification of the height function  into another one 
 that has the desired set 
of critical points and that is as close to  as 
possible~\cite{Bremer:03,Ni:04,Soille:04,Soille:04a,vKS09}. A popular
approach is to decompose the surface into pieces and modify each
piece so that it has a unique minimum or maximum~\cite{Soille:03a}.  In some applications, it is also desirable to impose the additional constraint that the function  is Lipschitz; see below for further discussion. 

\paragraph{Problem statement.} 

Let  be a planar graph with vertex set  and arc (edge) set . We may treat  as undirected in which case we take the pairs  and  as both representing the same undirected edge connecting  and .
Let  be a real
parameter. A function  is called
\begin{itemize} 
\item [(L)]{\em -Lipschitz} if   implies .
\end{itemize}
Note that if  is undirected, then Lipschitz constraint on an edge  implies .  
For an undirected planar graph , a function 
 is called
\begin{itemize}
\item [(U)]{\em unimodal} if  has a unique {\em local maximum}, i.e. only one vertex  such that  for all .
\end{itemize}
For a directed planar graph , a function 
 is called
\begin{itemize}
\item [(I)]{\em isotonic} if  implies 
.\footnote{A function  satisfying the isotonicity constraint (I) must assign 
the same value to all the vertices of a directed cycle of  
(and indeed to all vertices in the same strongly connected component). 
Therefore, without loss of generality, we assume 
 to be a directed {\em acyclic} graph (DAG).}
\end{itemize}
For an arbitrary function  and a 
parameter , the {\em -Lipschitz unimodal regression 
(-LUR\/)} of  is a function  
that is -Lipschitz and unimodal on  and 
minimizes . 
Similarly, if  is a directed planar graph, then 
 is the {\em -Lipschitz isotonic regression 
(-LIR\/)} of  if  satisfies (L) and (I) and 
minimizes .
The more commonly studied {\em isotonic regression (IR)\/} and \emph{unimodal
regression}~\cite{AHKW06,ABERS55,BBBB72,Preparata:85} are the special cases of 
LIR and LUR, respectively, in which  , and therefore only the 
condition (I) or (U) is enforced.

Given a planar graph , a parameter , and 
, the LIR (resp.\ LUR) problem
is to compute the -LIR (resp.\ -LUR) of . In this 
paper we propose near-linear-time algorithms for the LIR and LUR problems
for two special cases: when  is a path or a tree. We study the special case where  is a path prior to the more general case where it is tree because of the difference in running time and because doing so simplifies the exposition to the more general case. 

\paragraph{Related work.}
As mentioned above, there is extensive work on simplifying the topology
of a height function while preserving the geometry as much as possible.
Two widely used approaches in GIS are the so-called \emph{flooding} and
\emph{carving} techniques~\cite{Agarwal:06,Danner:07,Soille:04a,Soille:04}. 
The former technique raises the height of 
the vertices in ``shallow pits'' to simulate the effect of flooding,
while the latter lowers the value of the height function along a 
path connecting two pits so that the values along the path 
vary monotonically. As a result, one pit drains to the other and 
thus one of the minima ceases to exist. 
Various methods based on Laplacian smoothing have been proposed in the 
geometric modeling community to remove unnecessary critical 
points; see \cite{Bajaj:98,Bremer:03,Guskov:01,Ni:04} and references therein.
For example, Ni~\etal~\cite{Ni:04} proposed
the so-called \emph{Morse-fairing} technique, which solves a 
relaxed form of Laplace's equation, to remove undesired 
critical points.

A prominent line of research on topological simplification was 
initiated by Edelsbrunner~\etal~\cite{Edelsbrunner:00,Edelsbrunner:03} who introduced the 
notion of \emph{persistence}; see also~\cite{Edelsbrunner:07,Zomorodian:05,Zomorodian:09}. 
Roughly speaking,  each homology class of the 
contours in sublevel sets of a height function is characterized by 
two critical points at one of whom the class is born and at the other it is destroyed. The persistence of this class is then the height difference between these two critical points and can be thought of as the life span of 
that class. The persistence of a class effectively suggests the ``significance'' of its defining critical points. 
Efficient algorithms have been developed for computing the persistence
associated to the critical points of a height function and for simplifying topology based on 
persistence~\cite{Edelsbrunner:03,Bremer:03}.
Edelsbrunner~\etal~\cite{Edelsbrunner:06} and Attali~\etal~\cite{Attali:08} proposed algorithms 
for optimally eliminating all critical points of persistence below a 
threshold where the error is measured as 
. No efficient 
algorithm is known to minimize .

The isotonic-regression (IR)  problem has been studied in 
statistics \cite{ABERS55,BBBB72,Preparata:85} since the 1950s.  It has many applications ranging from 
statistics \cite{Sto00} to bioinformatics \cite{BBBB72}, and from operations 
research \cite{MM85}  to differential optimization \cite{GW84}.  
Ayer \emph{et. al.} \cite{ABERS55} famously solves the \ir problem on 
paths in  time using the 
\emph{pool adjacent violator algorithm} (PAVA).  The algorithm 
works by initially treating each vertex as a level set and merging consecutive level sets that are out of order.  This algorithm is correct regardless of the order of the merges~\cite{RWD88}.  
Brunk \cite{Bru55} and Thompson \cite{Tho62} initiated the study of the \ir problem on general DAGs and trees, respectively.
Motivated by the problem of finding an optimal insurance rate structure over given risk classes for which a desired pattern of tariffs can be specified, Jewel \cite{Jewel:75} introduced the problem of Lipschitz isotonic regression on DAGs and showed connections between this problem and network flow with quadratic cost functions\footnote{It may at first seem that Jewel's formulation of the LIR  of an input function  on the vertices of a DAG is more general than ours in that he requires that for each ,  where  are defined separately for each edge (in our formulation  and  for every edge ). Moreover, instead of minimizing the  distance between  and  he requires  to be minimized where  is a constant assigned to vertex . However,  all our algorithms in this paper can be modified in a straight-forward manner to handle this formulation without any change in running-times.}.
Stout \cite{Sto08} solves the \ur problem on paths in  time.
Pardalos and Xue \cite{PX99} give an  algorithm for the \ir problem on trees.  
For the special case when the tree is a star they give an  algorithm.  
Spouge \etal \cite{SWW03} give an  time algorithm for the \ir problem on DAGs.  
The problems can be solved under the  and  norms on 
paths \cite{Sto08} and DAGs \cite{AHKW06} as well, with an 
additional  factor for . Recently Stout~\cite{Stout:08} has
presented an efficient algorithm for istonotic regression in a planar
DAG under the  norm. To our knowledge no polynomial-time algorithm is known for the 
UR problem on planar graphs, and there is no prior attempt on achieving efficient algorithms for  
Lipschitz isotonic/unimodal regressions in the literature. 

\paragraph{Our results.} 
Although the LUR problem for planar graphs remains elusive, we present
efficient exact algorithms for LIR and LUR problems on two special cases of 
planar graphs: paths and trees.  
In particular, we present an  algorithm for computing the LIR on a path of length  (Section~\ref{sec:pathlir}), and an  algorithm on a tree with  nodes (Section~\ref{sec:treelir}). 
We present an  algorithm for computing the
LUR problem on a path of length  (Section~\ref{sec:pathlur}). 
Our algorithm can be extended to solve the LUR problem on an unrooted tree in  time (Section~\ref{sec:treelur}). 
The LUR algorithm for a tree is particularly interesting because of 
its application in the aforementioned carving 
technique~\cite{Danner:07,Soille:04,Soille:03a}. 
The carving technique modifies the height function along a number 
of trees embedded on the terrain where the heights of the vertices 
of each tree are to be changed to vary monotonically towards 
a chosen ``root'' for that tree. In other words, to perform the 
carving, we need to solve the IR problem on each tree. The downside of 
doing so is that the optimal IR solution happens to be a step function 
along each path toward the root of the tree with potentially large 
jumps. Enforcing the Lipschitz condition prevents sharp jumps 
in function value and thus provides a more natural solution 
to the problem.

Section~\ref{sec:datastructure} presents a data structure, called {\em affine composition tree} (ACT), for maintaining a -monotone polygonal chain, which can be regarded as the graph of a monotone piecewise-linear function .  
Besides being crucial for our algorithms, ACT is interesting in its own right.
A special kind of binary search tree, an ACT supports a number of operations to query and update the chain, each taking  time.
Besides the classical insertion, deletion, and query (computing  or  for a given ),  one can apply an  operation that modifies a contiguous subchain provided that the chain  remains -monotone after the transformation, i.e., it remains the graph of a monotone function.  




\section{Energy Functions}
On a discrete set , a real valued function  can be viewed as a point in the -dimensional Euclidean space in which coordinates are indexed by the elements of  and the component  of  associated to an element  is . We use the notation  to represent the set of all real-valued functions defined on . 

Let  be a directed acyclic graph on which we wish to compute -Lipschitz isotonic regression of an {\em input function} . For any set of vertices , let  denote the subgraph of  induced by , i.e. the graph , where .  The set of -Lipschitz isotonic functions on the subgraph  of  constitutes
a convex subset of , denoted by .  It is the common intersection of all half-spaces determined by the isotonicity and Lipschitz constraints associated 
with the edges in , i.e.,  and  
for all .  

For  we define  as 

Thus, the -Lipschitz isotonic regression of the input function  is .  For a subset  and  define the function  as 



\begin{lemma}\label{lem:convex-general}
For any  and , the function  is continuous and strictly convex. 
\end{lemma}
\begin{proof}Given  and , consider the functions 

The strong convexity of  implies that 

Furthermore, the convexity of  implies that  is also in .  Since by definition , we deduce that 

Thus, the minimum is precisely . 
This proves that  is strictly convex on  and therefore continuous (and in fact differentiable except in countably many points).
\end{proof}



\section{Affine Composition Tree}\label{sec:datastructure}

In this section we introduce a data structure, called \emph{affine 
composition tree} (ACT), for representing an 
-monotone polygonal chain in , which is being deformed 
dynamically. Such a chain can be regarded as the graph of a 
piecewise-linear monotone function , and thus is bijective. 
A {\em breakpoint} of  is the -coordinate of a {\em vertex} of 
the graph of  (a vertex of  for short), i.e., a 
 at which the left and right derivatives of  disagree. The number of breakpoints of  will be denoted by . 
 A continuous piecewise-linear function  with breakpoints 
 can be characterized by its vertices 
 together with the {\em slopes}
 and  of its {\em left} and {\em right unbounded pieces},
respectively, extending to  and . 
An {\em affine transformation} of  is a map 
 where  is a nonsingular  matrix, 
(a {\em linear transformation\/}) and  is a 
{\em translation vector} --- in our notation we treat 
 as a column vector.


\begin{figure}
\begin{center}
\includegraphics[scale=.6]{atrans}\qquad\qquad
\includegraphics[scale=.6]{itrans}
\end{center}
\myCaption{\label{fig:trans} Left: the graph of a monotone piecewise 
linear function  (in solid black). Vertices are marked by hollow 
circles. The dashed curve is the result of applying the linear 
transform  where . 
The gray curve, a translation of the dashed curve under vector 
, is the 
result of applying  to  where . 
Right: , only the vertices of curve  
whose -coordinates are in the marked interval [ 
are transformed. The resulting curve is the thick gray curve. }
\end{figure}

An ACT supports the following operations on a monotone piecewise-linear
function  with vertices :
\begin{enumerate}
\item  and : Given any , return 
 or .

\item : Given a point  insert 
 as a new vertex of . If ,
this operation removes the segment  from the graph of 
 and replaces it with two segments  and , 
thus making  a new breakpoint of  with . 
If  or , then the affected unbounded piece of  is 
replaced with one parallel to it but ending at  and 
a segment connecting  and the appropriate vertex of  
( and  remain intact).  We assume that 
.

\item : Given a breakpoint  of , removes the vertex 
 from ; a delete operation modifies  in a 
manner similar to insert.

\item : Given an affine transformation 
, modify the 
function  to one whose graph is the result of application of 
 to the graph of . See Figure \ref{fig:trans}(Left).

\item : Given an affine transformation
 and  
and , this operation applies  to all vertices 
 of  whose -coordinate is in the range . Note that  is equivalent to  for . See Figure \ref{fig:trans}(Right).



\end{enumerate} 

It must be noted that  and  are applied 
with appropriate choice of transformation parameters so that the 
resulting chain remains -monotone.  

An ACT  is a red-black tree that stores the vertices
of  in the sorted order, i.e., the th node of  is
associated with the th vertex of . However, instead of storing
the actual coordinates of a vertex, each node  of  stores
an affine transformation 
. 
If  is the path in  from the root
 to , then let 
Notice that  is also an affine transformation. 
The actual coordinates of the vertex associated with  are  where .

Given a value  and , let  (resp.\ ) denote the rightmost (resp.\ leftmost) vertex  of  such that the -coordinate of   is at most (resp.\ least) .
Using ACT ,  and  can be computed in  time by following a path in , composing the affine transformations along the path, evaluating the result at , and comparing its -coordinate with .
We can answer  queries by  determining the vertices  and  of  immediately preceding and succeeding  and interpolating  linearly between  and ; if  (resp. ), then  is calculated using  and  (resp.  and ). 
Since  and  can be computed in  time and the interpolation takes constant time,  is answered in time . Similarly,  is answered using  and .


A key observation of ACT is that a standard rotation on any edge of  can be performed in  time by modifying the stored affine transformations in a constant number of nodes (see Figure \ref{fig:rotation}) based on the fact that an affine transformation  has an inverse affine transformation ; provided that the matrix  is invertible.
A point  is inserted into  by first computing the affine transformation  for the node  that will be the parent of the leaf  storing .  To determine  we solve, in constant time, the system of (two) linear equations .  The result is the translation vector .  The linear transformation  can be chosen to be an arbitrary invertible linear transformation, but for simplicity, we set  to the identity matrix. Deletion of a node is handled similarly. 

\begin{figure}[b]
\begin{center}
\includegraphics[scale=.4]{linear-composition-rotation-left}\qquad\qquad
\includegraphics[scale=.4]{linear-composition-rotation-right}
\end{center}
\myCaption{\label{fig:rotation} Rotation in affine composition trees. The affine function stored at a node is shown as a greek letter to its right (left). When rotating the pair , by changing these functions as shown (right) the set of values computed at the leaves remains unchanged. }
\end{figure}




To perform an  query, we first find the nodes  and  storing the vertices  and , respectively. 
We then successively rotate  with its parent until it becomes the root of the tree.  Next, we do the same with  but stop when it becomes the right child of .  At this stage, the subtree rooted at the left child of  contains exactly all the vertices  for which .  Thus we compose  with the affine transformation at that node and issue the performed rotations in the reverse order to put the tree back in its original (balanced) position. Since  and  were both within  steps from the root of the tree, and since performing each rotation on the tree can only increase the depth of a node by one,  is taken to the root in  steps and this increases the depth of  by at most . Thus the whole operation takes  time.

We can augment  with additional information so that for any  the function , where  is the leftmost breakpoint 
and  is value associated with , 
can be computed in  time; we refer to this operation as .  We provide the details in the Appendix.  
We summarize the above discussion:

\begin{theorem}\label{thm:ACT}
A continuous piecewise-linear monotonically increasing function 
 with  breakpoints can be maintained using a data structure 
 such that
\begin{enumerate}
\item  and  queries can be answered in  time,
\item an  or a  operation can be performed in  time, 
\item  and  operations can be performed in  and  time, respectively.
\item  operation can be performed in  time.  
\end{enumerate}
\end{theorem}

One can use the above operations to compute the sum of two increasing continuous piecewise-linear functions  and  as follows: we first compute  for every breakpoint  of  and insert the pair  into .  At this point  the tree still represents  but includes all the breakpoints of  as {\em degenerate} breakpoints (at which the left and right derivates of  are the same). Finally, for every consecutive pair of breakpoints  and  of  we  apply an  operation on  where  is the affine transformation  where
 
and 
 
in which  is the linear function that interpolates between  at  and  at  (similar operation using  and  of  for the unbounded pieces of  must can be applied in constant time). It is easy to verify that after performing this series of 's,  turns into . The total running time of this operation is . Note that this runtime can be reduced to ,  for , by using an algorithm of Brown and Tarjan~\cite{BT79} to insert all breakpoints and then applying all  operations in a bottom up manner.  Furthermore, this process can be reversed (i.e. creating  without the breakpoints of , given  and ) in the same runtime.  We therefore have shown:

\begin{lemma}
Given  and  for a piecewise-linear isotonic functions  and  where ,  or  can computed in  time.
\label{lem:F+G}
\end{lemma}


\paragraph{Tree sets.}
In our application we will be repeatedly computing the sum of two functions  and .  It will be too expensive to compute  explicitly using Lemma \ref{lem:F+G}, therefore we represent it implicitly.  More precisely, we use a \emph{tree set}  consisting of affine composition trees of  monotone piecewise-linear functions  to represent the function . We perform several operations on  or  similar to those of a single affine composition tree.  

 on  takes  time, by evaluating .  
 on  takes  time using a technique of Frederickson and Johnson~\cite{Frederickson:82}.

Given the ACT , we can convert  to  in two ways:
an  operation sets  in  time.
A  operations sets  in  time.
We can also perform the operation  and  operations that reverse the respective above operations in the same runtimes.  

We can perform an  where  describes a linear transform  and a translation vector .  To update  by  we update  by  and for  update  by just .  This takes  time.  It follows that we can perform  in  time, where .  Here we assume that the transformation  is such that each  remains monotone after the transformation.  



\section{LIR on Paths}\label{sec:pathlir}
In this section we describe an algorithm for solving the \lir problem on a path, represented as a directed graph  where  and .   A function  is isotonic (on ) if , and -Lipschitz for some real constant  if  for each . 
For the rest of this section let  be an input function on . 
For each , let , let  be the subpath , and let  and , respectively, be shorthands for  and .
By definition, if we let , then for each : 
 
By Lemma \ref{lem:convex-general},  is convex and continuous and thus has a unique minimizer .

\begin{lemma}\label{lem:convex}
For , the function  is given by the recurrence relation:

\end{lemma}
\begin{proof}
The proof is by induction on .  is clearly a single-piece quadratic function. For , since  is strictly convex, it is strictly decreasing on  and strictly increasing on . Thus depending on whether , , or , the value  that minimizes  is , , and , respectively. 
\end{proof}


\begin{figure}
\begin{center}
\includegraphics[scale=.9]{LIR-breakpoints}
\end{center}
\myCaption{\label{fig:LIR-breakpoints} The breakpoints of the function . For each ,  and  are the ``new'' breakpoints of . All other breakpoints of  come from  where those smaller than  remain unchanged and those larger are increased by .}
\end{figure}

Thus by Lemmas \ref{lem:convex-general} and \ref{lem:convex},  is strictly convex and piecewise quadratic. 
We call a value  that determines the boundary of two neighboring quadratic pieces of  a {\em breakpoint} of . Since  is a simple (one-piece) quadratic function, it has no breakpoints. For , the  breakpoints of the function  consist of  and , as determined by recurrence (\ref{eqei}), together with breakpoints that arise from recursive applications of . 
Examining equation (\ref{eqei}) reveals that all breakpoints of  that are smaller than  remain breakpoints in  and all those larger than  are increased by  and these form all of the breakpoints of  (see Figure \ref{fig:LIR-breakpoints}).  Thus  has precisely  breakpoints. 
To compute the point  at which  is minimized, it is enough to scan over these  quadratic pieces and find the unique piece whose minimum lies between its two ending breakpoints. 


\begin{lemma}\label{lem:backsolve}
Given the sequence , one can compute the -LIR  of input function  in  time. 
\end{lemma}
\begin{proof}For each , let , i.e.  is the -LIR of  where  for all  and in particular  where  is the -LIR of . From the definitions we get 
The uniqueness of the  and  implies that . In particular, . Suppose that the numbers  are determined, for some , and we wish to determine . Picking  entails that the energy of the solution will be at least 

Thus  has to be chosen to minimize  on the interval . If , then we simply have  which is the global minimum for . Otherwise, one must pick  if  and  if . Thus each  can be found in  time for each  and the entire process takes  time.
\end{proof}


One can compute the values of  in  iterations. The th iteration computes the value  at which  is minimized and then uses it to compute the function  as given by (\ref{eqei}) in  time.  After having computed 's, the -LIR of  can be computed in linear time.  
However, this gives an  algorithm for computing the -LIR of . We now show how this running time can be reduced to .





For the sake of simplicity, we assume for the rest of this paper that 
for each , , i.e., the point minimizing 
 is none of its breakpoints, although it is not hard to 
relax this assumption algorithmically. 
Under this assumption,  belongs to the interior of some interval on which  is quadratic. The derivative of this quadratic function is therefore zero at . In other words, if we know to which quadratic piece of  the point  belongs, we can determine  by setting the derivative of that piece to zero. 

\begin{lemma}\label{lem:monotone}
The derivative of  is a continuous monotonically increasing piecewise-linear function. 
\end{lemma}
\begin{proof}Since by Lemma \ref{lem:convex},  is strictly convex and piecewise quadratic, its derivative  is monotonically increasing. To prove the continuity of the derivative, it is enough to verify this at the ``new'' breakpoints of , i.e. at  and . The continuity of the derivative can be argued inductively by observing that both mappings of the breakpoints from  to  (identity or shifting by ) preserve the continuity of the function and its derivative. 

Notice that at , both the left and right derivatives of a quadratic piece of  becomes zero. From the definition of  in (\ref{eqei}), the left derivative of the function  at  agrees with the left derivative of  at the same point and is therefore zero. On the other hand, the right derivative of the function  is zero at  and agrees with its  right derivative at the same point as the function is constant on the interval . This means that the derivative of  is continuous at  and therefore the same holds for . A similar argument establishes the continuity of  at .
\end{proof} 

Let  denote the derivative of .
Using (\ref{eqei}), we can write the following recurrence for :

where 

if we set . As mentioned above,  is simply the solution of , which, by Lemma \ref{lem:monotone} always exists and is unique. 
Intuitively,  is obtained from  by splitting it at , shifting the right part by , and connecting the two pieces by a horizontal edge from  to  (lying on the -axis).  




In order to find  efficiently, we use an ACT  to represent .  It takes  time to compute  on .  
Once  is computed, we store it in a separate array for back-solving through Lemma~\ref{lem:backsolve}.  
We turn  into  by performing a sequence of
, , and  operations on  where 

the two insert operations add the breakpoints at  and  and the interval operation shifts the portion of  to the right of  by .
We then turn  into  by performing  operation on  where  where 
 
and 
.




Given ACT ,  and  can be computed in  time.  Hence, we can compute  in  time.  By Lemma \ref{lem:backsolve}, we can conclude the following.

\begin{theorem}
Given a path , a function , and a constant , the -Lipschitz isotonic regression of  on  can be found in  time.
\end{theorem}



\paragraph{ operation.}
We define a procedure  that encapsulates the process of turning  into  and returning .  
Specifically, it performs  of  to produce , then it outputs  on , and finally a sequence of , , 
and  operations on  to 
get .  
Performed on  where  has  breakpoints, an  takes  time.  
An  reverts the affects of an .  This requires that  is stored for the reverted version.  
Similarly, we can perform  and  on a tree set , in  time, the bottleneck coming from .


\begin{lemma}\label{lem:tree-update}
Given ,  and  take  time.
\end{lemma}
\begin{proof}Since , for a breakpoint  of , we get:

where  becomes breakpoint .

We can now compute  on .

Rewriting (\ref{eq:hatF}) for a breakpoint  of  that is neither of  or , using the fact that 

where  is the breakpoint of  that has become  in , we get:

We next combine and rewrite equations (\ref{eqxi}) and (\ref{eqfxi}) as follows:


The new breakpoints, i.e. the pairs  and , should be inserted into . 

To perform , we simply perform the inverse of all of these operations.  
\end{proof}



\section{LUR on Paths} \label{sec:pathlur}
Let  be an {\em undirected} path where  and  and given . For  let  be a directed graph in which all edges are directed 
towards ; that is, for ,  and for  .  
For each , let  and let .  
If , then  is the -LUR of  on .

We can find  in  time by solving the LIR problem, then traversing the path while maintaining the optimal solution using  and .  Specifically,
for each , let  and . 
For , let ,  be the functions on directed paths  and , respectively, as defined in (\ref{eqfi}).  Set .  Then the function  can be written as .  We store  as the tree set .  
By performing  we can compute  in  time (the rate limiting step), and then we can compute  in  time using .  
Assuming we have  and , we can construct  and  in  time be performing 
 and 
.  
Since  is constructed 
in  time, finding  by searching all  tree sets 
takes  time.


\begin{theorem}
Given an undirected path  and a  together with a real , the
-LUR of  on  can be found in  time.
\end{theorem}



\section{LIR on Rooted Trees} \label{sec:treelir} 

Let  be a rooted tree with root  and let for each vertex ,  denote the subtree of  rooted at . Similar to the case of path LIR, for each vertex  we use the shorthands  and . Since the subtrees rooted at distinct children of a node  are disjoint, 
one can  write an equation corresponding to (\ref{eq3}) in the case of paths, for any vertex  of :

where .
An argument similar to that of Lemma \ref{lem:monotone} together with Lemma \ref{lem:convex-general} implies that 
for every , the function  is convex and piecewise quadratic, and its derivative  is continuous, monotonically increasing, and piecewise linear.
We can prove that  satisfies the following recurrence where  is defined analogously to  in (\ref{eq:hatF}): 



Thus to solve the LIR problem on a tree, we post-order traverse the tree (from the leaves toward the root) and when processing a node , we compute and sum up the linear functions  for all children  of  and use the result in (\ref{eq9}) to compute the function . We then solve  to find . As in the case of path LIR,  can be represented by an ACT . For simplicity, we assume each non-leaf vertex  has two children  and , where .  We call  \emph{heavy} and  \emph{light}.  Given  and , we can compute  and  with the operation  in  time.  The merging of two functions dominates the time for the update.


\begin{theorem}\label{thm:LIRtree}
Given a rooted tree  and a function  together with a Lipschitz constant , one can find in  time, the -Lipschitz isotonic regression of  on .
\end{theorem}
\begin{proof}
Let  be rooted at , it is sufficient to bound the cost of all calls to  for each vertex: .  

For each leaf vertex  consider the path to the root .  For each light vertex  along the path, let its \emph{value} be .  Also, let each heavy vertex  along the path have \emph{value} .  
For any root vertex , the sum of values , since  for each light vertex and .  

Now we can argue that the contribution of each leaf vertex  to  is at most .  Observe that for , then  is the same for any leaf vertex  such that .  Thus we charge  to  for each  in the subtree rooted at .  Since  (for ), then the charges to each  is greater than its contribution to .  Then for each of  leaf vertices , we charge .  Hence, .  
\end{proof}



\section{LUR on Trees}
\label{sec:treelur}

Let  be an unrooted tree with  vertices.  
Given any choice of  as a root, we say  is the tree  rooted at .  
Given a function , a Lipschitz constraint , and a root , the -LIR regression of  on , denoted , can be found in  time using Theorem \ref{thm:LIRtree}.  We let  as defined in (\ref{eqte-tree}) and let .  The -Lipschitz unimodal regression (-LUR) of  on  is the function .  
Naively, we could compute the -LIR in  time by invoking Theorem \ref{thm:LIRtree} with each vertex as the root. In this section we show this can be improved to .


We first choose an arbitrary vertex  (assume it has at most two edges in ) as an honorary root of .  For simplicity only, we assume that every vertex of  has degree three or less.  
Let the weight  of a subtree of  at  be the number of vertices below and including  in .
With respect to this honorary root, for each vertex , we define  to be the parent of  and  (resp. ) to be the heavy (resp. light) child of  such that ;  has no parent and leaf vertices (with respect to ) have no children.  
If  has only one child, label it . 
If  we say  is a \emph{light} vertex, otherwise it is a \emph{heavy} vertex.
The following two properties are consequences of  for all .
\begin{itemize}
\vspace{-.1in}
\item[(P1)] .  
\vspace{-.1in}
\item[(P2)] For , the path  in  contains at most  light vertices.
\end{itemize}

As a preprocessing step, we construct the -LIR of  for  as described in Theorem \ref{thm:LIRtree}, and, for each light vertex , we store a copy of  before merging with .  The total size of all stored ACTs is  by property (P1).  
We now perform an inorder traversal of , letting each  in turn be the structural root of  so we can determine .  
As we traverse, when  is structural root, we maintain a tree set  at  with at most  trees, and ACTs  and  at  and , respectively.  
All functions  for  are defined the same as with rooted trees, when rooted at the structural root , for the subtree rooted at .   
Given these three data structures we can compute  in  time by first temporarily inserting the ACTs , , and  into , thus describing the function  for a rooted tree with  as the root, and then performing an  and  on .  Recall .  
Thus to complete this algorithm, we need to show how to maintain these three data structures as we traverse .  

During the inorder traversal of  we need to consider 3 cases: letting , , or  be the next structural root, as shown in Figure \ref{fig:traverse}.  
\begin{itemize}
\item[(a)]  is the next structural root: 
To create  we add  to the tree set  through a  operation and perform .  
The ACT  is stored at .  
To create the ACT  we perform , and then set , using .  

\item[(b)]  is the next structural root: 
To create  we perform  and then .  
The ACTs for  and  are produced the same as above.  

\item[(c)]  is the next structural root: 
To create  we set  and then perform .
To create  we first set .  
Then if , we perform , and  is stored at .
Otherwise , and we perform  to get , and  is the byproduct.  
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[scale=1]{traverse}
\end{center}
\myCaption{\label{fig:traverse} Illustration of  ordered with respect to honorary root .  This is redrawn, ordered with respect to  structural root as .  The tree is redrawn with structural root as  after case (a),  after case (b), and  after case (c).}
\end{figure}


We observe that on a traversal through any vertex , the number of ACTs in the tree set  only increases when  is the new structural root.  Furthermore, when the traversal reverses this path so that  is the new root where , the same ACT is removed from the tree set.  Thus, when any vertex  is the structural root, we can bound the number of ACTs in the tree set  by the number of light vertices on the path from  to .  Thus property (P2) implies that  has at most  ACTs.  

We can now bound the runtime of the full traversal algorithm described above.  Each vertex  is visited as the structural root  time and each visit results in  , , , , , and  operations.  
Performing a single  or  on a tree set takes at most  time, and each  and  operation takes  time, so the total time is .  
We now need to bound the global costs of all  and  operations.  In proving Theorem \ref{thm:LIRtree} we showed that the cost of performing a  of all light vertices into heavy vertices as we build  takes .  Since at each vertex  we only  and  light vertices, the total time of all  and  operations is again .  

Thus we can construct  for each possible structural root, and using  as the optimal root we can invoke Theorem \ref{thm:LIRtree} to construct the -LIR of  on .  

\begin{theorem}
Given an unrooted tree  and a function  together with a Lipschitz constraint , we can find in  time 
the -Lipschitz unimodal regression of  on .
\label{thm:LURtree}
\end{theorem}



\section{Conclusion}\label{sec:conclusion}
In this paper we defined the Lipschitz isotonic/unimodal regression 
problems on DAGs and provided efficient algorithms for solving it on the special cases where the graph is a path or a tree. 
Our ACT data structure has proven to be a powerful tool in our approach.




Our algorithms can be generalized in a number of ways, as
listed below, without affecting their asymptotic running times. 
\begin{itemize} 
\item One can specify different Lipschitz value  for each Lipschitz constraint, thus writing the Lipschitz constraint involving  and  as . 
\item Isotonicity constraints can be regarded as (backward) Lipschitz constraints for which the Lipschitz constant is zero. One can allow this constant to be chosen arbitrarily and indeed permit different constants for different isotonicity constraints. In other words, the constraint  can be replaced with .

\item The  norm  can be replaced with a ``weighted'' version  for any  by defining for a function :  

\end{itemize}


The most important open problem is solving LIR problem on general DAGs. The known algorithm for solving IR on DAGs runs in  and does not lend itself to the Lipschitz generalization. The difficulty here is to compute the function  (or in fact ) when the th vertex is processed and comes from the fact that the  function, , where  are vertices with incoming edges to , are not independent. Independently, it is an important question whether IR can be solved more efficiently. In particular, the special case of the problem where the given DAG is planar has important applications in terrain simplification. 

\subsection*{Acknowledgements}
We anonymous reviewers for many helpful remarks that helped improve the presentation of this paper and we thank G\"unter Rote for suggesting the improvement in the analysis of , similar to his paper~\cite{Rote}.  

\bibliographystyle{abbrv}
\bibliography{lir}

\newpage

\appendix

\section{ Operation}

\begin{lemma} 
We can modify an ACT datastructure for   with  and , so for any  we can calculate  in  time, 
and so the cost of maintaining  is asymptotically equivalent to without this feature.
\label{lem:integrate}
\end{lemma}
\begin{proof}
For any , we calculate  by adding  to , where  is the leftmost breakpoint in .  Maintaining  is easy and is explained first.  Calculating  requires storing partial integrals and other information at each node of  and is more involved.  

We explain how to maintain the value  in the context of our -LIR problems in order to provide more intuition, but a similar technique can be used for more general ACTs.  
We maintain values  such that  for the  values of .  This is done by adding , , and  to , , and  every time a new  is processed.  Or if two trees are merged, this can be updated in  time.  The significance of this equation is that  because the isotonic condition is enforced for all edges in , forcing all values  in the -LIR to be equal.  


When we construct  we keep the following extra information at each node: 
each node  whose subtree spans breakpoints  to  maintains the integral  (this is done for technical reasons; subtracting  ensures  for  since  is isotonic), the width , and the height .  
Recall that  is the composition of affine transformations stored at the nodes along the path from the root to .  
We build these values (, , and  for each ) from the bottom of  up, so we apply all appropriate affine transformations  for decedents  of , but not yet those of ancestors of .  In particular, if node  has left child  and right child , then we can calculate 
  
When a new breakpoint  is inserted or removed there are  nodes that are either rotated or are on the path from the node storing  to the root.  For each affected node  we recalculate ,  and  in a bottom up manner using 's children once they have been updated themselves.  Likewise, tree rotations require that  nodes are updated.  So to demonstrate that the maintenance cost does not increase, we just need to show how to update ,  and  under an affine transformation  in  time.  

For affine transformation  applied at node  (spanning breakpoints  through ), we can decompose it into three components: translation, scaling, and rotation.  
The translation does not affect , , or .  
The scaling can be decomposed into scaling of width  and of height .  We can then update , , and .  
To handle rotation, since translation does not affect , , or , we consider the data translated by  so that  is the origin and .  Then we consider rotations about the origin, as illustrated in Figure \ref{fig:rotate}.  
After a rotation by a positive angle , the point  becomes .  This updates  and .  We can update  by subtracting the area  that is no longer under the integral  and adding the area  newly under the integral  as illustrated in Figure \ref{fig:rotate}.  Thus .  A similar operation exists for a negative angle , using the fact that  must remain monotone.
As desired all update steps take  time.

\begin{figure}[h]
\begin{center}
\includegraphics{rotate-small} 
\hspace{.8in}
\includegraphics{integrate}
\end{center}
\myCaption{\label{fig:rotate} Left: Illustration of the change in integral caused by a rotate of  degrees counterclockwise. 
Right: Calculation of  from children  and  of .}
\end{figure}


Now for a value  we can calculate  as follows.  
Let  be the node and  be the breakpoint associated with .  We consider the path  from  to the root  of , and will inductively build an integral along .  
We calculate  similar to the way we calculated , but just over the integral .  
As a base case, let  (which is easy to calculate because  is linear in this range) and let  describe the integral and width associated with the range  and .  
For a node  that spans breakpoints  through , we calculate an integral  and width  using its two children:  and .  Let  be the right child which lies in  and for which we had inductively calculated  and .  
Then let  (as illustrated in Figure \ref{fig:rotate}) and let .  
Once  and  are calculated at the root, we need to adjust for the  term in the integral.  We add  to  to get .
So finally we set .  
Since the path  is of length at most ,  can be calculated in  time. 
\end{proof}


\end{document}
