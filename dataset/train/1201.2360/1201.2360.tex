In the following, we proceed with a trace-driven system simulation,
and focus on the performance metrics outlined in
Section~\ref{sec:motivations}. That is, we are interested in studying the
time required to backup and restore user data: we perform a
comparative study of the results achieved by a system using our
redundancy management scheme and the traditional approach used for
storage applications. For the latter case, we implement a technique in
which the coding rate is set once and for all based on a system-wide
average of host availability.

Note that, for the purpose of our study, it is not necessary to
implement in detail the coding mechanisms described in
Section~\ref{sec:coding}. All we need to know for the evaluation of
transfer times is the number of fragments each peer has to upload
during the backup operation.

We use traces as input to our simulator that cover both the online
behavior of peers and their uplink and downlink capacities. Instead,
long-term failures and the events of peers abandoning the
applications, which constitute the peer deaths, follow a simple model
driven by the parameter $\tau$, as explained in
Section~\ref{sec:redundancy}. Due to the lack of traces that represent
the realistic ``data production rate'' of Internet users, in this
simulation study we confine our attention to a homogeneous setting:
each user has an individual backup object of the same size.

\subsection{Datasets}\label{sec:datasets}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{plots/im_week.pdf}
\caption{Ratio of connected nodes in a representative week of the availability
trace. More users are connected during days than during night; in weekends, the total number of connected users drops.}
\label{fig:im-week}
\end{figure}

\paragraph*{\textbf{Availability trace}} The \emph{online behavior} of users,
i.e., their patterns of connection and disconnection over time, is
difficult to capture analytically. In this work we simulate a backup
application using a real application trace that exhibits both
heterogeneity and correlated user behavior. Our traces capture user
availability, in terms of login/logoff events, from an instant
messaging (IM) server for a duration of roughly 3 months. We argue
that the behavior of regular IM users constitutes a representative
case study. Indeed, for both an IM and an online backup application,
users are generally signed in for as long as their machine is
connected to the Internet; as it can be gleaned from
Figure~\vref{fig:im-week}, in this dataset it is possible to observe
strong diurnal and weekly patterns. Moreover, users have heterogeneous
behavior -- for example, some users often stay connected during
workdays while others have a less predictable uptime~\cite{bttf}.

In this work we only consider users that are online for an average of
at least four hours per day, as done in Wuala~\cite{mager09}. Once
this filter is applied, we obtain the trace of 376 users. Since in P2P
storage systems the number of neighbors each node interacts with is
very often limited by design and scalability issues~\cite{p2p11}, we
believe this trace size is acceptable.  As shown in
Figure~\ref{fig:traces}, most users are online for less than 40\% of the
trace length, while some of them are almost always connected.

\begin{figure}
  \centering
  \includegraphics[width=.4\textwidth]{./figures/traces.eps}
\caption{CDF of the host availability from our traces. Note that users
spending less than 4 hours per day online are filtered from our data.}
\label{fig:traces}
\end{figure}

\paragraph*{\textbf{Bandwidth distribution}} Uplink capacities of peers are
obtained by sampling a real bandwidth distribution measured at more
than 300,000 unique Internet hosts for a 48 hour period from roughly
3,500 distinct ASes across 160 countries
\cite{Piatek07doincentives}. These values have a highly skewed
distribution, with a median of 77 KBps and a mean of 428 KBps. To
represent typical asymmetric residential Internet lines, we assign to
each peer a downlink speed equal to four times its uplink.


\subsection{Simulation Settings}
The trace-driven online behavior of a peer is overridden only during
the restore phase: in this work we make the assumption that in such
case, a peer remains online for the whole duration of the restore
process.

In our study, each peer has $o=10$ GB of data to backup (as soon as
the simulation begins), and dedicates 50 GB of storage space to the
application. The high ratio between these two values lets us disregard
issues due to insufficient storage capacity and focus on the subjects
of our investigation. The fragment size is set to 160 MB, implying a
minimum of $k=64$ fragments needed for restores.

We define peers' lifetimes to be exponentially distributed random
variables with an expected value $\tau = \{90 \text{ days}, 1
\text{ year}, 4 \text{ years} \}$, conservative values that are
noticeably lower than the disk failure rates measured in real-world
scenarios~\cite{fast07} (see Section~\ref{sec:redundancy}).  Besides peer
deaths, we study the impact of the $w$ parameter, which contributes to
the duration of the time-window for which our redundancy management
policy guarantees data durability, without maintenance (see
Section~\ref{sec:redundancy}).  As a reminder (see
Section~\ref{sec:motivations}), $w$ accounts for failure detection
delays. In our experiments $w$ takes values from 0 to 4 weeks.

Our adaptive redundancy policy uses the following parameters: we set
the thresholds $\sigma_1 = 0.9999$, so that the durability $d \geq
\sigma_1$ and $\sigma_2 \leq \max\left(1~\mathrm{day}, 2 \cdot
  minTTR\right)$ so that the estimated TTR $eTTR \leq \sigma_2$. In
this work, we compare against a baseline redundancy policy that aims
to guarantee data availability~\cite{Kiran04totalrecall}, labeled here
as ``availability-based''. Here we set a target data availability of
$t=0.99$, and use the system-wide average availability $a=0.36$ as
computed from our availability traces. Hence, we obtain a value
$n=228$ and a redundancy rate $n / k = 3.56$.

For each set of parameters, the simulation results are obtained by
averaging ten simulation runs.

\subsection{Results}
\label{sec:results}

We begin our discussion by showing the bounds on TTB and TTR, as
defined in Section~\ref{sec:motivations}. Figure~\vref{fig:minTTR} shows
the cumulative distribution functions (CDF) of minTTB and minTTR
obtained using the input traces discussed above. Our working
assumption is that peers stay online during restore operations: as
such, only the (ordinary) backup phase suffers from peer
unavailability, and the distribution of minTTR depends only on the
bandwidth distribution, while minTTB also depends on the availability
traces.

While backup operations generally take days to complete, for a file
size of 10 GB, restore operations are several times faster. This can
be simply explained by the asymmetric bandwidth setup we use in our
simulations, and -- as discussed above -- by unavailability of peers
when data needs to be backed up. Since node bandwidth distribution is
skewed, a few nodes with very large bandwidth experience a much lower
value for both minTTR and minTTB; the tails with a very long minTTB
value are instead due to peers that remained disconnected for very
long time spans in our traces.

\begin{figure}
\centering
\includegraphics[width=.4\textwidth]{./figures/minTTR.eps}
\caption{CDF of the ideal times to transfer data: $minTTB$ and $minTTR$.}
\label{fig:minTTR}
\end{figure}

We now proceed to a detailed comparative study of our scheme to the traditional fixed-redundancy scheme. 
First, we focus on the data redundancy
level (that is, the code rate $r$) imposed by each approach.
\begin{figure}[bp]
  \centering
  \begin{tikzpicture}[font=\footnotesize,scale=0.9,tension=0.1]
    \pgfplotsset{every axis legend/.append style={
        cells={anchor=west}, at={(0.9,0.5)}, anchor=south,
        font=\scriptsize}}
    \pgfplotsset{every axis legend/.append style={font=\scriptsize}}
\pgfplotsset{every axis plot/.append style={smooth}}
    \pgfplotsset{every axis/.append style={line width=0.5pt}}


    \begin{axis}[xlabel=$w$ (weeks),
      ylabel=Redundancy factor ($r$),
      grid=both]

\addplot[color=black,mark=o,thick] file
     {./tikz/redundancy_0_0.txt};
     \addlegendentry{$eTTR$, $\tau=3$ months};
\addplot[color=black,mark=x,thick] file
      {./tikz/redundancy_1_0.txt};
      \addlegendentry{$eTTR$, $\tau=1$ year};
\addplot[color=black,mark=*,thick] file
      {./tikz/redundancy_2_0.txt};
      \addlegendentry{$eTTR$, $\tau=4$ years};


\addplot[color=black,dashed] file
      {./tikz/redundancy_1_1.txt};
      \addlegendentry{availability-based};


   \end{axis}
 \end{tikzpicture}
 \caption{Redundancy rate as a function of $w$, for different values of $\tau$.}
 \label{fig:redundancy}
\end{figure}
 

In Figure~\ref{fig:redundancy}, we show the average redundancy factor
for our mechanism and the one computed for the availability-based
scheme (which is fixed), as a function of the parameter $w$ and for
different values of $\tau$. We omit error bars from the plot as the
variance around the mean is negligible. Clearly, for increasing values
of $w$ the redundancy rate increases, as it is possible to evince from
Equation~\ref{eq:durability}. Note that our simulations account for a
realistic bandwidth distribution and for some real on-line user
behavior, which influence the eTTR
computation. Figure~\ref{fig:redundancy} also illustrates the impact of
$\tau$: when the dominant effect of non-transient failures is the
reliability of Internet hosts, that is $\tau$ is large, our mechanism
achieves data durability (and a controlled TTR) with a small
redundancy factor. Instead, when peer deaths are dominated by peers
abandoning the system, that is $\tau$ is small, our mechanism
compensates with a larger redundancy rate. In summary, our redundancy
management scheme obtains a redundancy factor ranging roughly between
\emph{half} and \emph{a third} of the availability-based scheme,
increasing the storage capacity of the system by a corresponding
factor between two and three. Since the amount of data to upload in
case of a disk crash is proportional to the redundancy level, the
impact of maintenance of system bandwidth decreases accordingly.

In addition to improving the aggregate storage capacity of the system,
our redundancy management scheme impacts both backup and restore
operations. Figure~\ref{fig:ttb} and \ref{fig:ttr} report the CDF of the
ratio of TTB and TTR over their respective ideal counterparts,
minTTB and minTTR. These plots are obtained with different values
of $w$, for a fixed $\tau=3$ months,\footnote{We present results for
$\tau=3$ months because the effects of $w$ are more marked. We obtain
similar qualitative results for larger values of $\tau$. Also, for
clarity of presentation, we omit the CDF for $w=4$ weeks.} and
illustrate the results of our mechanism and that achieved by the
availability-based scheme. Figure~\ref{fig:ttb} indicates that, due to a
lower redundancy factor, the median of the distribution of TTB is
roughly reduced by a factor of four. Moreover, increasing values of
$w$ have essentially little impact on TTB. The price to pay for fast
backup operations is shown in Figure~\ref{fig:ttr}: restore operations
take more time to complete w.r.t. a traditional approach to redundancy
management. Here the parameter $w$ plays an important role: for small
$w$ values, little redundancy is applied to backup data. As such, the
opportunity to retrieve enough encoded fragments to restore data is
largely affected by peer availability. Instead, when $w$ is large,
restore operations are more efficient and less sensitive to peer
availability.

In summary, our results support the rationale underlying the design of
our redundancy management scheme: TTB is generally several times
larger than TTR, even in an ideal case (as shown in
Figure~\ref{fig:minTTR}). Because of this unbalance, we argue that it is
reasonable to use a redundancy management scheme that trades longer
TTR (which affects only users that suffer a crash) for shorter TTB
(which affects all users).

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}[font=\footnotesize,scale=0.9,tension=0.1]
    \pgfplotsset{every axis legend/.append style={
        cells={anchor=west}, at={(0.88,0.02)}, anchor=south,
        font=\scriptsize}}
    \pgfplotsset{every axis legend/.append style={font=\scriptsize}}
\pgfplotsset{every axis plot/.append style={smooth}}
    \pgfplotsset{every axis/.append style={line width=0.5pt}}


    \begin{semilogxaxis}[xlabel=$\frac{TTB}{minTTB}$,
      ylabel=CDF,
      grid=both]

\addplot[color=black, semithick] file
      {./tikz/cdf_TTB_0_0_0.txt};
      \addlegendentry{$eTTR$,$w$ = 0 weeks};

      \addplot[color=darkgray,thick] file
      {./tikz/cdf_TTB_0_1_0.txt};
      \addlegendentry{$eTTR$, $w$ = 1 weeks};

      \addplot[color=gray,very thick] file
      {./tikz/cdf_TTB_0_2_0.txt};
      \addlegendentry{$eTTR$, $w$ = 2 weeks};

      \addplot[color=lightgray,ultra thick] file
      {./tikz/cdf_TTB_0_3_0.txt};
      \addlegendentry{$eTTR$, $w$ = 3 weeks};


\addplot[color=black, dashed] file
      {./tikz/cdf_TTB_0_0_1.txt};
      \addlegendentry{availability-based};


   \end{semilogxaxis}
 \end{tikzpicture}
 \caption{CDF of $TTB$ for different values of $w$, $\tau=3$ months.}
 \label{fig:ttb}
\end{figure}
 \begin{figure}[ht!]
  \centering
  \begin{tikzpicture}[font=\footnotesize,scale=0.9,tension=0.1]
    \pgfplotsset{every axis legend/.append style={
        cells={anchor=west}, at={(0.88,0.02)}, anchor=south,
        font=\scriptsize}}
    \pgfplotsset{every axis legend/.append style={font=\scriptsize}}
\pgfplotsset{every axis plot/.append style={smooth}}
    \pgfplotsset{every axis/.append style={line width=0.5pt}}


    \begin{semilogxaxis}[xlabel=$\frac{TTR}{minTTR}$,
      ylabel=CDF,
      grid=both]

\addplot[color=black,semithick] file
      {./tikz/cdf_TTR_0_0_0.txt};
      \addlegendentry{$eTTR$, $w$ = 0 weeks};

     \addplot[color=darkgray,thick] file
      {./tikz/cdf_TTR_0_1_0.txt};
      \addlegendentry{$eTTR$, $w$ = 1 weeks};

      \addplot[color=gray,very thick] file
      {./tikz/cdf_TTR_0_2_0.txt};
      \addlegendentry{$eTTR$, $w$ = 2 weeks};

      \addplot[color=lightgray,ultra thick] file
      {./tikz/cdf_TTR_0_3_0.txt};
      \addlegendentry{$eTTR$, $w$ = 3 weeks};

      \addplot[color=black, dashed] file
      {./tikz/cdf_TTR_0_3_1.txt};
      \addlegendentry{availability-based};


   \end{semilogxaxis}
 \end{tikzpicture}
 \caption{Distribution of $TTR$ for different values of $w$, $\tau=3$ months.}
 \label{fig:ttr}
\end{figure}
 
Now, we delve into the details of our scheme and study its sensitivity
to errors due to the heuristic we use to estimate TTR. The main
reason for errors on eTTR are due to the fact that the heuristic
defined in Equation~\ref{eq:ettr} assumes $k$ encoded fragments to be
downloaded from the $k$ fastest peers that hold backup data. In
practice, however, the $k$ encoded fragments are downloaded from the
peers that are available when a restore operation is
executed. Depending on the bandwidth distribution of the peers in the
system, such difference can cause the estimated TTR value to be
different from what achieved in practice.

Now, if eTTR is larger than TTR, more redundant data is injected
in the system, which has no negative impact on data durability. What
is the impact on durability if peers underestimate the TTR?  Using
Equation~\ref{eq:durability}, we compute the redundancy factor $r$, as a
function of eTTR, that meets the traget durability $\sigma_1$. Then,
using TTR and $r$, we compute the data durability
$d$. Figure~\ref{fig:durability} shows the impact of the relative
estimation error -- from a precise evaluation of TTR up to an error
of almost twice the TTR -- on the relative durability error using
the procedure described above, for different values of $\tau$ and for
$w=2$ weeks.
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[font=\footnotesize,scale=0.9,tension=0.1]
    \pgfplotsset{every axis legend/.append style={
        cells={anchor=west}, at={(0.25,0.9)}, anchor=north,
        font=\scriptsize}}
    \pgfplotsset{every axis legend/.append style={font=\scriptsize}}
    \pgfplotsset{every axis plot/.append style={smooth}}
    \pgfplotsset{every axis/.append style={line width=0.5pt}}

    \begin{axis}[xlabel=Estimation error: $\frac{|TTR - eTTR|}{TTR}$,
      ylabel=Durability error: $\frac{\sigma_1-d}{\sigma_1}$,
      grid=both]

\addplot[color=black, mark=o,thick] file
      {./tikz/error_TTR_0.txt};
      \addlegendentry{$\tau = 3$ weeks};
\addplot[color=black,mark=x, thick] file
      {./tikz/error_TTR_1.txt};
      \addlegendentry{$\tau = 1$ year};
\addplot[color=black, mark=*, thick] file
      {./tikz/error_TTR_2.txt};
      \addlegendentry{$\tau = 4$ years};


   \end{axis}
 \end{tikzpicture}
 \caption{Correlation between estimation errors and data durability.}
 \label{fig:durability}
\end{figure}
 When $\tau$ is large, we have that $w+eTTR \ll \tau$: as such, even
large estimation errors have little impact on the durability
$d$. Instead, when $\tau$ is small, we have that $w+eTTR \simeq \tau$: in
this case, data durability is more sensitive to estimation errors. As
a consequence, data redundancy may not be sufficient and data loss
events may occur.

In Table~\ref{tab:dataloss}, we illustrate the effects discussed by
quantifying data loss events for $w=2$ weeks. Here we count the
percentage of peers that have not been able to restore their data
after a local disk crash, averaged over 10 simulation runs. We break
down the data loss cases between \emph{incomplete backup}
and \emph{failed restore}: the latter case encompasses all cases where
peers lose data after completing their backup. Furthermore, we also
specify the percentage of \emph{unavoidable} cases in which peers fail
before $minTTB$: in this case, not even an ideal system could have
guaranteed a safe backup.

\begin{table}
\centering

\caption{Categorization of data loss events}
\label{tab:dataloss}
\begin{tabular}{|l||r||r|r|r|}
\hline
Avg. lifetime & Total & \multicolumn{2}{|c|}{Incomplete backup} &Failed \\
\cline{3-4}
($\tau$)         & events       & Total  & Unavoidable&restore \\
\hline
\hline

3 months & 13\% & 10.4\% & 8.4\%  & \textbf{2.6\%} \\

\hline


1 year & 2.6\%  & 2.6\% & 2.3\% & \textbf{None} \\

\hline

4 years & 0.5\%  & 0.5\% & 0.25\% & \textbf{None} \\

\hline

\end{tabular}
\end{table}

A lesson we can draw from Table~\ref{tab:dataloss} is that most data
loss episodes are simply due to node failure \emph{before the backup
is completed}; this result confirms that it is sensible to optimize
time to backup by reducing redundancy and hence also network load. In
addition, it can be noted that a large majority of data loss episodes
are \emph{unavoidable} with any online storage solution: nodes with
low bandwidth risk crashing before completing uploads even if saving
data to a reliable server with 100\% uptime and unlimited bandwidth.
``Failed restore'' events -- present only in unstable systems with low
$\tau$ -- are imputable to the impact of estimation error on
durability, at discussed above. However, we remark that the impact of
this effect even in such a situation is outnumbered by the unavoidable
data loss episodes; this leads us to conclude that nodes with very low
lifetime are intrinsically unsuited to any kind of online storage
solution, and not only to P2P backup.




