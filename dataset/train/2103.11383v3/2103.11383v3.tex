\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmicx}

\usepackage{algpseudocode}
\usepackage{subfigure}
\usepackage{CJK}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Multi-level Metric Learning for Few-shot Image Recognition}

\author{Haoxing Chen, Huaxiong Li, Yaohui Li, Chunlin Chen\\
	Nanjing University, Nanjing, China\\
	{\tt\small haoxingchen, yaohuili@smail.nju.edu.cn, huaxiongli, clchen@nju.edu.cn}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
	Few-shot learning is devoted to training a model on few samples. Recently, the method based on local descriptor metric-learning has achieved great performance. Most of these approaches learn a model based on a pixel-level metric. However, such works can only measure the relations between them on a single level, which is not comprehensive and effective. We argue that if query images can simultaneously be well classified via three distinct level similarity metrics, the query images within a class can be more tightly distributed in a smaller feature space, generating more discriminative feature maps. Motivated by this, we propose a novel Multi-level Metric Learning (MML) method for few-shot learning, which not only calculates the pixel-level similarity but also considers the similarity of part-level features and the similarity of distributions. First, we use a feature extractor to get the feature maps of images. Second, a multi-level metric module is proposed to calculate the part-level, pixel-level, and distribution-level similarities simultaneously. Specifically, the distribution-level similarity metric calculates the distribution distance (i.e., Wasserstein distance, Kullback-Leibler divergence) between query images and the support set, the pixel-level, and the part-level metric calculates the pixel-level and part-level similarities respectively. Finally, the fusion layer fuses three kinds of relation scores to obtain the final similarity score. Extensive experiments on popular benchmarks demonstrate that the MML method significantly outperforms the current state-of-the-art methods. 
\end{abstract}

\section{Introduction}



Humans can learn novel concepts and objects with just a few samples. Recently, many methods were proposed to learn new concepts with limited labeled data, such as semi-supervised learning~\cite{cvpr0003CCL20,cvprWangKG0K20}, zero-shot learning~\cite{cvprWuZZLZW20,cvprYuJHZ20}, and few-shot learning~\cite{ravi2016optimization,finn2017model,snell2017prototypical,sung2018learning,li2019revisiting,simon2020adaptive,chen2020multi}. Facing with the problem of data scarcity, these three paradigms propose solutions from different perspectives. Semi-supervised learning aims to train a model with few labeled data and a large amount of unlabeled data, and zero-shot learning devoted to identifying unseen categories with no labeled data, while few-shot learning focuses on learning new concepts with few labeled data. We propose a novel few-shot learning method to address the problem of data scarcity in this paper.

The few-shot learning methods can be roughly classified into two categories: meta-learning based methods~\cite{ravi2016optimization,finn2017model,sun2019meta} and metric-learning based methods~\cite{snell2017prototypical,sung2018learning,li2019revisiting,simon2020adaptive,chen2020multi}. Metric-based few-shot learning methods have achieved remarkable success due to their fewer parameters and effectiveness. In this work, we focus on this branch.

\begin{figure}[t]
	\centering
	\includegraphics[height=2.1cm,width=8cm]{fea-eps-converted-to.pdf}
	\caption{An example of feature representation at different level. (Best view in color.)}
\end{figure}

The basic idea of the metric-learning based few-shot learning method is to learn a good metric to calculate the similarity between query images and the support set. Therefore, how to learn good feature embedding representation and similarity metric are the key problem of metric-learning based few-shot learning method. For feature embedding representation, ProtoNet~\cite{snell2017prototypical} and RelationNet~\cite{sung2018learning} adopt image-level feature representations. However, due to the scarcity of data, it is not sufficient to measure the relation at the image-level~\cite{snell2017prototypical,sung2018learning}. Recently, CovaMNet~\cite{li2019distribution}, DN4~\cite{li2019revisiting} and MATANet~\cite{chen2020multi} introduce local representations (LRs) into few-shot learning and utilize these LRs to represent the image features, which can achieve better recognition results. 

For similarity metrics, these existing methods calculate similarities by different metrics. For example, Relation Nets~\cite{sung2018learning} proposes a network to learn the most suitable image-level similarity metric functions. DN4~\cite{chen2020multi} proposes a cosine-based image-to-class metric to measure the similarity on pixel-level.

However, all methods mentioned above only consider the distribution of support set, ignoring the natural distribution of query images. We argue that the distribution of the query image is also important for few-shot learning. It is necessary to design a distribution-level similarity metric to capture the distribution-level relations between query images and support set. Moreover, these methods only calculate similarities on a single level, i.e., pixel-level or image-level, which is not effective enough. Intuitively, under the few-shot learning setting, the features obtained by adopting a single similarity measure are not comprehensive, and the single similarity measure may lead to a certain similarity deviation, thus reducing the generalization ability of the model. It is necessary to adopt multi-level similarity metric, generating more discriminative features rather than using a single measure.

To this end, we propose a novel multi-level metric learning method (MML), which can be trained in an end-to-end manner. First, we represent all images as a set of LRs, rather than a global feature representation at the image-level. Moreover, inspired by CBAM~\cite{eccvWooPLK18}, we not only consider these features as a pixel-level set but also consider these features as a part-level set. Afterward, we employ a multi-level metric module to calculate the relations at different feature levels as illustrated in Figure 1, i.e., pixel-level, part-level, and distribution-level. Specifically, we use DN4~\cite{li2019revisiting} as the pixel-level similarity metric, to capture local relationships. We propose a novel part-level similarity metric to calculate the different semantic part relations, and employ a distribution similarity metric (i.e., Kullback-Leibler divergence and Wasserstein distance) to get the distribution-level relations. Finally, a fusion layer is proposed to fuse three kinds of relation scores to obtain the final similarity score.

The main contributions are summarized as follows:
\begin{itemize}
	\setlength{\itemsep}{0pt}
	\setlength{\parsep}{0pt}
	\setlength{\parskip}{0pt}
	\item We propose a part-level similarity metric for few-shot learning, which can capture the part-level semantic similarity between query images and support images.
	\item We propose a novel multi-level metric learning method by computing the semantic similarities on pixel-level, part-level, and distribution-level simultaneously, aiming to find more comprehensive semantic similarities.
	\item We conduct sufficient experiments on popular benchmark datasets to verify the advancement of our model and the performance of our model achieves the state-of-the-art.
\end{itemize}

\begin{figure*}[t]
	\centering
	\includegraphics[height=9.7cm,width=17cm]{mml-eps-converted-to.pdf}
	\caption{The framework of MML under the 5-way 1-shot image classification setting. The model mainly consists of three modules: the feature extractor  to learn local representations (LRs), the multi-level metric-learning module to capture the semantic similarity at a different level, i.e., pixel-level, part-level, and distribution-level, and the fusion layer  to fuse three kinds of relation score to get the final similarity score. (Best view in color.)}
	\label{mml_architecture}
\end{figure*}
\section{Related Works}
Recently, the domain of few-shot learning has shown increased interest. In this section, we first review recent metric-learning based few-shot learning methods and then introduce the work that inspire our work.

\textbf{Metric-learning based few-shot learning: } Metric-learning based methods can be roughly classified into two groups: 

(1) \emph{Learning feature embedding representation:}
Koch et al. \cite{koch2015siamese} used a Siamese Neural Network to tackle the one-shot learning problem, in which the feature extractor is of VGG styled structure and  distance is used to measure the similarity between query images and support images.
Snell et al. \cite{snell2017prototypical} proposed Prototypical Networks, in which the Euclidean distance is used to compute the distance between class-specific prototypes. 
Li et al. \cite{li2019revisiting} proposed an image-to-class mechanism to find the relation at pixel-level, in which the image features are represented as a local descriptor collection.


(2) \emph{Learning similarity metric: }
Sung et al.~\cite{sung2018learning} replaced the existing metric with the Relation Network, which measures the similarity between each query instance and support classes. Li et al.~\cite{li2019revisiting} proposed a Deep Nearest Neighbor Neural Network (DN4) to learn an image-to-class metric by measuring the cosine similarity between the deep local descriptors of a query instance and its neighbors from each support class. Li et al.~\cite{li2019distribution}explored the distribution consistency-based metric by introducing local covariance representation and deep covariance metric. Unlike these methods, the proposed MML measures the similarity at three different feature levels, i.e., pixel-level, part-level, and distribution-level.

\textbf{Other important works:}
Woo et at.~\cite{eccvWooPLK18} proposed a Convolutional Block Attention Module (CBAM), which is a kind of attention mechanism module combining spatial information and channel information. According to CBAM, each channel of the feature map can be regarded as a descriptor of a part. Inspired by this, we represent the feature map as a set of part-level feature descriptors and propose a component-level metric to capture semantic correlations between different components.


\section{Problem Definition and Formulation}
Standard few-shot image recognition problems are often formalized as \emph{N}-way \emph{M}-shot classification problem, in which models are given \emph{M} seen images from each of \emph{N} classes, and required to correctly classify unseen images. 
Different from traditional image recognition tasks, few-shot learning aims to classify novel classes after training. This requires
that samples used for training, validation, and testing should
come from disjoint label space.
To be more specific, given a dataset of visual concepts , we devide it into three parts: ,  and , and their label space satisfy .

To obtain a trained model, we train our model in an episodic way. That is, in each episode, a new task is randomly sampled from the training set  to train the current model. Each task consists of two subsets, including support set  and query set . The  contains  previously unseen classes, with  samples for each class. We focus on training our model to correctly determine which category each image in the  belongs to. Similarly, we randomly sample tasks from  and  for meta-validation and meta-testing scenarios.

\section{Proposed Approach}
As shown in Figure 2, our MML is mainly composed of three modules: a feature extractor , a multi-level metric-learning module, and a fusion layer . All images are first fed into the  to get feature embeddings. Then, we represented them as pixel-level and part-level feature descriptors. Afterward, the multi-level metric-learning module calculates similarities on part-level, pixel-level, and distribution-level simultaneously. Finally, we adaptively fuse the part-level, pixel-level, and distribution-level similarities together by a fusion layer . All the modules can be trained jointly in an end-to-end manner.

\subsection{Feature Embedding with Local Representations}
As proved by some recent studies~\cite{li2019distribution,li2019revisiting,chen2020multi}, feature representation based on local descriptor is more abundant than image-level feature representation, which can alleviate the problem of sample scarcity. Therefore, we employ local descriptors to represent each image.

Specifically, we employ a feature extractor , which can extract informative local descriptors. Given an image , through , we can get a three-dimensional (3D) feature vector . As proposed by~\cite{li2019distribution,li2019revisiting,chen2020multi}, the 3D feature vector can be regarded as a set of  -dimensional pixel-level feature descriptors:
 
where  is the -th pixel-level feature descriptor. 

Inspired by~\cite{eccvWooPLK18,cvprHuSS18}, each channel in  can be regarded as a semantic representation of a specific part in the image, as shown in Figure 1(b). Therefore, the 3D feature vector can also be regarded as a set of  -dimensional part-level feature descriptors:
 
where  is the -th part-level feature descriptor. 


\subsection{Multi-level Metric Learning}
Under \emph{N-way M-shot} few-shot learning setting, given a query image  and a certain support class , through feature extractor , we can get the fearure representation  and , respectively. The  can be regards as 

Also, the  can be regards as 



\textbf{Part-level similarity metric.}
After obtaining the part-level feature representation of the query image  and the support class , we calculate the correlation matrix  between the query image and the support class on part-level:

where ,   is  element of  reflecting the distance between the -th part-level feature descriptor of the query image and the -th part-level feature descriptor of support clss and  is Cosine  similarity function. Each row in  represents the semantic relation of each part-level feature descriptor in the query image to all part-level feature descriptors of all images in the support class.
For each part-level feature descriptor  of the query image , we find its  most similar part-level feature descriptors. Then, we sum  selected part-level feature descriptors as the part-level similarity between the query image and the support class

where  means selecting the  largest elements in each row of the .

Typically,  is set as 1 in our work.

\textbf{Pixel-level similarity metric}
We adopt DN4~\cite{li2019revisiting} as our pixel-level similarity metric, which proposed an image-to-class measure to capture the local relations between query images and support class. Here, we have a brief review of DN4. Given a query image and a support class, we can get their pixel-level feature set  and . Then, we calculate the correlation matrix   between the query image and the support class on pixel-level and select the  largest element in each row of the correlation matrix:

Typically,  is set as 1 in our work.

\textbf{Distribution-level similarity metric}
Inspired by~\cite{li2019distribution,ADM}, we assume the distribution of pixel-level feature descriptors are multivariate Gaussian. Therefore, we can use  and  as the query image's distribution and the support class's distribution, respectively.
 indicate the mean vector and  indicate covariance matrix of a specific distribution. Then the distribution-level metric can be denoted as

where  is a certain distribution metric. There are many options for distance metric function. In this paper, Kullback-Leibler divergence~\cite{Kullback51klDivergence} and Wasserstein distance are selected for experiments:

(1) \emph{Kullback-Leibler divergence}:
We use KL divergence to match the distribution of  to the one of support class , so the KL divergence can be denoted as

where  is the value of the determinant of a square matrix,   denotes the trace operation of matrixes.

(2) \emph{Wasserstein distance}: 2-Wasserstein distance can also be taken as a choice of , whose formulation is defined as follows

However, since the square root of the matrix incurs huge computational time, we use the approximate form proposed by~\cite{pamiHeWST19}:





\subsection{Fusion Layer}
Since three different level similarities have been calculated, i.e., part-level, pixel-level, and distribution-level, we need to design a fusion module to integrate them.
In order to tackle this problem, we adopt an adaptive fusion strategy, which use a learnable vertor 
to integrate these three parts. Specifically, the final similarity can be obtained by the following equation

Note that since the distribution level measure represents difference rather than sameness, we use negative numbers to indicate similarity. Under the 5-way 1-shot few-shot learning setting, input a query image, we will get five-dimensional vectors on each level. We first concatenate these three vectors and balance the size of these three vectors by a Batch Normalization layer. 
Then, we use a 1D convolution layer with the kernel size of  and the dilation value of 3. Finally, we can get a weighted 5-dimensional similarity vector , we use it for final classification. 

\begin{table}[t]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		\textbf{Dataset} &\textbf{}&\textbf{}&\textbf{}&\textbf{}
		\\
		\midrule
		\emph{mini}ImageNet&100&64&16&20\\
		\emph{tiered}ImageNet&608&351&97&160\\
		Stanford Dogs&120&70&20&30\\		
		Stanford Cars&196&130&17&49\\
		CUB Birds&200&100&50&50\\
		\bottomrule
	\end{tabular}	
	\caption{The splits of evaluation datasets.  is the number of all classes. ,  and  indicate the number of classes in training set, validation set and test set.}
\end{table}

\begin{table*}[t]
	
	\centering
	\begin{tabular}{cccccc}
		\toprule
		\label{mini}
		\textbf{Method} &\textbf{Venue} &\textbf{Backbone} & \textbf{Type}  & \textbf{5-way 1-shot} & \textbf{5-way 5-shot}\\
		\midrule
MAML+L2F \cite{baik2020learning} &CVPR'20 & Conv-32F  &Meta& 52.10\footnotesize{0.49}& 69.38\footnotesize{0.46}\\
BOIL \cite{BOIL2021ICLR}&ICLR'21 &  Conv-64F  &Meta& 49.61\footnotesize{0.16}& 66.45\footnotesize{0.37} \\
		MatchingNet \cite{vinyals2016matching} & NeurIPS'16&Conv-64F & Metric  & 43.56\footnotesize{0.84} &55.31\footnotesize{0.73}  \\
		ProtoNet \cite{snell2017prototypical} &NeurIPS'17& Conv-64F & Metric &49.42\footnotesize{0.78} & 68.20\footnotesize{0.66} \\
		RelationNet \cite{sung2018learning} & CVPR'18&Conv-64F & Metric &50.44\footnotesize{0.82}&65.32\footnotesize{0.70}\\
CovaMNet \cite{li2019distribution} & AAAI'19&Conv-64F & Metric & 51.19\footnotesize{0.76} &67.65\footnotesize{0.63}\\\
		DN4 \cite{li2019revisiting} & CVPR'19&Conv-64F & Metric & 51.24\footnotesize{0.74} &71.02\footnotesize{0.64} \\
		CTM  \cite{ctm19cvpr} & CVPR'19&Conv-64F & Metric & 41.62\footnotesize{0.00} &58.77\footnotesize{0.00} \\
DSN \cite{simon2020adaptive} & CVPR'20&Conv-64F & Metric & 51.78\footnotesize{0.96}& 68.99\footnotesize{0.69} \\
		ADM \cite{ADM} & IJCAI'20&Conv-64F & Metric & 54.26\footnotesize{0.63}& 72.54\footnotesize{0.50} \\
		Align(Centroid) \cite{Afraeccv20} & ECCV'20&Conv-64F & Metric & 53.14\footnotesize{1.06}& 71.45\footnotesize{0.72} \\
Neg-Margin \cite{liu20eccv} & ECCV'20 &Conv-64F & Others & 52.68\footnotesize{0.76} & 70.41\footnotesize{0.66} \\
		Two-stage \cite{Das2020TIP} & T-IP'2020 &Conv-64F & Others & 52.68\footnotesize{0.51} & 70.91\footnotesize{0.85} \\
		\midrule
		\textbf{MML(KL)} &Ours& Conv-64F & Metric &\textcolor{red}{55.14\footnotesize{0.63}}  & \textcolor{red}{73.33\footnotesize{0.50}} \\
		\textbf{MML(Wass)} &Ours& Conv-64F & Metric &\textcolor{blue}{54.94\footnotesize{0.64}}  & \textcolor{blue}{72.81\footnotesize{0.50}} \\
		\bottomrule
		ProtoNet \cite{snell2017prototypical} &NeurIPS'17& ResNet-12 & Metric &60.37\footnotesize{0.83} & 78.02 \footnotesize{0.57} \\
		CTM \cite{ctm19cvpr} & CVPR'19&ResNet-12 & Metric & 64.12\footnotesize{0.82} &80.51\footnotesize{0.13} \\
		DSN \cite{simon2020adaptive} & CVPR'20&ResNet-12 & Metric & 62.64\footnotesize{0.66}& 78.73\footnotesize{0.45} \\
		MetaOptNet \cite{cvprLeeMRS19} & CVPR'19 &ResNet-12 & Others & 62.64\footnotesize{0.61} & 78.63\footnotesize{0.46} \\
		Neg-Margin \cite{liu20eccv} & ECCV'20 &ResNet-12& Others & 63.85\footnotesize{0.81} & 81.57\footnotesize{0.56} \\
		DeepEMD \cite{ZhangCLS20} & CVPR'20&ResNet-12 & Metric & \textcolor{red}{65.91\footnotesize{0.82}} &79.74\footnotesize{0.56} \\
		\midrule
		\textbf{MML(KL)} &Ours& ResNet-12& Metric &\textcolor{blue}{65.55\footnotesize{0.77}}  & \textcolor{red}{81.86\footnotesize{0.51}} \\
		\textbf{MML(Wass)} &Ours& ResNet-12 & Metric &65.32\footnotesize{0.76}  & \textcolor{blue}{81.77\footnotesize{0.53}} \\				
		\bottomrule
		
	\end{tabular}
	\caption{Comparison with other state-of-the-art methods with  confidence intervals on \emph{mini}ImageNet. The third column shows which kind of embedding is employed. The fourth column shows which type of the method belongs to, i.e, meta-learning based, metric-learning based, and other kinds of methods. (Top two performances are shown in red and blue.) }
\end{table*}


\begin{table}[t]
	\centering
	\begin{tabular}{c c p{1.3cm}<{\centering} p{1.3cm}<{\centering} }
		\toprule
		\label{tiered}
		\multirow{2}{*}{\textbf{Model}}&\multirow{2}{*}{\textbf{Backbone}}& \multicolumn{2}{c}{\textbf{5-Way Accuracy()}}
		\\
		&& \textbf{1-shot} & \textbf{5-shot}\\
		\midrule
ProtoNet~\cite{snell2017prototypical} &Conv-64F& 48.67\footnotesize{0.87} & 69.57\footnotesize{0.75}\\
		RelationNet~\cite{sung2018learning} &Conv-64F& 54.48\footnotesize{0.93} & 71.32\footnotesize{0.78}\\
		CovaMNet~\cite{li2019distribution} &Conv-64F& 54.98\footnotesize{0.90} & 71.51\footnotesize{0.75}\\
		DN4~\cite{li2019revisiting} &Conv-64F& 53.37\footnotesize{0.86} & 74.45\footnotesize{0.70}\\
		ADM~\cite{ADM} &Conv-64F& 56.01\footnotesize{0.69} & \textcolor{blue}{75.18\footnotesize{0.56}}\\	
\midrule		
		\textbf{MML(KL)} &Conv-64F& \textcolor{blue}{57.37\footnotesize{0.70}} &74.98\footnotesize{0.55} \\
		\textbf{MML(Wass)}&Conv-64F &\textcolor{red}{57.89\footnotesize{0.69}}&
		\textcolor{red}{75.49\footnotesize{0.55}} \\
		\bottomrule
		ProtoNet~\cite{snell2017prototypical} &ResNet-12 & 68.37\footnotesize{0.23} & 83.43\footnotesize{0.16}\\
		RelationNet~\cite{sung2018learning} &ResNet-12 & 58.99\footnotesize{0.86} & 75.78\footnotesize{0.76}\\		
		DSN \cite{simon2020adaptive} & ResNet-12 & 67.39\footnotesize{0.82}& 82.85\footnotesize{0.56} \\
		DeepEMD \cite{ZhangCLS20} &ResNet-12  & 71.16\footnotesize{0.87} &83.95\footnotesize{0.58} \\
		\midrule
		\textbf{MML(KL)} &ResNet-12 & \textcolor{blue}{71.32\footnotesize{0.75}} &\textcolor{blue}{84.15\footnotesize{0.59}} \\
		\textbf{MML(Wass)}&ResNet-12 &\textcolor{red}{71.82\footnotesize{0.76}}&
		\textcolor{red}{84.53\footnotesize{0.61}} \\
		\bottomrule
	\end{tabular}
	\caption{Comparison with other state-of-the-art methods with  confidence intervals on \emph{tiered}ImageNet. (Top two performances are shown in red and blue.) }
\end{table}

\begin{table*}
	\centering
	\begin{tabular}{c  p{1.8cm}<{\centering}  p{1.8cm}<{\centering}  p{1.8cm}<{\centering}  p{1.8cm}<{\centering}  p{1.8 cm}<{\centering}  p{1.8cm}<{\centering} }
		\toprule
		\label{fsfg}
		\multirow{3}{*}{\textbf{Model}} & \multicolumn{6}{c}{\textbf{5-Way Accuracy()}}
		\\
		\cmidrule{2-7}
		&\multicolumn{2}{c}{\textbf{Stanford Dogs}} &\multicolumn{2}{c}{\textbf{Stanford Cars}} 
		&\multicolumn{2}{c}{\textbf{CUB Birds}} \\
		& 1-shot& 5-shot & 1-shot & 5-shot & 1-shot & 5shot\\
		\midrule MatchingNet \cite{vinyals2016matching} & 35.80\footnotesize{0.99} & 47.50\footnotesize{1.03}  & 34.80\footnotesize{0.98} & 44.70\footnotesize{1.03}  & 61.16\footnotesize{0.89} & 72.86\footnotesize{0.70} \\
		ProtoNet \cite{snell2017prototypical}& 37.59\footnotesize{1.00} & 48.19\footnotesize{1.03}  & 40.90\footnotesize{1.01} & 52.93\footnotesize{1.03} & 51.31\footnotesize{0.91} & 70.77\footnotesize{0.69}  \\
		GNN \cite{garcia2017few} & 46.98\footnotesize{0.98}& 62.27\footnotesize{0.95}  & 55.85\footnotesize{0.97} & 71.25\footnotesize{0.89}  & 51.83\footnotesize{0.98} & 63.69\footnotesize{0.94} \\
		MAML \cite{finn2017model} & 44.81\footnotesize{0.34} & 58.68\footnotesize{0.31} &47.22\footnotesize{0.39} & 61.21\footnotesize{0.28} & 55.92\footnotesize{0.95}  &  72.09\footnotesize{0.76} \\
		RelationNet \cite{sung2018learning} & 43.33\footnotesize{0.42} & 55.23\footnotesize{0.41} & 47.67\footnotesize{0.47}  & 60.59\footnotesize{0.40} & 62.45\footnotesize{0.98}  & 76.11\footnotesize{0.69} \\
		adaCNN \cite{munkhdalai2018rapid} & 41.87\footnotesize{0.42}& 53.93\footnotesize{0.44}  & 42.14\footnotesize{0.41} & 50.12\footnotesize{0.34}& 56.57\footnotesize{0.47} & 61.21\footnotesize{0.42} \\
		PCM \cite{pcm2019tip} & 	28.78\footnotesize{2.33}& 46.92\footnotesize{2.00}  & 29.63\footnotesize{2.38} & 52.28\footnotesize{1.46}& 42.10\footnotesize{1.96} & 62.48\footnotesize{1.21} \\
		CovaMNet  \cite{li2019distribution} & 49.10\footnotesize{0.76}  & 63.04\footnotesize{0.65} & 56.65\footnotesize{0.86} & 71.33\footnotesize{0.62}  & 60.58\footnotesize{0.69} & 74.24\footnotesize{0.68} \\
		DN4  \cite{li2019revisiting} & 45.41\footnotesize{0.76} & 63.51\footnotesize{0.62}  & 59.84\footnotesize{0.80} & 88.65\footnotesize{0.44}  & 52.79\footnotesize{0.86} & \textcolor{red}{81.45\footnotesize{0.70}}  \\
		 \cite{huang2020low} & 45.65\footnotesize{0.71} & 61.24\footnotesize{0.62} & 54.44\footnotesize{0.71}  & 67.36\footnotesize{0.61}  & 63.56\footnotesize{0.79} & 75.35\footnotesize{0.58} \\
		 \cite{huang2020low} & 45.72\footnotesize{0.75} & 60.94\footnotesize{0.66} & 60.28\footnotesize{0.76} & 73.29\footnotesize{0.58} & 63.63\footnotesize{0.77}& 76.06\footnotesize{0.58} \\
		\midrule	
		\textbf{MML(KL)} & \textcolor{red}{59.05\footnotesize{0.68}} & \textcolor{red}{75.59\footnotesize{0.51}}   & \textcolor{red}{72.43\footnotesize{0.65}}  & \textcolor{red}{91.05\footnotesize{0.30}} &  \textcolor{red}{63.86\footnotesize{0.67}}  & \textcolor{blue}{80.73 \footnotesize{0.46}} \\
		\textbf{MML(Wass)} & \textcolor{blue}{58.07\footnotesize{0.68}} & \textcolor{blue}{75.15\footnotesize{0.49}}   & \textcolor{blue}{72.40\footnotesize{0.63}}   & \textcolor{blue}{91.01\footnotesize{0.31}} &  \textcolor{blue}{63.64\footnotesize{0.69}}  & 80.63 \footnotesize{0.47} \\
		
		\bottomrule
	\end{tabular}
	\caption{Experimental results compared with other methods on three fine-grained datasets. (Backbone: Conv-64F. Top two performances are shown in red and blue.)}
\end{table*}





\begin{table*}
	\centering
	\begin{tabular}{ccccccc}
		\toprule
		\label{cross_results}
		\multirow{3}{*}{\textbf{Model}} & \multicolumn{6}{c}{\textbf{5-Way Accuracy()}}
		\\
		\cmidrule{2-7}
		&\multicolumn{2}{c}{\textbf{\emph{mini}ImageNetDogs}} &\multicolumn{2}{c}{\textbf{\emph{mini}ImageNetCars}} 
		&\multicolumn{2}{c}{\textbf{\emph{mini}ImageNetBirds}} \\
		& 1-shot& 5-shot & 1-shot & 5-shot & 1-shot & 5shot\\
		\midrule MatchingNet~\cite{vinyals2016matching} & 31.53\footnotesize{0.98} & 41.25\footnotesize{1.03}  & 24.88\footnotesize{0.95} & 40.55\footnotesize{0.98}  & 33.75\footnotesize{0.95} & 53.14\footnotesize{0.77} \\
		ProtoNet~\cite{snell2017prototypical}& 33.24\footnotesize{0.99} & 42.16\footnotesize{1.02}  & 31.47\footnotesize{1.03} & 48.75\footnotesize{1.02} & 42.09\footnotesize{0.88} & 62.02\footnotesize{0.65}  \\				
		RelationNet~\cite{sung2018learning} & 31.99\footnotesize{0.55} & 50.35\footnotesize{0.49}  & 37.14\footnotesize{0.51} & 44.95\footnotesize{0.43}  & 38.25\footnotesize{0.99} & 57.73\footnotesize{0.76} \\
		CovaMNet~\cite{li2019distribution}& 38.03\footnotesize{0.77} & 56.22\footnotesize{0.63}  & 32.33\footnotesize{0.75} & 49.86\footnotesize{0.63} & 43.11\footnotesize{0.65} & 63.22\footnotesize{0.69}  \\
		DN4~\cite{li2019revisiting}  & 38.37\footnotesize{0.72}& 
		56.50\footnotesize{0.72}  &
		32.51\footnotesize{0.62} &  
		50.19\footnotesize{0.73}&
		43.65\footnotesize{0.77} & 63.78\footnotesize{0.69} \\	
		\midrule 
		\textbf{MML(KL)} & \textcolor{blue}{40.94\footnotesize{0.57}}& \textcolor{red}{58.43\footnotesize{0.54}} & \textcolor{blue}{34.05\footnotesize{0.51}} & \textcolor{red}{53.19\footnotesize{0.56}} &  \textcolor{red}{44.86\footnotesize{0.58}} & \textcolor{red}{65.20\footnotesize{0.56}} \\
		\textbf{MML(Wass)} & \textcolor{red}{41.26\footnotesize{0.60}} & \textcolor{blue}{57.73\footnotesize{0.57}}   & \textcolor{red}{34.22\footnotesize{0.50}}  & \textcolor{blue}{52.80\footnotesize{0.54}} &  \textcolor{blue}{44.63\footnotesize{0.58}}  & \textcolor{blue}{64.98\footnotesize{0.52}} \\
		\bottomrule
	\end{tabular}
	\caption{Experimental results compared with other methods on \emph{mini}ImageNet under cross-domain few-shot learning setting. (Backbone: Conv-64F. Top two performances are shown in red and blue.)}
\end{table*}



\begin{table*}
	\centering
	\begin{tabular}{p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}p{1.4cm}<{\centering}cccc}
		\toprule
		\multirow{3}{*}{\textbf{Part}}
		&\multirow{3}{*}{\textbf{Pixel}} &\multirow{3}{*}{\textbf{Dis.(KL)}}&\multirow{3}{*}{\textbf{Dis.(Wass)}}& \multicolumn{4}{c}{\textbf{5-Way Accuracy()}}
		\\
		\cmidrule{5-8}&&&
		&\multicolumn{2}{c}{\textbf{\emph{mini}ImageNet}} &\multicolumn{2}{c}{\textbf{\emph{tiered}ImageNet}} \\
		& & && 1-shot & 5-shot& 1-shot & 5-shot  \\
		\midrule \checkmark & & &&49.82\footnotesize{0.65} & 65.33\footnotesize{0.98} &51.04\footnotesize{0.93} & 70.25\footnotesize{1.13}   \\
		&\checkmark&& &51.32\footnotesize{0.73}  & 70.95\footnotesize{0.67}  &53.22\footnotesize{0.85} & 74.33\footnotesize{0.71} \\
		&& \checkmark&& 52.87\footnotesize{0.64}   &  69.32\footnotesize{0.50}&55.43\footnotesize{0.68} & 74.21\footnotesize{0.56}  \\
		&& &\checkmark& 50.31\footnotesize{0.62}   &  67.62\footnotesize{0.53}&52.88\footnotesize{0.71} & 73.67\footnotesize{0.57}  \\
		\checkmark&\checkmark&  && 51.95\footnotesize{0.68}   &  71.35\footnotesize{0.65} &53.65\footnotesize{0.87} & 74.43\footnotesize{0.76}  \\
		\checkmark&& \checkmark && 53.15\footnotesize{0.62}   &  70.16\footnotesize{0.53} &55.74\footnotesize{0.71} & 74.07\footnotesize{0.55}  \\
		\checkmark&&&\checkmark& 51.33\footnotesize{0.65}   &  68.56\footnotesize{0.55} &53.62\footnotesize{0.76} & 74.32\footnotesize{0.59}  \\
		&\checkmark & \checkmark&& 53.82\footnotesize{0.63}   &  72.13\footnotesize{0.50} &55.69\footnotesize{0.71} & 74.46\footnotesize{0.56}  \\
		&\checkmark & &\checkmark& 53.42\footnotesize{0.59}   &  72.08\footnotesize{0.52} &54.85\footnotesize{0.68} & 73.85\footnotesize{0.53}  \\
		\checkmark&\checkmark & \checkmark&& \textcolor{red}{55.14\footnotesize{0.63}}   &  \textcolor{red}{73.12\footnotesize{0.69}}  &\textcolor{blue}{57.37\footnotesize{0.70}} &\textcolor{blue}{74.98\footnotesize{0.55}} \\
		\checkmark&\checkmark & &\checkmark& \textcolor{blue}{54.94\footnotesize{0.64}}  & \textcolor{blue}{72.81\footnotesize{0.50}}  &\textcolor{red}{57.89\footnotesize{0.69}}&
		\textcolor{red}{75.49\footnotesize{0.55}} \\       
		\bottomrule
	\end{tabular}
	\caption{Ablation study on \emph{mini}ImageNet. (Backbone: Conv-64F.)}
\end{table*}




\section{Experiments}
In this section, we perform extensive experiments to verify the advance and effectiveness of MML. 
\subsection{Datasets}
To verify the advance and effectiveness of our proposed MML, we performed experiments on five benchmark datasets: \emph{mini}ImageNet~\cite{finn2017model}, \emph{tiered}ImageNet~\cite{ren18iclr}, CUB Birds~\cite{wah2011caltech}, Stanford Dogs~\cite{khosla2011novel} and StanfordCars~\cite{krause20133d}.

\textbf{ImageNet derivatives:} Both \emph{mini}ImageNet dataset and \emph{tiered}ImageNet dataset are subsets of ImageNet \cite{deng2009imagenet}. The \emph{mini}ImageNet dataset consists 100 classes, each of which contains 600 samples, and the \emph{tiered}ImageNet contains 608 classes.


\textbf{Fine-grained dataset:} CUB Birds contains 11,788 images of 200 classes of birds. Stanford Dogs contains 20,580 annotated images from 120 dog species. Stanford Cars including 16,185 annotated images of 196 breeds of cars.

The partition of all data sets is shown in Table 1. All images are resized to .

\subsection{Network Architecture}
In order to make a fair comparison with other works, we adopt the shallow \emph{Conv64F} network~\cite{li2019distribution,li2019revisiting} and \emph{ResNet-12} network~\cite{cvprLeeMRS19} as our feature extrator . 

\emph{Conv64F} has four convolution blocks, each convolutional block consists of a convolutional layer with 64  filters, a batch normalization layer, and a Leaky ReLU activation layer. Besides, the  max-pooling layer is added in the first two blocks.

\emph{ResNet-12} has four residual blocks, each residual block has 3 convolutional layers with 3Ã—3 kernel, and a  max-pooling layer is added in the first residual block.


\subsection{Implementation Details}
We use pytorch~\cite{pytorch19nips} to implement all the experiments. \footnote{Our code is available at https://github.com/chenhaoxing/M2L.}We conduct our experiments on a series of \emph{N}-way \emph{M}-shot tasks, i.e., 5-way 1-shot and 5-way 5-shot.
On ImageNet derivatives and fine-grained datasets, we train our model 50 epochs.
In each epoch, we randomly sampled 10000 tasks. 
We use the Adam~\cite{kingma2014adam} optimizer and the cross-entropy (CE) loss to train our MML. Our batch size is set to 4, the initial learning rate is 0.001, and multiplied by 0.5 every 10 epochs. During the test stage, we report our performance based on an average of 1000 tasks. We report the average accuracy as well as the corresponding 95\% confidence interval over these 1000 tasks.

As we all know, using deeper networks to extract features or using pre-trained models can achieve higher accuracy. To make a fair comparison, following the previous works, when using \emph{Conv-64F} as feature extractor , we do not apply the pre-training strategy. And when \emph{ResNet-12} is used as feature extractor , we apply an additional pre-training strategy as suggested in \cite{liu20eccv}. 
\subsection{Comparison Against Related Approaches}
\textbf{Results on ImageNet derivatives.} 
As seen from Table 2, when adopting \emph{Conv-64F} as feature extractor, our MML(KL) achieves the highest accuracy on \emph{mini}ImageNet with 55.14\% and 73.33\% on 5-way 1-shot and 5-way 5-shot tasks respectively, which make a great improvement compared to the previous pixel-level metric-learning based methods. For example, our MML(KL) is 7.7\% and 7.6\% better than CovaMNet and DN4 on the 5-way 1-shot task, respectively. Although the experimental results of MML(Wasserstein) are not as good as MML(KL), MML(Wasserstein) is also better than other methods. 
Moreover, when adopting \emph{ResNet-12} as feature extractor, our MML(KL) achieves 65.55\% and 81.86\% in 5-way 1-shot and 5-way 5-shot respectively, which achieves competitive performance.

Table 3 summarizes the results on the \emph{tiered}ImageNet dataset. Our results outperform the state-of-the-art by a significant margin. For example, with \emph{Conv-64F} feature extrator, our MML(Wass) achieves new state-of-the-art results on \emph{tiered}ImageNet benchmark (57.89\% - up 5.3\% over the previous best) on 5-way 1-shot and (75.49\% - up 1.4\% over the previous best) on 5-way 5-shot. 


\textbf{Results on fine-grained datasets.}
Table 4 evaluates our method on three fine-grained datasets, i.e., Stanford Dogs, Stanford Cars, and CUB Birds. It can be seen that the proposed MML obtains significant improvements compared with previous state-of-the-art methods. For the 5-way 1-shot task, the proposed MML(KL) obtains state-of-the-art performance on all three fine-grained datasets. And for the 5-way 5-shot task, the proposed MML(KL) also achieves state-of-the-art performance on Stanford Dogs and Stanford Car , and a comparable performance on CUB Birds.
Specifically, compared with pixel-level metric-learning based methods (i.e., CovaMNet, DN4, and LRPABN), MML(KL) is 20.3\%, 20.2\%, and 0.4\% better than the best one of them on Stanford Dogs, Stanford Cars, and CUB Birds under 5-way 1-shot setting.

The reason why our MML can achieve these state-of-the-art performances is that MML can measure the semantic similarities on multiple levels, i.e., part-level, pixel-level, and distribution-level.



\subsection{Cross-Domain Few-Shot Learning}
Cross-domain few-shot learning assumes that images in the training set and test set can come from different domains. We proceed by meta-training the models on the \emph{mini}ImageNet training set and evaluate the model on three fine-grained test sets. This setting of domain shift results in a large margin between the distribution of training set and test set. We use the same dataset partitioning as shown in Table 1. 
To verify the effectiveness of our model, we compared five classical metric-learning based few-shot learning methods: MatchingNet, ProtoNet, RelationNet, CovaMNet, and DN4.

Table 5 gives the quantitative results. It can be seen that MML outperforms all the baseline methods. This verifies that our model is more robust under these more challenging tasks. The reason why our MML can achieve these results is that our MML can capture more comprehensive semantic similarities than previous metric-learning based methods. 
\subsection{Ablation Study}


\begin{table}[t]
	\centering
	\begin{tabular}{c p{1.4cm}<{\centering} p{1.2cm}<{\centering} p{1.2cm}<{\centering}}
		\toprule
		\label{complexity}
		\multirow{2}{*}{\textbf{Model}}&\multirow{2}{*}{\textbf{Params}}& \multicolumn{2}{c}{\textbf{5-Way Accuracy()}}
		\\		\cmidrule{3-4}
		&& \textbf{1-shot} & \textbf{5-shot}\\
		\midrule MatchingNet~\cite{vinyals2016matching}& 0.113M &35.80\footnotesize{0.99} &47.50\footnotesize{1.03}\\
		ProtoNet~\cite{snell2017prototypical}& 0.113M &37.59\footnotesize{1.00}&48.19\footnotesize{1.03}
		\\
		RelationNet~\cite{sung2018learning}&0.229M &43.33\footnotesize{0.42}&55.23\footnotesize{0.41}
		\\
		GNN~\cite{garcia2017few}&1.619M &46.98\footnotesize{0.98}&62.27\footnotesize{0.95}
		\\
		CovaMNet~\cite{li2019distribution}&0.113M &49.10\footnotesize{0.76}&63.04\footnotesize{0.65}\\
		DN4~\cite{li2019revisiting}&0.113M &45.41\footnotesize{0.76}&63.51\footnotesize{0.62}
		\\
		\midrule
		\textbf{MML(KL)} &0.113M &\textcolor{red}{59.05\footnotesize{0.68}}  & \textcolor{red}{75.59\footnotesize{0.51}} \\
		\textbf{MML(Wass)} &0.113M &\textcolor{blue}{58.07\footnotesize{0.68}}  & \textcolor{blue}{75.15\footnotesize{0.49}} \\
		\bottomrule
	\end{tabular}
	\caption{The number of trainable parameters in different models and the corresponding classification accuracies on Stanford Dogs. (Backbone: Conv-64F. Top two performances are shown in red and blue.)
	}
\end{table}

\begin{figure*}[t]
	\centering
	\includegraphics[height=5.8cm,width=16cm]{acc-eps-converted-to.pdf}
	\caption{Influence of superparameters  and .}
	\label{influence_k}
\end{figure*}


To further explore the effect of the multi-level metric learning module, we prune any of three similarity branches in the multi-level metric-learning module. Specifically, we remove one or two branches from the multi-level metric-learning module and experiment on the \emph{mini}ImageNet and \emph{tiered}ImageNet datasets. 

As seen in Table 6, each part of the MML is indispensable. It can be observed that the accuracy of few-shot image recognition using only one level of features is very low. The results were significantly improved when two or three levels of features were used together, and the results were best when all three levels were used together. Specifically, compared with the method that only using pixel-level features, our MML gains 7.6\% and 3.1\% improvements.

\subsection{Complexity Analysis }

As shown in Table 7, we compare the trainable parameters to prove that our model is both simple and effective.
Although our multi-level metric-learning module has three branches, each branch has no trainable parameters. 
Therefore, if we ignore three trainable parameters in the fusion layer, our MML is non-parametric if not considering the feature extractor . Our model achieved a great improvement in accuracy while maintaining the same number of trainable parameters as MatchingNet, ProtoNet, CovaMNet, and DN4. Specifically, on the Stanford Dogs dataset, our MML(KL) obtains 64.9\%, 57.1\%, 20.3\% and 30.0\% improvements over MatchingNet, ProtoNet, CovaMNet, and DN4 under the 5-way 1-shot few-shot learning setting, respectively. Moreover, on the 5-way 5-shot task, our MML(KL) obtains 59.1\%, 56.9\%, 19.9\% and 19.0\% improvements over MatchingNet, ProtoNet, CovaMNet, and DN4 respectively.

\subsection{Influence of Superparameter  and  .}
In the part-level and pixel-level similarity metrics, we need to choose suitable  and . For this purpose, we conduct a contrast experiment on \emph{mini}ImageNet dataset under both 5-way 1-shot and 5-way 5-shot settings by varying the value of  and the value of .
Experimental results are shown in Figure 3. It can be seen that when  and ,  the experimental result of MML(KL) is the best.

\section{Conclusion}
In this paper, we revisit the metric-learning based method and proposed a novel Multi-level Metric Learning (MML) method for few-shot image recognition, aiming to capture more comprehensive semantic similarities. Specifically, the MML can measure the semantic similarities on multiple levels and produce more discriminative features. Extensive experiments show the effectiveness and the superiority of the MML.

{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{MML}
}


\clearpage
\end{document}
