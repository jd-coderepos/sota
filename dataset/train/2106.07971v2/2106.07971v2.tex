\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{nicefrac}       \usepackage{wrapfig}        \usepackage{caption}
\usepackage{tabularx}
\definecolor{deepgreen}{HTML}{6AA84F}
\usepackage{bm}
\usepackage[ruled]{algorithm2e}
\usepackage[separate-uncertainty=true]{siunitx}  \usepackage{graphicx}
\DeclareSIUnit\debye{D}  \DeclareSIUnit\cal{cal}  
\graphicspath{ {./images/} }

\title{Simple GNN Regularisation for 3D Molecular Property Prediction \& Beyond}



\author{Jonathan Godwin, Michael Schaarschmidt, Alexander Gaunt, \\
\textbf{Alvaro Sanchez-Gonzales, Yulia Rubanova, Petar Veličković,} \\
\textbf{James Kirkpatrick \& Peter Battaglia} \\
DeepMind, London\\
\texttt{\{jonathangodwin\}@deepmind.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\pete}[1]{\textcolor{orange}{TODO (Pete): {#1}}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
In this paper we show that simple noisy regularisation can be an effective way to address GNN oversmoothing. First we argue that regularisers addressing oversmoothing should both penalise node latent similarity and encourage meaningful node representations. From this observation we derive ``Noisy Nodes'', a simple technique in which we corrupt the input graph with noise, and add a noise correcting node-level loss. The diverse node level loss encourages latent node diversity, and the denoising objective encourages graph manifold learning. Our regulariser applies well-studied methods in simple, straightforward ways which allow even generic architectures to overcome oversmoothing and achieve state of the art results on quantum chemistry tasks, and improve results significantly on Open Graph Benchmark (OGB) datasets. Our results suggest Noisy Nodes can serve as a complementary building block in the GNN toolkit.


\end{abstract}


\section{Introduction}
Graph Neural Networks (GNNs) are a family of neural networks that operate on graph structured data by iteratively passing learned messages over the graph's structure \citep{Scarselli2009GN,bronstein2017geometric,Gilmer2017NeuralMP,Battaglia2018RelationalIB, Shlomi_2021}. While Graph Neural Networks have demonstrated success in a wide variety of tasks \citep{zhou2020graph, wu2020comprehensive, Bapst2020UnveilingTP, Schtt2017SchNetAC, Klicpera2020Dimenet++}, it has been proposed that in practice ``oversmoothing'' limits their ability to benefit from overparametrization.

Oversmoothing is a phenomenon where a GNN's latent node representations become increasing indistinguishable over successive steps of message passing~\citep{Chen2019Oversmoothing}. Once these representations are oversmoothed, the relational structure of the representation is lost, and further message-passing cannot improve expressive capacity. We argue that the challenges of overcoming oversmoothing are two fold. First, finding a way to encourage node latent diversity; second, to encourage the diverse node latents to encode meaningful graph representations. Here we propose a simple noise regulariser, Noisy Nodes, and demonstrate how it overcomes these challenges across a range of datasets and architectures, achieving top results on OC20 IS2RS \& IS2RE direct, QM9 and OGBG-PCQM4Mv1.

Our ``Noisy Nodes'' method is a simple technique for regularising GNNs and associated training procedures.
During training, our noise regularisation approach corrupts the input graph's attributes with noise, and adds a per-node noise correction term. We posit that our Noisy Nodes approach is effective because the model is rewarded for maintaining and refining distinct node representations through message passing to the final output, which causes it to resist oversmoothing. Like denoising autoencoders, it encourages the model to explicitly learn the manifold on which the uncorrupted input graph's features lie, analogous to a form of representation learning. When applied to 3D molecular prediction tasks, it encourages the model to distinguish between low and high energy states. We find that applying Noisy Nodes reduces oversmoothing for shallower networks, and allows us to see improvements with added depth, even on tasks for which depth was assumed to be unhelpful.

This study's approach is to investigate the combination of Noisy Nodes with generic, popular baseline GNN architectures. For 3D Molecular prediction we use a standard architecture working on 3D point clouds developed for particle fluid simulations, the Graph Net Simulator (GNS) \citep{pmlr-v119-sanchez-gonzalez20a}, which has also been used for molecular property prediction \citep{Hu2021ForceNetAG}. Without using Noisy Nodes the GNS is not a competitive model, but using Noisy Nodes allows the GNS to achieve top performance on three 3D molecular property prediction tasks: the OC20 IS2RE direct task by 43\% over previous work, 12\% on OC20 IS2RS direct, and top results on 3 out of 12 of the QM9 tasks. For non-spatial GNN benchmarks we test a MPNN \citep{Gilmer2017NeuralMP} on OGBG-MOLPCBA and OGBG-PCQM4M \citep{hu2021ogblsc} and again see significant improvements. Finally, we applied Noisy Nodes to a GCN \citep{Kipf2016GCN}, arguably the most popular and simple GNN, trained on OGBN-Arxiv and see similar results. These results suggest Noisy Nodes can serve as a complementary GNN building block.

\section{Preliminaries: Graph Prediction Problem}

Let  be an input graph. The nodes are , where . The directed, attributed edges are : each edge includes a sender node index, receiver node index, and edge attribute, , respectively, where  and . The graph-level property is . 

The goal is to predict a target graph, , with the same structure as , but different node, edge, and/or graph-level attributes. We denote  as a model's prediction of . Some error metric defines quality of  with respect to the target , , which the training loss terms are defined to optimize. In this paper the phrase ``message passing steps'' is synonymous with ``GNN layers''. 

\section{Oversmoothing}

``Oversmoothing'' is when the node latent vectors of a GNN become very similar after successive layers of message passing. Once nodes are identical there is no relational information contained in the nodes, and no higher-order latent graph representations can be learned. It is easiest to see this effect with the update function of a Graph Convolutional Network with no adjacency normalization  with  and  the layer index. As the number of applications increases, the averaging effect of the summation forces the nodes to become almost identical. However, as soon as residual connections are added we can construct a network that need not suffer from oversmoothing by setting the residual updates to zero at a  similarity threshold. Similarly, multi-head attention \cite{Vaswani2017AttentionIA, Velickovic2018graph} and GNNs with edge updates \citep{Battaglia2018RelationalIB, Gilmer2017NeuralMP} can modulate node updates. As such for modern GNNs oversmoothing is primarily a ``training'' problem - i.e. how to choose model architectures and regularisers to encourage and preserve meaningful latent relational representations.

We can discern two desiderata for a regulariser or loss that addresses oversmoothing. First, it should penalise identical node latents. Second, it should encourage meaningful latent representations of the data. One such example may be the auto-regressive loss of transformer based language models (\cite{Brown2020LanguageMA}). In this case, each word (equivalent to node) prediction must be distinct, and the auto-regressive loss encourages relational dependence upon prior words. We can take inspiration from this observation to derive auxiliary losses that both have diverse node targets and encourage relational representation learning. In the following section we derive one such regulariser, Noisy Nodes.


\section{Noisy Nodes}

Noisy Nodes tackles the oversmoothing problem by adding a diverse noise correction target, modifying the original graph prediction problem definition in several ways. It introduces a graph corrupted by noise, , where  is constructed by adding noise, , to the input nodes, . The edges, , and graph-level attribute, , can either be uncorrupted by noise (i.e., , ), calculated from the noisy nodes (for example in a nearest neighbors graph), or corrupted independent of the nodes---these are minor choices that can be informed by the specific problem setting.

Our method requires a noise correction target to prevent oversmoothing by enforcing diversity in the last layers of the GNN, which can be achieved with an auxiliary denoising autoencoder loss. For example, where the  is defined with respect to graph-level predictions (e.g., predict the minimum energy value of some molecular system), a second output head can be added to the GNN architecture which requires denoising the inputs as targets. Alternatively, if the inputs and targets are in the same real domain as is the case for physical simulations we can adjust the target for the noise. Figure \ref{fig:noise_diagram} demonstrates this Noisy Nodes set up. The auxiliary loss is weighted by a constant coefficient . 

In Figure \ref{fig:mad_chart} we illustrate the impact of Noisy Nodes on oversmoothing by plotting the Mean Absolute Distance (MAD) \citep{Chen2020MeasuringAR} of the residual updates of each layer of an MPNN trained on the QM9 \citep{Ramakrishnan2014QuantumCS} dataset, and compare it to alternative methods DropEdge \citep{RongDropEdge2019} and DropNode \citep{Do2021GraphCN}. MAD is a measure of the diversity of graph node features, often used to quantify oversmoothing, the higher the number the more diverse the node features, the lower the number the less diverse. In this plot we can see that for Noisy Nodes the node updates remain diverse for all of the layers, whereas without Noisy Nodes diversity is lost. Further analysis of MAD across seeds and with sorted layers can be seen in Appendix Figures \ref{fig:seed_mad} and \ref{fig:unsorted_mad} for models applied to 3D point clouds.

\textbf{The Graph Manifold Learning Perspective.} By using an implicit mapping from corrupted data to clean data, the Noisy Nodes objective encourages the model to learn the manifold on which the clean data lies--- we speculate that the GNN learns to go from low probability graphs to high probability graphs. In the autoencoder case the GNN learns the manifold of the input data. When node targets are provided, the GNN learns the manifold of the target data (e.g. the manifold of atoms at equilibrium). We speculate that such a manifold may include commonly repeated substructures that are useful for downstream prediction tasks. A similar motivation can be found for denoising in \citep{Vincent2010StackedDA, Song2019GenerativeMB}.

\textbf{The Energy Perspective for Molecular Property Prediction.} Local, random distortions of the geometry of a molecule at a local energy minimum are almost certainly higher energy configurations. As such, a task that maps from a noised molecule to a local energy minimum is learning a mapping from high energy to low energy. Data such as QM9 contains molecules at local minima.

Some problems have input data that is already high energy, and targets that are at equilibrium. For these datasets we can generate new high energy states by adding noise to the inputs but keeping the equilibrium target the same,  Figure \ref{fig:noise_diagram} demonstrates this approach. To preserve translation invariance we use displacements between input and target , the corrected target after noise is .

\begin{figure}[]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/diagram}
        \setlength{\belowcaptionskip}{-20pt}
        \caption{Noisy Node mechanics during training. Input positions are corrupted with noise , and the training objective is the node-level difference between target positions and the noisy inputs.}
        \label{fig:noise_diagram}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/oversmoothing_comparison}
        \setlength{\belowcaptionskip}{-20pt}
        \caption{Per layer node latent diversity, measured by MAD on a 16 layer MPNN trained on OGBG-MOLPCBA. Noisy Nodes maintains a higher level of diversity throughout the network than competing methods.}
    \label{fig:mad_chart}
    \end{minipage}
    
\end{figure}

\section{Related Work}



\textbf{Oversmoothing.} Recent work has aimed to understand why it is challenging to realise the benefits of training deeper GNNs \citep{wu2020comprehensive}. Since first being noted in (\citep{li2018deeper}) oversmoothing has been studied extensively and regularisation techniques have been suggested to overcome it \citep{Chen2019Oversmoothing, Cai2020Oversmoothing, RongDropEdge2019, ZhouNodeNorm2020, yang2020revisiting, Do2021GraphCN, Zhao2020PairNormTO}. A recent paper, \citep{Li1000Layers2021}, finds, as in previous work, \citep{Li2019DeepGCNsCG, Li2020DeeperGCNAY}, the optimal depth for some datasets they evaluate on to be far lower (5 for OGBN-Arxiv from the Open Graph Benchmark \citep{Hu2020OpenGB}, for example) than the 1000 layers possible.

\textbf{Denoising \& Noise Models.} Training neural networks with noise has a long history \citep{Sietsma1991CreatingAN, Bishop1995TrainingWN}. Of particular relevance are Denoising Autoencoders \citep{Vincent2008ExtractingAC} in which an autoencoder is trained to map corrupted inputs  to uncorrupted inputs . Denoising Autoencoders have found particular success as a form of pre-training for representation learning \citep{Vincent2010StackedDA}. More recently, in research applying GNNs to simulation  \citep{SanchezGonzalez2018GraphNA, pmlr-v119-sanchez-gonzalez20a, Pfaff2020LearningMS} Gaussian noise is added during training to input positions of a ground truth simulator to mimic the distribution of errors of the learned simulator. Pre-training methods \citep{Devlin2019BERTPO, You2020GraphCL, Thakoor2021BootstrappedRL} are another similar approach; most similarly to our method \cite{Hu2020StrategiesFP} apply a reconstruction loss to graphs with masked nodes to generate graph embeddings for use in downstream tasks. FLAG \citep{Kong2020FLAGAD} adds adversarial noise during training to input node features as a form of data augmentation for GNNs that demonstrates improved performance for many tasks. It does not add an additional auxiliary loss, which we find is essential for addressing oversmoothing. In other related GNN work, \citep{Sato2021RandomFS} use random input features to improve generalisation of graph neaural networks. Adding noise to help input node disambiguation has also been covered in \citep{Dasoulas2019ColoringGN, Loukas2020HowHI, Vignac2020BuildingPA, Murphy2019RelationalPF}, but there is no auxiliary loss. 

Finally, we take inspiration from \citep{Vincent2008ExtractingAC, Vincent2010StackedDA, Vincent2011ACB, Song2019GenerativeMB} which use the observation that noised data lies off the data manifold for representation learning and generative modelling.

\textbf{Machine Learning for 3D Molecular Property Prediction.} One application of GNNs is to speed up quantum chemistry calculations which operate on 3D positions of a molecule \citep{Duvenaud2015ConvFingerprints, Gilmer2017NeuralMP, Schtt2017SchNetAC, Hu2021ForceNetAG}. Common goals are the prediction of molecular properties  \citep{Ramakrishnan2014QuantumCS}, forces \citep{Chmiela2017MachineLO}, energies \citep{Chanussot2020TheOC} and charges \citep{Unke_2019}.

A common approach to embed physical symmetries is to design a network that predicts a rotation and translation invariant energy \citep{Schtt2017SchNetAC, Klicpera2020Dimenet++, liu2021spherical}. The input features of such models include distances \citep{Schtt2017SchNetAC}, angles \citep{, Klicpera2020DirectionalMP, Klicpera2020Dimenet++} or torsions and higher order terms \citep{liu2021spherical}. An alternative approach to embedding symmetries is to design a rotation equivariant neural network that use equivariant representations \citep{Thomas2018TensorField, koehler2019equivariant, Kondor2018Covariant, Fuchs2020SE3Transformers3R, Batzner2021SE3EquivariantGN, Anderson2019CormorantCM, satorras2021en}.

\textbf{Machine Learning for Bond and Atom Molecular Graphs}. Predicting properties from molecular graphs without 3D points, such as graphs of bonds and atoms, is studied separately and often used to benchmark generic graph property prediction models such as GCNs \citep{Hu2020OpenGB} or GATs \citep{Velickovic2018graph}. Models developed for 3D molecular property prediction cannot be applied to bond and atom graphs. Common datasets that contain such data are OGBG-MOLPCBA and OGBG-MOLHIV. 


\section{3D Molecular Property Prediction Experiments and Results}\label{evaluation}

In this section we evaluate how a popular, simple model, the GNS \citep{pmlr-v119-sanchez-gonzalez20a} performs on 3D molecular prediction tasks when combined with Noisy Nodes. The GNS was originally developed for particle fluid simulations, but has recently been adapted for molecular property prediction \citep{Hu2021ForceNetAG}. We find that Without Noisy Nodes the GNS architecture is not competitive, but by using Noisy Nodes we see improved performance comparable to the use of specialised architectures.

We made minor changes to the GNS architecture. We featurise the distance input features using radial basis functions. We group layer weights, similar to grouped layers used in \cite{Jumper2021HighlyAP} for reduced parameter counts; for a group size of  the first  layer weights are repeated, i.e. the first layer with a group size of 10 has the same weights as the , ,  layers and so on.  contiguous blocks of layers are considered a single group. Finally we find that decoding the intermediate latents and adding a loss after each group aids training stability. The decoder is shared across groups.

We tested this architecture on three challenging molecular property prediction benchmarks:  OC20~\citep{Chanussot2020TheOC} IS2RS \& IS2RE, and QM9~\citep{Ramakrishnan2014QuantumCS}. These benchmarks are detailed below, but as general distinctions, OC20 tasks use graphs 2-20x larger than QM9. While QM9 always requires graph-level prediction, one of OC20's two tasks (IS2RS) requires node-level predictions while the other (IS2RE) requires graph-level predictions. All training details may be found in the Appendix.

\subsection{Open Catalyst 2020}\label{eval-oc}

\begin{figure}[]
    \centering
    \includegraphics[width=10.3cm]{mp_figure}
    \caption{Validation curves, OC20 IS2RE ID. \textbf{A)} Without any node targets our model has poor performance and realises no benefit from depth. \textbf{B)} After adding a position node loss, performance improves as depth increases. \textbf{C)} As we add Noisy Nodes and parameters the model achieves SOTA, even with 3 layers, and stops overfitting. \textbf{D)} Adding Noisy Nodes allows a model with even fully shared weights to achieve SOTA.}
    \label{fig:experimental_results}
\end{figure}


\textbf{Dataset.} The \href{https://opencatalystproject.org/}{OC20} dataset \citep{Chanussot2020TheOC} (CC Attribution 4.0) describes the interaction of a small molecule (the adsorbate) and a large slab (the catalyst), with total systems consisting of 20-200 atoms simulated until equilibrium is reached.

We focus on two tasks; the Initial Structure to Resulting Energy (IS2RE) task which takes the initial structure of the simulation and predicts the final energy, and the Initial Structure to Resulting Structure (IS2RS) which takes the initial structure and predicts the relaxed structure.  Note that we train the more common ``direct'' prediction task that map directly from initial positions to target in a single forward pass, and compare against other models trained for direct prediction.

Models are evaluated on 4 held out test sets. Four canonical validation datasets are also provided. Test sets are evaluated on a remote server hosted by the dataset authors with a very limited number of submissions per team.

Noisy Nodes in this case consists of a random jump between the initial position and relaxed position. During training we first sample uniformly from a point in the relaxation trajectory or interpolate uniformly between the initial and final positions , and then add I.I.D Gaussian noise with mean zero and . The Noisy Node target is the relaxed structure.

We first convert to fractional coordinates (i.e. use the periodic unit cell as the basis) which render the predictions of our model invariant to rotations, and append the following rotation and translation invariant vector  to the edge features where  are vectors of the unit cell. This additional vector provides rotation invariant angular and extent information to the .

\textbf{IS2RE Results.} In Figure \ref{fig:experimental_results} we show how using Noisy Nodes allows the GNS to achieve state of the art performance. Figure \ref{fig:experimental_results} A shows that without any auxiliary node target, an IS2RE GNS achieves poor performance even with increased depth. The fact that increased depth does not result in improvement supports the hypothesis that GNS suffers from oversmoothing. As we add a node level position target in B) we see better performance, and improvement as depth increases, validating our hypothesis that node level targets are key to addressing oversmoothing. In C) we add noisy nodes and parameters, and see that the increased diversity of the node level predictions leads to very significant improvements and SOTA, even for a shallow 3 layer network. D) demonstrates this effect is not just due to increased parameters - SOTA can still be achieve with shared layer weights .

In Table \ref{table_2_our_performance_energy_validation} we conduct an ablation on our hyperparameters, and again demonstrate the improved performance of using Noisy Nodes. Results were averaged over 3 seeds and standard errors on the best obtained checkpoint show little sensitivity to initialisation. All results in the table are reported using sampling states from trajectories. We conducted an ablation on ID comparing sampling from a relaxation trajectory and interpolating between initial \& final positions which found that interpolation improved our score from 0.47 to 0.45.


\begin{table}[]
    \captionsetup{width=13cm}
    \captionsetup{justification=centering}
    \caption{OC20 ISRE Validation, eV MAE,  . \\``GNS-Shared'' indicates shared weights. ``GNS-10'' indicates a group size of 10. }
    \label{table_2_our_performance_energy_validation}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}
      \toprule
      Model & Layers & OOD Both & OOD Adsorbate & OOD Catalyst & ID \\
      \midrule
      GNS & 50  & 0.59  & 0.65  & 0.55  & 0.54  \\
      GNS-Shared + Noisy Nodes & 50  & 0.49  & 0.54 & 0.51  & 0.51  \\
      GNS + Noisy Nodes & 50 &  0.48  & 0.53  & 0.49 & 0.48  \\
      GNS-10 + Noisy Nodes & 100 & \textbf{0.46} & \textbf{0.51 } & \textbf{0.48 } & \textbf{0.47 } \\  
      \bottomrule
\end{tabular}}
\end{table}

Our best hyperparameter setting was 100 layers which achieved a 95.6\% relative performance improvement against SOTA results (Table \ref{is2re_test}) on the AEwT benchmark. Due to limited permitted test submissions, results presented here were from one test upload of our best performing validation seed.

\begin{table}
\caption{Results OC20 IS2RE Test}
    \label{is2re_test}
    \centering
    \begin{tabular}{lccccc}
      \toprule
            &  \multicolumn{4}{c}{eV MAE } \\
      \midrule
        & SchNet & DimeNet++ & SpinConv &  SphereNet & GNS + Noisy Nodes\\
      \midrule
      OOD Both & 0.704 & 0.661 & 0.674 & 0.638 & \textbf{0.465 (-24.0\%)} \\ 
      OOD Adsorbate & 0.734 & 0.725 & 0.723 & 0.703  & \textbf{0.565 (-22.8\%)}\\
      OOD Catalyst & 0.662 & 0.576 & 0.569 & 0.571  & \textbf{0.437 (-17.2\%)}\\
      ID & 0.639 & 0.562 & 0.558 & 0.563  & \textbf{0.422 (-18.8\%)}\\
      \midrule
          &  \multicolumn{4}{c}{Average Energy within Threshold (AEwT) } \\
      \midrule
        & SchNet & DimeNet++ & SpinConv & SphereNet  & GNS + Noisy Nodes\\
      \midrule
      OOD Both & 0.0221 & 0.0241 & 0.0233 & 0.0241  & \textbf{0.047 (+95.8\%)} \\ 
      OOD Adsorbate & 0.0233 & 0.0207 & 0.026 & 0.0229  & \textbf{0.035 (+89.5\%)}\\
      OOD Catalyst & 0.0294 & 0.0410 & 0.0382 & 0.0409  & \textbf{0.080 (+95.1\%)}\\
      ID & 0.0296 & 0.0425 & 0.0408 & 0.0447 & \textbf{0.091 (+102.0\%)}\\
      \bottomrule
\end{tabular}
\end{table}

\textbf{IS2RS Results.}  In Table \ref{tab:positions_test} we see that GNS + Noisy Nodes is significantly better than the only other reported IS2RS direct result, ForceNet, itself a GNS variant.


\begin{table}
\captionsetup{justification=centering}
\caption{OC20 IS2RS Validation, ADwT, }
\label{table_2_our_performance_positions_validation}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
  \toprule
  Model &  Layers & OOD Both & OOD Adsorbate & OOD Catalyst & ID \\
  \midrule
  GNS & 50 & 43.0\% & 38.0\% & 37.5\%  & 40.0\% \\  
  GNS + Noisy Nodes & 50 &  50.1\% & 44.3\% & 44.1\% & 46.1\%  \\
  GNS-10 + Noisy Nodes  & 50 & 52.0\% & 46.2\% & 46.1\%  & 48.3\%  \\
  GNS-10 + Noisy Nodes + Pos only & 100 & \textbf{54.3}\% & \textbf{48.3}\% & \textbf{48.2}\%  & \textbf{50.0}\%  \\    
  \bottomrule
\end{tabular}}
\end{table}



\begin{table}
\caption{OC20 IS2RS Test, ADwT, }
\label{tab:positions_test}
\centering
\begin{tabular}{llcccc}
  \toprule
  Model & OOD Both & OOD Adsorbate & OOD Catalyst & ID \\
  \midrule
  ForceNet & 46.9\% & 37.7\% & 43.7\% & 44.9\% \\
  
  GNS + Noisy Nodes & \textbf{52.7\%} & \textbf{43.9\%} & \textbf{48.4\%} & \textbf{50.9\%} \\
  \midrule
  Relative Improvement & \textbf{+12.4\%} & \textbf{+16.4\%} & \textbf{+10.7\%} & \textbf{+13.3\%} \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{QM9}\label{eval-qm9}
\textbf{Dataset.} The QM9 benchmark \citep{Ramakrishnan2014QuantumCS} contains 134k molecules in equilibrium with up to 9 heavy C, O, N and F atoms, targeting 12 associated chemical properties (License: CCBY 4.0). We use 114k molecules for training, 10k for validation and 10k for test. All results are on the test set. We subtract a fixed per atom energy from the target values computed from linear regression to reduce variance.  We perform training in eV units for energetic targets, and evaluate using MAE. We summarise the results across the targets using mean standardised MAE (std. MAE) in which MAEs are normalised by their standard deviation, and mean standardised logMAE. Std. MAE is dominated by targets with high relative error such as , whereas logMAE is sensitive to outliers such as . As is standard for this dataset, a model is trained separately for each target.

\begin{table}
\caption{QM9, Impact of Noisy Nodes on GNS architecture.}
\centering
\begin{tabular}{lcccccc}
\toprule
    & Layers & std. MAE & \% Change & logMAE & \\
\midrule
GNS & 10  & 1.17 & - & -5.39 \\
GNS + Noise But No Node Target & 10 & 1.16 & -0.9\% & -5.32\\
GNS + Noisy Nodes & 10 & 0.90 & -23.1\% & -5.58\\
GNS-10 + Noisy Nodes & 20 & 0.89 & -23.9\% & -5.59\\
GNS-10 + Noisy Nodes + Invariance & 30 & 0.92 & -21.4\% & -5.57 \\
GNS-10 + Noisy Nodes & 30 & \textbf{0.88} & \textbf{-24.8\%} & \textbf{-5.60}\\
\bottomrule
\end{tabular}
\label{tab:qm9_comparison}
\end{table}

\begin{table}[h]
\caption{QM9, Test MAE, Mean \& Standard Deviation of 3 Seeds Reported.}
\label{tab:qm9_results}
\centering
\begin{tabular}{llcccccc}
\toprule
Target &                                                                   Unit  & SchNet & E(n)GNN & DimeNet++ & SphereNet & PaiNN &  \textbf{GNS + Noisy Nodes} \\
\midrule
                        &  \si{\debye} & 0.033 & 0.029 & 0.030 & 0.027  & \textbf{0.012} & 0.025  \\
                     & \si{\bohr^3} & 0.235 & 0.071 & \textbf{0.043} & 0.047  & 0.045 & 0.052 \\
       & \si{\milli\electronvolt} & 41 & 29.0 & 24.6 & 23.6 & 27.6 & \textbf{20.4} \\
       & \si{\milli\electronvolt} & 34 & 25.0 & 19.5 & 18.9 & 20.4 & \textbf{18.6} \\
             & \si{\milli\electronvolt} & 63 & 48.0 & 32.6 & 32.3 & 45.7 & \textbf{28.6}  \\
         & \si{\bohr^2} & \textbf{0.07} & 0.11 & 0.33 & 0.29 & 0.07 & 0.70 \\
ZPVE                         & \si{\milli\electronvolt} & 1.7 & 1.55 & 1.21 & \textbf{1.12} & 1.28 & 1.16  \\
                        & \si{\milli\electronvolt} & 14.00 & 11.00  & 6.32 & 6.26 & \textbf{5.85} & 7.30  \\
                          & \si{\milli\electronvolt} & 19.00 & 12.00 & 6.28 & 7.33  & \textbf{5.83} & 7.57  \\
                          & \si{\milli\electronvolt} & 14.00 & 12.00 & 6.53 & 6.40 & \textbf{5.98} & 7.43\\
                          & \si{\milli\electronvolt} & 14.00 & 12.00 & 7.56 & 8.0 & \textbf{7.35} & 8.30 \\
 \vspace{1pt}    &  \si[per-mode=fraction]{\cal\per\mol\per\kelvin}& 0.033 & 0.031 & 0.023 & \textbf{0.022} & 0.024 & 0.025  \\
\midrule
std. MAE & \si{\percent} & 1.76 & 1.22 & 0.98 & 0.94  & 1.00 & \textbf{0.88}\\
logMAE  &                & -5.17 & -5.43 & -5.67 & -5.68 & \textbf{-5.85} & -5.60 \\
\bottomrule
\end{tabular}
\end{table}

For this dataset we add I.I.D Gaussian noise with mean zero and  to the input atom positions. A denoising autoencoder loss is used.

\textbf{Results} In Table \ref{tab:qm9_results} we can see that adding Noisy Nodes significantly improves results by 23.1\% relative for GNS, making it competitive with specialised architectures. To understand the effect of adding a denoising loss, we tried just adding noise and found no where near the same improvement (Table \ref{tab:qm9_results}).

A GNS-10 + Noisy Nodes with 30 layers achieves top results on 3 of the 12 targets and comparable performance on the remainder (Table \ref{tab:qm9_results}). On the std. MAE aggregate metric GNS + Noisy Nodes performs better than all other reported results, showing that Noisy Nodes can make even a generic model competitive with models hand-crafted for molecular property prediction. The same trend is repeated for an rotation invariant version of this network that uses the principle axes of inertia ordered by eigenvalue as the co-ordinate frame (Table \ref{tab:qm9_comparison}).

, the electronic spatial extent, is an outlier for GNS + Noisy Nodes. Interestingly, we found that without noise GNS-10 + Noisy Nodes achieves 0.33 for this target. We speculate that this target is particularly sensitive to noise, and the best noise value for this target would be significantly lower than for the dataset as a whole.

\section{Non-Spatial Tasks}

The previous experiments use the 3D geometries of atoms, and models that operate on 3D points. However, the recipe of adding a denoising auxiliary loss can be applied to other graphs with different types of features. In this section we apply Noisy Nodes to additional datasets with no 3D points, using different GNNs, and show analagous effects to the 3D case. Details of the hyperparameters, models and training details can be found in the appendix.

\subsection{OGBG-PCQM4M}

This dataset from the OGB benchmarks consists of molecular graphs which consist of bonds and atom types, and no 3D or 2D coordinates. To adapt Noisy Nodes to this setting, we randomly flip node and edge features at a rate of 5\% and add a reconstruction loss. We evaluate Noisy Nodes using an MPNN  + Virtual Node \citep{Gilmer2017NeuralMP}. The test set is not currently available for this dataset.

\begin{table}[]
    \centering
    \caption{OGBG-PCQM4M Results}
    \begin{tabular}{cccc}
    \toprule
         Model & Number of Layers & Using Noisy Nodes & MAE  \\
     \midrule
         MPNN + Virtual Node & 16 & Yes & 0.1249  0.0003 \\
         MPNN + Virtual Node & 50 & No & 0.1236  0.0001 \\
         Graphormer \citep{Ying2021DoTR} & - & - & 0.1234 \\
         MPNN + Virtual Node & 50 & Yes & 	\textbf{0.1218  0.0001} \\
     \bottomrule
    \end{tabular}
    
    \label{tab:pcqm4m}
\end{table}

In Table \ref{tab:pcqm4m} we see that for this task Noisy Nodes enables a 50 layer MPNN to reach state of the art results. Before adding Noisy Nodes, adding capacity beyond 16 layers did not improve results.

\subsection{OGBG-MOLPCBA}

The OGBG-MOLPCBA dataset contains molecular graphs with no 3D points, with the goal of classifying 128 biological activities. On the OGBG-MOLPCBA dataset we again use an MPNN + Virtual Node and random flipping noise. In Figure \ref{fig:ogbg-molpcba} we see that adding Noisy Nodes improves the performance of the base model, accentuated for deeper networks. Our 16 layer MPNN improved from 27.6\%  0.004 to 28.1\%  0.002 Mean Average Precision (``Mean AP''). Figure \ref{fig:molpcba_curve} demonstrates how Noisy Nodes improves performance during training. Of the reported results, our MPNN is most similar to GCN\footnote{The GCN implemented in the official OGB code base has explicit edge updates, akin to the MPNN.} + Virtual Node and GIN + Virtual Node \citep{Xu2018Gin} which report results of 24.2\%  0.003 and 27.03\%  0.003 respectively. We evaluate alternative methods for oversmoothing, DropNode  and DropEdge in Figure \ref{fig:mad_chart} and find that Noisy Nodes is more effective at address oversmoothing, although all 3 methods can be combined favourably (results in appendix).

\begin{figure}[]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/ogbg-molpcba-MP-Steps}
        \caption{Adding Noisy Nodes with random flipping of input categories improves the performance of MPNNs, and the effect is accentuated with depth.}
        \label{fig:ogbg-molpcba}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{images/ogbg-molpcba-val-curve}
        \caption{Validation curve comparing with and without noisy nodes. Using Noisy Nodes leads to a consistent improvement.}
    \label{fig:molpcba_curve}
    \end{minipage}
\end{figure}



\subsection{OGBN-ARXIV}

The above results use models with explicit edge updates, and are reported for graph prediction. To test the effectiveness with Noisy Nodes with GCNs, arguably the simplest and most popular GNN, we use OGBN-ARXIV, a citation network with the goal of predicting the arxiv category of each paper. Adding Noisy Nodes, with noise as input dropout of 0.1, to 4 layer GCN with residual connections improves from 72.39\%  0.002 accuracy to 72.52\%  0.003 accuracy. A baseline 4 layer GCN on this dataset reports 71.71\%  0.002. The SOTA for this dataset is 74.31\% \citep{Sun2020AdaptiveGD}. 

\subsection{Limitations}\label{limitations}

We have not demonstrated the effectiveness of Noisy Nodes in small data regimes, which may be important for learning from experimental data. The representation learning perspective requires access to a local minimum configuration, which is not the case for all quantum modeling datasets. We have also not demonstrated the combination of Noisy Nodes with more sophisticated 3D molecular property prediction models such as DimeNet++\citep{Klicpera2020Dimenet++}, such models may require an alternative reconstruction loss to position change, such as pairwise interatomic distances. We leave this to future work.

Noisy Nodes requires careful selection of the form of noise, and a balance between the auxiliary and primary losses. This can require hyper parameter tuning, and models can be sensitive to the choice of these parameters. Noisy Nodes has a particular effect for deep GNNs, but depth is not always an advantage. There are situations, for example molecular dynamics, which place a premium on very fast inference time. However even at 3 layers (a comparable depth to alternative architectures) the GNS architecture achieves state of the art validation OC20 IS2RE predictions (Figure \ref{fig:experimental_results}). Finally, returns diminish as depth increases indicating depth is not the only answer (Table \ref{table_2_our_performance_energy_validation}).

\section{Conclusions}
In this work we present Noisy Nodes, a novel regularisation technique for GNNs with particular focus on 3D molecular property prediction. Noisy nodes helps address common challenges around oversmoothed node representations, shows benefits for GNNs of all depths, but in particular improves performance for deeper GNNs. We demonstrate results on challenging 3D molecular property prediction tasks, and some generic GNN benchmark datasets. We believe these results demonstrate Noisy Nodes could be a useful building block for GNNs for molecular property prediction and beyond.



\section{Reproducibility statement}

Code for reproducing OGB-PCQM4M results using Noisy Nodes is available on github, and was prepared as part of a leaderboard submission. \url{https://github.com/deepmind/deepmind-research/tree/master/ogb_lsc/pcq}.

We provide detailed hyper parameter settings for all our experiments in the appendix, in addition to formulae for computing the encoder and decoder stages of the GNS.

\section{Ethics statement}
\textbf{Who may benefit from this work?} Molecular property prediction with GNNs is a fast-growing area with applications across domains such as drug design, catalyst discovery, synthetic biology, and chemical engineering. Noisy Nodes could aid models applied to these domains. We also demonstrate on OC20 that our direct state prediction approach is nearly as accurate as learned relaxed approaches at a small fraction of the computational cost, which may support material design which requires many predictions.

Finally, Noisy Nodes could be adapted and applied to many areas in which GNNs are used---for example, knowledge base completion, physical simulation or traffic prediction.

\textbf{Potential negative impact and reflection.} Noisy Nodes sees improved performance from depth, but the training of very deep GNNs could contribute to global warming. Care should be taken when utilising depth, and we note that Noisy Nodes settings can be calibrated at shallow depth.
\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}

\appendix
\section{Appendix}

The following sections include details on training setup, hyper-parameters, input processing, as well as additional experimental results.

\subsection{Additional Metrics for Open Catalyst IS2RS Test Set}

Relaxation approaches to IS2RS minimise forces with respect to positions, with the expectation that forces at the minimum are close to zero. One metric of such a model's success is to evaluate the forces at the converged structure using ground truth Density Functional Theory calculations and see how close they are to zero. Two metrics are provided by OC20 \citep{Chanussot2020TheOC} on the IS2RS test set: Force below Threshold (FbT), which is the percentage of structures that have forces below 0.05 eV/Angstrom, and Average Force below Threshold (AFbT) which is FbT calculated at multiple thresholds.

The OC20 project computes test DFT calculations on the evaluation server and presents a summary result for all IS2RS position predictions. Such calculations take 10-12 hours and they are not available for the validation set. Thus, we are not able to analyse the results in Tables \ref{tab:IS2RS_force_1} and \ref{tab:IS2RS_force_2}  in any further detail. Before application to catalyst screening further work may be needed for direct approaches to ensure forces do not explode from atoms being too close together.

\begin{table}
\caption{OC20 IS2RS Test, Average Force below Threshold \%, }
\label{tab:IS2RS_force_1}
\centering
\begin{tabular}{llcccc}
  \toprule
  Model & Method & OOD Both & OOD Adsorbate & OOD Catalyst & ID \\
  \midrule
  Noisy Nodes & Direct & 0.09\% & 0.00\% & 0.29\% & 0.54\% \\
  \bottomrule
\end{tabular}
\end{table}

\begin{table}
\caption{OC20 IS2RS Test, Force below Threshold \%, }
\label{tab:IS2RS_force_2}
\centering
\begin{tabular}{llcccc}
  \toprule
  Model & Method & OOD Both & OOD Adsorbate & OOD Catalyst & ID \\
  \midrule
  Noisy Nodes & Direct & 0.0\% & 0.0\% & 0.0\% & 0.0\% \\
  \bottomrule
\end{tabular}
\end{table}

\subsection{More details on GNS adaptations for molecular property prediction.}

\textbf{Encoder.}

The node features are a learned embedding lookup of the atom type, and in the case of OC20 two additional binary features representing whether the atom is part of the adsorbate or catalyst and whether the atom remains fixed during the quantum chemistry simulation.

The edge features,  are the distances  featurised using  Radial Bessel basis functions, , and the edge vector displacements, , normalised by the edge distance:



Our conversion to fractional coordinates only applied to the vector quantities, i.e. .

\textbf{Decoder}

The decoder consists of two parts, a \textit{graph-level decoder} which predicts a single output for the input graph, and a \textit{node-level decoder} which predicts individual outputs for each node. The graph-level decoder implements the following equation:



Where  are node latents from the Processor,  are node latents from the Encoder,  and  are linear layers,  and  are biases, and  is the number of nodes. The node-level decoder is simply an MLP applied to each  which predicts .


\subsection{More details on MPNN for OGBG-PCQM4M and OGBG-MOLPCBA}

Our MPNN follows the blueprint of \cite{Gilmer2017NeuralMP}.  We use  to denote the latent vector of node  at message passing step , and  to be the computed message vector for the edge between nodes  and  at message passing step .  We define the update functions as:



Where the message function  and the update function  are MLPs. We use a ``Virtual Node'' which is connected to all other nodes to enable long range communication. Out readout function is an MLP. No spatial features are used.

\subsection{Experiment setup for 3D Molecular Modeling}
\textbf{Open Catalyst.}
All training experiments were ran on a cluster of TPU devices. For the Open Catalyst experiments, each individual run (i.e. a single random seed) utilised 8 TPU devices on 2 hosts (4 per host) for training, and 4 V100 GPU devices for evaluation (1 per dataset). 

Each Open Catalyst experiment was ran until convergence for up to 200 hours. Our best result, the large 100 layer model requires 7 days of training using the above setting. 
Each configuration was run at least 3 times in this hardware configuration, including all ablation settings. 

We further note that making effective use of our regulariser requires sweeping noise values. These sweeps are dataset dependent and can be carried out using few message passing steps.

\textbf{QM9.}
Experiments were also run on TPU devices. Each seed was run using 8 TPU devices on a single host for training, and 2 V100 GPU devices for evaluation. QM9 targets were trained between 12-24 hours per experiment.

Following \cite{Klicpera2020DirectionalMP} we define std. MAE as :



and logMAE as:



with target index , number of targets , dataset size , ground truth values , model , inputs  and , and standard deviation  of .

\subsection{Over Smoothing Analysis for GNS}

In addition to Figure \ref{fig:mad_chart}, we repeat the analysis with a mean MAD over 3 seeds \ref{fig:seed_mad}. Furthermore we remove the sorting layer by MAD value and find the trend holds.

\begin{figure}[]
    \centering
    \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=0.9\textwidth]{images/unsorted_mad}
            \caption{GNS Unsorted MAD per Layer Averaged Over 3 Random Seeds. Evidence of oversmoothing is clear. Model trained on QM9.}
            \label{fig:unsorted_mad}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
            \includegraphics[width=0.9\textwidth]{images/seed_mad}
            \caption{GNS Sorted MAD per Layer Averaged Over 3 Random Seeds. The trend is clearer when the MAD values have been sorted. Model trained on QM9.}
            \label{fig:seed_mad}
    \end{minipage}
\end{figure}

\subsection{Noise Ablations for OGBG-MOLPCBA}\label{noise-ogbg-molpcba}

We conduct a noise ablation on the random flipping noise for OGBG-MOLPCBA with an 8 layer MPNN + Virtual Node, and find that our model is not very sensitive to the noise value (Table \ref{tab:ogbg-molpcba-noise}), but degrades from 0.1.

\begin{table}[]
    \centering
    \begin{tabular}{c|c}
        \toprule
        Flip Probability & Mean AP \\
        \midrule
        0.01 &  27.8\% +- 0.002 \\
        0.03 &  27.9\% +- 0.003 \\
        0.05 &  +- 0.001 \\
        0.1 & 28.0\% +- 0.003 \\
        0.2 & 27.7\% +- 0.002 \\
        \bottomrule
    \end{tabular}
    \caption{OGBG-MOLPCBA Noise Ablation}
    \label{tab:ogbg-molpcba-noise}
\end{table}

\subsection{DropEdge \& DropNode Ablations for OGBG-MOLPCBA}\label{dropedge-ogbg-molpcba}

We conduct an ablation with our 16 layer MPNN  using DropEdge at a rate of 0.1 as an alternative approach to improving oversmoothing and find it does not improve performance for ogbg-molpcba (Table \ref{tab:ogbg-molpcba-dropedge}), similarly we find DropNode (Table \ref{tab:ogbg-molpcba-dropnode}) does not improve performance. In addition, we find that these two methods can't be combined well together, reaching a performance of 27.0\%  0.003. However, both methods can be combined advantageously with Noisy Nodes.

\begin{table}[]
    \centering
    \begin{tabular}{c|c}
        \toprule
         & Mean AP \\
        \midrule
        MPNN Without DropEdge & 27.4\%  0.002 \\
        MPNN With DropEdge &  27.5\%  0.001 \\
        MPNN + DropEdge + Noisy Nodes &    0.002 \\
        \bottomrule
    \end{tabular}
    \caption{OGBG-MOLPCBA DropEdge Ablation}
    \label{tab:ogbg-molpcba-dropedge}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c}
        \toprule
         & Mean AP \\
        \midrule
        MPNN With DropNode &  27.5\%  0.001 \\
        MPNN Without DropNode & 27.5\%  0.004 \\
        MPNN + DropNode + Noisy Nodes &    0.005 \\
        \bottomrule
    \end{tabular}
    \caption{OGBG-MOLPCBA DropNode Ablation}
    \label{tab:ogbg-molpcba-dropnode}
\end{table}

We also measure the MAD of the node latents for each layer and find the indeed Noisy Nodes is more effective at addressing oversmoothing in Figure \ref{fig:oversmoothing-comparison}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{images/oversmoothing_comparison}
    \caption{Comparison of the effect of techniques to address oversmoothing on MPNNs. Whilst Some effect can be seen from DropEdge and DropNode, Noisy Nodes is significantly better at preserving per node diversity.}
    \label{fig:oversmoothing-comparison}
\end{figure}

\subsection{Training Curves for OC20 Noisy Nodes Ablations Demonstrating Overfitting}\label{train_curves}

Figure \ref{fig:train_curves}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{images/train_curves}
    \caption{Training curves to accompany Figure \ref{fig:experimental_results}. This demonstrates that even as the validation performance is getting worse, training loss is going down, indicating overfitting.}
    \label{fig:train_curves}
\end{figure}

\subsection{Pseudocode for 3D Molecular Prediction Training Step}

\begin{algorithm}[H]
\SetAlgoLined
 =   \tcp{Input graph}
  \tcp{Initialize noisy graph}
  \tcp{Noisy Nodes Weight}
\If{not\_provided()}{  
}
\If{predict\_differences}{
     = 
}
\SetKwFor{Foreach}{for each}{do}{endfor}
\Foreach { }{
     = sample\_node\_noise(shape\_of())\;
     = \;
\If{predict\_differences}{
         = \;
    }
    }
 = recompute\_edges()\;
 = GNN()\;
\If{predict\_differences}{
    \;
}
Loss =  \text{NoisyNodesLoss}(, ) + \text{PrimaryLoss}(, ))\;
Loss.minimise()
\caption{Noisy Nodes Training Step}
\end{algorithm}


\subsection{Training Details}\label{mol-3d-training}

Our code base is implemented in JAX using Haiku and Jraph for GNNs, and Optax for training \citep{jax2018github, deepmind2020jax, jraph2020github, haiku2020github}. Model selection used early stopping. 

All results reported as an average of 10 random seeds. OGBG-PCQM4M \& OGBG-MOLPCBA were trained with 16 TPUs and evaluated with a single V100 GPU. OGBN-Arxiv was trained and evalated with a single TPU

\textbf{3D Molecular Prediction}

We minimise the mean squared error loss on mean and standard deviation normalised targets and use the Adam \citep{Kingma2015AdamAM} optimiser with warmup and cosine decay. For OC20 IS2RE energy prediction we subtract a learned reference energy, computed using an MLP with atom types as input.

For the GNS model the node and edge latents as well as MLP hidden layers were sized 512, with 3 layers per MLP and using shifted softplus activations throughout. OC20 \& QM9 Models were trained on 8 TPU devices and evaluated on a single V100 GPUs. We provide the full set of hyper-parameters and computational resources used separately for each dataset in the Appendix. All noise levels were determined by sweeping a small range of values () informed by the noised feature covariance. 


\textbf{Non Spatial Tasks}



\subsection{Hyper-parameters}
\textbf{Open Catalyst.} We list the hyper-parameters used to train the default Open Catalyst experiment. If not specified otherwise (e.g. in ablations of these parameters), experiments were ran with this configuration.

\begin{table}[]
\caption{Open Catalyst training parameters.}
    \label{oc_params}
    \centering
    \begin{tabular}{ll}
      \toprule
       Parameter & Value or description \\
       \midrule
       Optimiser & Adam with warm up and cosine cycling \\
          &  \\              
          &  \\                     
       Warm up steps    &  \\
       Warm up start learning rate    &  \\       
       Warm up/cosine max learning rate    &  \\       
       Cosine cycle length    &  \\       
       Loss type & Mean squared error \\
     \midrule
       Batch size & Dynamic to max edge/node/graph count \\
       Max nodes in batch & 1024 \\
       Max edges in batch & 12800 \\   
       Max graphs in batch & 10 \\    
     \midrule
       MLP number of layers & 3 \\
       MLP hidden sizes & 512 \\
       Number Bessel Functions & 512 \\
       Activation & shifted softplus \\
       message passing layers & 50 \\
       Group size & 10 \\
       Node/Edge latent vector sizes & 512 \\ 
     \midrule
       Position noise & Gaussian () \\
       Parameter update & Exponentially moving average (EMA) smoothing \\
       EMA decay & 0.9999 \\
       Position Loss Co-efficient & 1.0 \\
    \bottomrule

\end{tabular}

\end{table}

Dynamic batch sizes refers to constructing batches by specifying maximum node, edge and graph counts (as opposed to only graph counts) to better balance computational load. Batches are constructed until one of the limits is reached.

Parameter updates were smoothed using an EMA for the current training step with the current decay value computed through .
As discussed in the evaluation, best results on Open Catalyst were obtained by utilising a 100 layer network with group size 10.

\textbf{QM9}
Table \ref{qm9_params} lists QM9 hyper-parameters which primarily reflect the smaller dataset and geometries with fewer long range interactions. For , ,  and  we use a slightly larger number of graphs per batch - 16 - and a smaller position loss co-efficient of 0.01. 

\begin{table}[]
\caption{QM9 training parameters.}
    \label{qm9_params}
    \centering
    \begin{tabular}{ll}
      \toprule
       Parameter & Value or description \\
       \midrule
       Optimiser & Adam with warm up and cosine cycling \\
          &  \\              
          &  \\                     
       Warm up steps    &  \\
       Warm up start learning rate    &  \\       
       Warm up/cosine max learning rate    &  \\       
       Cosine cycle length    &  \\       
       Loss type & Mean squared error \\
     \midrule
       Batch size & Dynamic to max edge/node/graph count \\
       Max nodes in batch & 256 \\
       Max edges in batch & 4096 \\   
       Max graphs in batch & 8 \\    
     \midrule
       MLP number of layers & 3 \\
       MLP hidden sizes & 1024 \\
       Number Bessel Funtions & 512 \\
       Activation & shifted softplus \\
       message passing layers & 10 \\
       Group Size & 10 \\
       Node/Edge latent vector sizes & 512 \\ 
     \midrule
       Position noise & Gaussian () \\
       Parameter update & Exponentially moving average (EMA) smoothing \\
       EMA decay & 0.9999 \\
       Position Loss Coefficient & 0.1 \\
    \bottomrule

\end{tabular}
\end{table}

\textbf{OGBG-PCQM4M} Table \ref{pcqm4m-params} provides the hyper parameters for OGBG-PCQM4M.
\begin{table}[]
\caption{OGBG-PCQM4M Training Parameters.}
    \label{pcqm4m-params}
    \centering
    \begin{tabular}{ll}
      \toprule
       Parameter & Value or description \\
       \midrule
       Optimiser & Adam with warm up and cosine cycling \\
          &  \\              
          &  \\                     
       Warm up steps    &  \\
       Warm up start learning rate    &  \\       
       Warm up/cosine max learning rate    &  \\       
       Cosine cycle length    &  \\       
       Loss type & Mean absolute error \\
       Reconstruction type & Softmax Cross Entropy \\
     \midrule
       Batch size & Dynamic to max edge/node/graph count \\
       Max nodes in batch & 20,480 \\
       Max edges in batch & 8,192 \\   
       Max graphs in batch & 512 \\    
     \midrule
       MLP number of layers & 2 \\
       MLP hidden sizes & 512 \\
       Activation & relu \\
       Node/Edge latent vector sizes & 512 \\ 
     \midrule
      
       Noisy Nodes Category Flip Fate & 0.05 \\
       Parameter update & Exponentially moving average (EMA) smoothing \\
       EMA decay & 0.999 \\
       Reconstruction Loss Coefficient & 0.1 \\
    \bottomrule

\end{tabular}
\end{table}


\textbf{OGBG-MOLPCBA} Table \ref{molpcba-params} provides the hyper parameters for the OGBG-MOLPCBA experiments.

\begin{table}[]
\caption{OGBG-MOLPCBA Training Parameters.}
    \label{molpcba-params}
    \centering
    \begin{tabular}{ll}
      \toprule
       Parameter & Value or description \\
       \midrule
       Optimiser & Adam with warm up and cosine cycling \\
          &  \\              
          &  \\                     
       Warm up steps    &  \\
       Warm up start learning rate    &  \\       
       Warm up/cosine max learning rate    &  \\       
       Cosine cycle length    &  \\       
       Loss type & Softmax Cross Entropy \\
       Reconstruction loss type & Softmax Cross Entropy \\
     \midrule
       Batch size & Dynamic to max edge/node/graph count \\
       Max nodes in batch & 20,480 \\
       Max edges in batch & 8,192 \\   
       Max graphs in batch & 512 \\    
     \midrule
       MLP number of layers & 2 \\
       MLP hidden sizes & 512 \\
       Activation & relu \\
       Batch Normalization & Yes, after every hidden layer \\
       Node/Edge latent vector sizes & 512 \\ 
     \midrule
       Dropnode Rate & 0.1 \\
       Dropout Rate & 0.1 \\
       Noisy Nodes Category Flip Fate & 0.05 \\
       Parameter update & Exponentially moving average (EMA) smoothing \\
       EMA decay & 0.999 \\
       Reconstruction Loss Coefficient & 0.1 \\
    \bottomrule

\end{tabular}
\end{table}

\textbf{OGBN-ARXIV} Table \ref{arxiv-params} provides the hyper parameters for the OGBN-Arxiv experiments.

\begin{table}[]
\caption{OGBG-ARXIV Training Parameters.}
    \label{arxiv-params}
    \centering
    \begin{tabular}{ll}
      \toprule
       Parameter & Value or description \\
       \midrule
       Optimiser & Adam with warm up and cosine cycling \\
          &  \\              
          &  \\                     
       Warm up steps    &  \\
       Warm up start learning rate    &  \\       
       Warm up/cosine max learning rate    &  \\       
       Cosine cycle length    &  \\       
       Loss type & Softmax Cross Entropy \\
       Reconstruction loss type & Mean Squared Error \\
     \midrule
       Batch size & Full graph \\
     \midrule
       MLP number of layers & 1 \\
       Activation & relu \\
       Batch Normalization & Yes, after every hidden layer \\
       Node/Edge latent vector sizes & 256 \\ 
     \midrule
       Dropout Rate & 0.5 \\
       Noisy Nodes Input Dropout & 0.05 \\
       Reconstruction Loss Coefficient & 0.1 \\
    \bottomrule

\end{tabular}
\end{table}



\end{document}
