
\documentclass{article} \usepackage{iclr2021_conference,times}
\usepackage{subfigure, graphicx}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{changepage}
\usepackage{xcolor}

\usepackage{booktabs}

\usepackage{makecell}

\usepackage{multirow}

\usepackage{hyperref}


\usepackage{geometry}
\usepackage{changepage}
\usepackage{symbols}
\usepackage{placeins}

\usepackage{caption}
\usepackage[labelfont=bf]{caption}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{algpseudocode}
\usepackage{inconsolata}
\usepackage{tabularx}
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\usepackage{ragged2e} 
\usepackage{longtable}



\usepackage{amsmath,amsfonts,bm,amssymb}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}

\newcommand{\ncp}{\text{NCP}}

\iffalse
\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}
\fi

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\renewcommand{\KL}{\text{KL}}
\newcommand{\JSD}{\text{JSD}}
\renewcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\let\ab\allowbreak
 \usepackage{url}
\usepackage{xcolor}
\usepackage{cuted}


\setlength{\belowcaptionskip}{-6pt}

\newcommand{\AV}[1]{{\color{blue}Arash: #1}}
\newcommand{\JK}[1]{{\color{cyan}[JK: #1]}}
\newcommand{\JA}[1]{{\color{red}[Jyoti: #1]}}


\title{NCP-VAE: Variational Autoencoders with \\ Noise Contrastive Priors}



\author{Jyoti Aneja\thanks{Work done during an internship at NVIDIA},  Alexander Schwing \\
University of Illinois, Urbana-Champaign\\
\texttt{\{janeja2,aschwing\}@illinois.edu} \\
\And
Jan Kautz, Arash Vahdat \\
NVIDIA \\
\texttt{\{jkautz,avahdat\}@nvidia.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in various domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.
\end{abstract} \section{Introduction}


\begin{wrapfigure}{r}{0.25\textwidth}
\centering
\vspace{-1.0cm}
\includegraphics[scale=1.0,trim={1cm 15.24cm 24.cm 3.3cm},clip=true]{figs/fig1v2.pdf}
\caption{We propose an EBM prior using the product of a base prior  and a reweighting factor , designed to bring the base prior closer to the aggregate posterior .}
\vspace{-0.2cm}
\label{fig:teaser}
\end{wrapfigure}
Variational autoencoders (VAEs)~\citep{kingma2014vae, rezende2014stochastic} are one of the powerful likelihood-based generative models that have applications in image generation~\citep{brock2018large, karras2019style, razavi2019generating}, music synthesis~\citep{dhariwal2020jukebox}, speech generation~\citep{oord2016wavenet, ping2019waveflow}, image captioning~\citep{aneja2019sequential,deshpande2019fast,aneja2018convolutional}, semi-supervised learning~\citep{kingma2014semi, izmailov2019semi}, and representation learning~\citep{van2017vqvae, fortuin2018som}. Although there has been tremendous progress in improving the expressivity of the approximate posterior, several studies have observed that VAE priors fail to match the \textit{aggregate (approximate) posterior} \citep{rosca2018dmatching, hoffman2016elbo_surgery}. This phenomenon is sometimes described as \textit{holes in the prior}, referring to regions in the latent space that are not decoded to data-like samples. Such regions often have a high density under the prior but have a low density under the aggregate approximate posterior. 

The prior hole problem is commonly tackled by increasing the flexibility of the prior via hierarchical priors \citep{klushyn2019hierarchical}, autoregressive models \citep{gulrajani2016pixelvae}, a mixture of approximate posteriors \citep{tomczak2018VampPrior}, normalizing flows \citep{xu2019priorflow}, resampled priors \citep{bauer2019resampledPrior}, and energy-based models \citep{pang2020EBMPrior, Vahdat2018DVAE++, vahdat2018dvaes, vahdat2019UndirectedPost}. Among them, energy-based models (EBMs) \citep{du2019implicit, pang2020EBMPrior} have shown promising results in learning expressive priors. However, they require running iterative MCMC steps during training which is computationally expensive, especially when the energy function is represented by a neural network. Moreover, they scale poorly to hierarchical models where an EBM is defined on each group of latent variables.

Our key insight in this paper is that a trainable prior is brought as close as possible to the aggregate posterior as a result of training a VAE. The mismatch between the prior and the aggregate posterior can be reduced by simply reweighting the prior to re-adjust its likelihood in the area of mismatch with the aggregate posterior. To represent this reweighting mechanism, we formulate the prior using an EBM that is defined by the product of a reweighting factor and a base trainable prior as shown in Fig.~\ref{fig:teaser}. We represent the reweighting factor using neural networks and the base prior using Normal distributions. 

Instead of expensive MCMC sampling, we use noise contrastive estimation (NCE) \citep{gutmann2010nce} for training the EBM prior. We show that NCE naturally trains the reweighting factor in our prior by learning a binary classifier to distinguish samples from a target distribution (i.e., samples from the approximate posterior) vs.\ samples from a noise distribution (i.e., the base trainable prior). However, since NCE's success depends on how close the noise distribution is to the target distribution, we first train the VAE with the base prior to bring it close to the aggregate posterior. And then, we train the EBM prior using NCE.

In this paper, we make the following contributions: i) We propose an EBM prior termed \textit{noise contrastive prior (NCP)} which is trained by contrasting samples from the aggregate posterior to samples from a base prior. NCPs are learned as a post-training mechanism to replace the original prior with a more flexible prior, which can improve the generative performance of VAEs with any structure. ii)  
We also show how NCPs are trained on hierarchical VAEs with many latent variable groups. We show that training hierarchical NCPs scales easily to many groups, as they are trained for each latent variable group in parallel. iii) Finally, we demonstrate that NCPs improve the generative quality of VAEs by a large margin across datasets.
 \vspace{-0.2cm}
\section{Background}
We first review VAEs, their extension to hierarchical VAEs, and the prior hole problem.

\textbf{Variational Autoencoders:} VAEs learn a generative distribution 
 where  is a prior distribution over the latent variable  and  is a likelihood function that generates the data   given . 
VAEs are trained by maximizing a variational lower bound on the log-likelihood :

where  is an approximate posterior 
and KL is the Kullback–Leibler divergence. The final training objective is formulated by  where  is the data distribution \citep{kingma2014vae}.


\textbf{Hierarchical VAEs (HVAEs):} To increase the expressivity of both prior and approximate posterior, earlier work adapted a hierarchical latent variable structure \citep{vahdat2020NVAE, kingma2016improved, sonderby2016ladder, gregor2016ConvDraw}. In HVAEs, the latent variable  is divided into  separate {\em groups}, . 
The approximate posterior and the prior distributions are then defined by  and . Using these, the training objective becomes:
where  is the approximate posterior up to the  group\footnote{For , the expectation inside the summation is simplified to .}. 

\textbf{The Prior Hole Problem:} Let  denote the aggregate (approximate) posterior. In Appendix~\ref{app:vae_prior}, we show that maximizing   with respect to the prior parameters corresponds to bringing 
the prior as close as possible to the aggregate posterior by minimizing  w.r.t.\ .
Formally, the prior hole problem refers to the phenomenon that  fails to match . 
 \section{Noise Contrastive Priors (NCPs)}

One of the main causes of the prior hole problem is the limited expressivity of the prior that prevents it from matching the aggregate posterior. Recently, energy-based models have shown promising results in representing complex distributions. Motivated by their success, we introduce the noise contrastive prior (NCP) , where  is a base prior distribution, e.g., a Normal,  is a reweighting factor, and  is the normalization constant. The function  maps the latent variable  to a positive scalar, and can be implemented using neural nets. 

The reweighting factor  can be trained using MCMC sampling as discussed in Appendix~\ref{app:mcmc}. However, MCMC requires expensive sampling iterations that scale poorly to hierarchical VAEs. To address this, we describe a noise contrastive estimation based approach to train  without MCMC sampling.

\subsection{Learning The Reweighting Factor with Noise Contrastive Estimation}
Recall that training VAEs closes the gap between the prior and the aggregate posterior by minimizing  with respect to prior. Assuming the base prior  to be fixed,  is zero when . However, since we do not have the density function for , we cannot compute the ratio explicitly. 
Instead, in this paper, we propose to estimate  using noise contrastive estimation \citep{gutmann2010nce}, also known as the likelihood ratio trick that has been popularized in  machine learning  by predictive coding \citep{oord2018cpc} and generative adversarial networks (GANs) \citep{goodfellow_generative_2014}. Since, we can generate samples from both  and \footnote{We  generate samples from the aggregate posterior  via ancestral sampling: draw data from the training set () and then sample from .}, we train a binary classifier to distinguish samples from  and samples from the base prior  by minimizing the binary cross-entropy loss:

Here,  is a binary classifier that generates the classification prediction probabilities. 
Eq.~\eqref{eq:gan_nce} is minimized when . Denoting the classifier at optimality by , we estimate the reweighting factor  . The appealing advantage of this estimator is that it is obtained by simply training a  binary classifier rather than using expensive MCMC sampling. 

\begin{figure*}[t]
\centering
\vspace{-0.2cm}
\includegraphics[width=.86\linewidth,trim={0cm 0cm 0cm 0cm},clip]{figs/ncp_vae_method_4.pdf}
\caption{NCP-VAE is trained in two stages. In the first stage, we train a VAE using the original VAE objective. In the second stage, we train the reweighting factor  using noise contrastive estimation (NCE). NCE trains a classifier to distinguish samples from the prior and samples from the aggregate posterior. Our noise contrastive prior (NCP) is then constructed by the product of the base prior and the reweighting factor, formed via the classifier. At test time, we sample from NCP using SIR or LD. These samples are then passed to the decoder to generate output samples. }
\label{fig:training}
\end{figure*}

\subsection{Two-stage Training for Noise Contrastive Priors}
To properly learn the reweighting factor, NCE training requires the base prior distribution to be close to the target distribution. Intuitively, if  is very close to  (i.e., ), the optimal classifier will have a large loss value in Eq.~\eqref{eq:gan_nce}, and we will have . If  is instead far from , the binary classifier will easily learn to distinguish samples from the two distributions and it will not learn the likelihood ratios correctly. If  is roughly close to , then the binary classifier can learn the ratios.

To ensure that the base prior distribution is close to the target aggregate posterior distribution, we propose a two-stage training algorithm. In the first stage, we train the VAE with only the base prior . From Appendix~\ref{app:vae_prior}, we know that at the end of training,  is as close as possible to . In the second stage, we freeze the VAE model including the approximate posterior , the base prior , and the likelihood , and we only train the reweighting factor  using Eq.~\eqref{eq:gan_nce}. The second stage can be thought of as replacing the base distribution  with a more expressive distribution of the form . Hence, NCP matches the prior to the aggregate posterior  by using the reweighting factors. Note that our proposed method is generic as it only assumes that we can draw samples from  and , which applies to any VAE. Our training is illustrated in Fig.~\ref{fig:training}.


\subsection{Test Time Sampling}
To sample from a VAE with an NCP, we first generate samples from the NCP before passing them to the decoder to generate output samples (shown in Fig.~\ref{fig:training}). We propose two methods for sampling from NCPs.

\textbf{Sampling-Importance-Resampling (SIR):} We first generate  samples from the base prior distribution . We then resample one of the  proposed samples using importance weights proportional to . The benefit of this technique: both proposal generation and the evaluation of  on the samples are done in parallel.


\textbf{Langevin Dynamics (LD):} Since our NCP is an EBM, we can use LD for sampling. Denoting the energy function by , we initialize a sample  by drawing from  and  update the sample iteratively using:  where  and  is the step size. LD is run for a finite number of iterations, and in contrast to SIR, it can be slow given its sequential form. 


\subsection{Generalization to Hierarchical VAEs}
The state-of-the-art VAE~\citep{vahdat2020NVAE} uses a hierarchical  and . Appendix~\ref{app:hvae_prior} shows that training a HVAE encourages the prior to minimize  for each conditional, where  is the aggregate posterior up to the  group, and  is the aggregate conditional for the  group. 
Given this observation, we extend NCPs to hierarchical models to match each conditional in the prior with . Formally, we define hierarchical NCPs by  where each factor is an EBM.  resembles energy machines with an autoregressive structure among  groups~\citep{nash2019autoregressive}.  

In the first stage, we train the HVAE with prior . For the second stage, we use  binary classifiers, each for a hierarchical group. 
Following Appendix~\ref{app:cnce}, we train each classifier via:
where the outer expectation samples from groups up to the  group, and the inner expectations sample from approximate posterior and base prior for the  group, conditioned on the same . The discriminator  classifies samples  while conditioning its prediction on  using a shared context feature . 

The NCE training in Eq.~\eqref{eq:gan_nce_hvae} is minimized when . Denoting the classifier at optimality by , we obtain the reweighting factor   in the second stage. Given our hierarchical NCP, we use ancestral sampling to sample from the prior. For sampling from each group, we can use SIR or LD as discussed before. 

The context feature  extracts a representation from . Instead of learning a new representation at stage two, we simply use the representation that is extracted from  in the hierarchical prior, trained in the first stage. Note that the binary classifiers in the second stage are trained in parallel for all groups. 
 \vspace{-0.1cm}
\section{Related Work}
In this section, we review prior works related to the proposed method.

\textbf{Energy-based Models (EBMs):} Early work on EBMs relied on simple functions to represent the energy function \citep{hinton2002training, hinton2006fast}. These EBMs have proven effective for representing the prior in discrete VAEs~\citep{rolfe2016discrete, vahdat2018dvaes, Vahdat2018DVAE++, vahdat2019UndirectedPost}.  
Recently, \citet{pang2020EBMPrior}  used neural EBMs to represent the prior distribution. However, in this case, the prior is trained using MCMC sampling, and it has been limited to a single group of latent variables. 
NCE \citep{gutmann2010nce} has recently been  used for training a normalizing flow on data distributions \citep{gao2020flowNCE}. \citet{han2019divergence, han2020joint} have used divergence triangulation to sidesteps MCMC sampling. In contrast, we use NCE to train an EBM prior where a noise distribution is easily available through a pre-trained VAE. 

\textbf{Adversarial Training:} Similar to NCE, generative adversarial networks (GANs)~\citep{goodfellow_generative_2014} also rely on a discriminator to learn the likelihood ratio between noise and real images. However, GANs use the discriminator to update the generator, whereas in NCE, the noise generator is fixed. 
In spirit similar are recent works \citep{azadi2018rejection, turner2019mh-gan, che2020ganSecret} that link GANs, defined in the pixels space, to EBMs. 
We apply the likelihood ratio trick to the latent space of VAEs. The main difference: the base prior and approximate posterior are trained with the VAE objective rather than the adversarial loss. Adversarial loss has been used for training implicit encoders in VAEs \citep{makhzani2015adversarial, mescheder2017adversarial, engel2018latent_adversarial}. But, they have not been linked to energy-based priors as we do explicitly.

\textbf{Prior Hole Problem:} Among prior works on this problem, VampPrior~\citep{tomczak2018VampPrior} uses a mixture of encoders to represent the prior. However, this requires storing training data or pseudo-data to generate samples at test time.  \citet{takahashi2019implicitPrior} use the likelihood ratio estimator to train a simple prior distribution. However at test time, the aggregate posterior is used for sampling in the latent space.  \citet{bauer2019resampledPrior} propose a reweighting factor similar to ours, but it is trained via importance sampling.  

\textbf{Two-stage VAEs:} VQ-VAE~\citep{van2017vqvae, razavi2019generating} first trains an autoencoder and then fits an autoregressive PixelCNN~\citep{van2016pixel} prior to the latent variables which is slow to sample from. Two-stage VAE (2s-VAE)~\citep{dai2018diagnosing} trains a VAE on the data, and then, trains another VAE in the latent space. Regularized autoencoders (RAE)~\citep{ghosh2020deterministicVAE} train an autoencoder, and subsequently a Gaussian mixture model on latent codes. In contrast, we train the model with the original VAE objective in the first stage, and we improve the expressivity of the prior using an EBM. It is not clear how 2s-VAE or RAE are applied to state-of-the-art hierarchical models.
 \vspace{-0.1cm}
\section{Experiments}


Our implementation of NCP-VAE builds upon NVAE 
\citep{vahdat2020NVAE}, the state-of-the-art hierarchical VAE. We examine NCP-VAE on four datasets including dynamically binarized MNIST \citep{lecun1998mnist}, CIFAR-10 \citep{krizhevsky2009learning}, CelebA-64 \citep{liu2015deep} and CelebA-HQ-256 \citep{karras2017progressive}. For CIFAR-10 and CelebA-64, the model has 30 groups, and for CelebA-HQ-256 it has 20 groups. We sample from NCP-VAE using SIR with 5K proposal samples at the test (image generation) time. On these datasets, we measure the sample quality using the Fr\'echet Inception Distance (FID) score \citep{heusel2017gans} with 50,000 samples, as computing the log-likelihood requires estimating the intractable normalization constant. To report log-likelihood results, we train an NVAE model with a small latent space on MNIST with 10 groups of  latent variables. We estimate log normalization constant in NCPs using 1000 importance weighted samples. We intentionally limit the latent space to ensure that we can estimate the normalization constant correctly (standard deviation of  estimation ). Thus, on this dataset, we report negative log-likelihood (NLL). Implementation details are provided in Appendix~\ref{app:classifier_architecture}.


\vspace{-0.2cm}
\subsection{Quantitative Results}
\label{subsec:quan_res}

\begin{figure*}
	\centering
	 \setlength\tabcolsep{0pt}
	 \renewcommand{\arraystretch}{-2}
	\begin{tabular}{ccc}
        \includegraphics[width=0.32\linewidth]{figs/mnist_t=1.0.pdf} \hspace{0.01cm} \vspace{0.1cm}&
\includegraphics[width=0.32\linewidth]{figs/cifar_10_t=0.5_bn_adjust.pdf}
        \hspace{0.01cm}\vspace{0.1cm}&
\includegraphics[width=0.32\linewidth]{figs/celeb_64_t=0.7_bn_adjust.pdf} \\
        \vspace{0.1cm}
	    {(a) MNIST ()}&	{(b) CIFAR-10 ()} & {(c) CelebA 6464 ()}
\end{tabular}
\includegraphics[width= \linewidth]{figs/2_single_row_celeb_256_t=0.7_bn_adjust.pdf}
	{(d) CelebA HQ 256256 ()}
	\caption{Randomly sampled images from NCP-VAE with the temperature  for the prior. }
	\label{fig:qualitative mnist cifar celeb 64}
	\vspace{-0.2cm}
\end{figure*}


The quantitative results are reported in \tabref{tab:table celeba 64}, \tabref{tab:table cifar 10}, \tabref{tab:table celeba 256}, and \tabref{tab:table mnist}. On all four datasets, our model improves upon state-of-the-art NVAE, and it reduces the gap with GANs by a large margin.
On CelebA 64, we improve NVAE from an FID of 13.48 to 5.25, comparable to GANs. On CIFAR-10, NCP-VAE improves the NVAE  FID of 51.71 to 24.08. On MNIST, although our latent space is much smaller, our model outperforms previous VAEs. NVAE has reported 78.01 nats on this dataset with a larger latent space.


\textbf{Reconstruction:}  
In the last row of \tabref{tab:table celeba 64}, \tabref{tab:table cifar 10}, and \tabref{tab:table celeba 256}, we report the FID score for the reconstructed images  of the  NVAE baseline. Note how reconstruction FID is much lower than our FID, indicating that our model is far from memorizing the training data. In Appendix~\ref{app:nn}, we also provide nearest neighbours from training data for generated samples. In Appendix~\ref{app:single VAE}, we present our NCP applied to vanilla VAEs.

\begin{table}[t]
\small
\begin{minipage}{.49\linewidth}
    \centering
\caption{Generative performance on CelebA-64}
\label{tab:table celeba 64}
   \begin{tabular}{ll}
      Model & FID \\
      \midrule
      NCP-VAE (ours) & \bf 5.25\\
      NVAE \citep{vahdat2020NVAE} & 13.48\\
      \hline
      RAE \citep{ghosh2020deterministicVAE} & 40.95  \\
      2s-VAE \citep{dai2018diagnosing}& 44.4 \\
      WAE \citep{tolstikhin2018wassersteinAE} & 35 \\
      Perceptial AE\citep{zhang2019perceptual} & 13.8 \\
      \hline
      Latent EBM \citep{pang2020EBMPrior} & 37.87 \\
\hline
      \hline
COCO-GAN \citep{lin2019coco} & 4.0 \\
      QA-GAN \citep{parimala2019quality} & 6.42 \\
      \hline
      NVAE-{\color{blue}Recon} \citep{vahdat2020NVAE} & 1.03\\
\bottomrule
 \end{tabular}
\end{minipage}\hfill
\begin{minipage}{.49\linewidth}
    \centering
\caption{Generative performance on CIFAR-10}
\label{tab:table cifar 10}
   \begin{tabular}{ll}
      Model & FID\\
      \midrule
      NCP-VAE (ours) & \bf 24.08 \\
      NVAE \citep{vahdat2020NVAE} & 51.71 \\
      \hline
      RAE \citep{ghosh2020deterministicVAE} & 74.16  \\
      2s-VAE \citep{dai2018diagnosing}& 72.9 \\
      Perceptial AE \citep{zhang2019perceptual} & 51.51 \\
      \hline
      EBM \citep{du2019implicit} & 40.58 \\
      Latent EBM \citep{pang2020EBMPrior} & 70.15 \\
      \hline
      \hline
      Style-GANv2 \citep{karras2020training} & 3.26 \\
      Denoising Diffusion Process \citep{ho2020denoising} & 3.17 \\ 
\hline
NVAE-{\color{blue}Recon} \citep{vahdat2020NVAE} & 2.67\\
\bottomrule
 \end{tabular}
\end{minipage}
\end{table} \begin{table}[t]
\small
\begin{minipage}{.49\linewidth}
    \centering
\caption{Generative performance on CelebA-HQ-256}
\label{tab:table celeba 256}
   \begin{tabular}{ll}
      Model & FID \\
      \midrule
      NCP-VAE (ours) & \bf 24.79\\
      NVAE \citep{vahdat2020NVAE} & 40.26\\
      \hline
      GLOW \citep{kingma2018glow} & 68.93\\
      \hline
      \hline
      Advers. LAE \citep{pidhorskyi2020adversarial} & 19.21\\
      PGGAN \citep{karras2017progressive} & 8.03 \\
      \hline
      NVAE-{\color{blue}Recon} \citep{vahdat2020NVAE} & 0.45\\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{.5\linewidth}
    \centering
\caption{Likelihood results on MNIST in nats}
\label{tab:table mnist}
   \begin{tabular}{ll}
      Model & NLL \\
      \midrule
      NCP-VAE (ours) & \bf 78.10\\
      NVAE-small \citep{vahdat2020NVAE}    & 78.67\\
      \hline
BIVA \citep{maaloe2019biva} & 78.41 \\
      DAVE++ \citep{Vahdat2018DVAE++} & 78.49 \\
      IAF-VAE \citep{kingma2016improved} & 79.10 \\
      VampPrior AR dec. (\citeauthor{tomczak2018VampPrior}) & 78.45 \\
\bottomrule
\end{tabular}
\end{minipage}
\end{table}










 \vspace{-0.2cm}
\subsection{Qualitative Results}
We visualize samples generated by NCP-VAE in Fig.~\ref{fig:qualitative mnist cifar celeb 64} without any manual intervention. We adopt the common practice of reducing the temperature of the base prior  by scaling down the standard-deviation of the conditional Normal distributions \citep{kingma2018glow}\footnote{Lowering of the temperature is only used to obtain qualitative samples. It's not used when computing any of the quantitative results in Sec.~\ref{subsec:quan_res}.}. \citet{brock2018large,vahdat2020NVAE} also observe that re-adjusting the batch-normalization (BN), given a temperature applied to the prior, improves the generative quality. Similarly, we achieve diverse, high-quality images by re-adjusting the BN statistics as described by \citet{vahdat2020NVAE}. 
Additional qualitative results are shown in  Appendix~\ref{app:qual celeb 64}.
\vspace{-0.2cm}
\subsection{Additional Studies} \label{ablation}
\vspace{-0.009cm}
\setlength{\tabcolsep}{2pt}
\begin{wraptable}[7]{r}{3.7cm}
    \small
    \vspace{-2mm}
    \centering
    \caption{\small{\# groups \& generative performance in FID}}
    \vspace{-5mm}
    \begin{tabular}{ccc}\\ \toprule
    \makecell{\# groups}
        & NVAE  & NCP-VAE \\ \midrule
        6 & 33.18 & 18.68  \\  
        15 & 14.96 & 5.96  \\ 
        30 & 13.48 &\bf{5.25} \\
        \bottomrule
    \end{tabular}
    \label{tab:num_groups_ablation}
\end{wraptable} 
\setlength{\tabcolsep}{5pt} We perform additional experiments to study i) how hierarchical NCPs perform as the number of latent groups increases, ii) the impact of SIR and LD hyperparameters, and iii) what the classification loss in NCE training conveys about  and . All experiments in this section are done on CelebA-64 data.


\textbf{Number of latent variable groups:} \tabref{tab:num_groups_ablation} shows the generative performance of hierarchical NCP with different amounts of latent variable groups. 
As we increase the number of groups, the FID score of both NVAE and our model improves. This demonstrates the efficacy of our NCPs, even with expressive hierarchical priors and in the presence of many groups.

\begin{table}
\small
    \vspace{-5mm}
    \centering
    \caption{\small{Effect of SIR sample size and LD iterations. Time- is the time used to generate a batch of  images.}}
    \vspace{-5mm}
    \begin{tabular}{ccccc|ccccc}\\ \toprule  
       \makecell{\# SIR proposal \\ samples} & FID & \makecell{Time-1 \\ (sec)} & \makecell{Time-10 \\ (sec)} & \makecell{Memory \\ (GB)} & \makecell{\# LD \\iterations} &  FID & \makecell{Time-1 \\ (sec)} & \makecell{Time-10 \\ (sec)} & \makecell{Memory \\ (GB)}
        \\ \midrule
        5 & 11.75  & 0.34 & 0.42 & 1.96 & 5 & 14.44  & 3.08 & 3.07 & 1.94\\   
        50 & 8.58  & 0.40  & 1.21 & 4.30 & 50 & 12.76  & 27.85  & 28.55 & 1.94\\
        500 & 6.76 & 1.25 &9.43 & 20.53 & 500 & 8.12 & 276.13 & 260.35 & 1.94\\
        5000 & \textbf{5.25} & 10.11 &95.67 & 23.43 & 1000 & \textbf{6.98} & 552 & 561.44 & 1.94 \\
        \bottomrule
    \end{tabular}
    \label{tab: sir_ablation}
\end{table}
%
 \textbf{SIR and LD parameters:} The computationally complexity of SIR is similar to LD if we set the number of proposal samples in SIR equal to the number LD iterations. \tabref{tab: sir_ablation} reports the impact of these parameters. We observe that increasing both the number of proposal samples in SIR and the LD iterations leads to a noticeable improvement in  FID score. For SIR, the proposal generation and the evaluation of  are parallelizable. Hence, as shown in \tabref{tab: sir_ablation}, image generation is faster with SIR than with LD (LD iterations are sequential). However,  GPU memory usage scales with the number of SIR proposals, but not with the number of LD iterations. Interestingly, SIR, albeit simple, performs better than LD when using about the same compute. 

\textbf{Classification loss in NCE:}
We can draw a direct connection between the classification loss in Eq.~\ref{eq:gan_nce} and the similarity of  and . Denoting the classification loss in Eq.~\ref{eq:gan_nce} at optimality by , \citet{goodfellow_generative_2014} show that  where  denotes the Jensen–Shannon divergence between two distributions. 
Fig.~\ref{fig:disc loss and eff. sample size}(a) plots the classification loss (Eq.~\ref{eq:gan_nce_hvae}) for each classifier for a 15-group NCP trained on the CelebA-64 dataset. Assume that the classifier loss at the end of training is a good approximation of . We observe that 8 out of 15 groups have , indicating a good overlap between  and  for those groups. 
 
To further assess the impact of the distribution match on SIR sampling, in Fig.~\ref{fig:disc loss and eff. sample size}(b), we visualize the effective sample size (ESS)\footnote{ESS measures  reliability of SIR via , where  \citep{ownMCbook}.}  in SIR vs.\  for the same group. We observe a strong correlation between  and the effective sample size. SIR is more reliable on the same 8 groups that have high classification loss. These groups are mostly at the top of the NVAE hierarchy which have been shown to control the global structure of generated samples (see B.6 in \citet{vahdat2020NVAE}).

\begin{figure*}[t]
	\centering
	 \setlength\tabcolsep{0pt}
	 \renewcommand{\arraystretch}{-2}
	 \vspace{-0.2cm}
	\begin{tabular}{cc}
        \includegraphics[width=0.45\linewidth, trim={0 0 0.5cm 0.7cm}, clip=true]{figs/celeb64_disc_loss.pdf}&
        \includegraphics[width=0.45\linewidth, trim={0 0 0.5cm 0.7cm}, clip=true]{figs/effective_sample_size.pdf}\\
	    {\scriptsize (a)}&	{\scriptsize (b)}
	    \vspace{-2pt}
	    \end{tabular}
	\caption{ \textbf{(a)} Classification loss for binary classifiers on latent variable groups. A larger final loss upon training indicates that  and   are more similar. \textbf{(b)} The effective sample size vs.\ the final loss value at the end of training. Higher effective sample size implies similarity of two distributions. 
	}
	\label{fig:disc loss and eff. sample size}
	\vspace{-0.1cm}
\end{figure*}


 \vspace{-0.2cm}
\section{Conclusions}
\vspace{-0.2cm}
The prior hole problem is one of the main reasons for VAEs' poor generative quality. In this paper, we tackled this problem by introducing the noise contrastive prior (NCP), defined by the product of a reweighting factor and a base prior. We showed how the reweighting factor is trained by contrasting samples from the aggregate posterior with samples from the base prior. Our proposal is simple and can be applied to any VAE to increase its prior's expressivity. We also showed how NCP training scales to large hierarchical VAEs, as it can be done in parallel simultaneously for all the groups. Finally, we demonstrated that NCPs improve the generative performance of state-of-the-art NVAEs by a large margin, closing the gap to GANs. \bibliography{generative}
\bibliographystyle{iclr2021_conference}
\newpage
\appendix
\section{Training Energy-based Priors using MCMC}\label{app:mcmc}
In this section, we show how a VAE with energy-based model in its prior can be trained. Assuming that the prior is in the form , the variational bound is of the form:

where the expectation term, similar to VAEs, can be trained using the reparameterization trick. The only problematic term is the log-normalization constant , which  captures the gradient with respect to the parameters of the prior . Denoting these parameters by , the gradient of  is obtained by:

where the expectation can be estimated using MCMC sampling from the EBM prior.

\section{Maximizing the Variational Bound from the Prior's Perspective}
In this section, we discuss how maximizing the variational bound in VAEs from the prior's perspective corresponds to minimizing a KL divergence from the aggregate posterior to the  prior. 

\subsection{VAE with a Single Group of Latent Variables} \label{app:vae_prior}
Denote the aggregate (approximate) posterior by . Here, we show that maximizing the  with respect to the prior parameters corresponds to learning the prior by minimizing  . To see this,  note that the prior  only participates in the KL term in  (Eq.~\ref{eq:vae_loss}). We hence have:

where  denotes the entropy. Above, we  replaced the expected entropy  with  as the minimization is with respect to the parameters of the prior . 


\subsection{Hierarchical VAEs}\label{app:hvae_prior}
Denote hierarchical approximate posterior and prior distributions by:  and . The hierarchical VAE objective becomes:

where  is the approximate posterior up to the  group. Denote the aggregate posterior up to the  group by  and the aggregate conditional for the  group given the previous groups .

Here, we show that maximizing  with respect to the prior corresponds to learning the prior by minimizing  for each conditional:



\section{Conditional NCE for Hierarchical VAEs} \label{app:cnce}
In this section, we describe how we derive the NCE training objective for hierarchical VAEs given in Eq.~\eqref{eq:gan_nce_hvae}. Our goal is to learn the likelihood ratio between the aggregate conditional  and the prior . We can define the NCE objective to train the discriminator  that classifies  given samples from the previous groups  using:

Since  is in a high dimensional space, we cannot apply the minimization . Instead, we sample from  using the aggregate approximate posterior  as done for the KL in a  hierarchical model (Eq.~\eqref{eq:hier_kl}):

Since , we have:

Finally, instead of passing all the samples from the previous latent variables groups to , we can pass the context feature  that extracts a representation from all the previous groups:




\section{NVAE Based Model and Context Feature} \label{app:context s}

\textbf{Context Feature:} The base model NVAE \citep{vahdat2020NVAE} is hierarchical.  To encode the information from the lower levels of the hierarchy to the higher levels, during training of the binary classifiers, we concatenate the context feature  to the samples from both  and . The context feature for each group is the output of the residual cell of the top-down model and encodes a representation from . 

\textbf{Image Decoder :} The base NVAE~\citep{vahdat2020NVAE} uses a mixture of discretized logistic distributions for all the datasets but MNIST, for which it uses a Bernoulli distribution. In our model, we observe that replacing this with a Normal distribution for the RGB image datasets leads to significant improvements in the base model performance. This is also reflected in the gains of our approach. 


\vspace{-0.1cm}
\section{Implementation Details} \label{app:classifier_architecture}

\begin{figure}[t!]
\vspace{-1cm}
\centering
\includegraphics[scale=0.5]{figs/discriminator_arch.png}
\vspace{-0.1cm}
\caption{Residual blocks used in the binary classifier. We use {\em s}, {\em p} and {\em C} to refer to the stride parameter, the padding parameter and the number of channels in the feature map, respectively.}
\label{fig:classifer_arch}
\end{figure}
\vspace{-0.1cm}
The binary classifier is composed of two types of residual blocks as in Fig.~\ref{fig:classifer_arch}. The residual blocks use batch-normalization \citep{ioffe2015batch}, the Swish activation function \citep{ramachandran2017swish}, and the Squeeze-and-Excitation (SE) block \citep{hu2018squeeze}. SE performs a {\em squeeze} operation ({\em e.g.}, mean) to obtain a single value for each channel. An {\em excitation} operation (non-linear transformation) is applied to these values to get per-channel weights. The Residual-Block-B differs from Residual-Block-A in that it doubles the number of channels (), while down-sampling the other spatial dimensions. It therefore also includes a factorized reduction with  convolutions along the skip-connection. The complete architecture of the classifier is:

\begin{table}[h]
\centering
\scalebox{0.9}{
\texttt{
\begin{tabular} {c}
Conv 3x3 (s1, p1) + ReLU \\
 \\ 
Residual-Block-A \\
 \\ 
Residual-Block-A \\ 
 \\ 
Residual-Block-A \\ 
 \\ 
Residual-Block-B \\
 \\ 
Residual-Block-A \\
 \\ 
Residual-Block-A \\ 
 \\ 
Residual-Block-A \\ 
 \\ 
Residual-Block-B \\
 \\ 
2D average pooling \\
 \\ 
Linear + Sigmoid 
\end{tabular}}}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{c||c}
    Optimizer & Adam \citep{kingma2014adam} \\ 
    Learning Rate & Initialize at -, CosineAnnealing \citep{loshchilov2016sgdr} to - \\
    Batch size & 512 \small{(MNIST, CIFAR-10)}, 256 \small{(CelebA-64)}, 128 \small{(CelebA HQ 256 )} 
\end{tabular}
\caption{Hyper-parameters for training the binary classifiers.}
\label{tab:hyperparam_sac}
\end{table}

\section{Performance On Single Group VAE} \label{app:single VAE}
To demonstrate the efficacy of our approach on any off-the-shelf VAE, we, apply our NCE based approach to the VAE in \citep{ghosh2020deterministicVAE}. Note we use the vanilla VAE with Normal prior provided by the authors. The FID for CelebA 64 improves from 48.12 to 41.28. Note that the FID for reconstruction is reported as 39.12. Single group VAEs are known to perform poorly on the task of image geeration, which is reflected in the high FID value of reconstruction.


\section{Nearest Neighbors from the Training Dataset} \label{app:nn}
To highlight that hierarchical NCP generates unseen samples at test time rather than memorizing the training dataset, Figures~\ref{fig:nearest_neighbors_1}-\ref{fig:nearest_neighbors_2} visualize samples from the model along with a few training images that are most similar to them (nearest neighbors). To get the similarity score for a pair of images, we downsample  to , center crop to  and compute the Euclidean distance. The KD-tree algorithm is used to fetch the nearest neighbors. We note that the generated samples are quite distinct from the training images.\\\\
\newpage
{\color{blue}Query Image} \hfill {\color{blue}Nearest neighbors from the training dataset} \hfill .
\begin{figure*}[h]
    \centering
    \includegraphics[scale=0.15]{figs/nearest_neighbors_1.pdf}
    \caption{Query images (left) and their nearest neighbors from the CelebA-HQ-256 training dataset.}
    \label{fig:nearest_neighbors_1}
\end{figure*}

\newpage
{\color{blue}Query Image} \hfill {\color{blue}Nearest neighbors from the training dataset} \hfill .
\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.15]{figs/nearest_neighbors_2.pdf}
    \caption{Query images (left) and their nearest neighbors from the CelebA-HQ-256 training dataset.}
    \label{fig:nearest_neighbors_2}
\end{figure*}

\newpage
\section{Additional Qualitative Examples} \label{app:qual celeb 64}
\begin{figure*}[h!]
    \centering
    \includegraphics[scale=0.5]{figs/appendix_celeb_64_t=0.7_bn_adjust.pdf}
    \caption{ Additional samples from CelebA-64 at . }
    \label{fig: additional samples celeb 64}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.15]{figs/appendix_celeb_256_t=0.7_bn_adjust.pdf}
    \caption{ Additional samples from CelebA-HQ-256 at . }
    \label{fig: additional samples celeb 256}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.3]{figs/good_cherries_celeba_256.pdf}
    \caption{Selected good quality samples from CelebA-HQ-256. }
    \label{fig:cherries}
\end{figure*} \end{document}
