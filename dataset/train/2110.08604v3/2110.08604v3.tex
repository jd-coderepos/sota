\section{Experiments}
The experimental datasets and hyperparameters are provided in \pref{app:datasets} and \pref{app:hyperparameters}, respectively.

\begin{table*}[htbp]
  \centering
\caption{The overall performance of \our\ models on the three public datasets, and the best results are heightened in \textbf{bold} font. Numbers in parentheses are IQRs. $^{\dagger}$ indicates the results are the best performance in multiple runs, while other methods report the average performance; $^{\ddagger}$ indicates the experimental results of the models implemented by us. The introduction of compared models is available in \pref{app:compared_models}. }
  \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{lccccccc}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[1]{*}{\textbf{Model}}} &       & \multicolumn{2}{c}{\texttt{Laptop$14$}} & \multicolumn{2}{c}{\texttt{Restaurant14}} & \multicolumn{2}{c}{\texttt{MAMS}} \\
    \cline{3-8}   &   & \texttt{Acc}  & \texttt{F$1$}  & \texttt{Acc}  & \texttt{F$1$}  & \texttt{Acc}  & \texttt{F$1$}  \\
    \midrule
    
    \texttt{SK-GCN-BERT}~\cite{ZhouHHH20}  & \multirow{12}[1]{*}{\begin{sideways}Baselines\end{sideways}} 
                         & $79.00$ & $75.57$ & $83.48$ & $75.19$    & ---     & --- \\
    \texttt{SDGCN-BERT}~\cite{ZhaoHW20}   &       & $81.35$ & $78.34$ & $83.57$ & $76.47$    & ---     & --- \\
\texttt{DGEDT-BERT}~\cite{TangJLZ20}   &       & $79.80$ & $75.60$ & $86.30$ & $80.00$    & ---     & --- \\
    \texttt{DualGCN-BERT}~\cite{LiCFMWH20} & 	     & $81.80$ & $78.10$ & $87.13$ & $81.16$    & ---     & --- \\
    \texttt{ASGCN-RoBERTa}~\citet{DaiYSLQ21} &       & $83.33$ & $80.32$ & $86.87$ & $80.59$    & ---     & --- \\
    \texttt{RGAT-RoBERTa}~\citet{DaiYSLQ21} &       & $83.33$ & $79.95$ & $87.52$ & $81.29$    & ---     & --- \\
    \texttt{PWCN-RoBERTa}~\citet{DaiYSLQ21} &       & $84.01$ & $81.08$ & $87.35$ & $80.85$    & ---     & --- \\
    \texttt{TGCN-BERT}~\cite{LiCFMWH20}    &   	 & $80.88$ & $77.03$ & $86.16$ & $79.95$	& $83.38$ & $82.77$ \\
    \texttt{SARL-RoBERTa}$^{\dagger}$~\cite{WangSLZC21} &   & $85.42$ & $82.97$ & $88.21$ & $82.44$    & ---     & --- \\
    \texttt{RoBERTa}~\cite{LiuOGDJCLLZS19}$^{\ddagger}$  &  & $82.76(0.63)$ & $79.73(0.77)$ & $87.77(1.61)$ & $82.10(2.01)$ & $83.83(0.49)$ & $83.29(0.50)$ \\
    \texttt{DeBERTa}~\cite{HeGC21}$^{\ddagger}$ ~ &   & $82.76(0.31)$ & $79.45(0.60)$ & $88.66(0.35)$ & $83.06(0.29)$ & $83.06(1.24)$ & $82.52(1.25)$ \\
    
    \midrule
    \midrule
\texttt{\ourp-RoBERTa} & \multirow{3}[2]{*}{\begin{sideways}\our\end{sideways}} 
                          & $83.39(0.35)$ & $80.47(0.44)$ & $88.04(0.62)$ & $82.96(0.48)$ & $83.37(0.31)$ & $83.78(0.29)$ \\
    \texttt{\ourt-RoBERTa} &       & $83.44(0.56)$ & $80.47(0.71)$ & $88.30(0.37)$ & $83.09(0.45)$ & $83.31(0.41)$ & $84.60(0.22)$ \\
    \texttt{\ours-RoBERTa} &       & $83.23(0.44)$ & $80.30(0.68)$ & $88.48(0.52)$ & $83.81(0.62)$ & $83.58(0.39)$ & $83.78(0.24)$ \\
    \midrule

    \texttt{\ourp-DeBERTa} & \multirow{3}[2]{*}{\begin{sideways}\our\end{sideways}} 
                          & $84.33(0.55)$ & $81.46(0.77)$ & $89.91(0.09)$ & $84.90(0.45)$ & $83.91(0.31)$ & $83.31(0.21)$ \\
    \texttt{\ourt-DeBERTa} &       & $84.80(0.39)$ & $82.00(0.43)$ & $89.91(0.40)$ & $85.05(0.85)$ & $84.28(0.32)$ & $83.70(0.47)$ \\
    \texttt{\ours-DeBERTa} &       & $84.17(0.08)$ & $81.23(0.27)$ & $89.64(0.66)$ & $84.53(0.79)$ & $83.61(0.30)$ & $83.07(0.28)$ \\
    \midrule

    \texttt{\ourpx-DeBERTa} & \multirow{3}[2]{*}{\begin{sideways}\ourx\end{sideways}}  
                          & $86.00(0.07)$ & $83.10(0.30)$ & $90.27(0.61)$ & $85.51(0.48)$ & $82.78(0.96)$ & $81.99(0.86)$ \\ 
    \texttt{\ourtx-DeBERTa} &      & \textbf{$86.31(0.20)$} & $83.93(0.27)$ & \textbf{$90.86(0.18)$} & \textbf{$86.26(0.22)$} & $84.21(0.42)$ & $83.72(0.46)$ \\
    \texttt{\oursx-DeBERTa} &      & $86.21(0.52)$ & \textbf{$83.97(0.64)$} & $90.33(0.37)$ & $85.55(0.46)$ & \textbf{$84.68(0.67)$} & \textbf{$84.12(0.64)$} \\
    \bottomrule
    \end{tabular}}
  \label{tab:main}\end{table*}


\subsection{Overall Performance}

Table \ref{tab:main} shows the experimental performance of our models and their counterparts.
Overall, our models obtain state-of-the-art performance on all datasets without any syntax information\footnote{Except for our syntax model, which utilizes the syntax tree to calculate the token distances. But it avoids structural modeling. }, which indicates sentiment coherency modeling is significant in existing ABSA datasets. Besides, we find that syntax tree modeling, which has been widely studied in sentiment dependency modeling, is not significantly better than non-syntactical methods for sentiment coherency learning. For example, our tree model and its large version outperform our syntax model and its large version in many scenarios, and the global IQR of our large tree model is smaller than other models. Our positional model and its large version are not as efficient as other models (approximately 3 times slower) because they have to learn aspect features separately, while we can reuse the global context feature for aspect-focused feature learning.

Compared to our model, low quality of syntax trees and the alignment problem between the tree node and tokenization node (which will be discussed in \pref{sec:rq4}) in sentiment dependency learning methods lead to limited performance. Some recent works are devoted to refining the quality of the syntax structure (\texttt{TGCN}\cite{TianCS21}, etc.) or improving the ability of dependency parsing (\texttt{Dual-GCN}\cite{LiCFMWH20}, etc.), while the techniques in these works are not easy to be adapted to other model architectures. Meanwhile, \citet{DaiYSLQ21} argue that the existing methods of syntax tree extraction are disappointing and propose to induce the tree structure by fine-tuning the PLM. Although, the performance of \texttt{ASGCN-RoBERTa}, \texttt{RGAT-RoBERTa}, and \texttt{PWCN-RoBERTa} shows hopeful improvement, their resource occupation is tremendous compared to other models. Since our model outperforms these three models on three datasets, it is hard to argue that the inducing syntax trees are necessary given its complexity. \texttt{SARL-RoBERTa} tries to alleviate the sentiment bias and align the sentiment prediction with opinion terms extraction, but it cannot be fairly compared with our model. Because it reports the best performance in ten runs while we report the average results. In conclusion, the experimental results of our model show sentiment coherency modeling substantially prompts aspect-based sentiment analysis.

\subsection{Research Questions}
\subsubsection*{RQ1: Does \our\ learn aspect sentiment coherency?}
\label{sec:rq1}
\begin{table}[htbp]
  \centering
  \caption{The examples for coherent sentiment learning based on \our. The target aspects are denoted in \textbf{bold} and the \underline{underlined words} indicates the aspects with coherent sentiments. ``Pos'', ``Neg'' and ``Neu'' represent positive, negative and neutral, respectively. }
  \resizebox{\linewidth}{!}{
   
    \begin{tabular}{|c|c|l|c|c|}
    \hline
    No.   & Domain & \multicolumn{1}{c|}{Example} & Model & Sentiment \\
    \hline
    \multirow{3}[4]{*}{$1$} & \multirow{3}[4]{*}{Restaurant} & Not only was the \textit{food} \underline{outstanding}, & \multirow{2}[2]{*}{\ourp-\texttt{BERT}} & \multirow{2}[2]{*}{Pos(Pos)~\textcolor{green}{\cmark}, Pos(Pos)~\textcolor{green}{\cmark}} \\
          &       & but also the \textbf{coffee} and \textbf{juice}! &       &  \\
\cline{3-5}          &       & Not only was the \textit{food} \underline{terrible},  & \multirow{2}[2]{*}{\ourp-\texttt{BERT}} & \multirow{2}[2]{*}{Neg(Neg)~\textcolor{green}{\cmark}, Neu(Neg)~\textcolor{red}{\xmark}} \\
          &       & but also the \textbf{coffee} and \textbf{juice}! &       &  \\
    \hline
    \multirow{3}[4]{*}{$2$} & \multirow{3}[4]{*}{Restaurant} & The \textit{servers} always \underline{surprise} us & \multirow{2}[2]{*}{\ours-\texttt{BERT}} & \multirow{2}[2]{*}{Pos(Pos)~\textcolor{green}{\cmark}} \\
          &       &  with a different \textbf{starter}. &       &  \\
\cline{3-5}          &       & The \textit{servers} always \underline{temporize} us & \multirow{2}[2]{*}{\ours-\texttt{BERT}} & \multirow{2}[2]{*}{Neg(Neg)~\textcolor{green}{\cmark}} \\
          &       &  with a different \textbf{starter}. &       &  \\
    \hline
    \multirow{3}[4]{*}{$3$} & \multirow{3}[4]{*}{TV} & The speakers of this TV is \underline{great}!  & \multirow{2}[2]{*}{\ourt-\texttt{DeBERTa}} & \multirow{2}[2]{*}{Pos(Pos)~\textcolor{green}{\cmark}} \\
          &       & Just like its \textbf{screen}. &       &  \\
\cline{3-5}          &       & The speakers of this TV \underline{sucks}! & \multirow{2}[2]{*}{\ourt-\texttt{DeBERTa}} & \multirow{2}[2]{*}{Neg(Neg)~\textcolor{green}{\cmark}} \\
          &       & Just like its \textbf{screen}. &       &  \\
    \hline
    \multirow{3}[4]{*}{$4$} & \multirow{3}[4]{*}{Camera} 
    & If you are worry about \textbf{usability},  & \multirow{2}[2]{*}{\texttt{DeBERTa}} & \multirow{2}[2]{*}{Neu(Pos)~\textcolor{red}{\xmark}} \\
              &       & think about \underline{the} \textit{quality} ! &       &  \\
    \cline{3-5}          &       & If you are worry about \textbf{usability},  & \multirow{2}[2]{*}{\texttt{DeBERTa}} & \multirow{2}[2]{*}{Pos(Pos)~\textcolor{green}{\cmark}} \\
          &       & think about \underline{it good} \textit{quality} ! &  &  \\
    \hline
    \end{tabular}}
  \label{tab:rq1}\end{table}To ascertain whether our model is capable of learning local sentiment coherency, we conduct experiments and present several cases in Table \ref{tab:rq1}. For example, the \texttt{DeBERTa} in example $\#4$ produces two inference errors of coherent sentiments while all our model variants based on the \texttt{DeBERTa} model yield correct results. Additionally, our positional and syntax-based models output fewer incorrect inferences of coherent sentiment. Furthermore, \ourp, \ourt\ and \ours\ models demonstrate promising robustness in dealing with perturbed examples containing local sentiments coherency. Although it is challenging to list more examples, all experiments demonstrate consistent observations compared to those in Table \ref{tab:rq1}. According to these experimental results, we assert our model's ability to learn sentiment coherency in ABSA.

\subsubsection*{RQ2: Does gradient-based aggregation window optimization improve implicit sentiment learning?}

\label{sec:rq2}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{fig/dynamic_eta.pdf}
	\caption{Visualization of learnable weights optimization in differential weighted local sentiment aggregation.}
	\label{fig:rq2dynamic_eta}
\end{figure}

Since $\eta_{l}$ and $\eta_{r}$ are specific to the dataset and hyper-parameters, we propose differential weighted window construction in our model. Initial attempts at building sentiment aggregation windows involve fixing static weights $\eta_{l}$ and $\eta_{r}$ for window components. However, we want these weights to be adaptive, allowing the model to automatically find the optimal weights. We thus use learnable weights $\eta_{l}^{*}$ and $\eta_{r}^{*}$ to guide window construction, optimizing them via gradient descent. Table \ref{tab:rq3} displays the experimental results of our model without DWA models, indicating a consistent performance drop when DWA is ablated. Additionally, the trajectory of $\eta_{l}^{*}$ and $\eta_{r}^{*}$ during the training process, as shown in Figure \ref{fig:rq2dynamic_eta}, reveals that the contribution of side aspects increases rapidly initially and then decreases. In summary, we observe approximately $0.2\%-0.5\%$ improvement in most scenarios compared to our model without DWA.

\subsubsection*{RQ3: Can sentiment coherency learning improve existing ABSC models?}

\begin{table*}[htbp]
  \centering
  \scriptsize
  \caption{The performance of \our\ based on different PLMs. The best experimental results are heightened in \textbf{bold}. }
  \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{lccccccr}
    \toprule
    \multicolumn{1}{c}{\multirow{2}[1]{*}{\textbf{Model}}} &       & \multicolumn{2}{c}{\texttt{Laptop$14$}} & \multicolumn{2}{c}{\texttt{Restaurant$14$}} & \multicolumn{2}{c}{\texttt{MAMS}}  \\
    \cline{3-8}  &  & \texttt{Acc}  & \texttt{F$1$}  & \texttt{Acc}  & \texttt{F$1$}  & \texttt{Acc}  & \texttt{F$1$} \\
    \midrule
    \texttt{BERT-BASE}     & \multirow{3}[2]{*}{\begin{sideways}Base\end{sideways}} 
                                      & $80.36(0.78)$ & $77.04(0.71)$ & $86.34(0.18)$ & $80.01(0.28)$ & $82.52(1.13)$ & $81.87(1.23)$ \\
    \texttt{RoBERTa-BASE}    &        & $82.76(0.63)$ & $79.73(0.77)$ & $87.77(1.61)$ & $82.10(2.01)$ & $83.83(0.49)$ & $83.29(0.50)$ \\
    \texttt{DeBERTa-BASE}    &        & $82.76(0.31)$ & $79.45(0.60)$ & $88.66(0.35)$ & $83.06(0.29)$ & $83.06(1.24)$ & $82.52(1.25)$ \\
    \midrule
    \texttt{\ourp-BERT}    & \multirow{9}[1]{*}{\begin{sideways}\our~w/o~DWA\end{sideways}} 
                                    & $80.67(0.47)$ & $77.20(0.69)$ & $86.43(0.13)$ & $80.71(0.47)$ & $83.58(0.56)$ & $83.00(0.55)$ \\
    \texttt{\ourt-BERT}    &        & $80.72(0.31)$ & $77.16(0.27)$ & $87.53(0.58)$ & $81.85(0.69)$ & $83.03(0.34)$ & $82.34(0.42)$ \\
    \texttt{\ours-BERT}    &        & $80.62(0.55)$ & $76.89(0.44)$ & $86.70(0.62)$ & $81.11(0.79)$ & $82.41(1.35)$ & $81.71(1.45)$ \\
    \texttt{\ourp-RoBERTa} &        & $82.55(0.78)$ & $79.93(0.83)$ & $87.68(0.48)$ & $82.46(0.65)$ & $83.31(0.47)$ & $82.90(0.62)$ \\
    \texttt{\ourt-RoBERTa} &        & $82.76(0.55)$ & $80.08(0.44)$ & $87.59(1.03)$ & $82.02(1.29)$ & $83.53(0.45)$ & $82.92(0.32)$ \\
    \texttt{\ours-RoBERTa} &        & $82.92(0.39)$ & $80.10(0.57)$ & $88.21(0.89)$ & $82.32(0.78)$ & $83.95(0.34)$ & $83.30(0.54)$ \\
    \texttt{\ourp-DeBERTa} &        & $84.27(0.47)$ & $81.38(0.23)$ & $89.60(0.51)$ & $84.90(0.49)$ & $84.06(0.08)$ & $83.57(0.18)$ \\
    \texttt{\ourt-DeBERTa} &        & $84.27(0.31)$ & $81.18(0.29)$ & $89.79(0.71)$ & $84.88(1.13)$ & $83.01(0.86)$ & $82.53(0.92)$ \\
    \texttt{\ours-DeBERTa} &        & $83.91(0.78)$ & $81.24(1.01)$ & $89.73(0.46)$ & $84.71(0.55)$ & $83.31(0.41)$ & $82.80(0.58)$ \\
    \midrule
    \texttt{\ourp-BERT}    & \multirow{9}[1]{*}{\begin{sideways}\our\end{sideways}} 
                                    & $81.35(0.63)$ & $77.79(0.48)$ & $87.23(0.22)$ & $81.06(0.67)$ & $83.13(0.30)$ & $82.53(0.44)$ \\
    \texttt{\ourt-BERT}    &        & $81.35(0.39)$ & $78.43(0.52)$ & $87.32(0.22)$ & $81.86(0.20)$ & $83.51(0.26)$ & $82.90(0.28)$ \\
    \texttt{\ours-BERT}    &        & $81.03(0.31)$ & $77.45(0.37)$ & $87.41(0.40)$ & $81.52(0.49)$ & $83.23(0.56)$ & $82.68(0.52)$ \\
    \texttt{\ourp-RoBERTa} &        & $83.39(0.35)$ & $80.47(0.44)$ & $88.04(0.62)$ & $82.96(0.48)$ & $83.37(0.31)$ & $83.78(0.29)$ \\
    \texttt{\ourt-RoBERTa} &        & $83.44(0.56)$ & $80.47(0.71)$ & $88.30(0.37)$ & $83.09(0.45)$ & $83.31(0.41)$ & $83.60(0.22)$ \\
    \texttt{\ours-RoBERTa} &        & $83.23(0.44)$ & $80.30(0.68)$ & $88.48(0.52)$ & $83.81(0.62)$ & $83.58(0.39)$ & $83.78(0.24)$ \\
    \texttt{\ourp-DeBERTa} &        & $84.33(0.55)$ & $81.46(0.77)$ & \textbf{$89.91(0.09)$} & $84.90(0.45)$ & $83.91(0.31)$ & $83.31(0.21)$ \\
    \texttt{\ourt-DeBERTa} &        & \textbf{$84.80(0.39)$} & \textbf{$82.00(0.43)$} & \textbf{$89.91(0.40)$} & \textbf{$85.05(0.85)$} & \textbf{$84.28(0.32)$} & \textbf{$83.70(0.47)$} \\
    \texttt{\ours-DeBERTa} &        & $84.17(0.08)$ & $81.23(0.27)$ & $89.64(0.66)$ & $84.53(0.79)$ & $83.61(0.30)$ & $83.07(0.28)$ \\

    \bottomrule
    \end{tabular}}
  \label{tab:rq3}\end{table*}

\label{sec:rq3}
We perform comparison experiments on several popular pre-trained language models (including \texttt{BERT}, \texttt{RoBERTa}, and \texttt{DeBERTa}), and the experimental results are shown in \pref{tab:rq3}. Compared with \texttt{\our-RoBERTa}, \our-\texttt{DeBERTa} achieves a more significant performance improvement. For \our-\texttt{BERT}, both \texttt{Accuracy} and \texttt{F$1$} achieved an absolute improvement ranging from $~0.5\%-1\%$. Based on DWA-based \our, a significant improvement in all datasets has been obtained compared to the prototype PLMs, especially the \our\ based on \texttt{DeBERTa}. 
However, We find that DWA fails to improve the \texttt{DeBERTa-Large} and \texttt{RoBERTa-Large} models noticeably. We believe this problem probably results from the inevitable redundant features in the sentiment aggregation window; the gradient optimization of $\eta_{l}^{*}$ and $\eta_{r}^{*}$ slows the learning process of the large pretrained models and interferes with the feature representation learning of PLM.



We perform comparative experiments on several popular pre-trained language models, including \texttt{BERT}, \texttt{RoBERTa}, and \texttt{DeBERTa}, with results displayed in Table \ref{tab:rq3}. Our model based on \texttt{DeBERTa} demonstrates a significant performance improvement. For our model based on \texttt{BERT}, both accuracy and F1 scores see an absolute improvement ranging from $~0.5\%-1\%$. A significant improvement across all datasets is achieved when using DWA-based models compared to prototype PLMs, especially with \texttt{DeBERTa}. However, DWA fails to noticeably improve the \texttt{DeBERTa-Large} and \texttt{RoBERTa-Large} models, likely due to inevitable redundant features in the sentiment aggregation window and the slowing down of the learning process. In a nutshell, our model is a paradigm rather than a complex network structure, making it extensible and flexible. In a nutshell, \our\ is a simple but effective paradigm. It can improve existing methods by simply applying local sentiment aggregation based on their original architecture. 

\subsubsection*{RQ4: How does our model compare to dependency learning methods in sentiment coherency learning?}
\label{sec:rq4}

Sentiment coherency is a unique type of sentiment dependency. As such, we evaluate our model's performance in relation to dependency learning-based methods. Prominent studies in ABSA sentiment dependency learning often favor syntax-based modeling. However, our results indicate that our model outperforms syntax-based methods in most scenarios. As depicted in Table \ref{tab:main}, our proposed paradigm, which does not involve modeling syntax information (except for our syntax-based variants), consistently exceeds the performance of syntax-based models. Furthermore, our tree-based and positional models achieve better performance than our syntax-based model across three datasets.

However, syntax-based methods present certain challenges. Firstly, these methods often grapple with the unresolved token-node alignment problem. Secondly, syntax-based learning in ABSA is inefficient due to the requirement for additional adjacent matrix modeling. Although Dai et al. (2021) propose using a pre-trained language model to alleviate the alignment issue, the approach still incurs a startup cost for fine-tuning a tree inducer. Table \ref{app:tab_rq4} presents the resource utilization of our model, demonstrating that syntax structure-based models require additional computational resources and time. In conclusion, our model exhibits superior performance compared to dependency learning-based methods in aspect sentiment coherency modeling.
