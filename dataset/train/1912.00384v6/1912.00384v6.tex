\section{Experiments}
\label{sec:exp}

\subsection{Experimental setup}
\label{Sec:setup}

\head{Unlabeled set .}
We choose the standard object detection datasets PASCAL VOC 2007 and 2012~\cite{pascalvoc} {for the unlabeled set}, having 20 classes. Each dataset contains a \emph{trainval} set and a \emph{test} set. For VOC 2007, the \emph{trainval} set has 5011 images and the \emph{test} set 4952 images. For VOC 2012, the size of \emph{trainval} and \emph{test} sets are 11540 and 10991, respectively.
{We use the \emph{trainval} sets as  to train the object detector by default.  We evaluate the detector on the \emph{test} set.}
{Importantly, except for the support set, we do not use any labels, not even image-level labels in the training set.}


\begin{figure*}[t]
	\centering
	\includegraphics[width=1\textwidth]{google_support_example.pdf} \3pt]
	\includegraphics[width=1\textwidth]{voc_support_example.pdf}
	\caption{\small (Top) examples of top-ranking web images, using class names as queries. (Bottom) random selection of images from PASCAL VOC 2007.  
	}
	\label{Fig:google}
\end{figure*}


\head{Support set .}
\iavr{Each image in the support set  should depict one of the known  classes (\emph{i.e.} 20 VOC classes). \iavr{A preferable way to collect  is from the web~\cite{modolo2017pami}: we use the class names as text queries and collect the top- results per class from web image search (\eg~Google).
		The motivation is that these images}
	are clean, \emph{i.e.} they mostly contain objects against a simple background and in a canonical pose and viewpoint, without clutter or occlusion (see examples in Fig.~\ref{Fig:google} (top)). Notwithstanding, they are not perfect, lacking diverse appearance and poses of the object class.
	Collecting images from the web is easy
	and does not need any human effort.
	We choose this option by default.
}

{Another common
	option
	is to randomly
	sample  images per class
	from an existing collection~\cite{ShHX15,marvaniya2012drawing} (\emph{e.g.} VOC 2007). This is a harder setting,
	as these images may depict small objects, multiple instances, object classes in non-canonical pose, clutter and occlusion, \eg bottle, chair, and person in Fig.~\ref{Fig:google} (bottom). }
We experiment with both options.



\head{Networks.} We choose VGG16~\cite{vgg} as our student  by default, which is consistent with most WSOD methods~\cite{wsddn,tang2017cvpr,tang2018eccv,shen2019cvpr,tang2018pami}.
{Since the teacher network  (including the feature extractor )} is not used at inference time, we choose the more powerful ResNet-152~\cite{resnet}.
Both networks are pre-trained on the ILSVRC classification task~\cite{RDS+14}.

\head{Implementation details.} We use  images per class by default for . Following representative WSOD methods~\cite{wsddn,tang2017cvpr,tang2018pami,zhang2018cvpr}, we adopt \emph{edge boxes}~\cite{edgeboxes} to extract  proposals on average per image in . For the default teacher model , we first resize the input image to  pixels on the short side and then crop it to . We set the batch size to  and the learning rate to  initially with cosine decay. For the default student model , we feed the network with one image per batch. The training lasts for  iterations in total; the learning rate starts at  and decays by an order of magnitude at  iterations.

\begin{table*}[t]
	\footnotesize
	\centering
	\setlength{\tabcolsep}{2pt}
	{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			\ours  & 57.9 & {59.7} & 43.2 & {10.5} & 13.1 & 62.7 & 58.6 & 43.9 & 10.6 & 51.1 & \textbf{25.7} & 49.8 & 39.3& {60.6} & 14.9 & 10.9 & {33.5} & 45.2 & {42.5} & \textbf{27.8} &{38.0} \\  \ours (07+12) & 51.5& \textbf{65.2} & \textbf {48.9} &  \textbf{13.2} & \textbf{19.7} & \textbf{64.8} & \textbf{59.3} & \textbf{55.5} & \textbf{12.4} & \textbf{59.3} & 24.3 & \textbf{54.1} & \textbf{47.4} & \textbf{62.8} & \textbf{20.7} & \textbf{15.0} & \textbf{39.5} & \textbf{51.3} & \textbf{53.8} & 21.4 & \textbf{42.0} \\
			\midrule
			\iavr{NS-FT} & 56.7 & 37.2 & 31.8 & 10.7 & 4.6 & 44.7 & 42.7 & 51.4 & 3.5 & 17.7 & 4.2 & 37.6 & 22.5 & 51.6 & 13.1 & 10.0 & 28.9 & 36.3 & 39.2 & 14.3 & \iavr{27.9} \\
			\iavr{NS-NN} & 59.2 & 33.3 & 28.3 & 22.5 & 5.4 & 43.7 & 39.3 & 32.3 & 2.3 & 40.1 & 7.5 & 42.2 & 34.2 & 33.2 & 12.6 & 7.7 & 30.5 & 31.1 & 47.6 & 13.7 & \iavr{28.3} \\
			NS-MT-v1& 49.6 & 33.9 & 29.6 & 15.5 & 9.5 & 47.9 & 32.9 & 49.1 & 0.2 & 13.2 & 21.1 & 34.4 & 19.7 & 31.5 & 9.6 & 9.9 & 35.6 & 43.1 & 38.9 & 15.0 & 27.0\\
			NS-MT-v2 & 46.6 & 22.5 & 25.6 & 7.4 & 4.2 & 49.0 & 35.4 & 71.4 & 0.4 & 25.0 & 22.5 & 56.7 & 38.3 & 58.8 & 6.9 & 10.3 & 27.0 & 59.1 & 22.9 & 6.0 & 29.8 \\
			\midrule
			WSDDN~\cite{wsddn}& 39.4 &  50.1 & 31.5 & 16.3 & 12.6& 64.5  & 42.8 & 42.6 & 10.1 & 35.7 &  24.9 &  38.2 &  34.4 &  55.6 &  9.4  & 14.7 &  30.2 & 40.7& 54.7 & 46.9 & 34.8 \\
			OICR~\cite{tang2017cvpr} & \textbf{58.0} & 62.4 & 31.1 & 19.4  & 13.0  & \textbf{65.1} & 62.2 & 28.4 &  24.8& 44.7 & 30.6& 25.3 & 37.8 & 65.5 & 15.7 &24.1 &41.7& 46.9& \textbf{64.3}& 62.6& 41.2\\
			WSRPN~\cite{tang2018eccv} & 57.9 & \textbf{70.5}& 37.8 &5.7 &21.0& 66.1& \textbf{69.2}& 59.4 &3.4 &\textbf{57.1}& \textbf{57.3}& 35.2& \textbf{64.2}& \textbf{68.6}& \textbf{32.8}& \textbf{28.6}& \textbf{50.8}& 49.5& 41.1& 30.0& 45.3 \\
			PCL~\cite{tang2018pami} & 54.4& {69.0} & 39.3& 19.2& 15.7& 62.9& 64.4& 30.0& \textbf{25.1}& 52.5& 44.4& 19.6 & 39.3& 67.7 & 17.8& 22.9& 46.6& \textbf{57.5} & 58.6& \textbf{63.0} & 43.5\\
			WS-JDS~\cite{shen2019cvpr} & 52.0 & 64.5 & \textbf{45.5} & \textbf{26.7} & \textbf{27.9}& 60.5& 47.8& \textbf{59.7}& 13.0 & 50.4& 46.4& \textbf{56.3}& 49.6& 60.7 & 25.4 & 28.2 & 50.0 & 51.4& 66.5& 29.7 & \textbf{45.6}\\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{Detection mAP on the \emph{test} set of PASCAL VOC 2007. \ours: our nano-supervised object detection framework;
		\iavr{NS-FT: nano-supervised fine-tuning; NS-NN: nano-supervised nearest neighbor; NS-MT: Nano-supervised mean teacher}.
		Unless otherwise stated, \ours, NS-FT, NS-NN use  support images per class by default. All compared methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr} use the image-level labels in the unlabeled set ; \ours,
		\iavr{NS-FT, NS-NN and NS-MT}
		do not.}
	\label{tab:det_map_voc2007}
\end{table*}

\yannis{
	\head{Evaluation protocol.}
	We evaluate the performance of our NSOD framework
	on both image classification and object detection.
	For image classification, we measure the \emph{average precision} (AP) and \emph{mean AP} (mAP) for \emph{multi-class predictions}~\cite{wei2016pami,zhu2017cvpr}, as well as the accuracy of the top-1 class prediction per image on the \emph{trainval} set of .
	For object detection, we quantify
	localization performance
	on the \emph{trainval} set by \emph{CorLoc}~\cite{wsddn,shi2016eccv,tang2017cvpr,zhang2018cvpr} and detection performance on the \emph{test} set by mAP.
	At test time, the detector can localize multiple instances of the same class per image and mAP is identical to what is used to evaluate fully supervised object detectors with an IoU threshold of 0.5. By using the same IoU threshold, we measure the recall rate of \emph{edge boxes} over ground truth to be 92.51\% and 91.27\% on VOC 2007 and 2012 respectively. This shows the capacity of \emph{edge boxes} to cover most ground truth regions.
	
	\head{Evaluation scenarios.}
	Below, we first present the object detection results on the \emph{test} set of  under two scenarios: \emph{support set  by web search} (\autoref{sec:web}) and \emph{by sampling VOC 2007} (\autoref{sec:voc}). Then in the ablation study (\autoref{sec:ablation}), we provide detection and classification results on the \emph{trainval} set of  using the web search scenario.
}

\subsection{{Support set  by web search}}
\label{sec:web}
{We first collect the support set by web search and evaluate our NSOD on both VOC 2007 and 2012. We also combine the two sets as well as images from ImageNet as distractors to evaluate our method in the wild.}

\subsubsection{Results on VOC 2007}\label{sec:results-voc2007}
\head{Comparison to weakly-supervised methods.}
We compare to several representative WSOD methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr} in Table~\ref{tab:det_map_voc2007}. For fair comparison, all these methods use the same VGG16 backbone as we do, without bells and whistles.
\ours requires no annotation on the unlabeled set , while weakly-supervised methods assume image-level labels for all images in .

One directly competing method is PCL trained on gro\-und truth image-level labels in . Despite using no annotation on , \ours achieves an mAP that is only 5.5\% below that of PCL (38.0 \vs 43.5). The result is is also competitive to other methods, \eg OICR~\cite{tang2017cvpr}, WSRPN~\cite{tang2018pami}. {There are also WSOD methods employing large-scale web images/videos as extra data. For instance, \cite{tao2018tmm} and \cite{singh2019cvpr} build on the WSDDN pipeline~\cite{wsddn} and produce mAP 36.8 and 39.4 on VOC 2007, respectively. Unlike these works, our \ours uses few web images, an unlabeled set, and an advanced WSOD pipeline. Importantly, \ours also delivers competitive mAP.}
Fig.~\ref{Fig:result} gives some examples of detection results of \ours on PASCAL VOC 2007.

\head{Comparison to semi-supervised methods.}
\iavr{
	We first compare NSOD to two semi-supervised baselines: (1) fine-tune the teacher  on  as a -way classifier and use it to make predictions on , referred to as NS-FT; (2) use  from  as a global feature extractor to find the nearest neighbor in  for each image in , referred to as NS-NN. In both cases, we use the same support set with NSOD, infer image-level -way pseudo-labels on  and use them to train the student  by PCL. As shown in Table~\ref{tab:det_map_voc2007}, NS-FT and NS-NN deliver a mAP of 27.9 and 28.3, respectively.} \miaojing{Comparing to the mAP 38.0 of NSOD on the same setting (), these baselines are not satisfactory.
	This is due to the limited the supervision from the support set and justifies our choice of propagating labels from region to image level. }

We then adapt the \emph{mean teacher}~\cite{TV17} semi-supervised classification method to our setting. We use it in two ways: 1) using image-level class probabilities ~ in Eq.(\ref{eq:avg}), we select the top- scored images as positive for each class and the rest we treat as negative.
With those pseudo-labels, we train PCL on VGG16, applying the consistency loss of~\cite{TV17} to image-level predictions on . We call this \emph{nano-supervised mean teacher - variant 1} (NS-MT-v1).
We choose  as it works the best in practice. NS-MT then yields an mAP of 27.0 as shown in Table~\ref{tab:det_map_voc2007}. This result is lower than our \ours by {11.0\%} ({27.0} \vs 38.0); 2) using the labeled support set  and unlabeled set , we train the teacher model  as a mean-teacher by applying the cross-entropy loss on G and consistency loss between G and X. The rest pipeline remains as in NOSD. We name this \emph{nano-supervised mean teacher - variant 2} (NS-MT-v2) and obtain an mAP of 29.8 \vs 38.0 of NSOD. Both variants of NS-MT are clearly inferior to NSOD, which suggests that it is not straightforward to transfer a successful semi-supervised approach from the classification to the detection task.

We have also tried to directly infer object bounding
boxes on the \emph{test} set of VOC 2007 using naive approaches.
In particular:
\begin{enumerate}
	\item We use the -way classifier  trained on  (stage 3) to directly predict the class probabilities of object proposals per image in the test set.
	\item We adopt a naive -NN classifier by computing the feature similarities from images in the support set  to the object proposals per image in the test set.
\end{enumerate}
The mAP in both cases can be measured by ranking the proposals by their class probabilities. These methods fail, producing mAP lower than 10. We should emphasize the importance
of propagating similarity scores from region-level to image-level as we do in NSOD.


\subsubsection{Results on VOC 2012}
Using the same support set , we train an object detector with our \ours on VOC 2012. The mAP is reported on the \emph{test} set of VOC 2012 and compared to representative WSOD methods~\cite{tang2017cvpr,tang2018pami,zhang2018cvpr} in Table~\ref{tab:det_corloc_voc2012}.
Despite not using any VOC 2012 labels, \ours is only 4.0\% below PCL (36.6 \vs 40.6).


\begin{table*}[t]
	\footnotesize
	\centering
	\setlength{\tabcolsep}{2pt}
	{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			OICR~\cite{tang2017cvpr} & \textbf{67.7} & 61.2 &  41.5 &  \textbf{25.6} &  22.2 &  54.6 &  49.7 &  25.4 &  \textbf{19.9} &  47.0 &  18.1 &  26.0 &  38.9 &  67.7 &  2.0 & 22.6 &  41.1 &  34.3 &  37.9 & 55.3 &  37.9\\
			ZLDN~\cite{zhang2018cvpr}& 54.3 & 63.7 & 43.1 & 16.9 & 21.5 &  \textbf{57.8}&  \textbf{60.4} &  50.9&  1.2 & 51.5&  \textbf{44.4} &  36.6 & \textbf{ 63.6} &  59.3& 12.8& 25.6& \textbf{47.8}& \textbf{47.2}& 48.9& 50.6& \textbf{42.9} \\
			PCL~\cite{tang2018pami} & 58.2 & \textbf{66.0} & 41.8 & 24.8 & \textbf{27.2}& 55.7 & 55.2& 28.5 & 16.6& 51.0 & 17.5 & 28.6& 49.7& \textbf{70.5}& 7.1& \textbf{25.7}& 47.5 & 36.6 & 44.1& \textbf{59.2} & 40.6 \\
			\midrule
			\ours & 56.3 & 27.6 & 42.2 & 10.9 &  23.8 & 55.1 & 46.2 & 36.6  & 5.6  & 51.8 & 15.5 & 55.9 & 54.0 & 63.6 & \textbf{23.5} & 10.8 & 43.1 & 39.2 & \textbf{49.0} & 21.5 & 36.6 \\
			\ours (07+12) &  57.3 & 50.7 & \textbf{49.2} & 11.3 & 21.2 & 56.8 & 46.4 & \textbf{55.0} &  6.6 & \textbf{52.7} & 12.8 & \textbf{61.8} & 45.8 & 64.7 & 18.9 & 10.5 & 34.9 & 41.0 & 48.1 & 19.9 & 38.6\\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{Detection mAP on \emph{test} set of PASCAL VOC 2012. Our \ours uses  support images per class. All compared methods~\cite{tang2017cvpr,tang2018pami,zhang2018cvpr} use the image-level labels in the unlabeled set ; our \ours does not.}
	\label{tab:det_corloc_voc2012}
\end{table*}

\begin{table*}[t]
	\footnotesize
	\centering
	\setlength{\tabcolsep}{2pt}
	{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			{\ours (07+Dis5k)}   & 59.3 & 35.4 & 37.6 & 16.6 & 7.5 & 59.1 & 59.0 & 42.2 & 9.0 & 47.4 & 33.2 & 50.8 & 46.3 & 52.4 & 15.1 & 18.7 & 44.2 & 50.3 & 51.6 & 35.3 & 37.6\\
			{\ours (07+Dis10k)}  & 56.5 & 36.0 & 34.6 & 12.7 & 5.7 & 56.6 & 56.2 & 40.1 & 8.5 & 44.9 & 31.1 & 46.0 & 41.6 & 55.1 & 15.7 & 15.1 & 39.9 & 46.8 & 47.6 & 31.2 & 36.5 \\
			{\ours (07+Dis15k)} & {57.1} & {56.4} & {18.5} & {17.2} & {15.1} & {62.0} & {58.3} & {44.6} & {8.4} & {41.8} & {30.0} & {49.1} & {40.8} & {59.7} & {19.1} & {12.3} & {29.3} & {34.3} & {36.7} & {29.1} & {36.0} \\
			{\ours (07+Dis20k)} & {59.0} & {46.7} & {21.0} & {16.7} & {11.0} & {61.0} & {57.9} & {56.1} & {10.6} & {38.0} & {32.0} & {53.1} & {44.0} & {57.3} & {16.3} & {13.4} & {29.4} & {23.4} & {35.4} & {30.6} & {35.7} \\
			\ours (07+12+Dis5k)  & 59.8 & 65.8 & 50.1 & 12.5 & 16.5 & 58.6 & 52.1 & 57.0 & 15.8 & 51.1 & 31.5 & 53.9 & 36.4 & 58.8 & 18.1 & 15.4 & 43.3 & 50.4 & 48.1 & 38.8 & {41.7} \\
			\ours (07+12+Dis10k) & 51.4 & 68.1 & 36.1 & 11.8 & 17.7 & 59.6 & 63.1 & 61.8 & 10.2 & 46.5 & 32.1 & 57.0 & 37.1 & 61.3 & 17.7 & 17.1 & 44.0 & 47.7 & 44.9 & 33.0 & {40.9} \\
			{\ours (07+12+Dis15k)} & {54.7} & {52.2} & {29.0} & {18.7} & {18.4} & {63.6} & {60.2} & {44.4} & {8.9} & {57.1} & {29.0} & {58.7} & {49.2} & {60.8} & {20.3} & {13.0} & {44.1} & {48.7} & {44.9} & {38.1} & {40.7} \\
			{\ours (07+12+Dis20k)} & {59.2} & {53.6} & {33.7} & {12.3} & {18.6} & {59.6} & {55.3} & {44.3} & {9.7} & {50.8} & {35.3} & {50.4} & {53.5} & {58.7} & {22.3} & {15.4} & {39.4} & {45.8} & {40.4} & {42.9} & {40.2} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{Detection mAP on the \emph{test} set of PASCAL VOC 2007 in the presence of distractors. \ours: our object detection framework.}
	\label{tab:det_map_ablation}
\end{table*}

\subsubsection{Results on VOC 2007 + 2012}\label{sec:Results-2007-2012}
Because  is unlabeled and our method is computationally efficient, we can easily improve performance by simply using more unlabeled data.
As shown in Table~\ref{tab:det_map_voc2007} and~\ref{tab:det_corloc_voc2012}, if we train \ours on the union of VOC 2007 and VOC 2012 (07+12) on a large-scale, the mAP can be further improved on the \emph{test} set of both VOC 2007 and 2012. For instance, on VOC 2007, NSOD (07+12) yields a mAP of 42.0, which is an improvement by +4\% over using VOC 2007 alone. Since neither set is labeled, this improvement comes at almost no cost. This result is only 1.5\% below PCL (42.0 \vs 43.5), 
and even outperforms WSDDN~\cite{wsddn} and OICR~\cite{tang2017cvpr} when trained on VOC 2007 with image-level labels. This is a strong result that confirms the value of our core contribution; similarly, on VOC 2012, NSOD (07+12) increases the mAP to 38.6, now outperforming OICR.


\subsubsection{Results on PASCAL VOC + \yannis{Distractors}}
\miaojing{
	Despite being used without labels, VOC 2007 and 2012 are still \emph{curated}, \ie images depict at least one of the target classes. To further validate the effectiveness of our methd, we experiment with unlabeled data in the wild for , \ie, using images depicting unknown rather than target classes. In particular, we randomly select 5k, 10k, 15k and 20k images from ImageNet~\cite{RDS+14} and use the union of this set and VOC 2007, denoted by 07+Dis5k, 07+Dis10k,  07+Dis15k, and  07+Dis20k, as . Although there may be overlap between the 1000 ImageNet classes and the 20 PASCAL VOC classes, these images mostly contain unknown classes and play the role of distractors. The evaluation is on the test set of VOC 2007. As shown in Table~\ref{tab:det_map_ablation}, 07+Dis5k yields a mAP of 37.6, which almost retains the performance of using VOC 2007 alone as  (38.0). Further increasing the distractor set causes very little performance drop. For instance, the mAP for NSOD (07+Dis15k) on VOC 2007 is 36.0, which is -0.5\% compared to that of NSOD (07+Dis10k); while for NSOD (07+Dis20k), only -0.3\% is further observed upon NSOD (07+Dis15k). Considering the unlabelled set of VOC 2007 plus distractors as a whole, this shows our method is able to discover the relevant data and filter out most of the distractors, despite the distractor set being much larger than the curated set.}

Furthermore, we add the union of 5k/10k/15k/20k images from ImageNet and 10k images from VOC 2012 to VOC 2007, denoted by 07+12+Dis5k/10k/15k/20k, which achieves mAP 41.7/40.9/40.7/40.2. Similar to above, the performance drop by adding more distractors is also very small. These results are only slightly lower than that of NSOD (07+12) (mAP 42.0, Table 1), indicating our method mostly ignores distractors. We find that the distractors are mostly assigned no pseudo-labels due to thresholding of ~\eq{avg} in \ours. In other words, within the additional noisy unlabeled data (12+Dist5k,10k,15k,20k), our method discovers the relevant data (12) and uses them to improve from mAP 38.0 (07 alone), while mostly ignoring the irrelevant data (\ie Dis5k,10k,15k,20k). The additional noisy unlabeled data is meant to represent data in the wild, which can be obtained for free. Hence, depending on the ratio of labelled to unlabelled data, it is possible to improve the detection performance with no annotation cost.


\begin{figure*}[t]
	\centering
	\includegraphics[width = 1.0\textwidth]{img3-rows2-4.pdf}
	\caption{\small Detection results of \ours on PASCAL VOC 2007, using default settings ().
		Top 2 rows: positive results (red boxes). Bottom row: failure cases (white boxes).
	}
	\label{Fig:result}
\end{figure*}


\begin{figure}[t]
	\centering
	\includegraphics[width = 0.9\linewidth]{voc_support.pdf}
	\caption{\small Detection mAP of \ours, NS-Base, NS-FT and PCL on PASCAL VOC 2007, using different number  of images per class as support set.
	}
	\label{Fig:result_voc_support}
\end{figure}


\begin{table*}
	\footnotesize
	\centering
	\setlength{\tabcolsep}{2pt}
	{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			\oursg & 88.8 & 85.8 & 98.0 & 67.8 & 79.4 & 68.4 & \textbf{96.8} & 95.1 & \textbf{80.6} & 72.1 & 38.9 & 93.4 & \textbf{82.3} & 65.2 & 98.0 & 56.7 & 70.1 & 55.6 & 72.0 & 60.2 & 76.3\\
			\oursx  & 86.4 & \textbf{96.9} & 97.1 & \textbf{71.4} & \textbf{98.5} & 67.1 & 89.9 & 95.1 & 80.0 & 66.8 & 36.5 & 92.9 & 74.2 & 62.9 & 96.9 & 53.1 & 59.9 & 58.8 & 70.1 & \textbf{78.9} & 76.7\\
			\ours & \textbf{91.2} & 90.7 & \textbf{98.0} & 71.1 & 94.3 & \textbf{73.8} & 95.8 & 95.5 & 80.5 & \textbf{74.7} & \textbf{39.1} & \textbf{95.3} & 81.2 & \textbf{66.9} & \textbf{98.4} & \textbf{58.7} & \textbf{73.8} & \textbf{59.7} & \textbf{75.6} & 70.4 & \textbf{79.2}\\
			\midrule
			\textsc{Method} & aero & bike & bird & boat & bottle & bus & car & cat & chair & cow & table & dog & horse & mbike & persn & plant & sheep & sofa & train & tv & mAcc \\
			\midrule
			\oursg & 92.2 & \textbf{97.7} & 99.1 & 78.7 & 100.0 & 73.0 & 93.2 & \textbf{98.5} & \textbf{89.2} & 82.0 & 41.8 & \textbf{97.7} & 77.7 & 72.0 & \textbf{99.7} & 63.7 & 68.7 & 63.0 & 77.5 & 87.5 & 82.7\\
			\oursx &  93.1 & 93.4 & 98.2 & 79.2 & 100.0 & 78.9 & 96.3 & 96.7 & 84.0 & 83.5 & 45.1 & 95.7 & 84.2 & 72.9 & 98.5 & 73.3 & 77.2 & 66.1 & 83.3 & \textbf{87.7} & 84.3\\
			\ours & \textbf{93.8} & 92.4 & \textbf{99.3} & \textbf{80.4} & \textbf{100.0} & \textbf{81.1} & \textbf{97.8} &{97.1} & 78.6 & \textbf{86.7} & \textbf{49.7} & 97.1 & \textbf{88.2} & \textbf{77.2} & 99.6 & \textbf{79.7} & \textbf{79.1} & \textbf{67.9} & \textbf{87.5} & 85.6 & \textbf{85.9} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{Classification mAP for multi-class prediction (top) and classification mAcc for top-1 class prediction (bottom) on the \emph{trainval} set of PASCAL VOC 2007. \ours: our Nano-supervised object detection framework.}
	\label{tab:cls_map_voc2007_ablation}
\end{table*}

\begin{table*}[t]
	\footnotesize
	\centering
	\setlength{\tabcolsep}{2pt}
	{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			\oursg & 57.2 & 52.7 & 36.0 & 14.1 & 11.0 & 50.6 & 46.9 & 35.8 & 5.7 & 47.1 & 16.1 & 52.8 & 34.3 & 54.4 & 14.8 & 11.4 & 29.0 & 48.8 & 43.4 & 13.9 & 33.9 \\
			\oursx & \textbf{58.5} & 51.5 & 37.5 & 11.6 & 10.6 & 55.3 & 48.2 & 40.4 & 5.8 & 49.9 & 16.0 & 51.3 & 31.6 & 56.3 & 14.6 & 9.0 & 34.3 & 45.5 & 42.2 & 20.3 & 34.5\\
			\ours  & 57.9 & {59.7} & 43.2 & {10.5} & 13.1 & 62.7 & 58.6 & 43.9 & 10.6 & 51.1 & \textbf{25.7} & 49.8 & 39.3& {60.6} & 14.9 & 10.9 & {33.5} & 45.2 & {42.5} & \textbf{27.8} &{38.0} \\  \midrule
			\ours () & 53.0 & 58.0 & 24.4 & 13.3 & 11.3 & 41.3 & 43.8 & 43.6 & 2.3 & 50.3 & 6.1 & 32.4 & 19.0 & 50.5 & 15.0 & 8.7 & 35.7 & 41.7 & 42.8 & 6.2 & 30.0
			\\
			\ours () & 57.2 & 27.8 & 40.4 & 9.7 & 11.2 & 61.2& 57.0 & 25.9 & 13.4 & 47.2 & 6.2 & 45.5 & 35.7 & 53.0 & 21.2 & 14.1 & 34.8 & 43.7 & 39.8 & 19.8 & 33.2\\
			\ours ()  & 57.9 & {59.7} & 43.2 & {10.5} & 13.1 & 62.7 & 58.6 & 43.9 & 10.6 & 51.1 & \textbf{25.7} & 49.8 & 39.3& {60.6} & 14.9 & 10.9 & {33.5} & 45.2 & {42.5} & \textbf{27.8} &{38.0} \\ 
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{\emph{Ablation study}. Detection mAP on the \emph{test} set of PASCAL VOC 2007. \ours: our nano-supervised object detection framework.}
	\label{tab:det_map_voc2007_ablation}
\end{table*}


\begin{table*}[!]
	\footnotesize
	\centering
	\setlength{\tabcolsep}{2pt}
	{
		\begin{tabular}{cccccccccccccccccccccc}
			\toprule
			\textsc{Method} & aero & bike & bird & boat & bott & bus & car & cat & char & cow & tabl & dog & hors & mbik & prsn & plat & shep & sofa & tran & tv & mAP \\
			\midrule
			WSDDN~\cite{wsddn}& 65.1 & 58.8 & 58.5 & 33.1 & 39.8 & 68.3 & 60.2 & 59.6&  34.8 & 64.5&  30.5&  43.0&  56.8&  82.4& 25.5& 41.6& 61.5& 55.9& 65.9& 63.7& 53.5 \\
			OICR~\cite{tang2017cvpr} & 81.7 & 80.4& 48.7 & \textbf{49.5} & 32.8&  81.7 & 85.4 & 40.1 & \textbf{40.6}&  79.5 &  35.7 &  33.7 &  60.5&  88.8 &  21.8&  57.9&  76.3&  59.9&  75.3&  \textbf{81.4}&  60.6\\
			WSRPN~\cite{tang2018eccv} & 77.5 &{81.2}& 55.3& 19.7& 44.3 &80.2& \textbf{86.6} &\textbf{69.5} &10.1 &\textbf{87.7} &\textbf{68.4} &52.1 &\textbf{84.4}& \textbf{91.6}& \textbf{57.4} &\textbf{63.4} &\textbf{77.3} &58.1 &57.0 &53.8 &63.8 \\
			PCL~\cite{tang2018pami} & 79.6 & \textbf{85.5}& 62.2 & 47.9 & 37.0 & \textbf{83.8 }& {83.4} & 43.0 & 38.3 &  80.1 & 50.6 & 30.9&  57.8 &  90.8&  27.0& 58.2& 75.3& \textbf{68.5}& 75.7& 78.9& 62.7\\
			WS-JDS~\cite{shen2019cvpr} & \textbf{82.9} & 74.0& \textbf{73.4}& 47.1& \textbf{60.9}& 80.4& 77.5& 78.8& 18.6& 70.0 &56.7& 67.0 &64.5& 84.0& 47.0& 50.1& 71.9& 57.6 &\textbf{83.3} &43.5& \textbf{64.5}\\
			\midrule
			\ours & 80.0 & 73.3 & 66.1& 34.0 & 29.0& 72.6& 76.5& 56.4 & 17.7& 74.7& 47.5& 61.4& 60.5& 86.4& 31.9& 36.6& 60.8& 59.1 & 57.4& 49.1& 56.6\\
			\ours (07+12) & 78.3 & 78.4 & 70.3 & 34.0 & 34.0 & 75.1 & 76.6 & 66.9 & 24.8 & 76.0 & 45.6 & \textbf{69.8} & 67.7 & 88.8 & 34.4 & 41.4 & 67.0 & 62.1 & 67.3 & 40.9 & 60.0\\
			\bottomrule
		\end{tabular}
	}
	\vspace{3pt}
	\caption{CorLoc on the \emph{trainval} set of PASCAL VOC 2007. All compared methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr} use the image-level labels in ; our \ours does not.}
	\label{tab:det_corloc_voc2007}
\end{table*}


\subsection{{Support set by sampling VOC 2007}}
\label{sec:voc}

{As discussed in Sec.~\ref{Sec:setup}, the support set  can be collected by randomly selecting  images per class from the unlabeled set . This is more challenging than web search, as one image may depict more than one object, as shown in Fig.~\ref{Fig:google}. We randomly sample  images per class from
	VOC 2007 with image-level labels as  and evaluate on its \emph{test} set.} 
We compare \ours with two baselines: (1) only using  to train the student , denoted by NS-Base; (2) using NS-FT as described in Sec.~\ref{sec:results-voc2007}.

{As shown in Figure~\ref{Fig:result_voc_support}, \ours yields significantly higher mAP at every  compared to the baselines. In particularly, with small , our improvement is substantial; with  (around 30\% of VOC 2007 training data), \ours achieves accuracy already very close (on par) to PCL~\cite{tang2018pami} (dotted horizontal line) that uses image-level labels of 100\% data in VOC 2007.}


\subsection{Ablation Study}
\label{sec:ablation}
\yannis{
	We conduct the ablation study
	on our labeling strategy, support set size, and localization on the \emph{trainval} set of PASCAL VOC 2007. The support set  is collected by web search.
	
	\subsubsection{Labeling strategy (classification)}
	Referring to \autoref{Sec:teacherstudent} in the paper, we ablate combining  and  to generate image-level pseudo-labels.  is computed based on  alone, while  is computed based on the teacher model trained on . We apply a hard threshold of  on the predicted class probabilities of  and  to generate two sets of image-level pseudo-labels. We train two different models separately on the two sets of pseudo-labels, which we denote by \oursg and \oursx, respectively.
	
	The classification accuracy of the two sets of pseudo-labels is first evaluated on the \emph{trainval} set of VOC 2007 and shown in Table~\ref{tab:cls_map_voc2007_ablation}. It can be seen that \oursg and \oursx produce a similar classification mAP of  \vs , while the AP on individual classes differs. However, in terms of top-1 class accuracy, \oursx is better than \oursg. This is reasonable, as \oursx is fine-tuned as a -way classifier, which takes the top-1 class predictions of  as pseudo-labels. The two sets of pseudo-labels are complementary by averaging  and  according to~Eq.(4), denoted by \ours. This improves both multi-class and top-1 class predictions, reaching the highest scores of  and , respectively.
	
	\subsubsection{Labeling strategy (detection)}
	To further investigate the complementary effect of the two models \oursg and \oursx, we evaluate their detection result on the \emph{test} set of VOC 2007 (Table~\ref{tab:det_map_voc2007_ablation}). The mAP of \oursx (34.5) is slightly greater than that of \oursg (33.9). Their combination (our full model \ours) further increases mAP by  to .
	The detection result on the \emph{test} set is consistent with the classification result on the \emph{trainval} set, which validates our idea of distilling knowledge from the support set to the unlabeled set and from the teacher to the student model.
	
	\subsubsection{Support set size} \label{sec:support_set_size}
	We evaluate performance for different number  of web images per class of the support set  in Table~\ref{tab:det_map_voc2007_ablation}: mAP is 30.0 for , 33.2 for  and 38.0 for .
	Further increasing  presumably brings more noisy examples.  How to deal with large-scale noisy web images/videos is an open problem~\cite{guo2018eccv,tao2018tmm,singh2019cvpr,liang2015cvpr}. We keep  small to avoid bringing too many noisy images, while at the same time using the unlabeled unlabeled set  for
	{more diversity.}
}


\yannis{
	\subsubsection{Localization on the trainval set}
	Apart from the mAP on the \emph{test} set, in Table~\ref{tab:det_corloc_voc2007} we report CorLoc on the \emph{trainval} set of VOC 2007, as is common for weakly-supervised detection methods~\cite{wsddn,tang2017cvpr,tang2018eccv,tang2018pami,shen2019cvpr}. Our NSOD delivers CorLoc 56.6, which is very close to other WSOD methods despite using no annotations on . 
	Like in \autoref{sec:Results-2007-2012}, if we train \ours on the union of VOC 2007 and VOC 2012 (07+12) on a large-scale, the CorLoc of \ours(07+12) on VOC 2007 (see Table~\ref{tab:det_corloc_voc2007}) is increased to 60.0, which is only 2.7\% below PCL (62.7) and generally among the best-performing WSOD methods (\eg OICR has 60.6).
}

