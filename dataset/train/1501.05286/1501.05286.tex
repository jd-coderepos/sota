\label{sec:TSVQ}

\begin{figure*}[t]
		\centering
{\begin{minipage}{0.8\columnwidth}
		\begin{algorithm}[H]
\label{fig:treeGrowingPseudoCode}
		\caption{Index generation}
    \begin{algorithmic}[1]
\Function{index}{data}
\State 													\Comment{Initialize model (root node)}
						\State  															\Comment{Label the dataset}
            \State 										\Comment{Get leaves list}
						\While{}									\Comment{Iterate for each leaf}
								\State 
						    \If{not }
							      \State 
									  \State 
										\State 
								\Else
										\State  \Comment{Prevent further bloomings}
								\EndIf
								
						\EndWhile
        \EndFunction
    \end{algorithmic}

		\end{algorithm}

		\end{minipage}}
		\caption{Pseudo code of the tree growing procedure used by the indexing subsystem.}
\end{figure*}



A crucial component for a large-scale CBIR system is an efficient  methodology for the generation of a global index. 
In the proposed tile-based retrieval system, the index is the result of an unsupervised hierarchical clustering subsystem that has the purpose to provide an efficient way to group image tiles that are characterized by similar scattering behavior and that are therefore all likely to be relevant to a specific query.
Without an index, the search engine would need to perform an  scan of the entire archive with unsustainable computational costs  for mining even moderate size archives.

In a ``big data" context, it is fundamental for the system to be able to perform indexing on large-scale datasets containing billions of entries, each representing the image content and eventually consisting of a large number of descriptors, and to allow fast and accurate retrieval results.
In order to generate the content-based index, the system has to analyze the set of tile descriptors generated by the feature extraction module.
Such analysis typically requires the application of machine learning algorithms.
However, indexing large scale archives of data requires suitable implementations, able to distribute the computation among multiple processors in the cluster. To this purpose, we propose a mechanism for scalable indexing based on Tree-Structured Vector Quantization~\cite{gersho1992vector}. 






















\subsubsection{Tree Structured Vector Quantization}



\begin{figure}[t]
	\centering
\includegraphics[width=0.5\columnwidth]{./figures/vectorQuantization}
	\caption{Encoding/decoding scheme in Vector Quantization. The encoder maps input vectors to a finite set of codewords and sends the codeword index through the channel. The decoder holds a lookup table and is able to find the original codeword based on the received index.}
	\label{fig:vectorQuantization}
\end{figure}

Vector Quantization is a signal processing technique that is popular in data compression. 
A generic schema of a vector quantizer consists of an encoding/decoding system.
Let us represent the generic entry (a set of tile descriptors, in our case) in the database  by a -dimensional feature vector .
The encoder maps the -dimensional vectors in the vector space  to a finite set of vectors , called code vectors or codewords, operating according to a nearest neighbor or minimum distortion rule. 
We here consider the squared error distortion, i.e. the  square of the Euclidean distance between the  input vector and the codeword:

In this way, each codeword  has an associated nearest neighbor region, also called a Voronoi region, defined by:

such that

and

Once a codeword is associated to an input -dimensional vector, the corresponding codeword index is sent to the decoding system through a channel (a file system or a communication link, depending on the application).
The decoder consists of a lookup table, i.e. a codebook containing all the possible codewords. When the index of a codeword is received, it will return the codeword corresponding to that index.
The schema of the encoding/decoding system is shown in Fig.~\ref{fig:vectorQuantization}.



Several different approaches can be considered in the  design a Vector Quantizer.
Tree-Structured Vector Quantization (TSVQ) is a class of constrained structure quantizers~\cite{cosman1993tree}.
In TSVQ, the codebook is constrained to have a tree structure.
The encoder builds the codeword associated to an input vector by performing a sequence of binary comparisons, following a minimum distortion rule at each branch, until a leaf (terminal) node is reached.
The path followed to reach the leaf node starting from the root indicates the binary sequence associated with the codeword.





A pseudo-code of the implementation of the tree-growing procedure is reported in~\ref{fig:treeGrowingPseudoCode}.
The tree structure is built starting from the root node. 
The root node is represented by the centroid of the entire set of feature vectors. From the root node, two new child nodes are estimated by applying the \mbox{-means} clustering algorithm, each corresponding to the centroid of the space partitions  minimizing the within-cluster sum of squares cost function:

where  represents the i\emph{th} partition centroid. Then, each data point is assigned to its respective centroid by a binary string labeling, so that each group of data points defines a sub-tree.
The process continues iterating on the discovered nodes, until a predefined stopping criterion is reached.
The centroids of the items belonging to the leaves of the final tree structure, each with an associated binary string, represent the entries of the index.
During the growing process, each split produces a decrease of the average distortion and an increase in the average length of the binary string.
Based on these observations, several conditions are possible as stopping criteria.
For example, the algorithm can stop growing a sub-tree (thereby defining a leaf node) when one of the following events occurs:
\begin{itemize}
	\item the node contains a predefined minimum number of data points  under which further partitions become untrustworthy;
	\item the distortion measure given by the within-cluster sum of squares of the data vectors associated with the node is below a minimum threshold ;
	\item the maximum height of the tree  or, equivalently, the maximum binary code length have been reached.
\end{itemize}




















\subsubsection{Scalable in-memory clustering}
\label{sec:inmem_clustering}

\begin{figure*}[t]
	\centering
\includegraphics[width=0.7\columnwidth]{./figures/mapRedClustColor}
	\caption{General MapReduce implementation of the \mbox{-means} algorithm. The assignment and the update step can be implemented by a sequence of map\&combine and reduce phases. Tile descriptors stored in a distributed storage system are given as input to mappers together with the current centroids estimates. Each mapper performs the assignment step, putting out a key/value pair for each input vector, with the value being the input vector and the key being its nearest centroid. A combiner performs then a group-by-key, coupling to each centroid-key the sum and the number of its associated vectors. Reducers finally aggregate the sets of key/value pair to provide updated estimates of centroids.}
	\label{fig:mapRedClust}
\end{figure*}


The proposed indexing mechanism is based on a scalable version of the TSVQ algorithm. 
The method builds on a distributed implementation of the \mbox{-means} algorithm to estimate the optimal space partitioning, allowing us to parallelize the clustering operations among the worker nodes of the computing cluster. 


A schema of a possible MapReduce implementation of the \mbox{-means} algorithm is shown in Fig.~\ref{fig:mapRedClust}. 
Input data points are stored on a distributed file system as key/value pairs. 
The mapping phase consists of the so called \emph{assignment} step. 
At the i\emph{th} iteration, the set of  centroid estimates  is globally broadcast to map workers. 
Each map function has in input a group of data points and iteratively evaluates the squared Euclidean distance between the data points and each of the centroids vectors. 
The outputs consist of a key/value pair for each data point, with the value being the input vector and the key being its nearest centroid. 
Each map worker can perform a combine operation, coupling to each centroid key the sum and the number of its associated vectors.
This last local combination operation is in principle optional, yet it allows to avoid using expensive network resources by compressing the information generated by each node.
The \emph{update} step of the \mbox{-means} algorithm is finally implemented by the reduce workers. 
Each reduce function takes in input the grouped-by-key intermediate key/value pairs and computes for each centroid the average of the associated values

where  is the current iteration and  is the set of vectors belonging to centroid .
Each average value represents an updated centroid to be used in successive iterations. 
The algorithm stops when the update variations are below a given threshold.

The assignment step of the algorithm requires to repeatedly access data points and to evaluate the distances between each value vector and the cluster centroids:

As the data points are fixed during iterations, the first term in the second line of the expression in Eq.~\ref{eq:inMemoryEuclidean} does not need to be recomputed at each iteration of the algorithm. 
Furthermore, also the third term, i.e. the centroid squared Euclidean norm, is repeatedly used inside each single iteration. 
This indicates the advantage of performing operations in-memory and avoiding, at each iteration, costly operations like both recomputing invariable quantities and storing and reading intermediate results on low performing devices. 
While standard cluster computing frameworks like Hadoop~\cite{white2009hadoop} routinely materialize to the distributed file system intermediate results produced by the directed acyclic graph of operations, with massive usage of cluster resources especially in the case of iterative processing schemes, in-memory ones like Apache Spark~\cite{zaharia2010spark, zaharia2012resilient} avoid this step and have therefore been selected as the base for our implementation.

Further improvements can be be obtained accounting for the sparsity of the feature vectors.
If the vectors are sparse, the computational complexity of performing \mbox{-means} decreases from  to , where  is the number of data points,  is the data space dimensionality,  is the number of clusters,  is the number of iterations needed for convergence and  is the (average) number of non-zero elements in each data point.






















\subsubsection{TSVQ Complexity}
\label{sec:TSVQComplexity}
The implemented TSVQ for scalable indexing processing works with a fixed number of clusters, . This choice allows the association of binary codes to the leaves of the tree, each with a length equal to the level of the corresponding leaf. 
As each leaf corresponds to a partition centroid, all the points will be labeled with the binary code of the leaf they belong to.
Given the tree structure generated by the TSVQ algorithm and a query vector, retrieving the nearest feature vectors consists of traversing the tree from the root node and choosing, at each branch, the nearest child node, until a leaf is reached. The retrieved data points are the ones with the same codeword as the final leaf.

There are two main motivations behind the choice of the TSVQ algorithm for the index formation. First, this approach leads to the definition of a binary indexed tree, a structure that allows efficient lookup and update operations. 
In particular, search and modification operations can be executed in constant or  logarithmic average times, instead of the  linear access times required by linked lists. 
Second, as described in Sec~\ref{sec:inmem_clustering}, the \mbox{-means} algorithm can be reimplemented to scale to massive sets of data.
In addition, scalable implementations of seeding strategies have been recently proposed, allowing for faster runs and more robust searches with respect to suboptimal solutions~\cite{bahmani2012scalable}. 

%
