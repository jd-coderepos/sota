\documentclass[10pt,journal,letterpaper,compsoc]{IEEEtran}
\ifCLASSOPTIONcompsoc
\else
\fi






\ifCLASSINFOpdf
\else
\fi































































\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{subfigure}
\usepackage{url}
\usepackage{amsmath}
\usepackage[normalem]{ulem}
\usepackage{epsfig,colortbl}
\usepackage{amssymb,comment}
\usepackage{enumerate}
\usepackage{times}
\usepackage{multirow,multicol}
\usepackage[ruled, vlined]{algorithm2e}
\newcounter{ctr}\setcounter{ctr}{0}
\newcommand{\stp}{\addtocounter{ctr}{1}\arabic{ctr}.}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{problem}{Problem}

\makeatletter

\newcommand{\redd}{\textcolor{red}}
\newcommand{\blu}{\textcolor{blue}}
\newcommand{\be}{}
\newcommand{\nn}{\nonumber}

\newcommand{\bm}{\boldmath}
\newcommand{\Nbar}{\bar N}
\newcommand{\lra}{\longrightarrow}
\newcommand{\lla}{\longleftarrow}
\newcommand{\mc}{\multicolumn}
\newcommand{\qed}{{\hfill}}
\newcommand{\C}{\mbox{\bm }}
\newcommand{\Cs}{\mbox{\scriptsize\bm }}
\newcommand{\cc}{\mbox{\bm }}
\newcommand{\m}{\mbox{\bm }}
\newcommand{\PP}{\mbox{\bm }}
\newcommand{\uu}{\mbox{\bm }}
\newcommand{\ccc}{{\bf c}}
\newcommand{\xx}{\mbox{\bm }}
\newcommand{\xxx}{{\bm x}}
\newcommand{\rr}{\mbox{\bm }}
\newcommand{\Z}{\mbox{\bm }}
\newcommand{\0}{{\bf 0}}
\newcommand{\N}{{\cal N}}
\newcommand{\zero}{\mbox{\bm  }}
\newcommand{\vv}{\mbox{\bm }}
\newcommand{\vvb}{\mbox{\scriptsize\bm }}
\newcommand{\yy}{\mbox{\bm }}
\newcommand{\G}{\mbox{\bm }}
\newcommand{\I}{\mbox{\bm }}
\newcommand{\A}{\mbox{\bm }}
\newcommand{\B}{\mbox{\bm }}
\newcommand{\bydef}{{\buildrel{\triangle}\over =}}
\newcommand{\tht}{\theta}
\newcommand{\Zhat}{{\hat Z}}
\newcommand{\yhat}{{\hat y}}
\newcommand{\dbar}{{\bar d}}
\newcommand{\cund}{{\underline c}}
\newcommand{\Nhat}{{\hat N}}
\newcommand{\mubar}{{\bar\mu}}
\newcommand{\sigmahat}{{\hat \sigma}}
\newcommand{\sigmabar}{{\bar \sigma}}
\newcommand{\si}{{\sigma}}
\newcommand{\rhohat}{{\hat \rho}}
\newcommand{\thetahat}{{\hat \theta}}
\newcommand{\xb}  {{\mbox{\boldmath }}}
\newcommand{\vb}  {{\mbox{\boldmath }}}
\newcommand{\mem}[1]{\label{eq:#1}}
\newcommand{\rec}[1]{(\ref{eq:#1})}
\newcommand{\nono}{\nonumber}
\newcommand{\scc}{{\cal S}}
\newcommand{\yc}{{\cal C}}
\newcommand{\Cb}  {{\mbox{\boldmath }}}
\newcommand{\li} [1]{\lim_{#1\rightarrow\infty}}
\newcommand\Bydef{~\bydef~}
\newcounter{step}
\newtheorem{property}{Property}

\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{1.5ex plus .2ex minus .3ex}{-0em}{\normalsize\bf}}




\begin{document}
\title{Progressive Decoding for Data Availability and Reliability in Distributed Networked Storage}


\author{Yunghsiang~S.~Han,~\IEEEmembership{Senior Member,~IEEE,}
	Soji~Omiwade,~\IEEEmembership{Member,~IEEE,}
        and~Rong~Zheng,~\IEEEmembership{Member,~IEEE}\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Han is with the Department of Electrical  Engineering, National Taiwan University of Science and Technology, Taiwan, R.O.C. (e-mail: yshan@mail.ntust.edu.tw)\IEEEcompsocthanksitem Omiwade and Zheng are with the Department of Computer Science, University of Houston, Houston, TX 77204 USA (e-mail: 
\{ooo00a,rzheng\}@cs.uh.edu.}}

\IEEEcompsoctitleabstractindextext{\begin{abstract}
To harness the ever growing capacity and decreasing cost of storage,
providing an abstraction of dependable storage in the presence of
crash-stop and Byzantine failures is compulsory.  We propose a
decentralized Reed Solomon coding mechanism with minimum communication
overhead.  Using a progressive data retrieval scheme, a data collector
contacts only the necessary number of storage nodes needed to guarantee
data integrity.  The scheme gracefully adapts the cost of successful data
retrieval to the number of storage node failures. Moreover, by
leveraging the Welch-Berlekamp algorithm, it avoids unnecessary
computations. Compared to the state-of-the-art decoding scheme, the
implementation and evaluation results show that our progressive data
retrieval scheme has up to  times better computation performance
for low Byzantine node rates.  Additionally, the communication cost in
data retrieval is derived analytically and corroborated by Monte-Carlo
simulation results. Our implementation is flexible in that the level of
redundancy it provides is independent of the number of data generating
nodes, a requirement for distributed storage systems.
\end{abstract}



\begin{keywords}
Reliability, Availability, Fault tolerance, Error control codes
\end{keywords}}


\maketitle


\IEEEdisplaynotcompsoctitleabstractindextext



\IEEEpeerreviewmaketitle


\section{Introduction}
\label{sect:intro}


\IEEEPARstart{C}{ost} of storage for data availability over networks has decreased drastically over the years.  Companies such
as Google and Amazon offer TB of online storage for free or at a very low
cost.  Also, low-power storage media are widely used in embedded devices
or mobile computers. However, to harness the ever growing capacity and
decreasing cost of distributed storage for persistent data availability, a number of challenges need to be
addressed, (i) volatility of storage due to network
disconnectivity, varying administrative restriction or user preferences, and
nodal mobility (of mobile devices); (ii) (partial) failures of storage devices.
For example, flash media are known to be engineered to trade-off error
probabilities for cost reduction; (iii) software bugs or malicious attacks, where an
adversary manages to compromise enough storage nodes to guarantee that integrity cannot be guaranteed.

To ensure availability and data integrity despite failure or compromise of storage nodes,
survivable storage systems spread data redundantly across a set of distributed
storage nodes. At the core of a survivable storage system is a coding scheme
that maps information bits to stored bits, and vice versa. 
The unit of such mapping are referred to as \emph{symbols} in this paper. A  coding is
defined by the following two primitives:
\begin{itemize}
\item[-] {\bf encode}  takes as input 
 information symbols  and
returns a coded
vector .
The coded symbols are stored on storage nodes, one per node.
\item[-] {\bf decode}  accesses a
subset of storage nodes and returns the original  information symbols from possibly
corrupted symbols.
\end{itemize}

Most existing approaches to survivable and dependable storage assume crash-stop behaviors.
That is, a storage device becomes unavailable if failed (also called ``erasure").
Solutions such as various RAID configurations~\cite{raid} and their extensions
are engineered for high read and write data throughput. In this case, typically
low-complexity (replication or XOR-based) coding mechanisms are employed to
recover the original data from limited degree of erasure. We argue that Byzantine failures, where
devices fail in arbitrary manner and cannot be \emph{trusted}, are becoming more
pertinent with the prevalence of cheap storage devices, software bugs and
malicious attacks. Efficient encode and decode primitives that can detect data
corruption and handle Byzantine failures serve as a fundamental building block to
support higher level abstractions such as multi-reader multi-writer atomic
register~\cite{Goodson04} and digital fingerprints~\cite{Krawczyk93} in dependable distributed systems. 

For fixed error correction capability, the efficiency of encode and decode
primitives can be evaluated by three metrics, i) {\it storage overhead}
measured as the ratio between the number of storage symbols and total
information symbols (); ii) {\it encoding and decoding computation time};
and iii) {\it communication overhead} measured in the number of bits
transferred in the network for encode and decode. Communication overhead is of
much importance in wide-area and/or low-bandwidth storage systems. Even though Reed-Solomon (RS) codes have been used for distributed storage for a single system, they have been found unsuitable for distributed networked storage due to their centralized nature~\cite{DIM06} and high communication overhead~\cite{lin2007dpl}. However, by encoding data at each data node, we found that RS codes can avoid the above disadvantages and provide better performance in almost every aspect than existing storage schemes. Hence, in this
paper, we propose a novel solution to spreading redundant information efficiently
across distributed storage nodes using incremental RS decoding.
By virtue of RS codes, our scheme is storage optimal. The key novelty of the proposed
approach lies in a progressive data retrieval procedure, which retrieves
just enough data from live storage nodes, and performs decoding
incrementally.  As a result, both communication and computation cost are
minimized, and adapt to the degree of errors in the system. We
provide a theoretical characterization of the communication cost and success
rate of data retrieval using the proposed scheme in presence of arbitrary
errors in the system. Our implementation studies demonstrate up to 20 times
speed-up of the progressive data retrieval scheme in computation time, relative
to a classical scheme. Moreover, the proposed scheme is comparable to that of a
genie-aid decoding process, which assumes knowledge of failure modes of storage
nodes.

In this paper, we make the following contributions:
\begin{itemize}
\item Design of a novel progressive data retrieval algorithm that is storage
and communication optimal, and computationally efficient. It handles Byzantine
failures in storage nodes gracefully as the probability of failures increases.
\item Development of an analytical model to evaluate the communication cost of
our data retrieval algorithm.
\item Eliminate the need for the number of data nodes, , to equal the number of
information symbols, --a constraint that is restrictive and unrealistic
for general distributed storage systems.
\end{itemize}




The rest of the paper is organized as follows. Related work is given in
Section~\ref{sect:related}. The progressive data retrieval scheme is presented
in Section~\ref{sect:progressive}, with the details of the incremental RS
decoding algorithm in Section~\ref{sect:algo}.  An analysis of our coding,
communication and success rate complexity is provided in
Section~\ref{sect:complexity}, and Section~\ref{sect:era_comp} compares our scheme with leading erasure coding protocols. Evaluation results are presented in
Section~\ref{sect:eval}.  Finally, we
conclude the paper in Section~\ref{sect:conc}.
\section{Background and Related Work}
\label{sect:related}
\begin{figure*}[thp]
\begin{center}
\includegraphics[width=6in]{figure/bch_decoder.eps}
\caption{Block diagram of RS decoding. Above each block, the corresponding existing algorithms are indicated.}
\label{fig:rs_decode}
\end{center}
\end{figure*}
In storage systems, ensuring reliability and data integrity requires the introduction of
redundancy. A file is divided into  symbols, encoded into  coded  symbols and
stored at  nodes. One important metric of coding efficiency is the
redundancy-reliability trade off defined as . The simplest form of redundancy
is replication.  As a generalization of replication, erasure coding offers
better storage efficiency. The Maximum Distance Separable (MDS) codes are
optimal as it provides largest separation among code words, and an MDS
code will be able to recover from any  errors if 
,
where  is the number of erasures (or irretrievable symbols).
\subsection{Reed-Solomon codes}
RS codes are the most well-known class of MDS codes. They not only can recover data when nodes fail, but can guarantee
recovery when a certain subset of nodes are Byzantine. RS codes operate on symbols of
 bits. An  RS code is a linear code, with each symbol in
, and parameters  and  where  is the total
number of symbols in a codeword,  is the total number of information
symbols, and  is the symbol-error-correcting capability of the code.
\paragraph*{Encoding}
Let the sequence of  information symbols in  be
 and  be the information polynomial of 
represented as

The codeword polynomial, , corresponding to  can be
encoded as

where  is a generator polynomial of the RS code. It is
well-known that  can be obtained as

where  is a primitive element in ,  an arbitrary integer, and .
\paragraph*{Decoding}
The decoding process of RS codes is more complex. Complete description of decoding of RS codes can be found
in~\cite{MOO05}.

Let  be the received polynomial and
, where  is the error polynomial,   the erasure polynomial, and
  the
errata polynomial. Note that  and (hence)  have
 as roots. This property is used
to determine the error locations and recover the information symbols.

The basic procedure of RS decoding is shown in Figure~\ref{fig:rs_decode}. The
last step of the decoding procedure involves solving a linear set of equations,
and can be made efficient by the use of Vandermonde generator matrices~\cite{william1988numerical}.


In , addition is equivalent to bit-wise exclusive-or (XOR), and
multiplication is typically implemented with multiplication tables or discrete
logarithm tables.  To reduce the complexity of multiplication, Cauchy
Reed-Solomon (CRS) codes~\cite{Blomer95anxor-based} have been proposed to use a different
construction of the generator matrix, and convert multiplications to XOR
operations for erasure.  However, CRS codes incur the same complexity as RS codes for
error corrections. 
\subsection{Existing work}
Several XOR-based erasure codes (in a field of
GF(2))~\cite{corbett4rdp,blaum1999ldm,blaum:eca,lin2007dpl} have been used in
storage systems.  In RAID-6 systems, each disk is partitioned into strips of
fixed size. Two parity strips are computed using one strip from each data disk,
forming a stripe together with the data strips.  EVEN-ODD\cite{blaum:eca}, Row
Diagonal Parity (RDP)\cite{corbett4rdp}, and Minimal Density RAID-6
codes\cite{blaum1999ldm}  use XOR operations, and are specific to RAID-6.
A detailed comparison of the encoding and decoding
performance of several open-source erasure coding libraries for storage is provided\cite{Plank09}.
We mention that the gain in computation efficiency of XOR-based erasure codes is achieved by
trading off fault tolerance. 
Our progressive data retrieval algorithm--however--
can tolerate as much fault--according to the configuration of the code's robustness--as is needed 
\emph{and} is highly efficient both in computation and communication costs.
Moreover, RAID-6 systems can recover from the loss of
exactly two disks but cannot handle Byzantine failures, thereby eliminating the application of such
systems for sensor networks. 

In the context of network storage for wireless sensor networks,
randomized linear codes~\cite{DIM06} and fountain codes~\cite{lin2007dpl} have
been applied with the objective that a data collector can retrieve unit data
from each of  data sources by accessing any  out  storage nodes,
and thus up to  crash-stop node failures can be tolerated.  However, such schemes
cannot recover from data modifications in the field. Compared to erasure
based solutions, the key distinctions are i) coding is done at the storage
nodes rather than at the data source, and ii) each storage node only has unit
capacity. Later, we provide a reference implementation of a single data
collector problem using the proposed primitives. Our evaluation studies shows that
our implementation outperforms the distributed storage scheme based on random
linear network coding in almost all metrics. 
\section{Progressive Data Retrieval}
\label{sect:progressive}
We use the abstractions of a data node which is a source of information that
must be stored, and a storage node which corresponds to a storage device.
Nodes are subject to both crash-stop failures, where data cannot be accessed
and Byzantine failures, where arbitrary data may be returned.  The
communication cost of transferring one unit of data from the data source to a
storage node is assumed to be constant independent of the location of the
storage node.
Unlike existing decentralized schemes for distributed networked storage, the message length in each encoding process of the proposed scheme is not tied with the number of data  node, . Hence, the RS code used in the proposed scheme is denoted as an  code. The scheme given in~\cite{Han10-Infocom} is a special case when . It will be shown that the value of  affects the storage efficiency and the communication cost.
\subsection{Data storage}
\label{sect:data_storage}
The data storage scheme consists of two steps. First, for data integrity, a
message authentication code (MAC) is added to each data block generated by a data node
before it is encoded. One-way hash functions such as MD5, SHA-1, SHA-2 can
be used. For simplicity, we adopt  CRC code for error detection with 
redundant bits~\cite{MOO05,REE99}. The portion of
errors that cannot be detected by a CRC code is dependent only on its number of
redundant bits. That is, a CRC code with  redundant bits {\it cannot} detect
 portion of errors.  If  is the size of the original data with header information, then the size of the resulting data with CRC is . It is easy to see that the CRC overhead can be amortized by using large data blocks.  

In the second step, a data block is partitioned into information symbols of
length  bits and RS codes are applied. The data node divides its
data into  symbols such
that each symbol represents an element in . Next 
the  symbols are divided into  information groups each of 
symbols.\footnote{CRC codes are added to every information group. Hence, the data size  includes those bits added by CRC codes. The last information group may have less than  symbols.
In this case, zeros are appended during the encoding procedure.} Let
 symbols of the -th group be the
components in information vector
,
where .   The node encodes  into

with  symbols as 
where 

Recall that  is a primitive element (generator) of  which can be
determined in advance. The data node then packs all , , and sends them with
their index  to storage node  via the network. 
\subsection{Data retrieval}
To reconstruct the source data, a collector needs to access sufficient number
of storage nodes to ensure data integrity. Among  storage nodes, let the
number of erasures, which includes the number of crash-stop nodes and the
number of nodes that have not been accessed, be . Identity of crash-stop
nodes can be determined by the use of keep-alive messages.  Additionally, there
are  nodes with Byzantine failures. Neither  nor the identity of these
nodes are known to the data collector. 

 given in~(\ref{eq:generator}) is a generator
matrix of a RS code~\cite{MOO05} and thus an error-erasure decoding algorithm
can recover all data if there is no error in at least  encoded symbols.  Without loss
of generality, we assume that the data collector retrieves encoded symbols from
storage nodes , ,. If no error is
present, the  symbols in the -th group of any data node can be
recovered by solving the following system of linear equations:

where
  can be constructed by the primitive element
and the index associated with , .  

When the number of erroneous (or compromised) nodes is unknown but is bounded, the
proposed progressive  procedure for data retrieval
minimizes communication cost without any prior knowledge regarding
failure models of nodes.

From Section~\ref{sect:related}, we know that RS codes can recover from any  errors if
. Therefore, if the number of compromised
nodes () is small, more erasures () can be tolerated, and less nodes need
to be accessed (by treating them as unavailable). The data retrieval procedure
proceeds in stages. At stage ,  errors are assumed. If RS decoding fails
or the decoded information symbols fail the CRC check, there must exist more
erroneous nodes than RS error-erasure decoding can handle at this stage. In
order to correct {\it one} more error, {\it two} more symbols need to be collected,
since the number of erasures allowed is reduced by two. Therefore, the total
number of symbols retrieved at stage  is . 



This procedure is clearly optimal in communication costs as additional symbols
are retrieved only when necessary. However, if applied naively, its computation
cost can be quite high since RS decoding shall be performed at each stage.  For
example, when , , with 1\% error probability defined as
probability that a storage node is faulty, our analytical results
from Section~\ref{sect:complexity} show that--on average- storage nodes need
to be accessed.  That is, the decoding needs to be done  times.
On the other hand, consider a naive scheme that
retrieves coded symbols from each of  storage nodes and decodes only once.
The naive scheme may incur less computation, but suffers from a high
communication cost. Such trade-offs between computation and
communication are avoidable as we show in Section~\ref{sect:algo},
where  we devise an algorithm that
can utilize intermediate computation results from previous stages and perform
RS decoding incrementally. Combined with the incremental decoding of stored
symbols, the proposed progressive data retrieval scheme (detailed in
Algorithm~\ref{algo:retrieval}) is both computation and communication
efficient. For simplicity, Algorithm~\ref{algo:retrieval} is  presented only
for one group of encoded symbols. It is applied to all groups of encoded
symbols to retrieve all the original data.


\begin{algorithm}[h]
\Begin {
;

The data collector randomly chooses  storage nodes and retrieves encoded data,
;\\ 



\Repeat {} {
;

\lnl{crc} \eIf{} {
Delete CRC checksum from  to obtain ; \\
\Return ;
} {
\Repeat {\lnl{rs} \{  \}} {


Two more encoded data from remaining nodes , are retrieved



} 
}
}

\Return FAIL;
}
\caption{Progressive Data Retrieval}
\label{algo:retrieval}
\end{algorithm}


In Algorithm~\ref{algo:retrieval}, for each  (or accordingly stage  where the number of errors ), the decoding process declares
errors in one of two cases. In Line~\ref{rs}, the proposed incremental RS decoding
algorithm () may fail to produce decoded symbols. Otherwise, in
Line~\ref{crc}, the decoded symbols fail the CRC check. Our implementation
(Section~\ref{sect:eval}) shows that the former happens frequently. Thus, in
most cases, CRC checking is carried out only once throughout the entire
decoding process.
\section{Progressive Decoding}
\label{sect:algo}
In this section, we present the incremental RS decoding algorithm. Compared to
the classic RS decoding, it utilizes intermediate computation results and
decodes incrementally as more symbols become available. 
\subsection{The basic algorithm}
Given the received coded symbols  with erasures
set to be zero, the generalized syndrome polynomial  can be
calculated~\cite{ARA92} as follows:

where  is an arbitrary polynomial with degree . Assume that  errors occur in unknown locations
 and  erasures in known locations  of the received polynomial. Then

and

where  is the value of the -th error, , and  is the value of the -th erasure, . Since the received values in the erased positions are zero,  for . The decoding
process is to find all , , , and . Let , , and . Clearly, .
It has been shown that a key equation for decoding is

where

If , then~\eqref{key_equation} has a unique solution . Instead of solving \eqref{key_equation} by either the Euclidean or Berlekamp-Massey algorithm we introduce a
reduced key equation ~\cite{ARA92} that can be solved by the Welch-Berlekamp
(W-B) algorithm~\cite{MOO05}. It will be demonstrated that by using W-B
algorithm and the reduced key equation, the complexity of decoding can be reduced drastically. Let .  Let a set of
coordinates  be defined by . A polynomial  is then defined by
 which is known for the
receiver since  and  are both known. Since 
divides both  and , according to~\eqref{key_equation}, it
also divides . Hence, we have the following reduced key equation:

where

Note that  is still a multiple of the error location polynomial . The reduced key equation can have a unique solution if

where  is the degree of a polynomial and  is the number of elements in set .

For all , by~\eqref{reduced_key_equation}, we have

since . Note that  is a sampling point and
 the sampled value for~\eqref{W-B_key_equation}. The unique
solution  can then be found by the W-B
algorithm with time complexity ~\cite{MOO05}.  Once all
coefficients of the errata polynomial are found, the error locations 
can be determined by successive substitution through Chien search~\cite{LIN04}.
When the solution of~\eqref{reduced_key_equation} is obtained, the errata
values can be calculated. Since there is no need to recover the errata values in our application
we omit the calculations. In summary, there are three steps in the
decoding of RS codes that must be implemented. First, the sampled values of
 for  must be calculated. Second, the W-B
algorithm is performed based on the pairs 
in order to obtain a valid . If a valid  is
obtained, then error locations are found by Chien search; otherwise, decoding
failure is reported. 
\subsection{Incremental computation of , , }
Let us choose

where  are those corresponding positions of missing data symbols after
the data collector has retrieved encoded symbols from  storage nodes. In the
decoding process, these are erased positions before the first iteration of
error-erasure decoding. Let .
The generator polynomial of the RS code encoded by~\eqref{eq:generator}
has  as roots.
The error-erasure decoding algorithm is mainly based on W-B algorithm which is
an iterative rational interpolation method.

In the -th iteration,  errors are assumed in the data and the
number of erasures is . Let  and  be the two storage nodes just accessed in the -th iteration. Let . Based on  the W-B algorithm will find  and  which satisfy
 where  is the generalized syndrome with  for all . It has been shown that  for any  by a property of W-B algorithm. Thus, if , then the unique solution will exist due to~\eqref{criterion}. By the definition of generalized syndrome polynomial in~\eqref{syndrome}, for , we have


where  is the derivative of  and . Note that  It is easy to see that  is not related to any , where  and . Hence,  for all . This fact implies that all sampled values in previous iterations can be directly used in current iteration of the W-B algorithm.

Define  The
incremental RS decoding algorithm is described in
Algorithm~\ref{algo:incremental}. Upon success, the incremental RS decoding
algorithm returns  non-error symbols. The procedure will report failure either as the result
of mismatched degree of the error locator polynomial, or insufficient number of
roots found by Chien search (Line~\ref{chien}). In both cases, no further erasure decoding is
required.  This reduces the decoding computation time.
\begin{algorithm}[h]
\SetLine
\SetKwInOut{Initialization}{init}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\Initialization{Calculate  given in~\eqref{syndrome_j} for all . \\
; ; \\ ; .}
\Input{stage , two new symbols at storage nodes  and }
\Output{FAIL or non-error symbols }
\Begin{


\ForEach{} {
\lnl{error-syndrome}  and }

\For{ {\bf to} }{ 
;

\eIf {} {
;
;
;
 } {
;
;
; 
; 
.
}

\If{}{
swap .
}

\eIf{}{
; 
; 
, ; 
} {
; 
; 
; 
.
}
}
\If {}{
\Return FAIL;
}

NumErrorLoc = ChienSearch(). 

\lnl{chien} \If{} {
\Return FAIL;
}
\Return  non-error symbols ;
}
\label{algo:incremental}
\caption{Incremental RS Decoding }
\end{algorithm}
\section{Complexity Analysis}
\label{sect:complexity}
This section provides a complexity analysis for data storage and retrieval in Sections~\ref{sect:enc_comm}
and~\ref{sect:retrieve} respectively. Both have computational and communication costs associated with them. Specifically, data
storage is composed of both an encoding and data dissemination phase, while data retrieval is composed of both an incremental collection
and decoding phase.
Included in 
Section~\ref{sect:retrieve} are Monte-Carlo simulations that are consistent with our data retrieval complexity analysis.
 Finally, in Section~\ref{sect:dynamic}, the benefit of relaxing the
 constraint that was imposed by our previous work~\cite{Han10-Infocom}, is provided.

\subsection{Data Storage}
\label{sect:enc_comm}
From Section~\ref{sect:data_storage}, the  communication cost incurred by the encoded data generated by a
data node is  bits. The total communication cost 
is then a factor of  more.
Also, it is easy to see that the total bits stored in each storage node is
, which is approximately
 when  and  is much larger than . 
Assuming a software implementation on field operations without use of look-up tables,
the computation complexity of encoding can be estimated as follows. 
Given that computation of one multiplication in  is of  bit
exclusive-ORs. At the data node,  multiplications are performed, which is equivalent
to  bit exclusive-ORs.
\subsection{Data Retrieval}
\label{sect:retrieve}
Given a set of coded symbols, Section~\ref{sect:decoding_complexity}
analyzes the computational decoding costs. Then a derivation of the 
communication complexity is provided in Section~\ref{sect:analysis}
\subsubsection{Decoding}
\label{sect:decoding_complexity}
In the subsequent complexity analysis, the worst case is assumed, namely, no
failure on decoding is reported in Algorithm~\ref{algo:incremental}
(Line~\ref{chien}), and the algorithm runs to completion.

In CRC checking, one polynomial division is performed. Since the dividend is of
degree  and the divider is of degree , the computation complexity is .

Let  be the number of errors when the decoding procedure is completed.
In the -th iteration,  errors are assumed in the data and the
number of erasures is . We first need to calculate two
syndrome values. This can be obtained by the  calculated initially. For instance, in the first iteration, according to~\eqref{syndrome_j}, the
computation complexity is of  since there are  's to be calculated and each is a product of  terms. In the next iteration, two more symbols are
added to~\eqref{syndrome}. Hence, the updated syndrome values can be obtained by an extra
 computations. To find the error-locator polynomial, the W-B
algorithm is performed two steps in each iteration with complexity .
Since we only consider software implementation,  the Chien search can be
replaced by substituting a power of  into the error-locator polynomial.
It needs to test for at most  positions to locate  non-error
positions such that it takes  computations. Finally, inversion
of a Vandermonde matrix  can be done in ~\cite{GOH94}, though for implementation purposes,
we use a  inversion algorithm (see, e.g.,~\cite{william1988numerical}).
In
summary, the computation in the -th iteration for  is

Counting for  iterations and the complexity of calculating  we have

Note that the computation complexity is measured by finite field multiplications,
which is  equivalent to  bit exclusive-ORs.  Since the correctable number
of errors  is at most , the decoding complexity is at most
. For small , 
the second term  dominates, which corresponds to syndrome computation. 
Note that the decoding procedure needs to be performed  times in order to decode all data.
\subsubsection{Communication}
\label{sect:analysis}
In this section, we provide a probabilistic analysis of the cost of
communication by determining the number of stages the algorithm needs to take,
and the probability of successful execution. Given  storage nodes and a
MDS code, the minimum and maximum number of storage nodes to access
in the proposed scheme is  and  respectively. We assume that
the CRC code always detect an error if it occurs.  Without loss of
generality, we assume that all failures are Byzantine failures, since  crash-stop
failures can be easily modeled by replacing  with . An important metric
of the decoding efficiency is the average number of accessed storage nodes when
the probability of compromising each storage node is . Failure to recover
data correctly may occur in two cases. First, , i.e., there are
insufficient number of healthy storage nodes. Second,
, in which the sequence of accessing
determines the outcome (success or failure) of the decoding process.  For
example, if the first  nodes accessed are all compromised nodes, correct
decoding is impossible. In both cases, the decoding algorithm stops after 
accesses and declares a failure.  The communication cost is . 
The main result is summarized in the following theorem.
\begin{theorem}
With the progressive data retrieval scheme, the average number of accesses as well as the 
probability of successful decoding are given in Eq.~\eqref{eq:average} and~\eqref{eq:sucprob} respectively.
\begin{table*}
\par\noindent\rule{0.9\textwidth}{0.4pt}


\par\noindent\rule{0.9\textwidth}{0.4pt}\end{table*}
\label{thm:average_access}
\end{theorem}
The details of the proof can be found in the Appendix, and numerical 
backing of this analysis is illustrated in what follows.

\paragraph*{Numerical Corroboration}
\label{sect:num}
\begin{figure}[thp]
\begin{center}
\includegraphics[width=2.5in]{figure/hist.eps}
\caption{The effect of  on the data retrieval cost for a  MDS; . The error probability here is ; . }
\label{fig:khat}
\end{center}
\end{figure}
We verify the correctness of the analytical model using Monte-Carlo simulations
implemented in Matlab. Figure~\ref{fig:khat} shows the distribution of the number of
storage nodes accessed when the algorithm terminates, and the number of iterations
correspond to the number of node accesses during data retrieval.
The bar charts depict 
histograms from Monte-Carlo simulations with  runs, and the curves
represent the numerical results from our analytical model. We choose ,  and  so that  runs give sufficient statistics in the simulations. From Figure~\ref{fig:khat},
it can be observed that the analytical results agree well with the Monte-Carlo simulations.
Note that the number of information symbols, , yields different distribution results.
Specifically, increasing  reduces storage--as derived in Section~\ref{sect:enc_comm}.
However, Figure~\ref{fig:khat} shows that the expense--in terms of data retrieval--is undesirably high.
\begin{figure}[thp]
\begin{center}
\includegraphics[width=2.5in]{figure/retrieval.eps} 
\caption{Number of storage nodes accessed as a function of the probability of malicious attacks for a MDS; .}
\label{fig:ret}
\end{center}
\end{figure}

Next, we fix , and vary the number of information symbols, , from
 to  and the error probability  from  to , while keeping the number
of data nodes constant.
Figure~\ref{fig:ret} shows the increasing communication cost as the
probability of failures increases. The number of crash-stop failures is set to
zero, and all Byzantine failures result in incorrect data.  Clearly, when the
error probability  is small, the communication cost is close to . And when  increases, the
communication cost monotonically increases, as expected. We also analyze the success rate of decoding. And observe
that for  and , decoding will always be successful. However, for , decoding is always successful \emph{only} for . When , the probability of successful decoding is only .

\subsection{The Dynamics of }
\label{sect:dynamic}
One advantage of the proposed scheme is that the number of information symbols, ,
is not tied to the number of data nodes .
Hence, one may choose appropriate values of  and  for any given application. 
For example, in wireless sensor networks, data nodes
are power-limited but the given data collector typically has no power constraint.
In such applications, one should reduce the 
cost to disseminate coded symbols from data nodes to storage nodes,
and shift the cost to the data collection phase. This can be done 
by choosing . As shown in Section~\ref{sect:enc_comm}, the total 
communication cost is
 such that this cost roughly varies 
linearly with the ratio of  and . If one takes , then 
this dissemination cost is half the dissemination cost of . However, from our 
analytical model, we see that the data retrieval cost is then doubled. 
\section{Relative Analysis}
\label{sect:era_comp}
In this section, we compare the proposed scheme (IRD) to Decentralized Erasure Codes
(DEC) proposed by Dimakis et al.~\cite{DIM06}, and Decentralized Fountain Codes (DFC) proposed by Lin et al.~\cite{lin2007dpl}. To make this comparison, we assume
that there are  data nodes that contain
the data to be redundantly stored amongst  storage nodes.  The  data
 nodes collectively generate  bits, where there are  bits per node. As mentioned in Section~\ref{sect:data_storage}, each IRD data node adds  CRC bits to its data. For brevity, we therefore will write  for the data size of each IRD data node. To facilitate understanding,
we set  in all schemes utilizing information symbols.


In this analysis, data  and storage nodes' data can be partitioned into \emph{data} and \emph{storage} symbols respectively. The number of bits in a data symbol is always . Depending on the storage scheme, a storage symbol may be larger than . Multiplication of a -bit and -bit symbol, , requires  XORs, while an addition requires only , where the field of operation for the symbols is . 

Similar to Section~\ref{sect:data_storage}, encoding and decoding for all schemes is done in groups. DEC and DFC both have  data symbol per data node per group, while IRD encodes  data symbols per data node per group. Consequently, each data node in DEC/DFC and IRD has  and  groups per data node, respectively. As we will see, although fountain coding minimizes the encoding and decoding complexities, IRD minimizes 
communication significantly, especially for decoding.

Quantifying the performance over storage codes requires a comparison over the metrics shown in Table~\ref{tab:comparison}.
Respectively, these metrics include the  storage nodes' redundancy--total bits stored; storage nodes' overhead required for decoding; dissemination cost--communication cost between data and storage nodes; collection cost for  and all of the original data, respectively; encoding computation cost; a data-collector's decoding computation cost; ability to detect and correct errors; and finally, ability to deterministically guarantee reconstruction of the original data.
We now provide a qualitative comparison amongst three storage schemes, based on these metrics.
\begin{table*}[tbp]
\caption{Performance comparison of erasure coding schemes for storage}
\label{tab:comparison}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& DEC & DFC & IRD\\  
\hline
\hline
Storage &  &  &  \\
Overhead &  &  & \\
\hline
Dissemination &  &  & \\
-collection &   &   & \\
-collection &   &   & \\
\hline
Encoding &  &  & \\ 
Decoding &  &  & \\
\hline
Error detection & no & no & yes\\
Error correction & no & no & yes\\
Deterministic guarantee & no & no & yes\\
\hline
\end{tabular}
\end{center}
\end{table*}


\subsection{Decentralized erasure codes}
DEC have been applied in wireless or wired networks, where a data collector accesses {\it any}  out of 
storage nodes for data reconstruction.
Each
storage node selects random and independent coefficients in a finite field
, and stores a linear combination of all the received data (modulo
). Randomized linear codes are used, where each
data node routes its packet multiple times to at least  storage nodes, so that the  storage nodes collectively store  bits.

The storage complexity per storage node is  bits, which holds because i) arithmetic is done in , which means each stored symbol has  bits, and ii) there are  groups.

The overhead can be calculated as follows. Since each storage node stores the linear combination coefficients and there are   data nodes connected to a storage node, the overhead--to store the coefficients--per storage node is 
 bits. For any storage node, note that the  outside the parentheses denotes the number of nodes connecting to the storage node, while the  term inside the parentheses are the bits to identify any connecting data node~\cite{DIM06}. The last parenthesized term identifies the coding group.


The DEC dissemination cost is given by   because i) there are  data  nodes, ii) each data node repeatedly sends its data out  times, and iii) there are  groups.

Since the code structure is inherently random,  nodes must be contacted in order to obtain any one symbol. Specifically,  symbols are collected from  storage nodes to reconstruct the generated data, per group.

The encoding cost per storage node is given by: 

because i) the cost of a linear combination (multiplication) is  bit operations, since a combination coefficient and a data symbol are  and  bits respectively, ii) a storage symbol is the result of  linear combinations, and iii) there are  groups to be encoded.
Similarly, the decoding complexity to reconstruct the entire data is given by: 

This complexity can be derived from the following: i) a multiplication costs  bit operations, since each storage node stores -bit symbols, ii) matrix inversion is performed, and iii) there are  groups to be decoded.

Although DEC can
be efficiently constructed, error-detection and correction are both infeasible: Assuming the use of CRC for error-detection, a data-collector must continue to enumerate all possible  symbols from  out of  storage nodes, until the original data can be reconstructed correctly. Therefore, DEC cannot be applied to dependable storage systems where data integrity is desired in the midst of errors and malicious users.

\subsection{Decentralized fountain codes}
DFC is a decentralized LT code~\cite{luby2002lt}, and were proposed for the special purpose of guaranteeing data availability in the presence of crash-stop failures for networks with several data generators and storage nodes. Storage node, , where , chooses a \emph{degree}, , which is defined as the number of data symbols from which to form a linear combination.  then linearly combines  data symbols--using the XOR operation--from  arbitrarily chosen
data generators. DFC--like other fountain codes--trades communication for computation: decoding requires more than  storage nodes to be contacted, though both encoding and decoding computations are linear in the number of original symbols. In performance evaluation, we assume that  is the probability of successful decoding. 
Different from DEC, instead of pulling data from candidate data  nodes, a deterministic and probabilistic scheme is devised to push data from the
source nodes to storage nodes~\cite{lin2007dpl}. Aside from using fountain codes, the authors use \emph{random walks} to remove the need for a geometric routing protocol for propagating data from data  nodes to storage nodes.

The storage complexity per storage node is
 bits because there are  groups and  bits per storage symbol. Note that--unlike DEC--the number of bits in a storage symbol is independent of the size of the operating field.

The overhead complexity per storage node is given by  The derivation here is similar to that of DEC with the following differences: the average degree of a storage symbol is  and there are \emph{no linear combination coefficients}, since every linear combination is simply an XOR of a set of data symbols. 

The dissemination cost for DFC is given by
 This holds because i) there are  storage nodes,
and ii) each node stores  symbols on average.

DFC is an LT code that is not \emph{systematic}. Also, to reconstruct any data, more than  nodes must be contacted. Specifically, the communication cost to collect all data symbols is given by
 This holds because  symbols must be collected for successful data reconstruction of any  symbols. 

Encoding and decoding can be very efficient in DFC. The encoding complexity per storage node is given by
 since i) the cost of an XOR of two -bit symbols is  XORs and
ii) each encoded symbol is the XOR of  -bit data symbols, where  is the average degree of an encoded symbol.
In a similar manner, the decoding for DFC codes is given by
 because DFC uses the  belief propagation algorithm for decoding.
DFC is most efficient in decoding. However, like all fountain codes, the decoding efficiency comes at a communication trade-off that is determined by the parameter, .

\subsection{IRD}
Each data  node in IRD encodes its own data, with  symbols per group to the  storage nodes. Therefore, altogether there are 
 groups. Note that IRD has the same storage complexity as DFC even though the numbers of groups and data symbols per group differ. Because IRD utilizes a code structure known to all storage nodes, it has the minimum overhead complexity per storage node:


Unlike DEC and DFC, IRD does not replicate transmissions to storage nodes, and therefore its dissemination cost exactly equals its storage complexity, leaving IRD with the minimum dissemination cost as well.  
IRD is also preferable because it is systematic, allowing partial collection of a subset of data. Since  storage nodes store the data nodes' data in original form, anyone of these  storage node can be contacted to collect -portion of the original data, for all  groups. This is particularly important where not every sequence of generated data is immediately significant to a data-collector.

Encoding for all groups and all data  nodes yields

because i) each node performs encoding, ii) we use a classical matrix multiplication, and iii) all groups are encoded.
From Section~\ref{sect:decoding_complexity} and iterating over all groups, the decoding complexity is  XORs in the absence of erroneous storage nodes, and given by
 in the presence of  erroneous storage nodes. Note that IRD is also the only erasure coding scheme to detect and efficiently correct errors. Moreover, IRD adapts decoding computation to the number of erroneous nodes. Finally, neither DEC nor DFC are suitable for real time dependable applications, since they are not deterministic. That is, their ability to decode is probabilistic and cannot be guaranteed.
\section{Implementation and Evaluation}
\label{sect:eval}
We have implemented the proposed and baseline algorithms, where each data node's 
information is a memory buffer in a single machine having
 GHz Intel Xeon CPU,  KB cache and
 GB RAM. A randomly generated message is first
partitioned into either  or  information symbols and then encoded into  coded
symbols of length  bits. 
Thus, the field size is . A
stored symbol is corrupted with an error probability  independently.
Comparing our error-erasure code to either decentralized or fountain erasure codes for error-correction performance is pointless, since these codes
\emph{cannot} feasibly guarantee data availability in the presence of errors. Therefore, in this section, the following three algorithms are considered.
\begin{itemize}
\item{\it BMA} is the Berlekamp-Massey (BMA) algorithm~\cite{MOO05} for
RS decoding. Similar to Algorithm~\ref{algo:retrieval}, BMA {\it progressively}
retrieves data from each storage node and performs decoding until
the decoded symbols passes the CRC checks or failure is declared. However,
decoding cannot be performed incrementally. 
\item{\it BMA-genie} knows \emph{a priori} how many symbols are needed to
successfully decode. BMA-genie decodes only once after retrieving sufficient
number of symbols. Note that BMA-genie is impossible to implement in practice,
and is included for comparison purpose only.
\item{\it IRD} is the proposed progressive data retrieval algorithm. 
\end{itemize}
\begin{comment}
In place of BMA, either the Euclidean or Welch-Berlekamp algorithm could have been used. 
They have the same asymptotic time complexity. 
Figures~\ref{fig:alg_time} and \ref{fig:breakdown} show the time it takes to
correctly decode a data block. 
\end{comment}
\subsection{Total computation time}
Figure~\ref{fig:alg_time}(a) and \ref{fig:alg_time}(b) illustrate the computation time (in log scale)
spent in decoding when  and , respectively. Note that  in these simulations. The storage overhead
 is 10.13 and 2.55 with the maximum number of errors correctable being 461
and 311. From Figure~\ref{fig:alg_time}, we observe that the BMA and IRD
computation time increases  as  increases. But the rate of increment in
IRD is much slower. When  is small or the redundancy is higher
(Figure~\ref{fig:alg_time}(a)), IRD is faster than the genie-aided
BMA. This is because in the genie-aided BMA, the computations of erasure
polynomials (with ) dominate the decoding time when  is small.
In contrast, IRD does not compute erasure polynomials.

In Section~\ref{sect:complexity}, the data collection costs were shown to depend on 
. We quantified the encoding and decoding computational
complexity. 
The evaluation results--for a given Byzantine node rate ,  data nodes, and a
MDS code, where --are
consistent with the analysis in Section~\ref{sect:complexity}: 
encoding computational costs are invariant
of , and are relatively insignificant. However, decoding computational costs increase according to Eq.~(\ref{eq:dec_comp}).
\begin{comment}
From these results, we conclude that if the joint probability of Byzantine and fail-stop
nodes' is known {\it a priori} to be high,  should only be made small, if the given data collector can bear the
increase in decoding computational costs.
\end{comment}
\begin{figure*}[thp]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=2.5in]{figure/algorithm_running_times_101.eps} &
\includegraphics[width=2.5in]{figure/algorithm_running_times_401.eps} \\
(a) k = 101 & 
(b) k = 401 
\end{tabular}
\caption{Average computation time for MDS decoding against Byzantine node rate, . 
For the given  values, both IRD and BMA cannot successfully decode for }
\label{fig:alg_time}
\end{center}
\end{figure*}

\subsection{Decoding Breakdown}
We break down the decoding computation time to understand the dominant operations in the algorithms 
as the error probability increases. The break down includes the time 
to find the error-locator polynomial ({\it elp-time}), find the error locations ({\it
chien-time}) and solve for the information polynomial ({\it inv-mat-time}). This
breakdown is also illustrated in
Figure~\ref{fig:rs_decode}, where the st and nd blocks shows the elp-time, while the rd
and th blocks give the chien-time and inv-mat-time, respectively. 

When the error probability is low (Figure~\ref{fig:breakdown}(a)), computation
of error-locator polynomials dominates for small , while the
matrix inversion time becomes significant when  is large. In our
implementation, the cost of a matrix inversion is quadratic in the number of
symbols decoded. Chien search though asymptotically is the most time consuming
procedure, it can be performed quite fast. When the error
probability is high (Figure~\ref{fig:breakdown}(b),(c)), computation of error location polynomials
dominates except in IRD. Also, from Figure~\ref{fig:breakdown},
we observe that the computation time in matrix inversion is almost
negligible (on the order of tens of milliseconds) in BMA and IRD, and
is comparable to that in BMA-genie (recall that BMA-genie knows the number of
errors in advance and thus performs matrix inversion only once). This is
because even though there are more errors with larger  (and thus more
iterations), the decoding algorithm is likely to fail in or before Chien search
(e.g., Algorithm~\ref{algo:incremental} (Line~\ref{chien})). Thus, in most
cases, BMA and IRD perform matrix inversion once.

\begin{figure*}[thp]
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=2.3in]{figure/time_breakdown_1.eps} &
\includegraphics[width=2.3in]{figure/time_breakdown_10.eps} &
\includegraphics[width=2.3in]{figure/time_breakdown_20.eps} \\
(a)  &
(b)  &
(c)  
\end{tabular}
\caption{Average computational time breakdown for decoding one MDS codeword,
. Because IRD progressively decodes, its performance
does not deteriorate with an increasing Byzantine node rate, .}
\label{fig:breakdown}
\end{center}
\end{figure*}

From the experiments, IRD is more efficient because it utilizes intermediate results from
previous iterations. Up to  times speed up can be attained, relative to
BMA.
\begin{comment}
Also, the computation complexity in
Section~\ref{sect:complexity} only provides the worst-case order analysis. In
practice, the computation time in the average case can differ significantly in
part due to hidden constant factors.
\end{comment}
\section{Conclusions}
\label{sect:conc}
\vspace{-0.25em}
We have developed a communication-optimal algorithm to guarantee data dependability 
and availability for distributed storage systems, in the midst of Byzantine-faulty and crash-stop nodes.
The communication cost for data retrieval is minimized by utilizing intermediate computation 
results and collecting only the minimum data required 
for successful data reconstruction. The efficient encode and decode
primitives serve as a fundamental building block for distributed dependable
storage systems. An analytical model to evaluate the communication complexity of our incremental data retrieval algorithm is provided.
Moreover, our previous work restricted 
, a constraint that is unrealistic for distributed networked storage systems. In this paper, the constraint is eliminated, and 
 is invariant of the number of data nodes, . Finally, our implementation
results show that our progressive scheme outperforms the state-of-the-art scheme by a factor of  in computation costs.
Moreover, they are consistent with our analytical model, for any  and any
Byzantine node rate.
\begin{comment}
The proposed scheme guarantees latency of real time applications by collecting data in each data retrieval, according to the probability that a storage node is Byzantine. Our scheme is desirable for real-time systems, where energy minimization in communication is critical. Our scheme is also desirable under high data rates as it minimizes communication costs by retrieving only necessary symbols. 
\end{comment}
\appendix[Proof of Theorem~\ref{thm:average_access}]
\label{app:proof}
Let  be the event that there are  compromised
storage nodes in the network, and  be the event that the error-erasure
decoding algorithm  executes  times when it completes {\it successfully}.
Note that  also indicates how many errors the error-erasure decoding
algorithm has corrected since our proposed scheme asks for extra data to
correct one more error in each iteration.  

Therefore, the average number of accesses
is given as,

The first term gives the average number of accesses  in a  successful run. The third and second  terms
correspond to the first and second failure cases discussed in Section~\ref{sect:analysis}, respectively.  
is simply given as 

To determine , we first derive , i.e., the
probability that only erasure decoding is needed. Clearly,  occurs if
the first  copies of data are from healthy nodes. Therefore,  Note this results holds even when .
			
When , let
 and  represent the number of erroneous data and correct data
received at the data collector after accessing  live storage nodes. Clearly,
.  The event  occurs under the following conditions, (i)
 and ; {\it and} (ii) ,; {\it and} (iii)  and .
Evolution of  and  can be modeled as a lattice path from the origin
 to  in the A-B coordinate system, recording the running
totals as more nodes are accessed. Condition (i) implies the
path has to go through the point .  Condition (ii) implies that the path {\it
never} intersects the line  except in the th step.  Condition (iii) implies the last data
retrieve needs to be from a healthy node. The lattice path is the result of
directional random walks, where each move is conditionally independent of the
previous moves given the current coordinates. At step , the probability  and  is given by

since there are  and
 remaining nodes and compromised nodes, respectively. The probability
 and  is given by

since there are  and  remaining nodes and healthy nodes,
respectively. Therefore, the probability for Condition (iii) to hold on Condition  (i) is
 The probability for Condition (i) to hold is simply,


Now, what remains to be derived is the probability of (ii) on condition (i).
We use the bijection arguments due to Antone Desir\'{e} Andr\'{e}~\cite{west:01}, and count
instead the number of ``bad" paths that cross the line . Consider the point  which is the reflection of the point  along the line . Clearly, the point  is above the line . Thus, all paths from  to  must hit the line  at least once. Consider the
first time such a path  hits the line  at . After this point, the remaining number of correct data  along this path is  and that of erroneous data is . Now consider a path  coinciding with  up to point  and afterward it has exact opposite branches to . That is,
the results of data retrieval are switched afterward, namely, all data from compromised data
are counted toward  and all data from healthy nodes are counted toward
. As a result,  and  for .  Thus, for any path reaching , reflecting the remainder of the path
after it first hits  yields a ``bad" path to . Similarly, every ``bad" path to   has a corresponding such path to  that intersects with the line  by construction. This
establishes a bijection between the set of ``bad" paths to  and
the set of paths to . Clearly, there are 
``bad" paths. Therefore, the probability for Condition (ii) to hold on Condition (i) is,


To this end, we obtain the probability that the error-erasure decoding
algorithm stops at the th iteration when there are  compromised nodes as
follows, ,


We now have the average number of accesses and the probability of
successful decoding given in Eq.~\eqref{eq:average} and
Eq.~\eqref{eq:sucprob}, respectively.
\section*{Acknowledgment}
Han's work was supported  by 
the National Science Council of Taiwan, under grants NSC
96-2221-E-305-002-MY3 and his
visit to LIVE lab at University of Texas at Austin. Omiwade and Zheng's work is supported in part by NSF CNS 0546391.
\bibliographystyle{IEEE}
\bibliography{network,nfs}
\end{document}
