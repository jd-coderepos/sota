\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[final]{EMNLP2022}

\usepackage{times}
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{breqn}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tcolorbox}
\usepackage{CJK}
\usepackage{adjustbox}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multicol}
\usepackage{vwcol} 
\newsavebox{\algleft}
\newsavebox{\algright}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{ctable} \usepackage{amsmath,stackengine}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{times}
\usepackage{latexsym}
\usepackage{bbm}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{bm}
\usepackage{microtype}
\newcommand{\zixuan}[1]{{\color{blue}{\small\bf\sf [zixuan: #1]}}}
\newcommand{\bing}[1]{{\color{red}{\small\bf\sf [bing: #1]}}}
\newcommand{\hu}[1]{{\color{orange}{\small\bf\sf [hu: #1]}}}
\newcommand{\ls}[1]{{\color{magenta}{\small\bf\sf [lei: #1]}}}


\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}




\title{Continual Training of Language Models for Few-Shot Learning}
\author{
Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu\thanks{~~Now at Google Research  \url{leishu@google.com}}~~~and Bing Liu\\ 
Department of Computer Science, University of Illinois at Chicago\\
Wangxuan Institute of Computer Technology, Peking University\\
\texttt{\{zke4,hxu48,liub\}@uic.edu}\\  \texttt{\{linhaowei, shaoyj\}@pku.edu.cn} \\
}

\begin{document}


\maketitle
\begin{abstract}
{\color{black}{\color{black}Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications.~Adapting or post-training an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains.  
The resulting system is called CPT (\textit{C}ontinual \textit{P}ost-\textit{T}raining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness.\footnote{{\color{red}\url{https://github.com/UIC-Liu-Lab/CPT}}}}



}








\end{abstract}












\section{Introduction}
\label{sec.intro}





Recent work has shown that large LMs have the ability to perform few-shot (or even zero-shot) learning well \cite{DBLP:conf/nips/BrownMRSKDNSSAA20,DBLP:journals/corr/abs-2112-11446,DBLP:journals/corr/abs-2201-11990}. 
{\color{black}\textit{Post-training} (a.k.a., \textit{domain-adaptive pre-training} or \textit{pre-finetuning}) an LM with a large unlabeled domain corpus before end-task fine-tuning in the domain achieves better results~\cite{DBLP:conf/naacl/XuLSY19,gururangan2020don} than directly fine-tuning the  LM.
This paper goes a step further to study the problem of improving an LM's ability to handle new and ever emerging domains. For this, one needs to \textit{continually post-train} the LM with a sequence of domains. A key issue associated with this problem is \textit{catastrophic forgetting} (CF).\footnote{
CF means that learning a new task/domain may need to modify the existing network, which degrades the performance of previous tasks/domains~\cite{mccloskey1989catastrophic}.} This paper thus investigates how to continually extend the LM's knowledge without suffering from CF. {\color{black}From a broader perspective, since training a large LM from scratch is extremely expensive and computation intensive, incrementally updating the LM with the latest language data reflecting the ever changing development of the language itself, social events and the knowledge from different fields is becoming more and more critical. As humans are very effective at incremental learning, if we can imitate this human capability with little or no forgetting, we will be pushing the AI research forward significantly.}   

The proposed system, called CPT, is a  continual learning (CL) system for post-training. Starting from a pre-trained LM (e.g., RoBERTa \cite{DBLP:journals/corr/abs-1907-11692}), it incrementally post-trains the LM with a sequence of domains using their unlabeled corpora. Once a task (a domain in our case) \footnote{We will use the term \textit{domain} in this paper to be consistent with the post-training literature} is trained, its data is no longer accessible. At any time, the resulting continually post-trained LM can be used {\color{black}by end-tasks in the trained domains.} This is in the \textit{task-incremental learning} (TIL) setting of CL, where the task id (domain id in our case) is provided when the learned model of a task needs to be used later (the use of domain id is discussed in Sec.~\ref{sec:task_mask}).\footnote{CL has two other settings: \textit{class-incremental learning} and \textit{domain-incremental learning}~\cite{Ven2019Three}.}  This paper proposes an effective approach called CPT and focuses on the  challenging and practical scenario of \textit{few-shot} end-task learning after post-training a sequence of domains.} 





Continual post-training is different from conventional CL~\cite{chen2018lifelong}. The key difference is that in conventional CL, each task is an end-task, but in our case the end-task involves fine-tuning the continual post-trained LM (called p-LM). This causes major forgetting, which we call the \textit{catastrophic butterfly effect (CBE)} and does not happen in conventional CL. Our proposed system, CPT, can solve both CF and CBE, based on a novel hard masking mechanism (Sec. \ref{sec:butterfly}) and can achieve \textit{no} forgetting. As shown in Sec. \ref{sec:results}, naively applied existing CL systems cannot effectively prevent CF (even though some existing techniques have shown almost perfect CF prevention ability in conventional CL).

Experiments in 4 domains and their corresponding end-tasks demonstrate the effectiveness of the proposed CPT system. 

\vspace{+2mm}
\noindent
{\color{black}\textbf{Related Work.} Overcoming CF is a major goal of CL~\cite{chen2018lifelong}. There are many existing approaches, e.g., 
\textit{regularization-based approaches}~\cite{Kirkpatrick2017overcoming,Seff2017continual}, 
\textit{replay-based approaches}~\cite{Rebuffi2017,Lopez2017gradient} and  \textit{parameter isolation based approaches} \cite{Serra2018overcoming,fernando2017pathnet}. {\color{black}Our CPT is based on parameter isolation and uses masks in continual post-training.} Recently, CL has drawn attention in NLP. It has been used for slot filling~\cite{shen-etal-2019-progressive}, language learning~\cite{li2019compositional}, sentence embedding~\cite{liu2019continual}, translation~\cite{khayrallah2018regularized}, cross-lingual modeling~\cite{liu2020exploring}, question answering~\cite{greco2019psycholinguistics} and text classification~\cite{DBLP:journals/corr/abs-2112-02706,ke2021Classic,sun2020lamol,huang2021continual,chuang2020lifelong,mehta2021empirical,madotto2020continual}. However, none of them tries to improve an LM.

CPT is closely related to ELLE~\cite{DBLP:conf/acl/QinZLL0SZ22}, which does \textit{continual pre-training}. The key difference is that ELLE starts from random initialization, while our CPT starts from a pre-trained LM. We tried to adapt ELLE for continual post-training by learning from a pre-trained RoBERTa but it fails to converge. This also indicates it is non-trivial to do well in our setting. Readers can refer to Appendix \ref{Sectionrelated.work} for a full coverage of the related work.
}

\section{Proposed CPT System}
\label{sec:cpt}



\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{src/overview_v3.pdf}
\vspace{-6mm}
\caption{
Architecture of CPT, which has two CL-plugins inserted in the transformer layers of RoBERTa in a parallel manner (FFN: feed-forward network). \textbf{(A)} CPT for \textit{continual post-training}. It uses a masked language model (MLM) head for unsupervised post-training of the CL-plugins only. \textbf{(B)} CPT for \textit{individual fine-tuning}. CPT is evaluated by the corresponding individual end-task performance of all post-trained tasks. 
Each CL-plugin has numbers and colors indicating its masks and is illustrated in {Appendix} \ref{sec.illustration}.
}
\label{overview}
\end{figure}


CPT continually post-trains RoBERTa~\cite{DBLP:journals/corr/abs-1907-11692}. This is achieved by two \textit{continual learning plug-in} (called CL-plugin) modules inserted into each transformer layer of RoBERTa. CL-plugin is inspired by adapters in~\cite{Houlsby2019Parameter}. While adapters can isolate different tasks, one needs to allocate a new adapter for each task and no knowledge can be shared among different tasks' adapters. The CL-plugin, however, is a CL system that learns a sequence of tasks with adapters shared by all domains. 
Figure~\ref{overview} gives the CPT architecture with two CL-plugins added to RoBERTa. 

{\color{black}\textbf{Sequential vs. Parallel CL-plugin.} Instead of following the original sequential adapter \cite{Houlsby2019Parameter}, CL-plugin adopts the parallel adapter idea in \cite{DBLP:journals/corr/abs-2110-04366}. The difference is that the former inserts an adapter \textit{after} the FFN/attention layer while the latter inserts it \textit{before} FFN/attention layer (see Fig. \ref{overview}). We choose the parallel version as it performs better (see Sec. \ref{sec:results}). 
}

In post-training, only the two CL-plugins are trained. The components of the original pre-trained RoBERTa are fixed. In end-task fine-tuning, all components are trainable. A CL-plugin is a two-layer fully connected network with a task mask mechanism. It takes two inputs: (1) hidden states  from the feed-forward layer in a transformer layer and (2) task ID  needed by task incremental learning (TIL). 
Inside a CL-plugin, task masks (TMs), which indicate task- specific neurons, are used to {deal with CF}. Since TMs is differentiable, the whole CPT can be trained end-to-end. 











\subsection{Task Masks (TMs)} 
\label{sec:task_mask}






In each layer of a CL-plugin, task masks are used to protect those neurons that are important for previous tasks to overcome CF. The masks basically forbid gradient updates to those neurons during backpropagation in learning a new task. Note that a task is also a domain in our case.

Learning a new task/domain consists of two main steps: (1) apply the mask in each layer for each old task to block off the gradient flow to protect the model for the old task, and (2) learn domain  and its masks for future use. We present (2) first.  
 











\textbf{Learning Task Masks for Overcoming CF.} In learning each task , a mask (a ``soft'' binary mask)  is trained for the task at each layer  in CL-plugin, indicating the neurons that are important for the task. We borrow the hard attention idea in \cite{Serra2018overcoming} and leverage the task ID embedding to train the mask. For a task ID , its embedding  consists of differentiable deterministic parameters that can be learned together with other parts of the network. 
To generate the task mask  from , \textit{Sigmoid} is used as a pseudo-gate (mask) function.  is computed with

where  is a temperature variable, linearly annealed from 1 to  (a small positive value). 

In the forward pass, given the output of each layer , , we element-wise multiply mask ,

{The masked output  of the last layer in CL-plugin is} fed to the next layer of the RoBERTa with a skip-connection. After learning task , the final  is saved and added to the set .

\textbf{Applying Task Masks.}
Before learning a new task , we first accumulate and set the masks  on the neurons in each layer  for all old tasks 
so that in backpropagation, the gradient  for task  will not flow to these neurons. 
Since  is {pseudo} binary, we use 
max-pooling to achieve the accumulation and condition the gradient:  

Those gradients corresponding to the 1 entries in  are set to 0 (to block off gradient flow) while the others remain unchanged. 
In this way, neurons in old tasks are protected. Note that we expand (copy) the vector  to match the dimensions of .

















\subsection{Catastrophic Butterfly Effect in Fine-tuning}
\label{sec:butterfly}


To perform an end-task in a post-trained domain, we fine-tune the mask-protected model of the domain, which is indicated by the task/domain id. The fine-tuning uses the corresponding domain neurons for the specific end-task {by setting  and condition the output via Eq. \ref{eq:forward}}. With the masks, there should be no forgetting for continual post-training and the end-task fine-tuning performance should be similar to post-train each domain separately. However, we found that this is not the case.\footnote{For example, fine-tuning an end restaurant sentiment classification task achieves macro-F1 (MF1) of 0.64 right after post-training the restaurant domain but its fine-tuning MF1 drops to 0.44 after post-training three more domains.} Our investigation found that the problem is due to the \textit{pseudo-gate} function in Eq.~\ref{eq:mask}. No matter how small  is, Eq. \ref{eq:mask} can only gives us a mask almost 0 (or 1). This causes the following: (1) During post-training, 
{the gradients for used neurons in Eq.~\ref{eq:gradient} are not exactly 0 but a very small number.} (2) During fine-tuning, we cannot make use of the corresponding neurons for the specific end-task by simply setting . The small change in the neurons for old domains during post-training caused by (1) is neglect-able in conventional CL because in conventional CL we evaluate the model using test sets and no weights update involved. However, in CPT, the end-task needs to fine-tune the continually post-trained LM model (p-LM), which involves weight updating. A small change to the p-LM during continual post-training can result in a different initialization for the end-task fine-tuning and give totally different fine-tuning results. We call this \textit{butterfly effect} inspired by the term indicating a small state change in  nature (e.g., the flap of a butterfly’s wings in Brazil) can result in large differences in a later state (e.g., a tornado in Texas). 

We propose a simple method to solve it, i.e., adding a threshold  to the  to make it a \textit{hard binary mask},

We then apply it to Eq. \ref{eq:gradient} in gradient manipulation and Eq. \ref{eq:forward} in end-task fine-tuning.  can be easily set (we use 0.5) since Eq.~\ref{eq:mask} already gives a pseudo-binary mask. Note that this has almost no effect on post-training as it is used to block the backward pass gradient flow during post-training and select the corresponding neurons during fine-tuning. 







\begin{table*}[]
\centering
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{cc|cccccccccccc}
\specialrule{.2em}{.1em}{.1em}
\multirow{2}{*}{Category} & Domain & \multicolumn{2}{c}{Restaurant} & \multicolumn{2}{c}{AI} & \multicolumn{2}{c}{ACL} & \multicolumn{2}{c}{AGNews} & \multicolumn{2}{c}{Average} & \multicolumn{2}{c}{Forget R.} \\
 & Model & MF1 & Acc & MF1 & Acc & MF1 & Acc & MF1 & Acc & MF1 & Acc & MF1 & Acc \\
\specialrule{.1em}{.05em}{.05em}
\multirow{4}{*}{Non-CL} & RoBERTa & 50.61 & 74.77 & 27.88 & 28.44 & 32.19 & 34.59 & 64.19 & 65.95 & 43.72 & 50.94 & \multicolumn{2}{c}{---} \\
& Adapter & 45.40 & 67.28 & 23.69 & 24.56 & 24.99 & 27.55 & \textbf{64.53} & \textbf{66.50} & 39.65 & 46.48 & \multicolumn{2}{c}{---} \\
 & RoBERTa-ONE & 53.63 & \textbf{76.73} & 29.86 & 30.11 & 33.05 & 35.72 & 62.57 & 65.13 & 44.78 &51.92 & \multicolumn{2}{c}{---} \\
 & Adapter-ONE & 52.19 & 74.20 & 30.80 & 31.59 &36.59 & 36.99 & 61.66 & 63.94 & 45.31 & 51.68 & \multicolumn{2}{c}{---} \\
 & Prompt-ONE & 28.93 & 59.79 & 21.06 & 22.10 & 28.02 & 29.22 & 60.70 & 62.58 & 34.68 & 43.42 &
 \multicolumn{2}{c}{---} \\
 & DEMIX & 53.14 & 75.28 & 27.68 & 27.29 & 37.63 & 38.57 & 63.18 & 65.13 & 45.41 & 51.57 &
 \multicolumn{2}{c}{---} \\
\hline
\multirow{8}{*}{CL} & RoBERTa-NCL & 42.59 & 67.56 & \textbf{31.57} & \textbf{31.62} & 33.07 & 34.54 & 60.18 & 63.50 & 41.85 & 49.30 & 3.27 & 2.82 \\
 & Adapter-NCL & 47.42 & 70.23 & 29.56 & 29.90 & 35.92 & 37.58 & 61.73 & 64.45 & 43.65 & 50.54 & 2.21 & 1.69 \\
 & HAT & 50.45 & 71.78 & 28.33 & 29.41 & 34.93 & 37.15 & 62.97 & 65.05 & 44.17 & 50.85 & 2.43 & 2.04 \\
& BCL & 51.70 & 74.34 & 29.66 & 30.96 & 32.85 & 34.82 & 63.60 & 65.47 & 44.45 & 51.40 & 1.47 & 0.82 \\
 & KD & 39.75 & 67.11 & 29.63 & 29.33 & \textbf{38.30} & \textbf{42.09} & 62.85 & 65.39 & 42.63 & 50.98 & 4.92 & 3.07 \\
 & EWC & 48.32 & 71.59 & 30.96 & 31.01 & 35.96 & 38.05 & 62.29 & 64.95 & 44.38 & 51.40 & 1.40 & 0.80 \\
& DER++ & 48.09 & 71.79 & 30.71& 30.54 &34.25 & 35.77 & 64.24 & 66.11 & 44.32 & 51.05 & 1.79 & 1.62 \\
 & CPT & \textbf{53.90} & 75.13 & 30.42 & 30.89 & 37.56 & 38.53 & 63.77 & 65.79 & \textbf{46.41} & \textbf{52.59} & \textbf{0.00} & \textbf{0.00} \\
\specialrule{.1em}{.05em}{.05em}
\end{tabular}}
\caption{End-task macro-F1 (MF1), accuracy and forgetting rate results 
for all domains \textit{after continual post-training of all domains}. The results are averages of 5 random seeds (the domain training order is as they appear in the first row). Due to space limits, the results for \textit{different domain orders} and the \textit{standard deviations} are reported in Appendix~\ref{ap:order} and Appendix~\ref{ap:std}, respectively). Non-CL baselines has no forgetting. 
} 
\label{tab:overall_results}
\end{table*}


\section{Experiments}
\label{Sectionexperiments}
The proposed paradigm uses a different evaluation from that of conventional continual learning (CL). After unsupervised continual post-training of an LM (RoBERTa in our case) with a sequence of domains, the resulting p-LM is used to fine-tune an end {\color{black}few-shot classification task} from any post-trained domain. There is no CL during end-task fine-tuning. Each fine-tuning task is done separately. 

\subsection{Datasets and Baselines}
\label{sec.data-baselines}
\textbf{Datasets:} We use 4 \textbf{unlabeled domain datasets}: \textit{Yelp Restaurant} \cite{DBLP:conf/naacl/XuLSY19}, \textit{AI Papers} \cite{DBLP:conf/acl/LoWNKW20}, \textit{ACL Papers} \cite{DBLP:conf/acl/LoWNKW20} and \textit{AGNews} \cite{DBLP:conf/nips/ZhangZL15} and their 4 corresponding \textbf{end-task classification datasets}.\footnote{These are popularly used in related works. Details of the datasets are given in {Appendix} \ref{sec:data_stat}.
We conduct experiments using \textit{few-shot} learning end-tasks. Following~\cite{gu2021ppt}, we use 32 training samples for \textit{Restaurant} and \textit{AGNews}, 48 training samples for \textit{ACL} and 56 training samples for \textit{AI} due to different numbers of classes in each dataset.}







\vspace{+1.5mm}
\noindent
\textbf{Baselines.} Since no existing method can perform our task, we use 6 \textit{non-CL} and 7 \textit{adapted CL} methods as our baselines. The non-CL baselines include \textbf{(1) RoBERTa} and \textbf{(2) Adapter} where we directly fine-tune the pre-trained model or adpater (without any post-training); \textbf{(3) RoBERTa-ONE}, \textbf{(4) Adapter-ONE} and \textbf{(5) Prompt-ONE}, where we build a model for each task using a separate network. It has no knowledge transfer or CF. \textbf{(6) DEMIX} \cite{gururangan2021demix} trains a separate adapter for each task and initializes the adapter from its most similar previous task adapter. The 7 adapted CL baselines include \textbf{(7) RoBERTa-NCL} and \textbf{(8) Adapter-NCL}, where we post-train the domains one by one with no mechanism to deal with CF/transfer. Other are state-of-the-art CL baselines and we adapt them for continual post-training.\footnote{Readers can refer to Appendix \ref{ap:baselines} for the detailed of each of these baselines.}

{\color{black}\subsection{Implementation Details} \textbf{Architecture.} We adopt  as our backbone LM. A masked language model head is applied for post-training. The fine-tuning follows the standard practice \cite{DBLP:conf/naacl/DevlinCLT19}, where we pass the final layer \texttt{</s>} token representation to a task-specific feed-forward layer for prediction. The feed-forward layer with softmax output is used as the classification heads, together with the categorical cross-entropy loss. Note that for the aspect sentiment classification task (see Table~\ref{tab:dataset}), we adopt the ASC formulation in \cite{DBLP:conf/naacl/XuLSY19}, where the aspect (e.g., ``\textit{sound}'') and review sentence (e.g., ``\textit{The sound is great}'') are concatenated via \texttt{</s>}. 

\textbf{Hyperparameters.}
Unless otherwise stated, the same hyper-parameters are used in all experiments.  We use  for  in Eq.~\ref{eq:mask} and  is set to 0.5 in Eq.~\ref{eq:threshold} in the main paper. 
As shown in Figure \ref{overview}, there are two CL-plugins for each Transformer layer (one at the bottom in parallel with attention and one at the top in parallel with FFN). We search the CL-plugin size within \{128, 256, 512, 768, 1024\} and adopt 512 for the bottom one and 768 for the top one based on validation experiments. The task id embeddings have the same size as the hidden layer dimension of the CL-plugin.
The maximum input length is set to 164 which is long enough for all datasets. We use Adam optimizer and set the learning rate to 1e-4 for post-training and 5e-5 for fine-tuning. The batch size is set to 48 for post-training and 20 for fine-tuning. Since each of our domain-specific dataset has a different size, we train CPT on each task/domain for 1 epoch for post-training, which is approximately 13K steps, following \cite{DBLP:conf/acl/GururanganMSLBD20,DBLP:conf/naacl/XuLSY19}. We train on end-task fine-tuning datasets for 20 epochs and take the results for the last epoch, assuming no validation sets. We empirically found 20 epochs can give us a relatively stable results.} 















\subsection{Evaluation Results and Analysis}
\label{sec:results}
We report the average results of the 4 different fine-tuning tasks (or datasets) in accuracy and Macro-F1 after post-training on all unlabeled domain datasets in
Table~\ref{tab:overall_results}. The forgetting rate (forget R.) \cite{DBLP:conf/cvpr/LiuSLSS20} is also reported. The higher the forgetting rate is, the more forgetting it has. Negative rates indicate positive knowledge transfer.\footnote{Forgetting rate is {\color{black}computed as as follows~\cite{DBLP:conf/cvpr/LiuSLSS20},} , where  is the end-task performance right after its domain  is post-trained, and  is the performance of the end-task of domain  after post-training the last domain. We average over all end-tasks except the last one as the last domain has no forgetting.}


\textbf{Superiority of CPT.} Clearly, CPT outperforms all baselines and achieves no forgetting. More specifically, CPT markedly outperforms the two baselines without post-training (RoBERTa and Adapter), indicating CPT can learn new domains well. These two baselines are also  significantly worse than other baselines, indicating that fine-tuning the pre-trained RoBERTa alone is weak. 
Comparing with CL baselines, \textbf{CPT achieves \textit{no forgetting}} (we can see the forgetting rate is 0), indicating the high effectiveness of the proposed approach. We also note that CPT is even slightly better than those ONE baselines, indicating \textbf{\textit{some positive knowledge transfer in CPT}}. 


\subsection{Ablations} 
In Table \ref{tab:ablation_results}, we give the ablation results. We are interested in the following:

\textbf{(1) Catastrophic butterfly effect} (CBE). The third row with ``w/o butterfly'' shows results without the hard binary mask mechanism in Eq.~\ref{eq:threshold}. Clearly, the results are worse and the model suffers from forgetting. This indicates CBE and our approach is effective. 

\textbf{(2) Different Architecture.} CPT is based on CL-plugin, which is inspired by adapters. Another popular way to use adapters is to make it sequential \cite{Houlsby2019Parameter}. Sequential adapter (first row) is clearly poorer than our current parallel one. This conforms to the observation in \cite{DBLP:journals/corr/abs-2110-04366}. 


\textbf{(3) Different Orders.} Table \ref{tab:overall_results} only reports the results of one fixed domain order (\texttt{Restaurant}\texttt{AI}\texttt{AGNews}). We are interested in how the order impacts CPT results. We give the detailed results for all the other baselines and detailed domain orders in Appendix~\ref{ap:order}. We can see the results of CPT does not change much and it still outperforms other baselines. This indicates the CPT's robustness to domain orders in  post-training. 

















\begin{table}[]
\centering
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{c||ccc}
\specialrule{.2em}{.1em}{.1em}
\multirow{2}{*}{Model} & \multicolumn{2}{c}{Final Performance}  \\
 & MF1 & Acc \\
\specialrule{.1em}{.05em}{.05em}
CPT (Sequential Adapter)  &  43.00  & 50.25  \\
\hline
CPT (w/o butterfly)  &  44.17  & 50.85   \\
CPT (w/o masking)  &  43.65  & 50.54   \\
\hline
CPT  & 46.41 &  52.58 \\
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{Ablation experiment results. }
\label{tab:ablation_results}
\vspace{-6mm}
\end{table}




\section{Conclusion}
{\color{black}This paper proposed to continually post-train an LM with a sequence of domains using their unlabeled domain corpora. An effective method (CPT) is also proposed. An end-task from any post-trained domain can fine-tune the resulting LM. Experimental results demonstrate the effectiveness of CPT. } 


\section{Limitations}
{\color{black}We list two limitations of CPT. First, CPT adds CL-plugins for continual post-training with no change to the underlying LM in training. Although a CL-plugin is small compared to an LM, it is still interesting and may be more effective to explore the idea of updating the LM directly without any additional modules. Second, domain ids are needed in both training and testing for CPT. In some applications, it may be hard to provide a domain id for each fine-tuning end-task. We will explore these in our future work as specializing and/or incrementally improving an LM is an important problem.} 

\section*{Acknowledgments}
{\color{black}The work of Zixuan Ke and Bing Liu was supported in part by two National Science Foundation (NSF) grants (IIS-1910424 and IIS-1838770).} 

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix


\newpage

\section{Related Work}
\label{Sectionrelated.work}




Our work is related to \textit{continual learning}, \textit{post-training} and \textit{few-shot learning}.

\textbf{Continual learning (CL).} In general, overcoming CF is a major goal in CL~\cite{chen2018lifelong}. (1) \textit{Regularization methods}~\cite{Kirkpatrick2017overcoming,Seff2017continual} add a regularization to ensure minimal changes to weights for previous tasks.
(2) \textit{Replay methods} retain~\cite{Rebuffi2017,Lopez2017gradient,wang2020efficient,guo2022online} or generate some data of old tasks~\cite{Shin2017continual,He2018overcoming} and use them in learning a new task.
(3) \textit{Parameter isolation methods} \cite{Serra2018overcoming,fernando2017pathnet} allocate parameters for different tasks and mask them out in learning a new task. {\color{black}Our CPT is based on (3) and uses masks in continual post-training.} Recently, CL has drawn attention in NLP. It has been used for slot filling~\cite{shen-etal-2019-progressive}, language learning~\cite{li2019compositional}, sentence embedding~\cite{liu2019continual}, translation~\cite{khayrallah2018regularized}, cross-lingual modeling~\cite{liu2020exploring}, question answering~\cite{greco2019psycholinguistics} and text classification~\cite{DBLP:journals/corr/abs-2112-02706,ke2021Classic,sun2020lamol,huang2021continual,chuang2020lifelong,mehta2021empirical,madotto2020continual}. However, none of them tries to improve an LM.

\textbf{Post-training} is an effective approach to mitigate the discrepancies between pre-trained domains and the target domain. Researchers have applied post-training to many domains, e.g., reviews \cite{DBLP:conf/naacl/XuLSY19,sun2019fine}, news and academic papers \cite{DBLP:conf/acl/GururanganMSLBD20}, and shown improved end-task results. However, none of them consider the continual learning paradigm. 

\textbf{Few-shot learning (FL)} aims to learn tasks with a few labeled examples. The main issue of FL is over-fitting, due to the scarcity of
labeled training data. Existing methods to overcome over-fitting fall in three main families: (i) model-based methods try to reduce the hypothesis space of the few-shot task \cite{DBLP:conf/nips/TriantafillouZU17,DBLP:conf/coling/HuLT0S18}, (ii) data-based methods try to augment additional data to the few-shot set \cite{DBLP:conf/nips/BenaimW18,DBLP:conf/aaai/GaoHX0LLS20}, and (iii) algorithm-based solutions try to improve strategies for searching for the best hypothesis. Recently, a new
paradigm using prompts achieves promising results for few-shot language learning as shown
in GPT-3 \cite{brown2020language}, PET \cite{DBLP:conf/eacl/SchickS21} and LM-BFF \cite{DBLP:conf/acl/GaoFC20}. {\color{black}However, none of them does few-shot fine-tuning in continual post-training. 

\textbf{Continual few-shot learning.} Several researchers have studied this problem recently \cite{DBLP:journals/corr/abs-2004-11967,DBLP:journals/corr/abs-2110-07298,DBLP:conf/emnlp/JinLR021,DBLP:conf/naacl/XiaYFY21,DBLP:conf/acl/0001LX22}. It continually learns a sequence of few-shot tasks. However, this is very different from our continual post-training because our continual learning happens in the post-training stage instead of the end-task fine-tuning stage. We only evaluate the proposed CPT system after continual post-training by conducting few-shot learning tasks individually by fine-tuning the post-trained language model (p-LM) in each of the post-trained domains. No continual learning is involved in few-shot learning.}



\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{src/plugin_v3.pdf}
\caption{
Architecture of CPT, which has two CL-plugins inserted in the transformer layers of RoBERTa in a parallel manner. \textbf{(A)} CPT for \textit{continual post-training}. It uses a masked language model (MLM) head for unsupervised post-training of the plugins only. \textbf{(B)} CPT for \textit{individual fine-tuning}. The performance of CPT is evaluated by the corresponding individual end-task performance of all post-trained tasks using the \textit{final} post-trained model (with different mask). Each CL-plugin module (\textbf{to the right of the transformer}) has two fully connected layers and a skip connection. On top of each fully connected layer, there is a mask computed from task ID  with the same size as the fully connected layer.}
\label{plugin}
\end{figure*}




\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccc|ccccc}
\specialrule{.2em}{.1em}{.1em}
\multicolumn{3}{c|}{Unlabeled Domain Datasets} & \multicolumn{4}{c}{End-Task Classification Datasets} \\
Dataset & Source & \#training & Dataset & Task & \#training & \#testing & \#classes \\
\specialrule{.1em}{.05em}{.05em}
Yelp Restaurant & Yelp Review & 1,132,359 & SemEval-res & Aspect Sentiment Classification & 32 & 1,120 & 3 \\
AI & AI Papers & 707,368 & SCIERC  & Relation   Classification & 56 & 2,388 & 7 \\
ACL & ACL Papers & 1,208,449 & ACL-ARC & Citation Intent Classification & 48 & 421 & 6 \\
AGNews & News Article & 73,750 & AGNews-FT & News  Classification & 32 & 7,568 & 4 \\
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{
Statistics for unlabeled domain datasets and end task supervised classification datasets. } \label{tab:dataset}
\end{table*}



\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccccccc} 
\specialrule{.2em}{.1em}{.1em}
\multirow{2}{*}{Category} & Domain      & \multicolumn{2}{c}{Restaurant} & \multicolumn{2}{c}{AI} & \multicolumn{2}{c}{ACL} & \multicolumn{2}{c}{AGNews} & \multicolumn{2}{c}{Average}  \\
                          & Model       & MF1    & Acc                   & MF1    & Acc           & MF1    & Acc            & MF1    & Acc               & MF1    & Acc                 \\ 
\specialrule{.1em}{.05em}{.05em}
\multirow{6}{*}{Non-CL}   & RoBERTa     & 0.0456 & 0.0274                & 0.0208 & 0.0233        & 0.0391 & 0.0338         & 0.0121 & 0.014             & 0.0066 & 0.0062              \\
                          & Adapter     & 0.0214 & 0.0223                & 0.0111 & 0.0102        & 0.0375 & 0.0386         & 0.0221 & 0.0224            & 0.0155 & 0.0142              \\
                          & RoBERTa-ONE & 0.0095 & 0.0087                & 0.0364 & 0.0358        & 0.0382 & 0.0432         & 0.0169 & 0.0162            & 0.0197 & 0.0187              \\
                          & Adapter-ONE & 0.0292 & 0.0223                & 0.0207 & 0.0222        & 0.0076 & 0.0063         & 0.0141 & 0.0157            & 0.0074 & 0.0054              \\
                          & Prompt-ONE  & 0.0427 & 0.0991                & 0.0297 & 0.0254        & 0.0386 & 0.0325         & 0.0115 & 0.0100            & 0.0151 & 0.0292              \\
                          & DEMIX       & 0.0329 & 0.0293                & 0.0259 & 0.0283        & 0.0297 & 0.0367         & 0.0336 & 0.0309            & 0.0152 & 0.0165              \\ 
\hline
\multirow{8}{*}{CL}       & RoBERTa-NCL         & 0.0374 & 0.0238                & 0.0156 & 0.0158        & 0.0293 & 0.0349         & 0.0218 & 0.0154            & 0.0130 & 0.0160              \\
                          & Adapter-NCL & 0.0250 & 0.0194                & 0.0232 & 0.0184        & 0.0183 & 0.0264         & 0.0136 & 0.0151            & 0.0095 & 0.0137              \\
                          & HAT         & 0.0264 & 0.012                 & 0.0236 & 0.0251        & 0.0294 & 0.0287         & 0.0106 & 0.009             & 0.0078 & 0.0112              \\
& BCL         & 0.0255 & 0.0124                & 0.0121 & 0.0105        & 0.0182 & 0.0126         & 0.0100 & 0.0069            & 0.0094 & 0.0032              \\
                          & KD          & 0.0642 & 0.0435                & 0.0295 & 0.0233        & 0.0271 & 0.0267         & 0.0160 & 0.0133            & 0.0117 & 0.0109              \\
                          & EWC         & 0.0324 & 0.0259                & 0.0281 & 0.0189        & 0.0177 & 0.0196         & 0.0041 & 0.0096            & 0.0079 & 0.0062              \\
                          & DER++       & 0.0250 & 0.0183                & 0.0231 & 0.0319        & 0.0116 & 0.0163         & 0.0196 & 0.0178            & 0.0126 & 0.0128              \\
                          & CPT         & 0.0264 & 0.0120                & 0.0236 & 0.0251        & 0.0294 & 0.0287         & 0.0106 & 0.0090             & 0.0078 & 0.0112              \\
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{Standard deviations of the corresponding metrics of the proposed CPT system and the baselines.}
\label{tab:std}
\end{table*}





\begin{table}[]
\centering
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{c||ccc}
\specialrule{.2em}{.1em}{.1em}
\multirow{2}{*}{Model} & \multicolumn{2}{c}{Final Performance}  \\
 & MF1 & Acc \\
\specialrule{.1em}{.05em}{.05em}
CPT (Sequential Adapter)  & {0.0347}  & {0.0350}  \\
\hline
CPT (w/o butterfly)  &  {0.0102}  & {0.0079}   \\
CPT (w/o masking)  &  {0.0095}  & {0.0137}   \\
\hline
CPT  & {0.0078} &  {0.0112} \\
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{Standard deviations of the corresponding metrics of the proposed CPT system and the ablations. }
\label{tab:ablation_std}
\end{table}





\begin{table*}[]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{cc!{\vrule width \lightrulewidth}cccc|cccc|cccc|cccc|cccc} 
\specialrule{.2em}{.1em}{.1em}
        \multirow{3}{*}{Category} & Order & \multicolumn{4}{c|}{\texttt{AI}\texttt{ACL}\texttt{Restaurant}\texttt{AGNews}}                                   & \multicolumn{4}{c|}{\texttt{Restaurant}\texttt{AI}\texttt{AGNews}\texttt{ACL}}                                   & \multicolumn{4}{c|}{\texttt{AI}\texttt{ACL}\texttt{AGNews}\texttt{Restaurant}}                                   & \multicolumn{4}{c|}{\texttt{AGNews}\texttt{ACL}\texttt{Restaurant}\texttt{AI}} & \multicolumn{4}{c}{\textbf{Average}}                                    \\
         & Metric & \multicolumn{2}{c}{Performance} & \multicolumn{2}{c|}{Forget R.} & \multicolumn{2}{c}{Performance} & \multicolumn{2}{c|}{Forget R.} & \multicolumn{2}{c}{Performance} & \multicolumn{2}{c|}{Forget R.} & \multicolumn{2}{c}{Performance} & \multicolumn{2}{c|}{Forget R.}   & \multicolumn{2}{c}{\textbf{Performance}} & \multicolumn{2}{c}{\textbf{Forget R.}} \\
         & Model & MF1 & Acc                   & MF1 & Acc                     & MF1 & Acc                   & MF1 & Acc                     & MF1 & Acc                   & MF1 & Acc                     & MF1 & Acc                   & MF1 & Acc        
                 & MF1 & Acc                   & MF1 & Acc\\ 
\specialrule{.1em}{.05em}{.05em}
\multirow{6}{*}{Non-CL}   &  RoBERTa  & 43.72 & 50.94 & \multicolumn{2}{c|}{---} & 43.72 & 50.94 & \multicolumn{2}{c|}{---} &43.72 & 50.94 & \multicolumn{2}{c|}{---} & 43.72 & 50.94 & \multicolumn{2}{c|}{---} & 43.72 & 50.94 & \multicolumn{2}{c}{---}\\
&  Adapter  & 39.65 & 46.48 & \multicolumn{2}{c|}{---} & 39.65 & 46.48 & \multicolumn{2}{c|}{---} & 39.65 & 46.48 & \multicolumn{2}{c|}{---} & 39.65 & 46.48 & \multicolumn{2}{c|}{---} & 39.65 & 46.48 & \multicolumn{2}{c}{---}\\
&  RoBERTa-ONE  & 44.78 &51.92 & \multicolumn{2}{c|}{---} & 44.78 &51.92 & \multicolumn{2}{c|}{---} &44.78 &51.92 & \multicolumn{2}{c|}{---} &44.78 &51.92 & \multicolumn{2}{c|}{---}&44.78 &51.92 & \multicolumn{2}{c}{---}\\
&  Adapter-ONE  & 45.31 & 51.68 & \multicolumn{2}{c|}{---} & 45.31 & 51.68 & \multicolumn{2}{c|}{---} &45.31 & 51.68 & \multicolumn{2}{c|}{---} &45.31 & 51.68 & \multicolumn{2}{c|}{---}&45.31 & 51.68 & \multicolumn{2}{c}{---}\\
&  Prompt-ONE  & 34.68 & 43.42 &
 \multicolumn{2}{c|}{---} & 34.68 & 43.42 &
 \multicolumn{2}{c|}{---} &34.68 & 43.42 &
 \multicolumn{2}{c|}{---} &34.68 & 43.42 &
 \multicolumn{2}{c|}{---}&34.68 & 43.42 &
 \multicolumn{2}{c}{---}\\
&  DEMIX  & 45.41 & 51.57 &
 \multicolumn{2}{c|}{---} & 45.41 & 51.57 &
 \multicolumn{2}{c|}{---} &45.41 & 51.57 &
 \multicolumn{2}{c|}{---} &45.41 & 51.57 &
 \multicolumn{2}{c|}{---}&45.41 & 51.57 &
 \multicolumn{2}{c}{---}\\
\hline
\multirow{8}{*}{CL}   &  RoBERTa-NCL  & 42.62 & 49.95 & 2.45 & 1.79 & 42.22 & 49.52 & 3.10 & 2.33 &42.88 & 50.11 & 0.29& 0.18 &44.33 & 51.51 &1.76 & 1.21 & 43.01 & 50.28 & 1.90 & 1.38\\
& Adapter-NCL & 44.71 & 51.67 & 1.71 & 1.08 & 44.61 & 51.07 & 1.14 & 1.23 & 44.91 & 51.57 & 1.41 &1.23 & 45.52 & \textbf{52.15} & 0.72& 0.44 & 44.94 & 51.62 & 1.25 & 0.99  \\
& HAT &  45.10 & 51.50 & 1.66 & 1.19 & 43.29 & 49.96 & 2.76 & 2.09 & 46.06 & \textbf{52.07} & 0.50 & 0.21 & 44.94 & 51.45 & 0.86 & 0.25 & 44.85 & 51.25 & 1.45 & 0.93 \\
& BCL  & 43.97 & 50.74 & 2.20 & 1.50 & 45.30 & 51.54 & 0.36 & -0.14 & 45.28 & 51.79 & 0.36 & 0.11 & 45.59 & 51.61 & 0.08 & 0.11 & 45.04 & 51.42 & 0.75 & 0.40\\
& KD & 42.09 & 50.22 & 0.57 & 0.08 &45.18 & \textbf{52.68} &1.22 &0.57 & 42.63 & 50.45 & \textbf{-0.31} & \textbf{-0.56} &42.93 & 50.70 & 1.10& 0.32 & 43.21 & 51.01 & 0.64 & 0.10  \\
& EWC & 43.97 & 50.74 &0.16 & 0.03 & 43.65 & 50.29 & \textbf{-0.29}& \textbf{-0.20} & 45.52 & 51.36 & 0.17& 0.15 & 43.42 & 49.85 & 0.12& 0.10 & 44.14 & 50.56 & 0.04 & 0.02 \\
& DER++  & 44.56 & 50.13 & 2.95 & 2.31 & 44.02 & 49.99 & 1.24 & 1.12 & 43.98 & 50.23 & 1.44 & 1.27 & 44.32 & 50.13 & 1.32 & 1.09 & 44.22 & 50.12 & 1.74 & 1.45 \\
& CPT    & \textbf{46.49} & \textbf{52.47} & \textbf{0.00} & \textbf{0.00} & \textbf{45.71} & 51.71 & 0.00 & 0.00 & \textbf{46.15} & 51.93 & 0.00 & 0.00 & \textbf{45.89} & 51.86 & \textbf{0.00} & \textbf{0.00} & \textbf{46.06} & \textbf{51.99} & \textbf{0.00} & \textbf{0.00} \\
\specialrule{.1em}{.05em}{.05em}
\end{tabular}
}
\caption{CPT performance averaged over all domains after the final post-trained with different orders (averaged over 5 random seeds) and the average of these orders. }
\label{tab:order}

\end{table*}


\section{Illustration of Task Masks}
\label{sec.illustration}
Figure \ref{plugin} illustrates the CPT architecture and the task mask learning. Note that fine-tuning is for evaluating the domain post-training and should not affect any parameters of post-training. 
During \textbf{continual post-training} (Figure~\ref{plugin} (A)), after training domain/task 1, we obtain its useful neurons indicated by the 1 entries. Before training domain/task 2, those useful neurons for domain 1 are first masked (those previous 1's entries are turned to 0's). After training domain 2, two neurons with 1 are used by the domain. When domain  arrives, all used neurons by domains 1 and 2 are masked before training, i.e., their entries are set to 0. After training domain , we see that domains  and 1 have a shared neuron (the cell with two colors, red and green), which is used by both of domains. After continual post-training, we evaluate CPT by \textbf{individual fine-tuning}. During fine-tuning (Figure~\ref{plugin} (B)), we only make use of those neurons that are useful for domain/task id  (red cells) and freeze all other neurons (grey cells).

\section{Dataset Statistics}
\label{sec:data_stat}


Table \ref{tab:dataset} shows the statistics of the \textit{unlabeled domain datasets} and \textit{end-task classification datasets}.
Note that the full AGNews is very large. We use only its author provided training split as our domain-specific datasets as our \textit{unlabeled AGNews} dataset for continual post-training. The remaining testing set is used as the labeled end-task (\textit{AGNews-FT}). The other three corresponding end task datasets are \textit{SemEval-res} \cite{DBLP:conf/naacl/XuLSY19}, \textit{ACL-ARC} \cite{DBLP:journals/tacl/JurgensKHMJ18}, and \textit{SCIERC}\cite{DBLP:conf/emnlp/LuanHOH18}. 



\section{Details of the CL baselines}
\label{ap:baselines}



\textbf{Non-Continual Learning Baselines}: Each of these baselines builds a separate model for each
task independently.
It thus has no CF. 


(1,2) \textbf{RoBERTa, Adapter}~\cite{DBLP:journals/corr/abs-1907-11692,Houlsby2019Parameter} use the original RoBERTa/Adapter for the end-task fine-tuning without any post-training. These are the only two without any post-training. All the following baselines use the masked language model loss (MLM) for post-training. 

(3) \textbf{RoBERTa-ONE} is the existing post-training method in~\cite{DBLP:conf/acl/GururanganMSLBD20}. To our knowledge, the existing post-training systems are all based on the MLM loss. 

(4) \textbf{ Adapter-ONE}~\cite{madotto2020continual,Houlsby2019Parameter} adds small adapter layers between layers of Transformer for post-training. We follow the adapter design in~\cite{madotto2020continual,Houlsby2019Parameter}. An adapter is simply two fully connected layers. During post-training, the Transformer is fixed, only the added adapters are trainable. The bottleneck size (adapter size) is set to 128. 
During end-task fine-tuning, both RoBERTa and the adapters are trainable to ensure fair comparison.


(5) \textbf{Prompt-ONE}~\cite{DBLP:conf/emnlp/LesterAC21} adds a sequence of real vector tokens (called virtual tokens or prompt tokens) to the end of the original input sequence. In post-training, RoBERTa (the LM) is fixed and only the prompt tokens are trained. In end-task fine-tuning, both LM and the trained prompt are trainable. We initialize 100 tokens and set the learning rate of the prompt token to 0.3 in post-training, following the setting in \cite{DBLP:conf/emnlp/LesterAC21}. 

(6) \textbf{DEMIX}~\cite{gururangan2021demix} is a recent model to adapt a pre-trained LM with new domains. It adds a new adapter once a new domain arrives (network expansion is needed) and initializes the new adapter with the parameters of the previous trained adapter nearest to the new domain data. They use the perplexity on held-out samples to choose the most probable adapter. 


\textbf{Continual Learning (CL) Baselines.}


(7) \textbf{RoBERTa-NCL (Naive continual learning)} is a naive extension of \cite{DBLP:conf/acl/GururanganMSLBD20}, which continually/incrementally post-trains the LM to learn all domains using the MLM loss with no mechanism to deal with forgetting or CF.

(8) \textbf{Adapter-NCL}~\cite{Houlsby2019Parameter} is similar to the Adapter based system. The only difference is that the same set of adapters is shared across all domains, rather than using a new adapter for each new domain.



(9) \textbf{Hard attention to overcome forgetting (HAT)} is derived from HAT~\cite{Serra2018overcoming}, the state-of-the-art parameter-isolation based method with almost no forgetting. {\color{black} However, HAT suffers from forgetting in continual post-training due to the catastrophic butterfly effect.}


(10) \textbf{BCL}~\cite{ke2021adapting} is a continual learning model that can avoid forgetting and encourage knowledge transfer. It is similar to Adapter-NCL. The difference is that its adapters consist of two modules, one is a capsule network (a new capsule is added once a new domain arrives) to encourage transfer, and the other is similar to HAT to avoid forgetting. Similar to HAT, task/domain information is needed in end-task fine-tuning. We replace the backbone network from BERT with RoBERTa for fair comparison. 



(11) \textbf{Knowledge distillation (KD)}~\cite{hinton2015distilling}
minimizes the representational deviation between the learned representation and the new representation in post-training. We
compute the KL divergence between the representations (the output before the masked language model prediction head) of each token of the previous post-trained LM and current LM as the distillation loss.

(12) \textbf{EWC}~\cite{buzzega2020dark} is a popular
regularization-based continual learning method that adopts elastic weights consolidation to add  regularization to penalize parameter changes.

(13) \textbf{DER++}~\cite{buzzega2020dark} is a recent replay method using distillation to regularize the new task training. We store 16.4K tokens for each learned domain as the memory, which is the largest memory we can use for the system to run.

\iffalse
\section{Implementation Details}
\label{sec:imp_detail}

\textbf{Architecture.} We adopt  as our backbone LM. A masked language model head is applied for post-training. The fine-tuning follows the standard practice \cite{DBLP:conf/naacl/DevlinCLT19}, where we pass the final layer \texttt{</s>} token representation to a task-specific feed-forward layer for prediction. The feed-forward layer with softmax output is used as the classification heads, together with the categorical cross-entropy loss. Note that for the aspect sentiment classification task (see Table~\ref{tab:dataset}), we adopt the ASC formulation in \cite{DBLP:conf/naacl/XuLSY19}, where the aspect (e.g., ``\textit{sound}'') and review sentence (e.g., ``\textit{The sound is great}'') are concatenated via \texttt{</s>}. 

\textbf{Hyperparameters.}
Unless otherwise stated, the same hyper-parameters are used in all experiments.  We use  for  in Eq.~\ref{eq:mask} and  is set to 0.5 in Eq.~\ref{eq:threshold} in the main paper. 
As shown in Figure \ref{overview}, there are two CL-plugins for each Transformer layer (one at the bottom in parallel with attention and one at the top in parallel with FFN). We search the CL-plugin size within \{128, 256, 512, 768, 1024\} and adopt 512 for the bottom one and 768 for the top one based on validation experiments. The task id embeddings have the same size as the hidden layer dimension of the CL-plugin.
The maximum input length is set to 164 which is long enough for all datasets. We use Adam optimizer and set the learning rate to 1e-4 for post-training and 5e-5 for fine-tuning. The batch size is set to 48 for post-training and 20 for fine-tuning. Since each of our domain-specific dataset has a different size, we train CPT on each task/domain for 1 epoch for post-training, which is approximately 13K steps, following \cite{DBLP:conf/acl/GururanganMSLBD20,DBLP:conf/naacl/XuLSY19}. We train on end-task fine-tuning datasets for 20 epochs and take the results for the last epoch, assuming no validation sets. We empirically found 20 epochs can give us a relatively stable results. 
\fi

\section{Results for Different Domain Orders}
\label{ap:order}

Table~\ref{tab:overall_results} in the main paper reported the results for the order \texttt{Restaurant}  \texttt{AI}  \texttt{ACL}  \texttt{AGnews}. We now look at how the order affects the results.
Table~\ref{tab:order} shows baselines and CPT's results of 4 different orders. Note that the results for the Non-CL baselines are the same across different orders (and the same as those in Table~\ref{tab:overall_results}) because they are not effected by orders. We can see CPT is always better than other baselines, and achieve 0 forgetting rate, demonstrating the effectiveness of CPT. We also note that some baselines in some sequence has negative forgetting rate, indicating they have some backward transfer (new domain learning helps learned domains). However, their final results are much worse than CPT's.

\section{Standard Deviations}
\label{ap:std}


Table~\ref{tab:std} reports the standard deviations of the corresponding results in Table~\ref{tab:overall_results} (in the main paper) of CPT and the considered baselines over 5 runs with random seeds. We can see the results of CPT are stable. Some baselines (e.g., RoBERTa, RoBERTa-ONE) can have quite large standard deviations. 

Table~\ref{tab:ablation_std} reports the standard deviations of the corresponding results in Table~\ref{tab:ablation_results} (in the main paper) of CPT and the considered baselines over 5 runs with random seeds. We can see the results of sequential adapters has a high variance while CPT and other variants are stable. 















\end{document}
