\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps\\
}

 
\author{\IEEEauthorblockN{Kieran Saunders}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Aston University}\\
Birmingham, United Kingdom \\
190229315@aston.ac.uk}
\and
\IEEEauthorblockN{George Vogiatzis}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Loughborough University}\\
Leicestershire, United Kingdom \\
g.vogiatzis@lboro.ac.uk}
\and
\IEEEauthorblockN{Luis J. Manso}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Aston University}\\
Birmingham, United Kingdom \\
l.manso@aston.ac.uk}
}

\maketitle

\begin{abstract}
Self-supervised monocular depth estimation has been a subject of intense study in recent years, because of its applications in robotics and autonomous driving. Much of the recent work focuses on improving depth estimation by increasing architecture complexity.
This paper shows that state-of-the-art performance can also be achieved by improving the learning process rather than increasing model complexity.
More specifically, we propose
(i) disregarding \textit{small} potentially dynamic objects when training,
and (ii) employing an appearance-based approach to separately estimate object pose for truly dynamic objects. We demonstrate that these simplifications reduce GPU memory usage by 29\% and result in qualitatively and quantitatively improved depth maps. 
The code is available at https://github.com/kieran514/Dyna-DM.
\end{abstract}

\begin{IEEEkeywords}
 Computer vision, Autonomous vehicles, 3D/stereo scene analysis, Vision and Scene Understanding
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle 

\section{Introduction}


The problem of depth estimation from visual data has recently received increased interest because of its emerging application in autonomous vehicles.
Using cameras as a replacement for LiDAR and other distance sensors leads to cost efficiencies and can improve environmental perception. 

There have been several attempts at visual depth estimation, for example, using stereo images \cite{godard2017unsupervised, chang2018pyramid, li2019stereo} and optical flow \cite{ilg2017flownet, wang2018occlusion}. Other methods focus on supervised monocular depth estimation, which requires large amounts of ground truth depth data. Examples of supervised monocular architectures can be found in \cite{bhat2021adabins, song2021monocular, ranftl2021vision}. Ground truth depth maps such as those available in the KITTI dataset \cite{geiger2013vision} are usually acquired using LiDAR sensors. Supervised methods can be very precise but require expensive sensors and post-processing to obtain clear ground truth. This paper focuses on self-supervised monocular depth estimation, as it can provide the most affordable physical set-up and does not require annotated data.

\blfootnote{979-8-3503-0121-2/23/\0.9(I_t, I_{t+1})(D_{t}, D_{t+1})(M_{t}^i, M_{t+1}^i)(I_t, I_{t+1})(P_{t \to {t+1}}^{i=0},P_{{t+1} \to t}^{i=0})K\hat{I}_{t \to {t+1}}^{fw}\hat{M}_{t \to {t+1}}^{fw, i=n}M_{t+1}^{i=n}I_{t+1}(P_{t \to {t+1}}^{i=n}, P_{{t+1} \to t}^{i=n})n=1,2,3\dots\hat{I}_{t \to {t+1}}^{fw\to iw, i=n}\hat{I}_{t \to {t+1}}^{iw, i=0}(\odot)I_tI_{t+1}(\theta)\thetatt+2tt+1\theta\theta\mathcal{L}_{pe}V_{t\to {t+1}}(p)\mathcal{L}_gD_{t \to {t+1}}^{diff}\hat{M}_{t \to {t+1}}\mathcal{L}_s\mathcal{L}_hp_hh^{i=n}n^{th}\Bar{D}I_{t+1}I_tI_tI_{t+1}\beta_1=0.9\beta_2=0.999\times10^{-4}gamma = 0.975(\lambda_{pe}, \lambda_g, \lambda_s, \lambda_h)=(2, 1, 0.1, 0.02)\alpha = 0.85\theta\theta\theta\theta\downarrow\downarrow\downarrow\theta\theta\downarrow\downarrow\downarrow\theta\theta\theta0.9\theta\downarrow\downarrow\downarrow\theta\theta\downarrow\downarrow\downarrow\delta < 1.25\uparrow\downarrow\downarrow\theta\theta\theta\theta\theta\theta\theta\theta=0.9\downarrow\downarrow\downarrow\downarrow\downarrow\downarrow\downarrow\delta < 1.25\uparrow\delta < 1.25^2\uparrow\delta < 1.25^3\uparrow\downarrow\downarrow^{\mathrm{a}}\theta0.9\leq^{\mathrm{b}}\downarrow\downarrow\downarrow\downarrow\delta < 1.25\uparrow\delta < 1.25^2\uparrow\delta < 1.25^3\uparrow\downarrow\downarrow\theta\theta\delta < 1.25^3$ when comparing Dyna-DM to Insta-DM in Table \ref{table:8}. These metric improvements are accompanied by improvements in GPU usage. We most notably see a 29.6\% reduction in memory usage when training comparing Dyna-DM to Insta-DM, this allows us to improve the accuracy of our pose and depth estimations while requiring less computation.  




As we know that the CityScape dataset has more potentially dynamic objects than KITTI we can train Dyna-DM and Insta-DM on CityScape and test with the Eigen test split. Table \ref{table:9} demonstrates even greater improvements in all metrics when comparing Dyna-DM and Insta-DM. Our model can remove most static objects from these object motion estimations while isolating truly dynamic objects, thereby leading to significant improvements in pose estimation. This further improves the reconstructions and leads to improved depth estimations. 
When re-training Insta-DM using a maximum of 13 dynamic objects, we see worse results than reported in the original paper \cite{lee2021learning}. We believe this is because our method is able to interpret many potentially dynamic objects, whereas, Insta-DM results in worse object motion estimation for a larger maximum number of dynamic objects processed. 




Note that in some real-life circumstances, we have the limitation that there will be unique dynamic objects like debris that would not be segmented in this case. Therefore for an efficient, truly autonomous vehicle, we would have to address these safety issues.


\section{Conclusion}
Dyna-DM reduces memory and power usage during training monocular depth-estimation while providing better accuracy in test time.
Although not all dynamic objects are always classified accordingly (e.g., slow-moving cars) Dyna-DM detects significant movements which are the ones which would cause the greatest reconstruction errors.
We believe that the best step forward is to make this approach completely self-supervised by detecting all dynamic objects, including debris, using an auto-encoder for safety and efficiency. This means we will have to be able to handle textureless regions and lighting issues which will be shown in future work.
Other further improvements include using depth maps to determine objects' distances, and removing object motion estimations for objects at larger distances.

\section*{Acknowledgment}
Most experiments were run on Aston EPS Machine Learning Server, funded by the EPSRC Core Equipment Fund, Grant EP/V036106/1.





































\bibliographystyle{IEEEtranstyle}
\bibliography{mybibfile}




\end{document}
