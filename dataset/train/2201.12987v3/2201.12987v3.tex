\documentclass{article}

\usepackage{icml2022_author_response}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}       \usepackage{booktabs} \usepackage{amsfonts}       \usepackage{nicefrac}       

\usepackage{lipsum}

\begin{document}


\newcommand{\Rt}{\textbf{R3}}
\newcommand{\Rf}{\textbf{R4}}
\newcommand{\Rv}{\textbf{R5}}

\newpage
We thank the reviewers for the valuable feedback and actionable suggestions. Overall, all the reviewers greatly appreciate our observations on the issues of the post-hoc interpretation framework and the effectiveness of our method demonstrated by extensive experiments. \Rt~and \Rv~further proposed many \emph{insightful} suggestions including exploring interpretability under self-supervision, investigating node features, and adopting two different GNNs as graph encoders, which we believe are good future directions. Actually, we have tried injecting stochasticity into node features and using two different GNNs. We have detected some interpretable node features. And, using two GNNs sometimes works better but not significantly better. So, we tend to keep the model simple to convey our key messages more clearly. 

Next, we focus on addressing the main questions raised by the reviewers. For the presentation issues and the typos, we will address them accordingly in the final version.   

\textbf{GSAT v.s. IB-subgraph.} GSAT and IB-subgraph are similar in the spirit of using the IB principle. But other than that, the two models are substantially different. Regrading \Rt's question on the randomness in IB-subgraph, our claim that IB-subgraph does not inject stochasticity, i.e.  in IB-subgraph is a deterministic function, is \emph{correct}. The sampling process in IB-subgraph refers to sampling batches of graphs  to estimate . Here, each  generates one deterministic , which thus reduces  to . In GSAT, however,  is random.

The above observation may explain \Rt's another question on the large variances of IB-subgraph. This is an issue of IB-subgraph itself and is also mentioned in their paper. Specifically, we believe as the estimation of  depends on  across the entire dataset, it can be extremely unstable when one just uses mini-batches of  to estimate it. We have tried to tune IB-subgraph and the details can be found in L846. Rather than estimate , GSAT optimizes a variational upper bound of , whose estimation is much stabler. With or without stochasticity, GSAT and IB-subgraph have very different information control mechanisms, which also denies \Rf's claim that two models use the same mutual information and thus GSAT is not novel. 



\textbf{The novelty of GSAT.} We respectfully disagree with \Rf's statement that our theory and method make marginal contributions. \Rf's claim that GSAT and PGExplainer are only different in whether the extractor and the predictor are co-trained or not is wrong. \underline{First}, this claim \emph{misses} our key message: Post-hoc methods have fundamental drawbacks (as shown in Sec. 3.2 and Fig.5) while co-training can address the issues. This message by itself is significant. This contribution gets appreciated by \Rt~and \Rv. \Rf~also commented later by oneself ``it inspires more works focusing on this problem'' while \Rf~seemed to neglect this contribution when comparing GSAT with PGExplainer. \underline{Second}, the two models are also different in their working mechanisms.  PGExplainer performs the search of interpretable  from the discrete subgraph space with subgraph size regularization. So, after training,  sampled by PGExplainer tends to be small, and is directly used for interpretation. However, in GSAT, the sampled graph  contains most of the edges (as ) in  and is thus of large size, as discussed in L227, L243 and L374 (right columns). So,  is not directly used as interpretable patterns. Instead, \emph{the almost deterministic part of }, i.e. the edges with  provides explanations. Such difference is due to the fact that GSAT adopts information control (by imposing randomness) while PGExplainer adopts sparsity control (via -norm). Fig. 7 reveals the advantage of information control over sparsity control.

\Rf's argument that our work lacks novel theory is unwarranted. Besides what we have stated when comparing with IB-subgraph, our Thm. 4.1 proves the significance of co-training: The critical subgraphs are the optimal solutions of \emph{co-training via IB}. Neither L2X nor IB-subgraph has such a guarantee. Moreover, L2X is not post-hoc so \Rf's argument is incorrect. Also, we assume the lower bound \Rf~referred to is the one between Eq.(3) and Eq.(4) in the L2X paper (the only lower bound we found in the paper). It just corresponds to Eq.(6) in our paper. An even more significant aspect of GSAT compared to L2X is that L2X assumes that the interpretation is selected from a -dim feature subspace of regular inputs, which cannot be applied to irregular data (how to define a feature subspace for graph data?). Our interpretation based on information control instead of subspace selection well addresses the problem (proved by Thm. 4.1).   

\textbf{The choice of the marginal distribution .} \Rf's argument by viewing our choice of  as an assumption is wrong. Our choice of  is not an assumption. Any choice of  satisfies the bound Eq.(7), as stated in L261. We just choose the one that is easy to compute. This kind of choice is similar in the spirit of using standard Gaussian as the latent distribution for variational auto-encoders, which just works as a regularizer.    

\textbf{Other clarifications.} 
\textbf{Responses to} \Rt: 1) Although conditional independence between sampled edges indicates that the edges are essentially mutually dependent, we agree further injecting prior knowledge may benefit GSAT for some applications. 
2) We think the observation in Table 4 for  is because when , injecting stochasticity loses regularization and becomes more aggressively, which could help prevent a model from overfitting the spurious correlation (for large ). However, there is no guarantee that it always happens. 3) NOSTOCH- = 0 corresponds to vanilla binary attention on edges, which shares the similar spirit of GAT but not exactly the same.
4) We use all edges to evaluate AUC and accuracy by following previous works. If one wants to select top  edges, best  may correlate with . 
\textbf{Responses to} \Rv: 1) We evaluate AUC across all edges using edge-level explanation labels.
2) GIN+GSAT means replacing the base graph encoder of GSAT with a GIN.








\newpage
We thank the reviewers for the valuable feedbacks and actionable suggestions. Overall, all the reviewers greatly appreciate our observations on the issues of the previous post-hoc interpretation frameworks, and the effectiveness of our method demonstrated by extensive experiments. \Rv~further proposed many \emph{insightful} suggestions including the investigation of node features or adopting two different GNNs as the extractor and the predictor, respectively. Actually, we indeed have tried those by injecting stochasticity into node features and using two different GNNs. We have detected some interpretable node features. Also, using two GNNs sometimes works better but not significantly better. So, we tend to keep the model simple to convey our key messages more clearly. Next, we focus on addressing the questions raised by the reviewers. For the presentation issues and typos, we will address them accordingly in the final version.   

\textbf{GSAT v.s. IB-subgraph.} GSAT and IB-subgraph are similar in the spirit of using the IB principle. But other than that, two models are substantially different. To respond to \Rt's question on the randomness in IB-subgraph, our claim that IB-subgraph does not inject stochasticity, i.e.  in IB-subgraph is a deterministic function is \emph{correct}. The sampling process in IB-subgraph refers to sampling batches of graphs  to estimate . Here, each  generates one deterministic , which thus reduces  to . In GSAT, however,  is random.

The above observation also explains \Rt's another question, i.e., why IB-subgraph has large variance in L391 and Table 1. Specifically, the estimation of  depends on  across the entire dataset, which could be extremely unstable when one just uses mini-batches of  to estimate. We have tried to tune IB-subgraph and have explained the details in L846. Rather than estimate , GSAT optimizes a variational bound of , whose estimation is much stabler. With or without stochasticity, GSAT and IB-subgraph have very different information control mechanisms, which also denies \Rf's argument that two models use the same mutual information and thus GSAT is not novel. 



\textbf{The novelty of GSAT.} We respectfully disagree with \Rf's statement that our theory and method make marginal contributions. \Rf's claim that GSAT and PGExplainer are only different in whether the extractor and the predictor are co-trained or not is wrong. \underline{First}, this claim \emph{misses} our key message: Post-hoc methods have drawbacks (as demonstrated by Sec. 3.2 and Fig.5) while co-training can address the issue. This message by itself is significant. This contribution gets  appreciation by \Rt~and \Rv. \Rf~also commented later by oneself ``it inspires more works focusing on this problem'' while \Rf~seemed to neglect this contribution when comparing GSAT with PGExplainer. \underline{Second}, the two models are also different in their interpretation mechanisms besides using co-training or not. Specifically, PGExplainer performs the search of interpretable  from the discrete subgraph space with subgraph size regularization. So, after training,  sampled by PGExplainer tends to be small, and is directly used for interpretation. However, in GSAT, the sampled graph  contains most of the edges (as ) in  and is thus of large size, as discussed in L227, L243 and L374 (right columns). So,  cannot be directly used as interpretable pattern. Instead, \emph{the almost deterministic part of }, i.e. the edges with  provide explanation. Such difference is due to the fact that GSAT adopts information control (by adding randomness) while PGExplainer adopts sparsity control (via -norm). Fig. 7 reveals the advantage of information control over sparsity control.

\Rf's argument that our work lacks novel theory is unwarranted. Our Thm. 4.1 proves the significance of co-training: Co-training via the IB principle can detect the critical subgraphs. Neither L2X nor IB-subgraph has such a guarantee. Moreover, L2X is not post-hoc so \Rf's argument is wrong again. Also, we are not clear which lower bound in L2X \Rf~refered to. The only lower bound we saw is between Eq.(3) and Eq.(4) in the L2X paper, which just corresponds to Eq.(6) in our paper. An even more significant aspect of our work compared to L2X is that L2X assumes that the interpretation is selected from a -dim subspace of the input feature space, which is good for data in a regular space, where the subspace can be well defined. However, for graph-structured data, how to define subspace is unclear. Our interpretation based on IB well addresses the problem, which also gets theoretically demonstrated in Thm. 4.1.   

\textbf{The choice of the marginal distribution .} \Rf's argument by viewing our choice of  as an assumption is wrong. Our choice of  is not an assumption. Any choice of  satisfies the bound Eq.(7). We just choose one that is easy to be computed. This kind of choice is similar in the spirit of using standard Gaussian as the latent distribution for variational auto-encoders, which just works as a regularizer.    

\textbf{Other clarifications.} 
\textbf{Responses to} \Rt: 1) \emph{Conditional} independence between the sampled edges just indicates that the edges are essentially mutually dependent. 2) We thank \Rt~for pointing out the worth-trying future direction on interpretability with self-supervision. 3) We guess the observation in Table 4 when  is because when , the injected stochasticity loses regularization and may work most aggressively to prevent the model from overfitting the spurious correlation (for large ), but when this happens may not have guarantee. 4) NOSTOCH- = 0 corresponds to vanilla binary attention on edges, which shares the similar spirit of GAT but not exactly the same.
5) The interpretation metric AUC does not need to specify the number of selected edges by definition. If one wants to indeed select top-ranked  edges, best  may be correlated with . \textbf{Responses to} \Rv: 1) We evaluate AUC across all edges using edge-level explanation labels by following previous works.
2) GIN+GSAT means replacing the base graph encoder of GSAT with GIN.















\newpage
We thank the reviewers for their time, valuable feedback and insightful suggestions. Overall, it seems most of the reviewers found our contributions significant and interesting. Below we address the questions raised by the reviewers and clarify the fundamental misunderstandings that R4 had.

\textbf{GSAT and IB-subgraph.} R3 raised a question on our claim that there is no stochasticity in IB-subgraph, which reduces their  to the entropy . We believe the statement is correct, but we will make it clearer in the final version. The sampling process mentioned in IB-subgraph refers to sample batches of inputs to approximate the expected values in their Eq.(9), which means their sampling occurs when sampling batches of , and each  generates  deterministically. This makes  and reduces  to . Then following our reasoning in Sec. 4.3 it should be clear that GSAT is fundamentally different from IB-subgraph, which should answer the novelty concern of R4. In addition, our Theorem 4.1 for IB is also critical, as this may shed lights on discovering invariant rationale purely based on IB.

\textbf{The novelty of GSAT.} R4 raised concerns on the novelty of GSAT, but it seems there are fundamental misunderstandings, and it is unfair to judge the novelty purely based on the seemingly similar model pipeline. GSAT is fundamentally different from previous works as shown in Sec. 3.1 and 4.3.

First, it seems R4 assumes there is not much difference between post-hoc methods and inherently interpretable models. As much appreciated by R5, we point out the fundamental drawbacks of post-hoc methods with both theoretic (Fig.4) and empirical (Fig.5) justifications (Sec. 3.2). We believe post-hoc methods are not the right way to design interpretable models and show inherently interpretable models can be neat and can achieve better results. As appreciated by R3, it is important for the community to be aware of this.
Second, it seems there is a fundamental misunderstanding of the mechanism of GSAT since R4 summarized that ``the sampled graphs can be taken as good explainability results". We would like to emphasize that the sampled graphs  does NOT provide explainability, which makes GSAT fundamentally different from all previous works. As discussed in the right column of L227, L243 and L374, GSAT always generates dense  that contains most of the edges () in , which is a critical reason why GSAT does not hurt accuracy. Then, with a dense , the interpretability of GSAT comes from the stochasticity control. Intuitively, driven by the cross-entropy loss, GSAT must assign critical edges low dropping probability (); regularized by the information control term (Eq.(9)), GSAT can push  of non-critical edges close to , as dropping them may not hurt accuracy. Then, the ranking of  provides interpretability. 

Now, it should be clear that GSAT is fundamentally different from PGExplainer despite the seemingly similar pipeline. 
R4 may also misunderstand L2X. L2X is not post-hoc and only lower bounds MI via cross-entropy, which is a common practice and it was not first introduced by L2X neither. The bound is adopted by most IB-related works and it is nothing but our Eq.(6). So, we believe the suggestion ``provide a strong theoretical guarantee ... L2X provides ... for post-hoc explanation" itself is also a wrong statement. As shown in Sec. 3.1, since L2X needs pre-decide a fixed  number of features, it is impossible to apply on irregular data, and this is where our information regularization makes a difference.

\textbf{Presentation and literature.} We agree with the reviewers and will refine our presentation and literature accordingly.

\textbf{Other clarifications.} 
R3: 1) Although we focus on generalizing the idea of GSAT without injecting proir knowledge, we totally agree that introducing conditional dependency can be a future direction;
2) We greatly thank R3 point out a potential new research direction on interpretability with self or un-supervised models via GSAT. It is definitely an interesting future direction that is worth trying; 
3) Instability is a fundamental issue of IB-subgraph, which is also mentioned by their authors. We tried our best to tune it based on their source code, and we show how we tune it in L846;
4) Regarding the ablation study when , we believe this is because when , it applies GSAT without regularization, which may introduce stochasticity more aggressively and potentially help prevent a model with limited expressive power from overfitting dense  when  is extremely large, but there is no performance guarantee. As shown in Table 8, with a more powerful model,  greatly hurts results;
5) NOSTOCH- = 0 corresponds to vanilla attention on edges;
6) We evaluate AUC and accuracy using all edges following previous works, but in practice one can select top edges;

R4: Regarding the sampling distribution, based on the wording and the notation, if R4 refers to the distribution of , which is , where we assume  are conditionally independent given , as pointed out by R3. We believe this is a reasonable assumption. If R4 refers to  of the marginal distribution, as stated in L261, no matter how we choose , the related bound Eq.(7) always holds, which gives no reason to make it complex. Besides, independency assumption on the marginal distribution is common, and as shown empirically it is good enough to achieve SOTA performance. So, we believe it is not a strong assumption.

R5: 1) We totally agree that feature importance is also needed, and actually we are working on this for a recent project to generalize GSAT;
2) We indeed tried to use separate GNN encoders in the early stage of this work and we observed slightly better performance. But this might need to double the trainable parameters, so later we focused on the shared encoder version. We totally agree with R5 that this is an interesting future direction to further improve GSAT;
3) We evaluate AUC across all edges using edge-level explanation labels following previous works;
4) Yes, GIN+GSAT means replacing the base graph encoder of GSAT with GIN.








\newpage
We thank the reviewers for their time, valuable feedback, and insightful suggestions. Overall, it seems most of the reviewers found our contributions significant and interesting, but that there was a fundamental misunderstanding on the working mechanism of GSAT, which makes GSAT sound like nothing novel. We will clarify how GSAT is fundamentally different from previous methods in detail below. Besides, the reviewers also raised questions regarding our claim on no stochasticity in IB-subgraph (R3), 

\textbf{The fundamental difference between GSAT and IB-subgraph.} R3 raised a question on our claim that there is no stochasticity in IB-subgraph, which reduces their  to the entropy  and makes GSAT fundamentally different from IB-subgraph. We would like to clarify that by stochasticity we mean the stochasticity on obtaining  given its original graph . The authors of IB-subgraph refer the sampling method as the way to approximate their expectation terms in Eq. 9 using a batch of input graphs. In this case, essentially each input graph   generates a subgraph  deterministically, which makes , and the Eq. 9 can be approximated by a batch of . Therefore, their sampling essentially occurs when sampling batches of , but our stochasticity is imposed on generating  given . With this difference, it then should be clear that GSAT is fundamentally different from IB-subgraph as IB-subgraph are essentially minimizing  by decreasing , but GSAT minimizes  mainly by increasing , which should also answer the novelty concern raised by R4. Basically, IB-subgraph proposes a way to optimizing  by applying the DONSKER-VARADHAN representation of KL-divergence, which reduces to optimize  and makes their model complicated, inefficient, unstable and implicitly prefer small-sized . In contrast, we propose to minimize  by deriving an upper bound for it based on the stochastic attention mechanism. We refer R4 to L267 in section 4.3 for more detail on the difference.

\textbf{The Novelty of GSAT.} R4 raised concerns on the novelty of GSAT. It seems there are fundamental misunderstandings on the working mechanism of GSAT, and R4 overlooked all our contributions, which are much appreciated by other reviewers. And it is also unfair to judge the novelty purely based on the model pipeline. From many perspectives, GSAT is fundamentally different from previous works, such as PGExplainer, L2X and IB-subgraph mentioned by R4. 

First of all, it seems R4 assumes there is not much difference between post-hoc explanation methods and inherently interpretable models. This is NOT correct. Designing inherently interpretable models that achieve both SOTA interpretability and generalization capability is arguably harder, and this is why DIR and IB-subgraph took very complicated ways to ensure inherent interpretability and generalization capability. GSAT simplifies the current inherently interpretable GNNs significantly while obtaining significantly better performance because of the neat stochastic attention mechanism we propose. Second, as much appreciated by R5, most current GNN explanation methods focus on post-hoc interpretability, but we point out the fundamental drawbacks of post-hoc methods with both theoretic and empirical justifications (see Sec. 3.2). We believe post-hoc methods are not the right way to design interpretable ML models and it is important to make the community aware of this. Third, it seems to be a fundamental misunderstanding of the working mechanism of GSAT as R4 summarizes that ``Therefore, the sampled graphs can be taken as good explainability results". We would like to emphasize that the sampled graphs  does NOT provide explainability, which makes GSAT fundamentally different from all previous works. As discussed in the right column of L243 and of L374,  generated by GSAT is always dense, because setting  in Eq. 9 generally performs great enough and we never set  to be smaller than , which means  on average contains  of nodes/edges in the original graph . This is why GSAT can ensure generalization performance while provide SOTA interpretability, as sparsity constraint is too aggressive for inherently interpretable models and will significantly hurt model performance. Then, with a dense , the interpretability of GSAT comes from the stochasticity level that the model allows for each edge. The rationale behind is that critical edges cannot be dropped too frequently as this would hurt predication accuracy, so the stochasticity of these critical edges should be low enough (ideally ), while for non-critical edges they could be fine to remain high stochasticity levels (aka ). Hence, the rank of  provides interpretability, and GSAT frees any potentially biased constraints, which makes it fundamentally different all previous works and potentially can be extended to many other ML domains for inherent interpretability.

Now, we further show how GSAT is fundamentally different from PGExplainer, L2X and IB-subgraph mentioned by R4. As for PGExplainer: 1) PGExplainer is a post-hoc method and suffers the issues we discussed in Sec. 3.2, and thus there is no guarantee that PGExplainer can provide faithful interpretability and thus GSAT performs significantly better; 2) The working mechanism of GSAT is totally different from PGExplainer despite seemingly similar pipeline. PGExplainer adopts sparsity constraint and the resulting sparse subgraphs provide post-hoc explanations. As we discussed above, sparsity constraint is not suitable for inherently interpretable models and GSAT encourages generating dense subgraphs and provide interpretability by stochasticity control; 3) Stochasticity control we proposed sets free GSAT from any potentially biased constraints such as sparsity and connectivity constraints, which is commonly adopted by previous works. As for L2X: 1) we do not know which solid proofs of the lower bound of mutual information that R4 is referring to. As far as we know, L2X only lower bounds mutual information using cross-entropy (in its Eq. 4), which is a common practice to lower bound mutual information and it is adopted by most of IB-related works, and it is nothing but Eq. 6 in our paper; 2) As discussed in the left column of L122, L2X is fundamentally not suitable for tasks with irregular data or data with many features, as it needs to pre-decide a fixed  number of features to use, and then search over the feature space. L2X is almost impossible to apply on graph learning tasks, and this is where our proposed information constraint makes a difference; 4) As for the suggestion made in 3. Soundness, saying ``providing rigor bound proofs... As an example, L2X provides such a bound guarantee on the mutual information for post-hoc explanation methods.", we believe the statement is wrong as: 1) L2X is not post-hoc; 2) L2X only lower bounds mutual information using cross-entropy, which is essentially our Eq. 6 and we don't see how this relates to ``strong theoretical guarantee for further interpretation methods"; 3) the drawbacks of post-hoc methods we show in Sec. 3.2 is more than just intuitive explanations. It shows from the perspective of optimization theory, post-hoc methods are always sub-optimal due to the projection step they essentially perform. As for IB-subgraph: as we have showed in the previous section, GSAT is fundamentally different from IB-subgraph. We not only propose a new way to optimize GIB with stochastic attention mechanism, and also significantly outperform it in terms of interpretability, efficiency, stability and simplicity. Besides, to best of our knowledge, no previous works have ever showed Theorem 4.1 for IB, which shows the potential of discovering invariant rationale purely based on IB. Unfortunately, this contribution is also ignored by R4.

\textbf{Presentation and literature.} We agree with R3 and we'll add some examples to illustrate the problem of post-hoc methods and re-write and conclusion paragraph in the final version. We also agree class acc. vs AUC would be interesting to see, and we'll include this. R4 suggests to better show how specifically different GNNs can be incorporated with GSAT. We'll add more description on this, but we would like to clarify that directly replace the GNN module shown in figure 1 with specific GNNs like GIN or PNA is enough to apply GSAT and there is no other tricks. As for the literature, we agree with R4 and R5 and we'll cite the paper and discuss more on gradient-based methods in the final version of the paper.

\textbf{Other clarifications.} R3: 1) We appreciate the insightful suggestion to introduce conditional dependency based on prior knowledge to further improve the model performance. Even though in this paper we just use the simplest we assumption without prior knowledge, we agree with R3 and we believe this can be a future direction to improve GSAT; 2) Regarding the loss function, we greatly thank the valuable suggestion for training GSAT without class labels, this is definitely an interesting future direction that is worth trying to provide interpretability for self or un-supervised models, even though currently we only consider interpretability under supervision and did not derive the bound from the perspective of variational autoencodoers; 3) Regarding the large variance of IB-subgraph, this is essentially a fundamental issue of IB-subgraph and the authors of IB-subgraph are also aware of this problem and mentioned this problem in their paper, which can be found in the last to second paragraph in their Section 1, their Section 4.3, their table 2 and their second paragraph in section 5.3. They proposed a connectivity loss to alleviate this problem, but it seems to be not enough and imposed some unnecessary constraints. For example, it would favor generating connected and small-sized subgraphs, which may not work well when these assumptions do not hold. Here we tried our best to tune IB-subgraph and we state how we tune it in L846. Here we would also like to emphasize that IB-subgraph indeed proposed a way to optimize GIB, but it is so inefficient and would need, for example,  hours to train  epochs over  seeds on OGBG-Molhiv on a Quadro RTX 6000. In contrast, GSAT would need  hours in total. So, GSAT is generally much neater and performs better; 4) ``the best prediction accuracy for  is achieved with ", we believe this shows that the general idea of stochastic attention works. When , it means we still apply stochastic attention, but we do not regularize its values. We believe this may result in an initialization issue and high variance, as empirically showed by the other two columns in table 4 and table 8. But it is possible in some cases its initialization is good enough and performs great. However, we cannot guarantee this is always the case, and thus a regularization term is still needed to make sure there is no initialization issue; 5) Yes, GSAT-NOSTOCH- = 0 corresponds to using vanilla attention on edges, which is similar to GAT; 6) We mentioned in L251 to select top edges for the case that one must obtain certain critical subgraphs (rather than relative importance). But when we evaluate GSAT, we follow previous works, and do not set any budget, and AUC and accuracy AUC and accuracy are computed across all edges. We agree with R3 that when this is the use case, it could be better to consider both  and the number of edge to be use simultaneously. We thank R3 for giving so many insight suggestions, we would work on them and make GSAT better.

R4: Regarding the concerns of the assumption on the marginal distribution , based on the wording in the first section of the review and the notation used, we are not sure if R4 actually refers to the sampling distribution of , which is essentially  as discussed in L239, where we assume  are conditionally independent given , as pointed out by R3. We believe this is a reasonable assumption and we agree with R3 that it is likely that it could further benefit GSAT by introducing conditional dependency based on prior knowledge. Yet, to generalize the idea of GSAT, in this paper we do not consider using any prior knowledge. However, if R4 essentially refers the sampling distribution of  of the marginal distribution, then we would like to emphasize that  is used to characterize the marginal distribution . And as stated in L261, no matter how we choose , the related bound Eq.(7) always holds, which gives us no reason to make it complicated as we aim at in this paper generalizing the idea of GSAT and making it neat instead of introducing any potentially biased constraints. In addition, as shown empirically by our experiments, such assumption on  is already good enough to achieve SOTA performance. Nonetheless, we do agree that it is possible to improve GSAT by incorporating prior knowledge in , which could be a future direction.

R5: We greatly appreciate the insightful suggestions from R5 and we indeed are working on some of your suggestions for a recent project. 1) it is true this in this work we only consider interpretability on graph structures (edges/nodes), because graph structures are one of the most important features in traditional graph learning tasks. However, the importance of node/edge features could also be interesting to investigate for modern GNNs. We totally agree with R5 and we are working on generalizing the idea of GSAT to node/edge features in a recent project. Preliminary results show that we can, but some part of current GSAT needs to modify accordingly to stabilize training. Forgive us that we cannot share the exact detail, but we happen to think along the same direction to generalize GSAT; 2) as for using different graph encoders in GSAT, we had similar thoughts with R5 again. We greatly thank R5 to thoroughly review our work and provide valuable feedback. In the early stage of this work, we indeed tried to use two different graph encoders and we observed slightly better performance. But this might need to double the trainable parameters, and we find that a shared encoder can already provide SOTA performance. So, we later focused on the neater version of GSAT and use a shared encoder. We totally agree with R5 that GSAT could be further improved along this direction and it would also be interesting to use different GNN backbones for the two graph encoders; 3) as for how ROC AUC is computed, we follow previous works, we use edge-level explanation labels to evaluate all our experiments, we'll make this clear in the final version; 4) Yes, GIN+GSAT means replacing the base graph encoder of GSAT with GIN.




\end{document}
