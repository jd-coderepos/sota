
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{floatrow}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage[flushleft]{threeparttable}
\usepackage{amsmath}
\usepackage{booktabs}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{lipsum}



\title{Angle-optimized Text Embeddings}



\author{Xianming Li, Jing Li \thanks{Corresponding author} \\
Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR \\
\texttt{xianming.li@connect.polyu.hk, jing-amelia.li@polyu.edu.hk}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. 
However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. 
To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE.
The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. 
To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. 
Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data.
Extensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone.  
These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.
\end{abstract}

\section{Introduction}
The development of text embeddings \citep{skipthought-ryan-2015,DBLP:conf/naacl/HillCK16,conneau-etal-2017-supervised,cer-etal-2018-universal,sbert-nils-2019,simcse_gao_2021} is an essential research challenge in the NLP community. 
Text embeddings effectively feature key semantic and syntactic information in language, which broadly affects the performance of downstream tasks, such as text classification \citep{li2021merging}, sentiment analysis \citep{suresh-ong-2021-negatives,zhang2022leveraging}, semantic matching \citep{grill2020bootstrap,lu2020deep}, clustering \citep{sbert-nils-2019,contrastive_review_xu_2023}, and question-answering (QA) system \citep{yue-etal-2021-contrastive}. 
In particular, text embedding models play a crucial role in LLMs such as ChatGPT \citep{chatgpt, gpt4}, LLaMA \citep{touvron2023llama, touvron2023llama2}, and ChatGLM \citep{du2022glm}-based applications. These LLM-based applications heavily rely on high-quality text embeddings for tasks such as vector search, where related documents are retrieved for LLM QA \citep{asai-etal-2023-retrieval}. 
\begin{figure}[ht]
    \centering
\includegraphics[width=0.49\textwidth]{cosine-saturation.pdf}
    \caption{The saturation zones of the cosine function. The gradient at saturation zones is close to zero. During backpropagation, if the gradient is very small, it could kill the gradient and make the network difficult to learn.}
    \label{cosine-saturation-zone-figure}
\end{figure}

Recent studies \citep{simcse_gao_2021,promcse_jiang_2022,chuang-etal-2022-diffcse,chanchani-huang-2023-composition,zhuo-etal-2023-whitenedcse} have utilized pre-trained language models such as BERT \citep{DevlinCLT19BERT} and RoBERTa \citep{roberta-liu-2019} in combination with contrastive learning to enhance the quality of text embeddings. These approaches involve pulling semantically similar samples together and pushing apart those not \citep{simcse_gao_2021}. 
In these contrastive models, positive samples that are semantically similar can be generated by data augmentation, while negative samples that are dissimilar are selected from different texts within the same mini-batch (in-batch negatives). However, supervised negatives are underutilized, and the correctness of in-batch negatives is difficult to guarantee without annotation, which can lead to performance degradation. Although some models such as \citep{simcse_gao_2021} optimize hard negative samples, they rely on strict triple formats (, , ). While most existing supervised STS datasets only provide pairs (, ) or (, ), where  refers to the positive sample of  while  the negative sample of .
Thus, most contrastive models are used in unsupervised settings yet might not benefit from human supervision.

For supervised STS \citep{sbert-nils-2019,cosent_su_2022}, most efforts to date employed the cosine function in their training objective to measure the pairwise semantic similarity. 
However, the cosine function has saturation zones, as shown in Figure \ref{cosine-saturation-zone-figure}.
It can impede the optimization due to the gradient vanishing issue and hinder the ability to learn subtle distinctions between texts in backpropagation. 
Additionally, many STS datasets such as MRPC \footnote{https://www.microsoft.com/en-us/download/details.aspx?id=52398} and QQP \footnote{https://www.quora.com/q/quoradata/} provide binary labels representing dissimilar () and similar (), which naturally fall within the saturation zone of the cosine function. 
To overcome this challenge, this paper proposes a novel angle-optimized text embedding. It optimizes not only the cosine similarity between texts but also the angle to mitigate the negative impact of the saturation zones of the cosine function on the learning process. 
Specifically, it first divides the text embedding into real and imaginary parts in a complex space. 
Then, it follows the division rule in complex space to compute the angle difference between two text embeddings. 
After normalization, the angle difference becomes an objective to be optimized. 
It is intuitive to optimize the normalized angle difference, because if the normalized angle difference between two text embeddings is smaller, it means that the two text embeddings are closer to each other in the complex space, i.e., their similarity is larger.


In the STS experimental setup, we observed that the majority of existing STS benchmarks focus on evaluating models on short texts. Unfortunately, there is a lack of datasets specifically designed to evaluate the STS performance of models on long texts. Long texts are prevalent in real-world applications such as financial documents, legal documents, and health reports \citep{li-etal-2023-recurrent}. 
To tackle this challenge, this paper presents a new high-quality long-text STS dataset. This dataset allows for a more thorough evaluation of model performance on long texts. Specifically, the dataset is collected from GitHub Issues with roughly K samples, we use the duplicate issues as the positive samples and the non-duplicate issues as the negative samples.

We first experimented with both short and long-text datasets and showed that AnglE outperforms the SOTA STS models in both transfer and non-transfer STS tasks. 
For example, AnglE shows an average Spearman correlation of  in non-transfer STS tasks, compared to  for SBERT.
Then, an ablation study shows that all components contribute positively to AnglE's superior performance.
Next, we discuss the domain-specific scenarios with limited annotated data that are challenging for AnglE-like supervised STS, where it is observed that AnglE can work well with LLM-supervised data.
Finally, we find that AnglE can benefit downstream retrieval applications and can learn representations closer to actual representations.

In summary, the contributions of this paper are listed as follows: 

 We investigate the negative effects of saturation zone in the cosine function widely applied in STS and propose a novel angle-optimized text embedding model to mitigate this issue.

 We extend the existing STS benchmark with a newly collected long-text dataset from Github Issues to allow a more comprehensive empirical study in STS.

 We present extensive experiments on STS and demonstrate that AnglE can substantially improve the text embedding quality in various scenarios.




\section{Related Work}
This section is organized as follows: we first introduce the unsupervised approaches, then the supervised approaches, and finally give a summary.

\paragraph{Unsupervised Approaches} Early studies \citep{DBLP:conf/naacl/HillCK16,pagliardini-etal-2018-unsupervised} have demonstrated the efficacy of augmenting word2vec \citep{word2vec_mikolov_2013} with n-gram embeddings, yielding strong results in text embeddings.
Recently, BERT-flow \citep{li-etal-2020-sentence} has introduced a flow-based approach that maps BERT embeddings to a standard Gaussian latent space. On the other hand, BERT-whitening \citep{su2021whitening} applies the whitening operation to BERT embeddings to enhance text embeddings.
Furthermore, very recent research \citep{carlsson2020semantic, zhang-etal-2020-unsupervised, giorgi-etal-2021-declutr, simcse_gao_2021, consert_yan_2021, chuang-etal-2022-diffcse, promcse_jiang_2022, zhuo-etal-2023-whitenedcse} has focused on leveraging contrastive objectives to improve the quality of text embeddings.

\paragraph{Supervised Approaches} Supervised text embeddings usually perform better than their unsupervised counterparts \citep{simcse_gao_2021}. Various studies have effectively utilized supervised datasets to enhance the learning of text embeddings. In particular, \citet{conneau-etal-2017-supervised} introduced a method that leverages supervised Natural Language Inference (NLI) tasks for this purpose. Building on a transformer backbone, USE \citep{cer-etal-2018-universal} incorporates the SNLI dataset to augment unsupervised training, resulting in improved performance. Furthermore, SBERT \citep{sbert-nils-2019} enhances text embedding by combining BERT with a siamese architecture. \citet{jiang-etal-2022-promptbert,jiang2023scaling} proposed the use of prompt engineering to improve text embeddings.

However, most existing models optimize the cosine similarity but neglect the negative effect of the saturation zone of the cosine function. To address this issue, this paper proposes a novel angle-optimized text embedding model to improve the quality of text embedding.

\section{Methodology}
This section will introduce the components of the proposed angle-optimized text embedding model, including the input layer, cosine objective, in-batch negative objective, and angle objective.

\subsection{Input Layer}
For the input sentences, we first apply padding to ensure a consistent length . Next, we map each word to a continuous -dimensional space to produce word embeddings . These word embeddings are then concatenated to form the model input: . Subsequently, the model input is passed through an encoder such as BERT \citep{DevlinCLT19BERT}, RoBERTa \citep{roberta-liu-2019}, and LLaMA \citep{touvron2023llama, touvron2023llama2}  to obtain the contextual representation .

\subsection{Cosine Objective}
Following the prior study \citep{cosent_su_2022}, we employ the cosine objective function for end-to-end optimization of cosine similarity between representations, as follows:

where  is a temperature hyperparameter,  is the cosine similarity function, and  is the similarity between  and . By optimizing the , we expect the cosine similarity of the high similarity pair to be greater than that of the low similarity pair.

\subsection{In-batch Negative Objective}
To further improve performance, we integrate the in-batch negative objective function. Because in-batch negative samples can serve as a data augmentation technique, which can benefit the generalization. Unlike existing contrastive learning models \citep{simcse_gao_2021,consert_yan_2021} that generate positive samples through data augmentation, we use supervised positive samples. Recognizing that there might be identical sentences within a batch that are not explicitly labeled as positive samples, causing them to become in-batch negatives, we identify these duplicate sentences and assign them as positive samples, thereby reducing potential noise. 
The formulation for the in-batch negative objective function (ibn) is as follows:

where  is a temperature hyperparameter,  stands for the -th batch,  and  are the respective positive samples of  and ,  represents the number of positive pairs in -th batch,  is the batch size, and  is the cosine similarity function.

\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=0.6\textwidth]{angle-complex-space.pdf}
         \caption{}
         \label{angle-complex-space}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=0.7\textwidth]{saturation-angle.pdf}
         \caption{}
         \label{figure-saturation-angle}
     \end{subfigure}
    \caption{(a) Division in complex space.  is the angle difference between dividend  and divisor  in complex space. (b) Angle optimization in cosine saturation zones. Even though  could kill the gradient, the corresponding angle difference in complex space is still distinct for optimization.}
    \label{}
\end{figure}

\subsection{Angle Objective}
We found that both the cosine and in-batch negative objectives employ the cosine function to measure similarity. However, it is important to note that the cosine function includes saturation zones, which can hinder the optimization process. We optimize the angle difference in complex space to mitigate these adverse effects. Figure \ref{angle-complex-space} draws the division in complex space, and Figure \ref{figure-saturation-angle} depicts how angle optimization works in cosine saturation zones. To optimize the angle difference, we define  and  are the real part and the imaginary part of . We follow the implementation of \citep{sun2019rotate} to obtain  and  by the chunking strategy. Specifically, for the pair , their representations in the complex space are defined as follows:

where , , , and  .
To compute the angle difference between  and , we calculate division in complex space in polar coordinates, as follows:

where  and  represent the magnitudes of  and , while  and  denote the respective angles of  and . 
Next, we compute the value of  by the division rule in complex space, as follows:

By employing Eq. \ref{division-complex-1} and Eq. \ref{division-complex-2}, we can calculate the angle difference between  and  by multiplying both sides by , which can be seen as a normalization operation. In this paper, we determine the absolute normalized angle difference using the following expression:

Then, the angle difference can be optimized by the following objective function:

where  is a temperature hyperparameter and  is the similarity between  and . By optimizing the , our objective is to minimize the normalized angle difference for pairs with high similarity compared to those with low similarity.

Finally, we combine the aforementioned three objective functions in the following manner to form the final objective function:

where , , and  are constants.


\section{Experiment}



\subsection{Datasets and Evaluation Metrics}
\paragraph{Existing STS Benchmarks} We mainly evaluate our model on several widely-adopted STS datasets, namely: MRPC, QQP, QNLI \footnote{https://gluebenchmark.com/}, STS 2012-2016 \citep{agirre-etal-2012-semeval, agirre-etal-2013-semeval, agirre-etal-2014-semeval, agirre-etal-2015-semeval, agirre-etal-2016-semeval}, SICK-R \citep{marelli-etal-2014-sick}, and STS-B \citep{cer-etal-2017-semeval}. These datasets mainly consist of short text, but real-world scenarios often involve long text documents. Thus, we introduce a newly long-text dataset called \textit{GitHub Issues Similarity Dataset} to comprehensively evaluate the STS task. 

\paragraph{GitHub Issues Similarity Dataset}
We observed the presence of many duplicate issues on GitHub. Typically, the maintainers of open source organizations tend to mark these duplicate issues as closed with a comment like ``closing as a duplicate of \#id". Consequently, these duplicate issues inherently serve as a source of the STS task. It is also worth noting that most issues contain long texts because of the inclusion of extensive code within the issues.
To compile the dataset, we extracted duplicated issues from  popular open-source projects (see \ref{appendix_github_repos}) on GitHub using GitHub API \footnote{https://docs.github.com/en/rest}. The duplicated issues were used as positive samples, while the remaining issues were considered negative samples. Table \ref{table-github-issue-statistics} presents statistics of the GitHub Issues Similarity Dataset, while Figure \ref{github-issue-length-distribution} shows a violin plot illustrating the token-level text length distribution. The visualization reveals a substantial number of long texts. Specifically, the proportion of long texts (token length )  for the train, validation, and test sets is at , , and , respectively.

\paragraph{Evaluation Metrics} To ensure a fair comparison, we follow previous studies and use Spearman's correlation for evaluation. We use SentEval \citep{conneau-kiela-2018-senteval} to compute Spearman's correlation and report the results in the ``all'' setting, which is consistent with the baselines.

\begin{figure}\CenterFloatBoxes
\begin{floatrow}
\ffigbox[\FBwidth]
{\caption{Log token length distribution of the GitHub Issue Similarity Dataset.}\label{github-issue-length-distribution}}
{
    \includegraphics[width=0.8\linewidth]{github_issue_log_text_length_distrition.pdf}
}
\killfloatstyle\ttabbox[\Xhsize]
{\caption{Statistics of the proposed GitHub Issues Similarity Dataset. \#Positive denotes the count of positive pairs, and \#Negative represents the number of negative pairs.}\label{table-github-issue-statistics}}
{
    \begin{tabular}{c||ccc}
    \toprule
            Split & Train & Validation & Test \\
    \midrule
    \#Positive &  &  &  \\
    \#Negative &  &  &  \\
    \midrule
    Total &  &  &  \\
    \bottomrule
    \end{tabular}
}
\end{floatrow}
\end{figure}

\subsection{Implementation Details}
In this paper, we use the pre-trained uncased BERT base model (110M parameters) as the backbone model. For a fair comparison, all BERT-based baselines also adopt this setting.
We set the value of  for the cosine objective and the in-batch negative objective to , based on prior research. Additionally, we determined the value of  for the angle objective to be  through grid search. 

\subsection{Main Results}
\label{sec-main-result}
In this section, we will first introduce the baselines, then the results of the transfer STS tasks, then the results of the non-transfer STS tasks, and finally a summary.

\paragraph{Baselines} We compare our proposed model with widely used baselines, encompassing both unsupervised and supervised models. The unsupervised models are average GloVe \citep{pennington-etal-2014-glove}, 
BERT-flow \citep{li-etal-2020-sentence}, BERT-whitening \citep{su2021whitening}, LLaMA2 \citep{touvron2023llama2}, and contrastive learning models including IS-BERT \citep{zhang-etal-2020-unsupervised}, CT-BERT \citep{carlsson2020semantic}, SimCSE \citep{simcse_gao_2021}, ConSERT \citep{consert_yan_2021}, and DiffCSE \citep{chuang-etal-2022-diffcse}. On the other hand, the chosen supervised models are InferSent \citep{conneau-etal-2017-supervised}, USE \citep{cer-etal-2018-universal}, SBERT \citep{sbert-nils-2019}, CoSENT \citep{cosent_su_2022}, as well as supervised versions of SimCSE and ConSERT.

\paragraph{Transfer STS Tasks} For a fair comparison, we train AnglE with the NLI datasets MNLI \citep{williams-etal-2018-broad} and SNLI \citep{bowman-etal-2015-large} and then transfer it to evaluate seven STS benchmark datasets. The evaluation results are presented in Table \ref{table-main-sts-results}. It is evident that AnglE-BERT and AnglE-LLaMA consistently outperform the baselines with a gain of  and  in average score, respectively, over the previous SOTA SimCSE-BERT and SimCSE-LLaMA. Note that supervised SBERT and CoSENT show lower results than other unsupervised contrastive learning models like SimCSE and DiffCSE. This difference might arise from the difference in data distributions between the training and test data in the transfer STS tasks. They struggle to effectively generalize to STS tasks when trained solely with NLI datasets.
In contrast, contrastive learning models exhibit better generalization capabilities due to their alignment and uniformity features.
Because AnglE optimizes both the supervised cosine objective and the in-batch negative objective. This can allow AnglE to generalize well in transfer STS tasks. Additionally, the angle optimization in AnglE mitigates the negative impact of the saturation zone in the cosine function to produce better performance than other baselines. 


\begin{table*}[ht]
\small
\centering
\begin{threeparttable}
\begin{tabular}{lcccccccc}
\toprule
Model & STS12 & STS13 & STS14 & STS15 & STS16 & STS-B & SICR-R & Avg. \\

\midrule
\midrule
\multicolumn{9}{c}{\textit{Unsupervised Models}} \\

\midrule
GloVe (avg.)  &  &  &   &   &   &    &  &  \\ 
BERT-flow  &  &  &   &   &   &    &  &  \\
BERT-whitening  &  &  &   &   &   &    &  &  \\
IS-BERT  &  &  &  &  &  &  &  &  \\
CT-BERT  &  &  &  &  &  &  &  &  \\
ConSERT-BERT &  &  &  &  &  &  &  &  \\
DiffCSE-BERT &  &  &  &  &  &  &  &  \\
SimCSE-BERT  &  &  &  &  &  &  &  &  \\
LLaMA2-7B  &  &  &  &  &  &  &  &  \\

\midrule
\midrule
\multicolumn{9}{c}{\textit{Supervised Models}} \\
\midrule

InferSent-GloVe   &  &  &  &  &  &  &  &  \\
USE  &  &  &  &  &  &  &  &  \\
ConSERT-BERT &  &  &  &  &  &  &  &  \\
CoSENT-BERT  &  &  &  &  &  &  &  &  \\
SBERT  &  &  &  &  &  &  &  &  \\
SimCSE-BERT &  &  &  &  &  &  &  &  \\





SimCSE-LLaMA2-7B  &  &  &  &  &  &  &  &  \\ 

\midrule
AnglE-BERT &  &  &  &  &  &  &  &  \\



AnglE-LLaMA2-7B &  &  &  &  &  &  &  &  \\ 







\bottomrule
\end{tabular}
\end{threeparttable}
\caption{Text embedding performance on STS tasks. We report the Spearman's correlation  of the ``all'' setting computed by SentEval. For supervised LLaMA-based models, we fine-tuned them using the LoRA \citep{hu2021lora} technique and used the prompt ``\textit{Summarize sentence \{sentence\} in one word:}'' sparked by \citep{jiang2023scaling}. Results marked with  are obtained from \citep{sbert-nils-2019}, while results marked with  are retrieved from \citep{simcse_gao_2021}. Additionally, results marked with  denote our own implementation using official code. For the remaining baselines, we refer to the corresponding original papers to obtain their results. 
}
\label{table-main-sts-results}
\end{table*}

\paragraph{Non-transfer STS Tasks} To provide a comprehensive analysis, we also evaluate the performance of the baselines in the non-transfer setting. We train the baselines on the train set and evaluate them on the test or validation set. Two typical models, SimCSE and SBERT, representing contrastive and supervised learning, are compared with our model. The results of the non-transfer STS tasks are listed in Table \ref{table-supervised-sts-results}, where we evaluate the baselines on four short-text datasets (MRPC, STS-B, QQP, and QNLI) and one long-text dataset (GitHub Issues Similarity Dataset). SimCSE notably performs poorly compared to SBERT and AnglE in the non-transfer setting. This is due to the limitation of the small-scale training set, as there are not enough samples for SimCSE to effectively learn representations. Furthermore, the datasets only provide pair-supervised data, namely  or , which prevents SimCSE from utilizing its hard negative objective that relies on triple-supervised data . This limitation might affect its performance. On the other hand, AnglE consistently outperforms SBERT, achieving an absolute gain of . This can support the idea that angle-optimized text embedding can mitigate the negative impact of the cosine function, resulting in better performance. Furthermore, we explore applying the long text model RAN (86M parameters) \citep{li-etal-2023-recurrent} as the backbone to test the performance on long text. The results show that AnglE-BERT outperforms AnglE-RAN across all short text datasets. This advantage might be attributed to the larger parameter size of BERT and its proficiency in handling short texts. However, we observe a remarkable shift in long-text STS. AnglE-RAN outperforms AnglE-BERT in this scenario, suggesting that AnglE-RAN can handle long texts well despite having fewer parameters.

\begin{table*}[ht]
\small
\centering
\begin{threeparttable}
\begin{tabular}{lcccccc}
\toprule
        \multirow{2}{*}{Model}   & \multicolumn{1}{c}{MRPC} & \multicolumn{1}{c}{STS-B} & 
        \multicolumn{1}{c}{QQP} & \multicolumn{1}{c}{QNLI} & \multicolumn{1}{c}{GitHub Issues.} & \multirow{2}{*}{Avg.}\\

        \cmidrule{2-6}
        &  
          \multicolumn{1}{c}{test} & \multicolumn{1}{c}{test} & 
        \multicolumn{1}{c}{validation} & \multicolumn{1}{c}{validation} &
        \multicolumn{1}{c}{test} \\
\midrule

SimCSE-BERT &  &  &  &  &  &  \\
SBERT &  &  &  &  &  &  \\

\midrule
AnglE-RAN &  &  &  &  &  &  \\
AnglE-BERT &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\end{threeparttable}
\caption{Results on the STS tasks. All baseline results are our implementation using the official code. Spearman's correlation () serves as the reported metric.} \label{table-supervised-sts-results}
\end{table*}

In short, this evidence suggests AnglE's superiority in transfer and non-transfer settings, its ability to produce high-quality text embeddings, and its robustness and adaptability to different backbones.

\subsection{Ablation Study}
To gain a deeper understanding of AnglE, we conducted an ablation study examining different objectives and their effects. The results in table \ref{table-ablation-study-results} indicate that AnglE shows improved performance with all three objectives. In particular, we observe that AnglE experiences a greater drop in performance without the angle objective than without the in-batch negative (ibn) objective. This suggests that angle optimization is more important than ibn in improving text embedding. Additionally, we find that using the angle objective alone yields performance close to that of using the cosine objective alone, demonstrating the effectiveness of angle optimization. We also evaluated five different pooling strategies and found that the ``cls'' strategy performed the best. Finally, we compared the ibn with/without identical sentence pair (ISP) detection and found that ibn without ISP detection has about  performance drop than with. This indicates that ibn with ISP detection is effective.

\begin{minipage}[l]{0.5\textwidth}
    \centering
    \small
    \begin{tabular}{lc}
        \toprule
        Model & Spearman's Correlation \\
        \midrule
        \midrule
        \multicolumn{2}{c}{\textit{Objective}} \\
        \midrule
        AnglE-BERT-all &  \\
        - w/o ibn &  \\
        - w/o angle &  \\
        only cosine &  \\
        only ibn &  \\
        only angle &  \\
        \midrule
        \midrule
        \multicolumn{2}{c}{\textit{Pooling Strategy}} \\
        \midrule
        cls &  \\
        cls-last-avg &  \\
        last-avg &  \\
        last-max &  \\
        first-last-avg &  \\
        \bottomrule
    \end{tabular}
    \captionof{table}{Ablation study of AnglE. The results are Spearman's correlations on the STS-B test set.}
    \label{table-ablation-study-results}
\end{minipage}
\begin{minipage}[l]{0.5\textwidth}
    \centering
    \small
    \captionof{table}{Results of unsupervised and LLM supervised models on the STS-B test set. For ChatGPT,  LLaMA, and ChatGLM, we use the gpt-turbo-3.5, 7B LLaMA2, and 6B ChatGLM, respectively.}
    \begin{tabular}{lc}
        \toprule
        Model & Spearman's \\
        \midrule
        \midrule
        \multicolumn{2}{c}{\textit{Unsupervised Models}} \\
        \midrule
        SimCSE-BERT &  \\
        ConSERT-BERT &  \\
        DiffCSE-BERT &  \\
        \midrule
        \midrule
        \multicolumn{2}{c}{\textit{LLM-supervised Models}} \\
        \midrule
        AnglE-BERT + ChatGPT &  \\
        AnglE-BERT + LLaMA &  \\
        AnglE-BERT + ChatGLM &  \\
        AnglE-BERT + Ensemble &  \\
        \bottomrule
    \end{tabular}
    \label{table-supervised-learning-results}
\end{minipage}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{llm-supervised.pdf}
    \caption{The procedures of the LLM-supervised learning. For the STS task, we use the prompt ``\textit{You are a highly smart same-meaning/opposite-meaning sentence-generating system. Your job is to generate \{size\} synonymous/antonym sentences of a given input sentence. Input sentence: \{text\}. Output:}'' to generate positive/negative pairs. \{size\} and \{text\} are placeholders for the generated size and the input text, respectively.}
    \label{llm-supervised-learning-figure}
\end{figure*}

\subsection{Discussion and Analysis}

\paragraph{Discussion of LLM-supervised Learning}
AnglE, a supervised learning model, must be trained on labeled data. However, the limited availability of domain-supervised data poses a challenge in real-world applications. To overcome this problem, we propose LLM-supervised learning. This approach applies LLMs as data annotators to label the pseudo-supervised data for AnglE training. Figure \ref{llm-supervised-learning-figure} outlines the procedures involved in LLM-supervised learning. In this study, we compare the LLM-supervised AnglE and unsupervised contrastive learning models. To simulate domain application, we extract all ``sentence1'' texts from the STS-B train set and employ LLM-supervised learning to train the AnglE model. Table \ref{table-supervised-learning-results} shows the results on the STS-B test set. It is evident from the results that LLM-supervised AnglE performs better than unsupervised contrastive baselines, and the ensemble of LLMs shows the best results.
This evidence suggests the effectiveness of LLM-supervised learning and indicates that it can alleviate the domain-supervised data scarcity problem.

\paragraph{Discussion of Text Retrieval}
\label{sec-long-text-retrieval}
We also evaluate the performance of the text retrieval task by experimenting on the test split of the flickr30k dataset \citep{young-etal-2014-image}. This dataset consists of five caption texts for each photo, and these texts are similar to each other. We use the first caption text vector to retrieve the top 5 similar sentences using faiss \footnote{https://github.com/facebookresearch/faiss}. The strict accuracy \footnote{Strict accuracy means only all the top five sentences retrieved are equal to the five reference sentences considered correct} of AnglE, SimCSE (supervised), and SBERT are , , and , respectively. This evidence indicates the effectiveness of using AnglE for the retrieval task. 

\paragraph{Discussion of Transfer Tasks}
\label{sec-transfer tasks}
In addition, we evaluate the performance of text embedding in transfer tasks. In particular, our approach involves training text embedding on STS tasks and then transferring it to seven other kinds of tasks. Notably, AnglE outperforms baselines, showing a significant improvement of  and  over DiffCSE and SimCSE, respectively. These results suggest that AnglE can produce better embeddings that effectively improve performance in various tasks. A more detailed description of the experiment can be found in section \ref{appendix_transfer_task_experiment}.


\paragraph{Analysis of Text Embedding Distribution}
Figure \ref{cos-sentence-distribution-figure} depicts the density plots of the cosine similarities between sentence pairs in the STS-B test set to provide an intuitive visualization of the text embedding quality. Figure \ref{cos-gold-distribution-figure} displays the golden scores for the same sentence pairs. Analyzing the overall density of cosine similarities, we find that AnglE's distribution resembles the golden distribution more closely than SimCSE (supervised) and SBERT's. Figure \ref{cos-gold-distribution-figure} illustrates a peak in the 0-1 range; however, only AnglE shows a distinct peak in this range in Figure \ref{cos-sentence-distribution-figure}. 
Also, Figure \ref{cos-gold-distribution-figure} portrays a higher peak around  than around  in the 4-5 range, only AnglE demonstrates this feature properly in Figure \ref{cos-sentence-distribution-figure}. 
Notably, the 0-1 and 4-5 ranges in Figure \ref{cos-gold-distribution-figure} represent two saturation zones of the cosine function. This evidence suggests that AnglE can mitigate the negative effect of the saturation zone. In conclusion, we can confidently assert that AnglE produces better text embeddings with a cosine similarity density closely resembling the actual distribution than the baselines.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth,height=4cm]{cos_distribution.pdf}
         \caption{}
         \label{cos-sentence-distribution-figure}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.53\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth,height=4cm]{cos_gold.png}
         \caption{}
         \label{cos-gold-distribution-figure}
     \end{subfigure}
     \caption{(a) Density plots of cosine similarities between sentence pairs in the STS-B test set. The pairs have been categorized into 6 groups, reflecting the ground truth ratings (where higher ratings indicate a higher degree of similarity), visually represented on the y-axis. The x-axis represents the cosine similarity. (b) Density plots of golden scores between sentence pairs in the STS-B test set.}
\end{figure}


\section{Conclusion and Future Work}
In this paper, we have presented a novel text embedding model called AnglE, which optimizes the angle difference in complex space to overcome the adverse impact of the saturation zone of the cosine function, thereby improving text embeddings. 
To comprehensively evaluate the STS tasks, we have introduced the GitHub Issues Similarity Dataset to evaluate model performance on the long-text STS task. Furthermore, we have proposed an LLM-supervised learning method to cope with the scarcity of domain-supervised data. Extensive experimental results have demonstrated that AnglE outperforms baselines, indicating that AnglE can handle both short and long-text STS tasks and work effectively in various scenarios. In future work, we plan to explore the application of AnglE in real-world scenarios and provide further insights into AnglE.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Appendix}

\subsection{List of Open-source Projects in GitHub Issue Similarity Dataset}
\label{appendix_github_repos}

We collected GitHub issues from the following  GitHub repositories.

\begin{table}[ht]
\small
\begin{tabular}{|l|l|l|}
tensorflow/tensorflow & pytorch/pytorch & huggingface/transformers \\
keras-team/keras & freeCodeCamp/freeCodeCamp & vuejs/vue \\
facebook/react & angular/angular & elastic/elasticsearch \\
numpy/numpy & scikit-learn/scikit-learn & pandas-dev/pandas \\
psf/requests & scipy/scipy & matplotlib/matplotlib \\
scrapy/scrapy & opencv/opencv & bumptech/glide \\
spring-projects/spring-framework & apache/dubbo & apache/superset \\
apache/airflow & apache/druid & apache/shardingsphere \\
kubernetes/kubernetes & flutter/flutter & google/jax \\
twbs/bootstrap & axios/axios & microsoft/vscode \\
sqlalchemy/sqlalchemy & mwaskom/seaborn & microsoft/TypeScript \\
microsoft/PowerToys & microsoft/terminal & microsoft/playwright \\
symfony/symfony & babel/babel & electron/electron \\
denoland/deno & mrdoob/three.js & mui/material-ui \\
webpack/webpack & atom/atom & vercel/next.js \\
ansible/ansible & npm/cli & pallets/flask \\
tiangolo/fastapi & DefinitelyTyped/DefinitelyTyped & celery/celery \\
neo4j/neo4j & rust-lang/rust & JuliaLang/julia \\
golang/go \\
\end{tabular}
\end{table}


\subsection{Transfer Task Experiment}
\label{appendix_transfer_task_experiment}

Table \ref{table-transfer-tasks} presents the results of the transfer tasks. A comprehensive analysis reveals that AnglE outperforms baselines, achieving the best results in terms of average score, with 6 out of 7 best results. Notably, AnglE exhibits improvements of  and  over DiffCSE and SimCSE, respectively. This compelling evidence asserts that AnglE can produce better embeddings that effectively assist in various tasks.

\begin{table*}[ht]
\small
\centering
\begin{threeparttable}
\begin{tabular}{lcccccccc}
\toprule
Model & MR & CR & SUBJ & MPQA & SST2 & TREC & MRPC & Avg. \\

\midrule
\midrule

GloVe (avg.)  &  &  &   &   &  &   &  &   \\ 
Skip-thought (avg.)  &  &  &  &  &   &  &  &  \\
\midrule

Avg. BERT  &  &  &  &  &   &  &  &  \\

BERT-CLS  &  &  &  &  &  &  &  &  \\

IS-BERT  &  &  &  &  &   &  &  &   \\
\midrule

SimCSE-RoBERTa  &  &  &  &  &  &  &  &   \\
DiffCSE-RoBERTa  &  &  &  &  &  &  &  &  \\

\midrule
AnglE-LLaMA &  &  &  &  &  &  &  &  \\
\bottomrule

\end{tabular}
\end{threeparttable}
\caption{Transfer task results of different sentence embedding models (measured as accuracy). : results from \citet{sbert-nils-2019}; : results from \citet{zhang-etal-2020-unsupervised}; : results from \citet{simcse_gao_2021}. : results from \citet{chuang-etal-2022-diffcse}.}
\label{table-transfer-tasks}
\end{table*}

\end{document}
