We first study two synthetic tasks that allow us to analyze our proposed gating, attention mechanism and its generalization ability, and then in Section \ref{sec:large_sacelQA} we study the more complex tasks of natural language question and answering.

The synthetic tasks are the \emph{picking task} and the \emph{Pixel-by-Pixel MNIST QA task}. For the \emph{picking task}, we analyze the gating mechanism and show how the model utilizes the question (context) to dynamically group the passage tokens and how the attention mechanism utilizes this information. We also test the generalization ability our model following the setup in \cite{graves2014neural}. For the \emph{Pixel-by-Pixel MNIST QA task}, we show better accuracy with our FHE module over the baseline. The  tasks are chosen due to the natural of the tasks. The gating mechanism for the \emph{picking task} depends solely on the question, whereas the gating mechanism for the \emph{Pixel-by-Pixel MNIST QA task} is independent of the question, but solely dependent on the data.

We compare the performance of our \emph{focused hierarchical encoder} module to two baseline architectures: a 1-layer LSTM (LSTM1) and a 2-layer LSTM (LSTM2)\footnote{Note that baseline models are equivalent to FHE with the boundary gate fully open for LSTM2 ($b_t = 1$ for each $t$) or always closed for LSTM1 ($b_t = 0$ for each $t$).}. 

For the \emph{picking task}, FHE utilizes less memory compare to LSTM2, as the baseline LSTM2 model needs to store and attend over all states, whereas FHE only needs to attend to unique elements of $H^u$. For example, when gate openness is below $10\%$, the attention module for FHE only attends to than $10\%$ of memory compared to a LSTM2 baseline model.

\begin{table}[ht!]
\vskip -0.1in
\caption{Sample points for \emph{picking task} (sequence length \mbox{$n=30$}). The first $k$ digits are underlined and the target mode is bolded.}
\label{table:pickingSample}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c | c} 
\toprule
\multicolumn{2}{c|}{\textbf{Input}} & \textbf{Target}\\
\textbf{Sequence} & \textbf{k} & \textbf{Mode}\\
\midrule
\textit{random examples} &&\\
\underline{8\textbf{0}56\textbf{0}2\textbf{0}17\textbf{0}}82838371701316304473 & 10 & \textbf{0}\\
\underline{6\textbf{3}87\textbf{33}290890\textbf{3}966902559\textbf{3}}7986485 & 23 & \textbf{3}\\
\underline{\textbf{1}6455\textbf{1}9375793738968\textbf{1}398\textbf{11}2}5982 & 26 & \textbf{1}\\
\midrule
\textit{malicious examples} &&\\
\underline{\textbf{666}333}666288882888819999999990 & 6 & \textbf{6}\\
\underline{\textbf{666}333\textbf{666}2}88882888819999999990 & 10 & \textbf{6}\\
\underline{6663336662\textbf{8888}2\textbf{8888}1}9999999990 & 20 & \textbf{8}\\
\underline{66633366628888288881\textbf{999999999}}0 & 30 & \textbf{9}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.25in
\end{table}

\paragraph{Hyper-parameters} All models (FHE, LSTM1 and LSTM2) for a certain task has the same number of hidden units (256 for \emph{picking task} and 128 for \emph{Pixel-by-Pixel MNIST QA task}).
In FHE module we used $\alpha=0$ hence, we did not use exploration term mentioned in Section \ref{subsec:training}. Instead, we used simpler idea that is sufficient in the  synthetic experiments conducted -- we add a small value to $b_t$ (0.01 for \emph{picking task} and 0.1 for \emph{Pixel-by-Pixel MNIST QA task}) to encourage exploration. The values of $\beta$ and $\gamma$ depend on the task and are provided later. Learning rates used for all models are 0.0001 with the Adam optimizer \cite{kingmaandba2014}.

\subsection{Picking task}

Given a sequence of randomly generated digits of length $n$, the goal of the \emph{picking task} is to determine the most frequent digit within the first $k$ digits\footnote{If there is more than one  mode, the largest value digit should be picked.}, where $k \leq n$. Hence, the value of $k$ is understood as the question. We study three tasks with input sequences of $n \in \{100, 200, 400\}$ digits respectively. Sample points for the task are presented in Table~\ref{table:pickingSample}.

The input digits $\mathbf{x}_i$ are one-hot encoded vectors (size 10) and the question embedding $\mathbf{q}$ is a vector retrieved from the lookup table (that is learnt during training) with $n$ entries. To obtain the final sequence representation, soft attention (as in \citet{bahdanau2014}) is applied on the upper-level states $H^u$ (for FHE and LSTM2) or the lower-level states $H^l$ (for LSTM1). Finally, the representation is concatenated with the question embedding and fed to one layer feed-forward neural network to produce the final prediction (i.e.\, probabilities for all 10 classes).

As introduced in Section~\ref{subsec:training}, there are two hyper-parameters ($\beta$ and $\gamma$) that affect the sparsity of higher-level representations in FHE. We explore two approaches for determining their values.

One approach is to fix these hyper-parameters to a small value (for example  $\beta = 0.1$ and $\gamma = 0.25$) in the beginning of training, such that the gates can almost freely open. Once the desired accuracy has been reached, we enforce constraints on our hyper-parameters. This provides a level of control over the accuracy-sparsity trade-off -- we used this approach  with the requirement of achieving a desired accuracy $a$. We tested FHE models with \mbox{$a \in \{80\%, 90\%, 95\%, 98\%\}$} and call them FHE80, FHE90, etc. The relationship between accuracy and gate openness is visualized in Figure \ref{accVSopen100}. 

Another approach is to set $\beta$ and $\gamma$ to a fixed value from the start, so the gate openness of the model is more restricted right from the start. We find that the model performs better with fixed the hyper-parameters (the results for $\beta = 1$ and $\gamma = 10\%$ are presented in Table \ref{table:picking} as FHE-fixed).

The results achieved for each model and sequence length are presented in Table \ref{table:picking} and Table \ref{table:pickingCHAa}. For each setup at least two runs were performed and the difference in result between the pair were typically neglectable ($<0.5\%$).

\begin{table}[t]
\vskip -0.1in
\caption{Accuracy ($\%$) for \emph{picking task} for LSTM1, LSTM2 and FHE-fixed. Our model and LSTM2 are on par with performing while LSTM1 is behind for longer input sequences.}
\label{table:picking}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c || c c | c} 
\toprule
\textbf{Length} & \textbf{LSTM1} & \textbf{LSTM2} & \textbf{FHE-fixed}\\
\midrule
100 & 99.4 & \textbf{99.7} & 99.5\\
200 & 97.0 & 99.2 & \textbf{99.4}\\
400 & 92.9 & \textbf{97.5} & 96.9\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}

\caption{Accuracy ($\%$) for \emph{picking task} for the models providing a level of control over the accuracy-sparsity trade-off at a cost of slightly lower performance.}
\label{table:pickingCHAa}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c || c c c c}
\toprule
\textbf{Length} & \textbf{FHE80} & \textbf{FHE90} & \textbf{FHE95} & \textbf{FHE98}\\
\midrule
100 & 93.4 & 94.2 & 96.6 & \textbf{98.7}\\
200 & 92.3 & 92.4 & \textbf{93.6} & \textbf{93.6}\\
400 & 87.2 & 90.5 & 90.0 & \textbf{91.0}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}

\caption{Test accuracy ($\%$) for longer sequence length for \emph{picking task} on model trained on sequence length \mbox{$n = 200$}.}
\label{table:pickingGeneralization}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c || c c | c} 
\toprule
\textbf{Length} & \textbf{LSTM1} & \textbf{LSTM2} & \textbf{FHE-fixed}\\
\midrule
200 & 97.1 & 99.2 & \textbf{99.4}\\
400 & 55.9 & 61.4 & \textbf{97.6}\\
800 & 39.6 & 39.7 & \textbf{95.6}\\
1600 &  29.5 & 28.6  & \textbf{93.3}  \\
10000 &  18.5 & 14.8 & \textbf{66.8} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.25in
\end{table}

\begin{figure}[hpt!]
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{100.png}
\vspace{-0.00cm}
\caption{A relationship between accuracy and gate openness for \emph{picking task} and sequence length $n =100$. The best performance is achieved for gate openness around $10\%$.}
\label{accVSopen100}
\end{minipage}

\bigskip

\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.81\linewidth]{117198_3.png} 
\includegraphics[width=0.81\linewidth]{117198_50.png}
\includegraphics[width=0.81\linewidth]{117198_100.png} 
\includegraphics[width=0.81\linewidth]{117198_198.png} 


\includegraphics[width=0.81\linewidth]{117202_3.png} 
\includegraphics[width=0.81\linewidth]{117202_50.png}
\includegraphics[width=0.81\linewidth]{117202_100.png} 
\includegraphics[width=0.81\linewidth]{117202_198.png} 
\vspace{-0.00cm}
\caption{Gate openness (G) conditioned on the position asked (P). Focus (F) is the average of final attention weight set for a given step. Hence, focus sums to one and it is always lower than gate openness (because our model attends only over unique states). Result showed for sequence length $n = 200$. The first four plots illustrate FHE model having $99.4\%$ accuracy and $10\%$ gate openness, while the last four are for FHE model having $97\%$ accuracy but $5\%$ gate openness.}
\label{fig:openness}
\end{minipage}

\bigskip

\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.8\linewidth]{mnist-half.png}
\caption{A visualization of the gating mechanism learned using the Pixel-by-Pixel MNIST dataset. Red pixels indicate a gate opening and are overlayed on top of the digit which is white on a gray background. The digits are vectorized row-wise which explains why white pixels appear left of the red pixels.}
\label{fig:qca}
\vspace{-0.00cm}
\end{minipage}
\end{figure}

The \emph{picking task} is useful to validate our gating mechanism. Once trained we can inspect the positions of the opened gates. Figure~\ref{fig:openness} shows that our model learns to open gates around $k$'th step only and attend a single gate right after the $k$'th step. The lower-level LSTM is used to count the occurrences of the various digits. That information is then passed to the upper-level LSTM at a single gate. The attention mechanism then uses the information around the same time step to provide the mode (i.e.\, solve the task).

We tested the generalization ability of FHE. The models trained on short sequences (\mbox{$n=200$}) were evaluated on longer sequences and \mbox{$k\leq200$}. The results are in Table~\ref{table:pickingGeneralization}. The models can not be evaluated for $k$ larger than maximum sequence length used during training because the question embeddings are parts of the models. FHE generalizes better to longer sequences by a wide margin. We believe this is due to the boundary gates being open for only the first k-steps, and the attention mechanism not attending over possibly misleading states.

\subsection{Pixel-by-Pixel MNIST QA task}

\begin{table}[t!]
\vskip -0.1in
\caption{Accuracy (\%) for validation set of \emph{Pixel-by-Pixel MNIST QA task}. Our model slightly outperform both LSTM1 and LSTM2.}
\label{table:mnist}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c c | c}
 \toprule
 \textbf{LSTM1} &  \textbf{LSTM2} & \textbf{FHE-fixed}\\
 \midrule
 97.3 & 98.4 &  \textbf{99.1}\\
 \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.25in
\end{table}

We adapt the Pixel-by-Pixel MNIST classification task \cite{lecun1998gradient,DBLP:journals/corr/LeJH15} to the question and answering setting. The passage encoder reads in MNIST digits one pixel at a time. The question asked is whether the image is a specific digit and the answer is either \emph{True} or \emph{False}. The data is balanced such that approximately half of the answers are \emph{True} and the other half are \emph{False}.

The LSTM2 reached an accuracy of $98.4\%$ on the validation set, and FHE\footnote{We used $\beta=0.0001$ and $\gamma=50\%$.} outperformed the baseline by having an accuracy of $99.1\%$. Figure \ref{fig:qca} shows a visualization of the gates for the passage encoder learned by the model. The model learns to open the boundary gate almost always around the digit. We also found that for this particular task, the gates do not depend on the question. We hypothesize that this is because it is much easier for the passage encoder to learn to open the gates when there is a white pixel. In any case, these experiments illustrate how our proposed mechanism  modulates gates based on input questions and features in the data.