\documentclass[letterpaper,12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}


\def\epsilon{\varepsilon}
\def\bigoh{\mathcal{O}}
\def\phi{\varphi}
\def\th{{\rm th}}
\def\ith{{\it th}}
\def\rd{{\rm rd}}
\def\ird{{\it rd}}
\def\nd{{\rm nd}}
\def\ind{{\it nd}}
\def\st{{\rm st}}
\def\ist{{\it st}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{observe}[theorem]{Observation}
\newtheorem{remark1}[theorem]{Remark}

\newenvironment{observation}{\begin{observe} \rm}{\end{observe}}
\newenvironment{remark}{\begin{remark1} \rm}{\end{remark1}}




\title{A fast algorithm for computing minimal-norm solutions
       to underdetermined systems of linear equations}
\author{Mark Tygert}
\date{September 8, 2009}


\begin{document}


\maketitle

\begin{abstract}
We introduce a randomized algorithm
for computing the minimal-norm solution
to an underdetermined system of linear equations.
Given an arbitrary full-rank matrix  with ,
any vector , and any positive real number 
less than 1, the procedure computes a vector 
approximating to relative precision  or better
the vector  of minimal Euclidean norm satisfying
.
The algorithm typically requires

floating-point operations,
generally less than the  required
by the classical schemes based on -decompositions or bidiagonalization.
We present several numerical examples illustrating the performance
of the algorithm.
\end{abstract}



\section{Introduction}

Underdetermined systems of linear equations have arisen frequently
in modern statistics and data analysis, and have been attracting
much attention recently in various application domains;
see, for example,~\cite{bruckstein-donoho-elad}, \cite{candes-tao},
and~\cite{donoho}.
The solutions to underdetermined systems are not unique;
the present article focuses on the solutions whose Euclidean norms
are minimal.

Given a full-rank matrix  and a vector ,
with , we would like to compute an accurate approximation
to the vector  of minimal Euclidean norm satisfying

Classical algorithms using -decompositions or bidiagonalization require

floating-point operations in order to compute 
(see, for example, Chapter~5 in~\cite{golub-van_loan}).

The present paper introduces a randomized algorithm that,
given any positive real number  less than 1,
computes a vector  approximating 
to relative precision  or better
with respect to the Euclidean norm, that is,
the algorithm produces a vector  such that

where  denotes the Euclidean norm.
This algorithm typically requires

floating-point operations.
When  is sufficiently large and  is much greater than 
(that is, the system of linear equations is highly underdetermined),
then the cost in~(\ref{randomized}) is less than
the cost in~(\ref{classical}).
Moreover, in the numerical experiments of Section~\ref{numerical},
the algorithm of the present article runs substantially faster than
the standard methods based on -decompositions.

The present paper describes an algorithm optimized for the case
when the entries of  and  are complex valued.
Needless to say, real-valued versions of our scheme are similar.
The present article has the following structure:
Section~\ref{notation} sets the notation.
Section~\ref{prelims} discusses a randomized linear transformation
which can be applied rapidly to arbitrary vectors.
Section~\ref{algorithm} describes the algorithm of the present paper.
Section~\ref{proofs} proves that the procedure succeeds with high probability.
Section~\ref{costs} estimates the computational costs of the algorithm.
Section~\ref{numerical} illustrates the performance of the scheme
via several numerical examples.
Section~\ref{conclusions} contains several concluding comments.



\section{Notation}
\label{notation}


In this section, we set notational conventions employed
throughout the present paper.

We abbreviate ``independent and identically distributed'' to ``i.i.d.''
We consider the entries of all vectors and matrices in this paper
to be complex valued.
For any vector , we define  to be the Euclidean () norm of .
For any matrix , we define  to be the adjoint of .
We define the condition number of  to be the  condition number of ,
that is, the greatest singular value of  divided
by the least singular value of .

For any positive integer ,
we define the discrete Fourier transform  to be the matrix
with the entries

for ,~, \dots, ,~, where  and .



\section{Preliminaries}
\label{prelims}

In this section, we discuss a subsampled randomized Fourier transform.
\cite{ailon-chazelle}, \cite{drineas-mahoney-muthukrishnan-sarlos},
\cite{sarlos3}, and~\cite{sarlos4} introduced a similar transform
for similar purposes (these articles motivated us to write the present paper).

For any positive integers  and  with ,
we define the  SRFT to be the random matrix

where  and  are defined as follows.

In~(\ref{SRFT}),  is the random matrix given by the formula

where  is the matrix whose entries are all zeros,
aside from a single 1 in column  of row 
for ,~, \dots, ,~,
where ,~, \dots, ,~ are
i.i.d. integer random variables,
each distributed uniformly over ;
moreover,  is the discrete Fourier transform
defined in~(\ref{DFT}),
and  is the diagonal matrix whose diagonal entries
,~, \dots, ,~ are i.i.d. complex random variables,
each distributed uniformly over the unit circle.
(In our numerical implementations,
we drew ,~, \dots, ,~
from  without replacement,
instead of using i.i.d. draws.)
We observe that both  and  are unitary.

In~(\ref{SRFT}),  is the random matrix given by the formula

where  and 
are permutation matrices chosen independently and uniformly at random, and
 and  are diagonal matrices
whose diagonal entries ,~, \dots, ,~
and ,~, \dots,
,~ are i.i.d. complex random variables,
each distributed uniformly over the unit circle; furthermore,
 and  are the matrices
defined via the formulae

and (the same as~(\ref{big_def}), but with tildes)

where ,~, \dots, ,~,
,~, \dots,
,~
are i.i.d. real random variables drawn uniformly from .
We observe that , ,
, ,
, and~ are all unitary,
and so  is also unitary.

We call the transform  an ``SRFT'' for lack of a better term.

The following technical lemma is a slight reformulation
of Lemma~4.4 of~\cite{woolfe-liberty-rokhlin-tygert}.

\begin{lemma}
\label{reformulation}
Suppose that  and  are real numbers greater than 1,
and , , and  are positive integers, such that

Suppose further that  is the SRFT defined in~(\ref{SRFT}),
and that  is a matrix whose columns are orthonormal.

Then, the least singular value  of 
satisfies

with probability at least .
\end{lemma}



\section{Description of the algorithm}
\label{algorithm}


Suppose that  is a positive real number less than 1,
and , , and  are positive integers with .
Suppose further that  is a full-rank matrix,
 is a vector, and  is the vector
of minimal Euclidean norm satisfying
.
In order to construct a vector  such that

with high probability (increasingly high probability as a parameter 
increases), we compute the vector  of minimal Euclidean norm
that is a linear combination of random vectors
and satisfies ,
then use the algorithm of~\cite{rokhlin-tygert}
to compute the orthogonal projection of 
onto the column span of .
More precisely, we perform the following five steps:


\begin{enumerate}
\item Construct the matrix

applying the SRFT  defined in~(\ref{SRFT})
      to every column of 
      (see, for example, Subsection~3.3 of~\cite{woolfe-liberty-rokhlin-tygert}
      for details on applying the SRFT rapidly).
\item Construct the vector  of minimal Euclidean norm
      solving the system of linear equations

where  is the matrix defined in~(\ref{randmat})
      (see, for example, Algorithm~5.7.2 in~\cite{golub-van_loan}
      for details on constructing ).
\item Construct the vector

where  is the vector of minimal Euclidean norm
      solving~(\ref{lineq}), and  is the same realization
      of the SRFT as in~(\ref{randmat})
      (see, for example, Subsection~3.3 of~\cite{woolfe-liberty-rokhlin-tygert}
      for details on applying the adjoint of the SRFT rapidly).
\item Use the algorithm of~\cite{rokhlin-tygert} for the construction
      of a vector  minimizing

to relative precision  or better,
      where  is the vector defined in~(\ref{nonminnorm}).
      The parameter  for the algorithm of~\cite{rokhlin-tygert}
      should be the same as for the present algorithm.
\item Construct the desired vector

where  is the vector from Step~4.
\end{enumerate}


\begin{remark}
In Step~2 above, we assume that  defined in~(\ref{randmat})
is a full-rank matrix. Lemma~\ref{reformulation} in Section~\ref{prelims}
above guarantees this with high probability when ,
by taking  in the lemma arbitrarily large;
numerical experiments indicate that  suffices.
\end{remark}


\begin{remark}
It is possible to improve the approximation 
via preconditioned conjugate gradient iterations similar to those
proposed in~\cite{rokhlin-tygert}.
However, the approximation produced by the above algorithm
is already highly accurate (see, for example, Section~\ref{numerical}
or Theorem~\ref{main_theorem} in Section~\ref{proofs} below),
and further iterative improvement may double the running time of the algorithm.
\end{remark}



\section{Proof of accuracy}
\label{proofs}

In this section, we prove Theorem~\ref{main_theorem},
guaranteeing that the algorithm of Section~\ref{algorithm}
produces high accuracy with high probability.


The following lemma states that the orthogonal projection
onto the column span of 
of the vector  defined in~(\ref{nonminnorm})
is the vector of minimal Euclidean norm that the algorithm aims to approximate.

\begin{lemma}
\label{baselemma}
Suppose that , , and  are positive integers with .
Suppose further that  is a full-rank matrix,
 is a vector,
 is the matrix defined in~(\ref{randmat})
and is a full-rank matrix,
and  is the vector defined in~(\ref{nonminnorm}).

Then, the orthogonal projection 
of  onto the column span of 
is the vector of minimal Euclidean norm satisfying

\end{lemma}

\begin{proof}
Combining~(\ref{randmat}), (\ref{lineq}), and~(\ref{nonminnorm}) yields that

Combining~(\ref{easy}) and the fact that 
is the orthogonal projection of  onto the column span
of  completes the proof.
\end{proof}


The following lemma states that, with high probability, the Euclidean norm
of the vector  defined in~(\ref{nonminnorm})
is not too much greater than the Euclidean norm
of its orthogonal projection  onto the column span
of .

\begin{lemma}
\label{problemma}
Suppose that  and  are real numbers greater than 1,
and , , and  are positive integers, such that~(\ref{weak}) holds.
Suppose further that  is a full-rank matrix,
 is a vector,
 is the vector defined in~(\ref{nonminnorm}),
and  is the orthogonal projection of 
onto the column span of .

Then,

with probability at least .
\end{lemma}

\begin{proof}
Using the fact that  is a full-rank matrix,
we construct a -decomposition

such that the columns of  are an orthonormal basis
for the column span of .
We first show that the SRFT  used
in~(\ref{randmat}) and~(\ref{nonminnorm}) provides

with probability at least ,
where  is the vector of minimal Euclidean norm
solving~(\ref{lineq}).
We then express the left- and right-hand sides of~(\ref{intermed})
in terms of  and ,
rather than , in order to obtain~(\ref{energy}).

It follows from the fact that  is the vector
of minimal Euclidean norm solving~(\ref{lineq})
that  belongs to the column span
of  from~(\ref{lineq}), that is,
there exists a vector  such that

Combining~(\ref{linearcombo}), (\ref{randmat}), and~(\ref{QR}) yields that

The Cauchy-Schwarz inequality yields that

It follows from~(\ref{inverse}) that

with probability at least .
Combining~(\ref{QR}), (\ref{randmat}), and~(\ref{linearcombo}) yields that

Combining~(\ref{uno}), (\ref{dos}), (\ref{tres}), and~(\ref{cuatro})
yields~(\ref{intermed}).

We now express the left- and right-hand sides of~(\ref{intermed})
in terms of  and ,
rather than .

First, we consider the left-hand side of~(\ref{intermed}).
Combining~(\ref{nonminnorm}) and the fact that the columns
of  are orthonormal yields that


Next, we consider the right-hand side of~(\ref{intermed}).
It follows from the fact that the columns of 
are an orthonormal basis for the column span of 
that the orthogonal projection 
of  onto the column span of  is

Combining~(\ref{projector}) and the fact that the columns
of  are orthonormal yields that

Combining~(\ref{normproj}) and~(\ref{nonminnorm}) yields that


Finally, combining~(\ref{intermed}), (\ref{intermed1}), and~(\ref{intermed2})
yields~(\ref{energy}).
\end{proof}


The following lemma states that the vector 
produced by the algorithm is an accurate approximation
to the orthogonal projection onto the column span of 
of the vector  defined in~(\ref{nonminnorm}),
provided that the projection  satisfies~(\ref{energy}).

\begin{lemma}
\label{mainlemma}
Suppose that  and  are positive real numbers
with ,
and , , and  are positive integers with .
Suppose further that  is a full-rank matrix,
 is a vector,
 is the matrix defined in~(\ref{randmat})
and is a full-rank matrix,
 is the vector defined in~(\ref{nonminnorm}),
 is the orthogonal projection of 
onto the column span of ,
 is a vector minimizing~(\ref{overdet})
to relative precision  or better,
and  is the vector defined in~(\ref{output}).
Suppose in addition that~(\ref{energy}) holds.

Then,

\end{lemma}

\begin{proof}
It follows from~(\ref{output}) that 
belongs to the column span of .
Combining this fact and the fact that 
is the orthogonal projection of 
onto the orthogonal complement of the column span of 
yields that  is the orthogonal projection
of 
onto the orthogonal complement of the column span of .
Similarly,  is the orthogonal projection
of 
onto the column span of .
We thus obtain the Pythagorean identity


It follows from the fact that  is the orthogonal projection
of  onto the column span of  that
the minimal value of~(\ref{overdet})
is .
Combining this fact, (\ref{output}), and the fact that 
minimizes~(\ref{overdet}) to relative precision 
or better yields that


Combining~(\ref{first}) and~(\ref{second}) yields that


It follows from the fact that 
is the orthogonal projection of 
onto the orthogonal complement of the column span of  that

Combining~(\ref{third}), (\ref{fourth}), and~(\ref{energy})
yields~(\ref{goodapprox}).
\end{proof}


Combining Lemmas~\ref{reformulation}, \ref{baselemma}, \ref{problemma},
and \ref{mainlemma} yields the following theorem,
guaranteeing that the algorithm produces high accuracy with high probability.

\begin{theorem}
\label{main_theorem}
Suppose that , , and  are positive real numbers
with  and ,
and , , and  are positive integers, such that~(\ref{weak}) holds.
Suppose further that  is a full-rank matrix,
 is a vector,
and  is the vector of minimal Euclidean norm satisfying


Then, the vector  defined in~(\ref{output}) satisfies

with probability at least .
\end{theorem}


\begin{remark}
The probability of failure in Theorem~\ref{main_theorem} is ,
rather than just the probability  of failure
in Lemma~\ref{problemma}, in order to account for the possibility that
the algorithm of~\cite{rokhlin-tygert} fails to produce a vector
 minimizing~(\ref{overdet}) to relative precision
 or better,
after using at most the number of floating-point operations
specified in Section~\ref{costs} below.
\end{remark}


\begin{remark}
Empirically, requiring~(\ref{weak}) appears to be excessive.
In practice, choosing any  and  makes failure
of the algorithm too improbable to detect
({\it cf.} Remark~2 in~\cite{rokhlin-tygert}).
\end{remark}



\section{Computational costs}
\label{costs}

In this section, we tabulate the numbers of floating-point operations
required by the five steps in the algorithm of Section~\ref{algorithm}:

\begin{enumerate}
\item Applying  to every column of 
      costs .
\item Constructing  costs .
\item Applying  to 
      costs .
\item Constructing  via the algorithm of~\cite{rokhlin-tygert}
      costs .
\item Applying  to  costs .
\end{enumerate}

Summing up the costs in the five steps above,
and using the facts that  and ,
we see that the cost of the entire algorithm is

floating-point operations.
Theorem~\ref{main_theorem} in Section~\ref{proofs} above
guarantees that the algorithm produces
high accuracy with high probability when  and  satisfy~(\ref{weak}).
In practice, choosing  and  makes failure
of the algorithm too improbable to detect
(see also Remark~2 in~\cite{rokhlin-tygert}),
and the cost in~(\ref{theoretical}) then becomes

floating-point operations.



\section{Numerical Results}
\label{numerical}

In this section, we describe the results of several numerical tests
of the algorithm of the present paper.

For various positive integers  and  with ,
we use the algorithm to compute a vector 
approximating the vector  of minimal Euclidean norm
satisfying ,
where  is a vector,
and  is the matrix defined via the formula

in the experiments described below,
 is obtained by applying the Gram-Schmidt process
to the columns of an  matrix whose entries are
i.i.d. centered complex Gaussian random variables,
 is obtained by applying the Gram-Schmidt process
to the columns of an  matrix whose entries are
i.i.d. centered complex Gaussian random variables,
and  is a diagonal matrix, with the diagonal entries

for ,~, \dots, ,~.
Clearly, the condition number  of  is

The vector  is defined via the formula

where  is the vector defined via the formula

with ,~, \dots, ,~
being pseudorandom positive and negative ones,
and ,~, \dots,
,~ being the columns
of .

\begin{remark}
By construction,  is the vector of minimal Euclidean norm
such that ;
the Euclidean norm of  is minimal since 
belongs to the column span of .
The aim of the algorithm is to construct an approximation 
to .
\end{remark}

For the direct computations, we used the classical algorithm
for pivoted -de\-com\-po\-si\-tions
based on plane (Householder) reflections
(see, for example, Chapter~5 in~\cite{golub-van_loan}).
We implemented the algorithms in Fortran~77 in double-precision arithmetic,
and used the Lahey/Fujitsu Express v6.2 compiler,
with the optimization flag {\tt -{}-o2} enabled.
We used one core of a 1.86~GHz Intel Centrino Core Duo microprocessor
with 2~MB of L2 cache and 1~GB of RAM.
For the algorithm of~\cite{rokhlin-tygert} used in Step~4
of the algorithm of Section~\ref{algorithm},
we requested that  minimize~(\ref{overdet})
to relative precision  or better,
where  is the condition number of 
given in~(\ref{condnum}).
We used a double-precision version of P.~N. Swarztrauber's FFTPACK library
for fast Fourier transforms.

Table~1 displays timing results with  for various values of ;
Table~2 displays the corresponding errors.
Table~3 displays timing results with  for various values of ;
Table~4 displays the corresponding errors.


\begin{table}
\hfil {\bf Table 1} \hfil\hfil {\bf Table 2} \hfil \\\vspace{-.75em}\\
\begin{tabular}{c|c|c|c|c|c}
 &    &   &  &  &  \\\hline
128 & 16384 &  512 & .27E1 &       .24E1 &             1.2 \\
256 & 16384 & 1024 & .11E2 &       .56E1 &             2.0 \\
512 & 16384 & 2048 & .60E2 &       .20E2 &             3.0 \\
\end{tabular}
\hfil
\begin{tabular}{c|c|c|c|c}
 &    &   &  &  \\\hline
128 & 16384 &  512 &     .14E--14 &           .16E--14 \\
256 & 16384 & 1024 &     .11E--14 &           .17E--14 \\
512 & 16384 & 2048 &     .80E--15 &           .29E--14 \\
\end{tabular}

\bigskip
\bigskip

\hfil {\bf Table 3} \hfil\hfil {\bf Table 4} \hfil \\\vspace{-.75em}\\
\begin{tabular}{c|c|c|c|c|c}
 &    &   &  &  &  \\\hline
256 &  4096 & 1024 & .26E1 &       .20E1 &             1.3 \\
256 &  8192 & 1024 & .51E1 &       .32E1 &             1.6 \\
256 & 16384 & 1024 & .11E2 &       .56E1 &             2.0 \\
256 & 32768 & 1024 & .29E2 &       .16E2 &             2.5 \\
\end{tabular}
\hfil
\begin{tabular}{c|c|c|c|c}
 &    &   &  &  \\\hline
256 &  4096 & 1024 &     .27E--15 &           .31E--14 \\
256 &  8192 & 1024 &     .45E--15 &           .27E--14 \\
256 & 16384 & 1024 &     .11E--14 &           .17E--14 \\
256 & 32768 & 1024 &     .22E--14 &           .16E--14 \\
\end{tabular}

\bigskip
\end{table}


The headings of the tables have the following meanings:
\begin{itemize}
\item  is the number of rows in the matrix ,
      as well as the length of the vector ,
      in .
\item  is the number of columns in the matrix ,
      as well as the length of the vector ,
      in .
\item  is the number of rows in the matrix  used
      in Steps~1 and~3 of the algorithm of Section~\ref{algorithm},
      as well as the analogous parameter used in the algorithm
      of~\cite{rokhlin-tygert} needed in Step~4.
\item  is the time in seconds required
      by the direct, classical algorithm.
\item  is the time in seconds required
      by the algorithm of the present paper.
\item  is the factor by which
      the algorithm of the present paper
      is faster than the classical algorithm.
\item  is defined via the formula

where  is the condition number of  given
in~(\ref{condnum}), and  is the vector
produced by the direct, classical algorithm
approximating the vector  of minimal Euclidean norm
such that .
\item  is defined via the formula

where  is the condition number of  given
in~(\ref{condnum}), and  is the vector
produced by the algorithm of Section~\ref{algorithm}
approximating the vector  of minimal Euclidean norm
such that .
\end{itemize}

\begin{remark}
Standard perturbation theory shows that
 and  are the appropriately normalized measures
of the relative precision produced by the algorithms;
see, for example, Section~5.5.3 in~\cite{dahlquist-bjorck}.
\end{remark}

The values for  reported in the tables are
the worst (maximal) values encountered during 10 independent randomized trials
of the algorithm, as applied to the same matrix 
and vector .
The values for  reported in the tables are the average values
over 10 independent randomized trials.
None of the quantities reported in the tables varied significantly
over repeated randomized trials.

The following observations can be made from the examples reported here,
and from our more extensive experiments:

\begin{enumerate}
\item When , ,
and the condition number of  is ,
the randomized algorithm runs 3 times faster than the classical algorithm
based on plane (Householder) reflections, even at full double precision.
\item Our choice  appears to make failure of the algorithm
too improbable to detect.
\item The algorithm of the present article reliably produces high precision
at reasonably low cost.
\end{enumerate}



\section{Conclusion}
\label{conclusions}

This article provides a fast algorithm for computing the minimal-norm solution
to an underdetermined system of linear equations.
If the matrices  and 
associated with the system of linear equations
can be applied sufficiently rapidly to arbitrary vectors, then the algorithm
of the present paper can be accelerated further.

The theoretical bounds in Lemma~\ref{reformulation}, Lemma~\ref{problemma},
and Theorem~\ref{main_theorem} should be considered preliminary.
Our numerical experiments indicate that the algorithm
of the present article performs better than our estimates guarantee.
Furthermore, there is nothing magical
about the subsampled randomized Fourier transform defined in~(\ref{SRFT}).
In our experience, several other similar transforms seem
to work at least as well, and we are investigating these alternatives
(see, for example,~\cite{ailon-liberty}).



\section*{Acknowledgments}
We would like to thank Vladimir Rokhlin and Arthur Szlam
for helpful discussions.



\newpage


\bibliographystyle{siam}
\bibliography{under}


\end{document}
