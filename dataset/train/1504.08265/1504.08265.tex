


\long\def\commsingle #1\commsingleend{}
\long\def\commdouble #1\commdoubleend{#1}



\long\def\commabs #1\commabsend{#1}
\long\def\commshort #1\commshortend{#1}
\long\def\commlong #1\commlongend{}


\long\def\oldProbLowerBound #1\oldProbLowerBoundEnd{}


\long\def\commEREZDELA #1\commEREZDELAend{}









\documentclass[11pt]{article}




\usepackage[dvips]{graphicx}
\usepackage{epsfig,graphicx}
\usepackage{amsmath, amssymb}






\setlength{\textheight}{8.7in}
\setlength{\textwidth}{6.5in}
\setlength{\evensidemargin}{-0.18in}
\setlength{\oddsidemargin}{-0.18in}
\setlength{\headsep}{10pt} \setlength{\topsep}{0in}



\def\inline#1:{\par\vskip 7pt\noindent{\bf #1:}\hskip 10pt}
\def\midinline#1:{\par\noindent{\bf #1:}\hskip 10pt}
\def\dnsinline#1:{\par\vskip -7pt\noindent{\bf #1:}\hskip 10pt}
\def\ddnsinline#1:{\newline{\bf #1:}\hskip 10pt}







\newtheorem{theorem}{Theorem}[section]
\newtheorem{dfn}{Definition}[section]
\newtheorem{note}{Note}[section]
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{preposition}[theorem]{Preposition}
















\def\proof{\par\noindent{\bf Proof:~}}

\def\blackslug{\hbox{\hskip 1pt \vrule width 4pt height 8pt
    depth 1.5pt \hskip 1pt}}

\def\QED{\quad\blackslug\lower 8.5pt\null\par}
\def\inQED{\quad\quad\blackslug}



\newcommand{\blineon}[0]{\mbox{{\bf LINE}}^{\mbox{\bf on}}}
\newcommand{\rot}[0]{v_0}
\newcommand{\TriV}[0]{\nabla_\calV}
\newcommand{\TriE}[0]{\nabla_\calE}
\newcommand{\baseV}[1]{\mbox{\sc Base}_\calV(#1)}
\newcommand{\baseE}[1]{\mbox{\sc Base}_\calH(#1)}


\newcommand{\E}{\mathbb{E}}
\newcommand{\movie}{{\sc Multimedia Content Delivery}}
\newcommand{\MCD}{\mbox{\sc MCD}}
\newcommand{\DMCD}{\mbox{\sc DMCD}}
\newcommand{\MCS}{\mbox{\sc MCD}}
\newcommand{\StRSA}{\mbox{\sc SRSA}}
\newcommand{\SRSA}{\mbox{\sc SRSA}}
\newcommand{\RSA}{\mbox{\sc RSA}}

\newcommand{\ceil}[1]{\left\lceil {#1} \right\rceil}
\newcommand{\COMMIT}[0]{\mbox{\sc commit}}
\newcommand{\commit}[0]{commit}


\newcommand{\horEdge}[1]{#1_{\calH}}
\newcommand{\arc}[1]{#1_{\calA}}
\newcommand{\pathoff}[0]{PATH^{\mbox{\upshape off}}}

\newcommand{\Hassign}[1]{\calH_{assign}(#1)}
\newcommand{\Aassign}[1]{\calA_{assign}(#1)}
\newcommand{\Aassigntuple}[2]{\calA^{-1}_{assign-tuple}[(\rep{#1}{#2},\rep{#1}{#2+1})]}
\newcommand{\Aassignsession}[2]{\calA^{-1}_{assign-session}[(\rep{#1}{#2},\rep{#1}{#2+1})]}


\newcommand{\neigh}[0]{N}
\newcommand{\NI}[1]{\neigh(#1)}
\newcommand{\NRI}[1]{\neigh^{R}(#1)}
\newcommand{\NLI}[1]{\neigh^{L}(#1)}
\newcommand{\NXI}[1]{\neigh^{X}(#1)}



\newcommand{\stayactive}[0]{\mbox{stays-active}}
\newcommand{\Active}[0]{\mbox{active}}


\newcommand{\Inter}[2]{{I^{#1}_{#2}}}
\newcommand{\Interk}[2]{{I^{#2}(#1)}}
\newcommand{\lfun}[1]{\ell(#1)}
\newcommand{\ttt}[0]{t}

\newcommand{\cost}[0]{cost}

\newcommand{\lrangle}[1]{\langle #1\rangle}




\newcommand{\sqlog}[0]{\Delta}
\newcommand{\median}[2]{m_{#1}^{#2}}
\newcommand{\medrep}[2]{p_{#1}^{#2}}
\newcommand{\cbit}[2]{b_{#1}^{#2}}

\newcommand{\calD}{\mathcal{D}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calLN}{\mathcal{LN}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calS}{\mathcal{S}}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\reals}{\mathbb{R}}


\newcommand{\opt}{\mbox{\textsc{opt}}}
\newcommand{\alg}{\textsc{alg}}

\newcommand{\Adet}[0]{A_{\mbox{\sc det}}}
\newcommand{\Cdet}{\mathcal{C}_{\mbox{\sc det}}}
\newcommand{\Crand}{\mathcal{C}_{\mbox{\sc rand}}}
\newcommand{\randpar}[0]{\Delta}


\newcommand{\lineon}{\mbox{\sc Line}^{\mbox{\upshape on}}}
\newcommand{\lineonp}{\mbox{\sc Line}_+^{\mbox{\upshape on}}}
\newcommand{\lineonmn}{\mbox{\sc Line}_{\MM,\nn}^{\mbox{\upshape on}}}
\newcommand{\onRSAmn}{\mbox{\sc rsa}^{\mbox{\upshape on}}_{\MM,\nn,p}}
\newcommand{\onRSAmnk}[1]{\mbox{\sc rsa}^{\mbox{\upshape on}}_{\MM_{#1},\nn_{#1},p_0^{#1}}}
\newcommand{\onRSAmnp}[0]{\mbox{\sc rsa}^{\mbox{\upshape on}}_{\MM,\nn,p}}
\newcommand{\onRSAn}{\mbox{\sc rsa}^{\mbox{\upshape on}}_{\nn}}
\newcommand{\onRSAnk}[1]{\mbox{\sc rsa}^{\mbox{\upshape on}}_{\nn_{#1}}}
\newcommand{\onRSA}{\mbox{\sc rsa}^{\mbox{\upshape on}}}


\newcommand{\Arand}{\mathcal{A}_{\mbox{\sc rand}}}


\newcommand{\Base}[0]{\mbox{\sc Base}}
\newcommand{\DBase}[0]{{\mbox{\sc Base}}}

\newcommand{\calFT}[0]{\calF^{\mbox{\small\sc t}}}
\newcommand{\calHT}[0]{\calH^{\mbox{\small\sc t}}}
\newcommand{\calAT}[0]{\calA^{\mbox{\small\sc t}}}
\newcommand{\Von}[0]{\calV^{\mbox{\upshape on}}}
\newcommand{\Aon}[0]{\calA^{\mbox{\upshape on}}}
\newcommand{\Hon}[0]{\calH^{\mbox{\upshape on}}}
\newcommand{\Ton}[0]{\calT^{\mbox{\upshape on}}}
\newcommand{\Fon}[0]{\calF^{\mbox{\upshape on}}}
\newcommand{\Eon}[0]{\calE^{\mbox{\upshape on}}}

\newcommand{\Agrid}[1]{\calA^{\mbox{\upshape on}}_{\langle #1\rangle}}
\newcommand{\Hgrid}[1]{\calH^{\mbox{\upshape on}}_{\langle #1\rangle}}
\newcommand{\Fgrid}[2]{\calF^{#1}_{#2}}
\newcommand{\Fgridon}[2]{\calF^{\mbox{\upshape on}}_{#1,#2}}

\newcommand{\RSAFon}[0]{\calF^{\mbox{\upshape St}}_{\MM,\nn}}
\newcommand{\xmaxQ}[0]{\mbox{max}_x\calQ}
\newcommand{\xmaxQi}[0]{\max_x(\calQ_i)}
\newcommand{\xmaxQgk}[1]{\max_x(\calQ_{#1})}
\newcommand{\xmax}[1]{\max_x({#1})}


\newcommand{\Triangle}[0]{\mbox{\sc Triangle}}
\newcommand{\Square}[0]{\mbox{\sc Square}}


\newcommand{\grid}[2]{\Gamma^{#1}_{#2}}
\newcommand{\gridQmn}[0]{\grid{\MM}{\nn}(\calQ)}
\newcommand{\gridQmni}[0]{\grid{\MM}{\nn}(\calQ_i)}
\newcommand{\RmnQ}[0]{\calR^{\MM}_{\nn}(\calQ)}
\newcommand{\RmnQi}[0]{\calR^{\MM}_{\nn}(\calQ_i)}
\newcommand{\RmnZ}[0]{\calR^{\MM}_{\nn}(\calZ)}

\newcommand{\numgrupQ}[0]{\#(\calQ)}


\newcommand{\Qij}[1]{\calQ\langle #1\rangle}


\newcommand{\MM}[0]{M}

\newcommand{\Xmn}[1]{x^{\MM}_{\nn}(#1)}
\newcommand{\Ymn}[1]{y^{\MM}_{\nn}(#1)}

\newcommand{\lline}[1]{\langle #1\rangle}
\newcommand{\linev}[1]{L_{ver}\langle #1\rangle}
\newcommand{\lineh}[1]{L_{hor}\langle #1\rangle}

\newcommand{\rep}[2]{(#1,#2)}
\newcommand{\rr}{r}
\newcommand{\qq}{q}
\newcommand{\NN}{N}
\newcommand{\sizeR}{N}
\newcommand{\nn}{n}

\newcommand{\qon}{q^{\mbox{\upshape on}}}
\newcommand{\uon}{u^{\mbox{\upshape on}}}
\newcommand{\son}{s^{\mbox{\upshape on}}}

\newcommand{\qT}{q^{\mbox{\small\sc t}}}
\newcommand{\uT}{u^{\mbox{\small\sc t}}}
\newcommand{\sT}{s^{\mbox{\small\sc t}}}


\newcommand{\rhoT}[1]{\rho^{\mbox{\small\sc t}}_{ #1}}
\newcommand{\ron}[1]{\rho^{\mbox{\upshape\small on}}_{ #1}}

\newcommand{\PHon}[0]{\calP_\calH^{\mbox{\upshape\small on}}}
\newcommand{\PVon}[0]{\calP_\calV^{\mbox{\upshape\small on}}}




\newcommand{\stime}[1]{x^{first}_{#1}}
\newcommand{\ltime}[1]{x^{last}_{#1}}

\newcommand{\tminus}[2]{t^{-}_{#1}#2}
\newcommand{\tplus}[1]{t^{+}_{#1}}
\newcommand{\numper}[1]{{\#}_{per}({#1})}

\newcommand{\next}[0]{next}
\newcommand{\last}[0]{last}
\newcommand{\bin}[0]{bin}
\newcommand{\payer}[0]{payer}

\newcommand{\combin}[0]{\COMMIT}
\newcommand{\horbin}[0]{\calHT}
\newcommand{\arcbin}[0]{\calAT}
\newcommand{\BIN}[0]{\mbox{\sc bin}}
\newcommand{\Amin}[0]{\calA^{-1}}

\newcommand{\tstarp}[1]{t^*{\langle#1\rangle}}
\newcommand{\tstar}[0]{t^*}

\newcommand{\vstarp}[1]{v^*\langle{#1}\rangle}
\newcommand{\vstar}[0]{v^*}



\newcommand{\distinf}[1]{{dist}^{\rightarrow}_{\infty}(#1)}
\newcommand{\distDL}[1]{\overrightarrow{dist}_{\calL}(#1)}
\newcommand{\norminf}[1]{\|#1\|_{\infty}}


\newcommand{\bfDlineon}[0]{\mbox{{\bf D-Line}}^{\mbox{\bf on}}}
\newcommand{\Dlineon}[0]{\mbox{{\sc D-Line}}^{\mbox{\upshape on}}}
\newcommand{\NID}[2]{\overrightarrow{\neigh}^{#1}(#2)}

\newcommand{\Dron}[1]{{\rho}^{\mbox{\upshape\small on}}_{ #1}}
\newcommand{\DPHon}[0]{\calH^{\mbox{\upshape\small on}}}
\newcommand{\DPVon}[0]{\calV^{\mbox{\upshape\small on}}}

\newcommand{\Dqon}{{q}^{\mbox{\upshape on}}}
\newcommand{\Duon}{{u}^{\mbox{\upshape on}}}
\newcommand{\Dson}{{s}^{\mbox{\upshape on}}}

\newcommand{\qSQ}{q^{\mbox{\small\upshape serve}}}
\newcommand{\uSQ}{u^{\mbox{\small\upshape serve}}}
\newcommand{\sSQ}{s^{\mbox{\small\upshape serve}}}

\newcommand{\qclose}{q^{\mbox{\small\upshape close}}}
\newcommand{\uclose}{u^{\mbox{\small\upshape close}}}
\newcommand{\sclose}{s^{\mbox{\small\upshape close}}}


\newcommand{\tail}[0]{\mbox{\sc tail}}
\newcommand{\FSQ}[0]{\calF^{\mbox{\small\upshape SQ}}}
\newcommand{\HSQ}[0]{\calH^{\mbox{\small\upshape SQ}}}
\newcommand{\ASQ}[0]{\calA^{\mbox{\small\upshape SQ}}}
\newcommand{\ADon}[0]{\overrightarrow{\calA}^{\mbox{\upshape on}}}
\newcommand{\HDon}[0]{\overrightarrow{\calH}^{\mbox{\upshape on}}}
\newcommand{\FDon}[0]{\overrightarrow{\calF}^{\mbox{\upshape on}}}
\newcommand{\EDon}[0]{\calE^{\mbox{\upshape on}}}

\newcommand{\pivot}[0]{pivot}
\newcommand{\JJ}[0]{J}

\newcommand{\NDI}[1]{\overrightarrow{\neigh}(#1)}
\newcommand{\DNLI}[1]{\overrightarrow{\neigh}^{L}(#1)}

\newcommand{\DCOMMIT}[0]{\mbox{\sc commit}}
\newcommand{\Dcommit}[0]{commit}

\newcommand{\eqdf}{\stackrel{\scriptscriptstyle \triangle}{=}}

\newcommand{\rhoSQ}[0]{\rho^{\mbox{\small\upshape SQ}}}
\newcommand{\Jnter}[3]{I\langle#1\rangle_{#2}^{#3}}

\newcommand{\seq}[1]{\calS[#1]}

\newcommand{\dist}[1]{\mbox{dist}(#1)}


\newcommand{\reck}[0]{\mbox{\sc rec}}


\newcommand{\hh}[1]{h_{#1}}


\newcommand{\GG}[1]{G_{#1}}
\newcommand{\GGm}[1]{\widehat{\GG{#1}}}
\newcommand{\GGmax}[1]{{g_{#1}^{max}}}
\newcommand{\GGmin}[1]{{g_{#1}^{min}}}


\newcommand{\sep}[0]{g}


\newcommand{\Tree}[0]{\mbox{\sc tree}}
\newcommand{\Edis}[0]{\overline{\mbox{\sc E-Disjoint}}}


\newcommand{\parent}[0]{parent}
\newcommand{\suns}[0]{\mbox{\sc children}}
\newcommand{\roots}[0]{\mbox{\sc roots}}


\newcommand{\SQball}[0]{Q{\mbox{\sc -ball}}^{\mbox{\sc sq}}}
\newcommand{\uncover}[0]{\mbox{\sc uncover}}
\newcommand{\cover}[0]{\mbox{\sc cover}}
\newcommand{\Qball}[0]{Q{\mbox{\sc -ball}}}
\newcommand{\reqtangle}[1]{\mbox{\sc reqtangle}[#1]}


\newcommand{\radroute}[0]{rad\mbox{-}route}

\newcommand{\qlast}[0]{q^{\mbox{\small\upshape last}}}

\newcommand{\rhodl}[0]{\rho^{\mbox{\upshape DL}}}


\newcommand{\loglogratio}[1]{\frac{\log #1}{\log\log #1}}

\newcommand{\ff}[0]{f}
\newcommand{\nguess}[0]{n\mbox{-}guess}
\newcommand{\Mguess}[0]{M\mbox{-}guess}
\newcommand{\tetration}[1]{2^{2^{2#1}}}
\newcommand{\costm}[0]{\widetilde{\cost}}
\newcommand{\FRSAn}[0]{\calF^{\mbox{\sc rsa}}_{\nn}}
\newcommand{\FRSAnk}[1]{\calF^{\mbox{\sc rsa}}_{\nn_{#1}}}
\newcommand{\FRSAmn}[0]{\calF^{\mbox{\sc rsa}}_{\MM,\nn,p}}
\newcommand{\FRSAQ}[0]{\calF^{\mbox{\sc rsa}}(\calQ)}
\newcommand{\FRSA}[0]{\calF^{\mbox{\sc rsa}}}

\newcommand{\Dlineonp}{\mbox{\sc D-Line}_+^{\mbox{\upshape on}}}

\newcommand{\half}{\frac{1}{2}}
\newcommand{\diam}{\text{diam}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\inv}[1]{\frac{1}{#1}}

\newcommand{\onalgrsa}[0]{\mbox{\sc onalg}_{\mbox{\sc\small mcd}}}
\newcommand{\logoverloglog}[0]{\frac{\log n}{\log\log n}}
\newcommand{\diag}[0]{\mbox{\sc diag}}





\begin{document}



\title{
Optimal competitiveness for the Rectilinear Steiner Arborescence problem
}

\author{
Erez Kantor
\thanks{CSAIL, MIT, Cambridge, MA.
Supported in a part by NSF Awards 0939370-CCF, CCF-1217506 and  CCF-AF-0937274 and AFOSR FA9550-13-1-0042.
}\\
{\small\tt erezk@csail.mit.edu}
\and
Shay Kutten
\thanks{Department of
Industrial Engineering and Management,
IE\&M, Technion,
Haifa, Israel.  Supported in part by the
ISF,
Israeli ministry of science
and by the Technion Gordon Center.
} \\
{\small\tt kutten@ie.technion.ac.il}
}






\date{}

\maketitle

\begin{abstract}
We present optimal online algorithms for two related known problems involving Steiner Arborescence, improving both the lower and the upper bounds.
One of them is the well studied continuous problem of the {\em Rectilinear Steiner Arborescence} ().
We improve the lower bound and the upper bound on the competitive ratio for  from  and  to ,
where  is the number of Steiner points.
This separates the competitive ratios of  and the Symetric- , two problems for which the bounds of Berman and Coulston is STOC 1997 were identical.
The second problem is one of the Multimedia Content Distribution problems presented by Papadimitriou et al. in several papers and Charikar  et al. SODA 1998.
It can be viewed as the discrete counterparts (or a network counterpart) of .
For this second problem we present tight bounds also in terms of the network size,
in addition to presenting tight bounds in terms of the number of Steiner points (the latter are similar to those we derived for ).
\end{abstract}

\paragraph*{\bf Keywords: Online Algorithm, Approximation Algorithm, Video-on-Demand}




\section{Introduction}
\label{sec: Introduction}

Steiner trees, in general, have many applications, see e.g.
\cite{steiner-book} for a rather early survey that already included hundreds of items.
In particular, Steiner Arborescences\footnote{A Steiner arborescence is a Steiner tree directed away from the root.
}
are useful for describing the evolution of processes in time.
Intuitively, directed edges represent the passing of time.
Since there is no way to go back in time in such processes, all the directed edges are directed away from the initial state of the problem (the root), resulting in an arborescence. Various examples are given in the literature
such as processes in constructing a
Very Large Scale Integrated electronic circuits (VLSI),
optimization problems computed in iterations (where it was not feasible to return to results of earlier iterations),
dynamic programming, and problems involving DNA, see, e.g. \cite{berman,CDL01,vlsi,KnuthYao09}.
Papadimitriou at al. \cite{papa1,papa3} and Charikar et al. \cite{halperin} presented the discrete version, in the context of Multimedia Content Delivery ()
to model locating and moving caches for titles on a path graph.
The formal definition of (one of the known versions ) of this problem, Directed-,
appears in Section \ref{sec:preliminaries}.







We present new tight lower and upper bounds for two known interrelated problems involving Steiner Arborescences:
{\em Rectilinear Steiner Arborescence ()} and Directed- (\DMCD).
We also deal indirectly with a third known arborescence problem: the {\em Symmetric-} () problem
by separating its competitive ratio from that of . That is, when the competitive ratios of  and  were discussed originally by Berman and Coulston \cite{berman}, the same lower and upper bounds were  presented for both problems.





\paragraph*{The {\em RSA} problem:}
This is a rather heavily studied problem, described also e.g. in \cite{ptas1,shor-rsa,berman,natansky,presented-rsa}.
A rectilinear line segment in the plane is either horizontal or vertical.
A rectilinear path contains only rectilinear line segments.
This path is also -{\em monotone} (respectively, -{\em monotone}) if during the traversal, the  (resp., ) coordinates of the successive points
are never decreasing.
The input is a set of {\em requests}

called Steiner terminals (or points) in the positive quadrant of the plane.
A feasible solution  to the problem is a set of rectilinear segments connecting all the  terminals to the origin ,
where the path from the origin to each
terminal is both -monotone and -monotone (rectilinear shortest path).
The goal is to find a feasible solution in which the sum of lengths of all the segments is the minimum possible.
The above mentioned third problem, 
was defined in the same way, except that the above paths were not required to be -monotone (only -monotone).

Directed- defined in Section \ref{sec:preliminaries} is very related to .
Informally, one difference is that it is discrete (Steiner points arrive only at discrete points) whiling  is continuous.
In addition, in  each `` coordinates''  represents a network nodes.
Hence, the number of  coordinates is bounded from above by the network size.
This resemblance turned out to be very useful for us, both for solving  and for solving .


\paragraph*{The {\em online} version of {\em RSA} \cite{berman}:}
the given requests (terminals) are presented to the algorithm with nondecreasing
-coordinates.
After receiving the 'th request  (for ),
the on-line  algorithm must extend the existing arborescence solution to incorporate .
There are two additional constraints:
(1) a line, once drawn (added to the solution), cannot be deleted,
and
(2) a segment added when handling a request , can
only be drawn in the region between  (the -coordinates of the previous request ) and upwards
(grater -coordinates).
If an algorithm obeys constraint (1) but not constraint (2), then we term it a {\em pseudo online} algorithm.
Note that quite a few algorithms known as ``online'', or as ``greedy offline'' fit this definition of ``pseudo online''.




\paragraph*{Additional Related works.}
Online algorithms for  and  were presented by Berman and Coulston \cite{berman}.
The online algorithms in \cite{berman} were  competitive (where  was the number of the Steiner points) both for  and .
Berman and Coulston also presented  lower bounds for both continuous problems.
Note that the upper bounds for both problems were equal, and were the squares of the lower bounds.
A similar gap for  arose from results of Halperin, Latombe, and  Motwani \cite{halperin1}, who gave a similar  competitive ratio of ,
while  Charikar, Halperin,  and Motwani \cite{halperin} presented a lower bound of   for various variants of ,  where  was the size of the network.
Their upper bound was again the square of the lower bound:
  (translating their parameter  to the parameter  we use).




Berman and Coulston also conjectured that to close these gaps, both the upper bound and the lower bound
for both problems could be improved.
This conjecture was disproved in the cases of   and of  on undirected line networks \cite{KK2014}.
The latter paper closed the gap by presenting an optimal competitive ratio of  for  and
 for  on the undirected line network with  nodes.
They left the conjecture of Berman and Coulston open for  and for  on directed line networks.
In the current paper, we prove this conjecture (for  and for Directed-), thus separating  and  in terms of their competitive ratios.



Charikar, Halperin, and Motwani \cite{halperin} also studied the  the offline case for , for which they gave   a constant approximation.
The offline version of  is heavily studied.
It was  attributed to    \cite{natansky} who gave an exponential integer programming solution and to \cite{presented-rsa} who gave an exponential time dynamic programming algorithm.
An exact and polynomial algorithm was proposed in \cite{rsa-error}, which seemed surprising, since many Steiner  problems are NP Hard.
Indeed, difficulties in that solution were noted by Rao, Sadayappan, Hwang, and Shor \cite{shor-rsa}, who also presented an approximation algorithm.
Efficient algorithms are claimed in \cite{another-at-poly} for VLSI applications.
However, the problem was proven NP-Hard in \cite{rsa-nph}.
(The rectilinear Steiner tree problem was proven NPH in \cite{garey-johnson}).
Heuristics that are fast ``in practice'' were presented in \cite{cong}.
A PTAS was presented by \cite{ptas1}.
An optimal logarithmic competitive ratio for  on {\em general undirected} networks was presented in \cite{MAicalp12}.
They also present a constant off-line approximation for  on grid networks.

\paragraph*{On the relation between this paper and \cite{KK2014}.}
An additional contribution of the current paper is the further development of the approach of developing (fully) online algorithms in two stages:
(a) develop a pseudo online algorithm; and
(b) convert the pseudo online into an online algorithm.
As opposed to the problem studied in \cite{KK2014} where a pseudo online algorithm was known, here the main technical difficulty was to develop such an algorithm.
From \cite{KK2014} we also borrowed an interesting twist on the rather common idea to translate between instances of a discrete and a continuous problems: we translate in {\em both} directions, the discrete  solutions helps in optimizing the continuous one {\em and vice versa}.




\paragraph*{Our Contributions.}
We improve both the upper and the lower bounds of   to show that the competitive ratio is .
This proves the conjecture for  of  Berman and Coulston  \cite{berman} and also separates the competitive ratios of  and .
We also provide tight upper and lower bound for Directed-, the network version of  (both in terms of   and of ).
The main technical innovation is the specific pseudo online algorithm we developed here,
in order to convert it later to an online algorithm.
The previously known offline algorithms for  and for  where {\em not} pseudo online, so we could not use them.
In addition to the usefulness of the new algorithm in generating the online algorithm, this pseudo online algorithm may be interesting in itself:
It is -competitive for  and for  (via the transformation)
for a {\em different} (but rather common) online model (where
each request must be served before the next one arrives, but no time passes between requests).






\paragraph*{Paper Structure.}
Definitions are given in Section \ref{Sec:preliminaries}.
The pseudo online algorithm  for  is presented and analyzed in Section \ref{sec:square}.
In Section \ref{subsec: Algorithm Donline}, we transform  to a (fully) online algorithm   for .
Then, Section \ref{subsec: optRSA} describes the transformation of the online  algorithm   to become an optimal online algorithm for , as well as a transformation back from  to  to make the  online algorithm also optimal in terms of  (not just ).
These last two transformations are taken from \cite{KK2014}. Finally, a lower bound is given in Section \ref{sec:Lower bound}.
The best way to understand the algorithms in this paper may be from a geometric point of view.
Hence, we added multiple drawings
to illustrate both the algorithms and the proofs.




\vspace{-0.3cm}
\section{Preliminaries}
\label{Sec:preliminaries}
\label{sec:preliminaries}




\paragraph*{\bf The networktime grid} (Papadimitriou et. al, \cite{papa3}).
A {\em directed line network }  is a network whose node set is  and its edge set is .
Given a directed line network , construct ''time-line'' graph , intuitively, by ``layering'' multiple replicas of ,
one per time unit, where in addition, each node in each replica is connected to the same node in
the next replica
(see Fig. \ref{figure:TimeNet}).
Formally, the node set  contains a {\em node replica}
(sometimes called just a {\em replica})  of every ,
coresponding to each time step .
That is, .
The set of directed edges  contains \emph{horizontal directed edges}
,
connecting network nodes in every time step (round),
and directed {\em vertical edges}, called {\em arcs},
,
connecting different copies of .
When  is clear from the context, we may write just  rather than , for every .
Notice that  can be viewed geometrically as a
grid of  by  whose grid points are the replicas.
Following Fig. \ref{figure:TimeNet}, we consider the time as if it proceeds upward.
We use such geometric presentations also in the text, to help clarifying the description.




\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.4\textwidth]{TimeNet.eps}
\end{center}
\caption{\sf \label{figure:TimeNet}
An example of a time-line graph .
Each node in  is represented by a circle;
 each horizontal edge in  is represented by a horizontal segment (see, as an example,  for an horizontal directed edge in the marked  rectangle on the right);
each arc in  is represented by a horizontal arrow (see, as an example,
 for an arc in the marked  rectangle on the left).
}
\end{figure}




\paragraph*{\bf The  problem.}

We are given a directed line network , an \emph{origin} node  ,
and a set of \emph{requests} .
A feasible solution
is a subset of directed edges 
such that for every request , there exists a path in  from the origin  to .
Intuitively a directed horizontal edge  is for delivering a copy of a multimedia title from node  to node  at time .

A directed vertical edge (arc)  is for storing a copy of the title at node  from time  to time .
For convenience, the endpoints  of edges in  are also considered parts of the solution.
For a given algorithm , let  be the solution of , and let , (the cost of  algorithm ), be .
(We assume that each storage cost and each delivery cost is .)
The goal is to find a minimum cost feasible solution.
Let  be the set of edges in some optimal solution whose cost is .













\vspace{-0.3cm}
\paragraph{\bf Online {\bf \em DMCD}.}
\label{app:online}
In the online versions of the problem,  the algorithm receives as input a sequence of events.
One type of events is a request in the (ordered)
set  of requests ,
where the requests times are in a non-decreasing order, i.e.,   (as in ).
A second type of events is a time event (this event does not exists in ),
where we assume a clock that tells the algorithm that
no additional requests for time  are about to arrive (or that there are no requests for some time  at all).
The algorithm then still has the opportunity to complete its calculation for time  (e.g., add arcs from some replica  to ).
Then time  arrives.

When handling an event ,
the algorithm only knows the following:
(a) all the previous requests
;
(b) time ; and
(c) the solution arborescence  it constructed so far (originally containing only the origin).
In each event,
the algorithm may need to
make decisions of two types, before seeing future events: \begin{itemize}
\vspace{-0.1cm}

\item [(1.)] If the event is the arrival of a request , then from which {\em current} (time ) cache
(a point already in the solution arborescence  when  arrives) to serve 
by adding {\em horizontal} directed edges to .


\item[(2.)]
If this is the time event for time , then at which nodes to store a copy for time , for future use: select some replica (or replicas)
 already in the solution  and add to  an edge directed from  to  .


\end{itemize}

\noindent Note that at time , the online algorithm cannot add nor delete any edge with an endpoint that corresponds to previous times.
Similarly to e.g. \cite{MAicalp12,papa1,papa2,papa3,halperin},
at least one copy must remain in the network at all times.






\paragraph*{\bf General definitions and notations.\commsingle.\commsingleend}

Consider an interval 
and two integers , s.t. .
Let  (see Fig. \ref{figure:subgraphJ}) be the
{\em ``rectangle subgraph''} of  corresponding to vertex set  and time interval .
This rectangle consists of the
replicas and edges of the nodes of  corresponding to every time in the interval .
For a given subsets ,  and ,
denote by
(1)  replicas of  corresponding to times
 . Define similarly (2)  for horizontal edges of ; and (3)  arcs of .
(When , we may write
, for .)
\begin{figure}[http!]
\begin{center}
\includegraphics[width=0.4\textwidth]{subgraphJ.eps}
\end{center}
\caption{\sf \label{figure:subgraphJ}
A subgraph rectangle , where .}
\end{figure}
Consider also two nodes  s.t. .
Let 
be the set of horizontal directed edges of the  path from  to .
Let  be the set of arcs
of the path from  to .
Let  be the ``directed'' distance from  to  in  norm.
Formally, , if  and  and , otherwise.



\vspace{-0.0cm}


\vspace{-0.3cm}
\section{Algorithm , a pseudo online algorithm}
\label{sec:square}




This section describes a pseudo online algorithm named  for the  problem.
Developing  was the main technical difficulty of this paper.
Consider a requests set  such that .
When Algorithm  starts, the solution includes just .
Then,  handles, first, request , then, request , etc...
In handling a request , the algorithm
may add some edges to the solution.
(It never deletes any edge from the solution.)
After handling , the solution is an arborescence rooted at  that spans the request replicas .
Denote by  the solution of  after handling the 'th request.
For a given replica  and a positive integer ,
let
\vspace{-0.1cm}

denotes the rectangle subgraph (of the layered graph) whose top right corner is  induced by the set of replicas
that contains every replica  such that
(1) there is a directed path in the layer graph from  to ; and
(2) the distance from  to  in  is at most .
For each request , for ,  performs the following
(The pseudo code of  is given in Fig. \ref{figure:Pseducode Squarem}
and an example of an execution in Fig. \ref{fig: Square execution SQ3}).



\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.35]{SqrA.eps}
\hfill
\includegraphics[scale=0.35]{SqrB.eps}
\hfill
\includegraphics[scale=0.35]{SqrC.eps}
\end{center}
\caption{\sf  execution example.
(a) in case (SQ3),  ('s solution after handling point );  defines the radius .
(b) the serving replica  is the leftmost in .
(c) .
\label{fig: Square execution SQ3}
}
\end{figure}


\begin{itemize}

\item[(SQ1)] Add the vertical path from  to .


\item[(SQ2)] Let replica  be such that  is already in the solution  and
    (1) the distance in  norm from  to  is minimum (over the replicas already in the solution); and
    (2) over those replicas choose the latest,
    that is, .
Define the {\em radius} of  as .
Call  the {\em closest} replica of the 'th request.




\item [(SQ3)] Choose a replica  such that  is already in the solution 
              and  is the leftmost node (over the nodes corresponding to replicas of  that are already in the solution).
              Call  the {\em serving replica} of the 'th request.



\item[(SQ4)]
Deliver a copy from  to  via .
This is done by storing a copy in node 
from time  to time ,
and then delivering a copy from  to 
\footnote{More formally, add the arcs of 
and the horizontal directed edges of 
to the solution.
}.


\item[(SQ5)]
Store a copy in  from time  to time 
\footnote{
More formally, add the arcs of 
to the solution.
}.

\end{itemize}


\noindent Intuitively, steps SQ1--SQ4 utilize previous replicas in the solution, while
step SQ5 prepares the contribution of  to serve later requests.
Note that  is not an online algorithm, since in step SQ4,
it may add to the solution some arcs corresponding to previous times.
Such an action cannot be preformed by an online algorithm.
Denote by  the feasible solution  of .
Let  and let 
(notice that  because of step SQ4).
Similarly, let 
be the nodes of the path 
(added to the solution in  step SQ5)
and let .
Note that  is indeed an arborescence rooted at .




\begin{figure}[ht!]
\fboxsep=0.2cm
\framebox[\textwidth]{
\begin{minipage}{0.95\textwidth}

~  is the 's solution after handling request .

~ When request  arrives do:
\begin{enumerate}

    \item .

    \item [2'.].

    \item Choose a replica  such that  is in  and\\
     (a) ; and\\
     (b) .



    \item Choose the serving replica  such that
    .\\
      is the leftmost node
    corresponding to the replicas of 


    \item .

     deliver a copy from  to  via .

    \item .

     leave a copy at  from current time  to time .


    \end{enumerate}

\end{minipage}
}
\caption{\label{figure:Pseducode Squarem}
Algorithm .
}
\end{figure}



\newpage
\subsection{Analysis of }
\label{subsec: Analysis Square}

\vspace{-0.2cm}

First, bound the cost of   as a function of the radii (defined in SQ2).


\begin{observation}

\label{obser:sqr: cost Square > 14 sum radii}
\end{observation}
\proof
For each request , Algorithm  pay a cost of  to the path between 's serving replica  a  itself (see step SQ4) and additional cost of  for serving a copy to all replicas of  (see step SQ5).
\QED


It is left to bound from below the cost of the optimal solution as a function of the radii.


\paragraph*{\bf Quarter balls.}
Our analysis is based on the following notion.
A \emph{quarter-ball}, or a , of \emph{radius}
 centered at a replica  contains every replica
from which there exists a path of length  to 
\footnote{
This is, actually, the definition of the geometric place ``ball''.
We term them ``quarter ball'' to emphasize that we deal with directed edges.
That is, it is not possible to reach  from above nor from the right.
}
.
For every request , denote by 
\footnote{
Note that  is different from , since the first ball considers distances in  norm and the last considers distances in  norm.
}
(also  for short) the quarter-ball centered at  with radius .

Intuitively, for every request  (where  obey the observation's condition below), 's solution starts outside of ,
and must reach  with a cost of  at least.

\begin{observation}
Consider some subset  of requests.
If the -balls,  and
, of every two requests   are edges disjoint, then
.
\label{obser:sqr:opt geq sum rho_i}
\end{observation}
Intuitively, for every request  (where  obey the observation's condition), 's solution starts outside of ,
and must reach  with a cost of  at least.
\proof
Consider some request .
Any directed path from  to  must enter the quarter ball  of radius  to reach .
The length of this path inside the  is .
All the s of  are disjoint, which implies the observation.
\QED






\subsubsection{Covered and uncovered requests}
Consider some request  and its serving replica 
(see step SQ3).
We say that   is {\em covered}, if  (see SQ2 and SQ3).
Intuitively, this means the solution  is augmented by the whole top of the square ; see Figure \ref{fig: Square execution SQ3} (a) and (b).
Otherwise, we say that  is {\em uncovered}.
Let  and let .
Given Observation \ref{obser:sqr:opt geq sum rho_i},
the following lemma implies that





\begin{lem}
Consider two {\bf covered} requests  and .\\
Then, the  quarter balls
 and 
are edge disjoint.
\label{lem:Square: covered requests are edges disjoint}
\end{lem}

\begin{proof}
Assume without loss of generality that, .
Thus,  (see SQ4) is already in the solution when handling request .
Also, , since  is covered.
Consider three cases.


\begin{itemize}
\item[{\bf Case 1.}]
  , see Figure \ref{fig:Square: covered requests case 1}.
Since,  is covered, .
If , then these two s are edges disjoint.
Otherwise, .
Then  which implies that these two s are edges disjoint.


\item[{\bf Case 2.}] , see Figure \ref{fig:Square: covered requests case 2}.
Then, also , since  is covered.
Thus, in particular, .
Hence, , which implies that these two s are edges disjoint.


\item[{\bf Case 3.}] .
The -ball of  is on the right of any possible (radius) -ball with  as a center.
Thus, these -balls are edges disjoint.

\end{itemize}
\QED
\end{proof}



\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.37]{CoverreqA1.eps}
\hfill
\includegraphics[scale=0.37]{CoverreqA2.eps}
\end{center}
\caption{\sf Two covered requests are edge disjoint, case 1; (a)  is on the right of , since ;
(b)  implying that the whole  is above .
\label{fig:Square: covered requests case 1}
}
\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.37]{CoverreqB.eps}
\end{center}
\caption{\sf
Two covered requests are edge disjoint, case 2.
\label{fig:Square: covered requests case 2}
}
\end{figure}





By the above lemma and observations \ref{obser:sqr: cost Square > 14 sum radii}, \ref{obser:sqr:opt geq sum rho_i},
and Inequality (\ref{ineq:sqr: opt geq sum cover radii}), we have:

\begin{observation}
\label{obs:covered}
's cost for covered requests is no more than .
\end{observation}
\vspace{0.3cm}
\noindent It is left to bound the cost of  for the uncovered requests.




\subsubsection{Overview of the analysis of the cost of uncovered requests}

Unfortunately, unlike the case of covered requests, balls of two {\em uncovered} requests may not be disjoint.
Still, we managed to have a somewhat similar argument that we now sketch.
(The formal analysis appears later in Subsection \ref{subsec: Formal Ana Square}.)
Below, we partition the balls of uncovered requests into disjoint subsets.
Each has a representative request, a {\em root}.
We show that the  of roots {\em are} edge disjoint.
This implies by Observation \ref{obser:sqr: cost Square > 14 sum radii} and Observation \ref{obser:sqr:opt geq sum rho_i}
that the cost  pays for the roots is smaller than 14 times the total cost of an optimal solution.
Finally, we show that the cost of  for all the requests in each subset is at most twice the cost of   for the root of the subset.
Hence, the total cost of  for the uncovered requests is also just a constant times the total cost of the optimum.


To construct the above partition, we define the following relation:
ball  is the {\em child} of  (for two {\em uncovered} requests  and ) intuitively,
if the  is the first ball (of a request later then ) such that  and  are {\em not} edge disjoint.
Clearly, this parent-child relation induces a forest on the s of uncovered requests.
The following observation follows immediately from the definition of a root.

\begin{observation}
\label{obs:roots-distjoint}
The quarter balls of every two root requests are edge disjoint.
\end{observation}
\begin{proof}
Consider two root requests  and . Assume W.O.L.G that .
Also assume, toward contradiction that  and  are {\bf not} edge disjoint.
By the definition of the child parent relationship, either  is child of , or  is a child of some other request  for some .
In both cases,  has a parent, hence  is not a root request which contradict to choice of  as a root request.
The observation follow.
\QED
\end{proof}


\noindent The above observation together with Observation
\ref{obser:sqr:opt geq sum rho_i}, implies the following.

\vspace{0.2cm}
\begin{observation}
\label{obs:roots-cost}
The cost of  for the roots is  at most.
\end{observation}
\vspace{0.2cm}

It is left to bound the cost that  pays for the balls in each tree (in the forest of s) as a constant function of the cost it pays for the tree root.
Specifically, we show that the sum of the radii of the s in the tree (including that of the root) is at most twice the radius of the root.
This implies the claim for the costs by Observation \ref{obser:sqr: cost Square > 14 sum radii} and Observation \ref{obser:sqr:opt geq sum rho_i}.
To show that, given any non leaf ball  (not just a root), we first analyze
only 's ``latest child'' .
That is,

We show
that the radius of the latest child is, at most, a quarter of the radius of .
(See Property (P1) of Lemma \ref{lemma:square:uncoverd req not edegs disjoint} in Subsection \ref{subsec: Formal Ana Square}.)
Second, we show that the {\em sum} of the radii of the rest of the children (all but the latest child) is, at most, a quarter of the radius of  too.
Hence, the radius of a parent ball is at least twice as the sum of its children radii.
This implies that the sum of the radii of all the s in a tree is at most twice the radius of the root.



The hardest technical part here is in the following lemma that,
intuitively, states that ``a lot of time'' (proportional to the request's radius) passes between the time one child ball ends and the time the next child ball starts, see Fig. \ref{fig:sqr:ParentAndChildrenAA}.



\begin{figure*}
\begin{center}
\includegraphics[scale=0.35]{ParentAndChildrenBody.eps}
\end{center}
\caption{\sf Geometric look on a parent  (note that a  is a triangle) and its children  and .
\label{fig:sqr:ParentAndChildrenAA}
}
\end{figure*}




\begin{lem}
Consider some uncovered request  which has at least two children.
Let ,  some two children of , such that .
Then,
. \label{lemma:sqr: tj-tk leq 4 radius k}
\label{app:lemma:sqr: tj-tk leq 4 radius k}
\end{lem}
Intuitively, the radius of a parent  is covered by the radii of its children s, plus the tails (see step SQ5) between them.
Restating the lemma,
the time of the earliest replica in  is not before the time of the latest replica in
.
Intuitively,
recall that the tail length of a request is much grater than the radius of the request's .
Hence,  the fact that the
radius of a latest child is at most a quarter of the radius of its parent, together with Lemma \ref{lemma:sqr: tj-tk leq 4 radius k},
imply that the sum of the childrenâ€™s radii is less than half of the radius of the
parent .









The full proof of Lemma \ref{lemma:sqr: tj-tk leq 4 radius k} (appears in Subsection \ref{subsec: Formal Ana Square}) uses geometric considerations.
Outlining the proof, we first establish an additional lemma.
Given any two requests  and  such that , the following lemma formalizes the following:
Suppose that the node  of request  is ``close in space (or in the network)'' to the node  of another request .
Then, the whole  of  is ``far in time'' (and later) from .

\begin{lem}
\label{lemma:suppose two con hold}
Suppose that,  and .
Then, the time of the earliest replica in  is not before the time of the latest replica in ,
i.e., .
\end{lem}
Intuitively, Lemma \ref{lemma:suppose two con hold} follows thanks to the tail left in step SQ5 of ,
as well as to the action taken in SQ3 for moving  further left of .
\noindent In the proof of Lemma \ref{lemma:sqr: tj-tk leq 4 radius k}, we show that in the case that two requests  and  are siblings,
either
{\bf (1)} they satisfy the conditions of Lemma \ref{lemma:suppose two con hold}, or
{\bf (2)} there exists some request  such that  such that   and  satisfy the conditions of  Lemma \ref{lemma:suppose two con hold}.
Moreover, the time of the last replica in  is even later then the time of the last replica in .
In both cases, we apply Lemma \ref{lemma:suppose two con hold} to show that
the time of the earliest replica in  is not before the time of the latest replica in 
as needed for the lemma.





To summarize, we show
(1) For {\em covered} requests the cost of  is  of ; see Observation \ref{obs:covered}.
(2) For {\em uncovered} requests, we prove
two facts:
(2.a) the s of the root requests are edges disjoint, and hence by
Observation \ref{obs:roots-cost}, the sum of their radii is  of  too.
(2.b) On the other hand, the sum of root's radii is at least half of the sum of the radii of all the uncovered requests.
This establishes Theorem \ref{thm: square is O(1)-approx} (which prove appears in Subsection \ref{subsec: Formal Ana Square}).


\begin{theorem}
Algorithm  is -competitive for
 under the pseudo online model.
\label{thm: square is O(1)-approx}
\end{theorem}
\def\AppSquareThm{
The ratio for covered request follows Inequality (\ref{ineq:sqr: opt geq sum cover radii}).
For uncovered requests it follows from
Observation \ref{obs:roots-distjoint} and Observation \ref{obser:sqr:opt geq sum rho_i} that

Combining this with Lemma
\ref{lema:sqr:root radi geq sum of its children}, we have,

Thus, also,

The Theorem follows from Observation
\ref{obser:sqr: cost Square > 14 sum radii}.
\QED
} 











\subsection{Formal analysis of the cost of uncovered requests}
\label{subsec: Formal Ana Square}

We start with a formal definitions of the forest of parent-child relationships.
\paragraph*{Forest of balls.}


For any uncovered request , define the following notations.
\begin{enumerate}

\item Let  be the minimal index grater than  such that   and  are not edges disjoint, if such exists, otherwise .

\item .

\item , if , otherwise .

\item .

\end{enumerate}
(We also abuse the definition and say that request  is child of request ; and  is child of , if  is child of .)


We now, state four observations about uncovered requests.
The main lemmas use these observations heavily.
Recall that,  is the closest replica of  (see SQ2).

\begin{observation}
The radius of an uncovered request is determined by the {\em time difference} from its closest replica.
That is, if a request  is uncovered, then . \label{obser:sqr: if ri is uncovered then rad(i)= ti-s'i}
\end{observation}
\proof
If  is uncovered, then .
In addition, , since  (see SQ2).
Thus,  too.
Therefore, , since  (see SQ2).
\QED


\begin{observation}
The replicas of the ``rectangle-graph'' 
are not in .
\label{obser:sqr: no replicas is in left rectangle}
\end{observation}
\proof
Assume by the way of contradiction that some replica  is in .
This implies that ,
contradicting the choice (in step SQ3) of node  of the serving replica  as the leftmost node over all replicas that are in the solution and in .
\QED





\begin{observation}
Consider some request .
Assume that its closest replica  is added to 's solution when handling request  (for some ).
Then,  (the time of the 'th closest replica is not before the time  of ).
\label{obser:sqr: closest replica is of time later than}
\end{observation}
\proof
The replica  is added to the solution in step SQ4 or in step SQ5 while  is handling request .
If  is added to the solution in step SQ4, then the replica  is added to the solution in that step too;
otherwise,  is added in step SQ5, and then a replica of  at time  (for some time ) is added to the solution.
This implies that , see step SQ2 for the selection of .
\QED



\begin{observation}
If there exists a replica  in the solution of  such that ,
then  is a covered request.
\label{obser:sqr: if soff=ti then ri is covered}
\end{observation}
\proof
By the definition,

since the distance from  to  is a candidate for .
The observation now follows from the definition of a covered request.
\QED



\subsubsection{Parent ball in tree larger then its child}

As promised (in the overview), Property (P1) of Lemma \ref{lemma:square:uncoverd req not edegs disjoint}
below implies that a parent ball in tree is at least four times larger than its ``last child''.
In fact, the lemma is more general (Property (P2) is used in the proof of other lemmas)\footnote{
Actually, this lemma shows that property for any other child too,
but for the other children this is not helpful, since there may be too many of them.
}.


\begin{lem}
Consider two {\bf uncovered} requests  and  such that .
If  and 
are not edges disjoint, then
the following properties hold.\\
(P1) ; and\\
(P2)  (
the leftmost replicas of  are on left of ;  is on the left of  and even on the left of the 'th serving replica).
\label{lemma:square:uncoverd req not edegs disjoint}
\end{lem}
\proof
We first prove Property (P2).
Since  and  are {\bf not} edges disjoint,

Since also  (see Figure \ref{fig:UnCovercase0}),

where the equality below follows from Observation \ref{obser:sqr: if ri is uncovered then rad(i)= ti-s'i},
since  is uncovered; and
the inequality holds since, on the one hand,   and  are not edge disjoint, hence has a common edge;
on the other hand, (1) for every edge in , at least one of its corresponding replicas corresponds to time strictly grater than ;
however, non of the edges of  corresponding to replicas of time strictly grater than ~.



The left inequality of Property (P2) holds by the left inequality of (\ref{ineq:claim:uncovered req: vi geq vj -radius(j)}).
The right inequality of Property (P2) holds trivially, see step SQ3.
Assume by the way of contradiction that the remaining inequality does not holds, i.e., .
Consider two cases.\\


\begin{itemize}

\item [{\bf Case 1.}] , see Figure \ref{fig:UnCovercase1}.
Then,  is in  ('s solution after handling request ).
Thus, , contradicting Inequality (\ref{ineq:claim:uncovered radius(i)> ti -tj }).

\item [{\bf Case 2.}]  , see Figure \ref{fig:UnCovercase2}.
In this case, .
By the second inequality of (\ref{ineq:claim:uncovered req: vi geq vj -radius(j)}), .
Hence, .
Again, this contradicts Inequality (\ref{ineq:claim:uncovered radius(i)> ti -tj }).

\end{itemize}

\noindent
These two cases shows that Property (P2) holds.
We next show that Property (P1) holds too.
For that, consider two cases.




\begin{itemize}

\item [{\bf Case A:}] .
In other words, the closest replica  to  is on the left of 
(see Figure \ref{fig:UnCovercase3}).
Recall that the closest replica  defines the radius  (see SQ2), i.e.,
.
We have
(the second inequality bellow follows by substituting  using the first inequality of (\ref{ineq:claim:uncovered req: vi geq vj -radius(j)})),

as needed for Property (P1).
(Intuitively, since  intersect  on the left, 's is at most  left of ;
however, we assumed that  is at least  left of ;
hence, .)




\item [{\bf Case B:}] .
We have that (see Figure \ref{fig:UnCovercase4}),

where the inequality on the right holds by Property (P2) of this lemma.
Assume that  is added to the solution when  handles some request  (for some ).
By Observation \ref{obser:sqr: closest replica is of time later than}, .
If , then , which means that 
contradicting Inequality (\ref{ineq:claim:uncovered radius(i)> ti -tj }).
Thus .
Therefore, by Observation \ref{obser:sqr: no replicas is in left rectangle},
.
However, by Inequality (\ref{ineq:sqr: vj- 5rhoj leq u'i leq uSQi}),
.
Also by Inequality (\ref{ineq:claim:uncovered radius(i)> ti -tj }), .
Hence, ,
implying . Property (P1) follows.


\end{itemize}
As showed above Property (P1) and Property (P2) hold, the lemma follows too.
\QED












\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{UnCovercase0.eps}
\end{center}
\caption{\sf
 and  are {\em not} edges disjoint implying inequalities
(\ref{ineq:claim:uncovered req: vi geq vj -radius(j)}) and (\ref{ineq:claim:uncovered radius(i)> ti -tj }).
\label{fig:UnCovercase0}
}
\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.35]{UnCovercase1.eps}
\end{center}
\caption{\sf
\label{fig:UnCovercase1}
 is on the right of  and on the left of  (case 1, ).
}
\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.35]{UnCovercase2.eps}
\end{center}
\caption{\sf
\label{fig:UnCovercase2}
 is on the right of  (case 2, ).
}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{UnCovercase3.eps}
\end{center}
\caption{\sf
\label{fig:UnCovercase3}
 is on the left of .
}
\end{figure}
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.35]{UnCovercase4.eps}
\end{center}
\caption{\sf
\label{fig:UnCovercase4}
 is below .
}
\end{figure}






\def\TestA{
\begin{figure*}
\centering
\begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\textwidth]{UnCovercase0.eps}
                \caption{}
                \label{fig:UnCovercase0}
        \end{subfigure}\begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\textwidth]{UnCovercase1.eps}
               \caption{}
                \label{fig:UnCovercase1}
        \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
                \centering
                \includegraphics[width=\textwidth]{UnCovercase2.eps}
                \caption{}
                \label{fig:UnCovercase2}
        \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
                \centering
                \includegraphics[width=\textwidth]{UnCovercase3.eps}
                \caption{}
                \label{fig:UnCovercase3}
        \end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
                \centering
                \includegraphics[width=\textwidth]{UnCovercase4.eps}
                \caption{}
                \label{fig:UnCovercase4}
        \end{subfigure}
\caption{\sf
(a)  and  are {\em not} edges disjoint implying inequalities
(\ref{ineq:claim:uncovered req: vi geq vj -radius(j)}) and (\ref{ineq:claim:uncovered radius(i)> ti -tj });
(b)  is on the right of  and on the left of  (case 1, );
(c)  is on the right of  (case 2, );
(d)  is on the left of ; and
(e)  is below .
\label{fig: Square uncovered a}
}
\end{figure*}

} 


\newpage


\subsubsection{Uncovered request has at least two children}


The previous lemma suffices for the case that an uncovered request has only one child.
We now consider the case where an uncovered request has at least two children.
We first establish Lemma \ref{lemma:suppose two con hold} (which state in the proof overview)
that deals with the case that the quarter ball of request  is later than the tail of some previous request  (for some ).
Before representing the proof of this lemma, let us make two ``geometric'' definitions.
Consider two given requests  and  such that .
Intuitively,  is {\em later} than , if the time of earliest replica of  is not before the time of the last replica of .
Formally,  is later than , if .
In addition, we say that  (which contains only replicas of ) is in the {\em range} of 
(which contains replicas of the nodes of ),
if 
(in other words,  and there exists a replica of  in ).









\newpage

Before presenting the proof of Lemma \ref{lemma:suppose two con hold}, let us remaind and a bit restate this lemma (using formal notations).



\noindent {\bf Lemma \ref{lemma:suppose two con hold}.}
{\em
Consider two requests  and  such that .
Suppose that,
 is in the range of , i.e., .
Then,  is later than . That is,

}



\noindent {\bf Proof of Lemma \ref{lemma:suppose two con hold}:}
Consider two requests  and  that satisfy the conditions of the lemma.
We begin by showing a slightly weaker assertion, that  itself is later than .
That is, .
Assume the contrary, that .
Note that
the replicas of  of time no later than  (if such do exists)
are ``candidates'' for the closest and the serving replicas of the 'th request
(since they belong to the solution ).
Thus,

That is, the inequality holds since  (see step SQ2);
the equality holds since  and .
This means that the complete 'th quarter-ball is on the right of the 'th serving replica , i.e.,

This contradicts the condition of the lemma, hence  as promised.


We now prove the lemma's assertion that .
Denote by  the latest replica in , i.e., .
Note that  is a candidate for the closest replica of the 'th request, since 
and the time of  is earlier than the time of  (i.e., ).
Thus, the radius  of the 'th request is at most as the distance between  to , see step SQ2.
That is,

In addition, by the condition of the lemma,

Thus, by Inequalities (\ref{ineq: radi j ;eq dist top tailk}) and (\ref{ineq: vj - serv k leq radi(j)})

Recall that, .
Hence, by Ineq. (\ref{ineq: vj -uell < distinf qlast, rj}),

Combining this with Ineq. (\ref{ineq: radi j ;eq dist top tailk}), we get also that .
The Lemma follows.
\QED







Now, we are ready to show the main lemma (Lemma \ref{lemma:sqr: tj-tk leq 4 radius k}), which
intuitively, shows that ``a lot of time'' (proportional to the request's radius)
passes between the time the one child ball ends and the time the next child ball starts.

We begins by remaining this lemma and restate it a bit (using formal notations).



\noindent {\bf Lemma \ref{lemma:sqr: tj-tk leq 4 radius k}.}
{\em
Consider some uncovered request  such that .
Let  such that .
Then,  is later than .
That is, . }




















\vspace{0.2cm}
\noindent{\bf Proof of Lemma \ref{lemma:sqr: tj-tk leq 4 radius k}:}
We consider two cases regarding the relation between the serving replica  of the 'th request and the node  of the 'th replica.

\vspace{0.5cm}
\noindent{\bf Case 1: } . This is the simpler case.
Apply Lemma \ref{lemma:suppose two con hold} with the requests  and .
First, note that  as required to apply Lemma  \ref{lemma:suppose two con hold}.
To use this lemma, it is also required to show that

The right inequality holds by the assumption of this case.
The left inequality holds
since

where the first inequality holds by Lemma \ref{lemma:square:uncoverd req not edegs disjoint} Property (P2) with  and ;
the second inequality holds by Lemma \ref{lemma:square:uncoverd req not edegs disjoint} Property (P2) with  and .
Thus, in this case, the lemma follows by Lemma \ref{lemma:suppose two con hold}.



\vspace{0.5cm}

\noindent{\bf Case 2:}  (that is,  is on the left of the 'th serving replica ).
Note that, unlike the previous case,  is {\em not} in the range of .
Thus, the condition of Lemma \ref{lemma:suppose two con hold} does not holds,
and we cannot apply Lemma \ref{lemma:suppose two con hold} with  and .
Fortunately, we show that in this case, we can use another request for which Lemma \ref{lemma:suppose two con hold} can be applied.
That is, we claim that
in this case, there exists  a request  that has the following three properties.
\begin{enumerate}
\item [(P1)] ;

\item [(P2)]  is in the range of  (it satisfies the condition of Lemma  \ref{lemma:suppose two con hold}); and

\item [(P3)] .

\end{enumerate}
Note that if indeed such a request  (that has the above three properties) does exists,
then
applying Lemma \ref{lemma:suppose two con hold},
we will get that

The last inequality follows from Property (P3) and since  (since ).
This will imply the lemma.
It is left to show that such a request  {\bf must} exist.
Let

and let

the index of the first request in which the solution  contains some replicas in .
Note that  is well defined since  by Lemma \ref{lemma:square:uncoverd req not edegs disjoint}, Property (P2).
We completes the proof by showing
that   exists and has  properties (P1)-(P3).
Hence, we can choose  and the lemma will follow.




\begin{enumerate}





\item {\bf  has Property (P1), i.e., .}\\
We first show that  does not contain  any replica from the rectangle graph .
That is,

Then, we show that  does contain some replicas from the rectangle graph .
That is,

Once we prove the above two inequalities, they will imply that  does exist, and in particular,  as needed.



\begin{enumerate}
\item[] {\bf Proving Ineq (\ref{ineq: Square(k) cap reck = emptyset}):}
Note that when Algorithm  handles , it does not add any replica in the above rectangle,
since it only adds replicas on the right hand side of .
(Recall that,  and we are now analysing case (2) where , i.e., .)

\vspace{1cm}
It is left to prove that  does not include a replica in .
By Observation \ref{obser:sqr: no replicas is in left rectangle},
it follows that  and  do not contain  any replica from the ``bottom part'' of ,
since

where the inequality holds since
 (by Lemma \ref{lemma:square:uncoverd req not edegs disjoint}, Property (P2)); and
 (the assumption of case (2)).





It is left to prove that  does not contain any replica from the ``top part'' of .

\vspace{0.2cm}

Assume by the way of contradiction that there exists a replica  such that .
Let  be the request in which  added  to the solution (that is, when  was handling , it added  to the solution).
The assumption that  implies that such a request  does exist, and in particular, .
Thus, , and hence,

This implies that  is added to the solution in step SQ5 and
.
Therefore, also,  (since  and ), and in particular,



\vspace{0.5cm}


In addition, , since , and also

where the first and the last inequalities hold by Lemma \ref{lemma:square:uncoverd req not edegs disjoint} Property (P2) with  and ;
the second and the third inequalities hold since ; and
the fourth inequality holds by the assumption of case (2).

\vspace{0.5cm}
Therefore, in particular, .
Thus, by Observation \ref{obser:sqr: if soff=ti then ri is covered},  is a covered request,
contradicting the assumption that  is child of  (covered requests have no parents).
Therefore,  and (as mentioned) also .
Hence, Ineq. (\ref{ineq: Square(k) cap reck = emptyset}) holds.



\vspace{0.5cm}
\item[]{\bf Proving Ineq. (\ref{ineq: Square(j-1) cap reck not= emptyset}):}
Recall that the 'th closest replica , see step SQ2.
Thus, to show that Ineq. (\ref{ineq: Square(j-1) cap reck not= emptyset}) holds, it is sufficient to show that .


The assumption that  and  are not edge disjoint implies that the 'th serving and closest replicas are on the right of .
That is,

where the first and the second inequalities hold by Lemma \ref{lemma:square:uncoverd req not edegs disjoint}, Property (P2);
the third and the forth inequalities hold by steps SQ2 and SQ3;
and the fifth inequality is the assumption in the current case (2).

This implies, in particular, that

In addition,
by Observation \ref{obser:sqr: if ri is uncovered then rad(i)= ti-s'i},
the radius of an uncovered request is the {\em time} difference between the request and its closest replica, that is,
, and equivalently

Recall that  and  are children of , thus  and  are edges disjoint.
This, together with inequalities (\ref{ineq: uclose in the range of (vi,vj]}) and (\ref{ineq: rhosq j = tj -s'j})
imply that

Hence,  by inequalities (\ref{ineq: uclose in the range of (vi,vj]}) and (\ref{ineq: sclosej geq tk}) and since .
Thus, Ineq. (\ref{ineq: Square(j-1) cap reck not= emptyset}) holds as promised.

\end{enumerate}



We have shown that inequalities (\ref{ineq: Square(k) cap reck = emptyset}) and (\ref{ineq: Square(j-1) cap reck not= emptyset}) hold
as we argued above
this implies that  has Property (P1).











\item {\bf  has Property (P2), i.e.,  is in the range of .}
Recall that ; and  and  are not edge disjoint, thus by Lemma \ref{lemma:square:uncoverd req not edegs disjoint}, Part 2,

We show
that

which implies together with Ineq. (\ref{ineq:vj-rho j < vi < vj}) that
 as needed (for showing that  is in the range of ).

It remains to show that Ineq. (\ref{ineq:vi < uSQell vi < vj}) holds.
Note that, on the one hand,
the choice of  (as the first request which the solution  contains a replica in ) implies that
some replica  is added to the solution when  handles .
On the other hands, when Algorithm  handles ,
it only adds replicas (in steps SQ4 and SQ5) to the right of  and to the left of .
Thus, on the one hand, , and on the other hand, .
Hence, also




This already establish the right inequality of (\ref{ineq:vi < uSQell vi < vj}).
To show that its left inequality holds too,
assume toward contradiction that .
Combining this with the left inequality of (\ref{ineq: two inequalities}), we have

This implies that, when Algorithm  handles ~, it added the replica , in step SQ4 to the solution.
Hence, ,
and is a candidates for the 'th close replica (see step SQ2).
Thus,

Hence, the time of each of 's replicas is at least .
Recall that,  (since );
and that in each edge  of  at least one of 's endpoints is corresponds to time later  than .
Therefore,  and  are edge disjoint,
which contradicts the choice of  as a child of .
Hence, ,  Ineq. (\ref{ineq:vi < uSQell vi < vj}) holds and  maintains Property (P2) as promised.










\item {\bf  has Property (P3), i.e., .}\\


We first show that the time  of the serving replica  of the 'th request is before .
That is,

The choice of  implies that .
On the other hand, the serving replica  does belong to  (see step SQ3).
This implies, in particular, that

Recall that  by Ineq. (\ref{ineq:vi < uSQell vi < vj}),
hence .
Inequality (\ref{ineq; sSQ ell* < tk -5rhotk}) holds, since .

Summarizing what we know so far,   and .
Thus, on  one hand,

On the other hand,
 (see step SQ3),
which implies that


Inequalities (\ref{ineq: distinf qSQell* geq 5 rho(k)}-\ref{ineq: distinf qSQell* leq 5 rho(l)}), imply that  as needed.
Hence,  maintains Property (P3).


\end{enumerate}

\noindent We have shown that  maintains the three properties, implying the lemma for case (2) too.
[Lemma \ref{lemma:sqr: tj-tk leq 4 radius k}]
\QED









The previous lemma shows that a lot of time pass between the time of the last replica in the quarter ball of a child and the time of first replica in the quarter ball of the next child.
The next, lemma use this property to show that the radius of a root is at least half of the sum of the radii of its children in its tree.



\begin{lem}
Consider some root request .
Then,

\label{lema:sqr:root radi geq sum of its children}
\end{lem}
\proof
We begin by showing that the radius of each ball  in the tree is at least, twice the sum of the radii of its children.
Consider some non leaf request  (that is, ).
Let us first, show that

If  ( has exactly one child), then (\ref{ineq:sqr: rhoSQ(i) geq 2sum rhoSQ(j)}) follows from
property (P1) of Lemma \ref{lemma:square:uncoverd req not edegs disjoint}.
Otherwise, , where  and .
(For simplicity, to avoid double subscripts, we may write  instead of .)
By Lemma \ref{lemma:sqr: tj-tk leq 4 radius k} with  and , it follows that

for every .
Now, (see Figure \ref{fig:sqr:ParentAndChildren})

where the first inequality holds since the  and  are {\em not} edges disjoint;
the second inequality holds by Inequality (\ref{ineq:sqr: t(jl) + 4 rad(jl) + rad}), since .
\begin{figure*}
\begin{center}
\includegraphics[scale=0.35]{ParentAndChildren.eps}
\end{center}
\caption{\sf Geometric vision on a parent and its children relationships.
\label{fig:sqr:ParentAndChildren}
}
\end{figure*}
In addition, by Property (P1) of Lemma
\ref{lemma:square:uncoverd req not edegs disjoint},

which implies Inequality (\ref{ineq:sqr: rhoSQ(i) geq 2sum rhoSQ(j)})
that implies the lemma.
\QED




So far, we have shown that
(1) the quarter-ball of the covered requests are edges disjoint;
(2) the quarter-ball of the root requests are edges disjoint, and hence by Observation \ref{obser:sqr: cost Square > 14 sum radii} and Observation \ref{obser:sqr:opt geq sum rho_i}, the sum of their radii of the covered request and the root requests is no more than 28 times the cost of .
On the other hand,
the sum of root's radii is at least half of the sum of the radii of the uncovered requests.
This, in fact, establishes Theorem \ref{thm: square is O(1)-approx}.


\noindent {\bf Proof of Theorem \ref{thm: square is O(1)-approx}:}
\AppSquareThm






\section{Algorithm  - the ``real'' online algorithm}
\label{subsec: Algorithm Donline}








In this section, we transform the pseudo online algorithm  of Section \ref{sec:square} into a (fully) online algorithm   for \footnote{
We comment that it bears similarities to the transformation of the pseudo online algorithm Triangle to a (full) online algorithm for {\em undirected}  in \cite{KK2014}. The transformation here is harder, since there the algorithm sometimes delivered a copy to a node  from some node on 's right, which we had to avoid here (since the network is directed to the right).
}.
Let us first give some intuition here.

The reason Algorithm  is {\em not} online, is one of the the actions it takes at step SQ4.
There, it stores a copy at the serving replica   for request  from time  to time .
This requires ``going back in time'' in the case that the time .
A (full) online algorithm cannot perform such an action.
Intuitively, Algorithm  ``simulates'' the impossible action by
(1) storing additional copies (beyond those stored by ); and
(2) shifting the delivery to request  (step SQ4 of ) from an early time to time  of .
It may happen that the serving node  of  does not have a copy (in ) at .
In that case, Algorithm  also (3)
delivers first a copy to  from some node  on the left of .
Simulation step (1) above (that we term the storage phase) is the one responsible for ensuring that such a node  exists, and is ``not too far'' from .

For the storage phase, Algorithm  covers the network by ``intervals'' of various lengthes (pathes that are subgraphs of the network graph).
There are overlaps in this cover, so that each node is covered by intervals of various lengthes.
Let the length of some interval  be .
Intuitively, given an interval  and a time ,
if  kept a copy in a node of interval  ``recently'' (``recent'' is proportional to ),
then  makes sure that a copy is kept at the left most node of this interval, or ``nearby''
(in some node in the interval just left to ).


Now, we (formally) illustrated Algorithm . We begins by giving some definitions.










\paragraph*{\bf Partitions of  into intervals\commsingle.\commsingleend}
Consider some positive integer  to be chosen later.
For convenience, we assume that  is a power of .
(It is trivial to generalize it to other values of .)
Define  {\em levels} of partitions of the interval .
In level , partition  into  intervals,
, ,...,, each of size .
That is,
,
for every  and every .
Let  be the set of all such intervals.
When it is clear from the context, we may omit  from  and  and write  and , respectively.
Let  be the {\em level} of an interval , i.e., .
For a given interval ,
denote by , for  the {\em neighbor} interval of level  that is on the left of .
That is, . Define that . Let

We say that  is the {\em neighborhood } of .



Denote by  (for every node  and every level ) the  interval in level  that contains .
That is,
.
The {\em neighborhood}  of a node  contains all those nodes in the neighborhood 
(of the interval of level  of ) that are left of .
That is, .


\vspace{-0.3cm}
\paragraph*{\bf Active node\commsingle.\commsingleend}
\label{subsection: def delta active intervals}
Consider some node , some level .
Node  is called {\em -active} at time ,
if
.
Intuitively,  Algorithm 
kept a movie copy in , at least once, and ``not to long'' before time .
We say that
 is {\em -}, intuitively, if  is {\bf not} ``just about to stop being -active'',
that is, if .






Let us now construct , the set of replicas corresponding to the nodes that store copies from time  to time  in a  execution.
Let .
(The algorithm will also leave a copy in  always.)
To help us later in the analysis, we also added an auxiliary set
.
Initially, .
For each time , consider first the case that there exists at least one request corresponding to time , i.e., .
Then, for each request ,
 simulates  to find the radius  and the serving node  of the serving replica  of .
Unfortunately, we may not be able to deliver, at time , a copy from  may be .
Hence,  delivers a copy to  via  (this is called the {\em ``delivery phase''}).
That is, for each  do:
\begin{itemize}
\item[(D1)] choose a closest (to ) replica 
on the left of   of time  already in the solution;

\item[(D2)] add the path 
to the solution.

\end{itemize}
Let .
(Note that  is served from , after that, the path  is added; and  is served from , etc.)


Recall that before the delivery phase, the replicas of  have copies.
It is clear, that the delivery phase of time  ensures that the replicas of  have copies too.
That is, at the end of the delivery phase of time , at least the replicas of  have copies.
It is left to decide which of the above copies to leave for time .
That is (the {\em ``storage phase''}),
 chooses the set
.
Initially,  (as we choose to leave copy at the replicas of the tails and to leave a copy at  always).
Then, for each level , in an {\em increasing} order, the algorithm goes over and
each node , in an {\em increasing} order, selects as follows.
\begin{itemize}
\item[(S1)] Choose a node  such that
(1)  is level - at ; but
(2) no replica has been selected in level  's neighborhood ().
If such a node  does exist, then perform steps (S1.1--S1.3) below.

\item[(S1.1)] Add the tuple  to the auxiliary set ;
we say that the interval  {\em commits} at level  at time .

\item[(S1.2)]
Select a node  such that a replica of  at time  is in
 (by Observation \ref{obser:Dlinoon: well defined} below, such a replica does exist,
recall that all these replicas have copies at this time).
\item[(S1.3)] Add   to  and add the arc  to the solution.

\end{itemize}

The solution constructed by  is denoted , where

represents the horizontal edges added in the delivery phases and

represents the arcs added in the storage phase.


\begin{observation}
{\sc (``Well defined'').}
If a node  is level - at time ,
then there exists a replica  such that . \label{obser:Dlinoon: well defined}
\end{observation}
\def\AppDlineonObserWellDefined{
\proof
Consider some node  and a time . If - at time ,
then either
 or  and
 is also - at time  (and
);
hence, .
The observation follows.
\QED
} \AppDlineonObserWellDefined

Moreover, a  node  has a copy in its neighborhood longer (for an additional round).



\begin{observation}
{\sc (``A -active node has a near by copy'').}
If a node  is - at time , then, either
(1) ,
or
(2) .
\label{obser:Dlineon: a l active has a near by copy}
\end{observation}
\def\AppObserDlineonActiveNearByCopy{
\proof
Consider a node  that is -  at time .
If , then the observation follows.
Assume that .
Then, the fact that  is - at ,
but , implies also, that  is - at time .
Thus,  either (1)  commit at  (at step (S1.1)) which ``cause'' adding an additional replica to  from  (at step (S1.2));
or (2)  does not commit at , since  has, already, a replica from .
\QED
} \AppObserDlineonActiveNearByCopy



\begin{observation}
{\sc (``Bound from above on '').}
.
\label{obser:Dlineon: |Commit|=|Aon-0|}
\end{observation}
\def\AppObserDlineonBoundFromAboveonCommit{
\proof
Let
.
Now we prove that .
Every arc in  (that add at step (S1.3)) corresponds to exactly one tuple  of an interval  that commits at time  (in step (S1.1));
and every interval commits at most once in each time  that corresponds to exactly one additional arc in .
Thus, .
The observation follows.
\QED
} \AppObserDlineonBoundFromAboveonCommit


\vspace{-0.5cm}
\paragraph*{\bf Analysis of \commsingle.\commsingleend}
We, actually, compare the cost of Algorithm  to that of the pseudo online Algorithm .
The desired competitive ratio for  will follow, since we have shown that  approximates the optimum (Theorem \ref{thm: square is O(1)-approx}).
A similar usage of a (very different) pseudo online algorithm utilized in \cite{KK2014}.
\commlong

\commlongend
\commshort

\commshortend
This implies the desired competitive ratio of  by
Theorem \ref{thm: square is O(1)-approx}.
We first show,
that the number of horizontal edges in  ({\em``delivery cost''}) is .
Then, we show,
that the the number of arcs in  ({\em``storage cost''}) is .
Optimizing , we get a competitiveness of .

\commsingle
\commsingleend
\vspace{-0.2cm}
\paragraph{\bf Delivery cost analysis.}



For each request , the delivery phase (step (D2)) adds 
to the solution.
Define the {\em online} radius of  as .
We have,
\vspace{-0.3cm}

It remains to bound  as a function of  from above.
Restating Observation \ref{obser:Dlineon: a l active has a near by copy} somewhat differently
we can use the distance
 (see (SQ3)) and the time difference  for bounding .
That is, we show that  has a copy at time  (of ) at a distance at most  from  (of  of ).
Since, ,  has a copy at distance at most  from  (of ).




\vspace{0.2cm}
\begin{lem}
.
\label{lemma:Dlineon: delivery cost}
\end{lem}
\def\AppLemmaDlineonDevCost{
\proof
The following claim
restating Observation \ref{obser:Dlineon: a l active has a near by copy} somewhat differently and
help us to prove that the serving replica has a ``near by'' copy.

\vspace{0.2cm}
\begin{claim}
Consider some base replica  and some , such that, .
Then, there exists a replica  such that . \label{claim: dist(v, C t+rho) leq 2 delta rho}
\end{claim}
\begin{proof}
Assume that .
Consider an integer .
Let .
Node  is - at time .
Thus, by Observation \ref{obser:Dlineon: a l active has a near by copy},
there exists some node 
that keep a copy for time .
That is, a replica  does exists.
The fact that  implies that .
The claim follows, since .
\end{proof}
\QED


Recall that  serves request  from some base replica  already include in the solution.
That  may correspond to some earlier time. That is, .
In the case that ,  can serve  from .
Hence, .
In the more interesting case, .
By Claim \ref{claim: dist(v, C t+rho) leq 2 delta rho}
(substituting , , and ),
there exists a replica 
such that .
Recall that  (see (SQ3)).
Thus, .
Hence,  as well.
\QED
} \AppLemmaDlineonDevCost





\noindent The following corollary holds, by combining together the above lemma  with Inequality (\ref{Ineq: |Hon| leq sum Dron(i)}).



\begin{corollary}
.
\label{corollary:Dlineon: delivery cost}
\end{corollary}


\vspace{-0.3cm}
\paragraph*{\bf Analysis of the storage cost\commsingle.\commsingleend}



By Observation \ref{obser:Dlineon: |Commit|=|Aon-0|},
it remains to bound the size of  from above.
Let  if  (otherwise 0).
Hence,
.
We begin by bounding the number of commitments in  made by nodes for level .
Observation \ref{obser:Dlineon Base acounts level l=0 committments} below follows directly from the definitions of  and .

\vspace{0.2cm}
\begin{observation}

\label{obser:Dlineon Base acounts level l=0 committments}
\end{observation}
\proof
Consider some commitment , where interval  is of level .
Interval  commit at time  only if there exists a node  such that  is - at  (see step (S1) in ).
This  status at time  occur only if .
Hence, each base replica causes at most one commitment at  of one interval of level .
\QED


The following lemma is not really new.
The main innovation of the paper is the special pseudo online algorithm we developed here.
The technique for simulating the pseudo online algorithm by a ``true'' online one, as well as the following
analysis of the simulation, are not really new.
For completeness we still present a (rather detailed) proof sketch for Lemma \ref{lem:Dlineon: storage cost < Hoff + Aoff}.
Its more formal analysis is deferred to the full paper
(and a formal proof of a very similar lemma for very similar mapping of undirected )
can be found in Lemma 3.8 of \cite{KK2014TR}.


\vspace{0.2cm}
\begin{lem}
.
\label{lem:Dlineon: storage cost < Hoff + Aoff}
\end{lem}
\noindent{\bf Proof sketch:}
The  term in the statement of the lemma follows from Observation \ref{obser:Dlineon Base acounts level l=0 committments} for commitments of nodes for level .
The rest of the proof deals with commitments of nodes for level .

Let us group the commitments of each such interval (of level ) into {\em ``bins''}.
Later, we shall ``charge'' the commitments in each bin on certain costs of the pseudo online algorithm .
Consider some level  interval  an input .
We say that  is a {\em committed-interval} if  commits at least once in the execution of  on .
For each committed-interval  (of level ),
we define (almost) non-overlapping {\em``sessions''}
(one session may end at the same time the next session starts;
hence, two consecutive sessions may overlap on their boundaries).
The first session of
 does {\em not} contain any commitments (and is termed an {\em uncommitted-session}); it begins at time  and ends at the first time that  contains some base replica.
Every other session (of ) contains at least one commitment (and is termed a {\em committed-session}).


Each commitment (in ) of  belongs to some committed session.
Denote by  the leftmost node in , i.e., .
Given a commitment  that  makes at time ,
let us identify 's session. Let  be the last time (before ) there was a base replica in .
Similarly, let  be the next time (after ) there will be a base replica in 
(if such a time does exist; otherwise, ).
The session of commitment  starts at  and ends at .
Similarly, when talking about the 's session of interval , we say that the session starts at 
and ends at .
When  is clear from the context, we may omit and write , .
A bin is a couple  of a committed-interval and the th commitment-session of .
Clearly, we assigned all the commitments (of level  intervals) into bins.



Before proceeding, we claim that the bins indeed do not overlap (except, perhaps, on their boundaries).
This is because the boundaries of the sessions are times when  has a  replicas.
At such a times ,  does not commit.
This is because the pivot of  is - at  and hence keeps a copy.
On the other hand,  is of higher level
(we are dealing with the case of );
hence, it is treated later by the algorithm (see step (S1)).
Hence,  indeed does not commit at .
Therefore, there is no overlap between the sessions, except the ending and the starting times.
That is, ,
where  is the number of bins that  has.


Let us now point at costs of algorithm  on which we ``charge'' the set of commitments  in bin  for the th session of .
We now consider only a bin  whose  committed session  is not the last.
Note that the bin corresponds to a rectangle of  by  replicas.
Expand the bin by  replicas left, if such exist.
This yields the {\em payer} of bin ; that is the payer is a rectangle subgraph of 
by  replicas.
We point at specific costs  had in this payer.


Recall that every non last session of  ends with a {\em base} replica in , i.e., .
The solution of  contains a route ( route) that starts at the root and reaches

by the definition of a base replica.
For the charging, we use {\em some} (detailed below) of the edges in the intersection of that  route and the payer rectangle.


The easiest case is that the above  route enters the payer at the payer's bottom () and stays in the payer until .
In this case ({\bf EB}, for Entrance from Below), each time () there is a commitment in the bin,
there is also an arc  in the  route (from time  to time ).
We charge that commitment on that arc .
The remaining case ({\bf SE}, for Side Entrance) is that
the  route enters the payer from the left side of the payer.
(That is,  delivers a copy to  from some other node  outside 's neighborhood,
rather than stores copies at 's neighborhood from some earlier time).
Therefore, the route must ``cross'' the left neighbor interval of  in that payer.
Thus, there exists at least  horizontal edges in the intersection between the payer (), of   and
the  route.

Unfortunately, the number of commitments in bin  can be much grater than .
However, consider some replica , where  is the last time there was a base replica in  at its 'th session.
The number of commitments in bin  corresponding to the times {\em after}  is  at most.
(To commit, an interval must have an active node; to be active, that node needs a base replica in the last  times.)
The commitments of times  to  are charged on the horizontal edges in the intersection between

and 's route that reach .
Recall that, on the one hand, there are  commitments at most in bin  corresponding to times .
On the other hand, there exists at least  horizontal edges in the intersection between  route and .

We charge the commitments of times  to  on the arcs
in the intersection between the payer (), of   and
the 's route that reaches .
(The route of  that reach  must contain an arc 
in  for every time ;
this implies that in each time () there is a commitment in the bin,
there is also an arc  in  solution (from time  to time );
we charge that commitment on that arc .)



For each interval , it is left to account for commitments in 's last session.
That is, we now handle the bin  where  has  commitment-sessions.
This session may not end with a base replica in the pivot of , so we cannot apply the argument above
(that  must have a route reaching the pivot of  at ).
On the other hand, the first session of  (the uncommitted-session) does end with a base replica in , but has no commitments.
Intuitively, we use the payer of the first session of  to pay for the commitments of the last session of .
Specifically, in the first session, the  route must enter the neighborhood of  from the left side;
Hence, we apply the argument of case SE above.

To summarize,
{\bf (1)} each edge that belongs to 's solution may be charged at most once to each payer that it belongs too.
{\bf (2)} each edge belongs to  payers at most (there are  levels;
the payer rectangle of each level is two times wider than the bins; two consecutive sessions may intersect only at their boundaries)\footnote{
Note that, unlike the analysis of  for undirected line network \cite{KK2014,KK2014TR},
we don't claim that each arc is charged just for constant number of times.
}.
This leads to the term  before the  in the statement of the lemma.
\QED




We now optimize a tradeoff between the storage coast and the delivery cost of .
On the one hand, Lemma \ref{lem:Dlineon: storage cost < Hoff + Aoff} shows that a large 
reduces the number of commitments.
By Observation \ref{obser:Dlineon: |Commit|=|Aon-0|}, this means a large  reduces the storage cost of .
On the other hand, corollary \ref{corollary:Dlineon: delivery cost} shows that a {\em small}  reduces the delivery cost.
To optimize the tradeoff (in an order of magnetite), fix .
Thus, .
Corollary \ref{corollary:Dlineon: delivery cost},
Lemma \ref{lem:Dlineon: storage cost < Hoff + Aoff} and Observation \ref{obser:Dlineon: |Commit|=|Aon-0|}
imply that .
Thus, by Theorem \ref{thm: square is O(1)-approx}, we
have the proof of the following theorem.

\begin{theorem}
Algorithm
 is -competitive for
 problem.
\label{thm: Dlineon is frac(log n)(log log n) competitive}
\end{theorem}



\vspace{-0.3cm}
\section{Optimal algorithm for  and for }
\label{subsec: optRSA}


\vspace{-0.2cm}

\noindent Algorithm  in Section \ref{subsec: Algorithm Donline} solves .
To solve also , we transform Algorithm  to an algorithm  that solves .
First, let us view the reasons why the  solution for  (Section \ref{subsec: Algorithm Donline}) does not yet solve .
In ,  the  coordinate of every request (in the set ) is taken from a known set of size  (the network nodes ).
On the other hand, in , the  coordinate of a {\em point} is arbitrary.
(A lesser obstacle is that the  coordinate is a real number, rather than an integer.)
The main idea is to make successive guesses of the number of Steinr points and of the largest  coordinate and solve under is proven wrong
(e.g. a point with a larger  coordinate arrives) then readjust the guess for future request.
Fortunately, the transformation is exactly the same as the one used in \cite{KK2014TR,KK2014} to transform the algorithm for undirected  to solve .
For completeness, we nevertheless present the transformation here.

















\subsection{Proof Outline}
\label{App;subsec:Proof Outline}
The following outline is taken (almost) word for word from \cite{KK2014}.
(We made minor changes, e.g. replacing the word  by the word ).

First, let us view the reasons why the  solution for  (Section \ref{subsec: Algorithm Donline}) does not yet solve .
In ,  the  coordinate of every request (in the set ) is taken from a known set of size  (the network nodes ).
On the other hand, in , the  coordinate of a {\em point} is arbitrary.
(A lesser obstacle is that the  coordinate is a real number, rather than an integer.)
The main idea is to make successive guesses of the number of Steinr points and of the largest  coordinate and solve under is proven wrong
(e.g. a point with a larger  coordinate arrives) then readjust the guess for future request.
Let us now transform, in three conceptual stages,  into an optimal algorithm for the online problem of :
\vspace{-0.2cm}
\begin{enumerate}
     \item Given an instance of , assume temporarily (and remove the assumption later) that the number  of points is known, as well as , the maximum  coordinate any request may have. Then, simulate a network where  and , and the  nodes are spaced evenly on the interval between  and . Transform each  request to the nearest grid point. Solve the resulting  problem.

     \item Translate  these results to results of the original
            instance.

\item  Get rid of the assumptions.
\end{enumerate}
\vspace{-0.2cm}
The first stage is, of course, easy. It turns out that ``getting rid of the assumptions'' is also relatively easy.
To simulate the assumption that  is known, guess that  is some .
Whenever a guess fails, (a request  arrives, where ), continue with an increased guess .
A similar trick is used for guessing . In implementing this idea, our algorithm turned out paying a cost of .
(This is  per failed guess, since each application of  to a new instance, for a new guess, starts with delivering a copy to every node in the simulated network; see the description of Algorithm .) On the other hand, an (optimal) algorithm that knew  could have paid  only once.
IF  is ``sufficiently'' larger than , then .






The second stage above (translate the results) proved to be somewhat more difficult, even in the case that  and  are known (and even if they are equal).
Intuitively, following the first stage, each request  is inside a grid square. The solution of  passes via a corner of the grid square. To augment this into a solution of , we need to connect the corner of the grid square to
 . This is easy in an offline algorithm. However, an online algorithm is not allowed to connect a point at the top of the grid square (representing some time t) to a point somewhere inside the grid square (representing some earlier time ).

Somewhat more specifically,
following the first stage,
each request  is in some grid square, where the corners of the square are points of the simulated  problem. If we normalize  to be , then the left bottom left corner of that square is .
Had we wanted an  {\em offline} algorithm, we could have solved an instance of , where the points are  .
Then, translating the results of  would have meant just augmenting with segments connecting each  to .
Unfortunately, this is not possible in an {\em online} algorithm, since   is not yet known at .
Similarly, we cannot use the upper left corner of the square (for example) that way, since at time , the algorithm may no longer be allowed to add segments reaching the earlier time .



\subsection{Informal description of the transformed {\bf\em RSA} algorithm assuming   and  and  is known}
\label{subsec: onRSAn}


The algorithm under the assumptions above appears in Figure \ref{figure: onRSAn}.
Below, let us explain the algorithm and its motivation informally.

When describing the solution of , it was convenient for us to assume that the network node were .
In this section (when dealing with ),
it is more convenient for us to assume that  solves  with the set of network nodes being .
Clearly, it is trivial (though cumbersome) to change  to satisfy this assumption.

Assume we are given a set of points  for .
We now translate  points to  requests (Fig. \ref{fig: pointsTogrid}).
That is, each point  that is not already on a grid node, is located inside some square whose corners are the grid vertices.
We move point  to the grid vertex (replica)  on the left top corner of this square.
That is, we move  (if needed) somewhat later in time, and somewhat left on the  axis.
We apply  to solve the resulting .
This serves  from some other replica , where  may be slightly later than the time  we must serve .
After  solves the  instance, we modify the  solution to move the whole horizontal route
 of request 
(route from  to 
somewhat earlier in time (from time  to time ).
This now serves a point , where  may be slightly left of .
Hence,
we extend the above horizontal route
by the segment from  to .
In addition,
the transformed algorithm leaves extra copies in every network node along the route , until time  (see Fig. \ref{fig: onRSAmn execution}(d));
a little more formally, the algorithm adds to the solution of  the
vertical line segment

(a vertical segment between the points  and ),
for every  such that .







\def\FigpointTogrid{
\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.45]{pointTogrid.eps}
\end{center}
\caption{\sf Point  is transformed upward and leftward to ;  is transformed upward and  is is transformed leftward;  the points transform to the same vertex point.
\label{fig: pointsTogrid}}
\end{figure}
} \FigpointTogrid

There is a technical point here:
 had a copy in  and we need a ``copy''  in  where .
That is, we need that the solution of  problem will already includes .

\begin{observation}
The solution of  problem already includes .
\label{obser:SRSA: (u,y) in the solution}
\end{observation}
\def\AppObserSRSAuyInTheSolution{
\proof
To make sure such a copy in  does exist, let us consider the way the copy reached  in .
If  stored a ``copy'' in  from time  to 
(see Fig. \ref{fig: onRSAmn execution}(c)
),
then also  belong to the solution.
Otherwise,  moved the copy to  over a route

from some other grid vertex .

Note that  appeared in the transformed algorithm because that algorithm served a point
, of a time

(see Fig. \ref{fig: onRSAmn execution}(d)).
The transformed algorithm moved this route 
earlier in time to
 and left copies in those network node until time  (see Fig. \ref{fig: onRSAmn execution}(d)).
In particular, it leave a copy also in  from time  to time , hence  is already in the solution of the transform algorithm.
\QED
} \AppObserSRSAuyInTheSolution



So far, we described how to transform the delivery phase of . The storage phase of  does not need to be transformed.
(Actually,  even has some minor extra difficulty that does not exist in ;
consider some request  in , and suppose that the
next request  is at time ; then time  arrives, and  must make some decisions, without knowing that the next request will be at time ; then time  arrives, etc; no such notion of time passing (without new points arriving) exists in the definition of ; that is, the  coordinate  of the next request  is known right after the algorithm finished handling ; the storage phase of the transformed algorithm does not make any use of this extra freedom in 
and simulates the ``times'', or the  coordinates, one by one; note that for that purpose, the transformation of the delivery phase ensured the following property: that if a copy in
 exists in a replica  in , this replica also contains a copy in the transformed algorithm.)
Denote the solution of  on  by .
For the pseudo code, see Fig. \ref{figure: onRSAn}.

\def\FigCodeOnRSAn{
\begin{figure}[ht!]
\fboxsep=0.2cm
\framebox[\textwidth]{
\begin{minipage}{0.8\textwidth}


\begin{enumerate}

\item For  do:
\begin{enumerate}
\item compute the translated request  of ; .
    \item ;
\end{enumerate}


\item For each point  do:

\begin{enumerate}

\item compute the translate request  of ;
\item .

\item \underline{``Vertical phase''}
    \begin{enumerate}

    \item If , then for each time  do:

        \begin{enumerate}
        \item ``Simulate''  on  to find .



        \item .


        \end{enumerate}
    \end{enumerate}

\item \underline{``Horizontal phase''}

    \begin{enumerate}
    \item ``Simulate''  on  to find .\\



    \item .


    \item 

\item .

    \end{enumerate}

\end{enumerate}
\item Return 
\end{enumerate}
\end{minipage}
}
\caption{\label{figure: onRSAn}
Subroutine  assumes the knowledge of  and that  and .
}
\end{figure}

} \FigCodeOnRSAn



\paragraph*{\bf Analysis sketch of the transformed algorithm with known parameters\commsingle.\commsingleend}



It is not hard to see that an optimal solution for that instance of  is ``not that far'' from an optimal solution of the original instance of .
To see that, given an optimal solution of , one can derive a feasible solution of the resulting  by adding 2 segments of length at most   for each point . (One vertical such segment plus a horizontal one are enough to connect a point  to the replica
 where we moved ).
 The total of those distances is  at most.
On the other hand, an optimal solution of  would need to pay at least .
Hence, an optimal solution for  would have implied a constant approximation of .
Intuitively, an approximation (and a competitive ratio) for  implies an approximation (and a competitive ratio) of  in a similar way.
For a given Algorithm  for  and a set  of input points, let  be the cost of  on .
Let  be an optimal algorithm for .

\begin{lem}
Assume that  and .
Then, .
If also  and , then  is  -competitive for .
\label{lemma: onRSAn is O(sqrt log n)-competitve}
\end{lem}




\def\ProofLEMMAonRSAnLOGcomp{
\proof
It is easy to verify that  computes a feasible solution (see the ``technical point'' comments in parentheses in section \ref{subsec: onRSAn}).
Consider some input point set  such that 
and .
Let  be the translated instance of the  problem.


Recall how does  translate the solution of
\commdouble\\\commdoubleend
.
An horizontal edge  (that  add to its solution when handling request , see step (D2) in )
is translated into a horizontal line segment .
An arc  (of 's solution on ) is translated into a vertical line segment .
Hence, the total cost of those parts of the solution of  is exactly the same as the cost of the solution of .



Thus, the cost of  on  differ from the cost of  on  only by
two kinds of ``short'' segments (Segment of length at most 1).
For the first kind, recall (technical point in Section \ref{subsec: onRSAn})
that for every moved horizontal path ,
 added a short vertical segment for every network node  of that path from  to .
The second kind of addition is an horizontal short segment connecting the input point  to , where .



The total cost of the second kind is bounded by , since .
We claim that the total cost of the short segment of the first kind is  at most.
To see that, notice that we have at most 1 such ``short'' segment (shorter than 1) per replica that appears in the solution of  on .
That solution of  contains at least as many edges as it contains replicas.
Formally, the cost of  is at most,
\commsingle

\commsingleend
\commdouble

\commdoubleend
Thus, by Theorem \ref{thm: Dlineon is frac(log n)(log log n) competitive},

where  is some constant.


Let us look the other direction, from an {\em optimal} solution of  for  to optimal solution of  for .
Recall that  can be served from  at a cost of 2 (at most).
Hence,

Thus, by Inequalities (\ref{ineq: c(onRSAn, Q) leq c1 sqrt log c(opt,R)+n}) and (\ref{ineq: cost opt Q geq cost Z' -2M size/ netsize}),
\commsingle

\commsingleend
\commdouble

\commdoubleend
The first statement of the lemma holds.
Now, let us prove the second statement of the lemma.
Assume that  and .
Thus also,

Therefore, by Inequalities (\ref{ineq: c(onRSAn Q) leq O(sqrt log n) (c(opt Q)+n)}) and (\ref{ineq: cost opt Q geq 2/M}),
\commsingle

\commsingleend
\commdouble

\commdoubleend
The lemma follows, since .
\QED
}\ProofLEMMAonRSAnLOGcomp


Below,   is used as a module in another algorithm, responsible for implementing the assumptions.
In each execution of the other algorithm,  is invoked multiple times, for multiple subsets of the input.
Unfortunately, not every time, the other algorithm uses , all the assumptions are ensured.
This is the reason of the ``extra'' factor  in the first part of the above lemma above.
Fortunately, these extra factors of all the invocations are bounded separately later.



\def\FigonRSAnEXE{
\begin{figure}
\begin{center}
\includegraphics[scale=0.4]{onRSAmnA.eps}
\hfill
\includegraphics[scale=0.4]{onRSAmnB.eps}
\hfill
\includegraphics[scale=0.4]{onRSAmnC.eps}
\hfill
\includegraphics[scale=0.4]{onRSAmnD.eps}
\end{center}
\caption{\sf Example of execution of .
(a) 's solution after handling point ;
(b)  simulates the storage phase of  on  for times ;
(c)  handles point , moves  from ``time''  to ``time''  (it serves this path from , who ``receives a copy'' when  handles time  in the storage phase),
and ``leaves copies'' at the nodes  from ``time''  to ``time'' ;
(d)  handles point ,
 moves  from ``time''  to ``time''  (it serves this path from , who ``receives a copy'' when handling point ), and
 ``leaves  copies'' at the nodes of  from ``time''  to ``time'' .
\label{fig: onRSAmn execution}}
\end{figure}
} \FigonRSAnEXE





\subsection{Getting rid of the assumption that {\bf\em M=N}}
\label{subsec: onRSAmn}
We now describe an online algorithm  that is somewhat more general than . Algorithm  is {\em not} based on the assumption that the upper bound  on  is also the number of points.
That is, we now do {\em not} assume that . Getting rid of this assumption is straightforward.
The new online algorithm  transforms the  coordinate of each input point to the interval . Algorithm  passes the transformed point to the online algorithm  of Section \ref{subsec: onRSAn} that is assumed to be executing in parallel. The transformation of a point is, though, a little more involved, as detailed below.

Later on (in Section \ref{subsec: onRSA}),  will be used by an even more general algorithm in a similar way. For that, it is more convenient for us to define algorithm  a somewhat more general algorithm then is needed by the description so far.
We now assume that the origin is not necessarily , but is rather some .
(Meanwhile, we still assume that ).
 Hence, algorithm  translates the  coordinate of each input point  to .
To keep the proportion between the axes, the  coordinate  is translated to
. (Recall that .)
Finally, the solution of  is translated back to the coordinates of  applying the transformation  to every point of the solution.
(Clearly, this is a polynomial task, since the solution is described using a polynomial number of points).
The pseudo code appears in Fig. \ref{fig:middle-alg}.
By Lemma \ref{lemma: onRSAn is O(sqrt log n)-competitve} and the description of , it is easy to see the following.

\begin{observation}
Assume that  and .
Then, .
If also  and , then  is
\commdouble\\\commdoubleend
-competitive for
.
\label{corollary: onRSAmn is O(sqrt)-cometitve}
\end{observation}





\def\FigCodeOnRSAmn{
\begin{figure}[ht!]
\fboxsep=0.2cm
\framebox[\textwidth]{
\begin{minipage}{0.8\textwidth}

 origin is .

\begin{enumerate}

\item .

\item For each point  do:
\begin{enumerate}
    \item ;
    \item .
    \item Call  as a subroutine on  to find ;
    \item ;
\end{enumerate}

\end{enumerate}
\end{minipage}
}
\caption{\label{fig:middle-alg}
Algorithm .
}
\end{figure}

} \FigCodeOnRSAmn


\subsection{Getting rid of the {knowledge} assumptions}
\label{subsec: onRSA}

To give up the assumption that  is known, we use a standard trick.
We first guess that  is
``about'' twice the  coordinate of the first point.
Whenever the guess for  is proven wrong (some  arrives with  larger then our guess for ),
we double the guess.
We do not change the solution for the points we already served.
Simply, the points that arrive from now on, are treated as a new instance of , to be solved (by ) by a translation to a new instance of .
Intuitively, every instance of  may need to pay an additional cost
that is proportional to our current guess of .
This is justified by the fact that
(1) the new guess is at least double our previous guess of ;
and (2) any optimal algorithm would need now to pay  guessed before.
(A minor technical point is that the origin of the new instance of  may not be
;
instead, the new origin is , where  is the -coordinates of the last point served.)



For justifying the other assumption, that the number of points is known in advance,
we use a similar trick; however, its justification is more complex.
That is, if the number of points grows larger beyond our current guess, ,
we increase our guess of the number of points.
We then start a new instance of  with the new guess.
(In turn, this leads to a new activation of  with  as the new network size.)
Hence, we start a new  instance with an increased ``network size''.
The ``new'' guess  of the number of  points is (not doubled but) the
power of 4 of our ``current''  (yielding a double exponential sequence).
Each new  instance  is associated with a cost of  at most.
Thanks to using a double exponential groth rather than an exponential growth, this would increase the competitive ratio just by a factor of .
Clearly, one should not increase the guess (of the number of points) more than polynomially each time
(since otherwise, for the last guess , the value would have been too high compared to the desired  competitive ratio.)
Summarizing the above informal description, given an instance of , we use ``guesses'' of  and 
to partition the points  into subsets.
Each such subset defines a problem we translate separately to  via .


Given an instance of , we now define its partition of multiple instances. For that, we define the partition of  into subsets , , .... The first  points will belong to , the next  will belong to , etc. We shall also show how to detect online the first point in , the first in , etc.
Before that, we must tackle some technicality. The original  problem with defined for an origin of  and .
However, after solving for the  instance , the next point is at  coordinate that is larger than zero.
Moreover, when solving , we allowed the origin to be at any node (that is, in any  coordinate).
Hence, it is convenient to generalize
the definition of the  to the setting were the input includes an origin point , in the positive quadrant. The input point set  includes only points (in the positive quadrant), whose -coordinates are grater than or equal to .




Consider a point set .
Algorithm  partitions  into subsets as follows.
For every  define that
\vspace{-0.0cm}

where , and
\vspace{-0.0cm}

where  is integer such that .
Note that, . Hence, the growth of the sequence  is for the power of 4.




Let us use the above guesses to generate the subset.
Specifically, we generate a sequence  (for some )
of separators between consecutive subsets.
That is, , then , etc.
A separator is the index of a point where one of the guess fails.
Specifically, let  and if  or , then let
\vspace{-0.0cm}

Define that the guess  of  is
 and the guess  of  is
, for every .
The origin points of these subsets are defined as follows:
Let
 be the -axis of the {\em last} point  in 
and let  and  (for ).
The origin point of  is , for every  (see Fig. \ref{fig: onRSA partition to groups}).



All the above functions can be computed online.
As
sketched,
Algorithm  handles a point after point, and a subset after subset.
For every point ,  finds the subset  that  belongs to (i.e., ), then 
passes the point to an instance of  executing (in parallel to ) on ,
with the origin point , and with the  parameter 
and the  parameter .
Denote the solution
of 
on  by .
The solution of  is the union of the solutions of  on all the subsets. That is,
's solution is .
The pseudo code of  is given in Fig. \ref{figure: onRSA}.



\def\FigonRSA{
\begin{figure}[ht!]
\begin{center}
\includegraphics[scale=0.4]{onRSA.eps}
\end{center}
\caption{\sf Partitioning  into subsets ;
each instance corresponds to a subset , origin ,   and  .
\label{fig: onRSA partition to groups}
}
\end{figure}
}\FigonRSA




\begin{figure}[ht!]
\fboxsep=0.2cm
\framebox[\textwidth]{
\begin{minipage}{0.95\textwidth}


\begin{enumerate}

\item when the first point  arrives
    \begin{enumerate}
    \item ; ; ; ; ; and origin .

    \item start an instance of  on ;




    \end{enumerate}

\item when an input point arrives  (for ), /* the points are 
    \begin{enumerate}

    \item if  and , then .
\item Otherwise,  ( ``open a new instance''), then
\begin{enumerate}
        \item ;
        \item ;
        \item ;

        \item ;

        \item ;

        \item ;
        \item start an instance of  on ;
        \end{enumerate}

    \end{enumerate}
\item pass  to the instance of  executing on  with origin ; ; and  and compute .



\item  .\\




\end{enumerate}

\end{minipage}
}
\caption{\label{figure: onRSA}
Algorithm .
}
\end{figure}




\def\AppThmonRSA{
\begin{thm}
Algorithm
 is optimal and is
\commdouble \\ \commdoubleend
 -competitive.
\label{appthm: onRSA}
\end{thm}
\begin{proof}
Consider a point set .
Let  be the separator indices of the subsets .
Recall that, for each subset , we have two types of guesses,
 for - and  for , and an origin at
.
These parameters are computed by  online.
It is easy to see that
the solution  constructed by  is feasible (the vertical segment  is contained in ;
thus, all the  origins  are connected in ;
in addition, since subroutine  solves  for the th instance (with ), there exists a path from the origin point  to every point of ;
finally, every point of  belongs to some subset ).
Thus also,

since the solution of  on  is a collection of the solutions of   on the instances  with the relates origins and guessing parameters.


Clearly, from definition (\ref{def: g k+1= separator}), for the last guess (of ),  and , it holds that
 and . Thus,

and



We now compare the actions of an optimal algorithm  for  when working on two different instances of .
The first instance  has the input  and the origin at . The second instance  has the input  and the origin at .
Both the executions of  on the first instance , and the execution on the second instance , must serve the points in . We now compare their costs for serving those points only.

To define those costs, let  and (for every ),
let  be the time of the last point in  (that is,
) and  be the time of the last point in .
Given the solution  of  on the first instance
, we can speak on the {\em part} of that solution
dealing with . Formally,
let
 be
 projected on the time interval .
The total length of the segments thus defined is
.

The above solution of the first instance   may serve the points of  from any other parts of solution  projected on times earlier than  (or, of course, from points of ).
Note that (from the definition of ), the  coordinate of those earlier points is within the interval
.
On the other hand,  performing on the second instance  may serve points in  only from either the origin of the second instance (or from points in ). This means that
.


However, let us now consider a revision of the  problem (with the instance
. In the revised problem, a copy is given initially not only in the origin  but also along the path of length  at time . Clearly, an optimal solution for that problem is not worse than
, since it can simulate
 . On the other
hand, clearly, an optimal solution for the revised problem is better than
 by an additive factor of .
This leads for the following inequality,

Rewriting  Observation \ref{corollary: onRSAmn is O(sqrt)-cometitve},
\commsingle

\commsingleend
\commdouble

\commdoubleend
where  is some constant.
Therefore,
\commsingle

\commsingleend
where the first inequality holds by Inequality (\ref{ineq: thm cost onRSAmn Q<k> leq sqrt}) and Inequality (\ref{ineq:thm: cost(RSA Q)= sum of subsets costs});
the second inequality holds by Inequality (\ref{ineq: thm cost(opt, Q) geq sum cost Q'<k>}) and since .














Inequalities (\ref{cost(opt Q) geq M/2}) and (\ref{sqrt(log sizeR)=Theta(sqrt(log NetSize)})
bounds the right hand side above. Hence,
it is left to bound  from above  as a function of  and .
Recall that in some cases  started a new instance  (for ``new'' subset )
when  larger than  and white in some other cases , but .
Let  be the indices
of instances of  of the first case above ().
Similarly, let 
be the indices of instances of the second case ().
We have
\commsingle

\commsingleend
\commdouble

\commdoubleend
since 
( belongs to both sets; for every , either  or , thus ),
 and .
Recall that  for each .
Thus,

Similarly, , for each .
Thus,

since, .
Hence,

The theorem follows (from inequalities
(\ref{ineq:thm: cost(opt Q) geq M/2}), (\ref{ineq:thm: sqrt(log sizeR)=Theta(sqrt(log NetSize)}),
(\ref{ineq:thm: cost (SRSA,Q) leq sqrt opt Q + sum log nk mk})
and (\ref{ineq:thm: sum Mk log nk leq Mtau log ntau})).
\QED\end{proof}
} 





\begin{theorem}
Algorithm  is optimal and is -competitive.
\label{thm: onRSA}
\end{theorem}












\subsection{Optimizing  for a small number of requests}
\label{sec:Optimal-mcd-for-few-requests}
Algorithm  was optimal only as the function of the network size.
Recall that our solution for  was optimal as a function of the number of requests.
We obtain this property for the solution of  too, by   transforming our  algorithm back to solve , and obtain the promised competitiveness,
 .



Algorithm  was optimal as the function of the network size (Theorem \ref{thm: Dlineon is frac(log n)(log log n) competitive}).
This means that it may not be optimal in the case that the number of requests is much smaller than the network size. In this section, we use Theorem \ref{thm: onRSA} and algorithm  to derive an improve algorithm for . This algorithm, , is competitive optimal (for ) for any number of requests.
Intuitively, we benefit from the fact that  is optimal for any number of points (no notion of network size exists in ).

This requires the solution of some delicate point. Given an instance  of , we would have liked to just translate the set  of  requests into a set  of  points and
apply  on them.
This may be a bit confusing, since  performs by converting back to .
Specifically,
recall that
  breaks  into several subsets, and translates back first the first subset  into an the requests set  of a new instance
  of .
Then,  invokes   on this new instance .
The delicate point is that  is different than .

In particular, the fact that  contains only {\em some} of the points of , may cause  to ``stretch'' their  coordinates to fit them into the network of .
Going carefully over the manipulations performed by 
reveals that the solution of  may not be a feasible solution
of  (even though it applied  plus some manipulations).
Intuitively, the solution of 
  may ``store copies'' in places that are not grid vertices in the grid of . Thus the translation to a solution of  is not immediate.

Intuitively, to solve this problem, we translate a solution of  to a solution of  in a way that is similar to the way we translated a solution of  to a solution of . That is,  each request of  we move to a ``nearby'' point of . This is rather straightforward, given the description of our previous transformation (of Section \ref{subsec: onRSAn}).
The details are left for the full paper.




\begin{theorem}
Algorithm  is optimal and it
\begin{center}
-competitive.
\end{center}
\end{theorem}














\vspace{-0.2cm}
\section{Lower Bound for }


\label{sec:Lower bound}



In this section, we prove the following theorem, establishing a tight lower bound for  and for  on directed line networks.
Interestingly,  this lower bound is not far from the one proven by Alon and Azar  for {\em undirected} Euclidian Steiner trees \cite{AlonAzar93}.
Unfortunately,  the lower bound of \cite{AlonAzar93} does not apply to our case since their construct uses edges directed in what would be the wrong direction in our case (from a high  value to a low one).

\begin{theorem}
The competitive ratio of any deterministic online algorithm for
 in directed line networks is ,
implying also an
 lower bound for .
\label{thm: lower bound for RSA and MCD on directed}
\end{theorem}
\proof
We first outline the proof.
Informally, given a deterministic online algorithm , we construct an adversarial input sequence.
Initially, the request set includes the set .
That is, at each time step , the request  is made.
In addition, if the algorithm leaves ``many copies'' then the lower bound is easy.
Otherwise, the algorithm leaves ``too few copies'' from some time  until time .
For each such time, the adversary makes another request at  for some  defined later.
The idea is that the adversary can serve this additional request from the diagonal copy at  paying the cost of .
On the other hand,
the algorithm is not allowed at time  to decide to serve from .
It must serve from a copy it did leave.
Since the algorithm left only ``few'' copies to serve time  the replica,  can be chosen at least at distance  from any copy the algorithm did leave.
Hence, the algorithm's cost for such a time  is  times greater than that of the adversary.



More formally, let .
Partition the line at time  into
 intervals:
, where .
(Note that the intervals are well defined, since , for every , which implies that  for every .)
Given an online algorithm , the adversary constructs the set of
requests  as follows.  Initially, .
For each
time , denote by  the set of
nodes that hold the movie for time  (just before  receives
the requests for time ).
The adversary may add a request at  according to .
In particular,
if  leaves a copy in at least one of the nodes of every such intervals , for ,
then the only adversary request for time  is  (while  left copies in at least  nodes).
Otherwise, the adversary adds the request  to , where  is an arbitrary index such that .
That is, the adversary request set of time  is  in the first case and   in the second case.


For each time ,
one of the following two cases hold:
{\bf (1)}
 pays at least  for storing at least  copies from time  to time ,
while the adversary pays just  (to serves request ); or
{\bf (2)}  pays, at least,  for delivering a copy  to  from some node outside the interval ,
while the adversary pays  for storing the movie in node  from time  to time 
(that is, serving from replica  on the diagonal) and additional two edges (to serve request ).
Thus, in that case,  pays at least  times more than the adversary.
This establishes Theorem \ref{thm: lower bound for RSA and MCD on directed}.
\QED






\def\thepage{}
{\small

\begin{thebibliography}{10}

\bibitem{AlonAzar93}
N.~Alon and Y.~Azar.
\newblock On-line {S}teine trees in the euclidean plane.
\newblock {\em Discrete {\&} Computational Geometry}, 10:113--121, 1993.

\bibitem{MAicalp12}
R.~Bar-Yehuda, E.~Kantor, S.~Kutten, and D.~Rawitz.
\newblock Growing half-balls: Minimizing storage and communication costs in
  {C}{D}{N}s.
\newblock In {\em ICALP}, pages 416--427, 2012.

\bibitem{KnuthYao09}
W.~Bein, M.~Golin, L.~Larmore, and Y.~Zhang.
\newblock The {K}nuth-{Y}ao quadrangle-inequality speedup is a consequence of
  total monotonicity.
\newblock {\em ACM Transactions on Algorithms}, 6(1), 2009.

\bibitem{berman}
P.~Berman and C.~Coulston.
\newblock On-line algorrithms for {S}teiner tree problems.
\newblock In {\em STOC}, pages 344--353, 1997.

\bibitem{halperin}
M.~Charikar, D.~Halperin, and R.~Motwani.
\newblock The dynamic servers problem.
\newblock In {\em 9th Annual Symposium on Discrete Algorithms (SODA)}, pages
  410--419, 1998.

\bibitem{CDL01}
X.~Cheng, B.~Dasgupta, and B.~Lu.
\newblock Polynomial time approximation scheme for symmetric rectilinear
  {S}teiner arborescence problem.
\newblock {\em J.\ Global Optim.}, 21(4):385--396, 2001.

\bibitem{another-at-poly}
J.~D. Cho.
\newblock A min-cost flow based min-cost rectilinear {S}teiner
  distance-preserving tree construction.
\newblock In {\em ISPD}, pages 82--87, 1997.

\bibitem{cong}
J.~Cong, A.~B. Kahng, and K.~S. Leung.
\newblock Efficient algorithms for the minimum shortest path {S}teiner
  arborescence problem with applications to vlsi physical design.
\newblock {\em IEEE Trans. on CAD of Integrated Circuits and Systems},
  17(1):24--39, 1998.

\bibitem{presented-rsa}
R.~R.~Ladeira de~Matos.
\newblock A rectilinear arborescence problem.
\newblock {\em Dissertation, University of Alabama}, 1979.

\bibitem{garey-johnson}
M.~R. Garey and D.~S. Johnson.
\newblock The rectilinear {S}teiner tree problem is {N}{P}-complete.
\newblock {\em SIAM J. Appl. Math.}, 32(4):826--834, 1977.

\bibitem{halperin1}
D.~Halperin, J.~C. Latombe, and R.~Motwani.
\newblock Dynamic maintenance of kinematic structures.
\newblock In {\em J.P. Laumond and M. Overmars, editors, Algorithmic
  Foundations of Robotics. A.K. Peters Publishing}, pages 155--170, 1997.

\bibitem{steiner-book}
F.~K. Hwang and D.~S. Richards.
\newblock {S}teiner tree problems.
\newblock {\em Networks}, 22(1):55--897, 1992.

\bibitem{vlsi}
A.~Kahng and G.~Robins.
\newblock On optimal interconnects for {\sc\large vlsi}.
\newblock {\em Kluwer Academic Publishers}, 1995.

\bibitem{KK2014TR}
E.~Kantor and S.~Kutten.
\newblock Optimal competitiveness for symmetric rectilinear {S}teiner
  arborescence and related problems.
\newblock {\em CoRR}, abs/1307.3080, 2013.

\bibitem{KK2014}
E.~Kantor and S.~Kutten.
\newblock Optimal competitiveness for symmetric rectilinear {S}teiner
  arborescence and related problems.
\newblock In {\em ICALP(2)}, pages 520--531, 2014.

\bibitem{ptas1}
B.~Lu and L.~Ruan.
\newblock Polynomial time approximation scheme for rectilinear {S}teiner
  arborescence problem.
\newblock {\em Combinatorial Optimization}, 4(3):357--363, 2000.

\bibitem{natansky}
L.~Nastansky, S.~M. Selkow, and N.~F. Stewart.
\newblock Cost minimum trees in directed acyclic graphs.
\newblock {\em Z. Oper. Res.}, 18:59--67, 1974.

\bibitem{papa1}
C.H. Papadimitriou, S.~Ramanathan, and P.V. Rangan.
\newblock Information caching for delivery of personalized video programs for
  home entertainment channels.
\newblock In {\em IEEE International Conf. on Multimedia Computing and
  Systems}, pages 214--223, 1994.

\bibitem{papa3}
C.H. Papadimitriou, S.~Ramanathan, and P.V. Rangan.
\newblock Optimal information delivery.
\newblock In {\em 6th ISAAC}, pages 181--187, 1995.

\bibitem{papa2}
C.H. Papadimitriou, S.~Ramanathan, P.V. Rangan, and S.~Sampathkumar.
\newblock Multimedia information caching for personalized video-on demand.
\newblock {\em Computer Communications}, 18(3):204--216, 1995.

\bibitem{shor-rsa}
S.~Rao, P.~Sadayappan, F.~Hwang, and P.~Shor.
\newblock The {R}ectilinear {S}teiner {A}rborescence problem.
\newblock {\em Algorithmica}, pages 277--288, 1992.

\bibitem{rsa-nph}
W.~Shi and C.~Su.
\newblock The rectilinear {S}teiner arborescence problem is {N}{P}-complete.
\newblock In {\em SODA}, pages 780--787, 2000.

\bibitem{rsa-error}
V.A. Trubin.
\newblock Subclass of the {S}teiner problems on a plane with rectilinear
  metric.
\newblock {\em Cybernetics and Systems Analysis}, 21(3):320--324, 1985.

\end{thebibliography}


}






\end{document}
