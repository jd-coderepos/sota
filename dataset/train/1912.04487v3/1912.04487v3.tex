\section{Experiments}\label{sec:results}
\vspace{-0.05in}

Using a total of 4 datasets, we evaluate our approach for accurate and efficient clip-level action recognition (Sec.~\ref{Sec:clip_level_results}) and video-level action recognition (Sec.~\ref{Sec:video_level_results}).

\vspace{-0.05in}
\paragraph{Datasets:} Our distillation network is trained on Kinetics~\cite{kay2017kinetics}, and we evaluate on four other datasets: Kinetics-Sounds~\cite{arandjelovic2017look}, UCF-101~\cite{soomro2012ucf101}, ActivityNet~\cite{caba2015activitynet}, and Mini-Sports1M~\cite{karpathy2014large}. Kinetics-Sounds and UCF-101 contain only short trimmed videos, so we only test on them for clip-level recognition; ActivityNet contains videos of various lengths, so it is used as our main testbed for both clip-level and video-level recognition; Mini-Sports1M contains only long untrimmed videos, and we use it for evaluation of video-level recognition. See Supp. for details of these datasets.

\vspace{-0.05in}
\paragraph{Implementation Details:} We implement in PyTorch. For \textsc{ImgAud2Vid}, the R(2+1)D-18~\cite{tran2018closer} teacher model takes 16 frames of size $112 \times 112$ as input. The student model uses a ResNet-18 network for both the visual and audio streams, which take the starting RGB frame of size $112 \times 112$ and a 1-channel audio-spectrogram of size $101 \times 40$ (1 sec.~audio segment) as input, respectively. We use $\lambda=100$ for the distillation loss in Equation~\ref{equation:distillation}. For \textsc{ImgAud-Skimming}, we use a one-layer LSTM with 1,024 hidden units and a dimension of 512 for the indexing key vector.  We use $T = 10$ time steps during training. See Supp. for details.

\vspace{-0.05in}
\subsection{Clip-level Action Recognition}~\label{Sec:clip_level_results}
\vspace{-0.2in}

First, we directly evaluate the performance of the image-audio network distilled from the video model. We fine-tune on each of the three datasets for clip-level recognition and compare against the following baselines:

\vspace{-0.1in}
\begin{itemize}[leftmargin=4mm]
\itemsep0em
\item \textbf{Clip-based Model:} The R(2+1)D-18 teacher model.
\vspace{-0.05in}
\item \textbf{Image-based Model (distilled/undistilled):} A ResNet-18 frame-based model. The undistilled model is pre-trained on ImageNet, and the distilled model is similar to our method except that the distillation is performed using only the visual stream. 
\vspace{-0.05in}
\item \textbf{Audio-based Model (distilled/undistilled):} The same as the image-based model except here we only use the audio stream for recognition and distillation. The model is pre-trained on ImageNet to accelerate convergence.
\vspace{-0.05in}
\item \textbf{Image-Audio Model (undistilled):} The same image-audio network as our method but without distillation.
\end{itemize}
\vspace{-0.1in}

\begin{figure}
    \center
    \includegraphics[scale=0.53]{clip_level_comparison.pdf}
    \caption{Clip-level action recognition on Kinetics-Sounds, UCF-101, and ActivityNet. We compare the recognition accuracy and the computational cost of our model against a series of baselines. Our \textsc{ImgAud2Vid} approach strikes a favorable balance between accuracy and efficiency.
    \vspace{-0.1in}} 
    \label{fig:clip_level_comparison}
    \vspace{-0.1in}
\end{figure}

For each baseline, we use the corresponding model as initialization and fine-tune on the same target dataset for clip-based action recognition. Note that our purpose here is not to compete on recognition accuracy using R(2+1)D-18 (or any other expensive video features), but rather to demonstrate our distilled image-audio features can approximate its recognition accuracy much more efficiently.

Figure~\ref{fig:clip_level_comparison} compares the accuracy vs.~efficiency for our approach and the baselines. Our distilled image-audio network achieves accuracy comparable to that of the clip-based teacher model, but at a much reduced computational cost. Moreover, the models based on image-only or audio-only distillation produce lower accuracy. This shows that the image or audio alone is not sufficient to hallucinate the video descriptor, but when combined they provide sufficiently complementary information to reduce the accuracy gap with the true (expensive) video-clip descriptor. 

To understand when audio helps the most, we compute the $\mathcal{L}_1$ distance of the hallucinated video descriptor to the ground-truth video descriptor by our \textsc{ImgAud2Vid} distillation and the image-based distillation. The top clips for which we best match the ground-truth tend to be dynamic scenes that have informative audio information, \eg, grinding meat, jumpstyle dancing, playing cymbals, playing bagpipes, wrestling, and welding. See Supp. for examples.

\begin{table*}
\resizebox{1\linewidth}{!}{
\begin{tabular}{@{}r|cgcgcgcgcgc@{}}
\toprule
& \textsc{Random} & \textsc{Uniform} & \textsc{Front} & \textsc{Center} & \textsc{End} & \textsc{SCSampler}~\cite{korbar2019scsampler} & \textsc{Dense} & \textsc{LSTM} &  \textsc{Non-Recurrent} & Ours (sparse / dense) \\
\hline
ActivityNet &  63.7     &     64.8     &    39.0   &    59.0  &            38.1         &     69.1      &    66.3  & 63.5 & 67.5  & \textbf{70.3} / \textbf{71.1} \\ 
Mini-Sports1M       &  35.4      &  35.6       &   17.1         &  29.7   &  17.4  &     38.4       & 37.3    &  34.1          & 38.0  &  \textbf{39.2} / \textbf{39.9} \\ \bottomrule
\end{tabular}
}
\vspace{-1mm}
\caption{Video-level action recognition accuracy (in \%) on ActivityNet (\# classes: 200) and Mini-Sports1M (\# classes: 487). Kinetics-Sounds and UCF-101 consist of only short trimmed videos, so they are not applicable here. Our method consistently outperforms all baseline methods. Ours (sparse) uses only about 1/5 the computation cost of the last four baselines, while achieving large accuracy gains. See Table~\ref{Table:activitynet_sota} for more computation cost comparisons.
\vspace{-0.1in}
}
\vspace{-0.05in}
\label{Table:video_level_comparison}
\end{table*}

\vspace{-0.05in}
\subsection{Untrimmed Video Action Recognition}~\label{Sec:video_level_results}
\vspace{-0.2in}

Having demonstrated the clip-level performance of our distilled image-audio network, we now examine the impact of the $\textsc{ImgAud-Skimming}$ module on video-level recognition. We evaluate on ActivityNet~\cite{caba2015activitynet} and Mini-Sports1M~\cite{karpathy2014large}, which contain long untrimmed videos. 

\vspace{-0.05in}
\paragraph{Efficiency \& accuracy trade-off.} 
Before showing the results, we introduce how we use feature interpolation to further enhance the efficiency of our system. Apart from using features from all $N$ time stamps as described in Sec.~\ref{Sec:video_level_app}, we experiment with using \emph{sparse} indexing features extracted from a subset of image-audio pairs, \ie, subsampling along the time axis. Motivated by the locally-smooth action feature space~\cite{Dwibedi_2019_CVPR} and based on our empirical observation that neighboring video features can be linearly approximated well, we synthesize the missing image and audio features by computationally cheap linear interpolation to generate the full feature sequences of length $N$. Figure~\ref{fig:interpolation} shows the recognition results when using different subsampling factors. We can see that recognition remains robust to even aggressive subsampling of the indexing features.

Next we investigate early stopping as an additional means to reduce the computational cost. Instead of repeating the skimming procedure for 10 times as in the training stage, we can choose to stop early after a few recurrent steps. Figure~\ref{fig:early_stop} shows the results when stopping at different time steps. We can see that the first three steps yield sufficient cues for recognition. This suggests that we can stop around the third step with negligible accuracy loss. See Supp. for a similar observation on Mini-Sports1M.

\begin{figure}
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{interpolation.pdf}
     \vspace{-0.2in}
    \caption{Feature interpolation}
    \label{fig:interpolation}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{early_stop.pdf}
        \vspace{-0.2in}
    \caption{Early stopping}
    \label{fig:early_stop}
  \end{subfigure}
  \caption{Trade-off between efficiency and accuracy when using sparse indexing features or early stopping on ActivityNet. Uniform denotes the \textsc{Uniform} baseline in Table~\ref{Table:video_level_comparison}.
    \vspace{-0.15in}}
  \vspace{-0.1in}
\end{figure}

\paragraph{Results.} We compare our approach to the following baselines and several existing methods~\cite{yeung2016end,fan2018watching,wu2019adaframe,wu2019multiagent,korbar2019scsampler}:
\vspace{-0.05in}
\begin{itemize}[leftmargin=4mm]
\itemsep0em
\item \textsc{Random:} We randomly sample 10 out of the $N$ time stamps, and average the predictions of the image-audio pairs from these selected time stamps using the distilled image-audio network.
\vspace{-0.05in}
\item \textsc{Uniform:} The same as the previous baseline except that we perform uniform sampling.
\vspace{-0.05in}
\item \textsc{Front / Center / End:} The same as before except that the first / center / last 10 time stamps are used.
\vspace{-0.05in}
\item \textsc{Dense:} We average the prediction scores from all $N$ image-audio pairs as the video-level prediction.
\vspace{-0.05in}
\item \textsc{SCSampler~\cite{korbar2019scsampler}:} We use the idea of \cite{korbar2019scsampler} and select the 10 image-audio pairs that yield the largest confidence scores from the image-audio classifier. We average their predictions to produce the video-level prediction. 
\vspace{-0.05in}
\item \textsc{LSTM}: This is a one-layer LSTM as in our model but it is trained and tested using all $N$ image-audio features as input sequentially to predict the action label from the hidden state of the last time step.
\vspace{-0.05in}
\item \textsc{Non-Recurrent}: The same as our method except that we only use a single query operation without the recurrent iterations. We directly obtain the 10 time stamps from the indexes of the 10 largest attention weights.
\vspace{-0.05in}
\end{itemize}

Table~\ref{Table:video_level_comparison} shows the results. Our method outperforms all the baselines. The low accuracy of \textsc{Random} / \textsc{Uniform} / \textsc{Front} / \textsc{Center} / \textsc{End} indicates the importance of the context-aware selection of useful moments for action recognition. Using sparse indexing features (with a subsampling factor of 5), our method outperforms \textsc{Dense} (the status quo of how most current methods obtain video-level predictions) by a large margin using only about 1/5 of its computation cost. Our method is also better and faster than \textsc{SCSampler}~\cite{korbar2019scsampler}, despite their advantage of densely evaluating prediction results on all clips. \textsc{LSTM} performs comparably to \textsc{Random}. We suspect that it fails to aggregate the information of all time stamps when the video gets very long. \textsc{Non-Recurrent} is an ablated version of our method, and it shows that the design of recursive prediction of the ``next'' interesting moment in our method is essential. Both \textsc{LSTM} and \textsc{Non-Recurrent} support our contribution as a whole framework, \ie, iterative attention based selection.

\begin{figure}
    \center
    \includegraphics[scale=0.3]{frame_selection_comparison}
    \caption{Comparisons with other frame selection methods on ActivityNet. We directly quote the numbers reported in AdaFrame~\cite{wu2019adaframe} and MultiAgent~\cite{wu2019multiagent} for all the baseline methods and compare the mAP against the average GFLOPs per test video. See text for details.
      \vspace{-0.15in}}
    \label{Figure:frame_selection}
    \vspace{-0.1in}
\end{figure}

\vspace{-0.05in}
\paragraph{Comparison to state of the art frame selection methods.} Fig.~\ref{Figure:frame_selection} compares our approach to state-of-the-art frame selection methods given the same computational budget. The results of the baselines are quoted from AdaFrame~\cite{wu2019adaframe} and MultiAgent~\cite{wu2019multiagent}, where they both evaluate on ActivityNet. For fair comparison, we test a variant of our method with only the visual modality, and we use the same ResNet-101 features for recognition. Our framework also has the flexibility of using cheaper features for indexing (frame selection). See Supp. for details about the single-modality architecture of our \textsc{ImgAud-Skimming} network and how we use different features for indexing and recognition. We use three different combinations denoted as {\em Ours (``indexing features'' $\vert$ ``recognition features'')} in Fig.~\ref{Figure:frame_selection}, including using MobileNetv2~\cite{sandler2018mobilenetv2} features for efficient indexing similar to~\cite{wu2019adaframe}. Moreover, to gauge the impact of our \textsc{ImgAud2Vid} step, we also report the results obtained by using image-audio features for recognition.

Our method consistently outperforms all existing methods and achieves the best balance between speed and accuracy when using the same recognition features, suggesting the accuracy boost can be attributed to our novel differentiable indexing mechanism. Furthermore, with the aid of \textsc{ImgAud2Vid} distillation, we achieve much higher accuracy with much less computation cost; this scheme combines the efficiency of our image-audio clip-level recognition with the speedup and accuracy enabled by our \textsc{ImgAud-Skimming} network for video-level recognition.

\begin{figure}
    \center
    \includegraphics[width=1\linewidth]{fig_qual2-compressed}
    \caption{Qualitative examples of 5 uniformly selected moments and the first 5 visually useful moments selected by our method for two videos of the actions \emph{throwing discus} and \emph{rafting} in ActivityNet. The frames selected by our method are more indicative of the corresponding action. \vspace{-0.2in}
    }
    \label{fig:qualitative_examples}
    \vspace{-0.1in}
\end{figure}

\paragraph{Comparison to the state of the art
on ActivityNet.} 
Having compared our skimming approach to existing methods for frame selection, now we compare to state-of-the-art activity recognition models that forgo frame selection. For fair comparison, we use the ResNet-152 model provided by~\cite{wu2019multiagent}. This model is pre-trained on ImageNet and fine-tuned on ActivityNet with TSN-style~\cite{wang2016temporal} training. As shown in Table~\ref{Table:activitynet_sota}\textcolor{blue}{a}, our method consistently outperforms all the previous state-of-the-art methods. To show that the benefits of our method extend even to more powerful but expensive features, we use R(2+1)D-152 features for recognition in Table~\ref{Table:activitynet_sota}\textcolor{blue}{b}. When using R(2+1)D-152 features for both indexing and recognition, we outperform the dense approach while being \emph{$10\times$ faster}. We can still achieve comparable performance to the dense approach if using our image-audio features for indexing, while being \emph{20$\times$ faster}.

\begin{table}
\begin{subtable}{\linewidth}\centering
{\resizebox{1\linewidth}{!}{
\begin{tabular}{zccccc}
\toprule
     & Backbone & Pre-trained & Accuracy & mAP
     \\ \hline
IDT~\cite{wang2013action}  & --  & ImageNet &  64.7 & 68.7 \\ 
C3D~\cite{tran2015learning}  & --    & Sports1M    &   65.8 & 67.7 \\ 
P3D~\cite{qiu2017learning}  &  ResNet-152  &  ImageNet &  75.1 & 78.9   \\ 
RRA~\cite{zhu2018fine}  &  ResNet-152  &  ImageNet    &  78.8 & 83.4 \\ 
MARL~\cite{wu2019multiagent} &  ResNet-152  & ImageNet &   79.8 & 83.8 \\ 
Ours &    ResNet-152      &     ImageNet  & \textbf{80.3}  &  \textbf{84.2}   \\
\bottomrule
\end{tabular}
}}
\vspace{-0.05in}
\caption{Comparison to prior work with ResNet-152 features.}
\vspace{0.05in}
\end{subtable}
\begin{subtable}{\linewidth}\centering
{\resizebox{1\linewidth}{!}{
\begin{tabular}{zccccc}
\toprule
     & Indexing & Recognition & mAP & TFLOPs
     \\ \hline
Dense  &   --  & R(2+1)D-152 &  88.9 &  25.9 \\
Uniform  &   --  & R(2+1)D-152 &  87.2 & 1.26 \\
Ours  &   Image-Audio  & R(2+1)D-152 & 88.5 & 1.31 \\
Ours  &   R(2+1)D-152 & R(2+1)D-152 & \textbf{89.9} & 2.64 \\
\bottomrule
\end{tabular}
}}
\vspace{-0.05in}
\caption{Accuracy vs. Efficiency with R(2+1)D-152 features.}
\end{subtable}
\vspace{0.05in}
\caption{ActivityNet comparison to SOTA methods.
\vspace{-0.2in}
}
\label{Table:activitynet_sota}
\vspace{-0.1in}
\end{table}

\vspace{-0.05in}
\subsection{Qualitative Analysis}
Figure~\ref{fig:qualitative_examples} shows frames selected by our method using the visual modality versus those obtained by uniform sampling. The frames chosen by our method are much more informative of the action in the video compared to those  uniformly sampled. See Supp.~video\footnote{\scriptsize\label{project_page}\url{http://vision.cs.utexas.edu/projects/listen_to_look/}} for examples of acoustically useful moments selected by our method.

We can inspect per-class performance to understand what are the classes that benefit the most from our skimming mechanism compared to uniform sampling. The top classes in descending order of accuracy gain are: cleaning sink, beer pong, gargling mouthwash, painting furniture, archery, laying tile, and triple jump---classes where the action is sporadic and is often exhibited over a short segment of the video. See Supp.~for more analysis.