\section{Experiments}
\label{sec:exp}
In this section, we conduct experiments on our methods with state-of-the-art methods on image harmonization. Firstly, we illustrate all the training/testing details of our methods and the baselines for comparisons in Section.~\ref{sec:imp} and Section.~\ref{sec:baseline} respectively. Then, for the comparison, we evaluate our method for image harmonization and image harmonization without the mask on several datasets in Section.~\ref{sec:com} and Section.~\ref{sec:comw}. Finally, a detailed analysis of the proposed SAM is reported in Section.~\ref{sec:attention}.

\subsection{Implementation detail} 
\label{sec:imp}
All the models are trained with PyTorch~\cite{paszke2017automatic} v1.0 and CUDA v9.0. We train the image with the resolution of  and run the model \SI{80} and \SI{60} epochs for converging on S-COCO and S-Adobe5K, respectively. All the optimizers are Adam~\cite{Kingma:2014us} with the learning rate of \SI{0.001}, and there is no learning rate adjustment schedule for a fair comparison. For testing, our model runs at 0.012 seconds pre-image on a single NVIDIA 1080 GPU with a resolution of .
Since the synthesized datasets have the ground truth target, we evaluate our approach with other methods on multiple popular numerical criteria, such as Mean Square Error (MSE), Structural Similarity (SSIM) and Peak Signal-to-Noise Ratio (PSNR). All the results in the synthesized datasets are the mean of the k and k test images in S-COCO and S-Adobe5K, separately. Moreover, following the metrics in previous works~\cite{Tsai:2017kv,Wu:2017wq}, a pre-trained realism prediction model~\cite{Zhu:2015tl} is used to evaluate all the results from the viewpoint of the neural network. Finally, We evaluate the state-of-the-art methods on the real composite images which are collected from the real world with a user study.

\subsection{Baselines}
\label{sec:baseline}

We list all the image harmonization baselines for comparison on the synthesized datasets as follows: 
\begin{figure*}[th!]
\centering     \subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/input_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dih256seg_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rascseg_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/pix2pixrasc_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/target_1448}}


\par\bigskip\vspace*{-2em}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/mask_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_jet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dih256seg_jet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rascseg_jet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_jet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/pix2pixrasc_jet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_jet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_jet_1448}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/placeholder}}


\par\bigskip\vspace*{-2em}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/input_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dih256seg_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rascseg_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/pix2pixrasc_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/target_781}}

\par\bigskip\vspace*{-2em}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/mask_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_jet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dih256seg_jet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rascseg_jet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_jet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/pix2pixrasc_jet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_jet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_jet_781}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/placeholder}}
\par\bigskip\vspace*{-2em}

\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/input_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dhn256_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rasc_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/target_668}}

\par\bigskip\vspace*{-2em}

\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/mask_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_jet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dhn256_jet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rasc_jet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_jet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/pix2pix_r_jet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_jet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_jet_668}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/placeholder}}



\par\bigskip\vspace*{-2em}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/input_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dhn256_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rasc_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_712}}
\subfigure{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/target_712}}

\par\bigskip\vspace*{-2em}

\addtocounter{subfigure}{-63}
\subfigure[Input/Mask]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/mask_712}}
\subfigure[R-CNN\cite{Chen:2017ta}]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/zhu_jet_712}}
\subfigure[DIH\cite{Tsai:2017kv}]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/dhn256_jet_712}}
\subfigure[DIH+]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/rasc_jet_712}}
\subfigure[pix2pix]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/ounet_jet_712}}
\subfigure[pix2pix+]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/pix2pix_r_jet_712}}
\subfigure[UNET\cite{Isola:2016tp}]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/unet_jet_712}}
\subfigure[UNET+]{\centering\includegraphics[width=0.21\columnwidth]{figures/figurecoco/radhnv3xgaussian_jet_712}}
\subfigure[Target]{\centering\includegraphics[width=0.21\columnwidth]{figures/placeholder}}


\caption{Comparison on synthesized datasets. The two top samples are selected from S-COCO while the two bottom images come from S-Adobe5K. The methods with  represent we replace the original coarser skip-connection with our SASC. To visualize the differences better, we plot the absolute differences colormap between the harmonized image and target under each result. It is apparent that our method shows better results in both image quality and difference colormap. We enlarge the colormap 10 for better visualization.}
\vspace{-1.5em}
\label{fig:COCO}
\end{figure*}
 
\textbf{Deep Image Harmonization (DIH) \cite{Tsai:2017kv}} DIH harmonizes images in a supervised manner with an encoder-decoder structure. Additionally, they use the ground truth segmentation mask as a redundant decoder since the harmonized images should also share the same segmentation results with the target.
We train the DIH on the same synthesized datasets for fair comparison. As our SAM can be plugged into DIH easily by replacing the original skip-connections in color and semantic branch, we modify the original model for comparison on S-COCO and S-Adobe5K. Notice that, because S-Adobe5K has no ground truth segmentation mask, we only evaluate the color branch.

\textbf{RealismCNN (R-CNN)~\cite{Zhu:2015tl}} RealismCNN uses the pre-trained VGG16 network as the basic feature extractor and fine-tune the model in the fully-connection layer for classifying the realism of the input image. Additionally, they freeze the parameters in the realism model and optimize the color of the input image by a novel loss function. However, this method tries to solve a non-convex function by selecting the minimal cost from multiple running. We compare the results on our datasets with the official implementation\footnote{\hyperlink{https://github.com/junyanz/RealismCNN}{https://github.com/junyanz/RealismCNN}}.


\textbf{pix2pix~\cite{Isola:2016tp}} Pix2pix use GAN in the image-to-image transformation task firstly. GAN has significant benefits synthesizing the meaningful photo-realistic images because the discriminator in GAN distinguishes the real or fake image by the neural network itself. However, it fails to get the accurate numerical results. Since we use unet as our backbone network, we compare the unet, unet with SAM as Skip-connection~(Unet+SASC) and unet with SAM as Decoder~(Unet+SAD). Furthermore, we compare pix2pix by considering Unet and Unet+SASC as the generators of GAN respectively. Notice that, we train the GAN framework under the default configuration by the author's  implementation\footnote{\hyperlink{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}}. 



\subsection{Comparison}
\label{sec:com}

\subsubsection{Comparison on Synthesized Datasets.}


\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Method & MSE & SSIM & PSNR  & LPIPS \\
\hline
\hline
\multicolumn{5}{|c|}{S-COCO dataset}  \\
\hline
Original Copy-and-Paste  & 25.94 & 0.9469 & 26.62 & 0.0507 \\
R-CNN \cite{Zhu:2015tl} & 31.21 &  0.9509 & 27.17 & 0.0458 \\
DIH~\cite{Tsai:2017kv}  &  20.88      &   0.9662     & 31.73 &  0.0308         \\
\textbf{DIH + SASC}  &   18.09      &  0.9734      & 33.55    &   0.0174   \\
pix2pix~\cite{Isola:2016tp}    &   21.24   & 0.9633 & 32.13     & - \\
\textbf{pix2pix~\cite{Isola:2016tp} + SASC}     &   17.67   & 0.9707 & 33.32  &   -  \\
Unet~\cite{Isola:2016tp}       & 17.28         & 0.9757          & 33.55 &        0.0162 \\
\textbf{Unet + SASC } & 15.59 & \underline{0.9790} & 34.74 & \underline{0.0144} \\
\textbf{Unet + SAD} & \underline{14.88} & \underline{0.9790} & \underline{35.23} & \textbf{0.0128} \\
\textbf{Unet + SAD + pp} & \textbf{14.26} & \textbf{0.9811} & \textbf{35.38} & \textbf{0.0128} \\
\hline
\hline
\multicolumn{5}{|c|}{S-Adobe5K dataset}  \\
\hline
Original Copy-and-Paste  & 37.43  & 0.9401 & 26.60 & 0.0501 \\
R-CNN~\cite{Zhu:2015tl}  & 41.29 & 0.9338 & 26.11 & 0.0506 \\
DIH\cite{Tsai:2017kv}  & 34.71 &  0.9243 & 27.94 & 0.0506 \\
\textbf{DIH + SASC}   & 26.96 & 0.9586 & 31.45  & 0.0282    \\
pix2pix~\cite{Isola:2016tp}      & 32.26       & 0.9560         & 30.01     &  - \\
\textbf{pix2pix~\cite{Isola:2016tp}  + SASC}     &  22.67         & 0.9702         & 32.59    &   - \\
Unet~\cite{Isola:2016tp}   &     23.18 & 0.9693 & 32.46 & 0.0232   \\
\textbf{Unet + SASC}  & 21.83 & 0.9711 & 33.31 & 0.0210 \\
\textbf{Unet + SAD} & \underline{21.52} & \underline{0.9743} & \underline{33.67} & \textbf{0.0186} \\
\textbf{Unet + SAD + pp} & \textbf{20.47} & \textbf{0.9757} & \textbf{33.94} & \underline{0.0195} \\
\hline
\end{tabular}
\end{center}

\caption{The numerical comparison on synthesized datasets. Here, SASC and SAD are the different interpolate method of our SAM. \textbf{pp} represent the post-processing in Section.~\ref{sec:net}}.

\label{table:COCO}
\end{table} For fair compare the effect of the backbone network and our attention module, we mainly compare the state-of-the-art methods on Unet+SASC. Additionally, we compare the Unet+SASC with Unet+SAD individually.

In S-COCO, we train all the learning-based methods under the same framework except the model structure. As shown in Table.~\ref{table:COCO}, RASC module get better numerical results when it is plugged into three baseline structures: DIH, unet and pix2pix. Moreover, our full method (Unet+SASC) shows the best results. These experiments confirm that our method is more suitable for image harmonization task comparing with others because their approaches only gain the results relying on larger datasets. Additionally, our method achieves better visual effects. We plot the harmonized results in Figure.~\ref{fig:COCO} and calculate the absolute difference colormap between the target and result for better visualization. It is clear that our method outperforms other methods to a large extent. 
Figure.~\ref{fig:COCO}, our method shows better results in various image content and color form. It is obvious that our approach gains better results when the bed or person is the spliced region and performs well in both gray images and color images.


Similar improvements are also observed in S-Adobe5K. This dataset is more complicated than S-COCO for the data is limited, and the mask is inaccurate.
We show the visual comparisons in Figure.~\ref{fig:COCO} and report the numerical results in Table.~\ref{table:COCO}. Both results indicate that our methods gain better performances than others. As shown in the third example in Figure.~\ref{fig:COCO}, the three baseline methods with SAM perform better with inaccurate ground truth mask because our SAM filter the regional features separately in the skip-connection. 

\begin{figure}[t]
\centering
  \includegraphics[width=0.95\columnwidth]{figures/pretrained}
  \caption{The predicted score on the pre-trained model of R-CNN~\cite{Zhu:2015tl}. The methods with  represent we replace the original coarser skip-connection with our SASC. A higher score means a more realistic result.}
  \label{figure:pretrained}
  \vspace{-1.5em}
\end{figure}

Besides the numerical and visual comparison, we also evaluate all the methods on the pre-trained composite realism model in R-CNN~\cite{Zhu:2015tl}. Notice that, the pre-trained model has not been trained on any images in our dataset before. The predicted score in Figure.~\ref{figure:pretrained} shows the predicted score of the composite images. It is clear that our SAM improves the predicted scores from the viewpoint of the pre-trained model in most cases. Meanwhile, our Unet+SAM method shows the best performance results comparing with others. However, it is not surprising that R-CNN performed better when the experiment is performed on its own pre-trained model. Interestingly, for R-CNN, we do not observe a similar phenomenon in S-Adobe5K. It is probably because the pre-trained model optimizes the difference in color by semantic while the mask is not always meaningful in S-Adobe5K.

We compare the results of SASC and SAD on two synthesized datasets. We argue that, in UnetSAD, our SAM learns from the high-level information and low-level information altogether. Besides the numerical analysis in Table.~\ref{table:COCO}, SAD show better global consistence that SASC as shown in Fig.~\ref{fig:sad}. It is clear that SAD gets the benefits from the upsampled high-level features and generates more realistic results from the global context. For example, the shoulder of the human in the third example and the hat of the child in the last sample are not synthesized well in SASC while we get the better results from the interpolate of high-level features and low-level information from encoder.

\begin{figure}[t!]
\centering     \subfigure{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figsad/input_ExpertA_ExpertE_a2425-Ja_Pe-44_0}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/rascv1_ExpertA_ExpertE_a2425-Ja_Pe-44_0}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/rascv2_ExpertA_ExpertE_a2425-Ja_Pe-44_0}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/target_ExpertA_ExpertE_a2425-Ja_Pe-44_0}}
\par\bigskip\vspace*{-2em}

\subfigure{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figsad/input_ExpertA_ExpertE_a1136-LS051026_day_1_arive31_11}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/rascv1_ExpertA_ExpertE_a1136-LS051026_day_1_arive31_11}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/rascv2_ExpertA_ExpertE_a1136-LS051026_day_1_arive31_11}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/target_ExpertA_ExpertE_a1136-LS051026_day_1_arive31_11}}
\par\bigskip\vspace*{-2em}

\subfigure{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figsad/input_000000182611_1}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figsad/rascv1_000000182611_1}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/rascv2_000000182611_1}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/target_000000182611_1}}
\par\bigskip\vspace*{-2em}

\addtocounter{subfigure}{-12}
\subfigure[Input]{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figsad/input_000000345361_1.jpg}}
\subfigure[SASC]{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/rascv1_000000345361_1.jpg}}
\subfigure[SAD]{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/rascv2_000000345361_1.jpg}}
\subfigure[Target]{\centering\includegraphics[width=0.24\columnwidth]{figures/figsad/target_000000345361_1.jpg}}

\caption{Comparison between the SASC and SAD. Although SASC harmonizes the tampered region, there are still some fake details while SAD show realistic global results.}
\label{fig:sad}
\vspace{-1.5em}
\end{figure} 

\subsubsection{Comparison on Real Dataset with User Study.}

\begin{figure}[t!]
\centering     

\subfigure{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figurereal/image_IMG_0665}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figurereal/dih_IMG_0665}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figurereal/zhu_IMG_0665}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figurereal/rasc_60}}
\par\bigskip\vspace*{-2em}

\subfigure{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figurereal/image_test28_Su}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figurereal/dih_test28_Su}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figurereal/zhu_test28_Su}}
\subfigure{\centering\includegraphics[width=0.24\columnwidth]{figures/figurereal/rasc_test28_su}}
\par\bigskip\vspace*{-2em}



\addtocounter{subfigure}{-9}
\subfigure[Original]{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figurereal/image_test19}}
\subfigure[DIH~\cite{Tsai:2017kv}]{\centering\includegraphics[width=0.24\columnwidth,height=0.24\columnwidth]{figures/figurereal/dih_test_44}}
\subfigure[Zhu~\cite{Zhu:2015tl}]{\centering\includegraphics[width=0.24\columnwidth]{figures/figurereal/zhu_test19}}
\subfigure[Ours]{\centering\includegraphics[width=0.24\columnwidth]{figures/figurereal/rasc_91}}
\caption{Comparison on the real dataset. As shown in the figure, our method gain better results than DIH~\cite{Tsai:2017kv} and Zhu~\cite{Zhu:2015tl} with the pre-trained model provided by the author.}
\label{fig:real}
\end{figure} Although we train the model on the synthesized dataset, our method can also harmonize the real samples of the spliced image. We compare our method with the two most relevant state-of-the-art methods on the real composite images with provided spliced mask. This dataset contains 99 images with various different scenes and it is collected by DIH~\cite{Tsai:2017kv}. As shown in Figure.~\ref{fig:real}, our method out-perfumes other states-of-the-art methods with a larger margin on this dataset. For there is no available ground truth target image for the harmonized version of these images, we conduct a user study to evaluate our method on the real dataset and synthesized datasets by Amazon Mechanical Turk. Our user study template derivates from the web page in \cite{Wu:2017wq}. Specifically, for each task, giving the original copy-and-paste image and corresponding mask, the users need to choose the best realistic image from all the harmonized results created by different algorithms. In user study, we use the structure of Unet+SAD structure and train our model on two synthesized datasets with 30 epochs from scratch. As for the comparison, we compare our method with the pre-trained DIH model provided by author\footnote{\hyperlink{https://github.com/wasidennis/DeepHarmonization}{https://github.com/wasidennis/DeepHarmonization}}. This model pre-trained on the semantic segmentation task and fine-tune the network on three datasets synthesized by their own. As shown in the Table.~\ref{tab:ustudy}, our method gains much more votes than others with a larger margin.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Method & Total votes   & Average votes  \\
\hline\hline
Zhu~\cite{Zhu:2015tl}  &  120  &   21\%   \\
DIH~\cite{Tsai:2016id} &  212  &   35\%  \\
Ours  &  263  &  44\%  \\
\hline


\end{tabular}
\end{center}

\caption{The comparison between our method and other state-of-the-art methods under user study.}

\label{tab:ustudy}

\end{table}




\subsection{Image Harmonization without mask}
\label{sec:comw}

\subsubsection{Comparison on Synthesized Dataset}

As described in Section.~\ref{sec:net}, our method can be adapted to the image harmonization task without the ground truth as input. So in this task, we regard the previous Unet as the baseline network structure by feeding the color image to the network only. We evaluate our method on the S-COCO dataset for comparison. As shown in Fig.~\ref{fig:unmask}, even without the feeding of the tampered mask, our method can still predict the realistic harmonized image and achieve much more better performance than the baseline unet~\cite{Isola:2016tp} both visual and numerical quality. We also plot the intermediate attention map (coarser level of the attention loss) created by our methods in Fig.~\ref{fig:unmask}. The mask predicted by our method is reasonable. We also report the numerical results under this task in Table.~\ref{tab:umask}, surprisedly, our full method get comparable results with the DIH methods (which need the mask as input) under the same datasets. Table.~\ref{tab:umask} also report the intermediate results of our methods. Each experiment shows the necessity in the network.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Method & MSE & SSIM & PSNR \\

\hline\hline
Original Copy-and-Paste  & 25.94 & 0.9469 & 26.62 \\
Unet~(w/o mask) ~\cite{Isola:2016tp}  &  29.86     &    0.9518     &   28.96  \\
Ours w/o Attention Loss &  26.08 & 0.9608 & 30.10  \\
Ours w/o GAN Loss        &   23.93   &  0.9622     &   30.89  \\
Ours w/o post-processing    & 23.86 & 0.9590 & 30.61 \\
Ours (Unet+SAD)  &   \textbf{20.73}    &    \textbf{0.9657}     &  \textbf{31.15}  \\
\hline
\end{tabular}
\end{center}

\caption{We evaluate the proposed method for image harmonization without the ground truth mask as input on the S-COCO datasets.}
\label{tab:umask}
\vspace{-1.5em}
\end{table}

\subsubsection{Evaluation of Attention Loss}
The key component of our method for image harmonization without the mask is attention loss. We tuning the choice of attention loss of , \textit{Binary Cross Entropy} under a subset of S-COCO dataset. This subset contains 10k training images and we test it on the same test dataset as the full S-COCO dataset. We choose this subset S-COCO for evaluating the attention loss because it trains 4 faster than the original dataset. As shown in the red line and blue line in Fig.~\ref{fig:attpsnr},  loss is a more suitable choice for attention loss than \textit{Binary Cross Entropy} under the same ratio between the pixel loss. We also try to find the best proportion by rescaling the percentage of the attention loss to 0.1, 0.01,10. From the Fig.~\ref{fig:attpsnr}, it is clear that the best proportion between attention loss and pixel-level loss is .

\begin{figure}[h]
  \includegraphics[width=\columnwidth]{figures/psnr}
  \caption{The Evaluations of the attention loss on the Subset of S-COCO. We plot the PSNR on the full test set in each epoch when we train the network with different attention loss. We also tune the hyper-parameters  in Equation.\ref{eq:att}.  means the proportion between the pixel level loss parameters  and our attention loss . This figure is best view in color.}
    \label{fig:attpsnr}
    \vspace{-1.5em}
\end{figure}

\begin{figure*}[h]
\centering     

\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/input_180}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/mask_180}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_basicnaiveuno_180}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/zhu_180}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_mmaskedgannaivemmucrossx_180}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/attention_180}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/target_180}}


\bigskip
\vspace{-2em}

\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/input_952}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/mask_952}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_basicnaiveuno_952}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/zhu_952}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_mmaskedgannaivemmucrossx_952}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/attention_952}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/target_952}}

\bigskip
\vspace{-2em}

\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/input_149}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/mask_149}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_basicnaiveuno_149}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/zhu_149}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_mmaskedgannaivemmucrossx_149}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/attention_149}}
\subfigure{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/target_149}}

\bigskip
\vspace{-2em}
\addtocounter{subfigure}{-21}

\subfigure[Input]{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/input_905}}
\subfigure[Tampered Mask]{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/mask_905}}
\subfigure[Unet~\cite{Isola:2016tp}]{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_basicnaiveuno_905}}
\subfigure[Zhu~\cite{Zhu:2015tl}]{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/zhu_905}}
\subfigure[Ours]{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/_mmaskedgannaivemmucrossx_905}}
\subfigure[Attention Map]{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/attention_905}}
\subfigure[Target]{\centering\includegraphics[width=0.28\columnwidth]{figures/coco/target_905}}

\caption{The Comparison results of our methods and other baseline methods on the S-COCO datasets without the ground truth mask as input. We also plot the attention map created by our method and the ground truth map here for comparison.}

\label{fig:unmask}
\vspace{-1.5em}
\end{figure*}

\subsection{Attention Mechanisms Analysis}
\label{sec:attention}
We first evaluate the proposed SAM against other state-of-the-art attention mechanisms: \textit{Squeeze-and-Excitation Network~(SE-Block)}~\cite{Hu:2017tf} and \textit{Convolution Block Attention Module~(CBAM)}~\cite{Woo:2018wr} for image harmonization task in Section.~\ref{sec:evaatt}. Their methods are proposed for image classification tasks, so we replace the original skip-connection in Unet for comparison with the proposed SASC. Besides, we increase the channels in each levels of the original Unet to match the parameters of our Unet+SAM model for a fair comparison~( More channels in Table.~\ref{table:evaluation}). Finally, we compare the attention modules with the basic CONV-Block. Each CONV-Block block has the structure of CONV~()-ELU-BN-CONV~()-ELU-BN, which is the same as the learning-able block in SAM. Then, we conduct a detailed ablation study of our module in Section.~\ref{sec:astudy} and Section.~\ref{sec:attaly}. All the methods are experimented on S-COCO dataset in the same framework with default configurations.



\subsubsection{Evaluation of Attention Modules}
\label{sec:evaatt}




As illustrated in Table.~\ref{table:evaluation}, Unet+CONV-Block and Unet+SE-Block show poor performance than the baseline method for similar reasons. In these methods, the convolution operators are performed on the whole image and their method cannot learn the disparities in the spliced region and preserve the features in the non-spliced region simultaneously. SE-Block shows better results than CONV-Block because SE-Block learns the channel attention additionally in the skip-connection.
Moreover, it is not surprising that CBAM gets better results than the baseline since the spatial attention part in CBAM can learn the specific region automatically, and the input mask of our network makes it easy.
However, our unet+SASC method gets a better result than other attention modules with a large margin. This is because, with the help of the hard-coded mask, we can design more channel attention modules for different purposes and learning specifically while the SE-Block and CBAM can only focus on certain channels or certain regions by learning from data. Besides, the baseline network with more parameters also improve the performance, however, it still far worse than our method. Overall, the benefits of our attention module are obvious. From Fig.\ref{fig:attentionmodule}, our method gets a cleaner difference colormap in the background than other attention modules for their methods only consider  soft attention. Another improvement between Figure.\ref{fig:attention_cbam} to Figure.\ref{fig:attention_rasc} is the boundary of the spliced mask. From the figure, we can find our method gets better harmonized edges while the ground mask is not always true while other methods fail. It is probably because the soft spatial attentions in CBAM can not always get an accurate attention map even with the input ground truth mask while we use the hard-coded mask and Gaussian Filter in the module. 





\begin{table}[h]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Method & MSE & SSIM & PSNR \\
\hline\hline
baseline unet~\cite{Isola:2016tp}  & 17.28         & 0.9757          & 33.55        \\
~~ More Channels  & 16.93  & 0.9770  & 33.80  \\
~~ CONV-Block   & 17.85 &  0.9757 & 33.34  \\
~~ SE-Block~\cite{Hu:2017tf} & 17.45 & 0.9762 & 33.42 \\
~~ CBAM~\cite{Woo:2018wr}  & 16.50 & 0.9768 & 34.32 \\
\textbf{unet with SASC (Ours)} & 15.59 & \textbf{0.9790} & \textbf{34.74} \\
 ~~  & 17.93 & 0.9758 & 33.32 \\
 ~~  & 15.80 & 0.9779 & 34.49 \\
 ~~ Gaussian Mask & 15.67 & 0.9788 & 34.72 \\
 ~~ 6 layers  SASC  & 15.66 & 0.9787 & 34.73 \\
 ~~ Learning block() & \textbf{15.49} & 0.9781 & 34.73  \\
\hline
\end{tabular}
\end{center}

\caption{The evaluation of SAM with the different attention modules and the ablation study of SAM on S-COCO dataset.}
\label{table:evaluation}
\vspace{-2em}
\end{table}
 
\begin{figure}[h]
\centering   
\subfigure[Input]{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/inputx}}
\subfigure[Mask]{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/mask_329}}
\subfigure[Target]{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/target}}

\par\bigskip\vspace*{-2em}
\subfigure{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/senet}}
\subfigure{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/cbam}\label{fig:attention_cbam}}
\subfigure{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/rasc}\label{fig:attention_rasc}}


\par\bigskip\vspace*{-2em}
\addtocounter{subfigure}{-3}
\subfigure[SE-Block\cite{Hu:2017tf}]{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/senet_jet_329}}
\subfigure[CBAM\cite{Woo:2018wr}]{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/cbam_jet_329}}
\subfigure[SAM~(Ours)]{\centering\includegraphics[width=0.30\columnwidth]{figures/figureattention/radhnv3xgaussian_jet_329}}


\caption{Comparisons on different attention modules. We plot the results of Unet+SE-Block, Unet+CBAM and Unet+SAM~(Ours) for comparison. For each method, we show the harmonized results of the red region and the absolute difference colormap between the result and target image. It is clear that our Unet + SAM method gains better results than others in the boundary of arms (best view with zoom in) and the absolute difference colormaps. Moreover, our method also shows a better prediction in the non-spliced region. The colormaps are enlarged 30 for better visualize the differences in the non-spliced region.}
\label{fig:attentionmodule}
\vspace{-1.5em}
\end{figure}
 

\subsubsection{Ablation Study}
\label{sec:astudy}
We conduct ablation experiments on the inner structure of the proposed SAM model under \textit{\textbf{SASC}}. All the experiments are performed on the S-COCO dataset with the same configuration. We illustrate all the numerical results in Table.~\ref{table:evaluation}.

\textbf{Without .} {By comparison with the baseline, Unet w/o  gets slightly worse results because the hard-coded mask will force the learning-able block  to learn the necessary and unnecessary features altogether without any identification.}




\textbf{Without .} We remove the  in SASC module for necessity. As shown in Table.~\ref{table:evaluation}, compared with SASC, the method without  performs slightly worse because  selects the original features in the spliced region which is unnecessary to transfer.


\textbf{Without Gaussian Filter.} A detailed explanation of Gaussian Filter has been introduced in the Section.\ref{sec:rasc}. Here, we report the results of experiments. As shown in Fig.~\ref{fig:gaussian}, the Gaussian Filter shows better performance in the boundary of spliced edges. We also report the numerical results in Table.~\ref{table:evaluation}, it is obvious that the Gaussian Filter improves the performance of our module.

\begin{figure}[h!]
\centering     \subfigure[w/o Gaussian Filter]{\centering\includegraphics[width=0.45\columnwidth]{figures/wo_gaussian}}
\subfigure[w/  Gaussian Filter]{\centering\includegraphics[width=0.45\columnwidth]{figures/w_gaussian}}
\caption{Gaussian Filter smooth the binary mask to fit object. For example, the little girl in the image is the tampered region with inaccuracy spliced mask. From two zoom-in regions in the image, the method with Gaussian Filter show better harmonization boundary.}
\label{fig:gaussian}
\vspace{-1.5em}
\end{figure}


\textbf{With 6 layers SASC.} The model gets a slightly worse performance when we  replace all the skip-connection with SASC in Unet (Unet  6 layers SASC in Table.~\ref{table:evaluation}). This experiment also fit our assumption: the style is much more related to low-level features, and the high-level features should be the same. However, in all 6 layers SASC, our method still gains much better results than the baseline. 




\textbf{Without learning-able Block.} We evaluate the importance of our learning-able block. As shown in Table.\ref{table:evaluation}, the methods without learning-able block  show worse results than our full method, especially in SSIM. This is because SSIM measures the image quality from the luminance, contrast, and structure while the small convolutions in the learning-able block will learn more detail. 



\subsubsection{Attention Analysis}
\label{sec:attaly}

We also analyze the detail responses of all three \textit{Channel Attention Modules} in our SAM modules. We use \textbf{\textit{SASC}} in unet structure for evaluating for the features used in SASC all comes from the encoder. As shown in Fig.~\ref{figure:attention_analysis}, 
we visualize the weighting of  in the three layers of SASC modules at the bottom of each sample. 
Particularly, the blacker block indicates the channel weighting is low while the whiter block means it is important for filtered features. So our first observation is that our SASC module allocates and weights the features according to the input images by comparing the response maps between two images. Moreover, Another interesting observation is that there are more white blocks in  than  and  in the  module while the opposite phenomenon is shown in . This fact perfectly explains our assumption: the coarser features in the spliced region need to learn specifically while the high-level features are almost the same.


\begin{figure}[h]
	\centering
	\includegraphics[width=\columnwidth]{figures/attention}

	\caption{From coarse-to-fine, we plot the first  attention channels response on three different levels of SASC in network structure named ,  and , respectively. For each level of attention, we visualize the Channel Attention Module with the order ,  and  from the top to the bottom.}
	\label{figure:attention_analysis}

\end{figure}

