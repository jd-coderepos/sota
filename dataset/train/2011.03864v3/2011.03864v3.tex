\documentclass[tablecaption=bottom,pmlr]{jmlr}





\usepackage{booktabs}
\usepackage[load-configurations=version-1]{siunitx} 

 \usepackage{xurl}

\newcommand{\cs}[1]{\texttt{\char`\\#1}}

\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

\jmlrvolume{1}
\jmlryear{2021}
\jmlrsubmitted{submission date}
\jmlrpublished{publication date}
\jmlrworkshop{NeurIPS 2020 Preregistration Workshop} 

\title{Latent Neural Differential Equations\;\\ for Video Generation}












\author{\Name{Cade Gordon} \Email{cadegordonml@gmail.com}\and
   \Name{Natalie Parde} \Email{parde@uic.edu}\\
   \addr Department of Computer Science \\ University of Illinois at Chicago}








\begin{document}

\maketitle

\begin{abstract}
  Generative Adversarial Networks have recently shown promise for video generation, building off of the success of image generation while also addressing a new challenge: time. Although time was analyzed in some early work, the literature has not adequately grown with temporal modeling developments. We study the effects of Neural Differential Equations to model the temporal dynamics of video generation. The paradigm of Neural Differential Equations presents many theoretical strengths including the first continuous representation of time within video generation. In order to address the effects of Neural Differential Equations, we investigate how changes in temporal models affect generated video quality. Our results give support to the usage of Neural Differential Equations as a simple replacement for older temporal generators. While keeping run times similar and decreasing parameter count, we produce a new state-of-the-art model in 6464 pixel unconditional video generation, with an Inception Score of 15.20.
\end{abstract}

\section{Introduction}

Generative modeling remains an important problem within computer vision, with new developments providing a better understanding of high-dimensional data modeling and even aiding the supervised learning sphere. Good representations of distributions improve
feature space visualisation, clustering, and classification.  Many approaches have tackled the problem of representing a distribution, including Generative Adversarial Networks (GANs) \citep{gans}, which have recently shown immense potential for image generation. As time progresses GANs become more robust, allowing for greater image size \citep{dcgan,proggan,biggan} and quality \citep{stylegan1,stylegan2}.

The success of GANs in image generation propelled them towards being the prominent methodology for video generation. However, the application of GANs to video generation has come with new challenges. Adding time to the preexisting color, width, and height dimensions has increased computational costs and complexity by an order of magnitude. Early models generated videos of a meagerly 64 by 64 pixels \citep{vgan,tgan,mocogan}. The addition of the new temporal component not only restricted video size, it also opened many questions regarding the best way to navigate an entirely new dimension. The first model to use GANs for video generation was VGAN \citep{vgan}, which used 3D convolutional kernels to account for time, framing it as no more than an extra feature channel blended in with color, width and height.

Treating temporal features as a separate dimensional scope allowed for the subsequent TGAN \citep{tgan} to outperform VGAN in terms of Inception Score (IS) \citep{inceptionscore}. The authors proposed two separate generative architectures: a 1D convolutional temporal generator and an image generator. Further investigation of the temporal latent space was done by MoCoGAN \citep{mocogan}, in which the authors proposed decomposing the image generator's input into a single content vector and an evolving motion vector. Experimentation has also gone into increasing frame size and network depth, with some reflections on computational mitigation \citep{tganv2,dvdgan,moflowgan}, but little work has gone into rigorously examining time.

Our work reopens the discussion of the temporal latent space. After the revelation of separate temporal generation, researchers have stopped asking questions about the temporal  generator. Works after TGAN employed Long Short-Term Memory (LSTM) \citep{lstm} or Convolutional LSTM (CLSTM) \citep{clstm} blocks. To this day, the LSTM remains and has never been fully ablated or examined with control. A similar previously unproven but accepted notion was content motion decomposition. First published in 2018 as part of MoCoGAN \citep{mocogan}, content and motion decomposition was not ablated until the publication of MoFlowGAN \citep{moflowgan} in 2020. With very limited analysis, much of the temporal space remains an open question within these models.

While able to model temporal dynamics, recurrent models such as the LSTM and its variants only represent discrete samples. We propose to re-explore the temporal space under a continuous paradigm. Neural Ordinary Differential Equations (NODEs) \citep{neuralode} offer the potential for a continuous representation of the temporal dimension. Extending the paradigm of Neural Differential Equations, we propose the first continuous video generation model.  Our work makes the following contributions:

\begin{itemize}
    \item We establish the first continuous GAN for video generation.
    \item We experiment with multiple novel architectures for video generation.
    \item We analyze how changes in the temporal latent space modality affect visual fidelity through an ablation study.
\end{itemize}



\section{Related Work}

\subsection{Generative Adversarial Networks}

Two neural networks compose GANs: a Discriminator  and Generator . The generator transforms a sampled noise vector  from a distribution  and maps it to an image (or in our case a video). The Discriminator functions by taking an input image or video  and mapping it to a value representing whether it believes  is sampled from the real distribution  or the distribution produced by the generator . The two compete to minimize or maximize a loss function that may be represented generically as shown below, where  is a function of the Discriminator's prediction and the truth label represented as  (real) or  (fake):



 is typically the identity function, cross entropy function, or hinge loss function. Loss choice has been shown to be less consequential so long as a Lipschitz constraint is met \citep{lossfuncgan}. GANs are often difficult to train, and two approaches to increasing their stability during training time are through applying a form of Lipschitz constraint or multi-scale generation. WGAN \citep{wgan} and WGAN-GP \citep{wgangp} showed the effectiveness of the Lipschitz regularization. SNGAN \citep{sngan} subsequently showed a refined way to enforce the constraint through spectral normalization. Progressive GAN \citep{proggan} stabilized training by increasing the generated resolution over time.

\subsection{Video Prediction}

Video prediction conditions a model on a sample of frames, and models the subsequent frames. A common approach is the use of a recurrent architecture such as an LSTM \citep{vidrecurrent1,vidrecurrent2,vidrecurrent3,vidrecurrent4,vidrecurrent5,vidrecurrent6}. Another common methodology is using optical flow \citep{flow1,flow2,flow3,flow4}. Prior work has also explored the stochastic nature of videos \citep{stochasticgen,stochasticvar,savp,slatentres,stochhighfid}.


\subsection{Video Generation}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{videogen.pdf}
    \caption{A latent variable  is transformed by  into a series of temporal vectors . Each temporal vector  is concatenated with  and transformed into an image. Said images are joined to compose a video.}
    \label{fig:vid_gen}
\end{figure}

To the best of our knowledge, the first work to use a GAN to generate videos was VGAN \citep{vgan}. VGAN generated videos using  spatio-temporal convolutions with 3D kernels and fractional strides, separately generating the motion and background. In order to combine the two it used a learned mask to produce the final output. 

Its successor, TGAN \citep{tgan}, separately generated temporal and frame features. TGAN transformed a single noise vector into multiple vectors accounting for time with a temporal generator , a series of 1D convolutions. The generated vectors concatenated with the starting single noise vector were then fed into an image generator . By separating temporal generation into its own process, TGAN outperformed VGAN \citep{inceptionscore}. The general form of  may be seen in Figure \ref{fig:vid_gen}.

MoCoGAN \citep{mocogan} continued in the line of temporal manipulation by using an LSTM to generate temporal features. The authors assumed that the temporal space was composed of a motion and content subspace. Though their work outperformed TGAN, it was not until MoFlowGAN \citep{moflowgan}, two years later, that the content and motion decomposition was fairly ablated, with results showing that it led to a positive increase in IS. It is hard to know for many of these models which features actually allowed for their success, since the discriminator and image generation architectures change and increase in complexity from one paper to the next. In light of this, a properly controlled analysis of our proposed model will fill in many of the gaps in the current literature.

Other papers focus on increasing the dimension of the video output. DVD-GAN and MoFlowGAN \citep{dvdgan, moflowgan} produce 128x128 pixel videos. The current state-of-the-art TGANv2 \citep{tganv2} even boasts 192x192 pixel videos. Recently TGAN-F \citep{tganf} further improved performance by simplifying the discriminator of TGAN.

\subsection{Neural Differential Equations}

NODEs \citep{neuralode} transformed the vision of ResNets \citep{resnet} by giving them a continuous definition. Instead of the singular discrete additions of a neural network function , they proposed integrating using ordinary differential equation (ODE) solvers.  The new interpretation allows for an approximate continuous temporal representation, where  represents the hidden state of a layer, and  represents the ordering of layers:




We use  to represent time.
Works like  \citep{ode2vae} extended this to second order ODEs. Much of the work surrounding NODEs revolves around Variational Autoencoders \citep{neuralode, ffjord, ode2vae}. Recently, even more differential equation families have been explored; Neural Stochastic Differential Equations (NSDEs) are a successful example \citep{sde, sde2}.

\section{Neural Differential Equation Video GANs}

There is a significant gap in the literature explaining the choice for temporal generators. To remedy this, we propose to explore it under a paradigm common to general physics: using differential equations to represent temporal dynamics. While using historical image generator functions, we will observe changes in performance metrics with different temporal generator functions. Comparisons between the families of generative functions may be visualized by Figure \ref{fig:latent_space}.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{traversal.pdf}
    \caption{When compared with typical LSTMs, neural differential equations have more frequent observations, and SDEs have greater potentiality for solutions.}
    \label{fig:latent_space}
\end{figure}

\subsection{Ordinary Differential Equations (ODEs)}

Instead of an auto-regressive LSTM or 1D Kernel, a differential equation may be used to model the evolution of a latent variable . By a learned function , future  may be found by integrating:




The image generator  may then produce an image from . Using a differential equation the model may account for the finer nuances of traversing the latent space accounting for motion in . LSTMs only view sparse time steps; the model moves from  to  never accounting for a . NODEs allow for the intermediate  values to be traversed, which may potentially lead to better performance as this can more closely approximate a latent trajectory.

The family of NODEs also allows for higher order interpretations of the model. Our  may represent higher orders than simple , such as  or higher. A first-order ODE parameterizes more immediate changes during integration, whereas higher orders represent much more long-term shifts, such as concavity, in the latent variable.

\subsection{Stochastic Differential Equations (SDEs)}

NODEs allow for path approximations in determinate systems. Every  will produce a single , but this isn't reflective of how videos truly function. There is an inherent stochastic nature to how videos progress---actors have a branching tree of decisions and so do particles for their motion. NSDEs may be a good way to represent the random nature present in videos, offering all of the benefits of NODEs while allowing for randomness with their added noise. Under this form, and letting  and  represent drift and diffusion respectively, we find  with:



Each of  and  are parameterized by a neural network.  is a Wiener process, a continuous series of values with Gaussian increments.  The validity of this formulation may be exemplified by thinking about a video of a face changing expressions. If the actor starts out with a neutral face they may then produce a sad one after that. However, a smile would be equally likely. By injecting randomness either path may be explored by the model.

\subsection{Benefits of Differential Equations}

Differential equations allow for increased control over how paths are traversed because of their continuous properties. Because  is found by integration, there are two unique characteristics that other modalities do not possess. First,  can be integrated backwards in time allowing the discovery of . This can be thought of as what happens before the first frame. Second, if increased frame rates are desired, they can easily be accounted for. The differential equation solver will necessitate evaluations of . To achieve a higher frame rate, the image generator simply needs to sample some of the intermediate  evaluations. Control like this is impossible in recurrent models.



\section{Experimental Protocol}

The most widely used and comparable metrics for video generation are IS and Fr\'echet Inception Distance (FID) \citep{ttur}. These are calculated by a C3D model \citep{c3d} pretrained on the UCF101 dataset \citep{ucf101}.
Their values quantify visual fidelity of the generated videos. We observe changes to these metrics as we alter . We train the following models on UCF101:
\begin{itemize}
    \item TGAN
    \item MoCoGAN
    \item TGANv2 (used for Effects of Family and Order only)
\end{itemize}

Each model will run for 100,000 epochs using the model's originally proposed hyperparameters. IS will be calculated on samples of 2,048 videos every 2,000 epochs. The epoch with the highest IS will be used to calculate the model's final statistics. Using the best performing epoch, five batches of 2,048 videos will be created. FID and IS will be calculated on each batch, and we will report the mean value and standard deviation for each metric.


\subsection[Finding f(x)]{Finding }

In order to generate videos under this paradigm an appropriate neural network architecture for  needs to be studied. To find , TGAN and MoCoGAN will be trained under different s. For each,  will be found by integrating  as a first order NODE. Ablation will occur with the following s:  using a single learned layer with a nonlinearity;  where , with 
and  being also single learned layers with a nonlinearity; and the same functions as the previous setup but with  and  equalizing parameters of each model's original . Testing these choices of 
across both TGAN and MoCoGAN allows for greater evidence for or against how well each  generalizes to the task and architecture.

\subsection{Effects of Family and Order}

With an effective , we can ablate the multiple families and orders. TGAN, MoCoGAN, and now TGANv2 will be tested under the following motion generators: the model's original , the first order ODE, the second order ODE, the third order ODE, and the SDE. For each configuration we will report IS and FID using the process specified earlier.

\section{Implementation}

In this section we further detail model architectures and explain minor alterations to the planned experimental design. All experiments are performed on an NVIDIA RTX 2080 TI GPU and are written in PyTorch. To promote future research and replication we make our code available to the community.\footnote{\url{https://github.com/Zasder3/Latent-Neural-Differential-Equations-for-Video-Generation}}

\subsection{Further Specification}

As outlined above, said  designs necessitated careful measures to make them functional and fairly comparable to their original model's counterparts. In all models except TGANv2,  composes the final or intermediate activation function. It satisfies the continuous and Lipschitz constraint for uniqueness specified by \citet{neuralode}. Furthermore, as the final layer it permits both positive and negative resulting values stopping  from being monotonically increasing with time.

Additionally, we feed the starting noise vector through a fully connected network (FCN) aiming to equalize the number of nonlinearities. This design choice originated from comparing the number of activation functions in  in our experimental groups to those of the original models. For example, in TGAN the temporal generator is composed of four ReLUs and a final . As it stands,  has only one nonlinearity. To amend the nonlinearity gap between our proposed  and the original models, we prepended an FCN to  with equal nonlinearity count to the original model's . This means when integrating, individual s will be in a comparatively complex space. We follow this protocol of prepending the FCN to the integration for all experiments except those with TGANv2 and MoCoGAN with equal parameterization. 

\subsection{Deviations from Original Plans}

Instead of calculating IS every 2,000 training iterations, we calculated IS every 1,000 iterations. We also increased the number of samples used to compute an IS and FID mean and standard deviations. The original 5 measures became 10 to increase precision and to become more inline with that of TGANv2's protocol. 

We also found it infeasible to compute every  family variation of TGANv2 under our setup due to limited computational resources. Opting to train only a first order ODE, with a batch size of 32, the model took three days to train on an A100 GPU costing over \f(x)f(x)\pm\pm\pm\pm\pm\pm\pm\pmf(x)f(x)\pm\pm\pm\pm\pm\pm\pm\pmf(x)f(x)\times\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pmf(x)\times$64 pixel image generation.

The results of this work reopen the case for investigating the temporal generator and provide a novel direction for others to build upon. We are eager to see the outcomes of researchers' efforts as they scale the video size, use the models under different problem formulations, and increase the frame-rate to further explore this paradigm.

\bibliography{refs.bib}

\end{document}
