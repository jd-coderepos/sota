\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{mathtools}
\usepackage{empheq}
\usepackage{bbold}
\usepackage{color}
\usepackage[hidelinks]{hyperref}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{units}
\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}
\usepackage{cite} \usepackage{dblfloatfix}


\newcommand*{\bPhi}[1]{\boldsymbol{\Phi}_{\mathbf{#1},l}}
\newcommand*{\bhPhi}[1]{\widehat{\boldsymbol{\Phi}}_{\mathbf{#1},l}}
\newcommand*{\bgamma}[1]{\boldsymbol{\gamma}_{\mathbf{#1},l}}
\newcommand*{\bhgamma}[1]{\widehat{\boldsymbol{\gamma}}_{\mathbf{#1},l}}
\newcommand*{\hxi}{\widehat{\xi}_l}
\newcommand*{\mbf}[1]{\mathbf{f}^{l}_{\mathbf{#1}}}
\newcommand*{\f}[1]{f^{l}_{#1}}
\newcommand*{\bh}[1]{\mathbf{h}_{\mathbf{#1},l}}
\newcommand*{\bhh}[1]{\widehat{\mathbf{h}}_{\mathbf{#1},l}}
\newcommand*{\bhH}[1]{\widehat{\mathbf{H}}_{\mathbf{#1},l}}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\conj}{^{\mathsf{*}}}
\newcommand*{\hermconj}{^{\mathsf{H}}}

\title{Deep Multi-Frame MVDR Filtering for\\
	Single-Microphone Speech Enhancement}
\name{Marvin Tammen, Simon Doclo
	\thanks{This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project ID 390895286 – EXC 2177/1.}}
\address{Department of Medical Physics and Acoustics and Cluster of Excellence Hearing4all\\
	University of Oldenburg, Germany\\marvin.tammen, simon.doclo@uni-oldenburg.de}
\begin{document}
\ninept
\maketitle
\begin{abstract}
	Multi-frame algorithms for single-microphone speech enhancement, e.g., the multi-frame minimum variance distortionless response (MFMVDR) filter, are able to exploit speech correlation across adjacent time frames in the short-time Fourier transform (STFT) domain.
	Provided that accurate estimates of the required speech interframe correlation vector and the noise correlation matrix are available, it has been shown that the MFMVDR filter yields a substantial noise reduction while hardly introducing any speech distortion.
	Aiming at merging the speech enhancement potential of the MFMVDR filter and the estimation capability of temporal convolutional networks (TCNs), in this paper we propose to embed the MFMVDR filter within a deep learning framework.
	The TCNs are trained to map the noisy speech STFT coefficients to the required quantities by minimizing the scale-invariant signal-to-distortion ratio loss function at the MFMVDR filter output.
	Experimental results show that the proposed deep MFMVDR filter achieves a competitive speech enhancement performance on the Deep Noise Suppression Challenge dataset.
	In particular, the results show that estimating the parameters of an MFMVDR filter yields a higher performance in terms of PESQ and STOI than directly estimating the multi-frame filter or single-frame masks and than Conv-TasNet.
\end{abstract}
\begin{keywords}
	Single-Microphone Speech Enhancement, Multi-Frame Filtering, Temporal Convolutional Networks
\end{keywords}
\section{Introduction}
In many hands-free speech communication systems such as hearing aids, mobile phones and smart speakers, ambient noise may degrade the speech quality and intelligibility of the recorded microphone signals. 
To alleviate this issue, several single- and multi-microphone speech enhancement algorithms have been proposed ~\cite{vary_digital_2006,hendriks_dft-domain_2013,benesty_speech_2011,doclo_multichannel_2015,vincent_audio_2018}. 
Single-microphone speech enhancement algorithms typically 1) transform the noisy time-domain signal to a domain that is better suited for speech enhancement, e.g., the short-time Fourier transform (STFT) domain, 2) apply a (real- or complex-valued) gain/mask to the transform-domain coefficients to obtain an estimate of the clean speech, and 3) transform the modified coefficients back to the time-domain.
For such single-frame algorithms, many traditional model-based approaches~\cite{vary_digital_2006,hendriks_dft-domain_2013,ephraim_speech_1984,gerkmann_phase_2015} as well as supervised learning-based approaches~\cite{xu_regression_2015,wang_training_2014,kolbaek_speech_2017,luo_conv-tasnet_2019,tan_gated_2019} have been proposed.
A disadvantage of single-frame algorithms is that attenuation of the noise component may be accompanied by some distortion of the speech component in the enhanced signal. 

In contrast to single-frame algorithms, multi-frame algorithms have been proposed which apply a complex-valued filter to the noisy speech STFT coefficients~\cite{benesty_speech_2011}.
Also for multi-frame algorithms both model-based approaches such as the multi-frame minimum variance distortionless response (MFMVDR) filter~\cite{huang_multi-frame_2012,schasse_estimation_2014,fischer_sensitivity_2017,fischer_subspace-based_2020} as well as supervised learning-based approaches~\cite{mack_deep_2019,xu_neural_2020} have been proposed.
The MFMVDR filter has been derived to explicitly take speech correlations across adjacent time frames into account and requires an estimate of the noise correlation matrix and the so-called speech interframe correlation (IFC) vector in each time-frequency bin.
Although it has been shown that the MFMVDR filter can yield a good noise reduction performance and little speech distortion~\cite{huang_multi-frame_2012,fischer_sensitivity_2017}, its performance is very sensitive to estimation errors of the required quantities, in particular the speech IFC vector~\cite{fischer_sensitivity_2017}. 

To estimate the speech IFC vector from the noisy speech STFT coefficients, several model-based approaches have been proposed.
In \cite{schasse_estimation_2014} a maximum likelihood approach has been derived, assuming that the speech and noise IFC vectors follow multi-variate Gaussian distributions.
This approach requires an estimate of the a-priori signal-to-noise ratio (SNR), which can be estimated, e.g., using the decision-directed approach~\cite{ephraim_speech_1984} or using a supervised learning-based approach~\cite{tammen_dnn-based_2020}.
In \cite{fischer_subspace-based_2020} a subspace estimator has been proposed, which is based on a low-rank speech model.
However, simulation results have shown that estimating the required quantities from the noisy speech STFT coefficients using these model-based approaches typically results in a large performance degradation compared to the oracle MFMVDR filter.

In this paper we propose to embed the MFMVDR filter within a deep learning framework as shown in Fig. \ref{fig:block diagram}.
More in particular, we propose to train temporal convolutional networks~\cite{bai_empirical_2018,luo_conv-tasnet_2019} to map the noisy speech STFT coefficients to the required quantities, i.e., the noise correlation matrix and the a-priori SNR, by minimizing the scale-invariant signal-to-distortion ratio loss function~\cite{roux_sdr_2019} at the MFMVDR filter output.
Experimental results using the INTERSPEECH 2020 Deep Noise Suppression (DNS) Challenge dataset~\cite{reddy_interspeech_2020} show that the proposed deep MFMVDR filter outperforms complex-valued masking as well as directly estimating the multi-frame filter without exploiting the MFMVDR structure and Conv-TasNet~\cite{luo_conv-tasnet_2019}.

\section{Signal Model}
\begin{figure*}
	\centering
	\def\svgwidth{\linewidth}
	\begingroup \makeatletter \providecommand\color[2][]{\errmessage{(Inkscape) Color is used for the text in Inkscape, but the package 'color.sty' is not loaded}\renewcommand\color[2][]{}}\providecommand\transparent[1]{\errmessage{(Inkscape) Transparency is used (non-zero) for the text in Inkscape, but the package 'transparent.sty' is not loaded}\renewcommand\transparent[1]{}}\providecommand\rotatebox[2]{#2}\newcommand*\fsize{\dimexpr\f@size pt\relax}\newcommand*\lineheight[1]{\fontsize{\fsize}{#1\fsize}\selectfont}\ifx\svgwidth\undefined \setlength{\unitlength}{422.9270058bp}\ifx\svgscale\undefined \relax \else \setlength{\unitlength}{\unitlength * \real{\svgscale}}\fi \else \setlength{\unitlength}{\svgwidth}\fi \global\let\svgwidth\undefined \global\let\svgscale\undefined \makeatother \begin{picture}(1,0.20203211)\lineheight{1}\setlength\tabcolsep{0pt}\put(0,0){\includegraphics[width=\unitlength,page=1]{block_diagram_icassp2021.pdf}}\put(0.8819273,0.08432238){\color[rgb]{0,0,0}\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}loss \eqref{eq:loss}\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=2]{block_diagram_icassp2021.pdf}}\put(0.68271096,0.02413722){\color[rgb]{0,0,0}\makebox(0,0)[lt]{\lineheight{1.25}\smash{\begin{tabular}[t]{l}:trainable\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=3]{block_diagram_icassp2021.pdf}}\put(0.78178486,0.0241372){\color[rgb]{0,0,0}\makebox(0,0)[lt]{\lineheight{1.25}\smash{\begin{tabular}[t]{l}:non-trainable\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=4]{block_diagram_icassp2021.pdf}}\put(0.60232619,0.09655921){\color[rgb]{0,0,0}\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}MFMVDR\\filter \eqref{eq:MFMVDR}\end{tabular}}}}\put(0.08564073,0.0849539){\color[rgb]{0,0,0}\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}+\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=5]{block_diagram_icassp2021.pdf}}\put(0.32147359,0.10809544){\color[rgb]{0,0,0}\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\\a-priori\\SNR \eqref{eq:a-priori snr}\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=6]{block_diagram_icassp2021.pdf}}\put(0.74233915,0.08444589){\color[rgb]{0,0,0}\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}filter \eqref{eq:speech estimate vector}\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=7]{block_diagram_icassp2021.pdf}}\put(0.18159259,0.16803614){\color[rgb]{0,0,0}\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\footnotesize correlation\\\footnotesize matrices \eqref{eq:correlation matrix states}, \eqref{eq:correlation matrices}\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=8]{block_diagram_icassp2021.pdf}}\put(0.18092182,0.10504359){\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=9]{block_diagram_icassp2021.pdf}}\put(0.18091288,0.06306605){\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\end{tabular}}}}\put(0.37684333,0.00459272){\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\end{tabular}}}}\put(0.39130765,0.11069834){\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\end{tabular}}}}\put(0.53144489,0.11218422){\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\end{tabular}}}}\put(0.67158254,0.11218422){\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\end{tabular}}}}\put(0.81171947,0.11218422){\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}\end{tabular}}}}\put(0,0){\includegraphics[width=\unitlength,page=10]{block_diagram_icassp2021.pdf}}\put(0.46212383,0.09626828){\color[rgb]{0,0,0}\makebox(0,0)[t]{\lineheight{1.25}\smash{\begin{tabular}[t]{c}speech IFC\\vector \eqref{eq:speech IFC vector estimate}\end{tabular}}}}\end{picture}\endgroup  	\caption{Block diagram of the training process of the proposed deep MFMVDR filter. 
		The speech enhancement-related loss function is used to update the weights of the temporal convolutional networks estimating the noisy speech and noise correlation matrices  and  as well as the a-priori SNR .}
	\label{fig:block diagram}
\end{figure*}
We consider an acoustic scenario with a single microphone recording one speech source and additive ambient noise. 
In the STFT-domain, the noisy microphone signal is given by

where , , and  denote the noisy speech component, the speech component, and the noise component, respectively, at the -th frequency bin and the -th time frame. 
Since all frequency bins are assumed to be independent, the index  will be omitted in the remainder of this paper.

In single-frame speech enhancement algorithms~\cite{vary_digital_2006,hendriks_dft-domain_2013,ephraim_speech_1984,gerkmann_phase_2015,wang_training_2014,kolbaek_speech_2017,luo_conv-tasnet_2019,tan_gated_2019}, the speech component  is typically estimated by applying a (real- or complex-valued) gain/mask  to the noisy speech STFT coefficients, i.e.,

In multi-frame speech enhancement algorithms~\cite{benesty_speech_2011}, the -dimensional noisy speech vector  is defined as

where  denotes the transpose operator.
Using \eqref{eq:signal model}, the noisy speech vector  can be written as , where the speech vector  and the noise vector  are defined similarly as  in \eqref{eq:multi-frame vector}.
The speech component  is then estimated by applying a (complex-valued) finite impulse response filter  with  taps to the noisy speech STFT coefficients, i.e.,

where  denotes the Hermitian operator.

Assuming that the speech and noise components are uncorrelated, the -dimensional noisy speech correlation matrix , with  the expectation operator, can be written as

with the speech and noise correlation matrices  and .
In~\cite{huang_multi-frame_2012}, it has been proposed to exploit the speech correlation across adjacent time frames by decomposing the speech vector into a temporally correlated and a temporally uncorrelated part, i.e.,

where the (highly time-varying) speech IFC vector  describes the correlation between the current and previous time frames w.r.t. the speech STFT coefficient , i.e.,

where  denotes the complex-conjugate operator,  is an -dimensional selection vector, and  corresponds to the speech power spectral density (PSD).
Using \eqref{eq:mic correlation matrix} and \eqref{eq:gamma_x}, the speech IFC vector  can be written as

where  denotes the a-priori SNR, with  the noise PSD, and  denotes the noise IFC vector.

\section{Deep Multi-Frame MVDR Filter}
In~\cite{huang_multi-frame_2012,benesty_speech_2011} the MFMVDR filter for single-microphone speech enhancement has been proposed, which aims at minimizing the output noise PSD while not distorting the correlated speech component , i.e.,

Solving this constrained optimization problem yields the MFMVDR filter vector:


To implement the MFMVDR filter, estimates of the noise correlation matrix  as well as the speech IFC vector  are required. 
In \cite{schasse_estimation_2014,fischer_sensitivity_2017} it has been shown that the speech enhancement performance of the MFMVDR filter strongly depends on how well these quantities can be estimated from the noisy speech STFT coefficients.
Previously proposed model-based approaches~\cite{schasse_estimation_2014,fischer_subspace-based_2020} simply use an estimate of the noisy speech correlation matrix  instead of the noise correlation matrix  (leading to the multi-frame minimum \emph{power} distortionless response filter) and estimate the speech IFC vector  based on \eqref{eq:speech IFC vector} by using the decision-directed approach~\cite{ephraim_speech_1984} to estimate the a-priori SNR  and assuming the noise IFC vector  to be constant for all time-frequency points.

In this paper we propose to estimate the required quantities for the MFMVDR filter in \eqref{eq:MFMVDR} from the noisy speech STFT coefficients using a supervised learning-based approach by minimizing a speech enhancement-related loss function at the MFMVDR filter output.

\subsection{Speech IFC Vector}
Due to its highly time-varying nature, the speech IFC vector  is difficult to estimate accurately.
Since preliminary experiments have shown that using \eqref{eq:speech IFC vector} instead of directly estimating  with a DNN yields a higher speech enhancement performance, we propose to estimate  using the estimated correlation matrices  and  as well as the estimated a-priori SNR , i.e.,


\subsection{Correlation Matrices}
\label{sec:correlation matrices}
Since the -dimensional correlation matrices  and  can be assumed to be Hermitian positive semidefinite (PSD), each matrix consists of a total of  real-valued coefficients, denoted by  and .
As illustrated in Fig. \ref{fig:block diagram}, we propose to estimate these coefficients using separate DNNs  and  in state , i.e.,

where  denotes an estimate of , and  denotes the vector of the real and imaginary parts of the noisy speech STFT coefficient .
Since the coefficients  and  are unbounded, a linear activation is used for  and .
The Hermitian PSD correlation matrix estimates are obtained as

where  assembles an -dimensional Hermitian matrix from the (real-valued) -dimensional vector , and the matrix multiplication ensures that the correlation matrix estimates are Hermitian PSD.
It should be noted that no correlation matrix labels are used in the training process --- instead, the DNNs are trained to minimize the speech enhancement-related loss function at the MFMVDR filter output (see Section \ref{sec:settings}).

\subsection{A-Priori SNR}
\label{sec:a-priori snr}
Similarly to the approach described in the previous section, we propose to use a DNN  to map noisy speech features to an a-priori SNR estimate , i.e.,

Since , a softmax activation is used for .
Similarly as for the correlation matrices, the DNN is trained to output an a-priori SNR estimate  such that the speech enhancement-related loss function at the MFMVDR filter output is minimized (see Section \ref{sec:settings}).

\section{Experimental Results}
In this section, the speech enhancement performance of the proposed deep MFMVDR filter is compared to several baseline deep learning-based speech enhancement algorithms (see Section \ref{sec:baselines}).
In Sections \ref{sec:dataset} - \ref{sec:settings} we discuss the used dataset, DNN architecture, and algorithm settings.
In Section \ref{sec:results} we present the results in terms of the perceptual evaluation of speech quality (PESQ)~\cite{itu-t_perceptual_2001} and short-time objective intelligibility (STOI)~\cite{taal_algorithm_2011} improvement.

\subsection{Baseline Algorithms}
\label{sec:baselines}
As baseline single-microphone speech enhancement algorithms, we consider three deep learning-based algorithms:
\begin{enumerate}
	\item \emph{Masking}: in order to investigate the possible benefit of multi-frame filtering, i.e., , we also consider the complex-valued gain/mask in \eqref{eq:masking}, i.e., :
	
	where the bounds for the real and imaginary parts are motivated by \cite{le_roux_phasebook_2019}.
	\item \emph{Direct filtering}: in order to investigate the possible benefit of using the MFMVDR filter structure in \eqref{eq:MFMVDR}, we also consider directly estimating the complex-valued coefficients of the -dimensional multi-frame filter  in \eqref{eq:speech estimate vector}, similarly to \cite{mack_deep_2019}:
	
	where the bounds for the real and imaginary parts are motivated by \cite{mack_deep_2019}.
	\item \emph{Conv-TasNet}~\cite{luo_conv-tasnet_2019}: instead of considering the STFT-domain as the transform-domain for speech enhancement, Conv-TasNet uses learnable transformations and applies a real-valued mask in the transform-domain.
	For a fair comparison with the other considered algorithms, we considered the causal version of Conv-TasNet~\cite{luo_conv-tasnet_2019}.
\end{enumerate}

\subsection{Dataset}
\label{sec:dataset}
All considered algorithms were trained and evaluated on the DNS Challenge dataset~\cite{reddy_interspeech_2020}.
In total, this dataset contains more than \unit[500]{h} of speech from 2150 speakers and \unit[180]{h} of noise from 150 different noise classes at a sampling frequency of \unit[16]{kHz}.
For training and validation, we randomly selected a subset of 45000 utterances of length \unit[4]{s}, with SNRs uniformly sampled from \unit[{[0, 20]}]{dB}.
Using a validation split of \unit[20]{}, this resulted in \unit[40]{h} for training and \unit[10]{h} for validation, respectively.
Evaluation was performed on the DNS Challenge synthetic test set without reverberation.
This test set is disjoint from the training and validation set and includes 20 speakers, 12 VoIP-relevant noise sources, and SNRs uniformly sampled from \unit[{[0, 25]}]{dB}, in total consisting of 150 utterances of length \unit[10]{s}.

\subsection{DNN Architecture}
\label{sec:dnn architecture}
As the DNN architecture for all estimators, we used temporal convolutional networks (TCNs)~\cite{bai_empirical_2018}\footnote{We used the implementation provided by the authors of \cite{luo_conv-tasnet_2019}, available at \href{https://github.com/naplab/Conv-TasNet}{https://github.com/naplab/Conv-TasNet}.}, which have been demonstrated to exhibit strong temporal and spectral modeling capabilities~\cite{luo_conv-tasnet_2019}.
Without performing extensive hyperparameter optimization, we fixed the hyperparameters of all TCN modules (except for the Conv-TasNet baseline, for which we used the hyperparameters proposed in \cite{luo_conv-tasnet_2019}) to 2 stacks of 4 layers each, with a kernel size of 3, resulting in a temporal receptive field of \unit[128]{ms}.
Aiming at a fair comparison, the number of hidden dimensions was varied to obtain a similar total number of trainable weights for all considered algorithms (cf. Table \ref{tab:hyperparams}).
Note that this hyperparameter was varied as opposed to the number of stacks/layers or the kernel size, since increasing the temporal receptive field might give an unfair advantage.
\begin{table}
	\begin{tabularx}{\linewidth}{l|l|l}
		algorithm & hidden dimension & trainable weights\\
		\toprule[2pt]
		masking & 226 & \unit[5.0]{M}\\
		direct filtering & 225 & \unit[5.1]{M}\\
		Conv-TasNet & 128 & \unit[5.0]{M}\\
		\midrule[1pt]
		deep MFMVDR & 128 &\unit[5.3]{M}
	\end{tabularx}
	\caption{TCN module hyperparameters.}
	\label{tab:hyperparams}
\end{table}

\subsection{Algorithm Settings}
\label{sec:settings}
In order to be able to exploit speech correlation, an STFT with high temporal resolution, i.e., a frame length of \unit[8]{ms} and a frame shift of \unit[2]{ms}, was employed for all STFT-based algorithms, similarly as in~\cite{huang_multi-frame_2012}.
A Hann window was used both as analysis and synthesis window.
The multi-frame algorithms (i.e., the proposed deep MFMVDR filter and direct filtering) use a filter length of , such that speech correlation within \unit[16]{ms} can be exploited.
To improve the numerical stability of the matrix inversion in \eqref{eq:MFMVDR}, Tikhonov regularization with a regularization constant  was used~\cite{huang_multi-frame_2012,schasse_estimation_2014}.
Finally, a minimum gain of \unit[-17]{dB} was included in all algorithms except Conv-TasNet.

The TCNs were trained for 50 epochs using the Adam optimizer~\cite{kingma_adam:_2014} with an initial learning rate of .
The learning rate was halved after the validation loss did not decrease for 3 consecutive epochs, and training was stopped early in case the validation loss did not decrease for 10 consecutive epochs.
The gradient norms were clipped to 5, and the batch size was set to 6 to maximize the usage of graphics card memory.
As loss function, we used the negative scale-invariant signal-to-distortion ratio (SI-SDR)~\cite{roux_sdr_2019}, i.e.,

where  and  denote the speech signal and the estimated speech signal in the time-domain.
All algorithms were implemented in PyTorch 1.6.0, and training and evaluation were performed on an NVIDIA GeForce\textsuperscript{\textregistered} RTX 2080 Ti graphics card.

\subsection{Results}
\label{sec:results}
For all considered algorithms, Table \ref{tab:results} shows the average improvement in terms of PESQ and STOI w.r.t. the noisy microphone signals using the speech signal as reference signal.
As can be observed, all considered algorithms yield a significant PESQ and STOI improvement, where the proposed deep MFMVDR filter outperforms all other algorithms.
A minor performance improvement can be observed between direct filtering (~=~) and masking (~=~), hinting at the potential of exploiting multiple frames.
A much larger improvement can be observed between deep MFMVDR filtering and direct filtering, showing that exploiting the MFMVDR filter structure and guiding the TCN training to estimate the required quantities (correlation matrices and a-priori SNR) instead of directly estimating the filter coefficients is advantageous.
Exemplary audio examples for all considered algorithms are available online\footnote{\scriptsize\href{https://uol.de/en/mediphysics-acoustics/sigproc/research/audio-demos}{https://uol.de/en/mediphysics-acoustics/sigproc/research/audio-demos}}.

As a measure for computational complexity, Table \ref{tab:results} also shows the average real-time factor (RTF) for all considered algorithms, defined as , processed using 4 cores of an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} CPU clocked at \unit[2.6]{GHz}.
Although the proposed deep MFMVDR filter results in a larger computational complexity than directly estimating the filter coefficients, its computational complexity is similar to that of Conv-TasNet.
A PyTorch implementation of the deep MFMVDR filter is available online\footnote{\scriptsize\href{https://uol.de/en/mediphysics-acoustics/sigproc/research/code-examples}{https://uol.de/en/mediphysics-acoustics/sigproc/research/code-examples}}.

\begin{table}
	\begin{tabularx}{\linewidth}{l|l|l|l}
		algorithm & PESQ / MOS & STOI & real-time factor\\
		\toprule[2pt]
		masking & 0.65 & 0.037 & \textbf{0.068}\\
		direct filtering & 0.67 & 0.038 & 0.070\\
		Conv-TasNet & 0.67 & 0.041 & 0.194\\
		\midrule[1pt]
		deep MFMVDR & \textbf{0.76} & \textbf{0.042} & 0.176
	\end{tabularx}
	\caption{PESQ and STOI improvement as well as real-time factors obtained on the DNS Challenge synthetic test set without reverberation, averaged over all utterances.}
	\label{tab:results}
\end{table}

\section{Conclusion}
In this paper we proposed a supervised learning-based approach to estimate the required parameters of an MFMVDR filter for single-microphone speech enhancement.
Because the MFMVDR filter requires accurate estimates of the noisy speech and noise correlation matrices as well as the speech IFC vector, we proposed to utilize the temporal and spectral modeling capabilities of TCNs for this estimation task.
The TCNs are trained to map the noisy speech STFT coefficients to the required parameters by minimizing the SI-SDR loss function at the output of the MFMVDR filter.
Experiments on the DNS Challenge dataset demonstrate the benefits of (i) using a multi-frame algorithm as compared to a single-frame algorithm, and (ii) guiding the TCN training by using the MFMVDR filter structure instead of directly estimating the filter coefficients.

\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
