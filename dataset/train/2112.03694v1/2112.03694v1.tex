\section{Experiment}
\label{sec:experiment}


\subsection{Dataset}
\label{sec:data}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=3.8in]{pic/dataset.pdf}
\caption{Selected samples. (a) DigestPath2019. (b) Camelyon16. (c) Chaoyang.}
  \label{fig:dataset}
\end{figure}





















We extensively validate our method on {five} datasets. 
\begin{enumerate}
\item DigestPath2019: It has 250 malignant images with pixel-level annotation and 410 benign images. We cropped all images into small patches, using a patch size of  and a stride of 64 pixels. {With the segmentation annotations, we defined patches with the lesion area accounts for more than  of the whole patch as malignant samples. All benign samples were cropped from benign images.} Finally, we got 29334 malignant samples, 28419 benign samples, and randomly partition them into 24611 malignant, 23824 benign, and 4723 malignant, 4595 benign samples for training and testing, respectively. The training and testing patches were from different original images. The sample patches are shown in Fig. \ref{fig:dataset} (a).
\item Camelyon16: It has 110 tumor WSIs (whole slide images) and 110 normal WSIs, and we preprocessed it in the same way as the DigestPath2019 dataset. Finally, we got 16050 malignant samples, 14812 negative samples, and randomly partition them into 11262 malignant, 11052 benign, and 4788 malignant, 4760 benign samples for training and testing, respectively. Similarly, the training and testing patches were from different WSIs. The sample patches are shown in Fig. \ref{fig:dataset} (b).
\item Chaoyang: Colon slides from Chaoyang hospital, the patch size is . We invited 3 professional pathologists to label the patches, respectively. We took the parts of labeled patches with consensus results from 3 pathologists as the testing set. Others we used as the training set. For the samples with inconsistent labeling opinions of the three doctors in the training set {(this part accounts for about 40\%)}, we randomly selected the opinions from one of the three doctors. Finally, we got 1111 normal, 842 serrated, 1404 adenocarcinoma, 664 adenoma, and 705 normal, 321 serrated, 840 adenocarcinoma, 273 adenoma samples for training and testing, respectively. This noisy dataset is constructed in the real scenario. Fig. \ref{fig:dataset} (c) shows the sample patches.
{
\item CIFAR-10 \cite{2009Learning}: It consists of 60000 colour images with size  in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The classes in the dataset are airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and trunk. It is a popular public natural computer vision dataset for image classification.
}
{\item Webvision \cite{2017WebVision}: It contains 2.4 million images in 1000 classes, which crawled from websites. The training set contains many real-world noisy labels. Since the dataset is quite large, for quick experiments, we follow the previous work\cite{chen2019understanding} and only use the first 50 classes of the Google image subset. Finally, it contains 65944 samples for training and 2500 samples for testing.
}
\end{enumerate}

{Among them, 1) to 3) are medical scenario datasets; 4) and 5) are natural computer vision datasets. We randomly added different ratios (10\%, 20\%, 30\%, and 40\%) of noise to the DigestPath2019 and the Camelyon16. Due to these two datasets only have two classes, the noise type is simply changing labels into another class. For CIFAR-10 dataset, as it originally does not contain label noise, following previous work \cite{chen2019understanding}, we experiment with two types of label noise: symmetric and asymmetric. Symmetric noise is generated by randomly replacing the labels for a percentage of the training data with all other classes, and asymmetric noise is only generated by replacing the labels with adjacent class. Following work \cite{chen2019understanding}, we tested noise ratios 20\%, 50\%, and 80\% for symmetric noise, and noise ratio 40\% for asymmetric noise. The Chaoyang and Webvision datasets are constructed in the real scenario, and the noise refers to the actual labeled samples that are wrong, rather than the artificial addition.}


\begin{table*}[]
\scriptsize
\centering
\caption{{Average test ACC, F1 Score, AUC, {Precision}, Recall(\%, 3 runs) with standard deviation on DigestPath2019 dataset.}}
\label{table:objMiccai}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multicolumn{1}{c}{Noise ratio} & \multicolumn{5}{c}{10\%}                                                                                                                & \multicolumn{5}{c}{20\%}                                                                                                                \\ \midrule
Method                & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} \\ \midrule
Joint                     & 91.490.95          & 97.621.25          & 91.250.90               & 95.411.77          & 87.450.21             & 88.780.28          & 95.400.32          & 89.000.27               & 88.470.24          & 89.530.32             \\
Co-tea.                     & 91.080.79          & 98.211.00          & 90.590.80               & \textbf{97.491.18} & 84.600.51             & 90.330.70          & 95.860.87          & 90.120.65               & 93.491.29          & 87.000.11             \\
INCV                            & 93.580.96          & 97.911.01          & 93.620.96               & 94.340.92          & 92.921.00             & 92.090.39          & 96.430.50          & 91.970.38               & \textbf{94.740.65} & 89.360.14             \\
OUSM                       & 90.270.98          & 96.871.02          & 89.810.94               & 95.871.95          & 84.480.16             & 88.191.56          & 94.191.48          & 88.181.69               & 89.300.85          & 87.112.49             \\
NF-Net                          & 83.080.43          & 90.640.24          & 81.950.42               & 89.260.73          & 75.750.18             & 83.092.17          & 89.092.45          & 82.822.21               & 85.402.28          & 80.402.15             \\
DM                       & 92.690.84          & 97.760.64          & 92.690.81               & 93.981.30          & 91.450.59             & 90.530.87          & 96.091.11          & 90.460.83               & 92.511.49          & 88.510.96             \\
SELF                            & 92.220.77          & 97.430.96          & 92.210.79               & 92.551.04          & 91.780.66             & 91.230.42          & 95.890.55          & 91.190.36               & 91.440.33          & 90.990.41             \\
\textbf{Ours}                   & \textbf{94.910.32} & \textbf{98.400.57} & \textbf{95.050.35}      & 93.680.63          & \textbf{96.460.21}    & \textbf{94.460.20} & \textbf{98.300.34} & \textbf{94.530.23}      & 94.510.13          & \textbf{94.550.38}    \\ \midrule
\multicolumn{1}{c}{Noise ratio} & \multicolumn{5}{c}{30\%}                                                                                                                & \multicolumn{5}{c}{40\%}                                                                                                                \\ \midrule
Method                & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} \\ \midrule
Joint                     & 87.170.57          & 94.720.63          & 86.410.66               & 93.340.27          & 80.440.98             & 84.300.87          & 91.391.02          & 84.460.92               & 84.770.60          & 84.151.26             \\
Co-tea.                     & 86.490.37          & 93.550.46          & 86.900.33               & 85.500.47          & 88.340.20    & 84.920.57          & 92.980.68          & 84.940.54               & 86.020.74          & 83.890.41             \\
INCV                            & 87.840.52          & 94.600.23          & 87.650.52               & 90.360.56          & 85.100.49             & 85.200.48          & 93.520.56          & 84.470.50               & 90.190.56          & 79.440.48             \\
OUSM                       & 86.521.22          & 92.491.25          & 86.531.23               & 87.641.18          & 85.451.37             & 84.040.38          & 93.510.42          & 82.570.41               & 92.430.48          & 74.600.36             \\
NF-Net                          & 83.860.59          & 90.620.34          & 83.590.53               & 86.240.93          & 81.110.18             & 82.370.84          & 88.291.02          & 82.380.81               & 83.490.97          & 81.300.70             \\
DM                       & 88.871.03          & 95.100.78          & 88.601.12               & 92.040.54          & 85.411.63             & 85.960.37          & \textbf{96.870.38} & 84.490.43               & \textbf{95.970.39} & 75.460.46             \\
SELF                            & 90.031.21          & 95.780.64          & 89.961.42               & 90.970.89          & \textbf{88.661.66}             & 86.230.98          & 94.210.89          & 86.021.00               & 87.520.88          & 84.031.21             \\
\textbf{Ours}                   & \textbf{91.720.69} & \textbf{97.380.78} & \textbf{91.470.75}      & \textbf{95.740.34} & 87.571.10             & \textbf{87.150.41} & 94.260.46          & \textbf{86.490.41}      & 88.580.63          & \textbf{85.160.25}    \\ \bottomrule
\end{tabular}
\end{table*}

\begin{table*}[]
\scriptsize
\centering
\caption{{Average test ACC, F1 Score, AUC, {Precision}, Recall(\%, 3 runs) with standard deviation on Camelyon16 dataset.}}
\label{table:objCamelyon}
\begin{tabular}{@{}lllllllllll@{}}
\toprule
\multicolumn{1}{c}{Noise ratio} & \multicolumn{5}{c}{10\%}                                                                                                                & \multicolumn{5}{c}{20\%}                                                                                                                \\ \midrule
Method                & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} \\ \midrule
Joint                     & 97.510.56          & 98.940.67          & 97.530.55               & 96.890.68          & 98.180.42             & 96.510.70          & 98.750.68          & 96.490.72               & 97.130.15          & 95.861.32             \\
Co-tea.                     & 98.160.30          & 99.340.27          & 98.130.31               & \textbf{99.870.09} & 96.450.51             & 97.570.27          & 99.290.30          & 97.530.27               & 99.290.27          & 95.840.27             \\
INCV                            & 97.870.16          & 99.230.16          & 97.840.16               & 99.570.18          & 96.170.15             & 97.650.12          & 99.330.13          & 97.620.12               & 99.420.07          & 95.870.17             \\
OUSM                       & 98.080.41          & 99.370.50          & 98.060.41               & 99.550.55          & 96.610.52             & 96.571.11          & 98.900.20          & 96.521.13               & 98.331.21          & 94.781.33             \\
NF-Net                          & 90.240.21          & 93.530.19          & 89.700.24               & 95.160.05          & 84.840.39             & 90.721.02          & 92.930.45          & 90.301.03               & 94.921.55          & 86.120.68             \\
DM                       & 95.100.40          & 96.700.50          & 94.910.43               & 99.150.19          & 91.010.63             & 93.980.63          & 96.900.62          & 94.010.66               & 93.850.34          & 94.171.10             \\
SELF                            & 97.890.42          & 99.020.34          & 97.850.39               & 98.020.52          & 97.530.39             & 97.440.32          & 98.890.15          & 97.490.41               & 97.880.50          & 97.060.37             \\
\textbf{Ours}                   & \textbf{98.820.20} & \textbf{99.810.14} & \textbf{98.810.20}      & 99.730.18          & \textbf{97.730.22}    & \textbf{98.610.11} & \textbf{99.780.18} & \textbf{98.690.11}      & \textbf{99.610.14} & \textbf{97.400.21}    \\ \midrule
\multicolumn{1}{c}{Noise ratio} & \multicolumn{5}{c}{30\%}                                                                                                                & \multicolumn{5}{c}{40\%}                                                                                                                \\ \midrule
Method                & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} \\ \midrule
Joint                     & 96.100.55          & 98.790.64          & 96.030.55               & 98.090.75          & 94.060.38             & 91.971.72          & 95.511.43          & 91.561.82               & 96.943.08          & 86.862.79             \\
Co-tea.                     & 97.490.12          & 99.020.11          & 97.440.12               & 99.680.04          & 95.300.20             & 95.200.71          & 96.250.72          & 95.090.73               & 97.560.77          & 92.740.69             \\
INCV                            & 96.280.32          & 97.640.32          & 96.240.31               & 97.410.43          & 95.100.20             & 95.400.23          & 96.260.34          & 95.280.24               & 98.060.10          & 92.650.36             \\
OUSM                       & 96.430.23          & 98.730.20          & 96.320.24               & 99.630.29          & 93.220.32             & 91.983.21          & 96.353.07          & 91.323.71               & 98.641.29          & 85.115.40             \\
NF-Net                          & 89.610.60          & 95.810.19          & 89.280.59               & 92.500.93          & 86.290.35             & 85.531.18          & 91.720.75          & 85.341.28               & 86.760.86          & 93.961.66             \\
DM                       & 93.391.07          & 95.281.19          & 93.081.17               & 97.740.60          & 88.851.64             & 89.550.12          & 97.720.06          & 88.390.14               & \textbf{99.840.12} & 79.300.15             \\
SELF                            & 96.980.54          & 98.720.17          & 97.000.46               & 97.560.38          & 96.430.56             & 96.021.05          & 97.530.31          & 95.891.22               & 96.781.31          & 94.951.20             \\
\textbf{Ours}                   & \textbf{98.320.41} & \textbf{99.570.22} & \textbf{98.300.42}      & \textbf{99.840.09} & \textbf{96.800.46}    & \textbf{98.170.84} & \textbf{99.510.24} & \textbf{98.160.28}      & 99.120.23          & \textbf{97.220.32}    \\ \bottomrule
\end{tabular}
\end{table*}

\subsection{Implementation and Parameter Settings}
\label{sec:impl}

For medical scenario datasets (DigestPath2019, Camelyon16, and Chaoyang). We used the Resnet-34 as the backbone and trained it using Adam with a momentum of 0.9, and a batch size of 96. During the label correction phase, the network was trained for 30 epochs. We set the initial learning rate as 0.001, and linearly reduced it after 15 epochs. For the NSHE phase, the networks were trained for 40 epochs. We set the initial learning rate as 0.001, and linearly reduced it after 15 epochs.

{For natural computer vision datasets (CIFAR-10, Webvision), we followed the same settings in work \cite{chen2019understanding}. For CIFAR-10, we used the Resnet-32 as the backbone and trained it using SGD with a momentum of 0.9, a learning rate of 0.02, and a batch size of 128. The networks were trained for 300 epochs both in the label correction phase and the NSHE phase. For Webvision, we used the Inception-Resnet v2 \cite{2017Inception} as the backbone and trained it using SGD with a momentum of 0.9, a learning rate of 0.01, and a batch size of 32. The networks were trained for 80 epochs both in the label correction phase and the NSHE phase.}

{For all the datasets,  was set to 0.1 for 80\% noise ratio, and  for other noise ratios, where  is the dataset noise ratio. The parameter  in focal loss we set to , and the discarding ratio  was set to . For real-world datasets Chaoyang and Webvision,  was estimated by the noise cross-validation algorithm of \cite{chen2019understanding}.}

\subsection{Evaluation Criteria}

We used Accuracy (ACC), {Precision}, Recall, F1 Score (F1), AUC, and {ROC} curve as evaluation criteria. Their definitions are as follows:









where TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives, respectively.

{ROC} curve is the receiver operating characteristic curve. Its abscissa is false positive rate and ordinate is the true positive rate. AUC is the area under the {ROC} curve. 

For multi-classification tasks, we compute the {Precision}, Recall, F1 Score, {ROC} curve, and AUC for each class and average them by using macro-average.
\subsection{Objective Comparison}
We compare our methods with the following methods using the same network architecture.


\begin{table}[]
\scriptsize
\centering
\caption{{Average test ACC, F1 Score, AUC, {Precision}, Recall(\%, 3 runs) with standard deviation on Chaoyang dataset.}}
\label{table:objChaoyang}
\begin{tabular}{@{}llllll@{}}
\toprule
Method & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} \\ \midrule
Joint      & 75.990.64          & 90.430.84          & 67.722.36               & 70.972.42          & 67.912.77             \\
Co-tea.      & 79.390.29          & 91.720.68          & 71.970.96               & 74.571.45          & 70.771.17             \\
INCV             & 80.340.36          & 92.630.11          & 74.110.43               & 76.220.22          & 73.060.36             \\
OUSM        & 80.531.10          & 93.690.42          & 73.700.96               & 74.811.76          & 73.270.39             \\
NF-Net           & 51.231.18          & 69.921.17          & 33.210.39               & 37.190.94          & 36.270.62             \\
DM        & 77.250.21          & 87.580.36          & 69.780.32               & 70.680.23          & 69.110.38             \\
SELF             & 80.490.42          & 93.990.58          & 75.310.63               & 76.140.78          & 74.690.59             \\
\textbf{Ours}    & \textbf{83.400.20} & \textbf{94.510.34} & \textbf{76.540.33}      & \textbf{78.330.30} & \textbf{75.450.42}    \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[]
  \centering
  \includegraphics[width=2.9in,height=8.667in]{pic/roc_new_v4.png}
\caption{{(a) DigestPath2019 dataset average {ROC} curve from 10\% to 40\% noise ratios (used marco-average). (b) Camelyon16 dataset average {ROC} curve from 10\% to 40\% noise ratios (used marco-average). (c) Chaoyang dataset average {ROC} curve from 4 classes (used marco-average). (d) CIFAR-10 dataset average {ROC} curve from 10 classes with 20\% to 80\% noise ratios (used marco-average). (e) Webvision dataset average {ROC} curve from 50 classes (used marco-average).}}
  \label{fig:roc}
\end{figure}

\begin{table*}[]
\scriptsize
\centering
\caption{{Average test ACC, F1 Score, AUC, {Precision}, Recall(\%, 3 runs) with standard deviation on CIFAR-10 dataset.}}
\label{table:objCIFAR}
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\multicolumn{1}{c}{Noise ratio} & \multicolumn{5}{c}{20\% Sym.}                                                                                    & \multicolumn{5}{c}{50\% Sym.}                                                                                    \\ \midrule
Method                & ACC                  & AUC                  & F1 Score             & {Precision}                  & Recall               & ACC                  & AUC                  & F1 Score             & {Precision}                  & Recall               \\ \midrule
Joint                     & 88.370.06          & 98.880.06          & 88.330.03          & 88.380.03          & 88.370.06          & 81.850.16          & 97.880.06          & 81.870.16          & 82.030.12          & 81.850.16          \\
Co-tea.                     & 87.800.32           & 99.080.02          & 87.850.31          & 87.950.30          & 87.800.32           & 83.210.48          & 98.290.04          & 83.270.44          & 83.480.41          & 83.210.48          \\
INCV                            & 89.020.24          & 99.100.05           & 89.080.24          & 89.890.22          & 89.020.24          & 84.980.32          & 98.380.09          & 85.100.33           & 85.880.33          & 84.980.32          \\
OUSM                       & 81.880.20          & 97.010.06          & 81.850.24          & 81.880.25          & 81.880.20          & 68.660.81          & 93.770.23          & 68.500.77          & 69.390.52          & 68.660.81          \\
NF-Net                          & 80.271.15          & 96.760.50          & 80.191.03          & 81.021.21          & 80.271.15          & 67.381.24          & 90.340.34          & 67.581.11          & 67.881.07          & 67.381.24          \\
DM                       & 91.760.09          & 99.180.06          & 91.710.10          & 91.760.12          & 91.760.09          & 90.460.27          & 98.850.01          & 90.430.29          & 90.460.28          & 90.460.27          \\
SELF                            & 91.010.12          & 99.150.09          & 91.290.22          & 91.580.23          & 91.010.12          & 90.290.44          & 98.540.10          & 90.430.23          & 90.510.27          & 90.290.44          \\
\textbf{Ours}                   & \textbf{92.350.09} & \textbf{99.230.07} & \textbf{92.430.10} & \textbf{92.500.17} & \textbf{92.350.09} & \textbf{91.330.09} & \textbf{99.070.06} & \textbf{91.360.09} & \textbf{91.500.07} & \textbf{91.330.09} \\ \midrule
\multicolumn{1}{c}{Noise ratio} & \multicolumn{5}{c}{80\% Sym.}                                                                                    & \multicolumn{5}{c}{40\% Asym.}                                                                                   \\ \midrule
Method                & ACC                  & AUC                  & F1 Score             & {Precision}                  & Recall               & ACC                  & AUC                  & F1 Score             & {Precision}                  & Recall               \\ \midrule
Joint                     & 57.831.93          & 89.620.95          & 57.901.96          & 57.932.02          & 57.841.93          & 87.050.32          & 98.030.12          & 87.040.31          & 87.090.37          & 87.050.32          \\
Co-tea.                     & 24.374.21          & 61.972.88          & 24.924.52          & 25.684.02          & 24.374.21          & 82.861.13          & 98.150.28          & 82.931.01          & 83.240.79          & 82.861.13          \\
INCV                            & 53.982.68          & 85.761.23          & 53.982.35          & 53.992.24          & 53.982.68          & 85.880.67          & 98.250.09          & 85.90.54           & 85.990.52          & 85.880.67          \\
OUSM                       & 40.621.03          & 81.180.97          & 40.591.24          & 40.871.46          & 40.421.03          & 73.871.20          & 95.870.10          & 73.241.29          & 75.140.47          & 72.871.20          \\
NF-Net                          & 18.283.31          & 56.301.89          & 18.903.65          & 19.783.72          & 18.283.31          & 69.981.11          & 91.430.18          & 70.021.03          & 70.161.12          & 69.981.11          \\
DM                       & 58.041.67          & 88.621.01          & 58.771.35          & 59.711.22          & 58.041.67          & 86.500.19          & 97.510.02          & 86.240.19          & 86.730.05          & 86.500.19          \\
SELF                            & 59.752.13          & 86.401.34          & 59.241.99          & 60.021.87          & 59.752.13          & 87.140.54          & 98.120.11          & 87.270.64          & 87.350.48          & 87.140.54          \\
\textbf{Ours}                   & \textbf{61.691.22} & \textbf{89.910.77} & \textbf{61.61.34}  & \textbf{61.721.45} & \textbf{61.691.22} & \textbf{88.260.12} & \textbf{98.770.03} & \textbf{88.240.10} & \textbf{88.290.07} & \textbf{88.260.12} \\ \bottomrule
\end{tabular}
\end{table*}


\begin{table}[]
\scriptsize
\centering
\caption{{Average test ACC, F1 Score, AUC, {Precision}, Recall(\%, 3 runs) with standard deviation on Webvision dataset.}}
\label{table:objWebvision}
\begin{tabular}{@{}llllll@{}}
\toprule
Method & \multicolumn{1}{c}{ACC} & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{F1 Score} & \multicolumn{1}{c}{{Precision}} & \multicolumn{1}{c}{Recall} \\ \midrule
Joint      & 60.280.92             & 96.700.07             & 60.401.03                  & 67.511.22             & 60.280.92                \\
Co-tea.      & 63.771.31             & 96.980.09             & 63.921.46                  & 64.021.51             & 63.771.31                \\
INCV             & 65.020.86             & 97.110.04             & 65.110.98                  & 65.051.02             & 65.020.86                \\
OUSM        & 70.860.50             & 98.160.05             & 70.950.41                  & 73.050.50             & 70.860.50                \\
NF-Net           & 58.031.27             & 95.860.08             & 58.771.34                  & 59.011.54             & 58.031.27                \\
DM        & 76.920.24             & 97.690.02             & 76.980.27                  & 77.080.43             & 76.920.24                \\
SELF             & 69.180.32             & 97.170.05             & 69.210.38                  & 69.800.63             & 69.180.32                \\
\textbf{Ours}    & \textbf{77.520.51}    & \textbf{98.320.04}    & \textbf{77.580.59}         & \textbf{78.110.87}    & \textbf{77.520.51}       \\ \bottomrule
\end{tabular}
\end{table}

\begin{enumerate}

{\item \textbf{SELF}\cite{2020SELF} (Duc Tam Nguyen \textit{et al.} 2020) first obtains the self-ensemble predictions of all training samples and then progressively removes samples whose ensemble predictions do not agree with their annotated labels \cite{2020Learning}.}

\item \textbf{DM}{\cite{2020DivideMix}} (Junnan Li \textit{et al.} 2020) lets its two-component and one-dimensional Gaussian mixture model be fitted to the training loss to obtain the confidence of an annotated label. By setting a confidence threshold, the training data is categorized into a labeled set and an unlabeled set. Subsequently, \textbf{MixMatch} \cite{david2019mixmatch} is employed to train a DNN for the transformed data.
\item \textbf{INCV}{\cite{chen2019understanding}} (Pengfei Chen \textit{et al.} 2019) randomly divides noisy training data and then employs cross-validation to classify true-labeled samples while removing large-loss samples at each training round. 
\item \textbf{Joint}{\cite{tanaka2018joint}} (Tanaka \textit{et al.} 2018)  jointly optimizes the sample labels and the network parameters.
\item \textbf{Co-tea.}{\cite{han2018co}} (Han \textit{et al.} 2018) maintains two networks. Each network selects samples of small training loss from the mini-batches and feeds them to the other network. 

\item \textbf{NF-Net}{\cite{cao2020breast}} (Zhantao Cao \textit{et al.} 2020) adopts a double-softmax classification module to prevent deep models from overfitting the noisy labels and a teacher-student module to strengthen the effect of clean labels. 
\item \textbf{OUSM}{\cite{cheng2019robust}} (Cheng Xue \textit{et al.} 2019) proposes online uncertainty sample mining and individual re-weighting methods to train their network.

\end{enumerate}



{Among them, work 1) to 5) are the state-of-the-art methods for general noisy data processing in recent years; work 6) and work 7) are the state-of-the-art methods proposed for medical data scenarios. We choose these schemes to contrast with to fully prove the superiority of our method. The testings of 2) to 6) are based on the open-source codes from the authors. We re-implemented and tested 1) and 7) based on the settings from the original papers.}











{For experiments on medical scenarios datasets, Table \ref{table:objMiccai} and Table \ref{table:objCamelyon} shows the test ACC, AUC, F1 Score, {Precision}, Recall on DigestPath2019 and Camelyon16 with different levels of label noise ranging from 10\% to 40\%. Our method almost outperforms the state-of-the-art methods across all noise ratios. Table \ref{table:objChaoyang} shows these metrics on the Chaoyang dataset. Our method outperforms all other methods by a large margin in every criterion. 

For experiments on natural computer vision datasets, Table \ref{table:objCIFAR} shows the test ACC, AUC, F1 Score, {Precision}, Recall on CIFAR-10 with different levels and different types of label noise ranging from 20\% to 80\%. Our method outperforms the state-of-the-art methods across all noise ratios. Table \ref{table:objWebvision} shows these metrics on Webvision dataset. Our method consistently outperforms all other methods. Besides, we show the mean {ROC} curves of all five datasets in Fig. \ref{fig:roc}.}





\begin{table}[]
\scriptsize
\centering
\caption{Average ACC(\%, 3 runs) with standard deviation of different ablation study on the DigestPath2019 dataset.}
\label{table:albMiccai}
\begin{tabular}{lllll}
\toprule
\multicolumn{1}{c}{Noise ratio}        & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{20\%} & \multicolumn{1}{c}{30\%} & \multicolumn{1}{c}{40\%} \\ \midrule
Method                 & \multicolumn{4}{c}{ACC}                                                                                     \\ \midrule
Ours                                       & \textbf{94.910.52}      & \textbf{94.460.20}      & \textbf{91.720.69}      & \textbf{87.150.41}     \\ 
w/o NSHE                       & 93.460.12               & 91.480.43               & 90.860.24               & 86.850.60              \\ 
w/o (NSHE + EHN)       & 90.540.45               & 88.320.17               & 86.250.48               & 85.151.87              \\ 
w/o whole & 90.010.49               & 87.240.98               & 85.111.31               & 82.401.42              \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[]
\scriptsize
\centering
\caption{Average ACC(\%, 3 runs) with standard deviation of different ablation study on the Camelyon16 dataset.}
\label{table:albCame}
\begin{tabular}{lllll}
\toprule
\multicolumn{1}{c}{Noise ratio}            & \multicolumn{1}{c}{10\%} & \multicolumn{1}{c}{20\%} & \multicolumn{1}{c}{30\%} & \multicolumn{1}{c}{40\%} \\ \midrule
Method                 & \multicolumn{4}{c}{ACC}                                                                                     \\ \midrule
Ours                                       & \textbf{98.820.20}      & \textbf{98.610.11}      & \textbf{98.320.41}      & \textbf{98.170.24}     \\ 
w/o NSHE                        & 97.150.50               & 97.000.98               & 96.830.89               & 96.552.11              \\ 
w/o (NSHE + EHN)       & 95.990.73               & 94.951.13               & 93.781.54               & 93.452.44              \\ 
w/o whole & 94.600.66               & 93.340.91               & 93.110.78               & 92.231.82              \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Ablation Study}

We study the effect of removing different components to provide insights into what makes our method successful. Fig. \ref{fig:albMiccai} and Fig. \ref{fig:albCame} show the ablation study results in different noise ratios. The result details are shown in Table \ref{table:albMiccai} to \ref{table:albCame}, and we discuss them below.



\begin{figure}[]
  \centering
  \includegraphics[width=3.48in]{pic/ALBmiccai.pdf}
\caption{{Ablation study results in terms of test accuracy on DigestPath2019.}}
  \label{fig:albMiccai}
\end{figure}
\begin{figure}[]
  \centering
  \includegraphics[width=3.48in]{pic/ALBcame.pdf}
\caption{{Ablation study results in terms of test accuracy on Camelyon16 dataset.}}
  \label{fig:albCame}
\end{figure}


To study the effects of the NSHE scheme, we removed the NSHE scheme (w/o NSHE), namely train the single model by using the dataset from label correction. The results show the hard samples play quite a significant role in training final models. By removing the NSHE scheme, the test accuracy decreased by an average of about 1.5\%.

{To study the effects of the EHN detection scheme, we removed both the EHN detection scheme and NSHE scheme (w/o (EHN + NSHE)). In this situation, following work \cite{tanaka2018joint}, we directly used the classification model as the correction model, and the dataset is processed only by the correction model. The results show the EHN detection scheme is very effective to save more hard samples and filtered more noisy ones. Without the EHN detection scheme, the test accuracy further decreased by an average of about 2.8\%.}

{To study the effects of the correction model, we further removed the whole label correction phase (w/o whole), namely we train the model by using original data. To converge the model under the same epoch, we adjusted the learning rate, which would smooth the test accuracy in the last epoch but the highest accuracy will be affected. By removing the correction model, the test accuracy further decreased by an average of about 1.2\%.}

Among the NSHE scheme, EHN detection scheme, and correction model, the EHN detection scheme introduces the maximum performance gain, followed by the NSHE scheme. All components have a certain gain at any noise ratio.

\begin{figure}[]
  \centering
  \includegraphics[width=2.7in]{pic/alb2.pdf}
\caption{{Our method with different rounds (just first stage) results in terms of test accuracy on DigestPath2019 dataset with 20\% noise ratio.}}
  \label{fig:alb2}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=2.7in]{pic/simple_ours_acc_new.pdf}
\caption{{Test accuracy of a simple Resnet-34 and our algorithm (backbone is Resnet-34) on DigestPath2019 dataset with different noise ratios.}}
  \label{fig:simple_ours}
\end{figure}

{To analyze the effect of self-training rounds, we recorded the training times and test accuracy in different self-training rounds. Fig. \ref{fig:alb2} shows the results on DigestPath2019 dataset with 20\% noise ratio. Training more rounds consumes more computing resources but brings little gain. Therefore, we choose to train only one round in our experiment. We also studied how much higher noise ratio could our scheme tolerate to reach a similar performance against a simple model. We first directly trained a simple Resnet-34 under 5 different noise ratios (0\%, 10\%, 20\%, 30\%, 40\%, respectively) on DigestPath2019 dataset. Then we also carried out experiments }{our algorithm (backbone is Resnet-34) at 10\%, 15\%, 20\%, 25\%, 30\%, 35\%, 40\%, 45\%, 49\% noise ratios (note that we select 49\% noise ratio because it is close to the theoretical limit of 50\% for this type of noise on binary classification task). The results are shown in Fig. \ref{fig:simple_ours}.}

{According to Fig. \ref{fig:simple_ours}, to reach the same performance, for 0\%, 10\%, 20\%, 30\%, and 40\% noise ratios, our scheme can tolerate about up to 22\%, 34\%, 40\%, 44\%, and 49\% noise ratio, respectively. We also find that in the noise ratios of 0\% to 20\% on DigestPath2019 dataset, the results of our method are even better than the trained simple Resnet-34 on a completely clean dataset. We believe that this phenomenon is due to our enhancement of the information of hard samples.}


\subsection{Analysis of EHN Detection Scheme}

{\textbf{Effectiveness and Convergence Analysis.} To analyze the effectiveness and convergence of our EHN detection scheme, we trained on DigestPath2019 dataset with different noise ratios and plotted the curves of test ACC vs. Iterations (``test'' set here means ) of . The results are shown in Fig. \ref{fig:ehn_acc}. According to Fig. \ref{fig:ehn_acc},  has converged in the later training stage at each noise ratio, so the convergence of  is relatively stable. Also from Fig. \ref{fig:ehn_acc},  can achieve 98\%, 97\%, 96\%, and 90\% classification accuracy in 10\% to 40\% noise ratios, respectively. Thus,  can effectively distinguish the hard and noisy samples. We also recorded the confusion matrix of EHN detection scheme as Fig. \ref{fig:EHN_diffNoiseR} {(clean samples} {are divided into easy and hard in this test, as depicted by Fig. \ref{fig:meanHistorgam} (b)).}}

\begin{figure}[]
  \centering
  \includegraphics[width=2.6in]{pic/EHN_ACC.pdf}
\caption{{Accuracy vs. iterations graph of  in DigestPath2019 dataset with different noise ratios.}}
  \label{fig:ehn_acc}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=3.55in]{pic/EHN_diffNoiseR_v2.pdf}
\caption{{Confusion matrix and accuracy (ACC) of EHN detection scheme in DigestPath2019 dataset with different noise ratios.}}
  \label{fig:EHN_diffNoiseR}
\end{figure}



\begin{figure*}[]
  \centering
  \includegraphics[width=5.8in]{pic/EHN_diffPar_v3.pdf}
\caption{{Confusion matrix and accuracy (ACC) of EHN detection scheme on different ``{}'' and ``'' in DigestPath2019 dataset with 30\% noise ratio.}}
  \label{fig:EHN_diffPar}
\end{figure*}

\begin{figure}[]
  \centering
  \includegraphics[width=3.5in]{pic/EHN_k_taue.pdf}
\caption{{(a) ACC vs. {} graph of EHN detection scheme. (b) ACC vs.  graph of EHN detection scheme.}}
  \label{fig:EHN_n}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=3.5in]{pic/clean_noise_response_new.pdf}
\caption{{(a) Mean prediction probability histogram of the clean and noisy samples in DigestPath2019 dataset (40\% noise ratio) . (b) Mean prediction probability histogram of the clean and noisy samples in corresponding . }}
  \label{fig:clean_noise}
\end{figure}

\textbf{{Parameter Analysis.}} {To analyze how sensitive the EHN detection scheme is to the training epochs {} and easy sample ratio , we trained on different {} and  in DigestPath2019 dataset with 30\% noise ratio, recorded the confusion matrix, and calculated the ACC of EHN detection scheme. Specifically, we first adjusted the value of {} with fixed  = 0.55, and thus obtained the sensitivity of EHN to {}. Then we adjusted the value of  with fixed {} = 30, and thus obtained the sensitivity of EHN to . There are a total of 9 different parameter configurations. The results are shown in Fig. \ref{fig:EHN_diffPar}. And we also plotted the ACC vs. Parameters graph as Fig. \ref{fig:EHN_n}. These results show that as {} increases from 0, the performance continues to increase. When {} reaches about 30, the performance achieves maximum accuracy. However, as {} increases further, the performance begins to decrease slightly. But overall, when {} exceeds a certain threshold, the performance of EHN is relatively stable. And for the parameter , changing the value within a reasonable range has little impact on the performance. By the way, our utilized parameters are effective according to Fig. \ref{fig:EHN_n}.}



{\textbf{Why  works?} To study why  trained on the artificial created  can recognize hard and noisy samples in original dataset , we plotted both the mean prediction probability histogram of the clean and noisy samples in DigestPath2019 dataset (40\% noise ratio)  and the corresponding . The results are shown in Fig. \ref{fig:clean_noise}. According to Fig. \ref{fig:clean_noise} (b), there are also some clean samples which can not be distinguished from the noisy ones by mean prediction probability. Although the samples in  are all easy ones to dataset , part of the samples became hard ones to dataset . In Fig. \ref{fig:clean_noise}, it should be noted that the mean prediction probabilities of samples trained on  have similar distribution with the original dataset , and this is why our EHN detection scheme works by using the artificial created .}




\subsection{Hard and Noisy Sample Analysis}

\begin{figure}[]
  \centering
  \includegraphics[width=2.8in]{pic/fl_diagram.pdf}
\caption{{The diagram of ``Learning'' and ``Forgetting'' event.}}
  \label{fig:fl_diagram}
\end{figure}

\begin{figure*}[hbt]
  \centering
  \includegraphics[width=6.6in]{pic/hsa.pdf}
\caption{(a) The forgetting and learning event frequency histogram. (b) The gradient absolute value frequency histogram.} 
  \label{fig:hsa}
\end{figure*}



\textbf{Behavior Analysis.} To analyze the training process behavior of the hard sample and the noisy sample, inspired by \cite{toneva2018empirical}, we calculated the frequency of the ``Learning'' event and ``Forgetting'' event of them through the whole training epochs. The ``Learning'' event in the  epoch is defined as an event that the prediction probability of the labeled class is less than  in  epoch, while greater than  in  epoch. The ``Forgetting'' event in the  epoch is defined as an event that the prediction probability of the labeled class is greater than  in  epoch, while less than  in  epoch. Fig. \ref{fig:fl_diagram} shows the diagram of ``Learning'' and ``Forgetting'' event. The statistical results are shown in Fig. \ref{fig:hsa} (a). This results show that the hard sample and the noisy sample have great behavioral differences with the increase of the epoch in training. In the early stage of training, the hard samples tend to have more learning events, while the noisy samples tend to have more forgetting events. In the late stage of training, the hard samples tend to have more forgetting events while the noisy samples tend to have more learning events. On the whole, the frequency of learning and forgetting events of the hard samples is higher than that of the noisy samples during the whole training epochs.

\begin{figure}[]
  \centering
  \includegraphics[width=2.7in]{pic/hsa_c.pdf}
\caption{Sample distribution diagram, the red color represents the noisy sample. Yellow circles denote the center of clustering.}
  \label{fig:hsa_c}
\end{figure}

\begin{figure}[]
  \centering
  \includegraphics[width=3.5in]{pic/tsne.pdf}
\caption{Visualization of Camelyon16 dataset with 20\% noise ratio by using t-SNE\cite{laurens2008visiualizing}, where patches with red solid border are clean malignant samples, patches with blue solid border are clean benign samples, patches with red dotted border are noisy malignant samples (true labels are benign), patches with blue dotted border are noisy benign samples (true labels are malignant).}
  \label{fig:tsne}
\end{figure}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=3.5in]{pic/training.pdf}
\caption{Training history graph of the selected hard and noisy samples.}
  \label{fig:hsa_d}
\end{figure}

\begin{table*}[]
\scriptsize
\centering
\caption{{The final noise ratios of different label correction schemes on DigestPath2019 dataset.}}
\label{table:hsa}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Noise ratio  & \multicolumn{3}{c}{10\%}                                          & \multicolumn{3}{c}{20\%}                                          \\ \midrule
Strategy     & Remain samples         & Remain noisy samples & Final noise ratio & Remain samples         & Remain noisy samples & Final noise ratio \\ \midrule
Ours         & \multirow{2}{*}{46181} & 26                   & 0.0563\%          & \multirow{2}{*}{45895} & 159                  & 0.346\%           \\
Drop by mean &                        & 181                  & 0.392\%           &                        & 859                  & 1.87\%            \\ \midrule
Noise ratio  & \multicolumn{3}{c}{30\%}                                          & \multicolumn{3}{c}{40\%}                                          \\ \midrule
Strategy     & Remain samples         & Remain noisy samples & Final noise ratio & Remain samples         & Remain noisy samples & Final noise ratio \\ \midrule
Ours         & \multirow{2}{*}{42361} & 551                  & 1.30\%            & \multirow{2}{*}{39348} & 2227                 & 5.66\%            \\
Drop by mean &                        & 1537                 & 3.63\%            &                        & 6626                 & 16.8\%            \\ \bottomrule
\end{tabular}
\end{table*}

We also calculated the frequency histogram of the gradient absolute value. The gradient absolute value is the absolute value of the gradient between adjacent epochs, that is, the absolute value of the difference between the prediction probabilities of adjacent epochs. As shown in Fig. \ref{fig:hsa} (b), the gradient absolute value of the hard samples tends to be higher than the noisy samples. We believe that the reason for this phenomenon is that the hard samples' hard fitting attribute makes the prediction probability of the model jump frequently. Some noisy samples, however, are conspicuously at the center of other classes; they are ``super hard" for the model to fit. Their effect on the optimization of model parameters would be suppressed by the clean samples around. So in the latter part of the training process, their output probabilities would not change much. As shown in Fig. \ref{fig:hsa_c}, the noisy samples are scattered throughout the dataset. When these samples fall in the intersection area of two categories, their training behaviors are more similar to the hard ones; when these samples fall in the center area of other classes (yellow circles in Fig. \ref{fig:hsa_c}), they have distinct training behavior differences with the hard samples. To further prove the conjecture above, we selected samples in the Camelyon16 dataset with 20\% noise ratio and used the Resnet-34 model (pre-trained in ImageNet) to extract the features and visualized them by t-SNE\cite{laurens2008visiualizing} in Fig. \ref{fig:tsne}. We selected the noisy samples that fall in the center of other classes in Fig. \ref{fig:tsne} and plotted their training history to compare with the hard samples in Fig. \ref{fig:hsa_d}. It can be seen that these noisy samples are indeed difficult to predict in the latter part of the training, which confirms our previous analysis.






\textbf{Benefit Analysis.} To show the gain of our hard-aware label correction phase more intuitively, we count the remaining noise of the processed dataset and compare it with the baseline in Table \ref{table:hsa}. The baseline is set to update the labels by classification model and drop out the samples by using the mean prediction probability of the training history. The results show that with the same number of remaining samples, our strategy eliminates more noisy samples by protecting hard samples as much as possible, and thus generates a higher quality data set for the second phase of training. {Besides, for the real medical scenario dataset ``Chaoyang'', our hard-aware label correction phase only drops out 191 samples (total 4021 samples), and this shows that our method reduces noise with little damage to the original dataset.}



\section{Conclusion}
\label{sec:conclusion}
Deep learning-based histopathology image classification can improve the diagnosing accuracy of cancer. It is difficult to collect a large clean dataset for training such a classification model. The existing noisy label correction methods fail to distinguish the hard samples from the noisy samples and thus ruin the model performance. In this study, we proposed a hard sample aware noise robust learning for histopathology image classification to save more clean samples, and thus boosted the model performance. We found that the training prediction history can be used to distinguish the hard samples and noisy samples. By integrating our EHN detection scheme into the noise removing, more hard clean samples can be saved. Besides, in our NSHE scheme under co-learning architecture, we adopted different parameter updating speed for the two models. This can further suppress the interference of noisy samples. Our results provide compelling performance for the noisy dataset, and the proposed method can be directly applied to the real-word noisy scenario. In the future, we will conduct hard sample aware semantic segmentation, such as malignant tissue segmentation for histopathology images. 