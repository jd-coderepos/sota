

\documentclass[final,3p,times]{elsarticle}





\usepackage{graphicx}


\usepackage{amssymb}






\biboptions{comma,square}




\def\epsilon{\varepsilon}
\def\bigoh{\mathcal{O}}
\def\0s{{\bf 0}}
\def\phi{\varphi}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma1}[theorem]{Lemma}
\newtheorem{remark1}[theorem]{Remark}

\newenvironment{lemma}{\begin{lemma1}}{\end{lemma1}}
\newenvironment{remark}{\begin{remark1}\rm}{\end{remark1}}


\journal{the arXiv}

\begin{document}

\begin{frontmatter}





\title{Fast algorithms for spherical harmonic expansions, III}



\author{Mark Tygert}

\address{Courant Institute of Mathematical Sciences, NYU,
         251 Mercer St., New York, NY 10012}

\begin{abstract}
We accelerate the computation of spherical harmonic transforms,
using what is known as the butterfly scheme.
This provides a convenient alternative to the approach taken
in the second paper from this series
on ``Fast algorithms for spherical harmonic expansions.''
The requisite precomputations become manageable when organized
as a ``depth-first traversal'' of the program's control-flow graph,
rather than as the perhaps more natural ``breadth-first traversal''
that processes one-by-one each level of the multilevel procedure.
We illustrate the results via several numerical examples.
\end{abstract}

\begin{keyword}
butterfly \sep algorithm \sep spherical harmonic \sep transform \sep
interpolative decomposition



\end{keyword}

\end{frontmatter}







\section{Introduction}

The butterfly algorithm, introduced in~\cite{michielssen-boag}
and~\cite{oneil-woolfe-rokhlin}, is a procedure for rapidly applying
certain matrices to arbitrary vectors.
(Section~\ref{prelims} below provides a brief introduction to the butterfly.)
The present paper uses the butterfly method
in order to accelerate spherical harmonic transforms.
The butterfly procedure does not require the use of extended-precision
arithmetic in order to attain accuracy very close to the machine precision,
not even in its precomputations --- unlike the alternative approach taken
in the predecessor~\cite{tygert_sph} of the present paper.

Unlike some previous works on the butterfly,
the present article does not use on-the-fly evaluation
of individual entries of the matrices
whose applications to vectors are being accelerated.
Instead, we require only efficient evaluation of full columns of the matrices,
in order to make the precomputations affordable.
Furthermore, efficient evaluation of full columns enables
the acceleration of the application to vectors
of both the matrices and their transposes.
On-the-fly evaluation of columns of the matrices
associated with spherical harmonic transforms is available
via the three-term recurrence relations
satisfied by associated Legendre functions
(see, for example, Section~\ref{spharmonics} below).

The precomputations for the butterfly become affordable when organized
as a ``depth-first traversal'' of the program's control-flow graph,
rather than as the perhaps more natural ``breadth-first traversal''
that processes one-by-one each level of the multilevel butterfly procedure
(see Section~\ref{precomps} below).

The present article is supposed to complement~\cite{oneil-woolfe-rokhlin}
and~\cite{tygert_sph}, combining ideas from both.
Although the present paper is self-contained in principle,
we strongly encourage the reader to begin with~\cite{oneil-woolfe-rokhlin}
and~\cite{tygert_sph}.
The original is~\cite{michielssen-boag}.
Major recent developments are in~\cite{candes-demanet-ying} and~\cite{ying}.
The introduction in~\cite{tygert_sph} summarizes
most prior work on computing fast spherical harmonic transforms;
a new application appears in~\cite{reuter-ratner-seideman}.
These articles and their references highlight the computational use
of spherical harmonic transforms in meteorology and quantum chemistry.
The structure of the remainder of the present article is as follows:
Section~\ref{overview} reviews elementary facts
about spherical harmonic transforms.
Section~\ref{prelims} describes basic tools from previous works.
Section~\ref{precomps} organizes the preprocessing for the butterfly
to make memory requirements affordable.
Section~\ref{spharmonics} outlines the application of the butterfly scheme
to the computation of spherical harmonic transforms.
Section~\ref{numerical} describes the results of several numerical tests.
Section~\ref{conclusions} draws some conclusions.

Throughout, we abbreviate ``interpolative decomposition'' to ``ID''
(see Subsection~\ref{ID_subsection} for a description of the ID).
The butterfly procedures formulated in~\cite{michielssen-boag},
\cite{oneil-woolfe-rokhlin}, and the present paper
all use the ID for efficiency.



\section{An overview of spherical harmonic transforms}
\label{overview}

The spherical harmonic expansion of a bandlimited function 
on the surface of the sphere has the form

where  are the standard spherical coordinates
on the two-dimensional surface of the unit sphere in ,
 and ,
and  is the normalized associated Legendre function
of degree  and order  (see, for example, Subsection~\ref{basic}
for the definition of normalized associated Legendre functions).
Please note that the superscript~ in  denotes an index,
rather than a power. ``Normalized'' refers to the fact that
the normalized associated Legendre functions
of a fixed order are orthonormal on 
with respect to the standard inner product.
Obviously, the expansion~(\ref{expansion}) contains  terms.
The complexity of the function  determines .

In many areas of scientific computing, particularly those
using spectral methods for the numerical solution
of partial differential equations,
we need to evaluate the coefficients 
in an expansion of the form~(\ref{expansion})
for a function  given by a table of its values
at a collection of appropriately chosen nodes
on the two-dimensional surface of the unit sphere.
Conversely, given the coefficients  in~(\ref{expansion}),
we often need to evaluate  at a collection of points
on the surface of the sphere.
The former is known as the forward spherical harmonic transform,
and the latter is known as the inverse spherical harmonic transform.
A standard discretization of the surface of the sphere
is the ``tensor product,''
consisting of all pairs of the form ,
with ,~, \dots,
,~
being the Gauss-Legendre quadrature nodes of degree , that is,

and

for ,~, \dots, ,~,
and with ,~, \dots, ,~
being equispaced on the interval , that is,

for ,~, \dots, ,~.
This leads immediately to numerical schemes
for both the forward and inverse spherical harmonic transforms
whose costs are proportional to .

Indeed, given a function  defined on the two-dimensional surface
of the unit sphere by~(\ref{expansion}),
we can rewrite~(\ref{expansion}) in the form

For a fixed value of , each of the sums over 
in~(\ref{alternate_expansion}) contains no more than  terms,
and there are  such sums (one for each value of );
since the inverse spherical harmonic transform involves  values
,~, \dots, ,~,
the cost of evaluating all sums over  in~(\ref{alternate_expansion})
is proportional to . Once all sums over  have been evaluated,
each sum over  may be evaluated for a cost proportional to 
(since each of them contains  terms),
and there are  such sums to be evaluated
(one for each pair ),
leading to costs proportional to  for the evaluation of
all sums over  in~(\ref{alternate_expansion}).
The cost of the evaluation of the whole inverse spherical harmonic transform
(in the form~(\ref{alternate_expansion})) is the sum
of the costs for the sums over  and the sums over ,
and is also proportional to ;
a virtually identical calculation shows that the cost of evaluating
of the forward spherical harmonic transform is also proportional to .

A trivial modification of the scheme described in the preceding paragraph
uses the fast Fourier transform (FFT) to evaluate the sums over 
in~(\ref{alternate_expansion}),
approximately halving the operation count of the entire procedure.
Several other careful considerations
(see, for example,~\cite{adams-swarztrauber} and~\cite{swarztrauber-spotz})
are able to reduce the costs by 50\% or so,
but there is no simple trick for reducing the costs
of the whole spherical harmonic transform (either forward or inverse)
below . The present paper presents faster (albeit more complicated)
algorithms for both forward and inverse spherical harmonic transforms.
Specifically, the present article provides a fast algorithm
for evaluating a sum over  in~(\ref{alternate_expansion})
at ,~, \dots, ,~,
given the coefficients ,~, \dots,
,~, for a fixed .
Moreover, the present paper provides a fast algorithm
for the inverse procedure of determining the coefficients
,~, \dots, ,~
from the values of a sum over  in~(\ref{alternate_expansion})
at ,~, \dots, ,~.
FFTs or fast discrete sine and cosine transforms can be used
to handle the sums over  in~(\ref{alternate_expansion}) efficiently.
See~\cite{reuter-ratner-seideman} for a detailed summary and novel application
of the overall method. The present article modifies portions of the method
of~\cite{reuter-ratner-seideman} and~\cite{tygert_sph},
focusing exclusively on the modifications.



\section{Preliminaries}
\label{prelims}


In this section, we summarize certain facts from mathematical
and numerical analysis, used in Sections~\ref{precomps} and~\ref{spharmonics}.
Subsection~\ref{ID_subsection} describes interpolative decompositions (IDs).
Subsection~\ref{butterfly} outlines the butterfly algorithm.
Subsection~\ref{basic} summarizes basic properties of normalized
associated Legendre functions.


\subsection{Interpolative decompositions}
\label{ID_subsection}

In this subsection, we define interpolative decompositions (IDs)
and summarize their properties.

The following lemma states that, for any  matrix 
of rank , there exist an  matrix 
whose columns constitute a subset of the columns of ,
and a  matrix , such that
\begin{enumerate}
\item some subset of the columns of  makes up
the  identity matrix,
\item  is not too large, and
\item .
\end{enumerate}
Moreover, the lemma provides an approximation 

when the exact rank of  is greater than ,
yet the (+)st greatest singular value of  is still small.
The lemma is a reformulation
of Theorem~3.2 in~\cite{martinsson-rokhlin-tygert1}
and Theorem~3 in~\cite{cheng-gimbutas-martinsson-rokhlin};
its proof is based on techniques described
in~\cite{goreinov-tyrtyshnikov}, \cite{gu-eisenstat96}, 
and~\cite{tyrtyshnikov}.
We will refer to the approximation in~(\ref{2.1}) of 
as an interpolative decomposition (ID).
We call  the ``interpolation matrix'' of the ID.

\begin{lemma}
\label{interpolation_lemma}
Suppose that  and  are positive integers,
and  is a real  matrix.

Then, for any positive integer  with  and ,
there exist a real  matrix ,
and a real  matrix  whose columns constitute a subset
of the columns of ,
such that
\begin{enumerate}
\item some subset of the columns of  makes up
the  identity matrix,
\item no entry of  has an absolute value greater than ,
\item the spectral norm (that is, the -operator norm) of 
satisfies
,
\item the least (that is, the \/\,th greatest) singular value
of  is at least ,
\item 
when  or , and
\item when  and , the spectral norm
(that is, the -operator norm)
of 
satisfies

where  is the (+)st greatest singular value of .
\end{enumerate}
\end{lemma}

Properties~1,~2,~3, and~4 in Lemma~\ref{interpolation_lemma}
ensure that the ID 
of  is a numerically stable representation.
Also, property~3 follows directly from properties~1 and~2,
and property~4 follows directly from property~1.

\begin{remark}
\label{QR_algorithm}
Existing algorithms for the computation of the matrices 
and  in Lemma~\ref{interpolation_lemma}
are computationally expensive.
We use instead the algorithm of~\cite{cheng-gimbutas-martinsson-rokhlin}
and~\cite{gu-eisenstat96}
to produce matrices  and 
which satisfy slightly weaker conditions than those
in Lemma~\ref{interpolation_lemma}.
We compute  and  such that
\begin{enumerate}
\item some subset of the columns of  makes up
the  identity matrix,
\item \label{computable_bound1}
no entry of  has an absolute value greater than ,
\item the spectral norm (that is, the -operator norm) of 
satisfies
,
\item the least (that is, the th greatest) singular value
of  is at least ,
\item \label{exact}

when  or , and
\item when  and , the spectral norm
(that is, the -operator norm)
of 
satisfies

where  is the (+)st greatest singular value of .
\end{enumerate}
For any positive real number ,
the algorithm can identify the least  such that
.
Furthermore, the algorithm computes both  and 
using at most

floating-point operations, typically requiring only

\end{remark}


\subsection{The butterfly algorithm}
\label{butterfly}

In this subsection, we outline a simple case of the butterfly algorithm
from~\cite{michielssen-boag} and~\cite{oneil-woolfe-rokhlin};
see~\cite{oneil-woolfe-rokhlin} for a detailed description.

Suppose that  is a positive integer, and  is an  matrix.
Suppose further that  and  are positive real numbers,
and  is a positive integer, such that
any contiguous rectangular subblock of  containing at most  entries
can be approximated to precision  by a matrix whose rank is 
(using the Frobenius/Hilbert-Schmidt norm to measure the accuracy
of the approximation);
we will refer to this hypothesis as ``{\it the rank property}.''
The running-time of the algorithm will be proportional to ;
taking  to be roughly proportional to  suffices
for many matrices of interest
(including nonequispaced and discrete Fourier transforms),
so ideally  should be small.
We will say that two matrices  and  are equal
to precision ,
denoted , to mean that the spectral norm
(that is, the -operator norm) of 
is .

We now explicitly use the rank property for subblocks of multiple heights,
to illustrate the basic structure of the butterfly scheme.

Consider any two adjacent contiguous rectangular subblocks  and  of ,
each containing at most  entries and having the same numbers of rows,
with  on the left and  on the right.
Due to the rank property, there exist IDs

and

where  is a matrix having  columns,
which constitute a subset of the columns of ,
 is a matrix having  columns,
which constitute a subset of the columns of ,
 and  are matrices each having  rows,
and all entries of  and  have absolute values
of at most 2.

To set notation, we concatenate the matrices  and ,
and split the columns of the result in half (or approximately in half),
obtaining  on top and  on the bottom:

Observe that the matrices  and  each have at most  entries
(since  and  each have at most  entries).
Similarly, we concatenate the matrices  and ,
and split the columns of the result in half (or approximately in half),
obtaining  and :

Observe that the  columns of  are also columns of ,
and that the  columns of  are also columns of .

Due to the rank property, there exist IDs

and

where  is a matrix having  columns,
which constitute a subset of the columns of ,
 is a matrix having  columns,
which constitute a subset of the columns of ,
 and  are matrices
each having  rows, and all entries of 
and  have absolute values of at most 2.

Combining~(\ref{1})--(\ref{6}) yields that

and


If we use  to denote the number of rows in  (which is the same
as the number of rows in ),
then the number of columns in  (or ) is at most ,
and so the total number of entries in the matrices
in the right-hand sides of~(\ref{1}) and~(\ref{2})
can be as large as ,
whereas the total number of nonzero entries in the matrices
in the right-hand sides of~(\ref{top}) and~(\ref{bottom})
is at most .
If  is nearly as large as possible --- nearly  ---
and  and  are much smaller than ,
then  is about half .
Thus, the representation provided in~(\ref{top}) and~(\ref{bottom})
of the merged matrix from~(\ref{3})
is more efficient than that provided in~(\ref{1}) and~(\ref{2}),
both in terms of the memory required for storage,
and in terms of the number of operations required for applications to vectors.
Notice the advantage of using the rank property for blocks of multiple heights.

Naturally, we may repeat this process of merging adjacent blocks
and splitting in half the columns of the result,
updating the compressed representations after every split.
We start by partitioning  into blocks
each dimensioned 
(except possibly for the rightmost block, which may have fewer 
than  columns),
and then repeatedly group unprocessed blocks (of whatever dimensions)
into disjoint pairs, processing these pairs by merging and splitting them
into new, unprocessed blocks having fewer rows.
The resulting multilevel representation of  allows us to apply 
with precision 
from the left to any column vector, or from the right to any row vector,
using just  floating-point operations
(there will be  levels in the scheme,
and each level except for the last
will only involve  interpolation matrices
of dimensions ,
such as  and ).
Figure~\ref{multi} illustrates the resulting partitionings of  into blocks
of various dimensions (but with every block having the same number of entries),
when  and .
For further details, see~\cite{oneil-woolfe-rokhlin}.

\begin{remark}
Needless to say, the same multilevel representation of 
permits the rapid application of  both from the left to column vectors
and from the right to row vectors.
There is no need for constructing multilevel representations
of both  and the transpose of .
\end{remark}

\begin{remark}
\label{adaptivity}
In practice, the IDs used for accurately approximating subblocks of 
do not all have the same fixed rank~. Instead, for each subblock,
we determine the minimal possible rank such that the associated ID still
approximates the subblock to precision , and we use this ID
in place of one whose rank is .
Determining ranks adaptively in this manner accelerates the algorithm
substantially. For further details, see~\cite{oneil-woolfe-rokhlin}.
All our implementations use this adaptation.
\end{remark}


\begin{figure}
\begin{center}
\caption{The partitionings in the multilevel decomposition
         for an  matrix with }
\label{multi}
\vspace{1.5em}
\begin{minipage}{1in}
\begin{center}
\scalebox{.25}{\includegraphics{level1}} \\
Level 1
\end{center}
\end{minipage}
\begin{minipage}{1in}
\begin{center}
\scalebox{.25}{\includegraphics{level2}} \\
Level 2
\end{center}
\end{minipage}
\begin{minipage}{1in}
\begin{center}
\scalebox{.25}{\includegraphics{level3}} \\
Level 3
\end{center}
\end{minipage}
\begin{minipage}{1in}
\begin{center}
\scalebox{.25}{\includegraphics{level4}} \\
Level 4
\end{center}
\end{minipage}
\\\vspace{1 em}
 indicates the left member of a pair;  indicates the right member. \\
 indicates the top member of a pair;  indicates the bottom member.
\end{center}
\end{figure}


\subsection{Normalized associated Legendre functions}
\label{basic}


In this subsection, we discuss several classical facts concerning
normalized associated Legendre functions.
All of these facts follow trivially from results contained,
for example, in~\cite{abramowitz-stegun} or~\cite{szego}.


For any nonnegative integers  and  such that ,
we use  to denote the normalized associated Legendre function
of degree  and order , defined on  via the formula

where  is the Legendre polynomial of degree ,

(see, for example, Chapter~8 of~\cite{abramowitz-stegun}).
``Normalized'' refers to the fact that
the normalized associated Legendre functions
of a fixed order  are orthonormal on 
with respect to the standard inner product.
If  is even, then 
for any .
If  is odd, then 
for any .


The following lemma states that the normalized associated Legendre functions
satisfy a certain self-adjoint second-order linear (Sturm-Liouville)
differential equation.

\begin{lemma}
Suppose that  is a nonnegative integer.

Then,

for any , and ,~, , \dots.
\end{lemma}


The following lemma states that the normalized associated Legendre function
of order  and degree  has exactly  zeros inside ,
and, moreover, that the normalized associated Legendre function
of order  and degree  also has exactly  zeros inside .

\begin{lemma}
\label{distinct_theorem}
Suppose that  and  are nonnegative integers with .

Then, there exist precisely  real numbers
,~, \dots, ,~ such that

and

for ,~, \dots, ,~.

Moreover, there exist precisely  real numbers
,~, \dots, ,~ such that

and

for ,~, \dots, ,~.
\end{lemma}


Suppose that  and  are nonnegative integers with .
Then, we define real numbers
,~, \dots, ,~,
,~, \dots, ,~,
and  via the formulae

for ,~, \dots, ,~,
where ,~, \dots, ,~ are from~(\ref{zeros}),

for ,~, \dots, ,~,
where ,~, \dots, ,~ are from~(\ref{zeros2}), and


The following lemma describes what are known as Gauss-Jacobi quad\-rature
formulae corresponding to associated Legendre functions.

\begin{lemma}
\label{quadratures}
Suppose that  and  are nonnegative integers with .

Then,

for any even polynomial  of degree at most ,
where ,~, \dots, ,~ are from~(\ref{zeros}),
and ,~, \dots, ,~
are defined in~(\ref{weights1}).

Furthermore,

for any even polynomial  of degree at most ,
where ,~, \dots, ,~ are from~(\ref{zeros2}),
and ,~, \dots, ,~
are defined in~(\ref{weights2}) and~(\ref{weights2_0}).
\end{lemma}

\begin{remark}
Formulae~(35) and~(36) of~\cite{tygert_sph} incorrectly
omitted the factors  and 
appearing in the analogous~(\ref{Christoffel_definition})
and~(\ref{Christoffel_definition2}) above.
\end{remark}


Suppose that  is a nonnegative integer.
Then, we define real numbers ,~, , \dots\
and ,~, , \dots\ via the formulae

for ,~,~, \dots, and

for ,~,~, \dots.


The following lemma states that the normalized associated Legendre functions
of a fixed order  satisfy a certain three-term recurrence relation.

\begin{lemma}
\label{rec_relation}
Suppose that  is a nonnegative integer.

Then,

for any , and  or , and

for any , and ,~,~, \dots,
where ,~,~, \dots\ are defined in~(\ref{c_rec}),
and ,~,~, \dots\ are defined in~(\ref{d_rec}).
\end{lemma}



\section{Precomputations for the butterfly scheme}
\label{precomps}

In this section, we discuss the preprocessing required
for the butterfly algorithm summarized in Subsection~\ref{butterfly}.
We will be using the notation detailed in Subsection~\ref{butterfly}.

Perhaps the most natural organization of the computations required
to construct the multilevel representation of an  matrix 
is first to process all blocks having  rows
(Level~1 in Figure~\ref{multi} above),
then to process all blocks having about  rows
(Level~2 in Figure~\ref{multi}),
then to process all blocks having about  rows
(Level~3 in Figure~\ref{multi}), and so on.
Indeed, \cite{oneil-woolfe-rokhlin} uses this organization,
which amounts to a ``breadth-first traversal''
of the control-flow graph for the program applying  to a vector
(see, for example, \cite{aho-hopcroft-ullman}
for an introduction to ``breadth-first'' and ``depth-first'' orderings).
This scheme for preprocessing is efficient when the entries of 
can be efficiently computed on-the-fly, individually.
(Of course, we are assuming that  has a suitable rank property, that is,
that there are positive real numbers  and ,
and a positive integer , such that
any contiguous rectangular subblock of  containing at most  entries
can be approximated to precision  by a matrix whose rank is ,
using the Frobenius/Hilbert-Schmidt norm to measure the accuracy
of the approximation. Often, taking  to be roughly proportional
to  suffices, and ideally  and  are small.)
If the entries of  cannot be efficiently computed individually,
however, then the ``breadth-first traversal'' may need
to store  entries at some point during the precomputations,
in order to avoid recomputing entries of the matrix.

If individual columns of 
(but not necessarily arbitrary individual entries) can be computed efficiently,
then ``depth-first traversal'' of the control-flow graph
requires only  floating-point words of memory
at any point during the precomputations, for the following reason.
We will say that we ``process'' a block of  to mean that we merge it
with another, and split and recompress the result,
producing a pair of new, unprocessed blocks.
Rather than starting the preprocessing by constructing all blocks
having  rows, we construct each such block only after processing
as many blocks as possible which previous processing creates,
but which have not yet been processed.
Furthermore, we construct each block having  rows only after having already
constructed (and possibly processed) all blocks to its left.
To reiterate, we construct a block having  rows only after having exhausted
all possibilities for both creating and processing blocks to its left.

For each {\it processed} block , we need only store the interpolation matrix
 and the indices of the columns chosen for the ID;
we need not store the  columns of  selected for the ID,
since the algorithm for applying  (or its transpose) to a vector
never explicitly uses any columns of a block that has been merged
with another and split, but instead interpolates from (or anterpolates to)
the shorter blocks arising from the processing.
Conveniently, the matrix  that we must store is small
-- no larger than .
For each {\it unprocessed} block , we do need to store the  columns
in  selected for the ID, in addition to storing 
and the indices of the columns chosen for the ID,
facilitating any subsequent processing.
Although  may have many rows, it has only as many rows as 
and hence is smaller when  has fewer rows.
Thus, every time we process a pair of tall blocks, producing a new pair
of blocks having half as many rows,
the storage requirements for all these blocks together nearly halve.
By always processing as many already constructed blocks as possible,
we minimize the amount of memory required.



\section{Spherical harmonic transforms via the butterfly scheme}
\label{spharmonics}


In this section, we describe how to use the butterfly algorithm
to compute fast spherical harmonic transforms,
via appropriate modifications of the algorithm of~\cite{tygert_sph}.


We substitute the butterfly algorithm for the divide-and-conquer algorithm
of~\cite{gu-eisenstat95} used in Section~3.1 of~\cite{tygert_sph},
otherwise leaving the approach of~\cite{tygert_sph} unchanged. Specifically,
given numbers ,~, \dots, ,~,
we use the butterfly scheme to compute the numbers
,~, \dots, ,~ defined
via the formula

for ,~, \dots, ,~,
where  is a nonnegative integer,
,~, \dots,
,~
are the normalized associated Legendre functions of order  defined
in~(\ref{association}),
,~, \dots, ,~ are the positive zeros
of  from~(\ref{zeros}),
and ,~, \dots, ,~
are the corresponding quadrature weights from~(\ref{Christoffel_definition}).
Similarly,
given numbers~,~, \dots, ,~,
we use the butterfly scheme to compute the numbers
,~, \dots, ,~
satisfying~(\ref{transform}).
The factors ,~, \dots,
,~ ensure that the linear transformation
mapping ,~, \dots, ,~
to ,~, \dots, ,~
via~(\ref{transform}) is unitary
(due to~(\ref{association}),~(\ref{Christoffel_definition}),
and the orthonormality of the normalized associated Legendre functions
on ), so that the inverse of the linear transformation
is its transpose.

Moreover, given numbers
,~, \dots, ,~,
we use the butterfly scheme to compute the numbers
,~, \dots, ,~ defined via the formula

for ,~, \dots, ,~,
where  is a nonnegative integer,
,~, \dots,
,~
are the normalized associated Legendre functions of order  defined
in~(\ref{association}),
,~, \dots, ,~ are the positive zeros
of  from~(\ref{zeros2}),
and ,~, \dots, ,~
are the corresponding quadrature weights from~(\ref{Christoffel_definition2}).
Similarly,
given numbers~,~, \dots, ,~,
we use the butterfly scheme to compute the numbers
,~, \dots, ,~
satisfying~(\ref{transform2}).
As above, the factors ,~, \dots,
,~ ensure that
the linear transformation mapping
,~, \dots, ,~
to ,~, \dots, ,~
via~(\ref{transform2}) is unitary, so that its inverse is its transpose.

Computing spherical harmonic transforms requires
several additional computations, detailed in~\cite{tygert_sph}.
(See also Remark~\ref{omission} below.)
The butterfly algorithm replaces only the procedure described
in Section~3.1 of~\cite{tygert_sph}.

In order to use~(\ref{transform}) and~(\ref{transform2}) numerically,
we need to precompute the positive zeros
,~, \dots, ,~ 
of  from~(\ref{zeros}),
the corresponding quadrature weights
,~, \dots, ,~
from~(\ref{Christoffel_definition}),
the positive zeros ,~, \dots, ,~
of  from~(\ref{zeros2}),
and the corresponding quadrature weights
,~, \dots, ,~
from~(\ref{Christoffel_definition2}).
Section~3.3 of~\cite{tygert_sph} describes suitable procedures
(based on integrating the ordinary differential equation
in~(\ref{sturm-liouville_Leg}) in ``Pr\"ufer coordinates'').
We found it expedient to perform this preprocessing in extended-precision
arithmetic, in order to compensate for the loss of a couple of digits
of accuracy relative to the machine precision.

To perform the precomputations described in Section~\ref{precomps} above
associated with~(\ref{transform}) and~(\ref{transform2}),
we need to be able to evaluate efficiently all  functions
,~, \dots,
,~
at any of the precomputed positive zeros
,~, \dots, ,~ 
of  from~(\ref{zeros}),
and, similarly, we need to be able to evaluate efficiently all  functions
,~, \dots,
,~
at any of the precomputed positive zeros
,~, \dots, ,~ 
of  from~(\ref{zeros2}).
For this, we may use the recurrence relations~(\ref{recurrence0})
and~(\ref{recurrence}), starting with the values of ,
, ,
and~ obtained via~(\ref{association}).
(We can counter underflow by tracking exponents explicitly,
in the standard fashion.)
Such use of the recurrence is a classic procedure;
see, for example, Chapter~8 of~\cite{abramowitz-stegun}.
The recurrence appears to be numerically stable when used
for evaluating normalized associated Legendre functions of order 
and of degrees at most ,
at these special points ,~, \dots, ,~ 
and ,~, \dots, ,~, even when  is very large.
We did not need to use extended-precision arithmetic for this preprocessing.

\begin{remark}
\label{omission}
The formula~(88) in~\cite{tygert_sph} that is analogous to~(\ref{transform})
of the present paper omits the factors ,~, \dots,
,~ included in~(\ref{transform}).
Obviously, the vectors

and

differ by a diagonal transformation, and so we can obtain either one
from the other efficiently.
In fact, the well-conditioned matrix  from Section~3.1 of~\cite{tygert_sph}
represents the same diagonal transformation,
mapping~(\ref{no_diag}) to~(\ref{with_diag}).
Similar remarks apply to~(\ref{transform2}), of course.
\end{remark}



\section{Numerical results}
\label{numerical}

In this section, we describe the results of several numerical tests
of the algorithm of the present paper.
(Computing spherical harmonic transforms requires
several additional computations, detailed in~\cite{tygert_sph}
--- see Section~\ref{spharmonics} for further information.
The butterfly algorithm replaces only the procedure described
in Section~3.1 of~\cite{tygert_sph}.)

Tables~\ref{evens_time}--\ref{small_pre}
report the results of computing from real numbers
,~, \dots, ,~
the real numbers ,~, \dots, ,~
defined by the formula

for ,~, \dots, ,~,
where ,~, \dots,
,~
are the normalized associated Legendre functions defined
in~(\ref{association}),
,~, \dots, ,~ are the positive zeros
of  from~(\ref{zeros}),
and ,~, \dots, ,~
are the corresponding quadrature weights from~(\ref{Christoffel_definition}).
We will refer to the map via~(\ref{evens})
from ,~, \dots, ,~
to ,~, \dots, ,~
as the forward transform, and the map via~(\ref{evens})
from ,~, \dots, ,~
to ,~, \dots, ,~
as the inverse transform (the inverse is also the transpose,
due to~(\ref{association}),~(\ref{Christoffel_definition}),
and the orthonormality of the normalized associated Legendre functions
on ).
The values of  differ in Tables~1--2 and 3--4.

Tables~\ref{odds_time} and~\ref{odds_pre} report the results
of computing from real numbers
,~, \dots, ,~
the real numbers ,~, \dots, ,~
defined by the formula

for ,~, \dots, ,~,
where ,~, \dots,
,~
are the normalized associated Legendre functions defined
in~(\ref{association}),
,~, \dots, ,~ are the positive zeros
of  from~(\ref{zeros2}),
and ,~, \dots, ,~
are the corresponding quadrature weights from~(\ref{Christoffel_definition2}).
We will refer to the map via~(\ref{odds})
from ,~, \dots, ,~
to ,~, \dots, ,~
as the forward transform, and the map via~(\ref{odds})
from ,~, \dots, ,~
to ,~, \dots, ,~
as the inverse transform (as above, the inverse is also the transpose).

For the test vectors  and  whose entries appear
in~(\ref{evens}) and~(\ref{odds}), we used normalized vectors whose entries
were pseudorandom numbers drawn uniformly from ,
normalized so that the sum of the squares of the entries is 1.

As described in Remark~\ref{adaptivity},
we compute for each block in the multilevel representation of 
an ID whose rank is as small as possible while still approximating
the block to nearly machine precision.

\begin{itemize}
\item[] The following list describes the headings of the tables:
\item  is the size of the transform, the size of the vectors
       and  whose entries are given in~(\ref{evens}),
      and of the vectors  and  whose entries are given
      in~(\ref{odds}).
\item  is the order of the normalized associated Legendre functions
      used in~(\ref{evens}) and~(\ref{odds}).
\item  is the maximum of the ranks of the IDs for the blocks
      in the multilevel representation.
\item  is the average of the ranks of the IDs for the blocks
      in the multilevel representation.
\item  is the standard deviation of the ranks of the IDs
      for the blocks in the multilevel representation.
\item  is the time in seconds required to apply an 
      matrix to an  vector using the standard procedure.
      We estimated the last two entries for  by multiplying
      the third-to-last entry in each table by 4 and 16,
      since the large matrices required to generate those entries
      cannot fit in the available 2~GB of RAM. We indicate that these entries
      are estimates by enclosing them in parentheses.
\item  is the time in seconds required by the butterfly algorithm
      to compute the forward transform via~(\ref{evens}) or~(\ref{odds}),
      mapping from ,~, \dots, ,~
      to ,~, \dots, ,~,
      or from ,~, \dots, ,~
      to ,~, \dots, ,~.
\item  is the time in seconds required by the butterfly algorithm
      to compute the inverse transform via~(\ref{evens}) or~(\ref{odds}),
      mapping from ,~, \dots,
      ,~
      to ,~, \dots, ,~,
      or from ,~, \dots, ,~
      to ,~, \dots, ,~.
\item  is the time in seconds required in the precomputations
      to compute the quadrature nodes ,~, \dots, ,~
      or ,~, \dots, ,~,
      and weights ,~, \dots, ,~
      or ,~, \dots, ,~,
      from~(\ref{Christoffel_definition}) or~(\ref{Christoffel_definition2}),
      used in~(\ref{evens}) and~(\ref{odds}).
\item  is the time in seconds required to construct
      the compressed multilevel representation used in the butterfly algorithm,
      after having already computed
      the quadrature nodes ,~, \dots, ,~
      or ,~, \dots, ,~,
      and weights ,~, \dots, ,~
      or ,~, \dots, ,~,
      from~(\ref{Christoffel_definition}) or~(\ref{Christoffel_definition2}),
      used in~(\ref{evens}) and~(\ref{odds}).
\item  is the maximum number of floating-point words
      of memory required to store entries of the transform matrix
      during any point in the precomputations
      (all other memory requirements are negligible in comparison).
\item  is the maximum difference between the entries
      in the result of the forward transform computed
      via the butterfly algorithm and those computed directly
      via the standard procedure for applying a matrix to a vector.
      (The result of the forward transform is the vector 
      whose entries are given in~(\ref{evens})
      or the vector  whose entries are given in~(\ref{odds}).)
\item  is the maximum difference between the entries
      in a test vector and the entries in the result of applying
      to the test vector first the forward transform and
      then the inverse transform, both computed via the butterfly algorithm.
      (The result of the forward transform is the vector 
      whose entries are given in~(\ref{evens})
      or the vector  whose entries are given in~(\ref{odds}).
      The result of the inverse transform is the vector 
      whose entries are given in~(\ref{evens})
      or the vector  whose entries are given in~(\ref{odds}).)
      Thus,  measures the accuracy
      of the butterfly algorithm without reference to the standard procedure
      for applying a matrix to a vector (unlike ).
\end{itemize}

For the first level of the multilevel representation
of the  matrix, we partitioned the matrix
into blocks each dimensioned 
(except for the rightmost block, since  is not divisible by 60).
Every block on every level has about the same number of entries
(specifically,  entries).
We wrote all code in Fortran 77, compiling it using
the Lahey-Fujitsu Linux Express v6.2 compiler, with optimization flag
{\tt {-}{-}o2} enabled.
We ran all examples on one core of a 2.7~GHz Intel Core~2 Duo microprocessor
with 3~MB of L2 cache and 2~GB of RAM.
As described in Section~\ref{spharmonics},
we used extended-precision arithmetic during the portion of the preprocessing
requiring integration of an ordinary differential equation,
to compute quadrature nodes and weights
(this is not necessary to attain high accuracy, but does yield a couple
of extra digits of precision).
Otherwise, our code is compliant with the IEEE double-precision standard
(so that the mantissas of variables have approximately one bit of precision
less than 16 digits, yielding a relative precision of about .2E--15).


\begin{remark}
\label{empirical_rank}
Tables~\ref{evens_time}, \ref{small_time}, and~\ref{odds_time}
indicate that the linear transformations in~(\ref{evens}) and~(\ref{odds})
satisfy the rank property discussed in Subsection~\ref{butterfly},
with arbitrarily high precision, at least in some averaged sense.
Furthermore, it appears that the parameter  discussed
in Subsection~\ref{butterfly} can be set to be independent
of the order  and size  of the transforms in~(\ref{evens})
and~(\ref{odds}), with the parameter  discussed
in Subsection~\ref{butterfly} roughly proportional to .
The acceleration provided by the butterfly algorithm thus
is sufficient for computing fast spherical harmonic transforms,
and is competitive with the approach taken in~\cite{tygert_sph}
(though much work remains in optimizing both approaches
in order to gauge their relative performance).
Unlike the approach taken in~\cite{tygert_sph},
the approach of the present paper does not require the use
of extended-precision arithmetic during the precomputations
in order to attain accuracy close to the machine precision,
even while accelerating spherical harmonic transforms about as well.
Moreover, the butterfly can be easier to implement.
\end{remark}

\begin{remark}
The values in Tables~\ref{evens_pre}, \ref{small_pre}, and~\ref{odds_pre}
vary with the size  of the transforms in~(\ref{evens}) and~(\ref{odds})
as expected.
The values for 
are consistent with the expected values of a constant times .
The values for 
are consistent with the expected values of a constant times 
(with the constant being proportional to ).
The values for 
are consistent with the expected values of a constant times 
(again with the constant being proportional to );
these modest memory requirements make the preprocessing feasible
for large values of  such as those in the tables.
\end{remark}

\begin{remark}
In the current technological environment,
neither the scheme of~\cite{tygert_sph} nor the approach of the present paper
is uniformly superior to the other.
For example, the theory from~\cite{tygert_sph} is rigorous
and essentially complete, while the theory of the present article ideally
should undergo further development, to prove that the rank properties discussed
in Remark~\ref{empirical_rank} are as strong as numerical experiments indicate,
yielding the desired acceleration.
In contrast, to attain accuracy close to the machine precision,
the approach of~\cite{tygert_sph} requires the use of extended-precision
arithmetic during its precomputations,
whereas the scheme of the present paper does not.
Implementing the procedure of the present article can be easier.
Finally, an anonymous referee kindly compared the running-times
of the implementations reported in~\cite{tygert_sph} and the present paper,
noticing that the newer computer system used in the present article is
about 2.7 times faster than the old system; the algorithms of~\cite{tygert_sph}
and of the present article are roughly equally efficient ---
certainly neither appears to be more than twice faster than the other.
However, both implementations are rather crude, and could undoubtedly benefit
from further optimization by experts on computer architectures;
also, we made no serious attempt to optimize the precomputations.
Furthermore, with the advent of multicore and distributed processors,
coming changes in computer architectures might affect
the two approaches differently, as they may parallelize and utilize cache
in different ways.
In the end, the use of one approach rather than the other may be
a matter of convenience, as the two methods are yielding similar performance.
\end{remark}



\section{Conclusions}
\label{conclusions}

This article provides an alternative means for performing the key
computational step required in~\cite{tygert_sph} for computing
fast spherical harmonic transforms.
Unlike the implementation described in~\cite{tygert_sph}
of divide-and-conquer spectral methods,
the butterfly scheme of the present paper does not require the use
of extended precision during the compression precomputations in order
to attain accuracy very close to the machine precision.
With the butterfly, the required amount of preprocessing is quite reasonable,
certainly not prohibitive.

Unfortunately, there seems to be little theoretical understanding
of why the butterfly procedure works so well for associated Legendre functions
(are the associated transforms nearly weighted averages
of Fourier integral operators?).
Complete proofs such as those in~\cite{oneil-woolfe-rokhlin}
and~\cite{tygert_sph} are not yet available for the scheme
of the present article.
By construction, the butterfly enables fast, accurate
applications of matrices to vectors when the precomputations succeed.
However, we have yet to prove that the precomputations
will compress the appropriate  matrix
enough to enable applications of the matrix to vectors
using only  floating-point operations (flops).
Nevertheless, the scheme has succeeded in all our numerical tests.
We hope to produce rigorous mathematical proofs that the precomputations
always compress the matrices as much as they did in our numerical experiments.

The precomputations for the algorithm of the present article
require  flops.
The precomputations for the algorithm of~\cite{tygert_sph} also
require  flops as implemented for the numerical examples
of that paper; however, the procedure of~\cite{gu-eisenstat95}
leads naturally to precomputations for the approach of~\cite{tygert_sph}
requiring only  flops
(though these ``more efficient'' precomputations do not become more efficient
in practice until  is absurdly large, too large even to estimate reliably).
We do not expect to be able to accelerate the precomputations
for the algorithm of the present article without first producing
the rigorous mathematical proofs mentioned in the previous paragraph.
Even so, the current amount of preprocessing is not unreasonable,
as the numerical examples of Section~\ref{numerical} illustrate.



\section*{Acknowledgements}

We would like to thank V. Rokhlin for his advice, for his encouragement,
and for the use of his software libraries,
all of which have greatly enhanced this paper
and the associated computer codes.
We are also grateful to R. R. Coifman and Y. Shkolnisky.
We would like to thank the anonymous referees for their useful suggestions.



\begin{table}[p]
\caption{Ranks and running-times for even degrees}
\label{evens_time}
\vspace{1em}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rrcccccc}
 &    &  &  &  &  &  &  \\\hline
 1250 &  1250 &           170 &          65.3 &         29.5 &       .25E--2 &       .18E--2 &       .15E--2 \\\hline
 2500 &  2500 &           168 &          67.0 &         32.6 &       .98E--2 &       .46E--2 &       .38E--2 \\\hline
 5000 &  5000 &           195 &          70.5 &         35.9 &       .39E--1 &       .12E--1 &       .96E--2 \\\hline
10000 & 10000 &           247 &          73.5 &         38.7 &       .15E--0 &       .28E--1 &       .23E--1 \\\hline
20000 & 20000 &           308 &          75.9 &         41.5 &      (.60E--0)&       .67E--1 &       .55E--1 \\\hline
40000 & 40000 &           379 &          78.0 &         43.8 &       (.24E+1)&       .16E--0 &       .13E--0 \\\hline
\end{tabular*}
\end{table}


\begin{table}[p]
\caption{Precomputation times, memory requirements, and accuracies
         for even degrees}
\label{evens_pre}
\vspace{1em}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rrccccc}
 &    &  &  &  &  &  \\\hline
 1250 &  1250 &          .46E2 &          .96E0 &         .86E6 &             .62E--14 &             .19E--13 \\\hline
 2500 &  2500 &          .92E2 &          .41E1 &         .20E7 &             .37E--14 &             .25E--13 \\\hline
 5000 &  5000 &          .18E3 &          .17E2 &         .50E7 &             .59E--14 &             .43E--13 \\\hline
10000 & 10000 &          .37E3 &          .82E2 &         .14E8 &             .32E--14 &             .57E--13 \\\hline
20000 & 20000 &          .74E3 &          .39E3 &         .29E8 &             .30E--14 &             .88E--13 \\\hline
40000 & 40000 &          .15E4 &          .17E4 &         .64E8 &             .24E--14 &             .13E--12 \\\hline
\end{tabular*}
\end{table}


\begin{table}[p]
\caption{Ranks and running-times for even degrees}
\label{small_time}
\vspace{1em}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rrcccccc}
 &    &  &  &  &  &  &  \\\hline
 1250 &     0 &           110 &          67.0 &         23.3 &       .25E--2 &       .18E--2 &       .15E--2 \\\hline
 2500 &     0 &           110 &          70.0 &         25.0 &       .98E--2 &       .48E--2 &       .40E--2 \\\hline
 5000 &     0 &           111 &          73.9 &         26.1 &       .39E--1 &       .12E--1 &       .10E--1 \\\hline
10000 &     0 &           111 &          77.3 &         26.8 &       .15E--0 &       .29E--1 &       .24E--1 \\\hline
20000 &     0 &           112 &          80.2 &         27.1 &      (.60E--0)&       .68E--1 &       .57E--1 \\\hline
40000 &     0 &           169 &          82.7 &         27.4 &       (.24E+1)&       .16E--0 &       .13E--0 \\\hline
\end{tabular*}
\end{table}


\begin{table}[p]
\caption{Precomputation times, memory requirements, and accuracies
         for even degrees}
\label{small_pre}
\vspace{1em}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rrccccc}
 &    &  &  &  &  &  \\\hline
 1250 &     0 &          .44E2 &          .96E0 &         .86E6 &             .49E--14 &             .12E--12 \\\hline
 2500 &     0 &          .88E2 &          .41E1 &         .20E7 &             .35E--14 &             .14E--12 \\\hline
 5000 &     0 &          .18E3 &          .18E2 &         .51E7 &             .23E--14 &             .35E--12 \\\hline
10000 &     0 &          .36E3 &          .82E2 &         .14E8 &             .18E--14 &             .63E--12 \\\hline
20000 &     0 &          .74E3 &          .40E3 &         .29E8 &             .20E--14 &             .22E--11 \\\hline
40000 &     0 &          .14E4 &          .18E4 &         .66E8 &             .16E--14 &             .37E--11 \\\hline
\end{tabular*}
\end{table}


\newpage


\begin{table}[h]
\caption{Ranks and running-times for odd degrees}
\label{odds_time}
\vspace{1em}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rrcccccc}
 &    &  &  &  &  &  &  \\\hline
 1250 &  1250 &           170 &          65.3 &         29.5 &       .25E--2 &       .17E--2 &       .15E--2 \\\hline
 2500 &  2500 &           169 &          67.0 &         32.6 &       .98E--2 &       .48E--2 &       .39E--2 \\\hline
 5000 &  5000 &           196 &          70.5 &         35.9 &       .39E--1 &       .12E--1 &       .97E--2 \\\hline
10000 & 10000 &           247 &          73.5 &         38.7 &       .15E--0 &       .28E--1 &       .24E--1 \\\hline
20000 & 20000 &           308 &          75.9 &         41.4 &      (.60E--0)&       .68E--1 &       .56E--1 \\\hline
40000 & 40000 &           379 &          78.0 &         43.8 &       (.24E+1)&       .16E--0 &       .13E--0 \\\hline
\end{tabular*}
\end{table}


\begin{table}[h]
\caption{Precomputation times, memory requirements, and accuracies
         for odd degrees}
\label{odds_pre}
\vspace{1em}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}rrccccc}
 &    &  &  &  &  &  \\\hline
 1250 &  1250 &          .46E2 &          .94E0 &         .86E6 &             .41E--14 &             .19E--13 \\\hline
 2500 &  2500 &          .92E2 &          .40E1 &         .20E7 &             .41E--14 &             .29E--13 \\\hline
 5000 &  5000 &          .18E3 &          .17E2 &         .50E7 &             .40E--14 &             .51E--13 \\\hline
10000 & 10000 &          .37E3 &          .80E2 &         .14E8 &             .31E--14 &             .62E--13 \\\hline
20000 & 20000 &          .74E3 &          .39E3 &         .29E8 &             .34E--14 &             .10E--12 \\\hline
40000 & 40000 &          .15E4 &          .18E4 &         .64E8 &             .25E--14 &             .14E--12 \\\hline
\end{tabular*}
\end{table}



\vspace{.333in}











\section*{References}
\bibliographystyle{elsarticle-num_sorted}
\bibliography{but.bib}














\end{document}
