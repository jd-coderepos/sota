\def\bfv{\mathbf{v}}
\def\bfc{\mathbf{c}}
\def\bfp{\mathbf{p}}
\def\bfw{\mathbf{w}}
\def\bft{\mathbf{t}}
\def\bfd{\mathbf{d}}
\def\bfX{\mathbf{X}}
\def\bfS{\mathbf{S}}
\def\bfx{\mathbf{x}}
\def\bfSigma{\mathbf{\Sigma}}
\def\bfdelta{\mathbf{\delta}}
\def\bfDelta{\mathbf{\Delta}}
\def\cO{\mathop{\mathcal{O}}}
\def\barmatrix#1#2#3#4{\left(\begin{array}{c|c}#1&#2\\\hline#3&#4\end{array}\right)}
\def\barvector#1#2{\left(\begin{array}{c|c}#1&#2\end{array}\right)}


\ifx\thechapter\undefined\else

\author{\ChapSevenAuthorOne}

\chapterauthor{\ChapSevenAuthorOne}

\setcounter{chapter}{6}


\chapter{The block Lanczos algorithm}\label{chap7:blocklanczos}

\contributor{\ChapSevenAuthorOne
\affiliation{\ChapSevenAuthorOneAffiliation}}


We present the block Lanczos algorithm proposed by Peter Montgomery,
which is an efficient means to tackle the sparse linear algebra problem which
arises in the context of the number field sieve factoring algorithm and
its predecessors. The presentation incorporates some
simplifications and improvements.

\fi

\section{Linear systems for integer factoring}
\label{sec:blocklanczos:intro}

For factoring a composite integer , algorithms based on the technique
of combination of congruences look for several pairs of integers 
such that  This equality is hoped to be non
trivial for at least one of the obtained pairs, letting 
unveil a factor of the integer .

Several algorithms use this strategy: the CFRAC algorithm,  the
quadratic sieve and its variants, and the number field sieve. Pairs
 as above are obtained by combining relations which have been collected
as a step of these algorithms. Relations are written multiplicatively as
a set of valuations. All the algorithms considered seek a multiplicative
combination of these relations which can be rewritten as an equality of
squares. This is achieved by solving a system of linear equations defined
over , where equations are parity constraints on each valuation
considered, and unknowns indicate whether or not relations are to be
selected as part of the combination.


We are therefore facing a linear algebra problem. Writing the relations
collected as the rows of a matrix  with coefficients in , we are
to find several solutions to the homogeneous linear system  To
fix notations, we let the matrix  be square of size . It is
noteworthy that the matrix  is extremely sparse, as can be illustrated
by data from some factoring experiments: for the factoring of RSA-512 in
1999, the matrix  had  and  non-zero
coefficients per row, and for the RSA-768 factorization in 2009, the
matrix  had  and  non-zero coefficients per
row.

This sparsity property can be exploited to yield efficient algorithms
which solve the linear system in a ``black-box'' fashion, that is,
without ever modifying the matrix . The only access to the matrix 
which is allowed to such algorithms is the operation of multiplying 
(or its transpose) by a vector, and obtain the result. The interesting
black-box algorithms are those which solve the linear system using at
most  times this operation. For sparse matrices, this approach is
considerably cheaper than ``dense'' algorithms which do not exploit the
sparsity property, with regard to both the time and space complexity
(which would, for dense algorithms, be  and ).

\section{The standard Lanczos algorithm}
\label{sec:blocklanczos:lanczos}

Dealing with sparse linear systems is an important topic which goes
beyond computational number theory.  Among the sparse algorithms which
can be employed (reviewed in early works such as~\cite{C:LaMOdl90a}), we
find the conjugate gradient and the Lanczos algorithms, which were both
originally stated in the context of solving numerical systems occurring,
for example, in the context of the solution of partial differential
equations. With some adaptation work, it is possible to use these
algorithms over finite fields, with limitations which we will
mention in §\ref{sec:blocklanczos:char2}.
The Wiedemann algorithm~\cite{Wiedemann86} was proposed as a
method particularly well adapted to finite fields. We will discuss in
§\ref{sec:blocklanczos:recent} how it
compares with the Lanczos and block Lanczos algorithms.

As a first step towards presenting the block Lanczos algorithm, we give
here an overview of how the standard Lanczos algorithm can be used to
solve homogeneous or inhomogeneous linear systems over finite fields.
Arguments which appear in the justification of the standard Lanczos are
also important to the block Lanczos context, which explains this
preliminary overview.  Within this section, we assume that the base field
is  for some prime .

Briefly put, the Lanczos algorithm is the Gram-Schmidt orthogonalization
process applied to a Krylov subspace. We need to work with a symmetric
matrix  defined over .  Different problems can be stated, for
example depending on whether we intend to solve a homogeneous or
inhomogeneous linear system. Another distinction comes from the linear
system which we want to solve in the first place. While in some cases it
does indeed define a symmetric matrix , it may also be that we form
 as , and solve a linear system involving  as a derived
means of solving one involving . Such a strategy would be natural in
the prospect of solving the linear systems as defined in
§\ref{sec:blocklanczos:intro}. In that case, the matrix  is never actually
computed, and the black box ``multiplication by '' is instead realized
as the composition
of the two black boxes multiplying by  and .

For expository purposes, we assume in
this section that we have a right-hand side vector ,
and intend to solve for  the equation


The matrix  being symmetric, we may consider
the inner product defined from  as  for 
vectors . We say that  and  are -orthogonal
whenever . A vector is -isotropic if it is -orthogonal to
itself.

The Lanczos algorithm focuses on the sequence of Krylov subspaces of 
defined as ,
where .
It is clear that the sequence of subspaces
 is
strictly increasing up to some index, and then stationary.

We define a sequence of vectors , 
computed so as to satisfy the two following conditions:

We proceed by induction, and assume that a sequence of vectors  to
 has been computed so that the two conditions above hold.
We now see how to compute .
We begin by noting (using condition~\eqref{eq:lanczos:condition-span}
inductively) that 
so
that setting  to be any vector within the affine subspace
 fulfils condition~\eqref{eq:lanczos:condition-span} for
index . In
order to satisfy condition~\eqref{eq:lanczos:condition-orth}, we let:

We leave for further discussion the important question of the
non-degeneracy of the denominators in the expression of .

It turns out that the equation above defining  can be
simplified.
Indeed, because , we have that
 whenever . This implies that only two terms in
the sum above are non-zero, yielding the following shorter equation for
defining :

Note also that we have ,
so that . We can
then simplify the expression of  as


The sequence of vectors  can thus be computed
with a simple recurrence procedure, requiring only a short amount of
history to be updated from each iteration to the next (namely, the
vectors  and  as well as the scalar ). 

We now discuss the termination of the computation of the sequence of
vectors . It is clear that  can be computed only
as long as the following condition holds:

We assume
that condition~\eqref{eq:lanczos:condition-non-isotropic} holds until
some index  (not included), and that . This implies
. Define now  as

By construction\footnote{This argument uses the fact that we have chosen
. Had we chosen  arbitrarily, then we would need to assume
.}, we have . Vectors  for indices 
form an -orthogonal basis of , therefore  for all
 because of the expression of . It follows from~\eqref{eq:lanczos:condition-orth}
and~\eqref{eq:lanczos:condition-non-isotropic} that we have .
Computing the summands of  can be done at the same
time as the sequence  is computed, adding the need for
one extra vector of .

Condition~\eqref{eq:lanczos:condition-non-isotropic} may fail to hold
without reaching  , however. This is because the positive characteristic
setting does not forbid -isotropic vectors: it may happen that
 without . In this case, the algorithm fails. In~\cite{EbKa97},
Eberly and Kaltofen show that
condition~\eqref{eq:lanczos:condition-non-isotropic} is equivalent to the
matrix  being of generic rank
profile (all leading principal minors are non-zero). They further show
how it is possible to control the failure probability with appropriate
randomization, under the assumption that the coefficient field is large
enough.

\section{The case of characteristic two}
\label{sec:blocklanczos:char2}
When , the standard Lanczos algorithm cannot work, as -isotropic
vectors are bound to occur.  This problem is an incurable failure
condition in the finite field case, but an analogous mishap can also be
encountered in the numerical case: if some  happens to be very
close to zero, then numerical instability occurs.

Techniques to address this issue have been proposed in the numerical
context quite early on, namely the look-ahead Lanczos
algorithm~\cite{PaTaLi85} which suggests to compute  from
several of the previous iterates.
In the context of integer factorization and
linear systems defined over , early techniques suggested, e.g.
in~\cite{C:LaMOdl90a}, to overcome the issue of -isotropic vectors
were quite inefficient, requiring for example to do all computations in
a field  for some .
Coppersmith~\cite{Coppersmith93a} and Montgomery~\cite{EC:Montgomery95},
in a somewhat simpler form, proposed to efficiently solve this problem by
taking inspiration from the look-ahead technique, and more importantly by
considering several vectors simultaneously.

The theoretical benefit is that if
we consider a block of  vectors  (represented by a
matrix of size ), the matrix  might
fail to be invertible, but its rank defect may be expected to be
reasonably small, thereby allowing the algorithm to proceed.

Considering blocks of vectors is also a great practical benefit when
dealing with sparse matrices defined over . Multiplying a sparse
matrix by a vector requires, for each matrix coefficient, to access a
single coefficient (hence a single bit) of the input vector. Despite the
fact that memory access probably reaches the nearby bits of the input
vector as well, these do not matter and one expects that their value is
most often discarded. When a block of  vectors is considered, and 
is equal to the machine word size (say, ), then there is a
natural alternative way to proceed. Storing blocks of vectors as
-element arrays of -bit machine words, it is possible to compute
simultaneously the product of a sparse matrix by a block of vectors in
essentially the same number of distinct memory accesses than required for
doing a single matrix-times-vector operation. The question is then
whether such an approach leads to a modification of the Lanczos algorithm
which requires fewer iterations.

\section{Orthogonalizing a sequence of subspaces}

The key to the block Lanczos algorithm is the idea of considering a
sequence of subspaces of dimension larger than 1. We use boldface letters
to denote blocks of  vectors, and the notation
 denotes the subspace of  spanned by
the  columns of . We extend this trivially to

As in the case of the standard Lanczos,
we define notions which are related to the inner product defined by the
matrix . We say that spaces  and
 are -orthogonal whenever . It is
clear that  is an  matrix with
coefficients in .

We first describe a naive extension of the Lanczos algorithm to the
setting of blocks of vectors, and explain why it does not work (at least
not if  may be small).
We need to define an analogue to the sequence of mutually orthogonal
vectors  considered in the standard Lanczos algorithm.
Let us fix an arbitrary vector block   as a starting point (to be
discussed in §\ref{sec:blocklanczos:termination}).
We may attempt to define a sequence of vector spaces with
 non-singular as follows.
\begin{itemize}
    \item Set .
    \item Define  as a maximal set of columns within
         so that  is
        invertible.
\end{itemize}
The key problem with the approach above is that  is
a block of possibly fewer vectors than : when
 above is not of full rank, some vectors are
discarded and not selected in . This implies that
after
some steps, the expected dimension of the block
 collapses to zero, with no further progress
possible. (A rule of thumb expecting a rank defect of 1 with
probability  predicts that no more than  steps can be done
before this collapse.)


To address this issue, Montgomery suggested an idea related to 
the look-ahead Lanczos~\cite{PaTaLi85} (but apparently discovered
independently): allow to build orthogonal subspaces
from a larger number of the previous iterates.
For notational
ease, we depart slightly here from the notations used
in~\cite{EC:Montgomery95}.
We define sequences , , and
, where ,
, and 
diagonal with entries in .
We require, for all :

The diagonal matrix
 essentially encodes the choice of a subset of
, which justifies the notation
 for the principal minor
attached to this set
(note that we have ).
It is clear that
condition~\eqref{eq:blocklanczos:condition-orth} also implies
 whenever .

In Montgomery's algorithm, the sequence of orthogonal subspaces is the
sequence , which are formed from as many columns from  as
possible (condition~\ref{eq:blocklanczos:condition-minor} imposes that
the inner product defined by  is non-degenerate on
). Vectors from  which are \emph{not}
selected in , instead of being dropped, are considered again
for selection in the next iterations.


As for the standard Lanczos algorithm, we explain how the conditions
above can be satisfied with an explicit inductive construction. The
starting point of each iteration is the vector block . The first
step is to compute   (and hence ) so as to satisfy
condition~\eqref{eq:blocklanczos:condition-minor}.
In a second step, we
compute  so as to satisfy
condition~\eqref{eq:blocklanczos:condition-orth}.

\section{Construction of the next iterate}
We first discuss how to compute  (and hence
) from . 
    We need the following lemma:
\begin{lemma}
    \label{lem:rank-sym}
    Let  be a symmetric matrix of rank
    , and  be indices of  independent
    columns of . Then the principal minor  is
    non-zero.
\end{lemma}
To see this, assume without loss of generality that .
Columns of indices  and above can be expressed as combinations of
the first  columns. 
We may write a matrix  so
that  has only its  first columns non zero.
The matrix  has its last  rows
and columns equal to zero,
so that only its
leading  submatrix is non-zero. Since
 has rank  and this submatrix coincides with the leading
 submatrix of , this is saying that
, as claimed.
\medskip

Lemma~\ref{lem:rank-sym} implies that computing  so as to
satisfy condition~\eqref{eq:blocklanczos:condition-minor}
only amounts to Gaussian elimination on the  matrix
.

An inverse of the submatrix whose row and column indices are encoded by
 can be computed from the same Gaussian elimination procedure.
Therefore, we assume that a by-product of the computation of
 is an  matrix  such that:

The former condition above expresses the fact that  is zero
outside the row and column indices encoded by , while the latter
expresses the fact that it is an inverse to the corresponding submatrix.
Note that this construction implies that  is symmetric.
\medskip


Assuming  and  have been derived from
, we now build  from  and . Given that
 has, by construction,  zero columns, we
complete it with the columns of  which were not selected in
. We then write 
as follows.

It is clear from the quantities computed so far that  is
-orthogonal to  for all , which is
condition~\eqref{eq:blocklanczos:condition-orth}.  We remark that we
could have used, as Montgomery does, the vector block
 instead of the value chosen above for
, and this would have led to the same value for
.


\section{Simplifying the recurrence equation}

The previous section defines a complete set of equations for determining
.  However the expression above for  is a very deep
recurrence, which would lead to poor time and space complexity. We
therefore need, as is done in the standard
Lanczos algorithm, to show that the recurrence equation can be
simplified.

We first restate the recurrence relations from the previous section, and
introduce some auxiliary notation .

Condition~\eqref{eq:blocklanczos:condition-orth} implies that the
second summand in the expression of  is zero for .
We now examine
the first summand for .
Consider equation~\eqref{eq:blocklanczos:recurrence} for index , and
multiply by . We obtain:

where the notation , for  a
subspace of , denotes any vector
block whose columns belong .

The equations above yield the following simpler form for
:

Better, for , we
consider
equation~\eqref{eq:blocklanczos:recurrence} for index  and
multiply it by . We obtain the following equation, from
which we rewrite  in an interesting way:

This implies, for , , and more generally for any 
(repeatedly using the last fact):


We remark that this expression for  is simpler than 
in~\cite{EC:Montgomery95}.

In the normal course of the computation, it is easy to ensure that
: this expresses the fact that column indices
which are not selected among the independent columns in 
used to define  have to be given priority when defining
 at the next step. As long as this can be achieved, we obtain
that whenever , we have .
In~\cite{EC:Montgomery95}, Montgomery does exactly like this, and computes
each iterate  with access to only the
three previous iterates , , and .

The particular form of equation~\eqref{eq:blocklanczos:ci-general},
however, allows to write a simpler recurrence, which has the advantage of
limiting the storage needs of the algorithm. Define

By equations~\eqref{eq:blocklanczos:recurrence}
and~\eqref{eq:blocklanczos:ci-general}, we see that the contribution of
all iterates before  in the expression of  can be
simplified as:





The computation of the sequence of vector blocks
 from a starting vector block  can now be
summarized. At the start, we have .
For all , we proceed through the following steps.
\begin{itemize}
    \item Compute  and
        .
        Deduce  (giving, or not, priority to indices not selected
        in  --- it makes no difference) and .
        If , terminate (see §\ref{sec:blocklanczos:termination}).
    \item Compute
        
    \item Memorize  and  for the next iteration.
\end{itemize}

\section{Termination}
\label{sec:blocklanczos:termination}

As the computation of the sequence of vector block proceeds, we clearly
have

Therefore, the number of iterations can be studied by first examining the
expected rank of . Montgomery writes
in~\cite{EC:Montgomery95} the generating
function for the rank defect of an arbitrary  symmetric matrix
over . For , the result obtained is that the expected rank
defect is . We thus have
, from which we expect
that at most an expected value of  iterations are
computed.


The actual termination condition which causes the iterative process to stop at index
 (more
exactly, become stationary, if we consider  to
be a legitimate value) is when we reach , which means that
. When the block dimension  is exceptionally small,
this might happen sooner than the expected value computed above, out of bad luck. We consider here that the block
dimension is large enough, so that this situation does not happen.

Heuristically, we expect a large intersection of
 with the null space of , which allows to find
close to  solutions to the homogeneous linear system .

We provide here some justification for this fact. Let  be a
vector block with , and let  be an arbitrary
vector block.  We consider the two sequences corresponding to 
and .  It is easy to see that both sequences
evolve synchronously, as the matrices  are equal for
both sequences at each step.  Let
.
We have

We claim that  is invertible. Indeed, we have:

The latter matrix is clearly of full rank. A consequence is that
, and that  is
expected to be close to . We thus have no reason to expect
that  is an abnormally poor supply of elements of the null space
of .


In the case which is relevant for integer factorization problems, we want
to solve the equation , and use the block Lanczos algorithm with
. In this case, we expect (as above, heuristically) that the intersection of
 with the (left) null space of  is large enough
to obtain close to   solutions to the linear system (provided the null
space itself is large enough).
\medskip

The block Lanczos algorithm can also be used to solve
inhomogeneous linear systems, to some extent.
In this case, we assume that the null space dimension is small compared
to the block dimension . We want to solve  for several vectors .
We set the starting vector block  with our vectors , and
complete with random vectors so as to form an -dimensional vector block. We compute

Note that  can be computed online at little extra cost, since for
 we have

with  as above.
Maintaining the evolution of  throughout the computation of the
sequence costs some extra memory.

By construction, we have .
In~\cite{EC:Montgomery95}, Montgomery argues that heuristically, we have
. Based on the
assumption that the null space of  is small enough, we hope to find
linear combinations of the columns of  and  which
provide some solutions to .


\section{Implementation in parallel}

Several implementations of the block Lanczos algorithm exist, and adapt
reasonably well to parallel computing environments. Different
processors (which can be different nodes communicating via message
passing, or simply processor threads) can collectively compute the
sequence .  It is useful to organize processors in a
two-dimensional (possibly toroidal) mesh, following the explanation in~\cite{Montgomery00}.
For simplicity, we assume the mesh has size .  Each processor
``owns'' part of the data: all vectors considered in the algorithm are
divided in  fragments, and the matrix  itself is also spread
across processes, in  fragments.
An example organization, assuming that  has dimension 
(both assumed to be divisible by ), distributes data as follows for
the processor on row  and column  (both indexed from ) within the mesh:
\begin{itemize}
    \item For vector blocks of size , row indices  with .
    \item For vector blocks of size , row indices  with .
    \item Sub-block of  at position  when split in blocks of
size .
\end{itemize}
In fact, load balancing has to be taken into account, so that the
distribution may actually be slightly different, or equivalently we may
need to permute rows and columns of  adequately.

In this setting, many operations on vectors can be performed locally.  The
only collective operation at each step is the multiplication by ,
decomposed into  first, then , where
 and  are vector blocks of size  and ,
respectively. Communication goes as follows. After ,
processors on the same mesh column need to share their results so as to
form  valid coefficients of the resulting vector . For the
operation , the processors in this same mesh column all
need these same  input coefficients. Therefore, the communication 
operation required after each of these two products is in fact a pretty
common pattern. In the Message Passing Interface, this operation is called
``All-reduce'', and is usually well optimized and tuned on most serious
MPI implementations.

The other operations within each iteration are either of moderate cost (dot
products, or multiplication of vectors by  matrices) or totally
negligible (arithmetic directly involving  matrices). It should
be noted however that the parallelization of the block Lanczos algorithm
can only go as far as the communication speed allows, since synchronization
has to occur after each multiplication by .

\section{Recent developments}
\label{sec:blocklanczos:recent}

The block Lanczos algorithm has been successful in factoring projects since
its inception, including record computations until 2005. Compared to the
block Wiedemann algorithm~\cite{Coppersmith94}, the block Lanczos algorithm
seems to need a smaller number of multiplications of matrices by blocks of vectors.
With blocking dimension , block Lanczos requires  products
in total (counting two for each iteration). The block Wiedemann algorithm
requires instead  products, depending on the two
blocking dimensions  and . When these are
chosen
straightforwardly as , the algorithm needs  products. This
comparison can shift towards
being in favour of the block Wiedemann algorithm in two ways. First,
if for example when  is a valid choice, only 
products are needed. Also, if large blocking dimensions can be considered
(say we use blocking dimensions  and  that are two appropriate multiples of ), then by
\cite[Theorem 7]{Kaltofen95}, only  products are needed,
which is better than block Lanczos.
However, the reason why the block Wiedemann algorithm has been
preferred in most factoring records since 2005 is simply because of the better
distribution opportunities it offers, a criterion which has been most
important given the composite nature of the hardware platforms used.

One may wonder whether the block Lanczos algorithm can be profitably used
in the context of the computation of discrete logarithms, in particular
with the number field sieve variants. An artifact of the number field sieve
for discrete logarithms, called Schirokauer maps, divides the presentation
of the linear algebra problem in two different settings. Given a sparse
matrix  defined over a large finite field, the Schirokauer maps form a
dense matrix block  (with very few columns, but with large coefficients)
such that the linear system to be solved can be written as
. It is not, however, the only way to proceed: any
vector  such that  is a satisfactory solution.
As it turns out, this approach is viable both in the block Lanczos
algorithm, as discussed in §\ref{sec:blocklanczos:termination}, as well as in the block
Wiedemann algorithm, as discussed in~\cite[§8]{Coppersmith94}. In both
cases, this is possible as long as the number of columns of the block
 is less than the block dimension .

%