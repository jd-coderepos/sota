\documentclass{bmvc2k}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wrapfig}



\title{Image Composition Assessment with Saliency-augmented Multi-pattern Pooling}

\addauthor{Bo Zhang}{bo-zhang@sjtu.edu.cn}{1}
\addauthor{Li Niu}{ustcnewly@sjtu.edu.cn}{1}
\addauthor{Liqing Zhang}{zhang-lq@cs.sjtu.edu.cn}{1}

\addinstitution{
 MoE Key Lab of Artificial Intelligence \\
 Shanghai Jiao Tong University \\
 Shanghai, China
}

\runninghead{Zhang, Niu, Zhang}{Image Composition Assessment with SAMP}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\begin{document}

\maketitle
\begin{abstract}
Image composition assessment is crucial in aesthetic assessment, which aims to assess the overall composition quality of a given image. However, to the best of our knowledge, there is neither dataset nor method specifically proposed for this task. In this paper, we contribute the first composition assessment dataset CADB with composition scores for each image provided by multiple professional raters. Besides, we propose a composition assessment network SAMP-Net with a novel Saliency-Augmented Multi-pattern Pooling (SAMP) module, which analyses visual layout from the perspectives of multiple composition patterns. We also leverage composition-relevant attributes to further boost the performance, and extend Earth Mover's Distance (EMD) loss to weighted EMD loss to eliminate the content bias. The experimental results show that our SAMP-Net can perform more favorably than previous aesthetic assessment approaches.
\end{abstract}


\section{Introduction}
\label{sec:intro}
Image aesthetic assessment aims to judge aesthetic quality automatically in a qualitative or quantitative way, which can be widely used in many down-stream applications such as assisted photo editing, intelligent photo album management, image cropping, and smartphone photography \cite{Bhattacharya2011AHA,Chen2017LearningTC,Datta2006StudyingAI,rawat2015context,fang2020perceptual,tu2020image,rawat2017spring,rawat2016clicksmart}. 
Among the factors related to image aesthetics, image composition, which mainly concerns the arrangement of the visual elements inside the frame \cite{Prakel2010TheFO}, is very critical in estimating image aesthetics \cite{savakis2000evaluation,obrador2010role,Liu2020CompositionAwareIA}, because composition directs the attention of viewer and has a significant impact on the aesthetic perception \cite{Prakel2010TheFO,Freeman2007ThePE,Martnez1988VisualFA}. 


Despite the importance of image composition, there is no dataset readily available for image composition assessment. Some existing aesthetic datasets contain annotations related to image composition \cite{Kong2016PhotoAR, Murray2012AVAAL, Jin2019AestheticAA, Chang2017AestheticCG}.
\textcolor[rgb]{0,0,0}{However, they only have composition-relevant attributes without overall composition score except for PCCD dataset \cite{Chang2017AestheticCG}, but PCCD dataset only presents one reviewer's composition rating for each image and this reviewer, an anonymous website visitor, may be unprofessional. So the ratings might be biased and inaccurate, which are far below the requirement for scientific evaluation.}
To this end, we contribute a new image Composition Assessment DataBase (CADB) on the basis of Aesthetics and Attributes DataBase (AADB) dataset \cite{Kong2016PhotoAR}. Our CADB dataset contains 9,497 images with each image rated by 5 individual raters who specialize in fine art for the overall composition quality. The details of our CADB dataset will be introduced in Section~\ref{sec:dataset}.



\begin{wrapfigure}{R}{0.5\linewidth}
  \vspace{-7mm}
  \begin{center}
    \includegraphics[width=1\linewidth]{figure/example.jpg}
  \end{center}
  \vspace{-6mm}
  \caption{Evaluating composition quality from the perspectives of different composition patterns. The first (\emph{resp.}, second) row shows a good example and a bad example considering symmetrical (\emph{resp.}, radial) balance.}
  \vspace{-2mm}
  \label{fig:pattern_example}
\end{wrapfigure}

To the best of our knowledge, there is no method specifically designed for image composition assessment. However, some previous aesthetic assessment methods also take composition into consideration. We divide the existing composition-relevant approaches into two groups. 1) The composition-preserving methods \cite{Mai2016CompositionPreservingDP, Chen2020AdaptiveFD} can maintain image composition during both training and testing. However, these approaches fail to extract composition-relevant feature for composition assessment task. 2) The composition-aware approaches \cite{Liu2020CompositionAwareIA, Ma2017ALampAL, Wang2019ModelingHP} extract composition-relevant feature by modeling the mutual dependencies between all pairs of objects or regions in the image. However, redundant and noisy information is likely to be introduced during this procedure, which may adversely affect the performance of composition assessment. Moreover, there are some previous methods \cite{wu2017high,Dhar2011HighLD,Bhattacharya2010AFF,wu2010good,Tang2013ContentBasedPQ,liu2010optimizing} designed to model the well-established photographic rules (\emph{e.g.}, rule of thirds and golden ratio \cite{joshi2011aesthetics}), which humans use in evaluating image composition quality. 
\textcolor[rgb]{0,0,0}{However, these rule-based methods have two major limitations: 1) The hand-crafted feature extraction is tedious and laborious compared with deep learning features \cite{li2020novel}. 2) Each rule is valid only for specific scenes and they did not consider which rules are applicable for a given scene \cite{su2021camera}. }

Interestingly, composition pattern, as an important aspect of composition assessment, is not explicitly considered by the above methods. As shown in Figure~\ref{fig:pattern_example}, each composition pattern divides the holistic image into multiple non-overlapping partitions, which can model human perception of composition quality. In particular, by analyzing the visual layout (\emph{e.g.}, positions and sizes of visual elements) according to composition pattern, \emph{i.e.}, comparing the visual elements in various partitions, we can quantify the aesthetics of visual layout in terms of visual balance (\emph{e.g.}, symmetrical balance and radial balance) \cite{jahanian2015learning, lok2004evaluation,Lee2017SemanticLD}, composition rules (\emph{e.g.}, rule of thirds, diagonals and triangles) \cite{thommes2018instagram,Lee2018PhotographicCC}, and so on. Different composition patterns offer different perspectives to evaluate composition quality. 
For example, the composition pattern in the top (\emph{resp.}, bottom) row in Figure~\ref{fig:pattern_example} can help judge the composition quality in terms of symmetrical (\emph{resp.}, radial) balance.


To dissect visual layout based on different composition patterns, we propose a novel multi-pattern pooling module at the end of backbone to integrate the information extracted from multiple patterns, in which each pattern provides a perspective to evaluate the composition quality. Considering that the sizes and locations of salient objects are representative of visual layout and fundamental to image composition \cite{lok2004evaluation}, we further integrate visual saliency \cite{Hou2007SaliencyDA} into our multi-pattern pooling module to encode the spatial and geometric information of salient objects, leading to our Saliency-Augmented Multi-pattern Pooling (SAMP) module. Additionally, since some composition patterns may play more important roles, we design weighted multi-pattern aggregation to fuse multi-pattern features, which can adaptively assign different weights to different patterns.  

Moreover, because our dataset is built upon AADB dataset \cite{Kong2016PhotoAR} with composition-relevant attributes, we further leverage composition-relevant attributes to boost the performance of composition assessment. Specifically, we propose an Attentional Attribute Feature Fusion (AAFF) module to fuse composition feature and attribute feature. Finally, after noticing the content bias existing in our dataset, that is, composition score distribution is severely influenced by object category, we extend Earth Mover's Distance (EMD) loss in \cite{Hou2016SquaredEM} to weighted EMD loss to eliminate the content bias. 

The main contributions of this paper can be summarized as follows: 1) We contribute the first image composition assessment dataset CADB, in which each image has the composition scores annotated by five professional raters. 2) We propose a novel composition assessment method with Saliency-Augmented Multi-pattern Pooling (SAMP) module. 3) We investigate the effectiveness of auxiliary attributes and weighted EMD loss for composition assessment. 4) Our model outperforms previous aesthetic assessment methods on our dataset. 
\vspace{-3mm}
\section{Related Work}

\subsection{Aesthetic Assessment Dataset}
Many large-scale aesthetic assessment datasets have been collected in recent years, like Aesthetic Visual Analysis database (AVA) \cite{Murray2012AVAAL}, AADB \cite{Kong2016PhotoAR}, Photo Critique Captioning Dataset (PCCD) \cite{Chang2017AestheticCG}, AVA-Comments \cite{Zhou2016JointIA}, AVA-Reviews \cite{Wang2019NeuralAI}, FLICKER-AES \cite{Ren2017PersonalizedIA}, and DPC-Captions \cite{Jin2019AestheticAA}.
However, these datasets only have composition-relevant attributes without overall composition score, or only have one inaccurate composition score for each image, which are far below the requirement for composition assessment research.
Unlike the existing aesthetic datasets, our CADB dataset contains composition ratings assigned to each image by multiple professional raters. Besides, we guarantee the reliability of our dataset based on sanity check and consistency analysis (see Section~\ref{sec:dataset}).

\begin{figure*}[tbp]
\begin{center}
   \includegraphics[width=1\linewidth]{figure/pipeline.jpg}
\end{center}
   \vspace{-5mm}
   \caption{The overall pipeline of our SAMP-Net for composition assessment. We use ResNet18 \cite{he2016deep} as backbone. The detailed structure of our Saliency-Augmented Multi-pattern Pooling (SAMP) module and Attentional Attribute Feature Fusion (AAFF) module are illustrated in Figure~\ref{fig:pattern_and_samp} and Figure~\ref{fig:aaff} respectively.}
  \vspace{-5mm}
\label{fig:pipeline}
\end{figure*}
\subsection{Composition-relevant Aesthetic Assessment}
 We can divide existing composition-relevant aesthetic assessment methods into traditional methods and deep learning methods. 
 As surveyed in \cite{Deng2017ImageAA, brachmann2017computational}, traditional methods \cite{liu2010optimizing,Tang2013ContentBasedPQ, Bhattacharya2010AFF, Zhang2014FusionOM, Murray2012AVAAL, Zhou2015ModelingPE,Su2011ScenicPQ, Li2010AestheticQA, wu2010good,Marchesotti2011AssessingTA,obrador2010role,savakis2000evaluation,rawat2015context} usually employed hand-crafted features or generic image features (\emph{e.g.}, bag-of-visual-words \cite{Su2011ScenicPQ} and Fisher vectors \cite{Perronnin2007FisherKO}) to learn image aesthetic evaluation, yet their generalization ability is limited by the complexity of image composition assessment task.
The deep learning based methods can be divided into two groups. The composition-preserving approaches \cite{Mai2016CompositionPreservingDP, Chen2020AdaptiveFD}, without explicitly learning composition representations, produce inferior results on composition evaluation task.
 The composition-aware approaches \cite{Liu2020CompositionAwareIA, Ma2017ALampAL, Wang2019ModelingHP} consider the relationship between all pairs of objects or regions in the image for modeling image composition, which is likely to introduce redundant and noisy information.
Moreover, the above methods did not explicitly consider composition patterns. In contrast, we design a novel Saliency-Augmented Multi-pattern Pooling (SAMP) module, which provides an insightful and effective perspective for evaluating composition quality.

\begin{figure}[tbp]
\begin{center}
  \includegraphics[width=0.9\linewidth]{figure/pattern_and_samp.jpg}
\end{center}
  \vspace{-6mm}
  \caption{Our designed eight composition patterns and Saliency-augmented Multi-pattern Pooling (SAMP) module.}
  \vspace{-4mm}
\label{fig:pattern_and_samp}
\end{figure}
\section{Composition Assessment DataBase (CADB)}
\label{sec:dataset}
To the best of our knowledge, there is no prior dataset specifically constructed for composition assessment. To support the research on this task, we build a dataset upon the existing AADB dataset \cite{Kong2016PhotoAR}, from which we collect a total of 9,958 real-world photos. We adopt a composition rating scale from 1 to 5, where a larger score indicates better composition.  We make annotation guidelines for composition quality rating and train five individual raters who specialize in fine art.  So for each image, we can obtain five composition scores ranging from 1 to 5.
Given the subjective nature of human aesthetic activity \cite{savakis2000evaluation,Prakel2010TheFO,Freeman2007ThePE}, we perform sanity check and consistency analysis. Similar to \cite{Yu2018LearningTD}, we use 240 additional ``sanity check'' images during annotating to roughly verify the validness of our annotations. We also examine the consistency of composition ratings provided by five individual raters  (see Supplementary).  Similar to \cite{Murray2012AVAAL, Kong2016PhotoAR}, we average the composition scores as the ground-truth composition mean score for each image, which is denoted as . More details about our CADB dataset will be elaborated in Supplementary. 

 Besides, we observe the content bias in our CADB dataset, that is, there are some biased categories whose score distributions are concentrated in a very narrow interval. After removing 461 biased images, we split the remaining images into 8,547 training images and 950 test images, in which the test set is made less biased for better evaluation (see Supplementary).
 












\section{Methodology}
\label{sec:methodology}
To accomplish the composition assessment task, we propose a novel network SAMP-Net, which is named after Saliency-Augmented Multi-pattern Pooling (SAMP) module. The overall pipeline of our method is illustrated in Figure~\ref{fig:pipeline}, where we first extract the global feature map from input image by backbone (\emph{e.g.}, ResNet18 \cite{he2016deep}) and then yield aggregated pattern feature through our SAMP module, which is followed by Attentional Attribute Feature Fusion (AAFF) module to fuse the composition feature and attribute feature. After that, we predict composition score distribution based on the fused feature and predict the attribute score based on the attribute feature, which are supervised by weighted EMD loss and Mean Squared Error (MSE) loss respectively.
\subsection{Saliency-augmented Multi-pattern Pooling}

\noindent\textbf{Multi-pattern Pooling:} As demonstrated in Figure~\ref{fig:pattern_and_samp}(a), we empirically design eight basic composition patterns inspired by classic composition guidelines. \textcolor[rgb]{0,0,0}{For instance, pattern 1,2,6,7 are inspired by symmetrical composition.
Pattern 3,4 are inspired by diagonal composition.
Pattern 5 is inspired by centre composition.
Pattern 8 is inspired by rule of thirds \cite{thommes2018instagram,Lee2018PhotographicCC}.
Although our pattern design is inspired by composition rules, there is no strict one-to-one correspondence between composition rules and patterns. Each pattern provides a perspective for evaluating composition quality, which may be beyond the scope of a single rule.  For example, Pattern 8 is related to rule of thirds, but not limited to rule of thirds. Based on Pattern 8, more useful information can be excavated by comparing the visual elements in nine partitions.}


Since humans typically employ multiple perspectives when analysing image composition, composition assessment should be accomplished based on all composition patterns in a comprehensive way.
Therefore, we propose multi-pattern pooling to achieve this goal, which is illustrated in Figure~\ref{fig:pattern_and_samp}(b). Given an  global feature map  with  channels, which is extracted from input image by backbone, we represent the pixel-wise feature at each location as , where , . For the -th pattern, we divide  into  non-overlapping partitions  and  is the total number of partitions in this pattern. Then, the feature of the -th partition can be obtained via average pooling: .



\noindent\textbf{Saliency-augmented Multi-pattern Pooling:} 
Considering the significance of salient objects for composition assessment, we further incorporate the saliency information (\emph{i.e.}, locations and scales of salient objects) into multi-pattern pooling. 
To achieve this goal, we utilize an unsupervised saliency detection method \cite{Hou2007SaliencyDA} to produce saliency maps for input images. We have also tried several supervised methods \cite{hou2017deeply,cornia2018predicting,Zhao_2019_CVPR}, which prove to be less effective. 
After obtaining the saliency map, we downsample it to  through max pooling. Recall that the size of global feature map is , we set  and  for retaining more details of salient objects. 

Different from  using average pooling,  we directly reshape each partition of saliency map into a vector, because the pooling operation will result in significant information loss.
Specifically, for the -th partition in the -th pattern, we reshape the saliency map in this partition into a saliency vector , in which  varies with partition and pattern. Then, we concatenate  and  to generate the partition feature . 

For the -th pattern, we concatenate the partition features of  partitions into a long vector , which is followed by a fc layer and  activation function to produce the pattern vector .  \textcolor[rgb]{0,0,0}{Intuitively,  extracts the visual information in each partition and   encodes the relationship among visual elements in different partitions.}

\noindent\textbf{Weighted Multi-pattern Aggregation:} 
Since some composition patterns may play more important roles when evaluating image composition, our model is trained to assign different weights for different patterns. Precisely, we apply global average pooling, a fc layer, and  normalization to the global feature map , producing the multi-pattern weight  for the -th pattern. Then, we have the aggregated pattern feature via weighted summation , in which  is the number of composition patterns (). Based on the learnt weights, we can know the dominant patterns in determining the overall composition quality and provide interpretable guidance for users (see Section~\ref{pattern_analysis}).

\noindent\textbf{Comparison with Spatial Pyramid Pooling}: Although the proposed SAMP and Spatial Pyramid Pooling (SPP) \cite{He2015SpatialPP} are similar in architecture, both of which pool features from multiple sets of partitions, SAMP is significantly different from SPP in three main aspects: 1) our pooling patterns are specifically designed and well-tailored for image composition evaluation, which can analyse the composition quality from the viewpoint of composition patterns; 2) we introduce visual saliency into multi-pattern pooling; 3) we learn pattern weights which provide interpretable guidance for improving composition quality.


\begin{wrapfigure}{R}{0.6\linewidth}
  \vspace{-8mm}
  \begin{center}
    \includegraphics[width=1\linewidth]{figure/aaff1.jpg}
  \end{center}
  \vspace{-5mm}
  \caption{Attentional Attribute Feature Fusion (AAFF) module. \emph{fc} means a fully-connected layer with  activation and  are attention coefficients.}
  \label{fig:aaff}
\end{wrapfigure}
\subsection{Attentional Attribute Feature Fusion}
\label{aaff}
Since our dataset is built upon AADB \cite{Kong2016PhotoAR}, which is associated with composition-relevant attributes, it is natural to consider using them to help composition assessment. We use five composition-relevant attribute annotations: \emph{rule of thirds}, \emph{balancing elements}, \emph{object emphasis}, \emph{symmetry}, and \emph{repetition}. 

Specifically, as illustrated in Figure~\ref{fig:pipeline}, we decompose the aggregated pattern feature  into composition feature  and attribute feature  by using two separate fc layers, the dimensions of which are both set to . We dynamically weigh the contributions of  and  for the composition assessment task, as illustrated in Figure~\ref{fig:aaff}. First, we apply a fc layer and  activation to the concatenation of   and , to learn the attention coefficients  for two types of features. Then, we concatenate the weighted composition feature and attribute feature, yielding the fused feature . During training, an additional layer is added to perform attribute prediction based on the attribute feature . We employ MSE loss denoted as  for attribute prediction.

As mentioned in Section~\ref{sec:dataset}, we observe the content bias in our dataset, in which case the network may find a shortcut to simply rate images based on their contents.
To mitigate the content bias in training set, we extend EMD loss to weighted EMD loss denoted as   (see Supplementary), which assigns smaller weights to biased samples when calculating EMD Loss.
Finally, our SAMP-Net can be trained in an end-to-end manner with attribute prediction loss  and weighted EMD loss :

where  is a trade-off parameter set as  via cross validation.





\vspace{-3mm}
\section{Experiments}

\begin{table*}
    \begin{center}
    \setlength{\tabcolsep}{2.5mm}{
        \begin{tabular}{|l c c c c c c|c|c|c|c|}
            \hline
            &  \emph{WE}&        \emph{MP}&   \emph{PW}&  \emph{SA}&  \emph{AF}& \emph{AA}&    MSE   &EMD    &SRCC     &LCC \\
            \hline\hline            
            1&      &               &           &           &           &           &           0.4534           & 0.1943            & 0.6025            & 0.6148       \\
            2&      \checkmark&     &           &           &           &           &           0.4373           & 0.1859            & 0.6105            & 0.6258       \\
            \hline
            3&      \checkmark& \checkmark&     &           &           &           &           0.4170           & 0.1847            & 0.6292            & 0.6435       \\
            4&      \checkmark& \checkmark& \checkmark&     &           &           &           0.4134           & 0.1829            & 0.6323            & 0.6483       \\
            5&      \checkmark& \checkmark& \checkmark& \checkmark&     &           &           0.4088           & 0.1820            & 0.6421            & 0.6544       \\
            6&      \checkmark& &  \checkmark& \checkmark&     &           &           0.4274           & 0.1854            & 0.6226            & 0.6293       \\
            7&      \checkmark& & \checkmark& \checkmark&     &           &           0.4205           & 0.1845            & 0.6319            & 0.6363       \\
            \hline
            8&      \checkmark&     &           &           &       \checkmark&     &           0.4320           & 0.1850            & 0.6200            & 0.6303       \\
            9&      \checkmark& \checkmark& \checkmark& \checkmark& \checkmark&     &           0.3979           & 0.1817            & 0.6439            & 0.6610       \\
            10&     \checkmark& \checkmark& \checkmark& \checkmark& \checkmark& \checkmark& \textbf{0.3867}    &\textbf{0.1798}    &\textbf{0.6564}    &\textbf{0.6709} \\
            \hline
        \end{tabular}}
    \end{center}
    \caption{Ablation studies of different components in our model.  means Spatial Pyramid Pooling (SPP) \cite{He2015SpatialPP}.  means Multi-scale Pyramid Pooling (MPP) \cite{yoo2015multi}. \emph{WE} means weighted EMD loss. \emph{MP} means multi-pattern pooling. \emph{PW} means pattern weights. \emph{SA} means saliency-augmented. \emph{AF} indicates attribute feature and \emph{AA} indicates attentional attribute feature fusion.}
    \label{table_ablation}
    \vspace{-5mm}
\end{table*}

\subsection{Implementation Details and Evaluation Metric}
\label{sec:implement_and_metric}
We use ResNet18 \cite{he2016deep} pretrained on ImageNet \cite{deng2009imagenet} as the backbone of our SAMP-Net. Unless otherwise specified, all input images are resized to  for both training and testing following \cite{li2020personality,schwarz2018will,ko2018pac}, leading to a global feature map of , and the saliency map is downsampled to  before passing to the SAMP. More details can be found in Supplementary. All experiments are conducted on our CADB dataset. 

To evaluate the composition score distribution and composition mean score predicted by different models, it is natural to adopt EMD and MSE as the evaluation metrics. EMD measures the closeness between the predicted and ground-truth composition score distributions as in \cite{Hou2016SquaredEM}. MSE is computed between the predicted and ground-truth  composition mean scores. Moreover, following existing aesthetic assessment approaches \cite{Kong2016PhotoAR, Talebi2018NIMANI, Chen2020AdaptiveFD}, we also report the ranking correlation measured by Spearman's Rank Correlation Coefficient (SRCC) and the linear association measured by Linear Correlation Coefficient (LCC) between the predicted and ground-truth composition mean scores.  
\vspace{-2mm}
\subsection{Ablation Study}
\label{sec:ablation}

To evaluate the effectiveness of each individual component in our SAMP-Net, we conduct a series of experiments and report all the evaluation metrics described in Section~\ref{sec:implement_and_metric}. In this section, we start from ResNet18 backbone and build up our holistic model step by step.

\noindent\textbf{Weighted EMD Loss:} We start from basic ResNet18 \cite{he2016deep}, and report the results using EMD loss and weighted EMD loss in Table~\ref{table_ablation}. The experimental results show that training with weighted EMD loss (row 2) performs better than standard EMD loss (row 1) with a clear gap of test EMD between these two models, which is attributed to the advantage of weighted EMD loss in eliminating content bias. 

\noindent\textbf{Saliency-Augmented Multi-pattern Pooling (SAMP):} Based on ResNet18 with weighted EMD loss (row 2), we add our SAMP module and also explore its ablated versions. 
We first investigate vanilla multi-pattern pooling without saliency or pattern weights (row 3), in which saliency vector is excluded from partition feature and the pattern features of multiple patterns are simply averaged. 
Then, we learn pattern weights to aggregate multiple pattern features (row 4). By comparing row 3 and row 4,  it is beneficial to adaptively assign different weights to different pattern features. 
We further incorporate saliency map into SAMP module (row 5). The comparison between row 4 and row 5 proves that is useful to emphasize the layout information of salient objects.
Considering the architecture similarity between Spatial Pyramid Pooling (SPP) \cite{He2015SpatialPP} and our multi-pattern pooling, we replace our multi-pattern pooling with SPP using scales  following \cite{Chen2020AdaptiveFD} (row 6).
\textcolor[rgb]{0,0,0}{In addition, we also show the results of using Multi-scale Pyramid Pooling (MPP) \cite{yoo2015multi} in row 7, in which we make an image pyramid containing three scaled images. The comparisons (row 5 \emph{v.s.} row 6, row 5 \emph{v.s.} row7) show that the model using multi-pattern pooling outperforms both SPP and MPP.}
The reason is that our proposed multi-pattern pooling is specifically designed and well-tailored for composition assessment task.


\noindent\textbf{Attentional Attribute Feature Fusion (AAFF):} Built on row 2 (\emph{resp.}, row 5) in Table~\ref{table_ablation}, we additionally learn attribute feature and directly concatenate it with composition feature, leading to  row 8 (\emph{resp.}, row 9). The experimental results demonstrate that composition-relevant attributes can help boost the performance of composition evaluation. This sheds light on that composition-relevant attribute prediction and composition evaluation are two related and reciprocal tasks. 
Finally, we complete our attentional attribute feature fusion module by learning weights for weighted concatenation (row 10). 
From row 9 and row 10, we can observe that the model using weighted concatenation is better than that using plain concatenation, which validates the superiority of attentional fusion mechanism.


\subsection{Comparison with Existing Methods}
\begin{table}
    \begin{center}
    \setlength{\tabcolsep}{5.5mm}{
        \begin{tabular}{|l|c|c|c|c|}
            \hline
            Method                                          & MSE   &EMD    &SRCC     &LCC \\
            \hline\hline            
            ResNet18                                        & 0.4534            & 0.1943            & 0.6025            & 0.6148 \\
            \hline
            AADB \cite{Kong2016PhotoAR}                     & 0.4234            & 0.1923            & 0.6236            & 0.6415 \\
            MNA-CNN \cite{Mai2016CompositionPreservingDP}   & 0.4260            & 0.1944            & 0.6108            & 0.6375 \\
            A-Lamp \cite{Ma2017ALampAL}                     & 0.4230            & 0.1898            & 0.6270            & 0.6456 \\
            VP-Net \cite{Wang2019ModelingHP}                & 0.4304            & 0.1948            & 0.6169            & 0.6285 \\
            RG-Net \cite{Liu2020CompositionAwareIA}         & 0.4398            & 0.1915            & 0.6026            & 0.6218 \\
            AFDC-Net \cite{Chen2020AdaptiveFD}              & 0.4245            & 0.1910            & 0.6154            & 0.6388 \\
            \hline
            SAMP-Net (Ours)                                 &\textbf{0.3867}   &\textbf{0.1798}    &\textbf{0.6564}    &\textbf{0.6709} \\
            \hline
        \end{tabular}}
    \end{center}
    \caption{Comparison of different methods on the composition assessment task. All models are trained and evaluated on the proposed CADB dataset.}
    \label{table_baseline}
    \vspace{-4mm}
\end{table}
To the best of our knowledge, there is no method specifically designed for image composition assessment. Nevertheless, some previous aesthetic assessment methods \cite{Kong2016PhotoAR,Mai2016CompositionPreservingDP,Ma2017ALampAL,Wang2019ModelingHP, Liu2020CompositionAwareIA, Chen2020AdaptiveFD} explicitly take composition into consideration. Since most of these methods do not yield score distribution, we make a slight modification on the prediction layer of these methods to be compatible with EMD loss \cite{Hou2016SquaredEM}. For fair comparison, all methods are trained and tested on our CADB dataset with ResNet18 pretrained on ImageNet \cite{deng2009imagenet} as backbone.

In Table~\ref{table_baseline}, we compare our method with different composition-relevant aesthetic assessment methods.
The baseline model (ResNet18) only consists of the pretrained ResNet18 and a prediction head, which is the same as row 1 in Table~\ref{table_ablation}.
Among these baselines, A-Lamp is the most competitive one, probably because A-Lamp introduces additional saliency information to learn the pairwise spatial relationship between objects. 
Our SAMP-Net clearly outperforms all the composition-relevant baselines, which demonstrates that our method is more adept at image composition assessment.  

\subsection{Analysis of Composition Pattern}
\label{pattern_analysis}
\begin{figure}[tbp]
\begin{center}
  \includegraphics[width=1\linewidth]{figure/interpretability.jpg}
\end{center}
  \vspace{-6mm}
   \caption{Analysis of the correlation between an image and its dominant pattern with the largest weight. We show the estimated pattern weights and the largest weight is colored green. We also show the ground-truth/predicted composition mean score in blue/red.}
  \vspace{-5mm}
\label{fig:model_interpretability}
\end{figure}



To take a close look at the learnt pattern weights which indicate the importance of different patterns on the overall composition quality, we show the input image, its saliency map, its ground-truth/predicted composition mean score, and its pattern weights in Figure~\ref{fig:model_interpretability}. 

For each image, the composition pattern with the largest weight is referred to as its dominant pattern. For each pattern, we show one example image with this pattern as dominant pattern and overlay this pattern on the image in Figure~\ref{fig:model_interpretability}, which reveals from which perspective the input image is given a high or low score.
For example, in the right figure of the last row, the surfer is placed at the intersection point between the gridlines of pattern 8, which implicates that the image conforms to the rule of thirds properly, yielding a relatively high score.
On the contrary, in the right figure of the first row, the arch slightly deviates from its symmetrical axis under pattern 2. So the low score implies that maintaining horizontal symmetry may enhance the composition quality.
In the left figure of the third row, per the low score under pattern 5, the dog is suggested to be moved to the center. 
In summary, our SAMP module can facilitate composition assessment by integrating the information from multiple patterns and provide constructive suggestions for improving the composition quality.
\vspace{-2mm}

\subsection{Additional Experiments in Supplementary}
Due to the space limitation, we present some experiments in Supplementary, including the results of using different training set sizes, backbones, and hyper-parameters  in  (\ref{total_loss}), weighted EMD loss analysis, the effectiveness of each pattern, the impact of using more composition patterns, comparison with the performance of human raters, more results on the CADB and PCCD \cite{Chang2017AestheticCG} datasets.
\vspace{-2mm}

\subsection{Limitations}
\label{sec:limitation}
\begin{figure}[tbp]
\begin{center}
  \includegraphics[width=1.0\linewidth]{figure/failure_cases_hor.jpg}
\end{center}
  \vspace{-6mm}
   \caption{We show some failure cases in the test set,  which have the highest absolute errors between the predicted composition mean scores (out of bracket) and the ground-truth composition mean scores (in bracket). }
  \vspace{-5mm}
\label{fig:failure_cases}
\end{figure}

\textcolor[rgb]{0,0,0}{While our method can generally achieve accurate and reliable composition assessment, it still has some failure cases. We show several failure cases in Figure \ref{fig:failure_cases}, which have the highest absolute errors between the predicted and ground-truth composition mean scores. We can observe that our model tends to predict relatively low scores for these images with high composition mean scores, which is probably due to the distracting backgrounds and complicated composition patterns. 
In addition, there is a clear gap between our method and human raters on ranking the composition quality of different images (see Supplementary), which needs to be enhanced in the  future work.
}
\vspace{-4mm}

\section{Conclusion}
In this paper, we have contributed the first composition assessment dataset CADB with five composition scores for each image. 
We have also proposed a novel method SAMP-Net with saliency-augmented multi-pattern pooling. Equipped with SAMP module, AAFF module, and weighted EMD loss, our method is capable of achieving the best performance for composition assessment.
\vspace{-4mm}
\section*{Acknowledgement}
This work is  sponsored by National Natural Science Foundation of China (Grant No. 61902247) and Shanghai Sailing Program (19YF1424400).

\bibliography{egbib}
\end{document}
