\documentclass[a4paper,11pt]{article}
\usepackage[american]{babel}
\usepackage[boxruled,vlined]{algorithm2e}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{pxfonts}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{sfmath}


\usepackage[T1]{fontenc}
\newcommand{\EDF}{\textrm{EDF}}
\newcommand{\RM}{\textrm{RM}}
\newcommand{\DM}{\textrm{DM}}
\newcommand{\pfair}{{\sf PFair}}
\newcommand{\drop}[1]{}
\newtheorem{Theorem}{Theorem}
\newtheorem{Definition}{Definition}
\newtheorem{Lemma}[Theorem]{Lemma}
\newtheorem{Corollary}[Theorem]{Corollary}
\newtheorem{Conjecture}[Theorem]{Conjecture}
\newtheorem{exa}[Theorem]{Example}
\newcommand{\equals}{\stackrel{\mathrm{def}}{=}}
\newcommand{\lcm}{\mathrm{lcm}}
\newcommand{\maxm}{\mathrm{max}}
\newcommand{\bigOh}{\mathcal{O}}
\parindent0em
\parskip1ex
\newtheorem{Example}{Example}
\newtheorem{Remark}{Remark}
\newtheorem{Algorithm}{Algorithm}
\newtheorem{Heuristic}{Heuristic}

\begin{document}

\title{Exact Feasibility Tests for Real-Time Scheduling of
Periodic Tasks upon Multiprocessor Platforms\footnote{This paper is
an extended version of ``Feasibility Intervals for Fixed-Priority Real-Time
Scheduling on Uniform Multiprocessors'', Proceedings of 11th IEEE
International Conference on Emerging Technologies and Factory
Automation (ETFA06) and of ``Feasibility Intervals for
Multiprocessor Fixed-Priority Scheduling of Arbitrary Deadline Periodic
Systems'', Proceedings of 10th Design, Automation and Test in Europe
(DATE07).}}

\date{}

\author{
\begin{tabular}[t]{c@{\extracolsep{7em}}c}Liliana Cucu\thanks{Supported in part by FNRS Grant.}  & Jo\a"el Goossens \\
  LORIA-INPL & 
Universit\'e Libre de Bruxelles (\textsc{u.l.b.})\\
615 rue du Jardin Botanique &50 Avenue Franklin D. Roosevelt\\
54600 Villers-les-Nancy, France &1050 Brussels, Belgium  \\
{\em liliana.cucu@loria.fr} & {\em joel.goossens@ulb.ac.be}
\end{tabular}
}

\maketitle



\begin{abstract}
In this paper we study the global scheduling of periodic task
systems upon multiprocessor platforms. We first show two very
general properties which are well-known for uniprocessor platforms
and which remain for multiprocessor platforms: \textit{(i)} under
few and not so restrictive assumptions, we show that feasible
schedules of periodic task systems are periodic from some point with
a period equal to the least common multiple of task periods and
\textit{(ii)} for the specific case of synchronous periodic task
systems, we show that feasible schedules repeat from the origin. We
then present our main result: we characterize, for task-level fixed-priority schedulers and for asynchronous constrained or arbitrary deadline periodic task models, \emph{upper bounds} of the first time instant where the schedule repeats. We show that job-level fixed-priority schedulers are predictable upon unrelated multiprocessor platforms. For task-level fixed-priority schedulers, based on the upper bounds and the predictability property, we provide for asynchronous constrained or arbitrary deadline periodic task sets, \emph{exact} feasibility tests. Finally, for the \emph{job}-level fixed-priority \EDF{} scheduler, for which such an upper bound remains unknown, we provide an \emph{exact} feasibility test as well.
\end{abstract}

\section{Introduction} \label{intro} The use of computers to control
safety-critical real-time functions has increased rapidly over the
past few years. As a consequence, real-time systems --- computer
systems where the correctness of each computation depends on both the
logical results of the computation and the time at which these results
are produced --- have become the focus of much study. Since the
concept of ``time'' is of such importance in real-time application
systems, and since these systems typically involve the sharing of one
or more resources among various contending processes, the concept of
scheduling is integral to real-time system design and
analysis. Scheduling theory as it pertains to a finite set of requests
for resources is a well-researched topic. However, requests in
real-time environment are often of a recurring nature. Such systems
are typically modelled as finite collections of simple, highly
repetitive tasks, each of which generates jobs in a very predictable
manner. In this work, we consider \emph{periodic task systems}, each periodic 
task  generates jobs at each integer multiple of its period  with the restriction that the first job is released at time  (the task
offset).



The \emph{scheduling algorithm} determines which job[s] should be
executed at each time instant. When there is at least one schedule
satisfying all constraints of the system, the system is said to be
\emph{feasible}.

\emph{Uniprocessor} real-time systems are well studied since the
seminal paper of Liu and Layland~\cite{Liu} which introduces a model
of periodic systems. The literature considering scheduling algorithms
and feasibility tests for uniprocessor scheduling is tremendous. In
contrast for \emph{multiprocessor} parallel machines the problem of
meeting timing constraints is a relatively new research area.



In the design of scheduling algorithms for multiprocessor environments,
one can distinguish between at least two distinct approaches. In
\emph{partitioned scheduling}, all jobs generated by a task are
required to execute on the \emph{same} processor. \emph{Global scheduling},
by contrast, permits \emph{task migration} (i.e., different jobs of an
individual task may execute upon different processors) as well as \emph{job
migration} (an individual job that is preempted may resume execution
upon a processor different from the one upon which it had been
executing prior to preemption).

From theoretical and practical point of view we can distinguish between at
least three kinds of multiprocessor machines (from less
general to more general):

\begin{description}
\item[Identical parallel machines] Platforms upon which
all the processors are identical, in the sense that they have the same
computing power.

\item[Uniform parallel machines] By contrast, each processor in a
uniform parallel machine is characterized by its own computing
capacity, a job that executes on processor  of computing capacity
 for  time units completes  units of execution.

\item[Unrelated parallel machines] In unrelated parallel
machines, there is an execution rate  associated with each
job-processor pair, a job  that executes on processor
 for  time units completes  units of execution. This kind of heterogeneous architectures models dedicated processors (e.g., if  means that  cannot serve job ).
\end{description}

\paragraph{Related research.} The problem of scheduling periodic task
systems on multiprocessors was originally studied
in~\cite{liu2}. Recent studies provide a better understanding of that
scheduling problem and provide first solutions. E.g., \cite{carpenter}
presents a categorization of real-time multiprocessor scheduling
problems. It is important to notice that, to the best of our
knowledge, the literature does not provide \emph{exact} feasibility
tests for global scheduling of periodic systems upon
multiprocessors. Moreover, we know that uniprocessor feasibility
results do not remain for multiprocessor scheduling. For instance the
synchronous case (i.e., considering that all tasks start their
execution synchronously) is not the worst case anymore upon
multiprocessors. Another example is the fact that the first busy
period (see~\cite{Lehoczky90} for details) does not provide a
feasibility interval upon multiprocessors (see~\cite{goossens4} for
such counter-examples). Initial results indicate that real-time
multiprocessor scheduling problems are typically not solved by
applying straightforward extensions of techniques used for solving
similar uniprocessor problems. Unfortunately, too often, researchers
use uniprocessor arguments to study multiprocessor scheduling problems
which leads to incorrect properties. This fact motivated our rigorous
and formal approach; we will present and prove correct, rigorously,
in this paper, our exact feasibility tests (and related properties).

\paragraph{This research.} In this paper we consider
preemptive global scheduling and we present exact
feasibility tests upon multiprocessors for various scheduling
policies and various periodic task models.

Our feasibility tests are based on \emph{periodicity}
properties of the schedules and on \emph{predictability}
properties of the considered schedulers. The latter
properties are not obvious because of multiprocessor
scheduling anomalies (see~\cite{Ha} for details).

More precisely, in the first part of this paper we
prove that, under few and no so restrictive assumptions,
\emph{any} feasible schedule of periodic tasks repeat from some point
in time. Then we prove that job-level fixed-priority
schedulers (e.g., \EDF{}  and \RM) are predictable upon unrelated multiprocessor platforms.

We also characterize for task-level fixed-priority schedulers and for the various periodic task models an upper bound of the first time instant where the schedule
repeats (and its period).

Lastly, we combine the periodicity and predictability
properties to provide for these various kind of periodic
task sets and various schedulers \emph{exact} feasibility tests.

\paragraph{Organization.} This paper is organized as follows. Section~\ref{model} introduces the definitions, the model of computation and our assumptions. We prove the periodicity of feasible schedules of periodic systems in Section~\ref{sectionMainPer}. In Section~\ref{sectionExactFebTest} we prove that job-level fixed-priority schedulers (e.g., \EDF{}  and \RM) are predictable upon unrelated multiprocessor platforms and we combine the periodicity and predictability
properties to provide for these various kind of periodic  task sets and various schedulers \emph{exact} feasibility tests. Lastly, we conclude in Section~\ref{conclusion}.

\section{Definitions and assumptions}\label{model}

We consider the scheduling of periodic task systems. A
system  is composed by  periodic tasks , each task is characterized by a
period , a relative deadline , an execution requirement 
 and an offset . Such a periodic task generates an
infinite sequence of jobs, with the  job
arriving at time-instant  (), having an execution requirement of  units,
and a deadline at time-instant  It is important to notice that we assume in the
first part of this manuscript that each task instance of
the same task (say ) has the very same execution
requirement (); we will relax this assumption in
the second part of this manuscript by showing that our
analysis is \emph{predictable}.

We will distinguish between {\it implicit deadline} systems where
; {\it constrained deadline} systems where  and {\it arbitrary deadline} systems where there
is no relation between the deadlines and the periods.
Notice that arbitrary deadline systems includes
constrained deadline ones which includes the implicit deadline ones.

In some cases, we will consider the more general problem of
scheduling set of jobs, each job  is characterized
by a release time , an execution requirement  and an absolute
 deadline . The job  must execute for  time units over
the interval . A job becomes {\em active} from its release
time to its completion.

A periodic system is said to be {\it synchronous} if there is an instant where
all tasks make a new request simultaneously, i.e.,  such that  (see
\cite{goossens5} for details). Without loss of generality, we consider
 for synchronous systems. Otherwise
the system is said to be {\it asynchronous}.

We denote by ,
by , by  and .

We consider in this paper multiprocessor platforms 
composed of  unrelated processors (or one of its
particular cases: uniform and identical platforms):
. Execution rates  are
associated to each task-processor pair, a task  that
executes on processor  for  time units completes
 units of execution. For each task
 we assume the associated set of processors

ordered in the decreasing order of the execution rates
relatively to the task: . For identical execution
rates, the ties are broken arbitrarily, but consistently,
such that the set of processors associated to each task is
\emph{total} ordered. Consequently, the \emph{fastest} processor
relatively to task  is , i.e., the
first processor of the ordered set associated to the task.
Moreover, for a task  in the following we consider
that a processor  is {\em faster} than  (relatively to its associated set of processors) if  even if we have . For the processor-task pair  if
 then  is said to be an \emph{eligible} processor
for . Notice that these concepts and definitions
can be trivially adapted to the scheduling of jobs upon
unrelated platforms. 

We consider in this paper a discrete model, i.e., the characteristics
of the tasks and the time are integers. 



We define now the notions of the state of the system and the schedule.

\begin{Definition} [State of the system ] \label{defState}
  For any arbitrary deadline system  we define the {\em state}  of the system  at instant  as  with  where
  

\end{Definition}
Notice that at any instant  several jobs of the same task might be
active and we consider that the oldest job is scheduled first,
i.e., the FIFO rule is used to serve the various jobs of
given task. 

\drop{
If we consider the case of constrained deadline task systems, then Definition~ref{defState} is modified as follows:
\begin{Definition} [State of the system ] \label{defStatebis}
  For any constrained deadline system  we define the {\em state}  of the system  at instant  as  with  where
  

\end{Definition}
}

\begin{Definition} [Schedule ] \label{defSched}
  For any task system  and any
   set of  processors  we define the
  {\em schedule}  of system  at instant  as
    where  with \\
  
\end{Definition}

Notice that Definition~\ref{defSched} can be extended trivially to the
scheduling of jobs.

A system  is said to be {\it feasible} upon a multiprocessor
platform if there exists at least one schedule in which all tasks meet
their deadlines. If  is an algorithm which schedules  upon a
multiprocessor platform to meet its deadlines, then the system 
is said to be -feasible.

In this work, we consider that \emph{task parallelism is forbidden}: a task
cannot be scheduled at the same instant on different processors,
i.e.  and  such that .

The scheduling algorithms considered in this paper are {\em
  deterministic} and work-conserving with the following definitions

\begin{Definition}[Deterministic algorithm]\label{detAlg} 
A scheduling algorithm is said to
be \emph{deterministic} if it generates a unique schedule for any
given sets of jobs .
\end{Definition}

In uniprocessor (or identical multiprocessor) scheduling, a {\em
work-conserving} algorithm is defined to be the one that never idles a
processor while there is at least one active task. For unrelated multiprocessors
we adopt the following definition:

\begin{Definition} [Work-conserving algorithm] \label{defWorkC} 
  An unrelated multiprocessor scheduling algorithm is said
\emph{work-conserving} if at each instant, the algorithm schedules jobs
  to processors as follows: the highest priority (active) job  is
  scheduled on its fastest (and eligible) processor . The very
  same rule is then applied to the remaining active jobs on the
  remaining available processors.
\end{Definition}

Moreover, we will assume that the decision of the scheduling
algorithm at time  is not based on the past, nor on the
actual time  but only on the characteristics of active tasks and on
the state of the system at time . More formally, we consider
\emph{memoryless} schedulers.

\begin{Definition}[Memoryless algorithm]\label{def:memoryless} 
  A scheduling algorithm is said to be \emph{memoryless} if the
  scheduling decision made by it at time  depends only on the
  characteristics of active tasks and on the current state of the
  system, i.e., on .
\end{Definition}

Consequently, for memoryless and deterministic schedulers we have the
following property:




It follows by Definition~\ref{defWorkC} that a processor  can
be idled and a job  can be active at the same time if and
only if .

In the following, we will distinguish between two kinds of scheduler:

\begin{Definition}[Task-level fixed-priority]\label{task-level}
The priorities are assigned to the tasks
beforehand, at run-time each job \emph{inherits} of its task
priority and remains constant.
\end{Definition} 

\begin{Definition}[Job-level fixed-priority]\label{prioDr}
  A scheduling algorithm is a {\em job-level fixed-priority}
  algorithm if and only if it satisfies the condition that for every
  pair of jobs  and , if  has higher priority than
   at some time instant, then  always has higher priority
  than .
\end{Definition} 

Popular task-level fixed-priority schedulers include the Rate Monotonic (\RM) or the Deadline Monotonic (\DM); popular job-level fixed-priority schedulers include the Earliest Deadline First (\EDF), see~\cite{Liu} for details.

We denote by  the   job of task  which
becomes active at time instant .

\begin{Definition}[]
  For any task , we define  to be the amount of
  time already executed for  in the interval .
\end{Definition}

We introduce now the availability of the processors for any schedule
.

\begin{Definition} [Availability of the processors , task scheduling]
\label{defAvai} For any task system  and any set of  processors
   we define the {\em availability of the processors}
   of system  at instant  as the set of available processors
   .
\end{Definition}

\section{Periodicity of feasible schedules}\label{sectionMainPer}

It is important to remind that we assume in this section that all task execution requirements are \emph{constant}, we will relax this assumption in Section~\ref{sectionExactFebTest}. 

This section contains four parts,  we give in each part of this section results concerning the periodicity of feasible schedules. By periodicity (assuming that the period is ) of a schedule , we understand there is a time instant  such that .

The first part of this section provides periodicity results for a
(very) general scheduling algorithm class: deterministic,
memoryless and work-conserving schedulers. 

The second part of this section provides periodicity results for
synchronous periodic task systems.

The third and the fourth part of this section present periodicity
results for task-level fixed-priority scheduling algorithms for constrained and arbitrary deadline systems, respectively. 

\subsection{Periodicity of deterministic, memoryless and
  work-conserving scheduling algorithms} \label{genGenSyst}

We show that feasible schedules of periodic task systems obtained
using deterministic, memoryless and work-conserving algorithms are
periodic from some point. Moreover we prove that the schedule repeats
with a period equal to  for a sub-class of such schedulers. Based on that property, we provide two
interesting corollaries for preemptive task-level fixed-priority algorithms (Corollary~\ref{task-levelperiod}) and for preemptive deterministic \footnote{by deterministic {} we mean that ambiguous situations are solved deterministically.} (Corollary~\ref{edfAll1}).

We present first two preliminary results in order to prove Theorem~\ref{perGen}.
 
\begin{Lemma}\label{thZero} For any deterministic and memoryless
  algorithm , if an asynchronous arbitrary deadline system 
  is -feasible, then the -feasible schedule of  on 
  unrelated processors is periodic with a period \emph{divisible} by
  . 
\end{Lemma}

\begin{proof}
  First notice that from  all tasks are released,
  and the configuration  of each task is a triple of
  finite integers  with ,  and . Therefore there is a finite number of different system
  states, hence we can find two distinct instants  and 
  ( with the same state of the system
  (). The schedule repeats from that instant
  with a period dividing , since the scheduler is
  deterministic and memoryless.
  
  Notice that since the tasks are periodic, the arrival pattern of jobs repeats with a period equal to  from .

  We prove now by contradiction that  is necessarily a multiple
  of . We suppose that  such
  that  with
  ,  and . This implies that there are tasks for which the time elapsed since the last activation at  and the time elapsed since the last
  activation at  are not equal. But this is contradiction with the fact that . Consequently  must be equal to
   and, thus, we have . 
\end{proof}

For a sub-class of schedulers, we will show that the period of the schedule is , but first a definition (inspired from~\cite{GD99b}):

\begin{Definition}[Request-dependent scheduler]
A scheduler is said to be \emph{request-dependent} if  if and  only if , where  means that the request  has a higher priority than the request .
\end{Definition}

The next lemma extend results given for arbitrary
deadline task systems in the \emph{uniprocessor} case
(see~\cite{thesisJG}, p.~ for details).

\begin{Lemma} \label{prepTh} For any preemptive, job-level fixed-priority and request-dependent algorithm  and any asynchronous arbitrary deadline system  on  unrelated processors, we have that:
  for each task , for any time instant  and 
  such that , if there is no deadline
  missed up to time , then  with .
\end{Lemma}

\begin{proof}
  The proof is made by contradiction. Notice first that the function
   is a non-decreasing discrete step function with  and .

  We assume that a \emph{first} time instant  exists such that there are
   and  with  and
  . This assumption
  implies that there is a time instant  with 
  such that  is scheduled at  while
   is not scheduled at . We obtain that  higher priority jobs are scheduled at  and among these jobs there is, at least, one
  job  of a task  with
   that is not
  scheduled at , while 
  is scheduled at  (). This implies
  that 
  but this is a contradiction with the fact that  is the first such
  time instant.
\end{proof}

\begin{Theorem}\label{perGen} 
For any preemptive job-level fixed-priority and request-dependent algorithm  and any -feasible asynchronous arbitrary deadline system  
upon  unrelated processors the schedule is periodic with a period equal to . 
\end{Theorem}

\begin{proof}
  By Lemma~\ref{thZero} we have that  with  such that
  . We know also that the arrivals of jobs of
  tasks repeat with a period equal to  from . Therefore
  for all time instants ,  (i.e. ), we have that the time elapsed since the last activation at
   is the same for all tasks. Moreover
  since  we have that
  
  with ,. But by Lemma~\ref{prepTh} we also
  have that . Consequently we obtain that  and  which implies
  that . 
\end{proof}

\begin{Corollary}\label{task-levelperiod}
For any preemptive task-level fixed-priority algorithm , if an asynchronous arbitrary deadline system  is -feasible upon  unrelated processors is periodic with a period equal to .
\end{Corollary}

\begin{proof}
The result is a direct consequence of Theorem~\ref{perGen}, since task-level fixed-priority algorithms are job-level fixed-priority and request-dependent schedulers.
\end{proof}

\begin{Corollary}\label{edfAll1} A feasible schedule obtained using
  deterministic request-dependent global {} on  unrelated processors of an
  asynchronous arbitrary deadline system  is periodic with a
  period equal to . 
\end{Corollary}

\begin{proof}
  The result is a direct consequence of Theorem~\ref{perGen}, since
  {} is a job-level fixed-priority scheduler.
\end{proof}

\subsection{The particular case of synchronous periodic systems}
\label{SectSynchEns}

In this section we deal with the periodicity of feasible schedules of
\emph{synchronous} periodic systems. Using the results obtained for
deterministic, memoryless and work-conserving algorithms we prove in
Section \ref{synSect} that the feasible schedules of synchronous
constrained deadline periodic systems are periodic from time instant
equal to . In Section \ref{synSectarb} we study arbitrary deadline
periodic systems and the periodicity of feasible schedules of these
systems using preemptive task-level fixed-priority scheduling
algorithms.

\subsubsection{Synchronous constrained deadline periodic
  systems} \label{synSect} 

In this section we deal with the particular case of synchronous
periodic task systems and we show the periodicity of feasible
schedules.

\begin{Theorem}\label{synPer}
  For any deterministic, memoryless and work-conserving  algorithm ,
  if a synchronous constrained deadline system  is -feasible,
  then the -feasible schedule of  on  unrelated processors
  is periodic with a period  that begins at instant .
\end{Theorem}

\begin{proof} 
  Since  is a synchronous periodic system, all tasks become
  active at instants  and . Moreover, since  is a
  -feasible constrained deadline system, all jobs occurred strictly
  before instant  have finished their execution before or at
  instant . Consequently, at instants  and  the system is in
  the same state, i.e. , and a deterministic and
  memoryless scheduling algorithm will make the same scheduling
  decision. The schedule repeats with a period equal to .
\end{proof}

An interesting particular case of Theorem~\ref{synPer} is the following:

\begin{Corollary}\label{edfAll2} 
  A feasible schedule obtained using deterministic global {} of a
  synchronous constrained deadline system  on  identical or
  unrelated processors is periodic with a period  that begins at
  instant . 
\end{Corollary}


\subsubsection{Synchronous arbitrary deadline periodic
  systems} \label{synSectarb}

In this section we deal with the particular case of synchronous
arbitrary deadlines task systems and we show the periodicity of
feasible schedules obtained using preemptive task-level fixed-priority
scheduling algorithms.

In the following, and without loss of generality, we consider the tasks
ordered in decreasing order of their priorities .

\begin{Lemma}\label{synPerLem} For any preemptive task-level fixed-priority
  algorithm  and for any synchronous arbitrary deadline system 
  on  unrelated processors, if no deadline is missed in the time interval
   and if , then the schedule of  is periodic with a period  that begins at instant .
\end{Lemma}

\begin{proof}
  Since at time instants  and  the system is in the same state,
  i.e. , then at time instants  and  a
  preemptive task-level fixed-priority algorithm will make the same scheduling
  decision and the scheduled repeats from  with a period equal to
  .
\end{proof}

\begin{Theorem} \label{synPerNotFeb}
For any preemptive task-level fixed-priority algorithm  and any synchronous
arbitrary deadline system  on  unrelated processors, if all deadlines are met in  and , then  is not -feasible.
\end{Theorem}

\begin{proof} 
In the following, we denote by  the schedule of the task subset . Since , there is more than one active job of the same task at . We define  to be the smallest task index such that  has at least two active jobs at . In order to prove the property we will prove that  will miss a deadline.

By definition of  we have that  (at least for the schedule ) and
by Lemma~\ref{synPerLem} we have that the time instants, such that at
least one processor is available, are periodic with a period
, i.e., the schedule  obtained by
considering only the task subset  is periodic with
a period . Moreover, since  is a multiple of
, we know that the schedule  is periodic with
a period . Therefore in each time interval  with  after scheduling  there is the same number  of
time instants such that at least one processor is available and where
 is scheduled. At time instant , since the task
parallelism is forbidden, there are 
remaining units for execution of  and, consequently,
at each time instant  there will be  remaining units for
execution of . Consequently we can find  such that the job actived at  will miss its deadline
since it cannot be scheduled before older jobs of  and
there are  remaining units for execution of  at
.

Since we consider task-level fixed-priority scheduling, then the
tasks  with  will not interfere with the higher
priority tasks already scheduled, particularly with 
that misses its deadline, and consequently the system is not
-feasible.
\end{proof}

\begin{Corollary}\label{synPerArbi}
For any preemptive task-level fixed-priority algorithm  and any synchronous
arbitrary deadline system  on  unrelated processors, if  is -feasible, then the schedule of  is periodic with a period  that begins at instant .
\end{Corollary}

\begin{proof}
Since  is -feasible, we know by Theorem~\ref{synPerNotFeb} that . Moreover, a deterministic and memoryless scheduling algorithm will make the same scheduling decision at those instants. Consequently, the schedule repeats from the origin with a period of .
\end{proof}

\subsection{Task-level fixed-priority scheduling of asynchronous constrained
  deadline systems} \label{asynSect}

In this section we give another important result: any feasible schedules on  unrelated processors of asynchronous constrained deadline systems,
obtained using preemptive task-level fixed-priority algorithms, are periodic from some point (Theorem~\ref{asynPer}) and we characterize that point.

Without loss of generality we consider the tasks ordered in decreasing
order of their priorities .


\begin{Theorem} \label{asynPer} For any preemptive task-level fixed-priority
algorithm  and any -feasible asynchronous constrained deadline system  upon  unrelated processors is periodic with a period  from instant  where  is defined inductively as follows:

  \begin{itemize}
  \item ; 
  \item .
  \end{itemize}
\end{Theorem}

\begin{proof}
  The proof is made by induction by  (the number of tasks). We
  denote by  the schedule obtained by considering only
  the task subset , the first higher priority  tasks
  , and by  the corresponding
  availability of the processors. Our inductive hypothesis is the
  following: the schedule  is periodic from  with a
  period  for all .

  The property is true in the base case:  is periodic
  from  with period , for :
  since we consider constrained deadline systems, at instant  the
  previous request of  has finished its execution and the
  schedule repeats.

  We will now show that any -feasible schedules of  are
  periodic with period  from .

  Since  is periodic with a period  from  the
  following equation is verified:



We denote by  the first request of
 not before .

Since the tasks in  have higher priority than
, then the scheduling of  will not interfere with
higher priority tasks which are already scheduled. Therefore, we may
build  from  such that the tasks
 are scheduled at the very same
instants and on the very same processors as they were in
. We apply now the induction step: for all  in  we have  the
availability of the processors repeats. Notice that at those instants 
and  the available processors (if any) are the same. Consequently, at
only these instants task  {\em may} be executed.

The instants  with , where
 may be executed in , are periodic with
period . Moreover, since the system is feasible and we consider constrained deadlines, the only active request of  at  (respectively at ) is the one activated at  (respectively at ). Consequently, the instants at which the task-level fixed-priority algorithm  schedules  are periodic with period . Therefore the schedule  repeats from  with period equal to  and the property is true for all , in particular for   is periodic with period equal to  from  and the property follows.
\end{proof}


\subsection{Task-level fixed-priority scheduling of asynchronous arbitrary 
  deadline systems} \label{asynSectArb}

In this section we present another important result: any feasible schedule on
 unrelated processors of asynchronous arbitrary deadline systems,
obtained using preemptive task-level fixed-priority algorithms, is periodic from
some point (Theorem~\ref{asynPerbis}).

\begin{Corollary}\label{prepThbis} 
  For any preemptive task-level fixed-priority algorithm  and any asynchronous
  arbitrary deadline system  on  unrelated processors, we
  have that: for each task , for any time instant 
  and  such that , if there is no
  deadline missed up to time , then  with .
\end{Corollary}

\begin{proof}
  This result is direct consequence of Lemma~\ref{prepTh} since preemptive
  task-level fixed-priority algorithms are job-level fixed-priority and request-dependent schedulers.
\end{proof}





\begin{Corollary} \label{Coreither}
  For any preemptive task-level fixed-priority algorithm  and any asynchronous
  arbitrary deadline system  on  unrelated processors, we
  have that: for each task , for any time instant , if there is no deadline missed up to time , then either
   or  and
  , where by the triple 
   we denoted .
\end{Corollary}

\begin{proof} If , then either  or
  . Otherwise,
   where  is the number of jobs
  actived before or at , and  is the number of jobs that
  have completed their execution before or at . We have
   and by Corollary~\ref{prepThbis} we
  obtain that . Consequently
  , and if 
  then , and .
\end{proof}



\begin{Theorem} \label{asynPerbis} For any preemptive task-level fixed-priority
  algorithm  and any -feasible asynchronous arbitrary deadline system 
  upon  unrelated processors is periodic with a period  from instant  where 
  are defined inductively as follows: 

  \begin{itemize}
  \item 
  \item )
   \end{itemize}
\end{Theorem}

\begin{proof}
  The proof is made by induction by  (the number of tasks). We
  denote by  the schedule obtained by considering only
  the task subset , the first higher priority  tasks
  , and by  the corresponding
  availability of the processors. Our inductive hypothesis is the
  following: the schedule  is periodic from 
  with a period , for all .

  The property is true in the base case:  is periodic
  from  with period , for :
  since we consider feasible systems, at instant  the
  previous job of  has finished its execution () and the schedule repeats.

  We will now show that any -feasible schedule of 
  is periodic with period  from .

  Since  is periodic with a period  from  the
  following equation is verified:



We denote by  the time
instant obtained by adding  to the time instant which
corresponds to the first activation of  after .

Since the tasks in  have higher priority than
, then the scheduling of  will not interfere
with higher priority tasks which are already scheduled. Therefore, we
may build  from  such that the tasks
 are scheduled at the very same
instants and on the very same processors as there were in
. We apply now the induction step: for all  in  we have  the
availability of the processors repeats. Notice that at the instants
 and  the available processors (if any) are the same. Hence
at only these instants task  {\em may} be executed in the
time interval . 

The instants  such that , where
 may be executed in , are periodic with
period , since  is a multiple of  and . We prove now by contradiction that the system is in the
same state at time instant  and . We suppose that  .

We first prove that  such
that at  there is at least one available processor in
 and no job of  is scheduled at  in
. If there is such an instant , then by
Corollary~\ref{Coreither} we have that  since from the inductive hypothesis (notice that
 is multiple of ) and since  we obtain that
 for . Consequently,  which is
in contradiction with our assumption.

Secondly, since  then by Corollary~\ref{Coreither} we
have that either there are less active jobs at  than at
, or if there is the same number of active jobs of
 then the oldest active job at  was executed for
more time units than the oldest active at . Therefore
since  such that at 
there is at least one processor available in  and no job
of  is scheduled at  in , then we have
that there are no sufficient time instants when at least one processor
is available to schedule all the jobs actived of  in the
time interval . We obtain that the system
is not feasible, which is in contradiction with our assumption of
 being feasible.

Consequently , moreover by
definition of  (which corresponds to an activation of
) the task activations repeat from  which proves
the property.
\end{proof}

\section{Exact feasibility tests} \label{sectionExactFebTest}

In the previous sections, we assumed that the execution requirement of
each task is constant while the designer knows actually only an upper
bound on the actual execution requirement, i.e., the worst case
execution time (WCET). Consequently, we have to show that our tests are
\emph{robust}, i.e., considering the scenario where all task
requirements are the maximal ones is indeed the worst case scenario,
which is not obvious upon multiprocessors because of scheduling
anomalies. More precisely, we have to show that the considered
schedulers upon the considered platforms are \emph{predictable}. Based
on this property of predictability and the periodicity results of
Section~\ref{sectionMainPer}, we provide exact feasibility tests for
the various kind schedulers and platforms considered in this work.

First of all, we introduce and formalize the notion of \emph{feasibility
interval} necessary to provide the exact feasibility tests:

\begin{Definition}[Feasibility interval]
  For any task system  and any
  set of  processors , the {\em feasibility
    interval} is a finite interval such that if no deadline is missed
  while considering only requests within this interval then no
  deadline will ever be missed.
\end{Definition}

\subsection{Preliminary results} \label{sectPremRes}



In this section, we consider the scheduling of sets of job , (finite or infinite set of jobs) and without loss of generality we consider jobs in decreasing order of priorities ). We suppose that the execution times of each job  can be any value in the interval  and we denote by  the job defined from job  as follows:
. The associated execution rates of
 are .  Similarly,
 is the job defined from  as follows:
. Similarly, the associated execution rates
of  are . We denote
by  the set of the first  higher priority jobs. We denote
also by  the set  and by
 the set . Notice that the
schedule of an ordered set of jobs using a work-conserving and
job-level fixed-priority algorithm is unique. Let  be the time instant at
which the lowest priority job of  begins its execution in the
schedule. Similarly, let  be the time instant at which the
lowest priority job of  completes its execution in the schedule.

\begin{Definition}[Predictable algorithms]\label{predAlg}
  A scheduling algorithm is said to be {\em predictable} if  and , for all  and for all
feasible  sets of jobs.
\end{Definition}

In~\cite{Ha} the authors showed that work-conserving job-level fixed-priority
algorithms are predictable on \emph{identical} processors. We will now extend that result by considering \emph{unrelated} platforms.

But first, we will adapt the definition availability of processors (Definition~\ref{defAvai}) to deal with the scheduling of \emph{jobs}. 

\begin{Definition}[Availability of the processors , job scheduling]\label{defAvaiJob}
For any ordered set of jobs  and any set of  unrelated processors
, we define the
  {\em availability of the processors}  of the set of jobs 
  at instant  as the set of available processors: , where
   is the schedule of .
\end{Definition}

\begin{Lemma}\label{lemmaSoon} 
  For any feasible ordered set of jobs  (using the
  job-level fixed-priority and work-conserving schedule) upon an
  arbitrary set of unrelated processors , we
  have that , for all  and
  all . That is, at any time instant the processors available in
   are also available in . (We
  consider that the sets of jobs are ordered in the same decreasing
  order of the priorities, i.e.,  and
  .)
\end{Lemma}

\begin{proof}
  The proof is made by induction by  (the number of jobs).  Our
  inductive hypothesis is the following: , for all  and .
 
  The property is true in the base case since , for all . Indeed, . Moreover  and  are both scheduled on
  their fastest (same) processor , but  will
  be executed for the same or a larger amount of time than .

  We will show now that , for all .

  Since the jobs in  have higher priority than , then
  the scheduling of  will not interfere with higher priority jobs
  which are already scheduled. Similarly,  will not
  interfere with higher priority jobs of  which are
  already scheduled. Therefore, we may build the schedule
   from , such that the jobs , are scheduled at the very same instants and on the
  very same processors as they were in . Similarly, we
  may build  from .

  Notice that  will contain the same available
  processors as  for all  except the time instants at
  which  is scheduled, and similarly 
  will contain the same available processors as  for
  all  except the time instants at which  is
  scheduled. From the inductive hypothesis we have that
  , for all , and
  consequently, at any time instant  we have the following
  situations:

  \begin{itemize}
  \item there is at least one eligible processor in  and among them the fastest processor
    is faster than those belonging to . Consequently,
     can be scheduled at time instant  on faster
    processors than .
  \item there is no eligible processor in . Consequently,  can be scheduled at time
    instant  on the very same processor as .
 \end{itemize}

 Therefore,  can be scheduled either at the very same instants
 than  on the very same or faster processors, or may
 progress during additional time instants. Combined with the fact that
  the property follows for both situations.
\end{proof}

\begin{Theorem} \label{thNotWorkPred} Job-level fixed-priority algorithms are predictable on unrelated platforms.
\end{Theorem}

\begin{proof}
  For a feasible ordered set  of  jobs and a set of unrelated
  processors , we have to show that
   and
  , for all . (The sets of jobs are ordered in the same
  decreasing order of the priorities, i.e., ,  and .)

  The proof is made by induction by  (the number of jobs). We
  show the second part of each inequality, i.e.  and , for all . The proof of the first part of the inequality is
  similar.

  Our inductive hypothesis is the following:  and , for all .

  The property is true in the base case since  and .

  We will show now that  and
  .

  Since the jobs in  have higher priority than  then
  the scheduling of  will not interfere with higher priority
  jobs which are already scheduled. Similarly,  will not
  interfere with higher priority jobs of  which are
  already scheduled.  Therefore, we may build the schedule
   from , such that the jobs , are scheduled at the very same instants and on the
  very same processors as they were in . Similarly, we
  may build  from .  The job
   can be scheduled only when processors, for which the
  associated execution rates are not equal to zero, are available in
   and at those time instants  for
  which  contains at least one eligible
  processor. Similarly,  may be scheduled at those time
  instants  for which 
  contains at least one eligible processor. By the inductive
  hypothesis we know that higher priority jobs complete sooner (or at
  the same time) consequently  and  begins
  its execution in  sooner or at the same instant than
   in , i.e. . It follows by Lemma~\ref{lemmaSoon} that from
  time  the job  can be scheduled at least at the very
  same instants and on the very same processors than ,
  but the job  may also progress at the very same instants on
  faster processors (relatively to its associated set of processors)
  or during additional time instants (since we consider
  work-conserving scheduling). Consequently, .
\end{proof}

\subsection{Asynchronous constrained deadline systems and task-level fixed-priority schedulers}

Now we have the material to define an exact feasibility test for
asynchronous constrained deadline periodic systems.



\begin{Corollary}\label{help} 
For any preemptive task-level fixed-priority algorithm  and for any asynchronous constrained deadline system  on  unrelated processors, we have that  is -feasible if and only if all deadlines are met in  and if , where  are defined inductively in Theorem~\ref{asynPer}. Moreover, for every task  one only has to check the deadlines in the interval .
\end{Corollary}

\begin{proof}
The Corollary~\ref{help} is a direct consequence of Theorem~\ref{asynPer} and Theorem~\ref{thNotWorkPred}, since task-level fixed-priority algorithms are job-level fixed-priority schedulers.
\end{proof}

The feasibility test given by Corollary~\ref{help} may be improved
as it was done in the uniprocessor case~\cite{Goossens2}, actually the prove remains for multiprocessor platforms since it  does not depend on the number of processors, nor on the kind of platforms but on the availability of the processors.

\begin{Theorem}[\cite{Goossens2}]\label{thLowBoundInt} Let  be
  inductively defined by  ; we have that  is -feasible if and only if all deadlines are met in  and if .
  \end{Theorem}

\subsection{Asynchronous arbitrary deadline systems and task-level fixed-priority policies}

Now we have the material to define an exact feasibility test for
asynchronous arbitrary deadline periodic systems.

\begin{Corollary}\label{helpbis} For any preemptive task-level fixed-priority algorithm  and for any asynchronous arbitrary deadline system  on  unrelated
processors, we have that  is -feasible if and only if all deadlines are met in  and if , where  are defined inductively in Theorem~\ref{asynPerbis}.
\end{Corollary}

\begin{proof}
  The Corollary~\ref{helpbis} is a direct consequence of
  Theorem~\ref{asynPerbis} and Theorem~\ref{thNotWorkPred}, since task-level   fixed-priority algorithms are job-level fixed-priority schedulers.
\end{proof}


Notice that the length of our (feasibility) interval is
proportional to  (the least common multiple of the
periods) which is unfortunately also the case of most
feasibility intervals for the \emph{simpler}
\emph{uni}processor scheduling problem (and for identical platforms or
simpler task models). In practice, the periods are usually
\emph{harmonics} which limits fairly the term .

\subsection{{} scheduling of asynchronous arbitrary
  deadline systems}\label{edf}

We know by Corollary~\ref{edfAll1} that any deterministic, request-dependent and feasible
\EDF{} schedule is periodic with a period equal to . Unfortunately,
from the best of our knowledge we have no upper bound on the time
instant at which the periodic part of the schedule begins. Examples
show that  is not such time instant for \EDF{} upon
multiprocessors (see~\cite{Braun2007Negative-Result} for instance).
Other examples, show that in some cases the periodic part of the
schedule begins after a very huge time interval (i.e., many
hyper-periods).

Based on Corollary~\ref{edfAll1} we will however define an \emph{exact} feasibility test under \EDF{} upon multiprocessors. The idea illustrated by Algorithm~\ref{algoedf} is to build the schedule (by means of simulation) and regularly check if the periodic part of the schedule is reached or not.

\begin{algorithm}
\SetKw{kwschedule}{Schedule}
\KwIn{task set }
\KwOut{feasible}
\Begin{
\kwschedule\ (from 0) to  \;
\{The function \kwschedule stops the program and return false once a deadline is missed\}\\
 :=  \;
\kwschedule\ (from ) to  \;
 :=  \;
current-time :=  \;
\While{}{
	 :=  \;
	\kwschedule\ (from current-time) to current-time +  \;
	current-time := current-time +  \;
	 := (current-time) \;
}\Return{true}\;
}\caption{Exact EDF-feasibility test upon multiprocessors\label{algoedf}}
\end{algorithm}

\subsection{The particular case of synchronous periodic task
  systems} \label{labelSectFebSynch}

In this section we present exact feasibility tests in the particular
case of synchronous periodic task systems. In Section
\ref{labelSectFebSynch1}, we study synchronous constrained deadline
task systems and in Section \ref{labelSectFebSynch2} synchronous
arbitrary deadline task systems.

\subsubsection{Synchronous constrained deadline task
  systems} \label{labelSectFebSynch1}

An exact feasibility test for synchronous constrained deadline systems
scheduled could be obtained directly by Theorem~\ref{thNotWorkPred}.

\begin{Corollary}\label{fixIdent2} For any deterministic, memoryless,
  job-level fixed-priority algorithm  and any synchronous constrained deadline system  on  unrelated processors, we have that  is -feasible if and only if all
  deadlines are met in the interval .
\end{Corollary}

\begin{proof}
  The result is a direct consequence of Theorem~\ref{synPer} and
  Theorem~\ref{thNotWorkPred}.
\end{proof}



\subsubsection{Synchronous arbitrary deadline task
  systems} \label{labelSectFebSynch2}

\begin{Corollary}\label{fixIdent2bis} For any preemptive task-level fixed-priority
  algorithm  and any synchronous arbitrary deadline system ,
   is -feasible on  unrelated processors if and only if: all deadlines are met in the interval , and . 
\end{Corollary}

\begin{proof}
  The result is a direct consequence of Corollary~\ref{synPerArbi} and
  Theorem~\ref{thNotWorkPred}, since task-level fixed-priority schedulers are
  priority-driven.
\end{proof}

\section{Conclusion} \label{conclusion}
In this paper we studied the global scheduling of periodic task systems upon heterogeneous multiprocessor platforms. We provided exact feasibility tests based on periodicity properties. 

For any asynchronous arbitrary deadline periodic task system and any task-level fixed-priority scheduler (e.g., \RM) we characterized an upper bound in the schedule where the periodic part begins. Based on that property we provide   feasibility intervals (and consequently an exact feasibility tests) for those schedulers.

From the best of our knowledge such an interval is unknown for \EDF, a \emph{job}-level fixed-priority scheduler. Fortunately, based on a periodicity property we provide an algorithm which determine (by simulation means) where the periodicity is already started (if feasible), this algorithm provides an \emph{exact} feasibility test for \EDF{} upon heterogeneous multiprocessors.

\bibliographystyle{acm}
\bibliography{biblio.bib}

\end{document}