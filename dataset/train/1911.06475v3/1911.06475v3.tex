\documentclass[review]{elsarticle}
\journal{Neurocomputing}

\bibliographystyle{elsarticle-num}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{stackengine}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{framed,multirow}
\usepackage{amssymb} 
\usepackage{amsmath,amssymb}
\usepackage{color}
\def\btheta{\boldsymbol{\theta}}
\begin{document}
\begin{frontmatter}
\title{Interpreting chest X-rays via CNNs that exploit \\ hierarchical disease dependencies and uncertainty labels}

\author{Hieu H. Pham, Tung T. Le, Dat Q. Tran, Dat T. Ngo, Ha Q. Nguyen}
\address{Medical Imaging Group, Vingroup Big Data Institute (VinBDI) \\ 458 Minh Khai street, Hai Ba Trung, Hanoi, Vietnam\
    \hat{y}_k = \dfrac{1}{1 + \exp(-z_k)},\quad k=1,\ldots,14,

    \ell(\btheta) = \sum_{i=1}^{N}\sum_{k=1}^{14} y_k^{(i)}\log\hat{y}_k^{(i)} + \left(1-y_k^{(i)}\right) \log \left(1-\hat{y}_k^{(i)}\right).

   p(\mathcal{C}) &= p(\mathcal{C,B,A}) \\
   &= p(\mathcal{A})p(\mathcal{B} |\mathcal{A})p(\mathcal{C} | \mathcal{\mathcal{B}},\mathcal{A}).
   
    p(\mathcal{D})&=p(\mathcal{D,B,A}) \\
    &= p(\mathcal{A})p(\mathcal{B} |\mathcal{A})p(\mathcal{D} | \mathcal{B},\mathcal{A}).
 
    \bar{y}_k^{(i)} = 
    \begin{cases}
        u, & \text{if } y_k^{(i)}=-1\\ 
        y_k^{(i)} , & \text{otherwise},
    \end{cases}
-0.5cm]
\hline 
\hline 
{\tiny \textbf{Method}}  &  {\tiny \textbf{Atelectasis}} &{\tiny\textbf{Cardiomegaly}}&{\tiny\textbf{Consolidation}} &  {\tiny \hspace*{0.2cm} \textbf{Edema}}  &  {\tiny \textbf{P. Effusion}}   &   {\tiny \hspace*{0.1cm} \textbf{Mean}} \\
\hline                                                         
{\scriptsize  \texttt{U-Ignore}}  &    \hspace*{0.3cm}{\scriptsize 0.768} & \hspace*{0.30cm}{\scriptsize 0.795}  &   \hspace*{0.40cm}{\scriptsize 0.915} &   \hspace*{0.4cm}{\scriptsize \textbf{0.914}} &   \hspace*{0.40cm}{\scriptsize 0.925} &  \hspace{0.2cm}{\scriptsize  0.863}
\\
{\scriptsize \textbf{\texttt{U-Ignore+CT}}} &   \hspace*{0.3cm}{\scriptsize \textbf{0.780}}  &    \hspace*{0.3cm}{\scriptsize \textbf{0.815}} &   \hspace*{0.40cm}{\scriptsize \textbf{0.922}}  &  \hspace*{0.40cm}{\scriptsize \textbf{0.914}}   &  \hspace*{0.40cm}{\scriptsize \textbf{0.928}} &  \hspace{0.2cm}{\scriptsize \textbf{0.872}}\\
\hline
{\scriptsize  \texttt{U-Zeros}}   \cellcolor{gray!30} & \cellcolor{gray!30} \hspace*{0.3cm}{\scriptsize 0.745}  &   \cellcolor{gray!30} \hspace*{0.3cm}{\scriptsize 0.813} & \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize 0.882} &  \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize 0.921}   &  \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize \textbf{0.930}} &  \cellcolor{gray!30} \hspace{0.2cm}{\scriptsize 0.858}\\
{\scriptsize \cellcolor{gray!30} \texttt{U-Zeros+CT}} &   \cellcolor{gray!30} \hspace*{0.3cm}{\scriptsize 0.782}  &    \cellcolor{gray!30} \hspace*{0.3cm}{\scriptsize \textbf{0.835}} &  \cellcolor{gray!30}  \hspace*{0.40cm}{\scriptsize 0.922}  &  \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize 0.923}   &  \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize 0.921} &  \cellcolor{gray!30} \hspace{0.2cm}{\scriptsize 0.877}\\
\cellcolor{gray!30} {\scriptsize \texttt{U-Zeros+LSR}}  & \cellcolor{gray!30} \hspace*{0.3cm}{\scriptsize 0.781} & \cellcolor{gray!30} \hspace*{0.30cm}{\scriptsize 0.815}  & \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize 0.920} &  \cellcolor{gray!30} \hspace*{0.4cm}{\scriptsize 0.923} &  \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize 0.918} & \cellcolor{gray!30} \hspace*{0.2cm}{\scriptsize 0.871} \\
\cellcolor{gray!30} {\scriptsize \texttt{\textbf{U-Zeros+CT+LSR}}}  & \cellcolor{gray!30} \hspace*{0.3cm}{\scriptsize \textbf{0.806}} & \cellcolor{gray!30} \hspace*{0.30cm}{\scriptsize 0.833}  &  \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize \textbf{0.929}} &  \cellcolor{gray!30} \hspace*{0.4cm}{\scriptsize \textbf{0.933}} &  \cellcolor{gray!30} \hspace*{0.40cm}{\scriptsize 0.921} &  \cellcolor{gray!30} \hspace*{0.2cm}{\scriptsize \textbf{0.884}} \\
\hline
{\scriptsize \cellcolor{gray!60} \texttt{U-Ones}} & \cellcolor{gray!60}  \hspace*{0.3cm}{\scriptsize 0.800}  &   \cellcolor{gray!60}  \hspace*{0.3cm}{\scriptsize 0.780} & \cellcolor{gray!60}  \hspace*{0.40cm}{\scriptsize 0.882}  & \cellcolor{gray!60}  \hspace*{0.40cm}{\scriptsize 0.918}   & \cellcolor{gray!60}  \hspace*{0.40cm}{\scriptsize 0.920} & \cellcolor{gray!60}  \hspace{0.2cm}{\scriptsize 0.860}\\


{\scriptsize \texttt{U-Ones+CT}} \cellcolor{gray!60} &  \cellcolor{gray!60} \hspace*{0.3cm}{\scriptsize 0.813}  &   \cellcolor{gray!60} \hspace*{0.3cm}{\scriptsize 0.816} &  \cellcolor{gray!60} \hspace*{0.40cm}{\scriptsize 0.895}  &   \cellcolor{gray!60}\hspace*{0.40cm}{\scriptsize 0.923}   &  \hspace*{0.40cm}{\scriptsize 0.912} \cellcolor{gray!60} &   \cellcolor{gray!60}\hspace{0.2cm}{\scriptsize 0.872}\\
{\scriptsize \texttt{U-Ones+LSR}}  \cellcolor{gray!60} &  \cellcolor{gray!60} \hspace*{0.3cm}{\scriptsize 0.818} & \cellcolor{gray!60} \hspace*{0.30cm}{\scriptsize 0.834}  &  \cellcolor{gray!60} \hspace*{0.40cm}{\scriptsize 0.874} &  \cellcolor{gray!60} \hspace*{0.4cm}{\scriptsize 0.925} &  \cellcolor{gray!60} \hspace*{0.40cm}{\scriptsize 0.921} & \cellcolor{gray!60} \hspace*{0.2cm}{\scriptsize 0.874} \\

{\scriptsize \texttt{U-Ones+CT+LSR}}  \cellcolor{gray!60} &  \cellcolor{gray!60} \hspace*{0.3cm}{\scriptsize \textbf{0.825}} &  \cellcolor{gray!60} \hspace*{0.30cm}{\scriptsize \textbf{0.855}}  & \cellcolor{gray!60} \hspace*{0.40cm}{\scriptsize \textbf{0.937}} &  \cellcolor{gray!60} \hspace*{0.4cm}{\scriptsize \textbf{0.930}} &  \cellcolor{gray!60} \hspace*{0.40cm}{\scriptsize \textbf{0.923}} &  \cellcolor{gray!60} \hspace*{0.2cm}{\scriptsize \textbf{0.894}} \\
\hline 
\hline
\end{tabular}
\end{table} 

\begin{table}
\centering
\caption{\label{tab:performance_comparison} Performance comparison using AUC metric between our ensemble of 6 models and previous works on the CheXpert validation set. The highest AUC scores are boldfaced.}
\begin{tabular}{p{2.25cm}p{1.3cm}p{1.3cm}p{1.2cm}p{1.2cm}p{1.3cm}p{0.9cm}}
\0.2cm]
    \includegraphics[width=3.8cm,height=3.8cm]{edema.png}
    \includegraphics[width=3.8cm,height=3.8cm]{pleural_effusion.png}\-0.2cm]
  {\scriptsize    (a) \hspace{3cm} (b) \hspace{3cm} (c)}\\
      \includegraphics[width=3.8cm,height=2.8cm]{performance_comparison_U_Zeros_LSR.png}
    \includegraphics[width=3.8cm,height=2.8cm]{performance_comparison_U_Ones_LSR.png}
    \includegraphics[width=3.8cm,height=2.8cm]{performance_comparison_U-Zeros+CT_U-Zeros+CT+LSR.png}\-0.2cm]
      {\scriptsize (g) \hspace{3cm} (h) \hspace{3cm} (i)}\-0.5cm]
    \caption{Performance comparison based on AUC metric for different settings.}
    \label{fig:roc_curves}
\end{figure}
\fi
\begin{figure}[ht]
  \centering
  \includegraphics[width=12cm,height=4.5cm]{predictions}
  \caption{Visualization of findings by the proposed network during the inference stage.}
  \label{fig:predictions}
\end{figure} 
\subsection{Independent evaluation and comparison to radiologists}

A crucial evaluation of any machine learning-based medical diagnosis system (ML-MDS) is to evaluate how well the system performs on an independent test set in comparison to human expert-level performance. To this end, we evaluated the proposed method on the hidden test set of CheXpert, which contains 500 CXRs labeled by 8 board-certified radiologists. The annotations of 3 of them were used for benchmarking radiologist performance and the majority vote of the other 5 served as ground truth. For each of the 3 individual radiologists, the AUC scores for the 5 selected diseases (\textit{Atelectasis}, \textit{Cardiomegaly}, \textit{Consolidation}, \textit{Edema}, and \textit{Pleural Effusion}) were computed against the ground truth to evaluate radiologists' performance. We then evaluated our ensemble model on the test set and performed ROC analysis to compare the model performance to radiologists. For more details, the ROCs produced by the prediction model and the three radiologists' operating points were both plotted. For each disease, whether the model is superior to radiologists' performances was determined by counting the number of radiologists' operating points lying below the ROC\footnote{This test was conducted independently with the support of the Stanford Machine Learning Group as the test set is not released to the public.}. The result shows that our deep learning model, when being averaged over the 5 diseases, outperforms 2.6 out of 3 radiologists with an AUC of 0.930. This is the best performance on the CheXpert leaderboard to date. The attained AUC score validates the generalization capability of the trained deep learning model on an unseen dataset. Meanwhile, the total number of radiologists under ROC curves indicates that the proposed method is able to reach human expert-level performance---an important step towards the application of an ML-MDS in real-world scenarios.

\section{Discussions}
\label{sec:5}
\subsection{Key findings and meaning}

By training a set of strong CNNs on a large scale dataset, we built a deep learning model that can accurately predict multiple thoracic diseases from CXRs. In particular, we empirically showed a major improvement, in terms of AUC score, by exploiting the dependencies among diseases and by applying the label smoothing technique to uncertain samples. We found that it is especially difficult to obtain a good AUC score for all diseases with a single CNN. It is also observed that the classification performance varies with network architectures, the rate of positive/negative samples, as well as the visual features of the lung disease being detected. 
In this case, an ensemble of multiple deep learning models plays a key in boosting the generalization of the final model and its performance. Our findings, along with recent publications~\cite{rajpurkar2017chexnet,guan2018diagnose,rajpurkar2018deep,kumar2018boosted}, continue to assert that deep learning algorithms can accurately identify the risk of many thoracic diseases and is able  to assist patient screening, diagnosing, and physician training.

\subsection{Limitations}
Although a highly accurate performance has been achieved, we acknowledge that the proposed method reveals some limitations. The conditional training strategy requires a predefined hierarchy of diseases, which is not easy to construct and usually imperfect. Furthermore, it seems difficult to extend this idea to deeper hierarchies of diseases, for which too many examples have to be excluded from the training set in the first phase. The use of LSR in this paper with heuristically chosen hyper-parameters, while significantly improving the classification performance, is not clearly justified. In addition, the use of TTA has also a limitation due to it decreases inference time.

Other challenges are related to the training data. For instance, the deep learning algorithm was trained and evaluated on a CXR data source collected from a single tertiary care academic institution. Therefore, it may not yield the same level of performance when applied to data from other sources such as from other institutions with different scanners. This phenomenon is called \textit{geographic variation}. To overcome this, the learning algorithm should be trained on data that are diverse in terms of regions, races, imaging protocols, etc. Next, to make a diagnosis from a CXR, doctors often rely on a broad range of additional data such as patient age, gender, medical history, clinical symptoms, and possibly CXRs from different views. This additional information should also be incorporated into the model training. Finally, CXR image quality is another problem. When taking a deeper look at the CheXpert, we found a considerable rate of samples in low quality (\textit{e.g.} rotated image, low-resolution, samples with texts, noise, etc.) that definitely hurts the model performance. In this case, a template matching-based method as proposed in this work may be insufficient to effectively remove all the undesired samples. A more robust preprocessing technique, such as that proposed in~\cite{ccalli2019frodo}, should be applied to reject almost all \textit{out-of-distribution} samples.

\section{Conclusion} 
\label{sec:6}
We presented in this paper a comprehensive approach for building a high-precision computer-aided diagnosis system for common thoracic diseases classification from CXRs. We investigated almost every aspect of the task including data cleaning, network design, training, and ensembling. In particular, we introduced a new training procedure in which dependencies among diseases and uncertainty labels are effectively exploited and integrated in training advanced CNNs. Extensive experiments demonstrated that the proposed method  outperforms the previous state-of-the-art by a large margin on the CheXpert dataset. More importantly, our deep learning algorithm exhibited a performance on par with specialists in an independent test. There are several possible mechanisms to improve the current method. The most promising direction is to increase the size and quality of the dataset. A larger and high-quality labeled dataset can help deep neural networks generalize better and reduce the need for transfer learning from ImageNet. For instance, extra training data from MIMIC-CXR~\cite{johnson2019mimic}, which uses the same labeling tool as CheXpert, should be considered. We are currently expanding this research by collecting a new large-scale CXR dataset with radiologist-labeled reference from several hospitals and medical centers in Vietnam. The new dataset is needed to validate the proposed method and to confirm its usefulness in different clinical settings. 
We believe the cooperation between a machine learning-based medical diagnosis system and radiologists will improve the outcomes of thoracic disease diagnosis and bring benefits to clinicians and their patients.

\section{Acknowledgements} 
This research was supported by the Vingroup Big Data Institute (VinBDI). The authors gratefully acknowledge Jeremy Irvin from the Machine Learning Group, Stanford University for helping us  evaluate the proposed method on the hidden test set of CheXpert.

\bibliography{references}
\


\
\end{document}