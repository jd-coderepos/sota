\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}\usepackage[table,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cuted}
\usepackage{float}
\usepackage[numbers,sort]{natbib}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\renewcommand \thepart{}
\renewcommand \partname{}
\newcommand{\xmark}{\ding{55}}\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}


\newcommand{\mb}[1]{\textcolor{BrickRed}{[max: #1]}}
\newcommand{\gul}[1]{\textcolor{DarkOrchid}{[gul: #1]}}
\newcommand{\arsha}[1]{\textcolor{cyan}{[arsha: #1]}}
\newcommand{\az}[1]{\textcolor{cyan}{[az: #1]}}
\newcommand{\draftfig}[1]{\fbox{\parbox[c][1in][c]{\linewidth}{#1}}}

\setlength{\parskip}{.2em}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\doparttoc \faketableofcontents 





\title{Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval}
\author{Max Bain \quad Arsha Nagrani\footnotemark[2] \quad G\"ul Varol \quad Andrew Zisserman \\
 Visual Geometry Group, University of Oxford \\
 LIGM, \'Ecole des Ponts, Univ Gustave Eiffel, CNRS \\
{\tt\small \{maxbain, arsha, gul, az\}@robots.ox.ac.uk}
}

\maketitle

\noindent
\vspace{-0.5cm}
\begin{strip}
    \centering\noindent
    \includegraphics[width=0.99\linewidth]{figures/arch.pdf}
    \captionof{figure}{\textbf{Joint Image and Video Training:}  Our dual encoding model consists of a visual encoder for images and video and a text encoder for captions. Unlike 2D or 3D CNNs, our space-time transformer encoder allows us to train flexibly on both images and videos with captions jointly, by treating an image as a single frame video.}
    \label{fig:model}
\end{strip}
\ificcvfinal\thispagestyle{empty}\fi
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[2]{Now at Google Research.}
\renewcommand*{\thefootnote}{\arabic{footnote}}
\begin{abstract}
Our objective in this work is video-text retrieval -- in particular a
joint embedding that enables efficient text-to-video retrieval. The
challenges in this area include the design of the visual architecture
and the nature of the training data, in that the available large scale
video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute.


We address both these challenges in this paper.
We propose an end-to-end trainable model that is
designed to take advantage of both large-scale \texttt{image} and
video captioning datasets. Our model is an adaptation and extension of the 
recent  ViT and Timesformer architectures, and consists of attention in both
space and time. The  model is flexible and can be trained on
both image and video text datasets, either independently or in
conjunction.  It  is trained with a curriculum learning schedule
that begins by treating images as `frozen' snapshots of video, and
then gradually learns to attend to increasing temporal context when
trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M,
comprised of over two million videos with weak captions scraped from
the internet. Despite training on datasets that are an order of magnitude smaller, 
we show that this approach yields state-of-the-art
results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.


\vspace{-2em}
\end{abstract} 
\section{Introduction}
Joint visual-text models have become increasingly popular as they enable a 
wide suite of downstream tasks, including text-to-visual retrieval~\cite{lin2014microsoft,wang2016learning,miech18learning,Liu19a},
visual captioning~\cite{vinyals2016show,you2016image,krishna2017dense}, and visual question and answering~\cite{antol2015vqa,lei2018tvqa}. Their rapid development is due to the usual improvements on
three fronts: new neural network architectures (e.g.\ transformers~\cite{vaswani2017attention} for both text and visual inputs); new large-scale
datasets; and new loss functions that are, for example, able to handle label noise~\cite{miech20endtoend}.
However, their development mostly proceeds on two independent tracks: one for {\em images}, with its own architectures,
training datasets and benchmarks~\cite{lin2014microsoft,krishna2017visual,sharma2018conceptual}; and the other for {\em videos} with a similar separation of training datasets and benchmarks~\cite{xu2016msr,anne2017localizing,krishna2017dense,rohrbach2017movie,zhou2018towards,bain2020condensed}. The only common link between the two is that often video networks are initialized by pre-training image networks
on image datasets~\cite{Carreira2017,bertasius2021spacetime}.
This separation of effort is suboptimal given the overlap in information that images and video convey over multiple tasks.
For example, although classifying some human actions requires the temporal ordering of video frames, many actions
can be classified from just their distribution over frames or even from a single frame~\cite{sevilla2021only}.

In this paper we take a step towards unifying these two tracks, by  proposing  a dual encoder architecture which utilises the flexibility of a transformer
visual encoder to train from images-with-captions, from video clips-with-captions, or from both (Fig.~\ref{fig:model}). 
We do this by treating images as a special case of videos that are `frozen
in time'. Using a transformer-based architecture allows us to train
with variable-length sequences, treating
an image as if it was a single frame video, unlike in standard 3D CNNs~\cite{Carreira2017,HaraCVPR2018,xie2018rethinking} where to train on images jointly with videos one must incur the cost
of actually generating a static video.
Furthermore, unlike many recent
methods~\cite{miech18learning,Liu19a, gabeur2020multi} for video-text dual encoding, we do not use a set of `expert networks' that are pre-trained on external image
datasets and then fixed, but instead train the model end-to-end.

This end-to-end training is facilitated by scraping the web for a new large-scale video-text captioning dataset of over two million video alt-text pairs (WebVid-2M). We also take advantage of large-scale image captioning datasets such as Conceptual Captions~\cite{sharma2018conceptual}.  

We make the following contributions: (i) we propose a new 
end-to-end model for video retrieval that does \textit{not} rely on `expert' features, but instead, inspired by~\cite{bertasius2021spacetime} employs a transformer 
architecture with a modified divided space-time attention applied directly to pixels; 
 (ii) because our architecture can gracefully handle inputs of different lengths, it is versatile and can be flexibly trained on both video and image datasets (by treating images as a single-frame video). We build on this flexibility by designing a curriculum learning schedule that begins with images and then gradually learns to attend to increasing temporal context when trained on video datasets through temporal embedding interpolation. We show that this increases efficiency, allowing us to train models with far less GPU time; 
 
 (iii) we introduce a new dataset called WebVid-2M, consisting of 2.5M video-text pairs scraped from the web; and finally (iv) we achieve state-of-the-art performance by only using the video modality on MSR-VTT~\cite{xu2016msr}, MSVD~\cite{chen2011collecting},
 DiDeMo~\cite{anne2017localizing}
 and LSMDC~\cite{rohrbach2017movie}

outperforming works that use pre-extracted experts from multiple 
modalities, as well as those that are pretrained on the noisy HowTo100M, which is 20x larger than our dataset in the 
number of video-text pairs.
 \section{Related Works}
\noindent\textbf{Pretraining for video-text retrieval.}
Given that most video-text retrieval datasets tend to be small-scale, the dominant paradigm for video retrieval has been to use a combination of pre-extracted features from `expert' models, including models trained for various diverse tasks and on multiple modalities such as face, scene and object recognition, action classification and sound classification. MoEE~\cite{miech18learning},
CE~\cite{Liu19a} and
MMT~\cite{gabeur2020multi} all follow this paradigm, with the overall similarity for a video-text pair obtained as a weighted sum of each expertâ€™s similarity with the text. 

However, since the release of the HowTo100M dataset~\cite{miech2019howto100m}, a large-scale
instructional video dataset, there has been a flurry of works leveraging large-scale pretraining to
improve video-text representations for tasks such as video question-answering~\cite{seo2021look},
text-video retrieval~\cite{patrick2020support} and video captioning~\cite{zhou2018end}. Although semantically rich and diverse, text supervision from instructional videos is extremely noisy, and hence incurs a large computational cost, as scale is required for competitive results. A few approaches have been proposed to combat the noise --  \eg using loss functions such as MIL-NCE~\cite{miech20endtoend} or using the raw audio~\cite{alayrac2020self,rouditchenko2020avlnet} directly to increase robustness. Given the large size of existing image-captioning datasets, some have naturally tried to overcome the lack of video-caption training data with joint image-text pretraining (such as in MoEE~\cite{miech18learning} and ClipBERT~\cite{lei2021less}). MoEE~\cite{miech18learning} trains on images jointly by feeding in zeros to all expert streams that require videos, such as the motion and audio features, while ClipBERT~\cite{lei2021less} restricts their feature extractors to 2D CNNs. Instead we propose an elegant transformer-based encoder that works well with either images or videos and can be trained effectively on both.

\noindent\textbf{End-to-end video representation learning.}
A large number of architectural developments have been driven by action recognition
on datasets such as Kinetics~\cite{Kinetics} where manual labelling has been relatively easier than obtaining textual descriptions for datasets. For a long time this space was dominated by spatio-temporal CNNs such as I3D~\cite{Carreira2017},
3D ResNets~\cite{HaraCVPR2018}, S3D~\cite{xie2018rethinking}
or `R(2+1)D' CNNs \cite{Tran2018ACL}. Here, images are used simply to initialise video models, through inflation~\cite{Carreira2017}. Multigrid scheduling has been proposed for efficient training~\cite{Wu2020AMM}.

\noindent\textbf{Transformers for vision.} A number of works use self-attention for images, either in combination with convolutions~\cite{wang2018non,vaswani2017attention,hu2017relation,carion2020endtoend} or even replacing them entirely. 

Works that use only self-attention blocks tend to apply them at an individual pixel level~\cite{parmar2018image,ramachandran2019stand,cordonnier2019relationship}, often requiring tricks to ensure computational tractability, including restricting the scope of self-attention to a local neighbourhood~\cite{ramachandran2019stand}, adding global self-attention on heavily downsized versions, or sparse key-value sampling~\cite{child2019generating}. To increase efficiency, ViT~\cite{dosovitskiy2021an} decompose images into a sequence of patches and then feeds linear embeddings of
these patches as inputs to a transformer, effectively adding a single convolutional layer to the image at the start. This idea has been extended in DeiT~\cite{touvron2020deit}. For video, previous works also employ self-attention blocks together with CNN layers, for action recognition~\cite{girdhar2017actionvlad} and video classification~\cite{chen20182}. 

In contrast, our architecture consists entirely of self-attention units and is heavily inspired by ViT~\cite{dosovitskiy2021an} and particularly the Timesformer~\cite{bertasius2021spacetime}, which uses divided space and time attention. Unlike these works, we use expandable temporal embeddings to allow flexible training of variable-length videos and images both jointly and separately. We are unaware of any previous works that use self-attention to train on both images and videos in the same model.


 \section{Method}


In this section, we describe our transformer-based spatio-temporal model architecture
(Section~\ref{subsec:architecture}),
and our training strategy (Section~\ref{subsec:training}).
Details are given in the Appendix.

\subsection{Model Architecture}
\label{subsec:architecture}

\noindent\textbf{Input.} The visual encoder takes as input an image or video clip  consisting of  frames of resolution , where  for images. The text encoder takes as input a tokenised sequence of words.

\noindent\textbf{Spatio-temporal patches.} Following the protocol in ViT and Timesformer~\cite{bertasius2021spacetime}, the input video clip is divided into  non-overlapping spatio-temporal patches of size , where .

\noindent\textbf{Transformer input.}
The patches  are fed through a 2D convolutional layer and the output is flattened, forming a sequence of embeddings  for input to the transformer, where  depends of the number of kernels in the convolutional layer.

Learned temporal and spatial positional embeddings, ,  are added to each input token:

such that all patches within a given frame  (but different spatial locations) are given the same temporal positional embedding , and all patches in the same spatial location (but different frames) are given the same spatial positional embedding . Thus enabling the model to ascertain the temporal and spatial position of patches.
 
In addition, a learned [CLS] token~\cite{devlin2019bert} is concatenated to the beginning of the sequence, which is used to produce the final visual embedding output embedding of the transformer.


\noindent\textbf{Space-time self-attention blocks.}
The video sequence is fed into a stack of space-time transformer blocks. We make a minor modification to the Divided Space-Time attention introduced by~\cite{bertasius2021spacetime}, by replacing the residual connection between the block input and the temporal attention output 
with a residual connection between the block input and the spatial attention output, see Fig.~\ref{fig:spacetimeattn}. Each block sequentially performs temporal self-attention and then spatial self-attention on the output of previous block. The video clip embedding is obtained from the [CLS] token of the final block.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/divided.pdf}
    \caption{\textbf{Attention block:} The original divided block used in the Timesformer~\cite{bertasius2021spacetime} architecture (left) compared to ours (right). We find that this minor modification of the input residual connection trains more quickly and is more stable than the original.}
    \label{fig:spacetimeattn}
\end{figure}

\noindent\textbf{Text encoding.}
The text encoder architecture is a multi-layer bidirectional transformer encoder, which has shown great success in natural language processing tasks~\cite{devlin2019bert}. For the final text encoding, we use the [CLS] token output of the final layer.

\noindent\textbf{Projection to common text-video space.}
Both text and video encodings are projected to a common dimension via single linear layers. We compute the simliarity between text and video by performing the dot product between the two projected embeddings.

\noindent\textbf{Efficiency.} Our model has independent dual encoder pathways (such as in MIL-NCE~\cite{miech20endtoend} and MMV networks~\cite{alayrac2020self}), requiring only the dot product between the video and text embeddings. This ensures retrieval inference is of trivial cost since it is indexable, i.e.\ it allows application of fast approximate nearest neighbour search, and is scalable to very large scale retrieval at inference time. Given  text queries and  videos in a target gallery, our retrieval complexity is . In contrast, ClipBERT~\cite{lei2021less} which inputs both text and video as input to a single encoder, has retrieval complexity  since every text-video combination be inputted to the model. Other expert-based retrieval methods such as MoEE~\cite{miech18learning}, CE~\cite{Liu19a} and MMT~\cite{gabeur2020multi} also contain a dual encoder pathway, however they still require query-conditioned weights to compute the similarity scores for each expert, while our model does not.




\subsection{Training Strategy}
\label{subsec:training}

\noindent\textbf{Loss.}
We employ~\cite{Zhai2019ClassificationIA} in a retrieval setting, where matching text-video pairs in the batch are treated as positives, and all other pairwise combinations in the batch are treated as negatives. We minimise the sum of two losses,
video-to-text
and text-to-video:


where  and  are the normalized embeddings of -th video and the -th text respectively in a batch of size  and  is the temperature.


\noindent\textbf{Joint image-video training.} In this work, we train jointly on both image-text pairs as well as video-text pairs, taking advantage of both for larger-scale pretraining. Our joint training strategy involves alternating batches between the image and video datasets. Since the attention mechanism scales with the square of input frames , the alternate batch training allows the image batches () to be far greater in size. 

\noindent\textbf{Weight initialisation and pretraining.}
Following~\cite{bertasius2021spacetime}, we initialise the spatial attention weights in the space-time transformer model with ViT~\cite{dosovitskiy2021an} weights trained on ImageNet-21k, and initialise the temporal attention weights intialised to zero.
The residual connections mean that under these initialisation settings, the model is at first equivalent to ViT over each input frame -- thereby allowing the model to learn to attend to time gradually as training progresses. Since transformer architectures have demonstrated most of their success from large-scale pretraining, we utilise two large-scale text-image/video datasets with a joint training strategy, resulting in large improvements in performance.

\noindent\textbf{Temporal curriculum learning.} The space-time transformer architecture allows a variable length input sequence and therefore a variable number of input video frames. If the model has only trained on videos up to length  however, then the temporal positional embedding  will only be learned up to .

Therefore, applying the model to input video of sequences up to length  will result the addition of , which would not yet be learned.

In this work we investigate two strategies for expanding the temporal embeddings to enable curriculum learning of longer and longer frames. Two temporal expansion methods are investigated: \textit{interpolation} and \textit{zero-padding}. Zeros can be filled in, , allowing the model to learn the additional temporal positions from scratch during training. Alternatively, interpolation could be used to upsample the temporal embeddings in the temporal dimension, . We investigate two methods of interpolation: nearest neighbour and bilinear. The effects of these different initialisations can be found in the Section~\ref{sec:temp_expansion} of the Appendix.

We employ this expansion strategy in order to perform curriculum learning in the number of input frames. Initially training on fewer frames has drastic savings in computation, whilst having comparable or even better performance (see Section~\ref{subsec:curriculumexp}).

\noindent\textbf{Frame sampling.} Given a video containing  frames, we subdivide it into  equal segments where  is the desired number of frames for the video encoder. During training, we sample a single frame uniformly from each segment (in a similar manner to  TSN~\cite{wang_tsn} and GST~\cite{gst}).
At test time, we sample the  frame in every segment, to get a video embedding . The values for  are determine using a stride , resulting in an array of video embeddings . The mean of these video embeddings is used as the final embedding for the video.

 

\begin{table*}
\centering
\caption{\textbf{Dataset Statistics:} We train on a new dataset mined from the web called WebVid2M. Our dataset is an order of magnitude larger than existing video-text datasets in the number of videos and captions. HowTo100M (highlighted in blue) is a video dataset with noisy, weakly linked text supervision from ASR.}
\begin{tabular}{llrrrr}
\toprule
\textbf{dataset} & \textbf{domain} & \textbf{\#video clips} & \textbf{avg len (sec)} & \textbf{\#sent} & \textbf{video time (hrs)}\\ \midrule
MPII Cooking~\cite{rohrbach2012database} & youtube(cooking) & 44 & 600 & 6K & 8\\ 

TACos~\cite{regneri2013grounding} & youtube(cooking) & 7K & 360 & 18K & 15.9  \\ 


DideMo~\cite{anne2017localizing} & flickr & 27K & 28 & 41K & 87\\ 
MSR-VTT~\cite{xu2016msr} & youtube & 10K & 15 & 200K & 40 \\ 
Charades~\cite{sigurdsson2016hollywood} & home & 10K & 30 & 16K & 82 \\ 
LSMDC15~\cite{rohrbach2017movie} & movies & 118K & 4.8 & 118K & 158\\ 
YouCook II~\cite{zhou2018towards} & youtube(cooking)& 14K & 316 & 14K & 176 \\ 
ActivityNet Captions~\cite{krishna2017dense} & youtube  (actions focused) & 100K & 180 & 100K & 849 \\ 
CMD~\cite{bain2020condensed} & movies & 34K & 132 & 34K & 1.3K \\
\textbf{WebVid-2M} & open  & \textbf{2.5M} & 18 & \textbf{2.5M} & \textbf{13K} \\ \rowcolor{aliceblue}
HowTo100M~\cite{miech2019howto100m} & Instructional & 136M & 4 & 136M &  134.5K \\ 



\bottomrule            
\end{tabular}
\label{tab:datastats}
\end{table*} 
\begin{figure*}
\captionsetup[subfigure]{labelformat=empty}
     \centering
     \begin{subfigure}[t]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/woman.jpg}
         \caption{\footnotesize{``Lonely beautiful woman sitting on the tent looking outside. wind on the hair and camping on the beach near the colors of water and shore. freedom and alternative tiny house for traveler lady drinking"}}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.23\textwidth}
         \centering
\includegraphics[width=\textwidth]{figures/shuttervids/billiard-page-001.jpg}
         \caption{\footnotesize{``Billiards, concentrated young woman playing in club"}}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/police-page-001.jpg}
         \caption{\footnotesize{``Female cop talking on walkie-talkie, responding emergency call, crime prevention''}}
     \end{subfigure}
     \hfill
    \begin{subfigure}[t]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/water.jpg}
         \caption{\footnotesize{``Get anchor for departure safari dive boat scuba diving maldives"}}
     \end{subfigure}
     \hfill
                  \caption{\textbf{Example video-caption pairs from the WebVid2M dataset:} Note the different captioning styles: from left to right, captions can be (i) long, slightly poetic, with disjoint sentences and phrases, (ii) succint and to the point, (iii) have a less defined sentence structure with keywords appended to the end, (iv) mention specific places (`maldives'). We show two randomly sampled frames for each video.}

        \label{fig:webvid2m}
\end{figure*} \section{Experiments}
We first describe the pretraining datasets including our WebVid-2M video-text dataset 
(Section~\ref{subsec:datasets1}), followed by the downstream datasets used for the evaluations
in our experiments (Section~\ref{subsec:datasets2}).
We then describe implementation details of our model (Section~\ref{subsec:implementation}).
Next, we ablate various training components on the MSR-VTT dataset,
in particular the effects of pretraining and our space-time attention modification (Section~\ref{subsec:ablation}),
and our proposed curriculum strategy (Section~\ref{subsec:curriculumexp}).
Then, we compare to the state of the art on
four benchmarks: MSR-VTT, MSVD, DiDeMo and LSMDC (Section~\ref{subsec:sota}).


\subsection{Pretraining Datasets}
\label{subsec:datasets1}
We jointly pretrain our model on image and video data.

\noindent\textbf{Video pretraining: The WebVid-2M Dataset.}
We scrape the web for a new dataset of videos with textual description annotations, called WebVid-2M. Our dataset consists of 2.5M video-text pairs, which is an order of magnitude larger than existing video captioning datasets (see Table~\ref{tab:datastats}).

The data was scraped from the web following a similar procedure to Google Conceptual Captions~\cite{sharma2018conceptual} (CC3M). We note that more than 10\% of CC3M images are in fact thumbnails from videos, which motivates us to use such video sources to scrape a total of 2.5M text-video pairs.
The use of data collected for this study is authorised via the Intellectual Property Officeâ€™s Exceptions to Copyright for Non-Commercial Research and Private Study\footnote{\url{www.gov.uk/guidance/exceptions-to-copyright/}}. We are currently performing further analysis on the dataset in its diversity and fairness.

Figure~\ref{fig:webvid2m} provides sample video-caption pairs. There are a variety of different styles used in caption 
creation, as can be seen from Figure~\ref{fig:webvid2m} (left to right) where the first video has a longer, poetic 
description compared to the succinct description for the second video. The third video caption has a less defined sentence 
structure, with keywords appended to the end, while the fourth video mentions a specific place (maldives). Time-specific information is important for the second and third example, where details such as ``talking on walkie-talkie'' or ``playing billiards'' would be missed when looking at certain frames independently.

We note that our video dataset is 10x smaller than HowTo100M in video duration and over 20x smaller in the number of paired clip-captions (Table \ref{tab:datastats}). Our dataset consists of manually generated captions, that are for the most part well formed sentences. In contrast, HowTo100M is generated from continuous narration with incomplete sentences that lack punctuation. The clip-text pairs are obtained from subtitles and may not be temporally aligned with the video they refer to, or indeed may not refer to the video at all~\cite{miech2019howto100m}. Our captions, on the other hand, are aligned with the video and describe visual content.

Moreover, there is no noise from imperfect ASR transcription and grammatical errors as is the case for HowTo100M. Our dataset also has longer captions on average (12 vs 4 words for HowTo) which are more diverse (Measure of Textual Lexical Diversity, MTLD~\cite{mccarthy2010mtld} = 203 vs 13.5).

\noindent\textbf{Image pretraining: Google Conceptual Captions~\cite{sharma2018conceptual}.}
This dataset consists of about 3.3M image and description pairs. Unlike the curated style of COCO images, Conceptual Captions (CC3M) images and their raw descriptions are harvested from the web, and therefore represent a wider variety of styles. The raw descriptions
are harvested from the Alt-text HTML attribute associated with web images.

\subsection{Downstream Datasets}
\label{subsec:datasets2}
We now describe the downstream text-video datasets that our model is evaluated on. \\
\noindent\textbf{MSR-VTT~\cite{xu2016msr}} contains
10K YouTube videos with 200K descriptions. Following other works~\cite{Liu19a}, we train on 9K train+val videos and report results on the 1K-A test set. \\
\noindent\textbf{MSVD~\cite{chen2011collecting}} consists of 80K English descriptions for 1,970 videos from YouTube, with each video containing 40 sentences each. We use the standard split of 1200, 100, and 670 videos for training, validation, and testing~\cite{patrick2020support,Liu19a}.

\noindent\textbf{DiDeMo~\cite{anne2017localizing}} contains 10K Flickr videos annotated with 40K sentences. Following \cite{lei2021less,Liu19a}, we evaluate paragraph-to-video retrieval, where all sentence descriptions for a video are concatenated into a single query. Since this dataset comes with localisation annotations (ground truth proposals), we report results with ground truth proposals (where only the localised moments in the video are concatenated and used in the retrieval set as done by~\cite{lei2021less}) as well as without (as done by~\cite{Liu19a}). \\

\noindent\textbf{LSMDC~\cite{Rohrbach_2015_CVPR}} consists of 118,081 video clips sourced from 202 movies. The validation set contains 7,408 clips and evaluation is done on a test set of 1,000 videos from movies disjoint from the train and val sets. This follows the protocol outlined in~\cite{rohrbach2017movie}.


For downstream datasets with separate \texttt{val} and \texttt{test} splits, we train all models for 75 epochs and use the epoch with the lowest validation loss for reporting test results. For downstream datasets without a \texttt{val} set we report results at 50 epochs.

\subsection{Implementation Details}
\label{subsec:implementation}
All experiments are conducted with PyTorch~\cite{NEURIPS2019_9015}. Optimization is performed with Adam, using a learning rate of , we use batch sizes of 16, 24, and 96 for 8, 4, and 1-frame inputs respectively. We use 8 frames per video when finetuning on downstream datasets. The temperature hyperparameter  for the loss defined in Eq.~\ref{eq:loss1} \&~\ref{eq:loss2} is set to 0.05.
For the visual encoder, all models have the following:  attention blocks, patch size , sequence dimension  and 12 heads.

The text encoder of all models, unless specified otherwise, is instantiated as DistilBERT base-uncased~\cite{distilbert} pretrained on English Wikipedia and Toronto Book Corpus. The dimensionality of the common text-video space is set to 256. For visual augmentation, we randomly crop and horizontally flip during training, and center crop the maximal square crop at test time. All videos are resized to  as input. At test-time we compute clip-embeddings for the video with a stride of 2 seconds.
For paragraph-retrieval settings, we employ text augmentation during training by randomly sampling and concatenating a variable number of corresponding captions per video.

\noindent\textbf{Finetuning time.}
A large motivation for using pre-extracted expert models for video retrieval is to save computational cost. Finetuning our 4-frame model for 50 epochs on MSR-VTT takes 10 hours on 2 Quadro RTX 6000k GPUs (with 24GB RAM each), which is similar to other works using \textit{pre-extracted expert features}~\cite{patrick2020support}. This shows that our model is lightweight and can be finetuned end-to-end on the downstream video datasets quickly with sufficient pretraining (which is of one-time cost).
\subsection{Ablation Study}
\label{subsec:ablation}

\begin{table}\centering
\caption{\textbf{Pretraining sources:} The effect of different pretraining sources. We use 4 frames per video in both pretraining and finetuning. Results are presented on the 1K-A MSR-VTT test set for text-video retrieval. \textbf{R@k:} Recall@K. \textbf{MedR:} Median Rank}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{lrrrrr}
\toprule
\textbf{Pre-training} & \textbf{\#pairs} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR}    \\
\midrule
-                   & -                   & 5.6     & 22.3     &  55  \\
ImageNet            &                     & 15.2    & 54.4     &  9.0     \\
HowTo-17M subset     & 17.1M                  & 24.1   & 63.9     & 5.0     \\
CC3M                & 3.0M                  & 24.5    & 62.7     & 5.0     \\
WebVid2M            & 2.5M                  & 26.0   & 64.9      & 5.0 \\
\textbf{CC3M + WebVid2M}    & 5.5M                   & \textbf{27.3}    & \textbf{68.1}         & \textbf{4.0} \\ \bottomrule
\end{tabular}
}
\label{tab:pretraining}
\end{table} 

In this section we study the effect of different pretraining strategies and the improvement when using our modified space-time attention block. In Section~\ref{sec:arch_ablations} of the Appendix, we provide architectural ablations on different temporal expansion methods, different visual backbones and different text backbones.

\noindent\textbf{Effect of pretraining.}
We compare performance on MSR-VTT with our model (i) trained from scratch, (ii) initialised with ImageNet weights and then 
finetuned, as well as (iii) initalised with ImageNet, and then pretrained on a number of different visual-text datasets before 
finetuning. For the video data, 4 frames are sampled at both pretraining and finetuning. Results on the MSR-VTT 1KA test set are shown in  Table~\ref{tab:pretraining}.
For HowTo100M, we pretrain on a random 17M subset due to computational constraints (the largest subset we could obtain at the time of writing) totalling 19K hours. To generate text-video pairs, we sample 5 contiguous speech-video pairs and concatenate them to form a longer video. This allows for robustness to the noisy alignment of speech and vision. We find that training on CC3M alone does reasonably well, outperforming the HowTo-17M subset. This demonstrates the benefit of our flexible encoder that can be cheaply trained on images and easily applied to videos. Training on WebVid2M also outperforms training on the HowTo17M subset, despite being much smaller, confirming that the HowTo100M dataset is noisy. The best performance is achieved by jointly training on both CC3M and WebVid2M, effectively exploiting image and video data. 

\noindent\textbf{Space-time attention.} Our modified space-time attention block, described in Section~\ref{subsec:architecture}, improves retrieval performance, as show in Table~\ref{tab:pret_attention}. We compare both variants during pretraining on WebVid-2M by reporting zero-shot results on MSR-VTT. We find once again that our modification leads to modest performance gains.


\begin{table}
\centering
\caption{\textbf{Space-time attention method:} Zero-shot results are presented on 1K-A MSR-VTT test set for text-video retrieval. The models were trained on WebVid-2M.}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Attention Method} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR}    \\
\midrule
Divided Space-Time~\cite{lei2021less} & 13.0    & 40.2      & 18.0  \\
Ours          & 14.6      & 42.7      & 16.0  \\
\bottomrule
\end{tabular}
\label{tab:pret_attention}
\end{table} 
\subsection{Curriculum strategy}
\label{subsec:curriculumexp}
Next, we evaluate the ability of our curriculum schedule
to gradually learn the temporal dimension of videos
by increasing the input number of frames.
Table~\ref{tab:curric} summarises the results.
Here, we show performance when pretraining on WebVid2M and
finetuning on MSR-VTT.
We explore two types of expansion in time:
at pretraining and at finetuning stages.
First, we observe that a single frame is not sufficient
to capture the video content (18.8 R@1).
Performing the temporal expansion at pretraining stage
is better than doing so at finetuning (26.0 vs 24.9 R@1 with 4 frames).
Finally, we obtain similar performance (slightly better at R@5) at half the computational
cost in GPU hours by employing a curriculum strategy at pretraining (26.6 R@1).
For 8 frames, the curriculum is even more useful, as we start training on 1 frame and then move to 4 before finally moving to 8 frames. Here, we obtain similar or better performance than training on 8 frames from the start, with almost a third of the computational cost. This is to be expected, as fewer frames significantly reduces forward pass times and enables larger batch sizes.
Note that for a fair comparison, we allow the same number of training
iterations for each row in the table.
\begin{table}
\centering
\caption{\textbf{Effect of \#frames and curriculum learning:} The effect of a different number of input frames at pretraining and finetuning.  indicates a within-dataset curriculum learning strategy. Results are presented on the 1K-A MSR-VTT test set for text-video retrieval. Pretraining here is done on WebVid2M only, with a total budget of one epoch through the entire dataset. \textbf{PTT:} total pretraining time in hours.}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}ccrrrr@{}}
\toprule
\textbf{PT \#frames}          & \textbf{FT \#frames} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR} & \textbf{PTT (hrs)} \\ \toprule
1                             & 1                    & 18.8         & 56.6          & 7.0 & 16.2          \\ \midrule
1                             & 4                    & 24.9         & 67.1          & 5.0 & 16.2           \\
4                             & 4                    & 26.0         & 64.9          & 5.0 & 45.6        \\
14               & 4                    & 26.6         & 65.5          & 5.0 & 22.1             \\ \midrule
8                             & 8                    & 25.4         & 67.3          & 4.0 & 98.0           \\
148 & 8                    & 27.4         & 67.3          & 4.0 & 36.0         \\ \bottomrule
\end{tabular}
}
\label{tab:curric}
\end{table}
 We further analyse our proposed temporal curriculum strategy and its effects on training time and accuracy. Figure~\ref{fig:curric} shows the zero-shot results on MSR-VTT for various checkpoints with and without curriculum. It shows that our curriculum method yields a significant training speedup with a gain in accuracy. Shorter frame models are able to pass through more of the dataset in a shorter amount of time, which can lead to significant performance benefits in a constrained setting.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/curric_plot.pdf}
    \caption{Plot showing the zero-shot performance (geometric mean of R@{1,5,10}) of various models on the MSR-VTT test set against their total training time in hours.  denotes a curriculum learning strategy.  denotes the multiple of dataset epochs completed.}
    \label{fig:curric}
\end{figure}

\begin{table*}[ht]
\centering
\caption{\label{tab:msr-vtt-sota}Comparison to state-of-the-art results on MSR-VTT for text-to-video retrieval. \textbf{E2E:} Works trained on pixels directly, without using pre-extracted expert features trained for other tasks. \textbf{Vis Enc. Init.:} Datasets used for pretraining visual encoders for tasks \textit{other than visual-text retrieval}, eg object classification. \textbf{Visual-Text PT:} Visual-text pretraining data. Rows highlighted in blue use additional modalities such as sound and speech from the MSR-VTT test videos.  Object, Motion, Face, Scene, Speech, OCR and Sound classification features.}
\begin{tabular}{@{}llllrrrrr@{}}
\toprule
\textbf{Method} & \textbf{E2E} & \textbf{Vis Enc. Init.} &\textbf{Visual-Text PT} & \textbf{\#pairs PT} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{MedR} \\ \midrule
JSFusion~\cite{yu2018joint} &  & - & -  & - &10.2 & 31.2 & 43.2 & 13.0   \\
HT MIL-NCE~\cite{miech2019howto100m} & & - &HowTo100M & 100M & 14.9 & 40.2 & 52.8 & 9.0   \\
ActBERT~\cite{zhu2020actbert} &  & VisGenome &HowTo100M & 100M & 16.3 & 42.8 & 56.9 & 10.0  \\
HERO~\cite{li2020hero} &  & ImageNet, Kinetics &HowTo100M & 100M & 16.8 & 43.4 & 57.7 & -  \\
VidTranslate~\cite{korbar2020video} &  & IG65M &HowTo100M & 100M & 14.7 & - & 52.8  \\
NoiseEstimation~\cite{amrani2020noise} & \xmark & ImageNet, Kinetics  &HowTo100M & 100M & 17.4 & 41.6 &  53.6 & 8.0  \\
\rowcolor{aliceblue} CE~\cite{Liu19a} & \xmark& Numerous experts &   -  & & 20.9 & 48.8 & 62.4 & 6.0\\
UniVL~\cite{luo2020univilm} & \xmark & - &HowTo100M & 100M & 21.2 & 49.6 & 63.1 & 6.0  \\ ClipBERT~\cite{lei2021less} & \checkmark & - &COCO, VisGenome & 5.6M & 22.0 & 46.8 & 59.9 & 6.0   \\ 
AVLnet~\cite{rouditchenko2020avlnet} & \xmark &ImageNet, Kinetics &HowTo100M & 100M & 27.1 & 55.6 & 66.6 & 4.0  \\
\rowcolor{aliceblue} MMT~\cite{gabeur2020multi} & \xmark&  Numerous experts &HowTo100M & 100M & 26.6 & 57.1 & 69.6 & 4.0 \\
Support Set~\cite{patrick2020support} & \xmark & IG65M, ImageNet& - &- &
27.4 & 56.3 & 67.7 & 3.0 \\
Support Set~\cite{patrick2020support} & \xmark& IG65M, ImageNet&HowTo100M & 100M & 30.1 & 58.5 &69.3 & \textbf{3.0}  \\
\textbf{Ours} &  \checkmark & ImageNet & CC3M & 3M & 25.5  & 54.5  & 66.1  & 4.0 \\ 
\textbf{Ours} &  \checkmark & ImageNet  & CC3M, WebVid-2M & 5.5M & \textbf{31.0} & \textbf{59.5} & \textbf{70.5} & \textbf{3.0} \\ 

\midrule
\textbf{Zero-shot} \\
\midrule
HT MIL-NCE~\cite{miech2019howto100m} & & - &HowTo100M & 100M & 7.5 & 21.2 & 29.6 & 38.0     \\
SupportSet~\cite{patrick2020support} &  & IG65M, ImageNet &HowTo100M & 100M & 8.7 & 23.0 & 31.1 & 31.0     \\
\textbf{Ours} & \checkmark & ImageNet & CC3M, WebVid-2M & 5.5M & \textbf{18.7} & \textbf{39.5} & \textbf{51.6} & \textbf{10.0} \\
\bottomrule

\end{tabular}
\end{table*}
 
\noindent\textbf{Expansion of temporal embeddings.}
We experiment with both zero padding and interpolation, and find that our model is robust to the type of temporal expansion strategy. More detailed results are provided in Section~\ref{sec:temp_expansion}.


\subsection{Comparison to the State of the Art}
\label{subsec:sota}
Results on MSR-VTT can be seen in Table~\ref{tab:msr-vtt-sota}.
We outperform all previous works, including many that pretrain on HowTo100M which is an order of magnitude larger than our pretraining dataset both in the number of hours (135K vs 13K) and in the number of caption-clip pairs (136M vs 5.5M). We also note that we outperform works that extract expert features (CE uses 9 experts, MMT uses 7) including object, motion, face, scene, sound and speech embeddings. We even outperform/perform on par with Support Set~\cite{patrick2020support}, which uses expert features from a 34-layer, R(2+1)-D model pretrained on IG65M, concatenated with ImageNet ResNet152 features, after which they add a transformer network and train end-to-end on HowTo100M. 

We also report zero-shot results (Table~\ref{tab:msr-vtt-sota}) with no finetuning on MSR-VTT, outperforming both MIL-NCE and Support Set that trains on HowTo100M. This shows that our model is more generalisable, and can be used out of the box, and also perhaps that the domain of WebVid-2M is closer to that of MSR-VTT than HowTo100M. We will release the weights of our models publicly.

For MSVD~\cite{chen2011collecting}, we outperform all previous methods (Table ~\ref{tab:msvd-sota}). In particular, we outperform Support Set~\cite{patrick2020support} even though they train on an order of magnitude more data.
\begin{table}
\centering
\caption{Text-to-video retrieval results on the MSVD~\cite{chen2011collecting} test set.}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method}  & \textbf{R@1}  & \textbf{R@5}  & \textbf{R@10} & \textbf{MedR} \\ \midrule
VSE~\cite{kiros2014unifying}             & 12.3          & 30.1          & 42.3          & 14.0          \\
VSE++~\cite{faghri2017vse++}              & 15.4          & 39.6          & 53.0          & 9.0           \\
Multi. Cues~\cite{mithun2018learning}            & 20.3          & 47.8          & 61.1          & 6.0           \\
CE~\cite{Liu19a}                   & 19.8          & 49.0          & 63.8          & 6.0           \\
Support Set~\cite{patrick2020support}           & 23.0          & 52.8          & 65.8          & 5.0           \\
Support Set~\cite{patrick2020support} (HowTo PT)   & 28.4          & 60.0          & 72.9          & 4.0           \\
\textbf{Ours}    & \textbf{33.7} & \textbf{64.7} & \textbf{76.3} & \textbf{3.0}  \\ \bottomrule
\end{tabular}
}
\label{tab:msvd-sota}
\end{table} 
Results on DiDeMo can be found in Table~\ref{tab:didemo-sota}. Note that on this dataset, our zero-shot performance is equivalent to CLIPBERT's results with finetuning, and after we finetune our model on the DiDeMo training set we get an additional 14.2\% boost in R@1.
\begin{table}\centering
\caption{Text-to-video retrieval results on the DiDeMo test set. We show results with and without ground truth proposals (GT prop.) as well as with finetuning and without (zero-shot).}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}llrrrr@{}}
\toprule
\textbf{Method} & \textbf{GT prop.} & \textbf{R@1}  & \textbf{R@5}  & \textbf{R@10} & \textbf{MedR} \\ \midrule
S2VT~\cite{venugopalan2014translating}            &          & 11.9          & 33.6          & -             & 13.0          \\   
FSE~\cite{zhang2018cross}             &          & 13.9          & 36.0          & -             & 11.0          \\
CE~\cite{Liu19a}&                    & 16.1          & 41.1          & -             & 8.3           \\
ClipBERT~\cite{lei2021less}        & \checkmark         & 20.4          & 44.5          & 56.7          & 7.0           \\
\textbf{Ours}   & \textbf{}          & \textbf{31.0} & \textbf{59.8} & \textbf{72.4} & \textbf{3.0}    \\
\textbf{Ours}   & \checkmark          & \textbf{34.6} & \textbf{65.0} & \textbf{74.7} & \textbf{3.0}  \\  
\midrule 
\textbf{Zero-shot} \\
\midrule 

\textbf{Ours}   &          & 21.1 & 46.0 & 56.2 & 7.0 \\
\textbf{Ours}   & \checkmark          & 20.2 & 46.4 & 58.5 &  7.0

\\\bottomrule
\end{tabular}
}

\label{tab:didemo-sota}
\end{table} Finally, we also show results on LSMDC in Table~\ref{tab:lsmdc-sota}, in which we outperform all methods except for MMT in Median Rank, which pretrained on HowTo100M and uses multiple experts including audio modalities. Our model uses visual information alone.

\begin{table}
\centering
\caption{Text-to-video retrieval results on the LSMDC test set.}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method}  & \textbf{R@1}  & \textbf{R@5}  & \textbf{R@10} & \textbf{MedR} \\ \midrule
JSFusion~\cite{yu2018joint}     & 9.1          & 21.2          & 34.1          & 36.0  \\
MEE~\cite{miech18learning}      & 9.3          & 25.1          & 33.4          & 27.0  \\
CE~\cite{Liu19a}                & 11.2          & 26.9          & 34.8          & 25.3  \\
MMT (HowTo100M)~\cite{gabeur2020multi}                & 12.9          & 29.2          & 38.8          & \textbf{19.3}  \\
\textbf{Ours}                   & \textbf{15.0} & \textbf{30.8} & \textbf{39.8} & 20.0  \\ \bottomrule
\end{tabular}
}
\label{tab:lsmdc-sota}
\end{table} 

\label{subsec:qualitative}

 \section{Conclusion}
\label{sec:conc}
To conclude, we introduce a dual encoder model for end-to-end training of text-video retrieval, designed to take advantage of both large-scale image and video captioning datasets. Our model achieves state-of-the-art performance on a number of downstream benchmarks, however we note that the performance of our model is not saturated yet, and performance could be further improved by training on the full HowTo100M dataset, larger weakly paired image datasets such as Google3BN~\cite{jia2021scaling}, as well as multi-dataset combinations thereof. 
\noindent \textbf{Acknowledgements.}
The authors would like to thank Samuel Albanie for his useful feedback. We are grateful for funding from a Royal Society Research Professorship, a Nielsen studentship, and a Google PhD Fellowship.

{\small\bibliographystyle{ieee_fullname}\bibliography{shortstrings,vgg_local,references}}
\clearpage
\appendix
\addcontentsline{toc}{section}{Appendices}

\part{Appendix} 
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0} 
\renewcommand{\thetable}{A.\arabic{table}}
\setcounter{table}{0} 

{
\hypersetup{linkcolor=black}
\parttoc 
}

\bigskip

\section{Text-to-Image Retrieval}

We evaluate on a text-to-image retrieval benchmark to demonstrate the versatility of our model in that it can be used to achieve competitive performance in image settings as well as state of the art in video retrieval. The Flickr30K~\cite{young-etal-2014-image} dataset contains 31,783 images with 5 captions per image. We follow the standard protocol of 1,000 images for validation, 1,000 images for testing and the remaining for training.
We report the results in Table~\ref{tab:flickr}.
Unlike other works~\cite{lee2018stacked,chen2020imram,diao2021similarity} which utilise high resolution regions extracted using a Faster-RCNN detector, our model is single stage and does not require any object detections. We compare to works with a similar number of training image-text pairs, and find that our model is comparable. We also note that training on WebVid2M provides a sizeable boost (5\% improvement in R@1). Note that there are other recent text-image works such as UNITER~\cite{chen2020uniter} and OSCAR~\cite{li2020oscar}, however these are trained on almost twice the number of samples. Recent works scale this up even further to billions of samples (ALIGN~\cite{jia2021scaling}). 
\begin{table}\centering
\caption{Text-to-\textbf{image} retrieval results on the Flickr30K test set. ++ indicates additional datasets: COCO Captions, SBU Captions. VisGenObjects denotes Visual Genome object bounding box annotations used to pretrain an FRCNN object feature extractor.} 
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method} & \multicolumn{1}{l}{\textbf{Vis PT. size}} & \multicolumn{1}{l}{\textbf{R@1}} & \multicolumn{1}{l}{\textbf{R@5}} & \multicolumn{1}{l}{\textbf{R@10}} \\ \midrule
SCANM~\cite{lee2018stacked}           & VisGenObj (3.8M)                             & 48.6                             & 77.7                             & 85.2                              \\
IMRAM~\cite{chen2020imram}           & VisGenObj (3.8M)                             & 53.9                             & 79.4                             & 87.2                              \\
SGRAF~\cite{diao2021similarity}           & VisGenObj (3.8M)                             & 58.5                             & 83.0                             & 88.8                              \\
Ours            & CC (3.0M)                                 &               54.2   &                             83.2     &        89.8                           \\
Ours            & CC,WebVid2M (5.5M)                      &                61.0           &                      87.5 &             92.7                      \\ \bottomrule
\end{tabular}
}
\label{tab:flickr}
\end{table}

%
 
\section{Architectural Details}

\subsection{Video Encoder}
The video encoder is composed of: (i) the patch embedding layer; (ii) learnable positional space, time and [CLS] embeddings; and (iii) a stack of  space-time attention blocks.
\begin{enumerate}
    \item The patch embedding layer is implemented as a 2D convolutional layer with a kernel and stride size equivalent to the target patch size , and  output channels (the chosen embedding dimensionality of the video encoder).
    \item The positional space and time embeddings are instantiated with shape  and  respectively, where  is the maximum number of input video frames and  is the maximum number of non-overlapping patches of size  within a frame (196 for a video resolution of ). The [CLS] embedding is instantiated with shape .
    \item Each space-time attention block consists of normalisation layers, temporal and spatial self-attention layers, and an MLP. The order and connections of these layers is shown in Figure~\ref{fig:atten_block_indepth}.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{figures/atten_block_indepth.pdf}
    \caption{Detailed diagram of the space-time self attention block.}
    \label{fig:atten_block_indepth}
\end{figure}


\subsection{Text Encoder}

Our text encoder is instantiated as \texttt{distilbert-base-uncased}~\cite{distilbert}. Distilbert follows the same general architecture as BERT~\cite{devlin2019bert}, but with the number of layers reduced by a factor of 2 and the token-type embeddings and the pooler removed. We use the HuggingFace\footnote{\url{https://huggingface.co/}} transformers library implementation.


\section{Architectural Ablations}
\label{sec:arch_ablations}
\subsection{Video Backbone}
We investigate the effects of using different video backbone architectures (Table~\ref{tab:vidbackbone}) and find that the space-time transformer encoder leads to large improvements in performance on MSR-VTT when compared to ResNets and 3D variants thereof. During testing, all frame-variants see an equal number of frames, since the video embeddings are averaged over multiple strides.

For the video backbone ablation, we fix the text backbone to \texttt{distilbert-base-uncased}. For the text backbone ablation, we fix the video backbone to the base space-time transformer with an input resolution of 224 and a patch size .
\begin{table}[h]
\centering
\caption{\textbf{Video backbone.} Text-to-video retrieval results on MSR-VTT test set with different video backbones. All models were pretrained on WebVid-2M and finetuned on MSR-VTT train set. 4 frames were given as input, except for the ResNet-101 which only supports image (1-frame) inputs. The text backbone is fixed to distilbert-base-uncased.}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Video Backbone}   & \textbf{\#params} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR} \\ \midrule
ResNet-101                &     45M &         11.5 &          44.1 & 14.5          \\
S3D-G                     &     76M &          3.6 &          20.4 & 59.5          \\
R(3D)-101                 &     85M &          9.3 &         38.3 &  20.0       \\
S-Tformer  B    &    114M &\textbf{26.8} & \textbf{68.2} & \textbf{4.0}        \\
\bottomrule
\end{tabular}
\label{tab:vidbackbone}
\end{table} 
\subsection{Text Backbone}
The choice of text backbone has a significant impact on downstream performance (see Table~\ref{tab:txtbackbone}), while~\cite{patrick2020support} use t5 text backbones, we find that they perform significantly worse than bert models with either a greater or similar number of parameters. DistilBERT and normal BERT achieve similar performance, with DistilBERT having far fewer parameters, therefore we chose to use DistilBERT in our work for efficiency.
\begin{table}\centering
\caption{\textbf{Text backbone.} Text-to-video retrieval results on MSR-VTT test set with different text backbones. All models were pretrained on WebVid-2M and finetuned on MSR-VTT train set. The video backbone is fixed to the base space-time transformer with an input resolution of 224 and a patch size .}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Text Backbone} & \textbf{\#params} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR} \\ \midrule
t5-small               &    60.5M &         15.1 &          51.4 &          10.0     \\
t5-base                &   222.9M &         24.0 &          62.8 &          6.0     \\
distilbert-base-uncased  &    66.4M & 26.8 & \textbf{68.2} & \textbf{4.0}  \\
bert-base-uncased        &   109.5M &  \textbf{27.5} &  67.3 & \textbf{4.0} \\

\bottomrule
\end{tabular}
}
\label{tab:txtbackbone}
\end{table} 
\subsection{Temporal Expansion}
\label{sec:temp_expansion}
\begin{table}[ht]
\centering
\caption{\textbf{Temporal expansion method}. The effect of different expansion methods increasing the input number of frames from 48. Results are presented on 1K-A MSR-VTT test set for text-video retrieval. The models were pre-trained on CC3M \& WebVid-2M and finetuned on MSR-VTT train set.}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Method} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR}    \\
\midrule
Zero-pad           & \textbf{30.7}       & 68.3      & 4.0   \\
Nearest Neighbour            & 29.4       & 69.5      & 4.0   \\
Bilinear           & 28.3    & \textbf{69.9}      & 4.0 \\
\bottomrule
\end{tabular}
\label{tab:temporalexp}
\end{table} We explore 3 different methods for expanding temporal positional embeddings (zero-padding and two interpolation methods), and observe robustness to all 3 (see Table \ref{tab:temporalexp}).

\section{Pretraining on Other Datasets}
In Table~\ref{tab:mo_pretraining}, we restrict the pretraining of our model to COCO Captions, a dataset with only 600k image-text pairs. We demonstrate that we are able to achieve generally competitive performance on MSR-VTT. We outperform ClipBERT -- which trains on both COCO Captions and Visual Genome (totalling 5.6M image-text pairs) -- by several percentage points, demonstrating the strength of our proposed architecture.
\begin{table}\centering
\caption{\textbf{Pretraining sources extended:} The effect of different  other pretraining sources. We use 4 frames per video when finetuning. Results are presented on the 1K-A MSR-VTT test set for text-video retrieval.}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{llrrrrr}
\toprule
\textbf{Method} & \textbf{Pre-training} & \textbf{\#pairs} & \textbf{R@1} & \textbf{R@10} & \textbf{MedR}    \\
\midrule
ClipBERT & COCO, VisGen          & 5.6M                  & 22.0   & 59.9      & 6.0 \\
\textbf{Ours} & COCO                & 0.6M                  & 25.5   & 64.6      & 5.0 

\\\bottomrule
\end{tabular}
}
\label{tab:mo_pretraining}
\end{table} 

\section{WebVid-2M Dataset Details}
In this section, we show further details of the new WebVid-2M dataset. Histograms of caption lengths and video durations can be found in Figure~\ref{fig:dataset_hist}. More qualitative examples of video-text pairs can be found in Figure~\ref{fig:dataset_qual}.


\begin{figure*}[t]
\captionsetup[subfigure]{labelformat=empty}
     \centering
     \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/truck.jpg}
         \caption{1990s: man driving excavator, rotates seat, opens windows in cab. hand presses lever.}
     \end{subfigure} \hfill
     \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/pancakes.jpg}
         \caption{\footnotesize{Frying pancakes in the kitchen at home. a woman is cooking traditional russian pancakes. modern kitchen, skillet and batter.}}
         \label{fig:threesinx1}
     \end{subfigure} \hfill
     \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/skyline.jpg}
         \caption{Twilight zhuhai famous mountain park top cityscape aerial panorama 4k timelapse china}
     \end{subfigure} \hfill
          \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/suitcase.jpg}
         \caption{A child with a suitcase. a happy little girl sits on a suitcase with a passport and money.}
     \end{subfigure} \hfill 
     \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/concert.jpg}
         \caption{\tiny{Kherson, ukraine - 20 may 2016: open, free, rock music festival crowd partying at a rock concert. hands up, people, fans cheering clapping applauding in kherson, ukraine - 20 may 2016. band performing'}}
         \label{fig:threesinx2}
     \end{subfigure} \hfill
     \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/cuckatoo.jpg}
         \caption{Cockatoos on the fence}
     \end{subfigure} \hfill
          \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/running.jpg}
         \caption{Runners feet in a sneakers close up. realistic three dimensional animation.}
     \end{subfigure} \hfill
     \begin{subfigure}[t]{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/shuttervids/snow.jpg}
         \caption{Ontario, canada january 2014 heavy pretty snow on tree branches}
         \label{fig:threesinx3}
     \end{subfigure} \hfill
        \caption{\textbf{WebVid-2M dataset examples:} We provide additional
        examples from our dataset by showing video-text pairs, using video thumbnails.}
        \label{fig:dataset_qual}
\end{figure*} 
\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/duo_lens.pdf}
    \caption{\textbf{WebVid-2M dataset statistics:} We report the histogram of video duration in seconds \textbf{(left)} and
    the histogram of caption length in words \textbf{(right)}.}
    \label{fig:dataset_hist}
\end{figure*} 

\end{document}
