\begin{filecontents*}{example.eps}
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
\RequirePackage{fix-cm}
\documentclass[twocolumn]{svjour3}          \smartqed  

\usepackage{graphicx}
\usepackage{cite}
\usepackage{authblk}
\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{hhline}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{url}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\norm}[1]{\| #1 \|}

\begin{document}

\title{EfficientPose: Scalable single-person pose estimation}
\titlerunning{EfficientPose}   

\author{Daniel Groos\textsuperscript{1} \and Heri Ramampiaro\textsuperscript{2} \and Espen A. F. Ihlen\textsuperscript{1}}
\authorrunning{Groos et al.}

\institute{
    Daniel Groos\\
    \quad \quad daniel.groos@ntnu.no\\\\
    Heri Ramampiaro\\
    \quad \quad heri@ntnu.no \\\\
    Espen A. F. Ihlen\\
    \quad \quad espen.ihlen@ntnu.no \\\\
\textsuperscript{1} Department of Neuromedicine and Movement Science, Norwegian University of Science and Technology, Trondheim, Norway\\\\
\textsuperscript{2} Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway\\
}



    \date{Received: date / Accepted: date}
\maketitle
\begin{abstract}
        \hfill\break
        Single-person human pose estimation facilitates markerless movement analysis in sports, as well as in clinical applications. Still, state-of-the-art models for human pose estimation generally do not meet the requirements of real-life applications. The proliferation of deep learning techniques has resulted in the development of many advanced approaches. However, with the progresses in the field, more complex and inefficient models have also been introduced, which have caused tremendous increases in computational demands. To cope with these complexity and inefficiency challenges, we propose a novel convolutional neural network architecture, called EfficientPose, which exploits recently proposed EfficientNets in order to deliver efficient and scalable single-person pose estimation. EfficientPose is a family of models harnessing an effective multi-scale feature extractor and computationally efficient detection blocks using mobile inverted bottleneck convolutions, while at the same time ensuring that the precision of the pose configurations is still improved. Due to its low complexity and efficiency, EfficientPose enables real-world applications on edge devices by limiting the memory footprint and computational cost. The results from our experiments, using the challenging MPII single-person benchmark, show that the proposed EfficientPose models substantially outperform the widely-used OpenPose model both in terms of accuracy and computational efficiency. In particular, our top-performing model achieves state-of-the-art accuracy on single-person MPII, with low-complexity ConvNets.
        \keywords{Human pose estimation \and Model scalability \and High precision \and Computational efficiency \and Openly available}
\end{abstract}



\section{Introduction}
\label{intro}

Single-person human pose estimation (HPE) refers to the computer vision task of localizing human skeletal keypoints of a person from an image or video frames. Single-person HPE has many real-world applications, ranging from outdoor activity recognition and computer animation to clinical assessments of motor repertoire and skill practice among professional athletes. The proliferation of deep convolutional neural networks (ConvNets) has advanced HPE and further widen its application areas. ConvNet-based HPE with its increasingly complex network structures, combined with transfer learning, is a very challenging task. However, the availability of high-performing ImageNet~\cite{deng2009imagenet} backbones, together with large tailor-made datasets, such as MPII for 2D pose estimation~\cite{andriluka14cvpr}, has facilitated the development of new improved methods to address the challenges.

An increasing trend in computer vision has driven towards more efficient models~\cite{sandler2018mobilenetv2, tan2019mnasnet, elsen2019fast}. Recently, EfficientNet~\cite{tan2019efficientnet} was released as a scalable ConvNet architecture, setting benchmark record on ImageNet with a more computationally efficient architecture. However, within human pose estimation, there is still a lack of architectures that are both accurate and computationally efficient at the same time. In general, current state-of-the-art architectures are computationally expensive and highly complex, thus making them hard to replicate, cumbersome to optimize, and impractical to embed into real-world applications. 

The OpenPose network~\cite{cao2018openpose} (OpenPose for short) has been one of the most applied HPE methods in real-world applications. It is also the first open-source real-time system for HPE. OpenPose was originally developed for multi-person HPE, but has in recent years been frequently applied to various single-person applications within clinical research and sport sciences~\cite{nakai2018prediction, noori2019robust, firdaus2019recognizing}. The main drawback with OpenPose is that the level of detail in keypoint estimates is limited due to its low-resolution outputs. This makes OpenPose less suitable for precision-demanding applications, such as elite sports and medical assessments, which all depend on high degree of precision in the assessment of movement kinematics. Moreover, by spending  billion floating-point operations (GFLOPs) per inference, OpenPose is considered highly inefficient. Despite these issues, OpenPose seems to remain a commonly applied network for single-person HPE performing markerless motion capture from which critical decisions are based upon~\cite{vitali2019new, barra2019gait}. 

In this paper, we stress the lack of publicly available methods for single-person HPE that are both computationally efficient and effective in terms of estimation precision. To this end, we exploit recent advances in ConvNets and propose an improved approach called EfficientPose. Our main idea is to modify OpenPose into a family of scalable ConvNets for high-precision and computationally efficient single-person pose estimation from 2D images. To assess the performance of our approach, we perform two separate comparative studies. First, we evaluate the EfficientPose model by comparing it against the original OpenPose model on single-person HPE. Second, we compare it against the current state-of-the-art single-person HPE methods on the official MPII challenge, focusing on accuracy as a function of the number of parameters. The proposed EfficientPose models aim to elicit high computational efficiency, while bridging the gap in availability of high-precision HPE networks.

In summary, the main contributions of this paper are the following: 
\begin{itemize}
 \item We propose an improvement of OpenPose, called EfficientPose, that can overcome the shortcomings of the popular OpenPose network on single-person HPE with improved level of precision, rapid convergence during optimization, low number of parameters, and low computational cost.
 \item With EfficientPose, we suggest an approach providing scalable models that can suit various demands, enabling a trade-off between accuracy and efficiency across diverse application constraints and limited computational budgets.
 \item We propose a new way to incorporate mobile ConvNet components, which can address the need for computationally efficient architectures for HPE, thus facilitating real-time HPE on the edge.
 \item We perform an extensive comparative study to evaluate our approach. Our experimental results show that the proposed method achieves significantly higher efficiency and accuracy in comparison to the baseline method, OpenPose. In addition, compared to existing state-of-the-art methods, it achieves competitive results, with a much smaller number of parameters.
\end{itemize}

The remainder of this paper is organized as follows: Section~\ref{sec:related} describes the architecture of OpenPose and highlights research which it can be improved from. Based on this, Section~\ref{sec:efficientpose} presents our proposed ConvNet-based approach, EfficientPose. Section~\ref{sec:experiments} describes our experiments and presents the results from comparing EfficientPose with OpenPose and other existing approaches. Section~\ref{sec:discussion} discusses our findings and suggests potential future studies. Finally, Section~\ref{sec:conclusion} summarizes and concludes the paper.

For the sake of reproducibility, we will make the EfficientPose models available at \url{https://github.com/daniegr/EfficientPose}.

\section{Related work}
\label{sec:related}

The proliferation of ConvNets for HPE following the success of DeepPose~\cite{toshev2014deeppose} has set the path for accurate HPE. With OpenPose, Cao et al.~\cite{cao2018openpose} made HPE available to the public. As depicted by Figure~\ref{fig:openpose}, OpenPose comprises a multi-stage architecture performing a series of detection passes. Provided an input image of  pixels, OpenPose utilizes an ImageNet pretrained VGG-19 backbone~\cite{simonyan2014very} to extract basic features (step 1 in Figure~\ref{fig:openpose}). The features are supplied to a DenseNet-inspired detection block (step 2) arranged as five dense blocks~\cite{huang2017densely}, each containing three  convolutions with PReLU activations~\cite{he2015delving}. The detection blocks are stacked in a sequence. First, four passes (step 3a-d in Figure~\ref{fig:openpose}) of part affinity fields~\cite{cao2017realtime} map the associations between body keypoints. Subsequently, two detection passes (step 3e and 3f) predict keypoint heatmaps~\cite{tompson2014joint} to obtain refined keypoint coordinate estimates. In terms of level of detail in the keypoint coordinates, OpenPose is restricted by its output resolution of  pixels.

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{Fig1.png}
\caption{OpenPose architecture utilizing 1) VGG-19 feature extractor, and 2) 4+2 passes of detection blocks performing 4+2 passes of estimating part affinity fields (3a-d) and confidence maps (3e and 3f)}
\label{fig:openpose}       
\end{center}
\end{figure*}
   
The OpenPose architecture can be improved by recent advancements in ConvNets, as follows: First, automated network architecture search has found backbones~\cite{tan2019efficientnet, zoph2018learning, tan2019mixconv} that are more precise and efficient in image classification than VGG and ResNets~\cite{simonyan2014very, he2016deep}. In particular, Tan and Le~\cite{tan2019efficientnet} proposed compound model scaling to balance the image resolution, width (number of network channels), and depth (number of network layers). This resulted in scalable convolutional neural networks, called EfficientNets~\cite{tan2019efficientnet}, with which the main goal was to provide lightweight models with a sensible trade-off between model complexity and accuracy across various computational budgets. For each model variant EfficientNet-B, from the least computationally expensive one being EfficientNet-B0 to the most accurate model, EfficientNet-B7 (), the total number of FLOPs increases by a factor of , given by

Here, ,  and  denote the coefficients for depth, width, and resolution, respectively, and are set as

Second, parallel multi-scale feature extraction has improved the precision levels in HPE~\cite{newell2016stacked, ke2018multi, sun2019deep, yang2017learning}, emphasizing both high spatial resolution and low-scale semantics. However, existing multi-scale approaches in HPE are computationally expensive, both due to their large size and high computational requirements. For example, a typical multi-scale HPE approach has often a size of  million parameters and requires  GFLOPS~\cite{sun2019deep, chu2017multi, zhang2019human, tang2018deeply, yang2017learning, newell2016stacked, rafi2016efficient}. To cope with this, we propose cross-resolution features, operating on high- and low-resolution input images, to integrate features from multiple abstraction levels with low overhead in network complexity and with high computational efficiency. Existing works on Siamese ConvNets have been promising in utilizing parallel network backbones~\cite{gao2019siamese, gao2020learning}. 
Third, mobile inverted bottleneck convolution (MBConv)~\cite{sandler2018mobilenetv2} with built-in squeeze-and-excitation (SE)~\cite{hu2018squeeze} and Swish activation~\cite{DBLP:conf/iclr/RamachandranZL18} integrated in EfficientNets has proven more accurate in image classification tasks~\cite{tan2019efficientnet, tan2019mixconv} than regular convolutions~\cite{he2016deep, huang2017densely, szegedy2017inception}, while substantially reducing the computational costs~\cite{tan2019efficientnet}. The efficiency of MBConv modules stem from the depthwise convolutions operating in a channel-wise manner~\cite{sifre2014rigid}. With this approach, it is possible to reduce the computational cost by a factor proportional to the number of channels~\cite{tan2019mixconv}.
Hence, by replacing the regular  convolutions with up to  input channels in the detection blocks of OpenPose with MBConvs, we can obtain more computationally efficient detection blocks. Further, SE selectively emphasizes discriminative image features~\cite{hu2018squeeze}, which may reduce the required number of convolutions and detection passes by providing a global perspective on the estimation task at all times. Using MBConv with SE may have the potential to decrease the number of dense blocks in OpenPose. Fourth, transposed convolutions with bilinear kernel~\cite{long2015fully} scale up the low-resolution feature maps, thus enabling a higher level of detail in the output confidence maps. 

By building upon the work of Tan and Le~\cite{tan2019efficientnet}, we present a pool of scalable models for single-person HPE that is able to overcome the shortcomings of the commonly adopted OpenPose architecture. This enables trading off between accuracy and efficiency across different computational budgets in real-world applications. The main advantage of this is that we can use ConvNets that are small and computationally efficient enough to run on edge devices with little memory and low processing power, which is impossible with OpenPose. 

\section{The EfficientPose approach}
\label{sec:efficientpose}

In this section, we explain in details the EfficientPose approach. This includes a detailed description of the EfficientPose architecture in light of the OpenPose architecture, and a brief introduction to the proposed variants of EfficientPose.

\subsection{Architecture}
\label{sec:architecture}
Figure~\ref{fig:openpose} and Figure~\ref{fig:efficientpose} depict the architectures of OpenPose and EfficientPose, respectively. As can be observed in these two figures, although being based on OpenPose, the EfficientPose architecture is different from the OpenPose architecture in several aspects, including 1) both high and low-resolution input images, 2) scalable EfficientNet backbones, 3) cross-resolution features, 4) and 5) scalable Mobile DenseNet detection blocks in fewer detection passes, and 6) bilinear upscaling. For a more thorough component analysis of EfficientPose, see Appendix~\ref{sec:ablation}.

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{Fig2.png}
\caption{Proposed architecture comprising 1a) high-resolution and 1b) low-resolution inputs, 2a) high-level and 2b) low-level EfficientNet backbones combined into 3) cross-resolution features, 4) Mobile DenseNet detection blocks, 1+2 passes for estimation of part affinity fields (5a) and confidence maps (5b and 5c), and 6) bilinear upscaling}
\label{fig:efficientpose}     
\end{center}
\end{figure*}

The input of the network consists of high and low-resolution images (1a and 1b in Figure~\ref{fig:efficientpose}). To get the low-resolution image, the high-resolution image is downsampled into half of its pixel height and width, through an initial average pooling layer. 

The feature extractor of EfficientPose is composed of the initial blocks of EfficientNets~\cite{tan2019efficientnet} pretrained on ImageNet (step 2a and 2b in Figure~\ref{fig:efficientpose}). High-level semantic information is obtained from the high-resolution image using the initial three blocks of a high-scale EfficientNet with  (see Equation~\ref{eq:scaling}), outputting C feature maps (2a in Figure~\ref{fig:efficientpose}). Low-level local information is extracted from the low-resolution image by the first two blocks of a lower-scale EfficientNet-backbone (2b in Figure~\ref{fig:efficientpose}) in the range . Table~\ref{tab:efficientnets} provides an overview of the composition of EfficientNet backbones, from low-scale B0 to high-scale B7. The first block of EfficientNets utilizes the MBConvs shown in Figure~\ref{fig:mbconvs}a and~\ref{fig:mbconvs}b, whereas the second and third blocks comprise the MBConv layers in Figure~\ref{fig:mbconvs}c and~\ref{fig:mbconvs}d. 

\begin{table*}[htbp]
\centering
\small
\setlength\tabcolsep{2pt}
\caption{The architecture of the initial three blocks of relevant EfficientNet backbones. For ,  denotes filter size,  is number of output feature maps, and  is stride.  denotes batch normalization.  defines input size, corresponding with image resolution on ImageNet, whereas  refers to the depth factor as determined by Equation~\ref{eq:scaling}}
\label{tab:efficientnets} 
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Block}                                                 & \textbf{B0}                                                     & \textbf{B1}                                                             & \textbf{B2}                    & \textbf{B3}                                                             & \textbf{B4}                                                             & \textbf{B5}                                                             & \textbf{B7}                                                             \\ \hline
                                             & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \\ \end{tabular}}                                                                                 & \begin{tabular}[c]{@{}c@{}}\\ \\ \end{tabular}   & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \\ \end{tabular}}                                                        & \begin{tabular}[c]{@{}c@{}}\\ \\ \end{tabular}   \\ \cline{2-8} 
                                                               & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                                                                                        & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                                                                                                                                         & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular}          \\ \cline{2-8} 
                                                               &                                                                & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                     & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                                                              & \begin{tabular}[c]{@{}c@{}}
                                                               \end{tabular} & \begin{tabular}[c]{@{}c@{}}\end{tabular} \\ \hline
                                             & \multicolumn{3}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                                                                                        & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                                                               & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular}          & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular}          \\ \cline{2-8} 
                                                               & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\end{tabular}}             & \begin{tabular}[c]{@{}c@{}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\end{tabular} \\ \hline
                                             & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                                                       & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\\ \end{tabular}}                      & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular}          & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular}          & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular}          \\ \cline{2-8} 
                                                               & \begin{tabular}[c]{@{}c@{}}\\ \end{tabular} & \begin{tabular}[c]{@{}c@{}}\end{tabular} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\end{tabular}}             & \begin{tabular}[c]{@{}c@{}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\end{tabular} \\ \hhline{|=|=|=|=|=|=|=|=|}
                                                     &                                                        &                                                                &                       &                                                                &                                                                &                                                                &                                                                \\ \hline
                                                     & \multicolumn{2}{c|}{}                                                                                                                   & \multicolumn{2}{c|}{}                                                                                  &                                                                       &                                                                       &                                                                       \\ \hline
 &                                   &                                            &   &                                            &                                            &                                           &                                            \\ \hline
\end{tabular}
\end{table*}

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{Fig3.png}
\caption{The composition of MBConvs. From left: a-d)  in EfficientNets performs depthwise convolution with filter size  and stride , and outputs  feature maps.  (b and d) extends regular MBConvs by including dropout layer and skip connection. e)  in Mobile DenseNets adjusts  with E-swish activation and number of feature maps in expansion phase as . All MBConvs take as input  feature maps with spatial height and width of  and , respectively.  is the reduction ratio of SE}
\label{fig:mbconvs}
\end{center}
\end{figure*}

The features generated by the low-level and high-level EfficientNet backbones are concatenated to yield cross-resolution features (step 3 in Figure~\ref{fig:efficientpose}). This enables the EfficientPose architecture to selectively emphasize important local factors from the image of interest and the overall structures that guide high-quality pose estimation. In this way, we enable an alternative simultaneous handling of different features at multiple abstraction levels. 

From the extracted features, the desired keypoints are localized through an iterative detection process, where each detection pass performs supervised prediction of output maps. Each detection pass comprises a detection block and a single  convolution for output prediction. The detection blocks across all detection passes elicit the same basic architecture, comprising Mobile DenseNets (see step 4 in Figure~\ref{fig:efficientpose}). Data from Mobile DenseNets are forwarded to subsequent layers of the detection block using residual connections. The Mobile DenseNet is inspired by DenseNets~\cite{huang2017densely} supporting reuse of features, avoiding redundant layers, and MBConv with SE, thus enabling low memory footprint. In our adaptation of the MBConv operation ( in Figure~\ref{fig:mbconvs}e), we consistently utilize the highest performing combination from~\cite{tan2019mnasnet}, i.e., a kernel size () of  and an expansion ratio of . We also avoid downsampling (i.e., ) and scale the width of Mobile DenseNets by outputting number of channels relative to the high-level backbone (). We modify the original  operation by incorporating E-swish as activation function with  value of ~\cite{gagana2018activation}. This has a tendency to accelerate progression during training compared to the regular Swish activation~\cite{DBLP:conf/iclr/RamachandranZL18}. We also adjust the first  convolution to generate a number of feature maps relative to the output feature maps  rather than the input channels . This reduces the memory consumption and computational latency since , with . With each Mobile DenseNet consisting of three consecutive  operations, the module outputs  feature maps. 

EfficientPose performs detection in two rounds (step 5a-c in Figure~\ref{fig:efficientpose}). First, the overall pose of the person is anticipated through a single pass of skeleton estimation (5a). This aims to facilitate the detection of feasible poses and to avoid confusion in case of several persons being present in an image. Skeleton estimation is performed utilizing part affinity fields as proposed in~\cite{cao2017realtime}. Following skeleton estimation, two detection passes are performed to estimate heatmaps for keypoints of interest. The former of these acts as a coarse detector (5b in Figure~\ref{fig:efficientpose}), whereas the latter (5c in Figure~\ref{fig:efficientpose}) refines localization to yield more accurate outputs.

Note that in OpenPose, the heatmaps of the final detection pass are constrained to a low spatial resolution, which are incapable of achieving the amount of details that are normally inherent in the high-resolution input~\cite{cao2018openpose}. To improve this limitation of OpenPose, a series of three transposed convolutions performing bilinear upsampling are added for  upscaling of the low-resolution heatmaps (step 6 in Figure~\ref{fig:openpose}). Thus, we project the low-resolution output onto a space of higher resolution in order to allow an increased level of detail. To achieve the proper level of interpolation while operating efficiently, each transposed convolution increases the map size by a factor of , using a stride of  with a  kernel.

\subsection{Variants}
\label{sec:variants}

Following the same principle as suggested in the original EfficientNet~\cite{tan2019efficientnet}, we scale the EfficientPose network architecture by adjusting the three main dimensions, i.e., input resolution, network width, and network depth, using the coefficients of Equation~\ref{eq:coefficients}. The results from this scaling are five different architecture variants that are given in Table~\ref{tab:variants}, referred to as EfficientPose I to IV and RT). As can be observed in this table, the input resolution, defined by the spatial dimensions of the image (), is scaled utilizing the high and low-level EfficientNet backbones that best match the resolution of high and low-resolution inputs (see Table~\ref{tab:efficientnets}). Here, the network width refers to the number of feature maps that are generated by each . As described in Section~\ref{sec:architecture}, width scaling is achieved using the same width as the high-level backbone (i.e., ). The scaling of network depth is achieved in the number of Mobile DenseNets (i.e.,  in Table~\ref{tab:variants}) in the detection blocks. Also, this ensures that receptive fields across different models and spatial resolutions have similar relative sizes. For each model variant, we select the number () of Mobile DenseNets that best approximates the original depth factor  in the high-level EfficientNet backbone (Table~\ref{tab:efficientnets}). More specifically, the number of Mobile DenseNets are determined by Equation~\ref{eq:depth}, rounding to the closest integer. In addition to EfficientPose I to IV, the single-resolution model EfficientPose RT is formed to match the scale of the smallest EfficientNet model, providing HPE in extremely low latency applications.



\begin{table*}
\centering
\caption{Variants of EfficientPose obtained by scaling resolution, width, and depth. Mobile DenseNets  computes  feature maps.  and  denotes the number of 2D part affinity fields and confidence maps, respectively.  defines transposed convolutions with kernel size , output maps , and stride }
\label{tab:variants}       
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Stage}        & \textbf{EfficientPose RT}   & \textbf{EfficientPose I}    & \textbf{EfficientPose II}    & \textbf{EfficientPose III}   & \textbf{EfficientPose IV}    \\ \hline
High-resolution input &                    &                    &                     &                     &                     \\ \hline
High-level backbone   & B0 (Block 1-3) & B2 (Block 1-3) & B4 (Block 1-3)  & B5 (Block 1-3)  & B7 (Block 1-3)  \\ \hline
Low-resolution input  &                            &                    &                     &                     &                     \\ \hline
Low-level backbone    &                            & B0 (Block 1-2) & B0 (Block 1-2)  & B1 (Block 1-2)  & B3 (Block 1-2)  \\ \hline
Detection block       &         &         &  &  &  \\ \hline
Prediction pass 1     & \multicolumn{5}{c|}{}                                                                                                                 \\ \hline
Prediction pass 2-3   & \multicolumn{5}{c|}{}                                                                                                                  \\ \hline
Upscaling             & \multicolumn{5}{c|}{}                                                                                                         \\ \hline
\end{tabular}
\end{table*}

\subsection{Summary of proposed framework}
\label{sec:summary}

As can be inferred from the discussion above, the EfficientPose framework comprises a family of five ConvNets (i.e., EfficientPose I-IV and RT) that are constructed by compound scaling~\cite{tan2019efficientnet}. With this, EfficientPose exploits the advances in computationally efficient ConvNets for image recognition to construct a scalable network architecture that is capable of performing single-person HPE across different computational constraints. More specifically, EfficientPose utilizes both high and low-resolution images to provide two separate viewpoints that are processed independently through high and low-level backbones, respectively. The resulting features are concatenated to produce cross-resolution features, enabling selective emphasis on global and local image information. The detection stage employs a scalable mobile detection block to perform detection in three passes. The first pass estimates person skeletons through part affinity fields~\cite{cao2017realtime} to yield feasible pose configurations. The second and third passes estimate keypoint locations with progressive improvement in precision. Finally, the low-resolution prediction of the third pass is scaled up through bilinear interpolation to further improve the precision level. 


\section{Experiments and results}
\label{sec:experiments}

\subsection{Experimental setup}
\label{sec:setup}

We evaluate EfficientPose and compare it with OpenPose on the single-person MPII dataset~\cite{andriluka14cvpr}, containing images of mainly healthy adults in a wide range of different outdoor and indoor everyday activities and situations, such as sports, fitness exercises, housekeeping activities, and public events (Figure~\ref{fig:mpii}a). All models are optimized on MPII using stochastic gradient descent (SGD) on the mean squared error (MSE) of the model predictions relative to the target coordinates. More specifically, we applied SGD with momentum and cyclical learning rates (see Appendix~\ref{sec:optimization} for more information and further details on the optimization procedure). The learning rate is bounded according to the model-specific value of which it does not diverge during the first cycle () and . The model backbones (i.e., VGG-19 for OpenPose, and EfficientNets for EfficientPose) are initialized with pretrained ImageNet weights, whereas the remaining layers employ random weight initialization. Supported by our experiments on training efficiency (see Appendix~\ref{sec:ablation}), we train the models for 200 epochs, except for OpenPose, which requires a higher number of epochs to converge (see Figure~\ref{fig:convergence} and Table~\ref{tab:epochs}). 

The training and validation portion of the dataset comprises K images, and by adopting a standard random split, we obtain K and K instances for training and validation, respectively. We augment the images during training using random horizontal flipping, scaling (), and rotation (/  degrees). We utilize a batch size of , except for the high-resolutional EfficientPose III and IV, which both require a smaller batch size to fit into the GPU memory,  and , respectively. The experiments are carried out on an NVIDIA Tesla V100 GPU. 

The evaluation of model accuracy is performed using the  metric.  is defined as the fraction of predictions residing within a distance  from the ground truth location (see Figure~\ref{fig:mpii}b).  is  of the diagonal  of the head bounding box, and  the accepted percentage of misjudgment relative to .  is the standard performance metric for MPII but we also include the stricter  metric for assessing models’ ability to yield highly precise keypoint estimates. As commonly done in the field, the final model predictions are obtained by applying multi-scale testing procedure~\cite{yang2017learning, sun2019deep, tang2018deeply}. Due to the restriction in the number of attempts for official evaluation on MPII, we only used the test metrics on the OpenPose baseline, and the most efficient and most accurate models, EfficientPose RT and EfficientPose IV, respectively. To measure model efficiency, both FLOPs and number of parameters are supplied.

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{Fig4.png}
\caption{The MPII single-person pose estimation challenge. From left: a) 10 images from the MPII test set displaying some of the variation and difficulties inherent in this challenge. b) The evaluation metrics  and  define the average of predictions within  distance () from the ground-truth location (e.g., left elbow), with  being 50\% and 10\%, respectively}
\label{fig:mpii}     
\end{center}
\end{figure*}

\subsection{Results}
\label{sec:results}

Table~\ref{tab:mpiival} shows the results of our experiments with OpenPose and EfficientPose on the MPII validation dataset. As can be observed in this table, EfficientPose consistently outperformed OpenPose with regards to efficiency, with  reduction in FLOPs and  fewer number of parameters. In addition to this, all the model variants of EfficientPose achieved better high-precision localization, with a  gain in \\  as compared to OpenPose. In terms of\\, the high-end models, i.e., EfficientPose II-IV, managed to gain  improvements against OpenPose. As Table~\ref{tab:mpiitest} depicts, EfficientPose IV achieved state-of-the-art results (a mean  of 91.2) on the official MPII test dataset for models with number of parameters of a size less than  million.

\begin{table*}
\centering
\caption{Performance of EfficientPose compared to OpenPose on the MPII validation dataset, as evaluated by efficiency (number of parameters and FLOPs, and relative reduction in parameters and FLOPs compared to OpenPose) and accuracy (mean  and mean )}
\label{tab:mpiival}       
\begin{tabular}{lllllll}
\hline\noalign{\smallskip}
\textbf{Model} & \textbf{Parameters} & \textbf{Parameter reduction} & \textbf{FLOPs} & \textbf{FLOP reduction} &  &   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
OpenPose~\cite{cao2018openpose} &  &	 &  &  & 	&  \\
EfficientPose RT &  &  &  &  &  &  \\
EfficientPose I	&  &  &  &  &  &  \\
EfficientPose II &  &  &  &  &  &  \\
EfficientPose III &  &  &  &  &  &  \\
EfficientPose IV &  &  &  &  &  &  \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

\begin{table*}
\centering
\caption{State-of-the-art results in  (both for individual body parts and overall mean value) on the official MPII test dataset~\cite{andriluka14cvpr} compared to the number of parameters}
\label{tab:mpiitest}       
\begin{tabular}{llllllllll}
\hline\noalign{\smallskip}
\textbf{Model} & \textbf{Parameters} & \textbf{Head} & \textbf{Shoulder} & \textbf{Elbow} & \textbf{Wrist} & \textbf{Hip} & \textbf{Knee} & \textbf{Ankle} & \textbf{Mean}  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Pishchulin et al., ICCV’13~\cite{pishchulin2013poselet} &  &  &  &  &  &  &  &  &  \\
Tompson et al., NIPS’14~\cite{tompson2014joint} &  &  &  &  &  &  &  &  &  \\
Lifshitz et al., ECCV’16~\cite{lifshitz2016human} &  &  &  &  &  &  &  &  &  \\
Tang et al., BMVC'18~\cite{tang2018cu} &  &  &  &  &  &  &  &  &  \\
Newell et al., ECCV’16~\cite{newell2016stacked} &  &  &  &  &  &  &  &  &  \\
Zhang et al., CVPR’19~\cite{zhang2019fast} &  &  &  &  &  &  &  &  &  \\
Bulat et al., FG'20~\cite{bulat2020toward} &  &  &  &  &  &  &  &  &  \\
Yang et al., ICCV’17~\cite{yang2017learning} &  &  &  &  &  &  &  &  &  \\
Tang et al., ECCV’18~\cite{tang2018deeply} &  &  &  &  &  &  &  &  &  \\
Sun et al., CVPR’19~\cite{sun2019deep} &  &  &  &  &  &  &  &  &  \\
Zhang et al., arXiv’19~\cite{zhang2019human} &  &  &  &  &  &  &  &  &  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
OpenPose~\cite{cao2018openpose} &  &  &  &  &  &  &  &  &  \\
EfficientPose RT &  &  &  &  &  &  &  &  &  \\
EfficientPose IV &  &  &  &  &  &  &  &  &  \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

Compared to OpenPose, EffcientPose also exhibited rapid convergence during training. We optimized both approaches on similar input resolution, which defaults to  for OpenPose, corresponding to EfficientPose II. The training graph shown in Figure~\ref{fig:convergence} demonstrates that EfficientPose converges early, whereas OpenPose requires up to  epochs before achieving proper convergence. Nevertheless, OpenPose benefited from this prolonged training in terms of precision, with a  improvement in  during the final  epochs, whereas EfficientPose II had a minor gain of  (see Table~\ref{tab:epochs}).

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{Fig5.png}
\caption{The progression of the mean error of EfficientPose II and OpenPose on the MPII validation set during the course of training}
\label{fig:convergence}       
\end{center}
\end{figure*}

\begin{table*}
\centering
\caption{Model accuracy on the MPII validation dataset in relation to the number of training epochs}
\label{tab:epochs}       
\begin{tabular}{lll}
\hline\noalign{\smallskip}
\textbf{Model} & \textbf{Epochs} &   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
OpenPose~\cite{cao2018openpose} &  &  \\
OpenPose~\cite{cao2018openpose} &  &  \\
OpenPose~\cite{cao2018openpose} &  &  \\
EfficientPose II &  &  \\
EfficientPose II &  &  \\
EfficientPose II &  &  \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

\section{Discussion}
\label{sec:discussion}

In this section, we discuss several aspects of our findings and possible avenues for further research.

\subsection{Improvements over OpenPose}
\label{sec:improvement}

The precision of HPE methods is a key success factor for analyses of movement kinematics, like segment positions and joint angles, for assessment of sport performance in athletes, or motor disabilities in patients. Facilitated by cross-resolution features and upscaling of output (see Appendix~\ref{sec:ablation}), EfficientPose achieved a higher precision than OpenPose~\cite{cao2018openpose}, with a  relative improvement in  on single-person MPII (Table~\ref{tab:mpiival}). What this means is that the EfficientPose architecture is generally more suitable in performing precision-demanding single-person HPE applications, like medical assessments and elite sports, than OpenPose. 

Another aspect to have in mind is that, for some applications (e.g., exercise games and baby monitors), we might be more interested in the latency of the system and its ability to respond quickly. Hence, the degree of correctness in keypoint predictions might be less crucial. In such scenarios, with applications that demand high-speed predictions, the 460K parameter model, EfficientPose RT, consuming less than one GFLOP, would be suitable. Nevertheless, it still manages to provide higher precision level than current approaches in the high-speed regime, e.g., \cite{bulat2020toward, tang2018cu}. Further, the scalability of EfficientPose enables flexibility in various situations and across different types of hardware, whereas OpenPose suffers from its large number of parameters and computational costs (FLOPs). 

\subsection{Strengths of the EfficientPose approach}
\label{sec:strengths}

The use of MBConv in HPE is to the best of our knowledge an unexplored research area. This has also been partly our main motivation for exploring the use of MBConv in our EfficientPose approach, recognizing its success in image classification~\cite{tan2019efficientnet}. Our experimental results showed that EfficientPose approached state-of-the-art performance on the single-person MPII benchmark despite a large reduction in the number of parameters (Table~\ref{tab:mpiitest}). This means that the parameter-efficient MBConvs provide value in HPE as with other computer vision tasks, such as image classification and object detection. This, in turns, makes MBConv a very suitable component for HPE networks. For this reason, it would be interesting to investigate the effect of combining it with other novel HPE architectures, such as Hourglass and HRNet~\cite{newell2016stacked, sun2019deep}. 

Further, the use of EfficientNet as a backbone, and the proposed cross-resolution feature extractor combining several EfficientNets for improved handling of basic features, are also interesting avenues to explore further. From the present study, it is reasonable to assume that EfficientNets could replace commonly used backbones for HPE, such as VGG and ResNets, which would reduce the computational overheads associated with these approaches~\cite{simonyan2014very, he2016deep}. Also, a cross-resolution feature extractor could be useful for precision-demanding applications by providing an improved performance on  (Table~\ref{tab:features}). 

We also observed that EfficientPose benefited from compound model scaling across resolution, width and depth. This benefit was reflected by the increasing improvements in  and  from EfficientPose RT through EfficientPose I to EfficientPose IV (Table~\ref{tab:mpiival}). To conclude, we can exploit this to further examine scalable ConvNets for HPE, and thus obtain insights into appropriate sizes of HPE models (i.e., number of parameters), required number of FLOPs, and obtainable precision levels.

In this study, OpenPose and EfficientPose were optimized on the general-purpose MPII Human Pose Dataset. For many applications (e.g., action recognition and video surveillance) the variability in MPII may be sufficient for directly applying the models on real-world problems. Nonetheless, there are other particular scenarios that deviate from the setting addressed in this paper. The MPII dataset comprises mostly healthy adults in a variety of every day indoor and outdoor activities~\cite{andriluka14cvpr}. In less natural environments (e.g., movement science laboratories or hospital settings) and with humans of different anatomical proportions such as children and infants~\cite{sciortino2017estimation}, careful consideration must be taken. This could include a need for fine-tuning of the MPII models on more specific datasets related to the problem at hand. As mentioned earlier, our experiments showed that EfficientPose was more easily trainable than OpenPose (Figure~\ref{fig:convergence} and Table~\ref{tab:epochs}). This trait of rapid convergence suggests that exploring the use of transfer learning on the EfficientPose models on other HPE data could provide interesting results. 

\subsection{Avenues for further research}
\label{sec:avenues}

The precision level of pose configurations provided by EfficientPose in the context of target applications is a topic considered beyond the scope of this paper and has for this reason been left for further studies. We can establish the validity of EfficientPose for robust single-person pose estimation already by examining whether the movement information supplied by the proposed framework is of sufficiently good quality for tackling challenging problems, such as complex human behavior recognition~\cite{liu2018learning, fernando2018tracking}. To assess this, we could, for example, compare the precision level of the keypoint estimates supplied by EfficientPose with the movement information provided by body-worn movement sensors. Moreover, we could combine the proposed image-based EfficientPose models with body-worn sensors, such as inertial measurement unit (IMU)~\cite{kundu2018hand}, or physiological signals, like electrical cardiac activity and electrical brain activity~\cite{fiorini2020unsupervised}, to potentially achieve improved precision levels and an increased robustness. Our hypothesis is that using body-worn sensors or physiological instruments could be useful in situations where body parts are extensively occluded, such that camera-based recognition alone may not be sufficient for accurate pose estimation.

Another path for further study and validation is the capability of EfficientPose to perform multi-person HPE. The improved computational efficiency of EfficientPose compared to OpenPose has the potential to also benefit multi-person HPE. State-of-the-art methods for multi-person HPE are dominated by top-down approaches, which require computation that is normally proportional to the number of individuals in the image~\cite{fieraru2018learning, zhang2019distribution}. In crowded scenes, top-down approaches are highly resource demanding. Similar to the original OpenPose~\cite{cao2018openpose}, and few other recent works on multi-person HPE~\cite{huang2020high, guan2019realtime}, EfficientPose incorporates part affinity fields, which would enable the grouping of keypoints into persons, and thus allowing to perform multi-person HPE in a bottom-up manner. This would reduce the computational overhead into a single network inference per image, and hence yield more computationally efficient multi-person HPE.

Further, it would be interesting to explore the extension of the proposed framework to perform 3D pose estimation as part of our future research. In accordance with recent studies, 3D pose projection from 2D images can be achieved, either by employing geometric relationships between 2D keypoint positions and 3D human pose models~\cite{yuan2020single}, or by leveraging occlusion-robust pose-maps (ORPM) in combination with annotated 3D poses~\cite{mehta2018single, benzine2020single}.

The architecture of EfficientPose and the training process can be improved in several ways. First, the optimization procedure (see Appendix~\ref{sec:optimization}) was developed for maximum  accuracy on OpenPose, and simply reapplied to EfficientPose. Other optimization procedures might be more appropriate, including alternative optimizers (e.g., Adam~\cite{kingma2014adam} and RMSProp~\cite{tieleman2012lecture}), and other learning rate and sigma schedules. 

Second, only the backbone of EfficientPose was pretrained on ImageNet. This could restrict the level of accuracy on HPE because large-scale pretraining not only supplies robust basic features but also higher-level semantics. Thus, it would be valuable to assess the effect of pretraining on model precision in HPE. We could, for example, pretrain the majority of ConvNet layers on ImageNet, and retrain these on HPE data. 

Third, the proposed compound scaling of EfficientPose assumes that the scaling relationship between resolution, width, and depth, as defined by Equation~\ref{eq:coefficients}, is identical in HPE and image classification. However, the optimal compound scaling coefficients might be different for HPE, where the precision level is more dependent on image resolution, than for image classification. Based on this, a topic for further studies could be to conduct neural architecture search across different combinations of resolution, width, and depth in order to determine the optimal combination of scaling coefficients for HPE. Regardless of the scaling coefficients, the scaling of detection blocks in EfficientPose could be improved. The block depth (i.e., number of Mobile DenseNets) slightly deviates from the original depth coefficient in EfficientNets based on the rigid nature of the Mobile DenseNets. A carefully designed detection block could address this challenge by providing more flexibility with regards to the number of layers and the receptive field size.

Fourth, the computational efficiency of EfficientPose could be further improved by the use of teacher-student network training (i.e., knowledge distillation)~\cite{bucilua2006model} to transfer knowledge from a high-scale EfficientPose teacher network to a low-scale EfficientPose student network. This technique has already shown promising results in HPE when paired with the stacked hourglass architecture~\cite{newell2016stacked, zhang2019fast}. Sparse networks, network pruning, and weight quantization~\cite{tung2018clip, elsen2019fast} could also be included in the study to facilitate the development of more accurate and responsive real-life systems for HPE. Finally, for high performance inference and deployment on edge devices, further speed-up could be achieved by the use of specialized libraries such as NVIDIA TensorRT and TensorFlow Lite~\cite{TensorRT:2020, TensorFlowLite:2020}.

In summary, EfficientPose tackles single-person HPE with an improved degree of precision compared to the commonly adopted OpenPose network~\cite{cao2018openpose}. In addition to this, the EfficientPose models have the ability to yield high performance with a large reduction in number of parameters and FLOPs. This has been achieved by exploiting the findings from contemporary research within image recognition on computationally efficient ConvNet components, most notably MBConvs and EfficientNets~\cite{sandler2018mobilenetv2, tan2019efficientnet}. Again, for the sake of reproducibility, we have made the EfficientPose models publicly available for other researchers to test and possibly further development. 


\section{Conclusion}
\label{sec:conclusion}

In this work, we have stressed the need for a publicly accessible method for single-person HPE that suits the demands for both precision and efficiency across various applications and computational budgets. To this end, we have presented a novel method called EfficientPose, which is a scalable ConvNet architecture leveraging a computationally efficient multi-scale feature extractor, novel mobile detection blocks, skeleton estimation, and bilinear upscaling. In order to have model variants that are able to flexibly find a sensible trade-off between accuracy and efficiency, we have exploited model scalability in three dimensions: input resolution, network width, and network depth. Our experimental results have demonstrated that the proposed approach has the capability to offer computationally efficient models, allowing real-time inference on edge devices. At the same time, our framework offers flexibility to be scaled up to deliver more precise keypoint estimates than commonly used counterparts, at an order of magnitude less parameters and computational costs (FLOPs). Taking into account the efficiency and high precision level of our proposed framework, there is a reason to believe that EfficientPose will provide an important foundation for the next-generation markerless movement analysis. 

In our future work, we plan to develop new techniques to further improve the model effectiveness, especially in terms of precision, by investigating optimal compound model scaling for HPE. Moreover, we will deploy EfficientPose on a range of applications to validate its applicability, as well as feasibility, in real-world scenarios.

\section*{Acknowledgements}
\label{sec:acknowledgements}

The research is funded by RSO funds from the Faculty of Medicine and Health Sciences at the Norwegian University of Science and Technology. The experiments were carried out utilizing computational resources provided by the Norwegian Open AI Lab.




\bibliographystyle{spmpsci}      \bibliography{references} 

\appendix
\section*{Appendices}
\section{Ablation study}
\label{sec:ablation}

To determine the effect of different design choices in the EfficientPose architecture, we carried out component analysis. 

\subsection*{Training efficiency}
\label{sec:training}

We assessed the number of training epochs to determine the appropriate duration of training, avoiding demanding optimization processes. Figure~\ref{fig:convergence} suggests that the largest improvement in model accuracy occurs until around  epochs, after which training saturates. Table~\ref{tab:epochs} supports this observation with less than  increase in  with  epochs of training. From this, it was decided to perform the final optimization of the different variants of EfficientPose over  epochs. Table~\ref{tab:epochs} also suggests that most of the learning progress occurs during the first  epochs. Hence, for the remainder of the ablation study  epochs were used to determine the effect of different design choices.

\subsection*{Cross-resolution features}
\label{sec:features}

The value of combining low-level local information with high-level semantic information through a cross-resolution feature extractor was evaluated by optimizing the model with and without the low-level backbone. Experiments were conducted on two different variants of the EfficientPose model. On coarse prediction () there is little to no gain in accuracy (Table~\ref{tab:features}), whereas for fine estimation () some improvement () is displayed taking into account the negligible cost of  more parameters and  increase in FLOPs. 

\begin{table*}
\centering
\caption{Model accuracy on the MPII validation dataset in relation to the use of cross-resolution features}
\label{tab:features}       
\begin{tabular}{llllll}
\hline\noalign{\smallskip}
\textbf{Model} & \textbf{Cross-resolution features} & \textbf{Parameters} & \textbf{FLOPs} &  &   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
EfficientPose I & \checkmark &  &  &  &  \\
EfficientPose I &  &  &  &  &  \\
EfficientPose II & \checkmark &  &  &  &  \\
EfficientPose II & &  &  &  &  \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

\subsection*{Skeleton estimation}
\label{sec:skeleton}

The effect of skeleton estimation through the approximation of part affinity fields was assessed by comparing the architecture with and without the single pass of skeleton estimation. Skeleton estimation yields improved accuracy with  gain in  and  in  (Table~\ref{tab:skeleton}), while only introducing an overhead in number of parameters and computational cost of  and , respectively.

\begin{table*}
\centering
\caption{Model accuracy on the MPII validation dataset in relation to the use of skeleton estimation}
\label{tab:skeleton}       
\begin{tabular}{llllll}
\hline\noalign{\smallskip}
\textbf{Model} & \textbf{Skeleton estimation} & \textbf{Parameters} & \textbf{FLOPs} &  &   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
EfficientPose I & \checkmark &  &  &  &  \\
EfficientPose I &  &  &  &  &  \\
EfficientPose II & \checkmark &  &  &  &  \\
EfficientPose II & &  &  &  &  \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

\subsection*{Number of detection passes}
\label{sec:passes}

We also determined the appropriate comprehensiveness of detection, represented by number of detection passes. EfficientPose I and II were both optimized on three different variants (Table~\ref{tab:passes}). Seemingly, the models benefit from intermediate supervision with a general trend of increased performance level in accordance with number of detection passes. The major benefit in performance is obtained by expanding from one to two passes of keypoint estimation, reflected by  increase in  and  in . In comparison, a third detection pass yields only  relative improvement in  compared to two passes, and no gain in  while increasing number of parameters and computation by  and , respectively. From these findings, we decided a beneficial trade-off in accuracy and efficiency would be the use of two detection passes.

\begin{table*}
\centering
\caption{Model accuracy on the MPII validation dataset in relation to the number of detection passes}
\label{tab:passes}       
\begin{tabular}{llllll}
\hline\noalign{\smallskip}
\textbf{Model} & \textbf{Detection passes} & \textbf{Parameters} & \textbf{FLOPs} &  &   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
EfficientPose I &  &  &  &  &  \\
EfficientPose I &  &  &  &  &  \\
EfficientPose I &  &  &  &  &  \\
EfficientPose II &  &  &  &  &  \\
EfficientPose II &  &  &  &  &  \\
EfficientPose II &  &  &  &  &  \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

\subsection*{Upscaling}
\label{sec:upscaling}

To assess the impact of upscaling, implemented as bilinear transposed convolutions, we compared the results of the two respective models. Table~\ref{tab:upscaling} reflects that upscaling yields improved precision on keypoint estimates by large gains of  in  and smaller improvements of  on coarse detection (). As a consequence of increased output resolution upscaling slightly increases number of FLOPs () with neglectable increase in number of parameters.

\begin{table*}
\centering
\caption{Model accuracy on the MPII validation dataset in relation to the use of upscaling}
\label{tab:upscaling}       
\begin{tabular}{llllll}
\hline\noalign{\smallskip}
\textbf{Model} & \textbf{Upscaling} & \textbf{Parameters} & \textbf{FLOPs} &  &   \\
\noalign{\smallskip}\hline\noalign{\smallskip}
EfficientPose I & \checkmark &  &  &  &  \\
EfficientPose I &  &  &  &  &  \\
EfficientPose II & \checkmark &  &  &  &  \\
EfficientPose II & &  &  &  &  \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table*}

\section{Optimization procedure}
\label{sec:optimization}

Most state-of-the-art approaches for single-person pose estimation are extensively pretrained on ImageNet~\cite{sun2019deep, zhang2019human}, enabling rapid convergence for models when adapted to other tasks, such as HPE. In contrast to these approaches, few models, including OpenPose~\cite{cao2018openpose} and EfficientPose, only utilize the most basic pretrained features. This facilitates construction of more efficient network architectures but at the same time requires careful design of optimization procedures for convergence towards reasonable parameter values.

Training of pose estimation models is complicated due to the intricate nature of output responses. Overall, optimization is performed in a conventional fashion by minimizing the MSE of the predicted output maps  with respect to ground truth values  across all output responses .

The predicted output maps should ideally have higher values at the spatial locations corresponding to body part positions, while punishing predictions farther away from the correct location. As a result, the ground truth output maps must be carefully designed to enable proper convergence during training. We achieve this by progressively reducing the circumference from the true location that should be rewarded, defined by the  parameter. Higher probabilities  are assigned for positions  closer to the ground truth position  (Equation~\ref{eq:confidence}). 



The proposed optimization scheme (Figure~\ref{fig:optimization}) incorporates a stepwise  scheme, and utilizes SGD with momentum of  and a decaying triangular cyclical learning rate (CLR) policy~\cite{smith2017cyclical}. The  parameter is normalized according to the output resolution. As suggested by Smith and Topin~\cite{smith2019super}, the large learning rates in CLR provides regularization in network optimization. This makes training more stable and may even increase training efficiency. This is valuable for network architectures, such as OpenPose and EfficientPose, less heavily concerned with pretraining (i.e., having larger portions of randomized weights). In our adoption of CLR, we utilize a cycle length of  epochs. The learning rate () converges towards  (Equation~\ref{eq:lrfinal}), where  is the highest learning rate for which the model does not diverge during the first cycle and , whereas  and  are the initial and final sigma values, respectively.



\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{Fig6.png}
\caption{Optimization scheme displaying learning rates  and  values corresponding to the training of EfficientPose II over  epochs}
\label{fig:optimization}       
\end{center}
\end{figure*}

\end{document}
