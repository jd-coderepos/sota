\documentclass[11pt]{article}
\usepackage{amsthm,amsmath,amsthm,amsfonts,amssymb,eufrak,multirow,url}
\usepackage{color, epsfig}

\usepackage{fullpage}


\usepackage{hyperref}
\hypersetup{backref, colorlinks=true, citecolor=blue, linkcolor=blue}


\newtheorem{lemma}{Lemma}\newtheorem{remark}{Remark}

\newcommand{\D}[2]{\frac{\partial #2}{\partial #1}}
\newcommand{\DD}[2]{\frac{\partial^2 #2}{\partial #1^2}}
\newcommand{\norm}[1]{||#1||}
\newcommand{\diag}{\mathop{\mathrm{diag}}}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{ax}{Axiom}
\newtheorem{example}{Example}

\usepackage[numbers]{natbib}
\bibliographystyle{plainnat}

\def \STEVE#1{{\bf \color{blue} (Steve: #1)}}
\def \ROB#1{{\bf \color{magenta} (Rob: #1)}}
\def \YUVAL#1{{\bf \color{red} (Yuval: #1)}}


\title{Achieving Both Valid and Secure Logistic Regression Analysis on Aggregated Data from Different Private Sources}
\author{Stephen E. Fienberg\thanks{Department of Statistics, Machine Learning Department and Cylab, Carnegie Mellon University, Pittsburgh, PA. \tt{mailto:fienberg@stat.cmu.edu}} \hspace{3pt}and Robert J. Hall\thanks{Department of Statistics and Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA. \tt{mailto:rjhall+@cs.cmu.edu}} \hspace{1pt}    and Yuval Nardi\thanks{Faculty of Industrial Engineering and Management, Technion - Israel Institute of Technology, Haifa, Israel. \tt{mailto:ynardi@ie.technion.ac.il}}}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Preserving the privacy of individual databases when carrying out statistical calculations has a long history in statistics and had been the focus of much recent attention in machine learning  In this paper, we present a protocol for computing logistic regression when the data are held by separate parties without actually combining information sources by exploiting results from the literature on multi-party secure computation.  We provide only the final result of the calculation compared with other methods that share intermediate values and thus present an opportunity for compromise of values in the combined database.  Our paper has two themes:  (1) the development of a secure protocol for computing the logistic parameters, and a demonstration of its performances in practice, and (2) and amended protocol  that speeds up the computation of the logistic function.   We  illustrate the nature of the calculations and their accuracy using an extract of data from the Current Population Survey divided between two parties.\\


{\bf Keywords:}  Distributed analysis; Logistic regression; Privacy-preserving computation; Secure multiparty computation.

\end{abstract}


\section{Introduction}

Privacy concerns are becoming more and more acute, especially in the digitized world where new supercomputers with an increasing processing capacities appear almost every day. These new machines together with impressive new technologies make the process of data collection, data storing and data analysis as easy as ever. This ``ease of use,'' may be manipulated by untrustful elements, whose aim is to deliberately cause harm by, for example, identifying and exposing sensitive data. It is the goal of privacy preserving methods to prevent or at least  lessen the chances of such harmful actions from happening. In this paper we present a novel way to achieve the goal when a certain statistical analysis is required.

Preserving the privacy of individual databases when carrying out statistical calculations has a long history in statistics and had been the focus of much attention in machine learning, e.g., see~\cite{aggarwal}. Once data are merged across sources, however, privacy becomes far more complex and a number of privacy issues arise for the linked individual files that go well beyond those that are considered with regard to the data within individual sources. When the goal is the production of the results of some statistical calculation, such as a regression analysis, c.f. Karr et al.~\cite{securereg-jcgs05,securereg-springer06}, we can often exploit results from the cryptography literature, borrowing tools  such as secure multi-party computation, e.g., see~\cite{lindell_pinkas_1,ppdm_book}. Secure multi-party protocols are concerned with distributed computation where each participating party, holding a private input, learns nothing but the result (see Section \ref{sec:smpc}).

This paper has two main themes. In both themes we conceptualize the existence of a single combined database containing all of the information for the individuals in the separate databases and for the union of the variables.    We propose an approach that gives full statistical calculation on this combined database without actually combining information sources, see~\cite{lindell_pinkas_2,lindell_pinkas_1}.  We focus mainly on logistic regression, but our methods and tools are essentially adaptable to other statistical models, as indicated in Section \ref{sec:extension}.  Our approach provides only the final result of the calculation compared with other methods that share intermediate values and thus present an opportunity for compromise of values in the combined database, c.f.~\cite{fien_secure,fien_secure2}. We remark that our problem differs from the one studied by Chaudhuri and Monteleoni~\cite{Chaudhuri} using differential privacy, since they are concerned with information leakage by the output of the computation, whereas we are concerned with leakage from the computation itself !


The first theme is the development of a novel approach to perform the calculations required for fitting logistic regression models when the data are distributed among several parties. In our settings the parties are unwilling or are simply forbidden (by law regulations) to share their respective data. They acknowledge the fact that pooling their private data into a conceptual global database, and running the logistic regression on the pooled data, rather than on their own data, can only lead to a better statistical analysis. We develop a secure protocol to compute the maximum likelihood estimates of the logistic parameters. Throughout the paper we make repeated use of what is known as random secret  sharing, which enables us to keep intermediate parameter values secret. The first theme aims at performing the required calculations by using operations which are restricted to a linear algebra type. Note that the fitting process requires computing the logistic function which is highly non-linear.
In principle, we may perform  any computation securely, by making use of Yao's general protocol \cite{yao82}.  Nonetheless, this is in essence a theoretical construction which will often be inefficient for large computations \cite{ppdm_book}. Instead, we craft a specially designed approximation to the logistic function, which can be securely evaluated using the machinery of random shares and Yao's millionaire protocol.

We establish the theoretical validity of the secure protocol for computing the logistic parameters, and show its performances in practice. In high dimensional problems with large number of cases our protocol may require faster computing resources. This is mainly because our approximation requires computing the predicate ``greater-than,'' which may take many encryptions. Indeed, evaluating this predicate by a reduction to Yao's protocol takes roughly  encryptions where  is the number of bits used to represent the numbers (this becomes dauntingly large due to the secret sharing scheme).

This leads us to the second theme, which tries to amend the protocol is a way that speeds up the computation of the logistic function. The main idea here is to avoid special circuit sub-protocols, such as  Yao's protocol. To that end, we show that we can perform the fitting process  using only sums and products. The advantage to this  is that these computations are very well studied primitives in secure multiparty computation and thus we can instantiate our method in a different secure multiparty computation scheme (e.g., \cite{ppdm_book, goldreich}), depending on the security demands of the data holders. We propose to approximate the vector of logistic function values, by repeatedly Taylor expanding around the current value and stepping along the gradient. Operations other than sums and products, are not needed here.   In principle, the approach represents the logistic function as the solution to a ordinary differential equation, then applies Euler's method to approximate the solution. As with the first theme, we show that we can make  the approximation arbitrarily accurate, at the expense of computational efficiency, and we present an illustrative empirical result.

We close the introduction with a brief description of logistic regression, mainly for the purpose of setting notation.
Logistic regression is used for predicting binary outcomes or class membership given a set of explanatory variables or predictors. We can use the fitted model  to predict class membership for a newly obtained record consisting of only the values of  the predictors. The basic framework of logistic regression treats binary responses  as realizations of  independent Bernoulli random variables, , whose mean depends on a set of predictors , as follows:

where , is the sigmoid (or the logistic) function, and  is a -dimensional parameter vector. This makes the log odds, , linear in the predictors.

A standard method for computing the maximum likelihood estimates of  is Newton-Raphson's  method, since closed form expressions do not exist. The fitting process  requires the user to supply the log-likelihood function associated with logistic regression, along with its first two derivatives.  Suppressing dependence on the data and vector of parameters, we let  be the  log-likelihood, i.e., . We also put on record the first two derivatives:







The gradient and the Hessian are assembled together to produce an estimate of the logistic parameters through the iterative process:  

Our protocol will be structured in rounds, where each round corresponds to an iteration of Newton's method (\ref{eq:nr}) followed by a convergence check.  Each round involves a loop through all the cases  to compute the contribution to the gradient and Hessian.  
We keep intermediate values of  unshared between the parties. This is made possible by representing  as random shares (see Section \ref{sec:blocks}).



The remainder of the paper is organized as follows. Section \ref{sec:smpc} presents the multi-party setup.  In section \ref{sec:blocks} we provide several sub-protocols which we will need. Sections \ref{sec:protocol1} and \ref{sec:protocol2} describe our protocol and an approach for speeding up the calculation involved, respectively. Section \ref{sec:security} describes implementation details. Section \ref{sec:experiment}  illustrates aspects of the computation on an extract of data from the Current Population Survey divided between two parties. Section  \ref{sec:extension} discusses possible extensions.  We defer all technical details  to Appendices \ref{sec:validity_1} and \ref{sec:validity_2}. 




\subsubsection*{Setting}

We let  denote the  design matrix, and  the -dimensional response vector. We assume the presence of  parties who are interested in computing logistic regression on the total of their data.  We suppose that the union of the parties data corresponds to the  and  of the logistic regression.  In particular, we suppose that party  holds onto the pair  with  and , where  is the  party design matrix, and  is her (binary) response vector.

In this work we consider a setting where each party has an ``additive share'' of the dataset.  That is,  and  where  and  correspond to the design matrix and response vector of the combined data on which the logistic regression is performed.  This subsumes all the partitioning schemes for the database (e.g., vertical and horizontal partitioning which are the cases considered in \cite{ppdm_book}) as in these cases for each element, one party holds the value and the remaining parties hold zero.  Furthermore this setup is applicable in a case where parties may have overlapping data, and the logistic regression is to be learned by using a linear function of the overlapping data (e.g., a weighted average) as a kind of measurement error model.  We suppose that the union of the individual data sets gives the complete data.  In cases where some data are missing, we can apply a privacy preserving imputation method such as in \citet{jagannathan}  as a preprocess, and then run our protocol.

We note that our method is general in the sense that it is applicable to every partitioning scheme, but it is clearly possible to treat specific cases such as vertically partitioned data  with more efficient specialized protocols.


\section{Secure Multi-Party Computation}\label{sec:smpc}

Ideally we would like our method to provide only the output of the calculation to the parties involved, and reveal nothing more.  This is a lofty goal without the aid of trusted third parties, however it is relaxed in a useful way in the cryptographic literature.  First it is assumed that the parties are not able to quickly solve computationally hard problems (such as breaking RSA encryption).  Then, a protocol is secure so long as intermediate values in the computation either contain almost no information (in the sense that the protocol would have to be re-run astronomically many times on the same input data in order to detect any information in the messages), or will only reveal information as the result of an intractable computation.  We now briefly review the security model we intend to use.

We consider the ``functionality'' (see \cite{goldreich}) which maps the data of each party into the logistic regression parameter vector :




The right hand side represents  copies of the parameter, so that each party receives the same output.  Note that each design matrix is of the same dimensions.

A protocol for computing the functionality is just a sequence of steps, consisting of parties performing local computations, and sending intermediate messages to each other. In this work we build up a protocol for computing (\ref{lr-fun}) which is secure in the presence of ``semi-honest'' parties.  That is, parties who obey the protocol (and do not try to e.g., inject malformed data) but keep a transcript of all the messages they receive.  Intuitively, a protocol is secure in this setting whenever the intermediate messages give no information about the secret inputs of other parties.  Formally, the ``view'' of the  party during the protocol is:


where  is a record of all the random draws made by party , and  is the  message received by that party (we have dropped dependence of  on  for readability).

The protocol is secure so long as there exists a polynomial time algorithm which, when given only the input and output of party , may output a random transcript of message which is \emph{computationally indistinguishable} from .   See  \citet{goldreich} for a definition and discussion of computational indistinguishability.  In essence, if the distribution of the sequence of messages depends only on the private input and output of party  then we can simulate messages by drawing from this distribution (so long as the random number generator returns samples which are computationally indistinguishable from draws from the distribution).  The existence of a simulator shows that intermediate messages do not depend on the private input of other parties, and so the protocol is secure in the sense that parties gain no more information about each other's private inputs than that revealed by the output of the protocol.  Note that this type of security is ``orthogonal'' to that studied in \citet{Chaudhuri}, which seeks to prevent leakage of secret information in the parameter vector.

An example of a protocol which does not achieve this definition of security is one where all parties send their data to party 1, who computes the parameter locally on the combined data and then sends it back to all other parties.  In this case the messages received by party 1 consist of the data of other parties, in general it is impossible to simulate these messages given only the input and output belonging to party 1.

In the next section we present a protocol for performing Newton's method on the logistic regression objective in a way which is secure in the presence of semi-honest parties. Our protocol makes use of a specially designed approximation for the logistic function. Section \ref{sec:protocol2} then describes a different approximation necessitating  the operations of only sums and products, and thus speeding-up the computations.

Although we propose to use the cryptographic model for security, others exist and deserve a place in the theory of privacy preserving data analysis.  The main alternatives we see are ``weak'' security, and {\em perturbation} of the data.  The former comprises a body of literature summarized in \citet{ppdm_book}.  The idea is that by giving weaker privacy guarantees, we can implement much more efficient protocols.  Whether it is acceptable to have this weaker privacy guarantee is a question which one must   consider  on a case-by-case basis.  Although we describe our protocol in terms of the cryptographic model, by replacing the primitive operations (in Section~\ref{sec:blocks}) with their weakly-secure counterparts, we convert our protocol into a weakly secure (but also computationally more efficient) one.

The second alternative is data perturbation or sanitization.  The idea would be for each party to somehow perturb his data until he is happy to release it to the other parties (e.g., through the addition of random noise).  Thereupon the parties would each have a noisy copy of all the data, and could locally compute whatever statistical method they wanted on the union of the data.  The difficulty with this approach is that to protect privacy may require the addition of noise of such amplitude as to render the data itself useless.


\section{Primitives for Secure Protocols}\label{sec:blocks}

In this section we lay out some primitives and  sub-protocols which we will commbine to make a full logistic regression protocol.  While, details of the implementation of these primitives are  in the references cited, we also include some  in the appendix.

\subsection{Secret Sharing}
In our construction we  make extensive use of additive secret sharing.  The idea is to divide a quantity of interest  into  random numbers  (one for each party) so that .  If the  are distributed uniformly in the field (e.g., the entirety of ) then any subset of the  will reveal nothing about .  In fact the sum over any subset is a random variable, the distribution of which does not depend on the secret value.

We use this construction to keep all intermediate quantities secret during the evaluation of Newton's method (i.e., the gradient, Hessian and intermediate parameter vectors).  As long as we can construct sub-protocols which compute random shares of a quantity, from random shares of inputs, then we can compose these sub-protocols together to finally obtain random shares of the logistic regression estimate.  With these in hand the parties can then exchange shares and reveal the vector itself.

Although the joint distribution of the  concentrates on the linear subspace corresponding to the secret value, marginally the shares are uniformly distributed and do not  depend on any parameters.  Hence we can easily simulate messages based on these shares since the marginal distributions are known, and  we achieve security as defined in Section~\ref{sec:smpc} .  Next we show how to compute additive shares of all the intermediate quantities using the abstract definition of additive shares.  Although this approach is intuitively appealing, computers would quickly run into problems representing samples drawn uniformly from .  Therefore, in the appendix we show how to approximate arbitrarily well the same computations in modular arithmetic on  for some large .

\subsection{Computing Sums and Products with Random Shares}\label{sec_la}

To implement Newton's method we must essentially  perform linear algebraic operations on random shares, for example by computing shares of the Newton step from shares of the gradient and inverse Hessian.  In this section we describe how to obtain random shares of sums and products of quantities that are themselves represented as random shares.  Using these constructions, we   compute inner and outer products of vectors of random shares, and hence also matrix multiplies.

Computing shares of the sum of two secret quantities  and  is direct, as it involves only the local computation  for each party . That is, party  simply adds his shares  and  together to get a random share of the quantity .
Obtaining random shares of the product of two secret quantities is more involved:



\noindent  The elements of the first sum on the right hand side can be computed locally by each party.  The second (double) sum, however,  requires products between random shares held by different parties.  To obtain these terms while maintaining the security of the protocol, we turn to \emph{oblivious function evaluation}.  That is, we pose the problem of computing the product as evaluating a function so that one party only knows the function and the other party only knows his input and the value of the function applied to that input.

The function set up by party  and evaluated by party  on his input, , is:


\noindent where  is a quantity generated uniformly at random by party .  Evaluation is done in a manner so that party  learns nothing of the output (and thus only learns about  which he generated) and party  learns only the output.  Since party  does not know the value of the random variable  he has learned potentially nothing about the true value of the product.  Taking  and  we have that , and thus they form random shares of the product .

Once parties compute random shares  for all the terms , they can locally compute random shares of  as:

Summing up these quantities, and utilizing the definition of the linear function set up in (\ref{prod-share-fn}), we easily obtain:

which shows that the 's in (\ref{eq:prod_shares}) are indeed (additive) shares of the product.

This protocol generates random shares of the product even if the original shares weren't themselves random, e.g., if they were due to the partitioning of the data.  A method which implements these encrypted multiplications using fixed-point arithmetic is given in \cite{fhn:10}.  

We also note that dividing one secret value into another securely is much more difficult than dealing with products and requires more elaborate (and computationally demanding) protocols.  Below we show how matrix inversion can be performed without any divisions.

\subsection{Evaluating Interval Membership}\label{sec:yaosgt}
We suppose we are able to evaluate the following predicate in a secure way:

Where  are secret values held by separate parties.  This is known as Yao's ``millionaires problem,'' since he described it in the context of determining which millionaire has the most money, without disclosing actual bank balances.

An example of a protocol which computes this predicate is given by \citet{gt_proto}.  We can also trivially extend it so that each party receives a random share of the output bit (i.e., each party receives a random bit, the ``xor'' of which yields the correct output bit).  Using this technique we can also check whether a secret value (i.e., a sum of random shares) is greater or less than some constant:



\noindent where  are the random shares of  held by two parties.

\subsection{Securely Inverting a Matrix}\label{sec:matrix_inversion}

We use a matrix inversion routine  built up entirely of matrix multiplications and subtractions, thus allowing us to use the constructions of the preceding sections to implement it securely. We obtain the reciprocal of a number   without necessitating any actual division  by an application of Newton's method to the function . Iterations follow , which requires multiplication and subtraction only.

It turns out that we can apply the same scheme   to matrix inversion,  e.g.,  see~\cite{guo_higham} and references therein. A numerically stable, coupled iteration for computing , takes the form:

where , and  is to be chosen by the user. A possible choice, leading to a quadratic convergence of  , is . In our actual implementation we used instead the trace (which dominates the largest eigenvalue, as the matrix in question is positive definite), since we can compute shares of the trace from shares of the matrix locally by each party. To compute  we use the same iteration, with scalars instead of matrices.  For this iteration we initialize with an arbitrarily small  (as convergence depends on the magnitude of the initial value being lower than that of the inverse we compute).  We use the constructions of section~\ref{sec_la} to iterate through (\ref{matrix-inv}) until convergence. As , we check for convergence by considering the absolute difference between the trace of  and the data dimension , and we can evaluate the function  on random shares of the trace of  using the same form as (\ref{yaos-gt-eqn}).






\section{First Protocol for Logistic Regression}\label{sec:protocol1}


We recall the usual Newton-Raphson iteration expression (\ref{eq:nr}). 
To perform the iteration  we first  compute random shares of the update direction: ,
\noindent via the formulation of matrix-vector products of random shares.  We can then add these random shares to the current parameters  to obtain random shares of .  To check convergence recall (from e.g., \cite{cvx_book}) we should end if:

We can compute (\ref{convergence}) securely using the same form as (\ref{yaos-gt-eqn}).  The result is sharable among all the parties, and the protocol ends whenever the result is 0, i.e., when  is not greater than .



By using the constructions of the previous section, we have the tools required to invert shares of the Hessian, and thus to compute the Newton step.  All that we need to do is  construct a secure protocol to evaluate the logistic (sigmoid) function.  In principle, a specialized sub-protocol could be built up using the construction of Yao \cite{yao82}.  The method would be to construct circuit that evaluates the sigmoid function in the same manner that the arithmetic logic unit in a CPU would.  Then we could give this circuit the secure treatment and make it into a protocol following \citet{goldreich}.  The disadvantage with this approach is that the circuit evaluation protocols are prohibitively expensive and thus they are not  useful in practice except for trivial circuits, see e.g.,  \citet{fairplay}.   Instead we use a specially crafted approximation to the logistic function in terms of indicator functions.  We describe this next.

\subsection{A Secure Approximation to the Logistic Function}\label{sec:RS_of_sigma}

The logistic function itself is the CDF of the logistic distribution.  We propose to approximate this function with an ``empirical CDF.''  This is a function of a set of  samples , taken independently from a logistic distribution:



Based on the Glivenko-Cantelli theorem, and later work by Dvoretzky, Kiefer and Wolfowitz, the rate at which the empirical CDF converges to the true CDF (i.e., the logistic function which is of interest) is known.  Using these results, we  obtain bounds on the maximum difference between the logistic function and our approximation, which hold with high probability.  See the remark below in Section~\ref{sec:ECDF} about the accuracy of the approximation.

We now turn attention to obtaining random shares of the logistic function evaluated at random shares of .  We obtain random shares of  by using the inner product construction for multiplying together random shares.  If we denote shares of this inner product by  for party , we  write:

Thus the problem reduces to getting random shares of the sum of indicators.   Note that we can re-write each indicator function as:


\noindent If party 2 generates the logistic random variables then we have a trivial reduction to (\ref{yaos-gt-eqn}).  In order to restrict the view of either party to a random share, we restrict the output to random bits , and , such that

where  is the exclusive or. The right-hand side of equation (\ref{eq:sigma-approx2}) requires (random shares of) the fraction of outputs with . This can be established by noticing that

where we denote  for . Jagannathan and Wright~\cite{jagannathan} use this method to convert xor shares into additive shares for a different privacy-preserving task.

In order for the output to behave this way, we can either use Yao's protocol directly, or take a (more efficient) GT protocol and modify it to give a (xor) random share.  In this work we use the protocol of \citet{gt_proto}.  Having computed random shares of the logistic function, we can use the constructions of Section~\ref{sec_la} to compute random shares of the gradient and Hessian, and hence build a full logistic regression protocol.
















\subsection{Quality of the Logistic Approximation}\label{sec:ECDF}
A comment about the accuracy of approximation (\ref{sigma-appox}), and the resulting logistic parameter estimator is in order. The tail behavior of the sup-norm of the error is given, for every , by:

known as the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality. One possible way to choose the number of Logistic variables  in practice, is by ensuring that the above probability is no more than a prescribed level of accuracy, say . Solving for  we obtain the (very conservative) bound . A less conservative bound might entail the maximum absolute error restricted to some interval (containing the origin). 

We relate error in the approximation of the sigmoid (and hence the gradient) to the error in the convergent parameter by the following inequality:


in which  is the optimizer of the exact log likelihood, and  is the optimizer of our approximation,  is the smallest eigenvalue of the Fisher information matrix  (on some interval, see Appendix~\ref{sec:validity_1}), and  is the radius of a ball which containing all the data vectors (i.e., ).  The proof of this inequality follows lemma 1 of \cite{Chaudhuri} in which the two convex functions are the exact log likelihood objective, and the difference between the exact and approximate objectives. See Appendix~\ref{sec:validity_1} for detailed theoretical derivation.

Since we can use expression (\ref{dkw_ineq})   to bound the numerator of  expression (\ref{parm_err_bound}),   the parameter output by our protocol, and that output by the exact (non-private) algorithm can be brought arbitrarily close (except on a set of negligible probability) by increasing the parameter .  Later, we perform an experiment to show how well the method performs with reasonably small .  Note that for Newton's method to converge in this approximation, we must use the same sample of  logistic random variables each time we approximate the sigmoid.  Otherwise assessing convergence would be difficult as the objective function would be constantly shifting.  We propose that the parties draw  logistic variables ahead of time, and use these for all the computations.

\subsection{Hessian Lower Bound Technique}\label{sec:hess_bound}

Notice that the Newton Raphson method requires inverting a matrix (the Hessian of the log likelihood) at each iteration.  In our setting, using our iterative inversion method this becomes very expensive.  Therefore we propose to use a well-studied approximation \cite{minka}, which replaces the iteration by:



First note that under this technique the algorithm only ever needs a single matrix inversion, since  is constant throughout all the iterations.  Second, this algorithm still eventually converges to the correct parameter value (modulo the other approximations we make in our protocol).  The reason is that the inverse hessian is always greater than , in the sense that the difference is positive semi-definite, see e.g, Minka \cite{minka} for more details.  What's more, this technique ensures that progress towards the optimum is monotonic,  and so  assessing convergence may be simpler.

\subsection{Computation and Communication Complexity}\label{sec:complexity_1}

First we count how many times we must run each of our primitives for each iteration of Newton's method.  The approximation of section~\ref{sec:RS_of_sigma} requires  instances of the GT protocol per round, as  instances are required per case.  Computing the gradient and the Hessian requires  multiplications.
Inverting the Hessian takes  multiplies and one GT per iteration of (\ref{matrix-inv}).  Since this inner iteration is quadratically convergent, it takes  iterations to converge, and thus takes  multiplies and  instances of GT. In total then, each outer iteration takes  multiplies, and  invocations of the GT protocol.

Each multiplication requires a number of encryptions and decryptions;  this scales quadratically with the number of parties  since they must exchange with one another.  Thus the computational workload increases as the data are split into more pieces.  Note that although repeated use of the cryptosystem is quite expensive, performance  on normal hardware is relatively rapid.  A machine  dedicated to the computation and running multiple threads can do thousands of encryptions per  second.

Each instance of GT using the protocol of \cite{gt_proto} requires  encryptions and decryptions (and operations on encrypted values etc.).  Therefore in total our approximation of section~\ref{sec:RS_of_sigma} requires  encryptions per iteration.  This may be too computationally demanding for large .  One way to reduce this cost is to run the scheme using a coarse approximation to the sigmoid (i.e., a small ) to convergence, then increase , resample the logistic variables and then continue Newton's method from the previous convergent parameter.  Although the latter iterations will still be computationally burdensome, there will be fewer of them. Another way is to use a different approximation to the sigmoid function. This is outlined next in Section \ref{sec:protocol2}.

Note that the total amount of communication by all parties is also proportional to the number of multiples and GT invocations.  For an invocation of either, a party must transmit  bits to another party, and then receive a message of the same length. There are a total of  messages which must be sent for each iteration.  If  the number of parties or cases, or the granularity of the approximation is large,   running the protocol over a high speed local area network would make the communication overhead manageable.


\section{Second Protocol for Logistic Regression}\label{sec:protocol2}

As we mentioned above, the computation complexity of evaluating  approximation (\ref{sigma-appox}) to the logistic function scales linearly with , since  on each of Newton's iteration we invoke Yao's protocol to compute the GT predicate, and we do it for every case .    This may be prohibitively expensive even for a moderate . A possible way to reduce this computational burden was briefly described in Section \ref{sec:complexity_1}. Here, we provide full details of a more structured approach, which is reminiscent of Euler's method. The approach is built (again) on computing Newton's iteration (\ref{eq:nr}). It would be more natural in this section to treat the logistic function in a {\it vectorized} fashion, i.e., , for an -dimensional vector . Therefore, we use different, albeit equivalent, representations for the gradient and Hessian:

Here  is the design matrix whose rows are , the units or feature vectors (see (\ref{grad-hess})). The symbol ``'' denotes the element-wise product, i.e., .



We modify the iteration so that we neither  explicitly compute the logistic function  which is involved in both the gradient and Hessian, nor use the approximation in expression (\ref{sigma-appox}).  Note that throughout the procedure we may treat each unit  as having an associated logistic function value .  We propose to track a vector of approximate function values  which will be updated after each iteration.  Then, these approximate values will be used to compute the next iteration of .  Note that the derivative of the logistic function is given by:

Therefore, knowing the value , we can determine the derivative of the logistic function around    by a single multiplication.  Linearizing around some value  gives:

where the second derivative is evaluated at some value  in the interval between  and .  Denote by  as in Section \ref{sec:protocol1}, then may make use of the approximation:

where  is applied element-wise to .


Over the course of the entire algorithm, the approximation  is updated repeatedly, in a manner very similar to using Euler's method to numerically integrate the differential equation (\ref{logistic_deriv}).  It is well known that the error of this method decreases with the size of the ``step'' taken at each iteration. In the above, the steps are of size , which will in general be different on each iteration, and will also be different for each unit.  In order to control the error we amend this approximation by breaking down the step into  smaller steps each of size , and performing  such updates.  As we shall see, we may base our choice of  on some aspect of the design matrix,  , in order to reach a desired level of error in the approximation.  We write this approximation as:


where the  are the intermediate values corresponding to the inner iterations, and we define  as the function which gives the average value of  evaluated on these values.

We summarize our method  in the following coupled iteration:

where  is the -dimensional vector of zeros and  is the -dimensional vector of ones.
The proposed iteration differs from the protocol of Section \ref{sec:protocol1} (and from the usual Newton-Raphson method).  The main difference is that we have replaced the logistic function approximation (\ref{sigma-appox}) with our Taylor approximation. Note that we are using again the bound on the Hessian (see Section \ref{sec:hess_bound}), which would make computation easier. We use this technique in our method for this reason, and also since it interacts well with our Taylor approximation by ensuring that convergence towards the optimum is in a sense monotonic, as shown in Section \ref{sec:convergence}.  In keeping with our goal of using only sums and products, we recall that it is possible to invert a matrix with just these operations (see Section \ref{sec:matrix_inversion}).



We now present a bound on the distance from our approximated regression coefficients , to the true optimizer of the log-likelihood which we denote by , as in (\ref{parm_err_bound}).
Since our iterations are guaranteed to converge (see Section \ref{sec:convergence}), we can choose to run the iterations until  is smaller than some threshold  (i.e., by choosing  accordingly):


Therefore we can bound the norm of the gradient of the logistic log-likelihood taken at our final parameter estimate:

where  is the radius of a ball containing all the data vectors, exactly as in (\ref{parm_err_bound}),  is some constant, and  is a quantity upper bounding the maximal Euler's step size.

We can use this to construct our main result about the quality of our approximation.  Suppose we choose , then from the above we have:

where  is the smallest eigenvalue of the Fisher information matrix  in the line segment between  and .  Note that  and  the factors of  cancel.

Therefore we can make the accuracy of our approximation arbitrarily good by decreasing  although, as we shall see there is a tradeoff involved.  A smaller  usually means a higher , resulting in increased computational demands. We refer the reader to Appendix~\ref{sec:validity_2} for complete technical details.

\subsection{Choice of }
Thus far we have that the error of the approximation decreases as  is decreased; however, this last variable is not controlled directly (as  was in protocol 1) but rather is a function of , the number of steps taken for each outer iteration of the algorithm.

In principle, to get at a prescribed step size , we can choose  by noting that:



This leads to the overly conservative choice of .  An alternative choice is to run the protocol with a small value of , e.g., 10, and then to re-run with different values to assess the sensitivity of the computation.  In Section~\ref{sec:experiment} we show that this technique performs wekk even with small .

\subsection{Computational Complexity}

We can measure the overall complexity of our method in terms of the number of products that are needed, since these are the most time-consuming operations we use.  First note that to construct the matrix  takes  products, and inversion of this matrix takes  using (\ref{matrix-inv}), where the constant is related to the condition of the matrix.  Then on each iteration, to compute  takes  products.  Our approximation to the logistic function takes  products, for a total of  products per iteration.


We compare this with the cost of a protocol which computes the logistic function via a specially designed sub protocol based on circuit evaluation, cf. \citet{yao82}.  If the latter may be evalutated using  encryptions, then the complexity would be  operations per iteration.  As mentioned before this number would typically be much larger than  (for example on the order of the number of bits used to represent the numbers).  Therefore on each iteration we can save a multiple of  operations, which may be especially important when  is large.


\section{Security Guarantees}\label{sec:security}

Since our protocol runs until convergence, the number of rounds is variable and depends on the data itself.  Furthermore a matrix inversion was performed by an iterative scheme which itself took some variable number of iterations to converge.  Therefore we amend the protocol so that the output for each party is a triple consisting of the convergent parameter value, and the number of iterations it took to converge, and the number of iterations taken for the matrix inversion.  This way the messages received from testing convergence are easily simulated (i.e. a zero on every round up until the number specified in the output, then a one on that iteration) and this clearly reveals no more information since the parties know ``where they are'' in the protocol at all times and  could count these numbers of iterations.  Having dealt with this technicality we will consider simulating the other intermediate messages in our simulator, and consider these convergence tests already taken care of.

In both of our protocols, the messages which are transmitted are always part of some sub-protocol, namely multiplication or evaluation of the ``greater than'' predicate.  The only exception to this is the final messages which are sent immediately before the output is reconstructed.  As those messages are themselves random shares they may be simulated easily (although they must be simulated in such a way that they sum to the correct output values, but this is trivial).  The messages which are passed during the sub-protocols may be simulated based on their respective input and outputs so long as the sub-protocols are cryptographically secure.  Since we take care to ensure that the intermediate values are random shares, the simulators for the sub-protocols ``compose'' to form a simulator for the main protocol (see \cite{goldreich}).

\section{Illustrative Experiments}\label{sec:experiment}

We provide two illustrative experiments to demonstrate our approach. The first aims at  showing the performance of our protocol from Section \ref{sec:protocol1}. Specifically, we examine the effect of  approximation (\ref{sigma-appox}) on the resulting parameter values, when small, and large number of logistic variables  are being used. The second example takes a look at the altered protocol from Section \ref{sec:protocol2}, which uses the coupled iteration (\ref{eq:coupled}) instead of approximation (\ref{sigma-appox}), and reports its performances for different values of , the number of Euler's ``steps''.

For both experiments we use an extract from the Current Population Survey (CPS) data (see \url{http://www.bls.gov/cps/}), which includes data on a sample of slightly more than  50,000 U.S. households.  We focus on predicting whether  household income is greater than 50,000 dollars.  We converted -category features into  binary features, and divided age into 4 bins corresponding to 20 year intervals.  Note that although we expressed our approach in terms of continuous covariates, it handles binary flags just as well, where said covariates take on e.g.,  and .

Our protocol from Section \ref{sec:protocol1} deviates from the exact computation in two ways, first we use an approximation to the gradient, and second we perform all the calculations in fixed-point arithmetic.  Both of these approximations can be made arbitrarily tight but at the expense of computational efficiency.  To demonstrate that our protocol can be implemented in an efficient manner and produce reasonably accurate results we implemented it in a simulator and compared the results to exact logistic regression on the CPS data.



For each of  and  we ran our first protocol 100 times. The table below shows the means and standard deviations of the resulting parameter values.  Evidently, as  gets bigger, the accuracy of the parameter values improves.  Figures \ref{fig_like1} and \ref{fig_like1a} show how the likelihood of the estimate maintained by the protocol increases with the number of iterations.  We computed the error bars   by removing the 5 samples that deviated from the mean by the greatest amount, and plotting the minimum and maximum from the remaining ones.  This then corresponds to an approximate 95\% confidence interval, and would become an exact interval if we were to perform more and more simulations.  For the purposes of comparison, we also plotted the likelihood achieved by the exact non-private Newton Raphson algorithm, and a non-private algorithm which we referred to as ``hessian lower bound.''  Both give upper bounds for what we  hope to achieve, the latter is an algorithm where we just use the approximation of (\ref{eq_hessian_lb}), and exact (i.e., non-private) logistic sigmoid values.  We see that as  increases, the first protocol more closely approximates the hessian lower bound technique, which converges more slowly than the exact Newton Raphson method.

For the second experiment, we ran our coupled iteration on the CPS data with . Although each iteration of our algorithm may be cheap, all is for nought if we require many more iterations for convergence. To determine whether this happens we compared our method to the Hessian lower bound method of (\ref{eq_hessian_lb}), since this represents our algorithm without the approximation. In figure \ref{fig_like2}, we plot the likelihoods of the second protocol against the iteration number.  Since there is no randomness in the second approximation, there are no error bars.  Even for small values of , much smaller than those suggested by (\ref{eq_choose_k}), the approximation to the Hessian lower bound technique is quite good, and increasing  further (e.g., to 50) results in curves which are exactly the same as that of the Hessian lower bound method.  In table \ref{tab:protocol2} we show the resulting parameter estimates for both methods.

\begin{table}
\begin{tabular}{|l | r | r r | r  r |  r | r | r|}
\hline

& NR & P1,  & s.d. & P1,  & s.d. & P2,  & P2, \\
\hline
Intercept & -10.7536 & -11.6306 & 1.0761 & -11.5732 & 0.5339 & -10.7944 & -11.2262 \\ \hline
Child Sup &0.0002 &  0.0002 & 0 & 0.0002 & 0 & 0.0002 & 0.0002 \\ \hline
Property Tax & 0.0003 &  0.0003 & 0 & 0.0003 & 0 & 0.0003 & 0.0003 \\ \hline
Num in Household & 0.9802  & 0.9916 & 0.0863 & 0.9881 & 0.0434 & 0.9259 & 0.9601 \\\hline
Num Children & -1.056  & -1.0721 & 0.0935 & -1.0685 & 0.047 & -1.0017 & -1.0384 \\\hline
Num Married & 0.0342  & 0.0343 & 0.0053 & 0.0343 & 0.0022 & 0.032 & 0.0333 \\\hline
Child Sup Ind. & -0.0001  & -0.0001 & 0 & -0.0001 & 0 & -0.0001 & -0.0001 \\\hline
Education & 0.3218 & 0.3276 & 0.0295 & 0.3265 & 0.0146 & 0.3058 & 0.3172 \\\hline
\multirow{4}{*}{Age} & -4.6178  & -3.9701 & 0.3451 & -3.94 & 0.1638 & -3.6202 & -3.8011 \\
& -4.2368  & -3.6782 & 0.3148 & -3.6596 & 0.1504 & -3.4817 & -3.5823 \\
& -3.9608 & -3.3355 & 0.2852 & -3.3157 & 0.135 & -3.1592 & -3.2481 \\
& -4.9575 & -4.4069 & 0.3795 & -4.3814 & 0.1829 & -4.1096 & -4.2624 \\ \hline
\multirow{6}{*}{Marital Status} &  0.0064 & 0.0051 & 0.0282 & 0.0001 & 0.012 & -0.0035 & -0.0007 \\
& -0.5362 & -0.5623 & 0.058 & -0.5562 & 0.0279 & -0.5205 & -0.5404 \\
& -0.4378 & -0.4718 & 0.0819 & -0.4609 & 0.0374 & -0.3934 & -0.4321 \\
& -0.5855 & -0.6096 & 0.0675 & -0.6018 & 0.0309 & -0.572 & -0.5889 \\
& -0.9617 & -1.0283 & 0.1146 & -1.0116 & 0.0549 & -0.957 & -0.9874 \\
& -0.7496& -0.7888 & 0.0852 & -0.7783 & 0.0401 & -0.736 & -0.7598 \\ \hline
-\multirow{3}{*}{Race} & 0.2417 & -0.2588 & 0.0276 & -0.2565 & 0.0135 & -0.2401 & -0.2491 \\
& -0.4981 & -0.5167 & 0.05 & -0.5128 & 0.0244 & -0.4835 & -0.4997 \\
& -0.1658 & -0.1796 & 0.0196 & -0.1785 & 0.0102 & -0.1631 & -0.1719 \\ \hline
Sex & -0.2763 & -0.2802 & 0.0244 & -0.2792 & 0.0125 & -0.2613 & -0.2712 \\
\hline
\end{tabular}
\caption{Estimates produced by the exact method (Newton Raphson), and the two protocols, for different parameter settings of the protocols.}
\label{tab:protocol2}
\end{table}

\begin{figure}[h]
  \centering
      \includegraphics[width=7in]{protocol1_likelihood.pdf}
  \caption{Log Likelihood vs iteration number for protocol 1 with , and that of the ``Hessian Lower bound'' algorithm, which is the same as protocol 1 except with exact sigmoid evaluations.  We also compare to the full newton raphson method, which inverts the hessian on each iteration.}
\label{fig_like1}
\end{figure}


\begin{figure}[h]
  \centering
      \includegraphics[width=7in]{protocol1a_likelihood.pdf}
  \caption{Log Likelihood vs iteration number for protocol 1 with , and that of the ``Hessian Lower bound'' algorithm, which is the same as protocol 1 except with exact sigmoid evaluations.  We also compare to the full newton raphson method, which inverts the hessian on each iteration.}
\label{fig_like1a}
\end{figure}


\begin{figure}[h]
  \centering
      \includegraphics[width=7in]{protocol2_likelihood.pdf}
  \caption{Log Likelihood vs iteration number for protocol 2 with , and that of the ``Hessian Lower bound'' algorithm, which is the same as protocol 1 except with exact sigmoid evaluations.  We also compare to the full newton raphson method, which inverts the hessian on each iteration.}
\label{fig_like2}
\end{figure}


\section{Beyond Logistic Regression}\label{sec:extension}

We can use the construction of Section \ref{sec:protocol1} to build secure protocols for similar statistical calculations, e.g.,  the constructions for computing shares of outer products and matrix inverses naturally yield a secure algorithm for performing linear regression, for details see~\cite{fhn:10}.  Furthermore using the ``ridge regression'' penalty on the weights (i.e., computing a MAP estimate under a Gaussian prior) can naturally be added to the protocol for both linear and logistic regression.  It is also possible to implement the coordinate ascent computation of the lasso (or sparse logistic regression) using these constructions (i.e., using the GT protocol to perform soft thresholding).

Our protocol generalizes to the class of Generalized Linear Models (GLMs) including logistic regression with other link functions.  GLMs consist of a random component  from an exponential family, a systematic component with a linear predictor , and a link function , where . If  makes the linear predictor , where  is the natural parameter of the exponential family,   is canonical.




For Poisson log-linear models with the canonical link, , we approximate the exponential function similarly. For Gamma models with the canonical link, , and for inverse-Gaussian models with the canonical link, ,  we can use the number inverting without division scheme. Our approach can also be extended to treat binary regression with non-canonical links, such as the \emph{probit} link function, or more generally, inverse CDF link functions. The general form of the gradient is:

Let  denote a given CDF ( leads to the probit link function, while, of course,  leads to the logit link function). Then, , and thus , where  is the density. Therefore, we should find approximations for  as well as for  (approximation for  will follow the same idea as for , i.e., using the empirical CDF).

\section{Conclusion}

We have demonstrated that a fully secure approach to logistic regression based on the cryptographic notion of security may be made practical for use on moderately large datasets shared between several parties.  Although it is slower than methods with weaker security guarantees, it offers more rigorous guarantees with respect to the privacy of the input data.  We emphasize that our protocol (like any cryptographic protocol) prevents leakage of information which may arise from the computation itself.  It does not address any leakage which results from the output.

The problem of secure regression is far from solved however, we have yet to deal with the problem of record linkage, and have implicity assumed that the parties know how their respective datasets are aligned.  Furthermore record linkage due to a statistical model may be incorrect and may result in errorful estimates of model parameters. 


\appendix
\section{Theoretical Validity of the First Protocol}\label{sec:validity_1}

Here we show how a bound on the error in the approximation (\ref{sigma-appox}) to the logistic function leads to a bound on the quality of the convergent parameter vector output by the protocol. Specifically, we establish the validity of (\ref{parm_err_bound}). Let  denote a constant such that , for . Recall the expressions for the gradient  and Hessian  given in (\ref{grad-hess}). Define the approximated gradient, by substituting  for :

Rewriting ,
and applying the triangle inequality we obtain a bound on the norm of the gradient of the logistic objective:

Next we  convert a bound in the norm of the gradient into a bound on the distance to the optimum.  











\begin{lem}\label{lem_meanval}
Let  be the optimizer of the logistic regression objective, and let  denote the smallest eigenvalue of the negative Hessian in the line segment between  and . Then:

\end{lem}

\begin{proof}
We  use the mean-value theorem (for vector-valued functions) to write the difference between gradient vectors at  and :

Now, for every (symmetric) matrix , and a non-zero vector , the Rayleigh quotient satisfies , where  is the minimal eigenvalue of . If , for a positive definite (symmetric) matrix , this reduces (after taking the square root on both sides) to .  Applying this to (\ref{eq:mean-value}), and using Weyl's inequality, we have:

This completes the proof.
\end{proof}

\begin{lem}\label{lem_gradmin}
Using the same notation we have:

where  is a (non-empty) set of logistic parameters defined in the proof.
\end{lem}
\begin{proof}
Consider a continuous, monotonically non-decreasing function  which satisfies .  Such a function clearly exists, for example the smooth nondecreasing curve which goes through all points  where  (where  is  smallest logistic variable used in ).  Since  is nondecreasing, it is the derivative of some convex function:

Consider the approximation to the logistic gradient which uses  instead of :

This is the derivative of a concave function:

which is indeed concave since it is a linear function minus a convex function.  Hence  has a unique maximum somewhere. Consider the functions  so that the maximum is in the interior of the space  (i.e., is not at infinity).  Hence for each such  we have a point  where the gradient is zero, i.e., . Denote the set of such  by , and note that  is not empty. 
An argument similar to the one that led to (\ref{exactnorm_bound}) shows that:

Therefore:

which completes the proof.
\end{proof}

We now put this all together and state the main result about our approximation .

\begin{lem}
If our approximation  is used as an approximation to the gradient of the logistic log likelihood, and numerical optimization is performed until , then:

where  is the optimizer of the exact logistic regression objective,  is the result of our numerical optimization,  is the radius of a ball containing all the , and  is the smallest eigenvalue of the Fisher information matrix  in the line segment between  and . \end{lem}
\begin{proof}
Notice that  is guaranteed in light of Lemma \ref{lem_gradmin}. The proof follows by substituting (\ref{exactnorm_bound}) into (\ref{distance_bound}), and by noticing that  and  the factors of  cancel.
\end{proof}


\section{Theoretical Validity of the Coupled Iteration}\label{sec:validity_2}

Here we establish the convergence of the coupled iteration (\ref{eq:coupled}), and the error in our Taylor approximation of the logistic function.


\subsection{Monotonicity and Convergence}\label{sec:convergence}

We show that the update described in (\ref{eq:coupled}) converges monotonically towards some final value .  We  relate the size of the step taken at one iteration  to the size of the step in the previous iteration.  We aim to show that first, these steps are always in the same directions for each unit, and secondly, the steps are monotonically decreasing and eventually the iterations converge.

\begin{lem}
 element-wise has the same sign as , in the sense that .
\end{lem}
\begin{proof}
If we define the idempotent matrix , then we write:

where we made use of the idempotency of .  Next considering the element-wise product as the diagonal of the outer product of these two matrices,



Since we clearly have that  no matter what value  takes (due to the definition of ), we have that this matrix is the product of positive semi-definite matrices, and therefore is itself positive semi-definite.  Therefore the diagonal elements are all non-negative, and we have proved the claim.
\end{proof}

This result allows us to analyze our approximation to the logistic function as though we were using the forwards Euler method to integrate the differential equation (\ref{logistic_deriv}), since all the steps for any particular unit will be in the same direction.

\begin{lem}
As long as each step  (where the inequality is element-wise), then  (i.e., the approximate logistic values will remain between 0 and 1).
\end{lem}
\begin{proof}
Suppose that the step is positive for all units and , then:

so we also have that  .  Likewise for units which are involved in a negative step, if they are greater than 0, then they remain so into the next iteration by an argument which is symmetric to the one above.  Therefore we have that our logistic values never leave the interval .
\end{proof}

With this we also have that  for all , from the definition of  and .  Substitution into (\ref{stepsize_rel}), yields that:

since  has eigenvalues which are each either 0 or 1.  This shows that the magnitude of the steps for the individual units is shrinking towards zero.  Therefore we  conclude that eventually, our approximations of the logistic values stop updating.  If we assume that  has  linearly independent columns, then this also implies that  is going towards zero, and therefore our algorithm eventually converges.

\subsection{Quality of the Logistic Approximation}\label{sec:p2error}

We now analyze the error in the approximation of the logistic function values.  We then use this together with  the convexity of the problem to  yield a bound on the error in the convergent parameters (see (\ref{eq:error_2})).  To aid the notation, in this section we consider the problem of estimating the logistic values for just a single case, and specifically one for which the steps are all positive.  Due to the symmetry of the logistic function about 0, we will then have the same type of bounds on the error when the approximation updates in the negative direction.  We first show a loose upper bound on the supremum of the error which would be encountered if the approximation was run for an infinite number of steps of size at most , and then use this to bound the error after finitely many such steps.

As we have shown by the above monotonicity argument, our approximation to the logistic function is essentially analogous to using Euler's method to integrate the derivative of the logistic function.  Since we consider approximating a single value, we change the names of our variables to avoid confusion with the previous vector valued approximation.  If we denote by  the approximated value after  steps of various sizes,  .  Thus   where . We  compare this approximation to the exact values and consider the error:


Making use of the step (\ref{sigma_lin}), we  evaluate the error in the next iteration:

Where we have defined


\noindent and  is some value in the interval about which the second derivative is taken.  Likewise  is bounded between  and .  As we have seen from (\ref{approx_sigma_bound}), as long as  then  for all .  Since we only consider positive steps  then we have that , and hence the same bound applies to .  Therefore we have that:

Therefore we see that:


Examining the form of , we find it to be a function which is everywhere negative.  Examining the third derivative, we find that the second derivative has exactly one stationary point in  which is located at:



Whats more, we see that  on , and  on .  Therefore we have that  is the minimum of the function.  Using this we  bound the sum (\ref{bound_sum}) by an integral:


Substituting this into (\ref{bound_sum}) and (\ref{eqn:zeta}) we have that:

We can make  the approximation arbitrarily tight by decreasing the step size.




\newpage
\begin{thebibliography}{99}

 

\bibitem[Aggarwal  and  Yu, 2008]{aggarwal} Aggarwal, C.  and  Yu, P. S. eds. (2008). \textit{Privacy Preserving Data Mining: Models and Algorithms}.   Springer-Verlag, New York.

\bibitem[Blake and Kolesnikov, 2004]{gt_proto} Blake, I. and Kolesnikov, V. (2004). Strong conditional oblivious transfer and computing on intervals. In \textit{Advances in Cryptology---ASIACRYPT 2004}, 515--529.

\bibitem[Boyd and Vandeberghe, 2004]{cvx_book} Boyd, S. and Vandenberghe, L. (2004).  \textit{Convex Optimization.}  Cambridge University Press, New York.

\bibitem[Chaudhuri and Monteleoni, 2008]{Chaudhuri} Chaudhuri, K. and  Monteleoni, C. (2008).  Privacy-preserving logistic regression. \textit{NIPS 2008},  289--296.

\bibitem[Fienberg, et al., 2006]{fien_secure}Fienberg, S. E.,  Fulp, W. J., and Slavkovic, A. B. and Wrobel, T. A. (2006).  ``{S}ecure'' log-linear and logistic regression analysis of distributed databases. \textit{Privacy in Statistical Databases: CENEX-SDC Project International Conference, PSD 2006}, 277--290.

\bibitem[Fienberg et al., 2011]{fhn:10} Fienberg, S. E., Hall, R. and Nardi, Y. (2010).  Secure multiple linear regression based on homomorphic encryption."  Submitted for publication.

\bibitem[Fienberg, et al., 2009]{fien_secure2}Fienberg, S. E., Slavkovic, A. B., and  Nardi, Y. (2009).  Valid statistical analysis for logistic regression with multiple sources. In P. Kantor and M. Lesk, eds.,  \textit{Proc. Workshop on Interdisciplinary Studies in Information Privacy and Security---ISIPS 2008}, USA  LNCS volume, Springer-Verlag, New York.


\bibitem[Goethals et al., 2004]{goethals}Goethals, B., Laur, S., Lipmaa, H., Mielikainen, T. (2004). On secure scalar product computation for privacy-preserving data mining. \textit{ISISC, 2004}.

\bibitem[Goldreich, 2004]{goldreich}Goldreich, O. (2004).  \textit{Foundations of Cryptography: Volume 2 Basic Applications.} Cambridge University Press, New York.

\bibitem[Goldwasser, 1997]{goldwasser97}Goldwasser, S. (1997). Multi-party computations: {P}ast and present. \textit{Proceedings of the 16th Annual ACM Symposium on Principles of Distributed Computing}, 1--6.

\bibitem[Guo and Higham, 2006]{guo_higham}Guo, C.  and Higham N. J. (2006). A Schur-Newton method for the matrix p'th root and its inverse. \textit{SIAM Journal on Matrix Analysis and Applications}, 28(3), 788--804.

\bibitem[Jagannathan and Wright, 2008]{jagannathan}Jagannathan, G. and Wright, R. (2008) Privacy-preserving imputation of missing data.  \textit{Data Knowl. Eng.}, 65(1), 40--56.

\bibitem[Karr et al., 2005]{securereg-jcgs05}Karr A.F., and  Lin, X., and  Reiter, J.P. and  Sanil, A .P. (2005) Secure regression on distributed databases. \textit{Journal of Computational and Graphical Statistics}, 14(2),  263--279.

\bibitem[Karr et al., 2006]{securereg-springer06}Karr A.F., and  Lin, X., and  Reiter, J.P. and  Sanil, A .P. (2006) Secure analysis of distributed databases.  In D. Olwell and A. G. Wilson and G. Wilson, eds.,
{\it Statistical Methods in Counterterrorism: Game Theory, Modeling, Syndromic Surveillance, and Biometric Authentication}, Springer-Verlag, New York, 237--261.


\bibitem[Lindel and Pinkas, 2002]{lindell_pinkas_2}Lindell, Y. and Pinkas, B. (2002). Privacy preserving data mining. \textit{Journal of Cryptology}, 15(3), 177--206.

\bibitem[Lindel and Pinkas, 2008]{lindell_pinkas_1}Lindell, Y. and Pinkas, B. (2009). Secure multiparty computation for privacy-preserving data mining. \textit{Journal of Privacy and Confidentiality}, 1(1), 59--98.

\bibitem[Malkhi et al., 2004]{fairplay} Malkhi, D. and Nisan, N. and Pinkas, B. and Sella, Y. (2004). Fairplay: A secure two-party computation system. \textit{Proceedings of the 13th conference on USENIX Security Symposium ---Volume 13}, 20--20.


\bibitem[Minka, 2003]{minka}Minka, T. (2003). A comparison of numerical optimizers for logistic regression.  Unpublished manuscript.


\bibitem[Paillier, 1999]{paillier}Paillier, P. (1999). Public-key cryptosystems based on domposite degree residuosity classes.  \textit{EUROCRYPT 1999}, 223--238.

\bibitem[Vaida et al., 2005]{ppdm_book}Vaidya, J. and Zhu, Y. and Clifton, C. (2005).  \textit{Privacy Preserving Data Mining}.  Springer-Verlag, New York.


\bibitem[Yao, 1982]{yao82}Yao, A. C. (1982). Protocols for secure computations. \textit{Proceedings of the 23rd Annual IEEE Symposium on Foundations of Computer Science}, 160--164.



\bibitem[Yao, 1986]{yao_secret}Yao, A. C. (1986). How to generate and exchange secrets. \textit{Proceedings of the 27th Symposium on Foundations of Computer Science (FOCS), IEEE}, 162--167.



\end{thebibliography}









\end{document}
