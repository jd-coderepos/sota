

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  




\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{bbding}
\usepackage{threeparttable}

\usepackage{dsfont}
\usepackage{caption}



\usepackage{cite} 

\usepackage{amssymb}
\usepackage{amsbsy}
\newcommand{\bfline}[1]{\textbf{\underline{#1}}}

\usepackage[table]{xcolor}

\usepackage{booktabs} \usepackage{makecell}
\usepackage{multirow}
\usepackage{overpic}
\usepackage{diagbox}

\usepackage{adjustbox}
\usepackage{array}

\usepackage{verbatim}




\usepackage{color, soul}
\sethlcolor{blue}
\newcommand{\hlbr}[1]{{\sethlcolor{red}\hl{#1}}}
\newcommand{\hlbb}[1]{{\sethlcolor{blue}\hl{#1}}}
\newcommand{\hlbg}[1]{{\sethlcolor{green}\hl{#1}}}
\newcommand{\hlby}[1]{{\sethlcolor{yellow}\hl{#1}}}
\newcommand{\hlbp}[1]{{\sethlcolor{pink}\hl{#1}}}
\newcommand{\hlbc}[1]{{\sethlcolor{cyan}\hl{#1}}}

\newcommand{\hlr}[1]{{\color{red}{#1}}}
\newcommand{\hlb}[1]{{\color{blue}{#1}}}
\newcommand{\hlg}[1]{{\color{green}{#1}}}
\newcommand{\hly}[1]{{\color{yellow}{#1}}}
\newcommand{\hlp}[1]{{\color{pink}{#1}}}
\newcommand{\hlc}[1]{{\color{cyan}{#1}}}


\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\btheta}{\mathbf{\theta}}
\newcommand{\dbtheta}{\Delta\mathbf{\theta}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bP}{\mathbf{P}}

\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      







\title{\LARGE \bf
CurveFormer: 3D Lane Detection by Curve Propagation with Curve Queries and Attention
}





\author{Yifeng Bai, Zhirong Chen, Zhangjie Fu, Lang Peng, Pengpeng Liang and Erkang Cheng \thanks{Yifeng Bai, Zhirong Chen, Zhangjie Fu, Lang Peng and Erkang Cheng are with NullMax, Shanghai, 201210, China. Pengpeng Liang is with School of Computer and Artificial Intelligence, Zhengzhou University, 450001, China.}
	\thanks{ Work done during an internship at NullMax.}
	\thanks{ Equal contribution. *Corresponding author.}
}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
3D lane detection is an integral part of autonomous driving systems. Previous CNN and Transformer-based methods usually first generate a bird's-eye-view (BEV) feature map from the front view image, and then use a sub-network with BEV feature map as input to predict 3D lanes. Such approaches require an explicit view transformation between BEV and front view, which itself is still a challenging problem. In this paper, we propose CurveFormer, a single-stage Transformer-based method that directly calculates 3D lane parameters and can circumvent the difficult view transformation step. Specifically, we formulate 3D lane detection as a curve propagation problem by using curve queries. A 3D lane query is represented by a dynamic and ordered anchor point set. In this way, queries with curve representation in Transformer decoder iteratively refine the 3D lane detection results. Moreover, a curve cross-attention module is introduced to compute the similarities between curve queries and image features. Additionally, a context sampling module that can capture more relative image features of a curve query is provided to further boost the 3D lane detection performance. We evaluate our method for 3D lane detection on both synthetic and real-world datasets, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. The effectiveness of each component is validated via ablation studies as well.
\end{abstract}

\section{INTRODUCTION}
Lane detection is a critical component of an autonomous driving system, and it plays an important role in lane keeping assist, lane departure warning, etc. Most of the current lane detection approaches are developed with 2D images using semantic segmentation~\cite{pan2017spatial, neven2018towards, hou2019learning, zheng2020resa} or line regression~\cite{ko2020key, wang2022keypoint, chen2019pointlanenet, li2020curvelane, li2019line, 2020Keep, zheng2022clrnet, qin2020ultra}. However, downstream tasks like planning and control prefer lanes that are represented by the curve parameters in 3D space. Subject to the lack of depth information and accurate real-time camera extrinsic parameters, the projection from the image plane to the BEV perspective is prone to the error propagation problem
(as shown in Fig.~\ref{fig_compare_methods} (a)). Additionally, these methods suffer from complex and time-consuming post-processing steps, such as cluster and curve fitting. 

\begin{figure}[ht]
 \centering
  \begin{tabular}{c}
   \includegraphics[width=0.8\linewidth]{fig/fig1_a.jpg}\\
   \begin{scriptsize}  
   (a) Image prediction \& post-processing. 
   \end{scriptsize} \\
   \includegraphics[width=0.8\linewidth]{fig/fig1_b.jpg}\\
   \begin{scriptsize}  
   (b) CNN-based dense BEV \& prediction.
   \end{scriptsize}  \\
   \includegraphics[width=0.8\linewidth]{fig/fig1_c.jpg}\\
   \begin{scriptsize}  
   (c) Transformer-based dense BEV \& prediction. 
   \end{scriptsize}  \\
   \includegraphics[width=0.8\linewidth]{fig/fig1_d.jpg}\\
   \begin{scriptsize}  
   (d) Transformer-based sparse 3D lane detection by curve query.
   \end{scriptsize}  \\
 \end{tabular}
 \caption{Comparisons of different 3D lane detection pipelines. (a) 2D image prediction and post-processing; (b) 3D lane detection with camera extrinsic prediction; (c) Transformer-based dense BEV map construction and 3D lane prediction; (d) Our proposed CurveFormer, directly provides 3D lane parameters by sparse curve queries with curve cross-attention mechanism in Transformer decoder.} 
 \label{fig_compare_methods}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\linewidth]{fig/fig3.jpg}
	\caption{Illustration of the curve query representation with dynamic anchor point set (a) and iterative curve propagation the image view (b).}
	\label{fig:representation}
\end{figure}

In order to mitigate the drawbacks of post-processing in two-stage methods, CNN-based approaches have been proposed for end-to-end 3D lane detection task~\cite{garnett20193d,efrat20203d,guo2020gen,yan2022once}. As shown in Fig.~\ref{fig_compare_methods} (b), 3D-LaneNet~\cite{garnett20193d} proposes an anchor-based 3D lane representation and predicts camera pose to project 2D features with Inverse Projective Mapping (IPM). 3D-LaneNet+~\cite{efrat20203d} reformulates 3D lane as an anchor-free 
representation to consider the restriction of the lane direction, and learns lane curve clustering in the network. Gen-LaneNet~\cite{guo2020gen} proposes a virtual top view to align the BEV features projected by IPM and lanes in the real-world. Although these methods make end-to-end 3D lane detection possible, the loss of lane height and the accuracy of camera pose estimation would affect the robustness of these methods. On the contrary, ONCE~\cite{yan2022once} performs 2D lane semantic segmentation and depth estimation, and integrates these information to obtain 3D lanes. A problem of ONCE is that  depth estimation might bring about errors at the far end of the lane.

Recently, inspired by the successes of Transformer in various vision and robotic tasks~\cite{dosovitskiy2020image, carion2020end,wang2022detr3d,peng2022bevsegformer},
several Transformer-based lane detection algorithms~\cite{liu2021end,liu2022learning,chen2022persformer} have been proposed. LSTR~\cite{liu2021end} introduces Transformer to the lane detection task and predicts 2D lane parameters directly. But it encounters difficulties in representing sharp curves or lanes with complex topology. STSU~\cite{can2021structured} follows the sparse query-based framework~\cite{wang2022detr3d} to produce topologically accurate lane graphs. Similar to the above CNN-based 3D methods, CLGo~\cite{liu2022learning} applies Transformer to enhance images features and predicts 3D lanes from distance-invariant top-view image in the second stage. PersFormer~\cite{chen2022persformer} builds a dense BEV query and uses Transformer to interact queries from BEV with image features (as shown in Fig.~\ref{fig_compare_methods} (c)). Although these methods try to utilize Transformer to the 3D lane detection task, the lack of image depth or BEV map height restricts their performance as they can not obtain the features that exactly correspond to the query. 

To address the above challenges, we propose CurveFormer, a Transformer-based method for 3D lane detection (Fig.~\ref{fig_compare_methods} (d)). Lanes are defined as sparse curve queries consisting of lane confidence, two polynomials and start and end points (Fig.~\ref{fig:representation} (a)). Inspired by DAB-DETR~\cite{liu2022dab}, we propose a set of 3D dynamic anchor points to interact curve queries with image features. Since the 3D anchor point  has height information, we can use camera extrinsic parameters to obtain accurate image features corresponding to the point. Dynamic anchor point set is iteratively refined within the sequence of Transformer decoders. We introduce a novel curve cross-attention module in the decoder part to investigate the effect of curve queries and dynamic anchor point set. Different from standard Deformable-DETR~\cite{zhu2020deformable } that directly predicts sampling offsets from the query, we introduce a context sampling unit to predict offsets from the combination of reference features and queries to guide sampling offsets learning. 
In addition, an auxiliary segmentation branch is adopted to enhance the shared CNN backbone. In this way, our design of CurveFormer lends itself to 3D lane detection.

To verify the performance of the proposed algorithm, we evaluate our CurveFormer on the Apollo Synthetic dataset~\cite{guo2020gen} and OpenLane dataset~\cite{chen2022persformer}. Our proposed CurveFormer sets a new state-of-the-art performances for 3D lane detection on the Apollo Synthetic test set. It also achieves promising performance on the OpenLane dataset compared with recently proposed Transformer-based 3D lane detection approaches. The effectiveness of each component is validated as well.

In general, our main contributions are three-fold:
\begin{itemize}
    \item We propose CurveFormer, a novel Transformer-based 3D lane detection algorithm, by formulating queries in decoder layers as dynamic anchor point set, and a curve cross-attention module is applied to compute the query-to-image similarity.
    
    \item We introduce a context sampling unit to predict offsets from the combination of reference features and queries to guide sampling offsets learning.
    
    \item Experimental results show that our method achieves promising performance compared with both CNN-based and Transformer-based state-of-the-art approaches.
\end{itemize}


\section{RELATED WORK}

\begin{figure*}[htb]
    \vspace{2mm}
	\centering
	\includegraphics[width=0.75\linewidth]{fig/pipelineV1.6.jpg}
		\caption{Overview of our proposed CurveFormer.}
	\label{fig:overview}
	\vspace{-5mm}
\end{figure*}

\noindent\textbf{2D Lane Detection.} It detects lanes in the image plane and projects them to 3D space with camera pose. In general, advanced monocular lane detectors can be categorized into segmentation approaches~\cite{pan2017spatial, neven2018towards,hou2019learning,zheng2020resa} 
and regression approaches~\cite{ko2020key, wang2022keypoint, chen2019pointlanenet,li2020curvelane,  li2019line,2020Keep,zheng2022clrnet,qin2020ultra, han2022laneformer}.


SCNN~\cite{pan2017spatial} is proposed to propagate context by slice-by-slice convolutions within feature maps. 
LaneNet~\cite{neven2018towards} introduces an instance segmentation approach for lane detection which combines a binary segmentation branch and an embedding branch. SAD~\cite{hou2019learning} allows a lane detection network to reinforce representation learning of itself without the need of additional labels and external supervisions. RESA~\cite{zheng2020resa} aggregates information in vertical and horizontal directions by shifting sliced feature maps recurrently.

Lane regression algorithms can be grouped into key points estimation~\cite{ko2020key, wang2022keypoint}, anchor-based regression~\cite{chen2019pointlanenet,li2020curvelane,  li2019line,2020Keep,zheng2022clrnet} and row-wise regression~\cite{qin2020ultra, han2022laneformer}. 
PINet~\cite{ko2020key} combines key points estimation and instance segmentation, and GANet~\cite{wang2022keypoint} represents lanes as a set of key points which are only related to the start point.
PointLaneNet~\cite{chen2019pointlanenet} and CurveLane-NAS~\cite{li2020curvelane} separate images into non-overlapping grids and regress lanes based on vertical anchors.
Line-CNN~\cite{li2019line} and LaneATT~\cite{2020Keep} regress lanes on the pre-defined ray-anchors, while CLRNet~\cite{zheng2022clrnet} dynamically refines the start point and angle of ray-anchors through pyramidal features. 
Ultra-Fast~\cite{qin2020ultra} introduces a novel row-wise classification method with remarkable speed.
Laneformer~\cite{han2022laneformer} applies row-column self-attentions to accommodate the conventional Transformer to capture the shape characteristics and semantic contexts of lanes.

Except for point regression, polynomial regression is also a method for 2D lane detection task. PolyLaneNet~\cite{tabelini2020polylanenet} uses a fully connected layer to directly predict the polynomial coefficients of lanes in the image plane. PRNet~\cite{wang2020polynomial} decomposes lane detection into three parts: polynomial regression, initial classification and height regression. Method in~\cite{van2019end} applies IPM and least square fitting to predict parabolic equations in BEV perspective. LSTR~\cite{liu2021end} introduces a Transformer-based network to predict lane parameters which reflect road structures and the camera pose.

\noindent\textbf{3D Lane Detection.} 3D lane detection has attracted more attention than its 2D counterpart recently, and a reason is that the results of the latter lack depth information and spatial transformations have the error propagation problem. 3D-LaneNet~\cite{garnett20193d} is a dual-pathway architecture based on intra-network inverse-perspective mapping and anchor-based lane representation. 3D-LaneNet+~\cite{efrat20203d} divides the BEV features into non-overlapping cells and detects lanes by regressing the lateral offset distance relative to the cell center, line angle and height offset. Method in~\cite{efrat2020semi} introduces uncertainty estimation in order to enhance the capabilities of the network. Gen-LaneNet~\cite{guo2020gen} first introduces a new geometry-guided lane anchor representation in virtual top-view coordinate frame rather than the ego-vehicle coordinate frame, and applies a specific geometric transformation to calculate 3D lane points from the network output directly. CLGo~\cite{liu2022learning} replaces the CNN backbone with Transformer to predict camera pose and polynomial parameters. PersFormer~\cite{chen2022persformer} builds a dense BEV query with known camera pose, and unifies 2D and 3D lane detection under one framework.


\section{METHOD}


\subsection{Overview}
Fig.~\ref{fig:overview} shows the overview of our CurveFormer. It consists of three major components: (1) a Shared CNN Backbone takes a single front-view image as input and outputs multi-scale feature maps; (2) a Transformer Encoder to enhance the multi-scale feature maps subsequently and (3) a curve Transformer Decoder to propagates curve queries by curve cross-attention and iteratively refine anchor point sets. Finally, a prediction head is applied to output 3D lane parameters. The -th output can be represented as ,
where  is the foreground confidence,  and  are start and end point in the  direction. Two polynomials of 3D lane are denoted by  and  with order  to model a traffic lane in X-O-Y and Y-O-Z plane, respectively.

\subsection{Shared Backbone and Transformer Encoder}
The backbone takes an input image and outputs multi-scale feature maps. We add an auxiliary segmentation branch in the training stage to enhance the shared CNN backbone.

Similar to~\cite{zhu2020deformable}, in the decoder part, we apply multi-scale deformable self-attention module for each scale feature map to exchange information among different scales. The multi-scale feature maps are written as .

\subsection{Representing Sparse Curve Query with Dynamic Anchor Point Set}

DAB-DETR~\cite{liu2022dab} provides a deep analysis of the role of queries for rectangle object detection which models queries as anchor boxes, i.e. 4D coordinates (x,y,w,h). Therefore, in the cross-attention module, it can leverage both the position and size information of each anchor box. Inspired by DAB-DETR, we represent queries in Transformer-based 3D lane detection with dynamic anchor point sets. As shown in Fig.~\ref{fig:representation} (a), these points are sampled at a set of fixed  locations. Typically, we denote  as the -th anchor curve.
Its corresponding content part and positional part are  and , respectively. The positional query  is calculated by:


where positional encoding (PE) generates embeddings from floating numbers, and the parameters of the MLP are shared among all layers. 

\begin{figure}[ht]
\centering
  \begin{tabular}{c}
   \includegraphics[width=0.9\linewidth]{fig/fig3_a.jpg}\\
   \begin{scriptsize}  
   (a)
   \end{scriptsize} \\
   \includegraphics[width=0.9\linewidth]{fig/fig3_b.jpg}\\
   \begin{scriptsize}  
   (b)
   \end{scriptsize}  \\
 \end{tabular}
\caption{Illustration of the Context Sampling Module. (a) Deformable DETR\cite{zhu2020deformable} predicts the reference points and sampling offsets by position embedding and query separately. (b) Our context sampling module learns sampling offset by leveraging both query and image features.}
\label{fig:context-sampling}
\end{figure}

By representing a curve query as an ordered anchor point set , we can refine the curve query layer-by-layer in the Transformer decoder. Specifically, each Transformer decoder estimates relative positions  by a shared parameters linear layer. In this way, the curve query representation is suitable for 3D lane detection and is able to accelerate the learning convergence via layer-by-layer refinement scheme. Fig.~\ref{fig:representation} (b) shows the iterative refine process in the image plane.

\subsection{Curve Transformer Decoder}


Our curve Transformer decoder contains a multi-head self-attention module, a context sampling module and a curve cross-attention module. 
We apply deformable attention~\cite{zhu2020deformable} in the self-attention module which focuses on a small set of key sampling points around the reference point, regardless of the spatial size of the feature map.

\noindent\textbf{Context Sampling Module} In deformable DETR~\cite{zhu2020deformable}, a learnable linear layer is used to predict offsets of the sampling locations corresponding to the reference points by queries, which are irrelevant to the image features. 
Different from it, we introduce a context sampling module to predict sampling offsets by incorporating more relative image features.
Fig.~\ref{fig:context-sampling} illustrates the difference between the standard sample offset module (a) and our context sampling module (b). 

First, a dynamic anchor point set  is projected to the image view with camera parameters. We apply bilinear interpolation to extract features from these projected points  on multi-scale feature maps . 
The final feature  is computed by

where  is used to determine whether a projected point  is outside -th feature map. And  is a small number to avoid division by zero.

We then use a learnable linear layer to predict  sampling offsets. Typically, for a curve query  with anchor point set , the context sampling module denotes as:

where  and .

\noindent\textbf{Curve Cross Attention.} We adapt deformable attention module in Deformable
DETR~\cite{zhu2020deformable} to our curve cross-attention module.
Mathematically, let  be a query element in , and its anchor point set , our curve cross-attention is calculated as:

where  index the attention head, feature level and the sampling point.  and  denote sampling offsets and attention weights of the -th sampling point in the -th feature level and the -th attention head. The scalar attention weight  is normalized to sum as 1.  re-scales the normalized coordinates to input feature maps.


\subsection{Curve Training Supervision}


\begin{table*}[t]
\centering
\begin{scriptsize}
    \caption{Comparison with previous methods on Apollo 3D Lane Synthetic Dataset. CurveFormer achieves best F-Score and AP and promising performance of X/Z error (m) on every scene set. error near and error far represents average offset within ,  along Y axis.}
    \label{tab:results-apollosim}
    \begin{tabular}{c|c|ccccccc}
    \toprule
    Dateset Splits & Methods & F-Score & AP & X error near & X error far & Z error near & Z error far \\
      \midrule
      \multicolumn{1}{c|}{\multirow{5}{5em}{Balaneced Scenes}}
     & 3D-LaneNet\cite{garnett20193d}   & 86.4 & 89.3 & 0.068 & 0.477 & 0.015 &  \textbf{0.202} \\
     & Gen-LaneNet\cite{guo2020gen}  & 88.1 & 90.1 & 0.061 & 0.496 & 0.012 & 0.214 \\
     & CLGo\cite{liu2022learning}         & 91.9 & 94.2 & 0.061 & 0.361 & 0.029 & 0.250 \\
     & PersFormer\cite{chen2022persformer}   & 92.9 & - &  \textbf{0.054} & 0.356 &  \textbf{0.010} & 0.234 \\
     & CurveFormer (ours) &  \textbf{95.8} &  \textbf{97.3} & 0.078 &  \textbf{0.326} & 0.018 & 0.219 \\
      \midrule
      \multicolumn{1}{c|}{\multirow{5}{5em}{Rarely Observed}} 
      & 3D-LaneNet\cite{garnett20193d}   & 72.0 & 74.6 & 0.166 & 0.855 & 0.039 & \textbf{0.521} \\
      & Gen-LaneNet\cite{guo2020gen}  & 78.0 & 79.0 & 0.139 & 0.903 & 0.030 & 0.539 \\
      & CLGo\cite{liu2022learning}         & 86.1 & 88.3 & 0.147 & \textbf{0.735} & 0.071 & 0.609 \\
      & PersFormer\cite{chen2022persformer}   & 87.5 & - & \textbf{0.107} & 0.782 & \textbf{0.024} & 0.602 \\
      & CurveFormer (ours)         & \textbf{95.6} & \textbf{97.1} & 0.182 & 0.737 & 0.039 & 0.561 &  \\
      \midrule
      \multicolumn{1}{c|}{\multirow{5}{5em}{Vivual Variants}} 
      & 3D-LaneNet\cite{garnett20193d}   & 72.5 & 74.9 & 0.115 & 0.601 & 0.032 & \textbf{0.230} \\
      & Gen-LaneNet\cite{guo2020gen}  & 85.3 & 87.2 & \textbf{0.074} & 0.538 & \textbf{0.015} & 0.232 \\
      & CLGo\cite{liu2022learning}         & 87.3 & 89.2 & 0.084 & 0.464 & 0.045 & 0.312 \\
      & PersFormer\cite{chen2022persformer}   & 89.6 & - & \textbf{0.074} & 0.430 & \textbf{0.015} & 0.266 \\
      & CurveFormer (ours)         & \textbf{90.8} & \textbf{93.0} & 0.125 & \textbf{0.410} & 0.028 & 0.254 &  \\
      \bottomrule
    \end{tabular}
\end{scriptsize}
\end{table*}


\begin{table*}[t]
  \begin{center}
  \begin{scriptsize}
    \caption{Performance comparison with other state-of-the-art 3D lane methods on OpenLane benchmark. CurveFormer outperforms previous 3D methods on five scenario sets.}
    \label{tab:results-openlane-scenario-set}
    \begin{tabular}{cccccccc}
    \toprule
      Method & All & Up\&Down & Curve & Extreme Weather & Night & Intersection & Merge\&Split \\
      \midrule
      3D-LaneNet\cite{garnett20193d} & 44.1 & 40.8 & 46.5 & 47.5 & 41.5 & 32.1 & 41.7 \\
      Gen-LaneNet\cite{guo2020gen} & 32.3 & 25.4 & 33.5 & 28.1 & 18.7 & 21.4 & 31.0 \\
      PersFormer\cite{chen2022persformer} & \textbf{50.5} & 42.4 & 55.6 & 48.6 & 46.6 & 40.0 & \textbf{50.7} \\
      CurveFormer (ours) & \textbf{50.5} & \textbf{45.2} & \textbf{56.6} & \textbf{49.7} & \textbf{49.1} & \textbf{42.9} & 45.4\\
      \bottomrule
    \end{tabular}
      \end{scriptsize}
  \end{center}
\end{table*}

\begin{table}[t]
  \begin{center}
    \caption{Comprehensive 3D Lane evaluation on OpenLane under the same metrics as Apollo 3D Synthetic. CurverFormer outperforms previous 3D methods on the metrics of near error and achieves comparable results on far error.}
    \label{tab:results-openlane-val-set}
    \begin{tiny} 
    \begin{tabular}{cccccc}
    \toprule
      Method & F-Score & X error near & X error far & Z error near & Z error far \\
      \midrule
      3D-LaneNet\cite{garnett20193d} & 44.1 & 0.479 & 0.572 & 0.367 & 0.443 \\
      Gen-LaneNet\cite{guo2020gen} & 32.3 & 0.591 & 0.684 & 0.411 & 0.521 \\
      Cond-IPM & 36.6 & 0.563 & 1.080 & 0.421 & 0.892 \\
      PersFormer\cite{chen2022persformer} & \textbf{50.5} & 0.485 &  \textbf{0.553} & 0.364 &  \textbf{0.431} \\
      CurveFormer (ours) & \textbf{50.5} & \textbf{0.340} & 0.772 &  \textbf{0.207} & 0.651 \\
      \bottomrule
    \end{tabular}
    \end{tiny} 
  \end{center}
\end{table}


In addition to the refined anchor point set , the prediction head of our CurveFormer outputs curve parameters of  3D lanes, where  is larger than the maximum number of labeled lanes across the training set. 
Similar to~\cite{liu2022learning}, we first associate the predicted curves  and ground truth lanes  by solving a bipartite matching problem, where  (0: background, 1: lane). We sample a set of 3D point  using the predicted curve parameters to compute the matching and training loss. The lane boundary (starting and ending points) is denoted by .

Let  be the set of predicted 3D lanes and  be the set of groundtruth.
Note that  is padded with non-lanes to fill enough the number of ground truth lanes to . The matching problem is formulated as a cost minimization problem by searching an optimal injective function , where  is the index of a 3D lane prediction  which is assigned to -th ground truth 3D lane :

The matching cost is calculated as:

where , , and  are coefficients which adjust the loss effects of classification, polynomial fitting and boundary regression, and  is an indicator function. 

After solving Eq.~\ref{matching-function} by Hungarian algorithms~\cite{carion2020end}, the final training loss can be written as , where  is the curve prediction loss,  is the deep supervision of refined anchor point set for each curve, and  is an auxiliary segmentation loss.
The curve prediction loss is defined as:

where , , and  are the same coefficients with Eq.~\ref{matching-cost}, and deep supervision of refined anchor point set is:



\section{EXPERIMENTS}

\begin{table*}[t]
  \begin{center}
  \begin{scriptsize}
    \caption{Experimental results of CurveFormer with different number of decoder layers on Rarely Observed scenario of Apollo 3D Lane Synthetic.}
    \label{tab:decoder-layer}
    \begin{tabular}{c|c|cccccc}
    \toprule
      Dataset split & \#Layer & F-Score & AP & X error near & X error far & Z error near & Z error far \\
      \midrule
      \multicolumn{1}{c|}{\multirow{5}{5em}{Rarely Observed}}
      & 2 & 93.1 & 95.5 & 0.200 & 0.752 & 0.043 & 0.599 \\
      & 4 & \textbf{94.1} & \textbf{96.4} & \textbf{0.181} & 0.729 & \textbf{0.038} & \textbf{0.557} \\
      & 6 & 93.2 & 95.6 & 0.188 & \textbf{0.723} & 0.040 & 0.588 \\
      & 8 & 94.0 & 96.2 & 0.184 & 0.769 & 0.039 & 0.563 \\
      & 10 & 93.6 & 95.9 & 0.198 & 0.734 & \textbf{0.038} & 0.566 \\
      \bottomrule
    \end{tabular}
    \end{scriptsize}
  \end{center}
\end{table*}


\subsection{Dataset}
\textbf{Apollo 3D Lane Synthetic Dataset.} Apollo Synthetic dataset~\cite{guo2020gen} consists of over 10k 1080 Ã— 1920 images which are built using unity 3D engine, including highway, urban, residential and downtown environments. 
The dataset is split into three different scenes: balanced scenes, rarely observed scenes and scenes with visual variations for evaluating algorithms from different perspectives.

\textbf{OpenLane Dataset.} OpenLane Dataset~\cite{chen2022persformer} is the first real world 3D lane dataset which consists of over 200K frames at a frequency of 10 FPS based on Waymo Open dataset\cite{Sun_2020_CVPR, waymo_open_dataset}. In total, it has a training set with 157k images and a validation set of 39k images. The dataset provides camera intrisics and extrinsics following the same data format as Waymo Open Dataset. 


\subsection{Experiment Settings}
\textbf{Implementation Details.}  We use EfficientNet~\cite{tan2019efficientnet} as backbone which gives 4 scale feature maps. The input image is resized to size of . The 3D-space range is set to  along  and  axis respectively. 
For curve representation, we use fixed y-positions . We set coefficients to , , , and . All experiments are performed with known camera poses and intrinsic parameters provided by two datasets. Our network uses Adam optimizer~\cite{kingma2014adam}, with a base learning rate of  and weight decay of . All models are trained from scratch with 100 epochs and the per-GPU batch size is set to 4. 

\subsection{Evaluation Metrics and Results}
\textbf{Evaluation metrics.} We follow the evaluation metrics designed by Gen-LaneNet~\cite{guo2020gen}. Point-wise Euclidean distance is calculated when a -position is covered by both prediction and the ground-truth. 
For each predicted lane, we consider it matched when  of its covered -positions have point-wise euclidean distance less than the max-allowed distance (1.5 meters). We report Average Precision (AP) , F-score, and errors (near range and far range) to investigate the performance of our model. 

\textbf{Results on Apollo 3D Lane Synthetic Dataset.} As shown in Table.~\ref{tab:results-apollosim}, we compare our CurveFormer with CNN-based 3D lane detection and Transformer-based 3D lane detection. Experimental results verify that our method outperforms the previous state-of-the-art approaches on Apollo 3D Lane Synthetic dataset. 
CurveFormer achieves the best F-Score and AP on every scene. Compared to PersFormer~\cite{chen2022persformer} on three different scenes, CurveFormer significantly improved F-Score by ,   and , respectively. 



\textbf{Results on OpenLane Dataset.} For OpenLane dataset, we evaluate CurveFormer on entire validation set and different scenario sets.
In Table.~\ref{tab:results-openlane-scenario-set}, our CurveFormer also gets comparable results compared to previous methods on the entire validation set, and achieves the highest F-Score on five scenario sets. We present detailed comparison with previous 3D lane detection SOTAs in Table.~\ref{tab:results-openlane-val-set}.





\subsection{Ablation Study}
In this section, we analyze the effects of the proposed key components via the ablation study conducted on Apollo 3D Lane Synthetic dataset~\cite{guo2020gen}. 

\textbf{Network Output: Curve Parameters vs Anchor Point Set.} 
We compare two different network outputs for 3D lane detection, curve parameters estimation and anchor point set prediction. The latter one is further interpolated as 3D curve for evaluation. Table.~\ref{tab:represent} lists the performance comparison on Apollo 3D Lane Synthetic dataset.
It shows that curve parameter prediction largely surpasses using anchor point set as network output, due to that lane parameter prediction can preserve the geometric property of 3D lane compared to predicting separated points.


\begin{table}[t]
\centering
\begin{scriptsize}
    \caption{Results with different forms of network output on Apollo 3D Lane Synthetic Dataset. C: curve parameter estimation; P: anchor point set prediction.}
    \label{tab:represent}
    \begin{tabular}{c|c|c|cc}
    \toprule
      Dataset Splits & C & P & F-Score & AP \\
      \midrule
      \multicolumn{1}{c|}{\multirow{2}{5em}{Balaneced Scenes}} 
      & \checkmark &  & \textbf{95.8} & \textbf{97.3} \\
      &  & \checkmark & 79.1 & 80.8 \\
      \midrule
      \multicolumn{1}{c|}{\multirow{2}{5em}{Rarely Observed}} 
      & \checkmark &  & \textbf{95.6} & \textbf{97.1} \\
      &  & \checkmark & 78.2 & 80.0 \\
      \midrule
      \multicolumn{1}{c|}{\multirow{2}{5em}{Vivual Variants}} 
      & \checkmark &  & \textbf{90.8} & \textbf{93.0} \\
      &  & \checkmark & 63.8 & 64.7 \\
      \bottomrule
    \end{tabular}
\end{scriptsize}
\end{table}

\textbf{Context Sampling.} 
We study the impact of the different ways to produce sampling offsets corresponding to 3D lane reference points. As shown in Table.~\ref{tab:context-sampling}, using context sampling offset (CSO) achieves best F-Score and AP compared to using standard sampling offset (SO). The results demonstrate the significance of image-query correlation for feature aggregation in our cross-attention module.

\begin{table}[t]
\centering
\begin{scriptsize}
    \caption{Ablation study about the Context Sampling module on Apollo 3D Lane Synthetic Dataset. Baseline (Exp 1) only interacts with image features using anchor point set without sampling offsets; SO: sampling offset; CSO: context sampling offset.}
    \label{tab:context-sampling}
    \begin{tabular}{c|c|cc|cc}
    \toprule
      Dataset Splits & Exp & SO & CSO & F-Score & AP \\
      \midrule
      \multicolumn{1}{c|}{\multirow{3}{5em}{Balaneced Scenes}} 
      & 1 &  &  & 95.6 & 97.2 \\
      & 2 & \checkmark &  & 95.4 & 97.2 \\
      & 3 &  & \checkmark  & \textbf{95.8} & \textbf{97.3} \\
      \midrule
      \multicolumn{1}{c|}{\multirow{3}{5em}{Rarely Observed}} 
      & 1 &  &  & 94.0 & 96.3 \\
      & 2 & \checkmark &  & 95.4 & 97.0 \\
      & 3 &  & \checkmark  & \textbf{95.6} & \textbf{97.1} \\
      \midrule
      \multicolumn{1}{c|}{\multirow{3}{5em}{Vivual Variants}} 
      & 1 &  &  & 87.1 & 89.2 \\
      & 2 & \checkmark &  & 88.2 & 90.1  \\
      & 3 &  & \checkmark  & \textbf{90.8} & \textbf{93.0} \\
      \bottomrule
    \end{tabular}
\end{scriptsize}
\vspace{-2mm}
\end{table}


\textbf{Number of Decoder Layer.} 
We vary the number of decoder layers and the performance of the model is shown in Table.~\ref{tab:decoder-layer}. It shows that using 4 decoder layers in our network achieves best performance. We can use few decoder layers due to curve propagation scheme of our CurveFormer method. Therefore, we set the number of decoder layers in our CurveFormer to 4 by default in the experiments.


\textbf{Auxiliary Segmentation.} Lastly, we study the effect of the auxiliary segmentation branch. 
Experimental results show that the auxiliary segmentation branch can slightly improve F-Score by 0.13, AP by 0.06 on the Balanced Scenes test set.


\section{CONCLUSIONS}

In this paper, we introduce CurveFormer, a Transformer-based 3D lane detection method. It uses dynamic anchor point set to construct queries, and refines it layer-by-layer in Transformer decoders. In addition, to attend to more relevant image features, we present a curve cross-attention module and a context sampling module to compute the key-to-image similarity. 
In the experiments, we show that CurveFormer achieves promising results compared with both CNN-based and Transformer-based approaches. In future work, we would like to explore video-based 3D lane detection for autonomous driving.

































\bibliographystyle{IEEEtran}
\bibliography{refs}





\end{document}
