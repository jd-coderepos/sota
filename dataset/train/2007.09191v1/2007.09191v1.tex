









\documentclass[a4paper,conference]{IEEEtran}


\usepackage[a4paper, left=0.514in, right=0.514in, bottom=1.569in, top=0.75in]{geometry}
\usepackage{adjustbox}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{soul}
\usepackage{color}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{threeparttable}

















\ifCLASSINFOpdf
\else
\fi














































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Attention2AngioGAN: Synthesizing Fluorescein Angiography from Retinal Fundus Images using Generative Adversarial Networks}


\author{\IEEEauthorblockN{Sharif Amit Kamran\IEEEauthorrefmark{1}, Khondker Fariha Hossain\IEEEauthorrefmark{2}, Alireza Tavakkoli\IEEEauthorrefmark{3} and Stewart Lee Zuckerbrod\IEEEauthorrefmark{4}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}\IEEEauthorrefmark{3}
\textit{University of Nevada, Reno}
NV, USA\\
\IEEEauthorrefmark{2}
\textit{Deakin University}
Melbourne, Australia\\
\IEEEauthorrefmark{4}
\textit{Houston Eye Associates}
Houston, TX, USA\\
skamran@nevada.unr.edu\IEEEauthorrefmark{1}, khossain@deakin.edu.au\IEEEauthorrefmark{2}, tavakkol@unr.edu\IEEEauthorrefmark{3}, szuckerbrod@houstoneye.com\IEEEauthorrefmark{4}}
}














\maketitle

\begin{abstract}
Fluorescein Angiography (FA) is a technique that employs the designated camera for Fundus photography incorporating excitation and barrier filters. FA also requires fluorescein dye that is injected intravenously, which might cause adverse effects ranging from nausea, vomiting to even fatal anaphylaxis. Currently, no other fast and non-invasive technique exists that can generate FA without coupling with Fundus photography. To eradicate the need for an invasive FA extraction procedure, we introduce an Attention-based Generative network that can synthesize Fluorescein Angiography from Fundus images. The proposed gan incorporates multiple attention based skip connections in generators and comprises novel residual blocks for both generators and discriminators. It utilizes reconstruction, feature-matching, and perceptual loss along with adversarial training to produces realistic Angiograms that is hard for experts to distinguish from real ones. Our experiments confirm that the proposed architecture surpasses recent state-of-the-art generative networks for fundus-to-angio translation task. 
\end{abstract}



\begin{IEEEkeywords}
Generative Adversarial Networks; Image-to-image Translation; Fluorescein Angiography; Retinal Fundoscopy; Residual Attention
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}


Retinal Funduscopy along with Fluorescein Angiography (FA) has been a popular diagnosing tool for retinal vascular and adverse pigment epithelial-choroidal conditions \cite{mary2016retinal}. In Fluorescein Angiography, a fluorescent dye is injected in the optic vein. It becomes noticeable after 10 minutes of insertion, depending on the age and the cardiovascular structure of the retinal layers \cite{mandava2004fluorescein}. While commonly considered healthy, non-fatal complications can arise, such as allergic reactions, nausea, vomiting, etc. Moreover, fatal cases have been documented, with symptoms such as anaphylaxis, heart attack, anaphylactic shock due to the leakage of the dye in the intravenous space \cite{lira2007adverse,kwan2006fluorescein,kwiterovich1991frequency}.  






Many automated systems have been proposed for the diagnosis of intrinsic conditions and diseases from fundus photos. They generally comprise of different image processing techniques and machine learning algorithms \cite{gurudath2014machine,fu2018disc,poplin2018prediction,lira2007adverse}.  Currently, there is no computational inexpensive alternative for generating reliable and reproducible fluorescein angiography images. Retinal fundoscopy is the only alternative for differential diagnosis that is easily available and financially viable. Optical coherence Tomography combined with basic image processing\cite{opticnet19} can be utilized for the diagnosis of retinal disease but is too expensive and not widely accessible in developing economies. As it stands, an efficient and faster procedure is crucial for avoiding any potential hazards associated with invasive fluorescein angiography.

In this paper, we introduce Attention-AngioGAN, a robust conditional Generative Adversarial Network (GAN) to produce fluorescein angiograms from retinal fundus images. For qualitative evaluation, we compare our automated technique with recent state-of-the-art GAN architectures such as pix2pixHD~\cite{wang2018high}, U-GAT-IT~\cite{kim2019u}, and Stargan-V2~\cite{choi2020stargan}. Moreover, we used Frechet inception Distance (FID)~\cite{heusel2017gans} and Kernel Inception Distance (KID)~\cite{binkowski2018demystifying} scores to quantify image quality and calculate similarity with real angiograms. 





\begin{figure*}[tp]
    \centering
    \includegraphics[width=16cm,height=9cm]{Fig1.png}
    \caption{The Proposed GAN architecture consist of two Generators [one Fine and one Coarse], four Discriminators [two Fine and two Coarse]. The fine discriminators take input image as the sample size while the coarse discriminators take input as half of the sample size.}
    \label{fig1}
\end{figure*}

\section{Literature Review}






Recently, there has been a surge of Generative adversarial networks(GAN) based applications ranging from image translation \cite{chen2018sketchygan,sangkloy2017scribbler}, editing \cite{zhu2016generative,dekel2018sparse} and image style transfer \cite{wang2018high,xian2018texturegan}. GANs can potentially extract and learn fine and coarse information from images by combining multiple architectures having multi-scale resolution \cite{burt1983laplacian,brown2003recognising}.
Such examples are wide-spread in both Conditional \cite{huang2017stacked,denton2015deep} and Unconditional  GAN settings\cite{chen2017photographic,zhang2017stackgan}. By incorporating multiple high-resolution architectures, they can learn distinct domain-specific features with high precision and robustness.


Emphasizing image to image translation, numerous prior work has been proposed, where they focused on architectural changes to acquire a higher quality result. To illustrate, pix2pixHD~\cite{wang2018high} utilized  PatchGAN as a multi-scale discriminator to achieve better visual representation containing local and global information. On the other hand,  U-GAT-IT~\cite{kim2019u}, an unsupervised learning architecture, extracts local features and texture by incorporating AdaLIN (Adaptive Layer-Instance Normalization). While most architectures ensure high quality of images, Stargan-V2~\cite{choi2020stargan}, a style-transfer network, focuses on the domain-specific features. By doing so, they allow diverse and scalable image-to-image translation in multiple domains within a single model.

Most of the image-to-image translation models are either focused on domain level transformation or combining style and textures of two images. For instance, high-resolution images generated by U-GAT-IT~\cite{kim2019u} and Stargan-V2~\cite{choi2020stargan} use attention modules to extract local features information and don't utilize perceptual loss. Whereas StyleGAN \cite{park2019semantic}, pix2pixHD  \cite{wang2018high} emphasizes more on incorporate perceptual loss with different styles of target images. By incorporating both these ideas we propose an architecture, where we combine perceptual loss and multi-scale discriminator to retain global information like the shape of optic-disc, color, contrast, etc. On the other hand, we utilize feature matching loss and introduce new multi-attention modules to retain local features like retinal venules, arteries, protein buildup, and microaneurysm. The visual representation and quantitative result prove that our proposed technique surpasses state-of-the-architectures and tricks, expert ophthalmologists, to think they are authentic.




\section{The Proposed Methodology}
This paper proposes an attention-based generative adversarial network (GAN) comprising of separate new residual blocks for generators and discriminators. Moreover, the training includes perceptual, feature matching, and reconstruction loss to synthesize more vivid looking angiograms from retinal fundus images. First, we discuss about coarse and fine generators in section \ref{subsec:generators}. Next, we elaborate our building blocks in Section \ref{subsec:encdec}, \ref{subsec:residualblock}, \ref{subsec:attention}. We then delve into the multi-scale discriminators and their interconnection with the generators to define the whole end-to-end pipeline for the generative network in sections  \ref{subsec:discriminators}. Ultimately, in section \ref{subsec:objective}, we discuss the loss minimizing and maximizing function and loss weight distributions for different losses interrelated to each of the separate architecture that forms the proposed model.

\iffalse
\subsection{Novel Residual Block}
\label{subsec:residualblock}

Recently, residual blocks have become the norm for implementing many image classification, detection and segmentation architectures \cite{he2016deep,he2016identity}. Generative architectures have employed these blocks in interesting applications ranging from image-to-image translation to super-resolution \cite{johnson2016perceptual,wang2018high,ledig2017photo}. In its atomic form, a residual unit consists of two consecutive convolution layers. The output of the second layers is added to the input, allowing for deeper networks. Computationally, regular convolution layers are expensive compared to a newer convolution variant, called separable convolution \cite{chollet2017xception}. Separable convolution performs a depth-wise convolution followed by a point-wise convolution. This, in turn helps to extract and retain depth and spatial information through the network. It has been shown that interspersing convolutional layers allows for more efficient and accurate networks~\cite{opticnet19}. We incorporate this idea to design a novel residual block to retain both depth and spatial information, decrease computational complexity and ensure efficient memory usage, as shown in Table.~\ref{table1}.

\begin{table}[htb]
\caption{Comparison between Original and Proposed Residual Block}
    \label{table1}
\centering
\begin{tabular}{|l|c|c|c|} 
\hline
\small Residual Block & \small Equation & \small Activation & \small No. of Parameters\\
\hline\hline
\small Original
& \small  & \small ReLU (Pre) \cite{he2016identity}& \small 18,688 \\ 
\small Proposed  & \small  &  \small Leaky-ReLU (Post) & \small 10,784\\ 
\hline
\end{tabular}
\footnotesize
\\  and  has kernel size , stride , padding  and No. of channel .
\end{table}



As illustrated in Fig.~\ref{fig2}, we replace the last convolution operation with a separable convolution. We also use Batch-normalization \cite{ioffe2015batch} and Leaky-ReLU as post activation mechanism after both convolution and separable Convolution layers. For better results, we incorporate reflection padding as opposed to zero-padding before each convolution operation. The entire operation can be formulated as shown in Eq.~\ref{eq1}:




Here,  refers to convolution operation while  and  signify the back-to-back convolution and separable convolution operations. By exploiting convolution and separable convolution layer with Leaky-ReLU, we ensure that two distinct feature maps (spatial \& depth information) can be combined to generate fine fluorescein angiograms. 


\fi


\begin{figure}[htb]
    \centering
    \includegraphics[width=8cm]{Fig2.png}
    \caption{Individual blocks of our proposed GAN architecture consisting of (a) Encoder, (b) Decoder, (c) Attention block, (d) Residual Block for Generator and (e) Residual Block for Discriminator where K stands for kernel size, S is for stride  and D is for Dilation rate }\label{fig2}
\end{figure}

\subsection{Coarse and Fine Generators}
\label{subsec:generators}
Coupling coarse-to-fine generator for image translation tasks results in very pristine and high quality images , as witnessed in recent architectures, such as pix2pixHD \cite{wang2018high}, SPADE \cite{park2019semantic}, and Starganv2 \cite{choi2020stargan}. We incorporate this into technique in our architecture by using two generators ( and ), as illustrated in Fig.~\ref{fig1}. The generator  synthesizes smooth FA from fundus images by learning local information such as retinal venules, blood vessels, hemorrhages, exudates, and protein buildup. On the contrary, the generator  tries to extract and preserve global information, such as the structures of the macula, optic disc, color intensity, contrast, and illumination, while producing less detailed angiograms. The generators consist of multiple encoders, decoder, attention, residual blocks, and a feature fusion block between the fine and coarse generator.   has an input dimension of   and produces outputs with the same resolution. Likewise,  takes an image with half the resolution () and synthesizes an image with the same size. Additionally, the  outputs a feature vector of the size  that is combined with one of the intermediate layers of  using the fusion operation. The representation of these generators is illustrated in Fig.~\ref{fig1}. In the following sections, we elaborate on each of these blocks in detail.

\subsection{Encoder and Decoder Blocks}
\label{subsec:encdec}
Both generators and discriminators incorporate the encoder blocks for downsampling the feature maps. On the contrary, only the generators use decoder blocks for upsampling to get the desired feature maps and output. The encoder block consists of a convolution layer followed by a batch-norm layer \cite{ioffe2015batch} and Leaky-ReLU activation function, as illustrated in Fig.~\ref{fig2}(a). In contrast, the decoder block comprises of transposed convolution layer and successive batch-norm \cite{ioffe2015batch} and Leaky-ReLU activation \ref{fig2}(b).  Interestingly,  is downsampled twice () using the encoder. After successive residual blocks, the decoder blocks are using to upsample twice again. For , the encoder is utilized once, and after the repetition of residual blocks, a single decoder is used to get the same spatial dimension of the output. We use a kernel size,  and stride,  for both of our convolution, and transposed convolution layers. 

\subsection{Distinct Residual Blocks for Generator and Discriminator}
\label{subsec:residualblock}
Lately, residual blocks have become the standard for generative models accomplishing image-to-image translation, image inpainting, and semantic segmentation tasks \cite{wang2018high,park2019semantic}. The fundamental design consists of a residual unit with two consecutive convolution layers and a skip connection that adds feature tensor of the input with the output. Regular convolution layers are computationally inefficient as opposed to separable convolution \cite{chollet2017xception}. Separable convolution consists of a depth-wise convolution followed by a point-wise convolution. By doing so, it extracts and retains the depth and spatial information through the network. Recent studies show that combining separable convolutional layers with dilation allows for more robust feature extraction~\cite{opticnet19}. We incorporate this technique to design two distinct novel residual blocks for our generators and discriminators, as shown in Fig.~\ref{fig2}(d) \& Fig.~\ref{fig2}(e). The residual block for our generator consists of a separable convolution layer followed by two branches of separable convolution. The difference is that one branch consists of a separable convolution with a dilation rate of,  and the other with dilation rate, . We use a kernel size,  and stride,  for all of our separable convolution layers. Each separable convolution is preceded by a Reflection padding layer and succeded by a Batch-Normalization and Leaky-ReLU activation layer. The skip connection and output of the two branches are all added together to produce the final output. In contrast, the residual block for the discriminator consists of a Separable convoluton layer, followed by Batch-Normalization and Leaky-ReLU activation function. The separable convolution has a kernel size of  and stride, .

\subsection{Attention block}
\label{subsec:attention}
Next, we elaborate our proposed attention block, as illustrated in
Fig.~\ref{fig2}(c). The block consists of two successive residual units, Convolution, BatchNorm, and Leaky-ReLU layers. Both convolution layer has kernel size of  and stride, . Other than that, there are two skip connections one coming from the input and being added to the output of the first residual unit. The other one is coming from the input and summed with the output of the last residual unit. We use attention block for combining feature information from the bottom layers of the network with the top layers of the network, as illustrated in Fig.~\ref{fig2}.  comprises of two attention block, coming out of the with two encoders and being added with the two decoders successively. In contrast, the  has only one attention block between the encoder and decoder.  The reason behind utilizing attention block is to retrieve and retain spatial information that can be combined with the learned features of the later layers of the architectures as observed in similar GAN architectures \cite{zhang2019self,chen2018attention}. 


\subsection{Multi-scale Markovian Discriminators}
\label{subsec:discriminators}
GAN discriminators need to adjust to coarse and fine generated outputs for distinguishing between real and synthesized images. To solve this underlying issue, we need a dense network with a huge amount of computable parameters. Alternatively, convolution with a wider receptive field can be utilized for extracting spatial information. This can easily lead to overfitting while training the model. To address this issue, we exploit the idea of using two Markovian discriminators, first introduced in a technique called PatchGAN \cite{li2016precomputed}. The method consists of discriminators with variable sized input resolution and can help with the overall adversarial training of the architecture as observed in \cite{wang2018high}

We use four discriminators that incorporates almost the same network structure but operate at two different resolutions. We organize the four discriminators into two sets,  and  as illustrated in Fig.~\ref{fig1}.  We resize each of the coarse and fine angiograms and fundus with size  and  by a factor of  using the Lanczos filter~\cite{duchon1979lanczos}.  and  have a unique average pooling layer right after the input which resizes the resolution to   and . Other than that, all four discriminators have identical layers consisting of three repetitive encoder and residual block pairs (in Fig.~\ref{fig2}(a) and Fig.~\ref{fig2}(e). Lastly, convolution layer is used for getting spatial dimension of  and   for  and  and  for   as outputs.

The coarse discriminators one that learns feature at a lower resolution tries to convince the coarse generator to retain more global features such as the macula, spherical optic disc, appearance, and illumination. On the other hand,  the fine discriminators dictate the fine generator to produce more detailed local features such as retinal vessels, arteries, exudates,  etc. By doing this we fuse features from both generators while training them autonomously with their joined multi-scale discriminators.

\subsection{Weighted Object Function and Adversarial Loss}
\label{subsec:objective}

With the given discriminators and generators, the objective function for our whole network can be formulated as Eq.~\ref{eq1}. It's a multi-objective problem of maximizing the loss of the discriminators while diminishing the loss of the generators. 
 

For adversarial training, we use Hinge-Loss \cite{zhang2019self,lim2017geometric} as illustrated in Eq.~\ref{eq2} and Eq.~\ref{eq3}. Effectively, all the fundus images and their corresponding angiogram pairs, are normalized to . This in turn helps with widening the gap between the pixel intensities of the real and synthesized angio images. In Eq.~\ref{eq4} we add them and use  as weight multiplier with the .



Here, In Eq.~\ref{eq2} and Eq.~\ref{eq3} the discriminators are first trained on the real fundus,  and real angiogram, , and then trained on the real fundus,  and synthesized angiogram, . We begin by batch-wise training the discriminators , and  for a couple of iterations on randomly sampled data. After that, we train the  while keeping the weights of the discriminators frozen. In the same manner, we train the  on a batch of random images while keeping weights of all the discriminators frozen. 

The generators also incorporate the reconstruction loss and perceptual loss~\cite{johnson2016perceptual} as shown in Eq.~\ref{eq5} and Eq.~\ref{eq6}. By utilizing these losses we ensure the synthesized images contain more realistic color, contrast, and vascular structure. We also employ feature matching loss ~\cite{wang2018high} with all our discriminators and as given in Eq.~\ref{eq7}.








For Eq.~\ref{eq5},  is the reconstruction loss for a real angiogram, , given a generated angiogram, . We use this loss for both  and  so that the model can generate high-quality angiograms of different scales. In Eq.~\ref{eq6},  calculates the difference between real and fake angio features extracted by pushing both of successively in VGG19 architecture~\cite{simonyan2014very}.  Lastly, Eq.~\ref{eq7} is calculated by taking the features from intermediate layers of the discriminator by first inserting the real and fake angiograms consecutively. Here,  and  stands for the number of feature layers extracted from VGG19 and the discriminators consecutively.

By incorporating Eq.~\ref{eq4}, \ref{eq5}, \ref{eq6} and \ref{eq7} we can formulate our final objective function as given in Eq.~\ref{eq8}.


Here, , ,  and  signifies loss weighting that are multiplied with their respective losses. The loss weighting dictates which networks to prioritize while training. For our architecture, more weight is given to the , , , and thus we select bigger  values for those. 

\section{Experiments}
In the next section, we detail our model experiments and evaluate our architecture based on qualitative and quantitative metrics. First, we elaborate on the structuring and pre-processing of our dataset in Sec.~\ref{subsec:dataset}. Then detail our hyper-parameter selection and tuning in Sec.~\ref{subsec:hyper}. Next, we describe our adversarial training scheme in Sec. ~\ref{subsec:training}. Also, we compare our architecture with existing state-of-the-art generative models based on some qualitative evaluation metrics in Sec.~\ref{subsec:quant}. Lastly, in Sec.~\ref{subsec:qual},  we analyze the quantification done by experts, by distinguishing between real and synthesized angiograms. 
\subsection{Dataset}
\label{subsec:dataset}
We use the fundus and angiography data-set provided in \cite{hajeb2012diabetic}. It consists of thirty image and twenty-nine pairs of the healthy and unhealthy fundus and angiogram images, collected from fifty-nine individual patients. Next, we clean the dataset by taking only seventeen pairs of images based on one-to-one alignment between the fundus and angiogram pairs. These image pairs are either accurately aligned or almost aligned. The original image size is , but we take 50 overlapping crops of   sized samples from each. By doing so, we end up having  850 images in total for training. The fundus images are in RGB format, and angiograms are in a Gray-scale format. For testing, we take fourteen image pairs and crop four overlapping quadrants of the image to generate a test set of fifty-six test images.

\begin{algorithm}[h]
\caption{Attention2AngioGAN training}
\label{alg1}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
 \renewcommand{\algorithmicensure}{\textbf{Output:}}
  \REQUIRE , 
 \ENSURE , 
  \STATE \textbf{Initialize hyper-parameters}: \\, , , , , ,   , , \\  , , , , ,  , , , ,  
  \FOR{}
  \STATE Sample , using batch-size  
    \FOR{}
\STATE ,   
        \STATE ,   
        \STATE , 
        \STATE 
        \STATE 
         \STATE 
    \ENDFOR
    \item[] \textbf{Freeze }
    \STATE Sample , using batch-size 
    \STATE ,
    \STATE 
    \STATE 
    \STATE 
    \STATE 
    \STATE 
    \item[] \textbf{Unfreeze }
    \STATE   
    \STATE 
    \STATE 
    \STATE 
    \item[] \textbf{Freeze }
    \STATE ,   
    \STATE ,   
    \STATE , 
    \STATE 
    \STATE 
    \STATE 
    \STATE Save weights and snapshot of 
    \STATE 
  \ENDFOR
\end{algorithmic}
\end{algorithm}



\subsection{Hyper-parameter tuning}
\label{subsec:hyper}
For adversarial training, we used hinge loss \cite{zhang2019self,lim2017geometric}. We picked  (Eq.~\ref{eq4}) and , ,  (Eq.~\ref{eq8}). For optimizer, we used Adam \cite{kingma2014adam}, with learning rate ,  and . We train with mini-batches with batch size,  for 200 epochs. It took approximately 48 hours to train our model on NVIDIA P100 GPU.




\subsection{Training procedure}
\label{subsec:training}
In this section, we elaborate on our detailed algorithm provided in Algorithm~\ref{alg1}. To train our Attention2AngioGAN, we start by initializing all the hyper-parameters. Next, we sample a batch of the real fundus and angiogram  images. We train the real fundus and fake angiogram pairs with . After that, we use  to synthesize fake angiograms and use the real fundus and fake angiograms,  to train discriminators . Following that, we calculate the adversarial loss, , and update the weights. We freeze the weights of the discriminators. Next, we train the generators and calculate the ,  losses and update both generator's weights.  Subsequently, we unfreeze both discriminators weights, calculate the feature matching loss   and update the discriminator weights. In the final stage, we freeze both discriminator's weights and jointly fine-tune all the discriminator and generators. We calculate the total loss by adding and multiplying with their relative weights. For testing, we save the snapshot of the model and its weights.




\subsection{Qualitative Evaluation}
\label{subsec:qual}
For assessing the performance of our network, we used 14 test samples and cropped four quadrants of the image with a size of . We conducted two sets of tests to evaluate our networks. First, for estimating the accurate visual representation without transforming the image. Next, for global and local changes due to transformation and distortion of the image. By doing so, we measured the network's ability to adjust to structural changes to the vascular patterns and formation of the retinal subspace. We used the GNU Image Manipulation Program (GIMP) \cite{gimp2019gimp} for carrying out transformation and distortion on the images.
 
\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm,height=14cm]{Fig3.png}
    \caption{Comparativie results of different Angiograms generated using different state-of-the-art architectures. Column (A) and (C) represents two samples of real fundus, real angio and predicted angio images. Whereas column (B) and (D) represents the red rectangle box to show zoomed in local venular structures corresponding.}
    \label{fig3}
\end{figure}



\begin{figure}[ht]
    \centering
    \includegraphics[width=9cm,height=14cm]{Fig4.png}
    \caption{Angiogram generated from transformed and distorted Fundus images with natural changes, imaging errors and biological markers.}
    \label{fig4}
\end{figure} 



For the first experiment, we train three variants of our network and four other state-of-the-art image-to-image translation architecture using the same number of epochs and batches of images. Next, we evaluate them using the same test sample. A side by side comparison of the results is illustrated in Fig.~\ref{fig3}. Column A \& C in Fig.~\ref{fig3} shows the global changes while column B \& D are zoomed-in to display local vascular structure and other local information. We use FM and PL to denote feature-matching and perceptual loss. By the looks of it, both of our models using the FM loss (with or without PL) produces vivid and convincing results. On the other hand, the visual result of our network without FM and PL produces distorted local structure due to not learning contrast and color of the optic disc using perceptual loss. Fundus2Angio and StarGANv2 also produce impressive results. However, if seen in the closed-up versions in columns B \& D, we can witness the right upper portion of the optic discs contains fewer blood vessels compared to our ones. U-GAT-IT and Pix2PixHD also fail to generate rich venules,  exudates, and protein buildup. 

In the second set of experiments, three transformations and two distortions were applied on the fundus images: 1) blurring to represent severe cataracts, 2) sharpening to represent dilated pupils, 3) signal noise to represent machine impedance during fundoscopy, 4) pinch, to visualize the pulled/pushed vascular formation, 5) whirl, for representing distortions caused by increased intraocular pressure (IOP). Improved robustness and adaption are represented by the generated angiograms similarity to the real FA image since these transformations and distortions may or may not affect the vascular structure of the retina. A side by side comparison of different architecture's predictions on these transformed images is illustrated in Fig.~\ref{fig4}. As it can be observed from the image, the proposed architecture produces images very similar to the ground-truth (GT). The results under these global and local vascular changes applied to the fundus image. 


\begin{table*}[ht]
\centering

\caption{Test results for different architectures}
\begin{adjustbox}{width=5.8in,totalheight=2.8in}
\begin{threeparttable}
    \begin{tabular}{|l|c|c|c|c|c|c|} 
    \hline
    \multicolumn{7}{|c|}{\textbf{Fréchet Inception Distance (FID)}}\\
    \hline
    Architecture &  Orig. &  Noise & Blur &  Sharp & Whirl & Pinch \\
    \hline
    \textbf{Ours + PL\tnote{1} + FM\tnote{2}} & 24.6 &  21.6 (3.0) &  30.0 (5.4) &  25.6 (1.0) &  40.0 (15.4) &  24.9 (0.3) \\
    \textbf{Ours + FM\tnote{2}} & \textbf{20.7} &  \textbf{20.8 }(0.1) &  \textbf{23.5} (2.8) &  \textbf{24.9} (4.2) & \textbf{27.8} (7.1) &  \textbf{19.5} (1.2) \\
    \textbf{Ours} & 47.5 &  43.1 (4.4) &  49.8 (2.3) &  50.5 (3.5) &  61.9 (14.5) &  46.7 (0.8) \\ 
    StarGAN-v2 \cite{choi2020stargan} & 27.7 & 35.1 (7.4) & 32.6 (4.9) & 27.4 (0.3) & 32.7 (5.0) & 26.7 (1.0) \\
    U-GAT-IT \cite{kim2019u} & 24.5 & 26.0 (1.5) & 30.4 (5.9)  & 26.8 (2.3) & 33.0 (9.5) & 29.1 (4.6) \\
    Fundus2Angio \cite{kamran2020fundus2angio} & 30.3 &  41.5 (11.2) & 32.3 (2.0) & 34.3 (4.0) & 38.2 (7.9) & 33.1 (2.8) \\ 
    Pix2PixHD \cite{wang2018high} & 42.8  & 53.0 (10.2)& 43.7 (1.1) & 47.5 (4.7) & 45.9 (3.1) & 39.2 (3.6) \\ 
    \hline
    \hline
    \multicolumn{7}{|c|}{\textbf{Kernel Inception Distance (KID)}}\\
    \hline
    Architecture &  Orig. &  Noise & Blur &  Sharp & Whirl & Pinch \\
    \hline
    \textbf{Ours + PL\tnote{1} + FM\tnote{2}} & \textbf{0.00087} & \textbf{0.05045} & \textbf{0.00235} & \textbf{0.05162} & 0.05390 & \textbf{0.04575} \\
    \textbf{Ours + FM\tnote{2}} & 0.00392 &	0.05390 & 0.00505 & 0.05301 & 0.05657	& 0.05341 \\
    \textbf{Ours} & 0.00595 & 0.05237 &	0.00617 & 0.05298 &	0.05613 & 0.05419 \\
    StarGAN-v2 \cite{choi2020stargan} & 0.00118 & 0.05274 & \textbf{0.00235} & 0.05331 & 0.05539 & 0.05271 \\
    U-GAT-IT \cite{kim2019u} & 0.00131	& 0.05610 & 0.00278 & 0.05533 & 0.05815 & 0.05719\\
    Fundus2Angio \cite{kamran2020fundus2angio} & 0.00184 & 0.05328 & 0.00272 & 0.05267 & \textbf{0.05278} & 0.04985 \\ 
    Pix2PixHD \cite{wang2018high} & 0.00258 & 0.05613 & 0.00254 & 0.05788 & 0.06029 & 0.05838 \\ 
    \hline
    \end{tabular}
    \begin{tablenotes}
         \item[1] PL = Perceptual Loss; FM = Feature-Matching Loss
         \item[2] FID: Lower is better; KID: Lower is better
    \end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\label{table1}
\end{table*}

In the case of \textbf{blurred} fundus images, our architecture with and without PL, is less affected compared to other state-of-the-art models, as seen in (row 6 to 9 of column 1) of Fig.~\ref{fig4}. The venular and cellular structures are better conserved as opposed to StarGANv2 and Pix2PixHD. For \textbf{sharpened} fundus, the angiogram produced by UGATIT and Fundus2Angio (row 7 and 8 of column 2) exhibits grainy artifacts around the blood vessels, which are not present in our model with and without PL. For \textbf{noisy} images, our result with and without PL, is still unaffected with this pixel-level modification. However, all other state-of-the-art models (row 6 to 9 of column 3) fail to generate thin and small venular formations by failing to extract local features from the retinal subspace. On the contrary, our model without PL and FM produces jittery motion artifacts and high contrast around the border of the optic disc for all these transformations.


For distortions like \textbf{Pinch} and \textbf{Whirl}, our experimental result with and without perceptual loss shows the versatility and reproducibility of the proposed network to uncover the changes in vascular structure as seen in Fig.~\ref{fig4} (row 3 and 4 of column 4 and 5). Compared to ours, only StarGANv2 and U-GAT-IT maintains the flattening condition and manifestation of vascular changes but loses the overall smoothness in the process (row 6 to 7 of column 4 and 5). As seen in Fig.~\ref{fig4} ours with and without PL network encodes the feature information of vessel structures and is much less affected by both kinds of contortion. The other architectures failed to generate microvessel structure due to IOP or vitreous changes as can be seen in Fig.~\ref{fig4}.Contrarily, our model without any perceptual and feature-matching loss fails to encode this information and vascular changes. Consequently, For all kinds of transformation and distortion our model with and without perceptual triumphs over existing state-of-the-art image-to-image translation models.

\begin{table}[ht!b]
\caption{Results of Qualitative with Undisclosed Portion of Fake/Real Experiment}
\label{table2}
\centering
\begin{adjustbox}{width=3.4in,totalheight=0.9in}
\begin{threeparttable}
\begin{tabular}{|l|l|c|c|c|c|c|} 
\hline
&&\multicolumn{2}{c|}{\small Results} & \multicolumn{3}{c|}{ \small Average}  \\\hline
&& \small Correct & \small Incorrect & \small Missed\tnote{1} & \small Found\tnote{1} & \small Precision\tnote{2}\\\hline\hline
\small \multirow{2}{*}{Ours + FM + PL} &\small Fake & \small 10\% & \small 90\% & \small \multirow{2}{*}{55\%} & \small \multirow{2}{*}{45\%} & \multirow{2}{*}{\small \textbf{47.1\%}} \\
&\small Real & \small 80\% & \small 20\% & & & \\
\hline
\small \multirow{2}{*}{Ours + FM} &\small Fake & \small 12\% & \small 88\% & \small \multirow{2}{*}{53\%} & \small \multirow{2}{*}{47\%} & \multirow{2}{*}{\small \textbf{48.2\%}} \\
&\small Real & \small 82\% & \small 18\% & & & \\
\hline
\end{tabular}
    \begin{tablenotes}
         \item[1] Missed higher is better; Found lower is better
         \item[2] Precision Lower is better
    \end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\end{table}
\subsection{Quantitative Evaluations}
\label{subsec:quant}
For quantitative evaluation, we performed two experiments. In the first experiment we use the Fréchet inception distance (FID) \cite{heusel2017gans} and Kernel Inception distance (KID) \cite{binkowski2018demystifying} which has been used to evaluate similar style-transfer GANs \cite{choi2020stargan,kim2019u}. We computed the FID and KID scores for different architectures on the generated FA image and original angiogram, including the five transformations and distortions. The results are reported in Table.~\ref{table1}. It should be noted that lower FID and KID score means better results. 

From Table.~\ref{table1},  out of our three networks, the best FID is achieved for ours without PL.  And it achieves the lowest scores among out of all other architecture, for both with and without distortions. For KID, our model with PL achieves the lowest score for four out of five types of distortions. Fundus2Angio scores lower KID for distorted images using whirl. Other than that, StarGAN-v2 achieves the same score as our network having a KID of 0.00235.



In the next experiment, we assess the quality of the synthesized angios by asking three expert ophthalmologists to identify fake angios among a collection of 50 balanced (50\%, 50\%) and randomly set of mixed angiograms. For this experiment, the exact number of fake and real images was not known by the experts. By not disclosing this information we tried to evaluate following criterion: 1) Correct fake and real angios found by the experts, where lower is better, 2) Incorrect fake and real angios missed by the experts, where higher is better and 3) The average precision representing the effective the identification is by the experts, where lower is better. The detailed results are shown in Table~\ref{table2}.

As it can be seen from Table~\ref{table2}, experts assigned 90\% and of 88\% of the fake angiograms as real, for images generated by two of our models. The result also shows that experts had difficulty in identifying fake images, while they easily identified real angiograms with 80\% and 82\% certainty. On average, the experts misclassified 55\% and 53\% of all images for two of our models consecutively. The average precision diagnosis of the experts are 47.1\% and 48.2\%. Consequently, our model with lower precision achieves the best result by fooling the experts to identify fake angios as real.

\section{Conclusion}
In this paper, we proposed a new image-to-image translation architecture called Attention2AngioGAN. The architecture synthesizes high quality and vivid looking angiograms from fundus images without any expert intervention. Additionally, we illustrated its robustness, flexibility, and reproducibility by producing high-quality angiograms from transformed and distorted images, which imitates biological markers seen in real fundus images. As a result, the proposed network can be efficiently employed to generate precise FA images of patients developing disease overtime. This is best suited for disease progression monitoring to predict the development of diseases in vivo. We hope to extend this work for other areas of ophthalmological data modalities.












































\bibliographystyle{IEEEtran}
\bibliography{reference}




\end{document}