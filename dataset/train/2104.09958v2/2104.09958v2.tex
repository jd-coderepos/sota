\documentclass{article}



\usepackage[preprint,nonatbib]{neurips_2021}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage[sort&compress,numbers]{natbib}
\usepackage{subcaption}
\captionsetup[table]{skip=10pt}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{mlmacros}
\usepackage{amsmath,amssymb}
\usepackage{bbm}
\usepackage{cleveref}

\variables{u,v,x,z,m,e}
\probdists{p,q}

\newcommand{\me}[1]{\textcolor{blue}{\textbf{me}: #1}}
\newcommand{\opj}[1]{\textcolor{green}{\textbf{opj}: #1}}
\newcommand{\hip}[1]{\textcolor{red}{\textbf{hip}: #1}}

\title{GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement}


\author{Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner\\
  Applied AI Lab, University of Oxford, UK\\
  \texttt{\{martin, oiwi, ingmar\}@robots.ox.ac.uk} \\
}

\begin{document}

\maketitle


\begin{abstract}
Advances in object-centric generative models (OCGMs) have culminated in the development of a broad range of methods for unsupervised object segmentation and interpretable object-centric scene generation.
These methods, however, are limited to simulated and real-world datasets with limited visual complexity.
Moreover, object representations are often inferred using RNNs which do not scale well to large images or iterative refinement which avoids imposing an unnatural ordering on objects in an image but requires the \emph{a priori} initialisation of a fixed number of object representations.
In contrast to established paradigms, this work proposes an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic, non-parametric stick-breaking process.
Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters \emph{a priori}.
This is used to develop a new model, \textsc{genesis-v2}, which can infer a variable number of object representations without using RNNs or iterative refinement.
We show that \textsc{genesis-v2} outperforms previous methods for unsupervised image segmentation and object-centric scene generation on established synthetic datasets as well as more complex real-world datasets.
\end{abstract}

\section{Introduction}

Reasoning about discrete objects in an environment is foundational to how agents perceive their surroundings and act in it.
For example, autonomous vehicles need to identify and respond to other road users (e.g. \cite{geiger2012are,cordts2016Cityscapes}) and robotic manipulation tasks involve grasping and pushing individual objects (e.g. \cite{devin2018deep}).
While supervised methods can be used to identify selected objects (e.g. \cite{ren2015faster,he2017mask}), it is intractable to manually collect labels for every possible object category.
Furthermore, we often also desire the ability to predict, or \emph{imagine}, how a collection of objects might behave (e.g. \cite{wu2021greedy}).
A range of works on object-centric generative models (OCGMs) have explored unsupervised segmentation and object-centric generation in recent years (e.g. \cite{huang2015efficient,eslami2016attend,xu2018multi,crawford2019spatially,lin2020space,kosiorek2018sqair,jiang2020scalor,burgess2019monet,engelcke2020genesis,engelcke2020reconstruction,greff2016tagger,greff2017neural,van2018relational,greff2019multi,veerapaneni2020entity,locatello2020object,kosiorek2019stacked,yang2020learning,bear2020learning,anciukevicius2020object,van2018case,chen2019unsupervised,bielski2019emergence,arandjelovic2019object,azadi2019compositional,nguyen2020blockgan,ehrhardt2020relate,niemeyer2020giraffe}).
These models are often formulated as variational autoencoders (\textsc{vae}s) \cite{kingma2013auto,rezende2014stochastic} which allow the joint learning of inference and generation networks to identify objects in images and to generate synthetic scenes in an object-centric fashion (e.g. \cite{engelcke2020genesis,anciukevicius2020object}).

Such models require a differentiable mechanism for separating objects in an image.
While some works use spatial transformer networks (STNs) \cite{jaderberg2015spatial} to crop out windows that contain objects (e.g. \cite{huang2015efficient,eslami2016attend,kosiorek2018sqair,xu2018multi,crawford2019spatially,lin2020space,jiang2020scalor}), others directly predict pixel-wise instance segmentation masks (e.g. \cite{greff2016tagger,greff2017neural,van2018relational,burgess2019monet,engelcke2020genesis,engelcke2020reconstruction,veerapaneni2020entity,greff2019multi,locatello2020object}).
The latter avoids the use of fixed-size sampling grids which are ill-suited for objects of varying size.
Instead, object representations are inferred either by iteratively refining a set of randomly initialised representations (e.g. \cite{greff2016tagger,greff2017neural,van2018relational,greff2019multi,veerapaneni2020entity,locatello2020object}) or by using a recurrent neural networks (RNN) (e.g. \cite{burgess2019monet,engelcke2020genesis,engelcke2020reconstruction}).
One particularly interesting model of the latter category is \textsc{genesis} \cite{engelcke2020genesis} which performs both scene segmentation and generation by capturing relationships between objects with an autoregressive prior.

As noted in \citet{novotny2018semi}, however, using RNNs for instance segmentation requires processing high-dimensional inputs in a sequential fashion which is computationally expensive and does not scale well to large images with potentially many objects.
We also posit that recurrent inference is not only problematic from a computational point of view, but that it can also inhibit the learning of object representations by imposing an unnatural \emph{ordering} on objects.
In particular, we argue that this leads to different \emph{object slots} receiving gradients of varying magnitude which provides a possible explanation for OCGMs collapsing to a single object slot during training (see \cite{engelcke2020reconstruction}).
While iterative refinement instead infers unordered object representations, it requires the \emph{a priori} initialisation of a fixed number of object slots even though the number of objects in an image is unknown.

In contrast, our work takes inspiration from the literature on supervised instance segmentation and adopts an \emph{instance colouring} approach (e.g. \cite{novotny2018semi,fathi2017semantic,bai2017deep,debrabandere2017semantic}) in which pixel-wise embeddings---or colours---are clustered into attention masks.
Typically, either a supervised learning signal is used to obtain cluster seeds (e.g. \cite{novotny2018semi,fathi2017semantic}) or clustering is performed as a  non-differentiable post-processing operation (e.g. \cite{bai2017deep,debrabandere2017semantic}).
Neither of these approaches is suitable for unsupervised, end-to-end learning of segmentation masks.
We hence develop an \emph{instance colouring stick-breaking process} (IC-SBP) to cluster embeddings in a differentiable, non-parametric fashion.
This is achieved by stochastically sampling cluster seeds from the pixel embeddings to perform a soft grouping of the embeddings into a set of randomly ordered attention masks.
It is therefore possible to infer object representations both without imposing a fixed ordering or performing iterative refinement.

Inspired by \textsc{genesis} \cite{engelcke2020genesis}, we leverage the IC-SBP to develop \textsc{genesis-v2}, a novel OCGM that learns to segment objects in images without supervision and that uses an autoregressive prior to generate scenes in an interpretable, object-centric fashion.
\textsc{genesis-v2} is comprehensively benchmarked against recent prior art \cite{burgess2019monet,engelcke2020genesis,locatello2020object} on established synthetic datasets---ObjectsRoom \cite{multiobjectdatasets19} and ShapeStacks \cite{groth2018shapestacks}---where it outperforms the baseline methods by significant margins.
We also evaluate \textsc{genesis-v2} on more challenging real-world images from the Sketchy \cite{cabi2019scaling} and the MIT-Princeton Amazon Picking Challenge (APC) 2016 Object Segmentation datasets \cite{zeng2016multi}, where it again achieves considerably better performance than previous methods.
Code and pre-trained models will be released.

\section{Related Work}

OCGMs are typically formulated either as autoencoders (e.g. \cite{huang2015efficient,eslami2016attend,kosiorek2018sqair,xu2018multi,crawford2019spatially,greff2016tagger,greff2017neural,van2018relational,burgess2019monet,greff2019multi,kosiorek2019stacked,veerapaneni2020entity,engelcke2020genesis,engelcke2020reconstruction,anciukevicius2020object,lin2020space,jiang2020scalor,yang2020learning,bear2020learning,locatello2020object}) or generative adversarial networks (\textsc{gan}s) (e.g. \cite{van2018case,chen2019unsupervised,bielski2019emergence,arandjelovic2019object,azadi2019compositional,nguyen2020blockgan,ehrhardt2020relate,niemeyer2020giraffe}).
Typical \textsc{gan}s are able to generate images, but lack an associated inference mechanism and often suffer from training instabilities (see e.g. \cite{goodfellow2014generative,brock2018large}).
A comprehensive review and discussion of the subject is provided in \citet{greff2020binding}.

Some object-centric autoencoders separate objects in images by extracting bounding boxes with STNs (e.g. \cite{huang2015efficient,eslami2016attend,kosiorek2018sqair,xu2018multi,crawford2019spatially,lin2020space,jiang2020scalor}), while others directly infer segmentation masks (e.g. \cite{greff2016tagger,greff2017neural,van2018relational,burgess2019monet,greff2019multi,kosiorek2019stacked,veerapaneni2020entity,engelcke2020genesis,engelcke2020reconstruction,locatello2020object}).
STNs can explicitly disentangle object location by cropping out a rectangular region from an input, allowing object appearance to be modelled in a canonical pose.
This operation, however, relies on a fixed-size sampling grid which is not well suited if objects vary broadly in terms of scale.
In addition, gradients are usually obtained via bi-linear interpolation and are therefore limited to the extent of the sampling grid which can impede training: for example, if the sampling grid does not overlap with any object, then its location cannot be updated in a meaningful way.

Purely segmentation based approaches, in contrast, use RNNs (e.g. \cite{burgess2019monet,engelcke2020genesis}) or iterative refinement (e.g. \cite{greff2016tagger,greff2017neural,van2018relational,greff2019multi,veerapaneni2020entity,locatello2020object}) to infer object representations from an image.
RNN based models need to learn a fixed strategy that sequentially attends to different regions in an image, but this imposes an unnatural ordering on objects in an image.
Avoiding such a fixed ordering leads to a \emph{routing problem}.
One way to address this is by randomly initialising a set of object representations and refining them in an iterative fashion.
More broadly, this is also related to Deep Set Prediction Networks \cite{zhang2019deep}, where a set is iteratively refined in a gradient-based fashion.
The main disadvantage of iterative refinement is that it is necessary to initialise a fixed number of clusters \mbox{\emph{a priori}}, even though ideally we would like the number of clusters to be input-dependent.
This is directly facilitated by the proposed IC-SBP.

Unlike some other works which learn unsupervised object representations from video sequences (e.g. \cite{kosiorek2018sqair,jiang2020scalor}), our work considers the more difficult task of learning such representations from individual images alone.
\textsc{genesis-v2} is most directly related to \textsc{genesis} \cite{engelcke2020genesis} and \textsc{slot-attention} \cite{locatello2020object}.
Like \textsc{genesis}, the model is formulated as a \textsc{vae} to perform both object segmentation and object-centric scene generation, whereby the latter is facilitated by an autoregressive prior.
Similar to \textsc{slot-attention}, the model uses a shared convolutional encoder to extract a feature map from which features are pooled via an attention mechanism to infer object representations with a random ordering.
In contrast to \textsc{slot-attention}, however, the attention masks are obtained with a non-parametric clustering algorithm that does not require iterative refinement or a predefined number of clusters.
Both \textsc{genesis} and \textsc{slot-attention} are only evaluated on synthetic datasets.
In this work, we use synthetic datasets for quantitative benchmarking, but we also perform experiments on two more challenging real-world datasets.


\section{\textsc{genesis-v2}}

An image  of height , width , with  channels, and pixel values in the interval  is considered to be a three-dimensional tensor .
This work is only concerned with RGB images where , but other input modalities with a different number of channels could also be considered.
Assigning individual pixels to object-like scene components can be formulated as obtaining a set of \emph{object masks}  with  for all pixel coordinate tuples  in an image, where  is the number of scene components.
This can be achieved by modelling the image likelihood  as an SGMM (e.g. \cite{greff2016tagger,greff2017neural,van2018relational,burgess2019monet,greff2019multi,kosiorek2019stacked,veerapaneni2020entity,engelcke2020genesis,engelcke2020reconstruction,locatello2020object}) where  are learnable parameters.
Assuming pixels are independent and a fixed standard deviation  that is shared across object slots, this likelihood can be written as

The summation in \Cref{eq:gen:sgmm} implies that  is \emph{permutation-invariant} to the order of the object representations  (see e.g. \cite{zaheer2017deep,wagstaff2019limitations}) provided that  is also permutation-invariant, therefore allowing the generative model to accommodate for a possibly variable number of object representations.

To segment objects in images and to generate synthetic images in an object-centric fashion requires the formulation of appropriate inference and generative models, i.e.  and , respectively, where  are also learnable parameters.
In the generative model, it is necessary to model relationships between object representations to facilitate the generation of coherent scenes.
Inspired by \textsc{genesis} \cite{engelcke2020genesis}, this is facilitated by an autoregressive prior

\textsc{genesis} uses two separate sets of latent variables to encode object masks and object appearances.
In contrast, \textsc{genesis-v2} uses a single set of latent variables  to encode both and increase parameter sharing.
The graphical model of \textsc{genesis-v2} is shown in comparison to other similar models in \Cref{app:pgm}.

While \textsc{genesis} relies on a recurrent mechanism in the inference model to predict segmentation masks, \textsc{genesis-v2} instead infers latent variables without imposing a fixed ordering and assumes object latents  to be conditionally independent given an input image , i.e., .
Specifically, \textsc{genesis-v2} first extracts an encoding with a deterministic UNet backbone.
This encoding is used to predict a map of \emph{semi-convolutional} pixel embeddings  (see \cite{novotny2018semi}).
These are the inputs to the IC-SBP to obtain a set of normalised \emph{attention masks}  with  via a distance kernel .
Semi-convolutional embeddings are introduced in \citet{novotny2018semi} to facilitate the prediction of unique embeddings for multiple objects of identical appearance.
The embeddings are obtained by the element-wise addition of relative pixel coordinates to two dimensions of the embeddings, which implicitly induces attention masks to be spatially localised.
We leverage this structure to derive principled initialisations for the scaling-factor of different \mbox{IC-SBP} distance kernels  in \Cref{sec:semiconv}.
Inspired by \citet{locatello2020object}, \textsc{genesis-v2} uses the attention masks  to pool a feature vector for each scene component from the deterministic image encoding.
A set of object latents  is then computed from these feature vectors.
This set of latents is decoded in parallel to compute the statistics of the SGMM in \Cref{eq:gen:sgmm}.
The object masks  are normalised with a softmax operator.
The inference and generation models can be jointly trained as a \textsc{vae} as illustrated in \Cref{fig:gpp:model_diagram}.
Further architecture details are described in \Cref{app:architecture}.

\begin{figure}
	\centering
	\includegraphics[trim=0 0 0 0, clip, width=0.9\linewidth]{figures/gpp_diagram.pdf}
	\caption{\textsc{genesis-v2} overview. The image  is passed into a deterministic backbone. The resulting encoding is used to compute the pixel embeddings  which are clustered into attention masks  by the IC-SBP. Features are pooled according to these attention masks to infer the object latents . These are decoded into the object masks  and reconstructed components .}
	\label{fig:gpp:model_diagram}
\end{figure}


\subsection{Instance Colouring Stick-Breaking Process}
\label{ch:gpp:sec:icsbp}
The IC-SBP is a non-parametric, differentiable algorithm that clusters pixel embeddings \mbox{} into a variable number of soft attention masks .
This is achieved by stochastically selecting pixel embeddings as cluster seeds, which are used to group pixels into a set of \emph{randomly ordered} soft clusters.
Inspired by \citet{burgess2019monet}, a \emph{scope}  is initialised to a matrix of ones  to track the degree to which pixels have been assigned to clusters and a matrix of \emph{seed scores} is initialised by sampling from a uniform distribution .
These scores determine which embeddings are selected as cluster seeds.
A single embedding vector  is selected at the spatial location  which corresponds to the argmax of the element-wise multiplication of the seed scores and the current scope.
This ensures that cluster seeds are sampled from pixel embeddings that have not yet been assigned to clusters.
An alpha mask  is computed as the distance between the cluster seed embedding  and all individual pixel embeddings according to a distance kernel .
The output of the kernel  is one if two embeddings are identical and decreases to zero as the distance between a pair of embeddings increases.
The associated attention mask  is obtained by the element-wise multiplication of the alpha masks by the current scope to ensure that the final set of attention masks is normalised.
The scope is then updated by an element-wise multiplication with the complement of the alpha masks and the process is repeated until a stopping condition is satisfied.
At this point, the final scope is added as an additional mask to explain any remaining pixels.
This is described formally in \Cref{alg:gpp:ic-sbp}.

\begin{algorithm}
	\caption{Instance Colouring Stick-Breaking Process}
	\label{alg:gpp:ic-sbp}
	\SetAlgoLined
	\textbf{Input:} embeddings \\
	\textbf{Output:} masks  with \\
	\textbf{Initialise:} masks , scope , seed scores \\
	\BlankLine
	\While{not StopCondition()}{
		i, j = argmax()\;
		 = DistanceKernel(, )\;
		.append()\;
		 = \;
	}
	.append()
\end{algorithm}

When the masks are non-binary, several executions of the algorithm lead to different sets of attention masks, whereby the attention mask for a certain object will also be slightly different between executions.
If the mask values are discrete and exactly equal to either zero or one, however, the set of cluster seeds and the set of attention masks are uniquely defined apart from their ordering.
This can be seen from the fact that if at each step of the IC-SBP produces a binary mask, then embeddings associated with this mask can never be sampled again as cluster seeds due to the masking of the seed scores by the scope.
A different cluster with an associated binary mask is therefore created at every step until all embeddings are uniquely clustered.
This could possibly be facilitated, for example, by applying recurrent mean-shift clustering \cite{kong2018recurrent} to the embeddings before running the IC-SBP.
Another interesting modification would be to make the output of the IC-SBP \emph{permutation-equivariant} with respect to the ordering of the cluster seeds.
This could be achieved either by directly using the cluster seeds for downstream computations or by separating the mask normalisation from the stochastic seed selecting; while the SBP formulation facilitates the selection of a diverse set of cluster seeds, the masks could be normalised separately after the cluster seeds are selected by using a softmax operation or similar.
The exploration of such modifications, however, is left for future work.

In contrast to \textsc{genesis}, the stochastic ordering of the masks implies that it is not possible for \textsc{genesis-v2} to learn a fixed sequential decomposition strategy.
While this does not strictly apply to the last mask which is set equal to the remaining scope, we find empirically that models learn a strategy where the final scope is either largely unused or where it corresponds to a generic background cluster with foreground objects remaining unordered as desired.
Unlike as in iterative refinement where a fixed number of clusters needs to be initialised \emph{a priori}, the IC-SBP can infer a variable number of object representations by using a heuristic that considers the current set of attention masks at every iteration in \Cref{alg:gpp:ic-sbp}.
While we use a fixed number of  masks during training for efficient parallelism on GPU accelerators, we demonstrate that a flexible number of masks can be extracted at test time with minimal impact on segmentation performance.


\subsection{Kernel Initialisation with Semi-Convolutional Embeddings}
\label{sec:semiconv}
For semi-convolutional embeddings to be similar according to a kernel , the model needs to predict a delta vector for each embedding to a specific pixel location to negate the additive relative pixel coordinates.
The model could for example learn to predict a delta vector for each pixel that describes the location of the pixel relative to the centre of the object that it belongs to.
Conversely, if the predicted delta vectors are small, then clustering the embeddings based on their distance to each other only depends on the relative pixel coordinates which results in spatially localised masks.
This structure implies that it is possible to derive a meaningful initialisation for free parameters in the distance kernel  of the IC-SBP.
Established candidates for  from the literature are the Gaussian  \cite{novotny2018semi}, Laplacian  \cite{novotny2018semi}, and Epanechnikov  kernels \cite{kong2018recurrent} with

whereby  and  are two embeddings of equal dimension.
Each kernel contains a scaling factor  which can be initialised so that attention masks at the beginning of training correspond to approximately equally-sized circular patches.
In particular, we initialise these scaling factors as

which is derived and illustrated in \Cref{app:kernel_initialisation}.
After initialisation,  is jointly optimised along with the other learnable parameters of the model.

\subsection{Training}
\label{sec:training}
Following \citet{engelcke2020genesis}, \textsc{genesis-v2} is trained by minimising the GECO objective \cite{rezende2018taming}, which can be written as a loss function of the form

The relative weighting factor  is updated at every training iteration separately from the model parameters according to

 is an exponential moving average of the negative image log-likelihood,  is a momentum factor,  is a step size hyperparameter, and  is a target reconstruction error.
Intuitively, the optimisation decreases the weighting of the KL regularisation term as long as the reconstruction error is larger than the target .
The weighting of the KL term is increased again once the target is satisfied.

We also conduct experiments with an additional auxiliary mask consistency loss that encourages attention masks  and object segmentation masks  to be similar which might be desirable in some applications.
This leads to a modified loss function of the form

in which  and  are interpreted as pixel-wise categorical distributions.
Preliminary experiments indicated that stopping the gradient propagation through the object masks  helps to achieve segmentation quality comparable to using the original loss function in \Cref{eq:loss}.

\section{Experiments}
\label{sec:experiments}

This section presents results on two simulated datasets---ObjectsRoom \cite{multiobjectdatasets19} and ShapeStacks \cite{groth2018shapestacks}---as well as two real-world datasets---Sketchy \cite{cabi2019scaling} and APC \cite{zeng2016multi}---which are described in \Cref{app:datasets}.
\textsc{genesis-v2} is compared against three recent baselines: \textsc{genesis} \cite{engelcke2020genesis}, \textsc{monet} \cite{burgess2019monet}, and \textsc{slot-attention} \cite{locatello2020object}.
Even though \textsc{slot-attention} is trained with a pure reconstruction objective and is not a generative model, it is an informative and strong baseline for unsupervised scene segmentation.
The other models are trained with the GECO objective \cite{rezende2018taming} following the protocol from \citet{engelcke2020genesis} for comparability.
We refer to \textsc{monet} trained with GECO as \textsc{monet-g} to avoid conflating the results with the original settings.
Further training details are described in \Cref{app:training}.
Following prior art (e.g. \cite{greff2019multi,engelcke2020genesis,engelcke2020reconstruction,locatello2020object}), segmentation quality is quantified using the Adjusted Rand Index (ARI) and the Mean Segmentation Covering (MSC) of pixels belonging to foreground objects.
These are averaged across 320 images from the respective test sets.
Similarly, generation quality is measured using the Fr\'{e}chet Inception Distance (FID) \cite{heusel2017gans} which is computed from 10,000 samples and 10,000 test set images using the implementation from \citet{Seitzer2020FID}.
When models are trained with multiple random seeds, we always show qualitative results for the seed that achieves the highest ARI.


\subsection{Benchmarking in Simulation}

\begin{figure}
	\centering
	\begin{subfigure}{0.259\linewidth}
		\includegraphics[trim=0 5 507 0, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_monet_5655e-4_vseed1.pdf}
		\caption{\textsc{monet-g}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 5 507 0, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_genesis_5655e-4_vseed1.pdf}
		\caption{\textsc{genesis}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 5 507 0, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_slot_attention_vseed1.pdf}
		\caption{\textsc{slot-attention}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 90 507 0, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_objectpool_unsorted_5655e-4_vseed1.pdf}
		\caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{\textsc{genesis-v2} learns better reconstructions and segmentations on ShapeStacks.}
	\label{fig:gpp:shapestacks:seg}
\end{figure}

\Cref{fig:gpp:shapestacks:seg} shows qualitative results on ShapeStacks and additional qualitative results are included in \Cref{app:simulation}.
Unlike the baselines, \textsc{genesis-v2} cleanly separates the foreground objects from the background and the reconstructions.
Moreover, \Cref{tab:gpp:seg} summarises quantitative segmentation results on ObjectsRoom and ShapeStacks.
Here, \textsc{genesis-v2} outperforms \textsc{genesis} and \textsc{monet-g} across datasets on all metrics, showing that the IC-SBP is indeed suitable for learning object-centric representations.
\textsc{genesis-v2} outperforms \textsc{slot-attention} in terms of the ARI on both datasets.
\textsc{slot-attention} manages to achieve a better mean MSC, but the variation of those metrics is much larger, which indicates training is not as stable.
While the ARI indicates the models ability to separate objects, it does not penalise the undersegmentation of objects (see \cite{engelcke2020genesis}).
The MSC, in contrast, is an IOU based metric and sensitive to the exact segmentation masks.
We conjecture that \textsc{slot-attention} manages to predict slightly more accurate segmentation masks given that is trained on a pure reconstruction objective and without KL regularisation, thus leading to a slightly better mean MSC.
A set of ablations for \textsc{genesis-v2} is also included in \Cref{app:simulation}.

\begin{table}
	\centering
    \caption{Means and standard deviations of the segmentation metrics from three seeds.}
	\begin{tabular}{lccccc}
		\toprule
		& & \multicolumn{2}{c}{ObjectsRoom} & \multicolumn{2}{c}{ShapeStacks} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-6}
		Model & Generative & ARI-FG & MSC-FG & ARI-FG & MSC-FG \\
		\midrule
		\textsc{monet-g}        & Yes & 0.540.00 & 0.330.01 & {0.700.04} & 0.570.12 \\
		\textsc{genesis}        & Yes & {0.630.03} & {0.530.07} & {0.700.05} & {0.670.02}\\
		\textsc{genesis-v2}      & Yes & \textbf{0.840.01}  & \textbf{0.580.03}  & \textbf{0.810.00} & \textbf{0.680.01}\\
        \midrule
        \textsc{slot-attention} & No & 0.790.02           & \textbf{0.640.13}  & 0.760.01          & \textbf{0.700.05}\\
		\bottomrule
	\end{tabular}
	\label{tab:gpp:seg}
\end{table}

We also examine whether the IC-SBP can indeed be used to extract a variable number of object representations.
This is done by terminating the IC-SBP according to a manual heuristic and setting the final mask to the remaining scope.
Specifically, we terminate the IC-SBP when the sum of the current attention mask values is smaller than twenty pixels.
The results when using a fixed number and a variable number slots after training \textsc{genesis-v2} on ShapeStacks are summarised in \Cref{tab:gpp:dynamic_k}.
With the original training objective in \Cref{eq:loss}, this heuristic only requires 6.3 rather than nine object slots on average, but also leads to a decrease in segmentation performance.
When using the auxiliary mask loss as in \Cref{eq:loss_modified}, only 5.6 object slots are needed on average at almost the same segmentation accuracy.
This heuristic works well on ShapeStacks, but it did not perform well on ObjectsRoom.
Here, the models sometimes predict non-empty attention masks which are associated with empty object segmentation masks, thus making it impossible to terminate the IC-SBP appropriately.
Even though this should be overcome by the mask consistency loss, we found that this auxiliary loss term often caused models to get stuck in local minima where images are segmented into vertical stripes.
These limitations can hopefully be addressed in future work.

\begin{table}
	\centering
	\caption{Means and standard deviations of the segmentation metrics on ShapeStacks from three seeds for \textsc{genesis-v2} with a fixed or flexible number of object slots.}
	\begin{tabular}{llccc}
		\toprule
		Training & Slots & Avg.  & ARI-FG & MSC-FG \\
		\midrule
		No mask loss & Fixed    & 9.00.0 & \textbf{0.810.00} & \textbf{0.680.01} \\
                     & Flexible & \textbf{6.30.4} & 0.770.00 & 0.630.01 \\
        \midrule
		With mask loss & Fixed & 9.00.0 & \textbf{0.810.01} & \textbf{0.680.01} \\
		               & Flexible & \textbf{5.60.1} & \textbf{0.810.00} & 0.670.01 \\
		\bottomrule
	\end{tabular}
	\label{tab:gpp:dynamic_k}
\end{table}

\begin{table}
    \centering
	\caption{Means and standard deviations of FID scores from three seeds.}
	\begin{tabular}{lcc}
		\toprule
		Model & ObjectsRoom & ShapeStacks \\
		\midrule
		\textsc{monet-g}   & 205.77.6         & 197.85.2  \\
		\textsc{genesis}   & 62.82.5          & 186.818.0 \\
		\textsc{slot-att.} & --- & --- \\
		\textsc{genesis-v2} & \textbf{52.62.7} & \textbf{112.73.2} \\
		\bottomrule
	\end{tabular}
	\label{tab:gpp:generation}
\end{table}

\begin{figure}
    \centering
    \parbox{0.45\linewidth}{
    \begin{subfigure}{\linewidth}
		\includegraphics[trim=342 0 0 0, clip, width=\linewidth]{figures/multi_object/gen_multi_object_monet_5655e-4.pdf}
		\caption{\textsc{monet-g}}
		\vspace{6pt}
	\end{subfigure}
    \begin{subfigure}{\linewidth}
		\includegraphics[trim=342 0 0 0, clip, width=\linewidth]{figures/multi_object/gen_multi_object_genesis_5655e-4.pdf}
		\caption{\textsc{genesis}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=342 0 0 0, clip, width=\linewidth]{figures/multi_object/gen_multi_object_objectpool_unsorted_5655e-4.pdf}
		\caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{ObjectsRoom samples.}
	\label{fig:objectsroom_samples}
	}
	\qquad
	\parbox{0.45\linewidth}{
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=342 0 0 0, clip, width=\linewidth]{figures/shapestacks/gen_shapestacks_monet_5655e-4.pdf}
		\caption{\textsc{monet-g}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=342 0 0 0, clip, width=\linewidth]{figures/shapestacks/gen_shapestacks_genesis_5655e-4.pdf}
		\caption{\textsc{genesis}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=342 0 0 0, clip, width=\linewidth]{figures/shapestacks/gen_shapestacks_objectpool_unsorted_5655e-4.pdf}
		\caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{ShapeStacks samples.}
	\label{fig:shapestacks_samples}
	}
\end{figure}

Regarding scene generation, \Cref{tab:gpp:generation} summarises the FID scores for \textsc{genesis-v2} and the baselines on ObjectsRoom and ShapeStacks.
\textsc{genesis-v2} consistently performs best with a particularly significant improvement on ShapeStacks.
Results on ShapeStacks, however, are not as good as for ObjectsRoom, which is likely caused by the increased visual complexity of the images in the ShapeStacks dataset.
Qualitative results for scene generation are shown in \Cref{fig:objectsroom_samples,fig:shapestacks_samples}.
Both \textsc{genesis} and \textsc{genesis-v2} produce reasonable samples after training on ObjectsRoom.
For ShapeStacks, samples from \textsc{genesis} contain significant distortions.
In comparison, samples from \textsc{genesis-v2} are more realistic, but also still show room for improvement.


\subsection{Real-World Applications}
\label{sec:experiments:realworld}

After validating \textsc{genesis-v2} on two simulated datasets, this section present experiments on the Sketchy \cite{cabi2019scaling} and APC datasets \cite{zeng2016multi}; two significantly more challenging real-world datasets collected in the context of robot manipulation.
Reconstructions and segmentations after training \textsc{genesis-v2} on Sketchy and APC are shown in \Cref{fig:gpp:sketchy:seg,fig:gpp:apc:seg}.
For Sketchy, it can be seen that \textsc{genesis-v2} and \textsc{slot-attention} disambiguate the individual foreground objects and the robot gripper fairly well.
\textsc{slot-attention} produces slightly more accurate reconstructions, which is likely facilitated by the pure reconstruction objective that the model is trained with.
However, \textsc{genesis-v2} is the only model that separates foreground objects from the background in APC images.
Environment conditions in Sketchy are highly controlled and \textsc{slot-attention} appears to be unable to handle the more complex conditions in APC images.
Nevertheless, \textsc{genesis-v2} also struggles to capture the fine-grained details and oversegments one of the foreground objects into several parts, leaving room for improvement in future work.
Additional qualitative results are included in \Cref{app:realworld}.

\begin{figure}
	\centering
	\begin{subfigure}{0.259\linewidth}
		\includegraphics[trim=0 5 507 0, clip, width=\linewidth]{figures/sketchy/inf_sketchy_monet_5645e-4_vseed0.pdf}
		\caption{\textsc{monet-g}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 5 507 0, clip, width=\linewidth]{figures/sketchy/inf_sketchy_genesis_5645e-4_vseed0.pdf}
		\caption{\textsc{genesis}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 5 507 0, clip, width=\linewidth]{figures/sketchy/inf_sketchy_slot_attention_vseed0.pdf}
		\caption{\textsc{slot-attention}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 90 507 0, clip, width=\linewidth]{figures/sketchy/inf_sketchy_objectpool_5645e-4_vseed0.pdf}
		\caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{In contrast to \textsc{monet-g} and \textsc{genesis}, \textsc{genesis-v2} as well as \textsc{slot-attention} are able to learn reasonable object segmentations on the more challenging Sketchy dataset.}
	\label{fig:gpp:sketchy:seg}
\end{figure}
\begin{figure}
	\centering
	\begin{subfigure}{0.259\linewidth}
		\includegraphics[trim=0 5 507 0, clip, width=\linewidth]{figures/apc/inf_mitprinceton_monet_5655e-4.pdf}
		\caption{\textsc{monet-g}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 5 507 0, clip, width=\linewidth]{figures/apc/inf_mitprinceton_genesis_5655e-4.pdf}
		\caption{\textsc{genesis}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 5 507 0, clip, width=\linewidth]{figures/apc/inf_mitprinceton_slot_attention_vseed0.pdf}
		\caption{\textsc{slot-attention}}
	\end{subfigure}
	\begin{subfigure}{0.24\linewidth}
		\includegraphics[trim=14 90 507 0, clip, width=\linewidth]{figures/apc/inf_mitprinceton_objectpool_5655e-4_vseed0.pdf}
		\caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{\textsc{genesis-v2} is the only model that separates the foreground objects in images from APC.}
	\label{fig:gpp:apc:seg}
\end{figure}


There are no ground-truth object masks available for Sketchy, but segmentation performance can be evaluated quantitatively on APC for which annotations are available with the caveat that there is only a single foreground object per image.
In this case, when computing the segmentation metrics from foreground pixels alone, the ARI could be trivially maximised by assigning all image pixels to the same component and the MSC will be equal to the largest IOU of the predicted masks with the foreground objects.
When considering all pixels, the optimal solution for both metrics is to have exactly one set of pixels assigned to the foreground object and another set of pixels being assigned to the background.
\Cref{tab:gpp:apc} therefore reports ARI and MSC scores computed from all pixels \textsc{genesis-v2} and the baselines.
\textsc{genesis-v2} performs much better than the baselines on both metrics, corroborating that \textsc{genesis-v2} takes a step in the right direction in terms of learning unsupervised object-representations from \emph{real-world} datasets.

FID scores for generated images are summarised in \Cref{tab:gpp:fid} and qualitative results are included in \Cref{app:realworld}.
\textsc{genesis-v2} achieves the best FID on Sketchy, but it is outperformed by \textsc{genesis} on APC.
Both models consistently outperform \textsc{monet-g}.
All of the FID scores, however, are fairly large which is not surprising given the much higher visual complexity of these images.
It is therefore difficult to draw strong conclusions from these beyond a rough sense of sample quality.
Further work is required to generate high-fidelity images after training on real-world datasets.

\begin{table*}
	\parbox{0.49\linewidth}{
	    \centering
    	\caption{Segmentation metrics on APC.}
    	\begin{tabular}{lcc}
    		\toprule
    		& ARI  & MSC \\
    		\midrule
    		\textsc{monet-g}     & 0.11 & 0.48 \\
    		\textsc{genesis}   & 0.04 & 0.29 \\
    		\textsc{slot-att.} & 0.03 & 0.25 \\
    		\textsc{genesis-v2} & \textbf{0.55} & \textbf{0.67} \\
    		\bottomrule
    	\end{tabular}
    	\label{tab:gpp:apc}
	}
	\parbox{0.49\linewidth}{
    	\centering
    	\caption{FID scores on Sketchy and APC.}
    	\begin{tabular}{lcc}
    		\toprule
    		& Sketchy & APC \\
    		\midrule
    		\textsc{monet-g}   & 294.3 & 269.3 \\
    		\textsc{genesis}   & 241.9 & \textbf{183.2} \\
    		\textsc{slot-att.} & --- & --- \\
    		\textsc{genesis-v2} & \textbf{208.1} & 245.6 \\
    		\bottomrule
    	\end{tabular}
    	\label{tab:gpp:fid}
	}
\end{table*}

\section{Conclusions}

This work develops \textsc{genesis-v2}, a novel object-centric latent variable model of scenes which is able to both decompose visual scenes into semantically meaningful constituent parts while at the same time being able to generate coherent scenes in an object-centric fashion.
\textsc{genesis-v2} leverages a non-parametric, differentiable clustering algorithm for grouping pixel embeddings into a variable number of attention masks which are used to infer an unordered set of object representations.
This approach is validated empirically on two established simulated datasets as well as two additional real-world datasets.
The results show that \textsc{genesis-v2} takes a step towards learning better object-centric representations without labelled supervision from real-world datasets.
Yet, there is still room for improvement in terms of reconstruction, segmentation, and sample quality.
It would also be interesting to investigate the ability of \textsc{genesis-v2} and the IC-SBP to handle out-of-distribution images that contain more objects than seen during training.

\clearpage


\begin{ack}


This research was supported by an EPSRC Programme Grant (EP/V000748/1) and a gift from AWS. The authors would like to acknowledge the use of the University of Oxford Advanced Research Computing (ARC) facility in carrying out this work, \url{http://dx.doi.org/10.5281/zenodo.22558}, and the use of Hartree Centre resources. The authors thank Adam R. Kosiorek for useful comments.
\end{ack}


{\small
\bibliographystyle{unsrtnat}
\bibliography{references}}













\clearpage

\appendix

\section{Graphical Models}
\label{app:pgm}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.85\linewidth]{figures/pgm.pdf}
	\caption{Graphical model of \textsc{genesis-v2} compared to a standard \textsc{vae} \cite{kingma2013auto,rezende2014stochastic}, \textsc{monet} \cite{burgess2019monet}, \textsc{iodine} \cite{greff2019multi}, and \textsc{genesis} \cite{engelcke2020genesis}.  denotes the number of refinement iterations in \textsc{iodine}. \textsc{genesis} and \textsc{genesis-v2} capture correlations between object slots with an autoregressive prior.}
	\label{fig:pgms}
\end{figure}


\section{GENESIS-V2 Architecture Details}
\label{app:architecture}

The \textsc{genesis-v2} architecture consists of four main components: a deterministic backbone, the attention and object pooling module, the component decoders, and an optional autoregressive prior which are described in detail below.

\paragraph{Backbone}
\textsc{genesis-v2} uses a UNet \citep{ronneberger2015u} encoder similar to the attention network in the re-implementation of \textsc{monet} in \citet{engelcke2020genesis} with  filters in the encoder and the reverse in the decoder.
Each convolutional block decreases or increases the spatial resolution by a factor of two and there are two hidden layers with 128 units each in between the encoder and the decoder.
The only difference to the UNet implementation in \citet{engelcke2020genesis} is that the instance normalisation (IN) layers \citep{ulyanov2016instance} are replaced with group normalisation (GN) layers \citep{wu2018group} to preserve contrast information.
The number of groups is set to eight in all such layers which is also referred to as a GN8 layer. 
The output of this backbone encoder is a feature map  with  output channels and spatial dimensions that are equal to the height and width of the input image.

\paragraph{Attention and Object Pooling}
Following feature extraction, an \emph{attention head} computes pixel-wise semi-convolutional embeddings  with eight channels, i.e. , as in \citet{novotny2018semi}.
The attention head consists of a  Conv-GN8-ReLU block with 64 filters and a  semi-convolutional layer.
The pixel embeddings are clustered into  attention masks  using the IC-SBP.
A Gaussian kernel  is used unless noted otherwise.
A \emph{feature head} consisting of a  Conv-GN8-ReLU block with 64 filters and a  convolution with 128 filters refines the encoder output  to obtain a new feature map  with .
Similar to \citet{locatello2020object}, the attention masks  are used to pool feature vectors from the feature map  by multiplying the feature map with an individual attention mask and summing across the spatial dimensions.
Each pooled feature vector is normalised by dividing by the sum of the attention mask values plus a small epsilon value to avoid numerical instabilities.
Finally, a \emph{posterior head} uses layer normalisation \citep{ba2016layer} followed by a fully-connected ReLU block with 128 units and a second fully-connected layer to compute the sufficient statistics of the individual object latents  with  from pooled feature vector.

\paragraph{Component Decoders}
Following \citet{greff2019multi} and \citet{locatello2020object}, the object latents are decoded by separate decoders with shared weights to parameterise the sufficient statistics of the SGMM in \Cref{eq:gen:sgmm}.
Each decoded component has four channels per pixel.
The first three channels contain the RGB values and the fourth channel contains the unnormalised segmentation logits which are normalised across scene components using a softmax operator. 
Again following \citet{locatello2020object}, the first layer is a spatial broadcasting module as introduced in \citet{watters2019spatial} which is designed to facilitate the disentanglement of the independent factors of variation in a dataset.
An additional advantage of spatial broadcasting is that it requires a smaller number of parameters than a fully-connected layer when upsampling a feature vector to a specific spatial resolution.
The spatial broadcasting module is followed by four stride-2 deconvolutional GN8-ReLU layers with 64 filters to retrieve the full image resolution before a final  convolution which computes the four output channels.

\paragraph{Autoregressive Prior}
Identical to \textsc{genesis} \cite{engelcke2020genesis}, the autoregressive prior for scene generation is implemented as an LSTM \citep{hochreiter1997long} followed by a fully-connected linear layer with 256 units to infer the sufficient statistics of the prior distribution for each component.

\section{Kernel Initialisation}
\label{app:kernel_initialisation}

Assume a maximum of  scene components to be present in an image and that the delta vectors for the semi-convolutional embeddings are initialised to zero at the beginning of training.
For each initial mask to cover approximately the same area of an image, further assume that the circular isocontours of the kernels are packed into an image in a square fashion.
Using linear relative pixel coordinates in  and dividing an image into  equally sized squares, each square has a side-length of .
Let the mask value decrease to  at the intersection of the square and the circular isocontour, i.e., at a distance of  from the centre of the kernel as illustrated in \Cref{fig:gpp:sigma_init}.
Solving this for each kernel in \Cref{eq:kernels} leads to

Examples of the initial masks obtained when running the IC-SBP with the proposed initialisations are illustrated in \Cref{fig:gpp:icsbp_init}.
\begin{figure}[h!]
    \begin{minipage}[b]{0.45\linewidth}
    	\centering
    	\includegraphics[trim=0 0 0 0, clip, width=\linewidth]{figures/sigma_init.pdf}
    	\vspace{25pt}
    	\caption{Illustration of packing  circular kernels into a square image and linear relative pixel coordinates in , resulting in circular isocontours of radius .}
    	\label{fig:gpp:sigma_init}
	\end{minipage}
    \quad
    \begin{minipage}[b]{0.45\linewidth}
    	\centering
    	\begin{subfigure}{\linewidth}
    		\includegraphics[trim=15 0 170 255, clip, width=\linewidth]{figures/icsbp_init/inf_multi_object_objectpool_gaussian_5655e-4.pdf}
    		\caption{Gaussian kernel - }
    		\vspace{6pt}
    	\end{subfigure}
    	\begin{subfigure}{\linewidth}
    		\includegraphics[trim=15 0 170 255, clip, width=\linewidth]{figures/icsbp_init/inf_multi_object_objectpool_laplacian_5655e-4.pdf}
    		\caption{Laplacian kernel - }
    		\vspace{6pt}
    	\end{subfigure}
    	\begin{subfigure}{\linewidth}
    		\includegraphics[trim=15 0 170 255, clip, width=\linewidth]{figures/icsbp_init/inf_multi_object_objectpool_epanechnikov_5655e-4.pdf}
    		\caption{Epanechnikov kernel - }
    		\vspace{6pt}
    	\end{subfigure}
    	\caption{Initial masks obtained when running the IC-SBP with different randomly sampled seed scores, using the initialisations in \Cref{eq:gpp:sigma_init} and .}
    	\label{fig:gpp:icsbp_init}
    	\vspace{1pt}
	\end{minipage}
\end{figure}


\clearpage

\section{Datasets}
\label{app:datasets}

We evaluate \textsc{genesis-v2} on simulated images from ObjectsRoom \cite{multiobjectdatasets19} and ShapeStacks \cite{groth2018shapestacks} as well as real-world images from Sketchy \cite{cabi2019scaling} and APC \cite{zeng2016multi}.
ObjectsRoom and ShapeStacks are well established in the context of training OCGMs and we follow the same preprocessing procedures as used in \citet{engelcke2020genesis} and \citet{engelcke2020reconstruction}.
As in these works, the default number of object slots is set to  and  for ObjectsRoom and ShapeStacks, respectively, across all models.
This work is the first to train and evaluate OCGMs on Sketchy and APC.
We therefore developed our own preprocessing and training/validation/test splits, which are described in detail below.
The exact splits that were used will be released along with the code for reproducibility.

\paragraph{Sketchy}
The Sketchy dataset \citep{cabi2019scaling} is designed for off-policy reinforcement learning (RL), providing episodes showing a robotic arm performing different tasks that involve three differently coloured shapes (blue, red, green) or a cloth.
The dataset includes camera images from several viewpoints, depth images, manipulator joint information, rewards, and other meta-data.
The dataset is quite considerable in size and takes about 5TB of storage in total.
We ease the computational and storage demands by only using a subset of this dataset.
Specifically, we use the high-quality demonstrations from the ``lift-green'' and ``stack-green-on-red'' tasks corresponding to a total of 395 episodes, 10\% of which are set aside as validation and test sets each.
Sketchy also contains episodes from a task that involves lifting a cloth and an even larger number of lower-quality demonstrations that offer a wider coverage of the state space.
We restrict ourselves to the high-quality episodes that involve the manipulation of solid objects.
The number of high-quality episodes alone is already considerable and we want to evaluate whether the models can separate multiple foreground objects.
From these episodes, we use the images from the front-left and front-right cameras which show the arm and the foreground objects without obstruction.

The raw images have a resolution of 600-by-960 pixels. To remove uninteresting pixels belonging to the background, 144 pixels on the left and right are cropped away for both camera views, the top 71 and bottom 81 pixels are cropped away for the front-left view, and the top 91 and bottom 61 are cropped away for the front-right view, resulting in a 448-by-672 crop.
From this 448-by-672 crop, seven square crops are extracted to obtain a variety of views for the models to learn from.
The first crop corresponds to the centre 448-by-448 pixels.
For the other six crops, the top and bottom left, centre, and right squares of size 352 are extracted.
Finally, we resize these crops to a resolution of 128-by-128 to reduce the computational demands of training the models.
This leads to a total of 337,498 training; 41,426 validation; and 41,426 test images.
Examples of images obtained with this preprocessing procedure are shown in \Cref{fig:gpp:sketchy:crops}.
The default number of object slots is set to  across all models to give them sufficient flexibility to discover different types of solutions.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fl_full.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fl_c0.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fl_c1.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fl_c2.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fl_c3.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fl_c4.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fl_c5.png}
		\caption{Front-left camera}
	\end{subfigure}
	\par\smallskip
	\begin{subfigure}{\linewidth}
		\centering
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fr_full.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fr_c0.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fr_c1.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fr_c2.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fr_c3.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fr_c4.png}
		\includegraphics[width=0.12\linewidth]{figures/sketchy/sketchy_fr_c5.png}
		\caption{Front-left camera}
	\end{subfigure}
	\caption{128-by-128 crops as used for training, extracted from the front-left and front-right cameras of a single image from the Sketchy dataset \citep{cabi2019scaling}. Showing from left to right: centre, top-left, top-centre, top-right, bottom-left, bottom-centre, and bottom-right crops.}
	\label{fig:gpp:sketchy:crops}
\end{figure}



\paragraph{APC}
For their entry to the 2016 Amazon Picking Challenge (APC), the MIT-Princeton team created and released an object segmentation training set, showing a single challenge object either on a shelf or in a tray \citep{zeng2016multi}.
The raw images are first resized so that the shorter image side has a length of 128 pixels.
The centre 128-by-128 pixels are then extracted to remove uninteresting pixels belonging to the background.
Example images after processing are shown in \Cref{fig:gpp:apc:crop}.
For each object, there exists a set of scenes showing the object in different poses on both the shelf and in the red tray.
For each scene, there are images taken from different camera viewpoints.
We select 10\% of the scenes at random to be set aside for validation and testing each so that scenes between the training, validation, and test sets do not overlap.
The resulting training, validation, and test sets consist of 109,281; 13,644; and 13,650 images, respectively.
As for Sketchy, the default number of object slots is set to  to provide enough flexibility for models to discover different types of solutions.
\begin{figure}[h!]
	\begin{subfigure}{\linewidth}
	    \centering
		\includegraphics[width=0.13\linewidth]{figures/apc/color1.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/color3.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/color6.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/color2.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/color4.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/color5.png}
		\caption{Images}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \centering
		\includegraphics[width=0.13\linewidth]{figures/apc/mask1.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/mask3.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/mask6.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/mask2.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/mask4.png}
		\includegraphics[width=0.13\linewidth]{figures/apc/mask5.png}
		\caption{Ground truth segmentation masks}
	\end{subfigure}
	\caption{Examples from the APC dataset \citep{zeng2016multi} after cropping and resizing.}
	\label{fig:gpp:apc:crop}
\end{figure}


\section{Training Details}
\label{app:training}

All models apart from \textsc{slot-attention} are trained with the protocol from \citet{engelcke2020genesis} for comparability, which minimises the GECO objective \cite{rezende2018taming} using the Adam optimiser \citep{kingma2014adam}, a learning rate of , a batch size of 32, and 500,000 training iterations.
The Gaussian standard deviation  in \Cref{eq:gen:sgmm} is set to  and GECO reconstruction goal is set to a negative log-likelihood value per pixel and per channel of  for the simulated datasets and the APC dataset.
For Sketchy, a GECO goal of  was found to lead to better segmentations and was used instead.
As in \citet{engelcke2020genesis}, the GECO hyperparameters are set to ,  when  and  otherwise.
 is initialised to  and clamped to a minimum value of .
For experiments with the auxiliary mask consistency loss in \Cref{eq:loss_modified}, we found that an initial high weighting of the mask loss inhibits the learning good segmentations, so in these experiments  is initialised to  instead.
We refer to \textsc{monet} trained with GECO as \textsc{monet-g} to avoid conflating the results with the original settings from \citet{burgess2019monet}.
\textsc{slot-attention} is trained using the official reference implementation with default hyperparameters.
For all models, training on 64-by-64 images from ObjectsRoom and ShapeStacks takes around two days with a single NVIDIA Titan RTX GPU.
Similarly, training on 128-by-128 images from Sketchy and APC takes around eight days.



\section{Benchmarking in Simulation}
\label{app:simulation}

\begin{table}[h!]
	\centering
	\caption{\textsc{genesis-v2} ablations showing means and standard deviations from three seeds.}
    
	\begin{tabular}{l cccc}
		\toprule
		& \multicolumn{2}{c}{ObjectsRoom} & \multicolumn{2}{c}{ShapeStacks} \\
		\cmidrule(lr){2-3} \cmidrule(lr){4-5}
		Variant & ARI & MSC & ARI & MSC \\
		\midrule
		Independent prior,      & \textbf{0.790.01}         & 0.470.17           & \textbf{0.790.01}        & \textbf{0.670.00} \\
		Independent prior,      & 0.740.08           & \textbf{0.480.20}         & \textbf{0.790.01}        & \textbf{0.670.01} \\
		Independent prior,      & 0.780.01           & 0.340.08           & 0.780.01          & 0.660.01\\
		\midrule
		Autoregressive prior,  & \textbf{0.840.01}  & \textbf{0.580.03}  & \textbf{0.810.00} &
		\textbf{0.680.01}\\
		\bottomrule
	\end{tabular}
	\label{tab:gpp:ablations}
\end{table}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 175 0 0, clip, width=\linewidth]{figures/multi_object/inf_multi_object_monet_5655e-4.pdf}
		\caption{Input images}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/multi_object/inf_multi_object_monet_5655e-4.pdf}
		\caption{\textsc{monet-g}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/multi_object/inf_multi_object_genesis_5655e-4.pdf}
		\caption{\textsc{genesis}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/multi_object/inf_multi_object_slot_attention_vseed0.pdf}
		\caption{\textsc{slot-attention}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 89 0 87, clip, width=\linewidth]{figures/multi_object/inf_multi_object_objectpool_unsorted_5655e-4.pdf}
		\caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{ObjectsRoom reconstructions and segmentations.}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 175 0 0, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_monet_5655e-4_vseed1.pdf}
		\caption{Input images}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_monet_5655e-4_vseed1.pdf}
		\caption{\textsc{monet-g}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_genesis_5655e-4_vseed1.pdf}
		\caption{\textsc{genesis}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_slot_attention_vseed1.pdf}
		\caption{\textsc{slot-attention}}
		\vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
		\includegraphics[trim=0 89 0 87, clip, width=\linewidth]{figures/shapestacks/inf_shapestacks_objectpool_unsorted_5655e-4_vseed1.pdf}
		\caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{ShapeStacks reconstructions and segmentations.}
\end{figure}

\clearpage


\section{Real-World Applications}
\label{app:realworld}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 175 0 0, clip, width=\linewidth]{figures/sketchy/inf_sketchy_monet_5645e-4_vseed0.pdf}
	    \caption{Input images}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/sketchy/inf_sketchy_monet_5645e-4_vseed0.pdf}
	    \caption{\textsc{monet-g}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/sketchy/inf_sketchy_genesis_5645e-4_vseed0.pdf}
	    \caption{\textsc{genesis}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/sketchy/inf_sketchy_slot_attention_vseed0.pdf}
	    \caption{\textsc{slot-attention}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 89 0 87, clip, width=\linewidth]{figures/sketchy/inf_sketchy_objectpool_5645e-4_vseed0.pdf}
	    \caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{Sketchy reconstructions and segmentations.}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 175 0 0, clip, width=\linewidth]{figures/apc/inf_mitprinceton_monet_5655e-4.pdf}
	    \caption{Input images}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/apc/inf_mitprinceton_monet_5655e-4.pdf}
	    \caption{\textsc{monet-g}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/apc/inf_mitprinceton_genesis_5655e-4.pdf}
	    \caption{\textsc{genesis}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 0 0 87, clip, width=\linewidth]{figures/apc/inf_mitprinceton_slot_attention_vseed0.pdf}
	    \caption{\textsc{slot-attention}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[trim=0 89 0 87, clip, width=\linewidth]{figures/apc/inf_mitprinceton_objectpool_5655e-4_vseed0.pdf}
	    \caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{APC reconstructions and segmentations.}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\linewidth}
	    \includegraphics[width=\linewidth]{figures/sketchy/gen_sketchy_monet_5645e-4.pdf}
	    \caption{\textsc{monet-g}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[width=\linewidth]{figures/sketchy/gen_sketchy_genesis_5645e-4.pdf}
	    \caption{\textsc{genesis}}
	    \vspace{6pt}
	\end{subfigure}
    \begin{subfigure}{\linewidth}
	    \includegraphics[width=\linewidth]{figures/sketchy/gen_sketchy_objectpool_5645e-4.pdf}
	    \caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{Sketchy samples.}
	\label{fig:gpp:sketchy:gen}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\linewidth}
	    \includegraphics[width=\linewidth]{figures/apc/gen_mitprinceton_monet_5655e-4.pdf}
	    \caption{\textsc{monet-g}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[width=\linewidth]{figures/apc/gen_mitprinceton_genesis_5655e-4.pdf}
	    \caption{\textsc{genesis}}
	    \vspace{6pt}
	\end{subfigure}
	\begin{subfigure}{\linewidth}
	    \includegraphics[width=\linewidth]{figures/apc/gen_mitprinceton_objectpool_5655e-4.pdf}
	    \caption{\textsc{genesis-v2}}
	\end{subfigure}
	\caption{APC samples.}
	\label{fig:gpp:apc:gen}
\end{figure}


\end{document}
