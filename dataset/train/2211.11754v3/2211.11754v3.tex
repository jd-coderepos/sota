

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{An_Algorithm_for_Routing_Vectors_in_Sequences}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[bottom]{footmisc}
\usepackage{url}
\usepackage[linesnumbered,lined,algoruled]{algorithm2e}
\usepackage{multicol}
\usepackage{qtree}
\captionsetup[figure]{labelfont=bf}

\graphicspath{{png/}}

\aclfinalcopy 



\newcommand\BibTeX{B{\sc ib}\TeX}

\title{	An Algorithm for Routing Vectors in Sequences }


\author{Franz A. Heinsen \\
{\tt franz@glassroom.com} \\ }

\date{[Month Day], 2021}

\newcommand{\suptag}[1]{^{{\scriptscriptstyle\tt (#1)}}}
\newcommand{\iters}{\suptag{iters}}
\newcommand{\inp}{\suptag{inp}}
\newcommand{\out}{\suptag{out}}
\newcommand{\use}{\suptag{use}}
\newcommand{\ign}{\suptag{ign}}
\newcommand{\seq}{\suptag{seq}}
\newcommand{\hid}{\suptag{hid}}
\newcommand{\emb}{\suptag{emb}}

\newcommand{\fnA}{\mathcal{A}}
\newcommand{\fnF}{\mathcal{F}}
\newcommand{\fnG}{\mathcal{G}}
\newcommand{\fnS}{\mathcal{S}}
\newcommand{\fnU}{\mathcal{U}}
\newcommand{\fnR}{\mathcal{R}}
\newcommand{\fnM}{\mathcal{M}}
\newcommand{\fnB}{\mathcal{B}}

\newcommand{\bigO}{\mathcal{O}}

\newcommand{\padval}{\square}
\newcommand{\eobval}{\boxtimes}

\newcommand{\routing}{\mathsf{R}}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\newcommand{\commentstyle}[1]{\scriptsize\sffamily{#1}}
\newcommand{\textcomment}[1]{\text{\commentstyle{#1}}}

\SetCommentSty{commentstyle}

\begin{document}
\maketitle

\begin{abstract}
We propose a routing algorithm that takes a sequence of vectors and computes a new sequence with specified length and vector size. Each output vector maximizes ``bang per bit,'' the difference between a net benefit to use and net cost to ignore data, by better predicting the input vectors. We describe output vectors as geometric objects, as latent variables that assign credit, as query states in a model of associative memory, and as agents in a model of a Society of Mind. We implement the algorithm with optimizations that reduce parameter count, computation, and memory use by orders of magnitude, enabling us to route sequences of greater length than previously possible. We evaluate our implementation on natural language and visual classification tasks, obtaining competitive or state-of-the-art accuracy and end-to-end credit assignments that are interpretable.\footnote{Source code and instructions for replicating our results are online at \href{https://github.com/glassroom/heinsen_routing}{https://github.com/glassroom/heinsen\_routing}.}
\end{abstract}


\section{Introduction}\label{sec:introduction}

A longstanding goal in Artificial Intelligence is to formulate learning systems that assign credit, such that, when they succeed or fail in a task, we can determine and interpret which components of the system are responsible. A possible approach to the credit assignment problem is to route capsules at multiple levels of composition. A capsule is a group ({\em e.g.}, vector, matrix) of artificial neurons representing the properties of an entity in a context ({\em e.g.}, a token of text in a paragraph, an object depicted in an image). Routing consists of assigning data from input capsules, each representing a detected entity, to output capsules, each representing a detectable entity, by finding or computing agreement in some form ({\em e.g.}, identifying clusters) among candidate output capsules proposed by transforming the input capsules. Each output capsule is computed as a mixture of the candidates proposed for it on which the most input capsules agree, thereby assigning credit to those input capsules. If we compose multiple routings into a deep neural network, in every forward pass it assigns credit to input capsules representing the entities detected at each level of composition.

To date, deep neural networks applying various routing methods have shown promise in multiple domains, including vision and natural language, but only on small-scale tasks \cite{tsai2020Capsules} \cite{ribeiro2020capsule} \cite{NEURIPS2019_e46bc064} \cite{DBLP:journals/corr/abs-1902-05770} \cite{DBLP:journals/corr/abs-1911-00792} \cite{DBLP:journals/corr/abs-1904-09546} \cite{xinyi2018capsule} \cite{DBLP:journals/corr/abs-1805-10807} \cite{DBLP:journals/corr/abs-1805-10807} \cite{wang2018an} \cite{46653} \cite{DBLP:journals/corr/abs-1710-09829}. Application of previously proposed routing methods to large-scale tasks has been impractical due to computational complexity, which increases in both space and time as a function of the length of input and output sequences, the number of elements per capsule, and the number of pairwise interactions between input, proposed, and output capsules.

\begin{figure*}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_sample_credit_assignments_natural_language_for_introduction}}
		\caption{Typical example of end-to-end credit assignment, in this case for classifying the sentiment of a movie review. See Figures \ref{fig:sample_credit_assignments_vision} and \ref{fig:sample_credit_assignments_natural_language} for additional examples, including a typical example in vision.}
		\label{fig:sample_credit_assignments_natural_language_for_introduction}
	\end{center}
	\vskip -0.2in
\end{figure*}

Here, we adapt the routing algorithm proposed by \citet{DBLP:journals/corr/abs-1911-00792} to operate on vectors as the capsules, generalize the algorithm by formulating it in terms of four neural networks (differentiable functions), and implement it with optimizations that reduce parameter count, memory use, and computation by orders of magnitude. The four neural networks are:  for obtaining an activation score per input vector,  for obtaining a different sequence of proposed output vectors given each input vector,  for predicting input vectors given a sequence of output vectors, and  for scoring actual versus predicted input vectors to quantify agreement. The algorithm is iterative. In each iteration, we update the state of all output vectors in parallel. We assign data from each input vector to the output vectors which best predict it, and compute each output vector's updated state by maximizing ``bang per bit,'' the difference between a net benefit to use and a net cost to ignore input vector data. The output sequence's final state is that which maximizes ``bang per bit'' by best predicting, or explaining, the given input sequence.

Motivated by consilience, we describe output vectors from four different viewpoints: First, we describe them as geometric objects whose states are updated by linearly combining dynamically computed coefficients and components in a different basis for each output vector. Second, we describe output vectors as latent variables whose updated states are computed via credit assignments that are additive, like the Shapley values obtainable via SHAP methods \cite{NIPS2017_7062}, and also composable on their own ({\em i.e.}, independently of data transformations), subject to certain conditions. Third, we describe output vectors as query states in a model of associative memory, which, if we disregard the net cost to ignore data and restrict  , , , and  in significant ways, reduces to a modern Hopfield network with the structure of a bipartite graph \cite{krotov2021large} \cite{ramsauer2021hopfield}, of which Transformer self-attention \cite{DBLP:journals/corr/VaswaniSPUJGKP17} is a notable special case. Fourth, we describe output vectors as agents competing to maximize utility by using or ignoring scarce resources through ``knowledge lines,'' or K-lines, in a ``block,'' which itself can interact with other blocks in a network modeling a Society of Mind \cite{10.5555/22939}.

Our sample implementation of the algorithm incorporates three significant optimizations: First, we define  as the composition of a scaled tensor product and a linear transformation with positionwise biases, instead of as a different linear transformation and bias per interaction (as in the original variant of the algorithm), reducing parameter count by orders of magnitude. Second, we evaluate  lazily in each iteration, instead of eagerly before the first iteration, to avoid storing all elements of 's output simultaneously in memory as intermediate values, reducing memory footprint by orders of magnitude while increasing computation only linearly in the number of iterations. Third, we decompose the computation of all updated output vector states into a sequence of efficient tensor contractions, reducing memory footprint and computation by orders of magnitude.

We measure our implementation's parameter count, memory footprint, and execution time, and find they are linear in each of the number of input vectors, the size of input vectors, the number of output vectors, and the size of output vectors, enabling fine-grained control over memory consumption and computational cost. We successfully route input sequences with 1 million vectors, each a capsule with 1024 elements, at full (32-bit floating point) precision, keeping track of gradients, consuming under 18GB of memory on widely available commodity hardware. To the best of our knowledge, no implementation of any previously proposed routing method has been able to route as many capsules on any kind of hardware.

Finally, we evaluate our implementation on classification benchmarks in natural language and vision. In all benchmarks, we obtain accuracy competitive with or better than the state of the art, along with additive credit assignments that are composable independently of data transformations. We compute end-to-end credit assignments and find they are interpretable (Figures \ref{fig:sample_credit_assignments_natural_language_for_introduction},  \ref{fig:sample_credit_assignments_vision}, \ref{fig:sample_credit_assignments_natural_language}).


\subsection{Notation}

In mathematical expressions of tensor transformations, we show all indices as subscript text, implicitly assume broadcasting for any missing indices, perform all operations elementwise, and explicitly show all summations. Superscript text in parenthesis denotes labels. See Table \ref{tab:notation_examples} for examples. We do not use the notation of Linear Algebra because it cannot handle more than two indices. We do not use Einstein's implicit summation notation because it would require the use of operators for raising and lowering indices, adding complexity that is unnecessary for our purposes.

\begin{table}[h]
	\small
	\begin{center}
		\begin{tabular}{@{}ll@{}}
			\toprule
			Example                                                              & Implementation in Python \\
			\midrule
			\addlinespace[0.6em]
			         & {\tt y = x1[:,:,None] + x2} \\
			\addlinespace[0.6em]
			           & {\tt y = x1[:,:,None] * x2} \\
			\addlinespace[0.6em]
			     & {\tt y = x1 @ x2} \\
			\addlinespace[0.6em]
			 & {\tt y = (x1 @ x2).exp().T} \\
			\addlinespace[0.6em]
			   & {\tt y = (x1 @ x2).sum(dim=0)} \\
			\addlinespace[0.6em]
			\bottomrule
		\end{tabular}
	\end{center}
	\caption{\label{tab:notation_examples}Examples of the notation we use, with all-subscript tensor indices, elementwise operations, implicit broadcasting, and explicit summations. In all examples,  and .}
\end{table}


\section{Proposed Routing Algorithm}\label{sec:proposed_algorithm}

The proposed algorithm executes a modified expectation-maximization loop with three steps: an E-Step for computing expected routing probabilities, a D-Step for computing shares of data used and ignored, and an M-Step for computing output vectors that maximize ``bang per bit'' by more accurately predicting the given input vectors. The original variant of the algorithm \cite{DBLP:journals/corr/abs-1911-00792} routes matrices instead of vectors, in a loop with the same three steps, and computes output matrices as Gaussian mixtures that maximize the probability of generating the proposed ones, weighted by probabilities that maximize ``bang per bit.'' {\em For ease of exposition, we describe the new variant of the algorithm assuming the reader has no familiarity with the original one.}


\subsection{Overview}

\begin{algorithm*}[t]
	\small
	\DontPrintSemicolon
	\caption{, , , and  are implementation-specific.  is the logistic function.  and  are parameters if  is fixed, implementation-specific transformations of  otherwise.}
	\label{alg1:General_Formulation}
	\KwIn{.}
	\KwOut{}
	\BlankLine
	 \tcp*{obtain input vector activation scores} \label{alg1:a_inp_i}
	 \tcp*{obtain votes (proposed output vectors)} \label{alg1:V_ijh}
	\For{ iterations}{
		\Begin(E-Step){
			\eIf{on first iteration}{
				 \tcp*{assign flat prior in first iteration} \label{alg1:initial_R_ij}
			}{
				 \tcp*{predict input vectors} \label{alg1:x_inp_hat_jd}
				
				\tcp*{score the predictions} \label{alg1:S_ij}
				 \tcp*{normalize to distributions} \label{alg1:R_ij}
			}
		}
		\Begin(D-Step){
			 \tcp*{compute shares of data used} \label{alg1:D_use_ij}
			 \tcp*{compute shares of data ignored} \label{alg1:D_ign_ij}
		}
		\Begin(M-Step){
			 \tcp*{maximize ``bang per bit''} \label{alg1:x_out}
		}
	}
\end{algorithm*}

We show the proposed algorithm as Algorithm \ref{alg1:General_Formulation}. Per sample, we accept a sequence of input vectors  and return a sequence of output vectors . The tensor indices, which we use consistently throughout the rest of this document, are:



where  and  are the number of input and output vectors, respectively, and  and  are the size, or number of features, of input and output vectors, respectively.

In the following subsections, we walk through all steps of Algorithm \ref{alg1:General_Formulation} in order of execution.

\subsection{Input Vector Activations}\label{ssec:a_inp_i}

We apply a neural network  to the input vectors to obtain their activation scores  (Algorithm \ref{alg1:General_Formulation}, line \ref{alg1:a_inp_i}), and subsequently apply a logistic function  to each activation score to obtain a probability per input vector  (Algorithm \ref{alg1:General_Formulation}, lines \ref{alg1:D_use_ij}-\ref{alg1:D_ign_ij}).\footnote{
	We represent the logistic function with  instead of , as is conventional, because the latter denotes standard deviation elsewhere in this document and in the original algorithm.
} We call each such probability an ``input vector activation'' and use it to gate the input vector's proposed output vectors.

A notable special case of , which we will revisit, is defining it as a constant function, , making all input vector activations , in which case we always activate all ({\em i.e.}, never gate any) proposed output vectors.

\subsection{Proposed Output Vectors, or Votes}\label{ssec:V_ijh}

We apply a neural network  to the input vectors to obtain a tensor of proposed output vectors  (Algorithm \ref{alg1:General_Formulation}, line \ref{alg1:V_ijh}). The tensor  has, for each input vector , a proposed vector for each possible output vector  with features . We call each proposed output vector a ``vote'' to distinguish it from actual output vectors, which we compute at the end of each iteration in the routing loop.  {\em should break symmetry}, {\em i.e.}, obtain from each input vector a different vote for each output vector; otherwise, all votes from each input vector  would be identical and routing would be pointless.

A notable special case of , which we will revisit, is defining it as a constant function that returns a parameter, , making all votes ``learnable memories'' that are independent of the given input vectors, retrieved instead of computed from them at inference.

Implementing  presents two difficulties to routing long sequences. First, storing the votes  requires  space, which becomes impractical as we increase the length of input and output sequences. Second, naive approaches to breaking symmetry require parameter counts that also become impractical as we increase the length of input and output sequences. For example, applying  different linear transformations (as in the original variant of the algorithm) would require  parameters, and  linear transformations would require  parameters. In Section \ref{sec:implementation}, we present a sample implementation with optimizations that overcome both difficulties, by lazily computing, weighting, and contracting 's elements without storing all of them simultaneously as intermediate values, in an efficient manner. For now, we set aside concerns about routing longer sequences and focus on the next step of the algorithm: the routing loop.


\subsection{Routing Loop}\label{ssec:routing_iterations}

\subsubsection{E-Step}\label{sssec:E_Step}

The E-Step computes a tensor  of expected routing probabilities, for assigning data from each input vector 's votes to compute each output vector . In the first iteration, we assign equal routing probability, , {\em i.e.}, a flat prior, over the votes from each input vector (Algorithm \ref{alg1:General_Formulation}, line \ref{alg1:initial_R_ij}).

In subsequent iterations, we assign greater routing probability to the output vector states which best predict the input vectors, as follows: First, we predict input vectors by applying a neural network  to the the previous iteration's output vector states (line \ref{alg1:x_inp_hat_jd}). We obtain  predicted input vectors. Second, we compute prediction scores  by applying a neural network  to actual and predicted input vectors (line \ref{alg1:S_ij}).  may compute a symmetric kernel (dot-product, Euclidean distance, radial basis function, etc.) or a non-symmetric kernel ({\em i.e.}, one that computes scores differently for different pairs of actual and predicted input vectors, breaking symmetry over the input vectors too).\footnote{
	Optionally,  may specify a generative model that samples the predicted input vectors given current output vector states, in which case  should compute or approximate the conditional log-probability densities of actual input vectors, given the predicted input vectors, as the scores .
} Finally, we apply a Softmax function to , normalizing over index , to obtain updated routing probabilities  which add up to 1 per input vector; {\em i.e.}, for each input vector  we obtain a distribution over the input vector's proposed output vector states  (line \ref{alg1:R_ij}).


\subsubsection{D-Step}\label{ssec:D_Step}

The D-Step computes the shares of data used  and ignored  from each input vector 's vote for computing the state of each output vector . We use these shares to put output vectors in competition with each other as they try to use ``more valuable bits'' and ignore ``less valuable bits'' of data from each input vector's votes, such that each output vector can use more data from an input vector's votes only if all other output vectors collectively ignore it, and vice versa.

We obtain  by multiplying input vector activations  by routing probabilities  (line \ref{alg1:D_use_ij}). Each element of  is in  and the elements of  along index  add up to 1, so the elements of  have values that range from 0 (``ignore all data from input vector 's vote for output vector '') to 1 (``use all data from input vector 's vote for output vector ''), but never exceed each input vector activation (``how much data from input vector 's votes can all output vectors collectively use?''). We then compute  by subtracting the shares used from the input vector activations (line \ref{alg1:D_ign_ij}), such that for every input vector  and every output vector ,



where



treating activated (non-gated) data as a scarce resource that cannot be wasted: Every bit must be ``fully used'' by one or more output vectors and ``fully ignored'' by all other output vectors.

\subsubsection{M-Step}

The M-Step computes updated output vector states  at the end of each iteration as the difference between each output vector's net benefit to use and net cost to ignore data from input vector votes, maximizing ``bang per bit'' (line \ref{alg1:x_out}). The word ``net'' denotes that values may be positive or negative---{\em i.e.}, it is possible for the net benefit to be negative and for the net cost to be positive.

We compute each output vector's net benefit to use data as a linear combination of the votes, where the coefficients are the shares of data used, scaled by a parameter  quantifying each output vector's net benefit per unit of data to use each vote---hence the term ``bang per bit.'' We compute the net cost to ignore data also as a linear combination of the votes, where the coefficients are the shares of data ignored, scaled by a parameter  quantifying each output vector's net cost per unit of data to ignore each vote.

For example, the first output vector's state is



where the tensor slice  has the votes from input vectors  for output vector 1 with elements ,  and  are the shares of data from each input vector  used and ignored by output vector 1, and  and  are the net benefit and net cost per unit of data from input vector  for output vector 1. We maximize the first output vector's net benefit from those votes it uses, less its net cost from those votes it ignores, in competition with all other output vectors, for which we do the same.

If no output vector can improve its net benefit less net cost, given the state of all other output vectors, the routing loop has reached a local optimum in a ``bang per bit'' landscape, specific to the implementation of neural networks , , , and , given the current sequence of input vectors.

\subsection{Training}\label{ssec:training}

We optimize all parameters for a training objective specified elsewhere as a dependency of the output vector states, which in turn are a function of (a)  and , (b)  and , and (c)  in each iteration. Provided the implementation of , , , and  exhibits Lyapunov stability in the routing loop, we can directly optimize (a), which are learnable parameters, and (c), the votes, which are proposed by a differentiable function (), but not (b), which we can optimize only indirectly, via the interaction of input vector activations  and actual-versus-predicted input vector scores , which together determine  and , subject to  \eqref{eq:sum_of_shares_equals_f_a_inp} and \eqref{eq:each_share_le_f_a_inp}, {\em inducing the algorithm to learn to activate and predict input vectors} as we optimize for the training objective.

If the training objective induces each output vector's elements to represent the properties of an object, concept, relationship, or other entity for which we, human beings, already have a label, each output vector is {\em a symbol for a known entity}. Otherwise, the algorithm learns to compute output vector states representing objects, concepts, relationships, or other entities for which we, human beings, may or may not have labels ({\em e.g.}, we may be unaware of their existence), making each output vector {\em a symbol for a discoverable entity}.

\section{Understanding Output Vectors}\label{sec:understanding_output_vectors}

\subsection{As Geometric Objects}

If we factorize out  from the expression in line \ref{alg1:x_out} of Algorithm \ref{alg1:General_Formulation}, we see that each iteration computes the updated state of each output vector as the linear combination of a vector basis in  with corresponding ``bang per bit'' coefficients :



Neural network  transforms input vectors into {\em a different basis for each output vector}. The tensor  consists of  votes specifying a basis for each of  output vectors of size .\footnote{For intuition's sake, we can think of each vote as a basis vector, even though the vote is properly a basis vector only if it is linearly independent of all other votes in the same basis.} In the special case where  is a constant function that returns a parameter , every basis is a learned memory, retrieved given the input vectors instead of computed from them at inference. Each basis may represent a different feature space.

For example, the computation of the first output vector's state  in \eqref{eq:x_out_1h} is factorized as



where the tensor slice  is the basis specified by votes  for output vector 1 with elements , and tensor slice  has the coefficients  for the votes that specify output vector 1's basis. Figure \ref{fig:as_geometric_objects} illustrates an example with three bases () and three slices with coefficients () obtained from two input vectors for computing the state of three output vectors.

When we maximize ``bang per bit,'' we find the coordinates in each basis that best predict the input vectors, subject to constraints \eqref{eq:sum_of_shares_equals_f_a_inp} and \eqref{eq:each_share_le_f_a_inp}, in service of a training objective, specified elsewhere as a dependency of the final output vector states.


\begin{figure}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_as_geometric_objects}}
		\caption{Each output vector 's state is the linear combination of its corresponding basis  in  with its dynamically updated coefficients  in . In this illustration,  and .}
		\label{fig:as_geometric_objects}
	\end{center}
	\vskip -0.2in
\end{figure} 


\subsection{As Latent Variables that Assign Credit}

We can describe each output vector as a latent or explanatory variable with  elements. From this viewpoint, each basis in  is a space of ``proposed hypotheses'' for one output vector. Different coordinates in each basis represent different hypotheses for explaining, or predicting, the given sequence of input vectors (Figure \ref{fig:latent_variables}). In the special case where  is a constant function that returns a parameter , every space of proposed hypotheses is a learned memory, retrieved instead of computed from the input vectors at inference.

\begin{figure}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_latent_variables}}
		\caption{We find the credit assignments in each space of proposed hypotheses that compute the output vector states which best explain the input vectors. In this diagram,  and .}
		\label{fig:latent_variables}
	\end{center}
	\vskip -0.2in
\end{figure} 


The ``bang per bit'' coefficients  \eqref{eq:phi_ij}, or coordinates in each space of proposed hypotheses, specify how much each input vector 's proposed hypothesis adds to, or subtracts from, each output vector 's updated state. That is, the coefficients {\em assign credit via addition and subtraction} of each input vector 's proposed hypothesis to compute each output vector 's updated state. Compared to SHAP methods \cite{NIPS2017_7062}, which estimate additive credit assignments by sampling model outputs on a sufficiently large number of perturbations applied to a given input sample, our algorithm gives us additive credit assignments ``for free'' via an iterative forward pass, without having to figure out how best to perturb input data.

From this viewpoint, maximizing ``bang per bit'' means finding the credit assignments  in all spaces of proposed hypotheses , for computing the output vector states  which best explain the sequence of input vectors , in service of a training objective specified elsewhere as a dependency of the final output vector states. If no output vector can improve its predictions, given the state of all other output vectors, the algorithm has reached a local credit-assignment optimum in a landscape of proposed hypotheses specific to the implementation of , , , and .

If we implement  to obtain each input vector's votes independently of other input vectors' votes, then data from different input vectors is mixed only by  (Figure \ref{fig:composable_credit_assignments}), making the credit assignments composable on their own, independently of data transformations: In a network of routings, data from different vectors is mixed only by the final credit assignments computed by each routing. In appendix \ref{app:composability_of_credit_assignments}, we show methods for computing end-to-end credit assignments over common compositions of routings, including residual layers.

\begin{figure}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_composable_credit_assignments}}
		\caption{If  keeps data from each input vector separate, then data from different input vectors is mixed only by . Here, we show three independently obtained votes for the first output vector.}
		\label{fig:composable_credit_assignments}
	\end{center}
	\vskip -0.2in
\end{figure} 




\subsection{As Associative Memory Query States}\label{ssec:as_associative_memory_states}

We can describe the proposed algorithm as applying an update rule  to output vectors in each iteration, given a sequence of input vectors:



where  composes all transformations we apply to output vectors in the E-Step, D-Step, and M-Step after the first iteration (lines \ref{alg1:x_inp_hat_jd}-\ref{alg1:x_out}). Grouping all such transformations into three newly defined neural networks, which we call , , and ,\footnote{See appendix \ref{app:derivation_of_update_rule} for the derivation of  in terms of these three newly defined neural networks: , , and .} we see that  is the update rule for a model of associative memory with the structure of a bipartite graph in which output vectors are query states and input vectors are keys to content-addressable ``memory values'' and ``memory biases:''



where  applies  and  to obtain updated routing probabilities  (E-Step, lines \ref{alg1:x_inp_hat_jd}-\ref{alg1:R_ij}),



and  and  compose and weight the application of  and  in the D-Step and M-Step to obtain memory values and biases for each key,



{\em i.e.},  and  compute different scalings of the input vector votes, , gated by corresponding input vector activations, .

In the special case where  is a constant function that returns a parameter , the votes are learned memories, retrieved instead of computed from the keys, and  and  compute different gated scalings of such retrieved memories. If  is a constant function that returns , all memories are always fully activated ({\em i.e.}, never gated).

The initial query states assign equal prior routing probability,  (E-Step, line \ref{alg1:initial_R_ij}), to their corresponding memory values given each key:




When we maximize ``bang per bit,'' we iteratively update query states as the mixtures of memory values, less memory biases, which best predict the given keys, in service of a training objective that we specify elsewhere as a dependency of the final query states. If no query can improve its predictions, given the state of all other queries, we have reached a local maximum in a ``bang per bit'' landscape (or equivalently, a local minimum in an energy landscape) specific to the implementation of neural networks , , , and .

Provided the implementation exhibits Lyapunov stability, we can view the algorithm as an ``infinitely deep'' recurrent neural network that repeatedly applies the same layer  to the queries until they converge to a stable state :



where  denotes  applications of  to the queries, given the keys. Alternatively, we can think of the algorithm as a ``single layer'' implicitly defined by its output, the stable state  that solves \eqref{eq:U_fixed_point}, making the algorithm a ``deep equilibrium model'' \cite{DBLP:journals/corr/abs-1909-01377}.\footnote{
	We consider only query states that evolve over a discrete number of iterations. Were we to extend our algorithm to the continuous setting, query states would evolve instead over time  by a system of ordinary differential equations:
	
	with initial condition at  given by an uniform prior distribution over each query's corresponding memory values given each key \eqref{eq:U_initial_state}. Alas, absent an analytical solution (or plausible implementation as a continuous physical process), we would have to approximate integration with numerical methods, requiring a discrete number of iterations anyway.
}

We believe our algorithm is the first model of associative memory to take into account a net cost to ignore data. If we simplify the algorithm, it reduces to a modern Hopfield network with bipartite structure \cite{krotov2021large} \cite{ramsauer2021hopfield}, of which Transformer self-attention \cite{DBLP:journals/corr/VaswaniSPUJGKP17} is a notable special case. The necessary simplifications are: (a) We would have to disregard the net cost to ignore data, {\em e.g.}, by restricting  to constant 0, eliminating the memory biases obtained by  from expressions \eqref{eq:U_definition} and \eqref{eq:U_initial_state}. (b) We would have to restrict  to constant 1, so as to avoid scaling votes differently for each pair of input and output vectors. (c) We would have to restrict  to a constant function that returns , always fully activating all ({\em i.e.}, never gating any) memory values obtained by . (d) We would have to restrict  (and thus ) to propose only one sequence of proposed output vectors, {\em i.e.}, not to break symmetry over them, making routing unnecessary, and apply instead attention over that single sequence of proposed output vectors. (e) We would have to restrict  (which composes  and ) and  (which composes  and ) to those transformations guaranteed to converge to local optima proposed by \citet{krotov2021large} and \citet{ramsauer2021hopfield}.


\subsection{As Agents in a Society of Mind}

Output vectors are multidimensional agents competing with each other to use or ignore data representing input vectors. Each input vector is a scarce resource that cannot be wasted, as we account for all data, ensuring each agent can use or ignore it only at the expense of other agents, as described in \ref{ssec:routing_iterations}. Agents improve their use and ignore shares by more accurately predicting the scarce resources.

Neural network  transforms each scarce resource, or input vector, into a different representation for each agent. In the special case where  is a constant function that returns a parameter  with learned memories, agents compete against each other to use or ignore, not representations computed from the actual scarce resources, but representations learned independently of such resources---``imagined resources,'' as it were.

From this viewpoint, ``bang per bit'' is a form of {\em utility}, and parameters  and  are {\em net prices} each agent pays or collects per unit of data used or ignored to maximize utility, in service of a training objective specified elsewhere as a dependency of the final agent states. If no agent can improve its utility, given the state of all other agents, the competition has reached a local optimum specific to the implementation of , , , and .

The ``bang per bit'' coefficients  \eqref{eq:phi_ij} function as ``knowledge lines,'' or {\em K-lines}, connecting agents to representations of resources as necessary to perform tasks learned in training. If we call an instance of the algorithm a ``block,'' multiple blocks can interact with each other via their respective agents' final states, dynamically connected via K-lines to perform tasks learned in training, modeling a Society of Mind \cite{10.5555/22939}, as shown in Figure \ref{fig:connected_blocks}, but with one significant difference: In our algorithm, the agents in each block incur a net cost for ignoring their representations of the available resources.

\begin{figure}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_connected_blocks}}
		\caption{A network of blocks in a model of a Society of Mind \cite{10.5555/22939}. In each block, agents use or ignore representations of resources via K-lines to perform tasks learned in training.}
		\label{fig:connected_blocks}
	\end{center}
	\vskip -0.2in
\end{figure} 


\section{Efficient Implementation}\label{sec:implementation}

The number of possible implementations of , , , and  is infinite. Here, we present one implementation (Algorithm \ref{alg2:Efficient_Implementation}), incorporating three significant optimizations that reduce parameter count, memory use, and computation by orders of magnitude, overcoming the difficulties to routing longer sequences discussed in \ref{ssec:V_ijh}.


\begin{algorithm*}[t]
	\small
	\DontPrintSemicolon
	\caption{Our implementation of , , , and . Trivial optimizations are not shown for ease of exposition.  denotes normalization of each vector's elements to zero mean and unit variance for numerical stability. If  is variable, we remove index  from all parameters that have it and compute  and .}
	\label{alg2:Efficient_Implementation}
	\KwIn{.}
	\KwOut{.}
	\BlankLine
	 \tcp*{apply , a scaled linear transformation with bias per input vector }
	\For{ iterations}{
		\Begin(E-Step){
			\eIf{on first iteration}{
				\;
			}{
				 \tcp*{apply , a two-layer neural network per output vector }
				 \tcp*{apply , a nonlinear transformation per dot-product }
				\;
			}
		}
		\Begin(D-Step){
			\;
			\;
		}
		\Begin(M-Step){
			 \tcp*{compute ``bang per bit'' coefficients  for each basis }\label{alg2:factored_out}
			 \tcp*{lazily evaluate  and efficiently contract votes}\label{alg2:lazy_F}
		}
	}
\end{algorithm*}


\subsection{Efficient Implementation of }

Our first significant optimization is to implement  with orders of magnitude fewer parameters than would be necessary were we to apply a different set of linear transformations per output vector (as in the original variant of the algorithm).

We define  as a two-layer neural network:



where



When we apply  to a sequence of input vectors ,  computes a tensor product:



giving us, for each of  input vectors,  different elementwise scalings, or Hadamard products, of its  elements. We scale the tensor product by  to keep the subsequent contraction of votes over index  in the same region for different values of .  applies parameter  as a linear transformation from  to , and then adds , a different bias per output vector basis, making it possible for all bases to span up to  dimensions when  but .\footnote{
	If  and we don't add biases to the bases, they would all span the same subspace of dimension .
} The tensor product and per-basis biases break symmetry.

's parameter count in this implementation is , versus  were we to apply  different linear transformations, or  were we to apply  different linear transformations to each input vector. The trade-off of this reduction in parameter count is that, for any fixed , , , and , the space of transformations learnable by  is smaller.

\subsection{Lazy Evaluation of }

Our second significant optimization is to evaluate  lazily in each iteration, in order to compute and contract votes as needed without having to store all of them simultaneously in memory as intermediate values: The tensor  disappears from all expressions. Only the output vectors need be stored at the end of each iteration (Algorithm \ref{alg2:Efficient_Implementation}, line \ref{alg2:lazy_F}). The lazy evaluation of  and immediate contraction of each vote can be done efficiently, {\em i.e.}, in parallel, because our implementation of  computes each input vector's vote for each output vector independently from every other vote.

By never storing votes in a tensor , we reduce memory footprint by . The trade-off of this reduction in footprint is an increase in computation that is linear in the number of iterations: We now compute all votes in every iteration, instead of only once before the loop.

\subsection{Efficient Evaluation of }

Our third significant optimization is necessary to avoid having to store intermediate-value tensors with  or  elements simultaneously in memory, and also to avoid computing votes twice in each iteration, which is a side effect of the lazy evaluation of , due to our computation of output vectors as a difference of two weighted sums of votes (Algorithm \ref{alg1:General_Formulation}, line \ref{alg1:x_out}), both now lazily evaluated.

We factorize the difference of weighted sums into the tensor contraction , where  are the ``bang per bit'' coefficients (Algorithm \ref{alg2:Efficient_Implementation}, line \ref{alg2:factored_out}), and algebraically manipulate it to obtain the expression in Algorithm \ref{alg2:Efficient_Implementation}, line \ref{alg2:lazy_F}. The expression computes, weights, and contracts votes in a memory-efficient manner in each iteration, and then applies  as a last step, {\em after} contracting all votes, reducing the number of linear transformations executed in parallel by a factor of . See appendix \ref{app:efficient_lazy_contraction_votes} for the derivation.


\begin{figure*}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_parameters_memory_execution}}
		\caption{Parameter count, memory footprint, and execution time of a forward pass as we vary each of , , , , and , while keeping the others constant at a baseline, at 32-bit precision, keeping track of gradients, on a recent hardware accelerator (GPU). Baseline values are 100, 100, 1024, 1024, and 2, respectively. Memory figures are peak allocations.}
		\label{fig:parameters_memory_execution}
	\end{center}
	\vskip -0.2in
\end{figure*}

\section{Experiments}\label{sec:experiments}

\subsection{Efficiency and Scalability}

We measure our implementation's parameter count, memory footprint, and execution time as we increase  from 100 to 1,000,000 input vectors,  from 100 to 100,000 output vectors,  from 1024 to 16384 elements per input vector,  from 1024 to 16384 elements per output vector, and number of iterations  from 2 to 10. We find that parameter count, memory footprint, and execution time are linear in each of , , , and  (Figure \ref{fig:parameters_memory_execution}), enabling fine-grained control over memory consumption and computational cost. Given a memory and compute budget, we can increase the maximum length of input sequences our implementation can route by reducing output sequence length, and vice versa. Memory footprint and execution time are also linear in the number of iterations.

We also compare our implementation's parameter count, memory footprint, and execution time to those of a Transformer encoder layer using self-attention as we increase sequence length up to 2000 vectors, keeping vector size constant at 1024. To make the comparison possible, we restrict our implementation to input and output sequences that have the same shape, routing over two iterations, the fewest possible. We find our implementation requires fewer parameters for sequences with up to 600 vectors, allocates less memory for sequences with up to 800 vectors, and incurs less computation for sequences with up to 1700 vectors (Figure \ref{fig:comparison_to_self_attention}), which is surprising to us, because our algorithm proposes  output vectors per input vector, whereas the query-key-value mechanism of self-attention proposes only one output vector (a ``value'') per input vector.


\begin{figure*}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_comparison_to_self_attention}}
		\caption{Comparison to a Transformer encoder layer using self-attention. To make comparison possible, we restrict our implementation to , , and . Data is for a forward pass on a recent hardware accelerator (GPU) at 32-bit precision, keeping track of gradients, using dense Softmax functions. Self-attention uses eight heads, the default. Memory figures are peak allocations.}
		\label{fig:comparison_to_self_attention}
	\end{center}
	\vskip -0.2in
\end{figure*}


\subsection{Performance on Benchmarks}

We test our implementation on six classification benchmarks in natural language and vision, obtaining accuracy that is competitive with, and in one case better than, the state of the art (Table \ref{tab:accuracies}). For each benchmark, we add a classification head to a pretrained Transformer. The head accepts as input all token embeddings computed by every Transformer layer, flattens them into a single sequence, and sequentially applies three routings:

\begin{center}
	\vskip 1em
	\begin{tabular}{l|cccc}
		               &  &          &  &  \\
		\midrule
		   & --      &          &  &  \\
		   &  &          &  &  \\
		   &  &  &  & 1       \\
	\end{tabular} \\
	\vskip 1em
\end{center}

where 's number of input vectors is unspecified because the flattened sequence's length is variable,  is a number of hidden explanatory vectors of our choosing,  is the pretrained Transformer's embedding size,  is the size of the hidden explanatory vectors, and  is the number of classes specific to each task.

For natural language tasks, we use RoBERTa-large \cite{DBLP:journals/corr/abs-1907-11692} as the pretrained Transformer. For visual tasks, we use BEiT-large with 1616 patches from 224224 images \cite{hangbo2021beit}. We freeze the Transformer. For all tasks, we specify  and . All routings execute  iterations, the fewest possible. Before flattenning the hidden embeddings we apply layer normalization at each level of depth. If the input sequence's length is greater than the Transformer's maximum sequence length, we split the input sequence into chunks, apply the Transformer to each chunk, and join the hidden states computed for all chunks at every level of Transformer depth. The longest flattened sequence we see among all benchmarks has 89,600 input vectors, computed by RoBERTa-large's 25 hidden layers for a natural language sample drawn from the IMDB movie review dataset, split in 7 chunks, each with 512 subword tokens.

\begin{table}[h]
	\begin{center}
		\small
		\vskip 0.1in
		\begin{tabular}{llc}
			\toprule
			Classification Benchmark        &      & Accuracy (\%) \\
			\midrule
			\em Natural Language            &      &        \\
			IMDB                            &      &   96.2 \\
			SST-5*                          &      &   59.8 \\
			SST-2                           &      &   96.0 \\
			\midrule
			\em Vision                      &      &        \\
			ImageNet-1K @ 224224    & Top1 &   86.7 \\
			                                & Top5 &   98.1 \\
			CIFAR-100                       &      &   93.8 \\
			CIFAR-10                        &      &   99.2 \\
			\bottomrule
			\multicolumn{2}{l}{\small * New state-of-the-art accuracy.}
		\end{tabular}
        \caption{\label{tab:accuracies}Classification accuracy.}
		\vskip 0.25in
	\end{center}
\end{table}

\subsection{End-to-End Credit Assignments}

Vectors remain independent of each other between each routing executed in the classification head, so we can compute end-to-end credit assignments for all benchmark tasks. Each head executes three routings, giving us three credit assignment matrices. We multiply them as described in Appendix \ref{app:composability_of_credit_assignments}, obtaining a matrix of end-to-end credit assigned to every hidden Transformer embedding  for each predicted classification score :



where  and , and  computes the standard deviation over all elements, scaling the credit assignments to unit variance. The largest end-to-end credit assignment matrix we see among all benchmarks has 49251000 elements, consisting of the end-to-end credit assigned to embeddings of a special token and 196 image patches computed by each of BEiT-large's 25 levels of depth, in a flattened sequence with 4925 input vectors, for 1000 predicted scores, each an output vector with one element, for ImageNet-1K classification.

We sum 's elements over all levels of Transformer depth to obtain the credit assigned to subword tokens and pixel patches, and over groups of tokens and patches to obtain the credit assigned to sentences and image regions. We find the end-to-end credit assignments are interpretable. Figures \ref{fig:sample_credit_assignments_vision} and \ref{fig:sample_credit_assignments_natural_language} show typical examples.

\begin{figure*}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_sample_credit_assignments_vision}}
		\caption{Typical example of end-to-end credit assigned to Transformer hidden states in a visual task. Here, our three-layer routing head assigns credit to the dog's entire body in shallower layers, and to its nose, mouth, ears, and paws in deeper layers. The matrix of end-to-end credit assignments  has 49251000 elements, consisting of credit assigned to 197 hidden embeddings at 25 levels of Transformer depth, or 4925 input vectors, for 1000 classification scores, each an output vector with one element. We show the absolute values of 4900 credit assignments to embeddings corresponding to 196 image patches, for the highest score, excluding 25 credit assignments to a special token added to the input sequence.}
		\label{fig:sample_credit_assignments_vision}
	\end{center}
	\vskip -0.2in
\end{figure*}

\begin{figure*}[t]
	\vskip 0.1in
	\begin{center}
		\centerline{\includegraphics{fig_sample_credit_assignments_natural_language}}
		\caption{Typical example of end-to-end credit assigned to Transformer hidden states in a natural language task. Here,  has 18502 elements, consisting of credit assigned to 74 hidden embeddings at 25 levels of Transformer depth, or 1850 input vectors, for 2 classification scores, each an output vector with one element. We show 1800 credit assignments to embeddings corresponding to 72 subword tokens for the highest score, excluding 50 credit assignments to two special tokens added to the input sequence.}
		\label{fig:sample_credit_assignments_natural_language}
	\end{center}
	\vskip -0.2in
\end{figure*}


\cleardoublepage


\bibliography{An_Algorithm_for_Routing_Vectors_in_Sequences}
\bibliographystyle{An_Algorithm_for_Routing_Vectors_in_Sequences}

\cleardoublepage


\appendix


\section{Composability of Credit Assignments}\label{app:composability_of_credit_assignments}

If each input vector's votes are independent of other input vectors' votes, then we can compose the ``bang per bit'' credit-assignment coefficients  on their own, independently of data transformations. Here, we show methods for computing end-to-end credit assignments over four common compositions of routings.\footnote{Subject to the same condition of independence, our methods apply also to modern Hopfield networks with bipartite structure, including Transformer self-attention, as they are simplifications of our routing algorithm. See \ref{ssec:as_associative_memory_states}.}


\subsection{In Sequential Routings}

If we compose the sequential application of two routings,  and , into a neural network,



we can obtain the neural network's end-to-end credit assignments , over both routings, by multiplying their credit-assignment matrices,



where  and  are the credit-assignment matrices computed by  and , respectively, and  is the common index over 's output vectors and 's input vectors.

For a longer sequence of routings, we can obtain end-to-end credit assignments by multiplying the corresponding chain of credit-assignment matrices, as matrix multiplication is associative.


\subsection{In Residual Routings}

If we apply one routing as a residual to another,



we can obtain end-to-end credit assignments  by adding the product of the two credit-assignment matrices to the first one,



where  and  are the credit assignment matrices computed by  and , respectively, and  (necessary for disambiguation).

For a sequence of residual routings, we can obtain end-to-end credit assignments by multiplying each additional residual credit-assignment matrix with, and then adding the result back to, the previous state of the end-to-end credit-assignment matrix, as matrix addition is associative.


\subsection{In Sums of Routings}

If we sum two independent routings  and ,



we can obtain end-to-end credit assignments by concatenating the two credit-assignment matrices over their mutually exclusive indices,

0.5em]
	\phi\suptag{\routing_2}_{i_2 j} \\
\end{bmatrix},

\begin{aligned}
x\suptag{hid1}_{j_1 h} \longleftarrow & \; \routing_1(x\suptag{inp1}_{i_1 d_1}) \\
x\suptag{hid2}_{j_2 h} \longleftarrow & \; \routing_2(x\suptag{inp2}_{i_2 d_2}) \\
x\out_{jh} \longleftarrow & \; x\suptag{hid1}_{j_1h} \oplus x\suptag{hid2}_{j_2 h},\\
\end{aligned}

\phi\suptag{e2e}_{ij} \longleftarrow \phi\suptag{\routing_1}_{i_1 j_1} \oplus \phi\suptag{\routing_2}_{i_2 j_2}
= \begin{bmatrix}
	\phi\suptag{\routing_1}_{i_1 j_1} & 0                                 \

where  and  are the credit assignment matrices computed by  and , respectively,  again denotes a direct sum over mutually exclusive indices, , and .

For three or more concatenations, we can obtain end-to-end credit assignments with direct sums over mutually exclusive indices, provided we fix the order of concatenation, as direct sums are associative but not commutative.


\cleardoublepage

\section{Derivation of Update Rule}\label{app:derivation_of_update_rule}

\vspace{-0.3in}
\begin{multicols}{1}
	
\end{multicols}
\vskip 0.05in

\section{Efficient Lazy Contraction of Votes}\label{app:efficient_lazy_contraction_votes}

\vspace{-0.3in}
\begin{multicols}{1}
	
\end{multicols}
\vskip 0.05in


\end{document}