

\section{Experimental Methodology}\label{sec:experimentmethodology}

\subsection{Simulation Platform and Workload}\label{sec:Simulation}
We use Sniper, a state-of-the-art x86-64 microarchitecture simulator, which is based on Pin \cite{CarHei2011_Sniper}.  We use interval core model with 128 entry ROB (reorder buffer), 2.2 GHz frequency and a dispatch width of 4 micro-operations.  All caches use a block size of 64B. Both L1D and L1I are 32KB, 4-way, LRU caches and have a latency of 2 cycles. The L2 cache is a 2MB, 8-way, LRU cache with 12 cycle latency.  The latency of main memory is 154 cycles and memory queue contention is also modeled. Interval length is 10M instructions.

We use all SPEC2006 benchmarks with \textit{ref} inputs. For maintaining clarity in the figures, we use three-letter acronyms of the benchmarks, as shown in Table \ref{tab:shortform}. The benchmarks are fast-forwarded for 10B instructions. Then, each benchmark is simulated for 400M instructions.  

\begin{table}[htbp]
  \centering
  \caption{Name of SPEC2006 Benchmarks and Their Acronyms}
    \begin{tabular}{|l|l||l|l|}
   \hline
    Name  & Acronym & Name  & Acronym \\
   \hline
    astar & ast   & libquantum & lib \\\hline
    bwaves & bwa   & mcf   & mcf \\\hline
    bzip2 & bzi   & milc  & mil \\\hline
    cactusADM & cac   & namd  & nam \\\hline
    calculix & cal   & omnetpp & omn \\\hline
    dealII & dea   & perlbench & per \\\hline
    gamess & gam   & povray & pov \\\hline
    gcc   & gcc   & sjeng & sje \\\hline
    gemsFDTD & gem   & soplex & sop \\\hline
    gobmk & gob   & sphinx & sph \\\hline
    gromacs & gro   & tonto & ton \\\hline
    h264ref & h26   & wrf   & wrf \\\hline
    hmmer & hmm   & xalancbmk & xal \\\hline
    lbm   & lbm   & zeusmp & zeu \\\hline
    leslie3D & les   &       &  \\\hline
  
    \end{tabular}\label{tab:shortform}\end{table}

 
 



\subsection{Comparison With Other Techniques}

We take eDRAM L2 cache with periodic refresh-all (i.e. both dirty and clean blocks are refreshed) policy as the baseline. For comparison, we also implement Refrint polyphase-valid (RPV) policy \cite{agrawalrefrint}, which works on the intuition that on a read or a write, the line is automatically refreshed and hence, it need not be refreshed for the duration of one retention period. RPV divides the retention period into number of phases. Each cache line maintains the information about the phase in which it was last updated. Afterwards, to reduce the number of refresh operations, RPV refreshes the line at the beginning of this phase, instead of refreshing at beginning of refresh period itself.  We use RPV with four phases, since this is shown to provide significant energy savings \cite{agrawalrefrint}.

Agrawal et al. \cite{agrawalrefrint} also propose Refrint polyphase-dirty (RPD) policy which eagerly invalidates valid blocks to avoid refreshing them and refreshes only dirty blocks. For applications which have only small fraction of dirty data, RPD policy would very aggressively invalidate almost the whole cache; and for small retention periods (e.g. 40$\mu$s translates to only 40,000 cycles  for a 1GHz processor), RPD will greatly increase the access to main memory. Since future technology generations are expected to have even smaller retention periods, RPD will incur large performance loss and hence, we do not evaluate this. Further, RPV policy has been shown to perform better than another policy proposed by Agrawal et al., namely the periodic-valid refresh policy \cite{agrawalrefrint} and hence, we do not evaluate periodic-valid refresh policy.
  
    

\begin{figure*}[htp]
\centering
\includegraphics [scale=0.45] {Figures/R40us/C1MB2_Energy.eps}
\includegraphics [scale=0.45] {Figures/R40us/C1MB2_WIPC.eps}
\includegraphics [scale=0.45] {Figures/R40us/C1MB2_Refresh.eps}
\includegraphics [scale=0.45] {Figures/R40us/C1MB2_AR.eps}
\includegraphics [scale=0.45] {Figures/R40us/C1MB2_MPKIInc.eps}
\caption{Results for different schemes at 40 $\mu$s refresh period. Note that \% improvement in performance refers to \% reduction in simulation time, and hence, a higher value is better. Similarly, for decrease in RPKI and \% energy saved, a higher value is better.}
\label{fig:results40}
\end{figure*}
 
 
 \subsection{Energy Model}\label{sec:energymodel}
We account for the energy consumption of L2 cache ($E_{L2}$), main memory ($E_{DRAM}$) and energy cost of algorithm ($E_{Algo}$), since the techniques evaluated here affect the other components only minimally. We use the following notations. $E^{dyn}_{xyz}$ and $P^{leak}_{xyz}$ show the dynamic energy per access and leakage energy per second, respectively, in a component xyz (e.g. L2 or DRAM). For our technique, $B$ shows the number of blocks which are turned on or off; $E_{\chi}$ shows the energy consumed in a single such block transition and $E_{tran}$ shows the total energy consumed in block transitions. $F_A$,  $H_{L2}$ and $M_{L2}$ show the active fraction of cache, number of L2 hits and L2 misses in an interval,  respectively.  $N_R$ shows the number of blocks which are refreshed within all refresh-events in an interval. $T$ denotes the time length of an interval in seconds. $A_{DRAM}$ shows the number of DRAM accesses.  

The L2 leakage energy is assumed to scale with the active fraction of cache 
\cite{mittal2013PhDThesis}. For computing L2 dynamic energy, an L2 miss is 
assumed to consume twice the dynamic energy as that of an L2 hit 
\cite{mittal2013PhDThesis}.  Thus, we have 

\begin{align}
 \label{eq:totalenergy}E&= E_{L2}+E_{DRAM}+E_{Algo} \\
 E_{L2} &= LE_{L2} + DE_{L2} +RE_{L2}  \\   
 LE_{L2} &= P^{leak}_{L2}\times F_{A} \times T \\
DE_{L2} &= E^{dyn}_{L2}\times(2 M_{L2}+H_{L2}) \\
RE_{L2} &= N_R \times E^{dyn}_{L2} \\
E_{DRAM} &= P^{leak}_{DRAM}\times T + E^{dyn}_{DRAM}\times A_{DRAM} \\
E_{Algo} &= E_{\chi}\times B + E_{prof}\\
E_{prof} &= P^{leak}_{prof} \times T + E^{dyn}_{prof} \times A_{prof}
\end{align}

We ignore the energy overhead of RPV algorithm, thus, for experiments with 
baseline eDRAM cache, SRAM cache and RPV, we have $E_{Algo}$ = 0 and $F_A$ = 1. 
We use CACTI \cite{cacti_53} to obtain the values of $E^{Dyn}_{L2}$ and 
$P^{Leak}_{L2}$  at 45nm for SRAM cache, assuming a bank size of 1MB. For 2MB 
cache, we get, $E^{Dyn}_{L2}$ = 0.648 nJ/access and  $P^{Leak}_{L2}$ = 1.296 
Watt. 



Following \cite{agrawalrefrint}, we assume that L2 cache access times and energy 
values are same in both SRAM and eDRAM caches. Further, the leakage energy 
consumption of eDRAM is 1/8th of the SRAM cache 
\cite{iyer2005embedded,agrawalrefrint}. For eDRAM, the time and energy consumed 
in refreshing a line is equal to the time and energy to access the line, 
respectively \cite{agrawalrefrint}. For eDRAM L2, the bank size of L2 is 1MB. We 
assume that each bank of L2 cache  has dedicated logic to process refresh 
requests and using pipelining, a line can be refreshed in a single cycle 
\cite{agrawalrefrint}. 

$ E^{dyn}_{DRAM}$  and $P^{leak}_{DRAM}$ are taken as 70 nJ and 0.18 Watt, 
respectively \cite{mittal2013PhDThesis} and $E_{\chi}$ is taken 
as 2 pJ \cite{mittal2013PhDThesis}. For computing energy values of profiling 
cache, we use CACTI and take the energy consumed in data array only 
\cite{mittal2013PhDThesis}. For a profiling cache, corresponding to 2MB L2, we 
get $E^{dyn}_{prof}$ = 0.0031 nJ/access and  $ P^{leak}_{prof}$ = 0.0050 Watt. 
Clearly, the energy consumption of profiling cache is negligible compared to 
that of L2 cache. 


\subsection{Evaluation Metrics}
We take the baseline as an eDRAM cache which periodically refreshes all the 
blocks at the given refresh period. For SRAM L2, RPV and our technique, we show 
the results on following metrics:
\begin{enumerate}
\item Percentage energy saving
\item  Percentage  reduction in execution time
\item Absolute reduction in number of lines refreshed per kilo instructions 
(RPKI). This result is shown only for RPV and our technique.  
\end{enumerate}
For our technique, we also show the results on the following metrics:
\begin{enumerate}
\item ActiveRatio (the fraction of active lines averaged over entire execution 
\cite{mittal2013PhDThesis}) 
\item Absolute increase in MPKI due to use of our technique
\end{enumerate}

 ActiveRatio enables us to evaluate the aggressiveness of cache turn-off of our technique and increase in MPKI helps in evaluating the increase in main memory traffic. Since RPV does not turn-off the cache or cause early invalidation, its ActiveRatio is always 100\% and the increase in MPKI is always zero. Similar is also true for SRAM L2. 
 

 
 

 

  
  
  
  
  
 
 










 
 








 
 
 



















