

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[inline]{enumitem}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{array}
\usepackage{xcolor}
\usepackage{stmaryrd}
\usepackage{scalerel}
\usepackage{multirow}

\newcommand{\ourtag}[1]{\ensuremath{\left\langle #1 \right\rangle}}

\newcommand{\shufmodel}[3]{\ensuremath{\text{#1}^{\text{#3}}}}

\makeatletter
\newcommand*{\bigcdot}{}\DeclareRobustCommand*{\bigcdot}{\mathbin{\mathpalette\bigcdot@{}}}
\newcommand*{\bigcdot@scalefactor}{.65}
\newcommand*{\bigcdot@widthfactor}{1.15}
\newcommand*{\bigcdot@}[2]{\sbox0{}\sbox2{}\hbox to \bigcdot@widthfactor\wd2{\hfil
    \raise\ht0\hbox{\scalebox{\bigcdot@scalefactor}{\lower\ht0\hbox{}}}\hfil
  }}
\makeatother

\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\sep}{}


\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Investigating Pretrained Language Models for Graph-to-Text Generation}

\author{Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Sch{\"u}tze and Iryna Gurevych \vspace{1mm} \\
\rule{0pt}{2.5ex}
  Research Training Group AIPHES and UKP Lab, Technical University of Darmstadt\\
  Center for Information and Language Processing (CIS), LMU Munich \\
  \rule{0pt}{2.5ex}
 \texttt{\href{https://www.ukp.tu-darmstadt.de}{www.ukp.tu-darmstadt.de}}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recently proposed pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. In particular, we report new state-of-the-art BLEU scores of 49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8\%, 4.5\%, and 42.4\%, respectively. In an extensive analysis, we identify possible reasons for the PLMs’ success on graph-to-text tasks. We find evidence that their knowledge about true facts helps them perform well even when the input graph representation is reduced to a simple bag of node and edge labels.\footnote{Our code and pretrained model checkpoints are available at \href{https://github.com/UKPLab/plms-graph2text}{https://github.com/UKPLab/plms-graph2text}.}
\end{abstract}



\section{Introduction}

Graphs are important data structures as they represent complex relations between a set of objects. For example, syntactic and semantic structures of sentences can be represented using different graph representations \cite{bastings-etal-2017-graph, banarescu-etal-2013-abstract} and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities \cite{gardent-etal-2017-webnlg}.

Graph-to-text generation, a subtask of data-to-text generation \cite{10.5555/3241691.3241693}, aims to create fluent natural language text to describe an input graph (see Figure~\ref{fig:graphs}). This task is important for numerous applications such as question answering \cite{duan-etal-2017-question}, dialogue generation \cite{moon-etal-2019-opendialkg}, and summarization \cite{fan-etal-2019-using}.




 \begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig-amr6.pdf}
    \caption{Examples of (a) AMR and (b) WebNLG graphs, the input for the models and the reference texts.}
    \label{fig:graphs}
\end{figure*}


Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures have considerably outperformed prior state of the art \cite{devlin-etal-2019-bert, liu2020roberta, radford2019language}. Following this trend, recent works \cite{mager2020gpttoo, harkous2020text} apply transfer learning to data-to-text generation, where a language model is first pretrained on large corpora before being fine-tuned on the target task. 



In this paper, we analyze the applicability of two recent text-to-text pretrained language models (PLMs), BART \cite{lewis2019bart} and T5 \cite{2019t5}, for graph-to-text generation. We choose these models because of their encoder-decoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We are also the first to investigate task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin.

While recent works have shown the benefit of explicitly encoding the graph structure in graph-to-text generation \cite{song-etal-acl2018, ribeiro-etal-2019-enhancing, ribeiro2020modeling,schmitt2020modeling, zhao-etal-2020-bridging}, our approaches based on PLMs consistently outperform these models, even though PLMs -- as sequence models -- do not exhibit any graph-specific structural bias.\footnote{The model architecture does not explicitly encode the graph structure, i.e., which entities are connected to each other, but has to retrieve it from a sequence that tries to encode this information.} This puts into question the importance of encoding the structure of a graph 
in the presence of a strong language model.
In our analysis
we investigate to what extent fine-tuned PLMs make use of the graph structure and whether they need it at all.
We notably observe that PLMs can achieve high performance on two popular KG-to-text benchmarks even when the KG is reduced to a mere bag of node and edge labels.
We also find evidence that factual knowledge from the pretraining phase poses a strong bias on the texts generated by PLMs -- to the extent that even unseen corrupted graph facts lead to correct output texts.
In summary, our contributions are the following:
\begin{enumerate*}[label=(\arabic*)]
    \item We examine and compare two PLMs, BART and T5, for graph-to-text generation, exploring \emph{language model adaptation} (\textsc{lma}) and \emph{supervised task adaptation} (\textsc{sta}) pretraining strategies, employing additional task-specific data.\item Our approaches consistently outperform the state of the art by a significant margin on three established graph-to-text benchmarks from different domains. 
    \item We demonstrate that PLMs perform well even when trained on a shuffled linearized graph representation without any information about connectivity (bag of node and edge labels), which is surprising since prior studies showed that explicitly encoding the graph structure improves models trained from scratch.
    \item We present evidence that the \emph{knowledge about facts} acquired during pretraining gives PLMs such an advantage on KG-to-text benchmarks that their performance is almost the same with or without access to the graph structure. 



\end{enumerate*}



\section{Related Work}

Graph-to-text generation can be divided into two main tasks: generating text (i) from meaning representations \cite{konsas_17} and (ii) from KGs \cite{gardent-etal-2017-webnlg}.
\vspace{-0.3cm}
 \paragraph{AMR-to-Text Generation.} Abstract meaning representation (AMR) is a semantic formalism that represents the meaning of a sentence as a rooted directed graph expressing ``who is doing what to whom'' \cite{banarescu-etal-2013-abstract}. In an AMR graph, nodes represent concepts and edges represent semantic relations. Various neural models have been proposed to generate sentences from AMR graphs. \citet{konsas_17} propose the first neural approach for AMR-to-text generation that uses a linearized input graph. Recent approaches \cite{song-etal-acl2018, beck-etal-2018-acl2018, damonte-cohen-2019-structural, ribeiro-etal-2019-enhancing, zhao-etal-2020-bridging} propose architectures based on GNNs to directly encode the AMR graph structure. Other methods \cite{zhu-etal-2019-modeling,cai-lam-2020-graph, doi:10.116200297, song-etal-2020-structural, yao-etal-2020-heterogeneous} employ Transformers to learn node representations injecting the graph structure into the self-attention aggregation. 
\vspace{-0.2cm}
\paragraph{KG-to-Text Generation.} Recent neural approaches for KG-to-text generation linearize the KG triples as input to sequence-to-sequence models \cite{trisedya-etal-2018-gtr, moryossef-etal-2019-step, castro-ferreira-etal-2019-neural}. \citet{marcheggiani-icnl18} use GNNs to capture node contexts, and demonstrate superior performance compared to LSTMs. \citet{koncel-kedziorski-etal-2019-text} propose a Transformer-based approach which directly encodes the input graph structure. Most recent approaches \citep{ribeiro2020modeling,schmitt2020modeling} propose to encode both global and local node contexts in order to better capture the graph topology.
\vspace{-0.2cm}
\paragraph{Pretrained Language Models.}  Pretrained Transformer-based models, such as BERT \cite{devlin-etal-2019-bert}, XLNet \cite{NIPS2019_8812}, or RoBERTa \cite{liu2020roberta}, have established a qualitatively new level of baseline performance for many widely used natural language understanding (NLU) benchmarks. Generative pretrained Transformer-based methods, such as GPT-2 \cite{radford2019language}, BART \cite{lewis2019bart}, and T5 \cite{2019t5}, are employed on many natural language generation (NLG) tasks.  \citet{mager2020gpttoo} were the first to employ a pretrained Transformer-based language model -- namely GPT-2 -- for AMR-to-text generation. Very recently, \citet{harkous2020text} and \citet{kale2020texttotext} demonstrate state-of-the-art results in different data-to-text datasets, employing GPT-2 and T5 models respectively. Different from the above works, we do not only investigate standard fine-tuning approaches but also new task-adaptive pretraining approaches for BART and T5, and we also provide the first analysis aimed at explaining the good performance of PLMs at graph-to-text tasks. Concurrent to our work, \citet{radev2020dart} propose DART, a new data-to-text dataset, and apply BART to the WebNLG dataset, augmenting the training data. Our study not only considers more benchmarks and PLMs, but also differs in that it focuses on transfer learning strategies that separate task-adaptive pretraining from fine-tuning on the actual task training data as opposed to the training data augmentation in \citet{radev2020dart}.


Recently, \citet{gururangan-etal-2020-dont} explored task-adaptive pretraining strategies for text classification.
While our \textsc{lma} (see \S\ref{sec:finetuning}) is related to their \textsc{dapt} as both use a self-supervised objective on a domain-specific corpus,
they notably differ in that \textsc{dapt} operates on the model input while \textsc{lma} models the output.
We are the first to consider additional pretraining for NLG with PLMs.




\section{PLMs for Graph-to-Text Generation}
\label{sec:finetuning}
\subsection{Models in this Study}
We investigate BART and T5, two PLMs based on the Transformer encoder-decoder architecture \cite{NIPS2017_7181}, for graph-to-text generation. They mainly differ in how they are pretrained and the input corpora used for pretraining.


BART is pretrained as a text-to-text denoising autoencoder: first, the input text is corrupted with a random noise function; then BART is trained to reconstruct the original text. The training corpus is a combination of books and Wikipedia data. We evaluate BART versions with different capacity: \emph{base} with 140M and \emph{large} with 400M parameters. 

T5 generalizes the text-to-text architecture to a variety of NLP tasks. The model is pretrained with randomly corrupted text spans with different mask ratios and span sizes. The training corpus is C4, a large cleaned corpus of web texts. We experiment with the following variants: \emph{small} with 60M, \emph{base} with 220M, and \emph{large} with 770M parameters.

We fine-tune BART and T5 for a few epochs on the supervised downstream graph-to-text datasets. For T5, in the supervised setup, we add a prefix ``translate from Graph to Text:'' before the graph input. We add this prefix to imitate the T5 setup, when translating between different languages. 







\subsection{Task-adaptive Pretraining} 
\label{sec:domainpretraining}



Inspired by previous work \cite{konsas_17,gururangan-etal-2020-dont}, we investigate whether leveraging additional task-specific data can improve the PLMs' performance on graph-to-text generation. Task-specific data refers to a pretraining corpus that is more task-relevant and usually smaller than the text corpora used for task-independent pretraining. In order to leverage the task-specific data, we add an intermediate pretraining step between the original pretraining and fine-tuning phases for graph-to-text generation. 



More precisely, we first continue pretraining BART and T5 using language model adaptation (\textsc{lma}) or supervised task adaptation (\textsc{sta}) training. In the supervised approach, we use pairs of graphs and corresponding texts collected from the same or similar domain as the target task. In the \textsc{lma} approach, we follow BART and T5 pretraining strategies for language modeling, using the reference texts that describe the graphs. Note that we do not use the graphs in the \textsc{lma} pretraining, but only the target text of our task-specific data collections. The goal is to adapt the decoder to the domain of the final task. In particular, we randomly mask text spans, replacing 15\% of the tokens.\footnote{Please, refer to \citet{lewis2019bart} and \citet{2019t5} for details about the self-supervised pretraining strategies.}
Before evaluation, we finally fine-tune the models using the original training set as usual.


\section{Datasets}
\label{sec:data}

We evaluate the text-to-text PLMs in three graph-to-text benchmarks: AMR17 (LDC2017T10), WebNLG \cite{gardent-etal-2017-webnlg}, and AGENDA \cite{koncel-kedziorski-etal-2019-text}. Table~\ref{tab:datastatistics} in the Appendix shows statistics for each dataset.



\paragraph{AMR17.} An instance in LDC2017T10 consists of a sentence annotated with its corresponding AMR graph. Following ~\citet{mager2020gpttoo}, we linearize the AMR graphs using the PENMAN notation (see Figure~\ref{fig:graphs}a).






\paragraph{WebNLG.} Each instance of WebNLG contains a KG from DBPedia \cite{10.5555/1785162.1785216} and a target text with one or multiple sentences that describe the graph. The test set is divided into two partitions: \textit{seen}, which contains only DBPedia categories present in the training set, and \textit{unseen}, which covers categories never seen during training. Their union is called \textit{all}.
Following previous work \cite{harkous2020text}, we prepend \ourtag{H}, \ourtag{R}, and \ourtag{T} tokens before the head entity, the relation and tail entity of a triple (see Figure~\ref{fig:graphs}b).

\paragraph{AGENDA.} In this dataset, KGs are paired with scientific abstracts extracted from proceedings of AI conferences. Each sample contains the paper title, a KG, and the corresponding abstract. The KG contains entities corresponding to scientific terms and the edges represent relations between these entities. This dataset has loose alignments between the graph and the corresponding text as the graphs were automatically generated. The input for the models is a text containing the title, a sequence of all KG entities, and the triples. The target text is the paper abstract. We add special tokens into the triples in the same way as for WebNLG. 





\subsection{Additional Task-specific Data}
In order to evaluate the proposed task-adaptive pretraining strategies for graph-to-text generation, we collect task-specific data for two graph domains: meaning representations (like AMR17) and scientific data (like AGENDA).
We did not attempt collecting additional data like WebNLG because the texts in this benchmark do not stem from a corpus but were specifically written by annotators.



\paragraph{AMR Silver Data.} In order to generate additional data for AMR, we sample two sentence collections of size 200K and 2M from the Gigaword\footnote{\href{https://catalog.ldc.upenn.edu/LDC2003T05}{https://catalog.ldc.upenn.edu/LDC2003T05}} corpus and use a state-of-the-art AMR parser \cite{cai-lam-2020-amr} to parse them into AMR graphs.\footnote{We filter out sentences that do not yield well-formed AMR graphs.} For supervised pretraining, we condition a model on the AMR silver graphs to generate the corresponding sentences before fine-tuning it on gold AMR graphs. For self-supervised pretraining, we only use the sentences.\footnote{Note that Gigaword and AMR17 share similar data sources.}


\paragraph{Semantic Scholar AI Data.} We collect titles and abstracts of around 190K scientific papers from the Semantic Scholar \cite{ammar-etal-2018-construction} taken from the proceedings of 36 top Computer Science/AI conferences. We construct KGs from the paper abstracts employing DyGIE++ \cite{wadden-etal-2019-entity}, an information extraction system for scientific texts. Note that the AGENDA dataset was constructed using the older SciIE system \cite{luan-etal-2018-multi}, which also extracts KGs from AI scientific papers. A second difference is that in our new dataset, the domain is broader as we collected data from 36 conferences compared to 12 from AGENDA. Furthermore, to prevent data leakage, all AGENDA samples used for performance evaluation are removed from our dataset. We will call the new dataset KGAIA (KGs from AI Abstracts). Table~\ref{tab:augstatistics} in the Appendix shows relevant dataset statistics.



\section{Experiments}



We develop our experiments based on pretrained models released by HuggingFace \citep{wolf2019huggingfaces}. Following \citet{wolf2019huggingfaces}, we use the Adam optimizer \cite{kingma:adam} with an initial learning rate of . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from \{2,4,8\} and \{1,3,5\}, respectively, based on the respective development set. We add all edge labels seen in the training set to the vocabulary of the models for AMR17. For the KG datasets, we add the \ourtag{H}, \ourtag{R}, and \ourtag{T} tokens to the models' vocabulary. Dev BLEU is used for model selection. 

\begin{table*}[t]
\centering
{\renewcommand{\arraystretch}{0.6}
\begin{tabular}{@{\hspace*{1mm}}l@{\hspace*{3mm}}ccccccccc@{\hspace*{1mm}}}  
\toprule
& \multicolumn{3}{c}{\textbf{BLEU}} & \multicolumn{3}{c}{\textbf{METEOR}} & \multicolumn{3}{c}{\textbf{chrF++}} \\
\midrule
\textbf{Model} & \textbf{A} & \textbf{S} & \textbf{U} & \textbf{A} & \textbf{S} & \textbf{U} & \textbf{A} & \textbf{S} & \textbf{U} \\
\midrule
\citet{castro-ferreira-etal-2019-neural} & 51.68 & 56.35 & 38.92 & 32.00 & 41.00 &21.00  & - & - & - \\
\citet{moryossef-etal-2019-step} & 47.24 & 53.30 & 34.41 & 39.00 & 44.00 & 37.00 & - & - & - \\
\citet{schmitt2020modeling} & - & 59.39 & - & - & 42.83 & - & - & 74.68 & - \\
\citet{ribeiro2020modeling} & - & 63.69 & - & - & 44.47 & - & - & 76.66 & - \\
\citet{zhao-etal-2020-bridging} & 52.78 & 64.42 & 38.23 & 41.00 & 46.00 & 37.00 & - & - & - \\
\midrule
\multicolumn{2}{l}{\small{\textit{based on PLMs}}} & & \.2em]
\citet{mager2020gpttoo}  & 33.02 & 37.68 &  -\\
\citet{harkous2020text}  & 37.70 & 38.90 &-\\
\midrule
BART\textsubscript{base}  & 36.71 & 38.64 & 52.47 \\
BART\textsubscript{large}  & 43.47 & 42.88 &
60.42 \\
T5\textsubscript{small} & 38.45 & 40.86 & 57.95 \\
T5\textsubscript{base} & 42.54 & 42.62 & 60.59 \\
T5\textsubscript{large} & \textbf{45.80} & \textbf{43.85} & \textbf{61.93} \\
\midrule
\multicolumn{3}{l}{\small{\textit{with task-adaptive pretraining}}} & \.7em]


BART\textsubscript{large} + \textsc{sta} \small{\textsc{(200K)}} & 44.72 & 43.65 & 61.03 \\
BART\textsubscript{large} + \textsc{sta} \small{\textsc{(2M)}} & 47.51 & 44.70 & 62.27 \\
T5\textsubscript{large} + \textsc{sta} \small{\textsc{(200K)}} & 48.02 & 44.85 & 63.86 \\
T5\textsubscript{large} + \textsc{sta} \small{\textsc{(2M)}} & \textbf{\textit{49.72}} & \textbf{\textit{45.43}} & \textbf{\textit{64.24}} \\
\bottomrule
\end{tabular}}
\caption{Results on AMR-to-text generation for the AMR17 test set. M and BT stand for METEOR and BLEURT, respectively. \textbf{Bold} (\textbf{\textit{Italic}}) indicates the best score without (with) task-adaptive pretraining.}
\label{tab:results-amr}
\end{table}

Following previous works, we evaluate the results with the automatic metrics BLEU \cite{Papineni:2002:BMA:1073083.1073135}, METEOR \cite{Denkowski14meteoruniversal}, and chrF++ \cite{popovic-2015-chrf}. We also use MoverScore~\cite{zhao-etal-2019-moverscore}, BERTScore~\cite{bert-score}, and BLEURT~\cite{sellam-etal-2020-bleurt} metrics, as they employ contextual embeddings to incorporate semantic knowledge and thus depend less on the surface symbols. Additionally, we also perform a human evaluation (cf.\ \S\ref{sec:human_eval}) quantifying the fluency, fidelity and meaning similarity of the generated texts.






\subsection{Results on AMR-to-Text}

Table~\ref{tab:results-amr} shows our results for the setting without additional pretraining, with additional self-supervised task-adaptive pretraining solely using the collected Gigaword sentences (\textsc{lma}), and with additional supervised task adaptation (\textsc{sta}), before fine-tuning. We also report several recent results on the AMR17 test set. \citet{mager2020gpttoo} and \citet{harkous2020text} employ GPT-2 in their approaches. Note that GPT-2 only consists of a Transformer-based decoder. We are the first to employ BART and T5, which have both a Transformer-based encoder and decoder, in AMR-to-text generation.

Only considering approaches without task adaptation, BART\textsubscript{large} already achieves a considerable improvement of 5.77 BLEU and 3.98 METEOR scores over the previous state of the art. With a BLEU score of 45.80, T5\textsubscript{large} performs best.
The other metrics follow similar trends. See Table~\ref{tab:results-amr-appendix} in the Appendix for evaluation with more automatic metrics. 



\paragraph{Task-specific Pretraining.}
\textsc{lma}
already brings some gains with T5 benefitting more than BART in most metrics.
It still helps less than \textsc{sta} even though we only have automatically generated annotations.
This suggests that the performance increases with \textsc{sta} do not only come from additional exposure to task-specific target texts and that the models learn how to handle graphs and the graph-text correspondence even with automatically generated graphs. Interestingly, gains from \textsc{sta} with 2M over 200K are larger in BART than in T5, suggesting that large amounts of silver data may not be required for a good performance with T5.
After task adaptation, T5 achieves 49.72 BLEU points, the new state of the art for AMR-to-text generation. In general, models pretrained on the \textsc{sta} setup converge faster than without task-adaptive pretraining. For example, T5\textsubscript{large} without additional pretraining converges after 5 epochs of fine-tuning whereas T5\textsubscript{large} with \textsc{sta} already converges after 2 epochs.



\subsection{Results on WebNLG}



Table~\ref{tab:results-webnlg} shows the results for the WebNLG test set. Neural pipeline models \cite{moryossef-etal-2019-step, castro-ferreira-etal-2019-neural} achieve strong performance in the \emph{unseen} dataset. On the other hand, fully end-to-end models \cite{ribeiro2020modeling,schmitt2020modeling} have strong performance on the \emph{seen} dataset and usually perform poorly in \textit{unseen} data. \citet{zhao-etal-2020-bridging} leverage additional information about the order that the triples are realized and achieve the best performance among approaches that do not employ PLMs. Note that T5 is also used in \citet{kale2020texttotext}. A particular difference in our T5 setup is that we add a prefix before the input graph. Our T5 approach achieves 59.70, 65.05 and 54.69 BLEU points on \emph{all}, \emph{seen} and \emph{unseen} sets, the new state of the art. We hypothesize that the performance gap between \emph{seen} and \emph{unseen} sets stems from the advantage obtained by a model seeing examples of relation-text pairs during training. For example, the relation \emph{party} (political party) was never seen during training and the model is required to generate a text that verbalizes the tuple: \emph{Abdul Taib Mahmud, party, Parti Bumiputera Sarawak}. Interestingly, BART performs much worse than T5 on this benchmark, especially in the \emph{unseen} partition with 9.7 BLEU points lower compared to T5.

For lack of a suitable data source (cf.\ \S\ref{sec:data}), we did not conduct experiments with \textsc{lma} or \textsc{sta} for WebNLG.
However, we explore cross-domain \textsc{sta} in additional experiments,
which we discuss in Appendix~\ref{sec:crossdomain}.









\subsection{Results on AGENDA}


\begin{table}[t]
\centering
{\renewcommand{\arraystretch}{0.6}
\begin{tabular}{@{\hspace*{1mm}}lccc@{\hspace*{1mm}}}  
\toprule
\textbf{Model} & \textbf{BLEU} & \textbf{M} & \textbf{BT}  \\
\midrule
Koncel et al. \citeyear{koncel-kedziorski-etal-2019-text}  & 14.30 & 18.80 & - \\
\citet{An2019RepulsiveBS} & 15.10 & 19.50 & -\\
\citet{schmitt2020modeling} & 17.33 & 21.43  & -\\
\citet{ribeiro2020modeling} & 18.01 & 22.23 & -\\
\midrule
BART\textsubscript{base} & 22.01 & 23.54 & -13.02 \\
BART\textsubscript{large} & \textbf{23.65} & \textbf{25.19} & \textbf{-10.93} \\
T5\textsubscript{small} & 20.22 & 21.62 & -24.10 \\
T5\textsubscript{base} & 20.73 & 21.88 & -21.03 \\
T5\textsubscript{large} & 22.15 & 23.73 & -13.96 \\
\midrule
\multicolumn{2}{l}{\small{\textit{with task-adaptive pretraining}}} & & \.7em]
BART\textsubscript{large} + \textsc{sta} & \textbf{\textit{25.66}} & \textbf{\textit{25.74}} & \textbf{\textit{-08.97}} \\
T5\textsubscript{large} + \textsc{sta} & 23.69 & 24.92 & -08.94 \\
\bottomrule
\end{tabular}}
\caption{Results on AGENDA test set. \textbf{Bold} (\textbf{\textit{Italic}}) indicates best scores without (with) task-adaptive pretraining.}
\label{tab:results-agenda}   
\end{table}

Table~\ref{tab:results-agenda} lists the results for the AGENDA test set. The models also show strong performance on this dataset. We believe that their capacity to generate fluent text helps when generating paper abstracts, even though they were not pretrained in the scientific domain. BART\textsubscript{large} shows an impressive performance with a BLEU score of 23.65, which is 5.6 points higher than the previous state of the art.

\paragraph{Task-specific Pretraining.} On AGENDA, BART benefits more from our task-adaptive pretraining, achieving the new state of the art of 25.66 BLEU points, a further gain of 2 BLEU points compared to its performance without task adaptation. The improvements from task-adaptive pretraining are not as large as for AMR17. We hypothesize that this is due to the fact that the graphs do not completely cover the target text, making this dataset more challenging. See Table~\ref{tab:results-agenda-appendix} in the Appendix for more automatic metrics. 



\begin{table}[t]
\centering
{\renewcommand{\arraystretch}{0.9}
\setlength{\belowrulesep}{0pt}
\setlength{\aboverulesep}{0pt}
\begin{tabular}{lll} 
\toprule
\textbf{Model} & \multicolumn{2}{c}{\textbf{AMR17}}  \\
\midrule
& \, \,\textbf{F} & \,\textbf{MS} \\
\midrule
\citet{mager2020gpttoo} &&  \\
\citet{harkous2020text} &&  \\
T5\textsubscript{large} &&  \\
BART\textsubscript{large} &&  \\
Reference && - \\
\midrule
\textbf{Model} & \multicolumn{2}{c}{\textbf{WebNLG}}  \\
 \midrule
& \, \,\textbf{F} & \, \,\textbf{A} \\
\midrule
 \citet{castro-ferreira-etal-2019-neural} &&  \\
 \citet{harkous2020text} &&  \\
 T5\textsubscript{large} &&  \\
BART\textsubscript{large} &&  \\
Reference &&  \\
\bottomrule
\end{tabular}}
\vspace{-1mm}
\caption{Fluency (F), Meaning Similarity (MS) and Adequacy (A) obtained in the human evaluation. Differences between models which have a letter in common are not statistically significant and where determined by pair-wise Mann-Whitney tests with .}
\label{tab:humanevevaluation}
\end{table}

 \begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/corrupt.pdf}
    \caption{Example graph from WebNLG dev linearized with the neutral separator tag, denoted \sep{}, (top left), its shuffled version (top right), texts generated with two fine-tuned versions of T5\textsubscript{small} and a gold reference (bottom). Note that T5 can produce a reasonable text even when the input triples are shuffled randomly.}
    \label{fig:graphs-shuffle}
\end{figure*}

\subsection{Human Evaluation}
\label{sec:human_eval}
To further assess the quality of the generated text, we conduct a human evaluation on AMR17 and WebNLG via crowd sourcing on Amazon Mechanical Turk.\footnote{We exclude AGENDA because its texts are scientific in nature and annotators are not necessarily AI experts.} Following previous works \cite{gardent-etal-2017-webnlg, castro-ferreira-etal-2019-neural}, we assess three quality criteria: (i) \emph{Fluency} (i.e., does the text flow in a natural, easy-to-read manner?), for AMR and WebNLG; (ii) \emph{Meaning Similarity} (i.e., how close in meaning is
the generated text to the reference sentence?) for AMR17; (ii) \emph{Adequacy} (i.e., does the text clearly express the data?) for WebNLG. We randomly select 100 texts from the generations of each model, which the annotators then rate on a 1-7 Likert scale. For each text, we collect scores from 3 annotators and average them.\footnote{Inter-annotator agreement for the three criteria ranged from 0.40 to 0.79, with an average Krippendorff's  of 0.56.} 




 \begin{figure}[t]
    \centering
    \includegraphics[width=.4\textwidth]{graph_samples.pdf}
    \caption{Performance of BART\textsubscript{base} and T5\textsubscript{base} in the dev set when experimenting with different amounts of training data.}
    \label{fig:graphs-trainingexamples}
\end{figure}

Table~\ref{tab:humanevevaluation} shows the results. There is a similar trend as in the automatic evaluation. Our approaches improve the fluency, meaning similarity, and adequacy on the two datasets compared to other state-of-the-art approaches with statistically significant margins (). Interestingly, the highest fluency improvement () is on AMR17, where our approach also has the largest BLEU improvement () over \citet{harkous2020text}. Finally, note that both models score higher than the gold sentences in fluency, highlighting their strong language generation abilities.








\subsection{Limiting the Training Data}

In Figure~\ref{fig:graphs-trainingexamples}, we investigate the PLMs' performance, measured with BLEU score, while varying (from 1\% to 100\%) the amount of training data used for fine-tuning. We find that, when fine-tuned with only 40\% of the data, both BART\textsubscript{base} and T5\textsubscript{base} already achieve similar performance as using the entire training data in all three benchmarks. Also note that in a low-resource scenario T5\textsubscript{base} considerably outperforms BART\textsubscript{base} in both datasets. In particular, with only 1\% of training examples, the difference between T5 and BART is 4.98 and 5.64 BLEU points for AMR and WebNLG, respectively. This suggests that T5 is a good candidate to be employed in low-resource graph-to-text tasks. Interestingly, the amount of training data has very little influence on the models' performance for AGENDA.






















\section{Influence of the Graph Structure}


We conduct further experiments to examine how much the PLMs consider the graph structure.
To this end, we remove parentheses in AMRs and replace \ourtag{H}, \ourtag{R}, and \ourtag{T} tokens with neutral separator tokens, denoted \sep{}, for KGs, such that the graph structure is only defined by the order of node and edge labels.
If we shuffle such a sequence,
the graph structure is thus completely obscured and the input effectively becomes a bag of node and edge labels.
See Figure~\ref{fig:graphs-shuffle} for an example of both a correctly ordered and a shuffled triple sequence.


\begin{table}[t]
\centering
{\renewcommand{\arraystretch}{0.6}

\begin{tabular}{lrrr}  
\toprule
\textbf{Model} & \textbf{AMR17} & \textbf{WebNLG} & \textbf{AGENDA} \\
\midrule


\shufmodel{T5}{small}{order} & 36.83 & 63.41 & 19.86 \\
\shufmodel{T5}{small}{shuf} & 15.56 &  61.54 & 19.08 \\
\bottomrule
\end{tabular}}
\caption{Impact (measured with BLEU) of using a bag of entities and relations (\emph{shuf}) as input for T5\textsubscript{small}.}
\label{tab:shuffle}
\end{table}


\begin{table*}[t]
    \centering
\footnotesize
    \begin{tabular}{@{\hspace{.1em}}c@{\hspace{.1em}}c@{\hspace{.1em}}p{3.8cm}@{\hspace{.8em}}p{3.5cm}@{\hspace{.8em}}p{3.5cm}@{\hspace{.8em}}p{3.4cm}@{\hspace{.2em}}}
    \toprule
         &\textbf{T/F}&\multicolumn{1}{c}{\textbf{Input Fact}} & \multicolumn{1}{c}{\textbf{\shufmodel{T5}{small}{order}}} & \multicolumn{1}{c}{\textbf{\shufmodel{T5}{small}{shuf}}} & \multicolumn{1}{c}{\textbf{CGE-LW}} \\
         
         \midrule


         
         (1) &F& \sep{} Ohio \sep{} is Part Of \sep{} Cleveland & Ohio is part of Cleveland. & Cleveland is part of Ohio. & ohio is part of cleveland. \\
(2) &F& \sep{} United States \sep{} is Part Of & Amarillo, Texas is part of & Amarillo, Texas is part of & united states is part of \\
         && \sep{} Amarillo \sep{} Texas & the United States. & the United States. & amarillo, texas. \\
(3) &F& \sep{} Leinster \sep{} is Part Of \sep{} Dublin & Leinster is part of Dublin. & Leinster is part of Dublin. & leinster is part of dublin.  \\
         \midrule
         (4) &T& \sep{} Italy \sep{} capital \sep{} Rome & italy’s capital is rome. & Rome is the capital of Italy. & rome is the capital of italy.\\
         (5) &F& \sep{} Rome \sep{} capital \sep{} Italy & Rome’s capital is Italy. & Rome is the capital of Italy. & rome is the capital of rome. \\
         (6) &T& \sep{} italy \sep{} capital \sep{} rome & Italy’s capital is rome. & Italy’s capital is rome. & - \\
         (7) &F& \sep{} rome \sep{} capital \sep{} italy & The capital of rome is italy. & Italy is the capital of rome. & - \\
\bottomrule
    \end{tabular}
    \caption{Example generations from corrupted (F) and true (T) WebNLG dev set facts by T5\textsubscript{small} fine-tuned on correctly ordered nodes (\emph{order}) and randomly shuffled nodes (\emph{shuf}) from the WebNLG training set, and CGE-LW.}
    \label{tab:qualitative}
\end{table*}

\subsection{Quantitative Analysis}

Table~\ref{tab:shuffle} shows the effect on T5\textsubscript{small}'s performance when its input contains correctly ordered triples (\shufmodel{T5}{small}{order}) vs.\ shuffled ones (\shufmodel{T5}{small}{shuf}) for both training and evaluation.
We first observe that \shufmodel{T5}{small}{order} only has marginally lower performance (around 2-4\%{}) with the neutral separators than with the \ourtag{H}/\ourtag{R}/\ourtag{T} tags or parentheses. We see that as evidence that the graph structure is similarly well captured by \shufmodel{T5}{small}{order}.
Without the graph structure (\shufmodel{T5}{small}{shuf}), AMR-to-text performance drops significantly.  KG-to-text performance, however, is not much lower,
indicating that most of the PLMs' success in this task stems from their language modeling rather than their graph encoding capabilities.
It has recently been argued that large PLMs acquire a certain amount of factual knowledge during pretraining \citep{petroni-etal-2019-language}.
We hypothesize that this knowledge makes it easier to recover KG facts based on a set of entities and relations than to reconstruct a corrupted AMR.






\subsection{Qualitative Analysis}
\label{sec:qualitative}

To further test our hypothesis that
PLMs make use of their knowledge about true facts
during KG-to-text generation, we take example facts from the WebNLG dev set, corrupt them, and feed them to both \shufmodel{T5}{small}{order} and \shufmodel{T5}{small}{shuf}. We also feed those triples to \emph{CGE-LW} \cite{ribeiro2020modeling}, a state-of-the-art KG-to-text model trained from scratch, i.e., without any pretraining. Table~\ref{tab:qualitative} shows the generated texts.

The model trained on correctly ordered input has learned a bit more to rely on the input graph structure. The false fact in example (1) is reliably transferred to the text by \shufmodel{T5}{small}{order} but not by \shufmodel{T5}{small}{shuf}, which silently corrects it. But even \shufmodel{T5}{small}{order} is not completely free from its factual knowledge bias,
as illustrated in example (2) where both models refuse to generate an incorrect fact.
This indicates that facts seen during pretraining serve as a strong guide during text generation, even for models that were fine-tuned with a clearly marked graph structure.
The fact that \emph{CGE-LW}, a graph encoder model trained from scratch on the WebNLG training set, has no difficulties in textualizing the false triples (except example 5) further supports this argument.

Interestingly, both T5 models leave the wrong input in (3) uncorrected. The fact that Leinster is a region in Ireland and not, e.g., a neighborhood of the city Dublin is probably unknown to T5. It seems that T5 falls back to the order of words in the input in such a case.
Examples (4)--(7) also illustrate this behavior. While the well-known entities ``Rome'' and ``Italy'' produce a similar behavior as ``Ohio'' and ``Cleveland'', i.e., \shufmodel{T5}{small}{order} complies with generating a false statement and \shufmodel{T5}{small}{shuf} rather follows its factual knowledge, lowercasing the entity names changes that.
With the unknown entities ``rome'' and ``italy'', both (case-sensitive) models fall back to the order of the input for their generations.


This experiment is related to testing factuality and trustworthiness of text generation models \cite{wiseman-etal-2017-challenges, falke-etal-2019-ranking}. It is important for a generation model to stay true to its input as its practical usefulness can be severely limited otherwise.
We are the first to detect this issue with the use of PLMs for data-to-text tasks.









\section{Conclusion}

We investigated two pretrained language models (PLMs) for graph-to-text generation and show that  language model adaptation (\textsc{lma}) and supervised task adaptation (\textsc{sta}) pretraining strategies are beneficial for this task. Our approaches outperform the state of the art by a substantial margin on three graph-to-text benchmarks. We also examined to what extent the graph structure is taken into account for the text generation process, and we found evidence that factual knowledge is a strong guide for these models. We believe that PLMs will play an important role in future endeavors to solve graph-to-text generation tasks and we expect our work to serve as guidance and as a strong baseline for them. A promising direction for future work is to explore ways of injecting a stronger graph-structural bias into large PLMs to thus possibly leveraging their strong language modeling capabilities and keeping the output faithful to the input graph.






\bibliography{anthology,acl2020}
\bibliographystyle{acl_natbib}

\clearpage

\appendix

\section{Appendices}

In this supplementary material, we provide: (i) additional information about the data used in the experiments, and (ii) results that we could not fit into the main body of the paper.

\subsection{Input Graph Size}
\label{section:inputgraphsize}
Figure~\ref{fig:graphs-triples} visualizes the T5\textsubscript{small}'s performance with respect to the number of input graph triples in WebNLG dataset.
We observe that \shufmodel{T5}{small}{order} and \shufmodel{T5}{small}{shuf} perform similarly for inputs with only one triple but that the gap between the models increases with larger graphs. While it is obviously more difficult to reconstruct a larger graph than a smaller one, this also suggests that the graph structure is more taken into account for graphs with more than 2 triples.
For the \textit{unseen} setting, the performance gap for these graphs is even larger, suggesting that the PLM can make more use of the graph structure when it has to.

 \begin{figure}[h]
    \centering
    \includegraphics[width=.3\textwidth]{graph_triples.pdf}
    \caption{chrF++ scores with respect to the number of triples for WebNLG \textit{seen} and \textit{unseen} test sets.}
    \label{fig:graphs-triples}
\end{figure}

\subsection{Cross-domain Pretraining}


For a given task, it is not always possible to collect closely related data -- as we saw, e.g., for WebNLG.
We therefore investigate how \textsc{sta} can help in a cross-domain setting for different KG-to-text benchmarks.
Table~\ref{tab:crossdomain_adddata} shows the results using BART\textsubscript{base} and T5\textsubscript{base}. While the texts in KGAIA and AGENDA share the domain of scientific abstracts, texts in WebNLG are more general. Also note that WebNLG graphs do not share any relations with the other KGs. For BART\textsubscript{base}, \textsc{sta} increases the performance in the cross-domain setting in most of the cases. For T5\textsubscript{base}, \textsc{sta} in KGAIA improves the performance on WebNLG.

In general, our experiments indicate that exploring additional pretraining for graph-to-text generation can improve the performance even if the data do not come from the same domain.



\label{sec:crossdomain}


\begin{table}
\centering
{\renewcommand{\arraystretch}{0.5}
\begin{tabular}{lcc}  
\toprule
\textbf{Pretrained on} & \multicolumn{2}{c}{\textbf{Fine-tuned \&{} Evaluated on}} \\
\cmidrule(lr){2-3}
 & WebNLG-\textit{Seen} & AGENDA \\
\midrule
\multicolumn{3}{c}{BART\textsubscript{base}} \\
\midrule
None & 58.71 & 22.01 \\
KGAIA & 63.20 & 23.48 \\
WebNLG & - & 21.98 \\
AGENDA & 61.25 & - \\

\midrule
\multicolumn{3}{c}{T5\textsubscript{base}} \\
\midrule
None & 62.93 & 20.73 \\
KGAIA & 63.19 & 22.44 \\
WebNLG & - & 20.27 \\
AGENDA & 62.75 & - \\

\bottomrule
\end{tabular}}
\caption{Effect (measured with BLEU score) of cross-domain \textsc{sta}.}
\label{tab:crossdomain_adddata}
\end{table}

\begin{table}
\centering
{\renewcommand{\arraystretch}{0.4}

\begin{tabular}{@{\hspace*{1mm}}l@{\hspace*{1mm}}r@{\hspace*{1mm}}r@{\hspace*{1mm}}r@{\hspace*{1mm}}r@{\hspace*{1mm}}}  
\toprule
 & \textbf{AMR17} & \textbf{WebNLG} & \textbf{AGENDA}  \\
\midrule
\#Train & 36,521 & 18,102 & 38,720  \\
\#Dev & 1,368 & 872 & 1,000  \\
\#Test & 1,371 & 1,862 & 1,000 \\
\midrule
\#Relations & 155 & 373 & 7 \\
Avg \#Nodes & 15.63 & 4.0 & 13.4 \\
\midrule
Avg \#Tokens & 16.1 & 31.5 & 157.9 \\


\bottomrule
\end{tabular}}
\caption{Statistics for the graph-to-text benchmarks.}
\label{tab:datastatistics}
\end{table}

\begin{table}
\centering
{\renewcommand{\arraystretch}{0.5}

\begin{tabular}{lcccc}  
\toprule
 & \textbf{Title} & \textbf{Abstract} & \textbf{KG}  \\
 \midrule
 Vocab & 48K & 173K & 113K \\
 Tokens & 2.1M & 31.7M & 9.6M \\
 Entities & - & - & 3.7M \\
 Avg Length & 11.1 & 167.1 & - \\
 Avg \#Nodes & - & - & 19.9 \\
 Avg \#Edges & - & - & 9.4 \\

\bottomrule
\end{tabular}}
\caption{Statistics for the KGAIA dataset.}
\label{tab:augstatistics}
\end{table}


\begin{table*}[t]
\centering
{\renewcommand{\arraystretch}{0.6}
\begin{tabular}{@{\hspace*{1mm}}l@{\hspace*{1mm}}c@{\hspace*{2mm}}c@{\hspace*{2mm}}c@{\hspace*{1mm}}}  
\toprule
\textbf{Model} & \textbf{chrF++} & \textbf{BS (F1)} & \textbf{MS}  \\
\midrule
\citet{dcgcnforgraph2seq19guo} & 57.30 & - & - \\
\citet{ribeiro-etal-2019-enhancing}  & - & - & - \\
\citet{zhu-etal-2019-modeling} & 64.05 & - & - \\
\citet{cai-lam-2020-graph} & 59.40 & - & - \\
\citet{zhao-etal-2020-line} &  - & - & - \\
\citet{doi:10.116200297} &  65.80 & - & - \\
\citet{yao-etal-2020-heterogeneous}  &  65.60 & - & - \\
\midrule
\small{\textit{based on PLMs}}  &  \.2em]
BART\textsubscript{large} + \textsc{lma} & 71.14 & 95.94 & 64.75  \\
T5\textsubscript{large} + \textsc{lma} & 72.83 & 96.32 & 67.44  \.2em]
BART\textsubscript{large} + \textsc{lma} & 51.33 & 89.12 & 33.42 \\
T5\textsubscript{large} + \textsc{lma} & 49.37 & 89.75 & 36.13 \.7em]
BART\textsubscript{large} + \textsc{sta} & \textbf{\textit{51.63}} & 89.27 & 34.28 \\
T5\textsubscript{large} + \textsc{sta} & 50.27 & \textbf{\textit{89.93}} & \textbf{\textit{36.86}} \\
\bottomrule
\end{tabular}}
\caption{Additional results on AGENDA test set. \textbf{Bold} (\textbf{\textit{Italic}}) indicates best scores without (with) task-adaptive pretraining.}
\label{tab:results-agenda-appendix}   
\end{table*}

\end{document}
