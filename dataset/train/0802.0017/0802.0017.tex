\documentclass[11pt,amssymb]{article}





\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{fullpage}

\newcommand{\comment}[1]{}

\def\il{i_{\sf left}}
\def\ir{i_{\sf right}}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{observation}{Observation }
\newtheorem{fact}{Fact}
\newtheorem{ire}{Implementation Remark}
\def\squareforqed{\hbox{\rlap{}}}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{example}{Example}
\newcommand{\proof}{\noindent\bf Proof: \rm}
\newcommand{\proofsketch}{\noindent\bf Proof Sketch: \rm}
\newcommand{\remark}{\noindent\bf Remark: \rm}
\newcommand{\qed}{\vrule height6pt width4pt\medskip}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\QED}{\hfill\qed}
\def\squareforqed{\hbox{\rlap{}}}
\def\qed{\ifmmode\squareforqed\else{\unskip\nobreak\hfil
\def\mod{\ {\rm mod}\ }
\penalty50\hskip1em\null\nobreak\hfil\squareforqed
\parfillskip=0pt\finalhyphendemerits=0\endgraf}\fi}






\newcommand{\Z}{{\Bbb Z}}
\newcommand{\C}{{\Bbb C}}
\newcommand{\R}{{\Bbb R}}
\newcommand{\Q}{{\Bbb Q}}
\newcommand{\F}{{\Bbb F}}
\newcommand{\N}{{\mathbb N}}


\parindent=0in
\parskip=10pt
\begin{document}
\title{Improved Deterministic Length Reduction}

\author{
\begin{tabular}{ccc}
Amihood Amir\thanks{ Department of Computer Science, Bar-Ilan
University, Ramat-Gan 52900, Israel, +972 3 531-8770; {\tt
amir@cs.biu.ac.il}; and Department of Computer Science, Johns Hopkins
University, Baltimore, MD 21218. Partly supported by ISF grant 35/05.}
&
Klim Efremenko \thanks{Department of Computer Science,
Bar-Ilan U., 52900 Ramat-Gan, Israel, (972-3)531-8408, {\tt
klimefrem@gmail.com}.} &
Oren Kapah\thanks{Department of Computer Science,
Bar-Ilan U., 52900 Ramat-Gan, Israel, (972-3)531-8408, {\tt
kapaho@cs.biu.ac.il}.}
\\
{\small Bar-Ilan University} & {\small Bar-Ilan University} & {\small
  Bar-Ilan University}\\
{\small and} \\
{\small Johns Hopkins University} \\
\\
Ely Porat\thanks{ Department of Computer Science, Bar-Ilan
University, 52900 Ramat-Gan, Israel, (972-3)531-7620; {\tt
porately@cs.biu.ac.il}.} &&
Amir Rothschild\thanks{ Department of
Computer Science, Bar-Ilan University, 52900 Ramat-Gan, Israel,
(972-3)531-7620; {\tt amirrot@gmail.com}.}
\\
{\small Bar-Ilan University} && {\small Bar-Ilan University}
\end{tabular}
}

\date{}

\maketitle
\begin{abstract}
This paper presents a new technique for deterministic length reduction.
This technique improves the running time of the algorithm presented
in ~\cite{LR07} for performing fast convolution in sparse data.
While the regular fast convolution of vectors  whose sizes
are  respectively, takes  using FFT, using
the new technique for length reduction, the algorithm proposed in
~\cite{LR07} performs the convolution in , where
 is the number of non-zero values in . The algorithm
assumes that  is given in advance, and  is given in
running time. The novel technique presented in this paper improves the
convolution time to  {\sl deterministically}, which
equals the best running time given achieved by a {\sl randomized}
algorithm.

The preprocessing time of the new technique remains the same as the
preprocessing time of~\cite{LR07}, which is . This assumes
and deals the case where  is polynomial in
. In the case where  is exponential in , a
reduction to a polynomial case can be used. In this paper we also
improve the preprocessing time of this reduction from
 to .
\end{abstract}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1 in}

\section{Introduction\label{sec:intro}}

The {\em  d-Dimensional point set matching problem} serves as powerful
tools in numerous application domains.
In the d-Dimensional point set matching problem, two sets of points
 consisting of  points, respectively, are given.
The goal is to determine if there is a rigid transformation under
which all the points in  are covered with points in . Among the
important application domains to which this problem contributes are
model based object recognition, image registration, pharmacophore
identification, and searching in music archives. For an explanation of
the uses of the point-set matching problem in these domains
see~\cite{LR07}.

The point-set matching problem has been studied in the literature in
many variation,  not the least of which in the
algorithms literature. In ~\cite{sc:98Sch} Cardoze and Schulman used a
randomized algorithm to reduce the space size of  and then
apply solve the problem in the reduced
space. In ~\cite{CH:02} Cole and Hariharan proposed a solution to
the {\it d-Dimensional Sparse Wildcard Matching}. This is a
generalization of the d-Dimensional point set matching problem where
every point in  is associated with a value. A match is declared
if the values of coinciding points are equal. The Cole and Hariharan
solution consists of two steps. The first step is a {\it Dimension
Reduction} where the inputs  are linearized into raw vectors
 of size polynomial in the number of non-zero values. The
second step was a {\it Length Reduction} where each of the raw vectors
 was replaced by  short vectors of size  where
 is the number of non-zeros. The idea is that the mapping to the
short vectors preserves the distances in the original vectors, thus
the problem is reduced to a matching problem of short vectors, to
which efficient solutions exist. The problem with the {\em length
reduction} idea is that more then one point can be mapped into the
same location, thus it is no longer clear whether there is indeed a
match in the original vectors. The proposed solution of Cole and
Hariharan was to create a set of  pairs of vectors using
 hash function rather then a single pair of vectors. Their
scheme reduced the failure probability.

In~\cite{LR07}, the first {\bf deterministic} algorithm for
finding  hash functions that reduce the size of the vectors to
 was presented. The algorithm guaranteed that each
non-zero value appears with no collisions in {\em at least} one of the
vectors, thus eliminating the possibility of en error.
The {\em length reduction} idea was used to solve the {\it Sparse
Convolution} problem posed in ~\cite{muthu-open}, where the aim is to
find the convolution vector  of two vectors  whose sizes
are , with  non-zero elements respectively (where
. It is assumed that the two vectors are not given
explicitly, rather they are given as a set of 
pairs. Using the Fast Fourier Transform (FFT) algorithm, the
convolution can be calculated in running time \cite{CLR-92}. In our context, though, the vectors  are
sparse. The aim of the algorithm is to compute  in time
proportional to the number of non-zero entries in , which may be
significantly smaller than . Clearly, this can be easily done
in time .

The goal of the length reduction is as follows: Given two vectors
 whose sizes are , with  non-zero
elements respectively (where , obtain two vectors
 of size  such that all the non-zero in  and
in  will appear as singletons in  and in 
respectively while maintaining the distance property.

The distance property which need to be maintained is defined as
follows: If  is aligned with , then
 will be aligned with .

This goal was not reached yet, rather a set of  vectors
of size  where obtained in ~\cite{LR07}, where each
non-zero in the text appears at least once as a singleton in the set
of vectors. This length reduction gave an 
algorithm for convolution in sparse data. In this paper we go one
step forward and reduce the size of the obtained vectors to
. This length reduction technique improves the running time
of the fast convolution presented in ~\cite{LR07} to , which is the running time for the randomized algorithm
presented in ~\cite{CH:02}.


\section{Preliminaries and Notations}\label{s:pre}

Throughout this paper, a capital letter (usually ) is used to
denote the size of the vector, which is equivalent to the largest
index of a non-zero value, and a small letter (usually ) is used
to denote the number of non-zero values. It is assumed that the
vectors are not given explicitly, rather they are given as a set of
 pairs, for all the non-zero values.

A convolution uses two initial functions,  and , to
produce a third function . We formally define a discrete
convolution.
\begin{definition}
Let  be a function whose domain is  and 
a function whose domain is . We may view  and
 as arrays of numbers, whose lengths are  and ,
respectively. The {\em discrete convolution of  and } is the
polynomial multiplication

\end{definition}

In the general case, the convolution can be computed by using the Fast
Fourier Transform (FFT)~\cite{CLR-92}. This can be done in time
, in a computational model with word size . In the sparse case, many values of  and  are
. Thus, they do not contribute to the convolution value. In our
convention, the number of non-zero values of  is . Clearly, we can compute the convolution in time
. The question posed by Muthukrishnan~\cite{muthu-open} is
whether the convolution can be computed in time .

Cole and Hariharan's suggestion was to use {\em length reduction}.
Suppose we can map all the non-zero values into a smaller vector, say
of size . Suppose also that this mapping is alignment
preserving in the sense that applying the same transformation on 
will guarantee that the alignments are preserved. Then we can simply
map the the vectors  and  into the smaller vectors and then
use FFT for the convolutions on the smaller vectors, achieving time
.

The problem is that to-date there is no known mapping with that
alignment preserving property. Cole and Hariharan~\cite{CH:02}
suggested a randomized idea that answers the problem with high
probability. The reason their algorithm is not deterministic is the
following:
In their length reduction phase, several indices of
non-zero values in the original vector may be mapped into the same
index in the reduced size vector. If the index of only one non-zero
value is mapped into an index in the reduced size vector, then this
index is denoted as {\it singleton} and the non-zero value is said
to appear as a {\it singleton}. If more then one non-zero value is
mapped into the same index in the reduced size vector, then this
index is denoted as {\it multiple}. The multiple case is problematic
since we can not be sure of the right alignment. Fortunately, Cole
and Hariharan showed a method whereby in  tries, the
probability that some index will {\em always} be in a multiple
situation is small. In~\cite{LR07}, a deterministic solution to the
multiple problem was presented. That solution utilized number
theoretic ideas. The new idea of this paper is to improve the
reduction size by using {\em polynomials} to represent the location of
the non- elements of the given vectors.


\section{The New Length Reduction Technique for the Polynomial
  Case}\label{s:length}

The proposed technique deals with the case that  is polynomial
in , thus the indices are bounded by . In the case
where,  is exponential in , the reduction to a polynomial
case can be used.

The main idea of the algorithm is to derive a set of unique polynomials
from each non-zero index in , and one polynomial for each non-zero
in . Each assignment for the polynomials in , where  is a
prime number of size  will give a different mapping of
the non-zeros in  and in  to vectors of size . The
convolution will be performed between the vectors obtained from
 and  under the same assignments.

The first step of the algorithm is to choose a prime number of size
, and create a polynomial for each non-zero index in
. The created polynomial of index  will be denoted as the base
polynomial of . The creation of the polynomial is done by
representing the index as a number in base . Each
digit is interpreted as a coefficient of the polynomial. For example:
If , then index  in base  is  in base  which is represented by the polynomial .

Since the indices in  are bounded by , and  is , then the degree of the polynomials which created in this step
is bounded by . In the next step, from each polynomial we create
 polynomials. This is done by giving to choices for each
coefficient of the polynomial: (1) Leave it as is. (2) Add  to the coefficient and decrease by 1 the coefficient of the
higher degree. We do this for all the coefficients of the polynomial
except for the coefficient of the highest degree.

\begin{example}
Suppose we have a non-zero index , using  we get the base
polynomial . After the second step we will obtain 
polynomials: , ,,.\\
The first polynomial is the base polynomial. The second polynomial was
obtained by adding  to the first coefficient and decreasing the
second coefficient by one. The 3rd and the 4th polynomials were created
by adding  to the second coefficient of the first and second
polynomials respectively, and decreasing the third coefficient by one.
\end{example}

The duplication of the polynomials was made to meet the distance
preserving requirement from the length reduction specified in the
following Lemma:

\begin{lemma}\label{l:PolAllignment}
For any assignment of , if  is aligned with the base
polynomial representing , then  will be aligned with
one of the polynomials representing .
\end{lemma}
\begin{proof}
Let  be the chosen prime number. Index  in  is
represented by the polynomial , and index  in  is
represented by the a polynomial .
Index  in  is represented by a polynomial of the form
, and index  in  is
represented by a polynomial . Note
that the coefficients  and  are smaller then .

Clearly, if  is aligned with , then for any
assignment of ,  will be aligned with the polynomial
. Now lets
look at the first coefficient of , since  and  are
smaller then , then there are only two cases: (1)
, thus . (2)
, thus 
which is covered by the
polynomial where  was added to the first coefficient.\\
In the later case, one was added to the second coefficient, thus we
decrease the next coefficient whenever we add  to the
current coefficient. The same cases exist also in all the
coefficient, but a polynomial was created for each possible case (
cases), thus one of the created polynomials will be equal to the
polynomial . 
\end{proof}

Note that all the  created polynomials are unique, and
in . Assigning a value to the polynomials in  will give a
vector of size .

\begin{lemma}\label{l:maxRoots}
Any two polynomials can be mapped to the same location in at most
 assignments.
\end{lemma}
\begin{proof}
The distance between any two polynomials gives a polynomial, where the
degree of the difference polynomial is bounded by . Since both
polynomials give the same index under the selected assignment, then the
assigned value is a root of the difference polynomial. The degree of
this polynomial is bounded by , thus it can have at most 
different roots in . 
\end{proof}

Since any polynomial can be mapped into the same location with at most
 other polynomials, and with each of them at most 
times, due to Lemma \ref{l:maxRoots}, then we get the following
Corollary:
\begin{corollary}\label{c:maxPolMultiples}
Any polynomial can appear as a {\it multiple} in not more then  vectors.
\end{corollary}

The last step of the length reduction algorithm is to find a set of
 assignments which will ensure that each polynomial will
appear as a singleton at least once.

The selection of the  assignments is done as follows:
Construct table  with  columns and  rows. Row  correspond to an assigned value
 and the corresponding reduced length vector . A
column corresponds to a polynomial . The value of  is set
to  if polynomial  appears as a {\it singleton} in vector
. Due to Corollary \ref{c:maxPolMultiples}, the number of
zeros in each column can not exceed . Thus,
in each column there are 's in at least half of the rows, which
means that the table is at least half full. Since the table is at
least half full there exists a row in which there is one in at least
half of the columns. The assignment value which generated this row
is chosen, and all the columns where there was a  in the selected
row are deleted from the table.

Recursively another assignment value is chosen and the table size is
halved again, until all the columns are deleted. Since at each step
at least half of the columns are deleted, the number of prime number
chosen can not exceed  .

{\bf Time:} Creating vector  (row ) takes  time.
Since we start with a full matrix of  rows then the
initialization takes  time. Choosing the 
assignment values is done recursively. The recurrence is:

The closed form of this recurrence is .




\section{The New Algorithm for The Exponential Case}\label{s:Exp}

In this case, as proposed in ~\cite{LR07}, each of the vectors 
and  is reduced into a single vector of size , where
all the non-zeros appear as singletons. The reduction is preformed
using the modulus function with a prime number  of size
. It was already proven there that there are at most
 prime number of size , which generate at least one
multiple. Thus, by testing  prime numbers we ensure that at
least one of them produce a vector with no multiples.

In order to find such a prime number, we find  of size
. Then we multiply all the prime numbers to receive a
large number . In addition we have at most  different
distances between any two non-zeros. We multiply all of them to
receive the large number . The next step is to find the greatest
common divider () between  and . Since there is at least
one prime number in  which does not divide , then 
is less then . Dividing  by the  will give  which
is the multiplication of all the prime numbers that create only
singletons. The last step is to find at least one of them. This is
done using a binary search on the prime numbers. We take the
multiplication of half of the prime numbers , and find the
. If  we continue with this set of prime
numbers and multiply half of them iteratively. Otherwise, we
continue with the other half of the prime numbers. After  iterations we will find one prime number which will generate
only singletons.

The algorithm appears in detail below.

\fbox{
\begin{minipage}{16cm}
{\bf Algorithm  --  is exponential in } {\sf
\begin{enumerate}
    \item Find  prime numbers of size .
    \item Multiply all the prime numbers to obtain .
    \item Multiply all the difference between any two non-zero indices to obtain .
    \item Set .
    \item Let  be the set of all prime numbers.
    \item While the size of  is larger then  do:
\begin{enumerate}
    \item Let  be a set of the first half of prime numbers in .
    \item Set  to be the multiplication of all the prime numbers in .
    \item If  then set , otherwise set .
\end{enumerate}
\end{enumerate}
{\bf end Algorithm} }
\end{minipage}
}

{\bf Correctness:} Immediately follows from the discussion.

{\bf Time:} Step 1 is performed in time 
using the primality testing described in ~\cite{berri:02}. Step 2 is
done by building a binary tree of multiplication where each node
contain the multiplication of the two number in the lower level. This
tree has  levels. In the leaves there are  prime
numbers with  bits, so the total number of bits in each
level is . A multiplication of two numbers can be
computed in time  ~\cite{SS-71}, where  is
the number of bits. Thus each level can be computed in time  and the total time for step 2 is . step 3 is preformed in the same way, but this time
in the leaves there are  numbers with  bits, thus each
level has  bits and the time for this step is . In step 4 we calculate the  of two numbers with  bits. This can be calculated in time  using ~\cite{SZ:02}. The calculation for step 6(b) was
already performed in step 2, and step 6(c) can be calculated in time
, thus the time of step 6 is . Following this discussion the total time of this
algorithm is .


\section{Conclusion and Open Problems}\label{s:conc}

Improved deterministic algorithms for Length Reduction and Sparse
Convolution where presented in this paper. These can be used as
tools to provide faster algorithms for several well known problems.
The deterministic time achieved for convolving input patterns with a
fixed text is the same as the best known randomized algorithm.

An important problem remains: Can the Length Reduction and Sparse
Convolution problems be solved in real time without the need of the
preprocessing step, or alternately, can the preprocessing time be
reduced from quadratic?

\bibliographystyle{plain}
\small{
\bibliography{PM}
}

\end{document}
