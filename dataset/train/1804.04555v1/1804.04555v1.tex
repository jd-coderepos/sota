\documentclass[5pt]{article}

\usepackage[letterpaper]{geometry}
\usepackage{spconf,amsmath,epsfig}
\usepackage{enumitem}
\usepackage{spconf,amsmath,epsfig}
\usepackage{epstopdf}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{fancyhdr}
\thispagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}

\lfoot{978-1-5386-1737-3/18/\\mathcal{C}_{t}\mathcal{D}_{t}\mathcal{T}\mathcal{D}_td_t^k \in \mathcal{D}_t \mathcal{C}_{t}c_n^k \in \mathcal{C}_{t} ; n \leq t,\mathcal{C}_{t}=\mathcal{C}_{t-1}\bigcup\mathcal{D}_{t-1}d_t^kc_t^kkt{c_t^k}{d_t^k}G=(\mathcal{V},\mathcal{E})V\mathcal{C}_t\in\mathcal{V}_L\mathcal{D}_t\in\mathcal{V}_Re_{ij}\in\mathcal{E}c_t^id_t^j[t, id, x, y, w, h, s](x,y)\mathcal{T}\tau^k \in \mathcal{T} \mathcal{S}(c_t^i,d_t^i)c_t^id_t^je_{ij}e_{ij}\in \{0,1\}F_a(c_t^i,d_t^i) L_2c_t^id_t^jf_{c_t^i}, f_{d_t^i}\alpha, \betaF_m(c_t^i,d_t^i) p_{d_t^i}\hat{p}_{c_t^i}[\hat{x},\hat{y},\hat{w},\hat{h}]x,y\mathcal{T}\mathcal{T}^+\mathcal{T}^- r_i^k,r_j^kij \tau^k\tau^k \in \mathcal{T}\varphi_{c,i}^k,\ i\in[1,L^k]\varphi_{c,i}^k\varphi_{g,i}^k,\ i\in[-L^k,-1]\cup[1,L^k]\{\varphi_{g,1}^k,\varphi_{g,-9}^k\},\{\varphi_{g,2}^k,\varphi_{g,-8}^k\}, ...\times (L^k-1)\varphi_d^k\varphi_{d,i}^k\varphi_d^k\varphi_{d,i}^k, i\in[1,9]\varphi_{d,4}^k\mathcal{L}_{glo}\mathcal{L}_{loc}\varphi_f^k\varphi_g^kE(\varphi_i,\varphi_j)y\in\{0,1\}\etaF(\varphi)\varphi_f^k\lambda\mathcal{L}_{v}\mathcal{L}_{id*}\mathcal{L}_{loc\_v}\mathcal{L}_{loc\_id}\mathcal{L}_{loc\_v}\delta\mathcal{L}_{loc\_id}\varphi_{d,i}^k\varphi_d^k=([\varphi_{d,1}^k,\varphi_{d,2}^k,...,\varphi_{d,L^k}^k])\varphi_f^k\mathcal{T}^+\tau^k\in \mathcal{T}^+\uparrow\uparrow\uparrow\downarrow\downarrow\downarrow\downarrow\downarrow\uparrowB(\cdot)[x,y,w,h]\bar{\tau^i}(v)\tau^i\mur_{L^i}^ir_1^j\tau^i\tau^jr_i^k(s)\tau^k(s)\varphi_f^k\times[x, y, w, h]L\in[3,10][\hat{x},\hat{y}, \hat{w}, \hat{h}]\varphi_{g,i}^k, i\in[-L^k,-1]\cup[1,L^k]$ are also 128-dimensional vectors, which are fed to FC network for verification loss and for comparison of the corresponding features for
classification loss. We use the AdamOptimizer~\cite{kingma2014adam} to train our network. The training dataset is extracted from dataset PathTrack~\cite{manen2017pathtrack} and video re-identification dataset MARS~\cite{zheng2016mars}.
\vspace{-0.1cm}
\begin{table}[h]\scriptsize
\vspace{-0.4cm}
\begin{center}
\caption{MOTA of each MOT16 sequences} \label{tab:cap}
\vspace{-0.1cm}
\begin{tabular}{c|c|c|c|c|c|c|c}

\hline
  Sequence  & 01& 03 & 06 & 07 & 08 & 12 & 14 \\
    \hline
  Static(s)\&Moving(m)  & s& s & m & m & s & m & m \\
  \hline
  \hline
  QuadMOT16~\cite{son2017multi} & 30.8  & 51.0 & 49.2& 41.9 & 29.9 & 38.0 & 24.0  \\
  EDMT~\cite{kim2015multiple} & 35.3  & 51.2 & 49.8 & \textbf{46.1} & 32.3 & 43.1 & 24.9  \\
  MHT\_DAM~\cite{Henschel2017A} & 35.8  & 52.7 & 49.1 & 39.3 & 33.2 & 44.3 &26.1  \\
  STAM16~\cite{chu2017online} & 35.7&53.8&48.4&38.0&32.3&42.3&24.6  \\
  NOMT~\cite{Henschel2017A} & 34.2&53.0&51.3&44.9&\textbf{36.7}&39.3&23.5 \\
  AMIR~\cite{sadeghian2017tracking} & 37.8&53.8&49.2&45.5&32.5&40.4&\textbf{29.4} \\
  NLLMPa~\cite{wang2016joint} & 30.7&56.4&49.8&40.7&33.3&43.3&23.3  \\
  FWT~\cite{Henschel2017A} & 33.6&55.7&51.8&40.3&35.1&44.67&24.7  \\
  LMP~\cite{tang2017multiple} &39.9&56.1&\textbf{52.3}&43.1&33.8&43.7&28.8 \\
  \hline
  \hline
Rank  & \textbf{1} & \textbf{1} & 10 & 3 & 2 & 8& 3  \\
  GCRA(Ours)  & \textbf{42.5} & \textbf{56.7} & 35.9 & 44.1 & 35.4 & 39.6& 28.3  \\

  \hline
\end{tabular}
\end{center}
\vspace{-0.9cm}
\end{table}

\vspace{-0.3cm}
\subsection{Results of Multi-Object Tracking}
\vspace{-0.1cm}
\textbf {Evaluation Metrics}.\ The MOTChallenge Benchmark depends on multiple evaluation index of trackers. These metrics~\cite{bernardin2008evaluating}~\cite{ristani2016performance} include Multiple Object Tracking Accuracy (MOTA), ID F1 Score (IDF1), Mostly tracked targets (MT), Mostly lost targets (ML), False Positives (FP), False Negatives (FN), Identity Switches (IDSw.), the total number of Fragment (Frag) and Processing Speed (Hz).


\noindent\textbf {MOTChallenge Benchmark}. We evaluated performance of our method on MOT16~\cite{DBLP:journals/corr/MilanL0RS16}. The sequences of dataset are captured from surveillance, hand-held shooting and driving recorder by static camera and moving camera.

\noindent\textbf {Result Comparison}. We compare the state-of-the-art methods on MOT16. The result of MOT benchmark is presented in Table 1. GCRA\_G is the tracklet generation in this paper, and the GCRA is our final method. Obviously, our method achieves higher performance of MOTA which is the primary evaluation metric. The result of static camera sequence is better than others especially, but moving camera is unsatisfactory because the temporal and spatial constraints are not suitable for it(as shown in Table 2).
\vspace{-0.3cm}
\begin{figure*}[t]
    \centering
    \includegraphics[width=16cm]{figure5.pdf}
    \vspace{-0.5cm}
    \caption{Qualitative results on the MOT16 benchmark. (a): MOT16-01 (b): MOT16-03}
\vspace{-0.3cm}
\end{figure*}
\section{Conclusion}
\vspace{-0.1cm}
We propose a novel tracklet association scheme to cleave and re-connect the tracklets on crowd or long-term occlusion by Deep Siamese Bi-GRU. The method calculates each output of bidirectional GRU to search the suitable split position and match the tracklets to reconnect the same person. For training, we extracted the tracklet dataset from existing MOT datasets for training our frameworks. Our proposal has better performance for static camera such as surveillance. The algorithm achieves 48.2\% in MOTA that approaches the state-of-the-art methods on MOT16 benchmark dataset. The qualitative result is shown in Fig.\ 5.
\vspace{-0.3cm}
\section{Acknowledgement}
\vspace{-0.2cm}
This work is partially supported by the National Science Foundation of China No.(61421062, 61602011), the Major National Scientific Instrument and Equipment Development Project of China under contract No.2013YQ030967, National Key Research and Development Program of China (2016YFB0401904) and NVIDIA NVAIL program.
\vspace{-0.2cm}


\vspace{-0.2cm}
\section{References}
\renewcommand\refname{}
\vspace{-10.5mm}
\bibliographystyle{IEEEbib}
\bibliography{icme2018template}
\vspace{-0.2cm}
\end{document}
