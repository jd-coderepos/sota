

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}

\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}

\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\definecolor{myblue}{RGB}{47, 114, 193}
\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}
\definecolor{brandeisblue}{rgb}{0.0, 0.44, 1.0}
\definecolor{blueish}{rgb}{0.0, 0.3, .6}
\definecolor{pink}{rgb}{1, 0, 1}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=blueish,linkcolor=brickred,bookmarks=false]{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\def\eg{\emph{e.g.}, } 
\def\ie{\emph{i.e.}, }
\def\cf{\emph{cf.}, } 
\def\etc{\emph{etc.} } \def\vs{\emph{vs.} }
\def\wrt{w.r.t. } \def\dof{d.o.f. }
\def\iid{i.i.d. } \def\wolog{w.l.o.g. }
\def\etal{\emph{et. al.}, }
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{wrapfig}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{8pt}{8pt}\ttfamily\selectfont,
  columns=fullflexible,
  numbers=none,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{8pt}{8pt}\color{codeblue},
  keywordstyle=\fontsize{8pt}{8pt}\color{codekw},
}
\usepackage{subcaption}
\usepackage{wrapfig}

\usepackage{mathtools}  
\usepackage{xfrac}  
\usepackage{booktabs}
\usepackage{cite}




\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{3370}  

\title{MaxViT: Multi-Axis Vision Transformer}


\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{MaxViT: Multi-Axis Vision Transformer}
\author{Zhengzhong Tu\inst{1,2} \and
Hossein Talebi\inst{1} \and
Han Zhang\inst{1} \and
Feng Yang\inst{1} \and \\
Peyman Milanfar\inst{1} \and
Alan Bovik\inst{2} \and
Yinxiao Li\inst{1}}
\authorrunning{Z. Tu et al.}
\institute{Google Research \and
University of Texas at Austin
}
\maketitle

\begin{abstract}
Transformers have recently gained significant attention in the computer vision community.
However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones.
In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity.
We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages.
Notably, MaxViT is able to ``see'' globally throughout the entire network, even in earlier, high-resolution stages.
We demonstrate the effectiveness of our model on a broad spectrum of vision tasks.
On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5\% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7\% top-1 accuracy.
For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment.
We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module.
The source code and trained models will be available at \url{https://github.com/google-research/maxvit}.

\keywords{Transformer, Image classification, Multi-axis attention.}
\end{abstract}


\section{Introduction}
Convolutional Neural Networks (ConvNets) have been the dominant architectural design choice for computer vision~\cite{krizhevsky2012imagenet,he2016deep,szegedy2015going,szegedy2016rethinking} since AlexNet~\cite{krizhevsky2012imagenet}.
ConvNets continue to excel on numerous vision problems by going deeper~\cite{szegedy2015going}, wider~\cite{szegedy2016rethinking,szegedy2017inception}, adding dense connections~\cite{huang2017densely}, efficient separable convolutions~\cite{howard2017mobilenets,sandler2018mobilenetv2}, atrous convolutions~\cite{chen2017deeplab}, using encoder-decoder frameworks~\cite{ronneberger2015u}, and even introducing modern micro-design components~\cite{liu2022convnet}.
Meanwhile, as inspired by the evolution of self-attention models like Transformers~\cite{vaswani2017attention} in natural language processing~\cite{devlin2018bert,yang2019xlnet,lan2019albert,raffel2019exploring}, numerous researchers have started to introduce attention mechanisms into vision~\cite{wang2018non,carion2020end}. 
The Vision Transformer (ViT)~\cite{dosovitskiy2020image} is perhaps the first fully Transformer-based architecture for vision, whereby image patches are simply regarded as sequences of words and a transformer encoder is applied on these visual tokens. When pre-trained on large-scale datasets~\cite{sun2017revisiting}, ViT can achieve compelling results on image recognition.


However, it has been observed that without extensive pre-training~\cite{dosovitskiy2020image,touvron2021training} ViT underperforms on image recognition. This is due to the strong model capacity of Transformers, that is imbued with less inductive bias, which leads to overfitting.
To properly regularize the model capacity and improve its scalability, numerous subsequent efforts have studied sparse Transformer models tailored for vision tasks such as local attention~\cite{liu2021swin,yang2021focal,li2021localvit,chu2021twins}. These methods typically re-introduce hierarchical architectures to compensate for the loss of non-locality.
The Swin Transformer~\cite{liu2021swin} is one such successful attempt to modify Transformers by applying self-attention on shifted non-overlapping windows. For the first time, this approach outperformed ConvNets on the ImageNet benchmark with a pure vision Transformer.
Despite having more flexibility and generalizability than the full attention used in ViT, window-based attention has been observed to have limited model capacity due to the loss of non-locality, and henceforth scales unfavorably on larger data regimes such as ImageNet-21K and JFT~\cite{dai2021coatnet}.
However, acquiring global interactions via full-attention at early or high-resolution stages in a hierarchical network is computationally heavy, as the attention operator requires quadratic complexity.
How to efficiently incorporate global and local interactions to balance the model capacity and generalizability under a computation budget still remains challenging.


\begin{figure}[!tb]
 \centering
 \begin{subfigure}[b]{0.49\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/imagenet_top1_224_flops.pdf}
     \caption{Accuracy \vs FLOPs performance scaling curve under ImageNet-1K training setting at input resolution 224224.}
     \label{fig:imagenet-flops}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.49\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figures/imagenet_top1_params.pdf}
     \caption{Accuracy \vs Parameters scaling curve under ImageNet-1K fine-tuning setting allowing for higher sizes (384/512).}
     \label{fig:three sin x}
 \end{subfigure}
\caption{\textbf{Performance comparison of MaxViT with state-of-the-art vision Transformers on ImageNet-1K.} Our model shows superior performance in terms of both accuracy \vs computation and accuracy \vs parameters tradeoff.}
\label{fig:imagenet-1k-main}
\end{figure}

In this paper, we present a new type of Transformer module, called multi-axis self-attention (Max-SA), that capably serves as a basic architecture component which can perform both local and global spatial interactions in a single block.
Compared to full self-attention, Max-SA enjoys greater flexibility and efficiency, \ie naturally adaptive to different input lengths with linear complexity; in contrast to (shifted) window/local attention, Max-SA allows for stronger model capacity by proposing a global receptive field.
Moreover, with merely linear complexity, Max-SA can be used as a general stand-alone attention module in any layer of a network, even in earlier, high-resolution stages.



To demonstrate its effectiveness and universality, we further design a simple but effective vision backbone called \textbf{M}ulti-\textbf{ax}is \textbf{Vi}sion \textbf{T}ransformer (\textbf{MaxViT}) by hierarchically stacking repeated blocks composed of Max-SA and convolutions.
While our proposed model belongs to the category of hybrid vision Transformers, MaxViT distinguishes from previous approaches~\cite{xiao2021early,dai2021coatnet} in that we strive for simplicity, by designing a basic block unifying convolution, local, and global attention, then simply repeating it.
Our experiments shows that the MaxViT significantly improves upon state-of-the-art (SOTA) performance under all data regimes for a broad range of visual tasks including classification, object detection and segmentation, image aesthetics assessment, and image generation.
Specifically, as Figure~\ref{fig:imagenet-1k-main} shows, MaxViT outperforms all recent Transformer-based models in regards to both accuracy \vs FLOPs and accuracy \vs parameter curves. Our contributions are:

\begin{itemize}[leftmargin=*]
\itemsep0em 
    \item A generic strong Transformer backbone, \textbf{MaxViT}, that can capture both local and global spatial interactions throughout every stage of the network.
\item A novel stand-alone multi-axis attention module composed of blocked local and dilated global attention, enjoying global perception in linear complexity.
\item We demonstrate large amounts of design choices including number of layers, layouts, the use of MBConv, \etc with extensive ablation studies, that eventually converge towards our final modular design, the MaxViT-Block.
    \item Our extensive experiments show that MaxViT achieves SOTA results under various data regimes for a broad range of tasks including image classification, object detection, image aesthetic assessment, and image generation.

\end{itemize}








\section{Related work}

\textbf{Convolutional networks.}
Since AlexNet~\cite{krizhevsky2012imagenet}, convolutional neural networks (ConvNets) have been used as \textit{de facto} solutions to almost all vision tasks~\cite{he2016deep,chen2020proxiqa,huang2017densely,zhao2022tracking,chen2022learning,li2021comisr,whang2022deblurring,talebi2021learning,wang2021rich} before the ``Roaring 20s''~\cite{liu2022convnet}. Phenomenal architectural improvements have been made in the past decade: residual~\cite{he2016deep} and dense connections~\cite{huang2017densely}, fully-convolutional networks~\cite{long2015fully}, encoder-decoder schemes~\cite{ronneberger2015u}, feature pyramids~\cite{Lin2017FeaturePN}, increased depths and widths~\cite{szegedy2015going}, spatial- and channel-wise attention models~\cite{hu2018squeeze,woo2018cbam}, non-local interactions~\cite{wang2018non}, to name a few. A remarkable recent work ConvNeXt~\cite{liu2022convnet} has re-introduced core designs of vision Transformers and shown that a `modernized' pure ConvNet can achieve  performance comparable to Transformers on broad vision tasks.

\noindent\textbf{Transformers in vision.}
Transformers were originally proposed for natural language processing~\cite{vaswani2017attention}. The debut of the Vision Transformer (ViT)~\cite{dosovitskiy2020image} in 2020 showed that pure Transformer-based architectures are also effective solutions for vision problems. The elegantly novel view of ViT that treats image patches as visual words has stimulated explosive research interest in visual Transformers. To account for locality and 2D nature of images, the Swin Transformer aggregates attention in shifted windows in a hierarchical architecture~\cite{liu2021swin}. More recent works have been focused on improving model and data efficiency, including sparse attention~\cite{dong2021cswin,yang2021focal,rao2021dynamicvit,wang2020axial, xu2022cobevt, arnab2021vivit}, improved locality~\cite{yuan2021tokens,han2021transformer}, pyramidal designs~\cite{wang2021pyramid,fan2021multiscale, xu2022v2x},
improved training strategies~\cite{touvron2021training,touvron2021going,zhou2021deepvit,bello2021revisiting}, \etc We refer readers to dedicated surveys~\cite{khan2021transformers,khan2021transformers} of vision Transformers for a comprehensive review.

\noindent\textbf{Hybrid models.}
Pure Transformer-based vision models have been observed to generalize poorly due to relatively less inductive bias~\cite{dosovitskiy2020image,dai2021coatnet,touvron2021training}. Vision Transformers also exhibit substandard optimizability~\cite{xiao2021early}. An intriguingly simple improvement is to adopt a hybrid design of Transformer and convolution layers such as using a few convolutions to replace the coarse patchify stem~\cite{xiao2021early,dai2021coatnet}. A broad range of works fall into this category, either explicitly hybridized~\cite{wu2021cvt,d2021convit,dai2021coatnet,xiao2021early,fan2021multiscale,xu2021co,bello2019attention} or in an implicit fashion~\cite{liu2021swin,chu2021twins}.

\noindent\textbf{Transformer for GANs.}
Transformers have also proven effective in generative adversarial networks (GANs)~\cite{goodfellow2014generative}. TransGAN~\cite{jiang2021transgan} built a pure Transformer GAN with a careful design of local attention and upsampling layers, demonstrating effectiveness on small scale datasets~\cite{krizhevsky2009learning,pmlr-v15-coates11a}. GANformer~\cite{hudson2021generative} explored efficient global attention mechanisms to improve on StyleGAN~\cite{karras2020analyzing} generator. HiT~\cite{zhao2021improved} presents an efficient Transformer generator based on local-global attention that can scale up to 1K high-resolution image generation.

\section{Method}

Inspired by the sparse approaches presented in~\cite{zhao2021improved,tu2022maxim}, we introduce a new type of attention module, dubbed blocked multi-axis self-attention (Max-SA), by decomposing the fully dense attention mechanisms into two sparse forms -- window attention and grid attention -- which reduces the quadratic complexity of vanilla attention to linear, without any loss of non-locality.
Our sequential design offers greater simplicity and flexibility, while performing even better than previous methods -- each individual module can be used either standalone or combined in any order (Tables \ref{tab:ablation-grid-attention}-\ref{tab:ablation-mbconv}), whereas parallel designs~\cite{zhao2021improved,tu2022maxim} offer no such benefits.
Because of the flexibility and scalability of Max-SA, we are able to build a novel vision backbone, which we call MaxViT, by simply stacking alternative layers of Max-SA with MBConv~\cite{howard2017mobilenets} in a hierarchical architecture, as shown in Figure~\ref{fig:maxt-architecture}.
MaxViT benefits from global and local receptive fields throughout the entire network, from shallow to deep stages, demonstrating superior performance in regards to both model capacity and generalization abilities.


\begin{figure}[!t]
\centering
\includegraphics[width=0.96\linewidth]{figures/MaxViT_Architecture.pdf}
\caption{\textbf{MaxViT architecture.} We follow a typical hierarchical design of ConvNet practices (e.g., ResNet) but instead build a new type of basic building block that unifies MBConv, block, and grid attention layers. Normalization and activation layers are omitted for simplicity.}
\label{fig:maxt-architecture}
\end{figure}

\subsection{Attention}
Self-attention allows for spatial mixing of entire spatial (or sequence) locations while also benefiting from content-dependent weights based on normalized pairwise similarity. 
The standard self-attention defined in \cite{vaswani2017attention,dosovitskiy2020image} is location-unaware, \ie non-translation equivariant, an important inductive bias imbued in ConvNets.
Relative self-attention~\cite{liu2021swin,dai2021coatnet,shaw2018self,jiang2021transgan} has been proposed to improve on vanilla attention by introducing a relative learned bias added to the attention weights, which has been shown to consistently outperform original attention on many vision tasks~\cite{liu2021swin,dai2021coatnet,jiang2021transgan}.
In this work, we 
mainly adopt the pre-normalized relative self-attention defined in~\cite{dai2021coatnet} as the key operator in MaxViT.


\subsection{Multi-axis Attention}

Global interaction is one of the key advantages of self-attention as compared to local convolution.
However, directly applying attention along the entire space is computationally infeasible as the attention operator requires quadratic complexity.
To tackle this problem, we present a multi-axis approach to decompose the full-size attention into two sparse forms -- local and global -- by simply decomposing the spatial axes.
Let  be an input feature map. Instead of applying attention on the flattened spatial dimension , we {block} the feature into a tensor of shape , representing partitioning into non-overlapping windows, each of size .
Applying self-attention on the local spatial dimension~\emph{i.e.}, , is equivalent to attending within a small window~\cite{liu2021swin}.
We will use this \textbf{block attention} to conduct local interactions.

\begin{figure}[!t]
\centering
\includegraphics[width=0.96\linewidth]{figures/Multi_axis_attention.pdf}
\caption{\textbf{Multi-axis self-attention (Max-SA)} (best viewed in color). An illustration of the multi-axis approach for computing self-attention (window/grid size is 44). The block-attention module performs self-attention within windows, while the grid-attention module attends globally to pixels in a sparse, uniform grid overlaid on the entire 2D space, with both having linear complexity against input size, as we use fixed attention footage. The same colors are spatially mixed by the self-attention operation.}
\label{fig:MaxViT-block}
\end{figure}

Despite bypassing the notoriously heavy computation of full self-attention, local-attention models have been observed to underfit on huge-scale datasets~\cite{dai2021coatnet,dosovitskiy2020image}.
Inspired by block attention, we present a surprisingly simple but effective way to gain sparse global attention, which we call \textbf{grid attention}.
Instead of partitioning feature maps using fixed window size, we {grid} the tensor into the shape  using a fixed  uniform grid, resulting in windows having adaptive size .
Employing self-attention on the decomposed grid axis~\emph{i.e.}, , corresponds to dilated, global spatial mixing of tokens.
By using the same \textit{fixed} window and grid sizes (we use  following Swin~\cite{liu2021swin}), we can fully balance the computation between local and global operations, both having only linear complexity with respect to spatial size or sequence length.
Note that our proposed Max-SA module can be a drop-in replacement of the Swin attention module~\cite{liu2021swin} with exactly the same number of parameters and FLOPs. Yet it enjoys \textit{global interaction} capability without requiring masking, padding, or cyclic-shifting, making it more implementation friendly, preferable to the shifted window scheme~\cite{liu2021swin}.
For instance, the multi-axis attention can be easily implemented with \texttt{einops}~\cite{rogozhnikov2022einops} without modifying the original attention operation (see Appendix).
It is worth mentioning that our proposed multi-axis attention (Max-SA) is fundamentally different from the axial-attention models~\cite{wang2020axial,ho2019axial}.
Please see Appendix for a detailed comparison.















\noindent\textbf{MaxViT block.} We sequentially stack the two types of attentions to gain both local and global interactions in a single block, as shown in Figure~\ref{fig:MaxViT-block}.
Note that we also adopt typical designs in Transformers~\cite{dosovitskiy2020image,liu2021swin}, including LayerNorm~\cite{ba2016layer}, Feedforward networks (FFNs)~\cite{dosovitskiy2020image,liu2021swin}, and skip-connections.
We also add a MBConv block~\cite{howard2017mobilenets} with  squeeze-and-excitation (SE) module~\cite{hu2018squeeze} prior to the multi-axis attention, as we have observed that using MBConv together with attention further increases the generalization as well as the trainability of the network~\cite{xiao2021early}.
Using MBConv layers prior to attention offers another advantage, in that depthwise convolutions can be regarded as conditional position encoding (CPE)~\cite{chu2021conditional}, making our model free of explicit positional encoding layers.
Note that our proposed stand-alone multi-axis attention may be used together or in isolation for different purposes -- block attention for local interaction, and grid attention for global mixing.
These elements can be easily plugged into many vision architectures, especially on high-resolution tasks that can benefit by global interactions with affordable computation.


\subsection{Architecture Variants}
We designed a series of extremely simple architectural variants to explore the effectiveness of our proposed MaxViT block, as shown in Figure~\ref{fig:maxt-architecture}.
We use a hierarchical backbone similar to common ConvNet practices~\cite{he2016deep,liu2022convnet,dai2021coatnet,tan2021efficientnetv2} where the input is first downsampled using Conv3x3 layers in stem stage (S0).
The body of the network contains four stages (S1-S4), with each stage having half the resolution of the previous one with a doubled number of channels (hidden dimension).
In our network, we employ \textit{identical} MaxViT blocks throughout the entire backbone.
We apply downsampling in the Depthwise Conv3x3 layer of the first MBConv block in each stage.
The expansion and shrink rates for inverted bottleneck~\cite{howard2017mobilenets} and squeeze-excitation (SE)~\cite{hu2018squeeze} are 4 and 0.25 by default.
We set the attention head size to be 32 for all attention blocks. We scale up the model by increasing block numbers per stage  and the channel dimension .
We summarize the architectural configurations of the MaxViT variants in Table~\ref{tab:model-variants}.

\begin{table}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{1.2pt}
\renewcommand{\arraystretch}{1.0}
\caption{\normalsize \textbf{MaxViT architecture variants.} \texttt{B} and \texttt{C} denotes number of blocks and number of channels for each stage. We set each attention head to 32 for all attention layers. For MBConv, we always use expansion rate 4 and shrinkage rate 0.25 in SE~\cite{hu2018squeeze}, following~\cite{tan2019efficientnet,tan2021efficientnetv2,dai2021coatnet}. We use two Conv layers in the stem.}
\label{tab:model-variants}
\begin{tabular}{l|c|l|l|l|l|l}
{Stage} & {Size} & {MaxViT-T} & {MaxViT-S} & { MaxViT-B} & { MaxViT-L} & {MaxViT-XL} \\
\toprule
\texttt{S0}: Conv-stem &  & \texttt{B=2  C=64} & \texttt{B=2 C=64} &  \texttt{B=2 \space  C=64} & \texttt{B=2 \space C=128}  & \texttt{B=2 \space C=192} \\
\texttt{S1}: MaxViT-Block &  & \texttt{B=2  C=64} & \texttt{B=2 C=96} &  \texttt{B=2  \space  C=96} & \texttt{B=2 \space C=128}  & \texttt{B=2 \space C=192} \\
\texttt{S2}: MaxViT-Block &  & \texttt{B=2  C=128} & \texttt{B=2 C=192} &  \texttt{B=6  \space  C=192} & \texttt{B=6 \space C=256}  & \texttt{B=6 \space C=384} \\
\texttt{S3}: MaxViT-Block &  & \texttt{B=5  C=256} & \texttt{B=5 C=384} &  \texttt{B=14  C=384} & \texttt{B=14  C=512}  & \texttt{B=14  C=768} \\
\texttt{S4}: MaxViT-Block &  & \texttt{B=2  C=512} & \texttt{B=2 C=768} &  \texttt{B=2  \space  C=768} & \texttt{B=2 \space C=1024}  & \texttt{B=2 \space C=1536} \\
\end{tabular}
\end{table}




\section{Experiments}

We validated the efficacy of our proposed model on various vision tasks: ImageNet classification~\cite{krizhevsky2012imagenet}, image object detection and instance segmentation~\cite{lin2014microsoft}, image aesthetics/quality assessment~\cite{murray2012ava}, and unconditional image generation~\cite{goodfellow2014generative}. More experimental details can be found in the Appendix.


\begin{table*}[!t]
\centering
\footnotesize
\setlength{\tabcolsep}{3.6pt}
\renewcommand{\arraystretch}{0.9}
\caption{\normalsize \textbf{Performance comparison under ImageNet-1K setting.} Throughput is measured on a single V100 GPU with batch size 16, following~\cite{liu2021swin,tan2021efficientnetv2,liu2022convnet}.}
\label{tab:imagenet1k-comparison}
\begin{tabular}{c|l|ccccc}
&
\multirow{2}{*}{Model} & \multirow{2}{*}{\begin{tabular}{@{}c@{}}Eval\\size\end{tabular}} & \multirow{2}{*}{Params} & \multirow{2}{*}{FLOPs} & \multirow{2}{*}{\begin{tabular}{@{}c@{}}Throughput \
\label{eq:mbconv}
\mathbf{x}\leftarrow \mathbf{x}+ \mathsf{Proj}(\mathsf{SE}(\mathsf{DWConv}(\mathsf{Conv}(\mathsf{Norm}(\mathbf{x}))))),

\label{eq:mbconv-downsampling}
\mathbf{x}\leftarrow \mathsf{Proj}(\mathsf{Pool2D}(\mathbf{x}))+ \mathsf{Proj}(\mathsf{SE}(\mathsf{DWConv}\!\downarrow\!(\mathsf{Conv}(\mathsf{Norm}(\mathbf{x}))))).

\label{eq:relative-attention}
\mathsf{RelAttention}(Q,K,V)=\mathsf{softmax}(QK^T/\sqrt{d}+B)V,

\label{eq:block-operator}
\mathsf{Block}: (H,W,C)\rightarrow(\frac{H}{P}\times P,\frac{W}{P}\times P,C)\rightarrow (\frac{HW}{P^2},P^2,C).

\label{eq:grid-operator}
\mathsf{Grid}:(H,W,C)\rightarrow(G\times \frac{H}{G},G\times\frac{W}{G},C)\rightarrow \underbrace{(G^2,\frac{HW}{G^2},C)\rightarrow
(\frac{HW}{G^2},G^2,C)}_{\text{swapaxes(axis1=-2,axis2=-3)}}

\begin{split}
\label{eq:multi-axis-attention}
\mathbf{x}&\leftarrow \mathbf{x} + \mathsf{Unblock}(\mathsf{RelAttention}(\mathsf{Block}(\mathsf{LN}(\mathbf{x})))) \\ 
\mathbf{x}&\leftarrow\mathbf{x}+\mathsf{MLP}(\mathsf{LN}(\mathbf{x})) \\
\end{split}

\begin{split}
\mathbf{x}&\leftarrow \mathbf{x} + \mathsf{Ungrid}(\mathsf{RelAttention}(\mathsf{Grid}(\mathsf{LN}(\mathbf{x})))) \\ 
\mathbf{x}&\leftarrow\mathbf{x}+\mathsf{MLP}(\mathsf{LN}(\mathbf{x})) \\
\end{split}
out size)\\\end{tabular}} & \multirow{2}{*}{MaxViT-T} & \multirow{2}{*}{MaxViT-S} \\
& & &  \\\toprule

\multirow{1}{*}{stem} & \begin{tabular}{c}\56\times 56\left[\begin{array}{c}
\text{MBConv, 64, E 4, R 4} \\
\text{Rel-MSA, P 77, H 2} \\
\text{Rel-MSA, G 77, H 2} 
\end{array}\right]\times 2 \left[\begin{array}{c}
\text{MBConv, 96, E 4, R 4} \\
\text{Rel-MSA, P 77, H 3} \\
\text{Rel-MSA, G 77, H 3} 
\end{array}\right]\times 28\times)\end{tabular} & 
\times\times & 
\times\times 


\\ \hline
\multirow{1}{*}{S4} &
\begin{tabular}{c}\112\!\times\!112\times\times\times\times4\times)\end{tabular} 
& 
\times\times 
& 
\times\times


\\ \cline{1-4}

\multirow{1}{*}{S3} &
\begin{tabular}{c}\7\times 7\left[\begin{array}{c}
\text{MBConv, 768, E 4, R 4} \\
\text{Rel-MSA, P 77, H 24} \\
\text{Rel-MSA, G 77, H 24} 
\end{array}\right]\times 2\left[\begin{array}{c}
\text{MBConv, 1024, E 4, R 4} \\
\text{Rel-MSA, P 77, H 32} \\
\text{Rel-MSA, G 77, H 32} 
\end{array}\right]\times 21/81/161/32\textbf{p}\widehat{\textbf{p}}\mbox{CDF}_{\textbf{p}}(k)\sum_{i=1}^{k} \textbf{p}_{i}N=10r=2\mathsf{softmax}z\sim \mathcal{N}(\mathbf{0},\mathbf{I})8\times 8/0.2/0.3/0.4/0.60.3/0.4/0.60.4/0.5/0.90.0/0.0/0.00.1/0.2/0.2224\times224896\times8960.8, 0.3, 0.3224 \times 224384 \times 384512 \times 512\timesR1\gammaR_1\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\rightarrow\rightarrow\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\bullet\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\circ\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond\diamond128\times 128$.}
\label{fig:more-gan-results}
\end{figure}

\clearpage
\bibliographystyle{splncs04}
\bibliography{egbib}
\end{document}
