\section{Experiments}
\label{section-experiments}
\renewcommand{\thefootnote}{\alph{footnote}}

This section is dedicated to the evaluation of the \modelname{} for paragraph recognition. We show that the \modelacc{} reaches state-of-the-art results on each dataset. We study the need for pretraining on isolated text lines of the target dataset. We show that this can be avoided by using a pretrained \modelacc{} on another dataset. It enables the model to be trained on the target dataset with the paragraph-level ground truth only, without the need for line segmentation ground truth, which is a considerable practical advantage. We study the different stopping strategies on the IAM dataset. We also provide a visualization of the attention process.


\subsection{Comparison with state-of-the-art paragraph-level approaches}
\label{section-exp-pg-sota}
The results presented in this section are given for the \modelacc{}, with pretraining on line images and using the learned-stop strategy. The following comparisons are made with approaches under similar conditions, \textit{i.e.} without the use of external data (to model the language for example) and at paragraph level.

Comparative results with state-of-the-art approaches on the RIMES dataset are given in Table \ref{table:rimes-pg}. The \modelacc{} achieves better results on the test set compared to the other approaches with a CER of 1.91\% and a WER of 6.72\%. 

\begin{table}[!h]
    \caption{Recognition results  of the \modelacc{} and comparison with paragraph-level state-of-the-art approaches on the RIMES dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{threeparttable}[b]
        \begin{tabular}{ l c c c c c}
        \hline
        \multirow{2}{*}{Architecture} & CER (\%) & WER (\%) & CER (\%) & WER (\%) & \multirow{2}{*}{\# Param.}\\ 
        & valid & valid & test & test\\
        \hline
        \hline
        \cite{Bluche2016} CNN+MDLSTM\tnote{a} & 2.5 & 12.0 & 2.9 & 12.6 & \\
        \cite{Wigington2018} RPN+CNN+BLSTM+LM & & & 2.1 & 9.3 & \\ 
        Ours & 1.83 & 6.26 & \textbf{1.91} & \textbf{6.72} & 2.7 M \\
        \hline
        \end{tabular}
        \begin{tablenotes}
            \item[a] With line-level attention.
        \end{tablenotes}
        \end{threeparttable}
    }
    \label{table:rimes-pg}
\end{table}

Table \ref{table:iam-pg} shows the results compared to the state of the art on the IAM dataset. As one can see, once again the \modelacc{} also achieves new state-of-the-art results with a CER of 4.45\% and a WER of 14.55\% on the test set. One can notice that we use more than 6 times fewer parameters than in \cite{Yousef2020} with 2.7 M compared to 16.4 M.

\begin{table}[!h]
    \caption{Comparison of the \modelacc{} with the state-of-the-art approaches at paragraph level on the IAM dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    
    \begin{threeparttable}[b]
        \begin{tabular}{ l c c c c c}
        \hline
        \multirow{2}{*}{Architecture} & CER (\%) & WER (\%) & CER (\%) & WER (\%) & \multirow{2}{*}{\# Param.}\\ 
        & valid & valid & test & test\\
        \hline
        \hline
        \cite{Bluche2017} CNN+MDLSTM\tnote{a} & &  & 16.2 &  & \\
        \cite{Bluche2016} CNN+MDLSTM\tnote{b} & 4.9 & 17.1 & 7.9 & 24.6 & \\
        \cite{Carbonell2019} RPN+CNN+BLSTM\tnote{c} & 13.8 & & 15.6 & & \\ 
        \cite{Chung2020} RPN+CNN+BLSTM & & & 8.5 & &  \\
        \cite{Wigington2018} RPN+CNN+BLSTM+LM & & & 6.4 & 23.2 & \\
        \cite{Yousef2020} GFCN & & & 4.7 & & 16.4 M \\
        Ours & 3.02 & 10.34 & \textbf{4.45} & \textbf{14.55} & 2.7 M\\
        \hline
        \end{tabular}
        
        \begin{tablenotes}
            \item[a] With character-level attention.
            \item [b] With line-level attention.
            \item [c] Results are given for page level.
        \end{tablenotes}
        \end{threeparttable}
    }
    \label{table:iam-pg}
\end{table}


To our knowledge, there are no results reported in the literature on the READ 2016 dataset at paragraph or page level. The recognition results are presented in Table \ref{table:read2016-pg}. Our approach reaches a CER of 3.59\% and a WER of 13.94\%.
\begin{table}[!h]
    \caption{\modelacc{} results for the READ 2016 dataset at paragraph level.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l c c c c c}
    \hline
    \multirow{2}{*}{Architecture} & CER (\%) & WER (\%) & CER (\%) & WER (\%) & \multirow{2}{*}{\# Param}\\ 
    & validation & validation & test & test\\
    \hline
    \hline
    Ours & 3.71 & 15.47 & \textbf{3.59} & \textbf{13.94} & 2.7 M\\
    
    \hline
    \end{tabular}
    }
    \label{table:read2016-pg}
\end{table}





The proposed \modelname{} achieves new state-of-the-art results on these three different datasets. 
For fair comparison with the other competitive approaches of the literature we highlight some comparative features in Table \ref{table:sota-details}. In this table, the approaches are analyzed with regards to the following features: 1- use of an explicit text region segmentation process 2- minimum segmentation level used (whether it is for pretraining, data augmentation or training itself) 3- number of hyperparameters adapted from one dataset to another (except for the last decision layer which is dependent of the alphabet size) 4- use of curriculum learning 5- use of data augmentation. 
Curriculum learning, which consists in progressively increasing the number of lines in the input images, can be considered as data augmentation (crop technique). It is important to note that the use of line segmentation ground truth during training is costly due to the human effort involved to create them; this is even more costly for word segmentation. Data augmentation, for its part, does not require any human effort. 

The approach proposed in \cite{Yousef2020} is the only one that does not use any segmentation label neither at line nor at word level. However, it is also the only one that requires the architecture to be adapted to each dataset. More specifically, the height and width of the input and of two intermediate upsampling layers are tuned for each dataset. It means that those 6 hyperparameters must be tuned manually to reach the performance reported, which makes the architecture not generic at all. On the contrary, the VAN architecture remains the same for every dataset in all the experiments. Another important point is that the use of line-level segmentation ground truth is not inherent to our approach, as it was only introduced as a pretraining step. As we will see in the following section, pretraining the model at line level is not mandatory. Indeed, similar performance can be obtained with paragraph-level cross-dataset pretraining, without the need for line segmentation ground truth from the target dataset, as detailed in Section \ref{section-pretraining}.


\begin{table}[!h]
    \caption{Training details of state-of-the-art approaches.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l c c c c c }
    \hline
    \multirow{2}{*}{Approach} & Explicit & Min. segment. & Hyperparam. & Curriculum  & Data \\ 
    & segmentation & label used & adaptation & learning & augmentation \\
    \hline
    \cite{Carbonell2019} & \cmark & Word & \xmark & \xmark & \xmark\\
    \cite{Chung2020} & \cmark & Word & \xmark & \xmark & \cmark\\
    \cite{Wigington2018} & \cmark & Line & \xmark & \xmark & \cmark\\
    \cite{Yousef2020} & \xmark& Paragraph & 6 &\xmark  & ? \\
    \cite{Bluche2016} & \xmark & Line & \xmark & \cmark & \cmark \\
    \cite{Bluche2017} & \xmark& Line & \xmark & \cmark & \cmark \\
    Ours & \xmark & Line  & \xmark & \xmark & \cmark\\
    \hline
    \hline
    \end{tabular}
    }
    \label{table:sota-details}
\end{table}


\subsection{Impact of pretraining}
\label{section-pretraining}

In this section, we study the impact of pretraining on the \modelacc{}, using the learned-stop approach. The dataset used for pretraining is named source dataset, and the dataset on which we want to evaluate the model is named target dataset. We conducted three kinds of experiments. They are intended to evaluate the requirement of line-level segmentation labels for the target dataset to reach the best performance. 

A first training strategy consists in training the architecture at paragraph level directly on the target dataset, from scratch. 
A second strategy consists in training the architecture at paragraph level on the target dataset, with pretrained weights for the encoder and the convolutional layer of the decoder, as detailed in Section \ref{section-training-details}. In this case, pretraining is carried out on the isolated text line images of the target dataset, prior to train the \modelacc{} at paragraph level. Here, the source dataset and the target dataset are the same. This pretraining approach is referred to as line-level pretraining.
A third training strategy consists in training the architecture at paragraph level on the target dataset with weights that are initialized with those of another \modelacc{}. This other \modelacc{} is trained on a source dataset, with the second strategy. The idea is not to use any segmentation label from the target dataset. In this case the training strategy is referred to as cross-dataset pretraining.

Figure \ref{fig:curves-pretraining} shows the evolution of the CTC loss on the IAM dataset during training from scratch and training with line-level pretraining. We also highlight the impact of dropout when training from scratch. The recognition performance are given on the test set in Table \ref{table:va-pretrain}. As was expected, we can clearly notice that training the model from scratch is feasible, but it takes much time to converge and it comes at the cost of an important increase of the CER, from 4.45\% up to 7.06\%. When training from scratch, the use of dropout highly slows down the convergence but it leads to a lower CER of 7.06\% compared to 8.06\%. This experiment shows us that state-of-the art results are reached with line-level pretraining.


\begin{figure}[htbp!]
\centering
\includegraphics[width=0.7\linewidth]{img/va_pretraining_loss_new.png}
        \caption{CTC training loss curves comparison for the \modelacc{}, with and without pretraining, on the IAM dataset.}
        \label{fig:curves-pretraining}

\end{figure}


\begin{table}[!h]
    \caption{Impact of the pretraining on lines for the \modelacc. Results are given on the test set of the IAM dataset.}
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{ l c c c c }
    \hline
    Pretraining  & Dropout & CER (\%) & WER (\%) & Training time\\ 
    \hline
    \hline
    \xmark & \xmark & 8.06 & 25.38 & 1.39 d \\
    \xmark & \cmark & 7.06 & 22.65 & 1.77 d \\
    \cmark & \cmark & \textbf{4.45} & \textbf{14.55} & 0.63 d \\
    \hline
    \end{tabular}
    }
    \label{table:va-pretrain}
\end{table}



Table \ref{table:cross} reports the recognition performance of the third training strategy, using cross-dataset pretraining. In this table we report the performance of the VAN on the three datasets when pretrained on one of the two other datasets. One can notice that this pretraining strategy performs almost similarly as line-level pretraining for every datasets, but without using any segmentation label of the target dataset. This is especially true with the two similar datasets RIMES and IAM, for which the two pretraining strategies reach very similar CER: 1.97\% compared to 1.91\% for RIMES and 4.55\% compared to 4.45\% for IAM.  
Although cross-dataset pretraining is a bit less efficient on READ 2016, it still leads to competitive results. This may be explained by the differences between this dataset (RGB color encoding, historical manuscripts and language) and the other datasets (RIMES or IAM) on which pretraining is performed.

\begin{table}[!h]
    \caption{Comparison between cross-dataset pretraining and line-level pretraining for the \modelacc{}. Results are given on the test sets.}
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{ l c c c }
    \hline
     \multirow{2}{*}{Source dataset}& RIMES & IAM & READ 2016\\
     & CER (\%) & CER (\%) & CER (\%) \\
    \hline
    \hline
    \textbf{Cross-dataset pretraining}\\
    RIMES & \xmark & 4.55 & 4.08 \\
    IAM & 1.97 & \xmark & 4.14\\
    READ 2016 & 2.36 & 5.20 & \xmark \\
    \\
    \textbf{Line-level pretraining}\\
    Target dataset & \textbf{1.91} & \textbf{4.45} & \textbf{3.59}\\
    
\hline
    \end{tabular}
    }
    \label{table:cross}
\end{table}

These experiments demonstrate the importance of pretraining for the \modelacc{}. However, we show that we can alleviate the need for line-level segmentation label of the target dataset through cross-dataset pretraining. We assume that the important pretraining effect is mainly due to the attention mechanism. Indeed, the authors of \cite{Bluche2016,Bluche2017}, who also proposed attention-based models, used curriculum learning to tackle convergence issues. We assume this is due to the direct relation between recognition and implicit segmentation (through soft attention). When the encoder is pretrained, the \modelacc{} only has to learn the attention module and the decoder, as the pretrained features may contain all the information needed for the recognition of the characters. Considering random distribution of the attention weights over the vertical axis at first, training will progressively increases values of weights where the features correspond to the correct characters. But when the encoder is randomly initialized, the features extraction must also be learned, which slows the whole training. The use of dropout makes this phenomenon even worse, but it is essential in this architecture to avoid overfitting. It seems that cross-dataset pretraining skips this issue since the attention mechanism is already learned and the encoder only needs to be fine tuned.


Finally, we can notice that without introducing any pretraining, the VAN architecture is able to converge using the paragraph annotations only, but it is not competitive anymore compared to the state of the art.
As a preliminary conclusion, we can highlight the capacity of the \modelacc{} to achieve state-of-the-art performance without no need to adapt the architecture to each dataset considered. Moreover, cross-dataset pretraining allows to reach similar results compared to line-level pretraining, without the need to use any line-level segmentation ground truth from the target dataset.


\subsection{Learning when to stop}
\label{section-exp-stop}
We now compare the three stopping strategies mentioned in Section \ref{section-stop-process}, namely fixed-stop, early-stop and learned-stop approaches.

Performance evaluation of these three methods are given in Table \ref{table:va-stop} for comparison purpose. Line-level pretraining is used for each approach, as detailed in Section \ref{section-training-details}. 
We define $d_\mathrm{mean}$, as the average of the absolute values of the differences between the actual number of lines in the image $n_\mathrm{i}$ and the number of recognized lines $n_\mathrm{r}$. This metric is used to evaluate the efficiency of the early-stop and learned-stop approaches. For $K$ images in the dataset:

\begin{equation}
    d_\mathrm{mean} = \frac{1}{K}\displaystyle \sum_{k=1}^{K} |n_{\mathrm{i}_k}-n_{\mathrm{r}_k}|.
\end{equation}


As one can see, equivalent CER and WER are obtained for each stopping strategy. Moreover, the prediction time is not significantly impacted since data formatting, tensor initialization and encoder-related computations take much longer than the recurrent process, which is made up of only a few layers at each iteration.

\begin{table}[!h]
    \caption{Comparison between fixed-stop, early-stop and learned-stop approaches with the \modelacc{} on the test set of the IAM dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l c c c c c}
    \hline
    Stop method  & CER (\%) & WER (\%) & Train. time & Pred. time &$d_\mathrm{mean}$\\ 
    \hline
    \hline
    Fixed & \textbf{4.41} & 14.69 & 1.20 d & 33 ms & 20.32 \\
    Early & \textbf{4.41} & \textbf{14.39} & 0.41 d & 32 ms & 0.02 \\
    Learned & 4.45 & 14.55 & 0.63 d & 32 ms & 0.03 \\
    \hline
    \end{tabular}
    }
    \label{table:va-stop}
\end{table}

Figure \ref{fig:loss-stop} illustrates the evolution of the CTC loss for the three approaches. One should keep in mind that the comparison is biased since the approaches do not iterate the same number of times for a same example. However, one can clearly notice that the early-stop and learned-stop approaches  converge similarly, in contrast to the fixed-stop approach, which requires far more epochs to converge. But in the end, they reach almost identical CER.

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.7\linewidth]{img/va_stop_loss_new.png}
        \caption{CTC training loss curves comparison for the \modelacc{} for each stopping approach on the IAM dataset.}
        \label{fig:loss-stop}

\end{figure}


Figure \ref{fig:diff-stop} compares the $d_\mathrm{mean}$ for the early-stop and learned-stop approaches on the IAM validation dataset. For visibility, the fixed-stop approach curve, which is a plateau at $d_\mathrm{mean}=20.32$, is not depicted in this figure. The learned-stop approach leads to a faster convergence of $d_\mathrm{mean}$ compared to the early-stop approach, whose curve is less stable. It results in a $d_\mathrm{mean}$ of 0.03 for the learned-stop approach and 0.02 for the early-stop approach for the test set of IAM.

\begin{figure}[htbp!]
\centering
\includegraphics[width=0.7\linewidth]{img/va_stop_lines_new_focus.png}
        \caption{Comparison of the evolution of $d_\mathrm{mean}$ (the mean difference between the true and estimated number of lines in the image ) on the validation set of IAM dataset for the early-stop and learned-stop approaches.}
        \label{fig:diff-stop}
\end{figure}

With the learned-stop approach, the model successfully learns both tasks: it recognizes text lines with a state-of-the-art CER of 4.45\% and it determines when to stop with a high precision since the $d_\mathrm{mean}$ on the test set is only 0.03. It means that, on average, one line is missed or processed twice every 33 paragraph images.
We choose to keep this stopping strategy since it slightly improves the stability of the convergence through the epochs while achieving nearly identical results.

\subsection{Visualization of the vertical attention }
Figure \ref{fig:viz-rimes} shows the processing steps of an image from the RIMES validation set with a complex layout. Images from top to bottom represent the attention weights of the 5 attention iterations, each one recognizing a line. The intensity of the weights is encoded with the transparency of the red color. Given that attention weights are only computed along the vertical axis, the intensity is the same for every pixels at the same vertical position. Attention weights are rescaled to fit the original image height; indeed, attention weights are originally computed for the features height, which is 32 times smaller. The recognized text lines are given for each iteration, below the image. 

As one can see, the \modelacc{} has learned the reading order, from top to bottom. The attention weights clearly focus on text lines following this reading order. Attention weights focus mainly on one features line, with smaller weights for the adjacent vertical positions. 

One can notice that, sometimes, the focus is not perfectly centered on the text line. This may be due to rescaling, but this can also be a normal behavior due to the large size of the receptive field, which enables to manage slightly inclined lines. The second iteration shows this phenomenon very well with only one misrecognized character on an inclined line. However, processing inclined line is only possible when the lines, although inclined, do not share the same vertical position. The \modelacc{} cannot handle the case where lines would overlap vertically, because the attention weights would mix the two lines in this case.

Furthermore, one can notice that the attention is less sharp when the layout is more complex between two successive lines, as in the third image, but it does not disturb the recognition process however. 


\begin{figure}[h!]
\centering
    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\textwidth, height=3cm, frame]{img/visual/att_0.png}    
    \caption*{Je me permet  de vous écrire ca\textbf{s} je v\textbf{M}ux}
    \end{subfigure}

    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\textwidth, height=3cm, frame]{img/visual/att_1.png}    
     \caption*{augmente\textbf{s} mes quantités de CD vierges}
    \end{subfigure}
    
    \par\bigskip

    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\textwidth, height=3cm, frame]{img/visual/att_2.png}   
     \caption*{j'ai commandé 50 CD et a\textbf{d}uellement je voudrai}
    \end{subfigure}

    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\textwidth, height=3cm, frame]{img/visual/att_3.png}   
     \caption*{en commander 100.}
    \end{subfigure}
    
    \par\bigskip
    
    \begin{subfigure}[t]{0.35\textwidth}
    \includegraphics[width=\textwidth, height=3cm, frame]{img/visual/att_4.png}   
    \caption*{Merci d'avance}
    \end{subfigure}
    
    
   
    \caption{Attention weights visualization on a sample of the RIMES validation set. Recognized text is given for each line and errors are shown in bold.}
    \label{fig:viz-rimes}
\end{figure}

So far, we have provided strong results in favor of the \modelname{} by achieving state-of-the-art performance on three datasets. We also evaluated the need for pretraining and the efficiency of the stopping strategies we propose. 


\section{Additional experimental studies}
\label{section-additional-exp}

We now provide additional results which show the superiority of the \modelacc{} on many criteria compared with a standard two-step approach (line segmentation followed by recognition).
We also highlight the positive contribution of the \modelacc{} applied to paragraph images compared with the single line recognition approaches.
Finally, we highlight the positive effect of the proposed new dropout strategy compared to standard ones.

\subsection{Comparison with the standard two-step approach}
In this section, we compare the \modelacc{} with the standard two-step approach on the IAM dataset. In this respect, we introduce two new models: a first model performs line segmentation and a second one is in charge of OCR at line level. Both models are trained separately and they do not use any pretraining strategy since they are already working at line level.

The line segmentation model follows a U-net shape architecture and is based on our FCN encoder. Indeed, the features $f$ are successively upsampled to match the features maps shapes of CB\_5, CB\_4, CB\_3, CB\_2 and CB\_1 (Figure \ref{fig:encoder-overview}). This upsampling process is handled by Upsampling Blocks (UB). UB consists in DSC layer followed by DSC Transpose layer and instance normalization. This block also includes DMD layers. Each UB output is concatenated with the feature maps from its corresponding CB. A final convolutional layer output only two feature maps, to classify each pixel of the original image between text and background.


The OCR model for text line images is illustrated in Figure \ref{fig:line-ocr}. It is made up of the FCN encoder, followed by an AdaptiveMaxPooling layer, that pushes the vertical dimension to collapse. A final convolutional layer predicts the probability of the characters and of the CTC null symbol.

\begin{figure*}[htbp!]
\centering
\includegraphics[width=\textwidth]{img/LineOCR.pdf}
        \caption{Text line recognition architecture overview.}
        \label{fig:line-ocr}
\end{figure*}

 The line segmentation model is trained on the paragraph images at pixel level with ground truth of line bounding boxes. It is trained with the cross-entropy loss. The OCR is trained on the line-level images with the CTC loss. We used a mini-batch size of 8 for the segmentation task and of 16 for the OCR.

We now detail the two steps of this approach. In a first step, paragraphs are segmented into lines: 
\begin{itemize}
    \item Ground truth bounding boxes are modified in order to avoid overlaps: we divide their height by 2.
    \item A paragraph image is given as input of the network.
    \item A 2-class pixel segmentation (text or background) is output from the network.
    \item Adjacent text-labeled pixels are grouped to form connected components.
    \item Bounding boxes are created as the smallest rectangles containing each connected component; their height is multiplied by 2.
    \item The input image is cropped using those bounding boxes to generate lines.
\end{itemize}

In a second step, the OCR model, trained on the IAM dataset at line level, is applied on those cropped line images. The segmented lines are ordered by their vertical position (from top to bottom) and the recognized text lines are concatenated to compute the CER and the WER at paragraph level.

The performances of both tasks taken separately and together are shown in Table \ref{table:seg-reco}. As one can see, the results are good for both tasks separately: we get 81.51\% for the  IoU and 85.09\% for the mAP concerning the segmentation task; and a CER of 5.01\% and a WER of 16.49\% for the OCR. 
However, when we take the output of the segmentation as input for the OCR, it leads to a CER increase of 1.54 points. Indeed, the line segmentation errors induce recognition errors.

\begin{table}[!h]
    \caption{Results of the two-step approach on the test set of IAM.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l c c c c c}
    \hline
    Architecture & IoU (\%) & mAP (\%)& CER (\%) & WER (\%) & \# Param.\\ 
    \hline
    \hline
    Line seg. model & 81.51 & 85.09 & & & 1.8 M\\
    OCR on lines & & & 5.01 & 16.49 & 1.7 M \\
    Two-step approach & 81.51 & 85.09 & 6.55 & 18.54 & 1.8+1.7M\\
    \hline
    \end{tabular}
    }
    \label{table:seg-reco}
\end{table}


We can now compare the \modelname{} to the two-step approach. Comparison on the IAM test set is summarized in Table \ref{table:summary}. First, one can notice that the \modelacc{} reaches a better CER of 4.45\% compared to 6.55\%.
Prediction time is computed as the average prediction time, on the test set, to process a paragraph image. As one can see, even though the segmentation step is without recurrence, it requires much more time for prediction due to the formatting of the input required for the OCR, including the bounding boxes extraction from the original images. Moreover, it cumulates prediction times of the two models involved. 
Despite its recurrent process, the total prediction time for the \modelacc{} is shorter than that of the two-step approach since one iteration is very fast. In addition, it implies fewer parameters, this is notably due to the two models required by the two-step approach. Except for the training time, which is a bit higher for the \modelacc{}, it only provides advantages compared to the two-step approach.

\begin{table}[!h]
    \caption{Comparison of the two-step approach with the \modelname{}, results are given for the test set of the IAM dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l c c c c c }
    \hline
    \multirow{2}{*}{Architecture} & \multirow{2}{*}{CER (\%)} & \multirow{2}{*}{WER (\%)} & \multirow{2}{*}{\# Param.} & Training & Prediction \\ 
    & &  & & time & time \\
    \hline
    \hline
    Two-step & 6.55 & 18.54 & 1.8+1.7 M & 0.03+0.59 d & 749+28 ms\\
   VAN & \textbf{4.45} & \textbf{14.55} & 2.7 M & 0.59+0.63 d & 32 ms \\
    
    \hline
    \end{tabular}
    }
    \label{table:summary}
\end{table}

\subsection{Line level analysis}
\label{section-exp-line}

In this section, we compare the results of the \modelacc{} with the state-of-the-art approaches evaluated in similar conditions \textit{i.e.} at line level and without using external data. We also provide results for the line-level OCR model (Figure \ref{fig:line-ocr}), which has the same encoder, and for the \modelacc{} applied at paragraph level. The aim is to highlight the contribution of the \modelacc{} processing full paragraph images instead of isolated lines.

To have a fair comparison between the results obtained at paragraph and line level, one should consider the difference in the ground truth: the transcriptions of paragraph contain line breaks (or space characters in our case) between the different line transcriptions. Table \ref{table:linebreak} shows the \modelacc{} results difference at paragraph level when removing the interline characters from the metrics. As one can see, the removal of the interline character leads to an increase of the CER of 0.24, 0.09 and 0.24 on the test set of RIMES, IAM and READ 2016 respectively. The WER is not impacted by this modification. In the following tables of this section, we will use the results without considering interline characters for fair comparison with line-level approaches.


\begin{table}[!h]
    \caption{\modelacc{} results at paragraph level with and without interline characters in ground truth for RIMES, IAM and READ 2016 dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{threeparttable}[b]
        \begin{tabular}{ l c c c c c }
        \hline
        \multirow{2}{*}{Dataset} & Line  & CER (\%) & WER (\%) & CER (\%) & WER (\%) \\ 
        & break & valid & valid & test & test\\
        \hline
        \hline
        \multirow{2}{*}{RIMES} & \cmark & \textbf{1.83} & \multirow{2}{*}{6.26} & \textbf{1.91} & \multirow{2}{*}{6.72} \\
        & \xmark & 2.10 &  & 2.15 & \\
        \hline
        \multirow{2}{*}{IAM} & \cmark  & \textbf{3.02} & \multirow{2}{*}{10.34} & \textbf{4.45} & \multirow{2}{*}{14.55} \\
        & \xmark & 3.07 & & 4.54 & \\
        \hline
        \multirow{2}{*}{READ 2016} & \cmark  & \textbf{3.71} & \multirow{2}{*}{15.47} & \textbf{3.59} & \multirow{2}{*}{13.94} \\
        & \xmark & 4.01 & & 3.83 & \\
        \hline
        \end{tabular}
    \end{threeparttable}
    }
    \label{table:linebreak}
\end{table}


Table \ref{table:rimes-line} shows state-of-the-art results on the RIMES dataset at line level. We report competitive results with a CER of 3.04\% for the line-level model and 3.08\% for the \modelacc{} on the test set compared to the model of \cite{Puigcerver2017} which reached 2.3\%. On should notice that Puigcerver \textit{et al.} \cite{Puigcerver2017} does not use exactly the same dataset split for training and validation. In conclusion we can highlight the performance of the VAN obtained at paragraph level which achieves a CER of 2.15\% on the test set, which corresponds to decreasing the CER by 0.93 compared to processing isolated lines.
\begin{table}[!h]
    \caption{Comparison with the state of the art on the line-level RIMES dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{threeparttable}[b]
        \begin{tabular}{ l c c c c r}
        \hline
        \multirow{2}{*}{Architecture} & CER (\%) & WER (\%) & CER (\%) & WER (\%) & \multirow{2}{*}{\# Param.}\\ 
        & valid & valid & test & test\\
        \hline
        \hline
        \cite{Voigtlaender2016} 2D-LSTM+LM &  & & 2.8 & \textbf{9.6} & \\
        \cite{Puigcerver2017} CNN+BLSTM\tnote{a} & 2.2 & 9.6 & \textbf{2.3} & \textbf{9.6} & 9.6 M\\
        Ours (line-level model) & 2.20 & 6.26 & 3.04 & 8.32 & 1.7 M\\
        Ours (\modelacc{} on lines) & 1.97 & 6.09 & 3.08 & 8.14 & 2.7 M \\
        \hline
        Ours (\modelacc{} on paragraphs) & 2.10 & 6.26 & \textbf{2.15} & \textbf{6.72} & 2.7 M \\
        \hline
        \end{tabular}
        
        \begin{tablenotes}
        \item [a] This work uses a slightly different split (10,203 for training, 1,130 for validation and 778 for test).
        \end{tablenotes}
    \end{threeparttable}
    
    }
    \label{table:rimes-line}
\end{table}

Comparison with state-of-the-art results on the IAM dataset is presented in Table \ref{table:iam-line}. We reach competitive results with a CER of 4.97\% on the test set. Models proposed in \cite{Yousef_line} and \cite{Michael2019} reach similar results but the former implies a large number of parameters compared to ours and the latter is more complex including a recurrent process with attention at character level. It should be noticed however that, in \cite{Puigcerver2017,Michael2019,Yousef_line}, the authors use a slightly different split from ours. As a matter of fact, since the \modelacc{} training implies pretraining at line level, it was not possible to use the same split since some lines for training and validation are extracted from the same paragraph image for example. On the IAM dataset, the VAN also reaches a better CER at paragraph level than at line level with 4.54\% compared to 4.97\%.
\begin{table}[!h]
    \caption{Comparison with the state of the art at the line level on the IAM dataset.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{threeparttable}[b]
        \begin{tabular}{ l c c c c r}
        \hline
        \multirow{2}{*}{Architecture} & CER (\%) & WER (\%) & CER (\%) & WER (\%) & \multirow{2}{*}{\# Param.}\\ 
        & validation & validation & test & test\\
        \hline
        \hline 
        \cite{Voigtlaender2016} CNN+MDLSTM+LM & \textbf{2.4} & \textbf{7.1} & \textbf{3.5} & \textbf{9.3} & 2.6 M \\
        \cite{Puigcerver2017} CNN+BLSTM\tnote{a}  & 3.8 & 13.5 & 5.8 & 18.4 & 9.3 M\\
        \cite{Yousef_line} GFCN\tnote{a} & 3.3 & & 4.9 & & > 10 M\\
        \cite{Michael2019} Seq2seq \tiny{(CNN+BLSTM)}\tnote{a} & & & 4.87 & & \\
        Ours (line-level model) & 3.37 & 11.52 & 5.01 & 16.49 & 1.7 M\\
        Ours (\modelacc{} on lines) & 3.15 & 10.77 & 4.97 & 16.31 & 2.7 M\\
        \hline
        Ours (\modelacc{} on paragraphs) & 3.07 & 10.34 & 4.54 & 14.55 & 2.7 M\\
        \hline
        \end{tabular}
     
        \begin{tablenotes}
        \item [a] These works use a slightly different split (6,161 for training, 966 for validation and 2,915 for test).
        \end{tablenotes}
        
    \end{threeparttable}
    }       
    \label{table:iam-line}
\end{table}

The results on the READ 2016 dataset are gathered in Table \ref{table:read2016-line}. We reached state-of-the-art CER on the test set with 4.10\% compared to 4.66\% for \cite{Michael2019}. Again, the paragraph-level \modelacc{} reaches better results than the \modelacc{} applied at line level with a CER of 3.83\%.

\begin{table}[!h]
    \caption{Comparison with the state-of-the-art line-level recognizers on READ 2016 dataset.}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{threeparttable}[b]
            \begin{tabular}{ l c c c c r}
            \hline
            \multirow{2}{*}{Architecture} & CER (\%) & WER (\%) & CER (\%) & WER (\%) & \multirow{2}{*}{\# Param.}\\ 
            & validation & validation & test & test\\
            \hline
            \hline
            \cite{Michael2019} Seq2seq (\tiny{CNN+BLSTM}) & & & 4.66 & & \\
            \cite{READ2016}\tnote{a}\: CNN+MDLSTM+LM & & & 4.8 & 20.9\\
            \cite{READ2016}\tnote{b}\: CNN+RNN & & & 5.1 & 21.1\\
            Ours (line-level model) & 4.49 & 18.22 & 4.25 & 17.14 & 1.7 M\\
            Ours (\modelacc{} on lines) & 4.42 & 18.17 & \textbf{4.10} & \textbf{16.29} & 2.7 M\\
            \hline
            Ours (\modelacc{} on paragraphs) & 4.01 & 15.47 & \textbf{3.83} & \textbf{13.94} & 2.7 M\\
            \hline
            \end{tabular}
            
            \begin{tablenotes}
                \item [a] results from RWTH.
                \item [b] results from BYU.
            \end{tablenotes}
        \end{threeparttable}
    }
    \label{table:read2016-line}
\end{table}


In conclusion, one can notice that the \modelacc{}, applied on isolated lines, performs at least similarly as the line-level model, for each dataset (except for RIMES with a small CER increase of 0.04 points). It also achieves state-of-the art results at line level on the READ dataset. 

The results also highlight the superiority of the \modelname{} on the RIMES, IAM and READ 2016 datasets, applied to whole paragraph images, compared to isolated lines. Multiple factors can explain this result: segmentation ground truth annotations of text lines are prone to variations from one annotator to another, bringing variability that is not present when dealing with paragraph images directly. Indeed, the model implicitly learns to segment the lines so it does not have to adapt to pre-formatted lines; it uses more context (with a large receptive field) and uses it to focus on the useful information for the recognition purpose. 

Moreover, the \modelacc{} decoder contains a LSTM layer that may have a positive impact acting as a language model without any loss of context when moving from one line to the next, when producing the output character sequence.

A key element to reach such results with a deep network is to use efficient regularization strategies. We discuss the new dropout strategy we propose in the following paragraph.

\subsection{Dropout strategy}
We use dropout to regulate the network training and thus avoid over-fitting. We defined Diffused Mix Dropout (DMD) in Section \ref{section-archi-dropout} to improve the results of the model. We carried out some experiments to highlight the contribution of DMD over commonly used standard and 2d dropout layers. Experiments are performed with the line-level model and the \modelacc{} on the IAM dataset; \modelacc{} is pretrained with weights from the corresponding line-level model. Results for the test set are shown in Table \ref{table:dropout}. The columns from left to right correspond respectively to the number of dropout layers per block (CB and DSCB), the type of dropout layer used (mix, standard or spatial), the associated dropout probabilities, the use of the diffuse option (using only one or all dropout layers per block) and the CER and WER for both models.

\label{section-exp-dropout}
\begin{table}[!h]
    \caption{Dropout strategy analysis. Results are given for the IAM test set.}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ l c c c c c c c r}
    \hline
    & \multirow{2}{*}{\#} & \multirow{2}{*}{type} & \multirow{2}{*}{p} &\multirow{2}{*}{diffused} &  \multicolumn{2}{c}{line-level model} & \multicolumn{2}{c}{\modelacc{}} \\ 
    & & & & & CER (\%) & WER (\%) & CER (\%) & WER (\%)\\
    \hline
    \hline
    Baseline & 3 & mix & 0.5/0.25 &\cmark & \textbf{5.01} & \textbf{16.49} & 4.45 & \textbf{14.55}\\
    (1) & 3 & std. & 0.5 &\cmark & 5.24 & 17.23 & 4.46 & 14.88 \\
    (2) & 3 & 2d & 0.25 & \cmark & 5.38 & 17.64 & 4.72 & 15.63 \\
    (3) & 1 & mix & 0.5/0.25 & \xmark & 5.33 & 17.70 & \textbf{4.43} & 15.25\\
    (4) & 1 & std. & 0.5 & \xmark & 5.56 & 18.40 & 4.78 & 15.91\\
    (5) & 1 & 2d & 0.25 & \xmark & 5.70 & 18.92 & 4.93 & 16.80\\
    (6) & 3 & mix & 0.5/0.25 & \xmark & 6.76 & 21.13 & 6.32 & 19.73\\
    (7) & 3 & mix & 0.16/0.08 & \xmark & 6.71 & 20.91 & 4.64 & 15.36 \\
    (8) & 3 & mix & 0.16/0.08 & \cmark & 7.51 & 23.60 & 5.57 & 18.50\\
    
    \hline
    \end{tabular}
    }
    \label{table:dropout}
\end{table}

In (1) and (2), Mix Dropout layers are respectively replaced by standard and 2d dropout layers, preserving their corresponding dropout probability. Using Mix Dropout leads to an improvement of 0.23 points of CER compared to standard dropout and of 0.37 compared to 2d dropout for the line-level model. These improvements are lower for the \modelacc{} with 0.01 and 0.27 points.

In (3), only one Mix Dropout is used, after the first convolution of the blocks, leading to a higher CER than the baseline, with a difference of 0.34 points for the line-level model. The CER is decreased by 0.02 points for the \modelacc{} but the WER is increased by 0.70 points.
In (4) and (5), we are in the same configuration as (3) \textit{i.e.} with only one dropout layer per block. MixDropout is superseded by standard dropout in (4) and by 2d dropout in (5) resulting in an increase of the CER compared to (3). This shows the positive impact of Mix Dropout layers in another configuration.

In (6) and (7), Mix Dropout layers are set at each of the three positions \textit{i.e.} they are all used at each execution, contrary to the baseline, which uses only one dropout layer per execution. While (6) keeps the same dropout probabilities, (7) divides them by 3. In both cases, the associated CER are higher than the baseline.

Finally, in (8), we are in the same context than the baseline, but dropout probabilities are divided by 3, leading to higher CER.

We can conclude that our dropout strategy leads to a CER improvement of 0.55 points for the line-level model and of 0.33 for the \modelacc{}, when compared to (4) and (5) that do not use Mix Dropout or diffuse option. 



\section{Discussion}
\label{section-discussion}

As we have seen, the \modelacc{} achieves state-of-the-art results on multiple datasets at paragraph level. However, there is one point that should be notice. Modern deep neural systems involve many training strategies (hyperparameters, optimizer, regularization strategies, pre-processings, data augmentation techniques, transfer learning, curriculum learning, and many others). This makes the comparison between architectures very difficult as some training tricks are more suited for some architectures than some others. This is why one should be convinced that the state-of-the-art results obtained in this paper are due to the whole proposition, including training strategies, and not only to the \modelacc{} architecture. However, we have provided experimental results that show the interest of the proposed training strategies of the generic \modelacc{} architecture.

We compared different stopping strategies and showed that the \modelacc{} can learn to detect the end of paragraph. This additional task has no significant impact on the performance and slightly improves the stability through training. We also compared favorably the \modelacc{} to a standard two-step approach and showed the positive impact of processing paragraph-level images compared with line-level ones, for this architecture. The new dropout strategy we propose enabled to reach even better results.

Moreover, the \modelacc{} has multiple advantages. The \modelacc{} is robust: whether it is at paragraph or line level, and no matter the dataset used, we did not adjust any hyperparameter for each dataset. The \modelacc{} takes input of variable sizes, so it could handle whole page images without any modification. As mentioned previously, the \modelacc{} can handle slightly inclined lines. However, it is limited to layouts in which there is no overlap between lines on their horizontal projection. Indeed, this case remains to be solved. A standard n-gram language model could process the outputs of the \modelacc{} architecture but its impact on the performance remains to be determined through experiments. 


However, there is still room for improvement.  Notably, we showed that the \modelacc{} needs pretraining on isolated text lines of the target dataset to reach state-of-the-art results. But the need for line-level annotations is not inherent to the \modelacc{}, this is only related to this pretraining step. As a matter of fact, we demonstrate that pretraining on another dataset (cross-dataset pretraining) can alleviate this issue, even if the datasets are really different. Indeed, for the three datasets, cross-dataset pretraining leads to results similar to those from line-level pretraining, but without using any line-level annotation for the target dataset

The \modelacc{} should be considered to process single-column text document only. As a matter of fact, as it is the case for \cite{Bluche2016} and supposedly \cite{Yousef2020}, the models are designed and limited to process single-column multi-line text documents with relatively horizontal text lines. The next step would be to focus on processing images with more complex layout such as multi-column text images.
