\documentclass{article}








\usepackage[preprint]{neurips_2023}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[table,xcdraw]{xcolor}
\usepackage{arydshln} \usepackage{multirow}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{lipsum}		\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfig}
\usepackage{wrapfig}

\usepackage{natbib}
\setcitestyle{numbers,square}
\usepackage{doi}
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\zy}[1]{\textcolor{red}{ziyan: #1}}
\newcommand{\xq}[1]{\textcolor{blue}{xinqi: #1}}

\definecolor{mygray}{gray}{0.95}
\floatname{algorithm}{\scriptsize Algorithm}


\title{DiffBIR: Towards Blind Image Restoration with
Generative Diffusion Prior}











\author{Xinqi Lin\thanks{Equal contribution} \quad Jingwen He \quad Ziyan Chen  \quad Zhaoyang Lyu  \quad Ben Fei 
 \, Bo Dai \\ \textbf{Wanli Ouyang}  \quad \textbf{Yu Qiao}  \quad \textbf{Chao Dong}\thanks{Corresponding author}\\
\textsuperscript{1}Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\\ \textsuperscript{2}Shanghai AI Laboratory 
}



\begin{document}

\maketitle

\begin{abstract}
We present DiffBIR, which leverages pretrained text-to-image diffusion models for blind image restoration problem. Our framework adopts a two-stage pipeline. In the first stage, we pretrain a restoration module across diversified degradations to improve generalization capability in real-world scenarios. The second stage leverages the generative ability of latent diffusion models, to achieve realistic image restoration. 
Specifically, we introduce an injective modulation sub-network -- LAControlNet for finetuning,
while the pre-trained Stable Diffusion is to maintain its generative ability. 
Finally, we introduce a controllable module that allows users to balance quality and fidelity by introducing the latent image guidance in the denoising process during inference.
Extensive experiments have demonstrated its superiority over state-of-the-art approaches for both blind image super-resolution and blind face restoration tasks on synthetic and real-world datasets. The code is available at \url{https://github.com/XPixelGroup/DiffBIR}.
\end{abstract}

\section{Introduction}
Image restoration aims at reconstructing a high-quality image from its low-quality observation. Typical image restoration problems, such as image denoising, deblurring and super-resolution, are usually defined under a constrained setting, where the degradation process is simple and known (\textit{e.g.}, Gaussian noise and Bicubic downsampling). They have successfully promoted a vast number of excellent restoration algorithms \cite{srcnn, dncnn, swinir, IPT, uformer, restormer, hat}, but are born to have limited generalization ability. To deal with real-world degraded images, blind image restoration (BIR) comes into view and becomes a promising direction. The ultimate goal of BIR is to realize realistic image reconstruction on general images with general degradations. BIR does not only extend the boundary of classic image restoration tasks, but also has a wide practical application field (\textit{e.g.}, old photo/film restoration).

\begin{figure}[t]
\centering
\subfloat[\small Visual comparison of blind image super-resolution (BSR) methods on real-world low-quality images.]{\includegraphics[width=1.0\textwidth]{figures/figure1_a.pdf}}\hfill
\subfloat[Visual comparison of blind face restoration (BFR) methods on real-world low-quality face images.]{\includegraphics[width=1.0\textwidth]{figures/figure1_b.pdf}}\hfill
\caption{\small Comparisons of DiffBIR and state-of-the-art BSR/BFR methods on real-world images.  Compared to BSR methods, DiffBIR is more effective to 1) generate natural textures; 2) reconstruct semantic regions; 3) not erase small details; 4) overcome severe cases. Compared to BFR methods, DiffBIR can 1) handle occlusion cases; 2) obtain satisfactory restoration beyond facial areas (\textit{e.g.}, headwear, earrings). (\textbf{Zoom in for best view})} \label{fig:teaser} \vspace{-1em}
\end{figure}


The research of BIR is still in its primary stage, thus requiring more explanations of its current state. According to the problem settings, existing BIR methods can be roughly grouped into three research topics, namely blind image super-resolution (BSR), zero-shot image restoration (ZIR) and blind face restoration (BFR). They all have achieved remarkable progress, but also have apparent limitations. BSR is initially proposed to solve real-world super-resolution problems, where the low-resolution image contains unknown degradations. According to the recent BSR survey \cite{bsr_survey}, the most popular solutions may be BSRGAN \cite{bsrgan} and Real-ESRGAN \cite{realesrgan}. They formulate BSR as a supervised large-scale degradation overfitting problem. To simulate real-world degradations, a degradation shuffle strategy and high-order degradation modeling are proposed separately. Then the adversarial loss \cite{srgan, gans, esrgan, sngan, unetgan} is incorporated for learning the reconstruction process in an end-to-end manner. They have indeed removed most degradations on general images, but cannot generate realistic details. Furthermore, their degradation settings are limited to  super-resolution, which is not complete for the BIR problem. The second group ZIR is a newly emerged direction. Representative works are DDRM \cite{ddrm}, DDNM \cite{ddnm}, and GDP \cite{gdp}. They incorporate the powerful diffusion model as the additional prior, thus having greater generative ability than GAN-base methods. With a proper degradation assumption, they can achieve impressive zero-shot restoration on classic IR tasks. However, the problem setting of ZIR is not in accordance with BIR. Their methods can only deal with clearly defined degradations (linear or non-linear), but cannot generalize well to unknown degradations. In other words, they can achieve realistic reconstruction on general images, but not on general degradations. The third group is BFR, which focuses on human face restoration. State-of-the-art methods can refer to CodeFormer \cite{codeformer} and VQFR \cite{vqfr}. They have a similar solution pipeline as BSR methods, but are different in the degradation model and generation network. Due to a smaller image space, these methods can utilize VQGAN and Transformer to achieve surprisingly good results on real-world face images. Nevertheless, BFR is only a sub-domain of BIR. It usually assumes a fixed input size and restricted image space, thus cannot be applied to general images. According to the above analysis, we can see that existing BIR methods cannot achieve (1) realistic image reconstruction on (2) general images with (3) general degradations, simultaneously. Therefore, we desire a new BIR method to overcome these limitations. 


In this work, we propose DiffBIR to integrate the advantages of previous works into a unified framework. Specifically, DiffBIR (1) adopts an expanded degradation model that can generalize to real-world degradations, (2) utilizes the well-trained Stable Diffusion as the prior to improve generative ability, (3) introduces a two-stage solution pipeline to ensure both realness and fidelity. We also make dedicated designs to realize these strategies. First, to increase generalization ability, we combine the diverse degradation types in BSR and the wide degradation ranges in BFR to formulate a more practical degradation model. This helps DiffBIR to handle diverse and extreme degradation cases. Second, to leverage Stable Diffusion, we introduce an injective modulation sub-network -- LAControlNet that can be optimized for our specific task. Similar to ZIR, the pre-trained Stable Diffusion is fixed during finetuning to maintain its generative ability. Third, to realize faithful and realistic image reconstruction, we first apply a Restoration Module (\textit{i.e.}, SwinIR) to reduce most degradations, and then finetune the Generation Module (\textit{i.e.}, LAControlNet) to generate new textures. Without this pipeline, the model may either produce over-smoothed results (remove Generation Module) or generate wrong details (remove Restoration Module). In addition, to meet users' diverse requirements, we further propose a controllable module that could achieve continuous transition effects between restoration result in stage one and generation result in stage two. This is achieved by introducing the latent image guidance during the denoising process without re-training. The gradient scale that applies to the latent image distance can be tuned to trade off realness and fidelity. 

Equipped with the above components, the proposed DiffBIR demonstrates excellent performance in both BSR and BFR tasks on synthetic and real-world datasets. It is worth noting that DiffBIR achieves a great performance leap in general image restoration, outperforming existing BSR and BFR methods (\textit{e.g.}, BSRGAN \cite{bsrgan}, Real-ESRGAN \cite{realesrgan}, CodeFormer \cite{codeformer}, et.al). We can observe the differences of these methods in some aspects. For complex textures, BSR methods tend to generate unrealistic details, while DiffBIR can produce visually  pleasant results, see Figure \ref{fig:teaser}(first row). For semantic regions, BSR methods tend to achieve over-smoothed effects, while DiffBIR can reconstruct semantic details, see Figure \ref{fig:teaser}(second row). For tiny stripes, BSR methods tend to erase those details, while DiffBIR can still enhance their structures, see Figure \ref{fig:teaser}(third row). 
Moreover, DiffBIR is able to deal with extreme degradations and regenerate realistic and vivid semantic content, see Figure \ref{fig:teaser}(the last row).
All these show that DiffBIR has successfully broken the bottlenecks of existing BSR methods. For blind face restoration, DiffBIR shows superiority in dealing with some hard cases, such as maintaining good fidelity on facial area occluded by other objects (see first row in \ref{fig:teaser} (b)), achieving successful restoration beyond facial areas (see first row in \ref{fig:teaser} (b)). In conclusion, our DiffBIR could obtain competitive performance for both BSR and BFR tasks in a unified framework for the first time. Extensive and intensive experiments have demonstrated the superiority of our proposed DiffBIR over the existing state-of-the-art BSR and BFR methods.


















\section{Related Work}
\noindent\textbf{Blind Image Super-Resolution.}
Latest advances \cite{bsr_survey} on BSR have explored more complex degradation models to approximate real-world degradations. In particular, BSRGAN~\cite{bsrgan} aims to synthesize more practical degradations based on a random shuffling strategy, and Real-ESRGAN~\cite{realesrgan} exploits "high-order" degradation modeling. They both utilize GANs \cite{gans, sngan, unetgan, srgan, esrgan} to learn the image reconstruction process under complex degradations. SwinIR-GAN~\cite{swinir} uses the new prevailing backbone Swin Transformer~\cite{swin_transformer} to achieve better image restoration performance. FeMaSR~\cite{femasr} formulates SR as a feature-matching problem based on pre-trained VQ-GAN~\cite{vqgan}.
Although BSR methods can be useful to remove degradations in the real world, they are not good at generating realistic details. In addition, they typically assume the low-quality image input is downsampled by some certain scales (e.g. ), which is limited for BIR problem.



\noindent\textbf{Zero-shot Image Restoration.}
ZIR aims to achieve image restoration by leveraging a pre-trained prior network in an unsupervised manner. Earlier works \cite{zero_shot_gan_based1, zero_shot_gan_based2, pulse, dgp} mainly concentrate on searching a latent code within a pre-trained GAN's latent space. Recent advancements in this field embrace the utilization of Denoising Diffusion Probabilistic Models \cite{ddpm, songyang1, songyang2, sd, dalle2, imagen}. DDRM \cite{ddrm} introduces an SVD-based approach to handle linear image restoration tasks efficiently. Meanwhile, DDNM \cite{ddnm} analyzes the range-null space decomposition of a vector theoretically and then designs a sampling schedule based on the null space. Inspired by classifier guidance \cite{beatsgan}, GDP \cite{gdp} introduces a more convenient and effective guidance approach, in which the degradation model can be estimated during inference. Although these works contribute to the advancement of zero-shot image restoration techniques, ZIR methods still cannot achieve satisfactory restoration results in low-quality images from real world.

\noindent\textbf{Blind Face Restoration.}
As a specific sub-domain of general images, the face image typically carries more structural and semantic information. Early attempts utilize geometric priors (e.g. facial parsing maps~\cite{face_geo_prior_1}, facial landmarks\cite{fsrnet, face_geo_prior_2}, and facial component heatmaps~\cite{yu2018face}) or reference priors\cite{XiaomingLi2018LearningWG, XiaomingLi2020EnhancedBF, XiaomingLi2020BlindFR, BerkDogan2019ExemplarGF} as auxiliary information to guide the face restoration process. With the rapid development of generative networks, many BFR approaches incorporate powerful generative-prior to reconstruct faces in great realness. Representative GAN-prior-based methods~\cite{gfpgan, gpen, gcfsr, glean} have demonstrated their capability in achieving both high-quality and high-fidelity face reconstruction. State-of-the-art works~\cite{codeformer, vqfr, restoreformer} introduce the HQ codebook to generate surprisingly realistic face details by exploiting Vector-Quantized (VQ) dictionary learning~\cite{vqvae, vqgan}. 


\section{Methodology}
\label{method}
In this work, we aim to exploit a powerful generative prior -- Stable Diffusion to solve blind restoration problems for both general and face images. Our proposed framework adopts a two-stage pipeline that is effective, robust, and flexible.
First, we employ a Restoration Module to remove corruptions, such as noises or distortion artifacts, using regression loss.
As the lost local textures and coarse/fine details are still absent, we then leverage Stable Diffusion to remedy the information loss.
The overall framework is illustrated in Figure \ref{fig:architecture}. Specifically, we first pretrain a SwinIR \cite{swinir} on large-scale dataset to achieve the preliminary degradation removal across diversified degradations (Section \ref{sec:method1}).  
Then, the generative prior is leveraged for producing realistic restoration results (Section \ref{sec:method2}).
In addition, a controllable module based on latent image guidance is introduced for trade-off between \textit{realness} and \textit{fidelity}  (Section \ref{sec:method3}). 

\begin{figure}[h]
\centering
    \includegraphics[scale=0.5]{figures/final_arch_crop.pdf}
\caption{\small The two-stage pipeline of DiffBIR: 1) pretrain a Restoration Module (RM) for degradation removal to obtain ; 2) leverage fixed Stable Diffusion through our proposed LAControNet for realistic image reconstruction and obtain . RM is trained across diversified degradations in a self-supervised manner, and is fixed during stage-two. LAControlNet contains a parallel module that is partially initialized with the denoiser's checkpoint and has several fusion layers. It uses VAE's encoder to project the  to the latent space, and performs concatenation with the randomly sampled noisy  as the conditioning mechanism.}
    \label{fig:architecture} \vspace{-1em}
\end{figure}


\subsection{Pretraining for Degradation Removal}
\label{sec:method1}
\noindent\textbf{Degradation Model.} 
BIR aims to restore clean images from low-quality (LQ) ones with unknown and complex degradations. Typically, blur, noise, compression artifacts, and low-resolution are often involved.
In order to better cover the degradation space of the LQ images, we employ a comprehensive degradation model that considers \textit{diversified degradation} and \textit{high-order degradation}.
Among all degradations, \textbf{blur}, \textbf{resize}, and \textbf{noise} are the three key factors in real-world scenarios \cite{bsrgan}. Our \textit{diversified degradation} involves \textbf{blur}: isotropic Gaussian and anisotropic Gaussian kernels; \textbf{resize}: area resize, bilinear interpolation and bicubic resize; \textbf{noise}: additive Gaussian noise, Poisson noise, and \texttt{JPEG} compression noise. Regarding \textit{high-order degradation}, we follow \cite{realesrgan} to use the second-order degradation, which repeats the classical degradation model: \textbf{blur}-\textbf{resize}-\textbf{noise} process twice.
Note that our degradation model is designed for image restoration, thus all the degraded images will be resized back to their original size.




\noindent\textbf{Restoration Module.} 
To build a robust generative image restoration pipeline, we adopt a conservative yet feasible solution by first removing most of the degradations (especially the noise and compression artifacts) in the LQ images, 
and then use the subsequent generative module to reproduce the lost information. 
This design will promote the latent diffusion model to focus more on textures/details generation without the distraction of noise corruption, and achieve more realistic/sharp results without wrong details (see Section \ref{sec:ablation1}). 
We modify SwinIR \cite{swinir} as our restoration module. Specifically, we utilize the pixel unshuffle \cite{pixelshuffle} operation to downsample the original low-quality input  with a scale factor of 8. Then, a  convolutional layer is adopted for shallow feature extraction. All the subsequent transformer operations are performed in low resolution space, which is similar to latent diffusion model. The deep feature extraction adopts several Residual Swin Transformer Blocks (RSTB), and each RSTB has several Swin Transformer Layers (STL). The shallow and deep features will be added for maintaining both low-frequency and high-frequency information. For upsampling the deep features back to the original image space, we perform nearest interpolation for three times, and each interpolation is followed by one convolutional layer as well as one Leaky ReLU activation layer. 
We optimize the parameters of the restoration module by minimizing the  pixel loss. The formulation is as follows: 

where  and  denote the high-quality image and the low-quality counterpart, respectively.  is obtained by regression learning and will be used for the finetuning on latent diffusion model.

\subsection{Leverage Generative Prior for Image Reconstruction}
\label{sec:method2}
\noindent\textbf{Preliminary: Stable Diffusion.}
In this paper, we implement our method based on the large-scale text-to-image latent diffusion model -- Stable Diffusion.
Diffusion models learn to generate data samples
through a denoising sequence that estimate
the score of the data distribution. 
In order to achieve better efficiency and stabilized training, Stable Diffusion pretrains an autoencoder \cite{kingma2013auto} that converts an image  into a latent  with encoder  and reconstructs it with decoder . 
This latent representation is learned by using hybrid objectives of VAE \cite{vae}, Patch-GAN \cite{pix2pix}, and LPIPS \cite{lpips}.
The diffusion and denoising processes are performed in the latent space.
In diffusion process, Gaussian noise with variance  at time  is added to the encoded latent  for producing the noisy latent:

where , 
 and . When  is large enough, the latent  is nearly a
standard Gaussian distribution.

A network  is learned by predicting the noise  conditioned on  (\textit{i.e.}, text prompts) at a randomly picked time-step . The optimization of latent diffusion model is defined as follows:

where  are sampled from the dataset and ,  is uniformly sampled and  is sampled from the standard Gaussian distribution.

\noindent\textbf{LAControlNet.}
Although stage-one could remove most degradations, the obtained  is often over-smoothed and still far from the distribution of high-quality natural images.
We then leverage the pre-trained Stable Diffusion for image reconstruction with our obtained - pairs. 
First, we utilize the encoder of Stable Diffusion's pretrained VAE to map  into the latent space, and obtain the condition latent . 
The UNet \cite{unet} denoiser performs latent diffusion, which contains an encoder, a middle block, and a decoder. In particular, the decoder receives the features from encoder and fuses them in different scales.
Here we create a parallel module (denoted as orange in Figure \ref{fig:architecture}) that contains the same encoder and the middle block as in the UNet denoiser. 
Then, we concatenate the condition latent  with the randomly sampled noisy  as the input for the parallel module.
Since this concatenation operation will increase the channel number of the first convolutional layer in the parallel module, we initialize the newly added parameters to zero, where all other weights are initialized from the pre-trained UNet denoiser checkpoints. The outputs of the parallel module are added to the original UNet decoder. Moreover, one  convolutional layer is applied before the addition operation for each scale.
During finetuning, the parallel module and these  convolutional layers are optimized simultaneously, where the prompt condition is set to empty.  We aim to
minimize the following latent diffusion objective:

The obtained result in this stage is denoted as .
To summarize, only the skip-connected features in the UNet denoiser are tuned for our specific task.
This strategy alleviates overfitting in small training dataset, and could inherit the high-quality generation from Stable Diffusion.
More importantly, our conditioning mechanism is more straightforward and effective for image reconstruction task compared to ControlNet \cite{controlnet}, which utilizes an additional condition network trained from scratch for encoding the condition information. In our LAControlNet, the well-trained VAE's encoder is able to project the condition images into the same representation space as the latent variables.
This strategy significantly alleviates the burden on the alignment between the internal knowledge in latent diffusion model and the external condition information.
In practice, directly utilizing ControlNet for image reconstruction leads to severe color shifts as shown in the ablation study (see Section \ref{sec:ablation3}).

\subsection{Latent Image Guidance for Fidelity-Realness Trade-off}
\label{sec:method3}
Although the above two-stage approach could already achieve good restoration results, a trade-off between \textit{realness} and \textit{fidelity} is still needed due to various users' preferences.
Thus, we propose a controllable module that could guide the denoising process towards the obtained  in stage-one, thus obtaining an adjustment between realistic and smooth results.
Classifier guidance is proposed by Dhariwal and Nichol \cite{beatsgan} which utilizes a classifier trained on noisy images to guide generation towards target class. While in most cases, the pre-trained models that serve as guidance are usually trained on clean images. To handle this situation, the work in \cite{blend, gdp} turn to guide the intermediate variable  to control the generation process of diffusion models. Specifically, in the sampling process, they estimate a clean image  from the noisy image  by
estimating the noise in . In this work, the diffusion and denoising processes are based on the latent space. Thus, we aim to obtain a clean latent  by the following equation:
 
Then, a latent-based loss  is defined as the  distance between the latent image guidance  and the estimated clean latent :
 
The above guidance could iteratively force spatial alignment and color consistency between latent features, and guide the generated latent to preserve the content of the reference latent.
Therefore, one can control how much information (such as structure, layout and color) is maintained from the reference image , thus achieving a transition from generated output to more smooth result. The whole algorithm of our latent image guidance is illustrated in Algorithm \ref{alg:latent_image_guidance}.


\floatname{algorithm}{Algorithm}
\begin{algorithm}[htbp]
    \caption{Latent-guided diffusion, given a diffusion model , and the VAE's encoder  and decoder }
    \label{alg:latent_image_guidance}
    \begin{algorithmic}
    \Statex \textbf{Input:} Guidance image , text description  (set to empty), diffusion steps , gradient scale 
    \Statex \textbf{Output:} Output image 
    \Statex Sample  from 
    \For{ from  to }
\State 
        \State 
        \State Sample  by 
    \EndFor
\State \textbf{return} 
    \end{algorithmic}
  \end{algorithm}








\section{Experiments}

\subsection{Datasets, Implementation, Metrics}
\noindent\textbf{Datasets.}
We train DiffBIR on the ImageNet \cite{imagenet} dataset at  resolution for BIR. As for BFR, we use FFHQ \cite{stylegan1} dataset and resize it to .
To synthesize the LQ images, we utilize the proposed degradation pipeline to process the HQ images during training (please see Appendix \ref{appendix:degradation_details} for details). 
For BSR, we utilize RealSRSet \cite{cai2019toward} dataset for comparison in a real-world setting. For a more thorough comparison in real-world scenarios, we collect 47 images from the Internet, denoted as Real47. It contains general images of diverse scenes, such as natural outdoor landscapes, old photos, architecture, humans from portraits to dense people crowds, plants, and animals, etc.
For BFR task, we evaluate our method on a synthetic dataset CelebA-Test~\cite{celeba} and three real-world
datasets: LFW-Test \cite{gfpgan}, CelebChild-Test \cite{gfpgan}, and WIDER-Test \cite{codeformer}. In particular, CelebA-Test contains 3,000
images selected from the CelebA-HQ dataset, where LQ images are synthesized under the same degradation range as our training settings. 


\noindent\textbf{Implementation.}
The restoration module adopts 8 residual Swin Transformer blocks (RSTB), and each RSTB contains 6 Swin Transformer Layers (STL). The head number is set to 6 and the window size is set to 8. We train the restoration module with a batch size of 96 for 150k iterations. 
We utilize Stable Diffusion 2.1-base \footnote{\url{https://github.com/Stability-AI/stablediffusion}} as the generative prior, and finetune the diffusion model for 25k iterations with a batch size of 192.
We use Adam \cite{adam} optimizer and set the learning rate to . 
The training process is conducted on  resolution with 8
NVIDIA A100 GPUs. For inference, we adopt
spaced DDPM sampling \cite{iddpm} with 50 timesteps. 
Our DiffBIR is able to handle images with arbitrary sizes larger than .
For images with sides , we first upsample them with the short side enlarged to 512, and then resize them back.

\noindent\textbf{Metrics}. Regarding the evaluation with ground truth, we adopt the traditional metrics: PSNR, SSIM, and LPIPS \cite{lpips}.
To better evaluate the \textit{realness} for BIR task, we also include several no-reference image quality assessment (IQA) metrics: MANIQA\footnote{MANIQA (\url{https://github.com/IIGROUP/MANIQA}) won first place in the NTIRE2022 Perceptual Image Quality Assessment Challenge Track 2 No-Reference competition.} \cite{maniqa} and NIQE. For BFR, we evaluate the identity preservation - IDS \cite{codeformer}, and employ the widely used perceptual metric FID \cite{fid}.
We also deploy a user study for a more thorough comparison.

\subsection{Comparisons with State-of-the-Art Methods}

For BSR, we compare our DiffBIR with state-of-the-art BSR methods: Real-ESRGAN+ \cite{realesrgan}, BSRGAN \cite{bsrgan}, SwinIR-GAN \cite{swinir}, and FeMaSR \cite{femasr}. The recent state-of-the-art ZIR methods (DDNM \cite{ddnm} and GDP \cite{gdp}) are also included \footnote{DDNM and GDP are selected because they provide an approach to restore images with arbitrary sizes.}.
Regarding BFR task, we compare with the most recent state-of-the-art methods: DMDNet \cite{dmdnet}, GFP-GAN \cite{gfpgan}, GPEN \cite{gpen}, GCFSR \cite{gcfsr}, VQFR \cite{vqfr}, CodeFormer \cite{codeformer}, RestoreFormer \cite{restoreformer}.









\noindent{\textbf{BSR on real-world dataset.}}
We provide the quantitative comparison on real-world datasets in Table \ref{tab:general_wild}.
It is observed that our DiffBIR obtains the best scores in MANIQA on both the widely used RealSRSet \cite{realsr} and our collected Real47. While BSRGAN and Real-ESRGAN+ could achieve top-3 results in MANIQA on both two datasets.
The visual comparison results are presented in Figure \ref{fig:general_real}. It can be seen that DiffBIR is able to restore text information more naturally, while other methods tend to distort the characters or produce blurry output.  
On the other hand, our DiffBIR could also generate realistic texture details for natural images, where other methods produce over-smooth results. More visualization results can be found in Figure \ref{fig:bsr_real47} and Figure \ref{fig:bsr_realsr}.


\vspace{-1em}

\begin{table}[htbp]
\centering
\setlength\tabcolsep{1pt}
    \caption{\small Comparison with state-of-the-art BSR and ZIR methods on real-world datasets with a  upsampling scale. {\color{red}\textbf{Red}} and {\color{blue}blue} indicate the best and second best performance. The top 3 results are marked as \colorbox{mygray}{gray}.}
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{|c|c|cc|cccc|c|}
\hline
Dataset                     & Metric  & DDNM~\cite{ddnm}   & GDP~\cite{gdp}                                                   & Real-ESRGAN+\cite{realesrgan}                                             & BSRGAN~\cite{bsrgan}                                                & SwinIR-GAN~\cite{swinir} & FeMaSR~\cite{femasr}                                                & \textbf{DiffBIR(Ours)}                                \\ \hline \hline
                            & MANIQA↑ & 0.4535 & 0.4581                                                & \cellcolor[HTML]{F2F2F2}0.5376                        & \cellcolor[HTML]{F2F2F2}{\color[HTML]{0B5FD1} 0.5640} & 0.5295     & 0.5247                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 0.5906} \\
\multirow{-2}{*}{RealSRSet} & NIQE↓   & 6.8415 & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 5.0626} & 5.7401                                                & \cellcolor[HTML]{F2F2F2}5.6074                        & 5.6093     & \cellcolor[HTML]{F2F2F2}{\color[HTML]{0B5FD1} 5.2353} & 6.0738                                                \\ \hline \hline
                            & MANIQA↑ & 0.4813 & 0.5237                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{0B5FD1} 0.5900} & \cellcolor[HTML]{F2F2F2}0.5889                        & 0.5721     & 0.5718                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 0.6293} \\
\multirow{-2}{*}{Real47}    & NIQE↓   & 6.4768 & \cellcolor[HTML]{F2F2F2}3.9866                        & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 3.9103} & 4.0338                                                & 3.9910     & 4.1731                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{0B5FD1} 3.9240} \\ \hline
\end{tabular}     }
\label{tab:general_wild}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\linewidth]{figures/bsr_1.pdf}
\includegraphics[width=0.7\linewidth]{figures/bsr_2.pdf}
\vspace{-0.5em}
\caption{\small Visual comparison on real-world datasets with upsampling scale factor of 4.(\textbf{Zoom in for best view})}
\label{fig:general_real}
\vspace{-2em}
\end{figure}

\begin{wrapfigure}[17]{r}{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{figures/userstudy_plot_tight.pdf}
  \caption{\small The distribution of scores obtained by SwinIR-GAN, Real-ESRGAN+, BSRGAN, and our DiffBIR in user study.}
  \label{fig:usestudy_plot}
\end{wrapfigure}

To further compare DiffBIR with other state-of-the-art methods, we conduct a user study on our collected Real47 dataset. This user study compares DiffBIR, SwinIR-GAN, BSRGAN, and RealESRGAN+. For each image, users are asked to rank the results of the four methods and assign 1-4 points to different methods in an ascending order. To be more exact, better result obtains higher score. 
31 users are recruited to conduct this user study under detailed instruction. The distribution of scores obtained by each method is shown in Figure \ref{fig:usestudy_plot}. It can be observed that DiffBIR achieves the highest median score, and its upper quartile exceeds 3. This indicates that users tend to rank DiffBIR's results in the first place. The user study results again demonstrate that DiffBIR's visual results are superior to other methods, which aligns with its highest score on MANIQA.

\noindent{\textbf{BFR on both synthetic and real-world datasets.}}
We show the quantitative comparison on both synthetic and real-world datasets in Table \ref{tab:face}. For the synthetic dataset CelebA-Test~\cite{celeba}, our DiffBIR achieves the highest FID score. Meanwhile, it is also the top-3 methods regarding PSNR and IDS. This reveals that the proposed DiffBIR can successfully produce results with both high \textit{realness} and high \textit{fidelity}. 
For real-world datasets, DiffBIR obtains the best results on both LFW-Test~\cite{lfw} (mild degradation) and WIDER-Test~\cite{codeformer} (heavy degradation) datasets, and comparable results with state-of-the-art methods on CelebChild-Test.
Figure \ref{fig:face_syn} depicts a visual comparison of various methods on synthetic dataset. The first example demonstrates that only DiffBIR succeeds in restoring extremely degraded cases while other methods fail. It can be seen from the second example that only DiffBIR can successfully recover the occluded left eye. Figure \ref{fig:face_real} presents a visual comparison on real-world dataset.  It can be observed from the first example that DiffBIR is able to accurately restore the hair, while other methods mistake the hair for a part of the facial area. The second example suggests that our DiffBIR is the only method that can generate realistic details on non-face area (\textit{i.e.,} the decoration in the forehead).
More visualization results can be found in Figure \ref{fig:supp_face_syn} and Figure \ref{fig:supp_face_real}.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/face_syn_1_compressed.pdf}
\includegraphics[width=0.7\linewidth]{figures/face_syn_2_compressed.pdf}
\vspace{-1em}
\caption{\small Qualitative comparison of different BFR methods on synthetic datasets.(\textbf{Zoom in for best view})}
\label{fig:face_syn}
\vspace{-2em}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/face_real_1_compressed.pdf}
\includegraphics[width=0.7\linewidth]{figures/face_real_2_compressed.pdf} 
\vspace{-1em}
\caption{\small Qualitative comparison of different BFR methods on real-world datasets.(\textbf{Zoom in for best view})}
\label{fig:face_real}
\vspace{-1em}
\end{figure}

\begin{table}[H]
\small
    \centering
    \vspace{-0.5em}
    \setlength\tabcolsep{5pt}
    \caption{\small Comparison with state-of-the-art methods for BFR on both synthetic and real-world face datasets. {\color{red}\textbf{Red}} and {\color{blue}blue} indicate the best and second best performance. The top 3 results are marked as \colorbox{mygray}{gray}.}\label{tab:face}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{|c|ccccc|ccc|}
\hline
                          & \multicolumn{5}{c|}{Synthetic}                                                                                                                                                                                                                                                      & \multicolumn{3}{c|}{Wild}                                                                                                                                          \\
\multirow{-2}{*}{Dataset} & \multicolumn{5}{c|}{CelebA-Test}                                                                                                                                                                                                                                                    & LFW-Test                                             & WIDER-Test                                           & CelebChild-Test                                      \\ \hline
Method                    & PSNR↑                                                  & SSIM↑                                                 & LPIPS↓                                                & FID↓                                                 & IDS↑                                                & FID↓                                                 & FID↓                                                 & FID↓                                                 \\ \hline
GPEN~\cite{gpen}                      & 21.3995                                                & 0.5742                                                & 0.4687                                                & 23.92                                                & 0.48                                                & 51.97                                                & 46.35                                                & 76.58                                                \\
GCFSR~\cite{gcfsr}                     & \cellcolor[HTML]{F2F2F2}{\color[HTML]{0B5FD1} 21.8791} & \cellcolor[HTML]{F2F2F2}{\color[HTML]{0B5FD1} 0.6072} & 0.4577                                                & 35.49                                                & 0.44                                                & 52.20                                                & 40.86                                                & 76.29                                                \\
GFPGAN~\cite{gfpgan}                    & 21.6953                                                & \cellcolor[HTML]{F2F2F2}0.6060                        & \cellcolor[HTML]{F2F2F2}0.4304                        & \cellcolor[HTML]{F2F2F2}21.69                        & 0.49                                                & 52.11                                                & 41.70                                                & 80.69                                                \\
VQFR~\cite{vqfr}                      & 21.3014                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 0.6132} & \cellcolor[HTML]{F2F2F2}{\color[HTML]{1552D1} 0.4116} & \cellcolor[HTML]{F2F2F2}{\color[HTML]{1552D1} 20.30} & 0.48                                                & 49.88                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{1552D1} 37.87} & \cellcolor[HTML]{F2F2F2}{\color[HTML]{1552D1} 74.76} \\
RestoreFormer~\cite{restoreformer}             & 21.0025                                                & 0.5283                                                & 0.4789                                                & 43.77                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{0B5FD1} 0.56} & \cellcolor[HTML]{F2F2F2}48.43                        & 49.79                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 70.54} \\
DMDNet~\cite{dmdnet}                    & 21.6617                                                & 0.6000                                                & 0.4828                                                & 64.79                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 0.67} & \cellcolor[HTML]{F2F2F2}{\color[HTML]{1552D1} 43.36} & 40.51                                                & 79.38                                                \\
CodeFormer~\cite{codeformer}                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 22.1519} & 0.5948                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 0.4055} & 22.19                                                & 0.47                                                & 52.37                                                & \cellcolor[HTML]{F2F2F2}38.78                        & 79.54                                                \\
DiffBIR(Ours)             & \cellcolor[HTML]{F2F2F2}21.7509                        & 0.5971                                                & 0.4573                                                & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 20.02} & \cellcolor[HTML]{F2F2F2}0.51                        & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 39.58} & \cellcolor[HTML]{F2F2F2}{\color[HTML]{FF0000} 32.35} & \cellcolor[HTML]{F2F2F2}75.94                        \\ \hline
\end{tabular}
%
     }
    \vspace{-1.5em}
\end{table}

\subsection{Ablation Studies}
\noindent{\textbf{The Importance of Restoration Module.}}
\label{sec:ablation1}
In this part, we investigate the significance of our proposed two-stage pipeline. 
Here, we remove the Restoration Module (RM), and directly finetune the diffusion model with synthesized training pairs.
The removal of restoration module leads to a noticeable performance drop in FID/MANIQA across all real-world datasets (see Table \ref{tab:ablation_rm}).
The visual comparison is presented in Figure \ref{fig:ablation}(a).
As seen from the first example, the one-stage model (w/o RM) regards the degradations as semantic information by mistake. This demonstrates that the restoration module contributes to preserving fidelity. The second example clearly illustrates that solely finetuning the Stable Diffusion without applying the RM cannot fully remove the real-world noise/artifacts. This indicates that the RM is indispensable in degradation removal.





\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[width=.33\linewidth]{figures/ablation_rm_cropped.pdf}}\hfill
\subfloat[]{\includegraphics[width=.33\linewidth]{figures/compare_guidance.pdf}}\hfill
\subfloat[]{\includegraphics[width=.33\linewidth]{figures/color_shift.pdf}}
\caption{\small Visual comparison of ablation studies. (a) DiffBIR w/o restoration module performs poorly in both fidelity maintaining (first row) and degradation removal (second row); (b) w/o finetuning Stable Diffusion, directly applying the image guidance technique \cite{ddnm, gdp} is not able to produce realistic results. (c) ControlNet \cite{controlnet} has a color shift problem which can be addressed by our LAControlNet.(\textbf{Zoom in for best view})}
\label{fig:ablation}
\end{figure}

\begin{table}[htbp]
\vspace{-2em}
\centering
\small
\setlength\tabcolsep{2pt}
\caption{\small The effectiveness of restoration module. The best results are denoted as \color{red}{Red}.}
\begin{tabular}{|c|ccc|cc|}
\hline
                          & \multicolumn{3}{c|}{Face}                                                                  & \multicolumn{2}{c|}{General}                                \\
\multirow{-2}{*}{Dataset} & LFW-Test                     & WIDER-Test                   & CelebChild-Test              & RealSRSet                    & Real47                       \\ \hline
Method                    & FID↓                         & FID↓                         & FID↓                         & MANIQA↑                      & MANIQA↑                      \\ \hline
DiffBIR(w/o RM)           & 40.78                        & 33.22                        & 75.98                        & 0.582                        & 0.624                        \\
DiffBIR(w/ RM)            & {\color[HTML]{CB0000} 39.58} & {\color[HTML]{CB0000} 32.35} & {\color[HTML]{CB0000} 75.94} & {\color[HTML]{CB0000} 0.591} & {\color[HTML]{CB0000} 0.629} \\ \hline
\end{tabular}
%
 \label{tab:ablation_rm}
\end{table}

\noindent{\textbf{The Necessity of Finetuning Stable Diffusion.}}
\label{sec:ablation2}
Next, we illustrate the necessity of finetuning the latent diffusion model. 
Zero-shot IR methods \cite{ddnm, gdp} provide an effective approach that guides the reverse diffusion process using the degraded image in the image space. 
Following their methodology, we employ the smoothed result  to guide the original Stable Diffusion without finetuning. However, as depicted in Figure \ref{fig:ablation}(b),
this guidance strategy tends to generate unrealistic content (\textit{i.e.,} a bird with one leg missing). 
This demonstrates that the widely used guidance in image space may not effectively generalize to the latent space, thus finetuning Stable Diffusion becomes indispensable for this image reconstruction task. 

\noindent{\textbf{The Effectiveness of LAControlNet.}}
\label{sec:ablation3}
Then we aim to emphasize the effectiveness of our proposed LAControlNet that encodes  to the latent space. Here we compare with ControlNet \cite{controlnet}, which adopts an additional condition network trained from scratch for conditioning the input information. As shown in Figure \ref{fig:ablation}(c), ControlNet tends to output results with color shifts, as there is no explicit regularization on color consistency during training. One might use non-uniform sampling to increase the probability of optimization in the early sampling stage and achieves better color controlling \cite{t2iadapter}. Nevertheless, our method is much more straightforward and fully exploits the latent diffusion prior.

\noindent{\textbf{The Flexibility of Controllable Module.}}
\label{sec:ablation4}
Considering that generative restoration models may produce unexpected details, here we provide a controllable module for users to explore according to their personal preferences. The visualization result is shown in Figure \ref{fig:tradeoff}.
Our experiments suggest that a larger gradient scale  tends to produce a high-fidelity smooth result which is close to . As seen from the first row, DiffBIR's output  has some blue artifacts in the dog's eyes, thus we set  to 200 and higher as well for obtaining a better result. Moreover, the background is also changing (tends to be more blurry) as the gradient scale  grows.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\linewidth]{figures/tradeoff.pdf}
\caption{\small Our latent image guidance is able to achieve a trade-off between quality and fidelity. The gradient scale can be tuned to obtain transition effects between sharp  and smooth .(\textbf{Zoom in for best view})}
\label{fig:tradeoff}
\end{figure}

\section{Conclusion and Limitations}
We propose a unified framework for blind image restoration, named DiffBIR, which could achieve realistic restoration results by leveraging the prior knowledge of pre-trained Stable Diffusion. It consists of two stages: the restoration and generation stage, which ensures both fidelity and realness. Extensive experiments have validated the superiority of DiffBIR over existing state-of-the-art methods for both BSR and BFR tasks. Although our proposed DiffBIR has shown promising results, the potential of text-driven image restoration is not explored. Further exploitation in Stable Diffusion for image restoration task is encouraged. On the other hand, our DiffBIR method requires 50 sampling steps to restore a low-quality image, resulting in much higher computational resource consumption and more inference time compared to other image restoration methods.


{\small
\bibliographystyle{plain}
\bibliography{references}
}



\newpage
\appendix

\section{Degradation Details}
\label{appendix:degradation_details}
Degradation settings used for training our DiffBIR are introduced in this section. Following \cite{realesrgan}, we employ the second-order degradation process to enhance the robustness of the restoration module in real-world scenarios. Specifically, a degradation model in a certain stage consists of three operations: \textbf{blur}, \textbf{resize}, and \textbf{noise}. \noindent\textbf{Blur.} We utilize isotropic Gaussian blur or anisotropic Gaussian blur with equal probabilities. The size of the blur kernel follows a uniform distribution ranging from 7 to 21, and the blur sigma is uniformly sampled between 0.2 and 3 for the first degradation process and between 0.2 and 1.5 for the second degradation process. \noindent\textbf{Resize.} We consider multiple resize algorithms, including area resize, bilinear interpolation and bicubic resize. The scaling factor for resize follows a uniform distribution ranging from 0.15 to 1.5 for the first degradation process and from 0.3 to 1.2 for the second degradation process. \noindent\textbf{Noise.} We incorporate Gaussian noise, Poisson noise, and JPEG compression noise. The scale of Gaussian noise is uniformly sampled between 1 and 30 in the first degradation process and between 1 and 25 in the second degradation process. The scale of Poisson noise is randomly sampled from 0.05 to 3 and 0.05 to 2.5 for the first and second degradation processes, respectively. The quality of JPEG compression follows a uniform distribution ranging from 30 to 95.

Moreover, we combine the degradation settings adopted in blind face restoration. Specifically, we consider a large dowsampling range , and a large blur kernel range whose sigma is within . 
In this way, the generation module is trained to remedy the information loss within a wide range.

\section{More Qualitative Comparisons For BFR}
\label{appendix:supp_bfr}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\linewidth]{figures/supp_face_syn.pdf}
\vspace{-1.5em}
\caption{More qualitative comparisons for BFR on synthetic dataset.(\textbf{Zoom in for best view})}
\label{fig:supp_face_syn}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/supp_face_real.pdf}
\vspace{-1.5em}
\caption{More qualitative comparisons for BFR on real-world dataset.(\textbf{Zoom in for best view})}
\label{fig:supp_face_real}
\end{figure}

\newpage

\section{More Qualitative Comparisons For BSR}
\label{appendix:supp_bsr}

\begin{figure}[htbp]
    \centering
\subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/14.pdf}}\hfill

    \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/supp_general_real_7.pdf}}\hfill
    
    \caption{More qualitative comparisons on Real47 dataset.(\textbf{Zoom in for best view})}
    \label{fig:bsr_real47}
\end{figure}

\begin{figure}[hptb]
    \centering
    \ContinuedFloat \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/supp_general_real_9.pdf}}\hfill

    \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/supp_general_real_11.pdf}}\hfill
    
    \caption{More qualitative comparisons on Real47 dataset.(\textbf{Zoom in for best view})}
\end{figure}

\begin{figure}[htbp]
    \centering
    \ContinuedFloat \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/supp_general_real_10.pdf}}\hfill

    \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/supp_general_real_8.pdf}}\hfill
    
    \caption{More qualitative comparisons on Real47 dataset.(\textbf{Zoom in for best view})}
\end{figure}

\begin{figure}[htbp]
    \centering

    \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/2_color_fixed.pdf}}\hfill

    \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/16.pdf}}\hfill
    
    \caption{More qualitative comparisons on RealSRSet \cite{realsr} dataset.(\textbf{Zoom in for best view})}
    \label{fig:bsr_realsr}
\end{figure}

\begin{figure}[htbp]
    \centering
    \ContinuedFloat 

    \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/17_color_fixed.pdf}}\hfill

    \subfloat[]{\includegraphics[width=\textwidth]{figures/supp_sr_real/1.pdf}}\hfill
    
    \caption{More qualitative comparisons on RealSRSet \cite{realsr} dataset.(\textbf{Zoom in for best view})}
\end{figure}







\end{document}
