In this section we give a variational formulation of the control problem of determining the optimal scheduler for a CTMDP. 
We show how to approximate statistically in an unbiased way the functional gradient of the time-bounded reachability probability, and give a convergent algorithm to achieve this. 





\subsection{Reachability probability as a functional}












As defined in Section \ref{sec:preliminaries}, a scheduler is a way of resolving non-determinism by associating a (time-dependent) probability to each action/ state pair. 
We will realise a scheduler as a vector  of  functions , one for each action , where  is the compact subset of  used to define  for the PCTMDP formalism. The corresponding probability of an action  at a state  can be retrieved using the soft-max (logistic) transform as follows:

Given a scheduler , a CTMDP is reduced  to a CTMC , and the problem of estimating the probability of a reachability property  can be reduced to the computation of a transient probability for  by standard techniques \cite{Baier2003}.
The satisfaction probability can be therefore viewed as a {\it functional} 

where  is the set of all possible scheduler functions.
The functional is defined explicitly as follows: consider a sample trajectory  from the CTMC  obtained from the CTMDP by selecting a scheduler. Let ,  be a reachability property, and denote by  the fact that the trajectory reaches  within the specified time bound. We can encode it in the following indicator function:

Then the expected reachability value associated with the scheduler , represented by the vector of functions , is defined as follows:

where expectation is taken with respect to the distribution on trajectories of .
Notice that in general it is computationally very hard to analytically compute the r.h.s.\ in the above equation, as it amounts to transient analysis for a time-inhomogeneous CTMC; we therefore need to resort to statistical model checking methods \cite{SB:Zuliani:2009:StatMC,MC:YounesSimmons:INFCOMP2006:statMC} to approximate in a Monte Carlo way the expectation in equation \eqref{reward}.

To formulate the continuous time control problem of determining the optimal scheduler, we need to define the concept of functional derivative.

\begin{definition}
Let  be a functional defined on a space of functions . 
The {\it functional derivative} of  at  along a function , denoted by , is defined by 

whenever the limit on the r.h.s. exists.  \end{definition}
Notice that if we restrict ourselves to piecewise constant functions on a grid, the definition above returns the standard definition of gradient of a finite-dimensional function.
We can now give a variational definition of optimal scheduler
\begin{lemma}
An optimal scheduler  is associated with a function  such that 

where  denotes the  norm on functions.
\end{lemma}
The variational formulation above allows us to attack the problem via direct optimisation through a gradient ascent algorithm, as we will see below.





















\subsection{Stochastic Estimation of the Functional Gradient}

It is well-known that a gradient ascent approach is guaranteed to find the global optimum of a convex objective function.
Gradient ascent starts from an initial solution which is updated iteratively towards the direction that induces the steepest change in the objective function; that direction is given by the gradient of the function.
For a functional  the concept of gradient is captured by the functional derivative , which is a function of  that dictates the rate of change of the functional  when  is perturbed at the point .
In the case of functional optimisation, the gradient ascent update will have the form:

where  is the learning rate which controls the effect of each update, and  is the functional derivative of . Unfortunately, an analytic expression for the functional derivative of the functional defined in \eqref{reward} is usually not available.

We can however obtain an unbiased estimate of the functional derivative by using the infinite-dimensional generalisation of this simple lemma
\begin{lemma}\label{finDimGrad}
Let  be a smooth function, and let  be its gradient at a point . 
Let  be a random vector from an isotropic, zero mean distribution . 
For , define  
Then
\end{lemma}
\begin{proof}
The tangent space of  at the point  is naturally decomposed in the orthogonal direct sum of a subspace of dimension 1 parallel to the gradient, and a subspace of dimension  tangent to the level surfaces of the function . 
For small , any change in the value of the function  will be due to movement in the gradient direction. 
As the distribution  is isotropic, every direction is equally likely in ; however, the flipping operation in the definition of  in \eqref{flipper} ensures that the component of  along the gradient  is always positive, while it does not affect the orthogonal components. 
Therefore, in expectation,  returns the direction of the functional gradient.
\end{proof}



\subsection{Scheduler representation in terms of basis functions}

In order to obtain an unbiased estimate of a functional gradient, we need to define a zero-mean isotropic distribution on a suitable space of functions. To do so, we introduce the concept of Gaussian Process, a generalisation of the multivariate Gaussian distribution to infinite dimensional spaces of functions (see, e.g. \cite{Rasmussen2006}).

\begin{definition} A Gaussian Process (GP) over an input space  is an infinite-dimensional family of real-valued random variables indexed by  such that, for every finite subset , the finite dimensional marginal obtained by restricting the GP to  follows a multi-variate normal distribution.
\end{definition}
Thus, a GP can be thought as a distribution over functions  such that, whenever the function is evaluated at a finite number of points, the resulting random vector is normally distributed. In the following, we will only consider  for some integer .

Just as the Gaussian distribution is characterised by two parameters, a GP is characterised by two functions, the {\it mean} and {\it covariance} function. The mean function plays a relatively minor role, as one can always add a deterministic mean function, without loss of generality; in our case, since we are interested in obtaining small perturbations, we will set it to zero. The covariance function, which captures the correlations between function values at different inputs, instead plays a vital role, as it defines the type of functions which can be sampled from a GP. We will use the {\it Radial Basis Function} (RBF) covariance, defined as follows:

where  and  are the amplitude and length-scale parameters of the covariance function.
To gain insight into the geometry of the space of functions associated with a GP with RBF covariance, we report without proof the following lemma (see e.g. Rasmussen \& Williams, Ch 4.2.1 \cite{Rasmussen2006}).
\begin{lemma}\label{denseLemma}
Let  be the space of random functions   generated by taking linear combinations of basis functions  , with  and independent Gaussian coefficients .
The sample space of a GP with RBF covariance defined by \eqref{rbfCov} is the infinite union of the the spaces .
\end{lemma}

We refer to the basis functions entering in the constructive definition of GPs given in Lemma \ref{denseLemma} as {\it kernel functions}. Two immediate consequences of the previous Lemma are important for us:\begin{itemize}
\item{A GP with RBF covariance defines an {\it isotropic} distribution in its sample space (this follows immediately from the i.i.d.\ definition of the weights in Lemma \ref{denseLemma})};
\item{The sample space of a GP with RBF covariance is a dense subset of the space of all continuous functions (see also \cite{bortolussi:smoothed16} and references therein).}
\end{itemize}





GPs therefore provide us with a convenient way of extending the procedure described in Lemma \ref{finDimGrad} to the infinite dimensional setting.
In particular, Lemma \ref{denseLemma} implies that any scheduler function  that is a sample from a GP (with RBF covariance) can be approximated to arbitrary accuracy in terms of basis functions as follows:

where  is the centre of a Gaussian kernel function,  is a diagonal matrix that contains  squared length-scale parameters of the kernel functions, and  is the dimensionality of the state-space.
This formulation allows describing functions (aka points in an infinitely dimensional Hilbert space) as points in the finite vector space spanned by the weights .
Note that the proposed basis function representation implies relaxation of the population variables to the continuous domain, though in  practice we are only interested in evaluating  for integer-valued . 

The advantage of the kernel representation is that we do not need to account for all states , but only for  Gaussian kernels with centres  for .
Therefore, the value of the scheduler at a particular state  will be determined as a linear combination of the kernel functions, with proximal kernels contributing more due to the exponential decay of the kernel functions.
This method offers a compact representation of the scheduler, and essentially does not suffer from state-space explosion, as we treat states as continuous. Moreover, we do not lose accuracy, as every function on  can be extended to a continuous function on  by interpolation.
On the practical side, we consider that the kernel functions are spread evenly across the joint space (state space \& time), and the length-scale for each dimension is considered to be equal to the distance of two successive kernels.\footnote{Kernel functions typically also have an amplitude parameter, which we consider to be equal to 1.}




\subsection{A Stochastic Gradient Ascent Algorithm}

Given a scheduler , we first evaluate the reachability probability via statistical model checking. 
We then perturb the corresponding functions  by adding a draw from a zero-mean GP with marginal variance scaled by , and evaluate again by statistical model checking the probability of the perturbed scheduler. If this is increased, we take a step in the perturbed direction, otherwise we take a step in the opposite direction. Notice that this procedure can be repeated for multiple independent perturbation functions to obtain a more robust estimate. 
The whole procedure is described in Algorithm \ref{alg:estimateGradient}, which produces an estimate for the gradient of the functional  at a vector  of functions  by considering the average of  random directions.
\begin{algorithm}[ht!]
\caption{Estimate the functional gradient of }
\label{alg:estimateGradient}
\begin{algorithmic}
\REQUIRE Vector  of functions , scaling factor , batch size 
\ENSURE An estimate of the functional derivative (gradient) 
	\STATE Set gradient 
	\STATE Evaluate  via statistical model checking
	\FOR{ to }
		\STATE Consider random direction  such that , we have:
		
		\STATE Evaluate 
		\STATE Estimate the directional derivative:
		
		\IF{}
			\STATE 
		\ELSE
			\STATE 
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}
We are now ready to state our main result:
\begin{theorem}
Algorithm \ref{alg:estimateGradient} gives an unbiased estimate of the functional gradient of the functional .
\end{theorem}
\begin{proof} Since both the statistical model checking estimation and the gradient estimation are unbiased and independent of each other, this follows.
\end{proof}

\begin{algorithm}[ht!]
\caption{Stochastic gradient ascent for }
\label{alg:gradientAscent}
\begin{algorithmic}
\REQUIRE Initial function vector , learning rate ,  iterations
\ENSURE A function vector  that approximates a local optimum of 
	\FOR { \TO }
		\STATE Estimate the functional gradient  by using Algorithm \ref{alg:estimateGradient}
		\STATE Update: 
	\ENDFOR
\end{algorithmic}
\end{algorithm}


Therefore, we can use this stochastic estimate of the functional gradient to devise a stochastic gradient ascent algorithm which directly solves the variational problem in equation \eqref{varFormulation}.
This is summarised in Algorithm \ref{alg:gradientAscent}, which requires as input an initial vector of functions , and a learning rate .
The effects of the learning rate on the convergence properties of the method have been extensively studied in the literature.
In particular, for a decreasing learning rate convergence is guaranteed in the strictly convex scenario, if the following conditions are satisfied:  and  \cite{Murata1999,Bottou2010}, suggesting a  decrease for the learning rate.
In non-convex problems, such as the ones considered in this work, the  decrease is generally too aggressive, leading to vulnerability to local optima.
Following the recommendations of \cite{Bottou2012}, we adopt a more conservative strategy:

where  is an initial value for the learning rate, which is problem dependent.
















