\documentclass[10pt,journal,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
\usepackage[nocompress]{cite}
\else
\usepackage{cite}
\fi

\ifCLASSINFOpdf
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{array}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{rotating}


\newtheorem{definition}{Definition}
\newcommand{\citet}{\cite}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\appendixref}[1]{Appendix}
\newcommand{\equref}[1]{Equation (\ref{#1})}

\begin{document}
\title{Heterogeneous Graph Representation Learning with Relation Awareness}









\markboth{IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,~Vol.~XX, No.~X, XX~XXXX}
{Yu \MakeLowercase{\textit{et al.}}: Heterogeneous Graph Representation Learning with Relation Awareness}


\appendix
\label{section-appendix}
In the appendix, details of the experiments are introduced.

\begin{table*}[!htbp]
\centering
\caption{Settings of dropout and learning rate on all the methods.}
\label{tab:hyperparameters}
\begin{tabular}{c|c|cccccccccc}
\hline
Datasets                      & Hyper-parameters & MLP   & GCN   & GraphSAGE & GAT   & RGCN  & RSHN  & HAN   & HetSANN & HGT   & R-HGNN \\ \hline
\multirow{2}{*}{IMDB}         & dropout          & 0.8   & 0.5   & 0.0       & 0.5   & 0.5   & 0.4   & 0.5   & 0.1     & 0.3   & 0.6     \\ 
                              & learning rate    & 0.005 & 0.01  & 0.005     & 0.001 & 0.005 & 0.001 & 0.005 & 0.01    & 0.01  & 0.001   \\ \hline
\multirow{2}{*}{OGB-MAG}      & dropout          & 0.0   & 0.3   & 0.1       & 0.1   & 0.1   & 0.2   & ---   & 0.3     & 0.1   & 0.5     \\ 
                              & learning rate    & 0.01  & 0.001 & 0.001     & 0.001 & 0.001 & 0.001 & ---   & 0.001   & 0.001 & 0.001   \\ \hline
\multirow{2}{*}{OAG-Venue}    & dropout          & 0.1   & 0.3   & 0.2       & 0.2   & 0.3   & 0.2   & ---   & 0.3     & 0.3   & 0.3     \\
                              & learning rate    & 0.001 & 0.001 & 0.001     & 0.001 & 0.001 & 0.001 & ---   & 0.001   & 0.001 & 0.001   \\ \hline
\multirow{2}{*}{OAG-L1-Field} & dropout          & 0.2   & 0.2   & 0.2       & 0.2   & 0.3   & 0.2   & ---   & 0.2     & 0.1   & 0.3     \\ 
                              & learning rate    & 0.001 & 0.001 & 0.001     & 0.001 & 0.001 & 0.001 & ---   & 0.001   & 0.001 & 0.001   \\ \hline
\end{tabular}
\end{table*}

\subsection*{Details of the Datasets} \label{section-appendix-datasets}
\begin{itemize}
    \item \textbf{IMDB}:
    Plot keywords of movies are provided by the IMDB. Following \cite{DBLP:conf/www/WangJSWYCY19}, we use the bag-of-words representation of plot keywords to denote movie features, corresponding to a 1,537-dimensional feature for each movie. Director/actor features are the average representation of movies that they directed/acted, whose dimensions are both 1,537.
    
    \item \textbf{OGB-MAG}:
    Open Graph Benchmark (OGB) \cite{DBLP:conf/nips/HuFZDRLCL20} contains a diverse set of challenging benchmark datasets for graph machine learning research. Leaderboards are set up for each dataset and state-of-the-art models are ranked based on their performance. Moreover, all the models are listed with open-sourced implementation to reproduce the results. OGB-MAG is a heterogeneous academic network in OGB, where each paper is associated with a 128-dimensional Word2Vec feature. For nodes that do not have features, we generate their features by the metapath2vec \cite{DBLP:conf/kdd/DongCS17} model.
    As a result, the feature of each author/ field/ institution node corresponds to a 128-dimensional vector. The feature of each paper is the concatenation of the given 128-dimensional Word2Vec feature and the generated 128-dimensional structural feature, corresponding to a 256-dimensional vector.
    
    \item \textbf{OAG-Venue}:
    We use the pre-processed graph in the Computer Science (CS) domain extracted from Open Academic Graph (OAG) by \citet{DBLP:conf/www/HuDWS20} to conduct experiments\footnote{HGT authors only shared the graph in the CS domain.}. Features of all types of nodes are given in the OAG dataset. Specifically, the feature of each paper is a 768-dimensional vector, corresponding to the weighted combination of each word's representation in the paper's title. Each word's representation and the attention score are obtained from a pre-trained XLNet \cite{DBLP:conf/nips/YangDYCSL19}. The feature of each author is the average of his/her published paper representations, corresponding to a 768-dimensional vector as well. The features of other types of nodes are generated by the metapath2vec model to reflect the heterogeneous graph structure, whose dimensions are all set to 400. One potential issue with the OAG dataset in \cite{DBLP:conf/www/HuDWS20} is the information leakage, since target nodes and the nodes with ground truth are connected with edges. To solve this issue, we remove all the edges between paper nodes and nodes with ground truth that we aim to predict. Specifically, the classification task on OAG-Venue is to predict the published venues of papers, so we remove all edges between paper nodes and venue nodes in the original OAG dataset. We select venues that associated with no less than 200 papers to conduct experiments. In total, there are 241 venues in OAG-Venue, making the task as a 241-class classification problem.
    
    \item \textbf{OAG-L1-Field}:
    The classification task on OAG-L1-Field is to predict the -level field that each paper belongs to, so we remove all the edges between paper nodes and field nodes in the original OAG dataset. We select fields that associated with no less than 100 papers to conduct experiments. In total, there are 52 fields in OAG-L1-Field, making the task as a 52-class classification problem. 
\end{itemize}


\subsection*{Selection of Dropout and Learning Rate} \label{section-appendix-hyper-parameters}
On IMDB, the dropout and learning rate are searched in  and , respectively. On OGB-MAG, we search the dropout and learning rate in  and . On OAG-Venue and OAG-L1-Field,the dropout and learning rate are searched in  and , respectively. The settings of dropout and learning rate on all the methods are shown in \tabref{tab:hyperparameters}.




\subsection*{Node Clustering} \label{section-appendix-node_clustering}
On the small-scale dataset, we feed the learned representations of all the movie nodes into k-means algorithm to achieve the clustering performance of different models. On large-scale datasets, it is infeasible to feed all the paper nodes into k-means algorithm. Therefore, we first select top-five classes of papers in the testing set and then randomly select 1000 papers from each class, and finally obtain 5,000 papers. Then we feed the selected 5,000 paper nodes into k-means algorithm to get the clustering results. The number of clusters is equal to the number of real classes in each dataset (i.e., 3 for IMDB, and 5 for OGB-MAG, OAG-Venue and OAG-L1-Field).

\subsection*{Link Prediction} \label{section-appendix-link_prediction}
Due to the huge number of edges on large-scale datasets, it is infeasible to do link prediction on all the edges. Therefore, we adjust the number of sampled edges on the datasets. In particular, 3\%, 1\% and 1\% of the edges are sampled as training, validation and testing sets on OGB-MAG, respectively. Correspondingly, 15\%, 5\% and 5\% on OAG-Venue, and 30\%, 10\% and 10\% on OAG-L1-Field. Each edge in the training set is associated with five randomly sampled negative edges, and each edge in the validation or testing sets is associated with a randomly sampled negative edge.






\bibliographystyle{IEEEtran}
\bibliography{reference}
























\end{document}