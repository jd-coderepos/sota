\section*{APPENDIX: Rethinking Attention with Performers}
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\label{sec:appendix}

\section{Hyperparameters for experiments}
\label{appendix:hyperparameters}
This optimal setting (including comparisons to approximate softmax) we use for the Performer is specified in the Generalized Attention (Subsec. \ref{subsec:generalized_default}), and \textbf{unless specifically mentioned (e.g. using name "Performer-SOFTMAX"), "Performer" refers to using this generalized attention setting.}

\subsection{Metrics}
We report the following evaluation metrics:
\vspace{-3mm}
\begin{enumerate}[itemsep=0.2mm]
\item \textbf{Accuracy}: For unidirectional models, we measure the accuracy on next-token prediction, averaged across all sequence positions in the dataset. For bidirectional models, we mask each token with  probability (same as \citep{bert}) and measure accuracy across the masked positions.
\item \textbf{Perplexity}: For unidirectional models, we measure perplexity across all sequence positions in the dataset. For bidirectional models, similar to the accuracy case, we measure perplexity across the masked positions.
\item \textbf{Bits Per Dimension/Character (BPD/BPC)}: This calculated by loss divided by .
\end{enumerate}
\vspace{-3mm}
We used the full evaluation dataset for TrEMBL in the plots in the main section, while for other datasets such as ImageNet64 and PG-19 which have very large evaluation dataset sizes, we used random batches (>2048 samples) for plotting curves. 

\subsubsection{PG-19 Preprocessing}

The PG-19 dataset \citep{compr} is presented as a challenging long range text modeling task. It consists of out-of-copyright Project Gutenberg books published before 1919. It does not have a fixed vocabulary size, instead opting for any tokenization which can model an arbitrary string of text. We use a unigram SentencePiece vocabulary \citep{sentencepiece} with 32768 tokens, which maintains whitespace and is completely invertible to the original book text. Perplexities are calculated as the average log-likelihood per token, multiplied by the ratio of the sentencepiece tokenization to number of tokens in the original dataset. The original dataset token count per split is: train=1973136207, validation=3007061, test=6966499. Our sentencepiece tokenization yields the following token counts per split: train=3084760726, valid=4656945, and test=10699704. This gives log likelihood multipliers of train=1.5634, valid=1.5487, test=1.5359 per split before computing perplexity, which is equal to .

Preprocessing for TrEMBL is extensively explained in Appendix \ref{appendix:protein_extended}.

\subsection{Training Hyperparameters} 
Unless specifically stated, all Performer + Transformer runs by default used  grad clip,  weight decay,  dropout,  fixed learning rate with Adam hyperparameters , with batch size maximized (until TPU memory overload) for a specific model. 

All 36-layer protein experiments used the same amount of compute (i.e. 16x16 TPU-v2, 8GB per chip). For concatenated experiments, 16x16 TPU-v2's were also used for the Performer, while 8x8's were used for the 1-3 layer  Transformer models (using 16x16 did not make a difference in accuracy).

\textbf{Note that Performers are using the same training hyperparameters as Transformers, yet achieving competitive results} - this shows that FAVOR can act as a simple drop-in without needing much tuning.

\subsection{Approximate Softmax Attention Default Values} The optimal values, set to default parameters\footnote{\url{https://github.com/google-research/google-research/blob/master/performer/fast_attention}}
, are: renormalize\_attention = True, numerical stabilizer = , number of features = 256, ortho\_features = True, ortho\_scaling = 0.0.

\subsection{Generalized Attention Default Values} \label{subsec:generalized_default} The optimal values, set to default parameters\footnote{\url{https://github.com/google-research/google-research/blob/master/performer/fast_attention}}
, are: renormalize\_attention = True, numerical stabilizer = 0.0, number of features = 256, kernel = ReLU, kernel\_epsilon = . 

\subsection{Reformer Default Values} \label{subsec:reformer_default}
For the Reformer, we used the same hyperparameters as mentioned for protein experiments, without gradient clipping, while using the defaults\footnote{\url{https://github.com/google/trax/blob/master/trax/supervised/configs/reformer_imagenet64.gin}} (which instead use learning rate decay) for ImageNet-64. In both cases, the Reformer used the same default LSH attention parameters.

\subsection{Linformer Default Values} \label{subsec:linformer_default}
Using our standard pipeline as mentioned above, we replaced the attention function with the Linformer variant via Jax, with  (same notation used in the paper \citep{linformer}), where  is the exponent in a renormalization procedure using  as a multiplier in order to approximate softmax, while  is the dimension of the projections of the  and  matrices. As a sanity check, we found that our Linformer implementation in Jax correctly approximated exact softmax's output within  error for all entries.

Note that for rigorous comparisons, our Linformer hyperparameters are even stronger than the defaults found in \citep{linformer}, as:

\begin{itemize}
\item We use , which is more than twice than the default  from the paper, and also twice than our default  number of features.
\item We also use redrawing, which avoids "unlucky" projections on  and . 
\end{itemize}


\newpage

\section{Main Algorithm: FAVOR+}
\label{appendix:main_algorithm}
We outline the main algorithm for FAVOR+ formally: 

\begin{algorithm}[h]
\SetAlgoLined
\SetKwInOut{Input}{Input}
\Input{ ,  - binary flag.}
\KwResult{ if ,  otherwise.}
Compute  and  as described in Section \ref{sec:gka} and Section \ref{howto} and take \;
\eIf{}{
    \;
}{
    Compute  and its prefix-sum tensor  according to (\ref{eq:cumsum})\;
    \;
}
\;
\Return \;
\caption{FAVOR+ (bidirectional or unidirectional).}
\label{alg:1}
\end{algorithm}


\subsection{Unidirectional Case and Prefix Sums}
\label{subsec:unidirectional}
We explain how our analysis from Section \ref{sec:gka} can be extended to the unidirectional mechanism in this section.
Notice that this time attention matrix  is masked, i.e. all its entries not in the lower-triangular part (which contains the diagonal) are zeroed (see also Fig. \ref{fig:prefix_sum}).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/prefix_sum.jpg}
  \caption{\small{Visual representation of the prefix-sum algorithm for unidirectional attention. For clarity, we omit attention normalization in this visualization. The algorithm keeps the prefix-sum which is a matrix obtained by summing the outer products of random features corresponding to keys with value-vectors. At each given iteration of the prefix-sum algorithm, a random feature vector corresponding to a query is multiplied by the most recent prefix-sum (obtained by summing all outer-products corresponding to preceding tokens) to obtain a new row of the matrix  which is output by the attention mechanism.\\}}
  \label{fig:prefix_sum}
\end{figure}

For the unidirectional case, our analysis is similar as for the bidirectional case, but this time our goal is to compute  without constructing and storing the -sized matrix  explicitly, where
. In order to do so, observe that :

where  are 3d-tensors. Each slice  is therefore a result of a prefix-sum (or cumulative-sum) operation applied to : . An efficient algorithm to compute the prefix-sum of  elements takes  total steps and  time when computed in parallel \citep{cumsum, cormen}. 
See Algorithm \ref{alg:1} for the whole approach.


\subsection{Orthogonal Random Features - Extensions}
\label{subsec:extensions}

As mentioned in the main text, for isotropic  (true for most practical applications, including regular attention), instead of sampling  independently, we can use \emph{orthogonal random features} (ORF) \citep{ort, unreas, geom}: these maintain the marginal distributions of samples  while enforcing that different samples are orthogonal. If we need , ORFs still can be used locally within each  block of  \citep{ort}.

ORFs were introduced to reduce the variance of Monte Carlo estimators \citep{ort, unreas, geom, kama, hron,psrnn, uni} and we showed in the theoretical and experimental sections from the main body that they do indeed lead to more accurate approximations and substantially better downstream results. There exist several variants of the ORF-mechanism and in the main body we discussed only the base one (that we refer to here as \textit{regular}). Below we briefly review the most efficient ORF mechanisms (based on their strengths and costs) to present the most complete picture.

\textbf{(1) Regular ORFs [R-ORFs]:} Applies Gaussian orthogonal matrices \citep{ort}. Encodes matrix  of -samples (with different rows corresponding to different samples) in  space. Provides algorithm for computing  in  time for any . Gives unbiased estimation. Requires one-time  preprocessing (Gram-Schmidt orthogonalization).

\textbf{(2) Hadamard/Givens ORFs [H/G-ORFs]:} Applies random Hadamard \citep{unreas} or Givens matrices \citep{uni}. Encodes matrix  in  or  space. Provides algorithm for computing  in  time for any . Gives small bias (tending to  with ).

\subsection{Time and Space Complexity - Detailed Analysis}

We see that a variant of bidirectional FAVOR+ using iid samples or R-ORFs has  space complexity as opposed to  space complexity of the baseline. Unidirectional FAVOR+ using fast prefix-sum pre-computation in parallel \citep{cumsum, cormen} has  space complexity to store  which can be reduced to  by running a simple (though non-parallel in ) aggregation of  without storing the whole tensor  in memory. From Subsec. \ref{subsec:extensions}, we know that if instead we use G-ORFs, then space complexity is reduced to  and if the H-ORFs mechanism is used, then space is further reduced to . Thus for  all our variants provide substantial space complexity improvements since they do not need to store the attention matrix explicitly.

The time complexity of Algorithm \ref{alg:1} is  (note that constructing  and  can be done in time ). Note that the time complexity of our method is much lower than  of the baseline for .

As explained in Subsec. \ref{subsec:extensions}, the R-ORF mechanism incurs an extra one-time  cost (negligible compared to the  term for ). H-ORFs or G-ORFs do not have this cost, and when FAVOR+ uses them, computing  and  can be conducted in time  as opposed to  (see: Subsec. \ref{subsec:extensions}). Thus even though H/G-ORFs do not change the asymptotic time complexity, they improve the constant factor from the leading term. This might play an important role in training very large models. 

The number of random features  allows a trade-off between computational complexity and the level of approximation: bigger  results in higher computation costs, but also in a lower variance of the estimate of . In the theoretical section from the main body we showed that in practice we can take .

Observe that the FAVOR+ algorithm is highly-parallelizable, and benefits from fast matrix multiplication and broadcasted operations on GPUs or TPUs.

\newpage

\section{Experimental Details for Protein Modeling Tasks}
\label{appendix:protein_extended}
\subsection{TrEMBL Dataset}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.4}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c|c|c|c|c| } 
\hline
\multirow{2}{*}{\bf Dataset} & \multirow{2}{*}{\bf Set Name} & \multirow{2}{*}{\bf Count} & \multicolumn{5}{|c|}{\bf Length Statistics} \\
\cline{4-8}
 & &  & \textbf{Min} & \textbf{Max}& \textbf{Mean}& \textbf{STD} & \textbf{Median} \\
\hline
\multirow{4}{*}{TrEMBL}
& Train & 104,863,744 & 2 & 74,488 & 353.09 & 311.16 & 289.00 \\ 
& Valid & 102,400 & 7 & 11,274 & 353.62 & 307.42 & 289.00 \\ 
\cline{2-8}
& Test & 1,033,216 & 8 & 32,278 & 353.96 & 312.23 & 289.00 \\ 
& OOD & 29,696 & 24 & 4,208 & 330.96 & 269.86 & 200.00 \\ 
\hline
& & & & & & & \vspace{-4.5mm}\\
\hline
\multirow{2}{*}{\begin{tabular}{c}TrEMBL\\ (concat)\end{tabular}} & Train & 4,532,224 & \multirow{2}{*}{8,192} & \multirow{2}{*}{8,192} & \multirow{2}{*}{8,192} & \multirow{2}{*}{0} & \multirow{2}{*}{8,192}\\
& Valid & 4,096 & & & & & \\ 
\hline
\end{tabular}
}
\vspace{2mm}
\caption{Statistics for the TrEMBL single sequence and the long sequence task.}
\label{table-trembl-statistics}
\end{table}

We used the TrEMBL dataset\footnote{\url{https://www.uniprot.org/statistics/TrEMBL}}, which contains 139,394,261 sequences of which 106,030,080 are unique. While the training dataset appears smaller than the one used in Madani et al. \citep{progen}, we argue that it includes most of the relevant sequences. Specifically, the TrEMBL dataset consists of the subset of UniProtKB sequences that have been computationally analyzed but not manually curated, and accounts for  of the total number of sequences in the UniProtKB dataset\footnote{\url{https://www.uniprot.org/uniprot/}}.

Following the methodology described in Madani et al. \citep{progen}, we used both an OOD-Test set, where a selected subset of Pfam families are held-out for valuation, and an IID split, where the remaining protein sequences are split randomly into train, valid, and test tests. We held-out the following protein families (PF18369, PF04680, PF17988, PF12325, PF03272, PF03938, PF17724, PF10696, PF11968, PF04153, PF06173, PF12378, PF04420, PF10841, PF06917, PF03492, PF06905, PF15340, PF17055, PF05318), which resulted in 29,696 OOD sequences. We note that, due to deduplication and potential TrEMBL version mismatch, our OOD-Test set does not match exactly the one in Madani et al. \citep{progen}. We also note that this OOD-Test selection methodology does not guarantee that the evaluation sequences are within a minimum distance from the sequences used during training. In future work, we will include rigorous distance based splits.

The statistics for the resulting dataset splits are reported in Table \ref{table-trembl-statistics}. In the standard sequence modeling task, given the length statistics that are reported in the table, we clip single sequences to maximum length , which results in few sequences being truncated significantly.

In the long sequence task, the training and validation sets are obtained by concatenating the sequences, separated by an end-of-sequence token, and grouping the resulting chain into non-overlapping sequences of length .



\subsection{Empirical Baseline}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\linewidth]{img/empirical_distribution.png}
  \caption{\small{Visualization of the estimated empirical distribution for the 20 standard amino acids, colored by their class. Note the consistency with the statistics on the TrEMBL web page.}}
  \vspace{2mm}
  \label{figure-empirical-baseline}
\end{figure}

A random baseline, with uniform probability across all the vocabulary tokens at every position, has accuracy  (when including only the 20 standard amino acids) and  (when also including the 5 anomalous amino acids \citep{uniprot2019uniprot}). However, the empirical frequencies of the various amino acids in our dataset may be far from uniform, so we also consider an \textit{empirical baseline} where the amino acid probabilities are proportional to their empirical frequencies in the training set.

Figure \ref{figure-empirical-baseline} shows the estimated empirical distribution. We use both the standard and anomalous amino acids, and we crop sequences to length 1024 to match the data processing performed for the Transformer models. The figure shows only the 20 standard amino acids, colored by their class, for comparison with the visualization on the TrEMBL web page\footnote{\url{https://www.uniprot.org/statistics/TrEMBL}}.

\subsection{Tabular Results}

Table \ref{table-trembl} contains the results on the single protein sequence modeling task (). We report accuracy and perplexity as defined in Appendix \ref{appendix:hyperparameters}:

\begin{comment}
We report the following evaluation metrics:
\begin{enumerate}
\item \textbf{Accuracy}: For unidirectional models, we measure the accuracy on next-token prediction, averaged across all sequence positions in the dataset. For bidirectional models, we mask each token with  probability and measure accuracy across the masked positions.
\item \textbf{Perplexity}: For unidirectional models, we measure perplexity across all sequence positions in the dataset. For bidirectional models, similar to the accuracy case, we measure perplexity across the masked positions. \end{enumerate}
\end{comment}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.4}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c| } 
\hline
\textbf{Model Type} & \textbf{Set Name} & \textbf{Model} & \textbf{Accuracy} & \textbf{Perplexity} \\
\hline
\multirow{6}{*}{UNI} & \multirow{3}{*}{Test} & Empirical Baseline & 9.92 & 17.80 \\ 
& & Transformer & 30.80 & 9.37\\ 
& & Performer (generalized) & 31.58 & 9.17 \\ 
\cline{2-5}
& \multirow{3}{*}{OOD} & Empirical Baseline & ~9.07 & ~17.93 \\ 
& & Transformer & 19.70 & 13.20 \\ 
& & Performer (generalized) & 18.44 & 13.63 \\
\hline
& & & &\vspace{-4.5mm}\\
\hline
\multirow{6}{*}{BID} & \multirow{3}{*}{Test} & Transformer & 33.32 & 9.22 \\ 
& & Performer (generalized) & 36.09 &  8.36 \\ 
& & Performer (softmax) & 33.00 &  9.24 \\ 
\cline{2-5}
& \multirow{3}{*}{OOD} & Transformer & 25.07 & 12.09 \\ 
& & Performer (generalized) & 24.10 & 12.26 \\ 
& & Performer (softmax) & 23.48 & 12.41 \\ 
\hline
\end{tabular}
}
\vspace{2mm}
\caption{Results on single protein sequence modeling (). We note that the empirical baseline results are applicable to both the unidirectional (UNI) and bidirectional (BID) models.}\label{table-trembl}
\end{table}

\begin{comment}
\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.4}
\resizebox{0.9\textwidth}{!}{\begin{tabular}{ |c|c|c|c|c| } 
\hline
\textbf{Model Type} & \textbf{Set Name} & \textbf{Model} & \textbf{Accuracy} & \textbf{Perplexity} \\
\hline
\multirow{14}{*}{BID} & \multirow{3}{*}{Test-1} & Empirical Baseline & 9.92 & 17.80 \\
& & Transformer & {\color{red} ?} & {\color{red} ?}\\ 
& & Performer (generalized) & {\color{red} ?} & {\color{red} ?}\\ 
\cline{2-5}
& \multirow{2}{*}{Test-5} & Transformer & {\color{red} ?} & {\color{red} ?}\\ 
& & Performer (generalized) & {\color{red} ?} & {\color{red} ?}\\ 
\cline{2-5}
& \multirow{2}{*}{Test-10} & Transformer & {\color{red} ?} & {\color{red} ?}\\ 
& & Performer (generalized) & {\color{red} ?} & {\color{red} ?}\\ 
\cline{2-5}
& & & &\vspace{-4.5mm}\\
\cline{2-5}
& \multirow{3}{*}{OOD-1} & Empirical Baseline & 9.07 & 17.93 \\ 
& & Transformer & {\color{red} ?} & {\color{red} ?}\\ 
& & Performer (generalized) & {\color{red} ?} & {\color{red} ?}\\ 
\cline{2-5}
& \multirow2{*}{OOD-5} & Transformer & {\color{red} ?} & {\color{red} ?}\\ 
& & Performer (generalized) & {\color{red} ?} & {\color{red} ?}\\ 
\cline{2-5}
& \multirow{2}{*}{OOD-10} & Transformer & {\color{red} ?} & {\color{red} ?}\\ 
& & Performer (generalized) & {\color{red} ?} & {\color{red} ?}\\ 
\hline
\end{tabular}
}
\vspace{2mm}
\caption{Results on concatenated protein sequence modeling on the TrEMBL dataset. }\label{table-trembl-concatenated}
\end{table}
\end{comment}



\subsection{Attention Matrix Illustration}

In this section we illustrate the attention matrices produced by a Performer model. We focus on the bidirectional case and choose one Performer model trained on the standard single-sequence TrEMBL task for over 500K steps. The same analysis can be applied to unidirectional Performers as well.

We note that while the Transformer model instantiates the attention matrix in order to compute the attention output that incorporates the (queries , keys , values ) triplet (see Eq.~\ref{eq:attnorm} in the main paper), the FAVOR mechanism returns the attention output directly (see Algorithm~\ref{alg:1}). To account for this discrepancy, we extract the attention matrices by applying each attention mechanism twice: once on each original  triple to obtain the attention output, and once on a modified  triple, where  contains one-hot indicators for each position index, to obtain the attention matrix. The choice of  ensures that the dimension of the attention output is equal to the sequence length, and that a non-zero output on a dimension  can only arise from a non-zero attention weight to the  sequence position. Indeed, in the Transformer case, when comparing the output of this procedure with the instantiated attention matrix, the outputs match.

\textbf{Attention matrix example.} We start by visualizing the attention matrix for an individual protein sequence. We use the BPT1\_BOVIN protein sequence\footnote{\url{https://www.uniprot.org/uniprot/P00974}}, one of the most extensively studied globular proteins, which contains 100 amino acids. In Figure \ref{fig:attention_matrices}, we show the attention matrices for the first 4 layers. Note that many heads show a \textit{diagonal} pattern, where each node attends to its neighbors, and some heads show a \textit{vertical} pattern, where each head attends to the same fixed positions. These patterns are consistent with the patterns found in Transformer models trained on natural language \citep{kovaleva2019revealing}. In Figure \ref{fig:model_view} we highlight these attention patterns by focusing on the first 25 tokens, and in Figure \ref{fig:attention_heads}, we illustrate in more detail two attention heads.

\textbf{Amino acid similarity.} Furthermore, we analyze the amino-acid similarity matrix estimated from the attention matrices produced by the Performer model, as described in Vig et al. \citep{bertology}. We aggregate the attention matrix across 800 sequences. The resulting similarity matrix is illustrated in Figure \ref{figure:amino-acid-similarity}. Note that the Performer recognizes highly similar amino acid pairs such as (D, E) and (F, Y).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/attention_matrices.png}
  \vspace{2mm}
  \caption{\small{We show the attention matrices for the first 4 layers and all 8 heads (each row is a layer, each column is head index, each cell contains the attention matrix across the entire BPT1\_BOVIN protein sequence). Note that many heads show a \textit{diagonal} pattern, where each node attends to its neighbors, and some heads show a \textit{vertical} pattern, where each head attends to the same fixed positions.}}
  \label{fig:attention_matrices}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.49\textwidth]{img/head_1_2.png}
  \includegraphics[width=0.49\textwidth]{img/head_4_1.png}
  \vspace{2mm}
  \caption{\small{We illustrate in more detail two attention heads. The sub-figures correspond respectively to: \textbf{(1)} Head 1-2 (second layer, third head), \textbf{(2)} Head 4-1 (fifth layer, second head). Note the block attention in Head 1-2 and the vertical attention (to the start token (`M') and the 85th token (`C')) in Head 4-1.}}
  \label{fig:attention_heads}
\end{figure}

\newpage

\begin{figure}[ht]
    \centering
  \includegraphics[height = 0.6\linewidth, width=0.9\linewidth]{img/model_view.png}
  \vspace{2mm}
  \caption{\small{We highlight the attention patterns by restricting our attention to the first 25 tokens (note that we do not renormalize the attention to these tokens). The illustration is based on Vig et al. \protect\citep{vig2019multiscale, analyzing_attention}. Note that, similar to prior work on protein Transformers \protect\citep{progen}, the attention matrices include both local and global patterns.}}
  \label{fig:model_view}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\textwidth]{img/blosum.pdf}
  \includegraphics[width=0.49\textwidth]{img/performer.pdf}
  \vspace{1mm}
  \caption{\small{Amino acid similarity matrix estimated from attention matrices aggregated across a small subset of sequences, as described in Vig et al. \protect\citep{bertology}. The sub-figures correspond respectively to: \textbf{(1)} the normalized BLOSUM matrix, \textbf{(2)} the amino acid similarity estimated via a trained Performer model. Note that the Performer recognizes highly similar amino acid pairs such as (D, E) and (F, Y).}}
  \label{figure:amino-acid-similarity}
\end{figure}

\clearpage

\section{Extended approximation and comparison results}
\label{appendix:extended_approx}

\subsection{Backwards Compatibility - Error Propagation}
Although mentioned previously (Sec. \ref{subsec:approx_error_compatibility}) that the Performer with additional finetuning is backwards compatible with the Transformer, we demonstrate below in Fig. \ref{fig:appendix_approx} that error propagation due to non-attention components of the Transformer is one of the primary reasons that pretrained Transformer weights cannot be immediately used for inference on the corresponding Performer.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{img/transformer_output_layer.png}
  \caption{Output approximation errors between a vanilla Transformer and a Performer (with orthogonal features) for varying numbers of layers.}
  \label{fig:appendix_approx}
\end{figure}

\begin{comment}
\subsection{Redrawing in Softmax}
\label{subsec:redraw}
While the benefits of redrawing features was shown in Subsec. \ref{subsec:softmax_approx_transformer} of the main body of the paper, we also demonstrate its benefits when there are multiple layers with large scale (16x16 TPU-v2) training. This can be found below in Fig. \ref{fig:appendix_redraw}.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/progen_B_redraw.png}
  \caption{The importance of redrawing features. If redrawing is not used, an "unlucky" set of random features may cause training degradation, shown by the early-stopped curve with Seed 1, while a 'lucky' set of random features may cause no issue, shown by the curve with Seed 2. Redrawing allows the training to correct itself, as seen at the black vertical line.}
  \label{fig:appendix_redraw}
\end{figure}



\subsection{Softmax Unidirectional}
\label{subsec:softmax_unidirectional}
While we have shown on TrEMBL that Performer with generalized ReLU attention outperforms softmax, we also show that approximate softmax attention can still be a solid choice, for example on ImageNet64 (U), in Fig. \ref{fig:im64_relu_softmax}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/im64_relu_softmax.png}
  \caption{Using the same 8x8 TPU-v2 compute and same 6-layer standard model, approximate softmax with positive features achieves the same result as generalized ReLU attention.}
  \label{fig:im64_relu_softmax}
\end{figure}

\end{comment}
\subsection{Approximate Softmax - Extended Properties}
\label{subsec:redraw}
\label{subsec:softmax_unidirectional}
\label{subsec:unstable_trig}
We show the following properties of our softmax approximation, in Fig. \ref{fig:appendix_redraw}:

\textbf{Redrawing:} While the benefits of redrawing features was shown in Subsec. \ref{subsec:softmax_approx_transformer} of the main body of the paper, we also demonstrate its benefits when there are multiple layers with large scale (16x16 TPU-v2) training.

\textbf{Unidirectional:} While we have shown on TrEMBL that Performer with generalized ReLU attention outperforms softmax, we also show that approximate softmax attention can still be a solid choice, for example on ImageNet64 (U). After 100K steps of training, the Performer-ReLU, Performer-Softmax, and Performer-Softmax (SMREG) variants achieve respectively, 3.67, 3.69, 3.67 BPD.

\textbf{Instability of Trigonometric Features:} We see the full view of the unstable training curve when using Trigonometric softmax.

\vspace{2mm}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/three_softmax.png}
  \caption{Best viewed zoomed in. \textbf{Left:} The importance of redrawing features. If redrawing is not used, an "unlucky" set of random features may cause training degradation, shown by the early-stopped curve with Seed 1, while a `lucky' set of random features may cause no issue, shown by the curve with Seed 2. Redrawing allows the training to correct itself, as seen at the black vertical line. \textbf{Middle:} Using the same 8x8 TPU-v2 compute and same 6-layer standard model, approximate softmax with positive features achieves the same result as generalized ReLU attention. \textbf{Right:} Zoomed out view of right subfigure of Fig. \ref{fig:backward_compatibility}, showing that Trigonometric softmax causes very unstable training behaviors.
  }
  \label{fig:appendix_redraw}
  \label{fig:im64_relu_softmax}
  \label{fig:pg19_full_softmax}
\end{figure}



\subsection{Generalized Attention}
\label{subsec:appendix_generalized_attention}
We investigated Generalized Attention mechanisms (mentioned in Sec. \ref{sec:gka}) on TrEMBL when  for various kernel functions. This is similar to \citep{tsai2019transformer} which also experiments with various attention kernels for natural language. Using hyperparameter sweeps across multiple variables in FAVOR, we compared several kernels and also renormalization on/off (Fig. \ref{fig:attention_comparisons_2x2} and Fig. \ref{fig:attention_comparisons_4x4}), where  corresponds to applying  operator in attention, as for the standard mechanism, though we noticed that disabling it does not necessarily hurt accuracy) to produce the best training configuration for the Performer. We note that the effective batch size slightly affects the rankings (as shown by the difference between 2x2 and 4x4 TPU runs) - we by default use the generalized ReLU kernel with other default hyperparameters shown in Appendix \ref{appendix:hyperparameters}, as we observed that they are empirically optimal for large batch size runs (i.e. 8x8 or 16x16 TPU's).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/attention_comparisons_2x2.png}
  \caption{To emphasize the highest accuracy runs but also show the NaN issues with certain kernels which caused runs to stop early, we set both x and y axes to be log-scale. We tested kernels defined by different functions  (see: Sec. \ref{sec:gka}): sigmoid, exponential, ReLU, absolute, gelu, cosine (original softmax approximation), tanh, and identity. All training runs were performed on 2x2 TPU-v2's, 128 batch size per device.}
  \label{fig:attention_comparisons_2x2}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/attention_comparisons_4x4.png}
  \caption{We also performed a similar setup as Fig. \ref{fig:attention_comparisons_2x2} for 4x4 TPU-v2's.}
  \label{fig:attention_comparisons_4x4}
\end{figure}

\subsection{Comparison with Linear Transformer}
\label{appendix:linear_transformer}

We use the attention implementation of the Linear Transformer from \citep{trans-rnns}, which mainly involves setting our feature map , where  is the shifted-eLU function from \citep{elu}. 

\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/linear_transformer.png}
  \caption{\textbf{Left:} In the unidirectional 36-ProGen setting, we ran 3 seeds of the Linear Transformer, and found that all 3 seeds produced exploding gradients very early on, stopping the training run. \textbf{Right:} The Linear Transformer in the bidirectional setting also produced an exploding gradient in the middle of training, near 125K steps. Exploding gradients can be evidenced by the sharp drop in train accuracy right before a NaN error.}
  \label{fig:linear_transformer}
\end{figure}

For the sake of fairness and to prevent confounding results, while \citep{trans-rnns} also uses the GeLU nonlinearity for the MLPs in the Linear Transformer, we instead use the original ReLU nonlinearity. We also used the exact same training hyperparameters as Performer-ReLU on our exact ProGen setting from Fig. \ref{fig:big_benchmarking}. Ultimately, we empirically found that the Linear Transformer possessed numerical instability during training via unstable training curves, \textbf{ultimately stopping training by producing exploding gradients (NaNs)} (Fig. \ref{fig:linear_transformer}).

\subsection{Long Range Arena}
Performers are compared against many additional (scalable and not scalable) methods not included in our paper: \textit{Local Attention}, \textit{Sparse Attention}, \textit{Longformer}, \textit{Sinkhorn Transformer}, \textit{Synthesizer}, \textit{Big Bird} and the aforementioned \textit{Linear Transformer} on challenging long range context tasks in the Long Range Arena \citep{lra}, with Fig. \ref{fig:lra_figure} displaying the original paper's results. Performers obtain the largest LRA (Long Range Arena) score among all tested \textbf{scalable} Transformers methods (which we define by having speed of > 100 examples/sec). 

Tasks used for comparison include: \textbf{(1)} a longer variation of the standard ListOps task proposed in \citep{listops}, \textbf{(2)} byte-level text classification using real-world data, \textbf{(3)} byte-level document retrieval, \textbf{(4)} image classification on sequences of pixels, and \textbf{(5)} Pathfinder task (long-range spatial dependency problem). In the Long Range Arena paper, the authors found that all models do not learn anything on Path-X task (denoted by FAIL), contrary to the Pathfinder task, which shows that increasing the sequence length can cause seriously difficulties for model training.
\vspace{0.2cm}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.99\textwidth]{img/lra-all-3.pdf}
  \vspace{0.15cm}
  \caption{\textbf{Upper Table:} Results on Long-Range Arena benchmark. Best model is in boldface and second best is underlined. \textbf{Lower Table:} Benchmark results of all X-former models with a consistent batch size of 32 across all models. The authors report relative speed increase/decrease in comparison with the vanilla Transformer in brackets besides the steps per second. Memory usage refers to per device memory usage across each TPU device. Benchmarks are run on 4x4 TPU-v3 chips. \textbf{Right Fig:} Performance (y-axis), speed (x-axis), and memory footprint (size of the circles) of different models.} 
  \label{fig:lra_figure}
\end{figure}

\newpage 

\section{Computation costs - Extended results}

In this subsection, we empirically measure computational costs in terms wall clock time on forward and backward passes for three scenarios in Fig. \ref{fig:appendix_runtime}:
\begin{enumerate}
\item Performer, with varying number of layers. We show that our method can scale up to (but not necessarily limited to) even 20 layers.
\item Attention time complexities when comparing standard attention (from Transformer) and FAVOR (from Performer). Note that the maximum memory size here is not reflective of the maximum memory size in an actual model (shown below), as this benchmark requires computing explicit tensors (causing memory increases) in Jax, while a model does not.
\item Time complexities when comparing the Transformer and Performer models. "X" (OPT) denotes the maximum possible speedup achievable, when attention simply returns the -vector, showing that the Performer is nearly optimal. We see that the maximum possible power of 2 length allowed on a V100 GPU (16GB) is  using regular dimensions.
\end{enumerate}

Since some of the computational bottleneck in the Transformer may originate from the extra feed-forward layers \citep{reformer}, we also benchmark the ``Small" version, i.e.  as well, when the attention component is the dominant source of computation and memory. We remind the reader that the ``Regular" version consists of  . 

\vspace{0.1cm}
\label{appendix:computation_costs_bidirectional}

\begin{comment}
\begin{figure}
  \includegraphics[width=1.0\textwidth]{img/performer_layers_small.png}
  \includegraphics[width=1.0\textwidth]{img/performer_layers_regular.png}
  \caption{Varying layers when using Performer. We show that our method can scale up to (but not necessarily limited to) even 20 layers.}
  \label{fig:appendix_layers}
\end{figure}


\begin{figure}
  \includegraphics[width=1.0\textwidth]{img/attention_small.png}
  \includegraphics[width=1.0\textwidth]{img/attention_regular.png}
  \caption{Attention time complexities when comparing standard attention (from Transformer) and FAVOR (from Performer). Note that the maximum memory size here is not reflective of the maximum memory size in an actual model (shown below), as this benchmark requires computing explicit tensors (causing memory increases) in Jax, while a model does not.}
  \label{fig:appendix_attention}
\end{figure}


\begin{figure}
  \includegraphics[width=1.0\textwidth]{img/model_small_opt.png}
  \includegraphics[width=1.0\textwidth]{img/model_regular_opt.png}
  \caption{Time complexities when comparing the Transformer and Performer models. "X" (OPT) denotes the maximum possible speedup achievable, when attention simply returns the -vector, showing that the Performer is nearly optimal. We see that the maximum possible length allowed on a V100 GPU (16GB) is  using regular dimensions.}
  \label{fig:appendix_transformer}
\end{figure}
\end{comment}


\begin{figure}[h]
  \includegraphics[width=1.0\textwidth]{img/performer_layers_small.png}
  \includegraphics[width=1.0\textwidth]{img/performer_layers_regular.png}
  \includegraphics[width=1.0\textwidth]{img/attention_small.png}
  \includegraphics[width=1.0\textwidth]{img/attention_regular.png}

  \caption{Captions (1) and (2) for each 2x2 subfigure mentioned above.}
  \label{fig:appendix_runtime}
\end{figure}

\clearpage
\begin{figure}[ht]
  \includegraphics[width=1.0\textwidth]{img/model_small_opt.png}
  \includegraphics[width=1.0\textwidth]{img/model_regular_opt.png}

  \caption{Caption (3) for this 2x2 subfigure mentioned above.}
  \label{fig:appendix_runtime_2}
\end{figure}





\clearpage

\section{Theoretical results}
\label{appendix:theoretical_results}
We provide here the proofs of all theoretical results presented in the paper.

\subsection{Proof of Lemma \ref{pos_random_features_lemma}}

\begin{proof}
We first deduce that for any 

Next, let . We use the fact that

for any  and derive:

That completes the proof of the first part of the lemma. An identity involving hyperbolic cosine function is implied by the fact that for every  and  the following is true:

The cancellation of the odd moments  follows directly from the fact that  is taken from the isotropic distribution (i.e. distribution with pdf function constant on each sphere).
That completes the proof.
\end{proof}

\subsection{Proof of Lemma \ref{mse-lemma}}
\begin{proof}
Denote:  and .
Note that by using standard trigonometric identities (and the fact that the variance of the sum of independent random variables is the sum of variances of those random variables), we can get the following for :

Using the fact that (see: Lemma 1 in \citep{ort}; note that in that lemma they use notation:  for what we denote as: ):

we obtain:

which completes the first part of the proof.
To obtain the formula for: 
notice first that:

The above immediately follows from the fact that positive random feature maps provide unbiased estimation of the softmax-kernel, thus the following is true:


Therefore we obtain:

where the last equality follows from Equation \ref{neat-fact}.
Therefore we have:


Finally, 

In the chain of equalities above we used the fact that random variables  and 
 have the same distribution. This is true since  and  have the same distribution ( is Gaussian).
That completes the proof.
\end{proof}

\subsection{Proof of Theorem \ref{reg-theorem}}
\begin{proof}
Let  be respectively a query/key.
Note that from the definition of  we have
for :

where . To obtain the above we used the fact that  is isotropic (that in particular implies zeroing of the even terms in the Taylor expansion).

Let us denote: . It turns out that:

The proof of that fact can be found in the supplement of \citep{geom}, yet we provide it below for completeness and the convenience of the Reader:
\begin{lemma}
Expression  satisfies the following for  :

\end{lemma}
\begin{proof}
Note first that for  the density function  of the angle between a vector  chosen uniformly at random from the unit sphere and  is given by the following formula:

Let us denote: .
Using partial integration, we get:

Thus we conclude that: .
Therefore we have: 

We again conduct partial integration and get:

Therefore we conclude that:

which completes the proof.
\end{proof}
Applying the above lemma, we get:

where 
and .

Thus we obtain:

Note first that for  we have: , thus:

We also have for :

where  stands for the random variable of Poisson distribution with parameter .
Therefore we get for :

where the last equality is implied by the formula for the Laplace Transform for the Poisson random variable:

Notice that: 
.
We conclude that:

That completes the proof.
\end{proof}

\subsection{Proofs of Theorem \ref{var-theorem},Theorem \ref{ort-theorem} \& Beautiful Functions}

We will provide here much more general theoretical results which will imply  Theorem \ref{ort-theorem} and Theorem \ref{var-theorem}. We need the following definition:

\begin{definition}
\label{beauty-definition}
We say that function  is beautiful if  can be expressed as:

for a probabilistic isotropic distribution , and where  is an entire function
with non-negative power-series coefficients 
(i.e.  for every  and with  for ).
In the formula above we assume that the expectation on the RHS exists.
\end{definition}

Interestingly, beautiful functions can be used to define softmax and consequently, Gaussian kernels (both standard and regularized), leading to our PRF mechanism presented in the main body of the paper, as we explain below.

\begin{remark}
\label{imp_remark}
If one takes (note that  is isotropic) and  (such  is clearly entire with nonnegative power-series coefficient) then the following is true for :

Similarly: , where  stands for the distribution corresponding to Haar measure on the sphere of radius  (which is clearly isotropic). 
Therefore general concentration results for Monte Carlo estimators of beautiful functions immediately imply corresponding results for the (standard and regularized) softmax (and thus also Gaussian) kernel.
\end{remark}

We will consider two estimators of the beautiful functions from Definition \ref{beauty-definition} that directly lead (through Remark \ref{imp_remark}) to: PRF-based approximation of the softmax-kernel and its enhanced version with orthogonal features. Standard Monte Carlo estimator samples independently , where  stands for the number of samples and then computes:

Orthogonal Monte Carlo estimator samples  () in such a way that marginally we have: , but  for  (such an orthogonal ensemble can be always created if  is isotropic, as we already mentioned in the main body of the paper). We define:


\subsubsection{Orthogonality universally improves concentration}

Denote by  a moment generating function of the random variable .
Note first that estimators of beautiful functions based on standard Monte Carlo procedure using independent vectors  guarantee strong concentration bounds since 
independent s provide a way to obtain exponentially small upper bounds on failure probabilities through moment generating functions.
We summarize this classic observation which is a standard application of Markov's Inequality below. \\

\begin{lemma}
\label{iid-lemma}
Consider an estimator  of the beautiful function  evaluated at . Then the following holds for any , :

where , .
\end{lemma}

The above result provides us with exponentially small (in Legendre Transform) upper bounds on tail probabilities for the standard estimator.
Below we provide our two main theoretical results. 

\begin{theorem}[orthogonality provides smaller tails]
\label{general-ort-theorem}
If  is a beautiful function then the following holds for ,  as in Lemma \ref{iid-lemma} and any , : 

\end{theorem}

This result shows that features obtained from the ensembles of pairwise orthogonal random vectors provide exponentially small bounds on tail probabilities and that these bounds are strictly better than for estimators using unstructured features. Furthermore, the result is \textbf{universal}, i.e. holds for any dimensionality , not just asymptotically for  large enough.

We also obtain similar result regarding mean squared errors (MSEs) of the considered estimators:

\begin{theorem}
\label{general-var-theorem}
If  is a beautiful function then the following holds for : 

\end{theorem}
As before, an orthogonal estimator leads to better concentration results and as before, this is the case for any , not only asymptotically for large enough .

\textbf{Note that from what we have said above, Theorem \ref{var-theorem} and Theorem \ref{ort-theorem} follow immediately from Theorem \ref{general-var-theorem} and Theorem \ref{general-ort-theorem} respectively.} 

Thus in the remainder of this section we will prove Theorem \ref{general-var-theorem} and Theorem \ref{general-ort-theorem}.

\subsubsection{Proof of Theorem \ref{general-ort-theorem}}

\begin{proof}
Note that by the analogous application of Markov's Inequality as in Lemma \ref{iid-lemma}, we get:

where we have:
.
We see that it suffices to show that for any  the following holds: 
.
We have: 

where .


Thus we have:


Similarly, we get:


Therefore we get:




Note first that using the fact that  is entire, we can rewrite each  as:


where 
and .
Similarly,


By plugging in the above formulae for  and  int the formula for  and expanding power-expressions, we obtain:

for some ordered subsets of indices (with potentially repeating entries)  and some nonnegative  (exact formula for those can be given but we do not need it to complete the proof and since it is technical, it would unnecessarily complicate the proof so we skip it)
and  defined as:


Our next goal is to re-write the formula for . Denote:


Observe that  has the same distribution as  defined as:



where  is a Gaussian vector taken from the  distribution, independently from: . 

This comes from the fact that for a fixed  one can think about the set:
 as a random rotation of the system of  canonical basis vectors: .
Thus instead of applying a random rotation to: , one can equivalently randomly rotate vector . Randomly rotated vector  has the same distribution as: . 



Now note that lengths of vectors  are chosen independently.

Therefore we obtain:

where .

Denote .
Thus we obtain:


Now let us focus on the second expression from the formula on . We have:


where the first equality comes from the fact that
different s are independent and the second one is implied by the analogous analysis to the one conducted above.

We will need the following lemma:

\begin{lemma}
\label{useful-lemma}
For every  such that  and every  the following holds:

\end{lemma}

\begin{proof}
Take , where  is an independent copy of . Note that .
We have:

where the first equality comes from the independence of different elements of 
and the second equality is implied by the fact that  is independent from .

Therefore we have:

That completes the proof since  and .
\end{proof}

Note that by Lemma \ref{useful-lemma}, we can rewrite the right expression from the formula on 

as: 

The left expression from the formula on 
 can be rewritten as:


Since marginal distributions of  and  are the same, we can rewrite  as:

where  is defined as:

We need now few observations regarding .
Note firsr that since odd moments of the Gaussian scalar distribution  are zero,  is zero if at least of of  is odd. Furthermore,  is trivially zero if all but at most one  are zero.

With our new notation,  can be rewritten as:


Note also that we have:


Therefore (see: our observations on ) to complete the proof it suffices to show that:  if at least two: ,  for  are nonzero and all  are even.
\begin{lemma}
\label{tau-lemma}
The following holds if for some  we have:  and all  are even:

\end{lemma}
\begin{proof}
Note that  can be rewritten as:

where  stands for the  moment of the -distribution with  degrees of freedom.
Note that ,
where  is the so-called \textit{Gamma-function}.

Using the fact that:  and  for , it is easy to see 
that for a fixed , the RHS of the Equality \ref{multi-d} is maximized when  and  for some  and . Furthermore, straightforward calculations show that in that case the value of the RHS from Equality \ref{multi-d} is . That completes the proof of the Lemma.
\end{proof}


By  denote a subset of  formed by only keeping  such that for some ,  and all  are even. As we have shown above,  when . Otherwise,

Hence, since all terms in the sum

are nonnegative, we'll get a lower bound on  by only taking a subset of these terms. For this subset, we take , a subset of  with only two nonzero  for some  (there are  combinations of such ). Then, we take only those  from  which correspond to  in (\ref{x_iid_formula}) for  and  for all other 's. Hence,  and all other 's are zero and the corresponding weight from the second sum in (\ref{eq:doublesum}) would be . For  in such set, we'll have  by Lemma \ref{tau-lemma} and, hence, . As the result, we get the following lower bound on :

Since ,  and . This results in

which concludes the proof.


\end{proof}
\subsubsection{Proof of Theorem \ref{general-var-theorem}}
\begin{proof}
We will use the notation from the proof of Theorem \ref{general-ort-theorem}.
Since both estimators:  and
 are unbiased, we have:
 and
.
We have:


Similarly,


We have: 


Similarly, we get:


Therefore, since marginal distributions of  and  are the same,  we have:

Plugging in the formula for  and  from Equation \ref{x_ort_formula} and Equation \ref{x_iid_formula}, and using our analysis from the proof of Theorem \ref{ort-theorem} we obtain:

for  and .

Based on the definition of  (\ref{eq:taudef}), if  or ,  and the whole corresponding term in the sum (\ref{eq:vardec}) is zero. Also, if  is odd,  and, again, the corresponding term in the sum (\ref{eq:vardec}) is zero. Same holds for  from (\ref{eq:vardec}). Based on the analysis from Theorem \ref{general-ort-theorem}'s proof and 's definition we have:

where in the second transition we use the fact that  for odd .


Hence, we can rewrite (\ref{eq:vardec}) by excluding terms which are definitely zero and using Lemma \ref{tau-lemma}:

That completes the proof.
\end{proof}

\subsection{Proof of Theorem \ref{thm:uniform}}

We showed in the main body of the paper that in contrast to other methods approximating the attention matrix , our algorithm provides strong concentration guarantees. This is the case also for trigonometric random features, yet, as discussed in the main body of the paper, due to attention renormalization and higher variance of the estimation of small entries of the attention matrix, trigonometric mechanism is sub-optimal.
We show here that , the optimal number of random projections for the trigonometric orthogonal mechanism for accurate estimation of the attention matrix does not depend on  but only on . In fact, we prove that if we take , then with -time, we can approximate  up to any precision, regardless of the number of tokens . In order to provide those guarantees, we leverage recent research on the theory of negative dependence for ORFs \citep{Lin2020DemystifyingOM}.

We prove the more general version of Theorem \ref{thm:uniform} from the main body of the paper: 

\begin{theorem}[Uniform convergence for the trigonometric mechanism]
\label{uniform_convergence}
Define entries of the attention matrix  as follows:  for some
 and where  is a radial basis function (RBF) kernel \citep{geom} with corresponding spectral distribution  (e.g. Gaussian kernel for which ). Assume that the rows of matrices  and  are taken from a ball  of radius , centered at  (i.e. norms of queries and keys are upper-bounded by ). 
Define  and take  and
.
Then for any , 
and the number of random projections  for  the following holds:

with any constant probability,
where  approximates generalized attention matrix via orthogonal trigonometric random features.
\end{theorem}

The result holds in particular for regular softmax-attention for which  is a Gaussian kernel and . In that case  since .

\begin{proof}
Let  be a diagonal matrix with entries of the form:  and let  be a diagonal matrix with entries of the form: . Denote 
. Denote by  and approximation of the attention matrix obtained from trigonometric orthogonal random features and by  an approximation of matrix  that those random features provide.
We rely on Theorem 3 from \citep{Lin2020DemystifyingOM}.
Note that we can apply it in our case, since for RBF kernels the corresponding functions  satisfy ,  (thus in particular are bounded). Also, it is not hard to observe (see for instance analysis in  Claim 1 from \citep{fourierapprox}) that we can take:  (for  as in Theorem 3 from \citep{Lin2020DemystifyingOM}). 
Using Theorem 3 from \citep{Lin2020DemystifyingOM}, we conclude that:

with any constant probability as long as
,
where  and  is the diameter of the smallest ball  containing all vectors of the form . 
Since , we conclude that  and thus one can take .
We have:

Taking  completes the proof.
\end{proof}

\subsection{Discussion of Theorem \ref{thm:uniform}} \label{sec:ball}

As a consequence of Theorem~\ref{thm:uniform}, the number  of random projections required to approximate the attention matrix within  error is a function of data dimensionality , the parameter  and the radius  of the ball within which the queries and keys live:

The dependence on  and  is fairly easy to understand: with a larger dimensionality  we need more random projeections (on the order of magnitude ) to get an approximation within  error. The dependence on  means that the length of queries and keys cannot grow at a fixed  if we want to retain the quality of the approximation.
In particular, this means that FAVOR cannot approximate hard attention on sequences of unlimited length with a fixed . When the sequence length increases, even the standard attention requires longer and longer vectors to make the softmax concentrated enough to pick single elements. Nevertheless, as seen in our experiments, this limitation does not manifest itself in practice at the lengths we experimented with.