[{'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (High)', 'Score': '49.1'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (Middle)', 'Score': '68.1'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (High)', 'Score': '47.5'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (Middle)', 'Score': '64.3'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (High)', 'Score': '42.3'}}, {'LEADERBOARD': {'Task': 'Reading Comprehension', 'Dataset': 'RACE', 'Metric': 'Accuracy (Middle)', 'Score': '57.9'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MGSM', 'Metric': 'Average (%)', 'Score': '55.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '77.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '69.3'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '540'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '55.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '81.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '69.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Tokens (Billions)', 'Score': '780'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '90.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'EM', 'Score': '69.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '92.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '81.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TriviaQA', 'Metric': 'EM', 'Score': '76.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '39.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '29.3'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '21.2'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '100'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OBQA', 'Metric': 'Accuracy', 'Score': '53.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'OBQA', 'Metric': 'Accuracy', 'Score': '50.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '43.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '22.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'WebQuestions', 'Metric': 'EM', 'Score': '10.6'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Novel Concepts)', 'Metric': 'Accuracy', 'Score': '71.9'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Novel Concepts)', 'Metric': 'Accuracy', 'Score': '59.4'}}, {'LEADERBOARD': {'Task': 'Cross-Lingual Question Answering', 'Dataset': 'TyDiQA-GoldP', 'Metric': 'EM', 'Score': '52.9'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '26.2'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@100', 'Score': '76.2'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '23.7'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '15.9'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@100', 'Score': '46.3'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '3.6'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@100', 'Score': '18.7'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Known Unknowns)', 'Metric': 'Accuracy', 'Score': '73.9'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Winowhy)', 'Metric': 'Accuracy', 'Score': '65.9'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Winowhy)', 'Metric': 'Accuracy', 'Score': '61.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '81.1'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '77.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'F1', 'Score': '94.6'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'EM', 'Score': '94.0'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '78.8'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'F1', 'Score': '100'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'Accuracy', 'Score': '100'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '95.7%'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '89.7'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '81.8'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'LAMBADA', 'Metric': 'Accuracy', 'Score': '77.9'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'WSC', 'Metric': 'Accuracy', 'Score': '89.5'}}, {'LEADERBOARD': {'Task': 'Extreme Summarization', 'Dataset': 'GEM-XSum', 'Metric': 'ROUGE-2', 'Score': '21.2'}}, {'LEADERBOARD': {'Task': 'Extreme Summarization', 'Dataset': 'GEM-XSum', 'Metric': 'Parameters', 'Score': '540 B'}}, {'LEADERBOARD': {'Task': 'Extreme Summarization', 'Dataset': 'GEM-XSum', 'Metric': 'ROUGE-2', 'Score': '21.0'}}, {'LEADERBOARD': {'Task': 'Extreme Summarization', 'Dataset': 'GEM-XSum', 'Metric': 'ROUGE-2', 'Score': '18.5'}}, {'LEADERBOARD': {'Task': 'Extreme Summarization', 'Dataset': 'GEM-XSum', 'Metric': 'Parameters', 'Score': '62 B'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '83.8'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '83.6'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '83.4'}}, {'LEADERBOARD': {'Task': 'Auto Debugging', 'Dataset': 'Big-bench Lite', 'Metric': 'Exact string match', 'Score': '38.2'}}, {'LEADERBOARD': {'Task': 'Auto Debugging', 'Dataset': 'Big-bench Lite', 'Metric': 'Exact string match', 'Score': '14.7'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (StrategyQA)', 'Metric': 'Accuracy', 'Score': '73.9'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (StrategyQA)', 'Metric': 'Accuracy', 'Score': '65.4'}}, {'LEADERBOARD': {'Task': 'Memorization', 'Dataset': 'BIG-bench (Hindu Knowledge)', 'Metric': 'Accuracy', 'Score': '95.4'}}, {'LEADERBOARD': {'Task': 'Memorization', 'Dataset': 'BIG-bench (Hindu Knowledge)', 'Metric': 'Accuracy', 'Score': '77.7'}}]
