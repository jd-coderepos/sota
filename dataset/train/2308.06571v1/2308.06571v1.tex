\section{Experiments}

\subsection{Implementation details}

\subsubsection{Datasets}

\paragraph{LAION~\cite{schuhmann2021laion}.} We utilize the LAION-5B dataset as image-text pairs, specifically the LAION2B-en subset, as the model focuses on English input. The LAION dataset encompasses objects, people, scenes, and other real-world elements. 

\paragraph{WebVid~\cite{Bain21}.} The WebVid dataset comprises almost 10 million video-text pairs, with a majority of the videos having a resolution of $336 \times 596$. Each video clip has a duration of approximately 30 seconds. During the model training, we selected the middle square portion and randomly picked 16 frames with 3 frames per second as training data.

\paragraph{MSR-VTT~\cite{xu2016msr-vtt}.} The MSR-VTT dataset is used to validate our model performance and is not utilized for training. This dataset includes 10k video clips, each of which is annotated with 20 sentences. To obtain FID-vid and FVD metric results, 2,048 video clips were randomly selected from the test set, and one sentence was randomly chosen from each clip to generate videos. When evaluating on CLIPSIM metric, we followed previous works~\cite{singer2022make-a-video,wu2022nuwa} and use nearly 60k sentences from the whole test split as prompts to generate videos.

\subsubsection{Model instantiation and hyper-parameters}
We employ DDPM~\cite{ho2020denoising} with $T = 1,000$ steps for training and use DDIM sampler~\cite{song2020denoising} in classifier-free guidance~\cite{ho2022classifier} with $50$ steps for inference by default.
ModelScopeT2V primarily consists of three modules: the Text encoder $\tau$, VQGAN, and Denoising UNet. 
The pretrained checkpoint for initializing VQGAN and Denoising UNet are obtained from Stable Diffusion~\cite{rombach2022high} version 2.1\footnote{https://github.com/Stability-AI/stablediffusion}.
The parameters in VQGAN remains frozen during training and inference.
The outputs of temporal convolution and temporal attention are initialized as zeros, enabling ModelScopeT2V to generate meaningful yet temporally discontinuous frames at the beginning of training.
As the training progresses, the temporal structures will be optimized to learn the temporal correspondence between frames, thereby synthesising continuous videos.



\subsubsection{Training details}

We train ModelScopeT2V using the AdamW optimizer~\cite{loshchilov2017AdamW} with a learning rate of $5\times 10^{-5}$.
Our model is trained on 80G NVIDIA A100 GPUs.
We perform multi-frame training as detailed in Section~\ref{sec:multi-frame_training}, specifically using a batch size of 1,400 for images and a batch size of 3,200 for videos, and training 267 thousand iterations. 
The compression factor of VQGAN is 8, meaning that it converts RGB images of size $256 \times 256$ into latent representations of size $32 \times 32$ .
For the text encoder, we set the maximum text length to $N_p=77$, and embedding dim $N_c=768$, which are consistent with the pre-trained OpenCLIP\footnote{https://github.com/mlfoundations/open\_clip}.


We empirically observe that employing either temporal convolution or temporal attention can augment ModelScopeT2V's ability to capture temporal dependency.
This observation is partly supported by VideoCraft\footnote{https://github.com/VideoCrafter/VideoCrafter} which only contains temporal attention for temporal modeling.
We take a step further by employing both the temporal convolution and the temporal attention, which facilitates the ModelScopeT2V to achieve superior temporal modeling.
In detail, we use $N_2 = 4$ temporal convolution blocks and $N_4 = 2$ temporal attention block for each spatio-temporal block.
These temporal blocks account for 552 million parameters out of the total 1,345 million parameters in our UNet, indicating that 39\% parameters of the UNet parameters are dedicated to capturing temporal information. 
As a result, the entire ModelScopeT2V model (including VQGAN and the text encoder) comprises approximately 1.7 billion parameters.


We observe that use more layers of temporal convolution would lead to better temporal ability. Since the kernel size of 1D CNN in temporal convolution is 3, $N_2=4$ temporal convolution layers could lead the local receptive field as 81 in each spatio-temporal block, which is enough for 16 output frames per video. For multi-frame training, the temporal convolution and temporal attention mechanisms are still active. Our experiments show it is unnecessary to change the range of parameters for different frame settings.

\subsection{Main results}

\subsubsection{Quantitative results}
ModelScopeT2V is evalutated on MSR-VTT~\cite{xu2016msr-vtt} dataset. 
We conduct the evaluation under a zero-shot setting since ModelScopeT2V is not trained on MSR-VTT. 
We compare ModelScopeT2V with several state-of-the-art models using FID-vid~\cite{heusel2017gans_nash_equilibrium}, FVD~\cite{unterthiner2018FVD}, and CLIPSIM~\cite{wu2021godiva} metrics. 
The FID-vid and FVD are assessed based on 2,048 randomly selected videos from MSR-VTT test split, where we compute the metrics using the middle 16 frames of each video with an FPS of 3. 
CLIPSIM are evaluated based on all captions from MSR-VTT test split following~\cite{singer2022make-a-video}.
The resolution of the generated videos is consistently $256 \times 256$.

As shown in Table~\ref{tab:results}, ModelScopeT2V achieves the best performance on both FID-vid (\ie, 11.09) and FVD (\ie, 550), indicating that our generated videos are visually similar to the ground truth videos.
Our model also obtains a competitive score of 0.2930 on CLIPSIM, suggesting that our generated videos are semantically similar to the text prompts. 
The CLIPSIM score of our model is only marginally lower than that of Make-A-Video~\cite{singer2022make-a-video}, while they utilize additional data from HD-VILA-100M~\cite{xue2022HD-VILA-100M} for training. 


\begin{table}[t]
\centering
\begin{tabular}{c|ccc}
\hline
\textbf{Models}              & FID-vid ($\downarrow$)   &  FVD ($\downarrow$) & CLIPSIM ($\uparrow$) \\ \hline
N\"UWA~\cite{wu2022nuwa}       & 47.68   & -  & 0.2439        \\
CogVideo (Chinese)~\cite{hong2022cogvideo}   & 24.78 & - & 0.2614      \\
CogVideo (English)~\cite{hong2022cogvideo}   & 23.59  & 1294 & 0.2631  \\
MagicVideo~\cite{zhou2022magicvideo}          & -    & 1290 & -        \\
Video LDM~\cite{blattmann2023align}      & -   &  - & 0.2929 \\ 
Make-A-Video~\cite{singer2022make-a-video}        & 13.17 &  -& \textbf{0.3049}     \\ \hline
ModelScopeT2V (ours) &   \textbf{11.09} & \textbf{550} & 0.2930    \\ \hline
\end{tabular}
\caption{
\textbf{Quantitative comparison with state-of-the-art models on MSR-VTT.} We evaluate the models with three metrics (\ie, FID-vid~\cite{heusel2017gans_nash_equilibrium}, FVD~\cite{unterthiner2018FVD}, and CLIPSIM~\cite{wu2021godiva}).
}
\label{tab:results}
\end{table}


\subsection{Qualitative results} 
In this subsection, we compare the qualitative results of ModelScopeT2V with other state-of-the-art methods. 
To facilitate comparison with Make-A-Video and Imagen Video, generated video frames with the same frame index are presented in the same column with ModelScopeT2V.
The videos generated by Make-A-Video~\cite{singer2022make-a-video}\footnote{https://makeavideo.studio} and Imagen Video~\cite{ho2022imagenvideo}\footnote{https://imagen.research.google/video} were downloaded from their official webpages.
Six frames are uniformly sampled from each video for comparison. 
This comparison is fair in terms of video duration, as all three methods (\ie, Make-A-Video, Imagen Video, and ModelScopeT2V) generate 16-frame videos aligned with given texts.
One difference is that Imagen Video generates videos with a aspect ratio of $2:5$, while the other two generate videos with a aspect ratio of $1:1$.



\begin{figure}[htb]
        \centering
        \includegraphics[height=15cm]{./figures/compare_with_make_a_video.pdf}
        \caption{
        \textbf{Qualitative results comparison with Make-A-Video.}
        We present three examples, each displaying the provided prompt, the video generated by Make-A-Video and the video generated by ModelScopeT2V.
        }
        \label{fig:compare_with_make_a_video}
\end{figure}


The quanlitative comparison between ModelScopeT2V and Make-A-Video is displayed in Figure~\ref{fig:compare_with_make_a_video}.
We can observe that both methods generate videos of high quality, which is consistent with the quantitative results. 
However, the ``robot'' in the first example and the ``dog'' in the second example generated by ModelScopeT2V exhibit a superior degree of realism.
We attribute this advantage to our model's joint training with image-text pairs, which enhances its comprehension of the correspondence between textual and visual data.
In the third example, while the ``industrial site'' generated by Make-A-Video is more closely aligned with the prompt, depicting the overall scene of a ``storm'', ModelScopeT2V produces a distinctive interpretation showcasing two ``abandoned'' factories and a gray sky, captured from various camera angles.
This difference stems from Make-A-Video's use of image CLIP embedding to generate videos, which can result in less dynamic motion information.
In general, ModelScopeT2V demonstrates a wider range of motion in its generated videos, distinguishing it from Make-A-Video.










\begin{figure}[htb]
        \centering
        \includegraphics[height=13.5cm]{./figures/compare_with_imagen.pdf}
        \caption{
        \textbf{Qualitative results comparison with Imagen Video.}
        We present three examples, each displaying the provided prompt, the video generated by Imagen Video and the video generated by ModelScopeT2V.
        }
        \label{fig:compare_with_imagen}
\end{figure}



The comparison of our method, ModelScopeT2V, with Imagen Video is illustrated in Figure~\ref{fig:compare_with_imagen}.
While Imagen Video generate more vivid and contextually relevant video content, ModelScopeT2V effectively depicts the content of the prompt, albeit with some roughness in the details.
For instance, in the first example of Figure~\ref{fig:compare_with_imagen}, it's noteworthy that Imagen Video generates a video whose second frame illustrates a significantly distorted dog's tongue, exposing the model's limitations in accurately rendering the real world.
On the other hand, ModelScopeT2V demonstrates its potential in robustly representing the content described in the prompt.
It's worth noting that the superior performance of Imagen Video can be attributed to the employment of the T5 text encoder, a base model with a larger number of parameters, and a larger-scale training dataset, which is not utilized in ModelScopeT2V.
Considering the performance, our  ModelScopeT2V lays a strong foundation for future improvements and shows considerable promise in the domain of text-to-video generation.




\subsection{Community development} 

We have made the code for ModelScopeT2V publicly available on the GitHub repositories of ModelScope\footnote{https://github.com/modelscope/modelscope/blob/master/modelscope/models/multi\_modal/video\_synthesis} and Diffuser\footnote{https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis/blob/main/app.py}.
Additionally, we have provided online demos of ModelScopeT2V on ModelScope\footnote{https://modelscope.cn/studios/damo/text-to-video-synthesis/summary} and HuggingFace\footnote{ https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis}. 
The open-source community has actively engaged with our model and uncovered several applications of ModelScopeT2V.
Notably, projects such as sd-webui-text2video\footnote{https://github.com/deforum-art/sd-webui-text2video} and Text-To-Video-Finetuning\footnote{https://github.com/ExponentialML/Text-To-Video-Finetuning}have extended the model's usage and broadened its applicability.
Additionally, the video generation feature of ModelScopeT2V has already been successfully utilized for the creation of short videos\footnote{https://youtu.be/Ank49I99EI8}.

