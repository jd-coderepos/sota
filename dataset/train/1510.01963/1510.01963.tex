\documentclass[a4paper,11pt]{article}

\usepackage{a4wide}

\usepackage[round]{natbib}

\makeatletter
\providecommand{\doi}[1]{
  \begingroup
    \let\bibinfo\@secondoftwo
    \urlstyle{rm}
    \href{http://dx.doi.org/#1}{
      doi:\discretionary{}{}{}
      \nolinkurl{#1}
    }
  \endgroup
}
\makeatother


\usepackage{authblk}
\usepackage{array,multirow,graphicx}

\usepackage{amsthm}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{prop}[lemma]{Proposition}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{application}[lemma]{Application}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{observation}[lemma]{Observation}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{proposition}[lemma]{Proposition}

\usepackage{dsfont}
\usepackage{hyperref}

\usepackage{amssymb, amsmath}

\usepackage{subfig}

\usepackage{verbatim}

\usepackage{enumitem}



\newcommand{\K}{\ensuremath{\mathds{K}}}
\newcommand{\R}{\ensuremath{\mathds{R}}}
\newcommand{\Q}{\ensuremath{\mathds{Q}}}
\newcommand{\N}{\ensuremath{\mathds{N}}}
\newcommand{\Z}{\ensuremath{\mathds{Z}}}
\newcommand{\D}{\ensuremath{\mathds{D}}}
\newcommand{\W}{\ensuremath{\mathds{W}}}
\newcommand{\B}{\ensuremath{\mathds{B}}}
\newcommand{\NS}{N}

\newcommand{\Obj}{f}

\newcommand{\ObjFeas}{Y}
\newcommand{\ObjSub}{Q}
\newcommand{\DecFeas}{X}

\newcommand{\ND}{{\ObjFeas}_{\rm nd}}
\newcommand{\NDSet}{\ND}
\newcommand{\bz}{\bar{z}}
\newcommand{\bu}{\bar{u}}
\newcommand{\hN}{\hat{N}}
\newcommand{\hZ}{\hat{Z}}
\newcommand{\hz}{\hat{z}}
\newcommand{\BM}{z^{\text{r}}}
\newcommand{\Bm}{0}
\newcommand{\Bzero}{0}
\newcommand{\Bone}{1}

\newcommand{\onetod}{\{1,\dots,p\}}
\newcommand{\dummy}{\hat{z}}

\newcommand {\bigo}{\mathcal{O}}
\newcommand {\NP}{\mathcal{NP}}

\newcommand{\GP}{GP}
\newcommand{\NGP}{NGP}

\newcommand{\lub}{local upper bound}
\newcommand{\ubs}{upper bound set}

  \usepackage[ruled]{algorithm2e}
  \SetCommentSty{emph}
  \SetKwComment{tcp}{-- }{}
  \SetKwComment{tcc}{-- }{ --}

    \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKwInOut{InOutput}{in/out}
  \SetKwInOut{Global}{global}
  \SetKwInOut{Returns}{returns}
  \SetKw{SuchThat}{such that}
  \SetKw{True}{true}
  \SetKw{False}{false}
  \SetKw{Not}{not}
  \SetKw{Break}{break}
  \SetKw{KwAnd}{and}
  \SetKw{Boolean}{boolean}
  \SetKw{KwWhile}{while}
  \SetKwData{Dominated}{dominated}
  \SetKwData{Updated}{updated}
    \SetKwFunction{Choose}{choose}

  \SetKwFunction{UBSI}{ubsI}
  \SetKwFunction{UBSNI}{ubsNI}
  \SetKwFunction{WFG}{wfg}
  \SetKwFunction{EHV}{exclHv}
  \SetKwFunction{IHV}{inclHv}
  \SetKwFunction{PMAX}{pMax}

  \SetEndCharOfAlgoLine{}

\usepackage{diagbox}
\usepackage{nicefrac}


\setlength{\tabcolsep}{0pt}


\usepackage{enumitem}

\usepackage{mathdots}

\newcounter{piccnt}
\newcounter{legendcnt}

\begin{document}

\title{A Box Decomposition Algorithm\\ to Compute the Hypervolume Indicator}
\author[1]{Renaud Lacour\footnote{Corresponding author.}}
\author[1]{Kathrin Klamroth}
\affil[1]{Department of Mathematics and Computer Science, 

University of Wuppertal, Germany

\texttt{\{lacour,klamroth\}@math.uni-wuppertal.de}}
\author[2]{Carlos M. Fonseca}
\affil[2]{CISUC, Department of Informatics Engineering, University of Coimbra, Portugal\\
\texttt{cmfonsec@dei.uc.pt}}
\maketitle


\begin{abstract}
We propose a new approach to the computation of the hypervolume indicator,
based on partitioning the dominated region into a set of axis-parallel hyperrectangles or boxes.
We present a nonincremental algorithm and an incremental algorithm, 
which allows insertions of points,
whose time complexities are 
and , respectively.
While the theoretical complexity of such a method is lower bounded
by the complexity of the partition, which is, 
in the worst-case, larger than the best upper bound on 
the complexity of the hypervolume computation,
we show that it is practically efficient.
In particular, the nonincremental algorithm competes with the currently
most practically efficient algorithms.
Finally, we prove an enhanced upper bound of 
and a lower bound of 
for 
on the worst-case complexity of the WFG algorithm.

\end{abstract}


\section{Introduction}

In multi-objective optimization (MOO), since objective functions are often conflicting in practice, 
there is typically no single solution that simultaneously optimizes all objectives.
Instead there are several efficient solutions, 
i.e.\  that cannot be improved on one objective without degrading at least another objective.
Due to the possible large number of efficient solutions or even nondominated points 
-- their images in the objective space --
approximation algorithms are often favored in practice.
These algorithms are able to generate several discrete approximations or representations 
of the nondominated set and quality indicators are required to compare such approximations.
Among them is the hypervolume indicator or Lebesgue measure \citep[see e.g.][]{ZitThi98}, 
which measures the volume of the part of the objective space dominated by the points
of the approximation and bounded by some reference point. 

Practically efficient algorithms to approximate the nondominated set include in particular
evolutionary multi-objective (EMO) algorithms.
Most often the hypervolume indicator is used within EMO algorithms 
to compare the quality 
of the computed approximations \citep{BeuNauEmm07, WagBeuNau07}.
Because these algorithms repeatedly generate
approximations of large cardinality there is a clear need for efficient algorithms 
to compute the hypervolume indicator.
It has been shown \citep{BriFri10} that, under ``'', 
there is no algorithm to compute the hypervolume indicator in time polynomial 
in the number of objectives. 
Therefore, we will assume throughout the paper that the number of objectives, 
although arbitrary, is fixed.

In this paper, we are interested in the efficient computation 
of the hypervolume associated to a given set of points.
We make a difference between the nonincremental case
and the incremental case.
In the nonincremental case, 
the whole set of points has, in general, to be known in advance
because the computation approach imposes an order on the points to be processed.
In the incremental case, the hypervolume is updated each time a new point is considered
without restriction on the order new points come up.


\subsection{Terminology and notations}

We consider a minimization problem formulated as follows:

where  is the feasible set and 
 are  objective functions mapping from  to .
We assume that all feasible points  are located 
in some open hyperrectangle of ,
namely .
In order to compare points of the objective space, 
we define the following binary relations. For :

For any set  of points of , we define the dominated region as

where  is used as a \emph{reference point}.
Considering the set 

of all nondominated points of , 
we have . 
Therefore, we assume in the remainder that  is a \emph{stable} set of points 
for the dominance relation,
i.e.\ for all , .

We denote by  the volume of the polytope  
which is also referred to as the hypervolume 
associated to .

For any , we let  be the -dimensional
vector of all components of  excluding component~, for a given
. 
Finally, for any  and any , 
denotes the vector .


\subsection{Literature review on the computation of the hypervolume indicator}\label{sec:prev_algo}

The currently most efficient algorithm for the computation of the hypervolume indicator in the case 
in terms of theoretical worst-case complexity 
is by \citet{Cha13}.
For the computation of the dominated hypervolume of  -dimensional points,
his algorithm runs in  time. 
To our knowlege, there is currently, however, no available implementation of this approach
and no evidence of its practical efficiency.
The complexity of computing the hypervolume indicator is 
\citep[see][]{BeuFonLopPaqVah09}.

On the practical point of view, the ``HV4D'' algorithm of \citet{GueFonEmm12},
specialized for the case , 
is the most efficient in this case 
\citep[see e.g. the computational results of][]{RusFra14, NowMaeIzz14}.
Their algorithm achieves  time complexity.
Above , the Walking Fish Group (WFG) \citep{WhiBraBar12} 
and Quick Hypervolume (QHV) \citep{RusFra14} algorithms are currently
two of the most efficient according to the computational experiments 
conducted by the authors.
The best upper bounds on their complexity are, however, in both cases exponential.


\subsection{Goals and outline}

In this paper, we propose to compute the hypervolume indicator
by partitioning the dominated region into hyperrectangles. 
Given that this partition is determined by a set of corner points or local upper bounds of the dominated region,
we present approaches to compute these points.
Specifically, we propose a nonincremental approach
which requires that the points for which the hypervolume is computed
are sorted with respect to one of the objective functions,
and an incremental approach which relaxes this assumption
at some extra cost.
Both approaches have a good worst-case complexity and perform very well in practice.
We also provide new insight into the theoretical complexity of the WFG algorithm.

The remainder of this paper is organized as follows.
In Section~\ref{sec:the_approach}, we present the proposed approach.
Section~\ref{sec:wfg-complexity} provides an enhanced analysis
of the complexity of the WFG algorithm.
Section~\ref{sec:expe} describes computational experiments 
conducted to compare the proposed algorithms to the state-of-the-art and presents the results.
Section~\ref{sec:concl} concludes the paper.




\section{The Hypervolume Box Decomposition Algorithm}\label{sec:the_approach}

This section describes the proposed new approach to compute the hypervolume indicator:
the Hypervolume Box Decomposition Algorithm (HBDA).
Section~\ref{sub:partition} defines the decomposition scheme.
Section~\ref{sub:ubs} presents two algorithms to compute an auxiliary set of points, 
referred to as an upper bound set, that is necessary to obtain the decomposition.
In Section~\ref{sub:time-complex}, the worst-case time complexity
of HBDA is discussed.
The general position assumption that is made in the first two sections is relaxed in Section~\ref{sub:NGP}.
Section~\ref{sub:impl} is concerned with the implementation of HBDA.

\subsection{Partitioning the dominated region into disjoint hyperrectangles}\label{sub:partition}

The dominated region  is a union of hyperrectangles 
of the type  where . 
The idea of the proposed approach is to rely on another description of 
as a union of pairwise disjoint hyperrectangles
so that the hypervolume can be computed as the sum of their volumes.
We illustrate the concepts of this section on a bi-objective instance represented 
in Figure~\ref{fig:decomp}.


\begin{figure}
  \begin{center}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt}
  \end{center}
  
  \caption{Decomposition of the dominated region in the bi-objective case
  \label{fig:decomp}}
\end{figure}

In \citet[Section 3]{KapRubShaVer08}, a decomposition of  into a set of pairwise disjoint hyperrectangles
is established.
Their approach is based on the computation of a set  of auxiliary points
associated to , which are identical to \emph{local nadir points} in the bi-objective case,
or to \emph{local upper bounds} in the general multi-objective case \citep[see][]{KlaLacVan15}.
The set  is defined as the set of all the points  that satisfy
the two following properties:
\begin{itemize}[itemindent=20pt,labelsep=10pt,itemsep=\medskipamount]
      \item[()]  does not contain any point of .
  \item[()]  is maximal for property (), i.e.\  for any 
   such that ,  does not satisfy ().
\end{itemize}
Following \citet{KlaLacVan15}, we denote by \emph{\lub{}s} the elements of 
and by \emph{\ubs{}} the set  itself.

We first make a simplifying ``general position'' assumption that will be relaxed later.
Under this assumption, no two distinct points,
among the points of  to be considered,
share the same value in any dimension.
We also define additional dummy points 
where  for all .
These points are all the points of 
that (1) are not dominated by and do not dominate any point of 
 and (2) are minimal with respect to the dominance relation  for (1).

Let .
We have  and each vector 
is defined by  distinct points  of 
in the sense that  and 
for all .
We refer to \citet{KlaLacVan15} for more details on this aspect.



Then, according to 
\citet{KapRubShaVer08}, the set  where

is a partition of .
We refer to Figure~\ref{fig:decomp} for an illustration of this partition in the bi-objective case.
Therefore, the hypervolume of  is obtained as the sum of the hypervolumes of the 's,
for all , which requires a computation time linear in .


In the next section, we discuss the computation of the set .

\subsection{Computing upper bound sets}\label{sub:ubs}

Several approaches exist for the computation of the set . 
\citet{KapRubShaVer08} propose a nonincremental algorithm
which requires that the points of  are sorted 
in increasing order according to any fixed component.
Their algorithm is output-sensitive
and achieves  time complexity 
using dynamic range trees.
\citet{PrzGanEhr10} also provide an algorithm to compute 
which is used to define search zones for an algorithm to compute the nondominated
set of MOCO problems.
In \citet{KlaLacVan15} we propose another algorithm which uses, 
as in \citet{DaeKla14} for the tri-objective case,
the relation between points of  and their defining points to avoid a filtering step with respect to Pareto dominance
found in \citet{PrzGanEhr10}.
Both \citet{PrzGanEhr10} and \citet{KlaLacVan15} propose incremental algorithms.
We note that since defining points are tracked in \citet{KapRubShaVer08} and \citet{KlaLacVan15}, the corresponding algorithms
make it directly possible to compute the hypervolume indicator using the partition described by~\eqref{eq:Bu}.

We first present the algorithm of \citet{KlaLacVan15}, 
which is used
when arbitrary insertions into  are required (Section~\ref{sub:incremental}). 
Then we present the nonincremental algorithm based on both \citet{KapRubShaVer08} and a theorem of \citet{KlaLacVan15},
which is expected to be more efficient when the whole set  is known in advance
(Section~\ref{sub:nonincremental}).

\subsubsection{Incremental algorithm}\label{sub:incremental}

The incremental algorithm to compute an upper bound set is presented in Algorithm~\ref{alg:incremental}.
Given a stable set  and a new point  such that  is also a stable set,
it identifies from the upper bound set for  the set  of all local upper bounds
that no longer satisfy property~ with respect to  (Step~\ref{alg:incremental:A}).
From the set , Step~\ref{alg:incremental:nr} generates the valid new local upper bounds 
using the result provided in Theorem~\ref{th:incremental}. 


\begin{theorem}[\citealp{KlaLacVan15}]\label{th:incremental}
  Let  be a point of  such that  
  is a stable set of points in general position. 
  Consider a \lub{}  such that .
  
  Then, for any ,  
  is a \lub{} of  
  if, and only if, .
\end{theorem}

To be able to use this result, it is required to keep track of the associated defining points for each local upper bound. 
This is done by setting  
at Step~\ref{alg:incremental:init}
and 

at Step~\ref{alg:incremental:nr},
for all .

\begin{algorithm}
  \begin{tabular}{llll}
    \nl\UBSI{}~      & =~ & \multicolumn{2}{l}{\nllabel{alg:incremental:init}\;}\\
    &&&\-8pt]
    \nl\UBSNI{}~ & =~ & \multicolumn{2}{l}{\nllabel{alg:nonincremental:p}\;}\\
    \nl                         &    & \multicolumn{2}{l}{\nllabel{alg:nonincremental:nr}\;}\\ 
    \nl                         &    & \multicolumn{2}{l}{\;}\\ 
    \nl                         &    & where~ &  = N\nllabel{alg:nonincremental:A}\;\\
    \nl                         &    &        &  = N\;
  \end{tabular}
  \caption{Nonincremental algorithm to compute an upper bound set 
  -- assumes that , for all \label{alg:nonincremental}}
\end{algorithm}

For the computation of the hypervolume indicator,
the local upper bounds of  need not be kept.
Indeed they are not modified later, which implies that the associated hyperrectangles according to~\eqref{eq:Bu}
will not change.




\bigskip

Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental}
and the decomposition of the dominated region (Section~\ref{sub:partition})
yield two Hypervolume Box Decomposition Algorithms:
an incremental version (HBDA-I) and a nonincremental version
(HBDA-NI), respectively.

\subsection{Time complexity of the algorithms}\label{sub:time-complex}

The time complexity of HBDA-I and HBDA-NI 
mainly depends on the size of the current upper bound set  and of the set 
in Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental}, respectively.
Indeed for both algorithms, a constant time is spent on each element of .
Let  and  be upper bounds 
on the time complexity of HBDA-I and HBDA-NI,
respectively,
applied to  points of dimension .
We obtain the following relations:

where  is the worst-case size of an upper bound set 
on  points of dimension , which is equal to  \citep{KapRubShaVer08}.
(We recall that, in the case of HBDA-NI, only an upper bound set 
for at most  -dimensional points needs to be maintained.)
Therefore, we have:


Note that here we assume the worst-case for 
but Algorithm~\ref{alg:nonincremental} is an output-sensitive algorithm \citep{KapRubShaVer08}.



\subsection{Relaxing the simplifying ``general position'' assumption}\label{sub:NGP}

Real or generated instances may contain points that are not in general position. 
Therefore, it is important to allow, in algorithms to compute the hypervolume indicator, 
points having equal component values in the same dimension.
The partition of the dominated region presented in Section~\ref{sub:partition}
is based on the existence and uniqueness, for each local upper bound ,
of a -uple of points that define the  components of .
The uniqueness in particular is guaranteed by the general position assumption
and Theorem~\ref{th:incremental} assumes general position.
We show, however, that Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental}
can be applied without any modification to non-general position instances.

\begin{proposition}
  Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental} 
  are still valid when the input points are in non-general position.
\end{proposition}


\begin{proof}
Comparison between component values of points appear at three different places in the algorithms,
namely 
(a) in the strict dominance tests at Steps~\ref{alg:incremental:A} and~\ref{alg:nonincremental:A} 
of  Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental}, respectively,
(b) in the application of the condition of Theorem~\ref{th:incremental} at Steps~\ref{alg:incremental:nr}
and~\ref{alg:nonincremental:nr} of  Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental}, respectively, and 
(c) in the computation of the hypervolume of a box according to \eqref{eq:Bu}.

For (a), note that the strict dominance tests at Steps~\ref{alg:incremental:A}
and~\ref{alg:nonincremental:A} 
of  Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental}, respectively,
should remain the same,
since they are related to property (), which does not assume general position.

For (b) and (c), we follow the idea of symbolically perturbating 
the component values of the input points
as suggested in \citet{KapRubShaVer08}.
More precisely, given a set  of points in non-general position, 
one can define for each component  a total order  
on the values of the points of  on component :

for all .
This relation is obviously compatible with the natural strict ordering on real numbers, 
in the sense that if  then .
Thus we can use , or more precisely the symmetric  
in place of  to apply the condition of Theorem~\ref{th:incremental} in the non-general position case.
In fact, if we label the points of  so that the current point 
always gets the largest index among the points considered so far, 
the relation  can be used equivalently to , 
since the left-hand side of the comparison is always a component value of the current point.

Finally, it is equivalent to use  or  to compute the maxima in \eqref{eq:Bu}.
\end{proof}

\subsection{Implementation and data structures}\label{sub:impl}

Computing the set  in {\UBSI} and {\UBSNI}
requires to determine the subset of a set of local upper bounds that are strictly
dominated by a given point.
We expect that the size of the output subset is much smaller than the cardinality
of the input set, therefore a specialized data structure could be used instead of a simple linked list.

Options for this are range trees, d-trees and generalized quadtrees \citep{deBChevanOve08}.
Generalized quadtrees become inefficient for large dimensional point sets, since the number
of children of an internal node is equal to .
Besides, the chosen data structure needs to handle both insertions and deletions,
which is costly to achieve with range trees and d-trees.

Therefore, for the incremental algorithm {\UBSI}, we still suggest 
to store local upper bounds in a linked list. 
The list is sorted in nondecreasing order of the sum of the component values 
of each local upper bound to avoid some of the dominance tests.

For the case of the nonincremental algorithm {\UBSNI},
we propose to take advantage of the information provided 
by the point set , which is assumed to be known in advance.
We suggest to use a combination of a d-tree and a set of linked lists.
Namely, we build a balanced d-tree from the points of .
Then we consider the partition of the objective space induced by this d-tree.
For each new local upper bound, we identify the cell of the partition it belongs to
by traversing the tree. Instead of creating a new node for this local upper bound,
we insert it in a linked list located in place of this potential new node.
To perform Step~\ref{alg:nonincremental:A} of {\UBSNI},
local upper bounds strictly dominated by a given point 
are identified by first searching the tree with the query interval 
 and then the linked lists containing local upper bounds 
in some cell intersecting . 
The corresponding local upper bounds can then be removed in constant time 
without altering the tree structure.



\section{A better bound on the worst-case time complexity of the WFG algorithm}\label{sec:wfg-complexity}

In this section, we propose an improved analysis 
of the worst-case time complexity of the WFG algorithm of \citet{WhiBraBar12}
While the upper bound provided by the authors is ,
we show that it
can be lowered at least to , matching the upper bound of the  
Hypervolume by Slicing Objectives (HSO) of \citet{WhiHinBarHub06}.
Moreover, we show that its complexity is at least 
for .

In Section~\ref{sub:wfg}, we briefly describe the WFG algorithm. 
Then in Section~\ref{sub:wfg_complex} we prove the new upper and lower bounds.

\subsection{Brief description of the WFG algorithm}\label{sub:wfg}

The WFG algorithm computes the hypervolume in a recursive way.
Given a stable set  of points and a reference point ,
the volume of  is computed as the sum of the volume of 
and the \emph{exclusive hypervolume} associated to , i.e.
.
This last quantity is equal to , 
where  is the stable subset of all orthogonal projections of the points of  onto the dominance cone
.
This idea is illustrated with a 3-dimensional example represented in Figure~\ref{fig:wfg1}, 
where  and it is assumed that 
, for all .
The basic algorithm is summarized in Algorithm~\ref{alg:wfg1}, 
where  is the parallel maximum of  and , 
i.e.\  .

\begin{figure}
  \begin{center}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt}
  \end{center}
  
  \caption{Orthogonal projection on the hyperplane  of a 3-dimensional instance\label{fig:wfg1}}
\end{figure}



\begin{algorithm}  
  \begin{tabular}{lll}
    \WFG{}~     &  =~ & 0\; \\
    &&\-8pt]
    \WFG{}~ &  =~ &  \WFG{} + \WFG{}  \WFG{}\;\\
                            &     & where \;
  \end{tabular}\;
  \caption{The basic WFG algorithm\label{alg:wfg1}}
\end{algorithm}

If the points are sorted in non-decreasing order of some component, say component , 
then, as remarked by \citet{WhiHinBarHub06},
\WFG{} is a -dimensional subproblem. 
Indeed, the dominated region of , , 
can be described by sliding its orthogonal projection on the hyperplane
of equation  along the  axis, 
from level  to level .
Therefore, the hypervolume of  can be obtained by multiplying by 
the hypervolume of  projected on the hyperplane defined by .
Besides, a simple algorithm is used for subproblems with .
The basic algorithm together with this important enhancement 
is what we refer to as WFG algorithm hereafter.


\subsection{New upper and lower bounds}\label{sub:wfg_complex}

We first show an improved upper bound in Proposition~\ref{prop:wfg-complexity}.

\begin{proposition}\label{prop:wfg-complexity}
  The worst-case time complexity of the WFG algorithm 
  in the case where the points are sorted in non-decreasing order of some arbitrary component 
  is bounded by .
\end{proposition}

\begin{proof}
  Let  be the worst-case time complexity of the WFG algorithm 
  as described in Algorithm~\ref{alg:wfg1}, 
  with  and . We have:
  
  where , , and  are the 
  worst-case time complexities of \WFG{}, \WFG{}, and \WFG{}, respectively,
  and  is an upper bound on the complexity of computing ,
   and  being constants with respect to  and .  
  We have  if the points are sorted in non-decreasing order of some component, therefore we obtain 
  . It follows, since  that 
  
  for any integers  and .
  If an -algorithm \citep{BeuFonLopPaqVah09} or even just
  an -algorithm is used for the case , then the complexity becomes
  
  for any integers  and .
\end{proof}



Now we prove a non-trivial lower bound in Proposition~\ref{prop:wfg-complexity1}

\begin{proposition}\label{prop:wfg-complexity1}
  The worst-case complexity of the WFG algorithm is 
  
  for .
\end{proposition}

\begin{proof}
Let 
and  be a null matrix of the same order as .
For any  and any even 
we define the following -dimensional
block-matrix:

and consider the -dimensional instance 
where the points are the  rows of .
Note that the rows are already sorted in non-decreasing order of component .
Consider any point  taken among the last  rows of ,
denoted by ,
and the computation of \WFG{},
where  is the set of all rows above  in .
The set  involved in this computation contains
all the first  rows of 
with the last two components replaced by  and , respectively,
and, in the case , the vector 
(and possibly further points, depending on the value of 
 satisfying , that are however dominated by 
 in ).
If the points of  are sorted in non-decreasing order of component ,
all (case ) or all but the last point (case ) of  
have identical values on components  and ,
the other components being those of .
Therefore, for each of these  computations,
there will be a call of {\WFG} on an instance of the same structure 
with  points of dimension .
Thus we have:

which, given that , is equivalent to

Given that the recursions with  are solved in ,
we obtain  when  is even.

For an odd , we apply the above analysis for ,
adding a zero-column to  . 
We then obtain  in this case.

Overall we have shown that 
for any .
\end{proof}



Note that we assumed in the proof of Proposition~\ref{prop:wfg-complexity1}
that, for each recursive call of {\WFG}, 
the sorted component is imposed to the algorithm.
This corresponds to the description given by the authors as well as their implementation.
Other choices for the sorted component than the one given in the proof
can make the solution 
to the instance type given in the proof a lot more efficient.


\section{Computational experiments}\label{sec:expe}

In this section, we provide the setup (Sections~\ref{sub:setup} and~\ref{sub:instances}) 
and the results (Section~\ref{sub:results}) of computational experiments
conducted to compare the efficiencies of Algorithms~\ref{alg:incremental} and~\ref{alg:nonincremental},
as well as the HV4D \citep{GueFonEmm12}, QHV \citep{RusFra14}, and WFG \citep{WhiBraBar12} algorithms
to compute the hypervolume indicator.

\subsection{Implementations and general setup}\label{sub:setup}

We implemented HBDA-I and HBDA-NI
in C.
We used for the HV4D, QHV, and WFG algorithms the implementations provided by the authors,
namely \citet[version 2.0 RC 2]{HV4D},  
\citet[retrieved on April, 2015]{QHV}, 
and \citet[version 1.10]{WFG}, respectively.
We note that, in the implementation of the QHV algorithm, 
the number of objectives is fixed at compile time,
thus the final executable may take advantage of this information,
which the other implementations do not.

From the implementation of the WFG algorithm, we derived an incremental version.
In this version, the initial set of points is not assumed to be sorted, 
thus it may not be known entirely in advance.
The auxiliary sets  (see Algorithm~\ref{alg:wfg1}) are, however, sorted 
since they are built from points for which the exclusive hypervolume has already been computed.
In other words, this corresponds to considering 
that the initial set of points is sorted according to an extra -th 
component (compatible with the order in which the points are actually processed)
that is, however, not taken into account in the computation of the hypervolume.
We denote by ''WFG incremental`` this approach.

All implementations were compiled using \texttt{GCC 4.3.4} with the same options 
\texttt{-O3 -DNDEBUG -march=native}. Compilation and tests were both performed 
under SUSE Linux Enterprise Server 11 
on identical workstations equipped with four Intel Xeon E7540 CPU at 2.00GHz and with 128GB of RAM.




\subsection{Instances}\label{sub:instances}

All algorithms were tested on stable sets of points generated according to several schemes.
We considered the following four instance types:
\begin{itemize}
  \item[(C)] \emph{Concave} or so-called \emph{spherical} \citep{DebThiLauZit02} instances
  \item[(X)] \emph{Convex} instances
  \item[(L)] \emph{Linear} instances  
  \item[(H)] \emph{Hard} instances
\end{itemize}

\noindent Instances of types (C), (X), and (L) are obtained by drawing uniformly points 
from the open hypercube . Then each point  is modified as follows:
\begin{itemize}
  \item[(C)] , 
  for each , i.e.\ the component values of  are divided by
  their -norm,  \item[(X)] ,
  for each ,
  \item[(L)] , 
  for each , i.e.\ the component values of  are divided by
  their -norm.\end{itemize}

\noindent Note, in particular, that for type (C) and (X) instances, we followed the suggestion of \citet{RusFra14}
to project uniformly distributed points on a hypersphere, instead of using the instances of \citet{DebThiLauZit02}.

\noindent The points of the instances of types (C), (X), and (L) are randomly distributed on some subset of 
, namely 
 for type (C),  
 for type (X), and
 for type (L), 
where  denotes a hypersphere of  with center  and radius .



Instances of type (H) are motivated by the instance type built 
to derive a lower bound on the worst-case complexity of the WFG algorithm
in Section~\ref{sub:wfg_complex} (Equation~\ref{eq:simple_wc}).
We slightly modified these instances for the computational experiments
since the presence of many identical component values
could perturb the comparison depending on the way algorithms handle this case.
Therefore, we defined the following instance type in general position
which yields the same lower bound on the worst-case complexity of the WFG algorithm.
Let .
For any even  and , a hard instance 
is defined by the set of all rows of the following block matrix:

and consists of  points of dimension .
Matrices  have the same property of matrices ,
which is to have decreasing coefficients on the first column and increasing in the second column.
Moreover,  is also built by repeating along the diagonal
the same matrix, which has strictly larger coefficients
than the other matrices of the block matrix.

Some implementations of the algorithms we consider in this section 
have certain restrictions on the instances that can be solved. 
We summarize these restrictions in Table~\ref{tab:restrictions}.

\begin{table}
  \begin{center}
    \setlength{\tabcolsep}{5pt}
    \resizebox{\textwidth}{!}{\begin{tabular}{lcccc}
      \hline
      Algorithm          & Optimization direction & Coordinates range & Ref. point & \# points\\\hline
      HBDA-I, HBDA-NI  & \emph{any}             & \emph{any}        & \emph{any} & \emph{any} \\
      HV4D               & minimize               & \emph{any}        & \emph{any} & \emph{any} \\
      QHV                & maximize               &            &     &  \\
      WFG                & maximize               & \emph{any}        & \emph{any} & \emph{any} \\\hline
    \end{tabular}}
  \end{center}

  \caption{Restrictions of the tested implementations of the hypervolume algorithms considered for computational experiments\label{tab:restrictions}}
\end{table}

Because of the restrictions of the implementation of the QHV algorithm, 
we normalized the hard instances so that the points are all in 
the open hypercube .
Moreover, for all instances, we chose as reference point the all-ones vector 
and the null vector in the minimization and maximization cases, respectively.
To cope with the restrictions on the optimization direction, 
we generated instances primarily for the minimization case, 
and for each instance, a symmetric instance with points in  for the maximization case,
by taking for each point  and component  the complement .

For types (C), (X), and (L) we generated instances for each 
and . For type (H) we considered 
 and for each value of  we chose 10 values for the parameter 
so as to obtain 10 sizes of instances.
Since for any algorithm, type (H) instances are significantly harder to solve 
than the other types considered in this paper,
we limited the maximal number of points to 1\,000, 900, 300, and 150 
for , respectively.

For a fixed type, number of objectives, and number of points, we generated 10 instances. 
The implementations were run up to 100 times on small instances to obtain significant
computation times. The results we report are thus averaged.


\subsection{Results}\label{sub:results}

Figures~\ref{fig:res_concave}, \ref{fig:res_convex}, \ref{fig:res_linear}, and~\ref{fig:res_wfg_hard} 
show computation times for nonincremental algorithms 
on instances of type (C), (X), (L) and (H), respectively.

\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Nonincremental algorithms on type (C) instances \label{fig:res_concave}}
\end{figure}

\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Nonincremental algorithms on type (X) instances\label{fig:res_convex}}
\end{figure}





\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Nonincremental algorithms on type (L) instances \label{fig:res_linear}}
\end{figure}


\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt}
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Nonincremental algorithms on type (H) instances\label{fig:res_wfg_hard}}
\end{figure}







Our approach HBDA-NI performs better 
than all other algorithms on types (C), (X), and (L), 
for , and on type (H) for all the tested dimensions above 4.
For 4-dimensional instances of any type, 
it is confirmed that the HV4D algorithm
performs the best while the computation times obtained with HBDA-NI
are very close.
HBDA-NI is still faster than the QHV algorithm for 
on concave instances.




We also show computation times for the incremental algorithms (HBDA-I
and the incremental implementation of the WFG algorithm)
in Figures~\ref{fig:res_incr_concave}, \ref{fig:res_incr_convex}, \ref{fig:res_incr_linear}, and~\ref{fig:res_incr_wfg_hard}. 




\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt}
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Incremental algorithms on type (C) instances \label{fig:res_incr_concave}}
\end{figure}

\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt}
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Incremental algorithms on type (X) instances \label{fig:res_incr_convex}}
\end{figure}

\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt}
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Incremental algorithms on type (L) instances \label{fig:res_incr_linear}}
\end{figure}


\begin{figure}
  \begin{center}\rotatebox[origin=c]{90}{\hfill\footnotesize Computation time (seconds)}
    \begin{tabular}{rr}
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} \\
    \includegraphics{hypervolume-arxiv-figure\thepiccnt.pdf}\stepcounter{piccnt} &
    \end{tabular}
    
    {\footnotesize Number of points}
    
    \medskip
    \includegraphics{hypervolume-arxiv-figure_crossref\thelegendcnt.pdf}
    \stepcounter{legendcnt}
  \end{center}
\caption{Incremental algorithms on type (H) instances \label{fig:res_incr_wfg_hard}}
\end{figure}





According to these results, the incremental WFG algorithm
performs significantly better than Algorithm~\ref{alg:incremental} for almost all instances
except on 6 and 8 objectives hard instances, where both algorithms behave similarly.
The relative poorer efficiency of our incremental approach 
can be explained by the fact that its implementation lacks
an efficient data structure to identify local upper bounds
strictly dominated by the point that is currently processed.
Also the incremental WFG algorithm is still able to reduce the dimension of the points in subproblems.







\section{Conclusion}\label{sec:concl}

In this paper, we investigated a new way of computing the hypervolume indicator 
by calculating a partition of the dominated region into hyperrectangles.
This decomposition is based on the computation of local upper bounds. 
We proposed an incremental and a nonincremental approach.
These approaches provide a good worst-case complexity
and the nonincremental version, through an efficient implementation
is very competitive in practice.
In fact, we demonstrate that computing explicitly the dominated region,
in the sense of computing all its vertices, i.e.\ all local upper bounds 
additionally to feasible points,
is an interesting approach with respect to the computation time.

Future work includes improving the incremental approach.
This could be done by implementing an efficient dynamic data structure to identify
the local upper bounds that have to be updated or removed when a new point is considered,
which is subject to current work.
Alternatively, a special property of the dominated region
such as the neighborhood relation between local upper bounds 
elaborated in \cite{DaeKlaLacVan15} could be exploited.

It would also be interesting to refine the analysis 
of the worst-case time complexity of the WFG algorithm,
because the gap between the lower and upper bounds we showed is still large.
Finally, one could think of using the concept of local upper bounds 
and the associated decomposition of the dominated region
to compute hypervolume contributions.


\bibliographystyle{abbrvnat}
\bibliography{hypervolume-arxiv}
\end{document}