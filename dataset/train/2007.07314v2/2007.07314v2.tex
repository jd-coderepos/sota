We now present experiments confirming our main claims:
\begin{enumerate*}[label=(\roman*),itemsep=0pt,topsep=0pt,leftmargin=16pt]
    \item on simple binary problems, existing weight normalisation and loss modification techniques 
    may not converge to the optimal solution (\S\ref{sec:synth-expt});
    
    \item on real-world datasets, 
    our post-hoc logit adjustment 
    outperforms weight normalisation,
    \newedit{
    and
    one can obtain further gains via our logit adjusted softmax cross-entropy
    (\S\ref{sec:experiments-real}).}
\end{enumerate*}

\subsection{Results on synthetic dataset}
\label{sec:synth-expt}

We 
consider 
a binary classification task,
wherein samples from class  are drawn from a 2D Gaussian
with 
isotropic covariance
and means .
We introduce class imbalance by setting .
The Bayes-optimal classifier 
for the balanced error is (see Appendix~\ref{app:gaussian_ber})

i.e., it is a linear separator passing through the origin.
We compare this separator against those found by 
several 
margin
losses
based on~\eqref{eqn:unified-margin-loss}:
standard ERM (),
the adaptive loss~\citep{Cao:2019} (),
an instantiation of the equalised loss~\citep{Tan:2020} (),
and our logit adjusted loss ().
For each loss, we train an affine classifier on a sample of  instances,
and evaluate the balanced error on a test set of  samples
over  independent trials.

Figure~\ref{fig:lt_synthetic} confirms that 
the logit adjusted margin loss attains a balanced error close to that of the Bayes-optimal,
which is visually reflected by its learned separator closely matching that in~\eqref{eqn:bayes-gaussian}.
This is in line with our claim of the logit adjusted margin loss being consistent for the balanced error, unlike other approaches.
Figure~\ref{fig:lt_synthetic} also compares post-hoc weight normalisation and logit adjustment
for varying scaling parameter  (c.f.~\eqref{eqn:weight-normalisation},~\eqref{eqn:logit-adjustment}).
Logit adjustment is seen to approach the performance of the Bayes predictor;
\emph{any} weight normalisation is however seen to hamper performance.
This verifies the consistency of logit adjustment,
and inconsistency of weight normalisation (\S\ref{sec:weight-norm-critique}).

\begin{figure}[!t]
    \centering
    \resizebox{0.99\linewidth}{!}
    {
\includegraphics[scale=0.21,valign=t]{figs/lt_synthetic_boxplot.pdf}
\quad \includegraphics[scale=0.21,valign=t]{figs/lt_synthetic_separators.pdf}
\quad \includegraphics[scale=0.21,valign=t]{figs/lt_synthetic_post_hoc.pdf}
}

    \caption{
    Results on synthetic binary classification problem.
Our logit adjusted loss tracks the Bayes-optimal solution
    and separator (left \& middle panel).
    Post-hoc logit adjustment matches the Bayes performance
    with suitable scaling (right panel);
    however, \emph{any} weight normalisation fails.
    }
    \label{fig:lt_synthetic}
    \vspace{-0.5\baselineskip}
\end{figure}



\subsection{Results on real-world datasets}
\label{sec:experiments-real}

We present results on the CIFAR-10, CIFAR-100, ImageNet and iNaturalist 2018 datasets.
Following prior work,
we create ``long-tailed versions'' of the CIFAR datasets by suitably downsampling examples per label 
following the
\expp profile of~\citet{Cui:2019,Cao:2019} with imbalance ratio 
. 
Similarly, we use the long-tailed version of ImageNet produced by~\citet{Liu:2019}.
We employ a ResNet-32 for CIFAR,
and a ResNet-50 for ImageNet and iNaturalist.
All models are trained using SGD with momentum;
see Appendix~\ref{app:architectures} for more details.
See also Appendix~\ref{app:additional_results} for results on CIFAR under the {\sc Step} profile considered in the literature.

\begin{table}[!t]
    \centering
    \renewcommand{\arraystretch}{1.25}

    \scalebox{0.8}{
    \begin{tabular}{@{}lllll@{}}
        \toprule
        \textbf{Method} & \textbf{CIFAR-10-LT} & \textbf{CIFAR-100-LT} & \textbf{ImageNet-LT} & \textbf{iNaturalist} \\
        \toprule
        ERM                  & 
        27.16 & 
61.64 &
53.11 &
38.66 \\
        Weight normalisation ()~\citep{Kang:2020} & 
        24.02 & 
58.89 &
52.00 &
        48.05 \\ Weight normalisation ()~\citep{Kang:2020} & 
        21.50 & 
58.76 &
49.37 & 
34.10 \\ Adaptive~\citep{Cao:2019}             & 
        26.65 & 
60.40 & 
52.15 &
35.42 \\
        Equalised~\citep{Tan:2020}            & 
        26.02 & 57.26 & 54.02 &
38.37 \\
        \midrule
        Logit adjustment post-hoc () & 
        22.60 & 
58.24 &
49.66 &
33.98 \\
        Logit adjustment post-hoc () & 
        \best{19.08} & 
        57.90 & 
49.56 &
        33.80 \\
        Logit adjustment loss ()    & 
        {22.33}      & 
        \best{56.11} & \best{48.89} &
        \best{33.64} \\
        \bottomrule
    \end{tabular}
    }

    \caption{
    Test set balanced error (averaged over  trials) on real-world datasets.
    \arxiv{}{We use a ResNet-32 for the CIFAR datasets, and ResNet-50 for the ImageNet and iNaturalist datasets.}
    Here, ,  are numbers for ``LDAM + SGD'' from~\citet[Table 2, 3]{Cao:2019}
    and ``-normalised'' from~\citet[Table 3, 7]{Kang:2020}.
    \arxiv{See Figure~\ref{fig:cifar100_post_hoc_comparison} for results when , and discussion about further extensions.}{Here,  refers to using the best possible value of tuning parameter .
    See Figure~\ref{fig:cifar100_post_hoc_comparison} for plots as a function of , and the ``Discussion'' subsection for further extensions.}
    }
    \label{tbl:results}
    \arxiv{\vspace{-\baselineskip}}{}
\end{table}

\textbf{Baselines}.
We consider:
\begin{enumerate*}[label=(\roman*)]
    \item empirical risk minimisation (ERM) on the long-tailed data,

    \item post-hoc weight normalisation~\citep{Kang:2020}
per \eqref{eqn:weight-normalisation} (using  and ) 
applied to ERM,

    \item the adaptive margin loss~\citep{Cao:2019}
    per~\eqref{eqn:cao},
    and

    \item the equalised loss~\citep{Tan:2020}
    per~\eqref{eqn:equalised},
    with 

    for the threshold-based  of~\citet{Tan:2020}.
\end{enumerate*}
\citet{Cao:2019} demonstrated superior performance of their adaptive margin loss against several other baselines,
such as the balanced loss of~\eqref{eqn:balanced-loss},
and that of~\citet{Cui:2019}.
Where possible, we report numbers for the baselines 
(which use the same setup as above)
from the respective papers.
See also our concluding discussion about extensions to such methods that improve performance.

We compare the above methods against
our proposed post-hoc logit adjustment~\eqref{eqn:logit-adjustment},
and logit adjusted loss~\eqref{eqn:logit-adjusted-loss}.
For post-hoc logit adjustment, we fix the scalar ;
we analyse the effect of tuning this in Figure~\ref{fig:cifar100_post_hoc_comparison}.
We do not perform \emph{any} further tuning of our logit adjustment techniques.

\textbf{Results and analysis}.
Table~\ref{tbl:results} summarises our results,
which demonstrate
our proposed logit adjustment techniques consistently outperform existing methods.
\newedit{
Indeed,
while weight normalisation offers gains over ERM, these are improved significantly by post-hoc logit adjustment (e.g., 8\% relative reduction on CIFAR-10).
Similarly loss correction techniques are generally outperformed by our logit adjusted softmax cross-entropy (e.g., 6\% relative reduction on iNaturalist).


\begin{figure}[!t]
    \centering

    \subcaptionbox{CIFAR-10.}{\includegraphics[scale=0.21]{figs/cifar10_post_hoc_comparison.pdf}}\quad \subcaptionbox{CIFAR-100.}{\includegraphics[scale=0.21]{figs/cifar100_post_hoc_comparison.pdf}}\quad \subcaptionbox{iNaturalist.}{\includegraphics[scale=0.21]{figs/inat2018_post_hoc_comparison.pdf}}

    \caption{
Comparison of balanced error
    for post-hoc correction techniques    
    when varying scaling parameter  (c.f.~\eqref{eqn:weight-normalisation},~\eqref{eqn:logit-adjustment}).
    Post-hoc logit adjustment consistently outperforms weight normalisation.
    }
    \label{fig:cifar100_post_hoc_comparison}
    \arxiv{\vspace{-0.5\baselineskip}}{}
\end{figure}

\begin{figure}
    \centering
\subcaptionbox{CIFAR-10.}{\includegraphics[scale=0.1875,valign=t]{figs/cifar10_per_class_acc.pdf}}\quad \subcaptionbox{CIFAR-100.}{\includegraphics[scale=0.1875,valign=t]{figs/cifar100_per_class_acc.pdf}}
    \quad \subcaptionbox{iNaturalist.}{\includegraphics[scale=0.1875,valign=t]{figs/inat2018_per_class_acc.pdf}}    


    \caption{Per-class error rates of loss modification techniques. 
For (b) and (c), we aggregate the classes into 10 groups.
    ERM displays a strong bias towards dominant classes (lower indices).
    Our proposed logit adjusted softmax loss achieves significant gains on rare classes (higher indices). 
}
    \label{fig:per_class_accuracies}
    \arxiv{\vspace{-1.5\baselineskip}}{}
\end{figure}

Figure~\ref{fig:cifar100_post_hoc_comparison}  studies the effect of tuning the scaling parameter 
afforded by post-hoc weight normalisation (using ) and post-hoc logit adjustment.
Even without \emph{any} scaling, post-hoc logit adjustment generally offers superior performance to the best result from weight normalisation {(cf.~Table~\ref{tbl:results})};
with scaling, this is further improved.
See Appendix~\ref{sec:additional_experiments} for a plot on ImageNet-LT.

Figure~\ref{fig:per_class_accuracies} breaks down the per-class accuracies on CIFAR-10, CIFAR-100, and iNaturalist.
On the latter two datasets, for ease of visualisation,
we aggregate the classes into ten groups based on their frequency-sorted order (so that, e.g., group  comprises the top  most frequent classes).
As expected,
dominant classes generally see a lower error rate with all methods.
However,
the logit adjusted loss is seen to systematically improve performance over ERM, particularly on rare classes.

While our logit adjustment techniques perform similarly,
there is a slight advantage to the loss function version.
Nonetheless, the strong performance of post-hoc logit adjustment corroborates the ability to decouple representation and classifier learning in long-tail settings~\citep{Zhang:2019}.

\arxiv{\textbf{Discussion and future work}.}{\textbf{Discussion and extensions}}
Table~\ref{tbl:results} shows the advantage of logit adjustment over recent post-hoc and loss modification proposals,
under standard setups from the literature.
We believe further improvements are possible by fusing complementary ideas, and remark on four such options.

First,
one may use a more complex base architecture;
our choices are standard in the literature,
but, e.g.,~\citet{Kang:2020} found gains on ImageNet-LT by employing a ResNet-152,
with further gains from training it for  as opposed to the customary  epochs.
\arxiv{Given the results in Figure~\ref{fig:cifar100_post_hoc_comparison}, we believe logit adjustment can similarly see gains.}{Table~\ref{tbl:results_architecture} confirms that logit adjustment similarly benefits from this choice.
For example, on iNaturalist, we obtain an improved balanced error of {} for the logit adjusted loss.
When training for more () epochs per the suggestion of~\citet{Kang:2020}, this further improves to {}.

{
Second, one may combine together the 's for various special cases of the pairwise margin loss. 
Indeed, we find that combining our relative margin with the adaptive margin of~\citet{Cao:2019} 
---
i.e., using the pairwise margin loss with 
---
results in a top- accuracy of {} on iNaturalist.
When using a ResNet-152, this further improves to {} when trained for 90 epochs,
and {} when trained for 200 epochs.
While such a combination is nominally heuristic, we believe there is scope to formally study such schemes, e.g., in terms of induced generalisation performance.
}

\begin{table}[!t]
    \centering
    \renewcommand{\arraystretch}{1.25}

    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}llllll@{}}
        \toprule
        & \multicolumn{2}{c}{\bf ImageNet-LT} & \multicolumn{2}{c}{\bf iNaturalist} \\
        \textbf{Method} & \textbf{ResNet-50} & \textbf{ResNet-152} & \textbf{ResNet-50} & \textbf{ResNet-152} & \textbf{ResNet-152} \\
        \textbf{} & \textbf{} & \textbf{} & \textbf{90 epochs} & \textbf{90 epochs} & \textbf{200 epochs} \\
        \toprule
        ERM                  & 
        53.11 &  53.30 &
        38.66 & 
        35.88 & 
        34.38 \\
        Weight normalisation ()~\citep{Kang:2020} & 
        52.00 & 51.49 &
        48.05 & 
        45.17 & 
        45.33 \\ Weight normalisation ()~\citep{Kang:2020} & 
        49.37 &
        48.97 &
        34.10 & 
        31.85 & 
        30.34 \\ Adaptive~\citep{Cao:2019}             & 
        52.15 & 53.34 &
        35.42 & 
        31.18 & 
        29.46 \\
        Equalised~\citep{Tan:2020}            & 
        54.02 & 51.38 &
        38.37 & 
        35.86 & 
        34.53 \\
        \midrule
        Logit adjustment post-hoc () & 
        49.66 & 49.25 &
33.98 & 
        31.46 & 
        30.15 \\
        Logit adjustment post-hoc () & 
        49.56 & 49.15 &
        33.80 & 
        31.08 & 
        29.74 \\
        Logit adjustment loss ()    & 
        \best{48.89} & \best{47.86} &
        {33.64} & 
        {31.15} & 
        30.12 \\
        Logit adjustment plus adaptive loss ()    & 
        {51.25} & {50.46} & 
        \best{31.56} & 
        \best{29.22} & 
        \best{28.02} \\
        \bottomrule
    \end{tabular}
    }

    \caption{
    Test set balanced error (averaged over  trials) on real-world datasets with more complex base architectures.
    Employing a ResNet-152 is seen to systematically improve all methods' performance, with logit adjustment remaining superior to existing approaches.
    The final row reports the results of combining logit adjustment with the adaptive margin loss of~\citet{Cao:2019}, which yields further gains on iNaturalist.
    }
    \label{tbl:results_architecture}
    \arxiv{\vspace{-\baselineskip}}{}
\end{table}
}

{Third,}
\citet{Cao:2019} observed that their loss benefits from a deferred reweighting scheme (DRW), wherein the model 
begins training
as normal,
and then
applies class-weighting
after a fixed number of epochs.
On CIFAR-10-LT and CIFAR-100-LT,
this 
achieves  and 
error respectively;
both are outperformed by
our vanilla logit adjusted loss.
On iNaturalist\arxiv{}{ with a ResNet-50}, 
this
achieves an error of , outperforming our .
\arxiv{However, given the strong improvement of our loss over that in~\citet{Cao:2019} when both methods use SGD,
we expect that employing DRW 
(which applies to any loss)
can be similarly beneficial for our method.}
{(Note that our simple combination of the relative and adaptive margins outperforms these reported numbers of DRW.)
However, given the strong improvement of our loss over that in~\citet{Cao:2019} when both methods use SGD,
we expect that employing DRW 
(which applies to any loss)
may be similarly beneficial for our method.}

\arxiv{Second,}{Fourth,} per~\S\ref{sec:background}, one may perform data augmentation;
e.g., see~\citet[Section 6]{Tan:2020}.
While further exploring such variants are of empirical interest,
we hope to have illustrated the conceptual and empirical value of logit adjustment,
and leave this for future work.
}
