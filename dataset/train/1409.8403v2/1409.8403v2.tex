\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{dsfont}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{color}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\iftrue
	\newcommand{\topic}[1]{\textcolor{gray}{\textbf{(#1.)}}}
	\newcommand{\outline}[1]{{\textcolor{blue}{[[{#1}]]}}}
	\newcommand{\commenttext}[1]{\textcolor{red}{[[{#1}]]}}
	\newcommand{\commentfoot}[1]{\footnote{\textcolor{red}{\textit{#1}}}}
\else 
	\newcommand{\topic}[1]{}
	\newcommand{\outline}[1]{}
	\newcommand{\commenttext}[1]{}
	\newcommand{\commentfoot}[1]{}
\fi

\newcommand{\cutcaptiondown}{\vspace*{-0.12in}}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{algpseudocode}
\usepackage{algorithm}
\graphicspath{{./images/}}


\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\myparagraph}[1]{\vspace{2pt}\noindent{\bf #1}}
\newcommand{\SJElong}{Structured Joint Embedding\xspace}
\newcommand{\SJE}{SJE\xspace}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\mynote}[2]{{\bf {#1}:~{#2}}}
\newcommand{\mat}{\boldsymbol}
\renewcommand{\vec}{\boldsymbol}
\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\amax{\mathop{\rm max}}
\def\amin{\mathop{\rm min}}

\def\loss{\ell}
\def\a{\alpha}
\def\d{\delta}
\def\l{\lambda}
\def\D{\Delta}
\def\Re{\mathbb R}
\def\p{\varphi}
\def\1{\mathds{1}}
\def\b{\beta}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{{e.g}\onedot} \def\Eg{{E.g}\onedot}
\def\ie{{i.e}\onedot} \def\Ie{{I.e}\onedot}
\def\cf{{c.f}\onedot} \def\Cf{{C.f}\onedot}
\def\etc{{etc}\onedot} \def\vs{{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{{et al}\onedot}

\g@addto@macro\normalsize{\setlength\abovedisplayskip{7pt}
  \setlength\belowdisplayskip{7pt}
  \setlength\abovedisplayshortskip{5pt}
  \setlength\belowdisplayshortskip{5pt}
}

\makeatother

\usepackage{framed}
\usepackage{xcolor}
\definecolor{gainsboro}{RGB}{220,220,220}

\newlength{\imgwidth}
\newlength{\imgheight}

\newcommand{\w}{\mathbf{w}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\def\minimize{\operatornamewithlimits{Minimize}}

\newcommand{\note}[2]{{\bf \color{red}{#1}:~{#2}}}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}


\cvprfinalcopy 

\def\cvprPaperID{1690} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Evaluation of Output Embeddings for Fine-Grained Image Classification}

\author{Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee and Bernt Schiele \vspace{4mm} \\ 
{\small
\begin{tabular}{cp{1cm}c}
 Computer Vision and Multimodal Computing & &  Computer Science and Engineering Division \\
 Max Planck Institute for Informatics, Saarbrucken, Germany & & University of Michigan, Ann Arbor \\
\end{tabular}
}
}

\maketitle


\begin{abstract}
Image classification has advanced significantly in recent years with the availability of large-scale image sets.
However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of fine-grained categories.
This project shows that compelling classification performance can be achieved on such categories even without labeled training data. 
Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score.
We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora.
We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets.
Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with fine-grained text) achieve compelling results, even outperforming the previous supervised state-of-the-art.
By combining different output embeddings, we further improve results.
\end{abstract}




\vspace*{-0.15in}

\section{Introduction}

The image classification problem has been redefined by the emergence of large scale datasets such as ImageNet~\cite{DDS09}. 
Since deep learning methods~\cite{KSH12} dominated recent Large-Scale Visual Recognition Challenges (ILSVRC12-14), the attention of the computer vision community has been drawn to Convolutional Neural Networks (CNN)~\cite{LBBH98}.
Training CNNs requires massive amounts of labeled data; but, in fine-grained image collections, where the categories are visually very similar, the data population decreases significantly.
We are interested in the most extreme case of learning with a limited amount of labeled data, zero-shot learning, in which no labeled data is available for some classes.



Without labels, we need alternative sources of information that relate object classes. 
Attributes~\cite{FZ07, FEHF09, LNH13}, which describe well-known common characteristics of objects, are an appealing source of information, and they can be easily obtained through crowd-sourcing techniques~\cite{DKF13, PG11}. However, fine-grained concepts present a special challenge: due to the high degree of similarity among categories, a large number of attributes are required to effectively model these subtle differences.
This increases the cost of attribute annotation. 
One aim of this work is to move towards 
eliminating the human labeling component from zero-shot learning, \eg by using alternative sources of information. 
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth, trim=30 40 30 40]{SJE}
\end{center}
\caption{\SJElong leverages images () and labels () by learning parameters  of a function  that measures the compatibility between input () and output embeddings (). It is a general framework that can be applied to any learning problem with more than one modality.} \vspace{-2mm}
\label{fig:methodology}
\cutcaptiondown
\end{figure}

On the other hand, large-margin support vector machines (SVM) operate with labeled training images, so a lack of labels limits their use for this task.
Inspired by previous work on label embedding~\cite{WBU10, BWG10, APHS13} and structured SVMs~\cite{TJH05,NL11}, we propose to use a \SJElong (\SJE) framework (Fig.~\ref{fig:methodology}) that relates input embeddings (\ie image features) and output embeddings (\ie side information) through a compatibility function, therefore taking advantage of a structure in the output space. The \SJE framework separates the subspace learning problem from the specific input and output features used in a given application. As a general framework, it can be applied to any learning problem where more than one modality is provided for an object. 

Our contributions are: (1) We demonstrate that unsupervised class embeddings trained from large unlabeled text corpora are competitive to previously published results that use human supervision. (2) Using the most recent deep architectures as input embeddings, we significantly improve the state-of-the-art (SoA). (3) We extensively evaluate several unsupervised output embeddings for fine-grained classification in a zero-shot setting on three challenging datasets. (4) 
By combining different output embeddings we obtain best results, surpassing the SoA by a large margin. 
(5) We propose a novel weakly-supervised Word2Vec variant that improves the accuracy when combined with other output embeddings.  

The rest of the paper is organized as follows. Section~\ref{sec:prev} provides a review of the relevant literature; Sec.~\ref{sec:model} details the \SJE method; Sec.~\ref{sec:output_emb} explains the output embeddings that we analyze; Sec.~\ref{sec:exp} presents our experimental evaluation; Sec.~\ref{sec:conclusion} presents the discussion and our conclusions.





\section{Related Work }
\label{sec:prev}
Learning to classify in the absence of labeled data (zero-shot learning)~\cite{YA10,RSS11,KKTH12,LNH13,APHS15,NMBSSFCD13,MGS14, FHXFG14} is a challenging problem, and achieving better-than-chance performance requires structure in the output space.
Attributes~\cite{FZ07, FEH10, LNH13} provide one such space; they relate different classes through well-known and shared characteristics of objects. 


Attributes, which are often collected manually~\cite{KKTH12, PG11, DPCG12}, have shown promising results in various applications, \ie caption generation~\cite{KPD11,OKB11}, face recognition~\cite{SKBB12,CGG13}, image retrieval~\cite{SFD11,DRS11}, action recognition~\cite{LKS11,YJKLGL11} and image classification~\cite{LNH13, APHS15}.
The main challenge of attribute-based zero-shot learning arises on more challenging fine-grained data collections~\cite{CaltechUCSDBirdsDataset, StanfordDogsDataset}, in which categories may visually differ only subtly.
Therefore, generic attributes fail at modeling small intra-class variance between objects. Improved performance requires a large number of specific attributes which increases the cost of data gathering.


As an alternative to manual annotation, side information can be collected automatically from text corpora. Bag-of-words~\cite{H54} is an example where class embeddings correspond to histograms of vocabulary words extracted automatically from unlabeled text. Another example is using taxonomical order of classes~\cite{TJH05} as structured output embeddings. Such a taxonomy can be built automatically from a pre-defined ontology such as WordNet~\cite{WordNet,RSS10,APHS13} . In this case, the distance between nodes is measured using semantic similarity metrics~\cite{JC97, LC94, Lin98, Res95}. Finally, distributed text representations~\cite{MSCCD13, PSM14} learned from large unsupervised text corpora can be employed as structured embeddings. We compare several representatives of these methods (and their combinations) in our evaluation.

Embedding labels in an Euclidean space is an effective tool to model latent relationships between classes~\cite{BWG10}. These relationships can be collected separately from the data~\cite{HKL09, DB95}, learned from the data~\cite{WBU10,Hastie:Tibshirani:Friedman:2008} or derived from side information~\cite{FZ07, FCSBM13, APHS13, NMBSSFCD13}. In order to collect relationships independently of data, compressed sensing~\cite{HKL09} uses random projections whereas Error Correcting Output Codes~\cite{DB95} builds embeddings inspired from information theory. WSABIE~\cite{WBU10} uses images with their corresponding labels to learn an embedding of the labels, and CCA~\cite{Hastie:Tibshirani:Friedman:2008} maximizes the correlation between two different data modalities. DeViSE~\cite{FCSBM13} employs a ranking formulation for zero-shot learning using images and distributed text representations. The ALE~\cite{APHS13} method employs an approximate ranking formulation for the same using images and attributes. ConSe~\cite{NMBSSFCD13}  uses the probabilities of a softmax-output layer to weigh the semantic vectors of all the classes. 
In this work, we use the multiclass objective to learn structured output embeddings obtained from various sources.

Among the closest related work, ALE~\cite{APHS13} uses Fisher Vectors (FV~\cite{PD07}) as input and binary attributes / hierarchies as output embeddings. Similarly, DeviSe~\cite{FCSBM13} uses CNN~\cite{KSH12} features as input and Word2Vec~\cite{MSCCD13} representations as output embeddings. In this work, we benefit from both ideas: (1) We use SoA image features, \ie FV and CNN, (2) among others, we also use attributes and Word2Vec as output embeddings. Our work differs from \cite{FCSBM13} \wrt two aspects: (1) We propose and evaluate several output embedding methods specifically built for fine-grained classification. (2) We show how some of these output embeddings complement each other for zero-shot learning on general and fine-grained datasets. The reader should be aware of~\cite{APHS15}.





\section{{\SJElong}s}
\label{sec:model}

In this work, we aim to leverage input and output embeddings in a joint framework by learning a compatibility between these embeddings. We are interested in the problem of zero-shot learning for image classification where training and test images belong to two disjoint sets of classes. 

Following~\cite{APHS13}, given input/output  and  from , \SJElong (\SJE) learns  by minimizing the empirical risk
 
where  defines the cost of predicting  when the true label is . Here, we use the  loss.

\subsection{Model} 
\label{subsec:model}
We define a compatibility function  between an input space  and a structured output space . Given a specific input embedding, we derive a prediction by maximizing the compatibility  over \SJE as follows:

The parameter vector  can be written as a  matrix  with  being the input embedding dimension and  being the output embedding dimension. This leads to the bi-linear form of the compatibility function:

Here, the input embedding is denoted by  and the output embedding by . The matrix  is learned by enforcing the correct label to be ranked higher than any of the other labels (Sec.~\ref{subsec:learning}), \ie multiclass objective. This formulation is closely related to~\cite{APHS13, FCSBM13, WBU10}. Within the label embedding framework, ALE~\cite{APHS13} and DeViSe~\cite{FCSBM13} use pairwise ranking objective, WSABIE~\cite{WBU10} learns both  and  through ranking, whereas we use multiclass objective. Similarly, \cite{PPH09,SGSBMN13} use the regression objective and CCA~\cite{Hastie:Tibshirani:Friedman:2008} maximizes the correlation of input and output embeddings. 

\subsection{Parameter Learning} 
\label{subsec:learning}
According to the unregularized structured SVM formulation~\cite{TJH05}, the objective is:

where the misclassification loss  takes the form:

For the zero-shot learning scenario, the training and test classes are disjoint. Therefore, we fix  to the output embeddings of training classes and learn . For prediction, we project a test image onto the  and search for the nearest output embedding vector (using the dot product similarity) that corresponds to one of the test classes.

We use Stochastic Gradient Descent (SGD) for optimization which consists in sampling  at each step and searching for the highest ranked class . If , we update  as follows: 

where  is the learning step-size used at iteration . We use a constant step size chosen by cross-validation and we perform regularization through early stopping.

\subsection{Learning Combined Embeddings}
\label{subsec:comb_emb}
For some classification tasks, there may be multiple output embeddings available, each capturing a different aspect of the structure of the output space. Each may also have a different signal-to-noise ratio. Since each output embedding possibly offers non-redundant information about the output space, as also shown in~\cite{RSS11,APHS15}, we can learn a better joint embedding by combining them together. We model the resulting compatibility score as

where  are the joint embedding weight matrices corresponding to the  output embeddings (). In our experiments, we first train each  independently, then perform a grid search over  on a validation set. Interestingly, we found that the optimal  for previously-seen classes is often different from the one for unseen classes. Therefore, it is critical to cross-validate  on the zero-shot setting.

Note that if we take , Equation~\ref{eqn:form_ensemble} is equivalent to simply concatenating the . This corresponds to stacking the  into a single matrix  and computing the standard compatibility as in Equation~\ref{eqn:form}. However, such a stacking learns a large  where a high dimensional  biases the final prediction. In contrast,  eliminates the bias, leading to better predictions. 
Thus,  can be thought of as the confidence associated with  whose contribution we can control. We show in Sec.~\ref{subsec:results} that finding an appropriate  can yield improved accuracy compared to any single .

\section{Output Embeddings}
\label{sec:output_emb}

In this section, we describe three types of output embeddings: human-annotated attributes, unsupervised word embeddings learned from large text corpora, and hierarchical embeddings derived from WordNet.

\subsection{Embedding by Human Annotation: Attributes}
\label{subsec:attributes}
Annotating images with class labels is a laborious process when the objects represent fine-grained concepts that are not common in our daily lives.
Attributes provide a means to describe such fine-grained concepts.
They model shared characteristics of objects such as color and texture which are easily annotated by humans and converted to machine-readable vector format.
The set of descriptive attributes may be determined by language experts~\cite{LNH13} or by fine-grained object experts~\cite{CaltechUCSDBirdsDataset}.
The association between an attribute and a category can be a binary value depicting the presence/absence of an attribute (~\cite{LNH13, APHS13, RSS11}) or a continuous value that defines the confidence level of an attribute (~\cite{LNH13, APHS15, RSSS12}) for each class.  
We write per-class attributes as:

where  can be  or a real number that associates a class with an attribute,  denotes the associated class and  is the number of attributes. Potentially,  encodes more information than . For instance, for classes \emph{rat, monkey, whale} and the attribute \emph{big},   implies that in terms of size \emph{rat}  \emph{monkey}  \emph{whale}, whereas  can be interpreted as \emph{rat}  \emph{monkey}  \emph{whale} which is more accurate. We empirically show the benefit of  over  in Sec.~\ref{subsec:results}.
In practice, our output embeddings use a per-class vector form, but they can vary in dimensionality (). For the rest of the section we denote the output embeddings as  for brevity.


\subsection{Learning Label Embeddings from Text}
\label{subsec:text}
In this section, we describe unsupervised and weakly-supervised label embeddings mined from text. With these label embeddings, we can (1) avoid dependence on costly manual annotation of attributes and (2) combine the embeddings with attributes, where available, to achieve better performance.

\myparagraph{Word2Vec ().} In Word2Vec~\cite{MSCCD13}, a two-layer neural network is trained to predict a set of target words from a set of context words. Words in the vocabulary are assigned with one-shot encoding so that the first layer  acts as a look-up table to retrieve the embedding for any word in the vocabulary. The second layer predicts the target word(s) via hierarchical soft-max. Word2Vec has two main formulations for the target prediction: skip-gram (SG) and continuous bag-of-words (CBOW). In SG, words within a local context window are predicted from the centering word. In CBOW, the center word of a context window is predicted from the surrounding words. Embeddings are obtained by back-propagating the prediction error gradient over a training set of context windows sampled from the text corpus.

\myparagraph{GloVe ().} GloVe~\cite{PSM14} incorporates co-occurrence statistics of words that frequently appear together within the document. Intuitively, the co-occurrence statistics encode meaning since semantically similar words such as ``ice'' and ``water'' occur together more frequently than semantically dissimilar words such as ``ice'' and ``fashion.'' The training objective is to learn word vectors such that their dot product equals the co-occurrence probability of these two words. This approach has recently been shown to outperform Word2Vec on the word analogy prediction task~\cite{PSM14}.

\myparagraph{Weakly-supervised Word2Vec ().} The standard Word2Vec~\cite{MSCCD13} scans the entire document using each word within a sample window as the target for prediction. However, if we know the global context, \ie the topic of the document, we can use that topic as our target. For instance, in Wikipedia, the entire article is related to the same topic. Therefore, we can sample our context windows from any location within the article rather than searching for context windows where the topic explicitly appears in the text. We consider this method as a weak form of supervision.

We achieve the best results in our experiments using our novel variant of the CBOW formulation. Here, we pre-train the first layer weights using standard Word2Vec on Wikipedia, and fine-tune the second layer weights using a negative-sampling objective~\cite{goldberg2014word2vec} only on the fine-grained text corpus. These weights correspond to the final output embedding. The negative sampling objective is formulated as follows:

where  and  are the label embeddings we seek to learn, and  is the average of word embeddings  within a context window around word . 
 consists of context  
and matching targets , and  consists of the same  and mismatching . 
To find the  (which are the columns of the first-layer network weights), we take them from a standard unsupervised Word2Vec model trained on Wikipedia.

During SGD, the  are fixed and we update each sampled  and  at each iteration.
Intuitively, we seek to maximize the similarity between context and target vectors for matching pairs, and minimize it for mismatching pairs.

\myparagraph{Bag-of-Words ().} BoW~\cite{H54} builds a ``bag'' of word frequencies by counting the occurrence of each vocabulary word that appears within a document. It does not preserve the order in which words appear in a document, so it disregards the grammar. We collect Wikipedia articles that correspond to each object class and build a vocabulary of most frequently occurring words. We then build histograms of these words to vectorize our classes. 


\subsection{Hierarchical Embeddings}
\label{subsec:hie}
Semantic similarity measures how closely related two word senses are according to their meaning. Such a similarity can be estimated by measuring the distance between terms in an ontology. WordNet\footnote{\texttt{http://wordnetweb.princeton.edu/}}, a large-scale hierarchical database of over 100,000 words for English, provides us a means of building our class hierarchy. 
To measure similarity, we use Jiang-Conrath~\cite{JC97} (), Lin~\cite{Lin98} () and path () similarities formulated in Table~\ref{tab:similairty}. We denote our whole family of hierarchical embeddings as . For a more detailed survey, the reader may refer to \cite{BHBKP05}.

\begin{table}[t]
 \begin{center}
   \small
{\renewcommand{\arraystretch}{1.5}
  \begin{tabular}{l}
	\hline
	\hline
	  \vspace{1mm}\\
	   \\
	 \vspace{1mm} \\ 
	\hline
	\hline	
  \end{tabular}
} 
 \end{center}
\caption{Notations~\cite{BHBKP05}: mscs (most specific common subsumer), pth (set of paths between two nodes), len (path length), IC (Information Content, defined as the log of the probability of finding a word in a text corpus independent of the hierarchy).} 
\cutcaptiondown
\label{tab:similairty}
\end{table}





 





\section{Experiments}
\label{sec:exp}

While our main contribution is a detailed analysis of output embeddings, good image representations are crucial to obtain good classification performance. 
In Sec.~\ref{subsec:setting} we detail datasets, input and output embeddings used in our experiments and in Sec.~\ref{subsec:results} we present our results.

\subsection{Experimental Setting}
\label{subsec:setting}

We evaluate \SJE on three datasets: Caltech UCSD Birds (CUB)~\cite{WBPB11} and Stanford Dogs (Dogs)\footnote{We use 113 classes that appear in the Federation Cynologique Internationale (FCI) database of dog breeds.}~\cite{StanfordDogsDataset} are fine-grained, and Animals With Attributes (AWA) \cite{LNH13} is a standard attribute dataset for zero-shot classification. 
CUB contains 11,788 images of 200 bird species, Dogs contains 19,501 images of 113 dog breeds and AWA contains 30,475 images of 50 different animals. 
We use a truly zero-shot setting where the train, val, and test sets belong to mutually exclusive classes. We employ train and val, \ie disjoint subsets of training set, for cross-validation. We report average per-class top-1 accuracy on the test set. For CUB, we use the same zero-shot split as \cite{APHS13} with 150 classes for the train+val set and 50 disjoint classes for the test set. AWA has a predefined split for 40 train+val and 10 test classes. For Dogs, we use approximately the same ratio of classes for train+val/test as CUB, \ie 85 classes for train+val and 28 classes for test. This is the first attempt to perform zero-shot learning on the Dogs dataset.

\myparagraph{Input Embeddings.} 
We use Fisher Vectors (FV) and Deep CNN Features (CNN). 
FV~\cite{PD07} aggregates per image statistics computed from local image patches into a fixed-length local image descriptor. 
We extract 128-dim SIFT from regular grids at multiple scales, reduce them to 64-dim using PCA, build a visual vocabulary with 256 Gaussians~\cite{VF08} and finally reduce the FVs to 4,096.
As an alternative, we extract features from a deep convolutional network.
Features that are typically obtained from the activations of the fully connected layers have been shown to induce semantic similarities.
We resize each image to 224224 and feed into the network which was pre-trained following the model architecture of either AlexNet~\cite{KSH12} or GoogLeNet~\cite{szegedy2014going,ioffe2015batch}.
For AlexNet (denoted as CNN) we use the 4,096-dim top-layer hidden unit activations (“fc7”) as features, and for GoogLeNet (denoted as GOOG) we use the 1,024-dim top-layer pooling units.
For both networks, we used the publicly-available BVLC implementations~\cite{jia2014caffe}.
We do not perform any task-specific pre-processing, such as cropping foreground objects or detecting parts.


\myparagraph{Output Embeddings.} 
AWA classes have 85 binary and continuous attributes. CUB classes have 312 continuous attributes and the continuous values are thresholded around the mean to obtain binary attributes. 
The Dogs dataset does not have human-annotated attributes available.


We train Word2Vec () and GloVe () on the English-language \texttt{Wikipedia} from 13.02.2014. 
We first pre-process it by replacing the class-names, \ie \emph{black-footed albatross}, with alternative unique names, \ie scientific name, \emph{phoebastrianigripes}.
We cross-validate the skip-window size and embedding dimensions.
For our proposed weakly-supervised Word2Vec (), we use the same embedding dimensions as the plain Word2Vec ().
For BoW, we download the \texttt{Wikipedia} articles that correspond to each class and build the vocabulary by omitting least- and most-frequently occurring words.
We cross-validate the vocabulary size.
 is a histogram of the vocabulary words as they appear in the respective document.


For hierarchical embeddings (), we use the WordNet hierarchy spanning our classes and their ancestors up to the root of the tree.
We employ the widely used NLTK library\footnote{\texttt{http://www.nltk.org/}} for building the hierarchy and measuring the similarity between nodes.
Therefore, each  vector is populated with similarity measures of the class to all other classes.

\myparagraph{Combination of output embeddings.}
We explore combinations of five types of output embeddings: supervised attributes , unsupervised Word2Vec , GloVe , BoW  and WordNet-derived similarity embeddings .
We either concatenate (\emph{cnc}) or combine (\emph{cmb}) different embeddings. In \emph{cnc}, for instance in AWA, 85-dim  and 400-dim  would be merged to 485-dim output embeddings. In this case, if we use 1,024-dim GOOG as input embeddings, we learn a single 1,024485-dim . In \emph{cmb}, we first learn 1,02485-dim  and 1,024400-dim  and then cross-validate the  coefficients to determine the amount each embedding contributes to the final score.  

\subsection{Experimental Results}
\label{subsec:results}

In this section, we evaluate several output embeddings on the CUB, AWA and Dogs datasets.

\begin{table}[t]
 \begin{center}
   \small
  \begin{tabular}{|c|r|c c |c c |c c|}
	\hline
	 & & \multicolumn{2}{c|}{\textbf{AWA}} & \multicolumn{2}{c|}{\textbf{CUB}} \\ 
	\hline
	 &  &  &  &  &  \\ 
	\hline
	\multirow{3}{*}{Ours} & FV (4K) & 36.6 & 42.3 & 15.2 & 19.0 \\ 
	& CNN (4K) & 45.9 & 61.9 & 30.0 & 40.3 \\ 
	& GOOG (1K) & 52.0 & \textbf{66.7} & 37.8 & \textbf{50.1} \\
	\hline
	SoA & ALE~\cite{APHS15} (64K)& 44.6 & 48.5 & 22.3 &  26.9 \\
	\hline
  \end{tabular}
 \end{center}
\caption{Discrete () and continuous () attributes with \SJE vs SoA. For AWA (CUB) \cite{APHS15} achieves 49.4\% (27.3\%) by combining  and binary hierarchies. 
}
\cutcaptiondown
\label{tab:att}
\end{table}

\myparagraph{Discrete vs Continuous Attributes.} 
Attribute representations are defined as a vector per class, or a column of the (class  attribute) matrix. These vectors (85-dim for AWA, 312-dim for CUB) can either model the presence/absence () or the confidence level () of each attribute.
We show that continuous attributes indeed encode more semantics than binary attributes by observing a substantial improvement with  over  with deep features (Tab.~\ref{tab:att}).
Overall, CNN outperforms FV, 
while GOOG gives the best performing results; therefore in the following, we comment only on our results obtained using GOOG.


On CUB, \ie a fine-grained dataset,  obtains 37.8\% accuracy, which is significantly above the SoA (26.9\%~\cite{APHS15}). 
Moreover,  achieves an impressive 50.1\% accuracy; outperforming the SoA by a large margin.
We observe the same trend for AWA, which is a benchmark dataset for zero-shot learning. 
On AWA,  obtains 52.0\% accuracy and  improves the accuracy substantially to 66.7\%, significantly outperforming the SoA (48.5\%~\cite{APHS15}).
To summarize, we have shown that  improves the performance of  using deep features, which indicates that with , the \SJE method learns a matrix  that better approximates the compatibility of images and side information than . 



\begin{table}[t]
 \begin{center}
   \small
  \begin{tabular}{|c|c|c|c|c|c|}
	\hline
	supervision & source &  & \textbf{AWA} & \textbf{CUB} & \textbf{Dogs} \\
	\hline
	
	\multirow{4}{*}{unsupervised} & text &  & 51.2 & \textbf{28.4} & 19.6\\
	 & text &  & \textbf{58.8} & 24.2 & 17.8\\
	 & text &  & 44.9 & 22.1 & \textbf{33.0} \\
	 & WordNet &  & 51.2 & 20.6 & 24.3\\

	\hline
	
	\multirow{2}{*}{supervised} & human &  & 52.0 & 37.8 & -\\
	& human &  & \textbf{66.7} & \textbf{50.1} & -\\
	\hline
  \end{tabular}
 \end{center}
\caption{Summary of zero-shot learning results with \SJE \wrt supervised and unsupervised output embeddings (Input embeddings: 1K-GOOG).}
\cutcaptiondown
\label{tab:summary}
\end{table}


\myparagraph{Learned Embeddings from Text.} 
As the visual similarity between objects in different classes increases, \eg in fine-grained datasets, the cost of collecting attributes also increases. Therefore, we aim to extract class similarities automatically from unlabeled online textual resources. 
We evaluate three methods, Word2Vec (), GloVe () and the historically most commonly-used method BoW (). We build  and  on the entire English Wikipedia dump. Note that the plain Word2Vec~\cite{MSCCD13} was used in~\cite{APHS15}; however, rather than using Word2Vec in an averaging mechanism, we pre-process the Wikipedia as described in Sec~\ref{subsec:text} so that our class names are directly present in the Word2Vec vocabulary. This leads to a significant accuracy improvement. For  we use a subset of Wikipedia populated only with articles that correspond to our classes. 
On CUB (Tab.~\ref{tab:summary}), the best accuracy is observed with  (28.4\%) improving the supervised SoA (26.9\%~\cite{APHS15}, Tab.~\ref{tab:att}).
This is promising and impressive since  does not use any human supervision.
On AWA (Tab.~\ref{tab:summary}), the best accuracy is observed with  (58.8\%) followed by  (51.2\%), improving the supervised SoA (48.5\%~\cite{APHS15}) significantly.
On Dogs (Tab.~\ref{tab:summary}), the best accuracy is obtained with  (33.0\%). On the other hand, using  (19.6\%) and  (17.8\%) leads to significantly lower accuracies. 
Unlike birds, different dog breeds belong to the same species and thus they share a common scientific name. As a result, our method of cleanly pre-processing Wikipedia by replacing the occurrences of bird names with a unique scientific name was not  possible for Dogs. This may lead to vectors obtained from Wikipedia for dogs that are vulnerable to variation in nomenclature.
In summary, our results indicate no winner among ,  and . These embeddings may be task specific and complement each other. We investigate the complementarity of embeddings in the following sections.


\begin{table}[t]
 \begin{center}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{|r|c c c |c c c | c|}
	\hline
	& \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} &  (W) + \\

	& B & W & B+W & B & W & B+W &  (B) \\
	\hline
	FV & 10.5 & 13.3 & 13.2  & 16.0 & 16.0 & 16.5 & 17.1 \\
	CNN & 13.4 & 20.6 & 20.6 &  20.0 & 24.1 &  21.4 & 25.1 \\
	GOOG & 13.7 & 24.2 & \textbf{26.1} & 22.5 & \textbf{28.4} & 27.5 & \textbf{29.7}\\
	\hline
  \end{tabular}
}
 \end{center}
\caption{Comparison of Word2Vec () and GloVe () learned from a bird specific corpus (B), Wikipedia (W) and their combination (B + W), evaluated on CUB (Input embeddings: 4K-FV, 4K-CNN and 1K-GOOG).}
\cutcaptiondown
\label{tab:text_corpora}
\end{table}


\myparagraph{Effect of Text Corpus.} 
For  and , we analyze the effects of three text corpora (B, W, B+W) with varying size and specificity.
We build our specialized bird corpus (B) by collecting bird-related information from various online resources, \ie \texttt{audubon.org}, \texttt{birdweb.org}, \texttt{allaboutbirds.org} and BNA\footnote{\texttt{http://bna.birds.cornell.edu/bna/}}. In combination, this corresponds to 50MB of bird-related text.
We use the English-language \texttt{Wikipedia} from 13.02.2014 as our large and general corpus (W) which is 40GB of text.
Finally, we combine B and W to build a large-scale text corpus enriched with bird specific text (B+W).
On W and B+W, a small window size (10 for  and 20 for ); on B, a large window size (35 for  and 50 for ) is required. We choose parameters after a grid search. 
Increased specificity of the text corpus implies semantic consistency throughout the text. Therefore, large context windows capture semantics well in our bird specific (B) corpus. On the other hand, W is organized alphabetically \wrt the document title; hence, a large sampling window can include content from another article that is adjacent to the target word alphabetically. Here, small windows capture semantics better by looking at the text locally. We report our results in Tab.~\ref{tab:text_corpora}.

Using , B+W (26.1\%) gives the highest accuracy, followed by W (24.2\%).
One possible reason is that when the semantic similarity is modeled with cooccurrence statistics, output embeddings become more informative with the increasing corpus size, since the probability of cooccurrence of similar concepts increases. 


Using , the accuracy obtained with B (22.5\%) is already higher than the  -based SoA (22.3\%), illustrating the benefit of using fine-grained text for fine-grained tasks. Another advantage of using B is that, since it is short, building  is efficient. 
Moreover, building  with B does not require any annotation effort. Building  using W (28.4\%) gives the highest accuracy, followed by W + B (27.5\%) which improves the supervised SoA (26.9\%). 
We speculate that since Word2Vec is a variant of the Feedforward Neural Network Language Model (FNNLM)~\cite{BDVJ03}, a deep architecture, it may learn more from negative data than positives.
This was also observed for CNN features learned with a large number of unlabeled surrogate classes \cite{DSRB14}. 

Additionally, we propose a weakly-supervised alternative to Word2Vec framework (, Sec.~\ref{subsec:text}). The weak-supervision comes from using the specialized B corpus to fine-tune the weights of the network and model the bird-related information.
With  alone, we obtain 21.0\% accuracy.
However, when it is combined with  (28.4\%), the accuracy improves to 29.7\%. Compared to the results in Tab.~\ref{tab:text_corpora}, 29.7\% is the highest accuracy obtained using unsupervised embeddings. We regard these results as a very encouraging evidence that Word2Vec representations can indeed be made more discriminative for fine-grained zero-shot learning by integrating a fine-grained text corpus directly to the output embedding learning problem.


\begin{figure}[t]
   \centering
   \includegraphics[width=\linewidth,trim=40 400 40 80]{Hie_Emb}
\caption{Comparison of WordNet similarity measures: ,  and . We use  as a general name for hierarchical output embedding. (Input embedding: 1K-GOOG).} 
\cutcaptiondown
\label{fig:hie}
\end{figure}


\myparagraph{Hierarchical Embeddings.}  
The hierarchical organization of concepts typically embodies a fair amount of hidden information about language, such as synonymy, semantic relations, \etc. Therefore, semantic relatedness defined by hierarchical distance between classes can form numerical vectors to be used as output embeddings for zero-shot learning. 
We build ontological relationships between our classes using the WordNet~\cite{WordNet} taxonomy. Due to its large size, WordNet encapsulates all of our AWA and Dog classes.
For CUB, the high level bird species, \ie albatross, appear as synsets in WordNet, but the specific bird names, \ie black-footed albatross, are not always present.
Therefore we take the hierarchy up to high level bird species as-is and we assume the specific bird classes are all at the bottom of the hierarchy located with the same distance to their immediate ancestors. The WordNet hierarchy contains 319 nodes for CUB (200 classes), 104 nodes for AWA (50 classes) and 163 nodes for Dogs (113 classes).
We measure the distance between classes using the similarity measures from Sec~\ref{subsec:attributes}. 

While as shown in Fig.~\ref{fig:hie} different hierarchical similarity measures have very different behaviors on each dataset. 
The best performing  obtains 51.2\% (Tab.~\ref{tab:summary}) accuracy on AWA which reaches our  (52.0\%) and improves  (44.9\%) significantly. On CUB,  obtains 20.6\% (Tab.~\ref{tab:summary}) which remain below our  (37.8\%) and approaches  (22.1\%). On the other hand, on Dogs  obtains 24.3\% (Tab.~\ref{tab:summary}) which is significantly higher than the unsupervised text embeddings  (19.6\%) and  (17.8\%).


\newcommand{\vv}{\checkmark}

\begin{table}[t]
 \begin{center}
\resizebox{\linewidth}{!}{
  \begin{tabular}{|m{0.2cm} m{0.2cm} m{0.2cm} m{0.2cm} m{0.2cm} | c c | c c | c c |}
	\hline
	& & & & & \multicolumn{2}{c|}{\textbf{AWA}} & \multicolumn{2}{c|}{\textbf{CUB}} & \multicolumn{2}{c|}{\textbf{Dogs}} \\
	\hline
	 &  &  &  &  & \emph{cnc} & \emph{cmb} & \emph{cnc} & \emph{cmb} & \emph{cnc} & \emph{cmb}  \\
	\hline
	  &  &       &       &  & 53.9 & 55.5 & 28.2 & 29.4 & 23.5 &  26.6 \\ 
	  &       &  &       &  & \textbf{60.1} & 59.5 & 28.5 & \textbf{29.9} & 23.5 & 26.7 \\ 
	  &       &       &  &  & 49.4 & 49.2 & 26.4 & 27.7 & \textbf{35.1}  &  28.2 \\ 
	\hline

	 &  &       &       &  & 71.3 & 73.5 & 45.1 & 51.0 & - & -  \\
	 &       &  &       &  & 73.3 & \textbf{73.9} & 42.2 & \textbf{51.7} & - & - \\
	 &       &       &  &  & 69.4 & 71.1 & 40.9 & 51.5 & - & - \\
	\hline
  \end{tabular}
}
\end{center}
\caption{Attribute ensemble results for all datasets. : lin for CUB, path for AWA and Dogs. Top part shows combination results of unsupervised embeddings and bottom part integrates supervised embeddings to the rest (Input embeddings: 1K-GOOG).}
\cutcaptiondown
\label{tab:ensemble_all}
\end{table}
\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth, , trim=30 50 30 30]{cont_glove_comb_goog}
\end{center}
\caption{Highest ranked 5 images for \emph{chimpanzee}, \emph{leopard} and \emph{seal} (AWA) using ,  and . For \emph{chimpanzee},  ranks chimpanzees on trees at the top, whereas  models the social nature of the animal ranking a group of chimpanzees highest,  synthesizes both aspects. For \emph{leopard}  puts an emphasis on the head,  seems to place the animal in the wild. In case of \emph{seal},  retrieves images related to \emph{water}, whereas  adds more context by placing seals in the icy natural environment and  combines both.}
\cutcaptiondown

\label{fig:toprankedimg}
\end{figure*}


\myparagraph{Combining Output Embeddings.}
In this section, we combine output embeddings obtained through human annotation (), from text () and from hierarchies ().\footnote{We empirically found that the hierarchical embeddings  consistently improved performance when combined or concatenated with other embeddings. Therefore, we report results using  by default.}
As a reference, Tab.~\ref{tab:summary} summarizes the results obtained using one output embedding at a time. Our intuition is that because the different embeddings attempt to encapsulate different information, accuracy should improve when multiple embeddings are combined. 
We can observe this complementarity either by simple concatenation (\emph{cnc}) or systematically combining (\emph{cmb}) output embeddings (Sec.\ref{subsec:comb_emb}) also known as early/late fusion~\cite{APHS15}. 
For \emph{cnc}, we perform full SJE training and cross-validation on the concatenated output embeddings.
For \emph{cmb}, we learn joint embeddings  for each output separately (which is trivially parallelized), and find ensemble weights  via cross-validation.
In contrast to the \emph{cnc} method, no additional joint training is used, although it can improve performance in practice. 
We observe (Tab.~\ref{tab:ensemble_all}) in almost all cases \emph{cmb} outperforms \emph{cnc}.


We analyze the combination of unsupervised embeddings (). On AWA,  (58.8\%, Tab.~\ref{tab:summary}) combined with  (51.2\%, Tab.~\ref{tab:summary}), 
we achieve 60.1\% (Tab.~\ref{tab:ensemble_all}) which improves the SoA (48.5\%, Tab.~\ref{tab:att}) by a large margin.
On CUB, combining  (24.2\%, Tab.~\ref{tab:summary}) with  (20.6\%, Tab.~\ref{tab:summary}), 
we get 29.9\% (Tab.~\ref{tab:ensemble_all}) 
and improve the supervised-SoA  (26.9\%, Tab.~\ref{tab:att}).
Supporting our initial claim, unsupervised output embeddings obtained from different sources, \ie text vs hierarchy, seem to be complementary to each other.
In some cases, \textit{cmb} performs worse than \textit{cnc}; \eg 28.2\% versus 35.1\% when using  with  on Dogs.
In most other cases \textit{cmb} performs equivalent or better.
Combining supervised () and unsupervised embeddings () shows a similar trend. On AWA, combining  (66.7\%, Tab.~\ref{tab:summary}) with  and  leads to 73.9\% (Tab.~\ref{tab:ensemble_all}) which significantly exceeds the SoA (48.5\%, Tab.~\ref{tab:att}). On CUB, combining  with  and   leads to 51.7\% (Tab.~\ref{tab:ensemble_all}),
improving both the results we obtained with  (50.1\%, Tab.~\ref{tab:summary}) and the supervised-SoA (26.9\%, Tab.~\ref{tab:att}). We have shown with these experiments that output embeddings obtained through human annotation can also be complemented with unsupervised output embeddings using the \SJE framework.


\begin{table}[t]
 \begin{center}
   \small
  \begin{tabular}{c|c|c|c|c}
	\hline
	supervision & method &\textbf{AWA} & \textbf{CUB} & \textbf{Dogs} \\
	\hline
	unsupervised & \SJE (best from Tab.~\ref{tab:ensemble_all}) & 60.1 & 29.9 & \textbf{35.1}\\
	\hline
	\multirow{2}{*}{supervised} & \SJE (best from Tab.~\ref{tab:ensemble_all}) & \textbf{73.9} & \textbf{51.7} & -- \\
	 & AHLE~\cite{APHS15} & 49.4 & 27.3 & -- \\
	\hline
  \end{tabular}
 \end{center}
\caption{Summary of best zero-shot learning results with \SJE with or without supervision along with SoA.} \vspace{-3mm} 
\cutcaptiondown
\label{tab:summary_all}
\end{table}


\myparagraph{Qualitative Results.} Fig.~\ref{fig:toprankedimg} shows top-5 highest ranked images for classes \emph{chimpanzee}, \emph{leopard} and \emph{seal} that are selected from 10 test classes of AWA. We use GOOG as input embeddings and as output embeddings we use supervised , the best performing unsupervised embedding on AWA (), and the combination of the two (). For the class \emph{chimpanzee},  emphasizes that chimpanzees live on trees, which is among the list of attributes. On the other hand,  models the social nature of the animal, ranking a group of chimpanzees interacting with each other at the highest. Indeed this information can easily be retrieved from Wikipedia.  synthesizes both aspects. Similarly, for \emph{leopard}  puts an emphasis on the head where we can observe several of the attributes, \ie color, spotted, whereas  seems to place the animal in the wild.  combines both aspects. In case of class \emph{seal},  retrieves images related to \emph{water} and ranks whales and seals highest, whereas  adds more context by placing seals in the icy natural environment and within groups. Finally,  ranks seal-shaped animals on ice, close to water and within groups the highest. We find these qualitative results interesting as they depict how (1) unsupervised embeddings capture nameable semantics about objects and (2) different output embeddings are semantically complementary for zero-shot learning.  

\section{Conclusion}
\label{sec:conclusion}
We evaluated the \SJElong (SJE) framework on supervised attributes and unsupervised output embeddings obtained from hierarchies and unlabeled text corpora. We proposed a novel weakly-supervised label embedding technique. 
By combining multiple output embeddings (\emph {cmb}), we established a new SoA on AWA (73.9\%, Tab.~\ref{tab:summary_all}) and CUB (51.7\%, Tab.~\ref{tab:summary_all}).
Moreover, we showed that unsupervised zero-shot learning with \SJE improves the SoA, to 60.1\% on AWA and 29.9\% on CUB, and obtains 35.1\% on Dogs (Tab.~\ref{tab:summary_all}). 

We emphasize the following take-home points: (1) Unsupervised label embeddings learned from text corpora yield compelling zero-shot results, outperforming previous supervised SoA on AWA and CUB (Tab.~\ref{tab:att} and~\ref{tab:summary}).
(2) Integrating specialized text corpora helps due to incorporating more fine-grained information to output embeddings (Tab.~\ref{tab:text_corpora}).
(3) Combining unsupervised output embeddings improve the zero-shot performance, suggesting that they provide complementary information (Tab.~\ref{tab:ensemble_all}).
(4) There is still a large gap between the performance of unsupervised output embeddings and human-annotated attributes on AWA and CUB, suggesting that better methods are needed for learning discriminative output embeddings from text. (5) Finally, supporting~\cite{APHS15, RSSS12}, encoding continuous nature of attributes significantly improve upon binary attributes for zero-shot classification (Tab.~\ref{tab:att}).


As future work, we plan to investigate other methods to combine multiple output embeddings and to improve the discriminative power of unsupervised and weakly-supervised label embeddings for fine-grained classification.

\subsection*{Acknowledgments}
\vspace*{-0.05in}
This work was supported in part by ONR N00014-13-1-0762, NSF CMMI-1266184, Google Faculty Research Award, and NSF Graduate Fellowship.


{\small
\bibliographystyle{ieee}
\bibliography{myrefs}
}

\end{document}
