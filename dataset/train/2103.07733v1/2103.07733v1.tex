

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{color}
\usepackage[numbers, sort]{natbib}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\red#1{\textbf{\textcolor[RGB]{244, 67, 54}{#1}}}
\def\blue#1{\textbf{\textcolor[RGB]{60, 120, 216}{#1}}}
\def\redl#1{\textcolor[RGB]{244, 67, 54}{#1}}
\def\bluel#1{\textcolor[RGB]{60, 120, 216}{#1}}

\def\cvprPaperID{2862} \def\confYear{CVPR 2021}



\begin{document}

\title{ReDet: A Rotation-equivariant Detector for Aerial Object Detection}

\author{Jiaming Han, 
Jian Ding, 
Nan Xue, 
Gui-Song Xia
\ \label{eq:equivariance}
   \Phi[T_g^X(x)]=T_g^Y[\Phi(x)] \quad \forall(x, g) \in(X, G),
\label{eq:trans_conv}
      \left[\left[T_t f\right] * \psi\right](x) = \left[T_t \left[f * \psi \right] \right](x),
\label{eq:rot_conv}
   \left[\left[T_g f\right] * \psi\right](g) = \left[T_g \left[f * \psi \right] \right](g).
 \label{eq:rot_eq_layer}
   L_i[T_r(g)] = T_r[L_i(g)]\quad g\in G.
 \label{eq:rot_eq_layers}
   [\prod_{i=1}^M L_i](T_rI) = T_r[\prod_{i=1}^M L_i](I).
 \label{eq:rot_eq_region}
   \Phi(T_rI_R) = T_r\Phi(I_R).
 \label{eq:rot_eq_region_inv}
   \Phi(I_R) = T_r^{'}\Phi(T_rI_R).
\label{eq:ori_align}
\hat f_R= Int(SC(f_R,r),\theta),\ r=\lfloor \theta N/2\pi \rfloor,
\label{eq:feature_int}
   \hat f_R^{(i)}= (1-\alpha) f_R^{(i)} + \alpha f_R^{(i+1)},

where  indicates the distance factor for 1D-interpolation. Note that we use the \texttt{mod} function to ensure  (as well as ).

{\bf Comparison with RRoI Align+MaxPool.} Different from RiRoI Align, warping RoI features with RRoI Align and then maxpooling over the orientation dimension (\emph{i.e.}, orientation pooling) is another approach to extract rotation-invariant features.
The orientation pooling operation is usually adopted in classification tasks~\cite{cohen2016gcnn,zhou2017orn,weiler2018learning}. For each location in the feature map, it only preserves the orientation with the strongest response, while features from other orientations are abandoned.
However, we argue that the response from all orientations, no matter strong or weak, is indispensable for object recognition. In our RiRoI Align, features from all orientations are preserved and aligned with the orientation alignment operation.
We will conduct experiments to show the advantage of our RiRoI Align in Sec.~\ref{sec:exp}.


\section{Experiments and Analysis}
\label{sec:exp}

\subsection{Datasets}
{\bf DOTA~\cite{xia2018dota}} is the largest dataset for oriented object detection in aerial images with two released versions: {\bf DOTA-v1.0} and {\bf DOTA-v1.5}.
{\bf DOTA-v1.0} contains 2806 large aerial images with the size ranges from  to  and 188, 282 instances among 15 common categories: Plane (PL), Baseball diamond (BD), Bridge (BR), Ground track field (GTF), Small vehicle (SV), Large vehicle (LV), Ship (SH), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball field (SBF), Roundabout (RA), Harbor (HA), Swimming pool (SP), and Helicopter (HC).
{\bf DOTA-v1.5} is released for DOAI Challenge 2019\footnote[3]{\url{https://captain-whu.github.io/DOAI2019}} with a new category, Container Crane (CC) and more extremely small instances (less than 10 pixels). DOTA-v1.5 contains 402, 089 instances.
Compared with DOTA-v1.0, DOTA-v1.5 is more challenging but stable during training.

Following the settings in previous methods~\cite{ding2018transformer,han2020align}, we use both training and validation sets for training and the test set for testing. We crop the original images into  patches with a stride of 824. Random horizontal flipping is adopted to avoid over-fitting during training, and no other tricks are utilized.
For fair comparisons with other methods, we prepare multi-scale data at three scales \{0.5, 1.0, 1.5\}, and random rotation for training and testing.

{\bf HRSC2016~\cite{liu2017hrsc2016}} is a challenging ship detection dataset with OBB annotations, which contains 1061 aerial images with the size ranges from  to . It includes 436, 181 and 444 images in the training, validation and test set, respectively. We use both training and validation sets for training and the test set for testing. All images are resized to (800, 512) without changing the aspect ratio. Random horizontal flipping is applied during training.

\subsection{Implementation Details}
{\bf ImageNet pretrain.} For the original ResNet~\cite{he2016resnet}, we directly use the ImageNet pretrained models from Pytorch~\cite{paszke2019pytorch}. 
For ReResNet, we implement it based on the \texttt{mmclassification}\footnote[4]{\url{https://github.com/open-mmlab/mmclassification}}. We train ReResNet on the ImageNet-1K with an initial learning rate of 0.1. All models are trained for 100 epochs and the learning rate is divided by 10 at \{30, 60, 90\} epochs. The batch size is set to 256. 

{\bf Fine-tuning on detection.} We adopt ResNet~\cite{he2016resnet} with FPN~\cite{lin2017feature} as the backbone of the baseline method. ReResNet with ReFPN is adopted as the backbone of our proposed ReDet.
For RPN, we set 15 anchors per location of each pyramid level. For R-CNN, we sample 512 RoIs with a 1:3 positive to negative ratio for training. For testing, we adopt 10000 RoIs (2000 for each pyramid level) before NMS and 2000 RoIs after NMS.
We adopt the same training schedules as \texttt{mmdetection}~\cite{chen2019mmdetection}. SGD optimizer is adopted with an initial learning rate of 0.01, and the learning rate is divided by 10 at each decay step. The momentum and weight decay are 0.9 and 0.0001, respectively. 
We train all models in 12 epochs for DOTA and 36 epochs for HRSC2016. We use 4 V100 GPUs with a total batch size of 8 for training and a single V100 GPU for inference.


\begin{table}
   \begin{center}
\small
\resizebox{\linewidth}{!}{\begin{tabular}{c|c|c|c|c} \hline
        backbone    & group  & cls. (\%)      & det. (\%)  & size (Mb)   \\ \hline
R50-FPN     & -      & \textbf{76.55} & 65.03                & 103                 \\
        ReR50-ReFPN &   & 72.81          & 65.43                & 24                 \\
        ReR50-ReFPN &   & 71.20          & \textbf{66.86}       & 12                 \\
        ReR50-ReFPN && 61.60          & 64.36                & \textbf{6}        \\ \hline
    \end{tabular}
}
\end{center}    \vspace{-3mm}
   \caption{\textbf{Performance comparisons of the rotation-equivariant backbone on classification (cls.) and detection (det.)}. group indicates the rotation group that the backbone is equivariant to. We report the top-1 accuracy on ILSVRC 2012 without FPN and the detection performance on DOTA-v1.5 test set in terms of mAP. The model size only includes the size of the backbone.}
   \vspace{-1mm}
   \label{tab:re_backbone}
\end{table}

\begin{table}
      \begin{center}
    \small
\begin{tabular}{c|c|c|c} \hline
            method                             & backbone    & mAP (\%)        & size (Mb)   \\ \hline
            \multirow{2}{*}{FR-O}  & R50-FPN     & 62.00           & 158         \\
                                               & ReR50-ReFPN & \textbf{62.36}  & \textbf{68} \\ \hline
            \multirow{2}{*}{RetinaNet-O}     & R50-FPN     & 58.74           & 140         \\
                                               & ReR50-ReFPN & \textbf{59.64}  & \textbf{34} \\ \hline


        \end{tabular}
\end{center}       \vspace{-3mm}
      \caption{\textbf{The performance of rotation-equivariant backbone on other detectors}. Faster R-CNN OBB (FR-O) and RetinaNet OBB (RetinaNet-O) are our re-implemented version for OBBs.}
      \vspace{-1mm}
      \label{tab:other_method}
\end{table}

\begin{table}
      \begin{center}
    \small
\begin{tabular}{c|c|c} \hline
            method            & \#interpolate  & mAP (\%)               \\ \hline
            RRoI Align        & -              & 65.99                  \\
       RRoI Align+\emph{MP.}  & -              & 64.60 (-1.39)          \\
            RiRoI Align       & 1              & 66.44 (+0.45)          \\
            RiRoI Align       & 2              & \textbf{66.86 (+0.87)} \\
            RiRoI Align       & 4              & 66.32 (+0.33)          \\ \hline
        \end{tabular}
\end{center}       \vspace{-3mm}
      \caption{\textbf{Comparisons of our RiRoI Align with RRoI Align}. \#interpolate indicates the number of orientation channels used for interpolation (same as  in Sec.~\ref{sec:riroi_align}). For an RRoI with the orientation , we use its nearest \{1, 2, 4\} orientation channels to interpolate its features. \emph{MP.} is short for MaxPool. ReR50+ReFPN is adopted as the backbone.}
      \vspace{-1mm}
      \label{tab:riroi_align}
\end{table}

\begin{table}
   \begin{center}
    \small
\begin{tabular}{c|c|c|c|c} \hline
            method       & rot.     & schd.    & mAP (\%)       & training (h)\\ \hline
ReDet        &  & 1x       & 62.62          & \textbf{8} \\
            baseline     &\checkmark& 1x       & 64.07          & 11         \\
            ReDet    &  & 1x       & 66.66          & 13         \\
            baseline     &\checkmark& 2x       & \textbf{67.34} & 22         \\  \hline
        \end{tabular}
\end{center}    \vspace{-3mm}
   \caption{\textbf{Comparison with rotation augmentation}. We compare the performance of the baseline method with rotation (rot.) augmentation and ReDet without rotation augmentation. ReDet preserves a similar amount of parameters with the baseline. We report the mAP with R18 (for baseline) and ReR18 (for ReDet) backbone under the cyclic group . For fair comparison, we randomly select rotation angles from .}
  \vspace{-1mm}
   \label{tab:rot_comp}
\end{table}

\begin{table}
   \begin{center}
    \resizebox{\linewidth}{!}{\begin{tabular}{c|c|c|c|c|c|c} \hline
        \multirow{2}{*}{method} & \multicolumn{3}{c|}{DOTA-v1.0}             & \multicolumn{3}{c}{HRSC2016}                   \\ \cline{2-7} 
                                & AP50     & AP75     & mAP                  & AP50     & AP75     & mAP                       \\ \hline
        baseline                & 75.62    & 48.37    & 46.13                & 90.18    & 80.48    & 68.17                     \\ 
        ReDet                   & 76.25    & 50.86    & \textbf{47.11(+0.98)}& 90.46    & 89.46    & \textbf{70.41(+2.24)}    \\ \hline
    \end{tabular}}
    \end{center}

%
    \vspace{-3mm}
   \caption{\textbf{Performance of the proposed ReDet on other datasets}. We report the performance on DOTA-v1.0 and HRSC2016 in COCO style. We use ReR50+ReFPN (\emph{resp.} R50+FPN) as the backbone of ReDet (\emph{resp.} baseline).}
  \vspace{-3mm}
   \label{tab:other_dataset}
\end{table}

\begin{table*}
   \begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|ccccccccccccccc|c}
\hline
method                                     & backbone    & PL           & BD           & BR           & GTF          & SV           & LV           & SH           & TC           & BC           & ST           & SBF          & RA           & HA           & SP           & HC           & mAP          \\ \hline 
\textbf{single-scale:}&&&&&&&&&&&&&&&& \\
FR-O~\cite{xia2018dota}                    & R101        & 79.42        & 77.13        & 17.70        & 64.05        & 35.30        & 38.02        & 37.16        & 89.41        & 69.64        & 59.28        & 50.30        & 52.91        & 47.89        & 47.40        & 46.30        & 54.13        \\
ICN~\cite{azimi2018towards}                & R101-FPN    & 81.36        & 74.30        & 47.70        & 70.32        & 64.89        & 67.82        & 69.98        & 90.76        & 79.06        & 78.20        & 53.64        & 62.90        & 67.02        & 64.17        & 50.23        & 68.16        \\
CADNet~\cite{zhang2019cad}                 & R101-FPN    & 87.80        & 82.40        & 49.40        & \blue{73.50} & 71.10        & 63.50        & 76.60        & \red{90.90}  & 79.20        & 73.30        & 48.40        & 60.90        & 62.00        & 67.00        & 62.20        & 69.90        \\
DRN~\cite{pan2020dynamic}                  & H-104       & 88.91        & 80.22        & 43.52        & 63.35        & 73.48        & 70.69        & 84.94        & 90.14        & 83.85        & 84.11        & 50.12        & 58.41        & 67.62        & 68.60        & 52.50        & 70.70        \\
CenterMap~\cite{wang2020centermap}         & R50-FPN     & 88.88        & 81.24        & \blue{53.15} & 60.65        & \red{78.62}  & 66.55        & 78.10        & 88.83        & 77.80        & 83.61        & 49.36        & \blue{66.19} & \blue{72.10} & \red{72.36}  & 58.70        & 71.74        \\
SCRDet~\cite{yang2019scrdet}               & R101-FPN    & \red{89.98}  & 80.65        & 52.09        & 68.36        & 68.36        & 60.32        & 72.41        & 90.85        & \red{87.94}  & \red{86.86}  & \red{65.02}  & \red{66.68}  & 66.25        & 68.24        & \blue{65.21} & 72.61        \\
RDet~\cite{yang2019r3det}              & R152-FPN    & \blue{89.49} & 81.17        & 50.53        & 66.10        & 70.92        & \blue{78.66} & 78.21        & 90.81        & 85.26        & 84.23        & \blue{61.81} & 63.77        & 68.16        & \blue{69.83} & \red{67.17}  & 73.74        \\
SA-Net~\cite{han2020align}             & R50-FPN     & 89.11        & \red{82.84}  & 48.37        & 71.11        & 78.11        & 78.39        & \blue{87.25} & 90.83        & 84.90        & 85.64        & 60.36        & 62.60        & 65.26        & 69.13        & 57.94        & \blue{74.12} \\
ReDet (Ours)                               & ReR50-ReFPN & 88.79        & \blue{82.64} & \red{53.97}  & \red{74.00}  & \blue{78.13} & \red{84.06}  & \red{88.04}  & \blue{90.89} & \blue{87.78} & \blue{85.75} & 61.76        & 60.39        & \red{75.96}  & 68.07        & 63.59        & \red{76.25}  \\ \hline
\textbf{multi-scale:}&&&&&&&&&&&&&&&& \\
RoI Trans.~\cite{ding2018transformer}  & R101-FPN    & 88.64        & 78.52        & 43.44        & 75.92        & 68.81        & 73.68        & 83.59        & 90.74        & 77.27        & 81.46        & 58.39        & 53.54        & 62.83        & 58.93        & 47.67        & 69.56        \\
O-DNet~\cite{wei2020oriented}      & H104        & 89.30        & 83.30        & 50.10        & 72.10        & 71.10        & 75.60        & 78.70        & \red{90.90}  & 79.90        & 82.90        & 60.20        & 60.00        & 64.60        & 68.90        & 65.70        & 72.80        \\
DRN~\cite{pan2020dynamic}              & H104        & 89.71        & 82.34        & 47.22        & 64.10        & 76.22        & 74.43        & 85.84        & 90.57        & 86.18        & 84.89        & 57.65        & 61.93        & 69.30        & 69.63        & 58.48        & 73.23        \\
Gliding Vertex~\cite{xu2019gliding}    & R101-FPN    & 89.64        & 85.00        & 52.26        & 77.34        & 73.01        & 73.14        & 86.82        & 90.74        & 79.02        & 86.81        & 59.55        & \red{70.91}  & 72.94        & 70.86        & 57.32        & 75.02        \\
BBAVectors~\cite{yi2020bbavector}      & R101        & 88.63        & 84.06        & 52.13        & 69.56        & 78.26        & 80.40        & 88.06        & 90.87        & \blue{87.23} & 86.39        & 56.11        & 65.62        & 67.10        & 72.08        & 63.96        & 75.36        \\
CenterMap~\cite{wang2020centermap}     & R101-FPN    & \blue{89.83} & 84.41        & 54.60        & 70.25        & 77.66        & 78.32        & 87.19        & 90.66        & 84.89        & 85.27        & 56.46        & \blue{69.23} & 74.13        & 71.56        & 66.06        & 76.03        \\
CSL~\cite{yang2020arbitrary}           & R152-FPN    & \red{90.25}  & \red{85.53}  & 54.64        & 75.31        & 70.44        & 73.51        & 77.62        & 90.84        & 86.15        & 86.69        & 69.60        & 68.04        & 73.83        & 71.10        & 68.93        & 76.17        \\
SCRDet++~\cite{yang2020scrdet++}       & R152-FPN    & 88.68        & \blue{85.22} & 54.70        & 73.71        & 71.92        & \blue{84.14} & 79.39        & 90.82        & 87.04        & 86.02        & 67.90        & 60.86        & 74.52        & 70.76        & \blue{72.66} & 76.56        \\
SA-Net~\cite{han2020align}         & R50-FPN     & 88.89        & 83.60        & \blue{57.74} & \red{81.95}  & \red{79.94}  & 83.19        & \red{89.11}  & 90.78        & 84.87        & \red{87.81}  & \red{70.30}  & 68.25        & \blue{78.30} & \blue{77.01} & 69.58        & \blue{79.42} \\
ReDet (Ours)                           & ReR50-ReFPN & 88.81        & 82.48        & \red{60.83}  & \blue{80.82} & \blue{78.34} & \red{86.06}  & \blue{88.31} & \blue{90.87} & \red{88.77}  & \blue{87.03} & \blue{68.65} & 66.90        & \red{79.26}  & \red{79.71}  & \red{74.67}  & \red{80.10}  \\ \hline 
\end{tabular}}
\end{center}    \vspace*{-3mm}
   \caption{\textbf{Comparisons with state-of-the-art methods on DOTA-v1.0 OBB Task}. H-104 means Hourglass 104.  indicates multi-scale training and testing. The results with \redl{red} and \bluel{blue} colors indicate the best and second-best results of each column, respectively.}
  \vspace*{-1mm}
   \label{tab:dota_sota}
\end{table*}

\begin{table*}
   \begin{center}
    \resizebox{\textwidth}{!}{\begin{tabular}{l|cccccccccccccccc|c} \hline
    method          &PL& BD & BR & GTF & SV & LV & SH & TC & BC & ST & SBF & RA & HA & SP & HC & CC & mAP \\ \hline
    \textbf{OBB results:}&&&&&&&&&&&&&&&&& \\
    RetinaNet-O~\cite{lin2017focal}      &71.43&77.64&42.12&64.65&44.53&56.79&73.31&90.84&76.02&59.96&46.95&69.24&59.65&64.52&48.06&0.83&59.16 \\
    FR-O~\cite{ren2017faster}            &71.89&74.47&44.45&59.87&51.28&68.98&79.37&90.78&77.38&67.50&47.75&69.72&61.22&65.28&60.47&1.54&62.00 \\
    Mask R-CNN~\cite{he2017maskrcnn}     &76.84&73.51&49.90&57.80&51.31&71.34&79.75&90.46&74.21&66.07&46.21&70.61&63.07&64.46&57.81&9.42&62.67\\
    HTC~\cite{chen2019hybrid}            &77.80&73.67&51.40&63.99&51.54&73.31&80.31&90.48&75.12&67.34&48.51&70.63&64.84&64.48&55.87&5.15&63.40\\
    OWSR~\cite{li2019oswr}&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&74.90\\
    ReDet (Ours)                        &79.20&82.81&51.92&71.41&52.38&75.73&80.92&90.83&75.81&68.64&49.29&72.03&73.36&70.55&63.33&11.53&66.86 \\
    ReDet (Ours)                    &88.51&86.45&61.23&81.20&67.60&83.65&90.00&90.86&84.30&75.33&71.49&72.06&78.32&74.73&76.10&46.98&\textbf{76.80} \\\hline
    \textbf{HBB results:}&&&&&&&&&&&&&&&&& \\
    RetinaNet-O~\cite{lin2017focal}     &71.66&77.22&48.71&65.16&49.48&69.64&79.21&90.84&77.21&61.03&47.30&68.69&67.22&74.48&46.16&5.78&62.49 \\
    FR-O~\cite{ren2017faster}           &71.91&71.60&50.58&61.95&51.99&71.05&80.16&90.78&77.16&67.66&47.93&69.35&69.51&74.40&60.33&5.17&63.85 \\
    HTC~\cite{chen2019hybrid}           &78.41&74.41&53.41&63.17&52.45&63.56&79.89&90.34&75.17&67.64&48.44&69.94&72.13&74.02&56.42&12.14&64.47\\
    Mask R-CNN~\cite{he2017maskrcnn}    &78.36&77.41&53.36&56.94&52.17&63.60&79.74&90.31&74.28&66.41&45.49&71.32&70.77&73.87&61.49&17.11&64.54\\
    OWSR~\cite{li2019oswr}&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&-&77.90\\
    ReDet (Ours)                        &79.51&82.63&53.81&69.82&52.76&75.64&87.82&90.83&75.81&68.78&49.11&71.65&75.57&75.17&58.29&15.36&67.66\\
    ReDet (Ours)                    &88.68&86.57&61.93&81.20&73.71&83.59&90.06&90.86&84.30&75.56&71.55&71.86&83.93&80.38&75.62&49.55&\textbf{78.08}\\\hline
    \end{tabular}}
\end{center}
    \vspace*{-3mm}
   \caption{\textbf{Performance comparisons on DOTA-v1.5 test set}. Note the results of Faster R-CNN OBB (FR-O)~\cite{ren2017faster}, RetinaNet OBB (RetinaNet-O)~\cite{lin2017focal}, Mask R-CNN~\cite{he2017maskrcnn} and Hybrid Task Cascade (HTC)~\cite{chen2019hybrid} are our re-implemented version for DOTA. OWSR~\cite{li2019oswr} is a method from DOAI 2019, and we report its single model performance for fair comparisons. The HBB results of our method are converted from OBB results by calculating the axis-aligned bounding boxes.  means multi-scale training and testing.}
   \vspace*{-3mm}
   \label{tab:dota15_sota}
\end{table*}


\begin{table*}
   \small
\begin{center}
\begin{tabular}{c|cccccccc|c} \hline
        method  & RC2~\cite{liu2017rrpnship} &RRPN~\cite{ma2018arbitrary}  & RPN~\cite{zhang2018toward} & RRD~\cite{liao2018rotation}  & RoI Trans.~\cite{ding2018transformer} & Gliding Vertex~\cite{xu2019gliding}  \\ 
        mAP     & 75.7                       &79.08                        & 79.6                 & 84.3                         & 86.2                                  & 88.2                                              \\ \hline
        method  & RDet~\cite{yang2019r3det} & DRN~\cite{pan2020dynamic} & CenterMap~\cite{wang2020centermap}      & CSL~\cite{yang2020arbitrary}      & SA-Net~\cite{han2020align}         &ReDet (Ours)       \\ 
        mAP     & 89.26                         & 92.7                  & 92.8                                & 89.62                             & 90.17 / 95.01  & \textbf{90.46} /  \textbf{97.63} \\ \hline
        \end{tabular}
\end{center}
   \vspace{-3mm}
  \caption{\textbf{Comparisons of state-of-the-art methods on HRSC2016}.  indicates that the result is evaluated under VOC2012 metrics, while other methods are all evaluated under VOC2007 metrics. We report both results for fair comparisons.}
   \vspace*{-1mm}
   \label{tab:hrsc2016_sota}
\end{table*}

\begin{figure*}
   \begin{center}
      \includegraphics[width=\linewidth]{figures/dota_results_v2.pdf}      
   \end{center}
   \vspace{-3mm}
   \caption{\textbf{Qualitative comparisons} between the proposed ReDet and the baseline method on DOTA-v1.5.}
  \vspace*{-3mm}
   \label{fig:dota_results}
\end{figure*}



\subsection{Ablation Studies}
In this section, we conduct a series of ablation experiments on DOTA-v1.5 test set to evaluate the effectiveness of our proposed method.
Note that we use the original ResNet+FPN and RRoI Align as the backbone and RoI warping method for the baseline method, respectively. 

{\bf Rotation-equivariant backbone.} We evaluate the effectiveness of rotation-equivariant backbone with ReResNet50+ReFPN under different settings. 
As shown in Tab.~\ref{tab:re_backbone}, compared to ResNet50, ReResNet50 achieves lower classification accuracy due to the reduction of parameters, but it obtains higher detection mAP.
We find the backbone under the cyclic group  achieves better accuracy-parameter trade-off. ReResNet50+ReFPN under  gains \textbf{1.83} detection mAP improvements with only {\bf 1/8} parameters (103 Mb \emph{vs.} 12 Mb).
Besides, we also extend ReResNet+ReFPN to other methods in Tab.~\ref{tab:other_method}. Both Faster R-CNN OBB and RetinaNet OBB with ReResNet50+ReFPN outperform its counterpart which further demonstrates the effectiveness of rotation-equivariant backbones.

{\bf Effectiveness of RiRoI Align.} As shown in Tab.~\ref{tab:riroi_align}, compared with RRoI Align, RiRoI Align shows significant improvements due to its orientation alignment mechanism.
While RRoI Align+MaxPool leads to a significant drop in mAP, indicating that the orientation pooling is undesirable in oriented object detection.
RiRoI Align with a  interpolation achieves the highest \textbf{66.86} mAP and \textbf{0.87} mAP improvements than RRoI Align.
Besides, we find RiRoI Align with a  interpolation only gains \textbf{0.33} mAP. The reason may be that too many interpolations hurt the equivariant property and inner relation between orientations.

{\bf Comparison with rotation augmentation.} From another perspective, our method can be viewed as a special in-network rotation augmentation, which learns from one orientation and can be applied to multiple orientations. In contrast, rotation augmentation enhances the network by generating samples with more orientations and usually requires more time to converge.
As shown in Tab.~\ref{tab:rot_comp}, although our method does not exceed the rotation augmented baseline under 1x schedule,
our ReDet, which preserves the similar amount of parameters, shows \textbf{2.59} mAP improvements with only \textbf{18\%} extra training time.
Moreover, the 2x baseline with rotation augmentation is \textbf{0.68} higher than our ReDet, but it takes \textbf{twice} the training time.

{\bf Performance on other datasets.} To prove the generalization of our proposed method, we also evaluate the performance of ReDet on DOTA-v1.0 and HRSC2016.
As is shown in Tab.~\ref{tab:other_dataset}, compared with the baseline, ReDet achieves better performance on both datasets.
Moreover, ReDet has significant improvements in \textbf{AP75} and \textbf{mAP}, which demonstrates its accurate localization capabilities.


\subsection{Comparisons with the State-of-the-Art}
{\bf Results on DOTA-v1.0.} As shown in Tab.~\ref{tab:dota_sota}, we compare our ReDet with other state-of-the-art methods on DOTA-v1.0 OBB Task. Without bells and whistles, our single-scale model achieves \textbf{76.25} mAP, outperforming all single-scale models and most multi-scale models.
With limited data augmentation (\emph{i.e.}, multi-scale data and random rotation), our method achieves state-of-the-art \textbf{80.10} mAP in the whole dataset, and obtains the best or second-best results among \textbf{12/15} categories.

{\bf Results on DOTA-v1.5.} Compared with DOTA-v1.0, DOTA-v1.5 contains many extremely small instances, which increases the difficulty of object detection. We report both OBB and HBB results on DOTA-v1.5 test set in Tab.~\ref{tab:dota15_sota}. With single-scale data, our method achieves \textbf{66.86} OBB mAP and \textbf{67.66} HBB mAP, outperforming RetinaNet OBB, Faster R-CNN OBB, Mask R-CNN~\cite{he2017maskrcnn} and HTC~\cite{chen2019hybrid} by a large margin.
Especially for the categories with small instances (\emph{e.g.}, HA, SP, CC) and large scale variations (\emph{e.g.}, PL, BD), our method performs better.
Besides, as shown in Fig.~\ref{fig:accuracy_parameter}, our ReDet achieves better parameter \emph{vs.} accuracy trade-off, which further demonstrates its efficiency.
Compared to previous best results by OWSR~\cite{li2019oswr}, our multi-scale model achieves state-of-the-art performance, about \textbf{76.80} OBB mAP and \textbf{78.08} HBB mAP. 
Qualitative comparisons between our ReDet and the baseline method are visualized in Fig.~\ref{fig:dota_results}.

{\bf Results on HRSC2016.} The HRSC2016 contains a lot of thin and long ship instances with arbitrary orientation.
We compare our ReDet with other state-of-the-art methods in Tab.~\ref{tab:hrsc2016_sota}. 
Our method achieves the state-of-the-art performance, \ie, with mAP of \textbf{90.46} and \textbf{97.63} under the VOC2007 and VOC2012 metrics, respectively.


\vspace{-1mm}
\section{Conclusions}
This paper presents a Rotation-equivariant Detector for aerial object detection, which consists of two parts: the rotation-equivariant backbone and the RiRoI Align.
The former produces rotation-equivariant features, while the latter extracts rotation-invariant features from rotation-equivariant features.
Extensive experiments on DOTA and HRSC2016 demonstrate the effectiveness of our method.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
