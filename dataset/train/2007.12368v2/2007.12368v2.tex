We introduce here the technical notation for our multi-task approach across domains and specify the objectives in each of the considered settings. 
Let us assume to observe data $\{(\bx_i^s,\by_i^s)\}_{i=1}^{n^s}$ from one or more source distributions. 
Here $\bx_i^s$ represent the $i$-th image while 
$\by_i^s$ is the corresponding one-hot vector label of dimension $|\mathcal{Y}^s|$.
Starting from these images we can always apply different procedures to generate self-supervised variants. One simple choice is that of applying rotation to produce 4 copies of each sample with \{$0^\circ$, $90^\circ$, $180^\circ$, $270^\circ$\} orientation. The related self-supervised task consists in choosing the correct image rotation. 
A more structured alternative is that of decomposing the original images according to a $3\times 3$ grid: this produces $9$ squared patches from every sample, which are then moved from their original locations and re-positioned to form a set of $9!$ shuffled images. {This task is reminiscent of the jigsaw puzzle game}, where the tiles have to be rearranged to get back the original image.
For both the described cases,  $\{(\bz_k^s,\bp_k^s)\}_{k=1}^{K^s}$ refer to the newly obtained images. The dimension of the one-hot vector label $\bp$ is $4$ when applying rotation, while for patch shuffling we choose a subset $P$ of the $9!$ possible permutations selected by following the Hamming distance based algorithm in \cite{NorooziF16}. The total number of images changes depending on the self-supervised task:  $K^s=4\times n^s$ for rotation and  $K^s=P\times n^s$ for patch shuffling.
Regardless of the specific chosen self-supervised objective we can combine it with supervised learning through a 
standard hard-parameter sharing 
multi-task model realized with a multi-branch ending network  \cite{Caruana:1997}. One output branch will be dedicated to the supervised task exploiting the labels of the source data, while the other will solve the self-supervised problem: rotation or jigsaw puzzle permutation recognition (see Figure \ref{fig:jigen}). 
{The auxiliary self-supervised objective contributes in extracting relevant semantic features from the data, with a final beneficial effect on the object recognition performance. 
Since the self-supervised objective is label agnostic it can run both over supervised and unsupervised domains, supporting generalization and adaptation.
}


\subsection{Domain Generalization}
For our network we indicate the convolutional feature extraction backbone with $G_f$, parametrized by $\theta_f$. The parameters of the object classifier $G_c$ and of the self-supervised task $G_p$ are respectively $\theta_c$ and $\theta_p$. Overall we train the network to obtain the optimal model through \vspace{-3mm}
\begin{align} \small
    \arg \min_{\theta_f, \theta_c, \theta_p} ~~  & 
    \frac{1}{n^s} \sum_{i=1}^{n^s}   \mathcal{L}_c(G_c(G_f(\bx^s_i)),\by_i^s) +  \nonumber \\
    & \alpha^s \frac{1}{K^s}\sum_{k=1}^{K^s}\mathcal{L}_p(G_p(G_f(\bz^s_k)),\bp_k^s)
    \label{equationDG}\vspace{-5mm}
\end{align}
where $\mathcal{L}_c$ and $\mathcal{L}_p$ are cross entropy losses for both the object and self-supervised classifiers. We underline that the self-supervised loss is also calculated on the original images. Indeed, the $0^\circ$ orientation as well as the correct patch sorting correspond
also to one of the possible self-supervised image transformation variants. Differently, the supervised classification loss is not influenced by the shuffled or rotated images, as this would make object recognition tougher. At test time we use the object classifier $G_c$ to predict on the new target images.

\subsection{Domain Adaptation}
By its nature self-supervised learning does not need manual annotation and it can exploit the unlabeled target data $\{{\bx}_j^t\}_{j=1}^{n^t}$ when available in the DA setting.
The target samples are transformed (rotated, shuffled) so that each newly produced instance $\{{\bz}_k^t\}_{k=1}^{K^t}$ gets its own self-supervised label ${\bp}_k^t$.

{An alternative and widely used way 
to involve the target data in the learning process consists in applying the source supervised knowledge on them to evaluate the pseudo-labels $\hat{{\by}}^t=G_c(G_f({\bx}^t))$, and minimize the prediction uncertainty measured by the entropy  $H=-\sum_{l=1}^{|\mathcal{Y}^s|}\hat{\by}^t_{l} \log \hat{\by}^t_{l}$ ~\cite{mancini2018boosting,featurenorm_PDA}. This is a semi-supervised technique which guides the class decision boundary to pass through 
low-density target areas, but its success across domains depends on moderate levels of domain shift to avoid wrong pseudo-labels.
}
{Given their orthogonal and possibly complementary nature, in our DA analysis we combine the entropy term with the supervised and self-supervised loss.}
The overall learning objective is formalized as \vspace{-1mm}
\begin{align} \small
    \arg \min_{\theta_f, \theta_c, \theta_p} ~~  
    \frac{1}{n^s} \sum_{i=1}^{n^s}   & \mathcal{L}_c(G_c(G_f(\bx^s_i)),\by_i^s) + \nonumber \\
    \alpha^s \frac{1}{K^s}\sum_{k=1}^{K^s} & \mathcal{L}_p(G_p(G_f(\bz^s_k)),\bp_k^s) +  \nonumber \\
    \eta~\frac{1}{n^t} \sum_{j=1}^{n^t}  H(G_c(G_f(\bx^t_j))) + 
   \alpha^t \frac{1}{K^t} \sum_{k=1}^{K^t} & \mathcal{L}_p(G_p(G_f(\bz^t_k)),\bp_k^t) .
    \label{equationDA}
\end{align}

\subsection{Partial Domain Adaptation}
\label{subsec:PDA}
In PDA the label space of the target domain is contained in that of the source domain $\mathcal{Y}^t \subseteq \mathcal{Y}^s$. This further shift in the label space makes the problem even more challenging: 
if the matching between the whole source and target data is forced, any adaptive method may incur in a degenerate case producing worse performance than its plain non-adaptive version due to negative transfer \cite{Rosenstein05totransfer}. 

The two $\mathcal{L}_p$ terms in (\ref{equationDA}) help domain shift reduction, however their co-presence may be redundant: the features are already chosen to minimize the 
source classification loss and the self-supervised task on the target back-propagates inducing a cross-domain adjustment on the learned features. Thus, for PDA we can drop the source self-supervised term, which corresponds to setting $\alpha^s=0$. This choice has a double positive effect: on one side it reduces the number of hyper-parameters in the learning process, leaving space for the introduction of other complementary learning conditions, on the other we let the self-supervised module focus only on the target without involving the extra classes of the source. 

\begin{figure}[!t]
  \centering
\includegraphics[width=0.5\textwidth]{final_figures/SSPDA_PAMI.pdf}
\caption{Our PDA approach with jigsaw puzzle self-supervision. The main blocks 
of the network are in gray. The solid line arrows indicate the contribution of each group of training samples
to the corresponding final tasks. The related optimization goals appear at the end of the black/green/ocher arrows. The red blocks illustrate the domain adversarial 
classifier and source sample weighting procedure (weight $\gamma$). 
An analogous scheme holds with self-supervised rotation recognition.}\vspace{-4mm}
\label{fig:scheme}
\end{figure}

To further enforce the focus on the shared classes, we extend our approach by integrating a weighting mechanism analogous to that presented in \cite{PADA_eccv18}. The source classification output on the target data are accumulated with 
$\boldsymbol{\gamma} = \frac{1}{n^t}\sum_{j=1}^{n^t}\hat{\by}_j^t$
and normalized as $\boldsymbol{\gamma} \leftarrow \boldsymbol{\gamma}/ \max(\boldsymbol{\gamma})$, obtaining a $|\mathcal{Y}^s|$-dimensional vector that quantifies the contribution of each source class. 
Moreover, we can easily integrate a \emph{source vs target domain discriminator} $G_d$ as in \cite{Ganin:DANN:JMLR16} and adversarially maximize the related binary cross-entropy to increase the domain confusion, taking also into consideration the defined class weighting 
procedure for the source samples. In more formal terms, the final objective of our multi-task problem in the PDA setting is \vspace{-2mm}
{
\begin{align} \small 
    \arg \min_{\theta_f, \theta_c, \theta_p} \max_{\theta_d} 
    \frac{1}{n^s} \sum_{i=1}^{n^s} {\gamma_{y_i}} \Big( \mathcal{L}_c(G_c(G_f({\bx}^s_i),\by_i^s)  + \nonumber \\[-5pt] 
    \lambda \log (G_d(G_f({\bx}^s_i))) \Big)+  \nonumber \\[-5pt]
    \frac{1}{n^t} \sum_{j=1}^{n^t} \Big( \eta ~   H(G_c(G_f({\bx}^t_j))) +  
    \lambda \log (1 - G_d(G_f({\bx}^t_j)))\Big) + \nonumber \\[-10pt]
    \alpha^t \frac{1}{K^t} \sum_{k=1}^{K^t} \mathcal{L}_p(G_p(G_f({\bz}^t_k)),\bp_k^t)
    \label{equationPDA}
\end{align}}
where {$\gamma_{y_i}$ is the class weight for the ground truth label of the source point ${\bx}^s_i$} and $\lambda$ is a hyper-parameter that adjusts the importance of the introduced domain discriminator. 
When $\lambda=0$ and $\gamma_{y} = 1/|\mathcal{Y}^s| $ we fall back to the standard DA case. 
A schematic illustration of the method is presented in Fig. \ref{fig:scheme}.

\subsection{Implementation details}
\label{subsec:implementation}
We designed our multi-task network to leverage over different convolutional deep architectures: the backbone $G_f$ may inherit the structure of standard networks as AlexNet or ResNet.  
The specific object and self-supervised classifiers heads $G_c,G_p$ are respectively implemented by an ending fully connected layer.
{When including  multiple self-supervised tasks in the model (\ie Jigsaw+Rotation), a $G_p$ head is assigned to each self-supervised objective. Specifically, shuffled images are directed to the Jigsaw final head $G_p^J$ , while the rotated images to the Rotation recognition head $G_p^R$.}
In the PDA setting we introduced the domain classifier $G_d$ by adding three fully connected layers after the last pooling layer of the main backbone, and using a sigmoid function for the last activation as in \cite{Ganin:DANN:JMLR16}. For all our experiments we trained the network end-to-end by fine-tuning all the feature layers from Imagenet pre-trained models \cite{imagenet}, while $G_c, G_p$ and $G_d$ are learned from scratch. 

Overall the network for DG has two main hyper-parameters: $\alpha$ that weights the self-supervised loss, and the data bias parameter $\beta$ which regulates the data input process. The self-supervised variants of the images enter the network together with the original ones, hence each image batch contains both of them with $\beta$ specifying their relative ratio. For instance $\beta=0.6$ means that for each batch, $60\%$ of the images are standard, while the remaining $40\%$ are rotated or composed of shuffled patches. {In our experiments we chose $\alpha$ and $\beta$ by keeping a source validation set ($10\%$ of the training data) and performing model selection on it by following \cite{anonymous2021in}. 
When combining Jigsaw+Rotation we have respectively $\alpha_J$ and $\alpha_R$, while the fraction of transformed images regulated by $\beta$ are rotated or shuffled with equal probability.
In the DA setting $\alpha$ decouples in $\alpha^s$ and  $\alpha^t$ respectively for source and target data. While discussing the experimental results we will see the outcome of cross-validating $\alpha$ on the source and then setting $\alpha=\alpha^s=\alpha^t$ or fixing $\alpha^s=0$ as well as the effect on model robustness when manually tuning $\alpha^t$. 
Further parameters in DA and PDA are $\eta$ and  $\lambda$. The first is the weight assigned to the entropy loss which we safely fixed to small values: $0.1$ for DA and $0.2$ for PDA. Finally, $\lambda$ balances the importance of the gradient reversal layer when included in PDA and we adopted the same scheduling of \cite{Ganin:DANN:JMLR16} to update its value, so that the importance of the domain discriminator increases with the training epochs.}

In designing the jigsaw puzzle task we need to choose the image patch grid size $n \times n$, and the cardinality of the patch permutation subset $P$. 
As we will detail in the following section, our multi-task approach is robust to these values and for all our experiments we kept them fixed ($3 \times 3$ grid, $P=30$).

We used a simple data augmentation protocol by randomly cropping the images to retain between $80-100\%$ and randomly applied horizontal flipping. By following \cite{Noroozi_2018_CVPR}, we also randomly ($10\%$ probability) convert an image tile to grayscale.
Our DG/DA model is trained with an SGD solver, $30$ epochs, batch size $128$,  learning rate set to $0.001$ and stepped down to $0.0001$ after $80\%$ of the training epochs.
Our PDA model is trained with 
SGD with momentum set at $0.9$, weight decay  $0.0005$ and $24$ epochs. We used batch size of 64 and initial learning rate $0.0005$.
Some specific training details are used in the PrDA setting and will be  described in Sec. \ref{sec:exp_DG_6}.
We implemented our deep methods in PyTorch and the code is available at \url{https://github.com/silvia1993/Self-Supervised_Learning_Across_Domains}.


