

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}

\usepackage[accsupp]{axessibility}  





\graphicspath{{figures/}}
\def\sig{\text{sign\,}}
\def\diag{\text{diag\,}}
\def\exp{\text{exp\,}}
\def\eg{\emph{e.g.}\,}
\def\ie{\emph{i.e.}\,}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{arydshln}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true
}

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{3821}  

\title{Multi-Granularity Prediction for Scene Text Recognition} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Multi-Granularity Prediction for Scene Text Recognition}
\author{Peng Wang \thanks{Equal contribution.  Corresponding author.} \and
Cheng Da \and
Cong Yao  }

\authorrunning{P. Wang et al.}
\institute{Alibaba DAMO Academy, Beijing, China\\
\email{\{wdp0072012,dc.dacheng08,yaocong2010\}@gmail.com}}
\maketitle

\begin{abstract}
Scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this challenging problem, numerous innovative methods have been successively proposed and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet powerful vision STR model, which is built upon ViT and outperforms previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an \textit{implicit} way, \ie, subword representations (BPE and WordPiece) widely-used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push the performance envelop of STR to an even higher level. Specifically, it achieves an average recognition accuracy of  on standard benchmarks. Code is available at~\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR}.
\keywords{Scene Text Recognition, ViT, Multi-Granularity Prediction}
\end{abstract}



\section{Introduction}  \label{sec:intro}
Reading text from natural scenes is one of the most indispensable abilities when building an automated machine with high-level intelligence. This explains the reason why researchers from the computer vision community sedulously have explored and investigated this complex and challenging task for decades. Scene text recognition (STR) involves decoding textual content from natural images (usually cropped sub images), which is a key component in text reading pipelines.

Previously, a number of methods~\cite{CRNN,Focusing,ASTER,MASTER} have been proposed to address the problem of scene text recognition. Recently, there emerges a new trend that linguistic knowledge is introduced into the text recognition process. SRN~\cite{SRN} devised a global semantic reasoning module (GSRM) to model global semantic context. ABINet~\cite{ABInet} proposed bidirectional cloze network (BCN) as the language model to learn bidirectional feature representation. Both SRN and ABINet adopt an independent and separate language model to capture rich language prior. 

\begin{figure}[!htp]\centering
\vspace{-2mm}
 \includegraphics[width=0.85\textwidth]{motivation.pdf}
\vspace{-2mm}
 \caption{Pipelines of classic CNN-based, ViT-based and the proposed MGP-STR scene text recognition methods are illustrated in (a), (b) and (c), respectively. (d) Examples of Character, BPE and WordPiece subword tokenization. (Best viewed in color.) }
 \label{fig:motivation}
\vspace{-2mm}
\end{figure}
In this paper, we propose to integrate linguistic knowledge in an \textbf{\textit{implicit}} way for scene text recognition. Specifically, we first construct a pure vision STR model based on ViT~\cite{dosovitskiy2020image} and a tailored Adaptive Addressing and Aggregation (A) module inspired by TokenLearner~\cite{tokenlearner}. This model serves as a strong baseline, which already achieves better performance than previous methods for scene text recognition, according to the experimental comparisons. To further make use of linguistic knowledge to enhance the vision STR model, we explore a Multi-Granularity Prediction (MGP) strategy to inject information from the language modality. The output space of the model is expanded that subword representations (BPE and WordPiece) are introduced, \ie, the augmented model would produce two extra subword-level predictions, besides the original character-level prediction. Notably, there is no independent and separate language model. In the training phase, the resultant model (named MGP-STR) is optimized 
with a standard multi-task learning paradigm (three losses for three types of predictions) and the linguistic knowledge is naturally integrated into the ViT-based STR model. In the inference phase, the three types of predictions are fused to give the final prediction result. Experiments on standard benchmarks verify that the proposed MGP-STR algorithm can obtain state-of-the-art performance. Another advantage of MGP-STR is that it does not involve iterative refinement, which could be time-consuming in the inference phase. The pipeline of the proposed MGP-STR algorithm as well as that of previous CNN-based and ViT-based methods are shown in Fig.~\ref{fig:motivation}. In a nutshell, the major difference between MGP-STR and other methods is that it generates three types of predictions, representing textual information at different granularities: from individual characters to short character combinations, and even whole words. 

The contributions of this work are summarized as follows: (1) We construct a pure vision STR model, which combines ViT with a specially designed A module. It already outperforms existing methods. (2) We explore an implicit way for incorporating linguistic knowledge by introducing subword representations to facilitate multi-granularity prediction, and prove that an independent language model (as used in SRN and ABINet) is not indispensable for STR models. (3) The proposed MGP-STR algorithm achieves state-of-the-art performance.













\section{Related Work}
Scene Text Recognition (STR) is a long-term subject of attention and research~\cite{zhu2016scene,long2021scene,chen2021text}. With the popularity of deep learning methods~\cite{vgg,resnet,rnn1}, its effectiveness in the field of STR has been extensively verified. Depending on whether linguistic information is applied, we roughly divide STR methods into two categories, \ie, language-free and language-augmented methods.

\subsection{Language-Free STR Methods}

The mainstream way for image feature extraction in STR methods is CNN~\cite{vgg,resnet}.
For example, previous STR methods~\cite{CRNN,rare,rnn1} utilize VGG.
Current STR methods~\cite{Rosetta,STAR-Net,deep,GCRNN} employ ResNet~\cite{resnet} for better performance.
Based on the powerful CNN features, various methods~\cite{zhang2022context,PIMNet,liu2022perceiving} are proposed to tackle the STR problem.
CTC-based methods~\cite{CRNN,wan20192d,STAR-Net,gtc,ctc2} use the Connectionist Temporal Classication (CTC)~\cite{ctc} to accomplish sequence recognition.
Segmentation-based methods~\cite{seg,Vocabulary,TextSpotter,TextScanner} cast STR as a semantic segmentation problems. 

Inspired by the great success of Transformer~\cite{trans} in natural language processing (NLP) tasks, the application of Transformer in STR has also attracted more attention. Vision Transformer (ViT)~\cite{dosovitskiy2020image} that directly processes image patches without convolutions opens the beginning of using Transformer blocks instead of CNNs to solve computer vision problems~\cite{liu2021swin,SegFormer}, leading to prominent results. ViTSTR~\cite{ViTSTR} attempts to simply leverage the feature representations of the last layer of  ViT for parallel character decoding. In general, language-free methods often fail to recognize low-quality images due to the lack of language information.

\subsection{Language-Augmented STR Methods}

Obviously, language information is favourable to the recognition of low-quality images.
RNN-based methods~\cite{CRNN,rnn1,GCRNN}  can effectively capture the dependency between sequential characters, which can be regarded as an implicit language model. 
However, they cannot execute decoding in parallel during training and inference.
Recently, Transformer blocks are introduced into CNN-based framework to facilitate language content learning. 
SRN~\cite{SRN} proposes a Global Semantic Reasoning Module (GSRM) to capture the global semantic context through multiple parallel transmissions.
ABINet~\cite{ABInet} presents a Bidirectional Cloze Network (BCN) to explicitly model the language information, which is further used for iterative correction.
VisionLAN~\cite{vlan} proposes a visual reasoning module that simultaneously captures visual and language information by masking input images at the feature level. 
The mentioned above approaches utilize a specific module to integrate language information.
Meanwhile, most works~\cite{MJ1,ABInet} capture semantic information based on character-level or word-level.
In this paper, we manage to utilize multi-granularity (character, subword and even word) semantic information based on BPE and WordPiece tokenizations.

\begin{figure}[t]\centering
 \includegraphics[width=0.8\textwidth]{overview2.pdf}
 \caption{The architecture of the proposed MGP-STR algorithm. }
 \label{fig:overview}
\end{figure}

\section{Methodology}

The overview of the proposed MGP-STR method is depicted in Fig.~\ref{fig:overview}, which is mainly built upon the original Vision Transformer (ViT) model~\cite{dosovitskiy2020image}. We propose a tailored Adaptive Addressing and Aggregation (A) module to select a meaningful combination of tokens from ViT and integrate them into one output token corresponding to a specific character, denoted as Character A module.
Moreover, subword classification heads based on BPE A module and WordPiece A module are devised for subword predictions, so that the language information can be implicitly modelled. 
Finally, these multi-granularity predictions are merged via a simple and effective fusion strategy. 
\subsection{Vision Transformer Backbone}
The fundamental architecture of MGP-STR is Vision Transformer~\cite{dosovitskiy2020image,deit}, where the original image patches are directly utilized for image feature extraction by linear projection. 
As shown in Fig.~\ref{fig:overview}, an input RGB image    is split into non-overlapping patches.
Concretely, the image is reshaped  into a sequence of flattened 2D patches ,
where  is the resolution of each image patch and  is the number of feature channels of . 
In this way, a 2D image is represented as a sequence with  tokens, which serve as the effective input sequence of Transformer blocks.
Then, these tokens of   are linear transcribed into  dimension patch embeddings. 
Similar to the original ViT~\cite{dosovitskiy2020image} backbone, a learnable   token embedding with  dimension is introduced into patch embeddings.
And position embeddings are also added to each patch embedding to retain the positional information,
where the standard learnable 1 position embedding is employed.
Thus, the generation of patch embedding vector is formulated as follows:

where  is the  embedding,   is a linear projection matrix and   is the position embedding.

The resultant feature sequence  serves as the input of Transformer encoder blocks,
which are mainly composed of Multi-head Self-Attention (MSA), Layer Normalization (LN), Multilayer Perceptron (MLP) and residual connection as in Fig.\ref{fig:overview}. The Transformer encoder block is formulated as:

Here,  is the depth of Transformer block and  .
The MLP consists of two linear layers with GELU activation.
Finally, the output embedding  of Transformer is utilized for subsequent text recognition.

\begin{figure}[t]\centering
 \includegraphics[width=0.8\textwidth]{positionlearner.pdf}
 \caption{The detailed architectures of the three A modules. }
 \label{fig:token}
\end{figure}

\subsection{ Adaptive Addressing and Aggregation (A) Modules} \label{sec:token}

Traditional Vision Transformers~\cite{dosovitskiy2020image,deit} usually prepend a learnable  token to the sequence of patch embeddings, which directly collects and aggregates the meaningful information and serves as the image representation for the classification of the whole image.
While the task of scene text recognition aims to produce a sequence of character predictions, where each character is only related to a small patch of the image.
Thus, the global image representation   is inadequate for text recognizing task.
ViTSTR~\cite{ViTSTR} directly employs the first  tokens of   for text recognition, where   is the maximum text length. Unfortunately, the rest tokens of  are not fully utilized.

In order to take full advantage of the rich information of the sequence  for text sequence prediction, 
we propose a tailored Adaptive Addressing and Aggregation (A)  module to select a  meaningful combination of tokens  and integrate them into one token corresponding to a specific character.
Specifically, we manage to learn  tokens  from the sequence  for the subsequent text recognizing task.
An aggregation function is, thus, formulated as , which converts the input   to a token vector . And  such  functions are constructed for the sequential output of text recognition.
Typically, the aggregation function  is implemented via a spatial attention mechanism~\cite{tokenlearner} to adaptively select the tokens from  corresponding  to  character.
Here, we employ function  and softmax function to generate precise spatial attention 
mask  from .
Thus, each output token  of A module is produced by 

Here, (·) is implemented by group convolution with one  kernel. And   is a linear mapping matrix for learning feature . Therefore, the resulting tokens of different aggregation functions are gathered together to form the final output tensor as follows:


Owing to the effective and efficient  A module, the ultimate
representation of the text sequence is denoted as  in Eq.~(\ref{eq:Y}). Then, a character classification head is built by  for text sequence recognition, where  is a linear mapping matrix,  is the number of categories and  is the classification logist. 
We regard this module as Character  A for character-level prediction, of which the detailed structure is illustrated in Fig.~\ref{fig:token}(a). 

\subsection{Multi-Granularity Predictions}
\label{sec:MGP}
Character tokenization that simply splits text into characters is commonly-used in scene text recognition methods.
However, this naive and standard way ignores the language information of text. 
In order to effectively resort to linguistic information for scene text recognition, we incorporate subword~\cite{subword} tokenization mechanism in NLP~\cite{BERT} into text recognition method. Subword tokenization algorithms aim to decompose rare words into meaningful subwords and remain frequently used words, so that the grammatical information of word has already been captured in the  subwords. Meanwhile, since A module is independent of Transformer encoder backbone, we can directly add extra parallel subword A modules for subword predictions. 
In such a way, the language information can be implicitly injected into model learning for better performance. Notably, previous methods, \ie, SRN~\cite{SRN} and ABINet~\cite{ABInet}, design an explicit transformer module for language modelling, while we cast linguistic information encoding problem as a character and subword prediction task without an explicit  language model. 

Specifically, we employ two subword tokenization algorithms Byte-Pair Encoding (BPE)~\cite{BPE} and WordPiece~\cite{wordpiece}
\footnote{Considering the potential out-of-vocabulary (OOV) issue in the inference phase, we did not directly predict whole words.} 
to produce various combinations as shown in Fig.\ref{fig:motivation}(b)(c). 
Thus, BPE A module and WordPiece A module are proposed for subword attention.
And two subword-level classification heads are used for subword predictions.
Since subwords could be whole words (such as ``coffee'' in WordPiece), subword-level and even word-level predictions can be generated by the BPE and WordPiece classification heads. 
Along with the original character-level prediction, we denote these various outputs as multi-granularity predictions for text recognition. In this way, character-level prediction guarantees the fundamental recognition accuracy, and subword-level or word-level predictions can serve as complementary results for noised images via linguistic information. 

Technically, the architecture of BPE or WordPiece A module is the same as Character one. They are independent of each other with different parameters.
And the numbers of categories are different for different classification heads, which depend on the vocabulary size of each tokenization method.
The cross entropy loss is employed for classification.
Additionally, the mask  precisely indicates the attention location of the  character in Character A module, 
while it roughly shows the  subword region of the image in subword A modules, due to the higher complexity and uncertainty of learning subword splitting. 


\subsection{Fusion Strategy for Multi-Granularity Results }
\label{sec:fuse}
Multi-granularity predictions (Character, BPE and WordPiece) are generated by different A modules and classification heads. Thus, a fusion strategy is required to merge these results. At the beginning, we attempt to fuse multi-granularity information by aggregating text features   of the output of different A modules at feature level. However, since these features are from different granularities, the   token  of character level is not aligned with the  token  (or ) of BPE level (or WordPiece level), so that these features cannot be added for fusion. Meanwhile, even if we concatenate features by [], only one character-level head can be used for final prediction. The subword information will be greatly impaired in this way, resulting in less improvement.

Therefore, decision-level fusion strategy is employed in our method. However, perfectly fusing these predictions is a challenging problem~\cite{rrlrgu}.
We, thus, propose a compromised but efficient fusion strategy based on the prediction confidences.
Specifically, the recognition confidence of each character or subword can be obtained by the corresponding classification head.
Then, we present two fusion functions  to produce the final recognition score based on atomic confidences:


We only consider the confidence of valid character or subword and ending symbol , and ignore padding symbol .
``Mean'' recognition score is generated by the mean value function as in Eq.~(\ref{eq:mean}). 
And ``Cumprod'' represents the score produced by cumulative product function.
Then, three recognition scores of three classification heads for one image can be obtained by .
We simply pick the one with the highest recognition score as the the final predicted result.


\section{Experiment}
\label{Sec:Experiment}
\subsection{ Datasets}

For fair comparison, we use MJSynth~\cite{MJ1,MJ2} and SynthText~\cite{ST} as training data. MJSynth contains  realistic text images and SynthText includes  synthetic text images. The test dataset consists of ``regular'' and ``irregular'' datasets. The ``regular'' dataset is mainly composed of horizontally aligned text images. IIIT 5K-Words (IIIT)~\cite{IIIT} consists of 3,000 images collected on the website. Street View Text (SVT)~\cite{SVT} contains 647 test images. ICDAR 2013 (IC13)~\cite{IC13} contains 1,095 images cropped from mall pictures, but we eventually evaluate on 857 images, discarding images that contain non-alphanumeric characters or less than three characters. The text instances in the ``irregular'' dataset are mostly curved or distorted. ICDAR 2015 (IC15)~\cite{IC15} includes 2,077 images collected from Google Eyes, but we use 1,811 images without some extremely distorted images. Street View Text-Perspective (SVTP)~\cite{SVTP} contains 639 images collected from Google Street View. CUTE80 (CUTE)~\cite{CUTE} consists of 288 curved images.







\subsection{Implementation Details}

\subsubsection{Model Configuration.} 
MGP-STR is built upon DeiT-Base model~\cite{deit}, which is composed of  stacked transformer blocks.
For each layer, the number of head is  and the embedding dimension  is . 
More importantly, square  images~\cite{dosovitskiy2020image,deit,ViTSTR} are not adopted in our method.
The height  and width  of the input image are set to  and .
The patch size  is set to 4 and thus  plus one  tokens  can be produced.
The maximum length  of the output sequence  of A module is set to . 
The vocabulary size  of Character classification head is set to 38, including , ,  for padding symbol and  for ending symbol.
The vocabulary sizes of BPE and WordPiece heads are set to  and .



\subsubsection{Model Training.} 
The pretrained weights of DeiT-base~\cite{deit} are loaded the initial weights, except the patch embedding model, due to inconsistent patch sizes.
Common data augmentation methods~\cite{randaug} for text image are applied, such as perspective distortion, affine distortion, blur, noise and rotation.
We use  NVIDIA Tesla V100 GPUs to train our model with a batch size of . Adadelta~\cite{Adadelta} optimizer is employed with an initial learning rate of .
The learning rate decay strategy is Cosine Annealing LR~\cite{cosinlr} and the training lasts  epochs.

\subsection{Discussions on Vision Transformer and A Modules} 

\begin{table*}[t]\centering
\setlength{\tabcolsep}{1pt}
\caption{The ablation study of the proposed vision model and the accuracy comparisons with some SOTA methods based on only vision information.}
\label{tab:char}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
\hline
Methods & Vision & Image size (Patch) &IC13&SVT  &IIIT   & IC15 & SVTP &CUTE  &AVG \\
\hline
MASTER~\cite{MASTER} &   \multirow{3}{*}{CNN}  & - &95.3 &90.60 &95.0 &79.4 &84.5 &87.5 &89.5	 \\
SRN~\cite{SRN}  &  & - &93.2 &88.1 &92.3 &77.5 &79.4 &84.7 &86.9	 \\
ABINet~\cite{ABInet}   &  & - &94.9 &90.4 &94.6 &81.7 &84.2 &86.5 &89.8 \\
\hline
MGP-STR &  \multirow{3}{*}{ViT}   &  &95.68	&91.96	&95.13	&83.88	&85.74	&90.28	&91.07	 \\
MGP-STR &  &  &96.62	&92.27	&95.40	&84.76	&86.98	&88.54	&91.58	 \\
MGP-STR  &  &  &96.50 &93.20 &{96.37} &86.25 &89.46 &{90.63} &92.73 \\
\hline
\end{tabular}
\end{table*}

We analyse the influence of the patch size of Vision Transformer and the effectiveness of A module in the proposed MGP-STR method (shown in Table~\ref{tab:char}). MGP-STR represents the model that simply uses the first  tokens of  for text recognition as in ViTSTR~\cite{ViTSTR}, where the input image is reshaped to  and the patch size is set to . In order to retain the significant information of the original text image,  images with  patches are employed in MGP-STR. MGP-STR outperfrrms MGP-STR, which indicates that the standard image size of ViT~\cite{dosovitskiy2020image,deit} is incompatible with text recognition. Thus,  images with  patches are used in MGP-STR.

When the Character A module is introduced into MGP-STR, denoted as MGP-STR, the recognition performance will be further improved. MGP-STR and MGP-STR cannot fully learn and utilize the all tokens, while the Character A module can adaptively aggregate features of the last layer, resulting in more sufficient learning and higher accuracy. Meanwhile, compared with SOTA text recognition methods with CNN feature extractors, the proposed MGP-STR method achieves substantially performance improvement.


\begin{table*}[t]\centering
\setlength{\tabcolsep}{4pt}
\ra{1}
\caption{The accuracies of MGP-STR with different fusion strategies.}
\label{tab:fuse}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
Method & Mode &IC13&SVT  &IIIT   & IC15 & SVTP &CUTE  &AVG \\
\hline
\multirow{3}{*}{MGP-STR}    &  Char &96.49	&93.66	&96.1	&86.14	&88.83	&89.58	&92.53	 \\
& Mean &97.31	&94.28	&96.60	&86.97	&90.23	&90.97	&93.28	 \\
& Cumprod &97.32	&94.74	&96.40	&87.24	&91.01	&90.28	&93.35	 \\
\hline
\end{tabular}
\end{table*}

\subsection{Discussions on Multi-Granularity Predictions }

\begin{table*}[t]\centering
\setlength{\tabcolsep}{1.5pt}
\ra{1}
\caption{The accuracy results of the four variants of MGP-STR model. ``Char'', ``BPE'' and ``WP'' at ``Output'' represent predictions of Character, BPE and WordPiece classification head  in each model, respectively. ``Fuse'' represents the fused results.}
\label{tab:CBW}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Methods & Char & BPE & WP &Output &IC13&SVT  &IIIT   & IC15 & SVTP &CUTE  &AVG \\
\hline
MGP-STR &\checkmark &  &  &  Char &96.50 &93.20 &{96.37} &86.25 &89.46 &{90.63} &92.73 \\
\hline
\multirow{3}{*}{MGP-STR} &  \multirow{3}{*}{\checkmark} & \multirow{3}{*}{ \checkmark} &  \multirow{3}{*}{} &  Char &97.43	&93.82	&96.53	&85.92	&89.15	&90.28	&92.84	 \\
\cline{6-12}
& & & &  BPE &97.78	&94.13	&90.00	&81.12	&88.37	&82.64	&88.63	 \\
& & & &  Fuse &97.67 &94.47	&96.73	&86.97	&88.99	&89.93	&93.24	 \\
\hline
\multirow{3}{*}{MGP-STR} &  \multirow{3}{*}{\checkmark} &  \multirow{3}{*}{} & \multirow{3}{*}{ \checkmark}  &  Char &96.97	&93.97	&96.30	&86.20	&90.39	&89.93	&92.87	 \\
& & & &  WP &95.92	&93.35	&87.70	&78.74	&89.30	&80.21	&86.78	 \\
& & & &  Fuse &97.32	&93.82	&96.60	&86.91	&90.54	&90.97	&93.25	 \\
\hline
\multirow{4}{*}{MGP-STR} &  \multirow{4}{*}{\checkmark} &  \multirow{4}{*}{\checkmark} & \multirow{4}{*}{ \checkmark}  &  Char &96.49	&93.66	&96.10	&86.14	&88.83	&89.58	&92.53	 \\
& & & &  BPE &95.56	&93.66	&88.73	&79.84	&89.76	&83.33	&87.63	 \\
& & & &  WP &95.79	&94.59	&86.37	&77.36	&89.61	&79.86	&85.99	 \\
& & & &  Fuse &97.32	&94.74	&96.40	&87.24	&91.01	&90.28	&93.35	 \\
\hline
\end{tabular}
\end{table*}

\subsubsection{Effect of Fusion Strategy. }
Since the subwords generated by subword tokenization methods contain grammatical information, we directly employ subwords as the targets of our method to capture the language information implicitly. As described in Sec.~\ref{sec:token}, two different subword tokenizations (BPE and WordPiece) are employed for complementary multi-granularity predictions. 
Besides the character prediction, we propose two fusion strategies to further merge these three results, denoted as ``Mean’’ and ``Cumprod’’ as mentioned in Sec.~\ref{sec:fuse}. We denote this method that merges three results as MGP-STR, and the accuracy results of MGP-STR with different fusion strategies are listed in Table~\ref{tab:fuse}. Additionally, the first line ``Char'' in Table~\ref{tab:fuse} records the result of character classification head in MGP-STR. It is clear to see that both ``Mean’’ and ``Cumprod’’ fusion strategies can significantly improve the recognition accuracy over single character-level result. Due the better performance of ``Cumprod’’ strategy, we employ it as the fusion strategy in the following experiments.

\subsubsection{Effect of Subword Representations.}
We evaluate four variants of the MGP-STR model, and the performances of these four methods are elaborately reported in Table~\ref{tab:CBW}, including the fused results and the results of each single classification. Specifically, MGP-STR with only Character A module has already obtained promising results. MGP-STR and MGP-STR incorporate Character A module with BPE A module and WordPiece A module, respectively. No matter which subword tokenization is used alone, the accuracy of ``Fuse’’ can exceed that of ``Char’’ in both MGP-STR and MGP-STR methods, respectively. Notably, the performance of the classification  of ``BPE’’ or ``WP’’ could be better than that of ``Char’’ on SVP and SVTP datasets in the same model. These results show that subword predictions can 
boost text recognition performance by implicitly introducing language information. Thus, MGP-STR with three A modules can produce complementary multi-granularity predictions (character, subword and even word). By fusing these multi-granularity results, MGP-STR obtains the best performance. 


\subsubsection{Comparison with BCN.}
Bidirectional cloze network (BCN) is designed in ABINet~\cite{ABInet} for explicit language modelling, and it leads to favorable improvement over pure vision model. We equip MGP-STR with BCN as a competitor of MGP-STR to verify the advantage of multi-granularity predictions. Concretely, we first reduce the dimension  of representation feature  to  for feature fusion of the output of BCN. Following the training setting in~\cite{ABInet}, the model results are reported in Table~\ref{tab:BCN}.  The accuracy of ``V+L'' is  further imporved over the pure vision prediction ``V'' in MGP-STR+BCN, and better than the original ABINet~\cite{ABInet}. However, the performance of MGP-STR+BCN is a little worse than that of MGP-STR. In addition, we provide the upper bound on the performance of MGP-STR, denoted as MGP-STR in Table 4. If one of the three predictions (``Char'', ``BPE'' and ``WP'') is right, the final prediction is considered correct. 
The highest score of MGP-STR demonstrates the good potential of  multi-granularity predictions.
Moreover, MGP-STR only requires two new subword prediction heads, rather than the design of a specific and explicit language model in~\cite{ABInet,SRN}. 


\begin{table*}[t]\centering
\setlength{\tabcolsep}{4pt}
\ra{1}
\caption{The accuracy results of MGP-STR equipped with BCN and multi-granularity prediction. ``V'' represents the results of the pure vision output. ``V+L'' represents the results based on the both vision and language parts.}
\label{tab:BCN}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
Methods & Mode &IC13&SVT  &IIIT   & IC15 & SVTP &CUTE  &AVG \\
\hline
\multirow{2}{*}{MGP-STR+BCN}  &  V &96.97	&93.82	&95.90	&85.53	&89.15	&89.58	&92.40	 \\
& V+L &97.32	&95.36	&95.97	&86.69	&91.78	&89.93	&93.14	 \\
\hline
MGP-STR   & V+L &97.32	&94.74	&96.40	&87.24	&91.01	&90.28	&93.35	 \\
MGP-STR   & V+L &97.66	&96.29	&96.97	&89.06	&92.09	&92.01	&94.38	 \\
\hline
\end{tabular}
\end{table*}

\subsection{Results with Different ViT Backbones} 

\begin{table*}[t]\centering
\setlength{\tabcolsep}{5pt}
\ra{1}
\caption{The accuracy results of MGP-STR with different ViT backbones.}
\label{tab:y0}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
Backbone & Output &IC13&SVT  &IIIT   & IC15 & SVTP &CUTE  &AVG \\
\hline
\multirow{4}{*}{DeiT-Tiny}  &  Char &93.47	&90.57	&93.93	&82.94	&81.71	&84.38	&89.36	 \\
&  BPE &87.40	&84.39	&83.17	&73.72	&77.83	&71.53	&80.48	 \\
&  WP &53.79	&45.44	&60.07	&52.57	&42.79	&42.71	&53.92	 \\
&  Fuse &94.05	&91.19	&94.30	&83.38	&83.57	&84.38	&89.91	 \\
\hline
\multirow{4}{*}{DeiT-Small}  &  Char &95.92	&91.04	&94.97	&84.59	&85.89	&86.81	&91.01	 \\
&  BPE &96.27	&93.35	&89.37	&79.74	&86.67	&82.29	&87.61	 \\
&  WP &75.50	&70.48	&74.70	&66.81	&68.06	&62.15	&71.36	 \\
&  Fuse &96.38	&93.51	&95.30	&86.09	&87.29	&87.85	&91.96	 \\
\hline
\multirow{4}{*}{DeiT-Base}  &  Char &96.49	&93.66	&96.10	&86.14	&88.83	&89.58	&92.53	 \\
&  BPE &95.56	&93.66	&88.73	&79.84	&89.76	&83.33	&87.63	 \\
&  WP  &95.79	&94.59	&86.37	&77.36	&89.61	&79.86	&85.99	 \\
&  Fuse &97.32	&94.74	&96.40	&87.24	&91.01	&90.28	&93.35	 \\
\hline
\end{tabular}
\end{table*}

All of the proposed MGP-STR models mentioned earlier are based on DeiT-Base~\cite{deit}. We also introduce two smaller models, namely DeiT-Small and DeiT-Tiny as presented in~\cite{deit} to further evaluate the effectiveness of MGP-STR method. Specifically, the embedding dimensions of DeiT-Small and DeiT-Tiny are reduced to  and , respectively. Table 5 records the results of each prediction head of the MGP-STR method with different ViT backbones. Clearly, fusing multi-granularity predictions can still improve the performance of pure character-level prediction in every backbone. And bigger models achieve better performance in the same  head. More importantly, the results of ``Char’’ in DeiT-Small and even DeiT-Tiny have already surpassed the SOTA pure CNN-based vision models, referring to Table~\ref{tab:char}. Therefore, MGP-STR with small or tiny ViT backbone is also a competitive vision model and multi-granularity prediction can also work well in different ViT backbones. 

\begin{table*}[t]
\setlength{\tabcolsep}{5pt}
\ra{1}
\centering
\caption{The comparisons with SOTA methods on several public benchmarks.}
\label{tab:sota}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Methods}   &\multicolumn{3}{c|}{Regular Text} &\multicolumn{3}{c|}{Irregular Text} &\multirow{2}{*}{AVG}\\
\cline{2-7}
&IC13 &SVT  &IIIT   & IC15 & SVTP &CUTE &  \\
\hline
TBRA~\cite{deep}   &93.6 &87.5 &87.9 &77.6 &79.2  & 74.0  & 84.6 \\
ViTSTR~\cite{ViTSTR} &93.2 & 87.7 & 88.4 &78.5 &81.8 &81.3 &85.6 \\
ESIR~\cite{ESIR}   &91.3  &90.2 &93.3 &76.9 &79.6 &83.3  &  87.1  \\
DAN~\cite{DAN} &93.9  &89.2 &94.3  &74.5  &80.0 &84.4   &  87.2  \\
SE-ASTER~\cite{SEED}   &92.8 &89.6 &93.8 &80.0 &81.4 &83.6  & 88.3  \\
RobustScanner~\cite{RobustScanner}     & 94.8 & 88.1  & 95.3 & 77.1 & 79.5 &  90.3 & 88.4 \\
TextScanner~\cite{TextScanner} &92.9 &90.1 &93.9 &79.4 &84.3 &83.3  & 88.5 \\
SATRN~\cite{SATRN}    &94.1 &91.3 &92.8  &79.0 &86.5 &87.8 &  88.6 \\
MASTER~\cite{MASTER} &95.3	&90.6	&95.0	&79.4	&84.5	&87.5 & 89.5 \\
\hline
SRN~\cite{SRN}     & 95.5 & 91.5  & 94.8  &82.7  &85.1   &87.8 & 90.4 \\
VisionLAN~\cite{vlan} &95.7 &91.7& 95.8 &83.7 &86.0 &88.5 &91.2 \\
ABINet~\cite{ABInet}    &\textbf{97.4} &{93.5} &{96.2} &{86.0} &{89.3} &89.2  & {92.6} \\
\hline
MGP-STR  &96.50 &93.20 &{96.37} &86.25 &89.46 &\textbf{90.63} &92.73 \\
MGP-STR  &{97.32}	&\textbf{94.74}	&\textbf{96.40}	&\textbf{87.24}	&\textbf{91.01}	&{90.28}	&\textbf{93.35} \\
\hline
\end{tabular}
\end{table*}


\subsection{Comparisons with State-of-the-Arts} 
\label{Sec:sota}
We compare the proposed MGP-STR and MGP-STR methods with previous state-of-the-art scene text recognition methods, and the results on  standard benchmarks  are summarized in Table~\ref{tab:sota}. All of compared methods and ours are trained on synthetic datasets MJ and ST for fair evaluation. And the results are obtained without any lexicon based post-processing. Generally, language-aware methods (\ie, SRN~\cite{SRN}, VisionLAN~\cite{vlan}, ABINet~\cite{ABInet} and MGP-STR) perform better than other language-free methods, showing the significance of linguistic information. Notably, MGP-STR without any language information has already outperformed the state-of-the-art method ABINet with explicit language model. Owing to the multi-granularity prediction, MGP-STR obtains more impressive results further, which outperforms ABINet with  improvement on average accuracy. 

\subsection{Details of Multi-Granularity Predictions}  \label{Sec:fa}

\begin{table*}[t]\centering
\setlength{\tabcolsep}{2pt}
\ra{1}
\caption{The details of multi-granularity prediction of MGP-STR, including the scores of each prediction head, the intermediate multi-granularity (Gra.) results and the final prediction (Pred.). Best viewed in color.   }
\label{tab:fa}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Images & GT &Output &Char  &BPE   & WP  & Fuse \\
\hline
    \multirow{3}{*}{
    \begin{minipage}[b]{0.15\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=\linewidth]{CUTE80_tabbe_0.1643_table_0.9521_table_0.9813_table_0.9813_table.png}}
	\end{minipage}
	}
	& \multirow{3}{*}{table}
	&Score & 0.1643 & 0.9813 & 0.9521 & 0.9813\\
	\cline{3-7}
	& &Gra. & tabbe & \textcolor{green}{table} & \textcolor{green}{table}  & -\\
	\cline{3-7}
	& &Pred. & tabbe & table &  table & table \\
\hline
    \multirow{3}{*}{
    \begin{minipage}[b]{0.15\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=\linewidth]{IC13_857_divsoory_0.0316_dvisory_0.2574_d_visory_0.8218_dvisory_0.8218_dvisory.png}}
	\end{minipage}
	}
	& \multirow{3}{*}{dvisory}
	&Score &0.0316 & 0.8218 & 0.2574 & 0.8218\\
	\cline{3-7}
	& &Gra. &divsoory &  \textcolor{red}{d}  \textcolor{blue}{visory} &  \textcolor{green}{dvisory} & - \\
	\cline{3-7}
	& &Pred. &divsoory & dvisory  & dvisory & dvisory \\
	\hline
    \multirow{3}{*}{
    \begin{minipage}[b]{0.15\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=\linewidth]{SVT_watercourss_0.1565_waterco_0.632_water_course_0.8295_watercourse_0.8295_watercourse.png}}
	\end{minipage}
	}
	& \multirow{3}{*}{watercourse}
	&Score &0.1565 & 0.8295 & 0.632 & 0.8295\\
	\cline{3-7}
	& &Gra. & watercourss &  \textcolor{red}{water}  \textcolor{blue}{course} &  \textcolor{green}{waterco} & - \\
	\cline{3-7}
	& &Pred. &watercourss & watercourse  & waterco & watercourse \\
		\hline
    \multirow{3}{*}{
    \begin{minipage}[b]{0.15\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=\linewidth]{CUTE80_1869_0.9999_18_0.0354_18_69_0.9207_1869_0.9999_1869.png}}
	\end{minipage}
	}
	& \multirow{3}{*}{1869}
	&Score &0.9999 & 0.9207 & 0.0354 & 0.9999\\
	\cline{3-7}
	& &Gra. & 1869 &  \textcolor{red}{18}  \textcolor{blue}{69} &  \textcolor{green}{18} & - \\
	\cline{3-7}
	& &Pred. &1869 & 1869  & 18 & 1869 \\
	\hline
    \multirow{3}{*}{
    \begin{minipage}[b]{0.12\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=\linewidth]{IC15_1811_thday_0.9998_today_0.7638_th_day_0.5983_thday_0.9998_thday.png}}
	\end{minipage}
	}
	& \multirow{3}{*}{thday}
	&Score &0.9998 & 0.5983 & 0.7638 & 0.9998 \\
	\cline{3-7}
	& &Gra. & thday &  \textcolor{red}{th}  \textcolor{blue}{day} &  \textcolor{green}{today} & - \\
	\cline{3-7}
	& &Pred. &thday & thday  & today & thday \\
				\hline
    \multirow{3}{*}{
    \begin{minipage}[b]{0.12\columnwidth}
		\centering
		\raisebox{-.5\height}{\includegraphics[width=\linewidth]{IC13_857_guice_0.9675_guide_0.1131_gu_ice_0.6959_guice_0.9675_guide.png}}
	\end{minipage}
	}
	& \multirow{3}{*}{guide}
	&Score &0.9675 & 0.6959 & 0.1131 & 0.9675 \\
	\cline{3-7}
	& &Gra. & guice &  \textcolor{red}{gu}  \textcolor{blue}{ice} &  \textcolor{green}{guide} & - \\
	\cline{3-7}
	& &Pred. &guice & guice  & guide & guice \\
	\hline
\end{tabular}
\end{table*}

We show the detailed prediction process of the proposed MGP-STR method on  test images from standard datasets. In the first three images, the results of character-level prediction are incorrect, due to irregular font, motion blur and curved shape, respectively. The scores of character prediction are very low, since the images are difficult to recognize and one character is wrong in each image. However, ``BPE'' and ``WP'' heads can recognize ``table'' image with high scores. And ``BPE'' can make correct predictions with two subwords on ``dvisory'' and ``watercourse'' images, while ``WP''  is wrong in ``watercourse'' image. After fusion, the mistakes can be corrected. From the rest three images, interesting phenomena can be observed. The predictions of ``Char'' and ``BPE''  conform to the images. The predictions of ``WP'', however, attempt to produce strings with more linguistic content, like ``today'' and ``guide''.
Generally, ``Char'' aims to produce characters one by one, while ``BPE'' usually generates n-gram segments related to image and ``WP'' tends to directly predict words that are linguistically meaningful. These prove the predictions of different granularities convey text information in different aspects and are indeed complementary.

\subsection{Visualization of Spatial Attention Maps of A Modules} 

\begin{figure}[t]\centering
 \includegraphics[width=0.8\textwidth]{viscase2.pdf}
 \caption{The illustration of spatial attention masks on Character A  module, BPE A  module and WordPiece A  module, respectively. }
 \label{fig:viscase}
\end{figure}

Exemplar attention maps  of Character, BPE and WordPiece A modules are shown in Fig.~\ref{fig:viscase}.
Character A module shows extremely precise addressing ability on a variety of text images.
Specifically, for the ``7'' image with one character, the attention mask seems like the ``7'' shape.
For the ``day'' and ``bar'' images with three characters, the attention masks of middle character ``a'' are  completely different, verifying the adaptiveness of A module.
As depicted in Fig.\ref{fig:motivation}(d) and in Table~\ref{tab:fa}, BPE tends to generate short segments, thus the attention masks of BPE are spilt into  or  areas as shown in ``leaves'' and ``academy'' images.
This is probably because that performing subword splitting and character addressing simultaneously is difficult.
Moreover, WordPiece often produces a whole word, and the attention maps should be the whole feature map. Since the attention maps produced by the softmax function are usually sparse, the attention maps of WordPiece are not as appealing as those of Character A module.
These results are consistent to those of Table~\ref{tab:CBW}, where the accuracies of ``BPE'' and ``WP'' are relatively lower than ``Char'', due to the difficulty of precise subword prediction. 


\subsection{Comparisons of Inference Time and Model Size} 

\begin{table}[h]\centering
\setlength{\tabcolsep}{5pt}
\caption{Comparisons on inference time and model size.}
\label{tab:speed}
\begin{tabular}{|l|l|l|}
\hline
{Methods}  & {Time} (ms/image) & Parameters ()  \\
\hline
ABINet-S-iter1/iter2/iter3 & 13.7/18.6/24.3 & 32.8 \\
\hline
ABINet-L-iter1/iter2/iter3 & 16.1/21.4/26.8 & 36.7 \\
\hline
MGP-STR-tiny/small/base  & 10.6/10.8/10.9  & 5.4/21.4/85.5 \\
\hline
MGP-STR-tiny/small/base  & 12.0/12.2/12.3  & 21.0/52.6/148.0 \\
\hline
\end{tabular}
\end{table}

The model sizes and latencies of the proposed MGP-STR with different settings as well as those of ABINet are depicted in Table.~\ref{tab:speed}~\footnote{All the evaluations are conducted on a NVIDIA V100 GPU.}. Since MGP-STR is equipped with a regular Vision Transformer (ViT) and involves no iterative refinement, the inference speed of MGP-STR is very fast: ms with ViT-Base backbone. Compared with ABINet, MGP-STR runs much faster (ms vs. ms), while obtaining higher performance. The model size of MGP-STR is relatively large. However, a large portion of the model parameter is from the BPE and WordPiece branches. For the scenarios that are sensitive to model size or with limited memory space, MGP-STR is an excellent choice.

\section{Conclusion}

We presented a ViT-based pure vision model for STR, which shows its superiority in recognition accuracy. To further promote recognition accuracy of this baseline model, we proposed a Multi-Granularity Prediction strategy to take advantage of linguistic knowledge. The resultant model achieves state-of-the-art performance on widely-used dadatsets. In the future, we will extend the idea of multi-granularity prediction to broader domains.


\bibliographystyle{splncs04}
\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{ViTSTR}
Atienza, R.: Vision transformer for fast and efficient scene text recognition.
  In: ICDAR. vol. 12821, pp. 319--334 (2021)

\bibitem{deep}
Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What
  is wrong with scene text recognition model comparisons? dataset and model
  analysis. In: ICCV. pp. 4714--4722 (2019)

\bibitem{Rosetta}
Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: Large scale system for text
  detection and recognition in images. In: Guo, Y., Farooq, F. (eds.) SIGKDD.
  pp. 71--79 (2018)

\bibitem{chen2021text}
Chen, X., Jin, L., Zhu, Y., Luo, C., Wang, T.: Text recognition in the wild: A
  survey. ACM Computing Surveys (CSUR)  \textbf{54}(2),  1--35 (2021)

\bibitem{Focusing}
Cheng, Z., Bai, F., Xu, Y., Zheng, G., Pu, S., Zhou, S.: Focusing attention:
  Towards accurate text recognition in natural images. In: CVPR. pp. 5086--5094
  (2017)

\bibitem{randaug}
Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated
  data augmentation with a reduced search space. In: {CVPR} Workshops. pp.
  3008--3017 (2020)

\bibitem{BERT}
Devlin, J., Chang, M., Lee, K., Toutanova, K.: {BERT:} pre-training of deep
  bidirectional transformers for language understanding. In: NAACL-HLT. pp.
  4171--4186 (2019)

\bibitem{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for
  image recognition at scale. In: ICLR (2021)

\bibitem{ABInet}
Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: Autonomous,
  bidirectional and iterative language modeling for scene text recognition. In:
  CVPR. pp. 7098--7107 (2021)

\bibitem{ctc}
Graves, A., Fern{\'{a}}ndez, S., Gomez, F.J., Schmidhuber, J.: Connectionist
  temporal classification: labelling unsegmented sequence data with recurrent
  neural networks. In: ICML. vol.~148, pp. 369--376 (2006)

\bibitem{rrlrgu}
Gu, J., Meng, G., Da, C., Xiang, S., Pan, C.: No-reference image quality
  assessment with reinforcement recursive list-wise ranking. In: AAAI. pp.
  8336--8343 (2019)

\bibitem{ST}
Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in
  natural images. In: CVPR. pp. 2315--2324 (2016)

\bibitem{resnet}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
  recognition. In: CVPR. pp. 770--778 (2016)

\bibitem{ctc2}
He, P., Huang, W., Qiao, Y., Loy, C.C., Tang, X.: Reading scene text in deep
  convolutional sequences. In: AAAI. pp. 3501--3508 (2016)

\bibitem{gtc}
Hu, W., Cai, X., Hou, J., Yi, S., Lin, Z.: {GTC:} guided training of {CTC}
  towards efficient and accurate scene text recognition. In: AAAI. pp.
  11005--11012 (2020)

\bibitem{MJ1}
Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and
  artificial neural networks for natural scene text recognition. NIPS Deep
  Learning Workshop  (2014)

\bibitem{MJ2}
Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the
  wild with convolutional neural networks. Int. J. Comput. Vis.
  \textbf{116}(1),  1--20 (2016)

\bibitem{IC15}
Karatzas, D., Gomez{-}Bigorda, L., Nicolaou, A., Ghosh, S.K., Bagdanov, A.D.,
  Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait,
  F., Uchida, S., Valveny, E.: {ICDAR} 2015 competition on robust reading. In:
  ICDAR. pp. 1156--1160 (2015)

\bibitem{IC13}
Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i~Bigorda, L.G., Mestre,
  S.R., Mas, J., Mota, D.F., Almaz{\'{a}}n, J., de~las Heras, L.: {ICDAR} 2013
  robust reading competition. In: ICDAR. pp. 1484--1493 (2013)

\bibitem{subword}
Labeau, M., Allauzen, A.: Character and subword-based word representation for
  neural language modeling prediction. In: SWCN@EMNLP. pp. 1--13 (2017)

\bibitem{rnn1}
Lee, C., Osindero, S.: Recursive recurrent nets with attention modeling for
  {OCR} in the wild. In: CVPR. pp. 2231--2239 (2016)

\bibitem{SATRN}
Lee, J., Park, S., Baek, J., Oh, S.J., Kim, S., Lee, H.: On recognizing texts
  of arbitrary shapes with 2d self-attention. In: CVPR Workshops. pp.
  2326--2335 (2020)

\bibitem{TextSpotter}
Liao, M., Lyu, P., He, M., Yao, C., Wu, W., Bai, X.: Mask textspotter: An
  end-to-end trainable neural network for spotting text with arbitrary shapes.
  {IEEE} Trans. Pattern Anal. Mach. Intell.  \textbf{43}(2),  532--548 (2021)

\bibitem{seg}
Liao, M., Zhang, J., Wan, Z., Xie, F., Liang, J., Lyu, P., Yao, C., Bai, X.:
  Scene text recognition from two-dimensional perspective. In: AAAI. pp.
  8714--8721 (2019)

\bibitem{liu2022perceiving}
Liu, H., Wang, B., Bao, Z., Xue, M., Kang, S., Jiang, D., Liu, Y., Ren, B.:
  Perceiving stroke-semantic context: Hierarchical contrastive learning for
  robust scene text recognition. In: AAAI. pp. 1702--1710 (2022)

\bibitem{STAR-Net}
Liu, W., Chen, C., Wong, K.K., Su, Z., Han, J.: Star-net: {A} spatial attention
  residue network for scene text recognition. In: BMVC (2016)

\bibitem{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
  transformer: Hierarchical vision transformer using shifted windows. CoRR
  \textbf{abs/2103.14030} (2021)

\bibitem{long2021scene}
Long, S., He, X., Yao, C.: Scene text detection and recognition: The deep
  learning era. IJCV  \textbf{129}(1),  161--184 (2021)

\bibitem{cosinlr}
Loshchilov, I., Hutter, F.: {SGDR:} stochastic gradient descent with warm
  restarts. In: ICLR (2017)

\bibitem{MASTER}
Lu, N., Yu, W., Qi, X., Chen, Y., Gong, P., Xiao, R., Bai, X.: {MASTER}:
  Multi-aspect non-local network for scene text recognition. Pattern
  Recognition  \textbf{117},  107980 (2021)

\bibitem{IIIT}
Mishra, A., Alahari, K., Jawahar, C.V.: Scene text recognition using higher
  order language priors. In: BMVC. pp. 1--11 (2012)

\bibitem{SVTP}
Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with
  perspective distortion in natural scenes. In: ICCV. pp. 569--576 (2013)

\bibitem{PIMNet}
Qiao, Z., Zhou, Y., Wei, J., Wang, W., Zhang, Y., Jiang, N., Wang, H., Wang,
  W.: Pimnet: {A} parallel, iterative and mimicking network for scene text
  recognition. In: ACM MM. pp. 2046--2055 (2021)

\bibitem{SEED}
Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: {SEED:} semantics enhanced
  encoder-decoder framework for scene text recognition. In: CVPR. pp.
  13525--13534 (2020)

\bibitem{CUTE}
Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary text
  detection system for natural scene images. Expert Syst. Appl.
  \textbf{41}(18),  8027--8048 (2014)

\bibitem{tokenlearner}
Ryoo, M.S., Piergiovanni, A.J., Arnab, A., Dehghani, M., Angelova, A.:
  Tokenlearner: What can 8 learned tokens do for images and videos? CoRR
  \textbf{abs/2106.11297} (2021)

\bibitem{wordpiece}
Schuster, M., Nakajima, K.: Japanese and korean voice search. In: ICASSP. pp.
  5149--5152 (2012)

\bibitem{BPE}
Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words
  with subword units. In: ACL. The Association for Computer Linguistics (2016)

\bibitem{CRNN}
Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for
  image-based sequence recognition and its application to scene text
  recognition. IEEE TPAMI  \textbf{39}(11),  2298--2304 (2017)

\bibitem{rare}
Shi, B., Wang, X., Lyu, P., Yao, C., Bai, X.: Robust scene text recognition
  with automatic rectification. In: CVPR. pp. 4168--4176 (2016)

\bibitem{ASTER}
Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: {ASTER:} an attentional
  scene text recognizer with flexible rectification. IEEE TPAMI
  \textbf{41}(9),  2035--2048 (2019)

\bibitem{vgg}
Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
  image recognition. In: ICLR (2015)

\bibitem{deit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J{\'{e}}gou, H.:
  Training data-efficient image transformers {\&} distillation through
  attention. In: ICML. vol.~139, pp. 10347--10357 (2021)

\bibitem{trans}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
  Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS. pp.
  5998--6008 (2017)

\bibitem{TextScanner}
Wan, Z., He, M., Chen, H., Bai, X., Yao, C.: Textscanner: Reading characters in
  order for robust scene text recognition. In: AAAI. pp. 12120--12127 (2020)

\bibitem{wan20192d}
Wan, Z., Xie, F., Liu, Y., Bai, X., Yao, C.: 2d-ctc for scene text recognition.
  arXiv preprint arXiv:1907.09705  (2019)

\bibitem{Vocabulary}
Wan, Z., Zhang, J., Zhang, L., Luo, J., Yao, C.: On vocabulary reliance in
  scene text recognition. In: CVPR. pp. 11422--11431 (2020)

\bibitem{GCRNN}
Wang, J., Hu, X.: Gated recurrent convolution neural network for {OCR}. In:
  NeurIPS. pp. 335--344 (2017)

\bibitem{SVT}
Wang, K., Babenko, B., Belongie, S.J.: End-to-end scene text recognition. In:
  ICCV. pp. 1457--1464 (2011)

\bibitem{DAN}
Wang, T., Zhu, Y., Jin, L., Luo, C., Chen, X., Wu, Y., Wang, Q., Cai, M.:
  Decoupled attention network for text recognition. In: AAAI. pp. 12216--12224
  (2020)

\bibitem{vlan}
Wang, Y., Xie, H., Fang, S., Wang, J., Zhu, S., Zhang, Y.: From two to one: {A}
  new scene text recognizer with visual language modeling network. In: ICCV.
  pp. 1--10 (2021)

\bibitem{SegFormer}
Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer:
  Simple and efficient design for semantic segmentation with transformers. CoRR
   \textbf{abs/2105.15203} (2021)

\bibitem{SRN}
Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards
  accurate scene text recognition with semantic reasoning networks. In: CVPR.
  pp. 12110--12119 (2020)

\bibitem{RobustScanner}
Yue, X., Kuang, Z., Lin, C., Sun, H., Zhang, W.: Robustscanner: Dynamically
  enhancing positional clues for robust text recognition. In: ECCV. vol. 12364,
  pp. 135--151 (2020)

\bibitem{Adadelta}
Zeiler, M.D.: {ADADELTA:} an adaptive learning rate method. CoRR
  \textbf{abs/1212.5701} (2012)

\bibitem{ESIR}
Zhan, F., Lu, S.: {ESIR:} end-to-end scene text recognition via iterative image
  rectification. In: CVPR. pp. 2059--2068 (2019)

\bibitem{zhang2022context}
Zhang, X., Zhu, B., Yao, X., Sun, Q., Li, R., Yu, B.: Context-based contrastive
  learning for scene text recognition. In: AAAI. pp. 888--896 (2022)

\bibitem{zhu2016scene}
Zhu, Y., Yao, C., Bai, X.: Scene text detection and recognition: Recent
  advances and future trends. Frontiers of Computer Science  \textbf{10}(1),
  19--36 (2016)

\end{thebibliography}
 \end{document}
