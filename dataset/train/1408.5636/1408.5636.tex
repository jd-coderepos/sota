\documentclass{LMCS}

\def\dOi{10(3:15)2014}
\lmcsheading {\dOi}
{1--24}
{}
{}
{Mar.~18, 2014}
{Sep.~10, 2014}
{}

\ACMCCS{[{\bf Theory of computation}]: Computational complexity and cryptography---Problems, reductions and completeness}

\keywords{random strings, truth-table degrees, strong reducibilities,
  minimal pair} 

\usepackage{amsfonts,amsmath,amsthm,amssymb,url,ifpdf,xspace}





\newcommand{\etth}{\ }
\newcommand{\ntth}{\ }

\newcommand{\delzt}{\Delta_2^0}
\newcommand{\zd}{0''}
\newcommand{\tre}{\leq_{\text T}}
\newcommand{\tr}{<_{\text T}}
\newcommand{\lown}{\ }
\newcommand{\highn}{\ }
\newcommand{\lowt}{\ }
\newcommand{\nonlowt}{\ }
\newcommand{\enlowt}{}
\newcommand{\ennonlowt}{}
\newcommand{\psil}{\psi_\lambda}
\newcommand{\tauss}{\tau_{\sigma}^s}
\newcommand{\taus}{\tau_{\sigma}}
\newcommand{\tauesn}{\tau_{e,s_0}}
\def\uhr{\upharpoonright}
\def\phi{\varphi}
\def\res{\!\!\upharpoonright\!}     \def\eres{\!\upharpoonright\!}     \def\nes{n_{e,s}}
\def\nesp{n_{e,s+1}}

\def\+{\oplus}
\def\*{\odot}
\newcommand{\bolda}{\mbox{\bf a}}
\newcommand{\boldb}{\mbox{\ b}}
\newcommand{\boldc}{\mbox{\bf c}}
\newcommand{\boldd}{\mbox{\bf d}}
\newcommand{\0}{\mathbf{0}}
\newcommand{\Hh}{\mathbb{H}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\proj}{\mathbb{P}}
\newcommand{\ep}{\varepsilon}
\newcommand{\de}{\delta}
\newcommand{\ra}{\rightarrow}
\newcommand{\lra}{\longrightarrow}
\newcommand{\lreq}{\overset\sim\longrightarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\lRa}{\Longrightarrow}
\newcommand{\lrsa}{\leftrightsquiqarrow}
\newcommand{\ba}{}
\newcommand{\bes}{\begin{enumerate}\topsep=1.5mm \itemsep=-1mm}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\rint}{\calR\!\int}
\newcommand{\calC}{\mathcal C}
\newcommand{\calS}{\mathcal S}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\lowerint}{\mbox{\b{}}}
\newcommand{\upperint}{{\textstyle\bar{\int}}}








\def\N{{\mathbf N}}
\def\Z{{\mathbf Z}}
\def\Q{{\mathbf Q}}
\def\R{{\mathbf R}}
\def\C{{\mathbf C}}
\def\P{{\mathbf P}}
\def\F{{\mathbf F}}
\def\lra{\longrightarrow}




\newcommand{\ov}{\overline}

\newcommand\eT{\equiv_{\rm T}}
\newcommand\leT{\leq_{\rm T}}
\newcommand{\lebT}{\leq_{\rm {bT}}}
\newcommand{\lett}{\leq_{\rm {tt}}}
\newcommand{\nleT}{\nleq_{\rm T}}
\def\geT{\geq_{\rm T}}
\newcommand\lT{<_{\rm T}}
\def\gT{>_{\rm T}}
\def\equivT{\equiv_{\rm T}}
\newcommand{\es}{\emptyset}
\newcommand{\dn}{\!\downarrow}
\newcommand{\up}{\!\uparrow}
\def\diverges{\!\uparrow\,}
\def\converges{\!\downarrow\,}
\def\Up{\!\uparrow\;}
\def\down{\!\downarrow}
\def\Down{\!\downarrow\;}



\newcommand{\lr}{\leftrightarrow}


\newcommand{\ce}{c.e.\ }
\newcommand{\CE}{C.E.\ }




\newcommand{\RKUO}{R_1}
\newcommand{\RKUT}{R_2}
\newcommand{\RKUj}{R_j}
\newcommand{\KUj}{K_j}
\newcommand{\KUO}{K_{1}}
\newcommand{\KUT}{K_{2}}

\newcommand{\ei}{{\langle e,i\rangle}}
\newcommand{\eiprime}{{\langle e',i'\rangle}}
\newcommand{\eizero}{{\langle e_0,i_0\rangle}}
\newcommand{\eiz}{{\langle e,i\rangle_0}}
\newcommand{\ns}{{\langle n,s\rangle}}
\newcommand{\meix}{{m_{\ei,x}}}
\newcommand{\meizx}{{m_{\eiz,x}}}
\newcommand{\meizerox}{{m_{\eizero,x}}}

\newcommand\Gnos{\{\gamma_1(n',s) \mid n_1\leq n'\leq s\}}
\newcommand\Gnts{\{\gamma_2(n',s) \mid n_2\leq n'\leq s\}}
\newcommand\Gns{\{\gamma_j(n',s) \mid n \leq n'\leq s\}}
\newcommand{\eip}{{\langle e',i'\rangle}}

\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\w{\widehat{\phantom{\eta}}}
\def\uh{\upharpoonright}
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\newcommand{\concat}{\symbol{94}}
\newcommand{\dom}{\mbox{dom}}
\newcommand{\ran}{\mbox{ran}}
 

\begin{document}

\title[Random strings and tt-degrees of Turing complete C.E.\ sets]{Random strings and truth-table degrees of Turing complete C.E.\ sets}

\author[M.~Cai]{Mingzhong Cai\rsuper a}
\address{{\lsuper a}Department of Mathematics \\
  Dartmouth College \\
  Hanover, NH 03755, USA}
\email{Mingzhong.Cai@dartmouth.edu}
\thanks{{\lsuper a}The first author's research was partially supported
  by NSF grant DMS-1266214.}



\author[R.~G.~Downey]{Rodney G. Downey\rsuper b}
\address{{\lsuper b}Department of Mathematics, Statistics, and Operations
  Research \\
  Victoria University of Wellington \\
  P.O. Box 600 \\
  Wellington, NEW ZEALAND}
\email{rod.downey@msor.vuw.ac.nz}
\thanks{{\lsuper b}The second author's research was partially supported by a
  Marsden grant.}



\author[R.~Epstein]{Rachel Epstein\rsuper c}
\address{{\lsuper c}Department of Mathematics and Statistics\\
  Swarthmore College \\
  500 College Ave \\
  Swarthmore, PA 19081, USA}
\email{rachel.epstein@swarthmore.edu}
\thanks{{\lsuper c}The third author's research was partially supported by an
AMS-Simons Foundation Travel Grant.}



\author[S.~Lempp]{Steffen Lempp\rsuper d}
\address{{\lsuper{d,e}}Department of Mathematics \\
  University of Wisconsin \\
  Madison, WI 53706-1388, USA}
\email{\{lempp,jmiller\}@math.wisc.edu}

\thanks{{\lsuper d}The fourth author's research was partially
  supported by AMS-Simons Foundation Collaboration Grant 209087.}

\author[J.~S.~Miller]{Joseph S. Miller\rsuper e}
\thanks{{\lsuper e}The last author's research was partially supported
  by NSF grant DMS-1001847.}

\amsclass{Primary: 68Q30, 03D25, 03D30; Secondary: 68Q15, 03D32}

\begin{abstract}
We investigate the truth-table degrees of (co-)c.e.\ sets, in particular,
sets of random strings. It is known that the set of random
strings with respect to any universal prefix-free machine is Turing complete,
but that truth-table completeness depends on the choice of universal machine.
We show that for such sets of
random strings, any finite set of their truth-table degrees do not meet to
the degree~, even within the \ce truth-table degrees, but when
taking the meet over all such truth-table degrees, the infinite meet is
indeed~. The latter result proves a conjecture of Allender,
Friedman and Gasarch. We also show that there are two Turing
complete c.e.\ sets whose truth-table degrees form a minimal pair.
\end{abstract}

\maketitle\vfill



\section{Introduction}
\label{sec_intro}

Recent work in theoretical computer science has established a link between
computational complexity classes and the languages efficiently reducible to
sets of random strings. Intuitively, a random (finite, binary) string is one
that does not have a shorter description than itself. We use \emph{Kolmogorov
complexity} to formalize this intuition, but this leaves us with choices:
There are two common types of Kolmogorov complexity (plain and prefix-free),
and within each type, the set of random strings depends on the choice of
universal machine. See Section~\ref{background} for more detail. Irrespective
of these choices, however, the set of random strings can be shown to ``speed
up'' computation.

\begin{thm}[Buhrman, Fortnow, Kouck\'y and Loff~\cite{BFKL:10}; Allender,
Buhrman, Kouck\'y, van Melkebeek and Ronneburger~\cite{ABKMR:06}; Allender,
Buhrman and Kouck\'y~\cite{ABK:06}]\label{thm:low} Let~ be the set of all
random strings for either plain or prefix-free complexity.
\begin{itemize}
	\item .
	\item .
	\item .
\end{itemize}
\end{thm}

\noindent So, for example, a language in PSPACE can be recognized by a
polynomial-time machine with access to~.

It is also possible to give upper bounds for what can be efficiently
reducible to the set of random strings.

\begin{thm}[Allender, Friedman and Gasarch~\cite{AFG}]\label{thm:AFG}\
\begin{itemize}
\item .
\item .
\end{itemize}
Here~ ranges over universal prefix-free machines,~ is prefix-free
complexity as determined by~, and~ is the corresponding set of
random strings.
\end{thm}

Taking the intersection over all universal prefix-free machines has the
effect of ``factoring out'' the choice of machine. Why is this necessary?
First note that~ is not computable (hence not in PSPACE) but it is
efficiently reducible to itself. This example is unsatisfying, of course,
because we are already explicitly restricting to computable (i.e.,
) languages. For a better example, note that is possible to build
a universal prefix-free machine~ for which there is a computable set  that is not in EXPSPACE.\footnote{For plain
complexity, this follows from~\cite[Theorem~12]{ABK:06}. The authors point
out that the same proof works in the case of prefix-free complexity. It
remains open if for \emph{every} universal prefix-free machine~, there is
a computable set
, even
though the corresponding fact holds for plain complexity. See the discussion
after \cite[Theorem~16]{ABK:06}.}

There are two other ways that Theorem~\ref{thm:AFG} is restricted. For one,
it is only stated for prefix-free complexity; Allender, Friedman and
Gasarch~\cite{AFG} conjecture that it holds for plain complexity as well.
More important for our purposes is the explicit restriction to computable
languages. Allender et al.\ conjecture that this restriction is redundant.

\begin{conj}[Allender, Friedman and Gasarch~\cite{AFG}]\label{conj}
If , then~ is computable.
(Therefore,  can be removed from both parts of
Theorem~\ref{thm:AFG}.)
\end{conj}

\noindent We prove this conjecture and study related questions.

Our approach is purely computability-theoretic. Any set in 
is truth-table reducible to~, so we study tt-reduction to sets of random
strings, i.e., sets of the form~ for different choices of the
prefix-free universal machine~. We show in Theorem~\ref{thmfinite} that
every finite collection of sets of random strings can tt-compute some
noncomputable computably enumerable set. Note that sets of random strings are
Turing complete (and co-c.e.), so it is reasonable to ask if
Theorem~\ref{thmfinite} is a special case of a more general restriction on
the tt-degrees of Turing complete \ce sets. It is not;
Theorem~\ref{minpairthm} shows that there is a minimal pair of Turing
complete \ce sets within the tt-degrees. Finally, in
Theorem~\ref{knight-bishop} we prove that there is \emph{no} noncomputable
set that is tt-reducible to \emph{every} set of random strings. This verifies
Conjecture~\ref{conj}.

Putting Theorems~\ref{thm:low} and~\ref{thm:AFG} together with
Conjecture~\ref{conj}, we obtain:
\begin{itemize}
\item 
\item 
\end{itemize}
In each case,~ ranges over universal prefix-free machines.
Allender~\cite{allender} conjectures that the lower bounds are tight, i.e.,
that  and
, but this is still
\emph{very much} an open question.

\subsection{Definitions and background}\label{background}

The Kolmogorov complexity of a finite string \mbox{}
is a measure of how difficult it is to describe~. Let~ be partial computable function (we call such a
function a \emph{machine}). The plain complexity of~ with respect
to~ is

This depends on the choice of~, but it is straightforward to check that
there is a \emph{universal} machine~ such that~ is optimal for such
machines, up to an additive constant. Plain Kolmogorov complexity~ is
defined to be~ for a fixed universal machine~. Note that for any two
universal machines~ and~,  for some
constant~ depending on~ and~.

We define \emph{prefix-free Kolmogorov complexity} in a similar manner. We
say that a machine~ is \emph{prefix-free}
if whenever~ and~ are two distinct strings contained in the
domain of~, then neither is a prefix of the other (i.e.,
).  A \emph{universal} prefix-free machine is one that can
simulate all other prefix-free machines. Prefix-free complexity with respect
to a universal prefix-free machine~ is written~, and is
defined in the same way as~.  Similarly,~ is
 for some fixed universal prefix-free machine~.  As before,
the choice of~ can make at most a finite difference.

As a notational convention, we use~ after a term to mean the state of
that term after~ stages.  For instance,  is
the shortest length of any~ such that .

While plain complexity~ at first seems like the most natural way to define
complexity, it lacks some properties that we would expect a complexity
measure to have.  For example, it is not true that there is a constant~
such that .  Thus, to describe the
concatenation of~ and~, we cannot simply provide descriptions
of both strings along with some finite code for concatenation, as we would
expect that we could.  Prefix-free complexity~ satisfies more properties
that we would desire a complexity measure to satisfy.  For instance, there
does exist a constant~ such that .
(For more information, see~\cite[p.~83]{Nies},
or~\cite[p.~121]{Downey-Hirschfeldt}.)

Intuitively, for a string to be random, it should have no description shorter
than its own length.  This leads to the following definitions.  Let~ be a
universal prefix-free machine.  We define the set of random strings with
respect to~ by

For a fixed prefix-free universal machine~, we let .
Similarly, we can define~ and~ using plain complexity and a
standard universal machine~.

Note that while the choice of machine makes only a small difference in
complexity, the sets  and  could potentially be
quite different.  Thus, we cannot talk about a given string~ being
``random'' without specifying a machine.  In this paper, we look at how
different the sets of random strings with respect to different machines can
be.

It is known that both~ and~ are Turing complete, regardless of the
choices of universal machines; see Li and
Vit\'anyi~\cite[Exer\-cise~2.7.7]{LiVitanyi} for details.  (The exercise
states that the set  is Turing
complete, but the proof uses only~.  It is not difficult to extend
the proof to the prefix-free case, as the machine built to compress strings
in the exercise can easily be made to be prefix-free.)  In fact, by the same
argument,~ and~ are always bounded Turing-complete (or,
bT-complete, for short). That is, we can computably find a bound for the use
function in the computation that reduces the halting set to~
or~.\footnote{Note that bT-reducibility is also known as weak
truth-table reducibility, or wtt-reducibility.} Thus, comparing the Turing
degrees or bounded Turing degrees of two sets of random strings will not help
to differentiate them. So we turn instead to truth-table reducibility, the
next finer reducibility.

Truth-table reducibility is a strengthening of Turing reducibility and
bT-reduci\-bi\-lity.  For an arbitrary Turing functional~, there is no
computable way to know for which oracles~ and which input~ 
converges.  There is also no way to know how much of the oracle is needed to
perform a given computation.  For a truth-table reduction (tt-reduction),
this information can be computably known.  There are two standard ways of
defining a truth-table reduction.  One way is as a {\em total Turing}
reduction.  That is, a Turing functional~ is a tt-reduction if for
every oracle ,~ is a total function.  The other way to
define a truth-table reduction is using truth tables.  Each
tt-reduction~ is given by two computable functions,~ and~.  The
value~ gives a number that can be thought of as a bound for the use of
the computation~ for any oracle~. The value~ gives a code
that tells us, for each , what the value of
 is.  Thus, we can think of~ and~ as defining a table
whose rows consist of every string~ of length~ and the
corresponding output .  If  for a
tt-reduction~, we write .

We can effectively list the tt-reductions by also including some reductions
that are not total and are therefore not tt-reductions.  We let
 be a listing of the tt-reductions in the following
way.  Let , where  is the standard Cantor pairing function from  to~. Let~ be the reduction given by the functions~
and~ as above, where  and  (and where
 is the standard listing of the partial computable
functions).  We say that the truth table for~ has been defined
after~ steps if  and  both converge
and  codes the values for the rows of the table given by
.  If either function does not converge or if the functions
cannot be interpreted as giving a truth table, then the truth table is
undefined.

It is not hard to show that  if and only if  via a Turing
reduction that runs in a computably bounded time.  Truth-table reductions are
thus closely connected to computer science.


In our work below, we build on ideas from two beautiful theorems on the
tt-degrees of sets of random strings. The first is about the set of strings
random with respect to plain complexity:

\begin{thm}[Kummer~\cite{Kummer}]\label{KummerThm}
 is truth-table complete.
\end{thm}


\noindent Kummer's theorem does not depend on the choice of universal machine
used to define~.  Thus, every \ce set is contained in , where the intersection is taken over every universal
machine.  This does not hold for the prefix-free case, by the second result:

\begin{thm}[Muchnik~\cite{Muchnik}]\label{MuchnikThm}
There exists a prefix-free universal machine~ such that~ is not
truth-table complete.
\end{thm}

\noindent In fact, in Theorem~\ref{knight-bishop} we show that .  That is, the only sets tt-reducible to
every~ are the computable sets. Our proof relies heavily on ideas
that were introduced in Muchnik's proof, in particular, the idea of playing a
game to force the value of a truth-table reduction. It is worth noting that
the proof of Theorem~\ref{thm:AFG} was also inspired by Muchnik's proof.

Kummer's proof served as the basis of our proof of Theorem~\ref{thmfinite},
where we show that for any finite collection of~'s, there is a
noncomputable \ce set tt-reducible to each~. Essentially, we try to
transfer Kummer's coding method to the prefix-free case. While we cannot
code~, we do find that we can code some noncomputable \ce set.



\section{\texorpdfstring{There is no tt-minimal pair of 's}{There is no tt-minimal pairs of U-random sets}}\label{sec_nominpair}

We first state and prove the following theorem for two~'s and then
generalize it in Theorem~\ref{thmfinite} to the case of finitely
many~'s.

\begin{thm}\label{theorem1}
For any prefix-free universal machines~ and~, there is a
noncomputable \ce set~ such that  and .
\end{thm}

For notational simplicity, let  and  for
.

We use~ as the universal prefix-free machine that gives us the
prefix-free complexity~, so .  We use the
usual correspondence between finite strings and natural numbers to define
 for .  That is,~ is the string corresponding to~
if~ is the binary representation of .
By Chaitin's Counting Theorem~\cite{Chaitin}, there is a constant~ such
that , for each
; that is, the number of length~ strings in~ is bounded
by .

Let~ be the computable function, defined by Solovay in~\cite{Solovay},
with the property that  for all  and such that
 on an infinite set.  There is, however, no infinite \ce set on
which  (\cite{Solovay}, see~\cite[p.~132]{Gacs}).
We will construct an infinite set~ that is truth-table reducible to
both~ and~, and such that if~ is computable, then there is
an infinite \ce set on which .  Thus,~ is not computable,
showing that~ and~ do not form a minimal pair.  Also note that
there is a constant~ such that , so we may assume that
.

We will simultaneously construct two prefix-free machines~ and~.
Using the Recursion Theorem, we may assume that we know in advance that the
coding constants of machine~ with respect to~ are less than some
value~ for each ; that is,  for
all~ and each .
The purpose of the machines will be to compress strings, which will
force~ and~ to compress strings, which will allow us to code
information into~ and~.

As we have said, our proof is inspired by Kummer's proof of
Theorem~\ref{KummerThm}.  The main idea is that we know by Chaitin's Counting
Theorem that the number of nonrandom strings of length~ (with respect to
either~ or~) is less than  for all~ such that
.  We can divide the set
of natural numbers less than
 into  many regions of size , for
all~.  We know there is some maximal such region such that the size of the
set of nonrandom strings of length~ lies in that region, for infinitely
many~ with .
We can code information into~ by waiting until  and
choosing  strings that will be compressed if~ drops
below~.  For almost all~ in the maximal region, these strings will
only be compressed if , because otherwise we would contradict the
maximality of the region.  Thus, we are compressing strings to code
information about which elements~ satisfy .

We construct our machines by enumerating KC (Kraft-Chaitin) sets.  A KC set
is a \ce set of pairs  from
 such that the weight

of the set is at most~.  By the KC Theorem, also known as the Machine
Existence Theorem, a KC set determines a prefix-free machine~ such that
 with  for all .  Thus, any
universal prefix-free machine must also compress~ to length~
plus a coding constant.  (The KC Theorem is due to Levin~\cite{Levin},
Schnorr~\cite{Schnorr}, and Chaitin~\cite{Chaitin}.  See
also~\cite[p.~125]{Downey-Hirschfeldt}.)

To build our KC sets, we first build sets~ such that~ contains
strings of length~, for . We then enumerate  into a KC set for each  to
define machine~. We will construct~ so that if ,
then~ will be empty; and otherwise (i.e., if ) we have
. Thus the weight of our KC sets will
be no more than

Therefore,~ and~ will indeed be prefix-free machines.

In our construction, in addition to building~ and~, we will also
build finitely many \ce sets~.  One of these sets will be our desired
set~, which will be noncomputable and tt-reducible to~ and~.
However, we do not know which of the sets will be the true set~.


Let  for .  Note
that these are the strings of length~ that have been shown to be outside
of~ by stage~.


\subsection{Construction}



{\em Stage }.  Let  for all .


\medskip
{\em Stage , Part~.} For each pair~ with , in
decreasing order (starting from the largest~), check whether there is an
 such that
\begin{enumerate}[label=(\roman*)]
\item  is unused and ,
\item  for any  and any~,
\item ,
\item , and
\item .
\end{enumerate}

\noindent In other words, we check whether , and if so
we try to find the largest pair~ satisfying the above criteria.

If so, then take the least such~ and apply the following steps:

\begin{itemize}
\item Let  be the least~ elements in ,
where

and similarly for .
\item Let .
\item Increment  by~1.
\end{itemize}

{\em Stage , Part~.} If  for some~, we call~ a
{\em candidate} for~. If~ is an unused candidate for any~ and
 (i.e.,~ decreased from~(v) above when~ was made a
candidate), we declare~ to be {\em used} and let 
where~ is the greatest such that~ is a candidate for~ and~ is
such that . Similarly, we define .

When~ becomes used, for each~ such that~ is a candidate for~,
we enumerate  into~, where~ is the stage at
which~ became a candidate for~.

End of construction.

\medskip
Let~ be the machine that compresses each string in~ to length
, for .
As explained previously, these machines are guaranteed to be prefix-free by
the KC Theorem.

To see how we can know~ in advance, first note that by the Recursion
Theorem, we can know indices for the KC sets that we are building in order to
define~ and~.  Since we can effectively go from an index for a KC
set to an index for a machine, by the KC Theorem, we can find indices for the
machines~ and~.  Since~ and~ are universal prefix-free
machines, given indices for~ and~, we can effectively find coding
constants~ and~ such that  for each  and .  Thus, we can know~
and~ in advance, so let .

Note that for almost all~, .  For such~,

Thus, . Let~ be the infinite set of~ such that
 and .  For each , there is some~ such
that conditions (i)-(v) will hold.  Now each  can be a candidate for
each~ at most once, and~ will eventually become a candidate for
some~ since if~ is not already a candidate for some~,~ will
eventually become a candidate for .  Thus, there is some
largest~, which we will call , such that there are infinitely
many elements of~ that become candidates for~.  Note that neither
coordinate of~ can be equal to , as we know that for all
, since , the set of compressible strings of length~ is
strictly less than .

Let .  We will show that~ is tt-reducible to both~
and~ and is not computable.  (We will not use the other sets .)




\subsection{Verification}

\begin{lem}\label{ttlemma}
, for .
\end{lem}

The proof depends on the following two sublemmas.

\begin{slem}
\label{Esublemma} For all~ such that there exists~ with ,
we have that  implies  for some  and some , and that  implies
 for .
\end{slem}

\begin{proof}
In the construction, if ,~ will become used, and~ will
be defined.  Since~ is a candidate for , it must become a
candidate when it is still unused, so by the time it becomes used, the
greatest~ for which~ is a candidate is at least~.
Thus,~ will be defined as  for some . If
, then~ will never become used, and~ will never be
nonempty.
\end{proof}

\begin{slem}\label{containment}
For almost all~, for each ,

where~ is the stage at which  was defined.
\end{slem}

\begin{proof}
 Since ,
we have .  By  Sublemma~\ref{Esublemma},  for
some .   Since all strings in~ are compressed
by~ to length , they are compressed by~ for  to
length less than .    Thus, .
If , we are done, since in that case, .  Otherwise, , so  must have been defined after 
was defined because otherwise condition (ii) of the construction would not
allow  to be defined as~.
Thus,  as well, because anything
in  must have already been seen to be nonrandom
with respect to~ by the time  was defined, since such
strings are in .

 Let~ be such that for all , if 
for some , then~ never becomes a candidate for any .
Let  and . Let  become defined at
stage~. Suppose  for either
 or~. Without loss of generality, assume .
Then at some stage , all strings in  are in .
This means , where~ is the number of strings
in , all of which still appeared random at stage~. Recall
that the number of elements in  was chosen to be the minimum
of  and . In the former case, all strings of
length~ are non-random, which is impossible. So  has size
, and by condition (iii), ,
so . We also have that , so condition (iii) and (iv) will hold for some~ with
. If~ has not yet become used when this happens, all of
conditions (i)-(v) will hold for this~ and~, so~ will eventually
become a candidate for~. Thus, . Therefore, , and
so at some stage in the construction,~ will become used while it is a
candidate for~, and at this point 
will be enumerated into~.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{ttlemma}]
To determine if~ is in~, run the construction to see if 
is defined as~ for some~ at stage~.  If not, then .  If
so, then .  This works for both
 and~ by Sublemma~\ref{containment}.
\end{proof}

\begin{lem}  is not computable.
\end{lem}

\begin{proof}
Suppose~ is computable.  Let

Obviously~ is computable, and so  is computable.  is the set of
all candidate pairs  such that . Let . Then~ is an infinite \ce set such that 
(here~ is infinite by the choice of ). However,
Solovay~\cite{Solovay} showed that~ contains no infinite \ce set, so this
is a contradiction. Thus~ is not computable, proving the theorem.
\end{proof}

This proof can easily be modified to accommodate any finite set of universal
machines by replacing the pairs~ with -tuples.  This gives the
following:

\begin{thm}\label{thmfinite}
For any finite set of prefix-free universal machines 
there is a noncomputable \ce set~ such that  for each
.
\end{thm}

While sets of random strings cannot form a tt-minimal pair, there are Turing
complete sets that do form a tt-minimal pair, as we show in the following
section.

\section{A tt-minimal pair of Turing complete sets}\label{minpair}

In Theorem~\ref{theorem1}, we showed that there is no pair of sets of random
strings  and  that form a minimal pair in the
truth-table degrees, or even in the \ce truth-table degrees. We know that
 is always Turing complete. If no two Turing complete sets ever form
a minimal pair in the tt-degrees, Theorem~\ref{theorem1} would be a trivial
corollary. However, this is not the case, as we show in this section. By a
different method, Degtev~\cite{Degtev} proved that there are Turing complete
\ce sets that form a minimal pair in the \emph{c.e.\ }truth-table degrees. We
produce a minimal pair in the full structure of the tt-degrees.

\begin{thm}\label{minpairthm}
There exist Turing complete c.e.\ sets~ and~ whose tt-degrees form a
minimal pair.
\end{thm}

\begin{proof}
Let  be a computable listing of all partial
truth-table reductions.

Let~ be a Turing complete \ce set with a computable enumeration
 such that if~ enters~ at stage~, then~
enters~ at stage~ for all  not yet in~.  Such a set~
can be constructed using a standard movable marker construction.
We will build~ and~ as well as Turing functionals~
and~, such that  for  2, satisfying the
following requirements for all~:

By Posner's trick, the requirements~ (and the fact
that ) suffice to show that the tt-degrees of~ and~
form a minimal pair, because if , we could
build a single tt-reduction~ such that 
and .

We will build  in stages with uses , for
 2.  In particular, we will treat the use  as a movable
marker.  The marker  sits on an element not yet in~.  We
may change the value of  by enumerating 
into~.  The movement of the markers is subject to the following rules:

\begin{enumerate}
\item If , then .
\item  implies ,
    where by convention~ exceeds all numbers used in computations at
    stage~. We refer to this action as \emph{kicking}. Moreover, when
     is first appointed at the end of stage~, it is
    chosen to be the least element not yet in~ that is greater
    than~ and all other markers .
\item If~ enters~ at~ then we will  enumerate 
    into .
Once  we will no longer define .  The marker
will be removed.
\item If  enters , so do all currently defined
     for all .
\item Coding of~ is not the only reason  can move. The
    marker  may be moved by requirements~ in
    their attempts to seek satisfaction, but~ can only move
     if . As we will show, a single~
    can only move a specific  a finite number of times. If
      enters~ and , it will be
    redefined, and as usual, we will kick  to a fresh
    element past .
\item If  enters~ then one of  or
     must simultaneously enter~ for , where
     is the smallest  with .  That is,
     is the least marker still defined for .
\item If  moves or is enumerated, then~ is
    initialized for , meaning that all current values for 
    are discarded, and the strategies for~ are restarted.
\end{enumerate}

\noindent To achieve~, we will force disagreements at stage~ between
 and  whenever possible by enumerating
 into~ and  into~, where~ and~
are at most one defined marker apart, as specified in Rule~6.  According to
the rules that govern marker movement, we also enumerate all larger markers.

Let  be the length of agreement function given by

If the limit of  is infinity, then we will try to fix the values
of  so that~ is computable.  Given values for , we
will attempt to force a disagreement between  and
 while following the rules of marker movement.  Any
disagreement we force could only be injured finitely often, and we will
eventually either preserve a disagreement or reach a believable computation
for~.

We perform the construction on a tree of strategies. Each height~ will
correspond to the strategy~. Nodes of length~ will be
extended by the three possible outcomes for the strategy~:~,~d (for disagreement), and~w (for waiting), ordered by
 d  w. The~ outcome will correspond to the situation
where . The d outcome will correspond to the
situation where we are preserving a disagreement between  and
. Otherwise, the outcome will be w; this includes the case in
which  is not a total truth-table reduction and our strategy is
eventually stuck waiting for convergence.


We first discuss the basic module for~.  We will then modify this to the
-module by giving a formal construction in Section
\ref{minpairconst}.

\noindent
\subsection{\texorpdfstring{Basic module for }{Basic module for R0}}\label{basic}

The module works in order of~ to give a definition of~.

For :

\begin{itemize}
\item Wait till the first stage~ when . Immediately
    enumerate  into~ for all . This causes
     to be moved past the use of , so that
    enumeration into~ will not affect the computations.  We will say
    that the pair  has been {\em prepared}.
\item Wait until the next stage~ where . At this stage
    there are two possibilities.
\begin{enumerate}
\item Putting  into~, or 
    into~ or both, will cause a disagreement at argument~0.
\item Otherwise. Then define 

\end{enumerate}
\item Suppose we invoke~1. If we put both of the markers into their
    targets~ and~, then the strategy is successful by kicking
    because no markers will ever be defined below the use of 
    and thus our disagreement can never be injured. If we only changed
    one side, say~,  then this will cause a disagreement that holds
    forever, unless at a later stage~,~ enters~. At such a
    stage~ we would enumerate  into~, and noting
    that , this could potentially make the
    computation equal again. We would wait until the next stage 
    where , and define , safe
    in the knowledge that this is now an immutable computation.
\end{itemize}

\noindent Given , we act for :
\begin{itemize}
\item After defining , we wait for the stage~ where
    . We then enumerate all  into~ for
    . As before, this causes  to move past
    , the use of , and we call 
    {\em prepared}.
\item Wait until the next stage~ where . We examine
    the tt-reductions  and allowable enumerations of
     into~ for  below the use,
    , to see if we can cause a disagreement for argument
    . Again by kicking, everything else is too big.  If we can cause
    a disagreement, we will do so with the least possible elements.  To
    be more specific, given , let~ be the greatest
     such that  is defined and let~ be the
    least  such that  is defined. By the rules of
    movement,~ is the greatest number less than~ such that
     and~ is the least number greater than~ such
    that . Let~ be the least element such that we can
    cause a disagreement by enumerating  into~ and
    either , , or 
    into~, as well as all greater markers, according to the rules of
    movement.  We choose the least pairing that causes a disagreement and
    enumerate the appropriate elements.  Again, there are two
    possibilities:
\begin{enumerate}
\item We make such an enumeration to cause a disagreement.
When we implement the tree of strategies, nodes guessing that there
is a disagreement at~ will preserve the disagreement.
\item No such~ exists to cause a disagreement. Then define
    
\end{enumerate}
\end{itemize}

\noindent In case~1, the disagreement at  may be injured.  will not
act when it sees a disagreement, so injury can only occur by elements
entering~. If such elements do enter~, causing an agreement between
 and , we will wait until we see
 and will try again to cause a disagreement. When we cannot,
we will define . In fact, we will not be able to
find a new disagreement because we previously chose the minimal possible
disagreement.



\subsection{Tree of strategies}\label{treeofstrategies}

As mentioned previously, each node on our tree of strategies will be extended
by three possible outcomes:~ d  w, where the ordering is left to
right.  We will build an approximation to the true path through the tree,
which we call~.  We say~ is an {\em -stage} if~
is a prefix of~.  Nodes~ of length~ can act for
 only at -stages.  During such action, any attempt at
defining the function given by  will be
called~.  Whenever~ moves to the left
of~,~ will be initialized, undefining all values
of~.  For~ on the true path, which is  , this will only happen finitely often.

We build an approximation~ to the true path recursively as follows:
Given , we define .
If  is greater than it has been at any previous -stage,
then .
If we have acted at some stage  to cause a disagreement between
 and  and this disagreement has been
preserved, then  d. Otherwise  w, the waiting
outcome. We define~ in this way until we have defined
, so that~ has length~.

We will not allow any disagreements to be injured by nodes extending or to
the right of the ``d'' outcome.  To achieve this, we will only allow each
node  to enumerate elements  for~
greater than or equal to the last stage~ such that the
approximation to the true path was to the left of~.  If, for ,
 d, preserving a disagreement at~, then the last stage~
such that  must have been a stage where the truth-table
for  had already been defined, since we will not act to cause a
disagreement at~ until we first see .  By convention, any
stage at which the truth-table for  has been defined must be
greater than the use , so . Similarly,
if  w, then the last stage such that the true path went
through d or~ was larger than the use , for~ the
last spot where we caused a disagreement.



\subsection{Construction}\label{minpairconst}

{\em Stage .}  Let  and .  Let
, the empty string.  Define  for  2.
Note that we have guaranteed that .

{\em Stage .}

Suppose~ enters~ at stage .  Enumerate  into
 for  2.  We remove the marker , so we will
not define .  Initialize all~ for ,
undefining any values of~ for .

In increasing order of~, for every , do the following:

Let , where outcomes are as described in
Section~\ref{treeofstrategies}.  Let  be the last stage~ such
that~ was to the left of~, or~0 if the approximation to the
true path has never been to the left of~.  Let~ be the greatest
such that  is defined, or  if there is no such~.

{\em Step 1: Preparing .} If  has never before been prepared, and  for
the first time since defining , enumerate all
 into~ for all  satisfying .  We say the we have now {\em prepared} the pair .  Move each  (that is still defined), in
increasing order of~, to the next fresh spot greater than 
according to the rules of motion.  This will prevent 
from influencing  since it has been kicked past
. Initialize all~, for , as in
Rule~7.  Note that we will call these newly kicked markers
 until the end of the stage, where all markers will be
renamed to .  If we prepared some pair in this step,
begin the steps for .  Otherwise, go to Step~2.

{\em Step 2: Searching for a disagreement.}  If , we will
attempt to cause a disagreement at .  Let~ and~ be as defined
in the basic module in Section \ref{basic}.  Let 
be the least element such that we can cause a disagreement between
 and  by enumerating 
into~ and either  or  into~, as well as all larger markers.  (Of course, we do not consider
enumerating  unless .)
We choose the least pairing
that causes a disagreement and enumerate the pair and all larger markers into
the corresponding~.  We move all enumerated markers to the next fresh
spots greater than .  If we were unable to cause a disagreement, we
define .

Add a new marker  to the first fresh spot greater than
.  Note that this  will be greater than the current
(and former) locations of all other markers.
For any node~ to the right of , initialize~ by
undefining all values of~.

\subsection{Verification}\label{minpairverif}

Let the true path of the construction be .

\begin{lem}\label{finitemovement}
Each marker moves finitely often.  That is, for  and ,
there are finitely many stages~ such that .
\end{lem}

\begin{proof}
Induct on~.  Suppose the lemma is true for all  and .
We will show it holds for  as well.  If , then when  is
enumerated into~,  is enumerated into~ and the
marker is not redefined.  Thus  moves finitely often.  So we
will assume that .

According to Rule~5,~ can only move  if .  Thus it is enough to show that none of these~ moves
 infinitely often.  Suppose for a contradiction that~ moves  infinitely often and that~ is the least such
that this happens for either~.  There are two ways~ could
move .  First, by preparing  for some
 as in Step~1 of the Construction.  Since each pair  can only be prepared once, this can only happen finitely often.



The other way that~ can move  is by action of
Step~2 in the Construction, causing a disagreement.  By induction, there is a
stage~ after which no markers  ever move or are removed
for .  By the minimality of~, there is a stage~ after which
no~ moves  for any . By the previous
paragraph, there is a stage~ such that Step~1 of~ has
stopped moving  by stage~.  Let~ be the node of
length~ on the true path.  Let~ be a stage by which~ never
goes to the left of~ after stage~.  Finally, let , and~.

Note that since~ acts infinitely often by moving
, it must do so at infinitely many -stages,
for~ the length~ node on the true path.  This is because
each~ to the right of~ can only move elements greater than
the last stage at which they were initialized, and they will be initialized
infinitely often since they are not on the true path.

Now suppose that at some -stage ,~ acts by
enumerating  to cause a disagreement between
 and  for some~.  By assumption,
 will eventually act again at an -stage by enumerating
 to cause a disagreement between  and
 for some .  This means that at some stage ,
, so the disagreement achieved at stage~ will be injured.

We must examine how such an injury could happen.  Since the disagreement was
caused by enumerating , we also must have enumerated
\mbox{,} for , by Rule~6.  If we also
enumerated  itself, then injury would be impossible since
the only markers still below the use of  have stopped moving by
stage~.  Thus, we must not have enumerated  and
instead enumerated the marker succeeding it.  The only way the computation
can be injured is for  to be enumerated. This cannot be
enumerated by any higher priority~ or any~ to the left
of~, by the choice of~ and~.  It also cannot be enumerated
by any node to the right of~ because such a node will not be able to
move elements smaller than the last -stage, which must have been
bigger than the use of .  Any node extending d or
w must also preserve the disagreement because~ must
have been extended by~ at some stage after the truth-table for
 was defined, and nodes cannot move elements smaller than the
last stage at which they were initialized.  In addition, we may ignore any
node extending  because we would not go to that outcome
unless the disagreement in question had already been injured.  Thus, there is
no way for the disagreement to be injured, and~ will never act
again at an -stage, contradicting our assumption that it would act
infinitely often.

Since~ cannot move  infinitely often for
any~, we can see that  can only move finitely often.
Thus, by induction, each marker only moves finitely often.
\end{proof}

\begin{lem}
.
\end{lem}

\begin{proof}
For  or~, to compute~, run the construction until the first
stage  such that either  or .  Such a stage exists because
 can only move finitely often.  Now  if and only if
.  This is because, when~ enters~,  is
enumerated into~ before it is moved.
\end{proof}

\begin{lem}\label{Satisfied}
Requirement~ is satisfied for each .  That is, if
there is a total function~ such that \mbox{,}
then~ is computable.
\end{lem}

\begin{proof}
Suppose  total.  Then .
Let~ be the node of length~ on the true path.   We will show that
along the true path, for almost all~, .

Let~ be the greatest stage such that  is to the
left of~.  Then by the construction, after
stage~,~ will not be allowed to enumerate any
 for .  Let  be a stage such
that  has stopped moving by stage~ for all .  After this stage, the~ that we are building
will be the final~. Let~ be the greatest~ such that
 was defined before stage~ for the final~.
We will show that for , .


Suppose  for some .  Choose the
least such~.  Suppose  is defined at stage~.  After
stage~, some element enters~ or~ below the use .  At
some prior stage~,  was prepared as in Step~1 of the
Construction, kicking all  for  past~, which is
greater than .  Thus no  for  could enter
either~ below  for .  Therefore, any injury to the
current values of the  must be caused by some
 entering~ at stage  for either~, where~
satisfies  and .
Such  are the only markers that both would be allowed to
enter~ and would be able to cause injury.

\begin{clm}\label{cl:dis}
If we can cause a disagreement between  and
 at stage , then we could have caused a
disagreement at stage~ instead of defining~.
\end{clm}

\begin{proof}
Suppose enumerating  and  as well as all
greater markers, causes a disagreement between  and
, where  and .  Note that at least one of
 and  must be below the use of ,
hence less than~.  Any marker that is at a position less than~
at stage~ will have been at the same position at stage~, because no
markers are moved or added to numbers below~ at or after stage~.

\smallskip
{\em Case 1:} .  If both markers  and 
are in the same spots as  and , then the same
enumeration could have been made to cause a disagreement instead of defining
.  Suppose .  Then since one
marker must be below the use, .  Between
defining~ and stage~,  moved, but since
 didn't move,  couldn't have moved, by
Rule~6.  Thus, enumerating  and  at stage~
will give the same disagreement caused by enumerating  for
both , so we would have made this enumeration instead of defining
 at stage~.

\smallskip
{\em Case 2:}  at both stage~ and stage~.  As in Case~1, if
both markers are at the same numbers at stage~ as they were at stage~,
then the same enumeration could have been made instead of
defining~. It is not possible that , because any movement would have forced  to
move as well, by Rule~6, pushing it past the use.  Suppose  and   Then the least element
that was enumerated into~ and moved after stage~ was either
 or .  Thus, at stage~, we could enumerate
the appropriate one of  or  along with
 to cause the same disagreement instead of defining
.

\smallskip
{\em Case 3:}  at stage~, but not at stage~.  Then between
stage~ and stage~, some elements~,  entered~.  For the
least such~,  at stage~, so we could have enumerated
 and  to cause the same disagreement at
stage~ instead of defining .





Thus, any disagreement we could cause after defining  could have
been caused {\em instead} of defining .
\end{proof}

According to Claim~\ref{cl:dis}, in order to cause an injury to the agreement
between ,  and , the
enumeration must have caused a change in both  and
 to cause a new agreement between them that differs from
.  Consider the greatest possible enumeration that would have
caused such a change. Suppose that the least elements of the greatest
enumeration are  and  for  and
.  Then  and  would also be an
allowed enumeration. It could not be true that under such an enumeration,
, as this would contradict that the pair
 gave the greatest possible enumeration that changed
both computations to cause agreement again.  Thus, under this new
enumeration, a disagreement is caused between the two computations.  This is
impossible, since the existence of such a disagreement would have led to us
forcing the disagreement instead of defining , as shown in
Claim~\ref{cl:dis}.  Thus, there can be no greatest enumeration to cause a
change in values of , so the values will not change,
and~ was correct.  Since~ is a computable function, so is
.
\end{proof}

This concludes the proof of Theorem~\ref{minpairthm}.
\end{proof}

Degtev~\cite{Degtev} and Marchenkov~\cite{Marchenkov} showed there is a \ce
tt-degree minimal among the tt-degrees; that is, there is a \ce set~ such
that for all~ such that , ~is computable.  However, all
such tt-degrees are low, as shown by Downey and Shore~\cite{DS}. Thus,
there is no Turing complete \ce set of minimal tt-degree.

Our theorem cannot be extended to show the existence of a minimal pair of
Turing complete \ce sets within the bT-degrees (also known as wtt-degrees) by
the following

\begin{thm}[Ambos-Spies~\cite{Ambos-Spies}]
A \ce set is half of a minimal pair in the Turing degrees if and only if it
is half of a minimal pair in the bounded-Turing degrees.
\end{thm}

Thus, no \ce Turing complete set is half of a minimal pair in the bT-degrees.
In contrast, our Theorem~\ref{minpairthm} shows that not only can a \ce
Turing complete set be half of a minimal pair in the tt-degrees, but the
other half of the minimal pair may also be a \ce Turing complete set.

\begin{qu}
Is there a truth-table minimal pair of bT-complete \ce sets?
\end{qu}

If this question has a negative solution, then Theorem~\ref{theorem1} would
follow, since sets of random strings are always bT-complete.

\begin{qu}
Which Turing degrees contain minimal pairs of (c.e.)\ tt-degrees?
\end{qu}

Jockusch~\cite{Jockusch} showed that the hyperimmune-free degrees coincide
with the Turing degrees that contain a single tt-degree; therefore, such
degrees cannot contain a minimal pair of tt-degrees. Jockusch also showed
that if a Turing degree contains more than one tt-degree, it contains an
infinite chain of tt-degrees. It is not known which of the hyperimmune
degrees, apart from~, contain a minimal pair of tt-degrees. Not all do:
Kobzev~\cite{Kobzev} proved that there is a noncomputable c.e.\ set  such
that if , then .\footnote{In particular,
Kobzev showed this for any noncomputable, semicomputable, -maximal set
. We thank one of the anonymous referees for pointing us to this result.}
In other words, the tt-degree of  is least among all the tt-degrees in the
Turing degree of , so the Turing degree of  does not contain a minimal
pair of tt-degrees. The  that Kobzev constructed actually has minimal
tt-degree, hence must be low~\cite{DS}.

\section{\texorpdfstring{No noncomputable set is tt-reducible to every
}{No noncomputable set is tt-reducible to every U-random set}}
\label{sec_X}

We have seen in Theorem~\ref{thmfinite} that given a finite collection of
sets of random strings , there is a
noncomputable \ce set tt-reducible to each .  It is natural to
ask if there is in fact a noncomputable (and perhaps also c.e.)\ set
tt-reducible to every~.  We show that there is no such set.

\begin{thm}\label{knight-bishop}
Given any noncomputable set~, there is a universal prefix-free
machine~ such that~ is not truth-table reducible to~; that
is, there is no common noncomputable information tt-below every~.
\end{thm}

Note that this theorem is in contrast to the non-prefix-free case, since
every~ is tt-complete.

\begin{proof}We begin by giving a sketch of the construction. We will construct three
different prefix-free universal machines , and guarantee
that they cannot all tt-compute~. For convenience of notation, we denote
the corresponding 's by~,~, and~. At the moment, we
do not know whether this non-uniformity is necessary in the proof.

Since every~ is~, we need only consider
-sets~.  Let  be a listing of
partial tt-reductions.  We will meet the following requirements for all~:

By Posner's trick, this is enough to show that~ is not tt-reducible to all
three sets, as if it were, we could build a single tt-reduction~ such
that  for each .  To satisfy
requirement~, either~ will not be a total
tt-reduction, or we will force one of the following to hold:

\begin{enumerate}[label=(\roman*)]
\item  for some  and
some , or
\item  for some .
\end{enumerate}
The way we will achieve this is to build the machines in such a way that if
condition~(i) fails, then the set  must be computable, so it
cannot be~.

In order to make these machines universal, we fix a universal prefix-free
machine~ and simply require that  for
each . We consider this coding requirement as our \emph{opponent}
controlling~ of the total measure, and the diagonalization requirement
as ``we'', the other player controlling the remaining~ of the game board
(machines we build). Here the number of~'s is picked so that we have
some amount of space bigger than our opponent as needed in the verification
process.

\subsection{A single requirement \texorpdfstring{}{R0}}

We first consider how to satisfy only one requirement~.



\subsubsection{One-bit game}\label{onebit}
We begin by considering only one bit,~0; that is, we are looking only at the
first bit of the first tt-reduction.  We wait until the truth table of
 is defined.  Since~ may not be a total tt-reduction, this
may never occur.   Before this happens, we do nothing for this
requirement~. Once we have the truth table for~, we
can attempt to satisfy this requirement.

We modify the games used in Muchnik's proof that there is a universal
prefix-free machine~ such that~ is not tt-complete.  For the
moment, we define the game `` on~'' as follows. We
imagine that the game board is the truth table of  and that our
starting position on the game board is the current state of~.  The game
 is the game where the opponent (the coding requirement)
has~ measure to use, and we have~ measure to use to
enumerate strings (to change~).  We are building KC sets as defined in
Theorem~\ref{theorem1} to construct the~'s so that they will indeed be
prefix-free machines, so we must keep the weight of the sets below~1.
Since~ is the set of strings that are random with respect to~, we
change~ by compressing strings.  Each move consists of a player (the
opponent or us) compressing any number of strings, which may change bits
of~ from~1's to~0's.

When , i.e., when the game is symmetric, we always have a
winning strategy for forcing  to be either~ or~ for
each~.  We call the value being forced the \emph{value of game
 on~}.  Note that we can computably determine the
value of the game since the game is finite and has only finitely many
sequences of play.

For now fix a small~ (we call this~ the
\emph{starting measure} of the requirement~). If for some , the values of the games  played on~
and~ are different, i.e., we have strategies that can force
, then for the least such pair , we
use the strategies for both and play the games with the opponent.

There are two possible outcomes of this dual game. First, if the opponent
never uses more than~ measure (i.e., he does not cheat in the
game), then we satisfy the requirement~ in finitely many
stages by forcing a disagreement between  and .
If the opponent uses more than~ measure in the play, then we
simply reset the game.  Note that in this situation the opponent uses more
measure than we do.  In the end, he can only cheat by using over~
measure finitely often, since his total measure is bounded by~.

In the case where we cannot find such a pair , we know that for the
games  played on each set , the values
have to be the same. Now reduce the measure and consider the games
 on each set and compare the values to the
values given in the original games.

We first deal with the scenario when there exist  such that
 on~ and 
on~ have different values. In this case, we play both games at the same
time, forcing the values to be different. If the opponent does not cheat,
then we have a permanent win. If the opponent cheats, we will do a modified
game analysis (see \S~\ref{bishop}) to resettle agreement on the games.

The remaining case is that these two levels of games,~ and
~, on all three sets, all have the same value. In this case, we
continue to look at the next level~, then~, and
so on.  We call this sequence of games the \emph{stack of games} for the
first bit.  If we find a game, say
, on~ that has a different
value from all previous games, then we play that game simultaneously with the
game  on~ for the least .
It will be important to always choose the second game from the previous level
and not from another earlier level, so that if the opponent cheats in the
game , we will know that~
has only used at most twice the measure that the opponent used.

The tt-reduction has been fixed, so eventually we reach a small enough
measure so that the game is actually the -game, i.e., no one can enumerate
anything to change the tt-reduction, or the ``game board''. In this case, the
game is already determined by the current value of the tt-reduction, and in
such a case, we check if the current .  Note that
since we were unable to force a disagreement, this value will be the same for
each .  If , then we stop considering
this requirement~ since the requirement seems to be satisfied.
If~ changes value later we will continue the construction. If the two
values agree, then we move on to consider , i.e., the first two
bits of~ (see \S~\ref{multiple bits}).  We will show that this
process cannot continue forever, else~ would be computable, so we will
eventually satisfy requirement .


\subsubsection{``Knight and Bishop'' strategy}\label{bishop}

Now we discuss how to handle the scenario when the opponent cheats in an
intermediate level game, e.g.,  on~ and
 on~ (other cases are analogous). Note that
whenever his cheat amount is greater than~, we can always reset
the whole stack of games, as we know we will only have to do this finitely
often.

Now if the measure used by the opponent does not exceed~ but
exceeds the amount he is allowed to use in either game, i.e., he cheats, then
we reset the games on~ and~, and consider brand-new games
 on these two sets (with the current game boards).
On~, note that the opponent has used the same amount of measure, as his
actions on these three game boards are identical, but we haven't done
anything.  Consider the game  on~,
where~ is the amount the opponent has already used.

This modified game is not symmetric, but it is easy to see that we can force
the same value here as the value we could force for
 on~ when we started playing
 on~ and  on~. The reason
is that we have not yet made any move on~ since, and so we may regard
all of the opponent's actions since as the first move of his play, and we can
simply use the same winning strategy to force the same value. Note that our
winning strategy did not depend on the turn order of the game, as each player
is only capable of changing~1's to~0's in~, so turn order is not
important and we may allow that the opponent plays first. In the construction
in \S\ref{const}, the opponent is always given the opportunity to play first.

Now if the new games  on~ or~ have
different values from the modified game on~, then we can play the new
game on~ (or~) and the modified game on~ to force a
difference. If the opponent again cheats, then together with the amount he
already used before, he must have exceeded his allowed measure~,
and so we can reset the whole stack of games.

If these games all have the same value, we have reset the agreement on~
and~ for the first bit, and the new value being forced is the same as
the old value (before cheating). Now we consider the game
 on~, which could have a new value as the game
board has changed since we previously considered this game. This goes back to
the original set-up of symmetric games at the~ level, and so we
can continue the construction.  Note that the opponent can cheat only
finitely often because the stack of games is finite, so there is some minimal
measure  that the opponent must have used in order to
cheat, and the opponent's total measure is bounded by~.

In the above discussion, we can think of~ and~ as knights who have
gone off to fight a battle.  Their opponent has cheated and they return home.
The bishop,~, is waiting for them and restores their faith when they
return.  If the three new games  all force the same
value, it will be the same value as before.  We will use this in the
verification to show that if there is no disagreement between the three
tt-reductions, then the set they are computing must be computable and so it
cannot be~.  The idea is that if by stage~, the opponent has stopped
exceeding the~ limit, then any time after stage~ that the
values of the games  agree, this value will always
be the same.  To see this more clearly, we first must discuss the
multiple-bit game.

\subsubsection{Multiple bits}\label{multiple bits}

For the one-bit game as above, once we have a stack of games
from~ to~ (remember that a sufficiently small game is already
the -game, where neither player can change the game board), then we check
whether the value  we have forced agrees with the current
. If not, then we stop considering the requirement~;
if so, we continue to look at the -bit game and similarly build such a
stack of games. So now, by induction, let us consider an -bit game, i.e.,
we consider the first~ many bits of~.
An -bit game  on~ is defined similarly to the
one-bit game.  The game board is now the set of all truth tables for the
first~ bits of~, which may be thought of as one large truth table.
The starting position is again the current state of~.

Again we wait until the truth tables for each of the  bits have been
determined. Now the situation is slightly more complicated. Consider the
first game board~. Given a set , when we play the game
 on~, by symmetry we have a winning strategy
for either~ or its complement~; that is, we can force the
sequence of values of the first~ bits of  to be in
either~ or its complement. If we have a winning strategy for~, then we
call~ a {\em winning set}. The collection of all such winning sets gives
us a collection of subsets of~. If two games~ and~ do not have
the same collection of winning sets, then there is an~ such that we have
winning strategies for~ on~ and~ on~, both for the
game . Then we can simply start using the
strategies to play the game with the opponent, and the requirement is
satisfied unless the opponent cheats by using more than~ amount
of measure, in which case we reset the whole game board. Thus, if we cannot
start playing a game to cause a disagreement between  and
, then we may assume that all three sets~,~,
and~ have the same collection of winning sets.

We may also assume that the collection of winning sets forms an ultrafilter.
We have already mentioned that for any , either~ or its
complement is a winning set.  It is also easy to see that if 
and~ is a winning set, then so is~.  It is left to show that the
intersection of two winning sets~ and~ is also a winning set.  For a
contradiction let us assume that~ and~ are winning sets while 
is not. Then we know that  is a winning set.  But then we
can simultaneously play three games using the winning strategies of~
on~,~ on~ and  on~, causing a
disagreement between  and  for some~ and~
as long as the opponent does not cheat, in which case we reset the whole game
board.  Thus, if we cannot start playing a game to force a difference in this
way, then the collection of winning sets must be closed under intersection.

In a finite Boolean algebra such as the collection of subsets of~, every
ultrafilter is principal, so the collection of winning sets is generated by a
single . Thus, this singleton set  is itself a
winning set for the games  on these three sets.

Note that this  is compatible with the  we
found at the last step when we considered -bit games.  This is because
the set of both extensions of~ of length~ forms a winning set,
so~ must be an extension of~.


Now the construction proceeds in a similar way as in the one-bit game. We
consider the next level . If any of the three
games has the complement of  as a winning set, then we can play
the corresponding games to force a difference; for example,
 on~ using the strategy for  and
 on~ using the strategy for
. If the opponent cheats, then we will handle it in
the same way as in \S~\ref{bishop}, using the third set to resettle
agreement. We will see in the verification that if we have reached a
stage~ by which the opponent has stopped cheating by
exceeding~, then if there is an agreement between all three
tt-reductions, the first~ bits of the set they compute can be determined
by stage~. Since this does not depend on~, they would compute~,
which we assumed to be noncomputable.

This finishes the induction step and the analysis of a single
requirement~.

\subsection{Multiple requirements}

To handle multiple~-requirements, we follow one simple rule:
Whenever a higher-priority game acts, then we reset all lower-priority games
and reset their starting measure~ to be a new small number so
that any game playing with that measure will not change the game board for
higher-priority games.  The possible actions of the higher-priority game that
will lead to resetting the lower-priority games include convergence of a
tt-reduction so that a new relevant truth table is defined,
examining new games, and making a move in a game (as defined formally in
Remark~\ref{rem-acted}).

\subsection{Construction}\label{const}
Let  be a computable approximation of the
-set~.

We construct~ by building KC sets~ for each .  Given a
universal prefix-free machine~ and its corresponding \ce KC set~, the
opponent enumerates  into each~ whenever
 is enumerated into~.  In our construction,
whenever we want to enumerate additional elements into~ for the purpose
of our games, it will be to make a particular string~ be nonrandom with
respect to~; that is, the only move we can make is to enumerate some
 into~ with  so that 
changes from a~1 to a~0.  Therefore, whenever we (and not the opponent)
enumerate elements into~, the element may be assumed to be of the form
; so when we say that we are enumerating a
string~ into~, we are actually enumerating .

Begin with  for all , and with all~
undefined.

Stage , .  We will act for
requirement~ if able.  We call all stages of the form  {\em i-stages}.

First we allow the opponent to make any enumerations into  for
 as elements enter .

\medskip
{\emph Case .} Either  or for some ,  has acted
since the last -stage.

We must define a new~.  Reset any previous value of~
and define the new value of~ to be the greatest number of the
form~ where  such that  for all
previously defined values of~ for any , and such
that no element in any currently defined truth table for~ (for )
corresponds to a string of length greater than~.  This will ensure that we
do not add too much measure to~ and that games for lower-priority
requirements do not alter the game boards of games for higher-priority
requirements.  After defining~, go to the next stage.

\medskip
{\emph Case .}  has not acted for any  since the last
-stage,~ is currently defined, and we are not currently
playing any games for~.

Check if the truth table of  has converged after~ steps.  If
not, go to the next stage.  If so, we examine the one-bit games.  Make a
stack of games as described in \S\ref{onebit} and check if there are  and corresponding games in the stack so that we can force a disagreement.
If so, we begin to play the appropriate game.  We give the opponent the
opportunity to move first, which is to say that we will not make a move at
this stage.

If all games throughout the one-bit stack agree, then we ask if they agree
with   If not, then go to the next stage.  If so, then we move on to
two bits, and so on.  When we get to~ bits, we check if the truth tables
for  have been defined after~ steps.  If not, go to the next
stage.  If so, we determine the winning sets for the games  for  for each .

If the collections of winning sets differ on~ and~ for some
\mbox{}, then we choose an  such that~ is a
winning set for~ and~ is a winning set for~ and we
begin to play the games using these strategies. As before, we go to the next
stage, allowing the opponent to move first.

If the collections of winning sets are the same for all the~'s, we then
check if there are any winning sets~ and~ such that their intersection
is not a winning set.  If so, we begin to play three games, corresponding to
the strategies for~,~, and .  We go to the next
stage, as usual.

In the remaining situation, the collection of winning sets forms an
ultrafilter generated by some~.
We can examine the stack of games to see if any of the 
games have~ as a winning set.  If so, we begin to play
the appropriate games to force a disagreement and move to the next stage.
Otherwise, all games in the stack have~ as a winning set, so we
ask if .  If not, the requirement is temporarily satisfied
and we go to the next stage.  If , we must consider the
-bit situation.

(Note that Case~2 also encompasses the situation where we are simply waiting
for either a truth table to be defined or for~ to change so that it
agrees with the current tt-reduction.  Thus, the steps of checking the stacks
and finding~, for example, may be repeated unnecessarily in this
construction.)

\medskip
{\em Case .}  has not acted for any  since stage the
last -stage,~ is currently defined, and we have already begun
playing games.
Check if the opponent has cheated by enumerating more than his allowed value
in a game.

\smallskip
{\em Case a.} The opponent has not cheated. For each  such that we
are playing a game on~, we follow our designated strategy, which entails
enumerating some set of strings into .  We then go to the next
stage, allowing the opponent to play.



\smallskip
{\em Case b.} The opponent has cheated by exceeding  in
the game -bit game  for .  If this happens,
then we were playing two games, on, for example,~ and~.  We apply
the knight and bishop strategy of \S\ref{bishop}.  We ask if either new
-bit game  on~ and~ has a winning
strategy that could cause a disagreement with the game
 on~, where~ is the amount
used by the opponent since we started the game in which he cheated.  If so,
then we begin to play the appropriate games and move to the next stage.  If
not, then we simply move to the next stage.  (Note that in this situation, if
Case~1 does not apply, then Case~2 will apply and we will once again be
considering the games .  On~ and~, these
games will have the same winning sets through~ bits as they previously
had, because the ``bishop''~ has brought them back to their old values.)

\smallskip
{\em Case c.} The opponent has cheated by exceeding~ or by
exceeding  in an unbalanced game, as described in Case
3b. Stop playing the games and proceed exactly as in Case~2. The game boards
will have changed since the last time we performed the steps of Case~2.

\begin{rem}\label{rem-acted}
We say the requirement~ has ``acted'' at stage~, thus
causing Case~1 to apply at the next -stages for , if any of the
following occur:
\begin{enumerate}[label=(\roman*)]
\item Case 1 applies,
\item We examine games corresponding to a previously unexamined
    truth table (either by a truth table becoming defined or by moving to
    an additional bit),
\item We begin a new game, or
\item Case 3 applies and either we or the opponent makes a nonempty
    move in a game.
\end{enumerate}
\end{rem}


\subsection{Verification}

\begin{lem}
Every requirement~ eventually stops acting and is satisfied.
\end{lem}

\begin{proof}
We follow a standard finite-injury argument. Induct on~. Assume that for
all ,  has stopped acting by stage~. Thus, the
starting measure~ also settles down. Since the opponent cannot
exceed measure~, he will only cheat by exceeding~ finitely
many times. Let  be a stage after which the opponent never uses more
than~ measure that affects the -games.

Starting from stage~ and the one-bit game, we can always assume that the
tt-reduction converges to define a truth table, since otherwise we have an
automatic satisfaction and the requirement stops acting when it is waiting
for the tt-reduction to converge.

In addition, we can assume that starting from stage~, we never start
playing any~ measure games with the opponent for~,
since otherwise we have a permanent win as the opponent can no longer cheat,
and the requirement~ will eventually stop acting. Furthermore,
any other game started for~ must end in the opponent cheating,
else we would get a permanent win.

Assume for a contradiction that requirement~ is not satisfied.
Then for each , we can establish the stacks of games
from~ to the  game for every -bit game.  Note that as
described in \S\ref{bishop}, we may resettle agreement after intermediate
level cheating.  We can see that the set~ is going to be computable since
the~'s as in the construction have to be initial segments of~ in
order for the game to continue forever.  The purpose of the knight and bishop
argument in \S\ref{bishop} was to ensure that intermediate level cheating
could not alter the agreed-upon value of~, so we need only find the
first such~ after stage~ to know that~ is an initial
segment of~.  However, we know that~ is in fact not computable, so
there must be some~ such that  is going to be different from
, where~ is agreed upon, hence must be an initial segment
of the tt-reductions from each set~,~ and~.  When this 
settles down in its -approximation and we see that it differs
from , the requirement~ stops acting (possibly
after playing several more games to establish the current stack) and is
permanently satisfied.

In any of the ways in which~ can be satisfied, the requirement
stops acting after some finite stage. Thus, the induction can continue.
\end{proof}

\begin{lem}
In our construction of~ for , we do not exceed the measure we
are allowed to use, namely,~.  Thus, the~ are each universal
prefix-free machines.
\end{lem}

\begin{proof}
There are three portions of the measure usage. The first is the measure we
use for diagonalization with which we actually have a permanent win in the
end (the ``useful'' measure). The second is the measure we waste when we
reset games when higher-priority requirements act (the ``wasted'' measure);
the third is the measure we lose when the opponent cheats in the games (the
``lost'' measure). It is easy to see that, since each time we reset the
starting measure for a requirement, we pick a new starting
measure~ which can be arbitrarily small, the total amount of the
first and the second portions is easily bounded (by, for example,~). In
the construction, our choice of~ led to each~ contributing no
more than  to the measure, so the total amount contributed by
all~ is at most~.

For the third portion, we can compare the amount of lost measure to the
amount of measure the opponent uses.  When the opponent cheats, then we use
less than twice the measure the opponent uses.  To see this, note that there
are only two situations where we can use more measure than the opponent uses
in cheating.  One is when there are two games being played simultaneously,
and one is a  game while the other is a
 game.  The opponent can cheat by exceeding~,
while we may enumerate up to~ measure for~.  Thus, we enumerate
less than twice what the opponent enumerates.  The other situation is when
the opponent cheated previously by enumerating~ measure, which led
to us playing a  game on~ along with a
 game on~.  In the game on~,
since we enumerated nothing in the previous game, if the opponent cheats now,
his total measure in the two games will exceed~, while ours
for~ will not.  For~, we may have enumerated strings into~ in
both this game and the previous game.  However, our total for the two games
will not exceed 2, or twice the opponent's measure.  Thus, the
lost measure is bounded by~, which is twice the opponent's measure.

Note that we are not double-counting the opponent's moves when accounting for
lost measure. In particular, if , then no move in an
-game can affect an -game (by the choice of
). On the other hand, any move in an -game is
counted as an  action, so it resents any current
 game. This means that an opponent's move is only counted in
one game on~, for each .

Finally,  bounds the total amount of measure we use in
the construction, which therefore does not exceed the amount we are
allowed to use, namely,~.
\end{proof}

This concludes the proof of Theorem~\ref{knight-bishop}.
\end{proof}


\begin{thebibliography}{99}

\bibitem{allender}
  Eric W. Allender,
  Curiouser and curiouser: the link between incompressibility and complexity.
  {\em How the World Computes: Turing Centenary Conference and
  8th Conference on Computability in Europe, CiE 2012, Cambridge,
  UK, June 18-23, 2012. Proceedings},
  volume 7318 of {\em Lecture Notes in Computer Science}.
  Springer-Verlag, Berlin (2012), pp. 11--16.

\bibitem{ABK:06}
  Eric Allender, Harry Buhrman, and Michal Kouck{\'y}.
  What can be efficiently reduced to the {K}olmogorov-random strings?
  {\em Annals of Pure and Applied Logic}, {\bf 138} (2006) no. 1--3 pp. 2--19.

\bibitem{ABKMR:06}
  Eric Allender, Harry Buhrman, Michal Kouck{\'y}, Dieter van Melkebeek, and
  Detlef Ronneburger.
  Power from random strings.
  {\em SIAM Journal on Computing}, {\bf 35} (2006) no. 6, pp. 1467--1493.

\bibitem{AFG}
  Eric W. Allender, Luke B. Friedman, and William I. Gasarch.
  Limits on the computational power of random strings.
  {\em Information and Computation}, {\bf 222} (2013), pp. 80--92.

\bibitem{Ambos-Spies}
  Klaus Ambos-Spies.
  Cupping and noncapping in the r.e.\ weak truth table and Turing degrees.
  {\em Archiv f\"ur mathematische Logik und Grundlagenforschung},
  {\bf 25} (1985) no. 3--4, pp. 109--126.

\bibitem{BFKL:10}
  Harry Buhrman, Lance Fortnow, Michal Kouck{\'y} and Bruno Loff.
  Derandomizing from random strings.
  In {\em 25th {A}nnual {IEEE} {C}onference on {C}omputational
  {C}omplexity---{CCC} 2010}. IEEE Computer Soc., Los Alamitos,
  CA (2010), pp. 58--63.

\bibitem{Chaitin}
  Gregory J. Chaitin.
  A theory of program size formally identical to information theory.
  {\em Journal of the Association for Computing Machinery},
  {\bf 22} (1975), pp. 329--340.

\bibitem{Degtev}
  Alexander N. Degtev.
  - and -degrees,
  {\em Algebra i Logika} {\bf 12} (1973), pp. 143--161,
  transl. {\bf 12} (1973) pp. 78--89.

\bibitem{Downey-Hirschfeldt}
  Rodney G. Downey and Denis R. Hirschfeldt.
  {\em Algorithmic Randomness and Complexity.}
  {\em Theory and Applications of Computability.}
  Springer, New York (2010).

\bibitem{DS}
  Rodney G. Downey and Richard A. Shore.
  Degree-theoretic definitions of the low recursively enumerable degrees.
  {\em Journal of Symbolic Logic} {\bf 60} (1995) pp. 727--756.

\bibitem{Gacs}
  P\'eter G\'acs.
  Lecture notes on descriptional complexity and randomness.
  Boston University, 1993-2005.
  Available at http://www.cs.bu.edu/faculty/gacs/recent-publ.html.

\bibitem{Jockusch}
  Carl G. Jockusch, Jr.
  Relationships between reducibilities.
  {\em Transactions of the American Mathematical Society},
  {\bf 142} (1969), pp. 229--237.

\bibitem{Kobzev}
  G. N. Kobzev.
  On -degrees of recursively enumerable Turing degrees.
  {\em Mat. Sborn.} {\bf 106} (1978), pp. 507--514,
  transl. {\bf 35} (1979) pp. 173--180.

\bibitem{Kummer}
  Martin Kummer.
  On the complexity of random strings.
  In C. Puech and R. Reischuk, editors, {\em STACS '96.
  Proceedings of the 13th Annual Symposium on Theoretical Aspects of
  Computer Science held in Grenoble, Feb 22-24, 1996},
  volume 1046 of {\em Lecture Notes in Computer Science}.
  Springer-Verlag, Berlin (1996), pp. 25--36.

\bibitem{Levin}
  Leonid A. Levin.
  {\em Some Theorems on the Algorithmic Approach to Probability Theory and
  Information Theory.}  Dissertation in Mathematics, Moscow University (1971).
  In Russian.

\bibitem{LiVitanyi}
  Ming Li and Paul M. B. Vit\'anyi.
  {\em An Introduction to {K}olmogorov Complexity and Its Applications.}
  Third Edition.
  {\em Texts in Computer Science.}
  Springer, New York (2008).

\bibitem{Marchenkov}
  Sergey S. Marchenkov.
  The existence of recursively enumerable minimal -degrees,
  {\em Algebra i Logika} {\bf 14} (1975) pp. 422--429,
  transl. {\bf 14} (1975) pp. 257--261.

\bibitem{Muchnik}
Andrey A. Muchnik and Semen E. Positselky,
  Kolmogorov entropy in the context of computability theory.
  {\em Theoretical Computer Science}, {\bf 271} (2002), pp. 15--35.

\bibitem{Nies}
  Andr\'e Nies.
  {\em Computability and Randomness}.
  Volume 51 of {\em Oxford Logic Guides}.
  Oxford University Press, Oxford (2009).

\bibitem{Schnorr}
  Claus-Peter Schnorr.
  Process complexity and effective random tests.
  {\em Journal of Computer and System Sciences}, {\bf 7} (1973), pp. 376--388.

\bibitem{Solovay}
  Robert M. Solovay.
  {\em Draft of paper (or series of papers) on Chatin's work.}
  Unpublished notes, (1975).

\end{thebibliography}
\end{document}
