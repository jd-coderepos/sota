

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{svg}
\usepackage{gensymb}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{hhline}
\usepackage[super]{nth}
\usepackage{amssymb}
\usepackage{url}
\usepackage{xcolor,colortbl}
\usepackage{array}
\usepackage{booktabs}
\usepackage{stfloats}
\setlength\aboverulesep{0pt}
\setlength\belowrulesep{0pt}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

\newcommand{\method}{CaDDN}



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\setlength{\textfloatsep}{15pt}
\setlength{\dbltextfloatsep}{15pt}

\makeatletter
\newcommand\newtag[2]{#1\def\@currentlabel{#1}\label{#2}}
\makeatother
\definecolor{TableBlue}{HTML}{E0FFFF}

\def\cvprPaperID{3916} \def\confYear{CVPR 2021}



\begin{document}
\title{Categorical Depth Distribution Network for Monocular 3D Object Detection}
\author{
Cody Reading \ \ \ \ Ali Harakeh \ \ \ \ Julia Chae \ \ \ \ Steven L. Waslander\\
University of Toronto Robotics Institute\\
{\tt\small \{cody.reading, nayoung.chae\}@mail.utoronto.ca, ali.harakeh@utoronto.ca, stevenw@utias.utoronto.ca}
}
\maketitle
\begin{abstract}
Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with simple configuration compared to typical multi-sensor systems. The main challenge in monocular 3D detection lies in accurately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range measurement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categorical Depth Distribution Network (\method), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth interval in 3D space. We then use the computationally efficient bird's-eye-view projection and single-stage detector to produce the final output detections. We design \method~as a fully differentiable end-to-end approach for joint depth estimation and object detection. We validate our approach on the KITTI 3D object detection benchmark,  where we rank \nth{1} among published monocular methods. We also provide the first monocular 3D detection results on the newly released Waymo Open Dataset. We provide a code release for \method~which is made available \href{https://github.com/TRAILab/CaDDN}{here}.
\end{abstract}
\begin{figure}
\begin{center}
\includesvg[width=\columnwidth]{Figures/Intro.svg}
\end{center}
\vspace{-3mm}
 \caption{(a) Input image. (b) Without depth distribution supervision, BEV features from \method~suffer from smearing effects. (c) Depth distribution supervision encourages BEV features from \method~ to encode meaningful depth confidence, in which objects can be accurately detected.}
\label{fig:intro}
\end{figure}
\vspace{-4mm}
\section{Introduction} \label{sec:Introduction}
Perception in 3D space is a key component in fields such as autonomous vehicles and robotics, enabling systems to understand their environment and react accordingly. LiDAR~\cite{AVOD, PV-RCNN, PointRCNN} and stereo~\cite{e2e_pseudo-lidar, OCStereo, CGStereo, DSGN} sensors have a long history of use for 3D perception tasks, showing excellent results on 3D object detection benchmarks such as the KITTI 3D object detection benchmark~\cite{Kitti} due to their ability to generate precise 3D measurements.

Monocular based 3D perception has been pursued simultaneously, motivated by the potential for a low-cost, easy-to-deploy solution with a single camera~\cite{Mono3D, Deep3DBox, DeepMANTA, monopsr}.  Performance on the same 3D object detection benchmarks lags significantly relative to LiDAR and stereo methods, due to the loss of depth information when scene information is projected onto the image plane.

To combat this effect, monocular object detection methods~\cite{D4LCN, PatchNet, AM3D, Psuedo-LIDAR} often learn depth explicitly, by training a monocular depth estimation network in a separate stage. However, depth estimates are consumed directly in the 3D object detection stage without an understanding of depth confidence, leading to networks that tend to be overconfident in depth predictions. Over-confidence in depth is particularly an issue at long range~\cite{Psuedo-LIDAR}, leading to poor localization. Further, depth estimation is separated from 3D detection during the training phase, preventing depth map estimates from being optimized for the detection task.

Depth information in image data can also be learned implicitly, by directly transforming features from images to 3D space and finally to bird's-eye-view (BEV) grids~\cite{OFT, LiftSplatShoot}. Implicit methods, however, tend to suffer from feature smearing, wherein similar image features can exist at multiple locations in the projected space. Feature smearing increases the difficulty of localizing objects in the scene.

To resolve the identified issues, we propose a monocular 3D object detection method, \method, that enables accurate 3D detection by learning categorical depth distributions. By leveraging probabilistic depth estimation, \method~is able to generate high quality bird's-eye-view feature representations from images in an end-to-end fashion. We summarize our approach with three contributions.
\0.2\baselineskip]
\noindent\textbf{(2) End-To-End Depth Reasoning.} We learn depth distributions in an end-to-end fashion, jointly optimizing for accurate depth prediction as well as accurate 3D object detection. We argue that joint depth estimation and 3D detection reasoning encourages depth estimates to be optimized for the 3D detection task, leading to increased performance as shown in Section~\ref{sec:Ablation Studies}.
\0.2\baselineskip]
\indent
\method~is shown to rank first among all previously published monocular methods on the Car and Pedestrian categories of the KITTI 3D object detection test benchmark~\cite{KITTI_Test}, with margins of  and   respectively. We are the first to report monocular 3D object detection results on the Waymo Open Dataset~\cite{waymo}.
\section{Related Work} \label{sec:RelatedWork}
\noindent\textbf{Monocular Depth Estimation}.
Monocular depth estimation is performed by generating a single depth value for every pixel in an image. As such, many monocular depth estimation methods are based on architectures used in well-studied pixel-to-pixel mapping problems such as semantic segmentation. As an example, fully convolutional networks (FCNs)~\cite{FCN} were introduced for semantic segmentation, and were subsequently adopted for monocular depth estimation~\cite{Laina}. The atrous spatial pyramid pooling (ASPP) module was also first proposed for semantic segmentation in DeepLab~\cite{DeepLab, DeepLabAlt, DeepLabV3} and subsequently used for depth estimation in DORN~\cite{DORN} and BTS~\cite{BTS}. Further, many methods jointly perform depth estimation and segmentation~\cite{PAD-Net, JointSeg, SDC-Depth, Eigen2} in an end-to-end manner. We follow the design of the semantic segmentation network DeepLabV3~\cite{DeepLabV3} for estimating categorical depth distributions for each pixel in the image.
\0.2\baselineskip]
\noindent\textbf{Monocular 3D Detection}.
Monocular 3D object detection methods often generate intermediate representations to assist in the 3D detection task. Based on these representations, monocular detection can be divided into three categories: direct, depth-based, and grid-based methods.
\0.2\baselineskip]
\noindent\textit{Depth-Based Methods}.
Depth-based methods perform the 3D detection task using pixel-wise depth maps as an additional input, where the depth maps are precomputed using monocular depth estimation architectures~\cite{DORN}. Estimated depth maps can be used in combination with images to perform the 3D detection task~\cite{ROI-10D,UR3D,PatchNet,D4LCN}. Alternatively, depth maps can be converted to 3D point clouds, commonly known as Pseudo-LiDAR~\cite{Psuedo-LIDAR}, which are either used directly ~\cite{Mono3D_Plidar, DA-3Ddet} or combined with image information~\cite{MultiFusion,AM3D} to generate 3D object detection results. Depth-based methods separate depth estimation from 3D object detection during the training stage, leading to the learning of sub-optimal depth maps used for the 3D detection task. Accurate depth should be prioritized for pixels belonging to objects of interest, and is less important for background pixels, a property that is not captured if depth estimation and object detection are trained independently.
\0.2\baselineskip]
\noindent\textbf{Frustum Feature Network}.
The purpose of the frustum feature network is to project image information into 3D space, by associating image features to estimated depths. Specifically, the input to the frustum feature network is an image , where  are the width and height of the image. The output is a frustum feature grid , where  are the width and height of the image feature representation,  is the number of discretized depth bins, and  is the number of feature channels. We note that the structure of the frustum grid is similar to the plane-sweep volume used in the stereo 3D detection method DSGN~\cite{DSGN}.

A ResNet-101~\cite{ResNet} backbone is used to extract image features  (see Image Backbone in Figure~\ref{fig:architecture}). In our implementation, we extract the image features from \textit{Block1} of the ResNet-101 backbone in order to maintain a high spatial resolution. A high spatial resolution is necessary for an effective frustum to voxel grid transformation, such that the frustum grid can be finely sampled without repeated features.

The image features  are used to estimate pixel-wise categorical depth distributions , where the categories are the  discretized depth bins. Specifically, we predict  probabilities for each pixel in the image features , where each probability indicates the network's confidence that depth value belongs to a specified depth bin. The definition of the depth bins relies on the depth discretization method as discussed in Section~\ref{sec:Depth Discretization}.

We follow the design of the semantic segmentation network DeepLabV3~\cite{DeepLabV3} to estimate the categorical depth distributions from image features  (Depth Distribution Network in Figure~\ref{fig:architecture}), where we modify the network to produce pixel-wise probability scores of belonging to depth bins rather than semantic classes with a downsample-upsample architecture. Image features  are downsampled with the remaining components of the ResNet-101~\cite{ResNet} backbone (\textit{Block2}, \textit{Block3}, and \textit{Block4}). An atrous spatial pyramid pooling~\cite{DeepLabV3} (ASPP) module is applied to capture multi-scale information, where the number of output channels is set as . The output of the ASPP module is upsampled to the original feature size with bilinear interpolation to produce the categorical depth distributions . A softmax function is applied for each pixel to normalize the  logits into probabilities between  and .

In parallel to estimating depth distributions, we perform channel reduction (Image Channel Reduce in Figure~\ref{fig:architecture}) on image features   to generate the final image features , using a 1x1 convolution + BatchNorm + ReLU layer to reduce the number of channels from  to . Channel reduction is required to reduce the high memory footprint of ResNet-101 features that will be populated in the 3D frustum grid.

Let  represent a coordinate in image features  and  represent a coordinate in categorical depth distributions , where  are the feature pixel location,  is the channel index, and  is the depth bin index. To generate a frustum feature grid , each feature pixel  is weighted by its associated depth bin probabilities in  to populate the depth axis , visualized in Figure~\ref{fig:frustum}. Feature pixels can be weighted by depth probability using the outer product, defined as:

\noindent
where  is the predicted depth distribution and  is an output matrix of size . The outer product in Equation~\ref{eq:outer_prod} is computed for each pixel to form frustum features .
\begin{figure}
\begin{center}
\includesvg[width=\columnwidth]{Figures/Frustum_to_Voxel.svg}
\end{center}
 \caption{Sampling points in each voxel are projected into the frustum grid. Frustum features are sampled using trilinear interpolation (shown as blue in ) to populate voxels in .}
\label{fig:frustum2voxel}
\end{figure}
\0.2\baselineskip]
\noindent \textbf{Voxel Collapse to BEV}.
The voxel features  are collapsed to a single height plane to generate bird's-eye-view features . BEV grids greatly reduce the computational overhead while offering similar detection performance to 3D voxel grids~\cite{PointPillars}, motivating their use in our network. We concatenate the vertical axis  of the voxel grid  along the channel dimension  to form a BEV grid . The number of channels are reduced using a 1x1 convolution + BatchNorm + ReLU layer (see BEV Channel Reduce in Figure~\ref{fig:architecture}), which retrieves the original number of channels  while learning the relative importance of each height slice, resulting in a BEV grid .
\subsection{BEV 3D Object Detection} \label{BEV 3D Object Detection}
To perform 3D object detection on the BEV feature grid, we adopt the backbone and detection head of the well-established BEV 3D object detector PointPillars~\cite{PointPillars}, as it has been shown to provide accurate 3D detection results with a low computational overhead.  For the BEV backbone, we increase the number of 3x3 convolution + BatchNorm + ReLU layers in the downsample blocks from (4, 6, 6) used in the original PointPillars~\cite{PointPillars} to (10, 10, 10) for \textit{Block1}, \textit{Block2}, and \textit{Block3} respectively. Increasing the number of convolutional layers expands the learning capacity in our BEV network, important for learning from lower quality features produced by images compared to higher quality features originally produced by LiDAR point clouds. We use the same detection head as PointPillars~\cite{PointPillars} to generate our final detections.
\subsection{Depth Discretization} \label{sec:Depth Discretization}
The continuous depth space is discretized in order to define the set of  bins used in the depth distributions . Depth discretization can be performed with uniform discretization (UD) with a fixed bin size, spacing-increasing discretization (SID)~\cite{DORN}  with increasing bin sizes in  space, or linear-increasing discretization (LID)~\cite{Center3D} with linearly increasing bin sizes. Depth discretization techniques are visualized in Figure~\ref{fig:dist}.
\begin{figure}
\begin{center}
\includesvg[width=1.16\columnwidth]{Figures/Dist.svg}
\end{center}
 \caption{Depth Discretization Methods. Depth  is discretized over a depth range  into  discrete bins. Commonly used methods include uniform (UD), spacing-increasing (SID), and linear-increasing (LID) discretization.}
\label{fig:dist}
\end{figure}
We adopt LID as our depth discretization as it provides balanced depth estimation for all depths~\cite{Center3D}. LID is defined as:

\noindent
where  is the continuous depth value,  is the full depth range to be discretized,  is the number of depth bins, and  is the depth bin index.
\begin{table*}
\centering
\begin{tabular}{p{3.6cm}|c|x{0.9cm}x{0.9cm}x{0.9cm}|x{0.9cm}x{0.9cm}x{0.9cm}|x{0.9cm}x{0.9cm}x{0.9cm}}
\toprule
 &  & \multicolumn{3}{c|}{Car (IOU = 0.7)} & \multicolumn{3}{c|}{Pedestrian (IOU = 0.5)} & \multicolumn{3}{c}{Cyclist (IOU = 0.5)} \\
\multirow{-2}{*}{Method} & \multirow{-2}{*}{Frames} & Easy & \textbf{Mod.} & Hard & Easy & \textbf{Mod.} & Hard & Easy & \textbf{Mod.} & Hard \\ \hline
Kinematic3D~\cite{Kinematic3D} & 4 & 19.07 & 12.72 & 9.17 & -- & -- & -- & -- & -- & -- \\ \hline
OFT~\cite{OFT} & 1 & 1.61 & 1.32 & 1.00 & 0.63 & 0.36 & 0.35 & 0.14 & 0.06 & 0.07 \\
ROI-10D~\cite{ROI-10D} & 1 & 4.32 & 2.02 & 1.46 & -- & -- & -- & -- & -- & -- \\
MonoPSR~\cite{monopsr} & 1 & 10.76 & 7.25 & 5.85 & 6.12 & 4.00 & 3.30 & {\color[HTML]{FE0000} \textbf{8.37}} & {\color[HTML]{FE0000} \textbf{4.74}} & {\color[HTML]{FE0000} \textbf{3.68}} \\
Mono3D-PLiDAR~\cite{Mono3D_Plidar} & 1 & 10.76 & 7.50 & 6.10 & -- & -- & -- & -- & -- & -- \\
MonoDIS~\cite{MonoDis} & 1 & 10.37 & 7.94 & 6.40 & -- & -- & -- & -- & -- & -- \\
UR3D~\cite{UR3D} & 1 & 15.58 & 8.61 & 6.00 & -- & -- & -- & -- & -- & -- \\
M3D-RPN~\cite{M3D-RPN} & 1 & 14.76 & 9.71 & 7.42 & 4.92 & 3.48 & 2.94 & 0.94 & 0.65 & 0.47 \\
SMOKE~\cite{SMOKE} & 1 & 14.03 & 9.76 & 7.84 & -- & -- & -- & -- & -- & -- \\
MonoPair~\cite{MonoPair} & 1 & 13.04 & 9.99 & 8.65 & {\color[HTML]{3531FF} \textbf{10.02}} & {\color[HTML]{3531FF} \textbf{6.68}} & {\color[HTML]{3531FF} \textbf{5.53}} & 3.79 & 2.12 & 1.83 \\
RTM3D~\cite{RTM3D} & 1 & 14.41 & 10.34 & 8.77 & -- & -- & -- & -- & -- & -- \\
AM3D~\cite{AM3D} & 1 & 16.50 & 10.74 & 9.52 & -- & -- & -- & -- & -- & -- \\
MoVi-3D~\cite{MoVi-3D} & 1 & 15.19 & 10.90 & 9.26 & 8.99 & 5.44 & 4.57 & 1.08 & 0.63 & 0.70 \\
RAR-Net~\cite{RARNet} & 1 & 16.37 & 11.01 & 9.52 & -- & -- & -- & -- & -- & -- \\
PatchNet~\cite{PatchNet} & 1 & 15.68 & 11.12 & {\color[HTML]{3531FF} \textbf{10.17}} & -- & -- & -- & -- & -- & -- \\
DA-3Ddet~\cite{DA-3Ddet} & 1 & {\color[HTML]{3531FF} \textbf{16.77}} & 11.50 & 8.93 & -- & -- & -- & -- & -- & -- \\
D4LCN~\cite{D4LCN} & 1 & 16.65 & {\color[HTML]{3531FF} \textbf{11.72}} & 9.51 & 4.55 & 3.42 & 2.83 & 2.45 & 1.67 & 1.36 \\ \hline
\textbf{\method~(ours)} & 1 & {\color[HTML]{FE0000} \textbf{19.17}} & {\color[HTML]{FE0000} \textbf{13.41}} & {\color[HTML]{FE0000} \textbf{11.46}} & {\color[HTML]{FE0000} \textbf{12.87}} & {\color[HTML]{FE0000} \textbf{8.14}} & {\color[HTML]{FE0000} \textbf{6.76}} & {\color[HTML]{3531FF} \textbf{7.00}} & {\color[HTML]{3531FF} \textbf{3.41}} & {\color[HTML]{3531FF} \textbf{3.30}} \\
\cellcolor{TableBlue}\textit{Improvement} & \cellcolor{TableBlue}-- & \cellcolor{TableBlue}\textit{+2.40} & \cellcolor{TableBlue}\textit{+1.69} & \cellcolor{TableBlue}\textit{+1.29} & \cellcolor{TableBlue}\textit{+2.85} & \cellcolor{TableBlue}\textit{+1.46} & \cellcolor{TableBlue}\textit{+1.23} & \cellcolor{TableBlue}\textit{-1.37} & \cellcolor{TableBlue}\textit{-1.33} & \cellcolor{TableBlue}\textit{-0.38} \\ \bottomrule
\end{tabular}
\caption{3D detection results on the KITTI~\cite{Kitti} \textit{test} set. Results are shown using the  metric only for results that are readily available. We indicate the highest result with {\color[HTML]{FE0000} \textbf{red}} and the second highest with {\color[HTML]{3531FF} \textbf{blue}}. Full results for \method~can be accessed \href{http://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d593e0abe5e5f8b0a0ea1523816e7851b0266cd8}{here}.}
\label{tab:kitti-test}
\end{table*}
\subsection{Depth Distribution Label Generation} \label{sec:DepthLabel}
We require depth distribution labels  in order to supervise our predicted depth distributions. Depth distribution labels are generated by projecting LiDAR point clouds into the image frame to create sparse dense maps. Depth completion~\cite{IPBasic} is performed to generate depth values at each pixel in the image. We require depth information at each image feature pixel, so we downsample the depth maps of size  to the image feature size . The depth maps are converted to bin indices using the LID discretization method described in Section~\ref{sec:Depth Discretization}, followed by a conversion into a one-hot encoding to generate the depth distribution labels. A one-hot encoding ensures the depth distribution labels are sharp, essential to encourage sharpness in our depth distribution predictions via supervision.
\subsection{Training Losses} \label{sec:TrainingLosses}
Generally, classification is performed by predicting categorical distributions, and encouraging sharpness in the distribution in order to select the correct class~\cite{Class}. We leverage classification to encourage a single correct depth bin when supervising the depth distribution network, using the focal loss~\cite{FocalLoss}:

where  is the depth distribution predictions and  is the depth distribution labels. We found that autonomous driving datasets contain images with fewer object pixels than background pixels, leading to loss functions that prioritize background pixels when all pixel losses are weighted evenly. We set the focal loss~\cite{FocalLoss} weighting factor  as   for foreground object pixels and  for background pixels. Foreground object pixels are determined as all pixels that lie within 2D object bounding box labels, and background pixels are all remaining pixels. We set the focal loss~\cite{FocalLoss} focusing parameter .

We use the classification loss , regression loss , and direction classification loss  from PointPillars~\cite{PointPillars} for 3D object detection. The total loss of our network is the combination of the depth and 3D detection losses:

\noindent
where  are fixed loss weighting factors.
\section{Experimental Results} \label{Experiments}
To demonstrate the effectiveness of \method~we present results on both the KITTI 3D object detection benchmark~\cite{Kitti} and the Waymo Open Dataset~\cite{waymo}.

The KITTI 3D object detection benchmark~\cite{Kitti} is divided into 7,481 training samples and 7,518 testing samples. The training samples are commonly divided into a \textit{train} set (3,712 samples) and a
\textit{val} set (3,769 samples) following~\cite{3DOP}, which is also adopted here. We compare \method~with existing methods on the \textit{test} set by training our model on both the \textit{train} and \textit{val} sets. We evaluate on the \textit{val} set for ablation by training our model on only the \textit{train} set.

The Waymo Open Dataset~\cite{waymo} is a more recently released autonomous driving dataset, which consists of 798 training sequences and 202 validation sequences. The dataset also includes 150 test sequences without ground truth data. The dataset provides object labels in the full 360\degree~field of view with a multi-camera rig. We only use the front camera and only consider object labels in the front-camera's field of view (50.4\degree) for the task of monocular object detection, and provide results on the validation sequences. We sample every \nth{3} frame from the training sequences to form our training set (51,564 samples) due to the large dataset size and high frame rate.
\0.2\baselineskip]
\noindent
\textbf{Training and Inference Details}.
Our method is implemented in PyTorch~\cite{Pytorch}. The network is trained on a NVIDIA Tesla V100 (32G) GPU. The Adam~\cite{Adam} optimizer is used with an initial learning rate of 0.001 and is modified using the one-cycle learning rate policy~\cite{onecycle}. We train the model for 80 epochs on the KITTI dataset~\cite{Kitti} and 10 epochs on the Waymo Open Dataset~\cite{waymo}. We use a batch size of 4 for KITTI~\cite{Kitti} and a batch size of 2 for Waymo.  The values  are used for the loss weighting factors in Equation~\ref{eq:total_loss}. We employ horizontal flip as our data augmentation and train one model for all classes. During inference, we filter boxes with a score threshold of 0.1 and apply non-maximum suppression (NMS) with an IoU threshold of 0.01.
\subsection{KITTI Dataset Results} \label{KITTI Dataset Results}
Results on the KITTI dataset~\cite{Kitti} are evaluated using average precision . The evaluation is separated by difficulty settings (Easy, Moderate, and Hard) and by object class (Car, Pedestrian, and Cyclist). The Car class has an IoU criteria of 0.7 while the Pedestrian and Cyclist classes have an IoU criteria of 0.5, where IoU criteria is a threshold to be considered a true positive detection.

Table~\ref{tab:kitti-test} shows the results of \method~on the KITTI~\cite{Kitti} \textit{test} set compared to state-of-the-art published monocular methods, listed in rank order of performance on the Car class at the Moderate difficulty setting. We note that our method outperforms previous single frame methods by large margins on  of +2.40\%, +1.69\%, and +1.29\% on the Car class on the Easy, Moderate, and Hard difficulties respectively. Additionally, \method~ranks higher than the multi-frame method Kinematic3D~\cite{Kinematic3D}. Our method also outperforms the previous state-of-the art method on the Pedestrian class MonoPair~\cite{MonoPair} with margins on  of +2.85\%, +1.46\%, and +1.23\%. Our method achieves second place on the Cyclist class with margins on  of -1.37\%, -1.33\%, and -0.38\% relative to MonoPSR~\cite{monopsr}.
\subsection{Waymo Dataset Results} \label{Waymo Dataset Results}
\begin{table*}
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{c|c|cccc|cccc}
\toprule
 &  & \multicolumn{4}{c|}{3D mAP} & \multicolumn{4}{c}{3D mAPH} \\
\multirow{-2}{*}{Difficulty} & \multirow{-2}{*}{Method} & Overall & 0 - 30m & 30 - 50m & 50m -  & Overall & 0 - 30m & 30 - 50m & 50m -  \\ \hline
 & M3D-RPN~\cite{M3D-RPN} & 0.35 & 1.12 & 0.18 & 0.02 & 0.34 & 1.10 & 0.18 & 0.02 \\
 & \textbf{CaDNN (Ours)} & \textbf{5.03} & \textbf{14.54} & \textbf{1.47} & \textbf{0.10} & \textbf{4.99} & \textbf{14.43} & \textbf{1.45} & \textbf{0.10} \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}LEVEL\_1\\ (IOU = 0.7)\end{tabular}} & \cellcolor{TableBlue}\textit{Improvement} & \cellcolor{TableBlue}\textit{+4.69} & \cellcolor{TableBlue}\textit{+13.43} & \cellcolor{TableBlue}\textit{+1.28} & \cellcolor{TableBlue}\textit{+0.08} & \cellcolor{TableBlue}\textit{+4.65} & \cellcolor{TableBlue}\textit{+13.33} & \cellcolor{TableBlue}\textit{+1.28} & \cellcolor{TableBlue}\textit{+0.08} \\ \hline
 & M3D-RPN~\cite{M3D-RPN} & {\color[HTML]{1D1C1D} 0.33} & {\color[HTML]{1D1C1D} 1.12} & {\color[HTML]{1D1C1D} 0.18} & {\color[HTML]{1D1C1D} 0.02} & {\color[HTML]{1D1C1D} 0.33} & {\color[HTML]{1D1C1D} 1.10} & {\color[HTML]{1D1C1D} 0.17} & {\color[HTML]{1D1C1D} 0.02} \\
 & \textbf{CaDNN (Ours)} & \textbf{4.49} & \textbf{14.50} & \textbf{1.42} & \textbf{0.09} & \textbf{4.45} & \textbf{14.38} & \textbf{1.41} & \textbf{0.09} \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}LEVEL\_2\\ (IOU = 0.7)\end{tabular}} & \cellcolor{TableBlue}\textit{Improvement} & \cellcolor{TableBlue}\textit{+4.15} & \cellcolor{TableBlue}\textit{+13.38} & \cellcolor{TableBlue}\textit{+1.24} & \cellcolor{TableBlue}\textit{+0.07} & \cellcolor{TableBlue}\textit{+4.12} & \cellcolor{TableBlue}\textit{+13.28} & \cellcolor{TableBlue}\textit{+1.24} & \cellcolor{TableBlue}\textit{+0.07} \\ \hline
 & M3D-RPN~\cite{M3D-RPN} & 3.79 & 11.14 & 2.16 & 0.26 & 3.63 & 10.70 & 2.09 & 0.21 \\
 & \textbf{CaDNN (Ours)} & \textbf{17.54} & \textbf{45.00} & \textbf{9.24} & \textbf{0.64} & \textbf{17.31} & \textbf{44.46} & \textbf{9.11} & \textbf{0.62} \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}LEVEL\_1\\ (IOU = 0.5)\end{tabular}} & \cellcolor{TableBlue}\textit{Improvement} & \cellcolor{TableBlue}\textit{+13.76} & \cellcolor{TableBlue}\textit{+33.86} & \cellcolor{TableBlue}\textit{+7.08} & \cellcolor{TableBlue}\textit{+0.39} & \cellcolor{TableBlue}\textit{+13.69} & \cellcolor{TableBlue}\textit{+33.77} & \cellcolor{TableBlue}\textit{+7.02} & \cellcolor{TableBlue}\textit{+0.41} \\ \hline
 & M3D-RPN~\cite{M3D-RPN} & 3.61 & 11.12 & 2.12 & 0.24 & 3.46 & 10.67 & 2.04 & 0.20 \\
 & \textbf{CaDNN (Ours)} & \textbf{16.51} & \textbf{44.87} & \textbf{8.99} & \textbf{0.58} & \textbf{16.28} & \textbf{44.33} & \textbf{8.86} & \textbf{0.55} \\
\multirow{-3}{*}{\begin{tabular}[c]{@{}c@{}}LEVEL\_2\\ (IOU = 0.5)\end{tabular}} & \cellcolor{TableBlue}\textit{Improvement} & \cellcolor{TableBlue}\textit{+12.89} & \cellcolor{TableBlue}\textit{+33.75} & \cellcolor{TableBlue}\textit{+6.87} & \cellcolor{TableBlue}\textit{+0.34} & \cellcolor{TableBlue}\textit{+12.82} & \cellcolor{TableBlue}\textit{+33.66} & \cellcolor{TableBlue}\textit{+6.81} & \cellcolor{TableBlue}\textit{+0.36} \\ \bottomrule
\end{tabular}
}
\caption{Results on the Waymo Open Dataset Validation Set on the Vehicle class. We evaluate M3D-RPN~\cite{M3D-RPN} as a baseline for comparison.}
\label{tab:waymo-val}
\end{table*}
We adopt the officially released evaluation to calculate the mean average precision (mAP) and the mean average precision weighted by heading (mAPH) on the Waymo Open Dataset~\cite{waymo}. The evaluation is separated by difficulty setting (LEVEL\_1, LEVEL\_2) and distance to the sensor (0 - 30\si{\meter}, 30 - 50\si{\meter}, and 50\si{\meter} - ). We evaluate on the Vehicle class with an IoU criteria of 0.7 and 0.5.

To the best of our knowledge, no monocular methods have reported results on Waymo. In order to provide a baseline, we extend the official implementation of M3D-RPN~\cite{M3D-RPN} to support the Waymo Open Dataset~\cite{waymo}. Table~\ref{tab:waymo-val} shows the results of both the M3D-RPN~\cite{M3D-RPN} baseline and \method~on the Waymo validation set. Our method significantly outperforms M3D-RPN~\cite{M3D-RPN} with margins on AP/APH of +4.69\%/+4.65\% and +4.15\%/+4.12\% on the LEVEL\_1 and LEVEL\_2 difficulties respectively for an IoU criteria of 0.7.
\subsection{Ablation Studies} \label{sec:Ablation Studies}
\begin{table}
\small
\centering
\begin{tabular}{x{0.6cm}|cccc|ccc}
\toprule
\multirow{2}{*}{Exp.} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{LID} & \multicolumn{3}{c}{Car (IOU = 0.7)} \\
 &  &  &  &  & Easy & Mod. & Hard \\ \hline
\newtag{1}{itm:ab1} &  &  &  &  & 7.83 & 5.66 & 4.84 \\
\newtag{2}{itm:ab2} & \checkmark &  &  & & 9.33 & 6.43 & 5.30 \\
\newtag{3}{itm:ab3} & \checkmark & \checkmark &  & & 19.73 & 14.03 & 11.84 \\
\newtag{4}{itm:ab4} & \checkmark & \checkmark & \checkmark &  & 20.40 & 15.10 & 12.75 \\
\newtag{5}{itm:ab5} & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{23.57} & \textbf{16.31} & \textbf{13.84} \\
\bottomrule
\end{tabular}
\caption{\method~Ablation Experiments on the KITTI \textit{val} set using .  indicates depth distribution prediction,  indicates depth distribution supervision.  indicates separate setting of loss weighting factor for foreground object pixels in the depth loss function . LID indicates the LID discretization method.}
\label{tab:ablate}
\bigskip
\begin{tabular}{x{0.59cm}|ccc|ccc}
\toprule
\multirow{2}{*}{Exp.} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multicolumn{3}{c}{Car (IOU = 0.7)} \\
 &  &  &  &  Easy & Mod. & Hard \\ \hline
\newtag{1}{itm:dp1} & BTS~\cite{BTS} & Sep. &  & 16.69 & 10.18 & 8.63 \\
\newtag{2}{itm:dp2} & DORN~\cite{DORN} & Sep. & & 16.43 & 11.04 & 9.65 \\
\newtag{3}{itm:dp3} & \method & Sep. &  & 17.64 & 12.26 & 10.10 \\
\newtag{4}{itm:dp4} & \method & Joint & & 20.61 & 13.71 & 11.96 \\
\newtag{5}{itm:dp5} & \method & Joint & \checkmark & \textbf{23.57} & \textbf{16.31} & \textbf{13.84} \\ \bottomrule
\end{tabular}
\caption{\method~Depth Estimation Ablation on the KITTI \textit{val} set using .  indicates the source of the depth estimates used to generate depth distributions.  indicates if depth estimation and object detection are seperately or jointly optimized.  indicates if full distributions are used to generate frustum features .}
\label{tab:ablate-depth-estimation}
\end{table}
We provide ablation studies on our network to validate our design choices. The results are shown in Tables~\ref{tab:ablate} and~\ref{tab:ablate-depth-estimation}.
\0.2\baselineskip]
\noindent
\textbf{Object Weighting for Depth Distribution Estimation}.
Experiments~\ref{itm:ab1},~\ref{itm:ab2}, and~\ref{itm:ab3} in Table~\ref{tab:ablate} use a fixed loss weighting factor  for all pixels in the depth loss function . Experiment~\ref{itm:ab4} shows an improvement (+0.67\%, +1.07\%, +0.91\%) after depth loss weights  are set seperately for foreground object and background pixels (see Section~\ref{sec:TrainingLosses}). Setting a larger foreground object weighting factor  encourages depth estimation to be prioritized for object pixels, leading to more accurate depth estimation and localization for objects.
\0.2\baselineskip]
\noindent
\textbf{Joint Depth Understanding}.
Experiments~\ref{itm:dp1},~\ref{itm:dp2} and~\ref{itm:dp3} in Table~\ref{tab:ablate-depth-estimation} show the detection performance with separate depth estimation from BTS~\cite{BTS}, DORN~\cite{DORN}, and \method~respectively. The depth maps from BTS~\cite{BTS} and DORN~\cite{DORN} are converted to depth bin indices using LID discretization as outlined in Section~\ref{sec:Depth Discretization}, and converted to a one-hot encoding to generate the depth distributions . The one-hot encoding places the image feature at a single depth bin indicated by the input depth map when generating frustum features . We constuct an equivalent version of \method~that selects a single depth bin for each pixel, by selecting the bin with highest probability for each distribution in . Experiment~\ref{itm:dp4} shows improved performance (+2.97\%, +1.45\%, +1.86\%) when depth estimation and object detection are performed jointly, which we attribute to the well-known benefits of end-to-end learning for 3D detection.
\1mm]
\newtag{2}{itm:fr2} & \textit{Block2} &  & 20.57 & 14.86 & 12.93 \1mm]
\newtag{4}{itm:fr4} & \textit{Block4} &  & 20.71 & 14.94 & 12.91 \0.2\baselineskip]
\noindent
\textbf{Feature Resolution}.
Table~\ref{tab:feat-res} shows the detection peformance of~\method~when modifying the image feature extraction layer in the Image Backbone (See Figure~\ref{fig:architecture}). Experiment~\ref{itm:fr1} shows the performance when image features are extracted from \textit{Block1}. Experiments~\ref{itm:fr2},~\ref{itm:fr3}, and~\ref{itm:fr4} shows reduced performance when smaller resolution image features are extracted. Smaller spatial resolutions in the image features cause oversampling in the frustum to voxel grid transformation, leading to many voxel features  with similar features (See Section~\ref{sec:3D Representation Learning}).
\section{Additional Details} \label{sec:AddDetails}
\noindent
\textbf{Depth Distributions}.
We add an depth bin to our depth distributions  that represents any depth outside the range . The added bin is included in the depth distribution loss , but is removed when generating frustum features .
\0.2\baselineskip]
\noindent
\textbf{Training Details}.
We initialize the Image Backbone and Depth Distribution Network (See Figure~\ref{fig:architecture}) using the DeepLabV3~\cite{DeepLabV3} model pretrained on the MS-COCO dataset~\cite{COCO}. All other components are randomly initialized.
\end{document}
