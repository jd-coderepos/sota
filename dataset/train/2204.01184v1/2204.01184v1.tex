

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[accsupp]{axessibility}  


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{bbm}
\usepackage{color}
\usepackage{xspace}

\usepackage{graphicx}
\usepackage{booktabs}

\usepackage{algorithm}
\usepackage{algpseudocode}
\DeclareMathOperator*{\argmin}{arg\,min} \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\SPD}{\mathbb{S}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\dist}{\euscr{D}}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\td}{{\bf\color{red} FIXME~}}



\def\cvprPaperID{5623} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Exploiting Temporal Relations on Radar Perception for Autonomous Driving}

\author{Peizhao Li\thanks{Work done during the internship at MERL}, Pu Wang, Karl Berntorp, Hongfu Liu\\
Brandeis University, Mitsubishi Electric Research Laboratories\\
{\tt\small \{peizhaoli,hongfuliu\}@brandeis.edu, \{pwang,berntorp\}@merl.com}
}
\maketitle

\begin{abstract}
We consider the object recognition problem in autonomous driving using automotive radar sensors. Comparing to Lidar sensors, radar is cost-effective and robust in all-weather conditions for perception in autonomous driving. However, radar signals suffer from low angular resolution and precision in recognizing surrounding objects. To enhance the capacity of automotive radar, in this work, we exploit the temporal information from successive ego-centric bird-eye-view radar image frames for radar object recognition. We leverage the consistency of an object's existence and attributes (size, orientation, \etc.), and propose a temporal relational layer to explicitly model the relations between objects within successive radar images. In both object detection and multiple object tracking, we show the superiority of our method compared to several baseline approaches.
\end{abstract}

\vspace{-5mm}
\section{Introduction}
\label{sec:intro}

Autonomous driving utilizes sensing technology for robust dynamic object perception, and sequentially uses the perception for reliable and safe vehicle decision-making~\cite{yurtsever2020survey}. Among various perception sensors, camera and Lidar are the two dominant ones exploited for surrounding object recognition. The camera provides semantically rich visual features of traffic scenarios, while Lidar provides high-resolution point clouds that can capture the reflection from objects. Compared with camera and Lidar, radar enjoys the following unique advantages when applied in automotive applications. Primarily operating at  GHz, radar transmits electromagnetic waves at a millimeter wavelength to estimate the range, velocity, and angle of objects. At such a wavelength, it can penetrate or diffract around tiny particles in conditions such as rain, fog, snow, and dust, and offer long-range perception in these adverse weather conditions~\cite{ZengNickolaou14}. In contrast, laser sent by Lidar at a much shorter wavelength may bounce off these tiny particles, which leads to a significantly reduced operating range. Compared with the camera, radar is also resilient to light conditions, \eg, night and sun glare. Furthermore, radar offers a cost-effective and reliable option to complement other sensors. For the cost of Lidar, according to an aggressive estimate by Luminar, is expected to be the range of \1000~\cite{lidar_price}. In contrast, automotive radar is expected to be less than \1^{\circ}1^{\circ}MmqT_{\text{PRI}}f_c79s_p(t)R_0v_tN360^\circ\theta\thetaZ\in\RR^{C\times H\times W}CHWP(x,y)\{(x,y)\}_KKx, y\in\RRZ[P]P\RR^C\RR^{K\times C}I\in\RR^{1\times H\times W}I_{c+p}I_{p+c}\in\RR^{2\times H\times W}cp\mathcal{F}_\theta(\cdot)\mathcal{F}_\theta(\cdot)I_{p+c}I_{c+p}Z_{c}, Z_{p}\in\RR^{C\times \frac{H}{s}\times \frac{W}{s}}s\mathcal{G}_\theta^{\text{pre-hm}}: \RR^{C\times \frac{H}{s}\times \frac{W}{s}} \rightarrow \RR^{1\times \frac{H}{s}\times \frac{W}{s}}Z_cZ_pKP_cZ_c[\mathcal{G}_\theta^{\text{pre-hm}}(Z_{c})]_KK\mathcal{G}_\theta^{\text{pre-hm}}(Z_{c})\frac{H}{s}\times \frac{W}{s}xy(x,y)P_c|P_c| = KZ_pP_pZ_pKP_cP_p\textbf{H}_{c+p} \defeq \begin{bmatrix} \textbf{H}_{c},\textbf{H}_p\\ \end{bmatrix}^\top\in\RR^{2K\times C}K\textbf{H}_{c+p}\textbf{H}_{c+p}^{\text{pos}}\in\RR^{2K\times (C+D_\text{pos})}D_\text{pos}(x, y)[0,1]lll+1q(\cdot)k(\cdot)v(\cdot)d\textbf{M}\in\RR^{2K\times 2K}\textbf{1}_{K,K}K\times K\textbf{0}_{K,K}K\times K\mathbbm{1}_{2K}2K\sigma-(1\mathrm{e}{+10})\textbf{1}_{K,K}\textbf{0}_{K,K}\mathbbm{1}_{2K}\textbf{H}_{c}^{l+1}\textbf{H}_{p}^{l+1}\textbf{H}_{c+p}^{l+1}Z_cZ_pP_cP_p\mathcal{G}_\theta^{\text{hm}}: \RR^{C\times \frac{H}{s}\times \frac{W}{s}} \rightarrow \RR^{1\times \frac{H}{s}\times \frac{W}{s}}\sigmah_i\hat{h}_iiN\alpha\beta24\mathcal{G}_\theta^{\text{pre-hm}}\mathcal{G}_\theta^{\text{b}}: \RR^C \rightarrow \RR^2P_{\text{gt}}^k(x, y)kb^kkZZ_cZ_pL_1[0^{\circ}, 360^{\circ})\vartheta\mathcal{G}_\theta^{\text{r}}: \RR^C \rightarrow \RR^2{\sin(\hat{\vartheta})}{\cos(\hat{\vartheta})})\arctan({\sin(\hat{\vartheta})} / {\cos(\hat{\vartheta})})kc_x^kc_y^kks[\cdot]\mathcal{G}_\theta^{\text{o}}: \RR^C \rightarrow \RR^2L\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm360^{\circ}\times\timesD_p0.30.50.7\uparrow\uparrow\downarrow\downarrow\uparrow\uparrowK4t(-\infty, 100]\hat{\textbf{d}}T^{t-1} = \{(\textbf{c}, \text{id})_j^{t-1}\}_{j=1}^Mt-1\hat{B}^t = \{(\hat{\textbf{c}}, v, \hat{\textbf{d}})_i^t\}_{i=1}^N\hat{\textbf{c}}v\hat{\textbf{d}}\hat{B}^tvkbS\gets \emptysetT^t \gets \emptysetW\gets \text{Cost}(\hat{B}^t, T^{t-1})W_{ij} = || \hat{\textbf{c}}_i^t - \hat{\textbf{d}}_i^t, \hat{\textbf{c}}_j^{t-1} ||_2i\gets 1, Nj\gets \argmin_{j\notin S} W_{ij}w_{ij}\leq kT^t\gets T^t\cup (\hat{\textbf{c}}_i^t, \text{id}_j^{t-1})S\gets S\cup \{j\}jv_i\geq bT^t\gets T^t\cup (\hat{\textbf{c}}_i^t, \text{New id})T^tKKKKKK$ value.}
    \label{fig:k}
\end{figure}



\section{Additional Visualization Result}

We present additional visualization results in Fig.~\ref{fig:add_vis} on object detection. In the detection, green bounding boxes are ground-truth annotations, while red are predictions. The same observations are confirmed in the additional visualizations. False positive predictions are mainly due to the `ghost' objects in radar signals, and the rest are localized in the surroundings or outer space where the angular resolution is low.

\begin{figure*}
    \centering
    \includegraphics[width=1.9\columnwidth]{fig/add_vis.png}
    \caption{Visualizations on object detection. From left to right and top to bottom, the figures are from: motorway-2-1, tiny foggy, junction-1-10, and fog-6-0. Green bounding boxes are ground-truth annotations, while red are model predictions from `Ours-ResNet18-w. TRL.'}
    \label{fig:add_vis}
\end{figure*}



\section{A Short Review of Radar Dataset}

Besides the algorithmic design, many radar datasets are emerging which are crucial for machine learning research. Among these datasets, radar data are currently presented in various data formats, \ie radio frequency heatmap, radar reflection image, or point cloud. \textit{RadarScenes} dataset~\cite{schumann2021radarscenes} provide abundant point-wise annotations with doppler for automotive radar. However, there is no bounding box annotation for objects. \textit{Carrada} dataset~\cite{ouaknine2021carrada} records the range-angle and range-Doppler heatmap. Their data are mainly recorded in experimental sites like parking lots but not in real driving environment. \textit{CRUW} dataset~\cite{wang2021rethinking} offers radar's radio frequency images with camera-projected annotations. \textit{nuScenes}~\cite{caesar2020nuscenes} contains multi-modal data including Lidar, camera, and radar. However, radar data in \textit{nuScenes} only afford sparse point cloud, while the Lidar and camera data are the main advantage of this dataset. \textit{MulRan}~\cite{kim2020mulran} and \textit{Oxford}~\cite{barnes2020oxford} datasets present high-resolution radar images for urban driving scenarios but without object-level annotation. In our paper, we conduct detection and tracking experiments on point cloud-based radar images in adverse weather from \textit{Radiate} dataset~\cite{sheeny2020radiate}, and every significant object has bounding box and tracking ID annotations for training.

\end{document}
