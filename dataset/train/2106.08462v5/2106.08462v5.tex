\documentclass[rebuttal]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ulem}           



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{7462} 

\def\cvprPaperID{7462} \def\confYear{CVPR 2021}


\begin{document}

\title{\LaTeX\ Guidelines for Author Response}  

\thispagestyle{empty}




\definecolor{r1color}{RGB}{30, 126, 229}
\definecolor{r2color}{RGB}{255, 193, 7}
\definecolor{r3color}{RGB}{216, 27, 96}
\definecolor{r4color}{RGB}{0, 77, 64}

\newcommand{\ra}{\dotuline{\textcolor{r1color}{\textbf{R1}}}}
\newcommand{\rb}{\uwave{\textcolor{r2color}{\textbf{R2}}}}
\newcommand{\rc}{\uline{\textcolor{r3color}{\textbf{R3}}}}
\newcommand{\rd}{\dashuline{\textcolor{r4color}{\textbf{R4}}}}



We thank the reviewers for their valuable feedback. We appreciate the positive comments of: \rc{}, who feels our paper “represents very good work and is worth publishing”, and that “the experimental sections convincingly compare the new approach to various existing methods”; \rb{}, who found our idea to be “well-explained”, and that the “motivation and results demonstrate that this direction is fruitful”; \ra{}, who mentioned that “detailed experiments are performed to validate performance”. In this response, we address some reviewer comments, which we believe are due to some ideas not clearly communicated in the paper. We will incorporate all feedback in our revised draft. We hope that the reviewers increase their scores based on this response.

\rb : \textbf{“Why do you compare against a 1-scale FFJORD when the original FFJORD has multiple scales”}
What we call “1-scale FFJORD” \textit{is} the “original FFJORD model with multiple scales”. The original FFJORD model downsamples its feature resolution \textit{within its flow} from image to noise (lines 386-393). In contrast, our method stacks multiple of these original FFJORD models in the input image-pyramid space, conditioning the generation of finer images on coarser ones. We shall make this clearer in our revised draft. Hence, the main comparison in our paper is indeed already with respect to the original FFJORD model.

\rb : \textbf{“You use V100 with 32GB but most competitors use 1080/2080 with only 11GB.”}
We use a V100 with 16GB, we shall mention this in the revised draft. To our knowledge, most competitors use multiple GPUs and take several days to train. In contrast, our models were trained on one V100 GPU of 16GB in $<$1 day (Table 2), and provided better results than our competitors as noted by \rb{}.

\rb : \textbf{“Explain why improvement is not from parameters but from multi-scale”}
Fig. 3 shows a direct comparison of the BPDs of the original FFJORD model (a) and our models (b,c,d). \rb{} is correct in noticing that we may then have more parameters in our model than FFJORD. However, we conducted a thorough ablation study in Tables 2 \& 3 to examine precisely these issues (of which only the best results are presented in Table 1). For example, we compared the performance of our multi-scale models with the same total parameters as the original FFJORD model. The best results of these models from Table 2 are:
\vspace{-1em}
\begin{table}[ht]
\small
\setlength{\tabcolsep}{4pt}
\centering
\begin{tabular}{cccc|cccc}
Scale (Sc) & Params & BPD & & & Scale (Sc) & Params & BPD\\
\hline
1 (FFJORD) & 0.16M & 3.38 & & &
3 (Ours) & 0.13M & 1.70\\
2 (Ours) & 0.16M & 1.80 & & &
4 (Ours) & 0.17M & 1.56\\
\end{tabular}
\label{tab:rebuttal}
\vspace{-1.5em}
\end{table}

In all cases, our multi-scale models have better BPD, despite having the same total number of parameters. Moreover, BPD improved further with more scales. Hence, we conclude that this improvement indeed comes from the proposed multi-scale modification.


\ra : \textbf{“lacks novelty”}
We respectfully disagree. We are the first to apply a flow-based approach to an image pyramid, and have derived the mathematics for the tractable computation of likelihoods in this setup (Section 2.2). We then made a general model that stacks flows at multiple resolutions directly (MSFlow-Image, Section 2.3). Wavelets are a special but important case of this general formulation. Note that \rc{} stated that our method “thus complements the recently introduced WaveletFlow”. Notably, our approach provides much better BPDs compared to the wavelet approach, for similarly sized models. None of the works \ra{} mentioned contain formulations similar to our proposed MSFlow-Image approach. To our knowledge, our work is both novel and provides \textit{state-of-the-art} BPD performance compared to related, but different prior work.

\ra : \textbf{“BPD do not seem very convincing given the FID scores; add generated image samples”}
We shall attach generated image samples in the revised draft. In the interest of transparency and good science, we have reported the BPD as well as the FID of generated images, and do not see this as a weakness. Prior work (ref. [56, 69]) has noted that (Sec. 4.1) “likelihood-based models do not generate images with good sample quality”. Our models demonstrate this property (lines 607-618, Table 2): generated images have higher FID scores but lower BPD values.


\ra : \textbf{“The out-of-distribution (OoD) section does not show the strength of the proposed method”}
We cite several examples of prior work (ref. [69, 55, 65, 56]) that note that using the model likelihood of normalizing flows ``for detecting OoD images is not effective''. However, no such experiments have yet been conducted on Continuous Normalizing Flows (CNFs). We performed these experiments, and found similar OoD behaviour in CNFs (lines 749-775; Fig. 5). We thank \ra{} for pointing us to a recent paper which agrees with our findings, we shall add it to our list of references. We provided these results in the interest of science, and do not see it as a weakness of our submission.


\ra : \textbf{“Repetitions in Tables 1\&2, Table 2 has empty rows”}
We submitted the complete Tables 1 and 2 in the supplementary material. For each N-scale MSFlow, we have 4 rows corresponding to experiments with/without 2 types of regularizations (lines 329-345): RNODE/R, and STEER/S, as mentioned in the image captions.




\rc : \textbf{It would be desirable to check if the demonstrated improvements in BPD and FID scores translate into a reduction of the quality gap.}
The FIDs of MSFlow images are better than those of the FFJORD model. However, it is hard to see this improvement qualitatively. We shall attach generated images in the revised draft.

\rc : \textbf{How closely does the learned density actually represent the real data distribution?}
We see in Fig. 5 that for models trained on CIFAR, the likelihood histograms for the CIFAR-test and Tiny-ImageNet-test set are almost identical. Given that they contain qualitatively similar images, this is positive evidence. As GANs are notoriously inferior at being used for density estimation, it is difficult to compare GANs on density, and further exploration is needed.










\end{document}
