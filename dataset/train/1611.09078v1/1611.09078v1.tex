\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[utf8]{inputenc}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{bm}
\usepackage{bbm}


\newcounter{nbdrafts}
\setcounter{nbdrafts}{0}
\makeatletter
\newcommand{\checknbdrafts}{
\ifnum \thenbdrafts > 0
\@latex@warning@no@line{**********************************************************************}
\@latex@warning@no@line{* The document contains \thenbdrafts \space draft note(s)}
\@latex@warning@no@line{**********************************************************************}
\fi}
\newcommand{\draft}[1]{\addtocounter{nbdrafts}{1}{\color{red} #1}}
\makeatother
\newcommand{\cmt}[1]{\textcolor{red}{\textbf {#1}}}

\newcommand{\kronecker}{\raisebox{1pt}{\ensuremath{\:\otimes\:}}}

\newcommand{\comment}[1]{}

\newcommand{\bF}[0]{\mathbf{F}}
\newcommand{\bP}[0]{\mathbf{P}}
\newcommand{\bR}[0]{\mathbf{R}}
\newcommand{\bB}[0]{\mathbf{B}}
\newcommand{\bS}[0]{\mathbf{S}}
\newcommand{\bX}[0]{\mathbf{X}}
\newcommand{\bx}[0]{\mathbf{x}}
\newcommand{\bI}[0]{\mathbf{I}}
\newcommand{\bq}[0]{\mathbf{q}}
\newcommand{\bD}[0]{\mathbf{D}}
\newcommand{\bv}[0]{\mathbf{v}}
\newcommand{\bmm}[0]{\mathbf{m}}
\newcommand{\bb}[0]{\mathbf{b}}
\newcommand{\btheta}[0]{\bm{\theta}}
\newcommand{\balpha}[0]{\bm{\alpha}}
\newcommand{\bmu}[0]{\bm{\mu}}
\newcommand{\feta}[0]{\bm{\eta}}
\newcommand{\bh}[0]{\bm{h}}
\newcommand{\be}[0]{\bm{e}}
\newcommand{\bp}[0]{\bm{p}}
\newcommand{\ff}[0]{\bm{f}}

\newcommand{\mE}[0]{\mathcal{E}}
\newcommand{\mH}[0]{\mathcal{H}}
\newcommand{\mF}[0]{\mathcal{F}}
\newcommand{\mL}[0]{\mathcal{L}}
\newcommand{\mX}[0]{\mathcal{X}}
\newcommand{\mM}[0]{\mathcal{M}}
\newcommand{\mR}[0]{\mathbb{R}}
\newcommand{\mI}[0]{\mathcal{I}}
\newcommand{\mN}[0]{\mathcal{N}}
\newcommand{\mT}[0]{\mathcal{T}}

\newcommand{\KL}[0]{\text{KL}}
\newcommand{\ADHOC}[0]{\text{ADHOC}}

\newcommand{\EXP}[1]{\langle #1 \rangle}
\newcommand{\ONE}[0]{\mathbbm{1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\TB}[1]{\textcolor{magenta}{TB: #1}}
\newcommand{\PF}[1]{\textcolor{blue}{PF: #1}}
\newcommand{\FF}[1]{\textcolor{red}{FF: #1}}
\newcommand{\AAL}[1]{\textcolor{green}{AA: #1}}


\newcommand{\sttt}[1]{{\footnotesize{\texttt{#1}}}}
\newcommand{\smttt}[1]{{\small{\texttt{#1}}}}
 



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}



\title{Social Scene Understanding:\\ End-to-End Multi-Person Action Localization and Collective Activity Recognition}

\author{Timur Bagautdinov, 
Alexandre Alahi, 
François Fleuret, 
Pascal Fua, 
Silvio Savarese\\
École Polytechnique Fédérale de Lausanne (EPFL)\\
Stanford University\\
IDIAP Research Institute\\
{\tt\small \{timur.bagautdinov, francois.fleuret, pascal.fua\}@epfl.ch, 
{\tt\small \{alahi, ssilvio\}@stanford.edu}}
}

\maketitle




\begin{abstract}
We present a unified framework for understanding human social behaviors in raw image
sequences. 
Our model jointly detects multiple individuals, infers their social actions, and 
estimates the collective actions with a single feed-forward pass through a neural network. 
We propose a single architecture that does not rely on external detection 
algorithms but rather is trained end-to-end to generate
dense proposal maps that are refined via a novel inference scheme.
The temporal consistency is handled via a person-level matching Recurrent Neural
Network. The complete model takes as input a sequence of frames and outputs
detections along with the estimates of individual actions and collective activities. We
demonstrate state-of-the-art performance of our algorithm on multiple publicly
available benchmarks.
\end{abstract}



\vspace{-0.5cm}
\section{Introduction}
\vspace{-0.15cm}

Human social behavior can be characterized by ``social actions'' -- an
individual act which nevertheless takes into account the behaviour of other
individuals -- and ``collective actions'' taken together by a group of people
with a common objective. For a machine to perceive both of these actions, it
needs to develop a notion of collective intelligence, \textit{i.e.}, reason
jointly about the behaviour of multiple individuals. In this work, we propose a
method to tackle such intelligence. Given a sequence of image frames, our method 
jointly locates and describes the
social actions of each individual in a scene as well as the collective actions (see
Figure \ref{fi:intro:pull}). This perceived social scene representation can be
used for sports analytics, understanding social behaviour, surveillance, and
social robot navigation.

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.45\textwidth]{figures/pull}
\end{center}
\caption{Jointly reasoning on social scenes. Our method takes as input raw image
sequences and produces a comprehensive social scene interpretation: locations of
individuals (as bounding boxes), their individual social actions (e.g., ``blocking"), and the collective activity (``right spike" in the illustrated example).}
\label{fi:intro:pull}
\vspace{-0.5cm}
\end{figure}

Recent methods for multi-person scene understanding take a sequential approach
\cite{Ibrahim2016,Deng2016,Ramanathan2016}: i) each person is detected in
every given frame; ii) these detections are associated over time by a
tracking algorithm; iii) a feature representation is extracted for each
individual detection; and finally iv) these representations are joined via
a structured model. Whereas the aforementioned pipeline seems reasonable, it
has several important drawbacks.  
First of all, the vast majority of state-of-the-art detection methods do not use any
kind of joint optimization to handle multiple objects, but rather rely on heuristic post-processing, and thus are susceptible to greedy non-optimal decisions. Second,
extracting features individually for each object discards a large amount of context and
interactions, which can be useful when reasoning about collective behaviours. 
This point is particularly important because the locations and actions of humans can be
highly correlated. 
For instance, in team sports, the location and action of each player depend on the behaviour of other players as well as on the collective strategy.  Third, having
independent detection and tracking pipelines means that the representation used for
localization is discarded, whereas re-using it would be more efficient. Finally, the
sequential approach does not scale well with the number of people in the scene, since it
requires multiple runs for a single image.

Our method aims at tackling these issues. Inspired by recent work in
multi-class object detection~\cite{Ren2015,Redmon2016} and image
labelling~\cite{Johnson2016}, we propose a single architecture that jointly
localizes multiple people, and classifies the actions of each individual as well
as their collective activity. Our model produces all the estimates in a single
forward pass and requires neither external region proposals nor pre-computed 
detections or tracking assignments. 

Our contributions can be summarized as follows:

\begin{itemize}
\setlength\itemsep{0cm}
\item We propose a unified framework for social scene understanding by simultaneously
solving three tasks in a single feed forward pass through a Neural Network: multi-person
detection, individual's action recognition, and collective activity recognition.  
Our method operates on raw image sequences and relies on joint multi-scale features 
that are shared among all the tasks. 
It allows us to fine-tune the feature extraction layers early enough to enable the model to capture the context and interactions.
\item We introduce a novel multi-object detection scheme, inspired by the classical
work on Hough transforms. Our scheme relies on probabilistic inference that jointly
refines the detection hypotheses rather than greedily discarding them, which makes
our predictions more robust.
\item We present a person-level matching Recurrent Neural Network (RNN) model to
propagate information in the temporal domain, while not having access to the the
trajectories of individuals.
\end{itemize}

In Section~\ref{sec:evaluation}, we show quantitatively that these
components contribute to the better overall performance. Our model achieves
state-of-the-art results on challenging multi-person sequences, and 
outperforms existing approaches that rely on the ground truth annotations at
test time. We demonstrate that our novel detection scheme is on par with the 
state-of-the art methods on a large-scale dataset for localizing multiple 
individuals in crowded scenes. Our implementation will be made publicly available.

\begin{figure*}[htp!]
\vspace{-0.25cm}
\begin{center}
\includegraphics[width=\textwidth]{figures/overview}
\end{center}
\vspace{-0.5cm}
\caption{General overview of our architecture. Each frame of the given 
sequence is passed through a fully-convolutional network (FCN) to produce a 
multi-scale feature map , which is then shared between the detection and action 
recognition tasks. Our detection pipeline is another fully-convolutional network (DFCN) 
that produces a dense set of detections  along with the probabilities 
, followed by inference in a hybrid MRF. 
The output of the MRF are reliable detections  which are used to extract
fixed-sized representations , which are then passed to a matching RNN that reasons 
in the temporal domain. The RNN outputs the probability of an individual's action,
, and the collective activity,  across time. Note that  
(\ref{eq:method:loss-detection}) is the
loss function for the detections, and  (\ref{eq:method:loss-actions}) is the loss
function for the individual and collective actions.}
\label{fi:method:overview}
\vspace{-0.5cm}
\end{figure*} \vspace{-0.1cm}
\section{Related Work}
\vspace{-0.15cm}
\label{sec:related}
The main focus of this work is creating a unified model that can
simultaneously detect multiple individuals and recognize their
individual social actions and collective behaviour. 
In what follows, we give a short overview of the existing work on these tasks. 

\noindent \textbf{Multi-object detection} - There already exists large body of 
research in the area of object detection. Most of the current methods either 
rely on a sliding window approach~\cite{Sermanet2013,Zhang2015}, or on the object proposal
mechanism~\cite{Girshick2015,Ren2015}, followed by a CNN-based classifier. The
vast majority of those state-of-the-art methods do not reason jointly on the
presence of multiple objects, and rely on very heuristic post-processing
steps to get the final detections. A notable exception is the
ReInspect~\cite{Stewart2016} algorithm, which is specifically designed to handle
multi-object scenarios by modeling detection process in a sequential manner,
and employing a Hungarian loss to train the model end-to-end. We approach this
problem in a very different way, by doing probabilistic inference on top of 
a dense set of detection hypotheses, while also demonstrating state-of-the-art 
results on challenging crowded scenes. Another line of work that specifically 
focuses on joint multi-person detection~\cite{Fleuret2008, Bagautdinov2015} uses
generative models, however, those methods require multiple views or depth maps 
and are not applicable in monocular settings.

\noindent \textbf{Action recognition} - A large variety of methods for action
recognition traditionally rely on handcrafted features, such as
HOG~\cite{Dalal2005,Weinland10}, HOF~\cite{Laptev2008} and MBH~\cite{Wang2013}. 
More recently, data-driven approaches based on deep learning have
started to emerge, including methods based on 3D CNNs~\cite{Ji2013} and multi-stream
networks~\cite{Feichtenhofer2016,Singh2016a}. Some
methods~\cite{Wang2015,Singh2016b}, exploit the strengths of both handcrafted
features and deep-learned ones. Most of these methods rely in one way or
another on temporal cues: either through having a separate temporal
stream~\cite{Feichtenhofer2016, Singh2016b}, or directly encoding them into
compact representations~\cite{Laptev2008,Wang2013,Wang2013}. Yet another way to
handle temporal information in a data-driven way is Recurrent Neural Networks (RNNs).
Recently, it has received a lot of interest in the context of action
recognition~\cite{Singh2016a,Du2015,Veeriah2015,Donahue2015}. All these methods,
however, are focusing on recognizing actions for single individuals, and thus
are not directly applicable in multi-person settings.

\noindent \textbf{Collective activity recognition} - Historically, a large
amount of work on collective activity recognition relies on graphical models
defined on handcrafted features~\cite{Choi2014, Choi2011, Amer2014}. The
important difference of this type of methods with the single-person action
recognition approaches is that they explicitly enforce simultaneous 
reasoning on multiple people. The vast majority of the state-of-the-art methods for
recognizing multi-person activities thus also rely on some kind of structured model,
that allows sharing information between representations of individuals.
However, unlike earlier handcrafted methods, the focus of the recent
developments has shifted towards merging the discriminative power of neural networks
with structured models. In~\cite{Deng2016}, authors propose a way to refine
individual estimates obtained from CNNs through inference: they define a
trainable graphical model with nodes for all the people and the scene, and pass
messages between them to get the final scene-level
estimate. In~\cite{Ibrahim2016}, authors propose a hierarchical model that takes
into account temporal information. The model consists of two LSTMs: the first
operates on person-level representations, obtained from a CNN, which are then
max pooled and passed as input to the second LSTM capturing scene-level
representation.  \cite{Ramanathan2016} explores a slightly different
perspective: authors notice that in some settings, the activity is defined by
the actions of a single individual and propose a soft attention mechanism to
identify her. The complete model is very close to that of~\cite{Ibrahim2016},
except that the attention pooling is used instead of a max pool. All of those
methods are effective, however, they start joint reasoning in late inference
stages, thus possibly
discarding useful context information. Moreover, they all rely on ground truth
detections and/or tracks, and thus do not really solve the problem end-to-end.

Our model builds upon the existing work in that it also relies on the
discriminative power of deep learning, and employs a version of person-level
temporal model. It is also able to implicitly capture the context and perform
social scene understanding, which includes reliable localization and action
recognition, all in a single end-to-end framework.
 \vspace{-0.1cm}
\section{Method}
\vspace{-0.15cm}

Our main goal is to construct comprehensive interpretations of social scenes
from raw image sequences. To this end, we propose a unified way to jointly detect multiple
interacting individuals and recognize their collective and individual actions.

\subsection{Overview}
\vspace{-0.15cm}

The general overview of our model is given in Figure~\ref{fi:method:overview}.
For every frame  in a given sequence, we first
obtain
a dense feature representation , where  denotes the set of all pixel locations in the feature
map,  is the number of pixels in that map, and  is the
number of features. The feature map  is then shared between the
detection and action recognition tasks.
To detect, we first obtain a preliminary set of detection hypotheses,
encoded as two dense maps  and ,
where at each location ,  encodes the coordinates of the bounding box,
and  is the probability that this bounding box represents a person.
Those detections are refined jointly by inference in a hybrid Markov
Random Field (MRF). The result of the inference is a smaller set of 
reliable detections, encoded as bounding boxes . 
These bounding boxes are then used to smoothly extract
fixed-size representations  from the feature map
, where  is the size of the fixed representation in pixels. 
Representations  are then used as inputs to the matching RNN, which 
merges the information in the temporal domain. At each time step , RNN produces
probabilities  of individual actions for each detection 
, along with the probabilities of collective activity 
, where
 denote respectively the number of classes of individual and
collective actions. In the following sections, we will describe each of these
components in more detail. 



\subsection{Joint Feature Representation}
We build upon the Inception architecture~\cite{Szegedy2015} for getting our
dense feature representation, since it does not only demonstrate good performance
but is also more computationally efficient than some of the more popular
competitors~\cite{Simonyan2014, Krizhevsky2012}. 

One of the challenges when simultaneously dealing with multiple tasks
is that representations useful for one task may be quite inefficient for another. 
In our case, person detection requires
reasoning on the type of the object, whereas discriminating between actions can
require looking at lower-level details. To tackle this problem, we propose
using multi-scale features: instead of simply using the final convolutional
layer, we produce our dense feature map  (here and later
 is omitted for clarity) by concatenating multiple intermediate activation
maps. Since they do not have fitting dimensions, we resize them to the fixed
size  via differentiable bilinear interpolation. 
Note that similar approaches have been very successful for semantic
segmentation~\cite{Long2015,   Hariharan2015}, when one has to simultaneously
reason about the object class and its boundaries.



\subsection{Dense Detections}
\vspace{-0.15cm}

Given the output of the feature extraction stage, the goal of the detection
stage is to generate a set of reliable detections, that is, a set of
bounding box coordinates with their corresponding confidence scores. We do it in
a dense manner, meaning that, given the feature map ,
we produce two dense maps  and ,
for bounding boxes coordinates and presence probability,
respectively. Essentially,  represents a segmentation mask encoding
which parts of the image contain people, and 
represents the coordinates of the bounding boxes of the people present in the
scene, encoded relative to the pixel locations. This is illustrated by
Figure~\ref{fi:method:dense-maps}. 

We can interpret this
process of generating  from  in several different ways. With
respect to recent work on object
detection~\cite{Girshick2015,Redmon2016,Ren2015}, it can be seen as a
fully-convolutional network that produces a dense set of object proposals, where
each pixel of the feature map  generates a proposal. Alternatively, we
can see this process as an advanced non-linear version of the Hough transform,
similar to Hough Forests~\cite{Gall2011, Barinova2012}. In these methods, each
patch of the image is passed through a set of decision trees, which produce a
distribution over potential object locations. The crucial differences with the
older methods are, first, leveraging deep neural network as a more powerful 
regressor and, second, the ability to use large contexts in the image, in 
particular to reason jointly about parts. 

\begin{figure}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=0.12\textwidth]{figures/masks-example-gt-seg} &
\includegraphics[width=0.12\textwidth]{figures/masks-example-gt-y0} &
\includegraphics[width=0.12\textwidth]{figures/masks-example-gt-x0} \\
\includegraphics[width=0.12\textwidth]{figures/masks-example-pred-seg} &
\includegraphics[width=0.12\textwidth]{figures/masks-example-pred-y0} &
\includegraphics[width=0.12\textwidth]{figures/masks-example-pred-x0} \\
\end{tabular}
\end{center}
\vspace{-0.25cm}
\caption{Example of ground truth (top) and predicted (bottom) maps. We show
  segmentation map  projected on the original image, followed by two out of four
  channels of the regression map , which encode respectively vertical and horizontal displacement from the location  to one of the bounding box corners.}
\label{fi:method:dense-maps}
\vspace{-0.5cm}
\end{figure}

Let us now introduce  and  more formally, by defining how we convert
the given ground truth object locations into dense ground truth maps
. For each image , the detection ground truth is 
given as a set of bounding boxes
. To obtain the
value for the specific location  of the ground truth
probability map , we set  if 
 for any of the ground truth boxes,
and  otherwise. 
For the regression map, each location  represents a vector 
, where:


where  are scaling coefficients that are fixed, and can be taken
either as the maximum size of the bounding box over the training set, or the
size of the image. Ultimately, our formulation makes it possible to use
ground truth instance-level segmentation masks to assign each  to one of the ground
truth instances. However, since these masks  are not available, and there 
can be multiple ground truth bounding boxes that contain , we
assign each  to the bounding box with the highest  coordinate, 
as shown in Figure~\ref{fi:method:dense-maps}. Note that,  are only
defined only for , and the regression loss is constructed 
accordingly.

The mapping from  to ,  is a fully-convolutional network,
consisting of a stack of two  convolutional layers with 512 filters 
and a shortcut connection~\cite{He2016}. We use softmax activation function for
 and ReLU for . The loss is defined as follows:

where  is a weight that makes training focused
more on classification or regression. For datasets where classification is easy,
such as \texttt{volleyball}~\cite{Ibrahim2016}, we set it to , 
whereas for cluttered scenes with large variations in appearance lower values could be
beneficial.




\subsection{Inference for Dense Detection Refinement}
\vspace{-0.15cm}
\label{sec:method:inference}

The typical approach to get the final detections given a set of proposals is to
re-score them using an additional recognition network and then run non-maxima
suppression (NMS)~\cite{Johnson2016, Ren2015}. This has several
drawbacks. First, if the amount of the proposals is large, the re-scoring stage can
be prohibitively expensive. Second, the NMS step itself is by no means optimal, and is
susceptible to greedy decisions. Instead of this commonly used technique, we
propose using a simple inference procedure that does not require re-scoring, and
makes NMS in the traditional sense unnecessary. Our key observation is that instead
of making similar hypotheses suppressing each other, one can rather make them
refine each other, thus increasing the robustness of the final estimates.

To this end, we define a hybrid MRF on top of the dense
proposal maps , which we obtain by converting  to the global image
coordinates.  For each hypothesis location  we introduce two hidden
variables, one multinomial Gaussian , and one categorical .  encodes the ``true'' coordinates of the detection, and 
encodes the assignment of the detection to one of the
hypothesis locations in . Note that, although this assignment variable is discrete,
we  formulate our problem in a probabilistic way, through distributions, thus
allowing a detection to be ``explained'' by multiple locations. The joint
distribution over  is defined as follows:

\vspace{-0.25cm}

where  is the standard deviation parameter, which is fixed.

Intuitively, (\ref{eq:model:mrf-joint}) jointly models the relationship between 
the bounding box predictions produced by the fully-convolutional network. 
The basic assumption is that each location  on the feature map 
belongs to a single "true" detection location , which can be equal to , 
and the observation  should not be far from the observation  at this "true"
location. The goal of inference is to extract those "true" locations
and their corresponding predictions by finding the optimal assignments for  and
values of . In other words, we want to compute marginal distributions
. Unfortunately, the exact integration is not
feasible, and we have to resort to an approximation. We use the mean-field approximation, 
that is, we introduce the following factorized variational distribution: 

\vspace{-0.25cm}

where  and  are the variational
parameters of the Gaussian and categorical distributions respectively. Then,
we minimize the KL-divergence between the variational distribution
(\ref{eq:model:mrf-variational}) and the joint (\ref{eq:model:mrf-joint}), which
leads to the following fixed-point updates for the parameters of :


where  is the iteration number, 
 is the reparameterization of . The
complete derivation of those updates is provided in the supplementary material.

Starting from some initial , one can now use
(\ref{eq:model:mrf-update-eta}),
(\ref{eq:model:mrf-update-mu}) until convergence. In practice, we start with
 initialized from the estimates , thus conditioning our model
on the observations, and only consider those , for which the
segmentation probability , where  is a fixed threshold. 
Furthermore, to
get  we use the following smoothed update for a fixed number of
iterations :

where  is a damping parameter that can be interpreted as a
step-size~\cite{Baque2016}.

To get the final set of detections, we still need to identify the most likely
hypothesis out of our final refined set . Luckily, since we also have
the estimates  for the assignment variables , we can identify
them using a simple iterative scheme similar to that used in Hough
Forests~\cite{Barinova2012}. That is, we identify the hypothesis with the
largest number of locations assigned to it, then remove those
locations from consideration, and iterate until there are no unassigned
locations left. The number of assigned locations is then used as a detection
score with a very nice interpretation: a number of pixels that ``voted'' for
this detection. 


\subsection{Matching RNN for Temporal Modeling}
\vspace{-0.15cm}

Previous sections described a way to obtain a set of reliable detections
from raw images. However, temporal information is known to be a very important
feature when it comes to action recognition~\cite{Laptev2008, Wang2013}. To this
end, we propose using a matching Recurrent Neural Network, that allows us to 
merge and propagate information in the temporal domain.

For each frame , given a set of  detections , we first smoothly extract fixed-sized representations  from the the dense feature map , using
bilinear interpolation. This is in line
with the ROI-pooling~\cite{Ren2015}, widely used in object detection,  and can
be considered as a less generic version of spatial transformer
networks~\cite{Jaderberg2015}, which were also successfully used for image
captioning~\cite{Johnson2016}. Those representations  are then passed
through a fully-connected layer, which produces more compact embeddings
, where  is the number of features in the embedded
representation. These embeddings are then used as inputs to the RNN units.

We use standard Gated Recurrent Units (GRU) ~\cite{Chung2014} for each person in 
the sequence, with a minor modification. Namely, we do not have access to the track
assignments neither during training nor testing, which means that the hidden states 
 and , where  is the number 
of features in the hidden state, are not necessarily corresponding to the same person. 
Our solution to this is very simple: we compute the Euclidean distances between each
pair of representations at step  and , and then update the hidden state
based on those distances. A naive version that works well when the ground truth
locations are given, is to use bounding box coordinates  as the
matching representations, and then update  by the closest match
:

\vspace{-0.25cm}


Alternatively, instead of bounding box coordinates , one can use the
embeddings . This allows the model to learn a 
suitable representation, which can be potentially more robust to missing/misaligned
detections. Finally, instead of finding a \textit{single}
nearest-neighbor to make the hidden state update, we can use \textit{all} 
the previous representations, weighted by the distance in the embedding space as 
follows:
\vspace{-0.25cm}


We experimentally evaluated all of these
matching techniques, which we call respectively \texttt{boxes},
\texttt{embed} and \texttt{embed-soft}. We provide results 
in Section~\ref{sec:evaluation}.

To get the final predictions  for collective activities, we max pool over the
hidden representations  followed by a softmax classifier. 
The individual actions predictions  are computed by a separate softmax 
classifier on top of  for each detection . The loss is defined as follows:
\vspace{-0.25cm}

where  is the number of frames,  are the numbers of labels for
collective and individual actions,  is the number of detections, and 
 is the one-hot-encoded ground truth. The weight
 allows us to balance the two tasks differently, but we found that the model
is somewhat robust to the choice of this parameter. In our experiments, we set 
.
 \vspace{-0.1cm}
\section{Evaluation}
\vspace{-0.15cm}
\label{sec:evaluation}

In this section, we report our results on the task of multi-person scene 
understanding and compare them to the baselines introduced in 
Section~\ref{sec:related}. We also compare our detection pipeline to 
multiple state-of-the-art detection algorithms on a challenging dataset
for multi-person detection.

\subsection{Datasets}

We evaluate our framework on the recently introduced \texttt{volleyball} 
dataset~\cite{Ibrahim2016}, since it is the only
publicly available dataset for multi-person activity recognition that is 
relatively large-scale and contains labels for people locations, as well 
as their collective and individual actions.

This dataset consists of 55 volleyball games with 4830 labelled frames, where
each player is annotated with the bounding box and one of the 9 individual actions,
and the whole scene is assigned with one of the 8 collective activity labels, which
define which part of the game is happening. For each
annotated frame, there are multiple surrounding unannotated frames available. To
get the ground truth locations of people for those, we resort to the same
appearance-based tracker as proposed by the authors of the dataset~\cite{Ibrahim2016}.







\subsection{Baselines}

We use the following baselines and versions of our approach in the evaluation:
\begin{itemize}
\setlength\itemsep{0cm}
\item \texttt{Inception-scene} - Inception-v3 network~\cite{Szegedy2015},
  pre-trained on ImageNet and fine-tuned to predict collective actions on whole
  images, without taking into account locations of individuals.
\item \texttt{Inception-person} - similar to previous baseline, but trained to
  predict individual actions based on high-resolution fixed-sized images of
  individual people, obtained from the ground truth detections.
\item \texttt{HDTM} - A 2-stage deep temporal model model~\cite{Ibrahim2016},
  consisiting of one LSTM to aggregate person-level dynamics, and one LSTM to
  aggregate scene-level temporal information. We report multiple versions of this
  baseline: the complete version which includes both scene-level and person-level 
  temporal models, \texttt{scene}, which only uses scene-level LSTM, 
  and \texttt{person}, which only uses person-level LSTM.
\item \texttt{OURS-single} - A version of our model that does not use an RNN. 
We report results for ground truth locations, as well as detections
  produced by our detection pipeline.
\item \texttt{OURS-temporal} - A complete version of our model with GRU units for
  temporal modeling. We report results both for ground truth locations and our detections,
  as well as results for different matching functions.
\end{itemize}



\subsection{Implementation Details}
\label{sec:evaluation:implementation}

All our models are trained using backpropagation using the same optimization
scheme: for all the experiments and all datasets, we use stochastic gradient descent with
ADAM~\cite{Kingma2014}, with the initial learning rate set to , and
fixed hypereparameters to .


We train our model in two stages: first, we train a network on single frames, 
to jointly predict detections, individual, and collective actions. We then fix 
the weights of the feature extraction part of our model, and train our temporal RNN 
to jointly predict individual actions together with collective activities. 
Note that in fact our model is fully-differentiable, and the reason for this 
two-stage training is purely technical: backpropagation requires keeping all the
activations in memory, which is not possible for a batch of image sequences.
The total loss is simply a sum of the detection loss (\ref{eq:method:loss-detection}) 
and the action loss (\ref{eq:method:loss-actions}) for the first stage, and the action
loss for the second stage. We use a temporal window of length , which 
corresponds to 4 frames before the annotated frame, and 5 frames after. 

The parameters of the MRF are the same for all the experiments. We run inference 
on the bounding boxes with the probability  above the threshold , 
and set the standard deviation
, step size , and the number of iterations . 

Our implementation is based on TensorFlow~\cite{Abadi2015} and its running
time for a single sequence of  high-resolution (720x1080) images is
approximately 1.2s on a single Tesla-P100 NVIDIA GPU. 



\subsection{Multi-Person Scene Understanding}

The quantitative results on the \texttt{volleyball} dataset are given in Table~\ref{tab:eval:volley-baselines}. Whenever available,
we report accuracies both for collective action recognition and individual
action recognition. For variants of our methods, we report two numbers: when the
output of our detection pipeline was used (MRF), and the ground truth bounding
boxes (GT). Our method is able to achieve state-of-the-art
performance for collective activity recognition even without ground truth
locations of the individuals and temporal reasoning. With our matching RNN,
performance improvements are even more noticeable. 
The comparison to \texttt{Inception-person}, which
was fine-tuned specifically for the single task of individual action recognition,
indicates that having a joint representation which is shared across multiple
tasks leads to an improvement in average accuracy on individual actions. When we use the
output of our detections, the drop in performance is expected, especially 
since we did not use any data augmentation to make the action recognition robust to
imperfect localization. For collective actions, having perfect localization is somewhat
less important, since the prediction is based on multiple individuals.
In Figure~\ref{fi:eval:visual} we provide some visual results, bounding boxes and 
actions labels are produced by \texttt{OURS-temporal} model with \texttt{embed-soft}
matching from raw image sequences.


\begin{table}[ht!]
\vspace{-0.25cm}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Method                    & collective & individual \\\hline
\smttt{Inception-scene} (GT)  &  75.5      &  -       \\
\smttt{Inception-person} (GT) &   -        &  78.1    \\\hline
\smttt{HDTM-scene}~\cite{Ibrahim2016}(GT) & 74.7 & - \\
\smttt{HDTM-person}~\cite{Ibrahim2016}(GT) & 80.2  & - \\
\smttt{HDTM}~\cite{Ibrahim2016}(GT) & 81.9  & - \\\hline
\smttt{OURS-single} (MRF/GT) & 83.3 / 83.8  & 77.8 / 81.1  \\
\smttt{OURS-temporal} (MRF/GT) &  87.1 / \textbf{89.9} & 77.9 / \textbf{82.4} \\\hline
\end{tabular}
\end{center}
\caption{Results on the \texttt{volleyball} dataset. We report average accuracy
  for collective activity and individual actions. For \texttt{OURS-temporal}
  for the ground truth bounding boxes (GT) we report results with the \texttt{bbox}
  matching, and for the detections (MRF) we report results with the 
  \texttt{embed} matching.}
 \label{tab:eval:volley-baselines}
 \vspace{-0.25cm}
\end{table}


In Table~\ref{tab:eval:volley-matching} we compare different matching
strategies. For the ground truth detections, as expected, simply finding the
best match in the bounding box coordinates, \texttt{boxes}, works very well.
Interestingly, using the \texttt{embed} and \texttt{embed-soft} matching are beneficial
for the performance when detections are used instead of the ground truth. It is also
understandable: appearance is more robust than coordinates, but it also
means that our model is actually able to capture that robust appearance
representation, which might not be absolutely necessary for the prediction in a
single frame scenario. Note that, whereas for the collective actions the 
temporal data seems to help significantly, the improvement for the individual action
estimation is very modest, especially for the detections. We hypothesize that in order to
discriminate better between individual actions, it is
necessary to look at how the low-level details change, which could be potentially 
smoothed out during the spatial pooling, and thus they are hard to capture for our 
RNN. 

\renewcommand{\tabcolsep}{1pt}
\begin{figure*}[htp!]
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=0.31\textwidth]{figures/visual/volleyball-14-0-4.pdf} &
\includegraphics[width=0.31\textwidth]{figures/visual/volleyball-17-0-4.pdf} &
\includegraphics[width=0.31\textwidth]{figures/visual/volleyball-17-1-4.pdf} \\
\includegraphics[width=0.31\textwidth]{figures/visual/volleyball-8-0-4.pdf} &
\includegraphics[width=0.31\textwidth]{figures/visual/volleyball-25-3-4.pdf} &
\includegraphics[width=0.31\textwidth]{figures/visual/volleyball-28-1-4.pdf} \\
\end{tabular}
\end{center}
\caption{Examples of visual results (better viewed in color). 
Green boxes around the labels correspond to correct predictions, red correspond to 
mistakes. The bounding boxes in the images are produced by our detection scheme, 
and obtained in a single pass together with the action labels.}
\label{fi:eval:visual}
\vspace{-0.25cm}
\end{figure*}
\renewcommand{\tabcolsep}{6pt}





\begin{table}[ht!]
  \begin{center}
    \begin{tabular}{|l|c|c|}
      \hline
      Method  & collective & individual \\\hline
      \texttt{boxes} (MRF/GT) & 82.0 / 89.9  & 68.6 / \textbf{82.4}  \\
      \texttt{embed} (MRF/GT) & 87.1 / 90.0   & 77.9 / 81.9  \\
\texttt{embed-soft} (MRF/GT) &  86.2 / \textbf{90.6}  & 77.4 / 81.8  \\\hline 
    \end{tabular}
  \end{center}
  \caption{Comparison of different matching strategies for the
    \texttt{volleyball} dataset. \texttt{boxes}
    corresponds  to the nearest neighbour (NN) match in the space of bounding
    box coordinates, \texttt{embed} corresponds to the NN in the embedding space
    , and \texttt{embed-soft} is a soft matching in .}
  \label{tab:eval:volley-matching}
  \vspace{-0.25cm}
\end{table}

\begin{table}[ht!]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Method                    & collective & individual  \\\hline
\texttt{boxes} MRF  & 82.0 & 68.6  \\
\texttt{boxes} NMS  & 77.0 & 68.1  \\\hline
\texttt{embed} MRF  & \textbf{87.1} & \textbf{77.9}  \\
\texttt{embed} NMS  & 85.2 & 76.2 \\\hline
\texttt{embed-soft} MRF  & 86.2 & 77.4 \\  \texttt{embed-soft} NMS  & 85.1 & 75.7 \\\hline
\end{tabular}
\end{center}
\caption{Comparative results of detection schemes on the \texttt{volleyball}
  dataset. We report the average accuracy for the collective and individual action
  recognition.}
\label{tab:eval:volley-detection}
\vspace{-0.15cm}
\end{table}


We also conducted experiments to see if our joint detection using MRF is
beneficial, and compare it to the traditional non-maxima
suppression, both operating on the same dense detection maps. The results for various
matching strategies are given in Table~\ref{tab:eval:volley-detection}. For all of them, 
our joint probabilistic inference leads to better accuracy than
non-maxima suppression.

\subsection{Multi-Person Detection}

For completeness, we also conducted experiments for multi-person detection using
our dense proposal network followed by a hybrid MRF. Our main competitor is the
 \texttt{ReInspect} algorithm~\cite{Stewart2016}, which was specifically designed for
 joint multi-person detection. We trained and tested our model on the \texttt{brainwash}
dataset~\cite{Stewart2016}, which contains more than 11000 training and 500 testing
images, where people are labeled by bounding boxes around their heads. 
The dataset includes some highly crowded scenes in which there are a large
number of occlusions.

Many of the bounding boxes are extremely small and thus have very little image
evidence, however, our approach allows us to simultaneously look at different
feature scales to tackle this issue. We use 5 convolutional
maps of the original Inception-v3 architecture to construct our dense
representation . We do not tune any parameters on the validation set,
keeping them the same as for \texttt{volleyball} dataset. 

\begin{figure}[ht!]
\vspace{-0.25cm}
\begin{center}
\includegraphics[width=0.38\textwidth]{figures/brainwash-pr.pdf}
\vspace{0.1cm}
\begin{tabular}{|l|c|c|}
\hline
Method & AP & EER \\\hline
\texttt{Overfeat}~\cite{Sermanet2013} & 0.67 & 0.71 \\
\texttt{Faster-RCNN}~\cite{Ren2015} & 0.79 & 0.80 \\
\texttt{ReInspect}~\cite{Stewart2016} & 0.78 & 0.81 \\
\texttt{ReInspect-rezoom}~\cite{Stewart2016} & \textbf{0.89} & 0.85 \\
\texttt{OURS} & 0.88 & \textbf{0.87} \\\hline
\end{tabular}
\end{center}
\caption{Results for multi-person detection on the \texttt{brainwash}~\cite{Stewart2016}
  dataset (better viewed in color). 
  Our model outperforms most of the widely used baselines, and performs on 
  par with the state-of-the-art \texttt{ReInspect-rezoom}~\cite{Stewart2016}. }
\label{fi:eval:detection-brainwash}
\end{figure}

In Figure~\ref{fi:eval:detection-brainwash} we report average precision
(AP) and equal error rate (EER)~\cite{Everingham2015}, along with the
precision-recall curves. We outperform most of the existing
detection algorithms, including widely adopted \texttt{Faster-RCNN}~\cite{Ren2015},
by a large margin, and perform very similarly to \texttt{ReInspect-rezoom}.
One of the benefits of our detection method with respect to the
\texttt{ReInspect}, is that our approach is not restricted only to detection, 
and can be also used for instance-level segmentation.

 \vspace{-0.1cm}
\section{Conclusions}
\vspace{-0.15cm}
We have proposed a unified model for joint detection and activity recognition
of multiple people. Our approach does not require any external ground truth
detections nor tracks, and demonstrates state-of-the-art performance 
both on multi-person scene understanding and detection datasets. 
Future work will apply the proposed framework to explicitly capture 
and understand human interactions. 

 
\newpage
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
