\documentclass[accepted]{uai2021} \pdfoutput=1


\usepackage[american]{babel}


\usepackage{natbib} \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} \usepackage{booktabs} \usepackage{tikz} 

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amssymb,amsmath,bm}
\usepackage{multirow}
\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subcaption}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{}}



\newcommand{\swap}[3][-]{#3#1#2} 



\title{An Effective  Baseline for Robustness to Distributional Shift}

\author[1,3]{\href{mailto:Sunil Thulasidasan <sunil@lanl.gov>?Subject=DAC OoD paper}{Sunil~Thulasidasan}{}}
\author[2]{Sushil~Thapa}{}
\author[1]{Sayera~Dhaubhadel}
\author[1]{Gopinath~Chennupati}
\author[1]{Tanmoy~Bhattacharya}
\author[3]{Jeff~Bilmes}
\affil[1]{Los Alamos National Laboratory\\
    Los Alamos, NM, USA
}
\affil[2]{Dept. of Computer Science and Engineering\\
    New Mexico Tech\\
    Socorro, NM, USA
}

\affil[3]{Dept. of Electrical \& Computer Engineering\\
    University of Washington\\
    Seattle, USA
}


\begin{document}
\maketitle

\begin{abstract}
Refraining from confidently predicting when faced with  categories of inputs different from those seen during training is an important requirement for the safe deployment of deep learning systems. While simple to state, this has been  a particularly challenging problem in deep learning, where models often end up making overconfident predictions in such situations. In this work we present a simple, but highly effective approach to deal with out-of-distribution detection that uses the principle of abstention: when encountering a sample from an unseen class, the desired behavior is to abstain from predicting. Our  approach uses a network with an extra abstention class and is trained on a dataset that  is augmented with an uncurated  set that consists of a large number of out-of-distribution (OoD) samples that are assigned the label of the abstention class; the model is then trained to learn an effective discriminator between in and out-of-distribution samples. We  compare this relatively simple approach against a wide variety of more complex methods that have been proposed both for out-of-distribution detection as well as uncertainty modeling in deep learning, and empirically demonstrate its effectiveness on a wide variety of of benchmarks and deep architectures for image recognition and text classification, often outperforming existing approaches by significant margins. Given the simplicity and effectiveness of this method, we propose that this approach be used as a new additional baseline for future work in this domain.
\end{abstract}

\section{Introduction and Related Work}
\label{sec:intro}



Most of supervised machine learning has been developed with the assumption that the distribution of classes seen at train and test time are the same. However, the real-world is unpredictable and open-ended, and making machine learning systems robust to the presence of unknown categories and out-of-distribution samples has become increasingly essential for their safe deployment. While refraining from predicting when uncertain should be intuitively obvious to humans, the peculiarities of DNNs makes them overconfident to unknown inputs~\cite{nguyen2015deep} and makes this a challenging problem to solve in deep learning.

A very active sub-field of  deep learning, known as  \textit{out-of-distribution} (OoD) detection,
has emerged in recent years that attempts to impart  to deep  neural networks the quality of  "knowing when it doesn't know".   
The most straight-forward approach in this regard is based on using the DNNs output as a proxy for predictive confidence. For example, a simple baseline for detecting OoD samples using thresholded softmax scores was presented in ~\cite{hendrycks2016baseline}. 
where the authors provided empirical evidence that for DNN classifiers, in-distribution predictions do tend to have higher winning scores than OoD samples, thus empirically justifying the use of softmax thresholding as a useful baseline. However this approach is vulnerable to the pathologies discussed in~\cite{nguyen2015deep}.  Subsequently, increasingly sophisticated methods have been developed to attack the OoD problem. \cite{liangenhancing} introduced a detection technique that involves perturbing the inputs in the direction of increasing the confidence of the network's predictions on a given input, based on the observation that the magnitude of gradients on in-distribution data tend to be larger than for OoD data. 
The method proposed in ~\cite{lee2018simple} also involves input perturbation, but confidence in this case was measured by the Mahalanobis distance score using the computed mean and co-variance of the pre-softmax scores.  A drawback of such methods, however, is that it introduces  a number of hyperparameters that need to be tuned on the OoD dataset, which is infeasible in many real-world scenarios as one  does not often know in advance the properties of unknown classes. A modified version of the perturbation approach was recently proposed in in~\cite{hsu2020generalized} that circumvents some of these issues, though one still needs to ascertain an ideal perturbation magnitude, which might not generalize from one OoD set to the other.
 
Given that one might expect a classifier to be more uncertain when faced with OoD data,  many methods developed for estimating uncertainty for DNN predictions have also been used for OoD detection. A useful baseline in this regard is the temperature scaling method of~\cite{guo2017calibration} that was was proposed for calibrating DNN predictions on in-distribution data and has been observed to also serve as a useful OoD detector in some scenarios. Further, label smoothing  techniques like \textit{mixup}~\cite{zhang2017mixup} have also been shown to be able to improve OoD detection performance in DNNs~\cite{thulasidasan2019mixup}.
An ensemble-of-deep models approach, that is also augmented with adversarial examples during training, described in~\cite{lakshminarayanan2017simple} was also shown to improve predictive uncertainty and successfully applied to OoD detection.

In the Bayesian realm, methods such as~\cite{maddox2019simple} and ~\cite{osawa2019practical} have also been used for OoD detection, though at increased computational cost.  However, it has been argued that for OoD detection,  Bayesian priors on the data are not completely justified since one does not have access to the prior of the open-set~\cite{boult2019learning}. Nevertheless, simple approaches like dropout -- which have been shown to be equivalent to deep Gaussian processes~\cite{gal2016dropout} -- have been used as baselines for OoD detection.

Training the model to recognize unknown classes by using data from categories that do not overlap with classes of interest has been shown to be quite effective for out-of-distribution detection and a  slew of methods that use additional data for discriminating between ID and OD data have been proposed.
~\cite{devries2018learning} describes a method that uses
 a separate confidence branch
and misclassified  training data samples that serve as a proxy for OoD samples.
In the outlier exposure technique described in ~\cite{hendrycks2018deep}, the predictions on natural outlier images used in training are regularized against the uniform distribution to encourage high-entropy posteriors on outlier samples.
An approach that uses an extra-class for outlier samples is described in ~\cite{neal2018open}, where instead of natural outliers, \textit{counterfactual} images that lie just outside the class boundaries of known classes are generated using a GAN and assigned the extra class label. A similar approach using generative samples for the extra class, but using a conditional Variational Auto-Encoders~\cite{kingma2013auto} for generation, is described in~\cite{vernekar2019out}. A method to force a DNN to produce high-entropy (i.e., low confidence)  predictions and suppress the magnitude of feature activations for OoD samples was discussed in~\cite{dhamija2018reducing}, where, arguing that methods that use an extra background class for OoD samples force all such samples to lie in one region of the feature space,  the work also  forces separation by suppressing the activation magnitudes of samples from unknown classes 






The above works have shown that the use of \textit{known} OoD samples (or \textit{known unknowns}) often generalizes well to \textit{unknown unknown} samples. Indeed,  even though the space of unknown classes is potentially infinite, and one can never know in advance the myriad of inputs that can occur during test time, empirically this approach  has been shown to work. 
The abstention method that we describe in the next section borrows ideas from many of the above methods: as in~\cite{hendrycks2018deep}, we uses additional samples of real images and text from non-overlapping categories to train the model to abstain, but instead of entropy regularization over OoD samples, out method  \textit{uses an extra abstention class}.
While it has been sometimes argued in the literature that that using an additional abstention (or rejection) class is not an effective approach for OoD detection~\cite{dhamija2018reducing,lee2017training}, comprehensive experiments we conduct in this work demonstrate that this is not the case. Indeed,  we find that such an approach is not only simple but also highly effective for OoD detection, often outperforming existing methods that are more complicated and involve tuning of multiple hyperparameters.  The main contributions of this work are as follows:
\begin{itemize}
		\item To the best of our knowledge, this is the first work to comprehensively demonstrate the efficacy of using an extra abstention (or rejection class) in combination with outlier training data for effective OoD detection.
		\item In addition to being effective, our method is also simple: we introduce no additional hyperparameters in the loss function, and train with regular cross entropy. From a practical standpoint, this is especially useful for deep learning practitioners who might not wish to make modifications to the loss function while training deep models. In addition, since outlier data is simply an additional training class, no architectural modifications to existing networks are needed.
		\item Due to the simplicity and effectiveness of this method, we argue that this approach  be considered a strong baseline for comparing new methods in the field of OoD detection.
	\end{itemize}


\section{Out-of-Distribution Detection with an Abstaining Classifier (DAC)}
\label{sec:openset_det}

Our approach uses a DNN trained  with an extra \textit{abstention} class for detecting out-of-distribution and novel samples; from here on, we will refer to this as the deep abstaining classifier (DAC).  We augment our training set of in-distribution samples () with an auxiliary dataset of \textit{known} out-of-distribution samples (), that are known to be mostly disjoint from the main training set (we will use  to denote \textit{unknown} out-of-distribution samples that we use for testing).  We assign the training label of  to all the outlier samples in  (where  is the number of known classes) and train with cross-entropy; the minimization problem then becomes:

where  are the weights of the neural network. This is somewhat similar to the approaches described in~\cite{hendrycks2018deep} as well as in~\cite{lee2017training}, with the main difference being that in those methods, an extra class is not used; instead predictions on outliers are regularized against the uniform distribution. 
Further the loss on the outlier samples is weighted by a hyperparameter  which has to be tuned; in contrast, our approach does not introduce any additional hyperparameters. 

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\columnwidth]{uq_figs/openset/dnn_tiny_imagenet_ood_separation.pdf}
\label{fig:dnn_dist}
	\end{subfigure}
\hspace{0.1in}
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\columnwidth]{uq_figs/openset/dac_tiny_imagenet_ood_separation.pdf}
\label{fig:dac_dist}
	\end{subfigure}
\vspace{-0.2in}
	\caption{An illustration of the separability of  scores on in and out-of-distribution data for a regular DNN (left) and the DAC (right).}\label{fig:openset_score_dist}
\end{figure}


In our experiments, we find that the presence of an abstention class that is used to capture the mass in  significantly increases the ability to detect  during testing. For example, in Figure~\ref{fig:openset_score_dist}, we show the distribution of the winning logits (pre-softmax activations) in a regular DNN (left). For the same experimental setup, the \textit{abstention logit} of the DAC produces near-perfect separation of the in and out-of-distribution logits indicating that using an abstention class for mapping outliers can be a very effective approach to OoD detection.  Theoretically, it might be argued that the abstention class might only capture data that is aligned with the weight vector of that class, and thus this approach might fail to detect the myriad of OoD inputs that might span the entire input region. Comprehensive experiments over a wide variety of benchmarks described in the subsequent section, however,  empirically demonstrate that while the detection is not perfect, it performs very well, and indeed, much better than more complicated approaches.
 


Once the model is trained, we use a simple thresholding mechanism for detection. Concretely, the detector,  assigns label 1 (OoD) if the softmax score of the abstention class, i.e.,  is above some threshold , and label , otherwise:

Like in other methods, the threshold  has to be determined based on acceptable risk that might be specific to the application. However, using performance metrics like area under the ROC curve (AUROC), we can determine threshold-independent performance of various methods, and we use this as one of our evaluation metrics in all our experiments. We note here that recent work in~\cite{mohseni2020self} attacks the OoD problem along similar lines by using a rejection class and training on outlier data; however the results in this paper are much more comprehensive since we compare not only to a wide variety of OoD methods, but also with techniques that focus on predictive uncertainty in deep learning. Further, we also demonstrate the effectiveness of an abstention class using only a small fraction of the outlier data that was used in~\cite{mohseni2020self}. 




\section{Experiments}
The experiments we describe here can be divided into two sets: in the first set, we compare against methods that are explicitly designed for OoD detection, while in the second category, we compare against methods that are known to improve predictive uncertainty in deep learning. In both cases, we report  results over a variety of architectures to demonstrate the efficacy of our method.

\subsection{Datasets}
For all computer vision experiments, we use CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning} as the in-distribution datasets, in addition to augmenting our training set with K unlabeled samples from the Tiny Images dataset~\cite{torralba200880}.~For the out-of-distribution datasets, we test on the following:
\begin{itemize}
	\item \textbf{SVHN}~\cite{netzer2011reading}, a large set of  color images of house numbers, comprising of ten classes of digits . We use a subset of the 26K images in the test set.
	
	\item \textbf{LSUN}~\cite{yu2015lsun}, the Large-scale Scene Understanding dataset, comprising of 10 different types of scenes.
	
	\item \textbf{Places365}~\cite{zhou2017places}, a large collection of pictures of scenes that fall into one of 365 classes. 
	
	\item\textbf{ Tiny ImageNet}~\cite{tiny_imagenet} (not to be confused with Tiny Images)  which consists of images belonging to 200 categories that are a subset of ImageNet categories. The images are  color, which we scale down to  when testing.
	
	\item \textbf{Gaussian} A synthetically generated dataset consisting of  random Gaussian noise images, where each pixel is sampled from an i.i.d Gaussian distribution.
\end{itemize}
For the NLP experiments, we use 20 Newsgroup \cite{20ng}, TREC \cite{trec}, and SST \cite{sst} datasets as our in-distribution datasets, which are the same as those used by \cite{hendrycks2018deep} to facilitate direct comparison. 
We use the 50-category version of TREC, and for SST, we use binarized labels where neutral samples are removed.
For out OoD training data, we use unlabeled samples from Wikitext2 by assigning them to the abstention class. We test our model on the following OoD datasets:
\begin{itemize}
	\item \textbf{SNLI}~\cite{snli} is a dataset of predicates and hypotheses for natural language inference. We use the hypotheses for testing .
	
	\item \textbf{IMDB}~\cite{imdb} is a sentiment classification dataset of movie reviews, with similar statistics to those of SST.
	
	\item \textbf{Multi30K}~\cite{multi30k} is a dataset of English-German image descriptions, of which we use the English descriptions.
	
	\item\textbf{WMT16}~\cite{wmt16} is a dataset of English-German language pairs designed for machine translation task. We use the English portion of the test set from WMT16.
	
	\item \textbf{Yelp}~\cite{yelp} is a dataset of restaurant reviews.
\end{itemize}


\subsection{Comparison against OoD Methods}

In this section, we compare against a slew of recent state-of-the-art methods that have been explicitly designed for OoD detection. For the image experiments,  we compare against the following:

\begin{itemize}
	
	\item \textbf{Deep Outlier Exposure}, as described in~\cite{hendrycks2018deep} and discussed in Section~\ref{sec:intro}
	
	\item \textbf{Ensemble of Leave-out Classifiers} ~\cite{vyas2018out} where each classifier is trained by leaving out a random subset of training data (which is treated as OoD data), and the rest is treated as ID data.
	
	\item \textbf{ODIN}, as described in~\cite{liangenhancing} and discussed in Section~\ref{sec:intro}. ODIN uses input perturbation and temperature scaling to differentiate between ID and OoD samples. 

	\item \textbf{Deep Mahalanobis Detector}, proposed in~\cite{lee2018simple} which estimates the class-conditional distribution over hidden layer features of a deep model using Gaussian discriminant analysis and a Mahalanobis distance based confidence-score for thresholding, and further, similar to ODIN, uses input perturbation while testing.
	
	\item \textbf{OpenMax}, as described in~\cite{bendale2016towards} for novel category detection. This method uses mean activation vectors of ID classes observed during training followed by Weibull fitting to determine if a given sample is novel or out-of-distribution.
\end{itemize}

 For all of the above methods, we use published results when available, keeping the architecture and datasets the same as in the experiments described in the respective papers. For the NLP experiments, we only compare against the published results in~\cite{hendrycks2018deep}. For OpenMax, we re-implement the authors' published algorithm using the PyTorch framework~\cite{paszke2019pytorch}.

\subsubsection{Metrics}
Following established practices in the literature, we use the following metrics to measure detection performance of our method:
\begin{itemize}
	\item \textbf{AUROC} or Area Under the Receiver Operating Characteristic curve depicts the relationship between the True Positive Rate (TPR) (also known as Recall)and the False Positive Rate (FPR) and can be interpreted as the probability that a positive example is assigned a higher detection score than a negative example~\cite{fawcett2006introduction}. 
Unlike 0/1 accuracy, the AUROC has the desirable property that it is not affected by class imbalance\footnote{An alternate area-under-the-curve metric, known as Area under Precision Recall Curve, or AUPRC, is used when the size of the negative class is high compared to the positive class. We do not report AUPRC here, as we keep our in-distribution and out-of-distribution sets balanced in these experiments.}. 
	
	\item \textbf{FPR at 95\% TPR} which is  the probability that a negative sample is misclassified as a positive sample when the TPR (or recall) on the positive  samples is 95\%.
	
\end{itemize}

In work that we compare against, the out-of-distribution samples are treated as the positive class, so we do the same here, and treat the in-distribution samples as the negative class.

\subsubsection{Results}
\begin{table*}[htbp]
	\vspace*{-0.5in}
	\resizebox{0.8\textwidth}{!}{
		\begin{tabular}{cclclclcl}
			& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}vs. Outlier Exposure (OE)~\cite{hendrycks2018deep}\\ (Model: Wide ResNet 40x2)\end{tabular}}                                                                                                   \\ \hline
			\multicolumn{1}{l|}{}                    & \multicolumn{4}{c|}{: CIFAR-10}                                                                            & \multicolumn{4}{c}{: CIFAR-100}                                                                           \\ \hline
			\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{FPR95 }            & \multicolumn{2}{c|}{AUROC }               & \multicolumn{2}{c|}{FPR95 }                       & \multicolumn{2}{c}{AUROC }    \\
			\multicolumn{1}{c|}{}                    & OE   & \multicolumn{1}{c|}{Ours}                   & OE   & \multicolumn{1}{c|}{Ours}                    & OE            & \multicolumn{1}{c|}{Ours}                     & OE            & \multicolumn{1}{c}{Ours} \\ \hline
			\multicolumn{1}{c|}{SVHN}                & 4.8  & \multicolumn{1}{l|}{\textbf{2.0}}  & 98.4 & \multicolumn{1}{l|}{\textbf{99.46}} & \textbf{42.9} & \multicolumn{1}{l|}{\textbf{40.46}} & \textbf{86.9} & \textbf{85.44}  \\
			\multicolumn{1}{c|}{LSUN}                & 12.1 & \multicolumn{1}{l|}{\textbf{0.1}}  & 97.6 & \multicolumn{1}{l|}{\textbf{99.96}} & 57.5          & \multicolumn{1}{l|}{\textbf{9.27}}   & 83.4          & \textbf{97.67}  \\
			\multicolumn{1}{c|}{Places365}           & 17.3 & \multicolumn{1}{l|}{\textbf{0.22}} & 96.2 & \multicolumn{1}{l|}{\textbf{99.92}} & 49.8          & \multicolumn{1}{l|}{\textbf{23.37}}  & 86.5          & \textbf{93.98}  \\
			\multicolumn{1}{c|}{Gaussian}            & 0.7  & \multicolumn{1}{l|}{\textbf{0.13}} & 99.6 & \multicolumn{1}{l|}{\textbf{99.93}} & \textbf{12.1} & \multicolumn{1}{l|}{\textbf{13.26}} & \textbf{95.7} & \textbf{90.03}  \\





\hline
	\end{tabular}}
	
	\vspace*{0.2in}
	\resizebox{0.8\textwidth}{!}{
		\begin{tabular}{cllllllll}
			& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}vs.  Ensemble of Leave-out Classifiers (ELOC)~\cite{vyas2018out}\\ (Model: Wide ResNet 28x10)\end{tabular}}                                                                                                  \\ \hline
			\multicolumn{1}{l|}{}                    & \multicolumn{4}{c|}{: CIFAR-10}                                                                                               & \multicolumn{4}{c}{: CIFAR-100}                                                                            \\ \hline
			\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{FPR95 }                     & \multicolumn{2}{c|}{AUROC }                         & \multicolumn{2}{c|}{FPR95 }                       & \multicolumn{2}{c}{AUROC }      \\
			\multicolumn{1}{c|}{}                    & ELOC          & \multicolumn{1}{c|}{Ours}                   & ELOC           & \multicolumn{1}{c|}{Ours}                    & ELOC           & \multicolumn{1}{c|}{Ours}                    & ELOC           & \multicolumn{1}{c}{Ours} \\ \hline
			\multicolumn{1}{c|}{Tiny ImageNet}       & \textbf{2.94} & \multicolumn{1}{l|}{\textbf{1.91}} & \textbf{99.36} & \multicolumn{1}{l|}{\textbf{99.45}} & \textbf{24.53} & \multicolumn{1}{l|}{\textbf{18.68}} & \textbf{95.18} & \textbf{94.88}  \\
			\multicolumn{1}{c|}{LSUN}                & \textbf{0.88} & \multicolumn{1}{l|}{\textbf{1.5}}  & \textbf{99.7}  & \multicolumn{1}{l|}{\textbf{99.61}} & 16.53          & \multicolumn{1}{l|}{\textbf{9.23}}  & 96.77          & \textbf{97.89}  \\
			\multicolumn{1}{c|}{Gaussian}            & \textbf{0.0}  & \multicolumn{1}{l|}{\textbf{0.13}} & 99.58          & \multicolumn{1}{l|}{\textbf{99.95}} & 98.26          & \multicolumn{1}{l|}{\textbf{0.72}}  & 93.04          & \textbf{99.65}  \\ \hline
	\end{tabular}}
	
	\vspace*{0.2in}
	\resizebox{0.8\textwidth}{!}{
		\begin{tabular}{cllllllll}
			& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}vs.  ODIN~\cite{liangenhancing}\\ (Model: Wide ResNet 28x10)\end{tabular}}                                                                                                                              \\ \hline
			\multicolumn{1}{l|}{}                    & \multicolumn{4}{c|}{: CIFAR-10}                                                                                              & \multicolumn{4}{c}{: CIFAR-100}                                                                     \\ \hline
			\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{FPR95 }                    & \multicolumn{2}{c|}{AUROC }                         & \multicolumn{2}{c|}{FPR95 }                     & \multicolumn{2}{c}{AUROC } \\
			\multicolumn{1}{c|}{}                    & ODIN         & \multicolumn{1}{c|}{Ours}                   & ODIN           & \multicolumn{1}{c|}{Ours}                    & ODIN         & \multicolumn{1}{c|}{Ours}                    & ODIN    & \multicolumn{1}{c}{Ours}   \\ \hline
			\multicolumn{1}{c|}{Tiny ImageNet}       & 25.5         & \multicolumn{1}{l|}{\textbf{1.91}} & 92.1           & \multicolumn{1}{l|}{\textbf{99.45}} & 55.9         & \multicolumn{1}{l|}{\textbf{18.68}} & 84.0    & \textbf{94.88}    \\
			\multicolumn{1}{c|}{LSUN}                & 17.6         & \multicolumn{1}{l|}{\textbf{1.5}}  & 95.4           & \multicolumn{1}{l|}{\textbf{99.61}} & 56.5         & \multicolumn{1}{l|}{\textbf{9.23}}  & 86.0    & \textbf{97.89}    \\
			\multicolumn{1}{c|}{Gaussian}            & \textbf{0.0} & \multicolumn{1}{l|}{\textbf{0.13}} & \textbf{100.0} & \multicolumn{1}{l|}{\textbf{99.95}} & \textbf{1.0} & \multicolumn{1}{l|}{\textbf{0.72}}  & 98.5    & \textbf{99.65}    \\ \hline
	\end{tabular}}
	
	\vspace*{0.2in}
	\resizebox{0.8\textwidth}{!}{
		\begin{tabular}{cllllllll}
			& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}vs.  Deep Mahalanobis Detector (MAH)~\cite{lee2018simple}\\ (Model: ResNet 34)\end{tabular}}                                                                                     \\ \hline
			\multicolumn{1}{l|}{}                    & \multicolumn{4}{c|}{: CIFAR-10}                                                                            & \multicolumn{4}{c}{: CIFAR-100}                                                                 \\ \hline
			\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{FPR95 }            & \multicolumn{2}{c|}{AUROC }               & \multicolumn{2}{c|}{FPR95 }             & \multicolumn{2}{c}{AUROC }     \\
			\multicolumn{1}{c|}{}                    & MAH  & \multicolumn{1}{c|}{Ours}                   & MAH  & \multicolumn{1}{c|}{Ours}                    & MAH  & \multicolumn{1}{c|}{Ours}                    & MAH           & \multicolumn{1}{c}{Ours} \\ \hline
			\multicolumn{1}{c|}{SVHN}                & 24.2 & \multicolumn{1}{l|}{\textbf{1.89}} & 95.5 & \multicolumn{1}{l|}{\textbf{99.49}} & 58.1 & \multicolumn{1}{l|}{\textbf{41.31}} & 84.4 & \textbf{86.85}  \\
			\multicolumn{1}{c|}{Tiny ImageNet}       & 4.5  & \multicolumn{1}{l|}{\textbf{0.36}} & 99.0 & \multicolumn{1}{l|}{\textbf{99.88}} & 29.7 & \multicolumn{1}{l|}{\textbf{12.10}} & 87.9          & \textbf{97.14}  \\
			\multicolumn{1}{c|}{LSUN}                & 1.9  & \multicolumn{1}{l|}{\textbf{0.30}} & 99.5 & \multicolumn{1}{l|}{\textbf{99.91}} & 43.4 & \multicolumn{1}{l|}{\textbf{7.14}}  & 82.3          & \textbf{98.45}  \\ \hline
	\end{tabular}}
	
		\vspace*{0.2in}
	\resizebox{0.8\textwidth}{!}{
	
	\begin{tabular}{cllllllll}
			& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}vs.  OpenMax~\cite{bendale2016towards}\\ (Model: ResNet 34)\end{tabular}}                                                                                                                                                                                                                           \\ \hline
			\multicolumn{1}{l|}{}                    & \multicolumn{4}{c|}{: CIFAR-10}                                                                                                                    & \multicolumn{4}{c}{: CIFAR-100}                                                                                                                                       \\ \hline
			\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{FPR95 }                       & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}AUROC \\ \end{tabular}} & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}FPR95 \\ \end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}AUROC \\ \end{tabular}} \\
			\multicolumn{1}{c|}{}                    & OpenMax         & \multicolumn{1}{c|}{Ours}                   & OpenMax                  & \multicolumn{1}{c|}{Ours}                             & OpenMax                   & \multicolumn{1}{c|}{Ours}                              & OpenMax                                & \multicolumn{1}{c}{Ours}               \\ \hline
			\multicolumn{1}{c|}{SVHN}                & 23.67  & \multicolumn{1}{l|}{\textbf{1.89}} & 90.72           & \multicolumn{1}{l|}{\textbf{99.49}}          & 53.22            & \multicolumn{1}{l|}{\textbf{41.31}}           & 80.88                & \textbf{86.85}                \\
			\multicolumn{1}{c|}{Tiny ImageNet}       & 24.20  & \multicolumn{1}{l|}{\textbf{0.36}} & 93.39           & \multicolumn{1}{l|}{\textbf{99.88}}          & 32.67            & \multicolumn{1}{l|}{\textbf{12.10}}           & 81.22                         & \textbf{97.14}                \\
			\multicolumn{1}{c|}{LSUN}                & 18.68  & \multicolumn{1}{l|}{\textbf{0.30}} & 92.16           & \multicolumn{1}{l|}{\textbf{99.91}}          & 30.21            & \multicolumn{1}{l|}{\textbf{7.14}}            & 83.08                         & \textbf{98.45}                \\
			\multicolumn{1}{c|}{Places365}           & 27.27  & \multicolumn{1}{l|}{\textbf{0.84}} & 90.72           & \multicolumn{1}{l|}{\textbf{99.67}}          & 50.71            & \multicolumn{1}{l|}{\textbf{30.54}}           & 81.13                         & \textbf{92.69}                \\
			\multicolumn{1}{c|}{Gaussian}            & 40.58 & \multicolumn{1}{l|}{\textbf{0.04}} & 84.74          & \multicolumn{1}{l|}{\textbf{99.98}}          & 21.50           & \multicolumn{1}{l|}{\textbf{1.66}}            & 89.37                         & \textbf{99.48}                \\ \hline
	
	
	\end{tabular}}
	
	\caption{Comparison of the extra class method (ours) with various other out-of-distribution detection methods when trained on CIFAR-10 and CIFAR-100 and tested on other datasets. All numbers from comparison methods are sourced from their respective original publications. For our method, we also report the standard deviation over five runs (indicated by the subscript), and treat the performance of other methods within one standard deviations as equivalent to ours. For fair comparison with the Mahalanobis detector (MAH)~\cite{lee2018simple}, we use results when their method was not tuned separately on each OoD test set (Table 6 in~\cite{lee2018simple}. The OpenMax implementation was based on code available at \texttt{https://github.com/abhijitbendale/OSDN}  and re-implemented by us in PyTorch~\cite{paszke2019pytorch}.}
	\label{tab:vs_ood_methods}
\end{table*}





 
 
 
 
 \begin{table}[htbp]
 \resizebox{\textwidth}{!}{
\begin{tabular}{lcllll}
\multicolumn{6}{c}{\begin{tabular}[c]{@{}c@{}}vs.  Outlier Exposure  for NLP Classification\\ (Model: 2 Layered GRU)\end{tabular}}                                                                                                                       \\ \hline
\multicolumn{1}{l|}{}            & \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{FPR95 }                                & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}AUROC \\ \end{tabular}} \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{l|}{}                    & \multicolumn{1}{c}{OE}   & \multicolumn{1}{c|}{Ours}                   & OE                               & \multicolumn{1}{c}{Ours}                     \\ \hline
\multicolumn{1}{l|}{\multirow{5}{*}{20 Newsgroup}} & \multicolumn{1}{c|}{SNLI}                & \multicolumn{1}{c}{12.5} & \multicolumn{1}{l|}{\textbf{3.9}}  & 95.1                             & \textbf{98.32.49}                    \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{IMDB}                & \multicolumn{1}{c}{18.6} & \multicolumn{1}{l|}{\textbf{1.78}} & 93.5                             & \textbf{99.17}                       \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{Multi30K}            & \multicolumn{1}{c}{3.2}  & \multicolumn{1}{l|}{\textbf{0.80}} & 97.3                             & \textbf{99.52}                      \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{WMT16}               & \multicolumn{1}{c}{2.0}  & \multicolumn{1}{l|}{\textbf{1.4}}  & 98.8                             & \textbf{99.33}                      \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{Yelp}                & \multicolumn{1}{c}{3.9}  & \multicolumn{1}{l|}{\textbf{0.76}} & 97.8                             & \textbf{99.61}                      \\ \hline
\multicolumn{1}{l|}{\multirow{5}{*}{TREC}}         & \multicolumn{1}{c|}{SNLI}                & \textbf{4.2}             & \multicolumn{1}{l|}{12.0}           & \textbf{98.1}                    & 97.03                               \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{IMDB}                & 0.6                      & \multicolumn{1}{l|}{\textbf{0.0}}   & 99.4                             & \textbf{99.99}                       \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{Multi30K}            & \textbf{0.3}             & \multicolumn{1}{l|}{8.3}            & \textbf{99.7}                    & 97.56                                \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{WMT16}               & \textbf{0.2}             & \multicolumn{1}{l|}{4.67}          & 99.8                             & \textbf{99.94}                       \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{Yelp}                & 0.4                      & \multicolumn{1}{l|}{\textbf{0.0}}   & \textbf{99.7}                    & 99.0                                 \\ \hline
\multicolumn{1}{l|}{\multirow{5}{*}{SST}}          & \multicolumn{1}{c|}{SNLI}                & 33.4                     & \multicolumn{1}{l|}{\textbf{20.9}}  & 86.8                             & \textbf{92.24}                      \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{IMDB}                & 32.6                     & \multicolumn{1}{l|}{\textbf{0.7}}  & 85.9                             & \textbf{99.37}                       \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{Multi30K}            & \textbf{33.0}            & \multicolumn{1}{l|}{70.9}           & \textbf{88.3}                    & 70.65                                \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{WMT16}               & \textbf{17.1}            & \multicolumn{1}{l|}{31.6}           & \textbf{92.9}                    & 90.64                                \\
\multicolumn{1}{l|}{}                              & \multicolumn{1}{c|}{Yelp}                & 11.3                     & \multicolumn{1}{l|}{\textbf{0.06}}  & 92.7                             & \textbf{99.67}                      \\ \hline
\end{tabular}}
\caption{DAC vs OE for NLP Classification task. OE implementation was based on code available at \url{https://github.com/hendrycks/outlier-exposure}}.  
 	\label{tab:vs_OE_NLP}
\end{table}
 

Detailed results against the various OoD methods are shown in Tables~\ref{tab:vs_ood_methods} through ~\ref{tab:vs_OE_NLP} for vision and language respectively, where we have a clear trend: in almost all cases, the DAC outperforms the other methods, often by significant margins especially when the in-distribution data is more complex, as is the case with CIFAR-100. While the Outlier Exposure method~\cite{hendrycks2018deep} (shown at  the top in Table~\ref{tab:vs_ood_methods}) is conceptually similar to ours, the presence of an extra abstention class in our model often bestows significant performance advantages. Further, we do not need to tune a separate hyperparameter which determines the weight of the outlier loss, as done in ~\cite{hendrycks2018deep}.

In fact, the simplicity of our method is one of its striking features: we do not introduce any additional hyperparameters in our approach, which makes it significantly easier to implement than methods such as ODIN and the Mahalanobis detector; these methods  need to be tuned separately on each OoD dataset, which is usually not possible as one does not have access to the distribution of unseen classes in advance. Indeed, when performance of these methods is tested without tuning on the OoD test set, the DAC significantly outperforms methods such as the Mahalanobis detector (shown at the bottom of  Table~\ref{tab:vs_ood_methods}).   We also show the performance against the OpenMax approach of~\cite{bendale2016towards} and in every case, the DAC outperforms OpenMax by significant margins. 

While the abstention approach uses an extra class and OoD samples while training, and thus does incur some training overhead, it is significantly less expensive during test time, as the forward pass is  no different from that of a regular DNN. In contrast, methods like ODIN and the Mahalanobis detector require gradient calculation with respect to the input in order to apply the  input perturbation; the DAC approach thus offers a computationally simpler alternative.  Also, even though the DAC approach introduces additional network parameters in the final linear layers (due to the presence of an extra abstention class), and thus might be more prone to overfitting, we find that this to be not the case as evidenced by the generalization of OoD performance to different types of test datasets.

\subsection{Comparison against Uncertainty-based Methods}
Next we perform experiments to compare the OoD detection performance of the DAC against various methods that have been proposed for improving predictive uncertainty in deep learning. In these cases, one expects that such methods will cause the DNN to predict with less confidence when presented with inputs from a different distribution or from novel categories; we compare against the following methods:

\begin{itemize}
	\item \textbf{Softmax Thresholding} This is the simplest baseline, where OoD samples are detected by thresholding on the winning softmax score; scores falling \textit{below} a threshold are rejected. 


	\item \textbf{Entropy Thresholding} Another simple baseline, where OoD samples are rejected if the Shannon entropy calculated over the softmax posteriors is \textit{above} a certain threshold.
	
	\item \textbf{MonteCarlo Dropout} A Bayesian inspired approach proposed in~\cite{gal2016dropout} for improving the predictive uncertainty for deep learning. 
We found a dropout probability of  to perform well, and use  forward passes per sample during the prediction. 

	\item \textbf{Temperature Scaling}, which improves DNN calibration as described in~\cite{guo2017calibration}. The scaling temperature  is tuned on a held-out subset of the validation set of the in-distribution data.
	
	\item \textbf{Mixup} As shown in~\cite{thulasidasan2019mixup}, Mixup can be an effective OoD detector, so we also use this as one of our baselines.
	
	\item \textbf{Deep Ensembles} which was introduced in~\cite{lakshminarayanan2017simple} for improving uncertainty estimates for both classification and regression. In this approach, multiple versions of the same model are trained using different random initializations, and while training, adversarial samples are generated to improve model robustness.
We use an ensemble size of  as suggested in their paper.
	
	\item \textbf{SWAG}, as described in~\cite{maddox2019simple}, which is a Bayesian approach to deep learning and exploits the fact that SGD itself can be viewed as approximate Bayesian inference~\cite{mandt2017stochastic}. 
We use an ensemble size of  as proposed in the original paper.
	
\end{itemize}






\subsubsection{Results}

\begin{table*}[htbp]
	\resizebox{\textwidth}{!}{
\begin{tabular}{lllclllll}
	& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}: CIFAR-100 \\ AUROC  \end{tabular}}                                                                                                                                                                                                                                                                                                                                                                                    \\ \hline
	\multicolumn{1}{l|}{} & \multicolumn{1}{c}{\textbf{Softmax}} & \multicolumn{1}{c}{\textbf{Entropy}} & \textbf{\begin{tabular}[c]{@{}c@{}}Monte Carlo\\ Dropout\end{tabular}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Temp \\ Scaling\end{tabular}}} & \multicolumn{1}{c}{\textbf{Mixup}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Deep\\  Ensemble\end{tabular}}} & \multicolumn{1}{c}{\textbf{SWAG}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}DAC \\ (Ours)\end{tabular}}} \\
	\multicolumn{1}{l|}{LSUN}                & 87.86                       & 89.53                       & 84.35                                                         & 87.75                                                                       & 86.03                     & 89.36                                                                        & 83.87                    & \textbf{98.73}                                                            \\
	\multicolumn{1}{l|}{Places-365}          & 79.77                       & 80.34                       & 80.21                                                         & 79.53                                                                       & 82.09                     & 82.69                                                                        & 83.37                    & \textbf{93.39}                                                            \\
	\multicolumn{1}{l|}{Gaussian}            & 80.76                       & 80.46                       & 68.40                                                        & 80.52                                                                       & 92.13                     & 80.27                                                                        & 91.80                    & \textbf{99.69}                                                            \\
	\multicolumn{1}{l|}{SVHN}                & 78.45                       & 79.43                       & 81.04                                                         & 78.25                                                                       & 79.15                     & 82.42                                                                        & 80.45                    & \textbf{87.74}                                                            \\
	\multicolumn{1}{l|}{Tiny ImageNet}       & 86.63                       & 88.00                       & 82.07                                                         & 86.49                                                                       & 84.57                     & 86.64                                                                        & 80.90                    & \textbf{97.60}                                                            \\ \hline
	& \multicolumn{8}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\
	& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}: CIFAR-100 \\ FPR95  \end{tabular}}                                                                                                                                                                                                                                                                                                                                                                                  \\ \hline
	\multicolumn{1}{l|}{} & \multicolumn{1}{c}{\textbf{Softmax}} & \multicolumn{1}{c}{\textbf{Entropy}} & \textbf{\begin{tabular}[c]{@{}c@{}}Monte Carlo\\ Dropout\end{tabular}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Temp \\ Scaling\end{tabular}}} & \multicolumn{1}{c}{\textbf{Mixup}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Deep \\ Ensemble\end{tabular}}} & \multicolumn{1}{c}{\textbf{SWAG}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}DAC \\ (Ours)\end{tabular}}} \\
	\multicolumn{1}{l|}{LSUN}                & 35.64                       & 34.52                       & \multicolumn{1}{l}{42.93}                                     & 35.64                                                                       & 34.52                     & 32.48                                                                        & 40.50                    & \textbf{5.41}                                                             \\
	\multicolumn{1}{l|}{Places-365}          & 55.19                       & 55.81                       & \multicolumn{1}{l}{56.50}                                     & 55.19                                                                       & 55.81                     & 46.55                                                                        & 47.76                    & \textbf{29.59}                                                            \\
	\multicolumn{1}{l|}{Gaussian}            & 33.70                       & 32.82                      & \multicolumn{1}{l}{41.34}                                    & 33.70                                                                       & 32.82                    & 23.85                                                                        & 13.61                   & \textbf{0.89}                                                             \\
	\multicolumn{1}{l|}{SVHN}                & 55.28                      & 55.16                      & \multicolumn{1}{l}{50.08}                                     & 55.28                                                                      & 55.16                    & 47.57                                                                        & 49.43                    & \textbf{40.44}                                                            \\
	\multicolumn{1}{l|}{Tiny ImageNet}       & 37.27                       & 36.65                       & \multicolumn{1}{l}{47.10}                                     & 37.27                                                                       & 36.65                     & 38.58                                                                        & 45.05                    & \textbf{10.15}                                                            \\ \hline
\end{tabular}}
	
	\vspace*{0.2in}
	\resizebox{\textwidth}{!}{
\begin{tabular}{lllclllll}
	& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}: CIFAR-10 \\ AUROC \end{tabular}}                                                                                                                                                                                                                                                                                                                                    \\ \hline
	\multicolumn{1}{l|}{} & \multicolumn{1}{c}{\textbf{Softmax}} & \multicolumn{1}{c}{\textbf{Entropy}} & \textbf{\begin{tabular}[c]{@{}c@{}}Monte Carlo\\ Dropout\end{tabular}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Temp \\ Scaling\end{tabular}}} & \multicolumn{1}{c}{\textbf{Mixup}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Deep \\ Ensemble\end{tabular}}} & \multicolumn{1}{c}{\textbf{SWAG}} & \multicolumn{1}{c}{\textbf{DAC}} \\
	\multicolumn{1}{l|}{LSUN}                & 93.45                       & 93.84                       & 91.56                                                         & 93.99                                                                       & 96.58                     & 93.44                                                                        & 96.28                    & \textbf{99.92}          \\
	\multicolumn{1}{l|}{Places-365}          & 91.70                       & 92.08                       & 89.29                                                         & 92.39                                                                       & 96.38                     & 94.78                                                                        & 96.07                    & \textbf{100.00}         \\
	\multicolumn{1}{l|}{Gaussian}            & 79.39                      & 79.22                      & 94.26                                                         & 80.40                                                                       & 95.62                     & 95.82                                                                        & 96.51                    & \textbf{100.00}         \\
	\multicolumn{1}{l|}{SVHN}                & 91.38                       & 91.70                       & 83.56                                                         & 91.99                                                                       & 92.62                     & 90.58                                                                        & 96.12                    & \textbf{99.50}          \\
	\multicolumn{1}{l|}{Tiny ImageNet}       & 91.98                       & 92.31                       & 88.84                                                         & 92.59                                                                       & 95.67                     & 94.07                                                                        & 95.65                    & \textbf{99.88}          \\ \hline
	& \multicolumn{8}{l}{}  \\
	& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}: CIFAR-10 \\ FPR95  \end{tabular}} \\ \hline
	\multicolumn{1}{l|}{} & \multicolumn{1}{c}{\textbf{Softmax}} & \multicolumn{1}{c}{\textbf{Entropy}} & \textbf{\begin{tabular}[c]{@{}c@{}}Monte Carlo\\ Dropout\end{tabular}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Temp\\  Scaling\end{tabular}}} & \multicolumn{1}{c}{\textbf{Mixup}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Deep \\ Ensemble\end{tabular}}} & \multicolumn{1}{c}{\textbf{SWAG}} & \multicolumn{1}{c}{\textbf{DAC}} \\
	\multicolumn{1}{l|}{LSUN}                & 19.15                       & 18.93                       & \multicolumn{1}{l}{25.86}                                     & 18.76                                                                       & 12.47                     & 19.36                                                                        & 12.71                    & \textbf{0.27}           \\
	\multicolumn{1}{l|}{Places-365}          & 26.49                       & 26.42                       & \multicolumn{1}{l}{33.65}                                     & 26.37                                                                       & 14.77                     & 15.79                                                                        & 12.80                    & \textbf{0.00}           \\
	\multicolumn{1}{l|}{Gaussian}            & 52.84                      & 53.52                      & \multicolumn{1}{l}{12.63}                                     & 55.91                                                                      & 10.96                    & 9.15                                                                         & 9.55                     & \textbf{0.00}           \\
	\multicolumn{1}{l|}{SVHN}                & 24.64                       & 24.67                       & \multicolumn{1}{l}{53.32}                                    & 25.06                                                                       & 29.71                    & 24.99                                                                        & 13.04                    & \textbf{1.78}           \\
	\multicolumn{1}{l|}{Tiny ImageNet}       & 26.21                      & 26.24                      & \multicolumn{1}{l}{33.25}                                    & 24.96                                                                       & 14.84                     & 17.03                                                                        & 12.07                    & \textbf{0.33}           \\ \hline
	
	
	
	& \multicolumn{8}{l}{}  \\
	& \multicolumn{8}{c}{\begin{tabular}[c]{@{}c@{}}: SST \\ FPR95  \end{tabular}} \\ \hline
	\multicolumn{1}{l|}{} & \multicolumn{1}{c}{\textbf{Monte Carlo Dropout}} & \multicolumn{1}{c}{\textbf{Entropy}} & \textbf{\begin{tabular}[c]{@{}c@{}}Monte Carlo\\ Dropout\end{tabular}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Temp\\  Scaling\end{tabular}}} & \multicolumn{1}{c}{\textbf{Mixup}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Deep \\ Ensemble\end{tabular}}} & \multicolumn{1}{c}{\textbf{SWAG}} & \multicolumn{1}{c}{\textbf{DAC}} \\
	\multicolumn{1}{l|}{SNLI}                & 19.15                       & 18.93                       & \multicolumn{1}{l}{25.86}                                     & 18.76                                                                       & 12.47                     & 19.36                                                                        & 12.71                    & \textbf{0.27}           \\
	\multicolumn{1}{l|}{IMDB}          & 26.49                       & 26.42                       & \multicolumn{1}{l}{33.65}                                     & 26.37                                                                       & 14.77                     & 15.79                                                                        & 12.80                    & \textbf{0.00}           \\
	\multicolumn{1}{l|}{Multi30K}            & 52.84                      & 53.52                      & \multicolumn{1}{l}{12.63}                                     & 55.91                                                                      & 10.96                    & 9.15                                                                         & 9.55                     & \textbf{0.00}           \\
	\multicolumn{1}{l|}{WMT16}                & 24.64                       & 24.67                       & \multicolumn{1}{l}{53.32}                                    & 25.06                                                                       & 29.71                    & 24.99                                                                        & 13.04                    & \textbf{1.78}           \\
	\multicolumn{1}{l|}{Yelp Reviews}       & 26.21                      & 26.24                      & \multicolumn{1}{l}{33.25}                                    & 24.96                                                                       & 14.84                     & 17.03                                                                        & 12.07                    & \textbf{0.33}           \\ \hline
	
\end{tabular}}
	
	\caption{The performance of DAC as an OoD detector, evaluated on various metrics and compared against competing baselines. All experiments used the ResNet-34 architecture, except for MC Dropout, in which case we used the WideResNet 28x10 network.  and   indicate that higher and lower values are better, respectively. Best performing methods (ignoring statistically insignificant differences)on each metric are in bold.}
	\label{tab:openset_cifar_uncertainty_results}
\end{table*}

Detailed results are shown in Table ~\ref{tab:openset_cifar_uncertainty_results}, where the best performing method for each metric is shown in bold.  The DAC is the only method in this set of experiments that uses an augmented dataset, and as is clearly evident from the results, this confers a significant advantage over the other methods in most cases. 
Calibration methods like temperature scaling, while producing well calibrated scores on in-distribution data, end up reducing the confidence on in-distribution data as well, and thus losing discriminative power between the two types of data. We also note here that many of the methods listed in the table, like temperature scaling and deep ensembles, can be combined with the abstention approach.  Indeed, the addition of an extra abstention class and training with OoD data is compatible with most uncertainty modeling techniques in deep learning; we leave the exploration of such combination approaches for future work.






\section{Conclusion}

We presented a simple, but highly effective method for open set and out-of-distribution detection that clearly demonstrated the efficacy of using an extra abstention class and augmenting the training set with outliers. While previous work has shown the efficacy of outlier exposure~\cite{hendrycks2018deep}, here we demonstrated an alternative approach for exploiting outlier data that further improves upon existing methods, while also being simpler to implement compared to many of the other methods. The ease of implementation, absence of additional hyperparameter tuning and computational efficiency during testing makes this a very viable approach for improving out-of-distribution and novel category detection in real-world deployments; we hope that this will also serve as an effective baseline for comparing future work in this domain.








\bibliography{uai2021-template}



\end{document}
