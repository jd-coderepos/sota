\chapter{Outcomes \vs original intents: a retrospective}
\label{chap:outcomes}

The research on these topics was funded based on multiple research
proposals and statements of intent over time. In this section, I
explore \emph{published} motivations, where the scientific community has
agreed via the publication approval process that the motivations and
justifications were worthwhile.
 
Although most scientific projects are expected to diverge from their
original intents, published objectives reveal the general motivation
that drives the effort. Note that I do not focus here on
``result-oriented'' papers that report factually on research outcomes;
only on those publications that made statements of intent and
motivation before the corresponding work was carried out.

\section{I. Bell, N. Hasasneh \& C. Jesshope, JPP 2006}

Here we explore the contributions originally advertised in~\cite{bell.06.jpp}.

\subsection{Summary of the article}

Abstract:
\begin{quote}
\itshape Chip multiprocessors (CMPs) hold great promise for achieving
scalability in future systems. Microthreaded CMPs add a means of
exploiting legacy code in such systems. Using this model, compilers
generate parametric concurrency from sequential source code, which
can be used to optimise a range of operational parameters such as
power and performance over many orders of magnitude, given a scalable
implementation. This paper shows scalability in performance, power
and most importantly, in silicon implementation, the main contribution
of this paper. The microthread model requires dynamic register
allocation and a hardware scheduler, which must support hundreds of
microthreads per processor. The scheduler must support thread
creation, context switching and thread rescheduling on every machine
cycle to fully support this model, which is a significant
challenge. Scalable implementations of such support structures are
given and the feasibility of large-scale CMPs is investigated by
giving detailed area estimate of these structures.
\end{quote}

Problem statement:
\begin{quote}\itshape
In general, there are only a few requirements for the design of
efficient and powerful general-purpose CMPs, these are: scalability of
performance, area and power with issue width, and programmability from
legacy sequential code. Issue width is defined here as the number of
instructions issued on chip simultaneously, whether in a single
processor or in multiple processors and no distinction is made
here. To meet these requirements a number of problems must be solved,
including the extraction of ILP from legacy code, managing locality,
minimising global communication, latency tolerance, power-efficient
instruction execution strategies (i.e. avoiding speculation),
effective power management, workload balancing, and finally, the
decoupling of remote and local activity to allow for an asynchronous
composition of synchronous processors. Most CMPs address only some of
these issues as they attempt to reuse elements of existing processor
designs, ignoring the fact that these are suitable only for chips with
relatively few cores.
\end{quote}

Proposed main contribution:
\begin{quote}
\bfseries\itshape
In this paper a CMP is evaluated, that is based on microthreading, which addresses either directly or indirectly, all of the above issues and, theoretically, provides the ability to scale systems to very large number of processors.
\end{quote}

\subsection{Analysis}

The ``CMP'' in the paper's text refers to the D-RISC/Microgrid technology. Does this paper, and all the research
since then (8 years) support the claim that it solves ``all of the above issues''?
We list here how the technology addresses the issues in decreasing order of success.

\paragraph{Extraction of ILP from legacy code:} this was successful. ILP is extracted implicitly by D-RISC's dataflow scheduler, although
the ILP width is limited by the number of active threads and the number of registers per thread, because the reordering
information is stored in registers.

\paragraph{Decoupling of remote and local activity:} this is mostly successful, insofar the D-RISC's TMU control protocol
has different primitives to spawn work locally and remotely. 

\paragraph{Scalability of performance, area and power with issue width:} each D-RISC core uses a single-issue pipeline,
so this claim states that performance, area and power scales with the number of cores. The research has indeed
shown this to be true for large, regular, data-parallel computation kernels, but the picture is not so clear for
small or more heterogeneous workloads because of the inter-core latencies in the concurrency management protocol itself
and on-chip network contention due to cache coherency protocols.

\paragraph{Power-efficient instruction execution strategies:} this is only partly successful; with one thread
the execution is not too power efficient compared to traditional
single-issue, in-order designs, and efficiency only increases with the number of threads
active. The problem is that operand availability is only tested at the
read stage of the pipeline. When there are multiple threads,
annotations at the fetch stage prevent ``potentially suspending''
instructions from entering the pipeline, but if only one thread is
active, the instructions with missing operands will still enter the
pipeline, create a bubble after the read stage, and subsequently need
to be rescheduled. This is a form of speculation, thus inherently less
power-efficient than the traditional approach to stall the pipeline during
issue.


\paragraph{Latency tolerance:} this is only partly successful. The latency of intra-core operations is tolerated by intra-thread ILP, but then the same is possible with conventional barrel processors or out-of-order execution\footnote{Albeit possibly at a larger area and power budget. However, the actual area and power requirement of D-RISC are yet to be evaluated.}. Longer latencies can be tolerated as long as there are sufficient threads active on the core to interleave with a waiting thread. If most active threads are busy communicating, then latency tolerance is highly dependent on a full end-to-end support for split-phase transactions, \ie the rest of the system must support
a large number of in-flight transactions. In practice, the caches and
external memory interfaces become a bottleneck, and to this day no clear solution has emerged on this front. 

\paragraph{Minimising global communication:} this is only mildly successful. For synchronization and concurrency management, global communication is limited by constraining the TMU's automatic workload distribution to adjacent cores only. The responsibility of choosing an area of the chip where to start the distribution is left to an hypothetical resource manager, not yet researched/implemented. Next to this, one should also consider memory communication. Here all results so far use memory protocols that incur global communication for coherency. No results show yet that global memory communication has been minimized.

\paragraph{Workload balancing:} this is only very mildly successful. D-RISC's TMU can automatically spread a batch of threads to multiple cores using an even N/P distribution, but this is the only form of distribution supported. Due to bulk reuse and synchronization, this simple distribution causes irrecoverable imbalances as soon as the batch is heterogeneous. 

\paragraph{Managing locality:} this is not achieved, insofar that the data used by instructions is invisible to D-RISC's ``intelligence'' (its hardware TMU), and the memory and core networks are not topologically congruent. The software has to negociate locality of code and data explicitly with knowledge of the chip's layout.

\paragraph{Programmability from legacy sequential code:} to this date, most existing sequential code cannot
be reused as-is with D-RISC/Microgrids, because of incomplete support
for operating system services, \cf also \cref{sec:incompat}.

\paragraph{Effective power management:} to this date, power management has not been explored.
 
\section{NWO Microgrids: 2006-2010}

\subsection{Research question}

The project NWO Microgrids was funded by the Dutch government based on
the following research question:

\begin{quote} \itshape
Is it possible, through the introduction of simple and explicit concurrency controls, to develop a systematic approach to:
\begin{enumerate}
\item incrementally designing new processor architectures (i.e. based on an existing ISA and infrastructure);
\item dynamically managing and optimising the available resources for a variety of goals such as performance, power and reliability (i.e. resulting in autonomous and self-adaptive microgrids);
\item formally defining the architectures' execution properties; 
\item incrementally developing the architectures' infrastructure (i.e. simulators, compilers, binary-to-binary translators and even silicon intellectual property); 
\end{enumerate}
all within the context of ten to fifteen years of silicon-technology scaling (i.e. maintaining scalability over a thousand fold increase in chip density)?
\end{quote}

Note that this is a yes/no question. 

To answer ``yes,'' it suffices to propose at least one set of ``simple
and explicit concurrency controls'' and a corresponding ``systematic
approach'' that delivers on the four other points. Alternatively,
``yes'' can also be given, perhaps less satisfactorily, if a
theoretical analysis indirectly merely proves the systematic approach 
exists (``it is possible to develop it'') without actually developing
it.  To answer ``no,'' in contrast, it is necessary to demonstrate
that there cannot exist any set of ``simple and explicit concurrency
controls'' which makes a ``systematic approach'' possible.

Strategically, this question suggests its own answer:
\begin{itemize}
\item a ``no'' answer would be a formidable theoretical endeavour, likely very
difficult to obtain (possibly impossible within the proposed 10-15
years time frame);
\item a ``yes'' answer based on theoretical proof of existence would be equally difficult;
\item therefore, the question suggests a ``yes'' is expected, hinged
  on the ability of the researchers to use the features of an existing
  architecture as their candidate ``simple and explicit concurrency
  controls,'' and consequently show that a corresponding ``systematic
  approach'' delivers on the 4 other points.
\end{itemize}

In short, NWO Microgrids's ``declaration of scientific intent'' can be reformulated as follows:

\begin{quote} \itshape
We will show that D-RISC/Microgrids make it possible to develop a systematic approach to:
\begin{enumerate}
\item incrementally designing new processor architectures (i.e. based on an existing ISA and infrastructure);
\item dynamically managing and optimising the available resources for a variety of goals such as performance, power and reliability (i.e. resulting in autonomous and self-adaptive microgrids);
\item formally defining the architectures' execution properties; 
\item incrementally developing the architectures' infrastructure (i.e. simulators, compilers, binary-to-binary translators and even silicon intellectual property); 
\end{enumerate}
all within the context of ten to fifteen years of silicon-technology scaling.
\end{quote}

\subsection{Outcome \vs expectations}

Did NWO Microgrids deliver on its self-set expectations?

\paragraph{Did NWO Microgrids deliver a ``systematic approach'' with the desired properties?} No, the design of D-RISC/Microgrids
was instead carried out in an ad-hoc fashion, with multiple phases of
trial-and-error and backtracking. I highlight an ethical issue here,
because on the one hand the Dutch NWO funded a project on the
assumption that its outcome would be a \emph{systematic approach
  (method)} that could be reused with different architectures, and on
the other hand the research team knew well in
advance that systematization would not be studied.

For the sake of deconstruction, let us however stretch the word
``systematic approach'' to encompass ``the process of designing and
building D-RISC/Microgrids.'' Does this extended definition match the
other required properties?

\paragraph{Did the process of designing and building D-RISC/Microgrids incrementally designed a processor architecture, based on an existing ISA and infrastructure?} Here the answer is only partially ``yes'': the design was indeed incremental (starting from a simple, known-to-work RISC pipeline) and used an existing ISA (Alpha), but it did not reuse an existing infrastructure. Instead, all the infrastructure for the project was built from scratch.

\paragraph{Did the process of designing and building D-RISC/Microgrids enabled the dynamic management and optimisation of available resources for a variety of goals such as performance, power and reliability?} The jury is still out on this one; no answer was given yet after many years of research. Even a published doctoral thesis on the topic~\cite{vantol.13}, which merely touched performance-driven resource management, did not yield definite answers. As of this writing, another doctoral candidate is working on the reliability issue, but issues of power are still left untouched.

\paragraph{Did the process of designing and building D-RISC/Microgrids include a formal definition of the architecture's execution properties?} Yes, namely in~\cite{tdvu.07.icfem} and~\cite[Chap.~7]{poss.12}.

\paragraph{Did the process of designing and building D-RISC/Microgrids include an incremental development of the architecture's infrastructure? (i.e. simulators, compilers, binary-to-binary translators and even silicon intellectual property)} Here the answer
is only partially ``yes.'' Simulators and compilers were developed,
and parts of silicon IP (cf.~\cite{danek.12}), however there were no
binary-to-binary translators produced to establish a compatibility
path with existing code, as initially envisioned.

\subsection{Restrospective on the research question}

Since the process of designing D-RISC/Microgrids did not exhibit all
the expected properties, it cannot be used to answer ``yes'' firmly to
that project's research question. However, ``no'' cannot be
confidently given either. In other words, the question is still mostly
left unanswered.

Instead, I can only summarize the situation by proposing that the NWO
simply funded some additional \emph{development} of D-RISC/Microgrids,
and the project's description only merely \emph{guided the development
  process} without pressuring it into delivering scientific output.

Moreover, it is clear to me that the original research question
\emph{was so ill-phrased that it cannot be answered scientifically}: I
cannot see any experimental path that would yield a definite answer
within a reasonable time frame. 

Consequently, any rephrasing would be equally improductive, for example
determine whether there is any \emph{other} set of concurrency
controls that yield a ``yes'' answer on the same research question, or
whether D-RISC/Microgrids can be fixed/enhanced to this aim. 

Therefore, in my opinion, the original phrasing for the project NWO
Microgrids cannot be used to motivate further work in this area.  The
corollary is that researchers should not exploit the past attention
given by NWO's to this question as justification to spend more effort
in this area. If justification is needed, it must be found somewhere
else.

\section{C. Jesshope, APC 2008}

This whitepaper/article~\cite{jesshope.08.apc} made a statement of
intent about the applicability and the aims of D-RISC/Microgrids. It
introduces the ``SVP model,'' an intellectual construction used from
2008 to 2011. SVP intended to abstract the specific inner workings of
D-RISC/Microgrids, keeping only the high-level semantics of its TMU
concurrency management protocol visible to programmers.

\begin{table}
\begin{tabular}{p{.48\textwidth}p{.48\textwidth}}
Problem & Proposed answer \\
\hline \hline
How to effectively program distributed multiprocessor systems & Use SVP's simple concurrency control primitives \\
\hline
How to make architectures that are both efficient and can tolerate a large latency in responding to external events &
Use a combination of native support for dataflow scheduling and split-phase transactions throughout the system, such as found
 in SVP implementations \\
\hline
How to design programming model that is both deterministic and free from deadlock under concurrent composition & Use SVP's strictly
hierarchical concurrency and forward-only communication patterns \\
\hline
How to ensure binary compatibility across a range of implementations from a single processor to the highest level of concurrency a particular application can support & Use SVP's granularity-independent abstraction of concurrency resources \\
\end{tabular}
\caption{Problems purportedly solved by SVP.}\label{tab:mgprob}
\end{table}

According to this article, D-RISC/Microgrids as abstracted in SVP
should have solved the problems listed in \cref{tab:mgprob}. 
Note that this article defined SVP and its benefits \emph{before D-RISC's TMU, and thus Microgrids were fully defined.} 
As it happened, the advertised features of SVP 
ended up \emph{not being implementable in D-RISC/Microgrids}. Specifically:

\begin{itemize}
\item end-to-end asynchrony stops at the chip boundary, both at
the memory and I/O interfaces, and these latencies cannot be fully
tolerated;
\item the lack of hardware mechanisms to virtualize resources prevented the
proper implementation of deadlock-free composition.
\end{itemize}

Moreover, although binary compatibility is possible across chip
technologies, the execution performance of the code ended up not being
portable between different number of cores and interconnect
topologies. 

Since 2011, when D-RISC's TMU was well-enough defined that it was
both obviously \emph{different from and more powerful than SVP's abstractions},
the SVP model has been downplayed and is not a central component of
publications any more.

\section{EU Apple-CORE: 2008-2011}

The project Apple-CORE was funded by the European Union based on a
statement of intent via an \emph{abstract}, and via a list of explicit 
\emph{objectives}. There was no ``research question'' per se, as
the goal of the project was to build infrastructure and show
that the objectives were reached as a consequence.

\subsection{Summary of outcomes}

To an outsider, 
the EU seemed to have funded research to develop a new general-purpose processor,
that would extend and possibly even replace the technology currently
in use in commodity hardware. Had that objective been reached, the
project would have been disruptive indeed. This \emph{potential} to
both disrupt the state of the art and advance technology in a way
largely beneficial to society was sufficient, to the proposal's
initial reviewers, to justify the investment.

However, there is an ethical issue at hand. First, the Apple-CORE
proposal stated that D-RISC and Microgrids were already
designed and a code generator for TC was available prior
to the start of the project, whereas it was known to the authors of the proposal that
this was not true. 

Only after the first year, after reviewers had been induced to believe
the issues were minor, did it become clear that the EU was also
funding this prerequisite technology. Also, at that point there was no
evidence that this ``initial'' technology would be sufficient to
research all the project's original objectives in a timely fashion.
Indeed, what happened is that the overhead of producing these
prerequisites prevented the consortium from exploring all the issues.

In short, Apple-CORE \emph{did not actually have the potential to
  disrupt the state of the art and advance technology in the way
  announced using the budget requested}. Besides, the details of scientific
outcomes (\cf next section) do not reveal any strong evidence that the
Apple-CORE technology can replace existing processors (\cf also \cref{sec:incompat}). 

\paragraph{Disclaimer}
{\itshape The results and circumstances described below have been
brought to the attention of the project's reviewers while
the project was ongoing, and the project was judged successful
by both the reviewers and the project officer \emph{despite} these
issues. As I have learned since then, most
large, publicly-funded projects suffer from more serious issues and
the issues described here pale in comparison.}

\subsection{Outcomes \vs project objectives}

I present below how the project's outcome, as can be observed at the
end of 2012, relates to each stated objective in the project's
description of work. I list the objectives in decreasing order of
success, and mark with ``'' those
points that most diverge from the overall initial goal of the project.

\paragraph{Apple-CORE will investigate the support structures in 
implementing the SVP model in the LEON 3 processor and develop an SVP soft-core prototype.}
This was achieved, and was quite successful~\cite{danek.10.ddecs,sykora.11.lncs,danek.12}.

\paragraph{Apple-CORE will investigate the integration of instruction-set extensions to support custom accelerators based on both microthreads and families of microthreads.}
This was achieved, and was quite successful~\cite{danek.12}.

\paragraph{Apple-CORE will explore the gains of SVP in the context of data-parallel 
programming, investigate the implications of functional concurrency and explore the possible design space.}
This was achieved, and was quite successful~\cite{a-c-d44}.

\paragraph{Apple-CORE will support many-core processors by 
capturing concurrency systematically using 
instructions in the processors’ ISA and by dynamically mapping 
and scheduling that concurrency in the processors’ implementation (the SVP model).}
This was achieved~\cite{poss.12.dsd}.

\paragraph{Apple-CORE will derive a set of loop transformations to transform iterative computations into a combination of independent and dependent families of threads respecting the communication restrictions in the SVP model.}
This was achieved~\cite{saougkos.09.cpc,saougkos.11}.

\paragraph{Apple-CORE will extract task, loop and, implicitly in the SVP model, instruction level concurrency in the parallelising C compiler.}
This was only partially achieved: only loop and ILP was extracted. Task concurrency wasn't.

\paragraph{

Apple-CORE will provide binary-code compatibility across generations of multi-cores from few- to many-cores.}
This was achieved, although Apple-CORE also showed that code that performs well on
a small number of cores typically does not scale (performance- and
efficiency-wise) to large number of cores. Conversely, code that runs
with interesting speedups on large number of cores do not run
efficiently on small number of cores. The binary compatibility is thus
merely functional: it is possible to run the same code and obtain the
same results, but the performance is not portable. One can easily
argue that this form of compatibility is not really what was desired.

\paragraph{

Apple-CORE will implement and evaluate memory models and coherency protocols for many- core systems.}
This was achieved, only to conclude that the proposed models and
protocols were cumbersome to use, inefficient and otherwise
detrimental to performance for any configuration larger than 30-60
cores.

\paragraph{

Apple-CORE will study the resource management issues that are exposed in exploiting massive concurrency as it arises from data-parallel or functional program specifications.}
This was achieved: the management issues were indeed studied, but only
to conclude that the Apple-CORE strategy did not significantly
simplify the problem, which is otherwise shared by all research projects
in this field.



\paragraph{

Apple-CORE will provide high-level programming environments that
improve the programming productivity and automate the generation of
concurrency, or at least separate the concerns of concurrent
programming from its implementation, i.e. automate all scheduling and
synchronisation.}  This was only partly achieved. By funding extra
development on Single-Assignment C, Apple-CORE did indeed ``improve
the programming productivity and automate the generation of
concurrency.'' However this effort was not directly related to
D-RISC/Microgrids: the improvements on SaC are portable to any
parallel hardware supported by the SaC compiler. Furthermore
Apple-CORE did not ``separate the concerns of concurrent programming
from its implementation,'' and neither did it ``automate all
synchronization.''

\paragraph{

Apple-CORE will investigate and implement memory protection and security issues for many- core systems.}
This was investigated but not implemented.

\paragraph{

Apple-CORE will implement a port of a micro-kernel operating system onto one or more of the processor platforms (emulation and/or soft core).}
This was not investigated and not achieved, \cf also \cref{sec:incompat}.

\paragraph{
Apple-CORE will promote the TC language as a standard front-end
to the gcc compiler and will use it as a target for all user-level
compiler development.}  This did not occur, and TC is not being
used any more, for the reasons presented in~\cite[App.~G]{poss.12}. Instead,
the project used another front-end to D-RISC/Microgrids called SL~\cite{poss.12.sl}, which
is riddled with practical limitations and has yet to gain credentials in the scientific community.

\paragraph{

Apple-CORE will investigate and evaluate programming productivity issues for the tools developed.}
This was not investigated nor evaluated.

\paragraph{

Apple-CORE will select a range of benchmark applications of interest to potential users of the SVP model within the European computer industry.}
This was only partly achieved: the industrial participation in the project was low, therefore the relevance of the resulting benchmark selection
cannot be confidently ascertained.

\paragraph{Apple-CORE will build an infrastructure of tools that will enable the SVP model to be evaluated and adopted by the European Computer Industry.}
This objective is untestable. Although an infrastructure of tools was produced, no adoption by the European Computer Industry has yet occurred.



\subsection{Outcomes \vs intents in the project's abstract}

The project's abstract also declared research intents not
covered otherwise in the objectives. I review them here:

\paragraph{The benefits are large, [...] as compilers need only capture
concurrency in a virtual way rather than capturing, mapping and
scheduling it.}
These benefits were not observed. Instead, Apple-CORE taught us is not sufficient to capture concurrency;
some semantics that brings an intuition of the machine back to the programmer
was necessary after all.

\paragraph{This separates the
concerns of programming and concurrency engineering and opens the door
for successful parallelising compilers.}  There was no breakthrough in
programming and concurrency engineering during Apple-CORE, and no
``successful parallelising compiler'' has been produced as a
result. Technically, there were parallelising compilers produced, but
they are not yet ``successful'' insofar they have not yet gathered any
user base other than their own developers.

\paragraph{Particular benefits can be expected for data-parallel
and functional programming languages as they expose their concurrency
in a way that can be easily captured by a compiler.}
This was indeed shown, although this can be equally shown using
most parallel platforms in this day and age.

\paragraph{Another advantage
of this approach is the binary compatibility the new processor has
with the modified ISA. [...]  Once code is compiled with the new
tools, binary-code is executable on an arbitrary numbers of processors
and hence provides future binary-code compatibility.} See above: although
the code is binary-compatible, the performance is not portable.

\paragraph{The concurrency controls also allow for management of
partial failure, which together with the binary-code compatibility
provide the necessary support for reliable systems.}  This was
not shown in practice by Apple-CORE, although Apple-CORE's
infrastructure does simplify a research project on this topic,
started later on.

\paragraph{

Finally, this
approach exposes information about the work to be executed on each
processor and how much can be executed at any given time.
This
information can provide powerful mechanisms for the management of
power by load balancing processors based on clock/frequency
scaling. }
This was not researched in Apple-CORE.

\paragraph{

  In particular, the binary compatibility provides a unique
  opportunity to make an impact on commodity processors in Europe.}
This was not achieved, as there is no binary compatibility with
existing processors. Actually, the argument of ``binary
compatibility'' throughout the project proposal diverges from usual
expectations. ``Binary compatibility'' is usually understood to mean
``backward compatibility with existing binary code,'' meaning that
existing software from other platforms can be reused on the new
platform. However, Apple-CORE instead promotes ``binary compatibility
between multiple instances of the Apple-CORE technology,'' \ie no
binary backward compatibility with other platforms.


\section{C. Jesshope et al., ParCo 2009}

This article~\cite{jesshope.09.parco} rephrased the motivation
behind the D-RISC/Microgrid work, two years in the Apple-CORE project:

\begin{quote}
\itshape In a more general market [than embedded and special-purpose
  accelerators], the labour-intensive approach of hand mapping an
application is not feasible, as the effort required is large and
compounded by the many different applications. {\bfseries A more automated
approach from the tool chain is necessary. This investment in the tool
chain, in turn, demands an abstract target to avoid these
compatibility issues. That target or concurrency model then needs to
be implemented on a variety of platforms to give portability, whatever
the granularity of that platform.  Our experience suggests that an
abstract target should adopt concurrent rather than sequential
composition, but admit a well-defined sequential schedule. It must
capture locality without specifying explicit communication. Ideally,
it should support asynchrony using data-driven scheduling to allow for
high latency operations. However, above all, it must provide safe
program composition, i.e. guaranteed freedom from deadlock when two
concurrent programs are combined. Our SVP model is designed to meet
all of these requirements.} Whether it is implemented in the ISA of a
conventional core, as described here or encapsulated as a software API
will only effect the parameters described above, which in turn will
determine at what level of granularity one moves from parallel to
sequential execution of the same code.
\end{quote}

A few years afterwards, is the D-RISC/Microgrids management protocol
matching the claims? I review them here in decreasing order of
success.

\paragraph{The model should support asynchrony using data-driven scheduling to
allow for high-latency operations.} This was achieved (primary feature of D-RISC).

\paragraph{The model should adopt concurrent rather than sequential composition.}
This was achieved, although \emph{both} concurrent and sequential composition are equally promoted.

\paragraph{The model must admit a well-defined sequential schedule.}
This was mostly achieved~\cite[Chap.~10]{poss.12}. A sequential schedule is not properly defined
as soon as a program manipulates on-chip resources (in particular cores) explicitly.

\paragraph{The model needs to be implemented on a variety of platforms.} This was not achieved. 
A software emulation of D-RISC's \emph{envisioned} TMU was implemented early on~\cite{tol.09.jsa}, but
the actual D-RISC TMU ended up with different semantics which have not yet been implemented elsewhere.

\paragraph{The model must capture locality without specifying explicit communication.}
This was not achieved: explicit communication is required between different threads.

\paragraph{Above all, it must provide safe program composition, i.e. guaranteed freedom from deadlock
when two concurrent programs are combined.}
This was not achieved: the ability of code to manipulate resources explicitly, combined with the lack
of full resource virtualization, may cause compositions to deadlock from resource starvation. 

\section{ASCI 5-year research plan, 2010}

This document was submitted to a consortium of Dutch universities
at the start of 2010, to define the overall research plan of the consortium
over the period 2010-2014.

Over D-RISC/Microgrids this report states:

\begin{quote}
\itshape
Our work on fine-grain threaded architectures with data-driven
scheduling using the SVP concurrency model will continue but we will
also explore software implementations of SVP on other emerging
multi-core architectures such as Niagara and Intel's SCC. This will
allow us to explore multi-grain architecture and develop an
infrastructure to support such an approach. One of the major
directions in this work will be the development of a coherent set of
operating system services that support space sharing in these
heterogeneous environments and yet provide a secure operating
environment that can be scaled from chip-level micro-grids to globally
distributed Grids.  One of the major challenges, especially in
mainstream computing, will be in making these systems programmable
without specialized concurrency knowledge and we have designed
programming language support to express parallel computations and
systems at a very high level of abstraction and developed compilation
technologies that effectively map the abstract descriptions to
concurrent computing environments. We have international
collaborations developing the functional, data parallel language SAC
(Single Assignment C) and the asynchronous co-ordination language
S-Net. Our long-term vision is in the direction of a compilation
infrastructure that automatically adapts running programs derived from
high-level specifications to a heterogeneous and dynamically varying
execution environment based on continuous reflection of execution
parameters.
\end{quote}

To this date, SVP was not implemented on other architectures. No
operating systems services have yet developed that support space
sharing in heterogeneous environments and provide a secure exeuction
environment. The D-RISC/Microgrids language tools are not yet able to
map the abstraction of concurrent resources to maximize performance
and efficiency. ``Automatic adaption of running programs towards
heterogeneous and dynamically varying execution parameters based
on continuous reflection'' was not achieved either yet.

\begin{summary}
\begin{itemize}
\item 
The D-RISC/Microgrids project was purportedly intended to solve major issues in
micro-architecture research, related to scalability in performance and
efficiency in general-purpose microprocessors. 
\item
The strategy to solve these issues was to implement a
combination of dataflow scheduling with hardware support for thread
concurrency management within and across cores on chip. 
\item
Implementation
was carried out, but the results are inconclusive. On the one hand, the proposed
hardware does indeed provide higher performance and efficiency in
regular, data-parallel workloads, but these are also the ``boring''
applications which benefit equally well from vector units or accelerators
in conventional processors. On the other hand, no evidence has yet
been produced that the proposed hardware benefits larger applications
with more irregular workloads. 
\item
Power efficiency and intelligent resource
management was regularly advertised but not actively researched.
\item
Effort has been invested into widening the scope of the technology
towards applications and industrial relevance, but these applications
have not yet materialized.
\end{itemize}
\end{summary}

