
\documentclass{article} \usepackage{iclr2021_conference,times}

\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{comment}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{graphicx}



\usepackage{amsmath,amsfonts,bm,amssymb,mathtools,amsthm}
\usepackage{thm-restate}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}
\newcommand{\bb}[1]{{\mathbb{#1}}}
\newcommand{\norm}[1]{{\lVert {#1} \rVert}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}

\def\tq{{\overline{q}}}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\vpsi{{\bm{\psi}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\def\vpi{{\bm{\pi}}}
\def\vomega{{\bm{\omega}}}
\def\vlambda{{\bm{\lambda}}}
\def\vtheta{{\bm{\theta}}}
\def\vphi{{\bm{\phi}}}
\def\vmu{{\bm{\mu}}}
\def\Hw{{H_\vomega(\vx)}}
\def\Et{{E_\vtheta(\vx)}}
\def\HE{{\Hw + \Et}}
\newcommand{\at}[2][]{#1|_{#2}}
\def\otheta{{\vtheta^*}}
\def\oomega{{\vomega^*}}
\def\ophi{{\vphi^*}}
\def\vJ{{\bm{J}}}
\def\vK{{\bm{K}}}
\def\vP{{\bm{P}}}
\def\vQ{{\bm{Q}}}
\def\vS{{\bm{S}}}
\def\vU{{\bm{U}}}
\def\vW{{\bm{W}}}
\def\vI{{\bm{I}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\DF}{D_f}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\renewcommand{\emptyset}{\varnothing}
\newcommand{\dop}{\mathrm{do}}
\newcommand{\supp}{\mathrm{supp}} 
\usepackage[colorlinks=true,citecolor=brown,urlcolor=gray]{hyperref}
\usepackage{url}


\title{Negative Data Augmentation}

\newcommand{\kristy}[1]{ \color{cyan} [KC: #1] \color{black}}
\newcommand{\sj}[1]{ \color{magenta} [SJ: #1] \color{black}}
\newcommand{\ayush}[1]{ \color{blue} [Ayush: #1] \color{black}}
\newcommand{\burak}[1]{ \color{red} [Burak: #1] \color{black}}
\newcommand{\abhishek}[1]{ \color{purple} [Abhishek: #1] \color{black}}
\newcommand{\js}[1]{{\color{teal} [JS: #1]}}
\newcommand{\s}[1]{{\color{magenta} [SE: #1]}}
\newcommand{\se}[1]{{\color{magenta} [SE: #1]}}





\author{Abhishek Sinha\thanks{Equal Contribution} \\
  \And 
  Kumar Ayush\footnotemark[1] \\
  \And
  Jiaming Song\footnotemark[1] \\
  \And
  Burak Uzkent \\
  \And
  Hongxia Jin \\
  \And
  \hspace{6cm} Stefano Ermon \\
  
  \\
  \normalfont {Department of Computer Science}\\
  \normalfont {Stanford University}\\
  \normalfont {\{a7b23, kayush, tsong, buzkent, ermon\}@stanford.edu}
  \\
  \\
  \normalfont {Samsung Research America}\\
  }



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}






Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore \emph{negative} data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distribution,  and 
can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable conditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved conditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a 
contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image classification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks. \end{abstract}

\section{Introduction}
Data augmentation strategies for synthesizing new data in a way that is consistent with an underlying task are extremely effective in both supervised and unsupervised learning~\citep{oord2018representation,zhang2016colorful,noroozi2016unsupervised,asano2019critical}. Because they operate at the level of samples, they can be combined with most learning algorithms.
They allow for the incorporation of prior knowledge (inductive bias) about properties of typical samples from the underlying data distribution ~\citep{jaiswal2018unsupervised,antoniou2017data}, 
e.g., by leveraging invariances to produce additional ``positive'' examples of how a task should be solved.


To enable users to specify an even wider range of inductive biases,  
we propose to leverage an alternative
and complementary 
source of prior knowledge that specifies how a task should \emph{not} be solved.
We formalize this intuition by assuming access to a way of generating samples that are guaranteed to be out-of-support for the data distribution, which we call a \emph{Negative Data Augmentation} (NDA). 
\begin{comment}
While task-specific, we argue that NDA samples exist and are easy to define for common data modalities such as natural images, which are 
supported on low-dimensional manifolds \citep{fefferman2016testing}.
Focusing on natural images and videos, we provide generic NDA strategies building on known transformations from the literature that intentionally destroy the spatial coherence of an image~\citep{noroozi2016unsupervised,devries2017improved,yun2019cutmix}, such as the Jigsaw puzzle transformation.
\end{comment}
Intuitively, negative out-of-distribution (OOD) samples can 
be leveraged as a useful inductive bias because they provide information about the support of the data distribution to be learned by the model.
For example, in a density estimation problem we can bias the model to avoid putting any probability mass in regions which we know a-priori should have zero probability. This can be an effective prior if the negative samples cover a sufficiently large area. 
The best NDA candidates are ones that expose common pitfalls of existing models, such as prioritizing local structure over global structure~\citep{geirhos2018imagenet}; this motivates us to consider known transformations from the literature that intentionally destroy the spatial coherence of an image~\citep{noroozi2016unsupervised,devries2017improved,yun2019cutmix}, such as Jigsaw transforms. 


Building on this intuition, we introduce a new GAN training objective where we use 
NDA as an additional source of fake data for the discriminator as shown in Fig.~\ref{fig:NDA_fwork}. 
Theoretically, we can show that if the NDA assumption  
is valid, 
optimizing this objective will still recover the data distribution in the limit of infinite data. 
However, in the finite data regime, there is a need to generalize beyond the empirical distribution~\citep{zhao2018bias}. By explicitly providing the discriminator with samples we want to avoid, we are able to bias the generator towards avoiding undesirable samples thus improving generation quality. 

\begin{wrapfigure}{r}{0.33\textwidth}
\vspace{-12pt}
  \begin{center}
    \includegraphics[width=0.33\textwidth]{sections/images/nda_work.png}
  \end{center}
  \vspace{-10pt}
  \caption{Negative Data Augmentation for GANs.}
  \vspace{-13pt}
  \label{fig:NDA_fwork}
\end{wrapfigure}

Furthermore, we propose a way of leveraging NDA for unsupervised representation learning. We propose a new contrastive predictive coding~\citep{he2019momentum,han2019video} (CPC) objective that encourages the distribution of representations corresponding to in-support data to become disjoint from that of NDA data.  
Empirically, we show that applying NDA with our proposed transformations (e.g., forcing the representation of normal and jigsaw images to be disjoint)
improves performance in downstream tasks.



\begin{comment}
Building on this intuition, we apply NDA to generative modeling and contrastive representation learning. 
In adversarial generative models, there is a need to generalize beyond the empirical distribution~\citep{zhao2018bias}, which is only determined implicitly by the model architecture. By explicitly providing NDA samples as an additional source of fake data to the discriminator, we are able to regularize the generator such that it avoids undesirable samples, thus improving the overall generation quality. 
Theoretically, we can show that if the NDA assumption  
is valid, 
optimizing this objective will still recover the data distribution in the limit of infinite data and computational resources. 
In contrastive representation learning, the objective regularizes the representations of samples in the dataset, but not that of samples out of the dataset. This may lead to undesired cases such as the representation of a data is very close to another representation that is not from the dataset.
We introduce NDA in order to alleviate this issue, where we encourage the representations of data in the dataset to be far from that of NDA samples. \burak{I am having difficulty understanding the last two sentences.}


\end{comment}

With appropriately chosen NDA strategies, we obtain superior empirical performance on a variety of tasks, with almost no cost in computation. For generative modeling, models trained with NDA achieve better image generation, image translation and anomaly detection performance compared with the same model trained without NDA. Similar gains are observed on representation learning for images and videos over downstream tasks such as image classification, object detection and action recognition. These results suggest that NDA has much potential to improve a variety of self-supervised learning techniques.
























































 
\section{Negative Data Augmentation}
The input to most 
learning algorithms 
is a dataset of samples from an underlying data distribution . While  is unknown, learning algorithms always rely on prior knowledge about its properties (inductive biases \citep{wolpert1997no}), e.g., by using specific functional forms such as neural networks. Similarly, data augmentation strategies exploit known invariances of , such as the conditional label distribution being invariant to 
semantic-preserving transformations.


While typical data augmentation strategies exploit prior knowledge about what is in support of , 
in this paper, we propose to exploit prior knowledge about what is \emph{not} in the support of . This information is often available for common data modalities (e.g., natural images and videos) and is under-exploited by existing approaches. Specifically, we assume: (1) there exists an alternative distribution  such that its support is disjoint from that of ; and (2) access to a procedure to efficiently sample from . We emphasize  need not be explicitly defined (e.g., through an explicit density) -- it may be implicitly defined by a dataset or by a procedure 
that transforms
samples from 
 into ones from  by suitably altering their structure. 
\begin{comment}
We describe this as negative data augmentation (NDA), which is analogous to (positive) data augmentation where we exploit prior knowledge about what is in support of . 
In particular, NDA is described with the following (informal) definition:
\begin{definition}
\label{def:nda}
A negative data augmentation (NDA) strategy for a distribution  over  is an efficient procedure to sample from a distribution  over  with disjoint support from , where the distribution  should be restricted to an ``informative'' subset of the set of all distributions that are disjoint from . \ayush{included an extension}
\end{definition}
\end{comment}














 
 
 









\begin{figure*}[!h]
\centering
    \includegraphics[width=0.8\textwidth]{sections/images/NDA_augs.pdf}
    \caption{
Negative augmentations produce out-of-distribution samples lacking the typical structure of natural images; these negative samples can be used to inform a model on what it should \emph{not} learn. 
    }
    \label{fig:types}
\end{figure*}



 Analogous to typical data augmentations, NDA strategies are by definition domain and task specific. 
 In this paper, we focus on natural images and videos, and
leave the application to other domains (such as natural language processing) 
as future work.
How do we select a good NDA strategy?
According to the manifold hypothesis \citep{fefferman2016testing}, natural images lie on low-dimensional manifolds:  is supported on a low-dimensional manifold of the ambient (pixel) space. This suggests that many negative data augmentation strategies exist. Indeed, sampling random noise is in most cases a valid NDA. However, while this prior is  generic, it is not very informative, and this NDA will likely be ineffective for most learning problems. Intuitively, NDA is informative if its support is close (in a suitable metric) to that of , while being disjoint. These negative samples will provide information on the ``boundary'' of the support of ,
which we will show is helpful in several learning problems. 
In most of our tasks, the images are processed by convolutional neural networks (CNNs) that are good at processing local features but not necessarily global features~\citep{geirhos2018imagenet}. Therefore, we may consider NDA examples to be ones that preserve local features (``informative'') and break global features, so that it forces the CNNs to learn global features (by realizing NDAs are different from real data).




Leveraging this intuition, we show several image transformations from the literature that can be viewed as generic NDAs over natural images in Figure \ref{fig:types}, that we will use for generative modeling and representation learning in the following sections. Details about these transformations can be found in Appendix \ref{sec:transformation}.
































 

\section{NDA for Generative Adversarial Networks}


\begin{figure*}[!h]
\centering
    \includegraphics[width=0.4\textwidth]{sections/images/normal_clevr1.png}\hspace{2em}
    \includegraphics[width=0.4\textwidth]{sections/images/overview_clevr1.png}
    \caption{Schematic overview of our NDA framework. \textbf{Left}: In the absence of NDA, the support of a generative model  (blue oval) learned from samples (green dots) may ``over-generalize'' and include samples from  or . \textbf{Right}: With NDA, the learned distribution  becomes disjoint from NDA distributions  and , thus pushing   closer to the true data distribution  (green oval).
    As long as the prior is consistent, i.e. the supports of  and  are truly disjoint from , the best fit distribution in the infinite data regime does not change. 
}
    \label{fig:overview}
\end{figure*}

In GANs, we are interested in learning a generative model  from samples drawn from some data distribution ~\citep{goodfellow2014generative}.
GANs use a binary classifier, the so-called discriminator , to distinguish real data from generated (fake) samples. The generator  is trained 
via the following mini-max objective that performs variational Jensen-Shannon divergence minimization:

This is a special case to the more general variational -divergence minimization objective~\citep{nowozin2016f}. The optimal  for any  is , so the discriminator can serve as a density ratio estimator between  and . 



With sufficiently expressive models and infinite capacity,   will match . 
In practice, however, we have access to finite datasets and limited model capacity. 
This means that the generator needs to generalize beyond the empirical distribution, which is challenging because the number of possible discrete distributions scale \emph{doubly exponentially} w.r.t. to the data dimension.
Hence, as studied in~\citep{zhao2018bias}, the role of the inductive bias is critical.
For example, \cite{zhao2018bias} report that when trained on images containing 2 objects only, GANs and other generative models can sometimes 
``generalize''
by generating images with 1 or 3 objects (which were never seen in the training set).
The generalization behavior -- which may or may not be desirable -- is determined by factors such as network architectures, hyperparameters, etc., and is difficult to characterize analytically. 

Here we propose to bias the learning process
by directly specifying what the generator should \textit{not} generate through NDA. 
We consider an adversarial game based on the following objective:

where the negative samples are generated from a mixture of  (the generator distribution) and  (the NDA distribution); the mixture weights are controlled by the hyperparameter . 
Intuitively, this can help addresses the above ``over-generalization'' issue, as we can directly provide supervision on what should not be generated and thus guide the support of  (see Figure~\ref{fig:overview}) . For instance, in the object count example above, 
we can empirically prevent the model from generating images with an undesired number of objects (see Appendix Section A for experimental results on this task). 













In addition, the introduction of NDA samples will not affect the solution of the original GAN objective in the limit.
In the following theorem, we show that given infinite training data and infinite capacity discriminators and generators, using NDA will not affect the optimal solution to the generator, \textit{i.e.} the generator will still recover the true data distribution. 


\begin{restatable}{theorem}{thmgan}
Let  be any distribution over  with disjoint support than , \textit{i.e.}, such that . Let  be the set of all discriminators over ,  be a convex, semi-continuous function such that ,  be the convex conjugate of ,  its derivative, and  be a distribution with sample space . Then , we have:

where  is the objective for -GAN~\citep{nowozin2016f}. However, the optimal discriminators are different for the two objectives:

\end{restatable}
\begin{proof}
See Appendix \ref{sec:gan-theorem}.
\end{proof}

The above theorem shows that in the limit of infinite data and computation, adding NDA changes the optimal discriminator solution but not the optimal generator.
In practice, when dealing with finite data, existing regularization techniques such as weight decay and spectral normalization~\citep{miyato2018spectral} allow potentially many solutions that achieve the same objective value.
The introduction of NDA samples allows us to filter out certain solutions by providing additional inductive bias through OOD samples.  In fact, the optimal
discriminator 
will reflect the density ratio between  and  (see Eq.(\ref{eq:dr-nda-gan})),  
and its values will be higher for samples from  compared to those from  . As we will show in Section \ref{section:anomaly}, a discriminator trained with this objective and suitable NDA performs better than relevant baselines for other downstream tasks such as anomaly detection. 








\iffalse
\begin{theorem}
Let  be a distribution over . Let  be a distribution over  with disjoint support than , i.e., such that . Let

be an -divergence. Then for all 

\end{theorem}
\begin{proof}
Let us use  to denote the density functions of  and  respectively.

 where we use the fact that  is convex with Jensen's inequality in Eq.(\ref{eq:jensen-f}) and the fact that  in Eq.(\ref{eq:final-step}) since  and  has disjoint support.
 
 We also have
 
\end{proof}



As a corollary, let  be a data distribution. Let  be any distribution with disjoint support (e.g., Jigsaw). Then


TODO: check things still work when  is allowed to depend on 

\js{i think this is still true}
\fi






 












\section{NDA for Constrastive Representation Learning}




Using a classifier to estimate a density ratio is useful not only for estimating -divergences (as in the previous section) but also for estimating mutual information between two random variables.
In representation learning, mutual information (MI) maximization is often employed to learn 
compact yet useful  representations 
of the data,
allowing one to perform downstream tasks efficiently~\citep{tishby2015deep,nguyen2008estimating,poole2019on,oord2018representation}. Here, we show that NDA samples are also beneficial for representation learning.  








In contrastive representation learning (such as CPC~\citep{oord2018representation}), the goal is to learn a mapping  that maps a datapoint  to some distribution over the representation space ; once the network  is learned,  representations are obtained by sampling from .
CPC \textit{maximizes} the following objective:

where  is the marginal  distribution of the representations associated with . 
Intuitively, the CPC objective involves an -class classification problem  where  attempts to identify a matching pair (i.e. ) sampled from the joint distribution from the  non-matching pairs (i.e. ) sampled from the product of marginals distribution. Note that  plays the role of a discriminator/critic, and is implicitly estimating a density ratio. As , the optimal  corresponds to an un-normalized density ratio between the joint distribution and the product of marginals, and the CPC objective matches its upper bound which is the mutual information between  and ~\citep{poole2019variational,song2019understanding}.
However, this objective is no longer able to control the representations for data that are out of support of , so there is a risk that the representations are similar between  samples and out-of-distribution ones. 
















To mitigate this issue, we propose to use NDA in the CPC objective, where we additionally introduce a batch of NDA samples, for each positive sample:

where the expectation is taken over ,  (NDA distribution),  for all . 
    Here, the behavior of  when  is NDA is optimized explicitly, allowing us to impose additional constraints to the NDA representations. This corresponds to a more challenging classification problem (compared to basic CPC) that encourages learning more informative representations.
In the following theorem, we show that the proposed objective encourages the representations for NDA samples to become disjoint from the representations for  samples, \textit{i.e.} NDA samples and  samples do not map to the same representation. 



\begin{theorem}
\label{thm:cpc-informal}
(Informal) The optimal solution to  in the NDA-CPC objective maps the representations of data samples and NDA samples to disjoint regions.
\end{theorem}
\begin{proof}
See Appendix \ref{sec:rep-theory} for a detailed statement and proof.
\end{proof}














 



\section{NDA-GAN Experiments}
In this section we report experiments with different types of NDA for image generation. Additional details about the network architectures and hyperparameters can be found in Appendix \ref{app:archit}.

\begin{wrapfigure}{r}{0.30\textwidth}
\vspace{-20pt}
  \begin{center}
    \includegraphics[width=0.30\textwidth]{sections/images/trial.eps}
  \end{center}
  \vspace{-10pt}
  \caption{Histogram of difference in the discriminator output for a real image and it's Jigsaw version.}
  \vspace{-5pt}
  \label{fig:diff}
\end{wrapfigure}

\textbf{Unconditional Image Generation.} We conduct experiments on various datasets using the BigGAN architecture~\citep{brock2018large} for unconditional image generation\footnote{We feed a single label to all images to make the architecture suitable for unconditional generation.}. We first explore various image transformations 
from the literature to evaluate which ones are effective as NDA. For each transformation, we evaluate its performance as NDA (training as in Eq. \ref{eq:ndagan}) and as a traditional data augmentation strategy, where we enlarge the training set 
by applying the transformation to real images (denoted PDA for positive data augmentation).
Table \ref{tab:pdavsnda} shows the FID scores for different types of transformations as PDA/NDA. The results suggest that transformations that spatially corrupt the image are strong NDA candidates. It can be seen that Random Horizontal Flip is not effective as an NDA; this is because flipping does not spatially corrupt the image but is rather a semantic preserving transformation, hence the NDA distribution  is not disjoint from . On the contrary, it is reasonable to assume that if an image is likely under , its flipped variant should also be likely. This is confirmed by the effectiveness of this strategy as PDA.





We believe spatially corrupted negatives perform well as NDA in that they push the 
discriminator to focus on global features instead of local ones (e.g., texture). 
We confirm this by plotting the histogram of differences in the discriminator output for a real image and it's Jigsaw version as shown in Fig.~\ref{fig:diff}.
We show that the difference is \textit{(a)} centered close to zero for normal BigGAN (so \textit{without NDA training, the discriminator cannot distinguish real and Jigsaw samples well}), and \textit{(b)} centered at a positive number (logit 10) for our method (NDA-BigGAN). Following our findings, in our remaining experiments we use Jigsaw, Cutout, Stitch, Mixup and Cutmix as they achieve significant improvements when used as NDA for unconditional image generation on CIFAR-10.


Table \ref{table:FID_uncond} shows the FID scores for BigGAN when trained with five types of negative data augmentation on four different benchmarks. Almost all the NDA augmentations improve the baseline across datasets.
For all the datasets except CIFAR-100, , whereas for CIFAR-100 it is 0.5. We show the effect of  on CIFAR-10 performance in Appendix \ref{app:lambda}. We additionally performed an experiment using a mixture of augmentation policy. The results (FID 16.24) were better than the baseline method (18.64) but not as good as using a single strategy.


\begin{table}[]
\caption[]{FID scores over CIFAR-10 using different transformations as PDA and NDA in BigGAN. The results indicate that some transformations yield better results when used as NDA. The common feature of such transformations is they all spatially corrupt the images. 
}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|ll|ll|ll|ll|ll|ll|ll|ll@{}}
\toprule
 \multirow{2}{*}{\textbf{w/o Aug.}} & \multicolumn{2}{c}{\textbf{Jigsaw}} & \multicolumn{2}{c}{\textbf{Cutout}} & \multicolumn{2}{c}{\textbf{Stitch}} & \multicolumn{2}{c}{\textbf{Mixup}} & \multicolumn{2}{c}{\textbf{Cutmix}} & \multicolumn{2}{c}{\textbf{Random Crop}} & \multicolumn{2}{c}{\textbf{Random Flip}} & \multicolumn{2}{c}{\textbf{Gaussian}} \\ \cmidrule{2-17}
 & \textbf{PDA} & \textbf{NDA} & \textbf{PDA} & \textbf{NDA} & \textbf{PDA} & \textbf{NDA} & \textbf{PDA} & \textbf{NDA} & \textbf{PDA} & \textbf{NDA} &  \textbf{PDA} & \textbf{NDA} & \textbf{PDA} & \textbf{NDA} & \textbf{PDA} & \textbf{NDA} \\
 \midrule
18.64 & 98.09 & 12.61 & 79.72 & 14.69 & 108.69 & 13.97 & 70.64 & 17.29 & 90.81 & 15.01 & 20.02 & 15.05  & 16.65 & 124.32 & 44.41 & 18.72  \\
\bottomrule
 \end{tabular}}
 \label{tab:pdavsnda}
\end{table}


\textbf{Conditional Image Generation.}
We also investigate the benefits of NDA in conditional image generation using BigGAN. The results are shown in Table \ref{table:FID_cond}. In this setting as well, NDA gives a significant boost over the baseline model.
We again use  for CIFAR-10 and  for CIFAR-100. For both unconditional and conditional setups we find the Jigsaw and Stitching augmentations to achieve a better FID score than the other augmentations. 






\begin{table}[!t]
  \caption{Comparison of FID scores of different types of NDA for unconditional image generation on various datasets. The numbers in bracket represent the corresponding image resolution in pixels. Jigsaw consistently achieves the \textbf{best} or \textcolor{teal}{second best} result.}
  \label{table:FID_uncond}
  \centering
  \resizebox{0.95\textwidth}{!}{
  \begin{tabular}{llllllll}
    \toprule
    {} & \textbf{BigGAN} & \textbf{Jigsaw} & \textbf{Stitching} & \textbf{Mixup} & \textbf{Cutout}& \textbf{Cutmix} & \textbf{CR-BigGAN} \\
    \midrule
    \textbf{CIFAR-10 (32)} & 18.64 & \textbf{12.61} & \textcolor{teal}{13.97} & 17.29 & 14.69 & 15.01 & 14.56\\
    \textbf{CIFAR-100 (32)} & 22.19 & \textbf{19.72} & 20.99 & 22.21 & 22.08 & \textcolor{teal}{20.78} & -- \\
    \textbf{CelebA (64)} & 38.14 & \textcolor{teal}{37.24} & \textbf{37.17} & 37.51 & 37.39 & 37.46 & --\\
    \textbf{STL10 (32)} & 26.80 &  \textbf{23.94} & 26.08 & \textcolor{teal}{24.45} & 24.91 & 25.34 & -- \\
    \bottomrule
  \end{tabular}}
\end{table}





\begin{table}[!h]
\caption[]{FID scores for conditional image generation using different NDAs.\footnotemark}  
  \label{table:FID_cond}
  \centering
  \resizebox{0.9\textwidth}{!}{
  \begin{tabular}{lllllllll}
    \toprule
    {} & \textbf{BigGAN} & \textbf{Jigsaw} & \textbf{Stitching} & \textbf{Mixup} & \textbf{Cutout} & \textbf{Cutmix} & \textbf{CR-BigGAN} \\
    \midrule
    \textbf{C-10} & 11.51 & \textbf{9.42} & \textcolor{teal}{9.47} & 13.87 & 10.52 & 10.3 & 11.48 \\
    \textbf{C-100} & 15.04 & 14.12 & \textbf{13.90} & 15.27 & 14.21 & \textcolor{teal}{13.99} & -- \\
    \bottomrule
  \end{tabular}}
\end{table}
\footnotetext{We use a \href{https://github.com/ajbrock/BigGAN-PyTorch}{PyTorch} code for BigGAN. The number reported in~\citet{brock2018large} for C-10 is 14.73.}

\textbf{Image Translation.} Next, we apply the NDA method to image translation. In particular, we use the Pix2Pix model~\citep{isola2017image} that can perform image-to-image translation using GANs provided paired training data. Here, the generator is conditioned on an image , and the discriminator takes as input the concatenation of generated/real image and .
We use Pix2Pix for semantic segmentation on Cityscapes dataset~\citep{cordts2016cityscapes} (i.e. photos  labels). Table \ref{tab:pix2pix_nda} shows the quantitative gains obtained by using Jigsaw NDA\footnote{We use the official \href{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{PyTorch} implementation and show the best results. } 
while Figure \ref{fig:pix2pix_nda} in Appendix \ref{sec:pix2pix} highlights the qualitative improvements. The NDA-Pix2Pix model avoids noisy segmentation on objects including buildings and trees.

\begin{comment}
\begin{figure}[!h]
    \begin{minipage}{0.5\hsize}
\includegraphics[width=0.99\hsize]{sections/images/pix2pix.png}
  \captionof{figure}{Qualitative results on Cityscapes.}
  \label{fig:pix2pix_nda}
\end{minipage}
    \hfill
\begin{minipage}{0.48\hsize}
\resizebox{\textwidth}{!}{
\begin{tabular}{llll}
    \toprule
    \textbf{Metric} & \textbf{Pp. acc.} & \textbf{Pc. acc.} & \textbf{mIOU}\\
    \midrule
    \begin{tabular}[c]{@{}c@{}}Pix2Pix \\ (cGAN)\end{tabular} & 0.80 & 0.24 & 0.27 \\
    \begin{tabular}[c]{@{}l@{}}NDA-Pix2Pix \\ (cGAN)\end{tabular} & \textbf{0.84} & \textbf{0.34} & \textbf{0.28}  \\
    \midrule
   \begin{tabular}[c]{@{}l@{}}Pix2Pix \\ (L1+cGAN)\end{tabular} &
    0.72 & 0.23 & 0.18\\
    \begin{tabular}[c]{@{}l@{}}NDA-Pix2Pix \\ (L1+cGAN)\end{tabular} &
    \textbf{0.75} & \textbf{0.28} & \textbf{0.22}\\
    \bottomrule
  \end{tabular}}
\captionof{table}{Quantitative results on CityScapes, using per pixel (Pp.), per class (Pc.) accuracies and mean Intersection over Union (mIOU)}
    \label{tab:pix2pix_nda}
\end{minipage}
\end{figure}
\end{comment}

\begin{table}[!ht]
\begin{minipage}{0.48\hsize}
  \caption{Results on CityScapes, using per \\ pixel accuracy (Pp.), per class accuracy (Pc.) and mean Intersection over Union (mIOU). We compare Pix2Pix and its NDA version.}
  \label{tab:pix2pix_nda}
  \centering
  \resizebox{0.75\textwidth}{!}{
  \begin{tabular}{l|lll}
    \toprule
    \textbf{Metric} & \textbf{Pp.} & \textbf{Pc.} & \textbf{mIOU}\\
    \midrule
    \begin{tabular}[c]{@{}c@{}}Pix2Pix \\ (cGAN)\end{tabular} & 0.80 & 0.24 & 0.27 \\
    \begin{tabular}[c]{@{}l@{}}NDA\\ (cGAN)\end{tabular} & \textbf{0.84} & \textbf{0.34} & \textbf{0.28}  \\
    \midrule
   \begin{tabular}[c]{@{}l@{}}Pix2Pix \\ (L1+cGAN)\end{tabular} &
    0.72 & 0.23 & 0.18\\
    \begin{tabular}[c]{@{}l@{}}NDA\\ (L1+cGAN)\end{tabular} &
    \textbf{0.75} & \textbf{0.28} & \textbf{0.22}\\
    \bottomrule
  \end{tabular}}
\end{minipage}
\hfill
\begin{minipage}{0.50\hsize}
  \caption{AUROC scores for different OOD \\ datasets. OOD-1 contains different datasets, while OOD-2 contains the set of 19 different corruptions in CIFAR-10-C~\citep{hendrycks2018benchmarking} (the average score is reported). 
}
  \label{table:auroc}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l|l|lll}
    \toprule
& & BigGAN & Jigsaw & EBM\\
    \midrule
\multirow{6}{*}{OOD-1} & DTD & 0.70 & 0.69 & 0.48 \\
& SVHN & 0.75 & 0.61 & 0.63 \\
    & Places-365 & 0.35 & 0.58 & 0.68 \\
    & TinyImageNet & 0.40 & 0.62 & 0.67\\
    & CIFAR-100 & 0.63 & 0.64 & 0.50\\
    \cmidrule{2-5}
    & Average & 0.57 & \textbf{0.63} & 0.59\\
    \midrule
    OOD-2 & CIFAR-10-C & 0.56 & \textbf{0.63} & 0.60\\
    \bottomrule
  \end{tabular}
}
\end{minipage}
\end{table}



\textbf{Anomaly Detection.}
\label{section:anomaly}
As another added benefit of NDA for GANs, we utilize the output scores of the BigGAN discriminator for anomaly detection. We experiment with 2 different types of OOD datasets.
The first set consists of SVHN~\citep{netzer2011reading}, DTD~\citep{cimpoi14describing}, Places-365~\citep{zhou2017places}, TinyImageNet, and CIFAR-100 as the OOD datapoints following the protocol in \citep{du2019implicit,hendrycks2018deep}. 
We train BigGAN w/ and w/o Jigsaw NDA on the train set of CIFAR-10 and then use the output value of discriminator to classify the test set of CIFAR-10 (not anomalous) and different OOD datapoints (anomalous) as anomalous or not. We use the AUROC metric as proposed in~\citep{hendrycks2016baseline} to evaluate the anomaly detection performance. Table~\ref{table:auroc} compares the performance of NDA with a likelihood based model (Energy Based Models (EBM~\citep{du2019implicit}).
Results show that Jigsaw NDA performs much better than baseline BigGAN and other generative models. We did not include other NDAs as Jigsaw achieved the best results.



\begin{comment}
\begin{table}[!h]
  \caption{AUROC scores for different OOD datasets. OOD-1 contains different datasets, while OOD-2 contains the set of 19 different corruptions in CIFAR-10-C~\citep{hendrycks2018benchmarking} (the average score is reported). PixelCNN++ and Glow numbers are from \citep{du2019implicit}.}
  \label{table:auroc}
  \centering
  \resizebox{0.5\textwidth}{!}{
  \begin{tabular}{l|l|lllll}
    \toprule
    {} & {} & \multicolumn{5}{c}{Generative Models}  \\
    \midrule
    & & BigGAN & Jigsaw & PixelCNN++ & Glow & EBM\\
    \midrule
\multirow{6}{*}{OOD-1} & DTD & 0.70 & 0.69 & 0.33 & 0.27 & 0.48 \\
& SVHN & 0.75 & 0.61 & 0.32 & 0.24 & 0.63 \\
    & Places-365 & 0.35 & 0.58 & - & - & 0.68 \\
    & TinyImageNet & 0.40 & 0.62 & - & - & 0.67\\
    & CIFAR-100 & 0.63 & 0.64 & - & - & 0.50\\
    \cmidrule{2-7}
    & Average & 0.57 & \textbf{0.63} & - & - &  0.59\\
    \midrule
    OOD-2 & CIFAR-10-C & 0.56 & \textbf{0.63} & - & - & 0.60\\
    \bottomrule
  \end{tabular}}
\end{table}
\end{comment}

We consider the extreme corruptions in CIFAR-10-C~\citep{hendrycks2018benchmarking} as the second set of  OOD datasets. It consists of 19 different corruptions, each having 5 different levels of severity. We only consider the corruption of highest severity for our experiment, as these constitute a significant shift from the true data distribution. Averaged over all the 19 different corruptions, the AUROC score for the normal BigGAN is \textbf{0.56}, whereas the BigGAN trained with Jigsaw NDA achieves \textbf{0.63}.
The histogram of difference in discriminator's output for clean and OOD samples are shown in Figure \ref{fig:anomaly} 
in the appendix. 
High difference values imply that the Jigsaw NDA is better at distinguishing OOD samples than the normal BigGAN. 












\section{Representation Learning using Contrastive Loss and NDA}







\textbf{Unsupervised Learning on Images.}
In this section, we perform experiments on three benchmarks: (a) CIFAR10 (C10), (b) CIFAR100 (C100), and (c) ImageNet-100~\citep{deng2009imagenet} to show the benefits of NDA on representation learning with the contrastive loss function. In our experiments, we use the momentum contrast method~\citep{he2019momentum}, \emph{MoCo-V2}, as it is currently the state-of-the-art model on unsupervised learning on ImageNet. For C10 and C100, we train the MoCo-V2 model for unsupervised learning (w/ and w/o NDA)  for 1000 epochs. On the other hand, for ImageNet-100, we train the MoCo-V2 model (w/ and w/o NDA) for 200 epochs. Additional hyperparameter details can be found in the appendix. To evaluate the representations, we train a linear classifier on the representations on the same dataset with labels.
Table~\ref{table:moco_cifar10} shows the top-1 accuracy of the classifier. We find that across all the three datasets, different NDA approaches outperform \emph{MoCo-V2}. While Cutout NDA performs the best for C10, the best performing NDA for C100 and ImageNet-100 are Jigsaw and Mixup respectively. Figure \ref{fig:cosine} compares the cosine distance of the representations learned w/ and w/o NDA (jigsaw) and shows that jigsaw and normal images are projected far apart from each other when trained using NDA whereas with original MoCo-v2 they are projected close to each other.


\begin{table}[!h]
  \caption{Top-1 accuracy results on image recognition w/ and w/o NDA on MoCo-V2.
  }
  \label{table:moco_cifar10}
  \centering
  \resizebox{0.85\textwidth}{!}{
  \begin{tabular}{lllllll}
    \toprule
    {} & MoCo-V2 & Jigsaw & Stitching & Cutout & Cutmix & Mixup\\
    \midrule
    CIFAR-10 & 91.20 & \textcolor{teal}{91.66} & 91.59 & \textbf{92.26}  & 91.51 & 91.36\\
    CIFAR-100 & 69.63 & \textbf{70.17} & 69.21 & 69.81 & 69.83 & \textcolor{teal}{69.99} \\ 
    ImageNet-100 & 69.41 & \textcolor{teal}{69.95} & 69.54 & 69.77 & 69.61 & \textbf{70.01} \\ 
    \bottomrule
  \end{tabular}}
\end{table}

\textbf{Transfer Learning for Object Detection.}
We transfer the network pre-trained over ImageNet-100 for the task of Pascal-VOC object detection using a Faster R-CNN detector (C4 backbone)~\cite{ren2015faster}. We fine-tune the network on Pascal VOC 2007+2012 trainval set and test it on the 2007 test set. The baseline MoCo achieves  AP,  AP50,  AP75 whereas the MoCo trained with mixup NDA gets  AP,  AP50,  AP75 (an improvement of ). 

\textbf{Unsupervised Learning on Videos.}
In this section, we investigate the benefits of NDA in self-supervised learning of spatio-temporal embeddings from video, suitable for human action recognition. We apply NDA to Dense Predictive Coding~\citep{han2019video}, which is a single stream (RGB only) method for self-supervised representation learning on videos. For videos, we create NDA samples by performing the same transformation on all frames of the video (e.g. the same jigsaw permutation is applied to all the frames of a video). We evaluate the approach by first training the DPC model with NDA on a large-scale dataset (UCF101), and then evaluate the representations by training a supervised action classifier on UCF101 and HMDB51 datasets. As shown in Table \ref{table:DPC}, Jigsaw and Cutmix NDA improve downstream task accuracy on UCF-101 and HMDB-51, achieving new state-of-the-art performance among single stream (RGB only) methods for self-supervised representation learning (when pre-trained using UCF-101).



\begin{table}[!h]
  \caption{Top-1 accuracy results on action recognition in videos w/ and w/o NDA in DPC.}
\label{table:DPC}
  \centering
\resizebox{0.95\textwidth}{!}{  
  \begin{tabular}{lllllll}
    \toprule
    {} & DPC & Jigsaw & Stitching & Cutout & Cutmix & Mixup\\
    \midrule
UCF-101 (Pre-trained on UCF-101) & 61.35 & \textcolor{teal}{64.54} & \textbf{66.07} & 64.52 & 63.52 & 63.65\\
    HMDB51 (Pre-trained on UCF-101) & 45.31 & \textbf{46.88} & 45.31 & 45.31 & \textbf{48.43} & 43.75\\
    \bottomrule
  \end{tabular}}
\end{table}








 
\section{Related work}
In several machine learning settings, negative samples are produced from a statistical generative model. \cite{sung2019difference} aim to generate negative data using GANs for semi-supervised learning and novelty detection while we are concerned with efficiently creating negative data to improve generative models and self-supervised representation learning. \cite{hanneke2018actively} also propose an alternative theoretical framework that relies on access to an oracle which classifies a sample as valid or not, but do not provide any practical implementation. \cite{bose2018adversarial} use adversarial training to generate hard negatives that fool the discriminator for NLP tasks whereas we obtain NDA data from positive data to improve image generation and representation learning. \cite{hou2018generative} use a GAN to learn the negative data distribution with the aim of classifying positive-unlabeled (PU) data whereas we do not have access to a mixture data but rather generate negatives by transforming the positive data.

In contrastive unsupervised learning, common negative examples are ones that are assumed to be further than the positive samples semantically. Word2Vec~\citep{mikolov2013distributed} considers negative samples to be ones from a different context and CPC-based methods~\citep{oord2018representation} such as momentum contrast~\citep{he2019momentum}, the negative samples are data augmentations from a different image. Our work considers a new aspect of ``negative samples'' that are neither generated from some model, nor samples from the data distribution. Instead, by applying negative data augmentation (NDA) to existing samples, we 
are able to incorporate useful inductive biases that might be difficult to capture otherwise
~\citep{zhao2018bias}. 



\section{Conclusion}
We proposed negative data augmentation as a method to incorporate prior knowledge through out-of-distribution (OOD) samples. NDAs are complementary to traditional data augmentation strategies, which are typically focused on in-distribution samples. 
Using the NDA framework, we interpret existing image transformations (e.g., jigsaw) as producing OOD samples and develop new learning algorithms to leverage them. Owing to rigorous mathematical characterization of the NDA assumption, we are able to theoretically analyze their properties.
As an example, we 
bias the generator of a GAN to avoid the support of negative samples, improving results on conditional/unconditional image generation tasks. Finally, we leverage NDA for unsupervised representation learning in images and videos. By integrating NDA into MoCo-v2 and DPC, we improve results on image and action recognition on CIFAR10, CIFAR100, ImageNet-100, UCF-101, and HMDB-51 datasets. 
Future work include exploring other augmentation strategies as well as NDAs for other modalities. 

%
 



\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\appendix
\appendix


\section{Numerosity Containment}
\label{sec:numerosity}
\cite{zhao2018bias} systematically investigate generalization in deep generative models using two different datasets: (a) a toy dataset where there are  non-overlapping dots (with random color and location) in the image (see Figure \ref{fig:dots_data}), and (b) the CLEVR dataset where ther are  objects (with random shape, color, location, and size) in the images (see Figure \ref{fig:clevr_data}). They train a GAN model (WGAN-GP~\cite{gulrajani2017improved}) with (either) dataset and observe that the learned distribution does not produce the same number of objects as in the dataset it was trained on. The distribution of the numerosity in the generated images is centered at the numerosity from the dataset, with a slight-bias towards over-estimation. For, example when trained on images with six dots, the generated images contain anywhere from two to eight dots (see Figure \ref{fig:dots}). The observation is similar when trained on images with two CLEVR objects. The generated images contain anywhere from one to three dots (see Figure \ref{fig:clevr}). 

In order to remove samples with numerosity different from the train dataset, we use such samples as negative data during training. For example, while training on images with six dots we use images with four, five and seven dots as negative data for the GAN. The resulting distribution of the numerosity in the generated images is constrained to six. We observe similar behaviour when training a GAN with images containing two CLEVR objects as positive data and images with one or three objects as negative data.


\begin{figure*}[!h]
\centering
\begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{sections/images/dots_appendix.png}
    \caption{Dots}
    \label{fig:dots_data}
\end{subfigure}
\\
\begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{sections/images/clevr_appendix.png}
    \caption{CLEVR}
    \label{fig:clevr_data}
\end{subfigure}

\caption{Toy Datasets used in Numerosity experiments.}
\label{fig:toydataset}
\end{figure*}


\begin{figure*}[!h]
\centering
\begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{sections/images/dots.png}
    \caption{}
    \label{fig:dots}
\end{subfigure}~
\begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{sections/images/clevr.png}
    \caption{}
    \label{fig:clevr}
\end{subfigure}

\caption{\textbf{Left}: Distribution over number of dots. The arrows are the number of dots the learning algorithm is trained on, and the solid line is the distribution over the number of dots the model generates. \textbf{Right}: Distribution over number of CLEVR objects the model generates. Generating CLEVR is harder so we explore only one, but the behaviour with NDA is similar to dots.}
\label{fig:toyeexp}
\end{figure*}
\section{Image Transformations}
Given an image of size , the different image transformations that we used are described below.
\begin{description}
\label{sec:transformation}
    \item[Jigsaw-] \citep{noroozi2016unsupervised} We partition the image into a grid of  patches of size , indexed by . Then we shuffle the image patches according to a random permutation (different from the original order) to produce the NDA image. Empirically, we find  to work the best for Jigsaw- NDA.
    \item[Stitching] We stitch two equal-sized patches of two different images, either horizontally () or vertically (), chosen uniformly at random, to produce the NDA image.
    \item[Cutout / Cutmix] 
We select a random patch in the image with its height and width lying between one-third and one-half of the image height and width respectively. 
    To construct NDA images, this patch is replaced with the mean pixel value of the patch (like cutout~\citep{devries2017improved} with the only difference that they use zero-masking), or the pixel values of another image at the same location (cutmix~\citep{yun2019cutmix}). 
    \item[Mixup-] NDA image is constructed from a linear interpolation between two images  and ~\citep{zhang2017mixup}, ; .  is chosen so that the distribution has high density at 0.5.
    
\item[Other classes] NDA images are sampled from other classes in the same dataset. See Appendix A.

\end{description}

\section{NDA for GANs}
\label{sec:gan-theorem}
\thmgan*
\begin{proof}

Let us use  to denote the density functions of  and  respectively (and , ,  for the respective distributions).
First, from Lemma 1 in \cite{nguyen2008estimating}, we have that 

where  refers to the -divergence.
Then, we have

 where we use the fact that  is convex with Jensen's inequality in Eq.(\ref{eq:jensen-f}) and the fact that  in Eq.(\ref{eq:final-step}) since  and  has disjoint support.
 
 We also have
 
Therefore, in order for the inequality in \Eqref{eq:jensen-f} to be an equality, we must have that  for all . Therefore, the generator distribution recovers the data distribution at the equlibrium posed by the NDA-GAN objective, which is also the case for the original GAN objective.

Moreover, from Lemma 1 in~\cite{nguyen2008estimating}, we have that:

Therefore, by replacing  with  and , we have:

which shows that the optimal discriminators are indeed different for the two objectives.
\end{proof}
\section{NDA for Contrastive Representation Learning}
\label{sec:rep-theory}

We describe the detailed statement of Theorem 2 and proof as follows.
\begin{restatable}{theorem}{thmcpc}
For some distribution  over  such that , and for any maximizer of the NDA-CPC objective 
the representations of negative samples are disjoint from that of positive samples for ; \textit{i.e.}, ,

\end{restatable}

\begin{proof}
We use a contradiction argument to establish the proof.
For any representation mapping that maximizes the NDA-CPC objective, 
 suppose that the positive and NDA samples share some support, i.e., ,
 
We can always construct  that shares the same representation with  for  but have disjoint representations for NDA samples; i.e., , the following two statements are true:
\begin{enumerate}
    \item ;
    \item .
\end{enumerate}
Our goal is to prove that:

which shows a contradiction. 

For ease of exposition, let us allow zero values for the output of , and define  (in this case, if  assigns zero to positive values, then the CPC objective becomes , so it cannot be a maximizer to the objective).

Let  be an optimal critic to the representation model 
. We then define a following critic function:

In other words, the critic assigns the same value for data-representation pairs over the support of  and zero otherwise.
From the assumption over , , and ,

so 
can be sampled as a positive pair and .

Therefore, 

which proves the theorem via contradiction.
\end{proof}





\section{Pix2Pix}
\label{sec:pix2pix}
Figure \ref{fig:pix2pix_nda} highlights the qualitative improvements when we apply the NDA method to  Pix2Pix model~\citep{isola2017image}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\textwidth]{sections/images/pix2pix.png}
    \caption{Qualitative results on Cityscapes.}
    \label{fig:pix2pix_nda}
\end{figure}

\section{Anomaly Detection}
Here, we show the histogram of difference in discriminator’s output for clean and OOD samples in Figure \ref{fig:anomaly}. High difference values imply that the Jigsaw NDA is better at distinguishing OOD samples than the normal BigGAN.

\begin{figure*}[!ht]
\begin{subfigure}[b]{0.3\textwidth}
\centering
    \includegraphics[width=1.5in,height=1.5in]{sections/images/gaussiandisc_diff.png}
    \caption{Gaussian Noise}
    \label{fig:anomaly_zoom}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\centering
    \includegraphics[width=1.5in,height=1.5in]{sections/images/speckledisc_diff.png}
\caption{Speckle Noise}
\label{fig:anomaly_speckle}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
\centering
    \includegraphics[width=1.5in,height=1.5in]{sections/images/jpegdisc_diff.png}
\caption{JPEG Compression}
\label{fig:anomaly_jpeg}
\end{subfigure}
\caption{Histogram of D(clean) - D(corrupt) for 3 different corruptions.}
\label{fig:anomaly}
\end{figure*}

\section{Effect of hyperparameter on Unconditional Image generation}
\label{app:lambda}
Here, we show the effect of  for unconditional image generation on CIFAR-10 dataset.

\begin{table}[!h]
  \caption{Effect of  on the FID score for unconditional image generation on CIFAR-10 using Jigsaw as NDA.}
  \label{table:alpha_beta}
  \centering
  \begin{tabular}{llllll}
    \toprule
     & 1.0 & 0.75 & 0.5 & 0.25 & 0.15\\
    \midrule
    FID & 18.64 & 16.61 & 14.95 & \textbf{12.61} & 13.01\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Unsupervised Learning on Images}

\begin{figure}[!htb]
\centering
    \includegraphics[width=0.98\textwidth]{sections/images/cosine_distances.pdf}
    \caption{Comparing the cosine distance of the representations learned with Jigsaw NDA and Moco-V2 (\textbf{shaded blue}), and original Moco-V2 (\textbf{white}). With NDA, we project normal and its jigsaw image representations further away from each other than the one without NDA.}
    \label{fig:cosine}
\end{figure}

\section{Dataset Preparation for FID evaluation}
For dataset preparation, we follow the the following procedures: (a) CIFAR-10 contains 60K 3232
images with 10 labels, out of which 50K are used for training
and 10K are used for testing, (b) CIFAR-100 contains 60K 32  32 images with 100 labels, out of which 50K are used for training and 10K are used for testing, (c)
CelebA contains 162,770 train images and 19,962 test images (we resize the images to 6464px), (d) STL-10 contains 100K (unlabeled) train images and 8K (labeled) test images (we resize the images to 3232px). In our experiments the FID is calculated on the test dataset. In particular, we use 10K generated images vs. 10K test images for CIFAR-10, 10K vs. 10K for CIFAR-100, 19,962 vs. 19,962 for CelebA, and 8K vs 8K for STL-10.


\section{Hyperparameters and Network Architecture}
\label{app:archit}
\paragraph{Generative Modeling.} We use the same network architecture in BigGAN~\cite{brock2018large} for our experiments. The code used for our experiments is based over the author's  \href{https://github.com/ajbrock/BigGAN-PyTorch}{PyTorch} code. For CIFAR-10, CIFAR-100, and CelebA we train for 500 epochs whereas for STL-10 we train for 300 epochs. For all the datasets we use the following hyperparameters: batch-size = 64, generator learning rate = 2e-4, discriminator learning rate = 2e-4, discriminator update steps per generator update step = 4. The best model was selected on the basis of FID scores on the test set (as explained above).


\paragraph{Momentum Contrastive Learning.}  We use the official \href{https://github.com/facebookresearch/moco}{PyTorch} implementation for our experiments. For CIFAR-10 and CIFAR-100, we perform unsupervised pre-training for 1000 epochs and supervised training (linear classifier) for 100 epochs. For Imagenet-100, we perform unsupervised pre-training for 200 epochs and supervised training (linear classifier) for 100 epochs. For CIFAR-10 and CIFAR-100, we use the following hyperparameters during pre-training: batch-size = 256, learning-date = 0.3, temperature = 0.07, feature dimensionality = 2048. For ImageNet-100 pre-training we have the following: batch-size = 128, learning-date = 0.015, temperature = 0.2, feature dimensionality = 128. During linear classification we use a batch size of 256 for all the datasets and learning rate of 10 for CIFAR-10, CIFAR-100, whereas for ImageNet-100 we use learning rate of 30.

\paragraph{Dense Predictive Coding.} We use the same network architecture and hyper-parameters in DPC \cite{han2019video} for our experiments and use the official \href{https://github.com/TengdaHan/DPC}{PyTorch} implementation. We perform self-supervised training on UCF-101 for 200 epochs and supervised training (action classifier) for 200 epochs on both UCF-101 and HMDB51 datasets.

\section{Code}
The code to reproduce our experiments is given \href{https://anonymous.4open.science/r/99219ca9-ff6a-49e5-a525-c954080de8a7/}{here}. 
\end{document}
