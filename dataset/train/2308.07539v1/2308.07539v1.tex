\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

\usepackage[numbers,sort&compress]{natbib}
\usepackage[table]{xcolor} 
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=True]{hyperref}




\begin{document}

\title{Visual and Textual Prior Guided Mask Assemble for Few-Shot Segmentation and Beyond}


\author{Shuai Chen, Fanman Meng, Runtong Zhang, Heqian Qiu, Hongliang Li, Qingbo Wu, Linfeng Xu
\thanks{The Authors are with the School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China (e-mail: schen@std.uestc.edu.cn; fmmeng@uestc.edu.cn; 202211012322@std.uestc.edu.cn; hqqiu@std.uestc.edu.cn; hlli@uestc.edu.cn; qbwu@uestc.edu.cn; lfxu@uestc.edu.cn)}
\thanks{Corresponding author: Fanman Meng.}
}



\maketitle


\begin{abstract}
    Few-shot segmentation (FSS) aims to segment the novel classes with a few annotated images. Due to CLIP's advantages of aligning visual and textual information, the integration of CLIP can enhance the generalization ability of FSS model. However, even with the CLIP model, the existing CLIP-based FSS methods are still subject to the biased prediction towards base classes, which is caused by the class-specific feature level interactions. To solve this issue, we propose a visual and textual Prior Guided Mask Assemble Network (PGMA-Net). It employs a class-agnostic mask assembly process to alleviate the bias, and formulates diverse tasks into a unified manner by assembling the prior through affinity. Specifically, the class-relevant textual and visual features are first transformed to class-agnostic prior in the form of probability map. Then, a Prior-Guided Mask Assemble Module (PGMAM) including multiple General Assemble Units (GAUs) is introduced. It considers diverse and plug-and-play interactions, such as visual-textual, inter- and intra-image, training-free, and high-order ones. Lastly, to ensure the class-agnostic ability, a Hierarchical Decoder with Channel-Drop Mechanism (HDCDM) is proposed to flexibly exploit the assembled masks and low-level features, without relying on any class-specific information. It achieves new state-of-the-art results in the FSS task, with mIoU of  on  and  on  in 1-shot scenario. Beyond this, we show that without extra re-training, the proposed PGMA-Net can solve bbox-level and cross-domain FSS, co-segmentation, zero-shot segmentation (ZSS) tasks, leading an any-shot segmentation framework. 
\end{abstract}


 
\begin{IEEEkeywords}
  Few-shot segmentation, zero-shot, any-shot, class-agnostic, CLIP
\end{IEEEkeywords}


\section{Introduction}

\IEEEPARstart{T}{he} remarkable success has been made in the area of semantic segmentation by deep learning based methods~\cite{long2015fully,chen2017deeplab,cheng2021per,QiuYM18,9745353}. However, this progress heavily relies on the availability of large annotated datasets~\cite{lin2014microsoft,everingham2010pascal,zhou2019semantic}, requiring a time-consuming and laborious process of annotation. Additionally, these methods fail to handle the extremely low-data scenario on the novel classes in practice. Conversely, humans are capable of identifying and segmenting novel concepts with a few visual stimulation. Motivated by this, researchers have proposed the few-shot segmentation (FSS) task, aiming to build a class-agnostic model from the base classes, and segment object of any novel classes.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{imgs/motivation.pdf}
    \caption{(a): Activation maps on   and . Despite incorporating visual information from support as assistance, the baseline struggles to activate the "Bicycle" area and is prone to base class "Person". This is due to the insufficient guidance and an improper utilization of class-specific feature-to-mask mapping. (b): Previous methods is valid solely when both the support image  and support mask  are available. (c): This paper proposes to incorporate textual components via a class-agnostic prior-to-mask mapping to address these issues, and is capable of performing diverse tasks (FSS, bbox-level and cross-domain FSS, ZSS, co-segmentation, FSS with inaccurate support mask) in a unified form: assembling the prior by affinity.}
    \label{fig:motivation}
  \end{figure}

However, even equipped with the powerful meta-learning and metric-learning schemes, existing FSS models~\cite{zhang2019pyramid,liu2020part,tian2020prior,boudiaf2021few,min2021hypercorrelation,fan2022self,liu2023fecanet,9999056} still suffer from the inaccurate localization of the target object, along with an over-fitting to base class as shown in Figure~\ref{fig:motivation} (a). We contend that this is originating from two reasons: 1) They rely solely on a few visual annotations, with limitation on handling the vast variations of appearance, geometric and context among objects. 2) They mainly focus on learning a mapping from class-specific features to masks, which makes the mapping biased to the semantically rich features of the base class, leading to ineffectual inference on novel class. Moreover, as shown in Figure~\ref{fig:motivation} (b, c), existing FSS models fail to handle extra forms of guided segmentation tasks via one suite of model weights, such as bounding-box guided FSS, cross-domain FSS, co-segmentation and text-guided zero-shot segmentation tasks.  

This paper devotes to design a \textbf{purely class-agnostic} model to alleviate the training bias when incorporating the semantically rich textual information, and further capable of executing the above tasks in a \textbf{unified task form}. To achieve these goals, we integrate these tasks into a unified formulation that assembles the prior via affinity. On one hand, this formulation takes only the class-agnostic prior and affinity as direct inputs, both of which are class-agnostic. Therefore, it can alleviate training bias compared to the previous class-specific feature-level mapping. On the other hand, different tasks have different priors and affinities. For example, the ZSS task involves intra-image affinity and textual prior, while the FSS task includes inter-image affinity and visual prior. Despite these variations, the interaction mechanism remains the same. This uniformity in such formulation allows us to perform diverse tasks using same set of model weights.


Following the proposed scheme, we propose our PGMA-Net, it includes three main modules: 1) a \textbf{Prior Adapter} that converts class-relevant visual-textual features of CLIP~\cite{radford2021learning} into class-agnostic priors in the form of probability map. Additionally, an \textbf{Affinity Extractor} is also proposed to capture the class-independent pixel-to-pixel correspondence via inter- and intra-image, training-free, and high-order correlations. 2) a Prior-Guided Mask Assemble Module (\textbf{PGMAM}) including multiple General Assemble Units (GAUs) is introduced. It assembles diverse priors by corresponding affinities in a unififed manner. 3) To convert the assembled priors into the final prediction mask in class-agnostic manner, we propose a Hierarchical Decoder with Channel-Drop Mechanism \textbf{(HDCDM)}. It not only essentially ensures the generalization ability by taking in only the assembled priors and low-level features, but also allows our model to perform extra segmentation tasks. Overall, our contributions are as follows:

\begin{enumerate}
    \item We present a new architecture called the Prior Guided Mask Assemble Network (PGMA-Net) for the few-shot segmentation task. This network incorporates textual information by utilizing a class-agnostic prior-to-mask assembly process, which helps to alleviate the training bias towards the base class observed in previous methods.
    \item We introduce Prior-Guided Mask Assemble Module (PGMAM), which formulates diverse tasks in a unified manner by assembling the prior through affinity. It considers diverse plug-and-play interactions between priors (including visual and textual, support and query ones) and affinities (such as inter- and intra-image, training-free, and high-order ones).  
    \item We employ Hierarchical Decoder with Channel-Drop Mechanism (HDCDM), allowing for handling diverse tasks using one suit of mode weights.
    \item We achieve new state-of-the-art results in the FSS task, with mIoU of  on  and  on . Moreover, without re-training, the trained PGMA-Net shows promising performance across various tasks, including ZSS, box-level and cross-domain FSS, co-segmentation, FSS with inaccurate support annotations. This unified framework constitutes an any-shot segmentation model.
  \end{enumerate}



  
  
  \section{Related Work}
\subsection{Few-shot Learning}
Few-shot learning (FSL) intends to construct a classification model for new task with a few labeled examples. Meta-learning is the predominant paradigm for FSL, and it is further categorized into metric-based, optimization-based, and model-based methods. Metric-based methods employ either an embedding network~\cite{vinyals2016matching,snell2017prototypical,ZhangLK23} or a learnable distance~\cite{sung2018learning}. Optimization-based methods aim to learn a well-performed model initialization, followed by quick adaptation~\cite{finn2017model,rajeswaran2019meta}. Model-based methods are designed to create a model structure~\cite{cai2018memory,YangHLHW23} that is specifically tailored to meta-learning. Transfer learning is another way in FSL~\cite{hu2022pushing,zhang2022tip}, where knowledge is transferred from either the base class~\cite{hu2022squeezing}, or pre-trained models like DINO~\cite{caron2021emerging} and CLIP~\cite{radford2021learning}. 



\subsection{Few-shot Segmentation}
Few-shot segmentation aims to segment the novel classes with just a few annotated images. Episodic training strategy~\cite{vinyals2016matching} has been employed by previous approaches to learn a class-agnostic model, which  can be subdivided into prototype-based and matching-based methods. The prototype-based methods compressed the support features into class-specific prototypes, and then perform segmentation via fixed cosine distance or a learnable metric. Diverse prototypes can be formed, e.g., a single global foreground prototype obtained through masked average pooling~\cite{snell2017prototypical,wang2019panet,10109193}, multiple foreground prototypes obtained through clustering~\cite{li2021adaptive} and EM~\cite{yang2020prototype}, learnable meta-memory prototypes~\cite{wu2021learning}, and prototypes of base classes~\cite{cheng2023hpa}. However, compressing available support information into prototypes unavoidably leads to significant spatial information loss. Thus, the matching-based methods are proposed to explore pixel-to-pixel dense connections between support and query images, which can be achieved through graph attention mechanisms~\cite{zhang2019pyramid}, center-pivot 4D convolutions~\cite{min2021hypercorrelation}, and cycle-consistent transformer~\cite{zhang2021few}. The proposed PGMA-Net also belongs to matching-based method, but with a more class-agnostic prior-to-mask mapping.

\subsection{CLIP in FSS task}
Due to the exceptional efficacy in integrating visual and textual features in the CLIP~\cite{radford2021learning} embedding space, there have been attempts to utilize the CLIP prior for few-shot classification tasks~\cite{zhang2022tip,hu2022pushing}. However, CLIP's image-level training leads a discrepancy with dense tasks, rendering it a rising issue to extend in pixel-level tasks~\cite{li2022languagedriven,liu2023delving}. The complexity of this problem is even heightened when taking into account the interactions between the support and query in the FSS task. 

An attempt to incorporate CLIP prior in FSS is CLIPSeg~\cite{luddecke2022image}, involving a simple interaction among support feature, query feature, and textual feature via mix-up operation and FiLM layer~\cite{dumoulin2018feature}. But the generalization is hindered due to the use of such feature-level interaction. Similarly, when applying CLIP prior to FSS task without support mask in IMR-HSNet~\cite{wang2023iterative}, the improper usage of feature-level interaction also leads to limited performance. Compared to these methods, our work aims to better utilize CLIP in FSS task in a more class-agnostic manner, while still having enough flexibility to allow a single set of parameters to perform additional tasks, e.g., zero-shot segmentation, co-segmentation, box-level segmentation, cross-domain FSS, etc. 
\section{Methodology}
\subsection{Problem Setup}

Few-shot segmentation devotes to segment novel classes with only a limited number of labeled images. To achieve this, the model is trained on the base categories  of base dataset , and must possess the ability to provide reliable inference on the novel categories  of novel dataset . Noted that the set of base categories  and novel categories  are mutually exclusive, i.e., . 

Following previous works~\cite{tian2020prior,min2021hypercorrelation,shi2022dense}, the episodic sampling process is employed in both training and testing stages. Each episode task  consists of a support set  and a query set , e.g., . The most widely used formulation of an episodic task  is the -way -shot manner. This entails sampling  classes from the corresponding dataset, with only  labeled images available per class (typically 1 or 5). The few available labeled data are called support set  and the data waiting for segmenting are called query set , where ,, and  represent the support image, query image, ground-truth mask of support and query respectively.


\begin{figure*}[t!]
  \centering
  \includegraphics[width=1.0 \linewidth]{imgs/framework.pdf}
  \caption{The pipeline of our proposed PGMA-Net: it first extracts class-agnostic priors via Prior Adapter, and affinities via Affinity Extractor. Then performs Mask Assembly via 10 diverse interactions in a unified manner. Finally, the HDCDM is proposed to decode the multiscale assembled masks into prediction mask. Due to the unified way of assembling the coarse prior by affinity, as well as the introduction of channel-drop mechanism, the PGMA-Net trained for FSS is capable of addressing additional tasks with one suit of model weights during inference.}
  \label{fig:framework}
\end{figure*}

\subsection{Prior Guided Mask Assemble Network}

\subsubsection{Overview}
\label{subsubsection:Overview}

As shown in Figure~\ref{fig:framework}, the core of PGMA-Net is a class-agnostic and unified process: assembling the prior by affinity, where the prior in the form of probability map is extracted via Prior Adapter (Section~\ref{subsubsection:PA}), and the affinity is obtained by Affinity Extractor ((Section~\ref{subsubsection:AE})). Then the assembly is executed between coarse priors (from support or query) and affinities (inter- and intra-image, training-free, and high-order) in a unified form, detailed in Section~\ref{subsubsection:assemble}, and decode into the prediction mask via HDCDM (Section~\ref{subsubsection:HDCDM}). Due to the unified way of assembling the coarse prior by affinity, as well as the introduction of channel-drop mechanism, the PGMA-Net trained for FSS is capable of addressing additional tasks with one suit of model weights during inference.



\subsubsection{Prior Adapter}
\label{subsubsection:PA}

\textbf{}\textbf{Textual Prior Adapter}. A straightforward way~\cite{luddecke2022image} to utilize CLIP~\cite{radford2021learning} entails extracting semantically enriched visual features for downstream decoder. However, it is contradictory to our suggested underlying class-agnostic principle. Thus, we opt to first transform them into class-agnostic textual priors without requiring any additional training, and extend CLIP's ability in integration visual and textual features from image-level to spatial positioning. Specifically, prompt engineering is utilized to obtain the text embedding  for each category, where  is the dimension of the embedding space. Meanwhile, the visual feature  is obtained by feeding the \textit{value} of patch embedding to the last projection layer of the CLIP model, without the usage of pooling layer. The cosine similarity is calculated between  and , followed by a min-max normalization to highlight the area of interest. The class-agnostic textual prior in terms of probability map is obtained as:


where  is set to 1-8 to avoid division by zero. For brevity, we adopt  to denote the normalized visual-textual prior . The resultant CLIP prior is generated for both support and query images with up-sampling to image size, termed as  and , respectively.

\textbf{}\textbf{Visual Prior Adapter}. The basic visual prior that needs to undergo assembly is the support-guided visual prior . To obtain , the maximum value of each query axis from the cross-affinity  is selected, i.e., , followed with a min-max normalization operation to highlight the intended area, where  is detailed in Equation~\ref{Asq}. Besides, the annotation of support image can also serve as prior for aggregation. Concretely, we can leverage down-sampled ground-truth mask of support  and clip prior of support  as prior.

Taken together, these four priors () form the basic components for further assembly.

\subsubsection{Affinity Extractor}
\label{subsubsection:AE} 
The cross-affinity between support and query images is the first criterion for assembling. For layer  of the pre-trained backbone, support feature , query feature  and the down-sampled support mask  are obtained. For brevity, the layer index  is omitted. Then a transform is first applied as:

where  indicates the reshape operation: , and  denotes Hadamard product to discard the irrelevant features. Then the  cross-affinity  is calculated as:

where  indicates feature transpose. Previous methods ~\cite{min2021hypercorrelation,shi2022dense} rely solely on this cross-affinity  for segmentation, we contend that it's insufficient as this criterion ignores the self-affinity of both support and query images, which represent intrinsic structural information that is indispensable to understand an image. So the self-affinity of support  and that of query  are formulated identically as:


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0 \linewidth]{imgs/highorder-ae.pdf}
  \caption{The structure of our proposed affinity extractor, including parameter-free affinity extractor and high-order affinity extractor.}
  \label{fig:ae}
\end{figure}


The above three affinities are generated by parameter-free process, implying a simple yet effective set of criteria. However, this also limits their capacity for accommodating large variations among text, support and query~\cite{ZhangWWG22}. Therefore, we propose a high-order affinity extractor to enlarge its capacity, implemented by multi-head cross-attention mechanism~\cite{vaswani2017attention}. The high-order version of cross-affinity  is calculated as:

where the affinity  is considered as \textit{QUERY}\footnote[1]{The \textit{QUERY} in attention mechanism, is different from the query image in few-shot segmentation task, we use uppercase italics "\textit{QUERY}" and regular text "query" to distinguish them.} sequence with the length of , and in dim of . And the affinity  is considered as the \textit{KEY} and \textit{VALUE} sequence with the length of , and in dim of . The ,  and  denote the feed-forward network, layer normalization and multi-head attention respectively. Similarly, the high-order version of self-affinity  is calculated by considering  as \textit{QUERY},  as \textit{KEY} and \textit{VALUE}:

The high-order affinity extractor differs from~\cite{ZhangWWG22} in training-free input affinity, extra outputs (high-order self-affinity) and subsequent class-agnostic usage. In total, four affinities are obtained: . 


\subsubsection{Prior-Guided Mask Assemble Module}
\label{subsubsection:assemble}


Once priors are obtained, a straightforward way to explore such priors is to multiply them with image features at pixel-level via Hadamard product, yielding refined semantically rich features corresponding to its category by pooling operation. However, such a naive operation does not adhere to our suggested class-agnostic principle, and fails to fully exploit the possible interactions (visual-textual, intra- or inter-image), along with issue of information loss due to the pooling operation. Thus, we propose a new unified Prior-Guided Mask Assemble Module (PGMAM), achieving the class-agnostic prior-to-mask mapping by assembling diverse priors according to corresponding affinities at pixel-level. 


\begin{figure}[h]
  \centering
  \includegraphics[width=1.0 \linewidth]{imgs/assemble.pdf}
  \caption{The pipeline of PGMAM (consisting multiple GAUs): it achieves a class-agnostic assembly of diverse priors and corresponding affinities at pixel-level in a unified way. Regardless of whether prior is from support or query, the GAU consistently follows the pipeline of emphasizing the affinity along a specific axis and then integrates it through matrix multiplication and normalization process.}
  \label{fig:assemble}
\end{figure}

As shown in Figure~\ref{fig:assemble}, the GAU follows the pipeline of highlighting the affinity along a particular axis, and then integrating through matrix multiplication and min-max normalization:

This module is unified regardless of whether the input affinity is parameter-free ( and ) or high-order ( and ), or whether the prior is from support () or query (). We refer the GAU as parameter-free mask assemble unit (PFMAU) when the affinity is parameter-free, and high-order mask assemble unit (HOMAU) when the affinity is high-order.


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0 \linewidth]{imgs/divese_interaction.pdf}
  \caption{Visualization of heterogeneous interactions among diverse priors and affinities using GAU. Columns a to b: support and query images, c to d:  coarse prior of the query obtained through visual-textual and support-query, e to h: assembled priors by PFMAU, and i to l: assembled priors by HOMAU.}

  \label{fig:diverse_interaction}
\end{figure}


Figure~\ref{fig:diverse_interaction} presents visual representations of pre- and post-assemble states of the clip prior  (column c) and support-guided visual prior  (column d) using GAU. The proposed GAU has the potential to assemble the coarse priors into a refined prior by utilizing the 8 types of diverse interactions. The 8 assembled priors are: the parameter-free self-affinity guided prior  and , the high-order self-affinity guided prior  and , the parameter-free cross-affinity guided prior  and , and the high-order cross-affinity guided prior  and . These total 10 priors, each containing distinct information, are concatenated and subsequently fed into a hierarchical decoder for decoding into final segmentation results:



\subsubsection{Hierarchical decoder with channel-drop mechanism}
\label{subsubsection:HDCDM}
By employing the proposed PGMAM on multiscale features from various layers of a pre-trained backbone, several sets of assembled masks  are generated, with high-level and low-level  containing semantic and fine-grained information, respectively. To maximize inter-scale information utilization, a hierarchical decoder (HDCDM) is designed to decode these assembled masks from varying scales into segmentation outputs. Specifically, as shown in Figure~\ref{fig:framework}, this process involves initial feature transformation of the assembled masks at each scale using a convolution block, and if necessary, up-sampling before concatenation with higher-resolution features. 

Considering that each group of  constitutes assembled masks from different prior information sources with varying learning difficulties, to avoid the network overfitting to some simple priors and maximize the utilization of all assembly masks, we propose a channel-drop mechanism during training:

where the channel-drop probability  is a random vector of  and , with the same length as channel number in . Such channel-drop mechanism seamlessly incorporates various scenarios such as fully-supervised FSS, FSS without support mask, and zero-shot segmentation tasks into a unified framework. 


The training loss  was a weighted sum of cross-entropy loss  and dice loss :



where the weight  is set to 0.5,  is the ground-truth label,  is the predicted label. 



\begin{table*}
  \centering
  \caption{Comparison of the proposed PGMA-Net with the current SOTA on  ~\cite{shaban2017one}. Best results are in bold.}
  \label{tab:pascal_sota}
      \scalebox{0.85}{
      \begin{tabular}{cclc|cccccc|cccccc}
        \toprule
        \multirow{2}{*}{\shortstack{\textbf{Pretrain}}} & \multirow{2}{*}{\shortstack{\textbf{Backbone}}} & \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Publication}} & \multicolumn{6}{c}{\textbf{1-shot}} & \multicolumn{6}{c}{\textbf{5-shot}}  \\ 
        
        & & & &  &  &  &  & \textbf{mIoU} & \textbf{FB-IoU} &  &  &  &  & \textbf{mIoU} & \textbf{FB-IoU}  \\
        \midrule
        

        
        \multirow{22}{*}{IN1K} & \multirow{12}{*}{RN50}  & PPNet~\cite{liu2020part} & ECCV'20     & 48.6 & 60.6 & 55.7 & 46.5 & 52.8 & 69.2 & 58.9 & 68.3 & 66.8 & 58.0 & 63.0 & 75.8 \\  
        
        && PFENet~\cite{tian2020prior} & TPAMI'20  & 61.7 & 69.5 & 55.4 & 56.3 & 60.8 & 73.3 & 63.1 & 70.7 & 55.8 & 57.9 & 61.9 & 73.9  \\ 
        
        && RePRI~\cite{boudiaf2021few} & CVPR'21 & 59.8 & 68.3 & 62.1 & 48.5 & 59.7 & - & 64.6 & 71.4 & 71.1 & 59.3 & 66.6 & -  \\
        
        && HSNet~\cite{min2021hypercorrelation} & ICCV'21   & 64.3 & 70.7 & 60.3 & 60.5 & 64.0 & 76.7 & 70.3 & 73.2 & 67.4 & 67.1 & 69.5 & 80.6  \\

        && SSP~\cite{fan2022self}  & ECCV'22   & 60.5 & 67.8& 66.4 & 51.0& 61.4& - & 67.5 & 72.3 & 75.2 & 62.1 & 69.3 & - \\

        && DCAMA~\cite{shi2022dense} & ECCV'22  & 67.5 & 72.3 & 59.6 & 59.0 & 64.6 & 75.7 & 70.5 & 73.9 & 63.7 & 65.8 & 68.5 & 79.5  \\  

        && CATrans~\cite{ZhangWWG22} & IJCAI'22  & 67.6 & 73.2 & 61.3 & 63.2 & 66.3 & - & 75.1 & 78.5 & 75.1 & 72.5 & 75.3 & -  \\  

        && DPCN~\cite{liu2022dynamic} & CVPR'22 & 65.7 & 71.6 & 69.1 & 60.6 & 66.7 & 78.0 & 70.0 & 73.2 & 70.9 & 65.5 & 69.9 & 80.7  \\  
        
        && RPMG-FSS~\cite{zhang2023rpmg} & TCSVT'23  & 64.4 & 72.6 & 57.9 & 58.4 & 63.3 & - & 65.3 & 72.8 & 58.4 & 59.8 & 64.1 & -  \\  

        && HPA\cite{cheng2023hpa} & TPAMI'23  & 65.9 & 72.0 & 64.7 & 56.8 & 64.8 & 76.4 & 70.5 & 73.3 & 68.4 & 63.4 & 68.9 & 81.1  \\  

        && FECANet~\cite{liu2023fecanet} & TMM'23  & 69.2 & 72.3 & 62.4 & 65.7 & 67.4 & 78.7 & 72.9 & 74.0 & 65.2 & 67.8 & 70.0 & 80.7  \\

        && ABCNet~\cite{Wang_2023_CVPR} & CVPR'23 &68.8&73.4&62.3&59.5&66.0&76.0&71.7&74.2&65.4&67.0&69.6&80.0 \\  
        \cline{2-16} \-2.0ex]
        
        &\multirow{5}{*}{RN101} & PFENet~\cite{tian2020prior} & TPAMI'20  & {36.8} & {41.8} & {38.7} & {36.7} & {38.5} & {63.0} & {40.4} & {46.8} & {43.2} & {40.5} & {42.7} & {65.8} \\
        && HSNet~\cite{min2021hypercorrelation} & ICCV'21   & {37.2} & {44.1} & {42.4} & {41.3} & {41.2} & {69.1} & {45.9} & {53.0} & {51.8} & {47.1} & {49.5} & {72.4} \\
        && SSP~\cite{fan2022self}  & ECCV'22   & 39.1 & 45.1& 42.7 & 41.2& 42.0& - & 47.4 & 54.5 & 50.4 & 49.6 & 50.2 & -  \\
        && DCAMA~\cite{shi2022dense} & ECCV'22  & 41.5 & 46.2 & 45.2 & 41.3 & 43.5 & 69.9 & 48.0 & 58.0 & 54.3 & 47.1 & 51.9 & 73.3  \\  
        && HPA\cite{cheng2023hpa} & TPAMI'23  & 43.1 & 50.0 & 44.8 & 45.2 & 45.8 & 68.4 & 49.2 & 57.8 & 52.0 & 50.6 & 52.4 & 74.0  \\  
        \midrule 
     
        \multirow{5}{*}{CLIP} &CLIP-ViT-B/16&  CLIPSeg(COCO)~\cite{luddecke2022image} & CVPR'22 & - & - & - & - & 33.2 & - & - & - & - & - & - & - \\
        &CLIP-ViT-B/16&  CLIPSeg(COCO+N)~\cite{luddecke2022image} & CVPR'22 & - & - & - & - & 33.3 & - & - & - & - & - & - & - \\ 
              
        &CLIP-RN50& PGMA-Net (ours) &- &  49.9 & 56.7 & 55.8 & 54.7 & 54.3 & 75.8 & 49.5 & 61.7 & 59.1 & 57.9 & 57.1 & 76.7 \\
        
        &CLIP-RN101& PGMA-Net (ours)  &-&  \textbf{55.2} & \textbf{62.7} & \textbf{60.3} & \textbf{59.4} & \textbf{59.4} & \textbf{78.5} & \textbf{55.9} & \textbf{65.9} & \textbf{63.4} & \textbf{61.9} & \textbf{61.8} & \textbf{79.4} \\


        \bottomrule
      \end{tabular}
      }


  \hfill
\end{table*}



\begin{table*}[h]
  \centering
  \caption{Comparison of zero-shot segmentation task on ~\cite{shaban2017one}.}
  \label{tab:zss}\scalebox{0.8}{ 
        \begin{tabular}{cccc|cccccc}
        \toprule
        \textbf{Method} &\textbf{Backbone} &\textbf{Using CLIP} & \textbf{Publication} &  &  &  &  &  & \\
        \midrule
        SPNet~\cite{xian2019semantic} & RN101 &no & CVPR'19 &23.8&17.0&14.1&18.3&18.3&44.3 \\
        ZS3Net~\cite{bucher2019zero} & RN101 &no & NeurIPS'19 &40.8&39.4&39.3&33.6&38.3&57.7 \\
        LSeg~\cite{li2022languagedriven} & RN101 &yes &ICLR'22 &  52.8&53.8&44.4&38.5&47.4&64.1  \\ 
        LSeg~\cite{li2022languagedriven} & ViT-L/16 &yes &ICLR'22  &61.3&63.6&43.1&41.0&52.3&67.0 \\ 
        SAZS~\cite{liu2023delving} & DRN  &yes &CVPR'23  &57.3&60.3&58.4&45.9&55.5&66.4 \\ 
        SAZS~\cite{liu2023delving} & ViT-L &yes &CVPR'23  &62.7&64.3&60.6&50.2&59.4&69.0 \\ 
        \midrule
        PGMA-Net (ours) w/o fine-tuning & RN50 &yes & -&  &  &  &  &  &   \\ 
        PGMA-Net (ours) -retrain for ZSS &  RN50 &yes &-&  &  &  &  &  &   \\ 
       \bottomrule

        \end{tabular}}
\end{table*}






\textbf{Co-segmentation task.}
An even more challenging task is FSS without support mask~\cite{wang2023iterative, siam2020weakly}, where solely the support image serves as guidance clue. This setup exhibits similarity to co-segmentation~\cite{zhu2020adacoseg,li2019group}, but imposes a more significant challenge in terms of generalization for evaluation on novel categories. As evidenced by Table~\ref{tab:cross-domain-bbox-image} and Figure~\ref{fig:vis}, upon being equipped with the same CLIP-RN50 backbone, albeit not specifically designed or trained for this task, PGMA-Net had already surpassed IMR-HSNet~\cite{wang2023iterative} (intended for this task) by a notable margin. (67.3 \text{v.s.} 61.5)

\textbf{Zero-Shot Segmentation Task.}
Besides, due to the integration of the channel-drop mechanism and textual prior, a singular set of parameters trained for FSS exhibits adequate capacity and flexibility to perform ZSS task. Table~\ref{tab:zss} and Figure~\ref{fig:vis} provide evidence that, even when directly applied to ZSS, our proposed PGMA-Net already outperformed the current SAZS~\cite{liu2023delving} (60.1 \text{v.s.} 59.4). Moreover, our method's superiority became unequivocal (70.6 \text{v.s.} 59.4) when it's trained specifically for ZSS.


\textbf{Robustness agsinst inaccurate support mask and image quality.}
To demonstrate the robustness of PGMA-Net against inaccurate support mask, random erosion and dilations were applied to the support ground-truth mask, three levels of corruption were implemented. As illustrated in Figure~\ref{fig:robust_1}, PGMA-Net surpassed the DCAMA~\cite{shi2022dense} and HSNet~\cite{min2021hypercorrelation} by a significant margin in the extremely inaccurate support mask scenario (mIoU of 72.5 compared to 52.7 and 45.2). Meanwhile, the robustness against image noise and distortion can also be confirmed in Figure~\ref{fig:robust_2}.

\begin{figure}[htbp]
  \centering
  \subfloat[]{
  \label{fig:robust_1}
  \includegraphics[width=0.2\textwidth]{imgs/robust_1_ours_dcama_hsnet.pdf}}
  \hspace{0.01\textwidth}
  \subfloat[]{\label{fig:robust_2}
  \includegraphics[width=0.2\textwidth]{imgs/robust_2_ours_dcama_hsnet.pdf}}
  \caption{The robustness of PGMA-Net under (a): three levels of inaccurate support mask with corruption generated by applying random erosion and dilation. (b): three levels of corruption of adding noise and distortion to image.}
  \label{fig:robustness}
\end{figure}


The comparisons on \textbf{parameter complexity, multi-adds and speed} are illustrated in Table~\ref{tab:complexity}. Our model strikes the best balance among complexity, performance and versatility.



\begin{table}[h]
  \centering
  \caption{Comparison on parameter complexity, multi-adds and speed. PGMA-Net-lite is a variant of PGMA-Net that without using high-order affinities.}
  \label{tab:complexity}
  \scalebox{0.8}{ 
  \begin{tabular}{c|ccccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}}&\textbf{Learnable}&\textbf{Multi-adds}& \textbf{Speed}& \multirow{2}{*}{\textbf{mIoU}} & \textbf{Extra}\\
    &\textbf{parameters(M)}&\textbf{(G)}&\textbf{(fps)}&&\textbf{task}\\

    \midrule
    HSNet~\cite{min2021hypercorrelation}&2.6&\textbf{20.1}&16.0 &64.0&no\\
    DCAMA~\cite{shi2022dense}&14.2&39.8&\textbf{19.7}&64.6&no\\
    VAT~\cite{HongCNLK22}&3.2&69.0&8.1&65.3&no\\
    PGMA-Net&2.7&42.3&10.5&\textbf{74.1}&\textbf{yes}\\
    PGMA-Net-lite&\textbf{1.4}&41.8&12.0&73.1&\textbf{yes}\\
    \bottomrule
  \end{tabular}
  }
\end{table}




\begin{table*}[ht]
  \centering
  \caption{Without extra fine-tuning, the trained PGMA-Net has the ability to perform additional tasks, e.g., cross-domain FSS, bounding-box level FSS and co-segmentation tasks.}
  \label{tab:cross-domain-bbox-image} 
      \scalebox{0.8}{
      \begin{tabular}{cl|cccccc|cccccc}
              \toprule
              \multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{1-shot}} & \multicolumn{6}{c}{\textbf{5-shot}} \\ 
              
              & &  &  &  &  & \textbf{mIoU}  & \textbf{FB-IoU} &  &  &  &  & \textbf{mIoU} & \textbf{FB-IoU} \\

              \midrule
              \multirow{4}{*}{\shortstack{cross-domain\\FSS task}}
              & HSNet~\cite{min2021hypercorrelation}   & 48.7 & 61.5 & 63.0 & 72.8 & 61.5 & - & 58.2 & 65.9 & 71.8 & 77.9 & 68.4 & - \\
              & CWT~\cite{lu2021simpler}    & 53.5 & 59.2 & 60.2 & 64.9 & 59.4 & - & 60.3 & 65.8 & 67.1 & 72.8 & 66.5 & - \\
              & CDFSS~\cite{wang2022remember}   & \textbf{57.4} & 62.2 & 68.0 & 74.8 & 65.6 & - & \textbf{65.7} & 69.2 & 70.8 & 75.0 & 70.1 & - \\
              \cline{2-14} \-2.0ex]
              & PGMA-Net (ours)   & \textbf{72.4} & \textbf{80.8} & \textbf{68.9} & \textbf{70.7} & \textbf{73.2} & \textbf{82.5} & \textbf{73.1} & \textbf{81.5} & \textbf{69.8} & \textbf{72.1} & \textbf{74.1} & \textbf{83.1}  \\

              \midrule
              \multirow{5}{*}{\shortstack{co-segmentation\-2.0ex]
              & PGMA-Net (ours)  & \textbf{68.6} & \textbf{76.3} & \textbf{60.3} & \textbf{64.1} & \textbf{67.3} & \textbf{77.8} & \textbf{68.9} & \textbf{76.6} & \textbf{60.5} & \textbf{64.1} & \textbf{67.5} & \textbf{78.0}  \\
              
              \bottomrule
      \end{tabular}
      }

  
  
  \hfill

\end{table*}







\begin{table*}[ht]
  \small
  \centering
  \caption{The ablation studies to evaluate the effects of diverse interactions among priors and affinities of PGMA-Net. While lines 1-4 include only query image, which constitutes 0-shot segmentation task, lines 5-8 comprise both query and support images, thus leading to 1-shot FSS task.}
  \label{tab:ablation_implemetation}
  \scalebox{0.7}{
  \begin{tabular}{cccccccccccc|cccc}
  \toprule
  \multirow{2}{*}{\textbf{No.}}&\multirow{2}{*}{\textbf{Description}}&\multirow{2}{*}{} &\multirow{2}{*}{}&\multirow{2}{*}{} &\multirow{2}{*}{}&\multirow{2}{*}{}&\multirow{2}{*}{}&\multirow{2}{*}{}&\multirow{2}{*}{ }&\multirow{2}{*}{}&\multirow{2}{*}{}& \multicolumn{3}{c}{\textbf{0/1-shot}}\\
  &&&&&&&&&&&&\textbf{mIoU}&\textbf{FB-IoU}&  \\     
  \midrule
       1& only&&&&&&&&&&&66.1&76.8&-\\
       2&, w/  &&&&&&&&&&&69.2&79.2&3.2\\
       3&, w/  &&&&&&&&&&&69.0&79.4&2.9\\
       4&, w/  and &&&&&&&&&&&70.6&80.0&4.5\\
       \midrule
       5& and  only &&&&&&&&&&&71.6&81.4&5.5\\
       6& and , w/  &&&&&&&&&&&73.1&82.6&7.0\\
       7& and , w/ &&&&&&&&&&&72.6&81.9&6.5\\
       8& and , w/  and &&&&&&&&&&&74.1&83.5&8.0\\
  \bottomrule
  \end{tabular}
  }
\end{table*}






\begin{table}
  \parbox{0.4\linewidth}{
  \centering
  \caption{Ablations on channel-drop, model hierarchy, training loss and HOMAU.}
  \label{tab:ablation_on_channel-drop_and_hierarchy_and_loss}
  \scalebox{0.8}{ 
       \begin{tabular}{c|c}
            \toprule
            \textbf{Method}&\textbf{mIoU}\\
            \midrule
            PGMA-Net&74.1\\
            w/o channel-drop&72.8\\
            w/o hierarchy&71.1\\
            w/o DICE&72.2\\
            w/o CE&73.6\\
            w/o HOMAU&73.1\\
            \bottomrule
       \end{tabular}
  }}
  \hfill
  \parbox{0.5\linewidth}{
    \centering
    \caption{Ablation on switching backbone from CLIP to IN1K-pretrained.}
    \label{tab:ablation_backbone}
    \scalebox{0.85}{ 
      \begin{tabular}{c|cc}
        \toprule
        \textbf{Backbone}&\textbf{Method}&\textbf{mIoU}\\
        \midrule
        \multirow{2}{*}{CLIP-RN50}&CLIPSeg~\cite{luddecke2022image}&59.5\\
        &PGMA-Net&\textbf{74.1}\\
        \midrule
        \multirow{2}{*}{IN1K-RN50}&CLIPSeg~\cite{luddecke2022image}&39.0\\
        &PGMA-Net&\textbf{65.0}\\
        \bottomrule
      \end{tabular}
    }}
\end{table}

\subsection{Ablations}

To examine the influence of key components in our model, we conducted comprehensive ablation analysis. We utilized the CLIP-RN50 backbone on  dataset~\cite{shaban2017one} in 1-shot scenario for all ablation experiments.


\textbf{Ablation on the diverse interactions between priors and affinities.} 
The PGMA-Net proposed within this study contains ten distinctive interactions, detailed in Table~\ref{tab:ablation_implemetation}: 1) Lines 1-4 investigate the scenario in which solely the query image and text are available (ZSS task), with Line 1 acting as the baseline, achieving an mIoU of 66.1. The incorporation of different affinities, including a training-free affinity (Line 2: mIoU=69.2), high-order affinity (Line 3: mIoU=69.0), and the use of both affinities (Line 4: mIoU=70.6), leads to a noticeable and consistent improvement in overall performance. 2) Within the scope of the FSS task, Lines 5-8 simultaneously integrate both query and support images, yielding superior results as articulated by the mIoU gains of  with diverse affinities. Moreover, it becomes clear that the inclusion of affinities and the corresponding support-derived information can serve to effectively enhance performance in a complementary manner.

\textbf{Ablation on the channel-drop mechanism.} The influence of the channel-drop mechanism was investigated through Table~\ref{tab:ablation_on_channel-drop_and_hierarchy_and_loss}, showing an mIoU increase from  to  with the help of channel-drop mechanism.


\textbf{Ablation on the model hierarchy.} The impact of the hierarchical structure was studied using two methods: 1) using the final layer of each stage of RN50, which produced features with a length of (). 2) using all layers within each stage, which generated features with a length of (). Ablation study revealed that utilizing all features within each stage significantly improved performance (71.1 \text{v.s.} 74.1), as shown in Table~\ref{tab:ablation_on_channel-drop_and_hierarchy_and_loss}.


\textbf{Ablation on training loss function.} 
The default loss function of our proposed method is a weighted sum of cross-entropy loss and dice loss. An ablation study was carried out to examine the sensitivity of the two loss functions. Table~\ref{tab:ablation_on_channel-drop_and_hierarchy_and_loss} displays the results, confirming that integrating cross-entropy loss and dice loss leads to a substantial performance enhancement ().


\textbf{Ablation on switching backbone from CLIP to IN1K-pretrained.} To investigate the robustness on switching backbone from CLIP to IN1K-pretrained, we removed the text branch and re-train our model with IN1K-RN50. As shown in Table~\ref{tab:ablation_backbone}, albeit not specifically designed for this setting, this simple variant of PGMA-Net demonstrates a high level of compatibility and yields comparable results to recent traditional FSS methods (our 65.0 v.s. RPMG-FSS 63.3, HPA 64.8, DCAMA 64.6, only slightly lower than newest FECANet). Also, compared to CLIPSeg (), our PGMA-Net () showed remarkable robustness, thus confirming the effectiveness of prior assembly from only visual cues.


 \section{Conclusion}

In this paper, we proposed PGMA-Net, a class-agnostic model for few-shot segmentation by integrating textual information and a new prior-to-mask mapping. We introduced a general assemble unit (GAU) and a prior-guided mask assemble module (PGMAM) to fully exploit diverse interactions across different priors and affinities in a unified manner. We also proposed a hierarchical decoder with channel-drop strategy (HDCDM), which enables the model to perform additional challenging tasks without extra fine-tuning, thus leading an any-shot segmentation framework. Our approach achieved promising performance across various tasks, including FSS, ZSS, co-segmentation, box-level and cross-domain FSS tasks. In future work, we will focus on the N-way problem and further investigate the general few-shot segmentation task to advance the research and application in this area. 

{\small
\bibliographystyle{ieeetr}
\bibliography{egbib}
}


\end{document}