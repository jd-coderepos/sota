

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}


\usepackage{microtype}

\usepackage{subfig}
\usepackage{graphicx}

\aclfinalcopy \def\aclpaperid{810} 

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsfonts}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  
\newcommand{\ub}[1]{\underline{\textbf{#1}}}
\newcommand{\interalia}[1]{\citep[{inter alia}]{#1}}

\newcommand{\An}[1]{\textcolor{black}{#1}}
\newcommand*{\Liu}{\textcolor{blue}}






\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Learning Algebraic Recombination for Compositional Generalization}




\author{
  Chenyao Liu\thanks{\quad Work done during an internship at Microsoft Research. The first two authors contributed equally to this paper.} \quad Shengnan An \quad Zeqi Lin\thanks{\quad Corresponding author.} \quad Qian Liu \quad Bei Chen \\
  \textbf{ \quad Jian-Guang LOU  \quad Lijie Wen \quad Nanning Zheng  \quad Dongmei Zhang}\\
  
   School of Software, Tsinghua University \quad  Xi'an Jiaotong University\\
   Microsoft Research Asia \quad  Beihang University\\
  \texttt{\{liucy19@mails, wenlj@\}.tsinghua.edu.cn} \\
  \texttt{\{an1006634493@stu, nnzheng@mail\}.xjtu.edu.cn} \\
  \texttt{\{Zeqi.Lin, beichen, jlou, dongmeiz\}@microsoft.com} \\
  \texttt{qian.liu@buaa.edu.cn} 
}
  

\date{}

\newtheorem{property}{Property}

\begin{document}
\maketitle
\begin{abstract}
Neural sequence models exhibit limited \emph{compositional generalization} ability in semantic parsing tasks. Compositional generalization requires \emph{algebraic recombination}, i.e., dynamically recombining structured expressions in a recursive manner.
However, most previous studies mainly concentrate on recombining lexical units, which is an important but not sufficient part of algebraic recombination.
In this paper, we propose \textsc{LeAR}, an end-to-end neural model to learn algebraic recombination for compositional generalization.
The key insight is to model the semantic parsing task as a homomorphism between a latent syntactic algebra and a semantic algebra, thus encouraging algebraic recombination.
Specifically, we learn two modules jointly: a Composer for producing latent syntax, and an Interpreter for assigning semantic operations.
Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model.
The source code is publicly available at \href{https://github.com/microsoft/ContextualSP}{https://github.com/microsoft/ContextualSP}.
\end{abstract}

\section{Introduction}

The principle of compositionality is an essential property of language: the meaning of a complex expression is fully determined by its structure and the meanings of its constituents \cite{pelletier2003context-tpoc, szabo2004compositionality-tpoc}.
Based on this principle, human intelligence exhibits \textit{compositional generalization} --- the algebraic capability to understand and produce a potentially infinite number of novel expressions by dynamically recombining known components \cite{chomsky1957syntactic-hicg, fodor_connectionism_1988-hicg, fodor2002compositionality-hicg}.
For example, people who know the meaning of ``\textit{John teaches the girl}'' and ``\textit{Tom's daughter}'' must know the meaning of
``\textit{Tom teaches John's daughter's daughter}'' (Figure \ref{fig:prod}), even though they have never seen such complex sentences before.

In recent years, there has been accumulating evidence that end-to-end deep learning models lack such ability in semantic parsing (i.e., translating natural language expressions to machine interpretable semantic meanings) tasks \cite{lake2018scan, keysers2019measuring, kim2020cogs, tsarkov2020cfq-lcg}.

\begin{figure}
  \centering
  \subfloat[][Compositional generalization requires \textbf{algebraic recombination}, i.e., dynamically recombining structured expressions in a recursive manner.\label{fig:prod}]{\includegraphics[width=0.37\textwidth]{prod2.pdf}}\\
  \subfloat[][Most previous studies mainly concentrate on recombining lexical units, which is an important but not sufficient part of algebraic recombination.\label{fig:sys}]{\includegraphics[width=0.45\textwidth]{sys2.pdf}}
  \caption{Compositional generalization.
  }
\end{figure}

Compositional generalization requires \textbf{algebraic recombination}, i.e., dynamically recombining structured expressions in a recursive manner.
In the example in Figure \ref{fig:prod}, understanding ``\emph{John's daughter's daughter}'' is a prerequisite for understanding ``\emph{Tom teaches John's daughter's daughter}'', while ``\emph{John's daughter's daughter}'' is also a novel compound expression, which requires recombining ``\emph{John}'' and ``\emph{Tom's daughter}'' recursively.


Most previous studies on compositional generalization mainly concentrate on recombining lexical units (e.g., words and phrases) \cite{nips2019dataaug,li2019compositional,andreas2019dataaug,Gordon2020spstru,aky2020dataaug,guo2020dataaugl,russin2019spstru}, of which an example is shown in Figure \ref{fig:sys}.
This is a necessary part of algebraic recombination, but it is not sufficient for compositional generalization.
There have been some studies on algebraic recombination \cite{liu2020spstru, chen2020spstru}.
However, they are highly specific to a relative simple domain SCAN \cite{lake2018scan} and can hardly generalize to more complex domains.

In this paper, our main point to achieve algebraic recombination is to \textbf{model semantic parsing as a homomorphism between a latent syntactic algebra and a semantic algebra} \cite{montague1970universal-algre, marcus2019algebraic}.
Based on this formalism, we focus on learning the high-level mapping between latent syntactic operations and semantic operations, rather than the direct mapping between expression instances and semantic meanings.







Motivated by this idea, we propose \textsc{LeAR} (\textbf{Le}arning \textbf{A}lgebraic \textbf{R}ecombination), an end-to-end neural architecture for compositional generalization.
\textsc{LeAR} consists of two modules: a \textit{Composer} and an \textit{Interpreter}.
Composer learns to model the latent syntactic algebra, thus it can produce the latent syntactic structure of each expression in a bottom-up manner;
Interpreter learns to assign semantic operations to syntactic operations, thus we can transform a syntactic tree to the final composed semantic meaning.





Experiments on two realistic and comprehensive compositional generalization benchmarks (\textit{CFQ} \cite{keysers2019measuring} and \textit{COGS} \cite{kim2020cogs}) demonstrate the effectiveness of our model:
CFQ , COGS .



\section{Compositionality: An Algebraic View}\label{section:algebra}

A semantic parsing task aims to learn a meaning-assignment function , where  is the set of (simple and complex) expressions in the language, and  is the set of available semantic meanings for the expressions in .
Many end-to-end deep learning models are built upon this simple and direct formalism, in which the principle of compositionality is not leveraged, thus exhibiting limited compositional generalization.

To address this problem, in this section we put forward the formal statement that ``\textit{compositionality requires the existence of a homomorphism between the expressions of a language and the meanings of those expressions}'' \cite{montague1970universal-algre}.

Let us consider a language as a partial algebra , where  is the set of underlying syntactic (grammar) rules, and we use  to denote the syntactic operation with a fixed arity  for each .
Note that  is a partial function, which means that we allow  be undefined for certain expressions.
Therefore,  is a partial algebra, and we call it a \textbf{syntactic algebra}.
In a semantic parsing task,  is latent, and we need to model it by learning from data.

Consider now , where  are semantic operations upon .
 is also a partial algebra, and we call it a \textbf{semantic algebra}.
In a semantic parsing task, we can easily define this algebra (by enumerating all available semantic primitives and semantic operations), since  is a machine-interpretable formal system.

The key to compositionality is that the meaning-assignment function  should be a homomorphism from  to .
That is, for each -ary syntactic operation  in ,
there exists a -ary semantic operation  such that whenever  is defined,


Based on this formal statement, the task of learning the meaning-assignment function  can be transformed as two sub-tasks: (1) learning latent syntax of expressions (i.e., modeling the syntactic algebra ); (2) learning the operation assignment function .

\noindent \textbf{Learning latent syntax}.
We need to learn a syntactic parser that can produce the syntactic structure of each given expression.
To ensure compositional generalization, there must be an underlying grammar (i.e., ), and we hypothesize that  is a context-free grammar.


\noindent \textbf{Learning operation assignment}.
In the syntax tree, for each nonterminal node with  nonterminal children, we assign a -ary semantic operation to it.
This operation assignment entirely depends on the underlying syntactic operation  of this node.

In semantic parsing tasks, we do not have respective supervision for these two sub-tasks.
Therefore, we need to jointly learning these two sub-tasks only from the end-to-end supervision .

\section{Model}\label{sec:Model}

We propose a novel end-to-end neural model \textsc{LeAR} (\textbf{Le}arning \textbf{A}lgebraic \textbf{R}ecombination) for compositional generalization in semantic parsing tasks.
Figure \ref{fig:framework} shows its overall architecture.
\textsc{LeAR} consists of two parts:
(1) \emph{Composer} , which produces the latent syntax tree  of input expression ;
(2) Interpreter , which assigns a semantic operation for each nonterminal node in .
 and  refers to learnable parameters in them respectively.
We generate a semantic meaning  according to the predicted  and  in a symbolic manner, then check whether it is semantic equivalent to the ground truth semantic meaning  to produce rewards for optimizing  and .

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{framework2.pdf}
    \caption{An overview of \textsc{LeAR}: (1) Composer  is a neural network based on latent Tree-LSTM, which produces the latent syntax tree  of input expression ; (2) Interpreter  is a neural network that assigns a semantic operation for each nonterminal node in .}
    \label{fig:framework}
\end{figure}

\subsection{Composer}

We use  to denote an input expression of length .
Composer  will produce a latent binary tree  given .




\subsubsection{Latent Tree-LSTM}

We build up the latent binary tree  in a bottom-up manner based on Tree-LSTM encoder, called latent Tree-LSTM \cite{choi2018learning-lt, havrylov2019cooperative-lt}.

Given the input sequence  of length , latent Tree-LSTM merges two nodes into one parent node at each merge step, constructing a binary tree after  merge steps.
The merge process is implemented by selecting the adjacent node pair which has the highest merging score.

At the -th () merge step, we have:


Here ``Tree-LSTM'' is the standard child-sum tree-structured LSTM encoder \cite{tree_lstm_2015-trls}.
We use  to denote the -th cell at layer  (the -th merge step is determined by the -th layer), and use  to denote the representation of :



Then we can obtain a unlabeled binary tree, in which  are leaf nodes, and  are non-leaf nodes.









\subsubsection{Abstraction by Nonterminal Symbols}\label{sec:REDUCE}

As discussed in Section \ref{section:algebra}, our hypothesis is that the underlying grammar  is context-free.
Therefore, each syntactic rule  can be expressed in the form of:

where  is a finite set of nonterminals, and  is a finite set of terminal symbols.

\textbf{Abstraction} is an essential property of context-free grammar:
each compound expression  will be abstracted as a simple nonterminal symbol , then it can be combined with other expressions to produce more complex expressions, no matter what details  originally has.
This setup may benefit the generalizability, thus we want to incorporate it as an inductive bias into our model.



Concretely, we assume that there are at most  latent nonterminals in language  (i.e., , where  is a hyper-parameter).
For each node  in tree , we perform a -class classification:


We assign the nonterminal  to  when .
The collection of such nonterminal nodes are denoted as .
Then we modify Equation \ref{eq:treelstm}:


Equation \ref{eq:r} means that: in nonterminal nodes, the bottom-up message passing will be reduced from  to a nonterminal symbol , thus mimicking the abstraction setup in context-free grammar.











\subsection{Interpreter}







For each nonterminal node , Interpreter  assigns a semantic operation  to it.

We divide nonterminal nodes into two categories:
(1) \textit{lexical nodes}, which refer to those containing no any other nonterminal node in the corresponding sub-trees;
(2) \textit{algebraic nodes}, which refer to the rest of nonterminal nodes.

\paragraph{Interpreting Lexical Nodes}

For each lexical node , Interpreter assigns a semantic primitive (i.e., -ary semantic operation) to it.
Take the CFQ benchmark as an example:
it uses SPARQL queries to annotate semantic meanings, thus semantic primitives in CFQ are entities (e.g., \textit{m.0gwm\_wy}), predicates (e.g., \textit{ns:film.director.film}) and attributes (e.g., \textit{ns:people.person.gender m\_05zppz}).

We use a classifier to predict the semantic primitive:

where  is the collection of semantic primitives in the domain, and  is the contextual representation of the span corresponding to  (implemented using Bi-LSTM).
\textit{Contextually conditioned variation} is an important phenomenon in language: the meaning of lexical units varies according to the contexts in which they appear \cite{allwood2003meaning-ccv}.
For example, ``\emph{editor}'' means a predicate ``\emph{film.editor.film}'' in expression ``\emph{Is M0 an editor of M1?}'', while it means an attribute ``\emph{film.editor}'' in expression ``\emph{Is M0 an Italian editor?}''.
This is the reason why we use contextual representation in Equation \ref{eq:classifier}.

\paragraph{Interpreting Algebraic Nodes}

For each algebraic node , Interpreter assigns a semantic operation to it.
The collection of all possible semantic operations  also depends on the domain.
Take the CFQ benchmark as an example\footnote{It is not difficult to define  and  for each domain, as semantic meanings are always machine-interpretable. The semantic operations of another compositional generalization benchmark, COGS, are listed in the Appendix.}, this domain has two operations (detailed in Table \ref{tab:CFQ_operations}):  (conjunction) and JOIN.

We also use a classifier to predict the semantic operation of :

where  is the latent Tree-LSTM representation of node  (see Equation \ref{eq:r}).

In Equation \ref{eq:opr}, we do not use any contextual information from outside .
This setup is based on the assumption of \textbf{semantic locality}: each compound expression should mean the same thing in different contexts.

\begin{table}[t]
    \renewcommand\arraystretch{1.1}
    \centering
    \scalebox{0.79}{
    \begin{tabular}{ccl}
    \hline
    \textbf{Operation} & \begin{tabular}[c]{@{}c@{}}\small{\textbf{Args[, ]}}\\\small{\textbf{Result Type}}\end{tabular} & \multicolumn{1}{c}{\textbf{Example}} \\ \hline
    \multirow{7}{*}{(, )} 
    & [P, P]P & Who [\ub{direct} and \ub{act}] M0? \\ \cline{2-3} 
     & [E, E]E & Who direct [\ub{M0} and \ub{M1}]? \\ \cline{2-3} 
     & [A, A]A & Is M0 an [\ub{Italian} \ub{female}]? \\ \cline{2-3} 
     & [A, E]E & \multirow{2}{*}{Is [\ub{M0} an \ub{Italian female}]?} \\
     & [E, A]E &  \\ \cline{2-3} 
     & [A, P]P & \multirow{2}{*}{Is M0 M3's [\ub{Italian} \ub{editor}]?} \\
     & [P, A]P &  \\ \hline
    \multirow{4}{*}{JOIN(, )} 
    & [E, P]E & \multirow{2}{*}{Is M0 an [\ub{editor} of \ub{M1}]?} \\
     & [P, E]E &  \\ \cline{2-3} 
     & [A, P]E & \multirow{2}{*}{Who [\ub{marries} an \ub{Italian}]?} \\
     & [P, A]E &  \\ \hline
    \end{tabular}
    }
    \caption{Semantic operations in CFQ.
    A/P/E represents Attribute/Predicate/Entity.}
    \label{tab:CFQ_operations}
\end{table}







\section{Training}




Denote  as the trajectory produced by our model where  and  are actions produced from  Composer and Interpreter, respectively, and  as the reward of trajectory  (elaborated in Sec.~\ref{sec:reward}).
Using policy gradient \cite{policy_gradient_1999} with the likelihood ratio trick, our model can be optimized by ascending the following gradient:

where  and  are learnable parameters in Composer and Interpreter respectively and  is the abbreviation of . 
Furthermore, the REINFORCE algorithm \cite{reinforce_algo_1992} is leveraged to approximate Eq.~\ref{eq:gradient} and the mean-reward baseline \cite{reinforce_baseline_2001} is employed to reduce variance.















\subsection{Reward Design}\label{sec:reward}
The reward  combines two parts as:



\noindent \textbf{Logic-based Reward} .
We use ) and  to denote the predicted semantic meaning and the ground truth semantic meaning respectively.
Each semantic meaning can be converted to a conjunctive normal form\footnote{
For example, the semantic meaning of ``\emph{Who directed and edited  's prequel and ?}'' can be converted to a conjunctive normal form with four components:
``'',
``'',
``'', and
``''.}.
We use  and  to denote conjunctive components in ) and , then define  based on Jaccard similarity (i.e., intersection over union):


\noindent \textbf{Primitive-Based Reward} .
We use  and  to denote semantic primitives ocurred in ) and .
Then we define  as:


\subsection{Reducing Search Space}
To reduce the huge search space of , we make two constraints as follows.

\noindent \textbf{Parameter Constraint}.
Consider , a tree node with  nonterminal children.
Composer will never make  a nonterminal node, if no semantic operation has  parameters.

\noindent \textbf{Phrase Table Constraint.}
Following the strategy proposed in \citet{guo2020hierarchical}, we build a ``phrase table'' consisting of lexical units (i.e., words and phrases) paired with semantic primitives that frequently co-occur with them\footnote{Mainly based on statistical word alignment technique in machine translation, detailed in the Appendix.}.
Composer will never produce a lexical node outside of this table, and Interpreter will use this table to restrict candidates in Equation \ref{eq:classifier}.



\subsection{Curriculum Learning} \label{sec:curriculum}

To help the model converge better, we use a simple curriculum learning \cite{bengio2009curriculum} strategy to train the model.
Specifically, we first train the model on samples of input length less than a cut-off , then further train it on the full train set.







\section{Experimental Setup}

\noindent \textbf{Benchmarks}.
We mainly evaluate \textsc{LeAR} on \textit{CFQ} \cite{keysers2019measuring} and \textit{COGS} \cite{kim2020cogs}, two comprehensive and realistic benchmarks for measuring compositional generalization.
They use different semantic formulations:
CFQ uses SPARQL queries, and COGS uses logical queries (Figure \ref{fig:example} shows examples of them).
We list dataset statistics in Table \ref{tab:dataset}.
The input/output pattern coverage indicates that:
CFQ mainly measures the algebraic recombination ability, while COGS measures both lexical recombination () and algebraic recombination ().


In addition to these two compositional generalization benchmarks in which utterances are synthesized by formal grammars, we also evaluate \textsc{LeAR} on \textit{GEO} \cite{zelle1996learning-csp}, a widely used semantic parsing benchmark, to see whether \textsc{LeAR} can generalize to utterances written by real users.
We use the variable-free FunQL \cite{kate2005geofunql} as the semantic formalism, and we follow the compositional train/test split \cite{finegan2018geosplit} to evaluate compositional generalization.



\begin{figure}
\begin{minipage}[tp]{\linewidth}
\begin{minipage}[b]{\linewidth}
\centering
  \includegraphics[width=0.9\textwidth]{benchmark.pdf}
  \captionof{figure}{Examples of CFQ and COGS.}
  \label{fig:example}
\end{minipage}

\vspace{0.3cm}

\begin{minipage}[b]{\linewidth}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{ccc}
\hline
\textbf{Statistics} & \textbf{CFQ} & \textbf{COGS}   \\
\hline
Train Size & 95,743 & 24,155\\
Dev Size & 11,968 & 3,000\\
Test Size & 11,968 & 21,000\\
Vocab Size & 96 & 740	\\
\hline
Avg Input Len (Train/Test)  & 13.5/15.1 & 7.5/9.8 \\
Avg Output Len (Train/Test)  & 27.7/34.0  & 43.6/67.6 \\
Input Pattern Coverage\footnote{Input/output pattern coverage is the percentage of test / whose patterns occur in the train data. Output patterns are determined by anonymizing semantic primitives, and input patterns are determined by anonymizing their lexical units.} & 0.022 & 0.783 \\
Output Pattern Coverage & 0.045 & 0.782 \\
\hline
\end{tabular}}
\captionof{table}{Dataset statistics.}
\label{tab:dataset}
\end{minipage}
 \end{minipage}
 \end{figure}
 
\begin{table*}
\centering
\resizebox{.9\linewidth}{!}{
\begin{tabular}{lcccc}
\hline \textbf{Models} & \textbf{MCD-MEAN} & \textbf{MCD1} & \textbf{MCD2} & \textbf{MCD3} \\ \hline
LSTM+Attention \cite{keysers2019measuring} & 14.91.1 &	28.91.8 &	5.00.8 &	10.80.6 \\
Transformer \cite{keysers2019measuring} & 17.90.9 &	34.91.1 &	8.20.3 &	10.61.1 \\
Universal Transformer \cite{keysers2019measuring} & 18.91.4 &	37.42.2 &	8.11.6 &	11.30.3 \\
Evolved Transformer \cite{furrer2020compositional}  & 	20.80.7 &	42.41.0 &	9.30.8 &	10.80.2 \\
\hline 
T5-11B \cite{furrer2020compositional} & 40.94.3	& 61.44.8 &	30.12.2 &	31.25.7 \\
T5-11B-mod \cite{furrer2020compositional} & 42.19.1 &	61.612.4 &	31.312.8 &	33.32.3 \\
\hline 
Neural Shuffle Exchange \cite{furrer2020compositional} & 2.80.3 &	5.10.4 &	0.90.1 &	2.30.3 \\
CGPS \cite{furrer2020compositional, li2019compositional}  & 7.11.8 &	13.23.9 &	1.60.8 &	6.60.6 \\
HPD \cite{guo2020hierarchical} & 67.34.1 & 72.07.5 &	66.16.4 &	63.95.7 \\
\hline 
\textbf{\textsc{LeAR}} & \textbf{90.91.2} & \textbf{91.71.0} & \textbf{89.21.9} & \textbf{91.70.6} \\
\hspace{2em}  w/o Abstraction & 85.44.5 & 88.41.6 & 80.011 & 87.90.8 \\
\hspace{2em}  w/o Semantic locality & 87.92.7 & 89.81.7 & 87.31.8 & 86.54.6 \\
\hspace{2em}  w/o Primitive-based reward & 85.37.8 & 77.019 & 89.22.2 & 89.72.1 \\
\hspace{2em}  w/o Curriculum learning & 71.915.4 & 59.723 & 77.213.5 & 78.89.6 \\
\hspace{2em}  w/o Tree-LSTM & 30.43.2 & 40.11.9 & 25.66.1 & 25.41.8 \\
\hline
\end{tabular}}
\caption{Accuracy on three splits (MCD1/MCD2/MCD3) of CFQ benchmark. }
\label{tab:CFQ_res}
\end{table*}



\noindent \textbf{Baselines}.
For CFQ, we consider 3 groups of models as our baselines:
(1) sequence-to-sequence models based on deep encoder-decoder architecture, including LSTM+Attention \cite{hochreiter1997long, bahdanau2014neural}, Transformer \cite{vaswani2017attention}, Universal Transformer \cite{dehghani2018universal} and Evolved Transformer \cite{so2019evolved};
(2) deep models with large pretrained encoder, such as T5 \cite{raffel2019exploring};
(3) Models that are specially designed for compositional generalization, which include Neural Shuffle Exchange Network \cite{freivalds2019neural}, CGPS \cite{li2019compositional}, and state-of-the-art model HPD \cite{guo2020hierarchical}.
For COGS, we quote the baseline results in the original paper \cite{kim2020cogs}.
For GEO, we take the baseline results reported by \citet{herzig2020span-csp}, and also compare with two specially designed methods: SpanBasedSP \cite{herzig2020span-csp} and PDE \cite{guo2020geoiterative}.

\noindent \textbf{Evaluation Metric}.
We use accuracy as the evaluation metric, i.e., the percentage test samples of which the predicted semantic meaning  is semantically equivalent to the ground truth .

\noindent \textbf{Hyper-Parameters}.
We set  (the number of nonterminal symbols), and  for CFQ/COGS/GEO respectively.
In CFQ, the curriculum cut-off  is set to 11, as we statistically find that this is the smallest curriculum that contains the complete vocabulary.
We do not apply curriculum learning strategy to COGS and GEO, as \textsc{LeAR} can work well without curriculum learning in both benchmarks.
Learnable parameters ( and ) are optimized with AdaDelta \cite{adadelta_2012-apd}, and the setting of learning rate is discussed in Section \ref{section:ablation}.
We take the model that performs best on the validation set for testing, and all results are obtained by averaging over 5 runs with different random seeds.
See Appendix for more implementation details.

\section{Results and Discussion}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc}
    \hline
    \textbf{Model} & \textbf{Acc} \\ \hline
    Transformer \cite{kim2020cogs} & 35 ± 6 \\
    LSTM (Bi) \cite{kim2020cogs}  & 16 ± 8 \\
    LSTM (Uni) \cite{kim2020cogs}  & 32 ± 6 \\ \hline
    \textbf{\textsc{LeAR}} & \textbf{97.7 ± 0.7} \\ 
    \hspace{2em} w/o Abstraction & 94.5 ± 2.8 \\ 
    \hspace{2em}  w/o Semantic locality & 94.0 ± 3.6 \\ 
    \hspace{2em}  w/o Tree-LSTM & 80.7 ± 4.3 \\
    \hline
    \end{tabular}}
    \caption{Accuracy on COGS benchmark. }
    \label{result:COGS}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lc}
    \hline
    \textbf{Model} & \textbf{Acc} \\ \hline
    Seq2Seq \cite{herzig2020span-csp} & 46.0 \\
    BERT2Seq \cite{herzig2020span-csp} & 49.6 \\
    GRAMMAR \cite{herzig2020span-csp} & 54.0 \\ \hline
    PDE \cite{guo2020geoiterative} & 81.2 \\
    SpanBasedSP \cite{herzig2020span-csp} & 82.2 \\
    \textbf{\textsc{LeAR}} & \textbf{84.1} \\ \hline
    \end{tabular}}
    \caption{Accuracy on GEO benchmark. }
    \label{result:GEO}
\end{table}

Table \ref{tab:CFQ_res} shows average accuracy and 95\% confidence intervals on three splits of CFQ.
\textsc{LeAR} achieves an average accuracy of 90.9\% on these three splits, outperforming all baselines by a large margin.
We list some observations as follows.

\noindent \textbf{Methods for lexical recombination cannot generalize to algebraic recombination}.
Many methods for compositional generalization have been proved effective for lexical recombination.
Neural Shuffle Exchange and CGPS are two representatives of them.
However, experimental results show that they cannot generalize to CFQ, which focus on algebraic recombination.

\noindent \textbf{Knowledge of semantics is important for compositional generalization}.
Seq2seq models show poor compositional generalization ability ().
Pre-training helps a lot (), but still not satisfying.
\textsc{HPD} and \textsc{LeAR} incorporate knowledge of semantics (i.e., semantic operations) into the models, rather than simply model semantic meanings as sequences.
This brings large profit.

\noindent \textbf{Exploring latent compositional structure in a bottom-up manner is key to compositional generalization.}
\textsc{HPD} uses LSTM to encode the input expressions, while \textsc{LeAR} uses latent Tree-LSTM, which explicitly explores latent compositional structure of expressions.
This is the key to the large accuracy profit ().

Table \ref{result:COGS} shows the results on COGS benchmark.
It proves that \textsc{LeAR} can well generalize to domains which use different semantic formalisms, by specifying domain-specific  (semantic primitives) and  (semantic operations).
Table \ref{result:GEO} shows the results on GEO benchmark.
It proves that \textsc{LeAR} can well generalize to utterances written by real users (i.e., non-synthetic utterances).


\subsection{Ablation Study}
\label{section:ablation}

Table \ref{tab:CFQ_res} and \ref{result:COGS} also report results of some ablation models.
Our observations are as follows.

\noindent \textbf{Abstraction by nonterminal symbols brings profit}.
We use ``\textit{w/o abstraction}'' to denote the ablation model in which Equation \ref{eq:r} is disabled.
This ablation leads to  accuracy drop on CFQ/COGS.

\noindent \textbf{Incorporating semantic locality into the model brings profit}.
We use ``\textit{w/o semantic locality}'' to denote the ablation model in which a Bi-LSTM layer is added before the latent Tree-LSTM.
This ablation leads to  accuracy drop on CFQ/COGS.

\noindent \textbf{Tree-LSTM contributes significantly to compositional generalization}. 
In the ablation ``\emph{w/o Tree-LSTM}'', we replace the Tree-LSTM encoder with a span-based encoder, in which each span is represented by concatenating its start and end LSTM representations (similar to \citet{herzig2020span-csp}).
In Table \ref{tab:CFQ_res} and \ref{result:COGS}, we can see that span-based encoder severely affects the performance and even much worse than the results of ``\textit{w/o abstraction}'' and ``\textit{w/o semantic locality}''.
This ablation hints that Tree-LSTM is the main inductive bias of compositionality in our model.


\noindent \textbf{Primitive-based reward helps the model converge better}.
The ablation ``\textit{w/o primitive-based reward}'' leads to  accuracy drop on CFQ, and the model variance has become much larger.
The key insight is: primitive-based reward guides the model to interpret polysemous lexical units more effectively, thus helping the model converge better.


\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\hline \textbf{Ratio} & \textbf{MCD-MEAN} & \textbf{MCD1} & \textbf{MCD2} & \textbf{MCD3} \\ \hline
1:1:1  & 87.47.1  & 91.52.1  & \textbf{89.42.3}  &	81.217 \\
1:0.5:0.1  & \textbf{90.91.2} &	\textbf{91.71.0} &	89.21.9 &	\textbf{91.70.6} \\
1:0.1:0.1 & 86.73.9   & 89.41.6 	 & 85.82.7 	 & 84.97.5 	\\
\hline
\end{tabular}}
\caption{Results of different learning rate ratios  of lexical Interpreter, Composer, and algebraic Interpreter.}
\label{tab:lr_setting}
\end{table}

\begin{figure}[tp]
  \centering
  \includegraphics[width=0.48\textwidth]{line_chart.pdf}
  \caption{Performance by input length.}
  \label{fig:diff_len}
\end{figure}

\noindent \textbf{Curriculum learning helps the model converge better}.
The ablation ``\emph{w/o curriculum learning}'' leads to 19\% accuracy drop on CFQ, and the model variance has become much larger.
This indicates the importance of curriculum learning.
On COGS, \textsc{LeAR} performs well without curriculum learning.
We speculate that there are two main reasons:
(1) expressions of COGS is much shorter than CFQ;
(2) the input/output pattern coverage of COGS is much higher than CFQ.


\noindent \textbf{Higher component with smaller learning rate}. 
Inspired by the differential update strategy used in \citet{liu2020spstru}(i.e., the higher level the component is positioned in the model, the slower the parameters in it should be updated), we set three different learning rates to three different components in \textsc{LeAR} (in bottom-up order): lexical Interpreter, Composer, and algebraic Interpreter.
We fix the learning rate of lexical Interpreter to 1, and adjust the ratio of the learning rates of Composer and algebraic Interpreter to lexical Interpreter.
Table \ref{tab:lr_setting} shows the results on CFQ.
The hierarchical learning rate setup () achieves the best performance.





\begin{figure}[tp]
  \centering
  
  \subfloat[][Composer error. A correct syntax tree should compose ``\emph{parent of a cinematographer}'' as a constituent, while the predicted syntax tree incorrectly composes ``\emph{a cinematographer played M0}''.\label{fig:type2}]
  {\includegraphics[width=\linewidth]{error2.pdf}}
  
  \subfloat[][Interpreter error. In this expression, the first ``\emph{influenced}'' should be assigned a semantic primitive ``\emph{influence.influence\_node.influenced}'', while Interpreter incorrectly assigns ``\emph{influence.influence\_node.influenced\_by}'' (abbreviated as ``\emph{INFLU\_BY}'' in this figure) to it.\label{fig:type1}]
  {\includegraphics[width=\linewidth]{error1.pdf}} \\
  
  \caption{Two error cases. We use solid nodes to denote predicted nonterminal nodes. Incorrect parts are colored red.}\label{fig:err_type} 
  
\end{figure}

\subsection{Closer Analysis}

We also conduct closer analysis to the results of \textsc{LeAR} as follows.

\subsubsection{Performance by Input Length}

Intuitively, understanding longer expressions requires stronger algebraic recombination ability than shorter examples.
Therefore, we expect that our model should keep a good and stable performance with the increasing of input length.



Figure \ref{fig:diff_len} shows the performance of \textsc{LeAR} and \textsc{HPD} (the state-of-the-art model on CFQ) under different input lengths.
Specifically, test instances are divided into 6 groups by length: ), and we report accuracy on each group separately.
The results indicate that \textbf{\textsc{LeAR} has stable high performance for different input lengths}, with only a slow decline as length increases.
Even on the group with the longest input length, \textsc{LeAR} can maintain an average 86.3\% accuracy across three MCD-splits.




\subsubsection{Error Analysis}

To understand the source of errors, we take a closer look at the failed test instances of \textsc{LeAR} on CFQ.
These failed test instances account for  of the test dataset.
We category them into two error types:

\noindent\textbf{Composer error (CE)}, i.e., test cases where Composer produces incorrect syntactic structures (only considering nonterminal nodes).
Figure \ref{fig:type2} shows an example.
As we do not have ground-truth syntactic structures, we determine whether a failed test instance belongs to this category based on handcraft syntactic templates.

\noindent\textbf{Interpreter error (IE)}, i.e., test cases where Composer produces correct syntactic structures but Interpreter assigns one or more incorrect semantic primitives or operations.
Figure \ref{fig:type1} shows an example, which contains an incorrect semantic primitive assignment.


Table \ref{tab:err_type} shows the distribution of these two error types.
On average, 39.19\% of failed instances are composer errors, and the remaining 60.81\% are interpreter errors.

\begin{table}
\centering
\begin{tabular}{lccc}
\hline \textbf{Error Type} & \textbf{MCD1} & \textbf{MCD2} & \textbf{MCD3} \\ \hline
CE & 45.70\% & 32.05\% & 39.83\% \\
IE & 54.30\% & 67.95\% & 60.17\% \\
\hline
\end{tabular}
\caption{Distribution of CE (Composer Error) and IE (Interpreter Error). }
\label{tab:err_type}
\end{table}





\subsection{Limitations}

Our approach is implicitly build upon the assumption of \textbf{primitive alignment}, that is, each primitive in the meaning representation can align to at least one span in the utterance.
This assumption holds in most cases of various semantic parsing tasks, including CFQ, COGS, and GEO.
However, for robustness and generalizability, we also need to consider cases that do not meet this assumption.
For example, consider this utterance ``\textit{Obama's brother}'', of which the corresponding meaning representation is ``''.
Neither ``'' nor ``'' can align to a span in the utterance, as the composed meaning of them is expressed by a single word (``\textit{brother}'').
Therefore, \textsc{LeAR} is more suitable for formalisms where primitives can better align to natural language.





In addition, while our approach is general for various semantic parsing tasks, the collection of semantic operations needs to be redesigned for each task. We  need to ensure that these semantic operations are -ary projections (as described in Section \ref{section:algebra}), and all the meaning representations are covered by the operations collection.
This is tractable, but still requires some efforts from domain experts.






\section{Related Work}







\subsection{Compositional Generalization}

Recently, exploring compositional generalization (CG) on neural networks has attracted large attention in NLP community.
For SCAN \cite{lake2018scan}, the first benchmark to test CG on seq2seq models, many solutions have been proposed, which can be classified into two tracks:
data augmentation \cite{andreas2019dataaug, aky2020dataaug, guo2020dataaugl} 
and specialized architecture \cite{nips2019dataaug, li2019compositional, Gordon2020spstru}.
However, most of these works only focus on lexical recombination.
Some works on SCAN have stepped towards algebraic recombination \cite{liu2020spstru, chen2020spstru}, but they do not generalize well to other tasks such as CFQ \cite{keysers2019measuring} and COGS \cite{kim2020cogs}.

Before our work, there is no satisfactory solution on CFQ and COGS.
Previous works on CFQ demonstrated that MLM pre-training \cite{furrer2020compositional} and iterative back-translation \cite{guo2020revisiting} can improve traditional seq2seq models.
HPD \cite{guo2020hierarchical}, the state-of-the-art solution before ours, was shown to be effective on CFQ, but still far from satisfactory.
As for COGS, there is no solution to it to the best of our knowledge.



\subsection{Compositional Semantic Parsing}

In contrast to neural semantic parsing models which are mostly constructed under a fully seq2seq paradigm, compositional semantic parsing models predict partial meaning representations and compose them to produce a full meaning representation in a bottom-up manner \cite{zelle1996learning-csp, zettlemoyer2012learning-csp, liang2013learning-csp, berant2013semantic-csp, berant2015imitation-csp, pasupat2015compositional-csp, herzig2020span-csp}.
Our model takes the advantage of compositional semantic parsing, without requiring any handcraft lexicon or syntactic rule.

\subsection{Unsupervised Parsing}





Unsupervised parsing (or grammar induction) trains syntax-dependent models to produce syntactic trees of natural language expressions without direct syntactic annotation \cite{klein2002natural-unsup,  bod2006all-unsup, ponvert2011simple-unsup, pate2016grammar, shen2018ordered-unsup, kim2019compound-unsup, drozdov2020unsupervised-unsup}.
Comparing to them, our model learns both syntax and semantics jointly.

\section{Conclusion}

In this paper, we introduce \textsc{LeAR}, a novel end-to-end neural model for compositional generalization in semantic parsing tasks.
Our contribution is 4-fold:
(1) \textsc{LeAR} focuses on algebraic recombination, thus it exhibits stronger compositional generalization ability than previous methods that focus on simpler lexical recombination.
(2) We model the semantic parsing task as a homomorphism between two partial algebras, thus encouraging algebraic recombination.
(3) We propose the model architecture of \textsc{LeAR}, which consists of a Composer (to learn latent syntax) and an Interpreter (to learn operation assignments).
(4) Experiments on two realistic and comprehensive compositional generalization benchmarks demonstrate the effectiveness of our model.

\section*{Acknowledgments} 

The work was supported by the National Key Research and Development Program of China (No. 2019YFB1704003), the National Nature Science Foundation of China (No. 71690231), Tsinghua BNRist and Beijing Key Laboratory of Industrial Bigdata System and Application.

\section*{Ethical Consideration}

The experiments in this paper are conducted on existing datasets. We describe the model architecture and training method in detail, and provide more explanations in the supplemental materials. All the data and code will be released with the paper. The resources required to reproduce the experiments is a Tesla P100 GPU, and for COGS benchmark even one CPU is sufficient. Since the compositional generalization ability explored in this paper is a fundamental problem of artificial intelligence and has not yet involved real applications, there are no social consequences or ethical issues.





\begin{thebibliography}{57}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Akyürek et~al.(2020)Akyürek, Akyürek, and
  Andreas}]{aky2020dataaug}
Ekin Akyürek, Afra~Feyza Akyürek, and Jacob Andreas. 2020.
\newblock \href {http://arxiv.org/abs/2010.03706} {Learning to recombine and
  resample data for compositional generalization}.

\bibitem[{Allwood(2003)}]{allwood2003meaning-ccv}
Jens Allwood. 2003.
\newblock Meaning potentials and context: Some consequences for the analysis of
  variation in meaning.
\newblock \emph{Cognitive approaches to lexical semantics}, pages 29--66.

\bibitem[{Andreas(2019)}]{andreas2019dataaug}
Jacob Andreas. 2019.
\newblock \href {http://arxiv.org/abs/1904.09545} {Good-enough compositional
  data augmentation}.
\newblock \emph{CoRR}, abs/1904.09545.

\bibitem[{Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio}]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}.

\bibitem[{Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston}]{bengio2009curriculum}
Yoshua Bengio, J\'{e}r\^{o}me Louradour, Ronan Collobert, and Jason Weston.
  2009.
\newblock \href {https://doi.org/10.1145/1553374.1553380} {Curriculum
  learning}.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, ICML ’09, page 41–48, New York, NY, USA. Association
  for Computing Machinery.

\bibitem[{Berant et~al.(2013)Berant, Chou, Frostig, and
  Liang}]{berant2013semantic-csp}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1533--1544.

\bibitem[{Berant and Liang(2015)}]{berant2015imitation-csp}
Jonathan Berant and Percy Liang. 2015.
\newblock Imitation learning of agenda-based semantic parsers.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  3:545--558.

\bibitem[{Bod(2006)}]{bod2006all-unsup}
Rens Bod. 2006.
\newblock An all-subtrees approach to unsupervised parsing.
\newblock In \emph{Proceedings of the 21st International Conference on
  Computational Linguistics and 44th Annual Meeting of the Association for
  Computational Linguistics}, pages 865--872.

\bibitem[{Chen et~al.(2020)Chen, Liang, Yu, Song, and Zhou}]{chen2020spstru}
Xinyun Chen, Chen Liang, Adams~Wei Yu, Dawn Song, and Denny Zhou. 2020.
\newblock \href {http://arxiv.org/abs/2008.06662} {Compositional generalization
  via neural-symbolic stack machines}.

\bibitem[{Choi et~al.(2018)Choi, Yoo, and Lee}]{choi2018learning-lt}
Jihun Choi, Kang~Min Yoo, and Sang-goo Lee. 2018.
\newblock Learning to compose task-specific tree structures.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32.

\bibitem[{Chomsky(1957)}]{chomsky1957syntactic-hicg}
Noam Chomsky. 1957.
\newblock Syntactic structures (the hague: Mouton, 1957).
\newblock \emph{Review of Verbal Behavior by BF Skinner, Language}, 35:26--58.

\bibitem[{Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser}]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser. 2018.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}.

\bibitem[{Drozdov et~al.(2020)Drozdov, Rongali, Chen, O’Gorman, Iyyer, and
  McCallum}]{drozdov2020unsupervised-unsup}
Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim O’Gorman, Mohit Iyyer, and
  Andrew McCallum. 2020.
\newblock Unsupervised parsing with s-diora: Single tree encoding for deep
  inside-outside recursive autoencoders.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 4832--4845.

\bibitem[{Finegan-Dollak et~al.(2018)Finegan-Dollak, Kummerfeld, Zhang,
  Ramanathan, Sadasivam, Zhang, and Radev}]{finegan2018geosplit}
Catherine Finegan-Dollak, Jonathan~K Kummerfeld, Li~Zhang, Karthik Ramanathan,
  Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018.
\newblock Improving text-to-sql evaluation methodology.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 351--360.

\bibitem[{Fodor and Pylyshyn(1988)}]{fodor_connectionism_1988-hicg}
J.~A. Fodor and Z.~W. Pylyshyn. 1988.
\newblock Connectionism and cognitive architecture - a critical analysis.
\newblock \emph{Cognition}, 28(1-2):3--71.

\bibitem[{Fodor and Lepore(2002)}]{fodor2002compositionality-hicg}
Jerry~A Fodor and Ernest Lepore. 2002.
\newblock \emph{The compositionality papers}.
\newblock Oxford University Press.

\bibitem[{Freivalds et~al.(2019)Freivalds, Ozoli{\c{n}}{\v{s}}, and
  {\v{S}}ostaks}]{freivalds2019neural}
K{\=a}rlis Freivalds, Em{\=\i}ls Ozoli{\c{n}}{\v{s}}, and Agris {\v{S}}ostaks.
  2019.
\newblock Neural shuffle-exchange networks--sequence processing in o (n log n)
  time.
\newblock \emph{arXiv preprint arXiv:1907.07897}.

\bibitem[{Furrer et~al.(2020)Furrer, van Zee, Scales, and
  Sch{\"a}rli}]{furrer2020compositional}
Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch{\"a}rli. 2020.
\newblock Compositional generalization in semantic parsing: Pre-training vs.
  specialized architectures.
\newblock \emph{arXiv preprint arXiv:2007.08970}.

\bibitem[{Gordon et~al.(2020)Gordon, Lopez-Paz, Baroni, and
  Bouchacourt}]{Gordon2020spstru}
Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. 2020.
\newblock Permutation equivariant models for compositional generalization in
  language.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Guo et~al.(2020{\natexlab{a}})Guo, Kim, and Rush}]{guo2020dataaugl}
Demi Guo, Yoon Kim, and Alexander~M. Rush. 2020{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2011.09039} {Sequence-level mixed sample
  data augmentation}.

\bibitem[{Guo et~al.(2020{\natexlab{b}})Guo, Lin, Lou, and
  Zhang}]{guo2020hierarchical}
Yinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei Zhang. 2020{\natexlab{b}}.
\newblock Hierarchical poset decoding for compositional generalization in
  language.
\newblock \emph{arXiv preprint arXiv:2010.07792}.

\bibitem[{Guo et~al.(2020{\natexlab{c}})Guo, Lin, Lou, and
  Zhang}]{guo2020geoiterative}
Yinuo Guo, Zeqi Lin, Jian-Guang Lou, and Dongmei Zhang. 2020{\natexlab{c}}.
\newblock Iterative utterance segmentation for neural semantic parsing.
\newblock \emph{arXiv preprint arXiv:2012.07019}.

\bibitem[{Guo et~al.(2020{\natexlab{d}})Guo, Zhu, Lin, Chen, Lou, and
  Zhang}]{guo2020revisiting}
Yinuo Guo, Hualei Zhu, Zeqi Lin, Bei Chen, Jian-Guang Lou, and Dongmei Zhang.
  2020{\natexlab{d}}.
\newblock \href {http://arxiv.org/abs/2012.04276} {Revisiting iterative
  back-translation from the perspective of compositional generalization}.

\bibitem[{Havrylov et~al.(2019)Havrylov, Kruszewski, and
  Joulin}]{havrylov2019cooperative-lt}
Serhii Havrylov, Germ{\'a}n Kruszewski, and Armand Joulin. 2019.
\newblock Cooperative learning of disjoint syntax and semantics.
\newblock \emph{arXiv preprint arXiv:1902.09393}.

\bibitem[{Herzig and Berant(2020)}]{herzig2020span-csp}
Jonathan Herzig and Jonathan Berant. 2020.
\newblock Span-based semantic parsing for compositional generalization.
\newblock \emph{arXiv preprint arXiv:2009.06040}.

\bibitem[{Hochreiter and Schmidhuber(1997)}]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber. 1997.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9(8):1735--1780.

\bibitem[{Kate et~al.(2005)Kate, Wong, and Mooney}]{kate2005geofunql}
Rohit~J Kate, Yuk~Wah Wong, and Raymond~J Mooney. 2005.
\newblock Learning to transform natural to formal languages.
\newblock In \emph{AAAI}, volume~5, pages 1062--1068.

\bibitem[{Keysers et~al.(2019)Keysers, Sch{\"a}rli, Scales, Buisman, Furrer,
  Kashubin, Momchev, Sinopalnikov, Stafiniak, Tihon
  et~al.}]{keysers2019measuring}
Daniel Keysers, Nathanael Sch{\"a}rli, Nathan Scales, Hylke Buisman, Daniel
  Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz
  Stafiniak, Tibor Tihon, et~al. 2019.
\newblock Measuring compositional generalization: A comprehensive method on
  realistic data.
\newblock \emph{arXiv preprint arXiv:1912.09713}.

\bibitem[{Kim and Linzen(2020)}]{kim2020cogs}
Najoung Kim and Tal Linzen. 2020.
\newblock Cogs: A compositional generalization challenge based on semantic
  interpretation.
\newblock \emph{arXiv preprint arXiv:2010.05465}.

\bibitem[{Kim et~al.(2019)Kim, Dyer, and Rush}]{kim2019compound-unsup}
Yoon Kim, Chris Dyer, and Alexander~M Rush. 2019.
\newblock Compound probabilistic context-free grammars for grammar induction.
\newblock \emph{arXiv preprint arXiv:1906.10225}.

\bibitem[{Klein and Manning(2002)}]{klein2002natural-unsup}
Dan Klein and Christopher~D Manning. 2002.
\newblock Natural language grammar induction using a constituent-context model.
\newblock \emph{Advances in neural information processing systems}, 1:35--42.

\bibitem[{Lake(2019)}]{nips2019dataaug}
Brenden~M Lake. 2019.
\newblock Compositional generalization through meta sequence-to-sequence
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  9791--9801. Curran Associates, Inc.

\bibitem[{Lake and Baroni(2018)}]{lake2018scan}
Brenden~M. Lake and Marco Baroni. 2018.
\newblock Generalization without systematicity: On the compositional skills of
  sequence-to-sequence recurrent networks.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages
  2879--2888. {PMLR}.

\bibitem[{Li et~al.(2019)Li, Zhao, Wang, and Hestness}]{li2019compositional}
Yuanpeng Li, Liang Zhao, Jianyu Wang, and Joel Hestness. 2019.
\newblock Compositional generalization for primitive substitutions.
\newblock \emph{arXiv preprint arXiv:1910.02612}.

\bibitem[{Liang et~al.(2013)Liang, Jordan, and Klein}]{liang2013learning-csp}
Percy Liang, Michael~I Jordan, and Dan Klein. 2013.
\newblock Learning dependency-based compositional semantics.
\newblock \emph{Computational Linguistics}, 39(2):389--446.

\bibitem[{Liu et~al.(2020)Liu, An, Lou, Chen, Lin, Gao, Zhou, Zheng, and
  Zhang}]{liu2020spstru}
Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou,
  Nanning Zheng, and Dongmei Zhang. 2020.
\newblock \href {http://arxiv.org/abs/2006.10627} {Compositional generalization
  by learning analytical expressions}.

\bibitem[{Marcus(2019)}]{marcus2019algebraic}
Gary~F Marcus. 2019.
\newblock \emph{The algebraic mind: Integrating connectionism and cognitive
  science}.
\newblock MIT press.

\bibitem[{Montague(1970)}]{montague1970universal-algre}
Richard Montague. 1970.
\newblock Universal grammar.
\newblock \emph{1974}, pages 222--46.

\bibitem[{Och and Ney(2003)}]{och2003systematic-apd}
Franz~Josef Och and Hermann Ney. 2003.
\newblock A systematic comparison of various statistical alignment models.
\newblock \emph{Computational linguistics}, 29(1):19--51.

\bibitem[{Pasupat and Liang(2015)}]{pasupat2015compositional-csp}
Panupong Pasupat and Percy Liang. 2015.
\newblock Compositional semantic parsing on semi-structured tables.
\newblock \emph{arXiv preprint arXiv:1508.00305}.

\bibitem[{Pate and Johnson(2016)}]{pate2016grammar}
John~K Pate and Mark Johnson. 2016.
\newblock Grammar induction from (lots of) words alone.
\newblock In \emph{Proceedings of COLING 2016, the 26th International
  Conference on Computational Linguistics: Technical Papers}, pages 23--32.

\bibitem[{Pelletier(2003)}]{pelletier2003context-tpoc}
Francis~Jeffry Pelletier. 2003.
\newblock Context dependence and compositionality.
\newblock \emph{Mind \& Language}, 18(2):148--161.

\bibitem[{Ponvert et~al.(2011)Ponvert, Baldridge, and
  Erk}]{ponvert2011simple-unsup}
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
\newblock Simple unsupervised grammar induction from raw text with cascaded
  finite state models.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pages
  1077--1086.

\bibitem[{Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2019.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}.

\bibitem[{Russin et~al.(2019)Russin, Jo, O'Reilly, and
  Bengio}]{russin2019spstru}
Jake Russin, Jason Jo, Randall~C. O'Reilly, and Yoshua Bengio. 2019.
\newblock \href {http://arxiv.org/abs/1904.09708} {Compositional generalization
  in a deep seq2seq model by separating syntax and semantics}.
\newblock \emph{CoRR}, abs/1904.09708.

\bibitem[{Shen et~al.(2018)Shen, Tan, Sordoni, and
  Courville}]{shen2018ordered-unsup}
Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. 2018.
\newblock Ordered neurons: Integrating tree structures into recurrent neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.09536}.

\bibitem[{So et~al.(2019)So, Le, and Liang}]{so2019evolved}
David So, Quoc Le, and Chen Liang. 2019.
\newblock The evolved transformer.
\newblock In \emph{International Conference on Machine Learning}, pages
  5877--5886. PMLR.

\bibitem[{Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour}]{policy_gradient_1999}
Richard~S Sutton, David~A. McAllester, Satinder~P. Singh, and Yishay Mansour.
  2000.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In S.~A. Solla, T.~K. Leen, and K.~M\"{u}ller, editors,
  \emph{Advances in Neural Information Processing Systems 12}, pages
  1057--1063. MIT Press.

\bibitem[{Szab{\'o}(2004)}]{szabo2004compositionality-tpoc}
Zolt{\'a}n~Gendler Szab{\'o}. 2004.
\newblock Compositionality.

\bibitem[{Tai et~al.(2015)Tai, Socher, and Manning}]{tree_lstm_2015-trls}
Kai~Sheng Tai, Richard Socher, and Christopher~D. Manning. 2015.
\newblock \href {https://doi.org/10.3115/v1/p15-1150} {Improved semantic
  representations from tree-structured long short-term memory networks}.
\newblock In \emph{Proceedings of the 53rd Annual Meeting of the Association
  for Computational Linguistics and the 7th International Joint Conference on
  Natural Language Processing of the Asian Federation of Natural Language
  Processing, {ACL} 2015, July 26-31, 2015, Beijing, China, Volume 1: Long
  Papers}, pages 1556--1566. The Association for Computer Linguistics.

\bibitem[{Tsarkov et~al.(2020)Tsarkov, Tihon, Scales, Momchev, Sinopalnikov,
  and Sch{\"a}rli}]{tsarkov2020cfq-lcg}
Dmitry Tsarkov, Tibor Tihon, Nathan Scales, Nikola Momchev, Danila
  Sinopalnikov, and Nathanael Sch{\"a}rli. 2020.
\newblock *-cfq: Analyzing the scalability of machine learning on a
  compositional task.
\newblock \emph{arXiv preprint arXiv:2012.08266}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}.

\bibitem[{Weaver and Tao(2001)}]{reinforce_baseline_2001}
Lex Weaver and Nigel Tao. 2001.
\newblock The optimal reward baseline for gradient-based reinforcement
  learning.
\newblock In \emph{{UAI} '01: Proceedings of the 17th Conference in Uncertainty
  in Artificial Intelligence, University of Washington, Seattle, Washington,
  USA, August 2-5, 2001}, pages 538--545. Morgan Kaufmann.

\bibitem[{Williams(1992)}]{reinforce_algo_1992}
Ronald~J. Williams. 1992.
\newblock \href {https://doi.org/10.1007/BF00992696} {Simple statistical
  gradient-following algorithms for connectionist reinforcement learning}.
\newblock \emph{Mach. Learn.}, 8:229--256.

\bibitem[{Zeiler(2012)}]{adadelta_2012-apd}
Matthew~D. Zeiler. 2012.
\newblock \href {http://arxiv.org/abs/1212.5701} {{ADADELTA:} an adaptive
  learning rate method}.
\newblock \emph{CoRR}, abs/1212.5701.

\bibitem[{Zelle and Mooney(1996)}]{zelle1996learning-csp}
John~M Zelle and Raymond~J Mooney. 1996.
\newblock Learning to parse database queries using inductive logic programming.
\newblock In \emph{Proceedings of the national conference on artificial
  intelligence}, pages 1050--1055.

\bibitem[{Zettlemoyer and Collins(2012)}]{zettlemoyer2012learning-csp}
Luke~S Zettlemoyer and Michael Collins. 2012.
\newblock Learning to map sentences to logical form: Structured classification
  with probabilistic categorial grammars.
\newblock \emph{arXiv preprint arXiv:1207.1420}.

\end{thebibliography}
 \bibliographystyle{acl_natbib}
\bibliography{acl2021}

\clearpage
\appendix



\begin{table*}[h]
    \renewcommand\arraystretch{1.2}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{cccc}
    \hline
    \textbf{Operation} & \textbf{Arguments} & \textbf{Result Type} & \textbf{Example} \\ \hline
    ON(, ) & \multirow{4}{*}{[: Entity, : Entity]} & \multirow{4}{*}{Entity} & Emma ate [\ub{the cake} on \ub{a table}] . \\ \cline{1-1} \cline{4-4} 
    IN(, ) &  &  & A girl was awarded [\ub{a cake} in \ub{a soup}] . \\ \cline{1-1} \cline{4-4} 
    BESIDE(, ) &  &  & Amelia dusted [\ub{the girl} beside \ub{a stage}] . \\ \cline{1-1} \cline{4-4} 
    \tabincell{c}{, , } &  &  & NONE \\ \hline
    REC-THE(, ) & \multirow{4}{*}{[: Entity, : Entity]} & \multirow{4}{*}{Entity} & Lily gave [\ub{Emma} \ub{a strawberry}] . \\ \cline{1-1} \cline{4-4} 
    THE-REC(, ) &  &  & A girl offered [\ub{a rose} to \ub{Isabella}] . \\ \cline{1-1} \cline{4-4} 
    \tabincell{c}{AGE-THE, THE-AGE, \\ REC-AGE, AGE-REC } &  &  & - \\ \hline
    FillFrame(, ) & \tabincell{c}{[: Entity, : Pred/Prop] \\ {[}: Pred/Prop, : Entity]} & Proposition & A cat [\ub{disintegrated} \ub{a girl}] . \\ \hline
    CCOMP(, ) & \multirow{3}{*}{[:  Pred/Prop, :  Pred/Prop]} & \multirow{3}{*}{Proposition} & [\ub{Emma liked} that \ub{a girl saw}] . \\ \cline{1-1} \cline{4-4} 
    XCOMP(, ) &  &  & David [\ub{expected} to \ub{cook}] . \\ \cline{1-1} \cline{4-4} 
    ,  &  &  & NONE \\ \hline
    \end{tabular}
    }
    \caption{
    Semantic operations in COGS. 
    ``Pred'' and ``Prop'' are abbreviations of ``Predicate'' and ``Proposition'', respectively.
    ``AGE'', ``THE'' and ``REC'' are abbreviations of ``AGENT'', ``THEME'' and ``RECIPIENT'', respectively.
    ``-'' omits similar examples.
    Some operations contain ``NONE'' example, indicating that no example utilize these operations in dataset.
    }
    \label{tab:COGS_operations}
\end{table*}

\begin{table*}[h]
    \renewcommand\arraystretch{1.2}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{cccc}
    \hline
    \textbf{Operation} & \textbf{Arguments} & \textbf{Result Type} & \textbf{Example} \\ \hline
    UNION(, ) & \multirow{4}{*}{\tabincell{c}{[: Entity/Prop, \\ : Entity/Prop]}} & \multirow{4}{*}{Proposition} & what is the population of [\ub{var0} \ub{var1}] \\ \cline{1-1} \cline{4-4} 
    INTER(, ) &  &  & how many [\ub{cities named var0} \ub{in the usa}] \\ \cline{1-1} \cline{4-4} 
    \tabincell{c}{EXC(, ) \\ {EXC}(, ) } &  &  & \tabincell{c}{which [\ub{capitals} are not \ub{major cities}]}
    \\ \hline
    \tabincell{c}{CONCAT(, ) \\ {CONCAT}(, )} & \multirow{1}{*}{[: Pred, : Pred]} & \multirow{1}{*}{Pred} & what is the [\ub{capital} \ub{of} var0] 
    \\ \hline
    FillIn(, ) & \tabincell{c}{[: Entity/Prop, : Pred] \\ {[}: Pred, : Entity/Prop]} & Proposition & how many [\ub{citizens} in \ub{var0}] \\ \hline
    \end{tabular}
    }
    \caption{
    Semantic operations in GEO. 
    ``Pred'' and ``Prop'' are abbreviations of ``Predicate'' and ``Proposition'', respectively.
    ``INTER'', ``EXC'' and ``CONCAT'' are abbreviations of ``INTERSECTION'', ``EXCLUDE'' and ``CONCATENATION'', respectively.
    }
    \label{tab:GEO_operations}
\end{table*}

This is the Appendix for the paper:
``Learning Algebraic Recombination for Compositional Generalization''.

\section{Semantic Operations in COGS}

The semantic primitives used in COGS benchmark are entities (e.g., \textit{Emma} and \textit{cat(x\_1)}), predicates (e.g., \textit{eat}) and propositions (e.g., \textit{eat.agent(x\_1, Emma)}).
The semantic operations in COGS are listed in Table \ref{tab:COGS_operations}.

The operations with ``'' (e.g., ON) are right-to-left operations (e.g., ON(cake, table)table.ON.cake) while the operations without ``-1'' represent the left-to-right operations (e.g., ON(cake, table)cake.ON.table).
For operation FillFrame, the entity in its arguments will be filled into predicate/proposition as an AGENT, THEME or RECIPIENT, which is decided by model.

\An{\section{Semantic Operations in GEO and Post-process}}

\An{The semantic primitives used in GEO benchmark are entities (e.g., \textit{var0}), predicates (e.g., \textit{state()}) and propositions (e.g., \textit{state(var0)}).
The semantic operations in GEO are listed in Table \ref{tab:GEO_operations}.}

\An{To fit the FunQL formalism, we design two post-processing rules for the final semantics generated by the model.
First, if the final semantic is a predicate (not a proposition), it will be converted in to a proposition by filling the entity \textit{all}. 
Second, the predicate \textit{most} will be shifted forward two positions in the final semantics.}





\section{Policy Gradient and Differential Update}

In this section, we will show more details about the formulation of our RL training based on policy gradient and how to use differential update strategy on it.

Denoting  as the trajectory of our model where  and  are actions (or called results) produced from Composer and Interpreter, respectively, and  as the reward of a trajectory  (elaborated in Sec.~\ref{sec:reward}), 
the training objective of our model is to maximize the expectation of rewards as:

where  is the policy of the whole model  and  are the parameters in Composer and Interpreter, respectively.
Applying the likelihood ratio trick,  and  can be optimized by ascending the following gradient:

which is same with Eq.~\ref{eq:gradient}.

As described in Sec.~\ref{sec:Model} that the interpreting process can be divided into two stages: interpreting lexical nodes and interpreting algebraic nodes, the action  can also be split as the semantic primitives of lexical nodes  and the semantic operations of algebraic nodes .
In our implement, we utilize two independent neural modules for interpreting lexical nodes and interpreting algebraic nodes, with parameters  and  respectively.
Therefore,  in Eq.~\ref{eq:gradient} can be expanded via the chain rule as:


With Eq.~\ref{eq:gradient_split}, we can set different learning rates:

Furthermore, in our experiments, the AdaDelta optimizer \cite{adadelta_2012-apd} is employed to optimize our model.

\section{Phrase Table}

\begin{table*}[t]
    \renewcommand\arraystretch{1.2}
    \centering
    \scalebox{0.85}{
    \begin{tabular}{ccc}
    \hline
    \textbf{Lexical Unit} & \textbf{Semantic Primitive(s)} & \textbf{Type} \\ \hline
    M0 & M0 & Entity \\ \hline
    \multirow{2}{*}{executive producer} & film.film.executive\_produced\_by & Predicate \\
     & film.producer.films\_executive\_produced & Predicate \\ \hline
    \multirow{3}{*}{editor} & a film.editor & Attribute \\
     & film.editor.film & Predicate \\
     & film.film.edited\_by & Predicate \\ \hline
    Italian & people.person.nationality m\_03rjj & Attribute \\ \hline
    \end{tabular}
    }
    \caption{Some examples in CFQ phrase table.}
    \label{tab:phrase table}
\end{table*}

The phrase table consists of lexical units (i.e., words and phrases) paired with semantic primitives that frequently co-occur with them.
It can be obtained with statistical methods.

For CFQ, we leverage GIZA++\footnote{https://github.com/moses-smt/giza-pp.git} \cite{och2003systematic-apd} toolkit to extract alignment pairs from training examples.
We obtain 109 lexical units, each of which is paired with 1.7 candidate semantic primitives on average.
Some examples in phrase table are shown in Table \ref{tab:phrase table}

As to COGS, for each possible lexical unit, we first filter out the semantic primitives that exactly co-occur with it, and delete lexical units with no semantic primitive.
Among the remaining lexical units, for those only contain one semantic primitive, we record their co-occurring semantic primitives as ready semantic primitives.
For lexical units with more than one semantic primitives, we delete the ready semantic primitives from their co-occurring semantic primitives.
Finally, we obtain 731 lexical units and each lexical unit is paired with just one semantic primitive.

\An{As GEO is quite small, we obtain its phrase table by handcraft.}



\begin{figure*}
  \centering
  
  \subfloat[][An example of generated results in CFQ benchmark with the input ``Did M6‘ s star, costume designer, and director influence M0, M1, M2, and M3 and influence M4 and M5 ''.\label{fig:cfq_tree}]
  {\includegraphics[width=1.\textwidth]{cfq_tree.pdf}} \\
  
  \subfloat[][An example of generated results in COGS benchmark with the input ``Joshua liked that Mason hoped that Amelia awarded the hedgehog beside the stage in the tent to a cat''.\label{fig:cogs_tree}]
  {\includegraphics[width=1.\textwidth]{cogs_tree.pdf}}
  
  \caption{Examples of generated tree-structures and semantics in CFQ and COGS benchmarks.
  }\label{fig:deep_trees}
  
\end{figure*}

\section{More Examples}

We show more examples of generated tree-structures and semantics in Figure~\ref{fig:deep_trees}.

\end{document}
