For our primary experiments we considered two python datasets namely Django and CoNaLa. The former is based on Django web framework and the latter is annotated code snippets from stackoverflow answers. Additionally, we experiment on the SQL version of GeoQuery and ATIS from \citet{finegan2018improving} (with query split), WikiSQL \cite{zhong2017seq2sql}, and Magic (Java) \cite{ling2016latent}.


\textbf{Python Monolingual Corpora:} CoNaLa comes with  mined questions from Stackoverflow. We ignored the noisy source intents/sentences and just use the python snippets. To be comparable with \citet{xu2020incorporating}, we also select a corresponding  subset version for comparison. See Appendix \ref{sec:appen_data} for details on the SQL and Java monolingual corpora. 


\textbf{Experimental Setup:} In all experiments, we use label smoothing with a parameter of  and Polyak averaging \cite{polyak1992acceleration} of parameters with a momentum of  except for GeoQuery which we use . We use Adam \cite{kingma2014adam} and early stopping based on the dataset specific evaluation metric on dev set. The learning rate for the encoder is  over all datasets. We used the learning rate of  on all datasets except GeoQuery and ATIS which we use . The architecture overview is shows in Fig.\ \ref{fig:architecture}. 
At the inference time we use beam search with beam size of  and a length normalization based on \cite{Wu2016GooglesNM}. We run each experiment with  different random seeds and report the average and standard deviation. WordPiece tokenization is used for both natural language utterances and programming code.










\vspace{-0.1em}
\subsection{Empirical Analysis}
\vspace{-0.1em}
\label{sec:ablation}



\begin{table*}[t!]
 \begin{minipage}{.5\textwidth}
\centering 
\begin{tabular}{|p{.15\linewidth}|p{.74\linewidth}|}
\hline
{\tiny \textbf{Source:}} & {\tiny \textbf{
\makecell{call the function lazy with 2 arguments : \_string\_concat and \\ six.text\_type , substitute the result for string\_concat .
}}} \\
\hline
{\tiny \textbf{Gold \& TAE:}} &  
\begin{lstlisting}[boxpos=b]
string_concat = lazy(_string_concat, six.text_type)
\end{lstlisting} \\ \hline
{\tiny \textbf{Baseline:}} & 
\begin{lstlisting}[boxpos=b]
string_concat = lazy (<@\textcolor{red}{_concat_}@>concat , six.text_type )
\end{lstlisting}
\\\hline
{\tiny \textbf{Note:}} & {\tiny \textbf{copy mistake: wrong variable resulting from failed copy} }\\
 \hline
\end{tabular}
\end{minipage}\hfill
 \begin{minipage}{.5\textwidth}
\centering 
\begin{tabular}{|p{.15\linewidth}|p{.76\linewidth}|}
\hline
{\tiny \textbf{Source:}} & {\tiny \textbf{
\makecell{
define the function timesince with d , now defaulting to none,\\  reversed defaulting to false as arguments .
}}} \\
\hline
{\tiny \textbf{Gold \& TAE:}} &  
\begin{lstlisting}[boxpos=b]
def timesince(d, now=none, reversed=false):
    pass
\end{lstlisting} \\ \hline
{\tiny \textbf{Baseline:}} & 
\begin{lstlisting}[boxpos=b]
def timesince <@\textcolor{red}{(d = none, reversed ( d = false )}@>:
    pass 
\end{lstlisting}
\\ \hline
{\tiny \textbf{Note:}} & {\tiny \textbf{unbalanced paranthesis and multiple semantic mistakes.}}\\
\hline
\end{tabular}
\end{minipage}
 \caption{\label{table:mistake_examples_short} Example mistakes by the baseline that are fixed by TAE. More examples in Appendix \ref{ap_sec:more_examples_appendix}.}
\end{table*}
 First, we considered a scenario where the monolingual corpus comes from the same distribution as the bitext. We simulate this setup by using  of Django training data as labeled data while using all the python examples from Django as the monolingual dataset of  times bigger. Results with ``Authentic Dataset'' in Fig.\ \ref{fig:semi-supervised-django} shows the effectiveness of TAE vs other approaches.

Next, we used the monolingual dataset prepared for python (StackOverflow Corpus) which is from a different distribution. Fig.\ \ref{fig:semi-supervised-django}  shows even more considerable improvement, thanks to the larger monolingual set. We considered noisy intents provided in CoNaLa monolingual corpus and dummy source sentences where each monolingual sample is paired along with a random length array containing zeros. We also compared against other well-known approaches like fusion and back-translation, see experiments details in Appendix \ref{ap_sec:fusion}. TAE outperforms all those approaches by a large margin. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_django_full.png}
    \vspace{-1em}
    \caption{Analysis using only 10\% Django train bitext.}
    \label{fig:semi-supervised-django}
\vspace{-.5em}
\end{figure}










Now one important question is, what part of the model benefits from monolingual data most? In Sec.\ \ref{sec:main_method}, we conjectured that auto-encoding of monolingual data should mostly help the decoder, not the encoder. To verify this, we perform an ablation by comparing freezing encoder parameters versus not freezing over the monolingual set. Fig.\ \ref{fig:semi-supervised-django} shows that without freezing the encoder, performance drops slightly for TAE on authentic Django while dropping significantly when copying on Stackoverflow data. This confirms that the performance gain is due to its effect on the decoder, while the copied monolingual data might even hurts the encoder.



 




\vspace{-0.1em}
\subsection{Main Results on Full Data}
\vspace{-0.1em}

Table \ref{table:django}-\ref{table:conala} showcase our SOTA results on Django and CoNaLa. While our simple base seq2seq model does not outperform previous works, with TAE on the monolingual data, our performance improves and outperforms all the previous works. 


The most direct comparison is with \citet{xu2020incorporating} that also leverage the same extra data mined from StackOverflow ({\em EK} in Table \ref{table:conala}). As mentioned in Sec.\ \ref{sec:related}, they used the noisy parallel corpus for pre-training, whereas we only leverage the monolingual set. However, we obtain both larger relative improvements over our baseline ( from ) compared to \citet{xu2020incorporating} ( from ), as well as better absolute results in the best case. In fact, with only the  StackOverflow monolingual data, our result is on par with the best one from \citet{xu2020incorporating} that uses the additional python API bitext data. Finally, note that part of our superior performance is due to using BERT as an encoder.



\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\hline \textbf{Model} & \textbf{Django} \\ \hline
\small YN17 \scriptsize \cite{yin2017syntactic} &  \\
\small TRANX \scriptsize \cite{yin2018tranx} &  \\
\small Coarse2Fine \scriptsize \cite{dong2018coarse} & \\
\small TRANX2 \scriptsize \cite{yin2019reranking} &  \\
\small TRANX2 + BERT &  \\
\small Reranker \scriptsize \cite{yin2019reranking} &  \\
\hline
\small Our baseline &  \\
\small Our baseline + TAE &  {\boldmath } \\
\hline
\end{tabular}
\vspace{-0.5em}
\caption{\label{table:django} Exact match accuracy for Django test set. \citet{yin2019reranking} trained a separate model on top of SP to rank beam search outputs.}
    \vspace{1em}
\centering
\small
\begin{tabular}{lll}
\hline \textbf{Model} & \textbf{CoNaLa} \\ \hline
\small Reranker \scriptsize \cite{yin2019reranking} &  \\
\small TRANX {\scriptsize \cite{yin2019reranking}} + BERT &  \\
\small EK (baseline) \scriptsize \cite{xu2020incorporating} &  \\
\small EK + 100k \scriptsize \cite{xu2020incorporating} &  \\
\small EK + 100k + API \scriptsize \cite{xu2020incorporating} &  \\
\hline
\small Our baseline &  \\
\small Our baseline + TAE on 100k &   \\
\small Our baseline + TAE on 600k &  {\boldmath } \\
\hline
\end{tabular}
\vspace{-0.5em}
\caption{\label{table:conala} CoNaLa test BLEU. Methods with  trained a separate model on top of SP to rerank beam search outputs. \citet{xu2020incorporating} used an additional bitext corpus mined from python API documentation.}
    \vspace{1em}
   \centering
       \small
    \begin{tabular}{ccc}
\hline \textbf{Dataset} & \textbf{Baseline (\%)} & \textbf{Baseline + TAE (\%)} \\ \hline
GeoQuery &  &  \\
ATIS &  &  \\
Magic  &  &   \\
WikiSQL  &   & \\
\hline
\end{tabular}
\vspace{-0.5em}
\caption{\label{table:extra_dataset_result} Additional dataset results: test set exact match accuracy on all dataset.}
\vspace{-0.5em}
\end{table}

Finally, TAE also yields improvements on other programming languages, as shown for GeoQuery (SQL), ATIS (SQL) and Magic (Java) in Table \ref{table:extra_dataset_result}. We observe no improvement on WikiSQL. But it is not surprising given its large dataset size and the simplicity of its targets. As observed by previous works \cite{finegan2018improving}, more than half of queries follow simple pattern of ``{\fontfamily{qcr}\selectfont SELECT col FROM table WHERE col = value}".

The main results in terms of improvement over previous best methods are statistically significant in Table \ref{table:django}-\ref{table:conala}. On Django, our result is better than Reranker \cite{yin2019reranking} (best previous method in Table \ref{table:django}) with a P-value , under one-tailed two-sample t-test for mean equality. Since the previous state of the art on CoNaLa (EK + 100k + API in Table \ref{table:conala}) did not provide the standard deviation, we cannot conduct a two-sample t-test against it. Instead, we performed a one-tailed two-sample t-test against the TranX+BERT baseline and observed that our improvement is statistically significant with P-value . In Table \ref{table:extra_dataset_result}, improvements on GeoQuery and ATIS are statistically significant with P-value , while it is not the case for Magic and WikiSQL.


\subsection{Discussion}
Thus far, we have verified that the decoder benefits from TAE and the encoder does not. For a better understanding of what TAE improves in the decoder, we propose two metrics namely \textit{copy accuracy} and \textit{generation accuracy}. Copy accuracy only considers tokens appearing in the source sentence. If the model produces all of the tokens that need to be copied from the source sentence, and in the right order, then the score is one otherwise zero for the example. Generation-accuracy ignores tokens appearing in the source intent and computes the exact match accuracy of the prediction. We show how to compute these metrics for the following example:
\\
\textbf{Question:} define the function timesince with d, now defaulting to none, reversed defaulting to false as arguments.
\\
\textbf{Ground Truth:}\\
``{\fontfamily{qcr}\selectfont
def timesince(d, now=none, reversed=false): pass}''

We iterate over the ground truth script tokens one by one and remove those that can be copied from the source, leading to this code:\\
\textbf{Generation Ground Truth:} \\
``{\fontfamily{qcr}\selectfont def (=none=):pass}'',
and the removed tokens will be considered for copy ground truth.\\
\textbf{Copy Ground Truth:} ``{\fontfamily{qcr}\selectfont timesince d , now , reversed false}''.

We would then use the copy and generation ground truth strings to compute each metric. Note that the order of tokens are still important and exact equality is required.

As shown in Table \ref{table:accuracies} both metrics are improved. Table \ref{table:mistake_examples_short} illustrates one example from each type and with more samples in the Appendix \ref{ap_sec:more_examples_appendix}. Copy accuracy is important for producing the right variable names mentioned, and it is improved as expected. It is also encouraging to see quantitatively and qualitatively that grammar mistakes are reduced, meaning that the lack of prior knowledge of target language structure is compensated by learning from monolingual data.
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{l|c|c}
        \hline
         Model & Copy & Generation \\
         \hline
         10\% basline & 34.18 & 55.73 \\
         10\% baseline + TAE & 58.89 & 66.31 \\
         Full baseline & 80.11 & 81.27 \\
         Full baseline + TAE & 84.59 & 82.65\\
         \hline
    \end{tabular}
\vspace{-0.5em}
    \caption{\label{table:accuracies} {\small Copy and generation accuracies on Django test set} }
\vspace{-1em}
\end{table}














