\documentclass{article}
\pdfoutput=1
\usepackage{amsthm}
\usepackage{graphicx}

\author{
Jean-Guillaume Dumas\footnote{
Universit\'e de Grenoble. 
Laboratoire LJK,
umr CNRS, INRIA, UJF, UPMF, GINP.
51, av. des Math\'ematiques, F38041 Grenoble, France.
}~\footnotemark[3], 
Cl\'ement Pernet\footnote{
Universit\'e de Grenoble.
Laboratoire LIG,
umr CNRS, INRIA, UJF, UPMF, GINP.
51, av. J. Kuntzmann, F38330 Montbonnot St-Martin, France.
}~\footnotemark[3]
and Ziad Sultan\footnotemark[1]~\footnotemark[2]
\footnote{
\href{mailto:Jean-Guillaume.Dumas@imag.fr}{Jean-Guillaume.Dumas@imag.fr},
\href{mailto:Clement.Pernet@imag.fr}{Clement.Pernet@imag.fr},
\href{mailto:Ziad.Sultan@imag.fr}{Ziad.Sultan@imag.fr}.
}
}

\newcommand{\strechparskip}[1]{}
\newcommand{\strechparsep}[1]{}
\newcommand{\customvspace}[1]{}

\newcommand{\breakalgorithm}{}
\newcommand{\breakalgorithmsinglecolumn}{\State\Comment{the trailing
    parts of the algorithm are shown on next pages}\breakalgorithm{}}

\newfont{\seaddfnt}{phvr8t at 8pt}
\def\semail#1{{{\seaddfnt{\par #1}}}}
\usepackage{mdwlist}
\usepackage[utf8]{inputenc}
\usepackage{xspace}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmicreturn{\textbf{Return}}
\algrenewcommand\Return{\State\algorithmicreturn{} }

\usepackage{multirow}
\newcommand{\pluq}{\texttt{PLUQ}\xspace}
\newcommand{\ple}{\texttt{PLE}\xspace}
\newcommand{\cupd}{\texttt{CUP}\xspace}
\newcommand{\lqup}{\texttt{LQUP}\xspace}
\newcommand{\trsm}{\texttt{TRSM}\xspace}
\newcommand{\MM}{\texttt{MM}\xspace}
\newcommand{\mm}{\frac{m}{2}}
\newcommand{\nn}{\frac{n}{2}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}\xspace}
\newcommand{\GO}[1]{\ensuremath{O\left(#1\right)}\xspace}
\newcommand{\po}[1]{\ensuremath{o\left(#1\right)}\xspace}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Defintion}
\newtheorem{corrolary}{Corrolary}
\newtheorem{remark}{Remark}

\renewcommand{\breakalgorithm}{\algstore{bkbreak}\end{algorithmic}\end{algorithm}
\begin{algorithm}[htbp]
\begin{algorithmic}\algrestore{bkbreak}}


\title{Simultaneous computation of the row and column rank profiles}

\makeatletter
\usepackage{color,svgcolor}
\usepackage{hyperref}
\hypersetup{
pdftitle={\@title},
pdfauthor={Jean-Guillaume Dumas, Cl√©ment Pernet and Ziad Sultan},
breaklinks=true, colorlinks=true,
 linkcolor=darkred,
 citecolor=blue,
 urlcolor=darkgreen,
}
\makeatother

\begin{document}
\maketitle

\abstract{
Gaussian elimination with full pivoting generates a PLUQ matrix decomposition. 
Depending on the strategy used in the search for pivots, the
permutation matrices can reveal some information about the row or the column
rank profiles of the matrix. We propose a new pivoting strategy that makes it
possible to recover at the same time both row and column rank profiles of the input matrix and of any  of its leading sub-matrices.
We propose a rank-sensitive and quad-recursive algorithm that computes
the latter PLUQ triangular decomposition of an  matrix of rank  in
\GO{mnr^{\omega-2}} field operations, with  the exponent of matrix
multiplication. 
Compared to the LEU decomposition by Malashonock, sharing a similar recursive
structure, its time complexity is rank sensitive and has a lower leading
constant.
Over a word size finite field, this algorithm also improveLs the practical
efficiency of previously known implementations.
}
\section{Introduction}
Triangular matrix decomposition is a fundamental building block in computational
linear algebra. It is used to solve linear systems, compute the rank,
the determinant, the nullspace or the row and column rank profiles of a matrix.
The LU decomposition, defined for matrices whose leading principal
minors are all nonsingular, can be generalized to arbitrary dimensions and ranks
by introducing pivoting on sides, leading e.g. to the LQUP decomposition
of~\cite{Ibarra:1982:LSP} or the PLUQ decomposition~\cite{GoVa96,Jeffrey:2010:lufact}.
Many algorithmic variants exist allowing fraction free computations
\cite{Jeffrey:2010:lufact}, in-place computations~\cite{jgd:2008:toms,JPS:2011}
or sub-cubic rank-sensitive time complexity~\cite{Storjohann:2000:thesis,JPS:2011}. 
More precisely, the pivoting strategy reflected by the permutation matrices
 and  is the key difference between these PLUQ decompositions. 
In numerical linear algebra~\cite{GoVa96}, pivoting
is used to ensure a good numerical stability, good data locality, and reduce
the fill-in. In the context of exact linear algebra, the role of pivoting
differs. 
Indeed, only certain pivoting strategies for these decompositions will reveal
the rank profile of the matrix. The latter  is crucial
in many applications using exact Gaussian elimination, such as Gr\"obner basis
computations~\cite{F99a} and computational number
theory~\cite{stein2007modular}. 

The {\em row rank profile} of an  matrix with rank  is a
lexicographically smallest sequence of  row indices such that the
corresponding rows of the matrix are linearly independent. 
Similarly the {\em column rank profile} is a lexicographically smallest sequence
of  column indices such that the corresponding rows of the matrix are
linearly independent.

The common strategy to compute the row rank profile is to search for pivots in
a row-major fashion: exploring the current row, then moving to the next row only
if the current row is zero. Such a PLUQ decomposition can be transformed into a
CUP decomposition (where  is in column echelon form) and the first
 values of the permutation associated to  are exactly the row rank
profile~\cite{JPS:2011}. A block recursive algorithm can be derived from this scheme by
splitting the row dimension~\cite{Ibarra:1982:LSP}.
Similarly, the column rank profile can be obtained in a column major search:
exploring the current column, and moving to the next column only if the
current one is zero. The PLUQ decomposition can be transformed into a PLE
decomposition (where  is in row echelon form) and the first  values of
 are exactly the column rank profile~\cite{JPS:2011}. The corresponding block
recursive algorithm uses a splitting of the column dimension.


Recursive elimination algorithms splitting both row and column dimensions
include the TURBO algorithm~\cite{jgd:2002:PComp} and the LEU
decomposition~\cite{Malaschonok:2010}. 
No connection is made to the computation of the rank profiles in any of them.
The TURBO algorithm does not compute the lower triangular matrix  and
performs five recursive calls. It therefore implies an arithmetic overhead
compared to classic Gaussian elimination.
The LEU decomposition aims at reducing the amount of permutations and therefore also
uses many additional matrix products. As a consequence its time complexity
is not rank-sensitive.



We propose here a pivoting strategy following a Z-curve structure and
working on an incrementally growing leading sub-matrix. 
This strategy is first used in a recursive algorithm splitting both rows
and columns which recovers simultaneously both row and column rank profiles.
Moreover, the row and column rank profiles of any
leading sub-matrix can be deduced from the  and  permutations.
We show that the arithmetic cost of this algorithm remains rank sensitive of the
form  where  is the exponent of matrix
multiplication. The best currently known upper bound for  is
~\cite{Williams:2012:matmul}.
As for the CUP and PLE decompositions, this PLUQ decomposition can be computed
in-place. We also propose an iterative variant, to be used as a base-case.  
 


Compared to the CUP and PLE decompositions, this new algorithm has the following
new salient features: 
\customvspace{-1em}
 \begin{itemize}
\strechparskip{-3pt}
\strechparsep{-6pt}
 \item it computes {\em simultaneously} both rank profiles at the cost of one,
 \item it preserves the squareness of the matrix passed to the recursive calls,
  thus allowing more efficient use of the matrix multiplication building block,
\item it reduces the number of modular reductions in a finite field,
\item a CUP and a PLE decompositions can be obtained from it, with row and
  column permutations only.
 \end{itemize} 
\customvspace{-1em}
Compared to the LEU decomposition, 
\customvspace{-1em}
\begin{itemize}
\strechparskip{-3pt}
\strechparsep{-6pt}
\item it is in-place,
\item its time complexity bound is rank sensitive and has a better leading constant,
\item a LEU decomposition can be obtained from it, with row and column
  permutations.
\end{itemize}
\customvspace{-1em}


In Section~\ref{sec:rec} we present the new block recursive algorithm. 
Section~\ref{sec:leu} shows the connection with the LEU
decomposition and section~\ref{sec:rp} states the main property about
rank profiles.
We then analyze the complexity of the new algorithm in terms
of arithmetic operations: first we prove that it is rank
sensitive in Section~\ref{sec:comp} and second we show in
section~\ref{sec:mod} that, over a finite field, it reduces the number of
modular reductions when compared to state of the art techniques.
We then propose an iterative variant in Section~\ref{sec:base} to be
used as a base-case to terminate the recursion before the dimensions get too
small. Experiments comparing computation time and cache efficiency are presented
in section~\ref{sec:exp}.


\section{A recursive PLUQ algorithm}\label{sec:rec}

\setlength{\arraycolsep}{.8\arraycolsep}
We first  recall the name of the main sub-routines being used: \MM
stands for matrix multiplication, \trsm for triangular system solving
with matrix unknown (left and right variants are implicitly indicated
by the parameter list), \texttt{PermC} for matrix column permutation,
\texttt{PermR} for matrix row permutation, etc. For instance, we will use:
\customvspace{-12pt}
\begin{description}
\strechparskip{0pt}
\strechparsep{-1pt}
\item[] to denote ,
\item[] for  with  upper triangular,
\item[] for  with  lower triangular.
\end{description}
\customvspace{-12pt}
We  also denote by  the transposition of indices  and  and by 
, the storage of the two triangular matrices  and  one
above the other. 
Further details on these subroutines and notations can be found in~\cite{JPS:2011}.
In block decompositions, we  allow for zero dimensions. By convention,
the product of any  matrix by an  matrix is  the
 zero matrix.

We now present the block recursive algorithm~\ref{alg:pluq:4rec},
computing a PLUQ decomposition. 

{\scriptsize
\begin{algorithm}[H]
  \caption{\pluq}
  \label{alg:pluq:4rec}
\begin{algorithmic}
\Require{ a  matrix over a field}
\Ensure{:   and  permutation matrices}
\Ensure{: the rank of }
\Ensure{ where 
 is  unit lower triangular, 
 is  upper triangular,
and 
}
\If{m=1}
   \If{}
      
   \Else
      \State  the col. index of the first non zero elt. of 
      \State 
      \State Swap  and 
   \EndIf
   \Return 
\EndIf
\If{n=1}
   \If{}
       
   \Else
      \State  the row index of the first non zero elt. of 
      \State 
      \State Swap  and 
      \For{}
         
      \EndFor
   \EndIf
   \Return 
\EndIf

\breakalgorithmsinglecolumn{}

\State \Comment{Splitting  where  is .}
  \State Decompose  \Comment{}
  \State  \Comment{}
  \State  \Comment{}
  \State Here .
   \State  \Comment{}
   \State  \Comment{}
   \State  \Comment{}
   \State  \Comment{}
   \State  \Comment{}
   \State Here .
   
   \State Decompose  \Comment{}
   \State Decompose  \Comment{}
   \State  \Comment{}
   \State  \Comment{}
   \State  \Comment{}
   \State  \Comment{}
   \State  \Comment{}

\breakalgorithm{}

\State Here .
   \State  \Comment{}
   \State  \Comment{}
    \State  \Comment{}
    \State  \Comment{}
    \State     \Comment{}
    \State  \Comment{}
    \State  Decompose  \Comment{}
    \State  \Comment{}
    \State  \Comment{}
            

    \State Here .

\State 
\State 

 \State 
\State 



\State  \Comment{}
\State Here 
\Return 
\end{algorithmic}
\end{algorithm}
}


It is based on a splitting of the matrix in four quadrants. A first recursive
call is done on the upper left quadrant followed by a series of updates. Then
two recursive calls can be made on the anti-diagonal quadrants if the first
quadrant exposed some rank deficiency.
After a last series of updates, a fourth recursive call is done on the bottom
right quadrant.
Figure~\ref{fig:pluq:rec} illustrates the position of the blocks computed in the
course of algorithm~\ref{alg:pluq:4rec}, before and after the final permutation
with matrices ~and~.
\begin{figure}[ht]\centering
\includegraphics[width=\linewidth]{pluq-full-rank}
\caption{Block recursive Z-curve PLUQ decomposition and final block permutation.}
\label{fig:pluq:rec}
\customvspace{-3pt}
\end{figure}

This framework differs from the one in~\cite{jgd:2002:PComp} by the order in
which the quadrants are treated, leading to only four recursive calls in this
case instead of five in~\cite{jgd:2002:PComp}. We will show in
section~\ref{sec:rp} that this fact together with the special form of the block
permutations  and  makes it possible to recover rank profile information. 
The correctness of algorithm~\ref{alg:pluq:4rec} is proven in appendix~\ref{app:pluq:correct}.
\begin{remark}
Algorithm~\ref{alg:pluq:4rec} is in-place (as defined
in~\cite[Definition 1]{JPS:2011}):
all operations of the \trsm, \MM,
\texttt{PermC}, \texttt{PermR} subroutines work with  extra memory
allocations except possibly in the course of fast matrix multiplications.
The only constraint is for the computation of  which
would overwrite the matrix  that should be kept  for the final output. 
Hence a copy of  has to  be stored for the computation of . The matrix 
has dimension  and can be stored transposed in the zero block of
the upper left quadrant (of dimension , as
shown on Figure~\ref{fig:pluq:rec}).
\end{remark}
 \section{From PLUQ to LEU}\label{sec:leu}

\renewcommand{\arraystretch}{0.91}
\setlength{\arraycolsep}{.12\arraycolsep}

We now show how to compute the LEU decomposition of~\cite{Malaschonok:2010}
from the PLUQ decomposition. The idea is to write 

 and show that  and  are respectively lower and
 upper triangular. This is not true in general, but turns out to be
 satisfied by the  and  obtained in
 algorithm~\ref{alg:pluq:4rec}.\setlength{\arraycolsep}{.13\arraycolsep}
\begin{theorem}
Let  be the \pluq decomposition computed by algorithm~\ref{alg:pluq:4rec}.
Then for any unit lower triangular matrix  and any upper triangular matrix ,
the matrix
 is  unit lower triangular and

is upper triangular.
\end{theorem}

\begin{proof}
Proceeding by induction, we assume that the theorem is true on all four
recursive calls, and show that it is true for the matrices 

and 
.
Let 
where  is unit lower triangular of dimension .
From~the correctness of algorithm~\ref{alg:pluq:4rec} (see e.g. Equation~\ref{eq:PL}),



Hence  equals

By induction hypothesis, the matrices


,

and
 are unit lower triangular. Therefore the matrix   is also unit lower triangular.

Similarly, let 
where  is upper triangular of dimension .
The matrix  equals

Hence  equals
  
By induction hypothesis, the matrices
,

, 
and
 are upper triangular.
Consequently the matrix   is upper triangular.

For the base case with . The matrix  has dimension  and is unit lower triangular. If , then  is
upper triangular. If , then  where  is the column index of
the pivot and is therefore the column index of the  leading coefficient of the row
. Applying  on the left only swaps rows 1
  and , hence row  is the th row of
    . The latter is
therefore upper triangular. The same reasoning can be applied to the case .
\end{proof}
\begin{corrolary}\label{cor:leu}
  Let  and .
Then  is a LEU decomposition of .
\end{corrolary}




\begin{remark}
The converse is not always possible: given , there are
several ways to choose the last  columns of  and the last  rows of
. The LEU algorithm does not keep track of these parts of
the permutations.
\end{remark}


 \section{Computing the rank profiles}
\label{sec:rp}

We prove here the main feature of the PLUQ decomposition computed by
algorithm~\ref{alg:pluq:4rec}: it reveals the row and column rank profiles of all leading
sub-matrices of the input matrix.
We recall in Lemma~\ref{lem:rankprofiletransf} basic properties verified by the
rank profiles.
\begin{lemma} For any matrix,
\label{lem:rankprofiletransf}
\begin{enumerate}\customvspace{-5pt}\strechparsep{-1pt}\strechparskip{0pt}
\item\label{point:rrp} the row rank profile is preserved by right multiplication
  with an invertible matrix and by left multiplication with an invertible upper
  triangular matrix.
\item\label{point:crp} the column rank profile is preserved by left multiplication with an
  invertible matrix and by right multiplication with an invertible lower
  triangular matrix.
\customvspace{-5pt}\end{enumerate}
\end{lemma}






\begin{lemma}
  Let  be the \pluq decomposition computed by algorithm~\ref{alg:pluq:4rec}.
  Then the row (resp. column) rank profile of any leading  submatrix of
   is the row (resp. column) rank profile of the leading  submatrix
  of .
\end{lemma}
\begin{proof}With the notations of corollary~\ref{cor:leu}, we have:
  
Hence 
where  is the  leading submatrix of 
(hence it is an invertible lower triangular matrix) and  is the
   leading submatrix of  (hence it is an invertible
  upper triangular matrix). 
Now, Lemma~\ref{lem:rankprofiletransf} implies  that the rank profile of 

is that of 
.
\end{proof}



From this lemma we deduce how to compute the row and column rank profiles of any
 leading submatrix and more particularly of the matrix  itself.

\begin{corrolary}
  Let  be the \pluq decomposition of a  matrix computed by  algorithm~\ref{alg:pluq:4rec}.
  The row (resp. column) rank profile of any -leading submatrix of a
    is the sorted sequence of the row
  (resp. column) indices of the non zero rows (resp. columns) in the matrix 

\end{corrolary}

\begin{corrolary}
  The row (resp. column) rank profile of  is the sorted sequence of row
  (resp. column) indices of the non zero rows (resp. columns) of the first 
  columns of  (resp. first  rows of ).
\end{corrolary}
 \section{Complexity analysis}\label{sec:comp}

We study here the time complexity of algorihtm~\ref{alg:pluq:4rec} by counting
the number of field operations.
For the sake of simplicity, we will assume here that the dimensions  and 
are powers of two. The analysis can easily be extended to the general case for
arbitrary  and .

For  we denote by  the cost of the -th recursive call to
\pluq, on a  matrix of rank .
We also denote by  the cost of a call \trsm on a rectangular
matrix of dimensions , and by  the cost of multiplying
an  by an  matrix.

\begin{theorem}
  Algorithm~\ref{alg:pluq:4rec}, run on an  matrix of rank
  , performs  field operations.
\end{theorem}

\begin{proof}
Let  be the cost of algorithm~\ref{alg:pluq:4rec} run on a
 matrix of rank . From the complexities of the subroutines given, e.g., in~\cite{jgd:2008:toms} and the recursive calls in algorithm~\ref{alg:pluq:4rec}, we have:


for some constants  and  (we recall that  for ).

Let .Then we can prove by a simultaneous induction on  and  that 
.

Indeed, if  or  then .
Now if it is true for , then for , we have
 
\end{proof}

In order to compare this algorithm with usual Gaussian elimination algorithms,
we now refine the analysis to compare the leading constant of the time
complexity in the special case where the matrix is square and has a generic rank
profile:  and  at each recursive step. 

Hence we have


Writing , the constant  satisfies:   

which is equal to the constant of the CUP and LUP decompositions~\cite[Table
  1]{JPS:2011}. In particular, 
it equals  when , matching the constant of the classical
Gaussian elimination.


 \section{Number of modular reductions over a prime field}
\label{sec:mod}

In the following we suppose that the operations are done with full
delayed reduction for a single multiplication and any number of
additions: operations of the form  are reduced only once
at the end of the addition, but  requires two
reductions.
In practice, only a limited amount of accumulations can be done on an
actual mantissa without overflowing, but we neglect this in this
section for the sake of simplicity. 
See e.g. \cite{jgd:2008:toms} for more details. 
For instance, with this model, the number of reductions required by a
classic multiplication of matrices of size  by 
is simply: . We denote this by .
This extends e.g. also for triangular solving:

\begin{theorem}\label{thm:trsm}
Over a prime field modulo , the number of reductions modulo  required by
 with full delayed reduction is:

\customvspace{-5pt}
\end{theorem}
\begin{proof} 
If the matrix is unitary, then a fully delayed reduction is required
only once after the update of each row of the result.
In the generic case, we invert each diagonal element first and
multiply each element of the right hand side by this inverse diagonal
element, prior to the update of each row of the result. This gives
 extra reductions. 
\end{proof}

Next we show that the new pivoting strategy is more efficient in terms
of number of integer division. 
\begin{theorem} Over a prime field modulo  and on a full-rank
  square  matrix with generic rank profile, and  a power
  of two, the number of reductions modulo  required by the
 elimination algorithms with full delayed reduction is:
\customvspace{-5pt}

\customvspace{-10pt}
\end{theorem}
\begin{proof}
If the top left square block is full rank then \pluq reduces to
  one recursive call, two square TRSM (one unitary, one generic) one
  square matrix multiplication and a final recursive call. In terms of modular reductions, this gives: 
.
Therefore, using theorem~\ref{thm:trsm}, the number of reductions
within \pluq satisfies  so that
it is  if  is a power of two.

For row or column oriented elimination this situation is more
  complicated since the recursive calls will always be
  rectangular even if the intermediate matrices are full-rank.
We in fact prove, by induction on , the more generic:

First  since  is a triangular decomposition of the  matrix .
Now suppose that Equation~\ref{eq:rcup} holds for .
Then we follow the row oriented algorithm of \cite[Lemma 5.1]{jgd:2008:toms} which makes two recursive calls, one \trsm and one \MM to get  . 
We then apply the induction hypothesis on the recursive calls to get

The latter is also obtained by substituting  in Equation~\ref{eq:rcup} so that the induction is proven.
\customvspace{-15pt}\end{proof}

This show that the new algorithm requires much less modular reductions, as soon
as  is larger than . Over finite fields, since reductions can be much
more expensive than multiplications or additions by elements of the field, this
is a non negligible advantage. We show in the next section that this
participates to the better practical performance of the \pluq algorithm.
 \section{A base case  algorithm}\label{sec:base}

We propose in algorithm~\ref{alg:pluq:iter} an iterative algorithm computing the same PLUQ decomposition as
algorithm~\ref{alg:pluq:4rec}. 
The motivation is to offer an alternative to the recursive algorithm 
improving the computational efficiency on small matrix sizes. 
Indeed, as long as the matrix fits the cache memory, the amount of
page faults of the two variants are similar, but the iterative
algorithm reduces the amount of row and column permutations.
The block recursive algorithm can then be modified so that it switches to the
iterative algorithm whenever the matrix dimensions are below a certain
threshold.

Unlike the common Gaussian elimination, where pivots are searched in the whole
current row or column, the strategy is here to proceed with an incrementally growing
leading sub-matrix. This implies a Z-curve type search scheme, as shown on
figure~\ref{fig:iter}.
This search strategy is meant to ensure the properties on the rank profile that
have been presented in section~\ref{sec:rp}.
\begin{figure}[h]
\begin{center}
  \includegraphics[width=.52\linewidth]{pluq_iter}
\end{center}
  \caption{Iterative base case PLUQ decomposition}
\label{fig:iter}  
\end{figure}

{\scriptsize
\begin{algorithm}[H]
  \caption{\pluq iterative base case}
  \label{alg:pluq:iter}
\begin{algorithmic}
\Require{ a  matrix over a field}
\Ensure{:   and  permutation matrices}
\Ensure{: the rank of }
\Ensure{ where  is  unit lower triang.,  is  upper triang. and such that .
}

\State 
\While{ or }
\State \Comment{Let  and
      }
    \If{ and }
        \State  row index of the first non zero entry in 
        \State 
    \ElsIf{ and }
        \State  column index of the first non zero entry in 
        \State 
    \ElsIf{ and  and }
    \State 
    \State 
    \Else
    \State 
    \State continue
    \EndIf
    \Comment{At this stage,  is a pivot}
    \For{}
      \State 
      \State 
    \EndFor
    \State 
    \Comment{Swap pivot column}
    \State  \Comment{Swap
      pivot row}
    \State  \Comment{
      swaps indices  and }
    \State 
\EndWhile
\end{algorithmic}
\end{algorithm}
}

\begin{remark}
In order to further improve the data locality, this iterative
algorithm can be transformed into a left-looking
variant~\cite{DonDufSorVor98}. We did not write this version here for
the sake of clarity, but this is how we implemented the base case for
the experiments of section~\ref{sec:exp}.
\end{remark}
 \section{Experiments}\label{sec:exp}
We present here experiments comparing an implementation of
algorithm~\ref{alg:pluq:4rec} computing a PLUQ decomposition against the
implementation of the CUP/PLE decomposition, called \texttt{LUdivine} in the 
\texttt{FFLAS-FFPACK} library\footnote{\url{http://linalg.org/fflas-ffpack}}.
The new implementation of the PLUQ
decomposition is available in this same library from version svn@346.
We ran our tests on a single core of an Intel Xeon
E5-4620@2.20GHz using gcc-4.7.2.

Figures~\ref{fig:densefull} and~\ref{fig:densedeficient}  compare the computation time of
LUdivine, and the new PLUQ algorithm. In figure~\ref{fig:densefull}, the matrices are dense, with full rank. The
computation times are similar, the PLUQ algorithm with base case showing a
slight improvement over LUdivine. 
\begin{figure}[htb]
   \includegraphics[width=\linewidth]{speed-full}
   \caption{Computation time with dense full rank matrices over .}
   \label{fig:densefull}
\end{figure}
\begin{figure}[thb]
   \includegraphics[width=\linewidth]{timing-dense-deficientrank1009}
   \caption{Computation time with dense rank deficient matrices (rank is half
     the dimension)}
   \label{fig:densedeficient}
\end{figure}
\begin{figure}[htb]
   \includegraphics[width=\linewidth]{speed-half}
   \caption{Computation time with dense rank deficient matrices of larger dimension}
   \label{fig:densedeficientlarge}
\end{figure}
In figures ~\ref{fig:densedeficient} and~\ref{fig:densedeficientlarge}, the
matrices are square, dense with a rank equal to half the dimension. To ensure
non trivial row and column rank profiles, they are generated from a LEU
decomposition, where  and  are uniformly random non-singular lower and
upper triangular matrices, and  is zero except on  positions, chosen
uniformly at random, set to one.
The cutoff dimension for the switch to the base case has been set to an optimal
value of  by experiments. Figure~\ref{fig:densedeficient}
shows how the base case greatly improves the efficiency for PLUQ, presumably for
it reduces the number of row and column permutations. With the base case the computation
time is comparable to LUdivine. More precisely, PLUQ becomes faster than
LUDivine for dimensions above 9000. Figure~\ref{fig:densedeficientlarge} shows
that, on larger matrices, PLUQ can be about 10\% faster than LUdivine.







Table~\ref{tab:cachemisses} summarizes some of the data reported by the callgrind tool
of the valgrind emulator (version 3.8.1) concerning the cache misses. We also
report in the last column the corresponding computation time on the machine (without emulator). 
The matrices used are the same as in figure~\ref{fig:densedeficient}, with
rank half the dimension.
We first  notice the impact of the base case on the PLUQ algorithm: although it
does not change the number of cache misses, it strongly reduces the total number
of memory accesses (less permutations), thus improving the computation time.
Now as the dimension grows, the total amount of memory accesses and
the amount of cache misses plays in favor of PLUQ which becomes faster
than LUdivine. 
\begin{table*}[htbp]
\begin{center}
\begin{tabular}{llrrrrr}
\toprule
 Matrix & Algorithm & Accesses & L1 Misses & LL Misses & Relative  & Timing (s) \\\midrule
\multirow{3}{*}{A4K} & PLUQ-no-base-case & 1.319E+10 & 7.411E+08 & 1.523E+07 & .115 & 5.84 \\& PLUQ-base-case & 8.119E+09 & 7.414E+08 & 1.526E+07 & .188 & 2.65 \\& LUdivine & 1.529E+10 & 1.246E+09 & 2.435E+07 & .159 & \textbf{2.35} \\\midrule
\multirow{3}{*}{A8K} & PLUQ-no-base-case & 6.150E+10 & 5.679E+09 & 1.305E+08 & .212 & 28.4\\
& PLUQ-base-case & 4.072E+10 & 5.681E+09 & 1.306E+08 & .321 & 15.4 \\& LUdivine & 7.555E+10 & 9.693E+09 & 2.205E+08 & .292 & \textbf{15.2}\\\midrule
\multirow{3}{*}{A12K} & PLUQ-no-base-case & 1.575E+11 & 1.911E+10 & 4.691E+08 & .298 & 75.1 \\
& PLUQ-base-case & 1.112E+11 & 1.911E+10 & 4.693E+08 & .422 & \textbf{45.7} \\& LUdivine & 2.003E+11 & 3.141E+10 & 7.943E+08 & .396 & 46.4\\\midrule
\multirow{3}{*}{A16K} & PLUQ-no-base-case & 3.142E+11 & 4.459E+10 & 1.092E+09 & .347 & 152 \\
 & PLUQ-base-case & 2.302E+11 & 4.459E+10 & 1.092E+09 & .475 & \textbf{99.4} \\& LUdivine & 4.117E+11 & 7.391E+10 & 1.863E+09 & .452 & 103 \\\bottomrule
\end{tabular}
\end{center}
\caption{Cache misses for dense matrices with rank equal half of the dimension}  
\label{tab:cachemisses} 
\end{table*}



 





\section{Conclusion and perspectives}

The decomposition that we propose can first be viewed as an improvement over the
LEU decomposition, introducing a finer treatment of rank deficiency that reduces
the number of arithmetic operations, makes the time complexity rank sensitive
and allows to perform the computation in-place.

Second, viewed as a variant of the existing CUP/PLE decompositions, this new
algorithm produces more information on the rank profile and has better cache
efficiency, as it avoids calling matrix products with rectangular matrices of
unbalanced dimensions. It also  performs fewer modular reductions when computing
over a finite field. 

Overall the new algorithm is also faster in practice than previous
implementations when matrix dimensions get large enough. 

Now, in a parallel setting, it should exhibit more parallelism than row or
column major eliminations since the recursive calls in step 2 and 3 are
independent. This is also the case for the
TURBO algorithm of~\cite{jgd:2002:PComp}, but the latter requires more
arithmetic operations. Further experiments and analysis of communication costs
have to be conducted in shared and distributed memory settings to assess the
possible practical gains in parallel.

{\small
\bibliographystyle{abbrvurl}
\bibliography{pluq}
}
\appendix
\renewcommand{\arraystretch}{.7}
\setlength{\arraycolsep}{.2\arraycolsep}

\section{Correctness of algorithm~1}
\label{app:pluq:correct}


First note that 



Hence



Similarly,

and 



Now as
 and

we have

 \end{document}
