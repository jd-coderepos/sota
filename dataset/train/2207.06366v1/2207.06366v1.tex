\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{emnlp2022} 
\usepackage{booktabs}       \usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{dblfloatfix}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\newcommand{\minisection}[1]{\noindent{\bf #1}\nobreak}
\newcommand{\redTodo}[1]{{\textcolor{red}{[#1]}}}



\title{\textit{N}-Grammer: Augmenting Transformers with latent 
\textit{n}-grams}
\author{{\normalfont Aurko Roy}\thanks{* Equal contributions.}\,\,\,,\,\,
  {\normalfont Rohan Anil},\,\, 
  {\normalfont Guangda Lai},\,\,  
  {\normalfont Benjamin Lee},\,\,
  {\normalfont Jeffrey Zhao},\,\,   \\  
  {\normalfont Shuyuan Zhang}\,\,
  {\normalfont Shibo Wang},\,\,  
  Ye Zhang,\,\, 
  Shen Wu,\,\,  
  Rigel Swavely,\,\,  
  Tao (Alex) Yu,\,\,   \\
  Phuong Dao,\,\,  
  Christopher Fifty,\,\, 
  Zhifeng Chen,\,\,  
  Yonghui Wu\,\,  \\
  \\
  {Google Research, Brain Team, Mountain View, CA}\\
  \texttt{\{aurkor, rohananil\}@google.com} \\
}

\begin{document}
\maketitle
\begin{abstract}
Transformer models have recently emerged as one of the foundational models in natural language processing, and as a byproduct, there is significant
recent interest and investment in scaling these models. However, the 
training and inference costs of these large Transformer language models 
are prohibitive, thus
necessitating more research in identifying more efficient variants.
In this work, we propose a simple yet effective modification to the Transformer architecture 
inspired by the literature in statistical language modeling, 
by augmenting the model with \textit{n}-grams that are constructed from a 
discrete latent representation of the text sequence. 
We evaluate our model, the \textit{N}-Grammer on language modeling on the 
C4 data-set as well as text classification on the SuperGLUE data-set,
and find that it outperforms several strong
baselines such as the Transformer and the Primer.
We open-source our model for reproducibility purposes in Jax  \footnote{\url{https://github.com/tensorflow/lingvo/tree/master/lingvo/jax}}.
\end{abstract}

\section{Introduction}
The area of generative modeling of text has witnessed rapid and impressive 
progress driven by the adoption of self-attention to neural networks. Attention
for machine translation was proposed in \citet{bahdanau2014neural, cho2014learning, vaswani2017attention} 
and subsequent works such as  \citet{radford2018improving,devlin2018bert} applied the learned 
representations of language to several problems in natural language processing. The rapid progress has been made possible primarily by increasing the modeling capacity of these Transformer based models to billions of 
parameters  \citep{brown2020language} which comes at a large computational cost. The computational cost of 
Transformer models is being addressed in the literature by exploiting 
sparsity in self-attention  
\citep{ainslie2020etc, zaheer2020big, roy2021efficient},  
mixtures of experts \citep{shazeer2017outrageously,lepikhin2020gshard, fedus2021switch} for sparsity in the feed-forward
network, sparsity in the softmax computation \citep{correia2019adaptively},
and combining depth-wise convolution with attention \citep{wu2021cvt, so2021primer}. 

Motivated by the growing literature in training more efficient variants of Transformers, as well as the
classical literature on statistical language modeling \citep{koehn2009statistical},
we propose a simple modification to the Transformer architecture termed the \textit{N}-Grammer in this work.
The \textit{N}-Grammer layer improves the efficiency of language models by incorporating latent 
\textit{n}-gram representations into the model during training. Since the \textit{N}-Grammer layer only 
involves sparse operations during training and inference, we find that a Transformer model with the latent \textit{N}-Grammer layer can match the quality of a larger Transformer while being significantly faster at inference. This is due to the fact that on most hardware platforms,
the overhead of adding sparse operations such as an embedding look-up
required by the \textit{N}-Grammer is significantly lower
than that of dense matrix multiplication operations incurred by
scaling up the same Transformer model to have the same quality as the \textit{N}-Grammer.

\begin{figure*}[th!]
\centering
\includegraphics[width=0.8\linewidth]{ngrammer_design.pdf}
\caption{The \textit{N}-Grammer layer. It takes as input a 
sequence of uni-gram embeddings and outputs a parallel sequence 
of \textit{N}-gram augmented embeddings. The input embeddings are 
clustered into a discrete latent representation using PQ,
and \textit{n}-grams (bi-grams) IDs are computed over it. 
For each \textit{n}-gram ID, a trainable embedding is looked up 
from an embedding table and combined with 
the input embeddings to produce the output. }\label{fig:ngrammer}
\end{figure*}

\section{Related Work}
\minisection{Memory augmented models}
There has been a long line of work in augmenting sequence models with memory, e.g. the
Neural Turing Machine \citep{graves2014neural} and Memory Networks \citep{weston2014memory}. 
More recent works have proposed 
combining Transformer based models with product key look-up tables \citep{lample2019large},
while \citet{panigrahy2021sketch} propose memories based on sketches of past activations.
There has also been a lot of work on augmenting language models with non-parametric memory,
such as the -nearest neighbor language models of \citet{khandelwal2019generalization},
and similar retrieval augmented works such as 
\citet{lewis2020retrieval,guu2020realm,krishna2021hurdles}. 
In these retrieval augmented models, 
the model is conditioned on documents from the training corpus
or a knowledge base, with the hope that information from related articles 
can help improve the factual accuracy of the models.

\minisection{Discrete latent models for sequences}
Discrete latent models using Vector Quantization (VQ) have been widely used in speech
\citep{vqvae, wang2018style,schneider2019wav2vec} 
to learn unsupervised representations of audio signals.
Their use for modeling text sequences were studied in \citet{kaiser2018fast,
roy2018theory} where the motivation was to reduce the inference
latency for neural machine translation models by decoding in the latent space.


\minisection{\textit{N}-gram models for statistical language modeling}
\textit{N}-gram models have a long history in statistical modeling of 
language, see e.g., 
\citet{brown1992class, brown1993mathematics, katz1987estimation,kneser1995improved,chen1999empirical}. 
Before the advent of word vectors and distributed 
representations of language via neural networks
\citep{mikolov2013efficient, wu2016google}, \textit{n}-gram language models were the standard in the 
field of statistical language
modeling. More recent related work on combining neural
sequence models with \textit{n}-gram information is
due to \citet{sun2021revisiting} who propose concatenating
the representations within a local context, while 
\citet{huang2021lookup} propose combining RNN models with
\textit{n}-gram embedding tables. Our work differs from them 
in that we use an \textit{n}-gram look-up table on a 
discrete latent representation of the sequence,
which leads to a more meaningful assignment of shared
\textit{n}-gram representations.

\minisection{Product Quantization}
There has also been a long line of work on investigating variants of Vector
Quantization (VQ) that realize different trade-offs in data compression. 
The most related work in this domain is due to \citet{jegou2011product} who 
introduce a multi-head version of VQ which is termed
Product Quantization (PQ). PQ is widely used in computer 
vision, see e.g., \citet{ge2013optimized, yu2018product}. 
Our approach to learning
discrete latent codes use PQ over the attention heads.

\section{The \textit{N}-Grammer layer}
At a high level, we introduce a simple layer that augments the Transformer architecture with more memory based on latent \textit{n}-grams. While the \textit{N}-Grammer layer is general enough for
considering arbitrary \textit{N}-grams, 
we restrict ourselves to the use of bi-grams. 
We leave the exploration of higher-order \textit{n}-grams for future work.
The layer consists of four core operations: 
\begin{enumerate}
\item Given a sequence of uni-gram embeddings of a text, infer 
a sequence of 
discrete latent representation via PQ. 

\item Infer the bi-gram representation for the latent sequence.

\item Look up trainable bi-gram embeddings via hashing into the bi-gram vocabulary.

\item Combine the bi-gram embeddings with the input uni-gram embeddings.
\end{enumerate}
We describe each of these operations in more detail in the following sections.
To refer to a set of discrete items, we use the notation 
 to mean the set .

\subsection{Discrete latent representation of a sequence}\label{sec:vq}
The first step of the \textit{N}-Grammer layer is to obtain a parallel sequence of discrete latent
representations with Product Quantization (PQ) \citep{jegou2011product} by learning a codebook from the given sequence of input embeddings. The input embedding is a sequence of uni-gram 
embeddings , 
where  is the length of the sequence,  is the number of heads, and  is the embedding dimension per head. We learn a codebook  in  with  code-words with 
mini-batch -means \citep{bottou1995convergence}, and in the same step, we form the parallel sequence of 
discrete latent representation  of the sequence  by picking the codebook IDs 
that have the least distance from the input embeddings:
 
The advantage of this latent representation  is twofold. Firstly, it makes considering all  
bi-grams tractable by mapping the uni-gram embeddings to share the same code-word embedding based on 
similarity, thereby allowing us to use a smaller bi-gram embedding table. 
Secondly, when using a fixed size bi-gram vocabulary, having this latent 
representation allows for a more efficient representation to be learned
compared to directly using the uni-gram IDs. For instance, a uni-gram 
vocabulary of  would entail a bi-gram vocabulary of roughly 
 billion, which adds a significant memory overhead. 

\subsection{Bi-gram IDs from discrete latent 
representation}\label{sec:compute-bigram}
The second step is to convert the discrete latent 
representation  
computed in Section~\ref{sec:vq} to bi-gram IDs . The latent 
bi-gram IDs are formed at each
position by combining the uni-gram latent IDs  from the previous position as
i = 0
where  is the size of our codebook. 
This directly maps the discrete latent sequence from  a vocabulary space of  to the
latent bi-gram vocabulary space of .

\subsection{Constructing bi-gram representations}\label{sec:bigram} 
The third step is to construct bi-gram latent 
representations  of
the sequence. We can consider all  bi-grams and 
augment the model with an embedding for each such bi-gram. 
In practice, the compression for machine translation 
models with a  uni-gram vocabulary of  involves 
clustering each token into 
roughly  
clusters without sacrificing quality \citep{kaiser2018fast, roy2018theory}. 
In this instance, to consider all bi-grams would involve 
constructing an embedding table with  
million rows. Since this is still large, we map the latent
bi-gram IDs to a smaller bi-gram vocabulary of size ,
by using separate hash functions for each head.

More precisely, we have a latent bi-gram embedding table
, 
where  is the bi-gram vocabulary and  is
the bi-gram embedding dimension. The bi-gram embedding 
 of the text sequence
is then constructed as 

where for each head , we select a random prime  greater than , 
and  is chosen randomly in  and
 is chosen randomly in . 
This scheme is a universal hashing scheme and
guarantees a low collision probability for the discrete
latent codes of each head \citep{thorup2015high}.
Note that the bi-gram embedding vector  is a -dimensional vector. 

\begin{table*}[ht]
\centering
\begin{tabular}{lccccccc}
\toprule
       Model &  Layers & Params & Vocab size & Clusters & Dim & Inference Ex/sec & PP \\
       \midrule
        Transformer & 16 & 234M & - & -  & - & 402.00 & 15.32 \\ 
        Transformer-L & 20 & 284M & - & - & - & 331.12 & 14.70 \\\midrule
        Primer & 16 & 234M & - & -  &- & 346.32 & 15.10 \\ 
        Primer-L & 18 & 284M & - & - & - & 284.40  & 15.01 \\ \midrule
        \textit{N}-Grammer & 16 & 246M & 196K & - & 12.5\% & 379.60 &  15.36 \\ 
        \textit{N}-Grammer & 16 & 259M & 196K & - & 25.0\% & 378.64 & 15.27 \\
        \textit{N}-Grammer & 16 & 284M & 196K & - & 50.0\% & 375.40 & 15.50 \\ \midrule
        \textit{N}-Grammer & 16 & 251M & 196K & 4K & 12.5\% & 366.80 & 15.26 \\
        \textit{N}-Grammer & 16 & 263M & 196K & 4K & 25.0\% & 362.40 &  15.07 \\ 
        \textit{N}-Grammer & 16 & 288M & 196K & 4K & 50.0\% & 362.00 & 15.01 \\  \midrule
        \textit{N}-Grammer & 16 & 255M & 196K & 8K & 12.5\% & 359.52 & 15.58 \\ 
        \textit{N}-Grammer & 16 & 267M & 196K & 8K & 25.0\% & 358.96 & 15.44 \\
        \textit{N}-Grammer & 16 & 292M & 196K & 8K & 50.0\% & 358.16 & 15.01 \\ \midrule
\textit{N}-Grammer & 16 & 267M & 393K & 8K & 12.5\% & 363.60 & 15.32 \\
        \textit{N}-Grammer & 16 & 292M & 393K & 8K & 25.0\% & 360.16 & 15.97 \\ 
        \textit{N}-Grammer & 16 & 343M & 393K & 8K & 50.0\% & 356.94 & 14.79 \\ 
    \bottomrule
    \end{tabular}
\vspace{1mm}
\caption{Ablation results on auto-regressive language modeling on the C4 
data-set \citep{raffel2019exploring}. 
The column labeled \textit{Vocab Size} refers to the bi-gram vocabulary size, 
while the column labeled \textit{Dim} refers to the 
bi-gram embedding dimension as a percentage of the total 
model dimension. All models are trained with a batch size 
of  for a total of M steps. We report the test
perplexity (\textit{PP}) and as well as the inference
through-put in examples per second 
(\textit{Inference Ex/sec}) on a 
TPU-v3 with 8 cores (higher is better).}
\label{tab:c4-ablation}
\end{table*}

\subsection{Combining the embeddings}\label{sec:combining}
The final step is to form a new representation of the text sequence which is derived by combining the uni-gram embedding 
with the latent bi-gram embedding 
obtained in Section~\ref{sec:bigram}. The bi-gram embedding and uni-gram embedding are both independently layer normalized (, followed by simply concatenating the two along the embedding dimension to produce
 which is passed as input to rest of the Transformer network. 
Note that layer normalization \citep{ba2016layer} leads to more stable training. 

\section{Experiments \& Results}
We compare the \textit{N}-Grammer model with the Transformer architecture \citep{vaswani2017attention} 
as well as with the recently proposed Primer architecture \citep{so2021primer} on the C4 data-set
\citep{raffel2019exploring}
\footnote{\url{https://www.tensorflow.org/datasets/catalog/c4}}. To establish a strong baseline for our experiments we use a Gated Linear Unit
\citep{dauphin2017language} as the feed-forward network with a GELU activation function 
\citep{hendrycks2016gaussian} in all our models, except the Primer. The Primer architecture 
uses a  depth-wise convolution after the key,
query and value projections, and the squared RELU 
activation function as proposed in 
\citet{so2021primer}. For all experiments, we use the rotary position embedding (RoPE) from \citet{su2021roformer},
which greatly improves the quality of all models. 

We compare the \textit{N}-Grammer, Primer and Transformer models in Table~\ref{tab:c4-ablation}. The baseline Transformer model has  layers and  heads, with a model dimension of . We train all the models
with a batch size of  and a sequence length of  on a TPU-v3. A more detailed exposition of the various
hyper-parameter choices is given in Section~\ref{sec:hparams}.
For the \textit{N}-Grammer models, we ablate 
with different sizes for the bi-gram embedding dimension ranging from 
to . Since adding \textit{n}-gram embeddings increases 
the number of trainable parameters, we also train two large 
baselines in Table~\ref{tab:c4-ablation} (Transformer-L  and 
Primer-L) which have the same order of parameters as the 
\textit{N}-Grammer models. However, unlike the larger 
Transformer models, the training and inference cost of 
\textit{N}-Grammer does not scale 
proportional to the number of parameters in the
\textit{n}-gram embedding layer, 
since they rely on sparse look-up operations 
(see column Inference Ex/sec in 
Table~\ref{tab:c4-ablation}). Thus for example, we find from 
Table~\ref{tab:c4-ablation} that the best \textit{N}-Grammer
model with a \textit{n}-gram vocabulary of K and a discrete
latent vocabulary of K matches the quality of Transformer-L
and Primer-L in perplexity (14.79 vs 14.70 vs 15.01) while having
significantly higher through-put (356.94 vs 331.12 vs 284.40 
examples/sec).

We also examine a simple version of \textit{N}-Grammer where we compute the \textit{n}-grams
directly from the uni-gram vocabulary as in Section~\ref{sec:bigram} rather than from the latent 
representation of Section~\ref{sec:vq}. This is reported in 
Table~\ref{tab:c4-ablation} and corresponds to the 
\textit{N}-Grammer without an entry
in the clusters column. Note that in this case, the modulo 
hashing scheme of 
Section~\ref{sec:bigram} is random and independent of the 
content of the actual uni-gram embeddings. We inspect the 
individual cluster assignment in Section~\ref{sec:latent_rep} 
and find common themes among the groupings.

\section{Hyper-parameters for experiments}\label{sec:hparams}
In this section we report the hyper-parameter settings for all our experiments for reproducibility purposes.

\subsection{Optimizer hyper-parameters}
We use the Adam optimizer
\citep{adam} and tune the learning rate as well as
 as reported in 
\citep{agarwal2020disentangling}. 
We find that decreasing  from the standard 
setting of  to 
 benefits the Transformer models while having less of an effect on the
Primer \citep{so2021primer}. We use a learning rate of 
 for all models.
We use a  and  and clip the gradient norm to .
We do not use any weight decay. We train all models with a global batch size of  on a TPU-v3 with 
cores and a sequence
length of .

\subsection{\textit{N}-Grammer hyper-parameters}
For the \textit{N}-Grammer models, we use a discrete latent vocabulary of 
except for the baseline \textit{N}-Grammer models which directly compute \textit{n}-grams
on the uni-gram vocabulary. For training the 
\textit{n}-gram embedding tables we use the Adagrad 
optimizer~\citep{duchi2011adaptive}, which is known to be
more suitable for learning sparse features. We use a learning 
rate of  for training the \textit{n}-gram embedding table,
with the same learning rate schedule as the base model. We find that
using a  higher or lower learning rate leads to unstable
training of the \textit{N}-Grammer model.

We train the cluster centers for learning the discrete latent 
representation using mini-batch -means 
\citep{bottou1995convergence}. We do not use
any smoothing or exponential moving averages for either the counts or the centers, since we find 
empirically that it doesn't help in our setting. 
We use a learning rate of  for learning the
discrete representations.

\section{Position of the \textit{N}-Grammer layer}\label{sec:position_ablations}
We perform ablation experiments on the position of the
latent \textit{N}-Grammer layer, 
since potentially one may add it to any intermediate layer
of the network. We take the best \textit{N}-Grammer
model from Table~\ref{tab:c4-ablation}, corresponding to
an \textit{n}-gram vocabulary size of K and a latent
vocabulary of size K and ablate the position of
the \textit{N}-Grammer layer in Table~\ref{tab:position-ablation}. We observe that placing
the \textit{n}-gram layer at the beginning of the network
turns out to be the best choice, since moving the layer 
successively to the end of the network leads to progressively worse performance. We hypothesize
that this is due to the presence of 
fewer attention layers to leverage the improved 
representations from the -gram embeddings.
\begin{table}[ht]
\centering
\begin{tabular}{lc}
\toprule
       Model &  PP \\
       \midrule
        \textit{N}-Grammer & 14.79 \\ 
        \textit{N}-Grammer \textit{begin} & 14.92 \\ 
        \textit{N}-Grammer \textit{mid} & 15.13\\
        \textit{N}-Grammer \textit{end} & 15.17\\
    \bottomrule
    \end{tabular}
    \vspace{1mm}
    \caption{Ablation results on the position of the 
    \textit{N}-Grammer layer on the C4 
    data-set~\citep{raffel2019exploring}. We use the best
    \textit{N}-Grammer model from Table~\ref{tab:c4-ablation},
    with a vocab size of K. The \textit{N}-Grammer
    models labelled \textit{begin}, \textit{middle} and \textit{end} refer to the latent \textit{n}-gram 
    embedding layer being placed after the first layer,
    the middle layer and the end layer respectively.}
    \label{tab:position-ablation}
\end{table}

\section{Optimizing through-put}
We note that there is a trade-off in computing the discrete 
latent representation of a text sequence, 
where it may be more efficient in practice to cluster the 
uni-gram vocabulary directly rather than clustering the 
embedded text sequence. This is an important consideration 
when serving the \textit{N}-Grammer model, since the mapping 
from token to discrete latent is fixed after the completion
of training thereby allowing us to pay a one time cost
in computing this mapping for the entire vocabulary. 
We formulate this more precisely as follows.

Let the uni-gram vocabulary be , the latent vocabulary
, the sequence length , batch size ,
and let the  \textit{N}-Grammer model serve a 
total of  examples. 
If we were to compute the latent representation for each
sequence, we incur a cost of  per sequence. 
On the other hand, if we were to compute
the latent representation for the entire vocabulary up-front
and cache the mapping from token to latent, we pay a one time 
cost of  for inferring the latent representations. 
Assuming an  cost of looking up the latent 
representation per sequence, this
cost can be amortized over the  examples to get a per 
sequence cost of . As ,
i.e., the model is continuously deployed, 
we essentially get to compute the discrete latent 
representation in constant time per sequence during serving.

Since computing the \textit{n}-gram ID (see 
Section~\ref{sec:compute-bigram})
and retrieving the \textit{n}-gram
representations are also constant (with respect to the
number of attention layers, attention 
heads and model dimension)
time operations per sequence, this implies that when
served long enough, 
the \textit{N}-Grammer model essentially incurs a 
constant overhead over the Transformer.

\section{Convergence comparisons}
We have included training curve comparisons of the \textit{N}-Grammer with that of
the Transformer \citep{vaswani2017attention} and the Primer \citep{so2021primer}. We compare the
three models in Figures~\ref{fig:wall-ppl} 
and~\ref{fig:wall-acc} where the -axis denotes the wall clock
time on a TPU-v3 while the -axis denotes the log perplexity and top-1 accuracy respectively
on the C4 data-set \citep{raffel2019exploring}. 
From Figure~\ref{fig:wall} we see that the 
\textit{N}-Grammer model is roughly  faster than the Primer in wall clock time to reach
the same perplexity or accuracy. More precisely, the
baseline Primer model after 1M steps (180 TPU hours)
has a perplexity of
, which the 
best \textit{N}-Grammer model from Table~\ref{tab:c4-ablation}
achieves at K steps (90 TPU hours).

\begin{figure*}[htbp]
\centering
    \begin{subfigure}[l]{1.0\columnwidth}
        \includegraphics[width=1.0\textwidth]{figs/log_pplx_vs_hours.pdf}
        \caption{Log perplexity vs wall-clock time on TPU-v3}\label{fig:wall-ppl}
    \end{subfigure}
    \hfill{}
    \begin{subfigure}[r]{1.0\columnwidth}
        \includegraphics[width=1.0\textwidth]{figs/accuracy_vs_hours.pdf}
        \caption{Top-1 accuracy vs wall-clock time on TPU-v3}\label{fig:wall-acc}
    \end{subfigure}
\caption{Wall-clock time comparisons between Transformer with 
Gated GELU, Primer and \textit{N}-Grammer on the C4 data-set \citep{raffel2019exploring}.}
\label{fig:wall}
\end{figure*}

\section{Comparison on downstream tasks}\label{sec:super-glue}
We fine-tune the baseline Transformer, Primer (as well as their 
large variants) and the best \textit{N}-Grammer model from 
Table~\ref{tab:c4-ablation}
on the SuperGLUE benchmark~\citep{wang2019superglue} to evaluate
whether the perplexity gains of the \textit{N}-Grammer model
also result in downstream classification gains.
For all models we take the checkpoint at M steps,
and fine-tune for K steps with a constant learning
rate of . We report the downstream evaluation
metrics in Table~\ref{tab:superglue}. 
From Table~\ref{tab:superglue} we observe that the \textit{N}-Grammer
improves on the quality of the Transformer and Primer models on most 
SuperGLUE tasks. More surprisingly, we see that it also substantially
improves on the larger Transformer-L and Primer-L models on tasks like
COPA, RTE, WiC and WSC.
\begin{table*}[htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
       Model & BoolQ & CB & COPA & MultiRC & ReCoRD
       & RTE & WiC & WSC & Avg\\
       & Acc. & F1/Acc. & Acc. & F1/EM & F1/EM & Acc. & Acc.
       & Acc. & \\
       \midrule
        Transformer & 65.23 & 54.03/67.86 & 55.0 & 59.47/13.64  & 30.34/29.32 & 53.43 & 54.55 & 61.54 & 52.13\\
        Transformer-L & 66.15 & \textbf{73.74}/\textbf{75.00} & 59.0
        & 62.09/12.17 & 29.19/28.21 & 57.04 & 55.33 & 62.50 & \textbf{55.03}\\
        \midrule
        Primer & \textbf{66.27} & 62.64/71.43 & 53.0 & \textbf{62.89}/\textbf{12.91} 
        & \textbf{30.56}/\textbf{29.55} & 55.96 & 54.86 & 65.38 & 53.81\\
        Primer-L & 62.20 & 47.69/58.93 & 58.0 & 51.83/4.62 & 
        25.21/24.35 & 51.99 & 54.08 & 63.46 & 49.50\\
        \midrule
        \textit{N}-Grammer & 64.98 & 59.69/67.86 & \textbf{60.0} & 61.95/11.33 & 29.90/28.91 & \textbf{59.21} & \textbf{56.11} & \textbf{68.27} & 54.80\\ 
    \bottomrule
    \end{tabular}}
\caption{Fine-tuning results on 
    SuperGLUE~\citep{wang2019superglue} comparing the 
    Transformer, Primer and the best \textit{N}-Grammer
    model from Table~\ref{tab:c4-ablation}. The 
    \textit{N}-Grammer model has a discrete latent vocabulary
    of size K and a \textit{n}-gram vocabulary of
    size K. For all models we take the pre-trained 
    checkpoint at 1M steps and 
    fine-tune for K steps with a constant
    learning rate of .}
    \label{tab:superglue}
\end{table*}

\section{Analysis of the latent representations}\label{sec:latent_rep}
We inspect the discrete latent representations learned by the \textit{N}-Grammer layer by
examining the different uni-gram tokens that are assigned to the same cluster ID. 
We take a trained \textit{N}-Grammer model with  clusters, \textit{n}-gram embedding
dimension of  and \textit{n}-gram vocabulary of 196K.
We pass the entire set of  uni-gram embeddings as input to the \textit{N}-Grammer layer, 
thereby gathering the cluster assignment of every uni-gram token.
We present some of these in Table~\ref{tab:cluster},
where we find that the model learns to group related uni-gram tokens together:
\begin{enumerate}
\item the cluster with head ID  and cluster ID  corresponds to sports and games,

\item the cluster with head ID  and cluster ID  corresponds to places,

\item the cluster with head ID  and cluster ID  corresponds to animals and fruits,

\item the cluster with head ID  and cluster ID  corresponds to the arts,

\item the cluster with head ID  and cluster ID  also corresponds to the arts.
\end{enumerate}
We also observe that several heads independently learn a similar themed grouping, e.g., head  and
 both have a cluster dedicated to arts and entertainment.
\setlength{\arrayrulewidth}{0.5mm}
\setlength{\tabcolsep}{18pt}
\renewcommand{\arraystretch}{1.5}
\begin{table*}[htbp]
\centering
\begin{tabular}{|p{0.1\linewidth}|p{0.1\linewidth}|p{0.5\linewidth}|}
\hline
       Head ID & Cluster ID & Uni-gram Tokens \\
       \hline
        0 & 6259 & Baseball, football, ceramic, Galaxy, hockey, basketball, Cricket, Basketball, guitar, acquisition, athlete, Soccer, Squid, sports\\\hline
        2 & 5362 & Alchemist, Vegas, hanger, Seinfeld, Kenya, Heroic, Kurdish, Rodgers, Bolivia, Venom, Qatar, dosage, Arcade, Emperor, becua, Finnish, Taiwanese, Chennai, hood, dub, flake, Balkan, Psalm, Bueno, Moldova, flow, mosquito, Filipino, Throne, Siberia, Trout, Fist, Czech, Boulevard, Azerbaijan, Peru, OW, plaster, Kashmir, NZ, Priest, Palestinian, Tibetan, stencil, Aragon, coils, HBO, Iceland, strains, Zimbabwe, firewall, Nepal, Elves, Iranian, Mongol, Traffic, Camilla, parade, Afghan, hose, Serpent, Tarantino, web, Khal, Squid, Mala, Syrian, hood\\\hline
        0 & 7468 & Unknown, spoon, Shut, coconut, grapefruit, cran, Kami, moon, spider, yogurt, perfume, Wine, Skate, antique, snail, Onion, guinea, puppy, mineral, Reagan, elbow, bark, patio, beneath, snake, lever, bunny, falcon, rail, ribbon, knob, apples, quarry, corn, nach, hiking, invoice, Pour, flora, fishing, Paint, olive, violin, octopus, horizontal, blanket, circular, army, nickel, cattle, potato, dolphin, mosquito, citrus, shutter\\\hline
        2 & 8080 & Knicks, Shakespeare, SPE, nursing, spells, Alexa, arrow, vocalist, rehearsal, tunnel, eine, Critical, clar, BAN, remix, obstacle, musicians, BRO, legislature, EMS, Manga, piano, sword, vocal, bald, choir, Messi, Beta, cad, illustrator, organ, conjunction, lunar, bien, needles, musician, hiking, tad, poe, Pay, violin, Marxist, literary, Theater, gig, poetry, Illustrator, guitar, Pluto, Camaro, Fog, orbit, dancing, epub \\\hline
        4 & 6618 & Wise, vocalist, actor, cheek, musicians, TION, piano, tunes, choir, filmmaker, musician, Suzuki, violin, Theater, gig, Drama, guitar, logic, Entertainment\\
\hline
\end{tabular}
\vspace{1mm}
\caption{Mapping of uni-gram tokens to cluster IDs for the \textit{N}-Grammer
model. The \textit{N}-Grammer model has  heads,  clusters, an \textit{n}-gram embedding 
dimension of 16 and a \textit{n}-gram vocabulary of 196K. We report the head index (Head ID), the cluster
index (Cluster ID) and the uni-gram tokens assigned to those IDs for a random subset of clusters.}
\label{tab:cluster}
\end{table*}



\section{Conclusion}
We introduced the \textit{N}-Grammer layer for augmenting 
the Transformer architecture with latent \textit{n}-grams, 
and find that it can match a larger Transformer and Primer
in quality while being significantly faster in 
inference. The \textit{N}-Grammer architecture is particularly
suitable for devices that allow storing large embedding tables
while supporting only distributed gather-scatter operations. 
We also showed that by caching the mapping from token to discrete latent, 
one can serve the \textit{N}-Grammer architecture with only a constant 
overhead over the Transformer. This makes the \textit{N}-Grammer
attractive for deployment, since on most hardware platforms
sparse operations such as an embedding look-up is significantly faster
than dense operations such as matrix multiplications.

\clearpage
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}





\end{document}
