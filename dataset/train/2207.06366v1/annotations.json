[{'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'F1', 'Score': '61.95'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'MultiRC', 'Metric': 'EM', 'Score': '11.33'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '64.98'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'COPA', 'Metric': 'Accuracy', 'Score': '60.0'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'F1', 'Score': '29.90'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ReCoRD', 'Metric': 'EM', 'Score': '28.91'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'Words in Context', 'Metric': 'Accuracy', 'Score': '56.11'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'F1', 'Score': '59.69'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'CommitmentBank', 'Metric': 'Accuracy', 'Score': '67.86'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '59.21%'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'C4', 'Metric': 'Perplexity', 'Score': '14.79'}}, {'LEADERBOARD': {'Task': 'Coreference Resolution', 'Dataset': 'WSC', 'Metric': 'Accuracy', 'Score': '68.27'}}]
