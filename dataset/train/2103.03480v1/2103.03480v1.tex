\section{Experimental Results}
We implement our approach and evaluate it on the public  KITTI \cite{geiger2012we} 3D object detection benchmark. 
\subsection{Dataset and Implementation Details}
\subsubsection{Dataset:} the KIITI data is collected from the real traffic environment in Europe streets. The whole dataset has been divided into training and testing two subsets, which consist of  and  frames, respectively. Since the ground truth for the testing set is not available, we divide the training data into a training and validation set as in \cite{yan2018second,zhou2018voxelnet}, and obtain  data samples for training and  data samples for validation to refine our model. On the KITTI benchmark, the objects have been categorized into ``Easy'', ``Moderate'' and ``Hard'' based on their height in the image and occlusion ratio, etc. For each frame, both the camera image and the LiDAR point cloud have been provided, while only RGB image has been used for object detection and the point cloud is only used for visualization purposes.

\subsubsection{Evaluation Metric:} we focus on the evaluation on ``Car'' category because it has been considered most in the previous approaches. In addition, the number of the training data for ``Pedestrain'' and ``Cyclist'' is too small for training a stable model. For evaluation, the average precision (AP) with Intersection over Union (IoU) is used as the metric for evaluation. Specifically, before October 8, 2019, the KITTI test sever used the 11 recall positions for comparison. After that the test sever change the evaluation criterion from 11-points to 40-points because the latter one is proved to be more stable than the former \cite{simonelli2019disentangling}. Therefore, we use the 40-points criterion on the test sever, while we keep the 11-points criterion on validation dataset because most of previous methods only report their performance using 11-points criterion.  

\subsubsection{Implementation Details:} 
for each original image, we pad it symmetrically to  for both training and inference. Before training, these ground truth BBoxes whose depth larger than \SI{50}{\metre} or whose 2D projected center is out of the image range are eliminated and all the rest are used for training our model. Similar to \cite{liu2020smoke}, three types of data-augmentation strategies have been applied here: random horizontal flip, random scale and shift. The scale ratio is set to 9 steps from \numrange{0.6}{1.4}, and the shift ratio is set to 5 steps from \numrange{-0.2}{0.2}. To be clear, the scale and shift augmentation haven't been used for the regression branch because the 3D information is inconsistent after these transformations.   

\textbf{Parameters setting:} for each object, the ``depth'' prediction is defined as , where ,  are two predefined constants and  is the output of the network. Here, we set  experimentally and re-scale the output . For the focal loss in \eqref{eq:center-ness-loss} and \eqref{eq:mask_loss}, we set  and  for all the experiments. The group number for normalization in IAFA module is set to 8. For decreasing the GPU consumption, we set  and  in the IAFA module.

\textbf{Training:} Adam \cite{kingma2014adam} together with L1 weights regularization is used for optimizing our model. The network is trained with a batch size of 42 on 7 Tesla V100 for 160 epochs. The learning rate is set at  and drops at 80 and 120 epochs by a factor of 10. The total training process requires about 12 hours. During testing, top 100 center points with response above 0.25 are chosen as valid detection. No data augmentation is applied during inference process. 

\subsection{Evaluation on the ``test'' Split}


First of all, we evaluate our methods with other monocular based 3D object detectors on the KITTI testing benchmark. Due the limited space, we only list the results with public publications. For fair comparison, all the numbers are collected directly from the official benchmark website \footnote{\url{http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d}}. Here, we show the Bird-Eye-View (BEV) and 3D results with threshold of 0.7.



\iffalse
\begin{table*}[h!]
	\centering
	\resizebox{0.6\textwidth}{!}
	{\begin{tabular}{r|  ccc ccc}
			\hline
			\multicolumn{1}{r}{\multirow{2}{*}{Methods}}&   \multicolumn{3}{c}{\textbf{70 (\%)}} & \multicolumn{3}{c}{\textbf{70 (\%)}} \\
			\multicolumn{1}{c}{}     & \multicolumn{1}{l}{Moderate} & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard} & \multicolumn{1}{l}{Moderate} & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard} \\ \hline
			Baseline    & 9.76 & 14.03 &  7.84 & 14.49 & 20.83  &  12.75\\
			Proposed  & \textbf{12.01} & \textbf{17.81} & \textbf{10.61}  &  \textbf{17.88} & \textbf{25.88} & \textbf{15.35}\\ 
			Improvement  & \textbf{\R{+2.25}} &  \textbf{\R{+3.78}} & \textbf{\R{+2.77}}  & \textbf{\R{+3.39}} & \textbf{\R{+5.05}} & \textbf{\R{+2.6}} \\ \hline    
		\end{tabular}   
	}
	\caption{\normalfont Comparison with the baseline on the KITTI testing sever for 3D ``Car'' detection. The numbers in red represent the absolute improvements compared with the baseline.}
	\label{tab:comparison_with_baseline}
\end{table*}
\fi
\begin{table*}[ht!]
	\centering
	\resizebox{0.8\textwidth}{!}
	{\begin{tabular}{r| l c c c c c c c}
			\hline
			\multicolumn{1}{c}{\multirow{2}{*}{Methods}} & \multicolumn{1}{c}{\multirow{2}{*}{Modality}} & \multicolumn{3}{c}{\textbf{70 (\%)}} & \multicolumn{3}{c}{\textbf{70 (\%)}} & \multicolumn{1}{c}{\multirow{2}{*}{Time (s)}} \\
			\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{l}{Moderate} & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard} & \multicolumn{1}{l}{Moderate} & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard} & \multicolumn{1}{c}{}\\ \hline
			FQNet \cite{liu2019deep}  & Mono  & 1.51 & 2.77 & 1.01 &  3.23 & 5.40 & 2.46 & 0.50\\
			GS3D \cite{li2019gs3d} & Mono  & 2.90 & 4.47 & 2.47 &  6.08  & 8.41 & 4.94 & 2.0\\
			MVRA  \cite{choi2019multi} & Mono  & 3.27 & 5.19 & 2.49 &  5.84  &  9.05 & 4.50 & 0.18\\	
			Shift R-CNN \cite{naiden2019shift} &Mono  & 3.87 & 6.88 & 2.83 &  6.82  & 11.84 & 5.27 & 0.25\\
MonoGRNet \cite{qin2019monogrnet} &Mono &5.74  &9.61 &	4.25 &  11.17   & 18.19 & 8.73 & 0.04\\	
SMOKE \cite{liu2020smoke} &Mono  & 9.76 & 14.03 &  7.84 & 14.49 & 20.83  &  12.75 & 0.03\\
			MonoPair \cite{chen2020monopair} &Mono  & 9.99 & 13.04 &  8.65 &14.83 & 19.28  &  12.89  & 0.06\\
			RTM3D \cite{li2020rtm3d} & Mono  & 10.34 & 14.41 & 8.77 & 14.20 & 19.17  &  11.99 & 0.05\\
			\hline
			ROI-10D \cite{manhardt2019roi} &   & 2.02 & 4.32 &	1.46 &  4.91  & 9.78 &	3.74 & 0.20\\
			MonoFENet \cite{bao2019monofenet} &   &5.14 & 8.35 &	4.10 &  11.03  & 17.03 & 9.05 & 0.15\\
			Decoupled-3D \cite{cai2020monocular} &  & 7.02 & 11.08 &  5.63 &14.82 & 23.16  &11.25 & 0.08\\ 
			
			MonoPSR \cite{ku2019monocular} &   & 7.25 & 10.76 &  5.85 & 12.58 & 18.33  & 9.91 & 0.20\\
			AM3D \cite{ma2019accurate} & & 10.74 & 16.50 &  \B{9.52} & 17.32 & 25.03  &  \B{14.91} & 0.40\\
			RefinedMPL \cite{vianney2019refinedmpl} &    & 11.14& \textbf{18.09} & 8.94 & \B{17.60} &  \textbf{28.08} &  13.95 & 0.15\\ 
			D4LCN \cite{ding2020learning} &   & \B{11.72} & 16.65 &  9.51 & 16.02 & 22.51  &  12.55 & 0.20\\
		 \hline 
		    Baseline \cite{liu2020smoke} &Mono   & 9.76 & 14.03 &  7.84 & 14.49 & 20.83  &  12.75\\
			Proposed Method &Mono & \textbf{12.01} & \B{17.81} & \textbf{10.61}  &  \textbf{17.88} & \B{25.88} & \textbf{15.35} & 0.034\\ 
			Improvement & - & \textbf{\R{+2.25}} &  \textbf{\R{+3.78}} & \textbf{\R{+2.77}}  & \textbf{\R{+3.39}} & \textbf{\R{+5.05}} & \textbf{\R{+2.6}} \\
			\hline  
		\end{tabular}
	}
	\caption{\small Comparison with other public methods on the KITTI testing sever for 3D ``Car'' detection. For the ``direct'' methods, we represent the `` Modality'' with ``Mono'' only. For the other methods, we use ,  to indicate that the ``depth'' or ``3D model'' have been used by these methods during training or inference procedure. For each column, we have highlighted the top numbers in bold and the second best is shown in blue. The numbers in red represent the absolute improvements compared with the baseline.}
	\label{tab:test_kitti}
\end{table*}

\textbf{Comparison with SOTA methods:} we make our results public on the KITTI benchmark sever and the comparison with other methods are listed in \tabref{tab:test_kitti}. For fair comparison, the monocular-based methods can also be divided into two groups, which are illustrated in the top and middle rows of \tabref{tab:test_kitti}, respectively. The former is called the ``direct''-based method, which only uses a single image during the training and inference. In the latter type, other information such as depth or 3D model is used as an auxiliary during the training or inference. Our proposed method belongs to the former. 

Similar to the official benchmark website, all the results are displayed in ascending order based on the values of ``Moderate'' . From the table, we can find that the proposed method outperforms all the ``direct''-based method with a big margin among all the three categories. For example,  for ``Easy'' , our method achieved  points improvements than the best method of SMOKE \cite{liu2020smoke}. The minimum improvement happens on ``Moderate'' , even so, we also obtained  points of improvements than RTM3D \cite{li2020rtm3d}.   

Based on the evaluation criterion defined by KITTI (ranking based on values of ``Moderate''  ), our method achieved the first place among all the monocular-based 3D object detectors \footnote{Only these methods with publications have been listed for comparison here.} up to the submission of this manuscript (Jul. 8, 2020), including these models trained with depth or 3D models. Specifically, the proposed method achieved four first places, two second places among all the six sub-items. The run time of different methods is also provided in the last column of \tabref{tab:test_kitti}. Compared with other methods, we also show superiority on efficiency. By using the DAL34 as the backbone network, our methods can achieve 29 fps on Tesla V-100 with a resolution of .  

\textbf{Comparison with baseline:} from the table, we also can find that the proposed method significantly boosts the baseline method on both the BEV and 3D evaluation among all the six sub-items. Especially, for ``Easy'' category, we have achieved  and  points improvements for  and  respectively. For the other four sub-items, the proposed method achieves more  points improvement. The minimal improvement we have achieved is for ``Moderate'' , while it also provides  points of improvement.

\begin{table*}[ht!]
	\centering
	\resizebox{0.65\textwidth}{!}
	{\begin{tabular}{r| l ccc ccc}
			\hline
			\multicolumn{1}{c }{\multirow{2}{*}{Methods}}& \multicolumn{1}{c}{\multirow{2}{*}{Modality}} & \multicolumn{3}{c}{\textbf{70 (\%)}} & \multicolumn{3}{c}{\textbf{70 (\%)}}\\
			\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{l}{Mod } & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard} & \multicolumn{1}{l}{Mod} & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard}\\ \hline
			CenterNet \cite{zhou2019objects}  & Mono    & 1.06  & 0.86 & 0.66 &  3.23 & 4.46 & 3.53\\    
			Mono3D \cite{chen2016monocular}  & Mono  & 2.31 & 2.53 &	2.31  &  5.19 & 5.22 &	4.13\\   
			OFTNet  \cite{roddick2018orthographic} & Mono  & 3.27 &  4.07 & 3.29 &  8.79   & 11.06 & 8.91\\
			GS3D \cite{li2019gs3d} & Mono  & 10.51 & 11.63 & 10.51 &  - & - & - \\  
			MonoGRNet \cite{qin2019monogrnet} &Mono  & 10.19  & 13.88 &	7.62  &  -   & - & -\\ 
			ROI-10D \cite{manhardt2019roi} & Mono  & 6.63 &  9.61 &	6.29 &  9.91  & 14.50 &	 8.73 \\  
			MonoDIS \cite{simonelli2019disentangling} &Mono  & \B{14.98} & 18.05 & \B{13.42} & 18.43   &  \B{24.26} & 16.95\\  
			M3D-RPN \cite{brazil2019m3d} &Mono & \textbf{16.48} & \textbf{20.40} & 13.34 &  \B{21.15}   & \textbf{26.86} & \B{17.14}\\	 
				   
			\hline 
			Baseline \cite{liu2020smoke} &Mono & 12.85 & 14.76  & 11.50 & 15.61     & 19.99 & 15.28 \\
			Proposed &Mono & 14.96 & \B{18.95} & \textbf{14.84}  &  \B{19.60} & 22.75 & \textbf{19.21} \\  
			Improvements&  & \textbf{\R{+2.11}} & \textbf{\R{+4.19}} & \textbf{\R{+3.34}}  & \textbf{\R{+3.998}} & \textbf{\R{+2.76}} & \textbf{\R{+3.94}} \\ \hline 
		\end{tabular}   
	}
	\caption{\small Comparison with other public methods on the KITTI ``val'' split for 3D ``Car'' detection, where ``-`` represent the values are not provided in their papers. For easy understanding, we have highlighted the top number in bold for each column and the second best is shown in blue. The numbers in red represent the absolute improvements compared with the baseline.}
	\label{tab:val_kitti}
\end{table*}
\subsection{Evaluation on ``validation'' Split}
We also evaluate our proposed method on the validation split. The detailed comparison is given in \tabref{tab:val_kitti}.  As mentioned in \cite{wang2019pseudo}, the 200 training images of KITTI stereo 2015 overlap with the validation images of KITTI object detection. That is to say, some LiDAR point cloud in the object detection validation split has been used for training the depth estimation networks. That is the reason why some 3D object detectors (with depth for training) achieved very good performances while obtained unsatisfactory results on the test dataset. Therefore, we only list the ``direct''-based methods for comparison here. Compared with the baseline method, the proposed method achieves significant improvements among all the six sub-items. Especially, we achieve more than  points improvement in four items and the improvements for all the items are above  points. Comparison with other methods, we achieve \R{2} first places, \R{2} second places and \R{2} third places among all the 6 sub-items. 



\subsection{Ablation Studies}
In addition, we also have designed a set of ablation experiments to verify the effectiveness of each module of our proposed method. 
\begin{table*}[ht!]
	\centering
	\resizebox{0.5\textwidth}{!}
	{\begin{tabular}{r  ccc ccc}
			\hline
			\multicolumn{1}{c}{\multirow{2}{*}{Methods}}&   \multicolumn{3}{c}{\textbf{70 (\%)}} & \multicolumn{3}{c}{\textbf{70 (\%)}} \\
			\multicolumn{1}{c}{}     & \multicolumn{1}{l}{Mod } & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard} & \multicolumn{1}{l}{Mod}  & \multicolumn{1}{l}{Easy} & \multicolumn{1}{l}{Hard}\\ \hline
			Baseline         & 12.85 & 14.76 & 11.50 & 15.61 & 19.99 & 15.28 \\
			w/o supervision & 12.98 & 14.59 & 11.76 &  15.79 &20.12 & 14.98\\  
			w  supervision   & 14.96 & 18.95 & 14.84 & 19.60 & 22.75 & 19.21\\    
\hline 
		\end{tabular}   
	}
	\caption{\normalfont Ablation studies on the KITTI ``val'' split for 3D ``Car'' detection with/without instance mask supervision. From the table, we can easily find that the performances have been largely improved by adding the mask supervision signal.}
	\label{tab:mask_influence}
\end{table*}
\subsubsection{Supervision of the instance mask:} self-attention strategy is commonly used for in semantic segmentation \cite{liu2019deep}, \cite{fu2019dual} and object detection \cite{gu2018learning} etc. To highlight the influence of the supervision of the instance mask, we compare the performance of the proposed module with and without the supervision signal. From the table, we can easily found that the supervision signal is particularly useful for training IAFA module. Without the supervision, the detection performance nearly unchanged. The positive effect of the instance supervision signal is obvious. Furthermore, we also illustrated some examples of the learned attention maps in \figref{fig:Example_of_AttentionMap}, where the bottom sub-figures are the corresponding attention maps for the three instances respectively. From the figure, we can see that the maximum value is at the center of object and it decreases gradually with the increasing of its distance to the object center. 
\begin{figure}[ht!]
 	\centering
 	\includegraphics[width=0.995\textwidth]{Example_of_AttentionMap.pdf}
 	\caption{An example of feature map for IAFA module. Different brightness reveals different importance related to the target point (red dot). }
 	\label{fig:Example_of_AttentionMap}
 \end{figure}
\subsection{Qualitative Results}
Some qualitative detection results on the test split are displayed in \figref{fig:qualitative_results_1}. For better visualization and comparison, we also draw the 3D BBoxes in point cloud and BEV-view images. The results clearly demonstrate that the proposed framework can recover objects' 3D information accurately. 
 \begin{figure}[ht!]
 	\centering
 	\includegraphics[width=0.995\textwidth]{QualitativeRes.pdf}
 	\caption{Three examples of 3D detection results. The ``top'', ``Middle'' and ``Bottom`` are results are drawn in RGB image, 3D Point cloud and BEV-view respectively. The point cloud is only used for visualization purposes here. }
 	\label{fig:qualitative_results_1}
 \end{figure}


%
