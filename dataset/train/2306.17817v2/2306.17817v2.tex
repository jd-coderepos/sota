\vspace{-0.05in}
\section{Experiments}
\label{sec:experiments}
\begin{figure}
\begin{adjustbox}{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{final_figures/tasks}
    \end{adjustbox}
    \caption{\textbf{Tasks.} We conduct experiments on 92 simulated tasks in RLBench \cite{james2020rlbench} (only 10 shown), and 8 real-world tasks (only 5 shown). }
    \label{fig:tasks}
\end{figure}

We test \model{} in learning from demonstrations single-task and multi-task manipulation policies in simulation and the real world. 
We conduct our simulated experiments in  RLBench~\cite{james2020rlbench}, an established simulation benchmark for learning manipulation policies, for the sake of reproducibility and benchmarking. 
Our experiments aim to answer the following questions:

\textbf{1.} How does \model{} compare against SOTA 2D multiview and 3D manipulation policies in  single-task and multi-task settings with varying number of training demonstrations?

\textbf{2.} How does \model{} generalize across camera viewpoints compared to prior 2D multiview policies?

\textbf{3.} How do design choices such as relative 3D attention, pre-trained 2D backbones, weight-tied attention layers, and the number of coarse-to-fine sampling stages impact performance?





\begin{figure}[t!]
\begin{adjustbox}{center}
    \includegraphics[width=1.1\textwidth,keepaspectratio]{final_figures/results_single_task}
    \end{adjustbox}
    \caption{\textbf{Single-task performance.} On 74 RLBench tasks across 9 categories, \model{}~reaches 83\% success rate, an absolute improvement of 10\% over InstructRL~\cite{liu2022instruction}, prior SOTA in this setting.}
    \label{fig:results_single_task}
\vspace{0.5cm}
     \begin{adjustbox}{center}
     \begin{subfigure}[c]{0.35\textwidth}
         \includegraphics[width=0.6\textwidth]{final_figures/results_multi_task}
     \end{subfigure}
     \hspace{-60pt}
     \begin{subfigure}[c]{0.8\textwidth}
         \includegraphics[width=\textwidth]{final_figures/results_multi_task_table}
     \end{subfigure}
    \end{adjustbox}
     \caption{\textbf{Multi-task performance.} On 18 RLBench tasks with 249 variations, \model{} reaches 65\% success rate, an absolute improvement of 22\% over PerAct~\cite{shridhar2023perceiver}, prior SOTA in this setting.}
     \label{fig:multi_task_results}
\end{figure}
\subsection{Evaluation in simulation}


\paragraph{Datasets}  We test \model{} in RLbench~in two settings: \textbf{1.} \textbf{Single-task} manipulation policy learning. We consider 74 tasks grouped into 9 categories proposed by HiveFormer~\cite{guhur2023instruction}. Each task includes   variations which test generalization  
to novel arrangements of the same training objects.
Each method is trained with 100 demonstrations and evaluated on 500 unseen episodes.  \textbf{2.}  \textbf{Multi-task} manipulation policy learning. We consider 18 tasks with 249 variations proposed by PerAct~\cite{shridhar2023perceiver}.
Each task includes  2-60 variations, which test generalization  to new goal configurations that involve  novel object colors, shapes, sizes, and categories. This is a more challenging setting. Each method is trained with 100 demonstrations per task split across variations, and evaluated on 500 unseen episodes per task.


\paragraph{Baselines} We compare \model{}~with the following state-of-the-art manipulation policy learning methods: 
\textbf{1.} InstructRL~\cite{liu2022instruction}, a 2D policy that directly predicts 6 DoF poses from image and language conditioning with a pre-trained vision-and-language backbone. 
\textbf{2.}  PerAct~\cite{shridhar2023perceiver},  a 3D policy that voxelizes the workspace and detects the next best voxel action through global self-attention. 
\textbf{3.} HiveFormer~\cite{guhur2023instruction} and Auto-$\lambda$~\cite{liu2022auto}, hybrid methods that detect a contact point within an image input, then regress an offset from this contact point. We report numbers from the papers when available.

\paragraph{Evaluation metric} 
We evaluate policies by task completion success rate, the proportion of execution trajectories that lead to goal conditions specified in language instructions.

\paragraph{Single-task and multi-task manipulation results} 
We show single-task quantitative results of our model and baselines in Figure~\ref{fig:results_single_task}. 
\model{} \textbf{reaches 83\% success rate, an absolute improvement of 10\% over InstructRL~\cite{liu2022instruction}, prior SOTA in this setting}, and consistently outperforms it across all 9 categories of tasks.
With only 10 demonstrations per task, \model{} is competitive with prior SOTA using 100 demonstrations per task. 
Act3D outperforms 2D methods of InstructRL and Hiveformer because it reasons directly  in 3D. 
For the same reason, it generalizes much better than them to novel camera placements, as we show in Table \ref{table:ablations}. 














We show multi-task quantitative results of our model and PerAct in Figure~\ref{fig:multi_task_results}.
\model{} reaches 65\% success rate, an absolute improvement of 22\% over PerAct, prior SOTA in this setting, consistently outperforming it across most tasks. 
\textbf{With only 10 demonstrations per task, \model{} outperforms PerAct using 100 demonstrations per task.}
Note that \model{} also uses less than a third of PerAct's training computation budget: PerAct was trained for 16 days on 8 Nvidia V100 GPUs while we train for 5 days on the same hardware. Act3D outperforms PerAct because its coarse-to-fine relative attention based 3D featurization of the 3D workspace is more effective than the perceiverâ€™s latent bottleneck attention in generating spatially  disentangled features. 


\subsection{Ablations}
\label{sec:ablations}


We ablate the impact of our design choices in Table~\ref{table:ablations}. We perform most ablations in the single-task setting on 5 tasks: pick cup, put knife on chopping board, put money in safe, slide block to target, take umbrella out of stand. We ablate the choice of pre-trained 2D backbone in the multi-task setting with all 18 tasks.




\begin{table}[]
\caption{\textbf{Ablations.}}
\label{table:ablations}
\small
\centering
\begin{tabular}{llc}
\toprule
\multicolumn{1}{c}{}                        & \multicolumn{1}{c}{}                               & Average success rate in       \\
\multicolumn{1}{c}{}                        & \multicolumn{1}{l}{}                          & single-task setting (5 tasks) \\ \midrule
\multirow{6}{*}{Core design choices}        & Full Act3D   & \textbf{98.1}          \\
                                            & Only 2 stages of coarse-to-fine sampling          & 93.6         \\
& No weight tying across stages                      & 80.6                          \\
                                            & Absolute 3D positional embeddings                  & 55.4                          \\
                                            & Attention to only global coarse visual features    & 89.8              \\
                                            & Only 1000 ghost points at inference time           & 93.2                          \\ \midrule
\multirow{2}{*}{Viewpoint changes}          & Act3D  & \textbf{74.2}      \\
                                            & HiveFormer                                         & 20.4                          \\ \midrule


&     & Multi-task setting (18 tasks) \\ \midrule
\multirow{3}{*}{Backbone}                   & CLIP ResNet50 backbone                             & \textbf{65.1}                 \\
                                            & ImageNet ResNet50 backbone                         & 53.4                          \\


\end{tabular}
\end{table}



\paragraph{Generalization across camera viewpoints:}  We vary camera viewpoints at test time for both \model{} and HiveFormer~\cite{guhur2023instruction}.  The success rate drops to 20.4\% for HiveFormer, a relative 77\% drop, while \model{} achieves 74.2\% success rate, a 24\% relative drop. This shows detecting actions in 3D makes \model{} more robust to camera viewpoint changes than multiview 2D methods that regress offsets. 

\paragraph{Weight-tying and coarse-to-fine sampling:} All 3 stages of coarse-to-fine sampling are necessary: a model with only 2 stages of sampling and regressing an offset from the position selected at the second stage suffers a 4.5\% performance drop. Tying weights across stages and relative 3D positional embeddings are both crucial; we observed severe overfitting without, reflected in respective 17.5\% and 42.7\% performance drops.
Fine ghost point sampling stages should attend to local fine visual features with precise positions: all stages attending to global coarse features leads to a 8.3\% performance drop. 
\model{} can effectively trade off inference computation for performance: sampling 10,000 ghost points, instead of the 1,000 the model was trained with, boosts performance by 4.9\%.


\paragraph{Pre-training 2D features:} We investigate the effect of the pre-trained 2D backbone in the multi-task setting where language instructions are most needed. A ResNet50~\cite{radford2021learning} backbone pre-trained with CLIP improves success rate by 8.7\% over a ResNet50 backbone pre-trained on ImageNet. 



We found Random crops of RGB-D images to boost performance but yaw rotation perturbations did not help. The model  is robust to variations in hyperparameters such as the diameter of ghost point sampling balls  or the number of points sampled during training. For additional ablations regarding augmentations and sensitivity to hyperparameters, please see the Appendix section \ref{sec:ablateappendix}. 


\subsection{Evaluation in real-world}

\begin{wraptable}{r}{0.35\textwidth}
  \setlength\tabcolsep{2.3pt}
  \centering
\begin{tabular}{lcc} 
\toprule
Task          & \# Train~  & Success  \\ 
\midrule
reach target &10        & 10/10        \\
duck in oven &15           & 6/10        \\
wipe coffee   &15           & 7/10        \\
fruits in bowl & 10          & 8/10        \\
stack cups & 15            & 6/10        \\
transfer beans  & 15           & 5/10        \\
press handsan  & 10            & 10/10        \\
uncrew cap   & 10              & 8/10        \\
\bottomrule
\end{tabular}
    \caption{Real-world tasks.}
\label{tab:real}
\end{wraptable}In our real-world setup, we conduct experiments with a Franka Emika Panda robot and a single Azure Kinect RGB-D sensor. We consider 8 tasks (Figure \ref{fig:tasks}) that involve interactions with multiple types of objects, spanning liquid, articulated objects, and deformable objects. 
For each task, we collected 10 to 15 kinesthetic demonstrations
and trained a languaged-conditioned multi-task model with all of them. 
We report the success rate on 10 episodes per task in Table \ref{tab:real}. 
\model{} can capture semantic knowledge in demonstration well and performs reasonably well on all tasks, even with a single camera input. 
One major failure case comes from noisy depth sensing: when the depth image is not accurate, the selected point results in imprecise action prediction. 
Leveraging multi-view input for error correction could improve this, and we leave this for future work. For   videos of the robot executing the tasks, please see our project website.





\subsection{Limitations and future work}
Our framework currently has the following limitations: 
\textbf{1.} 
\model{} is limited by the motion planner used to connect predicted  keyposes with straight trajectory segments. It does not handle manipulation of articulated object  well, such as opening/closing doors, fridges, and ovens, where robot trajectories cannot be well approximated by few line segments.\textbf{2.} 
\model{}  does not utilize any  decomposition of tasks into subtasks.
A hierarchical framework that would predict language subgoals for subtasks~\cite{ahn2022can, huang2023grounded, lin2023text2motion} and feed those to our language-conditioned policy would allow better re-usability of skills across tasks. Addressing these limitations is a direct avenue for future work. 





