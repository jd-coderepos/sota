\documentclass[11pt]{article}



\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{6in}
\setlength{\textheight}{8in}
\setlength{\topmargin}{-0.0in}




\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amssymb,amsfonts,textcomp}
\usepackage{latexsym}
\usepackage[noend,ruled]{algorithm2e} 

\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{rotating}

\newcommand{\fixlist}{\addtolength{\itemsep}{-3pt}}


\newcommand{\reals}{{{\mathbb{R}}}}
\newcommand{\capacity}{{{{\mathrm{cap}}}}}
\newcommand{\cost}{{{{\mathrm{cost}}}}}
\newcommand{\pos}{{{{\mathrm{pos}}}}}
\newcommand{\bordainc}{{{{\mathrm{DB}}}}}
\newcommand{\bordadec}{{{{\mathrm{SB}}}}}
\newcommand{\OPT}{{{{\mathrm{OPT}}}}}
\newcommand{\opt}{{{{\mathrm{opt}}}}}
\newcommand{\sat}{{{{\mathrm{sat}}}}}
\newcommand{\E}{\mathop{\mathbb E}}
\newcommand{\w}{{{{\mathrm{W}}}}}
\newcommand{\qed}{{{{}}}}
\newcommand{\probability}{{{\mathrm{P}}}}

\newcommand{\Ev}{{\mathit{Ev}}}
\newcommand{\partialsol}{{\mathit{partial}}}
\newcommand{\best}{{\mathit{best}}}
\newcommand{\sol}{{\mathit{sol}}}
\newcommand{\hit}{{\mathit{hit}}}
\newcommand{\poly}{{\mathrm{poly}}}

\newcommand{\argmax}{{{{\mathrm{argmax}}}}}
\newcommand{\argmin}{{{{\mathrm{argmin}}}}}

\newcommand{\score}{{{\mathit{score}}}}
\newcommand{\quality}{{{\mathit{quality}}}}
\newcommand{\Cov}{{{\mathit{Cov}}}}
\newcommand{\uCov}{{{\mathit{uCov}}}}



\hyphenation{assign-ment}

\title{Approximating the MaxCover Problem with Bounded Frequencies in
  FPT Time}

\author{Piotr Skowron\\ 
        University of Warsaw\\
        Warsaw, Poland\\
        \and
        Piotr Faliszewski\\
        AGH University\\
        Krakow, Poland
}

\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}[theorem]{Proposition}

\newenvironment{proof}{\paragraph{Proof}}{\hfill\medskip}

\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}

\newcommand{\np}{{\mathrm{NP}}}
\newcommand{\fpt}{{\mathrm{FPT}}}
\newcommand{\wclass}{{\mathrm{W}}}
\newcommand{\wone}{{\mathrm{W[1]}}}
\newcommand{\wtwo}{{\mathrm{W[2]}}}
\newcommand{\wpclass}{{\mathrm{W[P]}}}
\newcommand{\p}{{\mathrm{P}}}
\newcommand{\integers}{{{\mathbb{Z}}}}
\newcommand{\naturals}{{{\mathbb{N}}}}
\newcommand{\realsplus}{{{\mathbb{R}}}_{+}}
\newcommand{\calI}{{{\mathcal{I}}}}
\newcommand{\calC}{{{\mathcal{C}}}}
\newcommand{\calA}{{{\mathcal{A}}}}
\newcommand{\calL}{{{\mathcal{L}}}}
\newcommand{\calP}{{{\mathcal{P}}}}
\newcommand{\calF}{{{\mathcal{F}}}}
\newcommand{\calS}{{{\mathcal{S}}}}
\newcommand{\owa}{{{\pmb{\alpha}}}}
\newcommand{\owab}{{{\pmb{\beta}}}}
\newcommand{\owag}{{{\pmb{\gamma}}}}






\allowdisplaybreaks

\begin{document}

\maketitle


\begin{abstract}
  We study approximation algorithms for several variants of the
  MaxCover problem, with the focus on algorithms that run in FPT time.
  In the MaxCover problem we are given a set  of elements, a family
   of subsets of , and an integer . The goal is to find
  up to  sets from  that jointly cover (i.e., include) as
  many elements as possible. This problem is well-known to be
  -hard and, under standard complexity-theoretic assumptions, the
  best possible polynomial-time approximation algorithm has
  approximation ratio .  We first consider a variant
  of MaxCover with bounded element frequencies, i.e., a variant where
  there is a constant  such that each element belongs to at most
   sets in .  For this case we show that there is an FPT
  approximation scheme (i.e., for each  there is a -approximation
  algorithm running in FPT time) for the problem of maximizing the number of
  covered elements, and a randomized FPT approximation scheme for the
  problem of minimizing the number of elements left uncovered (we take
   to be the parameter). Then, for the case where there is a
  constant  such that each element belongs to at least  sets
  from , we show that the standard greedy approximation
  algorithm achieves approximation ratio exactly
  . We conclude by considering an unrestricted
  variant of MaxCover, and show approximation algorithms that run in
  exponential time and combine an exact algorithm with a greedy
  approximation. Some of our results improve currently known results for
  MaxVertexCover.
\end{abstract}


\section{Introduction}

We study approximation algorithms for, and parametrized complexity of,
the MaxCover problem with bounded frequency of the elements. In the
MaxCover problem we are given a set  of  elements, a family
 of  subsets of , and an integer
. The goal is to find a size-at-most- subcollection of 
that covers as many elements from  as possible. In the variant with
bounded frequencies of elements we further assume that there is some
constant  such that each element appears in at most  sets. A
particularly well-known special case of MaxCover with frequencies
upper-bounded by  is the MaxVertexCover problem: We are given a
graph  and the goal is to find  vertices that, jointly,
are incident to as many edges as possible (i.e., the edges are the
elements to be covered and the vertices are the sets; clearly, each
edge ``belongs to'' exactly two vertices). Nonetheless, even for the
frequency upper bound , MaxCover is considerably more general than
MaxVertexCover (e.g., the former allows two sets to have more than one
element in common, which is impossible in the latter\footnote{This
  difference may not sound particularly significant, but due to it
  some algorithms for MaxVertexCover (e.g., an FPT approximation
  scheme of Marx~\cite{Marx06parameterizedcomplexity}) do not
  generalize easily to the MaxCover setting.}).
In addition to MaxCover with upper-bounded frequencies, we also study
a variant of the problem with lower-bounded frequencies, and the
general variant, without any restrictions on element frequencies.

Our paper differs from the typical approach to the design of
approximation algorithms in that we do not focus on polynomial-time
algorithms, but also consider exponential-time ones.  For example, we
are interested in FPT approximation schemes, that is, in approximation
algorithms that for each desired approximation ratio  output a
-approximate solution in exponential time, but where the
exponential growth is only with respect to the number  of sets that
we allow in the solution (and where  is considered to be a
constant when computing the running time). In that respect, our work
is very close in spirit to the recent study of Croce and
Paschos~\cite{cro-pas:j:cover}, who---among other results---give
moderately exponential time (but not FPT-time) approximation schemes
for the MaxVertexCover problem. (However, there is also an FPT-time
approximation scheme for MaxVertexCover due to
Marx~\cite{Marx06parameterizedcomplexity}.)  Such exponential-time
approximation algorithms are desirable because they can achieve much
better approximation ratios than the polynomial-time ones, while still
being significantly faster than the currently-known exact
algorithms. We give more detailed review of related work in
Section~\ref{sec:related} and below we briefly describe our findings
and the motivation behind our research.


We obtain the following results (unless we mention otherwise, we
always consider our problems to be parametrized by , the number of
the sets allowed in the solution).  First, building on the approach of
Guo et al.~\cite{guo-nie-wer:j:vertex-cover-variants}, in
Section~\ref{sec:worst-case} we show that the MaxCover problem with
bounded frequencies is -complete.
On the other hand, without the frequency upper-bound assumption,
MaxCover is -hard and we show that it belongs to . We
also consider several other parameters and, in particular, we show
that MaxCover is -complete for the parameter that combines the
number of sets we can use in the solution and the number of elements
that we are allowed to leave uncovered.  The core of the paper is,
however, in Section~\ref{sec:approximation}.  There, we show that for
each , , there is an FPT -approximation
algorithm for the MaxCover problem with bounded frequencies.  On the
other hand, for the case where each element appears in \emph{at least}
 out of  sets, we show that the standard MaxCover greedy
approximation algorithm (i.e., one that picks one-by-one those sets
that include most not-yet-covered elements) achieves approximation
ratio  (for the general case, this algorithm's
approximation ratio is ). Finally, we consider a
variant of the MaxCover problem where instead of maximizing the number
of covered elements, we minimize the number of those that remain
uncovered. We refer to this problem as the MinNonCovered problem.
Under the assumption of upper-bounded frequencies, we show a
randomized approximation algorithm that for each given , , and each given probability , outputs in FPT time a
-approximate solution with probability at least 
(the FPT time is with respect to , , and ).
Finally, in Section~\ref{sec:unrestricted} we consider two
exponential-time approximation algorithms for the unrestricted
MaxCover problem. Both of these algorithms solve a part of the problem
in a greedy way and a part using some exact algorithm, but they differ
in the order in which they apply each of these strategies. We show a
smooth transition between the running times of these algorithms and their
approximation ratios.



\subsection{Motivation}

We believe that the MaxCover problem with bounded frequencies is
an interesting and important problem on its own. However, the
particular reason why we study it is due to its connection to
winner-determination under Chamberlin--Courant's voting rule. Under
the Chamberlin--Courant's rule, a society of  voters chooses from a
group of  candidates a committee of  representatives. The rule
was originally proposed as a mean of electing
parliaments~\cite{ccElection}, but recently Boutilier and
Lu~\cite{budgetSocialChoice} pointed out that it might be very useful
in the context of recommendation systems and Skowron et
al.~\cite{sko-fal-sli:c:multiwinner} showed its connection to 
resource allocation problems.

There are many variants of the rule, depending on the so-called
misrepresentation function that it uses. Here, we will focus on the
approval variant, though we mention that perhaps the best-studied one
(though not necessarily the most practical one) is the variant that
uses the Borda misrepresentation function (we omit the details of
Borda misrepresentation here and point the reader to the original
paper defining the rule~\cite{ccElection}).

In the approval-based variant of Chamberlin--Courant's rule, the
voters submit ballots on which they list all the candidates that they
find acceptable as their representatives (that is, the candidates that
they approve of). For each size- subset  of candidates (referred
to as a \emph{committee}), the misrepresentation score of  is the
total number of voters who do not approve of any of the candidates in
. Chamberlin--Courant's rule elects a committee  that minimizes
the misrepresentation.  Naturally, there may be several committees
that minimize the misrepresentation and in practice one has to apply
some tie-breaking. In the computational studies of voting researchers
are usually interested in finding any such a committee and so do we.

The above description makes clear the connection between
approval-based Chamberlin--Courant's rule and the MaxCover problem:
The voters are the elements that need to be covered, the candidates
are the sets (a voter  belongs to the set defined by some candidate
 if  approves of ), and the size of the committee is the
number of sets one can pick. The achieved misrepresentation is the
number of uncovered elements.

Given this connection, clearly winner determination under
Chamberlin--Courant's rule is an -hard problem for the approval
misrepresentation~\cite{complexityProportionalRepr} (it also is for
Borda misrepresentation~\cite{budgetSocialChoice}). Further, in both
cases the problem is -hard, as shown by Betzler et
al.~\cite{fullyProportionalRepr}.  Thus if one wants to find the exact
winning committee, one is restricted to exponential time algorithms,
such as, e.g., solving a particular integer linear
program~\cite{potthoff-brams} or trying all possible committees.  On
the other hand, at least for the Borda misrepresentation function, the
problem is quite easy to approximate, both theoretically (there is a
polynomial-time approximation scheme due to Skowron et
al.~\cite{sko-fal-sli:c:multiwinner}) and in practice (as shown by
experiments~\cite{sko-fal-sli:c:monroe-cc-experimental}).
Unfortunately, the connection between the approval variant of the rule
and the MaxCover problem severely limits approximation possibilities:
In terms of polynomial-time algorithms the best we can get is the
standard greedy -approximation algorithm.


Yet, in practical elections it is somewhat unreasonable to expect that
each voter would list many candidates as approved.  Indeed, in some
political systems that use approval-like ballots, even the law itself
limits the number of candidates one can list (for example, in Polish
parliamentary elections the voters can list up to three
candidates). Thus it is most natural to consider the approval variant
of Chamberlin--Courant's rule for the case where each voter can
approve of at most a given number  of the candidates. This variant
of the rule directly corresponds to the MaxCover problem with bounded
frequencies. On the other hand, it is also natural to consider
settings were voters are required to approve of at least a given
number of candidates (such requirement can, for example, be imposed by
the election rules). This corresponds to the MaxCover problem were
elements' frequencies are \emph{lower bounded} by some value.


In effect, our results on the MaxCover problem with bounded
frequencies fill in the hole between efficient approximation
algorithms for the Borda variant of Chamberlin--Courant's rule given
by Skowron et al.~\cite{sko-fal-sli:c:multiwinner} and
-hardness results of Betzler et
al.~\cite{fullyProportionalRepr} for the general, unrestricted
approval variant of the rule.
























\section{Preliminaries}\label{sec:prelims}

We assume that the reader is familiar with standard notions regarding
(approximation) algorithms, computational complexity theory and
parametrized complexity theory. Below we provide a very brief review.
For each positive integer , we write  to mean . 

Let  be an algorithmic problem where, given some instance ,
the goal is to find a solution  that maximizes a certain function
.  Given an instance  of , we refer to the value 
of an optimal solution  as  (or, sometimes, simply as
 if the instance  is clear from the context). Let , , be some fixed constant.  An algorithm  that
given instance  returns a solution  such that  is called a -approximation algorithm for the
problem .
Analogously, we define  and the notion of a
-approximation algorithm, , for the case of a
problem , where the task is to find a solution that minimizes
a given goal function . (Specifically, given an instance  of
, a -approximation algorithm is required to return a
solution  such that ).  Given instance 
of some algorithmic problem, we write  to denote the length of
the standard, efficient encoding of .


In this paper we focus on the following two problems.
\begin{definition}
  An instance  of the MaxCover problem consists of a
  set  of  elements, a collection 
  of  subsets of , and nonnegative integer . The goal is to
  find a subcollection  of  of size at most  that
  maximizes .
\end{definition}

\begin{definition}
  The MinNonCovered problem is defined in the same way as the MaxCover
  problem, except the goal is to find a subcollection  such
  that  is minimal.
\end{definition}
In the decision variant of MaxCover (of MinNonCovered) we are
additionally given an integer  (an integer ) and we ask if
there is a collection of up to  sets from  that cover at
least  elements (that leave at most  elements
uncovered). MaxVertexCover is a variant of MaxCover where we are given
a graph , the edges are the elements to be covered, and
vertices define the sets that cover them (a vertex covers all the
incident edges). SetCover and VertexCover are variants of MaxCover and
MaxVertexCover, respectively, where we ask if it is possible to cover
all the elements (all the edges).


In terms of the optimal solutions, MaxCover and MinNonCovered are
equivalent. None\-theless, they do differ when considered from the
point of view of approximation. For example, if there were a solution
that covered all the  elements, then a -approximation
algorithm for MaxCover, , would be free to return a
solution that covered only  of them, but a
-approximation algorithm for the MinNonCovered problem,
, would have to provide an optimal solution that covered
all the elements.

Given an instance  of MaxCover (MinNonCovered), we say that an
element  has frequency  if it appears in exactly  sets.  We
mostly focus on the variants of MaxCover and MinNonCovered where there
is a given constant  such that each element's frequency is at most
. We refer to these problems as variants with bounded frequencies.

We will focus on (approximation) algorithms that run in FPT time (see
the books of Downey and Fellows~\cite{dow-fel:b:parameterized},
Niedermeier~\cite{nie:b:invitation-fpt}, and Flum and
Grohe~\cite{flu-gro:b:parameterized-complexity} for details on
parametrized complexity theory). To speak of an FPT algorithm for a
given problem, we declare a part of the problem as the so-called
parameter. Here, for MaxCover and MinNonCovered problems, we take the
parameter to be the number  of sets that we are allowed to use in
the solution (in Section~\ref{sec:worst-case} we briefly consider
MaxCover/MinNonCovered with parameters , , and their
combinations with ). Given an instance  of a problem with
parameter , an FPT algorithm is required to run in time
, where  is some computable function and  is
some polynomial.

From the point of view of parametrized complexity, FPT is seen as the
class of tractable problems. There is also a whole hierarchy of
hardness classes, . The standard definitions of  are quite involved and so we point the reader to
appropriate
overviews~\cite{dow-fel:b:parameterized,nie:b:invitation-fpt,flu-gro:b:parameterized-complexity}.
However, we can also define these classes through an appropriate
reduction notion and their complete problems.

\begin{definition}
  Let  and  be two decision problems
  parametrized by real non-negative parameters  and
  , respectively. We say that  reduces to
   through a parametrized reduction if there exist a
  mapping  (computable
  in FPT time with respect to parameter ) and two
  computable functions, 
  and , such that (i) for each
  instance  the answer to  is ``yes''
  if and only if the answer to  is ``yes'', (ii) 
  and  are the values of the parameters  and
   respectively, (iii) ,
  and (iv) .
\end{definition}

 is the class of all problems for which there is a parametrized
reduction to the Clique problem (i.e., the problem where we ask if a
given graph  has a clique of size at least , where 
is the parameter).  is the class of problems with parametrized
reductions to SetCover (with parameter ).  Interestingly,
VertexCover is well-known to be in FPT, but MaxVertexCover is
-complete~\cite{guo-nie-wer:j:vertex-cover-variants}.

One of the standard ways of showing -membership is to give a
reduction to the
Short-Non\-de\-ter\-ministic-Turing-Machi\-ne-Com\-putation problem
(shown to be -complete for parameter  by
Cesati~\cite{ces:j:turing-way-parameterized-complexity}).

\begin{definition}
  In the Short-Non\-de\-ter\-ministic-Turing-Machi\-ne-Com\-putation
  problem we are given a single-tape nondeterministic Turing machine
   (described as a tuple including the input alphabet, the work
  alphabet, the set of states, the transiation function, the initial
  state and the accepting/rejecting states), a string  over 's
  input alphabet, and an integer . The question is whether there is
  an accepting computation of  that accepts  within  steps.
\end{definition}


The Bounded-Non\-de\-ter\-ministic-Turing-Machi\-ne-Com\-putation
problem is defined similarly, but in addition we are also given an
integer , and we ask if  accepts its input within  steps, of
which at most  are nondeterministic. Cesati has shown that this
problem is
-complete~\cite{ces:j:turing-way-parameterized-complexity}
(we omit the exact definition of ; the reader can think of
 as the set of problems that have parameterized reductions
to the Bounded-Non\-de\-ter\-ministic-Turing-Machi\-ne-Com\-putation
problem).








\section{Related Work}\label{sec:related}

There is extensive literature on the complexity and approximation
algorithms for the SetCover and VertexCover problems.
On the other hand, the literature on MaxCover and MaxVertexCover is
more scarce
The literature on MaxCover with bounded frequencies of the elements is
scarcer yet. Below we survey some of the known results.


First, it is immediate that MaxCover, MinNonCovered, and
MaxVertexCover are -complete (this follows immediately from the
-completeness of SetCover and VertexCover). In terms of
approximation, a greedy algorithm that iteratively picks sets that
cover the largest number of yet uncovered elements achieves the
approximation ratio , and this is optimal unless  (see, e.g., the textbook~\cite{hoc:b:covers} for the analysis of
the greedy algorithm and the work of Feige~\cite{fei:j:cover} for the
approximation lower bound). However, our focus is on the MaxCover
problem with bounded frequencies and this problem is, in spirit,
closer to MaxVertexCover than to the general MaxCover problem. Indeed,
MaxVertexCover can be seen as a special case of MaxCover with
frequencies bounded by . However, we stress that even MaxCover with
frequencies bounded by  is considerably more general than
MaxVertexCover and, compared to MaxVertexCover, may require different
algorithmic insights.

As far as we know, the best polynomial-time approximation algorithm
for MaxVertexCover is due to Ageev and
Sviridenko~\cite{age-svi:b:covers}, and achieves approximation ratio
of . However, in various settings, it is possible to
achieve better results; we mention the papers of Han et
al.~\cite{han-ye-zha-zha:j:cover} and of Galluccio and
Nobili~\cite{gal-nob:j:cover} as examples.

From the point of view of parametrized complexity, MaxVertexCover was
first considered by Guo et
al.~\cite{guo-nie-wer:j:vertex-cover-variants}, who have shown that it
is -complete. The problem was also studied by
Cai~\cite{cai:j:cardinality-constrained} who gave the currently best
exact algorithm for it and by Marx, who gave an FPT approximation
scheme~\cite{Marx06parameterizedcomplexity}. There is also an FPT
algorithm for MaxCover , for parameter , i.e., the number of
elements to cover, due to Bl{\"a}ser~\cite{bla:j:partial-set-cover}.

In our paper, we attempt to merge parametrized study of MaxCover with
its study from the point of view of approximation algorithms.  In that
respect, our work is very close in spirit that of Croce and
Paschos~\cite{cro-pas:j:cover}, who provide moderately exponential
approximation algorithms for MaxVertexCover, and to the work of
Marx~\cite{Marx06parameterizedcomplexity}.  Compared to their results,
we consider a more general problem, MaxCover (with or without bounded
frequencies) and, as far as it is possible, we seek algorithms that
run in FPT time (the algorithm of Croce and Paschos is not
FPT). Interestingly, even though we focus on a more general problem,
our algorithms improve upon the results of Croce and
Paschos~\cite{cro-pas:j:cover} and of
Marx~\cite{Marx06parameterizedcomplexity}, even when applied to
MaxVertexCover
.








\section{Worst-Case Complexity Results}\label{sec:worst-case}

We start our parametrized study of the MaxCover problem by
considering its worst-case complexity. We first consider MaxCover with
bounded frequencies. It follows directly from the literature that the
problem is -hard, and here we show that it is, in fact,
-complete (unless the frequency bound  is exactly ; then
it is optimal to simply pick the sets with highest cardinalities).

\begin{theorem}
  For each constant  greater than , the MaxCover problem with
  frequencies upper-bounded by  is -complete (when
  parametrized by the number of sets in the solution).
\end{theorem}
\begin{proof}
  The hardness follows directly from the -hardness of the
  MaxVertexCover problem~\cite{guo-nie-wer:j:vertex-cover-variants}.
  We prove membership in  by reducing MaxCover with bounded
  frequencies to the Short-Nondeterministic-Turing-Machine-Computation
  problem.

  Let  be some fixed constant and let  be our
  input instance, where  is a set of elements,  is a family of subsets of  (each element from 
  appears in at most  sets from ), and  and  are two
  integers. This is the decision variant of the problem, thus we have
   in the input; we ask if there is a collection of up to  sets
  from  that jointly cover at least  elements.  W.l.o.g., we
  assume that . We form single-tape nondeterministic Turing
  machine  to execute the following algorithm (on empty input
  string); the idea of the algorithm is to employ the standard
  inclusion-exclusion principle:
  \begin{enumerate}
  \item Guess the indices  of  sets from .
  \item Set .
  \item For each subset  of  of size up to
    , do the following: If  is odd, add  to , and otherwise subtract  from .
  \item If  then we accept and otherwise we reject.
  \end{enumerate}
  It is easy to see that this algorithm can indeed be implemented on a
  single-tape nondeterministic Turing machine with a sufficiently
  large (but polynomially bounded) work alphabet and state space.  The
  only issue that might require a comment is the computation of
  . Since sets  contain at most 
  elements, we can precompute these values and store them in 's
  transition function.

  The correctness of the algorithm follows directly from the inclusion-exclusion
  principle and the fact that each element appears in at most  sets: 

In general, the above formula should include intersections of up to
   sets. However, since in our case each element appears in at most
   sets, the intersection of more than  sets are always
  empty. This shows that the algorithm is correct and concludes the
  proof.~
\end{proof}

For the sake of completeness, we mention that both the unrestricted
variant of the problem and the one where we put a lower bound on each
element's frequency are -hard.

\begin{theorem}
  For each constant , , MaxCover where each element
  belongs to at least  sets if -hard.
\end{theorem}
\begin{proof}
  To show -hardness, we give a reduction from SetCover. In the SetCover
  problem we ask whether there exist  subsets that cover all the elements
  (we give a reduction for the parameter ).
  Let  be an input instance of
  SetCover.  W.l.o.g., we can assume that each element from 
  belongs to at least one set in . We form an instance  of
  MaxCover which is identical to , except (a) for each ,
  we modify  to  additionally include  copies of set , and
  (b) we run the MaxCover algorithm asking whether the maximal number of the
  elements covered by  subsets is at least equal to .
  Clearly, in  each element belongs to at least  sets
  and  is a yes-instance of MaxCover if and only if  is a
  yes-instance of SetCover.~
\end{proof}

So far, we were not able to show that MaxCover (even with
lower-bounded frequencies) is in . Nonetheless, it is quite
easy to show that the problem belongs to .

\begin{theorem}
  For each constant , , MaxCover where each element
  belongs to at least  sets is in  (when parametrized by
  the number of sets in the solution).
\end{theorem}
\begin{proof}
  We give a reduction from MaxCover to the
  Bounded-Nondeterministic-Turing-Machine-Computation problem. On
  input , where , , and  are as usual
  and  is the lower bound on the number of elements that we should
  cover, we produce a machine that on empty input executes the following
  algorithm:
  \begin{enumerate}
  \item It nondeterministically guesses up to  names of sets from
     and writes these names on the tape (each name of a set
    from  is a single symbol).
  \item Deterministically, for each name of the set produced in the
    previous step, the machine writes on the tape the names of those
    elements from this set that have not been written on the tape yet.
  \item The machine counts the number of names of elements written on
    the tape.  If there were at least  of them, it accepts. Otherwise
    it rejects.
  \end{enumerate}
  It is easy to see that we can produce a description of such a
  machine in polynomial time with respect to . Further, it is
  clear that its nondeterministic running time is bounded by some
  polynomial of  and that it makes at most  nondeterministic
  steps.
\end{proof}



It is quite interesting to also consider MaxCover with other
parameters.  First, recall that for parameter , the number of
elements that we should cover, Bl{\"a}ser has shown that MaxCover is
in FPT~\cite{bla:j:partial-set-cover}.  What can we say about
parameter , i.e., the number of elements we can leave
uncovered (this, in essence, means considering the MinNonCovered
problem, but for the worst-case setting it is more convenient to speak
of the parameter )? In this case, the problem is immediately seen
to be para--complete (that is, the problem is -complete even
for a constant value of the parameter).

\begin{corollary}\label{cor:teqzero}
  The MaxCover problem is para--complete when parametrized by the
  number  of elements that can be left uncovered. This holds even
  if each element's frequency is upper-bounded by some constant ,
  .
\end{corollary}
\begin{proof}
  The following trivial reduction from SetCover suffices: Given an
  input instance , output an instance
  , i.e., an identical one, where we require that the
  number of elements left uncovered is . Since the reduction is
  clearly correct and works for the constant value of the parameter,
  we get pare--completeness. To obtain the result for upper-bounded frequencies,
  simply use VertexCover instead of SetCover in the reduction.
\end{proof}

However, if we consider the joint parameter , then the
MaxCover problem becomes -complete.

\begin{theorem}
  MaxCover is -complete when parametrized by both the number
   of sets that can be used in the solution and the number  of
  elements that can be left uncovered.
\end{theorem}
\begin{proof}
  We obtain -hardness by simply observing that the reduction
  given in Corollary~\ref{cor:teqzero} suffices. To prove
  -membership, we give a reduction from MaxCover (with
  parameter ) to SetCover (with parameter ).

  Let  be an input instance of MaxCover.  We
  form an instance  of SetCover as follows.
  Let , where 
  and . For each set 
  and each , we set .  We set
  , where (a) , and (b) .

  It is easy to see that if  is a yes-instance of MaxCover
  then  is a yes-instance of SetCover: If for  it is possible
  to cover  elements of  using  sets, then for  it is
  possible to (a) use  sets from  to cover 
  elements from  and all the elements from , and (b) use 
  sets from  to cover all the elements from  and the
  remaining  elements from .  For the other direction, assume
  that  is a yes-instance of SetCover. However, covering the
  elements from  requires one to use at least  sets from
   (which correspond to the sets from ) and
  covering the elements in  requires at least  sets from
  . Since each set from  covers exactly one
  element from , it is easy to see that if  is a yes-instance,
  then it must be possible to cover at least  elements from
   using  sets from .
\end{proof}





\begin{table}
  \begin{center}
    \begin{tabular}{r|l}
      parameter  & {worst-case complexity of MaxCover} \\
      \hline
      \multirow{2}{*}{}      & {-hard, in }\\
               &  -complete for upper-bounded frequencies\\
      \rule{0cm}{5.5mm} 
            & FPT~\cite{bla:j:partial-set-cover} \\
        & FPT~\cite{bla:j:partial-set-cover} \\
      \rule{0cm}{5.5mm} 
           & para--complete \\
       & -complete   \\
  \end{tabular}
  \caption{\label{tab:complexity}Parameterized worst-case complexity
    results for unrestricted MaxCover and MinNonCovered. The
    parameters are as follows:  is the number of sets we can use in
    the solution,  is the number of elements we are required to
    cover, and  is the number of elements we can leave
    uncovered.}
  \end{center}
\end{table}


We summarize our worst-case complexity resutls in
Table~\ref{tab:complexity}. Not surprisingly, using the parameter 
(i.e., in essence, considering the MinNonCovered problem) leads to
higher computational complexity than using parameter  (i.e., in
essence, considering the MaxCover problem). For the parameter , the
exact complexity of unrestricted MaxCover remains open.




\section{Algorithms for the Case of Bounded Frequencies}\label{sec:approximation}

In this section we present our approximation algorithms for the
MaxCover and MinNonCovered problems, for the case where we either
upper-bound or lower-bound the frequencies of the elements. We first
consider the with MaxCover problem, both with upper-bounded
frequencies and with lower-bounded frequencies, and then move on to
the MinNonCovered problem with upper-bounded frequencies.




\subsection{The MaxCover Problem with Upper Bounded Frequencies}

We will now present an FPT approximation scheme for MaxCover with
upper-bounded frequencies. While
Marx~\cite{Marx06parameterizedcomplexity} has already shown an FPT
approximation scheme for MaxVertexCover, his approach cannot be
directly generalized to the MaxCover problem with bounded frequencies
(although there are some similarities between the algorithms).
Also, our algorithm for MaxCover applied to the MaxVertexCover problem
is considerably faster than the algorithm of Marx~\cite{Marx06parameterizedcomplexity}.
We will give a brief comparison of the two algorithms after presenting our approach.


Intuitively, our algorithm works in a very simple way. Given an
instance  of MaxCover (with frequences bounded by
some constant ) and a required approximation ratio , the
algorithm simply picks some of the sets from  with highest
cardinalities (the exact number of these sets depends only on , , and
), tries all -element subcollections of sets from this
group, and returns the best one. This approach is formalized as
Algorithm~\ref{alg:pApproval}. The following theorem explains that
indeed the algorithm achieves a required approximation ratio.

\SetKwInput{KwParameters}{Parameters}
\begin{algorithm}[t]
   \small
 \KwParameters{\\  --- input MaxCover instance\\
            --- bound on the number of sets each element can belong to\\
            --- the required approximation ratio of the algorithm \\}
   \vspace{2mm}
   \SetAlCapFnt{\small}
 sets from  with the highest cardinalities \;
   \ForEach{-element subset  of }{
        the number of elements covered by \;
   }
   \Return{} \;
   \caption{\small The algorithm for the MaxCover problem with
     frequency upper bounded by .}
   \label{alg:pApproval}
\end{algorithm}

\begin{theorem}\label{thm:pApproval}
  For each instance  of MaxCover where each element
  from  appears in at most  sets in ,
  Algorithm~\ref{alg:pApproval} outputs a -approximate solution
  in time .
\end{theorem}
\begin{proof}
  It is immediate to establish the running time of the algorithm.  We
  show that its approximation ratio is, indeed,~.

  Consider some input instance .  Let  be the solution
  returned by Algorithm~\ref{alg:pApproval} and let  be
  some optimal solution. Let  be an arbitrary function such that
  for each element  such that ,
   is some  such that . We refer to
   as the \emph{coverage function}. Intuitively, the coverage
  function assigns to each element covered under  (by,
  possibly, many different sets) the particular set ``responsible''
  for covering it.
We say that  covers  if and only if . Let 
  denote the number of elements covered by .

  We will show that  covers at least 
  elements. Naturally, the reason why  might cover fewer
  elements than  is that some sets from  may not
  be present in , the set of the subsets considered by the algorithm.
  We will show an iterative procedure that
  starts with  and, step by step, replaces those members of
   that are not present in  with the sets from
  . The idea of the proof is to show that each such replacement
  decreases the number of covered element by at most a small amount.

  Let . Our procedure will
  replace the  sets from  that do not appear in 
  with  sets from .  We renumber the sets so that
  . We will
  replace the sets  with sets  defined through the following algorithm.  Assume
  that we have already computed sets  (thus
  for  we have not yet computed anything). We take  to be a
  set from  such that the set  covers as many
  elements as possible. During the 'th step of this algorithm,
  after we replace  with  in the set ,
  we modify the coverage function as follows:
  \begin{enumerate}
  \item for each element  such that , we set  to be undefined;
  \item for each element , if  is undefined then we set .
  \end{enumerate}

  After replacing  with , it may be the case that fewer
  elements are covered by the resulting collection of sets. Let 
  denote the difference between the number of elements covered by
   and by  (or , if by a
  fortunate coincidence there are more elements covered after
  replacing  with ). By the construction of the set 
  and the fact that , each set from 
  contains more elements than . Thus we infer that every set from
   must contain at least  elements covered by
  . Indeed, if some set 
  contained fewer than  elements covered by ,
   would have to cover at least  elements uncovered by . But this would
  mean that after replacing  with , the difference between
  the number of covered elements would be at most .




  Let  denote the set obtained after the
  above-described  iterations. Since, for each , the set
   is a subset of , we know
  that, for each , each set from  (there is
   such sets) must contain at least 
  elements covered by  (there is at most
   such elements).  Since each element is contained in at most
   sets, we infer that for each ,  and, as a consequence, .  Thus we
  conclude that (recall that ):
  
  That is, after our process of replacing the sets from  that
  do not appear in  with sets from , at most
   elements fewer are covered. This means that there
  are  sets in  that together cover at least 
  elements. Since the algorithm tries all size- subsets of ,
  it finds a solution that covers at least  elements.~
\end{proof}


Our analysis is tight up to the constant factor of .
Below we present a family of parameters  and instances of
MaxCover with upper-bounded frequencies on which our algorithm
achieves approximation ratio 



\begin{proposition}
  There is a family  of pairs  where  is an
  instance of MaxCover with bounded frequencies and  is a real
  number, , such that for each ,
  if we use Algorithm~\ref{alg:pApproval} to find a
  -approximate solution for , it outputs an at-most
  -approximate one.
\end{proposition}
\begin{proof}
  We describe how to construct pairs  from the set
  . We let  be the bound of the frequencies of elements in
   and we let  be the number of sets that we can use in the solution. We choose
   and  to be sufficiently large, and  to be sufficiently
  close to  (the exact meaning of ``sufficiently large'' and
  ``sufficiently close to '' will become clear at the end of the
  proof; elements of  differ in the particular choices of ,
  , and ).  We require that  is an
  integer and that  divides .

  We now proceed with the construction of instance 
  for our choice of , , and .  We set ;  is the number of highest-cardinality sets from
   that Algorithm~\ref{alg:pApproval} will consider on instance
  . By our choice of  and ,  is an integer and is
  divisible by .  We form , the set of elements to be covered,
  to consist of two disjoint subsets,  and , such that
   and .  We form the family  to consist of two
  subfamilies,  and , defined as follows:
  \begin{enumerate}
  \item There are  subsets in , . We form the sets in  so that: (a) sets from 
    are subsets of , (b) each element from  belongs to
    exactly  different sets from , and (c) no two elements
    from  belong to the same  sets from .
    Specifically, we build sets  as follows. Let
     be some one-to-one mapping between elements in  and
    -element subsets of . For each ,  belongs
    exactly to the sets  such that .  Note that each set 
    contains exactly 
    elements.

\item  contains  sets, each covering exactly  different elements from  (and no other
    elements) so that no two sets from  overlap.
\end{enumerate}
This completes our description of . It is easy to see that each
  optimal solution for  covers exactly 
  elements; each set contains exactly 
  elements and, there are  that are pairwise disjoint (for example
  the  sets in ).

  Nonetheless, Algorithm~\ref{alg:pApproval} is free to choose any 
  sets from  to include within , the collection of sets
  from which it forms the solution, and, in particular, it is free to
  pick the  sets from .\footnote{We could also ensure that
    each set in  contained one of  additional
    elements, forcing the algorithm to pick exactly the sets from
    , but that would obscure the presentation of our
    argument.} 

  Let us fix some arbitrary collection  of  sets from
  . For each , , let  be the number
  of elements from  that belong to exactly  sets in .
  The number of elements covered by  is exactly .  How to compute ?
  Using mapping , it suffices to count the number of -element
  subsets of  that contain the indices of exactly  sets from
  . In effect, we have . We upper bound the number of sets covered by  with:
  



  Consequently, on instance  Algorithm~\ref{alg:pApproval} achieves
  the following approximation ratio ,
  which is equal to:
  
  Now, if  is large in comparison with  and  (which happens
  for sufficiently large ), then . Also, for sufficiently large  and 
  (and for ) we have  and . Finally,
  for sufficiently large  we have . Thus, for large values of , , and , we
  can approximate the above ratio with the following expression:
  
This completes our argument.~
\end{proof}


Let us now compare our algorithm to that of
Marx~\cite{Marx06parameterizedcomplexity} for the case of MaxVertexCover.
Briefly put, the idea behind Marx's algorithm is as follows: Consider
vertices in the order of nonincreasing degrees. If the degree of the
vertex with the highest degree is large enough, then  vertices with
the highest degrees already cover sufficiently many edges to give a
desired approximate solution. If the 
highest degree is not large enough, then there is an exact,
color-coding based, FPT algorithm that solves the problem
optimally. Our algorithm is similar in the sense that we also focus on
a group of sets with highest cardinalities (sets' cardinalities in
MaxCover correspond to vertex degrees in MaxVertexCover). However,
instead of simply picking  largest ones, we make a careful decision
as to which exactly to take.\footnote{Indeed, it is possible to build
  an example where picking sets with highest cardinalities would not
  work. This trick works in Marx's algorithm because he considers
  graphs and, thus, can bound the negative effect of covering the same
  element by different sets; in the MaxCover problem this seems
  difficult to do.}
Further, our algorithm has a better running time than that of Marx.
To achieve approximation ratio , the algorithm presented by
Marx has running time at least
. For us, the
exponential factor in the running time is . On the other hand, we should point out that Marx's
algorithm's running time stems mostly from the exact part and the
algorithm given there is interesting in its own right.



\subsection{The MaxCover Problem with Lower-Bounded Frequencies}

Let us now move on the case of MaxCover with lower-bounded
frequencies.  It turns out that in this case the standard greedy
algorithm, given here as Algorithm~\ref{alg:greedy}, can---for
appropriate inputs---achieve a better approximation ratio than in the
unrestricted case.



\begin{algorithm}[t]
   \small
   \SetAlCapFnt{\small}
   \KwParameters{\\  --- input MaxCover instance\\
        --- lower bound on the number of the sets each element belongs to}
   \vspace{3mm}

   \;
   \For{ \KwTo }{
       \;
       \;
      
   }
   \Return{C}
   \caption{\small The algorithm  for the MaxCover problem with frequency lower bounded by .}
   \label{alg:greedy}
\end{algorithm}

\begin{theorem}\label{theorem:greedy}
  Algorithm~\ref{alg:greedy} is a polynomial-time -approximation algorithm 
  for the MaxCover problem with frequency lower bounded by ,
  on instances with  elements where we can pick up to  sets.
\end{theorem}
\begin{proof}
  The algorithm clearly runs in polynomial time and so we show it's
  approximation ratio. Let  be an input instance of
  MaxCover and let  be an integer such that each element from 
  belongs to at least  sets from .
  
  We prove by induction that for each , , after
  the 'th iteration of Algorithm~\ref{alg:greedy}'s main loop, the
  number of uncovered elements is at most . Naturally, for  the number of uncovered
  elements is exactly , the total number of elements.  Suppose that
  the inductive assumption holds for some ,  and
  let  be the number of elements still uncovered after the
  -th iteration (by the inductive assumtpion, we have ). Since each element belongs to at least
   sets and neither of the sets containing the uncovered elements
  is yet selected, by the pigeonhole principle there is a
  not-yet-selected set that contains at least  of the uncovered elements.  In consequence, the number of
  elements still uncovered after the -th iteration is at most:
  
  Thus after  iterations the number of uncovered elements is at
  most:
  
  Since the number of covered elements in the optimal solution is at
  most , the algorithm's approximation ratio is .~
\end{proof}

Naturally, the standard approximation ratio of  of the
greedy algorithm still applies and we get the following corollary.

\begin{corollary}
Algorithm~\ref{alg:greedy} gives approximation guarantee of .
\end{corollary}

The analysis given in Theorem~\ref{theorem:greedy} is tight. Below we
present a family of instances on which the algorithm reaches exactly
the promised approximation ratio.

\begin{proposition}
  For each , , there is an instance 
  of MaxCover (with  sets. element frequency lower-bounded by ,
   sets to use, and ) such that on input
  , Algorithm~\ref{alg:greedy} achieves approximation ratio
  no better than .
\end{proposition}
\begin{proof}
  Let us fix some , .  We choose integers ,
  , and  so that: (a) , (b) 
  (and, thus, ), and (c) , , and  are sufficiently
  large (the exact meaning of ``sufficiently large'' will become clear
  at the end of the proof). 

  We form instance  as follows.
We let , where  are
  pairwise-disjoint sets, each of cardinality 
  (thus ).  The family  consists of
  two subfamilies,  and :
  \begin{enumerate}
  \item  consists of  sets, ,
    constructed as follows. For each , , let 
    be some one-to-one mapping from  to -element subsets
    of . For each , , if  and
     then we include  in sets
    .  Note that for each
     in , ; for
    each , ,  contains  elements from ; to see this, it suffices to count how
    many -elements subsets of  there are that contain .
   

  \item .
  \end{enumerate}
Note that, by our construction, each element from  belongs to
  exactly  sets from  ( from  and one from
  ).

  Naturally, the  disjoint sets from  form the optimal
  solution and cover all the elements.  We will now analyze the
  operation of Algorithm~\ref{alg:greedy} on input .

  We claim that Algorithm~\ref{alg:greedy} will select sets from
   only. We show this by induction. Fix some , , and suppose that until the beginning of the 'th
  iteration the algorithm chose sets from  only.
This means that, for each , , each set 
  contains exactly  uncovered elements. Why is
  this the case? Assume that the algorithm selected sets . An element  is uncovered if and only
  if ;  is the number of -element subsets of  that
  do not contain any members of .  So, if in
  the 'th iteration the algorithm choses some set from ,
  it would cover these additional 
  elements. On the other hand, if it chose a set from , it
  would additionally cover  elements, where .  By our choice, we have 
  and, thus, . We can now see that the
  following holds:
  
  That is, in the 'th iteration Algorithm~\ref{alg:greedy} picks
  a set from . This proves our claim.

  Let us now assess the approximation ratio Algorithm~\ref{alg:greedy}
  achieves on . By the above reasoning, we know that it
  leaves  uncovered elements in each , .  Thus the fraction of the uncovered elements is
  bounded by the following expression (see some explanation below):
  
  The first inequality holds by iterative application of the simple
  observation that if  then .  To obtain the final estimate, we observe that for
  sufficiently large  and  (where ), we have
  . For
  sufficiently large ,  (by the fact that ).  Since the optiomal solution covers all the elements, we
  have that Algorithm~\ref{alg:greedy} on input  achieves
  approximation ratio no better than .~
\end{proof}

Theorem~\ref{theorem:greedy} has some interesting implications. Let us
consider a version of the MaxCover problem in which the ratio
 between the frequency lower bound  and the number of
sets  is constant.  This problems arises, e.g., if we use
approval-based variant of the Chamberlin-Courant's election system
with a requirement that each voter must approve at least some constant
fraction (e.g. 10\%) of the candidates. There exists a polynomial-time
approximation scheme (PTAS) for this version of the problem.

\begin{definition}
  For each , ,  let
  -MaxCover be a variant of MaxCover for instances that
  satisfy the following conditions: If  is a lower-bound on the
  frequencies of the elements and there are  sets, then
  .
\end{definition}

\begin{theorem}\label{thm:ptas}
  For each , , there is a PTAS for
  -MaxCover.
\end{theorem}
\begin{proof}
  Fix some , .  Let  be
  input instance of -MaxCover and let  be our desired
  approximation ratio.  We let  be the number of set in  and
   be the lower bound on element frequencies. By definition, we
  have .  If 
  then we can run Algorithm~\ref{alg:greedy} and, by
  Theorem~\ref{theorem:greedy}, we obtain approximation ratio
  . Otherwise,  is bounded by a constant and enumerating all
  -element subsets of  gives a polynomial exact algorithm
  for the problem.~
\end{proof}


The exact complexity of -MaxCover is quite interesting.  Using
Algorithm~\ref{alg:greedy}, we show that it belongs to the second
level of Kintala and Fisher's -hierarchy of limited
nondeterminism~\cite{fis-kin:j:beta}. In effect, it is unlikely that
the problem is -complete.

\begin{definition}[Kintala and Fisher~\cite{fis-kin:j:beta}]
  For each positive integer ,  is the class of decision
  problems that can be solved in polynomial time, using additionally
  at most  nondeterministic bits (where  is the size of
  the input instance).
\end{definition}
It is easy to see that  is simply the class of problems
solvable in polynomial time; we can simulate  bits of
nondeterminism by trying all possible combinations. However, class
 appears to be greater than  but smaller than  (of
course, since we do not know if , this is only a
conjecture).

\begin{theorem}
  For each , , the decision variant of
  -MaxCover is in .
\end{theorem}
\begin{proof}
  Fix some , .  We will give a
  -algorithm for -MaxCover. Let 
  be an instance of -MaxCover (recall that  is the number
  of elements we are required to cover). We let  be the lower bound
  on elements' frequencies in , we let , and we let
  . By definition, we have . W.l.o.g., we assume that .

  Our algorithm works as follows. If 
  then we run Algorithm~\ref{alg:greedy} and output its
  solution. Otherwise, we guess  names of the sets from  and
  check if these sets cover at least  elements. If so, we accept
  and otherwise we reject on this computation path. 
  
  First, it is clear that the algorithm uses at most 
  nondeterministic bits.  We execute the nondeterministic part of the
  algorithm only if  and each set's name requires at most  bits. Altogether, we use at most
   bits of nondeterminism.
 
  Second, we need to show the correctness of the algorithm. Clearly,
  if the algorithm uses the nondeterministic part then certainly it
  finds an optimal solution. Consider then that the algorithm uses the
  deterministic part, based on Algorithm~\ref{alg:greedy}.  In this
  case we know that . Thus, the
  approximation ratio of Algorithm~\ref{alg:greedy} is greater than:
  . That is,
  the algorithm returns a solution that covers more than
   elements and, since  and the
  number of covered elements is integer, the algorithm must find an
  optimal solution.
\end{proof}




\subsection{The MinNonCovered Problem}
In this section we considered the MinNonCovered problem, that is, a
version of MaxCover where the goal is to minimize the number of
elements left uncovered. In this case we give a randomized FPT
approximation scheme (presented as
Algorithm~\ref{alg:pApprovalDissat}).


Intuitively, the idea behind our approach is to extend a simple
bounded-search-tree algorithm for SetCover with upper-bounded
frequencies to the case of MaxCover. An FPT algorithm for SetCover
with frequencies upper-bounded by some constant  could work
recursively as follows: If there still is some uncovered element ,
then nondeterministically guess one of the at-most- sets that
contain  and recursively solve the smaller problem. The recursion
tree would have at most  levels and  leaves. The same approach
does not work directly for MaxCover because we do not know which
element  to pick (in SetCover the choice is irrelevant because we
have to cover all the elements). However, it turns out that if we
choose  randomly then, in expectation, we achieve a good result.



\begin{algorithm}[t!]
   \footnotesize
   \SetKwInput{KwParameters}{Parameters}
   \SetKwFunction{RecursiveSearch}{RecursiveSearch}
   \SetKwFunction{Main}{Main}
   \SetKwBlock{Block}
   \SetAlCapFnt{\footnotesize}
   \KwParameters{\\
            --- input MinNonCovered instance \\
            --- bound on the number of sets each element can belong to \\

            --- the required approximation ratio of the algorithm \\
            --- the allowed probability of achieving worse than  approximation ratio
}
\vspace{3mm}	
    \RecursiveSearch{, }:
	\Block{
		\eIf{ = }
		{
			\Return{}\;
		}
		{
			 randomly select element not-yet covered by\\
							\;
			\;
			\ForEach{ such that } {
				 \RecursiveSearch{, }\;
				\If{ {\rm\bf is better than} }
				{
					\;
				}
			}
			\Return{}\;
		}
	}
	\hspace{5mm} \\
	\Main{}:
	\Block{
		\;
		\For{ \KwTo }{
			 = \RecursiveSearch{, }\;
			\If{ {\rm\bf is better than} }
			{
				\;
			}
		}
		\Return{}\;
	}

   \caption{\small The algorithm for the MinNonCovered problem with frequency upper bounded by .}
   \label{alg:pApprovalDissat}
\end{algorithm}


\begin{theorem}\label{thm:pApprovalDissat}
  Algorithm~\ref{alg:pApprovalDissat} outputs a -approximate
  solution for the MinNonCovered problem with probability . The time complexity of the algorithm is .
\end{theorem}
\begin{proof}
  Let  be our input instance of the MinNonCovered
  problem and fix some , , and , . Each element from  appears in at most  sets
  from .

  By  we denote the probability that a single invocation of the
  function \texttt{RecursiveSearch} (from the \texttt{Main} function)
  returns a -approximate solution.  We will first show that
   is at least , and then
  we will invoke the standard argument that if we make
   calls to
  \texttt{RecursiveSearch}, then taking the best output gives a
  -approximate solution with probability .

  


  Let  be some optimal solution for , let  be the set of elements covered by , and let  be the set of the remaining, uncovered elements.
Consider a single call to \texttt{RecursiveSearch} from the ``for''
  loop within the function \texttt{Main}.  Let  denote the event
  that during such a call, at the beginning of each recursive call, at
  least a  fraction of the elements not covered
  by the constructed solution (i.e., the solution denoted
   in the algorithm) belongs to .
Note that if the complementary event, denoted ,
  occurs, then \texttt{RecursiveSearch} definitely returns a
  -approximate solution. Why is this the case? Consider some
  tree of recursive invocations of \texttt{RecursiveSearch}, and some
  invocation of \texttt{RecursiveSearch} within this tree. Let  be
  the number of elements not covered by  at the beginning
  of this invocation.  If at most  of the
  not-covered elements belong to , then---of course---the
  remaining at least  of them belong to . In
  other words, then we have  and,
  equivalently, . This means that 
  already is a -approximate solution, and so the solution
  returned by the current invocation of \texttt{RecursiveSearch} will
  be -approximate as well.  (Naturally, the same applies to the
  solution returned at the root of the recursion tree.)



  Now, consider the following random process . (Intuitively,
   models a particular branch of the \texttt{RecursiveSearch}
  recursion tree.)  We start from the set  of all the elements,
  , and in each of the next  steps we execute the following
  procedure: We randomly select an element  from  and if 
  belongs to , we remove from  all the elements covered by
  the first\footnote{We assume that the sets in  are ordered
    in some arbitrary way.} set from  that covers . Let
   be the probability that a call to \texttt{RecursiveSearch}
  (within \texttt{Main}) finds an optimal solution for , and let
   be the same probability, but under the condition that
   takes place.  It is easy to see that  is greater or
  equal than the probability that in each step  picks an
  element from .  Let  be the probability in each step
   picks an element from , under the condition that at the
  beginning of every step more than 
  fraction of the elements in  belong to . Again, it is easy
  to see that . Further, it is immediate
  to see that .

  Altogether, combining all the above fidnings, we know that the
  probability that \texttt{Re\-cur\-sive\-Search} returns a -approximate
  solution is at most:
  
  (That is, either the event  does not take place and
  \texttt{RecursiveSearch} definitely returns a -approximate
  solution, or  does occur, and then we lower-bound the
  probability of finding a -approximate solution by the
  probability of finding the optimal one.)

  To conclude, the probability of finding a -approximate
  solution in one of the 
  independent invocations of \texttt{RecursiveSearch} from
  \texttt{Main} is at least:
  
  Establishing the running time of the algorithm is immediate, and so
  the proof is complete.~
\end{proof}















\begin{algorithm}[t]
   \small
   \SetAlCapFnt{\small}
   \SetKwFunction{Expon}{Expon}
   \KwParameters{\\  --- input MaxCover instance\\
            --- the parameter of the algorithm\\
            --- an exact algorithm for MaxCover (returns the set of sets to be used in the cover)}
   \;
   \For{ \KwTo }{
       \;
      \;
      
   }
    \;
    \;
   \Return{}
   \caption{\small An approximation algorithm for the unrestricted MaxCover problem.}
   \label{alg:greedyAndExpo}
\end{algorithm}


Algorithm~\ref{alg:pApprovalDissat} is very useful, especially in
conjunction with Algorithm~\ref{alg:pApproval}. The former one has to
provide a very good solution if it is possible to cover almost all the
elements and the latter one has to provide a very good solution if in
every solution many elements must be left uncovered.



\section{Algorithms for the Unrestricted Variant}
\label{sec:unrestricted}

So far we have focused on the MaxCover problem where element
frequencies were either upper- or lower-bounded. Now we consider the
completely unrestriced variant of the problem. In this case we give
exponential-time approximation schemes that, nonetheless, are not FPT.

The main idea, which is similar to that of
Cygan~et.~al~\cite{journals/ipl/CyganKW09} and of Croce and
Paschos~\cite{cro-pas:j:cover}, is to solve part of the problem using
an exact algorithm and to solve the remaining part using the greedy
algorithm (i.e., Algorithm~\ref{alg:greedy}). There are two possible
ways in which this idea can be implemented: Either we can first run
the exact algorithm and then solve the remaining part of the instance
using the greedy algorithm, or the other way round. We consider both
approaches, though a variant of the ``brute-force-first-then-greedy''
approach appears to be superior (at least as long as we do not have
exact algorithms that are significantly faster than a brute-force
approach).


We start with the analysis of Algorithm~\ref{alg:greedyAndExpo}, which
first runs the greedy part and then completes it using an exact
algorithm.

\begin{theorem}\label{thm:greedyAndExpo}
  Let  be an exact algorithm for the MaxCover problem with time
  complexity .  For each instance  of
  MaxCover and for each , ,
  Algorithm~\ref{alg:greedyAndExpo} returns an -approximate solution for 
  and runs in time .
\end{theorem}
\begin{proof}
  Establishing the running time of the algorithm is immediate and,
  thus, below we focus on showing the approximation ratio.

  Let  be an instance of MaxCover and let  be an
  integer, .  We rename the elements in  so
  that  and  are the
  consecutive elements selected in the first, greedy, ``for loop'' in
  Algorithm~\ref{alg:greedyAndExpo}.  For each , ,
  let .  Let
   denote the set of elements covered by some optimal
  solution and set . Let  denote the set
  . (That is,  is the set of
  elements in the variable  in the
  Algorithm~\ref{alg:greedyAndExpo} right before executing the 'th
  iteration of the ``for loop''. Of course, .)
  Naturally, for each , , we have .

  We claim that for each , , there exist 
  sets from  that cover at
  least  fraction of the elements from . Why is this the case? First, note that there
  are some  sets from  that
  cover  (it suffices to take the 
  sets from some optimal solution, if need be, replace those that
  belong to  with some arbitrarily chosen
  ones from ). Let  be these  sets.  Consider some arbitrary assignment
  of the elements from  to the sets
  , such that each element is assigned to exactly
  one set. Further, consider an ordering of these sets according to
  the increasing number of assigned elements. If the 'th set in the
  ordering is assigned at most fraction  of the elements,
  than each of the sets preceding the 'th one in the ordering also
  is assigned at most fraction  of the elements. In
  consequence, the last  sets from the ordering cover at least
  fraction  of the elements. On the other hand, if the
  'th set in the order is assigned more than fraction 
  of the elements then the following sets also are and, once again,
  the last  elements cover at least fraction  of
  the elements.

  In consequence, we see that for each , , . The reason is that
  since there are  sets among  that cover fraction  of elements from
  , at least one of them must cover
  .   is chosen as a set that
  covers most sets from . It covers  elements from , and, thus, .


  We can now proceed with computing the algorithm's approximation
  ratio.  By the above reasoning, we observe that the solution
  provided by Algorithm~\ref{alg:greedyAndExpo} covers at least . Now, we assess
  the minimal value of . Minimization of
   can be viewed as a linear programming task with
  the following constraints: for each , , .  Since we have 
  variables and  constraints, we know that the minimum is achieved
  when each constraint is satisfied with equality (see, e.g.,
  \cite{Vazirani:2001:AA:500776}). Thus a solution to our linear
  program consists of values  that,
  for each , , satisfy . By induction,
  we show that for each , , . Indeed, the claim
  is true for :
  
  Now, assuming that , we calculate
  :
  

  Thus we can lower-bound the number of elements covered by
  Algorithm~\ref{alg:greedyAndExpo} as follows:
  
  This completes the proof.~
\end{proof}

The idea of the proof of Theorem~\ref{thm:greedyAndExpo} is simillar
to the algorithm of Cygan~et.~al~\cite{journals/ipl/CyganKW09} for the
problem of weighted set cover. Theorem~\ref{thm:greedyAndExpo} gives a
good-quality result provided we knew an optimal algorithm with the
better complexity than exhaustive search.
Otherwise, we can obtain even better results using
Algorithm~\ref{alg:expoAndGreedy}, which first runs a brute-force
approach and completes it using the greedy algorithm.

\begin{algorithm}[t]
   \small
   \SetAlCapFnt{\small}
   \SetKwFunction{better}{better}
   \KwParameters{\\  --- input MaxCover instance\\
            --- the parameter of the algorithm}
   \;
   \;
   \ForEach{-element subset  of }{
      \For{ \KwTo }{
           \;
           \;
          
      }
       better solution among  and \;
   }
   \Return{}
   \caption{\small The approximation algorithm for the MaxCover problem.}
   \label{alg:expoAndGreedy}
\end{algorithm}

\begin{theorem}
  For each instance  of MaxCover and each integer
  , , Algorithm~\ref{alg:expoAndGreedy} computes
  an -approximate solution for
   in time .
\end{theorem}
\begin{proof}
  Let  be our input instance and let ,
  , denote some optimal solution. Let
   denote a subset of -elements from 
  that together cover the greatest number of the elements. Thus the
  sets from  cover at least a fraction  of all
  the elements covered by the optimal solution. Consider the problem
  of covering the elements uncovered by  with  sets
  from . We know that  is an optimal solution for this problem. On the other
  hand, we also know that the greedy algorithm achieves approximation
  ratio  for the problem. Thus, the approximation
  ratio for the original problem is:
  
  It is immediate to establish the running time of the algorithm and
  so the proof is complete.~
\end{proof}

If we wish to solve MaxVertexCover rather than MaxCover, then in
Algorithm~\ref{alg:expoAndGreedy} we should replace the greedy
approximation algorithm with that of Ageev and
Sviridenko~\cite{age-svi:b:covers}.

\begin{corollary}\label{alg5:maxvertexcover}
There exists an -approximation algorithm for MaxVertexCover problem running in time 
\end{corollary}

It is quite evident that as long as algorithm  used within
Algorithm~\ref{alg:greedyAndExpo} is the simple brute-force algorithm
that tries all possible solutions, then
Algorithm~\ref{alg:expoAndGreedy} is superior; in the same time it
achieves a better approximation ratio. It turns out that, for the case
of MaxVertexCover, Algorithm~\ref{alg:expoAndGreedy} (in the variant
from Corollary~\ref{alg5:maxvertexcover}) is also better than the
algorithm of Croce and
Paschos~\cite{cro-pas:j:cover}.\footnote{Algorithm~\ref{alg:greedyAndExpo}
  cannot be directly compared to the algorithm of Croce~and
  ~Paschos~\cite{cro-pas:j:cover} for the following reason.
  Algorithm~\ref{alg:greedyAndExpo} uses specifically a greedy
  algorithm which is the best known approximation algorithm for
  MaxCover, but which is suboptimal for MaxVertexCover. In contrast,
  the algorithm of Croce~and ~Paschos~\cite{cro-pas:j:cover} can use,
  e.g., the -approximation algorithm of Ageev and
  Sviridenko~\cite{age-svi:b:covers}. One could, of course, try to use
  the algorithm of Ageev and Sviridenko in
  Algorithm~\ref{alg:greedyAndExpo}, but our analysis does not work
  for this case.}

The idea behind the algorithm of Croce~and
~Paschos~\cite{cro-pas:j:cover} for MaxVertexCover is similar to that
behind our Algortihm~\ref{alg:expoAndGreedy}.  Specifically, given two
algorithms for MaxVertexCover, approximation algorithm  and
exact algorithm , for a given value  it first uses
 to find am optimal solution that uses  vertices (out
of the  vertices that we are allowed to use in the full solution),
then it remeves these  vertices and solves the remaining part of
the problem using .  Assuming that  is the
approximation ratio of the algorithm , this approach
results in the approximation ratio equal to .

Below we compare Algorithm~\ref{alg:expoAndGreedy} (version from
Corollary~\ref{alg5:maxvertexcover}) with the algorithm of Croce and
Paschos~\cite{cro-pas:j:cover}. As the components  and
 we use, respectively, the -approximation
algorithm of Ageev and Sviridenko~\cite{age-svi:b:covers} and the
brute-force algorithm that tries all possible solutions.  The best
known exact algorithm for MaxVertexCover is due to
Cai~\cite{cai:j:cardinality-constrained} and has the complexity
, but this algorithm uses exponential amount of space.
Since exponential space complexity might be much less practical than
exponential time complexity, we decided to use the brute-force
approach (to the best of our knowledge there, there is no better exact
algorithm running in a polynomial space).
\begin{figure}[tb]
  \begin{center}
    \includegraphics[scale=0.5]{approxComparison}
  \end{center}
  \vspace{-0.75cm}
  \caption{The comparison of the approximation ratios of Algorithm~\ref{alg:expoAndGreedy} and the algorithm of Croce~and~Paschos~\cite{cro-pas:j:cover} for MaxVertexCover.}
  \label{fig:approximationComparison}
\end{figure}
We present our comparison in Figure~\ref{fig:approximationComparison}.
The -axis represents the parameter , measuring the
fraction of the solution obtained using the exact algorithm (for 
we use the approximation algorithm alone and for  we use the exact
algorithm alone). On the -axis we give approximation ratio of each
algorithm.  In other words, for each point on the -axis we set the
 parameters of the algorithms to be equal, so that their running
times are the same, and we compare their approximation guarantees.

We conclude that, as long as we use the brute-force algorithm as the
exact one, Algorithm~\ref{alg:expoAndGreedy} gives considerably better
approximation guarantees than that of Croce and
Paschos. Figure~\ref{fig:approximationComparison} also exposes one
potential weakness of the algorithm of Croce and Paschos. Apparently,
for some cases increasing the complexity of the algorithm results in
the decrease of its approximation guarantee.


It is quite interesting to understand the reasons behind the differing
performance of Algorithm~\ref{alg:expoAndGreedy} and that of Croce and
Paschos. In some sense, the algorithms are very similar. If we use the
brute-force algorithm as the exact one in the algorithm of Croce and
Paschos, then the main difference is that our algorithm runs the
approximation algorithm for each possible solution tried by the
brute-force algorithm, and Croce and Paschos's algorithm only runs the
approximation algorithm once, for the best partial solution. In
effect, our algorithm can exploit situations where it is better when
the exact algorithm does not find an optimal solution for the
subproblem, but rather leaves ground for the approximation algorithm
to do well. Naturally, such strategy is only possible if we have
additional knowledge of the structure of the exact algorithm (here,
the brute-force algorithm). The result of Croce and Paschos pays the
price for being more general and being able to use any combination of
the approximation algorithm and the exact algorithm.


\section{Conclusions}

Motivated by the study of winner-determination under
Chamberlin--Courant's voting rule (with approval misrepresentation),
we have considered the MaxCover problem with bounded frequencies and
its minimization variant, the MinNonCovered problem, from the point of
view of approximability by FPT algorithms. We have shown that for
upper-bounded frequencies there is an FPT approximation scheme for
MaxCover and a randomized FPT approximation scheme for
MinNonCovered. For lower-bounded frequences we have shown that the
standard greedy algorithm for MaxCover may achieve a better
approximation ratio than in the unrestricted case. Finally, we have
shown that in the unrestricted case there are good exponential-time
approximation algorithms (though, not FPT ones) that combine exact and
greedy algorithms and smoothly exchange the quality of the
approximation for the running time.
Some of our results regarding MaxCover with bounded frequencies
improve previously known results for MaxVertexCover. In particular,
our Algorithm~\ref{alg:pApproval} improves upon the approximation
scheme given by Marx, and our Algorithm~\ref{alg:expoAndGreedy}
improves upon the result of Croce and Paschos~\cite{cro-pas:j:cover}
(provided we use brute-force algorithm as the underlying exact
algorithm in the scheme proposed by Croce and Paschos; this is
reasonable if we are interested in algorithms that use only polynomial
amount of space).

There are several interesting directions for future research. For
example, is it possible to obtain FPT approximation schemes for
MaxCover with lower-bounded element frequencies? Further, what is the
exact complexity of MaxCover (with or without lower-bounded
frequencies)? We have quickly observed its -hardness, but does
it belong to ? (It is quite easy, however, to show that it
belongs to .) We are also interested in the exact complexity
of MaxCover with lower-bounded frequencies for the case where we
require the ratio of frequency lower-bound and the number of sets to
be at least some given value , ? We have given
a PTAS for this variant of the problem (see Theorem~\ref{thm:ptas})
and have shown its membership in , but we did not attempt to
prove its completeness for any particular complexity class.














\bibliographystyle{abbrv}
\bibliography{main}

\end{document}
