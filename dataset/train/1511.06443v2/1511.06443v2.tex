\pdfoutput=1
\documentclass{article} \usepackage{iclr2016_conference,times}



\usepackage[utf8]{inputenc} 
\usepackage{amsmath, amssymb, bm, mathtools, thmtools}
\usepackage{verbatim}
\usepackage{graphicx}\graphicspath{{figures/}}
\usepackage{multicol}
\usepackage{tabularx}
\usepackage{mathrsfs} 
\usepackage{url}



\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{hypernat}
\usepackage{datetime}



\DeclareMathAlphabet\EuRoman{U}{eur}{m}{n}
\SetMathAlphabet\EuRoman{bold}{U}{eur}{b}{n}
\newcommand{\eurom}{\EuRoman}



\usepackage{euscript,microtype}


\usepackage[capitalize]{cleveref}

\crefname{lemma}{Lemma}{Lemmas}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{theorem}{Theorem}{Theorems}

\makeatletter
\let\reftagform@=\tagform@
\def\tagform@#1{\maketag@@@{\ignorespaces\textcolor{gray}{(#1)}\unskip\@@italiccorr}}
\renewcommand{\eqref}[1]{\textup{\reftagform@{\ref{#1}}}}
\makeatother


\setlength{\marginparwidth}{1.25in}





\newcommand{\LATER}[1]{\error}
\newcommand{\fLATER}[1]{\error}
\newcommand{\TBD}[1]{\error}
\newcommand{\fTBD}[1]{}
\newcommand{\PROBLEM}[1]{\error}
\newcommand{\fPROBLEM}[1]{}
\newcommand{\NA}[1]{#1} 
 







\renewcommand{\appendixname}{}


  

\def{}
\def\*[#1#1
\hat X_{n,m} \defas 
\hat X(\uu_n,\vv_m,\UU_n,\VV_m) 
\defas f_\theta \bigl(\uu_n, \vv_m,\UU_{n,1}\hprod \VV_{m,1},\dotsc,\UU_{n,\DM} \hprod \VV_{m,\DM}\bigr).
\label{objfunc}
\sum_{(n,m) \in J} (X_{n,m} - \hat X_{n,m})^2 
 +  \lambda \Bigl [
      \sum_n ||\UU_n||^2_{F} 
 +  \sum_n ||\uu_n||^2_{2} 
 +  \sum_m ||\VV_m||^2_{F} 
 +  \sum_m ||\vv_m||^2_{2} \Bigr ],
\label{tensorrep}
\hat X_{n,m}  
\defas a^T \tanh \bigl( U_n^T Q^{[1:H]} V_m +  W
     \begin{bmatrix}
        U_n\\ V_m \end{bmatrix}
 	+ b\bigr),

\bar U_n = 
 \begin{bmatrix}
        \uu_n \\
        1_\DV  \\
        \UU_n 
     \end{bmatrix}  \in \Reals^{2\DV+\DM}, 
     \qquad    
\bar V_m=
  \begin{bmatrix}
        1_\DV \\ 
        \vv_m \\
        \VV_m 
     \end{bmatrix} \in \Reals^{2\DV+\DM}, 
  \qquad
 Q_{ij}^h = \begin{cases}
      W'_{h,i}, & \text{if } i=j, \\
      0, &\text{otherwise,}     
\end{cases}
     
where  denotes an -dimensional column vector with all entries equal to 1
and  is the weight matrix defining the first layer of the NNMF network.
 Then we recover the first layer of NNMF with the third-order tensor 
  
whose 'th slice is .


There have been many scalable techniques proposed to model very large KGs.
\citet{NickelMurphy2015} split existing models into two categories: latent feature models and graph feature models. Latent variable methods learn unobserved representations of entities and use them to predict relations,
while graph feature methods learn to predict relations directly from extracted features of local graph structure.
\citet{TC2015} argue through empirical comparisons that these two categories of models exhibit complimentary strengths.

A number of state-of-the-art proposals for collaborative filtering are perhaps best thought of as incorporating aspects of  graph feature models.
An example of a method relaxing the low-rank assumption using graph features is the Local Low Rank Matrix Approximation \citep{LLORMA}, which assumes that every entry in the matrix is given by a combination of low rank matrices, where the combination is specific to the entry.
LLORMA achieves impressive state-of-the-art performance.

Other approaches also use neural-network architectures but work by trying to predict the ground truth ratings directly from the observed ratings matrix .  For example,
in I-AutoRec \citep{AutoRec}, an autoencoder is learned that takes as input the observed movie ratings vector  for user  and produces as output the ground truth . (Missing entries are typically replaced by value~3.)
AutoRec achieves state-of-the-art performance, slightly besting LLORMA on some benchmarks, but a careful comparison would likely require a fresh data set and strict controls on how the numerous parameters for both models are chosen \citep{BH15,Dwork07082015}.
Another model in this category is the I-RBM \citep{I-RBM}, but its performance is now far from the state of the art.

Both LLORMA and I-AutoRec can be seen as models combining aspects of both graph feature and latent feature models. 
LLORMA identifies similar rows and columns (entities) using graph features, but model each local low-rank approximation using latent features. 
I-AutoRec takes as input all observed ratings (relations) for a user (entity), allowing the network to model the graph features, which in this case are similarities and distances among movies.

In \cref{experiments}, we compare the performance of NNMF and other approaches on benchmarks including link prediction in graphs, as well as collaborative filtering in movie rating datasets.
In our experiments, NNMF dominated other latent feature methods, as well as the I-RBM model.  However, NNMF was dominated by both LLORMA and I-AutoRec.
One possibility is that a different approach to learning the underlying neural network would deliver results on par with these methods. 
Another possibility is that the difference reflects some fundamental limitation of latent feature models, which assume that the ratings are conditionally independent given the latent feature representations. 
Local graph structure may contain information that would aid in predicting ratings.
In particular, NNMF does not learn from the pattern of missing ratings,which can reveal information about a user or movie: e.g., a user might tend only to give ratings when those ratings are extreme, and movies with low ratings are less likely to be viewed in the first place.
In contrast to NNMF, both LLORMA and AutoRec could, in principle, be taking advantage of the information latent in the pattern of missing ratings, although the strength of this effect has not been studied.  In LLORMA, the sparsity pattern affects the notion of locality.  In AutoRec, the entire pattern of ratings is fed as input, although the sparsity is obscured somewhat by missing entries being replaced by 3's.

Some recent work by \citet{LHG14} demonstrates that explicitly modeling the non-random pattern of missing ratings can lead to a slight improvement in performance for latent feature models, although the gains they demonstrated were not dramatic enough that they would have closed the gap between NNMF and LLORMA/AutoRec.
Indeed, we implemented a neural architecture similar in spirit to theirs, but were only able to improve the RMSE score by approximately .   A more careful analysis would be necessary to make more definitive conclusions. 











\begin{table}[t]
\centering
\begin{tabular}{lllll}
           & NIPS  & Protein & ML100k & ML1m    \\
&           \\
Vertices X & 234   & 230     & 943    & 6040    \\
Vertices Y & -   & -       & 1682   & 3900    \\
Edges      & 27144 &   52900      & 100000 & 1000209
\end{tabular}
\caption{Data sets and their dimensions.  The mark ``-'' highlights that the array is square.}
\label{datasetsinfo}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{lllll}
              & NIPS  & Protein & ML-100K \\
              & \\
RFM (3)          & 0.110 & 0.136   & -           \\  PMF (3)          &  0.130 &  0.139   & -     \\  PMF (60)          & 0.062 & 0.104   & 0.952    \\  BiasedMF  (60)     &  0.065    & 0.111        & 0.911  \\   NTN (60)               & 0.048 & 0.071 & 0.910 \\ \vspace*{-.75em}& \\      
NNMF (3HL)          & 0.040 & 0.065   & 0.907 \\    NNMF (4HL)          & - & -  & 0.903    \\ & 
\\ & \\
\end{tabular}\ \ \ \ 
\begin{tabular}{lllll}
              & ML-1M \\
              & \\
PMF (60)               &  0.883 \\ LLORMA-GLOBAL    & 0.865 \\  I-RBM                        &0.854 \\   BiasedMF (60)     & 0.852 \\  NTN (60)                   & 0.852  \\ LLORMA-LOCAL       & 0.833 \\  I-AutoRec                  & 0.831 \\  \vspace*{-.75em}& \\      
NNMF (3HL)                        & 0.846   \\ NNMF (4HL)			& 0.843
\end{tabular}
\caption{Results across the four data sets for a variety of techniques.  
The token  specifies that a rank- factorization was used. 
The token  specifies that  hidden layers were used. Scores reported for RFM and PMF (3) are taken from \citep{RFMLloyd}.  Scores for BiasedMF were obtained using LibRec \citep{LibRec}. Scores for LLORMA were taken from  \citep{LLORMA}, AutoRec and RBM, were taken from \citep{AutoRec}.}
\label{resultstable}
\end{table}



\section{Experiments}
\label{experiments}

We evaluated NNMF on two graph datasets (NIPS and Protein) and two collaborative filtering datasets (MovieLens 100K and 1M).  See \cref{datasetsinfo} for more information about the datasets.

NNMF, NTN, PMF model performance was evaluated on 5 randomly subsampled test sets, each comprising  of the data points, and then averaged. 
The remaining  of the data was split into training and validation sets:
For the graph datasets, we used a  of the training data for validation.
Due to the larger size of collaborative filtering datasets, we used  and  of the training data for validation on the MovieLens 100K and 1M datasets, respectively. These numbers were chosen to make the Monte Carlo error of the validation set estimate sufficiently small. \NA{(It is likely that results could be improved by better use of the training and validation data.)}

The regularization parameter, , and optimal stopping time were chosen by optimizing the error on the validation set.  For every fixed setting of , the network and features were learned by optimizing
\cref{objfunc} as described in \cref{learning}.

For simplicity, and to avoid the pitfalls of choosing parameters that produce good test set performance, 
the number and dimensionality of the features, as well as the network architecture, were fixed across experiments. It is conceivable that cross validating these parameters would have yielded better results.  
On the other hand, it would be wise to employ safeguards \citep{Dwork07082015} before embarking on an adaptive search for better architectures, learning rates, activation functions, etc.

We chose  feature dimensions to be preprocessed by an element-wise product, and included  additional features for each user and each movie. 
The feed-forward neural network was chosen to have 3 hidden layers with 50 sigmoidal units each.
The network weights were sampled uniformly in , where
,  denote the number of inbound and outbound connections.
The latent features were randomly initialized from a zero-mean Gaussian distribution with standard deviation .  
The features and weights were learned by gradient descent, and RMSPROP was used to adjust the learning rate, which was initialized to  for NIPS, Protein, and ML-100K, and to  for ML-1M.

To train the PMF model, we chose 60 dimensions after evaluating the performance of PMF with various choices for the dimensionality and finding that this worked best.  On each run, the regularization parameter was chosen from a large range by optimizing the validation error.  (We tried many other settings for PMF, and have reported the best numbers we obtained here to make the comparison conservative.) 

NTN model hyperparameters were chosen to match NNMF ones---we used 60-dimensional latent features, and  units in the hidden layer. This setup yields a third order tensor with  entries. Compared to the network underlying NNMF, a NTN of approximately the same size has roughly 20 times more parameters. The model was trained with gradient descent on the same objective function as for NNMF. We had to use mini-batches for the MovieLens 1M dataset to avoid memory issues. Just as for other models, we chose the regularization parameter  by optimizing it the error on the validation set. Note, that in the original NTN model was trained with a contrastive max-margin objective function with  regularization of all parameters. We applied a sigmoid nonlinearity to the output layer of the original NTN, to ensure that its outputs fell in .

The results appear in \cref{resultstable}.  As mentioned above, NNMF dominates PMF, RFM, and to a lesser extent NTN.  In \citep{RFMLloyd}, the performance of RFM is compared with PMF when both models use the same number of latent dimensions.  The performance of PMF, however, tends to improve with the higher dimension, assuming proper regularization, and so RFM (3) is seen here to perform worse than PMF (60).  It is possible that recent advances in Gaussian process regression could in turn improve the performance of RFM.   

NNMF outperforms BaisedMF, although the margin narrows as we move to the sparsely-observed MovieLens datasets. We note that adding bias correction terms to NNMF also improves the performance of NNMF, although the improvement is on the order of , and so may not be robust. It is also possible that using more of the training data might widen the gap.  

NNMF beats the (low-rank) global version of LLORMA, but not the local version that relaxes the low-rank constraint.  NNMF is also bested by AutoRec.    It is also not clear if we could have reliably found much better network weights and features had we made different choices around the architecture, composition, and training of the neural network.
Given that NNMF dominates PMF so handily on the graph datasets, it might stand to reason that there is a lot of room for improvement on MovieLens through better engineering of NNMF.  
It is worth noting that a `local' versions of NNMF could be developed along the same lines as were for LLORMA. Given that NNMF dominates PMF, it might then also stand to reason that a local version of NNMF would dominate LLORMA, because LLORMA can be understood as a local version of PMF.

To see whether deeper networks performed better on the collaborative filtering datasets, we also evaluated NNMF on the MovieLens data sets using a 4 hidden layer network.  We observed that fewer units per layer yielded better results. (We compared 50 units per layer when  to 20 units per layer when .) However, to draw any conclusions, more experiments would be needed, with care to avoid overfitting. We reported scores for 4 hidden layer networks, with 20 units per hidden layer, and  latent feature dimensions.
We believe that adding additional layers would likely improve the results, though we suspect the performance would saturate quickly (and then drop if we did not very carefully initialize and regularize the network).


\section{Discussion}

NNMF achieves state-of-the-art results among latent feature models, but is dominated by approaches that take into account local graph structure.  However, it is possible that our experiments have not identified the limits of the NNMF model.  It is difficult to exhaustively explore the range of network architectures, activation functions, regularization techniques, and cross-validation strategies.  Even if we could explore them all, we would be in danger of overfitting and losing any hope of insight into the usefulness of NNMF.  Indeed, we erred towards not trying to optimize over the model's many possible configuration.  It would be interesting to apply recent advances in adaptive estimation to control the possibility of overfitting during this phase of designing and evaluating a new model \citep{Dwork07082015}.









\section*{Acknowledgments}

The authors would like to thank Zoubin Ghahramani for feedback and helpful discussions.



\bibliographystyle{abbrvnat}
\bibliography{biblio}





\vfill

\end{document}
