
Experimental results in Section~\ref{sec:towardHybrid} establish that \dpl is the best performing interactive method; however, it still under-performs EUGkM.  Recall that EUGkM publishes a private synopsis of the the dataset, and thus enables other analysis to be performed on the dataset, beyond -means.  This means that currently the non-interactive method has a clear advantage over interactive methods, at least for -means clustering.  

An intriguing question is ``Whether EUGkM is the best we can do for -means clustering?'' In particular, can we further improve \dpl?  Recall that there are two key issues that greatly affect the accuracy of \dpl: the number of iterations and the choice of initial centroids.  In fact, these two are closely related.  If the initially chosen centroids are very good and close to the true centroids, one only needs perhaps one more iteration to improve it, and this reduction in the number of iterations would mean little noise is added.  Now if only we have a method to choose really good centroids in a differentially private way, then we can use part (e.g., half) of the privacy budget to get those initial centroids, and the remaining privacy budget to run one iteration of \dpl to further improve it.  

In fact, we do have such a method.  EUGkM does it.  This leads us to propose a hybrid method that combines non-interactive EUGkM with interactive \dpl.  We first use half the privacy budget to run EUGkM, and then use the centroids outputted by EUGkM as the initial centroids for one round of \dpl.  Such a method, however, may not actually outperform EUGkM, especially when the privacy budget  is small, since then one round of \dpl may actually worsen the centroids.  Therefore, when  is small, we should stick to the EUGkM method, and only when  is large enough should we adopt the EUGkM+\dpl approach.  In order to determine what  is large enough, we analyze how the errors depend on the various parameters in \dpl and in EUGkM.



\begin{comment}
The standard Lloyd's algorithm is sensitive to initial centroids. Given a set of  initial centroids, it converges to local optimum. There is no guarantee that a global optimum is reached. One effective heuristic to improve the Lloyd's algorithm is trying out multiple sets of initial centroids, and choose the output with the minimum NICV. Figure \ref{fig:convergence-rate} reports this improvement. The initial centroids generated in Section \ref{sec:existingAppExpt} are reused. The Lloyd-Best approach, which is the standard Lloyd's algorithm but takes the set of initial centroids that most favors its accuracy, clearly outperforms the Lloyd approach. A good setting of initial centroids also improves the accuracy of the \dpl method as shown in Figure \ref{fig:synthetic_data_heatmap} (a) and (b), where the \dpl-best method is more scalable with respect to both  and  than the \dpl method. The reason behind is that \dpl is easily to converge to local optimal because of the initial centroids. Still, because of the limited privacy budget, \dpl cannot try as many sets of initial centroids as needed to select one that optimizes its accuracy.


The non-interactive approach runs the \km clustering on differentially private synopses. Thus, it has the freedom of testing as many starting points as needed without violating \difp. This is an advantage of the non-interactive approach over the interactive approach. This is also one of the reasons that \eugkm outperforms \dpl in some cases (Figure \ref{fig:non-interactive_VS_interactive}). However, as the privacy budget and the dimensionality increase, \dpl overtakes \eugkm. If one's goal is to optimize the performance of the private \km clustering, then it is better to combine \eugkm and \dpl to take the best out of them. We thus propose a hybrid approach. It uses \eugkm to try multiple initial centroids, and takes the best output as the initial centroids to \dpl for further refinement. There are challenges to design the hybrid approach. They include
\begin{itemize}
  \item How to study the error behavior of \dpl method and the \eugkm method?
  \item When is it beneficial to apply the hybrid approach? That is, on what conditions the hybrid approach overtakes the \eugkm method and also the \dpl method?
\end{itemize}

In this section, we analyze the error behavior of \dpl and non-interactive approaches. The analysis motivates us to develop a hybrid scheme that combines \eugkm and \dpl.
\end{comment}



\subsection{Error Study of \dpl}\label{sec:dpl_Analysis}

DPLloyd adds noises to each iteration of updating centroids.  To study the error behavior of DPLloyd due to the injected Laplace noises, we focus on analyzing the mean squared error (MSE) between noisy centroids and true centroids in one iteration.  


Consider one centroid and its update in one iteration.  The true centroid's 'th dimension should be , where  is the number of data points in the cluster and  is the sum of 'th dimension coordinates of data points in the cluster.  Consider the noisy centroid ; its 'th dimension is , where  is the noise added to the count and  is the noise added to the .  The MSE is thus:




Derivation based on the above formula gives the following proposition.

\begin{proposition}\label{thm:DPLMSE}
In one round of \dpl, the MSE is

\end{proposition}

\begin{proof}
Let us first consider the MSE on the -th dimension.

The last step holds, because  and  are independent zero-mean Laplacian noises and the following formulas hold:

where  and  are the variances of  and , respectively. 

Suppose that on average , where  is the range of the 'th dimension. That is,  is the normalized coordinate of -th dimension of the cluster's centroid. Furthermore, suppose that each cluster is about the same size, i.e., . Then,  can be approximated as follows:

\dpl adds to each sum/count function Laplace noise . Therefore, both  and  are equal to . From Equation (\ref{eqn:DPLMSEoi}) we obtain

As the noise added to each dimension is independent, from Equation \ref{eq:DPLMSE} we know that the MSE is

When  is a small constant, this becomes .
\end{proof}

Proposition~\ref{thm:DPLMSE} shows that the distortion to the centroid proportional to , while  inversely proportional to . At first glance, this analysis seems to conflict with the experimental result in Figure \ref{fig:synthe-heatmap} (a), where \dpl is much less scalable to  than to . The reason behind is that the performance of \dpl is also affected by the fact that  rounds are not enough for it to converge.  When  increases, converging takes more time, and it is also more likely that choices of initial centroids lead to local optima that are far from global optimum. 
 




\begin{comment}


\dpl runs  iterations.  Within each round, the privacy budget needs to be divided among the count and the  sum queries.  Suppose  is allocated to the count query, and  is allocated to the sum query for the -th dimension, for each .
While all dimensions should be treated equally, i.e., , an interesting question is what should be the right ratio of .  This question has not answered before.
The \dpl approach allocates the privacy budget according to the sensitivities of different queries; thus , assuming that each dimension is normalized to .  Thus, different  values will result in different allocations of privacy budget.


This, however, is not optimal.  Plugging  and  into Equation~\ref{eqn:DPLMSEoi}, one obtains

Minimization of the above \text{Subject to }  can be solved using
the method of {\em Lagrange multipliers}.  The optimal proportion is



The optimal proportion improves the MSE of \dpl. Plugging it back in Equation \ref{eqn:DPLMSEsumOi}, it follows that

which is consistently lower than the MSE in Equation \ref{eqn:DPLMSE_approx} developed for the original \dpl.

We use \dplscopt to represent the \dpl method that has the above improved privacy budget allocation between sum and count functions. \todo{Experiments to show the improvements...}
\end{comment}

\subsection{Error Study of EUGkM}\label{sec:non_inter_analysis}

Non-interactive approach partitions a dataset into a grid of  uniform cells. Then, it releases private synopses for the cells, and runs \km clustering on the synopses to return the cluster centroids. 
Similar to the error analysis for \dpl, we analyze the MSE. Let  be the true centroid of a cluster, and  be its estimator computed by a non-interactive approach. The MSE between  and  is composed of two error sources. First, the count in each cell is inaccurate after adding Laplace noise. This results in the variance (i.e., ) of  from its expectation . Second, we no longer have the precise positions of data points, and only assume that they occur at the center in a cell. Thus, the expectation of  is not equal to , resulting in a bias (i.e., ). The MSE is the combination of these two errors.




\mypara{Analyzing the variance.} We assume that each cluster has a volume that is  of the total volume of the data space, and has the shape of a cube. In -dimensional case, the width of the cube is . Suppose that the {\em geometric} center\footnote{Note that this is not the cluster centroid.} of the cube is . Let  be the set of cells included in the cluster. For each cell , we use  to denote the number of tuples in ,  to denote the 'th dimension coordinate of the center of cell , and  to denote the noise added to the cell size.  Let  be the -th dimension of the noisy centroid. Then, the variance of  is

In the above, the first step follows because  as the cube geometric center is a constant. The last step is derived by assuming , that is, the noisy cluster size is approximately equal to the original cluster size .

We can see that within the cube, different cells' contribution to the variance is not the same.  Basically, the closer a cell is to the cube center, the less its contribution. The contribution is proportional to the squared distance to the cube center. We thus approximate the variance as follows:

In the above integral,  in the first term is the distance from a cell center to the cube center (i.e., ). The second term  is the number of cells per unit volume, and  is the volume of the -dimensional plane that has a distance of  to the cube center. The last term  is the variance of the cell size (i.e., ). Suppose that clusters are of equal size, that is, . Then, the variance of the noisy centroid by summing all the  dimensions  is


The analysis shows that the variance of the \eugkm is proportional to .  \eugkm sets  to .  Plugging it into Equation~\ref{eqn:nonInter_variance}, we get that the variance of \eugkm is \emph{inversely proportional} to .

\begin{comment}
The analysis shows that the variance of non-interactive approach is proportional to .
\eugkm sets  to , and MkM sets , respectively. 
Plugging them into Equation~\ref{eqn:nonInter_variance}, it follows that the variance of \eugkm is \emph{inversely proportional} to .
  while that of \mkm is \emph{inversely proportional} to . This gives a reason why \eugkm performs much better than \mkm for small  values (Figure \ref{fig:non-interactive_VS_interactive}). \end{comment}
\mypara{Analyzing the bias}.
Let  be the 'th dimension coordinate of a tuple . Then, the bias of  is

where the last step is developed by approximating  to the cluster size .

The bias developed in the above formula is dependent on data distribution. Its precise estimation requires to access real data. We thus only estimate its upper bound. Let . Non-interactive approach partitions each dimension into  intervals of equal length. Hence,  falls in the range of , and the upper bound of  is . Summing all the  dimensions, we obtain the upper bound of squared bias of noisy centroid



The estimation shows that the upper bound of squared bias decreases as a function of . This is consistent with the expectation. As  increases, the data space is partitioned into finer-grained cells. Therefore, the distance between a tuple in a cell to the cell center decreases on average.


\begin{comment}
\mypara{Analyzing the bias}.
Let  be the 'th dimension coordinate of a tuple . Then, the bias of  is

where the last step is developed by approximating  to the cluster size . The bias developed in the above formula is dependent on data distribution. A precise estimation of its contribution to MSE (i.e.,  in Equation \ref{eqn:mse}) requires to access real data. In the following, we estimate the contribution by analyzing the variance and expectation of .


Let . Non-interactive approach partitions each dimension into  intervals of equal length. Thus,  falls in the range of . For the simplicity, suppose that  is uniformly distributed in a cell, cells in a cluster are of equal size, and each cluster has the same number of cells. Then, , , and . The variance of  reaches its lower bound, if all the 's are independent within a cluster. On the other hand, if 's are highly correlated, then the variance reaches its upper bound. In empirical study, we found that the variance is in the middle between the two \mbox{bounds}. Therefore, we estimate the variance of 

where  is the number of cells in a cluster,  is the cell size, and  is a parameter falling in . The lower and upper bounds of  correspond to the lower and upper bounds of the variance. Empirically, we set  that estimates the variance well. This  setting is the same as the assumption---tuples (or 's) within a cell are highly correlated, while tuples across cells are independent.





On the basis of the variance and the expectation, we have

Sum all the  dimensions, and we estimate the squared bias of noisy centroid


The estimation shows that the upper bound of squared bias decreases as a function of . This is consistent with the expectation. As  increases, the data space is partitioned into finer-grained cells. Therefore, the distance between a tuple in a cell to the cell center decreases on average.


\mypara{MSE of non-interactive approach.} Combining the variance in Equation \ref{eqn:nonInter_variance} and squared bias in Equation \ref{eqn:nonInter_bias2}, we estimate the MSE (Equation \ref{eqn:mse}) of noisy centroid to


The choice of  is up to specific non-interactive approach. Given a dataset of  tuples and privacy budget , the \ugkm algorithm sets  to .

\end{comment}

\begin{comment}
\begin{proposition}\label{thm:ugkm_mse}
The MSE of cluster centroids output by the \ugkm algorithm is

\end{proposition}

\begin{proof}
\ugkm partitions the data space into  grid of equal size. Substitute the  value for that in Equation \ref{eqn:nonInter_mse}, and we have:

Since , the MSE of cluster centroids for small  is

\end{proof}


In a similar way, according to the choice of , we also develop the proposition for the \mkm algorithm.

\begin{proposition}\label{thm:mkm_mse}
The MSE of cluster centroids outputted by the \mkm algorithm is

\end{proposition}

\begin{proof}
\mkm partitions the data space into  grid cells of equal size. Substitute the  value for that in Equation \ref{eqn:nonInter_mse}, and we have:
\small{}
Since , the MSE of cluster centroids for small  is

\end{proof}

\mypara{Comparing \dpl, \ugkm and \mkm.} Now let us consider Proposition \ref{thm:DPLMSE}, Proposition \ref{thm:ugkm_mse} and Proposition \ref{thm:mkm_mse} together. We can see that the MSE of \dpl is inversely proportional to , and those of \ugkm and \mkm are inversely proportional to  and , respectively. This explains why the NICV of \dpl (\mkm) drops much faster than that of \ugkm when  grows. It also explains why \dpl does not overtake \ugkm on `small' dataset S1, but outperforms \ugkm on `big' dataset Tiger.

\note{The effect of dimensionality cannot be explained.}
\end{comment}



\begin{comment}
\mypara{Comparing \ugkm with \mkm.} The difference of the two non-interactive methods is the choice of . The \ugkm method sets it to , and the \mkm method sets it to . Figure \ref{fig:non-interactive_VS_interactive} shows that the performance of \ugkm is superior to that of \mkm. The following gives two possible reasons.

First, \mkm does not take  as a factor in . Thus, it is nonadaptive to the variation of . Plugging the  values into Equation \ref{eqn:nonInter_variance}, we know that the variance of \ugkm is inversely proportional to , while that of \mkm is inversely proportional to . This explains why \ugkm performs much better than \mkm for small  values (Figure \ref{fig:non-interactive_VS_interactive}), and it also explains why the NICV of \mkm drops faster than that of \ugkm.

Second, as the dimensionality increases, the number of cells generated by \mkm is far much more than the dataset size. When the dimensionality is big enough, \mkm generates approximately  cells. Thus, most cells are empty. After adding Laplace noise, they turn into random noises, and result in poor performance of k-means clustering. \todo{This explains why the \ugkm is superior to \mkm on the 6D Adult dataset.}
\end{comment}

\mypara{Comparing \dpl and \eugkm.} We now analyze the performance of \dpl and \eugkm in Figure \ref{fig:non-interactive_VS_interactive}. Equation \ref{eqn:DPLMSE_approx} shows that the MSE of \dpl is inversely proportional to . The MSE of \eugkm consists of variance and squared bias. Plugging  into Equation \ref{eqn:nonInter_variance} and Inequality \ref{eqn:nonInter_bias2},
it follows that the MSE of \eugkm is inversely proportional to . This explains why the NICV of \dpl, which is inversely proportional to  drops much faster than that of \eugkm as  grows. It also explains why \dpl has better performance on `big' dataset (e.g., the TIGER dataset).


The MSE of \eugkm is inversely proportional to . Thus, it increases exponentially as a function of . Instead, from Equation \ref{eqn:DPLMSE_approx}, it follows that the MSE of \dpl has only cubic growth with respect to . Therefore, in Figure \ref{fig:non-interactive_VS_interactive}, as the dimensionality of dataset increases, \dpl outperforms \eugkm. This also explains in Figure \ref{fig:synthe-heatmap}  why \dpl is more scalable to  than \eugkm.


\begin{comment}


In~\cite{QYL12}, \ug chooses  to optimize accuracy for range queries.  In this section, we investigate how to choose  to optimize for -means clustering.
To minimize the MSE value in Equation (\ref{eqn:nonInter_mse}), we set the derivative of the equation with respect to  to 0

which results in


Based on the  value developed above, we divide the data space into a  grid of equal size, where . We denote this new non-interactive approach by \augkm.

\begin{proposition}\label{thm:augkm_mse}
The MSE of the cluster centroids output by \augkm algorithm is approximately

\end{proposition}

\begin{proof}
Substituting the tailored value of  in Equation \ref{eqn:M_adaptiveUG} for that in Equation \ref{eqn:nonInter_mse}, we have

Therefore, when  is small, the MSE of the cluster centroids output by \augkm is

\end{proof}
\end{comment}

\begin{comment}
\begin{figure*}[p]
	\begin{tabular}{cc}
	\includegraphics[width = 3.25in]{s1-performance.eps} & \includegraphics[width = 3.25in]{s1-performance-closeup.eps}\\
	(a) S1 dataset & (b) S1 dataset, close-up at large \\
	\includegraphics[width = 3.25in]{gowalla-2d-performance.eps} & \includegraphics[width = 3.25in]{gowalla-2d-performance-closeup.eps}\\
	(c) Gowalla 2D dataset & (d) Gowalla 2D dataset, close-up at large \\
\includegraphics[width = 3.25in]{gowalla-3d-performance.eps} & \includegraphics[width = 3.25in]{gowalla-3d-performance-closeup.eps}\\
	(e) Gowalla 3D dataset & (f) Gowalla 3D dataset, close-up at large \\
	\end{tabular}
	\caption{The comparison of four algorithms proposed in this paper, by varying }\label{fig:exp-vary-epsilon}
\end{figure*}

\begin{figure*}[p]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[scale=2]{regression-all-in-one.eps}
\caption{The linear regression for empirically finding the  value.}\label{fig:regression}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width = 3.25in]{ds1.10-scalability.eps}\caption{The comparison of the UGkM and DPLloyd with varying dimensions.}\label{fig:exp-vary-N-dimension}
\end{minipage}
\end{figure*}
\end{comment}

\subsection{The Hybrid Approach}\label{ssec:hybrid}

Our hybrid approach combines \eugkm and \dpl. Given a dataset and privacy budget , the hybrid approach first \mbox{checks} whether it overtakes the \dpl method and also the \eugkm method. If this is not the case, the hybrid approach simply falls back to \eugkm. Otherwise, the hybrid approach allocates half privacy budget to \eugkm to output a synopsis and find  intermediary centroids that work well for the synopsis. Then, it runs \dpl for one iteration using the remaining half privacy budget to refine these  centroids.

We use MSE to heuristically determine the conditions, on which the hybrid approach overtakes the \dpl method and also the \eugkm method. Basically, we require that the MSE of the hybrid approach be smaller than those of the other two approaches, since smaller MSE implies smaller error to the cluster centroid.
From Equation \ref{eqn:DPLMSE_approx}, it follows that the MSE of \dpl with full privacy budget is

A precise estimation of the MSE of the \eugkm method requires to access the dataset, since the bias depends on the real data distribution. However, we have the approximate variance (Equation \ref{eqn:nonInter_variance}) by setting .

One-iteration \dpl with half privacy budget outputs the final  cluster centroids, if it is applied in the hybrid approach. Therefore, we approximate the MSE of the hybrid approach by that of the one-iteration \dpl

which is developed by setting  and privacy budget to  in Equation \ref{eqn:DPLMSE_approx}.

Comparing Formulas \ref{formula:DPLopt_MSE_tround} and \ref{formula:dplmse_1round}, it follows that the MSE of the hybrid approach is lower than or equal to that of the \dpl if


Variance is the lower bound of MSE. Thus, if the MSE of the hybrid approach is equal to or smaller than the variance of the \eugkm method, then it is sure that the hybrid approach has lower MSE. Setting Formula \ref{formula:dplmse_1round} smaller than or equal to Formula \ref{formula:eugkm_var} yields

where

and


Inequalities \ref{eqn:alpha} and \ref{eqn:boundaryEpsilon} give the conditions of applying the hybrid approach. Inequality \ref{eqn:alpha} is automatically satisfied since \dpl runs for  iterations.



\begin{comment}
\subsection{Improved Hybrid Approach}



Let 
 
and 



Optimze  over  by taking the first derivative of ,


Setting , we have 



Therefore,  has a global minimum. 

Two iterations case:







We have 

and 

therefore,  has a global minimum.  

\end{comment}



\begin{comment}


\mypara{The conditions.} We use MSE to heuristically determine the conditions, on which the hybrid approach overtakes the \dpl method and also the \eugkm method. From Equation \ref{eqn:DPLMSE_approx}, it follows that the MSE of \dpl with full privacy budget is

It immediately follows from Equation \ref{eqn:augkm_mse} that the MSE of \eugkm with full privacy budget is

One-iteration \dpl with privacy budget  outputs the final  cluster centroids, if it is applied in the hybrid approach. Therefore, we approximate the MSE of the hybrid approach by that of the one-iteration \dpl

which is developed by setting  and privacy budget to  in Equation \ref{eqn:DPLMSE_approx}.

MSE measures the error to a cluster centroid. Smaller MSE value indicates lower error. We thus develop the conditions on which the hybrid approach overtakes the other two approaches. To be more precise, we require that the MSE of the hybrid approach is at most  times as big as that of \dpl (and \eugkm). Comparing Formulas \ref{formula:DPLopt_MSE_tround} and \ref{formula:dplmse_1round}, it follows that the requirement holds if

Setting Formula \ref{formula:dplmse_1round} at most  times as big as Formula \ref{formula:augkm_mse} yields

where

and


Inequalities \ref{eqn:alpha} and \ref{eqn:boundaryEpsilon} give the conditions of applying the hybrid approach. In the experiments, we set .

\mypara{The  value.}  The hybrid approach allocates  privacy budget to \augkm and the remaining to one-iteration \dpl, when the conditions developed above hold. The one-iteration \dpl outputs the final centroids. So enough privacy budget needs to be allocated to it to optimize the accuracy. However, if the intermediary  centroids found by \augkm is of low quality, then the refinement on them by \dpl is still limited. Therefore, the  value needs to be well decided. Again, we use MSE for this task. We require that the MSE of \augkm should not be too big, when compared with one-iteration \dpl. Plugging privacy budget  in Equation \ref{eqn:nonInter_variance}, it follows that the MSE of \augkm is

We determine the  value by setting the MSE of \augkm to be at most  times as big as the MSE of one-iteration \dpl in Formula \ref{formula:dplmse_1round}, where  is empirically computed.
\end{comment}

\begin{comment}
Our hybrid approach combines \ugkm and \dplscopt. Given a dataset and privacy budget , the hybrid approach first checks whether it overtakes the \dplscopt method and also the \ugkm method. If this is not the case, the hybrid approach simply falls back to \ugkm. Otherwise, the hybrid approach allocates privacy budget  () to \ugkm to output a synopsis and find  intermediary centroids that work well for the synopsis. Then, it runs \dplscopt for one iteration using the remaining privacy budget  to refine these  centroids.

\mypara{The conditions.} We use MSE to heuristically determine the conditions, on which the hybrid approach overtakes the \dplscopt method and also the \ugkm method. Basically, we require that the MSE of the hybrid approach be smaller than those of the other two approaches, since smaller MSE implies smaller error to the cluster centroid. From Equation \ref{eqn:improvedDPLMSE}, it follows that the MSE of \dplscopt with full privacy budget is

A precise estimation of the MSE of the \ugkm method requires to access the dataset, since the bias depends on the real data distribution. However, we have the approximate variance (Equation \ref{eqn:nonInter_variance})

One-iteration \dplscopt with privacy budget  outputs the final  cluster centroids, if it is applied in the hybrid approach. Therefore, we approximate the MSE of the hybrid approach by that of the one-iteration \dplscopt

which is developed by setting  and privacy budget to  in Equation \ref{eqn:improvedDPLMSE}.

Comparing Formulas \ref{formula:DPLopt_MSE_tround} and \ref{formula:dplscopt_1round}, it follows that the MSE of the hybrid approach is lower than or equal to that of the \dplscopt if


Variance is the lower bound of MSE. Thus, if the MSE of the hybrid approach is equal to or small than the variance of the \ugkm method, then it is sure that the hybrid approach has lower MSE. Setting Formula \ref{formula:dplscopt_1round} smaller than or equal to Formula \ref{formula:ugkm_var} yields

where

and


Inequalities \ref{eqn:alpha} and \ref{eqn:boundaryEpsilon} give the conditions of applying the hybrid approach.

\mypara{The  value.}  The hybrid approach allocates  privacy budget to \ugkm and the remaining to one-iteration \dplscopt, when the conditions developed above hold. The one-iteration \dplscopt outputs the final centroids. So enough privacy budget needs to be allocated to it to optimize the accuracy. However, if the intermediary  centroids found by \ugkm is of low quality, then the refinement on them by \dplscopt is still limited. Therefore, the  value needs to be well decided. Again, we use MSE for this task. We require that the MSE of \ugkm should not be too big, when compared with one-iteration \dplscopt. The MSE of \ugkm is dependent on real data distribution. We thus consider its variance. Intuitively, if the MSE of \ugkm is not too big, then its variance should not be big either. Plugging privacy budget  in Equation \ref{eqn:nonInter_variance}, it follows that the variance of \ugkm is

We determine the  value by setting the variance of \ugkm to be at most \todo{x} times as big as the MSE of one-iteration \dplscopt in Formula \ref{formula:dplscopt_1round}, where  is empirically computed.
\end{comment}

\begin{comment}
We have found that when  is small, non-interactive -means clustering approaches such as \ugkm outperforms interactive \dpl; however, when  is large, \dpl performs better.   We observe that if one's goal is to optimize performance for -means clustering as much as possible, then one should use a hybrid approach.  We propose the following novel hybrid approach that can combine the best of both the non-interactive \ugkm and the interactive \dpl.

Given a dataset and privacy budget , the hybrid approach first checks whether  is large enough to benefit from a hybrid approach.  If  is small, the hybrid approach simply falls back to \ugkm. If  is large enough, the hybrid approach allocates privacy budget  () to \ugkm to output a synopsis, and finds  centroids that work well for the synopsis. Then, it runs \dpl for one iteration using the remaining privacy budget  to refine the  centroids. \dpl generates the final output, thus enough privacy budget needs to be allocated to it. However, if the accuracy of the intermediate output by \ugkm is too low, then the refinement by \dpl would be limited. Therefore, the  value needs to be well decided. We use MSE formulas for this purpose. \note{variance is used only for \ugkm} To ensure that the accuracy of \ugkm is reasonable, we require that the variance of \ugkm is not too big, when compared with that of \dpl. In particular, we know that the variance of \ugkm (by Equation \ref{eqn:nonInter_variance}) with privacy budget  is

and that of \dpl (by Equation \ref{eqn:improvedDPLMSE}) with privacy budget  is

By setting the former \todo{x} times as big as the latter, we determine the  value. Correspondingly, we also determine the budget allocation between the two steps.

One issue in the hybrid approach is to determine a sufficient privacy budget threshold , such that when  is larger than it, the hybrid approach would switch from \ugkm to \ugkm plus one round of \dpl. Again, we use MSE to determine this threshold. Equation \ref{eqn:nonInter_variance} gives the variance of \ugkm when it uses the full privacy budget. \note{the variance can be seen as the lower bound of the MSE of \ugkm.}
One-round of \dpl outputs the final  cluster centroids, if it is applied in hybrid approach. Thus, we approximate the MSE of hybrid approach with one-round of \dpl by that given in Equation \ref{formula:dpl_1round}. By setting the MSEs in the two equations to be equal, we obtain the threshold:

where


and

\end{comment}



\subsection{Experimental results}

\begin{comment}
We now compare the hybrid approach with \dpl and \mbox{\eugkm} using both synthetic and real datasets. When generating the synthetic datasets, we fix the dataset size to 10,000, and vary  and  from 2 to 10. For each dataset,  well separated Gaussian clusters of equal size are generated, and 30 sets of initial centroids are generated in the same way as in Section \ref{sec:existingAppExpt}. Since the real centroids of the Gaussian clusters are known, we include them as one set of initial centroids. The baseline NICV is computed using the real centroids. We configure \dpl and \eugkm in the same way as in Section \ref{sec:existingAppExpt}. We also include \dpl-best as a benchmark for \dpl. Its configuration is the same as \mbox{\dpl}, except that it takes only the real cluster centroids as the initial centroids. For the hybrid approach, we run one-iteration \dpl 100 times on the intermediary centroids output by \eugkm, and consider the average NICV value. Furthermore, we compute the difference of NICV between each approach and the baseline. The difference allows to investigate the scalability of each approach with respect to the baseline, when we vary the parameters of ,  and . We use \nicvminus to represent this difference of NICV.




Figure \ref{fig:synthe} reports the results. We first fix , and study the effect of  and  (the first row in Figure \ref{fig:synthe}). Clearly, the hybrid approach outperforms the other three approaches. It has the same \nicvminus as \eugkm on 2-dimensional datasets when  is 5 and 10. This happens, since the hybrid approach falls back to \eugkm on these two points. As expected, \dpl-best has lower \nicvminus than \dpl in all the cases, since it takes real centroids as the initial centroids. However, it is still inferior to the hybrid approach. This shows that running \dpl 5 iterations on real centroids only adds noise.
Our analysis shows that the MSE of \dpl is proportional to  while that of \eugkm increases exponentially as a function of . Therefore, when the dimensionality increases, the performance of \eugkm declines (i.e., \nicvminus increases) faster than those of \dpl and \dpl-best. Then, in the second row of Figure \ref{fig:synthe}, we fix  and vary  and . Again, the hybrid approach is better than other approaches. The MSE of \dpl is inversely proportional of , while that of \eugkm is inversely proportional of . Thus, as  grows, the performance of \dpl improves (i.e., \nicvminus decreases) faster than that of \eugkm. Finally, in the last row of Figure \ref{fig:synthe}, we fix  and vary  and . The hybrid approach outperforms the other approaches for . It falls back to \eugkm on the other two  values.


\discuss{Figure \ref{fig:synthe-heatmap} reports the experimental results using heatmap, where  is fixed to 1.0. According to Proposition \ref{thm:DPLMSE}, the MSE of \dpl is proportional to . Thus, as  and/or  increase, the performance of \dpl-best declines (i.e., \nicvminus increases), and the decline with  is more than that with  (Figure \ref{fig:synthe-heatmap} (a) ).  Figure \ref{fig:synthe-heatmap} (c) gives the heatmap for \eugkm. The MSE of \eugkm increases exponentially as a function of , while it increases linearly with . Thus, \eugkm is much more scalable to  than to . Comparing Figure \ref{fig:synthe-heatmap} (a), (c), and (d), it is clear that the hybrid approach is more scalable than \dpl-best and \eugkm with respect to both  and . This confirms the effectiveness of the hybrid approach. Figure \ref{fig:synthe-heatmap} (b) shows that \dpl is much less scalable to  than to . Still, this is not in conflict with our analysis of Proposition \ref{thm:DPLMSE}. The reason behind is that the performance of \dpl is also up to the initial centroids. When  increases, \dpl is more likely to converge to local optimal due to the initial centroids.}





\begin{figure*}[!htb]
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_k_2_vary_d.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_k_5_vary_d.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_k_10_vary_d.eps}\\
	(a) [, ] & (b) [, ]  & (c) [, ]
	\end{tabular}

	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/k_5_d_2_vary_eps.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/k_5_d_5_vary_eps.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/k_5_d_10_vary_eps.eps}\\
	(d) [, ] & (e) [, ]  & (f) [, ]
	\end{tabular}

	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_0.05_d_5_vary_k.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_0.2_d_5_vary_k.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_d_5_vary_k.eps}\\
	(g) [, ] & (h) [, ]  & (i) [, ]
	\end{tabular}

	\begin{center}
	\includegraphics[width = 5.0in]{figures/vldb/synthe-plugin-true/legend-cropped.eps}
	\end{center}
	\caption{Effects of the number of dimensions and the number of clusters}\label{fig:synthe}
\end{figure*}



\begin{figure*}[!htb]
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/dplloyd_med.eps} & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/dplloyd_min.eps} & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eugkm_nicv.eps}\\
	\hspace{-0.5cm}(a) DPLloyd-Median & \hspace{-0.5cm}(b) DPLloyd-Best  & \hspace{-0.5cm}(c) EUGkM
	\end{tabular}

	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/hybrid_med.eps} & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/pgkm.eps}  & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/gupt/synthe_eps_1.0.eps}\\
	\hspace{-0.5cm} (d) Hybrid & \hspace{-0.5cm} (e) PGkM & \hspace{-0.5cm} (f) GkM
	\end{tabular}


	\caption{Effects of the number of dimensions and the number of clusters, Heatmap}\label{fig:synthe-heatmap}
\end{figure*}

\end{comment}

We now compare the hybrid approach with \eugkm and \dpl. The configuration for \eugkm and \dpl is the same as in Section \ref{sec:existingAppExpt}.  For the hybrid approach, we run \eugkm 10 times to output 10 sets of intermediate centroids.  Then we run \dpl 10 times on each intermediate result.  We finally report the average of 100 NICV values. Figure \ref{fig:hybrid} gives the results on the six external datasets. In low dimensional datasets (S1, Gowalla, TIGER, and Image), the hybrid approach simply falls back to \eugkm for small  value. When  increases, both the hybrid approach and \eugkm converge to the baseline with the former having slightly better performance. For example, in the Gowalla dataset for , the average NICV of the hybrid approach is  and that of \eugkm is .


In higher dimensional datasets (Adult-num and Lifesci), the hybrid approach outperforms the other two approaches in most cases. It is worse than \dpl only for a few small  values, on which it falls back to \eugkm. There are two possible reasons. The first is that the MSE analysis assumes that datasets are well clustered and each cluster has equal size, but the real datasets are skewed. For example, the baseline approach partitions the Adult-num dataset into 5 clusters, in which the biggest cluster contains 13,894 tuples and the smallest contains 3,160 tuples. The second is that we use the variance of \eugkm as the lower bound of its MSE. Thus, it is possible that the MSE of the hybrid approach (approximated by the MSE of one-iteration \dpl with half privacy budget) is larger than the variance of \eugkm, but actually smaller than its MSE. In such cases, the hybrid approach gives lower NICV if it does not fall back to \eugkm. For example, on the Adult-num dataset for , the hybrid approach of falling back to \eugkm has the NICV of , while its NICV is , if it applies \eugkm plus one-iteration of \dpl.



\begin{comment}
In this section we experimentally compare the two non-interactive algorithms UGkM and AUGkM, the hybrid approach, and the \dpl.  Figure \ref{fig:hybrid} reports the results.  The Hybrid approach always outperforms the existing non-interactive algorithms and interactive algorithms over all datasets.  The advantage of it is more significant in the two high dimensional datasets, Adult-num and Lifesci.  Observe that the Hybrid approach has same performance as the AUGkM in the S1 dataset.  The reason is that the two-phase hybrid approach cannot improve the AUGkM result.  So we use one-phase hybrid approach, that is running AUGkM only.

In addition, we can see that AUGkM generally outperforms UGkM in all datasets.  The difference is significant When  is very small.  They generally converge to close NICV values when  becomes large.  This seems to suggest that they are reaching the limit of what non-interactive methods can do.  Also, UGkM and AUGkM can never converge to the baseline, as the non-interactive methods have some inherent noises due to the partitioning.  On the other hand, the two non-interactive approaches is superior to the \dpl in the four low dimension datasets.  But \dpl outperformed them in two high dimension datasets, Adult-num and Lifesci.  Therefore, the limits of the non-interactive approaches and the strength of \dpl in high dimension datasets motive the hybrid approach.
\end{comment}

















\begin{figure*}[!htb]
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{s1-hybrid.eps} &
	\includegraphics[width = 2.2in]{gowalla2d-hybrid.eps} &
	\includegraphics[width = 2.2in]{tiger-0.01-hybrid.eps}\\
	(a) S1 [, ]  & (b) Gowalla [, ] & (c) TIGER [, ]
	\end{tabular}
	
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{image-k3-hybrid.eps}&
	\includegraphics[width = 2.2in]{adult-num-hybrid.eps}&
	\includegraphics[width = 2.2in]{lifesci-k3-hybrid.eps}\\
	(d) Image [, ] & (e) Adult-num [, ] & (f) Lifesci [, ]
	\end{tabular}
	
    \caption{The comparison of the Hybrid approach with EUGkM and \dpl. x-axis: privacy budget  in log-scale. y-axis: NICV in log-scale.}\label{fig:hybrid}
\end{figure*}

We also evaluate the approaches using the synthetic datasets as generated in Section \ref{sec:existingAppExpt}. Figure~\ref{fig:heatmap-eugkm-hybrid} clearly shows that the hybrid approach is more scalable than \eugkm with respect to both  and . This confirms the effectiveness of the hybrid approach.

Figure~\ref{fig:running_time_dplloyd_eugkm} presents the runtime of \dpl and \eugkm on the six external datasets.  We follow the same experiment configuration as in Section~\ref{sec:existingAppExpt}.  As expected, the runtime of \dpl is much lower than that of \eugkm.  This is because \eugkm has to run \km clustering over 30 sets of initial centroids and output the centroids with the best NICV relative to the noisy synopsis.  Another reason is that \dpl sets the number of iterations to 5 while \eugkm runs \km clustering until converge.  


\begin{figure}[h]
\begin{tabular}{cc}
\includegraphics[width = 1.8in]{eugkm_nicv.eps}&
\hspace{-0.8cm}\includegraphics[width = 1.8in]{hybrid_med.eps}\\
(a) \eugkm & \hspace{-0.8cm} (b)Hybrid
\end{tabular}
\caption{Comparing hybrid and \eugkm by the heatmap}\label{fig:heatmap-eugkm-hybrid}
\end{figure}



\begin{figure}[h]
\includegraphics[width = 3.1in]{running_time.eps}
\caption{Comparing running time between \dpl and \eugkm, }\label{fig:running_time_dplloyd_eugkm}
\end{figure}





