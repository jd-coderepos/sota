
Experimental results in Section~\ref{sec:towardHybrid} establish that \dpl is the best performing interactive method; however, it still under-performs EUGkM.  Recall that EUGkM publishes a private synopsis of the the dataset, and thus enables other analysis to be performed on the dataset, beyond $k$-means.  This means that currently the non-interactive method has a clear advantage over interactive methods, at least for $k$-means clustering.  

An intriguing question is ``Whether EUGkM is the best we can do for $k$-means clustering?'' In particular, can we further improve \dpl?  Recall that there are two key issues that greatly affect the accuracy of \dpl: the number of iterations and the choice of initial centroids.  In fact, these two are closely related.  If the initially chosen centroids are very good and close to the true centroids, one only needs perhaps one more iteration to improve it, and this reduction in the number of iterations would mean little noise is added.  Now if only we have a method to choose really good centroids in a differentially private way, then we can use part (e.g., half) of the privacy budget to get those initial centroids, and the remaining privacy budget to run one iteration of \dpl to further improve it.  

In fact, we do have such a method.  EUGkM does it.  This leads us to propose a hybrid method that combines non-interactive EUGkM with interactive \dpl.  We first use half the privacy budget to run EUGkM, and then use the centroids outputted by EUGkM as the initial centroids for one round of \dpl.  Such a method, however, may not actually outperform EUGkM, especially when the privacy budget $\epsilon$ is small, since then one round of \dpl may actually worsen the centroids.  Therefore, when $\epsilon$ is small, we should stick to the EUGkM method, and only when $\epsilon$ is large enough should we adopt the EUGkM+\dpl approach.  In order to determine what $\epsilon$ is large enough, we analyze how the errors depend on the various parameters in \dpl and in EUGkM.



\begin{comment}
The standard Lloyd's algorithm is sensitive to initial centroids. Given a set of $k$ initial centroids, it converges to local optimum. There is no guarantee that a global optimum is reached. One effective heuristic to improve the Lloyd's algorithm is trying out multiple sets of initial centroids, and choose the output with the minimum NICV. Figure \ref{fig:convergence-rate} reports this improvement. The initial centroids generated in Section \ref{sec:existingAppExpt} are reused. The Lloyd-Best approach, which is the standard Lloyd's algorithm but takes the set of initial centroids that most favors its accuracy, clearly outperforms the Lloyd approach. A good setting of initial centroids also improves the accuracy of the \dpl method as shown in Figure \ref{fig:synthetic_data_heatmap} (a) and (b), where the \dpl-best method is more scalable with respect to both $k$ and $d$ than the \dpl method. The reason behind is that \dpl is easily to converge to local optimal because of the initial centroids. Still, because of the limited privacy budget, \dpl cannot try as many sets of initial centroids as needed to select one that optimizes its accuracy.


The non-interactive approach runs the \km clustering on differentially private synopses. Thus, it has the freedom of testing as many starting points as needed without violating \difp. This is an advantage of the non-interactive approach over the interactive approach. This is also one of the reasons that \eugkm outperforms \dpl in some cases (Figure \ref{fig:non-interactive_VS_interactive}). However, as the privacy budget and the dimensionality increase, \dpl overtakes \eugkm. If one's goal is to optimize the performance of the private \km clustering, then it is better to combine \eugkm and \dpl to take the best out of them. We thus propose a hybrid approach. It uses \eugkm to try multiple initial centroids, and takes the best output as the initial centroids to \dpl for further refinement. There are challenges to design the hybrid approach. They include
\begin{itemize}
  \item How to study the error behavior of \dpl method and the \eugkm method?
  \item When is it beneficial to apply the hybrid approach? That is, on what conditions the hybrid approach overtakes the \eugkm method and also the \dpl method?
\end{itemize}

In this section, we analyze the error behavior of \dpl and non-interactive approaches. The analysis motivates us to develop a hybrid scheme that combines \eugkm and \dpl.
\end{comment}



\subsection{Error Study of \dpl}\label{sec:dpl_Analysis}

DPLloyd adds noises to each iteration of updating centroids.  To study the error behavior of DPLloyd due to the injected Laplace noises, we focus on analyzing the mean squared error (MSE) between noisy centroids and true centroids in one iteration.  


Consider one centroid and its update in one iteration.  The true centroid's $i$'th dimension should be $o_i=\frac{S_i}{C}$, where $C$ is the number of data points in the cluster and $S_i$ is the sum of $i$'th dimension coordinates of data points in the cluster.  Consider the noisy centroid $\widehat{o}$; its $i$'th dimension is $\widehat{o_i}=\frac{S_i+\Delta S_i}{C +\Delta C}$, where $\Delta C$ is the noise added to the count and $\Delta S_i$ is the noise added to the $S_i$.  The MSE is thus:
\begin{equation} \label{eq:DPLMSE}
\mse{\widehat{o}}  = \E\left[ \sum_{i=1}^{d} \left(\frac{S_i + \Delta S_i}{C + \Delta C} - \frac{S_i}{C}\right)^2 \right]
\end{equation}



Derivation based on the above formula gives the following proposition.

\begin{proposition}\label{thm:DPLMSE}
In one round of \dpl, the MSE is
\begin{equation*}\label{eqn:dpl_mse}
\Theta\left(\frac{(kt)^2d^3}{(N\epsilon)^2}\right).
\end{equation*}
\end{proposition}

\begin{proof}
Let us first consider the MSE on the $i$-th dimension.
$$
\begin{array}{rl}
 & \hspace{-0.4cm}\mse{\widehat{o_i}} = \E\left[ \left( \frac{S_i + \Delta S_i}{C + \Delta C} - \frac{S_i}{C}\right)^2 \right]
\\
 \approx &\hspace{-0.3cm} \E\left[ \left(\frac{C \Delta S_i  - S_i \Delta C}{C^2} \right)^2 \right]\nonumber
\\
 = &\hspace{-0.3cm} \frac{\E[(\Delta S_i)^2]}{C^2} +  \frac{\E[S_i^2 (\Delta C)^2]}{C^4} + \frac{2CS_i\E[\Delta S_i \Delta C]}{C^4} \\
 = & \frac{\Var{\Delta S_i}}{C^2} +  \frac{S_i^2\Var{\Delta C}}{C^4}
\end{array}
$$
The last step holds, because $\Delta S_i$ and $\Delta C$ are independent zero-mean Laplacian noises and the following formulas hold:
$$
\begin{cases}
\E[\Delta S_i \Delta C]=0\\
\E[(\Delta S_i)^2]=\E[(\Delta S_i)^2]-(\E[\Delta S_i])^2=\Var{\Delta S_i}\\
\E[(\Delta C)^2] =\E[(\Delta C)^2]-(\E[\Delta C])^2= \Var{\Delta C},
\end{cases} \nonumber
$$
where $\Var{\Delta S_i}$ and $\Var{\Delta C}$ are the variances of $\Delta S_i$ and $\Delta C$, respectively. 

Suppose that on average $\frac{|S_i|}{2r\cdot C}=\rho$, where $[-r,r]$ is the range of the $i$'th dimension. That is, $\rho$ is the normalized coordinate of $i$-th dimension of the cluster's centroid. Furthermore, suppose that each cluster is about the same size, i.e., $C\approx \frac{N}{k}$. Then, $\mse{\widehat{o_i}}$ can be approximated as follows:
\begin{eqnarray}
\mse{\widehat{o_i}} & \approx &   \frac{k^2}{N^2} \left( \Var{\Delta S_i} +  (2\beta r)^2 \cdot \Var{\Delta C} \right) \label{eqn:DPLMSEoi}
\end{eqnarray}
\dpl adds to each sum/count function Laplace noise $\Lap{\frac{(dr+1)t}{\epsilon}}$. Therefore, both $\Var{\Delta S_i}$ and $\Var{\Delta C}$ are equal to $\frac{2((dr+1)t)^2}{\epsilon^2}$. From Equation (\ref{eqn:DPLMSEoi}) we obtain
\begin{eqnarray*}
\mse{\widehat{o_i}} & \approx &\frac{k^2}{N^2} \left( \Var{\Delta S_i} +  (2\rho r)^2 \cdot \Var{\Delta C} \right) \\
& = & 2 (1+(2\rho r)^2) \left(\frac{kt(dr+1)}{N\epsilon}\right)^2.
\end{eqnarray*}
As the noise added to each dimension is independent, from Equation \ref{eq:DPLMSE} we know that the MSE is
\begin{eqnarray}\label{eqn:DPLMSE_approx}
\mse{\widehat{o}} = \sum_{i=1}^d \mse{\widehat{o_i}} \approx 2d (1+(2\rho r)^2) \left(\frac{kt(dr+1)}{N\epsilon}\right)^2
\end{eqnarray}
When $r$ is a small constant, this becomes $\Theta\left(\frac{(kt)^2 d^3}{(N\epsilon)^2}\right)$.
\end{proof}

Proposition~\ref{thm:DPLMSE} shows that the distortion to the centroid proportional to $t^2k^2d^3$, while  inversely proportional to $(N\epsilon)^2$. At first glance, this analysis seems to conflict with the experimental result in Figure \ref{fig:synthe-heatmap} (a), where \dpl is much less scalable to $k$ than to $d$. The reason behind is that the performance of \dpl is also affected by the fact that $5$ rounds are not enough for it to converge.  When $k$ increases, converging takes more time, and it is also more likely that choices of initial centroids lead to local optima that are far from global optimum. 
 




\begin{comment}


\dpl runs $t$ iterations.  Within each round, the privacy budget needs to be divided among the count and the $d$ sum queries.  Suppose $\epsilon_0$ is allocated to the count query, and $\epsilon_i$ is allocated to the sum query for the $i$-th dimension, for each $i=1,2,\dots, d$.
While all dimensions should be treated equally, i.e., $\epsilon_1=\epsilon_2=\ldots=\epsilon_d$, an interesting question is what should be the right ratio of $\frac{\epsilon_i}{\epsilon_0}$.  This question has not answered before.
The \dpl approach allocates the privacy budget according to the sensitivities of different queries; thus $\frac{\epsilon_i}{\epsilon_0}=r$, assuming that each dimension is normalized to $[-r,r]$.  Thus, different $r$ values will result in different allocations of privacy budget.


This, however, is not optimal.  Plugging $\Var{\Delta S_i}=\frac{2r^2}{\epsilon_i^2}$ and $\Var{\Delta C}=\frac{2}{\epsilon_0^2}$ into Equation~\ref{eqn:DPLMSEoi}, one obtains
\begin{eqnarray}
\sum_{i=1}^d \mse{\widehat{o_i}} &\approx& \frac{k^2}{N^2} \sum_{i=1}^d\left( \Var{\Delta S_i} +  (2\beta r)^2 \cdot \Var{\Delta C} \right) \nonumber \\
&= & \frac{2r^2k^2}{N^2} \left(\sum_{i=1}^d \frac{1}{\epsilon_i^2} +  \frac{4d \beta^2}{\epsilon_0^2}\right ) \label{eqn:DPLMSEsumOi}
\end{eqnarray}
Minimization of the above \text{Subject to } $\sum_{i=1}^d \epsilon_i + \epsilon_0 = \epsilon/t$ can be solved using
the method of {\em Lagrange multipliers}.  The optimal proportion is
\begin{eqnarray}\label{eqn:sum_count_budget_ratio}
\epsilon_1 : \epsilon_2 :\dots : \epsilon_d:\epsilon_0= 1:1:\dots:1:\sqrt[3]{4d\beta^2}
\end{eqnarray}


The optimal proportion improves the MSE of \dpl. Plugging it back in Equation \ref{eqn:DPLMSEsumOi}, it follows that
\begin{equation}\label{eqn:improvedDPLMSE}
\mse{\widehat{o}}\approx 2\left(\frac{rkt}{N\epsilon}\right)^2 \left(d+\sqrt[3]{4d\beta^2}\right)^3,
\end{equation}
which is consistently lower than the MSE in Equation \ref{eqn:DPLMSE_approx} developed for the original \dpl.

We use \dplscopt to represent the \dpl method that has the above improved privacy budget allocation between sum and count functions. \todo{Experiments to show the improvements...}
\end{comment}

\subsection{Error Study of EUGkM}\label{sec:non_inter_analysis}

Non-interactive approach partitions a dataset into a grid of $M$ uniform cells. Then, it releases private synopses for the cells, and runs \km clustering on the synopses to return the cluster centroids. 
Similar to the error analysis for \dpl, we analyze the MSE. Let $o$ be the true centroid of a cluster, and $\widehat{o}$ be its estimator computed by a non-interactive approach. The MSE between $\widehat{o}$ and $o$ is composed of two error sources. First, the count in each cell is inaccurate after adding Laplace noise. This results in the variance (i.e., $\Var{\widehat{o}}$) of $\widehat{o}$ from its expectation $\EE{\widehat{o}}$. Second, we no longer have the precise positions of data points, and only assume that they occur at the center in a cell. Thus, the expectation of $\widehat{o}$ is not equal to $o$, resulting in a bias (i.e., $\BI{\widehat{o}}$). The MSE is the combination of these two errors.
\begin{equation}\label{eqn:mse}
\mse{\widehat{o}}  =  \Var{\widehat{o}} + (\BI{\widehat{o}})^2
\end{equation}



\mypara{Analyzing the variance.} We assume that each cluster has a volume that is $\frac{1}{k}$ of the total volume of the data space, and has the shape of a cube. In $d$-dimensional case, the width of the cube is $w=\frac{2r}{\sqrt[d]{k}}$. Suppose that the {\em geometric} center\footnote{Note that this is not the cluster centroid.} of the cube is $\tau_i$. Let $T$ be the set of cells included in the cluster. For each cell $t \in T$, we use $c_t$ to denote the number of tuples in $t$, $t_i$ to denote the $i$'th dimension coordinate of the center of cell $t$, and $\nu_t$ to denote the noise added to the cell size.  Let $\widehat{o_i}$ be the $i$-th dimension of the noisy centroid. Then, the variance of $\widehat{o_i}$ is
$$
\begin{array}{ll}
& \Var{\widehat{o_i}}  =   \Var{\widehat{o_i}-\tau_i}\vspace{1.5mm}
 \\
= & \Var{\frac{\sum_{t \in T} t_i (c_t + \nu_t)}{\sum_{t \in T} (c_t + \nu_t)}- \tau_i} \vspace{1.5mm}
 \\
= & \Var{\frac{\sum_{t \in T} (t_i - \tau_i) (c_t + \nu_t)}{\sum_{t \in T} (c_t + \nu_t)}}\vspace{1.5mm}
\\
\approx & \frac{1}{C^2}\sum_{t\in T} \left((t_i - \tau_i)^2 \cdot\Var{c_t + \nu_t}\right).
\\
\end{array}
$$
In the above, the first step follows because $\tau_i$ as the cube geometric center is a constant. The last step is derived by assuming $\sum_{t \in T} (c_t + \nu_t)\approx C$, that is, the noisy cluster size is approximately equal to the original cluster size $C$.

We can see that within the cube, different cells' contribution to the variance is not the same.  Basically, the closer a cell is to the cube center, the less its contribution. The contribution is proportional to the squared distance to the cube center. We thus approximate the variance as follows:
\begin{eqnarray*}
\Var{\widehat{o_i}}&\approx &\frac{1}{C^2} \int_{-\frac{w}{2}}^{\frac{w}{2}}x^2 \left(\frac{M}{(2r)^d}w^{d-1}\frac{2}{\epsilon^2}\right) \mathrm{d} x
 \\
&= & \frac{2Mr^2}{3C^2\epsilon^2k^{\frac{d+2}{d}}}.
\end{eqnarray*}
In the above integral, $x$ in the first term is the distance from a cell center to the cube center (i.e., $t_i-\tau_i$). The second term $\frac{M}{(2r)^d}$ is the number of cells per unit volume, and $w^{d-1}$ is the volume of the $(d-1)$-dimensional plane that has a distance of $x$ to the cube center. The last term $\frac{2}{\epsilon^2}$ is the variance of the cell size (i.e., $\Var{c_t + \nu_t}$). Suppose that clusters are of equal size, that is, $C=\frac{N}{k}$. Then, the variance of the noisy centroid by summing all the $d$ dimensions  is
\begin{equation}\label{eqn:nonInter_variance}
\Var{\widehat{o}}\approx\frac{2dMr^2k^{\frac{d-2}{d}}}{3N^2\epsilon^2}
\end{equation}

The analysis shows that the variance of the \eugkm is proportional to $\frac{M}{(N\epsilon)^2}$.  \eugkm sets $M$ to $\left(\frac{N\epsilon}{10}\right )^{\frac{2d}{2+d}}$.  Plugging it into Equation~\ref{eqn:nonInter_variance}, we get that the variance of \eugkm is \emph{inversely proportional} to $\left(N\epsilon\right)^{\frac{4}{2+d}}$.

\begin{comment}
The analysis shows that the variance of non-interactive approach is proportional to $\frac{M}{(N\epsilon)^2}$.
\eugkm sets $M$ to $\left(\frac{N\epsilon}{10}\right )^{\frac{2d}{2+d}}$, and MkM sets $\left(\frac{N}{\sqrt{\log(N)}}\right )^{\frac{2d}{2+d}}$, respectively. 
Plugging them into Equation~\ref{eqn:nonInter_variance}, it follows that the variance of \eugkm is \emph{inversely proportional} to $\left(N\epsilon\right)^{\frac{4}{2+d}}$.
  while that of \mkm is \emph{inversely proportional} to $N^{\frac{4}{2+d}}\epsilon^2$. This gives a reason why \eugkm performs much better than \mkm for small $\epsilon$ values (Figure \ref{fig:non-interactive_VS_interactive}). \end{comment}
\mypara{Analyzing the bias}.
Let $x_i$ be the $i$'th dimension coordinate of a tuple $x$. Then, the bias of $\widehat{o_i}$ is
$$
\begin{array}{ll}
& \BI{\widehat{o_i}} = \EE{\widehat{o_i}}-o_i \vspace{1.5mm}
 \\
= & \EE{\frac{\sum_{t \in T} t_i (c_t + \nu_t)}{\sum_{t \in T} (c_t + \nu_t)}} - \frac{\sum_{t \in T} \sum_{x\in t} x_i }{\sum_{t \in T} c_t} \vspace{1.5mm}
 \\
\approx & \frac{\sum_{t \in T} \sum_{x\in t} (t_i-x_i) }{C},
\\
\end{array}
$$
where the last step is developed by approximating $\sum_{t \in T} (c_t + \nu_t)$ to the cluster size $C$.

The bias developed in the above formula is dependent on data distribution. Its precise estimation requires to access real data. We thus only estimate its upper bound. Let $q_i=t_i-x_i$. Non-interactive approach partitions each dimension into $\sqrt[d]{M}$ intervals of equal length. Hence, $q_i$ falls in the range of $[-\frac{r}{\sqrt[d]{M}}, \frac{r}{\sqrt[d]{M}}]$, and the upper bound of $\BI{\widehat{o_i}} $ is $\frac{r}{\sqrt[d]{M}}$. Summing all the $d$ dimensions, we obtain the upper bound of squared bias of noisy centroid
\begin{equation}\label{eqn:nonInter_bias2}
(\BI{\widehat{o}})^2\leq\frac{dr^2}{M^{\frac{2}{d}}}.
\end{equation}


The estimation shows that the upper bound of squared bias decreases as a function of $M^{\frac{2}{d}}$. This is consistent with the expectation. As $M$ increases, the data space is partitioned into finer-grained cells. Therefore, the distance between a tuple in a cell to the cell center decreases on average.


\begin{comment}
\mypara{Analyzing the bias}.
Let $x_i$ be the $i$'th dimension coordinate of a tuple $x$. Then, the bias of $\widehat{o_i}$ is
$$
\begin{array}{ll}
& \BI{\widehat{o_i}} = \EE{\widehat{o_i}}-o_i \vspace{1.5mm}
 \\
= & \EE{\frac{\sum_{t \in T} t_i (c_t + \nu_t)}{\sum_{t \in T} (c_t + \nu_t)}} - \frac{\sum_{t \in T} \sum_{x\in t} x_i }{\sum_{t \in T} c_t} \vspace{1.5mm}
 \\
\approx & \frac{\sum_{t \in T} \sum_{x\in t} (t_i-x_i) }{C},
\\
\end{array}
$$
where the last step is developed by approximating $\sum_{t \in T} (c_t + \nu_t)$ to the cluster size $C$. The bias developed in the above formula is dependent on data distribution. A precise estimation of its contribution to MSE (i.e., $\BI{\widehat{o_i}}^2$ in Equation \ref{eqn:mse}) requires to access real data. In the following, we estimate the contribution by analyzing the variance and expectation of $\BI{\widehat{o_i}} $.
$$
\EE{\BI{\widehat{o_i}}^2}  =  \Var{\BI{\widehat{o_i}}} + \left(\EE{\BI{\widehat{o_i}}}\right)^2
$$

Let $q_i=t_i-x_i$. Non-interactive approach partitions each dimension into $\sqrt[d]{M}$ intervals of equal length. Thus, $q_i$ falls in the range of $[-\frac{r}{\sqrt[d]{M}}, \frac{r}{\sqrt[d]{M}}]$. For the simplicity, suppose that $q_i$ is uniformly distributed in a cell, cells in a cluster are of equal size, and each cluster has the same number of cells. Then, $\EE{q_i}=0$, $\Var{q_i}=\frac{1}{3}\left(\frac{r}{\sqrt[d]{M}}\right)^2$, and $\EE{\BI{\widehat{o_i}}} = 0$. The variance of $\BI{\widehat{o_i}}$ reaches its lower bound, if all the $q_i$'s are independent within a cluster. On the other hand, if $q_i$'s are highly correlated, then the variance reaches its upper bound. In empirical study, we found that the variance is in the middle between the two \mbox{bounds}. Therefore, we estimate the variance of $\BI{\widehat{o_i}}$
$$
\begin{array}{ll}
& \Var{\BI{\widehat{o_i}}} \approx \Var{\frac{\sum_{t \in T} \sum_{x\in t} (t_i-x_i) }{C}} \vspace{1.5mm}
 \\
= & \chi\left(\frac{1}{C^2}\cdot\frac{M}{k}\cdot \frac{C}{M/k}\cdot \frac{1}{3}\left(\frac{r}{\sqrt[d]{M}}\right)^2 \right), \vspace{1.5mm}
\\
\end{array}
$$
where $\frac{M}{k}$ is the number of cells in a cluster, $\frac{C}{M/k}$ is the cell size, and $\chi$ is a parameter falling in $\in[1, C]$. The lower and upper bounds of $\chi$ correspond to the lower and upper bounds of the variance. Empirically, we set $\chi=\frac{C}{M/k}$ that estimates the variance well. This $\chi$ setting is the same as the assumption---tuples (or $q_i$'s) within a cell are highly correlated, while tuples across cells are independent.





On the basis of the variance and the expectation, we have
$$
\begin{array}{lll}
\EE{\BI{\widehat{o_i}}^2} & \approx & \frac{kr^2}{3M^{\frac{2+d}{d}}}.
\\
\end{array}
$$
Sum all the $d$ dimensions, and we estimate the squared bias of noisy centroid
\begin{equation}\label{eqn:nonInter_bias2}
\BI{\widehat{o}}^2=\frac{dkr^2}{3M^{\frac{2+d}{d}}}
\end{equation}

The estimation shows that the upper bound of squared bias decreases as a function of $M^{\frac{2+d}{d}}$. This is consistent with the expectation. As $M$ increases, the data space is partitioned into finer-grained cells. Therefore, the distance between a tuple in a cell to the cell center decreases on average.


\mypara{MSE of non-interactive approach.} Combining the variance in Equation \ref{eqn:nonInter_variance} and squared bias in Equation \ref{eqn:nonInter_bias2}, we estimate the MSE (Equation \ref{eqn:mse}) of noisy centroid to
\begin{eqnarray}\label{eqn:nonInter_mse}
\mse{\widehat{o}} &\approx& \frac{2dMr^2}{3N^2\epsilon^2k^{\frac{2-d}{d}}} +\frac{dkr^2}{3M^{\frac{2+d}{d}}}.
\end{eqnarray}

The choice of $M$ is up to specific non-interactive approach. Given a dataset of $N$ tuples and privacy budget $\epsilon$, the \ugkm algorithm sets $M$ to $\frac{N\epsilon}{10}$.

\end{comment}

\begin{comment}
\begin{proposition}\label{thm:ugkm_mse}
The MSE of cluster centroids output by the \ugkm algorithm is
\begin{equation*}
O\left(\frac{d}{N\epsilon k^{\frac{2-d}{d}}}\right ).
\end{equation*}
\end{proposition}

\begin{proof}
\ugkm partitions the data space into $M=\frac{N\epsilon}{10}$ grid of equal size. Substitute the $M$ value for that in Equation \ref{eqn:nonInter_mse}, and we have:
\begin{eqnarray*}\label{eqn:ugkm_mse_prime}
\mse{\widehat{o}}&\approx& \frac{dr^2}{3}\left(\frac{1}{5N\epsilon k^{\frac{2-d}{d}}} + \frac{(10)^{\frac{2+d}{d}}k}{(N\epsilon)^{\frac{2+d}{d}}}\right).
\end{eqnarray*}
Since $N\epsilon<(N\epsilon)^{\frac{2+d}{d}}$, the MSE of cluster centroids for small $r$ is
$$O\left(\frac{d}{N\epsilon k^{\frac{2-d}{d}}}\right ).$$
\end{proof}


In a similar way, according to the choice of $M$, we also develop the proposition for the \mkm algorithm.

\begin{proposition}\label{thm:mkm_mse}
The MSE of cluster centroids outputted by the \mkm algorithm is
\begin{equation*}
O\left(\frac{d}{N^{\frac{4}{2+d}}\epsilon^2 k^{\frac{2-d}{d}}}\right ).
\end{equation*}
\end{proposition}

\begin{proof}
\mkm partitions the data space into $M=\left( \frac{N}{\sqrt{\log(N)}} \right)^{\frac{2d}{2+d}}$ grid cells of equal size. Substitute the $M$ value for that in Equation \ref{eqn:nonInter_mse}, and we have:
\small{\begin{eqnarray*}\label{eqn:mkm_mse_prime}
\mse{\widehat{o}}\approx \frac{dr^2}{3}\left(\frac{2}{N^{\frac{4}{2+d}}\epsilon^2 k^{\frac{2-d}{d}}(\log(N))^\frac{d}{2+d}} + \frac{k\log(N)}{N^2}\right).
\end{eqnarray*}}
Since $N^{\frac{4}{2+d}}<N^2$, the MSE of cluster centroids for small $r$ is
$$O\left(\frac{d}{N^{\frac{4}{2+d}}\epsilon^2 k^{\frac{2-d}{d}}}\right ).$$
\end{proof}

\mypara{Comparing \dpl, \ugkm and \mkm.} Now let us consider Proposition \ref{thm:DPLMSE}, Proposition \ref{thm:ugkm_mse} and Proposition \ref{thm:mkm_mse} together. We can see that the MSE of \dpl is inversely proportional to $(N\epsilon)^2$, and those of \ugkm and \mkm are inversely proportional to $N\epsilon$ and $N^{\frac{4}{2+d}}\epsilon^2$, respectively. This explains why the NICV of \dpl (\mkm) drops much faster than that of \ugkm when $\epsilon$ grows. It also explains why \dpl does not overtake \ugkm on `small' dataset S1, but outperforms \ugkm on `big' dataset Tiger.

\note{The effect of dimensionality cannot be explained.}
\end{comment}



\begin{comment}
\mypara{Comparing \ugkm with \mkm.} The difference of the two non-interactive methods is the choice of $M$. The \ugkm method sets it to $\frac{N\epsilon}{10}$, and the \mkm method sets it to $\left( \frac{N}{\sqrt{\log(N)}} \right)^{\frac{2d}{2+d}}$. Figure \ref{fig:non-interactive_VS_interactive} shows that the performance of \ugkm is superior to that of \mkm. The following gives two possible reasons.

First, \mkm does not take $\epsilon$ as a factor in $M$. Thus, it is nonadaptive to the variation of $\epsilon$. Plugging the $M$ values into Equation \ref{eqn:nonInter_variance}, we know that the variance of \ugkm is inversely proportional to $N\epsilon$, while that of \mkm is inversely proportional to $N^{\frac{4}{2+d}}\epsilon^2$. This explains why \ugkm performs much better than \mkm for small $\epsilon$ values (Figure \ref{fig:non-interactive_VS_interactive}), and it also explains why the NICV of \mkm drops faster than that of \ugkm.

Second, as the dimensionality increases, the number of cells generated by \mkm is far much more than the dataset size. When the dimensionality is big enough, \mkm generates approximately $N^2/\log(N)$ cells. Thus, most cells are empty. After adding Laplace noise, they turn into random noises, and result in poor performance of k-means clustering. \todo{This explains why the \ugkm is superior to \mkm on the 6D Adult dataset.}
\end{comment}

\mypara{Comparing \dpl and \eugkm.} We now analyze the performance of \dpl and \eugkm in Figure \ref{fig:non-interactive_VS_interactive}. Equation \ref{eqn:DPLMSE_approx} shows that the MSE of \dpl is inversely proportional to $(N\epsilon)^2$. The MSE of \eugkm consists of variance and squared bias. Plugging $M=\left(\frac{N\epsilon}{10}\right )^{\frac{2d}{2+d}}$ into Equation \ref{eqn:nonInter_variance} and Inequality \ref{eqn:nonInter_bias2},
it follows that the MSE of \eugkm is inversely proportional to $(N\epsilon)^{\frac{4}{2+d}}$. This explains why the NICV of \dpl, which is inversely proportional to $(N\epsilon)^2$ drops much faster than that of \eugkm as $\epsilon$ grows. It also explains why \dpl has better performance on `big' dataset (e.g., the TIGER dataset).


The MSE of \eugkm is inversely proportional to $(N\epsilon)^{\frac{4}{2+d}}$. Thus, it increases exponentially as a function of $d$. Instead, from Equation \ref{eqn:DPLMSE_approx}, it follows that the MSE of \dpl has only cubic growth with respect to $d$. Therefore, in Figure \ref{fig:non-interactive_VS_interactive}, as the dimensionality of dataset increases, \dpl outperforms \eugkm. This also explains in Figure \ref{fig:synthe-heatmap}  why \dpl is more scalable to $d$ than \eugkm.


\begin{comment}


In~\cite{QYL12}, \ug chooses $M$ to optimize accuracy for range queries.  In this section, we investigate how to choose $M$ to optimize for $k$-means clustering.
To minimize the MSE value in Equation (\ref{eqn:nonInter_mse}), we set the derivative of the equation with respect to $M$ to 0
$$
\frac{dr^2}{3}\left(\frac{2}{(N\epsilon)^2k^{\frac{2-d}{d}}}- \frac{(2+d)k}{d}M^{-\frac{2+2d}{d}}\right) = 0,
$$
which results in
\begin{equation}\label{eqn:M_adaptiveUG}
M = \left ( \frac{(N\epsilon)^2k^{\frac{2}{d}}(2+d)}{2d}\right )^{\frac{d}{2+2d}}.
\end{equation}

Based on the $M$ value developed above, we divide the data space into a $m^d$ grid of equal size, where $m=\lceil \sqrt[d]{M} \rceil$. We denote this new non-interactive approach by \augkm.

\begin{proposition}\label{thm:augkm_mse}
The MSE of the cluster centroids output by \augkm algorithm is approximately
\begin{equation*}\label{eqn:prop_augkm}
\Theta\left (\frac{d}{(N\epsilon)^{\frac{2+d}{1+d}}k^{\frac{2-d^2}{d(1+d)}}}\right ).
\end{equation*}
\end{proposition}

\begin{proof}
Substituting the tailored value of $M$ in Equation \ref{eqn:M_adaptiveUG} for that in Equation \ref{eqn:nonInter_mse}, we have
\begin{eqnarray}
& \mse{\widehat{o}} \nonumber \\
= &\frac{2dr^2}{3(N\epsilon)^{\frac{2+d}{1+d}}k^{\frac{2-d^2}{d(1+d)}}}\left( \left(\frac{2+d}{2d}\right)^{\frac{d}{2+2d}} + \frac{1}{2}\left(\frac{2d}{2+d}\right)^{\frac{2+d}{2+2d}} \right).\label{eqn:augkm_mse}
\end{eqnarray}
Therefore, when $r$ is small, the MSE of the cluster centroids output by \augkm is
$$\Theta\left (\frac{d}{(N\epsilon)^{\frac{2+d}{1+d}}k^{\frac{2-d^2}{d(1+d)}}}\right ).$$
\end{proof}
\end{comment}

\begin{comment}
\begin{figure*}[p]
	\begin{tabular}{cc}
	\includegraphics[width = 3.25in]{s1-performance.eps} & \includegraphics[width = 3.25in]{s1-performance-closeup.eps}\\
	(a) S1 dataset & (b) S1 dataset, close-up at large $\epsilon$\\
	\includegraphics[width = 3.25in]{gowalla-2d-performance.eps} & \includegraphics[width = 3.25in]{gowalla-2d-performance-closeup.eps}\\
	(c) Gowalla 2D dataset & (d) Gowalla 2D dataset, close-up at large $\epsilon$\\
\includegraphics[width = 3.25in]{gowalla-3d-performance.eps} & \includegraphics[width = 3.25in]{gowalla-3d-performance-closeup.eps}\\
	(e) Gowalla 3D dataset & (f) Gowalla 3D dataset, close-up at large $\epsilon$\\
	\end{tabular}
	\caption{The comparison of four algorithms proposed in this paper, by varying $\epsilon$}\label{fig:exp-vary-epsilon}
\end{figure*}

\begin{figure*}[p]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[scale=2]{regression-all-in-one.eps}
\caption{The linear regression for empirically finding the $\eta$ value.}\label{fig:regression}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width = 3.25in]{ds1.10-scalability.eps}\caption{The comparison of the UGkM and DPLloyd with varying dimensions.}\label{fig:exp-vary-N-dimension}
\end{minipage}
\end{figure*}
\end{comment}

\subsection{The Hybrid Approach}\label{ssec:hybrid}

Our hybrid approach combines \eugkm and \dpl. Given a dataset and privacy budget $\epsilon$, the hybrid approach first \mbox{checks} whether it overtakes the \dpl method and also the \eugkm method. If this is not the case, the hybrid approach simply falls back to \eugkm. Otherwise, the hybrid approach allocates half privacy budget to \eugkm to output a synopsis and find $k$ intermediary centroids that work well for the synopsis. Then, it runs \dpl for one iteration using the remaining half privacy budget to refine these $k$ centroids.

We use MSE to heuristically determine the conditions, on which the hybrid approach overtakes the \dpl method and also the \eugkm method. Basically, we require that the MSE of the hybrid approach be smaller than those of the other two approaches, since smaller MSE implies smaller error to the cluster centroid.
From Equation \ref{eqn:DPLMSE_approx}, it follows that the MSE of \dpl with full privacy budget is
\begin{equation}\label{formula:DPLopt_MSE_tround}
2d (1+(2\rho r)^2) \left(\frac{kt(dr+1)}{N \epsilon}\right)^2.
\end{equation}
A precise estimation of the MSE of the \eugkm method requires to access the dataset, since the bias depends on the real data distribution. However, we have the approximate variance (Equation \ref{eqn:nonInter_variance}) by setting $M=\left(\frac{N\epsilon}{10}\right )^{\frac{2d}{2+d}}$.
\begin{equation}\label{formula:eugkm_var}
\frac{2dr^2(k)^{\frac{d-2}{d}}}{3\times (10)^{\frac{2d}{2+d}}(N\epsilon)^{\frac{4}{2+d}}}
\begin{comment}\left \{
\begin{array}{ccc}
\Var{\widehat{o}} &\approx& \frac{2dr^2(k)^{\frac{d-2}{d}}}{3\times (10)^{\frac{2d}{2+d}}(N\epsilon)^{\frac{4}{2+d}}}\vspace{0.2cm}\\
\BI{\widehat{o}}^2 &\leq& \frac{(10)^{\frac{4}{2+d}}dr^2}{(N\epsilon)^{\frac{4}{2+d}}}.
\end{array}
\right. \end{comment}
\end{equation}
One-iteration \dpl with half privacy budget outputs the final $k$ cluster centroids, if it is applied in the hybrid approach. Therefore, we approximate the MSE of the hybrid approach by that of the one-iteration \dpl
\begin{equation}\label{formula:dplmse_1round}
8d (1+(2\rho r)^2) \left(\frac{k(dr+1)}{N \epsilon}\right)^2,
\end{equation}
which is developed by setting $t=1$ and privacy budget to $0.5\epsilon$ in Equation \ref{eqn:DPLMSE_approx}.

Comparing Formulas \ref{formula:DPLopt_MSE_tround} and \ref{formula:dplmse_1round}, it follows that the MSE of the hybrid approach is lower than or equal to that of the \dpl if
\begin{equation}\label{eqn:alpha}
t\geq 2.
\end{equation}

Variance is the lower bound of MSE. Thus, if the MSE of the hybrid approach is equal to or smaller than the variance of the \eugkm method, then it is sure that the hybrid approach has lower MSE. Setting Formula \ref{formula:dplmse_1round} smaller than or equal to Formula \ref{formula:eugkm_var} yields
\begin{equation}\label{eqn:boundaryEpsilon}
\epsilon \geq \left(\frac{X}{Y}\right )^{\frac{2+d}{2d}},
\end{equation}
where
$$
X= 8d (1+(2\rho r)^2) \left(\frac{k(dr+1)}{N}\right)^2,
$$
and
$$
Y = \frac{2dr^2(k)^{\frac{d-2}{d}}}{3\times (10)^{\frac{2d}{2+d}}N^{\frac{4}{2+d}}}.
$$

Inequalities \ref{eqn:alpha} and \ref{eqn:boundaryEpsilon} give the conditions of applying the hybrid approach. Inequality \ref{eqn:alpha} is automatically satisfied since \dpl runs for $t=5$ iterations.



\begin{comment}
\subsection{Improved Hybrid Approach}

\begin{align*}
\mathrm{Error}_{\mathrm{hybrid}} &= G(f)\\
                                 &= \frac{1}{3}\cdot\mathrm{Error}_{\mathrm{EUGkM}} + \mathrm{Error}_{\mathrm{DPLloyd}, t = 1} \\
                                 &= \frac{1}{3}\cdot \frac{2dr^2(k)^{\frac{d-2}{d}}}{3\times (10)^{\frac{2d}{2+d}}(Nf\epsilon)^{\frac{4}{2+d}}} \\
                                 &+ 2d (1+(2\rho r)^2) \left(\frac{k(dr+1)}{N (1-f) \epsilon}\right)^2
\end{align*}

Let 
\begin{align*}
\alpha = \frac{2dr^2(k)^{\frac{d-2}{d}}}{3\times (10)^{\frac{2d}{2+d}}\left(N\epsilon\right)^{\frac{4}{2+d}}}
\end{align*} 
and 
\begin{align*}
\beta = 2d(1+(2\rho r)^2) \left(\frac{k(dr+1)}{N\epsilon}\right)^2,
\end{align*}
\begin{align*}
G(f) &= \frac{\alpha}{3}\cdot\left(\frac{1}{f}\right)^{\frac{4}{2+d}} + \beta\cdot \left(\frac{1}{1-f}\right)^2
\end{align*}

Optimze $G(f)$ over $f$ by taking the first derivative of $\mathrm{Error}_{\mathrm{hybrid}}$,
\begin{align*}
\diff{G(f)}{f} &= -\frac{4\alpha}{3(2+d)} \cdot f^{-\frac{6+d}{2+d}} + 2\beta (1 - f)^{-3}
\end{align*}

Setting $\diff{G(f)}{f} = 0$, we have 
\begin{align*}
\frac{3\beta(2+d)}{2\alpha}\cdot f^{\frac{6+d}{2+d}} - (1 - f)^{3} = 0.
\end{align*}

\begin{align*}
\diff[2]{G(f)}{f} &= \frac{4\alpha(6+d)}{3(2+d)^2} \cdot f^{-\frac{8+2d}{2+d}} + 6\beta (1 - f)^{-4}\\
                                                &> 0.
\end{align*}
Therefore, $G(f)$ has a global minimum. 

Two iterations case:
\begin{align*}
\mathrm{Error}_{\mathrm{hybrid}} &= G(f_{1}, f_{2})\\
                                 &= \frac{1}{3}\cdot \left(\frac{1}{3}\cdot\mathrm{Error}_{\mathrm{EUGkM}} + \mathrm{Error}_{\mathrm{DPLloyd}, t = 2}\right) \\
                                 &+ \mathrm{Error}_{\mathrm{DPLloyd}, t = 2}\\
                                 & = \frac{1}{3}\cdot\left( \frac{\alpha}{3}\left(\frac{1}{f_1}\right)^{\frac{4}{2+d}} + 4\beta\left(\frac{1}{f_2}\right)^2 \right)\\
                                 &+ 4\beta\left(\frac{1}{1 - f_{1} - f_{2}}\right)^2,
\end{align*}


\begin{equation*}
\left\{\begin{aligned}
\frac{\partial G(f_{1}, f_{2})}{\partial f_{1}} &= -\frac{4\alpha}{9(2+d)} f_{1}^{-\frac{6+d}{2+d}} + 8\beta(1 - f_{1} - f_{2})^{-3}\\
\frac{\partial G(f_{1}, f_{2})}{\partial f_{2}} &= -\frac{8\beta}{3} f_{2}^{-3} + 8\beta(1 - f_{1} - f_{2})^{-3}\\
\end{aligned}
\right.
\end{equation*}

\begin{equation*}
\left\{
\begin{aligned}
\frac{\partial G(f_{1}, f_{2})^2}{\partial f_{1}^{2}} &= \frac{4\alpha(6+d)}{9(2+d)^{2}} f_{1}^{-\frac{8+2d}{2+d}} + 24\beta(1 - f_{1} - f_{2})^{-4}\\
\frac{\partial G(f_{1}, f_{2})^{2}}{\partial f_{2}^{2}} &= 8\beta f_{2}^{-4} + 24\beta(1 - f_{1} - f_{2})^{-4}\\
\frac{\partial G(f_{1}, f_{2})^2}{\partial f_{1}f_{2}} &= 24\beta(1 - f_{1} - f_{2})^{-4}\\
\end{aligned}
\right.
\end{equation*}

We have 
\begin{align*}
\frac{\partial G(f_{1}, f_{2})^2}{\partial f_{1}^2}\frac{\partial G(f_{1}, f_{2})^2}{\partial f_{2}^2} - \left(\frac{\partial G(f_{1}, f_{2})^2}{\partial f_{1}f_{2}}\right)^2 > 0
\end{align*}
and 
\begin{align*}
\frac{\partial G(f_{1}, f_{2})^2}{\partial f_{1}^2} > 0,
\end{align*}
therefore, $G(f_{1}, f_{2})$ has a global minimum.  

\end{comment}



\begin{comment}


\mypara{The conditions.} We use MSE to heuristically determine the conditions, on which the hybrid approach overtakes the \dpl method and also the \eugkm method. From Equation \ref{eqn:DPLMSE_approx}, it follows that the MSE of \dpl with full privacy budget is
\begin{equation}\label{formula:DPLopt_MSE_tround}
2d (1+(2\beta r)^2) \left(\frac{kt(dr+1)}{N \epsilon}\right)^2.
\end{equation}
It immediately follows from Equation \ref{eqn:augkm_mse} that the MSE of \eugkm with full privacy budget is
\begin{equation}\label{formula:augkm_mse}
\small{\frac{2dr^2}{3(N\epsilon)^{\frac{2+d}{1+d}}k^{\frac{2-d^2}{d(1+d)}}}\left( \left(\frac{2+d}{2d}\right)^{\frac{d}{2+2d}} + \frac{1}{2}\left(\frac{2d}{2+d}\right)^{\frac{2+d}{2+2d}} \right)}.
\end{equation}
One-iteration \dpl with privacy budget $(1-\alpha)\epsilon$ outputs the final $k$ cluster centroids, if it is applied in the hybrid approach. Therefore, we approximate the MSE of the hybrid approach by that of the one-iteration \dpl
\begin{equation}\label{formula:dplmse_1round}
2d (1+(2\beta r)^2) \left(\frac{k(dr+1)}{N (1-\alpha)\epsilon}\right)^2,
\end{equation}
which is developed by setting $t=1$ and privacy budget to $(1-\alpha)\epsilon$ in Equation \ref{eqn:DPLMSE_approx}.

MSE measures the error to a cluster centroid. Smaller MSE value indicates lower error. We thus develop the conditions on which the hybrid approach overtakes the other two approaches. To be more precise, we require that the MSE of the hybrid approach is at most $\eta$ times as big as that of \dpl (and \eugkm). Comparing Formulas \ref{formula:DPLopt_MSE_tround} and \ref{formula:dplmse_1round}, it follows that the requirement holds if
\begin{equation}\label{eqn:alpha}
\alpha \leq 1-\frac{\sqrt{\eta}}{t}.
\end{equation}
Setting Formula \ref{formula:dplmse_1round} at most $\eta$ times as big as Formula \ref{formula:augkm_mse} yields
\begin{equation}\label{eqn:boundaryEpsilon}
\epsilon \geq \epsilon^b = \left(\frac{X}{\eta Y}\right)^{\frac{1+d}{d}},
\end{equation}
where
$$
X= 2d (1+(2\beta r)^2) \left(\frac{k(dr+1)}{N (1-\alpha)}\right)^2,
$$
and
$$
\frac{2dr^2}{3(N)^{\frac{2+d}{1+d}}k^{\frac{2-d^2}{d(1+d)}}}\left( \left(\frac{2+d}{2d}\right)^{\frac{d}{2+2d}} + \frac{1}{2}\left(\frac{2d}{2+d}\right)^{\frac{2+d}{2+2d}} \right).
$$

Inequalities \ref{eqn:alpha} and \ref{eqn:boundaryEpsilon} give the conditions of applying the hybrid approach. In the experiments, we set $\eta=2$.

\mypara{The $\alpha$ value.}  The hybrid approach allocates $\alpha\epsilon$ privacy budget to \augkm and the remaining to one-iteration \dpl, when the conditions developed above hold. The one-iteration \dpl outputs the final centroids. So enough privacy budget needs to be allocated to it to optimize the accuracy. However, if the intermediary $k$ centroids found by \augkm is of low quality, then the refinement on them by \dpl is still limited. Therefore, the $\alpha$ value needs to be well decided. Again, we use MSE for this task. We require that the MSE of \augkm should not be too big, when compared with one-iteration \dpl. Plugging privacy budget $\alpha\epsilon$ in Equation \ref{eqn:nonInter_variance}, it follows that the MSE of \augkm is
$$
\frac{2dr^2}{3(N\alpha\epsilon)^{\frac{2+d}{1+d}}k^{\frac{2-d^2}{d(1+d)}}}\left( \left(\frac{2+d}{2d}\right)^{\frac{d}{2+2d}} + \frac{1}{2}\left(\frac{2d}{2+d}\right)^{\frac{2+d}{2+2d}} \right)
$$
We determine the $\alpha$ value by setting the MSE of \augkm to be at most $\lambda = 5$ times as big as the MSE of one-iteration \dpl in Formula \ref{formula:dplmse_1round}, where $\lambda$ is empirically computed.
\end{comment}

\begin{comment}
Our hybrid approach combines \ugkm and \dplscopt. Given a dataset and privacy budget $\epsilon$, the hybrid approach first checks whether it overtakes the \dplscopt method and also the \ugkm method. If this is not the case, the hybrid approach simply falls back to \ugkm. Otherwise, the hybrid approach allocates privacy budget $\alpha \epsilon$ ($0\leq \alpha\leq 1.0$) to \ugkm to output a synopsis and find $k$ intermediary centroids that work well for the synopsis. Then, it runs \dplscopt for one iteration using the remaining privacy budget $(1.0-\alpha)\epsilon$ to refine these $k$ centroids.

\mypara{The conditions.} We use MSE to heuristically determine the conditions, on which the hybrid approach overtakes the \dplscopt method and also the \ugkm method. Basically, we require that the MSE of the hybrid approach be smaller than those of the other two approaches, since smaller MSE implies smaller error to the cluster centroid. From Equation \ref{eqn:improvedDPLMSE}, it follows that the MSE of \dplscopt with full privacy budget is
\begin{equation}\label{formula:DPLopt_MSE_tround}
2\left(\frac{rkt}{N\epsilon}\right)^2 \left(d+\sqrt[3]{4d\beta^2}\right)^3.
\end{equation}
A precise estimation of the MSE of the \ugkm method requires to access the dataset, since the bias depends on the real data distribution. However, we have the approximate variance (Equation \ref{eqn:nonInter_variance})
\begin{equation}\label{formula:ugkm_var}
\frac{dr^2}{15N\epsilon k^{\frac{2-d}{d}}}.
\end{equation}
One-iteration \dplscopt with privacy budget $(1-\alpha)\epsilon$ outputs the final $k$ cluster centroids, if it is applied in the hybrid approach. Therefore, we approximate the MSE of the hybrid approach by that of the one-iteration \dplscopt
\begin{equation}\label{formula:dplscopt_1round}
2\left(\frac{rk}{N(1-\alpha)\epsilon}\right)^2 \left(d+\sqrt[3]{4d\beta^2}\right)^3,
\end{equation}
which is developed by setting $t=1$ and privacy budget to $(1-\alpha)\epsilon$ in Equation \ref{eqn:improvedDPLMSE}.

Comparing Formulas \ref{formula:DPLopt_MSE_tround} and \ref{formula:dplscopt_1round}, it follows that the MSE of the hybrid approach is lower than or equal to that of the \dplscopt if
\begin{equation}\label{eqn:alpha}
\alpha \leq 1-\frac{1}{t}.
\end{equation}

Variance is the lower bound of MSE. Thus, if the MSE of the hybrid approach is equal to or small than the variance of the \ugkm method, then it is sure that the hybrid approach has lower MSE. Setting Formula \ref{formula:dplscopt_1round} smaller than or equal to Formula \ref{formula:ugkm_var} yields
\begin{equation}\label{eqn:boundaryEpsilon}
\epsilon \geq \epsilon^b = \frac{X}{Y},
\end{equation}
where
$$
X= 2\left(\frac{rk}{N(1-\alpha)}\right)^2 \left(d+\sqrt[3]{4d\beta^2}\right)^3,
$$
and
$$
Y = \frac{dr^2}{15N k^{\frac{2-d}{d}}}.
$$

Inequalities \ref{eqn:alpha} and \ref{eqn:boundaryEpsilon} give the conditions of applying the hybrid approach.

\mypara{The $\alpha$ value.}  The hybrid approach allocates $\alpha\epsilon$ privacy budget to \ugkm and the remaining to one-iteration \dplscopt, when the conditions developed above hold. The one-iteration \dplscopt outputs the final centroids. So enough privacy budget needs to be allocated to it to optimize the accuracy. However, if the intermediary $k$ centroids found by \ugkm is of low quality, then the refinement on them by \dplscopt is still limited. Therefore, the $\alpha$ value needs to be well decided. Again, we use MSE for this task. We require that the MSE of \ugkm should not be too big, when compared with one-iteration \dplscopt. The MSE of \ugkm is dependent on real data distribution. We thus consider its variance. Intuitively, if the MSE of \ugkm is not too big, then its variance should not be big either. Plugging privacy budget $\alpha\epsilon$ in Equation \ref{eqn:nonInter_variance}, it follows that the variance of \ugkm is
$$
\frac{dr^2}{15N(\alpha\epsilon)k^{\frac{2-d}{d}}}.
$$
We determine the $\alpha$ value by setting the variance of \ugkm to be at most \todo{x} times as big as the MSE of one-iteration \dplscopt in Formula \ref{formula:dplscopt_1round}, where $x$ is empirically computed.
\end{comment}

\begin{comment}
We have found that when $N\epsilon$ is small, non-interactive $k$-means clustering approaches such as \ugkm outperforms interactive \dpl; however, when $\epsilon$ is large, \dpl performs better.   We observe that if one's goal is to optimize performance for $k$-means clustering as much as possible, then one should use a hybrid approach.  We propose the following novel hybrid approach that can combine the best of both the non-interactive \ugkm and the interactive \dpl.

Given a dataset and privacy budget $\epsilon$, the hybrid approach first checks whether $\epsilon$ is large enough to benefit from a hybrid approach.  If $\epsilon$ is small, the hybrid approach simply falls back to \ugkm. If $\epsilon$ is large enough, the hybrid approach allocates privacy budget $\alpha \epsilon$ ($0\leq \alpha\leq 1.0$) to \ugkm to output a synopsis, and finds $k$ centroids that work well for the synopsis. Then, it runs \dpl for one iteration using the remaining privacy budget $(1.0-\alpha)\epsilon$ to refine the $k$ centroids. \dpl generates the final output, thus enough privacy budget needs to be allocated to it. However, if the accuracy of the intermediate output by \ugkm is too low, then the refinement by \dpl would be limited. Therefore, the $\alpha$ value needs to be well decided. We use MSE formulas for this purpose. \note{variance is used only for \ugkm} To ensure that the accuracy of \ugkm is reasonable, we require that the variance of \ugkm is not too big, when compared with that of \dpl. In particular, we know that the variance of \ugkm (by Equation \ref{eqn:nonInter_variance}) with privacy budget $\alpha\epsilon$ is
\begin{equation*}
\frac{dr^2}{15N(\alpha\epsilon)k^{\frac{2-d}{d}}},
\end{equation*}
and that of \dpl (by Equation \ref{eqn:improvedDPLMSE}) with privacy budget $(1.0-\alpha)\epsilon$ is
\begin{equation}\label{formula:dpl_1round}
2\left(\frac{rk}{N(1-\alpha)\epsilon}\right)^2 \left(d+\sqrt[3]{4d\beta^2}\right)^3,
\end{equation}
By setting the former \todo{x} times as big as the latter, we determine the $\alpha$ value. Correspondingly, we also determine the budget allocation between the two steps.

One issue in the hybrid approach is to determine a sufficient privacy budget threshold $\epsilon^b$, such that when $\epsilon$ is larger than it, the hybrid approach would switch from \ugkm to \ugkm plus one round of \dpl. Again, we use MSE to determine this threshold. Equation \ref{eqn:nonInter_variance} gives the variance of \ugkm when it uses the full privacy budget. \note{the variance can be seen as the lower bound of the MSE of \ugkm.}
One-round of \dpl outputs the final $k$ cluster centroids, if it is applied in hybrid approach. Thus, we approximate the MSE of hybrid approach with one-round of \dpl by that given in Equation \ref{formula:dpl_1round}. By setting the MSEs in the two equations to be equal, we obtain the threshold:
\begin{equation}
\epsilon^b = \frac{X}{Y},
\end{equation}
where
$$
X= 2\left(\frac{rk}{N(1-\alpha)}\right)^2 \left(d+\sqrt[3]{4d\beta^2}\right)^3,


$$

and
$$
Y = \frac{dr^2}{15N k^{\frac{2-d}{d}}}.
$$
\end{comment}



\subsection{Experimental results}

\begin{comment}
We now compare the hybrid approach with \dpl and \mbox{\eugkm} using both synthetic and real datasets. When generating the synthetic datasets, we fix the dataset size to 10,000, and vary $k$ and $d$ from 2 to 10. For each dataset, $k$ well separated Gaussian clusters of equal size are generated, and 30 sets of initial centroids are generated in the same way as in Section \ref{sec:existingAppExpt}. Since the real centroids of the Gaussian clusters are known, we include them as one set of initial centroids. The baseline NICV is computed using the real centroids. We configure \dpl and \eugkm in the same way as in Section \ref{sec:existingAppExpt}. We also include \dpl-best as a benchmark for \dpl. Its configuration is the same as \mbox{\dpl}, except that it takes only the real cluster centroids as the initial centroids. For the hybrid approach, we run one-iteration \dpl 100 times on the intermediary centroids output by \eugkm, and consider the average NICV value. Furthermore, we compute the difference of NICV between each approach and the baseline. The difference allows to investigate the scalability of each approach with respect to the baseline, when we vary the parameters of $k$, $d$ and $\epsilon$. We use \nicvminus to represent this difference of NICV.




Figure \ref{fig:synthe} reports the results. We first fix $\epsilon=1.0$, and study the effect of $k$ and $d$ (the first row in Figure \ref{fig:synthe}). Clearly, the hybrid approach outperforms the other three approaches. It has the same \nicvminus as \eugkm on 2-dimensional datasets when $k$ is 5 and 10. This happens, since the hybrid approach falls back to \eugkm on these two points. As expected, \dpl-best has lower \nicvminus than \dpl in all the cases, since it takes real centroids as the initial centroids. However, it is still inferior to the hybrid approach. This shows that running \dpl 5 iterations on real centroids only adds noise.
Our analysis shows that the MSE of \dpl is proportional to $d^3$ while that of \eugkm increases exponentially as a function of $d$. Therefore, when the dimensionality increases, the performance of \eugkm declines (i.e., \nicvminus increases) faster than those of \dpl and \dpl-best. Then, in the second row of Figure \ref{fig:synthe}, we fix $k=5$ and vary $\epsilon$ and $d$. Again, the hybrid approach is better than other approaches. The MSE of \dpl is inversely proportional of $\epsilon^2$, while that of \eugkm is inversely proportional of $\epsilon^{\frac{4}{2+d}}$. Thus, as $\epsilon$ grows, the performance of \dpl improves (i.e., \nicvminus decreases) faster than that of \eugkm. Finally, in the last row of Figure \ref{fig:synthe}, we fix $d=5$ and vary $k$ and $\epsilon$. The hybrid approach outperforms the other approaches for $\epsilon = 1.0$. It falls back to \eugkm on the other two $\epsilon$ values.


\discuss{Figure \ref{fig:synthe-heatmap} reports the experimental results using heatmap, where $\epsilon$ is fixed to 1.0. According to Proposition \ref{thm:DPLMSE}, the MSE of \dpl is proportional to $k^2d^3$. Thus, as $k$ and/or $d$ increase, the performance of \dpl-best declines (i.e., \nicvminus increases), and the decline with $d$ is more than that with $k$ (Figure \ref{fig:synthe-heatmap} (a) ).  Figure \ref{fig:synthe-heatmap} (c) gives the heatmap for \eugkm. The MSE of \eugkm increases exponentially as a function of $d$, while it increases linearly with $k^{\frac{d-2}{d}}$. Thus, \eugkm is much more scalable to $k$ than to $d$. Comparing Figure \ref{fig:synthe-heatmap} (a), (c), and (d), it is clear that the hybrid approach is more scalable than \dpl-best and \eugkm with respect to both $k$ and $d$. This confirms the effectiveness of the hybrid approach. Figure \ref{fig:synthe-heatmap} (b) shows that \dpl is much less scalable to $k$ than to $d$. Still, this is not in conflict with our analysis of Proposition \ref{thm:DPLMSE}. The reason behind is that the performance of \dpl is also up to the initial centroids. When $k$ increases, \dpl is more likely to converge to local optimal due to the initial centroids.}





\begin{figure*}[!htb]
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_k_2_vary_d.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_k_5_vary_d.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_k_10_vary_d.eps}\\
	(a) [$\epsilon = 1.0$, $k = 2$] & (b) [$\epsilon = 1.0$, $k = 5$]  & (c) [$\epsilon = 1.0$, $k = 10$]
	\end{tabular}

	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/k_5_d_2_vary_eps.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/k_5_d_5_vary_eps.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/k_5_d_10_vary_eps.eps}\\
	(d) [$k = 5$, $d = 2$] & (e) [$k = 5$, $d = 5$]  & (f) [$k = 5$, $d = 10$]
	\end{tabular}

	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_0.05_d_5_vary_k.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_0.2_d_5_vary_k.eps} &
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eps_1.0_d_5_vary_k.eps}\\
	(g) [$d = 5$, $\epsilon = 0.05$] & (h) [$d = 5$, $\epsilon = 0.2$]  & (i) [$d = 5$, $\epsilon = 1.0$]
	\end{tabular}

	\begin{center}
	\includegraphics[width = 5.0in]{figures/vldb/synthe-plugin-true/legend-cropped.eps}
	\end{center}
	\caption{Effects of the number of dimensions and the number of clusters}\label{fig:synthe}
\end{figure*}



\begin{figure*}[!htb]
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/dplloyd_med.eps} & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/dplloyd_min.eps} & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/eugkm_nicv.eps}\\
	\hspace{-0.5cm}(a) DPLloyd-Median & \hspace{-0.5cm}(b) DPLloyd-Best  & \hspace{-0.5cm}(c) EUGkM
	\end{tabular}

	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/hybrid_med.eps} & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/synthe-plugin-true/pgkm.eps}  & \hspace{-0.5cm}
	\includegraphics[width = 2.2in]{figures/vldb/gupt/synthe_eps_1.0.eps}\\
	\hspace{-0.5cm} (d) Hybrid & \hspace{-0.5cm} (e) PGkM & \hspace{-0.5cm} (f) GkM
	\end{tabular}


	\caption{Effects of the number of dimensions and the number of clusters, Heatmap}\label{fig:synthe-heatmap}
\end{figure*}

\end{comment}

We now compare the hybrid approach with \eugkm and \dpl. The configuration for \eugkm and \dpl is the same as in Section \ref{sec:existingAppExpt}.  For the hybrid approach, we run \eugkm 10 times to output 10 sets of intermediate centroids.  Then we run \dpl 10 times on each intermediate result.  We finally report the average of 100 NICV values. Figure \ref{fig:hybrid} gives the results on the six external datasets. In low dimensional datasets (S1, Gowalla, TIGER, and Image), the hybrid approach simply falls back to \eugkm for small $\epsilon$ value. When $\epsilon$ increases, both the hybrid approach and \eugkm converge to the baseline with the former having slightly better performance. For example, in the Gowalla dataset for $\epsilon=0.7$, the average NICV of the hybrid approach is $0.02172$ and that of \eugkm is $0.02174$.


In higher dimensional datasets (Adult-num and Lifesci), the hybrid approach outperforms the other two approaches in most cases. It is worse than \dpl only for a few small $\epsilon$ values, on which it falls back to \eugkm. There are two possible reasons. The first is that the MSE analysis assumes that datasets are well clustered and each cluster has equal size, but the real datasets are skewed. For example, the baseline approach partitions the Adult-num dataset into 5 clusters, in which the biggest cluster contains 13,894 tuples and the smallest contains 3,160 tuples. The second is that we use the variance of \eugkm as the lower bound of its MSE. Thus, it is possible that the MSE of the hybrid approach (approximated by the MSE of one-iteration \dpl with half privacy budget) is larger than the variance of \eugkm, but actually smaller than its MSE. In such cases, the hybrid approach gives lower NICV if it does not fall back to \eugkm. For example, on the Adult-num dataset for $\epsilon=0.05$, the hybrid approach of falling back to \eugkm has the NICV of $0.370$, while its NICV is $0.244$, if it applies \eugkm plus one-iteration of \dpl.



\begin{comment}
In this section we experimentally compare the two non-interactive algorithms UGkM and AUGkM, the hybrid approach, and the \dpl.  Figure \ref{fig:hybrid} reports the results.  The Hybrid approach always outperforms the existing non-interactive algorithms and interactive algorithms over all datasets.  The advantage of it is more significant in the two high dimensional datasets, Adult-num and Lifesci.  Observe that the Hybrid approach has same performance as the AUGkM in the S1 dataset.  The reason is that the two-phase hybrid approach cannot improve the AUGkM result.  So we use one-phase hybrid approach, that is running AUGkM only.

In addition, we can see that AUGkM generally outperforms UGkM in all datasets.  The difference is significant When $\epsilon$ is very small.  They generally converge to close NICV values when $\epsilon$ becomes large.  This seems to suggest that they are reaching the limit of what non-interactive methods can do.  Also, UGkM and AUGkM can never converge to the baseline, as the non-interactive methods have some inherent noises due to the partitioning.  On the other hand, the two non-interactive approaches is superior to the \dpl in the four low dimension datasets.  But \dpl outperformed them in two high dimension datasets, Adult-num and Lifesci.  Therefore, the limits of the non-interactive approaches and the strength of \dpl in high dimension datasets motive the hybrid approach.
\end{comment}

















\begin{figure*}[!htb]
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{s1-hybrid.eps} &
	\includegraphics[width = 2.2in]{gowalla2d-hybrid.eps} &
	\includegraphics[width = 2.2in]{tiger-0.01-hybrid.eps}\\
	(a) S1 [$d = 2$, $k = 15$]  & (b) Gowalla [$d = 2$, $k = 5$] & (c) TIGER [$d = 2$, $k = 2$]
	\end{tabular}
	
	\begin{tabular}{ccc}
	\includegraphics[width = 2.2in]{image-k3-hybrid.eps}&
	\includegraphics[width = 2.2in]{adult-num-hybrid.eps}&
	\includegraphics[width = 2.2in]{lifesci-k3-hybrid.eps}\\
	(d) Image [$d = 3$, $k = 3$] & (e) Adult-num [$d = 6$, $k = 5$] & (f) Lifesci [$d = 10$, $k = 3$]
	\end{tabular}
	
    \caption{The comparison of the Hybrid approach with EUGkM and \dpl. x-axis: privacy budget $\epsilon$ in log-scale. y-axis: NICV in log-scale.}\label{fig:hybrid}
\end{figure*}

We also evaluate the approaches using the synthetic datasets as generated in Section \ref{sec:existingAppExpt}. Figure~\ref{fig:heatmap-eugkm-hybrid} clearly shows that the hybrid approach is more scalable than \eugkm with respect to both $k$ and $d$. This confirms the effectiveness of the hybrid approach.

Figure~\ref{fig:running_time_dplloyd_eugkm} presents the runtime of \dpl and \eugkm on the six external datasets.  We follow the same experiment configuration as in Section~\ref{sec:existingAppExpt}.  As expected, the runtime of \dpl is much lower than that of \eugkm.  This is because \eugkm has to run \km clustering over 30 sets of initial centroids and output the centroids with the best NICV relative to the noisy synopsis.  Another reason is that \dpl sets the number of iterations to 5 while \eugkm runs \km clustering until converge.  


\begin{figure}[h]
\begin{tabular}{cc}
\includegraphics[width = 1.8in]{eugkm_nicv.eps}&
\hspace{-0.8cm}\includegraphics[width = 1.8in]{hybrid_med.eps}\\
(a) \eugkm & \hspace{-0.8cm} (b)Hybrid
\end{tabular}
\caption{Comparing hybrid and \eugkm by the heatmap}\label{fig:heatmap-eugkm-hybrid}
\end{figure}



\begin{figure}[h]
\includegraphics[width = 3.1in]{running_time.eps}
\caption{Comparing running time between \dpl and \eugkm, $\epsilon = 0.1$}\label{fig:running_time_dplloyd_eugkm}
\end{figure}





