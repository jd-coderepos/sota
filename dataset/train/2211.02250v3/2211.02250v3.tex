



\begin{table}
\caption{Performance on the single-target extraction task. In our model,  and  correspond to encoder and decoder dimensionalities, respectively. RTF is the real-time factor for a consumer  CPU.}
\vspace{-.8em}
\centering
\begin{tabular}{lccc}
\toprule
Model                   & Model size & RTF  & SI-SNRi  \\
\midrule
Conv-TasNet              & 4.57M      & 1.34 & 6.14    \\
ReSepformer             & 13.24M     & 1.60 & {7.26}    \\
Ours () & 1.10M      & 0.66 & 9.02    \\
Ours () & 1.69M      & 0.75 & 9.40    \\
Ours () & 3.29M      & 0.88 & 9.26    \\
Ours () & 3.88M      & 0.94 & \textbf{9.43}    \\
\bottomrule
\end{tabular}
\label{table:single_target}
\end{table}



\begin{table}[t!]
\caption{SI-SNRi  comparison in the multi-target extraction task.}
\vspace{-.8em}
\centering

\begin{tabular}{l  c c c c}
\toprule
 & \multicolumn{3}{c}{\# selected classes} \\
Model                   & 1 & 2  & 3 & Mean \\
\midrule
Conv-TasNet              & 7.20 & 3.63 & 0.19 & 3.67 \\
ReSepformer              & 7.42 & 3.56 & 0.33 & 3.77 \\
Ours ()  & 9.06 & 4.78 & 1.51 & 5.11 \\
Ours ()  & 9.12 & 4.76 & 1.31 & 5.06 \\
Ours ()  & 9.39 & 4.92 & 1.39 & 5.23 \\
Ours ()  & 9.29 & 4.92 & 1.35 & 5.19 \\
\bottomrule
\end{tabular}
\vskip -0.15in
\label{table:multi_target}
\end{table}



\section{Experiments and results}








{\bf Dataset.} We use a synthetic sound mixture dataset created from the  FSD Kaggle 2018 dataset~\cite{Fonseca2018_DCASE}. FSD Kaggle 2018 is a set of sound event and class label pairs, with 41 different sound classes, which are a subset of the Audioset ontology \cite{gemmeke2017audio}. Our synthetic dataset consists of 50k training samples, 5k validation samples, and 10k test samples. {Sound mixtures are created using the Scaper toolkit \cite{8170052} with FSD Kaggle 2018 and TAU Urban Acoustic Scenes 2019 ~\cite{Mesaros2018_DCASE} as foreground and background sources, respectively}. Foreground sound classes are randomly sampled without replacement so that each sample has 3-5 unique classes. We construct the sound mixtures by sampling 3-5s crops from each foreground sound and then pasting them on a 6s background sound. The SNRs of the foreground sounds are randomly chosen between 15 and 25 dB, relative to the background sound. Our training and validation data are sampled from the development splits of FSD Kaggle 2018 and TAU Urban Acoustic Scenes 2019, while our test samples are  from the test splits.
From each mixture, up to 3 foreground sounds are randomly selected as targets.
During training, the choices of the target foreground sounds in the training set are randomized. Since we mainly consider human listening applications for streaming target sound extraction, we run  our experiments at a 44.1 kHz  sampling rate  to cover the full audible range.










\begin{figure}[t!]
\centering
\begin{subfigure}[l]{\linewidth}
    \includegraphics[width=\linewidth]{Figs/qual-mixed.png}
    \caption{Input mixture}
    \label{fig:qual_mixture}
\end{subfigure}
\centering
\begin{subfigure}[l]{\linewidth}
    \includegraphics[width=\linewidth]{Figs/qual-gt-Bark.png}
    \caption{Ground-truth for label `Bark'}
    \label{fig:qual_gt_bark}
\end{subfigure}
\centering
\begin{subfigure}[l]{\linewidth}
    \includegraphics[width=\linewidth]{Figs/qual-gt-Cough.png}
    \caption{Ground-truth for label `Cough'}
    \label{fig:qual_gt_cough}
\end{subfigure}
\centering
\begin{subfigure}[l]{\linewidth}
    \includegraphics[width=\linewidth]{Figs/qual-output-single.png}
    \caption{Output of the single-target extraction of `Bark'}
    \label{fig:qual_output_single}
\end{subfigure}
\centering
\begin{subfigure}[l]{\linewidth}
    \includegraphics[width=\linewidth]{Figs/qual-output-multi.png}
    \caption{Output of the multi-target extraction of `Bark' and `Cough'}
    \label{fig:qual_output_multi}
\end{subfigure}
\caption{{Visualization of time-domain waveforms of single-target and multi-target extraction (x-axis represents time). }}
\label{fig:qual}
\vskip -0.2in
\end{figure}

\vskip 0.05in\noindent{\bf Evaluation setup.} Prior works \cite{2020arXiv200605712O, delcroix2022soundbeam} show that Conv-TasNet,   originally proposed for speech separation, can also be used for target sound extraction. Further,   ReSepformer proposes an efficient transformer architecture for speech separation that allows a streaming inference. Here, we compare the performance of our architecture  with the causal or streaming implementations of Conv-TasNet and ReSepformer as described in the original papers~\cite{luo2019conv,subakan2022resource} for the  target sound extraction task. 




For all the models, we set the stride of the initial convolution, , to 32, which is  about  ms at 44.1 kHz. We train multiple configurations of our model with different encoder and decoder dimensions. We fix the number of DCC layers to 10, the number of transformer layers to 1, and the chunk length, , to 13. This chunk length corresponds to 416 samples in the time domain or a chunk duration of 9.43 ms.   
For Conv-TasNet, we follow the configuration used in \cite{2020arXiv200605712O} except for the number of repeats, which we set to 2. This ensures that the runtime of the Conv-TasNet baseline is not too large compared with that of our model's largest configuration. For the ReSepformer baseline, we set the model dimensionality to 512, the number of blocks to 2, the number of transformer layers to 2, and the chunk size to 13 (9.43 ms). We perform label integration after the first transformer block, as we found it to perform better than integrating it  at the beginning.

\vskip 0.05in\noindent{\bf Loss function and training hyper-parameters.} { We use a linear combination of 90\% signal-to-noise-ratio (SNR) and 10\% scale-invariant-signal-to-noise-ratio (SI-SNR) \cite{https://doi.org/10.48550/arxiv.1811.02508} as the loss function for training.
We set the initial learning rate to 5e-4 for our models and Conv-TasNet, and to 1.5e-4 for  ReSepformer. We train the models for 100 epochs and choose the model after the epoch that resulted in the best validation SI-SNRi.
}


\vskip 0.05in\noindent{\bf Results.} We separately train the models for single-target and multi-target extraction tasks and evaluate them on our testset. For multi-target evaluation, we train our model as well as baselines to make predictions with multi-hot query vectors, as opposed to one-hot queries used in the single-target evaluation. { During the multi-target training, 1-3 foreground sounds are randomly selected as target sounds. This training method using an arbitrary number of target sources helps the model learn multi-target embeddings. The same model configurations are used for both the single-target and multi-target experiments.  The Conv-TasNet and ReSepfromer baselines are also trained in the same way for the multi-target extraction task.


We also evaluate the real-time factors (RTFs) of the models on an Intel Core i5 CPU using a single thread. RTF is computed by measuring the runtime consumed by the models to process a 416 sample audio chunk (9.43 ms at 44.1 kHz), and dividing that by the chunk duration, 9.43 ms. {For the RTF measurement, we include the padding for dilated convolution layers in our model's DCC encoder and Conv-TasNet's Temporal Convolution Network (TCN) blocks, accounting for the entire receptive field. In the case of the ReSepformer, using a single chunk for RTF measurement excludes the overhead caused by causal attention masking in its inter-attention blocks. Consequently, the RTF value reported for ReSepformer is a lower bound of what is practically achievable.}}

Table~\ref{table:single_target} compares our models with different configurations with the baselines in terms of both efficiency and performance. We show that our approach  results in 2.2-3.3 dB SI-SNRi improvement compared with the baselines while being 1.5-2x more computationally efficient with 1.2-4x fewer parameters. Table~\ref{table:multi_target}  compares the performance of our models with the baselines for the multiple  target extraction task. It shows  that our method outperforms the baselines by 1.2-1.4 dB for the 2-target case and 1-1.2 dB for the 3-target case. As with prior work~\cite{2020arXiv200605712O}, the SI-SNR improvements are  lower in the 3-target selection task since there is greater similarity between the input mixture and the target signal, compared to the single-target  case, resulting in a larger input SI-SNR. {We obtained -values  for all comparisons except for the comparison between  and , for which the -value was .}



{
In Fig.~\ref{fig:qual}, we qualitatively show an example of single-target extraction and multi-target extraction from a 4-class input mixture, using our multi-target extraction model. Fig.~\ref{fig:qual_mixture} shows the input mixture waveform, and Figs.~\ref{fig:qual_gt_bark} and \ref{fig:qual_gt_cough} show the isolated ground-truth sounds. We provide the input mixture to our multi-target model, with a single-target query followed by a two-target query. Figs.~\ref{fig:qual_output_single} and \ref{fig:qual_output_multi} are the output waveforms obtained when the single target and the two targets are queried, respectively. The waveforms show that the model successfully recognizes the queried events and extracts the relevant sounds. It can also be observed that our model preserves the original amplitudes of the sounds in the input mixture well.
}



{We also implemented a non-causal version of the proposed Waveformer. The dilated causal convolution (DCC) block was made non-causal by padding on both sides of each DCC layerâ€™s input sequence, while the causal version only padded to the left. The non-causal transformer decoder attends to the previous and next chunks, in addition to the current chunk. Following \cite{2020arXiv200605712O}, we trained our non-causal model with both one-hot/multi-hot based extraction and Permutation Invariant Training + Oracle Selection (PIT + OS) objectives.  Table \ref{table:non_causal} compares the performance of the non-causal version of our model with non-causal baselines. The performance of our causal model is only 1.1 dB less than the non-causal version, in contrast, to the 3--5 dB observed in prior source separation works \cite{luo2019conv, subakan2022resource}. We achieve this resilience by using layer normalization throughout our architecture (avoiding the gLN to cLN switch in \cite{luo2019conv,subakan2022resource}) and a small context length for the transformer decoder.}













\begin{table}
\label{non_causal}
\caption{{Performance comparison with non-causal baselines.}}
\vspace{-.8em}
\centering
\begin{tabular}{lccc}
\toprule
Model                   & SI-SNRi  \\
\midrule
Listen to What You Want \cite{2020arXiv200605712O} & 9.91 \\
Ours (; Non-causal) & 10.50 \\
Ours (; Non-causal; PIT+OS) & 11.31 \\
\bottomrule
\end{tabular}
\label{table:non_causal}
\vskip -0.15in
\end{table}