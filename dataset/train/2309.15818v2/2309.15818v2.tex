
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{animate}
\usepackage[misc]{ifsym}
\usepackage{tabularx}
\usepackage{caption}


\definecolor{citecolor}{HTML}{0071BC}
\definecolor{linkcolor}{HTML}{ED1C24}
\definecolor{highlightcolor}{HTML}{ABCDEF}
\usepackage[pagebackref, breaklinks, colorlinks, letterpaper=true, citecolor=citecolor, linkcolor=linkcolor, bookmarks=false]{hyperref}


\title{Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation}







\author{\quad\quad\quad\quad\quad David Junhao Zhang\textsuperscript{*} 
  \quad\quad Jay Zhangjie Wu\textsuperscript{*}
  \quad\quad Jia-Wei Liu\textsuperscript{*}
  \AND Rui Zhao 
  \And Lingmin Ran 
  \And Yuchao  Gu 
  \And Difei Gao 
  \And Mike Zheng Shou \textsuperscript{\Letter} 
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}

\makeatletter
\let\oldfootnote\footnote
\def\footnote{\@ifstar\footnote@star\footnote@nostar}
\def\footnote@star#1{{\let\thefootnote\relax\footnotetext{#1}}}
\def\footnote@nostar{\oldfootnote}
\makeatother


\iclrfinalcopy \begin{document}

\footnote*{\textsuperscript{*}Equal Contribution. \textsuperscript{\Letter} Corresponding Author.}

\maketitle
\vspace{-2em}
\begin{center}
    Show Lab, National University of Singapore \\
    \vspace{2em}
    \url{https://showlab.github.io/Show-1/}
    \vspace{2mm}
\end{center}



{
\vspace{-1mm}
\setlength{\tabcolsep}{0.5pt}
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{c c c}
\animategraphics[width=0.33\textwidth]{8}{figures/teaser/cat/}{0001}{0029} & 
    \animategraphics[width=0.33\textwidth]{8}{figures/teaser/panda/}{0001}{0029} &
    \animategraphics[width=0.33\textwidth]{8}{figures/teaser/toad/}{0001}{0029} 
\end{tabular}
\begin{tabularx}{\textwidth}{m{0.318\textwidth} c m{0.318\textwidth} c m{0.318\textwidth}} 
    \emph{\small Close up of mystic cat, like a buring phoenix, red and black colors.} & \hspace{0.6em} & 
    \emph{\small A panda besides the waterfall is holding a sign that says ``Show 1".} & \hspace{0.6em} &
    \emph{\small Toad practicing karate.} 
\end{tabularx}
\begin{tabular}{c c c}
    \animategraphics[width=0.33\textwidth]{8}{figures/teaser/octopus/}{0001}{0029} &
    \animategraphics[width=0.33\textwidth]{8}{figures/teaser/knight/}{0001}{0029} & 
    \animategraphics[width=0.33\textwidth]{8}{figures/teaser/motor/}{0001}{0029}
\end{tabular}
\begin{tabularx}{\textwidth}{m{0.318\textwidth} c m{0.318\textwidth} c m{0.318\textwidth}} 
    \emph{\small Giant octopus invades new york city.} & \hspace{0.6em} &
    \emph{\small A medieval knight in gleaming armor rides confidently on a majestic horse, the sun setting behind them.} & \hspace{0.6em} & 
    \emph{\small Motorcyclist with space suit riding on moon with storm and galaxy in background.} 
\end{tabularx}
\captionof{figure}{Given text description, our approach generates highly faithful and photorealistic videos. \emph{\textcolor{magenta}{Click} the image to play the video clips. Best viewed with Adobe Acrobat Reader.}}
\vspace{5mm}
\label{fig:teaser}
}


\begin{abstract}

Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient  (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at  \url{https://github.com/showlab/Show-1}.
\end{abstract}



\section{Introduction}



\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/intro.pdf}
    \caption{Text-Video alignment comparisons among pixel-based VDM at low resolution, latent-based VDM at low resolution and latent-based at relatively high resolution.}
    \label{fig:intro}
\end{figure}


Remarkable progress has been made in developing large-scale pre-trained text-to-Video Diffusion Models (VDMs), including closed-source ones (\eg, Make-A-Video~\citep{singer2022make}, Imagen Video~\citep{ho2022imagen}, Video LDM~\citep{blattmann2023align}, Gen-2~\citep{esser2023structure}) and open-sourced ones (\eg, VideoCrafter~\citep{he2022latent}, ModelScopeT2V~\citep{wang2023modelscope}. 
These VDMs can be classified into two types: (1) Pixel-based VDMs that directly denoise pixel values, including Make-A-Video~\citep{singer2022make}, Imagen Video~\citep{ho2022imagen}, PYoCo~\citep{ge2023preserve}, and (2) Latent-based VDMs that manipulate the compacted latent space within a variational autoencoder (VAE), like Video LDM~\citep{blattmann2023align} and MagicVideo~\citep{zhou2022magicvideo}. 

However, both of them have pros and cons. \textbf{Pixel-based VDMs} can generate motion accurately aligned with the textual prompt but typically demand expensive computational costs in terms of time and GPU memory, especially when generating high-resolution videos.
\textbf{Latent-based VDMs} are more resource-efficient because they work in a reduced-dimension latent space.
But it is challenging for such small latent space (\eg,  for  videos) to cover rich yet necessary visual semantic details as described by the textual prompt.
Therefore, as shown in Fig.~\ref{fig:intro}, the generated videos often are not well-aligned with the textual prompts. On the other hand, if the generated videos are of relatively high resolution (\eg,  videos), the latent model will focus more on spatial appearance but may also ignore the text-video alignment.









To marry the strength and alleviate the weakness of pixel-based and latent-based VDMs, we introduce Show-1, an efficient text-to-video model that generates videos of not only decent video-text alignment but also high visual quality. Further, Show-1 can be trained on large-scale datasets with manageable computational costs.
Specifically, we follow the conventional coarse-to-fine video generation pipeline ~\citep{ ho2022imagen, blattmann2023align} which starts with a  module to produce keyframes at a low resolution and a low frame rate. Then we employs a temporal interpolation module and super-resolution module to increase temporal and spatial quality respectively.

In these  modules, prior studies typically employ either pixel-based or latent-based VDMs across all modules. While purely pixel-based VDMs tend to have heavy computational costs, exclusively latent-based VDMs can result in poor text-video alignment and motion inconsistencies.
In contrast, we combine them into Show-1 as shown in Fig.~\ref{fig:architecture}. To accomplish this, we employ pixel-based VDMs for the keyframe module and the temporal interpolation module at a low resolution, producing key frames of precise text-video alignment and natural motion with low computational cost.
Regarding super-resolution, we find that latent-based VDMs, despite their inaccurate text-video alignment, can be re-purposed to translate low-resolution video to high-resolution video, while maintaining the original appearance and the accurate text-video alignment of low-resolution video.
Inspired by this finding, for the first time, we propose a novel two-stage super-resolution module that first employs pixel-based VDMs to upsample the video from  to  and then design a novel expert translation module based on latent-based VDMs to further upsample it to  with low computation cost.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/method.pdf}
    \caption{Overview of Show-1. Pixel-based VDMs produce videos of lower resolution with better text-video alignment, while latent-based VDMs upscale these low-resolution videos from pixel-based VDMs to then create high-resolution videos with low computation cost.}
    \label{fig:architecture}
\end{figure}










In summary, our paper makes the following key contributions:

\begin{itemize}
    \item Upon examining pixel and latent VDMs, we discovered that: 1) pixel VDMs excel in generating low-resolution videos with more natural motion and superior text-video synchronization compared to latent VDMs; 2) when using the low-resolution video as an initial guide, conventional latent VDMs can effectively function as super-resolution tools by \textbf{simple expert translation}, refining spatial clarity and creating high-quality videos with greater efficiency than pixel VDMs.
   
    \item We are the first to integrate the strengths of both pixel and latent VDMs, resulting into a novel video generation model that can produce high-resolution videos of precise text-video alignment at low computational cost (15G GPU memory during inference).

    \item Our approach achieves state-of-the-art performance on standard benchmarks including UCF-101 and MSR-VTT. 




\end{itemize}


\section{Previous Work}

\noindent\textbf{Text-to-image generation.} \citep{reed2016generative} stands as one of the initial methods that adapts the unconditional Generative Adversarial Network (GAN) introduced by \citep{goodfellow2014generative} for text-to-image (T2I) generation. Later versions of GANs delve into progressive generation, as seen in \citep{zhang2017stackgan} and \citep{hong2018inferring}. Meanwhile, works like \citep{xu2018attngan} and \citep{zhang2021cross} seek to improve text-image alignment. Recently, diffusion models have contributed prominently to advancements in text-driven photorealistic and compositional image synthesis~\citep{ramesh2022hierarchical,saharia2022photorealistic}.   For attaining high-resolution imagery, two prevalent strategies emerge. One integrates cascaded super-resolution mechanisms within the RGB domain~\citep{nichol2021glide,ho2022cascaded,saharia2022photorealistic,ramesh2022hierarchical}. In contrast, the other harnesses decoders to delve into latent spaces~\citep{rombach2022high,gu2022vector}.  Owing to the emergence of robust text-to-image diffusion models, we are able to utilize them as solid initialization
 of text to video models.
 
\noindent\textbf{Text-to-video generation.} 
Past research has utilized a range of generative models, including GANs~\citep{vondrick2016generating,saito2017temporal,Tulyakov_2018_CVPR,tian2021a,Shen_2023_CVPR}, Autoregressive models~\citep{srivastava2015unsupervised,yan2021videogpt,le2021ccvs,ge2022long,hong2022cogvideo}, and implicit neural representations~\citep{skorokhodov2021stylegan,yu2021generating}. Inspired by the notable success of the diffusion model in image synthesis, several recent studies have ventured into applying diffusion models for both conditional and unconditional video synthesis~\citep{voleti2022masked,harvey2022flexible,zhou2022magicvideo,wu2022tune,blattmann2023videoldm,khachatryan2023text2video,hoppe2022diffusion,voleti2022masked,yang2022diffusion,nikankin2022sinfusion,luo2023videofusion,an2023latent,wang2023videofactory}. Several studies have investigated the hierarchical structure, encompassing separate keyframes, interpolation, and super-resolution modules for high-fidelity video generation. Magicvideo~\citep{zhou2022magicvideo} and Video LDM~\citep{blattmann2023align} ground their models on latent-based VDMs. On the other hand, PYoCo~\citep{ge2023preserve}, Make-A-Video~\citep{singer2022make}, and Imagen Video~\citep{ho2022imagen} anchor their models on pixel-based VDMs. Contrary to these approaches, our method seamlessly integrates both pixel-based and latent-based VDMs.


\vspace{-3mm}

\section{\textit{Show-1}}
\subsection{Preliminaries}

\noindent\textbf{Denoising Diffusion Probabilistic Models (DDPMs).} DDPMs, as detailed in~\citep{ho2020denoising}, represent generative frameworks designed to reproduce a consistent forward Markov chain . 
Considering a data distribution  , the Markov transition  is conceptualized as a Gaussian distribution, characterized by a variance . Formally, this is defined as:
{\small
}Applying the principles of Bayes and the Markov characteristic, it's feasible to derive the conditional probabilities  and , represented as:
{\small

}

  
In order to synthesize the chain , DDPMs utilize a reverse approach, characterized by a prior  and Gaussian transitions. This relation is:
{\small
}The model's adaptable parameters  are optimized to ensure the synthesized reverse sequence aligns with the forward sequence.

In their essence, DDPMs adhere to the variational inference strategy, focusing on enhancing the variational lower bound of the negative log-likelihood. Given the KL divergence among Gaussian distributions, this approach is practical. In practice, this framework resembles a series of weight-shared denoising autoencoders , trained to render a cleaner version of their respective input . This is succinctly represented by: 

\noindent\textbf{UNet architecture for text to image model.}
The UNet model is introduced by \citep{2015u} for biomedical image segmentation. Popular UNet for text-to-image diffusion model usually contains multiple down, middle, and up blocks. Each block consists of a resent2D layer, a self-attention layer, and a cross-attention layer. Text condition  is inserted into to cross-attention layer as keys and values. For a text-guided Diffusion Model, with the text embedding   the objective is given by:






\subsection{Turn Image UNet to Video} We incorporate the spatial weights from a robust text-to-image model. To endow the model with temporal understanding and produce coherent frames, we integrate temporal layers within each UNet block. Specifically, after every Resnet2D block, we introduce a temporal convolution layer consisting of four 1D convolutions across the temporal dimension. Additionally, following each self and cross-attention layer, we implement a temporal attention layer to facilitate dynamic temporal data assimilation. 
Specifically, a frame-wise input video ,
where  is the number of  channels and  and  are the spatial latent dimensions.
The spatial layers regard the video as a batch of independent images (by transposing the temporal axis into the batch dimension), and for each temporal layer, the video is reshaped back to temporal dimensions.

\subsection{Pixel-based Keyframe Generation Model} Given a text input, we initially produce a sequence of keyframes using a pixel-based Video UNet at a very low spatial and temporal resolution. This approach results in improved text-to-video alignment. The reason for this enhancement is that we do not require the keyframe modules to prioritize appearance clarity or temporal consistency. As a result, the keyframe modules pays more attention to the text guidance. The training objective for the keyframe modules is following Eq.~\ref{objective}.  

\noindent\textbf{Why we choose pixel diffusion over latent diffusion here?}
Latent diffusion employs an encoder to transform the original input  into a latent space. This results in a reduced spatial dimension, for example, , while concentrating the semantics and appearance into this latent domain. For generating keyframes, our objective is to have a smaller spatial dimension, like . If we opt for latent diffusion, this spatial dimension would shrink further, perhaps to around , which might not be sufficient to retain ample spatial semantics and appearance within the compacted latent space. On the other hand, pixel diffusion operates directly in the pixel domain, keeping the original spatial dimension intact. This ensures that  necessary semantics and appearance information are preserved. For the following low resolution stages, we all utilize pixel-based VDMs for the same reason.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/module.pdf}
    \caption{3D UNet and input of the UNet of interpolation and super-resolution modules. (a) shows that how we insert temporal modules into 2D UNet. (b) explains the input for interpolation and first super-resolution UNet.}
    \label{fig:block}
\end{figure}

\subsection{Temporal Interpolation Model}

To enhance the temporal resolution of videos we produce, we suggest a pixel-based temporal interpolation diffusion module. This method iteratively interpolates between the frames produced by our keyframe modules. The pixel interpolation approach is built upon our keyframe modules, with all parameters fine-tuned during the training process. We employ the masking-conditioning mechanism, as highlighted in \citep{blattmann2023align}, where the target frames for interpolation are masked. In addition to the original pixel channels C, as shown in Fig.~\ref{fig:block} we integrate 4 supplementary channels into the U-Net's input: 3 channels are dedicated to the RGB masked video input, while a binary channel identifies the masked frames. As depicted in the accompanying figure, during a specific noise timestep, we interpolate three frames between two consecutive keyframes, denoted as  and . For the added 3 channels, values of  and  remain true to the original pixel values, while the interpolated frames are set to zero. For the final mask channel, the mask values  and  are set to 1, signifying that both the initial and concluding frames are available, with all others set to 0. In conclusion, we merge these components based on the channel dimension and input them into the U-Net. For  and , we implement noise conditioning augmentation. Such augmentation is pivotal in cascaded diffusion models for class-conditional generation, as observed by \citep{ho2022imagen}, and also in text-to-image models as noted by \citep{he2022latent}. Specifically, this method aids in the simultaneous training of diverse models in the cascade. It minimizes the vulnerability to domain disparities between the output from one cascade phase and the training inputs of the following phase. Let the interpolated video frames be represented by . Based on Eq.~\ref{objective}, we can formulate the updated objective as:


\label{inter}

\vspace{-5mm}
\subsection{Super-resolution at Low Spatial Resolution}
To improve the spatial quality of the videos, we introduce a pixel super-resolution approach utilizing the video UNet. For this enhanced spatial resolution, we also incorporate three additional channels, which are populated using a bilinear upscaled low-resolution video clip, denoted as   through bilinear upsampling. In line with the approach of\citep{ho2022video}, we employ Gaussian noise augmentation to the upscaled low resolution video condition during its training phase, introducing a random signal-to-noise ratio. The model is also provided with this sampled ratio. During the sampling process, we opt for a consistent signal-to-noise ratio, like 1 or 2. This ensures minimal augmentation, assisting in the elimination of artifacts from the prior phase, yet retaining a significant portion of the structure.

Given that the spatial resolution remains at an upscaled version throughout the diffusion process, it's challenging to upscale all the interpolated frames, denoted as , to  simultaneously on a standard GPU with  24G memory. Consequently, we must divide the frames into four smaller segments and upscale each one individually.  

However, the continuity between various segments is compromised. To rectify this, as depicted in the Fig.~\ref{fig:block}, we take the upscaled last frame of one segment to complete the three supplementary channels of the initial frame in the following segment.

\vspace{-3mm}

\subsection{Super-resolution at High Spatial Resolution}
\label{section3.6}

Through our empirical observations, we discern that a latent-based VDM can be effectively utilized for enhanced super-resolution with high fidelity. Specifically, we design a distinct latent-based VDM that is tailored for high-caliber, high-resolution data. We then apply a noising-denoising procedure, as outlined by \emph{SDEdit}~\citep{meng2021sdedit}, to the samples from the preliminary phase. As pointed out by \citep{balaji2022ediffi}, various diffusion steps assume distinct roles during the generation process. For instance, the initial diffusion steps, such as from 1000 to 900, primarily concentrate on recovering the overall spatial structure, while subsequent steps delve into finer details. Given our success in securing well-structured low-resolution videos, we suggest adapting the latent VDM to specialize in high-resolution detail refinement. More precisely, we train a UNet for only the 0 to 900 timesteps (with 1000 being the maximum) instead of the typical full range of 0 to 1000, directing the model to be a expert emphasizing high-resolution nuances. This strategic adjustment significantly enhances the end video quality, namely expert translation. 
During the inference process, we use bilinear upsampling on the videos from the prior stage and then encode these videos into the latent space. Subsequently, we carry out diffusion and denoising directly in this latent space using the latent-based VDM model, while maintaining the same text input. This results in the final video, denoted as 
  .  
  
\noindent\textbf{Why we choose latent-based VDM over pixel-based VDM here?}  Pixel-based VDMs work directly within the pixel domain, preserving the original spatial dimensions. Handling high-resolution videos this way can be computationally expensive. In contrast, latent-based VDMs compress videos into a latent space (for example, downscaled by a factor of 8), which results in a reduced computational burden. Thus, we opt for the latent-based VDMs in this context.


\vspace{-2mm}

\section{Experiments}






\subsection{Implementation Details}

For the generation of pixel-based keyframes, we utilized DeepFloyd\footnote{https://github.com/deep-floyd/IF} as our pre-trained Text-to-Image model for initialization, producing videos of dimensions . In our interpolation model, we initialize the weights using the keyframes generation model and produce videos with dimensions of . For our initial  model, we employ DeepFloyd's SR model for spatial weight initialization, yielding videos of size . In the subsequent super-resolution model, we modify the ModelScope text-to-video model  and use our proposed expert translation to generate videos of .


\begin{table}[t]
\centering
\caption{Zero-shot text-to-video generation on UCF-101. Our approach achieves competitive results  in  both inception score and FVD metrics.}


\begin{tabular}{lcc}
\hline
Method              & IS     & FVD     \\
\midrule
CogVideo~\citep{hong2022cogvideo} (English) & 25.27 & 701.59 \\
Make-A-Video~\citep{singer2022make}         & \underline{33.00} & \textbf{367.23} \\
MagicVideo~\citep{zhou2022magicvideo}   & - & 655.00 \\
Video LDM~\citep{blattmann2023align}   & 33.45 & 550.61 \\
VideoFactory~\citep{wang2023videofactory}  & - & 410.00 \\
\hline

Show-1 (ours)      & \textbf{35.42} &  \underline{394.46} \\
\hline
\end{tabular}

\label{ucf1012}
\end{table}


 \begin{table}[t]
\centering


\caption{
Quantitative comparison with state-of-the-art models on MSR-VTT. Our approach achieves the state-of-the-art performance.}
\begin{tabular}{c|ccc}
\hline
\textbf{Models}              & FID-vid ()   &  FVD () & CLIPSIM () \\ \hline
N\"UWA~\citep{wu2022nuwa}       & 47.68   & -  & 0.2439        \\
CogVideo (Chinese)~\citep{hong2022cogvideo}   & 24.78 & - & 0.2614      \\
CogVideo (English)~\citep{hong2022cogvideo}   & 23.59  & 1294 & 0.2631  \\
MagicVideo~\citep{zhou2022magicvideo}          & -    & 1290 & -        \\
Video LDM~\citep{blattmann2023align}      & -   &  - & 0.2929 \\ 
Make-A-Video~\citep{singer2022make}        & 13.17 &  -& 0.3049     \\ 
ModelScopeT2V~\citep{wang2023modelscope}  &   11.09 & 550 & 0.2930    \\ 
\hline

Show-1 (ours) &  \textbf{13.08} & \textbf{538} & \textbf{0.3072}   \\ \hline
\end{tabular}



\label{mrtt}
\end{table} 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/compare_1.pdf}
    \caption{Qualitative comparison with existing video generative models. Words in red highlight the misalignment between text and video in other open-source approaches (\ie, ModelScope and ZeroScope), whereas our method maintains proper alignment. Videos from closed-source approaches (\ie, Imagen Video and Make-A-Video) are obtained from their websites.}
    \label{fig:compare_1}
\end{figure}

The dataset we used for training is WebVid-10M~\citep{bain2021frozen}. 
Training and hyperparameterdetails can be found in appendix Table~\ref{table:hyperparameters_ablation}.




\subsection{Quantitative Results}

\noindent\textbf{UCF-101 Experiment.} For our preliminary evaluations, we employ IS and FVD metrics. UCF-101 stands out as a categorized video dataset curated for action recognition tasks. When extracting samples from the text-to-video model, following PYoCo \citep{ge2023preserve}, we formulate a series of prompts corresponding to each class name, serving as the conditional input. This step becomes essential for class names like \textit{jump rope}, which aren't intrinsically descriptive.  We generate 20 video samples per prompt to determine the IS metric. For FVD evaluation, we adhere to methodologies presented in prior studies~\citep{le2021ccvs,tian2021a} and produce 2,048 videos.

From the data presented in Table~\ref{ucf1012}, it's evident that Show-1's zero-shot capabilities outperform or are on par with other methods. This underscores Show-1's superior ability to generalize effectively, even in specialized domains. It's noteworthy that our keyframes, interpolation, and initial super-resolution models are solely trained on the publicly available WebVid-10M dataset, in contrast to the Make-A-Video models, which are trained on other data.


\noindent\textbf{MSR-VTT Experiment.}
The MSR-VTT dataset~\citep{xu2016msr} test subset comprises  videos, accompanied by  captions. Every video in this set maintains a uniform resolution of . We carry out our evaluations under a zero-shot setting, given that Show-1 has not been trained on the MSR-VTT collection. In this analysis, Show-1 is compared with state-of-the-art models, on performance metrics including FID-vid~\citep{heusel2017gans_nash_equilibrium}, FVD~\citep{unterthiner2018FVD}, and CLIPSIM~\citep{wu2021godiva}. For FID-vid and FVD assessments, we randomly select 2,048 videos from the MSR-VTT testing division. CLIPSIM evaluations utilize all the captions from this test subset, following the approach ~\citep{singer2022make}. All generated videos consistently uphold a resolution of .

Table~\ref{mrtt} shows that, Show-1  achieves the best performance in both FID-vid (a score of 13.08) and FVD (with a score of 538). This suggests a remarkable visual congruence between our generated videos and the original content. Moreover, our model secures a notable CLIPSIM score of 0.3076, emphasizing the semantic coherence between the generated videos and their corresponding prompts. It is noteworthy that our CLIPSIM score surpasses that of Make-A-Video~\citep{singer2022make}, despite the latter having the benefit of using additional training data beyond WebVid-10M.
\begin{table}[t]
\centering
\caption{Human evaluation on  state-of-the-art open-sourced text-to-video models.}
 \centering
	\resizebox{0.7\textwidth}{!}{
\begin{tabular}{c |cc c }
\hline
   & Video Quality  & Text-Video alignment & Motion Fidelity  \\
\hline
Ours  ModelScope &    &   &  \\
Ours  ZeroSope &     &     &      \\


\hline
\end{tabular}}
\label{human}
\end{table} 

\begin{table}[t]
\centering
\caption{Comparisons of different combinations of pixel-based and latent-based VDMs in terms of text-video similarity and memory usage during inference.}
 \centering
	\resizebox{0.8\textwidth}{!}{
\begin{tabular}{c c|c c }
\hline
  Low resolution stage & High resolution stage & CLIPSIM & Max memory  \\
\hline
latent-based & latent-based & 0.2934 & 15GB  \\
latent-based & pixel-based & -- &  72GB \\
pixel-based & pixel-based & -- &  72GB \\
latent-based & pixel-based &\textbf{0.3072} & 15GB \\

\hline
\end{tabular}}
\label{ablation1}
\end{table} \begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation.pdf}
    \caption{Qualitative comparison for our expert translation. With expert translation, the visual quality is significantly improved.}
    \label{fig:compare_2}
\end{figure}


\noindent\textbf{Human evaluation.}
We gather an evaluation set comprising 120 prompts that encompass camera control, natural scenery, food, animals, people, and imaginative content. The survey is conducted on Amazon Mechanical Turk. Following Make a Video \citep{singer2022make}, we assess video quality, the accuracy of text-video alignment and motion fidelity. In evaluating video quality, we present two videos in a random sequence and inquire from annotators which one possesses superior quality. When considering text-video alignment, we display the accompanying text and prompt annotators to determine which video aligns better with the given text, advising them to overlook quality concerns. For motion fidelity, we let annotators to determine which video has the most natural notion. As shown in Table~\ref{human}, our method achieves the best human preferences on all evaluation parts.


\subsection{Qualitative Results}

As depicted in Fig.~\ref{fig:compare_1}, our approach exhibits superior text-video alignment and visual fidelity compared to the recently open-sourced ModelScope~\citep{wang2023modelscope} and ZeroScope\footnote{https://huggingface.co/cerspense/zeroscope-v2-576w}. Additionally, our method matches or even surpasses the visual quality of the current state-of-the-art methods, including Imagen Video and Make-A-Video. 





\vspace{-2mm}
\subsection{Ablation studies.}
\noindent\textbf{Impact of different combinations of pixel-based and latent-based VDMs.} To assess the integration method of pixel and latent-based VDMs, we conduct several ablations. For fair comparison, we employe the T5 encoder~\citep{2020t5} for text embedding in all low-resolution stages and the CLIP text encoder~\citep{clip} for high-resolution stages. As indicated in Tab.~\ref{ablation1}, utilizing pixel-based VDMs in the low-resolution stage and latent diffusion for high-resolution upscaling results in the highest CLIP score with reduced computational expenses. On the other hand, implementing pixel-based VDMs during the high-resolution upscaling stage demands significant computational resources. These findings reinforce our proposition that combining pixel-based VDMs in the low-resolution phase and latent-based VDMs in the high-resolution phase can enhance text-video alignment and visual quality while minimizing computational costs.


\noindent\textbf{Impact of expert translation of latent-based VDM as super-resolution model.} We provide visual comparison between models with and without expert translation. As elaborated in Section \ref{section3.6}, ``with expert translation" refers to training the latent-based VDMs using timesteps 0-900 (with a maximum timestep of 1000), while ``w/o expert translation" involves standard training with timesteps 0-1000. As evident in Fig.~\ref{fig:compare_2}, the model with expert translation produces videos of superior visual quality, exhibiting fewer artifacts and capturing more intricate details.
\vspace{-3mm}

\section{Conclusion}



We introduce Show-1, an innovative model that marries the strengths of pixel 
 and latent based VDMS. Our approach employs pixel-based VDMs for initial video generation, ensuring precise text-video alignment and motion portrayal, and then uses latent-based VDMs for super-resolution, transitioning from a lower to a higher resolution efficiently. This combined strategy offers high-quality text-to-video outputs while optimizing computational costs.





\section{ Ethics Statement }
Our pretrained T2I model, Deep-IF, is trained using web data, and our models utilize WebVid-10M. Given this, there's a potential for our method to not only learn but also amplify societal biases, which could include inappropriate or NSFW content. To address this, we can integrate the CLIP model to detect NSFW content and filter out such instances.
\section{Reproducibility Statement}
We take the following steps to guarantee reproducibility: (1) Our codes, along with model weights, will be public available. (2) The training and hyperparameter details can be found in appendix Table~\ref{table:hyperparameters_ablation}.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\appendix
\section{Appendix}
\begin{table}[h]

\caption{Hyperparameters for our all models presented.}
\centering
\resizebox{!}{0.3\textwidth}{
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Hyperparameter} & \textbf{Keyframe Module} & \textbf{Interpolation Module} & \textbf{First Superresolution} & \textbf{Second Superresolution} \\
    \midrule\midrule
 
    Space & pixel & pixel & pixel & latent \\
   
     & 2 & 8 & 8 & 8 \\
  
    Channels & 320 & 320 & 128 & 320 \\
    Depth & 4 & 4 & 5 & 4 \\
    Channel multiplier & 1,2, 4,4 & 1,2,4,4 & 1,2,4,6,6 & 1,2,4,4\\

    Head channels & 64 & 64 & 64 & 64 \\
  
    \midrule\midrule
    \textit{Training}  &  &  &\\
    Parameterization &  &  &  & \\
    \# train steps  & 120K  &  40K & 40K & 120K\\
    Learning rate &  &  &  & \\
    Batch size per GPU & 1 & 2 & 1 & 1 \\
    \# GPUs  & 48 & 16 & 16 & 24 \\
    GPU-type  & A100-40GB & A100-40GB & A100-40GB & A100-40GB \\
      & 0.1  & 0.1  & 0.1  & 0.1 \\
    \midrule\midrule
   
  
    \textbf{Diffusion Setup}  &  &  & \\
    Diffusion steps & 1000 & 1000 & 1000 & 1000 \\
    Noise schedule & Linear & Linear & Linear & Linear \\
     &  &  & & 0.0015 \\
     & 0.02 & 0.02 & 0.02 & 0.0195\\
    \midrule\midrule
    \textbf{Sampling Parameters}  &  &  & \\
    Sampler  &  DPM++ & DPM++   & DPM++ & DDIM  \\
    Steps  & 75 & 50  & 125 & 40 \\
      & 1.0 & 1.0  & 1.0  & 1.0 \\
    \bottomrule
\end{tabular}
}
\label{table:hyperparameters_ablation}
\vspace{-0.4cm}
\end{table} \subsection{Training and Hyperparameter details.} We list details of our models in Table~\ref{table:hyperparameters_ablation}.






\end{document}
