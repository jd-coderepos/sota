\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{subcaption}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{float}


\newcommand{\ie}{\emph{i.e., }}
\newcommand{\eg}{\emph{e.g., }}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\st}{\emph{s.t. }}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\wrt}{\emph{w.r.t. }}
\newcommand{\cf}{\emph{cf. }}
\newcommand{\aka}{\emph{aka. }}

\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand{\Lapl}{\mathbf{\mathop{\mathcal{L}}}}
\newcommand{\lapl}{\mathcal{L}}
\newcommand{\Trans}[1]{{#1}^{\top}}
\newcommand{\Trace}[1]{tr\left({#1}\right)}
\newcommand{\Bracs}[1]{\left({#1}\right)}
\newcommand{\Mat}[1]{\textbf{#1}}
\newcommand{\MatS}[3]{\mathbf{#1}^{#2}_{#3}}
\newcommand{\Space}[1]{\mathbb{#1}}
\newcommand{\Set}[1]{\mathcal{#1}}
\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\BlockMat}[2]{\left[\begin{matrix}#1\\#2\end{matrix}\right]}
\newcommand{\BlockMatSquare}[4]{\left[\begin{matrix}#1 & #2\\#3 & #4\end{matrix}\right]}



\definecolor{bluehightlight}{HTML}{203ED7}
\newcommand{\blue}[1]{\textcolor{bluehightlight}{#1}}

\definecolor{bestcolor}{HTML}{FAB0A8}
\usepackage{enumitem}
\newcommand{\Vtr}[1]{\boldsymbol{#1}}



\newcommand{\yuan}[1]{\textcolor{blue}{#1}}
\newcommand{\wx}[1]{\textcolor{black}{#1}}
\newcommand{\feihao}[1]{\textcolor{black}{\{FH: #1\}}}

\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}





\title{MolCA: Molecular Graph-Language Modeling\\ with Cross-Modal Projector and Uni-Modal Adapter}





\author{Zhiyuan Liu$^\dag$ \quad Sihang Li$^\ddag$ \quad Yanchen Luo$^\ddag$  \quad Hao Fei$^\dag$ \\ \textbf{Yixin Cao$^\S$ \quad Kenji Kawaguchi$^\dag$ \quad Xiang Wang$^\ddag$\thanks{~~Corresponding author. Xiang Wang is also affiliated with Institute of Artificial Intelligence, Institute of Dataspace, Hefei Comprehensive National Science Center.} \quad Tat-Seng Chua$^\dag$} \\
$^\dag$National University of Singapore, $^\ddag$University of Science and Technology of China\\
$^\S$Singapore Management University\\
\texttt{\{acharkq,sihang0520,luoyc0830,caoyixin2011,xiangwang1223\}@gmail.com} \\ \texttt{haofei37@nus.edu.sg}, \texttt{\{kenji,chuats\}@comp.nus.edu.sg}\\}

\begin{document}
\maketitle








\begin{abstract}
    Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception — a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose \textbf{MolCA}: \underline{Mol}ecular Graph-Language Modeling with \underline{C}ross-Modal Projector and Uni-Modal \underline{A}dapter. MolCA enables an LM (\ie Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (\ie LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines. Our codes and checkpoints can be found at \url{https://github.com/acharkq/MolCA}.
\end{abstract} 
\section{Introduction}
Language Models (LMs) have demonstrated significant achievements across various domains~\citep{BERT, LLMSurvey}. Notably, the wealth of biochemical literature in LMs' pretraining data has enabled LMs to obtain a high-level understanding of biochemical concepts and molecule properties. This can be reflected by their promising performances in biochemical and medical question-answering benchmarks~\citep{Galactica,GPT4}. Therefore, it becomes increasingly urgent to incorporate these LMs to augment research in chemistry and biology.



For this purpose, we aim to utilize LMs for molecule understanding. As shown in Figure~\ref{fig:framework_compare}a, most existing LMs~\citep{LLama, OPT, KVPLM} represent molecules by their 1D Simplified Molecular Input Line Entry System (SMILES) strings~\citep{SMILES} and process them in a manner similar to texts. While convenient, treating molecules as strings overlooks the molecules' 2D graph representations, which are crucial to human professionals in comprehending the molecule structures~\citep{wells2012structural}. To combat that, recent works~\citep{MoMu, MoleculeSTM} represent molecules as graphs and use a Graph Neural Network \citep[GNN;][]{xu2019powerful} as the molecular graph encoder. The graph encoder is trained jointly with an LM through cross-modal contrastive learning~\citep{CLIP,DeCLIP}, as illustrated in Figure~\ref{fig:framework_compare}b. 
However, the application scope of cross-modal contrastive learning is limited~\cite{Flamingo}: it is suitable for retrieval tasks, but is insufficient for open-ended molecule-to-text generation tasks, such as molecule captioning~\citep{MolT5} and molecule's IUPAC name prediction~\citep{Galactica}. This is because molecule-to-text generation is a conditional generation task~\cite{keskar2019ctrl, T5}. It requires the LM to understand 2D graphs as the generation conditions, which contrastive learning cannot achieve. 
~\citet{MoMu} attempt to directly input 2D graphs' representations into LMs, however showing limited improvement.

\begin{figure}
\end{figure}

\begin{figure*}[t!]
 \centering \small
 \includegraphics[width=0.9\textwidth]{figures/framework_compare.pdf}
 \vspace{-2mm}
 \caption{Comparison of molecular language modeling methods.}
 \label{fig:framework_compare}
 \vspace{-4mm}
\end{figure*}


\begin{figure}[t]
 \centering\small
 \includegraphics[width=0.98\columnwidth]{figures/pipeline}
 \vspace{-2mm}
 \caption{MolCA's three-stage training pipeline.}
 \vspace{-5mm}
\label{fig:pipeline}
\end{figure}


To bridge this gap, we devise \textbf{MolCA}: \underline{Mol}ecular Graph-Language Modeling with \underline{C}ross-Modal Projector and Uni-Modal \underline{A}dapter. MolCA enables the LM to understand 2D graphs as inputs, therefore effectively conditioning the molecule-to-text generation process. To enable the LM to understand 2D graphs, we identify that the key challenge is \textbf{cross-modal alignment}~\citep{BLIP2, LinearMapping, Flamingo}: translating the representations of 2D graphs into 1D soft prompts~\citep{PrefixTuning} in the text space so that the LM can understand. This translation is facilitated by the cross-modal projector, bridging the gap between the graph encoder's representation space and the LM's input space, as illustrated in Figure~\ref{fig:framework_compare}. Specifically, we implement the cross-modal projector as a Q-Former~\cite{BLIP2} due to its effectiveness in vision-language tasks. With an effective cross-modal projector, we can harness the power of existing large LMs~\cite{Galactica, LLama} for molecule-to-text generation. However, given a large LM with billion scale parameters, its efficiency of downstream fine-tuning arises as a new problem. Therefore, we integrate the LM with a uni-modal adapter, \ie LoRA~\cite{LoRA}, to enable its efficient adaptation. 













As Figure~\ref{fig:pipeline} illustrates, MolCA uses a three-stage training pipeline to integrate its components. The two pretrain stages aim to develop the cross-modal alignment ability of the cross-modal projector. In pretrain stage 1, the projector and the encoder are trained to extract the molecule features that are the most relevant to the text. This stage endows the resulting model with powerful molecule-text retrieval ability. In pretrain stage 2, the cross-modal projector is connected to a frozen LM and trained for molecule captioning. This task forces the cross-modal projector to produce soft prompts that the LM can understand. In the final stage, MolCA is fine-tuned for downstream generation tasks. 







Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=*]
 \item We propose MolCA, a pioneering method for molecular language modeling. MolCA enables an LM to perceive 2D molecular graphs, thereby facilitating molecule-to-text generation tasks.
 \item MolCA sets new state-of-the-arts in a variety of benchmarks. It surpasses the baselines by 4.0 and 8.7 BLEU-2 for molecule captioning on CheBI-20~\cite{MolT5} and our curated PubChem324k dataset, respectively. Moreover, in predicting IUPAC names, MolCA shows a significant advantage of 10.0 BLEU-2 over the baselines. For molecule-text retrieval, MolCA outperforms the baselines by 20\% retrieval accuracy in PubChem324k and achieves the best performances in PCDes~\cite{KVPLM} and MoMu datasets~\cite{MoMu}. 
 \item We conduct ablation studies to show MolCA's effectiveness of incorporating 2D graphs into LMs for molecule-related tasks. Additionally, our quantitative analysis shows that incorporating 2D graphs helps improve the LM's ability to count functional groups inside molecules. 
\end{itemize}


\begin{figure*}[t]
 \centering
 \includegraphics[width=0.9\textwidth]{figures/stage1}
 \caption{MolCA's pretrain stage 1. The graph encoder and the cross-modal projector (\ie Q-Former) are jointly optimized using three cross-modal tasks. Modules of the same color share weights.}
 \label{fig:stage1}
 \vspace{-10pt}
\end{figure*}

 \vspace{-3mm}
\section{Model Architecture}

\vspace{-2mm}
Here we introduce three key components of MolCA's architecture: 1) a graph encoder for 2D structure understanding, 2) an LM for text generation, and 3) a cross-modal projector to connect the graph encoder and the LM. We describe the uni-modal adapter in Section~\ref{sec:fine-tune}.



\textbf{Graph Encoder.} Given the rich structural patterns in molecules, we leverage a GNN-based encoder to encode molecular graphs. Specifically, we employ a five-layer GINE~\citep{pretrain_gnn} that is pretrained on 2 million molecules from the ZINC15~\citep{ZINC15} dataset by contrastive learning~\citep{GraphCL}. Given a molecular graph $g$, the graph encoder $f$ can generate structure-aware features for every node of $g$: 
\begin{equation}
 f(g)=\Mat{Z}\in \mathbb{R}^{|g|\times d},
\end{equation}
where $|g|$ denotes the number of nodes in $g$.

\textbf{Language Model.} To achieve effective text generation performance, we employ Galactica~\citep{Galactica} as the base LM. Galactica is pretrained on a large collection of scientific literature, which encompasses fields like chemistry, biology, and medicine. Its promising performance in text-based science question-answering benchmarks~\citep{MMLU, PubMedQA} underscores its understanding of high-level biochemical concepts. Notably, Galactica can process 1D SMILES of molecules, which can potentially benefit our downstream tasks. Galactica is a decoder-only transformer LM based on the OPT~\citep{OPT} architecture. 





\textbf{Cross-Modal Projector.} 
We implement the cross-modal projector as a Querying-Transformer (Q-Former)~\citep{BLIP2} to map the graph encoder's outputs to the LM's input text space. 
As shown in Figure~\ref{fig:stage1}, Q-former has different procedures for processing 2D molecular graphs and 1D texts. Given text inputs, Q-Former inserts \texttt{[CLS]} tokens at the beginning and processes the texts by N layers of self-attention modules and feed-forward networks. The self-attention modules adopt causal masks~\citep{T5} when the pretraining task is text generation. On the other hand, given a molecular graph $g$, Q-Former works as a molecule feature extractor. Specifically, it maintains a set of learnable query tokens $\{\Vtr{q}_k\}_{k=1}^{N_q}$ as inputs. These query tokens can interact with the graph encoder's output $\Mat{Z}$ through the cross-attention modules~\citep{Transformer} and extract molecule features. The cross-attention modules are added every two layers. Additionally, the query tokens can interact with the text inputs through the same self-attention modules. Note that, the query tokens and text inputs are processed by different feed-forward networks, in order to maintain capacities for processing molecules and texts.


We initialize Q-Former from Sci-BERT~\citep{SciBERT}, an encoder-only transformer pretrained on scientific publications. 
Q-Former's cross-attention modules are randomly initialized.

\section{Training Pipeline}
This section delves into the details of MolCA's three-stage training pipeline (\cf Figure~\ref{fig:pipeline}). The two pretrain stages leverage a dataset of molecule-text pairs $\Set{D}=\{(g_1, \Vtr{y}_1), (g_2, \Vtr{y}_2), ...\}$ to train the cross-modal projector and the graph encoder. The goal of pretraining is to translate 2D molecular graphs into soft prompts that a frozen LM can understand. The fine-tune stage focuses on efficient adaptation to downstream generation tasks.

\subsection{Pretrain Stage 1: Learning to Extract Text Relevant Molecule Representations}
In this stage, we aim to optimize the cross-modal projector (\ie Q-Former) to extract the molecule features most relevant to the text input. This stage serves as a ``warmup'' training for the cross-modal projector before connecting to the LM. Inspired by BLIP2~\cite{BLIP2}, we simultaneously apply three cross-modal pretraining tasks that are tailored for Q-Former's architecture: molecule-text contrasting, molecule-text matching, and molecule captioning. These pretraining tasks endow the Q-Former with a strong molecule-text retrieval ability. Therefore, we save the resulting model from this stage for downstream retrieval tasks. We now elaborate on the three pretraining tasks. 



\textbf{Molecule-Text Contrasting (MTC).} We apply cross-modal contrastive learning~\citep{CLIP} to train the Q-Former to extract text-revelant molecule features. 
In this task, query tokens and text inputs are fed into the Q-Former separately (left of Figure~\ref{fig:stage1}) to obtain Q-Former's molecule representations and text representations.





Formally, let $\{(g_1, \Vtr{y}_1), ..., (g_B, \Vtr{y}_B)\}$ be a batch of molecule-text pairs. We denote $g_i$'s Q-Former representations as $\{\Vtr{m}_{ik}\}_{k=1}^{N_q}$ (each element for one query token), and denote $\Vtr{y}_i$'s Q-Former representation as $\Vtr{t}_{i}$ (representation of the \texttt{[CLS]} token).
For arbitrary $i,j\in [1,B]$, we measure the similarity between $\Vtr{t}_i$ and $\{\Vtr{m}_{jk}\}_{k=1}^{N_q}$ by computing the maximum similarity between $\Vtr{t}_{i}$ and every element in $\{\Vtr{m}_{jk}\}_{k=1}^{N_q}$. The MTC loss $\ell_{\text{MTC}}$ can be written as:
\begin{gather}
 \nonumber \ell_{\text{g2t}} = \sum_{i=1}^{B} \log \frac{\exp(\max_k \text{cos}(\Vtr{m}_{ik}, \Vtr{t}_{i})/\tau)}{\sum_{j=1}^B \exp(\max_k \text{cos}(\Vtr{m}_{ik}, \Vtr{t}_{j})/\tau)}, \\
 \nonumber \ell_{\text{t2g}} = \sum_{i=1}^{B} \log \frac{\exp(\max_k \text{cos}(\Vtr{t}_{i}, \Vtr{m}_{ik})/\tau)}{\sum_{j=1}^B \exp(\max_k \text{cos}(\Vtr{t}_{i}, \Vtr{m}_{jk})/\tau)}, \\
 \ell_{\text{MTC}} = - \frac{1}{B}\ell_{\text{g2t}} - \frac{1}{B}\ell_{\text{t2g}},
\end{gather}
where $\text{cos}(\cdot,\cdot)/\tau$ is the temperature-scaled cosine similarity. Temperature $\tau$ is empirically set to $0.1$.


\textbf{Molecule-Text Matching (MTM).} MTM is a binary classification task, aiming to predict whether a molecule-text pair is matched (positive) or unmatched (negative). As Figure~\ref{fig:stage1} illustrates, MTM allows the queries and the texts to interact through the same self-attention module. In this way, the queries can extract multi-modal information from both molecules and texts. For MTM prediction, we attach a linear classifier after the mean pooling of all queries' Q-Former representations. Let $\rho(g, \Vtr{y})$ denotes MTM's predicted probability that $(g, \Vtr{y})$ is matched. MTM loss $\ell_{\text{MTM}}$ can be written as:
\begin{align}
 \nonumber \ell_{\text{MTM}} & = \frac{1}{B} \mathbb{E}_{j, k\sim \text{U}(1,B)} \big[
 \sum_{i=1}^B -\log \rho(g_i, \Vtr{y}_i) + \\
 & \log \rho(g_i, \Vtr{y}_{j}) + \log \rho(g_{k}, \Vtr{y}_i)\big],
\end{align}
where $\text{U}(1,B)$ is a uniform distribution; $\Vtr{y}_{j}$ and $g_{k}$ are random negative samples in batch.

Similar to MTC, MTM also computes the similarity between molecule-text pairs. The difference is that MTM can capture more fine-grained similarity between a molecule and a text through the self-attention and cross-attention modules, compared to the simple cosine similarity used by MTC. Therefore, in retrieval experiments, we use MTC to first retrieve the top k samples and use MTM for re-ranking, thereby improving the performance.


\textbf{Molecule Captioning (MCap).} MCap aims to generate the molecule's text description based on the molecule representations. For this task, we adopt a special masking strategy in self-attention modules to ensure that the queries learn to extract molecule features that correspond to the text descriptions. Specifically, we employ the bi-directional self-attention masks for queries, allowing them to see each other but not the text tokens. Further, we apply causal masks for texts on the same self-attention module to perform autoregressive decoding of text descriptions. Each text token can see the queries and the preceding text, but not the subsequent text tokens. Since the text tokens cannot directly interact with the graph encoder, they must obtain molecule information from the queries, forcing the queries to extract molecule information through the cross-attention modules. Let $p_1(\Vtr{y}| g)$ be the probability of Q-Former generating text $\Vtr{y}$ for a graph $g$. We use the following loss function:
\begin{align}
 \ell_{\text{MCap}} & = -\frac{1}{B}\sum_{i=1}^{B}\log p_1(\Vtr{y}_i| g_i)
\end{align}




\begin{figure*}[t]
\centering
\begin{minipage}{.3\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/stage2}
\caption{MolCA's pretrain stage 2 by molecule captioning.}
\label{fig:stage2}
\end{minipage}
\hspace{5pt}
\begin{minipage}{.566\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/finetune}
\caption{MolCA's fine-tune stage for molecule-to-text generation. The example shows the prediction of a molecule's IUPAC name.}
\label{fig:finetune}
\end{minipage}
 \vspace{-4mm}
\end{figure*}

\subsection{Pretrain Stage 2: Aligning 2D Molecular Graphs to Texts via Language Modeling}
In this stage, we aim to align the cross-modal projector's outputs to the text space of a frozen LM. As Figure~\ref{fig:stage2} illustrates, we feed the cross-modal projector's representations of 2D molecular graphs to the frozen LM as inputs, and train the model to generate molecules' text descriptions. This process encourages the cross-modal projector to provide representations that the LM can understand, so as to prompt the text generation. 
Additionally, we also use a molecule's 1D SMILES to guide the generation (\cf Figure~\ref{fig:stage2}). This is because most LMs~\citep{Galactica, LLama, OPT} use SMILES during pretraining. Therefore, these LMs have established some correlations between SMILES and their text contexts. Thus, including SMILES can potentially prompt the corresponding biochemical knowledge. On the other hand, incorporating 2D graphs can help capture structural patterns that are hard to learn from 1D SMILES. We will show later in experiments that combining 2D graphs and 1D SMILES can boost performance.

Formally, consider a molecule-text pair $(g, \Vtr{y})$ and $g$'s SMILES repsentation $\Vtr{s}$, The cross-modal projector representations of $g$ are denoted as $\{\Vtr{m}_k\}^{N_q}_{k=1}$. We define $p_2(\cdot)$ as the text distribution parameterized by the frozen LM. We optimize the cross-modal projector and the graph encoder by minimizing the following loss function:
\begin{align}
\nonumber & -\log p_2(\Vtr{y}|\{\Vtr{m}_k\}^{N_q}_{k=1}, \Vtr{s}) \\
= & -\sum_{l=1}^{L} \log p_2(y_l|y_1, ..., y_{l-1}, \{\Vtr{m}_k\}^{N_q}_{k=1}, \Vtr{s}).
\end{align}


\subsection{Fine-tune Stage: Uni-Modal Adapter for Efficient Downstream Adaptation}
\label{sec:fine-tune}
In this stage, we fine-tune MolCA for downstream generation tasks. As Figure~\ref{fig:finetune} illustrates, we append a text prompt of the task description after the molecule representations. Then, we apply language modeling loss to fine-tune MolCA for generation tasks, such as molecule's IUPAC name prediction. 

\textbf{Uni-Modal Adapter.} In MolCA, the LM is accounted for a large portion of computation overhead: it can have $\sim$1B parameters, while the cross-modal projector and graph encoder only have a total of $\sim$0.1B parameters. 
Therefore, we employ a uni-modal adapter for the LM's efficient adaptation to downstream tasks. Specifically, we employ the LoRA~\citep{LoRA} adapter due to its simple implementation and promising performances~\citep{DBLP:conf/nips/LiuTMMHBR22}. As shown in Figure~\ref{fig:finetune}, for selected weight matrices (\eg $\Mat{W} \in \mathbb{R}^{d_1\times d_2}$) in the LM, LoRA adds pairs of rank decomposition matrices (\eg $\Mat{BA}, \Mat{B}\in \mathbb{R}^{d_1\times r}, \Mat{A}\in \mathbb{R}^{r\times d_2}$) in parallel to them. The original $\Vtr{h}=\Mat{W}\Vtr{x}$ layer is changed to:
\begin{gather}
 \Vtr{h}=\Mat{W}\Vtr{x} + \Mat{BA}\Vtr{x},
\end{gather}
where $\Mat{W}$ is kept frozen and the newly added $\Mat{BA}$ is trained during adaptation. Given a small $r\ll \text{min}(d_1, d_2)$, LoRA can effectively adapt the LM to downstream tasks while requiring little memory overhead for storing gradients. 



%
 \begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccc}\toprule
Subset   & Size & Avg mol len & Min text len & Avg text len \\\midrule
Pretrain & 309689               & 35                  & 1               & 18              \\
Train    & 12000                & 32                  & 20              & 60              \\
Valid    & 1000                 & 32                  & 20              & 61              \\
Test     & 2000                 & 31                  & 20              & 60 \\\bottomrule
\end{tabular}
\caption{Statistics of the PubChem324k dataset. We count the text length by splitting the text at spaces.}
\label{tab:statistics}
 \vspace{-4mm}
\end{table}

\begin{table*}[t]
    \centering
    \small
    \begin{subtable}[t]{\textwidth}
    \setlength{\tabcolsep}{4pt}
    \centering
    \small
    \begin{tabular}{llcccccc} \toprule
    Model                  & \#Trainable params       & BLEU-2               & BLEU-4               & ROUGE-1              & ROUGE-2              & ROUGE-L              & METEOR               \\\midrule
    \multicolumn{2}{l}{\textbf{1D SMILES}}            & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    MolT5-Small            & 80M, full ft                      & 14.8                 & 8.5                  & 26.5                 & 13.5                 & 23.6                 & 18.5                 \\
    MolT5-Base             & 250M, full ft                     & 30.1                 & 20.9                 & 40.3                & 25.1                 & 33.8                 & 35.6                 \\
    MolT5-Large            & 780M, full ft                     & 30.2                 & 22.2                 & 41.5                & 25.9                 & 34.8                 & 36.6                 \\\midrule
    \multicolumn{2}{l}{\textbf{1D SMILES + 2D Graph}} &                      &                      &                      &                      &                      &                      \\
    MoMu-Small             & 82M, full ft                      & 19.1                 & 12.0                 & 29.7                 & 16.3                 & 26.7                 & 21.8                 \\
    MoMu-Base              & 252M, full ft                     & 30.2                 & 21.5                 & 40.5                 & 25.1                 & 34.4                 & 34.2                 \\
    MoMu-Large             & 782M, full ft                     & 31.1                 & 22.8                 & 41.8                 & 25.7                 & 36.7                 & 36.2                 \\
    MolCA, MolT5-Large & \multicolumn{1}{l}{877M, full ft} & \underline{33.7} & \underline{27.0} & \underline{49.7} & \underline{35.6} & \underline{44.4} & \underline{42.4} \\
    MolCA, $\text{Galac}_{\text{125M}}$    & 222M, full ft                 & 32.4                 & 24.9                 & 44.9                 & 30.1                 & 39.5                 & 39.2                 \\
    MolCA, $\text{Galac}_{\text{1.3B}}$        & 100M, LoRA ft*                     & \textbf{39.8}        & \textbf{31.7}        & \textbf{51.7}        & \textbf{37.3}        & \textbf{46.2}        & \textbf{46.8}         \\\bottomrule
    \end{tabular}
    \caption{PubChem324k dataset. Baseline performances are reproduced using their source codes~\citep{MolT5, MoMu}.}
    \label{tab:pubchem32k_cap}
    \end{subtable}
    \begin{subtable}[t]{\textwidth}
    \small
    \centering
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{llcccccc}\toprule
    Model                  & \#Trainable params       & BLEU-2               & BLEU-4               & ROUGE-1              & ROUGE-2              & ROUGE-L              & METEOR               \\\midrule
    \multicolumn{2}{l}{\textbf{1D SMILES}}            & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    T5-Small               & 80M, full ft                      & 50.1                 & 41.5                 & 60.2                 & 44.6                 & 54.5                 & 53.2                 \\
    T5-Base                & 250M, full ft                     & 51.1                 & 42.3                 & 60.7                 & 45.1                 & 55.0                 & 53.9                 \\
    T5-Large               & 780M, full ft                     & 55.8                 & 46.7                 & 63.0                 & 47.8                 & 56.9                 & 58.6                 \\
    MolT5-Small            & 80M, full ft                      & 51.9                 & 43.6                 & 62.0                 & 46.9                 & 56.3                 & 55.1                 \\
    MolT5-Base             & 250M, full ft                     & 54.0                 & 45.7                 & 63.4                 & 48.5                 & 57.8                 & 56.9                 \\
    MolT5-Large            & 780M, full ft                     & 59.4                 & 50.8                 & 65.4                 & 51.0                 & 59.4                 & 61.4                 \\\midrule
    \multicolumn{2}{l}{\textbf{1D SMILES + 2D Graph}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    MoMu-Small             & 82M, full ft                      & 53.2                 & 44.5                 & -                    & -                    & 56.4                 & 55.7                 \\
    MoMu-Base              & 252M, full ft                     & 54.9                 & 46.2                 & -                    & -                    & 57.5                 & 57.6                 \\
    MoMu-Large             & 782M, full ft                     & 59.9                 & 51.5                 & -                    & -                    & 59.3                 & 59.7                 \\
    MolCA, $\text{Galac}_{\text{125M}}$ & 222M, full ft & \underline{61.6}          & \underline{52.9}          & \underline{67.4}          & \underline{53.3}          & \underline{61.5}          & \underline{63.9}          \\
    MolCA, $\text{Galac}_{\text{1.3B}}$              & 110M, LoRA ft*              & \textbf{63.9}        & \textbf{55.5}        & \textbf{69.7}        & \textbf{55.8}        & \textbf{63.6}        & \textbf{66.9}       \\\bottomrule
    \end{tabular} 
    \caption{CheBI-20 dataset. Baseline performances are borrowed from their original papers~\citep{MolT5, MoMu}.}
    \label{tab:chebi20_cap}
\end{subtable}
\caption{Performances (\%) of molecule captioning on the PubChem324k and CheBI-20 datasets. \textbf{Bold} indicates the best performance and \underline{underline} indicates the second best performance. Full ft denotes full parameter fine-tuning. *The LoRA configurations for PubChem324k and CheBI-20 datasets are different. Details are in Appendix~\ref{app:settings}.}
 \vspace{-4mm}
\label{tab:cap}
\end{table*}

\section{Experiments}
\subsection{Experimental Setting}
Here we briefly present the experimental settings. More details can be found in Appendix~\ref{app:settings}.

\textbf{PubChem324k Dataset.} We collect PubChem-324k -- a dataset containing 324k molecule-text pairs from the PubChem website\footnote{\url{https://pubchem.ncbi.nlm.nih.gov}}. Table~\ref{tab:statistics} presents the dataset statistics. Notice that, the dataset includes many uninformative texts, such as ``The molecule is a peptide''. Therefore, we sample a high-quality subset of 15k pairs with text longer than 19 words for downstream tasks. This high-quality subset is further randomly divided into the train/valid/test sets. The remaining dataset, which is more noisy, is used for pretraining.


\textbf{Baselines.} For generation tasks, we compare MolCA with the following baselines: T5~\cite{T5}, MolT5~\cite{MolT5}, and MoMu~\cite{MoMu}. For molecule-text retrieval, we also include these methods: MoleculeSTM~\cite{MoleculeSTM}, KV-PLM~\cite{KVPLM}, and Sci-BERT~\cite{SciBERT}. 




\subsection{Molecule Captioning}
We evaluate MolCA for molecule captioning on the datasets of PubChem324k and CheBI-20~\cite{MolT5}. Specifically, we implement MolCA with the base LMs of Galactica$_{\text{1.3B}}$, Galactica$_{\text{125M}}$, and MolT5-Large. We employ full parameter fine-tuning for Galactica$_{\text{125M}}$ and MolT5-Large due to their smaller scales. We fine-tune MolCA and baselines on the dataset's training set and report the test set performance selected by the valid set. Following~\cite{MolT5}, we adopt BLEU~\cite{BLEU}, ROUGE~\cite{ROUGE}, and METEOR~\cite{METEOR} as the evaluation metrics. As shown in Table~\ref{tab:cap}, we observe that:

1. MolCA consistently outperforms the baselines by a large margin. Specifcally, MolCA, $\text{Galac}_{\text{1.3B}}$ achieves the highest performance on all metrics. It outperforms the baselines by 8.7 BLEU-2 on PubChem324k and 4.0 BLEU-2 on CheBI-20. 

2. MolCA, $\text{Galac}_{\text{125M}}$ outperforms baselines of larger sizes across all metrics, showing that MolCA's advantage is not limited to model scale.


\begin{table*}[t]
\small
\centering
\begin{tabular}{llcccccc} \toprule
Model & \#Trainable params       & BLEU-2               & BLEU-4               & ROUGE-1              & ROUGE-2              & ROUGE-L              & METEOR               \\ \midrule
\multicolumn{2}{l}{\textbf{1D SMILES}}            & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
MolT5-Small            & 80M, full ft                      & 48.6                 & 35.2                 & 40.0                 & 16.1                 & 34.3                 & 42.5                 \\
MolT5-Base             & 250M, full ft                     & 52.7                 & 41.5                 & 50.7                 & 26.0                 & 44.3                 & 53.2                 \\
MolT5-Large            & 780M, full ft                     & 59.4                 & 49.7                 & 55.9                 & 33.3                 & 49.1                 & 58.5                 \\\midrule
\multicolumn{2}{l}{\textbf{1D SMILES + 2D Graph}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
MolCA, Galac$_{\text{125M}}$  & 222M, full ft              & \underline{72.9}                 & \underline{65.1}                 & \underline{69.5}                 & \underline{48.0}                 & \underline{62.6}                 & \underline{71.6}                 \\
MolCA, $\text{Galac}_{\text{1.3B}}$        & 100M, LoRA ft                     & \textbf{74.6}        & \textbf{66.1}        & \textbf{70.5}        & \textbf{49.1}        & \textbf{64.2}        & \textbf{73.0}  \\ \bottomrule
\end{tabular}
\caption{Performances (\%) of predicting molecule's IUPAC names on the PubChem324k dataset. Baseline performances are obtained by running their source codes~\citep{MolT5}.}
 \vspace{-4mm}
\label{tab:iupac}
\end{table*}




\subsection{IUPAC Name Prediction}
The International Union of Pure and Applied Chemistry (IUPAC) has established a standardized naming system for chemical compounds, known as IUPAC names~\citep{IUPACName}. Notably, this naming system relies on identifying specific molecule structures, including hydrocarbon chains and double/triple bonds. Therefore, correctly predicting IUPAC names indicates a model's proficiency to understand molecule structures. We fine-tune MolCA and baselines using the PubChem324k's training set to generate a molecule's IUPAC name. As shown in Table~\ref{tab:iupac}, MolCA consistently outperforms the baselines by a large margin of 10.0 BLEU-2, highlighting MolCA's advantage in comprehending molecule structures.


\begin{table}[t!]
    \small
    \centering
    \begin{subtable}[t]{0.45\textwidth}
    \small
    \centering
    \begin{tabular}{lcccc} \toprule
                        & \multicolumn{2}{c}{M2T}             & \multicolumn{2}{c}{T2M}                \\\cmidrule(lr){2-3} \cmidrule(lr){4-5}
    Model              & Acc               & R@20                 & Acc                  & R@20                 \\ \midrule
    \multicolumn{2}{l}{\textbf{1D SMILES}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    Sci-BERT           & 39.3              & 86.1                 & 37.9                 & 85.1                 \\
    KV-PLM             & 38.8              & 86.3                 & 38.7                 & 85.6                 \\\midrule
    \multicolumn{2}{l}{\textbf{2D Graph}}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    MoMu-S*            & 11.5              & 41.2                 & 12.6                 & 43.6                 \\
    MoMu-K*            & 11.3              & 41.0                 & 12.4                 & 39.9                 \\
    MoMu-S             & 40.6              & 86.5                 & 40.6                 & 86.5                 \\
    MoMu-K             & 41.8              & 88.9                 & 42.4                 & 88.5                 \\
    MoleculeSTM        & 47.1              & 89.0                 & 45.4                 & 91.5                 \\
    MolCA w/o MTM      & 60.5              & 93.7                 & 58.6                 & 92.3                 \\
    MolCA              & \textbf{69.4}     & \textbf{95.7}        & \textbf{69.6}        & \textbf{94.6}       \\\bottomrule
    \end{tabular}
    \caption{Performances (\%) in the PubChem324k dataset. }
    \label{tab:retrieve_pubchem}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
        \small
        \centering
        \begin{tabular}{lcccc} \toprule
                & \multicolumn{2}{c}{PCDes dataset}           & \multicolumn{2}{c}{MoMu dataset}            \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
        Model              & M2T                  & T2M                  & M2T                  & T2M                  \\\midrule
        \textbf{1D SMILES} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
        Sci-BERT$^\dag$           & 60.7                 & 60.8                 & 0.3                  & 0.3                  \\
        KV-PLM$^\dag$             & 75.9                 & 64.3                 & 0.5                  & 0.3                  \\\midrule
        \textbf{2D Graph}  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
        MoMu-S$^\dag$             & 79.1                 & 75.5                 & 43.3                 & 43.4                 \\
        MoMu-K$^\dag$             & 80.2                 & 79.0                 & 43.7                 & 43.5                 \\
        MoleculeSTM       & 84.6                 & 85.1                 & 75.8                 & 74.5                 \\
        MolCA w/o MTM      & 88.0                 & 85.5                 & 81.5                 & 81.6                 \\
        MolCA              & \textbf{90.5}        & \textbf{87.6}        & \textbf{88.6}        & \textbf{87.3}       \\\bottomrule
        \end{tabular}
        \caption{Recall@20 (\%) in the PCDes and MoMu datasets. }
        \label{tab:retrieval_twoothers}
    \end{subtable}
    \caption{Molecule-text retrieval performances. We report performances of using molecule to retrieve text (M2T) and using text to retrieve molecule (T2M). * denotes performance evaluated on the baseline's released checkpoint. \dag ~denotes result borrowed from~\citep{MoMu}. Other models are trained on PubChem324k's pretrain subset. The complete results are in Appendix~\ref{app:experiment}}
    \label{tab:retrieval}
 \vspace{-4mm}
    \end{table}


\begin{table*}[t]
\centering
\small
\begin{subtable}[t]{\textwidth}
\centering
\small
\begin{tabular}{lcccccc} \toprule
Representation type      & BLEU-2            & BLEU-4           & ROUGE-1              & ROUGE-2              & ROUGE-L              & METEOR               \\ \midrule
\multicolumn{3}{l}{\textbf{Molecule   Captioning, PubChem324k}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
1D SMILES                & 33.7              & 26.0             & 45.4                 & 31.6                 & 40.7                 & 40.3                 \\
2D Graph                 & 35.7              & 27.4             & 47.3                 & 32.3                 & 41.8                 & 42.0                 \\
1D SMILES + 2D Graph     & \textbf{39.8}     & \textbf{31.7}    & \textbf{51.7}        & \textbf{37.3}        & \textbf{46.2}        & \textbf{46.8}        \\ \midrule
\multicolumn{3}{l}{\textbf{Molecule   Captioning, CheBI-20}}    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
1D SMILES                & 58.3              & 49.4             & 65.9                 & 51.3                 & 59.7                 & 62.4                 \\
1D SMILES + 2D Graph     & \textbf{63.9}     & \textbf{55.5}    & \textbf{69.7}        & \textbf{55.8}        & \textbf{63.6}        & \textbf{66.9}        \\ \midrule
\multicolumn{4}{l}{\textbf{IUPAC   Name Prediction, PubChem324k}}                      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
1D SMILES                & 70.7              & 60.7             & 68.6                 & 46.2                 & 61.7                 & 71.5                 \\
1D SMILES + 2D Graph     & \textbf{74.6}     & \textbf{66.1}    & \textbf{70.5}        & \textbf{49.1}        & \textbf{64.2}        & \textbf{73.0}   \\ \bottomrule
\end{tabular}
\caption{Ablating the representation type on tasks of molecule captioning and IUPAC name prediction. }
\end{subtable}
\begin{subtable}[t]{\textwidth}
\centering
\small
\begin{tabular}{lcccccc|c} \toprule
Representation type  & Bace              & BBBP              & ClinTox           & ToxCast           & Sider             & Tox21             & Mean          \\\midrule
1D SMILES            & 79.3±0.8          & \textbf{70.8±0.6} & 89.0±1.7          & 56.2±0.7          & 61.1±1.2          & 76.0±0.5          & 72.1          \\
1D SMILES + 2D Graph & \textbf{79.8±0.5} & 70.0±0.5          & \textbf{89.5±0.7} & \textbf{64.5±0.8} & \textbf{63.0±1.7} & \textbf{77.2±0.5} & \textbf{74.0} \\\bottomrule
\end{tabular}
\caption{ROC-AUC (\%) scores on six molecule property prediction datasets from MoleculeNet~\cite{MoleculeNet}. We use scaffold split following ~\cite{pretrain_gnn}. We report the performance's mean values and standard deviations across three random seeds.}
\end{subtable}
\caption{Ablating molecule's representation types. All compared models fine-tune the base LM of $\text{Galactica}_{\text{1.3B}}$.}
\label{tab:ablation}
\end{table*}



\subsection{Molecule-Text Retrieval}
\label{sec:retrieval}
We evaluate MolCA for molecule-text retrieval on the datasets of PubChem324k, PCDes~\cite{KVPLM} and MoMu~\cite{MoMu}. Specifically, we evaluate MolCA's checkpoint from pretrain stage 1 without further fine-tuning. For all experiments, MolCA first retrieves the top $128$ candidates using MTC, then employs the MTM module for re-ranking. 
We select Accuracy (Acc) and Recall@20 (R@20) as the evaluation metrics, and report the performances of retrieval in the entire test set. As shown in Table~\ref{tab:retrieval}, we observe that:

1. MolCA demonstrates superior performance over baselines. Specifically, in PubChem324k, MolCA improves the accuracy by more than 20\% over the baselines. 
In PCDes and MoMu, MolCA also consistently outperforms the baselines, demonstrating its effectiveness for molecule-text retrieval.

2. Incorporating MTM significantly improves MolCA's performance. This can be attributed to MTM's ability to model long-range interactions between molecule features and texts, achieved by the cross-attention and self-attention modules. 

3. MolCA's good performances can be partially attributed to our larger pretrain dataset -- PubChem324k. As shown in Table~\ref{tab:retrieve_pubchem}, we compare the performances of MoMu's original checkpoint (pretrained on 15k molecule-text pairs) with our reproduced MoMu using PubChem324k. The latter improves the retrieval accuracy by over 25\%. 


\subsection{Ablation Study on Representation Types}
Here we ablate the two representations types of molecules: 1D SMILES and 2D graphs. We compare MolCA with its two variants: 1) 1D SMILES: an LM that uses only 1D SMILES for pretraining and fine-tuning. For a fair comparison, we pretrain this variant on PubChem324k's pretrain subset for molecule captioning before its downstream adaptation; 2) 2D Graph: this variant follows the original MolCA's training pipeline, except not using 1D SMILES in pretrain stage 2 and fine-tune stage. 

\textbf{End Task Ablation.} Table~\ref{tab:ablation} presents the results for molecule-to-text generation and molecule property prediction~\cite{pretrain_gnn} tasks. We can observe that combing 2D graphs and 1D SMILES leads to improved performance in all the compared tasks. This demonstrates MolCA's effectiveness in incorporating molecules' 2D graph representations.


\textbf{Counting Functional Groups (FGs).} We ablate MolCA's capability of counting 85 types of FGs inside molecules. An FG is a molecule's subgraph that exhibits consistent chemical behaviors across different molecules~\cite{Grover}. Correctly counting FGs can help understand a molecule's properties. As shown in Figure~\ref{fig:regression}, incorporating 2D graphs significantly improves MolCA's performance in counting FGs, thereby enhancing its ability in understanding molecule structures.




\begin{figure}[t]
\centering
\small
\begin{subfigure}[t]{0.235\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/fg_regression/regression_train}
\caption{Training set loss.}
\label{fig:regression_train}            
\end{subfigure}
\begin{subfigure}[t]{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/fg_regression/regression_val}
\caption{Valid set performance.}
\label{fig:regression_valid}            
\end{subfigure}
\caption{Ablating MolCA for counting FGs inside molecules from PubChem324k. We plot average values across three random seeds. Blue shades indicate the range of ± one standard deviation. 
Evaluation metric is Root Mean Square Error (RMSE). Lower value indicates better performance.}
 \vspace{-5mm}
\label{fig:regression}
\end{figure}



 \section{Related Works}

 \vspace{-2mm}
Here we briefly review the molecule-related literature. We discuss MolCA's relations to vision-language pretraining methods in Appendix~\ref{app:related_works}.


\textbf{Molecule Understanding via 1D Language Modeling.} Due to the extensive biochemical literature in their training corpus, some open-domain LMs~\cite{OPT,LLama,PALM} have obtained a high-level understanding of molecular and chemical concepts. This is demonstrated through their promising performances in text-related biochemical and medical question-answering benchmarks~\cite{MMLU, PubMedQA}. Among these LMs, Galactica~\cite{Galactica} shows competitive performances for using a corpus that is primarily composed of scientific literature. Focusing on the chemistry domain, KV-PLM~\citep{KVPLM} models molecules by applying masked language modeling loss on 1D SMILES. \citet{Smiles2actions} propose to predict the chemistry experiment actions by reading chemical reaction equations. MolT5~\citep{MolT5} presents several T5-based~\cite{T5} LMs for SMILES-to-text and text-to-SMILES translations. Further, \citet{TextChemT5} propose to fine-tune T5 for chemical reaction prediction and retrosynthesis tasks. MolCA is different from these methods that exclusively utilize 1D SMILES to represent molecules. Instead, MolCA aims to enable LMs to perceive molecules' 2D graph representations.


\textbf{Molecule-Text Contrastive Learning.} Driven by the demand of a molecule-text retrieval system, Text2Mol~\citep{Text2Mol} employs cross-modal contrastive learning to train a molecular graph encoder of GCNs~\cite{GCN} and a text encoder of Sci-BERT~\cite{SciBERT}. Subsequent works~\cite{MoMu, MoleculeSTM, CLAMP} have proposed enhancements, including the addition of inter-modal contrastive learning loss~\citep{MoMu} and applying the model for text-based molecule editing~\citep{MoleculeSTM}. However, cross-modal contrastive learning is unsuitable for open-ended conditional generation task~\cite{Flamingo}, because of its focus on learning a similarity function. 
To resolve the problem, we propose MolCA to enable the LM's understanding of 2D molecular graphs, facilitating MolCA's capability of open-ended molecule-to-text generation.




%
 \vspace{-3mm}
\section{Conclusion and Future Works}

\vspace{-3mm}
In this work, we propose MolCA, a novel molecular language modeling method. MolCA aims to enable LMs to perceive 2D graphs for molecule-to-text generation. For this purpose, MolCA features a cross-modal projector to map representations of 2D graphs into the text space of LMs. It also employs a uni-modal adapter for efficient downstream adaptation. MolCA achieves state-of-the-art performances on molecule captioning and molecule-text retrieval benchmarks. Looking forward, we are interested in exploring LMs for 3D molecular modeling and drug discovery tasks.



\section*{Limitations}
This work focuses on utilizing LMs' generation ability for molecule-text tasks. Other interesting abilities of LMs, like in-context learning and chain-of-thought reasoning, are beyond the scope of this research. We leave that to future exploration. 

While MolCA offers improvements over baselines, we observe that the current performance in molecule captioning is not yet sufficient for practical application. This can be attributed to the scale of pretraining data. To our knowledge, our PubChem324k dataset is the largest dataset of molecule-text pairs. However, compared to the $\sim$10M scale dataset~\citep{CC12M} for vision-language pretraining, our dataset, consists of 324k data points, is comparatively smaller and limits the model's performance. Remedy solutions may include mining weakly supervised data from biochemical literature.






\section*{Broader Impacts}
Our work has established new state-of-the-art performances in molecule captioning and molecule-text retrieval. It has broader impacts in two aspects: 1) for chemistry professionals, our method of molecule captioning and molecule-text retrieval could be useful tools, potentially speeding up their research process;
2) for individuals without specialized chemistry knowledge, our method could provide a more affordable way to access the basic chemical information of molecules. 

Our model shares the risks of most LMs. It can generate inaccurate information and can potentially be abused to produce biased content. Further, considering the limited scale of our training data, we strongly advise strictly testing our model before applying it in real applications.

\section*{Acknowledgement}
This research is supported by the National Natural Science Foundation of China (9227010114) and the University Synergy Innovation Program of Anhui Province (GXXT-2022-040). This material is based upon work supported by the Google Cloud Research Credit program with the award (6NW8-CF7K-3AG4-1WH1). This research is supported by NExT Research Center. 


 






\bibliography{reference}
\bibliographystyle{acl_natbib}


\clearpage
\newpage
\appendix


\section{Complete Related Works}
\label{app:related_works}
We present the complete literature review. In addition to the molecule-related literature, as addressed in the main body of the paper, we also discuss MolCA's relation to vision-language pretraining. 


\textbf{Molecule Understanding via 1D Language Modeling.} Due to the extensive biochemical literature in their training corpus, some open-domain LMs~\cite{OPT,LLama,PALM} have obtained a high-level understanding of molecular and chemical concepts. This is demonstrated through their promising performances in text-related biochemical and medical question-answering benchmarks~\cite{MMLU, PubMedQA}. Among these LMs, Galactica~\cite{Galactica} shows competitive performances for using a corpus that is primarily composed of scientific literature. Focusing on the chemistry domain, KV-PLM~\citep{KVPLM} models molecules by applying masked language modeling loss on 1D SMILES. \citet{Smiles2actions} propose to predict the chemistry experiment actions by reading chemical reaction equations. MolT5~\citep{MolT5} presents several T5-based~\cite{T5} LMs for SMILES-to-text and text-to-SMILES translations. Further, \citet{TextChemT5} propose to fine-tune T5 for chemical reaction prediction and retrosynthesis tasks. MolCA is different from these methods that exclusively utilize 1D SMILES to represent molecules. Instead, MolCA aims to enable LMs to perceive molecules' 2D graph representations.


\textbf{Molecule-Text Contrastive Learning.} Driven by the demand of a molecule-text retrieval system, Text2Mol~\citep{Text2Mol} employs cross-modal contrastive learning to train a molecular graph encoder of GCNs~\cite{GCN} and a text encoder of Sci-BERT~\cite{SciBERT}. Subsequent works~\cite{MoMu, MoleculeSTM, CLAMP} have proposed improvements, including the addition of inter-modal contrastive learning loss~\citep{MoMu} and applying the model for text-based molecule editing~\citep{MoleculeSTM}. However, cross-modal contrastive learning is unsuitable for open-ended conditional generation task~\cite{Flamingo}, because of its focus on learning a similarity function. 
To resolve the problem, we propose MolCA to enable the LM's understanding of 2D molecular graphs, facilitating MolCA's capability of open-ended molecule-to-text generation.

\textbf{Vision-Language Pretraining (VLP).} Both VLP and Molecular Language Modeling aim to bridge the gap between text and another modality. Notably, VLP methods of CLIP~\cite{CLIP} and others~\cite{DeCLIP, FILIP} use contrastive learning to connect a visual encoder and a text encoder. These methods can be applied for tasks like image-text retrieval and zero-shot image classification. Recently, a series of VLP works~\cite{Frozen,LinearMapping, BLIP2, Flamingo} show that visual features can be aligned to the text space of LMs. This cross-modal alignment allows LMs to utilize their language generation and few-shot learning abilities for multi-modal tasks. MolCA draws inspiration from these findings. To the best of our knowledge, we are the first to align 2D molecular graphs to the text space of LMs. Furthermore, we incorporate a uni-modal adapter to improve the adaptation efficiency on downstream tasks.

\begin{table*}[t]
\small
\centering
\begin{tabular}{lcccccc} \toprule
Subset & \#Mol-text pairs & Usage & Avg mol len & Avg text len & Min text len & Max text len \\\midrule
Pretrain & 309689 & Pretrain stage 1 \& 2 & 35 & 18 & 1 & 1305 \\
Train & 12000 & Downstream fine-tune & 32 & 60 & 20 & 937 \\
Valid & 1000 & Downstream validation & 32 & 61 & 20 & 1197 \\
Test & 2000 & Downstream test & 31 & 60 & 20 & 879 \\\bottomrule
\end{tabular}
\caption{Statistics of the PubChem324k dataset.}
\label{tab:PubChem324k_detail}
\end{table*}


\begin{table*}[t!]
    \small
    \centering
    \begin{subtable}[t]{\textwidth}
    \small
    \centering
    \begin{tabular}{lcccccccc}
    \toprule
    & \multicolumn{4}{c}{Retrieval in batch} & \multicolumn{4}{c}{Retrieval in test set} \\ 
    & \multicolumn{2}{c}{M2T (\%)} & \multicolumn{2}{c}{T2M (\%)} & \multicolumn{2}{c}{M2T (\%)} & \multicolumn{2}{c}{T2M (\%)} \\\cmidrule(lr){2-3}\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    Model & Acc & R@20 & Acc & R@20 & Acc & R@20 & Acc & R@20 \\\midrule
    \multicolumn{2}{l}{\textbf{1D SMILES}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    Sci-BERT & 84.1 & 98.6 & 82.5 & 98.2 & 39.3 & 86.1 & 37.9 & 85.1 \\
    KV-PLM & 84.3 & 98.3 & 82.3 & 98.3 & 38.8 & 86.3 & 38.7 & 85.6 \\\midrule
    \multicolumn{2}{l}{\textbf{2D Graph}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    MoMu-S* & 42.3 & 90.1 & 43.7 & 90.1 & 11.5 & 41.2 & 12.6 & 43.6 \\
    MoMu-K* & 43.3 & 90.4 & 45.8 & 89.0 & 11.3 & 41.0 & 12.4 & 39.9 \\
    MoMu-S & 82.9 & 99.0 & 83.0 & 99.0 & 40.6 & 86.5 & 40.6 & 86.5 \\
    MoMu-K & 84.1 & 98.7 & 83.6 & 98.9 & 41.8 & 88.9 & 42.4 & 88.5 \\
    MoleculeSTM & 87.2 & 98.5 & 86.7 & 98.3 & 47.1 & 89.0 & 45.4 & 91.5 \\
    MolCA w/o MTM & 86.6 & 98.9 & 85.3 & 98.7 & 60.5 & 93.7 & 58.6 & 92.3 \\
    MolCA & \textbf{91.4} & \textbf{99.9} & \textbf{90.1} & \textbf{99.2} & \textbf{69.4} & \textbf{95.7} & \textbf{69.6} & \textbf{94.6} \\\bottomrule
    \end{tabular}
    \caption{Molcule-text retrieval performances in the PubChem324k dataset.}
   \end{subtable}
   \centering
   \small
   \begin{subtable}[t]{\textwidth}
   \centering
   \small
   \begin{tabular}{lcccccccc}\toprule
    & \multicolumn{4}{c}{Retrieval in batch} & \multicolumn{4}{c}{Retrieval in test set} \\
    & \multicolumn{2}{c}{M2T (\%)} & \multicolumn{2}{c}{T2M (\%)} & \multicolumn{2}{c}{M2T (\%)} & \multicolumn{2}{c}{T2M (\%)} \\\cmidrule(lr){2-5} \cmidrule(lr){6-9}
   Model & Acc & \multicolumn{1}{c}{R@20} & Acc & \multicolumn{1}{c}{R@20} & \multicolumn{1}{c}{Acc} & R@20 & \multicolumn{1}{c}{Acc} & R@20 \\\midrule
   \multicolumn{2}{l}{\textbf{1D SMILES}} & & \multicolumn{1}{l}{} & & & \multicolumn{1}{l}{} & & \multicolumn{1}{l}{} \\
   Sci-BERT$^\dag$ & 62.6 & - & 61.8 & - & - & 60.7 & - & 60.8 \\
   KV-PLM$^\dag$ & 77.9 & - & 65.0 & - & - & 75.9 & - & 64.3 \\\midrule
   \multicolumn{2}{l}{\textbf{2D Graph}} & & \multicolumn{1}{l}{} & & & \multicolumn{1}{l}{} & & \multicolumn{1}{l}{} \\
   MoMu-S$^\dag$ & 80.6 & - & 77.0 & - & - & 79.1 & - & 75.5 \\
   MoMu-K$^\dag$ & 81.1 & - & 80.2 & - & - & 80.2 & - & 79.0 \\
   MoleculeSTM & 86.2 & - & 83.9 & - & - & 84.6 & - & 85.1 \\
   MolCA w/o MTM & 88.4 & \multicolumn{1}{c}{98.8} & 85.5 & \multicolumn{1}{c}{98.5} & \multicolumn{1}{c}{54.6} & 88.0 & \multicolumn{1}{c}{51.8} & 85.5 \\
   MolCA & \textbf{91.4} & \multicolumn{1}{c}{\textbf{99.8}} & \textbf{88.4} & \multicolumn{1}{c}{\textbf{98.8}} & \multicolumn{1}{c}{\textbf{60.3}} & \textbf{90.5} & \multicolumn{1}{c}{\textbf{59.7}} & \textbf{87.6} \\\bottomrule
   \end{tabular}
   \caption{Molecule-text retrieval performances in the PCDes dataset.}
   \end{subtable}
   \begin{subtable}[t]{\textwidth}
   \small
   \centering
   \begin{tabular}{lcccccccc} \toprule
    & \multicolumn{4}{c}{Retrieval in batch} & \multicolumn{4}{c}{Retrieval in test set} \\
    & \multicolumn{2}{c}{M2T (\%)} & \multicolumn{2}{c}{T2M (\%)} & \multicolumn{2}{c}{M2T (\%)} & \multicolumn{2}{c}{T2M (\%)} \\\cmidrule(lr){2-5} \cmidrule(lr){6-9}
   Model & Acc & \multicolumn{1}{c}{R@20} & Acc & \multicolumn{1}{c}{R@20} & \multicolumn{1}{c}{Acc} & R@20 & \multicolumn{1}{c}{Acc} & R@20 \\\midrule
   \multicolumn{2}{l}{\textbf{1D SMILES}} & & \multicolumn{1}{l}{} & & & \multicolumn{1}{l}{} & & \multicolumn{1}{l}{} \\
   Sci-BERT$^\dag$ & 1.4 & - & 1.6 & - & - & 0.3 & - & 0.3 \\
   KV-PLM$^\dag$ & 1.5 & - & 1.3 & - & - & 0.5 & - & 0.3 \\\midrule
   \multicolumn{2}{l}{\textbf{2D Graph}} & & \multicolumn{1}{l}{} & & & \multicolumn{1}{l}{} & & \multicolumn{1}{l}{} \\
   MoMu-S$^\dag$ & 45.7 & - & 40.0 & - & - & 43.3 & - & 43.4 \\
   MoMu-K$^\dag$ & 46.2 & - & 38.5 & - & - & 43.7 & - & 43.5 \\
   MoleculeSTM & 81.8 & - & 81.9 & - & - & 75.8 & - & 74.5 \\
   MolCA w/o MTM & 77.3 & \multicolumn{1}{c}{97.7} & 77.3 & \multicolumn{1}{c}{97.5} & \multicolumn{1}{c}{38.1} & 81.5 & \multicolumn{1}{c}{37.3} & 81.6 \\
   MolCA & \textbf{83.7} & \multicolumn{1}{c}{\textbf{98.9}} & \textbf{84.3} & \multicolumn{1}{c}{\textbf{98.7}} & \multicolumn{1}{c}{\textbf{48.4}} & \textbf{88.6} & \multicolumn{1}{c}{\textbf{48.3}} & \textbf{87.3} \\\bottomrule
   \end{tabular}
   \caption{Molecule-text retrieval performances in the MoMu dataset.}
   \end{subtable}
   \caption{Complete molecule-text retrieval performances on the datasets of PubChem324k, PCDes and MoMu. * denotes performance evaluated on the baseline's released checkpoint. $\dag$ ~denotes result borrowed from~\cite{MoMu}. Other models are trained on PubChem324k's pretrain subset.}
   \label{tab:full_retrieval}
   \end{table*}

   \begin{table*}[t]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lcccccccc} \toprule
    Model & Pretrain stage 1 & Pretrain stage 2 & BLEU-2 & BLEU-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\\midrule
    MolCA, Galac$_{\text{1.3B}}$ & $\text{\xmark}$ & $\text{\xmark}$ & 35.8 & 27.6 & 47.4 & 33.0 & 42.1 & 42.2 \\
    MolCA, Galac$_{\text{1.3B}}$ & $\text{\cmark}$ & $\text{\xmark}$ & 37.7 & 29.5 & 49.2 & 34.9 & 43.8 & 44.5 \\
    MolCA, Galac$_{\text{1.3B}}$ & $\text{\cmark}$ & \textbf{$\text{\cmark}$} & \textbf{39.8} & \textbf{31.7} & \textbf{51.7} & \textbf{37.3} & \textbf{46.2} & \textbf{46.8} \\ \bottomrule
    \end{tabular}
    \caption{Ablating MolCA's two pretrain stages by the task of molecule captioning in the PubChem324k dataset.}
    \label{tab:ablate_pretrain}
   \end{table*}

   \begin{table*}[t]
    \centering
    \small
    \setlength{\tabcolsep}{4pt}
    \begin{tabular}{llcccccc} \toprule
    Cross-Modal Projector & Representation Type  & BLEU-2        & BLEU-4        & ROUGE-1       & ROUGE-2       & ROUGE-L       & METEOR        \\ \midrule
    -                     & 1D SMILES            & 33.7          & 26.0          & 45.4          & 31.6          & 40.7          & 40.3          \\
    Linear                & 1D SMILES + 2D Graph & 34.6          & 27.8          & 46.7          & 32.9          & 41.9          & 41.2          \\
    Q-Former              & 1D SMILES + 2D Graph & \textbf{39.8} & \textbf{31.7} & \textbf{51.7} & \textbf{37.3} & \textbf{46.2} & \textbf{46.8} \\ \bottomrule
    \end{tabular}
    \caption{Comparing different cross-modal projectors for molecule captioning on the PubChem324k dataset. All the compared methods apply LoRA fine-tuning on Galactica$_{\text{1.3B}}$.}
    \label{tab:cross_project}
    \end{table*}


   \begin{figure*}[t!]
    \centering
    \small
    \begin{subfigure}[t]{0.63\textwidth}
    \centering
    \small
    \includegraphics[width=\textwidth]{figures/case_caption}
    \caption{Samples of molecule captioning.}
    \label{fig:case_caption}            
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \small
    \includegraphics[width=\textwidth]{figures/case_iupac.pdf}
    \caption{Samples of IUPAC name prediction.}
    \label{fig:case_iupac}            
    \end{subfigure}
    \caption{Examples of MolCA's molecule-to-text generation results. We highlight text snippets in \blue{blue} that correctly describe the molecule structures in the predicted texts. To save space, some parts of texts are replaced by (...).}
    \label{fig:sample}
\end{figure*}

\section{Experimental Settings}
\label{app:settings}
\textbf{Pretrain Settings.} MolCA's pretrain stage 1 has 50 epochs and pretrain stage 2 has 10 epochs. Q-Former has $8$ query tokens ($N_q=8$). Our optimizer's configuration follows~\cite{BLIP2}. We use the AdamW optimizer~\citep{AdamW} with a weight-decay of $0.05$. The learning rate is scheduled by a combination of linear warmup and cosine decay. The peak learning rate is 1e-4 and the warmup has 1000 steps. 

\textbf{Molecule Captioning.} MolCA is fine-tuned for 100 epochs using the same configuration of optimizer and learning rate scheduler. LoRA is implemented using the OpenDelta library~\cite{OpenDelta} and the PEFT library~\citep{peft}. For the PubChem324k dataset, we set LoRA's rank $r$ to $8$ and apply LoRA to Galactica's modules of \texttt{[q\_proj, v\_proj]}. This configuration yields a LoRA adapter with 2M parameters, which constitutes 0.12\% of the parameters in the Galactica$_{\text{1.3B}}$. For the CheBI-20 dataset, we set LoRA's rank $r$ to $16$ and apply LoRA to Galactica's modules of \texttt{[q\_proj, v\_proj, out\_proj, fc1, fc2]}. This configuration yields a LoRA adapter with 12M parameters, which constitutes 0.94\% of the parameters in the Galactica$_{\text{1.3B}}$.


\textbf{IUPAC Name Prediction.} We collect IUPAC names for molecules in the train/valid/test sets of PubChem324k using the PubChemPy library\footnote{\url{https://github.com/mcs07/PubChemPy}}. The experiment uses the same hyperparameters as the molecule captioning experiment. We append a text prompt ``The molecule's IUPAC name is'' after the molecule representations as the task description (\cf Figure~\ref{fig:finetune}). 

\textbf{Molecule-Text Retrieval.} We use MolCA's checkpoint from pretrain stage 1 for retrieval without fine-tuning on any other datasets. This is similar to the setting of zero-shot retrieval in~\cite{MoMu, MoleculeSTM}.


\textbf{Molecule Property Prediction.} Following~\cite{pretrain_gnn}, we fine-tune the models for 100 epochs and report the test performance selected by the valid set. For molecule classification, we attach a linear classifier after the mean pooling of the LM's hidden states of the last layer. We use the AdamW optimizer with a constant learning rate of 1e-4 and weight decay of $0.05$. This experiment uses the same LoRA configuration as the molecule captioning experiment in the PubChem324k dataset.

\textbf{Counting Functional Groups (FGs).} We use the molecules in PubChem324k's train set for fine-tuning and use the molecules in the valid set for evaluation.
Following~\cite{Grover}, we use RDkit~\cite{rdkit} to obtain the ground truth counts of FGs in every molecule. For each FG type, we employ a separate linear classifier to regress its numbers. Our model is trained using the Mean Square Error (MSE) loss function. Other settings, including optimizer and LoRA, are the same as the Molecule Property Prediction experiment.

\textbf{Galactica.} Following the instructions in~\cite{Galactica}, we wrap SMILES sequences with special tokens of \texttt{[START\_I\_SMILES]} and \texttt{[END\_I\_SMILES]} before feeding them into Galactica. 

\textbf{PubChem324k Dataset.} Our dataset collection process follows the procedures described in~\cite{MoleculeSTM}. The resulting dataset is larger due to the frequent updates made to the PubChem database~\cite{PubChem}. For each molecule in this website, we use the ``description'' field in its webpage as the corresponding text description. To avoid information leakage, we replace any common name or IUPAC name of the molecule at the beginning of texts with a text template (\ie ``The molecule''). Detailed statistics of PubChem324k are presented in Table~\ref{tab:PubChem324k_detail}. 












\begin{table*}[t!]
\centering
\small
\begin{tabular}{lp{12cm}}\toprule
\textbf{Sample 1} & SMILES: C([C@@H]1[C@H]([C@@H]([C@H](C(O1)O)NS(=O)(=O)O)O)O[C@H]2 [C@@H]([C@H](C(=C(O2)C(=O)O)O)O)O)OS(=O)(=O)O\\\cmidrule(lr){2-2}
\textbf{Ground truth} & The molecule is an amino disaccharide consisting of alpha-(...) joined in sequence by a (1->4) glycosidic bond. It is a disaccharide derivative, an oligosaccharide sulfate, a member of sulfamic acids, a monocarboxylic acid (...) \\\cmidrule(lr){2-2}
\textbf{1D SMILES} & The molecule is a \blue{disaccharide} sulfate consisting of 2-acetamido-(...) \blue{joined in sequence by a (1->4) glycosidic bond}. It is functionally related to a N-acetyl-D-glucosamine and a N-acetyl-D-galactosamine. \\\cmidrule(lr){2-2}
\textbf{1D SMILES + 2D Graph} & The molecule is a \blue{disaccharide} that consists of 2-O-(...) residues \blue{joined in sequence by a (1->4) glycosidic bond}. It is a disaccharide, \blue{an amino disaccharide}, and \blue{a member of sulfamic acids}. \\
\midrule
\textbf{Sample 2} & SMILES: CCCCCCCCCCCCCCCCCCCCC(C(=O)O)O \\\cmidrule(lr){2-2}
\textbf{Ground truth} & The molecule is a long-chain fatty acid that is behenic acid substituted at position 2 by a hydroxy group. It is a 2-hydroxy fatty acid. It is functionally related to a docosanoic acid. It is a conjugate acid of a 2-hydroxybehenate. \\\cmidrule(lr){2-2}
\textbf{1D SMILES} & The molecule is a \blue{2-hydroxy fatty acid} that is the 2-hydroxy derivative of tetracosanoic acid. It is functionally related to a tetracosanoic acid. It is a \blue{conjugate acid} of a 2-hydroxytetracosanoate. \\\cmidrule(lr){2-2}
\textbf{1D SMILES + 2D Graph} & The molecule is a \blue{2-hydroxy fatty acid} that is hexacosanoic acid \blue{substituted at position 2 by a hydroxy group}. It is a \blue{long-chain fatty acid}. It is functionally related to an hexacosanoic acid. It is a \blue{conjugate acid} of a 2-hydroxyhexacosanoate. \\
\bottomrule
\end{tabular}
\caption{Molecule captioning samples of MolCA (\ie 1D SMILES + 2D Graph) and its variant of using only 1D SMILES. We highlight text snippets in \blue{blue} that correctly describe the molecule structures in the predicted texts. To save space, some parts of texts are replaced by (...).}
\label{tab:case}
\end{table*}




\section{More Experimental Results}
\label{app:experiment}

\textbf{Molecule-Text Retrieval}. Here we present MolCA's complete molecule-text retrieval performance on the PubChem324k, PCDes, and MoMu datasets. Following~\cite{MoMu}, we report the performance of retrieval in a batch of 64 random samples and the performance of retrieval in the entire test set. As shown in Table~\ref{tab:full_retrieval}, our conclusions align with those from Section~\ref{sec:retrieval}: 1) MolCA consistently outperforms the baselines for molecule-text retrieval; 2) applying the MTM module for re-ranking is crucial for MolCA's molecule-text retrieval performances.


\textbf{Ablating the Pretrain Stages.} We conduct ablation studies on MolCA's two pretrain stages. As shown in Table~\ref{tab:ablate_pretrain}, both the two pretrain stages have significant contributions to MolCA's molecule captioning performances.


\textbf{Ablating the Cross-Modal Projector.} We compare the performances of our selected cross-modal projector Q-Former and a linear cross-modal projector. For the linear cross-modal projector, we feed the node representations from the graph encoder to the base LM after the linear projector layer. We tune the weights of the graph encoder, linear projector, and the base LM's LoRA adapter. The experimental setting and hyperparameters are the same as those of MolCA. Table~\ref{tab:cross_project} shows the results. We can observe that: 1) Linear cross-modal projector underperforms Q-Former. We conjecture that a linear layer is suboptimal to bridge the modality gap between 2D molecules and 1D texts. This aligns with findings in the MME benchmark~\cite{MME}, where Q-Former-based methods (\eg BLIP-2, InstructBLIP~\cite{instructblip}, MiniGPT-4~\cite{minigpt4}) outperform linear cross-modal projector based method (\eg LLaVA~\cite{llava}). 2) Linear cross-modal projector slightly outperforms the SMILES-only baseline. We attribute this improvement to the usage of 2D molecular graphs, but the gains are limited because the linear projector is less effective.


\textbf{MolCA's Generation Results.} Figure~\ref{fig:sample} shows MolCA's molecule-to-text generation results. The two samples of molecule captioning is also presented in Table~\ref{tab:case}. Specifically, we compare MolCA (\ie 1D SMILES + 2D Graph) and its variant that is pretrained and fine-tuned using only 1D SMILES. We can observe that using both 1D SMILES and 2D graph leads to more accurate descriptions of molecule structures. 




\textbf{Computational Cost.} We present the real-world training time of MolCA's three training stages in Table~\ref{tab:comp_cost}. All experiments are conducted on two NVIDIA A100 40 GB GPUs. Notably, we observe that the fine-tuning stage is affordable in terms of computational resources. 



\begin{table*}[th!]
\centering
\small
\begin{tabular}{cllcc}\toprule
\multicolumn{1}{l}{Stage} & Base LM                        & Dataset                     & \multicolumn{1}{l}{Epochs} & Time  \\ \midrule
Pretrain stage 1          & -                              & PubChem324k pretrain subset & 50                         & 18.0h \\
Pretrain stage 2          & Galac$_{\text{1.3B}}$, freeze  & PubChem324k pretrain subset & 10                         & 9.0h  \\
Pretrain stage 2          & Galac$_{\text{125M}}$, freeze  & PubChem324k pretrain subset & 10                         & 3.0h  \\
Fine-tune stage           & Galac$_{\text{1.3B}}$, LoRA ft & PubChem324k train subset    & 100                        & 6.0h  \\
Fine-tune stage           & Galac$_{\text{125M}}$, full ft & PubChem324k train subset    & 100                        & 1.5h  \\\bottomrule
\end{tabular}
\caption{Compuational cost for MolCA's three stages.}
\label{tab:comp_cost}
\end{table*} 



\end{document}
