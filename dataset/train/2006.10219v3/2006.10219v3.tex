\section{Experiments}
\label{experiments}
We performed experiments on sub-sampling RGB and grayscale real images for classification,
depth real images for regression and RGB synthetic-generated for classification tasks. We describe them in details below.
\subsection{Classification}
\noindent \textbf{Datasets and Experimental Settings.} 
We evaluated the proposed AL methods on four challenging image classification 
benchmarks. These include three RGB image datasets, CIFAR-10\cite{cifar}, CIFAR-100\cite{cifar} and SVHN\cite{Goodfellow2014Multi-digitNetworks},
and a grayscale dataset, FashionMNIST\cite{Xiao2017Fashion-MNIST:Algorithms}.
Initially, for every benchmark, we consider the entire training set as an unlabelled pool ($\mathbf{D}_U$). As a cold-start, we randomly sample a small subset and query their labels, $\mathbf{D}_L$. 
For CIFAR-10, SVHN and FashionMNIST, the size of the seed labelled examples is 1,000. Whereas, for CIFAR-100 we select 2,000
due to their comparatively more number of classes (100 vs 10). 
We conduct our experiments for 10 cycles. At every stage, the budget is fixed at
1,000 images for the 10-class benchmarks and at 2,000
for CIFAR-100 which is a 100-class benchmark. 
Similar to the existing works of \cite{BeluchBcai2018TheClassification,Yoo2019LearningLearning},
we apply our selection on randomly selected subsets $\mathbf{D}_S \hspace{-1pt}\subset\hspace{-1pt} \mathbf{D}_U$ 
of unlabelled images. This avoids the redundant occurrences which are common in all datasets~\cite{data_redundant}. The size of $\mathbf{D}_S$ is set to 10,000 for all the experiments.
\\
\noindent \textbf{Implementation details.} 
ResNet-18 \cite{he2016deep}  is the favourite choice as learner due to its relatively  
higher accuracy and better training stability.
During training the learner, we set a batch size of 128. We use Stochastic Gradient Descent (SGD) 
with a weight decay $5 \times 10^{-4}$ and a momentum of $0.9$. At every selection stage, we train the model for
200 epochs. We set the initial learning rate of 0.1 and decrease it by the factor of 10 after 160 epochs. 
We use the same set of hyper-parameters in all the experiments. 
For the sampler, GCN has 2 layers and we set the dropout rate to 0.3 to avoid over-smoothing~\cite{zhao2019pairnorm}.
The dimension of initial representations of a node is 1024 and it is projected to 512.
The objective function is binary cross-entropy per node. 
We set the value of $\lambda = 1.2$ to give more importance to the larger unlabelled subset. 
We choose Adam \cite{Kingma2015ADAM:Optimization} optimizer with a weight decay of $5 \times 10^{-4}$ and 
a learning rate of $10^{-3}$. We initialise the nodes of the graph with the features of the images extracted from the learner. 
We set the value of $s_{margin}$ to 0.1. For the empirical comparisons, we suggest readers to refer Supplementary Material.
\\
\noindent \textbf{Compared Methods and Evaluation Metric:} We compare our method with
a wide range of baselines which we describe here. 
Random sampling is by default the most common sampling technique. 
CoreSet\cite{Sener2017ActiveApproach} on learner feature space is one of the
best performing geometric techniques to date and it is another competitive baseline for us. 
VAAL~\cite{Sinha2019VariationalLearning} and Learning Loss~\cite{Yoo2019LearningLearning} 
are two state-of-the-art baselines from task-agnostic frameworks.
Finally, we also compare with FeatProp~\cite{Wu2019ActivePropagation} which 
is a representative baseline for the GCN-based frameworks. This method is designed for cases 
where a static fixed graph is available. To approximate their performance, we construct 
a graph from the features extracted from learner and similarities between the features
as edges. We then compute the k-Medoids distance on this graph.
For quantitative evaluation, we report the mean average accuracy of 
5 trials on the test sets.

\begin{figure}
    \centering
    \includegraphics[trim=2.5cm 0cm 3.5cm 1.0cm, clip, width=0.80\linewidth]{images/cifar10f2.pdf}
    \includegraphics[trim=2.5cm 0cm 3.5cm 1.0cm, clip, width=0.80\linewidth]{images/cifar100f.pdf}
    \caption{Quantitative comparison on CIFAR-10(top) and CIFAR-100(bottom) (Zoom in the view)}
    \label{fig:cf10}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[trim=2.5cm 0cm 3.5cm 1cm, clip, width=0.80\linewidth]{images/fmnistf.pdf}
    \includegraphics[trim=2.5cm 0cm 3.5cm 1cm, clip,  width=0.80\linewidth]{images/svhnf.pdf}
    \caption{Quantitative comparison on FashionMNIST(top) and SVHN(bottom) (Zoom in the view)}
    \label{fig:fm}
\end{figure}
\noindent \textbf{Quantitative Comparisons.} We train the ResNet-18 learner with all the available training examples on every dataset separately and report the performance on the test set. Our 
implementation obtains 93.09\% on CIFAR-10, 73.02\% on CIFAR-100, 93.74\% on FashionMNIST, 
and  95.35\% on SVHN. This is comparable as reported on the official implementation~\cite{he2016deep}.
These results are also set as the upper-bound performance of the active learning frameworks.


Figure \ref{fig:cf10} (left) shows the performance comparison of UncertainGCN and CoreGCN 
with the other five existing methods on \textbf{CIFAR-10}. The solid line of the representation 
is the mean accuracy and the faded colour shows the
standard deviation. Both our sampling techniques surpass almost every other compared methods in every selection stage.
CoreSet is the closest competitor for our methods. After selecting 10,000 labelled examples, 
the CoreGCN achieves 90.7\% which is the highest performance amongst reported in the literature~\cite{Yoo2019LearningLearning,Sinha2019VariationalLearning}. 
Likewise, Figure \ref{fig:cf10}~(right) shows the accuracy comparison on \textbf{CIFAR-100}. 
We observe almost similar trends as on CIFAR-10.  
With only 40\% of the training data, we achieve 69\% accuracy by applying CoreGCN.
This performance is just 4\% lesser than when training with the entire dataset. 
Compared to CIFAR-10, we observe the better performance on VAAL 
in this benchmark.
The reason is that VAE might favour a larger query batch size ($>$1,000).
This exhaustively annotates large batches of data when the purpose of active learning 
is to find a good balance between exploration and exploitation as we constrain
the budget and batches sizes.


We further continue our evaluation on the image classification by applying our methods on 
FashionMNIST and SVHN. In Figure \ref{fig:fm}, the left and the right graphs show the comparisons on
\textbf{FashionMNIST} and \textbf{SVHN} respectively. As in the previous cases, our methods achieve at minimum 
similar performance to that of existing methods or outperforming them.
From the studies on these datasets, we observed consistent modest performance of FeatProp \cite{Wu2019ActivePropagation}.
This may be because it could not generalise on the unstructured 
data like ours.
\begin{figure}
    \centering
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=0.49\linewidth]{images/comb.png}
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip,  width=0.49\linewidth]{images/comb2.png}
\caption{Exploration comparison on CIFAR-10 between CoreSet and UncertainGCN (Zoom in the view)}
    \label{fig:tsne}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=0.49\linewidth]{images/tsne0.png}
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip,  width=0.49\linewidth]{images/tsne2.png}
\caption{Exploration comparison on CIFAR-10 between CoreSet and CoreGCN (Zoom in the view)}
    \label{fig:tsne_compare_coreset}
\end{figure}

\noindent \textbf{Qualitative Comparisons.}
To further analyse the sampling behaviour of our method we perform qualitative comparison with 
existing method. We choose CoreSet for its consistently better performance in 
empirical evaluations when compared to the other baselines.
We made this comparison on CIFAR-10.
For the two algorithms, we generate the t-SNE \cite{Maaten08visualizingdata} plots of 
both labelled and unlabelled extracted features from the learner at 
the first and fourth selection stage. To make a distinctive comparison of sampling behaviour 
from early stage, we choose to keep a difference 
of 3 stages. 
Figure~\ref{fig:tsne}, t-SNE plots, compares the sampling behaviour of CoreSet and UncertainGCN.
In the first sampling stage, the selected samples distribution
is uniform which is similar for both techniques. 
Without loss of generality, the learner trained with a small 
number of seed annotated examples is sub-optimal, and, hence 
the features of both labelled and unlabelled are not discriminative enough.
This makes the sampling behaviour for both methods near random. 
At the fourth selection stage, the learner becomes relatively more discriminative. 
This we can notice from the clusters representing each class of CIFAR-10.
Now, these features are robust to capture the relationship between the 
labelled and unlabelled examples which we encode in the adjacency matrix. 
Message-passing operations on GCN exploits the correlation between 
the labelled and unlabelled examples by inducing similar representations.
This enables our method to target on the out-of-distribution
unlabelled samples and areas where features are hardly distinguished. This characteristics we can observe 
on the plot of the fourth selection stage of UncertainGCN. 
Similarly, in Figure \ref{fig:tsne_compare_coreset}, 
we continue the qualitative investigation for the CoreGCN acquisition method.
CoreGCN avoids over-populated areas while tracking out-of-distribution unlabelled data.
Compared to UncertainGCN, the geometric information from CoreGCN maintains a 
sparsity throughout all the selection stages. Consequently, it preserves
the message passing through the uncertain areas while CoreSet keeps sampling 
closer to cluster centres. This brings a stronger balance in comparison to 
CoreSet between in and out-of-distribution selection with the availability of
more samples.

\noindent \textbf{Ablation Studies} To further motivate the GCN proposal, we conduct ablation studies on the sampler architecture. In Figure ~\ref{fig:abl_st}, still on CIFAR-10, we replace the GCN with a 2 Dense layer discriminator, \emph{UncertainDiscriminator}. This approach over-fits at early selection stages.
Although, GCN with 2 layers \cite{kipf2016semi} has been a de-facto optimal design choice,
we also report the performance with 1 layer (hinders long-range propagation) and 3 (over-smooths). However, to further quantify the significance of our adjacency matrix with feature correlations, we evaluate GCN samplers with identity (UncertainGCN Eye) and filled with 1s matrices (UncertainGCN Ones).
Finally, a study on 
two important hyper-parameters: drop-out (0.3, 0.5, 0.8) and the number of hidden units (128, 256, 512) is in the Supplementary B.2. 
We also fine-tune these parameters to obtain the optimal solution.



























\begin{figure}
    \centering
    \includegraphics[trim=2cm 0cm 1cm 1cm, clip, width=0.80\linewidth]{gcn_abl2.pdf}
\caption{Ablation studies (Please zoom in)}
    \label{fig:abl_st}
\end{figure}


\subsection{Regression} 
\noindent \textbf{Dataset and Experimental Settings:} We further applied our method on 
a challenging dataset for 3D Hand Pose Estimation benchmarks from depth images. 
ICVL~\cite{icvl} contains 16,004 hand depth-images in the training set and the test set has 1,600. At every selection stage, similar to the experimental setup of image classification, we 
randomly pre-sample 10\% of entire training examples $\mathbf{D}_S$ and apply the AL methods on 
this subset of the data. Out of this pre-sampled images subset, we apply our sampler to select the 
most influencing 100 examples.
\\
\noindent \textbf{Implementation Details:}
3D HPE is a regression problem  which involves estimating the 3D coordinates of the hand joints from depth images.
Thus, we replace ResNet-18 by commonly used \emph{DeepPrior} \cite{deepprior} as learner.
The sampler and the other components in our pipeline remain the same as in 
the image classification task. This is yet another evidence for our
sampling method being task-agnostic. For all the experiments, we train the 3D HPE with
Adam\cite{Kingma2015ADAM:Optimization} optimizer and with a learning rate of $10^{-3}$.
The batch size is 128. As pre-processing, we apply a pre-trained U-Net ~\cite{unet} model to detect hands, 
centre, crop and resize images to the dimension of 128x128.
\\
\noindent \textbf{Compared Methods and Evaluation Metric:}
We compare our methods from the two ends of the spectrum of baselines. One is random sampling 
which is the default mechanism. The other is CoreSet\cite{Sener2017ActiveApproach}, one of the best 
performing baselines from the previous experiments.
We report the performance in terms of mean squared error averaged from 5 different trials 
and its standard deviation. 
\begin{figure}\centering
\includegraphics[trim=2.5cm 0.2cm 3.5cm 1cm, clip,  width=.75\linewidth]{images/icvl2.pdf}
    \caption{Quantitative comparison on 3D Hand Pose Estimation (lower is better)}\label{fig:hands}\end{figure}
\\
\noindent \textbf{Quantitative Evaluations:} Figure~\ref{fig:hands} shows the performance comparison 
on ICVL dataset. In the Figure, we can observe that both our sampling methods, CoreGCN and UncertainGCN,
outperform the CoreSet and Random sampling consistently from the second selection stage.
The slope of decrease in error for our methods sharply falls down from the second 
till the fifth selection stage for UncertainGCN and till the sixth for CoreGCN. 
This gives us an advantage over the other methods when we have a very limited budget. 
At the end of the selection stage, CoreGCN gives the least error of 12.3 mm. 
In terms of performance, next to it is UncertainGCN. 
\subsection{Sub-sampling of Synthetic Data.} 


Unlike previous experiments of sub-sampling real images, we applied our method to select synthetic examples obtained from StarGAN~\cite{choi2018stargan} trained on RaFD~\cite{langner2010presentation} for translation of face 
expressions.
Although Generative Adversarial Networks~\cite{goodfellow2014generative} are closing the gap between real and synthetic data~\cite{ravuri2019seeing}, still the synthetic images and its associated labels are not 
yet suitable to train a downstream discriminative model. Recent study~\cite{bhattarai2020sampling} recommends sub-sampling the synthetic data before augmenting to the real data. Hence, we apply our algorithm to get a sub-set of the quality and influential synthetic examples. The experimental 
setup is similar to that of the image classification which we described in our previous Section. Learner and Sampler remain the same. 
The only difference will be in the nature of the pool images. Instead of real data, we have StarGAN 
synthetic images.
Figure~\ref{fig:rafd} shows the performance comparison of random selection vs our UncertainGCN method in 5 trials. From the experiment, we can observe our method achieving higher accuracy with less variance than commonly used random sampling. The mean accuracy drops for both the methods from the fourth selection stage. 
Only a small fraction of synthetic examples are useful to train the model~\cite{bhattarai2020sampling}. After the fourth stage, we force sampler to select more examples which may end up choosing noisy data.


\begin{figure}
    \centering
    \includegraphics[trim=2.5cm 0.2cm 3.5cm 1cm, clip, width=.75\linewidth]{images/rafdf.pdf}
    \caption{Performance comparison on sub-sampling synthetic data to augment real data for expression classification}
    \label{fig:rafd}
\end{figure}



























