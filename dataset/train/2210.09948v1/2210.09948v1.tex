

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{bbm}

\usepackage[accsupp]{axessibility}  


\usepackage{color}
\newcommand{\highlightChange}{\color{red}}
\def\HC{\highlightChange}
\newcommand{\Note}[1]{{\color{blue} \bf \small [NOTE: #1]}}
\newcommand{\jun}[1]{\textcolor{blue}{#1}}
\newcommand{\XL}[1]{\textcolor{red}{#1}}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{orcidlink}

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{}  

\title{Number-Adaptive Prototype Learning for \\ 3D Point Cloud Semantic Segmentation}

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{}
\end{comment}


\titlerunning{NAPL for 3D Point Cloud Semantic Segmentation}
\author{Yangheng Zhao\inst{1}\orcidlink{0000-0002-1467-2989} \and
Jun Wang\inst{2}\orcidlink{0000-0003-1543-5456} \and
Xiaolong Li\inst{3}\orcidlink{0000-0002-0662-4311} \and
Yue Hu\inst{1}\orcidlink{0000-0002-1125-8897} \and
Ce Zhang\inst{3}\orcidlink{0000-0002-4344-5259} \and 
Yanfeng Wang \inst{1,4} \and
Siheng Chen \inst{1,4}\orcidlink{0000-0001-6199-529X} 
\
    \mathcal{L}(z^{\prime}, z^{\text{gt}})=\sum_{i=1}^{N_q - M}\left[-\log s^{\prime}_{\sigma(i)}\left(c_{i}^{\text{gt}}\right) + \mathbbm{1}_{\left\{c_{i}^{\text{gt}} \neq \varnothing\right\}} \mathcal{L}_{\text{mask}}\left(m_{i}^{\text{gt}}, m^{\prime}_{\sigma(i)}\right)\right],

where for the padded token in , whose class label , we only calculate the cross entropy loss. 
For simplicity, we use the same  as DETR~\cite{carion2020end}.


\begin{table*}[t]
\begin{center}
\resizebox{\textwidth}{!}{\begin{tabular}{l|c|ccccccccccccccccccc}
\hline
  \textbf{Method} &
  \textbf{mIoU} &
  \rotatebox{90} {car} &
  \rotatebox{90} {bicycle} &
  \rotatebox{90} {motorcycle} &
  \rotatebox{90} {truck} &
  \rotatebox{90} {other-vehicle} &
  \rotatebox{90} {person} &
  \rotatebox{90} {bicyclist} &
  \rotatebox{90} {motorcyclist} &
  \rotatebox{90} {road} &
  \rotatebox{90} {parking} &
  \rotatebox{90} {sidewalk} &
  \rotatebox{90} {other-ground} &
  \rotatebox{90} {building} &
  \rotatebox{90} {fence} &
  \rotatebox{90} {vegetation} &
  \rotatebox{90} {trunk} &
  \rotatebox{90} {terrain} &
  \rotatebox{90} {pole} &
  \rotatebox{90} {traffic-sign} \\ \hline\hline
  \multicolumn{20}{c}{test set} \\ \hline
PointNet~\cite{qi2017pointnet}        & 14.6 & 46.3 & 1.3  & 0.3  & 0.1  & 0.8  & 0.2  & 0.2  & 0.0  & 61.6 & 15.8 & 35.7 & 1.4  & 41.4 & 12.9 & 31.0 & 4.6  & 17.6 & 2.4  & 3.7  \\
RandLANet~\cite{hu2020randla}       & 53.9 & 94.2 & 26.0 & 25.8 & 40.1 & 38.9 & 49.2 & 48.2 & 7.2 & 90.7 & 60.3 & 73.7 & 20.4 & 86.9 & 56.3 & 81.4 & 61.3  & 66.8 & 49.2 & 47.7 \\
KPConv~\cite{thomas2019kpconv}          & 58.8 & 96.0 & 30.2 & 42.5 & 33.4 & 44.3 & 61.5 & \textbf{61.6} & 11.8 & 88.8 & 61.3 & 72.7 & \textbf{31.6} & 90.5 & 64.2 & \textbf{84.8} & 69.2 & \textbf{69.1} & 56.4 & 47.4 \\ 
SqueezeSegv3~\cite{xu2020squeezesegv3}    & 55.9 & 92.5 & 38.7 & 36.5 & 29.6 & 33.0 & 45.6 & 46.2 & 20.1 & 91.7 & 63.4 & 74.8 & 26.4 & 89.0 & 59.4 & 82.0 & 58.7 & 65.4 & 49.6 & 58.9 \\
RangeNet++~\cite{milioto2019rangenet++}      & 52.2 & 91.4 & 25.7 & 34.4 & 25.7 & 23.0 & 38.3 & 38.8 & 4.8  & \textbf{91.8} & 65.0 & 75.2 & 27.8 & 87.4 & 58.6 & 80.5 & 55.1 & 64.6 & 47.9 & 55.9 \\
SalsaNext~\cite{cortinhal2020salsanext}       & 59.5 & 91.9 & \textbf{48.3} & 38.6 & 38.9 & 31.9 & \textbf{60.2} & 59.0 & 19.4 & 91.7 & 63.7 & \textbf{75.8} & 29.1 & 90.2 & 64.2 & 81.8 & 63.6 & 66.5 & 54.3 & \textbf{62.1} \\ 

Ours (NAPL) & \textbf{61.6}	& \textbf{96.6} & 32.3 & \textbf{43.6} & \textbf{47.3} & \textbf{47.5} & 51.1 & 53.9 & \textbf{36.5} & 89.6 & \textbf{67.1} & 73.7 & 31.2 & \textbf{91.9} & \textbf{67.4} & \textbf{84.8} & \textbf{69.8} & 68.8 & \textbf{59.1} & 59.2\\ \hline

\multicolumn{20}{c}{validation set} \\ \hline
PWC &	62.3 &	96.2 &	21.5 &	62.0 &	\textbf{78.6}&	50.8 &	68.5 &	\textbf{87.4}&	0.0&	\textbf{93.9}&	\textbf{51.0}&	\textbf{81.3}&	\textbf{1.2}&	\textbf{90.1}&	59.2&	87.8 &	66.1 &	\textbf{73.9}&	64.3 &	50.0	
 \\

Ours (NAPL) &\textbf{64.6}&	\textbf{97.4}&	\textbf{38.2}&	\textbf{71.5}&	74.3&	\textbf{66.2}&	\textbf{71.1}&	81.6&	0.0&	93.1&	48.4&	80.2&	0.2&	90.0&	\textbf{62.6}&	\textbf{89.0}&	\textbf{68.0}&	77.2&	\textbf{66.8}&	\textbf{52.2} \\

\hline
\end{tabular}}
\end{center}
\caption{
Quantitative comparison on SemanticKITTI dataset~\cite{behley2019semantickitti}. The proposed NAPL 
outperforms the recent 3D semantic segmentation methods.}
\label{tab:res-kiti}

\end{table*}


\section{Experiments}

\subsection{Implementation details}

\textbf{Dataset.} SemanticKITTI~\cite{behley2019semantickitti} is a widely used benchmark for 3D semantic segmentation. 
We follow ~\cite{hu2020randla} to use the standard training and validation set splits.


\textbf{Model architecture. }Without loss of generality, we use a MinkUNet~\cite{choy20194d} without classifier as our PFEM, which is a fully convolutional voxel-based model with four stages. The input voxel size is 0.05m. We use the fourth stage feature of PFEM and  queries as the input of the prototype learning module. 

\textbf{Training details.} 
We use AdamW optimizer and poly learning rate schedule with an initial learning rate of  for transformer and point decoder, and  for pre-trained backbone. We set the number of dropout prototypes . 
Our model is trained with batch size of 16 on 4 RTX 3090 GPUs for 20 epochs.

\begin{figure}[t]
    \centering
    \includegraphics[width=11cm]{figure/mask_example.pdf}
    \caption{We visualize a segmentation result of the ``person" class. 
    (a) and (c) shows that our model is better than baseline C. (b) and (d) explains the observation: a single prototype cannot cover the high variance intra-class patterns.}
    \label{fig:example}
\end{figure}
\subsection{Results}


\begin{figure}[t]
\begin{minipage}[b]{.48\linewidth}
    \centering
    \begin{tabular}{c|cccc}
        \hline
                    & A & B & C & Full \\
        \hline
         PFEM & \checkmark & \checkmark & \checkmark & \checkmark \\ 
         T &  &\checkmark & \checkmark& \checkmark\\
         PBW & & &\checkmark & \checkmark\\
         PD & & & &\checkmark \\
         \hline
         mIoU &62.30 & 48.86 & 63.67 & \textbf{64.62}\\
         \hline
         
    \end{tabular}
    \captionof{table}{Model component ablation study on SemanticKITTI val-set. 
    }
    \label{tab:ablation}
\end{minipage}
\begin{minipage}[b]{.48\linewidth}
    \centering
    \includegraphics[width=5.0cm]{figure/class_static.pdf}
    \captionof{figure}{Statistics of the average number of prototypes per class. 
    }
    \label{fig:class_static}
\end{minipage}

\end{figure}
We use the mean intersection of union (mIoU)~\cite{behley2019semantickitti} as our evaluation metric. The results are reported from both the validation and the test set of SemanticKITTI. 

\textbf{Quantitative evaluation.} In Table~\ref{tab:res-kiti}, we compare our number-adaptive prototype learning model with the existing 3D point cloud semantic segmentation models~\cite{qi2017pointnet,hu2020randla,thomas2019kpconv,xu2020squeezesegv3,milioto2019rangenet++,cortinhal2020salsanext} and the baseline point-wise classification model class-by-class. The result shows that our proposed number-adaptive prototype learning paradigm is better than the traditional point-wise classification paradigm. Specifically, in most of the classes where instances have different patterns including person, other-vehicle et al., our model has made significant improvements. 

\textbf{Ablation study.} In Table~\ref{tab:ablation}, we further study the effectiveness of individual components in our model, including the transformer decoder (T), pre-trained backbone weights (PBW), and the prototype dropout training strategy (PD). The results show that: i) directly adding a transformer module to the point cloud segmentation model and training them together greatly harms the performance by 13.44\%; ii) using a pre-trained backbone makes the model easy to train and boost the result by 1.37\%; and iii) the prototype dropout strategy can further promote the model performance
by 0.95\%. 

\textbf{Handling challenging cases.} In Fig.~\ref{fig:example}, we show a person segmentation case from the validation set to discuss the need for multiple prototypes for each class. Fig.~\ref{fig:example} (a) and (c) show the segmentation results of our model and baseline model C, respectively. The points in green are the true-positive points and the points in red are false-negative. Our model correctly segments all the points in this scene, while C misses the points belonging to the shorter person. Fig.~\ref{fig:example} (b) and (d) reveal the deeper 
reason: a single prototype cannot cover all the points of different persons, while two prototypes can describe the different patterns of people, and thus make a better segmentation. The visualization shows the superiority of our proposed number-adaptive prototype learning paradigm. 

\textbf{Prototype number analysis.}  Fig.~\ref{fig:class_static} presents the average number of prototypes for each class in each frame. With the prototype dropout strategy, our model can adaptively produce prototypes for each class, while the baseline model C using the same model architecture can only produce one prototype for each class. This result shows the effectiveness of our proposed training strategy. 


\section{Conclusions}
In this paper, we propose a novel number-adaptive prototype learning paradigm for 3D point cloud semantic segmentation. To realize this, we leverage a transformer decoder in our model to learn prototypes for semantic categories. To enable training, we design a prototype dropout strategy to promote our model to produce number-adaptive prototypes for each class. The experimental results and visualization on SemanticKITTI demonstrate the effectiveness of our design.


\noindent\textbf{Acknowledgements.} This work is  supported by National Natural Science Foundation of China
under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900 and CALT Grant 2021-01.
\clearpage
\bibliographystyle{splncs04}
\bibliography{ref}
\end{document}
