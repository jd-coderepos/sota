

\documentclass{article}

\usepackage{microtype}
\usepackage{mathrsfs} 
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{amsmath}
\usepackage[bb=px]{mathalfa} 

\newcommand{\hlx}{\textcolor{red}}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{icml2021}





\begin{document}

\twocolumn[
\icmltitle{Reduction of Class Activation Uncertainty with Background Information}













\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]







\begin{abstract}
Multitask learning is a popular approach to training high-performing neural networks with improved generalization. In this paper, we propose a background class to achieve improved generalization at a lower computation compared to multitask learning to help researchers and organizations with limited computation power. We also present a methodology for selecting background images and discuss potential future improvements. We apply our approach to several datasets and achieved improved generalization with much lower computation. We also investigate class activation mappings (CAMs) of the trained model and observed the tendency towards looking at a bigger picture in a few class classification problems with the proposed model training methodology. Applying transformer with the proposed background class, we receive state-of-the-art (SOTA) performance on STL-10, Caltech-101, and CINIC-10 datasets. Example scripts are available in the `CAM' folder of the following GitHub Repository: github.com/dipuk0506/UQ

\end{abstract}













\section{Introduction} \label{Sec1}
Although deep machine learning models have brought revolutionary performance in computer vision, deep learning models often perform poorly on test data due to poor generalization \cite{caruana2000overfitting}. The classification score for each class is a weighted sum of the final convolutional layer outputs. Researchers have recently observed that some spatial positions (x,y) of the last convolutional layer contribute a high value toward the score for an image \cite{zhou2016learning}. Such a spatial position (x,y) usually contains a pattern of that class. That brings us the opportunity to see the effect of performance enhancement techniques on recognizing individual features.

Uncertainties in the deep learning model are coming from deep layers and how the final head layer interprets the information from the last convolutional layers. The performance of transfer-learned models also varies depending on the structure of the models \cite{kolesnikov2020big}. Therefore, simpler deep neural networks (DNNs) may not propagate important features of a few classes in a large dataset \cite{zagoruyko2016wide}. However, even larger DNNs or DNNs of moderate size do not ensure good performance. One common reason for their poor performance is prioritizing less relevant features instead of important and robust patterns. Fig. \ref{Bird} shows one such situation and the effect of our proposed improvement.  Fig. \ref{Bird}(a) shows the image of the bird. Fig. \ref{Bird}(b),(c) shows class activation maps (CAMs) respectively for the traditional and for the proposed model. Later subplots present CAMs on images and deep feature factorization on the image. A bird might be sitting and its eyes can be closed in a test image. A network deciding the class of an image by seeing most portions of a bird is expected to be more robust than a network deciding a class by seeing the legs and eyes on an image \cite{kapidis2019multitask}. 

\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[clip, trim=2.2cm 0.9cm 1.6cm 0.5cm, width=\columnwidth,angle=0]{Figures/CAM_bird.pdf}}
\caption{Models with both traditional training and training with background class are applied to a bird image (a) in the STL-10 dataset. Subplots (b) and (c) show class activation mapping of the bird class on the final convolutional layer respectively for traditional and for training with background situations. Subplots (d) and (e) show class activation mapping with the image. Subplots (f) and (g) show deep feature factorization results on the image for the traditional and proposed method respectively.}
\label{Bird}
\end{center}
\vskip -0.2in
\end{figure}


Transfer learning and multitask learning are two revolutionary approaches toward improved generalization \cite{kolesnikov2020big, gesmundo2022evolutionary}. Transfer learning is a powerful technique for researchers and institutions with low computation resources. Although initial layers are generalized in transfer learning, randomly initialized head layers with inadequate training can cause a high generalization error. Multitask learning can potentially generalize both initial and final layers and has brought superior performance in several datasets \cite{gesmundo2022evolutionary}. Transfer learning with the help of publicly available pre-trained models requires much lower computation. Multitask learning requires much more computing compared to transfer learning and learning from random initialization. Moreover, multitask learning requires more attention to the data. While training NN for both handwritten digits and handwritten characters of different languages, images of two different classes can be very similar. Such as the number `0', and the letter `O' are similarly written by many participants. Moreover, the accuracy of the model in multitask learning can potentially degrade while training a shared layer for two completely different tasks \cite{kokiopoulou2021flexible, standley2020tasks}.  
 

Both the initial layers and head layers need good generalization to achieve good performance. Several models with highly generalized initial layers are publicly provided by reputed organizations. Therefore, we try to bring the benefit of both transfer learning and multitask learning with optimal computation. We develop a background class to bring better generalization in head layers.


\section{Background}
We validate the advantage of introducing a background class with both theory and practical results. The background class brings improved generalization. The generalization improvement can also be achieved through several other approaches. As the proposed method gets a similar effect to multiclass learning, we present multitask learning in this section. Class activation mapping (CAM) is an effective tool to evaluate potential reasons for any poor performance. This section also presents the existing use of the background class.

\subsection{Generalization}
Generalization error is also known as the out-of-sample error or the overfitting error in statistical learning. Both the training and test dataset have a finite number of samples. Moreover, the test samples are different from the training sample, unless there is an overlap of data. An overfitted model on the training data can potentially provide a significantly low accuracy on the test dataset \cite{bousquet2002stability}. One of the simplest estimates of generalization error is the \emph{leave one out error}, and another one is the \emph{empirical error}. Both of the estimates are derived from . Where,   is the expectation function,  is the loss function,  is the model function and  is the sample \cite{bousquet2002stability}.  is known as the \emph{generalization error} and expressed as:

where  is the number of samples in the dataset. Regularization is a popular technique for generalization improvement \cite{micchelli2005learning}. The cost function for regularized model training has the following form: 

where  is the  parameters of the model  and  is the number of parameters in . The research community considers high  values as an indication of sharp changes in predictions over the input domain, resulting in overfitting. Therefore,  high  values are penalized in the cost function.

Several other common approaches to reducing generalization error are: using more training data \cite{caruana2000overfitting}, data augmentation \cite{cubuk2019autoaugment}, early stopping \cite{caruana2000overfitting}, optimal model structure \cite{hernandez2023training}, neural architecture search \cite{pham2018efficient}, dropout \cite{gal2016dropout}, cross-validation \cite{efron1983estimating, krogh1994neural}, pre-trained Model \cite{kolesnikov2020big}, and multitask learning \cite{ndirango2019generalization}. Besides these common approaches, many researchers have proposed novel approaches to improve generalization. 

\subsection{Multitask learning}
Multitask learning can potentially improve generalization and bring superior performance when tasks are related. Several researchers have provided theories indicating improved generation in multitask learning. According to Ghifary et al., \cite{ghifary2015domain} multitask learning autoencoder training tries to minimize both share weights () and individual task-specific weights (), where  is the task number (). As they regularized  for all tasks in a loop, the model becomes more generalized.

Multitask learning can potentially keep good activation values over a larger space (x,y) in the final convolutional layer \cite{kapidis2019multitask}. Therefore, DNNs are often recognizing the class by seeing a larger portion of the image. When a model recognizes a bird by seeing only legs, the model may misclassify an image where a bird is sitting. Moreover, some branches in trees often have patterns that are quite similar to birds' legs. Therefore, prediction by seeing a larger portion of the bird is more robust compared to the prediction by seeing only the leg.
According to Ndirango et al., \cite{ndirango2019generalization} multitask learning can potentially bring improved generalization through the conditional improvement in loss function values.




\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[clip, trim=0.3cm 10.2cm 20.9cm 1.8cm, width=\columnwidth,angle=0]{Figures/Class_boundary.pdf}}
\caption{Rough diagrams to explain potential generalization improvement in multitask learning.  The decision boundary in traditional learning (a) becomes overfitted. The decision boundary in multitask learning can potentially be more generalized (b) than the traditional one. However, multitask learning can potentially bring underfitting issues.}
\label{Cluster}
\end{center}
\vskip -0.2in
\end{figure}

Fig. \ref{Cluster} presents two rough diagram visualizations to explain potential generalization improvement in multitask learning. Deep learning models have a large number of parameters. A few training samples do not generalize parameters. The decision boundary in traditional learning, shown in Fig. \ref{Cluster} (a) becomes overfitted. A slight variation in test data can potentially result in very low accuracy. The decision boundary in multitask learning can potentially be more generalized than the traditional one. In Fig. \ref{Cluster}(b), a proper decision boundary is depicted by a green contour. The Grey contour represents a decision boundary where the model is facing an underfit issue.
 

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[clip, trim=0.3cm 7.4cm 3.5cm 1.8cm, width=6.2in,angle=0]{Figures/Deffierent_Learning_NN.pdf}}
\caption{Rough diagrams presenting multitask learning, transfer learning, and proposed training with one or more background classes. In multitask learning (a), the child model is created from a parent model. Layers can be frozen, and the initialization of non-frozen layers can be transferred or randomly initialized. New layers can be added or removed. In traditional transfer learning (b), all initial layers are frozen. The head layer is structured according to the output format of new data and randomly initialized. However, researchers have also investigated transfer learning without freezing all initial layers and by unfreezing initial layers after certain epochs. In proposed work (c), we start with a transfer learned model. Initial values are copied from the pre-trained model but no layer is frozen. The head contains classes of the data and the background class.  }
\label{Chart}
\end{center}
\vskip -0.2in
\end{figure*}



\subsection{Class Activation Mapping}
Researchers have recently observed that convolutional neural networks can detect the class activity regions in the image besides providing an overall classification result \cite{zhou2016learning}. The spatial  values of the last convolutional layer get multiplied by the weights of the fully connected layer and added toward the score of each class (). The following equation presents the simplest form of computing score from the last convolutional layer: 

where,  is the class activation map value for  location at the last convolutional layer.  is computed as follows:

where,  is the activation unit  at  location at the last convolutional layer.


\subsection{Previous Use of Background Class}
The use of background class is common in detection-type problems. In such problems, the algorithm aims to detect one pattern, and other patterns are considered as the background. The concept of background class is previously applied in concrete detection \cite{son2012automated}, defect detection \cite{park2016machine}, ribonucleic acid (RNA) detection \cite{bacstanlar2014introduction} and energy physics \cite{whiteson2009machine}. Son et al. \cite{son2012automated} collected 108 images of concrete surfaces and other surfaces. They transformed red-green-blue (RGB) images to non-RGB color spaces and applied several models. They observed superior performance with hue-saturation-intensity (HSI) color space transformation and the support vector machine model. Park et al. \cite{park2016machine} proposed an approach for the automatic detection of wear, burrs, scratches, and dirties. They train CNNs for detection. They claimed time, cost-effectiveness, and higher accuracy than humans. According to Bacstanlar et al., \cite{bacstanlar2014introduction}, the RNA detection problem can also be similar to the detection problem where the desired pattern belongs to the target class and all other patterns belong to the background class.
Whiteson et al. \cite{whiteson2009machine} applied supervised machine learning with background classes for high-energy physics. They collected data from the Fermilab Tevatron accelerator and developed a dataset with five background classes. They applied a fully connected shallow neural network and received state-of-the-art (SOTA) results at the time when the work was published. 

The Caltech-101 dataset has a background class \cite{li2004caltech}, but the background class of that dataset contains a few distinctive images. Moreover, several background images of that dataset contain human faces which overlap with another class. Therefore, we develop background classes by ensuring that there is no common pattern.


\section{Contribution}
We start this section with our observations and hypotheses. After that, we propose the background class to bring improved generalization in head layers. As we propose initial layers to be transfer learned, they are well generalized on a large dataset. Therefore, training head layers with more samples can potentially improve the generalization of head layers. According to our theoretical understanding, the background class can potentially get a better generalization and can potentially restrict irrelevant background patterns from providing a high score to a class. 

\subsection{Observations}
We reached several conclusions based on our observations. These conclusions helped us to find the idea of adding a background class. Other groups of researchers may agree or may disagree with our conclusions. Therefore, we write our conclusions as hypotheses. Observations with hypotheses are as follows:  

Observation: Models trained on a similar large dataset can propagate almost all important features of the target dataset without further training. Post-trained head computes score-portions  for different classes  based on those features. Yosinski et al. \cite{yosinski2014transferable} performed a study on the transferability of features. They achieved similar findings. Later, Zhou et al. \cite{zhou2016learning} observed how the final layer computes scores from .

\emph{Hypothesis 1: Pre-trained models on a large dataset can be applied to classify new images of slightly different types with a post-trained head and such combinations often bring eye-catching performance. However, poor performance is often observed when new images are quite different.} 


Observation: Although DNNs have a large number of parameters, they can potentially face underfitting issues in very large datasets. Torchvision \cite{marcel2010torchvision} provided several pre-trained models with the ImageNet accuracy information. ResNet18 models have received much lower accuracy on the ImageNet dataset compared to the ResNet50 model. Different model structures with the same parameter count usually have different accuracy on the same dataset. However, the accuracy of the two models can be the same, accidentally. The propagation of features () and decoding may depend on the structure of the model. However, artificial intelligence is still a growing field. Researchers may find a better training method where ResNet18 can potentially provide comparable accuracy with bigger models. 

\emph{Hypothesis 2: Different DNN models usually exhibit different accuracies for the same dataset and training conditions. }



Observation: Fully connected head layers are capable of giving a high  to one class by decoding several features. Such as providing a high  value by detecting the eye and legs of a bird at two different  locations. The capability of giving a high value often exists when positions  of features are interchanged. In Fig. \ref{Bird}(f), the eye and legs are indicating the bird class. The white background indicates airplane class. The dorsal area of the bird indicates the dog class. Some features are decoded incorrectly. Different features are propagated parallelly over the 2D convolutional planes until the last convolutional layer. Each convolutional layer also performs a modulation based on weights and activation functions. 

\emph{Hypothesis 3: Convolutional layers propagate different features with modulations parallelly in spatial space (x,y). The fully-connected head of the DNN model computes scores for classes based on the modulated output features of the last convolutional layer.}


Observation: While detecting a bear in terrain, the terrain is the background. However, the model can potentially detect that terrain as a rugby ball\footnote{https://jacobgil.github.io/pytorch-gradcam-book}. That can potentially happen due to the presence of terrain in many images in the training set, containing a rugby ball. In Fig. \ref{Bird}(f), the white background indicates airplane class. That can potentially happen due to the presence of white background in many images in the airplane class.

\emph{Hypothesis 4: Features of background objects also contribute to scores of different classes. When a significant number of training images of one class contains certain background objects, the head of the model decodes the features of those background features as of that class.}

Observation: In traditional transfer learning, all of the initial layers are often kept frozen. Therefore,  becomes the same when the image and image augmentation are the same. The head can be trained for different datasets and with different training parameters but those training do not change . However, researchers have also investigated transfer learning without freezing all initial layers and by unfreezing initial layers after certain epochs \cite{goodfellow2016deep, guo2019spottune}.

\emph{Hypothesis 5: When all the initial layers are frozen in the transfer learning}, the value of  is a function of the image, augmentation, and pre-trained weights. Training of the head optimizes parameters ( and biases) in the head.



The number of parameters in a single layer fully connected head network  is directly proportional to the number of class  (i.e. ). Weight increases with the increase of both the number of features and the number of classes in a proportional manner. The number of biases on the fully connected head layer is equal to the number of classes. 
The multitask learning method trains models with several classification problems. Therefore, the training consists of training different heads for different datasets. The parameter in each head layer of subnetworks  is directly proportional to the number of class  of the corresponding dataset. The total number of trainable head parameters increases drastically with multitask learning. The number of samples also becomes the summation of individual sample numbers. Both the data and the number of trainable parameters increase the multitask learning by several times compared to traditional transfer learning. The GPU requirement also increases with increased parameters. The batch size of DNN training needs to be reduced to meet the GPU requirement. Therefore, the training time increases significantly.

Researchers and organizations with limited computation power may not have enough computation resources to train a large neural network with a large dataset within a reasonable time. We propose keeping one background class. That increases the number of classes by one. The number of samples also increases but that increment is much lower than the multitask learning. It is also possible to select an optimal number of background samples.  

Researchers prefer to use any publicly available datasets while developing a pre-trained model for publishing a result. Researchers also prefer to use publicly available models for higher transparency. Therefore, we develop the background class from publicly available image datasets.  

\subsection{Improvement with Background Class}
Before the invention of class activation mapping, researchers proposed to reduce patch distance \cite{zuo2014learning, frome2006image} to improve classification results. Forme et al. \cite{frome2006image} defined patch distance with the help of two terms: \emph{focal image} (), and \emph{a candidate image} (). The focal images are images in the training set, and the candidate image is the test image. They define image-to-image
distance function  as follows:

where,  is a vector of weights,  is the patch distance vector between  and .
They consider two different types of images: dissimilar images () and similar images (). For accurate prediction   . Their cost function tries to increase the difference between  and . Their cost function also regularizes weights ().

Similarly, the feature activation unit  can potentially represent one class (c). The optimization increases  over iteration when . Where,  is the superset of potential activation unit values for features representing class  near  location. The optimization also decreases  over iteration when . In a traditional multiclass classification problem,  contains both relevant activations from objects and irrelevant activations coming from backgrounds. Let,  be the superset of potential activation unit values for features representing the background. One class may contain a similar background in a good portion of training samples. The optimization process may consider such a feature as a part of that class object. 

In the proposed method,  weight to background activation unit values  are reduced over iteration. Weight to common activation unit values  are also reduced when there exist more example in the background class. The presence of an extra background class with an optimal number of background samples also optimizes . Instead of a very high  in a small region, and low  in other regions,  becomes high over a larger region of the class object in most samples. 

NNs face both underfitting and overfitting issues based on the size of the dataset and the NN structure. In the proposed method, we can adjust the background data. Through trial and error with different background images and with different numbers, we can optimize the performance of NN.

\subsection{Background Class Generation}
We follow the following principles for the generation of background class:
\begin{itemize}
  \item Images in the background class should not contain any object, which belongs to a class of the classification problem. 
  \item Images in the classification dataset often contain many patterns in the background. It may not be possible to construct a background class containing all of those background patterns. However, the background dataset developer should try to cover common patterns.
 \item The background class may contain a few monochromatic images so that, the learned models do not give a classification result by seeing only the color. 
 \item The background class may contain some textures that are irrelevant to the target classes so that, the learned models do not get overfitted in the texture domain. 
  \item The number of images in the background class should be suitable for the classification task. 
\end{itemize}
The designer of the background class need to investigate several images of the classification problem for different classes. For example, the FGVC Aircraft dataset contains images of different types of aircraft. The background of images contains sky, ground, buildings, tree, runway, etc. The designer have to find several other datasets containing those background patterns to construct the background class. For each problem, we develop a background class containing distinctive images from publicly available several datasets. 
Selection of the size of the background class also needs several considerations. A background class with a few images may not bring a good generalization. Also, a large background class may create problems of highly imbalanced data distribution over classes. According to our investigation, an acceptable range of the background class can be the size of individual classes to the size of the classification dataset. The background class needs to be small for highly uncertain classification problems. When the classification problem receives less than 50\% classification accuracy, adding a background class of a large size can potentially bring poor performance. When 90\% images on a dataset are background images, predicting all samples as the background can bring 90\% accuracy. The trained model may predict all samples as the background. 


\subsection{Weights to the background class on the head layer may not capture all background features}
The fully-connected layer of the model may not create optimal weight () for assigning high values for all background features . However, the optimization algorithm can limit () values for common features existing in backgrounds of class images (). Images of a class usually contain different background features  in different images. As the background class contains a large number of background images, () values for features  get reduced for the optimization of the loss function over iterations.

The training stage considers the extra background class, as shown in Fig. \ref{Chart}(c). However, we do not consider the score on the background class () in the test phase. As the problem is a classification problem, we consider the index of the maximum  value as the predicted class. 

 
\begin{figure*}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[clip, trim=3.7cm 8.1cm 7.0cm 2.6cm, width=5.2in,angle=0]{Figures/Background_data.pdf}}
\caption{A few example images in the background class for color images: (a) image of a forest, (b) color inverted image of (a), (c) glacier, (d) mountain, (e) monochromatic images, (f) grass, (g) clouds from a satellite image, (h) image of Galaxies, (i) water body from a satellite image, and (j) wood texture. }
\label{Background}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Class Activation Uncertainty Score}
In this study, we haven't proposed any score for class activation uncertainty. We concentrate on getting larger deep feature factorization regions for the predicted class. Many researchers are trying to develop robust scores for uncertainties in different machine learning domains \cite{marin2016prediction, kendall2018multi}. However, most scores depend on certain assumptions. Therefore, some other groups of researchers could not completely agree with them and developed another score for training and evaluating models \cite{pearce2018high, kabir2021optimal}. Future researchers may find a widely accepted uncertainty score for class activation and uncertainties in other machine learning domains.

\subsection{Structure of Proposed Models}
The multitask learning method can follow complex structural combinations \cite{gesmundo2022evolutionary}. The multitask learning model training can freeze several layers and retrain several other layers. It is possible to introduce new layers, omit layers, and train existing layers from random initialization. Fig. \ref{Chart}(a) presents a graphical representation of multitask learning. In our investigated multitask learning, we define a shared fully connected layer containing outputs of different datasets. Previous layers are already well generalized on a large dataset.  Therefore, we declare a common fully connected layer for multiple classification problems for a better generalization of weights on that layer.
The class number on the fully connected layer becomes equal to the summation of class numbers of different datasets. Datasets are also arranged accordingly for the model training.

We perform transfer learning without freezing any layer. As a result, the training becomes similar to traditional training with transferred initialization. The fully connected head of the model is modified according to the class number of the investigated dataset. Fig. \ref{Chart}(b) presents a graphical representation of transfer learning.

In the proposed learning with background class, the number of outputs from the fully connected layer becomes equal to the number of classes in the dataset plus one. The extra class is reserved for the background class. We generate the background class considering the classification dataset. Fig. \ref{Chart}(c) presents a graphical representation of the proposed training. Fig. \ref{Chart}(d) presents the meaning of several components in Fig. \ref{Chart}(a)-(c) subplots.







\begin{table*}[t]
\caption{Size of training, validation, and test data.}
\label{Datasize}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccccr}
\toprule
Dataset  &\multicolumn{3}{c}{\# Images in Dataset} & Multitask Pair& Datasize of the \\
 &    train & validation & test & & Background Class \\
\midrule  
STL-10  &4,500&500*&8,000&Oxford-102 & 3,001 \\
Oxford-102 &5,897&655*&818&STL-10 & 3,001\\



CUB-200-2011 &5,395&599*&5,794&FGVC Aircraft& 3,001\\

FGVC Aircraft &4,558 &2,109 &3,333 &CUB-200-2011 & 3,001\\
K-MNIST  &54,000&6,000*&10,000&EMNIST-Balanced  & 23,500\\
EMNIST-Balanced &101,520&11,280*&18,800&K-MNIST & 70,000 \\
EMNIST-Byclass &628,139&69,793*&116,323&K-MNIST & 70,000 \\


\bottomrule
\end{tabular}
\\ * We split the training data to create the validation set. 
\\  We transfer 12 images from each validation class folder to each training class folder. 
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Classification Accuracies on Different Datasets with Different Training Conditions with Traditional Models.}
\label{Tclass}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccr}
\toprule
Dataset & Model & \# Epoch & Transfer Learn. & Multitask Learn. & Proposed \\
\midrule  
STL-10  &WideResNet-101 &20& 98.40 0.19& 98.42 0.14 & \bf{98.58 0.08} \\
Oxford-102 &WideResNet-101 &20& 98.78 0.21& \bf{99.03 0.19} & 98.93 0.16 \\
CUB-200-2011 &Inception-v4 &20& 83.26 2.36& \bf{84.93 1.91}& 83.97 1.74 \\
FGVC Aircraft &Inception-v4 &20& \bf{86.30 1.20}& 85.12 1.60& 85.91 1.20\\
K-MNIST  &ResNet-18  &15& 98.14 0.21& 98.41 0.15& \bf{98.60 0.12} \\
EMNIST-Balanced &ResNet-18 &15& 88.91 1.25& 89.24 1.42& \bf{90.04 1.08} \\
EMNIST-Byclass &ResNet-18 &15& 87.82 0.99& 87.92 0.89& \bf{88.22 0.79} \\


\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Classification Accuracies on Different Datasets with Different Training Conditions with Transformer (ViT‑L/16).}
\label{TTransformer}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccr}
\toprule
Dataset & \# Epoch & ViT‑L/16 & + Spinal FC & + Background & + (Spinal FC \& Background) \\
\midrule  
STL-10  &2 &99.50 0.09& 99.58 0.06& 99.63 0.06 & \bf{99.71 0.06}* \\
Oxford-102  &2 &99.51 0.08& 99.41 0.09& \bf{99.75 0.03} & 99.60 0.05 \\
CIFAR-10  &3 &98.67 0.25& 98.81 0.19& 98.97 0.18 & \bf{99.05 0.14} \\
CIFAR-100  &3 &92.88 0.73& 93.11 0.55& 93.19 0.41& \bf{93.30 0.40} \\
Caltech-101  &2 &97.02 0.35& 97.31 0.29& \bf{98.02 0.18}*& 97.69 0.21 \\
CINIC-10  &2 &95.11 0.14& 95.43 0.19& \bf{95.80 0.07}* & \bf{95.80 0.10}* \\


\bottomrule
\end{tabular}
\\ * SOTA Performance on May 2023.
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\section{Results}
To compare transfer learning, multitask learning, and the proposed method has investigated our proposed methodology in STL-10, FGVC Aircraft, CUB-200-2011, Kuzushiji-MNIST, EMNIST-(Balanced, Byclass), and Oxford-102 datasets. The proposed method provides higher accuracy compared to transfer learning. The accuracy is slightly higher on average compared to multitask learning. The time required to train models in the proposed method is significantly lower than the multitask learning. We investigate the proposed method with the transformer on STL-10, CIFAR-10, CIFAR-100, Oxford-102, Caltech-101, and CINIC-10 datasets. While applying the transformer we compare transfer learning, the proposed method, and the proposed method with the Spinalnet fully-connected layer.

Many of the datasets do not have labeled training data. In such a situation, we split the training data. We keep 90\% samples as the training data and 10\% samples as the validation data. Also, we consider the validation data of the original dataset as the test data. We apply the stochastic gradient descent (SGD) optimizer with the cross entropy loss criterion to train our models in all datasets and models.
In the transfer learning method, the number of outputs in the model is equal to the number of classes on the data in which the model is trained.
We share the fully connected head layer in multitask learning and arrange data accordingly. In the multitask learning method, the number of outputs in the model is equal to the summation of the number of classes on datasets trained together. In the proposed method of training with background class, the number of outputs in the model is equal to the number of classes on the data in which the model is trained; plus one for the background class.
Table \ref{Datasize} presents sizes of datasets, and Table \ref{Tclass} presents classification accuracies on our investigated datasets.

\subsection{Transfer Learning and Multitask Learning}
\subsubsection{STL-10, and Oxford-102 Datasets}
The STL-10 dataset \cite{coates2011analysis} contains ten classes. As the dataset contains \emph{bird} and \emph{airplane} classes, multitask learning with CUB-200-2011 and FGVC Aircraft datasets can potentially degrade the performance. Therefore, we perform multitask learning with Oxford-102 flower dataset \cite{nilsback2008automated}.

The STL-10 dataset contains 9696 sized images. We observe higher accuracy with enlarged images. Therefore the images are resized to 256256 sized images while training CNN-type networks. The transformer we investigate receives 224224 sized images. Therefore the images are resized to 224224 sized images while training the transformer. We also perform random rotation, random crop, and random horizontal flips augmentations on training images. Oxford-102 contains images of varying sizes. We also resized them to the same size. Both the transfer learning training and multitask learning training are performed over 20 epochs with a learning rate of 0.001. We apply the WideResNet-101 pre-trained model for both learning methods. That model is pre-trained on the ImageNet dataset. We download the WideResNet-101 pre-trained model from the \emph{torchvision} package. 


\subsubsection{CUB-200-2011 and FGVC Aircraft}
As the CUB-200-2011 dataset \cite{wah2011caltech} contains several images from the ImageNet, we apply the Inception-v4 pre-trained model on CUB-200-2011, and  FGVC Aircraft datasets \cite{maji2013fine}. We download the Inception-v4 pre-trained model from the \emph{timm} package. The CUB-200-2011 dataset contains images of birds. The FGVC Aircraft datasets contain images of aircraft.  They do not have a class in common.

Images of both datasets are resized to 448448 sized images. We also perform random rotation, random crop, random perspective, random vertical flip, and random horizontal flips augmentations on training images. We train models for 20 epochs with a 0.01 learning rate.

\subsubsection{Kuzushiji-MNIST, EMNIST-(Balanced, Byclass)}
Both the KMNIST \cite{clanuwat2018deep} and the EMNIST \cite{cohen2017emnist} datasets contain images of 2828 sizes.
We apply the ResNet-18 pre-trained model from \emph{torchvision} package for the classification of handwritten letters and digits datasets as they do not require very large models for good performance. KMNIST is quite different from English letters and digits. Therefore, we perform two multitask learning: 1) learning with KMNIST and EMNIST-Balanced pair, and 2) learning with KMNIST and EMNIST-Byclass pair. We resize images to 120120 size to achieve slightly higher accuracy. We also perform random perspective, random crop, and random rotation augmentations on the training dataset. Both the transfer learning training and multitask learning training on KMNIST/EMNIST datasets consist of two stages: 1) high learning rate training at 0.01 learning rate for 10 epoch and 2) low learning rate training at 0.001 learning rate for 5 epoch.






\subsection{Training with Background Class}
We have generated a common background class for all of the investigated color image classification datasets. This work also classifies several handwritten letter and digit datasets. Handwritten letter and digit datasets contain grey-scale images. We use characters of one language as the background images for classifying characters of another language.

\subsubsection{Background Class for Color Images}
To develop the background class, we investigate background objects of different images. In this study, we investigate STL-10, Oxford-102, CUB-200-2011, and FGVC Aircraft datasets. Images usually contain land, sky, trees, grass, etc. There exist many other objects, and we try to develop the background class from publicly available color image datasets. Fig. \ref{Background} presents a few example images in the background class for color images. The Intel Image Classification dataset is publicly available.  Fig. \ref{Background}(a) presents an image of a forest. That image is collected from the Intel Image Classification\footnote{kaggle.com/datasets/puneet6060/intel-image-classification} dataset. Fig. \ref{Background}(b) presents the color-inverted image of Fig. \ref{Background}(a). We also keep inverted images of many backgrounds for robust model training. The presence of an inverted image can potentially train a more robust model which concentrates on exact textures instead of colors. Moreover, the presence of both original and color-inverted images on the background class can potentially make the domain of the background class larger. Fig. \ref{Background}(c)-(d) present glacier and mountain images, collected from the intel image classification dataset.
Fig. \ref{Background}(e) presents monochromatic background images. Monochromatic background images can potentially reduce color bias on classifications. 

Fig. \ref{Background}(f) presents an image of grass from the GrassClover Dataset\footnote{kaggle.com/datasets/usharengaraju/grassclover-dataset}. GrassClover Dataset contains high-resolution images and a few images contain flowers. We generate a few 512512 sized flower-free grass images from this dataset by resizing or cropping.
Fig. \ref{Background}(g) presents a satellite image from a dataset\footnote{kaggle.com/competitions/understanding\_cloud\_organization}. The image contains a cloud pattern at a location at a time. We extract multiple images from such satellite images by cropping and resizing and saving them as background images.  Fig. \ref{Background}(h) presents images of Galaxies, collected from Galaxy Zoo dataset\footnote{kaggle.com/datasets/jaimetrickz/galaxy-zoo-2-images}.
Fig. \ref{Background}(i) presents water body from another satellite image dataset\footnote{kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies}, and Fig. \ref{Background}(j) presents a wood texture from another dataset\footnote{kaggle.com/datasets/edhenrivi/wood-samples}. We took several images from each dataset to construct the background class. 

\subsubsection{Background Class for Greyscale Images}
We investigate our proposed method on several handwritten digits and characters datasets. These datasets contain grey-scale images. Color images are of different types. Therefore, performing multitask learning or training color and grey images together can potentially degrade performance. Therefore, we observe both English and Japanese characters. KMNIST and EMNIST datasets have no common pattern. Therefore, we use KMNIST data as the background while training EMNIST datasets. Moreover, we use EMNIST data as the background while training KMNIST datasets. The EMNIST-Balanced dataset is much larger than KMNIST. Therefore, we choose 500 samples from each class of the EMNIST-Balanced dataset as images of the background class while training models on KMNIST.


Training parameters with the background class are the same as the training parameters with transfer learning and multitask learning. We keep the same learning rate, epoch number, optimizer, etc. Table \ref{Tclass} presents classification accuracies on different datasets with different training conditions. 

The proposed method provides superior performance on STL-10, KMNIST, and EMNIST datasets. Performance is slightly lower compared to other investigated methods in fine-grained image classification datasets. We also perform deep feature factorization for STL-10 test images. The target class covered 36.8\% of areas on average for the traditional transfer learning method. The majority portion of the image is misclassified in most situations. The white portion of the background area is classified as an airplane, and the green portion of the background is classified as deer in many images. The target class covered 45.9\% of the areas on average in the proposed method. Although the majority portion of the image is still misclassified, a larger area gets the proper classification. Overall accuracy improvement also indicates uncertainty reduction.

\subsubsection{Training with Transformer}
Transformer architectures have received recent SOTA performances with significant improvement over traditional CNNs \cite{ranftl2021vision, fan2021multiscale}. Therefore, we investigate transformers on several classification datasets and obtain state-of-the-art (SOTA) performance in most situations. As background classes do not contain any target objects of newly investigated datasets, we apply the same background class. The background class we developed for color images in Table \ref{Tclass}. Table \ref{TTransformer} presents classification results while using the ViT‑L/16 transformer. We investigate the ViT‑L/16 model independently, with the SpinalNet fully connected (Spinal FC) layer \cite{kabir2022spinalnet}, and with the proposed background class. 
We have used the same augmentations with the transformer. However, the investigated transformer model takes 224224 sized images. Therefore we ensured that the augmented image meets the size criteria of the transformer. 
We receive SOTA performance on STL-10, Caltech-101, and CINIC-10 datasets. We have uploaded scripts on GitHub. We demonstrated SOTA performances on Kaggle servers. We have provided links to Kaggle notebooks on the GitHub repository.



\subsection{Discussion}
According to discussions with colleagues and reviewers' concern, the current process of background class generation requires manual labor. Finding datasets containing potential background images is not laborious. However, we often need to check whether an image contains the pattern of a target class or not. The requirement for human labor becomes high when the number of background class images is high. Future researchers can potentially extract background images from datasets where all objects in images are previously labeled. For example, the cityscapes dataset \cite{cordts2016cityscapes}. A script can select images based on labels. Background image class of the bicycle classification problem can potentially contain all images from the cityscapes dataset; except images containing bicycles. 

Although we apply both traditional CNNs and transformers, we obtain CAM from only CNNs. There is no well-established method to obtain CAM from transformers. Recently proposed methods haven't received popularity yet \cite{li2023transcam, qiang2022attcat}. Therefore, could not obtain statistical results of deep feature factorization using transformers. Future researchers can potentially investigate the effect of background class with CAM on transformers.

\section{Conclusion}
In this work, we have proposed background classes to reduce class activation uncertainty without significantly increasing the training time. Our theoretical study has also indicated an improved generalization while using an optimal background class. We have written a short methodology for developing background classes. According to our experiments on several publicly available datasets, we have statistically received superior performance while using the background class. We have received significant SOTA or near-SOTA performances in several datasets by applying both the ViT‑L/16 transformer and the background class. 


\bibliography{main.bib}
\bibliographystyle{icml2021}


\if 0
\appendix
\section{Do \emph{not} have an appendix here}

\textbf{\emph{Do not put content after the references.}}
Put anything that you might normally include after the references in a separate
supplementary file.

We recommend that you build supplementary material in a separate document.
If you must create one PDF and cut it up, please be careful to use a tool that
doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
pdftk usually works fine. 

\textbf{Please do not use Apple's preview to cut off supplementary material.} In
previous years it has altered margins, and created headaches at the camera-ready
stage.

\fi



\end{document}
