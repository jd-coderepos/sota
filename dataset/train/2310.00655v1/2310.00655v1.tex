
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage{enumitem}


\title{PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting}



\author{Zeying Gong, Yujin Tang \& Junwei Liang\thanks{Corresponding author} \\
Hong Kong University of Science and Technology (Guangzhou)\\
\texttt{zgong313@connect.hkust-gz.edu.cn}\\
\texttt{\{yujintang,junweiliang\}@hkust-gz.edu.cn} 
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 

\begin{document}


\maketitle

\begin{abstract}




Although the Transformer has been the dominant architecture for time series forecasting tasks in recent years, a fundamental challenge remains: the permutation-invariant self-attention mechanism within Transformers leads to a loss of temporal information. 
To tackle these challenges, we propose PatchMixer, a novel CNN-based model. It introduces a permutation-variant convolutional structure to preserve temporal information. Diverging from conventional CNNs in this field, which often employ multiple scales or numerous branches, our method relies exclusively on depthwise separable convolutions. This allows us to extract both local features and global correlations using a single-scale architecture. Furthermore, we employ dual forecasting heads that encompass both linear and nonlinear components to better model future curve trends and details.
Our experimental results on seven time-series forecasting benchmarks indicate that compared with the state-of-the-art method and the best-performing CNN, PatchMixer yields $3.9\%$ and $21.2\%$ relative improvements, respectively, while being 2-3x faster than the most advanced method.
We will release our code and model.

\end{abstract}

\section{Introduction}

Long-term time series forecasting (LTSF) is a crucial task aimed at predicting future trends over an extended period by leveraging substantial historical time-series data.
LTSF applications span a wide range of domains, including traffic flow estimation, energy management, and financial investment.



Transformer \citep{transformer} has been the dominant architecture in time series forecasting tasks in the last few years. 
It was first applied in the field of Natural Language Processing (NLP) and later extended as a universal architecture to the field of Computer Vision (CV) and so on. To address the limitations of the vanilla Transformer models, such as quadratic time or memory complexity, Informer \citep{informer} introduced an innovative Transformer architecture with reduced complexity.Subsequently, numerous Transformer variants \citep{autoformer, fedformer, pyraformer} emerged in the field of time series analysis, to enhance performance or improve computational efficiency.

However, the effectiveness of Transformers in LTSF tasks has been called into question by an experiment involving simple Multi-Layer Perception (MLP) networks \citep{dlinear}, which surprisingly surpassed the forecasting performance of all previous Transformer models. 
Therefore, they posed an intriguing question: Are Transformers effective for long-term time series forecasting? 
In response to this, a Transformer-based model, PatchTST \citep{patchtst}, used a patch-based technique motivated by CV and reached state-of-the-art (SOTA) prediction results. 
Recent transformers \citep{crossformer, petformer} also adopted patch-based representations and achieved noteworthy performance. This naturally gives rise to another important question: Does the impressive performance of PatchTST primarily stem from the inherent power of the Transformer architecture, or is it, at least in part, attributed to the use of patches as the input representation?


In this paper, we address this issue by introducing a novel backbone architecture called \textbf{PatchMixer}, which is based on Convolutional Neural Networks (CNNs). PatchMixer is primarily composed of two convolutional layers and two forecasting heads. 
Its distinguishing feature is the ``patch-mixing'' design, which means the model initially segments the input time series into smaller temporal patches and subsequently integrates information from both within and between these patches. Motivated by the multi-head attention mechanism in Transformer, we employ the dual forecasting heads design in our model. These enhancements enable PatchMixer to outperform other CNN-based models, leading to state-of-the-art accuracy in time series forecasting.


The main contributions of this work are as follows:

\vspace{-5pt}
\begin{itemize}
  \item We propose PatchMixer, a novel model built on convolutional architecture. This approach efficiently replaces the computation-expensive self-attention module in Transformers while leveraging a novel patch-mixing design to uncover intricate temporal patterns in the time series.
  \item PatchMixer is efficient in long-term time series forecasting. By adopting a single-scale structure and optimizing patch representations, our model achieves a significant performance boost. It is 3x faster for inference and 2x faster during training compared to the SOTA model. 
  \item  On seven popular long-term forecasting benchmarks, our PatchMixer outperforms the
SOTA method by $\mathbf{3.9\%}$ on MSE and $\mathbf{3.0\%}$ on MAE. Besides, our model achieves a substantial $\mathbf{21.2\%}$ relative reduction in MSE and $\mathbf{12.5\%}$ relative reduction in MAE on average compared with the previous best CNN model.
\end{itemize}



\section{Related Work}
\label{sec::related_work}
\textbf{CNNs in LTSF.}  Both CNN and Transformer serve as mainstream models in the CV domain. However, for LTSF tasks, the Transformer holds a dominant position. This is primarily due to the limitations imposed on CNN by the receptive field size. CNN-based approaches typically employ a local perspective, extending their perception field to the entire input space through the successive stacking of convolutional layers. For instance, TCN \citep{tcn} first introduced CNN structure into TSF tasks, using multi-layer causal convolution and dilated convolution to model temporal causal relationships and expanding receptive fields. Subsequent SCINet \citep{scinet} iteratively obtained information at different time resolutions using a multi-layer binary tree structure. Recently, MICN \citep{micn} adopted multi-scale hybrid decomposition and isometric convolution for feature extraction from both local and global perspectives. At the same time, TimesNet \citep{timesnet} used the Fast Fourier Transform (FFT) algorithm to convert the 1D series into the 2D tensor so it can use visual backbones such as Inception \citep{inceptionv1} to capture temporal patterns.  



\textbf{Depthwise Separable Convolution.} This is a widely employed technique used in the field of computer vision. 
The work of depthwise separable convolutions was initially unveiled \citep{van2018learning} in 2014. Later, this method was used as the first layer of Inception V1 \citep{inceptionv1} and Inception V2 \citep{inceptionv2}. During the same period, Google introduced an efficient mobile model, called MobileNets \citep{mobilenets}. Its core layers were built on depthwise separable filters. Consequently, the Xception \citep{xception} network demonstrated how to scale up depthwise separable filters. Recently, ConvMixer \citep{convmixer} via the method suggested the patch representation itself may be a critical component to the ``superior'' performance in CV tasks. 



\textbf{Channel Independence.} A multivariate time series can be seen as a signal with multiple channels. When the input tokens take the vector of all time series features and project it to the embedding space to mix information, it is called ``channel mixing''. ``channel independence'' is exactly the opposite. Intuitively, the correlation among variables may help improve prediction accuracy. \citet{dlinear} used this strategy for the first time in the LTSF field, and its effectiveness was further verified in \citet{patchtst}. These two studies have shown that strategies emphasizing channel independence are more effective than channel mixing methods for forecasting tasks. Therefore, we adopt a channel-independent approach instead of a channel-mixing design. Furthermore, motivated by this concept, we explore correlations between and within patches of each univariate time series, which aligns with the idea of ``patch mixing''.



\section{Proposed Method}

\subsection{Problem Formulation}

In this work, we address the following task: Given a set of multivariate time series instances with a historical look-back window $L: (\vx_1, \ldots, \vx_L)$, where each $\vx_t$ at time step $t$ represents a vector of $M$ variables. Our objective is to make predictions for the subsequent $T$ time steps, resulting in the prediction sequence $(\vx_{L+1}, \ldots, \vx_{L+T})$.

From the perspective of channel independence, the multivariate time series $(\vx_1, ..., \vx_L )$ is split to $M$ univariate series $\vx^{(i)} \in \R^{1 \times L}$. We consider the $i$-th univariate series of length $L$ as $\vx^{(i)}_{1:L} = (\vx_1^{(i)},..., \vx_L^{(i)})$ where $i=1,...,M$. These univariate series are independently fed into the model. At last, the networks provide prediction results $\hat{\vx}^{(i)}= (\hat{\vx}^{(i)}_{L+1},..., \hat{\vx}^{(i)}_{L+T}) \in \R^{1 \times T}$ accordingly.

\subsection{Model Structure}

The overall architecture of PatchMixer is illustrated in Figure \ref{fig::model_overview}. We employ a single-scale depthwise separable convolutional block to capture both the global receptive field and local positional features within the input series. We also devise dual forecasting heads, including one linear flatten head and one MLP flatten head. These forecasting heads jointly incorporate nonlinear and linear features to model future sequences independently. The prediction results from the dual heads are subsequently combined to produce the final prediction, denoted as $\hat{\vx}$. Detailed explanations of these components will be provided in the following sections.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.16]{figures/PatchMixer_0928.png}
\end{center}
\caption{PatchMixer overview.}
\label{fig::model_overview}
\end{figure}

\subsection{Patch Embedding}

\noindent \textbf{Comparison of Patch Representation.} Our work compares two different patch modes in recent LTSF research \citep{timesnet, patchtst}. 

\noindent\begin{itemize}
\item{\textbf{Top-k Frequencies:}} For the one-dimensional length-$L$ univariate time series $\mathbf{X}_{\text{1D}} \in\mathbb{R}^{L}$. This approach employs the FFT algorithm to transform the original temporal signal into the frequency domain and extracts top-k prominent frequencies $\{f_{1},\cdots,f_{k}\}$ with the amplitudes $\{\mathbf{A}_{f_{1}},\cdots,\mathbf{A}_{f_{k}}\}$, where $k$ is the hyper-parameter. The purpose of this step is to fold the signal for $k$ times in the time domain, where the period for each fold is determined by the corresponding frequency. Finally, we obtain $k$ sets of periodic patches $\mathbf{X}_{\text{2D}}$. The process can be summarized as the following equations: 
\begin{equation}\label{equ:fft_for_period}
  \begin{split}
    \mathbf{A} & = \operatorname{Amp}(\operatorname{FFT}(\mathbf{X}_{\text{1D}})),\
    \{f_{1},\cdots,f_{k}\} = \mathop{\arg\mathrm{Topk}}_{f_{\ast}\in\{1,\cdots,[\frac{T}{2}]\}}\left(\mathbf{A}\right)
  \end{split}
\end{equation}
\begin{equation}\label{equ:period}
  \begin{split}
    \operatorname{Period}(\mathbf{\hat{X}}_{1D}^{(i)} ) &= \frac{L}{f_i},\ i\in
    \{1,\cdots, k\} 
  \end{split}
\end{equation}
\begin{equation}\label{equ:reshape}
  \begin{split}
\mathbf{\hat{X}}_{\text{2D}}^{(i)} &= \operatorname{Reshape}\left(\operatorname{ZeroPad}(\mathbf{\hat{X}}_{\text{1D}}^{(i)})\right),\ i\in\{1,\cdots, k\}\\
  \end{split}
\end{equation}

\item{\textbf{Sliding Window:}} This method unfolds the input univariate time series $\mathbf{X}_{\text{1D}} \in\mathbb{R}^{L}$ through a sliding window with the length of $P$ and the step of $S$. Before transformation, it extends the original univariate time series $\mathbf{X}_{\text{1D}}$ by repeating its final value $S$ times. This process results in a series of 2D patches, maintaining their original relative positions. The patching process is illustrated by the following formulas.



\begin{equation}\label{equ:unfold}
  \begin{split}
\mathbf{\hat{X}}_{\text{2D}} &= \operatorname{Unfold}\left(\operatorname{ReplicationPad}(\mathbf{\hat{X}}_{\text{1D}}), \text{\texttt{size=}}P, \text{\texttt{step}=}S\right)\\
  \end{split}
\end{equation}
\end{itemize}



Comparing the two methods, we find that patch-based representations open up the possibility of adopting an approach similar to image processing. This is primarily attributed to the process of dimensionality expansion. Patch-based representations break down sequential time series data into smaller and structured segments, resembling image patches. The expansion into a 2D format introduces spatial considerations, aligning with the nature of image data and enabling the utilization of convolutional operations, which are well-suited for capturing local patterns and global relationships in the time series. 

Notably, we note that the first approach tends to prioritize the extraction of periodic patterns and introduces redundancy through zero-padding. This design may inevitably lead to the loss of temporal information, In contrast, the second approach preserves the original relative order of the data without altering or omitting any elements. Therefore, we have chosen to adopt the latter method with default settings, specifically $P = 16$ and $S = 8$. This configuration results in a series of patches with a half-overlap between each patch.
 




\noindent \textbf{Embedding without Positional Encoding.}  Local positional information, signifying the temporal order of time series data, holds significant importance. However, the self-attention layer within the Transformer architecture is unable to inherently preserve this positional information. To augment the temporal context of time series inputs, traditional Transformer models such as Informer \citep{informer}, Autoformer \citep{autoformer}, and FEDformer \citep{fedformer} employ three types of input embeddings. This process is depicted in Equation \ref{equ:emb1}, where \textit{TFE} represents temporal feature encoding (for example, MinuteOfHour, HourOfDay, DayOfWeek, DayOfMonth, and MonthOfYear), \textit{PE} represents position embedding, and \textit{VE} represents value embedding.
 \begin{equation}\label{equ:emb1}
  \begin{split}
\operatorname{Embedding}(\mathbf{X})=\operatorname{sum}(TFE + PE +VE): x^{L} \rightarrow x^{D}\\
  \end{split}
\end{equation}
 

Recent Transformers like PatchTST treat a patch as an input unit, eliminating the need for temporal feature encoding. Instead, they focus on capturing comprehensive semantic information that is not readily available at the point level. This is achieved by aggregating timesteps into subseries-level patches.

 \begin{equation}\label{equ:emb1}
  \begin{split}
\operatorname{Embedding}(\mathbf{X})=\operatorname{sum}(PE +VE): x^{N \times S} \rightarrow x^{N \times D}\\
  \end{split}
\end{equation}

Unlike the Transformer, the CNN structure inherently possesses permutation variance, obviating the necessity of using position embedding in our model. Ultimately, our embedding can be represented by the following formula \ref{equ:emb2}, which can be accomplished with a single linear layer.

 \begin{equation}\label{equ:emb2}
  \begin{split}
\operatorname{Embedding}(\mathbf{X})=VE: x^{N \times S} \rightarrow x^{N \times D}\\
  \end{split}
\end{equation}



\subsection{PatchMixer Layer}

As discussed in Section \ref{sec::related_work}, previous CNNs in LTSF often modeled global relationships within time series data across multiple scales or numerous branches. In contrast, our Patchmixer employs single-scale depthwise separable convolution as the core module. The patch-mixing design separates the per-location (intra-patch) operations with depthwise convolution, and cross-location (inter-patch) operations with pointwise convolution, which allows our model to capture both the global receptive field and local positional features within the input series.

\textbf{Depthwise Convolution:} We use a specific type of grouped convolution where the number of groups equals the number of patches, denoted as $N$. To expand the receptive field, we employ a larger kernel size, typically equal to our default patch step, $S$, resulting in $K=8$. In this process, each of the $N$ patches in the input feature map undergoes a separate convolution with one kernel. This operation generates $N$ feature maps, each corresponding to a specific patch. These feature maps are then concatenated sequentially to create an output feature map with $N$ channels. Depthwise convolution effectively employs group convolution kernels that are the same for patches sharing the same spatial locations. This allows the model to capture potential periodic patterns within the temporal patches. The following equation \ref{eq_depthconv} shows the process of one univariate series $x^{N \times D}$ in layer $l-1$ passing through the depthwise convolution kernel in layer $l$.
\begin{equation}
	x^{N \times D}_{l} = \operatorname{BatchNorm}\left(\sigma\{\text{\texttt{Conv}}_{_{N \to N}}(x^{N \times D}_{l-1}, \text{\texttt{stride=}}K, \text{\texttt{kernel\_size}=}K)\}\right)
\label{eq_depthconv}
\end{equation}
\textbf{Pointwise Convolution:} Our depthwise convolutional operation may not capture the inter-patch feature correlations effectively, which is why we follow it with pointwise convolution. Through this layer, we achieve temporal interaction between patches. 
\begin{equation}
	x^{N \times D}_{l} = \operatorname{BatchNorm}\left(\sigma\{\text{\texttt{ConvDepthwise}}(x^{N \times D}_{l-1}\}\right) + x^{N \times D}_{l-1}
\end{equation}
\begin{equation}
	x^{A \times D}_{l+1} = \operatorname{BatchNorm}\left(\sigma\{\text{\texttt{Conv}}_{_{N \to A}}(x^{N \times D}_{l}, \text{\texttt{stride=}}1, \text{\texttt{kernel\_size}=}1)\}\right)
\end{equation}
The above equations \ref{eq_depthconv} demonstrate the process of the univariate series $x^{N \times D}$ in layer $l$ passing through the pointwise convolution kernel in layer $l+1$, where $A$ means the number of output channels in pointwise convolution. Following each of these convolution operations, we apply an activation function and post-activation BatchNorm. In this context, $\sigma$ denotes an element-wise nonlinearity activation function. For our work, we employ the GELU activation function as described in reference \citep{gelu}.

We demonstrate the effectiveness of the separable convolution method, surpassing the attention mechanism and achieving superior overall performance compared to conventional convolution. The details of the experiments are presented in Section \ref{sec::ablation}. Additionally, pointwise convolution allows us to control the degree of information aggregation among patches by adjusting the number of output channels $A$, as illustrated in Figure \ref{fig::patch_agg}. We delve into this character further in the Appendix \ref{table::Patch Aggregation Analysis}. 



\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{figures/patch_agg.png}
\end{center}
\caption{Patch Aggregation and Patch Disaggregation via Pointwise Convolution.}
\label{fig::patch_agg}
\end{figure}

\subsection{Dual Forecasting Heads}
Previous LTSF methods often followed a paradigm of decomposing inputs, such as employing the seasonal-trend decomposition technique and combining the decomposed components to obtain prediction results. Similarly, the multi-head attention mechanism in Transformers also involves decomposing and aggregating multiple outputs. 

Motivated by the above instances, we propose a novel dual-head mechanism based on the decomposition-aggregation concept, one is dedicated to capturing linear features and the other focuses on capturing nonlinear variations. Specifically, PatchMixer extracts the overall trend of temporal changes through a linear residual connection spanning the convolution, and it uses an MLP forecasting head after a fully convolutional layer with a nonlinear function to meticulously fit the tiny changes in the prediction curve. Finally, we can derive the prediction results by summing their respective outputs. The utilization of dual heads yields a more effective mapping effect in comparison to the direct utilization of the previous single linear flattening head. We confirmed in Section \ref{sec::ablation} that both forecasting heads are indispensable for accurate prediction.

\subsection{Normalization and Loss Fuction}

\textbf{Instance Normalization.} This technique has recently been proposed to help mitigate the distribution shift effect between the training and testing data \citep{instance,revin}. It simply normalizes each time series instance $\vx^{(i)}$ with zero mean and unit standard deviation. In essence, we normalize each $\vx^{(i)}$ before patching and the mean and deviation are added back to the output after dual forecasting heads.   



\textbf{Loss Function.} Here we combine Mean Squared Error (MSE) and Mean Absolute Error (MAE) in an equal 1:1 ratio as our loss function. Surprisingly, we find that this simple method achieves superior accuracy overall, striking a balance between achieving lower MSE and MAE. The experimental details can be seen in Appendix \ref{sec::more_ablation}. 

The MSE loss is: 
\begin{equation}
\mathcal{L_{MSE}} = \frac{1}{M}\sum_{i=1}^M \| \hat{\vx}^{(i)}_{L+1:L+T} - \vx^{(i)}_{L+1:L+T} \|_2^2,
\end{equation}
while the MAE loss is: 
\begin{equation}
\mathcal{L_{MAE}} = \frac{1}{M}\sum_{i=1}^M \| \hat{\vx}^{(i)}_{L+1:L+T} - \vx^{(i)}_{L+1:L+T} \|.
\end{equation}

\section{Experiments}
\subsection{Multivariate Long-term Forecasting}
\label{subsection::time series forecasting}



\textbf{Datasets.} We evaluate the performance of our proposed PatchMixer on $7$ commonly used long-term forecasting benchmark datasets: Weather, Traffic, Electricity, and 4 ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). The statistics of those datasets are summarized in Appendix \ref{sec::datasets}. It should be noted that ETTh1 and ETTh2 are small datasets, while ETTm1, ETTm2, and Weather are medium datasets. Traffic and Electricity each have more than 800 and 300 variables, respectively, with each variable containing tens of thousands of time points, naturally categorizing them as large datasets. Generally, smaller datasets contain more noise, while larger datasets exhibit more stable data distributions.

\textbf{Baselines and metrics.} We choose SOTA and representative LTSF models as our baselines, including Transformer-based models like PatchTST \citeyearpar{patchtst}, FEDformer \citeyearpar{fedformer}, Autoformer \citeyearpar{autoformer}, Informer \citeyearpar{informer}, in addition to two CNN-based models containing MICN \citeyearpar{micn} and TimesNet \citeyearpar{timesnet}, with the significant MLP-based model DLinear \citeyearpar{dlinear} to served as our baselines. To assess the performance of these models, we employ widely used evaluation metrics: MSE and MAE. The details of each baseline are described in Appendix \ref{append::baselines}.  



\textbf{Results.} Table \ref{tab::multivariate} shows the multivariate long-term forecasting results. Our model outperforms all baseline methods significantly in all largest benchmarks, containing Traffic, Electricity, and Weather. On other datasets, we achieve the best performance across all or most prediction lengths on other datasets. Quantitatively, PatchMixer demonstrates an overall relative reduction of $\mathbf{3.9\%}$ on MSE and $\mathbf{3.0\%}$ on MAE in comparison to the state-of-the-art Transformer (PatchTST). When evaluated against the best-performing MLP-based model (DLinear), our model showcases an overall decline of $\mathbf{11.6\%}$ on MSE and $\mathbf{9.4\%}$ on MAE. Moreover, in comparison to the achievable outcomes with the best CNN-based model (TimesNet), we demonstrate a remarkable overall relative reduction of $\mathbf{21.2\%}$ on MSE and $\mathbf{12.5\%}$ on MAE.

\linespread{1.2}
\begin{table}[ht]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|cc|cc|cc|ccc}
			\cline{2-19}
&\multicolumn{2}{c|}{Models} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{PatchMixer}\\ \textbf{(Ours)}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}PatchTST\\ \citeyearpar{patchtst}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}DLinear\\ \citeyearpar{dlinear}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}MICN\\ \citeyearpar{micn}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}TimesNet\\ \citeyearpar{timesnet}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}FEDformer\\ \citeyearpar{fedformer}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Autoformer\\ \citeyearpar{autoformer}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Informer\\ \citeyearpar{informer}\end{tabular}} \\ \cline{2-19}
&\multicolumn{2}{c|}{Metrics}                  &MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & \textbf{0.151} & \textbf{0.193} & \uline{0.152} & \uline{0.199}                                              & 0.176                                                    & 0.237  & 0.172 & 0.240                                                   & 0.165                                                    & 0.222                                                    & 0.238                                                     & 0.314                                                    & 0.249                                                     & 0.329                                                     & 0.354                                                    & 0.405                                                     \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.194} & \textbf{0.236} & \uline{0.197} & \uline{0.243}                                                    & 0.220                                                    & 0.282 & 0.218                                                    & 0.281                                                         & 0.215                                                    & 0.264                                              & 0.275                                                     & 0.329                                                    & 0.325                                                     & 0.370                                                     & 0.419                                                    & 0.434                                                     \\
            &\multicolumn{1}{c|}{}& 336  & \textbf{0.225} & \textbf{0.267} & \uline{0.249} & \uline{0.283}                                                    & 0.265                                                    & 0.319 & 0.275                                                    & 0.329                                                      & 0.274                                                    & 0.304                                                 & 0.339                                                     & 0.377                                                    & 0.351                                                     & 0.391                                                     & 0.583                                                    & 0.543                                                    \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.305} & \textbf{0.323} & \uline{0.320} & \uline{0.335} & 0.323                                                    & 0.362 & 0.314                                                    & 0.354                                                     & 0.339                                                    & 0.349                                                  & 0.389                                                     & 0.409                                                    & 0.415                                                     & 0.426                                                     & 0.916                                                    & 0.705                                                   \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96  & \textbf{0.363} & \textbf{0.245} & \uline{0.367} & \uline{0.251}  & 0.410                                                    & 0.282 & 0.479                                                    & 0.295                                                   & 0.593                                                    & 0.321                                                                   & 0.576                                                     & 0.359                                                    & 0.597                                                     & 0.371                                                     & 0.733                                                    & 0.410                                                     \\
            &\multicolumn{1}{c|}{} & 192  & \textbf{0.384} & \textbf{0.254} & \uline{0.385} & \uline{0.259} & 0.423                                                    & 0.287                                                   & 0.482 & 0.297 & 0.617                                                    & 0.336                                                    & 0.610                                                     & 0.380                                                    & 0.607                                                     & 0.382                                                     & 0.777                                                    & 0.435                                                    \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.393} & \textbf{0.258} & \uline{0.398} & \uline{0.265} & 0.436                                                    & 0.296                                                   & 0.492 & 0.297 & 0.629                                                    & 0.336                                                    & 0.608                                                     & 0.375                                                    & 0.623                                                     & 0.387                                                     & 0.776                                                    & 0.434                                                    \\
            &\multicolumn{1}{c|}{}& 720  & \textbf{0.429} & \textbf{0.283} & \uline{0.434} & \uline{0.287} & 0.466                                                    & 0.315                                                   & 0.510 & 0.309 & 0.640                                                    & 0.350                                                    & 0.621                                                     & 0.375                                                    & 0.639                                                     & 0.395                                                     & 0.827                                                    & 0.466                                                     \\
            \cline{2-19}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96   & \textbf{0.129} & \textbf{0.221} & \uline{0.130} & \uline{0.222} & 0.140                                                    & 0.237
   & 0.153                                                    & 0.264 & 0.168                                                    & 0.272                                                      & 0.186                                                     & 0.302                                                    & 0.196                                                     & 0.313                                                     & 0.304                                                    & 0.393                                                   \\
			&\multicolumn{1}{c|}{}& 192 & \textbf{0.144} & \textbf{0.237} & \uline{0.148} & \uline{0.240} & 0.153                                                    & 0.249
   & 0.175                                                    & 0.286 & 0.184                                                    & 0.289                                                    & 0.197                                                     & 0.311                                                    & 0.211                                                     & 0.324                                                     & 0.327                                                    & 0.417                                                    \\
			&\multicolumn{1}{c|}{}& 336  & \textbf{0.164} & \textbf{0.257} & \uline{0.167} & \uline{0.261} & 0.169                                                    & 0.267           & 0.192                                                    & 0.303                                                 & 0.198                                                    & 0.300                                                    & 0.213                                                     & 0.328                                                    & 0.214                                                     & 0.327                                                     & 0.333                                                    & 0.422                                                    \\
			&\multicolumn{1}{c|}{}& 720 & \textbf{0.200} & \textbf{0.289} & \uline{0.202} & \uline{0.291} & 0.203                                                    & 0.301 & 0.215                                                    & 0.323                                                   & 0.220                                                    & 0.320                                                    & 0.233                                                     & 0.344                                                    & 0.236                                                     & 0.342                                                     & 0.351                                                    & 0.427                                                    \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96  & \textbf{0.353} & \textbf{0.381} & \uline{0.375} & \uline{0.399} &  \uline{0.375}                                                    & \uline{0.399} & 0.405                                                    & 0.430                                                   & 0.384                                                    & 0.402                                                    & 0.376                                                     & 0.415                                                    & 0.435                                                     & 0.446                                                     & 0.941                                                    & 0.769                                                    \\
            &\multicolumn{1}{c|}{}& 192  & \textbf{0.373} & \textbf{0.394} & 0.414 & 0.421  & \uline{0.405}                                                    & \uline{0.416}    & 0.447                                                    & 0.468                                                & 0.436                                                    & 0.429                                                    & 0.423                                                     & 0.446                                                    & 0.456                                                     & 0.457                                                     & 1.007                                                    & 0.786                                                 \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.392} & \textbf{0.414} & \uline{0.431} & \uline{0.436} & 0.439                                                    & 0.443  & 0.579                                                    & 0.549                                                  & 0.491                                                    & 0.469                                                    & 0.444                                                     & 0.462                                                    & 0.486                                                     & 0.487                                                     & 1.038                                                    & 0.784                                                    \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.445} & \textbf{0.463} & \uline{0.449} & \uline{0.466}                            & 0.472                                                    & 0.490   & 0.699                                                    & 0.635                                                & 0.521                                                    & 0.500                                                    & 0.469                                                     & 0.492                                                    & 0.515                                                     & 0.517                                                     & 1.144                                                    & 0.857                                                    \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96 & \textbf{0.225} & \textbf{0.300} & \uline{0.274} & \uline{0.336}& 0.289                                                    & 0.353
   & 0.349                                                    & 0.401 & 0.340                                                    & 0.374                                                    & 0.332                                                     & 0.374                                                    & 0.332                                                     & 0.368                                                     & 1.549                                                    & 0.952                                                   \\
            &\multicolumn{1}{c|}{}& 192  & \textbf{0.274} & \textbf{0.334} & \uline{0.339} & \uline{0.379}  & 0.383                                                    & 0.418   & 0.442                                                    & 0.448                                                & 0.402                                                    & 0.414                                                    & 0.407                                                     & 0.446                                                    & 0.426                                                     & 0.434                                                     & 3.792                                                    & 1.542                                                    \\
            &\multicolumn{1}{c|}{}& 336 & \textbf{0.317} & \textbf{0.368} & \uline{0.331} & \uline{0.380}  & 0.480                                                    & 0.465  & 0.652                                                    & 0.569                                                  & 0.452                                                    & 0.452                                                    & 0.400                                                     & 0.471                                                    & 0.477                                                     & 0.479                                                     & 4.215                                                    & 1.642                                                   \\
            &\multicolumn{1}{c|}{}& 720 & \uline{0.393} & \uline{0.426} & \textbf{0.379} & \textbf{0.422} & 0.605                                                    & 0.551  & 0.800                                                   & 0.652                                                   & 0.462                                                    & 0.468                                                    & 0.412                                                     & 0.469                                                    & 0.453                                                     & 0.490                                                     & 3.656                                                    & 1.619                                                     \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96 & \uline{0.291} & \textbf{0.340} & \textbf{0.290} & \uline{0.342} & 0.299                                                    & 0.343   & 0.302                                                    & 0.352                                                 & 0.340                                                    & 0.377                                                    & 0.326                                                     & 0.390                                                    & 0.505                                                     & 0.475                                                     & 0.626                                                    & 0.560                                                     \\
            &\multicolumn{1}{c|}{}& 192 & \textbf{0.325} & \textbf{0.362} & \uline{0.332} & 0.369 & 0.335                                                    & \uline{0.365} & 0.342                                                    & 0.380                                                   & 0.374                                                    & 0.387                                                    & 0.365                                                     & 0.415                                                    & 0.553                                                     & 0.496                                                     & 0.725                                                    & 0.619                                                     \\
            &\multicolumn{1}{c|}{}& 336  & \textbf{0.353} & \textbf{0.382} & \uline{0.366}                                                     &  0.453                                                   & 0.369                                                    & \uline{0.386}  & 0.381                                                    & 0.403                                                  & 0.392                                                    & 0.413                                                    & 0.392                                                     & 0.425                                                    & 0.621                                                     & 0.537                                                     & 1.005                                                    & 0.741                                                   \\
            &\multicolumn{1}{c|}{}& 720  & \textbf{0.413} & \textbf{0.413} & \uline{0.420}                                                     & 0.533                                                   & 0.425                                                    & \uline{0.421} & 0.434                                                    & 0.447                                                  & 0.433                                                    & 0.436                                                    & 0.446                                                     & 0.458                                                    & 0.671                                                     & 0.561                                                     & 1.133                                                    & 0.845                                                  \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTm2}} & 96   & 0.174 & \uline{0.256} & \textbf{0.165} & \textbf{0.255}  & \uline{0.167}                                                    & 0.260  & 0.188                                                    & 0.286                                                  & 0.183                                                    & 0.271                                                    & 0.180                                                     & 0.271                                                    & 0.255                                                     & 0.339                                                     & 0.355                                                    & 0.462                                                    \\
            &\multicolumn{1}{c|}{}& 192  & 0.227 & \uline{0.295} & \textbf{0.220} & \textbf{0.292}  & \uline{0.224}                                                    & 0.303 & 0.236                                                    & 0.320                                                  & 0.242                                                    & 0.309                                                    & 0.252                                                     & 0.318                                                    & 0.281                                                     & 0.340                                                     & 0.595                                                    & 0.586                                                    \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.266} & \textbf{0.323} & \uline{0.278} & \uline{0.329} & 0.281                                                    & 0.342  & 0.295                                                    & 0.355                                                  & 0.304                                                    & 0.348                                                    & 0.324                                                     & 0.364                                                    & 0.339                                                     & 0.372                                                     & 1.270                                                    & 0.871                                                  \\
            &\multicolumn{1}{c|}{}& 720 & \textbf{0.344} & \textbf{0.372} & \uline{0.367} & \uline{0.385}  & 0.397                                                    & 0.421 & 0.422                                                    & 0.445                                                    & 0.385                                                    & 0.400                                                    & 0.410                                                     & 0.420                                                    & 0.433                                                     & 0.432                                                     & 3.001                                                    & 1.267                                                    \\
			\cline{2-19}
		\end{tabular}
	}
	\caption{Multivariate long-term forecasting results with our model PatchMixer. We use prediction lengths $T\in \{96, 192, 336, 720\}$ for all datasets. The best results are in \textbf{bold} and the second best results are in \uline{underlined}.}
	\label{tab::multivariate}
\end{table}
\linespread{1}



\subsection{Ablation Study}
\label{sec::ablation}

\noindent \textbf{Training and Inference Efficiency.} We aim to demonstrate PatchMixer's superior efficiency in training and inference times compared to PatchTST, as shown in Figure \ref{fig::speed}. We conducted experiments using PatchTST's data loader and the ETTm1 dataset with a batch size of 8, resulting in data dimensions of $8 \times 7 \times L$ per batch. We report both inference time per batch and training time per epoch while varying the look-back length from 96 to 2880.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.46]{figures/speed.png}
\end{center}
\caption{Comparison of Training and Inference Times: PatchMixer vs. PatchTST}
\label{fig::speed}
\end{figure}

Our results highlight two key improvements. First, PatchMixer achieves a 3x faster inference and 2x faster training speed compared to PatchTST. Second, PatchTST's performance is highly sensitive to the length of the look-back window, particularly when it reaches or exceeds 1440. In contrast, PatchMixer exhibits fewer fluctuations in both inference and training times with increasing historic length, contributing to higher accuracy and computational efficiency. All experiments in this subsection are conducted on the same machine, utilizing a single GPU RTX4090 for consistent and reliable findings.

\textbf{Depthwise Separable Convolution vs. Self-attention Mechanism, Standard Convolution.} To assess the effectiveness of depthwise separable convolution, we replace the module in PatchMixer with the transformer encoder of PatchTST and standard convolution separably. Each uses one layer and follows the same configuration. 

\linespread{1.2}
\begin{table}[h]
\centering
	\resizebox{\linewidth}{!}{
\begin{tabular}{clccccccccccccc}
\hline
\multicolumn{3}{c}{Datasets}                                                                                                                           & \multicolumn{4}{c}{Traffic} & \multicolumn{4}{c}{ETTm1} & \multicolumn{4}{c}{ETTh1} \\  \cmidrule(r){4-7}  \cmidrule(r){8-11} \cmidrule(r){12-15} 
\multicolumn{3}{c}{Prediction Length T}                                                                                                                  & 96   & 192   & 336   & 720  & 96   & 192  & 336  & 720  & 96   & 192  & 336  & 720  \\ \hline
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}PatchMixer -\\ Attention Mechanism\end{tabular}}}             & \multicolumn{1}{c|}{MSE}   &   0.368   &  0.388    &  0.401 & 0.447 &   0.294   &   0.331   &  0.360    &  0.422  &  \uline{0.355} &  0.378 & \uline{0.393} & 0.451        \\
\multicolumn{2}{c|}{}                                                                                                     & \multicolumn{1}{c|}{MAE}   &   \textbf{0.240}   &  \textbf{0.249}     &  \textbf{0.256}    & \textbf{0.273}  &  \uline{0.340}   &   0.365   &   0.386   &  0.417   & \uline{0.382} &  0.397 & \uline{0.411} & \textbf{0.460}     \\ \hline
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}PatchMixer -\\ Standard Convolution\end{tabular}}}               & \multicolumn{1}{c|}{MSE}   &   \uline{0.366}   &  \uline{0.383}     &  \uline{0.393}     &  \textbf{0.426}  &  \textbf{0.290}    &  \textbf{0.324}    &  \uline{0.355}    &   \textbf{0.410}  & \textbf{0.353} &  \textbf{0.372} &  0.400 & \textbf{0.443}      \\
\multicolumn{2}{c|}{}                                                                                                     & \multicolumn{1}{c|}{MAE}   &   0.247   &   0.253    &   0.258    &   \uline{0.279}  & \textbf{0.339} &  \textbf{0.361}    &  \textbf{0.382}    &  \uline{0.414}   & \textbf{0.381} &  \uline{0.392} &  \uline{0.425} & \uline{0.462}         \\ \hline
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}PatchMixer -\\ Separable Convolution\end{tabular}}} & \multicolumn{1}{c|}{MSE}   &  \textbf{0.362}    &  \textbf{0.382}   &  \textbf{0.392}   &  \uline{0.428} &  \textbf{0.290}   &   \uline{0.325} & \textbf{0.353}   &  \uline{0.413}   & \uline{0.355} &  \uline{0.373} &  \textbf{0.391} & \uline{0.446}     \\
\multicolumn{2}{c|}{}                                                                                                     & \multicolumn{1}{c|}{MAE}   &  \uline{0.242}  &  \uline{0.252}    &  \uline{0.257}    &   0.282 &  \uline{0.340}   &   \textbf{0.361}  &   \textbf{0.382}   &  \textbf{0.413} & 0.383 &  \textbf{0.394} &  \textbf{0.410} & 0.463     \\ \hline

\end{tabular}} \label{tab::dsc_vs_attention}
	\caption{Ablation of depthwise separable convolution in the Traffic, ETTm1, and ETTm2 datasets. We replace the convolutional module with a transformer encoder in PatchTST and standard convolution. The better results are highlighted in \textbf{bold} and the second best results are in \uline{underlined}.}
\end{table}
\linespread{1}

The results are shown in Table  \ref{tab::dsc_vs_attention}, which implies that convolutional layers outperform attention layers in the majority of cases. The results of depthwise separable convolution are close to those of standard convolution, whereas standard convolution achieves its best results primarily in small and medium-sized datasets. In contrast, the superior predictive performance of separable convolution is evenly distributed across datasets of various sizes.

\noindent \textbf{Dual Forecasting Heads.} We use a single Linear Flatten Head as a baseline. In Figure \ref{fig::dual_heads}, it is evident that the dual-head mechanism outperforms all other results and is at least comparable to one of the output heads within the dual-head setup. This outcome underscores the effectiveness of the dual-head mechanism when compared to a single-layer output head.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{figures/v2_Dual_Heads.png}
\end{center}
\caption{ Ablation study of dual heads. We use prediction lengths $T\in \{96, 192, 336, 720\}$ in three datasets Weather, ETTm1, and ETTm2.}
\label{fig::dual_heads}
\end{figure}

\noindent \textbf{Varying Look-back Window.} In principle, the large receptive field is beneficial for improving performance, while the receptive field of the look-back window in time series analysis is also important. Generally speaking, a powerful LTSF model with a strong temporal relation extraction capability should be able to achieve better results with longer input historical sequences. However, as argued in \citet{dlinear}, this phenomenon has not been observed in most of the Transformer-based models. We also demonstrate in Figure \ref{fig::varying_lookback} that in most cases, these Transformer-based baselines except PatchTST have not benefited from longer look-back window $L$, which indicates their ineffectiveness in capturing long-term temporal information. In contrast, recent baselines such as PatchTST, DLinear, and our PatchMixer consistently reduce the MSE scores as the receptive field increases, which confirms our model's capability to learn from the longer look-back window.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.46]{figures/vary_lookback.png}
\end{center}
\caption[Forecasting performance (MSE) with varying look-back windows on $3$ largest datasets]{Forecasting performance (MSE) with varying look-back windows on $3$ largest datasets: Traffic, Electricity, and Weather. The look-back windows are selected to be $L=24,48,96,192,336,720$, and the prediction horizons are $T=96, 720$. We use our PatchMixer and the baselines for this experiment.\footnotemark}
\label{fig::varying_lookback}
\end{figure}

\footnotetext{We omit the results of Informer because its performance significantly deviated from the other models, which could distort the comparison results.}

\section{Conclusion and Future Work}

In this paper, we introduce PatchMixer, a novel CNN-based model for long-term time series forecasting. PatchMixer leverages depthwise separable convolution with an innovative patch-mixing design to efficiently capture both global and local temporal patterns without self-attention mechanisms. We also highlight the importance of modeling linear and nonlinear components separately through dual forecasting heads, further enhancing our model's predictive capability. Our experiments demonstrate that PatchMixer outperforms state-of-the-art methods in terms of prediction accuracy while being significantly faster in both training and inference. 

While our model has exhibited promising results, there is still potential for improvement, especially in the integration of external temporal features. Long-term time series forecasting often relies on external factors like holidays, weather conditions, or economic indicators. Effectively incorporating these features into patch-based models presents a challenge due to the inherently local nature of patch-based operations. These models tend to focus on individual time points rather than broader periods. We sincerely hope that further research in this direction could lead to more robust forecasting solutions.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Appendix}
\subsection{Experimental Details}
\label{append:exp}

\subsubsection{Datasets}
\label{sec::datasets}
We use $7$ popular multivariate datasets provided in \citep{autoformer} for forecasting. Detailed statistical data on the size of the datasets are as follows.


\begin{table}[htbp!]
\centering
\scalebox{0.85}{
\begin{tabular}{c|ccccccc}
\toprule  
Datasets & Weather  & Traffic & Electricity & ETTh1 & ETTh2 & ETTm1 & ETTm2 \\
\midrule  
Variables & 21 & 862 & 321 & 7 & 7 & 7 & 7 \\
Timesteps & 52696 & 17544 & 26304 & 17420 & 17420 & 69680 & 69680 \\
Frequencies & 10 Minutes & 1 Hour & 1 Hour & 1 Hour & 1 Hour & 15 Minutes & 15 Minutes \\
\bottomrule 
\end{tabular}}
\caption{Statistics of popular datasets used for benchmarking.}
\label{tab::datasets}
\end{table}

\begin{itemize}
\item {Weather:}\footnote{https://www.bgc-jena.mpg.de/wetter/} This dataset collects 21 meteorological indicators in Germany, such as humidity and air temperature. 
\item{Traffic:}\footnote{https://pems.dot.ca.gov/} This dataset records the road occupancy rates from different sensors on San Francisco freeways. \item{Electricity:}\footnote{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014} This is a dataset that describes 321 customers' hourly electricity consumption.
\item{ETT (Electricity Transformer Temperature):}\footnote{https://github.com/zhouhaoyi/ETDataset} These datasets are collected from two different electric transformers labeled with 1 and 2, and each of them contains 2 different resolutions (15 minutes and 1 hour) denoted with m and h. Thus, in total we have 4 ETT datasets: \textit{ETTm1}, \textit{ETTm2}, \textit{ETTh1}, and \textit{ETTh2}. 
\end{itemize}








\subsubsection{Baselines}
\label{append::baselines}

We choose SOTA and the most representative LTSF models as our baselines:
\begin{itemize}
    \item PatchTST \citep{patchtst}: the current SOTA LTSF model as of August 2023. It uses channel-independent and patch techniques and achieves the highest performance by utilizing the vanilla Transformer encoders.
\item DLinear \citep{dlinear}: a highly insightful work that employs simple linear models and trend decomposition techniques, outperforming all Transformer-based models at the time. This work inspired us to reflect on the utility of Transformers in LTSF and indirectly led to the numerous births of MLP-based models in recent studies. 
    \item MICN \citep{micn}: another non-transformer model that enhances the performance of CNN models in LTSF through down-sampled convolution and isometric convolution, outperforming many Transformer-based models. This excellent work has been selected for oral presentation at ICLR 2023.
    
    \item TimesNet \citep{timesnet}: it proposes a task-general backbone for time series analysis and achieves SOTA in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection before DLinear. 

    \item FEDformer \citep{fedformer}: it employs trend decomposition and Fourier transformation techniques to improve the performance of Transformer-based models in LTSF. It was the best-performing Transformer-based model before DLinear.
    \item Autoformer \citep{autoformer}: it combines trend decomposition techniques with an auto-correlation mechanism, inspiring subsequent work such as FEDformer.
    \item Informer \citep{informer}: it proposes improvements to the Transformer model by utilizing a sparse self-attention mechanism and generative-style decoder, inspiring a series of subsequent Transformer-based LTSF models. This work was awarded Best Paper at AAAI 2021.
\end{itemize}

Classical RNN-based and CNN-based models, such as DeepAR \citep{deepar} and LSTnet \citep{lstnet}, have been demonstrated in previous works to be less effective than previous Transformer-based models in LTSF \citep{informer, autoformer}. Therefore, we did not include them in our baselines. We also noted other excellent works recently, such as Crossformer \citep{crossformer}, TiDE \citep{tide}, and MTS-Mixers \citep{mtsmixer}. However, due to limited resources, we could only select the LTSF models that were most relevant to our work and most representative at each stage as our baselines.



The implementation of all baselines is from their respective code repository. We also adopt their default hyper-parameters to train the models to expect look-back windows. It is noted that default look-back windows for different baseline models could be different. For previous models, such as Informer, Autoformer, and FEDformer, the default look-back window is $L=96$; and for recent PatchTST and DLinear, the default look-back window is $L=336$. The reason for this difference is that previous Transformer-based baselines are easy to overfit when the look-back window is long while the latter tend to underfit recent models. However, this can lead to an underestimation of the baselines. 

Meanwhile, PatchTST\citep{patchtst} reports two versions of models, PatchTST/64 for the look-back window $L=512$ and PatchTST/42 for $L=336$. Therefore, to compare the best performance of our model and all baselines, we report $L=336$ for PatchMixer, PatchTST/42 for PatchTST, and the best result in $L={24,48,96,192,336}$ for the other baseline models by default. Thus it could be a strong baseline.



\subsubsection{Implementation Details}
\label{sec::implementation details}

\textbf{Model Parameters.} For all benchmarks, our model contains only $1$ PatchMixer layer and dimension of latent space $D=256$ by default. The MLP head consists of 2 linear layers with GELU \citep{gelu} activation function: one projecting the hidden representation $D= 2 \times T$ for the forecasting length $T$, and another layer that projects it back to the final prediction target $D=T$. The linear head includes a flatten operation and a linear layer aims to project the embed vector directly from $N \times D$ to $T$. Dropout with probability $0.2$ is applied in the patch embedding for all experiments. Our method uses the ADAMw optimizer. The training process will be early stopped after ten epochs if there is no loss degradation on the valid set. All the experiments are repeated 5 times with different seeds, implemented in PyTorch, and conducted on NVIDIA RTX 4090 24GB GPUs. The code will be publicly available.





\subsection{More Results on Ablation Study}
\label{sec::more_ablation}



\textbf{Loss Function.} We study the effects of different loss functions in Table \ref{tab::loss}. We include PatchTST as the SOTA benchmark for the Transformer-based model. By comparing results with MSE, MAE, SmoothL1loss, MSE, and MAE accordingly. The motivation of patching is natural: Since LTSF tasks usually use these two metrics, MSE and MAE, previous time series prediction tasks typically used MSE as the loss function, only a few models \citep{scinet} use MAE as the loss function for training. Recent work has also employed SmoothL1loss \citep{petformer} and we notice that SmoothL1loss is a type of loss function that attempts to combine the advantages of both MSE and MAE. This observation motivates us to explore a multi-task loss approach. 

\linespread{1.1}
\begin{table*}[!htbp]
	\centering
	\scalebox{0.75}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|ccc}
		    \cline{2-13}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{8}{c|}{PatchMixer}&  \multicolumn{2}{c}{PatchTST}& \\
			\cline{4-13}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{MSE+MAE}& \multicolumn{2}{c|}{MSE}& \multicolumn{2}{c|}{MAE}& \multicolumn{2}{c|}{SmoothL1loss} & \multicolumn{2}{c}{MSE}&\\
			\cline{2-13}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96    & \textbf{0.149} & \uline{0.193} & 0.154 & 0.196 & 0.152 & \textbf{0.190} & \uline{0.151} & \uline{0.193} & 0.152 & 0.199 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.191} & \uline{0.233} & 0.197 & 0.237 & \uline{0.193} & \textbf{0.231} & 0.194 & 0.237 & 0.197 & 0.243 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.225} & 0.269 & \textbf{0.225} & \uline{0.267} & 0.227 & \textbf{0.265} & 0.229 & 0.272 & 0.249 & 0.283 \\
            &\multicolumn{1}{c|}{}& 720   & \uline{0.307} & 0.324 & \textbf{0.302} & \uline{0.322} & 0.308 & \textbf{0.321} & 0.309 & 0.326 & 0.320 & 0.335 \\
			\cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96    & \textbf{0.362} & \uline{0.242} & 0.370 & 0.252 & 0.369 & \textbf{0.237} & \uline{0.367} & 0.252 & 0.367 & 0.251 \\
            &\multicolumn{1}{c|}{} & 192   & \textbf{0.382} & \uline{0.252} & 0.388 & 0.258 & 0.388 & \textbf{0.244} & 0.386 & 0.256 & \uline{0.385} & 0.259 \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.392} & \uline{0.257} & 0.400 & 0.266 & \uline{0.398} & \textbf{0.246} & 0.400 & 0.267 & 0.398 & 0.265 \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.428} & \uline{0.282} & 0.436 & 0.288 & \uline{0.429} & \textbf{0.266} & 0.435 & 0.290 & 0.434 & 0.287 \\
            \cline{2-13}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & \textbf{0.128} & \uline{0.221} & \textbf{0.128} & \uline{0.221} & \textbf{0.128} & \textbf{0.217} & 0.130 & 0.224 & 0.130 & 0.222 \\
			&\multicolumn{1}{c|}{}& 192   & 0.144 & 0.237 & \textbf{0.142} & \uline{0.236} & \uline{0.143} & \textbf{0.233} & 0.145 & 0.240 & 0.148 & 0.240 \\
			&\multicolumn{1}{c|}{}& 336   & 0.164 & 0.257 & \uline{0.163} & \uline{0.255} & \textbf{0.162} & \textbf{0.252} & 0.166 & 0.260 & 0.167 & 0.261 \\
			&\multicolumn{1}{c|}{}& 720   & 0.201 & 0.290 & \textbf{0.199} & \uline{0.289} & \textbf{0.199} & \textbf{0.284} & 0.204 & 0.293 & 0.202 & 0.291 \\
			\cline{2-13}
   			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96    & 0.355 & \uline{0.383} & \uline{0.354} & 0.384 & \textbf{0.353} & 0.379 & 0.356 & 0.384 & 0.375 & 0.399 \\
			&\multicolumn{1}{c|}{}& 192   & \textbf{0.373} & \uline{0.394} & 0.376 & 0.397 & 0.376 & \textbf{0.392} & \uline{0.375} & \uline{0.394} & 0.414 & 0.421 \\
			&\multicolumn{1}{c|}{}& 336   & \textbf{0.391} & \textbf{0.410} & 0.397 & 0.421 & 0.396 & \textbf{0.410} & \uline{0.394} & 0.411 & 0.431 & 0.436 \\
			&\multicolumn{1}{c|}{}& 720   & 0.446 & 0.463 & 0.446 & \uline{0.462} & \textbf{0.437} & \textbf{0.450} & \uline{0.444} & \uline{0.462} & 0.449 & 0.466 \\
			\cline{2-13}
   			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & \textbf{0.220} & \uline{0.298} & 0.226 & 0.300 & 0.224 & \textbf{0.296} & \uline{0.222} & \uline{0.298} & 0.274 & 0.336 \\
			&\multicolumn{1}{c|}{}& 192   & \textbf{0.267} & \uline{0.332} & 0.276 & 0.335 & 0.272 & \textbf{0.331} & \uline{0.270} & 0.333 & 0.339 & 0.379 \\
			&\multicolumn{1}{c|}{}& 336   & \textbf{0.304} & \textbf{0.363} & 0.319 & 0.368 & 0.311 & \uline{0.364} & \uline{0.307} & \uline{0.364} & 0.331 & 0.380 \\
			&\multicolumn{1}{c|}{}& 720   & \textbf{0.375} & \uline{0.417} & 0.395 & 0.427 & 0.380 & \textbf{0.416} & \uline{0.377} & \uline{0.417} & 0.379 & 0.422 \\
			\cline{2-13}
   			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & \uline{0.290} & 0.340 & 0.292 & 0.341 & \uline{0.290} & \textbf{0.334} & \textbf{0.289} & \uline{0.339} & 0.290 & 0.342 \\
			&\multicolumn{1}{c|}{}& 192   & \textbf{0.325} & \uline{0.361} & \uline{0.326} & 0.362 & 0.328 & \textbf{0.357} & 0.327 & 0.362 & 0.332 & 0.369 \\
			&\multicolumn{1}{c|}{}& 336   & \textbf{0.353} & \uline{0.382} & \uline{0.354} & \uline{0.382} & 0.355 & \textbf{0.377} & 0.355 & 0.382 & 0.366 & 0.392 \\
			&\multicolumn{1}{c|}{}& 720   & \textbf{0.413} & \uline{0.413} & 0.417 & \uline{0.413} & \uline{0.415} & \textbf{0.409} & 0.416 & \uline{0.413} & 0.420 & 0.424 \\
			\cline{2-13}
   			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96 & \textbf{0.164} & \uline{0.251} & 0.168 & 0.253 & 0.165 & \textbf{0.249} & \textbf{0.164} & \uline{0.251} & 0.165 & 0.255 \\
			&\multicolumn{1}{c|}{}& 192   & 0.220 & 0.291 & 0.224 & 0.291 & \textbf{0.219} & \textbf{0.285} & \textbf{0.219} & \uline{0.289} & 0.220 & 0.292 \\
			&\multicolumn{1}{c|}{}& 336   & \uline{0.264} & 0.322 & 0.265 & 0.320 & 0.265 & \uline{0.318} & \textbf{0.261} & \textbf{0.317} & 0.278 & 0.329 \\
			&\multicolumn{1}{c|}{}& 720   & \textbf{0.342} & 0.375 & \uline{0.343} & \textbf{0.370} & 0.347 & \textbf{0.370} & 0.345 & 0.371 & 0.367 & 0.385 \\
			\cline{2-13}
		\end{tabular}
	}
	\caption{Ablation study of loss functions for training in PatchMixer. 4 cases are included: (a) both MSE and MAE are included in loss function; (b) MSE; (c) MAE; (d) SmoothL1loss. The best results are in \textbf{bold} and the second best are in \uline{underlined}. }
	\label{tab::loss}
\end{table*}
\linespread{1}

In Table \ref{tab::loss}, we can intuitively observe that conventional training methods solely based on MSE do not yield optimal results, falling short of models trained solely on MAE or a combination of MSE and MAE (MSE+MAE). However, training exclusively on MAE tends to result in inferior MSE metrics. Taking all these factors into consideration, we ultimately decided to employ a training approach that combines MSE and MAE in a 1:1 ratio, aiming to strike a balance between these two loss functions to improve performance.



\textbf{Patch Aggregation vs Patch Disaggregation.} Intuitively, the relationships among patches are related to the potential period of the datasets, so fewer output channel numbers in pointwise convolution is beneficial for the model to learn periodic correlation through weight sharing, which is called patch aggregation. In the case of patch disaggregation, pointwise convolution can better fit the future development trend of time series by retaining more weights. Therefore, we can freely adjust the degree of patch aggregation by modifying the proportion of output channels and input channels in pointwise convolution. 

From Table \ref{table::Patch Aggregation Analysis}, we can see that for small and medium-sized datasets, patch aggregation has a significant improvement in prediction performance. It is noted that the results of full patch aggregation are similar to those using global average pooling. Both prevent overfitting by reducing the number of parameters, which is beneficial to improving the prediction accuracy of small and medium-sized data sets. However, for the two largest datasets, Traffic and Electricity, the effect of patch disaggregation is better. Moreover, we indicate that patch aggregation is a universal technique that can be used not only for PatchMixer but also for other models with patch presentation.
\linespread{1.2}
\begin{table*}[t]
	\centering
	\scalebox{0.75}{
		\begin{tabular}{cc|c|cc|cc|cc|cc}

			\cline{2-11}
			&\multicolumn{2}{c|}{\multirow{2}{*}{Models}}& \multicolumn{6}{c|}{PatchMixer}&  \multicolumn{2}{c|}{\multirow{2}{*}{PatchTST}}  \\
			\cline{4-9}
			&\multicolumn{2}{c|}{}& \multicolumn{2}{c|}{Pooling}& \multicolumn{2}{c|}{\textbf{PFA}}& \multicolumn{2}{c|}{\textbf{PDA}}& \multicolumn{2}{c|}{} \\
			\cline{2-11}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-11}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96  & 0.147 & 0.188 & \textbf{0.148} & \textbf{0.191} & 0.149 & 0.193 & 0.152 & 0.199  \\
            &\multicolumn{1}{c|}{}& 192 & 0.187 & 0.229 & \textbf{0.189} & \textbf{0.230} & 0.191 & 0.233 & 0.197 & 0.243   \\
            &\multicolumn{1}{c|}{}& 336 & 0.220 & 0.261 & \textbf{0.218} & \textbf{0.261} & 0.225 & 0.269 & 0.249 & 0.283   \\
            &\multicolumn{1}{c|}{}& 720 & 0.295 & 0.315 & \textbf{0.298} & \textbf{0.318} & 0.307 & 0.324 & 0.320 & 0.335    \\
			\cline{2-11}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96 & 0.385 & 0.257 & 0.382 & 0.256 & \textbf{0.362} & \textbf{0.242} & 0.367 & 0.251 \\
            &\multicolumn{1}{c|}{}& 192  & 0.402 & 0.265 & 0.397 & 0.262 & \textbf{0.382} & \textbf{0.252} & 0.385 & 0.259  \\
            &\multicolumn{1}{c|}{}& 336   & 0.414 & 0.272 & 0.409 & 0.269 & \textbf{0.392}  & \textbf{0.257} & 0.398 & 0.265   \\
            &\multicolumn{1}{c|}{}& 720   & 0.443 & 0.291 & 0.436 & 0.284 & \textbf{0.428} & \textbf{0.282} & 0.432 & 0.287  \\
                \cline{2-11}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96 & 0.133 & 0.226 & 0.131 & 0.224 & \textbf{0.128} & \textbf{0.221} & 0.130 & 0.222   \\
			&\multicolumn{1}{c|}{}& 192 & 0.149 & 0.244 & 0.144 & 0.237 & \textbf{0.144} & \textbf{0.237} & 0.148 & 0.240  \\
			&\multicolumn{1}{c|}{}& 336 & 0.169 & 0.263 & 0.166 & 0.258 & \textbf{0.164} & \textbf{0.257} & 0.167 & 0.261  \\
			&\multicolumn{1}{c|}{}& 720 & 0.209 & 0.295 & 0.202 & 0.289 & \textbf{0.201} & \textbf{0.290} & 0.202 & 0.291 \\
			\cline{2-11}
   			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96 & 0.355 & 0.383 & 0.357 & 0.384 & \textbf{0.355} & \textbf{0.383} & 0.375 & 0.399  \\
			&\multicolumn{1}{c|}{}& 192 & 0.376 & 0.396 & 0.380 & 0.399 & \textbf{0.373} & \textbf{0.394} & 0.414 & 0.421   \\
			&\multicolumn{1}{c|}{}& 336 & 0.391 & 0.410 & 0.393 & 0.411 & \textbf{0.391} & \textbf{0.410} & 0.431 & 0.436  \\
			&\multicolumn{1}{c|}{}& 720 & 0.445 & 0.457 & \textbf{0.442} & \textbf{0.456} & 0.446 & 0.463 & 0.449 & 0.466  \\
			\cline{2-11}
   			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96 & 0.220 & 0.298 & \textbf{0.221} & \textbf{0.299} & 0.225 & 0.300 & 0.274 & 0.336   \\
			&\multicolumn{1}{c|}{}& 192 & 0.267 & 0.332 & \textbf{0.269}& 0.335 & 0.275 & \textbf{0.334} & 0.339 & 0.379   \\
			&\multicolumn{1}{c|}{}& 336 & 0.304 & 0.363 & \textbf{0.306} & \textbf{0.366} & 0.316 & 0.368 & 0.331 & 0.380  \\
			&\multicolumn{1}{c|}{}& 720 & 0.375 & 0.417 & \textbf{0.379} & \textbf{0.420} & 0.397 & 0.427 & 0.379 & 0.422  \\
			\cline{2-11}
   			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96 & 0.301 & 0.343 & \textbf{0.289} & \textbf{0.338} & 0.290 & 0.340 & 0.290 & 0.342  \\
			&\multicolumn{1}{c|}{}& 192 & 0.336 & 0.363 & \textbf{0.323} & \textbf{0.358} & 0.325 & 0.361 & 0.332 & 0.369  \\
			&\multicolumn{1}{c|}{}& 336 & 0.364 & 0.386 & 0.355 & \textbf{0.378} & \textbf{0.353} & 0.382 & 0.366 & 0.392  \\
			&\multicolumn{1}{c|}{}& 720 & 0.428 & 0.416 & 0.416 & \textbf{0.408} & \textbf{0.413} & 0.413 & 0.420 & 0.424 \\
			\cline{2-11}
   			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96 & 0.165 & 0.252 & \textbf{0.165} & \textbf{0.252} & 0.176 & 0.257 & 0.165 & 0.255 \\
			&\multicolumn{1}{c|}{}& 192 & 0.220 & 0.289 & \textbf{0.220} & \textbf{0.291} & 0.227 & 0.295 & 0.220 & 0.292   \\
			&\multicolumn{1}{c|}{}& 336 & 0.262 & 0.320 & \textbf{0.261} & \textbf{0.320} & 0.267 & 0.322 & 0.278 & 0.329   \\
			&\multicolumn{1}{c|}{}& 720 & 0.341 & 0.373 & \textbf{0.341} & 0.373 & 0.344 & \textbf{0.372} & 0.367 & 0.385\\
			\cline{2-11}
		\end{tabular}
	}
	\caption{Patch Aggregation Analysis. We use prediction lengths $T\in \{96, 192, 336, 720\}$. \textbf{PFA} means \uline{P}atch \uline{F}ull \uline{A}ggregation and \textbf{PDA} means \uline{P}atch \uline{D}is-\uline{A}ggregation, while the better results of them are in \textbf{bold}.}
	\label{table::Patch Aggregation Analysis}
\end{table*}
\linespread{1}


\subsection{Univariate Forecasting}
\label{append:uni}

Table \ref{tab::univariate} summarizes the results of univariate forecasting on ETT datasets. There is a target feature "oil temperature" within those datasets, which is the univariate series that we are trying to forecast. We cite the baseline results from \citep{dlinear}.

\linespread{1.2}
\begin{table*}[!htbp]
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{cc|c|cc|cc|cc|cc|cc|cc|cc|ccc}
			\cline{2-19}
&\multicolumn{2}{c|}{Models}                                                   & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{PatchMixer}\\ \textbf{(Ours)}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}PatchTST\\ \citeyearpar{patchtst}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}DLinear\\ \citeyearpar{dlinear}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}MICN\\ \citeyearpar{micn}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}TimesNet\\ \citeyearpar{timesnet}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}FEDformer\\ \citeyearpar{fedformer}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Autoformer\\ \citeyearpar{autoformer}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}Informer\\ \citeyearpar{informer}\end{tabular}} \\ \cline{2-19}
			&\multicolumn{2}{c|}{Metric}&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE&MSE&MAE\\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTh1}}& 96    & \textbf{0.054} & \textbf{0.179} & \uline{0.055} & \textbf{0.179} & 0.056 & 0.180 & 0.062 & 0.198 & 0.056 & 0.182 & 0.079 & 0.215 & 0.071 & 0.206 & 0.193 & 0.377 \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.066} & \textbf{0.198} & \uline{0.071} & 0.205 & \uline{0.071} & \uline{0.204} & 0.079 & 0.223 & 0.072 & 0.209 & 0.104 & 0.245 & 0.114 & 0.262 & 0.217 & 0.395  \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.078} & \textbf{0.220} & \uline{0.081} & \uline{0.225} & 0.098 & 0.244 & 0.093 & 0.243 & 0.086 & 0.229 & 0.119 & 0.270 & 0.107 & 0.258 & 0.202 & 0.381  \\
            &\multicolumn{1}{c|}{}& 720   & 0.093 & 0.243 & \uline{0.087} & \uline{0.232} & 0.189 & 0.359 & 0.132 & 0.292 & \textbf{0.082} & \textbf{0.228} & 0.142 & 0.299 & 0.126 & 0.283 & 0.183 & 0.355 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & \textbf{0.119} & \textbf{0.268} & 0.129 & 0.282 & 0.131 & 0.279 & 0.131 & 0.282 & 0.136 & 0.286 & \uline{0.128} & \uline{0.271} & 0.153 & 0.306 & 0.213 & 0.373  \\
            &\multicolumn{1}{c|}{}& 192   & \textbf{0.147} & \textbf{0.305} & \uline{0.168} & \uline{0.328} & 0.176 & 0.329 & 0.193 & 0.350 & 0.186 & 0.340 & 0.185 & 0.330 & 0.204 & 0.351 & 0.227 & 0.387  \\
            &\multicolumn{1}{c|}{}& 336   & \textbf{0.166} & \textbf{0.332} & \uline{0.185} & \uline{0.351} & 0.293 & 0.437 & 0.194 & 0.355 & 0.220 & 0.373 & 0.231 & 0.378 & 0.246 & 0.389 & 0.242 & 0.401  \\
            &\multicolumn{1}{c|}{}& 720   & \textbf{0.217} & \textbf{0.374} & \uline{0.224} & \uline{0.383} & 0.276 & 0.426 & 0.295 & 0.442 & 0.241 & 0.392 & 0.278 & 0.420 & 0.268 & 0.409 & 0.291 & 0.439 \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & \uline{0.027} & \uline{0.123} & \textbf{0.026} & \textbf{0.121} & 0.028 & \uline{0.123} & 0.030 & 0.131 & 0.029 & 0.127 & 0.033 & 0.140 & 0.056 & 0.183 & 0.109 & 0.277  \\
            &\multicolumn{1}{c|}{}& 192   & \uline{0.040} & \uline{0.152} & \textbf{0.039} & \textbf{0.150} & 0.045 & 0.156 & 0.044 & 0.156 & 0.047 & 0.163 & 0.058 & 0.186 & 0.081 & 0.216 & 0.151 & 0.310  \\
            &\multicolumn{1}{c|}{}& 336   & \uline{0.055} & \uline{0.177} & \textbf{0.053} & \textbf{0.173} & 0.061 & 0.182 & 0.063 & 0.186 & 0.080 & 0.214 & 0.084 & 0.231 & 0.076 & 0.218 & 0.427 & 0.591  \\
            &\multicolumn{1}{c|}{}& 720   & \uline{0.075} & 0.211 & \textbf{0.074} & \textbf{0.207} & 0.080 & \uline{0.210} & 0.078 & \uline{0.210} & 0.084 & 0.222 & 0.102 & 0.250 & 0.110 & 0.267 & 0.438 & 0.586  \\
			\cline{2-19}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96    & 0.067 & 0.188 & 0.065 & 0.186 & \textbf{0.063} & \textbf{0.183} & \uline{0.064} & \uline{0.184} & 0.066 & 0.187 & 0.067 & 0.198 & 0.065 & 0.189 & 0.088 & 0.225  \\
            &\multicolumn{1}{c|}{}& 192   & 0.097 & 0.233 & \uline{0.094} & \uline{0.231} & \textbf{0.092} & \textbf{0.227} & 0.095 & 0.232 & 0.113 & 0.250 & 0.102 & 0.245 & 0.118 & 0.256 & 0.132 & 0.283  \\
            &\multicolumn{1}{c|}{}& 336   & 0.122 & 0.267 & \uline{0.120} & \uline{0.265}  & \textbf{0.119} & \textbf{0.261} & 0.122 & \uline{0.265} & 0.133 & 0.277 & 0.130 & 0.279 & 0.154 & 0.305 & 0.180 & 0.336 \\
            &\multicolumn{1}{c|}{}& 720   & \uline{0.172} & 0.324 & \textbf{0.171} & \uline{0.322} & 0.175 & \textbf{0.320} & 0.202 & 0.348 & 0.182 & 0.333 & 0.178 & 0.325 & 0.182 & 0.335 & 0.300 & 0.435  \\
			\cline{2-19}
		\end{tabular}
	}
	\caption{Univariate long-term forecasting results with PatchMixer. ETT datasets are used with prediction lengths $T\in \{96, 192, 336, 720\}$. The best results are in \textbf{bold} and the second best results are in \uline{underlined}.}
	\label{tab::univariate}
\end{table*}
\linespread{1}

\subsection{Robustness Analysis}

\subsubsection{Results with Different Random Seeds}

The main tables in this article, including Table \ref{tab::multivariate} and Table \ref{tab::univariate}, are the averages of five random experiments. Besides, the remaining tables are generated using a fixed random number seed 2021. To examine the robustness of our results, we train the PatchMixer model with 5 different random seeds: 2021, 2022, 2023, 2024, and 2025. We calculate the MSE and MAE scores with each selected seed. The mean and standard derivation of the results are reported in Table \ref{tab::different seeds}. The variances are considerably small which indicates the robustness of our model. 

\linespread{1.2}

\begin{table*}[h]
	\centering
	\scalebox{0.75}{
		\begin{tabular}{cc|c|c|c|c|c}
			\cline{2-7}
			&\multicolumn{2}{c|}{$L$}& \multicolumn{2}{c|}{PatchMixer (Multivariate)}& \multicolumn{2}{c}{PatchMixer (Univariate)} \\
			\cline{2-7}
			&\multicolumn{2}{c|}{Metric} & MSE & MAE & MSE & MAE  \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{Weather}}& 96  & 0.15090.0021 & 0.19340.0022 & - & - \\
            &\multicolumn{1}{c|}{}& 192   & 0.19350.0027  & 0.23600.0028 & - & - \\
            &\multicolumn{1}{c|}{}& 336   & 0.22460.0033 & 0.26750.0028 & - & -  \\
            &\multicolumn{1}{c|}{}& 720   & 0.30460.0026 & 0.32300.0019 & - & - \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{Traffic}}& 96  & 0.36340.0015 & 0.24470.0023 & - & - \\
            &\multicolumn{1}{c|}{} & 192   & 0.38360.0012 & 0.25370.0017 & - & - \\
            &\multicolumn{1}{c|}{}& 336   & 0.39310.0010 & 0.25830.0011 & - & - \\
            &\multicolumn{1}{c|}{}& 720   & 0.42910.0051 & 0.28260.0051 & - & - \\
            \cline{2-7}
			&\multirow{4}*{\rotatebox{90}{Electricity}}& 96    & 0.12850.0010 & 0.22080.0007 & - & - \\
			&\multicolumn{1}{c|}{}& 192   & 0.14420.0008 & 0.23730.0007 & - & - \\
			&\multicolumn{1}{c|}{}& 336   & 0.16430.0014 & 0.25690.0011 & - & -\\
			&\multicolumn{1}{c|}{}& 720   & 0.19980.0015 & 0.28890.0010 & - & - \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTh1}} & 96    & 0.35300.0017 & 0.38120.0019 & 0.05430.0018 & 0.17940.0042  \\
            &\multicolumn{1}{c|}{}& 192   & 0.37340.0020 & 0.39370.0023 & 0.06620.0004 & 0.19840.0008 \\
            &\multicolumn{1}{c|}{}& 336   & 0.39210.0070 & 0.41360.0109 & 0.07790.0009 & 0.21960.0009 \\
            &\multicolumn{1}{c|}{}& 720   & 0.44530.0020 & 0.46300.0016 & 0.09300.0031 & 0.24320.0034 \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTh2}}& 96    & 0.22540.0013 & 0.30040.0005 & 0.11880.0009 & 0.26840.0005  \\
            &\multicolumn{1}{c|}{}& 192   & 0.27430.0010 & 0.33440.0009 & 0.14650.0025 & 0.30450.0012 \\
            &\multicolumn{1}{c|}{}& 336   & 0.31680.0020 & 0.36760.0013  & 0.16620.0009 & 0.33190.0006 \\
            &\multicolumn{1}{c|}{}& 720   & 0.39340.0037 & 0.42630.0012 & 0.21680.0025 & 0.37440.0023 \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTm1}}& 96    & 0.29110.0016 & 0.33950.0012 & 0.02660.0001 & 0.12280.0003 \\
            &\multicolumn{1}{c|}{}& 192   & 0.32530.0013 & 0.36180.0007 & 0.04010.0004 & 0.15190.0003 \\
            &\multicolumn{1}{c|}{}& 336   & 0.35290.0008 & 0.38220.0014 & 0.05490.0005 & 0.17690.0009 \\
            &\multicolumn{1}{c|}{}& 720   & 0.41340.0035 & 0.41320.0006 & 0.07520.0012 & 0.21080.0039 \\
			\cline{2-7}
			&\multirow{4}*{\rotatebox{90}{ETTm2}}& 96    & 0.17390.0021 & 0.25580.0007 & 0.06650.0006 & 0.18750.0008 \\
            &\multicolumn{1}{c|}{}& 192   & 0.22740.0041 & 0.29540.0024 & 0.09670.0015 & 0.23340.0012 \\
            &\multicolumn{1}{c|}{}& 336   & 0.26610.0011 & 0.32290.0013 & 0.12200.0007 & 0.26660.0005 \\
            &\multicolumn{1}{c|}{}& 720   & 0.34280.0016 & 0.37270.0004 & 0.17240.0016 & 0.32420.0025 \\
			\cline{2-7}
		\end{tabular}
	}
	\caption{Long-term forecasting results with different random seeds in PatchMixer. }
	\label{tab::different seeds}
\end{table*}
\linespread{1}

\subsubsection{Results with Different Model Parameters}

To see whether PatchMixer is sensitive to the choice of different settings, we perform another experiment with varying model parameters. We vary the number of PatchMixer layers $L=\{1,2,3\}$ and select the model dimension $D=\{128, 256\}$. In total, there are 6 different sets of model hyper-parameters to examine. Figure \ref{fig::robustness} shows the datasets are robust to the choice of model hyper-parameters.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.32]{figures/v2_robust_hyper.png}
\end{center}
\caption{MSE scores with varying model parameters. Each bar indicates the MSE score of a parameter combination. The combinations $(L, D) = (1,128)$, $(1,256)$, $(2,128)$, $(2,256)$, $(3,128)$, $(3,256)$ are orderly labeled from 1 to 6 in the figure. The model is run with PatchMixer to forecast $96$ steps.}
\label{fig::robustness}
\end{figure}

\end{document}
