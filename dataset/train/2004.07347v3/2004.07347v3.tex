

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{xspace}
\newcommand{\nop}[1]{}

\newcommand{\xwh}[1]{{\bf \textcolor{blue}{Wenhan: {\sf #1}}}}
\newcommand{\ww}[1]{{\bf \textcolor{cyan}{William: {\sf #1}}}}
\newcommand{\dataset}{\textsc{HybridQA}\xspace}
\newcommand{\model}{\textsc{hybrider}\xspace}

\aclfinalcopy 



\title{HybridQA: A Dataset of Multi-Hop Question Answering \\over Tabular and Textual Data}

\author{Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Wang\\
University of California, Santa Barbara, CA, USA\\
\tt{\{wenhuchen, hwzha, xwhan\}@cs.ucsb.edu,}\\
\tt{\{zhiyuchen, hongwang600, william\}@cs.ucsb.edu}\\
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA\footnote{\url{https://github.com/wenhuchen/HybridQA}}, a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model. 2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20\%, while the hybrid model can achieve an EM over 40\%. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model's score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information. 
\end{abstract}

\section{Introduction}
Question answering systems aim to answer any form of question of our interests, with evidence provided by either free-form text like Wikipedia passages~\cite{rajpurkar2016squad,chen2017reading,yang2018hotpotqa} or structured data like Freebase/WikiData~\cite{berant2013semantic,kwiatkowski2013scaling,yih2015semantic,weston2015towards} and WikiTables~\cite{pasupat2015compositional}. Both forms have their advantages, the free-form corpus has in general better coverage while structured data has better compositionality to handle complex multi-hop questions. Due to the advantages of different representation forms, people like to combine them in real world applications. Therefore, it is sometime not ideal to assume the question has answer in a passage. This paper aims to simulate a more realistic setting where the evidences are distributed into heterogeneous data, and the model requires to aggregate  information from different forms for answering a question. There has been some pioneering work on building hybrid QA systems~\cite{sun2019pullnet,sun2018open,xiong2019improving}. These methods adopts KB-only datasets~\cite{berant2013semantic,yih2015semantic,talmor2018web} to simulate a hybrid setting by randomly masking KB triples and replace them with text corpus. Experimental results have proved decent improvement, which shed lights on the potential of hybrid question answering systems to integrate heterogeneous information.
\begin{table}[!t]
\small
\begin{tabular}{lcccc}
\toprule
Dataset      & Size & \#Docs       & \begin{tabular}[c]{@{}l@{}}KB/\\ Table\end{tabular} & Multi-Hop \\
\midrule
WebQuestions & 5.8K   & no              & yes                                                 & yes       \\
WebQSP       & 4.7K   & no              & yes                                                 & yes       \\
WebQComplex  & 34K    & no              & yes                                                 & yes       \\
MetaQA       & 400k   & no              & yes                                                 & yes       \\
WikiTableQA  & 22K    & no              & yes                                                 & yes       \\
\midrule
SQuAD-v1     & 107K   & 1               & no                                                  & no        \\
DROP         & 99K    & 1               & no                                                  & yes       \\
TriviaQA     & 95K    & \textgreater{}1 & no                                                  & no        \\
HotpotQA    & 112K   & \textgreater{}1 & no                                                  & yes       \\
Natural-QA   & 300K   & \textgreater{}1 & no                                                  & yes        \\
\midrule
\dataset     & 70K    & 1 & yes                                                 & yes     \\     
\bottomrule
\end{tabular}
\caption{Comparison of existing datasets, where \#docs means the number of documents provided for a specific question. 1) KB-only datasets: WebQuestions~\cite{berant2013semantic}, WebQSP~\cite{yih2016value}, WebComplex~\cite{talmor2018web}, MetaQA~\cite{zhang2018variational}, WikiTableQuestion~\cite{pasupat2015compositional}. 2) Text-only datasets with single passage: like SQuAD~\cite{rajpurkar2016squad}, DROP~\cite{dua2019drop}. 3) open-domain Text-Only dataset: TriviaQA~\cite{joshi2017triviaqa}, HotpotQA~\cite{yang2018hotpotqa}, Natural Questions~\cite{kwiatkowski2019natural}.}
\label{tab:dataset}
\vspace{-3ex}
\end{table}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{example.pdf}
    \caption{Examples of annotated question answering pairs from Wikipedia page\footnote{\url{https://en.wikipedia.org/wiki/List_of_flag_bearers_for_Myanmar_at_the_Olympics}}. Underlined entities have hyperlinked passages, which are displayed in the boxes. The lower part shows the human-annotated question-answer pairs roughly categorized based on their hardness.}
    \label{fig:example}
    \vspace{-2ex}
\end{figure*}
Though there already exist numerous valuable questions answering datasets as listed in~\autoref{tab:dataset}, these datasets were initially designed to use either structured or unstructured information during annotation. There is no guarantee that these questions need to aggregate heterogeneous information to find the answer. Therefore, designing hybrid question answering systems would probably yield marginal benefits over the non-hybrid ones, which greatly hinders the research development in building hybrid question answering systems.

To fill in the gap, we construct a heterogeneous QA dataset \dataset, which is collected by crowdsourcing based on Wikipedia tables. During annotation, each crowd worker is presented with a table along with its hyperlinked Wikipedia passages to propose questions requiring aggregating both forms of information. The dataset consists of roughly 70K question-answering pairs aligned with 13,000 Wikipedia tables. As Wikitables~\cite{bhagavatula2013methods} are curated from high-standard professionals to organize a set of information regarding a given theme, its information is mostly absent in the text. Such a complementary nature makes WikiTables an ideal environment for hybrid question answering. To ensure that the answers cannot be hacked by single-hop or homogeneous models, we carefully employ different strategies to calibrate the annotation process. An example is demonstrated in~\autoref{fig:example}. This table is aimed to describe Burmese flag bearers over different Olympic events, where the second column has hyperlinked passages about the Olympic event, and the fourth column has hyperlinked passages about biography individual bearers. The dataset is both multi-hop and hybrid in the following senses: 1) the question requires multiple hops to achieve the answer, each reasoning hop may utilize either tabular or textual information. 2) the answer may come from either the table or a passage. 

In our experiments, we implement three models, namely Table-only model, Passage-only, and a heterogeneous model \model, which combines both information forms to perform multi-hop reasoning. Our Experiments show that two homogeneous models only achieve EM lower than 20\%, while \model can achieve an EM over 40\%, which concludes the necessity to do multi-hop reasoning over heterogeneous information on \dataset. As the \model is still far behind human performance, we believe that it would be a challenging next-problem for the community.


\begin{table}[!t]
\small
\centering
\begin{tabular}{lccc}
\hline
\#Table & \#Row/\#Column & \#Cell &  \#Links/Table \\ 
\hline
13,000 & 15.7/4.4 & 70 & 44 \\
\hline
\#Passage & \#Words/Passage & \#Ques & \#Words/Ques \\
\hline
293,269 & 103 &  69,611 & 18.9 \\
\hline
\end{tabular}
\caption{Statistics of Table and Passage in our dataset.}
\label{tab:table_and_passage}
\vspace{-2ex}
\end{table}

\section{Dataset}
In this section, we describe how we crawl high-quality tables with their associated passages, and then describe how we collect hybrid questions. The statistics of \dataset is in~\autoref{tab:table_and_passage}.

\paragraph{Table/Passage Collection}
To ease the annotation process, we apply the following rules during table crawling. 1) we need tables with rows between 5-20, columns between 3-6, which is appropriate for the crowd-workers to view. 2) we restrain the tables from having hyperlinked cells over 35\% of its total cells, which provide an abundant amount of textual information. For each hyperlink in the table, we retrieve its Wikipedia page and crop at most the first 12 sentences from its introduction session as the associated passage. 3) we apply some additional rules to avoid improper tables and finally collect 13,000 high-quality tables.  

\paragraph{Question/Answer Collection}
We release 13K HITs (human intelligence task) on the Amazon Mechanical Turk platform, where each HIT presents the crowd-worker with one crawled Wikipedia table along with its hyperlinked passages. We require the worker to write six questions as well as their answers. The question annotation phase is not trivial, as we specifically need questions that rely on both tabular and textual information. In order to achieve that, we exemplify abundant examples in our Amazon Turker interface with detailed explanations to help crowd-workers to understand the essence of the ``hybrid" question. The guidelines are described as follows:
\begin{itemize}
    \item The question requires multiple steps over two information forms of reasoning to answer.
    \item Table reasoning step specifically includes (i) filter our table rows based on equal/greater/less, e.g. ``For the XXXI Olympic event", (ii)) superlative operation over a column, e.g. ``the earliest Olympic event", (iii) hop between two cells, e.g. ``Which event ... participate in ...", (iv) extract information from table, e.g. ``In which year did the player ... ".
    \item Text reasoning step specifically includes (i) select passages based on the certain mentions, e.g. ``the judoka bearer", (ii) extract a span from the passage as the answer.
    \item The answer should be a minimum text span from either a table cell or a specific passage.
\end{itemize}
Based on the above criteria, we hire five CS-majored graduate students as our ``human expert" to decide the acceptance of a HIT. The average completion time for one HIT is 12 minutes, and payment is \\rightarrow\rightarrow\rightarrow  \rightarrow q\tau\mathcal{C}cp_{f}(c | q, \mathcal{C})p_{h}(c' | q, c)p_{r}(a | P, q)H_cc\mathcal{N}_c\{H_c\}s_c\mathcal{C}(c \rightarrow c')H_{c, c'}=[H_c, H_{c'}]s_{c, c'}cP(c)Pg_s(P, q, index)g_e(P, q, index)|P|p_{r}(a|P, q)a_saa_eap(a|q, T)cac\mathcal{C}\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau\tau87.4\% \times 87.9\% \times 89.2\% \times 61.9\% \approx 42\%$ and find that the result consistent well the overall accuracy, which demonstrates the necessity to perform each reasoning step correctly. Such error cascading makes the problem extremely difficult than the previous homogeneous question answering problems. 

By breaking down the reasoning into steps, \model layouts strong explainability about its rationale, but it also causes error propagation, i.e., the mistakes made in the earlier stage are non-reversible in the following stage. We believe future research on building an end-to-end reasoning model could alleviate such an error propagation problem between different stages in \model.   

\section{Related Work}
\paragraph{Text-Based QA} Since the surge of SQuAD~\cite{rajpurkar2016squad} dataset, there have been numerous efforts to tackle the machine reading comprehension problem. Different datasets like DrQA~\cite{chen2017reading}, TriviaQA~\cite{joshi2017triviaqa}, SearchQA~\cite{dunn2017searchqa} and DROP~\cite{dua2019drop} have been proposed. As the SQuAD~\cite{rajpurkar2016squad} questions that are relatively simple because they usually require no more than one sentence in the paragraph to answer. The following datasets further challenge the QA model's capability to handle different scenarios like open-domain, long context, multi-hop, discrete operations, etc. There has been a huge success in proving that the deep learning model can show strong competence in terms of understanding text-only evidence. Unlike these datasets, \dataset leverages structured information in the evidence form, where the existing models are not able to handle, which distinguishes it from the other datasets.

\paragraph{KB/Table-Based QA} Structured knowledge is known as unambiguous and compositional, which absorbs lots of attention to the QA system built on KB/Tables. There have been multiple datasets like WebQuestion~\cite{berant2013semantic}, ComplexWebQuestions~\cite{talmor2018web}, WebQuestionSP~\cite{yih2015semantic} on using FreeBase to answer natural questions. Besides KB, structured or semi-structured tables are also an interesting form. Different datasets like WikiTableQuestions~\cite{pasupat2015compositional}, WikiSQL~\cite{zhong2017seq2sql}, SPIDER~\cite{yu2018spider}, TabFact~\cite{2019TabFactA} have been proposed to build models which can interact with such structured information. However, both KB and tables are known to suffer from low coverage issues. Therefore, \dataset combine tables with text as complementary information to answer natural questions.  

\paragraph{Information Aggregation} There are some pioneering studies on designing hybrid question answering systems to aggregate heterogeneous information.  GRAFT~\cite{sun2018open} proposes to use the early fusion system and use heuristics to build a question-specific subgraph that contains sentences from corpus and entities, facts from KB. PullNet~\cite{sun2019pullnet} improves over GRAFT to use an integrated framework that dynamically learns to retrieve and reason over heterogeneous information to find the best answers. More recently, KAReader~\cite{xiong2019improving} proposes to reformulate the questions in latent space by reading retrieved text snippets under KB-incomplete cases. These models simulate a `fake' KB-incomplete scenario by masking out triples from KB. In contrast, the questions in \dataset are inherently hybrid in the sense that it requires both information forms to reason, which makes our testbed more realistic.


\section{Conclusion}
We present \dataset, which is collected as the first hybrid question answering dataset over both tabular and textual data. We release the data to facilitate the current research on using heterogeneous information to answer real-world questions. We design \model as a strong baseline and offer interesting insights about the model. We believe \dataset is an interesting yet challenging next-problem for the community to solve.

\section*{Acknowledgement}
The authors would like to thank the anonymous reviewers and area chairs for their thoughtful comments. We would like to thank Amazon AWS Machine Learning Research Award for sponsoring the computing resources.

\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}

\end{document}
