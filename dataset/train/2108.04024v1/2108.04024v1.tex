\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[numbers,sort,compress]{natbib}
\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage{soul}
\usepackage[dvipsnames]{xcolor}
\usepackage{xspace}
\usepackage[normalem]{ulem}

\usepackage{tabularx,ragged2e,booktabs}
\usepackage{multirow}
\usepackage[percent]{overpic}

\usepackage[figuresleft]{rotating}

\usepackage{shadowtext}
\usepackage[export]{adjustbox}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}

\usepackage{enumitem}



\usepackage[heightadjust=all,valign=c]{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\usepackage[outline]{contour}
\usepackage{mathtools}
\usepackage{bbm}

\usepackage{sg-macros}



\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{1.05ex \@plus 1ex \@minus .2ex}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\dstname}{CIRR\xspace}
\newcommand{\dstnamefull}{Compose Image Retrieval on Real-life images\xspace}
\newcommand{\modelnamefull}{Composed Image Retrieval using Pretrained LANguage Transformers\xspace}
\newcommand{\modelname}{CIRPLANT\xspace}
\newcommand{\dsturl}{\url{https://cuberick-orion.github.io/CIRR/}\xspace}

\iccvfinalcopy 

\begin{document}

\title{Image Retrieval on Real-life Images with Pre-trained \\Vision-and-Language Models}

\author{Zheyuan Liu \quad Cristian Rodriguez-Opazo \quad Damien Teney \quad Stephen Gould \\
Australian National University\\
Australian Institute for Machine Learning, University of Adelaide\quad
Idiap Research Institute \\
{\tt\small \{zheyuan.liu, stephen.gould\}@anu.edu.au} \\
{\tt\small cristian.rodriguezopazo@adelaide.edu.au, damien.teney@idiap.ch}
}

\maketitle

\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
   We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image.
   Existing methods have only been applied to non-complex images within narrow domains, such as fashion products,
   thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts.
   To address this issue, we collect the \dstnamefull (\dstname) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text.
   To extend current methods to the open-domain, we propose \modelname, a transformer based model that leverages rich pre-trained vision-and-language (V\&L) knowledge for modifying visual features conditioned on natural language.
   Retrieval is then done by nearest neighbor lookup on the modified features.
   We demonstrate that with a relatively simple architecture, \modelname outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion.
   Together with the release of \dstname, we believe this work will inspire further research on composed image retrieval.
   Our dataset, code and pre-trained models are available at \dsturl.
\end{abstract}

\section{Introduction} \label{sec:intro}
We study the task of \emph{composed image retrieval}, that is, finding an image from a large corpus that best matches a user query provided as an image-language pair. Unlike traditional content-based~\cite{10.1145/500141.500159_cbir} or text-based~\cite{zhang_tbir,6126478_tbir} image retrieval where a single modality is used to describe the target image, composed image retrieval involves both visual and textual modalities to specify the user's intent. For humans the advantage of a bi-modal query is clear: some concepts and attributes are more succinctly described visually, others through language. By cross-referencing the two modalities, a reference image can capture the general gist of a scene, while the text can specify finer details. 
The challenge is the inherent ambiguity in knowing what information is important (typically one object of interest in the scene) and what can be ignored (\eg, the background and other irrelevant objects). However, existing datasets for this task fall short of allowing us to adequately study this problem.

\begin{figure}[t]
  \begin{center}
    \includegraphics[trim={0 5pt 0 0},clip, width=0.99\linewidth]{imgs/head_img_0.pdf}
  \end{center}
   \caption{Example of composed image retrieval from the proposed \dstname dataset. The input is composed of a reference image and a modifying text, to which the model must find a close match. A major challenge is the inherent ambiguity and underspecification of visual aspects to be preserved or modified. Our dataset includes open-domain images with rich contexts to facilitate the study of such challenge.}
   \label{fig:intro-0}
\end{figure}

Consider the example in \figref{fig:intro-0}. 
Real-life images usually contain rich object interactions on various scales. In each case, to readily identify the relevant aspects to keep or change and pay less attention elsewhere (\eg, the color of the dog's fur and background objects), a model must develop in-depth visual reasoning ability and infer implicit human agreements within both the visual and language contexts.
However, existing datasets are constrained to domains such as fashion products~\cite{fashioniq,han2017automatic_fashion200k,10.5555/1886063.1886114_shoes} or synthetic objects~\cite{Vo_2019_tirg} with relatively simple image contents.
We argue that the current datasets are insufficient for exploring the unique research opportunity mentioned above.

Motivated by this problem, we collect the \dstnamefull (\dstname) dataset. It is based on the open-domain collection of real images from NLVR~\cite{Suhr_2019_nlvr2}, for which we collected rich, high-quality annotations that aim to tease out the important aspects of the reference image and textual description for a given query.

Compared with existing datasets, \dstname places more emphasis on distinguishing between visually similar images, which provides a greater challenge, as well as a chance for studying fine-grained vision-and-language (V\&L) reasoning in composed image retrieval. 
Our dataset also allows for evaluation on fully labeled subsets, which addresses a shortcoming of existing datasets that are not fully labeled and therefore contain multiple false-negatives (as unlabeled images are considered negative).

Meanwhile, we propose \modelnamefull (\modelname), which extends current methods into open-domain images by leveraging the knowledge of large-scale V\&L pre-trained (VLP) model~\cite{oscar}. 
Although the advantages of such pre-trained models have been validated in many visiolinguistic tasks~\cite{oscar,vilbert,chen2020uniter}, to the best of our knowledge, none have been applied to composed image retrieval. We conjecture one of the reasons being the existing domain-specific datasets cannot greatly benefit from the pre-training, which uses more complex, open-world images. 
Moreover, to adopt the VLP models for fine-tuning, most of the downstream tasks are formulated as classification tasks~\cite{oscar,chen2020uniter}. For composed image retrieval, it requires taking as input both the reference and target images. However, this greatly raises the computational overhead for retrieval, as the model needs to exhaustively assess each input query paired with each candidate target before yielding the one with the highest prediction score. 
Instead, we propose to preserve the conventional metric learning pipeline, where the input queries are jointly embedded using the VLP model and later compared with features of candidate images through -norm distance. 
Specifically, our design maintains the same objective of ``language-conditioned image feature modification'' as previous work~\cite{Vo_2019_tirg,chen2020image_val,dodds2020modality_maaf}, while manages to utilize the pre-trained V\&L knowledge in large-scale models.
We demonstrate that our proposed model reaches state-of-the-art on the existing fashion dataset while outperforming current methods on \dstname.

\section{Related Work}\label{sec:related_work}

\paragraph{Image retrieval.}
Existing work on image retrieval using deep learning can be categorized by the type of queries considered.
Content-based Image Retrieval (CBIR) refers to the use of image-only queries for product search~\cite{liuLQWTcvpr16DeepFashion}, face recognition~\cite{schroff2015facenet,masi2018deep}, etc.
This setup leaves little room for iterative user feedback or refinement.
Other possible modalities to form queries include attributes~\cite{han2017automatic_fashion200k}, natural language~\cite{6126478_tbir,zhang_tbir}, and sketches~\cite{radenovic2018deep}.
These are motivated by a more natural user experience, but require more advanced retrieval mechanisms.
\citet{Vo_2019_tirg} propose \emph{composed image retrieval} that combines visual and text modalities.
Here the query consists of a reference image and short text describing desired differences with this image.
\citet{fashioniq} demonstrate the potential of this setup for the narrow domain of fashion recommendation.

Our work focuses on composed image retrieval in an open-domain setting, \ie, not restricted to fashion products for example. We specifically address the case of distinguishing visually similar images, which requires more in-depth, fine-grained reasoning ablility over both the visual and language modalities.

\paragraph{Compositional learning.} 
The topic of compositional learning has been extensively studied in V\&L tasks including visual question answering (VQA)~\cite{antol2015vqa}, image captioning~\cite{aneja2017convolutional, anderson2018bottom} and video retrieval~\cite{Xu_2019-T2C}.
The aim is to produce learned joint-embedding features that capture the salient information in both visual and text modalities along with their interactions.
For composed image retrieval, \citet{Vo_2019_tirg} first propose a residual-gating mechanism that aims to control variation of the input image features through text.
\citet{9157125_hosseinzadeh} use region-based visual features from R-CNN models~\cite{girshick2015fast_fastrcnn,ren2015faster_fasterrcnn} originally proposed for image captioning~\cite{anderson2018bottom} and VQA~\cite{teney2017tipsandtricks}. 
Recently, \citet{chen2020image_val} use a transformer-based model~\cite{vaswani2017attention_transformer} and inject the text modality at varying depths of the image model. \citet{dodds2020modality_maaf} introduce the concept of modality-agnostic tokens, which they obtain from ``divided'' spatial convolutional features and LSTM hidden states.
In this work, we propose a method that leverages the rich knowledge in VLP models. Our method can modify the input image features based on natural language without the need of developing monolithic architecture on the specific task.

\paragraph{Vision-and-language pre-training.}
The success of pre-trained BERT~\cite{devlin2018bert_bert} inspired numerous attempts on VLP models, including~\cite{chen2020uniter,vilbert,li2019_visualbert,oscar,tan2019lxmert}. The aim is to develop Transformer-based~\cite{vaswani2017attention_transformer} models trained on large-scale image-text triplets to produces V\&L representations applicable to various tasks. The advantage is clear, instead of training monolithic models on task-specific datasets from zero, different V\&L tasks can start with the representations learned from (usually) a considerably larger image-text corpus, and fine-tune on specific tasks.
Motivated by success in other V\&L tasks, we propose to adopt the VLP model on composed image retrieval. 
The key obstacle is to design the architecture to encourage a controlled modification of image features, which, differs greatly from the conventional use cases of such models.

\paragraph{Datasets for composed image retrieval.}
Most existing datasets suitable for composed image retrieval are repurposed from other tasks~\cite{Isola2015DiscoveringSA_mitstates,han2017automatic_fashion200k,Vo_2019_tirg}. Images are paired within classes and textual descriptions of their differences are generated automatically from existing labels. 
These datasets are relatively simple visually and only contain short descriptions with simple language.
CSS~\cite{Vo_2019_tirg} uses the synthetic images of geometric 3D shapes from CLEVR~\cite{clevr}, paired with descriptions generated according to differences in appearance of the objects.
Fashion200k~\cite{han2017automatic_fashion200k} contains approx. 200k images tagged with attributes that can be used to compose text descriptions of differences between images. 
MIT-States~\cite{Isola2015DiscoveringSA_mitstates} contains images of entities in different states each labelled with one noun and one adjective. The adjectives can describe limited differences between images.
More recent works introduced human-generated descriptions.
\citet{guo2018dialog} present annotations for Shoes~\cite{10.5555/1886063.1886114_shoes}, a dataset of 10k footwear images. 
Fashion-IQ~\cite{fashioniq} contains crowd-sourced descriptions of differences between images of fashion products. 
\citet{dodds2020modality_maaf} introduce benchmarks for the Birds-to-Words~\cite{forbes2019neural_birds} and Spot-the-Diff~\cite{jhamtani2018learning_spotthediff} datasets.

In this paper, we introduce a new dataset that addresses current deficiencies.
Our dataset is open-domain and not restricted, \eg, to fashion products~\cite{han2017automatic_fashion200k,fashioniq,10.5555/1886063.1886114_shoes}.
We design a careful collection process to produce high-quality pairs from our diverse collection of images by only associating visually- and semantically-related images.
We also address the issue of false-negative targets, that is, candidate target images that are valid for a certain input query, but not labeled as such.
Previous datasets failed to resolve this issue due to the cost of exhaustively labeling images against every possible query, which is mitigated by our data collection strategy.
Although not used in our current work, the dataset also contains a rich set of auxiliary annotations that clarify ambiguities not addressed in the textual query.


\section{The Proposed Model}\label{model}
In this section, we first briefly introduce the vision-and-language pre-trained (VLP) models, then we discuss our adaptation of it for the task of composed image retrieval. 

\begin{figure*}[t!]
  \begin{center}
    \includegraphics[trim={0pt 1pt 0 6pt},clip, width=0.85\linewidth]{imgs/model_0.pdf}
  \end{center}
  \caption{(Left) Schematic of our model. Given a pair of reference image and text as input, we aim at learning a \textit{modified} image feature of the reference image conditioned on the text, such that it matches the feature of the target image. To compare image features of reference and candidate target images, we extract ResNet features and use a shared FC-layer (with normalization) to project them into the same domain. (Right) Overview of the image-text composition module using vision-and-language pre-trained (VLP) multi-layer transformers. Dashed lines (not fully drawn) represent feature aggregation by attention, which learns a language-conditioned image feature modification.}
  \label{fig:model-0}
  \vspace{-1.0em}
\end{figure*}

 
\begin{figure*}[!ht]
  \centering
    \includegraphics[trim={0 1pt 0 5pt},clip, width=0.90\linewidth]{imgs/dst_collection.pdf}
  \caption{Overview of the data collection process. (a) We demonstrate the construction of an image subset. (b) We illustrate how we choose and form 9 image pairs within one subset, where each arrow suggests the direction from a reference to a target image. (c)  represents Human Tasks with AMT workers.  indicates the instruction that mitigates the issue of false-negative.}
  \label{fig:dset_collection_0}
\end{figure*}

\subsection{Vision-and-Language Pre-trained Models}

Contemporary VLP models are inspired by BERT~\cite{devlin2018bert_bert}, which is constructed with multi-layer transformers~\cite{vaswani2017attention_transformer}. 
The model accepts variable-length sequential inputs , which consist of a concatenation among words in the text sequence(s) , regional features from the image , and other optional tokens. For instance, in OSCAR~\cite{oscar}, an object label associated with each regional feature is appended to the end as .

Within each transformer layer, a multi-head self-attention mechanism is designed to capture the dependencies among the sequential tokens. Layers are stacked hierarchically to attend to the output of the previous layer. Once pre-trained on a large corpus, the final output representations can be used for fine-tuning on arbitrary downstream tasks, where the usage varies depending on the task.

That said, downstream tasks share some common aspects.
Mostly, a classification token \texttt{[CLS]} is inserted at the start of the input text sequence, which aggregates information from the modalities. The final \texttt{[CLS]} output is then used to make predictions, such as for image classification.

\subsection{Adaptation to Composed Image Retrieval}
The task of composed image retrieval can be formally described as finding the target image in a large corpus of images  that best matches a query provided by a reference image-text pair . 
Our goal is to learn a text-image composition module, which maps a given  into the same embedding space as, and close to, the corresponding . Intuitively speaking, this requires the composition module to modify  conditioned on .

In this work, we employ OSCAR~\cite{oscar}, a recently proposed VLP model with state-of-the-art performance as the composition module to perform the mapping as follows. 

\paragraph{Input sequence.}
We denote the input sequence of OSCAR as , where we initialize OSCAR without the optional object label inputs . We then follow Li \etal~\cite{oscar} for processing text sequences, but introduce the following adaptations on image representations.

Rather than including a set of regional features, we pre-process images through an ImageNet pre-trained ResNet~\cite{he2015deep} model and extract features from before the final FC-layer. We then process these features through a (newly) learned FC-layer and -normalization to give a single image feature  as the input to OSCAR. This same feature representation is used for the corpus of candidate target images  as shown in \figref{fig:model-0}.

We choose this relatively simple design for two reasons. First, recent work (\eg,~\cite{hong2020recurrent}) has shown the compatibility between VLP models and non-regional features of images. Second, we hypothesize that using global image features is easier to achieve our goal of modifying  conditioned on  so as to closely match .

\paragraph{Output token.}
As shown in \figref{fig:model-0}, contrary to typical downstream tasks, we do not use the final representation of the \texttt{[CLS]} token as the text-image joint embedding. Instead, we extract the representation corresponding to the image feature token and treat it as the composed image-text feature.
This resembles the fine-tuning of REF~\cite{li2019_visualbert}, as well as VLN-BERT~\cite{hong2020recurrent}. In both cases, tokens other than \texttt{[CLS]} are used for prediction. 
For composed image retrieval, our design makes sense since the transformer model includes residual connections between input and output tokens. Intuitively, the reference image features are \textit{modified} by aggregating the information from other word tokens to produce the target image features.


\paragraph{Metric learning.}
We use soft triplet-based loss with -norm distance as in Vo \etal~\cite{Vo_2019_tirg} to bring the composed image-text feature closer to the feature of the target image (positive pair), while pulling apart the features of negative pairs. 
In essence, given the -th positive pair  and an arbitrary negative  among all negatives , the loss is computed as:

where  is -norm distance. In training, we randomly sample the negative for each pair and average the loss over all sampled triplets .

\section{The \dstname Dataset}\label{dataset}
 
 Existing datasets for composed image retrieval~\cite{Vo_2019_tirg,fashioniq} contain training and testing examples as triplets  where  forms the query and  is (an example of) the desired target from a large image corpus .
 However, these existing datasets have two major shortcomings.
 First, they lack the sufficient visual complexity to facilitate the study of one of the major challenges in composed image retrieval, which is the subtle reasoning over what aspects are important and what shall be ignored.
 Second, since the candidate images cannot be extensively labeled for each  pair, existing datasets contain many false-negatives. That is, images  that are valid matches for the query but not labeled as the ground-truth target . Indeed, all images in  are considered as negatives. To circumvent this shortcoming, existing works choose to evaluate models with Recall and set  to larger values (\eg, 10, 50~\cite{fashioniq}), thus accounting for the presence of false-negatives. 
 However, the issue persists during training.
 Moreover, by setting larger  values, these methods are essentially trading in their ability for learning detailed text-image modifications.
 
 To mitigate these issues, we introduce the \dstnamefull (\dstname) dataset, which includes over 36,000 annotated query-target pairs, . Unlike existing datasets, we collect the modifying text to distinguish the target from a set of similar images (addressing the problem of false-negatives) and creating challenging examples that require careful consideration of visual and textual cues. Details are as follows.

 \subsection{Data Collection}
 We first form image pairs then collect related annotations by crowd-sourcing.
 The pairs are drawn from subsets of images, as described below. This strategy plays a major role in mitigating the issue of false negatives (see \secref{sec:extended_metric}).
 \figref{fig:dset_collection_0} outlines our data collection procedure.
 
 \paragraph{Image source.} We use the popular NLVR dataset for natural language visual reasoning~\cite{Suhr_2019_nlvr2} as our source of images.
 We choose NLVR for several reasons. 
 First, it contains images of real-world entities with reasonable complexity in ImageNet-type~\cite{krizhevsky2012imagenet}. 
 Second, the setup of our task requires image in pairs that are similar enough, and NLVR is designed to have collections of similar images regarding 1,000 synsets (\eg, acorn, seawall). 
 Also, \citet{Suhr_2019_nlvr2} employs an additional step to manually remove non-interesting images, thus ensuring the content quality.
 
 \paragraph{Image subset construction.} \label{sec:dset_imgset}
 The nature of our task requires collections of negative images with high visual similarity, as otherwise, it would be trivial to discriminate between the reference and target image.
 Thus, prior to forming reference-target image pairs, we construct multiple subsets of six images that are semantically and visually similar, denoted as , shown in \figref{fig:dset_collection_0}(a).
 
 Here, to construct a subset, we randomly pick one image from the large corpus . We then sort the remaining images in  by their cosine similarity to  using ResNet152~\cite{he2015deep} image feature vectors pre-trained on ImageNet~\cite{krizhevsky2012imagenet}. Denote by  the cosine similarity for image . We then pick five additional images to produce a similar yet diverse subset, as follows: First, we filter out images with  to avoid near-identical images to . Then for the next top-20 ranked images, we greedily add each image in turn, skipping an image if its cosine similarity is within 0.002 of the last image added. If a subset of size six cannot be created, then the entire set is discarded.
 
Once constructed we further filter the collection subsets to avoid heavy overlap. We obtain in total 52,732 subsets from NLVR, from which we randomly choose 4,351 for the construction of \dstname.
 
 \paragraph{Image pairing.}
 Within each constructed image subset , we draw nine pairs of images, as shown in \figref{fig:dset_collection_0}(b).
 We choose these pairs to have (1) consecutive modifications that will allow future training of a dialogue systems; and (2) multiple outcomes from the same reference image.

 \begin{table*}[t!]
   \centering \scalebox{0.70}{
     \centering
     \renewcommand{\arraystretch}{1.05} 

     \begin{tabular}{ll cc l} 
       \toprule
       \multicolumn{1}{l}{\multirow{2}{*}{}} & \multicolumn{1}{l}{\multirow{2}{*}{Semantic aspect}} & \multicolumn{2}{c}{Coverage (\%)}         & \multirow{2}{*}{Example (boldface added here for emphasis)}                                                                    \\ 
& & \dstname & \multicolumn{1}{c}{Fashion-IQ} &                                                                                             \\ 
       \midrule
       1 & Cardinality                                   & 29.3 & --                              & Only \textbf{one} of the boars and the ground is browner.                                   \\
       2 & Addition                                      & 15.2 & 15.7                          & \textbf{Add} human feet and a collar.                                                       \\
       3 & Negation                                      & 11.9 & ~4.0                  & \textbf{Remove} the chair, make the dog sit in an open box.                                 \\ 
       \midrule
       4 & Direct Addressing                             & 57.4 & 49.0                   & Show some lemons with a glass of lemonade.                                                  \\
       5 & Compare \& Change                               & 31.7 & ~3.0                           & \textbf{Same computer but} different finish and black background.                                  \\
       6 & Comparative Statement                                  & 51.7 & 32.0                   & A \textbf{bigger} pub with \textbf{more} people on it.                                      \\
       7 & Statement with Conjunction                                & 43.7 & 19.0                   & Remove all but one bird \textbf{and} have it facing right \textbf{and} putting food in its mouth.  \\ 
       \midrule
       8 & Spatial Relations \& Background               & 61.4 & --                              & Change the sky to blue color.                                                               \\
       9 & Viewpoint                                     & 12.7 & --                              & Focus widely on all available cookies package.                                              \\
\specialrule{\heavyrulewidth}{2pt}{0pt}
       \rowcolor{Gray}
       & \textit{Avg. Sentence length (words)} & 11.3 & 5.3                              &                                               \\
       \bottomrule
       \end{tabular}} \3.5ex]
      \multicolumn{10}{p{0.99\linewidth}}{
        \textbf{(a)} Randomly pick an image as  (leftmost), sort the remaining images in the large image corpus  by their cosine similarity to  using ResNet features pre-trained on ImageNet, noted as  for . Images are ranked from left to right.
      }\\
      \hline
      &
      &      
      &      
      &      
      &      
      &      
      &      
      &
      &
      \\
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-0}}& 
\frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-2}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-3}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-4}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-5}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-6}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-7}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-8}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-9}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-n}} \\
      &
      0.9981&      
0.8691&      
      0.8663&      
      0.8603&      
      0.8490&      
      0.8488&
      0.8456&
      0.8435&
      ... \\
      \hline
      \multicolumn{10}{p{0.99\linewidth}}{} \0.5ex]
      \multicolumn{10}{p{0.99\linewidth}}{
        \textbf{(c)} Select the next top-20 ranked images (not fully shown below).
      } \\
      \hline
          
      & \multicolumn{2}{c}{ Shifted} 
      &
      &      
      &      
      &      
      &
      &
      &
       top-20\\
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-0}}& 
\frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-3}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-4}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-5}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-6}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-7}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-8}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-9}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-10}}&
      \frame{\includegraphics[height=8.0ex]{imgs_sup/exp0-n}} \\
      &
0.8691&      
      0.8663&      
      0.8603&      
      0.8490&      
      0.8488&
      0.8456&
      0.8435&
      0.8421&
      ... \\
      \hline
    \multicolumn{10}{p{0.99\linewidth}}{} \0.5ex]
      \multicolumn{10}{p{0.99\linewidth}}{
        \textbf{(e)} Form an image subset  if 6 images can be greedily added (true for this example), otherwise discard the entire set and restart at \textbf{(a)}.
      }
    \end{tabular}
    \end{minipage}
    \0.5ex]
      \multicolumn{6}{p{0.85\linewidth}}{
        : Turn on the flat screen tv in the living room.
      }    \\
      \multicolumn{6}{p{0.85\linewidth}}{
        : Pull up the blinds to let in sunlight.
      }    \\
      \multicolumn{6}{p{0.85\linewidth}}{
        : Put a window by the fireplace.
      }    \\
      \multicolumn{6}{p{0.85\linewidth}}{
        : Have a window on the right-hand wall.
      }    \\
      \multicolumn{6}{p{0.85\linewidth}}{
        : Have a bookshelf to the right of the window.
      }    \\
      \multicolumn{6}{p{0.85\linewidth}}{
        : Have multiple television screens.
      }    \\
      \multicolumn{6}{l}{}   
      \end{tabular}
\end{minipage}
\caption{Left: The six pairs we draw from a subset (in total we draw nine) that form a closed-loop dialogue. Each arrow represents a reference-to-target image pair with modification sentences.
  Right: An example of consecutive modification sentences that forms a dialogue.}
  \label{fig:dialogue_0}
\end{figure*}

\begin{figure*}[!ht]
  \centering\footnotesize
  \noindent
  \begin{minipage}{.25\textwidth}
    \centering
    \includegraphics[height=20.0ex]{imgs_sup/subset_to_pair_1}
  \end{minipage}\begin{minipage}{.75\textwidth}
    \centering\setlength{\tabcolsep}{2pt}
    \begin{tabular}{cccccc}
      &  &  &  &  &          \\
      \frame{\includegraphics[height=12.0ex]{imgs_sup/exp1-0-train-4025-0-img0}}&  
      \frame{\includegraphics[height=12.0ex]{imgs_sup/exp1-5-train-11850-1-img1}}&  
      \frame{\includegraphics[height=12.0ex]{imgs_sup/exp1-4-train-2379-1-img1}}&  
      \frame{\includegraphics[height=12.0ex]{imgs_sup/exp1-3-train-7125-1-img1}}&  
      \frame{\includegraphics[height=12.0ex]{imgs_sup/exp1-2-train-7537-3-img0}}&          
      \frame{\includegraphics[height=12.0ex]{imgs_sup/exp1-1-train-10458-2-img1}}\\ 
      \1pt]
\begin{minipage}{.98\textwidth}
  \fontsize{7pt}{8pt}\selectfont
   Nb. pairs not pre-defined, pairs are generated on-the-fly.\1.75pt]
   Each pair has two sentences.\1.75pt]
\end{minipage}
  \caption{Comparison between CIRR (bolded) and existing datasets for composed image retrieval. CIRR is comparable in size (nb. pairs) while containing richer annotations of open-domain images.}
  \label{tab:comparison_dst}
  \end{table*}
 
\tabref{tab:comparison_dst} compares CIRR with existing datasets used for composed image retrieval. We demonstrate that CIRR is comparable in size with existing datasets. Additionally, it provides rich auxiliary annotations for open-domain images.

\paragraph{False-negative analysis.}
\shadowoffset{2pt}
\begin{figure*}[!ht]
  \centering\footnotesize
  \begin{minipage}{0.98\linewidth}
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lcccccc}
      \textbf{(a)}&
      \multicolumn{6}{l}{Is shiny and silver with shorter sleeves  fit and flare.}  
      \\
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_0-0}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_0-1}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_0-2}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_0-3}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_0-4}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_0-5}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_0-6}}\\
      \multicolumn{7}{l}{} \1ex]
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_1-0}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_1-1}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_1-2}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_1-3}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_1-4}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_1-5}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_1-6}}\\
      \multicolumn{7}{l}{} \1ex]
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_2-0}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_2-1}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_2-2}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_2-3}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_2-4}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_2-5}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_2-6}}\\
      \multicolumn{7}{l}{} \1ex]
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_3-0}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_3-1}}& 
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_3-2}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_3-3}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_3-4}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_3-5}}&
      \frame{\includegraphics[height=15.0ex]{imgs_sup/fiq_3-6}}\\
    \end{tabular}
    \end{minipage}
  \caption{Examples of false-negatives in Fashion-IQ~\cite{fashioniq}. First column shows the reference image. Each sample contains two modification sentences. For each query set (reference image + modification sentences), only one candidate image is labeled as the target. Thus, rendering the remaining valid predictions as false-negatives.}
  \label{fig:false-negatives}
\end{figure*}
 \figref{fig:false-negatives} demonstrates the presence of false-negatives in Fashion-IQ~\cite{fashioniq}, as explained in \secref{dataset}. 
For comparison, our data collection procedures ensure that no false-negatives are present within each image subset, as discussed in \secref{sec:sup_subset}. Examples of CIRR are shown in \figref{fig:dialogue_0}, \figref{fig:dialogue_1}, and \figref{fig:cirr_examples}.

\section{Additional Examples of \dstname} \label{sec:sup_examples}
\shadowoffset{2pt}
\setlength{\fboxsep}{0pt}
\begin{figure*}[!ht]
  \centering\footnotesize
  \begin{minipage}{0.98\linewidth}
    \centering
    \setlength{\tabcolsep}{1.5pt}
    \begin{tabular}{lccccc}
\textbf{(a)}&
      \\
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_1-1-0}}& 
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_1-1-1}}& 
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_1-1-5}}&
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_1-1-3}}&
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_1-1-4}}&
      \textcolor{ForestGreen}{\fboxrule=2pt\fbox{\includegraphics[height=12.0ex]{imgs_sup/cirr_1-1-2}}}\\
      &\multicolumn{5}{l}{
        \textit{Main} -- Goes from a black and white dog running to two dogs running.
      }  \\
      &\multicolumn{5}{l}{
        \textit{Q1} -- [N/A] Nothing worth mentioning
      }\\
      &\multicolumn{5}{l}{
        \textit{Q2} -- \textbf{Change to a brown-and-white dog and a black-and-white dog.}
      }\\
      &\multicolumn{5}{l}{
        \textit{Q3} -- [N/A] Nothing worth mentioning
      }\\
      &\multicolumn{5}{l}{
        \textit{Q4} -- Make the grass a darer green.
      }\0.05ex]
\multicolumn{6}{l}{}\\
      \textbf{(c)}&
\\
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_2-1-0}}& 
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_2-1-4}}& 
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_2-1-3}}&
      \textcolor{ForestGreen}{\fboxrule=2pt\fbox{\includegraphics[height=12.0ex]{imgs_sup/cirr_2-1-2}}}&
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_2-1-1}}&
      \frame{\includegraphics[height=12.0ex]{imgs_sup/cirr_2-1-5}}\\
      &\multicolumn{5}{l}{
        \textit{Main} -- Remove the seashells and make the water green.
      }  \\
      &\multicolumn{5}{l}{
        \textit{Q1} -- Shows manta rays.
      }\\
      &\multicolumn{5}{l}{
        \textit{Q2} -- Make the rays older, spread the rays further apart.
      }\\
      &\multicolumn{5}{l}{
        \textit{Q3} -- \textbf{View straight on.}
      }\\
      &\multicolumn{5}{l}{
        \textit{Q4} -- [N/A] Covered in main annotation
      }\0.05ex]
\end{tabular}
    \end{minipage}\3pt]
  \begin{minipage}{.95\textwidth}
    \centering
    \fbox{
    \includegraphics[width=95.0ex]{imgs_sup/P2.23_dev_v3_1.pdf}
    }
  \end{minipage}
  \^\dagger^\ddagger^2^{\S}I_\text{1}I_\text{6}\ast\dagger\ddagger{\S}$1.75pt]
\end{minipage}
  \caption{Data structure as in the data files. For details please refer to our project website.}
  \label{tab:json}
  \end{table*}
   \tabref{tab:json} summarizes information we provide for each image pair.
 \end{document}
