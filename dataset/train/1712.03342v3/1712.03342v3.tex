\section{Experimental Evaluations}
\label{sec:results}

\begin{figure*}
    \captionsetup[subfigure]{labelformat=empty}
    \centering


    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_dsoA_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_dsoA_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_posenet_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_posenet_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_pgo_dso_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_pgo_dso_0_q.pdf}
    \end{subfigure}

    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/heads_dsoA_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/heads_dsoA_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/heads_posenet_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/heads_posenet_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/heads_vidvo_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/heads_vidvo_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/heads_vidvo_online_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/heads_vidvo_online_0_q.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/heads_vidvo_online_pgo_dso_0_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/heads_vidvo_online_pgo_dso_0_q.pdf}
    \end{subfigure}






    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_dsoA_3_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_dsoA_3_q.pdf}
        \caption{DSO~\cite{Engel2017DSO}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_posenet_3_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_posenet_3_q.pdf}
        \caption{PoseNet~\cite{Kendall17cvpr, Kendall15iccv, Kendall16icra}}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_3_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_3_q.pdf}
        \caption{MapNet}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_3_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_3_q.pdf}
        \caption{MapNet+}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_pgo_dso_3_t.pdf}
        \includegraphics[width=\linewidth]{figures/7scenes/redkitchen_vidvo_online_pgo_dso_3_q.pdf}
        \caption{MapNet+PGO}
    \end{subfigure}
    \vspace{-1em}
    \caption{\small \textbf{Camera localization results on 7-Scenes dataset~\cite{Shotton13Scene7}}.
    For each subfigure, the top 3D plot shows the camera trajectory (green for the ground truth and red for the prediction), and the bottom color bar 
    shows rotation error for all the frames. From top to bottom, the three testing sequences are: Redkitchen-seq-03, Heads-seq-01, and Redkitchen-seq-12.
    See Table~\ref{tab:result_7scenes} for quantitative comparison.}
    \label{fig:map_compare}
    \vspace{-1em}
\end{figure*}

 
\paragraph{Datasets} We evaluate our algorithms on two well-known public datasets ---
7-Scenes~\cite{Shotton13Scene7} for small-scale, indoor, AR/VR-type
scenarios, and Oxford RobotCar~\cite{RobotCarDatasetIJRR} for
large-scale, outdoor, autonomous driving scenarios. 7-Scenes contains RGB-D image sequences of seven indoor
environments (with the spatial extent less than 4 meters) captured with a
Kinect sensor. Multiple sequences were captured for each environment, and each
sequence is 500 or 1000 frames. The ground truth camera poses are
obtained with KinectFusion. The 7-Scenes dataset has recently
been  evaluated extensively as a benchmark~\cite{Kendall15iccv, Kendall16icra,
Kendall17cvpr,
Melekhov17Hourglass,Walch17LSTM,Clark17VidLoc,Brachmann17RANSAC}, which makes
it ideal for us to compare with prior state-of-the-art methods.


Oxford RobotCar contains over 100 repetitions of a consistent route (about 10km)
through central Oxford captured twice a week over a period of over a year. Thus
the dataset captures different combinations of weather, traffic, pedestrians,
construction and roadworks. In addition to the images captured with the six
cameras mounted on the car, the dataset also contains LIDAR, GPS and INS
measurements, as well as stereo visual odometry (VO).
We extracted two subsets from this dataset: LOOP (Fig.~\ref{fig:teaser})
with a total length of 1120m, which was also used in VidLoc~\cite{Clark17VidLoc}, and
FULL (Fig.~\ref{fig:map_compare_robotcar_full}) with a total length
of 9562m. Details of the training, validation, and testing sequences are provided
in the supplementary material.

\vspace{-1em}
\paragraph{Baselines and Data Augmentation}

We compare our approach with two groups of prior methods on 7-Scenes. For the DNN-based
prior work, we compare with PoseNet15~\cite{Kendall15iccv},
PoseNet16~\cite{Kendall16icra}, PoseNet17~\cite{Kendall17cvpr},
Hourglass~\cite{Melekhov17Hourglass}, LSTM-PoseNet~\cite{Walch17LSTM}, and
VidLoc~\cite{Clark17VidLoc}.  For the traditional visual odometry based
methods, we used DSO~\cite{Engel2017DSO} to compute the VO and integrate to obtain camera poses. We run the DSO with images at the same spatial resolution as 
MapNet. On Oxford RobotCar, only VidLoc~\cite{Clark17VidLoc} reported
results on the LOOP scene but it did not provide training and
testing sequences. Thus, the two baselines to compare are 
the provided stereo VO, as well as our version of
PoseNet (with ). In RobotCar, we randomly perturb the brightness,
saturation, hue and contrast of images during training for experiments, which
we found essential for performing cross-weather and cross-time
localization. 




\subsection{Experiments on the 7-Scenes Dataset}



\begin{table}
    \footnotesize
    \centering
    \caption{\small Translation and rotation error on the 7-Scenes dataset.}
    \vspace{-1em}
    \begin{tabular}{ll|ll}
        \toprule
        \multirow{2}{*}{Scene}       & PoseNet17            & PoseNet      & PoseNet+ \\ 
                                     & \cite{Kendall17cvpr}  & (ResNet34)   & (ResNet34)              \\
        \midrule
        Chess       & 0.13m, 4.48\degree	 & {\bf 0.11m, 4.24\degree}  & {\bf 0.11m}, 4.29\degree  \\
        Fire        & {\bf 0.27m, 11.30\degree} & 0.29m, 11.68\degree & {\bf 0.27m}, 12.13\degree \\
        Heads       & {\bf 0.17m}, 13.00\degree	 & 0.20m, 13.11\degree & 0.19m, {\bf 12.15\degree} \\
        Office      & {\bf 0.19m, 5.55\degree}	 & {\bf 0.19m}, 6.40\degree  & {\bf 0.19m}, 6.35\degree  \\
        Pumpkin     & 0.26m, {\bf 4.75\degree}	 & 0.23m, 5.77\degree  & {\bf 0.22m}, 5.05\degree  \\
        Red Kitchen & {\bf 0.23m}, 5.35\degree	 & 0.27m, 5.81\degree  & 0.25m, {\bf 5.27\degree}  \\
        Stairs      & 0.35m, 12.40\degree   & 0.31m, 12.43\degree & {\bf 0.30m, 11.29\degree} \\
        \midrule
        Average     & 0.23m, 8.12\degree    & 0.23m, 8.49\degree  & {\bf 0.22m, 8.07\degree}   \\
        \bottomrule
    \end{tabular}
    \label{tab:result_logq}
    \vspace{-1em}
\end{table}


\vspace{-.5em}
\paragraph{Effects of Rotation Parameteriation} 

In Section~\ref{subsec:regress}, we introduced a new parameterization of camera
orientation for PoseNet and used ResNet34 as the base network.
Table~\ref{tab:result_logq} shows the quantitative results of these
modifications to the baseline PoseNet. Following the same convention of prior
work~\cite{Kendall15iccv,Kendall16icra,Kendall17cvpr,Melekhov17Hourglass,Walch17LSTM,Clark17VidLoc},
we compute the median error for camera translation and rotation.\footnote{Other statistics
of the camera pose estimation errors are also provided in the supplementary material, which
support the same conclusion.}
As shown, our proposed rotation parameterization does improve
performance. 

\begin{table*}
    \footnotesize
    \centering
    \caption{\small Translation error (m) and rotation error (\degree) for various methods on the 7-Scenes dataset~\cite{Shotton13Scene7}.}
    \vspace{-1em}
    \begin{tabular}{llllll|lll}
        \toprule
\multirow{2}{*}{Scene}   & PoseNet17 & Hourglass & LSTM-Pose & VidLoc & DSO & MapNet & MapNet+ & MapNet+PGO \\
                & \cite{Kendall17cvpr} & \cite{Melekhov17Hourglass} & \cite{Walch17LSTM} & \cite{Clark17VidLoc} & \cite{Engel2017DSO} &
                 &  &  \\
        \midrule
        Chess       &  0.13m, 4.48\degree   & 0.15m, 6.17\degree  & 0.24m, 5.77\degree & 0.18m, NA & 0.17m, 8.13\degree & {\bf 0.08m}, 3.25\degree  & 0.10m, {\bf 3.17\degree}  & 0.09m, 3.24\degree \\
        Fire        &  0.27m, 11.30\degree  & 0.27m, 10.84\degree & 0.34m, 11.9\degree & 0.26m, NA & {\bf 0.19m}, 65.0\degree & 0.27m, 11.69\degree & 0.20m, {\bf 9.04\degree}  & 0.20m, 9.29\degree \\
        Heads       &  0.17m, 13.00\degree  & 0.19m, 11.63\degree & 0.21m, 13.7\degree & 0.14m, NA & 0.61m, 68.2\degree & 0.18m, 13.25\degree & 0.13m, 11.13\degree & {\bf 0.12m, 8.45\degree} \\
        Office      &  0.19m, 5.55\degree   & 0.21m, 8.48\degree  & 0.30m, 8.08\degree & 0.26m, NA & 1.51m, 16.8\degree & {\bf 0.17m, 5.15\degree}  & 0.18m, 5.38\degree  & 0.19m, 5.42\degree\\
        Pumpkin     &  0.26m, 4.75\degree   & 0.25m, 7.01\degree  & 0.33m, 7.00\degree & 0.36m, NA & 0.61m, 15.8\degree & 0.22m, 4.02\degree  & {\bf 0.19m, 3.92\degree}  & {\bf 0.19m}, 3.96\degree \\
        Kitchen &  0.23m, 5.35\degree   & 0.27m, 10.15\degree & 0.37m, 8.83\degree & 0.31m, NA & 0.23m, 10.9\degree & 0.23m, 4.93\degree  & 0.20m, 5.01\degree  & {\bf 0.20m, 4.94\degree} \\
        Stairs      &  0.35m, 12.40\degree  & 0.29m, 12.46\degree & 0.40m, 13.7\degree & {\bf 0.26m}, NA & 0.26m, 21.3\degree & 0.30m, 12.08\degree & 0.30m, 13.37\degree & 0.27m, {\bf 10.57\degree} \\
        \midrule
        Average     &  0.23m, 8.12\degree   & 0.23m, 9.53\degree  & 0.31m, 9.85\degree & 0.25m, NA & 0.26m, 29.4\degree & 0.21m, 7.77\degree & 0.19m, 7.29\degree   & {\bf 0.18m, 6.55\degree}\\
        \bottomrule
    \end{tabular}
    \label{tab:result_7scenes}
    \vspace{-.5em}
\end{table*}




\vspace{-1em}
\paragraph{Comparison with Prior Methods}

Figure~\ref{fig:map_compare} shows the camera trajectories for several testing
sequences from the 7-Scenes dataset for DSO VO, PoseNet, MapNet, MapNet+, and MapNet+PGO.
Table~\ref{tab:result_7scenes} shows quantitative comparisons.
The unlabeled data used to fine-tune MapNet+ for these experiments are the unlabeled test sequences.
This is a transductive learning scenario~\cite{Chapelle06book, Segonne08transduction}.
As shown, DSO often drifts over time and PoseNet results in noisy predictions. In contrast, by
including various geometric constraints into network training and inference our proposed approaches
MapNet, MapNet+ and MapNet+PGO successively improve the performance.
A complete table for all testing sequences from the 7-Scenes dataset is included in the
supplementary material.  


















\begin{figure*}[h!]
    \small
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/stereo_v2A_t.pdf}
        \vspace{-1.5em}
        \caption{\small Stereo VO (40.20m, 12.85\degree) }
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/posenet_t.pdf}
        \vspace{-1.5em}
        \caption{\small PoseNet (25.29m, 17.45\degree)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/vidvo_v2_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet (9.84m, 3.96\degree)}
    \end{subfigure}
    \hfill
    \rulesep
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/gps_t.pdf}
        \vspace{-1.5em}
        \caption{\small GPS (7.03m, NA) }
    \end{subfigure}

    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/vidvo_online_1seq_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet+(1seq) (8.17m, 2.62\degree)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/vidvo_online_2seq_v2_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet+(2seq) (6.95m, 2.38\degree)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/vidvo_online_pgo_stereo_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet+PGO ({\bf 6.73m, 2.23\degree})}
    \end{subfigure}
    \hfill
    \rulesep
    \begin{subfigure}{0.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/vidvo_online_gps_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet+(GPS) ({\bf 6.78m, 2.72\degree})}
    \end{subfigure}
    \vspace{-.5em} 
    \caption{\small \textbf{Camera localization results on the LOOP scene (1120m long) of the
    Oxford RobotCar dataset~\cite{RobotCarDatasetIJRR}}. The ground truth camera trajectory
    is the black line, the star indicates the first frame, and the red lines show the
    camera pose predictions. The caption of each figure shows the mean translation error (m) and mean rotation error (\degree).
    MapNet+(1seq) uses one unlabeled sequence, while MapNet+(2seq) uses two unlabeled sequences.
    {\bf Left}: MapNet+ trained with unlabeled images and stereo VO. {\bf Right}: MapNet+ trained with unlabeled images and GPS data.}
    \label{fig:map_compare_robotcar}
\end{figure*}
 \begin{figure*}
    \captionsetup[subfigure]{labelformat=empty}
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar_full/stereoA_t.pdf}
        \vspace{-1.5em}
        \caption{\small Stereo VO (222.0m, 13.7\degree)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar_full/posenet_t.pdf}
        \vspace{-1.5em}
        \caption{\small PoseNet (125.6m, 27.1\degree)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar_full/vidvo_v2_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet (41.4m, 12.5\degree)}
    \end{subfigure}

    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/robotcar_full/err_t_v2.pdf}
        \vspace{-.5em}
        \caption{\small Cumulative Distribution Translation Error}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar_full/vidvo_online_v2_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet+ (30.3m, 7.8\degree)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar_full/vidvo_online_pgo_stereo_v2_t.pdf}
        \vspace{-1.5em}
        \caption{\small MapNet+PGO ({\bf 29.5m, 7.8\degree})}
    \end{subfigure}
    \vspace{-1em}
    \caption{\small Comparison of camera localization results on the FULL scene (9562m long) of the
    Oxford RobotCar dataset~\cite{RobotCarDatasetIJRR}. The ground truth camera trajectory
    is the black line, and the star indicates the first frame. The red lines show the
    results of stereo VO (provided by the dataset),
    our version of PoseNet+, MapNet, and its variations. The caption
    of each figure shows the mean translation error (m) and mean rotation error (\degree).
    A plot of the cumulative distribution of the translation error is also included.}
    \label{fig:map_compare_robotcar_full}
\end{figure*}

 
\begin{figure}
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/err_t1.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/robotcar/err_t_gps3d.pdf}
    \end{subfigure}
    \vspace{-1em} 
    \caption{\small Cumulative distributions of the translation errors (m) for all the methods 
    evaluated on Oxford RobotCar LOOP. -axis is the translation error and -axis is the percentage of frames with error less than
    the value.} 
    \label{fig:res_err_robotcar}
\end{figure}


\subsection{Experiments on the Oxford RobotCar Dataset}

\paragraph{Results on the LOOP Route}
We first train a baseline PoseNet (with the  parameterization for rotation) and a MapNet model using 
two labelled sequences captured on the LOOP route under cloudy weather,
while the testing sequence is captured under sunny weather. We then perform two experiments with different auxiliary data for MapNet+.

In the first experiment, MapNet+ is trained on additional unlabeled LOOP sequences separate from the testing sequence,
with stereo VO provided with the dataset. To tease apart the influence of labeled and unlabeled data in the effectiveness of our MapNet+ models,
we train them with varying amounts of labeled (one to two sequences)
and unlabeled data (zero to three sequences). Figure~\ref{fig:loop_ablation_robotcar} shows the mean
translation and rotation errors of these models on the testing sequence.
While labeled data is clearly more important than an equal amount of unlabeled data,
we show that unlabeled data does consistantly improve performance as more becomes available. This trend bodes well
for real-world scenarios, where the amount of unlabeled data available far exceeds the amount of labeled data.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation_position.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation_rotation.pdf}
    \end{subfigure}
    \vspace{-1em} 
    \caption{\small \textbf{Left}: Mean translation error (m) and \textbf{right}: mean rotation error (\degree) for MapNet+ models trained with 
    varying amounts of labeled and unlabeled data on the Oxford RobotCar LOOP sequence. 
    X-axis indicates the number of labeled sequences (1-2), Y-axis indicates the number of unlabeled sequences (0-3) and Z-axis indicates the error.}
    \label{fig:loop_ablation_robotcar}
\end{figure}


In the second experiment, MapNet+ is trained with GPS \ie, 
the dataset  contains two sequences of images and their GPS locations, which are separate from the testing sequence.
Since GPS measurements are sparse
(less than 10\% of images have corresponding GPS measurements), we
first linearly interpolate GPS measurements for entire sequence. We
define the loss of auxillary data  in Equation~(\ref{eq:mapnet+})
as , where  is the linearly interpolated GPS measurement of the 2D
camera location. 

Figure~\ref{fig:map_compare_robotcar} shows the estimated camera poses for all
the methods in these two experiments along with the mean translation error (m) and rotation
error (\degree). 
Figure~\ref{fig:res_err_robotcar} shows the cumulative distributions of
the translation errors for all the methods on the LOOP route. In both figures, 
the left part shows that MapNet significantly improves the estimation compared to PoseNet and stereo VO.
MapNet+ and MapNet+PGO further improve the pose predictions. 
The right part shows that by fusing GPS signals,
MapNet+(GPS) obtains better results compared to MapNet and GPS alone.

\vspace{-.5em}
\paragraph{Results on the FULL Route}




We also evaluated our approach on the challenging 9562 m long FULL route of the Oxford RobotCar
dataset. Figure~\ref{fig:map_compare_robotcar_full} shows the results of all the
models with mean translation error (m) and rotation error (\degree). 
MapNet significantly outperforms the baseline PoseNet (both trained for 100 epochs) and the stereo VO (provided by the dataset).
By fusing the stereo VO information, MapNet+ and MapNet+PGO further improve the result.
The cumulative distributions of the translation errors also show the large improvement over
the baselines. 

Note that there are some outlier predictions in both LOOP (Fig.~\ref{fig:map_compare_robotcar}) and FULL (Fig.~\ref{fig:map_compare_robotcar_full}).
These often correspond to images with large over-exposed regions, and can be filtered out with simple post-processing (e.g. temporal median filtering)
as shown in the supplementary material. We also computed saliency maps  (magnitude gradient of the mean of the 6-element output w.r.t. input image, maxed over the 3 color channels) for PoseNet
and MapNet+ on both the 7-scenes and RobotCar dataset. We find that compared to PoseNet, MapNet+ focuses more on geometrically meaningful regions
and its saliency map is more consistent over time. Examples are shown in the supplementary material.











