

\documentclass[10pt,twocolumn,letterpaper]{article}


\usepackage{cvpr}              \usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\definecolor{yellow}{rgb}{0.89, 0.82, 0.2}
\definecolor{green}{rgb}{0.2, 0.9, 0.4}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\yellow}[1]{\textcolor{yellow}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\usepackage{algorithm}
\algnewcommand{\LineComment}[1]{\State  #1}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\gnote}[1]{{\color{red}{[Guy: #1]}}}
\newcommand{\ynote}[1]{{\color{blue}{[Yossi: #1]}}}



\usepackage[dvipsnames]{xcolor}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}


 
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\def\paperID{328} \def\confName{3DV\xspace}
\def\confYear{2024\xspace}

\title{Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI}

\author{Meir Yossef Levi, Guy Gilboa\\
Viterbi Faculty of Electrical and Computer Engineering \\
Technion - Israel Institute of Technology, Haifa, Israel\\
{\tt\small me.levi@campus.technion.ac.il ; guy.gilboa@ee.technion.ac.il}
}

\begin{document}
\maketitle
\begin{abstract}
The ability to cope accurately and fast with 
Out-Of-Distribution (OOD) samples is crucial
in real-world safety demanding applications. 
In this work we first study
the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points.
We generalize the notion of critical points into importance measures. We  show that training a classification network based only on 
less important points  dramatically improves robustness, at a cost of minor performance loss on the clean set.
We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute.  We show it can be used for
a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We reach SOTA results on the two latter tasks. 
\end{abstract}

\section{Introduction}
Ranking the importance of points within a point cloud is fundamental for gaining deeper understanding and for improving the network's performance in various tasks. Being able to compute importance fast, without resorting to gradient computations, can be of great advantage. It allows to use this knowledge at inference, providing additional capabilities for the network. 
In this paper we present a very fast way to obtain importance for point cloud classification. The method generalizes the binary notion of
\textit{Critical Points}\cite{pointnet} into either a (discrete) rank measure or a (continuous) soft measure. We show how  importance  can be used to filter out points and to gain by that out-of-distribution (OOD) robustness, to defend against adversarial attacks, to remove outliers and to visualize the data.


\begin{figure}[ptbh!]
    \centering
    \captionsetup[subfigure]{justification=centering}
\begin{subfigure}[t]{0.24\linewidth} \includegraphics[width=1\linewidth]{figs/teaser_critical.pdf}
\caption{{\bf  Critical Points}}
        \label{subfig:teaser_critical}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth} \includegraphics[width=1\linewidth]{figs/teaser_soft.pdf}
\caption{{\bf Soft Measure (Ours)} }    
        \label{subfig:teaser_soft}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth} \includegraphics[width=1\linewidth]{figs/teaser_sor.pdf}
\caption{Outlier removal by {\bf  SOR}}
        \label{subfig:teaser_sor}
    \end{subfigure}
    \begin{subfigure}[t]{0.20\linewidth} \includegraphics[width=1\linewidth, height=2.1\linewidth]{figs/teaser_ours.pdf}
\caption{Outlier removal - {\bf  Ours}}
        \label{subfig:teaser_ours}
    \end{subfigure}
    \caption{{\bf Critical Points vs. Critical Points++.} \yellow{Yellow}-Critical, \blue{Blue}-Uncritical. \red{Red}-Corrupted, \green{green}-UnCorrupted. \textit{Critical Points} and our \textit{Soft Measure} on a bookshelf corrupted by Add-Local. Filter is applied using Statistical Outlier Removal (SOR), and our approach. Despite the corruption's smoothness (not contains distinct outliers), our method substantially filters it, whereas SOR produces inferior results.}
    \label{fig:critical_points}
\end{figure}





\textit{Critical Points} \cite{pointnet} are a subset of the most important input elements for a classification task. They are well known for being shape discriptive, and represent the most discriminative parts of the structure \cite{pointnet, deep_learning_in_an_adversarial_setting, Generating_3d_adversarial_point_clouds}. Moreover, they can be obtained by a simple close-form calculation, avoiding gradient derivation throughout the network.
In this work we generalize this notion and propose an importance measure for point clouds, which is computed efficiently, producing high fidelity results. We then show several applications where importance can be used. 
We first connect importance to classification robustness.

\textit{How importance can be used to improve robustness?}
Safety-critical applications require algorithms which are highly robust to noise and outliers.
It was shown in \cite{epic, point_cert, pointguard} that using ensembles of sampled point clouds can significantly increase OOD robustness. 
\begin{figure}[ptbh!]
  \centering
\includegraphics[width = 1\linewidth, height =0.6 
   \linewidth]{figs/timing.pdf}
   \caption{{\bf Robustness Vs. Inference Time (Log-Scale).} Our method is as fast as vanilla networks, with outstanding robustification, on-par with EPiC \cite{epic} (highlighted in blue). Combining our approach as extra sampling strategy achieves SOTA mCE (highlighted in green).}
   \label{fig:timing}
\end{figure}
The main reason is that sampling increases ensemble diversity and that some of the sampled point clouds are less exposed to the introduced corruptions \cite{epic}.
The previously proposed sampling mechanisms, however, were quite simplistic (\textit{Random}, \textit{Patch} or \textit{Random-Walk}) and required an entire ensemble to provide high quality robust results.
We show here that sampling guided by importance is a much more 
sophisticated strategy, which can compete well with an entire ensemble.

Our key observation is that corruptions are often interpreted by a classifier as critical, or important points (see Fig. \ref{fig:critical_points}). We thus filter the most important points and train a classifier on a harder task of classifying (the clean training set) based only on the ``uncritical'', or less important points.
This is done adaptively, based on a normalized entropy criterion to account for the degree of filtering. We found that by slightly decreasing
the accuracy on the clean data set we gain a considerable robustness boost on corrupted data. We reach SOTA-comparable performance, where computation time is reduced by almost two orders of magnitude  (see Fig. \ref{fig:timing}).
Settling for a slower algorithm, by combining our method in an ensemble, we surpass current SOTA by a considerable margin.










We found that the calculation of point importance can enhance a wide variety of applications. By screening out dominant input elements, we limit the ability of malicious processes to deliberately manipulate the point-cloud in order to deceive the classifier. Points getting further from the surface deviate from the training distribution, yielding higher importance. This, most probably, will cause their exclusion by the proposed point filtering method. We check our defense algorithm versus SOTA Shape-Invariant-Attack\cite{shape_invariant}, showing a dramatic reduction in attack success rate and an increase in query cost, compared to other common defenses.
Our main contributions are summarized as:
\begin{enumerate}
    \item We observe that OOD regions, with large deviation from training distribution, are more likely to be selected as critical points. Filtering out these prominent parts and training on a harder problem, with less significant features, yields more resilient and reliable networks.
    \item Our suggested approach is purely parameter-free and extremely fast. Therefore, feasible for highly time-demanding applications.
    \item We demonstrate a wide range of applications:  \emph{Robust Classification}, \emph{Adversarial Defense}, \emph{Outliers removal}, \emph{Uncertainty Estimation} and \emph{XAI Visualization}.
\end{enumerate}


























\section{Related Work}
\textbf{3D Robust Classification.}
Point cloud classification has gained attention in recent years \cite{dgcnn, curvenet, cloud_walker, pointnet, pointnet++, point_mlp, pointbert, gdanet, paconv, pct} due to the growing demand in various applications, i.e. autonomous driving \cite{autonomous_driving1, autonomous_driving2} and robotics \cite{robotics}. Although the ability of point clouds classification networks to overcome corruptions is a fundamental task in safety-critical applications, the research on this topic is relatively scarce.
PointCleanNet \cite{pointcleannet} suggested a learnable approach for outliers removal. PointASNL \cite{point_asnl} suggested an attention mechanism for adaptive sampling, which avoids outliers. Both approaches operate within the context of the primal Euclidean space. In contrast, our proposed work takes into consideration the semantic space, through importance, as ``viewed'' by the network. PointNet \cite{pointnet} proposed the concept of \emph{critical points}. These are the subset of points which are active following the last pooling layer. Thus,  prediction is agnostic to perturbations or outliers which do not belong to this critical set. Our observation, however, is that outliers are often interpreted as critical points.
In \cite{modelnet_c} a dataset referred to as ModelNet-C was introduced, consisting of real-world corruptions. It was revealed that PointNet performs well under some corruptions but mostly struggles dealing with diverse set of corruptions. In \cite{modelnet_c} Robust Point-cloud Classifier (RPC) was proposed, an algorithm based on a collection of the most robust modules across typical classification networks. At the time RPC reached SOTA performance on ModelNet-C. Recently, Ensemble of Partial Point Clouds (EPiC) \cite{epic} suggested to combine different sampling schemes: patches, curves, and random sampling. An ensemble was constructed, based on three classification networks, each specializing in a certain sampling method.
This algorithm was able to outperform RPC, both in the unaugmented and in the augmented cases. 
We note that such ensemble methods are relatively slow, and require considerable hardware for parallelism. Our focus here is on a very light and fast approach for online applications.


\begin{table*}
\begin{center}
  \begin{tabular}{p{2.0cm} || C{4cm} || C{2cm} C{2cm} C{2cm} C{2cm}}
  
    \hline
    \multirow{2}{*}{Model} &
    \multirow{2}{*}{Approach} &
      \multicolumn{2}{c}{WolfMix Augmented\cite{pointwolf, rsmix}}&
      \multicolumn{2}{c}{Un-Augmented}\\
      \cmidrule(l){3-4} \cmidrule(l){5-6}
    & & OA↑ & mCE↓ & OA↑ & mCE↓\\
    \hline
    \multirow{5}{*}{DGCNN\cite{dgcnn}} & Vanilla & \textbf{93.2\%} & 0.590 & 92.6\% & 1.000\\
     & EPiC \cite{epic} & 92.1\% & 0.529 & 93.0\% & 0.669\\
      & PointGuard \cite{pointguard} & 81.9\% & 1.154 & 83.8\% & 1.165\\
      & C.P++\textbf{(Ours)} & 91.4\% & 0.560 & 91.6\% & 0.688\\
     & EPiC \& C.P++ \textbf{(Ours)} & 92.9\% & \textbf{0.484} & \textbf{93.4\%} & \textbf{0.557}\\
    \hline
    \multirow{5}{*}{RPC\cite{modelnet_c}} & Vanilla & \textbf{93.3\%} & 0.601 & 93.0\% & 0.863\\
     & EPiC \cite{epic} & 92.7\% & 0.501 & \textbf{93.6\%} & 0.750\\
      & PointGuard \cite{pointguard} & 83.2\% & 1.067 & 86.9\% & 1.051\\
      & C.P++\textbf{(Ours)} & 91.2\% & 0.562 & 91.6\% & 0.728\\
     & EPiC \& C.P++ \textbf{(Ours)} & 92.9\% & \textbf{0.476} & 93.2\% & \textbf{0.616}\\
    \hline
    \multirow{5}{*}{GDANet\cite{gdanet}} & Vanilla & \textbf{93.4\%} & 0.571 & 93.4\% & 0.892\\
     & EPiC \cite{epic} & 92.5\% & 0.530 & \textbf{93.6\%} & 0.704\\
      & PointGuard \cite{pointguard} & 83.2\% & 1.059 & 84.8\% & 1.132\\
      & C.P++\textbf{(Ours)} & 91.8\% & 0.528 & 91.4\% & 0.718\\
     & EPiC \& C.P++ \textbf{(Ours)} & 92.8\% & \textbf{0.493} & 93.4\% & \textbf{0.587}\\
    \hline
  \end{tabular}
\end{center}
\caption{\textbf{Comparison on ModelNet-C. Wolfmix Augmented and Augmented Free.} C.P++ is a short-term for Critical Points++. Our approach is on-par with EPiC with extremely faster inference time (See Fig. \ref{fig:timing}). Combining our approach as extra sampling strategy in EPiC based on RPC is achieves SOTA results in terms of robustness.}
\label{table:robust_classification}
\end{table*}






\textbf{3D Adversarial Attacks.}
Another main concern in robustness of classification networks  is the ability to overcome deliberately manipulated samples, namely, adversarial attacks. 
A plethora of 3D adversarial attacks methods were introduced in the last few years \cite{minimal_adversarial, adversarial_shape_perturbations, shape_adv, robust_adversarial_objects, saliency_maps, adversarial_attack_and_defense, explainable_one_point_attack, Generating_3d_adversarial_point_clouds, advpc, lg_gan, isometry_attack, deep_learning_in_an_adversarial_setting, shape_invariant}. The main sources of attack are based on point perturbation, detachment or attachment with emphasis on imperceptible manipulations. Our proposed importance measure and point filtering approach may be used for adversarial defense. 
We thus outline here the main current directions for attacks based on importance.
Shape-Invariant attack\cite{shape_invariant} proposed a sensitivity map which is highly consistent across diverse neural networks. The algorithm calculates importance on a surrogate model and uses this as a good approximation for the network to be attacked. Based on this map, the authors of \cite{shape_invariant}  suggest to slide points along the tangent plane. 
Point Cloud Saliency Maps \cite{saliency_maps} is another importance based attack. Importance is calculated by analyzing the gradient loss when shifting points to the spherical center. 
\cite{adversarial_attack_and_defense} extends critical-points to a class-dependent importance ranking using first-order Taylor approximation. Importance can be approximated only for a given class (w.r.t the true class label). We note that all the methods above use a greedy scheme in  which points are iteratively manipulated by saliency order.
The work of \cite{deep_learning_in_an_adversarial_setting} generalizes the idea of critical points\cite{pointnet} to voxel-based networks, and leverage this to occlusion attacks.
Tan et al. \cite{explainable_one_point_attack} extend the ideas introduced by \cite{axiomatic}, originally for images, and use Integrated-Gradients as an explainable method to construct One-Point-Attack.
In \cite{Generating_3d_adversarial_point_clouds} suggest adding small clusters of points initialized in the ``critical points''.
These methods are slow and require large computational resources for importance evaluation. The main reasons are gradients calculation, or some differentiable hand-crafted per-point processes.
LG-GAN\cite{lg_gan} offered a generative based approach to produce adversarial examples.
We compare our proposed defensive scheme against SOTA Shape-Invariant Attack\cite{shape_invariant}, underscoring that even imperceptible perturbation may alter the importance. This illustrates that our approach is based upon a very general assumption.

\begin{figure*}[ptbh!]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.45\textwidth} \includegraphics[width=1\textwidth]{figs/sor_roc_curve.pdf}
\caption{{\bf SOR ROC-Curve} }    
        \label{subfig:sor_roc_curve}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth} \includegraphics[width=1\textwidth]{figs/outlier_removal_add_local2.pdf}
\caption{{\bf SOR vs. Our Method}}
        \label{subfig:outlier_removal_add_local}
    \end{subfigure}
    \caption{\textbf{Outlier Removal on challenging Add-Local corruption in ModelNet-C.} Our approach is parameter-free. our method outperform Statistical Outlier Removal (SOR) regardless parameter selection.
    }
    \label{fig:outlier_removal}
\end{figure*}

\textbf{3D Adversarial Defenses.} The use of advanced 3D augmentation techniques, e.g PointWolf \cite{pointwolf} and RSMix \cite{rsmix}, can significantly increase the network's robustness against corruptions \cite{modelnet_c}. In analogous manner, Adversarial Training techniques \cite{Ensemble_adversarial_training, Pointcutmix, extending_adversarial_attacks_and_defenses, adversarial_attack_and_defense, pagn, lpf_defense, point_acl} are used to defend against malicious attacks.
However, these approaches intentionally introduce perturbations to the point-clouds during training to improve their classification during inference, necessitating prior knowledge. As our focus lies in fast OOD defense, where we lack prior information about the corruptions, we do not compare against these methods.
PointGuard \cite{pointguard} and PointCert \cite{point_cert}, suggested a certified defense scheme by splitting the point-cloud randomly to many sub-samples and selecting the unified prediction by majority-voting. Their theoretical analysis advocates using very small samples, compensated by a huge ensemble (of 10,000 members in their experiments). 
These ensemble strategy, unfortunately, yields a non-feasible computational load.
Several simple methods, effectively applying a preprocessing step before feeding input samples to victim models \cite{dupnet, lpf_defense, if_defense,extending_adversarial_attacks_and_defenses, sor}.
Simple Random Sampling (SRS) as mentioned in \cite{dupnet}, simply screening out some of the input points randomly, and Statistical Outlier Removal (SOR) \cite{sor}, suggested to filter out points that are relatively far from their K nearest neighbors, hopefully most of them are outliers.
Dup-Net\cite{dupnet} is a network consist of a denoiser, based on SOR, and a learnable upsampler module. However, the upsampling naturally enlarge the amount of input points, consequently the inference becomes slower.
LPF-Defense\cite{lpf_defense} suggest to train a model based mainly on the low-frequency features and use it in inference. This is done by applying spherical harmonics transformation, projecting the point-cloud into a sphere. Their underlying assumption is that outliers are lay in the high-frequencies, which is not certainly true, especially in shape-invariant attacks.
IF-Defense\cite{if_defense} proposed an Implicit Function Defense by restoring and optimizing the original point-cloud in terms of surface distortion and perturbation. However, a training phase is required in inference. With respect to importance, \cite{extending_adversarial_attacks_and_defenses} is the only importance based adversarial defense suggested so far, to the best of our knowledge. Per-point importance is calculated by a derivative of the prediction with respect to the input. The problem is that propagating gradients throughout the network is a time consuming process, which not scalable for large networks. Although using importance maps for defensive purposes is a natural approach, we believe that it is rarely explored since the computational complexity of current importance maps algorithms is very high.
Our work attempts to close this gap. Defense candidates for comparison were selected based on Shape-Invariant attack\cite{shape_invariant} baseline. Additionally, we compare against LPF-Defense\cite{lpf_defense}.









\section{Our Method}
\subsection{Background and Definitions}
\subsubsection{Critical Points}
PointNet\cite{pointnet} defined a subset of points that span the skeleton of the object. These points are the ones that stay active after the last MaxPooling layer. Let us define this more formally.
 is used to denote .
Let a point-cloud be a set of  points in the  space: , where for each , .
Denote by  the per-point feature vector, assigning to each point , a vector  of  real valued features.
Since all point permutations represent the same object, the network must hold permutation invariance. Therefore, the pooling strategy is permutation invariant, and a combination of  and  are the most commonly used. This bottleneck can be used to single out subset of critical points that determine the classification result. Critical Points are defined for each point  in a binary manner, as follows:

The critical point set is 

\subsubsection{Critical Points++}
The absolute influence of Critical Points on the classification outcome, especially in regions with deviation from the training distribution (Fig. \ref{fig:critical_points}), leads to a fundamental question: \textit{Do all points within the critical set possess equal influence?} Our key realization is that critical points exhibit varying degrees of influence on the classification, thereby suggesting the feasibility of ranking them according to their meaningfulness. To address this, we propose two novel variations:

\textbf{Discrete Measure}. Simply defined as follows:

We can define  as a mapping function, 
where  is an indicator function (1 when true and 0 otherwise), in fact we count the number of features determined by each of the points.
Note that the following equation holds:



The discrete measure is ranking the points by their meaningfulness by monitoring only the most prominent features. There is no difference between the second most influential feature than the most insignificant one. Therefore, for classification task, it is natural to rank the points only according to the maximal feature.

\textbf{Soft Measure.} Defined as follows:


In the \textit{Discrete Measure}, outliers that have no influence on the classification are considered as non-important ones. However, in some applications one would like to examine the characteristics of all points in the point-cloud, i.e XAI or outlier removal. We thus want to obtain a more precise measure of the affect of any point on 
the network's behavior.
This is accomplished by a smooth importance map, as defined in \eqref{eq:soft_importance}.

The two point measures, \eqref{eq:importance} and \eqref{eq:soft_importance}, are fast to calculate, both consist of straightforward closed-form operations applied on matrices of the network. 
\begin{table*}
\begin{center}
  \begin{tabular}{p{2.5cm} C{2cm} C{2cm} C{2cm} C{2cm} C{2cm} C{2cm}}
  
    \hline
    \multirow{2}{*}{Defense} &
      \multicolumn{2}{c}{DGCNN \cite{dgcnn}}&
      \multicolumn{2}{c}{PointNet\cite{pointnet}} &
      \multicolumn{2}{c}{GDANet\cite{gdanet}} \\
      \cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7}
    & A.S.R (\%) & A.Q (times) & A.S.R (\%) & A.Q (times) & A.S.R (\%) & A.Q (times) \\
    \hline
    Undefended & 99.3 & 106.7 & 99.8 & 18.9 & 99.8 & 18.9 \\
SRS(50\%) \cite{dupnet}
    & 78.4 & 566.3 & 94.0 & 190.9 & 78.1 & 595.4 \\
SRS(30\%) \cite{dupnet} & 68.6 & 790.3 & 97.6 & 93.5 & 72.4 & 714.0\\
    SOR \cite{sor} & 75.6 & 795.6 & 78.4 & 592.9 & 69.9 & 913.2 \\
LPF-Defense\cite{lpf_defense} & 47.8 & 1148.0 & 98.2 & 123.1 & 52.6 & 1071.4\\
    C++ (Ours) & \textbf{37.5} & \textbf{1376.18} &\textbf{72.0} & \textbf{730.4} & \textbf{34.6} & \textbf{1425.5}\\
    \hline
  \end{tabular}
\end{center}
\caption{\textbf{Adversarial Defenses on Shape-Invariant Attack\cite{shape_invariant} on ModelNet40\cite{modelnet40}.} Attack Success Rate (A.S.R) is consistently the lowest and mean Query Cost (A.Q) is the highest over all examined networks compare to all other defense methods. For DGCNN and GDANet, Attack Success Rate is extremely decreased.}
\label{table:adversarial_attack}
\end{table*}

\subsubsection{Normalized Entropy}
Levi et al. \cite{epic} suggest to “spread” the importance as evenly as possible among the cloud points, and Chefer et al. \cite{relevancy_maps} suggest to manipulate the relevancy map to focus on regions within image foreground mask. Motivated by these insights we investigate the correlation between success-rate and the degree of uniformity of the importance. 
Treating importance as a probability distribution, a general measure for uniformity of a probability distribution is entropy: it is low when the distribution has sharp peaks and increases as the distribution becomes more even. To obtain a normalized measure, in the range , we choose to use normalized entropy. This measure simply divides the entropy by the maximum possible entropy for that sample (which is the uniform distribution): 
, where  is the entropy and . In this setting  is a discrete distribution of  points, the probability at each point is denoted by , where , , and  . It is well known that entropy is maximized for the uniform distribution, , . Plugging this in  and rearranging yields the following expression for 
the normalized entropy,

To compute \eqref{eq:H_n} for importance measures, we first have to normalize them to sum to one:

where  in this context stands for any point measure defined by either Eqs. \eqref{eq:critical}, \eqref{eq:importance} or \eqref{eq:soft_importance}.


\begin{figure}[ptbh!]
  \centering
\includegraphics[width = 1\linewidth, height =0.6 
   \linewidth]{figs/entropy_vs_success_rate_all.pdf}
   \caption{{\bf  Vs. Success-Rate}. ModelNet-C\cite{modelnet_c}, DGCNN\cite{dgcnn}}
   \label{fig:entropy_vs_success_rate}
\end{figure}

Since all measures are non-negative, following normalization the measures can be interpreted as probability distributions and normalized entropy can be computed. We note that normalized entropy is also a measure of efficiency. In our setting, if points contribute more evenly to the classification, the point cloud representation is more efficient. 
In Fig. \ref{fig:entropy_vs_success_rate}
the success rate is shown as a function of normalized entropy.
We see typical narrow range for the clean set and a much wider range for the corrupted set.
This observation and the fact that often corruptions are viewed as important by the network (Fig. \ref{fig:critical_points}) leads us to the following proposed method of point filtering to increase robustness. 

\subsection{Our Approach}
\subsubsection{Reign of the Uncriticals}
Building upon our observation that critical points tend to spread amongst outliers, and our proposed modification enabling ranking of critical points in a meaningful order, it becomes intuitive to avoid corruptions by cropping the most important critical points. Thus, the importance redistributed amongst the remaining points. This, should filter out corruptions and align the entropy closer to the narrow region of the clean set. 






\subsubsection{Adaptive Threshold}
We claim that samples experiencing addition of points should be subjected to more aggressive filtering compared to samples that have undergone detachment of points. In fact, it becomes questionable whether the latter samples should be sampled at all. However, a pertinent question arises: \textit{How many points should be sampled?} 
 represents the network's efficiency with respect to a particular point cloud. Therefore, we suggest to use this measurement as a criterion for determining the ratio of remaining points during the filtering process. Thus, we can define the remaining number of points as: .  

We evaluate on a grid of  values, compare to our adaptive threshold,
and plot the accuracy and robustness on classification task (see Fig. \ref{fig:adaptive_vs_fixed}). The results strongly support our proposition to employ a selective threshold rather than a fixed number of points. The utilization of our suggested selective filtering approach significantly improves the mean corruption error (mCE) compared to any fixed number of remaining points.
\begin{figure}[ptbh!]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.48\linewidth} \includegraphics[width=1\linewidth]{figs/ablation_mce.pdf}
\caption{{\bf  mCE Vs. N++.}}
        \label{subfig:ablation_mce}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth} \includegraphics[width=1\linewidth]{figs/ablation_oa.pdf}
\caption{{\bf Overall Accuracy (OA) Vs. N++.} }    
        \label{subfig:ablation_oa}
    \end{subfigure}
    \caption{{\bf Adaptive vs. Constant Threshold.} Our adaptive threshold is superior than any fixed threshold in terms of robustness.}
    \label{fig:adaptive_vs_fixed}
\end{figure}







\begin{figure*}[ptbh!]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.45\textwidth} \includegraphics[width=1\textwidth]{figs/entropy_histogram_vanilla.pdf}
\caption{{\bf Vanilla}}
        \label{subfig:entropy_histogram_vanilla}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth} \includegraphics[width=1\textwidth]{figs/entropy_histogram_ours.pdf}
\caption{{\bf Ours} }    
        \label{subfig:entropy_histogram_ours}
    \end{subfigure}
    \caption{\textbf{From OOD to ID.}  histogram on ModelNet-C dataset using vanilla DGCNN (left) and after cropping the most influential input points, 600 points are retained (right). Our method is align the histogram with regard , therefore align corrupted samples to the distribution seen during training.
    }
    \label{fig:entropy_histogram}
\end{figure*}

\section{Applications and Experiments}


We now show how Critical Points ++ and the respective point sampling method can be used for various applications.

\subsection{Robust Classification}

\begin{algorithm}
\caption{ (Inference)}\label{alg:inference}
\begin{algorithmic}
\Require{} 
\State \Comment{Load Pretrained}
\State  \Comment{First forward-pass}
\State  \Comment{Calculate , }
\State 
\State 
\State   \Comment{Adaptive Threshold}
\LineComment{Select K lowest importance points}
\State 
\State  \Comment{Second forward-pass}
\State 

\end{algorithmic}
\end{algorithm}
Inference procedure is described in details in Alg. \ref{alg:inference}. In training process (described in Supp.), the basic network is trained on the clean set only, and adapted to accept wide range of remaining points (256-1024). The same basic network is used for querying  (for calculating importance) and for the actual prediction on the filtered sample. Thus, this process can be thought as a self-restraining process. We ask the network for mapping the prominent features, and use the same network for classifying based on the non-importants. The inference includes dual forward-pass, and simple and fast extra calculations. Thus, efficient in terms of time consuming. We examine our approach on ModelNet-C\cite{modelnet_c} dataset on several networks, and compare to robustification methods (See Tab. \ref{table:robust_classification}). Our combined approach with EPiC \cite{epic} (explained in Supp.) achieve SOTA results on this dataset.
















\subsection{Adversarial Defense}


Another major threat for point cloud classification networks is adversarial attacks. It has been shown that main classification networks are vulnerable for this attacks, even for a barely distinguishable transformed clouds \cite{shape_invariant, saliency_maps}. In this regime the classification network has no knowledge regarding the manipulation, thus, there is a clear advantage for OOD robustification. We applied our method (as described in Alg. \ref{alg:inference}) as a defense scheme against Shape-Invariant Attack\cite{shape_invariant} and compared it with several OOD defenses. Our method achieves non-comparable results, reducing the Attack Success Rate (ASR) to a limited 37.5\% (compare to 47.8\% using LPF-Defense\cite{lpf_defense}) when embedded to DGCNN\cite{dgcnn}. We applied on the extreme case, when DGCNN is functioning as the surrogate model and as the attacked one (transferability is no longer an issue in this case). The results are organized in Tab. \ref{table:adversarial_attack}.

\subsection{Explainable AI}
We suggest using our soft version in order to visualize the network's performance on a certain point-cloud. We show the visualization capabilities of our approach and visualize several chairs from ScanObjectNN \cite{scanobjectnn} data-set consist of real-world corruptions, i.e ground surface and entire occluded parts (See Fig. \ref{fig:scan_object_nn_chairs}). The importance is spread amongst dominant parts of the shape as the leg tips or the chair support, together with emphasize prominent corruptions. Pay attention that since the floor background is not OOD (There exist classes in ModelNet40 consist of flat parts in the bottom, i.e the basis of a bookshelf) they are not highlighted with large influence.




\begin{figure*}[ptbh!]
  \centering
\includegraphics[width = 0.65\linewidth, height =0.4 
   \linewidth]{figs/scan_object_nn_chairs.pdf}
   \caption{{\bf Explainable AI using Critical Points ++ on chair examples from ScanObjectNN \cite{scanobjectnn}.} The importance is spread reasonably amongst the chairs dominant features (i.e the chair support and leg tips) even under severe degradations.}
   \label{fig:scan_object_nn_chairs}
\end{figure*}

\subsection{Anomaly Detection and Outliers Removal}
Outlier detection and removal is a long standing concept in a wide range of applications, with two distinct approaches prevailing. The first category encompasses learnable methods such as PointCleanNet \cite{pointcleannet} and SSPCN \cite{learnable_outlier_removal1}. However, these supervised methods necessitate clean point-clouds as ground truth and are therefore unsuitable for the OOD regime. Since our main focus is on OOD methods, we decided to exclude them from our comparison. The second category comprises classical methods, among which Statistical Outlier Removal (SOR) \cite{sor, dupnet} is commonly employed. SOR aims to remove data elements that deviate far from their  nearest neighbors by at least . Nevertheless, SOR has two major drawbacks. Firstly it  requires prior knowledge on the characteristics of the outliers. Secondly, although it performs well against distinct outliers, it does not generalize well to more delicate corruptions, such as Add-Local, which commonly appear in real-world scenarios.
Our proposed method involves the use of the \textit{Soft Measure}, wherein the remaining points are those whose importance scores are lower than the mean importance score of the entire shape, , this approach is parameter-free, since the threshold is adaptively selected. To evaluate our approach, we compare it against SOR. Initially, we scan a wide range of s and calculate the mean recall and precision over the entire set of Add-Local at the highest severity in ModelNet-C \cite{modelnet_c} benchmark. Utilizing our completely parameter-free method, we conduct the same experiment and plot our mean recall and precision on the same graph. The results, presented in Fig. \ref{subfig:sor_roc_curve}, show that for any parameter selection, SOR is unable to achieve the precision reached by our method. To ensure a fair comparison, we further analyze per-sample recall and precision calculations to acquire the same mean recall score, depicted in Fig. \ref{subfig:outlier_removal_add_local}. Our findings conclusively demonstrate the superiority of our approach in effectively detecting outliers, particularly in the context of corruption which added smoothly and locally.

\subsection{Uncertainty Estimation}
Uncertainty estimation in the context of point-cloud classification is not highly investigated yet. The ability to estimate classification uncertainty becomes particularly advantageous when integrating predictions from various sources, such as different acquisition angles of the same object. Preliminary results, shown in Fig. \ref{fig:entropy_vs_success_rate}, can indicate potential directions for estimating uncertainty. The normal distribution of the clean set can be conveniently obtained during training, facilitating as an estimation map for newly unseen samples. Moreover, in real-world scenarios, it becomes challenging to ascertain the type of corruption (i.e. addition or detachment) affecting the data. However, as depicted in Fig. \ref{fig:entropy_histogram}, the relation between sample entropy and clean-set entropy offers valuable insights for determining the corruption type. Samples with higher entropy compared to the clean set are more likely to be exposed to detachment of points, while lower entropy is correlated with an addition of points.
We further study this on real-world ScanObjectNN\cite{scanobjectnn} dataset. Samples contains background are yielding lower entropy than samples without background. Since the background May contain strong features that "steal" the network attention. (See Fig. \ref{fig:scan_object_nn_success_rate}).
\begin{figure}[ptbh!]
  \centering
\includegraphics[width = 0.7\linewidth, height =0.48 
   \linewidth]{figs/scan_object_nn_success_rate2.pdf}
   \caption{{\bf  on real-world ScanObjectNN\cite{scanobjectnn}.} Samples contains background yields lower  than those without background.}
   \label{fig:scan_object_nn_success_rate}
\end{figure}

\section{Analysis}
\subsection{From OOD to ID}
We plot  histogram on vanilla DGCNN\cite{dgcnn} on addition and detachment of points from ModelNet-C\cite{modelnet_c} dataset (Fig. \ref{subfig:entropy_histogram_vanilla}), and recalculate  after using our method and cropping to fixed size of 600 points (Fig. \ref{subfig:entropy_histogram_ours}.) we note two main insights: 1) On outliers insertion,  decreases. This indicates that the network tends to overlook these regions and discard other points in favor of the newly added ones, due to their prominence. Conversely, when points are removed from the cloud, the network manage to distribute attention across a larger number of points, leading to an increase in . 2) sub-sampling by discard the most prominent points is aligning the  histogram, insert samples from OOD to ID.






















\section{Conclusion}
In this work, we extend the concept of Critical Points \cite{pointnet} from a binary definition into either  \textit{discrete} or \textit{continuous} measures, ranking the point cloud by importance. 
Studying the proposed importance measures on OOD samples, we find high correlation between  importance and corruptions. We thus propose to filter out the most prominent points, training a classifier on points of milder importance. We gain a boost in network robustness for OOD classification at the cost of a slight degradation in accuracy. Moreover, we develop an adaptive threshold based on normalized entropy to determine the degree of point filtering. Thus our filtering is completely automatic without any meta-parameters.
Our approach is simple and fast to compute, hence, suitable for time-demanding applications. We demonstrate that our approach is highly successful on a wide variety of applications: XAI, outlier removal, uncertainty estimation, adversarial defense and robust classification.
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}
\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Filtering in feature space}
The algorithm outlined in the main paper involves a dual forward pass. The first pass is necessary for querying the importance score, while the second pass is utilized to predict the point cloud filtered based on the previously calculated importance score. Employing filtering in Euclidean space serves as a safeguard against the propagation of outliers from the primal space to the feature space.
An alternative approach we propose is to perform filtering directly in the feature space instead of the input space. This alternative method requires only a single forward pass of the base network, incurring no additional latency. This approach can be likened to a dropout layer that emphasizes the filtration of dominant feature elements instead of random ones.
While feature space filtering yields lower robustness compared to filtering in the primal space, as outliers in the Euclidean space can still contaminate features, it does provide a notable boost in robustness, compared to the vanilla base network.
Thus, if speed is essential, one can consider also this approach in point cloud classification networks. 
To illustrate this improvement, we integrated feature filtering into the DGCNN \cite{dgcnn} and RPC \cite{modelnet_c} models and evaluated their performance on the ModelNet-C \cite{modelnet_c} dataset. The results, as presented in Tab. \ref{table:critical_points_dropout}, highlight the significant enhancement in robustness achieved through the implementation of our proposed layer. This improvement is demonstrated in comparison to the vanilla version of the network, with no associated latency cost.

\section{High-Low Entropy Visualization}
We illustrate the interpretation of  in exemplar point-clouds by visualizing its variation from two perspectives. Initially, we examine the samples with the highest and lowest values of  across the entire clean ModelNet40 dataset \cite{modelnet40} (see Fig. \ref{fig:low_to_high_clean}). Here, we observe that samples spanning predominantly over a 2D plane tend to exhibit lower values of , while samples demonstrating symmetry to a certain extent tend to yield higher values of . In the former scenario, the network appears to emphasize specific regions, focusing on informative areas that contribute more than others. Conversely, in the latter scenario, most input points convey similar information about the overall shape, collectively contributing to the global feature pool.
Furthermore, we present the visualization of  under different corruptions (Add-Local, Add-Global, UnCorrupted, Drop-Local, and Drop-Global), as shown in Fig. \ref{fig:low_to_high_corruptions}. This visualization effectively demonstrates and exemplifies the trend highlighted in the main paper (Fig. \ref{fig:entropy_histogram}).


\label{sec:intuition_visualization}

\begin{table}
\begin{center}
  \begin{tabular}{p{1.2cm} || C{3.0cm} || C{0.8cm} C{0.8cm} C{0.5cm}}
  
    \hline
    Model & Approach & OA\% & mCE & F.P\\
    \hline
    \multirow{1}{*}{DGCNN} & Vanilla & 92.6 & 1.000 & 1\\
     \cite{dgcnn} & C.P++ Feat.\textbf{(Ours)} & 92.7 & 0.818 & 1\\
     & C.P++ Euclid.\textbf{(Ours)} & 91.6 & 0.688 & 2\\
    \hline
    \multirow{1}{*}{RPC} & Vanilla & 93.0 & 0.863 & 1\\
     \cite{modelnet_c}& C.P++ Feat.\textbf{(Ours)} & 92.0 & 0.797 & 1\\
     & C.P++ Euclid.\textbf{(Ours)} & 91.6 & 0.728 & 2\\
    \hline
  \end{tabular}
\end{center}
\caption{\textbf{Filtering in Feature Space vs. Euclidean Space Using Critical Points++} Employing \textit{Critical Points++} for filtering in the feature space, as opposed to the primary Euclidean space, enhances robustness compared to the standard network, all without incurring any latency costs.}
\label{table:critical_points_dropout}
\end{table}

\section{Recall-Precision in details}
Here we give in more detail the definitions and computations of the \textit{Recall} and \textit{Precision} metrics for the outlier removal segment discussed in the main paper. First, we define the following:

\begin{itemize}
\item \textbf{True Positives (TP)}: These are outliers that have been accurately filtered out.

\item \textbf{False Positives (FP)}: These are outliers that were incorrectly unfiltered.

\item \textbf{False Negatives (FN)}: These are the benign points that were mistakenly filtered out.
\end{itemize}

With these definitions in place, we can formally define \textit{Recall} and \textit{Precision} as follows:





\label{sec:recall_precision}

\section{Fixed vs. Adaptive Sampling Size}
\label{sec:fixed_vs_entropy}
In the main paper, we conduct a comparison between a fixed sampling size and our innovative adaptive threshold, based on normalized entropy. We demonstrate the superior performance of the adaptive threshold in robust classification using the Modelnet-C \cite{modelnet_c} dataset, applied to the DGCNN\cite{dgcnn} model. To provide a comprehensive overview, we also perform an experiment involving the fixed sampling size within the context of an Adversarial Defense task. The results of this experiment are summarized in Tab. \ref{table:adversarial_attack_supp}. Our adaptive threshold effectively reduces the attack success rate (ASR) and increases the number of required attack queries (A.Q) for two out of the three examined models. Therefore, the application of the adaptive threshold offers stronger defense compared to the use of a na\"ive fixed-size threshold. Naturally, one also gains the advantage of avoiding to figure out a significant meta-parameter.

\section{Implementation Details}
A detailed overview of the training phase is provided in Alg. \ref{alg:training}.
\begin{algorithm}
\caption{ (Training)}\label{alg:training}
\begin{algorithmic}
\Require{} 
\For {}
\State  
\State  \Comment{Calculate , }
\State 
\State 
\State   
\State 
\State  
\State  
\State  
\EndFor
\State 
\end{algorithmic}
\end{algorithm}

\subsection{Adversarial Defense}
\textbf{Defense parameters.} Following Shape-Invariant \cite{shape_invariant} paper, SRS\cite{dupnet} was applied with 30\% of filtered points, as well as with 50\%, while SOR\cite{sor} was applied with 2 KNN and .
Regarding LPF-Defense \cite{lpf_defense}, we utilized , and in line with the ablation study conducted in LPF-Defense paper, we executed it with . The paper recommends training the network on the clean dataset and employing these parameters during inference. Consequently, for each network, we underwent training with these parameters and conducted testing using the same parameters. The training process encompassed 300 epochs, employing the ADAM optimizer with a learning rate of , momentum of , and weight decay of .

\textbf{Attack setting.} To underscore our defense capabilities, and recognizing that the Shape-Invariant attack leverages a sensitivity map characterized by transferability across diverse networks, we decided to examine the most challenging scenario. This entails investigating the situation where the surrogate model, responsible for generating the sensitivity map, aligns with the victim model. DGCNN\cite{dgcnn} serves as both the surrogate model and the victim. Similarly, PointNet\cite{pointnet} fulfills dual roles as the surrogate and victim, as does GDANet\cite{gdanet}.
\subsection{Robust Classification}
In the unaugmented setting, we initially apply conventional augmentation methodologies to adhere to the Out-of-Distribution (OOD) principle. In contrast, for the augmented iteration using WolfMix\cite{pointwolf, rsmix}, the augmentation process is first applied to the sample, followed by our importance-based subsampling technique. To mitigate the effects of randomness, a predetermined seed is utilized.
All models undergo training for 300 epochs in the unaugmented scenario and 500 epochs in the augmented case, utilizing a learning rate of 5e-4. A cosine annealing scheduler \cite{cosine_annealing} is employed to drive the learning rate to converge to zero. A batch size of 64 is adopted.
In the unaugmented version, the augmentation protocol outlined by DGCNN \cite{dgcnn} is followed, encompassing two steps: 1) stochastic anisotropic scaling spanning the range of [2/3, 3/2]; and 2) random translation within the interval of [-0.2, +0.2]. The implementation makes use of the PyTorch library \cite{pytorch}. The primary training objective involves minimizing the Cross-Entropy loss.
During training, we computed the importance score and cropped the sample to a random size within the range of [256, 1024] contains points with the lowest importance. This prepares the model to accommodate a wide range of sample sizes during inference, where we crop the sample to an unknown size based on the adaptive threshold. Due to variations in the number of points in each cloud, inference is performed using a batch size of 1, a common practice in real-world applications. Timing estimates are obtained by averaging the time taken for 100 iterations of batches with a size of 1.

The combination of our sampling approach with EPiC\cite{epic} is achieved by applying our sub-sampling technique once for each sample, resulting in a fixed size of 600 points. The prediction is then duplicated four times and concatenated with other sub-samples. This duplication is carried out to ensure an equal impact for each of the sampling schemes. Consequently, the ensemble created using our sampling approach comprises 16 members.
\section{Benchmarks}
\textbf{ModelNet40 \cite{modelnet40}}.
Synthetic dataset, constitutes a widely employed collection of CAD meshes spanning 40 distinct classes, encompassing objects such as monitors, beds and persons. These meshes have undergone uniform sampling procedures, resulting in the formation of 3D point clouds, each comprising 1024 points. The dataset encompasses a total of 12,311 samples, partitioned into 9,843 samples designated for training and 2,468 samples allocated for testing purposes.

\textbf{ModelNet-C\cite{modelnet_c}}. Ren et al., introduced a corrupted point cloud benchmark denoted as ModelNet-C, based on ModelNet40, to facilitate the OOD robustness. ModelNet-C encompasses a spectrum of seven distinct corruption types, namely jitter, scale, rotation, add-global, add-local, drop-global, and drop-local, each exhibiting five levels of difficulty. To quantitatively gauge robustness, \cite{modelnet_c} introduced a comprehensive metric termed mean Corruption Error (mCE), a relative robustness measure with DGCNN algorithm serving as a pivot network (and thus by definition mCE value of 1). Given the error-based nature of this metric, a lower mCE score is indicative of superior performance. 

\textbf{ScanObjectNN\cite{scanobjectnn}}. Contains 2902 point clouds spanning 15 distinct categories, constituting a more intricate collection that arises from real-world scans featuring background elements and instances of occlusion. There exist classes which overlapp with ModelNet40 classes e.g. chairs, desks and sofas.
\section{Full results}
We present comprehensive and detailed results encompassing both the Adversarial Attack experiment (refer to Tab. \ref{table:adversarial_attack_supp}) and the Robust Classification analysis (refer to Tab. \ref{table:robust_classification_full}). 
\textbf{Adversarial Attack Experiment.} The comprehensive table includes three additional distance measures between adversarial and benign shapes: Chamfer, Hausdorff, and MSE distances.
\textbf{Robust Classification}. The detailed table contains additional results for mCE under each specified corruption. Notably, certain defenses display a correlation with specific corruptions across various networks. For instance, Drop-Local is most effectively mitigated by EPiC\cite{epic}, while Scale is best addressed by PointGuard\cite{pointguard}. However, when considering the mCE metric, which aggregates the overall robustness scores across all corruptions, EPiC \& C.P++ (our method combined with EPiC) consistently achieve the highest scores among the networks examined.

\begin{table*}
\begin{center}
\begin{tabular}{c || c || c c c c c}
\hline
Surrogate Model & Defense & ASR (\%) & A.Q (times) & C.D () & H.D () & MSE \\
\hline\hline
DGCNN \cite{dgcnn} & -- & 99.3 & 106.7 & 3.18 & 4.54 & 1.22\\
& SOR \cite{sor} & 75.6 & 795.6 & 2.59 & 3.48 & 1.65\\
& SRS (50\%) & 78.4 & 566.3 & 1.21 & 3.59 & 0.69\\
& SRS (30\%) & 68.6 & 790.3 & 1.64 & 4.11 & 0.85\\
& LPF-Defense\cite{lpf_defense} & 47.8 & 1148.0 & 1.84 & 4.09 & 0.93\\
& C.P++ (Ours) - Fixed (600) & 43.5 & 1265.0 & 2.30 & 4.16 & 1.11\\
& C.P++ (Ours) - Adaptive& \textbf{37.5} & \textbf{1376.1} & 2.28 & 4.06 & 1.17\\
\hline
PointNet \cite{pointnet} & - & 99.8 & 18.9 & 2.09 & 4.46 & 0.80\\
& SOR \cite{sor} & 78.4 & 592.9 & 9.26 & 4.33 & 2.74\\
& SRS (50\%) & 94.0 & 190.9 & 1.67 & 4.42 & 0.71\\
& SRS (30\%) & 97.6 & 93.5 & 1.76 & 4.43 & 0.72\\
& LPF-Defense\cite{lpf_defense} & 98.2 & 123.1 & 5.36 & 4.56 & 1.41\\
& C.P++ (Ours) - Fixed (600) & 74.1 & 693.6 & 11.33 & 4.70 & 2.66\\
& C.P++ (Ours) - Adaptive & \textbf{72.0} & \textbf{730.4} & 11.13 & 4.68 & 2.72\\
\hline
GDANet \cite{gdanet} & - & 99.4 & 95.6 & 4.16 & 4.67 & 1.27 \\
& SOR \cite{sor} & 69.9 & 913.2 & 3.67 & 3.98 & 1.78\\
& SRS (50\%) & 78.1 & 595.4 & 1.70 & 4.16 & 0.78\\
& SRS (30\%) & 72.4 & 714.0 & 2.19 & 4.42 & 0.92\\
& LPF-Defense\cite{lpf_defense} & 52.6 & 1071.48 & 2.18 & 4.26 & 0.94\\
& C.P++ (Ours) - Fixed (600) & \textbf{32.9} & \textbf{1447.3} & 2.18 & 4.32 & 1.03\\
& C.P++ (Ours) - Adaptive & 34.6 & 1425.5 & 2.13 & 4.21 & 1.03\\
\hline
\end{tabular}
\end{center}
\caption{\textbf{Comprehensive Table of Adversarial Defenses against Shape-Invariant Attack\cite{shape_invariant} on the ModelNet40\cite{modelnet40} Dataset.} Incorporating our Approach with a Fixed Threshold (600 points). Our adaptive approach outperforms the fixed threshold in two out of the three evaluated networks.}
\label{table:adversarial_attack_supp}
\end{table*}

\begin{figure*}[ptbh!]
  \centering
\includegraphics[width = 1\linewidth, height =0.6 
   \linewidth]{figs/low_to_high_clean.pdf}
   \caption{{\bf Highest and Lowest  on clean set.} Samples that result in high values of  distribute importance across a broad spectrum of points, whereas in the case of the lowest s samples, importance becomes concentrated within specific regions. Samples containing flat areas, with some extent of symmetry against the center of the shape, contribute to an increase in . It is possible that samples predominantly spanning a 2D plane prompt the network to prioritize attention towards distinctive regions.}
   \label{fig:low_to_high_clean}
\end{figure*}

\begin{figure*}[ptbh!]
  \centering
\includegraphics[width = 1\linewidth, height =0.6 
   \linewidth]{figs/low_to_high_corruptions.pdf}
   \caption{{\bf Variation of  Across Different Corruptions.} The noise variations are presented in the following order: Add-Local, Add-Global, Uncorrupted, Drop-Local, Drop-Global. This visualization complements Fig. \ref{fig:entropy_histogram} by illustrating the correlation between  and specific corruption types for particular samples.}
   \label{fig:low_to_high_corruptions}
\end{figure*}


\begin{table*}
\begin{center}
  \begin{tabular}{p{1.8cm} || C{3.2cm} || C{1cm} C{1cm} C{1cm} C{1cm} C{1.1cm} C{1.1cm} C{1cm} C{1cm} C{1cm}}
  
    \hline
    Model & Approach & OA↑ & mCE↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate\\
    \hline
    \multirow{5}{*}{DGCNN\cite{dgcnn}} & Vanilla & 92.6\% & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000\\
     & EPiC \cite{epic} & 93.0\% & 0.669 & 1.000 & 0.680 & \textbf{0.331} & \textbf{0.498} & 0.349 & 0.807 & 1.019\\
      & PointGuard \cite{pointguard} & 83.8\% & 1.165 & \textbf{0.743} & 0.809 & 0.840 & 0.812 & 0.838 & 0.785 & \textbf{0.584}\\
      & C.P++\textbf{(Ours)} & 91.6\% & 0.688 & 0.896 & 0.868 & 0.891 & 0.820 & 0.902 & 0.829 & 0.779\\
     & EPiC \& C.P++ \textbf{(Ours)} & \textbf{93.4\%} & \textbf{0.557} & 0.957 & \textbf{0.494} & 0.335 & 0.522 & \textbf{0.258} & \textbf{0.382} & 0.949\\
    \hline
    \multirow{5}{*}{RPC\cite{modelnet_c}} & Vanilla & 93.0\% & 0.863 & 0.840 & 0.892 & 0.492 & 0.797 & 0.929 & 1.011 & 1.079\\
     & EPiC \cite{epic} & 93.6\% & 0.750 & 0.915 & 1.057 & 0.323 & \textbf{0.440} & 0.281 & 0.902 & 1.330\\
      & PointGuard \cite{pointguard} & 86.9\% & 1.051 & \textbf{0.773} & 0.817 & 0.868 & 0.843 & 0.867 & 0.804 & \textbf{0.590}\\
      & C.P++\textbf{(Ours)} & 91.6\% & 0.728 & 0.901 & 0.808 & 0.912 & 0.842 & 0.862 & 0.819 & 0.744\\
     & EPiC \& C.P++ \textbf{(Ours)} & \textbf{93.2\%} & \textbf{0.616} & 0.957 & \textbf{0.690} & \textbf{0.319} & 0.464 & \textbf{0.258} & \textbf{0.444} & 1.177\\
    \hline
    \multirow{5}{*}{GDANet\cite{gdanet}} & Vanilla & 93.4\% & 0.892 & 0.830 & 0.839 & 0.794 & 0.894 & 0.871 & 1.036 & 0.981\\
     & EPiC \cite{epic} & \textbf{93.6\%} & 0.704 & 0.936 & 0.864 & \textbf{0.315} & \textbf{0.478} & 0.295 & 0.862 & 1.177\\
      & PointGuard \cite{pointguard} & 84.8\% & 1.132 & \textbf{0.755} & 0.804 & 0.847 & 0.819 & 0.846 & 0.787 & \textbf{0.589}\\
      & C.P++\textbf{(Ours)} & 91.4\% & 0.718 & 0.900 & 0.848 & 0.884 & 0.812 & 0.908 & 0.810 & 0.763\\
     & EPiC \& C.P++ \textbf{(Ours)} & 93.4\% & \textbf{0.587} & 0.926 & \textbf{0.617} & 0.323 & 0.512 & \textbf{0.258} & \textbf{0.393} & 1.079\\
    \hline
  \end{tabular}
\end{center}
\caption{\textbf{Comprehensive Comparison Table for Augmented Free ModelNet-C\cite{modelnet_c}.}}
\label{table:robust_classification_full}
\end{table*} 
\end{document}
