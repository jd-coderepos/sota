


\begin{abstract}
A lower bound is presented which shows that a class of heap algorithms in the pointer model with only heap pointers must spend  amortized time on the \opDc\ operation (given  amortized-time \opEm). 
Intuitively, this bound shows the key to having -time \opDc\ is the ability to sort  items in  time; Fibonacci heaps [M.~.L.~Fredman and R.~E.~Tarjan. J.~ACM 34(3):596-615 (1987)] do this through the use of bucket sort. Our lower bound also holds no matter how much data is augmented; this is in contrast to the lower bound of Fredman [J. ACM 46(4):473-501 (1999)] who showed a tradeoff between the number of augmented bits and the amortized cost of \opDc. A new heap data structure, the \emph{sort heap}, is presented. This heap is a simplification of the heap of Elmasry [SODA 2009: 471-476] and shares with it a  amortized-time \opDc, but with a straightforward implementation such that our lower bound holds. Thus a natural model is presented for a pointer-based heap such that the amortized runtime of a self-adjusting structure and amortized lower asymptotic bounds for \opDc\ differ by but a  factor.
\end{abstract}
\vfill

\begin{shortonly}
\begin{center}
We have chosen to include a full version which contains all proofs, in addition to some results omitted from the 10-page version and a more detailed and generously typeset presentation. The full version appears starting on page~\pageref{full}. The references are at the end, starting on page~\pageref{refs}.
\end{center}
\vfill
\end{shortonly}

\pagebreak

\begin{fullonly}
\begin{sidewaystable} \footnotesize 
\begin{tabular}{c|ccccc}
&Fibonacci Heap& Pointer\\
&Rank-Pairing Heap &  Rank-Pairing & Pairing heap & Elmasry's heap & Sort heap\\ \hline \hline
Introduced &  \parbox{0.5in}{\center \cite{DBLP:journals/jacm/FredmanT87} \cite{DBLP:journals/siamcomp/HaeuplerST11}} & \parbox{1.5in}{\center Variant of Rank-Pairing heap described here} & \cite{DBLP:journals/algorithmica/FredmanSST86}&\cite{DBLP:conf/esa/Elmasry10,DBLP:conf/soda/Elmasry09}  & This paper\\ 
\hline
Upper bound for round &  &  &  &  & \\
&  & & \cite{DBLP:conf/focs/Pettie05} \\ 
\hline
Trivial lower bound &  &  &  &  & \\
\parbox{1.5in}{\center Fredman's lower bound \cite{DBLP:journals/jacm/Fredman99}} &  & &  &Does not apply &Does not apply\\
Our lower bound& Does not apply &   &   & Does not apply &  \\
\hline
Tightness of analysis &  &  &  &  & \\
\hline
\hline
Augmented data & Yes & Yes & No & No & No
\end{tabular}
\caption{Comparison of various heaps, giving their amortized runtimes per round, where a round consists of one \opIns,  \opDc\ operations, and one \opEm\ on a heap of size . Tightness of analysis is the ratio of the best upper bound to the best lower bound. }
\jlabel{t1}
\end{sidewaystable}
\end{fullonly}



\section{Introduction}



While \opIns\ and \opEm\ are supported by all priority queues, there is one additional operation which is of use in some algorithms: \opDc. The fast execution of \opDc\ is vital to the runtime of several algorithms, most notably Dijkstra's algorithm \cite{springerlink:10.1007/BF01386390} for single-source shortest paths in a graph. The constant-amortized-time implementation of the \opDc\ operation is the defining feature of the \emph{Fibonacci heap} \cite{DBLP:journals/jacm/FredmanT87} data structure. 

In \cite{DBLP:journals/algorithmica/FredmanSST86}, a new heap called the \emph{pairing heap} was introduced. The pairing heap is a self-adjusting heap, whose design and basic analysis closely follows that of splay trees \cite{DBLP:journals/jacm/SleatorT85}. They are much simpler in design than Fibonacci heaps, and they perform well in practice.
\begin{fullonly}
 They were included in the (pre-STL) C++ data structures library. We have shown that they have a working-set like runtime bound \cite{DBLP:conf/swat/Iacono00}. 
\end{fullonly}

It was conjectured at the time of their original presentation that pairing heaps had the same  amortized time \opEm, and  amortized time \opIns\ and \opDc\ as Fibonacci heaps, however, at their inception only a  amortized bound was proven for all three operations. Stasko and Vitter~\cite{DBLP:journals/cacm/StaskoV87} provided some simulation results which showed that  \opDc\ appeared likely. We have shown that \opIns\ does in fact have  amortized time \cite{DBLP:journals/corr/abs-1110-4428,DBLP:conf/swat/Iacono00}. 

However in \cite{DBLP:journals/jacm/Fredman99}, Fredman refuted the conjectured constant-amortized-time \opDc\ in pairing heaps by proving that pairing heaps have a lower bound of  on the \opDc\ operation. The result he proved was actually more general: he created a model of heaps which includes both pairing heaps and Fibonacci heaps, and produced a tradeoff between the number of bits of data augmented and a lower bound on the runtime of \opDc. Pairing heaps have no augmented bits, and were shown to have a  amortized lower bound on \opDc\ while he showed that in his model a  \opDc\ requires  bits of augmented information per node, which is the number of bits of augmented information used by Fibonacci heaps and variants\begin{fullonly}
 (A number called rank, which is an integer with logarithmic range is stored in each node)\end{fullonly}
 . More recently, Pettie has shown a upper bound of   for \opDc\ in pairing heaps \cite{DBLP:conf/focs/Pettie05}. It remains open where in the range  the true cost of \opDc\ in a pairing heap lies.

Elmasry has shown recently that a simple variant of pairing heaps has  amortized time \opDc\ operation \cite{DBLP:conf/esa/Elmasry10,DBLP:conf/soda/Elmasry09}. However, 
\begin{fullonly}
for technical reasons described later, including a non-standard implementation of \opDc, 
\end{fullonly}
this variant is not in Fredman's model and thus the  lower bound does not apply. 

So, it would seem from the preceding exposition that the situation has been essentially resolved: Fibonacci heaps are complex but optimal, while the elegant pairing heaps (and Elmasry's variant) are as good as a self-adjusting structure can get (One defining feature of a \emph{self-adjusting} structure is that they store no augmented data in every node). In the case of dictionaries, we have the hope of instance-based optimality, as evidenced by the dynamic optimality conjecture \cite{DBLP:journals/jacm/SleatorT85}, while in the case of heaps, self-adjusting structures can not even achieve optimal amortized asymptotic runtimes. We will now propose an alternate interpretation of the facts which leads to a much nicer conclusion.

Fredman's model is nuanced, and has limitations which cause us to introduce here a new model, which we call the \emph{pure heap} model. We first informally describe our model, and then describe how it differs from Fredman's.
A pure heap is a pointer-based forest of rooted trees, each node holding one key, that obeys the heap property (the key of the source of every pointer is smaller than the key of the destination); the nodes may be augmented, and all operations must be valid in the pointer model; heap pointers are only removed if one of the nodes is removed or if a \opDc\ is performed on the node that a heap pointer points to. 
This model is both simple and captures the spirit of many heaps  like the pairing heap, and is meant to be a clean definition analogous to that of the well-established binary search tree (BST) model \cite{DBLP:journals/siamcomp/Wilber89}.
However, Fibonacci heaps are not pure model heaps in the standard textbook presentation \cite{clrs} for the following reason: each node is augmented with a -bit integer (in the range ), and in the implementation of \opEm\ it is required to separate a number of nodes, call it , into groups of nodes with like number. This is done classically using bucket sort in time . However, bucket sort using indirect addressing to access each bucket is distinctly not allowed in the pointer model; realizing this, there is a note in the original paper \cite{DBLP:journals/jacm/FredmanT87}  showing how by adding another pointer from every node to a node representing its bucket, bucket sort can be be simulated at no additional asymptotic cost. However, these pointers are non-heap pointers to nodes with huge indegree which store no keys and thus violate the tree requirement of the pure heap model, as well as the spirit of what we usually think of as a heap. 

There have been several alternatives to Fibonacci heaps presented with the same amortized runtimes which are claimed to be simpler than the original. These include
thin heaps \cite{DBLP:journals/talg/KaplanT08},
violation heaps \cite{DBLP:journals/dmaa/Elmasry10},
and rank-pairing heaps \cite{DBLP:journals/siamcomp/HaeuplerST11}.
All of these heaps also use indirect addressing or non-heap pointers. The rank-pairing heap has the cleanest implementation of all of them, and implements \opDc\ by simply disconnecting the node from it parents and not employing anything more complicated like the cascading cuts of Fibonacci heaps.

We can easily modify Fibonacci heaps and the aforementioned alternatives to only use heap pointers by simply 
using  time to determine which of the  buckets each of the  keys are in. We call such a variant of rank pairing heaps a \emph{pointer Rank-Pairing} heap\shortfull{.}{, and list in in Table~\jref{t1}.} However, this alteration has the effect of increasing the time of \opDc\ in a Fibonacci heaps and their variants to . 

Fredman's model differs from ours in several regards. First, Fredman's model requires that comparisons can not be performed unless the nodes being compared must be linked by a heap pointer after the comparison. Second, in the course of an \opEm, any two children of the former root can be randomly accessed and compared at unit cost. Third, the number of augmented bits per node is a parameter of the model. The first restriction, while it admits pairing heaps and Fibonacci heaps, excludes Elmasry's variant. This is because Elmasry's variant sorts the keys in nodes in order to determine how to link them; such sorting is directly against what is allowed in Fredman's model. Our sort heaps, presented in section~\ref{full:s:sortheap}\shortfull{ in the appendix}{}, are not in Fredman's model for the same reason. Also, subjectively, we find the first restriction a bit odd, but it appears to be vital to the result. The second difference means that a fundamental cost in the pointer model is not counted: moving pointers to reach the desired nodes to compare or otherwise manipulate. So, compared to our model, Fredman's model is more restrictive because of the first condition, and more permissive with the second condition. We feel our model is more natural.
\begin{shortonly}
A more detailed discussion of how our model differs from Fredman's can be found in \S\ref{full:s:fh} in the appendix.
\end{shortonly}

Thus, we conclude that the reason that Fibonacci heaps have fast \opDc\ is not (only) because of the augmented bits as Fredman's result suggests, but rather because they depart from the pure heap model. We prove that any pure-heap-model heap has an  amortized lower bound on decrease key, \emph{no matter how many bits of data each node is augmented with}. 
In our view, Fibonacci heaps are a typical RAM-model structure that squeeze out a  factor over the best structure in a natural pointer-based model by beating the sorting bound using bit tricks (of which bucket sort is a very primitive example).


Given our lower bound, we still have the issue that it does not apply to any known self-adjusting heaps known to have fast \opDc\ times. We rectify this in \shortfull{the appendix in }{}\S\ref{full:s:sortheap}, by introducing the \emph{sort heap}. This heap is simply Elmasry's heap with the non-standard \opDc\ replaced with the standard one, or to put in another way, it is identical to the pairing heap except for the choice of pairings used to implement the \opEm\ operation. Our sort heap has an  amortized \opDc\ operation, and features an analysis that differers markedly from other self-adjusting heaps with fast \opDc.

The paper is structured as follows: 
\shortfull {a review of the priority queue ADT is in the appendix in \S\ref{full:s:pq},}{we begin by reviewing the priority queue ADT in \S\ref{full:s:pq}, }and then formally present our pure heap model (\S\jref{s:ph})\shortfull{.}{ followed by a comparison with Fredman's generalized pairing heap model (\S\jref{s:fh}).}
\shortfull {The main result, our lower bound, is presented in \S\jref{s:lb}. The presentation of sort heaps is in the appendix in \S\ref{full:s:sortheap}. }{The main result, our lower bound, is presented in \S\jref{s:lb}, and this is followed by sort heaps which are presented in \S\jref{s:sortheap}. }In \S\jref{s:comments} \shortfull{closing comments appear.}{with some thoughts and directions for further work.}



\begin{fullonly}
\section{The priority queue abstract data type} \jlabel{s:pq}

A \emph{priority queue} supports the following operations to maintain a totally ordered set \theset:

\begin{itemize}

\item \opIns: Inserts the key  into  and returns a pointer  used to perform future \opDc\ operations on this key.

\item \opEm: Removes the minimum item in  from  and returns it.

\item \opDc: Reduces the key value pointed to by  by some non-negative amount .

\end{itemize}

The key values can be in any form so long as they are totally ordered and an -time comparison function is provided. (This is more permissive than saying they they are comparison-based. We do not restrict algorithms from doing things like making decisions based on individual bits of a key). Some priority queues, including Fibonacci heaps and our sort heaps, also efficiently support the \opMg\ operation where two priority queues are combined into one.
\end{fullonly}


\section{The pure heap model} \jlabel{s:ph}

Here we define the \emph{pure heap} model, and how priority queue operations on data structures in this model are executed in it.

\subsection{Structural invariants and terminology}

The pure heap model requires that at the end of every operation, the data structure is an ordered forest of general heaps. 
Each node is associated with a key . We will use  both to refer to a key and the node in the heap containing the key. Inside each node is stored the key value, pointers to the parent, leftmost child and right sibling of the node, along with other possible augmented information.

The \emph{structure} of the heap is the shape of the forest, without regard to the contents of the nodes. The \emph{location} of a node is its position in the forest of heaps relative to the right (e.g. a node could be described as being the fifth child from the right of a node which is the third child from the right of the fourth root from the right. The idea is that location is invariant under adding new siblings to the left).

An algorithm in the pure heap model implements the priority queue operations as follows:
\shortfull{}{\begin{itemize\shortfull{*}{}}}
\shortfull{}\item \opIns\ operations are executed\shortfull{ at unit cost}{} by adding the new item as a new leftmost heap. \shortfull{}{The cost is defined to be 1.}
\shortfull{}\item \opDc\ is executed\shortfull{ at unit cost}{}  by disconnecting from its parent the node containing the key to be decreased (if it is not a root), decreasing the key, and then placing it as the leftmost root in the forest of heaps. \shortfull{}{The cost is defined to be 1.}
\shortfull{}\item A \opEm\ operation is performed by first executing a sequence of pointer-based suboperations which are fully described below in \S\jref{suboperations}. After executing the suboperations, the forest is required to be monoarboral (i.e.~have only one heap). 
Thus, the root of this single tree has as its key the minimum key in . This node is then removed, the key value is returned, and its children become the new roots of the forest. The cost is the number of suboperations performed. 
\shortfull{}{\end{itemize\shortfull{*}{}}}


Note that some data structures are not presented exactly in the framework as described above but can be easily put into this mode by being lazy. For example, in a pairing heap, the normal presentations of \opIns\ and \opDc\ cause an immediate pairing with the single existing root. However, such pairings can easily be deferred until the next \opEm, thus putting the resulting structure in our pure-heap framework. 
\begin{fullonly}
For a more elaborate example of transforming a heap into one based on pairings, see \cite{DBLP:conf/wae/Fredman99}.
\end{fullonly}




\subsection{Executing a \opEm; suboperations} \jlabel{suboperations}

\shortfull{To}{Obviously, in order to} execute an \opEm, the minimum must be determined. 
\shortfull{In an \opEm, the forest of heaps must be combined into a single heap using an operation called \emph{pairing}, which takes two roots and attaches the root with larger key value as the leftmost child of the root with smaller key value.}{At the beginning of the \opEm\ operation, the structure of the heap may consist of a forest of many heap-ordered trees, the pure-heap model requires that these trees be combined into one tree through a process called \emph{pairing}. The pairing operation takes two roots and attaches the root with larger key value as the leftmost child of the root with smaller key value.}\begin{shortonly}
\footnote{(Note that while the pairing operation brings to mind pairing heaps, it is the fundamental building block of many heaps. Even the \emph{skew heap} \cite{DBLP:journals/siamcomp/SleatorT86}, which seems at first glance to not use anything that looks like the pairing operation, can be shown in all instances to be able to be transformed into a pairing-based structure at no decrease in cost \cite{DBLP:conf/wae/Fredman99}).}
\end{shortonly}
\begin{fullonly}
(Note that while the pairing operation brings to mind pairing heaps, it is the fundamental building block of many heaps. Even the \emph{skew heap} \cite{DBLP:journals/siamcomp/SleatorT86}, which seems at first glance to not use anything that looks like the pairing operation, can be shown in all instances to be able to be transformed into a pairing-based structure at no decrease in cost \cite{DBLP:conf/wae/Fredman99}).  We require this process to happen in the pointer model, where there is some constant number of pointers that start at the leftmost root and move around and perform pairings. 

For the purposes of the analysis, it is needed to have a very fine view of what constitutes a constant-time suboperation, but, what is presented below is constant-time-equivalent to other natural ways of having a pointer model view with the basic primitive being pairing of the roots.
\end{fullonly}

In the execution of the extract-min operation, the use of some constant number  of pointers  is allowed. They are all initially set to the leftmost root. The constant  is a parameter of the model. \shortfull{}{These are the suboperations that are allowed to execute the \opEm\ operation:}


\begin{shortonly}
The suboperations include: move one of the pointers to the parent, left or right sibling, or leftmost child; pair the nodes pointed to by two pointers; and copy pointers.
A special suboperation, \subop{End}{}, signifies the execution of the \opEm\ is complete.
A full list of suboperations and their precise definitions and preconditions can be found in the \S\ref{full:suboperations} in the appendix.
\end{shortonly}
\begin{fullonly}
\begin{enumerate}

\item \jlabel{so:chpar} \subop{HasParent}{i}: Return if the node pointed to by  has a parent.

\item \subop{HasLeftSibling}{i}: Return if the node pointed to by  has a left sibling.

\item \subop{HasRightSibling}{i}: Return if the node pointed to by  has a right sibling.

\item \subop{HasChildren}{i}: \jlabel{so:chchild} Return if the node pointed to by  has any children.

\item \subop{Compare}{i,j}:\jlabel{so:compare} Return if the key value in the node pointed to by  is less than or equal to the key value in the node pointed to by .

\item \subop{Pair}{i,j}:\jlabel{so:pair} Perform a \emph{pairing} on two pointers  and  where the tree that  points to is attached to  as its leftmost subtree. It is a required precondition of this suboperation that both  and  point to roots, and that this was verified by the \textsc{HasParent} suboperation, and that the key value in the node pointed to to  is smaller than the key value in the node pointed to by , and that is was verified by the \textsc{Compare} suboperation.

\item \subop{Set}{i,j}: Set pointer  to point to the same node as .

\item \subop{MoveToParent}{i}: Move a pointer  to the parent of the node currently pointed to.
It is a precondition of this operation that the node that  points to has a parent, and that this was verified by the \textsc{HasParent} operation.

\item \subop{MoveToLeftmostChild}{i}: Move the pointer  to the leftmost child.
It is a precondition of this operation that the node that  points to has children, and that this was verified by the \textsc{HasChildren} operation.


\item \subop{MoveToRightSibling}{i}: Move the pointer  to the sibling to the right.
It is a precondition of this operation that the node that  points to has a right sibling, and that this was verified by the \textsc{HasRightSibling} operation.


\item \subop{MoveToLeftSibling}{i}: \jlabel{so:mvleft} Move a pointer  to the sibling to the left
It is a required precondition of this operation that the node that  points to has a left sibling, and that this was verified by the \textsc{HasLeftSibling} operation.


\item \subop{End}{}: \jlabel{so:end} Marks the end of the suboperation sequence for a particular \opEm.
It is a required precondition of this operation that the forest of heaps contains only one heap, and that this was verified through the use of the \subop{HasParent}{i}, \subop{HasLeftSibling}{i}, and \subop{HasRightSibling}{i} on a pointer  that points to the unique root.

\end{enumerate}

Operations \jref{so:chpar}-\jref{so:chchild} return a boolean; the remainder have no return value.

\end{fullonly}
The total number of suboperations, including the parameters, is defined to be . Observe that , which is  since  is a constant. 
A sequence of suboperations is a \emph{valid} implementation of the \opEm\ operation if all the preconditions of each suboperation are met and the last suboperation is an \subop{end}{}.




In the pure heap model, the only thing that differentiates between different algorithms is in the choice of the suboperations to execute \opEm\ operations. In these operations it is the role of the particular \emph{heap algorithm} to specify which suboperations should be performed for each \opEm. We place no restrictions as to how an algorithm determines the suboperation sequence for each \opEm\ other than the suboperation sequence must be valid.

This definition of an algorithm encompasses and is more permissive than allowing the algorithm to make decisions to be made based on some data augmented at any node. This is because augmented information is just one type of function of the previous operations whereas we allow the algorithm to decide what suboperations to perform in any manner, subject only to determinism. We also note that while the definition of the pairing operation enforces the heap structure, where every parent is smaller than its children, the algorithm is not restricted from looking at, for example, the individual bits of a key and deciding which pairings to perform based on this. 

\begin{fullonly}
To summarize, our notion of an algorithm allows the algorithm, at every step of determining which suboperation to execute next, to make decisions in any deterministic manner. These computations to determine which suboperation to execute next have no cost; only the execution of the suboperations themselves incurs a cost.
\end{fullonly}

\begin{fullonly}
\section{Fredman's model} \jlabel{s:fh}

Fredman's model, which he calls \emph{generalized pairing heaps} differ from our \emph{pure heap} model in a number of significant aspects:

\shortfull{}{\begin{itemize\shortfull{*}{}}}
\shortfull{}{\item} Generalized pairing heaps are parametrized by the number of bits of augmented data allowed at each node. Pure heaps allow an algorithm to branch as any function of the past; this is equivalent to allowing unlimited augmented data.

\shortfull{}{\item} Generalized pairing heaps limit how performing a comparison can be done by the algorithm. In the process of executing an \opEm\ operation, suppose a comparison is performed between two nodes, neither of which will be removed by the operation. This limits the number of comparisons to be performed in a \opEm\ to be linear in the number of pairings performed. In a generalized pairing heap, the result of this comparison can not be used to determine the future actions of the algorithm. In pure heaps, the result of such a comparison can be used. It is this crucial difference that places Elmasry's variant of the pairing heap and our sort heap in the pure heap model, but not in the generalized pairing heap model. These algorithms perform \opDc\ operations on selected roots after performing comparisons to sort them; sorting roots is out-of-model in the generalized pairing heap model; this is easy to see because sorting requires doing a super-linear number of comparisons. 

\shortfull{}{\item} Generalized pairing heaps do not take into account the time needed to access the items that are to be paired; arbitrary pairing of roots is allowed at unit cost. In the pure heap model, one needs to move pointers to the nodes to be paired using pointer-model-operations on the heap, and these operations must be paid for.
\shortfull{}{\end{itemize\shortfull{*}{}}}
\end{fullonly}

\section{Lower Bound} \jlabel{s:lb}

\subsection{Statement of result} \jlabel{sec:sor}

\begin{theorem} \jlabel{th:main}
In the pure heap model with a constant number  pointers, if\/ \opEm\ and \opIns\ have an amortized cost of , then \opDc\ has an amortized cost of .\end{theorem}

The proof will follow by contradiction and will consume the rest of this section. Assume that there is a pure heap model algorithm  where \opEm\ and \opIns\ have an amortized cost of at most , for some constant , and \opDc\ has an amortized cost of at most  , for some ; since this is a lower bound we can also safely assume that . 
The existence of the algorithm  ,  the constant  and the function  will be assumed in the definitions and lemmas that follow. A sufficiently large  is also assumed, as there are several places noted in what follows, such as using the results of asymptotic expressions, where this is required.

\subsection{Overview of proof}

The proof is at its core an adversary argument. Such arguments look at what the algorithm has done and then decide what operations to do next in order to guarantee a high runtime. But, our argument is not straightforward as it works on sets of sequences of operations rather than a single operation sequence. There is a hierarchy of things we manipulate in our argument:

\begin{fullonly}
\begin{description}
\end{fullonly}

\shortfull{\emph{Suboperaton.}}{\item[Suboperaton.]} The suboperations of \S\jref{suboperations} are the very basic unit-cost primitives that can be used to implement \opEm, the only operation that does not have constant actual cost. It is at this level that definitions have been made to enforce pointer model limitations.
\shortfull{\emph{Operation.}}{\item[Operation.]} We use \emph{operation} to refer to a priority queue operation. \shortfull{}{In this proof, the adversary will only use \opIns, \opDc, and \opEm.}
\shortfull{\emph{Sequence.}}{\item[Sequence.]} Operations are combined to form sequences of operations. 
\shortfull{\emph{Set of operation sequences.}}{\item[Set of operation sequences.]} Our adversary does not just work with a single operation sequence but rather with sets of operation sequences. These sets are defined to have certain invariants on the heaps that result from running the sequences that bound the size of the sets of operation sequences under consideration.
\shortfull{\emph{Evolutions.}}{\item[Evolutions.]} We use the word \emph{evolution} to refer to a function the adversary uses to take a set of operation sequences, and modify it. The modifications performed are to append operations to sequences, remove sequences, and to create more sequences by taking a single sequence and appending different operations to the end.
\shortfull{\emph{Rounds.}}{\item[Rounds.]} Our evolutions are structured into \emph{rounds}.

\begin{fullonly}
\end{description}
\end{fullonly}

The proof will start with a set containing a single operation sequence, and then perform rounds of evolutions on this set; the exact choice of evolutions to perform will depend on how the algorithm executes the sequences of operations in the set. The evolutions in a round are structured in such a way that most rounds increase the size of the set of operations. After sufficiently many rounds, an upper bound on the maximum size of the set of operation sequences will be exceeded, thus giving a contradiction.

Our presentation is structured as follows: 
In \S\jref{s:rank}, we define a rank function.
In \S\jref{s:monotonic}-\jref{s:augmented} we give some invariants and facts about the sequences of operations we will be considering.
In \S\jref{s:evolutiondefinition} we introduce the idea of a set of sequences and explain the invariants of the sets that will be maintained.
We introduce the idea of an evolution in \S\jref{s:evolving} and then describe several types of evolutions in \S\jref{s:einsert}-\jref{ev:perm}. 
These evolutions are structured into rounds in \S\jref{s:rounds}, technical lemmas about rounds appear in \S\jref{s:ubot}, and the final work to obtain the contradiction is in \S\jref{s:pit}.


\subsection{Ranks: definitions and useful facts} \jlabel{s:rank}

\subsubsection{Motivation}

As in many previous works on heaps and trees, the notion of the \emph{rank} of a node in the heap is vital. The rank of a node is meant to be a rough proxy for the logarithm of the size of the subtree of the node. While the basic analysis of pairing heaps and splay trees \cite{DBLP:journals/algorithmica/FredmanSST86,DBLP:journals/jacm/SleatorT85} use exactly this as the rank, the definition of rank here is more delicate and is an extension of that used in \cite{DBLP:journals/jacm/Fredman99}. As in \cite{DBLP:journals/jacm/Fredman99}, rank here is always a nonnegative integer. In previous definitions of rank, the value typically depended only on the current structure of the heap (One exception to this has been in order to get better bounds on \opIns, nodes are treated differently for potential purposes depending on whether or not they will ever be deleted. See \cite{DBLP:journals/cacm/StaskoV87,DBLP:journals/corr/abs-1110-4428,DBLP:conf/swat/Iacono00}). Here, however, the definition is more nuanced in that for the purposes of the analysis only, nodes are classified into \emph{marked} and \emph{unmarked} categories based on the history of the structure, and these marks, along with the current structure of the heap, are used to compute the rank of each node. 


\subsubsection{Definition} \jlabel{sec:def}



For ease of presentation, the rank of a node is defined in terms of the function .
\begin{fullonly}

\end{fullonly}
The general idea is to have the rank of a node be maintained so it is the negation of the key value stored in the node. (Ranks will be non-negative, and we will only give nodes non-positive integer key values; these can be assumed to be perturbed arbitrarily to give a total ordering of key values). The rank of a node can increase as the result of a pairing, and the value of a node can decrease as the result of a \opDc. It is thus our goal to perform a \opDc\ on a node which has had its rank increase to restore it to the negation of its rank. During the time between when a rank increase occurs in a node and the time the \opDc\ is performed, we refer to the node as \emph{marked}. 

Call the \emph{unmarked subtree} of a node to be the subtree of a node if all marked nodes were detached from their parents; the \emph{unmarked structure} of the heap is the structure of the unmarked subtrees of the roots. The rank of a node at a given time will be defined to be a function of the structure of its unmarked subtree.
\begin{fullonly}
 We emphasize that the notion of marking a node is for the purposes of the analysis only, such marks need not be stored.
\end{fullonly}

The following assumes a particular heap structure and marking, as the rank of a node is always defined with respect to the structure of the heap after executing a sequence of heap operations. 
\begin{fullonly}

\end{fullonly}
Let  be the node we wish to compute the rank of. Let  denote the number of unmarked children of , and let  denote these children numbered right-to-left (i.e., in the order which they became children of ).
\begin{fullonly}

\end{fullonly}
Let  be a subtree of  consisting of  connected to only the subtrees induced by . We will define the function  as a function of . The rank of a  node,  is . 
\begin{fullonly}

\end{fullonly}
Each node  may be labeled as \emph{efficiently linked} to its parent.
If , then  is said to be efficiently linked to . The case of  will never occur, as pairings will only happen among unmarked nodes, where the rank perfectly matches the negation of the key value.
\begin{fullonly}

\end{fullonly}
We will have the property that  is either  or ; In the latter case,  is called \emph{incremental}. 
\begin{fullonly}

\end{fullonly}
Given a node , let  be defined to be the index of the first incremental node in the sequence ;  is defined to be 0 if there is no such incremental node.
The set  is defined to be ; that is,  and the maximal set of its non-incremental siblings to the right.
\begin{fullonly}

\end{fullonly}
Given these preliminaries, we can now give the full definition of the rank of a node that has defined rank:
\begin{fullonly}

\end{fullonly}
\begin{shortonly}

\end{shortonly}
While the rank and mark are interrelated, there is no circularity in their definitions---whether a node is marked depends on its rank and key value and the rank of a node is a function of the ranks and marks of its children. 



\subsection{Useful facts about ranks}\jlabel{s:ufar}

\begin{obs}[Structural property of rank]\jlabel{lem:structrank}
Given two nodes  and  with different ranks, the unmarked structure of their induced subtrees must be different.
\end{obs}

This follows directly from the fact that the rank of a node is a function of its induced unmarked subtree.
\begin{fullonly}
\begin{lemma}The size of a unmarked subtree induced by a node of rank  is at most .
\end{lemma}

\begin{proof}
Let  be the size of the maximum unmarked heap of rank . Such a heap can be created from a maximal unmarked heap of rank , which has been paired to  maximal unmarked heaps of rank  and  maximal unmarked heaps of rank . Thus:




By induction, 

 

For sufficiently large ,  (recall that  and ), thus



which completes the lemma. 

\end{proof}

\begin{cor} \jlabel{c:rootrank}
If there are  nodes in a node 's unmarked induced subtree, the rank of  is at least .
\end{cor}


\begin{lemma}[{Number of efficiently linked children}]
Suppose node  has rank  and at most  unmarked children. Then,  has at least  efficiently linked unmarked children, each having rank .
\end{lemma}


\begin{proof} If there were less than  efficiently linked children, than at least  rank increments would be caused by the default case of the definition of rank and thus there would be at least  unmarked children, a contradiction.
\end{proof}


\begin{cor}\jlabel{cor:effcountidea}
Consider a root with umarked subtree size . 
The root has rank  by Corollary~\jref{c:rootrank}. Suppose it has  unmarked children. Then it has  efficiently linked unmarked children.
\end{cor}

Observe that:





We now use these observations to restate Corollary~\jref{cor:effcountidea}. 
\end{fullonly}
Set  and . 
\shortfull{Using some technical lemmas in \S\ref{full:s:ufar} the appendix gives:}{Then:}

\begin{cor}\jlabel{cor:effcount}
Suppose a root with unmarked subtree of size  has  children. Then it has  efficiently linked children. 
\end{cor}

We note that  since .


\subsection{Monotonic operation sequences} \jlabel{s:monotonic}

Call the \emph{designated minimum root} the next node to be removed in an \opEm.
\begin{fullonly}
\begin{definition}[Monotonic operation sequence] \jlabel{def:monotonic}
\end{fullonly}
Define a \emph{monotonic operation sequence} to be one where \opDc\ operations are only performed on roots, children of the designated minimum root, or marked nodes.
\begin{fullonly}
\end{definition}
\end{fullonly}
All of the sequences of operations we define will be monotonic. We will need the following two observations\shortfull{}{ about monotonic sequences}:


\begin{obs}[{Monotonic sequences and structure}] In a monotonic operation sequence, for any node  with descendent  where all nodes on the path from 's child down to and including   are unmarked,  will remain in the same location in 's subtree until  becomes the designated minimum root.
\end {obs}

\begin{obs}[{Monotonic sequences and rank}]\jlabel{obs:monotonerank}
In a monotonic operation sequence, the rank of a node never decreases, from the time it is inserted until the time it becomes the designated minimum root.
\end{obs}






\subsection{Augmented suboperation} \jlabel{s:augmented}


We augment \shortfull{}{suboperation~\jref{so:pair},} the \subop{Pair}\cdot\ operation, to return whether or not the rank was incremented as a result of the pairing. This augmentation does not give any more power to the pure heap model, since the exact ranks of all nodes is a function of the suboperation sequence and is thus known to the algorithm already.
We use this augmentation to create a finer notion of what constitutes a distinct sequence of suboperations. In particular, we will use the following fact:

\begin{lemma} \jlabel{lem:subdistinct}
Suppose  and  are two structurally distinct states of the data structure. Suppose a single valid sequence of suboperations implementing an \opEm\ is performed on both, and the outcomes of all augmented suboperations that have return values are identical in both structures. Then, the position of all nodes who have had their ranks changed is identical in both.
\end{lemma}

\begin{fullonly}
\begin{proof}
The only time a node can have it's rank change is when something is paired to it, and this is now explicitly part of the return value of operation~\jref{so:pair}.
\end{proof}
\end{fullonly}



\subsection{Evolutions of indistinguishable sequences} \jlabel{s:evolution}

\subsubsection{Definitions} \jlabel{s:evolutiondefinition}

Let    be a sequence of priority queue operations.
\begin{fullonly}

\end{fullonly}
Let  be the sequence of augmented suboperations and their return values used by algorithm \alg\ to execute operation  if  is an \opEm ; if it is not  is defined to be the empty sequence.  is the concatenation of .
\begin{fullonly}

\end{fullonly}
We call two sequences of priority queue operations  and  \emph{algorithmically indistinguishable} if , else they are \emph{algorithmically distinct}.
\begin{fullonly}

\end{fullonly}
Let  be the structure of the heap after running sequence ; the \emph{terminal structure} of  is  which we denote as . Recall that by structure, we mean the raw shape of the heap without regard to the data in each node, but including which nodes are marked. Two sequences  and  are \emph{terminal-structure indistinguishable} if , else they are \emph{terminal-structure distinct}.
Given a set of mutually algorithmically indistinguishable and terminal-structurally distinct (AI-TSD) sequences of heap operations , the \emph{distinctness} of the set,  is defined to be . 
\begin{fullonly}

\end{fullonly}
{Note that having two sequences which are algorithmically indistinguishable does not imply anything about them being terminal-structure indistinguishable. For example, it may be possible to add a \opDc\ to a sequence, changing the terminal structure, while the sequence of suboperations performed to execute the sequence remains unchanged. (Recall that suboperations only occur during \opEm\ operations).}
\begin{fullonly}

\end{fullonly}
A critical observation is that the number of terminal-structurally distinct sequences is function of ; this is the basis for the contradiction at the end of the proof:

\begin{lemma} \jlabel{lem:maxdistinct}
The maximum distinctness of any set  of terminal-structurally distinct sequences, all of which have terminal structures of size , is .
\proofapp{lem:maxdistinct}
\end{lemma}

\begin{fullonly}
\begin{proof}
The number of different shapes of a rooted ordered forest with  nodes is , the \superscript{th} Catalan number. The number of different ways to mark some nodes in a forest with  nodes is . Since , the maximum number of distinct structures is at most . 
\end{proof}
\end{fullonly}



\subsubsection{Evolving} \jlabel{s:evolving}

We will now describe several functions on AI-TSD sets of heap operations; we call such functions \emph{evolutions}. The general idea is to append individual heap operations or small sequences of heap operations to all sequences in the input set  and remove some of the resulting sequences so as to maintain the property that the sequences in the resultant set of sequences  are algorithmically indistinguishable yet terminal-structure distinct. The evolutions will also have the property that if the time to execute all sequences in  is identical, then the runtime to execute all sequences in  will also be identical. The difference in the runtime to execute sequences in  versus those in  will be called the \emph{runtime} of an evolution.



\subsubsection{Insert evolution} \jlabel{s:einsert}

The \emph{insert evolution} has the following form: .
\begin{fullonly}

\end{fullonly}
In an insert evolution, a single \opIns\ operation of a key with value 0 is appended to the end of all  to obtain . Given  is AI-TDS, the set  is AI-TDS and trivially . The runtime of the evolution is 1 since the added \opIns\ has runtime 1. The rank of the newly inserted node is 0, and is thus unmarked.



\subsubsection{Decrease-key evolution} \jlabel{s:edc}

The \emph{decrease-key evolution} has the following form: , where  is a location which is either a root or a marked node in all terminal structures of sequences in . 
\begin{fullonly}

\end{fullonly}
In a \emph{\opDc\ evolution}, a \opDc operation is appended to the end of all sequences in   to obtain . 
The value of  is chosen such that the new key value of what  points to is set to is the negation of its current rank; this means  is always nonnegative because of the monotone property of ranks noted in Observation~\jref{obs:monotonerank}. 
Observe that if  points to a marked node, then it is unmarked after performing a \eDc. This requirement ensures that all structures that are distinct before this operation will remain distinct after the operation. Thus, the set  is AI-TDS and trivially . The runtime of the evolution is 1 since the added \opDc\ has runtime 1.


\subsubsection{Designated minimum root evolution} \jlabel{s:edmr}

The \emph{designated minimum root evolution} has the form , where  is the position of one root which exists in all terminal structures of .
\begin{fullonly}

\end{fullonly}
In a designated minimum root evolution, a \opDc\ operation on  to a value of negative infinity is appended to all sequences in  to give . It will always be the case that the (next) evolution performed on  will be an \eEm\ evolution; the root , which is known as the \emph{designated minimum root}, will be removed from all terminal structures of  in this subsequent \eEm. There is no change in distinctness caused by this operation: . The runtime of the evolution is 1 since the added \opDc\ has runtime 1.



\subsubsection{Extract-min evolution} \jlabel{s:em}

The \emph{extract-min evolution} has the form .
\begin{fullonly}

\end{fullonly}
The \eEm\ evolution is more complex than those evolutions previously described, and the derivation of  from  is done in several steps. First, a \opEm\ operation is appended to the end of all sequences in  to obtain an intermediate set of sequences which we call . Recall that the \opEm\ operation is implemented by a number of suboperations. There is no reason to assume that the suboperations executed by the algorithm in response to the \opEm\ in each of the elements of  are the same; thus the set   may no longer be algorithmically indistinguishable. We fix this by removing selected sequences from the set  so that the only ones that remain execute the appended \opEm\ by using identical sequences of supoperations. This is done by looking at the first suboperation executed in implementation of \opEm\ in each element of , seeing which suboperation is the most common, and removing all those sequences   that do not use the most common first suboperation. If the suboperation is one which has a return value, the return value which is most common is selected and the remaining sequences are removed. This process is repeated for the second suboperation, etc., until the most common operation is \subop{End}{} and thus the end of all remaining suboperation sequences has been simultaneously reached. Since there are only a constant  number of suboperations, and return values, if present, are boolean, at most a constant fraction (specifically ) of  is removed while pruning each suboperation. At the end of processing each suboperation by pruning the number of sequences, the new set is returned as . The set  can be seen to be terminal-structure distinct, since pairing identically positioned roots in structurally different heaps, and having the same nodes win the pairings, can not make different structures the same. 


Observe that the nodes winning pairings in the execution of the  might have their ranks increase, and thus become marked. \shortfull{By}{Moreover, due to} Lemma~\jref{lem:subdistinct}, the position of all such nodes is identical in all terminal structures of . The set of the locations of these newly marked  nodes is returned as , the \emph{violation set}.

Now that it has been ensured that all of the sets of operations execute the appended \opEm\ using the same suboperations, we 
define  to be this common number of suboperations used to implement the \opEm; this value is returned by the evolution. As each suboperation reduces the distinctness by at most a constant,
. The runtime of the evolution is  since that is the cost of the added \opEm.


\subsubsection{Big/small evolution} \jlabel{s:bs}
The \emph{big/small evolution} has the form .
\begin{fullonly}

\end{fullonly}
The goal of the big/small evolution is to ensure that the terminal structures of all sets are able to be executed in the same way in subsequent evolutions.
In a big/small evolution, each terminal structure of each of the operation sequences of  is classified according to the following, using the previously-defined function : 
\begin{itemize\shortfull{*}{}}

\item The exact number of roots if less than  or the fact that the number of roots is greater than  (we call this case \emph{many-roots}). 

\item If the exact number of roots is less than :
\begin{fullonly}
\begin{itemize}
\end{fullonly}
\shortfull{}{\item} The position of the root with the largest subtree (the leftmost such root if there is a tie). Call it . Observe that the size of 's subtree is at least 
.
\shortfull{}{\item} The exact number of children of  if less than  (we call this case \emph{small}) or the fact that the number of roots is greater than  (we call this case \emph{root-with-many-children}).

\begin{fullonly}
\end{itemize}
\end{fullonly}


\end{itemize\shortfull{*}{}}

There are at most  possible classifications. We create set  by removing from   sequences with all but the most common classification of their terminal structures. 
The return value is based on the resultant classification:
\shortfull{}{\begin{description}}
\shortfull{\emph{Many-roots:}}{\item[Many-roots:]} Return  where  and .
\shortfull{\emph{}Root-with-many-children:}{\item[Root-with-many-children:]} Return  where  is the location of the root with the largest subtree and .
\shortfull{\emph{Small:}}{\item[Small:]} Return  where  is the location of the root with the largest subtree and and .
\shortfull{}{\end{description}}

We bound the loss of distinctness, which is the logarithm of the number of classifications.
 Since , then , and thus . The runtime of the evolution is 0 since no operations are added to any sequence.



\subsubsection{Permutation evolution} \jlabel{ev:perm}



The \emph{permutation evolution} has the form , where the leftmost root  has in all terminal structures of the sequences of   a subtree size of at least  and at most  children; this will be achieved by being in the  small case of the big/small evolution and performing a decrease-key evolution on the relevant node.
It is also required that all terminal structures of sequences in  are entirely unmarked.
\begin{fullonly}

\end{fullonly}
In a permutation evolution, the goal is to increase the distinctness, and is the only evolution to increase the number of sequences\shortfull{.}{ in the process of converting  to .} 

\shortfull{By}{Combining the preconditions of the permutation evolution with} corollary~\jref{cor:effcount}\shortfull{}{, yields the fact that} all nodes in the terminal structures of  at location  have at least  efficiently linked children; since there are at most  efficiently linked children of each rank, \shortfull{}{that means} there are at least   efficiently linked children of different ranks in each terminal structure. Find such a set and call it the \emph{permutable set\shortfull{ (PS)}{}}\shortfull{.}{(chose one arbitrarily if there is more than one possibility).} \shortfull{P}{Look at the position of these permutable sets in all terminal structures of the sequences of , and p}ick the position of the \shortfull{PS}{permutable set} that is most common. Form the intermediate set of sequences  by removing from  all sequences that do not have this commonly located \shortfull{PS}{permutable set}.
An easy upper bound on the number of different locations \shortfull{PS}{permutable set}s could be in is
\shortfull{r}{}\Distinct(\hat{\Ids})-\Distinct(\Ids)=   - 
O\left( \log \left( \frac{\rcg \log \cst}{\rce   } \cdot  {\rcf  \log \cst} \right) \right).\shortfull{\rcf=o(\log n)\rcg=o(1)\rce=\omega(1)}{}
\end{fullonly}

\shortfull{Using the definitions of  and , the \shortfull{PS}{permutable set} is of size}{The \shortfull{PS}{permutable set} is of size . Using the definitions of  and , this is} . Let  be a constant such that the \shortfull{PS}{permutable set} is of size at least  for sufficiently large . Recall that in all the terminal structures
of the sequences of , the ranks of the items in the permutable sets are different, and in fact are strictly increasing, when viewed right-to-left as children of .


 We then create  by replacing each sequence in  with  new sequences created by appending onto the end of each existing sequence a sequence of all possible permutations of \opDc\ operations on all elements of an arbitrary subset of size  of the \shortfull{PS}{permutable set}.

The fact that all of the sequences in  have the same \shortfull{PS}{permutable set}s ensures that all terminal structures in  are terminal-structure distinct. 
 (Recall that Lemma~\jref{lem:structrank} says that different ranks imply different structures of induced subtrees). 
Thus, in this step distinctness increases by \shortfull{, which dominates the total change of disctintness.}{.} 
\begin{fullonly}

Thus, combining all the steps of the permutation evolution bounds the total increase of distinctness by
\shortfull{.}{

}

\end{fullonly}
\shortfull{The evolution costs at most , since that is the number of unit-cost \opDc\ operations appended to the sequences.}{The cost of the evolution is at most , since that is the number of \opDc\ operations appended to the sequences, and these all have unit cost.}


\subsection{Rounds} \jlabel{s:rounds}

\begin{fullonly}
\begin{algorithm}
\caption{Algorithmic
 presentation of how evolutions are used to build the sequence of AI-TSD sequences 
 , which are split into rounds where the index of the start of round  is .
} 
\jlabel{a:evolve}
\begin{algorithmic}
\State 
\State 
\State 
\State ;
\Loop
\State ; 
\If {} 
\State ; \Comment{Small round}
\State ; 
\Else 
\State ; \Comment{Big round}
\EndIf
\State  ; \Comment{Common to small and big rounds}
\For{each  in }
\State ;
\EndFor
\State ; 
\State ; 
\EndLoop
\end{algorithmic}

\end{algorithm}
\end{fullonly}



\shortfull{A sequence of evolutions  defines}
{We proceed to perform a sequence of evolutions  to define} a sequence of AI-TSD sets . The initial set  consists of a single sequence of operations: the operation \opIns, executed  times. Each subsequent AI-TSD  set  is derived from    by performing the single evolution ; thus in general  is composed of some of the sequences of  with some operations appended. 

These evolutions are split into \emph{rounds};  is the index of the first AI-TSD set of the th round. Thus round  begins with AI-TSD set  and ends with  through the use of evolutions

\begin{fullonly}

\end{fullonly}
 These rounds are constructed to maintain several invariants:
\shortfull{}{\begin{itemize}\item}
All terminal structures of all sequences in the AI-TSD set at the beginning and end of each round have size . This holds as in each round, exactly one \opIns\ evolution and exactly one \opEm\ evolution is performed.
\shortfull{}{\item} All nodes in all terminal structures in the AI-TSD sets at the beginning and end of each round are unmarked.
\shortfull{}{\end{itemize}}

There are two types of rounds, \emph{big rounds} and \emph{small rounds}. 
At the beginning of both types of round a big/small evolution is performed\shortfull{ which }{. The return value of the big/small evolution} determines \shortfull{the round type.}{whether it will be a big or a small round.}
\begin{fullonly}

\end{fullonly}
The reader may refer to Algorithm~\ref{full:a:evolve} 
\begin{shortonly}
in the Appendix
\end{shortonly}
for a concise presentation of how evolutions are used to construct . \shortfull{}{We now describe this process in detail.}


\subsubsection{The Big Round} \jlabel{s:br}

As the round begins, we know that the terminal structures of the AI-TSD set are entirely unmarked, and there are either at least  roots, or one root with at least  children.  The round proceeds as follows:
\begin{fullonly}

\end{fullonly}
\begin{shortonly}
\begin{inparaenum}[(1)]
\end{shortonly}
\begin{fullonly}
\begin{enumerate}
\end{fullonly}
\item Perform a designated minimum root evolution on the root with largest subtree; this is the node  from the big-small evolution whose location is encoded in the return value; as a result of the big-small evolution it is guaranteed to be in the same location in all of the terminal structures of the sequences of . \item Perform a \opEm\ evolution. \item For each item in the violation sequence returned by the \opEm\ evolution, perform a \opDc\ evolution. This makes the terminal structures of all heaps in  mark-free. \item Perform an \opIns\ evolution. \begin{shortonly}
\end{inparaenum}
\end{shortonly}
\begin{fullonly}
\end{enumerate}
\end{fullonly}

Assuming we are in round , Let  be the cost of the \opEm\ evolution, and let  be the size of the violation sequence.
The cost of the round (the sum of the costs of the evolutions) is , which is at least , and based on the evolutions performed the distinctness can be bounded as follows: 
\shortfull{}{}\Distinct_i-\Distinct_{i+1}=\overbrace{O(\acem_i)}^\opEm +\overbrace{O(\log \log n)}^\eBs-\overbrace{\Omega\left( \frac{\log n \log \log n}{ \fdc \log \fdc} \right)}^\ePerm\shortfull{\Ids_{\round_k}O(k \log n)\opB\Ids_{\round_k}\runbB\dcc_ii \frac{\rcm \log n}{\fdc \log \fdc}iii2+\dcc_i +    \acem_i +  \vs_i\opB \in \Ids_{\round_k}\rce\cem \log n1+dc_i + \vs_i\LL{\fdc}\dcc_i \leq \frac{\rcm\log n}{\fdc \log \fdc}\frac{\rcm\log n}{\fdc \log \fdc} \leq \frac{\log n}{\fdc}n\rcm\sum_{i=1}^k 
   \acem_i \leq \runb\rce\rce=2\fdc+1n\frac{\fdc}{\log n} +
\frac{1 }{\log n} 
   + 
    \frac{1}{\fdc}<1\cemk\frac{k}{2}\frac{k}{2}{\rcf {} \log \cst}\fdc=\omega(1)O(k \log n)ii\sum_{i=1}^k (\acem_i)\Ids_{\round_k}O(k {} \log n )\sum_{i=1}^k \acem_i = O(k {} \log n )}{}
Since , 
, and thus
the negative terms in\shortfull{ the previous equation}{~\jeqref{eq6}} can be absorbed, giving:
\shortfull{}{\zeta(x)=\frac{R(x)-\frac{1}{3}S(x)}{\frac{1}{3}S(x)}.S(x_i) \geq \left( \frac32 \right)^{\ell-i}S(x_\ell),   
 S(x_i) \geq \left( \frac32 \right)^{\ell-i}M . \jlabel{eq:grow}

 \zeta'(x_i) - \zeta(x_i)
& = \overbrace{\min\left(\overbrace{1}^{\substack{\text{right}\\\text{heavy}}},\overbrace{\frac{R'(x_i)-\frac{1}{3}S'(x_i)}{\frac{1}{3}S'(x_i)}}^{\text{transitional}}\right)}^{\text{potential after}} - 
\overbrace{\max \left( \overbrace{0}^{\substack{\text{left}\\ \text{heavy}}},\overbrace{\frac{R(x_i)-\frac{1}{3}S(x_i)}{\frac{1}{3}S(x_i)}}^{\text{transitional}} \right)}^{\text{potential before}} \nonumber \\
\intertext{The use of min and max works because the formula for a transitional node would be at most 0 when applied to a left heavy node and at least 1 when applied to a right heavy node. Removing the min and max can only increase the gain:}
& \leq \frac{R'(x_i)-\frac{1}{3}S'(x_i)}{\frac{1}{3}S'(x_i)} - 
 \frac{R(x_i)-\frac{1}{3}S(x_i)}{\frac{1}{3}S(x_i)}  \nonumber \\
\intertext{Using the fact that , and : }
&= \frac{R(x_i)-\frac{1}{3}(S(x_i)-M)}{\frac{1}{3}(S(x_i)-M)} - \frac{R(x_i)-\frac{1}{3}S(x_i)}{\frac{1}{3}S(x_i)}\nonumber   \nonumber \\
&= \frac{3R(x_i)M}{S(x_i)(S(x_i)-M)} \nonumber \\
\intertext{Since }
&<\frac{3M}{S(x_i)-M} \nonumber \\
\intertext{Using \jeqref{eq:grow}:}
&\leq \frac{3M}{\left( \frac32 \right)^{\ell-i}M-M} \nonumber \\
&= \frac{3}{\left( \frac32 \right)^{\ell-i}-1} \jlabel{eq:cutbound}

\sum_{i=1}^{\ell-1} (\zeta'(x_i)-\zeta(x_i)) \leq
 \sum_{i=1}^{\ell-1} \frac{3}{\left( \frac32 \right)^{\ell-i}-1} < 12 \jlabel{eq:sumcutbound}
 \nonumber
 
& \overbrace{O( \ell \log n \log \log n)}^{\text{actual cost}} +\overbrace{ \ell c \log_\frac32  n \log \log n}^{\text{new potential}}- \overbrace{(\ell-1)( \consd c \log n \log \log n - c\log_\frac32  n \log \log n)}^{\text{old potential}}
\nonumber
\\
& =O(\ell  \log n \log \log n)- \overbrace{\left( \consd -\frac{2}{\log \frac32} \right)}^{\approx 0.58}  \ell c  \log n \log \log n  + 
\overbrace{\left(\consd-\frac{1}{\log \frac32} \right)}^{\approx 2.29}  c\log n \log \log n
\intertext{As we noted in the definition of the potential, the choice of the constant  has been deferred until this point. The constant  is chosen to be large enough so that the second term is sufficiently large so as to cancel the big-O expression, thus giving the amortized cost as:}
& \leq \left(\consd-\frac{1}{\log \frac32} \right) c\log_\frac32 n \log \log n= O(\log n \log \log n).
\nonumber




\end{proof}
\end{fullonly}

\section{Comments} \jlabel{s:comments}

Both the proof presented here and the proof of Fredman share many similar aspects. Fredman's restriction that comparisons can not be performed without a pairing allowed him to observe a property he called \emph{consistency}. This is the notion that he did not have to worry about the key values themselves, since their ranks were a perfect proxy for key values. Much of our effort has been spent to get a result without the consistency property. To do this, we have made some changes to the rank function compared to Fredman so that the rank of a node changes more slowly; we attempt to keep a node's rank in sync with its key value, but this is not completely possible. However, we show that through the use of the violation list and additional \opDc\ operations to re-key items whose rank and key value no longer correlate, something like consistency can be managed. The ideas of evolution and having the adversary maintain sets of sequences are new.

\begin{fullonly}
Looking forward, there are still unanswered questions and loose ends for possible future work:

\shortfull{}{\begin{itemize\shortfull{*}{}}}

\shortfull{}{\item} Our lower bound of  for \opDc, differs by a  factor from the best known pure heaps, and also from Fredman's lower bound. Can our low bound can be improved to remove the triple log, or is there a heap in our model with  amortized \opDc? Such a heap would necessarily not be in Fredman's model.

\shortfull{}{\item} In our definition of the pure-pointer model of heaps, we do not allow the algorithm to detach a node from its parent unless a \opDc\ is performed on it or if its parent is being removed as part of a \opEm\ operation. The lower bound should be able to be extended to allow such operations, which would make our lower bound apply to Elmasry's variants of heaps.


Similarly, our lower bound does not apply to Fibonacci heaps largely because of the bucket sorting used; this is intentional. However, we described the Pointer Fibonacci heap which made the Fibonacci heap into one with only heap pointers.
However, the Pointer Fibonacci heap remains outside of our pure heap model because the implementation of \opDc\ involves more than just cutting the node from its parent; there is a process called \emph{cascading cuts} whereby, in some cases, ancestors of the node being \opDc ed also are cut from their parents (recall from the introduction, in contrast, rank-pairing heaps are a simplification of Fibonacci heaps that do not use cascading cuts, and thus our lower bound applies to the natural pointer model variant of them). This technicality caused by the cascading cuts places them outside of our model. Fredman's lower bound also would not be able to deal with the cascading cuts, but his sequence of operations used in the lower bound only performs \opDc\ operations on roots and children of the the roots, which prevents Fibonacci heaps from ever invoking the cascading cut. Although our \opDc\ operations performed as part of the permutation evolution also are only performed on roots or children of roots, the \opDc\ operations which are performed on marked nodes as part of processing the violation list could cause cascading cuts to be performed.
We believe that our bound could be extended to allow the algorithm to unlink arbitrary nodes from their parents to cover situations like cascading cuts. 

\shortfull{}{\item} Our sort heap has a -time \opEm\ and  \opDc. Is there a pure heap model heap with no augmented data with  amortized time \opEm\ and  \opDc? We still do not know whether or not pairing heaps are such a heap.
\end{fullonly}


\shortfull{}{\end{itemize\shortfull{*}{}}}

\begin{fullonly}
\section{Acknowledgments}

The idea behind this work came to me while listening to Robert E.~Tarjan give a talk entitled \emph{Rank-Pairing Heaps} 
at the 4th Bertinoro Workshop on Algorithms and Data Structures (ADS) in 2009. I would like to thank the organizers of the workshop, Andrew V. Goldberg,
Giuseppe F. Italiano,  
Valerie King, and
Robert E. Tarjan for inviting me and creating an incredible forum for data structures research.
I would also recognize the contribution of the late Mihai P\v{a}tra\c{s}cu for some conversations we had at ADS 2009, sitting in the shadow of the Colonna delle Anelle, about the possibility of a bound of the type presented here.
Mark Yagnatisky and several anonymous reviewers read earlier versions this paper and pointed out a number of minor bugs. 
Finally I would like to thank my doctoral advisor, Michael L.~Fredman. He had me read his then-new lower bound for my qualifying exam in 1999; without intimate knowledge and understanding of that lower bound, this one would not be possible.
\end{fullonly}