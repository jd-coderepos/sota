\section{Experiments and Results}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/toy_tasks.pdf}
    \caption{\textbf{Corridor Task (left)}- Agents must memorize the color of an object and walk through the door of the corresponding color at the end of a long corridor. \textbf{Portal Task (middle)}- An agent must trial-and-error to memorize the  sequence of doors. \textbf{Instruction Task (right)}- A model must recognize instructions, memorize them, and execute when at the correct location.}
    \label{fig:corridor_img}
\end{figure*}

We show that \textsc{Expire-Span} focuses on salient information on various constructed and real-world tasks that necessitate expiration. First, we describe baselines and efficiency metrics for comparing various models. Second, we illustrate the importance of expiration on various constructed tasks. Then, we highlight the scalability of \textsc{Expire-Span} when operating on extremely large memories.
Additional experiments and details are in the appendix.


\begin{figure*}[t]
    \includegraphics[width=0.31\linewidth]{figs/plot_corridor.pdf}
    \hspace{0.3cm}
    \includegraphics[width=0.305\linewidth]{figs/plot_portal.pdf}
    \hspace{0.3cm}
    \includegraphics[width=0.31\linewidth]{figs/plot_instruction.pdf}
    \hspace{0.3cm}
    \vspace{-3mm}
    \caption{We plot performance as a function of memory size for three tasks. Training scores are shown. Ideal models can achieve strong performance with small memories by identifying which information is important to remember. \textbf{Corridor Task (left)} --- We train 10 baseline models with different memory sizes, and five \textsc{Expire-Span} models with different seeds. 
    \textbf{Portal Task (middle)}- We train models with different memory sizes and random seeds. \textbf{Instruction Task (right)} --- We train 6 baseline models with different memory sizes, and five \textsc{Expire-Span} models with different seeds.}
    \label{fig:corridor_result}
\end{figure*}

\subsection{Baselines}
We compare our method against several baselines from~\secc{back} that take different approaches to access information in the past. 
We compare the performance of these methods, along with two efficiency metrics: GPU memory and training speed for a fixed model size and batch size.
First, we compare to \textit{Transformer-XL}~\citep{dai2019transformer}, which corresponds to the fixed-span approach where simply the last $L$ memories are kept. 
Our Transformer-XL implementation also serves as a base model for all the other baselines to guarantee that the only difference among them is how memories are handled. 
The other baselines are \textit{Adaptive-Span}~\citep{sukhbaatar2019adaptive} and \textit{Compressive Transformer}~\citep{rae2020compressive}, two popular approaches for long memory tasks. For Compressive Transformer, we implemented the  mean-pooling version, which was shown to have strong performance despite its simplicity. 

\subsection{Importance of Expiration: Illustrative Tasks}

\paragraph{Remembering One Key Piece of Information}

To illustrate a case where proper expiration of unnecessary memories is critical, we begin with an RL gridworld task: walking down a corridor. In this \textit{Corridor} task, depicted in Figure~\ref{fig:corridor_img} (left), the agent is placed at one end of a very long corridor, next to an object that is either red or blue. The agent must walk down the corridor and go to the door that corresponds to the color of the object that it saw at the beginning to receive $+1$ reward. The requirement on the memory is very low: the agent only needs to remember the object color so it can walk through the correct door. 

\textsc{Expire-Span} models can take advantage of this fact and keep the memory size small regardless of the corridor length, which can vary between 3 and 200. 
This is confirmed in the results shown in Figure~\ref{fig:corridor_result} (left) where the \textsc{Expire-Span} models achieve high performance on this task with very small memories. 
Without the ability to forget, the Transformer-XL models require large memory for storing all navigation steps that grow with the corridor length.


\paragraph{Remembering Long Sequences of Information} 

Next, we analyze \textsc{Expire-Span} on another reinforcement learning task, but this time testing memorization of sequences: Portal through Multiple Rooms. An agent in a gridworld must navigate through multiple rooms separated by different doors, depicted in Figure~\ref{fig:corridor_img} (middle). Each room has two exit doors with different colors --- one door portals to the adjacent room, while the other portals back to the start. 
However, which door works in which room is random for each episode. 
Thus, the only way to visit more rooms is by trial-and-error, where agents need to remember the sequence of correct doors to successfully navigate to the end. 
The environment is  partially observable and randomized at each episode.

We display results in Figure~\ref{fig:corridor_result} (middle). The Transformer-XL models need longer memory to perform better and visit more rooms, because each new room requires many navigation steps to reach.
However, those navigation steps are actually irrelevant because the agent only needs to memorize the colors of the correct doors.
Usually, the agent needs to pass through the same room multiple times to solve the remaining rooms, but it only needs to remember the door color from the first pass, while all subsequent passes can be expired.
Since \textsc{Expire-Span} models can discard irrelevant memories and focus its memory on memorizing the exact sequence of door colors, they achieve strong performance with much smaller memory compared to the Transformer-XL baseline. 



\paragraph{Remembering Long Sequences with Severe Distractors}

To illustrate a more difficult task where a model must learn to expire,
we use a dialogue-based story generation task from the LIGHT~\citep{urbanek2019learning} text world game environment. The model visits various locations and \textit{receives} instructions of the form \textit{can you tell the [butler] that the [town official] wants to see them?}. When the model is in a location where the \textit{butler} is present, they must \textit{execute} the instruction by generating \textit{You tell the butler ``town official wants to see you!''}.  Between receiving and executing, thousands of words of distractor text exist as shown \fig{corridor_img} (right). The model must learn to expire the distractors. Note multiple instructions can be in queue for execution.

We experiment with a dataset where the average distance between receiving and executing instructions is around 950 distractor words. Models are trained as language models, but evaluated only on their success in executing the instruction. 
Task details and model architecture are provided in the appendix. We illustrate in Figure~\ref{fig:corridor_result} (right) that \textsc{Expire-Span} is much more successful at this task than Transformer-XL and Adaptive-Span (see the appendix), as it can focus on the specific instruction lines. 



\subsection{Scalability of Expire-Span}

We analyze the scalability of \textsc{Expire-Span}. On a copy task, we train models with spans up to 128k timesteps. Then, we show the utility of \textsc{Expire-Span} on character-level language modeling --- Enwik8 and PG-19 --- and a moving objects task that is processed frame by frame. For these tasks, we also analyze the efficiency of \textsc{Expire-Span} compared to existing methods, and demonstrate that our method has a smaller memory footprint and faster processing speed. We quantify efficiency with two metrics: \textbf{(1)} peak GPU memory usage and \textbf{(2)} training time per batch (comparing fixed batch size for similar size models). 


\paragraph{Extremely Long Copy}

To illustrate the scalability of \textsc{Expire-Span}, we construct a copy task where the model sees a sequence of \textit{A} very far in the past. The rest of the characters are \textit{B}. The model must copy the correct quantity of \textit{A}. We design the task such that a long span (up to 128k) can be required, as the \textit{A} tokens are very far into the past. In Table~\ref{tab:copy}, we show that only by scaling the maximum span to 128k it is possible to achieve improved performance. We compare to a Transformer-XL baseline with 2k attention span and a \textsc{Expire-Span} model with smaller span.

\begin{table}
    \centering
    \begin{tabular}{lcc}
    \toprule
    Model & Maximum span & Accuracy (\%) \\
    \midrule
    Transformer-XL & 2k &  26.7 \\ 
    \textsc{Expire-Span} & 16k &  29.4 \\ 
    \textsc{Expire-Span} & 128k & \bf 52.1 \\ 
    \bottomrule
    \end{tabular}
    \caption{
    \textbf{Copy Task.} We report accuracy on the test set.
    }
    \label{tab:copy}
\end{table}


\paragraph{Character Level Language Modeling: Enwik8} 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/plot_enwik8.pdf}
    \caption{\textbf{Performance as a Function of Memory Size on Enwik8.} Lower bpb and smaller memory size is better.}
    \label{fig:enwik8_span}
\end{figure}

\begin{table}[t]
\setlength{\tabcolsep}{2.0pt}
    \centering
    \begin{tabular}{lcc}
    \toprule
    Model & Params & Test \\
    \midrule
    \multicolumn{3}{l}{\small\emph{Small models}} \\
    Trans-XL 12L~{\footnotesize {\citep{dai2019transformer}}} & 41M  & 1.06 \\
    Adapt-Span 12L~{\footnotesize {\citep{sukhbaatar2019adaptive}}} & 39M & 1.02 \\
    Our Trans-XL 12L baseline & 38M  & 1.06 \\
    \textsc{Expire-Span} 12L & 38M & \bf 0.99 \\ 
    \midrule
    Trans-XL 24L~{\footnotesize {\citep{dai2019transformer}}} & 277M  & 0.99 \\
    Sparse Trans.~{\footnotesize {\citep{child2019generating}}} & 95M  & 0.99 \\
    Adapt-Span 24L~{\footnotesize {\citep{sukhbaatar2019adaptive}}} & 209M & 0.98 \\
    All-Attention~{\footnotesize {\citep{sukhbaatar2019augmenting}}} & 114M  & 0.98 \\
    Compressive Trans.~{\footnotesize {\citep{rae2020compressive}}} & 277M  & 0.97 \\
    Routing Trans.~{\footnotesize {\citep{roy2020efficient}}}& - & 0.99 \\ 
    Feedback Trans.~{\footnotesize {\citep{fan2020accessing}}} & 77M &  0.96 \\    
    \textsc{Expire-Span} 24L & 208M &  \bf 0.95 \\
\bottomrule
    \end{tabular}
    \caption{
    \textbf{Enwik8 Results.} We report bit-per-byte (bpb) on test and the number of parameters.
    }
    \label{tab:enwik8}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/plot_pg19.pdf}
    \caption{\textbf{Performance on Character-level PG-19.}  We report bit-per-character on test.}
    \label{fig:pg19}
\end{figure}

We subsequently experiment on Enwik8 for character level language modeling~\citep{mahoney2011large}.  
We compare the performance of \textsc{Expire-Span} with Adaptive-Span and Transformer-XL, varying the average span size (see \fig{enwik8_span}). Models with \textsc{Expire-Span} achieve stronger results --- when comparing at any given memory size, \textsc{Expire-Span} outperforms both baselines. Further, the performance of \textsc{Expire-Span} does not vary much even if the memory size is drastically reduced, indicating the model retains a small quantity of salient information for good performance.

Next, we compare \textsc{Expire-Span} to existing work in \tab{enwik8}. A small \textsc{Expire-Span} model with the maximum span $L=16\text{k}$ outperforms similarly sized baselines by a large margin.
We also trained a larger \textsc{Expire-Span} model with $L=32\text{k}$ and LayerDrop~\cite{Fan2020Reducing}, which outperforms the Compressive Transformer and sets a new state of the art on this task. 
This indicates that models can learn to expire relevant information and encode long context effectively, even on very competitive language modeling benchmarks.

Finally, we compare the efficiency of \textsc{Expire-Span} with the Transformer-XL, Adaptive-Span and Compressive Transformer baselines. 
We find that \textsc{Expire-Span} models achieve much better performance, as shown in Table~\ref{tab:efficiency} with substantially less GPU memory and faster training time per batch. 


\paragraph{Character Level Language Modeling: PG-19} 

We use the PG-19~\citep{rae2020compressive} benchmark and convert it to character-level language modeling with a vocabulary size of 3506.
We train several baselines: Transformer-XL with maximum spans of 1k and 2k, and Adaptive-Span and Compressive Transformers with 16k span.
We train \textsc{Expire-Span} with maximum spans of 8k and 16k.
We present results in Figure~\ref{fig:pg19}, where we show that  \textsc{Expire-Span} is substantially better than Transformer-XL, and matches the performance of Adaptive-Span and Compressive Transformer.

\begin{figure}[t]
    \centering 
    \includegraphics[width=0.8\linewidth]{figs/collision_quadrant.png}
     \caption{\textbf{Object Collision} task tests if models can remember the location of specified colored collisions.}
    \label{fig:collision}
\end{figure}

However, \textsc{Expire-Span} uses its available memory very effectively. The 16k maximum span \textsc{Expire-Span} model has an average memory size of 860. 
In comparison, the Adaptive-Span model has an average memory size of 2440, almost 3x that of the 16k \textsc{Expire-Span} model. This indicates that \textsc{Expire-Span} enables models to identify the critical bits of information and expire the rest, reaching the same performance with a much smaller memory.

Finally, comparing efficiency (Table~\ref{tab:efficiency}),  \textsc{Expire-Span} trains at double the speed of Compressive Transformer. \textsc{Expire-Span} is faster than Adaptive-Span, though uses slightly more memory. The memory usage of \textsc{Expire-Span} is usually lower, around 12GB, but spikes for some sentences. Lastly, while the average span size of \textsc{Expire-Span} is lower than Adaptive-Span, the computation requires additional tensors allocated in memory, which can potentially be addressed by an optimized implementation. 

\paragraph{Frame-by-Frame Processing: Object Collision} 

An important setting where learning which long context may be important is in video understanding, a field with increasing focus as model architectures provide the capability to process long sequences.  Despite video data being memory intensive, salient events might be localized in space and time.
We test our model on a task where two objects move around and collide, and the goal is to reason about the location of specified-color collisions. Objects have a color that can randomly change. We divide the grid into four quadrants and the model is asked to recall the quadrants of the last collision of a specific color pair. Because the collisions are rare, and collisions of specific colors are even rarer, the model must process a large quantity of frames.  


\begin{table}[t]
    \centering
    \begin{tabular}{lcc}
    \toprule
    Model & Maximum Span & Test Error (\%) \\
    \midrule
    Transformer-XL & 1k & 73.3 \\
    Compressive & 8k & 63.8 \\
    Adaptive-Span & 16k & 59.8 \\
    \midrule 
    \multirow{3}{*}{\textsc{Expire-Span}}  & 16k & 52.2 \\
     & 32k & 36.7 \\ 
     & 64k & \bf 26.7 \\ 
    \bottomrule
    \end{tabular}
    \caption{
    \textbf{Results on Object Collision}. We report the error on the test set comparing to various baselines.
    }
    \label{tab:colliding}
\end{table}


\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{0.8}
    \begin{tabular}{lllccc}
    \toprule
    && Model &  Performance & GPU Memory (GB) & Time/Batch (ms) \\
    \midrule
    \multirow{4}{*}{Enwik8} && Transformer-XL  & 1.06 bpb & 27 & 649 \\ 
    && Compressive Transformer  & 1.05 bpb & 21 & 838 \\ 
    && Adaptive-Span & 1.04 bpb & 20 & 483 \\ 
    && \textsc{Expire-Span} & {\bf  1.03} bpb & \bf  15 & \bf 408 \\ 
    \midrule 
    \multirow{3}{*}{Char-level PG-19} && Compressive Transformer  & 1.07 bpc & 17 & 753 \\ 
    && Adaptive-Span & 1.07 bpc & \bf 13 & 427 \\ 
    && \textsc{Expire-Span} &  1.07 bpc & 15 & \bf 388 \\ 
    \midrule 
    \multirow{3}{*}{Object Collision} && Compressive Transformer  & 63.8\% Error & \bf 12 & 327 \\ 
    && Adaptive-Span & 59.8\% Error & 17 & 365 \\ 
    && \textsc{Expire-Span} & {\bf  52.2\%} Error & \bf  12 & \bf  130 \\ 
    \bottomrule
    \end{tabular}
    \caption{
    \textbf{Efficiency of \textsc{Expire-Span}}. We report peak GPU memory usage and per-batch training time, fixing the batch size. 
    }
    \label{tab:efficiency}
    \vspace{-2mm}
\end{table*}

We illustrate the task in Figure~\ref{fig:collision} and results in Table~\ref{tab:colliding}. The task requires many frames, so long context is very beneficial --- as the \textsc{Expire-Span} maximal span increases, performance steadily rises. Our largest span, 64k, matches the size of the largest attention limit reported to date~\citep{kitaev2019reformer} and has the strongest performance. This model is trained with the random drop regularization method described in Section 4.2. Compared to Compressive Transformer and Adaptive-Span baselines, our \textsc{Expire-Span} model has the strongest performance. 

Comparing efficiency, \textsc{Expire-Span} trains almost 3x faster than both baselines (see Table~\ref{tab:efficiency}) while having much stronger performance. Further, expiration is critical to this performance --- a Adaptive-Span model with $L=32\text{k}$ runs out of memory in the same setting where we trained our \textsc{Expire-Span} model with $L=64\text{k}$. Through expiration, our model can keep the GPU memory usage reasonable and train with the longer spans necessary for strong performance.

\begin{figure*}[t]
\begin{minipage}{0.63\textwidth}
    \centering 
    \includegraphics[width=\linewidth]{figs/humpty_dumpty.png}
    \caption{\textbf{Expiration in \textsc{Expire-Span} on Enwik8}. In \textbf{(a)}, the model strongly memorizes two areas,  ``Egypt'' and ``Alexander''. In \textbf{(b)}, if we replace ``Egypt'' with ``somewhere'', then it's forgotten fast. In \textbf{(c)}, we insert ``Humpty Dumpty'' and the model retains these rare words in memory.}
    \label{fig:vis_enwiki8}
\end{minipage}
\hfill
\begin{minipage}{0.33\textwidth}
\setlength{\tabcolsep}{3.2pt}
\includegraphics[width=0.99\linewidth]{figs/guinea-short-fontfix.pdf}
    \caption{\textbf{Accuracy Needs Memory.} As the maximum span is artificially decreased at inference time from 16k to only 1k, the prediction is less accurate.}
    \label{fig:enwik8_loss}
\end{minipage}
\end{figure*}



\section{Analysis and Discussion} 

\textsc{Expire-Span} creates the phenomena of \textit{selective forgetting}: it allows memories to be permanently deleted if the model learns they are not useful for the final task. In this section, we analyze the information retained and expired by \textsc{Expire-Span} models to better understand how models use the ability to forget. Additional analyses are in the appendix.

\paragraph{Retaining Salient Information}

We analyze what is retained by an \textsc{Expire-Span} model on Enwik8 to understand how models utilize the ability to forget. In Figure~\ref{fig:vis_enwiki8} (a), we show that the model retains information about named entities such as \textit{Egypt} and \textit{Alexander the Great} by giving them longer spans (darker color). Next, we analyze how expire-spans changes when we artificially edit the past text. In Figure~\ref{fig:vis_enwiki8} (b), we replace the entity \textit{Egypt} with the generic text \textit{somewhere}, and this generic word is quickly expired. In Figure~\ref{fig:vis_enwiki8} (c), we edit \textit{Egypt} to \textit{Humpty Dumpty}, which is a very rare entity, and the model retains it in memory without expiring.
In addition to entities, \textsc{Expire-Span} memorizes spaces, newlines, and section titles, all of which retain information about words, sentences, or sections. The model's expiration choices vary by layer, indicating that \textsc{Expire-Span} models use the memory at each layer to remember different information. 


\paragraph{Importance of Long Term Memory}
Next, we analyze which predictions benefit the most from memory capacity. We take an \textsc{Expire-Span} model trained on Enwik8 and decrease the maximum span size to 1024 at inference time, even though the model was trained with a maximum span of 16k. We then compare which predictions decreased in accuracy. In Figure~\ref{fig:enwik8_loss}, we see that  models have a much higher loss when predicting the named entity \textit{Guinea coast} compared to having the full 16k maximal span. \textit{Guinea coast} was mentioned 3584 tokens earlier, which indicates that long attention is often necessary to predict words mentioned in far away context. In general, we found that rare tokens and structural information about documents, such as section headings or document titles, required longer attention span to accurately predict.

\paragraph{Efficiency Advantages of Expire-Span} 

Finally, we end with a brief discussion about why \textsc{Expire-Span} is more efficient compared to existing architectures that focus on long context. First, Transformer-XL cannot adapt to the data at all, so it becomes slow and inefficient quite quickly as the span size increases. Adaptive-Span can adapt to the data and adjust its memory, but this memory size is fixed after training and does not have the dynamic adjustment of Expire-Span (where memory depends on local context even at inference time). Finally, the Compressive Transformer compresses past memories, but it compresses always at a fixed rate. The compression rate is an adjustable parameter, but aggressive compression potentially hurts performance. In contrast, \textsc{Expire-Span} can expire irrelevant content, which both improves performance by focusing on salient information, and reduces the load on GPU memory and allows for faster processing per batch. 

