\documentclass[10pt,onecolumn,letterpaper]{article}

\usepackage
[
        a4paper,left=1cm,
        right=2cm,
        top=3cm,
        bottom=4cm,
]
{geometry}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage[table,xcdraw,dvipsnames]{xcolor}
\usepackage{multirow}

\usepackage{enumitem}

\usepackage{subfig}
\setlist{nosep}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}




\begin{abstract}
Anomaly detection is crucial in industrial manufacturing to identify product defects, including incorrect parts, misaligned components, or damages in advance. Machine learning methods are widely studied to classify anomalies based on previously collected data, but this is challenging due to the rarity and unknown types of defects. To overcome such difficulty, recent approaches propose to utilize the common visual representation from natural image datasets. Several variants try to mitigate the discrepancy between the natural image and the industrial image by distilling knowledge or attaching additional modules after the feature extraction. 
However, existing approaches suffer from the lack of variety in the dataset and requires the input augmentation.  We propose ReConPatch, which can construct discriminative features for anomaly detection via training a simple modulation of pre-trained model using contrastive representation learning. ReConPatch achieves robust anomaly detection performance with its strengths: (1) training only a linear modulation of the patch feature and (2) not requiring fiddly input augmentation. We demonstrate our method outperforms existing methods on the challenging and widely used MVTec AD dataset. We also provide the ablation study of each components in our method.
\end{abstract}

\section{Introduction}
Anomaly detection in industrial manufacturing is important in order to identify the defects in products and maintain product quality. Types of anomalies can include incorrect parts, misaligned components or damages in the product. With increasing demands for automation, anomaly detection has been widely studied in machine learning fields, which aims to classify anomalies from the normal cases based on previously collected data. However, anomaly detection is regarded to be challenging, because defects are so rarely observed and unknown types of defects can occur. This situation, with the majority of normal cases and the scarcity of abnormal cases, leads to the improvements of one-class classification. 



The key concept of one-class classification for anomaly detection is to train a model to measure the distance between data and detect anomalies with a large distance from the nominal data.
Reconstruction-based approaches, such as using auto-encoding models \cite{davletshina2020unsupervised,nguyen2019anomaly,sakurada2014anomaly} or GANs
\cite{pidhorskyi2018generative,sabokrou2018adversarially}, have been proposed to detect anomalies by measuring the reconstruction errors.
As the variety of data is not rich enough to estimate a reliable nominal distribution from the scratch, recent works have shown that leveraging the common visual representation obtained from the natural image dataset \cite{deng2009imagenet} can achieve high anomaly detection performance \cite{bergman2020deep,cohen2020sub}. 
Even though pre-trained models can provide rich representations without adaptation, such representations are not distinguishable enough to identify subtle defects in industrial images. Distribution shift between the natural image and the industrial image also makes it difficult to extract anomaly-specific features. 
Training a model to learn the representation space that effectively discriminates borderline anomalies is essential for the performance of the anomaly detection model.

To alleviate the distributional shift between the pre-trained dataset and the industrial dataset, one can distill the features that are prominent for the anomaly detection by training a student model to reproduce the representation of the pre-trained model as a teacher supervision \cite{bergmann2020uninformed}.
Attaching normalizing flow \cite{dinhdensity} on top of the pre-trained model is another approach to make use of the pre-trained representation and estimate the distribution of normality \cite{rudolph2021same}. Existing methods still require the image augmentation, but achieving appropriate augmentation requires extensive handcrafts especially for the industrial images.

\begin{figure*}
\begin{center}
\includegraphics[width=0.8\linewidth]{figure1.png}
\end{center}
\caption{The overall structure of anomaly detection using ReConPath. ReConPatch is consists of two networks to train representations of the patch-level features which are composed of feature representation layer  and projection layer  respectively. Upper network  and  for calculating pairwise and contextual similarity between patch-level feature pairs, and the bottom network for representation learning of patch-level features is trained by relaxed contrastive loss .}
\label{fig:num1}
\end{figure*}
In this paper, we introduce unsupervised metric learning framework to enhance the arrangement among the features, \textit{ReConPatch}. Unfortunately, contrastive learning based training scheme has a weakness in modeling variations within nominal instances, which may increase the false positive rate. To this end, ReConPatch adapts the contextual similarity \cite{kim2022self} among features obtained from the teacher model as a pseudo label for the student model. Instead of training the entire network, ReConPatch demonstrates that the feature representation can be efficiently adapted by training only a simple linear transformation. 






\section{Related Work}
Unsupervised machine learning approaches in anomaly detection using neural networks have been widely analyzed.
Deep SVDD trains a neural network to map each data to the hyperspherical embedding and detect anomaly by measuring distance from the center of the hypersphere \cite{ruff2018deep}. 
Patch SVDD extends Deep SVDD to the patch feature to improve the localization and fine-grained examination \cite{yi2020patch}.
Reconstruction-based approach assume that normal data can be accurately reconstructed or generated by training a model with nominal dataset, while abnormal data cannot. From this assumption, the anomaly score is calculated by the error between the original input and the reconstructed one.
Auto-encoding models have been widely used for the reconstruction \cite{davletshina2020unsupervised,nguyen2019anomaly,sakurada2014anomaly}. With the improvements of GANs, several approaches also shown the effectiveness of GANs for the anomaly detection \cite{pidhorskyi2018generative,sabokrou2018adversarially}. When training the model from the scratch, the variety and abundancy should be guaranteed, which is mostly not available for the anomaly detection.

To alleviate the shortage of data, there have been attempts to utilize the common visual representation trained from the rich natural image dataset \cite{deng2009imagenet}.
Prior work measures the distance between the representations of input data and its nearest neighbors to detect the anomaly\cite{bergman2020deep} and compare hierarchical sub-image features to localize where the anomaly is \cite{cohen2020sub}.
SPADE introduces a memory bank \cite{cohen2020sub}.

DifferNet \cite{rudolph2021same} has shown that a normalizing flow \cite{dinhdensity} is helpful to train a bijective mapping between the pre-trained feature distribution and the well-defined density of the nominal data, which can be used to identify the anomalies.
CFLOW-AD \cite{gudovskiy2022cflow} introduces the conditional normalizing flow using the positional encoding to improve. As the normalizing flow trains to map features to the nominal distribution, it is vulnerable to the outliers in the training dataset.

PatchCore proposes a locally aware patch feature and efficient greedy subsampling method to define the coreset \cite{roth2022towards}.
CFA trains a patch descriptor that maps features on the hypersphere which is centered at the nearest neighbor in the memory bank \cite{lee2022cfa}.
PaDim estimates a Gaussian distribution of patch features at each spatial location to detect and localize OOD as anomalies \cite{defard2021padim}.
PNI trains a neural network to predict the feature distribution of each spatial location and its neighborhoods \cite{bae2022image}.



\section{Method}
ReConPatch focuses on learning a representation space that maps the patch features with similar nominal characteristics to be grouped closely in unsupervised learning manner. 
Despite previous work \cite{roth2022towards} has shown the effectiveness of selecting representative nominal patch features using a pre-trained model, it still presents a biased representation to the natural image data, which has a gap with the target data.
The main concept of our approach is to train the target-oriented features that spreads out the distributions of patch features according to the variation of normal samples and gathers the similar features.


\subsection{Overall structure}
As shown in Fig. \ref{fig:num1}, our framework consists of the training phase and the inference phase.
In the training phase, we first collect the feature map  for each input  in the training data using the pre-trained CNN model 
\cite{gudovskiy2022cflow,cohen2020sub,defard2021padim,roth2022towards,lee2022cfa,bae2022image}.
Then we generate patch-level features,  by aggregating feature vectors of neighborhood within a specific patch size  in the same manner as PatchCore \cite{roth2022towards}.

ReConPatch utilizes two networks to train representations of the patch-level features,
One is the network for representation learning of patch-level features is trained by relaxed contrastive loss  in Eq. \ref{eq:L_RC}.
The representation network is made up of the feature representation layer  and the projection layer  respectively. 
When computing , soft labels for every pair of features should be given.
The other network is used for calculating pairwise and contextual similarity between patch-level feature pairs.
In addition, the similarity calculation network is gradually updated by an exponential moving average (EMA) of the representation network. To distinguish two networks, the layers in the latter one is denoted as  and  respectively.


After training representation, the patch-level features extracted from the pre-trained CNN are transformed into the target-oriented features by the feature representation layer  \cite{chen2020simple}.
The representative features are selected by the coreset subsampling approach based on greedy approximation algorithm \cite{sinha2020gan} and stored in a memory bank. 
In the inference phase, the features of a test sample extracted in the same process as training and the anomaly score is calculated by comparing the features with the nominal representative in the memory bank.

\subsection{Patch-level Feature Representation Learning} \label{sec3.2}
The objective of ReConPatch is to learn target-oriented features from the patch-level features, thereby enabling more effective discrimination between normal and abnormal features.
To accomplish this goal, we apply a patch-level features representation learning approach that aggregates highly similar features while repelling those with low similarity.
As the lack of labeled data is a common challenge in the anomaly detection dataset, we employ soft labels to indicate the degree of proximity between patch-level features.
To address the issue, we measure the similarity between patch-level features using the pairwise similarity and the contextual similarity.

For two arbitrary patch-level features  and  obtained by , 
let the projected representation be  and .
Then the pairwise similarity between two features, , is given as
 
where  is the bandwidth of Gaussian kernel, which can be adjusted to tune the degree of smoothing in the similarity measure \cite{Kim2021embedding,kim2022self}.
\begin{figure}
\subfloat[]{{\includegraphics[width=0.5\columnwidth]{figure2a.png}}}
\subfloat[]{{\includegraphics[width=0.5\columnwidth]{figure2b.png}}}
\caption{Illustrative examples of similarity measures in the representation space. In both (a) and (b), the pairwise similarity  between  and  is identical.
In (a) case, the -nearest neighbors  and  do not enclose each other. Therefore,  has lower value. Thus  and  pair should be apart.
In contrast, as  and  enclose each other in (b) case,
 takes higher value, so that  and  pair should attract each other.}
\label{fig:num2}
\end{figure}
We note that Eq. \ref{eq:pairwise_similarity} measures the Gaussian kernel similarity between  and , which is widely used to measure the anomaly score.
However, the pairwise similarity is not sufficient to consider the relationship among the groups of features.
As depicted in Fig. \ref{fig:num2}, for example, both (a) and (b) cases have the same pairwise similarity. In case of (a),  and  belong to different group of features, so they should be separated. In contrast, in (b), they belong to the same group and should be gathered. 


Therefore, we simultaneously measure contextual similarity which takes the neighborhood of an embedding vector into account. -nearest neighborhood of the feature index  is given as a set of indices,

where  is -th nearest neighbor.  denotes the Euclidean distance between two embedding vectors . 
Two patch-level features can be regarded to be contextually similar if they share more nearest neighbors in common \cite{liao2022contextual}.
The contextual similarity  between two patch-level features  and  is defined as





Query expansion is a technique used in information retrieval and search engines to improve the relevance of search results by adding additional terms or concepts to a user's original query. The goal of query expansion is to retrieve more relevant documents that may have been missed by the original query due to variations in terminology or language use.

In addition, it uses the idea of query expansion, which widely used to improve the information retrieval, by expanding the query to the neighbors of neighbors \cite{kim2022self,liao2022contextual}.
 is redefined by averaging the similarities over the set of -nearest reciprocal neighbors.


Because  is asymmetric, the contextual similarity is finally defined as the average of bi-directional similarity of a pair, which is given by

Then we define the final similarity between two patch-level features  and  as a linear combination of two similarities with amount ,


Patch-level features do not have explicit labels because an image is correlated between neighboring pixels. Moreover, the goal is to obtain target-oriented unique features rather than clearly distinguishing them. We thus adopt the relaxed contrastive loss \cite{Kim2021embedding} that takes into account inter-feature similarity as soft labels. Let  denote the relative distance between embedding vectors in a mini-batch. The relaxed contrastive loss is given by

where  is embedding vectors inferenced by ,  is the number of a mini-batch, and  is a margin for repelling.  in Eq.(7) determines the weights of the attracting and repelling loss term.


While the representation learning network  and  are trained with relaxed contrastive loss, the similarity calculation network  and  are slowly updated with an exponential moving average of the parameters of  and  respectively. Fast training of the similarity calculation network reduces the consistency of the relationships between patch-level features, leading to unstable training. Let  be the parameters of the similarity calculation network and  be the parameters of the representation learning network. Then  is updated by

where  is the hyper-parameter that adjusts the rate of momentum update.

\subsection{Anomaly detection with ReConPatch}
The anomaly score is calculated in the same manner as the PatchCore \cite{roth2022towards}. 
After training, the coreset is subsampled from the newly trained feature representation  in greedy manner \cite{sinha2020gan} and is stored in the memory bank .
Then the pixel-wise anomaly score is obtained by calculating the distance between the feature generated the feature representation layer  and its nearest coreset  within the memory bank.


where  is a nearest neighbor coreset and  is -nearest neighbor coresets of  in the memory bank.
In addition, the image-wise anomaly score is computed as the maximum score over the anomaly scores calculated for every patch feature in the image.

The accuracy of anomaly detection needs to be further improved by fusion of score-level from multiple models. Because each model has a different distribution of scores, score normalization is necessary to evenly fuse the score levels of each model. The anomaly score is normalized to the modified z-score \cite{agga2019zscore} defined as:

where  is the median value of the anomaly scores over the entire dataset for training and  is a constant that we set to 1.4826.


\section{Experiments}
\subsection{Experimental setup}
\textbf{Dataset}
In this study, we used the MVTec AD \cite{bergmann2019mvtec} dataset for our experiments. This dataset is widely used as an industrial anomaly detection performance benchmark. The dataset consists of 15 categories with 3,629 training images and 1,725 test images. The training dataset includes only normal images, while the test dataset includes both normal and anomalous images. Each category in the test dataset has a label for normal and abnormal, and an anomaly ground truth mask label for segmentation evaluation.
For the single model performance comparison, we performed the same pre-processing as in \cite{cohen2020sub, defard2021padim, lee2022cfa, roth2022towards}. Specifically, we resized each image to 256256 and then center cropped to 224224. For the ensemble model, we used the same pre-processing as in \cite{roth2022towards}, resized each image to 366366 and then center cropped to 320320. No data augmentation was applied to any category.

\textbf{Metrics}
To evaluate the performance of our model, we compared the anomaly detection and segmentation performance using the area under the receiver operation characteristic curve (AUROC) metric, following \cite{cohen2020sub, defard2021padim, lee2022cfa, roth2022towards}.
For the detection performance evaluation, we measure image-level AUROC by using the model output anomaly score and the normal/abnormal labels of the test dataset. For segmentation, we measures pixel-level AUROC using the anomaly scores obtained from the model output for all pixels and the anomaly ground truth mask labels.

\textbf{Implementation details.}
For the single model, we used ImageNet pre-trained WideResNet-50 \cite{zagoruyko2016wide} as a feature extractor. The  layer output size was set to 512, and the coreset subsampling percentage of 1\%. Our proposed ReConPatch was trained for 120 epochs for each category.
For the ensemble model, we used ImageNet pre-trained WideResNet-101 \cite{zagoruyko2016wide}, ResNext-101 \cite{xie2017aggregated}, and DenseNet-201 \cite{huang2017densely} as feature extractors to compare with the PatchCore \cite{roth2022towards}. The  layer output size was set to 384, and we applied a coreset subsampling percentage of 1\% to all models in the ensemble. We trained our proposed ReConPatch for 60 epochs for each category. We used hierarchy levels 2 and 3 for feature extraction in each model. The ReConPatch was trained using AdamP \cite{heo2020adamp} optimizer with a cosine annealing \cite{loshchilov2016sgdr} scheduler. The learning rate was set to 1e-5 for the single model and 1e-6 for the ensemble model, with a weight decay of 1e-2. For generating patch-level features, we applied a patch size of .


\begin{table}[]
\resizebox{\columnwidth}{!}{\begin{tabular}{l|ccc}
\hline
 \multicolumn{1}{c|}{Method}   & Ours-25\% & Ours-10\% & Ours-1\% \\
\hline
Detection    & 99.24           & 99.27                    & \textbf{99.49}          \\
Segmentation & 98.01           & \textbf{98.07}           & \textbf{98.07}          \\
\hline
\end{tabular}
}
\caption{Results of the ablation study on coreset subsampling percentage using our proposed ReConPatch model with WideResNet-50 backbone on the MVTec AD dataset.}
\label{table:1}
\end{table}

\begin{table}[]
\resizebox{\columnwidth}{!}{\begin{tabular}{l|ccccc}
\hline
\multicolumn{1}{c|}{Dimension} & 1024  & 512   & 256   & 128   & 64    \\
\hline
Detection                     & 99.49          & \textbf{99.56} & 99.53 & 99.52 & 99.14 \\
Segmentation                  & \textbf{98.07} & \textbf{98.07} & 98.03 & 97.94 & 97.68 \\
\hline
\end{tabular}
}
\caption{Ablation study results for  layer dimension on the MVTec AD dataset using our proposed ReConPatch model with a WideResNet-50 backbone.}
\label{table:2}
\end{table}

\begin{table}[]
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|c|c|}
\hline
\multicolumn{1}{|c|}{Metric}       & Detection              & Segmentation             \\
\hline
\multicolumn{3}{l}{WRN-50, , 512 dim, layer (2+3), Imagesize 224}    \\
\hline
AUROC                & 98.84                  & 97.82                    \\
\hline
\multicolumn{3}{l}{WRN-50, , 512 dim, layer (1+2+3), Imagesize 224}  \\
\hline
AUROC                & 98.7                   & 98.18                    \\
\hline
\multicolumn{3}{l}{WRN-101, , 512 dim, layer (1+2+3), Imagesize 480} \\
\hline
AUROC                & -                      & -                        \\
\hline
\end{tabular}
}
\caption{Ablation study results using a larger backbone model, more hierarchy levels, larger patch size, and larger image size on the MVTec AD dataset with our proposed ReConPatch model.}
\label{table:3}
\end{table}

\subsection{Ablation study} \label{ablation}
In this work, we aimed to investigate the optimal configuration of ReConPatch by performing an ablation study. Firstly, we aimed to find the optimal coreset subsampling percentage. To this end, we conducted a comparison of anomaly detection and segmentation AUROC metrics using three subsampling ratios: 25\%, 10\%, and 1\%, which are the same ratios used in PatchCore \cite{roth2022towards}. WideResNet-50 \cite{zagoruyko2016wide} backbone was used as a baseline for this experiment. The results are presented in Table \ref{table:1}, where we observe that the subsampling percentage of 1\% provides the best performance when the  layer is set to 1024 dimensions.
Additionally, we performed experiments with different dimensions of the  layer, 1024, 512, 256, 128, and 64, to determine the optimal dimension. The experiments were conducted with coreset subsampling at 1\%. The results are presented in Table \ref{table:2}, where we observe that the highest performance was achieved with a dimension of 512, and the performance decreased after the dimension of 512.

Table \ref{table:3} shows the results of an ablation study using a larger backbone model, more hierarchy levels, larger patch size, and larger image size on the MVTec AD \cite{bergmann2019mvtec} dataset with our proposed ReConPatch model. This experiment aimed to improve segmentation performance by utilizing more information about patch features. The results showed a decrease in detection performance, but an improvement in segmentation performance up to 98.18\% was achieved by increasing the patch size to 5 and using hierarchy levels 1, 2, and 3.


\begin{table*}[]
\centering
\begin{tabular}{l|c|ccccc|c}
\hline
Backbone                    & WRN-101  & \multicolumn{6}{c}{WRN-50}        \\
\hline
Image size                  & 480480         & 256256 & 224224  & 224224 & 224224  & 224224   & 224224 \\
\hline
 Class\textbackslash{}Method  & \begin{tabular}[c]{@{}c@{}}PNI \cite{bae2022image}\\ (w/ refine)\end{tabular} & CFLOW-AD \cite{gudovskiy2022cflow} & SPADE \cite{cohen2020sub} & PaDiM \cite{defard2021padim} &  PatchCore \cite{roth2022towards}    & CFA \cite{lee2022cfa} & \textbf{Ours}    \\
\hline
Bottle       & {\color{gray}100}             & {\color{gray}100}   & - & -    & \textbf{100}     & \textbf{100}       & \textbf{100}      \\
Cable        & {\color{gray}99.78}           & {\color{gray}97.59} & - & -    & 99.5             & 99.8               & \textbf{99.83}    \\
Capsule      & {\color{gray}99.52}           & {\color{gray}97.68} & - & -    & 98.1             & 97.3               & \textbf{98.8}     \\
Carpet       & {\color{gray}100}             & {\color{gray}98.73} & - & -    & 98.7             & 97.3               & \textbf{99.6}    \\
Grid         & {\color{gray}98.58}           & {\color{gray}99.6}  & - & -    & 98.2             & 99.2               & \textbf{100}      \\
Hazelnut     & {\color{gray}100}             & {\color{gray}99.98} & - & -    & \textbf{100}     & \textbf{100}       & \textbf{100}      \\
Leather      & {\color{gray}100}             & {\color{gray}100}   & - & -    & \textbf{100}     & \textbf{100}       & \textbf{100}      \\
Metal nut    & {\color{gray}100}             & {\color{gray}99.26} & - & -    & \textbf{100}     & \textbf{100}       & \textbf{100}      \\
Pill         & {\color{gray}97.19}           & {\color{gray}96.82} & - & -    & 96.6             & \textbf{97.9}      & 97.49             \\
Screw        & {\color{gray}98.87}           & {\color{gray}91.89} & - & -    & 98.1             & 97.3               & \textbf{98.52}    \\
Tile         & {\color{gray}99.96}           & {\color{gray}99.88} & - & -    & 98.7             & 99.4               & \textbf{99.78}    \\
Toothbrush   & {\color{gray}99.72}           & {\color{gray}99.65} & - & -    & \textbf{100}     & \textbf{100}       & \textbf{100}      \\
Transistor   & {\color{gray}100}             & {\color{gray}95.21} & - & -    & \textbf{100}     & \textbf{100}       & \textbf{100}      \\
Wood         & {\color{gray}99.39}           & {\color{gray}99.12} & - & -    & 99.2             & \textbf{99.7}      & 99.65             \\
Zipper       & {\color{gray}99.82}           & {\color{gray}98.48} & - & -    & 99.4             & 99.6               & \textbf{99.76}    \\
\hline
\multicolumn{1}{c|}{Average} & {\color{gray}99.52}        & {\color{gray}98.26}  & 85.5  & 95.3   & 99.1    & 99.3      & \textbf{99.56}  \\
\hline
\end{tabular}
\caption{Anomaly detection performance (Image-level AUROC) of the MVTec AD \cite{bergmann2019mvtec} dataset. The {\color{gray}gray} color indicates the use of a different backbone or image size compared to previous studies.}
\label{table:4}
\end{table*}

\begin{table*}[]
\centering
\begin{tabular}{l|c|ccccc|c}
\hline
Backbone                    & WRN-101  & \multicolumn{6}{c}{WRN-50}        \\
\hline
Image size                  & 480480         & 256256 &  224224  & 224224 & 224224 & 224224   & 224224 \\
\hline
 Class\textbackslash{}Method  & \begin{tabular}[c]{@{}c@{}}PNI \cite{bae2022image}\\ (w/ refine)\end{tabular} & CFLOW-AD \cite{gudovskiy2022cflow} & SPADE \cite{cohen2020sub} & PaDiM \cite{defard2021padim} &  PatchCore \cite{roth2022towards}  & CFA \cite{lee2022cfa} & \textbf{Ours}    \\
\hline
Bottle        & {\color{gray}98.98}          & {\color{gray}98.76}   & 98.4          & 98.3          & \textbf{98.6}   & -      & 98.29            \\
Cable         & {\color{gray}99.14}          & {\color{gray}97.64}   & 97.2          & 96.7          & \textbf{98.4}   & -      & 98.06            \\
Capsule       & {\color{gray}99.3}           & {\color{gray}98.98}   & \textbf{99}   & 98.5          & 98.8            & -      & 98.94            \\
Carpet        & {\color{gray}99.39}          & {\color{gray}99.23}   & 97.5          & \textbf{99.1} & 99              & -      & 99.04            \\
Grid          & {\color{gray}98.92}          & {\color{gray}96.89}   & 93.7          & 97.3          & \textbf{98.7}   & -      & 98.6             \\
Hazelnut      & {\color{gray}99.17}          & {\color{gray}98.82}   & \textbf{99.1} & 98.2          & 98.7            & -      & 98.59            \\
Leather       & {\color{gray}99.53}          & {\color{gray}99.61}   & 97.6          & 99.2          & \textbf{99.3}   & -      & 99.27            \\
Metal nut     & {\color{gray}99.26}          & {\color{gray}98.56}   & 98.1          & 97.2          & \textbf{98.4}   & -      & 98.13            \\
Pill          & {\color{gray}98.96}          & {\color{gray}98.95}   & 96.5          & 95.7          & 97.4            & -      & \textbf{98.04}   \\
Screw         & {\color{gray}99.41}          & {\color{gray}98.1}    & 98.9          & 98.5          & 99.4            & -      & \textbf{99.51}   \\
Tile          & {\color{gray}98.34}          & {\color{gray}97.71}   & 87.4          & 94.1          & \textbf{95.6}   & -      & 95.41            \\
Toothbrush    & {\color{gray}99.08}          & {\color{gray}98.56}   & 97.9          & \textbf{98.8} & 98.7            & -      & 98.69            \\
Transistor    & {\color{gray}98.42}          & {\color{gray}93.28}   & 94.1          & \textbf{97.5} & 96.3            & -      & 96.64            \\
Wood          & {\color{gray}96.46}          & {\color{gray}94.49}   & 88.5          & 94.9          & 95              & -      & \textbf{95.07}   \\
Zipper        & {\color{gray}99.3}           & {\color{gray}98.41}   & 96.5          & 98.5          & \textbf{98.8}   & -      & 98.74            \\
\hline
\multicolumn{1}{c|}{Average} & {\color{gray}98.91}           & {\color{gray}97.87}  & 96 & 97.5 &  98.1   & \textbf{98.2}       & 98.07            \\
\hline
\end{tabular}
\caption{Anomaly segmentation performance (Pixel-level AUROC) of the MVTec AD \cite{bergmann2019mvtec} dataset. The {\color{gray}gray} color indicates the use of a different backbone or image size compared to previous studies.}
\label{table:5}
\end{table*}


\subsection{Quantitative results}
We verify the performance of our method by comparing with the previous works \cite{cohen2020sub, defard2021padim, lee2022cfa, roth2022towards} that uses the same pre-trained model and image size. The performance of PNI \cite{bae2022image} and CFLOW-AD \cite{gudovskiy2022cflow}, which were displayed in gray color in tables \ref{table:4} and \ref{table:5}, were not compared to our results. In PNI \cite{bae2022image}, a WideResNet-101 model with 480480 image size was used. To improve its performance, a refinement network was included, which was trained in a supervised manner using artificially created defect dataset. CFLOW-AD \cite{gudovskiy2022cflow} used the WideResNet-50 model with a 256256 image size. The evaluation results used were the best performance obtained for each category when using the 256256 image size. 

The performance of the ReConPatch used in Tables \ref{table:4} and \ref{table:5} was obtained using 1\% coreset subsampling and a 512  layer dimension, as determined by Table \ref{table:2}.
Table \ref{table:4} compares the anomaly detection performance for each category of the MVTec AD \cite{bergmann2019mvtec} dataset using the single model and evaluating with image-level AUROC. Our proposed ReConPatch achieved an image-level AUROC of 99.56\%, outperforming CFA \cite{lee2022cfa} with 99.3\%. Furthermore, ReConPatch achieved a higher performance compared to the state-of-the-art PNI \cite{bae2022image} with WideResNet-101 \cite{zagoruyko2016wide}, 480480 image size, and a refine network, which achieved 99.52\%.

Table \ref{table:5} presents the anomaly segmentation performance evaluated with pixel-level AUROC. The performance of ReConPatch with a patch size of 5 and hierarchy levels 1, 2, and 3, which was obtained from Table \ref{table:3} with a segmentation performance of 98.18\%, was excluded from Table \ref{table:5} for fair evaluation. Our proposed method focuses on improving the anomaly detection performance, and as a result, the segmentation performance may not be as high as detection. However, as shown in Table \ref{table:3}, we can improve the segmentation performance by adding information about patch features.

Table \ref{table:6} presents the performance of our ensemble model, which evaluates using modified z-score Eq. \ref{zscore} for each output from WideResNet-101 \cite{zagoruyko2016wide}, ResNext-101 \cite{xie2017aggregated}, and DenseNet-201 \cite{huang2017densely} models. We achieved state-of-the-art results on the anomaly detection task of the MVTec AD dataset with 99.67\% AUROC. Furthermore, we outperformed PatchCore \cite{roth2022towards} performance of 98.2\% AUROC for anomaly segmentation, achieving an improved performance of 98.36\% AUROC.


\begin{table}[]
\resizebox{\columnwidth}{!}{\begin{tabular}{l|c|c|c}
\hline
\begin{tabular}[c]{@{}l@{}}Ensemble\\ Backbone\end{tabular} & \multicolumn{3}{c}{WRN-101 \& RNext-101 \& DenseN-201}                          \\
\hline
Image size                                   & 480480                & 320320             & 320320 \\
\hline
Method                                                      & \begin{tabular}[c]{@{}c@{}}PNI \cite{bae2022image} \\ (w/ refine)\end{tabular} & PatchCore \cite{roth2022towards} & Ours    \\
\hline
Detection                     & {\color{gray}99.55}                      & 99.6      & \textbf{99.67}   \\
Segmentation                  & {\color{gray}99.05}                      & 98.2      & \textbf{98.36}   \\
\hline
\end{tabular}}
\caption{Comparison of ensemble model anomaly detection (Image-level AUROC) and segmentation (Pixel-level AUROC) performance. The {\color{gray}gray} color indicates the use of a different image size compared to previous studies.}
\label{table:6}
\end{table}


\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{figure3.png}
\end{center}
\caption{The histogram displays the normal and abnormal distributions of the anomaly score for the bottle class in the MVTec AD \cite{bergmann2019mvtec} dataset. The left histogram represents the patch feature results, and the right histogram represents the  layer output, which is the ReConPatch feature.}
\label{fig:3}
\end{figure}


\subsection{Qualitative results}
In Figure \ref{fig:3}, we compared the distribution of anomaly scores obtained using patch features and the proposed ReConPatch features for anomaly detection, shown as the histogram of image-level anomaly scores. We examined the density and count distributions of the scores for the bottle class in the MVTec AD \cite{bergmann2019mvtec} dataset.
As a result, we observed that the score distribution of the ReConPatch feature was compressed for normal data and moved further away from the normal distribution for abnormal data, compared to the score distribution of the patch feature.
In this experiment, the patch features are the same as the locally aware patch features proposed in the PatchCore \cite{roth2022towards}. We were able to obtain target-oriented features by training patch-level feature representations using the method proposed in section \ref{sec3.2}, which allows for better discrimination between normal and abnormal features. As a result, we achieved an image-level AUROC of 99.56\%, which is higher than the anomaly detection performance of PatchCore, which is 99.1\%



\section{Conclusion}
In this paper, we introduce the ReConPatch to learn a target-oriented representation space, which can effectively distinguish the anomalies from the normal dataset. ReConPatch effectively trains the representation by applying the metric learning with softly guided by the similarity over the nominal features. When measuring the similarity, ReConPatch takes advantage of two different similarity measures, the pairwise and the contextual similarity.
We note that two similarity measure are well-aligned with the anomaly detection literature, in which the Gaussian kernel distance and the nearest neighbor are widely used.
ReConPatch shows the state-of-the-art performance on the MVTec anomaly detection dataset. We also verify that training the representation space with the smaller dimension do not diminish the performance a lot, which helps to improve the computational complexity for the industrial anomaly detection. Compared to the previous work about adapting the feature representation space, ReConPatch achieves the improvement using a linear modulation and without delicate data augmentation.

\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}