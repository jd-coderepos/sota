\pdfoutput=1
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{xcolor}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\pc}[1]{ \textcolor{red}{peter: #1} }




\usepackage[accepted]{icml2019}

\icmltitlerunning{Population Based Augmentation}

\begin{document}

\twocolumn[
\icmltitle{Population Based Augmentation: \\ Efficient Learning of Augmentation Policy Schedules}








\begin{icmlauthorlist}
\icmlauthor{Daniel Ho}{cal,googlex}
\icmlauthor{Eric Liang}{cal}
\icmlauthor{Ion Stoica}{cal}
\icmlauthor{Pieter Abbeel}{cal,cov}
\icmlauthor{Xi Chen}{cal,cov}
\end{icmlauthorlist}

\icmlaffiliation{cal}{EECS, UC Berkeley, Berkeley, California, USA}
\icmlaffiliation{cov}{covariant.ai, Berkeley, California, USA}
\icmlaffiliation{googlex}{Current affiliation: X, Mountain View, California, USA}

\icmlcorrespondingauthor{Daniel Ho}{daniel.ho@berkeley.edu}

\icmlkeywords{Machine Learning, ICML, PBA, Population Based Augmentation,  Population Based Training, augmentation, AutoAugment, neural network, CIFAR, SVHN, open source, ray, augmentation policy, augmentation schedule}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
 A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, \emph{Population Based Augmentation} (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy.
We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46\%, which is a slight improvement upon the current state-of-the-art.
The code for PBA is open source and is available at \url{https://github.com/arcelien/pba}.

\end{abstract}

\section{Introduction}
\label{intro}

Data augmentation techniques such as cropping, translation, and horizontal flipping are commonly used to train large neural networks \cite{nin}. Augmentation transforms images to increase the diversity of image data. While deep neural networks can be trained on enormous numbers of data examples to exhibit excellent performance on tasks such as image classification, they contain a likewise enormous number of parameters, which causes overfitting. Data augmentation acts as a regularizer to combat this. However, most approaches used in training state-of-the-art networks only use basic types of augmentation. While neural network architectures have been investigated in depth \cite{alexnet, resnet, googlenet, vgg, wrn, densenet, pyramidnet}, less focus has been put into discovering strong types of data augmentation and data augmentation policies that capture data invariances.

A key consideration when applying data augmentation is picking a good set of augmentation functions, since redundant or overly aggressive augmentation can slow down training and introduce biases into the dataset \cite{Graham14a}. Many recent methods learn augmentation policies to apply different functions to image data. Among these, AutoAugment \cite{autoaug} stands out with state-of-the-art results in CIFAR-10 \cite{cifar}, CIFAR-100 \cite{cifar}, and ImageNet \cite{imagenet}. Using a method inspired by Neural Architecture Search \cite{NAS1}, Cubuk et al. learn a distilled list of augmentation functions and associated probability-magnitude values, resulting in a distribution of possible augmentations which can be applied to each batch of data. However, the search technique used in the work is very computationally expensive, and code has not been released to reproduce it. In this work, we address these issues with a simple and efficient algorithm for augmentation policy learning.

\begin{figure}[t]
  \centering
  \includegraphics[width=8.2cm]{figures/chart.pdf}
     \vspace{-.3cm}
  \caption{PBA matches AutoAugment's classification accuracy across a range of different network models on the CIFAR-10 dataset, while requiring 1,000x less GPU hours to run. For the full set of results, refer to Table \ref{table-merged}. Assuming an hourly GPU cost of \7.5 for PBA vs \f(x, t)xtf^i(x)f \in F|F|^{|epochs|}(op, prob, mag)opprobmagxp(op, prob, mag)count = (op, prob, mag)pcount = 0< probcount = count - 1x = op(x, mag)x(10 \times 11)^{30} \approx 1.75 \times 10^{61}2.8 \times 10^{32}pparamp< 0.2paramamt = < 0.5param = param + amtparam = param - amtparam\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\sim15000 * 120 = 1.8m3200\sim\sim\simcountcount$ at 2 in line with AutoAugment's length 2 subpolicy, there may be room for performance improvement by carefully tuning the distribution.

We tried perturbation intervals of 2 and 4 once, but did not find this value to be sensitive. We also tried to run PBT for 100 epochs, but found this to slightly decrease performance when evaluated on models for 200 epochs. 

It may be interesting to consider training a larger child model (e.g, Shake-Shake) for 1,800 epochs to generate a schedule over the full training duration and eliminate the need to stretch the schedule. In a similar vein, an experiment to use PBT directly on the full CIFAR-10 dataset or Wide-ResNet-28-10 model may lead to better performance, and is computationally feasible with PBA.

\section{Conclusion}

This paper introduces PBA, a novel formulation of data augmentation search which quickly and efficiently learns state-of-the-art augmentation policy schedules. PBA is simple to implement within any PBT framework, and we release the code for PBA as open source.


































\section*{Acknowledgements}
We thank Richard Liaw, Dogus Cubuk, Quoc Le, and the ICML reviewers for helpful discussion.






\nocite{pbt}

\bibliography{ms}
\bibliographystyle{icml2019}

\clearpage

\appendix
\section{PBA Scalability with Compute}
\label{section-compute}

\begin{table*}[t]
\caption{Test error during PBT search and policy schedule evaluated afterwards, for varying population sizes and models. PBA Search with variation of model and compute, on Reduced CIFAR-10 dataset. ResNet-20 (Res) took approximately half the compute of WideResNet-40-2 (WRN). Number in title is the population size, and speedup is relative to AutoAugment. Note that models with larger population sizes, while scoring high during the search, don't actually perform better when re-evaluated.}
\label{table-cifar10-ablation-comp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcccccr}
\toprule
Model & 8-Res & 16-Res & 32-Res & 16-WRN & 32-WRN & 64-WRN  \\
\midrule
WRN-40-2 during search & - & - & - & 0.8484 & 0.8446 & 0.8523  \\
WRN-40-2 & - & - & - & 0.8452 & 0.8445 & 0.8446   \\
ResNet-20 during search & 0.7484 & 0.7657 & 0.7619 & - & - & -  \\
ResNet-20 & 0.7457 & 0.7545 & 0.7534 & - & - & -    \\
WRN-28-10 & 0.9711 & 0.9721 & 0.9740 & 0.9740 & 0.9736 & 0.9703 \\
\midrule
Relative Speedup & 2250x & 1125x & 562.5x & 562.5x & 281.25x & 140.625x \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*} 
\begin{figure}[t]
  \centering
  \includegraphics[width=8.2cm]{figures/comp-v3-cropped.png}
  \caption{Comparison of relative cost of search to test accuracy of augmentation policies evaluated on WideResNet-28-10. Child model WRN-40-2 was evaluated for population sizes from 2 to 64, and ResNet-20 was evaluated for sizes 2 to 32. All policies were trained on Reduced CIFAR-10.}
  \label{fig:compute}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=8.2cm]{figures/SVHN-vis-2.png}
  \caption{Augmentations applied to a SVHN ``4'' class image, at various points in our augmentation schedule learned on Reduced SVHN data. Each operation is formatted with name, probability, and magnitude value respectively.} \label{fig:svhn-viz}
\end{figure}

\begin{table*}[t]
\caption{Hyperparameters used for evaluation on CIFAR-10, CIFAR-100, and (R)educed-CIFAR-10. Besides Wide-ResNet-28-10 and Wide-ResNet-40-2 on Reduced SVHN, no hyperparameter tuning was done. Instead, all hyperparameters are the same as those used in AutoAugment.}
\label{table-hp-cifar10}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{ccccc}
\toprule
Dataset & Model & Learning Rate & Weight Decay & Batch Size \\
\midrule
CIFAR-10 & Wide-ResNet-40-2    &   0.1 & 0.0005 & 128 \\
CIFAR-10 & Wide-ResNet-28-10    &   0.1 & 0.0005 & 128 \\
CIFAR-10 & Shake-Shake (26 2x32d) & 0.01 & 0.001 & 128 \\
CIFAR-10 & Shake-Shake (26 2x96d) & 0.01 & 0.001 & 128 \\
CIFAR-10 & Shake-Shake (26 2x112d) & 0.01 & 0.001 & 128 \\
CIFAR-10 & PyramidNet+ShakeDrop   & 0.05 & 0.00005 & 64 \\
CIFAR-100 & Wide-ResNet-28-10    &   0.1 & 0.0005 & 128 \\
CIFAR-100 & Shake-Shake (26 2x96d) & 0.01 & 0.0025 & 128 \\
CIFAR-100 & PyramidNet+ShakeDrop   & 0.025 & 0.0005 & 64 \\
R-CIFAR-10 & Wide-ResNet-28-10    &   0.05 & 0.005 & 128 \\
R-CIFAR-10 & Shake-Shake (26 2x96d) & 0.025 & 0.0025 & 128 \\
SVHN & Wide-ResNet-40-2    &   0.05 & 0.005 & 128 \\
SVHN & Wide-ResNet-28-10 & 0.005 & 0.001 & 128 \\
SVHN & Shake-Shake (26 2x96d) & 0.01 & 0.00015 & 128 \\
R-SVHN & Wide-ResNet-28-10 & 0.05 & 0.01 & 128 \\
R-SVHN & Shake-Shake (26 2x96d) & 0.025 & 0.005 & 128 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*} 
\begin{figure}[t]
  \centering
  \begin{subfigure}[Operation magnitudes.]{
  \includegraphics[width=8.2cm]{figures/svhn-mag.png}
  }
  \end{subfigure}
  \begin{subfigure}[Normalized plot of operation probability parameters over time.]{
  \includegraphics[width=8.2cm]{figures/svhn-prob.png}
  }
  \end{subfigure}
  \caption{Plots showing the evolution of PBA operation parameters in a schedule learned on Reduced SVHN. Note that each operation actually appears in the parameter list twice; we take the mean parameter value for each operation in this visualization.}
  \label{fig:pba-svhn-schedule}
\end{figure}

In Figure \ref{fig:compute} we look at how large the model and PBT population size is necessary to learn an effective schedule. The population size determines how much of the search space is explored during training, and also the computational overhead of PBA. Our results indicate that a population size of 16 WRN-40-2 models performs the best. Having more than 16 trials seems not to help, and having less than 16 seems to lead to decreased performance. However, we found that results could fluctuate significantly between runs of PBT, most likely due to exploring a very limited search space with a noisy exploration strategy. 

Besides WRN-40-2, we also tried to use a ResNet-20 \cite{resnet-v2} model for PBT population, which required about half the compute. Empirical results (in  Table \ref{table-cifar10-ablation-comp} and Figure \ref{fig:compute}) suggest that the ResNet-20 population does not achieve as high of a test accuracy as with WRN-40-2, but results were relatively close. Because a ResNet-20 model has much less parameters, training accuracy plateaus faster than WRN-40-2, which may change the effects of augmentation.

\section{Model Hyperparameters}
The hyperparameters used to train WideResNet-40-2 to discover augmentation schedules, and also the ones used to train final models, are displayed in Table \ref{table-hp-cifar10}. For full details on the hyperparameters and implementation, see the open source code.

\section{SVHN discovered schedule}
See Figure \ref{fig:svhn-viz} for a visualization of the policy on an example image and Figure \ref{fig:pba-svhn-schedule} for a visualization of an example PBA policy on the SVHN dataset.

Examining the learned policy schedule, we observe that Cutout, Translate Y, Shear X, and Invert stand out as being present with high probability across all epochs. This fits with the findings of \cite{autoaug} indicating that Invert and geometric transformations are successful in SVHN because it is important to learn invariances to these augmentations. From another perspective, all of the augmentations appear with reasonable probability at some point in the schedule, which suggests that using a preliminary strategy like AutoAugment to filter out poor performing augmentations would be an interesting direction to explore.


\end{document}
