\vspace{-0.4cm}
\section{Experiments}
\vspace{-0.2cm}
\label{sec:exps}

In this section, we first detail our experimental protocol, then demonstrate the performance of \OURS on several zero-shot benchmarks, and finally propose a thorough analysis of several design choices.
\vspace{-0.2cm}

\begin{table}[t]
\caption{
      \textbf{Zero-shot transfer to image classification and retrieval.} 
      We report top-1 accuracy for classification and recall@1 for retrieval.
      We show the improvements obtained with \OURS on top of CLIP-B/32 and CLIP-L/14: 
      \emph{absolute} performance gains are between brackets.
      For reference, we also include the performance of K-Lite~\cite{shen2022k} (another retrieval-augmented method) and other standard image-text foundation models (Flava\cite{singh2022flava}, Align-base\cite{jia2021scaling}, LiT-L/16\cite{zhai2022lit} or PaLI-17B~\cite{pali2022}).
      We also report the total parameter count (``\# par.'') of the different models (in Million).
}
\centering
\small
  \setlength{\tabcolsep}{0.8pt}
     \begin{tabular}{l c llllll c ll c ll@{}}
    \toprule
     && \multicolumn{6}{c}{Image classification} && \multicolumn{2}{c}{T$\rightarrow$I} && \multicolumn{2}{c}{I$\rightarrow$T} \\
\cmidrule{3-8} \cmidrule{10-11} \cmidrule{13-14} 
Method & \# par.~~ & \footnotesize{Cars} & \footnotesize{CUB} & \footnotesize{Flowers} & \footnotesize{Im1k} & \footnotesize{Pl365} & \footnotesize{Dogs} && \footnotesize{COCO} & \footnotesize{Flickr} && \footnotesize{COCO} & \footnotesize{Flickr} \\
    \midrule
    CLIP-B/32 & 151 & 57.2 & 52.8 & 62.1 & 63.5 & 40.6 & 58.6 && 30.2 & 61.1 && 51.2 & 80.9 \\
   \rowcolor{aliceblue} ~~+ \OURS & 154 & 68.1\green{\tiny{(+10.9)}} & 63.0\green{\tiny{(+10.2)}} & 67.9\green{\tiny{(+5.8)}}& 64.6\green{\tiny{(+1.1)}} & 42.2\green{\tiny{(+1.6)}} & 59.7\green{\tiny{(+1.1)}} && 33.6\green{\tiny{(+3.4)}} & 65.7\green{\tiny{(+4.6)}} && 52.2\green{\tiny{(+1.1)}} & 81.8\green{\tiny{(+0.9)}} \\

     \midrule
    CLIP-L/14 & 428 & 75.6 & 61.7 & 75.6 & 75.5 & 42.0 & 72.7 && 35.2 & 68.6 && 57.2 & 87.5 \\
  \rowcolor{aliceblue} ~~+ \OURS & 435 & \textbf{82.8}\green{\tiny{(+7.2)}} & \textbf{73.4}\green{\tiny{(+11.7)}} & \textbf{79.5}\green{\tiny{(+3.9)}} & \textbf{76.1}\green{\tiny{(+0.6)}} & 43.6\green{\tiny{(+1.6)}} & \textbf{73.9}\green{\tiny{(+1.2)}} && 38.7\green{\tiny{(+3.5)}} & \textbf{72.6}\green{\tiny{(+4.0)}} && \textbf{58.0}\green{\tiny{(+0.8)}} & \textbf{88.5}\green{\tiny{(+1.0)}} \\
  \midrule
  \midrule
  \multicolumn{3}{l}{\textit{Other approaches}} \\
  K-Lite\cite{shen2022k} & 151 & 10.0 & ~~-- & 78.6 & 52.3 & ~~--  & ~~-- &&  ~~--  & ~~--  &&  ~~--  & ~~-- \\
  Flava\cite{singh2022flava} & 172 & ~~-- & ~~-- & ~~-- &~~--&~~--&~~--&& 38.4 & 65.2 && 42.7 & 67.7 \\
  Align\cite{jia2021scaling} & 247 & 78.7 & 38.2 & 64.9 & 67.6 & \textbf{44.0} & 56.3 && \textbf{40.2} & \textbf{72.6} && 55.1 & 86.7 \\
  LiT-L\cite{zhai2022lit}  & 638 & 24.8 & 61.7 & 74.1 & 75.8 & 42.6 & 70.8 && 31.1 & 57.6 && 48.5 & 77.7 \\
  PaLI\cite{pali2022} & 17,000 &  ~~-- &  ~~-- &  ~~-- & 72.1 & ~~-- & ~~-- &&~~--&~~--&&~~-- &~~-- \\
\bottomrule
 \end{tabular}
 
\vspace{-0.3cm}
    \label{tab:main_table}
\end{table}


\begin{table}[t]
 \caption{
      \textbf{Zero-shot performance on OVEN.}
We report top-1 accuracy on seen and unseen categories and their harmonic mean. We also indicate the total number of parameters of each model (``\# params'').
}
\centering
\small
\begin{tabular}{@{}l c lll@{}}
     \toprule
    Method & \# params (M) & Seen & Unseen & Harmonic mean \\
     \midrule
     \multicolumn{3}{l}{\textit{Zero-shot}} \\
     PaLI-17B~\cite{pali2022} & 17,000 & 4.4 & 1.2 & 1.9 \\
     CLIP-L/14~\cite{radford2021learning} & 428 & 5.6 & 4.9 & 5.3 \\
    \rowcolor{aliceblue} CLIP-L/14~\cite{radford2021learning} + \OURS (Ours) & 435 & \textbf{12.4 \green{\scriptsize{(+6.8)}}} & \textbf{12.7 \green{\scriptsize{(+7.8)}}} & \textbf{12.6 \green{\scriptsize{(+7.3)}}} \\
    \midrule
         \midrule
     \multicolumn{3}{l}{\textit{Fine-tuning on the OVEN Seen categories}} \\ 
     CLIP-L/14 Fusion~\cite{hu2023open} & 880 & 33.6 & 4.8 & 8.4 \\
PaLI-3B~\cite{pali2022} & 3,000 & 19.1 & 6.0 & 9.3 \\
CLIP-L/14 CLIP2CLIP~\cite{hu2023open} & 860 & 12.6 & 10.5 & 11.5 \\
     PaLI-17B~\cite{pali2022} & 17,000 & 28.3 & 11.2 & 16.1 \\
      \bottomrule
 \end{tabular}

    \label{tab:oven}
    \vspace{-0.7cm}
\end{table}


\subsection{Experimental setup}

\noindent\textbf{Training details.}
We train the fusion model on top of a frozen CLIP~\cite{radford2021learning}.
We use the CLIP-B/32 or the CLIP-L/14 versions.
We train on Conceptual Captions 12M (``$\text{CC}_{\text{12M}}$'')~\cite{changpinyo2021conceptual}, an image-text dataset containing about 10M pairs.
We use a batch size of 4096, learning rate of $1e^{-3}$ decayed with a cosine schedule and weight decay of $1e^{-5}$.
The temperature parameter is learned~\cite{radford2021learning}.
Training is done for 10 epochs, which lasts about 10 hours on a $4x4$ TPUv2 pod.
For the memory, we use the subset of WebLI~\cite{pali2022} containing 1B image-text pairs.
We remove the near-duplicates of the test images from the memory.
We have also explored using smaller but publicly available memory such as LAION-400M dataset~\cite{schuhmann2021laion} and show the results in Appendix.

\vspace{-0.1cm}
\noindent\textbf{Evaluation datasets.}
We consider the following six image classification datasets:
Stanford Cars (``Cars'')~\cite{krause20133d},
CUB-200-2011 (``CUB'')~\cite{wah2011caltech},
Oxford Flowers (``Flowers'')~\cite{nilsback2008automated},
ImageNet-1k (``Im1k'')~\cite{russakovsky2015imagenet},
Places365 (``Pl365'')~\cite{zhou2017places} and
Stanford Dogs (``Dogs'')~\cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011}.
We also report performance on text-to-image (``T$\rightarrow$I'') and image-to-text (``I$\rightarrow$T'') retrieval on Flickr30k (``Flickr'')~\cite{plummer2015flickr30k} and MS COCO (``COCO'')~\cite{lin2014microsoft}.
Finally, we consider the recent Open-domain visual entity recognition (OVEN) benchmark~\cite{hu2023open}, containing $729$K test images possibly belonging to $6$M entity candidates.
More details about these datasets can be found in Appendix or in their corresponding publication.

\vspace{-0.1cm}
\noindent\textbf{Evaluation protocol.}
We evaluate in the zero-shot setting for all the considered benchmarks, meaning that no adaptation is done to the downstream task.
As common in the literature~\cite{radford2021learning,jia2021scaling,singh2022flava,zhai2022lit}, we add prompts to the text of the downstream tasks, following~\cite{zhai2022lit}.
We report the top-1 accuracy for all the tasks.
We detail all evaluation protocols in Appendix.

\vspace{-0.2cm}
\subsection{Zero-shot transfer}
\vspace{-0.2cm}
\head{Image classification and image/text retrieval.}
In Tab.~\ref{tab:main_table}, we observe that \OURS allows to boost the zero-shot performance of CLIP on zero-shot image classification and retrieval, with large improvements especially on the fine-grained datasets.
For example, we improve the original CLIP-B/32 accuracy by $+10.9$ on Cars,  $+10.2$ on CUB and $+5.8$ on Flowers.
The performance is also improved on less fine-grained benchmarks such as ImageNet or Places, though by more moderate margins (i.e. respectively $+1.1$ and $+1.6$).
Secondly, we see in Tab.~\ref{tab:main_table} that the performance gains are consistent across the two considered vision-text backbones (CLIP-B/32 and CLIP-L/14).
For reference, we also report in Tab.~\ref{tab:main_table} the numbers from other popular vision-text approaches~\cite{singh2022flava,jia2021scaling,zhai2022lit,pali2022}.
Note that our work is orthogonal to the choice of vision-text model and we expect that combining it with other powerful backbones, \eg ALIGN~\cite{jia2021scaling}, OpenCLIP~\cite{cherti2022reproducible} or CoCa~\cite{yu2022coca}, has the potential to bring further performance gains, especially in the fine-grained settings where their performance might be sub-optimal.
Overall, the experiment in Tab.~\ref{tab:main_table} confirms our initial motivation that retrieval from an external memory improves zero-shot recognition tasks, especially in fine-grained settings.

\head{Open-domain visual entity recognition (OVEN).}
In Tab.~\ref{tab:oven}, we show the zero-shot performance of \OURS on the OVEN benchmark.
We see that our method improves greatly over CLIP-L/14 on this challenging task, with an impressive relative improvement of $+138\%$.
Note that we do not train or fine-tune our model on the OVEN training set. 
Remarkably, we observe in Tab.~\ref{tab:oven} that \OURS also significantly outperforms much bigger models which are directly \emph{fine-tuned for this task}, for example CLIP2CLIP~\cite{hu2023open} or PaLI-3B~\cite{pali2022} while using respectively 2 $\times$ and 7 $\times$ less parameters.
It even comes close to the performance of PaLI-17B while being 39 $\times$ smaller and not using any fine-tuning.



\subsection{Design choice analyses}
\vspace{-0.2cm}
In this section, we validate several components of our model, namely the uni-modal search and cross-modal fusion, training of the fusion module and the number of retrieved elements from the memory.
We also propose some qualitative examples to help understanding why \OURS improves over CLIP performance.
We use ViT-CLIP-B/32 throughout this section.

\begin{table}[t]
\caption{
      \textbf{Uni-modal search for cross-modal fusion.}
We report top-1 accuracy for zero-shot image classification.
We evaluate the impact of uni-modal versus cross-modal search and uni-modal versus cross-modal fusion.
These different mechanisms are conceptually illustrated in Fig.~\ref{fig:uni_cross}.
We report \emph{absolute} improvement between brackets and the average \emph{relative} improvement over not using retrieval (i.e. CLIP performance) in the last row (``Avg. rel. $\Delta$'').
}
\centering
\small
  \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{@{}p{.8em}@{}l l ccccc|c@{}}
      \toprule
	   & Search & Fusion & Cars & CUB & Flowers & Im1k & Pl365 & Avg. rel. $\Delta$ \\
     \midrule
    & -- & -- & 57.2 & 52.8 & 62.1 & 63.5 & 40.6 & -- \\
      \midrule
    &\multicolumn{3}{l}{$\phi$ = \textit{Transformer fusion}}\\
  \rowcolor{aliceblue} \rownumber{1} & Uni-modal & Cross-modal & ~~\textbf{68.1 \green{\scriptsize{(+10.9)}}} & \textbf{63.0 \green{\scriptsize{(+10.2)}}} & \textbf{67.9 \green{\scriptsize{(+5.8)}}} & \textbf{64.6 \green{\scriptsize{(+1.1)}}} & \textbf{42.5 \green{\scriptsize{(+1.9)}}} & \textbf{$+$ 9.0 \%} \\
   \rownumber{2} & Cross-modal & Cross-modal & 56.6 \red{\scriptsize{(-0.6)}} &53.8 \green{\scriptsize{(+1.0)}}  & 64.3 \green{\scriptsize{(+2.2)}} & 64.3 \green{\scriptsize{(+0.8)}} & 42.4 \green{\scriptsize{(+1.8)}} & $+$ 1.7 \% \\
  \rownumber{3}   & Uni-modal & Uni-modal & ~57.3 \green{\scriptsize{(+0.1)}} & 51.2 \red{\scriptsize{(-1.6)}} & 62.2 \green{\scriptsize{(+0.1)}} & 62.1 \red{\scriptsize{(-1.4)}} & 41.7 \green{\scriptsize{(+1.1)}} & $-$ 0.4 \% \\
   \rownumber{4}  &   Cross-modal & Uni-modal & 54.0 \red{\scriptsize{(-3.2)}} & 50.7 \red{\scriptsize{(-2.1)}} & 61.4 \red{\scriptsize{(-0.7)}} & 62.3 \red{\scriptsize{(-1.2)}} & 41.2 \green{\scriptsize{(+0.6)}} & $-$ 1.9 \% \\ 
     \midrule
  & \multicolumn{3}{l}{$\phi$ = \textit{Mean fusion}}\\
 \rownumber{5}  &  Uni-modal & Cross-modal & 46.9 \red{\scriptsize{(-10.3)}} & 44.9 \red{\scriptsize{(-7.9)}} & ~50.5 \red{\scriptsize{(-11.6)}} & ~~40.1 \scriptsize{\red{(-23.4)}} & ~~23.7 \red{\scriptsize{(-16.9)}} & $-$ 21.7 \% \\
 \rownumber{6}  &  Cross-modal & Cross-modal & 43.7 \red{\scriptsize{(-13.5)}} & 45.3 \red{\scriptsize{(-7.5)}} & 58.7 \red{\scriptsize{(-3.4)}} & 55.2 \scriptsize{\red{(-8.3)}} & 32.7 \red{\scriptsize{(-7.9)}} & $-$ 11.0 \% \\
  \rownumber{7}  &  Uni-modal & Uni-modal & 44.0 \red{\scriptsize{(-13.2)}} & 47.2 \red{\scriptsize{(-5.6)}} & 61.3 \red{\scriptsize{(-0.8)}} & 55.1 \scriptsize{\red{(-8.4)}} & 36.2 \red{\scriptsize{(-4.4)}} & ~~$-$ 9.8 \%  \\
  \rownumber{8}    &  Cross-modal & Uni-modal & 33.4 \red{\scriptsize{(-23.8)}} & ~30.2 \red{\scriptsize{(-22.6)}} & ~38.9 \red{\scriptsize{(-23.2)}} & ~40.0 \scriptsize{\red{(-23.5)}} & ~~24.7 \red{\scriptsize{(-15.9)}} & $-$ 33.0 \%  \\
      \bottomrule
 \end{tabular}
 
    \label{tab:fusion}
    \vspace{-0.4cm}
\end{table}

\head{Uni-modal search and cross-modal fusion.}
In Tab.~\ref{tab:fusion}, we evaluate different alternatives for our method, namely (i) performing cross-modal search in the memory instead of uni-modal search and (ii) fusing uni-modal items (i.e. combining text with text and image with image) instead of cross-modal fusion.
These different scenarios (uni- \textit{versus} cross- modal search and fusion) are detailed in Section~\ref{sec:retr} and conceptually illustrated in Fig.~\ref{fig:uni_cross}.
Firstly, we observe in Tab.~\ref{tab:fusion} that uni-modal search (row~\rownumber{1}) leads to a better performance compared to cross-modal search (row~\rownumber{2}), with $+9.0$ \textit{versus} $+1.7$ average relative improvement over CLIP.
We remark that the gap is especially important for fine-grained datasets such as Cars, CUB and Flowers.
This agrees with our hypothesis that cross-modal search suffers from the pre-trained CLIP model's lack of fine-grained alignment between different modalities.
By contrast, using the inherent strength of image and text representations within their respective modalities allows to retrieve relevant matches, as qualitatively observed in Fig.~\ref{fig:qualitative}.

Secondly, we observe in Tab.~\ref{tab:fusion} that uni-modal fusion (rows~\rownumber{3} and~\rownumber{4}) works substantially worse than cross-modal fusion (rows~\rownumber{1} and~\rownumber{2}).
Indeed, we see that augmenting text embeddings with other text embeddings and image embeddings with other image embeddings does not bring any significant improvement over the baseline, and even tends to hurt the performance.
Intuitively, a possible explanation is that cross-modal fusion allows us to inject complementary signal into the original embeddings~\cite{iscen2023improving}.
By contrast, uni-modal provides signal that is already similar to the input, hence not as much additional information.
Finally, we see in Tab.~\ref{tab:fusion} that all the variants (rows~\rownumber{5}, \rownumber{6}, \rownumber{7} and \rownumber{8}) fail when simply averaging retrieved and original embeddings instead of learning the fusion with a transformer.
This highlights the importance of \emph{learning} to incorporate the retrieved items to the original embeddings before deploying the model at inference.



\head{Image and text retrieval fusion modules.}
In Tab.~\ref{tab:cross_contrastive}, we compare models trained to fuse only text original embeddings (row~\rownumber{1}), only image original embeddings (row~\rownumber{2}) or both (row~\rownumber{3}).
We observe that while models trained to fuse only image or text perform reasonably well on some benchmarks, they typically lag behind on other benchmarks.
For example, the model trained for only image fusion (row~\rownumber{2}) is strong on zero-shot Dogs benchmark but behind on CUB and COCO.
Secondly, as shown in Tab.~\ref{tab:cross_contrastive}, unlike the vision-only or text-only variants, our model can be used in different modes at inference time in a flexible manner.
Indeed, because we have trained it to align the refined embeddings with the original ones (see the cross terms in the loss~\ref{eq:total}), we can choose to disable the retrieval for one of the branches at inference time, depending on the task.
Therefore, we compare in Tab.~\ref{tab:cross_contrastive} different options at inference time: using retrieval only for the image input, only for the text input or for both of them, denoted respectively by $\overline{\vv}$, $\overline{\vt}$ and $\overline{\vv}$\&$\overline{\vt}$.
We observe in Tab.~\ref{tab:cross_contrastive} that depending on the nature of the task, one of these options might be preferable over the others.
For example, for image classification we see that augmenting the image embeddings with retrieved text has more positive impact than augmenting the text embeddings, though the best of performance is obtained with both.
On the other hand, text and image retrievals seem to benefit more from augmenting the text rather than the image side.
This intuitively can be explained by the fact that text descriptions in retrieval benchmarks are typically highly specific compared to the class names in image classification and so augmenting with visual examples of what they refer to greatly helps the alignment.
We demonstrate qualitative examples of this hypothesis in Appendix.
Overall, at inference time, one can choose the best inference mode for a particular downstream task by validation on a held-out set.


\begin{table}[t]
 \caption{
      \textbf{Image and text retrieval fusion modules.}
We report zero-shot top-1 accuracy for image classification and recall@1 for image retrieval.
We compare models trained only for text fusion (row~\rownumber{1}), image fusion (row~\rownumber{2}) or both (row~\rownumber{3}).
Our model can be used in different modes at inference:
retrieval only for image ($\overline{\vv}$), retrieval only for text ($\overline{\vt}$) or retrieval for both image and text ($\overline{\vv}$\&$\overline{\vt}$).
}
\centering
\small
  \setlength{\tabcolsep}{0.8pt}
    \begin{tabular}{@{}p{.5em}@{} ccc c cccc c cccc c cccc@{}}
      \toprule
           & & &  &&  \multicolumn{4}{c}{CUB} && \multicolumn{4}{c}{Dogs} && \multicolumn{4}{c}{COCO T$\rightarrow$I} \\
            \cmidrule{1-4}  \cmidrule{6-9} \cmidrule{11-14} \cmidrule{16-19} 
& $\phi_{\text{image}}$ & $\phi_{\text{text}}$  & fusion training loss &&  $\overline{\vv}$ & $\overline{\vt}$ & $\overline{\vv}$\&$\overline{\vt}$ & \colorbox{Gray}{Best} && $\overline{\vv}$ & $\overline{\vt}$ & $\overline{\vv}$\&$\overline{\vt}$ & \colorbox{Gray}{Best} && $\overline{\vv}$ & $\overline{\vt}$ & $\overline{\vv}$\&$\overline{\vt}$ & \colorbox{Gray}{Best} \\
     \midrule
     
   \rownumber{1}& & \checkmark & \scriptsize{$\cL_{\text{NCE}}(\vV, \overline{\vT})$} && \ding{55} & 59.3 &\ding{55}& \colorbox{Gray}{59.3} && \ding{55} & 59.6 & \ding{55} & \colorbox{Gray}{59.6} && \ding{55} & 33.3 & \ding{55} & \colorbox{Gray}{33.3} \\
    
   \rownumber{2} & \checkmark & & \scriptsize{$\cL_{\text{NCE}}(\overline{\vV}, \vT)$} && 59.7 & \ding{55} & \ding{55} & \colorbox{Gray}{59.7} && 59.2 & \ding{55} & \ding{55} & \colorbox{Gray}{59.2} && 31.3 & \ding{55} & \ding{55} & \colorbox{Gray}{31.3} \\
       
 \rowcolor{aliceblue}\rownumber{3}& \checkmark & \checkmark & \tiny{$\cL_{\text{NCE}}(\overline{\vV}, \overline{\vT})$+$\cL_{\text{NCE}}(\vV, \overline{\vT})$+$\cL_{\text{NCE}}(\overline{\vV}, \vT)$} && 60.0 & 58.4 & 63.0 & \colorbox{Gray}{\textbf{63.0}} && 59.7 & 59.7 & 59.4 & \colorbox{Gray}{\textbf{59.7}} && 31.9 & 33.6 & 31.7 & \colorbox{Gray}{\textbf{33.6}} \\
     \bottomrule
 \end{tabular}

    \label{tab:cross_contrastive}
    \vspace{-0.2cm}
\end{table}


\begin{figure}[t]
\small
  \begin{minipage}{0.3\linewidth}
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{ @{} l l c@{}}
      \toprule
      Method & Train data & CUB \\
      \midrule
      CLIP-B/32 & -- & 52.8 \\
      \midrule
      ~~+ CLIP-style & $\text{CC}_{\text{12M}}$ & 44.8 \\
      ~~+ CLIP-style & $\text{CC}_{\text{12M}}$ + $\mathcal{R}_{\text{Webli}}$ & 46.8 \\
     \rowcolor{aliceblue} ~~+ \OURS & $\text{CC}_{\text{12M}}$ + $\mathcal{R}_{\text{Webli}}$ & \textbf{63.0} \\
      \bottomrule
    \end{tabular}
  \end{minipage}\qquad
  \begin{minipage}{0.32\linewidth}
    \centering
    \includegraphics[width=0.88\linewidth]{figures/mem.pdf}
  \end{minipage}
  \begin{minipage}{0.32\linewidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figures/knn.pdf}
  \end{minipage}
 \vspace{-0.1cm}
  \caption{
    \textbf{(left) Disantangling the effect of additional training and \OURS.
    (middle) Effect of updating the memory after training.
(right) Effect of the number $k$ of retrieved elements.
}
     We report zero-shot top-1 accuracy on CUB.
     The CLIP baseline is shown with symbol \tiny{\FiveStar }.
  }
  \label{fig:ablation}
  \vspace{-0.6cm}
\end{figure}


\head{Is the performance boost merely due to additional training?}
We validate the hypothesis that the performance gains are indeed due to our method, and not due to training an additional layer on top of CLIP.
We replace \OURS with an MLP layer of the same capacity initialized from scratch.
We train it in a CLIP-style manner on the subset of Webli that we use when training \OURS.
We denote this subset by $\mathcal{R}_{\text{Webli}}$: it contains the $k=10$ nearest-neighbors for each CC12M datapoint retrieved from the Webli dataset, and contains 61M unique examples.
Results are presented in Fig.~\ref{fig:ablation} (left).
We observe that training an extra layer on top of CLIP does not bring any performance gains and even deteriorates its performance.
This is somehow expected since CLIP was extensively trained on a large and rich dataset~\cite{radford2021learning} and additional training on a relatively small dataset deteriorates the general-purpose property of its representations.
Overall, this experiment validates our retrieval-augmented approach.

\head{Updating the memory after training.}
A clear advantage of the retrieval-based models is that the external memory can be updated with additional, and more contemporary information.
We evaluate the effectiveness of \OURS when using a larger memory that is not observed during the training.
We first create various random subsets of Webli by randomly removing a percentage of data.
Then, we train separate \OURS models with each Webli subset as its memory.
At inference, we evaluate each \OURS model either with the subset of memory that it was trained with, or the full Webli memory.
Results are shown in Fig.~\ref{fig:ablation} (center).
We observe that training and evaluating \OURS with only $1\%$ of Webli as the memory does not show improvements compared to the CLIP baseline.
However, we observe a significant improvement when evaluating the same model with full Webli memory at inference.
This confirms that \OURS is capable of utilizing an updated memory without re-training.


\head{Effect of the number of retrieved elements.}
In Fig.~\ref{fig:ablation} (right), we study the effect of the number of retrieved elements in the memory.
We evaluate different numbers of $k$-NN during the training and inference time, \ie we train our model with $k$ items from the memory but use $k'$ at inference.
We see in Fig.~\ref{fig:ablation}~(right) that \OURS generally obtains a higher performance when $k' > k$ at inference.
Interestingly, the performance saturates after $k=10$.
An explanation is that increasing the number of retrieved elements goes with a reduction of the relevancy of the retrieved items.


\head{Qualitative study.}
In Fig.~\ref{fig:qualitative}, we provide illustrative examples of why \OURS can be useful for fine-grained image classification on CUB or Cars datasets.
We compare our method with a variant using cross-modal search instead of uni-modal search to illustrate the importance of using the inherent strength of image-only and text-only representations.
We observe in Fig.~\ref{fig:qualitative} that uni-modal search allows to retrieve better matches for the query.
This is because image-to-image or text-to-text search retrieves more similar items to the query than crossing modalities.
As a result, retrieved items are more accurate, which leads to a higher accuracy for fine-grained tasks.


\begin{table}[t]
\centering
\small
  \setlength{\tabcolsep}{7pt}
\begin{tabular}{c|cc}
\toprule
Query &
\textbf{\underline{Uni}}-modal search (Ours) &
\textbf{\underline{Cross}}-modal search \\
\midrule
\includegraphics[width=0.094\linewidth]{figures/bird_query.png} &
\includegraphics[width=0.35\linewidth]{figures/bird1.png} &
\includegraphics[width=0.35\linewidth]{figures/bird2.png} \\
\includegraphics[width=0.094\linewidth]{figures/car_query.png} &
\includegraphics[width=0.35\linewidth]{figures/car1.png} &
\includegraphics[width=0.35\linewidth]{figures/car2.png} \\
\midrule
\includegraphics[width=0.094\linewidth]{figures/yellow_query.png} &
\includegraphics[width=0.35\linewidth]{figures/yellow1.png} &
\includegraphics[width=0.35\linewidth]{figures/yellow2.png} \\
\includegraphics[width=0.094\linewidth]{figures/nubira_query.png} &
\includegraphics[width=0.35\linewidth]{figures/nubira1.png} &
\includegraphics[width=0.35\linewidth]{figures/nubira2.png}\\
\bottomrule
\end{tabular}
\captionof{figure}{\textbf{Qualitative examples on CUB and Cars datasets.}
We compare uni- versus cross- modal search for two image queries (top) and two text queries (bottom).
Uni-modal search allows to find more suitable matches to the query, which improves the relevancy of the fused elements.
We frame in red (resp. green) the unrelevant (resp. relevant) retrieved items to be fused with the query.
\label{fig:qualitative}
\vspace{-0.8cm}
}
\end{table}
\vspace{-0.5cm}