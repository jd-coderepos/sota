\documentclass[journal]{vgtc}         









\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}

\usepackage{subfigure}


\usepackage[bookmarks,backref=true,linkcolor=black]{hyperref} \hypersetup{
  pdfauthor = {},
  pdftitle = {},
  pdfsubject = {},
  pdfkeywords = {},
  colorlinks=true,
  linkcolor= black,
  citecolor= black,
  pageanchor=true,
  urlcolor = black,
  plainpages = false,
  linktocpage
}

\onlineid{0}

\newcommand{\thename}{ActiveCanvas}
\newcommand{\remove}[1]{}

\vgtccategory{Research}

\vgtcinsertpkg





\title{Adding Semantic Information into Data Models by Learning Domain Expertise from User Interaction}



\author{Nathan O. Hodas, Alex Endert}
\authorfooter{
\item
Nathan Hodas is with Pacific Northwest National Laboratory. E-mail: nathan.hodas@pnnl.gov.
\item
Alex Endert is with Georgia Tech. E-mail: endert@gatech.edu.

}

\shortauthortitle{Hodas, Endert: Adding Semantic Information}


\abstract{
Interactive visual analytic systems enable users to discover insights from complex data. Users can express and test hypotheses via user interaction, leveraging their domain expertise and prior knowledge to guide and steer the analytic models in the system. \remove{Model steering is a more commonly used approach to integrating this human feedback into the system, creating joint cognitive systems.} For example, semantic interaction techniques enable systems to learn from the user's interactions and steer the underlying analytic models based on the user's analytical reasoning. However, an open challenge is how to not only steer models based on the dimensions or features of the data, but how to add dimensions or attributes to the data based on the domain expertise of the user. In this paper, we present a technique for inferring and appending dimensions onto the dataset based on the prior expertise of the user expressed via user interactions. Our technique enables users to directly manipulate a spatial organization of data, from which both the dimensions of the data are weighted, and also dimensions created to represent the prior knowledge the user brings to the system. We describe this technique and demonstrate its utility via a use case. 
} 

\keywords{Sense-making, Feature Reduction, Information Theory, Knowledge Generation}



\CCScatlist{ \CCScat{K.6.1}{Management of Computing and Information Systems}{Project and People Management}{Life Cycle};
 \CCScat{K.7.m}{The Computing Profession}{Miscellaneous}{Ethics}
}









\renewcommand{\topfraction}{0.01}
\renewcommand{\textfraction}{0.01}
\begin{document}



\firstsection{Introduction}

\maketitle

Visual analytic systems facilitate a cognitive process of discovery by combining data analytic and information visualization capabilities \remove{\cite{thomas_illuminating_2005}}. These technologies leverage computational models to generate visual representations of data. Through user interaction, analysts make use of analytical reasoning to pose questions, form hypotheses, and gain insights into the data. For exploratory data analysis, this process can be referred to as sensemaking \cite{pirolli_sensemaking_2005}.

The analytic models used in visual analytics typically approximate some characteristics, metrics, attributes, or other features about the data. Thus, user interactions are typically designed to augment the analytic or visual parameters. More recently, semantic interaction techniques enable analytic model steering via inferring data characteristics that are of interest from the user based on their exploratory user interactions \cite{endert_beyond_2013}. However, one limitation of such approaches is that they attempt to fit the inference of the user's feedback into the existing feature space in the data model (e.g., keywords for text corpora). \remove{Prior work has shown that sensemaking and analytical reasoning of text corpora often entails users not only reasoning about the existing keywords in a corpora, but incorporating additional features and domain expertise into the dataset as part of their reasoning process \cite{endert_semantics_2012}\cite{andrews_space_2011}\cite{klein_making_2006}.} Analysts create mental constructs and concepts about the data which do not necessarily reflect in the data features, but extend into the broader context and features existent in the domain of the data being explored. \remove{This is important, as it is because of this ability for humans to impart their expertise that visual analytics strives for user feedback and the "human-in-the-loop" approach to data science. }
Thus, \textit{how can visual analytic systems infer such additional, semantic dimensions about the data from the user?}

In this paper, we demonstrate a system for allowing a user to create additional semantic features and leverage these features for image classification and accelerating the sense-making process.  The system presents a canvas filled with randomly arranged images to the user, and the user positions images according to their internal mental model, leveraging their previous knowledge and task-specific motivations. We call this system ActiveCanvas, because it does not simply allow the user to shuffle images around; it actively participates in the sensemaking loop by using information theoretic tools to extract relevant features from the images and then repositioning the images in an attempt to  better reflect the user's mental model. The information theoretic tools implicitly capture the analytic reasoning of the user, and it allows the system to build knowledge over time by recording the position of users and saving them as new features to be utilized in the future.
 






\remove{The contributions of this work include:
\begin{itemize}
\item A visual analytic technique for inferring and creating additional dimensions for datasets pertaining to the domain expertise of the user.
\item A technique for leveraging  knowledge of prior users in the visual analytic process.
\end{itemize}
}


\section{Related Work}

Spatial workspaces are effective visual metaphors for supporting sensemaking and discovery tasks. For example, Andrews et al. showed how the spatial positioning, grouping, and re-grouping of documents was an effective method to help analysts conduct a sensemaking task \cite{andrews_space_2010}. Their work found that users were able to leverage the flexibility of a large, high-resolution to organize their information. Throughout the process, the analysts made use of spatial constructs including lists, piles, sequences, and categorizations of the information. Over time, these spatial constructs evolved as the analysts gained more insights about the data. This process of incrementally refining a spatial layout based on one's own increased understanding of the data is called \textit{incremental formalism} \cite{shipman_formality_1999}. 



User interaction is critical to the success of sensemaking and discovery tasks using visual analytics. Prior work has analyzed user interaction as a data source to understand more about the user, the domain, and the data being analyzed. For example, Dou et al. showed how the analysis of user interaction logs can uncover low-level strategies and tasks of analysts performing financial analysis tasks \cite{dou_recovering_2009}. Similarly, Brown et al. analyzed user interactions for a simple search task (Where's Waldo) to discover patterns of interactions that could accurately predict fast and slow task completion times \cite{brown_finding_2014}. 

\remove{
\subsection{Information Theoretic Model Reduction}

By considering user positioning of images as a noisy input signal, we may ask, "which features explain this noisy signal?" or "what refinement of this noisy signal would better explain the features?" Attempting to simultaneously optimize both of these quantities involves maximization of mutual information. Mutual information is defined as 
\begin{equation}\label{eq:mutualinformation}
MI(X;Y) = \sum_{y\in Y} \sum_{x\in X} p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right),
\end{equation}
where $p(x)$ is the probability of observing $x$ from random process $X$, and $p(x,y)$ is the joint probability of observing $x$ from $X$ together with $y$ from $Y$.

Although information theoretic measures admit the interpretation just provided, they have the drawback that there is usually not a unique arrangement that maximizes explanatory power.  For example, if $X$ has maximum mutual information with $Y$, it also has equal mutual information with $2Y$, $Y+1$, etc. In fact, mutual information is invariant under any invertible transformation (bijection)~\cite{pal2010estimation}.  If one is searching for a specific $Y$, this nonuniqueness often limits the utility of the resulting ``optimal solution," because optimization procedures may be stymied by the nonconvex nature of the problem. The interactive workspace presented here provides the opportunity to help guide the system away from local optima that may not be suitable for the user.  Regardless of the final arrangement, the aim of the ActiveCanvas is to provide the user with a tool to accelerate the sensemaking process and reduce the effort required to organize data.

Mutual information is a common technique for determining feature importance\cite{torkkola2003feature,faivishevsky2012dimensionality}. Multiple techniques have been used to find reduced vectors that maximize mutual information with larger feature sets, but these require a fundamental kernel function to extend to unobserved data~\cite{faivishevsky2012dimensionality}. However, Faivishevsky et al. demonstrated how reduced vectors that maximize mutual information with the original could be used to capture essential features of high-dimensional data.  In the present work, because of the two-dimensional nature of the screen, we intend to reduce high dimensional vectors to two dimensions in a way that maximizes mutual information between the two.

The fact that mutual information is invariant to nontrivial transformations allows it to be used to map hypothesized, abstract models to noisy systems without needing to know the mechanisms that connect the model to the system~\cite{kuo2008gene,dunleavy2015mutual}. Thus, one may capture hidden variables without needing to build explicit regression models.  In the present work, we use this fact to allow users to leverage the semantic features created by other users. Why not simply use correlation? Correlation, by construction, is only maximized when a linear relationship between two numbers exists. Any deviation from linearity reduces correlation, even if the relationship between the two variables is entirely deterministic. For this reason, mutual information may be used to align misregistered images, while traditional correlation performs poorly~\cite{kim1997mutual}. 
}


\remove{
\section{Method}\label{sec:method}

Here we lay out an outline of the ActiveCanvas event loop. We discuss the motivation and deeper reasoning behind each step in Sec.~\ref{sec:discussion}.

\remove{We created a web frontend using HTML5 and easlejs\footnote{http://www.createjs.com/EaselJS}. The frontend displayed 250 images as thumbnails, and the user could position each image within the workspace by clicking and dragging. When a user moves an image, that image is given a pink highlight to indicate to the user that the system will be using that image for training (see below). When the user is ready to fully engage with the active portion of ActiveCanvas, the $x,y$ arrangement of images on the screen is sent via websocket to an engine written in the Julia language\footnote{http://www.julialang.org} ~\cite{bezanson2014julia}. }

The engine then calculates the 50 most salient features as follows. The code calculates the mutual information between the positions of images moved by the user and the feature vector available for each image, using the NPEET Python library (called from Julia)\footnote{https://github.com/gregversteeg/NPEET}. In these case, from the images in the collection we extracted DenseSIFT features~\cite{bosch2007image} with a vocabulary size of 500. Thus, for each commit, the system appends 2 additional features to the originally 500-dimensional feature vector. We call these user provided features "semantic features," because they originate from a user's human-interpretable assessment of the content of the image, as opposed to abstract quantitative measures such as the DenseSIFT. Images untouched by the user were ignored for this calculation, to avoid adding unnecessary noise and accelerating the calculation. 

The feature vector was reduced to the top 50 features based on mutual information. Then, new positions for each touched image, denoted $x^\prime,y^\prime$, where determined by partially optimizing the mutual information between the reduced feature set and the new positions. Partial optimization in this case means taking 5 rounds of optimization of a simplex algorithm from the NLopt package in Julia. This particular number of steps was chosen to provide the user with incremental adjustments they may easily correct later.

Once the system determines the new positions of the images, it uses these new positions from the touched images as a training set for an SVM regression.  The SVM then predicts the new positions for all of the untouched images.  All of the newly predicted positions are then sent back to the web client, and the images are moved to their new positions. 

When a user has positioned all of the images to their satisfaction, then then press a button telling the engine to record all of the $x,y$ positions as new features. These features are appended to any features provided to the system. Other users (or the same user) may then use the new features in their own workflow, allowing ActiveCanvas to gain additional semantic information on the images over time.
}

\begin{figure}[h]
\centering
\begin{subfigure}[Initial Arrangement\label{fig:step1}]{\includegraphics[width=\columnwidth]{Figs/moved_images.pdf}}
\end{subfigure}
\begin{subfigure}[First Refinement\label{fig:step2}]{\includegraphics[width=\columnwidth]{Figs/round1.pdf}}
\end{subfigure}
\begin{subfigure}[Final Refinement, 20 images touched\label{fig:finalstep}]{\includegraphics[width=\columnwidth]{Figs/final_arrangment_20touched.pdf}}
\end{subfigure}
\caption{An example workflow.  (a) The user initially positions 8 images (pink arrows). (b) After the first refinement, ActiveCanvas creates initial clusters. The user positions additional images to provide additional feedback to the canvas. (c) After touching a total of 20 images, and 3  refinements, clear clusters emerge. Although not readily apparent, the three images outside of clusters do not fit in any category.}\label{fig:workflow}
\end{figure}

\section{ActiveCanvas}\label{sec:discussion}

The ActiveCanvas is designed to enable the user to leverage the hard work of other users to accelerate the work of a new user sorting images on a 2d virtual space.  Imagine that a user has a latent mental model of the data they are trying to organize, denoted by $X_L$. $X_L$ represents the implicit features a user internally leverages to understand the images in front of them. Initially, the system would present them with naive arrangements of images, each of which the user would arrange on the screen to create positions $Y$.  Thus, for this limited set, $Y$ explains $X_L$, to the best of the user's intentions. \remove{Information theoretic measures are promising objective functions to evaluate this explanatory power, although they are by no means the only tool at our disposal. Information theoretic methods are often invariant to invertible transformations, so the user doesn't need to generate a model for converting $X_L$ into $Y$ nor does the system need to know this mapping to optimize mutual information.} A key assumption is that the user places items close together on the screen ($Y$-space) because they are similar in the latent space $X_L$.  Thus, if we can find a way to slightly alter the arrangement of items on the screen to maximize mutual information with the feature vector provided by the system, these new positions will likely have high mutual information with the user's mental model.  \remove{That is, the new arrangement of items on the screen explains the user's mental model, and the user's mental model explains the arrangement on the screen.}


To this end, when a user has arranged some initial images to their satisfaction, the ActiveCanvas attempts to slightly alter the new positions of those images, $Y^\prime$ such that $MI(X:Y^\prime)$ increases, where $MI$ is the mutual information. We then predict the position of the remaining items using the new positions of the touched items as a training set. using RBF SVM.  If the user is not satisfied, the user continues to adjust images or move additional images to produce better estimates of mutual information and more training data for the SVM. After the user has become satisfied with the position of all items on the screen, they may then commit the positions as two new features, changing $X \rightarrow [X Y_{final}]$. This enables other users to leverage the hard work of the first uers.
\remove{
Both strategies rely on ActiveCanvas being provided feature-vectors for each item that can capture the user's mental model. If, as an extreme example, the feature-vectors are merely randomly generated numbers with no meaningful relation to the underlying content, no system will be able to aide a user, as it will have no information.  The user would be forced to interact with every item of interest.  More realistically, the user should assume that the underlying feature set is good, but not sufficient, for capturing the desired criteria.\footnote{For example,  text is frequently analyzed as a ``bag-of-words," disregarding the word order. This means the system can easily capture the difference between ``My dog ate the food" and ``My cat ate the food," but it can not distinguish between ``my dog ate my cat" and ``my cat ate my dog."} 
}

\remove{Usually, the ActiveCanvas will be able to move some some -- but not all -- of the untouched items to new locations on the screen consistent with the user's mental model, once the user requests this from the system. The remaining images will be mispositioned due to insufficiently discriminative feature vectors. This leaves the user with the task of moving the remaining items. Thus, the most accurate way to portray the utility of ActiveCanvas is as  a tool to accelerate the process of sense-making, signature discovery, and labeling. }

\remove{Although the ActiveCanvas is obviously not magical, it may sometimes appear to be because of a unique underlying feature.  Should the user decide that they have arranged the items on the screen to their satisfaction, they can turn those positions into new features for the data.  That is, if the system is originally provided with a set of features $X$, and the user is satisfied with the two-dimensional arrangement of the images on the workspace, $Y$, the system fuses these together to make $X^\prime = [X Y]$.  When the next user uses the system containing the enhanced feature set, they will benefit from the semantic information generated by the efforts of previous users. Thus, the system will appear to get smarter over time.}  

\remove{
Because of the information theoretic component of the ActiveCanvas, users do not need to know the arrangement strategies of other users to benefit from other users' hard work. Mutual information is invariant under invertible transformations, so by optimizing mutual information between the new user's positions and the features (positions) created by  old users, the system can infer how the user intends to arrange the items on the screen. Furthermore, user will not position all images with perfect precision with respect to their mental model. Instead, they will usually adopt a satisficing  strategy, hunting for arrangements that are ``good enough." This inevitably introduces some noise into the positioning scheme. Because mutual information is a measure of uncertainty reduction, it will be robust to imprecision.
}


This robustness to imprecision allows a lazy user to benefit from the diligent work of more dedicated users.  If two users have classified images according to two different strategies, the lazy user need not position their images with as much rigor as would otherwise be required to distinguish clear clusters.  Upon sending the system the lazily-positioned images, ActiveCanvas leverages the features (e.g., positions chosen and recorded by the other users) to infer the intended clusters of the lazy user.  \remove{Figure~\ref{fig:goodusers} shows an example, where User 1 positions images according to the labels \{deer,dice,elephant,frog\}. User 2 positions images according to the labels \{animal,not animal\}.} After being satisfied, each user commits their arrangement back to the system, creating a new features set comprised of the original feature vector plus the (x,y) positions from each user. 

\remove{Lazy user L1 (Fig.~\ref{fig:L1}) attempts to follow the same strategy as User 1, but does not adequately separate the clusters to create distinct dividing surfaces between clusters.  Similarly, lazy user L2 attempts to follow the same strategy as User 2, but instead of following the same distinct clustering as User 2, L2 chooses a different clustering topology and roughly places animals on the inside and inanimate objects in an outside ring  surrounding the animals (but, again without creating clear separation between the two). Note that L2 has chosen to (sloppily) arrange their canvas in a different topology; User 2 made two distinct clusters, but L2 chose to make a bullseye pattern.  The mutual information measure enables ActiveCanvas to identify the user's mental model, without new users understanding or known previous user's arrangements.

The system compares L1's positions to the combined features generated by User 1 and User 2.  Based on the innate image features and similarity to the features generated by User 1, the system correctly identified the desired mental model of Lazy user L1, shown in Fig.~\ref{fig:L1final}, as well as the mental model of Lazy user L2, shown in Fig.~\ref{fig:L2final}. Thus, the value of ActiveCanvas lies in its ability to capture a user's mental model from minimal interaction from the user.
}

To exemplify this, consider the following sample workflow, worked through in Fig.~\ref{fig:workflow}.  As above, the user approaches ActiveCanvas to make sense of images chosen because they contain one of \{frog,dice,deer,elephant,pram\}, although they may be interested in the images for any reason.  Previous users have utilized ActiveCanvas on the same dataset to make clusters based on their personal desired sorting, including \{human, not human\}, \{light background, dark background\}, \{outdoors, not outdoors\}. \{animal, not animal\}, as well as according to the base classes.  After sorting the images according to their preference, they each save the positions of the images as new features. The new user approaches the canvas initially seeing a random arrangement of pictures.  They then move only 8 images into rough piles, or even singleton piles, shown in Fig.~\ref{fig:step1}.  The user then asks ActiveCanvas to refine the arrangement according to the implicit mental model ActiveCanvas uncovers from the 8 images the user touched. This results in an initial arrangement shown in Fig.~\ref{fig:step2}, where the system has separated out a number of clusters, but it has not yet reached the user's mental model.  The user moves a few more images each time, iterating through the refinement process.  After touching 20 images and asking for a total of 3 refinements, the user will see the clear clusters they were hoping to uncover, shown in Fig.~\ref{fig:finalstep}. The total elapsed time, from the first touch to the final refinement, was approximately 90 seconds.  

\remove{

\begin{figure*}[htbp]
\includegraphics[width=1.8\columnwidth]{Figs/user_arrangement_response.pdf}
\caption{Without semantic features -- only SIFT features -- the system is not able to further properly further refine image positions, even based on the user's careful arrangement.}\label{fig:initial_arrangement}
\end{figure*}

\begin{figure}[htbp] \centering
   \begin{subfigure}[User 1's arrangement]{ \includegraphics[width=\columnwidth]{Figs/user1.pdf} }
   \end{subfigure}
   \begin{subfigure}[User 2's arrangement]{\includegraphics[width=\columnwidth]{Figs/user2.pdf} }\end{subfigure}
   \caption{Users 1 and 2 diligent classify images, each according to their own mental model. The system does not know each user's intent or mental model, and each user submits their positioning as new features for other users to leverage. Other users do not need to be aware of each users strategy nor arrangement geometry to benefit from the features they generate. The colors above are meant to denote different piles and are merely guides for the eye.}
   \label{fig:goodusers}
\end{figure}

\begin{figure*}[htbp] \centering
   \begin{subfigure}[Lazy User L1\label{fig:L1}]{ \includegraphics[width=3in]{Figs/lazyuser_labels.pdf} }
   \end{subfigure}
      \begin{subfigure}[Lazy User L2\label{fig:L2}]{ \includegraphics[width=3in]{Figs/lazyuser_labels2.pdf} }
   \end{subfigure}
     \begin{subfigure}[Lazy User L1 inferred mental model\label{fig:L1final}]{ \includegraphics[width=3in]{Figs/lazyuser_canvashelp.pdf} }
   \end{subfigure}
      \begin{subfigure}[Lazy User L2 inferred mental model\label{fig:L2final}]{ \includegraphics[width=3in]{Figs/lazyuser_canvashelp2.pdf} }
   \end{subfigure}
   \caption{Lazy users 1 and 2 touch only a small fraction of the images, and they only minimally move each one.}
   \label{fig:lazyusers}
\end{figure*}
}



\section{Conclusion}

Most feature generation techniques, particularly for images, do not produce semantically relevant features.  That is, features tend to be abstract numbers only meaningful in (often nonlinear) combination.  Because users tend to see images based on semantic components (e.g., indoors, flowers, daytime) and not quantitative vectors (e.g., average contrast, SIFT, gradient histograms), their spatial arrangement of images will reflect the  relationships the user identified from their domain knowledge. We present a tool to enable the user to produce those semantic arrangements and implicitly capture these data models as features for future image classification.

\remove{Our system may be used to supplement the extra raw image features that correspond the user's semantic arrangements, but if a given feature set does not produce sufficient discriminatory power, ActiveCanvas enables the user to add their current  positioning as two new features ($x$ and $y$ positions). When new users interact with the data, they can leverage the new features to enhance their own workflows. The system could also allow users to record annotations to document their mental models, and these annotations could be reported to new users when the system determines they are leveraging those corresponding features.

Because the system is inherently agnostic to any underlying features provided by an external algorithm, such as SIFT or dictionary learning, it may be used for text, images, or other media. By mixing these media in the same workspace, the user may position these items to generate mappings between the fundamentally different feature sets behind each type.  For example, text may be characterized by a vector of n-grams, and images may be characterized by a set of SIFT features. By positioning a text snippet at the same position as an image, the user implicitly maps the text to the image via their $x,y$ coordinates. Crucially, a different arrangement would reveal a different semantic relationship in a different context. For example, the text ``I love oranges'' could be associated with a picture of an orange, a picture of fruit or a picture of the user, depending on the context. The context would be provided by the user's relative arrangement of the items in the workspace, providing a realization of their mental model.
}

Future work will explore how many user-provided features essentially `cover' the semantic space.  Although there are potentially an infinite number of ways to arrange images on a canvas, users may only be able to distinguish a limited set of distinct arrangement strategies. How many different users would need to provide their own semantic features before new users would simply be referencing a superposition of existing features? If one relies on machine-generated features to cover most of the geometric or color features, would the number of semantic features needed be closer to 10 or 1000? This number is particularly relevant for evaluating the crowd-sourcing potential for this type of system.

\acknowledgments{
The authors wish to thank Nathan Baker, Landon Sego and Michael Henry. This work is part of the Signature Discovery Initiative at Pacific Northwest National Laboratory.  It was conducted under the Laboratory Directed Research and Development Program at PNNL, a multiprogram national laboratory operated by Battelle for the U.S. Department of Energy.}

\bibliographystyle{abbrv}
\bibliography{AllRefs}


\end{document}