\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/experiments-baselines.jpg}
    \vspace{-0.5cm}
    \caption{Qualitative comparison with baselines.}
    \vspace{-0.5cm}
    \label{fig:viscomp_img}
\end{figure}


\section{Experiments}

\subsection{Training}


For the experiments, we use a high-resolution virtual try-on dataset introduced by VITON-HD~\cite{choi2021viton}, which contains 13,679 frontal-view woman and top clothing image pairs.
The original resolution of the images is 1024$\times$768, and the images are bicubically downsampled to the desired resolutions when needed.
We split the dataset into a training and a test set with 11,647 and 2,032 pairs, respectively.
For detailed information on the model training, see appendix.






\subsection{Qualitative Results}


\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/experiments-segmentation2.jpg}
    \caption{Try-on synthesis results and corresponding segmentation maps.}
    \label{fig:viscomp_seg}
\end{figure}






\noindent\textbf{Comparison with Baselines.}
We compare our method with several state-of-the-art baselines, including CP-VTON~\cite{wang2018toward}, ACGPN~\cite{yang2020towards}, and VITON-HD~\cite{choi2021viton}.
We utilize the publicly available codes for baselines.
Fig.~\ref{fig:viscomp_img} shows that our method generates more photo-realistic images compared to the baselines.
Specifically, we observe that our model not only preserves the details of the target clothing images but also generates the neckline naturally.
As shown in Fig.~\ref{fig:viscomp_seg}, our try-on condition generator has the capability to produce the body shape more naturally compared to VITON-HD.
These results demonstrate that the quality of the conditions for the try-on image generator is crucial in achieving perceptually convincing results.
Furthermore, Fig.~\ref{fig:misalignment} shows that VITON-HD fails to eliminate the artifacts in the misaligned regions completely.
On the other hand, since our method can produce misalignment-free segmentation maps and warped clothing images, our method solves the misalignment problem inherently.
Thus, our method successfully synthesizes the high-quality images.


\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/experiments-misalignment.jpg}
    \caption{Synthesis results and corresponding misaligned regions indicated by yellow colored areas. VITON-HD suffers from the artifacts caused by misalignment.}
    \label{fig:misalignment}
    \vspace{0.5cm}
\end{figure}



\noindent\textbf{Effectiveness of Occlusion Handling.}
We analyze the impact of the occlusion handling process in our try-on condition generator.
Fig.~\ref{fig:occlusion} shows the effectiveness of the proposed body part occlusion handling. 
Without occlusion handling, the model excessively deforms the clothing image to fit the person's body shape, as shown in the $2nd$ column of Fig.~\ref{fig:occlusion}.
Due to the undesired deformation, the texture (\textit{e.g.}, logo and stripe) of the target clothing item is squeezed, causing the missing pattern in the final results (See the $3rd$ column of Fig.~\ref{fig:occlusion}).
On the other hand, the model with occlusion handling enables to warp the clothes without the pixel-squeezing, better preserving the high-frequency details of the garment.


\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/experiments-occlusion.jpg}
    \caption{Effects of the body part occlusion handling. The green colored areas indicate the pixel-squeezing artifacts.}
    \label{fig:occlusion}
    \vspace{0.5cm}
\end{figure}


\noindent\textbf{Effectiveness of Discriminator Rejection.}
To filter out the low-quality segmentation maps produced by our try-on condition generator, we propose a discriminator rejection method.
Fig.~\ref{fig:rejection} shows the accepted and the rejected samples of our discriminator rejection.
Different from the accepted samples, the segmentation maps of the rejected samples are considerably impaired, as shown in the $2nd$ row of Fig.~\ref{fig:rejection}.
We found that the incorrect segmentation maps are caused mainly by errors in the pre-processing step, such as obtaining the clothing mask.
Most virtual try-on methods rely on multiple conditions such as segmentation map and pose information obtained in the pre-processing stage and thus are prone to these errors.
We believe that our discriminator rejection method can be a simple and effective solution for filtering out the low-quality outputs.





\begin{figure}[t!]
    \includegraphics[width=1.0\linewidth]{figures/experiments-rejection.jpg}
    \caption{Examples of accepted (A) and rejected (B) segmentation maps by discriminator rejection, corresponding input clothes and clothing masks.}
    \label{fig:rejection}
\end{figure}


\subsection{Quantitative Results}
\begin{table}[b!]
    \centering
    \footnotesize
    \begin{tabular}{l|cc}
    \toprule
    Method & FID$_{\downarrow}$ & KID$_{\downarrow}$  \\ 
    \cmidrule(lr){1-3}
    HR-VITON & \textbf{10.91} & \textbf{0.179} \\ 
    $\llcorner$ w/o Condition Aligning                     & 12.05 & 0.356 \\
    $\llcorner$ w/o Feature Fusion Block                        & 12.41 & 0.381 \\
    $\llcorner$ w/o Feature Fusion Block \& Condition Aligning*     & 12.73 & 0.415 \\
    \bottomrule
    \end{tabular}
    \caption{Ablation study in unpaired setting. We describes the KID as a value multiplied by 100. *Last row denotes that there is no information exchange.}
    
    \label{table:ablation}
\end{table}


\begin{comment}
\renewcommand{\arraystretch}{1.5}
\begin{table}
  \begin{center}
      \begin{tabular}{p{0.3\textwidth} p{0.3\textwidth} | p{0.1\textwidth} p{0.1\textwidth}@{}}
        \hline
        \hfil Condition Aligning     &  \hfil Feature Fusion Block        & \hfil FID$_{\downarrow}$ &  \hfil KID$_{\downarrow}$  \\
        \hline
        \                &   \               & \hfil 12.73 & \hfil 0.415 \\
        \cline{3-4}
        \hfil \checkmark  &                 & \hfil 12.41 & \hfil 0.381 \\
        \cline{3-4}
                          & \hfil \checkmark & \hfil 12.05 & \hfil 0.356 \\
        \cline{3-4}
        \hfil \checkmark  & \hfil \checkmark & \hfil \textbf{10.91} & \hfil \textbf{0.179} \\
        \hline
      \end{tabular}
    \caption{Ablation study in unpaired setting. We describes the KID as a value multiplied by 100.}
    \label{table:ablation}
  \end{center}
\end{table}
\renewcommand{\arraystretch}{1}
\end{comment}
\begin{table}[]
\centering
\scriptsize
\begin{tabular}{l|cccc|cccc|cccc}
    \toprule
     & \multicolumn{4}{c}{256$\times$192} & \multicolumn{4}{c}{512$\times$384} & \multicolumn{4}{c}{1024$\times$768}\\ 
& LPIPS$_{\downarrow}$ & SSIM$_{\uparrow}$ & FID$_{\downarrow}$ & KID$_{\downarrow}$ & LPIPS$_{\downarrow}$ & SSIM$_{\uparrow}$ & FID$_{\downarrow}$ & KID$_{\downarrow}$ & LPIPS$_{\downarrow}$ & SSIM$_{\uparrow}$ & FID$_{\downarrow}$ & KID$_{\downarrow}$ \\ 
    \midrule
    CP-VTON  & 0.159 & 0.739 & 30.11 & 2.034 
             & 0.141 & 0.791 & 30.25 & 4.012
             & 0.158 & 0.786 & 43.28 & 3.762
             \\
    ACGPN    & 0.074 & 0.833 & 11.33 & 0.344
             & 0.076 & 0.858 & 14.43 & 0.587
             & 0.112 & 0.850 & 43.29 & 3.730
             \\
    VITON-HD & 0.084 & 0.811 & 16.36 & 0.871
             & 0.076 & 0.843 & 11.64 & 0.300
             & 0.077 & 0.873 & 11.59 & 0.247
             \\
    PF-AFN & - & - & - & -
             & - & - & - &
             & - & - & 14.01 & 0.588
             \\
    \midrule
    HR-VITON    & \textbf{0.062} & \textbf{0.864} & \textbf{9.38} & \textbf{0.153}   
             & \textbf{0.061} & \textbf{0.878} & \textbf{9.90} & \textbf{0.188}
             & \textbf{0.065} & \textbf{0.892} & \textbf{10.91} & \textbf{0.179}
             \\ 
    \bottomrule
\end{tabular}
\caption{Quantitative comparison with baselines. 
         We describes the KID as a value multiplied by 100. HR-VITON refers to our model.}
\label{table:main}
\end{table}

\begin{figure}[b!]
    \includegraphics[width=1.0\linewidth]{figures/Comparisons_PF-AFN.jpg}
    \caption{Qualitative comparison with PF-AFN on 1024Ã—768 resolution.}
    \label{fig:PF}
\end{figure}
Following previous studies, we evaluate a paired setting and an unpaired setting, where the paired setting is to reconstruct the person image with the original clothing image, and the unpaired setting is to change the clothing item of the person image.
For paired setting, we evaluate our method using two widely-used metrics: Structural Similarity (SSIM)~\cite{wang2004image} and Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}.
Additionally, to evaluate the unpaired setting, we measure Frechet Inception Distance (FID)~\cite{heusel2017gans} and Kernel Inception Distance (KID), which is a more descriptive metric than FID when the number of data is small.

\noindent \textbf{Ablation Study.}
Table~\ref{table:ablation} shows the effectiveness of the proposed feature fusion block and condition aligning.
Indeed, the benefits of fusion block and condition aligning are largely additive.
Notably, the model without feature fusion block and condition aligning yields suboptimal results, demonstrating the necessity of information exchange between the warping module and the segmentation map generator.

\noindent \textbf{Comparison with Baselines.} Table~\ref{table:main} demonstrates that our method outperforms the baselines for all evaluation metrics, especially at the 1024$\times$768 resolution.
The results indicate that CP-VTON and ACGPN can not handle the high-resolution images in the unpaired setting.
Furthermore, it is noteworthy that our framework surpasses VITON-HD, one of the state-of-the-art methods for high-resolution virtual try-on.
Although our try-on image generator is very similar to one of VITON-HD, our framework has superior performance due to the capability to produce high-quality conditions (\textit{i.e.}, segmentation map and warped clothing image).

\subsection{Comparison with Parser-free Virtual Try-on Methods}
Recently, several approaches~\cite{issenhuth2020not,ge2021parser} propose virtual try-on models that do not rely on a predicted segmentation map.
However, explicitly predicting a segmentation map helps the model distinguish the regions to be generated and the regions to be preserved, which is necessary for a high-resolution virtual try-on.
To verify this, we compare our model with PF-AFN~\cite{ge2021parser} on the high-resolution dataset.
Fig.~\ref{fig:PF} demonstrates that PF-AFN fails to remove the original clothing regions as it can not differentiate the parts to be generated and the parts to be left, resulting in significant artifacts in the outputs.
Moreover, Table~\ref{table:main} shows that our model outperforms PF-AFN by a large margin.
The results indicate that it is difficult to obtain convincing high-resolution results without predicting a segmentation map.

