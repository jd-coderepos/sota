\section{Theoretical statements}
\label{sec:appendix_theory}

\begin{definition}[-clusterable data set]

  We say that a data set  is \emph{()-clusterable} for fixed  and  if there exists
  a partitioning of it into subsets  which we call
  \emph{clusters}, each with their associated unit-norm cluster center ,
  that satisfy the following conditions:



  \begin{itemize}[leftmargin=*]

    \item  and ; 

    \item all the points in a cluster lie in the -neighborhood of their
      corresponding cluster center, i.e.\  for all  and all ;

    \item a fraction of at least  of the points in each cluster 
      have the same label, which we call the \emph{cluster label} and denote
      . The remaining points suffer from label noise;

    \item if two cluster  and  have different labels, then their
      centers are  far from each other, i.e.\ ;

    \item the clusters are balanced i.e.\ for all , where  and
       are two positive constants.

  \end{itemize}

\end{definition}

In our case, for a fixed label , we assume that the set
 is -clusterable into 
clusters. We further assume that each cluster  only includes a few noisy
samples from , i.e.\  and that for clusters  whose
cluster label is not , i.e.\ , it
holds that . 

We define the matrices  and
, with  and where  denotes the elementwise product. We use 
and  to denote the spectral norm and the smallest
eigenvalue of a matrix, respectively.

For prediction, we consider a 2-layer neural network model with  hidden
units, where . We can
write this model as follows:



The first layer weights  are initialized with random values drawn from
, while the last layer weights  have fixed values: half of them
are set to  and the other half is . We consider activation functions
 with bounded first and second order derivatives, i.e.\  and .
We use the squared loss for training, i.e.\  and take gradient descent steps to find the optimum of the loss
function, i.e.\ , where the step
size is set to .

We can now state the following proposition:

\begin{proposition}
  \label{proposition_appendix}

  Assume that  and , where  is a constant such that  and  is a constant that depends on .
  Then it holds with high probability  over the
  initialization of the weights that the neural network trained on  perfectly fits , 
  and , but not , after
   iterations.


\end{proposition}

This result shows that there exists an optimal stopping time at which the neural
network predicts the correct label on all ID points and the label 
on all the OOD points. As we will see later in the proof, the proposition is
derived from a more general result which shows that the early stopped model
predicts these labels not only on the points in  but also in an
-neighborhood around cluster centers. Hence, an  ensemble can be
used to detect holdout OOD samples similar to the ones in , after
being tuned on . This follows the intuition that classifiers
regularized with early stopping are smooth and generalize well.

The clusterable data model is generic enough to include data sets with
non-linear decision boundaries. Moreover, notice that the condition in
Proposition~\ref{proposition_appendix} is satisfied when  is -clusterable and  is
-clusterable and if the cluster centers of  are at
distance at least  from the cluster centers of . A situation in which these requirements are met is, for
instance, when the OOD data comes from novel classes, when all classes
(including the unseen ones that are not in the training set) are well separated,
with cluster centers at least  away in Euclidean distance. In addition,
in order to limit the amount of label noise in each cluster, it is necessary
that the number of incorrectly labeled samples in  is
small, relative to the size of .

In practice, we only need that the decision boundary separating
 from  is easier to learn than the classifier
required to interpolate the incorrectly labeled , which
is often the case, provided that  is large enough and the OOD
samples come from novel classes.

We now provide the proof for Proposition~\ref{proposition_appendix}:

\begin{proof}

  We begin by restating a result from \citet{mahdi}:

\begin{theorem}[\citep{mahdi}]
  \label{thm:mahdi}

  Let  be an -clusterable
  training set, with  and
  , where  is a constant that satisfies . Consider a two-layer neural network as described above,
  and train it with gradient descent starting from initial weights sampled
  i.i.d.\ from . Assume further that the step size is  and that the number of hidden units  is at least
  . Under these conditions, it
  holds with probability at least  over the random
  draws of the initial weights, that after
   gradient descent steps, the
  neural network  predicts the correct cluster label for all
  points in the -neighborhood of the cluster center, namely:

  

  where  yields one-hot embeddings of the
  labels. The constants  depend only on .

\end{theorem}

Notice that, under the assumptions introduced above, the set  is -clusterable, since the incorrectly labeled ID
points in  constitute at most a fraction  of the
clusters they belong to. As a consequence,
Proposition~\ref{proposition_appendix} follows directly from
Theorem~\ref{thm:mahdi}.

\end{proof}






\vspace{-0.5cm}
\section{Disagreement score for novelty detection}
\label{sec:appendix_statistic}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/disagreement.png}

  \caption{\small{Cartoon illustration showing a diverse ensemble of linear
      binary classifiers. We compare novelty detection performance for two
      aggregation scores:  (\textbf{Left}) and  with
      
      (\textbf{Right}). The two metrics achieve similar TPRs, but using
       instead of our score, , leads to more false positives,
      since the former simply flags as OOD a band around the averaged model
      (solid black line) and does not take advantage of the ensemble's
  diversity.}}

  \label{fig:ensemble_disagreement}
\end{figure}


As we argue in Section~\ref{sec:earlystopping},
Algorithm~\ref{algo:reto_training} produces an ensemble that disagrees on OOD
data, and hence, we want to devise a scalar score that reflects this model
diversity.
Previous works \citep{balaji, ood_ovadia} first average the softmax predictions
of the models in the ensemble and then use the entropy as a metric, i.e.\
 where
 and  is the 
element of \footnote{We abuse notation slightly and
  denote our disagreement metric as  to contrast it with the ensemble
  entropy metric , which first takes the average of the softmax outputs
and only afterwards computes the score.}. We argue later that averaging discards
information about the diversity of the models.

Recall that our average pairwise \emph{disagreement} between the outputs of 
models in an ensemble reads:\footnote{We abuse notation slightly and denote our
  disagreement metric as  to contrast it with the ensemble entropy metric
  , which first takes the average of the softmax outputs and only
afterwards computes the score.}



\noindent where  is a measure of disagreement between the softmax outputs
of two predictors, for example the total variation distance
 used in
our experiments.



We briefly highlight the reason why averaging softmax outputs \emph{first} like
in previous works relinquishes all the benefits of having a more diverse
ensemble, as opposed to the proposed pairwise score in
Equation~\ref{eq:statistic}. Recall that varying thresholds yield different true
negative and true positive rates (TNR and TPR, respectively) for a given
statistic.
In the sketch in Figure~\ref{fig:ensemble_disagreement} we show that the score
we propose, , achieves a higher TNR compared to , for a fixed
TPR, which is a common way of evaluating statistical tests. Notice that the
detection region for  is always limited to a band around the average
model for any threshold value . In order for the  to have large
TPR, this band needs to be wide, leading to many false positives. Instead, our
disagreement score exploits the diversity of the models to more accurately
detect OOD data. 

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{figures/vanilla_ensemble_diversity.png}
  \end{center}

  \caption{Relying only on the randomness of SGD and of the weight
    initialization to diversify models is not enough, as it often yields similar
    classifiers. Each column shows a different predictor trained from random
    initializations with Adam. All models have the same 1-hidden layer MLP
  architecture.}

  \label{fig:vanilla_diversity}
\end{figure}

We now provide further quantitative evidence to support the intuition presented
in Figure~\ref{fig:ensemble_disagreement}.  The aggregation metric is tailored
to exploit ensemble diversity, which makes it particularly beneficial for
.  On the other hand, Vanilla Ensembles only rely on the stochasticity
of the training process and the random initializations of the weights to produce
diverse models, which often leads to classifiers that are strikingly similar as
we show in Figure~\ref{fig:vanilla_diversity} for a few 2D data sets. As a
consequence, using our disagreement score  for Vanilla Ensembles can
sometimes hurt novelty detection performance. To see this, consider the extreme
situation in which the models in the ensemble are identical, i.e.\ .
Then it follows that , for all test points  and
for any function  that satisfies the distance axioms.  

We note that the disagreement score that we propose takes a form that is similar
to previous diversity scores, e.g.\ \citet{Zhang2010, mcd_ood}. In the context
of regression, one can measure uncertainty using the variance of the outputs
metric previously employed in works such as \citet{gal2016}. However, we point
out that using the output variance requires that the ensemble is the result of
sampling from a random process (e.g.\ sampling different training data for the
models, or sampling different parameters from a posterior). In our framework, we
obtain the ensemble by solving a different optimization problem for each of the
models by assigning a different label to the unlabeled data.  Therefore, despite
their similarities, our disagreement score and the output variance are, on a
conceptual level, fundamentally different metrics.

Table~\ref{table:score_comparison} shows that  leads to worse novelty 
detection performance for Vanilla Ensembles, compared to using the entropy of
the average softmax score, , which was proposed in prior work.
However, if the ensembles are indeed diverse, as we argue is the case for our
method  (see Section~\ref{sec:earlystopping}), then there is a clear
advantage to using a score that, unlike , takes diversity into account,
as shown in Table~\ref{table:score_comparison} for 5-model  ensembles. 


\begin{table}[h]
\tiny

\caption{\small{The disagreement score that we propose  exploits ensemble
    diversity and benefits in particular  ensembles. Novelty detection
    performance is significantly improved when using  compared to the
    previously proposed  metric. Since Vanilla Ensemble are not diverse
    enough, a score that relies on model diversity can hurt novelty detection
    performance. We highlight the AUROC and the TNR@95 obtained with the score
    function that is  and the
.}}

\label{table:score_comparison}
\begin{center}

\hyphenpenalty10000
\begin{tabularx}{0.8\textwidth}{ll| XXXX}
\toprule
\makecell{ID data} & \makecell{OOD data} & \makecell{Vanilla\\Ensembles\\} &  \makecell{Vanilla\\Ensembles\\} & \makecell{ERD\\} &  \makecell{ERD\\} \\
& & \multicolumn{4}{c}{AUROC  / TNR@95  } \\
\midrule
 &  & \bestnonreto{0.97} / \bestnonreto{0.88} & 0.96 / \bestnonreto{0.89} & 0.86 / 0.85 & \bestreto{0.99} / \bestreto{0.97} \\
 &  & \bestnonreto{0.92} / \bestnonreto{0.78} & 0.91 / \bestnonreto{0.78} & 0.92 / 0.92 & \bestreto{1.00} / \bestreto{1.00} \\
 &  & \bestnonreto{0.84} / \bestnonreto{0.48} & 0.79 / 0.46 & 0.36 / 0.35 & \bestreto{1.00} / \bestreto{1.00} \\
 &  & \bestnonreto{0.92} / \bestnonreto{0.69} & 0.91 / \bestnonreto{0.69} & \bestreto{0.94} / \bestreto{0.66} & \bestreto{0.94} / \bestreto{0.66} \\
 &  & \bestnonreto{0.80} / \bestnonreto{0.39} & \bestnonreto{0.80} / \bestnonreto{0.39} & \bestreto{0.91} / 0.65 & \bestreto{0.91} / \bestreto{0.66} \\
 &  & \bestnonreto{0.78} / \bestnonreto{0.35} & 0.76 / 0.34 & 0.63 / 0.38 & \bestreto{0.81} / \bestreto{0.40} \\

\midrule
\multicolumn{2}{c|}{Average} & \bestnonreto{0.87} / \bestnonreto{0.60} & 0.86 / 0.59 & 0.77 / 0.64 & \bestreto{0.94} / \bestreto{0.78} \\

\bottomrule
\end{tabularx} 
\end{center}
\end{table}

We highlight once again that other methods that attempt to obtain diverse
ensembles, such as MCD, fail to train models with sufficient disagreement, even
when they use oracle OOD for hyperparameter tuning
(Figure~\ref{fig:scores_mcd_es}).

\begin{figure*}[h]
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/score_distrib_mcd_es.png}
    \caption{Not enough diversity (MCD)}
    \label{fig:scores_mcd_es}
  \end{subfigure}
\begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/score_distrib_erd.png}
    \caption{Regularized diversity (ERD)}
    \label{fig:scores_erd}
  \end{subfigure}

\caption{Distribution of disagreement scores on ID and OOD data
  for an ensemble that is not diverse enough (\textbf{Left}), and an ensemble
  with regularized disagreement (\textbf{Right}). Note that MCD is early-stopped
  using oracle OOD data. ID=CIFAR10[0:4], OOD=CIFAR10[5:9].}


\end{figure*}







\section{Taxonomy of OOD detection methods according to overall objective}
\label{sec:appendix_related_work}

We now provide more details regarding the categorization of OOD detection
approaches based on the different surrogate objectives that they
use in order to detect OOD samples.

\paragraph{Learning the ID marginal .} We loosely define OOD samples as
all  for which , for a small constant .
Therefore, if we had access to the marginal training distribution ,
we would have perfect OOD detection. Realistically, however,  is
unknown, and we need to resort to estimating it. Explicit density estimation
with generative models \citep{ganomaly2018, nalisnick} is inherently
difficult in high dimensions. 
Alternatively, one-class classification \citep{mari2010, Ruff2020, sohn2021} and
PU learning approaches \citep{duPlessis14, Kiryo17} try to directly learn a
discriminator between ID and OOD data in the presence of known (e.g.\ A-UND) or
unknown (e.g.\ SSND) OOD data. However, these methods tend to produce
indistinguishable representations for inliers and outliers when the ID
distribution consists of many diverse classes.

\paragraph{Learning  using label information (ours).} Since in a
prediction problem, the ID training set has class labels, one can take advantage
of that additional information to distinguish points in the support of 
from OOD data. For instance, \citet{mahalanobis, gram_ood} propose to use the
intermediate representations of neural networks trained for prediction to detect
OOD data. Often, the task is to also simultaneously predict well on ID data, a
problem known as open-set recognition \citep{Geng2021} and
tackled by approaches like OpenHybrid \citep{openhybrid2020}.

\paragraph{Learning uncertainty estimates for .} In the prediction
setting, calibrated uncertainty estimates error could naturally be used to
detect OOD samples. Many uncertainty quantification methods are based on a
Bayesian framework \citep{gal2016, dpn} or calibration
improvement \citep{odin, Hafner2019}. However, neither of them perform as well
as other OOD methods mentioned above \citep{ood_ovadia}.




\section{Experiment details}
\label{sec:appendix_experiments}

\subsection{Baselines}

In this section we describe in detail the baselines with which we compare our
method and describe how we choose their hyperparameters. For all baselines we
use the hyperparameters suggested by the authors for the respective data sets
(e.g.\ different hyperparameters for CIFAR10 or ImageNet). For all methods, we
use pretrained models provided by the authors. However, we note that for the
novel-class settings, pretraining on the entire training set means that the
model is exposed to the OOD classes as well, which is undesirable. Therefore,
for these settings we pretrain only on the split of the training set that
contains the ID classes. Since the classification problem is similar to the
original one of training on the entire training set, we use the same
hyperparameters that the authors report in the original papers.

Moreover, we point out that even though different methods use different model
architectures, that is not inherently unreasonable when the goal is novelty
detection, since it is not clear if a complex model is more desirable than a
smaller model. For this reason, we use the model architecture recommended by the
authors of the baselines and which was used to produce the good results reported
in their published works. For Vanilla Ensembles and for  we show
results for different architectures in
Appendix~\ref{sec:appendix_different_arch}.

\begin{itemize}



  \item \textbf{Vanilla Ensembles} \citep{balaji}: We train an ensemble on the
    training set according to the true labels. For a test sample, we average the
    outputs of the softmax probabilities predicted by the models, and use the
    entropy of the resulting distribution as the score for the hypothesis test
    described in Section~\ref{sec:disagreement}. We use ensembles of 5 models,
    with the same architecture and hyperparameters as the ones used for
    . Hyperparameters are tuned to achieve good validation accuracy.

  \item \textbf{Gram method} \citep{gram_ood}: The Gram baseline is similar to
    the Mahalanobis method in that both use the intermediate feature
    representations obtained with a deep neural network to determine whether a
    test point is an outlier. However, what sets the Gram method apart is the
    fact that it does not need any OOD data for training or calibration. We use
    the pretrained models provided by the authors, or train our own, using the
    same methodology as described for the Mahalanobis baseline. For OOD
    detection, we use the code published by the authors. We note that for MLP
    models, the Gram method is difficult to tune and we could not find a
    configuration that works well, despite our best efforts and following the
    suggestions proposed during our communication with the authors.


  \item \textbf{Deep Prior Networks (DPN)} \citep{dpn}: DPN is a Bayesian Method
    that trains a neural network (Prior Network) to parametrize a Dirichlet
    distribution over the class probabilities.  We train a WideResNet WRN-28-10
    for  epochs using SGD with momentum , with an initial learning
    rate of , which is decayed by  at epochs , , and .
    For MNIST, we use EMINST/Letters as OOD for tuning. For all other settings,
    we use TinyImages as OOD for tuning.

  \item \textbf{Outlier Exposure} \citep{outlier_exposure}: This approach makes
    a model's softmax predictions close to the uniform distribution on the known
    outliers, while maintaining a good classification performance on the
    training distribution. We use the WideResNet architecture (WRN). For
    fine-tuning, we use the settings recommended by the authors, namely we train
    for  epochs with learning rate . For training from scratch, we
    train for  epochs with an initial learning rate of .  When the
    training data set is either CIFAR10/CIFAR100 or ImageNet, we use the default
    WRN parameters of the author's code, namely  layers,  widen-factor,
    droprate .  When the training dataset is SVHN, we use the author's
    recommended parameters of  layers,  widen-factor and droprate .
    All settings use the cosine annealing learning rate scheduler provided with
    the author's code, without any modifications. For all settings, we use
    TinyImages as known OOD data during training. In
    Section~\ref{sec:appendix_more_oe} we show results for known OOD data that
    is similar to the OOD data used for testing.

  \item \textbf{Mahalanobis} \citep{mahalanobis}: The method pretrains models on
    the labeled training data. For a test data point, it uses the intermediate
    representations of each layer as ``extracted features''. It then performs
    binary classification using logistic regression using these extracted
    features. In the original setting, the classification is done on
    ``training'' ID vs ``training'' OOD samples (which are from the same
    distribution as the test OOD samples).  Furthermore, hyperparameter tuning
    for the optimal amount of noise is performed on validation ID and OOD data.
    We use the WRN-28-10 architecture, pretrained for  epochs.  The initial
    learning rate is , which is decayed at epochs , , and  by
    . We use SGD with momentum , and the standard weight decay of . The code published for the Mahalanobis method performs a
    hyperparameter search automatically for each of the data sets. 

\end{itemize}

The following baselines attempt to leverage the unlabeled data that is available
in applications such as the one depicted in Figure~\ref{fig:practical_sketch},
similar to . 

\begin{itemize}

  \item \textbf{Non-negative PU learning (nnPU)} \citep{Kiryo17}: The method
    trains a binary predictor to distinguish between a set of known positives
    (in our case the ID data) and a set that contains a mixture of positives and
    negatives (in our case the unlabeled set). To prevent the interpolation of
    all the unlabeled samples, \citet{Kiryo17} proposes a regularized objective.
    It is important to note that most training objectives in the PU learning
    literature require that the ratio between the positives and negatives in the
    unlabeled set is known or easy to estimate. For our experiments we always
    use the exact OOD ratio to train the nnPU baseline. Therefore, we obtain an
    upper bound on the AUROC/TNR@95. If the ratio is estimated from finite
    samples, then estimation errors may lead to slightly worse OOD detection
    performance. We perform a grid search over the learning rate and the
    threshold that appears in the nnPU regularizer and pick the option with the
    best validation accuracy measured on a holdout set with only positive
    samples (in our case, ID data). 


  \item \textbf{Maximum Classifier Discrepancy (MCD)} \citep{mcd_ood}: The MCD
    method trains two classifiers at the same time, and makes them disagree on
    the unlabeled data, while maintaining good classification performance.  We
    use the WRN-28-10 architecture as suggested in the paper.  We did not change
    the default parameters which came with the author's code, so weight decay is
    , and the optimizer is SGD with momentum .  When available
    (for CIFAR10 and CIFAR100), we use the pretrained models provided by the
    authors. For the other training datasets, we use their methodology to
    generate pretrained models: We train a WRN-28-10 for 200 epochs.  The
    learning rate starts at 0.1 and drops by a factor of 10 at  and 
    of the training progress.

  \item \textbf{Mahalanobis-U}: This is a slightly different version of the
    Mahalanobis baseline, for which we use early-stopped logistic regression to
    distinguish between the training set and an unlabeled set with ID and OOD
    samples (instead of discriminating a known OOD set from the inliers). The
    early stopping iteration is chosen to minimize the classification errors on
    a validation set that contains only ID data (recall that we do not assume to
    know which are the OOD samples).

\end{itemize}

In addition to these approaches that have been introduced in prior work, we also
propose a strong novel baseline that
that bares some similarity to PU learning and to . 

\begin{itemize}

 \item \textbf{Binary classifier} The approach consists in discriminating
   between the labeled ID training set and the mixed unlabeled set, that
   contains both ID and OOD data. We use regularization to prevent the trivial
   solution for which the entire unlabeled set is predicted as OOD. Unlike PU
   learning, the binary classifier does not require that the OOD ratio in the
   test distribution is known. The approach is similar to a method described in
   \citep{scott08} which also requires that the OOD ratio of the unlabeled set is
   known.  We tune the learning rate and the weight of the unlabeled samples in
   the training loss by performing a grid search and selecting the configuration
   with the best validation accuracy, computed on a holdout set containing only
   ID samples.  We note that the binary classifier that appears in
   Section~\ref{sec:appendix_medical} in the medical benchmark, is not the same
   as this baseline. For more details on the binary classifier that appears in
   the medical data experiments we refer the reader to \citet{Cao2020}.

\end{itemize}

\vspace{-0.2cm}
\subsection{Training configuration for }

For  we always use hyperparameters that give the best validation
accuracy when training a model on the ID training set. In other words, we pick
hyperparameter values that lead to good ID generalization and do not perform
further hyperparameter tuning for the different OOD data sets on which we
evaluate our approach. We point out that, if the ID labeled set is known to
suffer from class imbalance, subpopulation imbalance or label noise, any
training method that addresses these issues can be used instead of standard
empirical risk minimization to train our ensemble (e.g.\ see 
\citet{mahdi}).

For MNIST and FashionMNIST, we train ensembles of 3-layer MLP models with ReLU
activations. Each intermediate layer has 100 neurons. The models are optimized
using Adam, with a learning rate of , for  epochs.

For SVHN, CIFAR10/CIFAR100 and ImageNet, we train ensembles of ResNet20
\citep{He2015}. The models are initialized with weights pretrained for 
epochs on the labeled training set. We fine-tune each model for 10 epochs using
SGD with momentum , and a learning rate of .  The weights are
trained with an  regularization coefficient of .  We use a batch
size of 128 for all scenarios, unless explicitly stated otherwise. We used the
same hyperparameters for all settings. 

For pretraining, we perform SGD for 100 epochs and use the same architecture and
hyperparameters as described above, with the exception of the learning rate that
starts at , and is multiplied by  at epochs ,  and .

Apart from , which fine-tunes the ensemble models starting from
pretrained weights, we also present in the Appendix results for ++.
This variant of our method trains the models from random initializations, and
hence needs more iterations to converge, making it more computationally
expensive than . We train all models in the ++ ensembles for
 epochs with a learning rate that starts at , and is multiplied by
 at epochs ,  and . All other hyperparameters are the same as
for  ensembles.

For the medical data sets, we train a Densenet-121 as the authors do in the
original paper \citep{Cao2020}. For ++, we do not use random weight
initializations, but instead we start with the ImageNet weights provided with
Tensorflow. The training configuration is exactly the same as for ResNet20,
except that we use a batch size of 32 due to GPU memory restrictions, and for
fine tuning we use a constant learning rate of .

\vspace{-0.2cm}
\subsection{Computational considerations for }

We note that  models reach the optimal stopping time within the first
10 epochs on all the data sets that we consider, which amounts to around 
minutes of training time 
if the models in the ensemble are fine-tuned in parallel on NVIDIA 1080 Ti GPUs.
This is substantially better than the cost of fine-tuning a large ViT
transformer model (which takes about 1 hour for 2500 iterations on the same
hardware). Moreover, since the loss we use to train the ensemble decouples over
the models, it allows for easy parallelization, unlike objectives like MCD where
the ensemble models are intertwined.




\vspace{-0.1cm}
\section{ID and OOD data sets}
\label{sec:appendix_datasets}
\vspace{-0.2cm}

\subsection{Data sets}

For evaluation, we use the following image data sets: MNIST \citep{mnist},
Fashion MNIST \citep{fashion}, SVHN \citep{svhn}, CIFAR10 and CIFAR100
\citep{cifar}.

For the experiments using MNIST and FashionMNIST the training set size is 50K,
the validation size is 10K, and the test ID and test OOD sizes are both 10K.
For SVHN, CIFAR10 and CIFAR100, the training set size is 40K, the validation
size is 10K, and the unlabeled set contains 10K samples: 5K are ID and 5K are
OOD. For evaluation, we use a holdout set of 10K examples (half ID, half OOD).
For the settings that use half of the classes as ID and the other half as OOD,
all the sizes are divided by 2.

\vspace{-0.2cm}
\subsection{Samples for the settings with novel classes}

\vspace{-0.2cm}
\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
     \centering

    \includegraphics[width=\textwidth]{figures/dataset_samples/mnist01234}
    \includegraphics[width=\textwidth]{figures/dataset_samples/mnist56789}
    \includegraphics[width=\textwidth]{figures/dataset_samples/fashion_mnist02378}
    \includegraphics[width=\textwidth]{figures/dataset_samples/fashion_mnist14569}

     \caption{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.4\textwidth}
     \centering

    \includegraphics[width=\textwidth]{figures/dataset_samples/cifar1001234}
    \includegraphics[width=\textwidth]{figures/dataset_samples/cifar1056789}
    \includegraphics[width=\textwidth]{figures/dataset_samples/svhn_cropped01234}
    \includegraphics[width=\textwidth]{figures/dataset_samples/svhn_cropped56789}

     \caption{}
  \end{subfigure}

  \caption{(a) Data samples for the MNIST/FashionMNIST splits. (b) Data samples for the CIFAR10/SVHN splits.}
  \label{fig:data_samples_mnist_splits}

\end{figure}







\section{More experiments}
\label{sec:appendix_more_experiments}

We now present more experimental results that provide additional insights about
the proposed approach. We note that, unless otherwise specified, we use 5-model
ERD ensembles in this section.

\vspace{-0.2cm}
\subsection{Evaluation on the unlabeled set}
\label{sec:appendix_transductive_results}

In the main text we describe how one can leverage the unlabeled set 
to obtain an novelty detection algorithm that accurately identifies outliers at test
time that similar to the ones in . It is, however, possible to also
use our method  to flag the OOD samples contained in the same set
 used for fine-tuning the ensemble. In
Table~\ref{table:ss_vs_transductive} we show that the novelty detection performance
of  is similar regardless of whether we use  for
evaluation, or a holdout test set  drawn from the same distribution as
.

\begin{table}[H]
\tiny

\caption{Comparison between the novelty detection performance of  when
using a holdout test set  for evaluation, or the same unlabeled set
 that was used for fine-tuning the models.}


\begin{center}

\hyphenpenalty10000
\begin{tabularx}{0.48\textwidth}{ll| cc}
\toprule
\makecell{ID data} & \makecell{OOD data} & \makecell{\eval on )} \\
& & \multicolumn{2}{c}{AUROC  / TNR@95  } \\
\midrule
 &  & 1.00 / 0.99 & 1.00 / 0.99 \\
 &  & 1.00 / 1.00 & 1.00 / 1.00 \\
 &  & 1.00 / 1.00 & 1.00 / 1.00 \\

\midrule
 &  & 0.94 / 0.67 & 0.94 / 0.67 \\

 &  & 0.95 / 0.74 & 0.96 / 0.79 \\
 &  & 0.93 / 0.70 & 0.93 / 0.69 \\
 &  & 0.82 / 0.44 & 0.80 / 0.36 \\

\midrule
\multicolumn{2}{c|}{Average} & 0.95 / 0.79 & 0.95 / 0.79 \\

\bottomrule
\end{tabularx}
 
\label{table:ss_vs_transductive}
\end{center}
\end{table}


\vspace{-0.3cm}
\subsection{Comparison with other related works}
\label{sec:appendix_cifar10_cifar100}

We compare 5-model ERD ensembles to more OOD detection approaches. For various
reasons we did not run these methods ourselves on the data sets for which we
evaluate our method in Section~\ref{sec:experiments} (e.g.\ code not available,
unable to replicate published results, poor performance reported by the authors
etc). We collected the AUROC numbers presented in
Table~\ref{table_cifar10_cifar100} from the papers that introduce each method.
We note that our approach shows an excellent overall performance, being
consistently better than or on par with the related works that we consider.
While the method of \citet{fort2021} performs significantly better than all
other baselines on CIFAR10/CIFAR100 tasks, we argue in
Appendix~\ref{sec:appendix_vit} that this is primarily due to the convenient
choice of the data set used for pretraining the transformer models (i.e.\
Imagenet21k) which is strikingly similar to the ID and OOD data.


OpenHybrid \citep{openhybrid2020} is an open set recognition approach which
reports great near OOD detection performance. We note that, despite our best
efforts, we did not manage to match in our own experiments the results reported
in the paper, even after communicating with the authors and using the code that
they have provided. Moreover, we point out that the performance of OpenHybrid
seems to deteriorate significantly when the ID data consists of numerous
classes, as is the case for CIFAR100.

Furthermore, we note that generative models \citep{nalisnick, 
ganomaly2018} and one-class classification approaches \citep{Ruff2020, Tack2020,
sohn2021} showed generally bad performance, in particular on near OOD data. When
the ID training set is made up of several diverse classes, it is difficult to
represent accurately all the ID data, and only the ID data.

\begin{table*}[h]
\tiny
\centering

\caption{AUROC numbers collected from the literature for a number of relevant
OOD detection methods. We note that the method of \citet{fort2021} () uses a large scale
visual transformer models pretrained on a superset of the OOD data, i.e.\
ImageNet21k, while the method of \citet{sehwag2021} () uses oracle OOD
samples for training from the same data set as test OOD. For the settings with random classes, the numbers are averages over 5
draws and the standard deviation is always strictly smaller than  for our
method.}
\label{table_cifar10_cifar100}

\begin{tabularx}{\textwidth} {@{}ll @{} @{\hskip 0.1cm} XXXXXXXXX @{}} 
  \toprule
ID data  & OOD data & \citet{fort2021} & \citet{openhybrid2020} &
\citet{winkens2020} & \citet{Tack2020} & \citet{sehwag2021} &\citet{liu2020hybrid} &
\citet{yujie2020} & ERD (ours) & ERD++ (ours)                   \\
\midrule

CIFAR10  & CIFAR100 & 98.52                             & 0.95          & 0.92
         & 0.92           &     0.93               & 0.91  & - & 0.92       & 0.95 \\
CIFAR100 & CIFAR10  & 96.23                             & 0.85
         & 0.78                                 & - & 0.78
         & - & -                                     & 0.91       & 0.94 \\
\midrule
\makecell[l]{SVHN: 6\\ random\\ classes} & \makecell[l]{SVHN: 4\\ random\\
classes}  & -                             & 0.94
          & -               & -                  & -
          & -                                & 0.91      & 0.94       & 0.94 \\
\makecell[l]{CIFAR10: 6\\ random\\ classes} & \makecell[l]{CIFAR10: 4\\ random\\
classes}  & -                             & 0.94
          & -                  & -               & -
          & -                                & 0.85      & 0.94       & 0.97 \\

  \bottomrule
\end{tabularx}
\vspace{0.5cm}

\end{table*}

\subsection{Shortcomings of pretrained ViT models for novelty detection}
\label{sec:appendix_vit}

In this section we provide further experimental results pointing to the fact
that large pretrained transformer models \citep{fort2021} can only detect near
OOD samples from certain specific data sets, and do not generalize well more
broadly.

\paragraph{Implementation details.} We fine-tune visual transformer (ViT) models
pretrained on Imagenet21k according the methodology described in
\citet{fort2021}. We report results using the ViT-S-16 architecture (~22 million
trainable parameters) which we fine-tune for 2500 iterations on labeled ID data.
We use the hyperparameters suggested by the authors and always ensure that the
prediction accuracy of the fine-tuned model on ID data is in the expected range.
The code published by the authors uses three different test statistics to detect
OOD data: the maximum softmax probability \citep{Hendrycks2017}, the vanilla
Mahalanobis distance \citep{mahalanobis} and a recently proposed variant of the
Mahalanobis approach \citep{Ren2021}. In Table~\ref{table:vit} we present only
the metrics obtained with the best-performing test statistic for ViT. We stress
that this favors the ViT method significantly, as different test statistics seem
to perform better on different data sets. Since test OOD data is unknown, it is
not possible to select which test statistic to use a priori, and hence, we use
oracle knowledge to give ViT models an unfair advantage.

\paragraph{Experimental results.} In Table~\ref{table:vit} we compare pretrained
visual transformers with 5-model  and ++ ensembles.  Notably,
the data sets can be partitioned in two clusters, based on ViT novelty detection
performance. On the one hand, if the ID or OOD data comes from CIFAR10 or
CIFAR100, ViT models can detect novel-class samples well. Perhaps surprisingly,
ViT fails short of detecting OOD data perfectly (i.e. AUROC and TNR@95 of 1) on
easy tasks such as CIFAR10 vs SVHN or CIFAR100 vs SVHN, unlike  and a
number of other baseline approaches.

On the other hand, ViT shows remarkably poor performance on all other data sets,
when neither the ID nor the OOD data come from CIFAR10/CIFAR100. This includes
some of the novel disease use cases from the medical OOD detection benchmark
(see Appendix~\ref{sec:appendix_medical} for more details about the data sets).
This unsatisfactory performance persists even for larger ViT models (we
have tried ViT-S-16 and ViT-B-16 architectures), when fine-tuning for more
iterations (we have tried both 2500 and 10000 iterations), or when varying
hyperparameters such as the learning rate.

\paragraph{Intuition for why ViT fails.} We conjecture that the novelty
detection performance with pretrained ViT models relies heavily on the choice of
the pretraining data set. In particular, we hypothesize that, since
CIFAR10/CIFAR100 classes are included in the Imagenet21k data set used for
pretraining, the models learn features that are useful for distinguishing ID and
OOD classes when the ID and/or OOD data comes from CIFAR10/CIFAR100. Hence, this
would explain the good performance of pretrained models on the data sets at the
top of Table~\ref{table:vit}. On the other hand, when ID and OOD data is
strikingly different from the pretraining data, both ID and OOD samples are
projected to the same concentrated region of the representation space, which
makes it difficult to detect novel-class points.  Moreover, the process of
fine-tuning as it is described in \citet{fort2021} seems to not help to
alleviate this problem. This leads to the poor performance observed on the near
OOD data sets at the bottom of Table~\ref{table:vit}.

In conclusion, having a large pretraining data set seems to be beneficial when
the OOD data shares many visual and semantic features in common with the
pretraining data. However, in real-world applications it is often difficult to
collect such large data sets, which makes the applicability of pretrained ViT
models limited to only certain specific scenarios.

\begin{table}[H]
\tiny

\caption{Pretrained ViT models tend to perform well when the ID and OOD data is
  semantically similar to (or even included in) the pretraining data, e.g.\
  CIFAR10, CIFAR100 (top part), and their detection performance deteriorates
  drastically otherwise (bottom part). We compare ViT-S-16 models pretrained on
Imagenet21k with 5-model  and ++ ensembles and
\bestnonreto{highlight} the best method. See Appendix~\ref{sec:appendix_medical}
for more details about the medical data sets.}

\label{table:vit}

\centering

\hyphenpenalty10000
\begin{tabularx}{0.59\linewidth}{ll| ccc}
\toprule
\makecell{ID data} & \makecell{OOD data} & \makecell{ViT} & \makecell{} & \makecell{++} \\
& & \multicolumn{3}{c}{AUROC  / TNR@95  } \\
\midrule
 &  & 0.98 / 0.90 & \bestnonreto{1.00 / 0.99} & \bestnonreto{1.00 / 0.99} \\
 &  & 0.99 / 0.98 & \bestnonreto{1.00 / 1.00} & \bestnonreto{1.00 / 1.00} \\
 &  & 0.96 / 0.84 & \bestnonreto{1.00 / 1.00} & \bestnonreto{1.00 / 1.00} \\
 &  & \bestnonreto{0.97 / 0.83} & 0.93 / 0.70 & 0.96 / 0.79 \\
 &  & \bestnonreto{0.90 / 0.56} & 0.82 / 0.44 & 0.85 / 0.45 \\

\midrule
 &  & 0.88 / 0.49 & 0.94 / 0.67 & \bestnonreto{0.95 / 0.71} \\

 &  & 0.89 / 0.68 & 0.95 / 0.74 & \bestnonreto{0.96 / 0.77} \\

 &  & 0.87 / 0.38 & 0.94 / 0.63 & \bestnonreto{0.99 / 0.97} \\
 &  & \bestnonreto{0.54 / 0.04} & 0.46 / 0.04 & 0.50 / 0.04 \\
 &  & 0.89 / 0.61 & \bestnonreto{0.99} / 0.99 & \bestnonreto{0.99 / 1.00} \\
 &  & 0.54 / 0.07 & \bestnonreto{0.77 / 0.17} & 0.72 / 0.08 \\
 &  & 0.61 / 0.20 & 0.91 / 0.73 & \bestnonreto{1.00 / 0.98} \\


\bottomrule
\end{tabularx}
 
\end{table}



\vspace{-0.5cm}
\subsection{OOD detection for data with covariate shift}
\label{sec:appendix_cov_shift}

\vspace{-0.2cm}
In this section we evaluate the baselines and the method that we propose on
settings in which the OOD data suffers from covariate shift.
The goal is to identify all samples that come from the shifted distribution,
regardless of how strong the shift is. Notice that mild shifts may be easier to
tackle by domain adaptation, but when the goal is OOD detection they
pose a more difficult challenge.

We want to stress that in practice one may not be interested in identifying
\emph{all} samples with distribution shift as OOD, since a classifier may still
produce correct predictions on some of them. 
In contrast, when data suffers from covariate shift we can try to learn
predictors that perform well on both the training and the test distribution, and
we may use a measure of predictive uncertainty to identify only those test
samples on which the classifier cannot make confident predictions. Nevertheless,
we use these covariate shift settings as a challenging OOD detection benchmark
and show in Table~\ref{table:all_resnet_results} that our method  does
indeed outperform prior baselines on these difficult settings.

We use as outliers corrupted variants of CIFAR10 and CIFAR100 \citep{cifar_c}, as
well as a scenario where ImageNet \citep{Deng2009} is used as ID data and
ObjectNet \citep{Barbu2019} as OOD, both resized to 32x32.
Figure~\ref{fig:data_samples_cifar_and_objectnet} shows samples from these data
sets. The Gram and nnPU baselines do not give satisfactory results on the
difficult CIFAR10/CIFAR100 settings in Table~\ref{table:main_results} and thus
we do not consider them for the covariate shift cases.  For the SSND methods
(e.g.\ MCD, Mahal-U and /++) we evaluate on the same unlabeled
set that is used for training (see the discussion in
Section~\ref{sec:appendix_transductive_results}).


\begin{figure*}[h]
\vspace{-0.2cm}
  \centering
  \hspace*{\fill}\begin{subfigure}[r]{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/dataset_samples/imagenet_objectnet}

\end{subfigure}
\hfill
\begin{subfigure}[c]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/dataset_samples/cifar10c_sev2}
  \includegraphics[width=\textwidth]{figures/dataset_samples/cifar10c_sev5}
\end{subfigure}

  \caption{Left: Samples from ImageNet and ObjectNet taken from the original
  paper by \citep{Barbu2019}. Right: Data samples for the corrupted CIFAR10-C
data set.}

  \label{fig:data_samples_cifar_and_objectnet}
\end{figure*}


Furthermore, we present results on distinguishing between CIFAR10 \citep{cifar}
and CIFAR10v2 \citep{recht}, a data set meant to be drawn from the same
distribution as CIFAR10 (generated from the Tiny Images collection).  In \citet{recht}, the authors argue
that CIFAR10 and CIFAR10v2 come from very similar distributions. They provide
supporting evidence by training a binary classifier to distinguish between them,
and observing that the accuracy that is obtained of 52.9\% is very close to
random.

Our experiments show that the two data sets are actually distinguishable,
contrary to what previous work has argued. First, our own binary classifier
trained on CIFAR10 vs CIFAR10v2 obtains a test accuracy of 67\%, without any
hyperparameter tuning. The model we use is a ResNet20 trained for 200 epochs
using SGD with momentum 0.9. The learning rate is decayed by 0.2 at epochs 90,
140, 160 and 180. We use 1600 examples from each data set for training, and we
validate using 400 examples from each data set.

\begin{table}[H]
\tiny
\caption{OOD detection performance on CIFAR10 vs CIFAR10v2}
\label{table:cifar10v2}
\begin{center}
\setlength{\tabcolsep}{1pt}
\hyphenpenalty10000
\begin{tabularx}{\textwidth}{@{}ll @{}| @{\hskip 0.2cm} X X X X X X X X  @{}}
\toprule
\makecell[l]{ ID data } & \makecell[l]{ OOD data } & \makecell[l]{ Vanilla\\Ensembles } & \makecell[l]{ DPN } & \makecell[l]{ OE } & \makecell[l]{ Mahal. } & \makecell[l]{ MCD } & \makecell[l]{ Mahal-U } & \makecell[l]{  } & \makecell[l]{ ++ } \\
& & \multicolumn{8}{c}{AUROC  / TNR@95 } \\
\midrule
\makecell[l]{ CIFAR10 } & \makecell[l]{ CIFAR10v2 } & \bestnonreto{0.64} / \bestnonreto{0.13} & 0.63 / 0.09 & \bestnonreto{0.64} / 0.12 & 0.55 / 0.08 & 0.58 / 0.10 & 0.56 / 0.07 & 0.76 / 0.26 & \bestreto{0.91} / \bestreto{0.80} \\


\bottomrule
\end{tabularx} \end{center}
\end{table}

Our OOD detection experiments (presented in Table~\ref{table:cifar10v2}) show
that most baselines are able to distinguish between the two data sets, with
 achieving the highest performance. The methods which require OOD data
for tuning (Outlier Exposure and DPN) use CIFAR100. 


\begin{table}[H]
\tiny

\caption{OOD detection performance on data with covariate shift. For 
and vanilla ensembles, we train 5 ResNet20 models for each setting. The
evaluation metrics are computed on the unlabeled set.}


\begin{center}

\clearpage{}\setlength{\tabcolsep}{1pt}
\hyphenpenalty10000
\begin{tabularx}{\textwidth}{@{}ll @{}| @{\hskip 0.2cm} X X X X X X X X  @{}}
\toprule
\makecell[l]{ ID data } & \makecell[l]{ OOD data } & \makecell[l]{ Vanilla\\Ensembles } & \makecell[l]{ DPN } & \makecell[l]{ OE } & \makecell[l]{ Mahal. } & \makecell[l]{ MCD } & \makecell[l]{ Mahal-U } & \makecell[l]{  } & \makecell[l]{ ++ } \\
& & \multicolumn{8}{c}{AUROC  / TNR@95 } \\
\midrule
 &  & 0.68 / 0.20 & 0.73 / 0.31 & 0.70 / 0.20 & \bestnonreto{0.84} / \bestnonreto{0.53} & 0.82 / 0.50 & 0.75 / 0.38 & 0.96 / 0.86 & \bestreto{0.99} / \bestreto{0.95} \\
 &  & 0.51 / 0.05 & 0.47 / 0.03 & 0.52 / 0.06 & \bestnonreto{0.58} / \bestnonreto{0.08} & 0.52 / 0.06 & 0.55 / 0.07 & 0.68 / 0.19 & \bestreto{0.86} / \bestreto{0.41} \\
 &  & 0.84 / 0.49 & 0.89 / 0.60 & 0.86 / 0.54 & 0.94 / 0.80 & \bestnonreto{0.95} / \bestnonreto{0.84} & 0.88 / 0.63 & \bestreto{1.00} / 0.99 & \bestreto{1.00} / \bestreto{1.00} \\
 &  & 0.60 / 0.10 & 0.72 / 0.10 & 0.63 / 0.11 & \bestnonreto{0.78} / \bestnonreto{0.27} & 0.60 / 0.08 & 0.68 / 0.12 & 0.98 / 0.86 & \bestreto{1.00} / \bestreto{1.00} \\
\midrule
 &  & 0.68 / 0.20 & 0.62 / 0.18 & 0.65 / 0.19 & \bestnonreto{0.82} / \bestnonreto{0.48} & 0.72 / 0.29 & 0.67 / 0.22 & 0.94 / 0.76 & \bestreto{0.97} / \bestreto{0.86} \\
 &  & 0.52 / 0.06 & 0.32 / 0.03 & 0.52 / 0.06 & \bestnonreto{0.55} / \bestnonreto{0.07} & 0.52 / 0.06 & 0.55 / 0.06 & 0.71 / 0.19 & \bestreto{0.86} / \bestreto{0.44} \\
 &  & 0.78 / 0.37 & 0.74 / 0.36 & 0.76 / 0.37 & \bestnonreto{0.92} / \bestnonreto{0.72} & 0.91 / 0.65 & 0.84 / 0.55 & 0.99 / 0.97 & \bestreto{1.00} / \bestreto{0.99} \\
 &  & 0.64 / 0.14 & 0.49 / 0.12 & 0.62 / 0.13 & \bestnonreto{0.71} / \bestnonreto{0.19} & 0.60 / 0.10 & 0.63 / 0.13 & 0.96 / 0.71 & \bestreto{0.98} / \bestreto{0.89} \\

\midrule
 &  & 0.82 / 0.49 & 0.70 / 0.32 & 0.79 / 0.37 & 0.75 / 0.26 & \bestnonreto{0.99} / \bestnonreto{0.98} & 0.72 / 0.25 & 0.98 / 0.88 & \bestreto{0.99} / \bestreto{0.98} \\

\midrule
\multicolumn{2}{c | @{\hskip 0.2cm}}{Average} & 0.67 / 0.23 & 0.63 / 0.23 & 0.67 / 0.23 & \bestnonreto{0.76} / 0.38 & 0.74 / \bestnonreto{0.39} & 0.70 / 0.27 & 0.91 / 0.71 & \bestreto{0.96} / \bestreto{0.83} \\

\bottomrule
\end{tabularx}\clearpage{}

\label{table:all_resnet_results}
\end{center}
\end{table}

\vspace{-0.5cm}
\subsection{Results with a smaller unlabeled set}
\label{sec:appendix_small_test_set}

We now show that our method performs well even when the unlabeled set is
significantly smaller. In particular, we show in the table below that 
maintains a high AUROC and TNR@95 even when only 1,000 unlabeled samples are
used for fine-tuning (500 ID and 500 OOD). 

\begin{table}[H]
\tiny

\caption{Experiments with a test set of size 1,000, with an equal number of ID
and OOD test samples. For  and vanilla ensembles, we train 5 ResNet20 models for
each setting. The evaluation metrics are computed on the unlabeled set.}
\vspace{-0.5cm}

\begin{center}

\clearpage{}\setlength{\tabcolsep}{1pt}
\hyphenpenalty10000
\begin{tabularx}{\textwidth}{@{}ll @{}| @{\hskip 0.2cm} X X X X X X X  @{}}
\toprule
\makecell[l]{ ID data } & \makecell[l]{ OOD data } & \makecell[l]{ Vanilla\\Ensembles } & \makecell[l]{ DPN } & \makecell[l]{ OE } & \makecell[l]{ Mahal. } & \makecell[l]{ MCD } & \makecell[l]{ Mahal-U } & \makecell[l]{  } \\
& & \multicolumn{7}{c}{AUROC  / TNR@95 } \\
\midrule
 &  & 0.97 / 0.88 & \bestnonreto{1.00} / \bestnonreto{1.00} & \bestnonreto{1.00} / \bestnonreto{1.00} & 0.99 / 0.98 & 0.97 / 0.85 & 0.99 / 0.95 & \bestreto{1.00} / \bestreto{0.99} \\
 &  & 0.92 / 0.78 & 0.95 / 0.85 & 0.97 / 0.89 & \bestnonreto{0.99} / \bestnonreto{0.96} & 1.00 / 0.98 & 0.99 / 0.96 & \bestreto{1.00} / \bestreto{1.00} \\
 &  & 0.84 / 0.48 & 0.77 / 0.44 & 0.82 / 0.50 & \bestnonreto{0.98} / \bestnonreto{0.90} & 0.97 / 0.73 & 0.98 / 0.92 & \bestreto{0.99} / \bestreto{1.00} \\
 &  & \bestnonreto{0.92} / 0.69 & 0.87 / 0.19 & 0.85 / 0.52 & \bestnonreto{0.92} / \bestnonreto{0.71} & 0.91 / 0.51 & 0.91 / 0.63 & \bestreto{0.97} / \bestreto{0.86} \\
 &  & 0.80 / 0.39 & \bestnonreto{0.82} / 0.32 & \bestnonreto{0.82} / \bestnonreto{0.41} & 0.79 / 0.27 & 0.69 / 0.25 & 0.64 / 0.13 & \bestreto{0.87} / \bestreto{0.50} \\
 &  & \bestnonreto{0.78} / \bestnonreto{0.35} & 0.70 / 0.26 & 0.74 / 0.31 & 0.72 / 0.20 & 0.70 / 0.26 & 0.72 / 0.19 & \bestreto{0.79} / \bestreto{0.38} \\

\midrule
 &  & 0.68 / 0.20 & 0.73 / 0.31 & 0.70 / 0.20 & \bestnonreto{0.84} / \bestnonreto{0.53} & 0.82 / 0.50 & 0.75 / 0.38 & \bestreto{0.91} / \bestreto{0.71} \\
 &  & 0.51 / 0.05 & 0.47 / 0.03 & 0.52 / 0.06 & \bestnonreto{0.58} / \bestnonreto{0.08} & 0.52 / 0.06 & 0.55 / 0.07 & \bestreto{0.57} / \bestreto{0.09} \\
 &  & 0.84 / 0.49 & 0.89 / 0.60 & 0.86 / 0.54 & \bestnonreto{0.94} / \bestnonreto{0.80} & 0.95 / 0.84 & 0.88 / 0.63 & \bestreto{0.99} / \bestreto{0.95} \\
 &  & 0.60 / 0.10 & 0.72 / 0.10 & 0.63 / 0.11 & \bestnonreto{0.78} / \bestnonreto{0.27} & 0.60 / 0.08 & 0.68 / 0.12 & \bestreto{0.92} / \bestreto{0.67} \\
\midrule
 &  & 0.68 / 0.20 & 0.62 / 0.18 & 0.65 / 0.19 & \bestnonreto{0.82} / \bestnonreto{0.48} & 0.72 / 0.29 & 0.67 / 0.22 & \bestreto{0.84} / \bestreto{0.48} \\
 &  & 0.52 / 0.06 & 0.32 / 0.03 & 0.52 / 0.06 & \bestnonreto{0.55} / \bestnonreto{0.07} & 0.52 / 0.06 & \bestreto{0.55} / 0.06 & \bestreto{0.55} / \bestreto{0.07} \\
 &  & 0.78 / 0.37 & 0.74 / 0.36 & 0.76 / 0.37 & \bestnonreto{0.92} / \bestnonreto{0.72} & 0.91 / 0.65 & 0.84 / 0.55 & \bestreto{0.96} / \bestreto{0.80} \\
 &  & 0.64 / 0.14 & 0.49 / 0.12 & 0.62 / 0.13 & \bestnonreto{0.71} / \bestnonreto{0.19} & 0.60 / 0.10 & 0.63 / 0.13 & \bestreto{0.81} / \bestreto{0.25} \\

\midrule
\multicolumn{2}{c | @{\hskip 0.2cm}}{Average} & 0.75 / 0.37 & 0.72 / 0.34 & 0.75 / 0.38 & \bestnonreto{0.82} / \bestnonreto{0.51} & 0.78 / 0.44 & 0.77 / 0.42 & \bestreto{0.87} / \bestreto{0.62} \\

\bottomrule
\end{tabularx}\clearpage{}

\end{center}
\end{table}


\vspace{-0.5cm}
\subsection{More results for Outlier Exposure}
\label{sec:appendix_more_oe}

The Outlier Exposure method needs access to a set of OOD samples during
training. The numbers we report in the rest of paper for Outlier Exposure are
obtained by using the TinyImages data set as the OOD samples that are seen
during training.  In this section we explore the use of an OOD
data set that is more similar to the OOD data observed at test time. This is a
much easier setting for the Outlier Exposure method: the closer
OOD is to OOD, the easier it will be for the
model tuned on OOD to detect the test OOD samples.

In the table below we focus only on the settings with corruptions. For each
corruption type, we use the lower severity corruption as OOD
and evaluate on the higher severity data and vice versa. We report for each metric
the average taken over all corruptions (A), and the value for the worst-case
setting (W).

\begin{table}[H]
  \small

\begin{center}


\hyphenpenalty10000
\begin{tabularx}{0.8\textwidth}{ll| cc}
\toprule
\makecell{ID data} & \makecell{OOD data} & \makecell{OE (trained on sev5)} & \makecell{OE (trained on sev2)} \\
& & \multicolumn{2}{c}{AUROC } \\
\midrule
 &  & 0.89 & N/A \\
 &  & 0.65 & N/A \\
 &  & N/A & 0.98 \\
 &  & N/A & 0.78 \\
\midrule
 &  & 0.85 & N/A \\
 &  & 0.59 & N/A \\
 &  & N/A & 0.97 \\
 &  & N/A & 0.67 \\

\midrule
\multicolumn{2}{c|}{Average} & 0.87 & 0.98 \\

\bottomrule
\end{tabularx}
 \end{center}


\label{table:oe}
    \caption{Results for Outlier Exposure, when using the same corruption type,
    but with a higher/lower severity, as OOD data seen during training.}

\end{table}



\vspace{-0.5cm}
\subsection{Results on MNIST and FashionMNIST}

\begin{table}[H]
\tiny

\caption{Results on MNIST/FashionMNIST settings. For  and vanilla
ensembles, we train 5 3-hidden layer MLP models for each setting. The evaluation metrics are computed on the unlabeled set.}


\begin{center}

\setlength{\tabcolsep}{1pt}
\hyphenpenalty10000
\begin{tabularx}{\textwidth}{@{}ll @{}| @{\hskip 0.2cm} X X X X X X X X X X  @{}}
\toprule
\makecell[l]{ ID data } & \makecell[l]{ OOD data } & \makecell[l]{ Vanilla\\Ensembles } & \makecell[l]{ DPN } & \makecell[l]{ OE } & \makecell[l]{ Mahal. } & \makecell[l]{ nnPU } & \makecell[l]{ MCD } & \makecell[l]{ Mahal-U } & \makecell[l]{ Bin.\\Classif. } & \makecell[l]{  } & \makecell[l]{ ++ } \\
& & \multicolumn{10}{c}{AUROC  / TNR@95 } \\
\midrule
 &  & 0.81 / 0.01 & \bestnonreto{1.00} / \bestnonreto{1.00} & \bestnonreto{1.00} / \bestnonreto{1.00} & \bestnonreto{1.00} / \bestnonreto{1.00} & \bestnonreto{1.00} / \bestnonreto{1.00} & \bestnonreto{1.00} / 0.98 & \bestnonreto{1.00} / \bestnonreto{1.00} & 1.00 / 1.00 & \bestreto{1.00} / \bestreto{1.00} & \bestreto{1.00} / \bestreto{1.00} \\
 &  & 0.87 / 0.42 & \bestnonreto{1.00} / \bestnonreto{1.00} & 0.68 / 0.16 & 0.99 / 0.97 & \bestnonreto{1.00} / \bestnonreto{1.00} & \bestnonreto{1.00} / \bestnonreto{1.00} & 0.99 / 0.96 & 1.00 / 1.00 & \bestreto{1.00} / \bestreto{1.00} & \bestreto{1.00} / \bestreto{1.00} \\
 &  & 0.94 / 0.72 & \bestnonreto{0.99} / 0.97 & 0.95 / 0.78 & \bestnonreto{0.99} / \bestnonreto{0.98} & \bestnonreto{0.99} / 0.97 & 0.96 / 0.76 & \bestnonreto{0.99} / \bestnonreto{0.98} & 0.99 / 0.94 & \bestreto{0.99} / 0.96 & \bestreto{0.99} / \bestreto{0.97} \\
 &  & 0.64 / 0.07 & 0.77 / 0.15 & 0.66 / 0.12 & 0.77 / 0.20 & \bestnonreto{0.95} / \bestnonreto{0.71} & 0.78 / 0.30 & 0.82 / 0.39 & 0.95 / 0.66 & \bestreto{0.94} / 0.67 & \bestreto{0.94} / \bestreto{0.68} \\

\midrule
\multicolumn{2}{c | @{\hskip 0.2cm}}{Average} & 0.82 / 0.30 & 0.94 / 0.78 & 0.82 / 0.51 & 0.94 / 0.79 & \bestnonreto{0.98} / \bestnonreto{0.92} & 0.94 / 0.76 & 0.95 / 0.83 & 0.98 / 0.90 & \bestreto{0.98} / \bestreto{0.91} & \bestreto{0.98} / \bestreto{0.91} \\

\bottomrule
\end{tabularx} 
\end{center}
\end{table}

\vspace{-0.4cm}
For FashionMNIST we chose this particular split (i.e. classes 0,2,3,7,8 vs
classes 1,4,5,6,9) because the two partitions are more similar to each other.
This makes novelty detection more difficult than the 0-4 vs 5-9 split.



\vspace{-0.2cm}
\subsection{Vanilla and  Ensembles with different architectures}
\label{sec:appendix_different_arch}

In this section we present OOD detection results for 5-model Vanilla and
 ensembles with different architecture choices, and note that the
better performance of our method is maintained across model classes. Moreover,
we observe that  benefits from employing more complex models, like the
WideResNet.

\begin{table}[H]
\tiny

\caption{Results with three different architectures for Vanilla and 
  ensembles. All ensembles comprise 5 models. For the corruption data sets, we
  report for each metric the average taken over all corruptions (A), and the
  value for the worst-case setting (W). The evaluation metrics are computed on
the unlabeled set.}


\begin{center}

\hyphenpenalty10000
\begin{tabularx}{\textwidth}{@{}ll @{} @{\hskip 0.2cm} XX >{\centering\arraybackslash} XX >{\centering\arraybackslash} XX@{}}
\toprule
& & \multicolumn{2}{c}{VGG16} & \multicolumn{2}{c}{ResNet20} & \multicolumn{2}{c}{WideResNet-28-10} \\
\cmidrule(l{2pt}r{2pt}){3-4}
\cmidrule(l{2pt}r{2pt}){5-6}
\cmidrule(l{2pt}r{2pt}){7-8}
\makecell{ID data} & \makecell{OOD data} & \makecell{Vanilla\\Ensembles} &  \makecell{ERD} & \makecell{Vanilla\\Ensembles} &  \makecell{ERD} & \makecell{Vanilla\\Ensembles} &  \makecell{ERD} \\
& & \multicolumn{6}{c}{AUROC  / TNR@95  } \\
\midrule
 &  & 0.97 / 0.88 & 0.99 / 0.94 & 0.97 / 0.88 & 0.99 / 0.97 & 0.96 / 0.86 & 1.00 / 0.99 \\
 &  & 0.88 / 0.69 & 1.00 / 1.00 & 0.92 / 0.78 & 1.00 / 1.00 & 0.94 / 0.81 & 1.00 / 1.00 \\
 &  & 0.89 / 0.60 & 0.93 / 0.63 & 0.92 / 0.69 & 0.94 / 0.66 & 0.91 / 0.62 & 0.96 / 0.78 \\
 &  & 0.74 / 0.29 & 0.91 / 0.63 & 0.80 / 0.39 & 0.91 / 0.66 & 0.80 / 0.35 & 0.94 / 0.71 \\

\midrule
 &  & 0.66 / 0.17 & 0.94 / 0.79 & 0.68 / 0.20 & 0.96 / 0.86 & 0.69 / 0.18 & 0.98 / 0.90 \\
 &  & 0.51 / 0.05 & 0.68 / 0.19 & 0.51 / 0.05 & 0.68 / 0.19 & 0.51 / 0.05 & 0.84 / 0.35 \\
 &  & 0.80 / 0.41 & 0.99 / 0.96 & 0.84 / 0.49 & 1.00 / 0.99 & 0.84 / 0.47 & 1.00 / 1.00 \\
 &  & 0.58 / 0.10 & 0.95 / 0.72 & 0.60 / 0.10 & 0.98 / 0.86 & 0.59 / 0.09 & 0.99 / 0.97 \\

\midrule
\multicolumn{2}{c}{Average} & 0.75 / 0.40 & 0.92 / 0.73 & 0.78 / 0.45 & 0.93 / 0.77 & 0.78 / 0.43 & 0.96 / 0.84 \\

\bottomrule
\end{tabularx} 
\end{center}
\end{table}


\vspace{-0.5cm}
\subsection{Impact of the ensemble size and of the choice of arbitrary label}
\label{sec:appendix_ensemble_size}

In this section we show novelty detection results with our method using a smaller
number of models for the ensembles. We notice that the performance is not
affected substantially, indicating that the computation cost of our approach
could be further reduced by fine-tuning smaller ensembles.

\begin{table}[H]
\tiny

\caption{Results obtained with smaller ensembles for . The numbers for
   are averages over 3 runs, where we use a different set of arbitrary
  labels for each run to illustrate our method's stability with respect the
  choice of labels to be assigned to the unlabeled set. We note that the
standard deviations are small ( for the AUROC values and
 for the TNR@95 values).}


\begin{center}

\hyphenpenalty10000
\begin{tabularx}{\textwidth}{@{}l@{\hskip -0.01cm}l @{} @{\hskip 0.1cm} XX >{\centering\arraybackslash} XX >{\centering\arraybackslash} XX >{\centering\arraybackslash} XX@{}}
\toprule
& & \multicolumn{2}{c}{K=2} & \multicolumn{2}{c}{K=3} & \multicolumn{2}{c}{K=4} & \multicolumn{2}{c}{K=5} \\
\cmidrule(l{2pt}r{2pt}){3-4}
\cmidrule(l{2pt}r{2pt}){5-6}
\cmidrule(l{2pt}r{2pt}){7-8}
\cmidrule(l{2pt}r{2pt}){9-10}
\makecell{ID data} & \makecell{OOD data} & \makecell{ERD} & \makecell{ERD++} & \makecell{ERD} & \makecell{ERD++} & \makecell{ERD} & \makecell{ERD++} & \makecell{ERD} & \makecell{ERD++} \\
& & \multicolumn{6}{c}{AUROC  / TNR@95  } \\
\midrule
 &  & 0.99 / 0.98 & 0.99 / 0.99 & 0.99 / 0.98 & 1.00 / 0.99 & 0.99 / 0.98 & 1.00 / 0.99 & 1.00 / 0.99 & 1.00 / 0.99 \\
 &  & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 \\
 &  & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 & 1.00 / 1.00 \\
 &  & 0.95 / 0.69 & 0.94 / 0.68 & 0.95 / 0.73 & 0.95 / 0.75 & 0.96 / 0.76 & 0.96 / 0.77 & 0.95 / 0.74 & 0.96 / 0.77 \\
 &  & 0.89 / 0.55 & 0.92 / 0.58 & 0.89 / 0.57 & 0.94 / 0.70 & 0.90 / 0.57 & 0.95 / 0.73 & 0.93 / 0.70 & 0.96 / 0.79 \\
 &  & 0.81 / 0.40 & 0.82 / 0.43 & 0.81 / 0.41 & 0.84 / 0.44 & 0.81 / 0.41 & 0.84 / 0.44 & 0.82 / 0.44 & 0.85 / 0.45 \\


\midrule
\multicolumn{2}{c}{Average} & 0.94 / 0.77 & 0.95 / 0.78 & 0.94 / 0.78 & 0.95 / 0.81 & 0.94 / 0.79 & 0.96 / 0.82 & 0.95 / 0.81 & 0.96 / 0.83 \\

\bottomrule
\end{tabularx}
 
\end{center}
\end{table}

\vspace{-0.5cm}
\paragraph{Impact of the choice of arbitrary labels.} Furthermore, we note that
in the table we report averages over 3 runs of our method, where for each
run we use a different subset of  to assign arbitrary labels to the
unlabeled data. We do this in order to assess the stability of 
ensembles to the choice of the arbitrary labels and notice that the novelty 
detection performance metrics do not vary significantly.  Concretely, the
standard deviations are consistently below  for all data sets for the
AUROC metric, and below  for the TNR@95 metric.


\vspace{-0.2cm}
\subsection{Detection performance on different OOD data}
\label{sec:appendix_different_ood}

In this section we investigate whether the proposed method maintains its good
novelty detection performance when the test-time OOD data comes from a different
data set compared to the OOD data that is present in the unlabeled set used for
fine-tuning. In particular, we are interested if our approach can still identify
outliers in situations when they suffer from various corruptions. This scenario
can sometimes occur in practice, when machine failure or uncurated data can lead
to mild distribution shift.

Concretely, we focus on the difficult near OOD scenarios and take as ID half of
the CIFAR10 or CIFAR100 classes, while the other half is OOD. For this
experiment, we fine-tune the ERD ensembles using clean OOD data from the other
half of CIFAR10 and CIFAR100, respectively. For evaluation, we use clean ID
data and corrupted OOD samples from CIFAR10-C and CIFAR100-C, respectively.
We give more details on these corrupted data sets in
Appendix~\ref{sec:appendix_cov_shift}. We consider corruptions of severity 2 and
5 from all corruptions types.

In Table~\ref{table:different_ood} we show the average AUROC and the worst AUROC
over all corruption types for vanilla and ERD ensembles. Note that our approach
maintains a similar performance compared to the numbers presented in
Table~\ref{table:main_results} for same test-time OOD data. It is also
noteworthy that all the average AUROC values are consistently larger than the
baselines in Table~\ref{table:main_results}.


\begin{table}[H]
  \small
  \begin{center}

    \caption{Results obtained when evaluating on an OOD data set different from
    the one used for fine-tuning. All ERD ensembles are tuned on clean ID and
  OOD data and are evaluated on OOD data with corruptions.}


\hyphenpenalty10000
\begin{tabularx}{0.88\textwidth}{lll| cc}
\toprule
\makecell{ID data} & \makecell{OOD data in\\unlabeled set} & \makecell{Test-time\\OOD data} & \makecell{Vanilla\\Ensemble} & \makecell{ERD} \\
                   & & & \multicolumn{2}{c}{AUROC } \\
\midrule
 &  &  & 0.82 & 0.93 \\
 &  &  & 0.77 & 0.88 \\
 &  &  & 0.85 & 0.91 \\
 &  &  & 0.79 & 0.86 \\
\midrule
 &  &  & 0.78 & 0.84 \\
 &  &  & 0.75 & 0.78 \\
 &  &  & 0.77 & 0.83 \\
 &  &  & 0.63 & 0.78 \\


\bottomrule
\end{tabularx}

\label{table:different_ood}
\end{center}
\end{table}






\vspace{-0.6cm}
\section{Medical OOD detection benchmark}
\label{sec:appendix_medical}

\vspace{-0.3cm}
The medical OOD detection benchmark is organized as follows. There are four
training (ID) data sets, from three different domains: two data sets with chest
X-rays, one with fundus imaging and one with histology images. For each ID data
set, the authors consider three different OOD scenarios:

\vspace{-0.4cm}
\begin{enumerate}[leftmargin=*]
  \compresslist

  \item Use case 1: The OOD data set contains images from a completely different
    domain, similar to our category of easy OOD detection settings.

  \item Use case 2: The OOD data set contains images with various corruptions,
    similar to the hard covariate shift settings that we consider in
    Section~\ref{sec:appendix_cov_shift}.

  \item Use case 3: The OOD data set contains images that come from novel
    classes, not seen during training.

\end{enumerate}

\vspace{-0.7cm}
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/dataset_samples/medical_samples.png}
  \end{center}

\vspace{-0.5cm}
  \caption{Samples from the medical image benchmark. There are 3 ID data sets
  containing frontal and lateral chest X-rays and retinal images. Hard OOD
samples contain images of diseases that are not present in the training set.}

  \label{fig:medical_samples}
\end{figure}

The authors evaluate a number of methods on all these scenarios. The methods can
be roughly categorized as follows:

\begin{enumerate}[leftmargin=*]
  \compresslist

  \item Data-only methods: Fully non-parametric approaches like kNN.

  \item Classifier-only methods: Methods that use a classifier trained on the
    training set, e.g.\ ODIN \citep{odin}, Mahalanobis \citep{mahalanobis}. 
    falls into this category as well.

  \item Methods with Auxiliary Models: Methods that use an autoencoder or a
    generative model, like a Variational Autoencoder or a Generative Adversarial
    Network. Some of these approaches can be expensive to train and difficult to
    optimize and tune.

\end{enumerate}

We stress the fact that for most of these methods the authors use (known) OOD
data during training. Oftentimes the OOD samples observed during training come
from a data set that is very similar to the OOD data used for evaluation.  For
exact details regarding the data sets and the methods used for the benchmark, we
refer the reader to \citet{Cao2020}. 


\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\textwidth]{figures/avg_medical_ood.png}
  \end{center}

\caption{AUROC averaged over all scenarios in the medical OOD detection
    benchmark \citep{Cao2020}. The values for all the baselines are computed
    using code made available by the authors of \citet{Cao2020}. Notably,
    most of the baselines assume oracle knowledge of OOD data at training time.}

  \label{fig:avg_medical_ood}
\end{figure}


\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\textwidth]{figures/avg_medical_ood_novel_class.png}
  \end{center}

\caption{AUROC averaged over the novel-class scenarios in the medical OOD
  detection benchmark \citep{Cao2020}, i.e.\ only use case 3.}

  \label{fig:avg_medical_novel_class}
\end{figure}


In addition, in Figure~\ref{fig:avg_medical_novel_class} we present the average
taken over only the novel-class settings in the medical benchmark. We observe
that the performance of all methods is drastically affected, all of them
performing much worse than the average presented in
Figure~\ref{fig:avg_medical_ood}. This stark decrease in AUROC and TNR@95
indicates that novelty detection is indeed a challenging task for OOD detection
methods even in realistic settings. Nevertheless, 2-model ERD ensembles maintain
a better performance than the baselines.

In Figures~\ref{fig:medical_nih}, \ref{fig:medical_pad}, \ref{fig:medical_drd}
we present AUROC and AUPR (Area under the Precision Recall curve) for 
for each of the training data sets, and each of the use cases.
Figure~\ref{fig:avg_medical_ood} presents averages over all settings that we
considered, for all the baseline methods in the benchmark.  Notably, 
performs well consistently across data sets. The baselines are ordered by their
average performance on all the settings (see Figure~\ref{fig:avg_medical_ood}).

For all medical benchmarks, the unlabeled set is balanced, with an equal
number of ID and OOD samples (subsampling the bigger data set, if necessary). We
use the unlabeled set for evaluation.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/medical_NIHCC.png}
  \end{center}

  \vspace{-0.5cm}
  \caption{Comparison between  and the various baselines on the NIH chest
  X-ray data set, for use case 1 (top), use case 2 (middle) and use case 3
(bottom). Baselines ordered as in Figure~\ref{fig:avg_medical_ood}.}
  \vspace{-0.2cm}

  \label{fig:medical_nih}
\end{figure}

  \vspace{-0.5cm}
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/medical_PAD.png}
  \end{center}

  \vspace{-0.5cm}
  \caption{Comparison between  and the various baselines on the PC chest
  X-ray data set, for use case 1 (top), use case 2 (middle) and use case 3
(bottom). Baselines ordered as in Figure~\ref{fig:avg_medical_ood}.}
  \vspace{-0.2cm}


  \label{fig:medical_pad}
\end{figure}

\vspace{-0.3cm}
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{figures/medical_DRD.png}
  \end{center}

  \vspace{-0.5cm}
  \caption{Comparison between  and the various baselines on the DRD fundus
    imaging data set, for use case 1 (top), use case 2 (middle) and use case 3
(bottom). Baselines ordered as in Figure~\ref{fig:avg_medical_ood}.}


  \label{fig:medical_drd}
\end{figure}




\vspace{-1cm}
\section{Effect of learning rate and batch size}
\label{sec:appendix_lr_bs}

We show now that  ensembles are not too sensitive to the choice of
hyperparameters. We illustrate this by varying the learning rate and the batch
size, the hyperparameters that we identify as most impactful. As
Figure~\ref{fig:lr_and_bs_hyperparam} shows, many different configurations lead
to similar novelty detection performance.

\vspace{0.5cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/learning_rate_and_batch_size_effect.png}

    \caption{
        AUROCs obtained with an ensemble of WRN-28-10 models, as the initial
        learning rate and the batch size are varied.
        We used the hardest setting, CIFAR100:0-50 as ID, and
        CIFAR100:50-100 as OOD.
    }
    \label{fig:lr_and_bs_hyperparam}
\end{figure}






\vspace{-0.2cm}
\section{Additional figure showing the dependence on the unlabeled set configuration}
\label{sec:appendix_vary_ood_ratio}

The configuration of the unlabeled set (i.e.\ the size of the unlabeled set, the
ratio of OOD samples in the unlabeled set) influences the performance of our
method, as illustrated in Figure~\ref{fig:vary_target_main}. Below, we show that
the same trend persists for different data sets too, e.g.\ when we consider
CIFAR10 as ID data and SVHN as OOD data.

\vspace{0.5cm}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/heatmap_cifar10_vs_svhn_cropped_ensemble3_holdout.png}

  \label{fig:vary_target}

  \caption{ The AUROC of a 3-model ERD ensemble as the number and proportion of
  ID (CIFAR10) and OOD (SVHN) samples in the unlabeled set are varied.  }


\end{figure}




\section{Learning curves for other data sets}
\label{sec:appendix_learning_curves}

In addition to Figure~\ref{fig:training_curves}, we present in this section learning curves for
other data sets as well. The trend that persists throughout all figures is that
the arbitrary label is learned first on the unlabeled OOD data. Choosing a
stopping time before the validation accuracy starts to deteriorate prevents the
model from fitting the arbitrary label on unlabeled ID data.

\paragraph{Impact of near OOD data on training  ensembles.} The
learning curves illustrated in Figure~\ref{fig:appendix_training_curves} provide
insight into what happens when the OOD data is similar to the ID training
samples and the impact that has on training the proposed method. In particular,
notice that for CIFAR10[0-4] vs CIFAR10[5-9] in
Figure~\ref{fig:learning_curves_cifar_split}, the models require more training
epochs before reaching an accuracy on unlabeled OOD samples of 100\%. The
learning of the arbitrary label on the OOD samples is delayed by the fact that
the ID and OOD data are similar, and hence, the bias of the correctly labeled
training set has a strong effect on the predictions of the models on the OOD
inputs. Since we early stop when the validation accuracy starts deteriorating
(e.g.\ at around epoch  in Figure~\ref{fig:learning_curves_cifar_split}), we
end up using models that do not interpolate the arbitrary label on the OOD
samples. Therefore, the ensemble does not disagree on the entirety of the OOD
data in the unlabeled set, which leads to lower novelty detection performance.
Importantly, however, our empirical evaluation reveals that the drop in
performance for  ensembles is substantially smaller than what we
observe for other OOD detection methods, even on near OOD data sets.

\begin{figure*}[h]
  \centering

  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/training_curves_pretrained_svhn_cropped.png}
    \caption{ID = SVHN; OOD = CIFAR10.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/training_curves_pretrained_svhn_cropped01234.png}
    \caption{ID = SVHN[0-4]; OOD = SVHN[5-9].}
  \end{subfigure}

  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/training_curves_pretrained_cifar10.png}
    \caption{ID = CIFAR10; OOD = SVHN.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/training_curves_pretrained_cifar1001234.png}
    \caption{ID = CIFAR10[0-4]; OOD = CIFAR10[5-9].}
    \label{fig:learning_curves_cifar_split}
  \end{subfigure}

  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/training_curves_pretrained_cifar100.png}
    \caption{ID = CIFAR100; OOD = SVHN.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/training_curves_pretrained_cifar1000-50.png}
    \caption{ID = CIFAR100[0-49]; OOD = CIFAR100[50-99].}
  \end{subfigure}

  \caption{ \small{Accuracy measured while fine-tuning a model
      pretrained on  (epoch 0 indicates values obtained with the
      initial pretrained weights). The samples in  are fit first, while
      the model reaches high accuracy on  much later. We
      fine-tune for at least one epoch and then early stop when the validation
      accuracy starts decreasing.}}

  \label{fig:appendix_training_curves}
\end{figure*}




\vspace{-0.3cm}
\section{Evolution of disagreement score during fine-tuning}
\label{sec:appendix_score_curves}

In this section we illustrate how the distribution of the disagreement score
changes during fine-tuning for ID and OOD data, for a 5-model ERD ensemble.
Thus, we can further understand why the performance of the  ensembles
is impacted by near OOD data.

Figure~\ref{fig:appendix_score_curves} reveals that for far OOD data (the left
column) the disagreement scores computed on OOD samples are well separated from
the disagreement scores on ID data (note that disagreement on OOD data is so
concentrated around the maximum value of  that the boxes are essentially
reduced to a line segment). On the other hand, for near OOD data (the right
column) there is sometimes significant overlap between the disagreement scores
on ID and OOD data, which leads to the slightly lower AUROC values that we
report in Table~\ref{table:main_results}.

The figures also illustrate how the disagreement on the ID data tends to
increase as we fine-tune the ensemble for longer, as a consequence of the models
fitting the arbitrary labels on the unlabeled ID samples. Conversely, in most
instances one epoch suffices for fitting the arbitrary label on the OOD data.

We need to make one important remark: While in the figure we present
disagreement scores for the ensemble obtained after each epoch of fine-tuning,
we stress that the final  ensemble need not be selected among these. In
particular, since each model for  is early stopped separately,
potentially at a different iteration, it is likely that the  ensemble
contains models fine-tuned for a different number of iterations. Since we select
the  ensembles from a strictly larger set, the final ensemble selected
by the our proposed approach will be at least as good at distinguishing ID and
OOD data as the best ensemble depicted in
Figure~\ref{fig:appendix_score_curves}.

\begin{figure*}[t]
  \centering

  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/score_curves_svhn_cropped.png}
    \caption{ID = SVHN; OOD = CIFAR10.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/score_curves_svhn_cropped01234.png}
    \caption{ID = SVHN[0-4]; OOD = SVHN[5-9].}
  \end{subfigure}

  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/score_curves_cifar10.png}
    \caption{ID = CIFAR10; OOD = SVHN.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/score_curves_cifar1001234.png}
    \caption{ID = CIFAR10[0-4]; OOD = CIFAR10[5-9].}
  \end{subfigure}

  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/score_curves_cifar100.png}
    \caption{ID = CIFAR100; OOD = SVHN.}
  \end{subfigure}
  \hfill
  \begin{subfigure}[r]{0.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/score_curves_cifar1000-50.png}
    \caption{ID = CIFAR100[0-49]; OOD = CIFAR100[50-99].}
  \end{subfigure}

  \caption{ \small{The distribution of the disagreement score measured during
  fine-tuning on ID and OOD data (blue and orange boxes, respectively). The box
  indicates the lower and upper quartiles of the distribution, while the
  middle line represents the median and the whiskers show the extreme values.
  Notice that the distributions of the scores are easier to distinguish for far
  OOD data (left column), and tend to overlap more for near OOD settings (right
  column).}}

  \label{fig:appendix_score_curves}
\end{figure*}



















