\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}









\usepackage[final]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{makecell}
\usepackage{bbding}
\usepackage{wrapfig}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{pifont}
\usepackage{diagbox}
\usepackage[ruled]{algorithm2e}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{enumerate}

\newcommand\titlefont[1]{{\usefont{T1}{cinzeldecorativebold}{m}{n}#1}}
\newcommand\normfont[1]{{\usefont{T1}{cinzeldecorative}{m}{n}#1}}

\newcommand{\systemtitle}{\titlefont{OmniVL}}
\newcommand{\newsystem}{\normfont{OmniVL}\xspace}

\newcommand*{\system}{OmniVL\@\xspace}
\newcommand*{\eg}{\emph{e.g.}\@\xspace}
\newcommand*{\etc}{\emph{etc}\@\xspace}
\newcommand*{\ie}{\emph{i.e.}\@\xspace}
\newcommand*{\etal}{\emph{et al}\@\xspace}
\usepackage[hang,flushmargin]{footmisc}


\newcommand{\red}[1]{\textcolor{black}{#1}}
\newcommand{\draftcomment}[3]{{\color{#2}\bf [#1: #3]}}
\newcommand{\zx}[1]{\draftcomment{ZX}{cyan}{#1}}
\newcommand{\ddc}[1]{\draftcomment{DDC}{blue}{#1}}
\newcommand{\luowei}[1]{\draftcomment{Luowei}{orange}{#1}}

\definecolor{citecolor}{RGB}{0, 113, 188}

\usepackage{hyperref}
\hypersetup{breaklinks=true,colorlinks,citecolor=citecolor}
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\title{{\system}: One Foundation Model for Image-Language and Video-Language Tasks}


\author{Junke Wang,~Dongdong Chen,~Zuxuan Wu,~Chong Luo,~Luowei Zhou, \\ \textbf{Yucheng Zhao,~Yujia Xie,~Ce Liu,~Yu-Gang Jiang,~Lu Yuan}\\
\\
Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University \\
Shanghai Collaborative Innovation Center on Intelligent Visual Computing \\
Microsoft Cloud + AI, Microsoft Research Asia 
} 

\begin{document}
\maketitle

\begin{abstract}
This paper presents \system, a new foundation model to support both image-language and video-language tasks using one universal architecture. It adopts a unified transformer-based visual encoder for both image and video inputs, and thus can perform joint image-language and video-language pretraining. We demonstrate, for the first time, such a paradigm benefits both image and video tasks, as opposed to the conventional one-directional transfer (\eg, use image-language to help video-language). To this end, we propose a \emph{decoupled} joint pretraining of image-language and video-language to effectively decompose the vision-language modeling into spatial and temporal dimensions and obtain performance boost on both image and video tasks. Moreover, we introduce a novel unified vision-language contrastive (UniVLC) loss to leverage image-text, video-text, image-label (\eg, image classification), video-label (\eg, video action recognition) data together, so that both supervised and noisily supervised pretraining data are utilized as much as possible. Without incurring extra task-specific adaptors, \system can simultaneously support visual only tasks (\eg, image classification, video action recognition), cross-modal alignment tasks (\eg, image/video-text retrieval), and multi-modal understanding and generation tasks (\eg, image/video question answering, captioning). We evaluate \system on a wide range of downstream tasks and achieve state-of-the-art or competitive results with similar model size and data scale. \blfootnote{*Work done during an internship at Microsoft, Corresponding authors.}
\end{abstract}

\section{Introduction}

Vision-language pretraining has been demonstrated to be a promising direction for building foundation models that can support a broad range of downstream AI tasks. By pretraining on web-scale noisy image-text data, the pioneering works~\cite{radford2021learning,hu2021scaling,yuan2021florence} suggest a unified model can be equipped with unprecedented capabilities (\eg, zero-shot classification) and achieve outstanding performance on various tasks, thus significantly reducing the cost of designing task-specific models. Following this thread, some works~\cite{singh2021flava,wang2022OFA,alayrac2022flamingo,li2022blip,zhu2021uni} are further proposed to support more tasks. There are also some efforts~\cite{fu2021violet,zellers2021merlot} studying video-language pretraining to solve video-related multi-modal tasks.  


\begin{table}[t]
  \caption{A system-level comparison between \system and existing Vision-Languange pretraining and foundation models. ``IL'',``VL'' denotes image-language pretraining and video-language pretraining, ``Non-Gen'' denotes non-generative tasks (\eg, visual only classification, cross-modal alignment), while ``Gen'' denotes multi-modal generation tasks (\eg, image/video question answering,captioning). ``I-L,V-L'' and ``I-T,V-T'' denote image/video-label and image/video-text data respectively. }
  \label{tab:com}
  \centering
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l|cc|cc|cccc@{}}
    \toprule
    \multirow{2}*{\textbf{Method}} & \multicolumn{2}{c|}{\small\textbf{Modality Unification}} & \multicolumn{2}{c|}{\small\textbf{Functionality Unification}} &
    \multicolumn{4}{c}{\small\textbf{Data Unification}} \\
    ~ &  ILP & VLP & Non-Gen & Gen & I-T & I-L & V-T & V-L \\
    \midrule
    CLIP~\cite{radford2021learning} & \Checkmark &  & \Checkmark &  & \Checkmark &  &  &  \\
    ALIGN~\cite{jia2021scaling} & \Checkmark &  & \Checkmark &  & \Checkmark &  &  & \\
    VLMO~\cite{wang2021vlmo} & \Checkmark &  & \Checkmark &  & \Checkmark &  &  & \\
    ALBEF~\cite{li2021align} & \Checkmark &  & \Checkmark &  & \Checkmark &  &  & \\
    SIMVLM~\cite{wang2022simvlm} & \Checkmark &  & \Checkmark & \Checkmark & \Checkmark &  &  &  \\
    UniVLP~\cite{zhou2020unified} & \Checkmark &  & \Checkmark & \Checkmark & \Checkmark &  &  &  \\
    BLIP~\cite{li2022blip} & \Checkmark &  & \Checkmark & \Checkmark & \Checkmark &  &  &  \\
    FiT~\cite{bain2021frozen} & & \Checkmark & \Checkmark & &  \Checkmark & & \Checkmark &  \\
    ALPRO~\cite{li2021prompt} & & \Checkmark & \Checkmark & &  \Checkmark & & \Checkmark &  \\
    VIOLET~\cite{fu2021violet} &  & \Checkmark  & \Checkmark &  & \Checkmark &  & \Checkmark &  \\
    FLAVA~\cite{singh2021flava} & \Checkmark &  & \Checkmark &  & \Checkmark &  &  &  \\
    Florence~\cite{yuan2021florence} & \Checkmark &  & \Checkmark &  & \Checkmark & \Checkmark &  &  \\
    \system (Ours) & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark & \Checkmark \\
    \bottomrule
  \end{tabular*}
\end{table}

In this paper, we take a step forward and aim to design an omni-vision-language foundation model \system, to support both image-language and video-language pretraining and corresponding downstream tasks\footnote{\noindent Here we regard models like~\cite{yuan2021florence,yu2022coca} as image-language only, as they only pretrain on image-language and naively regard video as independent frames without temporal modeling or need heavy adaption to video.}, including visual only tasks (\eg, image classification, video action recognition), cross-modal alignment tasks (\eg, image/video-text retrieval), and multi-modal understanding and generation tasks (\eg, image/video question answering, captioning) simultaneously. To the best of our knowledge, it is the first time to demonstrate that one model can benefit both image and video tasks \emph{bidirectionally}, as opposed to conventional single directional way, \ie, using image (/image-language) to help video(/video-language).

To support both image and video inputs, \system adopts a unified transformer-based visual encoder to extract visual representations, where video inputs share most transformer layers with images except for the 3D patch tokenizer and temporal attention blocks~\cite{gberta_2021_ICML}. Similar to existing vision-language models, \system has another text encoder to extract language representations. To support multiple tasks learning within the same architecture, \system follows an encoder-decoder structure with two visual-grounded decoders. One decoder is designed with bidirectional attention for visual-text semantic alignment, while the other is equipped with causal attention for text generation. We pretrain \system with image-language and video-language data in a \emph{decoupled} joint way, which is different from existing works~\cite{li2022blip,yu2022coca,yuan2021florence,radford2021learning,zellers2021merlot} that apply image-language only pretraining, video-language only pretraining or their joint pretraining from scratch. \red{More specifically, we first pretrain on image-language to focus on spatial representation learning, and then do joint pretraining with video-language together to learn the temporal dynamics incrementally while preserving/polishing the well-learned spatial representations. We believe this not only makes the learning more efficient from spatial to temporal dimensions, but also enforces the learning complementary to each other.} This bidirectional help has not been unraveled in prior works, and is important in pushing one foundation model to boost the performance on both image and video tasks.


Moreover, \system is motivated by the unified contrastive learning~\cite{yang2022unified} used in Florence~\cite{yuan2021florence}, and extends its scope to cover video-text and video-label (\eg, video action recognition) data. The underlying consideration lies in two aspects: 1) As mentioned above, we aim to leverage as much supervised (or noisily supervised) pretraining corpus as possible; 2) As shown in~\cite{yang2022unified},  human-annotated visual-label data (\eg, ImageNet~\cite{deng2009imagenet}) can help to derive more discriminative representations and benefit transfer learning tasks (\eg, image classification), while webly-crawled vision-language data cover broader visual concepts and benefit cross-modal and multi-modal tasks. This simple extension facilitates us to enjoy both advantages.

We call our foundation model OmniVL, since it unifies in three dimensions: modality (\ie, image-language and video-language pretrainings), functionality (\ie, non-generative and generative tasks), and data unification (\ie, image-text, video-text, image-label and video-label data) as demonstrated in Table~\ref{tab:com}. With the similar model size and data scale, \system achieves new state-of-the-art or at least competitive results on a wide scope of downstream tasks. For example, when using ViT-Base scale model to pretrain on a moderate data scale (\eg,  14M image-text, 2.5M video-text), we achieve state-of-the-art performance on image-text retrieval (82.1/64.8 R@1
on COCO for image-to-text/text-to-image), image captioning (39.8 BLEU@4 on COCO), text-to-video retrieval (47.8 R@1 on MSRVTT), and video question answering (51.9\% accuracy on MSVD).

\section{Related Work}
\noindent \textbf{Vision-Only Pretraining.} Large-scale pretraining plays a key role in the success of deep neural networks recently. In the computer vision field, supervised pretraining~\cite{he2016deep,he2019rethinking,kolesnikov2020big,dong2022cswin} on ImageNet~\cite{deng2009imagenet} is the most classical setting. Recently, BiT~\cite{kolesnikov2020big} shows that supervised pretraining on larger-scale datasets with larger models offers better transfer ability. In parallel, self-supervised pretraining has also been extensively studied in the literature, and dominant methods include contrastive learning~\cite{chen2020simple,li2021improve,he2020momentum} approaches or BERT-pretraining strategies~\cite{dong2021peco,bao2021beit,wang2022bevt,dong2022bootstrapped}. Despite their great success, they focus on unimodal pretraining and fail to support cross-modal or multi-modal tasks.

\noindent \textbf{Vision-Language Pretraining.} Vision-Language pretraining (VLP)~\cite{lu2019vilbert,tan2019lxmert,sun2019videobert,chen2020uniter,Su2020VL-BERT,radford2021learning,jia2021scaling,jia2021exploring} has attracted surging attention in the vision-language community, which aims to learn generic multi-modal representations to solve various tasks, \eg, image captioning, image-text retrieval, and video question answering. Depending on the modality of the input data and targeted downstream tasks, existing VLP approaches can be roughly divided into two categories: image-language pretraining methods~\cite{chen2020uniter,li2020oscar,zhou2020unified,wang2022simvlm} which learn a joint distribution over visual and linguistic representations from image-text pairs, and video-language methods~\cite{li2020hero,lei2021less,li2021prompt,fu2021violet,bain2021frozen,miech2020end,alayrac2020self,akbari2021vatt,patrick2020support} which model the semantic associations between video frames and texts from video-text pairs. Among them, some recent works~\cite{bain2021frozen,fu2021violet} also explore image-language and video-language joint pretraining to improve video-language tasks. Instead,  \system aims to integrate image-language and video-language within one foundation model. Moreover, inspired by the observation that decoupling spatial and temporal learning is better than direct joint spatial-temporal learning~\cite{wang2022bevt,zhang2021cotraining}, we introduce a decoupled joint pretraining paradigm, which first learns spatial visual representations with image-language and then conducts joint pretraining. With such a design, we demonstrate for the first time that they can help each other in a bidirectional way. Moreover, as a foundation model, we enable more unification in terms of functionality and pretraining corpus.   

\noindent \textbf{Vision Foundation Models.} Automating the understanding of our multi-modal world with machines requires the development of foundation models that work across different modalities and domains~\cite{bommasani2021opportunities,lu2021pretrained}. CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} are typically regarded as the pioneering explorations of foundation models. By pretraining on web-scale noisy image-text pair data, they excel at cross-modal alignment and zero-shot classification tasks. Florence~\cite{yuan2021florence} further extends the scope of foundation models to cover Space-Time-Modality space and performs better especially on vision-only tasks with unified contrastive learning. Despite their success, all the above approaches do not naturally support multi-modal generation tasks (\eg, visual question answering and captioning). To address this limitation, some recent works like FLAVA~\cite{singh2021flava}, BLIP~\cite{li2022blip} and CoCa~\cite{yu2022coca}  design one image-language foundation model to support both cross-modal alignment tasks and multi-modal generation tasks. While such image-language foundation models can be extended to support video-language tasks in the fine-tuning stage, they either need heavy task-specific adaptors or simply treat video as independent frames. In contrast, \system is designed to support both image-language and video-language starting from the pretraining stage without any extra adaptors.

\section{Methodology}


\subsection{Overall Framework}
\label{sec:arch}
The overall framework of \system is illustrated in Figure~\ref{fig:networks}, which follows an encoder-decoder like structure. \system consists of a unified visual encoder to extract the representations for both images and videos, a text encoder to obtain text representations, and two visual-grounded decoders for semantic alignment and open-ended text generation, respectively. Below we briefly introduce each component and leave the detailed structure in the supplementary material. 

\noindent \textbf{Unified Visual Encoder.} We unify images and videos in a transformer-based visual encoder by converting both of them into a series of tokens, where the independent 2D/3D convolution-based patch tokenizers are used for image/video respectively. Accordingly, spatial and temporal positional encodings are added to the input tokens to incorporate positional information. For the transformer structure, we follow TimeSformer~\cite{gberta_2021_ICML} to employ decoupled spatial-temporal attention, which individually models the static spatial appearance and temporal dynamics in visual data. Specifically, within each transformer block, we sequentially perform temporal self-attention and spatial self-attention. The temporal self-attention blocks will be automatically skipped for the image inputs. The final visual representation  is obtained from the [\texttt{CLS}] token of the last block. Note that we share the model weights for image and video inputs except for the temporal self-attention. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{./network.pdf}
\caption{An overview of \system. We unify the pretraining corpus (human-annotated data and webly-crawled data), modality (image, video, and language), and functionality (multi-modal understanding and generation tasks, visual classification tasks) in one universal framework.}
  \label{fig:networks}
\end{figure*}

\noindent \textbf{Text Encoder.} We adopt BERT~\cite{devlin2018bert} as the Text Encoder, which transforms input text into a sequence of token embeddings. The embedding of [\texttt{CLS}] token  is used as the language representation.

\noindent \textbf{Visual-grounded Alignment Decoder.} Even though the above unimodal encoders can support cross-modal alignment like CLIP~\cite{radford2021learning}, we employ an extra visual-grounded alignment decoder to further facilitate the learning and enhance the alignment accuracy like~\cite{li2022blip,fu2021violet}. It takes the text and output visual features from the unified visual encoder as input, and fuses the information of both modalities with stacked transformer blocks. Each block basically contains a self-attention layer, a cross-attention layer and a feed-forward layer. Additionally, a task-specific [\texttt{ENC}] token is added to the input text, the output embedding of which will be used as the fused cross-modal representation.


\noindent \textbf{Visual-grounded Generation Decoder.} We empower our model to own the multi-modal generation capability by attaching a visual-grounded text generation decoder. It adopts the similar architecture to the above alignment decoder, but replaces the bidirectional self-attention with causal self-attention. A [\texttt{DEC}] token and an [\texttt{EOS}] token are added to indicate the task type and signal the end, separately. 


\subsection{Pre-training Objectives}
\label{sec:obj}
We jointly optimize \system with the following three objectives:

\noindent \textbf{Unified Vision-Language Contrastive (UniVLC) Loss.} UniCL~\cite{yang2022unified} introduces a novel paradigm for visual representation learning by unifying the supervised learning from image-label data and contrastive learning from the natural language supervision. In this paper, we extend its scope to the unified visual domain, which incorporates both image and video data for cross-modal pretraining via a joint visual-label-text space. 

More specifically, we define manually-annotated image/video-label data and web-crawled image/video-text data in a triplet format , where  is the image/video data,  is the unique label indicating the index of the grouped language description in the whole pretrain dataset, and  is its corresponding language description. For image/video-label data,  is generated with the same prompt strategy used in CLIP~\cite{radford2021learning} and ActionCLIP~\cite{wang2021actionclip}(\ie, filling the class names into the prompt templates). Note that in this joint visual-label-text space, visual data from manually-annotated dataset belonging to the same category shares the common textual description.

Based on this, given the visual embedding of image/video  and the language embedding of its text  in a batch , we follow CLIP to apply a linear projection and normalization layer on them to obtain the latent visual vector  and text vector . To enjoy a large batch size for contrastive learning, we maintain three memory banks as~\cite{he2020momentum,li2021align} to store the most recent  visual vectors  and text vectors  from the momentum encoders, and the corresponding labels . Then we calculate the vision-to-text and text-to-vision contrastive loss as:

where , and  is a learnable temperature parameter. Finally, the unified vision-language contrastive loss is defined as:

where  and  denote the parameters of the unified visual encoder and text encoder.

\noindent \textbf{Vision-Language Matching (VLM) Loss.} VLM loss encourages the model to learn aligned visual and text representations. Specifically, we randomly replace the text  for  with
the text  from a different image/video in the same batch , and input them to the unified visual encoder and visual-grounded alignment decoder, respectively. Then a linear layer is applied to the output of visual-grounded alignment decoder to produce a two-category probability , which measures whether the input pair is matched. Finally, we optimize the parameters of the unified visual encoder  and the parameters of visual-grounded alignment decoder  with VLM loss:


where  = 1 if  and , otherwise  = 0. 

\vspace{0.05in}
\noindent \textbf{Language Modeling (LM) Loss.} Previous works indicate that LM facilitates the model to develop better text-induced generalization ability~\cite{wang2022simvlm}. Therefore, we optimize the output of visual-grounded generation decoder with a cross-entropy loss, which directly maximizes the likelihood of the input text sequence in an autoregressive manner:

where  is the length of the input text,  and  represent the parameters of the unified visual encoder and visual-grounded generation decoder.

Combining Eqn.~\ref{eq:univlc}-Eqn.~\ref{eq:lm}, the overall objectives can be summarized as:

where , , and  are weighting hyper-parameters and all set as 1 by default.

\subsection{Pretraining Corpus and Paradigms}
\label{sec:data}
\noindent \textbf{Corpus.} As mentioned before, our pretraining corpus includes both visual-text data and visual-label data, benefiting from the unified visual-label-text space. For the image-text data, we adopt the same pre-training dataset as~\cite{li2021align,li2022blip} with 14M images in total by default, including two human-annotated datasets
(COCO~\cite{lin2014microsoft} and Visual Genome~\cite{krishna2017visual}), and three web datasets (CC3M~\cite{sharma2018conceptual}, CC12M~\cite{changpinyo2021conceptual}, and SBU captions~\cite{ordonez2011im2text}). For the video-text data, we use WebVid~\cite{bain2021frozen} which contains 2.5M videos from the web. The visual-label datasets that we adopt includes the image dataset ImageNet-1K~\cite{deng2009imagenet} and video dataset Kinetics-400~\cite{kay2017kinetics}. As some baseline image-language methods only use 4M image-text data by excluding CC12M, we also provide the corresponding results in the experiment part. 

\textbf{Paradigms.} In contrast to some existing methods~\cite{bain2021frozen,fu2021violet} that conduct joint pretraining on image data and video data from scratch, we adopt a \emph{decoupled} joint pretraining paradigm instead. Specifically, we first pretrain our model on image-label-text data, and then perform joint training on both image-label-text data and video-label-text data. In this way, we decouple the multi-modal modeling into spatial and temporal dimensions. Such a design has two potential benefits: 1) Considering the expensive computational cost of video pretraining, applying the image data to learn the spatial representation first is more efficient. 2) The decoupled pattern makes the multimodal representation learning more effective, which is the key to make image-language and video-language benefit each other. 

\section{Experiments}
\noindent\textbf{Implementation Details.} 
By default, we use the TimeSformer base model and BERT base model for visual encoder and text encoder, respectively. As mentioned in Sec~\ref{sec:data}, our pretraining follows a \emph{decoupled} paradigm.For the image-language pretraining stage, we initialize spatial attention with ViT-B/16~\cite{dosovitskiy2020image} pretrained on ImageNet-1K~\cite{deng2009imagenet}. We take
random image crops of resolution 224  224 as inputs and apply RandAugment~\cite{cubuk2020randaugment}. The model is pretrained for 20 epochs using a batch size of 2880. For the joint pretraining, we sparsely sample 8  224  224 video clips, and train the model for 10 epochs with a batch size of 800 for video data and 2880 for image data. Our joint
pretraining alternates batches between the
image and video data. The model is optimized with AdamW~\cite{loshchilov2017decoupled} using a weight decay of 0.05. The learning rate is warmed-up to 3e-4 (image) / 8e-5 (joint) and decayed linearly with a rate of 0.85. During downstream fine-tuning, we increase the image resolution to 384  384 for both image-text and video-text tasks~\cite{li2021align,li2022blip}, unless otherwise specified. We randomly sample 8 frames per video for retrieval and 16 for QA, following~\cite{li2021prompt}. Temporal position embeddings in the spatial-temporal visual encoder are interpolated to accommodate the inputs of different lengths. Besides, we use task-specific learning rates and training epochs due to various data scales and domains, the details of which will be further illustrated in the supplementary material. 

\begin{table}[t]
  \caption{Linear probing evaluation on 6 image classification datasets.}
  \label{tab:linear}
  \small
  \centering
\setlength{\tabcolsep}{4pt} 
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc|ccccccc@{}}
    \toprule
    \textbf{Method} & \textbf{Img-text pairs} & Food101 & CIFAR10 & CIFAR100 &
    Pets & DTD & Flowers & Avg \\
    \midrule
    METER-CLIP-B/16~\cite{dou2021empirical} & 400M+4M & 79.2 &	91.8 & 70.3 & 40.4 & 62.2 & 67.1 & 68.5\\
    ALBEF~\cite{li2021align} & 14M & 84.0 & 95.6 & 80.8 & 68.4 & 73.4 & 86.5 & 81.4\\
    BLIP~\cite{li2022blip} & 14M & 85.5	& 95.2 & 80.0 & 65.3 & 74.6 & 88.4 & 81.5\\
    FLAVA~\cite{singh2021flava} & 14M & 85.2 & 90.4 & 76.2 & 82.3 & 74.2 & 92.7 & 83.5 \\
    FLAVA~\cite{singh2021flava} & 70M & 88.5 & 92.9 & 77.7 & 84.8 & 77.3 & \textbf{96.4} & 86.3 \\
    CLIP-ViT-B/16 & 400M & \textbf{92.8} & 96.2 & 83.1 & 86.7 & \textbf{79.2} & 93.1 & \textbf{88.5} \\
\system & 14M & 87.4 & \textbf{96.2} & \textbf{83.2} & \textbf{87.1} & 76.5 & 89.8 & 86.7\\
    \bottomrule
  \end{tabular*}
\end{table}



\subsection{Visual Only Tasks}
\label{sec:visual}
We first evaluate the representations of the visual encoder on visual only tasks. Here the classical image classification and video action recognition tasks are adopted for benchmarking. Note that, in the following tables, \textbf{\textit{we use the superscript "*" to denote extra video data is used}}.

\noindent \textbf{Image Classification.}
Linear probing is a commonly used method to evaluate the representation quality~\cite{radford2021learning,yuan2021florence,singh2021flava}. Following the implementation of CLIP~\cite{radford2021learning}, we freeze the visual encoder and fine-tune the newly appended linear layers for linear probing. We evaluate our method on 6 image classification datasets, which are not included in our pretraining set. We compare with METER~\cite{dou2021empirical}, ALBEF~\cite{li2021align}, BLIP~\cite{li2022blip}, FLAVA~\cite{singh2021flava}, and CLIP~\cite{radford2021learning} in Table~\ref{tab:linear}. Note that for fair comparisons with FLAVA~\cite{singh2021flava}, we pretrain their model (we adopt the implementation in torchmultimodal\footnote{https://github.com/facebookresearch/multimodal.}) on the 14M image-text data. Compared to METER, ALBEF, BLIP, and FLAVA, \system offers consistently better results. Compared to CLIP and FLAVA, even though we use far less pre-training data, our method still achieves overall comparable results. 

\begin{table}[t]
  \caption{Comparison with baseline methods on video action recognition datasets Kinetics-400 and Something-Something v2 under fine-tuning settings, and UCF101 and HMDB51 under linear probing settings. ``Sup21K'' denotes supervised pretraining on ImageNet-21 dataset.}
  \label{tab:video}
\centering
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l | cc | cccc@{}}
    \toprule
    \multirow{2}*{\textbf{Method}} & \textbf{UCF101} & \textbf{HMDB51} & \multicolumn{2}{c}{\textbf{K400}} & \multicolumn{2}{c}{\textbf{SSV2}} \\
    ~ & Top-1 & Top-1 & Top-1 & Top-5 & Top-1 & Top-5  \\
    \midrule
    TimeSformer~\cite{gberta_2021_ICML}-Sup21K & 82.9 & 60.1 & 78.0 & 93.7 & 59.5 & -  \\
    FiT~\cite{bain2021frozen} & 89.6 & 68.8 & 78.5 & 94.1 & 61.6 & 85.7 \\
    \system & \textbf{93.2} & \textbf{70.0} &  \textbf{79.1} &  \textbf{94.5} &  \textbf{62.5} &  \textbf{86.2} \\
    \bottomrule
  \end{tabular*}
\end{table}

\noindent \textbf{Video Action Recognition.} Video action recognition is one of the most representative tasks for video understanding~\cite{zhu2020comprehensive}. We first report the linear probing results on UCF101 and HMDB51. The results are summarized in the first two columns of Table~\ref{tab:video}. We see that \system achieves excellent performance, \ie, 93.2\% on UCF101, even without end-to-end training, which beats baseline methods by a large margin. Furthermore, we conduct fine-tuning experiments on Kinetics-400~\cite{kay2017kinetics} and Something-Something v2~\cite{goyal2017something} dataset. We compare our method with supervised pretrained TimeSformer~\cite{gberta_2021_ICML} and FiT~\cite{bain2021frozen} in Table~\ref{tab:video} since we share the same model architecture. The same training settings with TimeSformer are adopted for fair comparisons. \system outperforms TimeSformer by 1.4\% and 5.0\% on Kinetics-400 and Something-Something v2 in terms of Top-1 accuracy, respectively. Compared to the video-language model FiT~\cite{bain2021frozen} (\ie, performing joint pretraining from scratch), our results are also overall better, highlighting the advantage of our method.

\begin{table}[t]
  \caption{Fine-tuned image-text retrieval results on Flickr30K and COCO datasets. We report text recall@1 / recall@5 / recall@10, and image recall@1 / recall@5 / recall@10.}
  \label{tab:imgret}
  \small
  \centering
\setlength\tabcolsep{3pt}
\resizebox{\columnwidth}{!}{
  \begin{tabular}{lc | cccccc | cccccc}
    \toprule
    \multirow{2}*{\textbf{Method}} & \multirow{2}*{ \textbf{\makecell[c]{\# Img-Text \\ Pairs}}} & \multicolumn{6}{c|}{\textbf{COCO (5K test set)}}  & \multicolumn{6}{c}{\textbf{Flickr30K (1K test set)}}  \\
    ~ & ~ & \multicolumn{3}{c}{TR} & \multicolumn{3}{c|}{IR} & \multicolumn{3}{c}{TR} & \multicolumn{3}{c}{IR} \\
\midrule
\red{VirTex}~\cite{miech2021thinking} & - & - & - & - &  38.1 & 62.8 & - & - & - & - & 35.1 & 64.6 & - \\
    UNITER~\cite{chen2020uniter} & 4M & 65.7 & 88.6 & 93.8 & 52.9 & 79.9 & 88.0 & 87.3 & 98.0 & 99.2 & 75.6 & 94.1 & 96.8 \\
    OSCAR~\cite{li2020oscar} & 4M & 70.0 & 91.1 & 95.5 & 54.0 & 80.8 & 88.5 & -  & - & - & - & - & - \\
    UNIMO~\cite{li2020unimo} & 4M & - & - &- & - & - & - & 89.4 & 98.9 & 99.8 & 78.0  & 94.2 & 97.1 \\
    VLMO~\cite{wang2021vlmo} & 4M & 74.8 & 93.1 & 96.9 & 57.2 & 82.6 & \textbf{89.8} & 92.3 & 99.4 & 99.9 & 79.3 & 95.7 & 97.8 \\
    \system & 4M & \textbf{76.8} & \textbf{93.6} & \textbf{97.3} & \textbf{58.5} & \textbf{82.6} & 89.5 & \textbf{94.9} & \textbf{99.6} & \textbf{99.9} & \textbf{83.4} & \textbf{97.0} & \textbf{98.6} \\
    \midrule
    \red{FLAVA}~\cite{singh2021flava} & 70M & 61.5 & 82.1 & 89.6 & 50.1 & 74.4 & 83.2 &  85.4 & 95.7 & 98.3 & 73.2 & 92.7 & 95.5 \\
    METER~\cite{dou2021empirical} & 404M & 76.2 & 93.2 & 96.8 & 57.1 & 82.7 & 90.1 &  94.3 & 99.6 & 99.9 & 82.2 & 96.3 & 98.4 \\
    ALIGN~\cite{jia2021scaling} & 1.8B & 77.0 & 93.5 & 96.9 & 59.9 & 83.3 & 89.8 & 95.3 & 99.8 & 100.0 & 84.9 & 97.4 & 98.6 \\
    ALBEF~\cite{li2021align} & 14M & 77.6 & 94.3 & 97.2 & 60.7 & 84.3 & 90.5 & 95.9 & 99.8 & 100.0 & 85.6 & 97.5 & 98.9 \\
    BLIP~\cite{li2022blip} & 14M & 80.6 & 95.2 & 97.6 & 63.1 & 85.3 & 91.1 & 96.6 & 99.8 & 100.0 & 87.2 & 97.5 & 98.8 \\
    Florence~\cite{yuan2021florence} & 900M & 81.8 & 95.2 & - & 63.2 & 85.7 & - & 97.2 & 99.9 & - & 87.9 & \textbf{98.1} & - \\ 
    \system & 14M & \textbf{82.1} & \textbf{95.9} & \textbf{98.1}	& \textbf{64.8} & \textbf{86.1} & \textbf{91.6} & \textbf{97.3} & \textbf{99.9} & \textbf{100.0} & \textbf{87.9} & 97.8 & \textbf{99.1} \\
    \bottomrule
  \end{tabular}}
\end{table}

\begin{table}[t]
\caption{Comparison with SOTA text-to-video-retrieval methods on  MSRVTT and DiDeMo with fine-tune (left) and zero-shot (right) evaluation. R@1 / R@5 / R@10 are reported.}
\label{tab:vidret}
\small
\centering
\renewcommand\arraystretch{1.1}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{l | cccccc | cccccc}
    \toprule
    \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{Text-to-Video Retrieval}}  & \multicolumn{6}{c}{\textbf{Zero-shot Retrieval}}  \\
    ~ &  \multicolumn{3}{c}{MSRVTT} & \multicolumn{3}{c|}{DiDeMo} & \multicolumn{3}{c}{MSRVTT} & \multicolumn{3}{c}{DiDeMo} \\
    \midrule
    ClipBERT~\cite{lei2021less} & 22.0 & 46.8 & 59.9 & 20.4 & 48.0 & 60.8 & - & - & - & - & - & -\\
    \red{TT-CE+}~\cite{croitoru2021teachtext} & 29.6 & 61.6 & 74.2 & 21.6 & 48.6 & 62.9 & - & - & - & - & - & - \\
    VideoCLIP~\cite{xu2021videoclip} & 30.9 & 55.4 & 66.8 & - & - & - & 10.4 & 22.2 & 30.0 & 16.6 & 46.9 & - \\
    FiT~\cite{bain2021frozen} & 32.5 & 61.5 & 71.2 & 31.0 & 59.8 & 72.4 & 18.7 & 39.5 & 51.6 & 21.1 & 46.0 & 56.2 \\
    \red{TT-CE+ (+QB-NORM)}~\cite{bogolin2022cross} & 33.3 & 63.7 & 76.3 & 24.2 & 50.8 & 64.4 & - & - & - & - & - & - \\
    ALPRO~\cite{li2021prompt} & 33.9 & 60.7 & 73.2 & 35.9 & 67.5 & 78.8 & 24.1 & 44.7 & 55.4 & 23.8 & 47.3 & 57.9 \\
    VIOLET~\cite{fu2021violet} & 34.5 & 63.0 & 73.4 & 32.6 & 62.8 & 74.7 & 25.9 & 49.5 & 59.7 & 23.5 & 49.8 & 59.8  \\
    \system & \textbf{47.8} & \textbf{74.2} & \textbf{83.8} & \textbf{52.4} & \textbf{79.5} & \textbf{85.4} & \textbf{34.6} &  \textbf{58.4} & \textbf{66.6} & \textbf{33.3} & \textbf{58.7} & \textbf{68.5}  \\
    \bottomrule
  \end{tabular}}
\end{table}


\subsection{Cross-modal Alignment Tasks}
\label{sec:alignment} 
Using visual and text embeddings generated from the unimodal encoders, \system can easily handle cross-modal alignment tasks, \eg, image-text retrieval and text-to-video retrieval. In addition, in order to balance the inference efficiency and the deep fusion of multi-modal information, we follow~\cite{li2022blip,li2021align} to first select Top-K ( by default) candidates based on the vision-language similarity scores, which are further re-ranked by calculating their pairwise VLM scores. The pretrained model is fine-tuned with UniVLC loss and VLM loss. 

\noindent \textbf{Image-Text Retrieval.} We first evaluate \system on COCO~\cite{lin2014microsoft} and Flickr30K~\cite{plummer2015flickr30k} for both image-to-text retrieval and text-to-image retrieval. As shown in Table~\ref{tab:imgret}, \system outperforms other methods by clear margins. With 14M image-text pairs for pretraining, our model surpasses BLIP by 1.8\% and 0.8\% in terms of average recall@1 on COCO and Flickr30K, respectively, which is even competitive compared with Florence that is trained with much larger scale data and model . 



\begin{table*}[!t]
  \caption{Comparison with state-of-the-art image captioning methods on NoCaps and COCO Caption. C: CIDEr, S: SPICE, B@4: BLEU@4.}
  \label{tab:imgcap}
  \small
  \vspace{0.02in}
  \setlength{\tabcolsep}{3pt} 
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lc|cccccccc|cc@{}}
    \toprule
    \multirow{3}*{\textbf{Method}} & \multirow{3}*{ \textbf{\makecell[c]{\# Img-Text \\ Pairs}}} & \multicolumn{8}{c|}{\textbf{NoCaps}} & \multicolumn{2}{c}{\textbf{COCO Caption}} \\
    ~ & ~ & \multicolumn{2}{c}{in-domain} & \multicolumn{2}{c}{near-domain} & \multicolumn{2}{c}{out-domain} & \multicolumn{2}{c|}{overall} & \multicolumn{2}{c}{\textbf{Karpathy test}} \\
    ~ & ~ & C & S & C & S & C & S & C & S & B@4 & C \\
    \midrule
     Enc-Dec~\cite{changpinyo2021conceptual} & 15M & 92.6 & 12.5 & 88.3 & 12.1 & 94.5 & 11.9 & 90.2 & 12.1 & - & 110.9 \\
     VinVL~\cite{vinvl} & 5.7M & 103.1 & 14.2 & 96.1 & 13.8 & 88.3 & 12.1 & 95.5 & 13.5 & 38.2 & 129.3 \\
     LEMON~\cite{hu2021scaling} & 12M & 104.5 & 14.6 & 100.7 & 14.0 & 96.7 & 12.4 & 100.4 & 13.8 & - & - \\
     BLIP~\cite{li2022blip} & 14M & \textbf{111.3} & \textbf{15.1} & 104.5 & 14.4 & 102.4 & 13.7 & 105.1 & 14.4 & 38.6 & 129.7 \\
    
     \red{SIMVLM}~\cite{wang2022simvlm} & 1.8B & - & - & - & - & - & - & 94.8 & 13.1 &  39.0 & 134.8 \\
      \red{OFA}~\cite{wang2022OFA} & 14M & - & - & - & - & - & - & - & - &  38.7 & 130.5 \\
     \red{OFA}~\cite{wang2022OFA} & 21.4M & - & - & - & - & - & - & - & - &  \textbf{41.0} & \textbf{138.2} \\
     \system & 14M & 104.6 & 15.0 & \textbf{108.3} & \textbf{14.9} & \textbf{106.3} & \textbf{14.2} & \textbf{107.5} & \textbf{14.7} & 39.8 & 133.9 \\
    \bottomrule
\end{tabular*}
\end{table*}


\noindent \textbf{Text-to-Video Retrieval.} We compare \system with other methods on MSRVTT~\cite{xu2016msr} and DiDeMo~\cite{anne2017localizing} for text-to-video retrieval under both fine-tuning and zero-shot transfer settings in Table~\ref{tab:vidret}. Note that directly using the pretrained models, \system achieves 42.0\% and 40.6\% on MSRVTT and DiDeMo in terms of recall@1, respectively, significantly surpassing existing methods. The performance of \system is further improved under the fine-tuning settings. The results suggest the multi-modal representations learned by our method are very discriminative.

\subsection{Multi-modal Understanding and Generation Tasks}
\label{sec:und_gen}
The visual-grounded generation decoder equips \system with the capability for multi-modal understanding and generation so as to reason and describe. In this part, we fine-tune our model with the LM loss, and evaluate its performance on captioning and image/video question answering tasks.


\noindent \textbf{Image Captioning.} Image captioning requires the model to generate a textual description for a given image. We fine-tune our model on COCO and then evaluate on both COCO~\cite{lin2014microsoft} NoCaps~\cite{agrawal2019nocaps}. Following~\cite{wang2022simvlm,li2022blip}, we adopt a prefix prompt ``a picture of'' to guide the caption generation. The results are presented in Table~\ref{tab:imgcap}, from which we can see that \system achieves superior results on both datasets, \eg, 107.5 and 133.9 on NoCaps and COCO in terms of CIDEr. \red{Although SimVLM~\cite{wang2022simvlm} adopt much larger pretraining data than ours, \system still achieves comparable or even better results on some metrics, \eg, CIDEr, on COCO dataset. For fair comparison with OFA~\cite{wang2022OFA}, we pre-train their model on the 14M image-text data (with only image-text matching and image captioning objectives, denoted as OFA), the results demonstrate that using the same amount of pre-training data, \system performs better than OFA measured by all the metrics. Note that we don't show the results of SIMVLM on 14M data since they haven't released their code.}

\begin{table*}[!ht]
\centering
  \caption{Comparison with SOTA methods on Youcook2 dataset for video captioning. The results in gray denote using pretrained backbones to extract video-level features. }
  \label{tab:vid_cap}
    \renewcommand{\arraystretch}{1.1}
    \vspace{0.05in}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l |ccccc@{}}
    \toprule
    \textbf{Method} &  B@3 & B@4 & METEOR & ROUGE-L & CIDEr \\
    \midrule
    Bi-LSTM~\cite{zhou2018towards} & - & 0.87 & 8.15 & - & -\\
    EMT~\cite{zhou2018end} & - & 4.38 & 11.55 & 27.44 & 0.38\\
    \textcolor{gray}{VideoBERT}~\cite{sun2019videobert} & \textcolor{gray}{6.80} & \textcolor{gray}{4.04} & \textcolor{gray}{11.01} & \textcolor{gray}{27.50} & \textcolor{gray}{0.49} \\
    ActBERT~\cite{zhu2020actbert}  & 8.66 & 5.41 & 13.30 & 30.56 & 0.65 \\
    AT~\cite{hessel2019case} & - & 8.55 & 16.93 & 35.54 & 1.06 \\
    \textcolor{gray}{UniVL}~\cite{luo2020univl} &  \textcolor{gray}{16.46} & \textcolor{gray}{11.17} & \textcolor{gray}{17.57} & \textcolor{gray}{40.09} & \textcolor{gray}{1.27} \\
\system & \textbf{12.87} & \textbf{8.72} & \textbf{14.83} & \textbf{36.09} & \textbf{1.16} \\
    \bottomrule
\end{tabular*}
\end{table*}

\begin{table*}[!t]
\begin{minipage}[t]{0.58\linewidth}
\caption{Comparison with SOTA methods on VQA for visual question answering. }
  \label{tab:imgvqa}
  \small
  \vspace{0.01in}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lccc@{}}
    \toprule
    \textbf{Method} & \textbf{\# Img-Text Pairs} & \textbf{test-dev} & \textbf{test-std} \\
    \midrule
    FLAVA~\cite{singh2021flava} & 68M & 72.80 & - \\
    OSCAR~\cite{li2020oscar} & 4M & 73.16 & 73.44 \\
    ALBEF~\cite{li2021align} & 14M & 75.84 & 76.04 \\
    BLIP~\cite{li2022blip} & 14M & 77.54 & 77.62 \\
    METER~\cite{dou2021empirical} & 404M & 77.68 & 77.64 \\
    SimVLM~\cite{wang2022simvlm} & 1.8B & 77.87 & 78.14 \\
    \red{OFA}~\cite{wang2022OFA} & 21.4M & 78.00 &	78.10 \\
    \system & 14M & \textbf{78.33} & \textbf{78.35} \\
  \bottomrule
\end{tabular*}
\end{minipage}
\hfill
\begin{minipage}[t]{0.4\linewidth}
\caption{Accuracy (\%) of video question answering on MSRVTT and MSVD.}
  \label{tab:vidqa}
  \small
  \vspace{0.05in}
\renewcommand{\arraystretch}{1.1}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcc@{}}
    \toprule
    \textbf{Method} & \textbf{MSRVTT} & \textbf{MSVD} \\
    \midrule
    ClipBERT~\cite{lei2021less} & 37.4 & - \\
    JustAsk~\cite{yang2021just} & 41.5 & 46.3 \\
    ALPRO~\cite{li2021prompt} & 42.1 & 45.9 \\
    MERLOT~\cite{zellers2021merlot} & 43.1 & - \\
    VIOLET~\cite{fu2021violet} & 43.9 & 47.9 \\
    \system & \textbf{44.1} & \textbf{51.0} \\
  \bottomrule
\end{tabular*}
\end{minipage}
\end{table*}


\noindent \textbf{Video Captioning.} In this section, we evaluate \system on YouCook2~\cite{zhou2018towards} for video captioning. We follow~\cite{zhou2018end} to report the results on the validation sets in Table~\ref{tab:vid_cap}. We can see that \system outperforms most existing methods in all the metrics. The models marked in gray, \eg, UniVL~\cite{luo2020univl}, apply an extra pre-trained backbone network S3D~\cite{xie2018rethinking} to extract video-level features offline. Comparatively, \system is fine-tuned in an end-to-end manner.


\noindent \textbf{Visual Question Answering.} For VQA, the model is expected to predict an answer given an image and a related question. To this end, we follow~\cite{li2021align,li2022blip} to formulate it as a generation task and focus on open-ended VQA. It is worth noting that during fine-tuning, we first input the image and the question into the unified visual encoder and visual-grounded alignment decoder separately to obtain a fused multi-modal representation, and then feed the visual representation and multi-modal representation into the visual-grounded generation decoder to predict the final answer. We compare with existing SOTA methods in Table~\ref{tab:imgvqa}. We observe consistent performance gain compared with existing methods. Especially, \system even outperforms SimVLM~\cite{wang2022simvlm} by 0.6\% trained with 1.8B image-text pairs.


\noindent \textbf{Video Question Answering.} Table~\ref{tab:vidqa} summarizes the results of video question answering on MSRVTT-QA~\cite{xu2017video} and MSVD-QA~\cite{xu2017video}. \system surpasses both QA-specific methods, \eg, JustAsk~\cite{yang2021just}, and pretraining methods, \eg, VIOLET~\cite{fu2021violet} on both datasets, which validates the effectiveness of our method for complex multimodal modeling. 

\subsection{Ablation Study}
\noindent \textbf{Decoupled Joint Pretraining.} To verify the effect of decoupled joint pretraining, we conduct four ablation experiments with different pretraining strategies: image-only pretraining, video-only pretraining, joint pretraining from scratch, \red{and Img2Vid pretraining where we first pretrain \system on image and then on video}. We list some representative results of each task in Table~\ref{tab:decoupled_joint} (see data details and more results in the supplementary material). 

\begin{table}[!ht]
\small
  \caption{Comparison results on various downstream tasks by using image/video-only pretraining, joint pretraining from scratch, and decoupled joint pretraining. }
  \label{tab:decoupled_joint}
  \centering
\resizebox{\columnwidth}{!}{
  \begin{tabular}{l | cc|c|cc|c|c}
    \toprule
    \multirow{2}*{\textbf{Pretraining}} & \multicolumn{2}{c|}{\textbf{COCO (ret)}}  & \textbf{MSRVTT (ret)} & \multicolumn{2}{c|}{\textbf{COCO (caption)}} & \textbf{VQA} & \textbf{MSRVTT(QA)} \\
    ~ &  TR@1 & IR@1 & IR@1 & B@4 & C & test-dev & acc \\ 
\midrule
    \red{Without Pretraining} & 37.1 & 28.5 & 9.6 & 27.4 & 80.0 & 39.51 & 36.6 \\
    Video-only & - & - & 13.7 & - & - & - & 15.8 \\
    Image-only & 80.9 & 63.0 & 38.2 & 39.3 & 131.6 & 77.62 & 40.8 \\
    Joint & 50.2 & 35.0 & 23.6 & 29.7 & 94.6 & 47.78 & 38.8 \\
    \red{Img2Vid} & 79.7 & 61.8 & 42.5 & 38.6 & 129.5 & 77.43 & 42.8 \\
    Decoupled Joint (ours) & \textbf{82.1} & \textbf{64.8} & \textbf{47.8} & \textbf{39.8} & \textbf{133.9} & \textbf{78.33} & \textbf{44.1} \\
    \bottomrule
  \end{tabular}}
\end{table}

We can see that video-only pretraining leads to significant performance degradation due to limited data scale for pretraining. Comparatively, image-only pretraining is a competitive baseline, which however, is still far behind decoupled joint pretraining on video tasks, \eg, text-video-retrieval MSRVTT (ret) and video question answering MSRVTT (QA). Compared to video-only pretraining, joint pretraining from scratch can significantly improve the performance on video tasks, which has also been verified in~\cite{bain2021frozen,fu2021violet}. However, it produces limited results on image tasks, which is even worse than image-only pretraining. \red{Img2Vid is another competitive baseline, which however demonstrates degraded performance on image tasks compared to image-only pretraining. } This indicates the naive combination of image-language and video-language cannot enjoy their synergy. 

By contrast, our simple decoupled joint pretraining strategy can achieve better performance on both image and video tasks. We hypothesize this is because starting from image-only pretraining can help the model focus on spatial representation learning first and provides better initialization, and thus the subsequent joint pretraining would be more concentrated on learning the temporal dynamics incrementally while preserving/polishing the well-learned spatial representations.

\noindent \textbf{UniVLC Loss.} We further replace the UniVLC loss with vanilla contrastive loss to study its impact on various downstream tasks. Note that we exclude the visual-label data from the pretraining corpus under the ``w/o UniVLC'' setting. The example results shown in Figure~\ref{fig:univl} illustrate that our method performs comparably to vanilla contrast-based model on vision-language tasks, \eg, image/video-text retrieval, image captioning, and image/video question answering. But on visual only tasks, \eg, linear probing for image/video classification and video action recognition fine-tuning, the performance gain is much higher, indicating that UniVLC could facilitate model to learn more discriminative visual representations and benefit transfer learning tasks. 

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\linewidth]{./UniVLC.pdf}
  \vspace{-0.2in}
  \caption{Evaluation on different tasks w/ and w/o UniVLC. We report the text recall@1 on COCO retrieval, B@4 on COCO captioning, test-dev on VQA, recall@1 on MSRVTT(ret), accuracy on MSRVTT(QA), accuracy on CIFAR10, CIFAR100, UCF101 (linear probe), and K400(fine-tuned).}
  \label{fig:univl}
\end{figure*}

\section{Conclusion and Discussion of Broader Impact}
In this paper, we presented \system, a novel vision-language foundation model that unifies image-language and video-language. 
It naturally supports visual only tasks, cross-modal alignment tasks, and multi-modal understanding and generation tasks. The unified vision-language contrastive loss also enables \system to utilize image-text, image-label, video-text and video-label data together. Accordingly, a \emph{decoupled} pretraining paradigm is introduced to decouple vision-language modeling into spatial and temporal dimensions, which boosts the performance on both image and video tasks. 

Although our model has achieved superior results on a wide range of downstream tasks, it still lacks of the commonsense reasoning capability required by some visual-language interaction tasks (e.g., visual/video question answering). It also needs better architecture design to support the zero-shot capability for visual question answering and few-shot task customization capability like GPT-3. From the societal impact perspective, since our model is pretrained on the large-scale web-crawled data which may contain some toxic language or bias, and it is not easy to explicitly control the model output, much attention should be paid to ensure responsible model deployment. 

\vspace{0.05in}

\textbf{Acknowledgement} This project  was supported by NSFC under Grant No. 62102092. Y.-G. Jiang was sponsored in part by ``Shuguang Program'' supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission (No. 20SG01).

\clearpage


\bibliographystyle{abbrv}
\bibliography{references}


\section*{Checklist}

\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerNo{We will release the code after the paper is accepted}
  \item Did you specify all the training details (\eg, data splits, hyperparameters, how they were chosen)?
    \answerYes{}
        \item Did you report error bars (\eg, with respect to the random seed after running experiments multiple times)?
    \answerNo{Our method is stable}
        \item Did you include the total amount of compute and the type of resources used (\eg, type of GPUs, internal cluster, or cloud provider)?
    \answerYes{}
\end{enumerate}


\item If you are using existing assets (\eg, code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
    \answerNo{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerNo{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNo{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNo{}
\end{enumerate}



\end{enumerate}


\appendix

\section{Specification for the Visual-grounded Alignment / Generation Decoder}
\label{sec:decoder}
As mentioned in the paper, the visual-grounded alignment decoder is applied to enable the deep interaction of multimodal information with cross-attention blocks, while the visual-grounded generation decoder is adopted to generate natural languages conditioned on the visual input. We further specify their architectures in Figure~\ref{fig:decoder}.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.75\linewidth]{./decoder.pdf}
  \caption{Architecture of the visual-grounded alignment / generation decoder.}
  \label{fig:decoder}
\end{figure*}

Note that both visual-grounded alignment decoder and visual-grounded generation decoder are initialized with the Bert-base model~\cite{devlin2018bert}, which stacks 12 transformer layers.  

\section{Image / Video Question Answering}
Image / video question answering requires the model to answer a question according to a given image / video, which models the complex interaction between visual and linguistic representations. During finetuning, we rearrange the pre-trained model, as shown in Figure~\ref{fig:vqa}.

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.75\linewidth]{./vqa.pdf}
  \caption{Architecture of the visual-grounded alignment / generation decoder.}
  \label{fig:vqa}
\end{figure*}

Our setup is based on the following considerations. We first input the image / video to unified visual encoder, the output of which will be combined with the text features of the questions through the visual-grounded alignment decoder. Based on these deeply fused representations, we finally generate the predicted answers with the visual-grounded generation decoder.


\section{Finetuning Setups}
In this section, we describe the settings used when fine-tuning the pretrained models on various downstream tasks.

\subsection{Image-Language Tasks}
For image-text retrieval and image captioning, we resize the images to 384  384, while for visual question answering, we resize the images to 480  480, following~\cite{li2022blip}. We use RandomAugment~\cite{cubuk2020randaugment} for data augmentation. The default settings for finetuning on each dataset are shown in Table~\ref{tab:img_ft}.

\begin{table*}[!ht]
  \caption{End-to-end finetuning configurations for image-language downstream tasks.}
  \vspace{0.05in}
  \label{tab:img_ft}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l |c|c|c@{}}
    \toprule
    \textbf{Config} & \textbf{COCO (retrieval) \& Flickr30k} & \textbf{COCO (captioning)} & \textbf{VQA} \\
    \midrule
    optimizer & AdamW & AdamW & AdamW \\
    base learning rate & 1e-5 & 1e-5 & 2e-5 \\
    weight decay & 0.05 & 0.05 & 0.05 \\
    learning rate schedule & linear decay & linear decay & linear decay \\
    batch size & 512 & 512 & 256 \\
    training epochs & 10 & 10 & 10\\
    \bottomrule
\end{tabular*}
\end{table*}



\subsection{Video-Language Tasks}
For all video-language downstream tasks, we resize video frames to 384  384. During fine-tuning, we randomly sample N frames from each video, where N = 8 for text-to-video retrieval, N = 16 for video question answering following~\cite{li2021prompt}, and N = 24 for video captioning. We perform uniform sampling during inference. Similar with image-language tasks, we also adopt RandomAugment~\cite{cubuk2020randaugment} for data augmentation. The default settings for finetuning on each dataset are shown in Table~\ref{tab:vid_ft}.

\begin{table*}[!ht]
  \caption{End-to-end finetuning configurations for video-language downstream tasks.}
  \vspace{0.05in}
  \label{tab:vid_ft}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l |c|c|c|c|c@{}}
    \toprule
    \textbf{Config} & \textbf{MSRVTT (ret)} & \textbf{DiDeMo} & \textbf{MSRVTT (QA)} & \textbf{MSVD (QA)} & \textbf{Youcook2}\\
    \midrule
    optimizer & AdamW & AdamW & AdamW & AdamW & AdamW \\
    base lr & 5e-6 & 1e-5 & 5e-6 & 1e-5 & 1e-5 \\
    weight decay & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 \\
    lr schedule & linear decay & linear decay & linear decay & linear decay & linear decay \\
    batch size & 32 & 32 & 32 & 32 & 32 \\
    training epochs & 6 & 6 & 10 & 10 & 10 \\
    \bottomrule
\end{tabular*}
\end{table*}


\section{More Comparison Results on Vision-language Tasks for Different Pretraining Paradigms}
We demonstrate more comparison results using different pretraining paradigms (\ie, image-only, video-only, joint pretraining from scratch, and our decoupled pretraining) on various vision-language downstream tasks in Table~\ref{tab:decoupled}. Details of the pretraining data can be found in Table~\ref{tab:data}. Moreover, an ``img2vid'' strategy is also adopted for further comparison, where we start with image-only pretraining and then implement video-only pretraining. We can see our decoupled joint pretraining paradigm achieves consistently better results on all the downstream tasks.

\begin{table*}[!ht]
\caption{More comparison results on various vision-language tasks for different paradigms.}
\label{tab:decoupled}
\begin{minipage}[t]{\linewidth}
\setlength\tabcolsep{3pt}
   \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l | cccccc | cccccc@{}}
    \toprule
    \multirow{2}*{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{COCO (5K test set)}}  & \multicolumn{6}{c}{\textbf{Flickr30K (1K test set)}}  \\
    ~ & \multicolumn{3}{c}{TR} & \multicolumn{3}{c|}{IR} & \multicolumn{3}{c}{TR} & \multicolumn{3}{c}{IR} \\
    \midrule
    Image-only & 80.9 & 94.8 & 97.5 & 63.2 & 85.2 & 91.3 & 96.6 & 99.8 & 100.0 & 87.2 & 97.5 & 98.8\\
    Joint & 50.2 & 75.6 & 84.9 & 35.0 & 62.7 & 73.9 & 67.2 & 83.4 & 92.1 & 56.5 & 63.4 & 71.7 \\
    Img2Vid & 79.7 & 94.8 & 97.7 & 61.8 & 84.7 & 90.9 & 95.8 & 99.6 & 99.9 & 76.5 & 97.3 & 98.2 \\
    Decoupled Joint& \textbf{82.1} & \textbf{95.9} & \textbf{98.1}	& \textbf{64.8} & \textbf{86.1} & \textbf{91.6} & \textbf{97.3} & \textbf{99.9} & \textbf{100.0} & \textbf{87.9} & 97.8 & \textbf{99.1} \\
    \bottomrule
  \end{tabular*}
\end{minipage}
\hfill


\begin{minipage}[t]{\linewidth}
\setlength\tabcolsep{3pt}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l | cccccc | cccccc@{}}
    \toprule
    \multirow{3}*{\textbf{Method}} & \multicolumn{6}{c|}{\textbf{Text-to-Video Retrieval}}  & \multicolumn{6}{c}{\textbf{Zero-shot Retrieval}}  \\
    ~ &  \multicolumn{3}{c}{MSRVTT} & \multicolumn{3}{c|}{DiDeMo} & \multicolumn{3}{c}{MSRVTT} & \multicolumn{3}{c}{DiDeMo} \\
    \midrule
    Video-only & 13.7 & 33.5 & 41.9 & 18.2 & 43.6 & 52.5 & 6.7 & 19.4 & 29.4 & 7.1 & 18.1 & 27.8 \\
    Joint & 23.6 & 49.7 & 61.5 & 28.1 & 52.8 & 64.4 & 15.5 & 39.6 & 53.4 & 19.2 & 42.7 & 51.9 \\
    Img2Vid & 42.5 & 71.3 & 79.9 & 51.1 & 76.6 & 82.8 & 38.3 & 56.1 & 64.4 & 37.5 & 62.0 & 72.6 \\
    Decoupled Joint & \textbf{47.8} & \textbf{74.2} & \textbf{83.8} & \textbf{52.4} & \textbf{79.5} & \textbf{85.4} & \textbf{42.0} & \textbf{63.0} & \textbf{73.0} & \textbf{40.6} & \textbf{64.6} & \textbf{74.3}  \\
    \bottomrule
  \end{tabular*}
\end{minipage}
\hfill
\vspace{0.3in}

\begin{minipage}[t]{\linewidth}
  \setlength{\tabcolsep}{3pt} 
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l|cccccccc|cc@{}}
    \toprule
    \multirow{3}*{\textbf{Method}} & \multicolumn{8}{c|}{\textbf{NoCaps}} & \multicolumn{2}{c}{\textbf{COCO Caption}} \\
    ~ & \multicolumn{2}{c}{in-domain} & \multicolumn{2}{c}{near-domain} & \multicolumn{2}{c}{out-domain} & \multicolumn{2}{c|}{overall} & \multicolumn{2}{c}{\textbf{Karpathy test}} \\
    ~ & C & S & C & S & C & S & C & S & B@4 & C \\
    \midrule
     Image-only & 100.2 & 14.4 & 107.2 & 14.6 & 102.7 & 13.8 & 105.5 & 14.4 & 39.3 & 131.6\\
    Joint & 100.0 & 14.1 & 95.7 & 13.6 & 77.4 & 11.6 & 93.0 & 13.4 & 29.6 & 94.6\\
    Img2Vid & 99.2 & 14.1 & 102.7 & 14.2 & 98.5 & 13.4 & 101.5 & 14.0 & 38.6 & 129.5\\
     Decoupled Joint & 104.6 & 15.0 & \textbf{108.3} & \textbf{14.9} & \textbf{106.3} & \textbf{14.2} & \textbf{107.5} & \textbf{14.7} & \textbf{39.8} & \textbf{133.9} \\
    \bottomrule
\end{tabular*}
\end{minipage}
\hfill
\vspace{0.1in}

\begin{minipage}[t]{0.3\linewidth}
\small
\setlength\tabcolsep{1pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcc@{}}
    \toprule
    \textbf{Method} & \textbf{test-dev} & \textbf{test-std} \\
    \midrule
    Image-only & 77.55 & 77.53 \\
    Joint & 47.78 & 47.80\\
    Img2Vid & 77.43 & 77.48 \\
    Decoupled Joint & \textbf{78.33} & \textbf{78.35} \\
  \bottomrule
\end{tabular*}
\end{minipage}
\hfill 
\begin{minipage}[t]{0.33\linewidth}
\small
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{1pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcc@{}}
    \toprule
    \textbf{Method} & \textbf{MSRVTT} & \textbf{MSVD} \\
    \midrule
    Video-only & 15.8 & 17.3\\
    Joint & 38.8 & 39.2 \\
    Img2Vid & 42.8	& 48.3\\
    Decoupled Joint & \textbf{44.1} & \textbf{51.0} \\
  \bottomrule
\end{tabular*}
\end{minipage}
\hfill 
\begin{minipage}[t]{0.3\linewidth}
\small
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{1pt}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcc@{}}
    \toprule
    \textbf{Method} & \textbf{B@4} & \textbf{C} \\
    \midrule
    Video-only & 3.56 & 0.29 \\
    Joint & 4.47 & 0.55 \\
    Img2Vid & 7.80 & 1.05 \\
    Decoupled Joint &  \textbf{8.72} & \textbf{1.16} \\
  \bottomrule
\end{tabular*}
\end{minipage}
\end{table*}


\begin{table*}[!ht]
  \caption{Pretraining data used for different pretraining paradigms.}
  \vspace{0.05in}
  \label{tab:data}
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l |c|c|c|c|c@{}}
    \toprule
    \textbf{Method} & \textbf{Image-Text} & \textbf{Image-Label} & \textbf{Video-Text} & \textbf{Video-Label} \\
    \midrule
     Video-only & - & - & 2.5M & 0.3M \\
     Image-only & 14M & 1.3M & - & - \\
     Joint & 14M & 1.3M & 2.5M & 0.3M \\
     Img2Vid & 14M & 1.3M & 2.5M & 0.3M \\
     Decoupled Joint & 14M & 1.3M & 2.5M & 0.3M \\
    \bottomrule
\end{tabular*}
\end{table*}

\section{Image/Video Captioning Examples}
We show some image and video captioning results generated by our method in Figure~\ref{fig:captions} and Figure~\ref{fig:vid_captions}, respectively. We can see that the captions generated by \system are both natural and abundant. Specifically, for the image captioning, when the visual information in the images is relatively simple, the generated captions are relatively general (line 2 and line 3). While when the contents are rich, \system can generate more fine-grained descriptions (line 1).  Fo video captioning, \system could accurately describe the actions (\eg, ``add'' and ``pour'') and objects (\eg, ``lemon juice'' and ``fried chicken'') in videos. The visualization results demonstrate the superior multimodal generation capability of \system.



\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\linewidth]{./captioning.pdf}
  \caption{Some captions generated by \system.}
  \label{fig:captions}
\end{figure*}

\begin{figure*}[!ht]
  \centering
  \includegraphics[width=\linewidth]{./video_cap.pdf}
  \caption{Some video captions generated by \system.}
  \label{fig:vid_captions}
\end{figure*}


\end{document}