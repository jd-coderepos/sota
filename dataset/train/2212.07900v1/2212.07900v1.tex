\section{Introduction}
\label{sec:intro}
We are interested in the problem of spatio-temporal localization of anomalous activities in videos of a given scene. Informally, anomalous activities are  events that differ from those typically observed in a scene, such as a cyclist riding through an indoor shopping mall \cite{ramachandra2020survey}.  Like many other papers on anomaly detection, this work addresses the setting in which we have access to an initial set of videos that are used to define the typical, or `nominal' activities in a particular scene. Such a situation naturally arises in  surveillance and monitoring tasks \cite{saligrama2010video}, where it is easy to collect nominal data, but it is not practical to collect a representative set of possible anomalies for a scene.
Thus, the problem set-up is as follows: provided with a set of videos of a scene which do not contain any anomalies, (called the {\em nominal set}), the goal is to detect any events in a test video from the same scene that differ substantially from all events in the nominal set \cite{mahadevan2010anomaly, ramachandra2020survey}.  In defining anomaly detection, it is important to consider the role of {\em location}.
In real-world surveillance scenarios, an event may be normal in one location but anomalous in another. For example, a car driving on a road is typically not anomalous, while one driving on a sidewalk typically is. In view of this, we adopt the following definition \cite{ramachandra2020survey}.

\begin{definition}
\vspace{-8pt}
 An {\it anomaly} is any spatio-temporal region of test video that is significantly different from all of the nominal video in the same spatial region. 
 \vspace{-10pt}
\end{definition}

Unlike most recent work in anomaly detection, a key goal of our work is to produce not only a set of anomalies, but a simple and clear explanation for what makes them anomalous. 
We are motivated by how people tasked with watching video from a stationary surveillance camera would detect an unusual incident.
While monitoring a scene, we expect a person to note the types of objects seen (people, buildings, cars) and the motions of those objects (walking east or west on a sidewalk, driving northwest on the street) to characterize the given scene.  The person could then notice an anomaly when the objects or motions do not match what has been seen before.  The person could also explain why something was anomalous.

We design our video anomaly detection system using this sketch of how a human would solve the problem as motivation.  We want to use deep networks to give a high-level understanding of the objects and motions occurring in a scene. By 'high-level', we mean at the level of whole objects and not at the level of pixels or edges.
To do this, we train deep networks that take a spatio-temporal region of video (which we call a {\em video volume}) as input and output attribute vectors representing the object classes, the directions and speeds of motion and the fraction of stationary pixels (which gives information on the sizes of moving objects) occurring in a spatio-temporal region.   The feature vectors from the penultimate layers of these deep networks yield high-level representations, or {\em embeddings}, of the appearance and motion content of each video volume. Ten frames are used for video volumes in our experiments.

Unlike many other recent works, we do {\em not} learn new embedding functions (i.e. networks) for new scenes. We use the same embedding networks for every environment. Instead, to characterize the nominal video for a new scene, we store a representative set of all the embeddings found in the nominal video. That is, for every video volume in the nominal video of a new scene, we calculate our representations of appearance, motion direction, speed, and background fraction. We then reduce this set of embeddings to a smaller set which we call {\it exemplars} by removing redundant embeddings. This results in a compact, accurate, and location-dependent model of the nominal data in a new scene.
Since there is no training of deep networks for each new environment, modeling a new environment is `lightweight' compared to many other methods, making it efficient to model new scenes.  Our exemplar model also allows very efficient updating if new nominal video is introduced.  This is a crucial property for video anomaly detection methods because, in practice, it is unrealistic to assume that the initial nominal video covers every possible normal change.  New nominal video will occasionally need to be added, making it critical that models are easy to extend.

Given test video of the same scene, we compute our high-level features for each video volume.  We then compare these to the exemplars stored in the nominal model at the corresponding spatial region.  Any test feature with a high distance to \textit{every} nominal exemplar for that region is considered anomalous.
Because the feature vectors map to human-interpretable attributes, these attributes can be used to give human-understandable explanations for why our system labeled video volumes as normal or anomalous.
We define a method as 'explainable' if it can give human-understandable reasons for its decisions.  Details of how our system provides explanations are given in Section \ref{sec:explainability}.

In summary, we make the following key contributions:  1) We show that modeling scenes using high-level attributes leads to robust anomaly detection.
2) We introduce the idea of directly estimating high-level motion attributes from raw video volumes using deep networks.
3) We show how these high-level attributes also allow human-interpretable explanations.
4) Finally, we demonstrate an alternative to much of the previous work that is based on learning to reconstruct the nominal data.  Our alternative approach is practical since it does not require training deep networks for each new scene and allows for simple and efficient updates to a scene model given new nominal training data.






\section{Related Work}
\label{sec:related}

\begin{figure*}[]
    \centering
    \includegraphics[width=0.78\linewidth]{Figures/complete_pipeline2.pdf}
    \vspace{-10pt}
    \caption{Our pipeline for building a location-dependent model of  nominal video and detecting anomalies in test video. During the model building phase, we extract a high-level representation of each video volume using our appearance and motion networks. Using the exemplar selection method, we select a representative subset of video volumes for a given spatial region. By comparing video volumes in test video to the exemplar set we can detect anomalies.
    }
    \label{fig:modelbuilding}
    \vspace{-10pt}
\end{figure*}

Most Video Anomaly Detection (VAD) methods can be analyzed in terms of their representation learning or their detection method.
\\
\textbf{Representation of Nominal Data:}  Early approaches to VAD \cite{adam2008robust, antic2011video, cong2013abnormal, li2013anomaly, mehran2009abnormal, saligrama2012video, wu2010chaotic} primarily relied on the usage of handcrafted features. This included features like spatio-temporal gradients \cite{lu2013abnormal, ionescu2019detecting}, histogram of gradients \cite{saligrama2012video, ma2015anomaly}, flow fields \cite{adam2008robust, antic2011video, wu2010chaotic, mehran2009abnormal},  histogram of flows \cite{saligrama2010video,saligrama2012video,cong2013abnormal}, dense trajectories \cite{ma2015anomaly, stauffer2000learning} and foreground masks \cite{antic2011video}. Recently, most authors have used deep learning for this task \cite{doshi2020any, doshi2020continual, hasan2016learning, hinami2017joint, ionescu2019object, liu2018future, luo2017revisit, ramachandra2020learning, ravanbakhsh2017abnormal, ravanbakhsh2018plug, smeureanu2017deep, wang2018abnormal, sabokrou2017deep, 10.1016/j.cviu.2016.10.010}. These methods either use a pretrained model \cite{smeureanu2017deep, ionescu2019detecting, hinami2017joint, ravanbakhsh2018plug, luo2017revisit, sabokrou2017deep} for feature extraction or train a model to specifically optimize for a particular task related to anomaly detection. These tasks can generally be categorized as either a variant of training an auto-encoder architecture to minimize the reconstruction error of nominal frames \cite{hasan2016learning,ionescu2019object,nguyen2019anomaly,chang2020clustering,lu2020few,liu2021hybrid}, a generative adversarial network (GAN) to model nominal frames \cite{liu2018future, lu2020few}, or future frame prediction given a sequence of nominal frames \cite{liu2018future,wang2021prediction}.  To further improve their performance, recent works have tried specialized architectures and training methodologies. Particularly \cite{dong2020dual, liu2021hybrid, park2020learning} transform their respective generative model by memory-based modules to memorize the normal prototypes in the training data. Most recently \cite{ristea2021self} proposed a module based on masked convolution and channel attention  to  reconstruct  a  masked  part  of  the  convolutional receptive field.  A key drawback of reconstruction and future frame prediction methods is that they do not generate interpretable features. It is not clear what aspects of the video make it difficult to reconstruct since there is no mapping to higher-level features as in our work.
Our work mostly aligns with methods utilizing pretrained models. However, unlike most of these approaches, we incorporate the output of our pretrained models to interpret model decisions. We additionally make sure that the predictions of our model generalize to a wide variety of scenes through our data generation and training procedures.  One high-level motion attribute that our method learns is similar to the histogram of flow feature used in some early work \cite{saligrama2010video,saligrama2012video,cong2013abnormal}.  However, instead of computing the histogram of flow from an optical flow field, our method learns a deep network to predict these features directly from RGB video volumes and then uses the network's learned feature embedding as the representation.
\\
\textbf{Detection Methods:} Most methods use either standard outlier detection methods \cite{chandola2009anomaly} as an external module or utilize the reconstruction strategy to predict anomalies. General methods of detection that most authors have used in the past include one-class SVM \cite{smeureanu2017deep, ionescu2019detecting, ma2015anomaly, xu2015learning}, nearest neighbour approaches \cite{ramachandra2020street, ramachandra2020learning, hinami2017joint, doshi2020any, doshi2021efficient}, and Probabilistic Graphical Models \cite{antic2011video}. In \cite{astrid2021synthetic, georgescu2021background}, pseudo-anomalous samples are used during training to improve discriminative learning.  In \cite{georgescu2021anomaly}, an object detector is used to focus on regions around objects and then networks are trained for various 'proxy' tasks (such as predicting the arrow of time) on the nominal data.  Thus, unlike our work, they require training deep nework models for each different scene.
Our work has some similarity to the work of \cite{ramachandra2020street,ramachandra2020learning} in that we also use an overlapping grid of spatial regions, build exemplar-based models and use nearest neighbors distances as anomaly scores.  The high-level features that we use are the biggest difference as compared to the pixel-based features used in their work.  Our high-level features allow for explainable models as well as much smaller models than theirs.
\\
\textbf{Explainable VAD:} 
Our work is similar in spirit to the work of \cite{hinami2017joint, wu2021explainable} with respect to providing explanations for detecting anomalies. In \cite{hinami2017joint}, the authors pre-train their feature extractor on public image datasets (MS-COCO and Visual Genome) to detect objects and predict their attributes and actions. They further use these predictions for 'event recounting' on VAD benchmarks. In \cite{wu2021explainable}, the authors utilize models pretrained for semantic segmentation, object classification, and multi-object tracking
and use the output of these models directly as their feature representation.

Despite these coarse similarities, virtually all of the details of our methods are different. Specifically, in \cite{hinami2017joint}, they rely on object proposals to find candidate anomalous regions which can lead to missed detections for objects not represented in their training data. Our action/motion classes are also very different - ours being more generic (direction distributions and speed of motion) while in \cite{hinami2017joint} are much more specific (bending, riding) and hence not applicable to a wide variety of scenarios.
The method of \cite{wu2021explainable} is specific to detecting and tracking pedestrians and is not a general video anomaly detection method.  Furthermore, unlike ours, their method does not spatially localize anomalies.

\section{Our Approach}

Our method consists of three distinct stages: high-level attribute learning, model building, and anomaly localization.  The high-level attribute learning stage is done only once and uses training samples that are independent of any video anomaly detection dataset.  The resulting deep networks learn general representations of object appearances and object motions which can then be used in the subsequent two stages to build a model of a specific scene and to localize anomalies in that scene, for a wide variety of surveillance scenarios. The outside data used to train our high-level attribute models are equivalent to the outside data used in various prior works on VAD; for example, the MS-COCO and Visual Genome data used to train the models of Hinami et al. \cite{hinami2017joint}, the outside data used to train object detectors in \cite{georgescu2021anomaly,doshi2021efficient,ionescu2019detecting,smeureanu2017deep}, as well as the many deep models pretrained on ImageNet and applied to VAD.  Our outside data is not used to build models of a scene.

\vspace{-2pt}
\subsection{\bf{High-Level Attribute Learning}}

For this stage, our main objective is to learn features that are \textbf{(a)} transferable across scenes and \textbf{(b)} interpretable. Given this motivation, we learn an object recognizer for our appearance model as well as regression networks for estimating the following motion attributes for a given video volume: the fraction of stationary pixels, the distribution of motion directions and the average speed of movement in each direction.  We also learn a classifier to indicate whether a video volume is stationary or not.  We will describe each of these deep networks in the following subsections.

To clarify our terminology, we use the term {\em 'high-level attribute'} to denote the object classes, histogram of motion directions, vector of motion speeds or fraction of stationary pixels which are the final outputs of the various deep networks that are learned.  The term {\em 'high-level feature'} denotes the feature vector from the penultimate layer of one of the deep networks.  A high-level feature can be mapped to a high-level attribute using the final layer of that network.
\vspace{-10pt}
\subsubsection{Appearance model}
We formulate the task of object recognition as a multi-label image classification problem as any given input image patch may contain more than one object class (or none). 

\textbf{Training Data:}
We are particularly interested in learning to recognize objects that have high likelihood of being present in outdoor scenes. To this end, we select the following 8 categories as our primary set of object classes : [Person, Car, Cyclist, Dog, Tree, House, Skyscraper, and Bridge]. For our formulation, we want the learned features to generalize across different domains. To achieve this, we construct our training dataset of images from multiple sources. 
We use labeled examples of each class (as well as background images containing none of the classes) taken from the CIFAR-10 \cite{krizhevsky2009learning}, CIFAR-100 \cite{krizhevsky2009learning}, and MIO-TCD \cite{luo2018mio} datasets as well as a set of publicly available surveillance videos from static webcams that we collected and annotated.  More details about the data collection is given in the supplemental material.  In total we used 187,793 RGB training examples, resized to 64x64 pixels.

\textbf{Neural Architecture:}
We use a modified ResNext-50 network \cite{xie2017aggregated} as our backbone architecture. We modified the original model by adding an extra fully connected layer that maps the 2048-dimensional feature vector after the average pooling layer to a 128-dimensional layer. The 128-dimensional layer is then mapped by a final fully connected layer to an 8-dimensional output layer with sigmoid activations that represent the categories.  The extra fully connected layer gives us a 128-dimensional feature vector to represent appearance instead of the 2048-dimensional feature vector after RexNext-50's usual penultimate layer thus greatly improving memory efficiency.
To train our model, we utilize the Binary Cross Entropy as our loss function.  

Note that high-level features are usually distinctive even for unseen object classes despite the corresponding high-level attribute having low probabilities for all of the known object classes.  This allows our appearance model to handle object classes other than the eight that we train on.  See supplemental material for experiments on this.

\vspace{-5pt}
\subsubsection{Motion Model} To characterize the motion information for a given video volume, we train deep networks to estimate the following attributes directly from an RGB video volume: \textbf{(a)} histogram of optical flow  (), \textbf{(b)} a vector of the average speed of pixels in each direction of motion (), \textbf{(c)} background classifier () and \textbf{(d)} percentage of stationary pixels (). The histogram of optical flow consists of 12 bins each of which stores the fraction of pixels in the video volume that are estimated to be moving in one of the 30 degree directions of motion.  The average speed vector consists of the average speed (in pixels per frame) of all pixels falling in each of the 12 histogram of flow bins.  The background classifier classifies whether the video volume contains motion or not.  The percentage of stationary pixels in a video volume gives the rough size of the moving objects in a video volume.

\textbf{Motion Training Data:}
We use the set of surveillance videos mentioned above to learn motion attributes in a self-supervised way.
For each video, we sample video volumes from regions with significant 'motion' as well as very little motion ('background'). We identify these regions by computing their pixelwise optical flow fields using the TV-L1 method \cite{zach2007duality}, which is also used to automatically generate ground-truth motion attributes.  (Note that optical flow is only used to create ground truth for training our motion models.  It is not used in later stages.)  
In total we obtain  `background' video volumes and  `motion' video volumes. We use  of these for training our models and the remainder for validation.

The ground truth motion attributes for each training video volume are computed from the corresponding pixelwise flow fields as follows: 
We represent the  attribute as a single binary variable denoting if a video volume is 'background'() or not (). 
The ground truth for  and  are computed by first computing a 13-bin normalized histogram, wherein the first  bins represent the number of pixels with flow orientation in the ranges  with  , while the last bin denotes the number of pixels with flow magnitude below threshold.  The histogram is then normalized by the total number of pixels. 
The first 12 bins of this histogram are used as the ground truth for  and the  bin is the ground truth for .   Finally, we represent  as a -dimensional vector denoting the average flow magnitude for pixels in each of the 12 flow orientation ranges.

\textbf{Learning Task and Neural Architecture:} We treat each motion attribute independently and train separate models respectively. For each attribute prediction task, we use the same backbone architecture design, but train each model using different objective functions. Our model is a stack of 3D convolutions (3DConv) with batch normalization (BN) and ReLU. In total we have 3 layers of [3Dconv-BN-ReLU] followed by a fully connected layer. We provide additional details in the supplemental material.

We formulate  attribute prediction as a standard binary classification task and train the model using cross-entropy loss function. For  and  attribute prediction, we treat the learning task as a regression problem and train the model using mean squared error loss. And finally, for training the model to predict  attribute, we utilize KL Divergence loss. For all the tasks, we construct a simple light-weight CNN. The detailed configuration of our 3D CNN architecture is presented in the supplemental material.
\subsection{Model Building}

Once trained, the attribute deep nets are used to build a model of any scene given the nominal video. As illustrated in Figure \ref{fig:modelbuilding}, to process each nominal video, we slide a spatio-temporal window of dimension  with spatial stride  and temporal stride of  to construct video volumes.  In the experiments, we select  and choose  to be roughly the height in pixels of a person in a particular dataset.  For each RGB video volume, we extract its features using the previously trained appearance net and four motion nets. To get a single appearance feature vector for a video volume, the feature vectors computed by the appearance network for each frame of the video volume are averaged.
We concatenate the feature vectors from the penultimate layers of the appearance, angle, speed and background pixel nets along with the binary output of the background classifier net to create a combined feature vector.
We use  to denote a combined feature vector and , and  to denote the appearance, angle, magnitude and background pixel fraction feature vectors, each of size . Finally,  denotes the binary background classification of size .   is of size .

After computing features, we use the exemplar selection approach of \cite{jones2016exemplar, ramachandra2020learning} to create a region-specific compact model of the nominal data. For each region, we use the following greedy exemplar selection algorithm:

\vspace{-4pt}
\begin{enumerate}
\itemsep-0em 
    \item Add the first feature vector to the exemplar set.
    \item For each subsequent feature vector, compute its distance to each feature vector in the exemplar set and add it to the exemplar set only if all distances are above a threshold, .
\end{enumerate}
\vspace{-4pt}

To compute the distance between two feature vectors  and  we use  distances between corresponding components normalized by a constant to make the maximum distance for each component approximately 1.  When a video volume does not contain motion (as determined by the background classification, ), the motion component vectors are set to 0.    The distance function can be written as follows:

where ,

The normalization factors, , ,  and  are computed once by finding the max  distances between a large set of feature vector components computed from a validation set (UCSD Ped1 and Ped2 in our experiments). 

One big advantage of the exemplar learning approach is that updating the exemplar set in a streaming fashion is possible. This makes the approach scalable and adaptable to environmental changes over time.

\subsection{Anomaly Detection}
At test time, we process each test video in the same way (by sliding a  spatio-temporal window with spatial stride  and temporal stride of ) to generate video volumes. For each video volume, we compute the combined feature vector as before using the pre-trained nets. Each combined feature vector is compared with every exemplar for the corresponding region using the distance function in Equation \ref{eq:dist}. The anomaly score for the given test video volume is the minimum distance over the set of all exemplars from the same spatial region.  A pixelwise anomaly score map is maintained by assigning the anomaly score to all pixels corresponding to every frame of the video volume. If a pixel has already been assigned an anomaly score (because of partially overlapping video volumes), then the maximum of the previous score and the current score is assigned.  Figure \ref{fig:modelbuilding} shows our anomaly detection pipeline.
\label{sec:method}

\subsection{A Note on Computational Efficiency}

For both model building and anomaly detection, most of the time is spent computing feature vectors (forward passes of 5 networks).  This is greatly sped up by testing whether a video volume is the same as the previous video volume in time.  If there is no change then the anomaly score for the new video volume should be the same as the one before it and no computation of feature vectors is needed.  This allows our method to run at 20 to 100 fps (dataset dependent).  Details are in the supplemental material.



\section{Experiments}
\subsection{Datasets and Evaluation Criteria}
We experiment on five benchmark datasets: UCSD Ped1 and Ped2 \cite{mahadevan2010anomaly}, CUHK  Avenue \cite{lu2013abnormal}, Street Scene \cite{ramachandra2020street} and ShanghaiTech \cite{luo2017revisit}. We use UCSD Ped1 and Ped2 with modified ground truth for parameter tuning and CUHK Avenue, Street Scene and ShanghaiTech for evaluation.
\\
\textbf{UCSD Ped1 \& Ped2:}  UCSD Ped1 dataset contains 34 training videos and 36 test videos while UCSD Ped2 dataset contains 16 training videos and 12 test videos. Anomalies consists of bikers, skaters and cars in a pedestrian area.
\\
\textbf{CUHK Avenue:} The Avenue \cite{lu2013abnormal} dataset contains 16 training videos with normal activity and 21 test videos. Examples of abnormal events in Avenue are related to people running, throwing objects or walking in wrong direction.
\\
\textbf{Street Scene:} The Street Scene \cite{ramachandra2020street} dataset contains 46 training videos defining the normal events and 35 test videos. Prominent examples of anomalies include jaywalking, loitering and bikes or cars driving outside their lanes.
\\
\textbf{ShanghaiTech:} The ShanghaiTech \cite{luo2017revisit} dataset is a multi-scene benchmark for video anomaly detection. It consists of 330 training and 107 test videos. Major categories of anomalies include people fighting, stealing, chasing, jumping, and riding bikes or skating in pedestrian zones.

While our primary focus is on single scene video anomaly detection task, we consider the ShanghaiTech dataset only to highlight the ease of usability and robustness of our method to multi-scene benchmarks. Our method is applied to ShanghaiTech without modification even though the location-dependent aspect of our model is not necessary for a multi-scene dataset.  Improvements in accuracy are likely if we specialize our model to multi-scene datasets.

\textbf{Evaluation Criteria.} We use the Region-Based Detection Criterion (RBDC) and the Track-Based Detection Criterion (TBDC) as proposed in \cite{ramachandra2020street} for quantitative evaluation of our framework.  These criteria correctly measure the accuracy of spatially and temporally localizing anomalous regions (RBDC) and anomalous tracks (TBDC) versus false positive detections per frame.  We report the area under the curve (AUC) for false positive rates per frame from 0 to 1 for each of these criteria. As pointed out in \cite{ramachandra2020street}, frame-AUC \cite{mahadevan2010anomaly} is not an appropriate evaluation  metric for video anomaly detection methods that spatially localize anomalies. However, we report frame-AUC scores of our method for completeness and comparison with other older methods. We also do not use the pixel-level criterion \cite{mahadevan2010anomaly} because of its serious flaws as mentioned in \cite{ramachandra2020street}. 

\subsection{Implementation}

\textbf{Feature Learning.} To train our \textbf{appearance model}, we use SGD with a 0.001 learning rate and 0.9 momentum and train for 50 epochs. The model with lowest classification error on the validation set is selected.  For \textbf{motion models} we optimize with AdamW~\cite{loshchilov2018decoupled} with a 0.001 learning rate and train for 30 epochs. We select the best model for each attribute using the validation set.
\\
\textbf{Video volume parameters.} We define the dimensions  of a video volume for each dataset so that  is roughly the height of a person in pixels and . Specifically, for Ped1, Ped2, Avenue, Street Scene and ShanghaiTech, our region dimensions are , , ,  and  respectively. Zero-padding was used for edge regions as needed. The number of frames in a video volume, t, is 10 for all datasets.

\begin{table}[tb]
  \centering
  \resizebox{\hsize}{!}{
  \setlength{\tabcolsep}{8pt}
  \begin{tabular}{|p{0.5cm}|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Th} & \multicolumn{3}{c|}{UCSD Ped1} & \multicolumn{3}{c|}{UCSD Ped2}\\
    \cline{2-7}
    & RBDC & TBDC & NUM & RBDC & TBDC & NUM\\
\hline
    \hline
    3 & 36.866 & 77.83 & 288 & 64.808 & 89.13 & 350 \\ 
    2.5 & 49.36 & 89.43 & 424 & 78.813 & 93.716 & 761 \\ 
    2 & 57.524 & 89.6 & 944 &  84.66 & 95.97 & 1339 \\ 
    \bf{1.5} & \bf{61.65} & \bf{88.9} & \bf{4201} & \bf{87.44} & \bf{95.08} & \bf{4470} \\ 
    1 & 61.496 & 87.54 & 19926 & 87.408 & 95.776 & 19138 \\ 
    0.5 & 61.435 & 87.72 & 49113 & 87.195 & 95.12 & 34862 \\
    0.25 & 61.49 & 87.81 & 57636 & 87.199 & 95.127 & 45795 \\ 
    \hline
    
  \end{tabular}
  }
  \caption{RBDC and TBDC scores (in \%) of our method for different thresholds () on UCSD Ped1 and Ped2. NUM denotes the total number of exemplars across all regions.}
  \label{ucsd}
  \vspace{-10pt}
\end{table}

\textbf{Parameter Tuning.} To set a threshold  for exemplar selection without fitting to test data, a validation data set is needed. We chose Ped1 and Ped2 for this purpose, both because these data sets are  performance-saturated, and because previous works~\cite{ramachandra2020learning} have identified inconsistencies in their ground truth. Specifically,
ground-truth annotations of Ped1 and Ped2 do not label every location-specific anomaly.
To rectify this, we augment the existing ground truth annotations to include all anomalies consistent with Definition 1.  This is justified because we are using Ped1 and Ped2 to set our hyperparameters and not to compare against previous methods.  Table~\ref{ucsd} shows region-based and track-based AUC for different values of the threshold  used for exemplar selection for both Ped1 and Ped2.  We see that the accuracy of our method is robust to large variations of .  However, larger values of  lead to smaller numbers of exemplars and thus smaller models of the nominal video which is desirable. We select  as a good trade-off between accuracy and model-size.  We use this value on all datasets in our experiments. For Ped2, the average number of exemplars selected per region is about 13 ( of the total number of video volumes in the nominal video).
Exemplar selection typically finds tens to sometimes low hundreds of exemplars (for Street Scene) for regions with lots of activity.  Regions with very little activity typically have only 1 or 2 exemplars.  This leads to very compact models of the nominal video.



\subsection{Quantitative Results}
Tables \ref{t1} and \ref{t2} compare our method to other top methods on Avenue, ShanghaiTech and Street Scene.  On Avenue, we improve over all previous methods for the region-based detection criterion (RBDC) and are second best for the track-based detection criterion (TBDC).  On ShanghaiTech, we improve over the next best method for both RBDC and TBDC by significant margins.  For the frame-level criterion which does not measure spatial localization we are in the middle of the pack compared to other recent methods for both Avenue and ShanghaiTech. On the  difficult Street Scene dataset (Table \ref{t2}), we improve the previous state of the art for both RBDC and TBDC, the latter by more than 11\%.  The good results across five different datasets (including Ped1 and Ped2) show the generality of the high-level features that we use in our models.

\begin{table}[tb]
  \centering
  \resizebox{\hsize}{!}{
\setlength{\tabcolsep}{8pt}
  \begin{tabular}{|p{3cm}|c|c|c|c|c|c|}
    \hline
    \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Avenue} & \multicolumn{3}{c|}{ShanghaiTech}\\
    \cline{2-7}
    & RBDC & TBDC & Frame & RBDC & TBDC & Frame\\
\hline
    \hline
    Ionescu \textit{et al}.\cite{ionescu2019object} & 15.77 & 27.07 & 87.4 & 20.65 & 44.54 & 78.7 \\
     Ramachandra \textit{et al.} \cite{ramachandra2020street}&  35.80 & 80.90 & 72.0 & - & - & -\\
     Ramachandra \textit{et al.} \cite{ramachandra2020learning}  & 41.20 & 78.60 & 87.2 & - & - & -\\
     Georgescu \textit{et al.} \cite{georgescu2021anomaly}  & 57.00 & 58.30 & 91.5 & 42.80 & 83.90 & \Red{\bf{90.02}} \\
     Liu \textit{et al.} \cite{liu2018future} &  19.59 & 56.01 & 85.1 & 17.03 & 54.23 & 72.8 \\
     Liu \textit{et al.} \cite{liu2021hybrid} &  41.05 & 86.18 & 89.9 & 44.41 & 83.86 & 74.2 \\
     Georgescu \textit{et al.} \cite{georgescu2021background} &  65.05 & 66.85 & \Blue{\bf{92.3}} & 41.34 & 78.79 & 82.7\\
     Liu \textit{et al.}\cite{liu2018future}  Ristea \textit{et al.} \cite{ristea2021self} &  20.13 & 62.30 & 87.3 & 18.51 & 60.22 & 74.5 \\
     Liu \textit{et al.}\cite{liu2021hybrid}  Ristea \textit{et al.} \cite{ristea2021self} &  62.27 & \Red{\bf{89.28}} & 90.9 & \Blue{\bf{45.45}} & \Blue{\bf{84.50}} & 75.5\\
     Georgescu \textit{et al.}\cite{georgescu2021background}  Ristea \textit{et al.} \cite{ristea2021self} &  \Blue{\bf{65.99}} & 64.91 & \Red{\bf{92.9}} & 40.55 & 83.46 & \Blue{\bf{83.6}}\\
     Our Method &  \Red{\bf{68.2}} &  \Blue{\bf{87.56}} & 86.02 & \Red{\bf{59.21}} & \Red{\bf{89.44}} & 76.63\\
    
    \hline
    
  \end{tabular}
  }
  \caption{RRBDC, TBDC and Frame AUC scores (in \%) of various state-of-the-art methods on Avenue and ShanghaiTech datasets. The top score for each metric is highlighted in \Red{\bf{red}}, while the second best score is in \Blue{\bf{blue}}. }
  \label{t1}
  \vspace{-10pt}
\end{table}

\begin{table}[tb]
\setlength{\tabcolsep}{8pt}
\centering
\resizebox{0.7\hsize}{!}{
\begin{tabular}{| c | c | c | }
    \hline
    \bf{Methods} & RBDC & TBDC\\
    \hline
    \hline 
     Auto-encoder \cite{hasan2016learning} & 0.29 & 2.0 \\
     Dictionary method \cite{lu2013abnormal} & 1.6 & 10.0 \\
     Flow baseline \cite{ramachandra2020street} & 11.0 & 52.0 \\
     FG Baseline \cite{ramachandra2020street} & \Blue{\bf{21.0}} & \Blue{\bf{53.0}} \\
     Our Method &  \Red{\bf{24.26}} & \Red{\bf{64.5}}  \\
     \hline
\end{tabular}
}
\caption{RBDC and TBDC AUC scores (in \%) of various baseline methods on Street Scene dataset. The top score for each metric is highlighted in \Red{\bf{red}}, while the second best score is highlighted in \Blue{\bf{blue}}. }
\label{t2}
\vspace{-5pt}
\end{table}

\subsection{Qualitative Results: Explainability}
\label{sec:explainability}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/AnomalyVisualizaton_Explanation.pdf}
    \caption{Explanation of our ``instrument panel'' showing the estimated attributes for a video volume. The interpretation of this visualization would be (roughly) a car (class 1) taking up most of the video volume, moving right at a high speed.}
    \label{fig:viz_explanation}
\vspace{-10pt}
\end{figure*}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/AnomalyVisualization_Jaywalking.pdf}
    \caption{Visualization of the learned exemplars for a region of Street Scene and visualization of a test video volume explaining why it was detected as an anomaly.}
    \label{fig:visualization_jaywalk}
\vspace{-8pt}
\end{figure}

One of the big advantages of our method in addition to its accuracy is that it allows intuitive explanations of what the model has learned and why it labels a particular test video volume as anomalous or not.  To visualize the feature vector representing a video volume, the appearance and motion components of the combined feature vector are mapped using the last fully connected layer of the respective network to the high-level appearance and motion attributes.  We can then visualize these attributes as illustrated in Figure \ref{fig:viz_explanation}.

As an illustration of our model's explainability, the top of Figure \ref{fig:visualization_jaywalk} visualizes the exemplars learned from the nominal video for a spatial region in the middle of the street.
Cars travel down and to the right in this lane of the street.  The top six exemplars learned show mainly cars (or unknown objects, since video volumes containing only parts of cars are often not classified as cars) traveling down and to the right, as expected.  There are also exemplars for stationary background, as well as stationary cars (since occasionally traffic stops on this part of the street).  Thus, our learned model is understandable and consistent with what one expects.
Furthermore, for a video volume containing a person jaywalking, the visualization in the bottom, left of the figure shows that our networks correctly identify it as containing a person walking mainly to the right at moderate speed.  The closest exemplar to this test volume is an unknown object moving down and to the right  which yields a high anomaly score of 2.08.  (A threshold of 1.8 yields high detection rates with low false positive rates across all the datasets.)  Thus, the explanation of this anomaly is that there is an unusual object (person) walking in an unusual direction.

Another example is shown for Ped2 in Figure \ref{fig:visualization_ped2}.  Here we analyze a region on the sidewalk.  The exemplars learned for this region (shown at the top of Figure \ref{fig:visualization_ped2}) are mainly background with very little movement or people moving mainly left or right at slow speeds. Some video volumes containing only parts of people are not classified as people which leads to exemplars of unknown objects moving left or right.  Overall, these exemplars are again what we would expect for this region.  For the test frame shown, a cyclist is riding on the sidewalk.  The visualization of the video volume centered on that frame at that spatial region shows that it was classified as a cyclist traveling down and right at a high speed.  These high-level attributes differ from the nearest exemplar in terms of its object class and speed and therefore leads to a high anomaly score.

As a final illustration (we show more in the supplemental material), we look at an example from CUHK Avenue in Figure \ref{fig:visualization_avenue}. 
The top six exemplars for the region highlighted at the left of the figure show that the model has learned that this region contains either background with very little movement or else people or unknown objects moving mainly left or right slowly.  For the anomalous test video volume shown containing a person running to the left, the high-level features estimate an unknown object moving left at high speed.  Even though the object recognizer did not correctly predict that the video volume contains a person, the person class is the most likely out of the eight classes.  The nearest exemplar is an unknown object (closest to a person class) moving slowly to the left.  The main difference between the test video volume and the closest exemplar is the unusual speed which correctly explains this anomaly.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/AnomalyVisualization_Ped2.pdf}
    \caption{Visualization example for a region of UCSD Ped2 showing an explanation of the anomaly.}
    \label{fig:visualization_ped2}
   \vspace{-10pt}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.48\textwidth]{Figures/AnomalyVisualization_Avenue.pdf}
    \caption{Visualization example for a region of CUHK Avenue showing an explanation of the anomaly.}
    \label{fig:visualization_avenue}
\end{figure}
\label{sec:experiments}




\subsection{Ablation Study}
\label{sec:ablation}

We perform an ablation study on all the benchmarks to evaluate the benefit of each attribute in detecting anomalies. We consider features from each model separately, combining features from only motion models and finally our full model which uses all features. We accordingly change the distance function to compare features of two video volumes so that it only uses the provided features. 
We present our results in Table \ref{ablation1}. We see that different attributes can be important for different types of scenarios. However, we get best results across all the benchmarks only when we combine all the motion and appearance features.
This highlights the importance of modeling both appearance and different components of motion, especially to be able to predict anomalies under wide variety of scenarios.

\begin{table}[t]
\setlength{\tabcolsep}{1.5pt}
\centering
\begin{tabular}{| c | c | c | c | c |}
    \hline
    Attributes & Ped1 & Ped2 & Avenue & Street Scene\\
    \hline
    \hline 
     App & 29.6 / 64.8 & 77.7 / 92.5 & 75.6 / 67.4 & 1.1 / 4.8\\
     \hline
     Motion & 59.2 / 83.7 & 81.6 / 93.3 & 69.0 / 89.0 & 22.6 / 64.1\\
     \hline
     Angle &  40.9 / 70.6  &  70.8 / 88.1  &  66.4 / 89.3  &  24.5 / 65.9 \\
     \hline
     Mag &  60.6 / 90.1  &  70.1 / 88.5  &  59.5 / 91.1  & 16.6 / 50.8 \\
     \hline
     Bkg &  45.5 / 86.4  &  71.7 / 95.2  & 49.9 / 81.5  &  11.9 / 49.8 \\
     \hline
     App+Mot & 61.7 / 88.9 & 87.4 / 97.1 & 69.6 / 86.7 & 23.9 / 65.3\\
     \hline
\end{tabular}
\caption{RBDC / TBDC AUC scores (in \%) of our method when using only appearance, only motion (using angle, magnitude and background pixel predictions combined), each motion component separately and all the features}
\label{ablation1}

\end{table}

\begin{table}[t]
\setlength{\tabcolsep}{12pt}
\centering
\begin{tabular}{| c | c | c |}
    \hline
    Attributes & Ped1 & Ped2 \\
    \hline
    \hline 
     ImageNet & 25.148 / 44.63 & 64.67 / 83.17 \\
     \hline
     Ours & 29.6 / 64.8 & 77.7 / 92.5 \\
     \hline
\end{tabular}
\caption{RBDC / TBDC AUC scores (in \%) of our method when using our pre-trained model versus ImageNet pe-trained model as appearance feature extractor.}
\label{ablation2}
\vspace{-15pt}
\end{table}

We further perform an ablation study on UCSD Ped1 and Ped2 datasets to empirically evaluate the benefit of our appearance model to represent object features versus using ImageNet pre-trained features. For the ImageNet pre-trained model, we use the pre-classification layer output of ResNext-50 as features. 
We present our results in Table \ref{ablation2}. The superiority of our model is most likely due to the loss function used (binary cross-entropy) which allows an image patch to contain zero or multiple object classes.






\section{Discussion and Conclusions}
\label{sec:discuss}

We have presented a novel method for explainable video anomaly localization that has a number of desired properties.  Foremost, the method is accurate and general.  We have shown that it works very well on five different datasets and, in particular, achieves state-of-the-art results on CUHK Avenue, Street Scene and ShanghaiTech.  Setting it apart from almost all previous work, our model is understandable by humans and the decisions that our method makes are explainable.
Finally, because our method does not require a computationally expensive training phase on the nominal data, it is  easy to expand our model when new nominal data becomes available.


