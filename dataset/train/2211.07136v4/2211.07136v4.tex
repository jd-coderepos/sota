\documentclass{bmvc2k}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{wrapfig}
\usepackage{algorithm2e}
\usepackage{dsfont}
\usepackage{color}
\usepackage{comment}
\usepackage{graphics}

\newcommand{\shadow}[1]{}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\grn}[1]{\textcolor{green}{#1}}
\newcommand{\brown}[1]{\textcolor{brown}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

\def\b{\blue}
\def\s{\shadow}
\def\r{\red}
\def\o{\orange}
\def\br{\brown}


\title{C3: Cross-instance guided Contrastive Clustering}

\addauthor{Mohammadreza Sadeghi}{mohammadreza.sadeghi@mcgill.ca}{1,2}
\addauthor{Hadi Hojjati}{hadi.hojjati@mcgill.ca}{1,2}
\addauthor{Narges Armanfard}{narges.armanfard@mcgill.ca}{1,2}

\addinstitution{
Department of Electrical and Computer Engineering, McGill University\\ 
Montreal, QC, Canada\\
}
\addinstitution{
Mila - Quebec AI Institute\\
Montreal, QC, Canada
}

\runninghead{Sadeghi et al.}{C3:Cross-instance guided Contrastive Clustering}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\begin{document}

\maketitle

\begin{abstract}
Clustering is the task of gathering similar data samples into clusters without using any predefined labels. It has been widely studied in machine learning literature, and recent advancements in deep learning have revived interest in this field. Contrastive clustering (CC) models are a staple of deep clustering in which positive and negative pairs of each data instance are generated through data augmentation. CC models aim to learn a feature space where instance-level and cluster-level representations of positive pairs are grouped together. Despite improving the SOTA, these algorithms ignore the cross-instance patterns, which carry essential information for improving clustering performance. This increases the false-negative-pair rate of the model while decreasing its true-positive-pair rate. In this paper, we propose a novel contrastive clustering method, Cross-instance guided Contrastive Clustering (C3), that considers the cross-sample relationships to increase the number of positive pairs and mitigate the impact of false negative, noise, and anomaly sample on the learned representation of data. In particular, we define a new loss function that identifies similar instances using the instance-level representation and encourages them to aggregate together. Moreover, we propose a novel weighting method to select negative samples in a more efficient way. Extensive experimental evaluations show that our proposed method can outperform state-of-the-art algorithms on benchmark computer vision datasets: we improve the clustering accuracy by , , ,  and   on CIFAR-10, CIFAR-100, ImageNet-10, ImageNet-Dogs, and Tiny-ImageNet.
\end{abstract}

\vspace{-4mm}
\section{Introduction}
\label{sec:intro}
\begin{wrapfigure}[6]{r}{0.5\textwidth}
\vspace{-26mm}
  \begin{center}
    \includegraphics[width=1\linewidth]{TSNE_1.pdf}
    \vspace{-10pt}

  \caption{t-SNE visualization of Contrastive Clustering (CC) \cite{CC} and the proposed C3}
    \end{center}
  \label{fig:tsne_cifar}
\end{wrapfigure}
\s{\r{We can Remove the following}
\brown{As a fundamental task in unsupervised machine learning, clustering aims to group similar data points into the same clusters based on a similarity metric in the absence of labels. Clustering algorithms are helpful in many applications, including but not limited to image segmentation \cite{segment}, anomaly detection \cite{anomaly}, medical analysis \cite{medical}, and data retrieval \cite{retriv1,retriv2}, and are often considered an essential pre-processing step in data mining tasks \cite{mining}. It has been an active field of research in machine learning, and a flurry of clustering frameworks has been proposed throughout time. Traditional methods, such as k-means \cite{kmeans} and fuzzy c-means \cite{FCM}, could achieve a promising performance on lower-dimensional datasets \cite{review_old}. Still, they fail to cluster large-scale or high-dimensional data accurately. They assume that input data are already represented by an efficient feature vector, which is generally not valid in the case of high-dimensional data. }}



\s{Following the recent success of deep models in handling unstructured and high dimensional data such as images, researchers have turned their attention to developing deep learning algorithms for clustering \cite{reviewClustermain}. Most early works relied on the power of deep models in learning a generic representation from the input \cite{DEN}. Since this representation is not particularly suited for clustering, this approach yields sub-optimal results. More recently, \textit{deep clustering} methods have been proposed to tackle this issue \cite{DEC}. They jointly train a representation with the clustering task to learn a feature space in which the data can be more explicitly clustered into different groups. As a result, these models managed to achieve a reliable performance on complex computer vision datasets. }

In the past few years, self-supervised learning \cite{Rotate,lin2021inter,tiwari2021self}, particularly contrastive learning, has established itself as a state-of-the-art representation learning algorithm \cite{SimCLR}. They generate a transformed version of the input samples and attempt to learn a representation in which augmentations of the same data point are closer together and further away from other points. They managed to outperform other baselines, particularly in tasks related to computer vision, such as image classification \cite{SimCLR}, image anomaly detection \cite{Hojjati2022SelfSupervisedAD}, and object recognition \cite{Objectrecognition}. Encouraged by these results, several studies have attempted to apply contrastive learning to the clustering task. An early attempt by Li \etal \cite{CC} showed that contrastive clustering could significantly outperform other baselines on benchmark datasets. Despite these improvements, contrastive clustering and the majority of other deep clustering methods do not consider the interrelationship between data samples, which commonly leads to sub-optimal clustering \cite{DCSS}. However, it would be challenging to incorporate information about the cross-relationship of different instances in an unsupervised setting where we do not have access to data labels. As a result, existing unsupervised CC-based algorithms suffer from high false-negative-pair and low true-positive-pair rates because the model cannot effectively take advantage of the degree of similarity of samples.

\s{INCLUDE and describe here that the existing unsupervised CC-based methods suffer from high false-negative-pair and low positive-pair rate. Explain why this is the case... then in the next paragraph say that our C3 method solves it by discovering the samples similarities ....   apply it to the whole paper in abstract, ...  }

In this paper, we propose a groundbreaking technique to discover similarities between samples. Then, we employ these similarities to identify positive and negative pairs more accurately. We design a soft-weighting scheme that allows focusing on the more challenging to cluster data points. To realize this goal, we create a pool of \textit{weighted} sample pairs where higher weights are assigned to the samples closer to the data cluster boundaries. Such a weighting scheme mitigates the influence of easy-to-cluster data, noise, and, more importantly, the existing false-negative-pair issue in the CC-based methods. Also, by incorporating a larger number of positive samples, we improve the true-positive-pair rate. \s{A  DISCUSSION ON INCREASING TPPR MUST BE ADDED HERE AS WELL. } Overall, our method significantly improves the training efficiency of the model and leads to a cluster-friendly representation. In the remainder of this paper, we describe our idea more formally. Then, we carry out a series of extensive analyses to show that our scheme can significantly improve the clustering performance, and we try to explain how it can achieve such enhancement.

We summarize the contribution of this work as follows: \textbf{(1)} we propose a new contrastive loss function to incorporate the newly discovered positive pairs toward learning a more reliable representation space. \textbf{(2)} we propose a novel weighting scheme that aims to separate more challenging data samples, specifically those that are close to cluster boundaries. This improves the representation learning process and greatly impacts the false-negative-pair selection rate.\s{By doing so, we can improve the effectiveness of sample selection and enhance the quality of the learned representation. Our weighting method also reduces the impact of false negative pairs, noisy samples, and anomalies on the latent representation of the trained network, leading to more robust and accurate results and higher true-positive-rate.} \textbf{(3)} by carrying out extensive experiments, we show that our proposed scheme can significantly outperform current state-of-the-art by a significant margin in terms of several clustering criteria. \s{ inserting almost no extra computations, and}This significant improvement results from considering the pairwise data similarities. \textbf{(4)} We offer insight into the behavior of our developed model, discuss the intuition behind how it improves the clustering performance and support them by conducting relevant experiments.

\s{In this paper, we propose an effective technique to discover similarities between samples and employ them in providing a reliable and less ...  }

\s{\brown{\s{ focus on a suitable subset of challenging negative samples using soft weight assignments, i.e. higher weights are assigned to negative samples that are closer to the boundary of clusters. This weighting scheme also reduces the effect of false negatives, noisy samples, and anomalies on the representation}}. \brown{We} train the algorithm's network so that similar instances that form a cluster become closer together  \brown{and be far from instances of other clusters}.} 

\vspace{-6 mm}
\section{Related Works}
\vspace{-2 mm}
\setlength{\parskip}{0pt}
Deep learning-based clustering methods can be categorized into two groups \cite{reviewCluster}: (I) Models that use deep networks for embedding the data into a lower-dimensional representation and apply a traditional clustering such as k-means to the new representation, and (II) Algorithms that jointly train the neural network for extracting features and optimizing the clustering results. In order to achieve a more clustering-friendly representation, previous studies have added regularization terms and constraints to the loss function of neural networks. For example, Huang \etal \cite{DEN} proposed the Deep embedding network (DEN), which imposes a locality preserving and a group sparsity constraint to the latent representation of the autoencoder. These two constraints reduce the inner cluster and increase the inter-cluster distances to improve the clustering performance. In another work, Peng \etal \cite{PARTY} proposed deep subspace clustering with sparsity prior (PARTY) that enhances the clustering efficiency of the autoencoder by incorporating the structure's prior in order to consider the relationship between different samples. Sadeghi and Armanfard \cite{DML} proposed Deep Multi-Representation Learning for Data Clustering (DML), which uses a general autoencoder for instances that are easily clustered along separate AEs for difficult-to-cluster data to improve the performance.
More recent works jointly train the neural network with the clustering objective to further improve the clustering performance. For instance, Deep clustering network (DCN) \cite{DCN} uses k-means objective as the clustering loss and jointly optimizes it with the loss of an autoencoder. Analogously, Deep embedded clustering (DEC) \cite{DEC} first embeds the data into a lower-dimensional space by minimizing the reconstruction loss. Then, it iteratively updates the encoder part of the AE by optimizing a Kullback-Leiber (KL) divergence \cite{KL} loss between the soft assignments and adjusted target distributions. Following the success of DEC, a series of improved algorithms have been developed. For instance, improved deep embedded clustering with local structure preservation (IDEC) \cite{IDEC} jointly optimizes the clustering loss and AE loss to preserve the local structure of data, IDECF \cite{IDECF} adds a fuzzy c-mean network for improving the auxiliary cluster assignment of IDEC during training, and Deep embedded clustering with data augmentation (DEC-DA) \cite{DECDA} applies the DEC method along with the data augmentation strategy to improve the performance. 
Several other methods design auxiliary tasks for learning an efficient representation. E.g., JULE \cite{JULE} applies agglomerative clustering to learn the data representation and cluster assignments. In another algorithm named invariant information clustering (IIC) \cite{IIC}, the mutual information between the cluster assignment of a pair is maximized.
Recently, researchers have turned their attention to self-supervised learning (SSL) models for clustering. For example, MMDC (multi-modal deep clustering) \cite{MMDC} improves the clustering accuracy by solving the proxy task of predicting the rotation. SCAN (semantic clustering by adopting nearest neighbors) \cite{SCAN} first obtains a high-level feature representation using self-supervised learning and then improves the clustering performance by incorporating the nearest neighbor prior.
Contrastive learning is a self-supervised learning paradigm that learns data representation by minimizing the distance between the augmentations of the same sample while pushing them away from other instances. SimCLR \cite{SimCLR} is an example of a contrastive model for learning representation from images that can achieve performance on par with supervised methods. Researchers have increasingly utilized contrastive models for solving tasks such as clustering in the past couple of years. Zhong \etal proposed deep robust clustering (DRC) \cite{DRC} in which a contrastive loss decreases the inter-class variance and another contrastive loss increases the intra-class distance. Contrastive clustering (CC) \cite{CC} improves the clustering performance by jointly performing the instance and cluster-level contrastive learning.
\begin{figure*}[t]
\label{fig11}
  \centering
  \includegraphics[width=0.6\linewidth]{C3_Figure_1.jpg}
  \caption{An overview of the training phase of our proposed C3 method.}
  \vspace{-7mm}
\end{figure*}
\vspace{-5mm}
\section{Method}
\label{sec:method}
Given an unlabelled dataset  and a predefined cluster number parameter , the goal of the clustering problem is to partition  into  disjoint groups. Figure \r{2}. shows the overall scheme of the C3 framework.

\s{To realize this goal, our proposed method follows a two-step training procedure:}
\s{I suggest, at the begging of the paragraph, first discuss figure 2,  then go over the other matters. In this waa the encoder is defined in the fig2.  }

\s{SHOULD n't T be, ? Have you ever used T throughout the text?  and b are not defined}

\s{Notation:} Like other contrastive learning methods, we apply two data augmentations  and , sampled randomly from a pool of transformations, , to form a \textit{true positive pair} (, ) for a sample , where  and . In this paper, we used SimCLR transformation pool \cite{SimCLR}. As is shown in Figure \r{2}. , we use the encoder network  to extract features of augmented samples, i.e,  and . Inspired by CC \cite{CC}, we devise instance-level and cluster-level contrastive networks, denoted by  and , respectively. The instance-level network maps the extracted feature of augmented samples to the latent representation (aka z-space), i.e.  and . The output of the cluster-level network is the cluster assignments of samples to different clusters, i.e.  and . We call the output of the cluster-level network c-space. We first initialize our networks, i.e. , , and  using the CC algorithm.  \s{ MOVE THIS UP:  Hereafter, we call the output of the instance-level network with z-space and the output of the cluster-level network with c-space.}    



\s{We first map the data into a latent representation  using an encoder network , so that . Here,  is the dimension of the latent representation, which is usually less than the input size, and  denotes the parameters of the neural network  and is tuned during the training phase. Ideally, the latent space  should be suitable for clustering while preserving important characteristics of . To learn such representation, we utilize the self-supervised contrastive clustering (CC) \cite{CC} method. }

\s{Like other contrastive learning methods, CC applies two data augmentations  and , sampled randomly from a pool of transformations, , to form a \textit{positive pair} (, ) for a sample , where  and . These transformations preserve the important information of the original sample, and human eyes easily perceive their association. However, their pixel values differ significantly, and by learning to create a link between them, the neural network learns to focus on the important and consistent patterns of the data. \s{In this paper, we use the same augmentations as those employed in CC \cite{CC} namely resized crop, gray-scale, horizontal flip, color jittering, and Gaussian blur.}In the next step, the network  extracts representation of augmented samples, i.e,  and . After extracting representations, we conduct instance-level and cluster-level contrastive learning: The instance-level contrastive learning could be done by pulling together the representation of positive pairs and pushing them away from negative ones. Since no prior label is given in unsupervised settings, we treat each sample as an individual class, and the two augmentations of the same sample will be considered positive. At the same time, the rest of the batch will be negative. More specifically, for a given sample , (, ) forms its sole positive pair, and the rest of the  samples will be considered negative. In practice \cite{SimCLR}, instead of directly applying the contrastive learning on the representation , we first map it to another subspace using a multi-layer perceptron  to obtain  and then minimize the following loss function:}

\s{


}
\s{where  is the temperature parameter that defines the degree of attraction and repulsion between samples. If we normalize s, the similarity metric will become .}

\s{The final loss function is defined as the average of the loss for all positive pairs across the batch:
}
\s{
}
\s{The reason that  is used instead of  in Eq. (\ref{ccloss}), is because minimizing the contrastive loss on the representation  might cause it to drop essential information. Previous studies have also empirically shown that minimizing  leads to better results \cite{SimCLR}.
}

\s{Decoupled from the instance-level contrastive learning network , we train another network  that maps the h-space onto an M-dimensional c-space. Mathematically speaking, if we denote the output of  under the first augmentation as , then . Intuitively, in this subspace, the -th element corresponds to the probability of the sample belonging to the -th cluster.}

\s{If we show the -th column of  by , we can consider the representation of the second augmentation  as its positive pair while leaving the other  columns as negative. Then, similar to the instance-level loss, we define a contrastive loss function to pull the positive pair and push it away from the negatives as follows:}

\s{}

\s{Minimizing the above loss can lead to the trivial solution of mapping most data points to the same cluster. To avoid this, CC minimizes the negative entropy of cluster assignment probabilities defined below in Eq. (\ref{entropy}) where .}
\s{}


\s{The final cluster-level contrastive loss is calculated as:}

\s{
}

\s{The final loss of CC is the sum of the instance-level and cluster-level loss:}
\s{}


\s{As the first step in our method, we initialize our networks by training them to minimize the CC loss. }If we do such initialization process for a sufficient number of epochs, a partially reliable z-space will be obtained. However, the z-space obtained by minimizing the CC loss is sub-optimal for clustering, which results in a large false-negative-pair rate and a low true-positive-pair rate. To mitigate this issue, our method incorporates cross-sample similarities.  \s{REFRE TO THE ISSUE of FPPR and FNPR and that we use cross sample similarities to mitigate the issue and hen the next paragraph that it is unsupervised so we have to use a notion of self-supervision in refining positive and negative pairs... .... the cross-sample relationships in its training phase.}

Since our framework is unsupervised and we do not have access to the data labels to identify samples belonging to the same cluster, we use a notion of self-supervision to refine clusters. We employ the cross-sample similarities in the partially trained z-space to realize the self-supervision concept. The cross-sample similarity is measured by the cosine distance of samples in the z-space. If, for a pair of instances, the similarity is greater than or equal to a threshold , we consider those samples to be similar and pull them closer together by minimizing the loss function  defined below:
{\small
}
\s{}
{\small
}
where  denotes the indicator function.\s{Note that since s are normalized, .} Analogous to , we define  that considers similarity of  and other samples in the batch.
Furthermore, in the denominator of the proposed loss function, we included  to consider higher weights for the samples that are neither close together nor far from each other. In this way, we realize the goal of decreasing the false negative pair selection rate as, in the traditional CC-based methods, all augmented samples in the batch are equally considered when forming negative pairs, regardless of the possibility of them belonging to the same cluster and the difficulty of the samples to cluster.

We assume that the weight terms are given when minimizing the C3 loss defined in \eqref{C3loss}. To obtain an optimum value for a weight term , we propose solving the below optimization problem while the networks are frozen. The first term of the below optimization problem is defined based on our motivation to assign a very low weight to too close and too far away samples and, instead, let the remaining samples, which are more probably located on the cluster boundaries, take higher weights. The second term is to avoid the trivial solution of assigning a weight equal to one to the sample providing the maximum value of . To avoid instability, we include the constraint by which the summation of all weights must be equal to 1. 
\vspace{-3mm}
{\small
}
\s{as a weighting term that assigns weight to \o{the sample pairs}\s{WRONG: the negative samples -- WE MUST NOT CALL THEM NEGATIVE PAIRS AS IT INCLUDES POSITIVE PAIRS AS WELL.} based on their importance in determining the boundaries of the separation. \brown{For finding , we detach z-space from the gradient graph of our networks; hence, we do not update the parameters of our model with respect to .} In this weighting scheme, the network should not repel the samples that are very close and are deemed \o{to be} positive, i.e. , and, on the other hand, should not focus on maximizing the distance between the samples that are already mapped to \s{a further ??? OR farther ??? } \o{two far points in the z-space}\s{  representation}, i.e. . Instead, it should focus on the points that are near the boundary. For finding , we \o{suggest} to solve the following optimization problem that aims at \o{mitigating} \s{relaxing ?? IT IS NOT A GOOD TERM} the effect of\s{false ?????? ARE U Sure? **} false-negative-pair selection and \o{down-weighting} samples that are \o{already} mapped to far locations in the z-space.\s{DISCUSS a bit that WHY this optimization problem?? e.g. For finding , we need to solve an optimization problem that is not unique and aims at reducing the impact of false negative samples, i.e  
, and learning samples  making the **** avoid collapsing issue an, ...  then say that we suggest below while not unique...}}
In the above equation,  is a hyperparameter,  is the set of all weights, and  is the entropy function.\s{ that avoids converging to a trivial solution \b{where one weight that corresponds to the maximum value of  converges to one and other weights converges to zero}. The first term aims to assign lower weights to similar samples (i.e. ) and samples that are far enough from each other (i.e. )}\s{ REVISE the following based on the new writings we discussed on low TPR and high FNR: By solving this optimization problem, we relax the effect of false-negative-pairs, noisy samples, and anomalies in the learned representation.}
We solve the optimization problem defined in \eqref{eq11} using the Lagrange multiplier technique as below:
{\small
}
\vspace{-5mm}
{\small }
Where  is the Lagrange multiplier. By substituting \eqref{eq13} into the constraint of \eqref{eq11}, we have:
{\small }
If we substitute \eqref{eq14} to \eqref{eq13}, we have the final values for  as below:
{\small }

\s{The reason we use the z-space for similarity measurements rather than the h-space is mainly that computing the inner products in the z-space requires fewer mathematical operations as the dimensions of z are lower than that of h.}
\s{REVISE this paragrapgh TO ACCURATELY INCLUDE WHAT WE ACTually have ...}When comparing the loss function of other contrastive clustering algorithms, such as CC, with the loss of C3 (shown in Eq. \eqref{C3loss}), several differences become apparent. Firstly, while other contrastive algorithms typically only allow one positive pair to appear in the numerator, C3 enables the networks to be trained by considering a much larger number of positive pairs. This leads to more efficient training and better performance. Secondly, C3 adopts a weighting scheme when creating the negative pairs; the scheme assigns lower weights to samples that are either too close or too far from each other while assigning higher weights to those that are more in the mixing cluster areas\s{located on the boundaries of clusters}. This approach reduces the impact of false positive, noisy, and anomaly samples on the learning of representations. \s{*** Isn't this following paragraph redundant with the first and second items above?** remove if agree yes.*** Additionally, C3 \b{improves the clustering accuracy by simultaneously focusing on the negative samples that are located near the cluster boundaries (effect of the denominator) and increasing the number of true-positive pairs (effect of the numerator).}}
\s{The numerator Eq. (\ref{C3loss}) confirms that C3 allows the networks to get trained considering much more positive pairs, while other contrastive algorithms commonly allow only one positive pair to appear in the numerator. This is indeed a valuable property of C3, as considering all batch samples as negative samples is a misleading assumption due to the fact that samples of one cluster should indeed be considered as positive pairs and pulled together. It is worth noting that, from the number of required operations point of view, computing the C3 loss needs no extra computations compared to the vanilla contrastive loss -- C3 only needs to find positive pairs and perform a few more summations, in the numerator, equal to the number of extra positive pairs. }


\s{\begin{algorithm}[t]

\KwIn{Hyperparameter , dataset , transformation pool , initializing epochs , training epochs , batch size  and , learning rate  and , temperature parameters , number of clusters ,  and .}
\KwOut{Cluster assignment }
\caption{\r{C3 Pseudo code}}
\label{pcode}
\tcp{Training}
Apply contrastive clustering (CC) algorithm for  epochs to train \\
\For{epoch  \KwTo }
{
Sample a mini-batch  from \\
Sample two augmentations \\
Apply  to  to get:\\
\Indp \\
\Indm
Compute the instance representation by:\\
\Indp  \\
 \\
\Indm
Compute the loss  and  through Eq. \ref{C3loss}\\
Compute the overall loss  using Eq. \ref{totalloss}\\
Update  to minimize 
}
\tcp{Test}
\For{ in }
{
Calculate the latent representation by\\
\Indp \\
\Indm
Find the cluster assignment by\\
\Indp 
}
\end{algorithm}}

\s{In summary, in our framework, we initialize the networks by CC. This is mainly to train  and obtain a partially reliable z-space that will be used in the following step when creating more positive pairs to be employed in the C3 loss. In the second step, we boost the z-space ability in providing reliable representations by identifying similar samples and then bringing them closer together to form clusters with more explicit boundaries. Pseudocode of the proposed C3 method is presented in Algorithm \ref{pcode}.}
\vspace{-3mm}
\section{Experiments and Discussions}
\vspace{-3 mm}
\label{sec:experiments}
In this section, we demonstrate the effectiveness of our proposed scheme by conducting rigorous experimental evaluations.
We evaluated our method on five challenging computer vision benchmark datasets: CIFAR-10, CIFAR-100 \cite{Cifar}, ImageNet-10, ImageNet-Dog \cite{Imagenet}, and Tiny-ImageNet \cite{Tinyimage}. 
For CIFAR-10 and CIFAR-100, we combined the training and test splits. Also, for CIFAR-100, instead of 100 classes, we used the 20 super-classes as the ground-truth. To evaluate the performance, we use three commonly-used metrics in clustering namely clustering accuracy (ACC), Normalized Mutual Information (NMI), and Adjusted Rand Index (ARI) \cite{reviewCluster}. 
\s{\subsection{Implementation Details}}

For the sake of fair comparison, for all datasets, we used ResNet34 \cite{resnet} as the backbone of our encoder , which is the same architecture that previous algorithms have adopted. We set the dimension of the output of the instance-level projection head  to 128, for all datasets. The output dimension of the cluster-level contrastive head is set to the number of classes, M, in each dataset. All networks are initialized by the CC \cite{CC} algorithm with the hyperparameters suggested by its authors. The Adam optimizer with an initial learning rate of   and batch size of  is used for C3. All networks are trained for 20 epochs.\s{Like CC, we set the temperature parameters of instance-level and clustering-level networks to  and , respectively. Like CC, for training the first step, we used Adam optimizer \cite{kingma:adam} with  , batch size of  and  epochs. Also, the Adam optimizer with an initial learning rate of  and batch size of  is used in the second step, and the networks are trained for  epochs.} The experiments are run on NVIDIA TESLA V100 32G GPU.
\vspace{-4mm}
\subsection{Comparison with State-of-the-art}
\label{sec:sota}
\begin{table*}[!t]
\caption{Clustering performance of different methods.}
\vspace{-2mm}
\label{tab:results}
\setlength{\tabcolsep}{0.8pt}
\begin{center}
{\scriptsize
\makebox[\textwidth][c]{
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|cc}
    \toprule 
      &\multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{3}{c|}{ImageNet-10}& \multicolumn{3}{c|}{ImageNet-Dogs}& \multicolumn{3}{c|}{Tiny-ImageNet}\\
     \cline{2-18}
    Algorithm & NMI  & ACC & ARI & NMI  & ACC & ARI &NMI  & ACC & ARI &NMI  & ACC & ARI &NMI  & ACC & ARI \\
    \midrule
    K-means \cite{kmeans}&0.087&0.229&0.049&0.084&0.130&0.028&0.119&0.241&0.057&0.055&0.105&0.020&0.065&0.025&0.005\\
    SC \cite{SC}&0.103&0.247&0.085&0.090&0.136&0.022&0.151&0.274&0.076&0.038&0.111&0.013&0.063&0.022&0.004\\
    AC \cite{AC}&0.105&0.228&0.065&0.098&0.138&0.034&0.138&0.242&0.067&0.037&0.139&0.021&0.069&0.027&0.005\\
    NMF \cite{nmf}&0.081&0.190&0.034&0.079&0.118&0.026&0.132&0.230&0.065&0.044&0.118&0.016&0.072&0.029&0.005\\
    AE \cite{ae}&0.239&0.314&0.169&0.100&0.165&0.048&0.210&0.317&0.152&0.104&0.185&0.073&0.131&0.041&0.007\\
    DAE \cite{dae}&0.251&0.297&0.163&0.111&0.151&0.046&0.206&0.304&0.138&0.104&0.190&0.078&0.127&0.039&0.007\\
    DCGAN \cite{dcgan}&0.265&0.315&0.176&0.120&0.151&0.045&0.225&0.346&0.157&0.121&0.174&0.078&0.135&0.041&0.007\\
    DeCNN \cite{decnn}&0.240&0.282&0.174&0.092&0.133&0.038&0.186&0.313&0.142&0.098&0.175&0.073&0.111&0.035&0.006\\
    VAE \cite{vae}&0.254&0.291&0.167&0.108&0.152&0.040&0.193&0.334&0.168&0.107&0.179&0.079&0.113&0.036&0.006\\
    JULE \cite{JULE}&0.192&0.272&0.138&0.103&0.137&0.033&0.175&0.300&0.138&0.054&0.138&0.028&0.102&0.033&0.006\\
    DEC \cite{DEC}&0.275&0.301&0.161&0.136&0.185&0.050&0.282&0.381&0.203&0.122&0.195&0.079&0.115&0.037&0.007\\
    DAC \cite{dac}&0.396&0.522&0.306&0.185&0.238&0.088&0.394&0.527&0.302&0.219&0.275&0.111&0.190&0.066&0.017\\
    ADC \cite{adc}&-&0.325&-&-&0.160&-&-&-&-&-&-&-&-&-&-\\
    DDC \cite{DDC}&0.424&0.524&0.329&-&-&-&0.433&0.577&0.345&-&-&-&-&-&-\\
    DCCM \cite{DCCM}&0.496&0.623&0.408&0.285&0.327&0.173&0.608&0.710&0.555&0.321&0.038&0.182&0.224&0.108&0.038\\
    IIC \cite{IIC}&-&0.617&-&-&0.257&-&-&-&-&-&-&-&-&-&-\\
    PICA \cite{PICA}&0.591&0.696&0.512&0.310&0.337&0.171&0.802&0.870&0.761&0.352&0.352&0.201&0.277&0.098&0.040\\
    GATCluster \cite{gatcluster2020}&0.475&0.610&0.402&0.215&0.281&0.116&0.609&0.762&0.572&0.322&0.333&0.200&-&-&-\\
    CC \cite{CC} &0.678*&0.770*&0.607*&0.421*&0.423*&0.261*&0.850*&0.893*&0.811*&0.436*&0.421*&0.268*&0.331*&0.137*&0.062*\\
    EDESC \cite{edesc}&0.627&0.464&-&0.385&0.370&-&-&-&-&-&-&-&-&-&-\\
    \hline
    C3 (Ours) &\textbf{0.743}&\textbf{0.836}&\textbf{0.703}&\textbf{0.435}&\textbf{0.456}&\textbf{0.274}&\textbf{0.905}&\textbf{0.943}&\textbf{0.860}&\textbf{0.447}&\textbf{0.434}&\textbf{0.280}&\textbf{0.335}&\textbf{0.140}&\textbf{0.064}\\
\bottomrule
\end{tabular}
}}
\end{center}
\end{table*}

\begin{figure*}[t]
\vspace{-6mm}
  \centering
  \includegraphics[width=0.8\linewidth]{TSNE_2.pdf}
  \caption{t-SNE visualization of clusters learned by the CC and C3 methods.}
  \vspace{-5mm}
  \label{fig:tsne}
\end{figure*}


\begin{wrapfigure}{R}{0.5\textwidth}
\vspace{-4mm}
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm]{cifar10.png}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm]{cifar100.png}}
\end{minipage}
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0cm]{image10.png}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=3.0 cm]{imagedog.png}}
\end{minipage}
\caption{C3's performance vs epochs.}
\label{fig:convergence}
\end{wrapfigure}



Table \ref{tab:results} shows the results of our proposed method on benchmark datasets, compared to state-of-the-art and some common traditional clustering methods. For CC, we run the code provided by its authors for all datasets, and the results are indicated by (*). As is evident in this table, our proposed method significantly outperforms all other baselines in all datasets. Quantitatively, comparing to the second best algorithm (i.e. CC), C3 improves the ACC by \s{ARE THE FOLLOWING NUMBERS UPDATED BASED ON THE CURRENT TABLE?? }, , ,  and ), the NMI by , , ,  and , and the ARI by , , ,  and , respectively on CIFAR-10, CIFAR-100, ImageNet-10, ImageNet-Dogs, and Tiny-ImageNet. The main difference between our framework and other baselines, such as CC, is that we exploit an additional set of information, i.e. the similarity between samples, to further enhance the learned representation for clustering. In our approach, we increase the number of positive samples, which improves the true-positive-pair rate, and put more weight on clustering the samples that are more probably located on the boundary of the clusters, which leads to a decrease in the false-negative-pair selection rate. We believe this is the main reason our method's performance is superior compared to the baselines.

As opposed to the CC method that misses the global patterns present in each data cluster, C3 correctly tries to consider such patterns (at least partially) by employing cross-instance data similarities.\s{and assigning lower weights to false negative samples and samples that are located far enough from each other in the z-space.}\s{ AND ****FILL***.} Looking at \eqref{C3loss} and \eqref{eq11}, one can infer that C3 implicitly reduces the intra-cluster distance while maximizing the inter-cluster distance, which is what an efficient grouping technique would do in the presence of the data labels. This can be confirmed by visualizing the clusters before and after applying the C3 loss. As Figures \ref{fig:tsne_cifar} and \ref{fig:tsne} show, the samples are fairly clustered after initialization with CC\s{at the end of the first step training using the CC loss,}, i.e. before the start of training using the C3 loss. However, some of the difficult clusters are mixed in the boundaries. After initialization, clusters \s{obtained by minimizing only the CC loss} are expanded with a considerable number of miss-clustered data. However, after training with the proposed C3 method, we observe that the new cluster space is much more reliable, and individual clusters are densely populated while being distant from each other.



\subsection{Convergence Analysis}

Results of section \ref{sec:sota} depict the superiority of our proposed scheme. Now, we analyze C3's convergence and the computational complexity to evaluate at what cost it makes such an improvement over other baselines. We plotted the trend of clustering accuracy and NMI for four datasets during the training epochs in Figure \ref{fig:convergence}. We can readily confirm that although we are just training the C3 step for  epochs, the graphs quickly converge to a settling point, which corresponds to the peak performance. Also, we can observe that both ACC and NMI are improved throughout the C3 training phase in all datasets. The performance at  corresponds to the clustering performance after initialization with CC\s{final performance of the CC algorithm}. These figures clearly show that C3 improves its clustering quality and justifies the qualitative results shown in Section \ref{sec:sota}.


One may naively \s{(wrongly)}think that the better performance of C3 \s{compared to CC} is because it is being trained for 20 more epochs; note that in all our experiments, as suggested by the CC authors,  we trained the CC algorithm networks for 1000 epochs. We train the C3 networks for 1020 epochs.\s{i.e. 1000 epochs for the first step and 20 epochs for the second step.} We reject this argument and support it by training the CC networks for the same number of epochs as what the C3 is trained for, i.e. 1020 epochs. We observe that no improvement is obtained for CC when trained for an extra 20 epochs. \s{This is while when we keep training the networks with the C3 loss for only 20 more epochs, significant improvements are observed. This shows that the superior performance of C3 is not because of the extra  epochs but because its objective function helps the network discover patterns that are complementary to those that CC extracts.} The result of such an experiment on CIFAR-10 is shown in Figure \ref{fig:vscc}.

\subsection{How does C3 loss improve the clusters?}
\label{sec:improve}







\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-9mm}
    \centering
    \includegraphics[scale=0.25]{Epochvscc_1.png}
    \vspace{-5mm}
    \caption{Performance of CC and C3 after 1020 epochs.}
    \label{fig:vscc}
\end{wrapfigure}

As we saw in Figure \ref{fig:tsne}, the improvement that C3 achieves is mainly because it is able to reduce the distance between instances of the same cluster while repelling them from other clusters. We can justify this observation by considering the loss function of C3, i.e. Eq. (\ref{C3loss}). In this function, the term  indicates that if the cosine similarity of two samples is greater than , they should be moved further close to each other.
\begin{wrapfigure}[9]{}{0.5\textwidth}
  \centering
  \vspace{-6mm}
  \includegraphics[scale=0.25]{Loss-pos_pairs.pdf}
  \caption{Plot of loss and number of positive pairs versus epoch.}
  \vspace{-4mm}
  \label{fig:pairs}
\end{wrapfigure}
At the beginning of training with the C3 loss, since the networks are initiated with CC,\s{, since the z-space is initiated with the \r{THIS PARAGRAPH NEEDS REVISION}CC obtained z-space,} we can assume that the points that are very similar to each other have a cosine distance, in the z-space, less than threshold , so they will become further close by minimizing the C3 loss. For instance, take two points  and  with a cosine similarity larger than , and assume that  has a similarity greater than  with , but its similarity with  is smaller than . Therefore, according to the loss function,  and , as well as  and , are forced to become closer, but it is not the case for  and . However, these two points will also implicitly move closer to each other because their distance to  is reduced. As the training continues, at some point, the similarity of  and  also may pass the threshold . Therefore, as the similar pairs move closer to each other during the training, a series of new connections will be formed, and the cluster will become denser. To support this hypothesis, we plotted the average of the loss function and the average number of positive pairs of each data sample in Figure \ref{fig:pairs}-a and \ref{fig:pairs}-b, respectively. We can observe that the number of positive pairs exponentially increases during the training until it settles to form the final clusters. Corresponding to this exponential increase, we can see that the loss is decreasing, and the network learns a representation in which clusters are distanced from each other while samples of each cluster are packed together.

We can also deduct from this experiment that the number of positive pairs is also related to the number of classes in each dataset. For example, if we have an augmented batch size of , for Tiny-ImageNet that has 200 classes, we expect to have  positive pairs per sample which is very close to  and it is the reason that we do not see the same sharp increasing trend as other datasets in Tiny-ImageNet.
\vspace{-5mm}
\subsection{Effect of Hyperparameter  and }
\vspace{-2mm}

Our method, C3, introduces two new hyperparameter  and .  is a threshold for identifying similar samples. Throughout the experiments, we fixed , which yielded consistent results across datasets. Now, we carry out an experiment in which we change  and record the performance. Note that since , we can technically change  from -1 to 1. Intuitively, for a small or negative value of , most points in the z-space will be considered similar, and the resulting clusters will not be reliable. Therefore, in our experiment, we change  from 0.4 to 0.9 in 0.1 increments for CIFAR-10. For Tiny-ImageNet, as there are lots of clusters, we set . We then plot the accuracy, NMI, average loss, and the average of positive pairs per sample. The graphics are shown in Figure \ref{fig:zeta}.

In CIFAR-10 experiments, in Figure \ref{fig:zeta}-a and Figure \ref{fig:zeta}-b, we see that for , accuracy and NMI are indeed decreasing during the C3 training. This is because this value of  is too lenient and considers the points not in the same cluster to be similar. We can confirm this explanation by looking at Figure \ref{fig:zeta}-d. We can see that for smaller s, we will have more average positive pairs per sample. As we increase the , we can see that the performance begins to improve. For larger values such as , we can see that the performance does not significantly change during the training. This is because  is a strict threshold, and if we look at the number of positive pairs, only a few instances are identified as similar during the training.
 
 \begin{figure*}[t!]
  \centering
  \includegraphics[scale=0.18]{changing_zeta.png}
  \vspace{-2mm}
  \caption{Performance and behavior of C3 for different values of , for CIFAR 10 (top row) and Tiny-ImageNet (bottom row).} \vspace{-6mm}
  \label{fig:zeta}
\end{figure*}

Comparing the results of CIFAR-10 and Tiny-ImageNet experiments shows that the value of  also depends on the number of clusters. Since we have 200 classes in Tiny-ImageNet, a smaller value of  might yield two or more clusters merging together and this would decrease the accuracy. Therefore, we should choose a more strict threshold such as  or  to improve. In Figure \ref{fig:zeta}-c and Figure \ref{fig:zeta}-g, the average loss plot also conveys interesting observations about the behaviour of . We can see that for smaller values, the loss is exponentially converging to the minimum, but for larger , the rate is much slower. This can be due to the fact that a smaller  considers most points to be similar and of the same class, and therefore, it can yield the trivial solution of considering all points to be similar and mapping them into one central point. In the extreme case of , C3 considers a few samples as positive pairs, and therefore, we will not have any major improvement. In contrast, if we set , the loss considers all points to be positive and the numerator and denominator of Eq. (\ref{C3loss}) become equal. Therefore, the loss function becomes zero and the network does not train. Following the above discussion, we suggest a value like , which is a good balance. However, the choice of  might be influenced by the number of clusters in the data. If we have a large number of clusters, it would be better to choose a large . On the other hand, if the data has a small number of clusters, a smaller  (but not too small) is preferred since it trains faster. In our experiments, we set  unless in Tiny-ImageNet which has 200 classes where we used .
\begin{wrapfigure}[8]{R}{0.5\textwidth}
    \centering
    \vspace{-8mm}
    \includegraphics[scale = 0.15]{Changing_gamma.pdf}
  \caption{Performance of C3 for different values of , for CIFAR 10.}
  \vspace{-5mm}
  \label{fig:gamma}
\end{wrapfigure}


Figure \ref{fig:gamma} illustrates the impact of hyperparameter  on the performance C3. For very low values of  (i.e., ), all weights converge to the same value of . Conversely, for very high values of  (i.e., ), the effect of entropy in Eq. \eqref{eq11} is neglected, leading to a trivial solution where our weighting function in Eq. \eqref{eq:weighting} selects one negative sample having the highest value of  to minimize the first term in Eq. \eqref{eq11}. \s{WHAT??? Not clear. REVISE the following... one weight corresponding to the highest value of  converges to one, and all other weights converge to zero. }\s{Our experiments show that the best value of  for the CIFAR-10 dataset is 0.1}In all our experiments for all the datasets,  is set to 0.1 though a better performance may be obtained if we fine-tune it per dataset.\s{ and we do not change its value for other datasets.} Overall, our results demonstrate the importance of selecting an appropriate value for  to optimize the performance of our proposed method.
\vspace{-6mm}
\section{Conclusion}
\vspace{-3mm}
In this paper, we proposed C3, an algorithm for contrastive data clustering that incorporates the similarity between different instances to form a better representation for clustering. We experimentally showed that our method could significantly outperform the state-of-the-art on five challenging computer vision datasets. In addition, through additional experiments, we evaluated different aspects of our algorithm and provided several intuitions on how and why our proposed scheme can help in learning a more cluster-friendly representation. The focus of this work was on image clustering, but our idea can also be applied to clustering other data types, such as text and categorical data, in future works.



\bibliography{egbib}
\end{document}
