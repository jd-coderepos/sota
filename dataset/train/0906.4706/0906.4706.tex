\documentclass[12pt]{article}

\usepackage[paper=a4paper,dvips,top=1.5in,left=1.5in,right=1.5in, foot=1cm,bottom=1.8in]{geometry}


\setlength{\parindent}{0em}


\usepackage{amsmath, amssymb, amsfonts, amsthm}

\usepackage[T1]{fontenc}



\usepackage[ruled, vlined]{algorithm2e}
\setlength{\algomargin}{0.1cm}



\usepackage{graphics}












\def\N{{\rm I\mkern-3mu N}}
\def\R{{\rm I\mkern-3mu R}}	
\def\E{{\rm I\mkern-3mu E}}

\def\Exp{\qopname\relax m{E}}


\newcommand{\runtime}[3]{}


\def\Violators{{\sf V}}
\def\Extremes{{\sf X}}
\def\Basis{{\sf B}}


\newcommand{\Proof}[1]{\noindent \textbf{Proof} #1.~}\newsavebox{\smallProofsym}\savebox{\smallProofsym}{\begin{picture}(7,7)\put(0,0){\framebox(6,6){}}\put(0,2){\framebox(4,4){}}\end{picture}}
\newcommand{\smallpop}[1]{\mbox{} \hfill #1~~\usebox{\smallProofsym}\!\!\!\!\!\!\ }\newenvironment{theProof}[1][\hspace{-0.18cm}]{\Proof{#1}}{\smallpop{}\medskip }

\newcommand{\satisf}{\emph{satisfiable}}
\newcommand{\unsatisf}{\emph{unsatisfiable}}

\newsavebox{\smallEndsym}\savebox{\smallEndsym}{\begin{footnotesize}\end{footnotesize}}
\newcommand{\smalleop}[1]{\mbox{} \hfill #1~~\usebox{\smallEndsym}\!\!\!\!\!\!\ }
\newsavebox{\smallDefsym}
\savebox{\smallDefsym}{\begin{footnotesize}\end{footnotesize}}
\newcommand{\smalldop}[1]{\mbox{} \hfill #1~~\usebox{\smallDefsym}\!\!\!\!\!\!\ }

\newtheorem{theorem2}{Theorem}[section]
\newenvironment{theorem}{\begin{theorem2}}{\end{theorem2}}
\newtheorem{lemma2}[theorem2]{Lemma}
\newenvironment{lemma}{\begin{lemma2}}{\end{lemma2}}
\newtheorem{corollary2}[theorem2]{Corollary}
\newenvironment{corollary}{\begin{corollary2}}{\end{corollary2}}
\newtheorem{definition2}[theorem2]{Definition}
\newenvironment{definition}{\begin{definition2}}{\end{definition2}}
\newtheorem{claim2}[theorem2]{Claim}
\newenvironment{claim}{\begin{claim2}}{\end{claim2}}
\theoremstyle{remark}
\newtheorem{observation}[theorem2]{Observation}
\newtheorem{remark2}{Remark}[section]
\newenvironment{remark}{\begin{remark2}}{\end{remark2}}
\newtheorem{question}[theorem2]{Question}
\newtheorem{fct}{Fact}[section]
\newtheorem{example2}{Example}[section]
\newenvironment{example}{\begin{example2}}{\end{example2}}



\let\newEnumerate=\enumerate
\renewenvironment{enumerate}{\newEnumerate\itemsep=0.0cm}{\endlist}
\let\newItemize=\itemize
\renewenvironment{itemize}{\newItemize\itemsep=0.0cm}{\endlist}

\newcommand{\braces}[1]
{
	\left\{ #1 \right\} 
}
\newcommand{\brackets}[1]
{
	\left[ #1 \right]
}
\newcommand{\length}[1]
{
	\ell( #1 )
}


\newcommand{\satequiv}{\ensuremath{\mbox{~}\mathop{\equiv}^{\mbox{\tiny SAT}}}}



\newcommand{\true}
	{\top}
\newcommand{\false}
	{\bot}
\newcommand{\ngt}[1]		{\overline{#1}}

\newcommand{\emptyclause}
	{\oslash}
\newcommand{\trueclause}
	{\mathbf{1}}
\newcommand{\emptyformula}
	{\{\}}

\newcommand{\ass}
	{\alpha}
\newcommand{\bass}
	{\beta}
\newcommand{\cass}
	{\gamma}

\newcommand{\variables}
		{\mathrm{vbl}}
\newcommand{\Variables}[1]
	{\variables(#1)}
\newcommand{\VariablesP}[1]
	{\variables(#1)}
\newcommand{\restr}[2]
	{#1^{[#2]}}
\newcommand{\repl}[2]
	{#1{\left\langle #2\right\rangle}}

\newcommand{\SAT}		{{\mathrm{sat}}}
	\newcommand{\SAt}		{{\mathrm{sa{\mbox{\footnotesize }}}}}
\newcommand{\Sat}[2]
	{\SAT_{#1}(#2)}
\newcommand{\SatP}[2]
	{\SAT_{#1}(#2)}
\newcommand{\MSat}[2]
	{\widehat{\SAt}_{#1}(#2)}
\newcommand{\MSatP}[2]
	{\widehat{\SAt}_{#1}(#2)}
	\newcommand{\UNSAT}		{\overline{\SAt}}
\newcommand{\Unsat}[2]		{\UNSAT_{#1}(#2)}
\newcommand{\UnsatP}[2]		
	{\UNSAT_{#1}(#2)}
\newcommand{\NP}{{\cal N\!P}}
\newcommand{\FP}[1]{{\cal FP}_{\!#1}}
\newcommand{\PP}{{\cal P}}

\newcommand{\collist}[1]
	{{{L_{#1}}}}
\newcommand{\collistprime}[1]
	{L'_{#1}}
\newcommand{\collistprimeprime}[1]
	{L''_{#1}}
\newcommand{\col}
	{{{\chi}}}		\newcommand{\Col}
	{C} 

\newcommand{\vol}[2]
	{{\mathrm{vol}}(#1,#2)}

\newcommand{\ProblemName}[1]{{\sc #1}}
\newcommand{\Problem}[3]{\begin{center}
\begin{minipage}{0.85\textwidth}
\centerline{\ProblemName{#1}}
{\rm Input:} {\it #2}\\
{\rm Output:} {\it #3}
\end{minipage}
\end{center}}


\newcommand{\PROCctwos}[1]
	{\Function{c2s}{#1}}
\newcommand{\PROCfibctwos}[1]
	{\Function{fib\_c2s}{#1}}
\newcommand{\PROCcthrees}[1]
	{\Function{c3s}{#1}}
\newcommand{\PROCfibcthrees}[1]
	{\Function{fib\_c3s}{#1}}
\newcommand{\PROCuc}[1]
	{\Function{uc}{#1}}
\newcommand{\PROCuctwos}[1]
	{\Function{uc\_2s}{#1}}
\newcommand{\PROCrf}[1]
	{\Function{rf}{#1}}
\newcommand{\PROCsb}[1]
	{\Function{sb}{#1}}
\newcommand{\PROCcov}[1]
	{\Function{cov}{#1}}
\newcommand{\PROCrfb}[1]
	{\Function{rfb}{#1}}
\newcommand{\PROCrs}[1]
	{\Function{sch}{#1}}
\newcommand{\PROCppz}[1]
	{\Function{ppz}{#1}}
\newcommand{\PROCdec}[1]
	{\Function{decode}{#1}}
\newcommand{\PROCenc}[1]
	{\Function{encode}{#1}}






\newcommand{\atmost}[1]{}


\newcommand{\rest}[2]{#1_{|#2}}


\newcommand{\res}{\mathrm{res}}
\newcommand{\resolvent}[2]{#1 \bullet #2}
\newcommand{\poly}{\mathrm{poly}}

\newcommand{\op}{\circ}

\newcommand{\bigleftrightarrow}{\mbox{\LARGE }}

\newcommand{\Ass}{A}




\newcommand{\neigh}[2]{\Gamma_{#1}(#2)}

\newcommand{\prev}[2]{{\mathrm{pre}}_{#1}(#2)}


\newcommand{\mycaption}[2]{
	\begin{center}\begin{minipage}{0.95\textwidth}\begin{sl}
	\caption{#1}{#2}
	\end{sl}\end{minipage}\end{center}
	}

\newcommand{\blank}{\sqcup}
\newcommand{\lep}{\le_{\text{\rm P}}}
\newcommand{\nb}[1]{\text{\sf nb}(#1)}
\newcommand{\Vector}[1]{\mathbf{#1}}
\newcommand{\Formula}[1]{\text{\sf #1}}
 \renewcommand{\arraystretch}{1.15}

\def\stageI{\texttt{GA}}
\def\stageII{\texttt{SA}}
\def\stageIIforever{\texttt{SA\_forever}}
\def\stageIII{\texttt{BFA}}


\begin{document}

\date{June 25, 2009}
\title{Clarkson's Algorithm for Violator Spaces}
\author{Yves Brise, Bernd G\"artner\\
{\small Swiss Federal Institute of Techology (ETHZ)}\\{\small 8092 Zurich, Switzerland}\\{\small 
\texttt{ybrise | gaertner@inf.ethz.ch}}\\\\
}

\maketitle
\begin{center}
	\textbf{Abstract}\\
\end{center}
Clarkson's algorithm is a two-staged randomized algorithm for solving linear programs. 
This algorithm has been simplified and adapted to fit the framework of LP-type problems. In this
framework we can tackle a number of non-linear problems such as computing the smallest
enclosing ball of a set of points in .
In 2006, it has been shown that the algorithm in its original form works for violator spaces too, which 
are a proper
generalization of LP-type problems. It was not clear, however, whether previous simplifications of the 
algorithm carry over to the new setting.


In this paper we show the following theoretical results:  It is shown, for the first time, that Clarkson's
second stage can be simplified.   The previous simplifications of Clarkson's first stage carry
over to the violator space setting.  
 Furthermore, we show the equivalence of violator spaces and partitions of the hypercube by hypercubes.

~\\
\textbf{Keywords:} Clarkson's Algorithm, Violator Space, LP-type Problem, Hypercube Partition



\section{Introduction}
\paragraph{Clarkson's algorithm.}
Clarkson's randomized algorithm \cite{c-lvali-95} is the earliest
practical linear-time algorithm for linear programming with a fixed
number of variables.  Combined with a later algorithm by Matou{\v{s}}ek,
Sharir and Welzl \cite{MSW}, it yields the best (expected) worst-case
bound in the unit cost model that is known today.  The combined
algorithm can solve any linear program with  variables and 
constraints with an expected number of  arithmetic operations \cite{GaerWel1}.

Clarkson's algorithm consists of two primary stages, and it requires
as a third stage an algorithm for solving small linear programs with
 constraints. The first two stages are purely combinatorial and use
very little problem-specific structure.  Consequently, they smoothly
extend to the larger class of \emph{LP-type problems} \cite{MSW},
with the same running time bound as above for concrete problems in this
class, like finding the smallest enclosing ball of a set of  points
in dimension  \cite{GaerWel1}.

Both primary stages of Clarkson's algorithm are based on random sampling and
are conceptually very simple. The main idea behind the use of randomness
is that we can solve a subproblem subject to only a small number of
(randomly chosen) constraints, but still have only few (of all) constraints
that are violated by the solution of the subproblem.
 However, some extra machinery was
originally needed to make the analysis go through.  More precisely,
in both stages there needed to be a check that the each individual
random choice was ``good'' in a certain sense. Then in the analysis
one needed to make the argument that the bad cases do not
occur too often. For the first
stage, it was already shown by G\"artner and Welzl that these extra checks
 can be removed \cite{GWSampl01}. The result is what we call
the \emph{German Algorithm} below. In this paper, we do the removal
also for the second stage, resulting in the \emph{Swiss Algorithm}.
(The names come from certain aspects of German and Swiss mentality
that are reflected in the respective algorithms.) We believe that the
German and the Swiss Algorithm together represent the essence of
Clarkson's approach.

\paragraph{Violator spaces.}
G\"artner, Matou\v{s}ek, R\"ust, and \v{S}kovro\v{n} proved that
Clarkson's original algorithm is applicable in a still broader setting
than that of LP-type problems: It actually works for the class of
\emph{violator spaces} \cite{journals/dam/GartnerMRS08}. At first
glance, this seems to be yet another generalization to yet another
abstract problem class, but as \v{S}kovro\v{n} has shown, it stops
here: the class of violator spaces is the most general one for which
Clarkson's algorithm is still guaranteed to work \cite{skovronP}. In a
nutshell, the difference between LP-type problems and violator spaces
is that for the latter, the following trivial algorithm may cycle even
in the nondegenerate case: maintain the optimal solution subject to a
subset  of the constraints; as long as there is some constraint 
that is violated by this solution, replace the current solution by the
optimal solution subject to , and repeat. Examples of
such \emph{cyclic} violator spaces can be found in \cite{skovronP}.
For a very easy and intuitive example see also  \cite{journals/dam/GartnerMRS08}.

It was unknown whether the analysis of the German Algorithm (the
stripped-down version of Clarkson's first stage) also works for
violator spaces. For LP-type problems, the analysis is nontrivial and
constructs a ``composite'' LP-type problem. Here we show that this can
still be done for violator spaces, in essentially the same way.

For the Swiss Algorithm (the stripped-down version of Clarkson's
second stage), we provide the first analysis at all. The fact that it
works in the fully general setting of violator spaces comes naturally.

The main difference of the German and the Swiss algorithm compared
to their original formulations is the following. In both stages, at some point,
Clarkson's algorithm checks how many violated constraints some random
sample of constraints produces.
If there are too many, then the algorithm discards the sample and resamples.
The reason for this is that the analysis requires a bound on the number of
violators in each step. We essentially show that this bound only needs to hold
in expectation (and does so) for the analysis to go through.
So the  checks that we mentioned before are only an analytical tool, and not necessary for
the algorithms to work.

Let us point out that no subexponential
algorithm for finding the basis (i.e. ``solution'') of a violator space is known. Therefore,
we can only employ brute force to ``solve'' small violator spaces. Note that,
e.g., in the context of linear programming, finding a basis means identifying the
constraints which are tight at an optimal point. We
call this the Brute Force Algorithm (\stageIII). Hence, the resulting best
worst-case bound known degrades to , where  is
some exponential function of . In this paper, we will not investigate
this point further and use \stageIII{ }as a black box.

\paragraph{The German Algorithm (\stageI).}
Let us explain the algorithm for the problem of finding the smallest
enclosing ball of a set of  points in  (this problem fits
into the violator space framework). The algorithm proceeds in rounds
and maintains a working set , initialized with a subset  of 
points drawn at random. In each round, the smallest enclosing ball of
 is being computed (by some other algorithm). For the next round,
the points that are unhappy with this ball (the ones that are outside)
are being added to . The algorithm terminates as soon as everybody
is happy with the smallest enclosing ball of .

The crucial fact that we reprove below in the violator space framework
is this: the number of rounds is at most , and for , the expected maximum size of  is bounded by
. This means that \stageI{ }reduces a
problem of size  to  problems of expected size .
We call this the German Algorithm, because it takes -- typically
German -- one decision in the beginning which is then efficiently
pulled through.

\paragraph{The Swiss Algorithm (\stageII).} 
Like \stageI, this algorithm proceeds in rounds, but it
maintains a voting box that initially contains one slip per point. In
each round, a set of  slips is drawn at random from the voting box,
and the smallest enclosing ball of the corresponding set  is
computed (by some other algorithm). For the next round, all slips are
put back, and on top of that, the slips of the unhappy points are being
doubled. The algorithm terminates as soon as everybody is happy with
the smallest enclosing ball of the sample .

Below, we will prove the following: if , the expected number
of rounds is . This means that \stageII{ }reduces a
problem of size  to  problems of size . We call
this the Swiss Algorithm, because it takes -- typically Swiss -- many 
independent local decisions that magically fit together in the end.

\paragraph{Hypercube partitions.}
A hypercube partition is a partition of the vertices of the hypercube
such that every element of the partition is the set of vertices of
some subcube. It was known that every \emph{nondegenerate} violator
space induces a hypercube partition
\cite{SkovronMatousekLPdim,MatousekLPdim}.  We prove here that also
the converse is true, meaning that we obtain an alternative
characterization of the class of violator spaces. While this result is
not hard to obtain, it may be useful in the future for the problem of
counting violator spaces. Here, the initial bounds provided by
\v{S}kovro\v{n} are still the best known ones \cite{skovronP}.

\paragraph{Applications.} 
We would love to present a number of convincing
applications of the violator space framework, and in particular of the
German and the Swiss Algorithm for violator spaces. Unfortunately, we
cannot. There is one known application of Clarkson's algorithm that
really requires it to work for violator spaces and not just LP-type
problems \cite{journals/dam/GartnerMRS08}; this application (solving
generalized -matrix linear complementarity problems with a fixed
number of blocks) benefits from our improvements in the
sense that now also the German and the Swiss Algorithm are applicable
to it (with less random resources than Clarkson's algorithm).


Our main contributions are therefore theoretical: we show that
Clarkson's second stage can be simplified (resulting in the Swiss
Algorithm), and this result is new even for LP-type problems and
linear programming. The
fact that Clarkson's first stage can be simplified (resulting in the
German Algorithm) was known for LP-type problems; we extend it to
violator spaces, allowing the German Algorithm to be used for solving
generalized -matrix linear complementarity problems with a fixed
number of blocks.

We  believe that our results are significant
contributions to the theory of abstract optimization frameworks
themselves. We have now arrived at a point where Clarkson's algorithm
has been shown to work in the most general abstract setting that is
possible, and in probably the most simple variant that can still
successfully be analyzed.



\section{Prerequisites}

\subsection{The Sampling Lemma}
The following lemma is due to G\"artner and Welzl in 
\cite{GWSampl01} and was adapted to violator
spaces in \cite{journals/dam/GartnerMRS08}. We repeat it here for the sake
of completeness, and because its proof and formulation are very
concise.
Let  be a set of size , and   a function that maps any
set  to some value . Define

 is the set of \emph{violators} of , while
 is the set of \emph{extreme elements} in . Obviously,


For a random sample  of size , i.e., a set  chosen uniformly at random from the set  of all -element subsets of , we define random variables  and
, and we consider the expected values



\begin{lemma}[Sampling Lemma, \cite{GWSampl01, journals/dam/GartnerMRS08}]
\label{lemma:sampling}
For ,

\end{lemma}
\begin{proof}
	Using the definitions of  and  as well as (\ref{eqViolators}), we can argue as 
follows:


\begin{center}

\end{center}


Here,  is the indicator variable for the event in brackets. Finally, .
\end{proof}

\subsection{Violator Spaces}

\begin{definition}
\label{def:violatorspace}
A \emph{violator space} is a pair , where  is a finite set and 
is a mapping  such that the following two conditions are fulfilled.

\vspace{0.2cm}
\begin{tabular}{ll}
	Consistency: &  holds for all , and \\
	Locality: & for all , where ,\\
	& we have .
\end{tabular}  
\end{definition}

\begin{lemma}[Lemma 17, \cite{journals/dam/GartnerMRS08}]
\label{lemma:monotonicity}
Any violator space  satisfies \emph{monotonicity} defined as follows:

\vspace{0.2cm}
\begin{tabular}{ll}
	Monotonicity: &  implies \\
	& for all sets .
\end{tabular}  
\end{lemma}
\begin{proof}
Assume . Then locality yields
 which contradicts consistency.
\end{proof}

\begin{definition}
Consider a violator space .
\begin{itemize}
\item[(i)]We say that  is a \emph{basis}
if for all proper subsets  we have . For , a basis of  is a minimal subset  of  with .
A basis in  is a basis of some set .
\item[(ii)] The \emph{combinatorial dimension} of , denoted by ,
is the size of the largest basis in .
\item[(iii)]  is \emph{nondegenerate} if every set set , ,
has a unique basis. Otherwise  is \emph{degenerate}.
\end{itemize}
\end{definition}

Observe that a minimal subset  with  is indeed a basis:
Assume for contradiction that there is a set  such that .
Locality then yields , which contradicts the minimality of .
Also, note that, because of consistency, any basis  of  has no violators .


\begin{corollary}[of Lemma \ref{lemma:sampling}]
Let  be a violator space of combinatorial dimension , and . If we choose
a subset , , uniformly at random, then

\end{corollary}
\begin{proof}
The corollary follows from the Sampling Lemma \ref{lemma:sampling}, with the observation
that , .
\end{proof}

\section{Clarkson's Algorithm Revisited}
Clarkson's algorithm can be used to compute a basis of some violator space , .
It consists of two separate stages
and the Brute Force Algorithm (\stageIII). 
The results about the running time and the size of the sets involved is summarized in
Theorem \ref{theorem:runningtime1} and Theorem \ref{theorem:runningtime2}.

The main idea of both stages (\stageI{ }and \stageII) is the following:  We draw a random sample  of
size  and then compute a basis of  using some other algorithm. The crucial point here is that  hopefully. Obviously,
such an approach may fail to find a basis of , and we might have to reconsider and enter a second round.
That is the point at which \stageI{ }and \stageII{ }most significantly differ.

In both stages we assume that the size of the ground set, i.e., , is larger than , such that we can
actually draw a sample of that size. We can assume this w.l.o.g., because it is easy to
incorporate an if statement at the beginning that directly calls the other algorithm should  be too small. 

\subsection{The German Algorithm (\stageI)}
This algorithm works as follows. Let  be a violator space, , and .
 We draw a random sample , , only once, 
and initialize our working set  with . Then we enter a repeat loop, in which we compute a
basis  of  and check whether there are any violators in . If no,
then we are done and return the basis . If yes, then we add those violators to our working
set  and repeat the procedure.

The analysis will show that  the number of rounds is bounded by , and 
the size of  in any round is bounded by . See Theorem \ref{theorem:runningtime1}.

\begin{algorithm}[H]
    \SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}

	\Input{Violator space , , and }
	\Output{A basis  of }
	\Indp
	\SetInd{0.5cm}{0em}
	\BlankLine
			\;
			Choose  with ,  u.a.r.\;
			\;
			
			\Repeat{}{
				 \stageII()\;
				\;
		}
		\Return{}
	\BlankLine
    \caption{\stageI()\label{alg:stageI}}
    \end{algorithm}
    
    We will adopt some useful notations which we will use in the following proofs.
    First, let us point out that the notation  refers to the violator mapping
restricted to some set .
    \begin{definition}
    \label{def:intermediatesets}
    For , by
    
    we denote the sets , , and  computed in round  of the
    repeat loop above.
    Furthermore, we set , while 
    and   are undefined. In particular, we have that
    is a basis of , and  .
    If the algorithm performs exactly  rounds, sets with indices  are defined
    to be the corresponding sets in round .
    \end{definition}
    
    
    The next one is an auxiliary lemma that we will need further on in the analysis. It is a 
generalization
    of the fact that there is at least one element of the basis of  found as a violator in every 
round (see also
    Lemma \ref{lemma:53b}).
    \begin{lemma}
    \label{lemma:53}
    	For , .
    \end{lemma}
    \begin{proof}
    Assume that . Together with consistency,
    , this implies
    
   Now, applying locality and the definition of basis, we get
   
   On the other  hand, since  and
   ,
   we can apply monotonicity and derive
   
   Note that , because  always contains the
   violators from previous rounds. Additionally, by equations (\ref{eq:27A}) and (\ref{eq:27B}) we have 
that
   .
   Thus, we can build a contradiction of consistency,
   
   The last inequality holds because  is not the last round.
    \end{proof}
    
    
    The following lemma is the crucial result that lets us interpret the development of the set  in
    Algorithm \ref{alg:stageI} as a violator space itself.
    \begin{lemma}
    \label{lemma:54}
    Let  be a violator space of combinatorial dimension . For any subset  define
    
    Using this we can define a new violator mapping as follows,
    
    Then the following statements are true:
    \begin{enumerate}
    \item[\emph{(i)}]  is a violator space of combinatorial dimension at most .
    \item[\emph{(ii)}] The set  is given by
    
    \item[\emph{(iii)}] If  is nondegenerate, then so is .
    \end{enumerate}
    \end{lemma}
    
    In order to prove Lemma \ref{lemma:54} we first need an auxiliary claim. Note that 
    denotes disjoint union.
    \begin{claim}
    \label{claim:1}
   Let  be any set with  and . If
   
   then
   
    \end{claim}
    \begin{proof}[Proof of Claim \ref{claim:1}]
    We prove the claim by induction on . First, if  the precondition reads . It follows that
    .
  
    
    Suppose the claim is true for . From  we can deduce
    
    \end{proof}
    
    Before we proceed to the proof of Lemma \ref{lemma:54} let us first state the consequences, which
    we obtain by applying Lemma \ref{lemma:sampling} to the violator space that we constructed.
    
    \begin{theorem}[Theorem 5.5 of \cite{GWSampl01}]
    \label{theorem:sizeofG}
     For  with ,  and a random sample of size ,
     
     Choosing  yields
     
    \end{theorem}
    \begin{proof}[Proof of Theorem \ref{theorem:sizeofG}]
    The first inequality directly follows from the sampling lemma (Lemma \ref{lemma:sampling}),  
applied
    to the violator space , together with part  of Lemma \ref{lemma:54}. The 
second
    inequality follows from plugging in the value for .
    \end{proof}
    
    Let us now come back to the Lemma.
    \begin{proof}[Proof of Lemma \ref{lemma:54}]
    ~\newline
     \textbf{Proof of (i).}
    We first need to check consistency and locality as defined in Definition \ref{def:violatorspace}.
    
    Consistency is easy, by the definition of . Since the violators of  are 
chosen
    from  exclusively, we can be sure that  for all .
    
    Let us recall what locality means. For sets , if , then
    . This we are going to prove by induction on the size of .
    If , then the two sets are the same, and locality is obviously fulfilled. Now, 
suppose
    that  and locality is true for any smaller value . Consider some set  
fulfilling
     and . 
    First note that, if , then also .
    Therefore, the precondition for the induction hypothesis is fulfilled, and we can conclude that
    .  Bearing this in mind, we can make the following derivation:
    
    That shows the locality of the violator space .
    
    We still have to show that   has combinatorial dimension at most .
    To this end we prove that , where
    
     Note that , as we will show in {(iii)}, is in fact the unique basis of the set .
     By bounding the size of  we therefore bound the combinatorial dimension of
    . 
     Equivalent to   we show that , for , using induction on . For
     we get
    
    because  is disjoint from , the basis of . Therefore,
    
    can be removed from  without changing the set of violators.
    
    Now assume that the statement holds for  and consider the case . By Claim 
    \ref{claim:1}, we get .
    Since  is disjoint from the basis  of  it follows that
    
    
    To bound the size of , we observe that
    
    for all  (the number of rounds in which ). This follows
    from Lemma \ref{lemma:53}.  has at least one element in each of the  sets
    , which are in turn disjoint from . Hence we get
    
    
    ~\newline
     \textbf{Proof of (ii).}
     We show that if some constraint  is in  then it is also in  for
     some . On the other hand if  then  is not in
     any of the , . This proves the statement of .
     
     Assume  and let . Consider the largest index
      such that
     
     Note that such an index  must exist, because , which
     simply follows from  and .
     Then, from Claim \ref{claim:1} it follows that , and
     by assumption on  we know that . Therefore, by the contrapositive of locality,
     we conclude .
     This means that , because otherwise the 
consistency of 
     would be violated.
     
     On the other hand, if , then , or equivalently
     , for .
    However, because  is consistent it follows that , and
    therefore ,  for .
    
    ~\newline
     \textbf{Proof of (iii).}
     Nondegeneracy of  follows if we can show that every set 
     has the set  as its unique basis. To this end we prove that whenever we have
      with , then .
     
     Fix  with , i.e.,
     
     Claim \ref{claim:1} then implies
     
     and the nondegeneracy of  yields that  and
      have the same unique basis , for all .
     Note that  is indeed contained in , because 
      for . That means, if there exists a basis of ,
     that by definition would also be a basis
     of , but distinct from , nondgeneracy is violated.
     
     It follows that  contains
     
     so  contains
     
     The latter equality holds because  is disjoint from ,
     thus in particular from the union of the .
     \end{proof}
    
    \begin{theorem}
    \label{theorem:runningtime1}
    Let   be a violator space of combinatorial dimension , and  .
Then the algorithm \stageI~computes a basis of  with
 at most  calls to \stageII, with an expected number of 
at most 
constraints each.
    \end{theorem}
    \begin{proof}
    According to Lemma \ref{lemma:53} (and maybe more intuitively according to Lemma \ref{lemma:53b}),
    in every round except the last one we add at least one element of any basis of  to .
    Since the size of the basis is bounded by  we get that the number of rounds is at most . Furthermore,
    according to Theorem \ref{theorem:sizeofG}, and our choice , the expected size of  will not
    exceed  in any round.
    \end{proof}


\subsection{The Swiss Algorithm (\stageII)}
\label{subsec:secondstage}

The algorithm \stageII~proceeds similar as the first one.
Let the input be a violator space , , and .

First, let us 
(re)introduce the notation , , and  for , similar as in 
Definition \ref{def:intermediatesets}, for the sets ,  and  of round 
respectively. The set  is a basis of  and .
Since we draw a random sample in every round it does not make sense to index the sets
 and  by , so we drop this subscript.

After the initialization we enter the first round and choose a random sample  of size
 uniformly at random from . Then we compute an intermediate basis 
of the violator space   by using \stageIII~as
a black box. 
In the next step we compute the set of violated constraints, i.e., .
So far, it is the same thing as the first stage. But now, instead of enforcing the violated
constraints by adding them to the active set, we increase the probability that the violated
constraints are chosen in the next round. This is achieved by means of the multiplicity
or weight variable .

    \begin{definition}
\label{def:intermediatemultiplicities}
With every  we associate the \emph{multiplicity} . For an arbitrary
set  we define the \emph{cumulative multiplicity} as

For the analysis we also need to keep track of this value across different iterations
of the algorithm. For  we will use  (and
) to denote the (cumulative) multiplicity at the end of round .
We define  for any ,
and therefore .
\end{definition}

Now back to the algorithm. To increase the probability that a constraint  is chosen in the
random sample of round  we double the multiplicity of ,
i.e., .

The multiplicities determine
how the random sample  is chosen.
To this end we construct a multiset
 to which we add  copies of every element .
To simplify notation, let us for a moment fix the round  and drop the
corresponding superscript.

We define the function  as
the function that maps a set of elements from  to the set of corresponding elements in , 
i.e., for
 
 
where the , , are the distinct copies of . For example, .
Conversely, let  be the  function that collapses a given subset
of  to their original elements in , i.e., for ,


Reintroducing the superscript  we can simply say that we construct 
using the multiplicities from round .
The sample  is then chosen u.a.r. from the -subsets of
. In the following the multiset property will not be important any more and we
can discard multiple entries to obtain . Note that .
Then we continue as in round . Note that in the first round this is in fact equivalent
to choosing an -subset u.a.r. from , because  for all .

The algorithm terminates as soon as  for some round  and
returns the basis .

{
\parskip2em
\begin{algorithm}[H]
    \SetKwInOut{Input}{input}
	\SetKwInOut{Output}{output}
	\Input{Violator space , , and }
	\Output{A basis  of }
	\Indp
	\SetInd{0.5cm}{0em}
	\BlankLine
			 for all \;
			\;
			\Repeat{}{
				choose random  from  according to \;
				 \stageIII()\;
				 for all \;
			}
		\Return{}
	\BlankLine
    \caption{\stageII()\label{stageII}}
    \end{algorithm}


Let us first discuss an auxiliary lemma very similar in flavour to Lemma \ref{lemma:53}.
}

\begin{lemma}[Observation 22,  \cite{journals/dam/GartnerMRS08}]
\label{lemma:53b}
Let  be a violator space, , and .
Then  contains at least one element from every basis of .
\end{lemma}
\begin{proof}
Since the proof is pretty short we repeat it here.
Let  be some basis of  and assume that . From consistency we get
. Together this implies

Applying locality and monotonicity, we get

meaning that , a contradiction.
\end{proof}

The analysis of \stageII~will show that the elements in any basis  of
 will increase their multiplicity so quickly
that they are chosen with high probability after a logarithmic number
of rounds. This, of course, means that the algorithm will terminate, because there will be no violators.
Formally, we will have to employ a little trick though. We will consider a modification
of \stageII~that runs forever, regardless of the current set of violators! Let us call the modified algorithm \stageIIforever. We call a particular round  \emph{controversial} if
. Furthermore, let  be the event that the first   rounds are controversial
 in \stageIIforever.

\begin{lemma}
\label{lemma:lowerbound}
Let  be a violator space,  , ,
 any basis of , and  some positive integer.
Then, in \stageIIforever, the following holds for the expected cumulative
multiplicity of  after  rounds,

\end{lemma}
\begin{proof}
In any controversial round,
Lemma \ref{lemma:53b} asserts that . So, in every controversial round, the 
multiplicity of at least one element in  is doubled. Therefore, by conditioning on the event
that the first  rounds are controversial,
there must be a constraint in  that has been doubled at least  times (recall that 
). It follows that 
.
\end{proof}

\begin{lemma}
\label{lemma:upperbound}
Let  be a violator space, , ,
 any basis of , and  some positive integer.
Then, in \stageIIforever, the following holds for the expected cumulative
multiplicity of  after  rounds,

\end{lemma}
\begin{proof}

Let us point out first, that the following analysis goes through for
\stageIIforever{ }as well
as for \stageII, but to make it match Lemma \ref{lemma:lowerbound} we formulated it using
the former.

Note that , because . Therefore, if we show the upper bound for the latter expectation we are done. Let  
 be the number of rounds, and
 the increase of multiplicity from one round to another, for any  and
.
We write the expected weight of  after  rounds as the sum of the initial 
weight plus the expected increase in weight in every round from  to ,

The first term is easy, , and the second term we write as a conditional 
expectation,
assuming that the weight in round  was ,


Now comes the crucial step. According to Lemma \ref{lemma:sampling} we can 
upper bound

by interpreting it as the expected 
number of violators of a
multiset extension of .
To this end we construct a violator space
, where 
using the multiplicities from round .
Let us fix round  and drop the superscript for the moment. For any
 we define

We observe that  is indeed a violator space. For
, consistency
is preserved, because from consistency of  it follows that 
, and knowing
, we can conclude consistency of . 
Similarly,
for , locality of  tells us that if
 then
, and knowing ,
locality of  follows.

The violator space we just constructed has the same ground set  by means of which we 
draw the random
sample  in every round. By supplying a valid violator mapping we asserted that we can 
apply
the sampling lemma to that process.
Some thinking reveals that  (even though we 
introduced
degeneracy), and we can conclude

Therefore we get the simplified expression

\begin{center}

\end{center}

The first line is derived from (\ref{eq:d01}), (\ref{eq:d02}), and (\ref{eq:d03}). The rest is routine. 
Dropping the last term  we get the following recursive equation,

which easily resolves to the claimed bound.
\end{proof}


Using , and combining Lemmata \ref{lemma:lowerbound} and  \ref{lemma:upperbound}, 
we now know that


This inequality gives us a useful upper bound on , 
because the left-hand side power grows faster than the right-hand side 
power as a function of , given that  is chosen large enough. 

Let us choose
 for some constant .
We obtain

using   for all . This further gives us


This implies the following tail estimate.
\begin{lemma}
  For any , the probability that \stageIIforever~
  starts with at least  controversial 
  rounds is at most
   
\end{lemma}

\begin{proof}
  The probability for at least this many leading controversial rounds 
  is at most

\end{proof}

We can also bound the expected number of
leading controversial rounds in \stageIIforever, and this
bounds the expected number of rounds in \stageII, because \stageII~terminates upon 
the first non-controversial round it encounters.

\begin{theorem}
\label{theorem:runningtime2}
Let  be a violator space, , and .
Then the algorithm \stageII~computes a basis of  with
 an expected number of at most  calls to \stageIII, with 
at most 
constraints each.
\end{theorem}
\begin{proof}
By definition  of , the expected number of leading controversial rounds
in \stageIIforever{ }is 

For any , we can use (\ref{eq:C_k_up}) to bound this by



This upper bounds the expected number of rounds in \stageII. In every round of \stageII~one call to
\stageIII~is made, using  constraints, where 
is constant.
\end{proof}

\section{Hypercube Partitions}
Let  be a finite set. Consider the graph on the vertices , where two vertices 
are connected by an edge if they differ in exactly one element, i.e., , .
This graph is a hypercube of dimension . For
the sets , we define 
  and call any such
  an \emph{interval}. A \emph{hypercube  partition} is a partition 
 of  into (disjoint) intervals.
 
 Let  be a violator space. We call two sets  \emph{equivalent}
 if , and let  be the partition of  into
 equivalence classes w.r.t. this relation. We call  the \emph{violation pattern}
 of .
 

Before we formulate and prove the Hypercube Partition Theorem, we need to introduce some 
notation. We extend the notion of violator spaces by the concept of \emph{anti-basis}.

\begin{definition}
Consider a violator space . We say that  is an \emph{anti-basis} if
for all proper supersets  we have .
An anti-basis of  is a maximal superset  of  with . 
 \end{definition}
 
 Note that a maximal superset  of  such that  is 
indeed 
 an anti-basis of . Suppose that there is a set  with
 . Locality then decrees that , but this contradicts the maximality of .
 
 \begin{lemma}
 \label{lemma:uniqueantibasis}
Consider  the violator space . For any  there is a unique
 anti-basis  of .  
 \end{lemma}

 \begin{proof}
 Suppose that there exist two distinct anti-bases  and  of G.
 Because of  and consistency we have that
 .
 Therefore, by locality, .
 Since  and  are distinct, it cannot be that
   and 
  at the same time. Then, in any case,  or 
  holds, which contradicts the maximality of the anti-bases.
   \end{proof}
   
   \begin{corollary}
   Let  be a violator space, ,  any basis of , and 
   the unique anti-basis of . Then for any set , , 
   and  are equivalent, i.e., .
   \end{corollary}
   \begin{proof}
   This is an immediate consequence of monotonicity (Lemma \ref{lemma:monotonicity}).
   \end{proof}

\begin{lemma}
\label{lemma:patterndetspace}
 completely determines .
\end{lemma}

\begin{proof} Let . There is a unique anti-basis
 of , meaning that in , there is a unique
inclusion-maximal superset of  in the same class of the partition.
This implies that , so
 is reconstructible from .
\end{proof}

\begin{lemma}
\label{lemma:nondegdetpartition}
If  is nondegenerate (unique bases), then  is a hypercube
partition. 
\end{lemma}

\begin{proof}
We first show that  implies . The latter has been shown for the existence of a unique
anti-basis. For the former, we argue as follows. Let  be the unique
basis of . Then . But then  is also the
unique basis of  and . It follows that ,
and by locality we get .

This argument implies that any partition class  is contained
in the interval
.
On the other hand, the whole interval is contained in  by locality,
so we are done. 
\end{proof}

Lemma \ref{lemma:patterndetspace} and  \ref{lemma:nondegdetpartition}
together imply that there is an injective mapping 
from the set of nondegenerate violator spaces to the set of hypercube
partitions. It remains to show that the mapping is surjective.

\begin{theorem}
Any hypercube partition  is the violation pattern
of some nondegenerate violator space 
 \end{theorem}
\begin{proof}
Let , and let  be the interval containing
. We define  and claim that this is a
nondegenerate violator space with violation pattern . The
latter is clear, since  if and only if . To see the former, we observe that consistency holds because
of . To prove locality, choose  with
. In particular, , so  is also in  and we get  by definition
of .

It remains to show that the violator space thus defined is nondegenerate.
Let  be two sets with , meaning that they are in the
same partition class of . But then  is also in the
same class, and we get . This implies existence of
unique bases.
\end{proof}

\section{Conclusion}
We analyzed Clarkson's algorithm
in what we believe to be its most general as well as natural
setting. Additionally, we have given the equivalence between non-degenerate violator
spaces and hypercube partitions, which could help 
identifying further applications in computational geometry as well as other fields
of computer science. Another major challenge will be to establish
a subexponential analysis for the third stage, \stageIII, in the framework of violator spaces
(as there already exists for LP's and LP-type problems), in order
to get stronger bounds when the dimension is only moderately small.


\newpage
\bibliographystyle{plain}
\begin{thebibliography}{1}

\bibitem{c-lvali-95}
Kenneth~L. Clarkson.
\newblock Las {V}egas algorithms for linear and integer programming when the
  dimension is small.
\newblock {\em Journal of the ACM}, 42(2):488--499, 1995.

\bibitem{journals/dam/GartnerMRS08}
Bernd G{\"a}rtner, Ji{\v{r}}{\'{i}} Matou{\v{s}}ek, Leo R{\"u}st, and Petr
  {\v{S}}kovro{\v{n}}.
\newblock Violator spaces: Structure and algorithms.
\newblock {\em Discrete Applied Mathematics}, 156(11):2124--2141, 2008.

\bibitem{GaerWel1}
Bernd G{\"a}rtner and Emo Welzl.
\newblock Linear programming - randomization and abstract frameworks.
\newblock In {\em Proceedings of the 13th Annual Symposium on Theoretical
  Aspects of Computer Science ({STACS})}, volume 1046 of {\em Lecture Notes in
  Computer Science}, pages 669--687. Springer, 1996.

\bibitem{GWSampl01}
Bernd G{\"a}rtner and Emo Welzl.
\newblock {A simple sampling lemma: Analysis and applications in geometric
  optimization}.
\newblock {\em Discrete \& Computational Geometry}, 25(4):569--590, 2001.

\bibitem{MatousekLPdim}
Ji{\v{r}}{\'{i}} Matou{\v{s}}ek.
\newblock Removing degeneracy in {LP}-type problems revisited.
\newblock {\em Discrete \& Computational Geometry}, 2008.

\bibitem{MSW}
Ji{\v{r}}{\'{i}} Matou{\v{s}}ek, Micha Sharir, and Emo Welzl.
\newblock A subexponential bound for linear programming.
\newblock {\em Algorithmica}, 16:498--516, 1996.

\bibitem{SkovronMatousekLPdim}
Ji{\v{r}}{\'{i}} Matou{\v{s}}ek and Petr {\v{S}}kovro{\v{n}}.
\newblock Removing degeneracy may require a large dimension increase.
\newblock {\em Theory of Computing}, 3(1):159--177, 2007.

\bibitem{skovronP}
Petr {\v{S}}kovro{\v{n}}.
\newblock {\em Abstract models of optimization problems}.
\newblock PhD thesis, Charles University, Prague, 2007.

\end{thebibliography}


\end{document}