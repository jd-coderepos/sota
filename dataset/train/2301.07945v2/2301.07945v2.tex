\documentclass[letterpaper]{article} \usepackage{aaai23}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in} \usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{multirow}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{xspace}
\usepackage{dsfont}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{listings}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{threeparttable}
\usepackage{marvosym}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{color}

\newtheorem{mydef}{Definition}
\newcommand{\name}{PDFormer\xspace}
\newcommand{\paratitle}[1]{\vspace{1.5ex}\noindent\textbf{#1}}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\aka}{\emph{a.k.a.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\ignore}[1]{}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\changed}[1]{\textcolor{blue}{#1}}
\newcommand{\remove}[1]{\sout{\textcolor{orange}{#1}}}
\newcommand{\replace}[2]{\remove{#1}/\changed{#2}}
\newcommand\mathbox[1]{\mbox{}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\pthree}{PeMS03\xspace}
\newcommand{\pfour}{PeMS04\xspace}
\newcommand{\pseven}{PeMS07\xspace}
\newcommand{\peight}{PeMS08\xspace}
\newcommand{\taxi}{NYCTaxi\xspace}
\newcommand{\bike}{CHIBike\xspace}
\newcommand{\bj}{T-Drive\xspace}
\newcommand{\sah}{\xspace}
\newcommand{\tah}{\xspace}
\newcommand{\ssa}{\xspace}
\newcommand{\tsa}{\xspace}
\newcommand{\gsah}{\xspace}
\newcommand{\ssah}{\xspace}
\newcommand{\gssa}{\xspace}
\newcommand{\sssa}{\xspace}
\newcommand{\dft}{\xspace}
\newcommand{\p}{}


\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/TemplateVersion (2023.1)
}



\setcounter{secnumdepth}{0} 





\title{\name: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction}






\author{
Jiawei Jiang,\textsuperscript{\rm 1}\equalcontrib\,
    Chengkai Han,\textsuperscript{\rm 1}\equalcontrib\,
    Wayne Xin Zhao,\textsuperscript{\rm 4}\,
    Jingyuan Wang\textsuperscript{\rm 1,\rm 2,\rm 3}\thanks{Corresponding author: jywang@buaa.edu.cn\\}
}
\affiliations{
    \textsuperscript{\rm 1}School of Computer Science and Engineering, Beihang University, Beijing, China\\
    \textsuperscript{\rm 2}Pengcheng Laboratory, Shenzhen, China\\
    \textsuperscript{\rm 3}School of Economics and Management, Beihang University, Beijing, China\\
    \textsuperscript{\rm 4}Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\




\{jwjiang, ckhan, jywang\}@buaa.edu.cn; batmanfly@gmail.com
}









\begin{document}

\maketitle





\begin{abstract}
As a core technology of Intelligent Transportation System, traffic flow prediction has a wide range of applications. The fundamental challenge in traffic flow prediction is to effectively model the complex spatial-temporal dependencies in traffic data. Spatial-temporal Graph Neural Network (GNN) models have emerged as one of the most promising methods to solve this problem. However, GNN-based models have three major limitations for traffic prediction: i) Most methods model spatial dependencies in a static manner, which limits the ability to learn dynamic urban traffic patterns; ii) Most methods only consider short-range spatial information and are unable to capture long-range spatial dependencies; iii) These methods ignore the fact that the propagation of traffic conditions between locations has a time delay in traffic systems. To this end, we propose a novel \underline{P}ropagation \underline{D}elay-aware dynamic long-range trans\underline{Former}, namely \name, for accurate traffic flow prediction. Specifically, we design a spatial self-attention module to capture the dynamic spatial dependencies. Then, two graph masking matrices are introduced to highlight spatial dependencies from short- and long-range views. Moreover, a traffic delay-aware feature transformation module is proposed to empower \name with the capability of explicitly modeling the time delay of spatial information propagation. Extensive experimental results on six real-world public traffic datasets show that our method can not only achieve state-of-the-art performance but also exhibit competitive computational efficiency. Moreover, we visualize the learned spatial-temporal attention map to make our model highly interpretable.
\end{abstract}






\section{Introduction}

In recent years, rapid urbanization has posed great challenges to modern urban traffic management. As an indispensable part of modern smart cities, intelligent transportation systems (ITS)~\cite{intro} have been developed to analyze, manage, and improve traffic conditions (\eg reducing traffic congestion). As a core technology of ITS, \emph{traffic flow prediction}~\cite{intro2} has been widely studied, aiming to predict the future flow of traffic systems based on historical observations. It has been shown that accurate traffic flow prediction can be useful for various traffic-related applications~\cite{libcity}, including route planning, vehicle dispatching, and congestion relief.

For traffic flow prediction, the fundamental challenge is to effectively capture and model the complex and dynamic spatial-temporal dependencies of traffic data~\cite{intro3}. Many attempts have been made in the literature to develop various deep learning models for this task. As early solutions, convolutional neural networks (CNNs) were applied to grid-based traffic data to capture spatial dependencies, and recurrent neural networks (RNNs) were used to learn temporal dynamics~\cite{STResNet, dmvstnet}. Later, graph neural networks (GNNs) were shown to be more suited to model the underlying graph structure of traffic data~\cite{DCRNN, STGCN}, and thus GNN-based methods have been widely explored in traffic prediction~\cite{GWNET,STSGCN,MTGNN,STFGNN,STGODE,STG-NCDE}. 




\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{figure/intro3.pdf}
    \caption{The Findings about Traffic Prediction.}
    \label{fig:intro}
\end{figure}





Despite the effectiveness, GNN-based models still have three major limitations for traffic prediction. Firstly, the spatial dependencies between locations in a traffic system are highly \emph{dynamic} instead of being static, which are time-varying as they are affected by travel patterns and unexpected events. For example, as shown in Fig.~\ref{fig:intro}(b), the correlation between nodes  and  becomes stronger during the morning peak and weaker during other periods. While, existing methods model spatial dependencies mainly in a static manner (either predefined or self-learned), which limits the ability to learn dynamic urban traffic patterns. Secondly, due to the functional division of the city, two distant locations, such as nodes  and  in Fig.~\ref{fig:intro}(c), may reflect similar traffic patterns, implying that the spatial dependencies between locations are \emph{long-range}. Existing methods are often designed locally and unable to capture long-range dependencies. For example, GNN-based models suffer from over-smoothing, making it difficult to capture long-range spatial dependencies. Thirdly, the effect of \emph{time delay} might occur in the spatial information propagation between locations in a traffic system. For example, when a traffic accident occurs in one location, it will take several minutes (a delay) to affect the traffic condition in neighboring locations, such as nodes  and  in Fig.~\ref{fig:intro}(d). However, such a feature has been ignored in the immediate message passing mechanism of typical GNN-based models.



To address the above issues, in this paper, we propose a \underline{P}ropagation \underline{D}elay-aware dynamic long-range trans\underline{Former} model, namely \name, for traffic flow prediction. As the core technical contribution, we design a novel spatial self-attention module to capture the dynamic spatial dependencies. This module incorporates local geographic neighborhood and global semantic neighborhood information into the self-attention interaction via different graph masking methods, which can simultaneously capture the short- and long-range spatial dependencies in traffic data. Based on this module, we further design a delay-aware feature transformation module to integrate historical traffic patterns into spatial self-attention and explicitly model the time delay of spatial information propagation. Finally, we adopt the temporal self-attention module to identify the dynamic temporal patterns in traffic data. In summary, the main contributions of this paper are summarized as follows:

\begin{itemize}
\item We propose the \name model based on the spatial-temporal self-attention mechanism for accurate traffic flow prediction. Our approach fully addresses the issues caused by the complex characteristics from traffic data, namely dynamic, long-range, and time-delay. 
\item We design a spatial self-attention module that models both local geographic neighborhood and global semantic neighborhood via different graph masking methods and further design a traffic delay-aware feature transformation module that can explicitly model the time delay in spatial information propagation.
\item We conduct both multi-step and single-step traffic flow prediction experiments on six real-world public datasets. The results show that our model significantly outperforms the state-of-the-art models and exhibits competitive computational efficiency. Moreover, the visualization experiments show that our approach is highly interpretable via the learned spatial-temporal attention.
\end{itemize}

\section{PRELIMINARIES}
In this section, we introduce some notations and formalize the traffic flow prediction problem.
\subsection{Notations and Definitions}


\begin{mydef}[Road Network]
We represent the \emph{Road Network} as a graph , where  is a set of  nodes (,  is a set of edges, and  is the adjacency matrix of network . Here,  denotes the number of nodes in the graph.
\end{mydef}



\begin{mydef}[Traffic Flow Tensor]
We use  to denote the traffic flow at time  of  nodes in the road network, where  is the dimension of the traffic flow. For example,  when the data includes inflow and outflow. We use  to denote the traffic flow tensor of all nodes at total  time slices.
\end{mydef}


\subsection{Problem Formalization}
Traffic flow prediction aims to predict the traffic flow of a traffic system in the future time given the historical observations. Formally, given the traffic flow tensor  observed on a traffic system, our goal is to learn a mapping function  from the previous  steps' flow observation value to predict future  steps' traffic flow,




\section{Methods}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figure/overall2.png}
    \caption{The Overall Framework of \name.}
    \label{fig:framework}
\end{figure}







Fig.~\ref{fig:framework} shows the framework of \name, which consists of a data embedding layer, stacked  spatial-temporal encoder layers, and an output layer. We describe each module below in detail.

\subsection{Data Embedding Layer}
The data embedding layer converts the input into a high-dimensional representation. First, the raw input  is transformed into  through a fully connected layer,  is the embedding dimension. Then, we further design a spatial-temporal embedding mechanism to incorporate the necessary knowledge into the model, including the spatial graph Laplacian embedding to encode the road network structure and the temporal periodic embedding to model the periodicity of traffic flow.



To represent the structure of the road network, we use the graph Laplacian eigenvectors~\cite{lap}, which better describe the distance between nodes on the graph. First, we obtain the normalized Laplacian matrix by , where  is the adjacency matrix,  is the degree matrix, and  is the identity matrix. Then, we perform the eigenvalue decomposition  to obtain the eigenvalue matrix  and the eigenvector matrix . We use a linear projection on the  smallest nontrivial eigenvectors to generate the spatial graph Laplacian embedding . Laplacian eigenvectors embed the graph in Euclidean space and preserve the global graph structure information~\cite{lap2}.



In addition, urban traffic flow, influenced by people's travel patterns and lifestyle, has an obvious periodicity, such as morning and evening peak hours. Therefore, we introduce two embeddings to cover the weekly and daily periodicity, respectively, denoted as . Here  and  are functions that transform time  into the week index (1 to 7) and minute index (1 to 1440), respectively. The temporal periodic embeddings  are obtained by concatenating the embeddings of all  time slices.

Following the original Transformer~\cite{transformer}, we also employ a temporal position encoding  to introduce position information of the input sequence.


Finally, we get the output of the data embedding layer by simply summing the above embedding vectors as:

 will be fed into the following spatial-temporal encoders, and we use  to replace  for convenience.

\subsection{Spatial-Temporal Encoder Layer}



We design a spatial-temporal encoder layer based on the self-attention mechanism to model the complex and dynamic spatial-temporal dependencies. The core of the encoder layer includes three components. The first is a spatial self-attention module consisting of a geographic spatial self-attention module and a semantic spatial self-attention module to capture the short-range and long-range dynamic spatial dependencies simultaneously. The second is a delay-aware feature transformation module that extends the geographic spatial self-attention module to explicitly model the time delay in spatial information propagation. Moreover, the third is a temporal self-attention module that captures the dynamic and long-range temporal patterns.



To formulate self-attention operations, we use the following slice notation. For a tensor , the  slice is the matrix  and the  slice is .



\paratitle{Spatial Self-Attention (\ssa).} We design a \emph{Spatial Self-Attention} module to capture dynamic spatial dependencies in traffic data. Formally, at time , we first obtain the query, key, and value matrices of self-attention operations as:

where  are learnable parameters and  is the dimension of the query, key, and value matrix in this work. Then, we apply self-attention operations in the spatial dimension to model the interactions between nodes and obtain the spatial dependencies (attention scores) among all nodes at time  as:

It can be seen that the spatial dependencies  between nodes are different in different time slices, \ie \emph{dynamic}. Thus, the \ssa module can be adapted to capture the dynamic spatial dependencies. Finally, we can obtain the output of the spatial self-attention module by multiplying the attention scores with the value matrix as:








For the simple spatial self-attention in Eq.~(\ref{eq:ssa}), each node interacts with all nodes, equivalent to treating the spatial graph as a fully connected graph. However, only the interaction between a few node pairs is essential, including nearby node pairs and node pairs that are far away but have similar functions. Therefore, we introduce two graph masking matrices  and  to simultaneously capture the \emph{short-range} and \emph{long-range} spatial dependencies in traffic data.

From the short-range view, we define the binary geographic masking matrix , and only if the distance (\ie hops in the graph) between two nodes is less than a threshold , the weight is 1, and 0 otherwise. In this way, we can mask the attention of node pairs far away from each other. From the long-range view, we compute the similarity of the historical traffic flow between nodes using the Dynamic Time Warping (DTW)~\cite{dtw} algorithm. We select the  nodes with the highest similarity for each node as its semantic neighbors. Then, we construct the binary semantic masking matrix  by setting the weight between the current node and its semantic neighbors to 1 and 0 otherwise. In this way, we can find distant node pairs that exhibit similar traffic patterns due to similar urban functions.

Based on the two graph masking matrices, we further design two spatial self-attention modules, namely, \emph{Geographic Spatial Self-Attention} (\gssa) and \emph{Semantic Spatial Self-Attention} (\sssa), which can be defined as:

where  indicates the Hadamard product. In this way, the spatial self-attention module simultaneously incorporates short-range geographic neighborhood and long-range semantic neighborhood information.



\paratitle{Delay-aware Feature Transformation (\dft).}
There exists a \emph{propagation delay} in real-world traffic conditions. For example, when a traffic accident occurs in one region, it may take several minutes to affect traffic conditions in neighboring regions. Therefore, we propose a traffic delay-aware feature transformation module that captures the propagation delay from the short-term historical traffic flow of each node. Then, this module incorporates delay information into the key matrix of the geographic spatial self-attention module to explicitly model the time delay in spatial information propagation. Since traffic data can have multiple dimensions, such as inflow and outflow, here we only present the calculation process of this module using one dimension as an example.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figure/pattern3.png}
    \caption{Delay-aware Feature Transformation.}
    \label{fig:pattern}
\end{figure}


First, we identify a group of representative short-term traffic patterns from historical traffic data. Specifically, we slice the historical traffic data with a sliding window of size  and obtain a set of traffic flow series. Then, we perform k-Shape clustering algorithm~\cite{kshape} on these traffic flow series. The k-Shape algorithm is a time series clustering method that preserves the shape of the time series and is invariant to scaling and shifting. We use the centroid  of each cluster to represent that cluster, where  is also a time series of length . Then, we use the set  to represent the clustering results, where  is the total number of clusters. We can regard  as a set of short-term traffic patterns.



Similar traffic patterns may have similar effects on neighborhood traffic conditions, especially abnormal traffic patterns such as congestion. Therefore, we compare the historical traffic flow series for each node with the extracted traffic pattern set  to fuse the information of similar patterns into the historical flow series representation of each node as shown in Fig.~\ref{fig:pattern}. Specifically, given the -step historical traffic flow series of node  from time slice  to , denoted as , we first use the embedding matrix  to obtain a high-dimensional representation  as:

Then, we use another embedding matrix  to convert each traffic flow series in the traffic pattern set  into a memory vector as:

We compare the historical traffic flow representation  of node  with the traffic pattern memory vector  and obtain the similarity vector as:

Then, we perform a weighted sum of the traffic pattern set  according to the similarity vector  to obtain the integrated historical series representation  as:

where  is a learnable parameter matrix. The integrated historical series representation  contains the historical traffic flow information from time slice  to  of node . Finally, we use the integrated representation of  nodes, denoted as , to update  in Eq.~(\ref{eq:sscore}) as:

where  is obtained by concatenating all the integrated representation  of  nodes and  is the dimension of the key matrix in this work.



In this way, the new key matrix  at time slice  integrates the historical traffic flow information of all nodes from time slice  to . When computing the product of the query matrix and the new key matrix to obtain the spatial dependencies  at time slice  in Eq.~(\ref{eq:sscore}), the query matrix can take into account the historical traffic conditions of other nodes. This process explicitly models the \emph{time delay} in spatial information propagation. We do not add this module to the semantic spatial self-attention module because the short-term traffic flow of a distant node has little impact on the current node.



\paratitle{Temporal Self-Attention (\tsa).} There are dependencies (\eg periodic, trending) between traffic conditions in different time slices, and the dependencies vary in different situations. Thus, we employ a \emph{Temporal Self-Attention} module to discover the dynamic temporal patterns. Formally, for node , we first obtain the query, key, and value matrices as: 

where  are learnable parameters. Then, we apply self-attention operations in the temporal dimension and obtain the temporal dependencies between all time slices for node  as:

It can be seen that the temporal self-attention can discover the dynamic temporal patterns in traffic data that are different for different nodes. Moreover, the temporal self-attention has a global receptive to model the long-range temporal dependencies among all time slices. Finally, we can obtain the output of the temporal self-attention module as:



\paratitle{Heterogeneous Attention Fusion.} After defining the three types of attention mechanisms, we fuse heterogeneous attention into a multi-head self-attention block to reduce the computational complexity of the model. Specifically, the attention heads include three types, \ie geographic (\gsah), semantic (\ssah), and temporal (\tah) heads, corresponding to the three types of attention mechanisms, respectively. The results of these heads are concatenated and projected to obtain the outputs, allowing the model to integrate spatial and temporal information simultaneously. Formally, the spatial-temporal self-attention block is defined as:

where  represents concatenation,  are output concatenations and , ,  are the numbers of attention heads of \gssa, \sssa and \tsa, respectively, and  is a learnable projection matrix. In this work, we set the dimension .




In addition, we employ a position-wise fully connected feed-forward network on the output of the multi-head self-attention block to get the outputs . We also use layer normalization and residual connection here following the original Transformer~\cite{transformer}. 



\subsection{Output Layer}



We use a skip connection, consisting of 1 × 1 convolutions, after each spatial-temporal encoder layer to convert the outputs  into a skip dimension . Here  is the skip dimension. Then, we obtain the final hidden state  by summing the outputs of each skip connection layer. To make a multi-step prediction, we directly use the output layer to transform the final hidden state  to the desired dimension as:

where  is  steps' prediction results,  and  are 1 × 1 convolutions. Here we choose the direct way instead of the recursive manner for multi-step prediction considering cumulative errors and model efficiency.










\section{Experiments}




\begin{table}[t]
  \centering
  \caption{Data Description.}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
    Datasets & \#Nodes & \#Edges & \#Timesteps & \#Time Interval & Time range \\
    \midrule
    \pfour & 307   & 340   & 16992 & 5min  & 01/01/2018-02/28/2018 \\
    \pseven & 883   & 866   & 28224 & 5min  & 05/01/2017-08/31/2017 \\
    \peight & 170   & 295   & 17856 & 5min  & 07/01/2016-08/31/2016 \\
    \midrule
    \taxi & 75 (15x5) & 484   & 17520 & 30min & 01/01/2014-12/31/2014 \\
    \bike & 270 (15x18) & 1966  & 4416  & 30min & 07/01/2020-09/30/2020 \\
    \bj & 1024 (32x32) & 7812  & 3600  & 60min & 02/01/2015-06/30/2015 \\
    \bottomrule
    \end{tabular}}
  \label{tab:data_detail}\end{table}

\subsection{Datasets}
We verify the performance of \name on six real-world public traffic datasets, including three graph-based highway traffic datasets, \ie \pfour, \pseven, \peight~\cite{STSGCN}, and three grid-based citywide traffic datasets, \ie \taxi~\cite{nyctaxi}, \bike~\cite{libcity}, \bj~\cite{stmetanet}. The graph-based datasets contain only the traffic flow data, and the grid-based datasets contain inflow and outflow data. Details are given in Tab.~\ref{tab:data_detail}. 







\subsection{Baselines}
We compare \name with the following 17 baselines belonging to four classes. (1) \textit{Models For Grid-based Datasets}: We choose STResNet~\cite{STResNet}, DMVSTNet~\cite{dmvstnet} and DSAN~\cite{DSAN}, which are unsuitable for graph-based datasets. (2) \textit{Time Series Prediction Models}: We choose VAR~\cite{var} and SVR~\cite{svr}. (3) \textit{Graph Neural Network-based Models}: We choose DCRNN~\cite{DCRNN}, STGCN~\cite{STGCN}, GWNET~\cite{GWNET}, MTGNN~\cite{MTGNN}, STSGCN~\cite{STSGCN}, STFGNN~\cite{STFGNN}, STGODE~\cite{STGODE} and STGNCDE~\cite{STG-NCDE}. (4) \textit{Self-attention-based Models}: We choose STTN~\cite{STTN}, GMAN~\cite{GMAN}, TFormer~\cite{TFormer} and ASTGNN~\cite{astgnn}. 

\subsection{Experimental Settings}
\paratitle{Dataset Processing.}
To be consistent with most modern methods, we split the three graph-based datasets into training, validation, and test sets in a 6:2:2 ratio. In addition, we use the past hour (12 steps) data to predict the traffic flow for the next hour (12 steps), \ie a multi-step prediction. For the grid-based datasets, the split ratio is 7:1:2, and we use the traffic inflow and outflow of the past six steps to predict the next single-step traffic inflow and outflow.


\begin{table}[t]
  \centering
  \caption{Performance on Graph-based Datasets.}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|ccc|ccc|ccc}
    \toprule
    \multirow{2}[4]{*}{Model} & \multicolumn{3}{c|}{\pfour} & \multicolumn{3}{c|}{\pseven} & \multicolumn{3}{c}{\peight} \\
\cmidrule{2-10}          & MAE   & MAPE(\%) & RMSE  & MAE   & MAPE(\%) & RMSE  & MAE   & MAPE(\%) & RMSE \\
    \hline
    VAR   & 23.750  & 18.090  & 36.660  & 101.200  & 39.690  & 155.140  & 22.320  & 14.470  & 33.830  \\
    SVR   & 28.660  & 19.150  & 44.590  & 32.970  & 15.430  & 50.150  & 23.250  & 14.710  & 36.150  \\
    \hline
    DCRNN & 22.737  & 14.751  & 36.575  & 23.634  & 12.281  & 36.514  & 18.185  & 11.235  & 28.176  \\
    STGCN & 21.758  & 13.874  & 34.769  & 22.898  & 11.983  & 35.440  & 17.838  & 11.211  & 27.122  \\
    GWNET & 19.358  & 13.301  & 31.719  & 21.221  & 9.075  & 34.117  & 15.063  & 9.514  & 24.855  \\
    MTGNN & 19.076  & 12.961  & 31.564  & 20.824  & 9.032  & 34.087  & 15.396  & 10.170  & 24.934  \\
    STSGCN & 21.185  & 13.882  & 33.649  & 24.264  & 10.204  & 39.034  & 17.133  & 10.961  & 26.785  \\
    STFGNN & 19.830  & 13.021  & 31.870  & 22.072  & 9.212  & 35.805  & 16.636  & 10.547  & 26.206  \\
    STGODE & 20.849  & 13.781  & 32.825  & 22.976  & 10.142  & 36.190  & 16.819  & 10.623  & 26.240  \\
    STGNCDE & 19.211  & 12.772  & 31.088  & 20.620  & 8.864  & 34.036  & 15.455  & 9.921  & 24.813  \\
    \hline
    STTN  & 19.478  & 13.631  & 31.910  & 21.344  & 9.932  & 34.588  & 15.482  & 10.341  & 24.965  \\
    GMAN  & 19.139  & 13.192  & 31.601  & 20.967  & 9.052  & 34.097  & 15.307  & 10.134  & 24.915  \\
    TFormer & 18.916  & 12.711  & 31.349  & 20.754  & 8.972  & 34.062  & 15.192  & 9.925  & 24.883  \\
    ASTGNN & \underline{18.601}  & \underline{12.630}  & \underline{31.028}  & \underline{20.616}  & \underline{8.861}  & \underline{34.017}  & \underline{14.974}  & \underline{9.489}  & \underline{24.710}  \\
    \hline
    \name & \textbf{18.321 } & \textbf{12.103 } & \textbf{29.965 } & \textbf{19.832 } & \textbf{8.529 } & \textbf{32.870 } & \textbf{13.583 } & \textbf{9.046 } & \textbf{23.505 } \\
    \bottomrule
    \end{tabular}}
  \label{tab:res_graph}\end{table}

\begin{table*}[t]
  \centering
  \caption{Performance on Grid-based Datasets.}
   \resizebox{\textwidth}{!}{
    \begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
    \toprule
    Datasets & \multicolumn{6}{c|}{\taxi}                  & \multicolumn{6}{c|}{\bj}                  & \multicolumn{6}{c}{\bike} \\
    \hline
    Metrics & \multicolumn{3}{c|}{inflow} & \multicolumn{3}{c|}{outflow} & \multicolumn{3}{c|}{inflow} & \multicolumn{3}{c|}{outflow} & \multicolumn{3}{c|}{inflow} & \multicolumn{3}{c}{outflow} \\
    \hline
    Models & MAE   & MAPE(\%) & RMSE  & MAE   & MAPE(\%) & RMSE  & MAE   & MAPE(\%) & RMSE  & MAE   & MAPE(\%) & RMSE  & MAE   & MAPE(\%) & RMSE  & MAE   & MAPE(\%) & RMSE \\
    \hline
    STResNet & 14.492  & 14.543  & 24.050  & 12.798  & 14.368  & 20.633  & 19.636  & 17.831  & 34.890  & 19.616  & 18.502  & 34.597  & 4.767  & 31.382  & 6.703  & 4.627  & 30.571  & 6.559  \\
    DMVSTNet & 14.377  & 14.314  & 23.734  & 12.566  & 14.318  & 20.409  & 19.599  & 17.683  & 34.478  & 19.531  & 17.621  & 34.303  & 4.687  & 32.113  & 6.635  & 4.594  & 31.313  & 6.455  \\
    DSAN  & 14.287  & 14.208  & 23.585  & 12.462  & 14.272  & 20.294  & 19.384  & 17.465  & 34.314  & 19.290  & 17.379  & 34.267  & 4.612  & 31.621  & 6.695  & 4.495  & 31.256  & 6.367  \\
    \hline
    DCRNN & 14.421  & 14.353  & 23.876  & 12.828  & 14.344  & 20.067  & 22.121  & 17.750  & 38.654  & 21.755  & 17.382  & 38.168  & 4.236  & 31.264  & 5.992  & 4.211  & 30.822  & 5.824  \\
    STGCN & 14.377  & 14.217  & 23.860  & 12.547  & 14.095  & 19.962  & 21.373  & 17.539  & 38.052  & 20.913  & 16.984  & 37.619  & 4.212  & 31.224  & 5.954  & 4.148  & 30.782  & 5.779  \\
    GWNET & 14.310  & 14.198  & 23.799  & 12.282  & 13.685  & 19.616  & 19.556  & 17.187  & 36.159  & 19.550  & 15.933  & 36.198  & 4.151  & 31.153  & 5.917  & 4.101  & 30.690  & 5.694  \\
    MTGNN & 14.194  & 13.984  & 23.663  & 12.272  & 13.652  & 19.563  & 18.982  & 17.056  & 35.386  & 18.929  & 15.762  & 35.992  & 4.112  & 31.148  & 5.807  & 4.086  & 30.561  & 5.669  \\
    STSGCN & 15.604  & 15.203  & 26.191  & 13.233  & 14.698  & 21.653  & 23.825  & 18.547  & 41.188  & 24.287  & 19.041  & 42.255  & 4.256  & 32.991  & 5.941  & 4.265  & 32.612  & 5.879  \\
    STFGNN & 15.336  & 14.869  & 26.112  & 13.178  & 14.584  & 21.627  & 22.144  & 18.094  & 40.071  & 22.876  & 18.987  & 41.037  & 4.234  & 32.222  & 5.933  & 4.264  & 32.321  & 5.875  \\
    STGODE & 14.621  & 14.793  & 25.444  & 12.834  & 14.398  & 20.205  & 21.515  & 17.579  & 38.215  & 22.703  & 18.509  & 40.282  & 4.169  & 31.165  & 5.921  & 4.125  & 30.726  & 5.698  \\
    STGNCDE & 14.281  & 14.171  & 23.742  & 12.276  & 13.681  & 19.608  & 19.347  & 17.134  & 36.093  & 19.230  & 15.873  & 36.143  & 4.123  & 31.151  & 5.913  & 4.094  & 30.595  & 5.678  \\
    \hline
    STTN  & 14.359  & 14.206  & 23.841  & 12.373  & 13.762  & 19.827  & 20.583  & 17.327  & 37.220  & 20.443  & 15.992  & 37.067  & 4.160  & 31.208  & 5.932  & 4.118  & 30.704  & 5.723  \\
    GMAN  & 14.267  & 14.114  & 23.728  & 12.273  & 13.672  & 19.594  & 19.244  & 17.110  & 35.986  & 18.964  & 15.788  & 36.120  & 4.115  & 31.150  & 5.910  & 4.090  & 30.662  & 5.675  \\
    TFormer & 13.995  & 13.912  & 23.487  & 12.211  & 13.611  & 19.522  & 18.823  & 16.910  & 34.470  & 18.883  & 15.674  & 35.219  & 4.071  & 31.141  & 5.878  & 4.037  & 30.647  & 5.638  \\
    ASTGNN & \underline{13.844}  & \underline{13.692}  & \underline{23.177}  & \underline{12.112}  & \underline{13.602}  & \underline{19.201}  & \underline{18.798}  & \underline{16.101}  & \underline{33.870}  & \underline{18.790}  & \underline{15.584}  & \underline{33.998}  & \underline{4.068}  & \underline{31.131}  & \underline{5.818}  & \underline{3.981}  & \underline{30.617}  & \underline{5.609}  \\
    \hline
    \name & \textbf{13.152 } & \textbf{12.743 } & \textbf{21.957 } & \textbf{11.575 } & \textbf{12.820 } & \textbf{18.394 } & \textbf{17.832 } & \textbf{14.711 } & \textbf{31.606 } & \textbf{17.743 } & \textbf{14.649 } & \textbf{31.501 } & \textbf{3.950 } & \textbf{30.214 } & \textbf{5.559 } & \textbf{3.837 } & \textbf{29.914 } & \textbf{5.402 } \\
    \bottomrule
    \end{tabular}}
  \label{tab:res_grid}\end{table*}

\paratitle{Model Settings.}
All experiments are conducted on a machine with the NVIDIA GeForce 3090 GPU and 128GB memory. We implement \name~\footnote{\url{https://github.com/BUAABIGSCity/PDFormer}} with Ubuntu 18.04, PyTorch 1.10.1, and Python 3.9.7. The hidden dimension  is searched over \{16, 32, 64, 128\} and the depth of encoder layers  is searched over \{2, 4, 6, 8\}. The optimal model is determined based on the performance in the validation set. We train our model using AdamW optimizer~\cite{adam} with a learning rate of 0.001. The batch size is 16, and the training epoch is 200. 


\paratitle{Evaluation Metrics.} We use three metrics in the experiments: (1) Mean Absolute Error (MAE), (2) Mean Absolute Percentage Error (MAPE), and (3) Root Mean Squared Error (RMSE). Missing values are excluded when calculating these metrics. When we test the models on grid-based datasets, we filter the samples with flow values below 10, consistent with~\cite{dmvstnet}. Since the flow of CHIBike is lower than others, the filter threshold is 5. We repeated all experiments ten times and reported the average results.

\subsection{Performance Comparison}

The comparison results with baselines on graph-based and grid-based datasets are shown in Tab.~\ref{tab:res_graph} and Tab.~\ref{tab:res_grid}, respectively. The bold results are the best, and the underlined results are the second best. Based on these two tables, we can make the following observations. (1) Spatial-temporal deep learning models perform better than traditional time series prediction models such as VAR because the latter ignores the spatial dependencies in the traffic data. (2) Our \name significantly outperforms all baselines in terms of all metrics over all datasets according to Student's t-test at level 0.01. Compared to the second best method, \name achieves an average improvement of 4.58\%, 5.00\%, 4.79\% for MAE/MAPE/RMSE. (3) Among the GNN-based models, MTGNN and STGNCDE lead to competitive performance. Compared to these GNN-based models, whose message passing is immediate, \name achieves better performance because it considers the time delay in spatial information propagation. (4) As for the self-attention-based models, ASTGNN is the best baseline, which combines GCN and the self-attention module to aggregate neighbor information. Compared with ASTGNN, \name simultaneously captures short- and long-range spatial dependencies via two masking matrices and achieves good performance. (5) Although STResNet, DMVSTNet, and DSAN are designed for grid-based data, they need to extract features from different time periods, which are excluded here, resulting in relatively poor performance.

\subsection{Ablation Study}
To further investigate the effectiveness of different parts in \name, we compare \name with the following variants. (1) \textit{w/ GCN}: this variant replaces spatial self-attention (\ssa) with Graph Convolutional Network (GCN)~\cite{gcn}, which cannot capture the dynamic and long-range spatial dependencies. (2) \textit{w/o Mask}: this variant removes two masking matrices  and , which means each node attends to all nodes. (3) \textit{w/o GeoSAH}: this variant removes \gsah. (4) \textit{w/o SemSAH}: this variant removes \ssah. (5) \textit{w/o Delay}: this variant removes the delay-aware feature transformation module, which accounts for the spatial information propagation delay.

Fig.~\ref{fig:abla} shows the comparison of these variants on the \pfour and \taxi datasets. For the \taxi dataset, only the results for inflow are reported since the results for outflow are similar. Based on the results, we can conclude the following: (1) The results show the superiority of \ssa over GCN in capturing dynamic and long-range spatial dependencies. (2) \name leads to a large performance improvement over \textit{w/o Mask}, highlighting the value of using the mask matrices to identify the significant node pairs. In addition, \textit{w/o SemSAH} and \textit{w/o GeoSAH} perform worse than \name, indicating that both local and global spatial dependencies are significant for traffic prediction. (3) \textit{w/o Delay} performs worse than \name because this variant ignores the spatial propagation delay between nodes but considers the spatial message passing as immediate.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figure/abla.pdf}
    \caption{Ablation Study on \pfour and \taxi inflow.}
    \label{fig:abla}
\end{figure}







\subsection{Case Study}
In this section, we analyze the dynamic spatial-temporal attention weight map learned by the spatial-temporal encoder of \name to improve its interpretability and demonstrate the effectiveness of focusing on short- and long-range spatial dependencies simultaneously.

We compare and visualize the attention map in two cases, \ie with or without the two spatial mask matrices  and . Here, for simplicity, we merge the attention map of \gsah and \ssah. As shown in Fig.~\ref{fig:case1}(a),(d), without the mask matrices, the model focuses on the major urban ring roads (or highways) with high traffic volume, or the attention distribution is diffuse, and almost the entire city shares the model's attention. However, low-traffic locations should focus on locations with similar patterns rather than hot locations. Moreover, locations that are too far away have little impact on the current location. The model performance will weaken if it focuses on all locations diffusely. Instead, when  and  are introduced, attention focuses on surrounding locations and distant similar-pattern locations as shown in Fig.~\ref{fig:case1}(b),(e).



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figure/case3.pdf}
    \caption{Case Study of Attention Map.}
    \label{fig:case1}
\end{figure}



Let us take Region 592 in Fig.~\ref{fig:case1}(b) as an example. Highway S12 passes through this region, so the traffic volume is always high. In addition to the regions located upstream and downstream of the highway, region 592 also focuses on regions 648 and 753. From Fig.~\ref{fig:case1}(c), we can see that these two regions have similar historical traffic volumes as 592. Besides, from Fig.~\ref{fig:case1}(h)(i), these two regions are located near the Guomao Interchange and Beijing Second Ring Road, respectively, which are similar to 592 in their cities function as major traffic hubs. In another case, region 252 has low traffic volume, but we can observe similar patterns from regions 370, 403, 453, and 842 that region 252 focused on, \ie similar functional and historical traffic variations.

This case study shows that after introducing the spatial mask matrices, \name not only considers the short-range spatial dependencies but also identifies the global functional area to capture the long-range spatial dependencies. The above ablation experiments also quantitatively show that the model performance drops sharply after removing the mask matrices, which supports the idea presented here.

\subsection{Model Efficiency Study}
Due to the better performance of the attention-based models, we compare the computational cost of \name with other self-attention-based baselines on the \pfour and the \taxi datasets. Tab.~\ref{tab:time_cost} reports the average training and inference time per epoch. We find that \name achieves competitive computational efficiency in both short- and long-term traffic prediction. Compared to the best performing baseline ASTGNN on \pfour, \name reduces the training and inference time of over 35\% and 80\%, respectively. GMAN and ASTGNN retain a time-consuming encoder-decoder structure, which is replaced by a forward procedure in STTN and \name. TFormer performs only spatial attention, resulting in less computation time.

\section{Related Work}

\subsection{Deep Learning for Traffic Prediction}


In recent years, more and more researchers have employed deep learning models to solve traffic prediction problems. Early on, convolutional neural networks (CNNs) were applied to grid-based traffic data to capture spatial dependencies in the data~\cite{STResNet, DSAN}. Later, thanks to the powerful ability to model graph data, graph neural networks (GNNs) were widely used for traffic prediction~\cite{mrange,agcrn,stgdn,fc-gaga, ccrnn,dmstgcn,stden,msdr}. Recently, the attention mechanism has become increasingly popular due to its effectiveness in modeling the dynamic dependencies in traffic data~\cite{astgcn,stgnn,stconvlstm,astgnn,MGT}. Unlike these work, our proposed \name not only considers the dynamic and long-range spatial dependencies through a self-attention mechanism but also incorporates the time delay in spatial propagation through a delay-aware feature transformation layer.

\subsection{Transformer}




Transformer~\cite{transformer} is a network architecture based entirely on self-attention mechanisms. Transformer has been proven effective in multiple natural language processing (NLP) tasks. In addition, large-scale Transformer-based pre-trained models such as BERT~\cite{devlin2018bert} have achieved great success in the NLP community. Recently, Vision Transformers have attracted the attention of researchers, and many variants have shown promising results on computer vision tasks~\cite{image, swin}. In addition, the Transformer architecture performs well in representation learning, which has been demonstrated in recent studies~\cite{graphbert, rapt, jiang2023start}.

\begin{table}[t]
  \centering
  \caption{Training and inference time per epoch comparison between self-attention-based models. (Unit: seconds)}
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c}
    \toprule
    Dataset & \multicolumn{2}{c|}{\pfour} & \multicolumn{2}{c}{\taxi} \\
    \hline
    Model & Training & Inference & Training & Inference \\
    \hline
    GMAN  & 501.578 & 38.844 & 130.672 & 4.256 \\
    ASTGNN & 208.724 & 52.016 & 119.092 & 4.601 \\
\name & 133.871 & 8.120 & 85.305 & 2.734 \\
    STTN  & 100.398 & 12.596 & 68.036 & 2.650 \\
    TFormer & 71.099 & 7.156 & 76.169 & 2.575 \\
    \bottomrule
    \end{tabular}}
  \label{tab:time_cost}\end{table}

\section{Conclusion}
In this work, we proposed a novel \name model with spatial-temporal self-attention for traffic flow prediction. Specifically, we developed a spatial self-attention module that captures the dynamic and long-range spatial dependencies and a temporal self-attention module that discovers the dynamic temporal patterns in the traffic data. We further designed a delay-aware feature transformation module to explicitly model the time delay in spatial information propagation. We conducted extensive experiments on six real-world datasets to demonstrate the superiority of our proposed model and visualized the learned attention map to make the model interpretable. As future work, we will apply \name to other spatial-temporal prediction tasks, such as wind power forecasting~\cite{jiang2018buaa_bigscity}. In addition, we will explore the pre-training techniques in traffic prediction to solve the problem of insufficient data.
\clearpage

\clearpage
\section*{Acknowledgments}
This work was supported by the National Key R\&D Program of China (Grant No. 2019YFB2102100). Prof. Wang’s work was supported by the National Natural Science Foundation of China (No. 72171013, 82161148011, 72222022), the Fundamental Research Funds for the Central Universities (YWF-22-L-838) and the DiDi Gaia Collaborative Research Funds. Prof. Zhao’s work was supported by the National Natural Science Foundation of China (No. 62222215).

\bibliography{aaai23.bib}


































































\end{document}
