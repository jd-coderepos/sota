\newcommand{\ones}{\mathbf{1}}
\newcommand{\sbv}[1]{e^{(#1)}}

\subsection{Notation and preliminaries}
We begin by restating and adding to the notation used in Section~\ref{sec:theory}.
For a model with parameter vector  and input vector , we let  denote the model's logit output for -way classification. Throughout, we fix two endpoint models  and , and for an interpolation parameter  define

to be the ``soup'' weight averaged model and its corresponding logits. We also write

for the logits of the ensemble model. We write

for the difference of the two endpoints.

For a logit vector  and a ground-truth label , denote the cross-entropy loss by

For some distribution over  we write the expected -calibrated log losses of the soup and ensemble as

respectively.

We have the following expression for the derivatives of cross entropy w.r.t.\ logits. The gradient is

where  is the th standard basis vector and  has  in its th entry. The Hessian is

so that for any , we have


Finally, we use  to denote a vector in  whose th entry is . Similarly, 
   denotes a vector in  whose th entry is , where gradients and Hessian are with respect to . 




\subsection{An exact expression for logit difference}
We use the fundamental theorem of calculus and elemntary algebraic manipulation to obtain an exact integral form for the difference between the soup and ensemble logits. To streamline notation we drop the dependence of the logits on the input .


where

Note that


\subsection{Derivation of approximation}\label{app:theory-deriv}

We continue to suppress the dependence on  in order to simplify notation.
We begin with the following first order approximation of the pointwise log-loss difference between the ensemble and soup, which is also a lower bound due to convexity. 


Now, we approximate the ensemble and soup logit difference using eq.~\ref{eq:fose-fwse} by assuming that  for all ; this holds when the logits are approximately quadratic along the line between the checkpoints. The resulting approximation is


Combining the two approximation above, we obtain


To relate this expression to the Hessian of the loss with respect to the parameters, we note that for any  (by the chain rule)
 
When setting , we note that the second term on the RHS is (up to a constant) our approximation for the loss difference). Recalling the expression for the cross-entropy Hessian, the first term is


As a final approximation, we let

this holds when logits are too far from linear in . 

Substituting back and making  explicit, we obtain

where we have used


Scaling all logits by , the approximation becomes

Averaging the result over , we arrive at the approximation~\eqref{eq:approx}, which we repeat here for ease of reference:


\subsection{Detailed empirical evaluations}\label{app:theory-eval}

\paragraph{Evaluation setup.} We evaluated our bounds on checkpoints from the ViT-B/32 fine-tuning experiments from the \emph{extreme grid} search described in Section~\ref{app:clipdetails}. We selected three learning rate values (,  and ), two levels augmentation (none and RandAugment+MixUp), and considered two different random seeds ( and ). From these checkpoints (as well as the initialization) we constructed the following  pairs:
\begin{itemize}
	\item All pairs with different learning rate, the same augmentation level and seed 0,
	\item All pairs with the same learning rate, different augmentation level and seed 0,
	\item All pairs with the same learning rate and augmentation level, but different seeds,
	\item All checkpoints with seed 0 coupled with the initialization.
\end{itemize}  
This results in 21 pairs overall. For each pair and each  we evaluated , as well as the approximation~\eqref{eq:approx}. We performed this evaluation on the ImageNet validation set as well as on the 5 OOD test sets considered throughout this paper.  

\paragraph{The effect of temperature calibration.}
Since our ultimate goal is to accurately predict the difference in error rather than the difference in loss, we introduce the inverse-temperature parameter  to the loss, and tune it to calibrate the soup model. Specifically, for every model pair, value of  and test set, we take . 

While choosing  based on the soup rather the ensemble might skew the loss in favor of the soup, it has no effect on the difference in prediction error. Moreover, in preliminary experiments calibrating the ensemble produced very similar results. In contrast, as shown in Figure~\ref{fig:theory-eval}, fixing  throughout results in far poorer prediction of the difference in error.  