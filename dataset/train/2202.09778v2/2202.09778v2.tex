\documentclass{article}
\usepackage{iclr2022_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[abs]{overpic}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{diagbox}
\usepackage{newfloat}
\usepackage{colortbl}
\usepackage{tabu}
\usepackage{color}
\usepackage{enumitem}

\newtheorem{property}{Property}[section]

\title{Pseudo Numerical Methods for Diffusion Models on Manifolds}



\author{Luping Liu, Yi Ren, Zhijie Lin \& Zhou Zhao\thanks{Corresponding author} \\
Zhejiang University \\
\texttt{\{luping.liu,rayeren,linzhijie,zhaozhou\}@zju.edu.cn} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
   Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce final samples. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations.
   According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules.\footnote{Our implementation is available at \url{https://github.com/luping-liu/PNDM}.}
\end{abstract}

\section{Introduction}

Denoising Diffusion Probabilistic Models (DDPMs) \citep{sohl-dickstein2015, Ho2020} is a class of generative models which model the data distribution through an iterative denoising process reversing a multi-step noising process. DDPMs have been applied successfully to a variety of applications, including image generation \citep{Ho2020,Song2020}, text generation \citep{Hoogeboom2021, Austin2021}, 3D point cloud generation \citep{Luo2021}, text-to-speech \citep{kong2021a, chen2020} and image super-resolution \citep{saharia2021}.

Unlike Generative Adversarial Networks (GANs) \citep{goodfellow2014}, which require careful hyperparameter tuning according to different model structures and datasets, DDPMs can use similar model structures and be trained by a simple denoising objective which makes the models fit the noise in the data. To generate samples, the iterative denoising process starts from white noise and progressively denoises it into the target domain according to the noise predicted by the model at every step. However, a critical drawback of DDPMs is that DDPMs require hundreds to thousands of iterations to produce high-quality samples and need to pass through a network at least once at every step, which makes the generation of a large number of samples extremely slow and infeasible. In contrast, GANs only need one pass through a network. 

There have been many recent works focusing on improving the speed of the denoising process. Some works search for better variance schedules, including \citet{Nichol2021} and \citet{Watson2021}. Some works focus on changing the inference equation, including \citet{Song2020a} and \citet{Song2020}. Denoising Diffusion Implicit Models (DDIMs) \citep{Song2020a} relying on a non-Markovian process accelerate the denoising process by taking multiple steps every iteration. Probability Flows (PFs) \citep{Song2020} build a connection between the denoising process and solving ordinary differential equations and use numerical methods of differential equations to accelerate the denoising process. Additionally, we introduce more related works in Appendix \ref{gen_inst}.

\begin{wrapfigure}{R}{0.6\linewidth}
   \vspace*{-0.5cm}
   \centering
   \includegraphics[width=\linewidth]{data1/celeba_t.png}
   \caption{5, 10, 20, 50 and 100-steps generated results using DDIMs, classical numerical methods and PNDMs.}
   \vspace*{-0.5cm}
\end{wrapfigure}

However, this direct connection between DDPMs and numerical methods (e.g., forward Euler method, linear multi-step method and Runge-Kutta method \citep{Sauer2017}) has weaknesses in both speed and effect (see Section \ref{formula_trans}). Some numerical methods are straightforward, like the forward Euler method, but they can only trade quality for speed. Some numerical methods can accelerate the reverse process without loss of quality, like the Runge-Kutta method, but they need to propagate forward more times along a neural network at every step. Furthermore, we also notice that numerical methods can introduce noticeable noise at a high speedup rate, which makes high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs. This phenomenon is also mentioned in \citet{salimans2022progressive}.

To figure out the reason for the performance degradation in classical numerical methods, we conduct some analyses and find that classical numerical methods may sample data far away from the main distribution area of the data, and the inference equations of DDPMs do not satisfy a necessary condition of numerical methods at the last several steps (see Section \ref{sec_pro_cnm}).

To tackle these problems, we design new numerical methods called pseudo numerical methods for diffusion models (PNDMs) to generate samples along a specific manifold in $\mathbb{R}^n$, which is the high-density region of the data. We first compute the corresponding differential equations of diffusion models directly and self-consistently, which builds a theoretical connection between DDPMs and numerical methods. Considering that classical numerical methods cannot guarantee to generate samples on certain manifolds, we provide brand-new numerical methods called pseudo numerical methods based on our theoretical analyses. We also find that DDIMs are simple cases of pseudo numerical methods, which means that we also provide a new way to understand DDIMs better. Furthermore, we find that the pseudo linear multi-step method is the fastest method for diffusion models under similar generated quality.

Besides, we provide a detailed theoretical analysis of our new theory and give visualization results to support our theory intuitively. According to our experiments, our methods have several advantages:

\begin{itemize}[leftmargin=*]
   \item Our methods combine the benefits of DDIMs and high-order numerical methods successfully. We theoretically prove that our new methods PNDMs are second-order convergent while DDIMs are first-order convergent, which makes PNDMs 20x faster without loss of quality on Cifar10 and CelebA.
   \item Our methods can reduce the best FID of pre-trained models with even shorter sampling time. With only 250 steps, our new denoising process can reduce the best FID by around 0.4 points Cifar10 and CelebA. We achieve a new SOTA FID score of 2.71 on CelebA.
   \item Our methods work well with different variance schedules, which means that our methods have a good generalization and can be used together with those works introducing better variance schedules to accelerate the denoising process further.
\end{itemize}


\section{Background}
\label{background}

In this section, we introduce some backgrounds. Firstly, we present the classical understanding of DDPMs. Then we provide another understanding based on \citet{Song2020}, which inspires us to use numerical methods to accelerate the denoising process of diffusion models. After that, we introduce some background on numerical methods used later in this paper.

\subsection{Denoising Diffusion Probabilistic Models}

DDPMs model the data distribution from Gaussian distribution to image distribution through an iterative denoising process. Let $x_0$ be an image, then the diffusion process is a Markov process and the reverse process has a similar form to the diffusion process, which satisfies:
\begin{equation}
   \begin{split}
      x_{t+1} \sim &\mathcal{N}(\sqrt{1-\beta_t}x_t, \beta_t \text{I}),\ t=0, 1, \cdots, N-1. \\
      x_{t-1} \sim &\mathcal{N}(\mu_\theta(x_t, t), \beta_\theta(x_t, t) \text{I}),\ t= N, N-1, \cdots, 1.
   \end{split}
\end{equation}
Here, $\beta_t$ controls the speed of adding noise to the data, calling them the variance schedule. $N$ is the total number of steps of the denoising process. $\mu_\theta$ and $\beta_\theta$ are two neural networks, and $\theta$ are their parameters.


\citet{Ho2020} get some statistics estimations of $\mu_\theta$ and $\beta_\theta$. According to the properties of the conditional Gaussian distribution, we have:
\begin{equation}
   \begin{split}
      &q(x_t|x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t) \text{I}), \\
      &q(x_{t-1}| x_t, x_0) = \mathcal{N}(\bar{\mu}_t(x_t, x_0), \bar{\beta}_t \text{I}).
   \end{split}
   \label{qt}
\end{equation}
Here, $\alpha_t=1-\beta_t$, $\bar{\alpha}_t = \prod_{i=1}^t\alpha_i$, $\bar{\mu}_t = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t$ and $\bar{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$.  Then this paper sets $\beta_\theta = \bar{\beta}_t$ and designs a objective function to help neural networks to represent $\mu_\theta$.

\textbf{Objective Function} The objective function is defined by: 
\begin{equation}
   \begin{split}
      L_{t-1}  &= \mathbb{E}_q \left[||\bar{\mu}_t(x_t, x_0)-\mu_\theta(x_t, t)||^2\right] \\
               &= \mathbb{E}_{x_0, \epsilon} \left[||\frac{1}{\sqrt{\alpha_t}}\left(x_t(x_0,   \epsilon) - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right) - \mu_\theta(x_t(x_0, \epsilon), t) ||^2\right] \\
               &= \mathbb{E}_{x_0, \epsilon}\left[\frac{\beta_t^2}{\alpha_t(1-\bar{\alpha}_t)}||\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0+ \sqrt{1-\bar{\alpha}_t}\epsilon, t)||^2\right].
   \end{split}
   \label{ob_func_raw}
\end{equation}
Here, $x_t(x_0, \epsilon)= \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon,\ \epsilon\sim \mathcal{N}(0,1)$, $\epsilon_\theta$ is an estimate of the noise $\epsilon$. The relationship between $\mu_\theta$ and $\epsilon_\theta$ is $\mu_\theta=\frac{1}{\sqrt{\alpha}_t}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta)$. Because $\epsilon \sim \mathcal{N}(0, 1)$, we assume that the mean and variance of $\epsilon_\theta$ are 0 and 1.



\subsection{Stochastic Differential Equation}
According to \citet{Song2020}, there is another understanding of DDPMs. The diffusion process can be treated as solving a certain stochastic differential equation $dx = (\sqrt{1-\beta(t)}-1)x(t)dt + \sqrt{\beta(t)}dw$. According to \citet{Anderson1982}, the denoising process also satisfies a similar stochastic differential equation:
\begin{equation}
   dx = \left((\sqrt{1-\beta(t)}-1)x(t) - \beta(t)\epsilon_\theta(x(t), t)\right)dt + \sqrt{\beta(t)}d\bar{w}.
   \label{eq_re_sde}
\end{equation}
This is Variance Preserving stochastic differential equations (VP-SDEs). Here, we change the domain of $t$ from $[1, N]$ to $[0,1]$. When $N$ tends to infinity, $\{\beta_i\}_{i=1}^N$, $\{x_i\}_{i=1}^N$ become continuous functions $\beta(t)$ and $x(t)$ on $[0,1]$. \citet{Song2020} also show that this equation has an ordinary differential equation (ODE) version with the same marginal probability density as Equation (\ref{eq_re_sde}):
\begin{equation}
   dx = \left((\sqrt{1-\beta(t)}-1)x(t) - \frac{1}{2}\beta(t)\epsilon_\theta(x(t), t)\right)dt.
   \label{reverse_ode}
\end{equation}
This different denoising equation with no random item and the same diffusion equation together is Probability Flows (PFs). These two denoising equations show us a new possibility that we can use numerical methods to accelerate the reverse process. As far as we know, DDIMs first try to remove this random item, so PFs can also be treated as an acceleration of DDIMs, while VP-SDEs are an acceleration of DDPMs.

\subsection{Numerical Method}
\label{sec_num_method}

Many classical numerical methods can be used to solve ODEs, including the forward Euler method, Runge-Kutta method and linear multi-step method \citep{Sauer2017}.

\textbf{Forward Euler Method} For a certain differential equation satisfying $\frac{dx}{dt} = f(x, t)$. The trivial numerical method is forward Euler method satisfying $x_{t+\delta} = x_t + \delta f(x_t, t)$. 

\textbf{Runge-Kutta Method} Runge-Kutta method uses more information at every step, so it can achieve higher accuracy \footnote{To achieve higher accuracy, more information is not enough. The reason why these methods achieve higher accuracy can be found in Appendix \ref{order_method}}.
Runge-Kutta method satisfies:
\begin{equation}
   \label{eq_rk}
   \begin{cases}
      & k_1 = f(x_t, t)\qquad\qquad\ \ \ ,\quad k_2 = f(x_t+\frac{\delta}{2}k_1, t+\frac{\delta}{2})\\
      & k_3 = f(x_t+\frac{\delta}{2}k_2, t+\frac{\delta}{2})\,,\quad k_4 = f(x_t+\delta k_3, t+\delta)\\
      & x_{t+\delta} = x_t + \frac{\delta}{6}(k_1+2k_2+2k_3+k_4) .
   \end{cases}
\end{equation}

\textbf{Linear Multi-Step Method} Linear multi-step method is another numerical method and satisfies:
\begin{equation}
   \label{eq_ml}
   x_{t+\delta}=x_t+\frac{\delta}{24}(55f_t-59f_{t-\delta}+37f_{t-2\delta}-9f_{t-3\delta}),\ f_{t}=f(x_t, t).
\end{equation}


\section{Pseudo Numerical Method for DDPM}
\label{fast_method}

In this section, we first compute the corresponding differential equations of diffusion models to build a direct connection between DDPMs and numerical methods. As a byproduct, we can directly use pre-trained models from DDPMs. After establishing this connection, we provide detailed analyses on the weakness of classical numerical methods. To solve the problems in classical numerical methods, we dive into the structure of numerical methods by dividing their equations into a gradient part and a transfer part and define pseudo numerical methods by introducing nonlinear transfer parts. We find that DDIMs can be regarded as simple pseudo numerical methods. Then, We explore the pros and cons of different numerical methods and choose the linear multi-step method to make numerical methods faster. Finally, we summarize our findings and analyses and safely propose our novel pseudo numerical methods for diffusion models (PNDMs), which combine our proposed transfer part and the gradient part of the linear multi-step method. Furthermore, we analyze the convergence order of pseudo numerical methods to demonstrate the effectiveness of our methods theoretically.



\subsection{Formula Transformation}
\label{formula_trans}
According to \citet{Song2020a}, the reverse process of DDPMs and DDIMs satisfies:
\begin{equation}
   x_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}\right) + \sqrt{1-\bar{\alpha}_{t-1}-\sigma^2_t}\epsilon_\theta(x_t, t) + \sigma_t \epsilon_t.
   \label{ddpm_raw}
\end{equation}
Here, $\sigma_t$ controls the ratio of random noise. If $\sigma_t$ equals one, Equation (\ref{ddpm_raw}) represents the reverse process of DDPMs; if $\sigma_t$ equals zero, this equation represents the reverse process of DDIMs. And only when $\sigma_t$ equals zero, this equation removes the random item and becomes a discrete form of a certain ODE. Theoretically, the numerical methods that can be used on differential equations with random items are limited. And \citet{Song2020} have done enough research in this case. Empirically, \citet{Song2020a} have shown that DDIMs have a better acceleration effect when the number of total steps is relatively small. Therefore, our work concentrate on the case $\sigma_t$ equals zero.

To find the corresponding ODE of Equation (\ref{ddpm_raw}), we replace discrete $t-1$ with a continuous version $t-\delta$ according to \citep{Song2020a} and change this equation into a differential form, namely, subtract $x_t$ from both sides of this equation:
\begin{equation}
   x_{t-\delta}-x_{t}=(\bar{\alpha}_{t-\delta}-\bar{\alpha}_t) \left(\frac{x_t}{\sqrt{\bar{\alpha}_t}(\sqrt{\bar{\alpha}_{t-\delta}}+\sqrt{\bar{\alpha}_t})} - 
   \frac{\epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}(\sqrt{(1-\bar{\alpha}_{t-\delta})\bar{\alpha}_{t}} + \sqrt{(1-\bar{\alpha}_{t})\bar{\alpha}_{t-\delta}})}\right).
   \label{ddim_diff}
\end{equation}
Because $\delta$ is a continuous variable from $0$ to $t$, we can now compute the derivative of the generation data $x_t$ and get that $\lim\limits_{\delta \to 0} \frac{x_{t}-x_{t-\delta}}{\delta}=-\bar{\alpha}'(t)\left(\frac{x(t)}{2\bar{\alpha}(t)}-\frac{\epsilon_\theta(x(t),t)}{2\bar{\alpha}(t)\sqrt{1-\bar{\alpha}(t)}}\right)$. Here, $\bar{\alpha}(t)$ is the continuous version of $\{\bar{\alpha}_i\}_{i=1}^N$ like the definition of $x(t)$. Therefore, the corresponding ODE when $\delta$ tends to zero of Equation (\ref{ddim_diff}) is:
\begin{equation}
   \frac{dx}{dt} = -\bar{\alpha}'(t)\left(\frac{x(t)}{2\bar{\alpha}(t)}-\frac{\epsilon_\theta(x(t),t)}{2\bar{\alpha}(t)\sqrt{1-\bar{\alpha}(t)}}\right).
   \label{eq_limit}
\end{equation}





\subsection{Classical Numerical Method}
\label{sec_pro_cnm}

After getting the target ODE, the easiest way to solve it is through classical numerical methods. However, We notice that classical numerical methods can introduce noticeable noise at a high speedup rate, making high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs. This phenomenon is also mentioned in \citet{salimans2022progressive}. To make better use of numerical methods, we analyze the differences between Equation (\ref{eq_limit}) and usual differential equations and find two main problems when we directly use numerical methods with diffusion models.

\begin{wrapfigure}{R}{0.38\linewidth}
   \vspace*{-0.45cm}
   \centering
   \begin{overpic}[width=0.38\textwidth, keepaspectratio, trim=90 20 145 150, clip]{data1/norm_dist_1.jpg}
      \put(0,0){\includegraphics[width=0.38\textwidth, keepaspectratio, trim=25 5 40 50, clip]{data1/norm_dist.pdf}}
   \end{overpic}
   \caption{the density distribution of the norm of the data.}
   \label{data_dist}
   \vspace*{-0.6cm}
\end{wrapfigure}

The first problem is that the neural network $\epsilon_\theta$ and Equation (\ref{eq_limit}) are well-defined only in a limited area. Equation (\ref{qt}) shows that the data $x_t$ is generated along a curve close to an arc. According to Figure \ref{data_dist}, most of $x_t$ is concentrated in a band with a width of around 0.1, namely the red area in Figure \ref{data_dist}. This means that the neural network $\epsilon_\theta$ cannot get enough examples to fit the noise successfully away from this area. Therefore, $\epsilon_\theta$ and Equation (\ref{eq_limit}), which contains $\epsilon_\theta$, are only well-defined in this limited area. However, all classical numerical methods generate results along a straight line instead of an arc. The generation process may generate samples away from the well-defined area and then introduce new errors. In Section \ref{manifold_ex} we will give more visualization results to support this.

The second problem is that Equation (\ref{eq_limit}) is unbounded at most cases. We find that for most linear variance schedules $\beta_t$, Equation (\ref{eq_limit}) tends to infinity when $t$ tends to zero (see Appendix \ref{existence of derivative}), which does not satisfy the condition of numerical methods mentioned in Section \ref{sec_num_method}. This is an apparent theoretical weakness that previous works have not explored. On the contrary, in the original DDPMs and DDIMs, the prediction of the sample $x_t$ and the noise $\epsilon_\theta$ in the data are more and more precise as the index $t$ tends to zero (see Appendix \ref{sec_epsi}). This means original diffusion models do not make a significant error in the last several steps, whereas using numerical methods on Equation (\ref{eq_limit}) does. This explains why DDIMs are better than higher-order numerical methods.




\subsection{Pseudo Numerical Method on Manifold}
\label{sec_pseudo}

The first problem above shows that we should try to solve our problems on certain manifolds. Here, target manifolds are the high-density region of the data $x_t$ of DDPMs, which is defined by $x_t(x_0, \epsilon)= \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$, $\epsilon\sim \mathcal{N}(0,1)$.  \citet{Hairer1996} show several numerical methods to solve differential equations on manifolds that have analytic expressions. Unfortunately, it's challenging to use the above expression of manifolds. Because we do not know the target $x_0$ in the reverse process and random items $\epsilon$ are hard to handle, too. 

In this paper, we design a different way that we make our new equation of denoising process more fits with the equation of original DDIMs to make their results share similar data distribution. Firstly, we divide the classical numerical methods into two parts: gradient and transfer parts. The gradient part determines the gradient at each step, while the transfer part generates the result at the next step. For example, linear multi-step method can be divided into the gradient part $f'=\frac{\delta}{24}(55f_t-59f_{t-\delta}+37f_{t-2\delta}-9f_{t-3\delta})$ and the transfer part $x_{t+\delta}=x_t+\delta f'$. All classical numerical methods have the same linear transfer part, while gradient parts are different.

We define those numerical methods which use a nonlinear transfer part as pseudo numerical methods. And an expected transfer part should have the property that when the result from the gradient part is precise, then the result of the transfer part is as close to the manifold as possible and the error of this result is as small as possible. We find that Equation (\ref{ddim_diff}) satisfies such property.
\begin{property}
   If $\epsilon$ is the precise noise in $x_t$, then the result of $x_{t-\delta}$ from Equation (\ref{ddim_diff}) is also precise.
   \label{pro_pre}
   \vspace*{-0.4cm}
\end{property}
And we put the proof of this property in Appendix \ref{sec_epsi}. Therefore, we use: 

\begin{equation}
   \phi(x_t, \epsilon_t, t, t-\delta) = \frac{\sqrt{\bar{\alpha}_{t-\delta}}}{\sqrt{\bar{\alpha}_t}}x_t - 
   \frac{(\bar{\alpha}_{t-\delta}-\bar{\alpha}_t)}{\sqrt{\bar{\alpha}_t}(\sqrt{(1-\bar{\alpha}_{t-\delta})\bar{\alpha}_{t}} + \sqrt{(1-\bar{\alpha}_{t})\bar{\alpha}_{t-\delta}})}\epsilon_t
   \label{transfor}
\end{equation}

as the transfer part and $\epsilon_\theta$ as the gradient part. That if $\epsilon_\theta$ is precise, the result of $x_{t-\delta}$ is also precise, which means that $\epsilon_\theta$ can determine the direction of the denoising process to generate the final results. Therefore, such a choice also satisfies the definition of a gradient part. Now, we have our gradient part $\epsilon_\theta$ and transfer part $\phi$. 

This combination solves the two problems mentioned above successfully. Firstly, our new transfer parts do not introduce new errors. This property also means that it keeps the results at the next step on the target manifold because generating samples away is a kind of error. This shows that we solve the first problem. Secondly, we know that the prediction of $\epsilon_\theta$ is more and more precise in the reverse process in the above subsection. And our new transfer part can generate precise results according to the precise prediction of $\epsilon_\theta$. Therefore, our generation results are more and more precise using pseudo numerical methods, while classical numerical methods can introduce obvious error at the last several steps. This shows that we solve the second problem, too. We also find that their combination $\phi(x_t, \epsilon_\theta(x_t, t), t, t-1)$ is just the inference equation used by DDIMs, so DDIMs is a simple case of pseudo numerical methods. Here, we define DDIMs as DDIMs*, emphasizing that it is a pseudo numerical method.


\subsection{Gradient Part}
\label{LMSM}

Because we split numerical methods into two parts, we can use the same gradient part from different classical numerical methods freely (e.g., linear multi-step method), although we change the transfer part of our inference equation. Our theoretical analyses and experiments show that the gradient part from different classical methods can work well with our new transfer part (see Section \ref{sec_corder}, \ref{sec_qua_eff}). By using the same gradient part of the linear multi-step method, we have:

\begin{figure}[h]
   \vspace*{-0.2cm}
   \begin{minipage}[t]{0.54\linewidth}
      \vspace*{-0.1cm}
      \begin{equation}
         \begin{cases}
            &e_t = \epsilon_\theta(x_t, t)\\
            &e_t' = \frac{1}{24}(55e_t-59e_{t-\delta}+37e_{t-2\delta}-9e_{t-3\delta}) \\
            &x_{t+\delta} = \phi(x_t, e_t', t, t+\delta).
         \end{cases}
         \label{one_order}
      \end{equation}
      By using the same gradient part of Runge-Kutta method, we have:
      \begin{equation}
         \begin{cases}
            & e_t^1 = \epsilon_\theta(x_t, t)\\
            & x_t^1 = \phi(x_t, e_t^1, t, t+\frac{\delta}{2}) \\
            & e_t^2 = \epsilon_\theta(x_t^1, t+\frac{\delta}{2})\\
            & x_t^2 = \phi(x_t, e_t^2, t, t+\frac{\delta}{2}) \\
            & e_t^3 = \epsilon_\theta(x_t^2, t+\frac{\delta}{2})\\
            & x_t^3 = \phi(x_t, e_t^3, t, t+\delta) \\
            & e_t^4 = \epsilon_\theta(x_t^4, t+\delta)\\
            & e_t' = \frac{1}{6}(e_t^1+2e_t^2+2e_t^3+e_t^4) \\
            & x_{t-\delta} = \phi(x_t, e_t', t, t+\delta).
         \end{cases}
         \label{four_order}
      \end{equation}
   \end{minipage}
   \ \ 
   \vspace*{-\baselineskip}
   \begin{minipage}[t]{0.43\linewidth}
      \vspace*{-0.2cm}
      \begin{algorithm}[H]
         \small
         \caption{DDIMs}
         \label{origin_alg}
         \begin{algorithmic}[1]
            \STATE $x_T \sim \mathcal{N}(0, I)$
            \FOR {$t=T-1, \cdots, 1, 0$}
               \STATE $x_t = \phi(x_{t+1}, \epsilon_\theta(x_{t+1}, t+1), t+1, t)$ \\
            \ENDFOR
            \RETURN $x_0$
         \end{algorithmic}
      \end{algorithm}
      \vspace*{-0.7cm}
      \begin{algorithm}[H]
         \small
         \caption{PNDMs}
         \label{fourth_alg}
         \begin{algorithmic}[1]
            \STATE $x_T \sim \mathcal{N}(0, I)$
            \FOR {$t=T-1, T-2, T-3$}
               \STATE $x_t, e_t = \text{PRK}(x_{t+1}, t+1, t)$ \\
            \ENDFOR
            \FOR {$t=T-4, \cdots, 1, 0$}
               \STATE $x_t, e_t = \text{PLMS}(x_{t+1}, \{e_p\}_{p>t}, t+1, t)$
            \ENDFOR
            \RETURN $x_0$
         \end{algorithmic}
      \end{algorithm}
      \vspace*{-0.5cm}
   \end{minipage}
\end{figure}

Abbreviate Equation (\ref{one_order}) and (\ref{four_order}) as $x_{t+\delta}, e_t = \text{\small{PLMS}}(x_t, \{e_p\}_{p<t}, t, t+\delta)$, $x_{t+\delta}, e_t^1 = \text{\small{PRK}}(x_t, t, t+\delta)$.








Here, we have provided three kinds of pseudo numerical methods. Although advanced numerical methods can accelerate the denoising process, some may have to compute the gradient part $\epsilon_\theta$ more times at every step, like the Runge-Kutta method. Propagating forward four times along a neural network makes the denoising process slower. However, we find that the linear multi-step method can reuse the result of $\epsilon_\theta$ four times and only compute $\epsilon_\theta$ once at every step. And theoretical analyses tell us that the Runge-Kutta and linear multi-step method have the same convergence order and similar results. 

\begin{wraptable}{r}{7.2cm}
   \small
   \begin{tabular}{|l|l|p{65pt}|}
   \hline
      \diagbox{\small{$\phi$}}{\small{order}}  & first  & non-first \\ \hline
      linear & forward Euler & linear multi-step, Runge-Kutta... \\ \hline
      nonlinear & DDIM    & PNDM  \\ \hline
   \end{tabular}
   \caption{The relationship between different numerical methods.}
   \label{tb_relation}
   \vspace*{-0.5cm}
\end{wraptable}

Therefore, we use the gradient part of the linear multi-step method and our new transfer part as our main pseudo numerical methods for diffusion models (PNDMs). In Table \ref{tb_relation}, we show the relationship between different numerical methods. Here, we can see PNDMs combine the benefits of higher-order classical numerical methods (in the gradient part) and DDIMs (in the transfer part).

\subsection{Algorithm}
\label{algo_choice}

We can provide our whole algorithm of the denoising process of DDIMs now. According to \citet{Song2020a}, the algorithm of the original method satisfies Algorithm \ref{origin_alg}. And our new algorithm of PNDMs uses the pseudo linear multi-step and pseudo Runge-Kutta method, which satisfies Algorithm \ref{fourth_alg}. Here, we cannot use linear multi-step initially because the linear multi-step method cannot start automatically, which needs at least three previous steps' information to generate results. So we use the Runge-Kutta method to compute the first three steps' results and then use the linear multi-step method to calculate the remaining. 

We also use the gradient parts of two second-order numerical methods to get another pseudo numerical method. We introduce the details of this method in Appendix \ref{som_sec}. We call it S-PNDMs, because its gradient part uses information from two steps at every step. Similarly, we also call our first PNDMs F-PNDMs, which use data from four steps, when we need to distinguish them. 


\subsection{Convergence Order}
\label{sec_corder}


Change the transfer part of numerical methods may introduce unknown error. To determine the influence of our new transfer part theoretically, we compute the local and global error between the theoretical result of Equation (\ref{eq_limit}) $x(t+\delta)$ and our new methods, we find that $x(t+\delta) - x_{\text{DDIM}}(x+\delta) = O(\delta^2)$ and 
\begin{equation}
   x(t+\delta) - x_{\text{S/F-PNDM}}(x+\delta) = O(\delta^3).
\end{equation}
If the target ODE satisfies Lipschitz condition and local error $e_{\text{local}}=O(\delta^k)$, then there are $C$ and $h$ such that the global error $e_{\text{global}}$ satisfies $e_{\text{global}} \leq C\delta^k(1+e^{h}+e^{2h}+\cdots) \leq C'\delta^{k-1}$. And we have that the convergence order is equal to the order of the global error. The detailed proof can be found in Appendix \ref{ap_oapm}. Therefore, we get the following property:
\begin{property}
   S/F-PNDMs have third-order local error and are second-order convergent.
   \label{pro_conorder}
\end{property}











\section{Experiment}
\label{experiment}


\subsection{Setup}

We conduct unconditional image generation experiments on four datasets: Cifar10 ($32\times 32$) \citep{krizhevsky2009}, CelebA ($64\times 64$) \citep{liu2015}, LSUN-church ($256\times 256$) and LSUN-bedroom ($256\times 256$) \citep{yu2016}. According to the analysis in Section \ref{formula_trans}, we can use pre-trained models from prior works in our experiments. The pre-trained models for Cifar10, LSUN-church and LSUN-bedroom are taken from \citet{Ho2020} and the pre-trained model for CelebA is taken from \citet{Song2020a}. In these models, the number of total steps N is 1000 and the variance schedule is linear variance schedule. And we also use a pre-trained model for Cifar10, which uses a cosine variance schedule from improved denoising diffusion probabilistic models (iDDPMs \citep{Nichol2021}).

\subsection{Sample Efficiency and Quality}
\label{sec_qua_eff}

\begin{table}[!t]
   \begin{minipage}[t]{0.73\linewidth}
      \centering
      \small
      \begin{tabular}{l |l |c c >{\columncolor[gray]{0.8}} c c c c | c}
         \toprule[1pt]
         \hline
         dataset & \diagbox{\scriptsize{model}}{\scriptsize{FID}}{\scriptsize{step}} & 10 & 20 & 50 & 100 & 250 & 1000 & time\\
         \hline
         \multirow{2}{*}{Cifar10}   & DDIM   & 13.4 & 6.84 & 4.67 & 4.16 & & 4.04 \\
                                    & PF     & & 13.8 & 3.89 & 3.69 & 3.71 & 3.72 \\
         \hline
         \multirow{4}{*}{\shortstack{Cifar10\\(linear)}}   & DDIM*     & 18.5 & 10.9 & 6.99 & 5.52 & 4.52 & 4.00 & 0.337\\
                                    & FON      & 13.1 & 7.41 & 5.26 & 4.65 & 4.12 & 3.71 & 0.390 \\
                                    & S-PNDM     & 11.6 & 7.56 & 5.18 & 4.34 & 3.91 & 3.80 & 0.344 \\
                                    & F-PNDM    & 7.03 & 5.00 & 3.95 & 3.72 & \textbf{3.60} & 3.70 & 0.391 \\
         \hline
         \multirow{3}{*}{\shortstack{Cifar10\\(cosine)}}    & DDIM     & 14.5 & 8.79 & 5.86 & 4.92 & 4.30 & 3.69 & 0.505\\
                                    & S-PNDM     & 8.64 & 5.77 & 4.46 & 3.94 & 3.71 & 3.38 & 0.517\\
                                    & F-PNDM    & 7.05 & 4.61 & 3.68 & 3.53 & 3.49 & \textbf{3.26} & 0.595\\
         \hline
         \hline
         CelebA                     & DDIM  & 17.3 & 13.7 & 9.17 & 6.53 & & 3.51 \\
         \hline
         \multirow{4}{*}{\shortstack{CelebA\\(linear)}}    & DDIM*     & 16.9 & 13.4 & 8.95 & 6.36 & 4.44 & 3.41 & 1.237\\
                                    & FON         & 16.0 & 11.6 & 8.13 & 6.70 & 5.14 & 4.17 & 1.431\\
                                    & S-PNDM     & 12.2 & 9.45 & 5.69 & 4.03 & 3.19 & 2.99 & 1.258\\
                                    & F-PNDM     & 7.71 & 5.51 & 3.34 & 2.81 & \textbf{2.71} & 2.86 & 1.433\\
         
         \hline
         \bottomrule[1pt]
      \end{tabular}
   \end{minipage}
   \begin{minipage}[t]{0.26\linewidth}
      \vspace*{-3.4cm}
      \caption{\small{Image generation measured in FID on Cifar10 and CelebA. PFs use black box ODE solvers and we use the number of score function evaluations as the step of PFs. DDIM* is a retest of DDIM. The bold results mean the best ones using the same pretrained model. We use the 50-step, 512 batch size experiment on an RTX-3090 to test the computational cost and the column time is the average computational cost per step in seconds. And we put the results of standard deviation in Appendix \ref{sec_standard}}}
      \label{CIFAE-10}
   \end{minipage}
   \vspace*{-0.75cm}
\end{table}

To analyze the acceleration effect, we test Fenchel Inception Distance (FID \citep{heusel2018}) on different datasets under different steps and different numerical methods, including DDIMs, S-PNDMs, F-PNDMs and classical fourth-order numerical methods (FONs) (e.g., Runge-Kutta method and linear multi-step method). On Cifar10 and CelebA, we first provide the results of previous works DDIMs. Then, we use the same pre-trained models to test numerical methods mentioned in this paper and put the results in Cifar10 / CelebA (linear). We also use models from iDDPMs to test nonlinear variance schedules and put the results in Cifar10 (cosine). \citet{Song2020} do not provide detailed FID results of probability flows (PFs) under different steps, so we retest the results using its pretrained models by ourselves. 


\textbf{Efficiency}
Our two baselines are DDIM and PF. DDIM is a simple case of pseudo numerical methods, and PF is a case of classical numerical methods. However, PF uses a much bigger model than DDIM and uses some tricks to improve the sample quality. To ensure the experiment's fairness, we use fourth-order numerical methods on Equation (\ref{eq_limit}) and the model from DDIM. In Table \ref{CIFAE-10}, we find that the performance of FON is limited when the number of steps is small.
By contrast, our new methods, including S-PNDM and F-PNDM, can improve the generated results regardless of whether the number of steps used is large or small. According to Cifar10 / CelebA (linear), F-PNDM can achieve lower FID than 1000 steps DDIM using only 50 steps, making diffusion models 20x faster without losing quality.

\begin{wrapfigure}{R}{0.38\linewidth}
   \vspace*{-1cm}
   \centering
   \includegraphics[width=0.38\textwidth, keepaspectratio, trim=35 10 50 40, clip]{data1/fid1.pdf}
   \caption{The FID results under different computation costs and different numerical methods on Cifar10. The unit of time is the computational cost of 1-step DDIM, which is 0.337s.}
   \label{compute_cost}
   \vspace*{-0.8cm}
\end{wrapfigure}

We draw a line chart of computation cost with FID according to the results of Cifar10 (linear) above in Figure \ref{compute_cost}. Because F-PNDM uses the pseudo Runge-Kutta method to generate the first three steps, it is slower than other methods at the first several steps. Therefore, S-PNDM can achieve the best FID initially, then F-PNDM becomes the best and the acceleration is significant. 


\textbf{Quality}
When the number of steps is relatively big, the results of FON become more and more similar to that of pseudo numerical methods. This is because all the methods are solving Equation (\ref{eq_limit}), and their convergent results should be the same. However, pseudo numerical methods still work better using a large number of steps empirically. F-PNDM can improve the best FID around 0.4 using pre-trained models and achieves a new SOTA FID score of 2.71 on CelebA, which shows that our work can not only accelerate diffusion models but also improve the sample quality topline. We also notice that the FID results of F-PNDM converge after more than 250 steps. The FID results will fluctuate around a value then. This phenomenon is more pronounced when we test our methods on LSUN (see Table \ref{tb-church}, \ref{tb-bedroom}).


According to Cifar10 (cosine), the cosine variance schedule can lower FID using a relatively large number of steps. More analyses about variance schedule can be found in Appendix \ref{sec_variance}. What's more, we test our methods on other datasets and provide our FID results in Appendix \ref{more_fid} and image results in Appendix \ref{gen_results}. We can draw similar conclusions on our methods' acceleration and sampling quality, regardless of the datasets and the size of the images.




\subsection{Sample on Manifolds}
\label{manifold_ex}

\begin{figure}[!t]
   \begin{minipage}[t]{0.3296\linewidth}
      \begin{overpic}[width=\textwidth, keepaspectratio, trim=55 25 145 240, clip]{data1/background.jpg}
         \put(0,0){\includegraphics[width=\textwidth, keepaspectratio, trim=15 5 40 50, clip]{data1/var_test_5.pdf}}
      \end{overpic}
   \end{minipage}
   \begin{minipage}[t]{0.3296\linewidth}
      \begin{overpic}[width=\textwidth, keepaspectratio, trim=55 25 145 240, clip]{data1/background.jpg}
         \put(0,0){\includegraphics[width=\textwidth, keepaspectratio, trim=15 5 40 50, clip]{data1/var_test_10.pdf}}
      \end{overpic}
   \end{minipage}
   \begin{minipage}[t]{0.3296\linewidth}
      \begin{overpic}[width=\textwidth, keepaspectratio, trim=55 25 145 240, clip]{data1/background.jpg}
         \put(0,0){\includegraphics[width=\textwidth, keepaspectratio, trim=15 5 40 50, clip]{data1/var_test_20.pdf}}
      \end{overpic}
   \end{minipage}
   \begin{minipage}[t]{0.3296\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=15 10 40 40, clip]{data1/1017-019-5.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.3296\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=15 10 40 40, clip]{data1/1017-019-10.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.3296\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=15 10 40 40, clip]{data1/1017-019-20.pdf}   
   \end{minipage}
   \caption{The upper part shows the change of norm with the number of steps using different methods and 
   different steps. The lower part shows the generation curves of two points using different methods and 
   different steps. DDIM-n means n-step DDIM method. Experiments in this subsection all use the Cifar10 dataset and we use the 1000-step DDIM's result as our target result.}
   \label{curve}
   \vspace*{-0.5cm}
\end{figure}

Here, we design visualization experiments to show the effort of our new methods and support our analyses. Because it is hard to visualize high-dimensional data, we use the change of a global characteristic norm and a local characteristic pixel to show the change of the data under different steps. For pixel, we randomly choose two positions $p^1, p^2$. Then for a series of images $x_T, x_{T-k}, \cdots, x_0$ derived from the reverse process, we denote $y_t^k$ as the value of $x_t$ at position $p^k$. Then we draw a polyline $(y_t^1, y_t^2)_{t=T,\cdots}$ in $\mathbb{R}^2$. For norm, we first count the distribution of the norm of the training datasets under different steps and use this to make a heat map as the background. After that, we draw the norm of our generated results using different methods and steps above this heat map. 


In Figure \ref{curve}, we can see that the FON may run far away from the high-density area of the data, which explains why FON may introduce noticeable noise. However, PNDM can avoid this problem and appropriately fit the target result. More visualization results supporting our analysis can be found in Appendix \ref{visual_results}. What's more, we design a toy example to test our new methods without the influence of neural networks and get similar conclusions as to the real cases above. We put the detailed results in Appendix \ref{sec_toy}.

\section{Discussion}
\label{sec_disc}

In this paper, we provided PNDMs, a new numerical method suitable for solving the corresponding ODEs of DDPMs. PNDMs can generate high-quality images using fewer steps without loss of quality successfully. Based on the idea of this work, further improvement can be explored in our future works: 1) find a better variance schedule for PNDMs: although we tested PNDMs on linear variance schedule and cosine variance schedule in this work, there might be another variance schedule more suitable for our proposed numerical methods. 2) Find higher-order convergent pseudo numerical methods: we analyzed the convergence order of S/F-PNDMs, which are both second-order convergent. However, F-PNDMs achieve better FID than S-PNDMs in most cases. We think this is because the result between our transfer part and target ODE has a higher-order error, which limits the convergence order of F-PNDMs. This error from the change of the transfer part is theoretical but does not influence the quality of images according to the property of Equation (\ref{transfor}). 
However, making the transfer part higher-order convergent and finding the influence of such change is still exciting and needs more research. 3) Extend PNDMs to more general applications: when we proved the convergence order of PNDMs, we found that other kinds of transfer parts can keep the convergence order unchanged too, which means that pseudo numerical methods can be used on more general applications, like certain neural ODEs \citep{chen2019, dupont2019}. 


\subsubsection*{Acknowledgments}

We would like to thank Qinghai Zhang, Feng Wang, Shuang Hu, Yang Li, Ziyue Jiang and Rongjie Huang from Zhejiang University for their insightful discussions during the course of this paper.


\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}

\appendix
\section{Appendix}

\subsection{Related Work}
\label{gen_inst}

DDPMs have been well developed in the last few years. Some works concentrate on improving the quality and the speed of DDPMs and making DDPMs more practical. \citet{Song2020a} introduce new inference equations to accelerate DDPMs. \citet{Nichol2021}, \citet{Watson2021} and \citet{Kong2021} choose to find better variance schedules to improve the images quality. \citet{Vahdat2021} combine the advantages of DDPMs and Variational Autoencoders and get better results. Furthermore, \citet{Kim2021} try to solve an existing bottleneck that the inference equations of DDPMs are unbounded in some situations.

\citet{Song2020} find the similarity between DDPMs and noise conditional score networks (NCSNs \citep{Song2020c}), which is that they both use a process similar to Langevin dynamics to produce samples. Therefore, some works \citep{Song2020b, Kim2021} that can improve the results of NCSNs also can be used in DDPMs. Additional, \citet{Song2020} combine DDPMs and NCSNs under the framework of neural differential equations \citep{chen2019, dupont2019}. Therefore, numerical methods widely used in neural differential equations can also be applied to accelerate DDPMs. Our work successfully combines the advantages of \citet{Song2020a} and \citet{Song2020}. We use a transfer part from DDIMs and use different gradient parts from different numerical methods. Although we and \citet{Song2020} solve certain differential equations derived from DDPMs, we use different target different equations and different numerical methods, which get better results.

The application of DDPMs is not limited to unconditional image generation. Some works apply DDPMs to various types of data successfully, including text-to-speech \citep{chen2020, Lam2021}, singing voice \citep{liu2021diffsinger}, 3D Point Cloud \citep{Luo2021}, text generation \citep{Austin2021}. Additionally, DDPMs can also be used to generate conditional samples, too \citep{Jeong2021, Choi2021}.


\subsection{Convergent Order of Method}
\label{order_method}

We use the forward Euler method and linear multi-step method to show what is the order of a method. Assume that $x_{t}$ is precise and compute the error at $x_{t+\delta}$. For forward Euler method, we have:
\begin{equation}
   \begin{split}
      e_{t,\delta}   &= x(t+\delta) - x_{t,\delta} \\
                     &= \left(x(t) + \delta f(x(t), t) + \frac{\delta^2}{2}f''(c) \right) - (x(t) + \delta f(x_t, t)) \\
                     &= \frac{\delta^2}{2}f''(c) \leq \frac{\delta^2}{2}M
   \end{split}
   \label{error_one}
\end{equation}
Here, we assume that $f''$ is continuous, so $f''$ is bounded in a close area. $x(t+\delta)$ is the precise result and $x_{t,\delta}$ is the numerical result from $t$ to $t+\delta$.

For linear multi-step method, we have:
\begin{equation}
   \begin{split}
      e_{t,\delta}   =& x(t+\delta) - x_{t,\delta} \\
                     =& \left(x(t) + \frac{\delta}{1 !} f(x(t), t)+\frac{\delta^{2}}{2 !}f'(x(t), t)+\cdots+\frac{\delta^{4}}{4 !} f^{(3)}(x(t), t)+O\left(\delta^{5}\right)  \right) \\
                     & - \left(x(t) + b_1 \delta f(x(t), t) +\sum_{s=2}^4 b_{s} \delta \left(\sum_{k=0}^{4}\frac{(-(s-1) \delta)^{k}}{(k) !} f^{(k-1)}(x(t), t)+O\left(\delta^{5}\right)\right)  \right) \\
                     =& O(\delta^5)
   \end{split}
   \label{error_four}
\end{equation} 
Here, $\{b_s\}$ satisfy $\sum_{s=1}^4 b_s =1$ and the following equations for $j \in \{1, \cdots, 3\}$:
\begin{equation}
   (-1)^{j} b_{2}+(-2)^{j} b_{3}+(-3)^{j} b_{4}=\frac{1}{j+1}.
\end{equation}

We call the error at $x_{t,\delta}$ local error, and the error at $x_{t+M\delta}$ (M is big enough but finite) global error. Assume the local error of our method has order k+1 and the target ODE satisfies Lipschitz condition, then: 
\begin{equation}
   \begin{split}
      x(t+M\delta) - x_{t,M\delta} \leq& |x(t+M\delta) - x_{t+(M-1)\delta, \delta}| + |x_{t+(M-1)\delta, \delta} - x_{t,M\delta}| \\
                                    \leq& e_{t+(M-1)\delta, \delta} + e^{L\delta} |x(t+(M-1)\delta) - x_{t,(M-1)\delta}| \\
                                    \leq& e_{t+(M-1)\delta, \delta} + e^{L\delta} e_{t+(M-2)\delta, \delta} + \cdots \\
                                    \leq& C\delta^{k+1} \left(1+e^{L h}+\cdots+e^{(i-1) L h}\right) \\
                                    =& C h^{k+1} \frac{e^{i L \delta}-1}{e^{L \delta}-1} \leq C \delta^{k+1} \frac{e^{i L \delta}-1}{L\delta} \\
                                    =&O(\delta^k).
   \end{split}
\end{equation}

Therefore, the global error will be one order lower than the local error. From Equation (\ref{error_one}), we can see the forward Euler method has local error $O(\delta^2)$ and global error $O(\delta)$, so we call it the first-order numerical method. And the linear multi-step method has local error $O(\delta^5)$ and global error $O(\delta^4)$, and we call it the fourth-order numerical method.

In addition, assuming that a numerical method has kth-order global error, we can compute the convergence speed of this numerical method:
\begin{equation}
   \lim_{\delta\to 0}\frac{x^{2\delta}_{t+T} - x(t+T)}{x^{\delta}_{t+T} - x(t+T)} = \frac{(2\delta)^k}{\delta^k} = 2^k.
\end{equation}
Here, $x^\delta_{t+T}$ is the result at t+T and move $\delta$ every step. This shows that the fourth-order method can converge to the exact solution faster than the first-order method when $\delta \to 0$, which means that we can use a bigger iteration interval $\delta$ to achieve similar global error and a bigger iteration interval means that we can iterate fewer times to get results with high quality.

\subsection{Pseudo Second-Order Method}
\label{som_sec}
We introduce two second-order numerical methods. First is improved Euler method satisfying:
\begin{equation}
   \begin{cases}
      &k_1 = f(x_t, t) \\
      &k_2 = f(x_t+\delta k_1, t+\delta) \\
      &x_{t+\delta} = x_t + \frac{\delta}{2}(k_1+k_2)
   \end{cases}
\end{equation}

Second is another linear multi-step method called second-order linear multi-step method satisfying:
\begin{equation}
   x_{t+\delta} = x_t + \frac{\delta}{2}(3f_t - f_{t-\delta})
\end{equation}

And the corresponding pseudo improved Euler methods satisfying:
\begin{equation}
   \begin{cases}
      &e^1_t = \epsilon_\theta(x_t, t) \\
      &x^1_t = \phi(x_t, e_t^1, t, t+\delta) \\
      &e^2_t = \epsilon_\theta(x_t^1, t+\delta) \\
      &e'_t = \frac{1}{2}(e^1_t + e^2_t) \\
      &x_{t+\delta} = \phi(x_t, e'_t, t, t+\delta) 
   \end{cases}
   \label{eq_pie}
\end{equation}

Pseudo second-order linear multi-step method satisfying:
\begin{equation}
   \begin{cases}
      &e_t = \epsilon_\theta(x_t, t) \\
      &e'_t = \frac{1}{2}(3e_t - e_{t-\delta}) \\
      &x_{t+\delta} = \phi(x_t, e_t', t, t+\delta)
   \end{cases}
   \label{eq_plms2}
\end{equation}


\begin{figure}[h]
   \begin{minipage}[t]{0.52\linewidth}
      Similar to what we do to get F-PNDMs, We combine them to get S-PNDMs. Abbreviate Equation (\ref{eq_pie}) and (\ref{eq_plms2}) as 
      \begin{equation*}
         \begin{split}
            x_{t+\delta}, e_t &= PIE(x_t, \{e_p\}_{p<t}, t, t+\delta), \\
            x_{t+\delta}, e_t^1 &= PLMS'(x_t, t, t+\delta).
         \end{split}
      \end{equation*}
   \end{minipage}
   \vspace*{-\baselineskip}
   \quad
   \begin{minipage}[t]{0.45\linewidth}
      \vspace*{-0.7cm}
      \begin{algorithm}[H]
         \small
         \caption{S-PNDMs}
         \begin{algorithmic}[1]
            \STATE $x_T \sim \mathcal{N}(0, I)$
            \FOR {$t=T-1$}
               \STATE $x_t, e_t = PIE(x_{t+1}, t+1, t)$ \\
            \ENDFOR
            \FOR {$t=T-2, \cdots, 1, 0$}
               \STATE $x_t, e_t = PLMS'(x_{t+1}, \{e_p\}_{p>t}, t+1, t)$
            \ENDFOR
            \RETURN $x_0$
         \end{algorithmic}
      \end{algorithm}
   \end{minipage}
\end{figure}



\subsection{The Existence of a Derivative}
\label{existence of derivative}

Because $\bar{\alpha}_t$ is usually obtained by multiplying a linear variance schedule $\beta_t$. So we have 
\begin{equation}
   \bar{\alpha}_t = e^{at^2+bt+c},
\end{equation}
and $\bar{\alpha}_0 = 1$, so $c = 0$. Now we have
\begin{equation}
   \begin{split}
       & \lim _{\delta \rightarrow 0} \frac{x_{t-\delta}-x_{t}}{\delta} \\
      =& \lim _{\delta \rightarrow 0} \frac{\bar{\alpha}_{t-\delta}-\bar{\alpha}_t}{\delta} \left(\frac{x_t}{\sqrt{\bar{\alpha}_t}(\sqrt{\bar{\alpha}_{t-\delta}}+\sqrt{\bar{\alpha}_t})} - 
         \frac{\epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}(\sqrt{(1-\bar{\alpha}_{t-\delta})\bar{\alpha}_{t}} + \sqrt{(1-\bar{\alpha}_{t})\bar{\alpha}_{t-\delta}})}\right) \\
      =& \lim _{\delta \rightarrow 0} \frac{\bar{\alpha}_{t-\delta}-\bar{\alpha}_t}{\delta} \left(\frac{x_t}{2\bar{\alpha}_t} - 
         \frac{\epsilon_\theta(x_t, t)}{2\sqrt{1-\bar{\alpha}_{t}}\bar{\alpha}_{t}}\right) = (e^{at^2+bt})' \left(\frac{x_t}{2\bar{\alpha}_t} - \frac{\epsilon_\theta(x_t, t)}{2\sqrt{1-\bar{\alpha}_{t}}\bar{\alpha}_{t}} \right) \\
      =& (2at+b)\bar{\alpha}_t \left(\frac{x_t}{2\bar{\alpha}_t} - \frac{\epsilon_\theta(x_t, t)}{2\sqrt{1-\bar{\alpha}_{t}}\bar{\alpha}_{t}} \right) 
      =  \frac{1}{2}(2at+b)(x_t - \frac{\epsilon_\theta(x_t, t)}{\sqrt{1-\bar{\alpha}_t}}). \\
   \end{split}
\end{equation}
To make $\lim _{\delta \rightarrow 0} \frac{x_{t-\delta}-x_{t}}{\delta}|_{t=0}$ is well-defined, $b$ must equal to zero, or $(2at+b)\frac{\epsilon_\theta(x_t, t)}{\sqrt{1-\bar{\alpha}_t}}$ will tend to infinity. This is a strong condition that most variance schedules do not satisfy. In practice, DDPMs can choose the variance schedule very freely. This means that treating DDPMs as ODEs directly is not proper and has theoretical weakness.

\subsection{Relationship between $t$, $\epsilon_\theta$ and $x_t$}
\label{sec_epsi}

\textbf{Relationship between $t$ and $\epsilon_\theta$}. In Figure \ref{fi_delta}, we can see that the denoising process tends to converge, whether in the $\epsilon_\theta$ domain or the sample/image domain when the step-index tends to zero. Therefore, we can say that the noise becomes more and more precise when step, namely $t$, tends to zero.

\begin{figure}[!htbp]
   \centering
   \includegraphics[width=0.42\textwidth, keepaspectratio, trim=20 5 40 40, clip]{data1/delta.pdf}
   \caption{The norm $\delta$ of the difference between two adjacent terms under different steps}
   \label{fi_delta}
\end{figure}

\textbf{Relationship between $\epsilon_\theta$ and $x_t$} To prove Property \ref{pro_pre}, assume that $x_t= \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$, NN is the neural network and $\epsilon_\theta=\text{NN}(x_t, t)$. Because we assume that the gradient part is precise, then we have $\epsilon_\theta = \epsilon$. Then for all $t'\leq t$, we have:
\begin{equation}
   \begin{split}
      x_{t'} &= \sqrt{\bar{\alpha}_{t'}}\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta}{\sqrt{\bar{\alpha}_t}}\right) + \sqrt{1-\bar{\alpha}_{t'}}\epsilon_\theta \\
             &= \sqrt{\bar{\alpha}_{t'}}\left(\frac{\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}\right) + \sqrt{1-\bar{\alpha}_{t'}}\epsilon_\theta(x_t, t) \\
             &= \sqrt{\bar{\alpha}_{t'}}x_0 + \sqrt{1-\bar{\alpha}_{t'}}\epsilon.
   \end{split}
\end{equation}
Here, we can find that $x_{t'}=\sqrt{\bar{\alpha}_{t'}}x_0 + \sqrt{1-\bar{\alpha}_{t'}}\epsilon$ is also precise, so Property \ref{pro_pre} is true.

\subsection{Order Analysis of Pseudo Method}
\label{ap_oapm}
For the convenience of theoretical analysis, we generalize the problem. Let $\phi(x(t), \epsilon, t, \delta)=f(x(t),t,\delta)+g(t,\delta)\epsilon(x(t),t)$ and we have the property $f(x(t),t,0)=g(t,0)=0$. Then we have:
\begin{equation}
   \begin{split}
       x(1) &= x(0) + \sum_{\delta\to 0}(y(t+\delta) - y(t)) \\
            &= x(0) + \sum_{\delta\to 0}\left(f(x(t),t,\delta)+g(t,\delta)\epsilon(x(t),t)\right) \\
            &= x(0) + \int_0^1\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right).
   \end{split}
\end{equation}
Now, Equation (\ref{eq_limit}) becomes a special case of this more general version and, in this special case, we have:
\begin{equation}
   \begin{split}
      f(x(t),t,\delta)&=\left(\frac{\sqrt{\alpha(t+\delta)}}{\sqrt{\alpha}}-1\right)x(t) \\
      g(t, \delta)&=\sqrt{1-\alpha(t+\delta)} - \frac{\sqrt{(1-\alpha(t))\alpha(t+\delta)}}{\sqrt{\alpha(t)}}
   \end{split}
\end{equation}

Now, we compute the local error of S-PNDMs. We first compute the theoretical and numerical results of different numerical methods. We have:
\begin{equation}
   \begin{split}
           &x(t+\delta) \\
           =& x(t) + \delta\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right) + \\
               & \frac{\delta^2}{2}\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right)' + O(\delta^3) \\
           =& x(t) + \delta\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right) + O(\delta^3) + \\
           & \frac{\delta^2}{2}\left(\frac{\partial^2 f}{\partial \delta\partial t}(x(t),t,0)+\frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right) \right) + \\
           & \frac{\delta^2}{2} \left( \frac{\partial^2 g}{\partial \delta\partial t}(t,0)\epsilon(x(t),t) + \frac{\partial g}{\partial \delta}(t,0)\epsilon'(x(t),t)\right)     
   \end{split}
\end{equation}
and
\begin{equation}
   \begin{split}
        & x_{\text{S-PNDM}}(t+\delta) \\
       =& x(t) + f(x(t),t,\delta) + g(t,\delta)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t)+\phi(...,t,\delta), t+\delta)) \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \left(\delta\frac{\partial g}{\partial \delta}(t,0) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\right)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t+\delta)+O(\delta^2), t+\delta))  \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \left(\delta\frac{\partial g}{\partial \delta}(t,0) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\right)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t+\delta), t+\delta)) \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \left(\delta\frac{\partial g}{\partial \delta}(t,0) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\right)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t), t) + \delta \epsilon(x(t),t)') \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \delta\frac{\partial g}{\partial \delta}(t,0)\left(\epsilon(x(t), t) + \frac{1}{2}\delta \epsilon(x(t),t)'\right) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\epsilon(x(t),t) \\
       =& x(t) + \delta\left(\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t), t)\right) + O(\delta^3) + \\
        & \frac{\delta^2}{2}\left(\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + \frac{\partial g}{\partial \delta}(t,0) \epsilon(x(t),t)' + \frac{\partial^2 g}{\partial \delta^2}(t,0)\epsilon(x(t),t) \right).
   \end{split}
\end{equation}
Then we compute the difference between the theoretical and numerical results. We have:
\begin{equation}
   \begin{split}
        & x(t+\delta) - x_{\text{S-PNDM}}(x+\delta) \\
       =& \frac{\delta^2}{2}\left((\frac{\partial^2 f}{\partial \delta\partial t}-\frac{\partial^2 f}{\partial \delta^2})(x(t),t,0) +\frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial f}{\partial \delta}(x(t),t,0)\right) + \\
       & \frac{\delta^2}{2}\left((\frac{\partial^2 g}{\partial \delta\partial t}-\frac{\partial^2 g}{\partial \delta^2})(t,0) + \frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial g}{\partial \delta}(t,0)\right)\epsilon(x(t),t) + O(\delta^3)
   \end{split}
   \label{eq_diff_order}
\end{equation}
In this special case, we compute the derivatives of some items needed in Equation (\ref{eq_diff_order}). We have:
\begin{equation}
   \begin{split}
       \frac{\partial g}{\partial \delta}(t, \delta) =& \frac{\partial}{\partial \delta}\left(\sqrt{1-\alpha(t+\delta)} - \frac{\sqrt{(1-\alpha(t))\alpha(t+\delta)}}{\sqrt{\alpha(t)}}\right) \\
                                                     =& \frac{-\alpha'(t+\delta)}{2\sqrt{1-\alpha((t+\delta))}} - \frac{\sqrt{1-\alpha(t)}\alpha'(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}}
   \end{split}
\end{equation}
\begin{equation}
   \begin{split}
       \frac{\partial f}{\partial \delta}(x(t), t, \delta) =& \frac{\partial}{\partial \delta}\left(\left(\frac{\sqrt{\alpha(t+\delta)}}{\sqrt{\alpha}}-1\right)x(t)\right) \\
                                                           =& \frac{\alpha'(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}}x(t)
   \end{split}
\end{equation}
\begin{equation}
   \begin{split}
       \frac{\partial^2 f}{\partial \delta^2}(x(t),t,\delta)|_{\delta=0} =&\frac{\alpha''(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}}x(t) + \frac{-\alpha'(t+\delta)^2}{4\sqrt{\alpha(t)\alpha(t+\delta)^3}}x(t)|_{\delta=0} 
   \end{split}
\end{equation}
\begin{equation}
   \begin{split}
       \frac{\partial^2 f}{\partial \delta\partial t}(x(t),t,0) =& \frac{\alpha''(t)}{2\alpha(t)}x(t)-\frac{\alpha'(t)^2}{2\alpha(t)^2}x(t)
   \end{split}
\end{equation}
\begin{equation}
   \begin{split}
       \frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0) =& \frac{\alpha'(t)}{2\alpha(t)}
   \end{split}
\end{equation}
\begin{equation}
   \begin{split}
       \frac{\partial^2 g}{\partial \delta^2}(t, \delta)|_{\delta=0} =& \frac{-\alpha''(t+\delta)}{2\sqrt{1-\alpha((t+\delta))}} - \frac{\sqrt{1-\alpha(t)}\alpha''(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}} + \\
                                           & \frac{-\alpha'(t+\delta)^2}{4\sqrt{1-\alpha(t+\delta)}^3} + \frac{\sqrt{1-\alpha(t)}\alpha'(t+\delta)^2}{4\sqrt{\alpha(t)\alpha(t+\delta)^3}}|_{\delta=0} \\
                                           =& \frac{-\alpha''(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha''(t)}{2\sqrt{\alpha(t)\alpha(t)}} + \\
                                           & \frac{-\alpha'(t)^2}{4\sqrt{1-\alpha(t)}^3} + \frac{\sqrt{1-\alpha(t)}\alpha'(t)^2}{4\sqrt{\alpha(t)\alpha(t)^3}} \\
   \end{split}
\end{equation}
\begin{equation}
   \begin{split}
       \frac{\partial^2 g}{\partial \delta\partial t}(t, \delta)|_{\delta=0} =& \frac{\partial}{\partial t}\left(\frac{-\alpha'(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha'(t)}{2\sqrt{\alpha(t)\alpha(t)}}\right) \\
                                           =& \frac{-\alpha''(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha''(t)}{2\sqrt{\alpha(t)\alpha(t)}} + \\
                                           & \frac{-\alpha'(t)^2}{4\sqrt{1-\alpha(t)}^3} + \frac{\sqrt{1-\alpha(t)}\alpha'(t)^2}{2\alpha(t)^2} + \frac{\alpha'(t)^2}{4\sqrt{\alpha^2(t)(1-\alpha(t))}}  
   \end{split}
\end{equation}
Now we can compute the final result of Equation (\ref{eq_diff_order}). We split it into three parts and the values of the first two terms. We have:
\begin{equation}
   \begin{split}
        &(\frac{\partial^2 f}{\partial \delta\partial t}-\frac{\partial^2 f}{\partial \delta^2})(x(t),t,0) +\frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial f}{\partial \delta}(x(t),t,0) \\
       =& \left(\frac{\alpha''(t)}{2\alpha(t)}x(t)-\frac{\alpha'(t)^2}{2\alpha(t)^2}x(t)\right) - \left(\frac{\alpha''(t)}{2\alpha(t)}x(t) + \frac{-\alpha'(t)^2}{4\alpha(t)^2}x(t) \right) + \frac{1}{2\alpha(t)}\frac{\alpha'(t)}{2\alpha(t)}x(t) \\
       =& 0
   \end{split}
\end{equation}
and
\begin{equation}
   \begin{split}
        &(\frac{\partial^2 g}{\partial \delta\partial t}-\frac{\partial^2 g}{\partial \delta^2})(t,0) + \frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial g}{\partial \delta}(t,0) \\
       =&\frac{\sqrt{1-\alpha(t)}\alpha'(t)^2}{4\alpha(t)^2} + \frac{\alpha'(t)^2}{4\alpha(t)\sqrt{1-\alpha(t)}} + \frac{\alpha'(t)}{2\alpha(t)}\left(\frac{-\alpha'(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha'(t)}{2\alpha(t)}\right)\\
       =& \frac{\alpha'(t)^2}{4\alpha(t)^2\sqrt{1-\alpha(t)}} + \frac{\alpha'(t)}{2\alpha(t)}\left(\frac{-\alpha'(t)}{2\sqrt{1-\alpha(t)}\alpha(t)}\right) \\
       =&0
   \end{split}
\end{equation}
Finally, we get the final result of Equation (\ref{eq_diff_order}):
\begin{equation}
   x(t+\delta) - x_{\text{S-PNDM}}(x+\delta) = O(\delta^3)
\end{equation}
And the computation of the convergence order of F-PNDMs is similar, and we ignore it here. Therefore, Property \ref{pro_conorder} is true.


\subsection{Variance schedule}
\label{sec_variance}
According to Cifar10 (cosine) in Table \ref{CIFAE-10}, PNDMs can be used on both linear variance schedule and cosine variance schedule. However, we also notice cosine variance schedule can make FID lower when we use relatively big generation steps, but the effort is limited when the number of steps is small. F-PNDM uses information from four consecutive steps, so the smoothness of the schedule is more important for F-PNDM than DDIM. According to this experiment, our work can be used with works that pay attention to variance schedules to improve the acceleration effect further. However, a variance schedule that fits pseudo numerical methods better remains to be found in further work.

\subsection{Toy example}
\label{sec_toy}

Here, we design a toy example to test our new methods without the influence of neural networks. We randomly generate the initial input $x_1 = (m_1, m_2),\ m_i\sim U(0, 1)$ and use a simple analytic equations $\epsilon_\theta(x)=(\sin x[0], \cos x[1])$ to replace the neural networks in real cases. Let $\phi$ in Equation (\ref{transfor}) is unchanged and $\bar{\alpha}_t=\alpha(t)=1-t$, then we get:

\begin{equation}
   \begin{split}
      &\phi(x_t, \epsilon_\theta(x_t), t, t-\delta) \\
      = &\frac{\sqrt{\bar{\alpha}_{t-\delta}}}{\sqrt{\bar{\alpha}_t}}x_t - 
      \frac{(\bar{\alpha}_{t-\delta}-\bar{\alpha}_t)}{\sqrt{\bar{\alpha}_t}(\sqrt{(1-\bar{\alpha}_{t-\delta})\bar{\alpha}_{t}} + \sqrt{(1-\bar{\alpha}_{t})\bar{\alpha}_{t-\delta}})}\epsilon_t \\
      = &\frac{\sqrt{1-(t-\delta)}}{\sqrt{1-t}}x_t - 
       \frac{\delta}{\sqrt{1-t}\left(\sqrt{(t-\delta)(1-t)} + \sqrt{t(1-(t-\delta))}\right)}\epsilon_\theta(x_t)
   \end{split}
\end{equation}
Here, we use three different numerical methods to generate $x_0$. 

For DDIM, we have:
\begin{equation}
      x_{t-\delta}=x_t + \phi(x_t, \epsilon_\theta(x_t), t, t-\delta)
\end{equation}
For FON, according to Equation (\ref{eq_limit}), we have:
\begin{equation}
   \begin{split}
      e_t' & = \bar{\alpha}'(t)\left(\frac{x_t}{2\bar{\alpha}(t)}-\frac{\epsilon_\theta(x_t)}{2\bar{\alpha}(t)\sqrt{1-\bar{\alpha}(t)}}\right) \\
           & = -\left(\frac{x_t}{2(1-t)}-\frac{\epsilon_\theta(x_t)}{2(1-t)\sqrt{t}}\right) \\
      x_{t-\delta} & = x_t + \frac{\delta}{24}(55e_t'-59e_{t+\delta}'+37e_{t+2\delta}'-9e_{t+3\delta}')
   \end{split}
\end{equation}
For F-PNDM, we have:
\begin{equation}
   \begin{split}
      e' = \frac{1}{24}(55\epsilon_\theta(x_t)-59\epsilon_\theta(x_{t+\delta})&+37\epsilon_\theta(x_{t+2\delta})-9\epsilon_\theta(x_{t+3\delta})) \\
      x_{t-\delta} = x_t + \phi(x_t&, e', t, t-\delta)
   \end{split}
\end{equation}
Then we draw the corresponding generation curves in Figure \ref{fi_toy}. We find that the result is similar to the real cases. The main difference here is that FON can correct its results while the real case cannot. The reason is that the gradient is well-defined everywhere, while in real cases, the gradient is meaningful on the high-density region of the data $x_t$ of DDPMs.

\begin{figure}[!htbp]
   \centering
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/toy-5.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/toy-10.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/toy-20.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/toy-5.1.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/toy-10.1.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/toy-20.1.pdf}   
   \end{minipage}
   \caption{The generation curve of our toy example.}
   \label{fi_toy}
\end{figure}


\subsection{More FID Results}
\label{more_fid}

Here, We provide our more detailed FID results on Cifar10, CelebA, LSUN-church and LSUN-bedroom in Table \ref{tb_cifar_f}, \ref{tb_celeba_f}, \ref{tb-church}, \ref{tb-bedroom}.

\begin{table}[htbp]
   \begin{center}
      \begin{tabular}{l|c c c c c c c c c c c c}
         \toprule
         steps      & 5 & 10 & 20 & 25 & 40 & 50 & 100 & 125 & 200 & 250 & 500 & 1000 \\
         \midrule
         DDIM       &      & 13.4 &  & 6.84 &  & 4.67 & 4.16 & & & & & 4.04 \\
         DDIM*      & 44.5 & 18.5 & 10.9 & 9.61 & 7.65 & 6.99 & 5.52 & 5.19 & 4.69 & 4.52 & 4.17 & 4.00 \\
         FON         & 98.0 & 13.1 & 7.41 & 6.41 & 5.50 & 5.26 & 4.65 & 4.54 & 4.23 & 4.12 & 3.84 & 3.71 \\
         S-PNDM     & 22.8 & 11.6 & 7.56 & 6.79 & 5.57 & 5.18 & 4.34 & 4.18 & 3.97 & 3.91 & 3.81 & 3.80 \\
         F-PNDM     & 13.9 & 7.03 & 5.00 & 4.76 & 4.10 & 3.95 & 3.72 & 3.64 & 3.60 & \textbf{3.60} & 3.64 & 3.70 \\
         \midrule
         DDIM*      & 28.7 & 14.5 & 8.79 & 7.83 & 6.41 & 5.86 & 4.92 & 4.75 & 4.42 & 4.30 & 3.98 & 3.69 \\
         S-PNDM     & 18.3 & 8.64 & 5.77 & 5.45 & 4.76 & 4.46 & 3.94 & 3.85 & 3.69 & 3.71 & 3.60 & 3.38 \\
         F-PNDM     & 18.2 & 7.05 & 4.61 & 4.32 & 3.85 & 3.68 & 3.53 & 3.46 & 3.47 & 3.49 & 3.44 & \textbf{3.26} \\
         \bottomrule
      \end{tabular}
   \end{center}
   \caption{Cifar10 image generation measured in FID. The upper part uses linear variance schedule and the bottom half uses cosine variance schedule. The first line shows the FID provided by \citet{Song2020a}.}
   \label{tb_cifar_f}
\end{table}

\begin{table}[htbp]
   \begin{center}
      \begin{tabular}{l|c c c c c c c c c c c c}
         \toprule
         steps  & 5 & 10 & 20 & 25 & 40 & 50 & 100 & 125 & 200 & 250 & 500 & 1000 \\
         \midrule
         DDIM   &      & 17.3 &  & 13.7 &  & 9.17 & 6.53 & & & & & 3.51 \\
         DDIM*   & 24.4 & 16.9 & 13.4 & 12.3 & 9.99 & 8.95 & 6.36 & 5.74 & 4.78 & 4.44 & 3.75 & 3.41 \\
         FON     & 60.2 & 16.0 & 11.6 & 10.6 & 8.89 & 8.13 & 6.70 & 6.28 & 5.45 & 5.14 & 4.49 & 4.17 \\
         S-PNDM    & 15.2 & 12.2 & 9.45 & 8.42 & 6.50 & 5.69 & 4.03 & 3.72 & 3.30 & 3.19 & 3.01 & 2.99 \\
         F-PNDM    & 11.3 & 7.71 & 5.51 & 4.75 & 3.67 & 3.34 & 2.81 & 2.75 & \textbf{2.71} & 2.71 & 2.77 & 2.86 \\
         \bottomrule
      \end{tabular}
   \end{center}
   \caption{CelebA image generation measured in FID. All of them use linear variance schedule.}
   \label{tb_celeba_f}
\end{table}


\begin{table}[htbp]
   \begin{center}
      \begin{tabular}{l|c c c c c c c c c c c c}
         \toprule
         steps    & 5 & 10 & 20 & 25 & 40 & 50 & 100 & 125 & 200 & 250 \\
         \midrule
         DDIM     & & 19.5 & 12.5 & & & 10.8 & 10.6\\
         DDIM*    & 48.8 & 18.8 & 11.7 & 11.0 & 10.1 & 10.0 & 9.84 & 9.83 & 9.85 & 9.88 \\
         S-PNDM   & 20.5 & 11.8 & 9.20 & 9.13 & 9.31 & 9.49 & 9.82 & 9.88 & 10.0 & 10.0 \\
         F-PNDM   & 14.8 & \textbf{8.69} & 9.13 & 9.33 & 9.69 & 9.89 & 10.1 & 9.99 & 10.1 & 10.1 \\
         \bottomrule
      \end{tabular}
   \end{center}
   \caption{LSUN-church image generation measured in FID. All of them use linear variance schedule.}
   \label{tb-church}
\end{table}

\begin{table}[htbp]
   \begin{center}
      \begin{tabular}{l|c c c c c c c c c c c c}
         \toprule
         steps  & 5 & 10 & 20 & 25 & 40 & 50 & 100 & 125 & 200 & 250 \\
         \midrule
         DDIM      & & 17.0 & 8.89 & & & 6.75 & 6.62\\
         DDIM*     & 51.3 & 16.4 & 8.47 & 7.41 & 6.27 & 6.05 & 5.97 & 6.03 & 6.23 & 6.32 \\
         S-PNDM    & 18.1 & 10.2 & 6.50 & 6.02 & 5.74 & 5.81 & 6.29 & 6.44 & 6.69 & 6.75 \\
         F-PNDM    & 12.6 & 6.99 & \textbf{5.68} & 5.74 & 6.17 & 6.44 & 6.91 & 6.96 & 7.03 & 6.92\\
         \bottomrule
      \end{tabular}
   \end{center}
   \caption{LSUN-bedroom image generation measured in FID. All of them use linear variance schedule.}
   \label{tb-bedroom}
\end{table} 


\subsection{More Image Results}
\label{gen_results}

Here, we show more generated images on Cifar10, CelebA, LSUN-church and LSUN-bedroom in Figure \ref{fi_cifar_conti}, \ref{fi_cifar_f}, \ref{fi_celeba_conti}, \ref{fi_celeba_f}, \ref{fi_church_conti}, \ref{fi_bedroom_conti}.

\begin{figure}[!htbp]
   \centering
   \includegraphics[width=0.75\linewidth, trim=10 5 10 10]{data1/cifar_subcap.png}
   \caption{5, 10, 20, 50, 100, 250, 500-steps generated results using DDIMs, classical numerical methods and PNDMs on Cifar10.}
   \label{fi_cifar_conti}
\end{figure}

\begin{figure}[!htbp]
   \centering
   \includegraphics[width=0.75\linewidth, trim=10 5 10 10]{data1/celeba_subcap.png}
   \caption{5, 10, 20, 50, 100, 250, 500-steps generated results using DDIMs, classical numerical methods and PNDMs on CelebA.}
   \label{fi_celeba_conti}
\end{figure}


\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.7\linewidth]{data1/cifar_b.png}
   \caption{Generated images of PNDMs on Cifar10.}
   \label{fi_cifar_f}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.7\linewidth]{data1/celeba_b.png}
   \caption{Generated images of PNDMs on CelebA.}
   \label{fi_celeba_f}
\end{figure}


\begin{figure}[!htbp]
   \centering
   \begin{minipage}[t]{0.9\linewidth}
      \centering
      \includegraphics[width=\linewidth]{data1/church_2sub.png}  
   \end{minipage}
   \begin{minipage}[t]{0.9\linewidth}
      \centering
      \includegraphics[width=\linewidth]{data1/church_4sub.png}
   \end{minipage}
   \begin{minipage}[t]{0.9\linewidth}
      \centering
      \includegraphics[width=\linewidth]{data1/church_5sub.png}
   \end{minipage}
   \caption{5, 10, 20, 50, 100-steps generated results using DDIMs, classical numerical methods and PNDMs on LSUN-church.}
   \label{fi_church_conti}
\end{figure}

\begin{figure}[!htbp]
   \centering
   \begin{minipage}[t]{0.9\linewidth}
      \centering
      \includegraphics[width=\linewidth]{data1/bedroom_1sub.png}  
   \end{minipage}
   \begin{minipage}[t]{0.9\linewidth}
      \centering
      \includegraphics[width=\linewidth]{data1/bedroom_2sub.png}
   \end{minipage}
   \begin{minipage}[t]{0.9\linewidth}
      \centering
      \includegraphics[width=\linewidth]{data1/bedroom_14sub.png}
   \end{minipage}
   \caption{5, 10, 20, 50, 100-steps generated results using DDIMs, classical numerical methods and PNDMs on  LSUN-bedroom.}
   \label{fi_bedroom_conti}
\end{figure}

\clearpage

\subsection{More Visualization Results}
\label{visual_results}

Here, we put more visualization results similar to Figure \ref{curve} in Figure \ref{fi_visualization}.

\begin{figure}[!htbp]
   \centering
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/2311-2431-5.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/2311-2431-10.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/2311-2431-20.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/2311-2431-25.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/2311-2431-40.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/2311-2431-50.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/318-27-5.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/318-27-10.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/318-27-20.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/318-27-25.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/318-27-40.pdf}   
   \end{minipage}
   \begin{minipage}[t]{0.329\linewidth}
      \centering
      \includegraphics[width=\textwidth, keepaspectratio, trim=50 30 40 40, clip]{data1/318-27-50.pdf}   
   \end{minipage}
   \caption{Visualization results under 5, 10, 20, 25, 40 and 50 steps.}
   \label{fi_visualization}
\end{figure}

\subsection{FID result with Standard Deviation}
\label{sec_standard}

Here, we report the mean and standard deviation of FID results, tested over four sampling runs.

\begin{table}[!h]
   \centering
   \small
   \begin{tabular}{l |l |c c >{\columncolor[gray]{0.8}} c c c c }
      \toprule[1pt]
      dataset & \diagbox{\scriptsize{model}}{\scriptsize{FID}}{\scriptsize{step}} & 10 & 20 & 50 & 100 & 250 & 1000 \\
      \hline
      \multirow{4}{*}{\shortstack{Cifar10\\(linear)}}   
            & DDIM*     & 18.50$\pm$.06 & 10.86$\pm$.08 & 6.95$\pm$.04 & 5.49$\pm$.06 & 4.52$\pm$.02 & 4.02$\pm$.04 \\
            & FON       & 13.00$\pm$.11 & 7.33$\pm$.06 & 5.24$\pm$.05 & 4.64$\pm$.04 & 4.12$\pm$.03 & 3.73$\pm$.03 \\
            & S-PNDM    & 11.58$\pm$.10 & 7.53$\pm$.07 & 5.15$\pm$.05 & 4.34$\pm$.03 & 3.93$\pm$.02 & 3.83$\pm$.03 \\
            & F-PNDM    & 6.12$\pm$.07 & 5.04$\pm$.07 & 4.01$\pm$.02 & 3.75$\pm$.04 & \textbf{3.67$\pm$.03} & 3.78$\pm$.04 \\
      \bottomrule[1pt]
   \end{tabular}
   \caption{Image generation measured in FID on Cifar10. DDIM* means a kind of pseudo numerical method and also a retest of DDIM.}
\end{table}


\end{document}