\documentclass{article}
\usepackage{iclr2022_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[abs]{overpic}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{diagbox}
\usepackage{newfloat}
\usepackage{colortbl}
\usepackage{tabu}
\usepackage{color}
\usepackage{enumitem}

\newtheorem{property}{Property}[section]

\title{Pseudo Numerical Methods for Diffusion Models on Manifolds}



\author{Luping Liu, Yi Ren, Zhijie Lin \& Zhou Zhao\thanks{Corresponding author} \\
Zhejiang University \\
\texttt{\{luping.liu,rayeren,linzhijie,zhaozhou\}@zju.edu.cn} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle

\begin{abstract}
   Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce final samples. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations.
   According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules.\footnote{Our implementation is available at \url{https://github.com/luping-liu/PNDM}.}
\end{abstract}

\section{Introduction}

Denoising Diffusion Probabilistic Models (DDPMs) \citep{sohl-dickstein2015, Ho2020} is a class of generative models which model the data distribution through an iterative denoising process reversing a multi-step noising process. DDPMs have been applied successfully to a variety of applications, including image generation \citep{Ho2020,Song2020}, text generation \citep{Hoogeboom2021, Austin2021}, 3D point cloud generation \citep{Luo2021}, text-to-speech \citep{kong2021a, chen2020} and image super-resolution \citep{saharia2021}.

Unlike Generative Adversarial Networks (GANs) \citep{goodfellow2014}, which require careful hyperparameter tuning according to different model structures and datasets, DDPMs can use similar model structures and be trained by a simple denoising objective which makes the models fit the noise in the data. To generate samples, the iterative denoising process starts from white noise and progressively denoises it into the target domain according to the noise predicted by the model at every step. However, a critical drawback of DDPMs is that DDPMs require hundreds to thousands of iterations to produce high-quality samples and need to pass through a network at least once at every step, which makes the generation of a large number of samples extremely slow and infeasible. In contrast, GANs only need one pass through a network. 

There have been many recent works focusing on improving the speed of the denoising process. Some works search for better variance schedules, including \citet{Nichol2021} and \citet{Watson2021}. Some works focus on changing the inference equation, including \citet{Song2020a} and \citet{Song2020}. Denoising Diffusion Implicit Models (DDIMs) \citep{Song2020a} relying on a non-Markovian process accelerate the denoising process by taking multiple steps every iteration. Probability Flows (PFs) \citep{Song2020} build a connection between the denoising process and solving ordinary differential equations and use numerical methods of differential equations to accelerate the denoising process. Additionally, we introduce more related works in Appendix \ref{gen_inst}.

\begin{wrapfigure}{R}{0.6\linewidth}
   \vspace*{-0.5cm}
   \centering
   \includegraphics[width=\linewidth]{data1/celeba_t.png}
   \caption{5, 10, 20, 50 and 100-steps generated results using DDIMs, classical numerical methods and PNDMs.}
   \vspace*{-0.5cm}
\end{wrapfigure}

However, this direct connection between DDPMs and numerical methods (e.g., forward Euler method, linear multi-step method and Runge-Kutta method \citep{Sauer2017}) has weaknesses in both speed and effect (see Section \ref{formula_trans}). Some numerical methods are straightforward, like the forward Euler method, but they can only trade quality for speed. Some numerical methods can accelerate the reverse process without loss of quality, like the Runge-Kutta method, but they need to propagate forward more times along a neural network at every step. Furthermore, we also notice that numerical methods can introduce noticeable noise at a high speedup rate, which makes high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs. This phenomenon is also mentioned in \citet{salimans2022progressive}.

To figure out the reason for the performance degradation in classical numerical methods, we conduct some analyses and find that classical numerical methods may sample data far away from the main distribution area of the data, and the inference equations of DDPMs do not satisfy a necessary condition of numerical methods at the last several steps (see Section \ref{sec_pro_cnm}).

To tackle these problems, we design new numerical methods called pseudo numerical methods for diffusion models (PNDMs) to generate samples along a specific manifold in , which is the high-density region of the data. We first compute the corresponding differential equations of diffusion models directly and self-consistently, which builds a theoretical connection between DDPMs and numerical methods. Considering that classical numerical methods cannot guarantee to generate samples on certain manifolds, we provide brand-new numerical methods called pseudo numerical methods based on our theoretical analyses. We also find that DDIMs are simple cases of pseudo numerical methods, which means that we also provide a new way to understand DDIMs better. Furthermore, we find that the pseudo linear multi-step method is the fastest method for diffusion models under similar generated quality.

Besides, we provide a detailed theoretical analysis of our new theory and give visualization results to support our theory intuitively. According to our experiments, our methods have several advantages:

\begin{itemize}[leftmargin=*]
   \item Our methods combine the benefits of DDIMs and high-order numerical methods successfully. We theoretically prove that our new methods PNDMs are second-order convergent while DDIMs are first-order convergent, which makes PNDMs 20x faster without loss of quality on Cifar10 and CelebA.
   \item Our methods can reduce the best FID of pre-trained models with even shorter sampling time. With only 250 steps, our new denoising process can reduce the best FID by around 0.4 points Cifar10 and CelebA. We achieve a new SOTA FID score of 2.71 on CelebA.
   \item Our methods work well with different variance schedules, which means that our methods have a good generalization and can be used together with those works introducing better variance schedules to accelerate the denoising process further.
\end{itemize}


\section{Background}
\label{background}

In this section, we introduce some backgrounds. Firstly, we present the classical understanding of DDPMs. Then we provide another understanding based on \citet{Song2020}, which inspires us to use numerical methods to accelerate the denoising process of diffusion models. After that, we introduce some background on numerical methods used later in this paper.

\subsection{Denoising Diffusion Probabilistic Models}

DDPMs model the data distribution from Gaussian distribution to image distribution through an iterative denoising process. Let  be an image, then the diffusion process is a Markov process and the reverse process has a similar form to the diffusion process, which satisfies:

Here,  controls the speed of adding noise to the data, calling them the variance schedule.  is the total number of steps of the denoising process.  and  are two neural networks, and  are their parameters.


\citet{Ho2020} get some statistics estimations of  and . According to the properties of the conditional Gaussian distribution, we have:

Here, , ,  and .  Then this paper sets  and designs a objective function to help neural networks to represent .

\textbf{Objective Function} The objective function is defined by: 

Here, ,  is an estimate of the noise . The relationship between  and  is . Because , we assume that the mean and variance of  are 0 and 1.



\subsection{Stochastic Differential Equation}
According to \citet{Song2020}, there is another understanding of DDPMs. The diffusion process can be treated as solving a certain stochastic differential equation . According to \citet{Anderson1982}, the denoising process also satisfies a similar stochastic differential equation:

This is Variance Preserving stochastic differential equations (VP-SDEs). Here, we change the domain of  from  to . When  tends to infinity, ,  become continuous functions  and  on . \citet{Song2020} also show that this equation has an ordinary differential equation (ODE) version with the same marginal probability density as Equation (\ref{eq_re_sde}):

This different denoising equation with no random item and the same diffusion equation together is Probability Flows (PFs). These two denoising equations show us a new possibility that we can use numerical methods to accelerate the reverse process. As far as we know, DDIMs first try to remove this random item, so PFs can also be treated as an acceleration of DDIMs, while VP-SDEs are an acceleration of DDPMs.

\subsection{Numerical Method}
\label{sec_num_method}

Many classical numerical methods can be used to solve ODEs, including the forward Euler method, Runge-Kutta method and linear multi-step method \citep{Sauer2017}.

\textbf{Forward Euler Method} For a certain differential equation satisfying . The trivial numerical method is forward Euler method satisfying . 

\textbf{Runge-Kutta Method} Runge-Kutta method uses more information at every step, so it can achieve higher accuracy \footnote{To achieve higher accuracy, more information is not enough. The reason why these methods achieve higher accuracy can be found in Appendix \ref{order_method}}.
Runge-Kutta method satisfies:


\textbf{Linear Multi-Step Method} Linear multi-step method is another numerical method and satisfies:



\section{Pseudo Numerical Method for DDPM}
\label{fast_method}

In this section, we first compute the corresponding differential equations of diffusion models to build a direct connection between DDPMs and numerical methods. As a byproduct, we can directly use pre-trained models from DDPMs. After establishing this connection, we provide detailed analyses on the weakness of classical numerical methods. To solve the problems in classical numerical methods, we dive into the structure of numerical methods by dividing their equations into a gradient part and a transfer part and define pseudo numerical methods by introducing nonlinear transfer parts. We find that DDIMs can be regarded as simple pseudo numerical methods. Then, We explore the pros and cons of different numerical methods and choose the linear multi-step method to make numerical methods faster. Finally, we summarize our findings and analyses and safely propose our novel pseudo numerical methods for diffusion models (PNDMs), which combine our proposed transfer part and the gradient part of the linear multi-step method. Furthermore, we analyze the convergence order of pseudo numerical methods to demonstrate the effectiveness of our methods theoretically.



\subsection{Formula Transformation}
\label{formula_trans}
According to \citet{Song2020a}, the reverse process of DDPMs and DDIMs satisfies:

Here,  controls the ratio of random noise. If  equals one, Equation (\ref{ddpm_raw}) represents the reverse process of DDPMs; if  equals zero, this equation represents the reverse process of DDIMs. And only when  equals zero, this equation removes the random item and becomes a discrete form of a certain ODE. Theoretically, the numerical methods that can be used on differential equations with random items are limited. And \citet{Song2020} have done enough research in this case. Empirically, \citet{Song2020a} have shown that DDIMs have a better acceleration effect when the number of total steps is relatively small. Therefore, our work concentrate on the case  equals zero.

To find the corresponding ODE of Equation (\ref{ddpm_raw}), we replace discrete  with a continuous version  according to \citep{Song2020a} and change this equation into a differential form, namely, subtract  from both sides of this equation:

Because  is a continuous variable from  to , we can now compute the derivative of the generation data  and get that . Here,  is the continuous version of  like the definition of . Therefore, the corresponding ODE when  tends to zero of Equation (\ref{ddim_diff}) is:






\subsection{Classical Numerical Method}
\label{sec_pro_cnm}

After getting the target ODE, the easiest way to solve it is through classical numerical methods. However, We notice that classical numerical methods can introduce noticeable noise at a high speedup rate, making high-order numerical methods (e.g., Runge-Kutta method) even less effective than DDIMs. This phenomenon is also mentioned in \citet{salimans2022progressive}. To make better use of numerical methods, we analyze the differences between Equation (\ref{eq_limit}) and usual differential equations and find two main problems when we directly use numerical methods with diffusion models.

\begin{wrapfigure}{R}{0.38\linewidth}
   \vspace*{-0.45cm}
   \centering
   \begin{overpic}[width=0.38\textwidth, keepaspectratio, trim=90 20 145 150, clip]{data1/norm_dist_1.jpg}
      \put(0,0){\includegraphics[width=0.38\textwidth, keepaspectratio, trim=25 5 40 50, clip]{data1/norm_dist.pdf}}
   \end{overpic}
   \caption{the density distribution of the norm of the data.}
   \label{data_dist}
   \vspace*{-0.6cm}
\end{wrapfigure}

The first problem is that the neural network  and Equation (\ref{eq_limit}) are well-defined only in a limited area. Equation (\ref{qt}) shows that the data  is generated along a curve close to an arc. According to Figure \ref{data_dist}, most of  is concentrated in a band with a width of around 0.1, namely the red area in Figure \ref{data_dist}. This means that the neural network  cannot get enough examples to fit the noise successfully away from this area. Therefore,  and Equation (\ref{eq_limit}), which contains , are only well-defined in this limited area. However, all classical numerical methods generate results along a straight line instead of an arc. The generation process may generate samples away from the well-defined area and then introduce new errors. In Section \ref{manifold_ex} we will give more visualization results to support this.

The second problem is that Equation (\ref{eq_limit}) is unbounded at most cases. We find that for most linear variance schedules , Equation (\ref{eq_limit}) tends to infinity when  tends to zero (see Appendix \ref{existence of derivative}), which does not satisfy the condition of numerical methods mentioned in Section \ref{sec_num_method}. This is an apparent theoretical weakness that previous works have not explored. On the contrary, in the original DDPMs and DDIMs, the prediction of the sample  and the noise  in the data are more and more precise as the index  tends to zero (see Appendix \ref{sec_epsi}). This means original diffusion models do not make a significant error in the last several steps, whereas using numerical methods on Equation (\ref{eq_limit}) does. This explains why DDIMs are better than higher-order numerical methods.




\subsection{Pseudo Numerical Method on Manifold}
\label{sec_pseudo}

The first problem above shows that we should try to solve our problems on certain manifolds. Here, target manifolds are the high-density region of the data  of DDPMs, which is defined by , .  \citet{Hairer1996} show several numerical methods to solve differential equations on manifolds that have analytic expressions. Unfortunately, it's challenging to use the above expression of manifolds. Because we do not know the target  in the reverse process and random items  are hard to handle, too. 

In this paper, we design a different way that we make our new equation of denoising process more fits with the equation of original DDIMs to make their results share similar data distribution. Firstly, we divide the classical numerical methods into two parts: gradient and transfer parts. The gradient part determines the gradient at each step, while the transfer part generates the result at the next step. For example, linear multi-step method can be divided into the gradient part  and the transfer part . All classical numerical methods have the same linear transfer part, while gradient parts are different.

We define those numerical methods which use a nonlinear transfer part as pseudo numerical methods. And an expected transfer part should have the property that when the result from the gradient part is precise, then the result of the transfer part is as close to the manifold as possible and the error of this result is as small as possible. We find that Equation (\ref{ddim_diff}) satisfies such property.
\begin{property}
   If  is the precise noise in , then the result of  from Equation (\ref{ddim_diff}) is also precise.
   \label{pro_pre}
   \vspace*{-0.4cm}
\end{property}
And we put the proof of this property in Appendix \ref{sec_epsi}. Therefore, we use: 



as the transfer part and  as the gradient part. That if  is precise, the result of  is also precise, which means that  can determine the direction of the denoising process to generate the final results. Therefore, such a choice also satisfies the definition of a gradient part. Now, we have our gradient part  and transfer part . 

This combination solves the two problems mentioned above successfully. Firstly, our new transfer parts do not introduce new errors. This property also means that it keeps the results at the next step on the target manifold because generating samples away is a kind of error. This shows that we solve the first problem. Secondly, we know that the prediction of  is more and more precise in the reverse process in the above subsection. And our new transfer part can generate precise results according to the precise prediction of . Therefore, our generation results are more and more precise using pseudo numerical methods, while classical numerical methods can introduce obvious error at the last several steps. This shows that we solve the second problem, too. We also find that their combination  is just the inference equation used by DDIMs, so DDIMs is a simple case of pseudo numerical methods. Here, we define DDIMs as DDIMs*, emphasizing that it is a pseudo numerical method.


\subsection{Gradient Part}
\label{LMSM}

Because we split numerical methods into two parts, we can use the same gradient part from different classical numerical methods freely (e.g., linear multi-step method), although we change the transfer part of our inference equation. Our theoretical analyses and experiments show that the gradient part from different classical methods can work well with our new transfer part (see Section \ref{sec_corder}, \ref{sec_qua_eff}). By using the same gradient part of the linear multi-step method, we have:

\begin{figure}[h]
   \vspace*{-0.2cm}
   \begin{minipage}[t]{0.54\linewidth}
      \vspace*{-0.1cm}
      
      By using the same gradient part of Runge-Kutta method, we have:
      
   \end{minipage}
   \ \ 
   \vspace*{-\baselineskip}
   \begin{minipage}[t]{0.43\linewidth}
      \vspace*{-0.2cm}
      \begin{algorithm}[H]
         \small
         \caption{DDIMs}
         \label{origin_alg}
         \begin{algorithmic}[1]
            \STATE 
            \FOR {}
               \STATE  \\
            \ENDFOR
            \RETURN 
         \end{algorithmic}
      \end{algorithm}
      \vspace*{-0.7cm}
      \begin{algorithm}[H]
         \small
         \caption{PNDMs}
         \label{fourth_alg}
         \begin{algorithmic}[1]
            \STATE 
            \FOR {}
               \STATE  \\
            \ENDFOR
            \FOR {}
               \STATE 
            \ENDFOR
            \RETURN 
         \end{algorithmic}
      \end{algorithm}
      \vspace*{-0.5cm}
   \end{minipage}
\end{figure}

Abbreviate Equation (\ref{one_order}) and (\ref{four_order}) as , .








Here, we have provided three kinds of pseudo numerical methods. Although advanced numerical methods can accelerate the denoising process, some may have to compute the gradient part  more times at every step, like the Runge-Kutta method. Propagating forward four times along a neural network makes the denoising process slower. However, we find that the linear multi-step method can reuse the result of  four times and only compute  once at every step. And theoretical analyses tell us that the Runge-Kutta and linear multi-step method have the same convergence order and similar results. 

\begin{wraptable}{r}{7.2cm}
   \small
   \begin{tabular}{|l|l|p{65pt}|}
   \hline
      \diagbox{\small{}}{\small{order}}  & first  & non-first \\ \hline
      linear & forward Euler & linear multi-step, Runge-Kutta... \\ \hline
      nonlinear & DDIM    & PNDM  \\ \hline
   \end{tabular}
   \caption{The relationship between different numerical methods.}
   \label{tb_relation}
   \vspace*{-0.5cm}
\end{wraptable}

Therefore, we use the gradient part of the linear multi-step method and our new transfer part as our main pseudo numerical methods for diffusion models (PNDMs). In Table \ref{tb_relation}, we show the relationship between different numerical methods. Here, we can see PNDMs combine the benefits of higher-order classical numerical methods (in the gradient part) and DDIMs (in the transfer part).

\subsection{Algorithm}
\label{algo_choice}

We can provide our whole algorithm of the denoising process of DDIMs now. According to \citet{Song2020a}, the algorithm of the original method satisfies Algorithm \ref{origin_alg}. And our new algorithm of PNDMs uses the pseudo linear multi-step and pseudo Runge-Kutta method, which satisfies Algorithm \ref{fourth_alg}. Here, we cannot use linear multi-step initially because the linear multi-step method cannot start automatically, which needs at least three previous steps' information to generate results. So we use the Runge-Kutta method to compute the first three steps' results and then use the linear multi-step method to calculate the remaining. 

We also use the gradient parts of two second-order numerical methods to get another pseudo numerical method. We introduce the details of this method in Appendix \ref{som_sec}. We call it S-PNDMs, because its gradient part uses information from two steps at every step. Similarly, we also call our first PNDMs F-PNDMs, which use data from four steps, when we need to distinguish them. 


\subsection{Convergence Order}
\label{sec_corder}


Change the transfer part of numerical methods may introduce unknown error. To determine the influence of our new transfer part theoretically, we compute the local and global error between the theoretical result of Equation (\ref{eq_limit})  and our new methods, we find that  and 

If the target ODE satisfies Lipschitz condition and local error , then there are  and  such that the global error  satisfies . And we have that the convergence order is equal to the order of the global error. The detailed proof can be found in Appendix \ref{ap_oapm}. Therefore, we get the following property:
\begin{property}
   S/F-PNDMs have third-order local error and are second-order convergent.
   \label{pro_conorder}
\end{property}











\section{Experiment}
\label{experiment}


\subsection{Setup}

We conduct unconditional image generation experiments on four datasets: Cifar10 () \citep{krizhevsky2009}, CelebA () \citep{liu2015}, LSUN-church () and LSUN-bedroom () \citep{yu2016}. According to the analysis in Section \ref{formula_trans}, we can use pre-trained models from prior works in our experiments. The pre-trained models for Cifar10, LSUN-church and LSUN-bedroom are taken from \citet{Ho2020} and the pre-trained model for CelebA is taken from \citet{Song2020a}. In these models, the number of total steps N is 1000 and the variance schedule is linear variance schedule. And we also use a pre-trained model for Cifar10, which uses a cosine variance schedule from improved denoising diffusion probabilistic models (iDDPMs \citep{Nichol2021}).

\subsection{Sample Efficiency and Quality}
\label{sec_qua_eff}

\begin{table}[!t]
   \begin{minipage}[t]{0.73\linewidth}
      \centering
      \small
      \begin{tabular}{l |l |c c >{\columncolor[gray]{0.8}} c c c c | c}
         \toprule[1pt]
         \hline
         dataset & \diagbox{\scriptsize{model}}{\scriptsize{FID}}{\scriptsize{step}} & 10 & 20 & 50 & 100 & 250 & 1000 & time\\
         \hline
         \multirow{2}{*}{Cifar10}   & DDIM   & 13.4 & 6.84 & 4.67 & 4.16 & & 4.04 \\
                                    & PF     & & 13.8 & 3.89 & 3.69 & 3.71 & 3.72 \\
         \hline
         \multirow{4}{*}{\shortstack{Cifar10\cosine)}}    & DDIM     & 14.5 & 8.79 & 5.86 & 4.92 & 4.30 & 3.69 & 0.505\\
                                    & S-PNDM     & 8.64 & 5.77 & 4.46 & 3.94 & 3.71 & 3.38 & 0.517\\
                                    & F-PNDM    & 7.05 & 4.61 & 3.68 & 3.53 & 3.49 & \textbf{3.26} & 0.595\\
         \hline
         \hline
         CelebA                     & DDIM  & 17.3 & 13.7 & 9.17 & 6.53 & & 3.51 \\
         \hline
         \multirow{4}{*}{\shortstack{CelebA\
   \begin{split}
      e_{t,\delta}   &= x(t+\delta) - x_{t,\delta} \\
                     &= \left(x(t) + \delta f(x(t), t) + \frac{\delta^2}{2}f''(c) \right) - (x(t) + \delta f(x_t, t)) \\
                     &= \frac{\delta^2}{2}f''(c) \leq \frac{\delta^2}{2}M
   \end{split}
   \label{error_one}

   \begin{split}
      e_{t,\delta}   =& x(t+\delta) - x_{t,\delta} \\
                     =& \left(x(t) + \frac{\delta}{1 !} f(x(t), t)+\frac{\delta^{2}}{2 !}f'(x(t), t)+\cdots+\frac{\delta^{4}}{4 !} f^{(3)}(x(t), t)+O\left(\delta^{5}\right)  \right) \\
                     & - \left(x(t) + b_1 \delta f(x(t), t) +\sum_{s=2}^4 b_{s} \delta \left(\sum_{k=0}^{4}\frac{(-(s-1) \delta)^{k}}{(k) !} f^{(k-1)}(x(t), t)+O\left(\delta^{5}\right)\right)  \right) \\
                     =& O(\delta^5)
   \end{split}
   \label{error_four}

   (-1)^{j} b_{2}+(-2)^{j} b_{3}+(-3)^{j} b_{4}=\frac{1}{j+1}.

   \begin{split}
      x(t+M\delta) - x_{t,M\delta} \leq& |x(t+M\delta) - x_{t+(M-1)\delta, \delta}| + |x_{t+(M-1)\delta, \delta} - x_{t,M\delta}| \\
                                    \leq& e_{t+(M-1)\delta, \delta} + e^{L\delta} |x(t+(M-1)\delta) - x_{t,(M-1)\delta}| \\
                                    \leq& e_{t+(M-1)\delta, \delta} + e^{L\delta} e_{t+(M-2)\delta, \delta} + \cdots \\
                                    \leq& C\delta^{k+1} \left(1+e^{L h}+\cdots+e^{(i-1) L h}\right) \\
                                    =& C h^{k+1} \frac{e^{i L \delta}-1}{e^{L \delta}-1} \leq C \delta^{k+1} \frac{e^{i L \delta}-1}{L\delta} \\
                                    =&O(\delta^k).
   \end{split}

   \lim_{\delta\to 0}\frac{x^{2\delta}_{t+T} - x(t+T)}{x^{\delta}_{t+T} - x(t+T)} = \frac{(2\delta)^k}{\delta^k} = 2^k.

   \begin{cases}
      &k_1 = f(x_t, t) \\
      &k_2 = f(x_t+\delta k_1, t+\delta) \\
      &x_{t+\delta} = x_t + \frac{\delta}{2}(k_1+k_2)
   \end{cases}

   x_{t+\delta} = x_t + \frac{\delta}{2}(3f_t - f_{t-\delta})

   \begin{cases}
      &e^1_t = \epsilon_\theta(x_t, t) \\
      &x^1_t = \phi(x_t, e_t^1, t, t+\delta) \\
      &e^2_t = \epsilon_\theta(x_t^1, t+\delta) \\
      &e'_t = \frac{1}{2}(e^1_t + e^2_t) \\
      &x_{t+\delta} = \phi(x_t, e'_t, t, t+\delta) 
   \end{cases}
   \label{eq_pie}

   \begin{cases}
      &e_t = \epsilon_\theta(x_t, t) \\
      &e'_t = \frac{1}{2}(3e_t - e_{t-\delta}) \\
      &x_{t+\delta} = \phi(x_t, e_t', t, t+\delta)
   \end{cases}
   \label{eq_plms2}

         \begin{split}
            x_{t+\delta}, e_t &= PIE(x_t, \{e_p\}_{p<t}, t, t+\delta), \\
            x_{t+\delta}, e_t^1 &= PLMS'(x_t, t, t+\delta).
         \end{split}
      
   \bar{\alpha}_t = e^{at^2+bt+c},

   \begin{split}
       & \lim _{\delta \rightarrow 0} \frac{x_{t-\delta}-x_{t}}{\delta} \\
      =& \lim _{\delta \rightarrow 0} \frac{\bar{\alpha}_{t-\delta}-\bar{\alpha}_t}{\delta} \left(\frac{x_t}{\sqrt{\bar{\alpha}_t}(\sqrt{\bar{\alpha}_{t-\delta}}+\sqrt{\bar{\alpha}_t})} - 
         \frac{\epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}(\sqrt{(1-\bar{\alpha}_{t-\delta})\bar{\alpha}_{t}} + \sqrt{(1-\bar{\alpha}_{t})\bar{\alpha}_{t-\delta}})}\right) \\
      =& \lim _{\delta \rightarrow 0} \frac{\bar{\alpha}_{t-\delta}-\bar{\alpha}_t}{\delta} \left(\frac{x_t}{2\bar{\alpha}_t} - 
         \frac{\epsilon_\theta(x_t, t)}{2\sqrt{1-\bar{\alpha}_{t}}\bar{\alpha}_{t}}\right) = (e^{at^2+bt})' \left(\frac{x_t}{2\bar{\alpha}_t} - \frac{\epsilon_\theta(x_t, t)}{2\sqrt{1-\bar{\alpha}_{t}}\bar{\alpha}_{t}} \right) \\
      =& (2at+b)\bar{\alpha}_t \left(\frac{x_t}{2\bar{\alpha}_t} - \frac{\epsilon_\theta(x_t, t)}{2\sqrt{1-\bar{\alpha}_{t}}\bar{\alpha}_{t}} \right) 
      =  \frac{1}{2}(2at+b)(x_t - \frac{\epsilon_\theta(x_t, t)}{\sqrt{1-\bar{\alpha}_t}}). \\
   \end{split}

   \begin{split}
      x_{t'} &= \sqrt{\bar{\alpha}_{t'}}\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta}{\sqrt{\bar{\alpha}_t}}\right) + \sqrt{1-\bar{\alpha}_{t'}}\epsilon_\theta \\
             &= \sqrt{\bar{\alpha}_{t'}}\left(\frac{\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}\right) + \sqrt{1-\bar{\alpha}_{t'}}\epsilon_\theta(x_t, t) \\
             &= \sqrt{\bar{\alpha}_{t'}}x_0 + \sqrt{1-\bar{\alpha}_{t'}}\epsilon.
   \end{split}

   \begin{split}
       x(1) &= x(0) + \sum_{\delta\to 0}(y(t+\delta) - y(t)) \\
            &= x(0) + \sum_{\delta\to 0}\left(f(x(t),t,\delta)+g(t,\delta)\epsilon(x(t),t)\right) \\
            &= x(0) + \int_0^1\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right).
   \end{split}

   \begin{split}
      f(x(t),t,\delta)&=\left(\frac{\sqrt{\alpha(t+\delta)}}{\sqrt{\alpha}}-1\right)x(t) \\
      g(t, \delta)&=\sqrt{1-\alpha(t+\delta)} - \frac{\sqrt{(1-\alpha(t))\alpha(t+\delta)}}{\sqrt{\alpha(t)}}
   \end{split}

   \begin{split}
           &x(t+\delta) \\
           =& x(t) + \delta\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right) + \\
               & \frac{\delta^2}{2}\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right)' + O(\delta^3) \\
           =& x(t) + \delta\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right) + O(\delta^3) + \\
           & \frac{\delta^2}{2}\left(\frac{\partial^2 f}{\partial \delta\partial t}(x(t),t,0)+\frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\left(\frac{\partial f}{\partial \delta}(x(t),t,0)+\frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t),t)\right) \right) + \\
           & \frac{\delta^2}{2} \left( \frac{\partial^2 g}{\partial \delta\partial t}(t,0)\epsilon(x(t),t) + \frac{\partial g}{\partial \delta}(t,0)\epsilon'(x(t),t)\right)     
   \end{split}

   \begin{split}
        & x_{\text{S-PNDM}}(t+\delta) \\
       =& x(t) + f(x(t),t,\delta) + g(t,\delta)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t)+\phi(...,t,\delta), t+\delta)) \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \left(\delta\frac{\partial g}{\partial \delta}(t,0) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\right)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t+\delta)+O(\delta^2), t+\delta))  \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \left(\delta\frac{\partial g}{\partial \delta}(t,0) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\right)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t+\delta), t+\delta)) \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \left(\delta\frac{\partial g}{\partial \delta}(t,0) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\right)\frac{1}{2}(\epsilon(x(t),t) + \epsilon(x(t), t) + \delta \epsilon(x(t),t)') \\
       =& x(t) + \delta\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\delta^2}{2}\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + O(\delta^3) + \\
        & \delta\frac{\partial g}{\partial \delta}(t,0)\left(\epsilon(x(t), t) + \frac{1}{2}\delta \epsilon(x(t),t)'\right) + \frac{\delta^2}{2}\frac{\partial^2 g}{\partial \delta^2}(t,0)\epsilon(x(t),t) \\
       =& x(t) + \delta\left(\frac{\partial f}{\partial \delta}(x(t),t,0) + \frac{\partial g}{\partial \delta}(t,0)\epsilon(x(t), t)\right) + O(\delta^3) + \\
        & \frac{\delta^2}{2}\left(\frac{\partial^2 f}{\partial \delta^2}(x(t),t,0) + \frac{\partial g}{\partial \delta}(t,0) \epsilon(x(t),t)' + \frac{\partial^2 g}{\partial \delta^2}(t,0)\epsilon(x(t),t) \right).
   \end{split}

   \begin{split}
        & x(t+\delta) - x_{\text{S-PNDM}}(x+\delta) \\
       =& \frac{\delta^2}{2}\left((\frac{\partial^2 f}{\partial \delta\partial t}-\frac{\partial^2 f}{\partial \delta^2})(x(t),t,0) +\frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial f}{\partial \delta}(x(t),t,0)\right) + \\
       & \frac{\delta^2}{2}\left((\frac{\partial^2 g}{\partial \delta\partial t}-\frac{\partial^2 g}{\partial \delta^2})(t,0) + \frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial g}{\partial \delta}(t,0)\right)\epsilon(x(t),t) + O(\delta^3)
   \end{split}
   \label{eq_diff_order}

   \begin{split}
       \frac{\partial g}{\partial \delta}(t, \delta) =& \frac{\partial}{\partial \delta}\left(\sqrt{1-\alpha(t+\delta)} - \frac{\sqrt{(1-\alpha(t))\alpha(t+\delta)}}{\sqrt{\alpha(t)}}\right) \\
                                                     =& \frac{-\alpha'(t+\delta)}{2\sqrt{1-\alpha((t+\delta))}} - \frac{\sqrt{1-\alpha(t)}\alpha'(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}}
   \end{split}

   \begin{split}
       \frac{\partial f}{\partial \delta}(x(t), t, \delta) =& \frac{\partial}{\partial \delta}\left(\left(\frac{\sqrt{\alpha(t+\delta)}}{\sqrt{\alpha}}-1\right)x(t)\right) \\
                                                           =& \frac{\alpha'(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}}x(t)
   \end{split}

   \begin{split}
       \frac{\partial^2 f}{\partial \delta^2}(x(t),t,\delta)|_{\delta=0} =&\frac{\alpha''(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}}x(t) + \frac{-\alpha'(t+\delta)^2}{4\sqrt{\alpha(t)\alpha(t+\delta)^3}}x(t)|_{\delta=0} 
   \end{split}

   \begin{split}
       \frac{\partial^2 f}{\partial \delta\partial t}(x(t),t,0) =& \frac{\alpha''(t)}{2\alpha(t)}x(t)-\frac{\alpha'(t)^2}{2\alpha(t)^2}x(t)
   \end{split}

   \begin{split}
       \frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0) =& \frac{\alpha'(t)}{2\alpha(t)}
   \end{split}

   \begin{split}
       \frac{\partial^2 g}{\partial \delta^2}(t, \delta)|_{\delta=0} =& \frac{-\alpha''(t+\delta)}{2\sqrt{1-\alpha((t+\delta))}} - \frac{\sqrt{1-\alpha(t)}\alpha''(t+\delta)}{2\sqrt{\alpha(t)\alpha(t+\delta)}} + \\
                                           & \frac{-\alpha'(t+\delta)^2}{4\sqrt{1-\alpha(t+\delta)}^3} + \frac{\sqrt{1-\alpha(t)}\alpha'(t+\delta)^2}{4\sqrt{\alpha(t)\alpha(t+\delta)^3}}|_{\delta=0} \\
                                           =& \frac{-\alpha''(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha''(t)}{2\sqrt{\alpha(t)\alpha(t)}} + \\
                                           & \frac{-\alpha'(t)^2}{4\sqrt{1-\alpha(t)}^3} + \frac{\sqrt{1-\alpha(t)}\alpha'(t)^2}{4\sqrt{\alpha(t)\alpha(t)^3}} \\
   \end{split}

   \begin{split}
       \frac{\partial^2 g}{\partial \delta\partial t}(t, \delta)|_{\delta=0} =& \frac{\partial}{\partial t}\left(\frac{-\alpha'(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha'(t)}{2\sqrt{\alpha(t)\alpha(t)}}\right) \\
                                           =& \frac{-\alpha''(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha''(t)}{2\sqrt{\alpha(t)\alpha(t)}} + \\
                                           & \frac{-\alpha'(t)^2}{4\sqrt{1-\alpha(t)}^3} + \frac{\sqrt{1-\alpha(t)}\alpha'(t)^2}{2\alpha(t)^2} + \frac{\alpha'(t)^2}{4\sqrt{\alpha^2(t)(1-\alpha(t))}}  
   \end{split}

   \begin{split}
        &(\frac{\partial^2 f}{\partial \delta\partial t}-\frac{\partial^2 f}{\partial \delta^2})(x(t),t,0) +\frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial f}{\partial \delta}(x(t),t,0) \\
       =& \left(\frac{\alpha''(t)}{2\alpha(t)}x(t)-\frac{\alpha'(t)^2}{2\alpha(t)^2}x(t)\right) - \left(\frac{\alpha''(t)}{2\alpha(t)}x(t) + \frac{-\alpha'(t)^2}{4\alpha(t)^2}x(t) \right) + \frac{1}{2\alpha(t)}\frac{\alpha'(t)}{2\alpha(t)}x(t) \\
       =& 0
   \end{split}

   \begin{split}
        &(\frac{\partial^2 g}{\partial \delta\partial t}-\frac{\partial^2 g}{\partial \delta^2})(t,0) + \frac{\partial^2 f}{\partial \delta\partial x}(x(t),t,0)\frac{\partial g}{\partial \delta}(t,0) \\
       =&\frac{\sqrt{1-\alpha(t)}\alpha'(t)^2}{4\alpha(t)^2} + \frac{\alpha'(t)^2}{4\alpha(t)\sqrt{1-\alpha(t)}} + \frac{\alpha'(t)}{2\alpha(t)}\left(\frac{-\alpha'(t)}{2\sqrt{1-\alpha((t))}} - \frac{\sqrt{1-\alpha(t)}\alpha'(t)}{2\alpha(t)}\right)\\
       =& \frac{\alpha'(t)^2}{4\alpha(t)^2\sqrt{1-\alpha(t)}} + \frac{\alpha'(t)}{2\alpha(t)}\left(\frac{-\alpha'(t)}{2\sqrt{1-\alpha(t)}\alpha(t)}\right) \\
       =&0
   \end{split}

   x(t+\delta) - x_{\text{S-PNDM}}(x+\delta) = O(\delta^3)

   \begin{split}
      &\phi(x_t, \epsilon_\theta(x_t), t, t-\delta) \\
      = &\frac{\sqrt{\bar{\alpha}_{t-\delta}}}{\sqrt{\bar{\alpha}_t}}x_t - 
      \frac{(\bar{\alpha}_{t-\delta}-\bar{\alpha}_t)}{\sqrt{\bar{\alpha}_t}(\sqrt{(1-\bar{\alpha}_{t-\delta})\bar{\alpha}_{t}} + \sqrt{(1-\bar{\alpha}_{t})\bar{\alpha}_{t-\delta}})}\epsilon_t \\
      = &\frac{\sqrt{1-(t-\delta)}}{\sqrt{1-t}}x_t - 
       \frac{\delta}{\sqrt{1-t}\left(\sqrt{(t-\delta)(1-t)} + \sqrt{t(1-(t-\delta))}\right)}\epsilon_\theta(x_t)
   \end{split}

      x_{t-\delta}=x_t + \phi(x_t, \epsilon_\theta(x_t), t, t-\delta)

   \begin{split}
      e_t' & = \bar{\alpha}'(t)\left(\frac{x_t}{2\bar{\alpha}(t)}-\frac{\epsilon_\theta(x_t)}{2\bar{\alpha}(t)\sqrt{1-\bar{\alpha}(t)}}\right) \\
           & = -\left(\frac{x_t}{2(1-t)}-\frac{\epsilon_\theta(x_t)}{2(1-t)\sqrt{t}}\right) \\
      x_{t-\delta} & = x_t + \frac{\delta}{24}(55e_t'-59e_{t+\delta}'+37e_{t+2\delta}'-9e_{t+3\delta}')
   \end{split}

   \begin{split}
      e' = \frac{1}{24}(55\epsilon_\theta(x_t)-59\epsilon_\theta(x_{t+\delta})&+37\epsilon_\theta(x_{t+2\delta})-9\epsilon_\theta(x_{t+3\delta})) \\
      x_{t-\delta} = x_t + \phi(x_t&, e', t, t-\delta)
   \end{split}
linear)}}   
            & DDIM*     & 18.50.06 & 10.86.08 & 6.95.04 & 5.49.06 & 4.52.02 & 4.02.04 \\
            & FON       & 13.00.11 & 7.33.06 & 5.24.05 & 4.64.04 & 4.12.03 & 3.73.03 \\
            & S-PNDM    & 11.58.10 & 7.53.07 & 5.15.05 & 4.34.03 & 3.93.02 & 3.83.03 \\
            & F-PNDM    & 6.12.07 & 5.04.07 & 4.01.02 & 3.75.04 & \textbf{3.67.03} & 3.78.04 \\
      \bottomrule[1pt]
   \end{tabular}
   \caption{Image generation measured in FID on Cifar10. DDIM* means a kind of pseudo numerical method and also a retest of DDIM.}
\end{table}


\end{document}