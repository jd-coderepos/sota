\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage[flushleft]{threeparttable}
\begin{document}

\title{Domain Adaptation for sEMG-based Gesture Recognition with Recurrent Neural Networks}

\author{\IEEEauthorblockN{Istv\'an Ketyk\'o, Ferenc Kov\'acs and Kriszti\'an Zsolt Varga}
\IEEEauthorblockA{\textit{Nokia Bell Labs} \\
\{istvan.ketyko, ferenc.2.kovacs, krisztian.varga\}@nokia-bell-labs.com}
}

\maketitle

\IEEEpubid{\begin{minipage}{\textwidth}\ \ \label{eq:rnn}
\begin{aligned}
& \mathbf{h}_{t} = \sigma_{h} (\mathbf{w}_{h}\mathbf{x}_{t}+\mathbf{u}_{h}\mathbf{h}_{t-1}+\mathbf{b}_{n}) \\
& \mathbf{y}_{t} = \sigma_{y} (\mathbf{w}_{y}\mathbf{h}_{t}+\mathbf{b}_{y})
\end{aligned}
 \label{eq:lstm}
\begin{aligned}
& \mathbf{f}_{t} = \sigma( \mathbf{W}_{f} \cdot [ \mathbf{h}_{t-1}, \mathbf{x}_{t} ] + \mathbf{b}_{f} )  \\
& \mathbf{i}_{t} = \sigma( \mathbf{W}_{i} \cdot [ \mathbf{h}_{t-1}, \mathbf{x}_{t} ] + \mathbf{b}_{i} ) \\
& \mathbf{\widetilde{C}}_{t} = \tanh( \mathbf{W}_{C} \cdot [ \mathbf{h}_{t-1}, \mathbf{x}_{t} ] + \mathbf{b}_{C} ) \\
& \mathbf{C}_{t} = \mathbf{f}_{t} \ast \mathbf{C}_{t-1} + \mathbf{i}_{t} \ast \mathbf{\widetilde{C}}_{t} \\
& \mathbf{o}_{t} = \sigma( \mathbf{W}_{o} \cdot [ \mathbf{h}_{t-1}, \mathbf{x}_{t} ] + \mathbf{b}_{o} ) \\
& \mathbf{h}_{t} = \mathbf{o}_{t} \ast \tanh(\mathbf{C}_{t})
\end{aligned}

\begin{bmatrix}
  x'_{1}  \\
  \vdots \\
  x'_{f}
\end{bmatrix}
 &= \left[\begin{array}{ccc}
  m_{11} & \dots & m_{1f} \\
  \vdots & m_{ij} & \vdots \\
  m_{f1} & \dots & m_{ff} \\ 
\end{array}\right]
\begin{bmatrix}
  x_{1}  \\
  \vdots \\
  x_{f}
\end{bmatrix}
{} + \begin{bmatrix}
  b_{1}  \\
  \vdots \\
  b_{f}
\end{bmatrix}

  \label{eq:multipleentropy}
  \jmath_{\mbox{\tiny entropy}} = -\sum_{g \in G} {\textrm{I}_{g} \ln p_g}

  \label{eq:classification_accuracy}
  \mbox{Classification Accuracy} = \dfrac{\mbox{Correct classifications}}{\mbox{Total classifications}}*100\%


\subsection{Intra-session validation}

We first look at intra-session validation to benchmark our sequence classifier against the state-of-the-art in the least challenging scenario, on top of distinct datasets, without performing any further optimization.

In case of CapgMyo dataset we used the same evaluation procedure that was used in the previous study \cite{b18}.
For each subject, a classifier was trained by using 50\% of the data (E.g., trials 1, 3, 5, 7 and 9 for that subject) and tested by using the remaining half.
This procedure was performed on each sub-database. For DB-b, the second session of each subject was used for the evaluation.

In previous works on NinaPro DB-1 \cite{b23}, \cite{b18}, the training set consisted of approximately 2/3 of the gesture trials of each subject and the remaining trials constitute the test set.

We chose 150-ms sequence length for the RNN in all the cases for fair comparison. Table~\ref{table:intra-session} shows our average intra-session recognition accuracy together with the state-of-the-art.
Columns noted with DB-a, DB-b, DB-c belong to the CapgMyo dataset and columns noted with DB-1 12 gestures and DB-1 8 gestures belong to the NinaPro dataset.
\begin{table}[tbp]
	\centering
	\begin{threeparttable}
		\setlength{\tabcolsep}{3pt}
		\def\arraystretch{1.5}\begin{tabular}{|c|c|c|c|c|c|} 
			\hline
			& \multicolumn{3}{|c|}{CapgMyo} & \multicolumn{2}{|c|}{NinaPro} \\
			\hline
			& DB-a & DB-b & DB-c & DB-1 12 gestures & DB-1 8 gestures \\
			Du\cite{b18} & 99.5\% & 98.6\% & 99.2\% & 84\% & 83\% \\
			Hu\cite{b19} & 99.7\% & -\tnote{a} & - & - & - \\
			Atzori\cite{b23} & - & - & - & 90\% & - \\
			2SRNN & 97.1\% & 97.1\% & 96.8\% & 84.7\% & 90.7\% \\
			\hline
		\end{tabular}
		\caption{Intra-session recognition accuracy results}
		\begin{tablenotes}
			\item[a] '-' notes that the authors of that method did not focus on the scenario.
		\end{tablenotes}
		\label{table:intra-session}
	\end{threeparttable}
\end{table}
As can be seen from Table~\ref{table:intra-session} the accuracy achieved by our model is at most 2.4 percentage points worse than other methods for the CapgMyo database and with 0.7 up to 7.7 percentage points better for the NinaPro dataset. This accuracy has been achieved by keeping the training duration to constant 100 epochs and without any hyper-parameter tuning. This outcome indicates that that our model is at least comparable in the intra-session case with other approaches.

\subsection{Inter-session validation}

We evaluated inter-session recognition for CapgMyo DB-b,
in which the model was trained using data recorded from the first session and evaluated using data recorded from the second session.
In each case, we ran our domain adaptation for 100 epochs using the following 3 scenarios:
\begin{enumerate}
	\item Scenario 1: domain adaptation is not applied,
	\item Scenario 2: domain adaptation performed on the complete set of target data (all data of the target session); this scenario has only been considered for the purpose of comparability with alternative approaches,
	\item Scenario 3: domain adaptation performed on 50\% of the trials of the target session, while the validation set is the remaining 50\%.
\end{enumerate}
Our adaptation scheme enhanced inter-session recognition with 29 percentage points (accuracy of 83.8\% compared to 54.6\%) which is a 53\% improvement (shown in Table~\ref{table:inter-session}). 

\begin{table}[tpb]
	\centering
	\begin{threeparttable}
		\def\arraystretch{1.5}\begin{tabular}{|c|c|c|c|} 
			\hline
			& Scenario 1 & Scenario 2 & Scenario 3 \\
			\hline
			Du\cite{b18} & 47.9\% & - & 63.3\% \\
			2SRNN & 54.6\% & 85.8\% & 83.8\% \\
			\hline
		\end{tabular}
		\caption{Inter-session recognition accuracy results on CapgMyo DB-b}
		\label{table:inter-session}
	\end{threeparttable}
\end{table}

\subsection{Inter-subject validation}

In this experiment,
we evaluated inter-subject recognition of 8 gestures using the second recording session of CapgMyo DB-b and the recognition of 12 gestures using CapgMyo DB-c and the sub-set of 12 gestures from the NinaPro DB-1.
We performed a leave-one-out cross-validation,
in which each of the subjects was used in turn as the test subject and a classifier was trained using the data of the remaining subjects, using the following 3 scenarios:
\begin{enumerate}
	\item Scenario 1: domain adaptation is not applied,
	\item Scenario 2: domain adaptation performed on the complete set of target data (all data of the target subject); this scenario has only been considered for the purpose of comparability with alternative approaches,
	\item Scenario 3: domain adaptation performed on 50\% of the trials of the target subject, while the validation set is the remaining 50\%, with the following 2 variants:
	\begin{enumerate}
		\item CapgMyo DB-b, DB-c: 50\%-50\% of the target subject data (5 of the 10 trials are used for domain adaptation and another 5 is for its validation).
		\item NinaPro DB-1 12 gestures: 50\%-50\% of the target subject data (5 of the 10 trials are used for domain adaptation and another 5 is for its validation).
	\end{enumerate}
\end{enumerate}
In case of the CapgMyo DB-b and DB-c we ran our domain adaptation for 100 epochs, and in case of the Ninapro DB-1 for 400 epochs.
The sequence length of our RNN was 150 ms in case of the CapgMyo DB-b and DB-c for comparison reasons with \cite{b18}, and 400 ms in case of the Ninapro DB-1 for comparison reasons with \cite{b7}.
Table~\ref{table:inter-subject} shows the classification accuracies of the various methods.
\begin{table}[tpb]
	\centering
	\begin{threeparttable}
		\setlength{\tabcolsep}{1pt}
		\def\arraystretch{1.5}\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} 
			\hline
			& \multicolumn{3}{|c|}{Scenario 1} & \multicolumn{3}{|c|}{Scenario 2} & \multicolumn{3}{|c|}{Scenario 3} \\
			\hline
			& DB-b & DB-c & DB-1 & DB-b & DB-c & DB-1 & DB-b & DB-c & DB-1 \\
			Du\cite{b18} & 39.0\% & 26.3\% & - & - & - & - & 55.3\% & 35.1\% & - \\
			Atzori\cite{b23} & - & - & 25\% & - & - & - & - & - & -\\
			Patricia\cite{b7} & - & - & 30\% & - & - & - & - & - & 55\% \\
			2SRNN & 52.6\% & 34.8\% & 35.1\% & 96.8\% & 91.9\% & 65.7\% & 89.9\% & 85.4\% & 65.2\% \\
			\hline
		\end{tabular}
		\caption{Inter-subject recognition accuracy results}
		\label{table:inter-subject}
	\end{threeparttable}
\end{table}
Our adaptation scheme enhanced inter-subject recognition with a 71\% improvement on DB-b, 145\% improvement on DB-c and 86\% improvement on DB-1 12 gestures (shown in Table~\ref{table:inter-subject}).

We summarise the domain adaptation improvement results in Table~\ref{table:speed-up}. As indicated there, the performance of 2SRNN is superior in all cases: the improvement obtained from our domain adaptation in the inter-session and inter-subject cases exceeds those obtained through alternative domain adaptation approaches.
\begin{table}[tpb]
	\centering
	\begin{threeparttable}
		\setlength{\tabcolsep}{2pt}
		\def\arraystretch{1.5}\begin{tabular}{|c|c|c|c|c|c|} 
			\hline
			& Inter-session improvement & \multicolumn{3}{|c|}{Inter-subject improvement} \\
			\hline
			& DB-b & DB-b & DB-c & DB-1 \\
			Du\cite{b18} & 32\% & 42\% & 33\% & - \\
			Patricia\cite{b7} & - & - & - & 83\% \\
			2SRNN & \textbf{53\%} & \textbf{71\%} & \textbf{145\%} & \textbf{86\%} \\
			\hline
		\end{tabular}
		\caption{Domain adaptation improvement comparisons}
		\label{table:speed-up}
	\end{threeparttable}
\end{table}

It is natural to ask how much data is required to obtain a stable recognition accuracy and how our solutions relates to the common supervised fine-tuning method in deep learning.
Fig.~\ref{figure:lineChart} visualises a comparison of the inter-subject domain adaptation scenario (on the CapgMyo DB-b) based on our 2-stage RNN method and an adaptation based on supervised fine-tuning in one concrete scenario.
\begin{figure}[tbp]
	\centerline{\includegraphics[width=\columnwidth,keepaspectratio]{lineChart.pdf}}
	\caption{Recognition accuracy comparison of supervised approaches}
	\label{figure:lineChart}
\end{figure}
In this experiment we limited the available data to 20\%, 40\%, 60\%, 80\% and 100\%
of the total 5 trials used for domain adaptation (the remaining 5 trials are kept for validation). The mean classification accuracy is plotted as a function of the available target data for domain adaptation. Fig.~\ref{figure:lineChart} shows how the accuracy of the two method
increases with the amount of available target data and our 2SRNN remains persistently superior to the fine tuning method (by 20\%).
In each case we ran the domain adaptations for 5 epochs only since it is expected to get improvements quickly for better human-computer interactions.
On our server (with 2 Nvidia Titan V GPUs) these 5 training epochs took approximately 7.5 seconds for our 2SRNN and 27.7 seconds for supervised fine-tuning, respectively. Therefore a 20\% improvement in accuracy is complemented with a decrease in execution time by almost a factor of 4.

\section{Conclusions}

For real Human-Computer Interactions the sEMG-based gesture detection must overcome the inter-session and intersubject domain shifts. We proposed a 2-stage domain adaptation solution which has superior performance over the well-known supervised fine-tuning applied in deep learning and the state-of-the-art unsupervised adaptation methods. Empirical results show that the linear transformation of the input features is a good approximation for handling the domain shifts. It is fast and light weight and applicable (besides the RNN) to any machine learning approaches which are trainable with backpropagation. Combinations of this method with generative unsupervised models can be the next step of further improvements for usable HCI solutions.

The code is available at \url{https://github.com/ketyi/2SRNN}.

\begin{thebibliography}{00}
\bibitem{b1} H. Cheng, L. Yang, and Z. Liu, ``Survey on 3D Hand Gesture Recognition'', IEEE Trans. Circuits and Systems for Video Technology, Vol. 26, No. 9, September 2016, pp. 1659--1673.
\bibitem{b2} J. Li, T. Ma, X. Zhou, Y. Liu, S. Cheng, C. Ye, and Y. Wang, ``A Real-time Human Motion Recognition System Using Topic Model and SVM'', 2017 IEEE EMBS International Conference on Biomedical \& Health Informatics (BHI), 2017, pp. 173--176.
\bibitem{b3} S. Ranasinghe, F. Machot, and H. Mayr, ``A review on applications of activity recognition systems with regard to performance and evaluation'', Intl. Journal of Distributed Sensor Networks, 2016, Vol. 12(8).
\bibitem{b4} V. Patel, R. Gopalan, R. Li, and R. Chellappa, ``Visual Domain Adaptation: An Overview of Recent Advances'', IEEE Signal Processing Magazine, May 2015, Vol. 32 (3), pp. 53--69.
\bibitem{b5} V. Gregori, A. Gijsberts, and B. Caputo, ``Adaptive Learning to Speed-Up Control of Prosthetic Hands: a Few Things Everybody Should Know'', 2017 International Conference on Rehabilitation Robotics (ICORR), QEII Centre, London, UK, July 17-20, 2017, pp. 1130--1137.
\bibitem{b6} G. Andrew, R. Arora, J. Bilmes, and K. Livescu, ``Deep Canonical Correlation Analysis'', 30th Intl Conference on Machine Learning, Atlanta, Georgia, USA, 2013.
\bibitem{b7} N. Patricia, T. Tommasi, and B. Caputo, ``Multi-Source Adaptive Learning for Fast Control of Prosthetics Hand'',  Intl Conference on Pattern Recognition, Stockholm, Sweden, 2014.
\bibitem{b8} EMG dataset in lower limb, \url{http://archive.ics.uci.edu/ml/datasets/emg+dataset+in+lower+limb}, retrieved on 27.12.2018.
\bibitem{b9} R. N. Khushaba, M. Takruri, S. Kodagoda, and G. Dissanayake, ``Toward Improved Control of Prosthetic Fingers Using Surface Electromyogram (EMG) Signals'', Expert Systems with Applications, vol 39, no. 12, pp. 10731–10738, 2012.
\bibitem{b10} Mimetic Interfaces: Facial Surface EMG Dataset 2015: \url{https://tutcris.tut.fi/portal/en/datasets/mimetic-interfaces-facial-surface-emg-dataset-2015(8a21105e-4eca-4531-b021-a62509711ee0).html}.
\bibitem{b11} C. Sapsanis, G. Georgoulas, A. Tzes, D. Lymberopoulos, ``Improving EMG based classification of basic hand movements using EMD in 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society 13 (EMBC 13), July 3-7, pp. 5754 - 5757, 2013.''.
\bibitem{b12} CapgMyo: A High Density Surface Electromyography Database for Gesture Recognition: \url{http://zju-capg.org/myo/data}, accessed on 27.12.2018.
\bibitem{b13} S. Pizzolato et al., ``Comparison of six electromyography acquisition setups on hand movement classification tasks,'' PloS one, vol. 12, no. 10, p. e0186132, 2017.
\bibitem{b14} D. Rempel, M. Camilleri, and D. Lee, ``The Design of Hand Gestures for Human-Computer Interaction: Lessons from Sign Language Interpreters'', Int J Hum Comput Stud. 2015 Oct; 72(10-11): 728–735.
\bibitem{b15} J. Garcia, M. Cannito, and P. Dagenais, Hand Gestures: Perspectives andPreliminary Implications for AdultsWith Acquired Dysarthria, American Journal of Speech-Language Pathology, Vol. 9,  pp. 107–115, May 2000.
\bibitem{b16} U. Cote-Allard, C. Fall, A. Drouin, A. Campeau-Lecours, C. Gosselin, K. Glette, F. Laviolette, and B. Gosselin, ``Deep Learning for Electromyographic Hand Gesture Signal Classification Using Transfer Learning'', arXiv:1801.07756v4 [cs.LG] 19 Nov 2018.
\bibitem{b17} N. Nazmi, M. Rahman, S. Yamamoto, S. Ahmad, H. Zamzuri, and S. Mazlan, ``A Review of Classification Techniques of EMG Signals during Isotonic and Isometric Contractions'', Sensors (Basel). 2016 Aug; 16(8): 1304.
\bibitem{b18} Y. Du, W. Jin, W. Wei, Y. Hu, and W. Geng, ``Surface EMG-Based Inter-Session Gesture Recognition Enhanced by Deep Domain Adaptation'', Sensors 2017, 17, 458; doi:10.3390/s17030458.
\bibitem{b19} Y. Hu, Y. Wong, W. Wei, Y. Du, M. Kankanhalli, W. Geng, ``A novel attention-based hybrid CNN-RNN architecture for sEMG-based gesture recognition'', PLoS One, 2018;13(10):e0206049, Published 2018 Oct 30, doi:10.1371/journal.pone.0206049.
\bibitem{b20} Kim J, Mastnik S, André E. EMG-based hand gesture recognition for realtime biosignal interfacing. In: International Conference on Intelligent User Interfaces; 2008. p. 30–39.
\bibitem{b21} Menon R, Caterina GD, Lakany H, Petropoulakis L, Conway B, Soraghan J. Study on interaction between temporal and spatial information in classification of EMG signals in myoelectric prostheses. IEEE Transactions on Neural Systems and Rehabilitation Engineering. 2017;PP(99):1–1.
\bibitem{b22} Yun LK, Swee TT, Anuar R, Yahya Z, Yahya A, Kadir MRA. Sign Language Recognition System using SEMG and Hidden Markov Model. In: International Conference on Mathematical Methods, Computational Techniques and Intelligent Systems; 2013. p. 50–53.
\bibitem{b23} Atzori M., Gijsberts A., Castellini C., Caputo B., Hager A.G.M., Elsig S., Giatsidis G., Bassetto F., Müller H. Electromyography data for non-invasive naturally-controlled robotic hand prostheses. Sci. Data. 2014;1:140053. doi: 10.1038/sdata.2014.53.
\bibitem{adam} P. D. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, ICLR, 2014

\bibitem{emgbw} P. Konrad: The ABC of EMG: a practical introduction to kinesiological electromyography, ISBN: 0977162214, 2006


\bibitem{colahweb} 
Christopher Olah: Understanding LSTM Networks (retr. on 11.01.2018) \url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}.
\bibitem{razvan}
Razvan Pascanu et al.: On the difficulty of training recurrent neural networks. 2013.
\bibitem{hochreiter}
Hochreiter and J. Schmidhuber: Long short-term memory. Neural Computation, 1997.
\bibitem{chung}
Chung et al.: Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. (2014).

\end{thebibliography}
\end{document}
