

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  



\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{array}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{color}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\comment}[2]{\emph{[\textbf{#1}: #2]}}
\newcommand{\ana}[1]{\textcolor{blue}{\comment{Ana}{#1}}}
\newcommand{\thickhline}{\noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}

\usepackage{subcaption}
\usepackage{graphicx}

\title{\LARGE \bf
Domain and View-point Agnostic Hand Action Recognition



}

\author{Alberto Sabater \hspace{0.5cm} IÃ±igo Alonso  \hspace{0.5cm} Luis Montesano \hspace{0.5cm} Ana C.~Murillo

\thanks{ A. Sabater, I. Alonso, L. Montesano and A.C. Murillo are with 
DIIS - I3A, Universidad de Zaragoza, Spain. {\tt\small \{asabater, ialonso, montesano, acm\}@unizar.es}
}
\thanks{  L. Montesano is also with Bitbrain Technologies, Zaragoza, Spain. {\tt\small \{luis.montesano\}@bitbrain.com}
}
}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}
Hand action recognition is a special case of human action recognition with applications in human robot interaction, virtual reality or life-logging systems. Building action classifiers that are useful to recognize such heterogeneous set of activities is very challenging. There are very subtle changes across different actions from a given application but also large variations across domains (e.g. virtual reality vs life-logging).
This work introduces a novel skeleton-based hand motion representation model that tackles this problem. The framework we propose is agnostic to the application domain or camera recording view-point. 
We demonstrate the performance of our proposed motion representation model both working for a single specific domain (intra-domain action classification) and working for different unseen domains (cross-domain action classification). For the intra-domain case, our approach gets better or similar performance than current state-of-the-art methods on well-known hand action recognition benchmarks.
And when performing cross-domain hand action recognition (i.e., training our motion representation model in frontal-view recordings and testing it both for egocentric and third-person views), our approach achieves comparable results to the state-of-the-art methods that are trained intra-domain.
All our code, learned models and data splits will be released upon acceptance.

\end{abstract}

    


\section{Introduction}

Human action recognition is a well studied problem with many applications such as human-robot interaction, surveillance and monitoring \cite{krupke2018comparison, tanwani2017generative}. Deep models combined with skeleton-based representations \cite{bates2017line, moon2018v2v}, which efficiently encode human pose and motion, independently of appearance, surroundings and occlusions, have become a standard in robust human action recognition \cite{perez2019interaction, zhang2019view}.

Hand action recognition is a specific case of human action recognition. It is highly relevant due to the importance of hand movements in team work, assistive technologies, communication or in virtual reality applications \cite{abbasi2019multimodal, bates2017line}. 
Hand recognition methods combine, as in the full body case, skeleton representations and deep models \cite{yang2019make, zhang2016efficient}. These methods have shown good results typically focusing  on the classification of actions from a specific domain.
However, previous works have not studied robust representations that can generalize across different domains and view-points, that are key when working with limited amounts of labeled data.  

Hand actions expose some specific challenges to learn such robust representations. 
On one hand, there is a high variability across actions from different domains, e.g. user interface control vs. life-logging applications. Differently from full-body skeletons, different hand action domains often imply drastic view-point changes, e.g. egocentric vs. third-person view. 
On the other hand, fine grained details are essential. Different action categories are often quite similar and vary only subtly (e.g. pointing to different directions, sliding gestures, etc.). 
Moreover, hand skeleton joints present lower movement range than other full-body joints, increasing the correlation of skeleton joint motions and similar actions.









\begin{figure}[!tb]
    \centering
\includegraphics[width=0.9\columnwidth]{figures/TCN_pipeline_v3.png}
    \caption{Proposed motion representation model.
    \textbf{(a) Hand pose modeling}: pose features are extracted for each skeleton within the input hand skeleton stream.
    \textbf{(b) Motion description}: the pose features are processed by a Temporal Convolutional Network (TCN), generating a set of temporal motion descriptors. \textbf{(c) Motion summarization}: Motion descriptor weights are calculated to compute a summary of the descriptor sequence according to their relevance.
    }
    \label{fig:pipeline}
\end{figure}




This work presents a motion representation model, summarized in Fig. \ref{fig:pipeline}. It learns robust representations from labeled hand skeletons that can generalize for different application domains and view-points. The main components of our method are: 1) a set of skeleton features adapted to hand motion; 2) a Temporal Convolutional Network (TCN) encoding the stream of hand pose features into per-frame temporal descriptors; and 3) a summarization module that learns the relevance of each per-frame descriptor to describe each action.  
The motion representation model can be trained for the usual domain specific classification (\textbf{intra-domain}) or to recognize actions from new unseen domains (\textbf{cross-domain}) with an N-shot approach by using a K-Nearest Neighbors classifier (KNN).






Our experiments use the front view SHREC-17 dataset~\cite{de2017shrec}, the egocentric F-PHAB dataset \cite{garcia2018first} and the third-person MSRA \cite{sun2015cascaded} dataset, which include actions and gestures related to computer interaction, life-logging and sign language domains respectively.
Our intra-domain classification results show that our framework gets better or similar performance than current state-of-the-art intra-domain classifiers in well-known benchmarks. More importantly, our cross-domain classification approach obtains comparable accuracy to intra-domain methods by being trained just with the SHREC-17 dataset, and then evaluated on the F-PHAB and MSRA datasets. This demonstrates that our motion representation model generalizes well for different action domains and camera view-points.











\section{Related Work}





In the following, we summarize relevant works on the core topics of this work: pose modeling, skeleton-based action recognition models and generalization to unseen action categories. 



\subsection{Pose modeling modeling for action recognition}
Action recognition was first tackled by directly analyzing RGB videos \cite{feichtenhofer2016convolutional} or depth maps \cite{oreifej2013hon4d}. Current approaches have settled the standard of extracting the intermediate representation of skeleton poses \cite{zhang2016efficient, yang2019make}. This representation has shown great performance since it encodes human poses regardless their appearance and surrounding and presents strong robustness to occlusions.
 

Certain works directly use the Cartesian coordinates of the skeleton joints, using them as input for full-body action recognition \cite{perez2019interaction, liu2019ntu} and for  hand action recognition \cite{hou2018spatial, ma2020skeleton, li2021two}. 
In order to achieve a standardized and generic skeleton pose description, several full-body action recognition approaches propose different strategies, such as learning the most suitable view-point for each action \cite{zhang2019view} or transforming all coordinates to a common coordinate system \cite{sabater2021oneshot, su2020predict}. However, this kind of transformations cannot be directly applied to hand action recognition, where orientation plays a key role. 

In order to get more informative pose representations than raw Cartesian coordinates, many approaches propose to compute additional geometric features. Chen et al. \cite{chen2010learning} use joint orientations, velocity and acceleration, and their distance to other joints, lines and planes. Zhang et al. \cite{zhang2017geometric} calculate distances between joints and planes, and Yang et al.  \cite{yang2019make} use joint distances and their motion speeds at different scales.


Our approach proposes a simplification of the skeleton representation reducing coordinate redundancy by using just a set of key joints. 
Then, relative pose coordinates are computed to describe the hand pose, along with specific geometric features that describe its motion and orientation.

\subsection{Action recognition models}

As in many other fields, deep learning have become the state-of-the-art in action recognition. Of particular relevance for this work, Recurrent Neural Networks (RNN) have been widely used to model temporal dependencies in hand action recognition.
Ma et al. \cite{ma2020skeleton} use a LSTM-based Memory Augmented Neural Network to model dynamic hand gestures. Chen et al. \cite{chen2019mfa} use a LSTM Network to combine skeleton coordinates, global motions and finger motion features. Li et al.  \cite{li2021two} combine a bidirectional Independently Recurrent Neural Network with a self-attention based graph convolutional network.

Another common approach is to make use of Convolutional Networks. Liu et al. \cite{liu20203d} recognize posture and action by using a 3D convolutional neural network. Yang et al. \cite{yang2019make} use  1D convolutions to process and fuse different hand motion features. Hou \cite{hou2018spatial} propose  to focus on the most informative hand gesture features by using a  ResNet-like 1D convolutional network with attention.

Our method uses a Temporal Convolutional Network (TCN) \cite{bai2018empirical, oord2016wavenet} that implements 1D dilated convolutions to learn long-term temporal dependencies from variable-length input sequences, achieving comparable or better results than RNNs \cite{bai2018empirical}. TCNs  have already demonstrated good performance on hand motion unsupervised learning \cite{koneripalli2020rate} and general action recognition \cite{sabater2021oneshot, kim2017interpretable}.





\subsection{Generalization to unseen action categories}


Learning a model able to classify unseen categories is a challenging task. It is commonly tackled by encoding every new data sample into a descriptor and using a K-Nearest Neighbors classifier (KNN) to evaluate and assign labels according to the similarity between a few new category reference samples and the target samples \cite{wang2019simpleshot}.



Several works \cite{liu2019ntu,sabater2021oneshot} address this problem for action recognition by extracting intermediate feature maps from a supervised action recognition model.
Koneripalli et al. \cite{koneripalli2020rate} train an autoencoder to learn these descriptors in an unsupervised fashion. Ma et al. 
\cite{ma2020skeleton} learn these descriptors directly in a semi-supervised manner by training an encoder with metric-learning techniques.
Other recent works use wor2vec \cite{hahn2019action2vec} and sent2vec \cite{jasani2019skeleton} approaches for this descriptor learning.

Previous works \cite{liu2019ntu,hahn2019action2vec,jasani2019skeleton} are aimed to recognize unseen full-body action categories where no drastic camera view-points are found, but up to our knowledge, generalization to unseen hand view-points and domains is still to be studied.
The present work uses metric-learning along with specific data augmentation to learn meaningful hand sequence descriptors. Our framework is able to perform accurate action recognition of sequences from unseen categories and recording view-points.




\section{Hand action recognition framework}

The core of the proposed framework is the motion representation model summarized in Fig. \ref{fig:pipeline}. 
First, our approach calculates specific hand motion features for each skeleton. These features are fed to a Temporal Convolutional Network to generate a set of motion descriptors.
Additionally, a motion summarization module combine them, according to their relevance, into the final action representation.
In the following, we describe these steps, as well as, how to train our motion representation model, both for specific intra-domain and cross-domain classification. 



\subsection{Hand pose modeling}\label{sec:hand_modeling}



Human hand actions sequences are defined by sets of  hand skeleton poses , extracted from video frames. 
Each hand pose  is composed by a set of  joint coordinates, ,
which are logically connected by a set of  bones 
(see Fig. \ref{fig:min_hand}).



\subsubsection{Skeleton standardization}
Hand joints belonging to the same bones (fingers) are highly coupled and can be represented with a smaller number of degrees of freedom.
Based on this assumption, we propose to use just a subset of \textbf{7 joints to define a hand pose} (see Fig. \ref{fig:min_hand}), corresponding to the wrist, the top of the palm, and the tips of the 5 fingers; which we connect with a total of \textbf{6 hand bones}, one for the palm and one more for each one of the fingers. 
This simpler skeleton representation makes the learning process easier and less prone to overfitting.


\begin{figure}
    \centering
    \includegraphics[width=0.80\linewidth]{figures/minimal_hand_v2.png}
    \caption{Hand skeleton simplification. Left diagram refers to a detailed hand skeleton of 20 joints (dots) connected by 19 bones (lines). Right diagram refers to our proposed hand skeleton simplification of 7 joints (dots) and 5 bones (lines).
    }
    \label{fig:min_hand}
\end{figure}

Since action sequences can be performed by different people with heterogeneous hand sizes, we standardize each hand pose in order to achieve \textbf{scale-invariant} skeleton representations. Given a skeleton pose , its standardized version  is obtained by applying to its coordinates the transformation that makes the palm of size equal to 1:  where  is the euclidean distance between the bone defined by the wrist and the top of the palm.

Since we also want hand actions to be independent of the position where they are executed, we compute \textbf{location-invariant} coordinates by translating the top of the palm to the origin of the reference coordinate system. 




\subsubsection{Hand pose description}

These relative hand coordinates describe properly 
the intra-relation of their joints, but they do not have information about the hand translation.
Different from full body action sequences (e.g. walking) where this movement
can be inferred from the relative coordinates of its bones (e.g. legs),
hands can be translated trough any direction without any change of their relative coordinates. Since the translation information is essential in certain actions (e.g. pointing to specific directions), we generate extra \textbf{translation and orientation-aware} features from the original hand skeletons:
\begin{itemize}
    \item \textbf{Difference of coordinates}, defined as the difference of each joint coordinate with itself in the previous time-step. These features describe the translation direction and speed of each coordinate for each of the 3 axes:
        
    \item \textbf{Difference of bone angles}, defined as the difference of the elevation  and azimuth  of a bone  with itself in the previous time-step. These features describe the rotation direction and rotation speed of each bone:
        
        
\end{itemize}

Our final hand representation consists then of a vector of size 54, which stands for  relative hand coordinates,  coordinate difference features, and  bone angle differences.



\subsection{Motion representation model}\label{sec:tcn}



The core of our action recognition framework is a model that encodes the skeleton features from each frame, described in the previous section, into single descriptors with a Temporal Convolutional Network (TCN) \cite{bai2018empirical, oord2016wavenet}. 
The TCN processes sequences of skeleton features, generating a descriptor at each time-step that represents the motion performed up to that frame (see Fig. \ref{fig:pipeline}), i.e. with no information from the future.






\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/action_summarization.png}
    \caption{Action summarization module. Per-frame motion embeddings are simplified with a 1D Convolutional layer. Then, they are grouped with a single perceptron layer to calculate their summarization weights. Finally, the action sequence is summarized into a single descriptor by performing a weighted average over the initial motion embeddings.}
    \label{fig:summarization}
\end{figure}



For a given sequence, the last descriptor of the TCN encodes all the information up to that point and it is used to represent the sequence. 
In our case, however, the last part of action sequences is not always the most informative one to distinguish it from others. Consequently, the last descriptor is not the optimal one either. 


To alleviate this issue, we learn the relevance of the temporal patterns of the actions. More precisely, we add a \textbf{motion summarization module} after the TCN, which combines all the descriptors generated for 
the input hand motion, 
up the TCN memory length, by performing a weighted average over them (see Fig. \ref{fig:summarization}). 
These weights represent how important each descriptor is for the final action representation. They are learned with a simple Neural Network trained end-to-end along with the TCN. 
This network consists of a single 1D Convolutional layer with kernel 1 that reduces the per-frame descriptors dimensionality, and a single Fully Connected layer with a sigmoid activation layer, that takes as input all the simplified descriptors and outputs a vector of categorical probabilities (i.e. descriptor weights). These final weights are L1 normalized before performing the final descriptor summarization. 

This summarization module efficiently describes hand action sequences and helps the TCN to focus just on the meaningful data during training. However, there are real use cases where actions, at test time, present a longer length than our motion representation module can handle. In these cases, although the summarization module has been trained along with the TCN, 
it is more interesting to discard it and classify individually all the descriptors generated by the TCN, which still contains meaningful motion representations. 



So far, we have shown how to encode an action sequence  into a robust simple descriptor , where the function  represents our motion representation module. In the next two sections, we describe how to optimize these action representations to perform intra-domain classification and cross-domain classification







\subsection{Intra-domain classification}\label{sec:intra_dom}


Intra-domain action classification aims to recognize the same actions categories seen during the learning phase, with no drastic variation on the camera view-point. For this classification, intra-domain class probabilities  are predicted by a linear classifier  trained end-to-end along with our motion representation model . Intra-domain classification is learnt by the optimization of the categorical cross-entropy loss


which evaluates the predicted probabilities  that belongs to a class , given their true label .




For each iteration during training, the mini-batch is composed by two sequences sampled randomly for each one of the training classes . 
To ensure the data variability that might not be contained in small hand datasets, each motion sequence within the mini-batch is included three times more with different data augmentations. This data augmentation is applied to the per-frame skeletons , before the feature computation from Section \ref{sec:hand_modeling}, as follows:
\begin{itemize}
    \item Movement speed variation. Joint coordinates are randomly re-sampled by interpolation over the temporal dimension. This simulates that the action is performed at a different speed, and thus, having a different length.
    \item Frame skipping. Contiguous video frames contain similar joint information so, using one out of every three frames 
    reduces the data redundancy and makes the learning process easier. Our approach uses one out of three frames and augments each action sequence by initializing it randomly between the three first frames.
    \item Random cropping. When the sampled action is longer than a defined maximum length (i.e. TCN memory lenght), it is randomly cropped.
    \item Random noise. Gaussian noise is added to the skeleton coordinates to simulate inaccurate joint estimations.
    \item Random rotation noise. The whole action sequence is rotated randomly over the 3D axes. This rotation is limited to low angles, to simulate just subtle variations in the recording view-point. 
\end{itemize}




\subsection{Cross-domain classification}\label{sec:xdom}

Cross-domain hand action classification aims to recognize action sequences whose category and recording camera view-point were not present in the training data.  To obtain view-point agnostic motion representation, our motion representation model  is trained to learn an embedding space where descriptors belonging to the same action category must be close to each other, and far away from other category descriptors.
This is achieved optimizing 
the \textit{normalized temperature-scaled cross-entropy loss} (\textbf{NT-Xent}) \cite{chen2020simple}:


\noindent which is computed in each training iteration for each pair of actions  and  that belong to the same action category. NT-Xent maximizes the cosine similarity of both embedded actions  and  and minimizes their similarity to the descriptors related to different action classes.  is a temperature parameter.



The training of our motion representation model is performed with the same batch construction and data augmentation techniques described in Section \ref{sec:intra_dom}. We added an extra data augmentation step that rotates randomly all the action sequences of the mini-batch. This batch augmentation is crucial to boost the performance achieved with the NT-Xent loss in different domains and camera view-points. 



Once this generic motion representation model has been trained, on a given source domain, we use a N-shot approach~\cite{wang2019simpleshot} and generate motion sequence descriptors for a small set of N reference actions from a different target domain, with no specific training on the latter. To perform action classification in this new domain, 
we use a simple K-Nearest Neighbors classifier (KNN) to assign a label to new sequences depending on their descriptor distance to the descriptors of the reference examples of the target domain. To improve the performance of the KNN, 
we extend our reference action set by applying the same data augmentation strategies described in Section \ref{sec:intra_dom}, and we compute descriptors for all the new augmented sequences. 












\section{Experiments}
This section details the different datasets used to evaluate our framework and its implementation details.
Then, we expose the main framework design choices and evaluate its performance in different scenarios.
We evaluate our cross-domain classification, both for action sequences (using summarization module) and for long motion sequences (without summarization module, i.e., per-frame classification).
Finally, we evaluate our intra-domain classification of action sequences from well-known benchmarks.



\subsection{Experimental setup}
\subsubsection{Datasets} The validation of the presented approach has been run using different datasets (see frame samples in Fig. \ref{fig:frame_samples}), covering a variety of application domains as well as different camera view-points: 


\paragraph*{SHREC-17 \cite{de2017shrec}} it contains action sequences (22-joint hand skeletons) related to human-machine interaction domains recorded from a \textbf{frontal third-person view}. The data is categorized with two levels of granularity, presenting 14 and 28 actions respectively. The dataset contains 1960 action sequences for training and 840 sequences for validation. 
Actions are performed by 28 different users.

\paragraph*{F-PHAB \cite{garcia2018first}} it contains action sequences (21-joint hand skeletons) recorded from an \textbf{egocentric view} related to kitchen, office and social scenarios, which involve the interaction with different objects. 
Actions have been performed by 6 different users and labeled with 45 action categories.
The dataset consists of 1175 sequences and proposes the following data splits for train and test:
\begin{itemize}
    \item \textbf{1:3, 1:1, 3:1.} Stand for splitting the dataset according to different training/testing ratios at a sequence level.
    \item \textbf{cross-person}. Stand for a 6-fold leave-one-out cross-validation at a user level.
\end{itemize}
Only the original cross-subject and 1:1 splits are available, for the other two data partitions we create 3 random data folds to perform 3-fold cross-validation.

\paragraph*{MSRA \cite{sun2015cascaded}} it contains recordings (17-joint hand skeletons) of 17 different American Sign Language gestures performed by 9 different users. Each gesture sequence has a length of 500 frames recorded from a \textbf{third-person view}.
For the classification of this data, we use the action samples from the two first subjects as reference, leaving the remaining 7 ones as the target samples, as suggested in \cite{liu20203d}. 


\begin{figure}[!bt]
\centering
\begin{subfigure}[b]{\linewidth} 
    \includegraphics[width=0.32\linewidth]{figures/data_frames/shrec/01_14.png}
    \includegraphics[width=0.32\linewidth]{figures/data_frames/shrec/03_14.png}
    \includegraphics[width=0.32\linewidth]{figures/data_frames/shrec/05_14.png}
    \caption{SHREC-17 depth sample frames}
\end{subfigure}
\begin{subfigure}[b]{\linewidth} 
    \includegraphics[width=0.32\linewidth]{figures/data_frames/fphab/clean_glasses.jpeg}
    \includegraphics[width=0.32\linewidth]{figures/data_frames/fphab/handshake.jpeg}
    \includegraphics[width=0.32\linewidth]{figures/data_frames/fphab/pour_juice.jpeg}
    \caption{F-PHAB RGB sample frames}
\end{subfigure}
\begin{subfigure}[b]{\linewidth} 
    \includegraphics[width=0.32\linewidth]{figures/data_frames/msra/IP.jpg}
    \includegraphics[width=0.32\linewidth]{figures/data_frames/msra/RP.jpg}
    \includegraphics[width=0.32\linewidth]{figures/data_frames/msra/three.jpg}
    \caption{MSRA depth sample frames with skeleton joints}
\end{subfigure}
    \caption{Sample frames from the different evaluated data domains. (a) SHREC-17 dataset. Examples of actions \textit{grab}, \textit{expand} and \textit{rotation clockwise}. (b) F-PHAB dataset. Examples of actions \textit{clean glasses}, \textit{handshaking} and \textit{pour juice}. (c) MSRA dataset. Examples of the signs \textit{IP}, \textit{RP} and \textit{three}.}
    \label{fig:frame_samples}
\end{figure}






\subsubsection{Implementation and training details}
\paragraph*{Hand skeleton}
Since each dataset used provides different skeleton joints format, we use the 20 joints that SHREC-17 and F-PHAB have in common, and our proposed 7-joint skeletons representation, described in Section \ref{sec:hand_modeling}, suitable for the three datasets considered.

\paragraph*{Motion representation architecture}
the backbone of our motion representation model is a TCN consisting on two stacks of residual blocks with dilations of 1, 2 and 4 for the layers within each block,
and convolutional filters of size 4, making a memory length of 32 frames long. Note that, since the feature pre-processing filters out 2 out of 3 consecutive frames, this memory length is extended to 96 real frames.
Our backbone uses 256 filters in each convolutional layer, which generates action sequence embeddings with a size of 256. The summarization module reduces their dimensionality to 64 with a single 1D convolutional layer and then a single perceptron layer of size 32 generate the final descriptor weights. 
When the sequence summarization module is not used, the descriptor generated by the TCN at the last action time-step is the one used for the action representation (\textit{Last TCN descriptor}). 
This model has been optimized using Eq. \ref{eq:loss} with .

\paragraph*{KNN classifier}
Our KNN classifier weights pairs of target-reference descriptors according to the inverse of their distance.
We validate the use of different number of neighbors,  i.e. 1, 3, 5, 7, 9, 11, and we report the results of the neighbor that optimizes the final classification accuracy. Additionally, the reference augmentation step increases the reference descriptors set randomly up to 40 times.


















\subsection{General purpose framework design evaluation}

This subsection analyzes and validates the main components of our framework using the cross-domain approach of Section \ref{sec:xdom} since this setup is more demanding in terms of generalization capabilities.
We train our base motion representation model on the front view SHREC-17 dataset and then we validate it on the egocentric F-PHAB target. 
To analyze the effect of different design choices, we start using the descriptors generated at the last time-step of the input sequences for the final motion representation (no summarization). These are the descriptors that are later evaluated by the KNN classifier.







Table \ref{tab:skel_size} shows the benefits of using our proposed hand skeleton simplification. Using this 7-joint skeleton format we improve the generalization for all the different data splits.

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Skeleton size} & \textbf{1:3} & \textbf{1:1} & \textbf{3:1} & \textbf{cross-person} \\ \hline
20 joints & 63.8 & 69.9 & 69.8 & 51.4 \\ \hline
7 joints & 66.3 & 71.0 & 73.8 & 53.5 \\ \hline
\end{tabular}
\caption{\textbf{Influence of the number of skeleton joints} in the hand representation. Motion representation model trained on SHREC. Action recognition accuracy validated on F-PHAB. }
\label{tab:skel_size}
\end{table}


Additionally, as seen in Table \ref{tab:train_labels}, using the 28 SHREC classes to discriminate the action sequences while training the motion representation model provides more fine-grained information than the 14-labels format. Results show that by using more specific labels we are able to get a better representation of the action, improving the final performance.

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{SHREC categories} & \textbf{1:3} & \textbf{1:1} & \textbf{3:1} & \textbf{cross-person} \\ \hline
14 & 58.3 & 65.4 & 65.9 & 48.9 \\ \hline
28 & 66.3 & 71.0 & 73.8 & 53.5 \\ \hline
\end{tabular}
\caption{\textbf{Influence of the training categories.} Motion representation model trained on SHREC. Action recognition accuracy evaluated on F-PHAB.}
\label{tab:train_labels}
\end{table}



We now analyze the effect of the motion summarization module (see Table \ref{tab:summ_comp}). 
Results show how the learned summarization module improves the classification accuracy with respect to the last descriptor of our base TCN backbone.
Moreover, extending the reference action set with random data augmentation increases the accuracy in all the data splits by a noticeable margin.
These results also indicate that our framework performs good even when not many reference actions are available (splits 1:3). However, we still find an accuracy drop when generalizing to actions of users not available on the action reference set (cross-person splits). This is due to the high inter-subject action variability of the F-PHAB dataset, and the fact that no data from the target dataset has been involved to train our representation model.

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Action descriptor} & \textbf{1:3} & \textbf{1:1} & \textbf{3:1} & \textbf{cross-person} \\ \hline
\textbf{Last TCN descriptor} & 66.3 & 71.0 & 73.8 & 53.5 \\ \hline
\textbf{Summarization} & 70.6 & 75.5 & 77.7 & 58.4 \\ \hline
\textbf{Summarization*} & 76.2 & 79.7 & 82.0 & 62.7 \\ \hline
\multicolumn{5}{l}{\footnotesize  includes reference actions data augmentation} \\
\end{tabular}
\caption{\textbf{Influence of the action sequence summarization technique.} Motion representation model trained on SHREC. Action recognition accuracy validated on F-PHAB. \textit{TCN descriptor} refers to the descriptor generated by the TCN at the last time-step. \textit{Summarization} refers to descriptor generated by our summarization module for the final action representation.}
\label{tab:summ_comp}
\end{table}


Figure \ref{fig:att_weights} shows the weights learned by our summarization module on the F-PHAB validation split (1:1). Initial time-steps are the less relevant for the final action summarization, while weights tend to increase with time. However, they do not  exhibit a continuous growth along time, probably due to the fact that contiguous time descriptors contain similar information. Although final descriptors may encode information about the whole action, they may also encode motion not related with the action itself. Therefore, they are not always very relevant for the final action representation. 




\begin{figure}[ht]
    \centering
\includegraphics[width=0.47\textwidth]{figures/attention_weights_fphab_v2.png}
    \caption{Weights generated by our summarization module for each sample of the F-PHAB validation split (1:1).
The red line in front shows the average of all the generated weights.
    Sampled frames belong to the action \textit{pour liquid soap}.
Initial descriptors lack of relevance for not containing previous motion information. Ending frames gain relevance by encoding all the previous information. Often the last frames encode motion out of the action scope, reducing their relevance gain.
    }
    \label{fig:att_weights}
\end{figure}




\subsection{Cross-domain action classification}
\label{sec:fphab_bench}

This experiment evaluates the cross-domain generalization of our framework by classifying action sequences from action categories and camera view-points not seen in the training data. 
For this experiment, we train our motion representation model as defined in the Section \ref{sec:xdom} only on the front view SHREC-17 dataset (28 labels), and we measure its classification accuracy on the egocentric F-PHAB dataset. Results from our framework correspond to the processing of 7-joint skeletons and the use of our proposed action summarization module.


\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{1:3} & \textbf{1:1} & \textbf{3:1} & \textbf{cross-person} \\ \hline
\textbf{RGB \cite{feichtenhofer2016convolutional}} & -- & 75.3 & -- & -- \\ \hline
\textbf{Depth \cite{oreifej2013hon4d}} & -- & 70.61 & -- & -- \\ \hline
\textbf{LSTM \cite{zhu2016co}} & 58.75 & 78.73 & \textbf{84.82} & 62.06 \\ \hline
\textbf{DD-Net \cite{yang2019make}} & 75.09 & 81.56 & 88.26 & \textbf{71.8} \\ \hline \textbf{Gram Matrix \cite{zhang2016efficient}} & -- & 85.39 & -- & -- \\ \hline
\textbf{Two-stream NN \cite{li2021two}} & -- & \textbf{90.26} & -- & -- \\ \hline\hline
\textbf{DD-Net \cite{yang2019make}} & 59.6 & 63.7 & 67.5 & 51.2 \\ \hline
\textbf{Ours} & 70.6 & 75.5 & 77.7 & 58.4 \\ \hline
\textbf{Ours*} & \textbf{76.2} & 79.7 & 82.0 & 62.7 \\ \hline
\multicolumn{5}{l}{\footnotesize  includes reference actions data augmentation} \\
\end{tabular}
\caption{F-PHAB accuracy comparison with state-of-the-art. Upper block shows methods trained on the F-PHAB dataset (intra-domain classification). Bottom block shows the methods trained on SHREC-17 dataset (cross-domain classification).}
\label{tab:phab_ablation}
\end{table}


Table \ref{tab:phab_ablation} summarizes the accuracy of the best performing methods on the F-PHAB dataset which have been trained as an intra-domain problem (upper block), and the results of our cross-domain approach (bottom block). The later include the evaluation of DD-Net \cite{yang2019make}, one of the best performing methods on the SHREC-17 classification benchmark. We used the available public code to  train the DD-Net with the SHREC-17 dataset (20-joint skeletons) as the authors state, extracting F-PHAB descriptors from its backbone and classifying them with our N-shot approach. 
Results from the DD-Net cross-domain classification show a lack of domain adaptation, showing that our method clearly outperforms other methods in this scenario.

The results show that our approach clearly outperforms the  RGB \cite{feichtenhofer2016convolutional} and depth-based \cite{oreifej2013hon4d} models trained on the test domain.
We also get better or comparable results than a regular LSTM network \cite{zhu2016co}, specially when not many reference actions are available (1:3 split) or when not all the subjects have been present in the reference split (cross-person splits). 
Although our cross-domain performance is behind the best intra-domain classification model \cite{zhang2016efficient}, we show later in Section \ref{sec:intra-dom-res} that we outperform them when training in the same domain.  Remember that no specific training with the F-PHAB data splits has been performed in these evaluations of our approach.




\subsection{Cross-domain classification of frames from long video sequences}

In this experiment we use the MSRA dataset, whose hand motion sequences are much longer than the memory of our representation model. This will allow us to illustrate two characteristics of our method. First, that the features learned by the TCN are robust across domains thanks to our motion summarization module (see Table \ref{tab:summ_comp}). Second, that they can be used without summarization for long sequences that exceed the memory of the model.

To do so, we used the same model trained in section \ref{sec:fphab_bench} and evaluate its cross-domain performance in the MSRA dataset. Since sequences are too long for our summarization, we  perform the KNN classification of all the motion descriptors generated by our TCN at each time-step (denoted as \textit{online action classification}). We also report the average of the class probabilities of the frames within a video sequence for comparison with previous works (denoted as \textit{video classification}). For computational reasons, we randomly select just 8000 reference descriptors for the KNN evaluation. 














Table \ref{tab:msra_bench} shows that, even though MSRA motion sequences do not correspond to the kind of actions seen in the training data, our online KNN classification is able to properly classify 85.8\% of the validation frames. Moreover, a simple average of the predicted frame probabilities results in a 97.1\% accuracy, similar to current state-of-the-art results specifically trained on the MSRA dataset.
In this case, reference action data augmentation does not provide an edge, probably because the MSRA motion sequences already contain enough variations of hand poses.

\begin{table}[ht]
    \centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Online classification} & \multicolumn{1}{l|}{\textbf{Video classification}} \\ \hline
\textbf{3D PostureNet \cite{liu20203d}} & -- & 98.56 \\ \hline
\textbf{Ours} & 85.8 & 97.1 \\ \hline
\textbf{Ours*} & 86.7 & 97.1 \\ \hline
\multicolumn{3}{l}{\footnotesize  includes reference actions data augmentation} \\
\end{tabular}
    \caption{MSRA accuracy comparison. Batched vs. online predictions. Our results correspond to our motion representation model trained on SHREC-17 data (cross-domain).}
    \label{tab:msra_bench}
\end{table}






\subsection{Intra-domain classification and reference actions study}
\label{sec:intra-dom-res}
Our final experiment evaluates the performance of our method for intra-domain action classification using the linear classifier described in Section \ref{sec:intra_dom} and training and evaluating actions in the same dataset. 

\subsubsection{SHREC-17 evaluation}

Table \ref{tab:shrec_bench} shows the classification accuracy of our framework trained and evaluated on the SHREC-17 dataset.
Note that we are using just 7 out of the 22 original skeleton joints, that helps generalization to other datasets but it might lose domain-specific information. Still, our approach is able to get comparable results to the best performing methods for the benchmark.


\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Model} & \textbf{SHREC 14} & \textbf{SHREC 28} \\ \hline
\textbf{DD-Net \cite{yang2019make}} & 94.6 & 91.9 \\ \hline
\textbf{Two-stream NN \cite{li2021two}} & 96.31 & 94.05 \\ \hline
\textbf{Ours} & 93.57 & 91.43 \\ \hline
\end{tabular}
\caption{Intra-domain classification on SHREC-17 data splits.}
\label{tab:shrec_bench}
\end{table}




\subsubsection{F-PHAB evaluation}

Table \ref{tab:fphab_bench} shows the classification accuracy of our framework trained and evaluated on the F-PHAB dataset. For the DD-net results, we used the available original code and follow the paper to train on the F-PHAB dataset. 
Our intra-classification results outperform the current state-of-the-art in all the splits.
Note that, even when less training data is used (1:3 split), we achieve comparable accuracy to the data splits with larger training sets. 



\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{1:3} & \textbf{1:1} & \textbf{3:1} & \textbf{cross-person} \\ \hline
\textbf{DD-Net \cite{yang2019make}} & 75.09 & 81.56 & 88.26 & 71.8 \\ \hline \textbf{Two-stream NN \cite{li2021two}} & -- & 90.26 & -- & -- \\ \hline \textbf{Ours} & \textbf{92.90} & \textbf{95.93} & \textbf{96.76} & \textbf{88.70} \\ \hline
\end{tabular}
\caption{Intra-domain classification on F-PHAB data splits. only best methods from Table  \ref{tab:phab_ablation} are shown.}
\label{tab:fphab_bench}
\end{table}






















\section{Conclusions}

The present work introduces a complete solution for hand action recognition, that has been designed to work for different action domains and recording view-points.
Our framework processes skeleton hand action sequences by first simplifying the skeleton representation, calculating specific hand pose features, and then, our motion representation model encodes them into single descriptors.
This motion representation model is based on a Temporal Convolutional Network that generates sets of descriptors to describe the input motion up to each time-step. Then, a simple motion summarization module weights the descriptors, according to their relevance, ending up with a final action representation.
Finally, we demonstrate the performance of our motion representation model for different classification purposes.
First, we evaluate its classification accuracy when being trained for specific action domains, obtaining better or similar results than different state-of-the-art methods in well-known benchmarks.
Second, we demonstrate how our motion representation model generalizes to different unseen target domains and camera view-points (cross-domain). Here, with no specific training on the target domains, we get comparable results to best the methods that do train for those specific domains.



{
\bibliographystyle{IEEEtran}
\bibliography{biblio}
}






\end{document}
