\newcommand*{\RELEASE}{}  \newcommand*{\NOCOPYRIGHT}{}  

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmicx}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\graphicspath{{fig/}{../fig/}}

\usepackage[utf8]{inputenc}
\usepackage{paralist}  \usepackage{balance}
\usepackage{xspace}  \usepackage{soul}  \usepackage{setspace}  \usepackage{url}
\usepackage[bottom]{footmisc}  

\usepackage{booktabs}  \usepackage{multirow}  \usepackage{stfloats}  

\usepackage{subfigure}  \usepackage{wrapfig}  \usepackage{floatflt}  

\usepackage{algorithm}
\usepackage{algpseudocode}  

\usepackage[table]{xcolor}  



\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}



\ifdefined\RELEASE  \newcommand{\al}[1]{} \newcommand{\add}[1]{#1}  \newcommand{\refine}[2]{#1}
	\newcommand{\del}[1]{}  

	\newcommand{\old}[1]{}  \newcommand{\objective}[1]{}  

\newcommand{\dq}[1]{}
	\newcommand{\jgrm}[1]{}

	\newcommand{\pcm}[1]{}
\else
	\newcommand{\al}[1]{\textcolor{green(pigment)}{[AL: #1]}} \newcommand{\add}[1]{\textcolor{blue}{#1}}  \newcommand{\refine}[2]{\textcolor{violet}{#1}\textcolor{teal}{[#2]}}
	\newcommand{\del}[1]{\textcolor{blue}{\st{#1}}}  

	\newcommand{\old}[1]{\textcolor{gray}{#1}}  \newcommand{\objective}[1]{\textcolor{cyan}{[Objective: #1]\\}}  

\newcommand{\dq}[1]{\textcolor{brown}{(*** DQ: #1 ***)}}
	\newcommand{\jgrm}[1]{\textcolor{brown}{\sout{#1}}}

	\newcommand{\pcm}[1]{\textcolor{red}{[PCM: #1]}}
\fi

\newcommand{\anm}{ANONYMIZED\xspace}
\ifdefined\ANONYMOUS
	\newcommand{\urlSys}{\anm} \newcommand{\urlDaoc}{\anm} \newcommand{\urlGembev}{\anm}  \newcommand{\xms}{ANONYM_APP\xspace}  \newcommand{\urlXms}{\anm} \newcommand{\urlBmark}{\anm}  \newcommand{\urlLfr}{\anm} \newcommand{\citeStx}{\anm} \else
	\newcommand{\urlSys}{https://github.com/eXascaleInfolab/daor}
	\newcommand{\urlDaoc}{https://github.com/eXascaleInfolab/daoc}
	\newcommand{\urlGembev}{\url{https://github.com/eXascaleInfolab/GraphEmbEval}\xspace}  \newcommand{\xms}{xmeasures\xspace}  \newcommand{\urlXms}{\url{https://github.com/eXascaleInfolab/xmeasures}\xspace}
\newcommand{\urlBmark}{\url{https://github.com/eXascaleInfolab/clubmark}\xspace}  \newcommand{\urlLfr}{\url{https://github.com/eXascaleInfolab/LFR-Benchmark_UndirWeightOvp}}
	\newcommand{\citeStx}{StaTIX~\cite{Stx18}} \fi

\newcommand{\sys}{DAOR\xspace}
\newcommand{\bmark}{Clubmark\xspace}

\newtheorem{definition}{Definition}  \newtheorem{example}{Example}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}


\begin{document}

\setstretch{0.958}  

\title{Bridging the Gap between Community\\ and Node Representations:\\ Graph Embedding via Community Detection
\thanks{This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement 683253/GraphInt).}
}

\ifdefined\ANONYMOUS
\author{\IEEEauthorblockN{\anm}
\\
\\
}
\else
\author{\IEEEauthorblockN{Artem Lutov}  \IEEEauthorblockA{\textit{eXascale Infolab}\\
\textit{University of Fribourg}\\
Switzerland\\
artem.lutov@unifr.ch}
\and
\IEEEauthorblockN{Dingqi Yang}  \IEEEauthorblockA{\textit{eXascale Infolab}\\
\textit{University of Fribourg}\\
Switzerland\\
dingqi.yang@unifr.ch}
\and
\IEEEauthorblockN{Philippe Cudr{\'e}-Mauroux}  \IEEEauthorblockA{\textit{eXascale Infolab}\\
\textit{University of Fribourg}\\
Switzerland\\
pcm@unifr.ch}
}
\fi

\maketitle

\begin{abstract}
Graph embedding has become a key component of many data mining and analysis systems.
Current graph embedding approaches either sample a large number of node pairs from a graph to learn node embeddings via stochastic optimization or factorize a high-order node proximity/adjacency matrix via computationally intensive matrix factorization techniques.
These approaches typically require significant resources for
the learning process and rely on multiple parameters, which limits their applicability in practice.
Moreover, most of the existing graph embedding techniques operate effectively in one specific metric space only (e.g., the one produced with cosine similarity), do not preserve higher-order structural features of the input graph and cannot automatically determine a meaningful number of dimensions for the embedding space. Typically, the produced embeddings are not easily interpretable, which complicates further analyses and limits their applicability.
To address these issues, we propose \sys, a highly efficient and parameter-free graph embedding technique producing metric space-robust, compact and interpretable embeddings without any manual tuning. Compared to a dozen state-of-the-art graph embedding algorithms, \sys yields competitive results on both node classification (which benefits form high-order proximity) and link prediction (which relies on low-order proximity mostly). Unlike existing techniques, however, \sys does not require any parameter tuning and improves the embeddings generation speed by several orders of magnitude.
Our approach has hence the ambition to greatly simplify and speed up data analysis tasks involving graph representation learning.
\end{abstract}

\begin{IEEEkeywords}
parameter-free graph embedding, unsupervised learning of network representation, automatic feature extraction, interpretable embeddings, scalable graph embedding.  \end{IEEEkeywords}

\ifdefined\NOCOPYRIGHT
\else
	\blfootnote{978-1-7281-0858-2/19/\k\bullet\bullet\bulletkkd\mathcal{G}c_i\mathcal{G}n\mathcal{G}m\mathcal{G}k\mathcal{G}w\mathcal{G}w_i\dot{w}_ic_i\dot{w}_i = w_{c_i}\Delta Q_{i,j}\gammass \le kdd = |D|, d \le sQw_{i,j}w_iwC_i\delta(C_i, C_j)1C_i = C_j0\Delta Q\gamma\acute{\gamma}\check{w}\gamma = 1\gamma \in [0, 1)\Delta Qdnk=5+2d=4ddds \ge dsk \ge shierscls \gets []clsts \gets \{\}lev \in hiercl \in levres \gets \texttt{count}(cl.ances) = 0dens \gets cl.weight / \texttt{count}(cl.nodes)savdens \gets 0; savwgh \gets 0reshits \gets 0ac \in cl.ancesast \gets clsts[ac]savdens < ast.denssavwghsavwgh > ast.wghsavdens \gets ast.denssavwgh \gets ast.wghresdens \ge ast.denscl.weight \le ast.wghhits \gets hits + 1ast.reqs \gets ast.reqs + 1ast.reqs = \texttt{count}(ac.des)clsts.erase(cl)reshits = \textbf{count}( cl.ances)res \getssavwgh \gets cl.weight * wrstepclsts[cl] \gets (savdens, savwgh)resscls.add(cl)sclsr_wr_w \in [0.5, 1)r_w \rightarrow r_{w_{min}} = 0.5w_{C_{ovp}} < w_CCr_wr_w = 0.5 + \frac{(b - 1) \times w_{c_{ovp}}}{2\, b \times w_c}b \ge 2Cr_{w} = 0.5 \times 1.2 = 0.6v_i \in Vd = |D|D_jw_{i,D_j}V = D^Tdt \le d \le sd - ts - tdt > dt - (d-1)d - 1z < twt - zd - 1 - (t - z)w_i \ge \sqrt{w}td \ge ttd\gamma = 1\gammat \le d\gamma\alpha\gamma = (1 - \alpha)/\alphaO(n\sqrt{n}) \times O_0O(n^2) \times O_0nO_0\gamma\gamma \gg 1\gamma > 0\gamma\check{\dot{m}}m\gamma\check{\dot{m}}m\tilde{k}_{max} = \sqrt{n}n^{3/2}\breve{w}_{min}\gamma\gamma_{min} = \acute{\gamma}\acute{\gamma} \ge 1\gamma_{min} = 0.5\, .. \,1\acute{\gamma}\gamma\gamma_{max}\gamma_{min}r_\gammar_\gamma \rightarrow 1r_\gamma \rightarrow 0r_\gammar_\gamma > 2^{-2} = 0.25r_\gammar_\gamma \gtrsim r_{w_{min}}^2 = 0.36r_\gamma10-20\%r_\gamma \le 
1.1^{-2} = 0.826r_\gamma \in [0.36, 0.826]r_\gamma = 0.6r_\gammar_\gamma\Delta Q \ge 0i|h_i|dd\Delta Q \max \Delta Q < 0d \ne 0\Delta Qnodes, drg \gets 0.6gamma \gets \sqrt[3]{\texttt{weight}(nodes)/\texttt{min\_weightln}(nodes)}/4gmin \gets 0.5hier \gets []nodescls \gets \texttt{daocHierLev}(nodes, gamma, d)gamma * (rg + 1) / 2 \ge gmincls\texttt{count}(cls) \le \sqrt{\texttt{count}(nodes)}gamma \gets gamma * rgnodes \gets clsclshier.append(cls)gmin \gets \texttt{gammaOptim}(cls)d\texttt{count}(cls) > dnodes \gets clsnodes \gets []hier\bullet\bullet\bullet\bullet\bulletpqp,q \in \{0.25,0.05,1,2,4\}\alphak\{1,2,3,4,5,6\}d/kk-1\lceil d/k \rceild-(k-1)\lceil d/k \rceil\beta0.10.90.2T\{1,10\}\{1,2,3\}k6\alpha0.00011\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot$  the algorithm meta parameters are \emph{tuned} once for all datasets to maximize accuracy\\
:  the algorithm parameters are \emph{tuned for each dataset} to maximize accuracy\\
x  the algorithm is crashed on coarsening small disconnected components
}
\end{flushleft}
\vspace{-4pt}
\end{table*}
Table~\ref{res_link} shows the results of the link prediction task. Our proposed method, \sys, is among the top-3 best-performing techniques being unsupervised and parameter-free. The impact of our proposed multi-resolution clustering is especially visible on this task, were \sys significantly outperforms DAOC. 


\subsection{Robustness to the Metric Space}

Node embedding robustness to different metric spaces is shown in Table~\ref{tbl:metric} on the node classification task for our method \sys versus the two other best-performing methods from Table~\ref{tbl:classifkern} that technically can be evaluated with another metric space (NodeSketch is omitted because it uses a non-linear Hamming metric space, where cosine distance cannot be formally evaluated). For \emph{all} input graphs, \sys shows the best worst-case performance, i.e.,  \sys yields much more accurate results in its least accurate non-primary metric space than the other methods do. Moreover, Hamming distance is directly applicable to \sys without any preliminary binarization, unlike the algorithms operating in the cosine metric space.

\begin{table}[htbp]\small \vspace{-4pt}
\caption{Node embedding robustness to the metric space, where the native metric space for each algorithm is highlighted in bold}
\label{tbl:metric}
\centering
\vspace{-4pt}
\begin{tabular}{@{}ll|llll@{}}
\hline
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{4}{c}{\textbf{Micro-F1 (\%)}}                   \\ \cline{3-6}
&                                  & \textbf{Blog} & \textbf{PPI} & \textbf{Wiki} & \textbf{DBLP} \\ \hline
\multirow{4}{*}{DAOR}            & \textbf{jaccard}                 & 33.05         & 19.07        & 53.24         & 87.86         \\
                                 & cosine                           & 30.41         & 13.62        & 47.20         & 87.42         \\
                                 & hamming                          & \textit{23.87}         & 14.13        & 45.52         & \textit{86.98}         \\
                                 & binham                          & 23.89         & \textit{10.25}        & \textit{41.36}         & 87.17         \\ \hline
\multirow{3}{*}{NetMF}           & \textbf{cosine}                  & 40.04         & 15.03        & 57.62         & 93.59         \\
                                 & hamming                          & 17.54         & 6.55         & 40.93         & 70.32         \\
                                 & binham                          & 19.82         & 7.05         & 42.85         & 74.91         \\ \hline
\multirow{3}{*}{HOPE}           & \textbf{cosine}                  & 31.37          & 14.69          & 56.68 		 & 91.47         \\
                                 & hamming                          & 17.02         & 5.95         & 40.89         & 59.94         \\
                                 & binham                          & 18.23         & 6.21         & 40.89         & 74.36         \\ \hline
\end{tabular}
\begin{flushleft}
\footnotesize{
binham  - Hamming metric applied after a binarization of each dimension using its median value
}
\end{flushleft}
\vspace{-4pt}
\end{table}







\subsection{Runtime Performance}

In this experiment, we investigate the efficiency of the graph embedding learning process. Our evaluation was performed using an open-source graph embeddings evaluation framework, GraphEmbEval\footref{ftn:gembev}. on a Linux Ubuntu 16.04.3 LTS server with an Intel Xeon CPU E5-2620 v4 @ 2.10GHz CPU (16 physical cores) and 132 GB RAM. The training and execution termination constraints for each algorithm were set to 64 GB of RAM and 240 hours CPU time (we terminate the process when either of those thresholds are met).

\begin{table}[htbp]\small \vspace{-4pt}
\caption{Node embedding learning time (in seconds), where the top 3 results for each dataset are highlighted in bold
}
\label{res_runtime_embs}
{
\vspace{-4pt}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{l|rrrrr}\hline
\textbf{Method}           & \textbf{Blog} & \textbf{PPI}  & \textbf{Wiki} & \textbf{DBLP}  & \textbf{YouTube} \\ \hline DeepWalk          & 3375 & 1273 & 1369 & 4665  & 747060                                                        \\
Node2Vec          & 1073 & 383  & 1265 & 504   & -                                                             \\
LINE              & 2233 & 2153 & 1879 & 2508  & 29403                                                       \\
VERSE             & 1095 & 203  & 276  & 1096  & 245334                                                       \\ \hline  GraRep            & 3364 & 323  & 422  & 10582 & -                                                            \\
HOPE              & 239  & 100  & 78   & 283   & 15517                                                        \\
NetMF             & 487  & 124  & 708  & 213   & -                                                         \\   \hline  INH-MF            & 509  & 39   & 98   & 378   & -                                                              \\ 
NetHash           & 721  & 201  & 134  & 35    & 12708                                                         \\
NodeSketch        & \textbf{70}   & \textbf{8}    & \textbf{17}   & \textbf{8}     & \textbf{2439}               \\  \hline  HARP-DWalk              & 1436   & 299    & 483   & 958     & 336200                                                            \\
HARP-LINE              & 1274   & 106    & 189   & 90     & 13951                                                  \\  \hline   DAOC              & \textbf{7.9}   & \textbf{0.3}    & \textbf{1.8}   & \textbf{0.2}     & \textbf{4893}                                                            \\
DAOR              & \textbf{1.6}   & \textbf{0.2}    & \textbf{0.4}   & \textbf{0.2}     & \textbf{57.1}                                                            \\ \hline
\end{tabular}}
\begin{flushleft}
\footnotesize{
-  the algorithm was terminated by timeout
}
\end{flushleft}
\vspace{-4pt}
\end{table}

Table \ref{res_runtime_embs} shows the end-to-end embedding learning time. To discount the impact of the multi-threaded implementation of some of the methods, we dedicate a single logical CPU per each method implementation and report the total CPU time. Our method, \sys, is faster than existing state-of-the-art techniques by several orders of magnitude; it exhibits near-linear scaling when increasing the number of links in the graph.
\sys is also much more scalable than DAOC (on which \sys is built) due to its specific multi-scaling approach that boosts the number of nodes reaching consensus of the optimization function early on lower levels of the hierarchy. Moreover, unlike other graph embedding techniques, \sys execution time decreases when increasing the number of embedding dimensions, which is due to the early termination of the clustering as described in Section~\ref{subsec:clsbound}. 








\section{Conclusions}


In this paper, we presented a novel highly efficient and parameter-free graph embedding technique, \sys\hspace{-4pt}\footref{ftn:sys}, which produces metric-robust and interpretable embeddings without requiring any manual tuning. Compared to a dozen state-of-the-art graph embedding algorithms, \sys yields competitive results on diverse graph analysis tasks (node classification and link prediction), while being several orders of magnitude more efficient. 


In future work, we plan to recommend the minimal, maximal and optimal number of embedding dimensions, and conduct a comprehensive study on their quality and interpretability. Also, we plan to integrate further state-of-the-art community detection algorithms in addition to DAOC.






\bibliographystyle{IEEEtran}\balance
\bibliography{IEEEabrv,./daor}


\end{document}
