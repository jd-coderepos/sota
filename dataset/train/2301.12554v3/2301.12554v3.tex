\documentclass[11pt, letterpaper]{article}

\usepackage[pagenumbers]{goodonecolumn}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}  \usepackage{enumitem}
\usepackage{pifont}			

\usepackage{amsmath}
\usepackage{amssymb}
\let\proof\relax			\let\endproof\relax
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

\usepackage{subcaption}     \usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}

\newcommand{\mcrot}[4]{\multicolumn{#1}{#2}{\rlap{\rotatebox{#3}{#4}~}}}
\usepackage{footnote}
\usepackage{chngpage}
\usepackage{placeins}
\usepackage{ragged2e}
\usepackage[numbers]{natbib}

\usepackage[capitalize]{cleveref}
\Crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\Crefname{table}{Tab.}{Tabs.}

\crefformat{equation}{\textup{#2(#1)#3}}
\crefrangeformat{equation}{\textup{#3(#1)#4--#5(#2)#6}}
\crefmultiformat{equation}{\textup{#2(#1)#3}}{ and \textup{#2(#1)#3}}
{, \textup{#2(#1)#3}}{, and \textup{#2(#1)#3}}
\crefrangemultiformat{equation}{\textup{#3(#1)#4--#5(#2)#6}}{ and \textup{#3(#1)#4--#5(#2)#6}}{, \textup{#3(#1)#4--#5(#2)#6}}{, and \textup{#3(#1)#4--#5(#2)#6}}

\Crefformat{equation}{#2Equation~\textup{(#1)}#3}
\Crefrangeformat{equation}{Equations~\textup{#3(#1)#4--#5(#2)#6}}
\Crefmultiformat{equation}{Equations~\textup{#2(#1)#3}}{ and \textup{#2(#1)#3}}
{, \textup{#2(#1)#3}}{, and \textup{#2(#1)#3}}
\Crefrangemultiformat{equation}{Equations~\textup{#3(#1)#4--#5(#2)#6}}{ and \textup{#3(#1)#4--#5(#2)#6}}{, \textup{#3(#1)#4--#5(#2)#6}}{, and \textup{#3(#1)#4--#5(#2)#6}}

\crefdefaultlabelformat{#2\textup{#1}#3}

\usepackage{enumitem}
\setlist[enumerate]{leftmargin=.5in}
\setlist[itemize]{leftmargin=.5in}

\newcommand{\creflastconjunction}{, and~}


\input{Math_Macros}
\input{Math_Macros_2}


\hypersetup{
  colorlinks,
  citecolor=Green,
  linkcolor=RoyalBlue,
  urlcolor=RoyalBlue}

\setlength{\parskip}{3pt}
\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
    \OLDthebibliography{#1}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{4.2pt}
}


\begin{document}


\title{Improving the Accuracy-Robustness Trade-Off\\of Classifiers via Adaptive Smoothing\thanks{This work is an extension of \citep{Bai23a}, and was supported by grants from ONR, NSF, and C3 AI.}}

\author{
Yatong Bai,Brendon G.\ Anderson,Aerin Kim,Somayeh Sojoudi\\
\textit{University of California, Berkeley Scale AI}\\
{\tt\small \{\href{mailto:<yatong_bai@berkeley.edu>?Subject=Your 2023 paper}{yatong\_bai}{}, \href{mailto:<bganderson@berkeley.edu>?Subject=Your 2023 paper}{bganderson}{}, \href{mailto:<sojoudi@berkeley.edu>?Subject=Your 2023 paper}{sojoudi}{}\}@berkeley.edu,  \tt\small \href{mailto:aerinykim@gmail.com>?Subject=Your 2023 paper}{aerinykim@gmail.com}{}}
}

\maketitle


\begin{abstract}
While prior research has proposed a plethora of methods that build neural classifiers robust against adversarial robustness, practitioners are still reluctant to adopt them due to their unacceptably severe clean accuracy penalties.
This paper significantly alleviates this accuracy-robustness trade-off by mixing the output probabilities of a standard classifier and a robust classifier, where the standard network is optimized for clean accuracy and is not robust in general.
We show that the robust base classifier's confidence difference for correct and incorrect examples is the key to this improvement.
In addition to providing intuitions and empirical evidence, we theoretically certify the robustness of the mixed classifier under realistic assumptions. 
Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness.
The proposed flexible method, termed ``adaptive smoothing'', can work in conjunction with existing or even future methods that improve clean accuracy, robustness, or adversary detection.
Our empirical evaluation considers strong attack methods including AutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves an 85.21\% clean accuracy while maintaining a 38.72\% -AutoAttacked () accuracy, becoming the second most robust method on the RobustBench CIFAR-100 benchmark as of submission, while improving the clean accuracy by ten percentage points compared with all listed models.
The code that implements our method is available at \url{https://github.com/Bai-YT/AdaptiveSmoothing}.
\end{abstract}



\section{Introduction} \label{sec:intro}

Neural networks are vulnerable to adversarial attacks in various applications, including computer vision and audio \citep{Moosavi-Dezfooli16, Goodfellow15}, natural language processing \citep{Fursov22}, and control systems \citep{Huang17}. Due to the widespread application of neural classifiers, ensuring their reliability in practice is paramount.

To mitigate this susceptibility, researchers have explored ``adversarial training'' (AT) \citep{Kurakin17, Goodfellow15, Bai22a, Bai22b, Zheng20}, an empirical technique that trains neural networks on adversarial examples to build robust models. Since its initial introduction, multiple improved variants of adversarial training have been devised to enhance the performance of robust classifiers.
On the theoretical side, researchers have also considered ensuring the certified robustness of neural classifiers, which mathematically guarantees a model's robustness to adversarial attacks \citep{Anderson20, Ma20, Anderson21a} within a radius. ``Randomized smoothing'' (RS) is one such method that seeks to achieve certified robustness of an existing model at inference time \citep{Cohen19, Li19}, with improved variants incorporating dimension reduction methods \citep{Pfrommer23} and denoising diffusion components \citep{Carlini22}, thereby certifying much larger radii. Recent work \citep{Anderson21b} has demonstrated that a locally biased smoothing approach can improve over traditional data-blind randomized smoothing. However, this method is limited to binary classification problems and suffers from the performance bottleneck of its underlying one-nearest-neighbor classifier.

Despite the emergence of these proposed remedies to the adversarial robustness issue, many practitioners are reluctant to adopt them. As a result, existing publicly available services are still vulnerable \citep{Ilyas18, Borkar21}, presenting severe safety risks. One important reason for this reluctance is the potential for significantly reduced model performance on clean data. Specifically, some previous works have suggested a fundamental trade-off between accuracy and robustness \citep{Tsipas19, Zhang19}. Since the sacrifice in unattacked performance is understandably unacceptable in real-world scenarios, developing robust classifiers with minimal clean accuracy degradation is crucial.

Fortunately, recent research has argued that it should be possible to simultaneously achieve robustness and accuracy on benchmark datasets \citep{Yang20}. To this end, variants of adversarial training that improve the accuracy-robustness trade-off have been proposed, including TRADES \citep{Zhang19}, Interpolated Adversarial Training \citep{Lamb19}, and many others \citep{TBai21, Raghunathan20, Wang19a, Wang19b, Tramer18, Balaji19}. However, despite these improvements, degraded clean accuracy is often an inevitable price of achieving robustness. Moreover, standard non-robust models often achieve enormous performance gains by pre-training on larger datasets. In contrast, the effect of pre-training on robust classifiers is less understood and may be less prominent \citep{Chen20, Fan21}. As a result, the performance gap between these existing works and the possibility guaranteed in \citep{Yang20} is still huge.

This work builds upon locally biased smoothing \citep{Anderson21b} and makes a theoretically disciplined step towards reconciling adversarial robustness and clean accuracy, significantly closing this performance gap and thereby providing practitioners additional incentives for deploying robust models. We summarize our contributions below.
\begin{itemize}[leftmargin=6mm]
	\setlength\itemsep{1pt}
	\item In \Cref{sec:STD+ROB}, under the observation that the -nearest-neighbor (-NN) classifier, a crucial component of locally biased smoothing, becomes a bottleneck of the overall performance, we replace the -NN classifier with a robust neural network that can be obtained via various existing methods, and modify the smoothing formulation accordingly. The resulting formulation \cref{eq:adap_sm_4} is a convex combination of the output probabilities of a standard neural network and a robust neural network. When the robust neural network has a certified Lipschitz constant or is based on randomized smoothing, the combined classifier also has a certified robust radius. This section, along with the corresponding experiment results in \Cref{sec:certified_exp} and \ref{sec:more_compare_R}, are also presented in our conference submission \citep{Bai23a}.
	\item In \Cref{sec:ada_smo}, we propose to adaptively adjust the mixture of a standard model and a robust model by adopting a type of adversary detector as a ``mixing network''. The mixing network adjusts the convex combination of the output probabilities from the two base networks, further improving the accuracy-robustness trade-off. We then empirically verify the robustness of the proposed method using gray-box and white-box projected gradient descent (PGD) attack, AutoAttack, and adaptive attacks, demonstrating that the mixing network is robust against the adversary types it is trained with. When the mixing network is trained with a carefully designed adaptive AutoAttack, the composite model significantly gains clean accuracy while sacrificing little robustness. This section and the corresponding experiment results are entirely new relative to our conference submission \citep{Bai23a}, and are crucial for achieving the much improved accuracy-robustness trade-off over existing works.
\end{itemize}

Note that adaptive smoothing is agnostic to the method of obtaining standard and robust base models. Thus, adaptive smoothing can take advantage of pre-training on large datasets via the standard base classifier and benefit from ongoing advancements in robust training methods via the robust base classifier.



\section{Background and Related Works}


\subsection{Notations}

The symbol  denotes the  norm of a vector, while  denotes its dual norm. The matrix  denotes the identity matrix in . For a scalar ,  denotes its sign. For a natural number , . For an event , the indicator function  evaluates to 1 if  takes place and 0 otherwise. The notation  denotes the probability for an event  to occur, where  is a random variable drawn from the distribution .

Consider a model , whose components are , where  is the dimension of the input and  is the number of classes. A classifier  can be obtained via . In this paper, we assume that  does not have the desired level of robustness, and refer to it as a ``standard classifier'' (as opposed to a ``robust classifier'' which we denote as ). Throughout this paper, we regard  and  as the base classifier logits. To denote their output probabilities, we use  and . Similarly,  denotes the predicted probability of the  class from . Moreover, we use  to denote the set of all validation input-label pairs .

We consider -norm-bounded attacks on differentiable neural networks. A classifier  is considered robust against adversarial perturbations at an input data  if it assigns the same label to all perturbed inputs  such that , where  is the attack radius. We use PGD to denote the -step PGD attack.


\subsection{Related Adversarial Attacks and Defenses}

The fast gradient sign method (FGSM) and PGD attacks based on the first-order maximization of the cross-entropy loss have traditionally been considered classic and straightforward attacks \citep{Madry18, Goodfellow15}. However, these attacks have been shown to be insufficient as defenses designed against them are often easily circumvented \citep{Carlini17a, Athalye18a, Athalye18b, Papernot17}. To this end, various attack methods based on alternative loss functions, Expectation Over Transformation, and black-box perturbations have been proposed. Such efforts include MultiTargeted attack loss \citep{Gowal19}, AutoAttack \citep{Croce20a}, adaptive attack \citep{Tramer20}, minimal distortion attack \citep{Croce20b}, and many others, even considering attacking test-time defenses \citep{Croce22}. The diversity of attack methods has led to the creation of benchmarks such as RobustBench \citep{Croce20c} and ARES-Bench \citep{Liu23} to unify the evaluation of robust models.

On the defense side, while adversarial training \citep{Madry18} and TRADES \citep{Zhang19} have seen enormous success, such methods are often limited by a significantly larger amount of required training data \citep{Schmidt18}. Initiatives that construct more effective training data via data augmentation \citep{Rebuffi21, Gowal20, Gowal21} and generative models \citep{Sehwag22, Wang23} have successfully produced more accurate and robust models. Improved versions of adversarial training \citep{Jia22, Wang20, Shafahi19, Pagliardini22} have also been proposed.

Previous research has developed models that improve robustness by dynamically changing at test time. Specifically, Input-Adaptive Inference improves the accuracy-robustness trade-off by appending side branches to a single network, allowing for early-exit predictions \citep{Hu20}. Other initiatives that aim to enhance the accuracy-robustness trade-off include using the SCORE attack during training \citep{Pang22} and applying adversarial training for regularization \citep{Zheng21}.

Moreover, ensemble-based defenses, such as random ensemble \citep{Liu18}, diverse ensemble \citep{Pang19, Alam22, Adam20}, and Jacobian ensemble \citep{Co22}, have been proposed. In comparison, this work is distinct in that our mixing scheme uses two separate classifiers, incorporating one non-robust component while still ensuring the adversarial robustness of the overall design. By doing so, we take advantage of the high performance of modern pre-trained models, significantly alleviating the accuracy-robustness trade-off and achieving much higher overall performances. Additionally, unlike some previous ensemble initiatives, our formulation is deterministic and straightforward (in the sense of gradient propagation), making it easier to evaluate its robustness properly. The work \citep{Kumar22} also explored assembling an accurate classifier and a robust classifier, but the method considered robustness against distribution shift in a non-adversarial setting and was based on different intuitions. After the submission of this paper, the work \citep{Zhao23} also considered leveraging the power of a pair of standard and robust classifiers. However, instead of mixing the outputs, the authors proposed to distill a new model from the two base classifiers. While this approach also yielded impressive results, the distillation process is time-consuming.


\subsection{Locally Biased Smoothing} \label{sec:local_biased_smoothing}

Randomized smoothing, popularized by \citep{Cohen19}, achieves robustness at inference time by replacing the standard classifier  with the smoothed classifier

where  is a smoothing distribution, for which a common choice is a Gaussian distribution.

Note that  is independent of the input  and it is standard to use  that is zero-mean. The authors of \citep{Anderson21b} have shown that data-invariant smoothing enlarges the region of the input space at which the prediction of  stays constant. Such an operation may actually degrade both clean and robust accuracy (the limiting case is when  becomes a constant classifier). Furthermore, when  is a linear classifier, the zero-mean restriction on  leaves  unchanged. That is, randomized smoothing with a zero-mean distribution cannot help robustify even the most simple linear classifiers. To overcome these limitations, \citep{Anderson21b} allowed  to be input-dependent (denoted by ) and nonzero-mean and searched for distributions  that best robustify  with respect to the data distribution. The resulting scheme is ``locally biased smoothing''.

It is shown in \citep{Anderson21b} that, up to a first-order linearization of the base classifier, the optimal locally biased smoothing distribution  shifts the input point in the direction of its true class. Formally, for a binary classifier of the form  with continuously differentiable , maximizing the robustness of  around  over all distributions  with bounded mean yields the optimal locally biased smoothing classifier given by

where  is the true class of , and where  is the (fixed) bound on the distribution mean (i.e., ).

Intuitively, this optimal locally biased smoothing classifier shifts the input along the direction  when  as a means to make the classifier more likely to label  into class , and conversely shifts the input along the direction  when . Of course, during inference, the true class  is generally unavailable, and therefore \citep{Anderson21b} uses a ``direction oracle''  as a surrogate for , resulting in the locally biased smoothing classifier

Notice that unlike randomized smoothing, the computation \cref{eq:lbrs} is deterministic, which is a consequence of the closed-form optimization over .

In contrast to data-invariant randomized smoothing, the direction oracle  is learned from data as a means to incorporate the data distribution directly into the manipulation of the locally biased randomized smoothing classifier's decision boundaries. This allows for increases in nonlinearity when the data implies that such nonlinearities are beneficial for robustness, resolving a fundamental limitation of data-invariant smoothing. In general, the direction oracle should come from an inherently robust classifier. Since such a robust classifier  is often less accurate, the value  can be viewed as a trade-off parameter, as it encodes the amount of trust put into the direction oracle. The authors of \citep{Anderson21b} show that when the direction oracle is chosen to be a one-nearest-neighbor classifier, locally biased smoothing outperforms traditional randomized smoothing in binary classification. 


\subsection{Adversarial Input Detectors}

It has been shown that adversarial inputs can be detected via various methods. For example, \citep{Metzen17} proposes to append an additional detection branch to an existing neural network, and uses adaptive adversarial data to train the detector in a supervised fashion. However, \citep{Carlini17b} has shown that it is possible to bypass this detection method. They constructed adversarial examples via the C\&W attacks \citep{Carlini17a} and simultaneously targeted the classification branch and the detection branch by treating the two branches as an ``augmented classifier''. According to \citep{Carlini17b}, the detector is effective against the types of attack that it is trained with, but not necessarily the attack types that are absent in the training data. It is thus reasonable to expect the detector to be able to detect a wide range of attacks if it is trained using sufficiently diverse types of attacks (including those targeting the detector itself). While exhaustively covering the entire adversarial input space is intractable, and it is unclear to what degree one needs to diversify the attack types in practice, our experiments show that our modified architecture based on \citep{Metzen17} can recognize the state-of-the-art AutoAttack adversaries with a high success rate.

The literature has also considered alternative detection methods that mitigate the above challenges faced by detectors trained in a supervised fashion \citep{Carrara19}. Such initiatives include unsupervised detectors \citep{Aldahdooh21a, Aldahdooh21b} and re-attacking \citep{Ahmadi21}. Since universally effective detectors have not yet been discovered, this paper focuses on transferring the properties of the existing detector toward better overall robustness. Future advancements in the field of adversary detection can further enhance the performance of our method.



\section{Using a Robust Neural Network as the Smoothing Oracle} \label{sec:STD+ROB}

Locally biased smoothing was designed for binary classification problems, making it restrictive in practice. Here, we first extend it to the multi-class setting by treating the output  of each class independently, giving rise to:


Note that if  is large for some , then  can be large even if both  and  are small, potentially leading to incorrect predictions. To remove the effect of the magnitude difference across the classes, we propose a normalized formulation as follows:


The parameter  adjusts the trade-off between clean accuracy and robustness. When , it holds that  for all . When , it holds that  for all .

With the mixing procedure generalized to the multi-class setting, we now discuss the choice of the smoothing oracle . While -NN classifiers are relatively robust and can be used as the oracle, their representation power is too weak. On the CIFAR-10 image classification task \cite{cifar10}, -NN only achieves around  accuracy on clean test data. In contrast, an adversarially trained ResNet \citep{He16} can reach  accuracy on attacked test data \cite{Madry18}. This lackluster performance of -NN becomes a significant bottleneck in the mixed classifier's accuracy-robustness trade-off. To this end, we replace the -NN model with a robust neural network. The robustness of this network can be achieved via various methods, including adversarial training, TRADES, and traditional randomized smoothing.

Further scrutinizing \cref{eq:adap_sm_2} leads to the question of whether  is the best choice for adjusting the mixture of  and . In fact, this gradient magnitude term is a result of the assumption that , which is the setting considered in \cite{Anderson21b}. Here, we no longer have this assumption. Instead, we assume both  and  to be differentiable. Thus, we further generalize the formulation to

where  is an extra scalar term that can potentially depend on both  and  to determine the ``trustworthiness'' of the base classifiers. Here, we empirically compare four options for , namely, , , , and . In \Cref{sec:R_options}, we explain how these four options were designed.

Another design question is whether  and  should be the pre-softmax logits or the post-softmax probabilities. Since most attack methods are designed based on logits, the output of the mixed classifier should be logits rather than probabilities to avoid gradient masking, an undesirable phenomenon that makes it hard to evaluate the proposed method properly. Thus, we have the following two options that make the mixed model compatible with existing gradient-based attacks:
\begin{enumerate}[leftmargin=6mm]
	\setlength\itemsep{1pt}
	\item Use the logits for both  and .
	\item Use the probabilities for both base classifiers, and then convert the mixed probabilities back to logits. The required ``inverse-softmax'' operator is given simply by the natural logarithm, and does not change the prediction.
\end{enumerate}

\begin{figure}[t]
	\centering
	\begin{minipage}{.56\textwidth}
		\includegraphics[width=\textwidth]{Figures/ResNet_AT_Linf.pdf}
	\end{minipage}
	\begin{minipage}{.38\textwidth}
		\begin{itemize}[leftmargin=4mm]
			\setlength\itemsep{.8em}
			\item \small ``No Softmax'' represents Option 1, i.e., use the logits for  and . 
			\item \small ``Softmax'' represents Option 2, i.e., use the probabilities for  and , with a natural log applied to the assembled . 
		\end{itemize}
	\end{minipage}
	\vspace{-2mm}
	\caption{Comparing the ``attacked accuracy versus clean accuracy'' curve for various options for . Note that with the best formulation, high clean accuracy can be achieved with very little sacrifice on robustness.}
	\label{fig:compare_R}
\end{figure}

\Cref{fig:compare_R} visualizes the accuracy-robustness trade-off achieved by different choices for . Here, the base classifiers are a pair of standard and adversarially trained ResNet-18s. This ``clean accuracy versus PGD-attacked accuracy'' plot conclude sthat  gives the best accuracy-robustness trade-off, and  and  should be the probabilities. \Cref{sec:more_compare_R} confirms this selection by repeating \Cref{fig:compare_R} with alternative model architectures, different methods to train the robust base classifiers, and various attack budgets.

Our selection of  differs from  used in \citep{Anderson21b}. Intuitively, \citep{Anderson21b} used linear classifier examples to motivate estimating the trustworthiness of the base models with their gradient magnitudes. However, when the base classifiers are highly nonlinear neural networks as in our case, while the local Lipschitzness of a base classifier still correlates with its robustness, its gradient magnitude is not always a good estimator of the local Lipschitzness. \Cref{sec:more_compare_R} provides additional discussions on this matter. Additionally, \Cref{sec:certified_radius_thms} offers theoretical intuitions for favoring mixing probabilities over mixing logits.

With these design choices implemented, the formulation \cref{eq:adap_sm_3} can be re-parameterized as

where . We take  in \cref{eq:adap_sm_4}, which is a convex combination of base classifier probabilities, as our proposed mixed classifier. Note that \cref{eq:adap_sm_4} calculates the mixed classifier logits so that the mixed classifier acts as a drop-in replacement for existing models which usually produce logits. Removing the logarithm operator gives the output probabilities.


\subsection{Theoretical Certified Robust Radius} \label{sec:certified_radius_thms}

In this section, we derive certified robust radii for  introduced in \cref{eq:adap_sm_4}, given in terms of the robustness properties of  and the mixing parameter . The results ensure that despite being more sophisticated than a single model,  cannot be easily conquered, even if an adversary attempts to adapt its attack methods to its structure. Such guarantees are of paramount importance for reliable deployment in safety-critical control applications. Note that while the focus of this paper is improved empirical accuracy-robustness trade-off and the existing literature often considers empirical and certified robustness separately, we will discuss how the certified results in this section provide important insights into the empirical performance, as the underlying assumptions are realistic and (approximately) verifiable for many empirically robust models.

Noticing that the base model probabilities satisfy  and  for all , we introduce the following generalized and tightened notion of certified robustness.

\begin{definition} \label{def:robust_with_margin}
	Consider a model  and an arbitrary input . Further consider , , and . Then,  is said to be \textbf{certifiably robust at  with margin  and radius } if  for all  and all  such that .
\end{definition}

Intuitively, \cref{def:robust_with_margin} ensures that all points within a radius from a nominal point have the same prediction as the nominal point, with the difference between the top and runner-up probabilities no smaller than a threshold. For practical classifiers, the robust margin can be straightforwardly estimated by calculating the confidence gap between the predicted and the runner-up classes at an adversarial input obtained with strong attacks. As shown in the experiments in \Cref{sec:conf_properties}, if a practical robust model is robust at an input with a given radius, it is likely to be robust with a large margin.

\begin{lemma}
	\label{lem:certified_radius}
	Let  and . If it holds that  and  is certifiably robust at  with margin  and radius , then the mixed classifier  is robust in the sense that  for all  such that .
\end{lemma}

\begin{proof}
Suppose that  is certifiably robust at  with margin  and radius . Since , it holds that . Let . Consider an arbitrary  and  such that . Since , it holds that

Thus, it holds that  for all , and thus .
\end{proof}

While most existing provably robust results consider the special case with zero margin, we will show that models built via common methods are also robust with non-zero margins. We specifically consider two types of popular robust classifiers: Lipschitz continuous models (\cref{thm:certified_radius}) and RS models (\cref{thm:randomized_smoothing}). \cref{lem:certified_radius} builds the foundation for proving these two theorems, which amounts to showing that Lipschitz and RS models are robust with non-zero margins and thus the mixed classifiers built with them are robust. \cref{lem:certified_radius} can also motivate future researchers to develop margin-based robustness guarantees for base classifiers so that they immediately grant robustness guarantees for mixed architectures.

\cref{lem:certified_radius} additionally provides further justifications for using probabilities instead of logits in the smoothing operation. Intuitively,  is bounded between  and , so as long as  is relatively large (specifically, at least ), the detrimental effect of  when subject to attack can be overcome by . Had we used the logits , since this quantity cannot be bounded, it would have been much harder to overcome the vulnerability of .

Since we do not make assumptions on the Lipschitzness or robustness of , \cref{lem:certified_radius} is tight. To understand this, we suppose that there exists some  and  such that  that make  smaller than , indicating that . Since the only information about  is that  and thus the value  can be any number between  and , it is possible that  is smaller than . By \cref{eq:adap_sm_4}, when , it holds that , and thus .

\begin{definition}
	\label{def:lipschitz}
	A function  is called \emph{-Lipschitz continuous} if there exists  such that  for all . The \textbf{Lipschitz constant} of such  is defined to be 
	
\end{definition}

\begin{assumption}
	\label{as:lipschitz}
	The base model  is robust in the sense that, for all ,  is -Lipschitz continuous with Lipschitz constant .
\end{assumption}

\begin{theorem}
	\label{thm:certified_radius}
	Suppose that Assumption \ref{as:lipschitz} holds, and let , where  be arbitrary. Then, if , it holds that  for all  such that
	
\end{theorem}

\begin{proof}
	Suppose that , and let  be such that . Furthermore, let . It holds that
	
	Therefore,  is certifiably robust at  with margin  and radius . Hence, by \cref{lem:certified_radius}, the claim holds.
\end{proof}

Note that the  norm that we certify can be arbitrary (e.g., , , or ), so long as the Lipschitz constant of the robust network  is computed with respect to the same norm.

Assumption \ref{as:lipschitz} is not restrictive in practice. For example, Gaussian RS with smoothing variance  yields robust models with -Lipschitz constant  \cite{Salman19}. Moreover, empirically robust methods such as adversarial training and TRADES often train Lipschitz continuous models, even though there may not be closed-form theoretical guarantees. The notion of Lipschitz continuity has even motivated novel robustness methods \citep{Moosavi-Dezfooli19, Terjek20}.

Assumption \ref{as:lipschitz} can be relaxed to the even less restrictive scenario of using local Lipschitz constants over a neighborhood (e.g., a norm ball) around a nominal input  (i.e., how flat  is near ) as a surrogate for the global Lipschitz constants. In this case, \cref{thm:certified_radius} holds for all  within this neighborhood. As a practical example, suppose that for an arbitrary input  and an  attack radius , it holds that  and  for all  and all perturbations  such that . Furthermore, suppose that  as defined in \cref{eq:lip_cert_rad} is not smaller than . Then, if the robust base classifier is correct at the nominal point , then the mixed classifier is robust at  within the radius .

As shown in \citep{Yang20}, the local Lipschitz constant of a given differentiable classifier can be easily estimated using an algorithm similar to the PGD attack. The authors of \citep{Yang20} also showed that many existing empirically robust models, including those trained with AT or TRADES, are in fact locally Lipschitz. Note that \citep{Yang20} evaluates the local Lipschitz constants of the logits, whereas we analyze the probabilities, whose Lipschitz constants are much smaller. Therefore, \cref{thm:certified_radius} provides important insights into the empirical robustness of the mixed classifier.

If , then , which is the standard (global) Lipschitz-based robust radius of  around  (see, e.g., \cite{Fazlyab19, Hein17} for further discussions on Lipschitz-based robustness). On the other hand, if  is too small in comparison to the relative confidence of , namely, if there exists  such that , then , and in this case we cannot provide non-trivial certified robustness for . This is rooted in the fact that too small of an  value amounts to excess weight in the non-robust classifier . If  is  confident in its prediction, then  for all , and therefore this threshold value of  becomes , leading to non-trivial certified radii for . However, once we put over  of the weight into , a nonzero radius around  is no longer certifiable. Again, this is intuitively the best one can expect, since no assumptions on the robustness of  around  have been made.

To summarize our certified robustness results, \cref{lem:certified_radius} shows the connection between the robust margin of the robust classifier and the robustness of the mixed classifier, while \cref{thm:certified_radius} demonstrates how general Lipschitz robust base classifiers exploit this relationship. Since empirically robust models often satisfy the conditions of these two results, they guarantee that adaptive attacks cannot easily circumvent our proposed robustification.

In \Cref{sec:rs_radius}, we further tighten the certified radius in the special case when  is a randomized smoothing classifier and the robust radii are defined in terms of the  norm by exploiting the stronger Lipschitzness of  arising from the unique structure and smoothness granted by Gaussian convolution operations ( is the inverse Gaussian CDF function). We then visualize and compare the certified robustness of the mixed classifier to existing certifiably robust methods in \Cref{sec:certified_exp}.



\section{Adaptive Smoothing Strength with the Mixing Network} \label{sec:ada_smo}

So far,  has been treated as a fixed hyperparameter. A more intelligent approach is to allow  to be different for each  by replacing the constant  with a function . Here, we take  to be deterministic, as stochastic defenses can be much harder to evaluate.

\begin{figure}
    \centering
    \includegraphics[width=.55\textwidth, height=.29\textwidth, trim={4mm 4.8mm 4mm 3.5mm}, clip] {Figures/Adaptive_PreAttacked_Acc.pdf}
    \caption{Attacked accuracy of the standard classifier  and the robust classifier  when the adversary targets different values of . For better readability, we use  as the horizontal axis labels, where  denotes the inverse function of Sigmoid.}
    \label{fig:ada_acc_2}
\end{figure}

One motivation for adopting the adaptive trade-off parameter  is that the optimal  can vary when  changes. For example, when  is clean and unperturbed, the standard model  outperforms the robust model . If  is an attacked input targeting , then the robust model  should be used. However, as shown in \Cref{fig:ada_acc_2}, if the target of the attack is , then even though  is robust, a better choice is to feed  into . This is because the loss landscapes of  and  differ enough that an adversarial perturbation targeting  is benign to .

When the PGD adversary targets a mixed classifier , as  varies, the optimal strategy also changes. We provide a visualization in \Cref{fig:ada_acc_2} based on the CIFAR-10 dataset. Specifically, we assemble a composite model  using a ResNet-18 standard classifier  and a ResNet-18 robust classifier  (both from \citep{Na20}) via \cref{eq:adap_sm_4}. Then, we attack  with different values of  via PGD, save the adversarial instances, and report the accuracy of  and  evaluated on these instances. When , the robust model  performs better. When , the standard model  is more suitable.

Throughout the remainder of this section, we overload the notation  even though  may be a function of the input, i.e., we define .


\subsection{The Existence of  that Achieves the Trade-Off} \label{sec:alpha(x)}

The following theorem shows that, under realistic conditions, there exists a function  that makes the combined classifier correct whenever either  and  makes the correct prediction, which further implies that the combined classifier matches the clean accuracy of  and the attacked accuracy of .

\begin{theorem} \label{THM:ADA}
    Let , , and  (i.e., each input corresponds to a unique true label). Assume that , , and  are all bounded and that there does not exist  such that  and . Then, there exists a function  such that the assembled classifier  satisfies
    
    where  is an arbitrary distribution that satisfies .
\end{theorem}

\begin{proof}
Since it is assumed that the perturbation balls of the data are non-overlapping, the true label  corresponding to each perturbed data  with the property  is unique. Therefore, the indicator function

satisfies that
-1mm]
    \alpha (x+\delta) = 1 \quad\;\; & \text{ if } \quad\; \argmax_{i \in [c]} g_i (x+\delta) \neq y \text{ and } \argmax_{i \in [c]} h_i (x+\delta) = y.

    \hialpha (x+\delta) = g_i (x+\delta) \quad\; & \text{ if } \quad\; \argmax_{i \in [c]} g_i (x+\delta) = y, \
implying that 

which leads to the desired statement.	
\end{proof}

Note that the distribution  is arbitrary, implying that the test data can be clean data, any type of adversarial data, or some combination of both. As a special case, when the distribution  is a Dirac measure centered at the origin, \Cref{THM:ADA} implies that the clean accuracy of  is as good as the standard classifier . Conversely, when  is a Dirac measure centered at the worst-case perturbation, the adversarial accuracy of  is not worse than the robust model , implying that if  is inherently robust, then  inherits the robustness. One can then conclude that there exists a  that matches the clean accuracy of  and the robustness of .

While \cref{THM:ADA} guarantees the existence of an instance of  that perfectly balances accuracy and robustness, finding an  that achieves this trade-off can be hard. However, we will use experiments to show that an  represented by a neural network can retain most of the robustness of  while greatly boosting the clean accuracy. In particular, while we used the case of  being an indicator function to demonstrate the possibility of achieving the trade-off, \Cref{fig:compare_R} has shown that letting  take an appropriate value between  and  also improves the trade-off. Thus, the task for the neural approximator is easier than representing the indicator function. Also note that if certified robustness is desired, one can enforce a lower bound on  and take advantage of \cref{thm:certified_radius} while still enjoying the mitigated trade-off.


\subsection{Attacking the Adaptive Classifier} \label{sec:adaptive_attack}

When the combined model  is under adversarial attack, the function  provides an addition gradient flow path. Intuitively, the attack should be able to force  to be small through this additional gradient path, tricking the mixing network into favoring the non-robust . Following the guidelines for constructing adaptive attacks \citep{Tramer20}, in the experiments, we consider the following types of attacks:
\begin{enumerate}[label=\textbf{\Alph*}, leftmargin=7.5mm]
\setlength\itemsep{1pt}
	\item \textbf{Gray-box PGD:} In this setting, the adversary has access to the gradients of  and  when performing first-order optimization, but is not given the gradient of the mixing network . We consider untargeted PGD attack with a fixed initialization.
	\item \textbf{White-box PGD:} Since the mixed classifier is end-to-end differentiable, following \citep{Tramer20}, we allow the adversary to access the end-to-end gradient, including the gradient of the mixing network.
	\item \textbf{White-box AutoAttack:} ``AutoAttack'' is a stronger attack formed by an ensemble of four automated attack algorithms \citep{Croce20a}. The method considers APGD attacks generated via the untargeted cross-entropy loss and the targeted DLR loss, in addition to the targeted FAB attack and the black-box Square Attack (SA) \citep{Andriushchenko20}. Again, the end-to-end gradient of the mixed classifier is available to the adversary. AutoAttack requires much more computation budget than PGD.
	\item \textbf{Adaptive white-box AutoAttack:} Since the mixing network is a crucial component of the defense, we adapt AutoAttack to target the mixing network by adding an APGD loss component that aims to decrease .
\end{enumerate}

We will show that the adaptively smoothed model is robust against the attack that it is trained against. When trained using APGD attack with untargeted and targeted loss functions, our model becomes robust against AutoAttack while achieving a significant improvement in the accuracy-robustness trade-off. Additionally, in \Cref{sec:alpha_analysis}, we consider examples of transfer attacks.


\subsection{The Mixing Network} \label{sec:mixing_network}

In practice, we use a neural network  to learn an effective mixing network that adjusts the outputs of  and . Here,  represents the trainable parameters of the mixing network, and we refer to  as the ``mixing network''. To enforce an output range constraint, we apply a Sigmoid function to the mixing network output. Note that when training the mixing network , the base classifiers  and  are frozen. Freezing the base classifiers allows the mixed classifier to take advantage of existing accurate models and their robust counterparts, maintaining explainability and avoiding unnecessary feature distortions that the adversary can potentially exploit.

The mixing network's task of treating clean and attacked inputs differently is closely related to the adversary detection problem. To this end, we adapt the detection architecture introduced in \citep{Metzen17} for our mixing network. This architecture is end-to-end differentiable, making training and evaluation easier, and is also known for its performance and ease of implementation. While \citep{Carlini17b} has argued that simultaneously attacking the base classifier and the adversary detector can bring the detection rate of the detection method proposed in \citep{Metzen17} to near zero, we show that with several key modifications, the method is effective even against strong white-box attacks. Specifically, our mixing network  takes advantage of the two available models  and  by using the intermediate features of both networks via concatenation (\citep{Metzen17} only has one base model). More importantly, we include stronger adaptive adversaries during training to generate much more diverse examples.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figures/OverviewSIMODS.pdf}
    \caption{The overall architecture of the adaptively smoothed classifier introduced in \Cref{sec:ada_smo}. ``RNB'' stands for ResNetBlock and ``BN'' represents the 2D batch normalization layer.}
    \label{fig:mixing_arch}
    \vspace{-1mm}
\end{figure*}

The mixing network structure is based on a ResNet-18, which is known to perform well for a wide range of computer vision applications and is often considered the go-to architecture. We make minimum changes to the ResNet-18 to fit into our framework. As the mixing network takes information from both  and , it uses the concatenated embeddings from the base classifiers. While \citep{Metzen17} considers a single ResNet as the base classifier and uses the embeddings after the first ResNet block, to avoid the potential vulnerability against ``feature adversaries'' \citep{Sabour15}, we consider the embeddings from two different layers of the base model. \Cref{fig:mixing_arch} demonstrates the modified architecture. The detailed implementations used in the experiment section are discussed in \Cref{sec:mixing_arch_rn}.

Since \Cref{fig:compare_R} shows that even a constant  can alleviate the accuracy-robustness trade-off, our method does not excessively rely on the performance of the mixing network . In \Cref{sec:ada_exp}, we provide empirical results demonstrating that the above modifications help the overall mixed network defend against strong attacks.


\subsection{Training the Mixing Network} \label{sec:train_mixing_network}

Consider the following two loss functions for training the mixing network :
\begin{itemize}[leftmargin=6mm]
\setlength\itemsep{1pt}
    \item \textbf{Multi-class cross-entropy:} We minimize the multi-class cross-entropy loss of the combined classifier, which is the ultimate goal of the mixing network:
    
    where  is the cross-entropy (CE) loss for logits and  is the label corresponding to . The base classifiers  and  are frozen and not updated. Again,  denotes the perturbation, and the distribution  is arbitrary. In our experiments, to avoid overfitting to a particular attack radius,  is formed by perturbations with randomized radii.  
    
    \item \textbf{Binary cross-entropy:} The optimal  that minimizes  in \cref{eq:unsup_opt} can be estimated for each training point. Specifically, depending on whether the input is attacked and how it is attacked, either  or  should be prioritized. Thus, we treat the task as a binary classification problem and solve the optimization problem
    
    where  is the binary cross-entropy (BCE) loss for probabilities and  is the ``pseudo label'' for the output of the mixing network that approximates .
\end{itemize}

Using only the multi-class loss suffers from a distributional mismatch between the training set and the test set. The robust classifier  may achieve a low loss on adversarial training data but a high loss on adversarial test data. For example, with the CIFAR-10 dataset and our ResNet-18 robust classifier, the PGD adversarial training accuracy is  while the PGD test accuracy is . As a result, approximating \cref{eq:unsup_opt} with empirical risk minimization on the training set does not effectively optimize the true risk. When the adversary attacks a test input  targeting , the standard prediction  yields a lower loss than . However, if  is an attacked example in the training set, then the losses of  and  are similar, and the mixing network does not receive a strong incentive to choose  when it detects an attack targeting .

The binary loss, however, does not capture the potentially different sensitivity of each input. Certain inputs can be more vulnerable to adversarial attacks, and ensuring the correctness of the mixing network on these inputs is more crucial.

To this end, we combine the above two components into a composite loss function, incentivizing the mixing network to select the standard classifier  when appropriate, while forcing it to remain conservative. The composite loss for each data-label pair  is given by

where the hyperparameters , and  control the weights of the loss components. \Cref{sec:loss_abla} discusses how these hyperparameters affect the performance of the trained mixing model.



\section{Numerical Experiments} \label{sec:experiments}

\subsection{Robust Neural Network Smoothing with a Fixed Strength} \label{sec:exp_CNN_fix}

Here, we consider the case where the smoothing strength  is a fixed value, and use the CIFAR-10 dataset to evaluate the performance of the mixed classifier  with various values of .

\subsubsection{'s Influence on Robustness} \label{sec:alpha_analysis}

We first analyze how the accuracy of the mixed classifier changes with the mixing strength  under various settings. Specifically, we consider PGD attacks that target  and  individually (abbreviated as STD and ROB attacks), in addition to the adaptive PGD attack generated using the end-to-end gradient of , denoted as the MIX attack. Note that the STD and ROB attacks, which share the inspiration of \citep{Gao22}, correspond to the ``transfer attack'' setting, a common black-box attack strategy designed for defenses with unavailable or unreliable gradients. Note that the models with the best transferability with the mixed classifier  would likely be its base classifiers  and , precisely corresponding to the STD and ROB attack settings.

We use a ResNet18 model trained on clean data as the standard model  and use another ResNet18 trained on PGD data as the robust model . The test accuracy corresponding to each  value is presented in \Cref{fig:STD+ROB}. As  increases, the clean accuracy of  converges from the clean accuracy of  to the clean accuracy of . In terms of the attacked performance, when the attack targets , the attacked accuracy increases with . When the attack targets , the attacked accuracy decreases with , showing that the attack targeting  becomes more benign when the mixed classifier emphasizes . When the attack targets , the attacked accuracy increases with .

When  is around , the MIX-attacked accuracy of  quickly increases from near zero to more than  (which is two-thirds of 's attacked accuracy). This observation precisely matches the theoretical intuition provided by \Cref{thm:certified_radius}. When  is greater than , the clean accuracy gradually decreases at a much slower rate, leading to the noticeably alleviated accuracy-robustness trade-off. Note that this improved trade-off is achieved without any further training beyond the weights of  and . When  is greater than , neither STD attack nor ROB attack can reduce the accuracy of the mixed classifier below the end-to-end gradient-based attack (MIX attack), indicating that the considered transfer attack is weaker than gradient-based attack for practical  values, and implying that the robustness of  does not rely on obfuscated gradients. In \Cref{sec:conf_properties}, we will reveal that the source of 's robustness lies in 's well-calibrated confidence properties.

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
	\centering
    \includegraphics[width=\textwidth, height=.67\textwidth, trim={4.5mm 5mm 2.5mm 4mm}, clip] {Figures/STD+ROB_results_nograd.pdf}
    \caption{The performance of the mixed classifier . ``STD attack'', ``ROB attack'', and ``MIX attack'' refer to the PGD attack generated using the gradient of , , and  respectively, with  set to .}
    \label{fig:STD+ROB}
\end{minipage}
\hfill
\begin{minipage}{.47\textwidth}
	\centering
	\includegraphics[width=\textwidth, height=.65\textwidth, trim={4.5mm 5mm 2.5mm 4mm}]{Figures/Compare_with_TRADE_Linf.pdf}
	\caption{Accuracy-robustness trade-off comparison between our mixed classifier and TRADES models. ``Mixed'' stands for our mixed classifier . All TRADES models and the base models of  use the WideResNet-34-10 architecture as in \citep{Zhang19}.}
	\label{fig:compare_w_trades}
\end{minipage}
\end{figure}

\subsubsection{The Relationship between 's Robustness and 's Confidence} \label{sec:conf_properties}


Our theoretical analysis (\cref{lem:certified_radius}) has highlighted the relationship between the mixed classifier robustness and the robust base classifier 's robust margin. For practical models, the margin at a given radius can be estimated with the confidence gap between the predicted and runner-up classes evaluated on strongly adversarial inputs, such as images returned from PGD or AutoAttack. Moreover, the improved accuracy-robustness trade-off of the mixed classifier, as evidenced by the difference in how clean and attacked accuracies change with  in \Cref{fig:STD+ROB}, can also be explained by the prediction confidence of .

According to \cref{tab:confidence}, the robust base classifier  makes confident correct predictions even when under attack (average robust margin is 0.768 evaluated with PGD and 0.774 with AutoAttack\footnote{The calculation details the AutoAttacked confidence gap are presented in \Cref{sec:autoattack_margin}.}). Moreover, the robust margin of  follows a long-tail distribution. Specifically, the median robust margin is  (same number when evaluated with PGD or AutoAttack), much larger than the  average margin. Thus, most attacked inputs correctly classified by  are highly confident (i.e., robust with large margins), with only a tiny portion suffering from small robust margins. As \cref{lem:certified_radius} suggests, such property is precisely what adaptive smoothing relies on. Intuitively, once  becomes greater than  and gives  more authority over ,  can use its high confidence to correct 's mistakes under attack.

On the other hand,  is unconfident when producing incorrect predictions on clean data, with the top two classes' output probabilities separated by merely 0.434. This probability gap again forms a long-tail distribution (the median is 0.378 which is less than the mean), confirming that  is generally unconfident when it mispredicts, rarely making confident incorrect predictions. Now, consider clean unattacked data which  correctly classifies and  mispredicts (recall that we assume  to be a more accurate but less robust model so this scenario is common). Since  is confident and  is unconfident with high probability, even when  and  has less authority than  in the mixture,  can still correct some of the mistakes from .

In summary,  is confident when making correct predictions on attacked data, enjoying the large robust margin required by \cref{lem:certified_radius}. At the same time,  is unconfident when misclassifying clean data, and such a confidence property is the key source of the mixed classifier's improved accuracy-robustness trade-off. Additional analyses in \Cref{sec:more_compare_R} with alternative base models imply that multiple existing robust classifiers share the favorable confidence property and thus help the mixed classifier improve the trade-off.

The standard non-robust classifier  often does not have this desirable property: even though it is confident on clean data as are robust classifiers, it also makes highly confident mistakes under attack. Note that this does not undermine the mixed classifier robustness, since our formulation does not assume any robustness or smoothness from .

\begin{table}[!tb]
	\vspace{-2mm}
	\caption{Average gap between the probabilities of the predicted class and the runner-up class.  and  are the same ones used in \Cref{fig:STD+ROB}. The confidence difference highlighted by the bold numbers is crucial to the mitigated accuracy-robustness trade-off of the mixed classifier.}
	\begin{small}
	\begin{center}
	\begin{tabular}{l!{\vrule width 2pt}c|c|c!{\vrule width 2pt}c|c|c!{\vrule width 2pt}c|c|c}
		\toprule
		& \multicolumn{3}{c!{\vrule width 2pt}}{Clean} & \multicolumn{3}{c!{\vrule width 2pt}}{PGD} & \multicolumn{3}{c}{AutoAttack} \\
		& Accuracy & \cmark\ Gap & \xmark\ Gap & Accuracy & \cmark\ Gap & \xmark\ Gap & Accuracy & \cmark\ Gap & \xmark\ Gap \\
		\midrule
		 	&  	&  	& 			& 		& 			&  & 	& 				&  \\
		 	&  	&  	& 	& 		& 	&  &		& 	&  \\
		\bottomrule
	\end{tabular}
	\end{center}
	\vspace{1.5mm}
	\cmark\ Gap: The average gap between the confidences of the predicted class and the runner-up class among all correctly predicted validation data. \\
	\xmark\ Gap: The same quantity evaluated among all incorrectly predicted validation data.
	\end{small}
	\label{tab:confidence}
\end{table}

\subsubsection{Comparing the Accuracy-Robustness Trade-Off with TRADES} \label{sec:compare_with_trades}

TRADES \citep{Zhang19} is one of the most famous existing initiatives to improve the accuracy-robustness trade-off of classifiers. This subsection shows that our method achieves a much more benign trade-off than TRADES. Specifically, TRADES proposed to train robust models by minimizing the risk

where  is a trade-off parameter between the two loss components and  is the ``surrogate loss'' that promotes robustness. The larger  is, the more robust the trained model becomes at the expense of clean accuracy. By adjusting , we can adjust the accuracy-robustness trade-off of TRADES similarly to adjusting  in our mixed classifier.

The authors of \citep{Zhang19} reported that  optimized the adversarial robustness and released the corresponding model. We use this model and train three additional models with  set to , , and . Here,  is standard training, and the other two numbers were chosen so that the model accuracy spreads relatively uniformly between  and . For a fair comparison, we build mixed classifiers using the TRADES model trained with  as  and the  model as . We compare the relationship between the PGD accuracy and the clean accuracy of the two methods in \Cref{fig:compare_w_trades}. Note that the trade-off curve of the mixed classifier intercepts the TRADES curve at the two ends (since the models are exactly the same at the two ends), and is significantly above the TRADES in the middle, indicating that the accuracy-robustness trade-off of the mixed classifier is better than that of TRADES.

Note that for TRADES, adjusting the trade-off requires training a new model, which is costly. For our mixing classifier, adjusting  does not require re-training and is thus much more flexible and efficient while achieving a better Pareto curve.


\subsection{Robust Neural Network Smoothing with Adaptive Strength} \label{sec:ada_exp}

Having validated the effectiveness of the mixing formulation described in \cref{eq:adap_sm_4}, we are now ready to incorporate the mixing network for adaptive smoothing strength. As in \Cref{sec:ada_smo}, we denote the parameterized mixing network by , and slightly abuse notation by denoting the composite classifier with adaptive smoothing strength given by  by , which is defined by .

CIFAR-10 and CIFAR-100 are two of the most widely used benchmark datasets for evaluating adversarial robustness. Although recent progress in learning robust models has made the accuracy-robustness trade-off less noticeable for the easier CIFAR-10 dataset, with robust neural networks almost matching standard models on clean accuracy \citep{Rebuffi21, Gowal20, Gowal21}, the trade-off remains highly noticeable for more challenging tasks such as CIFAR-100, for which robust models suffer from significant accuracy degradation. As existing methods for improving standard model performance may not readily extend to robust models, mixing pre-trained classifiers via adaptive smoothing becomes particularly advantageous for these harder tasks. Therefore, we use CIFAR-10 models to perform ablation analyses and proof-of-concept demonstrations and show that our mixing classifiers reconcile the accuracy-robustness trade-off to an unprecedented level on the CIFAR-100 dataset.

In all experiments, we consider  attacks and use the AdamW optimizer \citep{Kingma15} for training the mixing network . The training data for  include clean images and the corresponding types of attacked images (attack settings A, B, and C presented in Section \ref{sec:adaptive_attack}). For setting C (AutoAttack), the training data only include targeted and untargeted APGD attacks, with the other two AutoAttack components, FAB and Square, excluded during training in the interest of efficiency but included for evaluation. To alleviate overfitting, when generating training-time attacks, we randomize the attack radius and the number of steps, and add a randomly-weighted binary cross-entropy component that aims to decrease the mixing network output to the attack objective (thereby tricking it into favoring ). Additionally, \Cref{sec:mixing_arch_rn} discusses the details of implementing the architecture shown in \Cref{fig:mixing_arch} for the ResNet base classifiers used in our experiments; \Cref{sec:loss_abla} performs an ablation study on the hyperparameters of the composite loss function \cref{eq:comp_loss}.


\subsubsection{Ablation Studies Regarding Attack Settings} \label{sec:adap_exp_abla}

We first use smaller base classifiers to analyze the behavior of adaptive smoothing by exploring various training and attack settings. The performance of the base models and the assembled mixed classifier are summarized in \cref{tab:cifar10}, where each column represents the performance of one mixed classifier. The results show that the adaptive smoothing model can defend against the attacks on which the underlying mixing network is trained.
Specifically, for the attack setting A (gray-box PGD),  is able to achieve the same level of PGD-attacked accuracy as  while retaining a similar level of clean accuracy as .
For the setting B (white-box PGD), the attack is allowed to follow the gradient path provided by  and deliberately evade the part of the adversarial input space recognized by . While the training task becomes more challenging, the improvement in the accuracy-robustness trade-off is still substantial. Furthermore, the composite model can generalize to examples generated via the stronger AutoAttack.
For the setting C (AutoAttack), the difficulty of the training problem further escalates. While the performance of  on clean data slightly decreases, the mixing network can offer a more vigorous defense against AutoAttack data, still improving the accuracy-robustness trade-off.

\begin{table}[!tb]
	\centering
	\vspace{-2mm}
	\captionof{table}{CIFAR-10 results of adaptive smoothing models trained with three different settings.}
	\label{tab:cifar10}
	\vspace{-.5mm}
	\begin{small}
		CIFAR-10 base classifier performances \\
		\begin{tabular}{l|l!{\vrule width 2pt}c|c|c}
			\toprule
			Model & \footnotesize{Architecture} & Clean & PGD & AutoAttack \\
			\midrule
			 \scriptsize{(accurate)} & ResNet-18 \scriptsize{(Standard non-robust training)} & 95.28 \% & 0.12 \% & 0.00 \% \\
			 \scriptsize{(robust)} & WideResNet-34-10 \scriptsize{(TRADES model \citep{Zhang19})} & 84.92 \% & 57.16 \% & 53.09 \% \\
			\bottomrule
		\end{tabular}
		
		\vspace{1.5mm}
		CIFAR-10 adaptive smoothing mixed classifier  performance \\
		\begin{tabular}{l!{\vrule width 2pt}c|c|c}
			\toprule
			Evaluation Data  Training Setting & \textbf{A} & \textbf{B} & \textbf{C} \\
			\midrule
			Clean 								   		   & 92.05 \% & 92.07 \% & 91.51 \% \\
			\textbf{A} \scriptsize{(gray-box PGD)}  & 57.22 \% & 57.25 \% & 56.30 \% \\
			\textbf{B} \scriptsize{(white-box PGD)} & 56.63 \% & 57.09 \% & 56.29 \% \\
			\textbf{C} \scriptsize{(white-box AutoAttack)} & 40.04 \% & 40.02 \% & 42.78 \% \\
			\textbf{D} \scriptsize{(adaptive AutoAttack)}  & 39.85 \% & 39.70 \% & 42.66 \% \\
			\bottomrule
		\end{tabular}
	\end{small}
	\vspace{-1mm}
\end{table}

\begin{table}[!tb]
	\centering
	\vspace{-2mm}
	\captionof{table}{CIFAR-100 results of adaptive smoothing models trained with the three settings.}
	\label{tab:cifar100}
	\vspace{-.5mm}
	\begin{small}
		CIFAR-100 base classifier performances \\
		\begin{tabular}{l|l!{\vrule width 2pt}c|c|c}
			\toprule
			Model & \footnotesize{Architecture} & Clean & PGD & AutoAttack \\
			\midrule
			 \scriptsize{(accurate)}	& ResNet-152 \scriptsize{(Based on BiT \citep{Kolesnikov20})}	& 91.38 \% & 0.14 \%  & 0.00 \% \\
			 \scriptsize{(robust)}	& WideResNet-70-16 \scriptsize{(From \citep{Gowal20})}		& 69.17 \% & 40.86 \% & 36.98 \% \\
			\bottomrule
		\end{tabular}
		
		\vspace{1.5mm}
		CIFAR-100 adaptive smoothing mixed classifier  performance \\
		\begin{tabular}{l!{\vrule width 2pt}c|c|c}
			\toprule
			Evaluation Data  Training Setting	& \textbf{A} & \textbf{B} & \textbf{C} \\
			\midrule
			Clean 								   			& 83.99 \% & 83.96 \% & 80.90 \% \\
			\textbf{A} \scriptsize{(gray-box PGD)}	& 40.04 \% & 39.80 \% & 39.26 \% \\
			\textbf{B} \scriptsize{(white-box PGD)}	& 30.59 \% & 34.48 \% & 38.92 \% \\
			\textbf{C} \scriptsize{(white-box AutoAttack)}	& 23.54 \% & 26.37 \% & 32.94 \% \\
			\textbf{D} \scriptsize{(adaptive AutoAttack)} 	& 23.78 \% & 26.17 \% & 32.80 \% \\
			\bottomrule
		\end{tabular}
	\end{small}
\end{table}

\cref{tab:cifar100} repeats the above analyses on the CIFAR-100 dataset. The results confirm that adaptive smoothing achieves even more significant improvements on the CIFAR-100 dataset. Notably, even for the most challenging attack setting C,  correctly classifies 1173 additional clean images compared with  (cutting the error rate by a third) while making only 404 additional incorrect predictions on AutoAttacked inputs (increasing the error rate by merely 6.4 relative percent). Such results show that  is capable of approximating a robust high-performance mixing network when trained with sufficiently diverse attacked data. The fact that  combines the clean accuracy of  and the robustness of  highlights that our method significantly improves the accuracy-robustness trade-off.


\subsubsection{Comparisons Against Existing Works} \label{sec:adap_exp_compare}

This section uses state-of-the-art base classifiers to show that adaptive smoothing can produce excellent results and offer comparisons with existing methods. Since the literature has generally regarded AutoAttack \citep{Croce20a} as one of the most reliable and appropriate robustness evaluation methods (weaker attacks such as PGD are known to be circumventable), we select existing works that use AutoAttack as the benchmark. For the accurate base classifier , we select BiT \citep{Kolesnikov20} ResNet-152 models. Specifically, we fine-tune the publicly released BiT checkpoint (pre-trained on ImageNet-21k) on CIFAR-10 or CIFAR-100 following the recipe specified in \citep{Kolesnikov20}, resulting in a  clean accuracy for CIFAR-10 and  for CIFAR-100. We highlight that the listed works should not be treated as competitors, since advancements in building robust classifiers can be incorporated into our framework as , helping adaptive smoothing perform even better.

For CIFAR-10, we select the robust model checkpoint released in \citep{Wang23} as the robust base classifier  and report the results in \cref{tab:compare_cifar10}. Compared with the robust base classifier, our adaptive smoothing model retains 96.3 (relative) percent of the robust accuracy while reducing the clean data error rate by 29.3 (relative) percent. Among all models that were available on RobustBench at the time of the submission, our model achieves the third highest AutoAttacked accuracy, only behind \citep{Wang23} (used as  in our model) and \citep{Kang21} (for which AutoAttack is unreliable and the best-known attacked accuracy is lower than ours). On the other hand, the clean accuracy of our mixed classifier is higher than all other CIFAR-10 models designed towards the  adversarial attack setting available on RobustBench, and is even higher than the listed non-robust model that uses standard training.

\begin{table}
\begin{center}
	\caption{The clean and AutoAttacked accuracy of adaptive smoothing on CIFAR-10 compared with existing works. The numbers for the existing works are reported by the authors.}
	\label{tab:compare_cifar10}
	\vspace{-1mm}

	\begin{small}
	\begin{tabular}{l|l!{\vrule width 2pt}c|c}
		\toprule
		Method & Model Architecture & Clean & AutoAttack \\
		\midrule
		Adaptive Smoothing (this work) 		& ResNet-152 + WRN-70-16		& 95.23 \% & 68.06 \% \\
		\midrule
		SODEF + TRADES \citep{Kang21}				& WRN-70-16	+ ODE Block		& 93.73 \% &  71.28 \%  \\
		Diffusion (EDM) + TRADES \citep{Wang23}		& WideResNet-70-16			& 93.25 \% & 70.69 \% \\
		Diffusion (DDPM) + TRADES \citep{Rebuffi21}	& WideResNet-70-16			& 92.23 \% & 66.58 \% \\
		Unlabeled data + TRADES \citep{Gowal20}		& WideResNet-70-16			& 91.10 \% & 65.88 \% \\
		TRADES \citep{Gowal20}						& WideResNet-70-16			& 85.29 \% & 57.20 \% \\
		\bottomrule
	\end{tabular}
	\end{small}
\end{center}
\small{: Uses ``EDM + TRADES'' \citep{Wang23} as the robust base model . } \\
\small{: AutoAttack raises the ``potentially unreliable'' flag (explained in the next page), and adaptive attack} \\
\small{ reduces the attacked accuracy to 64.20 \%. AutoAttack does not raise this flag for our models.}
\end{table}

\begin{table}
	\centering
	\caption{The clean and AutoAttacked accuracy of adaptive smoothing on CIFAR-100 compared with existing works.}
	\label{tab:compare_cifar100}
	\vspace{-1mm}

	\begin{small}
	\begin{tabular}{l|l!{\vrule width 2pt}c|c}
		\toprule
		Method & Model Architecture & Clean & AutoAttack \\
		\midrule
		Adaptive Smoothing (this work) 			& ResNet-152 + WRN-70-16		& 85.21 \% & 38.72 \% \\
		Adaptive Smoothing (this work) 	& ResNet-152 + WRN-70-16		& 80.18 \% & 35.15 \% \\
		\midrule
		Diffusion (EDM) + TRADES \citep{Wang23}			& WideResNet-70-16			& 75.22 \% & 42.67 \% \\
		Unlabeled data + TRADES \citep{Gowal20}			& WideResNet-70-16			& 69.17 \% & 36.98 \% \\
		TRADES \citep{Debenedetti22}				& XCiT-L12 \citep{Ali21}		& 70.76 \% & 35.08 \% \\
		Diffusion (DDPM) + TRADES \citep{Rebuffi21}		& WideResNet-70-16			& 63.56 \% & 34.64 \% \\
		SCORE Loss AT \citep{Pang22}						& WideResNet-70-16			& 65.56 \% & 33.05 \% \\
		Diffusion (DDPM) + AT \citep{Sehwag22}			& WideResNet-34-10			& 65.93 \% & 31.15 \% \\
		TRADES \citep{Gowal20}							& WideResNet-70-16			& 60.86 \% & 30.03 \% \\
		\bottomrule
	\end{tabular}
	\end{small}
	\vspace{3mm}

	\small{: Uses ``EDM + TRADES'' \citep{Wang23} as the robust base model . } \\
	\small{: Uses ``Unlabeled data + TRADES'' \citep{Gowal20} as the robust base model . }
\end{table}

While the above results demonstrate reconciled accuracy and robustness, the clean accuracy improvement over existing works is not highly prominent. This is because robust base classifiers are already highly accurate on the easier CIFAR-10 dataset, and there is not much improvement room for clean accuracy. As discussed earlier, adaptive smoothing is particularly advantageous for more challenging tasks where the accuracy-robustness trade-off is highly noticeable with existing models. We now support this claim with more significant improvements on the harder CIFAR-100 dataset.

For CIFAR-100, we assemble two instances of the mixed classifier using two different robust base classifiers and display the results in \cref{tab:compare_cifar100}. Compared with their corresponding robust base models, both implementations of adaptive smoothing improve the clean accuracy by ten percentage points, with the AutoAttacked accuracy only deteriorating by four percentage points. Notably, as of the submission of this paper, on the CIFAR-100 dataset, the composite classifier based on the robust model introduced in \citep{Wang23} achieved an AutoAttacked accuracy better than any other methods listed on RobustBench \citep{Croce20c}, except \citep{Wang23} itself. Simultaneously, this mixed model offers an improvement of ten percentage points compared with any other listed models. These results demonstrate that adaptive smoothing significantly alleviates the accuracy-robustness trade-off of neural classifiers.

We also report that the SA component of AutoAttack, which performs gradient-free black-box attacks on images that gradient-based attack methods fail to perturb, only changes very few predictions. Specifically, AutoAttack will raise a ``potentially unreliable'' flag if SA further reduces the accuracy by at least  percentage points. This flag is not thrown for our models in Tables \ref{tab:compare_cifar10} and \ref{tab:compare_cifar100}, indicating that the mixed classifiers' robustness is not a result of gradient obfuscation. Thus, gradient-based attacks in AutoAttack sufficiently evaluate our models.



\section{Conclusions}

This paper proposes ``adaptive smoothing'', a flexible framework that leverages the mixture of the probability outputs of an accurate model and a robust model to mitigate the accuracy-robustness trade-off of neural classifiers. We mathematically prove that the smoothed composite model can inherit the certified robustness of the robust base model under realistic assumptions. We then adapt an adversarial input detector into a (deterministic) mixing network, further improving the accuracy-robustness trade-off. Solid empirical results confirm that our method can simultaneously benefit from the high accuracy of modern pre-trained standard (non-robust) models and the robustness achieved via state-of-the-art robust classification methods. 

Because our theoretical studies demonstrate the feasibility of leveraging the mixing network to entirely eliminate the accuracy-robustness trade-off, future advancements in adversary identification can reconcile this trade-off even more effectively via our framework. Furthermore, the proposed method can be conveniently extended to various attack types and budgets. Thus, this work paves the way for future research to focus on either accuracy or robustness without sacrificing the other.



\bibliographystyle{plain}
\bibliography{journal}



\newpage
\appendix

\section{Larger Certified Robust Radius for Randomized Smoothing Base Classifiers} \label{sec:rs_radius}

In this section, we tighten the certified radius in the special case when  is a randomized smoothing classifier and the robust radii are defined in terms of the  norm. This enables us to visualize and compare the certified robustness of the mixed classifier to existing certifiably robust methods in \Cref{sec:certified_exp}.

Since randomized smoothing often operates on the probabilities and does not consider the logits, with a slight abuse of notation, we use  to denote the probabilities throughout this section (as opposed to denoting the logits in the main text).

\begin{assumption}
	\label{as:randomized_smoothing}
	The robust classifier  is a (Gaussian) randomized smoothing classifier, i.e.,  for all , where  is the output probabilities of a classifier that is non-robust in general. Furthermore, for all ,  is not 0 almost everywhere or 1 almost everywhere.
\end{assumption}

\begin{theorem}
	\label{thm:randomized_smoothing}
	Suppose that Assumption \ref{as:randomized_smoothing} holds, and let  be arbitrary. Let  and . Then, if , it holds that  for all  such that
	
\end{theorem}

\begin{proof}
	First, note that since every  is not 0 almost everywhere or 1 almost everywhere, it holds that  for all  and all . Now, suppose that , and let  be such that . Let  (conversely, ). We construct a scaled classifier , whose  entry is defined as
	
	Furthermore, define a scaled RS classifier  based on  by 
	
	Then, since it holds that
	
	it must be the case that  for all  and all , and hence, for all , the function  is -Lipschitz continuous with Lipschitz constant  (see \cite[Lemma~1]{Levine19}, or Lemma 2 in \cite{Salman19} and the discussion thereafter). Therefore,
	
	for all . Applying \cref{eq:lipschitz_inequality} for  yields that
	
	and, since  is monotonically increasing and  for all , applying \cref{eq:lipschitz_inequality} to  gives that
	
	Subtracting \cref{eq:lip_not_y} from \cref{eq:lip_y} gives that
	
	for all . By the definitions of , , and , the right-hand side of this inequality equals zero, and hence, since  is monotonically increasing, we find that  for all . Therefore,
	
	Hence,  for all , so  is certifiably robust at  with margin  and radius . Therefore, by \cref{lem:certified_radius}, it holds that  for all  such that , which concludes the proof.
\end{proof}


\subsection{Visualization of the Certified Robust Radii} \label{sec:certified_exp}

In this section of the appendix, we visualize the certified robust radii presented in \cref{thm:certified_radius} and \cref{thm:randomized_smoothing}. Again, the smoothing strength  is a fixed value. Since a (Gaussian) RS model with smoothing covariance matrix  has an -Lipschitz constant , such a model can be used to simultaneously visualize both theorems, with \cref{thm:randomized_smoothing} giving tighter certificates of robustness. Note that RS models with a larger smoothing variance certify larger radii but achieve lower clean accuracies, and vice versa. Here, we consider the CIFAR-10 dataset and select  to be a ConvNeXT-T model with a clean accuracy of , and use the RS models presented in \citep{Zhang19} as . For a fair comparison, we select an  value such that the clean accuracy of the constructed mixed classifier  matches that of another RS model  with a smaller smoothing variance. The expectation term in the RS formulation is approximated with the empirical mean of 10,000 random perturbations\footnote{The authors of \citep{Cohen19} showed that 10,000 Monte Carlo samples are sufficient to provide representative results.} drawn from , and the certified radii of  are calculated using Theorems \ref{thm:certified_radius} and  by setting  to . \Cref{fig:certified_radii} displays the calculated certified accuracies of  and  at various attack radii. The ordinate ``Accuracy'' at a given abscissa `` radius'' reflects the percentage of the test data for which the considered model gives a correct prediction and a certified radius at least as large as the  radius.

In both subplots of \Cref{fig:certified_radii}, the certified robustness curves of  do not connect to the clean accuracy when  approaches zero. This is because Theorems \ref{thm:certified_radius} and \ref{thm:randomized_smoothing} both consider robustness with respect to  and do not issue certificates to test inputs at which  makes incorrect predictions, even if  predicts correctly at some of these points. This is reasonable because we do not assume any robustness or Lipschitzness of , and  is allowed to be arbitrarily incorrect whenever the radius is non-zero.

\begin{figure*}[t]
	\centering
	\begin{subfigure}[t]{.4\textwidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[height=.74\textwidth]{Figures/CompareCert_0.pdf}
		\vspace{-6mm}
		\caption{: RS with . \hspace{14.3mm}  \.3mm]
		Consider two mixed classifier examples:  \\
		 uses  and  is RS with ;  \\
		 uses  and  is RS with . }
	\end{subfigure}
	\caption{Comparing the certified accuracy-robustness trade-off of RS models and our mixed classifier using both Lipschitz(Lip)-based certificates and RS-based certificates (Theorems \ref{thm:certified_radius} and \ref{thm:randomized_smoothing}, respectively). The clean accuracies are the same between  and  in each subfigure, and the empty circles represent certified accuracy discontinuities at radius .}
	\label{fig:certified_radii}
\end{figure*}

The Lipschitz-based bound of \cref{thm:certified_radius} allows us to visualize the performance of the mixed classifier  when  is an -Lipschitz model. In this case, the curves associated with  and  intersect, with  achieving higher certified accuracy at larger radii and  certifying more points at smaller radii. By adjusting  and the Lipschitz constant of , it is possible to change the location of this intersection while maintaining the same level of clean accuracy. Therefore, the mixed classifier structure allows for optimizing the certified accuracy at a particular radius, while keeping the clean accuracy unchanged.

The RS-based bound from \cref{thm:randomized_smoothing} captures the behavior of  when  is an RS model. For both  and , the RS-based bounds certify larger radii than the corresponding Lipschitz-based bounds. Nonetheless,  can certify more points with the RS-based guarantee. Intuitively, this phenomenon suggests that RS models can yield correct but low-confidence predictions when under attack with a large radius, and thus may not be best-suited for our mixing operation, which relies on robustness with non-zero margins. In contrast, Lipschitz models, a more general and common class of models, exploit the mixing operation more effectively. Moreover, as shown in \Cref{fig:STD+ROB}, empirically robust models often yield high-confidence predictions when under attack, making them more suitable for the robust base classifier for .



\section{Additional Analyses Regarding } \label{sec:more_R_analyses}

\subsection{The Four Options for } \label{sec:R_options}

Consider the four listed options of , namely , , , and . The constant  is a straightforward option.  comes from \cref{eq:adap_sm_2}, which is a direct generalization from the locally biased smoothing (binary classification) formulation to the multi-class case. Note that  is not practical for datasets with a large number of classes, since it requires the calculation of the full Jacobian of , which is very time-consuming. To this end, we use the gradient of the predicted class (which is intuitively the most important class) as a surrogate for all classes, bringing the formulation . Finally, unlike locally biased smoothing, which only has one differentiable component, our adaptive smoothing has two differentiable base networks. Hence, it makes sense to consider the gradient from both of them. Intuitively, if  is large, then  is vulnerable at  and we should trust it less. If , then  is vulnerable and we should trust  less. This leads to the fourth option, which is .


\subsection{Additional Empirical Supports and Analyses for Selecting } \label{sec:more_compare_R}

In this section, we use additional empirical evidence (Figures \ref{fig:compare_R_2a} and \ref{fig:compare_R_2b}) to show that  is the appropriate choice for the adaptive smoothing formulation, and that the post-SoftMax probabilities should be used for smoothing. While most of the experiments in this paper are based on ResNets, the architecture is chosen solely because of its popularity, and our method does not depend on any properties of ResNets. Therefore, for the experiment in \Cref{fig:compare_R_2a}, we select an alternative architecture by using a more modern ConvNeXT-T model \citep{Liu22} pre-trained on ImageNet-1k as . We also use a robust model trained via TRADES in place of an adversarially-trained network for . Moreover, while most of our experiments are based on  attacks, our method applies to all  attack budgets. In \Cref{fig:compare_R_2b}, we provide an example that considers the  attack. The experiment settings are summarized in \cref{tab:compare_R_settings}.

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{.48\textwidth}
		\captionsetup{justification=centering}
		\centering
		\includegraphics[width=.97\textwidth]{Figures/ConvNeXT_TRADE_Linf.pdf}
		\vspace*{-2mm}
		\caption{\footnotesize ConvNeXT-T and TRADES WideResNet-34 under  PGD attack.}
		\label{fig:compare_R_2a}
	\end{subfigure}
	\hspace{4mm}
	\begin{subfigure}[t]{.48\textwidth}
		\captionsetup{justification=centering}
		\centering
		\includegraphics[width=.97\textwidth]{Figures/ResNet_AT_L2.pdf}
		\vspace*{-2mm}
		\caption{\footnotesize Standard and AT ResNet-18s under  PGD attack.}
		\label{fig:compare_R_2b}
	\end{subfigure}
	
	\vspace{-4.5mm}
	\caption{Comparing the ``attacked accuracy versus clean accuracy'' curve of various options for  with alternative selections of base classifiers.}
	\label{fig:compare_R_2}
\end{figure}

\begin{table}[t]
	\centering
	\caption{Experiment settings for comparing the choices of .}
	\vspace{-1.5mm}
	\label{tab:compare_R_settings}
	\begin{small}
	\begin{tabular}{l|c|c|c}
		\toprule
		& PGD attack settings	&  Architecture	&  Architecture \\
		\midrule
		\Cref{fig:compare_R}    & , , 10 Steps	& Standard ResNet-18	&  AT ResNet-18				\\
		\Cref{fig:compare_R_2a} & , , 20 Steps	& Standard ConvNeXT-T &  TRADES WideResNet-34		\\
		\Cref{fig:compare_R_2b} & , , 20 Steps		  		  		& Standard ResNet-18	&  AT ResNet-18					\\
		\bottomrule
	\end{tabular}
	\end{small}
	\vspace{1.5mm}
\end{table}

Figures \ref{fig:compare_R_2a} and \ref{fig:compare_R_2b} demonstrate that setting  to the constant  achieves the best trade-off curve between clean and attacked accuracy. Moreover, smoothing using the post-SoftMax probabilities outperforms the pre-SoftMax logits. This result aligns with the conclusions of \Cref{fig:compare_R} and our theoretical analyses, demonstrating that various robust networks share the property of being more confident when classifying correctly than when making mistakes.

The most likely reason for  to be the best choice is that while the local Lipschitzness of a base classifier is a good estimator of its robustness and trustworthiness (as motivated in \citep{Anderson21b}), the gradient magnitude of this base classifier at the input is not always a good estimator of its local Lipschitzness. Specifically, local Lipschitzness, as defined in \cref{def:lipschitz}, requires the classifier to be relatively flat within an -ball around the input, whereas the gradient magnitude only focuses on the nominal input itself and does not consider the landscape within the -ball. For example, the gradient magnitude of the standard base classifier  may jump from a small value at the input to a large value at some nearby point within the -ball, which may cause  to change its prediction around this nearby point. In this case,  may be small, but  can have a high local Lipschitz constant.

As a result, while using  as  seems to make sense at first glance, it does not work as intended and can make the mixed classifier trust  more than it should. Therefore, within the -ball around a given , the attacker may be able to find adversarial perturbations at which the gradient magnitude is small, thereby bypassing the defense.

In fact, as discussed in \citep{Anderson21b}, the use of gradient magnitude results from approximating a neural classifier with a linear classifier. Our \Cref{fig:compare_R} shows that such an approximation results in a large mismatch and therefore does not make sense in our setting.
	
We also note that even if some gradient-dependent options for  work better than the constant , unless it works significantly better, the constant  should still be favored since it avoids performing backward passes within the forward pass of the mixed classifier, making the mixing formulation more efficient and less likely to suffer from gradient masking.



\section{Experiment Implementation Details}

\subsection{Implementation of the Mixing Network in Experiments} \label{sec:mixing_arch_rn}

Since our formulation does not depend on the architecture of the base classifiers, \Cref{fig:mixing_arch} presents the design of the mixing network in the context of general standard and robust classifiers. In the experiments presented in \Cref{sec:ada_exp}, Both  and  are based on variants of the ResNet family, which share the general structure of having four main blocks. Thus, we present the structure of the mixed classifier with ResNet-like base models in \Cref{fig:mixing_arch_rn}. Following \citep{Metzen17}, we consider the initial Conv2D layer and the first ResNet block as the upstream layers. The embeddings extracted by the first Conv2D layers in  and  are concatenated before being provided to the mixing network . We further select the second ResNet block as the middle layers. For this layer, in addition to concatenating the embeddings from  and , we also attach a linear transformation layer (Conv1x1) to match the dimensions, reduce the number of features, and improve efficiency.

\begin{figure}[!tb]
    \centering
    \includegraphics[width=\textwidth]{Figures/OverviewSIMODS_RN.pdf}
    \caption{The architecture of the mixed classifier introduced in \Cref{sec:ada_smo} when applied to a pair of ResNet base models.}
    \label{fig:mixing_arch_rn}
\end{figure}

As mentioned in \Cref{sec:alpha(x)}, the range of  can be constrained to be within  if certified robustness is desired. We empirically observe that setting  to be  works well for CIFAR-10, whereas  or  works well for CIFAR-100. This observation coincides with \Cref{fig:STD+ROB}, which shows that a slight increase in  can greatly enhance the robustness at the most sensitive region. The value of  can then be determined by enforcing a desired level of either clean validation accuracy or robustness. Following this guideline, we set the ranges of  to be  for the model discussed in \cref{tab:compare_cifar10}. The range is  and  respectively for the two models demonstrated in \cref{tab:compare_cifar100}. Note that this range is only applied during validation. When training , we use the full  range for its output, so that the training-time adversary can generate strong and diverse attacks that fully exploit , which is crucial for securing the robustness of the mixing network. 


\subsection{Ablation Study in Loss Function Hyperparameters} \label{sec:loss_abla}

In this section, we discuss the effects of the constants , , and  in the composite loss function \cref{eq:comp_loss}. Since multiplying the three weight constants by the same number is equivalent to using a larger optimizer step size and is not the focus of this ablation study (we focus on the loss function shape), we fix . To avoid the issue of becoming excessively conservative and always prioritizing the robust base model (as described in \Cref{sec:train_mixing_network}), we add a batch normalization layer without trainable affine transform to the output of the mixing network. Additionally, note that since the mixing network has a single output, one can arbitrarily shift this output to achieve the desired balance between clean and attacked accuracies. For a fair and illustrative comparison, after training a mixing network  with each hyperparameter setting, we add an appropriate constant to the output of the  so that the clean accuracy of the overall model  is , and compare the PGD attacked accuracy of  in \cref{tab:loss_abla}. As a baseline, when the smoothing strength  is a constant, the PGD accuracy is  when the clean accuracy is tuned to be  (the corresponding  value is 1.763). The above results demonstrate that , , and  works the best.

\begin{table}[!tb]
\centering
\caption{The PGD accuracy on CIFAR-10 with various loss hyperparameter settings. The setting is the same as in \cref{tab:cifar10}, and we consider both attack and defense in Setting B.}
\label{tab:loss_abla}
\begin{small}
\begin{tabular}{l!{\vrule width 2pt}c|c|c|c}
	\toprule
	&  &  &  &  \\
	&  &  &  &  \\
	\midrule
		& 54.5 \% & 52.8 \% & 53.8 \% & 54.4 \% \\
		& 54.3 \% & 54.1 \% & 54.0 \% & 54.1 \% \\
		& 55.1 \% & 54.2 \% & 54.3 \% & 53.9 \% \\
	\bottomrule
\end{tabular}
\end{small}
\end{table}

Our results also show that a small positive  is generally beneficial. This makes sense because the CE loss is low for a particular input if both  and  correctly predict its class. Thus, the smoothing strength should not matter for such input, and therefore the BCE loss is weighted by a small number. Compared with using only the BCE loss, the product term of the CE and the BCE components is lenient on inputs correctly classified by the mixed model , while assigning more weight to the data that are incorrectly predicted.

\begin{table}
\centering
\caption{Compare selections of the mixing network's Sigmoid activation scaling factor.}
\label{tab:scale_abla}
\begin{small}
\begin{tabular}{c|c|c|c}
	\toprule
	 &  &  &  \\
	\midrule
	55.1 \% & 55.5 \% & 55.7 \% & 55.6 \% \\
	\bottomrule
\end{tabular}
\end{small}
\end{table}

Recall that the output range of  is , which is enforced by appending a Sigmoid output activation function. In addition to shifting, one can arbitrarily scale the Sigmoid activation's input. By performing this scaling, we effectively calibrate the confidence of the mixing network. In \cref{tab:loss_abla}, this scaling is set to the same constant for all settings. In \cref{tab:scale_abla}, we select the best loss parameter and analyze the validation-time Sigmoid scaling. Again, we shift the Sigmoid input so that the clean accuracy is . While a larger scale benefits the performance on clean/attacked examples that are confidently recognized by the mixing network, an excessively large scale makes  less stable under attack. \cref{tab:scale_abla} shows that applying a scaling factor of  yields the best result for the given experiment setting.


\subsection{Calculating the AutoAttacked Robust Confidence Gap} \label{sec:autoattack_margin}

The original AutoAttack implementation released with \citep{Croce20a} does not return perturbations that fail to change the model prediction. To enable robustness margin calculation, we modify the code to always return the perturbation that achieves the smallest margin during the attack optimization. Since the FAB and Square components of AutoAttack are slow and do not successfully attack additional images on top of the APGD components, we only consider the two APGD components for the purpose of margin calculation.



\end{document}
