\documentclass[prodmode,acmtalg]{acmsmall}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\pagestyle{plain}
\usepackage{srcltx}

\let\accentvec\vec
\let\spvec\vec
\let\vec\accentvec

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[textsize=tiny]{todonotes}


\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand{\matodo}[1]{\todo[color=green!30]{#1}}
\newcommand{\mdtodo}[1]{\todo[color=red!30]{#1}}
\newcommand{\pwtodo}[1]{\todo[color=blue!30]{#1}}

\newcommand{\HH}{\mathcal{H}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\ZZ}{\mathcal{Z}}
\newcommand{\ex}{\textnormal{\textrm{ex}}}
\newcommand{\E}{\textnormal{\textrm{E}}}

\def\CS#1{\ensuremath{\textnormal{\textsf{CS}}^{(#1)}}}
\def\LCY#1{\ensuremath{\textnormal{\textsf{LCY}}^{(#1)}}}
\def\Lcyc{\ensuremath{\textnormal{\textsf{LCY}}}}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\def\IIN{\mathbb{N}}
\newcommand{\cc}{\textnormal{\textrm{cc}}}
\newcommand{\paren}[1]{{\left({#1}\right)}}
\newcommand{\good}[3]{\ensuremath{\mathcal{#1}^{#2}_{\textnormal{\textrm{good}}}
(#3) } }
\newcommand{\bad}[3]{\ensuremath{\mathcal{#1}^{#2}_{\textnormal{\textrm{bad}}}
(#3) } }
\newcommand{\abs}[1]{{\left\lvert{#1}\right\rvert}}
\def\LL{\ensuremath{\textnormal{\textsf{LL}}}}
\def\LLtwo{\ensuremath{\textnormal{\textsf{LL}}^{(2)}}}
\newcommand{\MaxLoop}{\ensuremath{\mathrm{MaxLoop}}}
\newcommand{\bG}{\ensuremath{G}}
\newcommand{\rmTheta}{\mathrm{\Theta}}
\newcommand{\addcost}{\ensuremath{A(n)}}
\usepackage{pgfplots}
\newcommand{\Cpp}{C{}\texttt{++}{}}
\newcommand{\Samplesize}{\ensuremath{n_{\textnormal{\textrm{s}}}}}

\markboth{M. Aum\"{u}ller and M. Dietzfelbinger}{Optimal Partitioning For Dual-Pivot Quicksort}

\title{Optimal Partitioning for Dual-Pivot Quicksort}

\author{Martin Aum\"{u}ller and Martin
Dietzfelbinger \affil{Technische Universität Ilmenau}}
\begin{abstract}
    \emph{Dual-pivot quicksort} refers to variants of classical quicksort
		where in the partitioning step two pivots are used to split the input into three segments.
		This can be done in different ways, giving rise to different algorithms. 
		Recently, a dual-pivot algorithm proposed by Yaroslavskiy
    received much attention, because a variant of it replaced the
    well-engineered quicksort algorithm in Sun's Java 7 runtime library.
		Nebel and Wild (ESA 2012) analyzed this algorithm and showed that on average it 
		uses  comparisons to sort an input of size ,
		beating standard quicksort, which uses  comparisons.
		We introduce a model that captures all dual-pivot algorithms,
		give a unified analysis, and identify new dual-pivot algorithms that
		minimize the average number of key comparisons among all
		possible algorithms up to a linear term. 
		This minimum is . For the case that the
		pivots are chosen from a small sample, we include a comparison
		of dual-pivot quicksort and classical quicksort. Specifically, 
                we show that dual-pivot quicksort benefits from a skewed choice of pivots.
                We experimentally evaluate our algorithms and compare them to
                Yaroslavskiy's algorithm and the recently described -pivot quicksort
                algorithm of Kushagra et al. (ALENEX 2014).
\end{abstract}
\category{F.2.2}{Nonnumerical Algorithms and Problems}{Sorting and searching}
\terms{Algorithms}
\keywords{Sorting, Quicksort, Dual-Pivot}
\acmformat{Martin Aum\"{u}ller and Martin Dietzfelbinger. 2014. Optimal Partitioning for Dual-Pivot Quicksort.}
\begin{document}
\begin{bottomstuff}
    A preliminary version of this work appeared in the \emph{Proceedings of the 40th International Colloquium
    on Automata, Languages, and Programming (ICALP)}, Part 1, 33--44, 2013.

    Author's addresses: M. Aum\"{u}ller; M. Dietzfelbinger, Fakultät für Informatik und Automatisierung, Techni\-sche 
    Universität Ilmenau, 98683 Ilmenau, Germany; e-mail: \{martin.aumueller,martin.dietzfelbinger\}@tu-ilmenau.de
\end{bottomstuff}
\maketitle
    

\section{Introduction}\label{sec:introduction}

Quicksort~\cite{Hoare} is a thoroughly analyzed classical sorting algorithm,
described in standard textbooks such as~\cite{CLRS,Knuth,FlajSedg} 
and with implementations in practically all algorithm libraries. 
Following the divide-and-conquer paradigm, on an input consisting of  elements
quicksort uses a pivot element to 
partition its input elements into two parts, 
the elements in one part being smaller than or equal to the pivot, the elements in the other 
part being larger than or equal to the pivot, and then uses recursion to sort these parts. 
It is well known that if the input consists of
 elements with distinct keys in random order and the pivot is picked by just
choosing an element then on average quicksort uses  comparisons.\footnote{
In this paper  denotes the natural logarithm and  denotes the logarithm to base .}
In 2009, Yaroslavskiy announced\footnote{An archived version of the relevant discussion in a Java newsgroup can be found at\\
    \texttt{http://permalink.gmane.org/gmane.comp.java.openjdk.core-libs.devel/2628}. Also see~\cite{nebel12}.} 
that he had found an improved quicksort implementation, the claim being backed by experiments. 
After extensive empirical studies, in 2009 a variant of Yaroslavskiy's
algorithm due to Yaroslavskiy, Bentley, and Bloch became the new standard quicksort algorithm in Sun's Java 7 runtime library. 
This algorithm employs two pivots to split the elements.
If two pivots  and  with  are used, 
the partitioning step partitions the remaining  elements into 3 parts:
elements smaller than or equal to , 
elements between  and , and elements larger than or equal to ,
see~Fig.~\ref{fig:dual:pivot:partition}.\footnote{In accordance with tradition, we assume in this theoretical study that all elements have different keys. 
Of course, in implementations equal keys are an important issue that requires a lot of care~\cite{SedgewickEqual}.}
Recursion is then applied to the three parts.
As remarked in \cite{nebel12}, it came as a surprise that two pivots should help, since in his thesis~\cite{sedgewick}
Sedgewick had proposed and analyzed  a dual-pivot
approach that was inferior to classical quicksort. Later, Hennequin in his
thesis~\cite{hennequin}
studied the general approach of using  pivot elements. According to \cite{nebel12}, he found only
slight improvements that would not compensate for the more involved partitioning procedure. 
(See \cite{nebel12} for a short discussion.)

\begin{figure}[tb]
    \centering
    \scalebox{0.5}{\includegraphics{partition}}
    \caption{Result of the partition step in dual-pivot quicksort schemes using
    two pivots  with . Elements left of  are smaller than or equal to , 
    elements right of  are larger than or equal to . The elements between  and 
    are at least as large as  and at most as large as .}
    \label{fig:dual:pivot:partition}
\end{figure}

In \cite{nebel12} (see also the full version \cite{WildNN15}), Nebel and Wild
formulated and analyzed a simplified version
of Yaroslavskiy's algorithm. (For completeness, this algorithm is given as
Algorithm~\ref{algo:yaroslavskiy:partition} in
Appendix~\ref{app:sec:yaroslavskiy}.) They showed that it makes  key comparisons on average, in contrast to the  of standard
quicksort and the  of Sedgewick's dual-pivot
algorithm. On the other hand, they showed that the number of swap operations in
Yaroslavskiy's algorithm  is  on average, which is much
higher than the  swap operations in classical quicksort.  In
this paper we concentrate on the comparison count as
cost measure and on asymptotic results. We leave the study of other cost measures
to further investigations (which already have taken place, see \cite{MartinezNW15}).
We consider other measures in experimental evaluations.

The authors of \cite{nebel12} state that the reason for Yaroslavskiy's algorithm
being superior were that his ``partitioning method is able to take advantage of
certain asymmetries in the
outcomes of key comparisons''. They also state that ``[Sedgewick's dual-pivot method] fails to utilize them, even though
being based on the same abstract algorithmic idea''.  
So the abstract algorithmic idea of using two pivots can lead to different algorithms with different behavior. 
In this paper we describe the design space from which all these algorithms originate.
We fully explain which simple property makes some dual-pivot algorithms perform
better and some perform worse w.r.t. the average comparison count
and identify optimal members (up to a linear term) of this design
space. The best ones use  comparisons on average---even less than Yaroslavskiy's
method.

The first observation is that everything depends on the cost, i.\,e., the comparison count, of the partitioning
step.
This is not new at all. Actually, in Hennequin's thesis~\cite{hennequin}
the connection between partitioning cost and overall cost 
for quicksort variants with more than one pivot is analyzed in detail.
For dual-pivot quicksort, Hennequin proved that 
if the (average) partitioning cost for  elements is , for a constant , then the average cost for sorting  elements is 

The partitioning cost of some algorithms presented in this paper will have a non-constant lower order term.
Utilizing the Continuous Master Theorem of \cite{Roura01},
we prove that \eqref{eq:1} describes the average cost even if the partitioning cost is  for 
some .
Throughout the present paper all that interests us is the constant factor with the leading term.
(The reader should be warned that for real-life  the linear term,
which can even be negative, can have a big influence
on the average number of comparisons.)

The second observation is that the partitioning cost depends on certain details of the 
partitioning procedure. This is in contrast to standard quicksort with one pivot
where partitioning always takes  comparisons. In \cite{nebel12} it is shown
that Yaroslavskiy's partitioning procedure uses  comparisons on average,
while Sedgewick's uses  many. The analysis of these two
algorithms is based on the study of how certain pointers move through the array,
at which positions elements are compared to the pivots, which of the two pivots
is used for the first comparison, and how swap operations
exchange two elements in the array.
For understanding what is going on, however, it is helpful to forget about 
concrete implementations with loops in which pointers sweep across arrays and entries are swapped,
and look at partitioning with two pivots in a more abstract way.
For simplicity we shall always assume that the input is a permutation of .
Now pivots  and  with  are chosen. 
The task is to \emph{classify} the remaining  elements into classes 
``small'' ( many), ``medium'' ( many), and ``large'' ( many), 
by comparing these elements one after the other with the smaller pivot or
the larger pivot, or both of them if necessary. 
Note that for symmetry reasons it is inessential in which order the elements are treated.
The only choice the algorithm can make is
whether to compare the current element with the smaller pivot or the larger pivot first. 
Let the random variable  denote the number of small elements compared with the larger pivot first,
and let  denote the number of large elements compared with the smaller pivot first.
Then the total number of comparisons is 


Averaging over all inputs and all possible choices of the pivots 
the term  will lead to  key comparisons on average,
independently of the algorithm. 
Let ,
the number of elements that are compared with the ``wrong'' pivot first.
Then  is the only quantity that is influenced by a particular partitioning procedure.


In the paper, we will first devise an easy method to calculate . 
The result of this analysis will lead to an asymptotically optimal strategy.
The basic approach is the following. Assume a partitioning procedure is given, and 
assume  and hence  and  are fixed, 
and let .
Denote the average number of elements compared to the smaller [larger] pivot first by  [].
If the elements to be classified were chosen to be small, medium, and large independently
with probabilities , , and , resp., then the
average number of 
small elements compared with the large pivot first would be ,
similarly for the large elements. 
Of course, the actual input is a sequence with exactly  [, ] small [medium, large] elements, and there is no independence. 
Still, we will show that the randomness in the order is sufficient to guarantee that, for some ,

The details of the partitioning procedure will determine  and , and hence
 up to .
This seemingly simple insight has two consequences, one for the analysis and one for the design of dual-pivot algorithms: 
\begin{itemize}
	\item[(i)] In order to \emph{analyze} the average comparison count of a
	    dual-pivot algorithm (given by its partitioning procedure) up to a linear term,
	determine  and  for this partitioning procedure. This will give
     up to , which must then be averaged over all  to find the
	average number of comparisons in partitioning. Then apply \eqref{eq:1}.
	\item[(ii)] In order to \emph{design} a good partitioning procedure w.r.t.
	    the average comparison count, try to
	    make  small.
\end{itemize}
We shall demonstrate approach (i) in Section~\ref{sec:methods}. An example: 
As explained in \cite{nebel12}, if  and  are fixed,  
in Yaroslavskiy's algorithm we have  and . 
By \eqref{eq:small:large:200} we get .
This must be averaged over all possible values of  and . The result is
,
which together with  gives , close
to the result  from \cite{nebel12}. 

Principle (ii) will be used to identify an asymptotically optimal partitioning procedure 
that makes 
key comparisons on average. 
In brief, such a strategy should achieve the following: 
	If , compare (almost) all entries with the smaller pivot first ( and ),
	otherwise compare (almost) all entries with the larger pivot first
	( and ). 
	Of course, some details have to be worked out:
	How can the algorithm decide which case applies? In which technical sense is this strategy optimal?
	We shall see in Section~\ref{sec:decreasing} how a sampling technique resolves these issues.
	

        In 
Section~\ref{sec:optimal:strategies}, we will consider the following simple and intuitive strategy:
\emph{If more elements have been classified as being large instead of being small so far,
compare the next element to the larger pivot first, otherwise compare it to the smaller
pivot first.}
We will show
that this strategy is optimal w.r.t. minimizing the average comparison count.

In implementations of quicksort, the pivot is usually chosen as the median 
from a small sample of  elements. Intuitively, this yields more balanced subproblems, 
which are smaller on average.
This idea already appeared in Hoare's original publication \cite{Hoare} without 
an analysis. This was later supplied by van Emden \cite{vanEmden}. The complete analysis of
this variant was given by Martinez and Roura in \cite{MartinezR01} in 2001. 
They showed that the optimal sample size is . For this
sample size the 
average comparison count of quicksort
matches the lower-order bound of  comparisons. 
In practice, one usually uses a sample of size . Theoretically, this decreases
the average comparison count from  to .
This strategy has been generalized in the obvious way to Yaroslavskiy's
algorithm as well. The implementation of Yaroslavskiy's algorithm in Sun's Java 7 uses
the two tertiles in a sample of size  as pivots, i.e., the elements of rank  and .
In
Section~\ref{sec:pivot:sample} we will analyze the comparison count of dual-pivot
quicksort algorithms with this sampling strategy. 
Yaroslavskiy's algorithm has an average comparison count of  
in this case, while the optimal average cost is .
In that section, we will also consider a question raised by Wild, Nebel, and Mart\'inez
in \cite[Section 8]{WildNM14} for Yaroslavskiy's algorithm, namely of which rank the
pivots should be to achieve  minimum sorting cost. While using
the tertiles of the input 
seems the obvious choice for balancing reasons, in \cite{WildNM14} it is shown
that for Yaroslavskiy's algorithm this minimum is attained for ranks
 and  and is . We will show that the simple strategy \emph{``Always
  compare with the larger pivot first''} achieves sorting cost ,
  i.\,e., the lower bound for comparison-based sorting,  when choosing the
  elements of rank  and  as the two pivots.

As noted in \cite{nebel13}, considering only key comparisons
and swap operations does not suffice for evaluating the practicability of
sorting algorithms. In Section~\ref{sec:experiments}, we will present
experimental results that indicate the following: When sorting integers, the
comparison-optimal algorithms of Section~\ref{sec:decreasing} and
Section~\ref{sec:optimal:strategies} are slower than Yaroslavskiy's
algorithm. However, an implementation of the simple strategy \emph{``Always compare
with the larger pivot first''} performs very well both in {\Cpp} and in Java in
our experimental setup.  We will also compare our algorithms to the fast
three-pivot quicksort algorithm described in \cite{Kushagra14}.
While comparing these algorithms, we will provide evidence that the theoretical cost measure ``cache misses'' 
described in \cite{Kushagra14} nicely predicts empirical cache behavior,
and comes closest for correctly predicting running time.

We emphasize that the purpose of this paper is not to arrive at better and
better quicksort algorithms by using all kinds of variations, but rather to
thoroughly analyze the situation with two pivots, showing the potential and the
limitations of this approach.  

\section{Basic Approach to Analyzing Dual-Pivot
Quicksort}\label{sec:average:case:analysis}
We assume the input sequence  to be a random permutation of ,
each permutation occurring with probability . If , there is
nothing to do; if , sort by one comparison. Otherwise, choose the first element  and
the last element  as the set of pivots, and set 
and . Partition the remaining elements into elements smaller
than 
(``small'' elements), 
elements between  and  (``medium'' elements), and elements larger than 
(``large'' elements), see Fig.~\ref{fig:dual:pivot:partition}.
Then apply the procedure recursively to these three 
groups.
Clearly, each pair  with 
appears as set of pivots with probability .
Our cost measure is the number of key comparisons needed to
sort the given input. Let
 be the random variable counting this number. Let  denote the
partitioning cost to partition the  non-pivot elements into the three
groups. As
explained by Wild and Nebel \cite[Appendix A]{nebel12}, the average number of
key comparisons obeys the following recurrence: 

If , for a constant , this can
be solved (\emph{cf.} \cite{hennequin,nebel12}) to give

In Section~\ref{sec:additional:cost:term} we will show that \eqref{eq:10} also holds
if  for some . 
For the proof we utilize the Continuous Master Theorem from \cite{Roura01}.

In view of this simple relation it is sufficient to study the cost of partitioning.
Abstracting from moving elements around in arrays, we arrive at the following
``classification problem'':
Given a random permutation  of  as the input
sequence and  and  as the two pivots  and , with ,
classify each of the remaining  elements as being small, medium, or large.
Note that there are exactly
 small elements,  medium elements, and  large
elements. 
Although this
classification does not yield an actual partition of the input sequence, 
a classification algorithm can be turned into a partitioning algorithm using
only swap operations
but no additional key comparisons. Since elements are only
compared with the two pivots, the randomness of subarrays is preserved. Thus,
in the recursion we may always assume that the input is arranged randomly.

We make the following observations (and fix notation) for all classification
algorithms.  One key comparison is needed to decide which of the elements 
and  is the smaller pivot  and which is the larger pivot .  For
classification, each of the remaining  elements has to be compared against
 or  or both. Each \emph{medium} element has to be compared to 
\emph{and} . On average, there are  medium elements. Let 
denote the number of small elements that are compared to the larger pivot first,
i.\,e., the number of small elements that need  comparisons for classification.
Analogously, let  denote the number of large elements compared to the
smaller pivot first. Conditioning on the pivot choices, and hence the values of
 and , we may calculate  as follows:\footnote{We use
the following notation throughout this paper: To indicate that sums run over all 
 combinations  with  and 
we simply write .}

We call the third summand in \eqref{eq:30} the \emph{additional cost term (ACT)}, as it is the only
value that depends on the actual classification algorithm.

\section{Analyzing the Additional Cost Term}\label{sec:additional:cost:term}

We will use the following formalization of a partitioning
procedure: A \emph{classification
strategy} is given as a three-way
decision tree  with a root and  levels numbered 
of inner nodes as well as one
leaf level. The root is on level . Each node  is labeled with an index
 and an element . If
 is , then at node  element  is compared with the
smaller pivot first; otherwise, i.\,e., , it is compared with the
larger pivot first. 
 The three edges out of a node are labeled  resp.,
representing the outcome of the classification as small, medium, large,
respectively. The label of edge  is called . 
On each of the  paths each index occurs exactly
once.  Each input
determines exactly one path  from the root to a leaf in the
obvious way; the
classification of the elements can then be read off from the node and edge
labels along this path. We call such a tree a \emph{classification tree}.
 \newcommand{\lv}[1]{\ensuremath{\textnormal{\textrm{level}}(#1)}}


\begin{figure}[bt]
    \centering
    \scalebox{0.65}{\includegraphics{decision_tree}}
    \caption{An example for a decision tree to classify three elements  
    and  according to the pivots  and . 
    Five out of the  leaves are explicitly drawn, showing the classification of the elements and 
    the costs  of the specific paths.}
    \label{fig:decision:tree:ex1}
\end{figure}

Identifying a path  from the root to a leaf  by the sequence of nodes and 
edges  on it, we define the cost  as 

For a given input, the cost of the path associated with this input exactly describes the number of additional
comparisons on this input. An example for such a classification tree is given in Figure~\ref{fig:decision:tree:ex1}.

We let  [] denote the random variable that for a random input
counts the number of small [large] elements classified in nodes with label
 [].  We now describe how we can calculate the ACT of a
classification tree . First consider fixed  and  and let the input
excepting the pivots be arranged randomly.  For a node  in ,  we let
, , and , resp., denote the number of edges labeled ,
, and , resp., from the root to . By the randomness of the
input, the probability that the element classified at  is ``small'', i.\,e.,
that the edge labeled  is used,  is exactly . The probability that it is ``medium'' is , and that it is ``large'' is . The
probability  that node  in the tree is reached is then just
the product of all these edge probabilities on the unique path from the root to
.  The probability that the edge labeled  out of a node  is used
can then be calculated as .
Similarly, the probability that the edge labeled  is used is
. Note that all this is
independent of the actual ordering in which the classification tree inspects
the elements. We can thus always assume some fixed ordering and forget about
the label  of node .
    
 By linearity of expectation, we can sum up the
    contribution to the additional comparison count for each node separately.
    Thus, we may calculate
   
The setup developed so far makes it possible to describe the connection between a classification tree  
and its average comparison count in general. 
Let  and  be two random variables that denote the
number of elements that are compared
with the smaller and larger pivot first, respectively,  when using . 
Then let  resp. 
 denote the average number of
comparisons with the larger resp. smaller pivot first, given  and . 
Now, if it was decided in each step by independent random experiments
with the correct expectations , , and , resp.,
whether an element is small, medium, or large, it would be clear that for example  is the average number of small elements that are compared with the larger pivot
first. The next lemma shows that one can indeed use this intuition in the
calculation of the average comparison count, except that one gets an additional  term due to 
the elements tested not being independent. 

\begin{lemma}\label{lemma:01}
Let  be a classification tree. Let  be the average number of 
key comparisons for classifying an input of  elements using . 
Then there exists a constant  such that 
\begin{rm}
\end{rm}\end{lemma}
\begin{proof}
    Fix  and  (and thus  and ). We will show that 
    
    (The lemma then follows by substituting this into \eqref{eq:30}.)

    We call a node  in  \emph{on-track} (to the expected values)  if 
    
    Otherwise we call  \emph{off-track}.
    
    
    We first obtain an upper bound. Starting from \eqref{eq:35}, we
    calculate:
    
where the last step follows by separating on-track and off-track nodes and using
\eqref{eq:v:good}.  (For off-track nodes we use that the left-hand side of the
inequalities in \eqref{eq:v:good} is at most .) For the sums in the last line
of \eqref{eq:act:a}, consider each level of the classification tree separately.
Since the probabilities  for nodes  on the same level sum up to
, the contribution of the  terms is bounded by . Using
the definition of  and , we continue
as follows:


where in the last step we just rewrote the sum to consider each level in the classification
tree separately.
So, to show \eqref{eq:20000} it  remains to
bound the sum in \eqref{eq:10000} by . 

To do this, consider a random input that is classified using .
Using an appropriate tail bound, viz. \emph{the method of average bounded differences},  
we will show that with very high probability we do not reach an off-track node in the
classification tree in the first  levels. Intuitively, this means 
that it is highly improbable that underway the observed fraction of small elements 
deviates very far from the average . 

Let  be the - random variable that is

if the -th classified element is small; let  be the - random
variable that is  if the -th classified element is large. Let  and  for .

\begin{claim}
    Let . Then 
    
\end{claim}

\begin{proof}
    We prove the first inequality. 
    First, we bound the difference  between the expectation of
     conditioned on  resp.  for .
    Using linearity of expectation we may calculate 
    
In this situation we may apply the bound known as the method of averaged bounded differences (see
    \cite[Theorem 5.3]{dp09}), which reads
    
    and get 
    
    which is not larger than .\qed
\end{proof}

Assume that . We
have

That means that for each of the first  levels with very high
probability we are in an \emph{on-track node} on level , because the deviation from the ideal case
that we see a small element with probability  is . 
Thus, for the first 
levels the contribution of the sums
of the probabilities of off-track nodes in \eqref{eq:10000} is at most . For the last
 levels of the tree, we use that the contribution of the  probabilities
that we reach an off-track node on level  is at most  for a fixed level. 

This shows that the contribution of the sum in \eqref{eq:10000} is . This 
finishes the proof of the upper bound on  given in \eqref{eq:10000}.
The calculations for the lower bound are similar and are omitted here. \qed
\end{proof}
There is the following technical complication when using this lemma in analyzing a strategy 
that is turned into a dual-pivot quicksort algorithm:
The cost bound is , and  Hennequin's result (Equation \eqref{eq:10}) cannot be applied directly to such
partitioning costs. 
However, the next theorem says that the leading term of \eqref{eq:10} applies
to this situation as well, and the additional  term 
in the partitioning cost
is completely covered in the  error term of \eqref{eq:10}.
\begin{theorem}\label{thm:10}
Let  be a dual-pivot quicksort algorithm that gives rise to
a classification tree  for each subarray of length . Assume
 for all , for some constants  and . 
Then .
\end{theorem}
\begin{proof}
By linearity of expectation we may split the partitioning cost into two terms  and 
, solve recursion \eqref{eq:25} independently for these two
cost terms, and add the results. Applying \eqref{eq:10} for average partitioning cost  yields 
an average comparison count of . Obtaining the bound of  for the 
term   is a standard application
of the Continuous Master Theorem of Roura \cite{Roura01}, and has been derived for the dual-pivot
quicksort recurrence by Wild, Nebel, and Martínez in a recent technical report
\cite[Appendix D]{WildNM14}. For completeness, 
the calculation is given in Appendix~\ref{app:proof:thm:10}.
\end{proof}
Lemma~\ref{lemma:01} and Theorem~\ref{thm:10} tell us that for the analysis of the 
average comparison count of a dual-pivot quicksort algorithm we just have to find out what 
 and  are for
this algorithm. Moreover, to design a good algorithm (w.r.t. the average
comparison count), we should try to make  small for each pair . 

\section{Analysis of Some Known Classification Strategies}\label{sec:methods}
In this section, we will study different classification strategies in the light
of the formulas from
Section~\ref{sec:additional:cost:term}.  

\paragraph{Oblivious Strategies}
We will first consider strategies that do not use information of previous
classifications for future classifications. To this end, we call a classification tree
\emph{oblivious} if for each level all nodes  on this level share the same
label . 
This means that
these algorithms do not react
to the outcome of previous classifications, but use a fixed sequence of pivot
choices.
Examples for
such strategies are, e.\,g., 
\begin{itemize}
    \item always compare to the smaller pivot first, 
    \item always
compare to the larger pivot first,
    \item alternate the pivots in each step.
\end{itemize}
Let  denote the average number of comparisons to the larger pivot
first. By assumption this value is independent of  and . Hence these
strategies make sure that  and
 for all pairs of values .

Applying Lemma~\ref{lemma:01} gives us 

Using Theorem~\ref{thm:10} we get ---the leading term being the same as in standard quicksort. 
So, for each strategy that does not adapt to the outcome of previous
classifications, there is no difference to the average
comparison count of classical quicksort. Note that this also holds for
\emph{randomized strategies} such as ``flip a coin to choose the pivot used in the
first comparison'', since such a strategy can be seen as a probability
distribution on oblivious strategies.


\paragraph{Yaroslavskiy's Algorithm}
Following \cite[Section 3.2]{nebel12}, Yaroslavskiy's algorithm is an
implementation of the following
strategy : \emph{Compare
  elements to  first, and compare the other elements to  first.}
We get that  and
. 
Applying Lemma~\ref{lemma:01}, we get 
 
Of course, it is possible to evaluate this sum by hand. We used Maple\textsuperscript{\textregistered} to obtain 
.
Using Theorem~\ref{thm:10} gives  
, as in \cite{nebel12}.

\paragraph{Sedgewick's Algorithm}\label{sec:sedgewick}
Following \cite[Section 3.2]{nebel12}, Sedgewick's algorithm amounts to an
implementation of the following strategy
: \emph{Compare (on average) a fraction of
 of the keys with  first, and compare the other keys with
 first.}
We get  and .

Plugging these values into Lemma~\ref{lemma:01}, we calculate 



Applying Theorem~\ref{thm:10} gives , as known from \cite{nebel12}.

Obviously, this is worse than the oblivious strategies considered before.\footnote{We remark that in his thesis
Sedgewick \cite{sedgewick} focused on the average number of swaps, not on the
comparison count.}   This is
easily explained intuitively: If the fraction of small elements is large, it
will compare many elements with  first. But this costs two comparisons for
each small element. Conversely, if the fraction of large elements is large, it
will compare many elements to  first, which is again the wrong decision.

Since Sedgewick's strategy seems to do exactly the opposite of what one should
do to lower the comparison count, we consider
the following modified 
strategy : \emph{For given 
and , compare (on average) a fraction of
 of the keys with  first, and compare the other keys with
 first.} ( simply uses  first when  would use
 first and vice versa.)

Using the same analysis as above, we get , which yields ---improving on the standard algorithm and even on
Yaroslavskiy's algorithm! Note that this has been observed by Wild in his Master's Thesis as well \cite{Wild2013}.

\paragraph{Remark} Swapping the first comparison with  and  as in the strategy described above is a
general technique. In fact, if the
leading coefficient of the average number of comparisons for a fixed rule 
for choosing  or  first is , e.\,g.,  for strategy
,  then the leading coefficient of the strategy that
does the opposite is , e.\,g.,  as in
strategy .



\section{An Asymptotically Optimal Classification Strategy}\label{sec:decreasing}
Looking at the previous sections, all strategies used the idea that we should
compare a certain fraction of elements to  first, and the other
elements to  first. In this section, we
will study the following strategy : \emph{If  then always compare with  first, otherwise always compare with 
first.} 

Of course, for an implementation of this strategy we have to deal with the
problem of finding out which case applies before the comparisons have been made.
We shall analyze a guessing strategy to resolve this. 

    \subsection{Analysis of the Idealized Classification Strategy}\label{sec:optimal:strategy}
 Assume for a moment that for a given random input with pivots  the
 strategy ``magically'' knows whether
  or not and correctly determines the pivot that should be used for all
 comparisons. For fixed  and  this means that for  the
 classification strategy makes exactly  additional comparisons, and for  it
 makes  additional comparisons.

 When we start from \eqref{eq:30}, 
 a standard calculation shows that for this strategy
 
 Applying \eqref{eq:10}, we get
,
which is by  smaller than the average number of key
comparisons in Yaroslavskiy's algorithm.

To see that this method is asymptotically optimal, 
recall that according to
Lemma~\ref{lemma:01} 
the average comparison count is determined up to a linear term by
the parameters  and
.
Strategy  chooses these values such that  is
either  or , minimizing each term of the sum in
Lemma~\ref{lemma:01}---and thus minimizing the sum.

\subsection{Guessing Whether  or not}
We explain how the idealized classification strategy just described can be approximated by an
implementation. The idea is to make a few comparisons and use the outcome
as a basis for a guess.

After  and  are chosen,  classify the first  many
elements (the \emph{sample})
and calculate  and , the number of
small and large elements in the sample. If , compare the remaining
 elements with  first, otherwise  compare them with
 first. We say that the guess was \emph{correct} if 
 and  or  and . In order not to clutter up formulas,
we will always assume that  is an integer. One would otherwise
work with . 

We incorporate guessing errors and sampling cost into \eqref{eq:80} as follows:
-1em]&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\,\,\Pr(\text{guess wrong} \mid s, \ell) \cdot
	\max(s, \ell)\bigg) + O(n^{1 - \varepsilon})\notag\-1em]
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\,\,\,\,\,\Pr(\text{guess
wrong} \mid s, \ell) \cdot \ell\bigg) + O(n^{1 - \varepsilon}),

	c_i = \bigl|\E(d \mid X_1,\ldots,X_i) - \E(d \mid X_1, \ldots, X_{i-1})\bigr| \leq
    3, \text{ for  } 1\leq i \leq \Samplesize.
    
        |&\E(d \mid X_1, \ldots, X_i) - \E(d \mid X_i,\ldots, X_{i-1})|\\
    	&= \Bigg| \sum_{j = 1}^i{X_j} + \sum_{j = i+1}^{\Samplesize} \bigg[\Pr(X_j = 1 \mid X_1,
    \ldots, X_i) - \Pr(X_j = -1 \mid X_1,\ldots, X_i)\bigg] \\ &\quad\quad- \sum_{j =
    1}^{i-1}{X_j}- \sum_{j = i}^{\Samplesize} \bigg[\Pr(X_j = 1 \mid X_1,
    \ldots, X_{i-1}) - \Pr(X_j = -1 \mid X_1,\ldots, X_{i-1})\bigg]\Bigg|\\
    &= \Bigg| X_i + \sum_{j = i + 1}^{\Samplesize} \left[\frac{s_i}{n - i} -
    \frac{\ell_i}{n-i}\right] - \sum_{j = i}^{\Samplesize} \left[\frac{s_{i - 1}}{n-i + 1} -
    \frac{\ell_{i - 1}}{n-i+1}\right]\Bigg|\\
    &= \Bigg| X_i + (\Samplesize - i)\cdot  \left[\frac{s_{i-1} - Y_i}{n - i} -
    \frac{\ell_{i-1} - Z_i}{n-i}\right] - (\Samplesize - i +1) \cdot \left[\frac{s_{i - 1}}{n-i + 1} -
    \frac{\ell_{i - 1}}{n-i+1}\right]\Bigg|\\
    &= \Bigg| X_i  + \frac{(\Samplesize-i) \cdot (Z_i - Y_i)}{n-i} + s_{i - 1}
    \left[\frac{\Samplesize-i}{n - i} -
    \frac{\Samplesize-i+1}{n-i+1}\right] + \ell_{i - 1} \left[\frac{\Samplesize - i + 1}{n-i+1} {-}
    \frac{\Samplesize - i}{n-i}\right]\Bigg|\\
    &= \Bigg|X_i  + \frac{(\Samplesize-i) \cdot (Z_i - Y_i)}{n-i} + (\ell_{i -
    1} - s_{i - 1}) \cdot \frac{n-\Samplesize}{(n-i)(n-i+1)}
    \Bigg|\\
    &\leq \left\vert X_i\right\vert + \left\vert Z_i - Y_i \right \vert +  \left\vert\frac{\ell_{i-1} - s_{i - 1}}{n - i + 1}\right\vert \leq 3.
    
	\Pr(d > \E(d) + t) \leq \exp\left(-\frac{t^2}{2\sum_{i \leq
	\Samplesize}c_i^2}\right), \text{ for } t > 0,
    
	\Pr(d > 0) \leq \exp\left(-\frac{n^{1/6}}{18}\right).\tag*{\qed}
    \frac{2}{\binom{n}{2}} \sum_{\ell =
		0}^{n^{3/4}}\sum_{s = 0}^{\ell} \ell = O(n^{1/4}).\frac{2}{\binom{n}{2}} \sum_{s = 0}^{n/2} \,\, \sum_{\ell
             = s}^{\textrm{m}_1(s, n)} \ell = O(n^{3/4}).
		&\frac{2}{\binom{n}{2}} \sum_{\ell = n^{3/4}}^{n}\sum_{s =
	    0}^{\textrm{m}_2(\ell, n)} \bigg(s + \exp(-n^{1/6}/18) \ell\bigg) =
	    \left(\frac{2}{\binom{n}{2}} \sum_{\ell = n^{3/4}}^{n}\sum_{s =
	    0}^{\textrm{m}_2(\ell, n)} s\right) + o(1) = \frac{n}{6} + O(1).
	     
        \frac{i_1 - (s - \ell)}{2} + \sum_{j = 1}^{k - 1} \frac{i_{j + 1} - i_j}{2} - k^\ast = \frac{n' - (s - \ell) }{2} - k^\ast = \ell - k^\ast,
        \label{eq:o:act}
    \label{eq:5000}
        \E\left(S_2 + L_2\right) \leq \frac{1}{\binom{n}{2}} \cdot \left( 2 \cdot \sum_{\substack{s + \ell \leq n \\\ell < s}} \ell + \sum_{\ell \leq n/2} \ell\right).
    
    \E(Z_{n'}) &= \sum_{1 \leq i \leq n'/2} \Pr(\text{there is a zero-crossing at
position })\\
&= \frac{1}{n' + 1} \sum_{i = 1}^{n'/2} \sum_{s = i}^{n' - i} \Pr(D_{n' - 2i} = 0 \mid \text{ small elements})\\
&= \frac{1}{n' + 1} \sum_{i = 1}^{n'/2} \sum_{s = i}^{n' - i}
\frac{\binom{2i}{i} \cdot \binom{n' - 2i}{s-i}}{\binom{n'}{s}}
\leq \frac{2}{n' + 1} \sum_{i = 1}^{n'/2} \sum_{s = i}^{n'/2}
\frac{\binom{2i}{i} \cdot \binom{n' - 2i}{s-i}}{\binom{n'}{s}},

    \E(Z_{n'})
    &= \Theta\left(\frac{1}{n'}\right) \sum_{i =1}^{n'/2} \frac{2^{2i}}{\sqrt{i}}
\sum_{s=i}^{n'/2} 
    \frac{\binom{n'-2i}{s - i}}{\binom{n'}{s}}\notag\\
&=\Theta\left(\frac{1}{n'}\right) \sum_{i=1}^{n'/2} \frac{2^{2i}}{\sqrt{i}} \sum_{s =
i}^{n'/2} 
\frac{(n' - s) \cdot \ldots \cdot (n' - s - i + 1) \cdot s 
\cdot \ldots \cdot (s-i + 1)}{n' \cdot \ldots \cdot (n' - 2i + 1)}\notag\\
&=\Theta\left(\frac{1}{n'}\right) \sum_{i = 1}^{n'/2} \frac{n'+1}{\sqrt{i}(n' {-} 2i {+} 1)} \sum_{j = 0}^{n'/2-i}
\prod_{k = 0}^{i - 1} \frac{(n'+2j - 2k)(n'-2j - 2k)}{(n' - 2k + 1)(n' -
2k)},\label{eq:zero:crossings:0}

\prod_{k = 0}^{i - 1} \frac{(n'+2j - 2k)(n'-2j - 2k)}{(n' - 2k + 1)(n' -
2k)} \leq \prod_{k = 0}^{i - 1} \left(1 - \left(\frac{2j}{n'-2k}\right)^2\right) 
\leq \left(1 - \left(\frac{2j}{n'}\right)^2\right)^i.

\E(Z_{n'}) &=  O\left(\frac{1}{n'}\right) \sum_{i=1}^{n'/2} \frac{n'+1}{\sqrt{i}(n' - 2i + 1)} \left(\int_0^{n'/2} 
    \left(1 - \left(\frac{2t}{n'}\right)^2\right)^{i} \text{ } + 1\right)\\
    &=O\left(\frac{1}{n'}\right) \sum_{1 \leq i \leq n'/2} \frac{n'+1}{\sqrt{i}(n' - 2i + 1)} 
        \cdot \left(n' \cdot \frac{\Gamma(i+1)}{\Gamma(i + 3/2)} + 1 \right),\\

    \E(Z_{n'}) &= O\left(\sum_{i = 1}^{n'/2} \frac{n'+1}{i(n' - 2i + 1)}\right) = O\left(\sum_{i = 1}^{n'/4} \frac{1}{i} + 
    \sum_{i = n'/4 + 1}^{n'/2}\frac{1}{n' - 2i  + 1}\right) = O(\log n').

    \frac{1}{\binom{n}{2}} \left( 2 \sum_{\substack{s + \ell \leq n \\\ell < s}} \ell + \sum_{\ell \leq n/2} \ell\right) - \E(S_2 + L_2) & \leq \frac{1}{\binom{n}{2}} \sum_{s + \ell \leq n - 2} \E(Z_{s + \ell} \mid
    s, \ell) \\ &= \frac{1}{\binom{n}{2}} \sum_{s + \ell \leq n - 2} O(\log (s+\ell)) = 
    O(\log n).
\label{eq:1000}
    \E(C_n) = \frac{1}{H_6 - H_2} \cdot a \cdot n \ln n  + O(n)= \frac{20}{19} \cdot a \cdot n \ln n + O(n),

    \E(P^\mathcal{Y}_n) &= \frac{4}{3}n  + \frac{1}{\binom{n}{5}}
    \sum_{s + \ell \leq n - 5} \frac{\ell \cdot (2s + m) \cdot s \cdot m \cdot
    \ell}{n-5} + O(n^{1 - \varepsilon}) = \frac{34}{21}n + O(n^{1 - \varepsilon}).

    \E(P^\mathcal{SP}_n) &=  \frac{4}{3}n +  \frac{2}{\binom{n}{5}}
    \sum_{\substack{s + \ell \leq n-5\\s \leq \ell}} s \cdot s \cdot m \cdot
    \ell + O(n^{1 - \varepsilon}) = \frac{37}{24}n + O(n^{1 - \varepsilon}).

	    \frac{1}{H_{k+1} - H_{(k+1)/2}} \cdot a \cdot n \ln n + O(n).
	    
	    \frac{1}{H_{k+1} - H_{(k+1)/3}} \cdot a \cdot n \ln n + O(n).
	    
    \frac{\binom{p-1}{(k-2)/3}\binom{q-p-1}{(k-2)/3}\binom{n-q}{(k-2)/3}}{\binom{n}{k}}.
\label{eq:700}
    \E(P^\mathcal{SP}_{n,k}) = \frac{4}{3}n +  \frac{2}{\binom{n}{k}}\sum_{s
    \leq \ell}\binom{s}{(k-2)/3}\binom{m}{(k-2)/3}\binom{\ell}{(k-2)/3} \cdot s + O(n^{1 - \varepsilon}).

	p^{\vec{\tau}}(n) \sim \frac{1+\tau_1+\tau_2}{-\sum_{1 \leq i \leq 3}
	\tau_i \ln \tau_i} n \ln n.
    
    p^{\vec{\tau}^\ast}\!(n) \sim \left(\frac{1}{\ln 2}\right) n \ln n = 1.4426.. n \ln
    n.

	p^{\vec{\tau}}(n) = n + (\tau_1 + \tau_2) n +
	p^{\vec{\tau}}(\tau_1 \cdot n) +
	p^{\vec{\tau}}(\tau_2 \cdot n) +
	p^{\vec{\tau}}(\tau_3 \cdot n).
    
	p^{\vec{\tau}}(n) \sim \frac{1 + \tau_1 + \tau_2}{-\sum_{i = 1}^{3} \tau_i \ln
	\tau_i} n \ln n.
    
        F_n = \begin{cases}
            b_n, & \text{for } 0 \leq n < N,\\
           t_n + \sum_{j=0}^{n-1}w_{n,j}F_j, & \text{for } n \geq N,
        \end{cases}
    \label{eq:cmt:shape:function}
        \sum_{j = 0}^{n-1} \left\vert w_{n,j} - \int_{j/n}^{(j+1)/n} w(z) \text{ } \right\vert = O(n^{-d}),
    
                \hat{H} := - (\beta + 1) \int_0^1 z^\alpha \ln (z) w(z) \text{ }.
            
                \int_0^1 z^c w(z) \text{ } = 1.
            
        w_{n,j} = \frac{6(n - j - 1)}{n (n-1)}.
    
        w(z) &= \lim_{n \rightarrow \infty} n \cdot w_{n, zn} = 6 (1 - z).
    
        &\phantom{=} \sum_{j = 0}^{n-1} \left\vert w_{n,j} - \int_{j/n}^{(j+1)/n} w(z) \text{ } \right\vert\\
        & = 6 \sum_{j = 0}^{n-1} \left\vert\frac{n - j - 1}{n ( n - 1)} - \int_{j/n}^{(j + 1)/n} (1-z) \text{ } \right\vert\\    
        & = 6 \sum_{j = 0}^{n-1} \left\vert\frac{n - j - 1}{n ( n - 1)} + \frac{2j + 1}{2n^2} - \frac{1}{n}\right\vert\\
        & < 6 \sum_{j = 0}^{n - 1} \left\vert\frac{1}{2n(n-1)} \right\vert = O(1/n).
    
        H := 1 - 6 \int_0^1 z^{1-\varepsilon} (1 - z) \text{ } < 0,
    
	6 \int_0^1 z^c (1 - z) \text{ } = 1,
    
    which is true for . Thus, an average partitioning cost of at most  
    yields average sorting cost of . 




\section{Dual-Pivot Quicksort: Algorithms in Detail}\label{app:sec:algorithms}

\subsection{General Setup}
The general outline of a dual-pivot quicksort algorithm is presented as Algorithm~\ref{algo:dual:pivot:outline}.

\renewcommand{\alglinenumber}[1]{\footnotesize{#1} }

\begin{algorithm}
    \caption{Dual-Pivot-Quicksort (outline)}\samepage\label{algo:dual:pivot:outline}
    \textbf{procedure} (, , )
    \begin{algorithmic}[1]
        \If{}
        \State \textit{InsertionSort}(, , );
            \State \Return;
        \EndIf
        \If{}
        \State swap \textit{A}[\textit{left}] and \textit{A}[\textit{right}];
        \EndIf
        \State ;
        \State ;
        \State \textit{partition}(\textit{A}, \texttt{p}, \texttt{q}, \textit{left}, \textit{right}, , );
        \State \textit{Dual-Pivot-Quicksort}(\textit{A}, \textit{left},  - 1);
        \State \textit{Dual-Pivot-Quicksort}(\textit{A},  + 1,  - 1);
        \State \textit{Dual-Pivot-Quicksort}(\textit{A},  + 1, \textit{right});
\end{algorithmic}
\end{algorithm}
To get an actual algorithm we have to implement a \textit{partition} function
that partitions the input as depicted in Figure~\ref{fig:dual:pivot:partition}.
A partition procedure in this paper has two output variables
 and  that are used to return
the positions of the two pivots in the partitioned array.

For moving elements around, we make use of the following two operations.

\begin{minipage}[t]{5cm}
\begin{algorithm}[H]
    \textbf{procedure} \textit{rotate3}()
    \begin{algorithmic}[1]
        \State ;
        \State ;
        \State ;
        \State ;
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\begin{minipage}[t]{3cm}
    \hfill
\end{minipage}
\begin{minipage}[t]{5cm}
    \begin{algorithm}[H]
    \textbf{procedure} \textit{rotate4}()
    \begin{algorithmic}[1]
        \State ;
        \State ;
        \State ;
        \State ;
        \State ;
    \end{algorithmic}
    \end{algorithm}
\end{minipage}

\subsection{Yaroslavskiy's Partitioning Method}\label{app:sec:yaroslavskiy}
As mentioned in Section~\ref{sec:methods}, Yaroslavskiy's algorithm makes sure that for
 large elements in the input  or  elements will be compared to
the larger pivot first. How does it accomplish this? By default, it
compares to the smaller pivot first, but for each large
elements that it sees, it will compare the next element to the larger pivot
first.

Algorithm~\ref{algo:yaroslavskiy:partition} shows the partition step of (a slightly modified version of) 
Yaroslavskiy's algorithm. In contrast to the algorithm
studied in \cite{nebel12}, it saves an index check at Line~8, and uses a \texttt{rotate3} operation
to save assignments. (In our experiments this makes Yaroslavskiy's algorithm about  faster.)

\begin{algorithm}
    \caption{Yaroslavskiy's Partitioning Method}\samepage\label{algo:yaroslavskiy:partition}
		\textbf{procedure} \textit{Y-Partition}(, , , ,
		, , )
    \begin{algorithmic}[1]
		\State ;
                \While{}
                    \If{}
                        \State swap  and ;
                        \State ;
                    \Else
                        \If{}
                            \While{} 
                                \State ;
                            \EndWhile
                            \If{}
                                \If{}
                                    \State 
                                    \State 
                                \Else
                                    \State swap  and ;
                                \EndIf
                                \State 
                            \EndIf
                        \EndIf
                    \EndIf
                    \State ;
		\EndWhile
                \State swap  and ;
		\State swap   and ;
	        \State 
    \end{algorithmic}
\end{algorithm}

\subsection{Algorithm Using ``Always Compare to the Larger Pivot First''}
\label{app:sec:algo:larger:first}
Algorithm~\ref{algo:always:q:first} presents an implementation of strategy  (\emph{``Always compare
to the larger pivot first''}). Like Yaroslavskiy's algorithm, it uses three pointers into the array.
One pointer is used to scan the array from left to right until a large element has been
found (moving small elements to a correct position using the second pointer on the way). Subsequently,
it scans the array from right to left using the third pointer until 
a non-large element has been found. These two elements
are then placed into a correct position. This is repeated until the two pointers have crossed. The design goal
is to check as rarely as possible if these two pointers have met, since this event occurs infrequently. (In contrast,
Yaroslavskiy's algorithm checks this for each element scanned by index  in Algorithm~\ref{algo:yaroslavskiy:partition}.)

This strategy makes  comparisons and  assignments on average.

\begin{algorithm}
    \caption{Always Compare To Larger Pivot First Partitioning}\samepage\label{algo:always:q:first}
    \textbf{procedure} \textit{L-Partition}(,
		, , ,
		, , )
    \begin{algorithmic}[1]
        \State 
        \While{}
            \While{}
                \State ;
            \EndWhile
            \While{}
                \If{}
                    \State swap  and ;
                    \State ;
                \EndIf
                \State ;
            \EndWhile
            \If{}
                \If{}
                \State \textit{rotate3}(, , 
                );
                    \State ;
                \Else
                    \State swap  and ;
                \EndIf
                \State ;
            \EndIf
        \State 
        \EndWhile
        \State swap  and ;
        \State swap  and ;
        \State ;
    \end{algorithmic}
\end{algorithm}
\subsection{Partitioning Methods Based on Sedgewick's Algorithm}\label{app:sec:sedgewick}
Algorithm~\ref{algo:sedgewick:partition} shows Sedgewick's partitioning method
as studied in \cite{sedgewick}.

Sedgewick's partitioning method uses two
pointers  and  to scan through the input. 
It does not swap
entries in the strict sense, but rather has two ``holes'' at positions
 resp.  that can be filled with small resp. large
elements. ``Moving a hole'' is not a swap operation in the strict sense (three elements are involved), but requires the
same amount of work as a swap operation (in which we have to save the content of a
variable into a temporary variable \cite{sedgewick}). An intermediate step in
the partitioning algorithm is depicted in Figure~\ref{fig:sedgewick:layout}.

\begin{figure}[tb]
    \centering
    \scalebox{0.6}{\includegraphics{sedgewick_layout}}
    \caption{An intermediate partitioning step in Sedgewick's algorithm.}
    \label{fig:sedgewick:layout}
\end{figure}

The algorithm works as follows: Using 
it scans the input
from left to right until it has found a large element, always
comparing to the larger pivot first. Small elements found in this way are moved
to a correct final position using the hole at array position . Subsequently, using  it scans
the input from right to left until it has found a small element, always
comparing to the smaller pivot first. Large elements found in this way are moved
to a correct final position using the hole at array position . Now it exchanges
the two elements at positions  resp.  and
continues until  and  have met.  


\begin{algorithm}
    \caption{Sedgewick's Partitioning Method}\samepage\label{algo:sedgewick:partition}
		\textbf{procedure} \textit{S-Partition}(,
		, , ,
		, , )
                \begin{algorithmic}[1]
		  \State 
                  \While{\textbf{true}}
                  \State 
                  \While{}
		  \If{} 
                  \State \textbf{break} outer while 
                  \EndIf
		  \If{} 
                  \State ;
                  \EndIf
		  \State ;
		 \EndWhile
		 \State 
                 \While{}
                 \If{} 
                 \State ; 
                 \EndIf
                 \If{} 
                 \State \textbf{break} outer while
                 \EndIf
		 \State ;
		 \EndWhile
		 \State 
		 \State 
		 \State 
		 \EndWhile
		 \State 
		 \State 
            \end{algorithmic}
        \end{algorithm}
Algorithm~\ref{algo:sedgewick:partition:modified} shows an implementation of the
modified partitioning strategy from Section~\ref{sec:sedgewick}. In the same way
as Algorithm~\ref{algo:sedgewick:partition} it scans the input from left to
right
until it has found a large element. However, it uses the smaller pivot for the
first comparison in this part. Subsequently, it scans the input from right
to left until it has found a small element. Here, it uses the larger pivot for
the first comparison. 

\begin{algorithm}
    \caption{Sedgewick's Partitioning Method, modified}\samepage\label{algo:sedgewick:partition:modified}
    \textbf{procedure} \textit{S2-Partition}(,
		, , ,
		, , )
    \begin{algorithmic}[1]
		  \State 
		  \While{\textbf{true}}
                  \State 
                  \While{\textbf{true}}
                  \If{}
                  \State \textbf{break} outer while
                  \EndIf
                  \If{} 
                \State ;
                 \ElsIf{}
                 \State \textbf{break} inner while
                 \EndIf
		 \State ;
		 \EndWhile
                 \State 
                 \While{\textbf{true}}
                 \If{}
                 \State ;
                 \ElsIf{} 
                 \State \textbf{break} inner while
                 \EndIf
                 \If{}
                 \State \textbf{break} outer while
                 \EndIf
		 \State ;
		 \EndWhile
                 \State 
		 \State 
		 \State 
		 \EndWhile
                 \State 
		 \State 
            \end{algorithmic}
            \end{algorithm}

\subsection{Algorithms for the Sampling Partitioning Method}\label{app:sec:our:algorithms}

The sampling method  from Section~\ref{sec:decreasing} uses a mix of two
classification algorithms: \emph{``Always compare to the smaller pivot first''},
and \emph{``Always compare to the larger pivot first''}.  
The actual partitioning method uses
Algorithm~\ref{algo:always:q:first} for the first 
classifications and then decides which pivot should be used for
the first comparison in the remaining input. (This is done by comparing the two variables \texttt{i} and
\texttt{k} in Algorithm~\ref{algo:always:q:first}.) If there are more large
elements than small elements in the sample it continues using
Algorithm~\ref{algo:always:q:first}, otherwise it uses
Algorithm~\ref{algo:simple:partition} below. If the input contains fewer than  elements, 
Algorithm~\ref{algo:always:q:first} is used directly.


\begin{algorithm}
    \caption{Simple Partitioning Method (smaller pivot first)}\samepage\label{algo:simple:partition}
		\textbf{procedure} \textit{SimplePartitionSmall}(,
		, , ,
		, , )
                \begin{algorithmic}[1]
		\State ;
                \While{}
                \If{}
		\State swap  and ;
		\State ;
		\State ;
		\Else
                \If{}
		\State ;
		\Else
		\State swap  and 
		\State ;
		\EndIf
                \EndIf
                \EndWhile
		\State swap  and ;
		\State swap  and ;
		\State ;
            \end{algorithmic}
    \end{algorithm}





            \subsection{Algorithm for the Counting Strategy}
            \label{app:sec:algo:counting:strategy}
            Algorithm~\ref{algo:counting:strategy} is an implementation
            of the counting strategy from Section~\ref{sec:optimal:strategies}. 
            It uses a variable  which stores the difference of
            the number of small elements and the number of large elements
            which have been classified so far.  On average this algorithm makes
             comparisons and  assignments.
\begin{algorithm}
    \caption{Counting Strategy }\samepage\label{algo:counting:strategy}
    \textbf{procedure} \textit{C-Partition}(,
		, , ,
		, , )
    \begin{algorithmic}[1]
        \State 
        \State  \Comment{ holds the difference of the number of small
        and large elements.}
        \While{}
            \If{}
                \If{}
                    \State swap  and ;
                    \State 
                \Else
                    \If{}
                        \State 
                    \Else
                        \State swap  and ;
                        \State 
                    \EndIf
                \EndIf
            \Else
                \While{}
                    \State 
                \EndWhile
                \If{}
                    \If{}
                        \State \emph{rotate3}();
                        \State ;
                         ;
                    \Else
                        \State swap  and ;
                    \EndIf
                    \State 
                \EndIf
            \EndIf
        \EndWhile
        \State swap  and ;
        \State swap  and ;
        \State ;
    \end{algorithmic}
\end{algorithm}

\section{A Fast Three-Pivot Algorithm}\label{app:sec:algo:three:pivot}
We give here the complete pseudocode for the three-pivot algorithm described in \cite{Kushagra14}. 
In contrast to the pseudocode given in \cite[Algorithm A.1.1]{Kushagra14}, we removed two unnecessary
bound checks (Line~ and Line~ in our code) and we move misplaced elements
in Lines~-- using less assignments. This is used in the implementation
of \cite{Kushagra14}, as well.\footnote{Code made available by Alejandro
L{\'o}pez-Ortiz.} On average, this algorithm makes 
comparisons 
and  assignments. 

\begin{algorithm}
    \caption{Symmetric Three-Pivot Sorting Algorithm}\samepage\label{algo:three:pivot}
    \textbf{procedure} \textit{3-Pivot}(, , )
    \begin{algorithmic}[1]
        \Require , 
            
        \State 
        \State 
        
        \While{}
            \While{}
                \If{}
                    \State swap  and ;
                    \State 
                \EndIf
                \State 
            \EndWhile
            \While{}
                \If{}
                    \State swap  and ;
                    \State 
                \EndIf
                \State 
            \EndWhile
            \If{}
                \If{}
                    \If{}
                        \State \textit{rotate4}();
                        \State ;
                    \Else
                        \State \textit{rotate3}();
                    \EndIf
                    \State 
                \Else
                    \If{}
                        \State \textit{rotate3}();
                        \State ;
                    \Else
                        \State swap  and 
                        ;
                    \EndIf
                \EndIf
                \State ;
            \EndIf
        \EndWhile
        \State \textit{rotate3}(, ,
                                );
        \State swap  and ;
        \State swap  and ;
        \State \textit{3-Pivot}(\textit{A}, \textit{left}, );
        \State \textit{3-Pivot}(\textit{A}, , );
        \State \textit{3-Pivot}(\textit{A}, , );
        \State \textit{3-Pivot}(\textit{A}, , );
    \end{algorithmic}
\end{algorithm}




\newpage

\begin{figure}
    \centering
\begin{tikzpicture}
  \begin{axis}[
    xlabel={Items []},
    ylabel={Comparisons },
    height=8cm,
    width=13cm,
    legend style = { at = {(0.5,0.15)}, anchor=west, draw=none},
    cycle list name = black white,
    legend columns = 2
    ]
\addplot coordinates { (9.0,1.78865) (10.0,1.79511) (11.0,1.80848) (12.0,1.8165) (13.0,1.84026) (14.0,1.84536) (15.0,1.85295) (16.0,1.86175) (17.0,1.87164) (18.0,1.88463) (19.0,1.88326) (20.0,1.89463) (21.0,1.89212) (22.0,1.8982) (23.0,1.90576) (24.0,1.90646) (25.0,1.91415) (26.0,1.91586) (27.0,1.92124) (28.0,1.92075) (29.0,1.92972) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.45758) (10.0,1.49378) (11.0,1.52517) (12.0,1.55546) (13.0,1.57976) (14.0,1.60364) (15.0,1.62469) (16.0,1.64334) (17.0,1.65741) (18.0,1.67506) (19.0,1.68142) (20.0,1.69266) (21.0,1.70593) (22.0,1.7126) (23.0,1.72189) (24.0,1.72627) (25.0,1.734) (26.0,1.74426) (27.0,1.74871) (28.0,1.75167) (29.0,1.75624) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.41905) (10.0,1.45543) (11.0,1.48845) (12.0,1.52313) (13.0,1.5485) (14.0,1.57257) (15.0,1.59275) (16.0,1.6127) (17.0,1.62554) (18.0,1.63843) (19.0,1.64904) (20.0,1.66139) (21.0,1.67216) (22.0,1.67947) (23.0,1.68814) (24.0,1.69448) (25.0,1.70266) (26.0,1.70795) (27.0,1.7169) (28.0,1.71934) (29.0,1.72985) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.36638) (10.0,1.40423) (11.0,1.43273) (12.0,1.46405) (13.0,1.48812) (14.0,1.51163) (15.0,1.53226) (16.0,1.54852) (17.0,1.56468) (18.0,1.58044) (19.0,1.58583) (20.0,1.59903) (21.0,1.61074) (22.0,1.61627) (23.0,1.62672) (24.0,1.63171) (25.0,1.63862) (26.0,1.64608) (27.0,1.6527) (28.0,1.65553) (29.0,1.66188) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.44368) (10.0,1.47899) (11.0,1.51176) (12.0,1.5384) (13.0,1.55507) (14.0,1.57245) (15.0,1.58835) (16.0,1.59667) (17.0,1.61334) (18.0,1.6218) (19.0,1.63937) (20.0,1.64333) (21.0,1.65426) (22.0,1.65839) (23.0,1.66621) (24.0,1.66821) (25.0,1.67469) (26.0,1.68026) (27.0,1.68387) (28.0,1.68803) (29.0,1.69487) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.57976) (10.0,1.61343) (11.0,1.64196) (12.0,1.66882) (13.0,1.69075) (14.0,1.71679) (15.0,1.73799) (16.0,1.75374) (17.0,1.76826) (18.0,1.78555) (19.0,1.79182) (20.0,1.80236) (21.0,1.81389) (22.0,1.82195) (23.0,1.83203) (24.0,1.83562) (25.0,1.83976) (26.0,1.85202) (27.0,1.85598) (28.0,1.85618) (29.0,1.8602) };
    \addlegendentry{};
\end{axis}
\end{tikzpicture}
    \caption{Average comparison count (scaled by ) needed to sort
    a random input of up to  integers. We compare classical
    quicksort (), Yaroslavskiy's algorithm (), the
    sampling algorithm (), the counting algorithm (),
    the modified version of Sedgewick's algorithm (), and algorithm
    . Each data point is the average over  trials.}
    \label{fig:comp:direct}
\end{figure}

\begin{figure}
    \centering
\begin{tikzpicture}
  \begin{axis}[
    xlabel={Items []},
    ylabel={Comparisons },
    height=8cm,
    width=12cm,
    legend style = { at = {(0.6,0.25)}, anchor=west, draw=none},
    cycle list name = black white,
    legend columns = 2
    ]
\addplot coordinates { (9.0,1.42665) (10.0,1.45338) (11.0,1.47645) (12.0,1.49746) (13.0,1.51036) (14.0,1.5264) (15.0,1.53518) (16.0,1.54957) (17.0,1.55918) (18.0,1.56989) (19.0,1.5747) (20.0,1.58247) (21.0,1.58809) (22.0,1.59361) (23.0,1.60111) (24.0,1.60402) (25.0,1.60878) (26.0,1.61369) (27.0,1.61573) (28.0,1.61976) (29.0,1.62307) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.15571) (10.0,1.20605) (11.0,1.25125) (12.0,1.2891) (13.0,1.31922) (14.0,1.3482) (15.0,1.3711) (16.0,1.39228) (17.0,1.41109) (18.0,1.42744) (19.0,1.44005) (20.0,1.45435) (21.0,1.46724) (22.0,1.47737) (23.0,1.48797) (24.0,1.49587) (25.0,1.50397) (26.0,1.51237) (27.0,1.52037) (28.0,1.52682) (29.0,1.53194) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.19402) (10.0,1.24248) (11.0,1.28258) (12.0,1.31495) (13.0,1.3431) (14.0,1.36743) (15.0,1.38773) (16.0,1.40774) (17.0,1.42345) (18.0,1.43888) (19.0,1.45104) (20.0,1.4635) (21.0,1.47314) (22.0,1.48305) (23.0,1.49217) (24.0,1.50007) (25.0,1.50677) (26.0,1.51435) (27.0,1.52087) (28.0,1.52672) (29.0,1.532) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.00335) (10.0,1.0624) (11.0,1.11138) (12.0,1.15448) (13.0,1.18752) (14.0,1.21779) (15.0,1.24343) (16.0,1.26743) (17.0,1.2875) (18.0,1.30518) (19.0,1.32057) (20.0,1.33474) (21.0,1.34813) (22.0,1.35965) (23.0,1.37159) (24.0,1.38038) (25.0,1.39069) (26.0,1.39843) (27.0,1.40665) (28.0,1.41364) (29.0,1.41987) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.09666) (10.0,1.1492) (11.0,1.19097) (12.0,1.22702) (13.0,1.25576) (14.0,1.28303) (15.0,1.30494) (16.0,1.32615) (17.0,1.34338) (18.0,1.35905) (19.0,1.3717) (20.0,1.38479) (21.0,1.39692) (22.0,1.40663) (23.0,1.41684) (24.0,1.42478) (25.0,1.43189) (26.0,1.44075) (27.0,1.4474) (28.0,1.45325) (29.0,1.45947) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.15571) (10.0,1.20605) (11.0,1.24103) (12.0,1.27446) (13.0,1.29945) (14.0,1.32383) (15.0,1.3434) (16.0,1.36201) (17.0,1.37719) (18.0,1.39099) (19.0,1.40213) (20.0,1.4137) (21.0,1.42453) (22.0,1.43305) (23.0,1.44214) (24.0,1.44908) (25.0,1.45526) (26.0,1.46325) (27.0,1.46912) (28.0,1.47424) (29.0,1.47947) };
    \addlegendentry{};
    \addplot coordinates { (9.0,1.20888) (10.0,1.26422) (11.0,1.30622) (12.0,1.3411) (13.0,1.36989) (14.0,1.4003) (15.0,1.4237) (16.0,1.44598) (17.0,1.4629) (18.0,1.48043) (19.0,1.49236) (20.0,1.50747) (21.0,1.52045) (22.0,1.52821) (23.0,1.54085) (24.0,1.54771) (25.0,1.55622) (26.0,1.563) (27.0,1.57188) (28.0,1.57542) (29.0,1.58435) };
    \addlegendentry{};
    \addplot coordinates { (9.0,0.994753) (10.0,1.04904) (11.0,1.09477) (12.0,1.13468) (13.0,1.16718) (14.0,1.19454) (15.0,1.21829) (16.0,1.23973) (17.0,1.25899) (18.0,1.27533) (19.0,1.29015) (20.0,1.30379) (21.0,1.31563) (22.0,1.32768) (23.0,1.33792) (24.0,1.34711) (25.0,1.3558) (26.0,1.36387) (27.0,1.37057) (28.0,1.3783) (29.0,1.38433) };
    \addlegendentry{};
\end{axis}
\end{tikzpicture}
    \caption{Average comparison count (scaled by ) needed to sort a random
        input of up to  integers. We compare classical quicksort
        () with the Median-of- strategy, Yaroslavskiy's
        algorithm (), the sampling algorithm (), the
        counting algorithm
        (), the modified
        version of Sedgewick's algorithm from Section~\ref{sec:methods}
        (), and algorithm . Each of these dual-pivot
        algorithms uses the tertiles in a sample of size  as the two pivots.
        Moreover,  is an implementation of strategy 
        which uses the third- and sixth-largest element from a sample of size
        .  is Yaroslavskiy's algorithm choosing the tertiles
        of a sample of size  as the two pivots. Each data point is the
        average over  trials. }
    \label{fig:comp:sample}
\end{figure}



\begin{figure}
    \centering
\begin{tikzpicture}
  \begin{axis}[
    xlabel={Items []},
    ylabel={Time  [ns]},
    height=8cm,
    width=12cm,
    legend style = { at = {(0.65,0.85)}, anchor=west, draw=none},
    cycle list name = black white,
    legend columns = 2
    ]
\addplot coordinates {  (12.0,4.7328) (13.0,4.80484) (14.0,4.82673) (15.0,4.78657) (16.0,4.62393) (17.0,4.41442) (18.0,4.33746) (19.0,4.31062) (20.0,4.35008) (21.0,4.2076) (22.0,4.14301) (23.0,4.22214) (24.0,4.13544) (25.0,4.1351) (26.0,4.13266) (27.0,4.13339) };
    \addlegendentry{};
    \addplot coordinates { (12.0,4.17461) (13.0,4.18093) (14.0,4.17313) (15.0,4.10297) (16.0,4.0024) (17.0,4.03064) (18.0,3.97671) (19.0,3.96124) (20.0,3.99598) (21.0,3.87268) (22.0,3.81213) (23.0,3.86882) (24.0,3.81088) (25.0,3.81342) (26.0,3.81468) (27.0,3.81686) };
    \addlegendentry{};
    \addplot coordinates {  (12.0,4.71282) (13.0,4.68086) (14.0,4.67387) (15.0,4.66958) (16.0,4.5656) (17.0,4.59854) (18.0,4.53572) (19.0,4.51942) (20.0,4.56842) (21.0,4.43126) (22.0,4.37192) (23.0,4.41077) (24.0,4.37252) (25.0,4.37589) (26.0,4.37843) (27.0,4.38028) };
    \addlegendentry{};
    \addplot coordinates { (12.0,4.2982) (13.0,4.29183) (14.0,4.29243) (15.0,4.30182) (16.0,4.21382) (17.0,4.25464) (18.0,4.2044) (19.0,4.19495) (20.0,4.24222) (21.0,4.12046) (22.0,4.06537) (23.0,4.09061) (24.0,4.07407) (25.0,4.0817) (26.0,4.08485) (27.0,4.09089) };
    \addlegendentry{};
    \addplot coordinates { (12.0,4.20207) (13.0,4.1712) (14.0,4.14683) (15.0,4.1387) (16.0,4.04234) (17.0,4.06099) (18.0,4.00508) (19.0,3.98714) (20.0,4.01641) (21.0,3.89226) (22.0,3.82797) (23.0,3.83479) (24.0,3.8209) (25.0,3.82063) (26.0,3.82055) (27.0,3.82052) };
    \addlegendentry{};
    \addplot coordinates {  (12.0,4.21324) (13.0,4.18325) (14.0,4.17531) (15.0,4.16714) (16.0,4.07734) (17.0,4.09946) (18.0,4.04156) (19.0,4.02275) (20.0,4.05023) (21.0,3.92884) (22.0,3.8655) (23.0,3.86197) (24.0,3.85758) (25.0,3.85806) (26.0,3.85668) (27.0,3.85547) };
    \addlegendentry{};

\end{axis}
\end{tikzpicture}
\caption{Running time experiments in {\Cpp}, setting 1 with compiler flags: \emph{-O2}. Each data point is
the average over 1000 trials. Times are scaled by .}
\label{fig:running:time:setting:1}
\end{figure}

\begin{figure}
    \centering
\begin{tikzpicture}
  \begin{axis}[
    xlabel={Items []},
    ylabel={Time  [ns]},
    height=8cm,
    width=12cm,
    legend style = { at = {(0.65,0.85)}, anchor=west, draw=none},
    cycle list name = black white,
    legend columns = 2
    ]
\addplot coordinates {  (12.0,4.92893) (13.0,4.91397) (14.0,4.88841) (15.0,4.90075) (16.0,4.77715) (17.0,4.54535) (18.0,4.43814) (19.0,4.26778) (20.0,4.33975) (21.0,4.19873) (22.0,4.1939) (23.0,4.23816) (24.0,4.184) (25.0,4.17993) (26.0,4.17757) (27.0,4.17404) };
    \addlegendentry{};
    \addplot coordinates {  (12.0,4.55754) (13.0,4.56718) (14.0,4.42987) (15.0,4.37486) (16.0,4.2772) (17.0,4.26282) (18.0,4.16655) (19.0,4.00821) (20.0,4.07047) (21.0,3.94463) (22.0,3.94113) (23.0,3.97194) (24.0,3.93071) (25.0,3.92669) (26.0,3.92428) (27.0,3.92304) };
    \addlegendentry{};
    \addplot coordinates {  (12.0,5.07395) (13.0,5.12557) (14.0,4.93912) (15.0,4.93189) (16.0,4.84409) (17.0,4.83273) (18.0,4.72362) (19.0,4.55909) (20.0,4.64028) (21.0,4.49571) (22.0,4.49348) (23.0,4.51536) (24.0,4.4856) (25.0,4.48459) (26.0,4.48226) (27.0,4.48206) };
    \addlegendentry{};
    \addplot coordinates {  (12.0,4.63465) (13.0,4.67941) (14.0,4.51464) (15.0,4.51489) (16.0,4.43613) (17.0,4.42222) (18.0,4.32784) (19.0,4.17154) (20.0,4.24671) (21.0,4.11733) (22.0,4.11852) (23.0,4.1299) (24.0,4.11465) (25.0,4.11507) (26.0,4.11543) (27.0,4.11576) };
    \addlegendentry{};
    \addplot coordinates {  (12.0,4.56033) (13.0,4.58935) (14.0,4.41201) (15.0,4.39107) (16.0,4.30099) (17.0,4.28473) (18.0,4.18026) (19.0,4.01642) (20.0,4.07635) (21.0,3.9481) (22.0,3.94131) (23.0,3.94176) (24.0,3.92996) (25.0,3.92507) (26.0,3.91964) (27.0,3.91647) };
    \addlegendentry{};
    \addplot coordinates {  (12.0,4.56197) (13.0,4.60267) (14.0,4.42271) (15.0,4.40071) (16.0,4.30075) (17.0,4.28105) (18.0,4.17829) (19.0,4.01525) (20.0,4.07339) (21.0,3.94594) (22.0,3.93846) (23.0,3.93093) (24.0,3.92121) (25.0,3.9156) (26.0,3.90977) (27.0,3.90636) };
    \addlegendentry{};

\end{axis}
\end{tikzpicture}
\caption{Running time experiments in \Cpp, setting  with compiler flags: \emph{-O2 -funroll-loops}. Each data point is
the average over 1000 trials. Times are scaled by .}
\label{fig:running:time:setting:2}
\end{figure}

\begin{figure}
    \centering
\begin{tikzpicture}
  \begin{axis}[
    xlabel={Items []},
    ylabel={Time  [ns]},
    height=8cm,
    width=12cm,
    legend style = { at = {(0.15,0.65)}, anchor=west, draw=none}, legend columns = 6,
    cycle list name = black white
    ]
\addplot coordinates { (15.0008,5.93946) (16.0008,5.86508) (17.0008,5.9699) (18.0008,5.92234) (19.0008,5.8372) (20.0008,5.81015) (21.0008,5.79861) (22.0008,5.79579) (23.0008,5.78734) (24.0008,5.77896) (25.0008,5.78091) (26.0008,5.7492) };
    \addlegendentry{};
    \addplot coordinates { (15.0008,5.56956) (16.0008,5.45866) (17.0008,5.45317) (18.0008,5.42361) (19.0008,5.41289) (20.0008,5.41302) (21.0008,5.41448) (22.0008,5.42014) (23.0008,5.4236) (24.0008,5.43069) (25.0008,5.43571) (26.0008,5.42317) };
    \addlegendentry{};
    \addplot coordinates { (15.0008,7.32291) (16.0008,7.23281) (17.0008,7.19554) (18.0008,7.20725) (19.0008,7.21956) (20.0008,7.23337) (21.0008,7.24906) (22.0008,7.27134) (23.0008,7.27759) (24.0008,7.30056) (25.0008,7.31583) (26.0008,7.30714) };
    \addlegendentry{};
    \addplot coordinates { (15.0008,5.65602) (16.0008,5.62368) (17.0008,5.62476) (18.0008,5.66502) (19.0008,5.67603) (20.0008,5.68516) (21.0008,5.69917) (22.0008,5.70824) (23.0008,5.71327) (24.0008,5.72505) (25.0008,5.73076) (26.0008,5.74282) };
    \addlegendentry{};
    \addplot coordinates { (15.0008,5.45384) (16.0008,5.39612) (17.0008,5.3653) (18.0008,5.36028) (19.0008,5.36584) (20.0008,5.35446) (21.0008,5.35281) (22.0008,5.35267) (23.0008,5.35661) (24.0008,5.36039) (25.0008,5.3675) (26.0008,5.34803) };
    \addlegendentry{};
    \addplot coordinates { (15.0008,5.36427) (16.0008,5.32594) (17.0008,5.31145) (18.0008,5.32954) (19.0008,5.3222) (20.0008,5.32684) (21.0008,5.33407) (22.0008,5.34376) (23.0008,5.34538) (24.0008,5.35092) (25.0008,5.35541) (26.0008,5.33182) };
    \addlegendentry{};

\end{axis}
\end{tikzpicture}
\caption{Running time experiments in Java 8. For warming up the JIT, we let each algorithm sort 10{\,}000
inputs consisting of  elements. Each
data point is the average over  trials. Times are scaled by .}
\label{fig:running:time:java:8}
\end{figure}

\end{document}
