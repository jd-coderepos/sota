

\documentclass{article}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{hyperref}       
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{array}
\usepackage{longtable}
\usepackage{subfigure}
\newcommand{\T}{\mathcal{T}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2020}

\icmltitlerunning{Agent57: Outperforming the Atari Human Benchmark}

\begin{document}

\twocolumn[
\icmltitle{Agent57: Outperforming the Atari Human Benchmark}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Adri\`a Puigdom\`enech Badia}{equal,to}
\icmlauthor{Bilal Piot}{equal,to}
\icmlauthor{Steven Kapturowski}{equal,to}
\icmlauthor{Pablo Sprechmann}{equal,to}
\icmlauthor{Alex Vitvitskyi}{to}
\icmlauthor{Daniel Guo}{to}
\icmlauthor{Charles Blundell}{to}
\end{icmlauthorlist}

\icmlaffiliation{to}{DeepMind}
\icmlcorrespondingauthor{Adri\`a Puigdom\`enech Badia}{adriap@google.com}

\icmlkeywords{Machine Learning, Deep Reinforcement Learning, Exploration}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. 
Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
The Arcade Learning Environment~\citep[ALE; ][]{bellemare2012arcade} was proposed as a platform for empirically assessing agents designed for general competency across a wide range of games. 
ALE offers an interface to a diverse set of Atari 2600 game environments designed to be engaging and challenging for human players.
As~\citet{bellemare2012arcade} put it, the Atari 2600 games are well suited for evaluating general competency in AI agents for three main reasons: \emph{(i)} varied enough to claim generality, \emph{(ii)} each interesting enough to be representative of settings that might be faced in practice, and \emph{(iii)} each created by an independent party to be free of experimenterâ€™s bias.

Agents are expected to perform well in as many games as possible making minimal assumptions about the domain at hand and without the use of game-specific information.
Deep Q-Networks \citep[DQN ;][]{mnih2015human} was the first algorithm to achieve human-level control in
a large number of the Atari 2600 games, measured by human normalized scores (HNS). Subsequently, using HNS to assess performance on Atari games has become one of the most widely used benchmarks in deep reinforcement learning (RL), despite the human baseline scores potentially under-estimating human performance relative to what is possible~\citep{toromanoff2019deep}.
Nonetheless, human benchmark performance remains an oracle for ``reasonable performance'' across the 57 Atari games.
Despite all efforts, no single RL algorithm has been able to achieve over 100\% HNS on all 57 Atari games with one set of hyperparameters. 
Indeed, state of the art algorithms in model-based RL, MuZero~\citep{schrittwieser2019mastering}, and in model-free RL, R2D2~\citep{kapturowski2018recurrent} surpass 100\% HNS on 51 and 52 games, respectively. 
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Agent57_Number_intro_zoom.png}
    \caption{Number of games where algorithms are better than the human benchmark throughout training for Agent57 and state-of-the-art baselines on the 57 Atari games.} 
    \label{fig:cappedhns}
    \vspace{-2ex}
\end{figure}
While these algorithms achieve well above average human-level performance on a large fraction of the games (e.g. achieving more than 1000\% HNS), in the games they fail to do so, they often fail to learn completely.
These games showcase particularly important issues that a general RL algorithm should be able to tackle. Firstly, \emph{long-term credit assignment}: which decisions are most deserving of credit for the positive (or negative) outcomes that follow? This problem is particularly hard when rewards are delayed and credit needs to be assigned over long sequences of actions, such as in the games of \textit{Skiing} or \textit{Solaris}. The game of \textit{Skiing} is a canonical example due to its peculiar reward structure. The goal of the game is to run downhill through all gates as fast as possible. A penalty of five seconds is given for each missed gate.  The reward, given only at the end, is proportional to the time elapsed. Therefore long-term credit assignment is needed to understand why an action taken early in the game (e.g. missing a gate) has a negative impact in the obtained reward. 
Secondly, \emph{exploration}: efficient exploration can be critical to effective learning in RL. 
Games like \textit{Private Eye}, \textit{Montezuma's Revenge}, \textit{Pitfall!} or \textit{Venture} are widely considered hard exploration games~\citep{bellemare2016unifying, ostrovski2017count} as hundreds of actions may be required before a first positive reward is seen. In order to succeed, the agents need to keep exploring the environment despite the apparent impossibility of finding positive rewards.
These problems are particularly challenging in large high dimensional state spaces where function approximation is required.

Exploration algorithms in deep RL generally fall into three categories:
randomized value functions \citep{osband2016deep,fortunato2017noisy,salimans2017evolution,plappert2017parameter,osband2018randomized}, unsupervised policy learning \citep{gregor2016variational,achiam2018variational,eysenbach2018diversity} and intrinsic motivation \citep{schmidhuber1991possibility,oudeyer2007intrinsic,barto2013intrinsic,bellemare2016unifying,ostrovski2017count,fu2017ex2,tang2017exploration,burda2018exploration,choi2018contingency,savinov2018episodic, badia2020never}. Other work combines handcrafted features, domain-specific knowledge or privileged pre-training to side-step the exploration problem, sometimes only evaluating on a few Atari games \citep{aytar2018playing,ecoffet2019go}.
Despite the encouraging results, no algorithm has been able to significantly improve performance on challenging games without deteriorating performance on the remaining games without relying on human demonstrations~\citep{pohlen2018observe}.
Notably, amongst all this work, intrinsic motivation, and in particular, \textit{Never Give Up}~\citep[NGU; ][]{badia2020never} has shown significant recent promise in improving performance on hard exploration games.
NGU achieves this by augmenting the reward signal with an internally generated intrinsic reward that is sensitive to novelty at two levels: short-term novelty within an episode and long-term novelty across episodes.
It then learns a family of policies for exploring and exploiting (sharing the same parameters), with the end goal of obtain the highest score under the exploitative policy.
However, NGU is not the most general agent: much like R2D2 and MuZero are able to perform strongly on all but few games, so too NGU suffers in that it performs strongly on a smaller, \emph{different} set of games to agents such as MuZero and R2D2 (despite being based on R2D2).
For example, in the game  \textit{Surround} R2D2 achieves the optimal score while NGU performs similar to a random policy.
One shortcoming of NGU is that it collects the same amount of experience following each of its policies, regardless of their contribution to the learning progress.
Some games require a significantly different degree of exploration to others.
Intuitively, one would want to allocate the shared resources (both network capacity and data collection) such that end performance is maximized.
We propose allowing NGU to adapt its exploration strategy over the course of an agent's lifetime, enabling specialization to the particular game it is learning.
This is the first significant improvement we make to NGU to allow it to be a more general agent.

Recent work on long-term credit assignment can be categorized into roughly two types: ensuring that gradients correctly assign credit \citep{ke2017sparse,weber2019credit,ferret2019credit,fortunato2019generalization} and
using values or targets to ensure correct credit is assigned \citep{arjona2019rudder,hung2019optimizing,liu2019sequence,harutyunyan2019hindsight}.
NGU is also unable to cope with long-term credit assignment problems such as \textit{Skiing} or \textit{Solaris} where it fails to reach 100\% HNS.
Advances in credit assignment in RL often involve a mixture of both approaches, as values and rewards form the loss whilst the flow of gradients through a model directs learning.

In this work, we propose tackling the long-term credit assignment problem by improving the overall training stability, dynamically adjusting the discount factor, and increasing the backprop through time window.
These are relatively simple changes compared to the approaches proposed in previous work, but we find them to be effective.
Much recent work has explored this problem of how to dynamically adjust hyperparameters of a deep RL agent, e.g., approaches based upon evolution~\citep{jaderberg2017population}, gradients~\citep{xu2018meta} or multi-armed bandits~\citep{schaul2019adapting}.
Inspired by \citet{schaul2019adapting}, we propose using a simple non-stationary multi-armed bandit~\citep{garivier2008upperconfidence} to directly control the exploration rate and discount factor to maximize the episode return, and then provide this information to the value network of the agent as an input.
Unlike~\citet{schaul2019adapting}, 1) it controls the exploration rate and discount factor (helping with long-term credit assignment), and
2) the bandit controls a family of state-action value functions that back up the effects of exploration and longer discounts, rather than linearly tilting a common value function by a fixed functional form.


In summary, our contributions are as follows:
\begin{enumerate}[leftmargin=12pt, align=left, labelwidth=10pt,  labelsep=0pt]
    \item A new parameterization of the state-action value function that decomposes the contributions of the intrinsic and extrinsic rewards. As a result, we significantly increase the training stability over a large range of intrinsic reward scales.
    \item A \emph{meta-controller}: an adaptive mechanism to select which of the policies (parameterized by exploration rate and discount factors) to prioritize throughout the training process. This allows the agent to control the \textit{exploration/exploitation trade-off} by dedicating more resources to one or the other.
    \item Finally, we demonstrate for the first time performance that is above the human baseline across all Atari 57 games. As part of these experiments, we also find that simply re-tuning the backprop through time window to be twice the previously published window for R2D2 led to superior long-term credit assignment (e.g., in \textit{Solaris}) while still maintaining or improving overall performance on the remaining games.
\end{enumerate}

These improvements to NGU collectively transform it into the most general Atari 57 agent, enabling it to outperform the human baseline uniformly over all Atari 57 games. Thus, we call this agent: Agent57.

\section{Background: Never Give Up (NGU)}
\label{sec:background}
Our work builds on top of the NGU agent, which combines two ideas: first, the curiosity-driven exploration, and second, distributed deep RL agents, in particular R2D2.

NGU computes an intrinsic reward in order to encourage exploration. This reward is defined by combining per-episode and life-long novelty. The per-episode novelty, , rapidly vanishes over the course of an episode, and it is computed by comparing observations to the contents of an episodic memory. The life-long novelty, , slowly vanishes throughout training, and it is computed by using a parametric model (in NGU and in this work Random Network Distillation~\citep{burda2018exploration} is used to this end). With this, the intrinsic reward  is defined as follows:

where  is a chosen maximum reward scaling. This leverages the long-term novelty provided by , while  continues to encourage the agent to explore within an episode. For a detailed description of the computation of  and , see~\citep{badia2020never}.
At time , NGU adds  different scales of the same intrinsic reward  (, ) to the extrinsic reward provided by the environment, , to form  potential total rewards . Consequently, NGU aims to learn the  different associated optimal state-action value functions  associated with each reward function . The exploration rates  are parameters that control the degree of exploration. Higher values will encourage exploratory policies and smaller values will encourage exploitative policies.
Additionally, for purposes of learning long-term credit assignment, each  has its own associated discount factor  (for background and notations on Markov Decision Processes (MDP) see App.~\ref{app:background}). Since the intrinsic reward is typically much more dense than the extrinsic reward,  are chosen so as to allow for long term horizons (high values of ) for exploitative policies (small values of ) and small term horizons (low values of ) for exploratory policies (high values of ).

To learn the state-action value function , NGU trains a recurrent neural network , where  is a one-hot vector indexing one of  implied MDPs (in particular ),  is the current observation,  is an action, and  are the parameters of the network (including the recurrent state).
In practice, NGU can be unstable and fail to learn an appropriate approximation of  for all the state-action value functions in the family, even in simple environments. This is especially the case when the scale and sparseness of  and  are both different, or when one reward is more noisy than the other. We conjecture that learning a common state-action value function for a mix of rewards is difficult when the rewards are very different in nature. Therefore, in Sec.~\ref{subsec:value1}, we propose an architectural modification to tackle this issue.

Our agent is a deep distributed RL agent, in the lineage of R2D2 and NGU. As such, it decouples the data collection and the learning processes by having many actors feed data to a central prioritized replay buffer. A learner can then sample training data from this buffer, as shown in Fig.~\ref{fig:distributed} (for implementation details and hyperparameters refer to App.~\ref{app:distributed}). 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/DistributedParadigm2.png}
    \caption{A schematic depiction of a distributed deep RL agent.} 
    \label{fig:distributed}
\end{figure}
More precisely, the replay buffer contains sequences of transitions that are removed regularly in a FIFO-manner. These sequences come from actor processes that interact with independent copies of the environment, and they are prioritized based on temporal differences errors~\citep{kapturowski2018recurrent}. The priorities are initialized by the actors and updated by the learner with the updated state-action value function . According to those priorities, the learner samples sequences of transitions from the replay buffer to construct an RL loss. Then, it updates the parameters of the neural network  by minimizing the RL loss to approximate the optimal state-action value function. Finally, each actor shares the same network architecture as the learner but with different weights. We refer as  to the parameters of the th actor. The learner weights  are sent to the actor frequently, which allows it to update its own weights . Each actor uses different values , which are employed to follow an -greedy policy based on the current estimate of the state-action value function . In particular, at the beginning of each episode and in each actor, NGU uniformly selects a pair . We hypothesize that this process is sub-optimal and propose to improve it in Sec.~\ref{subsec:adaptive1} by introducing a meta-controller for each actor that adapts the data collection process. 

\section{Improvements to NGU}
\label{sec:improvments}

\subsection{State-Action Value Function Parameterization}
\label{subsec:value1}

The proposed architectural improvement consists in splitting the state-action value function in the following way:

where  and  are the extrinsic and intrinsic components of  respectively. The sets of weights  and  separately parameterize two neural networks with identical architecture and . Both  and  are optimized separately in the learner with rewards  and  respectively, but with the same target policy . More precisely, to train the weights  and , we use the same sequence of transitions sampled from the replay, but with two different transformed Retrace loss functions~\citep{munos2016safe}. For  we compute an extrinsic transformed Retrace loss on the sequence transitions with rewards  and target policy , whereas for  we compute an intrinsic transformed Retrace loss on the same sequence of transitions but with rewards  and target policy . A reminder of how to compute a transformed Retrace loss on a sequence of transitions with rewards  and target policy  is provided in App.~\ref{app:retrace}.

In addition, in App.~\ref{app:decoupling}, we show that this optimization of separate state-action values is equivalent to the optimization of the original single state-action value function with reward  (under a simple gradient descent optimizer). Even though the theoretical objective being optimized is the same, the parameterization is different: we use two different neural networks to approximate each one of these state-action values (a schematic and detailed figures of the architectures used can be found in App.~\ref{app:neural}). By doing this, we allow each network to adapt to the scale and variance associated with their corresponding reward, and we also allow for the associated optimizer state to be separated for intrinsic and extrinsic state-action value functions.

Moreover, when a transformed Bellman operator~\citep{pohlen2018observe} with function  is used (see App.~\ref{app:background}), we can split the state-action value function in the following way:

In App.~\ref{app:decoupling}, we also show that the optimization of separated transformed state-action value functions is equivalent to the optimization of the original single transformed state-action value function. In practice, choosing a simple or transformed split does not seem to play an important role in terms of performance (empirical evidence and an intuition behind this result can be found in App.~\ref{app:mix}). In our experiments, we choose an architecture with a simple split which corresponds to  being the identity, but still use the transformed Retrace loss functions.

\subsection{Adaptive Exploration over a Family of Policies}
\label{subsec:adaptive1}
The core idea of NGU is to jointly train a family of policies with different degrees of exploratory behaviour using a single network architecture.
In this way, training these exploratory policies plays the role of a set of auxiliary tasks that can help train the shared architecture even in the absence of extrinsic rewards.
A major limitation of this approach is that all policies are trained equally, regardless of their contribution to the learning progress. 
We propose to incorporate a meta-controller that can adaptively select which policies to use both at training and evaluation time.
This carries two important consequences. Firstly, by selecting which policies to prioritize during training, we can allocate more of the capacity of the network to better represent the state-action value function of the policies that are most relevant for the task at hand.
Note that this is likely to change throughout the training process, naturally building a curriculum to facilitate training.
As mentioned in Sec.~\ref{sec:background}, policies are represented by pairs of exploration rate and discount factor, , which determine the discounted cumulative rewards to maximize. It is natural to expect policies with higher  and lower  to make more progress early in training, while the opposite would be expected as training progresses.
Secondly, this mechanism also provides a natural way of choosing the best policy in the family to use at evaluation time.
Considering a wide range of values of  with , provides a way of automatically adjusting the discount factor on a per-task basis. This significantly increases the generality of the approach.

We propose to implement the meta-controller using a non-stationary multi-arm bandit algorithm running independently on each actor. The reason for this choice, as opposed to a global meta-controller, is that each actor follows a different -greedy policy which may alter the choice of the optimal arm.
Each arm  from the -arm bandit is linked to a policy in the family and corresponds to a pair .
At the beginning of each episode, say, the -th episode, the meta-controller chooses an arm  setting which policy will be executed. We use capital letters for the arm  because it is a random variable.
Then the -th actor acts -greedily with respect to the corresponding state-action value function, , for the whole episode. 
The undiscounted extrinsic episode returns, noted , are used as a reward signal to train the multi-arm bandit algorithm of the meta-controller. 

The reward signal  is non-stationary, as the agent changes throughout training. 
Thus, a classical bandit algorithm such as Upper Confidence Bound \citep[UCB; ][]{garivier2008upperconfidence} will not be able to adapt to the changes of the reward through time. Therefore, we employ a simplified sliding-window UCB with -greedy exploration.
With probability , this algorithm runs a slight modification of classic UCB on a sliding window of size  and selects a random arm with probability  (details of the algorithms are provided in App.~\ref{app:bandits}). 


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Agent57_Timeline.png}
    \vspace{-3ex}
    \caption{Capped human normalized score where we observe at which point the agent surpasses the human benchmark on the last 6 games.} 
    \label{fig:superhuman_timeline}
    \vspace{-3ex}
\end{figure}

Note that the benefit of adjusting the discount factor through training and at evaluation could be applied even in the absence of intrinsic rewards. To show this, we propose augmenting a variant of R2D2 with a meta-controller. In order to isolate the contribution of this change, we evaluate a variant of R2D2 which uses the same RL loss as Agent57.
Namely, a transformed Retrace loss as opposed to a transformed n-step loss as in the original paper. We refer to this variant as R2D2 (Retrace) throughout the paper. In all other aspects, R2D2 (Retrace) is exactly the same algorithm as R2D2.
We incorporate the joint training of several policies parameterized by  to R2D2 (Retrace). We refer to this algorithm as \emph{R2D2 (bandit)}.

\section{Experiments}
\label{sec:experiments}

We begin this section by describing our experimental setup. Following NGU, Agent57 uses a family of coefficients  of size . The choice of discounts  differs from that of NGU to allow for higher values, ranging from  to  (see App.~\ref{app:family} for details). The meta-controller uses a window size of  episodes and  for the actors and a window size of  episodes and . All the other hyperparameters are identical to those of NGU, including the standard preprocessing of Atari frames. For a complete description of the hyperparameters and preprocessing we use, please see App.~\ref{app:hyperparameters}. For all agents we run (that is, all agents except MuZero where we report numbers presented in~\citet{schrittwieser2019mastering}), we employ a separate evaluator process to continuously record scores. We record the undiscounted episode returns averaged over  seeds and using a windowed mean over  episodes. For our best algorithm, Agent57, we report the results averaged over  seeds on all games to strengthen the significance of the results. On that average, we report the maximum over training as their final score, as done in~\citet{fortunato2017noisy, badia2020never}. Further details on our evaluation setup are described in App.~\ref{app:distributed}.

In addition to using human normalized scores , we report the capped human normalized scores, .
This measure is a better descriptor for evaluating general performance, as it puts an emphasis in the games that are below the average human performance benchmark. Furthermore, and avoiding any issues that aggregated metrics may have, we also provide all the scores that all the ablations obtain in all games we evaluate in App.~\ref{app:tabatari10}.

\begin{table*}
\scriptsize
\centering
\caption{Number of games above human, mean capped, mean and median human normalized scores for the 57 Atari games.}
\vspace{1ex}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Statistics & Agent57 & R2D2 (bandit) & NGU & R2D2 (Retrace) & R2D2 & MuZero \\
\hline
 Capped mean & \bf{100.00} & 96.93 & 95.07 & 94.20 & 94.33 & 89.92 \\
 Number of games  human & \bf{57} & 54 & 51 & 52 & 52 & 51 \\
 Mean & 4766.25 & 5461.66 & 3421.80 & 3518.36 & 4622.09 & \bf{5661.84} \\
 Median & 1933.49 & 2357.92 & 1359.78 & 1457.63 & 1935.86 & \bf{2381.51} \\
 40th Percentile & 1091.07 & \bf{1298.80} & 610.44 & 817.77 & 1176.05 & 1172.90 \\
 30th Percentile & 614.65 & \bf{648.17} & 267.10 & 420.67 & 529.23 & 503.05 \\
 20th Percentile & \bf{324.78} & 303.61 & 226.43 & 267.25 & 215.31 & 171.39 \\
 10th Percentile & \bf{184.35} & 116.82 & 107.78 & 116.03 & 115.33 & 75.74 \\
 5th Percentile & \bf{116.67} & 93.25 & 64.10 & 48.32 & 50.27 & 0.03 \\
\hline
\end{tabular}
\label{tab:normalizedscores}
\end{table*}

We structure the rest of this section in the following way: firstly, we show an overview of the results that Agent57 achieves. Then we proceed to perform ablations on each one of the improvements we propose for our model.


\subsection{Summary of the Results}
\label{subsec:summary}

Tab.~\ref{tab:normalizedscores} shows a summary of the results we obtain on all 57 Atari games when compared to baselines. MuZero obtains the highest uncapped mean and median human normalized scores, but also the lowest capped scores. This is due to the fact that MuZero performs remarkably well in some games, such as \textit{Beam Rider}, where it shows an uncapped score of , but at the same time catastrophically fails to learn in games such as \textit{Venture}, achieving a score that is on par with a random policy. We see that the meta-controller improvement successfully transfers to R2D2: the proposed variant R2D2 (bandit) shows a mean, median, and CHNS that are much higher than R2D2 with the same Retrace loss. Finally, Agent57 achieves a median and mean that is greater than NGU and R2D2, but also its CHNS is 100\%. This shows the generality of Agent57: not only it obtains a strong mean and median, but also it is able to obtain strong performance on the tail of games in which MuZero and R2D2 catastrophically fail. This is more clearly observed when looking at different percentiles: up to the th percentile, Agent57 shows much greater performance, only slightly surpassed by R2D2 (bandit) when we examine higher percentiles.
In Fig.~\ref{fig:superhuman_timeline} we report the performance of Agent57 in isolation on the  games. We show the last  games (in terms of number of frames collected by the agents) in which the algorithm surpasses the human performance benchmark. As shown, the benchmark over games is beaten in a long-tailed fashion, where Agent57 uses the first  billion frames to surpass the human benchmark on  games. After that, we find hard exploration games, such as \textit{Montezuma's Revenge}, \textit{Pitfall!}, and \textit{Private Eye}. Lastly, Agent57 surpasses the human benchmark on \textit{Skiing} after  billion frames.
To be able to achieve such performance on \textit{Skiing}, Agent57 uses a high discount (as we show in Sec. \ref{subsec:adaptive2}). This naturally leads to high variance in the returns, which leads to needing more data in order to learn to play the game. One thing to note is that, in the game of \textit{Skiing}, the human baseline is very competitive, with a score of  , where  is random and  is the optimal score one can achieve.

In general, as performance in Atari keeps improving, it seems natural to concentrate on the tail of the distribution, i.e., pay attention to those games for which progress in the literature has been historically much slower than average.
We now present results for a subset of 10 games that we call the \emph{challenging set}. It consists of the six hard exploration games as defined in~\cite{bellemare2016unifying}, plus games that require long-term credit assignment. More concretely, the games we use are: \textit{Beam Rider}, \textit{Freeway}, \textit{Montezuma's Revenge}, \textit{Pitfall!}, \textit{Pong}, \textit{Private Eye}, \textit{Skiing}, \textit{Solaris}, \textit{Surround}, and \textit{Venture}. 


In Fig.~\ref{fig:progression} we can see the performance progression obtained from incorporating each one of the improvements we make on top of NGU. Such performance is reported on the selection of  games mentioned above. We observe that each one of the improvements results in an increment in final performance. Further, we see that each one of the improvements that is part of Agent57 is necessary in order to obtain the consistent final performance of  CHNS.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/Agent57_Separate_Nets.png}
    \caption{Performance progression on the 10-game \emph{challenging set} obtained from incorporating each one of the improvements.} 
    \label{fig:progression}
\end{figure}

\subsection{State-Action Value Function Parameterization}
\label{subsec:value2}
 
We begin by evaluating the influence of the state-action value function parametrization on a minimalistic gridworld environment, called ``random coin''.
It consists of an empty room of size  where a coin and an agent are randomly placed at the start of each episode. The agent can take four possible actions (up, down, left right) and episodes are at most  steps long. If the agent steps over the coin, it receives a reward of  and the episode terminates.
In Fig.~\ref{fig:randomcoinbar} we see the results of NGU with and without the new parameterization of its state-action value functions.
We report performance after  million frames. 
We compare the extrinsic returns for the policies that are the exploitative () and the most exploratory (with the largest  in the family).
Even for small values of the exploration rates (), this setting induces very different exploratory and exploitative policies.
Maximizing the discounted extrinsic returns is achieved by taking the shortest path towards the coin (obtaining an extrinsic return of one), whereas maximizing the augmented returns is achieved by avoiding the coin and visiting all remaining states (obtaining an extrinsic return of zero).
In principle, NGU should be able to learn these policies jointly.
However, we observe that the exploitative policy in NGU struggles to solve the task as intrinsic motivation reward scale increases. 
As we increase the scale of the intrinsic reward, its value becomes much greater than that of the extrinsic reward.
As a consequence, the conditional state-action value network of NGU is required to represent very different values depending on the  we condition on. This implies that the network is increasingly required to have more flexible representations. 
Using separate networks dramatically increases its robustness to the intrinsic reward weight that is used.
Note that this effect would not occur if the episode did not terminate after collecting the coin. In such case, exploratory and exploitative policies would be allowed to be very similar: both could start by collecting the coin as quickly as possible. 
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/pycolab_2.png}
    \caption{Extrinsic returns for the exploitative () and most exploratory () on ``random coin'' for different values of the intrinsic reward weight, . \emph{(Top)} NGU\emph{(Bottom)} NGU with Separate networks for intrinsic and extrinsic values.} 
    \label{fig:randomcoinbar}
    \vspace{-3ex}
\end{figure}
In Fig.~\ref{fig:progression} we can see that this improvement also translates to the \emph{challenging set}. NGU achieves a much lower average CHNS than its separate network counterpart. We also observe this phenomenon when we incorporate the meta-controller. Agent57 suffers a drop of performance that is greater than  when the separate network improvement is removed.

We can also see that it is a general improvement: it does not show worse performance on any of the  games of the challenging set. More concretely, the largest improvement is seen in the case of \textit{Surround}, where NGU obtains a score on par with a random policy, whereas with the new parametrization it reaches a score that is nearly optimal. This is because \textit{Surround} is a case that is similar to the ``random coin'' environment mentioned above: as the player makes progress in the game, they have the choice to surround the opponent snake, receive a reward, and start from the initial state, or keep wandering around without capturing the opponent, and thus visiting new states in the world.

\subsection{Backprop Through Time Window Size}
\label{subsec:tracelength}

In this section we analyze the impact of having a backprop through time window size. More concretely, we analyze its impact on the base algorithm R2D2 to see its effect without NGU or any of the improvements we propose. Further, we also analyze its effect on Agent57, to see if any of the improvements on NGU overlap with this change. In both cases, we compare using backprop through time window sizes of  (default in R2D2) versus .

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Agent57_Solaris.png}
    \caption{\textit{Solaris} learning curves with small and long backprop through time window sizes for both R2D2 and Agent57.} 
    \label{fig:solaristracelength}
    \vspace{-3ex}
\end{figure}

In aggregated terms over the \emph{challenging set}, its effect seems to be the same for both R2D2 and Agent57: using a longer backprop through time window appears to be initially slower, but results in better overall stability and slightly higher final score. A detailed comparison over those  games is shown in App.~\ref{app:tracelength}. This effect can be seen clearly in the game of \textit{Solaris}, as observed in Fig.~\ref{fig:solaristracelength}. This is also the game showing the largest improvement in terms of final score.
This is again general improvement, as it enhances performance on all the \emph{challenging set} games. For further details we report the scores in App. \ref{app:tabatari10}.

\subsection{Adaptive Exploration}
\label{subsec:adaptive2}
In this section, we analyze the effect of using the meta-controller described in Sec.~\ref{subsec:value1} in both the actors and the evaluator. To isolate the contribution of this improvement, we evaluate two settings: R2D2 and NGU with separate networks, with and without meta-controller.
Results are shown in Fig. \ref{fig:r2d2progression}. Again, we observe that this is a general improvement in both comparisons. Firstly, we observe that there is a great value in this improvement on its own, enhancing the final performance of R2D2 by close to  CHNS. Secondly, we observe that the benefit on NGU with separate networks is more modest than for R2D2. This indicates that there is a slight overlap in the contributions of the separate network parameterization and the use of the meta-controller.
The bandit algorithm can adaptively decrease the value of  when the difference in scale between intrinsic and extrinsic rewards is large. 
Using the meta-controller allows to include very high discount values in the set .
Specifically, running R2D2 with a high discount factor,  surpasses the human baseline in the game of \textit{Skiing}. However, using that hyperparameter across the full set of games, renders the algorithm very unstable and damages its end performance. All the scores in the \emph{challenging set} for a fixed high discount () variant of R2D2 are reported in App.~\ref{app:tabatari10}.
When using a meta-controller, the algorithm does not need to make this compromise: it can adapt it in a per-task manner.
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Agent57_Bandit.png}
    \caption{Performance comparison for adaptive exploration on the 10-game \emph{challenging set}.} 
    \label{fig:r2d2progression}
    \vspace{-3ex}
\end{figure}

Finally, the results and discussion above show why it is beneficial to use different values of  and  on a per-task basis. At the same time, in Sec. \ref{sec:improvments} we hypothesize it would also be useful to vary those coefficients throughout training. In Fig. \ref{fig:mixture_chosen} we can see the choice of (, ) producing highest returns on the meta-controller of the evaluator across training for several games. Some games clearly have a preferred mode: on \textit{Skiing} the high discount combination is quickly picked up when the agent starts to learn, and on \textit{Hero} a high  and low  is generally preferred at all times. On the other hand, some games have different preferred modes throughout training: on \textit{Gravitar}, \textit{Crazy Climber}, \textit{Beam Rider}, and \textit{Jamesbond}, Agent57 initially chooses to focus on exploratory policies with low discount, and, as training progresses, the agent shifts into producing experience from higher discount and more exploitative policies.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/mixture_chosen.png}
    \caption{Best arm chosen by the evaluator of Agent57 over training for different games.}
    \label{fig:mixture_chosen}
    \vspace{-3ex}
\end{figure}

\section{Conclusions}
\label{subsec:conclusions}

We present the first deep reinforcement learning agent with performance above the human benchmark on all  Atari games. The agent is able to balance the learning of different skills that are required to be performant on such diverse set of games: \textit{exploration and exploitation} and \textit{long-term credit assignment}. To do that, we propose simple improvements to an existing agent, \textit{Never Give Up}, which has good performance on hard-exploration games, but in itself does not have strong overall performance across all  games. These improvements are i) using a different parameterization of the state-action value function, ii) using a meta-controller to dynamically adapt the novelty preference and discount, and iii) the use of longer backprop-through time window to learn from using the Retrace algorithm.

This method leverages a great amount of computation to its advantage: similarly to NGU, it is able to scale well with increasing amounts of computation. This has also been the case with the many recent achievements in deep RL~\citep{silver2016mastering, andrychowicz2018learning, vinyals2019grandmaster}. While this enables our method to achieve strong performance, an interesting research direction is to pursue ways in which to improve the data efficiency of this agent. Additionally, this agent shows an average capped human normalized score of . However, in our view this by no means marks the end of Atari research, not only in terms of efficiency as above, but also in terms of general performance. We offer two views on this: firstly, analyzing the performance among percentiles gives us new insights on how general algorithms are. While Agent57 achieves great results on the first percentiles of the  games and holds better mean and median performance than NGU or R2D2, as MuZero shows, it could still obtain much better average performance. Secondly, as pointed out by~\citet{toromanoff2019deep}, all current algorithms are far from achieving optimal performance in some games. To that end, key improvements to use might be enhancements in the representations that Agent57 and NGU use for exploration, planning (as suggested by the results achieved by MuZero) as well as better mechanisms for credit assignment (as highlighted by the results seen in \textit{Skiing}).

\section*{Acknowledgments}
We thank Daan Wierstra, Koray Kavukcuoglu, Vlad Mnih, Vali Irimia, Georg Ostrovski, Mohammad Gheshlaghi Azar, R\'emi Munos, Bernardo Avila Pires, Florent Altch\'e, Steph Hughes-Fitt, Rory Fitzpatrick, Andrea Banino, Meire Fortunato, Melissa Tan, Benigno Uria, Borja Ibarz, Andre Barreto, Diana Borsa, Simon Osindero, Tom Schaul, and many other colleagues at DeepMind for helpful discussions and comments on the manuscript.

\bibliography{biblio}
\bibliographystyle{icml2020}

\appendix
\onecolumn

\section{Background on MDP}
\label{app:background}
A Markov decision process~\citep[MDP; ][]{puterman1990markov} is a tuple , with  being the state space,  being the action space,  the state-transition distribution maps each state-action tuple  to a probability distribution over states (with  denoting the probability of transitioning to state  from  by choosing action ), the reward function  and  the discount factor. A stochastic policy  maps each state to a distribution over actions ( denotes the probability of choosing action  in state ). A deterministic policy  can also be represented by a distribution over actions  such that . We will use one or the other concept with the same notation  in the remaining when the context is clear. 

Let  be the distribution over trajectories  generated by a policy , with , ,  and . Then, the state-action value function  for the policy  and the  state-action tuple  is defined as:

The optimal state-action value function  is defined as:

where the  is taken over all stochastic policies.

Let define the one-step evaluation Bellman operator , for all functions  and for all state-action tuples , as:

The one-step evaluation Bellman operator can also be written with vectorial notations:

where  is a transition matrix representing the effect of acting according to  in a MDP with dynamics . The evaluation Bellman operator is a contraction and its fixed point is . 

Finally let define the greedy operator , for all functions  and for all state , as:

Then, one can show~\citep{puterman1990markov}, via a fixed point argument, that the following discrete scheme:

where  can be initialized arbitrarily, converges to . This discrete scheme is called the one-step value iteration scheme.

Throughout the article, we also use transformed Bellman operators (see Sec.~\ref{subsec:lossfunction}). The one-step transformed evaluation Bellman operator , for all functions  and for all state-action tuples , can be defined as:

where  is a monotonically increasing and invertible  squashing function that scales the state-action value function to make it easier to approximate for a neural network.  
In particular, we use the function :

with  a small number.
The one-step transformed evaluation Bellman operator can also be written with vectorial notations:


Under some conditions on ~\citep{pohlen2018observe} and via a contraction argument, one can show that the transformed one-step value iteration scheme:

where  can be initialized arbitrarily, converges. We note this limit . \section{Extrinsic-Intrinsic Decomposition}
\label{app:decoupling}
For an intrinsically-motivated agent, the reward function  is a linear combination of the intrinsic reward  and the extrinsic reward :

One can compute the optimal state-action value function  via the value iteration scheme:

where  can be initialized arbitrarily.


Now, we want to show how we can also converge to  using separate intrinsic and extrinsic state-action value functions. Indeed, let us consider the following discrete scheme:

where the functions  can be initialized arbitrarily.

Our goal is simply to show that the linear combination of extrinsic and intrinsic state-action value function :

verifies a one-step value iteration scheme with respect to the reward  and therefore converges to . To show that let us rewrite :

Therefore we have that  satisfies a value iteration scheme with respect to the reward :

and by the contraction property:


This result means that we can compute separately  and  and then mix them to obtain the same behavior than if we had computed  directly with the mixed reward . This implies that we can separately compute the extrinsic and intrinsic component. Each architecture will need to learn their state-action value for different mixtures  and then act according to the greedy policy of the mixture of the state-action value functions. This result could also be thought as related to~\citet{barreto2017successor} which may suggest potential future research directions.

The same type of result holds for the transformed state-action value functions. Indeed let us consider the optimal transformed state-action value function  that can be computed via the following discrete scheme:

where  can be initialized arbitrarily.

Now, we show how we can compute  differently using separate intrinsic and extrinsic state-action value functions. Indeed, let us consider the following discrete scheme:

where the functions  can be initialized arbitrarily.

We want to show that  defines as:

verifies the one-step transformed value iteration scheme with respect to the reward  and therefore converges to . To show that let us rewrite :


Thus we have that  satisfies the one-step transformed value iteration scheme with respect to the reward :

and by contraction:


One can remark that when the transformation  is the identity, we recover the linear mix between intrinsic and extrinsic state-action value functions.
 \section{Retrace and Transformed Retrace}
\label{app:retrace}
Retrace~\citep{munos2016safe} is an off-policy RL algorithm for evaluation or control. In the evaluation setting the goal is to estimate the state-action value function  of a target policy  from trajectories drawn from a behaviour policy . In the control setting the goal is to build a sequence of target policies  and state-action value functions  in order to approximate .

The evaluation Retrace operator , that depends on  and , is defined as follows, for all functions  and for all state-action tuples :

where the temporal difference  is defined as:

and the trace coefficients  as:

where  is a fixed parameter .
The operator  is a multi-step evaluation operator that corrects the behaviour of  to evaluate the policy . It has been shown in Theorem 1 of~\citet{munos2016safe} that  is the fixed point of . In addition, Theorem 2 of~\citet{munos2016safe} explains in which conditions the Retrace value iteration scheme:

converges to the optimal state-action value function , where  is initialized arbitrarily and  is an arbitrary sequence of policies that may depend on .

As in the case of the one-step Bellman operator, we can also define a transformed counterpart to the Retrace operator. More specifically, we can define the transformed Retrace operator , for all functions  and for all state-action tuples :

where the temporal difference  is defined as:

As in the case of the Retrace operator, we can define the transformed Retrace value iteration scheme:

where  is initialized arbitrarily and  is an arbitrary sequence of policies.

\subsection{Extrinsic-Intrinsic Decomposition for Retrace and Transformed Retrace}
\label{subsec:decompositionretrace}
Following the same methodology than App~.\ref{app:decoupling}, we can also show that the state-action value function can be decomposed in extrinsic and intrinsic components for the Retrace and transformed Retrace value iteration schemes when the reward is of the form . 

Indeed if we define the following discrete scheme:

where the functions  can be initialized arbitrarily and  is an arbitrary sequence of policies. Then, it is straightforward to show that the linear combination :

verifies the Retrace value iteration scheme:


Likewise, if we define the following discrete scheme:

where the functions  can be initialized arbitrarily and  is an arbitrary sequence of policies. Then, it is also straightforward to show that  defines as:

verifies the transformed Retrace value iteration scheme:


\subsection{Retrace and Transformed Retrace Losses for Neural Nets.}
\label{subsec:lossfunction}

In this section, we explain how we approximate with finite data and neural networks the Retrace value iteration scheme. To start, one important thing to remark is that we can rewrite the evaluation step:

with:

where  can be any norm over the function space . This means that the evaluation step can be seen as an optimization problem over a functional space where the optimization consists in finding a function  that matches the target .

In practice, we face two important problems. The search space  is too big and we cannot evaluate  everywhere because we have a finite set of data. To tackle the former, a possible solution is to use function approximation such as neural networks. Thus, we parameterize the state action value function  (where  is the set of parameters of the neural network) also called online network. Concerning the latter, we are going to build sampled estimates of  and use them as targets for our optimization problem. In practice, the targets are built from a previous and fixed set of parameters  of the neural network.  is called the target network. The target network is updated to the value of the online network at a fixed frequency during the learning.

More precisely, let us consider a batch of size  of finite sampled sequences of size :  starting from  and then following the behaviour policy .
Then, we can define the finite sampled-Retrace targets as:

where  is the target policy.

Once the targets are computed, the goal is to find a parameter  that fits those targets by minimizing the following loss function:

This is done by an optimizer such as gradient descent for instance. Once  is updated by the optimizer, a new loss with new targets is computed and minimized until convergence.

Therefore in practice the evaluation step of the Retrace value iteration scheme  is approximated by minimizing the loss  with an optimizer. The greedy step  is realized by simply being greedy with respect to the online network and choosing the target policy as follows: .

In the case of a transformed Retrace operator, we have  the following targets:


And the transformed Retrace loss function is:
 \section{Multi-arm Bandit Formalism}
\label{app:bandits}
This section describes succinctly the multi-arm bandit (MAB) paradigm, upper confidence bound (UCB) algorithm and sliding-window UCB algorithm. For a more thorough explanation and analysis we refer the reader to~\citet{garivier2008upperconfidence}. 

At each time , a MAB algorithm chooses an arm  among the possible arms  according to a policy  that is conditioned on the sequence of previous actions and rewards. Doing so, it receives a reward  . In the stationary case, the rewards  for a given arm  are modelled by a sequence of i.i.d random variables. In the non-stationary case, the rewards  are modelled by a sequence of independent random variables but whose distributions could change through time.

The goal of a MAB algorithm is to find a policy  that maximizes the expected cumulative reward for a given horizon :


In the stationary case, the UCB algorithm has been well studied and is commonly used. Let us define the number of times an arm  has been played after  steps:

Let us also define the empirical mean of an arm  after  steps:


The UCB algorithm is then defined as follows:


In the non-stationary case, the UCB algorithm cannot adapt to the change of reward distribution and one can use a sliding-window UCB in that case. It is commonly understood that the window length  should be way smaller that the horizon .
Let us define the number of times an arm  has been played after  steps for a window of length :

where  means . Let define the empirical mean of an arm  after  steps for a window of length :


Then , the sliding window UCB can be defined as follows:

where  means .

In our experiments, we use a simplified sliding window UCB with -greedy exploration:

where  is a random value drawn uniformly from  and  a random action drawn uniformly from . \section{Implementation details of the distributed setting}
\label{app:distributed}

{\bf Replay buffer:} it stores fixed-length sequences of \emph{transitions}  along with their priorities .
A transition is of the form  . Such transitions are also called \emph{timesteps} and the length of a sequence  is called the \emph{trace length}. In addition, adjacent sequences in the replay buffer overlap by a number of timesteps called the \emph{replay period} and the sequences never cross episode boundaries. Let us describe each element of a transition:
\begin{itemize}
    \item : extrinsic reward at the previous time.
    \item : intrinsic reward at the previous time.
    \item : action done by the agent at the previous time.
    \item : recurrent state (in our case hidden state of the LSTM) at the previous time.
    \item : observation provided by the environment at the current time.
    \item : action done by the agent at the current time.
    \item : recurrent state (in our case hidden state of the LSTM) at the current time.
    \item : the probability of choosing the action .
    \item : index of the pair  chosen at a beginning of an episode in each actor by the multi-arm bandit algorithm (fixed for the whole sequence).
    \item : extrinsic reward at the current time.
    \item : intrinsic reward at the current time
    \item : observation provided by the environment at the next time.
\end{itemize}
In our experiment, we choose a trace length of  with a replay period of  or a trace length of  with a replay period of .
Please refer to~\citep{kapturowski2018recurrent} for a detailed experimental of trade-offs on different treatments of recurrent states in the replay. Finally, concerning the priorities, we followed the same prioritization scheme proposed by~\citet{kapturowski2018recurrent} using a mixture of max and mean of the TD-errors in the sequence with priority exponent .

{\bf Actors}: each of the  actors shares the same network architecture as the learner but with different weights , with . The -th actor updates its weights  every 400 frames by copying the weights of the learner. At the beginning of each episode, each actor chooses, via a multi-arm bandit algorithm, an index  that represents a pair  in the family of pairs . In addition, the recurrent state is initialized to zero.
To act, an actor will need to do a forward pass on the network in order to compute the state-action value for all actions, noted . To do so the inputs of the network are :
\begin{itemize}
    \item : the observation at time .
    \item : the extrinsic reward at the previous time, initialized with .
    \item : the intrinsic reward at the previous time, initialized with .
    \item : the action at the previous time,  is initialized randomly.
    \item : recurrent state at the previous time, is initialized with .
    \item : the index of the pair  chosen by the multi-arm bandit algorithm (fixed for all the episode).
\end{itemize}

At time , the th actor acts -greedy with respect to :

where  is a random value drawn uniformly from  and  a random action drawn uniformly from . The probability  associated to  is therefore:

where  is the cardinal number of the action space,  in the case of Atari games.
Then, the actor plays the action  and computes the intrinsic reward  and the environment produces the next observation  and the extrinsic reward . This process goes on until the end of the episode.

The value of the noise  is chosen according to the same formula established by~\citet{horgan2018distributed}:

where  and  . In our experiments, we fix the number of actors to . Finally, the actors send the data collected to the replay along with the priorities.

{\bf Evaluator}: the evaluator shares the same network architecture as the learner but with different weights . The evaluator updates its weights  every  episodes frames by copying the weights of the learner. Unlike the actors, the experience produced by the evaluator is not sent to the replay buffer. The evaluator alternates between the following states every  episodes:

\begin{itemize}
    \item \textbf{Training bandit algorithm}: the evaluator chooses, via a multi-arm bandit algorithm, an index  that represents a pair  in the family of pairs . Then it proceeds to act in the same way as the actors, described above. At the end of the episode, the undiscounted returns are used to train the multi-arm bandit algorithm. 
    \item \textbf{Evaluation}: the evaluator chooses the greedy choice of index ,  , so it acts with . Then it proceeds to act in the same way as the actors, described above. At the end of  episodes and before switching to the other mode, the results of those  episodes are average and reported.
\end{itemize}

{\bf Learner}: The learner contains two identical networks called the online and target networks with different weights  and  respectively \cite{mnih2015human}.
The target network's weights  are updated to  every  optimization steps. For our particular architecture, the weights  can be decomposed in a set of intrinsic weights  and  that have the same architecture. Likewise, we have . The intrinsic and extrinsic weights are going to be updated by their own transformed Retrace loss.  and  are updated by executing the following sequence of instructions:
\begin{itemize}
    \item First, the learner samples a batch of size  of fixed-length sequences of transitions  from the replay buffer.
    \item Then, a forward pass is done on the online network and the target with inputs  in order to obtain the state-action values .
    \item Once the state-action values are computed, it is now easy to compute the transformed Retrace losses  and  for each set of weights  and , respectively, as shown in Sec~.\ref{app:retrace}. The target policy  is greedy with respect to  or with respect to  in the case where we want to apply a transform  to the mixture of intrinsic and extrinsic state-action value functions.
    \item The transformed Retrace losses are optimized with an Adam optimizer.
    \item Like NGU, the inverse dynamics model and the random network distillation losses necessary to compute the intrinsic rewards are optimized with an Adam optimizer.
    \item Finally, the priorities are computed for each sampled sequence of transitions  and updated in the replay buffer.
\end{itemize}

\textbf{Computation used}: in terms of hardware we train the agent with a single GPU-based learner, performing approximately  network updates per second (each update on a mini-batch of  sequences of length . We use  actors, with each one performing  environment steps per second on Atari. \clearpage
\section{Network Architectures}
\label{app:neural}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/R2D2_model_dual.png}
    \caption{Sketch of the Agent57.} 
    \label{fig:spectrumarchitecture.}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/R2D2_model_detailed_dual.png}
    \caption{Detailed Agent57.} 
    \label{fig:detailedspectrumarchitecture.}
\end{figure} \clearpage
\section{Hyperparameters}

\subsection{Values of  and }
\label{app:family}
The intuition between the choice of the set  is the following. Concerning the  we want to encourage policies which are very exploitative and very exploratory and that is why we choose a sigmoid as shown in Fig.~\ref{fig:beta_distrib}. Concerning the  we would like to allow for long term horizons (high values of ) for exploitative policies (small values of ) and small term horizons (low values of ) for exploratory policies (high values of ). This is mainly due to the sparseness of the extrinsic reward and the dense nature of the intrinsic reward. This motivates the choice done in Fig.~\ref{fig:gamma_distrib}.   

\begin{figure}[h]
\label{app:graphs}
    \centering
    \subfigure[Values taken by the ]{\includegraphics[width=0.4\textwidth]{figures/beta_curve.png}\label{fig:beta_distrib}}
    \subfigure[Values taken by the ]{\includegraphics[width=0.4\textwidth]{figures/gamma_curve.png}\label{fig:gamma_distrib}}
    \caption{Values taken by the  and the  for  and .} 
\end{figure}


, 

where ,  ,  and .




\subsection{Atari pre-processing hyperparameters}
\label{atari_hypers}
In this section we detail the hyperparameters we use to pre-process the environment frames received from the Arcade Learning Environment. On Tab.~\ref{table_hyper_atari} we detail such hyperparameters. ALE is publicly available at \url{https://github.com/mgbellemare/Arcade-Learning-Environment}.

\begin{table}[h]
\centering
\begin{tabular}{l|c}
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Max episode length &   \\ \hline 
Num. action repeats &  \\ \hline
Num. stacked frames &  \\ \hline
Zero discount on life loss &  \\ \hline
Random noops range &  \\ \hline
Sticky actions &  \\ \hline
Frames max pooled & 3 and 4\\ \hline
Grayscaled/RGB & Grayscaled \\ \hline
Action set & Full \\ \hline
\end{tabular}
\caption{Atari pre-processing hyperparameters.}
\label{table_hyper_atari}
\vspace{-2ex}
\end{table}

\clearpage
\subsection{Hyperparameters Used}
\label{app:hyperparameters}

The hyperparameters that we used in all experiments are exactly like those of NGU. However, for completeness, we detail them below in Tab. \ref{tab:hyperparameters}. We also include the hyperparameters we use for the windowed UCB bandit.

\begin{small}
\begin{longtable}[!ht]{l|c}
\centering
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Number of mixtures  &  \\ \hline
Optimizer & AdamOptimizer (for all losses) \\ \hline
Learning rate (R2D2) &   \\ \hline
Learning rate (RND and Action prediction) &  \\ \hline
Adam epsilon &   \\ \hline
Adam beta1 &   \\ \hline
Adam beta2 &   \\ \hline
Adam clip norm &   \\ \hline
Discount  &  \\ \hline
Discount  &  \\ \hline
Batch size &  \\ \hline
Trace length &  \\ \hline
Replay period &  \\ \hline
Retrace  &  \\ \hline
R2D2 reward transformation &  \\ \hline
Episodic memory capacity &  \\ \hline
Embeddings memory mode & Ring buffer\\ \hline
Intrinsic reward scale  &  \\ \hline
Kernel  &  \\ \hline
Kernel num. neighbors used &  \\ \hline 
Replay capacity &  \\ \hline
Replay priority exponent &  \\ \hline 
Importance sampling exponent &  \\ \hline 
Minimum sequences to start replay &  \\ \hline 
Actor update period &  \\ \hline
Target Q-network update period &  \\ \hline
Embeddings target update period & once/episode \\ \hline
Action prediction network L2 weight &  \\ \hline
RND clipping factor  &  \\ \hline
Evaluation  &  \\ \hline
Target  &  \\ \hline
Bandit window size &  \\ \hline
Bandit UCB  &  \\ \hline
Bandit  &  \\ \hline
\caption{Agent57 hyperparameters.}
\label{tab:hyperparameters}
\vspace{-2ex}
\end{longtable}
\end{small}

\subsection{Hyperparameters Search Range}
The ranges we used to select the hyperparameters of Agent57 are displayed on Tab.~\ref{table_hyper_ranges}.
\begin{table}[!ht]
\centering
\begin{tabular}{l|c}
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Bandit window size  &  \\ \hline
Bandit  &  \\ \hline
\end{tabular}
\caption{Range of hyperparameters sweeps.}
\label{table_hyper_ranges}
\end{table} \section{Experimental Results}

\subsection{Atari 10: Table of Scores for the Ablations}
\label{app:tabatari10}
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 Games & \tiny{R2D2 (Retrace) long trace}  & \tiny{R2D2 (Retrace) high gamma} & \tiny{NGU sep. nets} & \tiny{NGU Bandit} & \tiny{Agent57 small trace} \\
\hline
 beam rider & 287326.72  5700.31 & \bf{349971.96  5595.38} & 151082.57  8666.19 & 249006.62  19662.62 & 244491.89  25348.14 \\
 freeway & \bf{33.91  0.09} & 32.84  0.06 & 32.91  0.58 & 26.43  1.66 & 32.87  0.12 \\
 montezuma revenge & 566.67  235.70 & 1664.89  1177.26 & \bf{11539.69  1227.71} & 7619.70  3444.76 & 7966.67  2531.58 \\
 pitfall & 0.00  0.00 & 0.00  0.00 & 15195.27  8005.22 & 2979.57  2919.08 & \bf{16402.61  10471.27} \\
 pong & \bf{21.00  0.00} & 21.00  0.00 & 21.00  0.00 & 20.56  0.28 & 21.00  0.00 \\
 private eye & 21729.91  9571.60 & 22480.31  10362.99 & 63953.38  26278.51 & 43823.40  4808.23 & \bf{80581.86  28331.16} \\
 skiing & -10784.13  2539.27 & -4596.26  601.04 & -19817.99  7755.19 & \bf{-4051.99  569.78} & -4278.86  270.96 \\
 solaris & \bf{52500.89  2910.14} & 14814.76  11361.16 & 44771.13  4920.53 & 43963.59  5765.41 & 17254.14  5840.70 \\
 surround & \bf{10.00  0.00} & 10.00  0.00 & 9.77  0.23 & -7.57  0.05 & 9.60  0.20 \\
 venture & 2100.00  0.00 & 1774.89  83.79 & \bf{3249.01  544.19} & 2228.04  305.50 & 2576.98  394.84 \\
\hline
\end{tabular}
\normalsize

\clearpage
\subsection{Backprop window length comparison}
\label{app:tracelength}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/Agent57_Trace_Length.png}
    \caption{Performance comparison for short and long backprob window length on the 10-game \emph{challenging set}.}
\end{figure}

\subsection{Identity versus -transform mixes comparison}
\label{app:mix}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/Agent57_identity_vs_hmix.png}
    \label{fig:mix}
    \caption{Performance comparison for identity versus -transform mixes on the 10-game \emph{challenging set}.}
\end{figure}
As shown in Fig~\ref{fig:mix}, choosing an identity or an -transform mix does not seem to make a difference in terms of performance. The only real important thing is that a combination between extrinsic and intrinsic happens whether it is linear or not. In addition, one can remark that for extreme values of  (, ), the quantities  and   have the same  because  is strictly increasing. Therefore, this means that on the extremes values of , the transform and normal value iteration schemes converge towards the same policy. For in between values of , this is not the case. But we can conjecture that when a transform operator and and identity mix are used, the value iteration scheme approximates a state-action value function that is optimal with respect to a non-linear combination of the intrinsic and extrinsic rewards , respectively. 

\subsection{Atari 57 Table of Scores}
\label{app:tab}
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 Games & Average Human & Random & Agent57 & R2D2 (Bandit) & MuZero \\
\hline
 alien & 7127.70 & 227.80 & 297638.17  37054.55 & 464232.43  7988.66 & \bf{741812.63} \\
 amidar & 1719.50 & 5.80 & 29660.08  880.39 & \bf{31331.37  817.79} & 28634.39 \\
 assault & 742.00 & 222.40 & 67212.67  6150.59 & 110100.04  346.06 & \bf{143972.03} \\
 asterix & 8503.30 & 210.00 & 991384.42  9493.32 & \bf{999354.03  12.94} & 998425.00 \\
 asteroids & 47388.70 & 719.10 & 150854.61  16116.72 & 431072.45  1799.13 & \bf{6785558.64} \\
 atlantis & 29028.10 & 12850.00 & 1528841.76  28282.53 & 1660721.85  14643.83 & \bf{1674767.20} \\
 bank heist & 753.10 & 14.20 & 23071.50  15834.73 & \bf{27117.85  963.12} & 1278.98 \\
 battle zone & 37187.50 & 2360.00 & 934134.88  38916.03 & \bf{992600.31  1096.19} & 848623.00 \\
 beam rider & 16926.50 & 363.90 & 300509.80  13075.35 & 390603.06  23304.09 & \bf{4549993.53} \\
 berzerk & 2630.40 & 123.70 & 61507.83  26539.54 & 77725.62  4556.93 & \bf{85932.60} \\
 bowling & 160.70 & 23.10 & 251.18  13.22 & 161.77  99.84 & \bf{260.13} \\
 boxing & 12.10 & 0.10 & 100.00  0.00 & \bf{100.00  0.00} & 100.00 \\
 breakout & 30.50 & 1.70 & 790.40  60.05 & 863.92  0.08 & \bf{864.00} \\
 centipede & 12017.00 & 2090.90 & 412847.86  26087.14 & 908137.24  7330.99 & \bf{1159049.27} \\
 chopper command & 7387.80 & 811.00 & 999900.00  0.00 & \bf{999900.00  0.00} & 991039.70 \\
 crazy climber & 35829.40 & 10780.50 & 565909.85  89183.85 & \bf{729482.83  87975.74} & 458315.40 \\
 defender & 18688.90 & 2874.50 & 677642.78  16858.59 & 730714.53  715.54 & \bf{839642.95} \\
 demon attack & 1971.00 & 152.10 & 143161.44  220.32 & 143913.32  92.93 & \bf{143964.26} \\
 double dunk & -16.40 & -18.60 & 23.93  0.06 & \bf{24.00  0.00} & 23.94 \\
 enduro & 860.50 & 0.00 & 2367.71  8.69 & 2378.66  3.66 & \bf{2382.44} \\
 fishing derby & -38.70 & -91.70 & 86.97  3.25 & 90.34  2.66 & \bf{91.16} \\
 freeway & 29.60 & 0.00 & 32.59  0.71 & \bf{34.00  0.00} & 33.03 \\
 frostbite & 4334.70 & 65.20 & 541280.88  17485.76 & 309077.30  274879.03 & \bf{631378.53} \\
 gopher & 2412.50 & 257.60 & 117777.08  3108.06 & 129736.13  653.03 & \bf{130345.58} \\
 gravitar & 3351.40 & 173.00 & 19213.96  348.25 & \bf{21068.03  497.25} & 6682.70 \\
 hero & 30826.40 & 1027.00 & \bf{114736.26  49116.60} & 49339.62  4617.76 & 49244.11 \\
 ice hockey & 0.90 & -11.20 & 63.64  6.48 & \bf{86.59  0.59} & 67.04 \\
 jamesbond & 302.80 & 29.00 & 135784.96  9132.28 & \bf{158142.36  904.45} & 41063.25 \\
 kangaroo & 3035.00 & 52.00 & \bf{24034.16  12565.88} & 18284.99  817.25 & 16763.60 \\
 krull & 2665.50 & 1598.00 & 251997.31  20274.39 & 245315.44  48249.07 & \bf{269358.27} \\
 kung fu master & 22736.30 & 258.50 & 206845.82  11112.10 & \bf{267766.63  2895.73} & 204824.00 \\
 montezuma revenge & 4753.30 & 0.00 & \bf{9352.01  2939.78} & 3000.00  0.00 & 0.00 \\
 ms pacman & 6951.60 & 307.30 & 63994.44  6652.16 & 62595.90  1755.82 & \bf{243401.10} \\
 name this game & 8049.00 & 2292.30 & 54386.77  6148.50 & 138030.67  5279.91 & \bf{157177.85} \\
 phoenix & 7242.60 & 761.40 & 908264.15  28978.92 & \bf{990638.12  6278.77} & 955137.84 \\
 pitfall & 6463.70 & -229.40 & \bf{18756.01  9783.91} & 0.00  0.00 & 0.00 \\
 pong & 14.60 & -20.70 & 20.67  0.47 & \bf{21.00  0.00} & 21.00 \\
 private eye & 69571.30 & 24.90 & \bf{79716.46  29515.48} & 40700.00  0.00 & 15299.98 \\
 qbert & 13455.00 & 163.90 & 580328.14  151251.66 & \bf{777071.30  190653.94} & 72276.00 \\
 riverraid & 17118.00 & 1338.50 & 63318.67  5659.55 & 93569.66  13308.08 & \bf{323417.18} \\
 road runner & 7845.00 & 11.50 & 243025.80  79555.98 & 593186.78  88650.69 & \bf{613411.80} \\
 robotank & 11.90 & 2.20 & 127.32  12.50 & \bf{144.00  0.00} & 131.13 \\
 seaquest & 42054.70 & 68.40 & 999997.63  1.42 & \bf{999999.00  0.00} & 999976.52 \\
 skiing & -4336.90 & -17098.10 & -4202.60  607.85 & \bf{-3851.44  517.52} & -29968.36 \\
 solaris & 12326.70 & 1236.30 & 44199.93  8055.50 & \bf{67306.29  10378.22} & 56.62 \\
 space invaders & 1668.70 & 148.00 & 48680.86  5894.01 & 67898.71  1744.74 & \bf{74335.30} \\
 star gunner & 10250.00 & 664.00 & 839573.53  67132.17 & \bf{998600.28  218.66} & 549271.70 \\
 surround & 6.50 & -10.00 & 9.50  0.19 & \bf{10.00  0.00} & 9.99 \\
 tennis & -8.30 & -23.80 & 23.84  0.10 & \bf{24.00  0.00} & 0.00 \\
 time pilot & 5229.20 & 3568.00 & 405425.31  17044.45 & 460596.49  3139.33 & \bf{476763.90} \\
 tutankham & 167.60 & 11.40 & \bf{2354.91  3421.43} & 483.78  37.90 & 491.48 \\
 up n down & 11693.20 & 533.40 & 623805.73  23493.75 & 702700.36  8937.59 & \bf{715545.61} \\
 venture & 1187.50 & 0.00 & \bf{2623.71  442.13} & 2258.93  29.90 & 0.40 \\
 video pinball & 17667.90 & 0.00 & 992340.74  12867.87 & \bf{999645.92  57.93} & 981791.88 \\
 wizard of wor & 4756.50 & 563.50 & 157306.41  16000.00 & 183090.81  6070.10 & \bf{197126.00} \\
 yars revenge & 54576.90 & 3092.90 & 998532.37  375.82 & \bf{999807.02  54.85} & 553311.46 \\
 zaxxon & 9173.30 & 32.50 & 249808.90  58261.59 & 370649.03  19761.32 & \bf{725853.90} \\
\hline
\end{tabular}

\begin{tabular}{|c|c|c|c|c|}
\hline
 Games & Agent57 & NGU & R2D2 (Retrace) & R2D2 \\
\hline
 alien & 297638.17  37054.55 & 312024.15  91963.92 & 228483.74  111660.11 & \bf{399709.08  106191.42} \\
 amidar & 29660.08  880.39 & 18369.47  2141.76 & 28777.05  803.90 & \bf{30338.91  1087.62} \\
 assault & 67212.67  6150.59 & 42829.17  7452.17 & 46003.71  8996.65 & \bf{124931.33  2627.16} \\
 asterix & 991384.42  9493.32 & 996141.15  3993.26 & 998867.54  191.35 & \bf{999403.53  76.75} \\
 asteroids & 150854.61  16116.72 & 248951.23  7561.86 & 345910.03  13189.10 & \bf{394765.73  16944.82} \\
 atlantis & 1528841.76  28282.53 & \bf{1659575.47  4140.68} & 1659411.83  9934.57 & 1644680.76  5784.97 \\
 bank heist & 23071.50  15834.73 & 20012.54  20377.89 & 16726.07  10992.11 & \bf{38536.66  11645.73} \\
 battle zone & 934134.88  38916.03 & 813965.40  94503.50 & 845666.67  51527.68 & \bf{956179.17  31019.66} \\
 beam rider & \bf{300509.80  13075.35} & 75889.70  18226.52 & 123281.81  4566.16 & 246078.69  3667.61 \\
 berzerk & 61507.83  26539.54 & 45601.93  5170.98 & \bf{73475.91  8107.24} & 64852.56  17875.17 \\
 bowling & 251.18  13.22 & 215.38  13.27 & \bf{257.88  4.84} & 229.39  24.57 \\
 boxing & \bf{100.00  0.00} & 99.71  0.25 & 100.00  0.00 & 99.27  0.35 \\
 breakout & 790.40  60.05 & 625.86  42.66 & 859.60  2.04 & \bf{863.25  0.34} \\
 centipede & 412847.86  26087.14 & 596427.16  7149.84 & \bf{737655.85  25568.85} & 693733.73  74495.81 \\
 chopper command & 999900.00  0.00 & 999900.00  0.00 & 999900.00  0.00 & \bf{999900.00  0.00} \\
 crazy climber & \bf{565909.85  89183.85} & 351390.64  62150.96 & 322741.20  23024.88 & 549054.89  39413.08 \\
 defender & 677642.78  16858.59 & 684414.06  3876.41 & 681291.73  3469.95 & \bf{692114.71  4864.99} \\
 demon attack & 143161.44  220.32 & 143695.73  154.88 & \bf{143899.22  53.78} & 143830.91  107.18 \\
 double dunk & 23.93  0.06 & -12.63  5.29 & \bf{24.00  0.00} & 23.97  0.03 \\
 enduro & 2367.71  8.69 & 2095.40  80.81 & 2372.77  3.50 & \bf{2380.22  5.47} \\
 fishing derby & 86.97  3.25 & 34.62  4.91 & \bf{87.83  2.78} & 87.81  1.28 \\
 freeway & 32.59  0.71 & 28.71  2.07 & \bf{33.48  0.16} & 32.90  0.11 \\
 frostbite & \bf{541280.88  17485.76} & 284044.19  227850.49 & 12290.11  7936.49 & 446703.01  63780.51 \\
 gopher & 117777.08  3108.06 & 119110.87  463.03 & 119803.94  3197.88 & \bf{126241.97  519.70} \\
 gravitar & \bf{19213.96  348.25} & 14771.91  843.17 & 14194.45  1250.63 & 17352.78  2675.27 \\
 hero & \bf{114736.26  49116.60} & 71592.84  12109.10 & 54967.97  5411.73 & 39786.01  7638.19 \\
 ice hockey & 63.64  6.48 & -3.15  0.47 & 86.56  1.21 & \bf{86.89  0.88} \\
 jamesbond & \bf{135784.96  9132.28} & 28725.27  2902.52 & 32926.31  3073.94 & 28988.32  263.79 \\
 kangaroo & 24034.16  12565.88 & \bf{37392.82  6170.95} & 15185.87  931.58 & 14492.75  5.29 \\
 krull & 251997.31  20274.39 & 150896.04  33729.56 & 149221.98  17583.30 & \bf{291043.06  10051.59} \\
 kung fu master & 206845.82  11112.10 & 215938.95  22050.67 & 228228.90  5316.74 & \bf{252876.65  10424.57} \\
 montezuma revenge & 9352.01  2939.78 & \bf{19093.74  12627.66} & 2300.00  668.33 & 2666.67  235.70 \\
 ms pacman & \bf{63994.44  6652.16} & 48695.12  1599.94 & 45011.73  1822.30 & 50337.02  4004.55 \\
 name this game & 54386.77  6148.50 & 25608.90  1943.41 & 74104.70  9053.70 & \bf{74501.48  11562.26} \\
 phoenix & 908264.15  28978.92 & \bf{966685.41  6127.24} & 937874.90  22525.79 & 876045.70  25511.04 \\
 pitfall & \bf{18756.01  9783.91} & 15334.30  15106.90 & -0.45  0.50 & 0.00  0.00 \\
 pong & 20.67  0.47 & 19.85  0.31 & 20.95  0.01 & \bf{21.00  0.00} \\
 private eye & 79716.46  29515.48 & \bf{100314.44  291.22} & 34601.01  5266.39 & 18765.05  16672.27 \\
 qbert & 580328.14  151251.66 & 479024.20  98094.39 & 434753.72  99793.58 & \bf{771069.21  152722.56} \\
 riverraid & \bf{63318.67  5659.55} & 40770.82  748.42 & 43174.10  2335.12 & 54280.32  1245.60 \\
 road runner & 243025.80  79555.98 & 151326.54  77209.43 & 116149.17  18257.21 & \bf{613659.42  397.72} \\
 robotank & 127.32  12.50 & 11.62  0.67 & \bf{143.59  0.29} & 130.72  9.75 \\
 seaquest & 999997.63  1.42 & \bf{999999.00  0.00} & 999999.00  0.00 & 999999.00  0.00 \\
 skiing & \bf{-4202.60  607.85} & -24271.33  6936.26 & -14576.05  875.96 & -17797.59  866.55 \\
 solaris & \bf{44199.93  8055.50} & 7254.03  3653.55 & 6566.03  2209.91 & 11247.88  1999.22 \\
 space invaders & 48680.86  5894.01 & 48087.13  11219.39 & 36069.75  23408.12 & \bf{67229.37  2316.31} \\
 star gunner & 839573.53  67132.17 & 450096.08  158979.59 & 420337.48  8309.08 & \bf{923739.89  69234.32} \\
 surround & 9.50  0.19 & -9.32  0.67 & 9.96  0.01 & \bf{10.00  0.00} \\
 tennis & 23.84  0.10 & 11.06  6.10 & \bf{24.00  0.00} & 7.93  11.36 \\
 time pilot & 405425.31  17044.45 & 368520.34  70829.26 & 452966.67  5300.62 & \bf{454055.63  2205.07} \\
 tutankham & \bf{2354.91  3421.43} & 197.90  7.47 & 466.59  38.40 & 413.80  3.89 \\
 up n down & 623805.73  23493.75 & 630463.10  31175.20 & \bf{679303.61  4852.85} & 599134.12  3394.48 \\
 venture & \bf{2623.71  442.13} & 1747.32  101.40 & 2013.31  11.24 & 2047.51  20.83 \\
 video pinball & 992340.74  12867.87 & 973898.32  20593.14 & 964670.12  4015.52 & \bf{999697.05  53.37} \\
 wizard of wor & 157306.41  16000.00 & 121791.35  27909.14 & 134017.82  11871.88 & \bf{179376.15  6659.14} \\
 yars revenge & 998532.37  375.82 & 997642.09  455.73 & 998474.20  589.50 & \bf{999748.54  46.19} \\
 zaxxon & 249808.90  58261.59 & 129330.99  56872.31 & 114990.68  56726.18 & \bf{366028.59  49366.03} \\
\hline
\end{tabular}
\normalsize
\clearpage
\subsection{Atari 57 Learning Curves}
\label{app:scores}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/Agent57_scores.png}
    \caption{Learning curves for Agent57 on Atari57.} 
    \label{fig:atari57learningcurves}
\end{figure}

\newpage
\subsection{Videos}
\label{app:videos}

We provide several videos in \url{https://sites.google.com/corp/view/agent57}. We show 

\begin{itemize}

\item {\bf Agent57 on all 57 games:} We provide an example video for each game in the Atari 57 sweep in which Agent57 surpasses the human baseline.

\item {\bf State-action Value Function Parameterization:} To illustrate the importance of the value function parametrization we show videos in two games \textit{Ice Hockey} and \textit{Surround}. We show videos for exploitative and exploratory policies for both NGU and Agent57. In \textit{Ice Hockey}, exploratory and exploitative policies are quite achieving very different scores. Specifically the exploratory policy does not aim to score goals, it prefers to move around the court exploring new configurations. On the other hand, NGU with a single architecture is unable to learn both policies simultaneously, while Agent57 show very diverse performance. In the case of \textit{Surround} NGU is again unable to learn. We conjecture that the exploratory policy chooses to loose a point in order to start afresh increasing the diversity of the observations. Agent57 is able to overcome this problem and both exploitative and exploratory policies are able to obtain scores surpassing the human baseline.

\item {\bf Adaptive Discount Factor:} We show example videos for R2D2 (bandit) and R2D2 (retrace) in the game \textit{James Bond}. R2D2 (retrace) learns to clear the game with a final score in the order of 30,000 points. R2D2 (bandit) in contrast, learns to delay the end of the game to collect significantly more rewards with a score around 140,000 points. To achieve this, the adaptive mechanism in the meta-controller, selects policies with very high discount factors.

\item {\bf Backprop Through Time Window Size:} We provide videos showing example episodes for NGU and Agent57 on the game of \textit{Solaris}. In order to achieve high scores, the agent needs to learn to move around the grid screen and look for enemies. This is a long term credit assignment problem as the agent needs to bind the actions taken on the grid screen with the reward achieved many time steps later.

\end{itemize}
 
\end{document}
