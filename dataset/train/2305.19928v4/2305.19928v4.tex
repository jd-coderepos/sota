\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\pdfoutput=1
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[backref]{hyperref}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{lipsum}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{caption}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{CJKutf8}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\begin{document}

\title{Supplementary Features of BiLSTM for Enhanced Sequence Labeling}
\author{Conglei Xu, Kun Shen, Hongguang Sun
\thanks{ Conglei Xu, Department of Computer Science, Aalborg University, Aalborg East, 9220, Denmark (email: cxu@cs.aau.dk)} \thanks{Kun Shen,  Department of Electronic information Engineering, Rizhao Polytechnic, Rizhao, 276800, China (email: shenkun5410@rzpt.edu.cn)}
\thanks{Hongguang Sun,  School of Information Science and Technology, Northeast Normal University, Changchun, 130117, China (email: sunhg889@nenu.edu.cn)
}
\thanks{Conglei Xu is the corresponding author}
\thanks{Our code is available at: \url{https://github.com/conglei2XU/Global-Context-Mechanism}}
}





\maketitle

\begin{abstract}
Sequence labeling tasks require the computation of sentence representations for each word within a given sentence. A prevalent method incorporates a Bi-directional Long Short-Term Memory (BiLSTM) layer to enhance the sequence structure information. However, empirical evidence Li (2020) suggests that BiLSTM’s capacity to produce sentence representations for sequence labeling tasks is inherently limited \cite{Li2020}. This limitation primarily results from the integration of fragments from past and future sentence representations to formulate a complete sentence representation. In this study, we observed that the entire sentence representation, found in both the first and last cells of BiLSTM, can supplement each cell’s individual sentence representation. Accordingly, we devised a global context mechanism to integrate entire future and past sentence representations into each cell's sentence representation within the BiLSTM framework. By incorporating the BERT model within BiLSTM as a demonstration, and conducting exhaustive experiments on nine datasets for sequence labeling tasks—including named entity recognition (NER), part of speech (POS) tagging, and End-to-End Aspect-Based sentiment analysis (E2E-ABSA)—we noted significant improvements in F1 scores and accuracy across all examined datasets.
\end{abstract}

\begin{IEEEkeywords}
BiLSTM, BERT, global context, sequence labeling.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{B}{iLSTM} \cite{schuster1997bidirectional} has emerged as a widely embraced neural network for modeling structural information in inputs for sequence labeling tasks, such as NER \cite{Ghaddar2018}, \cite{Ma2016}, and POS tagging \cite{Plank2016}, since its initial application in speech recognition tasks. Recently, with the development of pretrained language models, BiLSTM has been employed as an auxiliary layer to augment sentence representations for sequence labeling \cite{Chen2020}, \cite{Xu2021}. Furthermore, due to its ability to capture sequential information in inputs, BiLSTM is also a popular architecture in other domains. Li (2019)  demonstrates the efficacy of combining BERT with BiLSTM \cite{Li20192}, which involves the concurrent identifications of aspect terms/categories and their corresponding sentiments within a sequence tagging framework. Other studies have shown that BiLSTM could effectively improve the accuracy of heart rate prediction \cite{lin2023new} and achieve competitive prediction results on long-term traffic flow forecasting \cite{mendez2023long}.

The issue of shallow connections between consecutive hidden states of BiLSTM has been recognized for some time. Pascanu (2013) proposed the deep transition RNN for language modeling \cite{pascanu2013construct}, which increases the transition depth between consecutive hidden states for richer representations. Meng and Zhang (2019) capitalized on a deep transitional architecture for machine translation \cite{Meng2019}, while Liu (2019) achieved state-of-the-art results on sequence labeling with a similar architecture \cite{Liu2019}. More recently,  Li (2020) examined the lack of global sentence information in inner cells of BiLSTM in the context of NER \cite{Li2020}, concluding that this specific limitation could not be resolved by merely increasing the transitional depth of consecutive hidden states. However, previous methods have primarily focused on enhancing transition depth of BiLSTM and modifying its inner structure, which could potentially impede the inference speed and are difficulty to use in the real-world applications.

In this paper, we propos a straightforward global mechanism designed to integrate global sentence information into the sentence representation of each cell. This mechanism could be conveniently incorporated into the BiLSTM framework. Specifically, we discovered that the entire past (forward) sentence representation, located in the last cell, can serve as supplementary features for each future (backward) sentence representation. Conversely, the entire future sentence representation situated in the first cell exhibits the same supplementary feature in reverse. Furthermore, this global mechanism could also be employed independently for certain tasks, separate from BiLSTM.

We assessed this global mechanism on nine datasets from sequence labeling tasks, including E2E-ABSA, POS tagging, NER. Using BERT with BiLSTM as an example, it bolsters the model’s performance on these tasks without significantly impacting the speed of inference and training. Further experiments were conducted to probe the global mechanism’s ability to decipher the relationships between tags. By directly adding it after BERT, improvements in F1 and accuracy scores were noted. 


\section{Related Work}

Pretrained language models: pretrained language models such as BERT \cite{Devlin2018}, XLNET \cite{Yang2019} have delivered state-of-the-art results across a myriad of tasks. An increasing body of literature has introduced  BiLSTM as an additional layer for pretrained language modles to enrich sentence representation in sequence labeling tasks. For instance, Jie and Lu (2019), Sarzynska-Wamer (2021) combined BiLSTM-CRF with ELMO \cite{sarzynska2021detecting}, \cite{jie2019}, achieving superior performance in NER tasks. Similarly, Xu (2021) applied an advanced BiLSTM with BERT \cite{Xu2021}, setting a new standard for NER tasks. In the realm of POS tagging tasks, Labrak and Dufour (2022) reached unprecedented level of success POS tagging tasks using flair embeddings \cite{Labrak2022} and BiLSTM \cite{Akbik2019}; Additionally, X. Li (2019) employed BERT with BiLSTM to substantially augment E2E-ABSA \cite{Li20192}. 

Deficiency of BiLSTM: Regarding the shallow representations of BiLSTM, Liu (2019) enhanced sentence representation for sequence labeling tasks with a deep RNN transitional architecture \cite{Liu2019}. Meanwhile Meng and Zhang (2019)  leveraged a Linear transformation enhanced GRU model to significantly improve the BLEU score in machine translation \cite{Meng2019}. Further, Li (2020) utilized a simple self-attentive mechanism on BiLSTM outputs to solve the lack of complete sentence information in inner cells \cite{Li2020}.

Gate mechanism: Concerning the gate mechanism, a concept well-established in LSTM \cite{Hochreiter1997}, several researchers have utilized it to fuse past contextual and current information. Specifically,  H. Chen (2019) and Zeng, (2019) improved sentence representation of CNN on natural language tasks with a gate mechanism \cite{Chen2019} \cite{Zeng2019} . Moreover, Yuan (2020) and X. Zeng (2016) used it to extract features from different support regions on object detection tasks \cite{Yuan2020}, \cite{Zeng2016}.
\begin{figure*}[t]
\centering
\captionsetup{justification=centering}

\includegraphics [width= 0.6 \textwidth, height=10cm] {overview}
\caption{Overview of the model architecture.}
\label{fig_1}
\end{figure*}


\section{Model}
Our global context mechanism experiments build upon a model that includes a BERT embedding layer with a downstream BiLSTM layer. We implement the global context mechanism following the BiLSTM layer and compare it with the baseline: self-attention network \cite{Vaswani2017}. An overview of the model is illustrated in Figure \ref{fig_1}.

\subsection{BERT Embedding Layer}
BERT embedding offers dynamic token representations based on input sentences, a noteworthy advancement over traditional word embedding methods GLOVE \cite{Pennington2014} and Word2Vector \cite{Mikolov2013} that generates a static matrix, Specifically, given a sequence of input $S=\{w_1,w_2,…,w_n\}$, n denotes the length of the input sentence. BERT provided its contextualized representation $Z=\{z_1,z_2,…,z_n\}$ as follows:
\begin{equation}
Z = BERT(S)
\end{equation}

\subsection{BiLSTM Layer}
BiLSTM has been a very powerful structure for sequence labeling tasks, owning to its ability to model sentence structure and keep dependencies in long sentences. In this researcher, we utilize BiLSTM to enhance sentence representation for each word. Using the time step t as an example, BiLSTM generates the sentence representation $H_t$ based on $Z=\{z_1,z_2,…,z_n\}$:

\begin{equation}
\overrightarrow{H_t} = \overrightarrow{LSTM_t}(\overrightarrow{H_{t-1}},z_t)
\end{equation}

\begin{equation}
\overleftarrow{H_t} = \overleftarrow{LSTM_t}(\overleftarrow{H_{t+1}},z_t)
\end{equation}

\begin{equation}
H_t = \overrightarrow{H_t}\parallel\overleftarrow{H_t}
\end{equation}

\subsection{Global Context Mechanism}
In light of the fact that the entire sentence information is confined to the first and last cells, we amalgamate it with the entire sentence representation $G= \overleftarrow{H_1} \parallel \overrightarrow{H_n}$using weights $i_H$ and $i_G$. Figure depicts the structure of the global context mechanism.
Given the BiLSTM outputs $H=\{H_i,H_2,…,H_n\} $ $H \ni R^{n×d}$; for the $t_th$ step, we derive $O_t  = G || H_t$, for gate mechanism to generate $i_H^t$ and $i_G^t$.

\begin{figure*}[]
\centering
\captionsetup{justification=centering}

\includegraphics [width= \textwidth, height=10cm] {context_mechanism}
\caption{Overview of the model architecture.}
\label{fig_2}
\end{figure*}

\begin{table*}[t]
\centering

\resizebox{\textwidth}{15mm}{\begin{tabular}{|l|ll|ll|ll|ll|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Models}} & \multicolumn{2}{c|}{Laptop14}                       & \multicolumn{2}{c|}{Rest14}                         & \multicolumn{2}{c|}{Rest15}                         & \multicolumn{2}{c|}{Rest16}                         \\ \cline{2-9} 
\multicolumn{1}{|c|}{}                        & \multicolumn{1}{c}{F1} & \multicolumn{1}{c|}{Speed} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c|}{Speed} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c|}{Speed} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c|}{Speed} \\ \hline
BERT                                          & 58.49                  & 12.87                      & 69.75                  & 13.61                      & 57.07                  & 15.37                      & 65.95                  & 15.81                      \\
BERT-BiLSTM                                   & 61.12                  & 12.46                      & 73.47                  & 13.17                      & 61.14                  & 14.79                      & 71.05                  & 14.66                      \\
BERT-BiLSTM-context                           & \textbf{62.92}         & 11.65                      & \textbf{73.84}         & 12.97                      & \textbf{63.24}         & 14.51                      & \textbf{71.51}         & 13.74                      \\
BERT-BiLSTM-attention                         & 59.48                  & 11.98                      & 72.73                  & 12.68                      & 60.34                  & 13.99                      & 69.05                  & 13.44                      \\ \hline
\end{tabular}}
\caption{Result on E2E-ABSA. Unit of speed is the number of iterations per second.\label{tab:table1}}
\end{table*}

\begin{table*}[]
\centering
\resizebox{0.8\textwidth}{!}{\begin{tabular}{|l|ll|ll|ll|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Models}} &
  \multicolumn{2}{c|}{Conll2003} &
  \multicolumn{2}{c|}{Wnut2017} &
  \multicolumn{2}{c|}{Weibo} \\ \cline{2-7} 
\multicolumn{1}{|c|}{} &
  \multicolumn{1}{c}{F1} &
  \multicolumn{1}{c|}{Speed} &
  \multicolumn{1}{c}{F1} &
  \multicolumn{1}{c|}{Speed} &
  \multicolumn{1}{c}{F1} &
  \multicolumn{1}{c|}{Speed} \\ \hline
BERT                  & 91.51          & 16.30 & 43.59          & 15.53 & 68.09          & 11.46 \\
BERT-BiLSTM           & 91.85          & 15.12 & 46.95          & 14.59 & 68.86          & 10.44 \\
BERT-BiLSTM-context   & \textbf{91.91} & 14.80 & \textbf{48.02} & 14.03 & \textbf{69.84} & 10.15 \\
BERT-BiLSTM-attention & 91.19          & 14.66 & 46.39          & 13.77 & 67.83          & 10.18 \\ \hline
\end{tabular}}
\caption{Result on NER. \label{tab:table2}}
\end{table*}
In the gate mechanism, a linear map is employed to select pertinent features from $O_t$ firstly.

\begin{equation}
R_H = W_HO_t + b_H
\end{equation}
\begin{equation}
R_G = W_GO_t + b_G
\end{equation}
Where $W_H$ and $W_G \in R^{2d×d}$; $R_G^t$, $R_G^t$ is for global information $G$ and current sentence representation $H_i$ respectively. 
And then weights $i_H^t$ and $i_G^t$ are given by a sigmoid function.
\begin{equation}
i_H^t = sigmoid(R_H^t)
\end{equation}
\begin{equation}
i_G^t = sigmoid(R_G^t)
\end{equation}
At last, $G$ and $H_t$ are fused by $i_H^t$ and $i_G^t$.
\begin{equation}
\hat{O_t} = i_H^t \odot H_t \parallel i_G^t \odot G
\end{equation}
Where $\odot$ denotes element wise product.

The prediction results are given by:
\begin{equation}
\tilde{O_t} = softmax(W_c\hat{O_t}+b_c)
\end{equation}

$W_c \in R^{d×u}$, u is the number of classes.
\subsection{Self-Attention Network}


\begin{table*}[t]
\centering
\resizebox{\textwidth}{10mm}{\begin{tabular}{|l|llll|lll|ll|}
\hline
Models              & Rest14 & Rest15 & Rest16 & Laptop14 & Conll2003 & Wnut2017 & Weibo & Conll2003 & UD    \\ \hline
BERT-BiLSTM         & 73.47  & 61.44  & 71.06  & 61.12    & 91.85     & 46.95    & 68.86 & 95.66     & 96.90 \\
BERT-BiLSTM-context & 73.84  & 63.24  & 71.51  & 62.92    & 91.91     & 48.02    & 69.84 & 95.62     & 97.01 \\
BERT-BiLSTM-$\tilde{context}$ & 72.07 & 61.90 & 67.88 & 60.97 & 91.21 & 48.08 & 68.47 & 95.50 & 96.76 \\ \hline
\end{tabular}}
\caption{Comparison between the directions for fusing sentence representations.\label{tab:table3}}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{10mm}{\begin{tabular}{|l|llll|lll|ll|}
\hline
Models              & Rest14 & Rest15 & Rest16 & Laptop14 & Conll2003 & Wnut2017 & Weibo & Conll2003 & UD    \\ \hline
BERT-BiLSTM         & 73.47  & 61.44  & 71.06  & 61.12    & 91.85     & 46.95    & 68.86 & 95.66     & 96.90 \\
BERT-BiLSTM-context & 73.84  & 63.24  & 71.51  & 62.92    & 91.91     & 48.02    & 69.84 & 95.62     & 97.01 \\
BERT-BiLSTM-$\hat{context}$ & 72.52  & 59.20  & 69.43  & 59.20    & 91.24     & 46.92    & 69.46 & 95.53     & 96.92 \\ \hline
\end{tabular}}
\caption{Comparison between global context mechanism with and without weights mechanism.\label{tab:table4}}
\end{table*}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{13mm}{\begin{tabular}{|l|ll|ll|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Models}} & \multicolumn{2}{c|}{Conll2003}                      & \multicolumn{2}{c|}{UD}                             \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                        & \multicolumn{1}{c}{F1} & \multicolumn{1}{c|}{Speed} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c|}{Speed} \\ \hline
BiLSTM         & 94.21 & 55.38 & 93.65 & 46.24 \\
BiLSTM-CRF     & 94.67 & 21.99 & 94.29 & 17.41 \\
BiLSTM-context & 94.38 & 47.95 & 93.88 & 39.03 \\ \hline
\end{tabular}}
\caption{Comparison between CRF and global context mechanism on pure BiLSTM.\label{tab:table5}}
\end{table}

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|ll|ll|}
\hline
\multicolumn{1}{|c|}{Models} & \multicolumn{2}{c|}{Conll2003}      & \multicolumn{2}{c|}{UD}             \\ \hline
BERT                         & \multicolumn{2}{l|}{95.56}          & \multicolumn{2}{l|}{96.85}          \\
BERT-BiLSTM                  & \multicolumn{2}{l|}{95.66}          & \multicolumn{2}{l|}{95.90}          \\
BERT-BiLSTM-context          & \multicolumn{2}{l|}{95.62}          & \multicolumn{2}{l|}{97.01}          \\
BERT-BiLSTM-attention        & \multicolumn{2}{l|}{95.30}          & \multicolumn{2}{l|}{\textbf{97.09}} \\
BERT-context                 & \multicolumn{2}{l|}{\textbf{95.67}} & \multicolumn{2}{l|}{96.90}          \\ \hline
\end{tabular}}
\caption{Result on POS tagging.\label{tab:table6}}
\end{table}

Another method to capture the interaction between past and future contexts at each time step for BiLSTM is through a token-level self-attentive mechanism proved by Li  (2020) \cite{Li2020}. Given the BiLSTM outputs H of a sentence, the model maps each $H_i \in H$ to different subspaces, this depends on whether it is being used as a query vector to consult other hidden states. The final representation is crafted by fusing value vectors according to weights computed by incoming queries between key and query vectors. In this study, we employ a multi-head attention self-attention network \cite{Vaswani2017}.

Formally assuming the number of head is m, for the head i, attention weight matrix $\alpha^{i}$ and context matrix $C^{i}$ are computed as follows:
\begin{equation}
\alpha^{i} = softmax(\frac{HW^{qi}(HW^{ki})^{T}}{\sqrt{d_C}})
\end{equation}
Where $W^{qi}$, $W^{vi}$, $W^{ki} \in R^{d_h*d_c }$ are trainable projection matrices.

Taking time step $t$ as example, the context matrix $C_t = C_t^1 \parallel C_t^2 \parallel  \cdots \parallel C_t^n$ and BiLSTM output $H_t$ are considered together for classification.

\begin{equation}
\hat{O_t} = H_i + C_t
\end{equation}
\begin{equation}
\tilde{O_t} = softmax(W_c \hat{O_t})
\end{equation}
\section{Experiments}
This section presents the results of global context mechanism applied on nine datasets from E2E-ABSA, NER, and POS tasks. We selected the best models based on the results from development dataset, using an early stopping setting. The BERT-base model, as implemented in the HuggingFace package, was used in this study. Detailed information about the learning rate can be found in the appendix. All experiments were conducted on a server equipped with an Nvidia A10 GPU.
\subsection{End-to-End Aspect-Based sentiment analysis}
E2E-ABSA aims to detect aspect terms and their corresponding sentiments jointly. The possible tag values include $B-\{POS,NEG,NEU\}$, 
$I-\{POS,NEG,NEU\}$, $E-\{POS,NEG,NEU\}$, $S-\{POS,NEG,NEU\}$ or $O$. These tags denote the beginning of an aspect, inside of an aspect, end of an aspect, single-word aspect, with positive, negative, or neutral sentiment respectively, as well as outside of aspect. 

Experiments are conducted on two review datasets originating from SemEval \cite{Pontiki2014}, \cite{Pontiki2015}, \cite{Pontiki2016}  re-prepared by Li and Bing (2019) \cite{Li2019} as a sequence labeling task. For Laptop14 and Resturant15 (Rest15), batch size of 16 is employed; for Restaurant16 and Restaurant14, a batch size of 32 is utilized AdamW is used for gradient update for all these four datasets.

As shown in Table \ref{tab:table1}. This mechanism attains $0.46\%$, $2.1\%$, $0.37\%$, $1.8\%$ absolute f1 improvements on Restaurant14, Restaurant15, Restaurant16 and Laptop14 respectively, while requiring minimal computing resources. The result suggests that fusing information by self-attention network does not benefit BiLSTM after BERT on E2E-ABSA.

\subsection{Named Entity Recognition} 
NER aims to predict entity type of each token, which could include Person, Organization, Location etc. In this section, we utilize two English Datasets: Conll2003 \cite{Sang2003}, Wnut2017 \cite{Derczynski2017} and a Chinese dataset, Weibo \cite{Peng2015}. All three datasets employ a batch size of 16 and the AdamW optimizer.


As shown in Table \ref{tab:table2}, the global context mechanism can increase F1 scores across all three datasets, whereas the self-attention method does not provide an F1 improvement.

\subsection{Part-of-speech Tagging}
Part-of-speech tagging involves marking a word with its part of speech, which can include noun, verb, adjective, adverb, etc., in English.

We design experiments for POS tagging on Universal Dependencies (UD) v2.11 \cite{Silveira2014} and Conll2003 using batch size of 16 and AdamW optimizer.

Table \ref{tab:table6} indicates that the global context mechanism also increases accuracy in POS tagging. When we directly add global context mechanism after BERT, the results reveal that global context mechanism could achieve same accuracy as BiLSTM with higher speed on Conll2003 POS tagging. 

To further investigate this, we compare the conditional random field (CRF) for comparison with POS tagging on pure BiLSTM. Table \ref{tab:table5} displays that the global context mechanism leads to accuracy improvements and is much faster than the CRF. This suggests that the global context mechanism can serve as a substitute for CRF when a trade-off between accuracy and speed is required.

 \begin{table*}[]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|l|llll|lll|ll|}
\hline
Models       & Rest14 & Rest15 & Rest16 & Laptop14 & Conll2003 & Wnut2017 & Weibo & Conll2003 & UD    \\ \hline
BERT         & 69.75  & 57.07  & 65.95  & 58.49    & 91.51     & 43.59    & 68.09 & 95.56     & 96.85 \\
BERT-context & 69.99  & 57.17  & 68.17  & 60.42    & 91.72     & 45.16    & 67.31 & 95.67     & 96.90 \\ \hline
\end{tabular}}
\caption{Experiments on adding global context mechanism directly. \label{tab:table7}}
\end{table*}



\section{Ablation Study}
An ablation study was performed to assess the effects of the weights used for fusing sentence representations and the direction of sentence representations.
\subsection{The directions of sentence representations}

We denote the combination of forward and backward sentence representation with their respective global sentence representation as $\tilde{context}$. As demonstrated in Table \ref{tab:table3}, for these three types of sequence labeling tasks, we observe that fusing sentence representations in same direction only proves effective for Wnut2017 and Returant15. From this, we infer the that the positional information for one direction may override each other when sentence representations are fused.
\subsection{Weights}
To evaluate the effectiveness of fusing sentence representations using weights, we introduced a comparison where sentence representations are added directly denoted as $\hat{context}$.


Table \ref{tab:table4} shows that F1 scores of adding sentence representations directly are inferior to those of BERT-BiLSTM. This suggests that the weight mechanism plays a crucial role in fusing sentence representations.

\subsection{Without BiLSTM}
As mentioned in the POS tagging experiments section, the global context mechanism can capture the relationship of tags for POS tagging. We conduct further experiments on this for other tasks by adding context mechanism directly after BERT. The results, presented in Table \ref{tab:table7}, indicates that the global context mechanism improves F1 and accuracy for most tasks and it is significantly faster than BERT with BiLSTM. 

\begin{CJK*}{UTF8}{gbsn}

\begin{table*}[]
\centering
\resizebox{0.6\textwidth}{4 cm}{\begin{tabular}{@{}llll@{}}
\toprule
Sentece & Gold Standard & context   & w/o context                       \\ \midrule
分       & O             & O         & O                                 \\
手       & O             & O         & O                                 \\
大       & O             & O         & \cellcolor[HTML]{FFFFC7}B-PER.NOM \\
师       & O             & O         & \cellcolor[HTML]{FFFFC7}I-PER.NOM \\
贵       & O             & O         & \cellcolor[HTML]{FFFFC7}B-PER.NOM \\
仔       & O             & O         & O                                 \\
邓       & B-PER.NAM     & B-PER.NAM & B-PER.NAM                         \\
超       & I-PER.NAM     & I-PER.NAM & I-PER.NAM                         \\
四       & O             & O         & O                                 \\
大       & O             & O         & O                                 \\
名       & O             & O         & O                                 \\
捕       & O             & O         & \cellcolor[HTML]{FFFFC7}I-PER.NOM \\ \bottomrule
\end{tabular}}
\caption{Weibo case analysis. The errors are in yellow.\label{tab:table8}}
\end{table*}
\end{CJK*}

\begin{figure*}[t]
\centering
\captionsetup{justification=centering}

\includegraphics [width= \textwidth] {scatter}
\caption{Visualization for weights. The weights for context and BiLSTM are in green and red respectively.}
\label{fig_4}
\end{figure*}

\subsection{Case studies}
Chinese language is characterized by a significant degree of polysemy, where the interpretation of each character heavily depends on the context. Analyses were performed on the predictive outputs of model both with and without context mechanism. The results suggest that the global context mechanism significantly assist model in understanding the polysemy inherent in Chinese characters. A representative example from the test dataset is provided in Table \ref{tab:table8}, with errors highlighted yellow. The Chinese characters \begin{CJK*}{UTF8}{gbsn}‘大’ and ’师’ \end{CJK*}can either refer to an individual or signify a title, depending on the context. Without the global context mechanism, the model struggles to discern whether these characters refer to a person or denote a title. However, with the aid of the global context mechanism, the model correctly assigns the relevant types for \begin{CJK*}{UTF8}{gbsn}‘大师’ \end{CJK*}. 

\subsection{Visualization}
The weights $i_H$ and $i_G$, corresponding to BiLSTM outputs and global context information respectively, are segregated into six divisions at intervals 100, followed by the construction of scatter graphs. Upon examination, we find that a significant, portion of positions in $i_H$ possess weights reaching one, whereas $i_G$ contains a higher number of positions with smaller weights. Additionally, about quarter of positions in $i_H$ and $i_G$ showcases similar values. We have chosen the Chinese character  \begin{CJK*}{UTF8}{gbsn}‘大’ \end{CJK*} from Table \ref{tab:table8} to visualize, as depicted in Figure \ref{fig_4}.



\section{Conclusion}
In this research, we discovered that the entirety future and past sentence representations could be supplementary for past and future sentence representations of each cell respectively in BiLSTM. Based on this, we introduced a straightforward global context mechanism designed to compensate for the absence of complete sentence information in the intermediate cells of BiLSTM. This methodology can be conveniently incorporated with BiLSTM in practical applications. Empirical evaluations are conducted on three distinct tasks–named entity recognition (NER), part of speech (POS) tagging and End-to-End Aspect-Based sentiment analysis (E2E-ABSA)–illustrate marked enhancements in both F1 score and accuracy across all these tasks, while maintaining the speed of training and inference. Moreover, the experimental results suggest that the global context mechanism effectively captures the relational information between tags. 



\bibliographystyle{IEEEtran}
\bibliography{export.bib}
\newpage

\section{Appendix}

\subsection{Learning Rate} 
Learning rates details of different layers are shown in Table \ref{tab:table9}.
\footnote{Dropout on global context G is cancelled.} 
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|llll|lll|ll|}
\hline
Layers         & Rest14 & Rest15 & Rest16 & Laoptop14 & Conll2003\footnote[1] & Wnut2017 & Weibo\footnote[1] & Conll2003 & UD   \\ \hline
BERT           & 1E-5   & 1E-5   & 1E-5   & 1E-5      & 1E-5      & 1E-5     & 1E-5  & 1E-5      & 1E-5 \\
BiLSTM         & 5E-4   & 1E-3   & 5E-4   & 5E-4      & 1E-3      & 1E-3     & 1E-3  & 1E-3      & 1E-3 \\
context        & 1E-3   & 1E-3   & 1E-5   & 1E-5      & 1E-3      & 1E-3     & 1E-3  & 1E-4      & 1E-3 \\
Classification & 1E-4   & 1E-4   & 1E-4   & 1E-4      & 1E-4      & 1E-4     & 1E-4  & 1E-4      & 1E-4 \\ \hline
\end{tabular}}
\caption{Learning rates of different layers on tasks. \label{tab:table9}}
\end{table}



\end{document}
