\pdfoutput=1
\documentclass[twocolumn,preprintnumbers,amsmath,amssymb,letter]{revtex4}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\newcommand{\vpi}{\mathbf{\pi}}
\newcommand{\rar}{\rightarrow}
\newcommand{\ra}{\rangle}
\newcommand{\la}{\langle}
\newcommand{\ttt}{\texttt}
\newcommand{\mb}{\mathbb}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newcommand{\qed}{\hfill  \hfill \\}

\begin{document}

\preprint{LAUR-07-5287}

\title{Mapping Semantic Networks to Undirected Networks}

\author{Marko A. Rodriguez \\
		T-7, Center for Non-Linear Studies \\
		Los Alamos National Laboratory \\
		Los Alamos, New Mexico 87545}
		

 \begin{abstract}
There exists an injective, information-preserving function that maps a semantic network (i.e~a directed labeled network) to a directed network (i.e.~a directed unlabeled network). The edge label in the semantic network is represented as a topological feature of the directed network. Also, there exists an injective function that maps a directed network to an undirected network (i.e.~an undirected unlabeled network). The edge directionality in the directed network is represented as a topological feature of the undirected network. Through function composition, there exists an injective function that maps a semantic network to an undirected network. Thus, aside from space constraints, the semantic network construct does not have any modeling functionality that is not possible with either a directed or undirected network representation. Two proofs of this idea will be presented. The first is a proof of the aforementioned function composition concept. The second is a simpler proof involving an undirected binary encoding of a semantic network.
\end{abstract}

\maketitle{}

\section{Introduction}

A network is a popular data structure for representing the relationship between discrete elements \cite{netanal:brandes2005,netsci:newman2006}. There are various types of networks such as the undirected network (i.e.~undirected unlabeled network), the directed network (i.e.~directed unlabeled network), and the semantic network (i.e.~directed labeled network). In an undirected network, there exists no order to the relationships between the vertices. An undirected network can be denoted , where  is the vertex set and any edge  denotes an undirected relationship. The directed network provides the concept of edge directionality. A directed network can be represented as , where  is the vertex set and any edge  denotes a directed relationship. All edges in both an undirected and directed network are homogeneous in meaning. In order to represent edge meaning, a semantic network can be used. In a semantic network, an edge connecting any two vertices maintains a label (e.g.~character string) that denotes the type of relationship between two vertices. A semantic network can be represented as , where  is the vertex set,  is the set of edge labels, and any edge (called a triple)  denotes an ordered, labeled relationship.

The semantic network is perhaps best known as a modeling construct from the early days of knowledge representation in the cognitive sciences \cite{sowa:semantic1991}. However, with the inception of the Semantic Web initiative \cite{lee:semantic2001,pubsem:lee2001} and with the development of triple-store technology (i.e.~semantic network databases) \cite{lee:triple2004,oracle:alexander2006,agraph:aasman2006}, there has been an increase in the use of the semantic network as a data structure for modeling data sets where there exists a heterogeneous set of vertices and edges. This trend has been occurring across various disparate domains such as bioinformatics \cite{sembio:quan2003,sembio:ruttenberg2007}, digital libraries \cite{lib:bax2004,semever:bollen2007}, and general computer-science \cite{rodriguez:gpsemnet2007}. Because of the use of the labeled edge, the semantic network is seen as the better modeling construct  than both the undirected and directed network for such data sets.

However, when ignoring space constraints, there is no modeling gain by using a semantic network representation as opposed to a directed network representation. Moreover, there is no modeling gain over using an undirected network representation. Through a series of information-preserving, injective mappings \footnote{An injective function is one such that if , then .}, this article demonstrates that it is possible to model a semantic network both as a directed and undirected network. While the directed and undirected models of a semantic network utilize more vertices and edges in their representation, they ultimately have the ability to capture the same information.

The outline of this article is as follows. Section \ref{sec:sem-to-dir} presents an injective function to map a semantic network to a directed network. Section \ref{sec:dir-to-und} presents an injective function to map a directed network to an undirected network. Finally, through function composition, Section \ref{sec:sem-to-und} presents an injective function to map a semantic network to an undirected network.

\section{Mapping a Semantic Network to a Directed Network\label{sec:sem-to-dir}}

This section will present an injective, information-preserving function that maps a semantic network to a directed network. There is a two step process to this function. First, the edge labels of a semantic network are represented as a binary string. Second, each binary string is represented as a unique directed network encoding. Given that a directed network can only represent vertices and directed edges, each edge label of the semantic network is encoded as a topological feature in the directed network.

Let  denote a semantic network where  is the set of all vertices and  is the set of all edge labels. Any triple  represents a directed edge from vertex  to vertex  with a label of .  An example semantic network triple is diagrammed in Figure \ref{fig:semantic-example}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.175\textwidth]{images/semantic-example}
	 \caption{\label{fig:semantic-example}An edge in a semantic network.}
\end{figure}

There exists the injective function  (a binary encoder) that represents every label in  as a unique binary string of length . While the minimum bits required to make a one-to-one mapping is , popular examples of other such one-to-one mappings include the ASCII and Unicode functions that map between human language characters and binary strings. Furthermore, there exist the inverse function  that maps a binary string to its original symbolic representation. Note that for labels already represented as unique binary strings,  and  are identity functions. Given the semantic network edge diagrammed in Figure \ref{fig:semantic-example}, the  mapping is represented in Figure \ref{fig:binary-transform}. Assume that  and thus, each  requires  bits to encode it.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.175\textwidth]{images/binary-transform}
	 \caption{\label{fig:binary-transform}A example of the  mapping.}
\end{figure}

Next, there exists the injective function  (a directed network encoder), where  is the family of all directed networks and any  is denoted . If  is the ordered multi-set (or bag) of the -bit string , then

If , then  is represented as diagrammed in Figure \ref{fig:label-transform}. The number of vertices in  with respects to  is . The number of directed edges in  with respects to  is . 
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.225\textwidth]{images/label-transform}
	 \caption{\label{fig:label-transform}A directed network representation of the edge label .}
\end{figure}

The function  is information preserving because there also exists the inverse function . If  is the single non-looping path in  that traverses every vertex in  (i.e.~the only Hamiltonian path), then

Thus, . From a set of functions that transform a symbolic edge label to a directed network encoding, it is possible to represent an entire semantic network as a a single directed network. In other words, given , .

\begin{proposition}[Semantic-to-Directed Injection]
A semantic network can be modeled as a directed network without loss of information. There exists an injective function , where  is a directed network representation of some .
\end{proposition}
\emph{Proof.} If  denotes an injective function that maps a semantic network to a directed network, then

where any  is a vertex in  and . With respects to the previous example figures, the  mapping is diagrammed in Figure \ref{fig:semantic-nonsemantic}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\textwidth]{images/semantic-nonsemantic}
	 \caption{\label{fig:semantic-nonsemantic}A -encoding of .}
\end{figure}

Let  denote the directed network . In , every vertex that does not self-loop and has an even degree was originally a vertex in . All other vertices in  are used to denote the edge labels of . The growth of the number of vertices in  with respects to  is . The growth of the number of edges in  with respects to  is .

In order to demonstrate the information-preserving quality of , the inverse function  also exists. Let  denote the degree of a vertex and let  be the set of paths from vertex  to vertex  in  such that

where  (i.e.~ and 's degree is even),  (i.e.~no self-loops), , ,  (i.e.~only  and  can be the same vertex), and no  is in a cycle with another  in the sequence. If

then

where  and  and thus, the original vertices in .

Given  and , a unique, one-to-one mapping between a semantic network and a directed network exists such that a semantic network can be modeled as a directed network without loss of information. \qed

There exists another proof of this concept. As demonstrated earlier, a binary string of arbitrary length can be represented as a single chain (i.e.~sequence, path) of vertices, where each vertex represents a bit. In this representation, a self-loop represents a bit with value  and no self-loop represents a bit with value . Because any representation of a semantic network, at the lowest level of computing, is ultimately represented as a sequence of bits, a directed network can  be used to model that sequence.

\section{Mapping a Directed Network to an Undirected Network\label{sec:dir-to-und}}

This section presents the injective, information-preserving function  that maps a directed network to an undirected network. A directed network is identified by a set of ordered vertex pairs. For instance, when ,  denotes a directed edge going from  (the source) to  (the sink). A directed edge between  and  is diagrammed in Figure \ref{fig:directed-example}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.175\textwidth]{images/directed-example}
	 \caption{\label{fig:directed-example}An edge in a directed network.}
\end{figure}

An undirected network denoted  does not represent edge directionality as elements of  are unordered thus,  states that  and  are connected, but that no particular direction exists. If a directed network is to be represented as an undirected network, then a topological feature in the undirected form must be used to represent edge directionality.

\begin{proposition}[Directed-to-Undirected Injection]
A directed network can be modeled as an undirected network without loss of information. There exists an injective function , where  is an undirected network representation of some .
\end{proposition}
\emph{Proof.} The function  maps each ordered vertex pair in  to a set of unique unordered vertex pairs in . If , then

where the vertices , , and  are unique for each . Any vertex with an undirected self-loop in  is an original vertex from . The vertices  and their respective edges represent the direction of the edge. The vertex  has one edge which denotes the tail of the original directed edge. The vertex  has two edges which denotes the head of the original directed edge.  incurs a vertex growth of  and an edge growth of . The  mapping of the directed edge represented in Figure \ref{fig:directed-example} is diagrammed in Figure \ref{fig:direction-transform}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.25\textwidth]{images/direction-transform}
	 \caption{\label{fig:direction-transform}An undirected network representation of a directed edge.}
\end{figure}

The function  is information preserving because there exists the inverse function  such that if  is defined as

then

Thus, a directed network can be modeled as an undirected network. \qed

\section{Mapping a Semantic Network to an Undirected Network\label{sec:sem-to-und}}

This section presents the unification of the concepts presented in the two previous sections. In this section, by means of function composition, it is demonstrated that a semantic network can be modeled as an undirected network without loss of information. This means that there exists a one-to-one mapping between a semantic network and some undirected network. In short, given the functions  and  presented previously, an undirected network has the same representative or modeling power as a semantic network.

\begin{proposition}[Semantic-to-Undirected Injection]
A semantic network can be modeled as an undirected network without loss of information. There exists an injective function , where  is an undirected network representation of some .
\end{proposition}
\emph{Proof.} Recall the injective functions  and . Through function composition, there exists the function  with the rule

 incurs a vertex growth of 

and an edge growth of



Finally, there also exists the inverse function , where

Thus, a semantic network can be modeled as an undirected network. \qed

Given the example semantic network triple diagrammed in Figure \ref{fig:semantic-example}, where  and , the undirected network representation given by  is diagrammed in Figure \ref{fig:semantic-undirected}. Note that each , , and  is a unique vertex even though they are not notated as such.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4975\textwidth]{images/semantic-undirected}
	 \caption{\label{fig:semantic-undirected}An undirected network representation of a semantic network triple.}
\end{figure}

It is interesting to note the various types of self-loops in the undirected network representation in Figure \ref{fig:semantic-undirected}. There are the undirected self-loops as demonstrated by the edges , , and . Next, there are the directed self-loops as demonstrated by the  and  sub-networks which include their respective  vertices. Finally, if , there also exists the semantic self-loop.

There exists another method to map a semantic network to an undirected network. As discussed previously,  a directed network can represent a binary string and any semantic network representation, computationally, is ultimately represented as a series of bits. Therefore, it is possible to represent a semantic network as a directed network binary string. Given , it is possible to represent that directed network binary string as an undirected network.

\section{Conclusion}

This article defined the injective function . This function demonstrates that a semantic network has a one-to-one mapping with some undirected network. In this model, because an edge in an undirected network is neither labeled nor directed, both the semantic network edge labels and the directionality of edges are represented as topological features of the undirected network. While representing a semantic network as an undirected network is perhaps an inefficient use of resources, it is theoretically possible.

\begin{thebibliography}{13}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname bibnamefont\endcsname\relax
  \def\bibnamefont#1{#1}\fi
\expandafter\ifx\csname bibfnamefont\endcsname\relax
  \def\bibfnamefont#1{#1}\fi
\expandafter\ifx\csname citenamefont\endcsname\relax
  \def\citenamefont#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\providecommand{\bibinfo}[2]{#2}
\providecommand{\eprint}[2][]{\url{#2}}

\bibitem[{\citenamefont{Aasman}(2006)}]{agraph:aasman2006}
\bibinfo{author}{\bibnamefont{Aasman}, \bibfnamefont{J.}},
  \bibinfo{year}{2006}, \emph{\bibinfo{title}{Allegro Graph}},
  \bibinfo{type}{Technical Report} \bibinfo{number}{1},
  \bibinfo{institution}{Franz Incorporated},
  \urlprefix\url{www.franz.com/products/allegrograph/allegrograph.datasheet.pdf}.

\bibitem[{\citenamefont{Alexander and Ravada}(2006)}]{oracle:alexander2006}
\bibinfo{author}{\bibnamefont{Alexander}, \bibfnamefont{N.}}, and
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Ravada}},
  \bibinfo{year}{2006}, in \emph{\bibinfo{booktitle}{Proceedings of the 22nd
  {I}nternational {C}onference on {D}ata {E}ngineering ({ICDE}'06)}}
  (\bibinfo{publisher}{IEEE Computer Society}, \bibinfo{address}{Washington,
  DC, USA}), p.~\bibinfo{pages}{93}, ISBN \bibinfo{isbn}{0-7695-2570-9}.

\bibitem[{\citenamefont{Bax}(2004)}]{lib:bax2004}
\bibinfo{author}{\bibnamefont{Bax}, \bibfnamefont{M.}}, \bibinfo{year}{2004},
  in \emph{\bibinfo{booktitle}{{I}nternational {C}onference on {E}lectronic
  {P}ublishing ({ICCC2004})}} (\bibinfo{address}{Bras{\'\i}lia, Brazil}).

\bibitem[{\citenamefont{Berners-Lee and Hendler}(2001)}]{pubsem:lee2001}
\bibinfo{author}{\bibnamefont{Berners-Lee}, \bibfnamefont{T.}}, and
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Hendler}},
  \bibinfo{year}{2001}, \bibinfo{journal}{Nature}
  \textbf{\bibinfo{volume}{410}}(\bibinfo{number}{6832}),
  \bibinfo{pages}{1023}, \urlprefix\url{http://dx.doi.org/10.1038/35074206}.

\bibitem[{\citenamefont{Berners-Lee}
  \emph{et~al.}(2001)\citenamefont{Berners-Lee, Hendler, and
  Lassila}}]{lee:semantic2001}
\bibinfo{author}{\bibnamefont{Berners-Lee}, \bibfnamefont{T.}},
  \bibinfo{author}{\bibfnamefont{J.~A.} \bibnamefont{Hendler}}, and
  \bibinfo{author}{\bibfnamefont{O.}~\bibnamefont{Lassila}},
  \bibinfo{year}{2001}, \bibinfo{journal}{Scientific American} ,
  \bibinfo{pages}{34}.

\bibitem[{\citenamefont{Bollen} \emph{et~al.}(2007)\citenamefont{Bollen,
  Rodriguez, {Van de Sompel}, Balakireva, and Hagberg}}]{semever:bollen2007}
\bibinfo{author}{\bibnamefont{Bollen}, \bibfnamefont{J.}},
  \bibinfo{author}{\bibfnamefont{M.~A.} \bibnamefont{Rodriguez}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{{Van de Sompel}}},
  \bibinfo{author}{\bibfnamefont{L.~L.} \bibnamefont{Balakireva}}, and
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Hagberg}},
  \bibinfo{year}{2007}, in \emph{\bibinfo{booktitle}{{ACM} {W}orld {W}ide {W}eb
  {C}onference}} (\bibinfo{publisher}{{ACM} Press}, \bibinfo{address}{Banff,
  Canada}).

\bibitem[{\citenamefont{Brandes and Erlebach}(2005)}]{netanal:brandes2005}
\bibinfo{editor}{\bibnamefont{Brandes}, \bibfnamefont{U.}}, and
  \bibinfo{editor}{\bibfnamefont{T.}~\bibnamefont{Erlebach}} (eds.),
  \bibinfo{year}{2005}, \emph{\bibinfo{title}{Network Analysis: Methodolgical
  Foundations}} (\bibinfo{publisher}{Springer}, \bibinfo{address}{Berling,
  DE}).

\bibitem[{\citenamefont{Lee}(2004)}]{lee:triple2004}
\bibinfo{author}{\bibnamefont{Lee}, \bibfnamefont{R.}}, \bibinfo{year}{2004},
  \emph{\bibinfo{title}{Scalability Report on Triple Store Applications}},
  \bibinfo{type}{Technical Report}, \bibinfo{institution}{Massachusetts
  Institute of Technology}.

\bibitem[{\citenamefont{Newman} \emph{et~al.}(2006)\citenamefont{Newman,
  Barabasi, and Watts}}]{netsci:newman2006}
\bibinfo{author}{\bibnamefont{Newman}, \bibfnamefont{M.}},
  \bibinfo{author}{\bibfnamefont{A.-L.} \bibnamefont{Barabasi}}, and
  \bibinfo{author}{\bibfnamefont{D.~J.} \bibnamefont{Watts}},
  \bibinfo{year}{2006}, \emph{\bibinfo{title}{The Structure and Dynamics of
  Networks}} (\bibinfo{publisher}{Princeton University Press}).

\bibitem[{\citenamefont{Quan} \emph{et~al.}(2003)\citenamefont{Quan, Martin,
  and Grossman}}]{sembio:quan2003}
\bibinfo{author}{\bibnamefont{Quan}, \bibfnamefont{D.}},
  \bibinfo{author}{\bibfnamefont{S.}~\bibnamefont{Martin}}, and
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Grossman}},
  \bibinfo{year}{2003}, in \emph{\bibinfo{booktitle}{2nd {I}nternational
  {S}emantic {W}eb {C}onference ({ISWC2003})}} (\bibinfo{address}{Sanibel
  Island, Florida}),
  \urlprefix\url{http://theory.csail.mit.edu/~dquan/iswc2003-bioinformatics.pdf}.

\bibitem[{\citenamefont{Rodriguez}(2007)}]{rodriguez:gpsemnet2007}
\bibinfo{author}{\bibnamefont{Rodriguez}, \bibfnamefont{M.~A.}},
  \bibinfo{year}{2007}, \emph{\bibinfo{title}{General-Purpose Computing on a
  Semantic Network Substrate}}, \bibinfo{type}{Technical Report}
  \bibinfo{number}{LA-UR-07-2885}, \bibinfo{institution}{Los Alamos National
  Laboratory}, \urlprefix\url{http://arxiv.org/abs/0704.3395}.

\bibitem[{\citenamefont{Ruttenberg}
  \emph{et~al.}(2007)\citenamefont{Ruttenberg, Clark, Bug, Samwald,
  Bodenreider, Chen, Doherty, Forsberg, Gao, Kashyap, Kinoshita, Luciano}
  \emph{et~al.}}]{sembio:ruttenberg2007}
\bibinfo{author}{\bibnamefont{Ruttenberg}, \bibfnamefont{A.}},
  \bibinfo{author}{\bibfnamefont{T.}~\bibnamefont{Clark}},
  \bibinfo{author}{\bibfnamefont{W.}~\bibnamefont{Bug}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Samwald}},
  \bibinfo{author}{\bibfnamefont{O.}~\bibnamefont{Bodenreider}},
  \bibinfo{author}{\bibfnamefont{H.}~\bibnamefont{Chen}},
  \bibinfo{author}{\bibfnamefont{D.}~\bibnamefont{Doherty}},
  \bibinfo{author}{\bibfnamefont{K.}~\bibnamefont{Forsberg}},
  \bibinfo{author}{\bibfnamefont{Y.}~\bibnamefont{Gao}},
  \bibinfo{author}{\bibfnamefont{V.}~\bibnamefont{Kashyap}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Kinoshita}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Luciano}}, \emph{et~al.},
  \bibinfo{year}{2007}, \bibinfo{journal}{BMC Bioinformatics}
  \textbf{\bibinfo{volume}{8}}(\bibinfo{number}{3}), \bibinfo{pages}{S2}, ISSN
  \bibinfo{issn}{1471-2105},
  \urlprefix\url{http://www.biomedcentral.com/1471-2105/8/S3/S2}.

\bibitem[{\citenamefont{Sowa}(1991)}]{sowa:semantic1991}
\bibinfo{editor}{\bibnamefont{Sowa}, \bibfnamefont{J.~F.}} (ed.),
  \bibinfo{year}{1991}, \emph{\bibinfo{title}{Principles of Semantic Networks:
  Explorations in the Representation of Knowledge}} (\bibinfo{publisher}{Morgan
  Kaufmann}, \bibinfo{address}{San Mateo, CA}).

\end{thebibliography}


\end{document}