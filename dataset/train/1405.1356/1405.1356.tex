\documentclass[draft,a4paper]{llncs}
\usepackage[utf8x]{inputenc}
\usepackage{amstext, amsmath, amssymb,amscd}
\usepackage{hyperref}
\newtheorem{observation}{Observation}

\usepackage{xspace}

\newcommand{\F}{\ensuremath{\mathcal{F}}\xspace}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Oh}{\mathcal{O}}

\newcommand{\yes}{\textsc{yes}\xspace}
\newcommand{\no}{\textsc{no}\xspace}

\newcommand{\NP}{\ensuremath{\mathsf{NP}}\xspace}
\newcommand{\FPT}{\ensuremath{\mathsf{FPT}}\xspace}
\renewcommand{\P}{\ensuremath{\mathsf{P}}\xspace}
\newcommand{\coNP}{\ensuremath{\mathsf{coNP}}\xspace}
\newcommand{\containment}{\ensuremath{\mathsf{NP \subseteq coNP/poly}}\xspace}
\newcommand{\ncontainment}{\ensuremath{\mathsf{NP \nsubseteq coNP/poly}}\xspace}

\newcommand{\halt}{\mathbin{\rangle\mkern-4mu|}}
\newcommand{\resume}{\mathbin{|\mkern-4mu\langle}}

\newcommand{\dHSk}{-\textsc{Hitting Set()}\xspace}
\newcommand{\dSMk}{-\textsc{Set Matching()}\xspace}
\newcommand{\EDSk}{\textsc{Edge Dominating Set()}\xspace}
\newcommand{\CEk}{\textsc{Cluster Editing()}\xspace}

\newcommand{\MFIk}{\textsc{Minimum Fill-In()}\xspace}

\newcommand{\FBVSk}{\textsc{Feedback Vertex Set()}\xspace}
\newcommand{\OCTk}{\textsc{Odd Cycle Transversal()}\xspace}
\newcommand{\CVDk}{\textsc{Cluster Vertex Deletion()}\xspace}
\newcommand{\CEDk}{\textsc{Cluster Deletion()}\xspace}
\newcommand{\BERk}{\textsc{Edge Bipartization()}\xspace}
\newcommand{\BVRk}{\textsc{Bipartization by Vertex Removal()}\xspace}
\newcommand{\BLTPk}{\textsc{Bipartization by Edge Replacing with Length-2 Paths()}\xspace}
\newcommand{\BCNk}{\textsc{Bipartite Colorful Neighborhood()}\xspace}
\newcommand{\SSk}{\textsc{Set Splitting()}\xspace}
\newcommand{\CoVDk}{\textsc{Cograph Vertex Deletion()}\xspace}
\newcommand{\TEDk}{\textsc{Triangle Edge Deletion()}\xspace}
\newcommand{\TVDk}{\textsc{Triangle Vertex Deletion()}\xspace}
\newcommand{\TPk}{\textsc{Triangle Packing()}\xspace}
\newcommand{\sSPk}{-\textsc{Star Packing()}\xspace}




\newcommand{\introduceparameterizedproblem}[4]{
\fbox{
\parbox{0.96\textwidth}{
	#1\hfill \textbf{Parameter:} #3\\
	\textbf{Input:} #2\\	
	\textbf{Question:} #4}}
\vspace{0.3cm}	
}

\title{\texorpdfstring{Streaming Kernelization\thanks{Supported by the Emmy Noether-program of the DFG, KR 4286/1.}}{Streaming Kernelization}} 
\author{Stefan Fafianie \and Stefan Kratsch}
\institute{TU Berlin, Germany, \texttt{stefan.fafianie,stefan.kratsch@tu-berlin.de}}

\begin{document}
\maketitle

\begin{abstract}
Kernelization is a formalization of preprocessing for combinatorially hard problems. We modify the standard definition for kernelization, which allows any polynomial-time algorithm for the preprocessing, by requiring instead that the preprocessing runs in a streaming setting and uses  bits of memory on instances . We obtain several results in this new setting, depending on the number of passes over the input that such a streaming kernelization is allowed to make. \textsc{Edge Dominating Set} turns out as an interesting example because it has no single-pass kernelization but two passes over the input suffice to match the bounds of the best standard kernelization. 

\end{abstract}

\section{Introduction}

When faced with an \NP-hard problem we do not expect to find an efficient algorithm that solves every instance exactly and in polynomial time (as this would imply \P~~\NP). The study of algorithmic techniques offers various paradigms for coping with this situation if we are willing to compromise on efficiency, exactness, or the generality of being applicable to all instances (or several of those). Before we commit to such a compromise it is natural to see how much closer we can come to a solution by spending only polynomial time, i.e., how much we can simplify and shrink the instance by polynomial-time \emph{preprocessing}. This is usually compatible with any way of solving the simplified instance and it finds wide application in practice (e.g., as a part of ILP solvers like CPLEX), although, typically, the applications are of a heuristic flavor with no guarantees for the size of the simplified instance or the amount of simplification.

The notion of \emph{kernelization} is one way of formally capturing preprocessing. A kernelization algorithm applied to some problem instance takes polynomial time in the input size and always returns an equivalent instance (i.e., the instances will have the same answer) of size bounded by a function of some \emph{problem-specific parameter}. For example, the problem of testing whether a given graph~ has a vertex cover of size at most~ can be efficiently reduced to an equivalent instance~ where~ has~ vertices and~ total bit size. The study of kernelization is a vibrant field that has seen a wealth of new techniques and results over the last decade. (The interested reader is referred to recent surveys by Lokshtanov et al.~\cite{LokshtanovMS12} and Misra et al.~\cite{MisraRS11}.) In particular, a wide-range of problems is already classified into admitting or not admitting\footnote{Unless  and the polynomial hierarchy collapses.} a \emph{
polynomial} kernelization, where the guaranteed output size bound is polynomial in the chosen parameter.
It is seems fair to say that this shows a substantial \emph{theoretical} success of the notion of kernelization.

From a practical point of view, we might have to do more work to convince a practitioner that our positive kernelization results are also \emph{worth implementing}. This includes choice of parameter, computational complexity, and also conceptual difficulty (e.g., number of black box subroutines, huge hidden constants). Stronger parameterizations already receive substantial interest from a theoretical point of view, see e.g.,~\cite{DBLP:journals/ejc/FellowsJR13}, and there is considerable interest in making kernelizations fast, see e.g.,~\cite{BevernHKNW11,Hagerup11,Bevern12,Kammer13}. Conceptual difficulty is of course ``in the eye of the beholder'' and perhaps hard to quantify.

In this work, we take the perspective that kernelizations that work in a restricted model might, depending on the model, be provably robust and useful/implementable (and hopefully also fast). Concretely, in the spirit of studying restricted models, we ask which kernelizations can be made to work in a \emph{streaming model} where the kernelization has a small local memory and only gets to look at the input once, or a bounded number of times. The idea is that the kernelization should maintain a sufficiently good sketch of the input that in the end will be returned as the reduced instance.

We think that this restricted model for kernelization has several further benefits: First of all, it naturally improves the memory access patterns since the input is read sequentially, which should be beneficial already for medium size inputs. (It also works more naturally for huge inputs, but huge instances of \NP-hard problems are probably only really addressable by outright use of heuristics or sampling methods.) Second, it is naturally connected to a dynamic/incremental setting since, due to the streaming setting, the algorithm has not much choice but to essentially maintain a simplified instance of bounded size that is equivalent to the input seen so far (or be able to quickly produce one should the end of the stream be declared). Thus, as further input arrives, the problem kernel is adapted to the now slightly larger instance without having to look at the whole instance again. (In a sense, the kernelization could run in parallel to the \emph{creation} of the 
actual input.) Third, it 
appears, at least in our positive results, that one could easily translate this to a parallel setting where, effectively, several copies of the algorithm work on different positions on the stream to simplify the instance (this however would require that an algorithm may delete data from the stream).

\emph{Our results.}
In this work we consider a streaming model where elements of a problem instance are presented to a kernelization algorithm in arbitrary order. The algorithm is required to return an equivalent instance of size polynomial in parameter  after the stream has been processed. Furthermore, it is allowed to use  bits of memory, i.e., an overhead factor of  is used in order to distinguish between elements of an instance of size .

We show that \dHSk and \dSMk admit streaming kernels of size  while using  bits of memory where  is the universal set of an input instance. We then consider a single pass kernel for \EDSk and find that it requires at least  bits of memory for instances with  edges. This rules out streaming kernels with  bits for instances with  vertices since for any fixed  and  there exist instances with . Insights obtained from this lower bound allow us to develop a general lower bound for the space complexity of single pass kernels for a class of parameterized graph problems. 

Despite the lower bound for single pass kernels, we show that \EDSk admits a streaming kernel if it is allowed to make a pass over the input stream twice. Finally, we use communication complexity games in order to rule out similar results for \CEk and \MFIk and show that multi-pass streaming kernels for these problems must use  bits of local memory for graphs with  vertices, even when a constant number of passes are allowed. 

\emph{Related work.} The data stream model is formalized by Henzinger et al.~\cite{raghavan1999computing}. Lower bounds for exact and randomized algorithms with a bounded number of passes over the input stream are given for various graph problems and are proven by means of communication complexity. An overview is given by Babcock et al.~\cite{babcock2002models} in which issues that arise in the data stream model are explored. An introduction and overview of algorithms and applications for data streams is given by Muthukrishnan \cite{muthukrishnan2005data}. 


\emph{Organization.} 
Section \ref{sec:prelim} contains preliminaries and a formalization of kernelization algorithms in the data streaming setting. Single pass kernels for \dHSk and \dSMk are presented in Section \ref{sec:singlepass}. The lower bounds for single pass kernels are given in Section \ref{sec:singlepass:bounds}. The 2-pass kernel for \EDSk is shown in Section \ref{sec:2pass} while lower bounds for multi-pass kernels are given in Section \ref{sec:multi}. Finally, Section \ref{sec:conc} contains concluding remarks.

\section{Preliminaries} \label{sec:prelim}
We use standard notation from graph theory. For a set of edges , let  be the set of vertices that are 
incident with edges in . For a graph , let  denote the subgraph of  induced by .  Furthermore, let  be the subgraph induced by , i.e. . 

A \emph{parameterized problem} is a language ;
the second component~ of instances~ is called the \emph{parameter}. 
A parameterized problem is \emph{fixed-parameter tractable} if there is an algorithm that decides if  
in  time, where  is any computable function. 
A \emph{kernelization algorithm (kernel)} for a parameterized problem  is an algorithm that, for input  outputs a pair  in  time such that  for some computable function , called the \emph{size} of the kernel, and . A \emph{polynomial kernel} is a kernel with polynomial size.

\subsection*{Kernelization in the data-streaming model}

An input stream is a sequence of elements of the input problem. 
We denote the start of an input stream by  and let  denote the end, 
e.g.  denotes an input stream for a sequence of  elements. 
We use  to denote a halt in the stream and  to denote its continuation, 
e.g.  and  denote the same input stream broken up in two parts.

A \emph{streaming kernelization algorithm} (\emph{streaming kernel}) is an algorithm that receives input  for a
parameterized problem in the following fashion. The algorithm is presented with an input stream where elements of  are presented in a sequence,
i.e. adhering to the cash register model \cite{muthukrishnan2005data}. 
Finally, the algorithm should return a kernel for the problem upon request. A \emph{-pass streaming kernel} is a streaming kernel that is allowed  passes over the input stream before a kernel is requested. 

If  is a graph, then the sequence of elements of  are its edges in arbitrary ordering. In a natural extension to hypergraphs, if  is a family of subsets on some ground set , then the sequence of elements of  are the sets of this family in arbitrary ordering. We assume that a streaming kernelization algorithm receives parameter  and the size of the vertex set (resp. ground set) before the input stream. Note that this way isolated vertices are given implicitly.

Furthermore, we require that the algorithm uses a limited amount of space at any time during its execution. In the strict streaming kernelization setting the streaming kernel must use at most  space where  is a polynomial.
We will refer to a 1-pass streaming kernelization algorithm which upholds these space bounds simply as a \emph{streaming kernelization}.

We assume that words of size  in memory can be compared in  operations when considering the running time of the streaming kernelization algorithms in each step.

\section{Single pass kernelization algorithms}\label{sec:singlepass}
In this section we will show streaming kernelization algorithms for -\textsc{Hitting Set} 
and -\textsc{Set Matching} in the 1-pass data-stream model. 
These algorithms make a single pass over the input stream after which they output a kernel.
We analyze their efficiency with regard to local space and the worst case processing time for a 
single element in the input stream.


\medskip
\noindent
\introduceparameterizedproblem{\dHSk}{A set~ and a family~ of subsets of~ each of size at most~, i.e.~, and~.}{.}{Is there a set~ of at most~ elements of~ that has a nonempty intersection with each set in~?}

In the following, we describe a single step of the streaming kernelization. After Step~, the algorithm has seen a set~, where~ denotes the whole set of edges provided in the stream. The memory contains some subset~, using for each~ a total of at most~ bits to denote the up to~ elements therein. The algorithm maintains the invariant that the number of sets~ that contain any~ as a subset is at most . For intuition, let us remark that this strongly relates to the sunflower lemma. Now, let us consider Step~. The memory contains some~ and a new set~ arrives.
\smallskip

1. Iterate over all subsets~ of~, ordered by decreasing size.

2. Count the number of sets in~ that contain~ as a subset.

3. If the result equals~ then the algorithm decides not to store~ and ends the computation for Step~, i.e., let~.
 
4. Else, continue with the next set~.
 
5. If no set~ gave a total of~ sets containing~ then the algorithm decides to store~, i.e.,~. Note that this preserves the invariant for all~ since only the counts for~ with~ can increase, but all those were seen to be strictly below the threshold~ so they can at most reach equality by adding~.

\smallskip
To avoid confusion, let us point out that at any time the algorithm only has a single set~; the index~ is used for easier discussion of the changes over time.

\begin{observation}\label{obs:dhs}
The algorithm stores at most~ sets at any point during the computation. This follows directly from the invariant when considering~.
\end{observation}

\begin{theorem}\label{thm:dhs} \emph{(\footnote{Proofs of statements marked with  are postponed to the appendix.})}
\dHSk admits a streaming kernelization which, using  bits of local memory and  time in each step, returns an equivalent instance of size .
\end{theorem}

The time spent in each step can be improved from  to  at the cost of an increase in local space
by a constant factor. This can be realized with a tree structure  in which the algorithm maintains the number of sets in  that contain a set  as a subset.

Each ,  has a corresponding node in  and in this node the number of supersets of  in  are stored. Let the root node represent  with a child for each set  of size 1. In general, a node is assigned an element in  and represents  where  is the set represented by its parent, i.e.  for nodes with depth . 

For each node, let  be assigned to child node . Furthermore, each node has a dictionary, i.e. a collection of (key, value) pairs  in order to facilitate quick lookup of its children. Let us assume that there is some arbitrary ordering on elements that are in sets of , e.g. by their identifier. Then the dictionary can be implemented as a self-balancing binary search tree. This allows us to find a child node and insert new child node in time  if there are  children.

\begin{corollary}\label{cor:dhs} \emph{()}
 \dHSk admits a streaming kernelization which, using  bits of local memory and  time in each step, returns an equivalent instance of size .
\end{corollary}


\medskip
\noindent
\introduceparameterizedproblem{\dSMk}{A set~ and a family~ of subsets of~ each of size at most~, i.e.~, and~.}{.}{Is there a matching  of at least~ sets in~, i.e. are there~ sets in~ that are pairwise disjoint?}

The streaming kernelization will mostly perform the same operations in a single step as the algorithm described above such that only the invariant differs. In this case it is maintained that the number of sets  that contain any  as a subset is at most .

\begin{observation}
 The algorithm stores at most~ sets at any point during the computation. This follows directly from the invariant when considering .
\end{observation}

\begin{theorem} \label{thm:dsm}  \emph{()}
 \dSMk admits a streaming kernelization which, using  bits of local memory and  time in each step, returns an equivalent instance of size .
\end{theorem}

Similar to the algorithm described in the previous section, the running time in each step can be improved at the cost of an increase in local space by a constant factor. We omit an explicit proof.

\begin{corollary}
 \dSMk admits a streaming kernelization which, using  bits of local memory and  time in each step, returns an equivalent instance of size .
\end{corollary}


\section{Space lower bounds for single pass kernels} \label{sec:singlepass:bounds}

We will now present lower bounds on the memory requirements of single pass streaming kernelization algorithms for a variety of graph problems. Before giving a general lower bound we first illustrate the essential obstacle by considering the \EDSk problem.  We show that a single pass kernel for \EDSk requires at least  bits of memory on instances with~ edges.



\medskip\noindent
\introduceparameterizedproblem{\EDSk}{A graph  and .}{.}{Is there a set~ of at most~ edges such that every edge in~ is incident with an edge in~?}

An obstacle that arises for many problems, such as \EDSk, is that they are not monotone under adding additional edges, i.e., additional edges do not always increase the cost of a minimum edge dominating set but may also decrease it. This decrease, however, may in turn depend on the existence of a particular edge in the input. Thus, on an intuitive level, it may be impossible for a streaming kernelization to ``decide'' which edges to forget, since worst-case analysis effectively makes additional edges behave adversarial. (Note that our lower bound does not depend on assumptions on what the kernelization decides to store.)

Consider the following type of instance as a concrete example of this issue. The input stream contains the number of vertices (immaterial for the example), the parameter value~, and a sequence of edges . That is, the first~ edges form a star with  leaves and center vertex~. In order to use a relatively small amount of local memory the kernelization algorithm is forced to do some compression such that not every edge belonging to this star is stored in local memory. Now a final edge arrives and the algorithm returns a kernel. Note that the status of the problem instance depends on whether or not this edge is disjoint from the star: If it shares at least one vertex~ with the star then there is an edge dominating set~ of size one. Otherwise, if it is disjoint then clearly at least two edges are needed. Thus, from the memory state after the final edge we must be able to extract whether or not~ is contained in~; in other words, this is equivalent to whether or not the output kernelized instance is \yes or \no. (We assume that~ for this example.) This, however, is a classic problem for streaming algorithms that is related to the \emph{set 
reconciliation problem} and it is known to require at least~ bits \cite{muthukrishnan2005data}; we give a short self-contained proof for our lower bound. 

\begin{theorem} \label{thm:eds}  \emph{()}
 A single pass streaming kernelization algorithm for \EDSk requires at least  bits of local memory for instances with  edges.
\end{theorem}


\subsection*{General lower bound for a class of parameterized graph problems}\label{ssec:singlepass:generallb}

In the following we present space lower bounds for a number of parameterized graph problems. By generalizing the previous argument we find a common property that can be used to quickly rule out single pass kernels with  memory. We then provide a list of parameterized graph problems for which a single pass streaming kernelization algorithm requires at least  bits of local memory.

\begin{definition}
 Let  be a parameterized graph problem and let . Then  has a --\emph{stream obstructing graph}  if , there is a set of edges  of size  such that ,  if and only if .
\end{definition}

In other words, each edge  could equally be critical to decide if  for a graph instance  induced by a subset  and a constant sized remainder of edges , depending on what  looks like. Note that  may contain isolated vertices which can also be used to form edge sets . We also consider  to be a --stream obstructing graph in the case that the above definition holds except that ,  if and only if . We omit the proofs for this symmetrical definition in this section.

\begin{lemma} \label{lem:gen} \emph{()}
 Let  be a parameterized graph problem and let . If  has a --stream obstructing graph  with  edges, then a single pass streaming kernelization algorithm for  requires at least  bits of local memory for instances with at most  edges.
\end{lemma}

The following theorem is an easy consequence of Lemma~\ref{lem:gen} for problems that, essentially, have stream obstructing graphs for all numbers~ of edges. Intuitively, of course also having such graphs only for an infinite subset of~ suffices to get a similar bound.

\begin{theorem} \label{thm:gen}
 Let  be a parameterized graph problem. If there exist  such that for every ,  has a --stream obstructing graph  with  edges, then a single pass streaming kernelization algorithm for  requires at least  bits of local memory.
\end{theorem}

\begin{proof}
 Let  be a single pass streaming kernelization algorithm for . Assume that there is a stream obstructing graph  for  with  edges for every . Then for every  there is a group of instances  where for each ,  for some  and remainder of edges  of size , i.e. . Let us consider all graph instances  with exactly  edges. Some of these
 instances are in , i.e.  for some . By Lemma \ref{lem:gen},  requires at least  bits of local memory in order to distinguish these instances correctly. \qed \end{proof}
 
 The following corollary is a result of Theorem \ref{thm:gen} and constructions of stream obstructing graphs of arbitrary size for a variety of parameterized graph problems. We postpone these constructions to Appendix \ref{app:list}, where we will also exhibit proofs of correctness for a few of them.

\begin{corollary}
 For each of the following parameterized graph problems, a single pass streaming kernelization requires at least  bits of local memory:
 \EDSk, \CEk, \CEDk, \CVDk, \CoVDk, \MFIk, \BERk, \FBVSk, \OCTk, \TEDk, \TVDk, \TPk, \sSPk, \BCNk.
\end{corollary}



\section{2-pass kernel for Edge Dominating Set} \label{sec:2pass}
Despite the previously shown lower bound of  bits for a single pass kernel, there is a space efficient streaming kernelization algorithm for \textsc{Edge Dominating Set}() if we allow it to make a pass over the input stream twice. We will first describe a single step of the streaming kernelization during the first pass. This is effectively a single pass kernel for finding a -vertex cover. After Step  the algorithm has seen a set . Some subset  of edges is stored in memory. Let us consider Step  where a new edge  arrives.

\smallskip

 1. Count the edges in  that are incident with ; do the same for~.
 
 2. Let  if either of these counts is at least .
 
 3. Otherwise, let .
 
 4. If , then return a \no instance.

\begin{lemma}\label{lem:vc} \emph{()}
 After processing any set  of edges on the first pass over the input stream the algorithm has a set  such
 that any set  of at most  vertices is a vertex cover for  if and only if  is a vertex cover for .
\end{lemma}

Let  be the edges stored after the first pass. If there are more than  vertices with degree  in  then the algorithm returns a \no instance. We will continue with a description of a single step during the second pass. 
After Step  the algorithm has revisited a set . Some subset  of edges is stored along with
. Now, let us consider Step  where the edge  is seen for the second time.

\smallskip

 1. Let  if  and .
 
 2. Otherwise, let .
 
 \smallskip

\noindent Let  be the edges stored during the second pass. The algorithm will return , which is effectively , after both passes have been 
processed without returning a \no instance.

\begin{lemma}\label{lem:eds} \emph{()}
 After processing both passes the algorithm has a set  such that there is an edge dominating set 
  of size at most  for  if and only if there is an edge dominating set  of size at most  for .
\end{lemma}

\begin{theorem} \label{thm:2pass} \emph{()}
 \EDSk admits a two-pass streaming kernelization algorithm which, using  bits of local memory and
  time in each step, returns an equivalent instance of size .
\end{theorem}
  
If the algorithm stores a counter for the size of  and a tree structure  in which it maintains the number of sets (edges) in  that are a superset of  as described in Section \ref{sec:singlepass}, then the operations in each step can be performed in  time. We give the following corollary and omit the proof.

\begin{corollary}
 \EDSk admits a two-pass streaming kernelization algorithm which, using  bits of local memory and  time in each step, returns an equivalent instance of size .
\end{corollary}


\section{Space lower bounds for multi-pass streaming kernels}\label{sec:multi}

In this section we will show lower bounds for multi-pass streaming kernels for \textsc{Cluster Editing}() and \textsc{Minimum Fill-In}(). Similar to \textsc{Edge Dominating Set}(), it is difficult to return a trivial answer for these problems when the local memory exceeds a certain bound at some point during the input stream. Additional edges in the stream may turn a \no instance into a \yes instance and vice versa, which makes single pass streaming kernels infeasible. Although there is a 2-pass streaming kernel for \textsc{Edge Dominating Set}(), we will show that a -pass streaming kernel for \textsc{Cluster Editing}() requires at least  bits of local memory for instances with  vertices. As a consequence,  bits are required when a constant number of passes 
are allowed. Furthermore,  passes are required when the streaming kernel uses at most  bits of memory. We show a similar result for \textsc{Minimum Fill-In}(). 

\medskip
\noindent
\introduceparameterizedproblem{\CEk}{A graph  and }{}{Can we add and/or delete at most~ edges such that~ becomes a disjoint union of cliques?}

Let us consider the following communication game with two players,  and . Let  be a set of  vertices and let . The players are given a subset of vertices,  and  respectively. Let  denote the edges of a clique on . Furthermore, let  denote the edges of a star with center vertex  and leaves . The object of the game is for the players to determine if  is a disjoint union of cliques. The cost of the protocol for this game is the number of bits communicated between the players such that they can provide the answer. We can provide a lower bound for this cost by using the notion of fooling sets as shown in the following lemma.

\begin{lemma} \emph{(\cite{arora2009computational})} \label{lem:fool}
 A function  has a size  fooling set if there is an -sized subset 
  and value  such that,
 
 \smallskip
 
 (1) for every pair ,  
  
 (2) for every distinct , either  of .
 
 \smallskip

 \noindent If  has a size- fooling set then  where  is the minimum number of bits communicated in a two-party protocol for .
\end{lemma}

Let  be a function modeling our communication game where  if  forms a 
disjoint union of cliques and  otherwise. We provide a fooling set for  in the following lemma.

\begin{lemma} \label{lem:foolce}
 has a fooling set .
\end{lemma}

\begin{proof}
For every  we have  in which there is a clique on vertices  while the vertices in  are completely isolated and thus form cliques of size 1, i.e.  for every . Now let us consider pairs . We must show that either  or .
Clearly  since . Let us assume w.l.o.g. that , i.e. there is a vertex . Then  since . However,  since  and by definition also  since  is a star with center . Thus,  is not a disjoint union of cliques, i.e.,  and the lemma holds. \qed
\end{proof}

The size of  is , implying by Lemma \ref{lem:fool} that the protocol for  needs at least  bits of communication. Intuitively, if we use less than  bits, then by the pigeonhole principle there must be some pairs  for which the protocol is identical. Then the players cannot distinguish between the cases , i.e. for each case the same answer will be given and thus the protocol is incorrect. We can now prove the following theorem by considering how the players could exploit knowledge of a multi-pass kernel for \CEk with small local memory in order to beat the lower bound of the communication game.


\begin{theorem} \label{thm:ce}
 A streaming kernelization algorithm for \CEk requires at least  bits of local memory for instances with  vertices if it is allowed to make  passes over the input stream.
\end{theorem}

\begin{proof}
 Let us assume that the players have access to a multi-pass streaming kernelization algorithm  for \CEk.
 They can then use  to solve the communication game for  by simulating passes over an input stream in the following way. First,  initiates  with budget . To let  make a pass over ,  feeds  with partial input stream . It then sends the current content of the local memory of  to , which is then able to resume  and feeds it with . In order to let  make multiple passes,  can send the local memory content back to . Finally, when enough passes have been made an instance can be requested from  for which the answer is \yes if and only if .
 
 Now suppose  is a -pass streaming kernel with less than  bits of local memory for instances with  vertices. In each pass the local memory is transmitted between  and  twice. Then in total the players communicate less than  bits of memory. This is a contradiction to the consequence of Lemmata \ref{lem:fool} and \ref{lem:foolce}. Therefore  requires at least  bits. \qed
\end{proof}

Note that this argument also holds for the \textsc{Cluster Deletion}() and \textsc{Cluster Vertex Deletion}()
problems where we are only allowed to delete  edges, respectively vertices to obtain a disjoint union of cliques.


\medskip
\noindent
\introduceparameterizedproblem{\MFIk}{A graph  and .}{.}{Can we add at most~ edges such that~ becomes chordal, i.e.  does not contain an induced cycle of length 4?}

Let us consider the following communication game with two players,  and . Let  be a set of  vertices and let . The players are given a subset of vertices,  and  respectively. Let  denote the edges of a star with center vertex  and leaves . Furthermore, let  denote the edges of a star with center vertex  and leaves . The object of the game is for the players to determine if  is a chordal graph. Let  be a function modeling this communication game, i.e.  if  is chordal and  otherwise. We provide a fooling set for  in the following lemma.

\begin{lemma} \label{lem:foolmfi}
  has a fooling set .
\end{lemma}

The size of  is , implying by Lemma \ref{lem:fool} that the protocol for  needs at least  bits of communication. The following results from a similar argument to that of the proof of Theorem \ref{thm:ce}. We omit an explicit proof.

\begin{theorem}
 A streaming kernelization algorithm for \MFIk requires at least  bits of local memory for instances with  vertices if it is allowed to make  passes.
\end{theorem}



\section{Conclusion} \label{sec:conc}
In this paper we have explored kernelization in a data streaming model. Our positive results include single pass kernels for \dHSk and \dSMk, and a 2-pass kernel for \EDSk. We provide a tool that can be used to quickly identify a number of parameterized graph problems for which a single pass kernel requires  bits of local memory for instances with  edges. Furthermore, we have shown lower bounds for the space complexity of multi-pass kernels for \CEk and \MFIk.



\bibliographystyle{abbrv}
\bibliography{references}

\newpage
\appendix
\section{Proofs omitted from Section \ref{sec:singlepass}}
\subsection{Proof for Theorem \ref{thm:dhs}}

\begin{lemma}\label{lem:dhs}
After processing any set~ of edges on the input stream the algorithm has a set~ such that any set~ of size at most~ is a hitting set for~ if and only if~ is a hitting set for~.
\end{lemma}

\begin{proof}
We prove the lemma by induction. Clearly, the lemma is true for~. Now, assume that the lemma holds for all~ and consider Step~ in which, say, a set~ appears on the stream.
Clearly, if there is a~-hitting set~ for~ then~ is also a~-hitting set for~. I.e., this direction holds independently of whether the algorithm decides to store~.

The converse, i.e., that a~-hitting set for~ is also a~-hitting set for~, could only fail if the algorithm decided not to put~ into~; otherwise, such a~-hitting set~ would intersect~ and all sets in~, with the latter implying (by induction) that it intersects all sets in~. Assume that~ which implies that the algorithm discovered a set~ such that

sets in~ are supersets of~. By the ordering of considered subsets~ of~ we know that for all~ of larger size there are strictly less than~ sets containing~. Note that if~ is contained in~ then this already enforces that any hitting set for~ also hits~, so w.l.o.g.\ we assume that all sets are strict supersets of~.

Let us consider the effect that adding~ would have on~, i.e., consider~. By the previous considerations for~ we conclude that in~ there are \emph{more than}~ that contain~ (since~ also contains it). For all larger sets~ we reach a count of \emph{at most}~. Crucially, this is where our invariant comes in, \emph{for all sets~ that are not subsets of~} the counts are not increased when going from~ to~, so there are also \emph{at most}~ sets containing any~.

Now, for analysis, consider any maximal packing~ of supersets of~ such that the sets~ are pairwise disjoint (i.e., the sets pairwise overlap exactly in~). This implies that all further supersets of~ in~ must overlap some~. Let~ and note that the size of~ is at most~ with equality if all~ have size~. For any~ we can consider~ and obtain that strictly less than

sets contain both~ and~. Since exactly~ contain~ as a strict subset (i.e., each contains at least one more element~), we get that

which implies~. Thus,~, i.e.,~. Now, we return from~ to~ and note that even without having~ at least~ of the sets~ are in~. (We do not make any assumption about presence of~ among these sets.) For ease of presentation let us rename~ of those sets to~.

Assume that~ has a~-hitting set, then by the induction hypothesis, there is also a~-hitting set~ for~. Since~ this set~ must also be a hitting set for~. Since~ some element~ must intersect at least two of the sets~, but then it also intersects the set~. Thus,~ intersects also~, implying that it is a~-hitting set for~ as claimed. This completes the inductive argument, and the proof.
\qed \end{proof}

Using the lemma, it is now straightforward to prove that the described algorithm is a streaming kernelization for \dHSk.

\begin{proof}
Correctness follows from Lemma \ref{lem:dhs}. As previously observed the algorithm stores at most  sets at any
time during the computation. The elements of a set can be stored using  bits, i.e.  bits are used in total.
After the input stream has been processed the elements in  can be relabeled such that they can be stored using ) bits, i.e. an equivalent instance of size  bits is returned.
 
The algorithm iterates over at most  subsets of the new set in each step. For each subset  the number of sets in  that are a superset of  can be counted in  time, i.e. the algorithms spends  time in each step. \qed
\end{proof}

\subsection{Proof for Corollary \ref{cor:dhs}}

\begin{proof}
 We can find the corresponding node for a set  by traversing  as follows. Let  denote the root and let 
 such that  for . Then node  is assigned  and is a 
child of node . Finally,  is the node corresponding to  in which the number of sets in  that contain a
superset of  is stored. Each node in the  has at most  children. We can look up the child that is 
assigned  in  time by using the binary search tree. This step is performed  times for each 
, i.e.  time is spent in each step. The case that there is no node for  in , i.e. there is no  such that , can be identified similarly.

If the algorithm decides that  then the number stored in the node corresponding to  is increased by 1 for 
each  in increasing order of cardinality. In the case that there is no node for  a new one will be
inserted at the appropriate place in , i.e. by finding the node in  corresponding to  and inserting a new child for element . Again, updating takes  time for each , i.e.  time in each step.

Each set  has at most  subsets, each of which has a corresponding node in the tree. Then there are at most
 nodes in the case that none of these subsets overlap. Instead of , the algorithm now uses  space. \qed
\end{proof}

\subsection{Proof for Theorem \ref{thm:dsm}}

\begin{lemma}
 After processing any set  of edges on the input stream the algorithm has a set  such that there is a -set matching  for  if and only if there is a -set matching  for .
\end{lemma}

\begin{proof}
 We prove the lemma with an argument that is similar to the proof of Lemma \ref{lem:dhs} and point out the key differences. 
 Clearly, if there is a -set matching  for  then  is also a -set matching for .
 
 The converse, i.e., that a -set matching  for  implies the existence of a -set matching  for , could only fail if the algorithm decided not to put  into ;
 if  is not required for  then it certainly does not obstruct such a matching. 
 Then let us assume  which implies that the algorithm discovered a set  for which there are
 at least  supersets , such that their pairwise intersection is exactly , and one of these sets is .
 Then there are at least  such sets in 
 
 Assume that  has a -set matching . If , then  is a matching for  and by induction hypothesis there is a -set matching  for . In the case that , then there is at least one set among  that can take the role of  in a matching . Each of these sets has at least one element that does not intersect . Then a matching of size  that does not contain a superset of  can contain at most  of these sets. This leaves at least one set  that does not intersect any set in the  matching. Then  can be replaced by  in , i.e. there is a -set matching in  and therefore also in . \qed \end{proof}
 
 The proof of Theorem \ref{thm:dsm} follows from the lemma in a similar way to the proof for \dHSk

\section{Proofs omitted from Section \ref{sec:singlepass:bounds}}
\subsection{Proof for Theorem \ref{thm:eds}}

\begin{proof}
Consider the following category of instances. Let  and define ~. Let  be a single pass streaming kernelization algorithm for \EDSk and let  receive budget  and partial input stream ,
where~, i.e., the stream contains exactly~ (the order therein is immaterial for our argument).
If  uses less than  bits of local memory, then by the pigeonhole principle there must be a pair  such that  where  and  result in the same memory state. 

Now  must return the same problem kernel for  and  for every edge  since its behavior always depends on its memory and the rest of the input stream. Let us assume w.l.o.g.\ that  and let~. It follows that~ is an edge dominating set for the instance with edge set~, making this instance \yes. The instance with edge~, however, has two connected components and thus is \no for budget~. Thus,~ cannot answer correctly for both instances; contradiction. Thus, any streaming kernelization must use at least~ bits for this type of instance with~ edges.

For every , if we set  then there is an instance with  edges for which  requires at least  bits. Therefore, any single pass streaming kernelization algorithm for \EDSk requires at least  bits of local memory. \qed \end{proof}
\subsection{Proof for Lemma \ref{lem:gen}}
\begin{proof}
 Let  be a single pass streaming kernelization algorithm for  using less than  bits. We consider a worst case scenario for the ordering in which edges appear in the input stream. If a streaming kernel requires some minimum amount of memory for this ordering, then it requires at least as much memory when the edges appear in some arbitrary order. 
 
 Let us consider instances with at most  edges that have an input stream of the type . 
 That is, a subset of edges  appears first, followed by some set  of size . If  uses less than  bits of local memory, then by the pigeonhole principle there must be a pair of subsets  such that , where  and  result in the same memory state.
 
 Now  must return the same problem kernel for  and  for every . Let us assume w.l.o.g. that  and let~. Thus, for the corresponding set  we have  and . We conclude that  is not a correct kernelization algorithm since it cannot answer both instances correctly if being in the same state after  and . Therefore, any single pass streaming kernelization algorithm for  requires at least  bits of local memory for instances with at most  edges.
\qed \end{proof}

\section{Construction of Stream Obstructing Graphs} \label{app:list}
\paragraph{Edge Dominating Set.} The construction for a 1-1-stream obstructing graph for \EDSk with an arbitrary number of edges is implicitly used in the proof of Theorem \ref{thm:eds}.

\paragraph{Cluster Editing.} We construct a 1-0-stream obstructing graph  for \CEk with  edges in the following way.
Let  be a set of pairwise disjoint edges such that  and let  where . The following lemma shows that this construction suffices.

\begin{lemma}
 is a 1-0-stream obstructing graph for \CEk. 
\end{lemma}

\begin{proof}
For every edge , choose , i.e. . For every subset  we show that  is a cluster graph if and only if  since we have a budget of 0. Suppose that . Then  has an induced  on vertices , i.e. it is not a cluster graph. In the other case suppose that . Then  is a set of  disjoint edges since  does not intersect with any edge in , i.e. it is a cluster graph. This completes the proof. \qed
\end{proof}

We observe that  is also a 1-0-stream obstructing graph for \CEDk and \CVDk since a budget of 0 forces any graph in a \yes instance to be a cluster graph.

If we choose , then we can show that  is a 2-0-stream obstructing graph for \BERk, \FBVSk, \OCTk, \TEDk and \TVDk since any induced triangle is a forbidden structure in any instance with budget 0 for these problems. There is a triangle in instances  if and only if , namely on vertices . Furthermore,  is a 2-1-stream obstructing graph for \TPk since there is a single triangle in the graph if and only if .

\paragraph{Cograph Vertex Deletion.} We construct a 2-0-stream obstructing graph  for \CoVDk with  edges in the following way. Let  be a set of pairwise disjoint edges such that  and let  where . For every  we choose , i.e. . Similar to the stream obstructing graph for \CEk, a budget of 0 forces any graph in a \yes instance to be a cograph. This only holds for graphs  for subsets  if  since otherwise the graph has an induced  on vertices .

If we choose , then we can show that  is a 3-0-stream obstructing graph  for \MFIk since there is a  in instances  if and only if , namely on vertices .

\paragraph{s-Star Packing.}
We construct a -1-stream obstructing graph  for \sSPk with  edges in the following way. Let  be a set of pairwise disjoint edges such that  and let  where . We show that this construction suffices.

\begin{lemma}
  is a -1-stream obstructing graph for \sSPk.
\end{lemma}

\begin{proof}
For every edge , choose , i.e. . For every subset  we show that  contains exactly one instance of . Suppose that . Then  contains a  with center vertex  and leaves . In the other case suppose that . Then  is a set of disjoint edges plus a  on center vertex  with leaves . \qed
\end{proof}

\paragraph{Bipartite Colorful Neighborhood.}
In a natural extension to graph streaming we assume that edges  in a stream for bipartite graphs  are given such that  and . We construct a 1-1-stream obstructing graph  for \BCNk with  edges in the following way. Let  be a set of pairwise disjoint edges such that  and let  where  and . The following lemma shows that this construction suffices.

\begin{lemma}
  is a 1-1-stream obstructing graph for \BCNk.
\end{lemma}

\begin{proof}
 For every edge , choose , i.e. . For every subset  we show that  has a vertex in  with two neighbors in  if and only if . Suppose that . Then  has  and edges  and , i.e. there is a vertex  with a neighborhood in  that can be two-colored. In the other case, suppose that . Then vertices in  in the graph  have at most one neighbor.
 \qed
\end{proof}

\section{Proofs omitted from Section \ref{sec:2pass}}

\subsection{Proof for Lemma \ref{lem:vc}}

\begin{proof}
 We prove the lemma by induction. Clearly, the lemma is true for . Now, assume that the lemma holds for all  and consider Step  in which an edge  appears on the stream. First, suppose that . Vertices are incident to at most  edges in , i.e.  vertices can cover at most  edges in . Therefore, there can be no solution and the algorithm can safely return a \no instance. 
 
 In the other case, let us assume that . Clearly, if there is a -vertex cover  for , then  is also a -vertex cover for . 
 The converse, i.e. that a -vertex cover for  is also a -vertex cover for  could
 only fail if the algorithm decided not to put  in ; otherwise, such a -vertex cover  would cover  and all edges in , with the latter implying (by induction) that  covers all edges in .
 
 Then let us assume  which implies that the algorithm discovered a vertex  that is incident to  and at
 least  edges in . Then any vertex cover  of size at most  must contain  in order to cover these edges, i.e.
  will cover  in any case. Thus  is also a -vertex cover for . \qed \end{proof}
 
\subsection{Proof for Lemma \ref{lem:eds}}

\begin{proof}
 Suppose  is an edge dominating set of size at most  for . Then  is a vertex cover of size at most   for  and therefore also for . By Lemma \ref{lem:vc} we have that  is also a vertex cover for . Therefore  is also an edge dominating set for  since the endpoints of edges in  cover all edges in  and .
 
 For the converse, suppose that  is an edge dominating set of size at most  for . Then  is a vertex cover of size at most  for . Each edge in  is incident with at least one vertex  such that  edges in  are incident with , i.e.  must be part of a . Therefore  is also a vertex cover for  since it also covers all edges in .
 
 Now let us verify that there is an edge dominating set  of size at most  for . We will show how to find  by considering edges  of . First, let . If , then , i.e. add  to  in order to cover neighboring edges of .
 If , then  does not require  since in this case every edge in  is incident with neither  nor .
 In the remaining case we have w.l.o.g. , , i.e. there are no edges in  that
 are incident with . Then  can be substituted by any other edge  in  that is incident with , i.e. add  to . At the end  is an edge dominating set of size at most  for . \qed \end{proof}

\subsection{Proof for Theorem \ref{thm:2pass}}

\begin{proof}
 Correctness of the algorithm follows from Lemma \ref{lem:eds}. After the first pass there is a set  with at most
  edges. Let  be the set of vertices in  of degree at least  and let  be the set of vertices of degree at most , i.e.  and .
 
 After the second pass there are  edges in  that are incident with two vertices in . None of the edges that are incident with two vertices in  were discarded in the first pass, i.e. there are  such edges in . Finally, there are at most  edges that are incident with a vertex in  and a vertex in  since every vertex in  has at most  neighbors in .
 
 An edge can be stored using  bits and  . The algorithm stores a subset of  at any time during the execution which requires  bits. Therefore, the algorithm uses  bits of memory in each step. After both passes have been processed the vertices of the equivalent instance can be relabeled such that they can be stored using  bits, i.e. the size of the instance is now . 
 
 Counting the size of  and the number of edges in  that are incident with a certain vertex  can be performed in  time. Similarly, verifying if an edge is in  and verifying if an edge in  is incident with a certain vertex  can be done in  time.
  \qed \end{proof}
  
\section{Proofs omitted from Section \ref{sec:multi}}
\subsection{Proof for Lemma \ref{lem:foolmfi}}

\begin{proof}
For every  we have  which is cycle free since  forms a tree rooted at  with leaves . Therefore  is chordal, i.e.  for every . Now let us consider pairs . We must show that either  or . Clearly  since . Let us assume w.l.o.g. that , i.e. there is a vertex . Then  since . Furthermore  since  because . Thus  contains an induced cycle on 4 vertices and is not chordal, i.e.,  and the lemma holds.
\end{proof}

\end{document}
