\begin{figure*}[t!]
\vspace{-2mm}
\begin{center}   \includegraphics[width=0.9\linewidth]{figures/Figure_VIBE_Cmp.png}
\end{center}
\vspace{-5mm}
\caption{Acceleration error curves for VIBE~\cite{kocabas2020vibe} results and our refined results. The right figure shows poses in consecutive timesteps corresponding to the reference images on the left.}
\label{vibe_cmp}
\vspace{-3mm}
\end{figure*}

\section{Application}  \label{sec:application}


Our HM-VAE provides a generalized motion prior that can be applicable in various tasks like 3D video pose estimation, motion interpolation and motion completion. In this section, we introduce the applications we consider and describe the effective strategy used for each application. We provide qualitative and quantitative results in the next section.

\paragraph{3D Video Pose Estimation.}
Our learned motion prior provides an effective strategy to refine video based pose estimations. Concretely, we take potentially noisy pose estimates as input to the encoder, then decode refined poses using the encoded latent vector.
Our HM-VAE is designed for a fixed window size of $T$ frames. In order to have our method process sequences of any length, we could simply partition the input sequence into windows of $T$ frames. However, with no overlap across the time windows, we observe that this may result in discontinuities. 
Therefore, we propose a sliding window strategy using center frames to process arbitrarily long sequences. Specifically, for each time window we process, we only update the pose of the center frame with the refined result and shift the time window one step. We take the $\frac{T}{2}$th frame $M_{\frac{T}{2}}$ as the refined final result, added to our final refined sequence $S$.
And the window is shifted by one timestep along the input motion sequence for processing the next window.
For each window size of pose sequences, we formally define the process as follows, where $W_2$ represents next window. 



\begin{align}
z &= Enc(N_1, ..., N_T), \\
M_1, ..., M_{\frac{T}{2}}, ..., M_T &= Dec(z), \\
S &= S \cup M_{\frac{T}{2}} \\
W_2 &= {N_2, ..., N_{T+1}}
\end{align}



\paragraph{Motion Interpolation and Completion.}
A common setup in motion synthesis is to generate motion sequences given a sparse set of keyframes, which we refer to as motion interpolation. Motion completion, on the other hand, focuses on synthesizing complete body motion from partial observations, e.g., completing the motion of the lower body by observing the upper body. For both motion interpolation and completion tasks, we simply utilize the decoder of HM-VAE to synthesize motion while searching for an optimal latent code to match the given observations (i.e., sparse keyframes or partial body motion). The optimization objective is to minimize the reconstruction error between the given observations and the corresponding decoded poses. We define the reconstruction objective as a combination of three terms including matching the joint rotations using both 6D rotation and rotation matrix representations and matching the joint positions after forward kinematics:
\begin{equation}
    L_{rec} = L_{6d} + L_{rot} + \lambda_1 L_{joints}
\end{equation}
Concretely, we perform optimization in two phases. Starting with randomly sampled latent vectors $z_l$ and $z_g$, in the first phase, we optimize for the latent vectors that minimize $L_{rec}$ as the only objective. The decoder parameters $\theta$ are fixed during this phase. In the next phase, we optimize for the decoder parameters $\theta$~\cite{yang2020deep} while keeping the latent vectors fixed. In this second phase, we introduce a regularization loss to constrain $\theta '$ and prevent it from deviating too much from the pre-trained parameters $\theta$. Thus, the optimization objective in the second phase becomes:  
\begin{equation}
    L_{opt} = L_{rec} + \lambda_2 ||\theta ' - \theta||^2
\end{equation}

\section{Experiments}
In this section, we first describe the dataset we use for training and evaluation. Then we showcase the results of applying our HM-VAE in the applications we introduced in the previous section. Finally, we perform an ablation study to validate the effectiveness of our overall approach. 

\paragraph{Dataset.}
We use the AMASS dataset~\cite{mahmood2019amass} for training HM-VAE. AMASS dataset is a large collection of 15 motion capture datasets with a unified data representation. The dataset has more than 40 hours of motion data and serves as a great testbed for motion modeling.  
We use the same validation and testing split introduced in VIBE~\cite{kocabas2020vibe}. 
For refining video based pose estimates, we use 3DPW~\cite{von2018recovering}, a 3D motion in the wild dataset, as our test set.  
For the motion interpolation task, we train our HM-VAE on the LAFAN1 dataset~\cite{harvey2020robust} to provide quantitative comparisons to the baseline methods. LAFAN1 consists of high-quality motion capture data with specific action types. We follow the data split proposed in \cite{harvey2020robust} and use subjects 1, 2, 3 and 4 as training, and subject 5 for testing.

\paragraph{Implementation Details.}
We use a batch size of $8$ for training. The KL divergence weight $\beta$ is set to $0.003$. Unless noted otherwise, we train HM-VAE with motion sequences of length 64. While training our HM-VAE, to prevent the learning dominated by either shallow or deep latent vector, we use similar strategy proposed in ~\cite{li2020progressive}. We first only train our model with deep latent vector, then start training both shallow and deep latent vectors after $50000$ iterations. 
For the motion interpolation experiments, we found our method converged at around $150$ iterations of optimization, with $50$ iterations for the first phase and $100$ iterations for the second phase.
For the motion completion experiments, we found our optimization converged at around $300$ iterations with $100$ iterations belonging to the first phase.





\subsection{Results}
\paragraph{3D Video Pose Estimation.}

In this section, we show that our model can be used to refine the results of off-the-shelf 3D video pose estimation methods. In order to adapt HM-VAE to different global rotations and frame rate among different datasets, we train our HM-VAE with data augmentation.
Our data augmentation consists of different frame rates and random global rotations.
Also, we use the HM-VAE model trained with a window size 8 in this application which we observe has a better reconstruction quality.


We show quantitative results in Table~\ref{table:pose-estimate-res} where we test our method with inputs obtained by both VIBE~\cite{kocabas2020vibe} and HumanDynamics (HD)~\cite{kanazawa2019learning}. We report errors using the same metrics as VIBE~\cite{kocabas2020vibe}. Specifically, we report the mean per joint position error with (PA-MPJPE) and without (MPJPE) the Procrustes-alignment, as well as the mean per joint acceleration and acceleration error. In Figure~\ref{vibe_cmp}, we show the acceleration error curves as well as example poses obtained for consecutive timesteps. Compared to current state-of-the-art methods, our refined motions have smaller acceleration errors. While previous approaches are prone to abrupt changes across consecutive poses as shown in Figure~\ref{vibe_cmp}, our model smooths out these noisy estimates. We refer the readers to the supplementary video for a detailed comparison.      


\begin{table}[t!]
\small
\begin{center}
\footnotesize{
\setlength{\tabcolsep}{5pt}
\begin{tabular}{@{}l||cccccc@{}} 
\hline
 & PA-MPJPE  & MPJPE & ACCEL& ACCER \\ \hline\hline
HD~\cite{kanazawa2019learning} & 72.17 & 115.97 & 14.96 & 14.73 \\ \hline
HD~\cite{kanazawa2019learning} w Prior & \textbf{71.39} & \textbf{113.90} & \textbf{5.21} & \textbf{8.36} \\ \hline
VIBE~\cite{kocabas2020vibe} & 56.56 & 93.59 & 27.12 & 27.99  \\ \hline
VIBE~\cite{kocabas2020vibe} w Prior & \textbf{55.84} & \textbf{92.43} & \textbf{6.03} & \textbf{9.15}  \\ \hline
\end{tabular}
}
\end{center}
\vspace{-4mm}
\caption{\small 3D Video Human Pose Estimation Results in 3DPW Testing Dataset. }
\label{table:pose-estimate-res}
\vspace{-4mm}
\end{table}




\begin{figure*}[h]
\begin{center}   \includegraphics[width=0.85\linewidth]{figures/Figure_local_trajectory.png}
\end{center}
\vspace{-4mm}
\caption{Local trajectory comparison for motion interpolation in AMASS data. From left to right is the trajectory when key frame interval is 30, 15, 5 respectively. The upper curves represent the left wrist, the lower curves represent the right ankle. The star symbol represents the starting point, the arrow symbol represent the position of key frames. Our results show similar moving patterns to ground truth, while Slerp differs a lot when key frame interval is large. }
\label{motion_interp_local_trajectory}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\includegraphics[width=0.8\textwidth]{figures/Figure_motion_interpolation.png}
\end{center}
\vspace{-4mm}
\caption{Global root trajectory comparison for motion interpolation in AMASS data. From left to right is the trajectory (in xy plane) when key frame interval is 30, 15, 5 respectively. The star symbol represents the starting point, the arrow symbol represents the position of key frames. 
}
\label{motion_interp_global_trajectory}
\vspace{-1mm}
\end{figure*}

\begin{figure}[t!]
\vspace{-2mm}
\begin{center}   \includegraphics[width=\linewidth]{figures/Figure_mesh_interpolation.png}
\end{center}
\vspace{-5mm}
\caption{Motion interpolation results in AMASS test data. The gray mesh shows key frame poses, the purple mesh show the generated poses. The interval between two key frames is 30 frames. The right figure shows the global trajectory comparison for this motion sequence.}
\label{motion_interp_mesh_vis}
\vspace{-4mm}
\end{figure}

\vspace{-4mm}
\paragraph{Motion Interpolation.}
In order to demonstrate the effectiveness of HM-VAE for the motion interpolation task, we compare it to appropriate baseline methods. Specifically, in order to interpolate local joint rotations, we use the standard spherical linear interpolation (Slerp). Since interpolation quality is directly related to the number of missing frames, we perform our evaluations in four settings where $5, 15, 30, 45$ frames are missing in each setting. Following the same setting as ~\cite{harvey2020robust}, given the starting $10$ frames and ending $1$ frame as key frames, we aim to generate the frames in-between. We show quantitative comparisons in terms of local pose estimation in Table~\ref{table:lafan_interpolation}. In addition to the metrics introduced before, we also report the global quaternion loss proposed by the original benchmark~\cite{harvey2020robust}. We show that our method outperforms the Slerp baseline quantitatively.
We also show that the performance achieved by our human motion prior is competitive against the in-betweening specific method from~\cite{harvey2020robust} in the global quaternion loss metric.
Please note that we use the LAFAN1 dataset for this evaluation to compare against the global quaternion errors directly reported by~\cite{harvey2020robust} since their code is not published and the authors were not able to run their model on our dataset.  
We also provide additional qualitative results in the AMASS dataset. We visualize local joint trajectories in Figure~\ref{motion_interp_local_trajectory} for a walking motion sequence.
Our results preserve the original motion patterns while Slerp fails to model the local motion when the interval between two key frames becomes large.
We further demonstrate global trajectory interpolation with our method and the alternatives.
Specifically, we use our global trajectory estimation module by providing the local motion predicted by our method as well as Slerp. In addition, we also define a simple baseline where we linearly interpolate the global root position of the sparse keyframes (lerp).  As shown in Figure~\ref{motion_interp_global_trajectory}, the trajectory estimated by our method more closely resembles the ground truth.      
We also show a mesh visualization result for motion interpolation in Figure~\ref{motion_interp_mesh_vis}. For more qualitative results, we encourage readers to check our accompanying video.   






\begin{figure*}[h]
\begin{center}
\vspace{-5mm}
\includegraphics[width=0.8\textwidth]{figures/Figure_motion_completion.png}
\end{center}
\vspace{-6mm}
\caption{Motion Completion Results. Given upper body joint rotation as optimization objective, the prior model can complete whole motion sequences.}
\label{motion_completion}
\vspace{-6mm}
\end{figure*}




\begin{table}[t!]
\small
\begin{center}
\footnotesize{
\setlength{\tabcolsep}{10pt}
\begin{tabular}{@{}l||cccccc@{}} 
\hline
 & 5  & 15 & 30 & 45 \\ \hline\hline
MPJPE-Slerp & 16.02 & 57.13 & 96.54 & 118.96 \\ \hline
MPJPE-Ours & \textbf{14.08} & \textbf{45.09} & \textbf{90.41} & \textbf{117.93} \\ \hline \hline
PAMPJPE-Slerp & 15.82 & 54.11 & 83.66 & 92.4 \\ \hline
PAMPJPE-Ours & \textbf{12.03} & \textbf{38.37} & \textbf{72.21} & \textbf{86.06} \\ \hline \hline
ACCEL-Slerp & 1.75 & 0.78 & 0.35 & 0.23 \\ \hline
ACCEL-Ours & 4.79 & 4.48 & 4.08 & 3.61 \\ \hline \hline
ACCER-Slerp & 5.98 & 5.97 & \textbf{6.05} & \textbf{6.06} \\ \hline
ACCER-Ours & \textbf{5.31} & \textbf{5.83} & 6.54 & 6.75 \\ \hline \hline
Global Quat-Slerp & 0.22 & 0.62 & 0.98 & 1.25 \\ \hline
Global Quat-\cite{harvey2020robust} & \textbf{0.17}  & \textbf{0.42} & \textbf{0.69} & \textbf{0.94} \\ \hline
Global Quat-Ours & 0.24 & 0.54 & 0.94 & 1.25 \\ \hline
\end{tabular}
}
\end{center}
\vspace{-5mm}
\caption{\small Quantitative Evaluation for Motion Interpolation in LAFAN1 Dataset.}
\label{table:lafan_interpolation}
\vspace{-6mm}
\end{table}

\vspace{-4mm}
\paragraph{Motion Completion.}
Given only upper body joint rotations as target, we aim to recover the complete body motion sequences.
For this experiment, we use motion sequences from the testing and validation split of the AMASS dataset.
As shown in Figure~\ref{motion_completion}, our approach is able to restore complete motions since the global latent space capture the correlations among different joints.
Therefore, the missing lower legs movement that matches the given upper body is retrieved from the learned latent space for human motion.


\subsection{Ablation Study}
\vspace{-2mm}
In order to motivate the design choices we made, we perform an ablation study where we compare our HM-VAE with a non-hierarchical motion VAE (M-VAE) and a VAE with only temporal convolution layers (TCN-VAE). The temporal convolution layers were used in training an autoencoder for motion processing~\cite{holden2015learning,holden2016deep}.
We compare our model to the alternatives in the task of motion reconstruction using the AMASS dataset. 
Specifically, for each testing sequence, we take the local joint rotations as input to the encoder and then decode the motion from the mean vector.
We measure the mean joint reconstruction error as shown in Table~\ref{table:motion_rec}.
Our HM-VAE model outperforms the M-VAE by a large margin in motion reconstruction evaluation.
And the model with skeleton-aware architecture has superior performance than its temporal convolution counterpart.
Therefore, we show that skeleton operations from the skeleton-aware architecture are important for modeling the human body structure in comparison to using standard temporal convolution.
Moreover, modeling a global and local motion latent spaces further improve the human motion modeling power of the skeleton-aware architecture. 
\\

\begin{table}[t!]
\small
\begin{center}
\footnotesize{
\setlength{\tabcolsep}{8pt}
\begin{tabular}{@{}l||cccccc@{}} 
\hline
 & PA-MPJPE  & MPJPE & ACCEL & ACCER \\ \hline\hline
TCN-VAE & 87.27 & 103.60 & 1.66 & 6.46 \\ \hline 
M-VAE & 59.71 & 74.34 & 2.36 & 6.15 \\ \hline
HM-VAE & \textbf{45.82} & \textbf{58.46} & 2.29 & \textbf{5.98} \\ \hline
\end{tabular}
}
\end{center}
\vspace{-5mm}
\caption{\small Motion Reconstruction Results in AMASS test data.}
\label{table:motion_rec}
\vspace{-6mm}
\end{table}


























