\documentclass{article}

\usepackage[a4paper, margin=3.3cm]{geometry}

\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{xspace}
\usepackage{subfig}
\usepackage{upgreek}
\usepackage{wrapfig}


\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\Delta}{\Updelta}

\arraycolsep=2.9pt
\def\arraystretch{1.0}

\newcommand{\TRUE}{\textsc{True}\xspace}
\newcommand{\FALSE}{\textsc{False}\xspace}

\newcommand{\indexing}{\textsc{Indexing}\xspace}
\newcommand{\equality}{\textsc{Equality}\xspace}
\newcommand{\disjointness}{\textsc{Disjointness}\xspace}

\newcommand{\SigmaP}{\ensuremath{\Sigma_{\textup{P}}}}
\newcommand{\SigmaT}{\ensuremath{\Sigma_{\textup{T}}}}
\newcommand{\local}{\ensuremath{\textsc{LocalPM}}}

\newcommand{\wildcard}{\ensuremath{\star}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{proviso*}{Proviso}


\title{Space Lower Bounds for Online Pattern Matching}
\author{
    Rapha\"{e}l Clifford,\thanks{~University of Bristol, Dept.\@ of Computer Science, Bristol, UK}
    \and Markus Jalsenius,\footnotemark[1]\\
    \and Ely Porat,\thanks{~Bar-Ilan University, Dept.\@ of Computer Science, Ramat-Gan, Israel}
    \and Benjamin Sach\footnotemark[1]}

\date{}

\begin{document}

\maketitle

\begin{abstract}
    We present space lower bounds for online pattern matching under a number of different distance measures. Given a pattern of length  and a text that arrives one character at a time, the online pattern matching problem is to report the distance between the pattern and a sliding window of the text as soon as the new character arrives. We require that the correct answer is given at each position with constant probability. We give  bit space lower bounds for , , , Hamming, edit and swap distances as well as for any algorithm that computes the cross-correlation/convolution. We then show a dichotomy between distance functions that have wildcard-like properties and those that do not. In the former case which includes, as an example, pattern matching with character classes, we give  bit space lower bounds. For other distance functions, we show that there exist space bounds of  and  bits. Finally we discuss space lower bounds for non-binary inputs and show how in some cases they can be improved.
\end{abstract}


\section{Introduction}
We combine existing results with new observations to present an overview of space lower bounds for online pattern matching.  Given a pattern that is provided in advance and a text that arrives one character at a time, the online pattern matching problem is to report the distance between the pattern and a sliding window of the text as soon as the new character arrives. In this formulation, the pattern is processed before the first text character arrives and once processed, the pattern is no longer available to the algorithm unless a copy is explicitly made.

This problem has recently gained a great deal of interest with breakthrough results
 given for exact matching and pattern matching under bounded
Hamming distance (-mismatch)~\cite{Porat:09}.
For both problems it was shown that space sublinear in the size of the pattern is sufficient to give the correct answer at every alignment with high probability.  These remarkable results immediately raise a number of significant unresolved questions. The first is for which other distance measures between strings might sublinear space randomised online algorithms be achievable and it is this question which we address here.

Our presentation is divided between what we term local and non-local online pattern matching problems. In the former case the distance function between a pattern  of length  and an -length substring of the text , starting at position , is defined by

where  and  are both binary operators. In Section~\ref{sec:addition} we show  bit space lower bounds for online pattern matching for the local problems of , , and Hamming distance as well as for any algorithm that computes the cross-correlation/convolution.

We then go on to show in Section~\ref{sec:wildcard} a  space dichotomy for local online pattern matching problems of the form 
where the range of  is . Where the distance function  has wildcard-like properties (qv.~Section~\ref{sec:wildcard}), we give an  space lower bound. Where it does not, we have  and  space bounds.  This implies, for example, that online pattern matching with character classes~\cite{LR:2009} requires linear space.

In Section~\ref{sec:otherbool} we go on to consider all eight possible binary Boolean associative operators and give a complete classification in terms of their known upper and lower space bounds.  One consequence is that determining if there is an exact ``non-match'', where the Hamming distance is the same as the pattern length, requires linear space in our online model. This bound also holds if, for example, only the parity of the Hamming distance is required.  In Section~\ref{sec:infty} we then show how our techniques can be used to give linear space lower bounds for   online pattern matching.  In Section~\ref{sec:nonbinary} we discuss a possible approach to space lower bounds for inputs with large alphabets, focussing on the Hamming distance problem. Finally, in Section~\ref{sec:non-local} we explore non-local problems and show  bit space lower bounds for both online edit and swap distance.


\section{Preliminaries and related work}

Let  and  denote the pattern and text alphabet, respectively. We say that  is \emph{text independent} with respect to the pattern  if the value of  is a constant independent of . We say that  is \emph{pattern independent} with respect to a pattern  if there is a function  such that  for all .

\begin{example}
    Let , ,  be the Boolean AND-operator and  be defined according to the table in Figure~\ref{fig:example-invalid}, where 1 is \TRUE and 0 is \FALSE. We can see that  is text independent with respect to the pattern  as it always outputs . It is also pattern independent with respect to  as  for all . In fact, for this particular definition of ,  is either text or pattern independent with respect to any pattern .
\begin{figure}[t]
        
        \caption{\label{fig:example-invalid}An example of  such that  is invalid (either text or pattern independent with respect to any pattern ).}
    \end{figure}
\end{example}

Suppose that  is text independent with respect to a pattern . Then any algorithm for  on  requires at most  space after preprocessing . If  is pattern independent with respect to  then  does not depend on the pattern and is outside the scope of this paper.

We say that  is \emph{invalid} if, for every pattern , it is either text or pattern independent with respect to .  is \emph{valid} if it is not invalid. The problem  in the previous example is therefore invalid. We will only consider from this point pattern matching problems  which are valid, and ignore patterns for which  is pattern or text independent.

Our focus is on online pattern matching algorithms which output correct answers with constant probability.  We are not aware of previous work that considers randomised lower bounds for this specific type of problem. There is however now a considerable literature on communication complexity and on streaming algorithms for single input streams, including those that process a sliding window of the input (see e.g.~\cite{DGIM:02})). This previous streaming work has typically focussed on deterministic or randomised bounds for finding approximate rather than exact solutions.  Quantum lower and classical upper bounds for the communication complexity of Hamming distance in more general models than we consider were given previously~\cite{HSZZ:06}. A linear lower bound for the randomised communication complexity of the inner product of two binary vectors is given in~\cite{CG:1985}.   The dichotomy presented in Section~\ref{sec:wildcard} and in particular the concept of  a matching relation that includes wildcard matching, although in a different setting and with different terminology, is similar to a time complexity dichotomy given previously by Muthukrishnan and Ramesh~\cite{MR:95}. On the topic of swap matching in Section~\ref{sec:non-local}, we note that in~\cite{AALLL:00}, the existence of a reduction for time rather than space, from Boolean convolutions to string matching with swaps is claimed without proof.


\section{Communication complexity problems}

Our results are based on reductions from various one-way randomised communication complexity problems with known lower bounds. We list the relevant problems below. In a one-way randomised communication model, only Alice can send messages to Bob and Bob must output the correct answer with probability at least . Note that the value  is inconsequential: any probability strictly greater than  can be amplified to a constant arbitrary close to 1. We assume private randomness.

\begin{definition}
    The \equality problem in one-way communication complexity is defined as follows. Alice has a string  and Bob has a string . Bob must determine whether . The communication complexity is  bits~\cite{Yao:79}.
\end{definition}

\begin{definition}
    \label{def:indexing}
    The \indexing problem in one-way communication complexity is defined as follows. Alice has a string  and Bob has an index . Bob must find . The problem is known to have an  bit lower bound (see~\cite{JKS:08} for an elementary proof).
\end{definition}


\section{Addition}\label{sec:addition}

In this section we consider the problem , where  is standard addition and the range of  is a subset of the integers. That is, the distance function is


\begin{theorem}
     requires  bits of space.
\end{theorem}
\begin{proof}
    Since  is not text independent, there must exist characters  and  such that . We reduce from \indexing: Alice has a string  and Bob has an index . Alice initialises a pattern matching algorithm  on the pattern  and feeds in her string . Then she sends the internal state of  to Bob, who feeds in  copies of the symbol . Let  be the output after those s. Bob then feeds in another . Let  be the output. If  then . If  then . If the probability of error per output is bounded by a constant , then the union bound for error on two outputs is , giving the \indexing problem an error probability of at most .
\end{proof}

\begin{corollary}
    \label{cor:standard-addition}
    Computing the ,  and Hamming distances, as well as the convolution, require  bits of space.
\end{corollary}


\section{Conjunction} \label{sec:wildcard}

In this section we consider , where  is the Boolean AND-operator and the range of  is  (where 0 denotes \FALSE and 1 denotes \TRUE). There are several natural pattern matching problems that fall under this category, for example, exact matching, matching with wildcards and exact matching with character classes.

The function  can be represented with a -matrix , where the rows and columns correspond to the symbols in  and , respectively. Thus, the entry . The  matrix in Figure~\ref{fig:bad-matrix} will play an important role, and we call it the \emph{wildcard matrix}.

\begin{figure}[t]
    ~
    \hfill
    \begin{minipage}[b]{0.5\linewidth}
        
        \vspace{1pt}
        \caption{\label{fig:bad-matrix}The wildcard matrix (left) and negated wildcard matrix (right).}
    \end{minipage}
    ~
    \hfill
    ~
    \begin{minipage}[b]{0.3\linewidth}
         

         \caption{\label{fig:and-linear} in the proof of Theorem~\ref{thm:and-linear}.\\~}
          \vspace{-18.25pt}
    \end{minipage}
    \hfill
    ~
\end{figure}

We say that  contains the wildcard matrix if it is a submatrix of  under some permutation of the rows and columns.

We demonstrate the following dichotomy for . If  contains the wildcard matrix, then  is solvable in  bits of space, otherwise it is solvable in  bits of space. The first class is equivalent to pattern matching with wildcards, and the second class is equivalent to exact matching. Note that both dichotomies are decidable due to the simple characteristic of the function .

\begin{theorem}
    \label{thm:and-linear}
    If  contains the wildcard matrix, then  requires  bits of space.
\end{theorem}
\begin{proof}
    Suppose that  ( represents a wildcard symbol) and  such that  is specified according to Figure~\ref{fig:and-linear}. We reduce from the \indexing problem, in which Alice has an -length bit string  and Bob has an index . Let the pattern  be the string . Let  be any algorithm that solves  on the pattern . Alice sends the internal state of  to Bob, who feeds the algorithm with the -length string that has the symbol  at every position except for at position  where the symbol is . The output is \TRUE iff .
\end{proof}



\noindent The following lemma will be useful for the next two theorems (see Figure~\ref{fig:identity}).

\begin{lemma}
    \label{lem:identity}
    Let  be the matrix obtained from  by first removing copies of identical rows and columns, keeping only rows and columns that are distinct in , and then removing any row or column that contains only zeros. If  does not contain the wildcard matrix, then  is the identity matrix, under some permutation of rows and columns.
\end{lemma}
\begin{proof}
    Suppose that  does not contain the wildcard matrix. Let  be obtained from  according to the statement of the lemma. We will show that every column and every row of  contains exactly one~1.

    First we show that every row of  must contain at least one~1. Suppose that some row  of  contains only 0s. Since zero-rows of  were removed and one copy of each column remains after the removal process, it is not possible that all columns in which row  is 1 were removed. We now show that  cannot contain a row  with two or more 1s. Without loss of generality, assume that there is a 1 in columns  and  of row . Since  does not contain a wildcard matrix, the elements of columns  and  must both be either 0 or 1 in every row. Thus, columns  and  are identical, and one of them must have been removed, contradicting the fact that there are two 1s in row  of . In order to show that every column of  contains exactly one 1, we use the exact same argument as for the rows. Thus,  is the identity matrix, under some permutation of rows and columns. (See Figure~\ref{fig:identity} for an illustration of the lemma).
\end{proof}

\begin{figure}[t]
       \vspace{-5pt}
       
        \caption{\label{fig:identity}An illustration of Lemma~\ref{lem:identity}.\\~}
\end{figure}

\begin{theorem}
    \label{thm:and-log}
    If  does not contain the wildcard matrix, then  requires  bits of space.
\end{theorem}
\begin{proof}
    We reduce from the \equality problem, where Alice has a string  and Bob has a bit string . Since  doesn't contain the wildcard matrix and as we only consider problems  that are valid, it follows from Lemma~\ref{lem:identity} that there must exist  and  such that  is according to Figure~\ref{fig:and-log}.
    Let  be the -length pattern obtained from  by replacing every 0 with  and every 1 with . The -length text  is obtained similarly from  by replacing every 0 with  and every 1 with . For any algorithm  that solves  on the pattern , Alice sends the internal state of  on pattern  to Bob, who feeds  with . The output is \TRUE iff .
\end{proof}

\begin{theorem}
    \label{lem:and-exact}
    If  does not contain the wildcard matrix, then  can be solved in  bits of space.
\end{theorem}
\begin{proof}
    We will describe an algorithm for solving  which uses the exact matching algorithm by Porat and Porat~\cite{Porat:09}, which runs in space  words, which is  bits of space (under the word-RAM model). In order to use the exact matching algorithm (as a ``black box'') we must ensure that we do not feed it with distinct symbols that are identical under . In other words, we can think of  specifying character classes, and for each class we want to use one representative symbol. We formalise this below.

    We make the very reasonable assumption that the alphabets  and  are both enumerable and that we can iterate through every symbol of  and , respectively, in no more than  bits of space. Let the order by which we iterate through the alphabets describe an ordering of the symbols in  and . We say that the symbol  is \emph{smaller} than  if  appears before  when iterating through . We use the same notation for the symbols of . We say that two symbols  are \emph{equivalent} if  for all . Similarly,  are equivalent if  for all . We define the \emph{smallest equivalent} symbol of  to be the symbol  such that  is equivalent to  and no other symbol equivalent to  is smaller than . The notion of smallest equivalent symbol is defined similarly on .

    Let  be the set of all symbols  such that the smallest equivalent symbol of  is  itself. We do not include any symbol  in  such that  for all . Similarly, let  be the set of all symbols  such that the smallest equivalent symbol of  is  itself. We do not include any symbol  in  such that  for all . By Lemma~\ref{lem:identity} we have that  on  and  is represented by an identity matrix under some permutation of the rows and columns. In the example of Figure~\ref{fig:identity},  and . We will ensure that we use the exact matching algorithm of~\cite{Porat:09} only on  and  (i.e., normal exact pattern matching).

    Given a symbol , we can find its smallest equivalent symbol by iterating through every symbol  and for each , we iterate through all  to check whether . Similarly we can find the smallest equivalent symbol of any symbol in .

    Let  be the pattern. We may assume that  does not contain a symbol  for which  for all . If it does, the output is always 0. Before we preprocess the pattern, we replace every symbol with its smallest equivalent symbol. Then we preprocess the pattern using the fingerprint technique described in~\cite{Porat:09}. Now we run the exact matching algorithm with the following additional step. When a new symbol  arrives, we replace it with its smallest equivalent symbol. The only caveat we must take care of is the situation when  for all . We can detect this case by iterating through the symbols of . As long as  is present in the last  characters of the stream, the output is zero. We use a flag to keep track of this.
\end{proof}

We now show how these results can be applied to a specific pattern matching problem that has not been considered in the online setting before. The pattern matching with character classes problem allows a set of characters to be defined for each position in the pattern~\cite{LR:2009}.  A character in the text matches a set at a pattern position if it is contained within it.  This is a generalisation of exact matching where each set would contain only one character. Using Theorems~\ref{thm:and-linear},~\ref{thm:and-log} and~\ref{lem:and-exact} we can determine precisely when this problem can and cannot be solved online in sublinear space.

\begin{corollary}
    Online pattern matching with character classes requires  bits of space in the worst case. However, where the character classes define a matching relation  which does not contain the wildcard matrix (see the example in Figure~\ref{fig:identity}),  bits suffice.
\end{corollary}

\begin{figure}[t]
        
        \caption{\label{fig:and-log} in the proof of Theorem~\ref{thm:and-log}.}
\end{figure}


\section{Other Boolean operators} \label{sec:otherbool}

In the previous section we demonstrated a dichotomy for , where  is the AND-operator. Here we will complete the classification of Boolean operators. There are eight associative Boolean operators :

\medskip
\noindent
\hfill
\textbf{1.} \TRUE \hfill
\textbf{2.} \FALSE \hfill
\textbf{3.}  \hfill
\textbf{4.}  \hfill
\textbf{5.}  \hfill
\textbf{6.}  \hfill
\textbf{7.}  \hfill
\textbf{8.}  \hfill

\medskip
The operators \TRUE and \FALSE are trivial; the output is either always \TRUE or \FALSE. The operator  is also easy; the output is always , where  is the last received symbol of the text stream.

The operator  is on the other hand more demanding. Here the output is , where  is the th last symbol received from the text stream. The pattern matching algorithm must therefore remember  received characters of the stream. More precisely, we see that  bits of space is necessary by reducing from the \indexing problem: Alice first feeds her array (text) into the pattern matching algorithm, for which  is a character that can distinguish between the characters of Alice's array. She then sends the internal state to Bob, who feeds in  symbols in order to determine the value at index  of Alice's array.

The OR-operator  is equivalent to  under De Morgan's laws: negate the outputs from  and negate the output from the pattern matching algorithm. Thus, the dichotomy for  applies to  as well, only that we characterise the classes with the wildcard matrix in which each element has been negated. This is called the negated wildcard matrix (see Figure~\ref{fig:bad-matrix}).

We now show that the equality operator ``'' requires  bits of space. First note that the output from the pattern matching algorithm is 0 if and only if  for an odd number of positions . For example, if  is the identity matrix,  gives us the parity of the Hamming distance.

Since  is valid, there are ,  such that  and . We reduce from the \indexing problem, where Alice has a string in  and Bob has an index . Alice initialises a pattern matching algorithm on the pattern  and feeds it with her string. She sends the internal state to Bob, who feeds the algorithm with  copies of the symbol . The first position of  is now aligned with the th character of Alice's string. Suppose the output from the algorithm is . Bob now feeds in another . Let  be the new output. If  then the character at position  of Alice's string must have been . If  then the character must have been .

The operator ``'' is similar to ``'' and also requires  bits of space. To see this, note that the output from the pattern matching algorithm is 0 if and only if  for an even number of positions . We may therefore prove the lower bound using a reduction from the \indexing problem similar to above.


\section{The  distance}\label{sec:infty}

In this section we consider the  distance problem which can be defined as , where  and  is the maximum of  and . In this section we assume that the pattern and text are integer valued. Here the distance function is the maximum  over all , that is


\begin{theorem}
    \label{thm:maximum}
    The  distance problem requires  bits of space.
\end{theorem}
\begin{proof}
    Let  and . Therefore  is specified according to Figure~\ref{fig:infinity}. Let  if ,
    otherwise . Therefore  contains the wildcard
    matrix and hence by Theorem~\ref{thm:and-linear},  requires  space.

    Let  be the distance under . If  then for all , , implying that  for all . Hence . If  then there exists a  such that , implying that  and hence . Therefore, if we can solve , we can solve .
\end{proof}

\begin{figure}[t]
    
    \caption{\label{fig:infinity} and  in the proof of Theorem~\ref{thm:maximum}.}
\end{figure}


\section{Non-binary alphabets}\label{sec:nonbinary}

The space lower bounds we have given so far have been either  or  bits. When the pattern or text alphabet is drawn from a large universe, the question arises as to whether even more space is required to perform online pattern matching.  We show by way of another different reduction a method that may be applicable to a wider range of pattern matching problems than we consider here. Our approach is to show a reduction from  the communication complexity problem \disjointness~\cite{Kushilevitz:97} to the Hamming distance problem.  In \disjointness Alice and Bob both have sets of  elements each chosen from a universe of size  and Bob wants to determine if their intersection is empty. The lower bound for the space complexity of the Hamming distance problem will then be determined by lower bounds for the one-way randomised communication complexity of the \disjointness problem with private coins.  A result regarded as folklore shows that this complexity is  when  is  ~\cite{Nisan:2011,P-folklore:09}. This in turn implies a superlinear lower bound for the space complexity of the online Hamming distance problem with large alphabets.

For an integer , we write  to denote the set . Alice has a set  and Bob has a set , and . The reduction performs the following steps. We assume for the moment that Alice and Bob both have a shared source of randomness and show later how this assumption can be removed.

\begin{enumerate}
 \item Alice creates a pairwise independent hash function , for some constant integer  and creates a pattern  of length  where each element is initialised to be some unique symbol \notin [U]P[h(x)] = xx\in AAPPBTcm\ for the initialisation of the text.
 \item Bob feeds the Hamming distance algorithm with the whole text . Bob concludes that  and  are disjoint iff the output is .
\end{enumerate}

\begin{theorem}
    Any randomised algorithm for Hamming distance where the symbols are chosen from a universe of size  uses  bits of space.
\end{theorem}
\begin{proof}
    Considering the reduction above, if  and  are disjoint, then a deterministic Hamming distance algorithm will always output . If  and  are not disjoint then a necessary condition for a deterministic Hamming distance algorithm to output  is if at least two elements are hashed to the same location by either Alice or Bob. We can see that the probability of incorrectly outputting  is maximised when  and  share exactly one element. Therefore, suppose that . The element  is hashed to position . By the union bound and the pairwise independence of the hash function, the probability that some other element in either  or  is mapped to  is at most . If we assume our randomised Hamming distance algorithm is correct with probability at least , then the overall process falsely reports disjointness with probability at most  (union bound).  The space complexity of Hamming distance is therefore lower bounded by the communication complexity of the disjointness problem if Alice and Bob have a shared source of random bits to select their common hash function. By Newman's Theorem~\cite{Newman:1991} the cost of transforming the protocol to work with only private coins is at most an additive  factor in the asymptotic complexity. Assuming that  grows polynomially in  and so  is , the overall lower bound for the space complexity of the Hamming distance problem is therefore .  To finish the proof for larger , we observe first that a lower bound for smaller universes must still hold for larger ones. The final additive  term is derived by simply setting  and follows directly from the randomised lower bound for \equality. Therefore the overall lower bound is  as required.
\end{proof}


\section{Non-local pattern matching} \label{sec:non-local}

So far we have focused only on local pattern matching where each position in the alignment contributes to the distance independently of the other positions. Here we take a brief look at space lower bounds for two non-local distance measures: edit distance and swap matching.

In online pattern matching, we define the \emph{edit distance}  as the minimum number of single character edit operations (insert, delete and replace) required to transform  into the last  characters of the streamed text. This implies that the number of insertions and deletions are equal.

We show that for binary , the online edit distance problem requires  bits of space. For non-binary inputs there is a reduction from the Hamming distance problem~\cite{ApproxEdit:04}. The reduction we give covers the binary alphabet case as well and follows directly from \indexing, where Alice has a string  and Bob has an index . Alice initialises a pattern matching algorithm on the pattern  and sends the internal state to Bob, who first feeds in  zeros. Let  be the output and note that  is the number of ones in . Bob then feeds in the -length string that consists of zeros at every position except for at position  where it is one. Let  be the output. Bob can now decide the value of  by comparing  with :  if , and  if . The probability of error is therefore upper bounded by the union bound on  and  being wrong.

Given a string , a \emph{swap} at position  means that the characters  and  swap positions. We say there is a \emph{swap match} if and only if the pattern  can be transformed into the last  characters of the streamed text through a set of swaps. Each  is swapped at most once.

We show that the online swap distance problem requires  space. Our proof is based on the techniques we have presented in this paper.  Specifically, we demonstrate a reduction from  where  contains the wildcard matrix, hence the space lower bound is . Suppose we have  as in Figure~\ref{fig:swap}. Let  and . From  we obtain  such that every  in  is replaced with  and every  is replaced with . When we receive characters from the text, we replace  with  and  with . It follows, under the transformation of the symbols, that there is a swap match if and only if  outputs \TRUE for the original (non-transformed) strings. To see this, note that both  and , under the transformation, swap match , but  does not swap match  (see Figure~\ref{fig:swap}). The transformation of the symbols does not allow swaps between adjacent characters; every possible swap will take place ``within'' the binary encoding of a symbol. Thus, a swap match directly corresponds to a match under .
\begin{figure}[t]
    
    \caption{\label{fig:swap} and alignments under swaps.}
\end{figure}


\section{Open problems}

We have considered space lower bounds and discussed how they can be derived from known communication complexity lower bounds.  Upper bounds can also be directly derived from existing online pattern matching algorithms.  For all the problems we have discussed there is at most a log factor gap between these upper and lower bounds.  However, where the known lower bound is sublinear, as is the case for exact matching for example, this gap may still be considered significant. Further, for bounded Hamming distance where the distance is only to be given if it is at most some constant , the best known randomised online space upper bound is ~\cite{Porat:09}).  The best known lower bound, on the other hand, is very different at ~\cite{HSZZ:06}.  Further, it is known that the lower bounds can not be increased to match the known upper bounds using the one-way communication complexity of the functions between two strings of the same length. Either more space efficient algorithms exist for these problems or novel techniques will be needed to improve the lower bounds.

\bibliographystyle{custom}
\bibliography{bib-latest}

\end{document}
