\documentclass{article}







\usepackage[final]{neurips_2021}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         



\usepackage{amsmath,amsfonts,bm}

\def\pNMLSingle{\underset{\theta \in \Theta}{\textit{max }} p_\theta (y|z^{N}, x, y)}
\def\thetagenie{\hat{\theta}(\mathcal{D}_N;x,y)}
\def\probthetagenie{p_{\thetagenie}(y|x)}
\def\probthetagenietag{p_{\thetagenietag}(y'|x)}
\def\pNMLSingle{\underset{\theta \in \Theta}{\textit{max }} p_\theta (y|z^{N}, x, y)}
\def\thetagenie{\hat{\theta}(\mathcal{D}_N;x,y)}
\def\thetagenietag{\hat{\theta}(\mathcal{D}_N;x,y')}
\def\thetageniein#1{\hat{\theta}(\mathcal{D}_N;x,y=#1)}
\def\probthetagenie{p_{\thetagenie}(y|x)}
\DeclareMathOperator{\diag}{diag}  
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\Theoref#1{Theorem~\ref{#1}}
\def\appref#1{appendix~\ref{#1}}
\def\Appref#1{Appendix~\ref{#1}}
\def\lemmaref#1{lemma~\ref{#1}}
\def\Lemmaref#1{Lemma~\ref{#1}}
\def\tableref#1{table~\ref{#1}}
\def\Tableref#1{Table~\ref{#1}}


\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{(\ref{#1})}
\def\Eqref#1{(\ref{#1})}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{url}
\usepackage{amsthm}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\usepackage{placeins}
\usepackage{array}\usepackage{lipsum} 
\usepackage{enumitem}

\usepackage{caption}        \usepackage{subcaption} 

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newcommand\myalgref{Algorithm~\ref}
\newcommand{\ignore}[1]{}

\usepackage{soul}
\newcommand{\minisection}[1]{\vspace{2mm}\noindent{\textbf{#1.}}}
\newcommand\tal[1]{\textcolor{red}{#1}}
\newcommand\talrm[1]{\tal{\st{#1}}}
\newcommand\koby[1]{\textcolor{blue}{#1}}










\title{Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection}



\author{Koby Bibas \\
    School of Electrical Engineering\\
    Tel Aviv University\\
    \texttt{kobybibas@gmail.com} \\
\And
    Meir Feder \\
    School of Electrical Engineering\\
    Tel Aviv University\\
    \texttt{meir@eng.tau.ac.il} \\
   \AND
   Tal Hassner \\
   Facebook AI \\
   \texttt{talhassner@gmail.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Detecting out-of-distribution (OOD) samples is vital for developing machine learning based models for critical safety systems. Common approaches for OOD detection assume access to some OOD samples during training which may not be available in a real-life scenario. Instead, we utilize the {\em predictive normalized maximum likelihood} (pNML) learner, in which no assumptions are made on the tested input. We derive an explicit expression of the pNML and its generalization error, denoted as the {\em regret}, for a single layer neural network (NN). We show that this learner generalizes well when (i) the test vector resides in a subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or (ii) the test sample is far from the decision boundary. Furthermore, we describe how to efficiently apply the derived pNML regret to any pretrained deep NN, by employing the explicit pNML for the last layer, followed by the softmax function. Applying the derived regret to deep NN requires neither additional tunable parameters nor extra data. We extensively evaluate our approach on 74 OOD detection benchmarks using DenseNet-100, ResNet-34, and WideResNet-40 models trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30 showing a significant improvement of up to 15.6\% over recent leading methods.
\end{abstract}


\section{Introduction}
\label{sec:intro}

An important concern that limits the adoption of deep neural networks (DNN) in critical safety systems is how to assess our confidence in their predictions, i.e, quantifying their \textit{generalization capability}~\citep{DBLP:conf/bmvc/KaufmanBBCH19,willers2020safety}.
Take, for instance, a machine learning model for medical diagnosis~\citep{bibas2021learning}. 
It may produce (wrong) diagnoses in the presence of test inputs that are different from the training set rather than flagging them for human intervention~\citep{singh2021uncertainty}. 
Detecting such unexpected inputs had been formulated as the out-of-distribution (OOD) detection task~\citep{hendrycks17baseline}, as flagging test inputs that lie outside the training classes, i.e., are not \textit{in-distribution} (IND).

Previous learning methods that designed to offer such generalization measures, include VC-dimension~\citep{zhong2017recovery,vapnik2015uniform} and norm based bounds~\citep{DBLP:conf/iclr/NeyshaburBS18,DBLP:conf/nips/BartlettFT17}. As a whole, these methods characterized the generalization ability based on the properties of the parameters. However, they do not consider the test sample that is presented to the model~\citep{DBLP:conf/iclr/JiangNMKB20}, which makes them useless for OOD detection.
Other approaches build heuristics over the \textit{empirical risk minimization} (ERM) learner, by post-processing the model output~\citep{gram} or modifying the training process~\citep{PAPADOPOULOS2021138,vyas2018out}. Regardless of the approach, these methods choose the learner that minimizes the loss over the \textit{training set}. This may lead to a large generalization error because the ERM estimate may be wrong on unexpected inputs; especially with large models such as DNN~\citep{belkin2019reconciling}.

To produce a useful generalization measure, we exploit the \textit{individual setting} framework~\citep{merhav1998universal}. 
In the individual setting, there is no assumption about how the training and the test data are generated, nor about their probabilistic relationship.
Moreover, the relationship between the labels and data can be deterministic and may therefore be determined by an adversary.
The generalization error in this setting is often referred to as the \textit{regret}~\citep{merhav1998universal}.
This regret is defined by the log-loss difference between a learner and the \textit{genie}: a learner that knows the specific test label, yet is constrained to use an explanation from a set of possible models.
The individual setting is the most general framework and so the result holds for a wide range of scenarios. Specifically, the result holds to OOD detection where the distribution of the OOD inputs is unknown.

The pNML learner~\citep{fogel2018universal} was proposed as the min-max solution of the regret, where the minimum is over the learner choice and the maximum is for any possible test label value.
Intuitively, the pNML assigns a probability for a potential outcome as follows: Add the test sample to the training set with an arbitrary label, find the ERM solution of this new set, and take the probability it gives to the assumed label. 
Follow this procedure for every label and normalize to get a valid probability assignment.
The pNML was developed before for linear regression~\citep{bibas2019new} and was evaluated empirically for DNN~\citep{fu2021offline,bibas2019deep}.

We derive an analytical solution of the pNML learner and its generalization error (the regret) for a single layer NN.
We analyze the derived regret and show it obtains low values when the test input either (i) lies in a subspace spanned by the eigenvectors associated with the large eigenvalues of the training data empirical correlation matrix or (ii) is located far from the decision boundary.
Crucially, although our analysis focuses on a single layer NN, our results are applicable to the last layer of DNNs {\em without changing the network architecture or the training process}:
We treat the pretrained DNN as a feature extractor with the last layer as a single layer NN classifier.
We can therefore show the usage of the pNML regret as a confidence score for the OOD detection task.


To summarize, we make the following contributions.
\begin{enumerate}
    \item \textbf{Analytical derivation of the pNML regret.} We derive an analytical expression of the pNML regret, which is associated with the generalization error, for a single layer NN. 
    \item \textbf{Analyzing the pNML regret.}
     We explore the pNML regret characteristics as a function of the test sample data, training data, and the corresponding ERM prediction. We provide a visualization on low dimensional data and demonstrate the situations in which the pNML regret is low and the prediction can be trusted.
    \item \textbf{DNN adaptation.} We propose an adaptation of the derived pNML regret to {\em any} pretrained DNN that uses the softmax function with neither additional parameters nor extra data.
\end{enumerate}

Applying our pNML regret to a pretrained DNN does not require additional data, it is efficient and can be easily implemented.
The derived regret is theoretically justified for OOD detection since it is the individual setting solution for which we do not require any knowledge on the test input distribution.
Our evaluation includes 74 IND-OOD detection benchmarks using DenseNet-BC-100, ResNet-34, and WideResNet-40 trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30.
Our approach outperforms leading methods in nearly all 74 OOD detection benchmarks up to a remarkable \% 




\section{Related work} 
\label{sec:related_work}

\minisection{OOD detection} 
Many recent work use additional OOD set in training~\citep{DBLP:conf/nips/MalininG18,DBLP:conf/iclr/HendrycksMD19,DBLP:conf/nips/NandyHL20,DBLP:conf/aaai/MohseniPYW20}.
A well-known method for OOD detection, named \textit{ODIN}~\citep{liang2017enhancing}, manipulates the input image based on the gradient of the loss function. 
This input manipulation increases the margin between the maximum probability of images with known classes and images with unknown objects.
However, the manipulation strength is defined by OOD data, which might be unavailable or different from the OOD samples in inference.




Different approaches propose to manipulate the loss function in the training phase.
\citet{lee2018simple} proposed to use the Mahalanobis distance between the test input representations and the class conditional distribution in the intermediate layers to train a logistic regression classifier to determine if the input sample is OOD.
The \textit{Energy} method~\citep{liu2020energy} adds a cost function to the training phase to shape the energy surface explicitly for OOD detection.
\citet{PAPADOPOULOS2021138} suggested the \textit{outlier exposure with confidence control} (OECC) method in which a regularization term is added to the loss function such that the model produces a uniform distribution for OOD samples.


All mentioned methods require manipulating the DNN architecture or adding additional data.
The \textit{Baseline} method~\citep{hendrycks17baseline} is one of the earliest approaches designed to identify whether a test input is IND and does not require changing the pretrained model. This method uses the maximum probability of the DNN output as the OOD score.
The \emph{Gram} method~\citep{gram} detects OODs based on feature representations obtained in intermediate layers. 
Given a test sample, the Gram matrices of the test sample features are compared with those of the training samples known to belong to the estimated class of the test sample. 
However, this method requires IND validation set and does not have a theoretical motivation.

Other work suggested using Bayesian techniques to estimate the prediction confidence~\citep{DBLP:conf/icml/GalG16,DBLP:conf/nips/Lakshminarayanan17,DBLP:conf/icml/AmersfoortSTG20}. However, the Bayesian techniques add extensive compute overhead to the prediction.

\minisection{The pNML learner}
The pNML learner is the min-max solution of the supervised batch learning in the individual setting~\citep{fogel2018universal}. For sequential prediction it is termed the conditional normalized maximum likelihood~\citep{rissanen2007conditional,roos2008sequentially}.

Several methods deal with obtaining the pNML learner for different hypothesis sets.
\citet{bibas2019new} and \citet{bibas2021predictive} showed the pNML solution for linear regression. \citet{rosas2020learning} proposed an NML based decision strategy for supervised classification problems and showed it attains heuristic PAC learning.
\citet{fu2021offline} used the pNML for model optimization based on learning a density function by discretizing the space and fitting a distinct model for each value.

For the DNN hypothesis set,
\citet{bibas2019deep} estimated the pNML distribution with DNN by fine-tuning the last layers of the network for every test input and label combination. This approach is computationally expensive since training is needed for every test input.
\citet{zhou2020amortized} suggested a way to accelerate the pNML computation in DNN by using approximate Bayesian inference techniques to produce a tractable approximation to the pNML.
\citet{pesso2021utilizing} used the pNML with adversarial target attack as a defense mechanism against adversarial perturbation.

\section{Notation and preliminaries} 
\label{sec:preliminaries}
In the supervised machine learning scenario, a training set consisting of  pairs of examples is given

where  is the -th data instance and  is its corresponding label.
The goal of a learner is to predict the unknown test label  given a new test data  by assigning probability distribution  to the unknown label. 
The performance is evaluated using the log-loss function


For the problem to be well-posed, we must make further assumptions on the class of possible models or {\em hypothesis set} that is used to find the relation between  and .
Denote  as a general index set, this class is a set of conditional probability distributions 

The ERM is the learner from the hypothesis set that attains the minimal log-loss on the training set.

\minisection{The individual setting}
An additional assumption required to solve the problem is related to how the data and the labels are generated.
In this work we consider the individual setting \citep{merhav1998universal,fogel2018universal,bibas2019new,bibas2019deep},
where the data and labels, both in the training and test, are specific individual quantities: We do not assume any probabilistic relationship between them, the labels may even be assigned in an adversarial manner.  

\minisection{The genie} 
In the individual setting the goal is to compete with a reference learner, a genie, that has to following properties: (i) knows the test label value, (ii) is restricted to use a model from the given hypotheses set , and (iii) does not know which of the samples is the test. 
This reference learner then chooses a model that attains the minimum loss over the training set and the test sample

The regret is the log-loss difference between a learner  and this genie:
 


\begin{theorem}[\citet{fogel2018universal}]
\label{theroem:pnml}
The universal learner, denoted as the pNML, minimizes the regret for the worst case test label 

The pNML probability assignment and regret are

\end{theorem}
\begin{proof}
The regret is equal for all choices of . If we consider a different probability assignment, it should assign a smaller probability for at least one of the outcomes. If the true label is one of those outcomes it will lead to a higher regret. For more information see \citet{fogel2018universal}.
\end{proof}
The pNML regret is associated with the model complexity~\citep{zhang2012model}. This complexity measure formalizes the intuition that a model that fits almost every data pattern very well would be much more complex than a model that provides a relatively good fit to a small set of data.
Thus, the pNML incorporates a trade-off between goodness of fit and model complexity as measured by the regret.


\minisection{Online update of a neural network}
Let  and  be the data and label matrices of  training points respectively
 
such that the number of input features and model outputs are  and  respectively.
Denote  as the Moore–Penrose inverse of the data matrix

 and  as the activation and inverse activation functions, and  as the learnable parameters.
The ERM, that minimizes the training set mean squared error (MSE), is

Recently, \citet{zhuang2020training} suggested a recursive formulation of the DNN weights. Using their scheme only one training sample is processed at a time:
Denote the projection of a sample  onto the orthogonal subspace of the training set correlation matrix as

the update rule when receiving a new training sample with data 
 with label  is

In their paper, \citet{zhuang2020training} applied this formulation for a DNN, layer by layer.


\section{The pNML for a single layer NN}
\label{sec:pnml_regret}

Intuitively, the pNML as stated in~\eqref{eq:pNML} can be described as follows: To assign a probability for a potential outcome, (i) add it to the training set with an arbitrary label, (ii) find the best-suited model, and (iii) take the probability it gives to the assumed label. 
Follow this procedure for every label and normalize to get a valid probability assignment. Use the log normalization factor as the confidence measure.
This method can be extended to any general learning procedure that generates a prediction based on a training set. 
One such method is a single layer NN.

A single layer NN maps an input  using the softmax function to a probability vector which represents the probability assignment to one of  classes

To align with the recursive formulation  of~\eqref{eq:update_nn}, the label  is a one-hot vector with  elements and is represented by a row vector in the standard base  (the i-th element is 1 and all others are 0).
 stands for the true test label such that  is the one-hot vector of the true label.
Also, the learnable parameters  are the columns of the parameter matrix of~\eqref{eq:nn_solution}.

We would like that the probability assignment of the true label to be 1. We define the following MSE minimization objective

Minimizing this MSE objective is equivalent to constrain the genie to the following Gaussian family

and minimizing the log loss with respect to it.
More details are in \appref{appendix:logloss}.
The inverse of the activation function is


To compute the genie, we assume that the label of the test sample is known and add it to the training set. We fit the model by optimizing the learnable parameters to minimize the loss of this new set.
\begin{lemma} \label{lemma:genie}
Given test data  with label , the genie prediction of the true label is 

where  is the probability assignment of the ERM model of the label , and  is as defined in \eqref{eq:update_nn}.
\end{lemma}
\begin{proof}
Using~\eqref{eq:update_nn}, the probability assignment of the genie can be written as follows.

The genie knows the true test label . The inverse activation function can be written as 
 where .
The simplified numerator is

Substituting to \eqref{eq:lemma_genie}
and divide the numerator and denominator by  provides the result.
\end{proof}

The true test label is not available to a legit learner. Therefore, in the pNML process, every possible label is taken into account. The pNML regret is the logarithm of the sum of models' prediction, each one trained with a different test label value.




\begin{theorem}
Denote  as the ERM prediction of label , the pNML regret of a single layer NN is

\end{theorem}
\begin{proof}
The normalization factor is the sum of the probabilities assignment of models that were trained with a specific value of the test sample
.
Using \Theoref{theroem:pnml}, the log normalization factor is the pNML regret. 
With \lemmaref{lemma:genie}, we get the explicit expression.
\end{proof}

The pNML probability assignment of label  is the probability assignment of a model that was trained with that label divided by the normalization factor  .

Let  and  be the -th eigenvector and eigenvalue of the training set data matrix  such that for , the quantity  is

We make the following remarks.
\begin{enumerate}
\item 
If the test sample  lies in the subspace spanned by the eigenvectors with large eigenvalues,  is small and the corresponding regret is low
.
In this case, the pNML prediction is similar to the genie and can be trusted.
\item
Test input that resides is in the subspace that corresponds to the small eigenvalues produces  and a large regret is obtained 
.
The prediction for this test sample cannot be trusted. In~\secref{sec:experiments} we show that in this situation the test sample can be classified as an OOD sample.
\item
As the training set size () increases  becomes smaller and the regret decreases.
\item If the test sample is far from the decision boundary, the ERM assigns to one of the labels probability 1. In this case, the regret is 0 no matter in which subspace the test vector lies.
\end{enumerate}


\begin{figure}[tbh]
    \centering
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/low_dim_data_erm_prob_seperable=True.pdf}
    \vspace{-8mm}
    \caption{ERM probability for the separable split\label{fig:syntetic_erm_prob}}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/low_dim_data_regret_seperable=True.pdf}
    \vspace{-8mm}
    \caption{pNML regret for the separable split\ \label{fig:syntetic_regret}}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/low_dim_data_erm_prob_seperable=False.pdf}
    \vspace{-8mm}
    \caption{ERM probability for the inseparable split\  \label{fig:syntetic_erm_prob_inseparable}}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/low_dim_data_regret_seperable=False.pdf}
    \vspace{-8mm}
    \caption{pNML regret for the inseparable split\ \label{fig:syntetic_regret_inseparable}}
\end{subfigure}
\caption{A single layer NN fitted to low dimensional data (the Iris flower set~\citep{fisher1936use}).
(a) and (c) show the ERM probability assignment of class  for a separable split and inseparable split respectively.
The derived pNML regret for the separable split is shown in (b) and for the inseparable split is shown in (d). The training data of class  and  are marked with red circles and red crosses respectively. 
Low regret is associated with the training data surroundings.
See \secref{sec:low_dim_set}.}
\label{fig:syntetic_sumylation}
\end{figure}

\subsection{The pNML regret characteristics using a low-dimensional dataset} \label{sec:low_dim_set}
We demonstrate the characteristics of the derived regret and show in what situations the prediction of the test sample can be trusted.
To visualize the pNML regret on a low-dimensional dataset, we use the Iris flower data set~\citep{fisher1936use}. We utilize two classes and two features and name them , , and feature 1, feature 2 respectively.

\Figref{fig:syntetic_erm_prob} shows the ERM probability assignment of class  for a single layer NN that was fitted to the training data, which are marked in red.
At the top left and bottom right, the model predicts with high probability that a sample from these areas belongs to class  and  respectively.

\Figref{fig:syntetic_regret} presents the analytical pNML regret. 
At the upper left and lower right, the regret is low: Although there are no training samples there, these regions are far from the decision boundary, adding one sample would not alter the probability assignment significantly, thus the pNML prediction is close to the genie.
At the top right and bottom left, there are no training points therefore the regret is relatively high and the confidence in the prediction is low.
In \secref{sec:experiments}, we show that these test samples, which are associated with high regret, can be classified as OOD samples.

In addition, we visualize the regret for overlapping classes.
In \figref{fig:syntetic_erm_prob_inseparable}, the ERM probability assignment for inseparable class split is shown.
The ERM probability is lower than 0.7 for all test feature values. \Figref{fig:syntetic_regret_inseparable} presents the corresponding pNML regret. The pNML regret is small in the training data surroundings (including the mixed label area). The regret is large in areas where the training data is absent, as in the figure edges.



\subsection{Deep neural network adaptation}
\label{sec:dnn_adaptation}
In previous sections, we derived the pNML for a single layer NN. 
We next show that our derivations can, in fact, be applied to {\em any pretrained NN}, without requiring additional parameters or extra data. 

First extract the embeddings of the training set:
Denote  as the embedding creation (feature extraction) using a pretrained ERM model, propagate the training samples through the DNN up to the last layer and compute the inverse of the data matrix .
Then, given a specific test example , extract its embedding  and its ERM probability assignment .
Finally calculate the regret as described in \Theoref{theroem:pnml} using the training set and test embedding vectors.



We empirically found that norms of OOD embeddings are lower than those of IND samples.
The regret depends on the norm of the test sample: For , the regret of  is lower than the regret of .
Hence, we normalize all embeddings (training, IND, and OOD) to have  norms equal to 1.0.

Samples with a high regret value are considered samples with a large distance from the genie, the learner that knows the true label, and therefore the prediction cannot be trusted. Our proposed method utilizes this regret value to determine whether a test data item represents a known or an unknown.





\section{Application to out-of-distribution detection} 
\label{sec:experiments}
We rigorously test the effectiveness of the pNML regret for OOD detection\footnote{Code is available in \url{https://github.com/kobybibas/pnml_ood_detection}}.
The motivation for using the individual setting and the pNML as its solution for OOD detection is that in the individual setting there is no assumption on the way the data is generated. The absence of assumption means that the result holds for a wide range of scenarios (PAC, stochastic, and even adversary) and specifically to OOD detection, where the OOD samples are drawn from an unknown distribution.


\subsection{Experimental setup} 
\label{sec:setup}
We follow the standard experimental setup~\citep{liu2020energy,gram,lee2018simple}.
All the assets we used are open-sourced with either Apache-2.0 License or Attribution-NonCommercial 4.0 International licenses.
We ran all experiments on NVIDIA K80 GPU.

\minisection{IND sets}
For datasets that represent known classes, we use CIFAR-100, CIFAR-10~\citep{krizhevsky2014cifar} and  SVHN~\citep{netzer2011reading}. These sets contain RGB images with 32x32 pixels.
In addition, to evaluate higher resolution images, we use ImageNet-30 set~\citep{DBLP:conf/nips/HendrycksMKS19}.

\begin{table}[tb]
\centering
\small
\caption{OOD detection for DenseNet-BC-100 model, comparing our pNML-based approach to leading methods. Results reported using AUROC show our method enhances previous work up to 69.4\%, 49.4\%, 3.1\%, and 2.2\%. See \secref{sec:results} for more details.}
\vspace{0.1mm}
\label{tab:auroc_densenet}
\begin{tabular}{clcccc}
\toprule
IND & OOD &        Baseline/+pNML &            ODIN/+pNML &            Gram/+pNML &            OECC/+pNML \\
\midrule
\multirow{8}{*}{CIFAR-100} & iSUN &  69.7 / \textbf{96.4} &  84.5 / \textbf{96.7} &  99.0 / \textbf{99.5} &  99.2 / \textbf{99.5} \\
     & LSUN (R) &  70.8 / \textbf{96.6} &  86.0 / \textbf{96.9} &  99.3 / \textbf{99.7} &  99.4 / \textbf{99.6} \\
     & LSUN (C) &  80.1 / \textbf{93.1} &  91.5 / \textbf{93.1} &  91.4 / \textbf{94.5} &  93.9 / \textbf{96.1} \\
     & Imagenet (R) &  71.6 / \textbf{97.4} &  85.5 / \textbf{97.6} &  99.0 / \textbf{99.5} &  99.0 / \textbf{99.5} \\
     & Imagenet (C) &  76.2 / \textbf{95.7} &  88.8 / \textbf{96.0} &  97.7 / \textbf{98.7} &  98.2 / \textbf{99.0} \\
     & Uniform &   43.3 / \textbf{100} &   83.7 / \textbf{100} &    100 / \textbf{100} &   99.9 / \textbf{100} \\
     & Gaussian &   30.6 / \textbf{100} &   50.6 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  82.6 / \textbf{96.2} &  92.5 / \textbf{96.2} &  97.3 / \textbf{98.4} &  97.0 / \textbf{97.5} \\
\midrule
\multirow{8}{*}{CIFAR-10} & iSUN &  94.8 / \textbf{98.7} &  98.9 / \textbf{98.9} &   99.8 / \textbf{100} &   99.9 / \textbf{100} \\
     & LSUN (R) &  95.5 / \textbf{98.9} &  99.2 / \textbf{99.2} &   99.9 / \textbf{100} &   99.9 / \textbf{100} \\
     & LSUN (C) &  93.0 / \textbf{96.4} &  95.8 / \textbf{96.4} &  97.5 / \textbf{98.7} &  98.9 / \textbf{99.9} \\
     & Imagenet (R) &  94.1 / \textbf{98.8} &  98.5 / \textbf{99.0} &  99.7 / \textbf{99.9} &  99.8 / \textbf{99.9} \\
     & Imagenet (C) &  93.8 / \textbf{97.7} &  97.6 / \textbf{97.9} &  99.3 / \textbf{99.7} &  99.5 / \textbf{99.9} \\
     & Uniform &   96.6 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &   97.6 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  89.9 / \textbf{98.4} &  94.6 / \textbf{98.7} &  99.1 / \textbf{99.6} &   99.6 / \textbf{100} \\
\midrule
\multirow{9}{*}{SVHN} & iSUN &  94.4 / \textbf{98.7} &  92.8 / \textbf{99.1} &  99.8 / \textbf{99.9} &    100 / \textbf{100} \\
     & LSUN (R) &  94.1 / \textbf{98.4} &  92.5 / \textbf{98.9} &   99.8 / \textbf{100} &    100 / \textbf{100} \\
     & LSUN (C) &  92.9 / \textbf{98.0} &  88.6 / \textbf{98.1} &  98.6 / \textbf{99.4} &   99.8 / \textbf{100} \\
     & Imagenet (R) &  94.8 / \textbf{98.6} &  93.3 / \textbf{99.0} &  99.7 / \textbf{99.9} &    100 / \textbf{100} \\
     & Imagenet (C) &  94.6 / \textbf{98.6} &  92.8 / \textbf{98.8} &  99.4 / \textbf{99.8} &    100 / \textbf{100} \\
     & Uniform &  93.2 / \textbf{99.8} &   91.6 / \textbf{100} &   99.9 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  97.4 / \textbf{99.8} &  98.9 / \textbf{99.9} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & CIFAR-10 &  91.8 / \textbf{96.7} &  88.9 / \textbf{97.8} &  95.4 / \textbf{97.3} &   99.5 / \textbf{100} \\
     & CIFAR-100 &  91.4 / \textbf{96.7} &  88.2 / \textbf{97.8} &  96.4 / \textbf{98.0} &   99.6 / \textbf{100} \\
\bottomrule
\end{tabular}
 \end{table}


\minisection{OOD sets}
The OOD sets are represented by TinyImageNet~\citep{liang2017enhancing}, LSUN~\citep{yu15lsun}, iSUN~\citep{xu2015turkergaze}, Uniform noise images, and Gaussian noise images. 
We use two variants of TinyImageNet and LSUN sets: a 32x32 image crop that is represented by ``(C)'' and a resizing of the images to 32x32 pixels that termed by ``(R)''.
We also used CIFAR-100, CIFAR-10, and SVHN as OOD for models that were not trained with them.

\minisection{Evaluation methodology}
We benchmark our approach by adopting the following metrics~\citep{gram,lee2018simple}: 
(i) AUROC: The area under the receiver operating characteristic curve of a threshold-based detector. A perfect detector corresponds to an AUROC score of 100\%.
(ii) TNR at 95\% TPR: The probability that an OOD sample is correctly identified (classified as negative) when the true positive rate equals 95\%.
(iii) Detection accuracy: Measures the maximum possible classification accuracy over all possible thresholds.

\begin{table}[tb]
\centering
\small
\caption{A comparison of our pNML regret based detection to leading methods for ResNet-34 model. 
Results measured by the AUROC metric show that our technique offers significant improvements over previous work of up to 41.5\%, 11.9\%, 2.4\%, and 2.1\%. See \secref{sec:results} for more details.}
\label{tab:auroc_resnet}
\vspace{0.1mm}
\begin{tabular}{clcccc}
\toprule
IND & OOD &        Baseline/+pNML &            ODIN/+pNML &            Gram/+pNML &            OECC/+pNML \\

\midrule
\multirow{8}{*}{CIFAR-100} & iSUN &  75.7 / \textbf{83.0} &  85.6 / \textbf{87.6} &  98.8 / \textbf{99.1} &  99.0 / \textbf{99.3} \\
     & LSUN (R) &  75.6 / \textbf{83.8} &  85.4 / \textbf{88.0} &  99.2 / \textbf{99.4} &  99.3 / \textbf{99.6} \\
     & LSUN (C) &  75.5 / \textbf{83.1} &  82.6 / \textbf{88.1} &  92.2 / \textbf{94.6} &  95.7 / \textbf{97.8} \\
     & Imagenet (R) &  77.1 / \textbf{84.4} &  87.7 / \textbf{88.5} &  98.9 / \textbf{99.2} &  98.7 / \textbf{98.9} \\
     & Imagenet (C) &  79.6 / \textbf{85.8} &  85.6 / \textbf{88.6} &  97.7 / \textbf{98.4} &  97.9 / \textbf{98.1} \\
     & Uniform &  85.2 / \textbf{98.1} &  99.0 / \textbf{99.4} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  45.0 / \textbf{86.5} &  83.8 / \textbf{95.7} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  79.3 / \textbf{90.9} &  94.0 / \textbf{95.4} &  96.0 / \textbf{97.9} &  97.0 / \textbf{97.6} \\
\midrule
\multirow{8}{*}{CIFAR-10} & iSUN &  91.0 / \textbf{96.4} &  94.0 / \textbf{97.5} &   99.8 / \textbf{100} &  99.9 / \textbf{99.9} \\
     & LSUN (R) &  91.1 / \textbf{96.6} &  94.1 / \textbf{97.7} &   99.9 / \textbf{100} &   \textbf{100} / 99.9 \\
     & LSUN (C) &  91.8 / \textbf{95.4} &  93.6 / \textbf{95.6} &  97.9 / \textbf{99.1} &  99.1 / \textbf{99.5} \\
     & Imagenet (R) &  91.0 / \textbf{95.4} &  93.9 / \textbf{96.6} &  99.7 / \textbf{99.9} &  99.9 / \textbf{99.9} \\
     & Imagenet (C) &  91.4 / \textbf{95.4} &  93.3 / \textbf{96.2} &  99.3 / \textbf{99.7} &  99.7 / \textbf{99.8} \\
     & Uniform &  96.1 / \textbf{99.8} &   99.9 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &   97.5 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  89.9 / \textbf{95.1} &  95.8 / \textbf{97.9} &  99.5 / \textbf{99.8} &  99.8 / \textbf{99.8} \\
\midrule
\multirow{9}{*}{SVHN} & iSUN &  92.2 / \textbf{97.1} &  91.4 / \textbf{98.0} &  99.8 / \textbf{99.9} &    100 / \textbf{100} \\
     & LSUN (R) &  91.5 / \textbf{96.7} &  90.6 / \textbf{97.7} &   99.8 / \textbf{100} &    100 / \textbf{100} \\
     & LSUN (C) &  92.8 / \textbf{97.0} &  92.3 / \textbf{97.1} &  98.8 / \textbf{99.6} &  99.7 / \textbf{99.9} \\
     & Imagenet (R) &  93.5 / \textbf{97.5} &  92.8 / \textbf{98.3} &  99.8 / \textbf{99.9} &    100 / \textbf{100} \\
     & Imagenet (C) &  94.2 / \textbf{97.5} &  93.7 / \textbf{98.2} &  99.5 / \textbf{99.9} &   99.9 / \textbf{100} \\
     & Uniform &  96.0 / \textbf{98.5} &  95.5 / \textbf{99.5} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  96.1 / \textbf{98.4} &  96.1 / \textbf{99.6} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & CIFAR-10 &  93.0 / \textbf{97.4} &  92.0 / \textbf{98.0} &  97.4 / \textbf{99.3} &  99.4 / \textbf{99.8} \\
     & CIFAR-100 &  92.5 / \textbf{97.1} &  91.7 / \textbf{97.8} &  97.5 / \textbf{99.2} &  99.4 / \textbf{99.8} \\
\bottomrule
\end{tabular}
 \end{table}


\subsection{Results} 
\label{sec:results}
We build upon the existing leading methods: Baseline~\citep{hendrycks17baseline}, ODIN~\citep{liang2017enhancing}, Gram~\citep{gram},  OECC~\citep{PAPADOPOULOS2021138}, and Energy~\citep{liu2020energy}.
We use the following pretrained models: ResNet-34~\citep{he2016deep}, DenseNet-BC-100~\citep{huang2017densely} and WideResNet-40~\citep{zagoruyko2016wide}. Training was performed using CIFAR-100, CIFAR-10 and SVHN, each training set used separately to provide a complete picture of our proposed method's capabilities. 
Notice that ODIN, OECC, and Energy methods use OOD sets during training and the Gram method requires IND validation samples.

\Tableref{tab:auroc_densenet} and \Tableref{tab:auroc_resnet} show the AUROC of different OOD sets for DenseNet and ResNet models respectively.
Our approach improves all the compared methods in nearly all combinations of IND-OOD sets. 
The largest AUROC gain over the current state-of-the-art is of CIFAR-100 as IND and LSUN (C) as OOD: For the DenseNet model, we improve Gram and OECC method by 3.1\% and 2.2\% respectively. For the ResNet model, we improve this combination by 2.4\% and 2.1\% respectively. 
The additional metrics (TNR at 95\% FPR and detection accuracy) are shown in \appref{appendix:ood_results}.

The Baseline method uses a pretrained ERM model with no extra data. 
Combining the pNML regret with the standard ERM model as shown in the Baseline+pNML column surpasses Baseline by up to 69.4\% and 41.5\% for DensNet and ResNet, respectively.
Also, Baseline+pNML is comparable to the more sophisticated methods:
Although it lacks tunable parameters and does not use extra data, Baseline+pNML outperforms ODIN in most
DenseNet IND-OOD set combinations.

\Tableref{tab:auroc_wrn_results} shows the results of the Energy method and our method when combined with the pretrained Energy model (Energy+pNML) with WideResNet-40 model on CIFAR-100 and CIFAR-10 as IND sets.
Evidently, our method improves the AUROC of the OOD detection task in 14 out of 16 IND-OOD combinations.
The most significant improvement is in CIFAR-100 as IND and ImageNet (R) and iSUN as the OOD sets. In these sets, we improve the AUROC by 15.6\% and 15.2\% respectively. 
For TNR at TPR 95\%, the pNML regret enhances the CIFAR-100 and Gaussian combination by 90.4\% and achieves a perfect separation of IND-OOD samples. 


For high resolution images, we use Resnet-18 and ResNet-101 models trained on ImageNet. We utilize the ImageNet-30 training set for computing . All images were resized to  pixels. We compare the result to the Baseline method in~\Tableref{tab:aruoc_imagnet30}.
The table shows that the pNML outperforms Baseline by up to 9.8\% and 8.23\% for ResNet-18 and ResNet-101 respectively. 


\begin{table}[t]
\centering
\small
\caption{OOD detection for WideResNet-40 model. Results show our method improves the Energy~\citep{liu2020energy} method up to 15.6\%, 90.4\%, and 15.9\% for AUROC, TNR at TPR 95\%, and Detection accuracy respectively. See \secref{sec:results} for more details.}
\label{tab:auroc_wrn_results}
\vspace{0.1mm}
\begin{tabular}{clccc}
\toprule
          IND & OOD &                 AUROC &        TNR at TPR 95\% &        Detection Acc. \\
 \midrule & & \multicolumn{3}{c}{Energy/+pNML} \\ \cmidrule{3-5} 

\multirow{8}{*}{CIFAR-100} & iSUN &  78.4 / \textbf{93.6} &  30.7 / \textbf{62.5} &  71.1 / \textbf{87.0} \\
          & LSUN (R) &  80.3 / \textbf{94.1} &  31.2 / \textbf{65.5} &  73.1 / \textbf{87.5} \\
          & LSUN (C) &  \textbf{95.9} / 95.5 &  \textbf{80.0} / 79.3 &  \textbf{89.3} / 89.1 \\
          & Imagenet (R) &  71.4 / \textbf{87.0} &  22.1 / \textbf{44.8} &  66.1 / \textbf{79.9} \\
          & Imagenet (C) &  79.7 / \textbf{87.3} &  36.9 / \textbf{49.4} &  72.8 / \textbf{79.7} \\
          & Uniform &  97.9 / \textbf{99.8} &   95.2 / \textbf{100} &  95.8 / \textbf{99.6} \\
          & Gaussian &  92.0 / \textbf{99.8} &    9.6 / \textbf{100} &  92.3 / \textbf{99.8} \\
          & SVHN &  \textbf{96.5} / 96.4 &  79.2 / \textbf{82.8} &  90.5 / \textbf{91.3} \\
\midrule
\multirow{8}{*}{CIFAR-10} & iSUN &  99.3 / \textbf{99.4} &  98.3 / \textbf{98.7} &  96.7 / \textbf{97.0} \\
          & LSUN (R) &  99.3 / \textbf{99.5} &  98.6 / \textbf{99.0} &  97.0 / \textbf{97.3} \\
          & LSUN (C) &  99.4 / \textbf{99.5} &  98.6 / \textbf{98.6} &  97.0 / \textbf{97.1} \\
          & Imagenet (R) &  98.1 / \textbf{98.1} &  92.0 / \textbf{92.4} &  94.0 / \textbf{94.0} \\
          & Imagenet (C) &  98.6 / \textbf{98.6} &  94.4 / \textbf{94.6} &  94.9 / \textbf{94.9} \\
          & Uniform &  99.0 / \textbf{99.9} &    100 / \textbf{100} &  98.7 / \textbf{99.8} \\
          & Gaussian &  99.1 / \textbf{99.9} &    100 / \textbf{100} &  98.7 / \textbf{99.8} \\
          & SVHN &  99.3 / \textbf{99.6} &  98.3 / \textbf{98.9} &  96.9 / \textbf{97.6} \\
\bottomrule
\end{tabular}
 \end{table}

\begin{table}[t]
\centering
\small
\caption{OOD detection with ImageNet-30 as the IND set. Results show that the pNML outperforms the Baseline~\citep{hendrycks17baseline} method up to 9.8\% and 8.23\% for ResNet-18 and ResNet-101 models respectively. See \secref{sec:results} for more details.}
\label{tab:aruoc_imagnet30}
\vspace{0.1mm}
\begin{tabular}{clcc}
\toprule
IND & OOD &        ResNet-18 &   ReseNet-101  \\
\midrule 
 & & Baseline/+pNML & Baseline/+pNML \\ \cmidrule{3-4} 
\multirow{8}{*}{ImageNet-30} 
    & iSUN      &  95.58 / \textbf{99.74}   &  96.26 / \textbf{99.54}   \\
    & LSUN (R)  &  95.51 / \textbf{99.72}   &  95.77 / \textbf{99.43}   \\
    & LSUN (C)  &  96.89 / \textbf{99.77}   &  98.00 / \textbf{99.86}   \\
    & Uniform   &  99.35 / \textbf{99.99}   &   98.70 / \textbf{100}    \\
    & Gaussian  &  98.78 / \textbf{100}     &   98.61 / \textbf{100}    \\
    & SVHN      &  99.18 / \textbf{99.99}   &  98.94 / \textbf{99.98}   \\
    & CIFAR-10  &  89.99 / \textbf{99.79}   &   91.24 / \textbf{99.47}  \\
    & CIFAR-100 &  92.15 / \textbf{92.15}   &  93.39 / \textbf{99.58}   \\
\bottomrule
\end{tabular}
 \end{table}



\section{Conclusions}
\label{sec:conclusions}
We derived the analytical expression of the pNML regret for a single layer NN. 
We showed that the model generalizes well when the test data resides in either a subspace spanned by the eigenvectors associated with the large eigenvalues of the training data correlation matrix or far from the decision boundary.
We showed how to apply the pNML for any pretrained DNN that uses the softmax layer with neither additional parameters nor extra data.
We demonstrated the effectiveness of our pNML regret--based approach on 74 IND-OOD detection benchmarks. Compared with the recent state of the art methods, our approach elevates absolute AUROC values by up to 15.6\%

\label{sec:limitations}
For future work, in our regret derivation we constrained the genie hypothesis set to the Gaussian family and a single layer NN. We would like to extend the derivation to larger hypothesis sets and more layers, believing it would improve performance.
Furthermore, the pNML regret can be used for additional tasks such as active learning, probability calibration, and adversarial attack detection.

\minisection{Societal impacts} \label{sec:social_impacts}
The negative social impact of this work depends on the application. For instance, in a surveillance camera scenario, a person from a minority group can be flagged as OOD if the minority group was not included in the training set.
We recommend that when OOD is flagged a human will intervene rather than an algorithmic response since the nature of the OOD is unknown.


\FloatBarrier
\bibliographystyle{apalike}
\bibliography{main}


\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}


\appendix
\onecolumn
\title{Single Layer Predictive Normalized Maximum Likelihood for Out-of-Distribution Detection \\  --Supplementary material--}


\maketitle

\section{MSE minimization is equivalent to log-loss minimization}
\label{appendix:logloss}
We use the same notations as in \secref{sec:pnml_regret}.

Denote  as a one-hot row vector of the true label, we define the hypothesis set that genie is allowed to choose from as

The genie chooses the learner from the hypothesis set that minimizes the log-loss.
Let  be the -th data with the label ,
 be a row vector where  is its  element.
We show that the log-loss minimizer of this hypothesis set is equal to the MSE minimizer:

We know that the training set label are one-hot vector  such that :

which is the MSE minimization objective we defined in \secref{sec:pnml_regret}.


\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/regret_simulation.pdf}
    \caption{The pNML regret for a two class predictor.  is the ERM prediction of class .}
    \label{fig:regret_simulation}
\end{figure}


\section{Single layer NN pNML regret simulation}
\label{appendix:regret_simulation}

We simulate the response of the pNML regret for two classes (C=2) and divide it by  to have the regret bounded between 0 and 1.
\Figref{fig:regret_simulation} shows the regret behaviour for different  (the ERM probability assignment of class 1) as a function of .

For an ERM model that is certain on the prediction ( that is represented by the purple curve), a slight variation of  causes a large response of the regret comparing to  that equals 0.55 and 0.85. 
All curves converging to the maximal regret for  greater than 6.


\section{The spectrum of real dataset} \label{appendix:spectrum}

We provide a visualization of the training data spectrum when propagated to the last layer of a DNN.

We feed the training data through the model up to the last layer to create the training embeddings. 
Next, we compute the correlation matrix of the training embeddings and perform an SVD decomposition.
We plot the eigenvalues for different training sets in~\figref{fig:svd}. 

\Figref{fig:DenseNet_svd} shows the eigenvalues of DenseNet-BC-100 model when ordered from the largest to smallest.
For the SVHN training set, most of the energy is located in the first 50 eigenvalues and then there is a significant decrease of approximately .
The same phenomenon is also seen in~\figref{fig:DenseNet_svd} that shows the eigenvalues of ResNet-40 model.
In our derived regret, if the test sample is located in the subspace that is associated with small eigenvalues (for example indices 50 or above for DenseNet trained with SVHN) then  is large and so is the pNML regret.

For both DensNet and ResNet models, the values of the eigenvalues of CIFAR-100 seem to be spread more evenly compared to CIFAR-10, and the CIFAR-10 are more uniform than the SVHN. 
How much the eigenvalues are spread can indicate the variability of the set: SVHN is a set of digits that is much more constrained than CIFAR-100 which has 100 different classes. 


\begin{figure}[tb]
    \centering
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/DenseNet_svd.pdf}
    \caption{DenseNet-BC-100  \label{fig:DenseNet_svd}}   
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/ResNet_svd.pdf}
    \caption{ResNet-40 \label{fig:ResNet_svd}}   
\end{subfigure}
\caption{The spectrum of the training embeddings.}
\label{fig:svd}
\end{figure}















\section{Gram vs. Gram+pNML}
\label{appendix:gram_vs_gram_plus_pnml}
We further explore the benefit of the pNML regret in detecting OOD samples over the Gram approach.
We focus on the DenseNet model with CIFAR-100 as the training set and LSUN (C) as the OOD set.

\Figref{fig:ind_hist} shows the 2D histogram of the IND set based on the pNML regret values and Gram scores. 
In addition, we plotted the best threshold for separating the IND and OOD of these sets.
pNML regret values less than 0.0024 and Gram scores below 0.0017 qualify as IND samples by both the pNML and Gram scores. 
Gram and Gram+pNML do not succeed to classify 1205 and 891 out of a total 10,000 IND samples respectively.

\Figref{fig:ood_hist} presents the 2D histogram of the LSUN (C) as OOD set. 
For regret values greater than 0.0024 and Gram score lower than 0.0017, the pNML succeeds to classify as IND but the Gram fails: 
There are 473 samples that the pNML classifies as OOD but the Gram fails, in contrast to 76 samples classified as such by the Gram and not by the pNML regret.
Most of the pNML improvement is in assigning a high score to OOD samples while there is not much change in the rank of the IND ones.


\begin{figure}[tb]
    \centering
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/ind_hist.pdf}
    \caption{IND 2D histogram  \label{fig:ind_hist}}   
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/ood_hist.pdf}
    \caption{OOD 2D histogram \label{fig:ood_hist}}   
\end{subfigure}
\caption{2D histogram of the pNML regret and the Gram score of a DenseNet model trained with CIFAR-100 as IND set and LSUN (C) as OOD.}
\label{fig:histograms}
\end{figure}



\begin{table}[tb]
\centering
\fontsize{8}{9}\selectfont
\caption{DenseNet-BC-100 model TNR at TPR95\% comparison. The compared methods are Baseline~\citep{hendrycks17baseline}, ODIN~\citep{liang2017enhancing}, Gram~\citep{gram}, and OECC~\citep{PAPADOPOULOS2021138}}
\label{tab:TNRatTPR95_densenet}
\begin{tabular}{clcccc}
\toprule
IND & OOD &        Baseline/+pNML &            ODIN/+pNML &            Gram/+pNML &            OECC/+pNML \\

\midrule
\multirow{8}{*}{CIFAR-100} & iSUN &  14.8 / \textbf{81.2} &  37.4 / \textbf{82.8} &  95.8 / \textbf{97.9} &  97.5 / \textbf{99.2} \\
     & LSUN (R) &  16.4 / \textbf{82.7} &  41.6 / \textbf{84.5} &  97.1 / \textbf{98.7} &  98.4 / \textbf{99.6} \\
     & LSUN (C) &  28.3 / \textbf{65.7} &  58.2 / \textbf{65.4} &  65.3 / \textbf{76.3} &  74.6 / \textbf{83.4} \\
     & Imagenet (R) &  17.3 / \textbf{86.4} &  43.0 / \textbf{87.9} &  95.6 / \textbf{98.0} &  96.5 / \textbf{99.0} \\
     & Imagenet (C) &  24.3 / \textbf{77.2} &  52.5 / \textbf{78.6} &  88.8 / \textbf{93.8} &  92.6 / \textbf{96.9} \\
     & Uniform &    0.0 / \textbf{100} &    0.0 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &    0.0 / \textbf{100} &    0.0 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  26.2 / \textbf{79.2} &  56.8 / \textbf{79.0} &  89.3 / \textbf{93.7} &  89.0 / \textbf{90.7} \\
\midrule
\multirow{8}{*}{CIFAR-10} & iSUN &  63.3 / \textbf{93.2} &  94.0 / \textbf{94.3} &  99.1 / \textbf{99.8} &   99.7 / \textbf{100} \\
     & LSUN (R) &  66.9 / \textbf{94.2} &  \textbf{96.2} / 95.8 &  99.5 / \textbf{99.9} &   99.8 / \textbf{100} \\
     & LSUN (C) &  52.0 / \textbf{79.9} &  74.6 / \textbf{80.2} &  88.7 / \textbf{94.4} &  95.7 / \textbf{99.6} \\
     & Imagenet (R) &  59.4 / \textbf{93.4} &  92.5 / \textbf{94.6} &  98.8 / \textbf{99.6} &  99.3 / \textbf{99.9} \\
     & Imagenet (C) &  57.0 / \textbf{87.1} &  86.9 / \textbf{88.3} &  96.8 / \textbf{98.7} &  98.6 / \textbf{99.8} \\
     & Uniform &   76.4 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &   88.1 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  40.4 / \textbf{92.2} &  77.0 / \textbf{95.0} &  96.0 / \textbf{98.2} &  98.5 / \textbf{99.9} \\
\midrule
\multirow{9}{*}{SVHN} & iSUN &  78.3 / \textbf{93.6} &  78.5 / \textbf{96.3} &  99.6 / \textbf{99.9} &    100 / \textbf{100} \\
     & LSUN (R) &  77.1 / \textbf{91.7} &  77.0 / \textbf{95.2} &   99.7 / \textbf{100} &    100 / \textbf{100} \\
     & LSUN (C) &  73.5 / \textbf{89.7} &  68.5 / \textbf{90.0} &  93.4 / \textbf{97.2} &   99.5 / \textbf{100} \\
     & Imagenet (R) &  79.7 / \textbf{93.6} &  79.0 / \textbf{95.8} &  99.2 / \textbf{99.8} &    100 / \textbf{100} \\
     & Imagenet (C) &  78.9 / \textbf{92.8} &  77.6 / \textbf{94.5} &  98.0 / \textbf{99.3} &   99.9 / \textbf{100} \\
     & Uniform &   66.1 / \textbf{100} &   71.7 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  88.7 / \textbf{99.7} &   95.6 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & CIFAR-10 &  69.1 / \textbf{81.0} &  66.6 / \textbf{88.5} &  75.1 / \textbf{86.8} &   98.9 / \textbf{100} \\
     & CIFAR-100 &  68.7 / \textbf{81.4} &  65.7 / \textbf{88.5} &  80.3 / \textbf{90.1} &   99.1 / \textbf{100} \\
\bottomrule
\end{tabular}
 \end{table}

\begin{table}[tbh]
\centering
\fontsize{8}{9}\selectfont
\caption{DenseNet-BC-100 model Detection Acc. comparison. The compared methods are Baseline~\citep{hendrycks17baseline}, ODIN~\citep{liang2017enhancing}, Gram~\citep{gram}, and OECC~\citep{PAPADOPOULOS2021138}}
\label{tab:DetectionAcc._densenet}
\begin{tabular}{clcccc}
\toprule
IND & OOD &        Baseline/+pNML &            ODIN/+pNML &            Gram/+pNML &            OECC/+pNML \\

\midrule
\multirow{8}{*}{CIFAR-100} & iSUN &  64.0 / \textbf{89.9} &  76.5 / \textbf{90.3} &  95.6 / \textbf{97.0} &  96.5 / \textbf{98.0} \\
     & LSUN (R) &  65.0 / \textbf{90.5} &  77.7 / \textbf{91.0} &  96.3 / \textbf{97.4} &  97.2 / \textbf{98.5} \\
     & LSUN (C) &  72.6 / \textbf{85.3} &  83.4 / \textbf{85.2} &  83.7 / \textbf{87.5} &  87.0 / \textbf{90.2} \\
     & Imagenet (R) &  65.7 / \textbf{91.6} &  77.3 / \textbf{92.1} &  95.5 / \textbf{97.0} &  96.0 / \textbf{97.8} \\
     & Imagenet (C) &  69.0 / \textbf{89.0} &  80.8 / \textbf{89.3} &  92.4 / \textbf{94.5} &  94.0 / \textbf{96.1} \\
     & Uniform &   64.2 / \textbf{100} &   85.0 / \textbf{100} &    100 / \textbf{100} &   99.9 / \textbf{100} \\
     & Gaussian &   58.8 / \textbf{100} &   66.9 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  75.5 / \textbf{90.3} &  86.0 / \textbf{90.3} &  92.3 / \textbf{94.4} &  92.1 / \textbf{93.0} \\
\midrule
\multirow{8}{*}{CIFAR-10} & iSUN &  89.2 / \textbf{94.2} &  94.6 / \textbf{94.8} &  98.0 / \textbf{99.0} &  98.7 / \textbf{99.6} \\
     & LSUN (R) &  90.2 / \textbf{94.7} &  \textbf{95.6} / 95.5 &  98.6 / \textbf{99.3} &  98.9 / \textbf{99.7} \\
     & LSUN (C) &  86.9 / \textbf{89.5} &  \textbf{89.7} / 89.4 &  92.1 / \textbf{94.8} &  95.5 / \textbf{98.8} \\
     & Imagenet (R) &  88.5 / \textbf{94.3} &  94.0 / \textbf{94.9} &  97.9 / \textbf{98.8} &  98.3 / \textbf{99.2} \\
     & Imagenet (C) &  88.0 / \textbf{91.9} &  \textbf{92.3} / 92.2 &  96.2 / \textbf{97.7} &  97.4 / \textbf{99.0} \\
     & Uniform &   94.8 / \textbf{100} &   99.7 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &   95.3 / \textbf{100} &   99.8 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  83.2 / \textbf{94.0} &  88.1 / \textbf{95.1} &  95.8 / \textbf{97.3} &  97.4 / \textbf{99.3} \\
\midrule
\multirow{9}{*}{SVHN} & iSUN &  89.7 / \textbf{94.6} &  87.7 / \textbf{95.7} &  98.3 / \textbf{99.1} &   99.8 / \textbf{100} \\
     & LSUN (R) &  89.2 / \textbf{93.8} &  87.2 / \textbf{95.1} &  98.6 / \textbf{99.2} &   99.9 / \textbf{100} \\
     & LSUN (C) &  88.0 / \textbf{92.8} &  83.6 / \textbf{92.8} &  94.3 / \textbf{96.4} &  98.5 / \textbf{99.8} \\
     & Imagenet (R) &  90.2 / \textbf{94.4} &  88.2 / \textbf{95.5} &  97.9 / \textbf{98.9} &   99.7 / \textbf{100} \\
     & Imagenet (C) &  89.8 / \textbf{94.2} &  87.6 / \textbf{94.8} &  96.7 / \textbf{98.1} &   99.5 / \textbf{100} \\
     & Uniform &  87.9 / \textbf{98.8} &  85.2 / \textbf{99.4} &   99.9 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  93.6 / \textbf{98.4} &  95.4 / \textbf{99.1} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & CIFAR-10 &  86.5 / \textbf{91.0} &  83.5 / \textbf{92.7} &  89.0 / \textbf{92.0} &  97.4 / \textbf{99.8} \\
     & CIFAR-100 &  86.5 / \textbf{91.0} &  83.1 / \textbf{92.8} &  90.4 / \textbf{93.2} &  97.7 / \textbf{99.8} \\
\bottomrule
\end{tabular}
 \end{table}

\begin{table}[tbh]
\centering
\fontsize{8}{9}\selectfont
\caption{ResNet-34 model TNR at TPR95\% comparison. The compared methods are Baseline~\citep{hendrycks17baseline}, ODIN~\citep{liang2017enhancing}, Gram~\citep{gram}, and OECC~\citep{PAPADOPOULOS2021138}}
\label{tab:TNRatTPR95_resnet}
\begin{tabular}{clcccc}
\toprule
IND & OOD &        Baseline/+pNML &            ODIN/+pNML &            Gram/+pNML &            OECC/+pNML \\

\midrule
\multirow{8}{*}{CIFAR-100} & iSUN &  16.6 / \textbf{26.1} &  \textbf{45.4} / 44.1 &  94.7 / \textbf{95.7} &  97.2 / \textbf{98.0} \\
     & LSUN (R) &  18.4 / \textbf{28.4} &  \textbf{45.5} / 44.6 &  96.6 / \textbf{97.1} &  98.3 / \textbf{99.0} \\
     & LSUN (C) &  18.2 / \textbf{30.1} &  44.0 / \textbf{51.2} &  64.6 / \textbf{72.9} &  80.3 / \textbf{89.8} \\
     & Imagenet (R) &  20.2 / \textbf{31.8} &  \textbf{48.7} / 47.6 &  94.8 / \textbf{96.2} &  95.5 / \textbf{95.8} \\
     & Imagenet (C) &  23.9 / \textbf{33.6} &  44.4 / \textbf{48.1} &  88.3 / \textbf{91.6} &  90.6 / \textbf{91.6} \\
     & Uniform &  10.1 / \textbf{89.1} &  98.4 / \textbf{98.5} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &   0.0 / \textbf{13.7} &   4.5 / \textbf{66.8} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  19.9 / \textbf{52.0} &  63.8 / \textbf{75.0} &  80.3 / \textbf{89.0} &  86.8 / \textbf{89.2} \\
\midrule
\multirow{8}{*}{CIFAR-10} & iSUN &  44.5 / \textbf{78.5} &  73.0 / \textbf{86.3} &  99.4 / \textbf{99.9} &   99.8 / \textbf{100} \\
     & LSUN (R) &  45.1 / \textbf{79.8} &  73.5 / \textbf{87.5} &  99.6 / \textbf{99.9} &   99.9 / \textbf{100} \\
     & LSUN (C) &  48.0 / \textbf{72.6} &  63.1 / \textbf{76.1} &  90.2 / \textbf{95.9} &  96.3 / \textbf{98.9} \\
     & Imagenet (R) &  44.0 / \textbf{72.8} &  71.8 / \textbf{81.9} &  98.9 / \textbf{99.6} &  99.6 / \textbf{99.8} \\
     & Imagenet (C) &  45.9 / \textbf{71.4} &  66.5 / \textbf{78.0} &  97.0 / \textbf{98.8} &  98.9 / \textbf{99.7} \\
     & Uniform &   71.4 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &   90.2 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  32.2 / \textbf{69.1} &  81.9 / \textbf{90.8} &  97.6 / \textbf{99.2} &  99.3 / \textbf{99.7} \\
\midrule
\multirow{9}{*}{SVHN} & iSUN &  77.0 / \textbf{85.6} &  79.1 / \textbf{90.6} &  99.5 / \textbf{99.9} &    100 / \textbf{100} \\
     & LSUN (R) &  74.4 / \textbf{82.9} &  76.6 / \textbf{88.3} &  99.6 / \textbf{99.9} &    100 / \textbf{100} \\
     & LSUN (C) &  76.1 / \textbf{86.3} &  78.5 / \textbf{86.4} &  94.5 / \textbf{98.4} &  99.3 / \textbf{99.9} \\
     & Imagenet (R) &  79.0 / \textbf{88.0} &  80.8 / \textbf{92.5} &  99.4 / \textbf{99.8} &    100 / \textbf{100} \\
     & Imagenet (C) &  80.4 / \textbf{88.4} &  82.4 / \textbf{91.5} &  98.6 / \textbf{99.7} &   99.9 / \textbf{100} \\
     & Uniform &  85.2 / \textbf{95.6} &  86.1 / \textbf{99.3} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  84.8 / \textbf{94.9} &  90.9 / \textbf{99.4} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & CIFAR-10 &  78.3 / \textbf{87.2} &  79.9 / \textbf{90.4} &  86.1 / \textbf{97.2} &  98.4 / \textbf{99.8} \\
     & CIFAR-100 &  76.9 / \textbf{85.8} &  78.5 / \textbf{89.1} &  87.6 / \textbf{96.9} &  98.4 / \textbf{99.8} \\
\bottomrule
\end{tabular}
 \end{table}

\begin{table}[tbh]
\centering
\fontsize{8}{9}\selectfont
\caption{ResNet-34 model Detection Acc. comparison. The compared methods are Baseline~\citep{hendrycks17baseline}, ODIN~\citep{liang2017enhancing}, Gram~\citep{gram}, and OECC~\citep{PAPADOPOULOS2021138}}
\label{tab:DetectionAcc._resnet}
\begin{tabular}{clcccc}
\toprule
IND & OOD &        Baseline/+pNML &            ODIN/+pNML &            Gram/+pNML &            OECC/+pNML \\

\midrule
\multirow{8}{*}{CIFAR-100} & iSUN &  70.1 / \textbf{76.0} &  78.6 / \textbf{79.3} &  95.0 / \textbf{95.4} &  96.2 / \textbf{96.9} \\
     & LSUN (R) &  69.8 / \textbf{76.5} &  78.1 / \textbf{79.8} &  96.0 / \textbf{96.2} &  96.9 / \textbf{97.6} \\
     & LSUN (C) &  69.4 / \textbf{76.0} &  75.7 / \textbf{79.9} &  84.3 / \textbf{87.4} &  89.3 / \textbf{92.8} \\
     & Imagenet (R) &  70.8 / \textbf{76.6} &  80.2 / \textbf{80.2} &  95.0 / \textbf{95.7} &  95.4 / \textbf{95.5} \\
     & Imagenet (C) &  72.5 / \textbf{78.2} &  78.7 / \textbf{80.2} &  92.1 / \textbf{93.6} &  93.2 / \textbf{93.6} \\
     & Uniform &  81.7 / \textbf{93.5} &  96.7 / \textbf{96.8} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  60.5 / \textbf{83.7} &  81.7 / \textbf{92.2} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  73.2 / \textbf{82.9} &  88.1 / \textbf{89.0} &  89.5 / \textbf{92.6} &  91.8 / \textbf{92.7} \\
\midrule
\multirow{8}{*}{CIFAR-10} & iSUN &  85.0 / \textbf{90.4} &  86.9 / \textbf{92.0} &  98.2 / \textbf{99.1} &  98.8 / \textbf{99.0} \\
     & LSUN (R) &  85.3 / \textbf{90.8} &  87.1 / \textbf{92.4} &  98.7 / \textbf{99.3} &  99.1 / \textbf{99.2} \\
     & LSUN (C) &  86.2 / \textbf{90.0} &  87.2 / \textbf{88.7} &  92.8 / \textbf{95.6} &  95.7 / \textbf{97.2} \\
     & Imagenet (R) &  84.9 / \textbf{89.0} &  86.3 / \textbf{90.4} &  97.9 / \textbf{98.8} &  98.5 / \textbf{98.7} \\
     & Imagenet (C) &  85.3 / \textbf{89.4} &  86.3 / \textbf{89.9} &  96.3 / \textbf{97.7} &  97.5 / \textbf{98.3} \\
     & Uniform &  93.5 / \textbf{98.8} &  99.3 / \textbf{99.9} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  95.5 / \textbf{99.7} &   99.8 / \textbf{100} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & SVHN &  85.1 / \textbf{90.3} &  89.1 / \textbf{93.0} &  96.8 / \textbf{98.1} &  98.1 / \textbf{98.4} \\
\midrule
\multirow{9}{*}{SVHN} & iSUN &  89.7 / \textbf{92.8} &  89.2 / \textbf{93.5} &  98.2 / \textbf{99.1} &  99.7 / \textbf{99.9} \\
     & LSUN (R) &  88.9 / \textbf{92.1} &  88.2 / \textbf{92.7} &  98.6 / \textbf{99.2} &  99.8 / \textbf{99.9} \\
     & LSUN (C) &  89.7 / \textbf{92.2} &  89.2 / \textbf{92.2} &  94.8 / \textbf{97.3} &  98.0 / \textbf{98.9} \\
     & Imagenet (R) &  90.4 / \textbf{93.4} &  90.0 / \textbf{94.2} &  98.0 / \textbf{99.1} &  99.5 / \textbf{99.8} \\
     & Imagenet (C) &  91.0 / \textbf{93.3} &  90.6 / \textbf{93.8} &  97.1 / \textbf{98.7} &  99.2 / \textbf{99.6} \\
     & Uniform &  92.9 / \textbf{95.7} &  92.3 / \textbf{97.4} &   99.9 / \textbf{100} &    100 / \textbf{100} \\
     & Gaussian &  92.9 / \textbf{95.4} &  93.0 / \textbf{97.5} &    100 / \textbf{100} &    100 / \textbf{100} \\
     & CIFAR-10 &  90.0 / \textbf{93.1} &  89.4 / \textbf{93.4} &  92.2 / \textbf{96.2} &  96.9 / \textbf{98.5} \\
     & CIFAR-100 &  89.6 / \textbf{92.5} &  89.0 / \textbf{93.1} &  92.4 / \textbf{96.1} &  97.0 / \textbf{98.5} \\
\bottomrule
\end{tabular}
 \end{table}


\section{Additional out of distribution metrics}
\label{appendix:ood_results}

The additional OOD metrics, TNR at 95\% FPR and Detection Accuracy, for the DensNet model are shown in \tableref{tab:TNRatTPR95_densenet} and \tableref{tab:DetectionAcc._densenet} respectively and for the ResNet are presented in \tableref{tab:TNRatTPR95_resnet} and \tableref{tab:DetectionAcc._resnet}.
We improve the compared methods for all IND-OOD sets except for 6 experiments of ODIN method with the TNR at 95\% metric.
We show the TNR vs FPR of these experiments in \figref{fig:tpr_vs_tnr}.
We state that for most of the TNR values, the pNML regret outperforms the ODIN method, as also shown in the AUROC metric.


\begin{figure}[tb]
\centering
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/tpr_tnr_odin_resnet_cifar100_20210109_170112_iSUN.pdf}
    \caption{ResNet CIFAR-100 iSUN  \label{fig:tpr_tnr_odin_resnet_cifar100_iSUN}}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/tpr_tnr_odin_resnet_cifar100_20210109_170112_Imagenet_resize.pdf}
    \caption{ResNet CIFAR-100 Imagenet (R) \label{fig:tpr_tnr_odin_resnet_cifar100_imagenet_resize}}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/tpr_tnr_odin_resnet_cifar100_20210109_170112_LSUN_resize.pdf}
    \caption{ResNet CIFAR-100 LSUN (R)  \label{fig:tpr_tnr_odin_resnet_cifar100_LSUN_resize}}
\end{subfigure}
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=1.0\textwidth]{figures/tpr_tnr_odin_densenet_cifar10_20210109_170052_LSUN_resize.pdf}
    \caption{DenseNet CIFAR-10 LSUN (R) \label{fig:tpr_tnr_odin_densenet_cifar10_LSUN_resize}}
\end{subfigure}
\caption{The TNR as a function of the TPR of IND-OOD sets for which the ODIN method is better than the pNML at TPR of 95\%.}
\label{fig:tpr_vs_tnr}
\end{figure} 
\end{document}
