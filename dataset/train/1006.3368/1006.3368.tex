\documentclass[letterpaper, 11pt]{article}
\bibliographystyle{plain}
\usepackage{amsmath,amssymb,graphicx,latexsym,amsthm,xspace}
\usepackage{framed}

\setlength{\topmargin}{0in}
\setlength{\textwidth}{6.5in} \setlength{\textheight}{9in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textfloatsep}{0in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}


\newcommand{\calA}{\mathcal{A}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calK}{\mathcal{K}}

\newcommand{\dtv}{d_{\mathrm{TV}}}

\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\biC}{\boldsymbol{C}}
\newcommand{\bib}{\boldsymbol{b}}
\newcommand{\bic}{\boldsymbol{c}}
\newcommand{\biv}{\boldsymbol{v}}
\newcommand{\biw}{\boldsymbol{w}}
\newcommand{\bix}{\boldsymbol{x}}
\newcommand{\biy}{\boldsymbol{y}}
\newcommand{\biz}{\boldsymbol{z}}
\newcommand{\bimu}{\boldsymbol{\mu}}
\newcommand{\blp}{\textsf{BasicLP}\xspace}
\newcommand{\bsdp}{\textsf{BasicSDP}\xspace}


\newcommand{\lp}{\mathbf{lp}}
\newcommand{\opt}{\mathbf{opt}}
\newcommand{\val}{\mathbf{val}}
\newcommand{\sdp}{\mathbf{sdp}}
\newcommand{\olopt}{\overline{\mathbf{opt}}}
\newcommand{\ollp}{\overline{\mathbf{lp}}}
\newcommand{\olval}{\overline{\mathbf{val}}}
\newcommand{\idx}{\mathrm{idx}}

\newcommand{\supp}{\mathrm{supp}}

\newcommand{\poly}{\mathrm{poly}}
\newcommand{\E}{\mathop{\mathrm{E}}}

\title{Optimal Constant-Time Approximation Algorithms and (Unconditional) Inapproximability Results for Every Bounded-Degree CSP}
\author{Yuichi Yoshida\thanks{Supported by MSRA Fellowship 2010. This work was conducted while the author was visiting Rutgers University. }\\\\
  School of Informatics, Kyoto University, and\\ Preferred Infrastructure, Inc.\\yyoshida@lab2.kuis.kyoto-u.ac.jp}

\date{}
\begin{document}
\setcounter{page}{0}
\maketitle
\begin{abstract}
  Raghavendra (STOC 2008) gave an elegant and surprising result:
  if Khot's Unique Games Conjecture (STOC 2002) is true, 
  then for every constraint satisfaction problem (CSP), 
  the best approximation ratio is attained by a certain simple semidefinite programming and a rounding scheme for it.

  In this paper, 
  we show that similar results hold for constant-time approximation algorithms in the bounded-degree model.
  Specifically, we present the followings:
  (i) For every CSP, we construct an oracle that serves an access, 
  in constant time,
  to a nearly optimal solution to a basic LP relaxation of the CSP.
  (ii) Using the oracle, 
  we give a constant-time rounding scheme that achieves an approximation ratio coincident with the integrality gap of the basic LP.
  (iii) Finally, we give a generic conversion from integrality gaps of basic LPs to hardness results.
  All of those results are \textit{unconditional}.
  Therefore, for every bounded-degree CSP, 
  we give the best constant-time approximation algorithm among all.

  A CSP instance is called -far from satisfiability if we must remove at least an -fraction of constraints to make it satisfiable.
  A CSP is called testable if there is a constant-time algorithm that distinguishes satisfiable instances from -far instances with probability at least .
  Using the results above, 
  we also derive, under a technical assumption,
  an equivalent condition under which a CSP is testable in the bounded-degree model.
  \end{abstract}

{\bf Key words:} Constant-time approximation, constraint satisfaction problems, linear programmings, rounding schemes, property testing.
\newpage

\section{Introduction}
In a \textit{constraint satisfaction problem} (CSP), 
the objective is to find an assignment to a set of variables that satisfies the maximum number of a given set of constraints on them.
Formally, a CSP~ is specified by a set of predicates over alphabets .
Every instance of  consists of a set of variables ,
and a set of constraints  on them.
Each constraint consists of a predicate from  applied to a subset of variables.
The objective is to find an assignment  to the variables that satisfies the maximum number of constraints. 
A large number of fundamental optimization problems,
such as \textsf{Max Cut} and \textsf{Max -Sat},
are examples of CSPs.

Approximation algorithms for CSPs have been intensively studied.
Goemans and Williamson~\cite{GW95} first exploited semidefinite programmings (SDP) to \textsf{Max Cut} and \textsf{Max 2SAT} achieving the approximation ratio .
After this breakthrough,
plethora of approximation algorithms using SDPs have been developed~\cite{KMS98,LLZ02}.
For inapproximability side,
tight hardness results have been successfully obtained for some important optimization problems such as \textsf{Max 3SAT}~\cite{Has01}.
However,
the approximability of many interesting CSPs such as \textsf{Max Cut} and \textsf{Max 2SAT} remains open.
Towards tightening this gap,
Khot~\cite{Kho02} introduced the Unique Games Conjecture (UGC).
Assuming the UGC,
tight hardness have been shown for \textsf{Max Cut}~\cite{KKMO04}, \textsf{Max 2SAT}~\cite{Aus07},
and \textsf{Max k-CSP}~\cite{AM08,ST06}.
Finally, 
Raghavendra~\cite{Rag08} succeeded to unify and generalize those approximation and inapproximability results for every CSP.
Specifically, Raghavendra showed that,
assuming the UGC,
for every CSP, 
a certain SDP combined with a certain rounding scheme attains the best approximation ratio among all polynomial-time approximation algorithms.
The ingenious technique in the proof is giving a generic conversion from integrality gaps of SDPs to hardness results via the UGC.

In this paper,
we are concerned with constant-time approximation algorithms CSPs.
That is, algorithms are supposed to run in time independent of sizes of instances.
We use the \textit{bounded-degree model},
which was originally introduced for graphs~\cite{GR08}.
In this model,
the number of alphabets, 
the maximum arity (the number of inputs to a predicate),
the maximum degree (the number of constraints where a variable appears),
and the maximum weight of constraints are bounded by constants.
Let  be a -CSP instance.
Since a constant-time algorithm cannot read the whole ,
we assume the existence of an oracle  with which we can get information of .
By specifying a variable  and an index ,  returns a constraint  where  is the -th constraint where  appears.
The efficiency of an algorithm is measured by the number of accesses to , which is called \textit{query complexity}.

In this paper, we show an analogous result to Raghavendra's result:
for every CSP,
a certain linear programming (LP) combined with a certain rounding scheme attains the best approximation ratio among all constant-time approximation algorithms.
Furthermore, our results are \textit{unconditional}.
To give the statements precisely, 
we need to define several notions.
For a -CSP instance  with the variable set  and the constraint set ,
there is a natural generic LP relaxation which we call \blp (see Section~\ref{sec:pre}).
Let  denote the objective value of an optimal solution to \blp for ,
 denote the value of an optimal solution of ,
and  denote the value obtained by an assignment .
We define  as the sum of weights of constraints in .
Then, we define  and .
The \textit{integrality gap curve}  and the \textit{integrality gap}  of a CSP~ is defined as 

The first result of this paper gives a tight approximation algorithm for every CSP.
\begin{theorem}\label{thr:upper}
  In the bounded-degree model,
  for every CSP~ and ,
  there exists an algorithm that,
  given a -CSP instance  with  variables and ,
  with probability at least ,
  outputs a value  such that .
  Also, 
  for some fixed assignment  such that ,
  given a variable  in ,
  it computes  in constant time.
\end{theorem}
The algorithm computes  by rounding an LP solution to \blp for .
Note that, for an instance  with ,
 is the best value we can hope for from the definition of .
Thus, in this sense, we will give a \textit{optimal rounding scheme} for \blp.

We mention that the additive error  cannot be removed.
To see this, 
suppose an instance  consisting of  variables and only one constraint.
Then, we have to see this constraint to approximate  if we do not allow the additive error.
However, it obviously takes  queries.

For hardness side, we show the following.
\begin{theorem}\label{thr:lower}
  In the bounded-degree model,
  for every CSP~ and ,
  there exists a  such that
  any algorithm that,
  given an instance  with  variables and ,
  with probability at least ,
  outputs a value  such that  requires  queries.
\end{theorem}
Note that, using the algorithm in Theorem~\ref{thr:upper}, given an instance ,
we can distinguish the case  from the case 
(Technically, we need that  is non-decreasing, but this is obvious from the definition).
On the contrary, Theorem~\ref{thr:lower} asserts that we cannot distinguish the case  from the case .
Thus, the algorithm given in Theorem~\ref{thr:upper} is not just the best among constant-time approximation algorithm using \blp,
but the best among all constant-time approximation algorithms.

A value  is called an \textit{-approximation} to a value  if it satisfies .
An algorithm is called an \textit{-approximation algorithm} for a CSP~ if,
given a -CSP instance ,
it computes an -approximation to  with probability at least ~\cite{NO08,PR07}.
The following is an immediate corollary achieved by Theorems~\ref{thr:upper} and~\ref{thr:lower}.
\begin{corollary}
  In the bounded-degree model, 
  for every CSP~ and ,
  there exists a constant-time -approximation algorithm for the CSP .
  On the other hand, 
  for every CSP~ and , there exists a  such that 
  any -approximation algorithm for the CSP  requires  queries.
\end{corollary}

Theorem~\ref{thr:lower} has much implication to property testing.
A -CSP instance  is called \textit{satisfiable} if there is an assignment to variables that satisfies all the constraints.
Also,  is called \textit{-far from satisfiability} if we must remove at least  constraints to make it satisfiable,
where  is the maximum degree, the maximum weight, and the number of variables, respectively.
An algorithm is called a \textit{testing algorithm} for (the satisfiability of) a CSP~ if,
given a -CSP instance,
it accepts with probability at least  if the instance is satisfiable,
and rejects with probability at least  if the instance is -far from satisfiability.
Unlike the hardness result given in~\cite{Rag08}, 
Theorem~\ref{thr:lower} holds also for ,
i.e., satisfiable instances.
Using this observation, we have the following theorem.
\begin{theorem}\label{thr:prop}
  In the bounded-degree model, the following holds for a CSP~.
  If , then any testing algorithm for the CSP~ requires  queries.
  If  and  is continuous at ,
  then there exists a constant-time testing algorithm for the CSP~.
\end{theorem}
We mention that Theorem~\ref{thr:prop} gives an ``if and only if'' condition of the testability of CSPs when their integrality gap curves are continuous at the point one
while we are not aware of any CSP for which the curve is not continuous at that point.

We give two direct applications of Theorem~\ref{thr:prop}.
An instance of \textsf{2-SAT} is a CNF formula where each constraint consists of at most two literals.
It is known that , and it follows that we need  queries to test \textsf{2-SAT}.
On the contrary, \textsf{2-SAT} is known to be testable with  queries~\cite{GR99}.
This fact implies that the lower bound in Theorem~\ref{thr:lower} is almost tight.
An instance of \textsf{Horn Sat} is a CNF formula where each constraint has at most one positive literal,
From~\cite{Zwi98}, it is easy to derive that  and  is continuous at .
Thus, \textsf{Horn SAT} is testable in constant time.

\vspace{-10pt}
\paragraph{Related Work:}
Subsequent to Raghavendra's work~\cite{Rag08},
under the UGC,
certain SDPs and LPs are shown to be the best approximation algorithms for several classes of problems,
such as 
graph labeling problems (including \textsf{-Way Cut}, \textsf{-Extension}, and \textsf{Metric Labeling})~\cite{MTRS08},
kernel clustering problems~\cite{KN09},
ordering CSPs (including \textsf{Maximum Acyclic Subgraph})~\cite{GMR08}, 
and strict monotone CSPs (including \textsf{Minimum Vertex Cover})~\cite{KMTV09}.

There have been many studies on constant-time approximation algorithms in the bounded-degree model.
For algorithmic side,
mainly graph problems have been studied,
e.g., \textsf{Minimum Spanning Tree}~\cite{CRT01},
\textsf{Minimum Vertex Cover}~\cite{NO08,PR07,YYI09}, 
\textsf{Maximum Matching}~\cite{NO08,YYI09}, 
\textsf{Maximum Independent Set}~\cite{Alo10},
and \textsf{Minimum Dominating Set}~\cite{NO08,YYI09}.
For inapproximability results of graph problems,
\textsf{Minimum Dominating Set}~\cite{Alo10} and \textsf{Maximum Independent Set}~\cite{Alo10,Yos10} have been considered.
For CSPs,
it is known that, for every , 
there exists  such that any -approximation algorithm for \textsf{Max E2LIN2} and -approximation algorithm for \textsf{Max E3SAT} require linear number of queries~\cite{BOT02}.

We can compute the optimal value of a CSP instance within an additive error  by sampling  variables and by solving the induced problem,
where  is the number of variables and  is the maximum arity~\cite{AdlVKK03,AE02}.
Thus, it is easy to approximate the solution of a dense instance in constant time.
Hence, we are concerned with the bounded-degree model in this paper.

\vspace{-10pt}
\paragraph{Proof Overview:}
We describe a proof sketch of Theorem~\ref{thr:upper}.
Let  be the oracle access to a -CSP instance .
First, 
we construct an oracle access  to a nearly optimal solution to  for ,
Namely, 
if we specify a variable in \blp, 
 outputs its value by accessing  constant number of times.
To this end, we use a distributed algorithm for packing/covering LP given in~\cite{KMW06}.
In the distributed setting,
a linear programing is bound to a graph .
Each primal variable  and each dual variable  is associated with a vertex  and , respectively. 
There are edges between primal and dual vertices wherever the respective variables occur in the corresponding inequality.
Thus,  if and only if  occurs in the -th inequality of the primal.
Let  denote the graph induced by vertices whose distance from  is at most .
Then, a \textit{distributed algorithm in  rounds} works in such a way that each vertex outputs a value of the corresponding variable based on .
In~\cite{KMW06},
it is shown that if the matrix in the LP is ``sparse,''
then there is a distributed algorithm that computes a nearly optimal solution to the LP in  rounds,
where  is an integer determined by the sparsity of the LP.
Suppose that the degree of the graph is bounded by .
Then, given a variable,
we can compute the value of it by performing  queries to  by simulating the process of the distributed algorithm.
With this method, we achieve .
Though \blp is not a packing/covering LP,
after applying several number of transformations,
we get a packing LP that has essentially the same behavior under approximation.
Technically, we need to show that \blp is robust in the sense that
even if we violate each constraint by small amount,
the optimal value does not significantly increase.
We finally mention that, a predicate can return values in  in \cite{Rag08} while it can only return  or  in this paper.
This restriction comes from that we cannot transform \blp to a packing LP anymore if we allow negative values.

Next, we exhibit a solution to the original instance by rounding the LP solution given by .
In~\cite{RS09}, Raghavendra and Steurer considered a certain SDP relaxation, which we call \bsdp,
and showed an optimal rounding scheme for it.
That is, it achieves an approximation ratio coincident with the integrality gap of \bsdp.
Our proof is based on their work.
First, from an instance  and its LP solution,
we create another instance  by merging variables of  that are close in the LP solution
so that the number of variables in  become constant.
Though we cannot explicitly construct the whole  since the number of constraints is not constant,
we can enumerate variables in .
Then, we perform brute force search on .
Specifically, we estimate the value obtained by each assignment to variables in  by accessing the oracle .
Let  be the assignment for  that takes the maximum among them.
Note that  can be unfolded to an assignment  for .
Then, with high probability, we have .
Since, from a variable  in , we can get the corresponding variable in  in constant time,
we can compute  in constant time.
The crucial fact used here is that the LP optimum does not change significantly after merging variables.

Now, we describe a proof sketch of Theorem~\ref{thr:lower}.
Let  be a -CSP instance such that  while  is arbitrarily close to .
Also, let  be the optimal LP solution to  for .
First, we create a distribution of instances  by blowing up variables of .
With high probability, 
an instance  generated by  satisfies that  where  is  an arbitrarily small constant.
Next, using the LP solution ,
we create another distribution of instances , 
which has the property that for all  generated by , 
.
From Yao's minimax principle,
by showing that any deterministic algorithm that distinguishes  from  with high probability requires  queries,
we have the desired result.

\vspace{-10pt}
\paragraph{Organization:}
In Section~\ref{sec:pre},
we give notations and basic technical tools used in this paper.
In Section~\ref{sec:lp},
we present an oracle access  to a (nearly) optimal solution to \blp.
Section~\ref{sec:round} is devoted to describe how to round the LP solution optimally and to prove Theorem~\ref{thr:upper}.
We give proofs of Theorems~\ref{thr:lower} and~\ref{thr:prop} in Section~\ref{sec:lower} and Appendix~\ref{apx:prop}, respectively.

\section{Preliminaries}\label{sec:pre}
\subsection{Definitions}
For an integer ,  denotes the set .
The \textit{arity} of a predicate  is the number of inputs to , i.e.,  here.
The \textit{degree} of a variable is the number of constraints where the variable appears.
For a constraint  in a CSP instance,
 denotes the set of variables in .
Let  be a vector or a set indexed by elements of a set .
For a subset ,
we define .

\begin{definition}
A \textit{bounded-degree constraint satisfaction problem}  is specified by ,
where  is a finite domain,
 is the maximum arity of predicates,
 is the maximum degree of variables, 
 is the maximum weight of predicates,
and  is a set of predicates.
\end{definition}
\begin{definition}
An instance  of a CSP~ is given by ,
where 
\begin{itemize}
\setlength{\itemsep}{0pt}
\item  is a set of variables taking values over ,
\item  is a set of constraints,
  consisting of predicates  applied to sequences  of variables  of size at most .
  More precisely, 
  when a predicate  is applied to a sequence ,
   takes variables  as the input. 
\item  is a set of weights  assigned to each constraint , 
  where .
\end{itemize}
The objective is to find an assignment to variables  that maximizes the total weight of satisfied constraints,
i.e., .
\end{definition}
\begin{definition}[Bounded-degree Model]
  In \textit{the bounded-degree model},
  an algorithm is given a CSP~ and the set of variables  beforehand.
  A -CSP instance  is represented by an oracle  such that , on two numbers ,
  returns  where  is the -th constraint where  appears.
  If no such constraint exists, it returns a special character .
  The \textit{query complexity} of an algorithm is the number of accesses to .
\end{definition}


In this paper, when there is no ambiguity,
symbols  and  are used to denote the parameters of a considered CSP.
Also, symbols  are used to denote the number of variables, the oracle access, and the total weight of an input instance , respectively.

We consider an LP relaxation for a CSP~ as follows, which we call \blp.

Here,  (resp., ) can be seen as a distribution over assignments to a variable  (resp., a constraint ), 
and we often identify them as distributions.
For an LP solution ,
we define  as the value of the objective function of \blp obtained by .
We call an LP solution  \textit{-infeasible} if it satisfies constraints of the form  and  and violates other constraints by at most .
We call a solution to an LP \textit{-approximate} if the objective value obtained by the solution is an -approximation to the optimal value of the LP.

\subsection{Basic Tools}

As a simple application of Hoeffding's inequality, we obtain the following.
\begin{lemma}\label{lmm:stat}
  Suppose that we have an oracle access to a function .
  That is, by specifying  as a query, 
  we can see the value of .
  Then, by querying  times, 
  with probability at least , 
  we can compute a -approximation to .
  \qed
\end{lemma}

Let  be a -CSP instance.
Not surprisingly,
we cannot compute the optimal solution  of \blp for  in constant time.
Even worse, it is also hard to obtain a feasible solution in constant time.
Instead, we will compute a feasible (nearly) optimal solution  of an LP obtained by relaxing equality constraints.
Though this is an infeasible solution in the original LP,
The following lemma states that  is close to .
The proof, which needs Fourier analysis, is given in Appendix~\ref{apx:robust}.
\begin{lemma}[Robustness of \blp]\label{lmm:robust}
  Let  be a -CSP instance.
  Suppose that  is an -infeasible LP solution for  of value .
  Then, it holds that
  
\end{lemma}


\section{A -approximation algorithm for \blp}\label{sec:lp}
In this section, we show the following theorem.
\begin{theorem}\label{thr:lp}
  In the bounded-degree model, 
  given a -CSP instance ,
  for any ,
  we can construct an oracle  that gives an access to an -feasible -approximate solution to \blp for .
  For each query, the number of queries performed to  is at most .
\end{theorem}
A packing LP is a problem of maximizing  subject to  and ,
where  is a non-negative matrix and  are non-negative vectors.
There is a constant-round distributed algorithm to compute a nearly optimal solution to the packing LP (see Appendix~\ref{sec:packing-lp} for a formal statement).
When a variable  or  is specified as a query,
we locally simulate the distributed algorithm and output the value for it.
The only issue is that \blp is not a packing LP.
In this section, we transform \blp to a packing LP, 
and we will show that we can restore a good approximation to \blp from an approximation to the resulting packing LP.
First, we substitute  by  and relax each equality constraint by .
Then, we obtain the following LP.

\begin{lemma}
  Let  be a -CSP instance and  be an -infeasible solution to LP~\eqref{lp:blp-relaxed} of value .
  Then,  holds.
\end{lemma}
\begin{proof}
  Clearly,  is an -infeasible solution to \blp of value .
  From Lemma~\ref{lmm:robust},
  the lemma holds.
\end{proof}

Next, 
to make the directions of the inequalities the same,
we introduce a complement variable for each variable, i.e., we define  and .
However, such equality constraints cannot be used in a packing LP.
Thus, we relax those equality constraints again.
That is, we introduce constraints of the form  and .
Instead, to discourage them to become much smaller than one, 
we add additional terms to the objective function.
By letting  where  is a large constant, we get the following LP.


Fortunately, 
the optimal solutions to LP~\eqref{lp:blp-relaxed-again} and LP~\eqref{lp:blp-relaxed} are essentially the same.
\begin{lemma}[Theorem~7 of~\cite{FS97}, in a special form]\label{lmm:fs97}
  Let  be a -CSP instance and  be the optimal solution to LP~\eqref{lp:blp-relaxed-again} with value  where  is the number of variables in LP~\eqref{lp:blp-relaxed-again}.
  Then,  and  hold.
  Also,  is the optimal solution to LP~(\ref{lp:blp-relaxed}) with value .
  \qed
\end{lemma}

Now, using the distributed algorithm given by~\cite{KMW06}, 
we have the following lemma.
The analysis of the query complexity is tedious and the proof is given in Appendix~\ref{sec:packing-lp}.
\begin{lemma}\label{lmm:packing-lp}
  In the bounded-degree model,
  given a -CSP instance ,
  for any ,
  we can construct an oracle that serves an access to , 
  which is a feasible -approximate solution to LP~\eqref{lp:blp-relaxed-again}.
  For each query, the number of queries performed to  is at most .
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thr:lp}]
  Let  be a feasible -approximate solution obtained by Lemma~\ref{lmm:packing-lp},
  where  is a constant determined later.
  For notational simplicity,
  we write the objective function as  where .
  Let  be the optimal solution to LP~\eqref{lp:blp-relaxed-again}.
  From Lemma~\ref{lmm:fs97}, .
  Also, let  be the number of variables in LP~\eqref{lp:blp-relaxed-again}.
  Then, we have
  
  Thus,
  
  In the former inequality, we used the fact that .
  In the latter inequality, we used the fact that .

  From the former inequality, we have
  
  Let  be the set of variables  ( or ) such that  where  is a constant determined later.
  From Markov's inequality,
  we have .
  Let  and .
  The variables in  and  are problematic since constraints in LP~\eqref{lp:blp-relaxed-again} involving them are far from being satisfied.
  Thus, in what follows,
  we modify these variables and obtain nearly feasible solution to LP~\eqref{lp:blp-relaxed-again}.

  First, we construct variables  by setting  if none of  is in  and  if otherwise.
  Then, we construct variables  as follows.
  If none of  was modified in the previous step,
  we set .
  If otherwise, 
  we set the values of  in such a way that the distribution  becomes consistent with the product distribution determined by .
  Note that each modification to  in the previous step involves at most  modifications to .

  We calculate the decrease of the objective function.
  The decrease caused by the modification to  is at most ,
  and the decrease caused by the modification to  is at most .
  Thus, the total decrease is at most .
  
  Note that for each unmodified variable  ( or ), 
   holds.
  Thus,  is an -infeasible solution with value at least 
  
  Thus,  is an -infeasible -approximate solution.
  By choosing  and ,
  we have an -infeasible -approximate solution.

  We need to look at  variables  to decide the value of ,
  and we need to look at at most  variables  to decide the value of .
  Thus, the number of queries performed to  is at most 
  .
\end{proof}

\section{Optimal Rounding of \blp}\label{sec:round}
In this section, using , we give an algorithm described in Theorem~\ref{thr:upper}.
Let  be a -CSP instance.
For a mapping , 
we define a new -CSP instance  on the variable set  by identifying variables of  that get mapped to the same variable in .
For each constraint  on the variable set  with weight ,
we have a constraint  on the variable set  with weight .
For , 
we define  where  is the positive integer such that .
We define  when .
In what follows, we assume that  is an integer.
If not, we slightly decrease  until  become an integer.
Let  be an LP solution for .
We identify variables  of  that have the same values .
Formally, we consider another -CSP instance  
where  is defined as .
We have following two lemmas, the proofs of which are in Appendix~\ref{apx:round-appendix}.
\begin{lemma}\label{lmm:discretize}
  Let  be a -CSP instance and  be an -infeasible LP solution for .
  Then,  is a -infeasible LP solution for .
\end{lemma}
\begin{lemma}\label{lmm:compression}
  Let  be a -CSP instance and  be an -infeasible -approximate LP solution for ,
  where  is a small constant.
  Then, the variable folding  satisfies that
  \begin{itemize}
    \setlength{\itemsep}{0pt}
  \item ,
  \item The variable set of  has a cardinality .
  \end{itemize}
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thr:upper}]
  Let  be a constant determined later and  be an -infeasible -approximate solution for .
  Consider a folded instance  on the variable set .
  Since there are at most  variables in ,
  there are at most  assignments to .
  For each assignment , 
  we estimate the value  as follows.
  First, we note that  can be unfolded to an assignment  to  with the same value.
  Then, for each variable ,
  we associate a value .
  It is clear that  and .
  Also, we can calculate the value  by querying  at most  times.
  Thus, using the algorithm given in Lemma~\ref{lmm:stat},
  we get a -approximation to  with probability at least  by querying  at most  times.

  By the union bound, 
  with probability at least ,
  we obtain a -approximation to  for every assignment .
  Let  be the assignment that takes the maximum value among those assignments.
  Then,  is a -approximation to .
  The number of queries performed to  is at most .

  Let  be the unfolded assignment of .
  We can safely assume that .
  If not,  is indeed a -approximation to .
  When , it holds that
  
  We are done by setting .
  The number of queries performed to  is at most
  .
  Once we have fixed , given a variable , we can compute  by accessing   times.
  The query complexity is at most .
\end{proof}

\section{Lower Bounds}\label{sec:lower}
In this section, we prove Theorem~\ref{thr:lower}.
As we described in the introduction, we utilize Yao's minimax principle.
That is, we construct two distributions of instances such that they have much different optimal values and also it is hard to distinguish them in constant time.
We fix a -CSP instance  with the optimal LP solution  throughout this section.
To convert the LP integrality gap  of  to hardness results,
we construct two distributions  and  using  and .
Here,  and  will determine the number of variables and the maximum degree of instances generated by  and , respectively.
We show that, by taking  as a large constant (independent of ),
almost all instances  in  satisfy that .
Also, we show that all instances  in  satisfy that .
Finally, we define  as the distribution that chooses  or  randomly and outputs an instance generated by the chosen distribution.
Then, given an oracle access  to an instance  generated by ,
a deterministic algorithm is supposed to guess the original distribution ( or ) of  with probability at least .
By showing that such an algorithm requires  queries,
we conclude that any randomized algorithm that,
given an instance ,
distinguishes the case  from the case 
requires  queries.
By choosing as  an instance with the worst integrality gap,
we have the desired result.
\begin{figure}[t]
  \begin{center}
    \includegraphics[scale=0.5]{lower}
    \caption{Construction of  and .
      Here, the alphabet size , 
      and we choose  and .
      Also, , , , and .
      It follows that , , , and .
      White (resp., black) variables in  indicate that they are assigned to  (resp., ).
    }
    \label{fig:lower}
  \end{center}
\end{figure}
\paragraph{Construction of :}
Before stating the construction of ,
we introduce a distribution  for a constraint  applied to a sequence of variables  (see Fig.~\ref{fig:lower}). 
An instance  of  is generated as follows.
The variable set of  is .
We naturally regard that the set of variables  corresponds to a variable .
Next, we create  constraints among those variables.
To this end, after splitting each variable of  into  copies,
we take random perfect -partite matching in such a way that each matching takes one variable from each .
For each such matching  where  is of the form ,
we create a constraint  of weight .
Finally, we merge the split variables.

We define the distribution  using .
An instance  of  is generated as follows.
For each ,
we create an instance  according to the distribution .
Then,  is a union of  obtained by merging variable sets as follows.
Let  be the set of constraints containing a variable .
We let  denote the set of variables in  corresponding to .
Then, we take random perfect -partite matching among ,
and we merge variables in each matching.
We repeat the same process for every .
We note that the variable set of  is ,
and the number of constraints in  is .
Now, we decide the indices of constraints, 
which are used as arguments of the oracle access .
We use the following rule.
Suppose that  is the -th constraint where  appears (in the sense of ),
then for a variable , 
we randomly assign  indices  to designate  constraints made by .
Finally, labels of vertices are randomly permuted.

\paragraph{Construction of :}
Before stating the construction of ,
again we introduce another distribution  for a constraint  applied to a sequence of variables . (see Fig.~\ref{fig:lower}).
An instance  is generated as follows.
The variable set of  is .
We naturally regard that the set of variables  corresponds to a variable .
For each ,
we take a -fraction of variables from each , and let  denote the set of such variables.
Variables in  are said to be \textit{assigned to }.
A subtlety here is that  may not be an integer.
We ignore this issue for simplicity since we can make the error arbitrarily small by choosing  large enough.
Next, we create  constraints among .
To this end, after splitting each variable into  copies,
we take random perfect -partite matching in such a way that each matching takes one variable from each .
For each matching  where  is of the form ,
we create a constraint  of weight .
Finally, we merge the split variables again.
We note that, for any variable ,
an -fraction of variables of  is assigned to .

We define the distribution  using .
An instance  of  is generated as follows.
For each ,
we create an instance  according to the distribution .
Then,  is a union of  obtained by merging variable sets as follows.
Let  be the set of constraints containing a variable .
We let  denote the set of variables in  that correspond to  and are assigned to .
Note that the sizes of  are the same from the construction of .
We take random perfect -partite matching among  and we merge vertices in each matching.
We repeat the same process for every  and .
Note that the variable set of  is  and the number of constraints in  is .
To decide the indices of constraints and labels of vertices, we use the same rule as .

We have the following three lemmas, 
the proofs of which are in Appendix~\ref{apx:lower-appendix}.
\begin{lemma}\label{lmm:less-than-opt}
  For every , there is a  satisfying the following.
  Let  be a -CSP instance generated by .
  With probability ,
  .
\end{lemma}
\begin{lemma}\label{lmm:more-than-lp}
  Let  be a -CSP instance generated by .
  Then,  holds.
\end{lemma}
\begin{lemma}\label{lmm:distinguish}
  In the bounded-degree model,
  any deterministic algorithm that,
  given an oracle access to  generated by ,
  correctly guesses the original distribution of  with probability at least  requires at least  queries.
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thr:lower}]
  Let us fix  and .
  Then, there exists a -CSP instance  such that  and  is arbitrarily close to .
  Suppose that there exists a deterministic algorithm  with query complexity  that, 
  given an instance  of  variables,
  with probability at least ,
  distinguishes the case  from the case .
  Let  be a constant given by Lemma~\ref{lmm:less-than-opt} by replacing  with .

  Suppose that  is generated by .
  Then, from Lemma~\ref{lmm:less-than-opt}, 
  with probability at least , 
  it holds that .
  In the last inequality, we use the fact that  when  is small.
  Suppose that  is generated by .
  Then, from Lemma~\ref{lmm:more-than-lp},
  it holds that .

  Thus, in total, the algorithm outputs the correct answer with probability at least
  .
  This contradicts Lemma~\ref{lmm:distinguish}.
\end{proof}

\newpage

\begin{thebibliography}{10}

\bibitem{Alo10}
Noga Alon.
\newblock On constant time approximation of parameters of bounded degree
  graphs, 2010.
\newblock manuscript.

\bibitem{AdlVKK03}
Noga Alon, Wenceslas~Fernandez de~la Vega, Ravi Kannan, and Marek Karpinski.
\newblock Random sampling and approximation of {MAX}-{CSPs}.
\newblock {\em Journal of Computer and System Sciences}, 67(2):212--243, 2003.

\bibitem{AE02}
Gunnar Andersson and Lars Engebretsen.
\newblock Property testers for dense constraint satisfaction programs on finite
  domains.
\newblock {\em Random Struct. Algorithms}, 21(1):14--32, 2002.

\bibitem{Aus07}
Per Austrin.
\newblock Balanced {MAX} 2-{SAT} might not be the hardest.
\newblock In {\em Proc.~of STOC 2007}, pages 189--197, 2007.

\bibitem{AM08}
Per Austrin and Elchanan Mossel.
\newblock Approximation resistant predicates from pairwise independence.
\newblock In {\em Proc.~of CCC 2008}, pages 249--258, 2008.

\bibitem{BOT02}
Andrej Bogdanov, Kenji Obata, and Luca Trevisan.
\newblock A lower bound for testing 3-colorability in bounded-degree graphs.
\newblock In {\em Proc.~of FOCS 2002}, pages 93--102, 2002.

\bibitem{CRT01}
Bernard Chazelle, Ronitt Rubinfeld, and Luca Trevisan.
\newblock Approximating the minimum spanning tree weight in sublinear time.
\newblock In {\em Proc.~of ICALP 2001}, pages 190--200, 2001.

\bibitem{FS97}
Dimitris~A. Fotakis and Paul~G. Spirakis.
\newblock Linear programming and fast parallel approximability, 1997.
\newblock manuscript.

\bibitem{GW95}
Michel~X. Goemans and David~P. Williamson.
\newblock Improved approximation algorithms for maximum cut and satisfiability
  problems using semidefinite programming.
\newblock {\em J. ACM}, 42(6):1115--1145, 1995.

\bibitem{GR99}
Oded Goldreich and Dana Ron.
\newblock A sublinear bipartiteness tester for bounded degree graphs.
\newblock {\em Combinatorica}, 19(3):335--373, 1999.

\bibitem{GR08}
Oded Goldreich and Dana Ron.
\newblock Property testing in bounded degree graphs.
\newblock {\em Algorithmica}, 32(2):302--343, 2008.

\bibitem{GT03}
Oded Goldreich and Luca Trevisan.
\newblock Three theorems regarding testing graph properties.
\newblock {\em Random Struct. Algorithms}, 23(1):23--57, 2003.

\bibitem{GMR08}
Venkatesan Guruswami, Rajsekar Manokaran, and Prasad Raghavendra.
\newblock Beating the random ordering is hard: Inapproximability of maximum
  acyclic subgraph.
\newblock In {\em Proc.~of FOCS 2008}, pages 573--582, 2008.

\bibitem{Has01}
Johan H{\aa}stad.
\newblock Some optimal inapproximability results.
\newblock {\em J. ACM}, 48(4):798--859, 2001.

\bibitem{KMS98}
David Karger, Rajeev Motwani, and Madhu Sudan.
\newblock Approximate graph coloring by semidefinite programming.
\newblock {\em J. ACM}, 45(2):246--265, 1998.

\bibitem{Kho02}
Subhash Khot.
\newblock On the power of unique 2-prover 1-round games.
\newblock In {\em Proc.~of STOC 2002}, pages 767--775, 2002.

\bibitem{KKMO04}
Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O'Donnell.
\newblock Optimal inapproximability results for {MAX}-{CUT} and other
  2-variable {CSPs}?
\newblock In {\em Proc.~of FOCS 2004}, pages 146--154, 2004.

\bibitem{KN09}
Subhash Khot and Assaf Naor.
\newblock Sharp kernel clustering algorithms and their associated grothendieck
  inequalities.
\newblock {\em CoRR}, abs/0906.4816, 2009.

\bibitem{KMW06}
Fabian Kuhn, Thomas Moscibroda, and Roger Wattenhofer.
\newblock The price of being near-sighted.
\newblock In {\em Proc.~of SODA 2006}, pages 980--989, 2006.

\bibitem{KMTV09}
Amit Kumar, Rajsekar Manokaran, Madhur Tulsiani, and Nisheeth~K. Vishnoi.
\newblock On the optimality of a class of {LP}-based algorithms.
\newblock {\em CoRR}, abs/0912.1776, 2009.

\bibitem{LLZ02}
Michael Lewin, Dror Livnat, and Uri Zwick.
\newblock Improved rounding techniques for the {MAX} 2-{SAT} and {MAX}
  {DI}-{CUT} problems.
\newblock In {\em Proc.~of IPCO 2002}, pages 67--82, 2002.

\bibitem{MTRS08}
Rajsekar Manokaran, Joseph~(Seffi) Naor, Prasad Raghavendra, and Roy Schwartz.
\newblock {SDP} gaps and {UGC} hardness for multiway cut, 0-extension, and
  metric labeling.
\newblock In {\em Proc.~of STOC 2008}, pages 11--20, 2008.

\bibitem{NO08}
Huy~N. Nguyen and Krzysztof Onak.
\newblock Constant-time approximation algorithms via local improvements.
\newblock In {\em Proc.~of FOCS 2008}, pages 327--336, 2008.

\bibitem{PR07}
Michal Parnas and Dana Ron.
\newblock Approximating the minimum vertex cover in sublinear time and a
  connection to distributed algorithms.
\newblock {\em Theor. Comput. Sci.}, 381(1-3):183--196, 2007.

\bibitem{Rag08}
Prasad Raghavendra.
\newblock Optimal algorithms and inapproximability results for every {CSP}?
\newblock In {\em Proc.~of STOC 08}, pages 245--254, 2008.

\bibitem{RS09}
Prasad Raghavendra and David Steurer.
\newblock How to round any {CSP}.
\newblock In {\em Proc.~of FOCS 2009}, pages 586--594, 2009.

\bibitem{ST06}
Alex Samorodnitsky and Luca Trevisan.
\newblock Gowers uniformity, influence of variables, and {PCPs}.
\newblock In {\em Proc.~of STOC 2006}, pages 11--20. ACM, 2006.

\bibitem{Wor99}
Nick Wormald.
\newblock Models of random regular graphs.
\newblock In {\em Surveys in Combinatorics}, pages 239--298. Cambridge
  University Press, 1999.

\bibitem{Yos10}
Yuichi Yoshida.
\newblock Lower bounds on query complexity for testing bounded-degree {CSPs},
  2010.
\newblock manuscript.

\bibitem{YYI09}
Yuichi Yoshida, Masaki Yamamoto, and Hiro Ito.
\newblock An improved constant-time approximation algorithm for maximum
  matchings.
\newblock In {\em Proc.~of STOC 2009}, pages 225--234, 2009.

\bibitem{Zwi98}
Uri Zwick.
\newblock Finding almost-satisfying assignments.
\newblock In {\em Proc.~of STOC 1998}, pages 551--560, 1998.

\end{thebibliography}

\newpage

\appendix
\noindent {\bf\Large \appendixname}

\section{Robustness of \blp}\label{apx:robust}
In this section, we give a proof of Lemma~\ref{lmm:robust}.
Our strategy is transforming  to a feasible solution without decreasing the LP value much.
In the first step,
we construct  from  that satisfies  for every .
\begin{lemma}\label{lmm:surgery}
  Let  be an -infeasible LP solution for a -CSP instance  where  is a small constant.
  Then,  can be transformed to  so that
  
  In particular, 
   is a -infeasible LP solution that satisfies  for every .
\end{lemma}
\begin{proof}
  We define .
  The condition (\ref{eq:surgery-1}) clearly holds.
  From the -infeasibility of , 
   holds.
  It follows that  when  is small.
\end{proof}

In the second step,
we construct  that satisfies  for all .
\begin{lemma}\label{lmm:smoothing}
  Let  be an -infeasible solution for a -CSP instance  satisfying  for every .
  Then,  can be transformed to  so that
  
  where .
\end{lemma}
\begin{proof}
  Let us fix a predicate  and .
  We may assume  where .
  We can think of  as a function  such that  is the probability of the assignment  under the distribution .

  Let  be an orthonormal basis of the vector space  such that .
  Here, orthonormal means that  for all  where  is Kronecker's delta.
  By tensoring this basis, 
  we obtain the orthonormal basis  of the vector space .
  That is,
  for ,
  we have .
  For a function ,
  we define .
  Note that .
  Therefore, 
  if we let  again be the function corresponding to , 
  we have
  
  Here,  where  and  for all .
  In the second inequality, we used that for every  with  for some ,
  the sum over the values of  vanishes.

  We let  be the function .
  We define a function  as follows.
  
  This is well-defined since for any , 
  it holds that .
  Therefore, the function  satisfies ,
  Then, we can define a distribution  corresponding to ,
  and we have
  
  Thus, it looks that the  is the desired distribution.
  However, in general, 
  the function  might take negative values.
  We will show that these values cannot be too negative and that the function can be made to a proper distribution by smoothing.
  
  Let  be an upper bound on the values of the functions .
  From the orthonormality of the functions, 
  it follows that .
  Let .
  Since the LP solution  is -infeasible,
  we have
  
  Therefore,  for all .
  Recall that  for  if there are  such that .
  Thus,
  
  where .
  Hence, if we let ,
  where  is the uniform distribution ,
  then
  
  It follows that  corresponds to another distribution  over assignments .
  Furthermore, it holds 
  
  Finally, let us estimate the statistical distance between the distributions  and .
  
  The first inequality is from the triangle inequality and the second inequality is from~\eqref{eq:f-f'-close}.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lmm:robust}]
  Let us consider an -infeasible LP solution  for a -CSP instance  of value .
  First, we construct vector  as in Lemma~\ref{lmm:surgery}.
  These variables together with the original local distributions  form an -infeasible LP solution for .
  Next, we construct local distributions  as in Lemma~\ref{lmm:smoothing}.
  Define new variables
  
  It follows that  is a feasible LP solution for .
  The LP value of this solution is
  
  We used  for the first inequality, and the second inequality follows from Lemma~\ref{lmm:smoothing}.
\end{proof}

\section{Proof of Lemma~\ref{lmm:packing-lp}}\label{sec:packing-lp}
In this section, we give a proof of Lemma~\ref{lmm:packing-lp}.
We consider a more restricted form of a packing LP:

where  is a non-negative matrix such that  or  for any ,
and  is a non-negative vector.

Define

Then, there is a distributed algorithm that solves this packing LP.
\begin{lemma}[\cite{KMW06}]\label{lmm:kmw06}
  For sufficiently small ,
  there exists a deterministic distributed algorithm that computes a feasible -approximate solution to LP~\eqref{lp:packing-restricted} in  rounds.
  \qed
\end{lemma}

In order to apply Lemma~\ref{lmm:kmw06} to LP~\eqref{lp:blp-relaxed-again},
we transform it to the form LP~\eqref{lp:packing-restricted}.
Note that, in the objective function,
the coefficient of  is  and the coefficients of  are .
Thus, by replacing  with  and replacing  with , respectively, we obtain the following LP.


We multiply each constraint in order to make every coefficient in the LHS at least .
Then, we have the following LP.


\begin{proof}[Proof of Lemma~\ref{lmm:packing-lp}]
  Note that LP~\eqref{lp:blp-super-relaxed} is of the form LP~\eqref{lp:packing-restricted}.
  After a calculation, we have
  
  We define the \textit{degree} of a variable in an LP as the number of inequalities where the variable appears.
  Let  and  be the maximum degree of primal variables and dual variables, respectively.
  Here, we treat LP~\eqref{lp:blp-super-relaxed} as a dual formulation.
  We have
  

  Applying the algorithm given in Lemma~\ref{lmm:kmw06} to LP~\eqref{lp:blp-super-relaxed},
  we obtain a distributed algorithm that calculates -approximate solution.
  The number of rounds is .
  Note that,
  given a variable,
  we can simulate the computation of the distributed algorithm involved by the variable with  queries,
  where  is the number of rounds.
  Thus, the query complexity becomes
  
\end{proof}

\section{Proofs from Section~\ref{sec:round}}\label{apx:round-appendix}
\subsection{Proof of Lemma~\ref{lmm:discretize}}
\begin{proof}
  Since we move each  by at most ,
  each constraint  can be at most -infeasible.
  Also, each constraint  can be at most -infeasible.
\end{proof}

\subsection{Proof of Lemma~\ref{lmm:compression}}
\begin{proof}
  Since the size of the range of  is , the second claim is obvious.

  Suppose that  has an LP value .
  From the fact that  is a -approximate solution, 
  we have .
  Also, by Lemma~\ref{lmm:discretize}, 
   is a -infeasible LP solution.
  Since only  affects the value of the objective function, 
  the LP value of  equals .
  A key observation is that  is also an LP solution for the folded instance .
  Thus, we see that  has a -infeasible solution of value at least .
  From Lemma~\ref{lmm:robust}, we have
  
  In the last inequality,
  we use the fact that .
\end{proof}


\section{Proofs from Section~\ref{sec:lower}}\label{apx:lower-appendix}


\subsection{Proof of Lemma~\ref{lmm:less-than-opt}}\label{sec:less-than-opt}
Let  be an instance generated by .
Let  be a constraint on a variable sequence  in .
Note that the arities of  are the same since they both are copies of .
For each ,
we choose  arbitrarily and  be the remaining one, i.e., .
Then, we define a constraint  on the variable sequence .
We create another instance  from  by replacing  by .
We call this method \textit{switching}.
The following concentration bound is obtained by a simple application of Theorem~2.19 in~\cite{Wor99}.
\begin{lemma}\label{lmm:chernoff-reg}
  If  is a random variable defined on  such that  holds where  and  are instances of  that only differ by a switching,
  then
  
  for all .
  \qed
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lmm:less-than-opt}]
  Let  be an assignment to .
  For  and , we define .
  Also, for  and , we define .
  Note that  (resp., ) gives a probability distribution over assignments to the variable  (resp., the variable set ).

  Let  be the sub-instance of  generated by  for .
  The expectation (over ) of the value gained by a constraint  in  is .
  Thus, it holds that
  
  Thus, it follows that
  

  Note that, for instances  and  generated by  such that they differ by a switching,
   and  can differ by at most .
  Then, from Lemma~\ref{lmm:chernoff-reg}, 
  
  Then,
  
  The last inequality is from the union bound.

  We choose  so that .
  We have
  
  
  We combine \eqref{eq:less-than-opt-1} and \eqref{eq:less-than-opt-2} with the union bound over all  assignments.
  It holds that
  
  by choosing , we have the desired result.
  Note that  and  can be seen as constants when  is sufficiently large.
\end{proof}

\subsection{Proof of Lemma~\ref{lmm:more-than-lp}}\label{sec:more-than-lp}
\begin{proof}
  Let  be the natural assignment to variables in .
  That is,  when the variable  is assigned to the value  in the construction of .
  Then,
  
\end{proof}

\subsection{Proof of Lemma~\ref{lmm:distinguish}}\label{sec:distinguish}
For notational simplicity, we omit subscripts  and  in this section.
We define some notions.
At each step of an algorithm,
a variable  is called \textit{seen} if  is appeared in queries to the oracle or answers by the oracle so far. 
Also, an index  of a variable  is called \textit{seen} if the -th constraint of  is already returned by the oracle.

Here, we only show a lower bound for a (randomized) algorithm whose behavior is slightly restricted.
That is, when an algorithms asks for a constraint incident to an unseen variable, 
we assume that the algorithm chooses the variable uniformly at random from the set of unseen variables.
We can get rid of this assumption using the technique presented in Section~4 of~\cite{GT03}.
Details are deferred to the full version of the paper.
In what follows, we regard that the oracle accepts two types of queries.
The first one is same as the original, i.e., when we specify a variable  and an index , the oracle returns the -th constraint of .
The second one simply returns a random variable from the set of unseen variables without receiving any argument.
When an algorithm asks for a constraint incident to an unseen variable,
it uses the second type of queries to get a variable first, 
and then it uses the first type of queries to get a constraint incident to the variable.

Now, we prove Lemma~\ref{lmm:distinguish}.
Recall that, from Yao's minimax principle, it suffices to consider deterministic algorithms.
We basically follow the approach presented in Section~7 of~\cite{GR08}.
Let  be a deterministic algorithm.
We introduce a randomized process  (resp.,~, 
which interacts with  so that  (resp.,~) answers queries of  to the oracle while constructing a random instance from  (resp.,~.
The final distribution of instances generated by  (resp.,~) coincides with  (resp.,~) no matter how  makes queries.
The interaction between  and  (resp.,) precisely simulates the interaction between  and  where  is an instance generated by the distribution  (resp.,~).
The process , which corresponds to the distribution ,
is simply a process that chooses  or  randomly and behave as the chosen process.

A \textit{transcript} is the part of an instance that  has seen through the interaction with a randomized process.
Note that, the transcript contains the information about labels of vertices and indices of constraints.
Let  (resp.,~ be the distribution of transcripts after -step interaction between  and  (resp.,~) (Here,  stands for \textit{knowledge}).
The statistical distance between  and  is defined as follows.

From the argument given in Section~7 of~\cite{GR08},
by showing that  when ,
we have the desired result.

We can safely assume that  never asks for the same constraint twice or more.
Also, we assume that,
if  returns a constraint containing a variable in the transcript,
 can correctly guess the process ( or ) with which  is interacting.
In other words,
we are assuming that, 
when  (resp.,~) returns a constraint containing a variable in the transcript,
it also returns a certificate stating that the current process is  (resp.,~).
This only improves the ability of  and makes the lower bound smaller.

Now, we define the randomized process .
We omit the definition of  as it is very similar to the construction of .
The process  has two stages.
The first stage proceeds as long as  perform queries.
In this stage, 
 chooses an answer for each query.
In the second stage, 
the process completes the transcript into an instance .

We identify  (resp., ) with the set of variables of  (resp., an instance generated by ).
Recall that,
in an instance generated by ,
the variable set  can be separated into  sets,
each of which corresponds to a variable .
The process  incrementally constructs this correspondence.
A (partial) correspondences is represented by a map .
For a variable ,
let  and .
Also, for each vertex  and an index ,
let jv and .

In the first stage,
given a query by an algorithm , 
 chooses an answer for it as follows.
\begin{itemize}
\setlength{\itemsep}{0pt}
\item When the query asks for a random unseen variable: 
  we choose a random unseen variable , 
  and set  with probability .
  Then, we return  to .
\item When the query asks for the -th constraint of :
  Note that  from the assumption that, when  asks for a constraint incident to an unseen variable, it asks for a random unseen variable beforehand.
  Let  be such that , and  be the -th constraint of  in ,
  which is applied to a sequence of variables  in  for which  for some .
  Also, let  be such that  is the -th constraint of the variable  in .
  Note that .
  
  Then, we choose a set of variables  as follows.
  For each variable  with ,
  we choose  as  with probability .
  If otherwise, we choose a random unused variable  as  ans set .

  Let  be a constraint applied to a sequence  of weight .
  Finally, we determine indices for each variable .
  We choose a random index  from unused indices in ,
  and set  be as the -th constraint of .
  Then, we return  as the answer for the query.
\end{itemize}
In the second stage of , 
the process uniformly selects an instance  among all those who are consistent with the final transcript.

\begin{lemma}\label{lmm:equivalent}
  For every algorithm , 
  the randomized process  (resp., ) when interacting with , 
  uniformly generates an instance  in  (resp., ).
  \qed
\end{lemma}
\begin{proof}
  The lemma easily follows by induction on the query complexity of .
  The base case is clear since if no query is made, 
  then the distribution on instances generated by  (or, ) is clearly uniform.
  The induction step follows directly from the definition of the process.
  In particular, 
  the distribution on instances resulting from the process switching to the second stage after it answers the query is exactly the same as the distribution resulting from the process performing the second stage without answering the query.
\end{proof}


\begin{proof}[Proof of Lemma~\ref{lmm:distinguish}]
  Let  be a deterministic algorithm.
  It is convenient to think that labels of variables are determined on the fly.
  That is,  decides labels of variables from  at the time when the variable appears for the first time in the interaction between an algorithm and .
  The distribution never change by this modification.
  Also, 
  we can think that the sequence of labels is determined beforehand,
  and for each time when a new variable appears,
  a new label for the variable is taken from the front of the sequence.
  Let  be the process obtained from  by fixing the sequence to .
  It is clear that  coincides with the process that takes  uniformly at random and acts as .
  Let  (resp.,~) be the process obtained from  (resp.,~) by fixing the sequence to .
  Then, it suffices to bound the statistical distance between the distribution of transcripts when  interacts with  and the one when  interacts with  for any sequence .

  A deterministic algorithm  with query complexity  can be expressed as a decision tree of depth at most .
  Here, each node in the decision tree corresponds to a query to the oracle,
  and each branch from the node corresponds to the answer by the oracle.
  Recall that, from the rule of indices,
  if we fix an index,
  the process always returns the same predicate (though the set of vertices to which the predicate is applied should differ).
  Also, 
  since we have fixed the sequence of labels ,
  at each node in the decision tree,  
  there is just one branch corresponding to the case that  finds a constraint such that any variable in the constraint (except the queried variable) is not in the transcript.
  Ignoring branches for which  outputs an answer,
  the decision tree has the property that the number of children of each node is at most one.
  Thus,  is essentially a non-adaptive algorithm.
  Without loss of generality,
  we assume that  outputs that the current instance is generated by  after  steps.
  
  Suppose that the current process is  and  is asking for a constraint incident to some variable in the -th query.
  Note that  has seen at most  variables.
  Then, from the construction of ,
  the probability that  returns a variable in the transcript is at most .
  Using the same argument,
  we can show that,
  in the -th query,
  the probability that  returns a variable in the transcript is at most  where  is the minimum of  except .
  
  Thus, from the union bound,
  after  steps,
  the probability that  returns a variable in the transcript is at most 
  
  Then, the probability that  outputs the correct answer is at most .
  To make this probability at least , we have to choose .
  Note that  is a positive constant independent of .
\end{proof}

\section{Proof of Theorem~\ref{thr:prop}}\label{apx:prop}
\begin{proof}
  We show the first part of the theorem.
  Let  be a CSP such that  for some .
  Suppose that there exists a testing algorithm for the CSP~ with  queries.
  Note that a -far instance  satisfies that .
  Thus, using the testing algorithm, given an instance , 
  with probability at least ,
  we can distinguish the case  from the case .
  However, instantiating Theorem~\ref{thr:lower} with ,
  the theorem asserts that
  any algorithm that, given an instance , 
  with probability at least ,
  distinguishes the case  from the case  requires  queries.
  This is a contradiction.
  
  We show the second part of the theorem.
  Let  be a CSP such that .
  Since  is continuous at ,
  for any , there exists  such that 
  .
  Consider the algorithm obtained by instantiating Theorem~\ref{thr:upper} replacing  with .
  Suppose that  is a satisfiable instance.
  Then, we obtain a value .
  Suppose that  is an instance -far from satisfiability.
  Then, we obtain a value .
  Thus, we can test the satisfiability of the CSP~ in constant time.
\end{proof}


\section*{Acknowledgements}
The author is grateful to Hiro Ito and Suguru Tamaki for valuable comments on an earlier draft of this paper.



\end{document}
