\begin{figure}
    \centering
    \begin{tikzpicture}
            \begin{axis}[
xlabel={\#Training Classes},
                ylabel={Accuracy},
                xmin=20, xmax=100,
                ymin=25, ymax=55,
                xtick={20,40,60,80,100},
                ytick={0,10,20,25,30,35,40,45,50,55},
                legend pos=south east,
                legend style={nodes={scale=0.8, transform shape}},
                ymajorgrids=true,
                grid style=dashed,
            ]
            \addplot[color=blue, mark=square] coordinates {(20,\results{apsr20})(40,\results{apsr40})(60,\results{apsr60})(80,\results{apsr80})(100,\results{apsr100})};\addlegendentry{APSR \cite{liu2019ntu}}
            
            \addplot[color=teal, mark=square] coordinates {(20,\results{sldml20})(40,\results{sldml40})(60,\results{sldml60})(80,\results{sldml80})(100,\results{sldml100})};\addlegendentry{SL-DML \cite{memmesheimer2020signal}}
            
            \addplot[color=red, mark=square] coordinates {(20,\results{sldmlreindex20})(40,\results{sldmlreindex40})(60,\results{sldmlreindex60})(80,\results{sldmlreindex80})(100,\results{sldmlreindex100})};\addlegendentry{Ours}
            \end{axis}
        \end{tikzpicture}
%
     \caption{Result graph for increasing auxiliary set sizes.}
    \label{fig:auxiliary_set_size_graph}
\end{figure}

We used skeleton sequences from the \textit{NTU RGB+D 120} \cite{liu2019ntu} dataset for large scale one-shot action recognition.

The dataset is split into an auxiliary set, representing action classes that are used for training, and an evaluation set.  In the one-shot protocol the evaluation set does only contain novel actions. One sample of each test class serves as reference demonstration. This protocol is based on the one proposed by \cite{liu2019ntu} for the \textit{NTU RGB+D 120} dataset. 
First we trained a model on the auxiliary set. The resulting model transforms skeleton-sequences encoded as an image representation into embeddings for the reference actions and then for the evaluation actions. We then calculate the nearest neighbour from the evaluation embeddings to the reference embeddings. 
As the embeddings encode action similarity we can estimate to which reference sample the given test sample comes closest.
Beside the standard one-shot action protocol and experiments with dataset reduction, we give an ablation study that gives a hint on which combination of embedding size, loss, transformation and representation are yielding best results with our approach.
Further, we integrated various related skeleton-based image representations that have been previously proposed for action recognition into our one-shot action recognition approach to compare them. 

\subsection{Dataset}
\label{ssec:dataset}

\begin{figure*}[th] \centering
\caption{From top to bottom: A RGB Frame, the corresponding skeleton sequences and the image representation of those sequences are shown. The latter is used in our one-shot action recognition approach. The first two sequences contain single person activities, whereas the remaining two contain two person interactions. The \textit{grab other person's stuff} sequence was shorter than the \textit{hugging other person} sequence.}
  \label{fig:examples}
\end{figure*}



\begin{table}[tb]
\caption{One-shot action recognition results on the \textit{NTU RGB+D 120} dataset.}
	\begin{center}
        \small
		\begin{tabular}{lr}
			\toprule
            Approach                                       &  Accuracy [\%]\\
            \toprule
            Attention Network \cite{liu2017global}       &  41.0                   \\ Fully Connected \cite{liu2017global}         &  42.1                   \\ Average Pooling \cite{liu2017skeleton}       &  42.9                   \\ APSR \cite{liu2019ntu}                       &  45.3          \\  TCN \cite{sabater2021one} & \results{sabater100} \\
            SL-DML \cite{memmesheimer2020signal}                  & \results{sldml100} \\ 
            Ours    & \textbf{\results{sldmlreindex100}} \\
            \bottomrule
		\end{tabular}
	\end{center}

	\label{tab:oneshot_results}
\end{table}

\begin{table}[tb]
	\caption{Results for different auxiliary training set sizes for one-shot recognition on the \textit{NTU RGB+D 120} dataset in \%.}
	\begin{center}
        \small
		\begin{tabular}{cccr}
			\toprule
			\#Train Classes   &  APSR \cite{liu2019ntu} & \textit{SL-DML} \cite{memmesheimer2020signal} &  Ours \\
\toprule
                            &     \results{apsr20}  & \textbf{\results{sldml20}}   &\results{sldmlreindex20}   \\ 
                            &     \results{apsr40}  & \textbf{\results{sldml40}}   &\results{sldmlreindex40}  \\ 
                            &     \results{apsr60}  & \textbf{\results{sldml60}}   &\results{sldmlreindex60}   \\ 
                            &     \results{apsr80}  & \results{sldml80}   & \textbf{\results{sldmlreindex80}}   \\ 
			               &     \results{apsr100} & \results{sldml100}  & \textbf{\results{sldmlreindex100}}   \\ \bottomrule
		\end{tabular}
	\end{center}

	\label{tab:ontshot2_results2}
\end{table}

The \textit{NTU RGB+D 120} \cite{liu2019ntu} dataset is a large scale action recognition dataset containing RGB-D image streams and skeleton estimates. 
The dataset consists of 114,480 sequences containing 120 action classes from 106 subjects in 155 different views.
We follow the one-shot protocol as described by the dataset authors. The dataset is split into two parts: an auxiliary set and an evaluation set. The action classes of the two parts are distinct. 100 classes are used for training, 20 classes are used for testing. The unseen classes and reference samples are documented in the accompanied dataset repository\footnote{\url{https://github.com/shahroudy/NTURGB-D}}. \textit{A1, A7, A13, A19, A25, A31, A37, A43, A49, A55, A61, A67, A73, A79, A85, A91, A97, A103, A109, A115} are previously unseen. As reference the demonstration for filenames starting with \textit{S001C003P008R001*} are used for actions with IDs below 60 and \textit{S018C003P008R001*} for actions with IDs above 60.
As no standard validation set is defined in the dataset paper we use the following classes during development for validation: \textit{A2, A8, A14, A20, A26, A32, A38, A44, A50, A56, A62, A68, A74, A80, A86, A92, A98, A104, A110, A116}.
One-shot action recognition results are given in \tabname \ref{tab:oneshot_results}. Like Liu \andothers \cite{liu2019ntu} we also experimented with the effect of the auxiliary set reduction. Results are given in \figname \ref{fig:auxiliary_set_size_graph} and \tabname \ref{tab:ontshot2_results2}. 
In addition we analyze different representations in \tabname\,\ref{tab:ablation_oneshot_ntu_representations} and the influence of different embedding vector sizes, metric losses and augmentations on two representations more detailed in \tabname\,\ref{tab:ablation_oneshot_ntu_embedding_size}.








\subsection{Training Set Size Reduction}
\label{ssec:training_set_reduction}

An interesting question that comes up when evaluating one-shot action recognition approaches is how much training classes are required to get a certain performance. 
Liu \andothers \cite{liu2019ntu} already proposed to evaluate the one-shot action recognition approach with varying training set sizes. Aligned with Liu \andothers \cite{liu2019ntu} we use training sets containing 20, 40, 60, 80 training classes while remaining a constant evaluation set size of 20. For practical systems, where only a limited amount of training data is available, this evaluation can give an important insight about which performance can be achieved with lower amounts of provided training data. It is also interesting to observe how an approach performs when adding more training data. \tabname \ref{tab:ontshot2_results2} and \figname \ref{fig:auxiliary_set_size_graph} give results for different training set sizes for \textit{SL-DML} \cite{memmesheimer2020signal}, \textit{APSR} \cite{liu2019ntu} and our \approachname{} approach, while remaining a static validation set. With just 20 training classes, our approach performs comparably to the \textit{APSR} approach. With a small amount of training classes the \textit{SL-DML} approach performs best. In our experiments \approachname{} performs better when providing a larger training set size. At a training set size of 60 classes, our approach performs comparably well to \textit{SL-DML}. With 80 classes in the training set our approach starts outperforming \textit{SL-DML}. It is interesting to note that, aligned with the results from \textit{SL-DML}, our approach seems to be confused by the 20 extra classes that are added to the 60 classes. 

\subsection{Ablation Study}
\label{ssec:ablation_study}

\begin{table}

    \caption{Ablation study for our proposed one-shot action recognition with different representations, embedding sizes, losses and augmentations. Results are given for a training over 200 epochs. Units are in \%.}
	\begin{center}
        \small
		\begin{tabular}{lrrrrr}
		    \toprule
            Representation                      & 128 & 256 & 512 & Transform & Loss \\
            \toprule
            SL-DML \cite{memmesheimer2020signal} & \textcolor{black}{\results{sldmlswap_transform_default_emb_128_loss_ms_miner_ms_alpha_1.0_beta_0.0}} & \results{sldmlswap_transform_default_emb_256_loss_ms_miner_ms_alpha_1.0_beta_0.0} &
            \results{sldmlswap_transform_default_emb_512_loss_ms_miner_ms_alpha_1.0_beta_0.0} & None & MS \\
            SL-DML \cite{memmesheimer2020signal} &
            \results{sldmlswap_transform_default_emb_128_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & \results{sldmlswap_transform_default_emb_256_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & \results{sldmlswap_transform_default_emb_512_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & None & TM  \\
            SL-DML \cite{memmesheimer2020signal} & \results{sldmlswap_transform_rotation_5_emb_128_loss_ms_miner_ms_alpha_1.0_beta_0.0} & \results{sldmlswap_transform_rotation_5_emb_256_loss_ms_miner_ms_alpha_1.0_beta_0.0} &
            \results{sldmlswap_transform_rotation_5_emb_512_loss_ms_miner_ms_alpha_1.0_beta_0.0} &  Rot & MS  \\
            SL-DML \cite{memmesheimer2020signal} &
            \results{sldmlswap_transform_rotation_5_emb_128_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & \results{sldmlswap_transform_rotation_5_emb_256_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & \results{sldmlswap_transform_rotation_5_emb_512_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & Rot & TM   \\
            Ours &
            \results{sldmlreindex_transform_default_emb_128_loss_ms_miner_ms_alpha_1.0_beta_0.0} & \results{sldmlreindex_transform_default_emb_256_loss_ms_miner_ms_alpha_1.0_beta_0.0} &
            \results{sldmlreindex_transform_default_emb_512_loss_ms_miner_ms_alpha_1.0_beta_0.0} & None & MS  \\
            Ours &
            \results{sldmlreindex_transform_default_emb_128_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & \results{sldmlreindex_transform_default_emb_256_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & \results{sldmlreindex_transform_default_emb_512_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & None & TM\\
            Ours &
            \results{sldmlreindex_transform_rotation_5_emb_128_loss_ms_miner_ms_alpha_1.0_beta_0.0} & \textcolor{black}{\results{sldmlreindex_transform_rotation_5_emb_256_loss_ms_miner_ms_alpha_1.0_beta_0.0}}&
            \textbf{\results{sldmlreindex_transform_rotation_5_emb_512_loss_ms_miner_ms_alpha_1.0_beta_0.0}} & Rot & MS \\
            Ours &
            \textcolor{black}{\results{sldmlreindex_transform_rotation_5_emb_128_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0}} & 
            \results{sldmlreindex_transform_rotation_5_emb_256_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0} & \textcolor{black}{\results{sldmlreindex_transform_rotation_5_emb_512_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0}} & Rot& TM  \\
            
            \bottomrule
		\end{tabular}
	\end{center}

	\label{tab:ablation_oneshot_ntu_embedding_size}
\end{table}

To distill the effects of the components we report their individual contributions. We examine influence of the representation, augmentation method and different resulting embedding vector sizes. 
Inspired by Roth \andothers \cite{roth2020revisiting} we experiment with different embedding vector sizes of 128, 256, 512. In addition we included the \textit{SL-DML} representation, compare a Triplet Margin loss (TM) and a Multi-Similarity loss (MS) and included an augmentation with random rotations of 5. In total 24 models were trained for this ablation study. We trained these models for 200 epochs as we expected longer convergence due to the additional augmented data. Results are given in \tabname \ref{tab:ablation_oneshot_ntu_embedding_size}. In the table we highlight important results. We highlight interesting results by different colors in the table (best result \textcolor{black}{without augmentation (\results{sldmlswap_transform_default_emb_128_loss_ms_miner_ms_alpha_1.0_beta_0.0}\%)}, \textcolor{black}{embedding size of 128 (\results{sldmlreindex_transform_rotation_5_emb_128_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0}\%)}, \textcolor{black}{embedding size of 256 (\results{sldmlreindex_transform_rotation_5_emb_256_loss_ms_miner_ms_alpha_1.0_beta_0.0}\%)}, \textcolor{black}{TM loss (\results{sldmlreindex_transform_rotation_5_emb_512_loss_ms_miner_triplet_margin_alpha_1.0_beta_0.0}\%)}, overall, MS loss, augmentation, embedding size of 512 (\results{sldmlreindex_transform_rotation_5_emb_512_loss_ms_miner_ms_alpha_1.0_beta_0.0}\%)). 
For \textit{SL-DML} the augmentation had a positive influence with higher embedding vector sizes of 512. Whereas the augmentation with embedding sizes of 128 only improved with the TM loss. With the MS loss and a low embedding size the augmentation did lower the result. For our \approachname{} representation the augmentation improved the results throughout the experiments for both losses. The best results without augmentation were achieved by the \textit{SL-DML} representation with an embedding vector of size 128 and a MS loss.
The overall best results were achieved with a MS loss and embedding vector size of 512 and augmentation by rotation using the \approachname{} representation, which improved the results of \skeletondmlaugmentedimpro{} over our approach under a comparable training setup as \textit{SL-DML}.

\subsection{Comparison with Related Representations}

\begin{table}
    \vspace{0.06in}
    \caption{Ablation study for different representations.}
	\begin{center}
        \small
		\begin{tabular}{lr}
		   \toprule
            Representation                      & Accuracy [\%] \\
            \toprule
            Skepxel \cite{liu2019skepxels} & \results{skepxel100}\\
            SkeleMotion Orientation \cite{caetano2019skelemotion} & \results{caetanoorientation100}\\
            SkeleMotion MagnitudeOrientation \cite{caetano2019skelemotion} & \results{caetanomagnitudeorientation100}\\
            TSSI \cite{yang2018action} & \results{tssi100}\\
            Gimme Signals \cite{memmesheimer2020gimme} & \results{gimme_signals100}\\
            SkeleMotion Magnitude \cite{caetano2019skelemotion} & \results{caetanomagnitude100}\\
            SL-DML \cite{memmesheimer2020signal} & \results{sldml100}\\
            Ours & \textbf{\results{sldmlreindex100}}\\
             
            \bottomrule
		\end{tabular}
	\end{center}

	\label{tab:ablation_oneshot_ntu_representations}
\end{table}

To support the effectiveness of our proposed representation in a metric learning setting we compare against other skeleton-based image representations. We use the publicly avail able implementation for the \textit{SkeleMotion} \cite{caetano2019skelemotion}, \textit{SL-DML} \cite{memmesheimer2020signal}, Gimme Signals \cite{memmesheimer2020gimme} and re-implementations of the \textit{TSSI} \cite{yang2018action} and \textit{Skepxels} \cite{liu2019skepxels} representations to integrate them into our metric learning approach. These representations have been described in 
\secname \ref{sec:related_work} more detailed. 

 The overall training procedure was identical as all models were trained with the parameters described in \secname  \ref{ssec:implementation}. The experiment only differed in the underlying representation. Results for the representation comparison  are given in \tabname \ref{tab:ablation_oneshot_ntu_representations}.
While most of the representations initially target action recognition and are not optimized for one-shot action recognition, they are still good candidates for integration in our metric learning approach. 
We did not re-implement the individual architecture proposed by the different representations but decided to use the Resnet18 architecture for better comparability.

Our \approachname{} approach shows best performance followed by \textit{SL-DML}. The \textit{SkeleMotion} Magnitude \cite{caetano2019skelemotion} representation transfers well from an action recognition setting to a one-shot action recognition setting. Interesting to note is that the \textit{SkeleMotion} Orientation  \cite{caetano2019skelemotion} representation, while achieving comparable results in the standard action recognition protocol, performs 10\% worse than the same representation encoding the magnitude of the skeleton joints. 
\textcolor{black}{An early fusion of Magnitude and Orientation on a representation level did not improve the Skelemotion representation but yields a result in between both representations. Similar observations have been made in \cite{memmesheimer2020signal} by the fusion of inertial and skeleton sequences. The lower performing modality adds uncertainty to the resulting model in our one-shot setting.}










A \textit{UMAP} embedding of all evaluation samples is shown in \figname \ref{fig:umap_embedding} for our \approachname{} approach. 
Our approach shows better capabilities in distinguishing the actions \textcolor{throw}{throw} and \textcolor{arm_circles}{arm circles}. In our approach those clusters can be separated quite well whereas \textit{SL-DML} struggles to discriminate those two classes.

\subsection{Result Discussion}
\label{ssec:results}



\begin{figure}
    \centering

    \caption{\textit{UMAP} embedding visualization four our approach. Classes are: drink water \textcolor{drink_water}{},
throw \textcolor{throw}{},
tear up paper \textcolor{tear_up_paper}{},
take off glasses \textcolor{take_off_glasses}{},
reach into pocket \textcolor{reach_into_pocket}{},
pointing to something with finger \textcolor{pointing_to_something_with_finger}{},
wipe face \textcolor{wipe_face}{},
falling \textcolor{falling}{},
feeling warm \textcolor{feeling_warm}{},
hugging other person \textcolor{hugging_other_person}{},
put on headphone \textcolor{put_on_headphone}{},
hush (quite) \textcolor{hush_(quite)}{},
staple book \textcolor{staple_book}{},
sniff (smell) \textcolor{sniff_(smell)}{},
apply cream on face \textcolor{apply_cream_on_face}{},
open a box \textcolor{open_a_box}{},
arm circles \textcolor{arm_circles}{},
yawn \textcolor{yawn}{},
grab other person’s stuff \textcolor{grab_other_persons_stuff}{},
take a photo of other person \textcolor{take_a_photo_of_other_person}{}.}
    \label{fig:umap_embedding}
\end{figure}

We evaluated our approach in an extensive experiment setup. Aside from lower performance on lower amounts of classes for training our approach outperformed other approaches. For fair comparison we report the result of \skeletondmlimpro{} over \textit{SL-DML} for training with 100 epochs and without augmentation, as under these conditions the \textit{SL-DML} result was reported. With augmentation and training for 200 epochs we could improve the baseline for \skeletondmlaugmentedimprosldml. Our approach learns to learn an embedding model that captures semantic relevance from joint movements well. E.g. \approachname{} differentiates well between activities that primarily contain hand- or leg-movements. Interactions between multiple person and single person activities are also separated well. Activities to which similar joint movements contribute to are still challenging. These are the activities that are formed by the main cluster in \figname \ref{fig:umap_embedding}.