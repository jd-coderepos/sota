

\documentclass{article}
\usepackage{dcase2019,amsmath,graphicx,url,times,booktabs,multirow,tabularx,amssymb,multirow}
\usepackage{subcaption}

\def\defeqn{\stackrel{\triangle}{=}}
\newcommand{\symvec}[1]{{\mbox{\boldmath }}}
\newcommand{\symmat}[1]{{\mbox{\boldmath }}}

\title{IMPROVING SOUND EVENT DETECTION IN DOMESTIC ENVIRONMENTS USING SOUND SEPARATION}






\name{
    Nicolas Turpault\thanks{Part of this work was made with the support of the French National Research Agency, in the framework of the  project LEAUDS “Learning to under-stand audio scenes” (ANR-18-CE23-0020) and the French region Grand-Est. Experiments presented in this paper were carried out using the Grid5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000).},
    Scott Wisdom,
    Hakan Erdogan,
    John R. Hershey,
    }
\secondlinename{
Romain Serizel,
      Eduardo Fonseca,
    Prem Seetharaman,
    Justin Salamon
    }
\address{ Universit{\'e} de Lorraine, CNRS, Inria, Loria, F-54000 Nancy, France \\
         Google Research, AI Perception, Cambridge, United States\\
         Music Technology Group, Universitat Pompeu Fabra, Barcelona\\
         Interactive Audio Lab, Northwestern University, Evanston, United States\\
         Adobe Research, San Francisco, United States
 }

\begin{document}

\ninept
\maketitle

\begin{sloppy}

\begin{abstract}
Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods to combine separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the sound separation model to the sound event detection data on both the sound separation and the sound event detection.

\end{abstract}

\begin{keywords}
Sound event detection, synthetic soundscapes, sound separation
\end{keywords}


\section{Introduction}
\label{sec:intro}
Sound event detection (SED) is the task of describing, from an audio recording, what happens and when each single sound event is occurring~\cite{virtanen2018computational}. This is something that we, as humans, do rather naturally to obtain information about what is happening around us. However, trying to reproduce this with a machine is not trivial, as the SED algorithm needs to cope with several problems, including audio signal degradation due to additive noise or overlapping events~\cite{benetos_detection_2016}. Indeed, in real-world scenarios, the recordings provided to the SED systems contain not only target sound events, but also sound events that can be considered as ``noise'' or ``interference.'' Also, several target sound events can occur simultaneously.

In the past, the overlapping sound events problem has been tackled from the classifier point of view. This can be done by training the SED as a multilabel system in which case the most energetic sound events are usually detected more accurately than the rest~\cite{salamon2015feature,serizel_2020}. Some other approaches tried to deal more explicitly with this problem using either a set of binary classifiers~\cite{mesaros2016tut}, using factorization techniques on the input of the classifier~\cite{benetos2016detection,bisot2017overlapping}, or exploiting spatial information when available~\cite{adavanne2018multichannel}. The additive noise problem is usually solved by training SED systems on noisy signals. This may be effective to some degree when the noise level is low, but much less so when the noise level increases~\cite{serizel_2020}.

Sound separation (SS) seems like a natural candidate to solve these two issues.
SS systems are trained to predict the constituent sources directly from mixtures. Thus, sound separation can both decrease the level of interfering noise and enable a SED system to detect quieter events in overlapping acoustic mixtures.
Until recently, SS has been mainly applied to specific classes of signals, such as speech or music. However, recent works has shown that sound separation can also be applied to separating sounds of arbitrary classes, a task known as ``universal sound separation''~\cite{kavalerov2019universal,tzinis2020improving,olvera:hal-02567542}.

In this paper, we propose to combine a universal SS algorithm~\cite{kavalerov2019universal,tzinis2020improving} used as a pre-processing to the DCASE 2020 SED baseline~\cite{turpault2020}. We investigate the impact of the data used to train the SS on the SED performance. We also explore different ways to combine the separated sound sources at different stages of SED.

\section{Problem and baselines description}
\label{sec:pb}

\begin{figure*}
    \centering
\begin{subfigure}[b]{0.16\textwidth}
  \centering
  \includegraphics[width=\columnwidth]{early.png}
  \caption{Early integration.}
\label{fig:sed_early}
\end{subfigure}\hfill
    \begin{subfigure}[b]{0.35\textwidth}
\centering
  \includegraphics[width=\columnwidth]{middle.png}
  \caption{Middle integration.}
\label{fig:sed_middle}
\end{subfigure}\hfill
    \begin{subfigure}[b]{0.41\textwidth}
\centering
  \includegraphics[width=\columnwidth]{late.png}
  \caption{Late integration.}
\label{fig:sed_late}
\end{subfigure}\hfill
\caption{Integration between the SS and SED (gray waveform represents mixture, and colored waveforms represent separated sources).}
\end{figure*}

We aim to solve a problem similar to that of DCASE 2019 Task 4~\cite{turpault_2019}. Systems are expected to produce strongly-labeled outputs (i.e.~detect sound events with a start time, end time, and sound class label), but are provided with weakly labeled data (i.e.~sound recordings with only the presence/absence of a sound event included in the labels without any timing information) for training. Multiple events can be present in each audio recording, including overlapping target sound events and potentially non-target sound events. Previous studies have shown that the presence of additional sound events can drastically decrease the SED performance~\cite{serizel_2020}.

\subsection{Sound event detection baseline}
\label{sub:sed_baseline}
The SED baseline system uses a mean-teacher model which is a combination of two models: a student model and a teacher model (both have the same architecture). The student model is the final model used at inference time, while the teacher model is aimed at helping the student model during training and its weights are an exponential moving average of the student model's weights. A more detailed description can be found in Turpault and Serizel~\cite{turpault2020}.

\subsection{Sound separation baseline}
The baseline SS model uses a similar architecture to an existing approach for universal sound separation with a fixed number of sources \cite{kavalerov2019universal, tzinis2020improving}, which employs a convolutional masking network using STFT and analysis and synthesis.
The training loss is negative stabilized signal-to-noise ratio (SNR) \cite{wisdom2020unsupervised} with a soft-threshold .
Going beyond previous work, the model in this paper is able to handle variable number sources by using different loss functions for active and inactive reference sources that encourage the model to only output as many nonzero sources as exist in the mixture. Additional source slots are encouraged to be all-zero.

\section{Sound event detection and separation}

\label{sec:sed_ss}
\subsection{Sound separation for sound event detection}
Overlapping sound events are typically more difficult to detect as compared to isolated ones.
SS can be used for SED by first separating the component sounds in a mixed signal and then applying SED on each of the separated tracks. The decisions obtained on separated signals may be more accurate than the ones on the mixed signal. On the other hand, separation of sounds is not a trivial problem and may introduce artifacts which in turn may make sound SED harder. So, it is necessary to jointly investigate SS and SED.

\subsection{Sound event detection on separated sources}

In the approaches described here, SS provides several audio clips that contain information related to the sound sources composing the original (mixture) clip. Each of these new audio clips (separated sound sources) are used together with the mixture clip within the SED. We compare three different approaches to integrate the information from these audio clips at different levels of the model.

\subsubsection{Early integration}
This approach is similar to the SED baseline except that all the audio clips (mixture and separated sound sources) are concatenated as input channels to form a new tensor (Figure~\ref{fig:sed_early}). The first channel always contains the mixture clip while the  separated sound source clips are provided with no particular order. The model is trained like the SED baseline using the annotations of the mixture clip. \subsubsection{Middle integration}
We re-use the CNN block from the SED baseline to extract embeddings from the mixture clip and the separated sound sources clips (Figure~\ref{fig:sed_middle}). The embeddings are concatenated along the feature axis and fed into a fully connected layer before training a new RNN classifier within a mean-teacher student framework. \subsubsection{Late integration}
For this approach, we apply the SED baseline on the mixture clip and the separated sound source clips (Figure~\ref{fig:sed_late}). The SED output for each of these clips are obtained from the , the raw outputs of the classifier corresponding to each sound class  among the  sound classes. The combined raw output (before thresholding and post-processing) for each class  is obtained as follows:

where  and  are the raw classifier outputs for the sound class  obtained on the mixture clips and the separated sound source clips, respectively. The sound source/mixture combination weight is . The classifier output for the sound class  is obtained from the raw classifier outputs on each individual separated sound sources as follows:

where  is the number of separated sound source clips obtained from the SS,  is the raw classifier output for the sound class  obtained for the separated sound source clip  and  is the sound sources combination weight.
\section{Baselines setup and dataset}
\label{sec:base_bdd}
\subsection{DESED dataset}
The dataset used for the SED experiments is DESED\footnote{\url{https://project.inria.fr/desed/}}, a flexible dataset for SED in domestic environments composed of 10-sec audio clips that are recorded or synthesized~\cite{turpault_2019,serizel_2020}. The recorded soundscapes are taken from AudioSet~\cite{Gemmeke2017audioset}. The synthetic soundscapes are generated using Scaper~\cite{salamon2017scaper}. The foreground events are obtained from FSD50k~\cite{font2013freesound,fonseca2020fsd50k}. The background textures are obtained from the SINS dataset (activity class ``other'')~\cite{Dekkers2017} and TUT scenes 2016 development dataset~\cite{mesaros_tut_2016}.

The dataset includes a synthetic validation set simulated from different isolated those in the training set (SYN\_VAL) , a validation set  and a public evaluation set composed of recorded clips (REC\_VAL and REC\_EVAL) that are used to adjust the hyper-parameters and evaluate the SED, respectively.


\subsection{FUSS dataset}

The Free Universal Sound Separation (FUSS)\footnote{\url{https://github.com/google-research/sound-separation/tree/master/datasets/fuss}} dataset \cite{wisdom2020fuss} is intended for experimenting with universal sound separation \cite{kavalerov2019universal}, and is used as training data for the SS system.
Audio data is sourced from \url{freesound.org}. Using labels from FSD50k \cite{fonseca2020fsd50k}, gathered through the Freesound Annotator \cite{Fonseca2017freesound}, these source files have been screened such that they likely only contain a single type of sound. Labels are not provided for these source files, and thus the goal is to separate sources without using class information.
To create reverberant mixtures, 10 second clips of sources are convolved with simulated room impulse responses. Each 10 second mixture contains between 1 and 4 sources. Source files longer than 10 seconds are considered "background" sources. Every mixture contains one background source, which is active for the entire duration.

\subsection{Sound event detection baseline}
The SED baseline\footnote{\url{https://github.com/turpaultn/dcase20_task4/}} architecture and parameters are described extensively in Turpault et al.~\cite{turpault2020}. The performance obtained with this baseline on DESED is presented in Table~\ref{tab:sed_base}.

\begin{table}[]
    \centering
    \caption{Performance for the SED baseline~\cite{turpault2020} on DESED.}
\begin{tabular}{|l|c|c|}
\hline & F1-Score & PSDS \\
  \hline
REC\_VAL & 37.8&	0.540\\
REC\_EVAL&39.0 &0.552\\\hline
SYN\_VAL &62.6&0.695\\
\hline
\end{tabular}
\label{tab:sed_base}
\end{table}










\subsection{Sound separation baseline}
The SS system is trained on 16-kHz audio\footnote{ \url{https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline}}. The input to the SS network is the magnitude of the STFT using window size 32ms and hop of 8ms. These magnitudes are processed by an improved time-domain convolutional network (TDCN++) \cite{kavalerov2019universal, tzinis2020improving}, which is similar to Conv-TasNet \cite{luo2019conv}. Like Conv-TasNet, the TDCN++ consists of four repeats of 8 residual dilated convolution blocks, where within each repeat the dilation of block  is  for . The main differences between the TDCN++ and Conv-Tasnet are (1) bin-wise normalization instead of global layer normalization, which averages only over basis frames instead of frames and frequency bins, (2) trainable scalar scale parameters multiplied after each dense layer, which are initialized with , and (3) additional residual connections between blocks, with connection pattern , , , , , .

This TDCN++ network predicts four masks that are the same shape as the input STFT. Each mask is multiplied with the complex input STFT, and a source waveform is computed by applying the inverse STFT. A weighted mixture consistency projection layer \cite{wisdom2019differentiable} is applied to the separated waveforms
to be consistent with the input mixture waveform
where the per-source weights
are predicted by an additional dense layer using the penultimate output of TDCN++. 

To separate mixtures with variable numbers of sources, different loss functions are used for active and inactive reference sources. For active reference sources (i.e. non-zero reference source signals), the soft-threshold for SNR is 30 dB, equivalent to the error power being below the reference power by 30 dB. For non-active reference sources (i.e. all-zero reference source signals), the soft-threshold is 20 dB measured relative to the mixture power, thus gradients are clipped when the error power is 20 dB below the mixture power. Thus, for a -source mixture, a -output model with  should output  non-zero sources, and  all-zero sources.

\subsection{Evaluation metrics}

SS systems are evaluated in terms of scale-invariant SNR (SI-SNR)~\cite{LeRoux2018a}. Since FUSS mixtures can contain one to four sources, we report two scores to summarize performance: multi-source SI-SNR improvement (MSi), which measures the separation quality of mixtures with two or more sources, and single-source SI-SNR (1S), which measures the separation model's ability to reconstruct single-source inputs.

\begin{table}[t]
\centering
\caption{SS and SED performance for FUSS-trained SS models: MSi (multi-source SI-SNR improvement) and 1S (single-source SI-SNR). Confidence intervals: ~1.2 (F1-score) and ~0.015 (PSDS).}
\begin{tabular}{|l|rrrr||c|c|}
\hline&\multicolumn{4}{c||}{FUSS test set}&\multicolumn{2}{c|}{REC\_VAL}\\
FUSS  & \multicolumn{2}{c}{Rev.} & \multicolumn{2}{c||}{Dry}&\multicolumn{2}{c|}{Late Integration}\\

 training & MSi & 1S & MSi & 1S & F1-Score & PSDS \\
  \hline
Rev. & \bf{12.5} & \bf{37.6} & \bf{10.4} & \bf{32.1}
    & 38.2&0.565\\
Dry & 10.4 & 31.2 & 10.2 & 31.8
    &\bf{39.2}&\bf{0.574}\\
\hline
\end{tabular}
\label{tab:ss_sed_fuss}
\end{table}
\begin{table}[t]
    \centering
    \caption{DESED+FUSS tasks.}
    \begin{tabular}{|l|l|}
        \hline
         Task & Sources  \\
         \hline
         DmFm & DESED mix, dry FUSS mix  \\
         BgFgFm & DESED bg, DESED fg mix, dry FUSS mix  \\
         PIT & DESED bg, dry FUSS mix, 5 DESED fg sources \\
         Classwise & DESED bg, 10 DESED classes, dry FUSS mix \\
         GroupPIT & DESED bg, 5 DESED fg sources, 4 dry FUSS srcs \\
         \hline
    \end{tabular}
    \label{tab:desedfuss}
\end{table}



\begin{table*}[t!]
\centering
\caption{SS and SED performance for various SS tasks. ``Bgd'' is DESED background, ``Fgd'' is DESED foreground, and ``Fmx'' is FUSS mixture. Confidence intervals: ~1.2 (F1-score) and ~0.015 (PSDS) on the validation set and ~2.3 (F1-score) on the synthetic set.}
\begin{tabular}{|l|rrr
||c|c|c|c|c|c||c|}
\hline&\multicolumn{3}{c||}{BgFgFm validation}
&\multicolumn{6}{c||}{REC\_VAL}&SYN\_VAL\\
SS training  & \multicolumn{3}{c||}{SI-SNRi (dB)}
& \multicolumn{2}{c|}{Early integration}& \multicolumn{2}{c|}{Middle integration} &  \multicolumn{2}{c||}{Late integration} &Late integration \\
task  & Bg & Fg & Fm
& F1-score & PSDS & F1-Score & PSDS & F1-Score & PSDS& F1-score \\
\hline
DmFm & & & 
    & \bf{35.4}&\bf{0.545}&\bf{35.9}&\bf{0.548}&36.2&0.573&62.3\\
BgFgFm & & & 
    & 35.1&0.529&33.2&0.531&37.7&0.568&\bf{62.6}\\
PIT & & & 
    & 31.6&0.461&33.2&0.472&37.9&\bf{0.574}&62.4\\
Classwise & & & 
    & 27.4&0.361&28.9&0.386&\bf{38.4}&0.566&62.0\\
GroupPIT & & & 
    & 31.6&0.486&32.3&0.473&38.2&0.570&62.2\\
\hline
\end{tabular}
\label{tab:ss_sed_desedfuss}
\end{table*}

\begin{figure*}
\centering
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{Varying_p_sub_sources.png}
    \caption{Impact of  (SYN\_VAL)}
    \label{fig:p_synth}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{Varying_p_sub_sources_validation_real.png}
    \caption{Impact of  (REC\_VAL)}
    \label{fig:p_record}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{Varying_q_sub_sources.png}
    \caption{Impact of  (REC\_VAL)}
    \label{fig:q_record}
\end{subfigure}
\caption{impact of the late integration weights on the SED performance (vertical bars represent confidence intervals)}
\label{fig:p_q_sed}
\end{figure*}

SED systems are evaluated according to an event-based F1-score with a 200~ms collar on the onsets and a collar on the offsets that is the greater of 200~ms and 20\% of the sound event's length. The overall F1-score is the unweighted average of the class-wise F1-scores (macro-average). F-scores are computed on a single operating point (decision thresholds=0.5) using the sed\_eval library~\cite{mesaros_metrics_2016}.


SED systems are also evaluated with poly-phonic sound event detection scores(PSDS)~\cite{Bilen2020}. PSDS are computed using 50 operating points (linearly distributed from 0.01 to 0.99) with the following parameters: detection tolerance parameter (), ground truth intersection parameter (), cross-trigger tolerance parameter (), maximum False Positive rate (). The weight on the cost trigger cost is set to  and the weight on the class instability cost is set to .



\section{Experiments}

Table \ref{tab:ss_sed_fuss} displays SS and SED performance on the FUSS test set and REC\_VAL. For SS we do a full cross-evaluation between the dry and reverberant versions. From this we can see that the reverberant FUSS-trained model achieves the best separation scores across both dry and reverberant conditions. However, in terms of SED performance, the dry FUSS-trained separation model yields the best performance in terms of both F1 and PSDS. This may be due to the synthetic room impulse responses used to create reverberant FUSS being mismatched to the real data in REC\_VAL. Thus, we opt to use the dry version of FUSS in the proceeding experiments.





Besides training SS systems on FUSS, we also constructed a number of tasks consisting of data from both DESED and FUSS, described in Table \ref{tab:desedfuss}. Some tasks are trained with permutation-invariant training (PIT) \cite{yu2017permutation} or groupwise PIT.



Table \ref{tab:ss_sed_desedfuss} reports the results of evaluating these models on the BgFgFm task. For models with more than three outputs, the sources for corresponding classes are summed together. For example, sources 1 through 5 are summed together for the PIT and GroupPIT models, and sources 0 through 9 for the Classwise model, to produce the separated estimate of the DESED foreground mixture. The BgFgFm-trained SS model achieves the best SS scores, since it is matched to the task. This model also achieves the highest F1 score on the SYN\_VAL set, although this is not statistically significant. However, on REC\_VAL, the Classwise model achieves the best F1 score. However, notice that the dry FUSS SS model achieves the overall best F1 and PSDS scores of 39.2 and 0.574 in Table \ref{tab:ss_sed_fuss}. This suggests that the DESED+FUSS-trained SS models do not generalize as well, since they are trained on more specific synthetic data compared to FUSS-trained models.

Figure~\ref{fig:p_q_sed} displays the impact of the late integration parameters  and  on the SED performance. Intuitively when the SS models aims at separating sources that corresponds to target sound events, the parameter  should be high so the source aggregation is close to a max pooling across sources. This is what can be observed on~Fig. \ref{fig:p_synth} for the PIT model. For the FUSS-trained SS separated sources do not correspond to target sources and the integration is better for low values of . This however is not confirmed on REC\_VAL (Fig.~\ref{fig:p_record}). This could be due to the mismatch between training and test for the SS leading to sound sources that are not properly separated.

The SED performance depending on the parameter  is presented on Figure~\ref{fig:q_record}. A high value for the parameter  means focusing only on the mixture or on the separated sounds and leads to degraded performance for all the SS models. The best performance is then obtained with the FUSS-trained SS and  and  (40.7\% F1-score and 0.570 PSDS on REC\_EVAL).












\section{Conclusion}
\label{sec:conc}
In this paper we proposed to use a SS algorithm as pre-processing to a SED system applied to complex mixtures including non-target events and background noise. We proposed to retrain the generic SS on task specific datasets. The combination has shown to have potential to improve the SED performance in particular when using a late integration to combine the prediction obtained from the separated sources. However, the benefits still remain limited most probably because of the mismatch between the SS training conditions and the SED test conditions.

\section{Acknowledgements}
We would like to thank the other organizers of DCASE 2020 task 4: Daniel P. W. Ellis and Ankit Parag Shah.
\bibliographystyle{IEEEtran}
\bibliography{refs}



\end{sloppy}
\end{document}
