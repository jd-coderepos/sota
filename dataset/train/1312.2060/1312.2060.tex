\documentclass{ifacconf}
\usepackage{bbm}
\usepackage{natbib}            \usepackage{graphicx}          \RequirePackage{xspace}
\RequirePackage{amsmath, amssymb, textcomp}
\usepackage{amsmath} \usepackage{amssymb}  \usepackage{graphicx,url}
\usepackage{bbold,dsfont,color}
\newcommand{\T}{\mathsf{T}}
\renewcommand{\b}{{ b}}
\newcommand{\g}{{ g}}
\newcommand{\xx}{{\bf x}}
\renewcommand{\gg}{{\bf g}}
\newcommand{\A}{{\bf A}}
\newcommand{\BB}{{\bf B}}
\newcommand{\Hb}{{\bf H}}
\DeclareMathOperator*\argmin{arg\,min}
\newcommand{\ww}{{\bf w}}
\newcommand{\DD}{{\bf D}}
\newcommand{\J}{{\bf J}}
\newcommand{\bb}{{\bf b}}
\renewcommand{\aa}{{\bf a}}
\renewcommand{\u}{{  u}}
\newcommand{\uu}{{\bf  u}}
\newcommand{\bvarphi}{{\bf \varphi}}
\newcommand{\btheta}{{\bf \theta}}
\newcommand{\X}{{\bf X}}
\newcommand{\Z}{{\bf Z}}
\newcommand{\I}{{\bf I}}
\newcommand{\0}{{\bf 0}}


\newcommand{\yy}{{\bf y}}
\newcommand{\y}{{ y}}
\renewcommand{\a}{a}
\newcommand{\eg}{\textit{e.g.,~}}
\newcommand{\ie}{\textit{i.e.,~}}
\def\subjto{{\mbox{subj. to}}}
\newcommand{\TODO}[1]{{\bf \color{red} #1}}
\renewcommand{\Re}{{\mathbb{R}}}
\newtheorem{df}{Definition}
\newtheorem{proposition}{Proposition}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\begin{document}

\begin{frontmatter}

\title{Blind Identification via Lifting\thanksref{footnoteinfo}} 

\thanks[footnoteinfo]{The work presented is supported by the NSF
Graduate Research Fellowship under grant DGE 1106400, NSF
CPS:Large:ActionWebs award number 0931843, TRUST (Team for Research in
Ubiquitous Secure Technology) which receives support from NSF (award
number CCF-0424422), and FORCES (Foundations Of Resilient
CybEr-physical Systems), the Swedish Research
  Council in the Linnaeus center CADICS, the European Research Council
   under the advanced grant LEARN, contract 267381, a postdoctoral grant from the Sweden-America
   Foundation, donated by ASEA's Fellowship Fund, and  by a postdoctoral
   grant from the Swedish Research Council.}

\author[Ber,Liu]{Henrik Ohlsson} 
\author[Ber]{Lillian Ratliff} 
\author[Ber]{Roy Dong}
\author[Ber]{S. Shankar Sastry}

\address[Ber]{Department of Electrical Engineering and Computer
  Sciences, University of California, Berkeley, CA, USA (e-mail:
  ohlsson@eecs.berkeley.edu).}                                              
\address[Liu]{Division of Automatic Control, Department of Electrical Engineering, Link\"oping University, Sweden.}


          
\begin{keyword}                           System identification; Parameter 
        estimation; Identification
        algorithms. \end{keyword}                             


\begin{abstract}                          Blind system identification  is known to be an ill-posed problem and
without further assumptions, no unique solution is at hand. In this contribution, we are concerned with the task of identifying an ARX model
from only output measurements. 
We phrase this as a constrained
rank minimization problem and present a relaxed convex formulation to approximate
its solution. To make the problem well posed we assume that the sought
input lies in some known linear subspace.
\end{abstract}

\end{frontmatter}

\section{Introduction}


Consider an auto-regressive exogenous input (ARX) model   
 with input  and output .
Estimation of this type of model is one of  the most common tasks in
system identification and a very well studied problem, see for instance \citet{Ljung:99}. 
The common
setting is that   is given and the summed
residuals{\small
}where , is minimized to obtain an estimate for . This estimate is often
referred to as the \textit{least squares} (LS) estimate.


In this paper we study the more complicated problem of
estimating an ARX model from solely outputs . This is
an ill-posed problem and it is easy to see that under no further
assumptions, it would be impossible to uniquely determine
. We will here therefore study this problem under the assumption
that the stacked inputs belong to some known subspace. The input
could for example be:
\begin{itemize}
\item known to change only at a set of discrete times
due to a discrete controller or  
\item known to be band-limited and therefore well represented by the projection
  on the first discrete Fourier transform basis vectors.  
\end{itemize}
It should be noticed that this assumption 
is not enough to uniquely determine the input or the ARX
model. Specifically, we will not be able to decide the input or the
ARX coefficients   more than up to a
multiplicative scalar. It should be stressed that this is not a
limitation of the method that we propose but an inherent
limitation of the system identification problem since the sought quantities
always appear as products. To uniquely determine the input and the
ARX coefficients , further knowledge is needed.  



The main contribution of the paper is a novel method for ARX model
identification from only output measurements. The method takes the
form of a convex optimization problem and gives a computationally  flexible framework
for handling different types of measurement noises, constraints, \textit{etc}.  




\section{Background}
\label{sec:background}

Blind system identifican (BSI) has a broad application area and has
been applied in fields such as  data communications, speech
recognition and seismic signal processing, see for instance
\cite{Abed97}. Common for the type of modeling problems  that BSI has been
applied to is that the input is difficult, costly or impossible to
measure. In for example  exploration seismology, the physical
properties of the earth are explored by studying the response of
 an excitation (often a charge of dynamite). The excitation is often
 difficult to measure and the modeling problem therefore a BSI problem, see \eg \cite{Zerva199947}.

Many methods have been proposed to solve the BSI problem throughout
the years. We give a short overview here but refer the interested
reader to  \cite{Abed97,Hua02}, for a more extensive and complete review. 

The maximum likelihood (ML) approach to BSI aims at finding the ML
estimate of the model and input. The resulting non-convex optimization
problem is often treated by alternating between optimizing with respect to
the input and the system model.  See \cite{meraim:EUSIPCO:1994}. The
channel subspace (CS) methods to BSI  indirectly determine the sought
finite impulse response (FIR)  model by estimating the nullspace of
the Sylvester matrix associated with the FIR model to be identified. This is
done by an eigen decomposition of a matrix derived from the
outputs. See for instance \cite{Abed-Meraim:06}.
The method proposed in \cite{Zerva00} works under the assumption that
two or more output series are available and that these were generated
by the same input. See also \cite{Van13}. The methods proposed in \cite{Sato75,Tong91}
assume that the input consists of  independent and identically
(iid)  distributed random variables and considers the autocorrelation of the
output to decide a FIR model and the unknown
input. 

A number of approaches consider the blind identification problem of
Hammerstein systems under the assumption that the input is piecewise
constant. \cite{Sun99,BaiLD02,Bai02,Wang07,Wang09,Wang10}. Our approach assumes that the input belongs to
some known subspace.  Piecewise constant signal can be represented
using the subspace assumption used here. However, we note that we
are not restricted to piecewise constant signals, and our approach is
significantly different. Also, we consider the blind identification of
ARX models while the blind identification problem of
Hammerstein systems is considered in
\cite{Sun99,BaiLD02,Bai02,Wang07,Wang09,Wang10}.  























The related problem of blind deconvolution have
been studied in a number of contributions. In particular,  see the
very interesting paper by
\cite{Ahmed:12} for a solution where the signals to be recovered are
assumed to be in some known subspaces. The development presented in
\cite{Ahmed:12} has similarities to the approach presented in this
paper and was done in parellel to our work. Note that only FIR models
are discussed in \cite{Ahmed:12}  and that the analysis does not apply.



\section{Problem Formulation}
Given the sequence of outputs , find an estimate for   and   such that  
 for  , where , and , 
some unknown zero mean noise.
We will for simplicity assume that  are
known. To make the problem well posed, we will seek an input in a given
subspace of . 


\section{Notation and Assumptions}

We will use  to denote the output and  the input. We will for
simplicity only consider \textit{single input single output} (SISO)
systems, however with some extra bookkeeping also MIMO systems could
be treated. We will assume
that  measurements of  are available and stack them in the vector
, \ie   We also introduce , , 
and 
 as
 We will use  to
denote the th element of . To pick out a subvector of 
consisting of the th to the th element we will use the notation
 and similarly for picking out a subvector of , 
and . To pick out a submatrix consisting of the th to the
th rows of  we use the notation .
We will use normal font to represent scalars and bold for vectors and
matrices. 
 is the zero (quasi) norm which returns the
number of nonzero elements of its argument and  the
nuclear norm returning the sum of the singular values.




We will assume that it is known that the sought input, , lies in
some known subspace. We can hence write 

for some known -matrix  and an unknown vector . It is assumed that .
\section{Blind Identification via Lifting}




Consider the noise free setting where 
We can formulate the problem of finding an input and the ARX coefficients as the feasibility problem
\label{eq:probform1}
& \hspace*{-2.05cm}\text{find}_{\begin{array}{cc}u(t),\,t=1,\dots,N,\\ \a_1,\dots,
\a_{n_a}, \b_1,\dots, \b_{n_b} \end{array}}  \quad  
\\  \hspace{0cm} \nonumber
  \subjto   \quad y(t)-&\sum_{k_2=1}^{n_a}\a_{k_2} y(t-k_2)
  \\   \label{eq:probform2}  =& \sum_{k_1=1}^{n_b} 
\b_{k_1} u(t-n_k - k_1), \quad  t=n,\dots,N.
 
Note that we need to require that  to not lose the
possibility to decompose  as .
The problem \eqref{eq:probformm} is equivalent to \eqref{eq:probform}  in the following
sense. Assume that \eqref{eq:probformm}, has a unique solution ,
then   must satisfy
, with  and  solving
\eqref{eq:probform}. Extracting the rank 1 component of , using
\eg singular value decomposition, we can hence decide both 
(and ) and
  up to a multiplicative scalar (note that we can never do
better with the information at hand, not even if we would be able to
solve \eqref{eq:probform}). The estimates of  will be
identical for both problems (if the estimates are unique). 

The technique of introducing
the matrix  to avoid products between  and  is well
known in optimization and referred to as \textit{lifting} \citep{shor87,Lovasz91,Nesterov98,Goemans:1995}.


Problem \eqref{eq:probformm} is  a
non-convex optimization problem and  not  easier to solve than
\eqref{eq:probform}. To get an optimization problem we can solve, we
 remove the rank
constraint and instead minimize the rank. Since the rank of a matrix is
not a convex function, we replace the rank with a convex
heuristic. Here we choose the nuclear norm, but other heuristics are
also available (see for instance \cite{Fazel01arank}).  We then obtain the convex
 program   
\min_{\X, \aa}  \quad   \|\X\|_*& 
\\  \nonumber 
 \subjto \quad  y(t) -&\sum_{k_2=1}^{n_a} \a_{k_2}
 y(t-k_2) 
\\ \label{eq:probformm2} =& \sum_{k_1=1}^{n_b} (\DD \X)(t-n_k-k_1,k_1) , \;
 t=n,\dots,N,
and if the noise is Gaussian,

\min_{\X, \aa, {\bf \eta}}  \quad    \|\X\|_*  + &\lambda \|{\bf \eta} \|_2^2 
\\ \nonumber
 \subjto \quad  y(t) -&\sum_{k_2=1}^{n_a} \a_{k_2}
 y(t-k_2)
\\  \label{eq:probformm3} = & \sum_{k_1=1}^{n_b} (\DD \X)(t-n_k-k_1,k_1)+\eta(t), \\
  t=&  n,\dots,N.
In the latter case, we see  as a design parameter  and seek the largest
 such that  becomes rank 1. 



\section{Analysis}
The number of optimization variables  in \eqref{eq:probform} is
essentially , under the assumption that . We can
hence not expect a  reliable identification result from fewer than
  measurements. One may wonder how many measurements that
are needed. Using that the constraint \eqref{eq:probformm2} of BIL is
linear in , we have the following result:
\begin{thm}[Guaranteed Recovery using BIL]\hfill


Consider the noise free blind ARX identification
problem~\eqref{eq:probform} and assume that it has a  unique solution
(up to a multiplicative scalar).
Let the row vector  be the :th row of . 
If  

has full column rank, then the ARX model and input solving
\eqref{eq:probform}  are recovered, up to
a multiplicative scalar, by BIL.
\end{thm}
\begin{pf}
From assumption, \eqref{eq:probform} has a unique solution. Form
 by multiplying the solutions,  and , of
\eqref{eq:probform}. That is,
. Let  denote the
corresponding values for  that solve
\eqref{eq:probform} and define  as  
We must have that
 
Note that the pair  and  is a feasible solution to \eqref{eq:probform3}.  Since the solution to \eqref{eq:uniq} must be  unique since 
has full column rank, we must have that
BIL gives . 
\qed
\end{pf}
Note that if the linear constraints of
\eqref{eq:probform3} alone give the solution of BIL, no optimization
is necessary. Seeking the
matrix  that gives the minimum nuclear norm is only of interest if
we have too few measurements for the constraints to uniquely define
the solution but more
measurements than
.





The noisy case is harder to analyze and we leave the analysis as
future work.

\section{Computing }
In the noisy version \eqref{eq:probform32} of BIL, the design parameter  has to be
chosen. Since  regulates the tradeoff between the nuclear
norm and the squared norm of the estimated noise , it is natural
to seek the largest  such that the estimate  is rank
1. In seeking this , the value for  may come handy.  is defined as the largest 
such that  in BIL. Since the estimate for  will stay the
same for all , we should limit our search of
 to be within  . One may for example start
with  and then successively increase
 as long as . 



  
\begin{thm}[Computing ]\hfill


Consider the optimization
problem given in
\eqref{eq:probform32}. There exists a , denoted
, such that whenever , solving \eqref{eq:probform32} results in
.  is given by:
 1/\lambda^{\text{min}}  =\argmin_{{\bf V}
    \in\Re^{m\times n_b}}  \quad &
  \|{\bf V}\| \\ \nonumber \subjto \quad & 0=  {\bf V }(i,j) - 2 \sum_{t=n}^N\bigg(
y(t)  \\ \label{eq:sip4}-\sum_{k_2=1}^{n_a} &\hat \a_{k_2} 
 y(t-k_2) \bigg) 
   \DD(t-n_k-j,i), \\ & i=1,\dots, m, \,j=1,\dots,n_b,
with 

\end{thm}
\begin{pf}
The noisy version of BIL can be rewritten to take the form

The nuclear norm is not differentiable and it follows that for  to be a valid solution,
zero needs to
be in the subdifferential of the objective with respect to 
evaluated at  (see \eg~\citet[Prop.~4.7.2]{Bert03}).
The subdifferential of the objective of \eqref{eq:sip} at  and
 with
respect to the th element of  can be shown
equal to

 We further have that  from the subdifferential of
 the nuclear norm (see for instance \citet{Watson199233} or \citet{recht10}).  is here  the operator norm (the largest singular
value). To find  we could now consider the
optimization problem 
 \max_{{\bf V}
    \in\Re^{m\times n_b} ,\lambda}  \quad &
 \lambda \\ \nonumber \subjto \quad & 0=  {\bf V }(i,j) - 2 \lambda \sum_{t=n}^N\bigg(
y(t)  \\ \label{eq:sip46}-\sum_{k_2=1}^{n_a} &\hat \a_{k_2} 
 y(t-k_2) \bigg) 
   \DD(t-n_k-j,i), \\ & i=1,\dots, m, \,j=1,\dots,n_b,\\\|{\bf V}&\|\leq 1,
which can be shown equivalent to \eqref{eqlambdamax}.
\qed
\end{pf}


 was also numerically verified. 

\section{Solution Algorithms and Software}

Many standard methods of convex optimization can be used to solve
problem \eqref{eq:probform3},
\eqref{eq:probform332} and \eqref{eq:probform32}. 
Systems such as CVX \citep{cvx1,cvx2} or YALMIP \citep{Yalmip}
can readily handle the nuclear norm. For large scale problems, the \textit{alternating direction
method of multipliers} (ADMM, see \eg \cite{bert:97,boyd:11}) is an attractive choice and we have
previously shown that ADMM can be very efficient on similar
problems \cite{ohlsson:13}. Code for solving
\eqref{eq:probform3}, \eqref{eq:probform332} and \eqref{eq:probform32} will
be made
available on \url{http://www.rt.isy.liu.se/~ohlsson/code.html}





\section{Numerical Illustration}
Consider the system given in the diagram below. \begin{center}
\tikzstyle{int}=[draw,line width=1pt, minimum size=1em]
\tikzstyle{init} = [pin edge={<-,black,line width=1pt},line width=1pt]
\begin{tikzpicture}[node distance=3.5cm,auto, >=latex',line width=1pt]
    \node [int] (a) {\small ZOH};
    \node (b) [left of=a,node distance=1cm, coordinate] {a};
    \node [int, pin={[init]above:},line width=1pt] (c) [right of=a]
    {{\small  }};
    \node [coordinate] (end) [right of=c, node distance=3cm]{};
    \path[->] (b) edge node {} (a);
    \path[->] (a) edge node {} (c);
    \draw[->] (c) edge node {} (end) ;
\end{tikzpicture}\end{center}
\bigskip


Here the values  were generated by independently sampling from a unit Gaussian
and the noise  by independently sampling from  a uniform distribution between
 and . The
ZOH (zero-order hold) block holds the input to the ARX system constant for 6 consecutive
samples. We can therefore express  in terms of  as
We identify the matrix in the relation between  and
 as . 
The ARX coefficients used were  
 
Figure~\ref{fig:ydata}  shows the output  for . 




If the noisy version of  BIL \eqref{eq:probform332} is used to estimate  and an ARX model, we get the input-estimate given in Figure~\ref{fig:udata} and the ARX
coefficients:

It is interesting to notice that if we instead would be given the true input
 and only estimated the ARX coefficients by minimizing the squared
residuals between the output  and the predicted output, we would
have got the estimates:   

As seen, these estimates are not that much better than what BIL is
providing (see \eqref{eq:bilest}). Remember that BIL is
only given the -measurements and not the inputs . It is
therefore quite remarkable that the estimates of BIL is comparable to
those given in \eqref{eq:LSest}. 
\begin{figure}
\includegraphics[width=0.99\columnwidth]{fig/ydata.pdf}
\caption{The noise free (solid line) and noisy outputs (circles).}
\label{fig:ydata}
 \end{figure}
\begin{figure}
\includegraphics[width=0.99\columnwidth]{fig/udata.pdf}
\caption{The estimated (dashed line) and the true input  (solid line).}
\label{fig:udata}
 \end{figure}


To further study the robustness of BIL we carried out a Monte Carlo
simulation. In the simulation, the noise level  was varied
between 0 and 5. For each noise level, 100 trials were carried out
with different noise and input realizations. The true ARX model was
kept fixed (the same as above). The results are summarized in
Figure~\ref{fig:error2}. 

\begin{figure}
 \includegraphics[width=0.99\columnwidth]{fig/error2.pdf}
\caption{The relative errors along with their 0.5 standard deviation
  error bounds for varying noise levels.}
\label{fig:error2}
 \end{figure}

The setup of above example does not give that  has full
column rank. Nevertheless, a perfect result was obtained in the noise free
case.  It can however be verified that if   is instead generated by
independently sampling each element from a unit Gaussian distribution
(everything else unchanged),
the resulting  has full column rank.






\section{Conclusion}
This paper presented a novel framework for blind system identification
of ARX model. The framework uses the fact that the problem can be
rewritten as a rank minimization problem. A convex relaxation is
presented to approximate the sought ARX parameters and the unknown
inputs.  

\bibliography{refHO}
\end{document}