\documentclass[letterpaper,11pt]{article} 
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage{cite}
\usepackage{color}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage[colorlinks,urlcolor=blue,citecolor=blue,linkcolor=blue]{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corol}[theorem]{Corollary}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{invariant}[theorem]{Invariant}




\mathchardef\lee="0214

\newcommand{\wlambda}{\widehat{\lambda}}
\newcommand{\eps}{\varepsilon}
\newcommand{\etal}{et al.}
\newcommand{\eqdef}{:=}
\newcommand{\EX}{\hbox{\bf E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\OPTMAX}{\text{OPT-MAX}}
\newcommand{\OPTCH}{\text{OPT-CH}}

\DeclareMathOperator{\UH}{conv}
\DeclareMathOperator{\seg}{seg}
\DeclareMathOperator{\vis}{vis}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\pen}{pen}
\DeclareMathOperator{\reg}{reg}
\DeclareMathOperator{\lev}{lev}
\DeclareMathOperator{\uss}{uss}
\DeclareMathOperator{\lss}{lss}

\newcommand{\loc}{\text{Locate}}
\newcommand{\findmax}{\texttt{find-max}}
\newcommand{\ins}{\texttt{insert}}
\newcommand{\delete}{\texttt{delete}}
\newcommand{\deckey}{\texttt{decrease-key}}

\newcommand{\bA}{\textbf{A}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\bT}{\textbf{T}}
\newcommand{\bU}{\textbf{U}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}

\newcommand{\marrow}{\marginpar[\hfill]{}}
\newcommand{\niceremark}[3]
   {\textcolor{red}{\textsc{#1 #2:} \marrow\textsf{#3}}}
\newcommand{\ken}[2][says]{\niceremark{Ken}{#1}{#2}}
\newcommand{\sesh}[2][says]{\niceremark{Sesh}{#1}{#2}}
\newcommand{\wolfgang}[2][says]{\niceremark{Wolfgang}{#1}{#2}}


\title{
  Self-improving Algorithms for Coordinate-Wise Maxima and Convex 
  Hulls\footnote{Preliminary versions appeared as
  K. L. Clarkson, W. Mulzer, and C. Seshadhri,
  \emph{Self-improving Algorithms for Convex Hulls} in Proc.~21st SODA, 
  pp.~1546--1565, 2010;
  and K. L. Clarkson, W. Mulzer and C. Seshadhri, 
  \emph{Self-improving Algorithms for
  Coordinate-wise Maxima} in Proc.~28th SoCG, pp.~277--286, 2012.
  }
}

\author{
  Kenneth L. Clarkson\footnote{IBM Almaden Research Center, San Jose, USA.
    \texttt{klclarks@us.ibm.com}
  }
  \and
  Wolfgang Mulzer\footnote{Institut f\"ur Informatik, Freie Universit\"at 
    Berlin, Berlin, Germany.
    \texttt{mulzer@inf.fu-berlin.de} 
  }
  \and
  C. Seshadhri\footnote{Sandia National Laboratories, Livermore, USA.
    \texttt{scomand@sandia.gov}
}
}

\begin{document} 

\maketitle

\begin{abstract}
  Finding the coordinate-wise maxima and the convex 
  hull of a planar point set are probably the most 
  classic problems in computational geometry.  We 
  consider these problems in the \emph{self-improving setting}. 
  Here, we have  distributions  
  of planar points. An input point set  
  is generated by taking an independent sample  
  from each , so the input is distributed 
  according to the product . 
  A \emph{self-improving algorithm} repeatedly gets inputs 
  from the distribution  (which is \emph{a priori} 
  unknown), and it tries to optimize its running time 
  for . The algorithm uses the first few inputs to 
  learn salient features of the distribution , before 
  it becomes fine-tuned to . Let  
  (resp.~) be the expected depth 
  of an \emph{optimal} linear comparison tree computing 
  the maxima (resp.~convex hull) for . 
  Our maxima algorithm eventually achieves expected 
  running time . 
  Furthermore, we give a self-improving algorithm for convex hulls
  with expected running time .

  Our results require new tools for understanding linear 
  comparison trees. In particular, we convert 
  a general linear comparison tree to a restricted 
  version that can then be related to the running time 
  of our algorithms. Another interesting feature 
  is an interleaved search procedure 
  to determine the likeliest point to be extremal 
  with minimal computation. This allows our algorithms
  to be competitive with the optimal algorithm for 
  .
\end{abstract}

\section{Introduction}
\label{sec:intro}

The problems of planar maxima and planar convex hull 
computation are classic computational geometry questions 
that have been studied since at least 1975~\cite{KungLuPr75}. 
There are well-known  time comparison-based 
algorithms ( is the number of input points), with 
matching lower bounds. Since then,
many more advanced settings have been addressed: one can 
get expected running time  for points uniformly 
distributed in the unit square; 
output-sensitive algorithms need  time
for output size ~\cite{KirkpatrickSe86}; 
and there are results for external-memory 
models~\cite{GoodrichTsVeVi93}. 

A major drawback of worst-case analysis
is that it does not always reflect the behavior
of real-world inputs. Worst-case algorithms 
must provide for extreme inputs that 
may not occur (reasonably often) in 
practice. Average-case analysis tries 
to address this problem by assuming some 
fixed input distribution. For example, in the 
case of maxima 
coordinate-wise independence covers a broad
range of inputs, and it leads to a clean 
analysis~\cite{Buchta89}. Nonetheless, it is still
unrealistic, and the right 
distribution to analyze remains a point of 
investigation. However, the assumption of 
randomly distributed inputs is very natural 
and one worthy of further study.

\paragraph{The self-improving model.} 
Ailon~\etal~introduced the self-improving 
model to address the drawbacks of average 
case analysis~\cite{ACCL}. 
In this model, there is a fixed but 
unknown distribution  that 
generates independent inputs, i.e., whole 
input sets . The algorithm initially
undergoes a \emph{learning phase} where 
it processes inputs with a worst-case
guarantee while acquiring information 
about . 
After seeing a (hopefully small) 
number of inputs, the algorithm shifts
into the \emph{limiting phase}. Now, it 
is tuned for , and the expected 
running time is (ideally) \emph{optimal 
for the distribution }. 
A self-improving algorithm 
can be thought of as able to attain 
the optimal average-case
running time for all, or at least 
a large class of, distributions. 

As in earlier work, 
we assume that the input follows a product 
distribution. An input 
is a set of  points in the plane. Each 
 is generated independently from a 
distribution , so the probability 
distribution of  is the product 
. The s themselves
are arbitrary, we only assume that they 
are independent. There are lower 
bounds~\cite{AilonCCLMS11} showing that 
some restriction on  is
necessary for a reasonable self-improving 
algorithm, as we shall explain below.

The first self-improving algorithm was 
for sorting, and it was later extended 
to Delaunay 
triangulations~\cite{CS_self_improve,AilonCCLMS11}.
In both cases, \emph{entropy-optimal} 
performance is achieved in the limiting phase.
Later, Bose~\etal~\cite{BoseDeDoDuKiMo10} 
described \emph{odds-on trees}, a general 
method for self-improving solutions to
certain query problems, e.g., 
point location, orthogonal range searching, 
or point-in-polytope queries.

\section{Results} 
\label{sec:results}

We give self-improving algorithms 
for planar coordinate-wise maxima
and convex hulls over product 
distributions. 
Let  be finite.
A point 
\emph{dominates} ,
if both the - and -coordinate
of  are at least as large as
the - and -coordinate of .
A point in  is \emph{maximal}
if no other point in  dominates
it, and \emph{non-maximal} otherwise.
The \emph{maxima problem} is
to find all maximal points in .
The \emph{convex hull} of  is the
smallest convex set that contains .
It is a convex polygon whose vertices
are points from . 
We will focus on the \emph{upper} hull of ,
denoted by .  A point in 
is \emph{extremal} if it appears on 
, otherwise it is 
\emph{non-extremal}. In the \emph{convex
hull problem}, we must find the extremal
points in .

\subsection{Certificates}

We need to make precise the notion of an \emph{optimal algorithm}
for a distribution . The 
issue with maxima and convex hulls
is their output sensitive nature.
Even though the actual output size
may be small, additional work 
is necessary to determine which
points appear in the output. We also want to consider algorithms
that give a correct output on \emph{all} instances, not just those
in the support of . For example, suppose for all inputs in the support of ,
there was a set of (say) three points that always formed the maxima. The optimal
algorithm just for  could always output these three points. But such an algorithm
is not a legitimate maxima algorithm, since it would be incorrect on other inputs.

To handle these issues, we demand that any algorithm must provide a simple proof
that the output is correct.
This is formalized through \emph{certificates} (see Fig.~\ref{fig:certs}).


\begin{figure}
  \centering
  \includegraphics{figures/certs}

   \caption{Certificates for maxima 
     and convex hulls: (left) both 
      and  are certificates of 
     non-maximality for ; (right) 
     both  and  
     are possible witness pairs 
     for non-extremality of .
   }
  \label{fig:certs}
\end{figure}

\begin{definition}\label{def:cert-max}
  Let   be finite.
  A \emph{maxima certificate}  
  for  consists of \textup(i\textup) 
  the indices of the maximal points 
  in , sorted from left to right; 
  and \textup(ii\textup) a \emph{per-point 
  certificate} for each non-maximal 
  point , i.e., the index 
  of an input point that dominates 
  .  A certificate  is 
  \emph{valid} for  if  
  satisfies conditions (i) and (ii) 
  for .
\end{definition}

Most known algorithms implicitly 
provide a certificate as in 
Definition~\ref{def:cert-max}~\cite{KungLuPr75,Golin94,KirkpatrickSe86}. 
For two points , we 
define the \emph{upper semislab} 
for  and , ,
as the open planar region 
bounded by the upward vertical 
rays through  and  and 
the line segment . 
The \emph{lower semislab} for 
 and , ,
is defined analogously. Two 
points  are a 
\emph{witness pair} for a 
non-extremal  if 
.


\begin{definition}\label{def:cert-ch}
  Let  be finite. A 
  \emph{convex hull certificate}  
  for  has \textup(i\textup) 
  the extremal points in , sorted from 
  left to right; and \textup(ii\textup) a 
  witness pair for each non-extremal point 
  in . The points in  are 
  represented by their indices in .
\end{definition}

To our knowledge, most current maxima and convex hull algorithms implicitly output 
such certificates (for example, when they prune non-extremal points). This is by no means
the only possible set of certificates, and one could design different types of certificates. Our notion
of optimality crucially depends on the definition of certificates. It is not \emph{a priori} clear
how to define optimality with respect to other definitions, though we feel that our certificates are quite
natural.

\subsection{Linear comparison trees}

To define optimality, we need 
a lower bound model to which
our algorithms can be compared.
For this, we use linear algebraic 
computation trees that perform 
comparisons according to query lines 
defined by the input points. 
Let  be a directed line. We write 
 for the open halfplane
to the left of , and  for 
the open halfplane to the right of . 

\begin{definition}\label{def:opt} 
  A \emph{linear comparison tree}  
  is a rooted binary tree. Each 
  node  of  is labeled with 
  a query of the form ``''. 
  Here,  is an input point and 
   a directed line. The 
  line  can be obtained 
  in four ways, in increasing complexity:
  \begin{asparaenum}
    \item\label{type:I} 
      a fixed line independent of 
      the input \textup(but dependent
      on \textup);
     \item\label{type:II} 
       a line with a fixed slope 
       \textup(dependent on \textup) 
       passing through a given input point;
     \item\label{type:III} 
       a line through an input point and 
       a fixed point , dependent on ; or
     \item\label{type:IV} 
       a line through two distinct input points. 
  \end{asparaenum}
\end{definition}

\begin{figure}
  \centering
  \includegraphics{figures/lincomp}
  \caption{
    We can compare with (a) a fixed line;
    (b) a line through input point , 
    with fixed slope ; 
    (c) a line through input  and a 
    fixed point ; and (d) a line through 
    inputs  and .
  }
  \label{fig:lincomp}
\end{figure}

Definition~\ref{def:opt} is illustrated 
in Fig.~\ref{fig:lincomp}. 
Given an input , an 
\emph{evaluation} of a linear 
comparison tree  on  
is the node sequence 
that starts at the root 
and chooses in each step
the child according to the 
outcome of the current 
comparison on . 
For a node  of 
there is a region 
 such 
that an evaluation of  
on input  reaches  if and only 
if . 

Why do we choose this model? 
For starters, it captures the standard ``counter-clockwise" (CCW) primitive. 
This is the is-left-of test that checks
whether a point  lies to the left, 
on, or to the right of the directed line 
, where , , and  are
input points~\cite{deBergChvKrOv08}.
The model also contains simple coordinate comparisons, the usual operation for maxima finding. Indeed, most planar maxima and convex hull algorithms only use these operations. Since we are talking about distributions of points, it also makes sense (in our opinion) to consider comparisons with fixed lines. All our definitions of optimality are dependent on this model,
so it would be interesting to extend our results to more general models. We may consider
comparisons with lines that have more complex dependences on the input points. Or, consider
relationships with more than 3 points. Nonetheless, this model is a reasonable starting point
for defining optimal maxima and convex hull algorithms.

We can now formalize linear
comparison trees for maxima and
convex hulls.

\begin{definition}
  A  linear comparison tree  
  \emph{computes the maxima
  of a planar point set} if
  every leaf  of  is labeled
  with a maxima certificate that is 
  valid for every input .
  A linear comparison tree for planar
  convex hulls is defined analogously.
\end{definition}

The \emph{depth}  of node  in  
is the length of the path from the root 
of  to .  Let   be the 
leaf reached by the evaluation of 
 on input .  The 
\emph{expected depth} of  over 
 is defined as 

For a comparison based algorithm whose
decision structure is modeled by , 
the expected depth of  gives a lower
bound on the expected running time.


\subsection{Main theorems}

Let  be the set of linear comparison
trees that compute the maxima of  points.
We define .
 is a lower bound on the 
expected running time of \emph{any} linear
comparison tree to compute the maxima 
according to . We prove the following
result:


\begin{theorem}\label{thm:main-max} 
  Let  be a fixed constant and 
   continuous planar point 
  distributions. Set .
  There is a self-improving algorithm for 
  coordinate-wise maxima according to 
  whose expected time in the limiting phase is 
  . 
  The learning phase takes  inputs. 
  The space requirement is .
\end{theorem}

We also give a self-improving algorithm 
for convex hulls. Unfortunately, it is 
slightly suboptimal. Like before, we set 
, 
where now  is the set of linear comparison 
trees for the convex hull of  points. 
The conference version~\cite{ClarksonMuSe10}
claimed an optimal result, but the analysis was incorrect. Our new analysis is simpler and 
closer in style to the maxima result.

\begin{theorem}\label{thm:main-ch} 
  Let  be a fixed constant 
  and  
  continuous planar point distributions. 
  Set . There is a 
  self-improving algorithm for convex hulls
  according to  whose expected time in 
  the limiting phase is 
  . 
  The learning phase takes  inputs. 
  The space requirement is .
\end{theorem}

Any optimal (up to multiplicative factor  in running time)
self-improving sorter requires  storage (Theorem 2 of~\cite{AilonCCLMS11}).
By the standard reduction of sorting to maxima and convex hulls, this shows that 
the  space is necessary.
Furthermore, self-improving sorters for arbitrary distributions requires exponential storage (Theorem 2 of~\cite{AilonCCLMS11}). So some 
restriction on the input distribution is necessary for a non-trivial result.

\begin{figure}
  \centering
  \includegraphics{figures/bad}

  \caption{Bad inputs: (i) the 
    upper hull  is fixed, while 
     roughly 
    constitute a random permutation of ; 
    (ii) point  is either at  
    or , so it affects the 
    extremality of the other inputs.}
  \label{fig:bad}
\end{figure}

\paragraph{Prior Algorithms.}
Before we go into the details of
our algorithms, let us explain why
several previous approaches fail.
We focus 
on convex hulls, but the arguments
are equally valid for maxima.
The main problem seems to be
that the previous approaches rely
on the sorting lower bound for 
optimality. However, this 
lower bound does not apply 
in our model.  Refer to 
Fig.~\ref{fig:bad}(i). 
The input comes in two groups: 
the lower group  is not on 
the upper hull, while all points
in the upper group 
are vertices of the upper hull. 
Both  and  have
 points. The input distribution 
 fixes the points 
 to form , 
and for each  with , 
it picks a random point from  
(some points of  may be repeated). 
The ``lower'' points form a random 
permutation of  points from . 
The upper hull is always given by , 
while all lower points have the same
witness pair . Thus 
an optimal algorithm requires  time.

In several other models, the 
example needs  
time. The output size is , so 
output-sensitive algorithms require 
 steps. Also, 
the \emph{structural entropy}
is ~\cite{Barbay}. 
Since the expected size of the
upper hull of a random -subset 
of  is , 
randomized incremental construction 
takes  time~\cite{CS89}. 
As the entropy of the 
-ordering is , 
self-improving algorithms
for sorting or Delaunay
triangulations are not
helpful~\cite{AilonCCLMS11}.
\emph{Instance optimal algorithms}
also require 
steps for each input from our 
example~\cite{AfshaniBC09}: 
this setting 
considers the input as a \emph{set}, 
whereas for us
it is essential to know the distribution of
each individual input point.

Finally, we mention the paradigm of 
preprocessing imprecise
points~\cite{BuchinLoMoMu11,EzraMu13,HeldMi08,LoefflerSn10,vKreveldLoMi10}.
Given a set  of 
planar regions, we must 
preprocess  to 
quickly find the (Delaunay) 
triangulation or convex hull 
for inputs with exactly 
one point from each region 
in .  If we consider inputs 
with a random point from each 
region, the self-improving setting applies,
and the previous results 
bound the expected
running time in the limiting phase.
As a noteworthy side effect, we 
improve a result
by Ezra and Mulzer~\cite{EzraMu13}:
they preprocess a set of
planar lines so that
the convex hull for 
inputs with one point from each
line can be found in near-linear
time. Unfortunately, the data structure
needs quadratic space.
Using self-improvement,
this can now be reduced
to .

\paragraph{Output sensitivity and dependencies.}
We introduced certificates in 
order to deal with output 
sensitivity. These certificates
may or may not be easy 
to find. In Fig.~\ref{fig:bad}(i), 
the witness pairs are all 
``easy''. However, if 
the points of  are placed
\emph{just below} the edges 
of the upper hull, we need 
to search for the witness 
pair of each point , 
for ;
the certificates are
``hard''. Furthermore, 
even though the individual 
points are independent, the 
upper hull can exhibit 
very dependent behavior. 
In Fig.~\ref{fig:bad}(ii),
point  can be either 
 or , while 
the other points are fixed. 
The points  
become extremal \emph{depending} 
on the position of . This 
makes life rather hard for 
entropy-optimality, since only 
if  the ordering of 
 must be determined.

Our algorithm, and plausibly 
any algorithm, performs a 
point location for each input .
If  is ``easily'' shown to be 
non-extremal, the search 
should stop early. 
However, it seems
impossible to know \emph{a priori}
how far to proceed: imagine 
the points  of Fig.~\ref{fig:bad}(i) 
doubled up and placed at \emph{both} 
the ``hard'' and the ``easy'' positions, 
and  for  chosen 
randomly among them. The 
search depth can only be 
determined from the actual 
position. Moreover, the certificates may 
be easy once the extremal points are known,
but finding them is what we wanted in the
first place.

\section{Preliminaries}
\label{sec:prelims}

 Our input point set 
is called 
, 
and it comes from a product
distribution 
. 
All distributions 
are assumed to be continuous.
For , we 
write  and  for the 
- and the -coordinate 
of .  Recall that 
 and  denote
the open halfplanes to
the left and to the right of
a directed line .
If  is measurable, 
a \emph{halving line} 
for  with respect to distribution
 has the property 

If , every 
line is a halving line for . 

We write  for a sufficiently 
large constant.
We say ``with high probability" for 
any probability larger than 
.
The constant in the exponent 
can be increased by increasing
the constant . We will take union 
bounds over polynomially many 
(usually at most ) 
low probability events and still get a 
low probability bound.

The main self-improving algorithms require a significant amount of preperation. This is detailed in Sections \ref{sec:comp}, \ref{sec:rest}, and \ref{sec:data-str}. These sections give some lemmas
on the linear comparison trees, search trees, and useful data structures for the learning phase.
We would recommend the reader to first
skip all the proofs in these sections, as they are somewhat unrelated to the actual self-improving algorithms.

\section{Linear comparison trees} \label{sec:comp}

We discuss  basic properties 
of linear comparison trees.
Crucially,  
any such tree can be
simplified without significant loss in
efficiency (Lemma~\ref{lem:lin->entropy}).
Let  be a linear comparison tree.
Recall that for each node  of , 
there is an \emph{open} region 
 such that 
an evaluation of  on  reaches  
if and only if 
(We define the regions as open,
because the continuous nature of
the input distribution lets us 
ignore the case that a point
lies on a query line.)
We call 
\emph{restricted}, if all nodes 
of depth at most  are of
of lowest complexity, 
i.e., type (\ref{type:I})
in Definition~\ref{def:opt}.
We show 
that in a \emph{restricted} linear 
comparison tree, each  for
a node of depth at most  is 
the Cartesian product of planar polygons. 
This will enable us to analyze each input point
independently.

\begin{prop}\label{prop:Cartesian}
  Let  be a restricted linear 
  comparison tree, and  a node of
   with . There exists a sequence 
  
  of (possibly unbounded) convex planar 
  polygons such that 
  . 
  That is, the evaluation of  on 
   
  reaches  if and only 
  if  for all .
\end{prop}

\begin{proof}
  We do induction on . For the root, 
  set . If 
  , let  be the
  parent of . By induction, there 
  are planar convex polygons  with 
  . 
  As  is restricted,  is 
  labeled with a test 
  ``?'', 
  the line  being independent of 
  . We take  for , 
  and , if  is 
  the left child of , and 
  , otherwise.
\end{proof}

Next, we restrict linear comparison 
trees even further, so that the depth 
of a node  relates to the probability 
that  is reached by a random input 
. This allows us to
compare the expected running time of our algorithms 
with the depth of a near optimal tree.

\begin{definition}
  A restricted 
  comparison tree is 
  \emph{entropy-sensitive} if the 
  following holds for any node 
  with :
  let 
  and  labeled 
  ``\textup{?}''.
  Then  is a halving line for .
\end{definition}

The depth of a node in an entropy-sensitive
linear comparison tree is related to the 
probability that it is being visited:

\begin{prop}\label{prop:entropyDepth}
  Let  be a node in an entropy-sensitive 
  tree with , and 
  .  Then, 
  
\end{prop}

\begin{proof}
  We do induction on . 
  The root has depth  and all 
  probabilities are . The claim 
  holds.  Now let  and 
   be the parent of . 
  Write  
  and . 
  By induction, 
  . 
  Since  is entropy-sensitive,  is 
  labeled  ``?'', 
  where   is a halving line
  for , i.e.,
  
  Since , for , and 
   or 
  ,
  it follows that
  
\end{proof}

We prove that it suffices to restrict our attention to entropy-sensitive comparison trees. 
The following lemma is crucial to the proof, as it gives handles on  and .

\begin{lemma}\label{lem:lin->entropy}
  Let  be a finite linear comparison 
  tree of worst-case depth , and  a product distribution over 
  points. There is an entropy-sensitive 
  tree  with expected depth 
  , as 
  .
\end{lemma}

This is proven by converting  
to an entropy-sensitive comparison 
tree whose expected depth is only 
a constant factor worse.  This is done in two steps. The first, more technical
step (Lemma~\ref{lem:lin->restricted}), goes from linear comparison trees to restricted comparison trees.
The second step goes from restricted comparison trees to entropy-sensitive trees (Lemma~\ref{lem:restricted->entropy}).
Lemma~\ref{lem:lin->entropy} follows immediately from Lemmas~\ref{lem:lin->restricted} and~\ref{lem:restricted->entropy}.

\begin{lemma}\label{lem:lin->restricted}
  Let  be a linear 
  comparison tree of worst-case
  depth  and  a product
  distribution. There is a restricted 
  comparison tree  with expected depth 
  , as 
  .
\end{lemma}

\begin{lemma}\label{lem:restricted->entropy}
  Let  a restricted linear comparison 
  tree. 
  There exists an entropy-sensitive comparison
  tree  with expected depth .
\end{lemma}


For convenience, we move the proofs to a separate subsection.

\subsection{Proof of Lemmas~\ref{lem:lin->restricted} and~\ref{lem:restricted->entropy}}
\label{sec:lin->restricted}

The heavy lifting is done by representing a single 
comparison by a restricted linear comparison 
tree, provided that  is drawn from a 
product distribution. The final transformation simply
replaces each node of  by the subtree given 
by the next claim. For brevity, we omit the 
subscript  from . 

\begin{claim} \label{clm:node} 
  Consider a comparison  as 
  in Definition~\ref{def:opt}. 
  Let  be a product distribution 
  for  with each  drawn 
  from a polygonal region . If  is not of
  type \textup(\ref{type:I}\textup), there is
  a restricted linear comparison tree  
  that resolves  with expected depth 
   (over ) and worst-case
  depth .
\end{claim}
\begin{figure}
  \centering
  \includegraphics{figures/compreplace}
  \caption{The different cases in the proof of Claim~\ref{clm:node}.}
  \label{fig:compreplace}
\end{figure}
\begin{proof}
We distinguish several cases 
according to Definition~\ref{def:opt}; 
see Fig.~\ref{fig:compreplace}.

\noindent\textbf{ is of type (\ref{type:II}).} 
We must determine 
whether the input point  lies 
to the left of the directed line 
with slope  
through the input .
This is done through binary search. 
Let  be the region in 
 corresponding to ,  
and  a halving line for 
 with slope~. We do 
two comparisons to determine 
on which side of  the inputs 
 and  lie. If they
lie on different sides, 
we can resolve the original comparison.
If not, we replace 
 with the new 
region and repeat. 
Every time, the success 
probability is at least .
As soon as the depth exceeds
, we use the original
type (\ref{type:II}) comparison.
The probability of reaching
a node of depth  is 
, so the expected 
depth is . 
   
\noindent\textbf{ is of type (\ref{type:III}).} 
We must determine whether 
the input point  lies 
to the left of the directed line 
through the input  
and the fixed point .
We partition the plane by a 
constant-sized family of cones
with apex , such that 
for each cone  in the family,
the probability that line 
 meets  
(other than at )
is at most . Such 
a family can be 
constructed by a sweeping a 
line around , or by
taking a sufficiently large, but 
constant-sized, sample from the 
distribution of , and 
bounding the cones by all lines 
through  and each point of 
the sample. As such a construction 
has a positive success probability,
the described family
of cones exists.
  
We build a restricted tree 
that locates a point in the 
corresponding cone, and
for each cone , we recursively 
build such a family of cones 
inside , together with a restricted
tree. Repeating for 
each cone, this gives an infinite 
restricted tree . We search 
for both  and  in . 
Once we locate them in 
two different cones of the same 
family, the comparison is resolved.
This happens with probability at least
, so 
the probability that the evaluation 
needs  steps is 
. Again, we revert
to the original comparison once the
depth exceeds .

\noindent\textbf{ is of type (\ref{type:IV}).} 
We must determine whether the 
input point  lies to the 
left of the directed line 
through inputs  and .
We partition the plane by a 
constant-sized family of triangles 
and cones, such that for each
region  in the family, the 
probability that the line 
 meets  is at 
most . Such a family can 
be constructed by taking a 
sufficiently large random sample 
of pairs , 
and by triangulating the arrangement 
of the lines through each pair. The
construction has positive success 
probability, so such a family exists. 
(Other than the source of the random 
lines, this 
scheme goes back at least to \cite{Clarkson87}; 
a tighter version, called \emph{cutting}, 
could also be used \cite{Chazelle93}.) 
 
Now suppose  is in region  of 
the family. If the line  
does not meet , the comparison
is resolved. This occurs with probability 
at least . Moreover, finding the 
region containing  takes a 
constant number of type (\ref{type:I})
comparisons.  Determining
if  meets  can be done 
with a constant number of
type (\ref{type:III}) comparisons: 
suppose  is a triangle. 
If , then  
meets . Otherwise, suppose  
is above all lines through  and each
vertex of ; then  does not 
meet . Also, if 
is below all lines through  and 
each vertex, then 
does not meet . Otherwise,  
meets . We replace each type (\ref{type:III})
query by a type (\ref{type:I})
tree, cutting off after  levels.
 
By recursively building a tree for each
region  of the family, comparisons of 
type (\ref{type:IV}) can be reduced to
a tree of depth 
whose nodes of depth at most  
use comparisons of 
type (\ref{type:I}) only. Since 
the probability of 
resolving the comparison 
with each family of regions that 
is visited, the expected
number of nodes visited is constant.
\end{proof}

Given Claim~\ref{clm:node}, we can 
now prove Lemma~\ref{lem:lin->restricted}.

\begin{proof}[Proof of Lemma~\ref{lem:lin->restricted}]
We incrementally transform 
 into .
In each step, we have 
a partial restricted 
comparison tree  
that eventually becomes .
Furthermore, during the 
process each node of 
 is in one of 
three different states:
\emph{finished}, \emph{fringe}, 
or \emph{untouched}. 
We also have a function 
that assigns to each finished and 
fringe node of  a subset 
 of nodes in .
The initial situation is 
as follows: all nodes of  
are untouched except for the 
root, which is fringe. 
The partial tree 
 has a single 
root node , and the function
 assigns the root of  
to the set  .

The transformation proceeds as 
follows: we pick a fringe node 
 in , and mark it as finished.
For each child  of , if  
is an internal node of , we mark 
it as fringe. Otherwise, we mark 
 as finished.
For each node ,
if  has depth more than ,
we copy the subtree of  in
 to a subtree of  in
.
Otherwise, 
we replace   
by the subtree given 
by Claim~\ref{clm:node}.  This
is a valid application of the claim, 
since  is a node of , a 
restricted tree. 
Hence  is a 
product set, and the distribution
 restricted to  is 
a product distribution.
Now  contains the roots of 
these subtrees. Each leaf of each such
subtree corresponds to an outcome of 
the comparison in .  
For each child  of , we define 
 as the set of all such leaves 
that correspond to the same outcome 
of the comparison as . 
We continue this process until 
there are no fringe nodes left. 
By construction, the resulting 
tree  is restricted. 

It remains to argue that 
. 
Let  be a node of . 
We define two random variables 
 and :  is 
the indicator random variable 
for the event that the node
 is traversed for a random 
input .  
The variable  denotes 
the number of nodes traversed 
in  that correspond to  
(i.e., the number of nodes
needed to simulate the comparison 
at , if it occurs).
We have 
, 
because if the leaf corresponding
input  has depth , exactly 
 nodes are traversed to reach it.
We also have 
, 
since each node in  corresponds 
to exactly one node  in . 
Claim~\ref{clm:YvBound} below 
shows that , 
completing the proof.
\end{proof}

\begin{claim}\label{clm:YvBound} 
  
\end{claim}

\begin{proof}
Note that 
.
Since the sets , , 
partition , 
we can write  as

Since  if , 
we have . 
Also, . 
Furthermore, by Claim~\ref{clm:node}, 
we have .
The claim follows.
\end{proof}

Lemma~\ref{lem:restricted->entropy} is proven using a similar construction.

\begin{proof} (of Lemma~\ref{lem:restricted->entropy}) The original tree is
restricted, so all queries are of the form , where 
only depends on the current node. Our aim is to only have queries with halving lines.
Similar to the 
reduction for type (\ref{type:II}) 
comparisons in Claim~\ref{clm:node}, 
we use binary search: 
let  be a halving line
for  parallel to . We 
compare  with . If 
this resolves the original comparison, 	
we are done. If not, we 
repeat with the halving 
line for the new region 
stopping after  steps. 
In each step, the success probability
is at least , so the resulting 
comparison tree has constant expected 
depth. We apply the construction
of Lemma~\ref{lem:lin->restricted} 
to argue that for a restricted 
tree  there is an entropy-sensitive 
version  whose
expected depth is higher by at 
most a constant factor.
\end{proof}


\section{Search trees and restricted searches} \label{sec:rest}

We introduce the central notion of 
\emph{restricted searches}. For this
we use the following more abstract setting:
let  be an ordered finite set 
and  be a distribution over  
that assigns each element , 
a probability .
Given a sequence  of numbers 
and an interval , we write  
for .
Thus, if  is an interval of , 
then  is the total probability of .

\begin{figure}
  \centering
  \includegraphics{figures/structures}
  \caption{(left) A universe of size  and a 
  search tree. The nodes are ternary, 
  with at most two internal children.
  Node  represents the interval 
  .
  (right) A vertical slab structure with 
   leaf slabs (including the left and
  right unbounded slab). 
   is a  slab with
   leaf slabs, .  }
 \label{fig:structures}
\end{figure}

Let  be a search tree over . We think 
of  as (at most) ternary, each node having at 
most two internal nodes as children.
Each internal node  of  is associated
with an interval  so that
every element in  has  on its 
search path; see Fig.~\ref{fig:structures}. 
In our setting,  is 
the set of leaf slabs of a slab structure 
; see Section~\ref{sec:data-str}.
We now define restricted searches.

\begin{definition}\label{def:rest}
  Let   be an interval.
  An \emph{-restricted distribution} 
   assigns to each 
  the probability , 
  where   fulfills , 
  if ; and , otherwise.  

  An \emph{-restricted search} for  
  is a search for  in  that terminates 
  as soon as it reaches the first node  with 
  .
\end{definition}

\begin{definition}\label{def:tree}
  Let .  A search tree 
   over  is \emph{-reducing} 
  if for any internal node  and 
  for any non-leaf child  of , we 
  have . 

  The tree  is \emph{-optimal 
  for restricted searches over } if 
  for every interval  
  and every -restricted distribution 
  , the expected time of an -restricted 
  search over  is at most . 
  (The values  are as in Definition~\ref{def:rest}.)
\end{definition}

Our main lemma states that a
search tree that is near-optimal for 
 also works for
restricted distributions. 

\begin{lemma}\label{lem:search-time} 
  Let  be a -reducing search tree 
  for . Then  is -optimal 
  for restricted searches over .
\end{lemma}

\subsection{Proof of Lemma~\ref{lem:search-time}}\label{sec:restricted}

We bound the expected number 
of visited nodes in an 
-restricted search. 
Let  be a node of .  
In the following, we use  
and  as 
a shorthand for the values 
 and . 
Let  be the expected number 
of nodes visited below , conditioned
on  being visited.
We prove below, by induction 
on the height of , that for all
visited nodes  with , 

for some constants .  

Given (\ref{equ:st_induction}), 
the lemma follows easily:
since  is -reducing, 
for  at depth , we 
have . Hence, 
we have  for 
all but the root and at most 
 nodes below 
it (at each
level of  there can be at most 
one node with ).
Let  be the set of nodes  of  
such that , but
, for the parent  
of . Since  has bounded degree,
. The expected 
number  of nodes visited 
in an -restricted search is at most


It remains to prove 
(\ref{equ:st_induction}).
For this, we examine the 
paths in  that an 
-restricted search can lead to.
It will be helpful to consider 
the possible ways how   
intersects the intervals 
corresponding to the nodes 
visited in a search. The 
intersection  of 
 with interval  is 
\emph{trivial} if it is 
either empty, , or . 
It is \emph{anchored} if it 
shares at least one boundary 
line with .  If 
, the search 
terminates at , since 
we have certified that . 
If , 
then  is contained in .
There can be at most one child of 
 that contains . If such a 
child exists, the search continues 
to this child. If not, all 
possible children (to which the search can proceed to) 
are anchored. The search
can continue to any child, at most two 
of which are internal nodes.
If  is anchored,  at most one child 
of  can be anchored with .
Any other child that intersects  must 
be contained in it; see
Fig.~\ref{fig:anchored}.

\begin{figure}
\centering
\includegraphics{figures/anchored}
\caption{() The 
intersections  in (i)-(iii) 
are trivial, the
intersections in (iii) and (iv) 
are anchored; () every 
node of 
has at most one non-trivial child, 
except for .}
\label{fig:anchored}
\end{figure}


Consider all nodes that 
can be visited by an -restricted 
search (remove all nodes that are 
terminal, i.e., completely 
contained in ).  They form a 
set of paths, inducing a subtree 
of . In this subtree, there 
is at most one node with two children. 
This comes from some node  
that contains  and has two 
anchored (non-leaf) children. Every 
other node of the subtree has a 
single child; see Fig.~\ref{fig:anchored}.
We now prove two lemmas.

\begin{claim}\label{clm:WA}
  Let  be a 
  non-terminal node that 
  can be visited by 
  an -restricted
  search, and let  
  be the unique non-terminal 
  child of . Suppose 
  and .
  Then, for , we have
  
\end{claim}

\begin{proof}
From the fact that when a 
search for  shows that 
it is contained in a node 
contained in , the 
-restricted search 
is complete, it follows that

Using the hypothesis, if follows that 

The function  
is increasing for , 
so  
for  .
Together with , 
this implies

for .
\end{proof}

Only a slightly weaker statement 
can be made for the node  having two
nontrivial intersections at child 
nodes  and .

\begin{claim}\label{clm:WR}
  Let  be as above, and 
  let  be the 
  two non-terminal children of
  . Suppose that 
  , 
  for . Then, 
  for , 
  we have
  
\end{claim}

\begin{proof}
Similar to (\ref{equ:vis-recursion}), 
we get

Applying the hypothesis, 
we conclude

Setting  
and using , we get 

for , 
as in (\ref{T_i recur}), except 
for the addition of .
\end{proof}

Now we use Claims~\ref{clm:WA} 
and~\ref{clm:WR} to prove 
(\ref{equ:st_induction}) by induction. 
The bound clearly holds for leaves.
For the visited nodes
below , we may inductively 
take  and , by 
Claim~\ref{clm:WA}.
We then apply Claim~\ref{clm:WR} for .
For the parent  of , we 
use Claim~\ref{clm:WA} with
 
and , getting
. 
Repeated application of Claim~\ref{clm:WA}  
(with the given value of ) gives 
that this bound also holds for the
ancestors of , at least up until 
the  top nodes. 
This finishes the proof of 
(\ref{equ:st_induction}), and 
hence of Lemma~\ref{lem:search-time}.




\section{Auxiliary data structures} \label{sec:data-str}

We start with a simple heap-structure 
that maintains (key, index) pairs.
The indices are distinct elements of 
, and the keys come
from the ordered universe  (). 
We store the pairs in a data structure
with operations \texttt{insert}, 
\texttt{delete} (deleting a pair),
\texttt{find-max} (finding the maximum 
key among the stored pairs), and 
\texttt{decrease-key}
(decreasing the key of a pair). 
For \texttt{delete} and
\texttt{decrease-key}, we assume the input 
is a pointer into the
data structure to the appropriate pair.

\begin{claim}\label{clm:ds} 
  Suppose there are  \textup{\findmax{}} 
  operations and  \textup{\deckey{}} operations,
  and that all insertions are performed at the beginning.
  We can implement the heap structure such that the total time 
  for all operations is . The storage requirement is
  .
\end{claim}

\begin{proof} 
  We represent the heap as an array 
  of lists. For every ,
  we store the list of indices with key 
  . We also maintain , the current maximum 
  key. The total storage is . A \findmax{}  
  takes  time, and \ins{} is 
  done by adding the element to the appropriate 
  list.  To \delete{}, we remove the element from 
  the list (assuming appropriate pointers are available), 
  and we update the maximum. If the list 
  at  is non-empty, no action is required. If it is empty,
  we check sequentially if the list at  
  is empty.  This eventually leads to the maximum. 
  For \deckey{}, we \delete{}, \ins{}, and then 
  update the maximum.
  Since all insertions happen at the start, the maximum 
  can only decrease, and the total overhead for finding new 
  maxima is .
\end{proof}



Our algorithms use several data structures
to guide the searches. A \emph{vertical slab structure}  is a 
sequence of vertical lines that
partition the plane into open \emph{leaf slabs}. 
(Since we assume continuous distributions, we 
may ignore the case that an input point lies on
a vertical line and consider the leaf slabs to 
partition the plane.)
More generally, a \emph{slab} is the 
region between any two vertical lines of .  
The \emph{size} of a slab , , 
is the number of leaf slabs in it. 
The size of , , is the total number 
of leaf slabs.  For any slab , 
the probability that  
is in  is denoted by .
Our algorithms construct slab 
structures in the learning phase, 
similar to the algorithm 
in~\cite{AilonCCLMS11}.

\begin{lemma}\label{lem:slabstruct}
  We can build a slab structure \textup{} 
  with  leaf slabs so that the following holds
  with probability  over the construction:
  for a leaf slab  of \textup{}, 
  let   be the number of points in a 
  random input  that lie in .  Then 
  , for every leaf slab
  .  The construction 
  takes  rounds and  time.
\end{lemma}

\begin{proof} The construction is identical to
the -list in Ailon \etal~\cite[Lemma~3.2]{AilonCCLMS11}: 
take  random inputs
, 
and let

be the sorted list of the -coordinates
of the points 
(extended by  and ).
The  values 
 
define the boundaries for the slabs in . 
Lemma~3.2 in Ailon \etal~\cite{AilonCCLMS11}
shows that  for each leaf slab 
of , the number  of points
in a random input  that lie in 
has  and 
, with 
probability at least  over
the construction of .
The proof is completed by noting that sorting the
 inputs  takes 
 time.
\end{proof}

The algorithms construct a  
specialized search tree on  for 
each distribution .
It is important to store these
trees with little space.  
The following lemma gives the details
the construction.

\begin{lemma}\label{lem:tree} 
  Let  be fixed and
   a slab structure with  
  leaf slabs. In  rounds 
  and  time, we can 
  construct search trees  
  over  such that the following holds:
  \textup(i\textup) the trees can be 
  need  total space;
  \textup(ii\textup) with probability  
  over the construction, 
  each  is -optimal for 
  restricted searches over .
\end{lemma}

Once  is constructed,
the search trees  can 
be found using essentially 
the same techniques in 
Ailon \etal~\cite[Section~3.2]{AilonCCLMS11}:
we use  rounds to 
build the first
 levels of each , 
and we use a balanced search tree 
for searches that proceed to a deeper 
level. This only costs a factor of .
The proof of Lemma~\ref{lem:tree} is 
almost the same as that in 
Ailon \etal~\cite[Section~3.2]{AilonCCLMS11},
but since we require the additional
property of restricted search optimality,
we redo it for our setting. 

\subsection{Proof of Lemma~\ref{lem:tree}}
\label{sec:learning}

Let  be a sufficiently 
small constant and 
be sufficiently large. 
We take  
random inputs, and  for each , we 
record the leaf slab of  that 
contains it. We break the
proof into smaller claims.

\begin{claim} \label{clm:prob} 
  Using  inputs, we can
  obtain estimates 
   for each input point  
  and each slab  such that following 
  holds (for all  and )
  with probability at least  over 
  the construction: 
  if at least  
  instances of  fell in ,
  then .\footnote{We 
  remind the reader that this the
  probability that .}
\end{claim}

\begin{proof}
Fix  and , and
let  be the 
number of times 
was in . 
Let  
be the empirical 
probability for this event.
 is a sum of independent
random variables, and
.
If , 
then 
, so
by a Chernoff 
bound~\cite[Theorem~1.1, Eq. (1.8)]{DubhashiPa09}, 

Hence, with probability at 
least , if 
, 
then .
If ,  
multiplicative Chernoff 
bounds~\cite[Theorem 1.1, Eq. (1.7)]{DubhashiPa09}
give

The proof is completed by taking a union bound over all  and .
\end{proof}

Assume that the event
of Claim~\ref{clm:prob} holds.
If at least  
inputs fell in , then 

and .
The tree  is constructed recursively.
We first create a partial search tree, where
some leaves may not correspond to leaf slabs.
The root of  corresponds to 
. Given a slab
, we proceed as follows:
if , we make
 a leaf. If not, we pick a leaf slab 
 such that the subslab 
 with all leaf slabs 
strictly to the left of  and
the subslab  with all 
leaf slabs strictly 
to the right of  have 
 and
. 
We make  a leaf child of , and 
we recursively create trees for 
 and  and attach them to . 
For any internal node , we have 
,
so the depth is . 
Furthermore,
the partial tree  is -reducing 
(for some constant ).
We get a complete tree
by constructing a balanced tree
for each -leaf that is not a leaf slab. 
This yields a tree  of depth 
at most .
We only need to store the 
partial tree, 
so the total space 
is .

\begin{claim}\label{clm:1/eps} 
  The tree  is -optimal
  for restricted searches. 
\end{claim}

\begin{proof}
Fix an -restricted
distribution . For each leaf
slab , let  
be the probability according to 
. Note that 
.
If , 
then .
Any search in  takes at 
most  steps,
so the search time is
.

Now suppose . 
Consider a search for .
We classify the search according to 
the leaf that it reaches in
the partial tree.
By construction, any leaf 
of  is either a leaf slab or
has .
The search is of \emph{Type 1} 
if the leaf of the partial tree 
represents a leaf slab (and 
hence the search terminates). 
The search is of \emph{Type 2} 
(resp. \emph{Type 3}) if the 
leaf of the partial tree 
is an internal node of 
and the depth is at least 
(resp.~less than) . 

As a thought experiment,
we construct a related tree 
: start with the partial 
, and for every leaf that 
is not a leaf slab, extend it 
using the true 
probabilities . That  
is, construct the 
subtree rooted at a new node 
in the following manner: pick a leaf 
slab  with
 and
 
(with  and  as above). 
This ensures that  is 
-reducing.
By Lemma~\ref{lem:search-time},  
is -optimal for restricted 
searches over  (we absorb 
 into the ).


If the search is of Type 1, 
it is identical in both  
and . If
it is of Type 2, it 
takes at least  
steps in  and at most
 steps 
in . 
Consider Type 3 searches.
The total number of leaves 
(that are not leaf slabs)
of the partial tree at depth 
less than  is 
at most .
The total probability mass 
of  on such leaves is 
.
Since ,
the probability of a Type 3 
search is at most .

Choose a random .
Let  be the event that 
a Type 3 search occurs. Furthermore, 
let  be the depth of the search 
in  and  be
the depth in . 
If  does not occur, 
we have argued that .
Also, . 
The expected search time is .
Hence,
	
Since ,
.
Combining everything, the 
expected search time is 
. 
Since  is -optimal 
for restricted searches, 
 is -optimal.
\end{proof}




\section{A self-improving algorithm for coordinate-wise maxima}

We begin with an informal overview.
If  is sorted by -coordinate, we
can do a
right-to-left sweep:
we maintain the maximum -coordinate  
seen so far. When a point  is visited,
if , then  is non-maximal, and the point  with
 gives a per-point certificate for .
If , then  is maximal. We update  and put
 at the beginning of the maxima list of .
This suggests the following approach to a 
self-improving algorithm: sort  with a 
self-improving sorter and then do the 
sweep. The sorting algorithm of \cite{AilonCCLMS11} 
works by locating each point of  within 
the slab structure  of 
Lemma~\ref{lem:slabstruct} using the trees  of
Lemma~\ref{lem:tree}.

As discussed in Section~\ref{sec:results}, 
this does not work. We need another approach:
as a thought experiment, suppose that the maximal
points of  are available, though not in sorted order. 
We locate the maxima in  and 
determine their sorted order. 
We can argue that the optimal algorithm must also
(in essence) perform such a search. To find 
per-point certificates for the non-maximal points,
we use the slab structure  and the search trees,
proceeding very conservatively.
Consider the search for a point . At any intermediate
stage,  is placed in a slab .
This rough knowledge of 's location 
may be enough to certify 
its non-maximality: 
let  denote the leftmost maximal
point to the right of  (since the 
sorted list of maxima is known, this information
can be easily deduced). We check if  dominates 
. If so, we have a per-point 
certificate, and we terminate the search. 
Otherwise, we continue the search by a single 
step in the search tree for , and we repeat. 

Non-maximal points that are dominated 
by many maximal points should 
have a short search, while points
that are ``nearly'' maximal should 
need more time. 
Thus, this approach
should derive just the ``right" amount 
of information to determine the maxima.
Unfortunately, our thought experiment requires 
that the maxima be known.
This, of course, is too much to ask, and 
due to the strong dependencies, it is not clear
how to determine the maxima before performing 
the searches.

The final algorithm overcomes this 
difficulty by interleaving the searches 
for sorting the points with confirmation
of the maximality of some points, in 
a rough right-to-left order
that is a more elaborate version of the traversal scheme given
above. 
The searches for all points  (in their 
respective trees ) are performed 
``together'', and their order is carefully chosen. 
At any intermediate
stage, each point  is located in 
some slab , represented by
a node of its search tree. We 
choose a specific point and advance
its search by one step. This choice is 
very important, and is the basis
of optimality. The algorithm is 
described in detail and analyzed in 
Section~\ref{sec:algorithm}.

\subsection{Restricted Maxima Certificates}

We modify the maxima certificate 
from Definition~\ref{def:cert-max} in order
to get easier proofs of optimality.
For this, we need the following observation,
see Fig.~\ref{fig:domregion}.

\begin{prop}\label{prop:dom} 
  Let  be a linear comparison
  tree for computing the maxima.
  Let  be a leaf of  
  and  be the region 
  associated with non-maximal point 
   in . 
  There is a region 
  associated with a maximal point  
  such that every point in 
  dominates every point in .
\end{prop}
\begin{figure}
\centering
\includegraphics{figures/domregion}
\caption{Every point in  dominates every point in .}
\label{fig:domregion}
\end{figure}

\begin{proof} 
The leaf  is associated with a 
certificate  that is valid 
for every input that reaches . 
The certificate  associates 
the non-maximal point  with 
 such that  dominates . 
For any input  in ,  
dominates . First, we argue 
that  can be assumed to be 
maximal. We construct a directed graph 
 with vertex set 
such that  has an edge  
if and only if (according to 
)  is dominated by 
. All vertices have outdegree at most 
, and there are no cycles in  
(since dominance is transitive).
Hence,  consists of trees with 
edges directed towards the root.
The roots are maximal vertices, and any 
point in a tree is dominated by the point 
corresponding to the root. We can thus
rewrite  so that all dominating 
points are extremal.

Since  is restricted, the 
region  
for  is a Cartesian product 
of polygonal regions .
Suppose there are two points
 and 
such that  does not dominate . 
Take an input  where the
remaining points are arbitrarily chosen from their 
respective regions. The certificate 
is not valid for , contradicting the 
nature of .
Hence, every point in  dominates every point 
in . 
\end{proof}

We need points 
in the maxima certificate to be 
``well-separated'' according to 
the slab structure . 
By Proposition~\ref{prop:dom},
every non-maximal point is associated with a dominating
region.

\begin{definition}\label{def:s-label}
  Let  be a slab structure. 
  A maxima certificate for an input  is 
  \emph{-labeled} if (i)
  every maximal point is labeled with 
  the leaf slab of  containing it; and
  (ii) every non-maximal point is either 
  placed in the containing leaf slab,
  or is separated from its dominating 
  region by a slab boundary.
\end{definition}

A tree  \emph{computes the -labeled
maxima} if the leaves are associated with 
-labeled certificates.

\begin{lemma}\label{lem:comp} 
  There is an entropy-sensitive comparison 
  tree  for computing the -labeled 
  maxima whose expected depth over  is 
  .
\end{lemma}

\begin{proof} 
We start with a linear comparison 
tree of depth  that computes the maxima, with
certificates as in 
Proposition~\ref{prop:dom}. 
Each leaf has a list  with the maximal
points in sorted order.  We merge  with the 
slab boundaries of  to label each 
maximal point with the leaf slab of  containing it. 
This needs  additional comparisons.
Now let  be the region associated 
with a non-maximal point , and  
the maximal dominating region. Let  
be the leaf slab containing .
The -projection of  cannot extend 
to the right of .
If there is a slab boundary separating 
 from , nothing needs to be done.
Otherwise,  intersects 
. With one more comparison, we can
place  inside  or strictly 
to the left of it.
In total, it takes  additional 
comparisons in each leaf to
that get a tree for the -labeled 
maxima. Hence, the expected depth is . 
We apply Lemma~\ref{lem:lin->entropy} 
to get an entropy-sensitive tree with
the desired properties.
\end{proof}

\subsection{The algorithm}
\label{sec:algorithm}

In the learning phase, the algorithm 
constructs a slab structure  and
search trees  as in 
Lemmas~\ref{lem:slabstruct} and~\ref{lem:tree}.
Henceforth, we assume that we have these structures, 
and we describe the algorithm in the limiting
phase.
The algorithm searches
each point  progressively 
in its tree , while
interleaving
the searches carefully. 

At any stage of the algorithm, 
each point  is placed in some slab 
.  The algorithm maintains a set 
 of \emph{active points}. All
other points are either proven 
non-maximal, or placed in 
a leaf slab. 
The heap structure  from
Claim~\ref{clm:ds} is used to
store pairs of indices of active points
and associated keys.
Recall that  supports the operations \ins{},
\delete{}, \deckey{}, and \findmax{}. The key 
for an active point  is the  
right boundary of the slab  
(represented as an element of ).
We list the variables of the algorithm.
Initially, , and each  is the largest 
slab in .  Hence, all points have  key , 
and we \ins{} all these pairs
into . 

\begin{asparaenum}
  \item : the list  of 
    active points is stored in 
    heap structure , with
    their associated right slab boundary as key.
  \item : Let  be the largest 
     key in . Then  is the leaf slab 
     with right boundary is  and
      is a set of points located in 
     so far. Initially  is empty and  is , 
     corresponding to the  boundary of the 
     rightmost, infinite, slab.
  \item :  is a sorted (partial) list 
    of the maximal points so far,
    and  is the leftmost among those. 
    Initially  is empty and  is a 
    ``null'' point that dominates no input point.
\end{asparaenum}

The algorithm involves a main 
procedure \textbf{Search}, and an 
auxiliary procedure
\textbf{Update}. The procedure \textbf{Search} 
chooses a point and advances
its search by a single node in the corresponding
search tree. Occasionally, \textbf{Search} invokes 
\textbf{Update} to change the global variables. 
The algorithm repeatedly calls \textbf{Search}
until  is empty. 
After that, we make a final call to 
\textbf{Update} in order to
process any remaining points.

\noindent
\textbf{Search}:  
Perform a \findmax{} in 
and let  be the resulting
point.
If the maximum key  in  is 
less than the right 
boundary of , 
invoke \textbf{Update}. 
If  is dominated by , 
delete  from .
If not, advance 
the search of  in  by 
a single node, if possible. 
This updates the slab . If 
the right boundary of  has 
decreased, perform a 
\deckey{} operation on . 
(Otherwise, do nothing.)
Suppose the point  reaches 
a leaf slab .  If 
, 
remove  from  and insert it
in  (in time ). Otherwise, 
leave  in .

	
\noindent
\textbf{Update}: 
Sort the points in  and update 
the list of maxima.
As Claim~\ref{clm:order} will show, we 
know the sorted list of maxima to 
the right of .  Hence, 
we can append to this list in 
time. We reset ,
set  to the leaf 
slab to the left of , and return.


The following claim states the main 
important invariant of the algorithm.

\begin{claim}\label{clm:order} 
  At any time in the algorithm, all 
  maxima to the right of  have 
  been found, in order from right to left.
\end{claim}

\begin{proof} 
The proof is by backward induction on 
, the right boundary of .
For , the claim is trivially true. 
Assume it holds for a given value of , 
and trace the algorithm's behavior until 
the maximum key becomes smaller than  
(which happens in \textbf{Search}). 
When \textbf{Search}
processes a point  with key  
then either (i) the key value decreases; 
(ii)  is dominated by ; 
or (iii)  is  placed in 
 (whose right boundary is ). 
In all cases, when the maximum key decreases
below , all points in  are 
either proven to be non-maximal
or are in . By the induction hypothesis, 
we already have a sorted
list of maxima to the right of .
The procedure \textbf{Update} sorts the 
points in  and all maximal points 
to the right of  are determined.
\end{proof}


\subsubsection{Running time analysis}\label{sec:runtime}

We prove the following lemma.

\begin{lemma}\label{lem:algoMaxima} 
  The maxima algorithm runs in 
   time.
\end{lemma}

We can easily bound the running time 
of all calls to \textbf{Update}.

\begin{claim}\label{clm:update} 
  The total expected time for
  calls to \textbf{Update} 
  is .
\end{claim}

\begin{proof}
The total time for the calls to 
\textbf{Update} is at most
the time needed for sorting 
points within each leaf slabs. 
By Lemma~\ref{lem:slabstruct}, 
this takes expected time
 
\end{proof}

The following claim is key to relating the time
spent by \textbf{Search} to 
entropy-sensitive comparison trees.

\begin{claim}\label{clm:search} 
  Let  be an entropy-sensitive 
  comparison tree computing -labeled 
  maxima.  Consider a leaf  with depth
   labeled 
  with the regions . 
  Conditioned on , the 
  expected running time of 
  \textup{\textbf{Search}} is 
  .
\end{claim}


\begin{proof} 
For each , let  be the 
smallest slab of  that completely contains 
. We will show that the algorithm
performs at most an -restricted search 
for input . 
If  is maximal, then  is 
contained in a leaf slab (because 
the output is -labeled). Hence 
 is a leaf slab and an -restricted 
search for a maximal  is just 
a complete search.

Now consider a non-maximal . By the properties 
of -labeled maxima, the associated region  
is either inside a leaf slab or
is separated by a slab boundary from the dominating 
region .  In the former case, an -restricted 
search is a complete search.
In the latter case, an -restricted 
search suffices to process :  
by Claim~\ref{clm:order}, when an -restricted 
search finishes, all maxima to the right of 
 have been determined.  In particular, we have 
found , so  dominates .
Hence, the search for  proceeds no further.


The expected search time taken 
conditioned on  is 
the sum (over ) of the conditional 
expected -restricted search times. 
Let  denote the event that 
, and  be the event 
that .  We have 
.
By the independence of the distributions 
and linearity of expectation

By Lemma~\ref{lem:search-time}, the time 
for an -restricted search 
conditioned on  is 
. 
By Proposition~\ref{prop:entropyDepth}, 
,
completing the proof.
\end{proof}

We can now prove the main lemma.

\begin{proof}[Proof of Lemma~\ref{lem:algoMaxima}] 
By Lemma~\ref{lem:comp}, there is
an entropy-sensitive 
tree that computes the -labeled 
maxima with expected depth .
Since the algorithm never exceeds
 steps and by Claim~\ref{clm:search}, the 
expected running time of 
\textbf{Search} is , and 
by Claim~\ref{clm:update} the 
total expected time for \textbf{Update}
is . Adding these bounds completes 
the proof.
\end{proof}

\section{A self-improving algorithm for convex hulls}

We outline the main ideas. The basic approach
is the same as for maxima. We set up 
a slab structure , and each
distribution has a dedicated tree for searching 
points. At any stage, each point
is at some intermediate node of the search tree, 
and we wish to advance searches for points 
that have the greatest potential for being extremal.
Furthermore, we would like to quickly 
ascertain that a point is not extremal, so that
we can terminate its search.

For maxima, this strategy is easy enough to 
implement. The ``rightmost'' active point 
is a good candidate for being maximal, so we always
proceed its search. The leftmost known maximal
point can be used to obtain certificates of non-maximality.
For convex hulls, this is much more problematic. At any 
stage, there are many points likely to be
extremal, and it is not clear how to choose.
We also need a procedure that can quickly identify
non-extremal points. 

We give a high-level 
description of the main algorithm.
We construct a \emph{canonical hull} 
 in the learning phase. The canonical hull
is a crude representative for the actual 
upper hull. The canonical hull has two key 
properties. First, any point that is below 
 is likely to be non-extremal. Second, 
there are not too many points above . 

The curve  is constructed as follows.
For every (upward) direction , take the normal line 
such that the expected total number of points above 
is . We can take the intersection of  over all ,
to get an upper convex curve . Any point below this curve is highly likely to be non-extremal.
Of course, we need a finite description, so we choose some finite set  of directions,
and only consider  for these directions to construct . We choose  to
ensure that the expected number of extremal points in the slab corresponding to a segment of  is .
We build the slab
structure  based on these segments of , and search for points
in . Each search for point  will result in one of the three conclusions:
 is located above ,  is located below , or  is located in a leaf slab.
This procedure is referred to as the \emph{location algorithm}.  

Now, we have some partial information about 
the various points that is used by
a \emph{construction algorithm} to find . We can ignore all points below ,
and prove that the  can be found on  time.

\subsection{The canonical directions}\label{sec:prelim_CH}

We describes the structures obtained 
in the learning phase.  In order 
to characterize the typical behavior 
of a random input ,
we use a set  of 
\emph{canonical directions}. A 
\emph{direction} is a two-dimensional 
unit vector. Directions are ordered 
clockwise, and we only consider 
directions that point upwards. Given 
a direction , we say that 
 is \emph{extremal} for  
if the scalar product 
 is maximum 
in . We denote the 
lexicographically smallest input 
point that is extremal for  
by .  The canonical directions 
are described in the following lemma, 
whose proof we postpone to 
Section~\ref{sec:canonicalDir}. They 
are computed in the learning phase. 
(Refer to Definition~\ref{def:cert-ch} 
and just above it for some of the 
basic notation below.)

\begin{lemma}\label{lem:canonicalDir}
  Let . There 
  is an  time procedure
  that takes  random inputs 
  and outputs an ordered sequence 
   of 
  directions with the following
  properties (with probability at least 
   over construction). Let .
  For ,  let , 
  let  be the number of points  from 
  inside , 
  and  the number of \emph{extremal}
  points inside .
  Then
  
\end{lemma}

We construct some special lines 
that are normal to the canonical directions.
The details are 
in Section~\ref{sec:ellTail}.

\begin{lemma}\label{lem:ellTail} 
  We can construct (in  time 
  with one random input) lines 
   with
   normal to , and with 
  the following property (with probability 
  at least  over the construction). 
  For  (and  large 
  enough), we have
  
\end{lemma}

We henceforth assume that the learning 
phase succeeds, so the directions
and lines have properties from
Lemma~\ref{lem:canonicalDir} and \ref{lem:ellTail}.
We call  is \emph{\textup{\textbf{V}}-extremal} 
if  for some .
Using the canonical directions 
from Lemma~\ref{lem:canonicalDir} and the
lines from Lemma~\ref{lem:ellTail}, we 
construct a \emph{canonical hull}  
that is ``typical'' for random .
It is the intersection of the halfplanes 
below the ,
i.e., .
Thus,  is a convex polygonal region 
bounded by the . 
The following corollary follows from a union bound 
of Lemma~\ref{lem:ellTail} over all . 
It implies that the total number
of points outside  is .

\begin{corol}\label{cor:ellTail} 
  Assume the learning phase succeeds. 
  With probability at least , 
  the following holds:  for all , the 
  extremal point for  lies outside . 
  The number of pairs , where 
  ,  is an edge of ,
  and  is visible from , is .
\end{corol}


\begin{figure}
  \centering
  \includegraphics{figures/pencil} 
  \caption{(left) The -leaf slabs
    are shown dashed. The shaded portion 
    represents a -slab . (right)
     is shown shaded:
     
    lies above the pencil:  inside it; 
     is not comparable to it.}
\label{fig:pencil}
\end{figure}

To give some intuition about , consider the simple example
where each distribution outputs a fixed point. We set  to be the direction
pointing leftwards, so the extremal point  is the leftmost point.
Starting from , continue to the first extremal point  such that
there are  extremal points between  and . Take any
direction  such that  is extremal for it. Continue in this manner
to get . For each , the line  is normal to  and has  points
above it. So  is ``well under" , but not too far
below.


We list some preliminary concepts related to ,
see Fig.~\ref{fig:pencil}.
By drawing a vertical line through each 
vertex of , we obtain a subdivision of 
the plane into vertical open slabs, the 
\emph{-leaf-slabs}. A contiguous
interval of -leaf slabs is again a 
vertical slab, called \emph{-slab}.
The -leaf-slabs define the slab 
structure for the upper hull algorithm, 
and we use Lemma~\ref{lem:tree} to construct 
appropriate search trees  for 
the -leaf slabs and for each   
distribution .

For a -slab , we let  
be the line segment between the two vertices of 
 that lie on the vertical boundaries of .
Let  be a point outside of , and let  
and  be the vertices of  where the 
two tangents for  through  touch 
. The \emph{pencil slab} for  is the
-slab bounded by the vertical lines through 
 and .  The \emph{pencil of }, 
 is the region inside the pencil slab for 
 that lies below the line segments  
and .  A point  is 
\emph{comparable} to  if it lies 
inside the pencil slab for . It lies 
\emph{above}  if it is comparable to
 but not inside it.

\subsection{Restricted Convex Hull Certificates}\label{sec:ch-cert}

We need to refine the certificates from 
Definition~\ref{def:cert-ch}.
Recall that a upper hull certificate
has a sorted list of extremal points 
in , and a witness pair for each 
non-extremal point in . The points 
 form a witness pair for  
if .
A witness pair  is 
\emph{extremal} if both  and
 are extremal; it is
\emph{\textup{}-extremal}
if both  and  are 
-extremal.  Two distinct 
extremal points  and  are 
called \emph{adjacent} if there is no 
extremal point with -coordinate 
strictly between
the -coordinates of  and . 
Adjacent -extremal points
are defined analogously.

We now define a 
\emph{-certificate} 
for .  It consists of (i) a 
list of the -extremal 
points of , sorted from left 
to right; and (ii) a list that 
has a -slab  for every 
other point . 
The -slab  contains  and 
can be of three different kinds; see
Fig.~\ref{fig:cert}. 
Either
\begin{asparaenum}
  \item  is a -leaf slab; or
  \item  lies below ;
    or
  \item  is the pencil slab for a 
    -extremal vertex  
    such that  lies in the pencil of 
    .
\end{asparaenum}
The following key lemma is crucial to 
the analysis.  We defer the proof to 
the next section. The reader may wish to skip
that section and proceed to learn about 
the algorithm.

\begin{figure}
  \centering
  \includegraphics{figures/cert} 
  \caption{The -slab  associated
    with  can either be (i) a leaf slab; (ii) such
    that  lies below ; or (iii)
    such that  lies in 
    for a -extremal vertex .}
\label{fig:cert}
\end{figure}

\begin{lemma}\label{lem:entropy-sensitive-CH}
  Assume  is obtained from a successful 
  learning phase. Let  be 
  a linear comparison tree that computes 
  the upper hull of . Then there is 
  an entropy-sensitive linear comparison tree 
  with expected depth  that computes 
  -certificates for . 
\end{lemma}

\subsection{Proof of Lemma~\ref{lem:entropy-sensitive-CH}}
\label{sec:certificate}

The proof goes through several 
intermediate steps that successively 
transform a upper hull certificate 
into a -certificate. 
Each step incurs expected linear 
overhead. Then, it suffices to 
apply Lemma~\ref{lem:lin->entropy} to obtain an
entropy-sensitive comparison tree with 
the claimed depth.
A certificate  is \emph{extremal} 
if all witness pairs in 
 are extremal. We provide the
required chain 
of lemmas and
give each proof in a different subsection.
The following lemma is proved in 
Section~\ref{sec:regular->extremal}.

\begin{lemma}\label{lem:regular->extremal}
  Let  be a linear comparison tree 
  for . There exists a linear 
  comparison tree with expected depth 
   that computes an 
  extremal certificate for .
\end{lemma}

A certificate is 
\emph{\textup{}-extremal} if 
it contains (i) a list of the 
-extremal points of , sorted 
from left to right; and (ii) a list that stores 
for every other point  \emph{either} 
a -extremal witness pair
for  \emph{or} two adjacent 
-extremal points  and  such 
that . The next 
lemma is proved in 
Section~\ref{sec:extremal->V-extremal}.

\begin{lemma}\label{lem:extremal->V-extremal}
  Let  be a linear comparison tree 
  that computes extremal certificates.
  There is a linear comparison tree 
  with expected depth  that computes 
  -extremal certificates.
\end{lemma}

Finally, we go from
-extremal certificates to 
-certificates. 
The proof is in Section~\ref{sec:V-extremal->canonical}.

\begin{lemma}\label{lem:V-extremal->canonical}
  Let  be a linear comparison tree 
  that computes -extremal 
  certificates. There is a linear 
  comparison tree with expected depth 
   that computes -certificates. 
\end{lemma}

Lemma~\ref{lem:entropy-sensitive-CH} follows by
combining Lemmas~\ref{lem:regular->extremal},
\ref{lem:extremal->V-extremal} 
and~\ref{lem:V-extremal->canonical}
with Lemma~\ref{lem:lin->entropy}.

\subsubsection{Proof of Lemma~\ref{lem:regular->extremal}}
\label{sec:regular->extremal}
 
\begin{figure}
  \centering
  \includegraphics[scale=0.6]{figures/shortcut}
  \caption{ The \textbf{shortcut} operation: 
    observe that computing the upper hull 
    of the out-neighbors of , and  suffices
    for removing  from all witness pairs.}
  \label{fig:shortcut}
\end{figure}

We transform  into a tree for 
extremal certificates. Since each leaf 
 of  corresponds to a certificate 
that is valid for all , it suffices 
to show how to convert a given certificate 
 for  to an extremal 
certificate by performing  additional 
comparisons on . We 
describe an algorithm for this task.

The algorithm uses two data structures: 
(i) a directed graph  whose vertices 
are a subset of ; and (ii) a stack .  
Initially,  is empty and  has 
a vertex for every . For each 
non-extremal , we add 
two directed edges  and  to , 
where  is the witness pair 
for  according to . 
In each step, the algorithm performs 
one of the following operations, until 
 has no more edges left (we will 
use the terms \emph{point} and 
\emph{vertex} interchangeably, since 
we always mean some ).
\begin{itemize}
  \item \textbf{Prune.} If  has 
    a non-extremal vertex  with 
    indegree zero, we delete  from 
     (together with its outgoing
    edges) and push it onto .
	
  \item \textbf{Shortcut.} If  
    has a non-extremal vertex  with
     indegree  or , we find 
     for each in-neighbor  of  
     a witness pair that does not include 
     , and we replace the out-edges 
     from  by edges to this 
     new pair. (We explain shortly 
     how to do this.) The indegree of 
      is now zero. 
\end{itemize}
An easy induction shows that the 
algorithm maintains the following 
invariants: (i) all non-extremal 
vertices in  have out-degree ; 
(ii) all extremal vertices of  
have out-degree ; (iii) for 
each non-extremal vertex  of , 
the two out-neighbors of  
constitute a witness pair for ; 
(iv) every  is either in  
or in , but never both; (iv) when 
a point  is added to , then 
we have a witness pair  for 
 such that . 

We analyze the number of comparisons 
on . \textbf{Prune} needs no 
comparisons.  \textbf{Shortcut} is 
done as follows: we consider for each
in-neighbor  of  the upper convex 
hull  for 's two out-neighbors
and 's other out-neighbor, and we 
find the edge of  that lies above 
. Since the  constant size and 
since  has in-degree at most , 
this takes  comparisons,
see Fig.~\ref{fig:shortcut}.
There are at most  calls to
\textbf{Shortcut}, so the total number 
of comparisons is .  Deciding which 
operation to perform depends solely on 
 and requires no comparisons on . 

We now argue that the algorithm cannot get 
stuck. That means that if  has at 
least one edge, \textbf{Prune} or 
\textbf{Shortcut} can be applied. 
Suppose that we cannot perform 
\textbf{Prune}. Then each non-extremal 
vertex has in-degree at least . 
Consider the subgraph  of  
induced by the non-extremal vertices. 
Since all extremal vertices have out-degree ,
all vertices in  have in-degree 
at least . The average out-degree 
in  is at most , so there must be 
a vertex with in-degree (in )  or 
.  This in-degree is the same in , 
so \textbf{Shortcut} can be applied.

Thus, we can perform \textbf{Prune} or 
\textbf{Shortcut} until  has no more 
edges and all non-extremal points are on 
the stack .  Now we pop the points from 
 and find extremal witness pairs for them.
Let  be the next point on .  By invariant 
(iv), there is a witness pair 
for  whose vertices are not on . Thus, 
each  and  is either extremal or we 
have an extremal witness pair for it.
Therefore, we can find an extremal witness 
pair for  with  comparisons, as 
in \textbf{Shortcut}. We repeat this process 
until  is empty. This takes  comparisons
overall, so we obtain an extremal 
certificate  from  with  
comparisons on . 

\subsubsection{Proof of Lemma~\ref{lem:extremal->V-extremal}}
\label{sec:extremal->V-extremal}

\begin{figure}
  \centering
  \includegraphics{figures/v-extremal}
  \caption{If  is in the blue region then 
    , , 
    or .}
  \label{fig:v-extremal}
\end{figure}

As in Section~\ref{sec:regular->extremal}, it 
suffices to show how to convert a given 
extremal certificate into a 
-extremal one
with  comparisons on . This 
is done as follows.  First, we determine 
the -extremal points on 
. This takes 
comparisons by a simultaneous traversal 
of  and .
Without further comparisons, we can 
now find for each extremal
point  in  the two adjacent 
-extremal points that
have  between them. This information 
is stored in the -extremal
certificate.

Now let  be non-extremal, and 
let  be the corresponding
extremal witness pair. We show how 
to find either a -extremal 
witness pair or the right pair of 
adjacent -extremal points.
We have determined adjacent 
-extremal points  
such that . 
(If  is itself -extremal, 
set .) Similarly, define 
adjacent -extremal points 
. We know that  lies in
 and hence 
.  Furthermore, 
the points  are 
in convex position.  Since  is in 
, one of the following 
must happen: , 
 lies in , or 
; see 
Fig.~\ref{fig:v-extremal}.  
We can determine which in 
 comparisons.

\subsubsection{Proof of Lemma~\ref{lem:V-extremal->canonical}}
\label{sec:V-extremal->canonical}

As in Sections~\ref{sec:regular->extremal} 
and~\ref{sec:extremal->V-extremal}, we convert a
-extremal certificate 
into a -certificate with  
expected comparisons.
For each -extremal point in 
, we perform a binary search to 
find the -leaf slab that contains 
it. This requires  comparisons, 
since there are at most  
-extremal points and 
since each binary search needs 
 comparisons. Next, we 
check for each  if 
the extremal point for  lies 
in . This takes one 
comparison per point. If any check 
fails, we declare failure and use 
binary search to find for every 
 a -leaf slab that 
contains it.

We now assume that there exists a 
-extremal point
in every . (This implies 
that all -extremal points
lie outside .) We use binary search 
to determine the pencil of each 
-extremal point. Again, 
this takes  comparisons.
Now let  be not 
-extremal. We use  
comparisons and either find the slab 
or determine that  lies above .
The certificate  assigns to 
 two -extremal points 
 and  such that either 
(i)  is a 
-extremal witness pair 
for ; or (ii)  and  
are adjacent and 
. 
We define  as the rightmost
visible point of  from  
and  as the leftmost visible
point from . 

Let us consider the first case; see 
Fig.~\ref{fig:canonical}(left). The
point  is below . 
Since  are in 
convex position,  is 
below their upper hull.
This means that one of the following 
holds:
, 
, or 
is below . 
This can be determined in 
comparisons. In the first two 
cases,  lies in a pencil (and 
hence we find an appropriate ),
and in the last case, we find a 
witness -slab. Now for the second case. 
We need the following claim.

\begin{claim}\label{clm:overlap}
  If, for all  there 
  is a -extremal 
  point in , then the 
  pencils of any two adjacent 
  -extremal points 
  either overlap or share a slab boundary.
\end{claim}

\begin{proof} 
Refer again to 
Fig.~\ref{fig:canonical}(left). 
Let  and  be two adjacent 
-extremal vertices 
such that their pencil slabs neither 
overlap nor share a boundary. Then 
 is not visible from . 
Consider the edge  of  
where  is the left endpoint. 
The edge  is not visible from 
either  or  and is 
between them. By assumption, 
there is an extremal point 
 of  that sees . But 
the point  cannot lie to the 
left of  or to the right 
of  (that would violate the 
extremal nature of  or ). 
Hence,  must be between  and 
, contradicting the fact that 
they are adjacent.
\end{proof}

Claim~\ref{clm:overlap} implies that
 is comparable to one of 
, .
By  comparisons, we can check 
if  is contained in either 
pencil or is above .
\begin{figure}
  \centering
  \includegraphics{figures/canonical}
  \caption{-certificates: in each part, 
    is contained in the shaded region.}
  \label{fig:canonical}
\end{figure}
Finally, for all points determined 
to be above , we use binary search
to place them in a -leaf slab. This 
gives an appropriate  for each 
, and the canonical certificate 
is complete.
We analyze the total number of comparisons.
Let  be the indicator random variable 
for the event that there exist some 
 without a -extremal 
point. Let  denote the number of 
points above . By 
Corollary~\ref{lem:ellTail}, 
and . 
The number of comparisons is at most 
,
the expectation of which is .

\subsection{The algorithm}

Finally, we are ready to 
describe the details of our convex 
hull algorithm. It has two parts: 
the \emph{location algorithm} and the 
\emph{construction algorithm}. The 
former algorithm determines the location 
of the input points with respect to the 
canonical hull . It must be careful 
to learn just the right amount of information 
about each point. The latter algorithm uses 
this information to compute the convex
hull of  quickly.

\subsubsection{The location algorithm}
\label{sec:loc_alg}
Using Lemma~\ref{lem:tree},
we obtain near-optimal search trees 
 for the -leaf slabs. 
The algorithm searches progressively 
for each  in its 
tree . Again, we interleave the
coordinate searches, and we abort the
search for a point as soon as we have 
gained enough information about
it. The location algorithm maintains 
the following information. 

\begin{itemize}
\item 
\textbf{Current slabs .}
  For each point , 
  we store a current -slab  
  containing  that corresponds 
  to a node of . 
\item \textbf{Active points .}
  The active points are stored in a 
  priority-queue  as in 
  Claim~\ref{clm:ds}. The key associated 
  with an active point  is the size 
  of the associated current slab  
  (represented as an integer between  and ).
\item \textbf{Extremal candidates .}
  For each canonical direction 
  , we store a point
   that lies outside 
  of .  We call  an 
  \emph{extremal candidate} for .
\item
  \textbf{Pencils for the points outside of .}
  For each point  that has been 
  located outside of , we store its
  pencil .
\item
  \textbf{Points with the left- and rightmost pencils.}
  For each edge  of , we store two  
  points 
  and  such that 
  (i)  and  lie outside of ;  
  (ii)  lies in  and ;
  (iii) among all pencils seen 
  so far that contain , the left boundary of 
    lies furthest to the left 
  and the right boundary of   
  lies furthest to the right. 
\end{itemize}

\begin{figure}
  \centering
  \includegraphics{figures/algorithm}
  \caption{The algorithm:  the boundary 
  of  is shown dashed, the pencil 
   is shaded.}
  \label{fig:algorithm}
\end{figure}

Initially, we set  and each 
 to the root of the corresponding search
tree . The extremal candidates 
 as well as the points 
 with the left- and 
rightmost pencils are set to the 
null pointer. The location algorithm 
proceeds in \emph{rounds}. In each round, 
we perform a \findmax{} on . 
Suppose that \findmax{} returns .
We compare  with the vertical 
line that corresponds to its current node
in  and advance  to 
the appropriate child. This reduces the
size of , so we also perform a 
\deckey{} on . Next, we distinguish 
three cases:

\noindent\textbf{Case 1}:  lies below 
. We declare  
inactive and \delete{} it from .

For the next two cases, we know 
that  lies above .
Let ,  be the canonical 
lines that support the edges
 and  of  that are 
incident to the boundary vertices of
 and lie inside of ; see 
Fig.~\ref{fig:algorithm}.
We check where  lies with respect 
to  and .

\noindent\textbf{Case 2}:  
 is above  or above . 
This means that  is outside of .  
We declare  inactive and
\delete{} it from . Next, we 
perform a binary search to find 
 and all the edges of 
 that are visible 
from . For each such edge ,
we compare  with the extremal 
candidate for , and if  is more 
extreme in the corresponding direction, 
we update the extremal candidate accordingly. 
We also update the points  and 
 to , if necessary. 

\noindent\textbf{Case 3}:  lies 
below  and . Recall that
 corresponds to the edge  of 
 and  corresponds to the 
edge  of . We take the rightmost 
pencil for  and the leftmost pencil for  
(if they exist); see Fig.~\ref{fig:algorithm}.
We compare  with these pencils. 
If  lies inside a pencil, we are done.
If  is above a pencil, we learn 
that  lies outside of , and we 
process as in Case~2. In both situations, 
we declare  inactive and \delete{} it 
from . 
If neither of these happen,  remains active.

The location algorithm continues until  is 
empty (note that every point becomes inactive 
eventually, because as soon as  is a leaf
slab, either Case 1 or Case 2 applies).


\subsubsection{Running time of the location algorithm}

We now analyze the running time of 
the location algorithm, starting
with some preliminary claims.
The algorithm is deterministic, so 
we can talk of deterministic 
properties of the behavior on any input.

\begin{claim}\label{clm:algoext} 
  Fix an input . Let
   be -extremal,
  and let  be the pencil
  slab for . Once the 
  search for  reaches a slab  
  with , 
   will be identified as an extremal point 
  for direction .
\end{claim}

\begin{proof} 
At least one vertical boundary 
line of  lies inside (the 
closure of)  and 
 contains at least 
one leaf slab. Since  is a 
pencil slab,  sees all edges 
of  in , so one of 
the boundary edges  or  
corresponding to , as used in 
Cases 2 and 3 of the algorithm 
(see Fig.~\ref{fig:algorithm}), must
be visible to . 
Hence,  lies in 
, and
this is detected in Case~2 
of the location algorithm.
\end{proof}


\begin{figure}
  \centering
  \includegraphics{figures/case-3}
  \caption{The left boundary of slab  is 
    contained in the pencil slab of .}
  \label{fig:case-3}
\end{figure}

\begin{claim}\label{clm:algo-inact} 
  Let  be 
  -extremal, and 
   the pencil slab for .
  Suppose  lies in 
  . Once
  the search for  reaches 
  a slab  with ,
  the point  becomes inactive  
  in the next round that it is 
  processed.
\end{claim}

\begin{proof} 
Consider the situation after 
the round in which  reaches  
with . 
The location algorithm schedules 
points according to the size 
of their current slab. 
Thus, when  is processed next, 
all other active points are placed 
in slabs of size at most . 
By Claim~\ref{clm:algoext},
if  is ever placed in slab of size 
at most , the algorithm
detects that it is -extremal 
and makes it inactive.

Hence, when  is processed next, 
 has been identified
as the extremal point in direction
. Note that ,
since . Some 
boundary (suppose it is the left one) of 
lies inside . Let  be the 
corresponding edge of , as used by 
the location algorithm; see Fig.~\ref{fig:case-3}.
Since  is visible from , 
and since  has been processed,
it follows that the pencil 
slab of the rightmost pencil 
for  spans all of .
In Case~3 of the location algorithm 
(in this round), 
will either be found inside this 
pencil, or outside of .
Either way,  becomes inactive.
\end{proof}

We arrive at the main lemma of this section.

\begin{lemma}\label{lem:algoCH} 
  The total number of rounds in 
  the location algorithm is 
  .
\end{lemma}

\begin{proof} 
Let  be an entropy-sensitive 
comparison tree that 
computes a -certificate for 
 in expected depth . 
Such a tree exists by 
Lemma~\ref{lem:entropy-sensitive-CH}.
Let  be a leaf of  with 
. By 
Proposition~\ref{prop:Cartesian},
 is a Cartesian product 
.
The depth of  is 
, 
by Proposition~\ref{prop:entropyDepth}. 
Now consider a random input , 
conditioned on . We 
show that expected number of rounds 
for  is  . This
also holds for , since
there are never more than  rounds.
The lemma follows, as the 
expected number of rounds is

Let  be a leaf with  and  the 
-certificate for 
. The main technical argument is 
summarized in the following claim.

\begin{claim}\label{clm:algoCH} 
  Let  and
  . The number 
  of rounds involving 
   is at most one more than 
  the number of steps required for an
  -restricted search for 
   in .
\end{claim}

\begin{proof} 
By definition of -certificates, 
 is one of three types. 
Either  is a -leaf slab, 
 is below ,
or  is a pencil slab of a 
-extremal vertex. 
In all cases,  contains .
When  is a leaf slab, an
-restricted search for  is 
a complete search. Hence, this is 
always at least the number of rounds 
involving .
Suppose  is below . 
For any slab ,
 is above . 
If  is located in any slab ,
it is made inactive (Case 1 of the algorithm). 

Now for the last case. 
The slab  is the pencil slab 
for a -extremal vertex ,
such that the , contains . 
Suppose the search for  leads to 
slab  and 
is still active.
By Claim~\ref{clm:algo-inact}, since 
,  becomes
inactive in the next round.
\end{proof}

Suppose  is chosen randomly from 
. The distribution restricted 
to  is simply random from .  
By Lemma~\ref{lem:search-time}, the 
expected -restricted search 
time is . Combining 
with Claim~\ref{clm:algoCH}, the expected
number of rounds is 

\end{proof}

\begin{lemma} 
  The expected running time of 
  the location algorithm is 
  .
\end{lemma}

\begin{proof}
By Claim~\ref{clm:ds},
the total overhead for the heap 
structure is linear in the number
of rounds. The time to implement 
Cases~1 and~3 is , as we only 
need to compare  with a 
constant number of lines. Hence, the 
total time for this is at most proportional to the 
number of rounds. 

In Case~2, we do a binary search 
for  and possibly update 
an extremal point (and pencil) 
for each edge 
visible from . The case
only occurs if  lies 
outside .  By 
Corollary~\ref{cor:ellTail}, the 
expected number such updates is . 
Overall, the total cost for Case~2 
operations is . Combining with 
Lemma~\ref{lem:algoCH}, the expected 
running time is 
.
\end{proof}

\subsubsection{The construction algorithm}

We now describe the upper hull 
construction that uses the information
from the location algorithm to compute
 quickly. First, we dive 
into the geometry of pencils.

\begin{claim}\label{clm:no_pencil_overlap}
  Suppose that all 
  \textup{}-extremal 
  points of  lie outside
  of , and let  be a 
  \textup{}-extremal point. 
  Then  does not lie
  in the pencil of any other point  
  outside .
\end{claim}

\begin{proof}
Suppose that   for 
another point  outside of
.  Then a vertex of  would
be more extremal in direction  than 
. It cannot be , since then
 would not be extremal in direction 
. It also cannot be a
vertex of , because  lies in 
, while all vertices of 
lie on  or in . 
Thus,  cannot exist.
\end{proof}


\begin{figure}
  \centering
  \includegraphics{figures/union}
  \caption{The lines perpendicular 
    to directions  and  define 
  the upper boundary of the shaded region 
  where  lies. All edges 
  seen by a point in the shaded region 
  can be seen by either  or .}
  \label{fig:union}
\end{figure}

\begin{claim}\label{clm:extremal_pencil}
  Suppose \textup{}-extremal 
  points of  lie outside
  of . Let  and  be 
  two adjacent \textup{}-extremal 
  points and let  be above  
  such that the -coordinate of
   lies between the -coordinates of  
  and .  Then, the portion of  
  below  is contained 
  in .
\end{claim}

\begin{proof} 
By Claim~\ref{clm:overlap}, the 
(closures of the) pencil slabs 
of  and  overlap.
Let  be the last canonical 
direction for which  is extremal 
and  the first canonical direction 
for which  is extremal. As  and  
are adjacent,  and  are 
consecutive in ; see 
Fig.~\ref{fig:union}.
Consider the convex region bounded 
by the vertical downward
ray from , the vertical 
downward ray from , the line parallel
to  through , and the line 
parallel to  through 
. By construction,  lies inside 
this convex region (the shaded
area in Fig.~\ref{fig:union}). 
By convexity, for every 
, at 
least one of  
or  is more extremal with respect 
to  than . Hence,
any edge of  visible from  is 
visible by either  or .
The portion of  below 
 is the union of 
regions below edges of  visible 
from . Therefore, it 
lies in .
\end{proof}

As described in Section~\ref{sec:loc_alg},
the location algorithm determines 
for for each  that
either
(a)  lies outside of ;
(b)  lies inside of , as witnessed 
by a segment ; or
(c)  lies in the pencil of a 
point located outside of .
We also have the -extremal 
vertices  for all .
We now use this information in order to 
find . By Corollary~\ref{cor:ellTail}, with
probability at least ,
for each canonical direction in  
there is a extremal point
outside of  and the total number 
of points outside  is .
We assume that these conditions 
hold. (Otherwise, we can compute  
in  time, affecting the expected
work only by a lower order term.)

For any point , the 
\emph{-pair for } is the pair
of adjacent -extremal points 
such that  lies between them.
The construction algorithm goes through a series 
of steps. The exact details
of some of these steps will be given 
in subsequent claims.
\medskip

\begin{asparaenum}
  \item \label{step-1} 
     Compute the upper hull of the 
     -extremal points.
  \item \label{step-2} 
    For each vertex  of , compute the 
    -pair for .
  \item \label{step-3} 
    For each input 
    point  outside , compute 
    its -pair by binary search.
  \item \label{step-4} 
    For each input point  
    below a segment , in 
     time, either find its -pair 
    or find a segment between -extremal
    points above it. (Details in Claim~\ref{clm:step-4}.)
  \item \label{step-5} 
    For each input point 
     located in the pencil 
    of a non--extremal point, 
    in  time, either locate  in the pencil of a 
    -extremal point or determine 
    that it is outside . In 
    the latter case, use binary search to find its
    -pair.
  \item \label{step-6} 
    For each input 
    point  located in the pencil 
    of an -extremal point, in  
    time, find a segment between -extremal 
    points above it or find its 
    -pair. (Details for 
    both steps in Claim~\ref{clm:step-56}.)
  \item \label{step-7} 
    By now, for every 
    non--extremal , 
    we found a -pair or proved 
     non-extremal through a -extremal 
    segment above it.  For every pair 
     of adjacent -extremal 
    points, find the set  of points
    that lie above . 
    Use an output-sensitive 
    upper hull algorithm~\cite{KirkpatrickSe86} to find the convex 
    hull of . Finally, concatenate the resulting 
    convex hulls to obtain . 
\end{asparaenum}

\begin{claim} \label{clm:ext-cand} 
  After the location algorithm, for each
  canonical direction , the extremal candidate 
   is the
  actual extremal point  in direction .  
\end{claim}

\begin{proof} 
By Claim~\ref{clm:no_pencil_overlap}, 
 does not lie in the pencil of any other
point in . Hence, the location algorithm 
classifies  as the extremal candidate for 
, and this choice does 
not change later on.
\end{proof}


\begin{claim} \label{clm:step-123} 
  The total running time for 
  Steps~\ref{step-1},\ref{step-2},\ref{step-3}
  and all binary searches in 
  Step~\ref{step-5} is .
\end{claim}

\begin{proof} 
There are  -extremal 
points, so finding their upper hull takes  time. 
We simultaneously traverse this upper hull and  
to obtain the -pairs
for all vertices of . As there are
 points outside of , the
total time for the binary searches is .
\end{proof}

\begin{figure}
  \centering
  \includegraphics{figures/step-5}
  \caption{(left) In Step~\ref{step-4}, the 
    region below  is 
  either below the middle segment of 
  or between one of the two -pairs. 
  (right) In Step~\ref{step-6}, 
  can be partitioned into regions below 
  , below , 
  between , or between .}
  \label{fig:step-5}
\end{figure}


\begin{claim} \label{clm:step-4} 
  Suppose  lies below a segment .
  Using the information gathered before Step~\ref{step-4},
  we can either find its -pair or a segment between 
  -extremal points above it in  time.
\end{claim}

\begin{proof} 
Let  and  be the endpoints of 
. Consider the 
upper hull  of the at most four
-extremal points  that define 
the -pairs of  and 
; see Fig.~\ref{fig:step-5}(left). 
The hull  has at most three edges, 
and only the middle one (if it 
exists) might not be between two 
adjacent -extremal points. 
If the middle edge of  exists, it lies 
strictly above . This is because 
the endpoints of the middle edge have -coordinates 
between  and  and lie outside of 
 (since they are -extremal), 
while  is inside .
Now we compare  with the upper hull . This 
either finds a -pair
for  (if  lies in the interval 
corresponding to the leftmost or rightmost 
edge of ) or shows that  lies below a segment between
two -extremal points (if it lies in the
interval corresponding to the middle edge of ).
\end{proof}

\begin{claim} \label{clm:step-56} 
  Suppose  lies in , 
  where  is above , and the 
  construction algorithm has completed 
  Step~\ref{step-4}.  If  is 
  non--extremal, in  time, 
  we can either find a -extremal point 
   such that , or determine
  that  is above . 
  If  is -extremal, then in  time
  we can find a -segment above  or find the 
  -pair for .
\end{claim}

\begin{proof} 
Let  be non--extremal.  
As  is outside , we know 
the -pair  for . 
By Claim~\ref{clm:extremal_pencil}, if
 lies below , it is in  or 
in . We can determine which (if at all)
in  time.

Let  be -extremal, and ,
 the vertices of  on the 
boundary of , where  is to the
left; see Fig.~\ref{fig:step-5}(right). 
Let  be 's -pair,
where  is to the left. 
Similarly,  is 's -pair.
The segments 
and  are above . Furthermore, 
the pencil slab of 
is between  and . One of the 
following must be true for any point in :
it is below , below 
, between ,
or between . This can be 
determined in  time.
\end{proof}

We are now armed with all the facts to bound the running time.

\begin{lemma}
  With the information from the location algorithm, 
   can be computed in expected time .
\end{lemma}

\begin{proof} 
By Claims~\ref{clm:step-123},~\ref{clm:step-4}, 
and~\ref{clm:step-56}, the  
first six steps take  time. Let 
the -extremal points
be ordered . Let 
 be the number of points
in  and  
the number of extremal points
in this set. We use an output-sensitive
upper hull algorithm, so
the running time of Step~\ref{step-7} is 
.
By Lemma~\ref{lem:canonicalDir}, this
is , as desired.
\end{proof}

\section{Proofs of 
  Lemma~\ref{lem:canonicalDir} and 
  Lemma~\ref{lem:ellTail}}
\label{sec:lproofs}

We begin with some preliminaries
about projective duality and 
a probabilistic claim
about geometric constructions 
over product distributions.
Consider an input .
As is well known, there is a 
\emph{dual} set  of lines
that helps us understand 
the properties of . More 
precisely, we use the standard
duality along the unit paraboloid 
that maps a point  
to the line 
and vice versa. The \emph{lower envelope} 
of  is the
pointwise minimum of the  
lines , 
considered as univariate functions. 
We denote it by .
There is a one-to-one correspondence 
between the vertices and edges of 
 and the edges and 
vertices of . 
More generally, for , 
the \emph{-level} of  is 
the closure of the set of all points 
that lie on lines of  and that 
have exactly  lines of  strictly 
below them.  The -level is 
an -monotone polygonal curve, 
and we denote it by ; 
see Fig.~\ref{fig:arrangement}.
Finally, the \emph{-level} 
of , , is the
set of all points on lines in  that 
are on or below . 

Consider the following abstract 
procedure. Let  be a constant,
and for any set  of  lines, 
let  be some geometric region
defined by the lines in .
That is,  is a function
from sets of lines of size  to 
geometric regions (i.e., subsets of
of ). For example, 
may be a triangle or trapezoid formed 
by some lines in . For some such 
region  and a line , let 
 be a boolean
function, taking as input a line 
and a geometric region.

Suppose we take a random 
instance , and we 
apply some procedure to determine 
various subsets  
of  lines from , 
chosen based on 
the sums  
.
Now generate another random 
instance . What
can be say about the values of 
? 
We expect them to resemble 
, 
but we have to deal with
subtle issues of dependencies. 
In the former case,  actually 
depends on , while in the 
latter case it does not. 
Nonetheless, we can apply 
concentration inequalities
to make statements about 
.
Let  be a set 
of  indices in , and 
set .
The following lemma can be 
seen as generalization of Lemma~3.2 in 
Ailon \etal~\cite{AilonCCLMS11}, with
a very similar proof.

\begin{lemma}\label{lem:rand-const} 
  Let  an integer, and 
  ,  increasing 
  functions such that 
  ,
  for a constant  and large enough
  . The 
  following holds with probability at 
  least  over a random 
  .  For all index sets 
   of size , if 
  , 
  then for some absolute constant ,
  
\end{lemma}

\begin{proof} 
Fix an index set  of size ,
and a set 
of lines. By independence, the distributions
, , remain unchanged, 
and we generate a random 
 conditioned on  being fixed. 
(This means that we sample random 
lines , for .) 
We have 
, 
so if 
, 
then 
,
for  
and .
What can we say about 
, 
for an independent ? 
Since  is fixed,
 and 
 are 
identically distributed.

Define (independent) indicator variables 
, 
and let 
 and 
. 
Given that one draw of 
 is in , 
we want to give bounds on another draw.
This is basically a Bayesian problem, 
in that we effectively construct 
a prior over .
Two Chernoff bounds suffice 
for the argument. 

\begin{claim}\label{clm:cher} 
  Consider a single draw of 
   and suppose that 
  ,
  With probability at least 
  , 
  .
\end{claim}

\begin{proof} 
Apply Chernoff 
bounds~\cite[Theorem~1.1]{DubhashiPa09}:
if ,
then , so 

noting that 
.
With probability at least 
, if , 
then .
We repeat the argument with 
a lower tail Chernoff bound. 
If , 

With probability at least 
, if , 
then .
Now take a union bound.
\end{proof}

In Claim~\ref{clm:cher},
we conditioned on a fixed , 
but the bound holds irrespective 
of , and hence is
holds unconditionally. 
Therefore, for a fixed , 
with probability at least 
 over , if 
, then 
.
Given that , this 
implies: 
if ,  then
.

There are  choices for , 
so by a union bound 
the above holds for all  simultaneously
with probability at least . 
Suppose we choose a  with this property,
and consider drawing . 
This is effectively an independent draw of ,
so applying Chernoff bounds again, 
for sufficiently small constants 
,

\end{proof}

\subsection{Proof of Lemma~\ref{lem:canonicalDir}} 
\label{sec:canonicalDir}

We sample a random input 
and take the -level 
of . Let 
the upper hull of its vertices; 
see Fig.~\ref{fig:arrangement}.
\begin{figure}
  \centering
  \includegraphics{figures/arrangement}
  \caption{The arrangement of : the 
    dark black line is . 
    The thick lighter line
    is , the upper hull of the vertices 
    in the level. The shaded region 
    is a possible trapezoid .}
  \label{fig:arrangement}
\end{figure}
\begin{claim}\label{clm:H_properties}
  The hull  has the following properties:
  \begin{asparaenum}
    \item 
      the curve  lies below ;
     \item 
       each line of  either supports 
       an edge of  or intersects 
       it at most twice; and
     \item 
        has  vertices.
  \end{asparaenum}
\end{claim}
\begin{proof} 
Let  be any point 
on , and let 
be the closest vertices of  with
.
Any line in  below
 must also be below  or . 
There are exactly  lines
under  and under , as they lie
on the -level. Hence,
there are at most  lines 
below .  The second property 
is a direct consequence of convexity.
The third property follows from the 
second: every vertex of  lies 
on some line of , and hence
there can be at most  vertices.
\end{proof}

Let  be the 
points given by every -th 
point in which a line of  meets 
 (either as an intersection point 
or as the endpoint of a segment), 
ordered from right to left.
By Claim~\ref{clm:H_properties}(2), there 
are  points . 
Let  be their upper upper hull.
Clearly,  lies below .
Draw a vertical downward ray through each vertex
. This subdivides the region below 
 into semi-unbounded trapezoids
 with
the following properties:
(i) each vertical boundary ray 
  of a trapezoid  is intersected
  by at least  and at most
   lines of  
  (Claim~\ref{clm:H_properties}(1)); and 
(ii) the upper boundary
  segment of each  is intersected 
  by at most  lines in 
   (by construction); see 
  Fig.~\ref{fig:arrangement}.
The next claim follows from an 
application of Lemma~\ref{lem:rand-const}.

\begin{claim}\label{clm:taui}
  With probability at least 
   \textup(over 
  \textup), the 
  following holds for all 
  trapezoids : 
  generate an
  independent .
  \begin{asparaenum}
    \item With probability 
    \textup(over \textup) 
    at least , 
    there exists a line in  
    that intersects both 
    boundary rays of ; and
    \item with probability 
    \textup(over \textup) at 
    least , at 
    most  lines 
    of  intersect .
\end{asparaenum}
\end{claim}

\begin{proof} 
We apply Lemma~\ref{lem:rand-const} 
for both parts.
For a set  
of four lines,
define  as the downward 
unbounded vertical trapezoid formed by 
the segment between the intersection
points of  and 
. 
All trapezoids  
are of this form, with 
a set of four lines from .
Set  (for line  
and trapezoid )
to  if  intersects both 
parallel sides of , and  otherwise.

Since  is an unbounded
trapezoid, a line that 
intersects it either 
intersects the upper segment
or intersects both boundary rays. 
In our sample , the number 
of lines with the former
property is at most 
and the number of lines with 
the latter property is in 
.
Hence, the sum 
 is at least 
 
and at most .
By Lemma~\ref{lem:rand-const}, 
the number of lines in  intersecting
both vertical lines is 
 with probability 
at least 
.

For the second part, we 
set  
if  
intersects , and  otherwise. 
Any line that intersects 
must intersect one of the 
vertical boundaries,
so 
.
By Lemma~\ref{lem:rand-const}, 
the number of lines in  intersecting
 is  
with probability at least .
\end{proof}

Each point  is dual 
to a line . We define the
directions in  by 
taking upward unit normals to 
the lines .  (Since the 's 
are ordered from right to left, this 
gives  in clockwise order.)
These directions can 
be found in 
time: we can compute  
and its upper hull  in  
time~\cite{ColeShYa87,Dey98}.
To determine the points , 
we perform  binary searches over
, and then sort the intersection points. 
When  is
known,  can be found 
in constant time.

Now consider a random . 
The -extremal 
vertices  and  
correspond to the lowest line in 
 that intersects the left and the
right boundary ray of . 
The number of extremal points between
 and  is the number of edges 
on the lower envelope
of  between  and 
. By Claim~\ref{clm:taui}(1),
this lower envelope lies entirely 
inside  with probability
at least . 
By Claim~\ref{clm:taui}(2) 
(and a union bound), the number  of
extremal points between  and
 is at most  
with probability at least .
Thus, 

Adding over , 


\subsection{Proof of Lemma~\ref{lem:ellTail}}\label{sec:ellTail}

To compute the canonical lines 
 for the directions
, we 
consider again the dual 
sample .
Let  be the point on
 with 
the same -coordinate as 
, where  
is a sufficiently
small constant.
Set .
Then 
 is normal to ,
and the construction takes  
 time. 
We restate the
main technical part 
of Lemma~\ref{lem:ellTail}.

\begin{lemma}
  With probability at least  
  over the construction, 
  for every , 
  
\end{lemma}

\begin{proof} 
A point  lies in  if
and only if 
intersects the downward 
vertical ray  from .
We set up an application of 
Lemma~\ref{lem:rand-const}. 
For a pair of lines  
(all in dual space), define 
 as 
the downward vertical ray
from . 
Every  
is formed by the intersection 
of two lines from . For 
such a region 
and line , set 
 to be  if 
 intersects 
and  otherwise.
By construction, 
.
We apply Lemma~\ref{lem:rand-const}. 
With probability at least 
 over  (for sufficiently 
large  and
small enough ),
.
\end{proof}

\section*{Acknowledgements} \label{sec:ack}

C. Seshadhri was supported 
by the Early Career LDRD 
program at Sandia National
Laboratories. Sandia National 
Laboratories is a multi-program laboratory
managed and operated by Sandia 
Corporation, a wholly owned subsidiary of
Lockheed Martin Corporation, for 
the U.S. Department of Energy's National
Nuclear Security Administration 
under contract DE-AC04-94AL85000.
W. Mulzer was supported in 
part by DFG grant MU/3501/1.

\bibliographystyle{abbrv}
\bibliography{si-ch}
\end{document}
