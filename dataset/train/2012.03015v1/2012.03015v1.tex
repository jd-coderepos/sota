\ifx\allfiles\undefined

\documentclass[letterpaper]{article}
\begin{document}
\else
\chapter{experiments}
\fi




\section{4\quad Experiments}

We evaluate our CIA-SSD on the KITTI 3D object benchmark~\cite{geiger2013vision} with 7,481 training samples and 7,518 test samples.
The training samples are further divided into a training set (3,712 samples) and a validation set (3,769 samples).
Following previous works,~\eg, SASSD~\cite{he2020structure} and SECOND~\cite{yan2018second}, we conducted experiments on the most commonly-used car category and evaluated the results by average precision (AP) with IoU threshold 0.7.
Also, the dataset has three difficulty levels (easy, moderate, and hard) based on the object size, occlusion, and truncation levels.
Figure~\ref{detresults} shows some of our predicted bounding boxes projected onto color images.



\begin{table}[t]
   \centering
   \footnotesize
   \resizebox{\columnwidth}{!}{
   \begin{tabular}{|c|c|c||c|c|c|}
       \hline
       \multicolumn{1}{|c|}{ \multirow{2}{*}{Type}} & \multicolumn{1}{c|}{ \multirow{2}{*}{Method}} & \multicolumn{1}{c||}{ \multirow{2}{*}{Data}} & \multicolumn{3}{|c|}{} \\ \cline{4-6}
       \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c||}{} & \multicolumn{1}{|c|}{Easy} & \multicolumn{1}{|c|}{Mod} & \multicolumn{1}{|c|}{Hard} \\
       \hline
       \hline
      \multirow {12}{*}{2-stage}
         & MV3D~\shortcite{MV3D}                               & {LiDAR+RGB} & 74.97  & 63.63  & 54.00 \\
      {} & F-PointNet~\shortcite{FPOINTNET}                    & {LiDAR+RGB} & 82.19  & 69.79  & 60.59 \\
      {} & AVOD~\shortcite{AVOD}                               & {LiDAR+RGB} & 83.07  & 71.76  & 65.73 \\
      {} & PI-RCNN~\shortcite{xie2020pi}                       & {LiDAR+RGB} & 84.37  & 74.82  & 70.03 \\
      {} & PointRCNN~\shortcite{shi2019pointrcnn}              & {LiDAR}     & 86.96  & 75.64  & 70.70 \\
      {} & F-ConvNet~\shortcite{wang2019frustum}               & {LiDAR+RGB} & 87.36  & 76.39  & 66.69 \\
      {} & 3D IoU Loss~\shortcite{zhou2019iou}                 & {LiDAR}     & 86.16  & 76.50  & 71.39 \\
      {} & Patches~\shortcite{lehner2019patch}                 & {LiDAR}     & 88.67  & 77.20  & 71.82 \\
      {} & Fast PointRCNN~\shortcite{Chen2019fastpointrcnn}    & {LiDAR}     & 85.29  & 77.40  & 70.24 \\
      {} & UberATG-MMF~\shortcite{liang2019multi}              & {LiDAR+RGB} & 88.40  & 77.43  & 70.22 \\
      {} & Part-~\shortcite{shi2020points}                & {LiDAR}     & 87.81  & 78.49  & 73.51 \\
      {} & 3D IoU-Net~\shortcite{li20203d}                     & {LiDAR}     & 87.96  & 79.03  & 72.78 \\
      {} & STD~\shortcite{yang2019std}                         & {LiDAR}     & 87.95  & 79.71  & 75.09 \\
      {} & 3D-CVF~\shortcite{yoo20203d}                        & {LiDAR+RGB}  & 88.84  & 79.72  & 72.80 \\
      {} & PV-RCNN~\shortcite{shi2020pv}                       & {LiDAR}     & {\em 90.25}  & {\em 81.43}  & {\em 76.82} \\
      \hline
      \hline
      \multirow {11}{*}{1-stage}
       & VoxelNet~\shortcite{zhou2018voxelnet}                 &{LiDAR}      & 77.82  & 64.17  & 57.51 \\
      {} & ContFuse~\shortcite{CONTFUSE}                       &{LiDAR+RGB}  & 83.68  & 68.78  & 61.67 \\
      {} & SECOND~\shortcite{yan2018second}                    &{LiDAR}      & 83.34  & 72.55  & 65.82 \\
      {} & PointPillars~\shortcite{lang2019pointpillars}       &{LiDAR}      & 82.58  & 74.31  & 68.99 \\
      {} & TANet~\shortcite{liu2020tanet}                      &{LiDAR}      & 84.39  & 75.94  & 68.82 \\
      {} & Associate-3Ddet~\shortcite{du2020associate}         &{LiDAR}      & 85.99  & 77.40  & 70.53 \\
      {} & Point-GNN~\shortcite{shi2020point}                  &{LiDAR}      & 88.33  & 79.47  & 72.29 \\
      {} & 3DSSD~\shortcite{yang20203dssd}                     &{LiDAR}      & 88.36  & 79.57  &\bf 74.55 \\
      {} & SASSD~\shortcite{he2020structure}                   &{LiDAR}      & 88.75  & 79.79  & 74.16 \\ \cline{2-6}
      {} & CIA-SSD (ours)                                      &{LiDAR} & \bf 89.59  & \bf 80.28  & 72.87 \\
      \hline
   \end{tabular}
   }
   \vspace*{-2mm}
   \caption{Comparison with the state-of-the-art methods on the KITTI test set.
   The 3D average precisions of 40 sampling recall points for car detection are evaluated on the KITTI official server; from the official ranking metric ``{\em Moderate AP\/},'' we can see that our CIA-SSD attains the top performance compared with all 1-stage detectors and is comparable with the top 2-stage detector on this challenging problem.
   }
   \label{table1}
\end{table}


\begin{table}[t]
   \centering \addtolength{\tabcolsep}{-1pt}
   \footnotesize
   \begin{tabular}{|c|c|c|c|}
      \hline
       \multicolumn{1}{|c|}{ \multirow{2}{*}{Method}} & \multicolumn{3}{|c|}{} \\ \cline{2-4}
       \multicolumn{1}{|c|}{}  & \multicolumn{1}{|c|}{Easy} & \multicolumn{1}{|c|}{Mod} & \multicolumn{1}{|c|}{Hard} \\
       \hline
       \hline
       VoxelNet~\shortcite{zhou2018voxelnet}          & 81.97  & 65.46 & 62.85 \\
       ContFuse~\shortcite{CONTFUSE}                  & 86.32  & 73.25 & 67.81 \\
       SECOND~\shortcite{yan2018second}               & 87.43  & 76.48 & 69.10 \\
       TANet~\shortcite{liu2020tanet}                 & 88.21  & 77.85 & 75.62  \\
       PointPillars~\shortcite{lang2019pointpillars}  &  -     & 77.98 & - \\
       Point-GNN~\shortcite{shi2020point}             & 87.89  & 78.34 & 77.38 \\
       Associate-3Ddet~\shortcite{du2020associate}    & 89.29  & 79.17 & 77.76 \\
       3DSSD~\shortcite{yang20203dssd}                & 89.71  & 79.45 & 78.67 \\
       SASSD~\shortcite{he2020structure}              & \bf90.15  & \bf79.91 & 78.78 \\ \cline{1-4}
       CIA-SSD (ours)                                 & 90.04  & 79.81 & \bf78.80 \\
      \hline
   \end{tabular}
   \vspace*{-2mm}
   \caption{Comparison with the state-of-the-art single-stage detectors on the KITTI validation set, in which the 3D average precisions for car detection are based on 11 sampling recall points (vs. 40 points in the KITTI test set).}
\label{table2}
   \vspace{-2mm} 
\end{table}


\subsection{4.1\quad Implementation Details}

We voxelize the input point cloud into a grid of resolutions [0.05, 0.05, 0.1] meters in ranges [0, 70.4], [-40, 40], and [-3, 1] meters along the , , and  axes, respectively.
The anchors are pre-defined evenly on the BEV feature map with the same dimensions (width=1.6m, length=3.9m, height=1.56m) and two possible orientations (0{\degree} or 90{\degree}).
Further, they are divided into three categories (positive, negative, and ignored) based on the matching strategy in~\cite{zhou2018voxelnet} with IoU thresholds 0.45 and 0.6.
 
We consider four types of data augmentations to enhance our model's generalization ability.
The first type is a global augmentation on the entire point cloud, including random rotation, scaling, and flipping.
The second type is a local augmentation on a portion of the point cloud around a ground-truth object, including random rotation and translation.
The third type is a ground-truth augmentation following SECOND~\cite{yan2018second}.
Last, we filter out objects with difficulty levels not attributed to easy, moderate, and hard to improve the quality of the positive samples, and take also objects of similar categories, such as van for car, as the targets to alleviate model confusion in the training.


In the SSFA module, the spatial and semantic groups have three stacked convolution layers of kernel 3x3 with a number of channels 128 and 256, respectively.
After the spatial and semantic groups, there is one 1x1 convolution layer with 128 and 256 channels separately.
The 2D DeConv layers have 3x3 kernels and 128 output channels with stride two.
Before the attentional fusion, we use a 3x3 convolution layer with 128 output channels to transform each group feature.
In the attentional fusion, the convolutional layers have 3x3 kernels and one output channel.
Other network settings follow those in SECOND~\cite{yan2018second}.

We use the ADAM optimizer~\cite{kingma2014adam} with the cosine annealing learning rate~\cite{loshchilov2016sgdr} to train our model with a batch size of four on a single GPU card for 60 epochs.
Further, we empirically set =\;4 (in the confidence function), =\;2.6 (in DI-NMS), and =\;\{0.0009, 0.009, 0.1, 1\} (in DI-NMS) for BEV distances in ranges [0, 20m), [20m, 40m), [40m, 60m), and [60m, 70.4m], respectively.





\subsection{4.2\quad Comparison with State-of-the-Arts}
We compare our CIA-SSD with the state-of-the-art methods listed in Table~\ref{table1}.
As shown in the table, our model ranks the 1 place in terms of moderate and easy AP,~\ie, 80.28\% and 89.59\% respectively, among all the single-stage detectors.
The ``moderate AP'' is the official ranking metric for 3D detection on the KITTI official test server.
Our CIA-SSD outperforms all the state-of-the-art single-stage detectors, including the very recent ones, including Point-GNN, 3DSSD, and SASSD by about 0.5 to 0.8 points under this metric.
Besides, while two-stage detectors generally perform better than single-stage detectors due to the extra second-stage refinement,
our proposed single-stage detector still outperforms most of the recent two-stage detectors,~\eg, 3D-CVF, STD, and Part- by about 0.6 to 1.8 points on moderate AP.
Furthermore, we shall show the high efficiency of our model compared with two-stage detectors in Section {\color{red} 4.4}.

Although our model sets the new state-of-the-art single-stage results on easy and moderate APs for the KITTI test set, the corresponding APs are slightly lower than some of the state-of-the-art results on the validation set, as shown in Table~\ref{table2}.
Also, our hard AP is lower than the state-of-the-art methods on the test set, while being the top on the validation set.
We argue that such inconsistency may be caused by the mismatched distributions between the KITTI val and test splits, as mentioned in Part-~\cite{shi2020points}.




\subsection{4.3\quad Ablation Study}


We adopt the 2D feature extraction module~\cite{yan2018second} grouped by seven stacked convolutional layers, the normal NMS, and multi-task head without the IoU prediction branch as the baseline modules in the ablation study.




\begin{table}[!t]
    \small
    \begin{center}
        \scalebox{1.0}{
            \begin{tabular}{ccccc|ccc}
                \hline
                \tabincell{c}{\textit{glo.}}
                & \tabincell{c}{\textit{loc.}}
                & \tabincell{c}{\textit{gt aug.}}
                & \tabincell{c}{\textit{sim.}}
                & \tabincell{c}{\textit{diff.}}
                & Easy & Mod & Hard \\
                \hline
                \hline
                           &            &            &            &            & 81.29 & 66.39 & 65.40 \\
                \checkmark &            &            &            &            & 86.53 & 75.73 & 74.77 \\
                \checkmark & \checkmark &            &            &            & 87.18 & 76.33 & 68.64 \\
                \checkmark & \checkmark & \checkmark &            &            & 87.97 & 78.03 & 76.80 \\
                \checkmark & \checkmark & \checkmark &\checkmark  &            & 88.81 & 78.35 & 77.26 \\
                \checkmark & \checkmark & \checkmark &\checkmark  &\checkmark  & \bf89.09 & \bf78.73 & \bf77.56 \\
                \hline
            \end{tabular}
        }
    \end{center}
    \vspace*{-2mm}
    \caption{Ablation study on our implemented data processing techniques on the baseline modules, in which we report the 3D average precisions of 11 sampling recall points for car detection on the KITTI val split.
    Here, ``glo.,'' ``loc,'' ``gt aug.,'' ``sim.,'' and ``diff.'' denote the global augmentation, local augmentation, ground truth augmentation, training with similar type of objects, and filtering objects with difficulty levels not attributed to easy, moderate, \& hard, respectively.}
    \label{table3}
\end{table}



\begin{table}[!t]
    \small
    \begin{center}
        \scalebox{1.0}{
            \begin{tabular}{ccc|ccc}
                \hline
                \tabincell{c}{\textit{SSFA}}
                & \tabincell{c}{\textit{CF}}
                & \tabincell{c}{\textit{DI-NMS}}
                & Easy & Mod  & Hard \\
                \hline
                \hline
                           &            &             & 89.09 & 78.73 & 77.56 \\
                \checkmark &            &             & 89.46 & 79.17 & 77.88 \\
                \checkmark & \checkmark &             & 89.66 & 79.63 & 78.64 \\
                \checkmark & \checkmark & \checkmark  & \bf90.04 & \bf79.81 & \bf78.80 \\
                \hline
            \end{tabular}
        }
    \end{center}
    \vspace*{-2mm}
    \caption{Ablation study on our modules: SSFA, CF, and DI-NMS.
    Here, we report the 3D average precisions of 11 sampling recall points for car detection on the KITTI val split.}
    \label{table4}
\end{table}


{\bf Effect of data processing}
Table~\ref{table3} shows results that reveal the effect of each data processing technique on the baseline modules.
Both global and local augmentations effectively improve easy and moderate APs.
Also, the ground truth augmentation, training with the similar type of objects, and
filtering objects with suitable difficulty levels for ground-truth augmentation boost all levels of AP effectively.
All these techniques help to build up a strong baseline for better validation of our proposed modules.


{\bf Effect of SSFA module}
As shown in Table~\ref{table4}, our SSFA module contributes an improvement of 0.37, 0.44, and 0.32 on the moderate, easy, and hard APs, respectively.
Besides, compared with the baseline 2D feature extraction module under a batch size of four, our SSFA module and the corresponding module in SASSD~\cite{he2020structure} increase our framework's GPU occupation by about 10\% and 27\%, respectively, validating that our SSFA module is lightweight.


{\bf Effect of confidence function}
In Table~\ref{table4}, our confidence function in the multi-task head improves the easy, moderate, and hard APs by 0.20, 0.46, and 0.76, respectively.
Also, we use the metric Pearson Correlation Coefficient (PCC)~\cite{pearson} to measure the correlation between the IoU and confidence of our predicted boxes and calculate the corresponding moderate AP for different  values in Table~\ref{table5}.
We can see that setting =\;4 leads to the highest PCC and moderate AP.
Besides, the AP still increases from 79.17 to 79.30 when setting =\;0 (\ie, without rectifying the classification scores), because the IoU prediction optimizes the model to make the learned features aware of the relative locations between the predicted boxes and ground truths.

{\bf Effect of DI-NMS}
Our DI-NMS raises the easy AP (by 0.38), moderate AP (by 0.18), and hard AP (by 0.16); see Table~\ref{table4}.
However, different from 2D detection with region proposals that often fully cover the objects,
candidate boxes in 3D detection may not have good object coverage, due to the missing points around the boundary, so it is hard to produce well-aligned boxes with the weighted average in DI-NMS.
Hence, the increase in AP with DI-NMS is lower than that with the SSFA module and confidence function.




\begin{table}[!th]
    \small
    \begin{center}
    \resizebox{\columnwidth}{!}{
        \scalebox{1.0}{
            \begin{tabular}{cc@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}c}
\hline
                \multicolumn{1}{c|}{}
                & 0      & 1     & 2     & 3     & 4     & 5  & 6 \\
                \hline
                \multicolumn{1}{c|}{AP}
                & 79.30   & 79.37   & 79.56  &79.61  & \bf79.63  &79.62  &79.61
                \\
                \multicolumn{1}{c|}{PCC}
                & 0.460
                & 0.471
                & 0.502
                & 0.509
                & \bf0.511
                & 0.510
                & 0.509 \\
                \hline
            \end{tabular}
        }}
    \end{center}
    \vspace*{-2mm}
    \caption{Hyperparameter analysis on  in the confidence function.
    Here, we report the 3D moderate average precision (AP) and Pearson Correlation Coefficient (PCC) between the IoU and confidence of the predicted boxes for different  values in car detection on the KITTI val split.}
    \label{table5}
\end{table}




\begin{table}[!th]
    \small
    \begin{center}
    \resizebox{\columnwidth}{!}{
        \scalebox{1.0}{
            \begin{tabular}{cc@{\hspace{3mm}}c@{\hspace{3mm}}c@{\hspace{3mm}}ccc}
                \hline
                \multicolumn{1}{c|}{1-stage}    & Point-GNN &Associate-3Ddet & SASSD & 3DSSD & TANet     & \multicolumn{1}{|c}{Ours (1-stage)}   \\
                \hline
                \multicolumn{1}{c|}{time (ms)}  & 643       &60 & 40.1  & 38    & 34.75     & \multicolumn{1}{|c}{\bf30.76}  \\
                \hline
            \end{tabular}
        }}
    \end{center}
    \vspace*{-2.5mm}
    \caption{Comparing the runtime (in millisecond) of our model with very recent state-of-the-art single-stage detectors, showing that our model runs the fastest among them.}
    \label{table7}
\end{table}


\begin{table}[!th]
    \small
    \begin{center}
    \resizebox{\columnwidth}{!}{
        \scalebox{1.0}{
            \begin{tabular}{cc@{\hspace{3mm}}c@{\hspace{3mm}}ccccc}
                \hline
                \multicolumn{1}{c|}{2-stage}         &  PointRCNN & Part- & STD    & Fast PointRCNN   & \multicolumn{1}{|c}{Ours (1-stage)} \\
                \hline
                \multicolumn{1}{c|}{time (ms)}  & 100         & 80   & 80       & 65    & \multicolumn{1}{|c}{\bf30.76}\\
                \hline
            \end{tabular}
        }}
    \end{center}
    \vspace*{-2.5mm}
    \caption{Comparing the runtime (in millisecond) of our model with state-of-the-art two-stage detectors, showing that our single-stage detector has much higher efficiency.}
    \label{table8}
    \vspace*{-1mm}
\end{table}





\subsection{4.4\quad Runtime Analysis}
Table~\ref{table7} compares the runtime of our CIA-SSD with five very recent state-of-the-art single-stage detectors, and we can see that our CIA-SSD is the fastest one.
Notice that our CIA-SSD is faster than SASSD as the number of channels in our SSFA module is only half of that of SA-SSD in most layers.
Next, Table~\ref{table8} compares the runtime of CIA-SSD with four recent two-stage detectors.
From the table, we can see that CIA-SSD runs significantly faster than these two-stage detectors, confirming the high runtime efficiency of single-stage detectors.
The average inference time of CIA-SSD is 30.76ms, including
(i) 2.84ms for data processing before the network forwarding;
(ii) 24.33ms for data processing in the network; and
(iii) 3.59ms for post-processing to produce the final predictions.
All the reported timing results were averaged from five runs of our program on an Intel Xeon Silver CPU and a single TITAN Xp GPU.





\section{5\quad Conclusion}

This paper presents a new object detector on point clouds, named Confident IoU-Aware Single-Stage object Detector (CIA-SSD).
Our main contributions include the spatial-semantic feature aggregation module for extracting robust spatial-semantic features for object predictions, the formulation of a confidence function to rectify the classification score and alleviate the misalignment between the localization accuracy and classification confidence, and the distance-variant IoU-weighted NMS to obtain smoother results and avoid redundant (zero-IoU) false positives.
The experimental results show that CIA-SSD achieves the state-of-the-art 3D detection performance on the official ranking metric (Moderate AP) for the KITTI benchmark, compared with all the existing single-stage detectors.
Also, CIA-SSD attains real-time detection efficiency and runs the fastest, compared with the very recent state-of-the-art detectors.



\section{Acknowledgement}
We thank reviewers for the valuable comments. This work is funded by the Research Grants Council of the Hong Kong Special Administrative Region (Proj. no. CUHK 14201918).

\bibliography{mybib}
\bibliographystyle{aaai21}


\ifx\allfiles\undefined
\end{document}
