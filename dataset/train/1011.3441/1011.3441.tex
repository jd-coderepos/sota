\documentclass{article}
\usepackage{graphicx,float}
\usepackage{t1enc}
\usepackage[latin1]{inputenc}
\usepackage{mathrsfs,xspace,theorem}
\usepackage{amsmath,amssymb}
\usepackage{textcomp}
\usepackage{authblk} 
\theorembodyfont{\rmfamily}




\floatstyle{ruled}
\newfloat{Algorithm}{htpb}{alg}

\newcommand{\Q}{\mathscr Q}
\newcommand{\E}{\mathscr E}
\newcommand{\gleq}{\sqsubseteq}
\newcommand{\gsup}{\sqcup}
\newcommand{\ginf}{\sqcap}
\newcommand{\gbsup}{\bigsqcup}
\newcommand{\gbinf}{\bigsqcap}
\newcommand{\sing}[1]{\left\{\?#1\?\right\}}
\newcommand{\?}{\mskip1.5mu}

\newcommand{\todo}[1]{\textbf{#1}}

\newcommand{\lst}[2]{,~, ,~}

\newcounter{prgline}
\newcommand{\pl}{\theprgline\addtocounter{prgline}{1}}

\newenvironment{renumerate}{\renewcommand{\theenumi}{(\roman{enumi})}\begin{enumerate}}{\end{enumerate}}

\newcommand{\IF}{\text{\textbf{if}}\xspace}
\newcommand{\BEGIN}{\text{\textbf{begin}}\xspace}
\newcommand{\FOR}{\text{\textbf{for}}\xspace}
\newcommand{\TO}{\text{\textbf{to}}\xspace}
\newcommand{\END}{\text{\textbf{end}}\xspace}
\newcommand{\DO}{\text{\textbf{do}}\xspace}
\newcommand{\OD}{\text{\textbf{od}}\xspace}
\newcommand{\TEST}{\text{\textbf{test}}\xspace}
\newcommand{\THEN}{\text{\textbf{then}}\xspace}
\newcommand{\ELSE}{\text{\textbf{else}}\xspace}
\newcommand{\TRUE}{\text{\textbf{true}}\xspace}
\newcommand{\FALSE}{\text{\textbf{false}}\xspace}
\newcommand{\VAR}{\text{\textbf{var}}\xspace}
\newcommand{\CONST}{\text{\textbf{const}}\xspace}
\newcommand{\RETURN}{\text{\textbf{return}}\xspace}
\newcommand{\ARRAY}{\text{\textbf{array}}\xspace}
\newcommand{\ZERO}{\text{\textbf{zero}}\xspace}
\newcommand{\INC}{\text{\textbf{inc}}\xspace}
\newcommand{\DEC}{\text{\textbf{dec}}\xspace}
\newcommand{\LOR}{\text{\textbf{or}}\xspace}
\newcommand{\LAND}{\text{\textbf{and}}\xspace}
\newcommand{\OF}{\text{\textbf{of}}\xspace}
\newcommand{\WHILE}{\text{\textbf{while}}\xspace}
\newcommand{\SET}{\text{\textbf{set}}\xspace}
\newcommand{\BOOLEAN}{\text{\textbf{boolean}}\xspace}
\newcommand{\PROC}{\text{\textbf{procedure}}\xspace}
\newcommand{\GOTO}{\text{\textbf{goto}}\xspace}
\newcommand{\STOP}{\text{\textbf{stop}}\xspace}
\newcommand{\SUCC}{\text{succ}}
\newcommand{\NULL}{\text{\textbf{null}}\xspace}
\newcommand{\FUNCTION}{\text{\textbf{function}}\xspace}
\newcommand{\PROCEDURE}{\text{\textbf{procedure}}\xspace}
\newcommand{\var}[1]{{\itshape #1}}

\providecommand{\comp}{\circ}

\newcounter{noqed}
\newcommand{\qed}{ \ifmmode\mbox{
}\fi\rule[-.05em]{.3em}{.7em}\setcounter{noqed}{0}}
\newenvironment{proof}[1][{}]{\noindent{\bf Proof#1.
}\setcounter{noqed}{1}}{\ifnum\value{noqed}=1\qed\fi\par\medskip}

\newcommand{\CCB}{Clarke--Cormack--Burkowski\xspace}

\newcommand{\rank}{\operatorname{rank}}
\newcommand{\bucket}{\operatorname{bucket}}
\newcommand{\cnt}{\operatorname{count}}
\newcommand{\select}{\operatorname{select}}
\newcommand{\logl}{\lceil\log\ell\rceil}

\newcommand{\AND}{\operatorname{AND}}
\newcommand{\OR}{\operatorname{OR}}
\newcommand{\BLOCK}{\operatorname{BLOCK}}
\newcommand{\OAND}{\operatorname{AND}_{\mathalpha\leq}}
\newcommand{\ONAND}{\operatorname{AND}_{\mathalpha<}}
\newcommand{\LOWPASS}{\operatorname{LOWPASS}}
\newcommand{\NEAR}{\operatorname{NEAR}}

\newcommand{\lsem}{\mathopen{[\![}}
\newcommand{\rsem}{\mathclose{]\!]}}
\newcommand{\sat}[2]{\lsem #2 \rsem_{#1}}

\def\..{\,\mathpunct{\ldotp\ldotp}} 

\newcommand{\ebsuff}{\trianglelefteq}
\newcommand{\rmore}{\trianglerighteq}
\newcommand{\sbprol}{\preceq}
\newcommand{\sbprolneq}{\prec}
\newcommand{\kw}[1]{\textbf{#1}}

\newcommand{\I}{\mathscr I}
\newcommand{\C}{\mathscr C}
\newcommand{\A}{\mathscr A}
\newcommand{\op}{{\operatorname{op}}}
\newcommand{\url}{\cite{myurl}}



\newtheorem{lemma}{Lemma} 
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}

\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}


\begin{document} 
\title{Worst case efficient single and multiple string matching in the Word-RAM model
\thanks{This work is supported by the french ANR project MAPPI.}
\thanks{A preliminary version of this paper was presented at the 21st International Workshop on Combinatorial Algorithms(IWOCA)  London 2010}
}   

\author{Djamal Belazzougui}
\affil{LIAFA, Univ. Paris Diderot - Paris 7, 75205 Paris Cedex 13, France dbelaz@liafa.jussieu.fr}
\bibliographystyle{abbrv}
\maketitle
\begin{abstract}


In this paper, we explore worst-case solutions for the problems of single and multiple matching on strings in the word RAM model with word length . 
In the first problem, we have to build a data structure based on a pattern  of length  over an alphabet of size  such that we can answer to the following query: given a text  of length , where each character is encoded using  bits return the positions of all the occurrences of  in  (in the following we refer by  to the number of reported occurrences). For the multi-pattern matching problem we have a set  of  patterns of total length  and a query on a text  consists in finding all positions of all occurrences in  of the patterns in . As each character of the text is encoded using  bits and we can read  bits in constant time in the RAM model, we assume that we can read up to  consecutive characters of the text in one time step. This implies that the fastest possible query time for both problems is . In this paper we present several different results for both problems which come close to that best possible query time. 
We first present two different linear space data structures for the first and second problem: the first one answers to single pattern matching queries in time  while the second one answers to multiple pattern matching queries to  where  is the length of the shortest pattern. We then show how a simple application of the four russian technique permits to get data structures with query times independent of the length of the shortest pattern (the length of the only pattern in case of single string matching) at the expense of using more space.



\end{abstract}
\section{Introduction}
The problems of string pattern matching and multiple string pattern matching are classical algorithmic problems in the area of pattern matching. In the multiple string matching problem, we have to preprocess a dictionary of  strings of total length  characters over an alphabet of size  so that we can answer to the following query: given any text of length , find all occurrences in the text of any of the  strings. In the case of single string matching, we simply have .
\\The textbook solutions for the two problems are the Knuth-Morris-Pratt~\cite{KMP77} (KMP for short) automaton for the single string matching problem and the Aho-Corasick~\cite{AC75} automaton (AC for short) for the multiple string matching problem. The AC automaton is actually a generalization of the KMP automaton. 
Both algorithms achieve  query time (where  denotes the number of reported occurrences) using  bits of space\footnote{In this paper we quantify the space usage in bits rather than in words as is usual in other papers} (both automatons are encoded using  pointers occupying  bits each). The query time of both algorithms is in fact optimal if the matching is restricted to read all the characters of the text one by one. However as it was noticed in may previous works, in many cases it is actually possible to avoid reading all the characters of the text and hence achieve a better performance. This stems from the fact that by reading some characters at certain positions in the text, one could conclude whether a match is possible or not without the need to read all the characters. This has led to various algorithms with so-called sublinear query time assuming that the characters of the patterns and/or the text are drawn from some random distribution. 
The first algorithm which exploited that fact was the Boyer-Moore algorithm~\cite{BM77}. Subsequently other algorithms with provably average-optimal performance were devised. Most  notably the BDM and BNDM for single string matching and the multi-BDM~\cite{CR94,CCGJLPR94} and multi-BNDM ~\cite{NR98} for multiple string matching. Those algorithms achieve  time for single string matching (which is optimal according to the lower bound in~\cite{YAO79}) and  time for multiple string matching, where  is the length of the shortest string in the set. 
Still in the worst case those algorithms may have to read all the text characters and thus have  query time (actually many of those algorithms have an even worse query time in the worst-case, namely ).  
\\A general trend has appeared in the last two decades when many papers have appeared trying to exploit the power of the word RAM model to speed-up and/or reduce the space requirement of classical algorithms and data structures. In this model, the computer operates on words of length  and usual arithmetic and logic operations on the words all take one unit of time. 
\\
In this paper we focus on the worst-case bounds in the RAM model with word length . That is we try to improve on the KMP and AC in the RAM model assuming that we have to read all the characters of the text which are assumed to be stored in a contiguous area in memory using  bits per characters. That means that it is possible to read  consecutive characters of the text in  time. Thus given a text of length  characters, an optimal algorithm should spend  time to report all the occurrence of matching patterns in the text. The main result of this paper is a worst case efficient algorithm whose performance is essentially the addition of a term similar to the average optimal time presented above
plus the time necessary to read all the characters of the text in the RAM model. Unlike many other papers, we only assume that , and not necessarily that . That is we only assume that a pointer to the manipulated data (the text and the patterns), fit in a memory word but the word length  can be arbitrarily larger than  or . This assumption makes it possible to state time bounds which are independent of  and , implying larger speedups for small values of  and . 
\\
In his paper Fredriksson presents a general approach~\cite{F02} which can be applied to speed-up many pattern matching algorithms. This approach which is based on the notion of super-alphabet relies on the use of tabulation (four russian technique). If this approach is applied to our problems of single and multiple string matching queries, given an available precomputed space , we can get a  factor speedup. 
In his paper~\cite{B09}, Bille presented a more space efficient method for single string matching queries which accelerates the KMP algorithm to answer to queries in time  using  bits of space for any constant  such that . 
More generally, the algorithm can be tuned to use an additional amount  of tabulation space in order to provide a  factor speedup. 
\\At the end of his paper, Bille asked two questions: the first one was whether it is possible to get an acceleration proportional to the machine word length  (instead of  or ) using linear space only. The second one was whether it is possible to obtain similar results for the multiple string matching problem. We give partial answers to both questions. Namely, we prove the following two results:
\begin{enumerate}
\item Our first result states that for  strings of minimal length , we can construct an index which occupies linear space and answers to queries in time . This result implies that we can get a speedup factor  if  and get the optimal speedup factor  if . 
\item Our second result implies that for  patterns of arbitrary lengths and an additional  bits of memory, we can obtain a factor  speedup using  bits of memory.
\end{enumerate}
Our first result compares favorably to Bille's and Fredriksson approaches as it does not use any additional tabulation space. In order to obtain any significant speedup, the algorithms of Bille and Fredriksson require a substantial amount of space  which is not guaranteed to be available. Even if such an amount of space was available, the algorithm could run much slower in case  as modern hardware is made of memory hierarchies, where random access to large tables which do not fit in the fast levels of the hierarchy might be much slower than access to small data which fit in faster levels of the hierarchy. 
\\ 
Our second result is useful in case the shortest string is very short and thus, the first result do not provide any speedup. The result is slightly less efficient than that of Bille for single string matching, being a factor  slower (compared to the  speedup of Bille's algorithm). However, our second result efficiently extends to multiple string matching queries, while Bille's algorithms seems not to be easily extensible to multiple string matching queries.\\
The third and fourth results in this paper are concerned with single string matching, where we can have solutions with a better query time than what can be obtained by using the first and second result for matching a single pattern. In particular our results imply the following:
\begin{enumerate}
\item Given a single pattern  of length , we can construct a data structure which occupies a linear space and which can find all  occurrences of  in any text of length  in time . This implies that we can get optimal query time   as long as .
\item For a single string of length  and having some additional  bits of space, we can build a data structure which occupies  bits of space such that all the  occurrences of  in any text of length  are reported in time 
\end{enumerate}

In a recent work~\cite{B10a}, we have tried to use the power of the RAM model to improve the space used by the AC representation to the optimal (up to a constant factor)  bits instead of  bits of the original representation, while maintaining the same query time. In this paper, we attempt to do the converse. That is, we try to use the power of the RAM model to improve the query time of the AC automaton while using the same space as the original representation. 
\\
We emphasize that our results are mostly theoretical in nature. The constants in space usage and query time of our data structures seem rather large. Moreover, in practice average efficient algorithms which have been tuned for years are likely to behave much better than any worst-case efficient algorithm. For example, for DNA matching, it was noted that DNA sequences encountered in practice are rather random and hence average-efficient algorithms tend to perform extremely well for matching in DNA sequences (see ~\cite{RSKKT09} for example). 
\section{Outline of the results}
\subsection{Problem definition, notation and preliminaries}
In this paper, we aim at addressing two problems: the single string pattern matching and the multiple string pattern matching problems. In the single string pattern matching problem we have to build a data structure on a single pattern (string) of length  over an alphabet of size ~\footnote{Our results also apply to the . The only change is in space bounds in which the term  should be replaced by }. In the multiple string pattern matching problem, we have a set  of  patterns of total length  characters where each character is drawn from an alphabet of size . In the first problem, we have to identify all occurrences of the pattern in a text  of length . In the second problem, we have to identify all occurrences of any of the  patterns. 
\\
In this paper, we assume a unit-cost RAM model with word length , and assume that . However  could be arbitrarily larger than  or . We assume that the patterns and the text are drawn from the same alphabet  of size . We assume that all usual RAM operations (multiplications, additions, divisions, shifts, etc...) take one unit of time. 
\\For any string  we denote by  (or ) the substring of  which begins at position  and ends at position  in the string . 
For any integer  we note by  the integer number .
\\
In the paper we make use of two kinds of ordering on the strings: the prefix lexicographic order which is the standard lexicographic ordering (strings are compared right-to-left) and the suffix-lexicographic order which is defined in the same way as prefix lexicographic, but in which string are compared left-to-right instead of right-to-left. The second ordering can be thought as if we write the strings in reverse before comparing them. Unless otherwise stated, string lengths are expressed in terms of number of characters. We make use of the fixed integer bit concatenation operator  which operates on fixed length integers, where  means that  is the integer whose bit representation consists in the concatenation of the bits of the integers  as most significant bits followed by the bits of the integer  as least significant bits. 
We define the function , which returns the number of elements of a set  which have a string  as a suffix. Likewise we define the function , which returns the number of elements of a set  which have a string  as a prefix. 
We also define two other functions  and  as the functions which return the number of elements of a set  which precede the string  in suffix and prefix lexicographic orders respectively.

\subsection{Results} 
The results of this paper are summarized by the following two theorems: 
\begin{theorem}
\label{theorem1}
Given a set  of  strings of total length , where the shortest string is of length , we can build a data structure of size  bits such that given any text  of length , we can find all occurrences of strings of  in  in time .
\end{theorem}
The theorem give us the following interesting corollaries:
For multiple string matching, we have the following two corollaries:
\begin{corollary}
\label{corollary2}
Given a set  of  strings of total length  where each string is of length at least  characters, we can build a data structure of size  bits of space such that given any text  of length , we can find all occurrences of strings of  in  in time .
\end{corollary}
For the case of even larger minimal length, we can get optimal query time :
\begin{corollary}
\label{corollary3}
Given a set  of  strings of total length  where each string is of length at least  characters, we can build a data structure occupying  bits of space such that given any text  of length , we can find all occurrences of strings of  in  in the optimal  time.
\end{corollary}  
The dependence of the bounds in theorem~\ref{theorem1} and its corollaries on minimal patterns lengths is not unusual. 
This dependence exists also in average-optimal algorithms like BDM, BNDM and their multiple patterns variants~\cite{CR94,CCGJLPR94,NR98}. Those  algorithms achieve a  speedup factor on average requiring that the strings are of minimal length . 
Our query time is the addition of a term which represents the time necessary to read all the characters of text in the RAM model and a term which is similar to the query time of the  average optimal algorithms. 
\\
We also show a variation of the first theorem which uses the four russian technique and which will mostly be useful in case the minimal length is too short: 
\begin{theorem}
\label{theorem2}
Given a set  of  strings of total length  and an integer parameter , we can build a data structure occupying  bits of space such that given any text  of length , we can find all  occurrences of strings of  in  in time .
\end{theorem}
The theorem could be interpreted in the following way: having some additional amount  of available memory space, we can achieve a speedup factor  for  using a data structure which occupies  bits of space.  
\\ The theorem gives us two interesting corollaries which depend on the relation between  and . In the case where , by setting  for any , we get the following corollary: 

\begin{corollary}
\label{corollary4}
Given a set  of  strings of total length , we can build a data structure occupying  bits of space such that given any text  of length , we can find all occurrences of strings of  in  in time , where  is any constant such that .
\end{corollary}


In the case  we can get a better speedup by setting :
\begin{corollary}
\label{corollary5}
Given a set  of  strings of total length , we can build a data structure occupying  bits of space such that given any text  of length , we can find all occurrences of strings of  in time .
\end{corollary}
We note that in the case , the result of corollary~\ref{corollary2} is worse by a factor  than that of Bille which achieves a query time of . However the result of Bille does not extend naturally to . The straightforward way of extending Bille's algorithm is to build  data structures and to match the text against all the data structures in parallel. This however would give a running time of  which is worse than our running time  which is linear in  rather than .

As of the technique of Fredriksson, in order to obtain query time , it needs to use at least space  which can be too much in case  is too large. 
In the case of single pattern matching, we can even get a stronger results as we prove the following theorem: 
\begin{theorem}
\label{theorem3}
Given a string  of length , we can build a data structure occupying  bits of space such that given any text  of length , we can find all  occurrences of the string  in  in time .
\end{theorem}
An important implication of this theorem is that single pattern matching in optimal time  is possible for strings of length . 
\\ Similarly to the case of~\ref{theorem1}, we can use the four russian technique to improve the result of~\ref{theorem2} in case  is too short: 
\begin{theorem}
\label{theorem4}
Given a string  of length  we can build a data structure occupying  bits of space such that given any text  of length , we can find all  occurrences of strings of  in  in time . 
\end{theorem}
This last theorem matches the result achieved in Bille's  algorithm. 
\section{Components}

Before we present the details of our main results we first present the main tools and components which are to be used in our solutions. In particular we will make use of several data structures and operations which exploit the power of the word-RAM model. We first describe some basic operations which will be explicitly used for implementing our algorithms. Then we describe some classical geometric and string processing oriented data structures which will be used as black-box components in our data structures.  
\subsection{Bit parallel string processing}
Before we describe the basic bit-parallel operations, we first define how the characters are packed in words. We assume that the pattern and the text are packed in a similar way. Each character is encoded using  bits. The text  is thus encoded using a bit array  which occupies  bits which is  words. We thus assume that have a representation of the text  which fits in a word array  \footnote{Notice that when  is not multiple of  , a character could span a boundary between two consecutive words}. An important technical point is about the endianness, that is the way the bits are ordered in a word which influences the way the characters are packed in memory. We basically have two possibilities: either the bits in a word are ordered from the least to the most significant (little endian) or the converse (big endian). Here we illustrate how a particular character  of the text is extracted. We only present the first case as (little endian) as the latter can easily be deduced from the former: 
\begin{enumerate}
\item First compute .
\item Then read the two words  and .
\item At last we distinguish two cases: 
\begin{itemize}
\item If  (the character  does not span two consecutive words), then return  
\item Otherwise (the character spans the two consecutive words  and ) we return .
\end{itemize}
\end{enumerate}
It can easily be seen that the extraction of a character can be done in constant tome. However, in general we will want to make operations on groups of characters instead of manipulating characters one bye one. This permits to get much faster operations on strings. In particular we will makes use of the following lemma whose proof is omitted and which can easily be implemented using standard bit-parallel instructions. 
\begin{lemma}
Given two strings of lengths  bits, one can compare them (for equality) in  time using bit-parallelism. Moreover, given two strings of length , one can compare them in time . 
\end{lemma}
\paragraph{MSB and LSB operations}
Our solutions for single string matching uses the special instruction  which returns the most significant bit set in a word and similarly  which returns the least significant set bit in a word. 
Those two operations can be simulated in constant time using classical RAM operations (see~\cite{AHNR98,FW93,Brodnik93}).
\begin{lemma}
The two functions  and  
can be implemented in  time provided that the bit-string  is of length  bits.
\end{lemma}

\paragraph{Longest repetition matching}
We will make use of the following tool: given a string  of length  and a string  of length  where both strings are over the same alphabet of size , we would wish to have the following two operations: 
\begin{enumerate}
\item Longest prefix repetition matching: find the largest  such that  ( repeated  times) is a prefix of .
\item Longest suffix repetition matching: find the largest  such that  is a suffix of .
\end{enumerate}
We argue that both operations can be done in . First consider the computation of Longest prefix repetition of a string  of length  into a string  of length . We have two cases:
\begin{enumerate}
\item Suppose that . In this case, it suffice to compare successively  with  for increasing values of  until we reach  or find a mismatch. Each comparison takes time  and thus the whole operation takes at most  time. 
\item Suppose that , in this case we first compute  and then compute  and note that . Now we first compare  with  for increasing values of  until we reach  or find a mismatch. Clearly this step takes time  also. Now, we have determined that . In the final step we compute  and finally  (where  denotes the xor operator) and let  (or  depending on the \emph{endiannes} or the way the processors orders the bits in its words). Now clearly,  is the position of the first bit in which  and  differ. It is clear that the first character in which  and  differ, is precisely character number . From there we deduce that . The computation of the  and the xor operator both take constant time. 
\end{enumerate}
The computation of the longest suffix repetition is symmetric to the computation of the longest prefix repetition except that we use  operation instead of  or vice-versa depending on the endiannes. 
\begin{lemma}
Given a string  of length  and a string  of length  where  the longest prefix (and suffix) repetition of  in  can be found in time .
\end{lemma}
\subsection{Data structures components}
For our results we will use several classical data structures which are illustrated with the following lemmata:
\begin{lemma}\cite{W83}
\label{lemma1}
Given a collection of  intervals over universe  where for any two intervals  and  we have either ,  or  (for any two intervals either one is included in the other or the two intervals are disjoint). We can build a data structure which uses  bits of space such that for any point , we can determine the interval which most tightly encloses  in  time (the smallest interval which encloses ). 
\end{lemma}
For implementing the lemma, we store the set of interval endpoints in a predecessor data structure, namely the Willard's y-fast trie ~\cite{W83} which is a linear space version of the Van Emde Boas tree~\cite{BKZ77}. Then those points divide the universe of size  into  segments and each segment will point to the interval which most tightly encloses the segment. Then a predecessor query will point to the segment which in turn points to the relevant interval. This problem can be thought as a restricted  stabbing problem (in the general problem we do not have the condition that for any two intervals either one is included in the other or the two intervals are disjoint).

\begin{lemma}
\label{lemma2}
Given a collection  of  strings of arbitrary lengths and a function  from  into , we can build a data structure which uses  bits and which which computes  for any  in time  (where  is the length of  in bits). When queried for any  the function returns any value from the set . 
\end{lemma}
This result can easily be obtained using minimal perfect hashing~\cite{FKS84,HT01}. Though perfect hashing is usually defined for fixed  bits integers, a standard string hash function~\cite{DGMP} can be used to first reduce the strings to integers before constructing the minimal perfect hashing on the generated integers. 


\begin{lemma}\cite[Theorem~1]{CHSV08}
\label{lemma3a}
Given a collection  of  strings of variable lengths occupying a memory area of  characters (the strings can possibly overlap), we can build an index which uses  bits so that given any string , we can find the string  which is the longest among all the strings of  which are prefix of  in time  (where  is the length of  in bits). More precisely, the data structure returns .
Moreover the data structure is able to tell whether .
\end{lemma}
This result which is obtained using a string B-tree~\cite{FG99} combined with an LCP array and a compacted trie~\cite{MM93} built on the set of strings, and setting the block size of the string B-tree to . The following lemma is symmetric of the previous one. 
\begin{lemma}
\label{lemma3b}
Given a collection  of  strings of variable lengths occupying a memory area of  characters of space (the strings can possibly overlap), 
we can build an index which uses  bits so that given any string , we can find the string  which is the longest among all the strings of  which are suffix of  in time  (where  is the length of  in bits). More precisely, the data structure returns .
Moreover the data structure is able to tell whether .
\end{lemma}

\begin{lemma}
\label{lemma4}\cite{C86}
Given a set of  rectangles in the plane, we can build a data structure which uses  bits of space so that given any point , we can report all the  occurrences of rectangles which enclose that point in time . 
\end{lemma}
The problem solved by lemma~\ref{lemma4} is called the  stabbing problem or sometimes called the planar point enclosure. The lemma uses the best linear space solution to the problem which is due to Chazelle~\cite{C86} (which is optimal according to the lower bound in ~\cite{P08a}). 

\section{Multiple string matching without tabulation}
\subsection{Overview}
The goal of this section is to show how we can simulate the running of the AC automaton~\cite{AC75}, by processing the characters of the scanned text in blocks of  characters. The central idea of the relies on a reduction of the problem of dictionary matching to the  and  stabbing problems, in addition to the use of standard string data structures namely, string B-trees, suffix arrays and minimal perfect hashing on strings.
At each step, we first read  characters of the text, find the matching patterns which end at one of those characters and finally jump to the state which would have been reached after reading the  characters by the AC automaton (thereby simulating all  and  transitions which would have been traversed by the standard AC automaton for the  characters). Finding the matching patterns is reduced to the  stabbing problems, while jumping to the next state is reduced to  stabbing problem. 
The geometric approach has already been used for dictionary matching problem and for text pattern matching algorithms in general. For example, it has been recently used in order to devise compressed indexes for substring matching~\cite{GV05,N04,CHSV08}. Even more recently the authors of~\cite{TWLY09} have presented a compressed index for dictionary matching which uses a reduction to  stabbing problem. 
\subsection{The data structure}
We now describe the data structure for in more detail. Given the set  of  patterns, we note by  the set of the prefixes of the patterns in  (note that ). It is a well-known fact that there is a bijective relation between the set  and the set of states of the AC automaton. We use the same state representation as the one used in~\cite{B10a}. That is we first sort the states of the automaton in the suffix-lexicographic order of the prefixes to which they correspond, attributing increasing numbers to the states from the interval . Thus the state corresponding to the  empty string gets the number , while the state corresponding to the greatest element of  (in suffix-lexicographic order) gets the largest number which is at most . 
We define  as the state corresponding to the prefix . 
\\ 
Now, the characters of the scanned text, are to be scanned in blocks of  characters. 
For finding occurrences of the patterns in a text , we do  steps. At each step  we do three actions:
\begin{itemize}
\item Read  characters of the text,  (or  characters of the text,  in the last step).
\item Identify all the occurrence of patterns which end at a position  of the text such that  ( in the last step). 
\item If not in the last step go to the next state corresponding to the longest element of  which is a suffix of . 
\end{itemize}
The details of the implementation of each of the last two actions is given in sections ~\ref{subsec:ident_occ} and~\ref{subsec:simul_trans}. 
\\Our AC automaton representation has the following components:
\begin{enumerate}
\item An array  which contains the concatenation of all of the patterns. This array clearly uses  bits of space.
\item Let  be the set of prefixes of  of lengths in . We use an instance of lemma~\ref{lemma3a}, which we denote by  and in which we store the set  (by means of pointers into the array ). Clearly  uses  bits of space (we have  elements stored in  and pointers into  take  bits). We additionally store a vector of  elements which we denote by  and which associates an integer in  with each element stored in . The table  uses 


\item We use an instance of lemma~\ref{lemma2}, which we denote by  and in which we store all the suffixes of strings in  (or equivalently all factors of the strings in ) of length  and for each suffix, store a pointer to its ending position in the array  (if the same factor occurs multiple times in the  we store it only once). As we have at most  elements in  and each pointer (in the array ) to each factor can be encoded using  bits, we conclude that  uses at most  bits of space. 
\item We use an instance of lemma~\ref{lemma3b} which we denote by  and in which we store all the suffixes of strings of  of lengths in  (We note that set by ). It can easily be seen that  also uses  bits of space. 
\item We use a  stabbing data structure (lemma~\ref{lemma1}) in which we store  segments where each segment corresponds to a state of the automaton. This data structure which uses  bits of space is used in order to simulate the transitions in the AC automaton. 
We also store a vector of integers of size  which we denote by  and which associates an integer with each interval stored in the  stabbing data structure. The table  uses  bits of space.
\item We use a  stabbing data structure (lemma~\ref{lemma4}) in which we store up to  rectangles. The space used by this data structure is  bits. We also use a table  which stores triplets of integers associated with each rectangle. The table  will also use  bits. 


\end{enumerate}
We deffer the details about the contents of each component to the full version which uses also to the full version.
Central to the working of our data structure is the following technical lemma:
\begin{lemma}
\label{lemma:tech_lemma}
Given a set of strings . We have that for any two strings  and :
\begin{itemize}
\item  iff  is a prefix of . 
\item  iff  is a suffix of .
\end{itemize}
\end{lemma}
The proof of the lemma is omitted.
\subsection{Simulating transitions}
\label{subsec:simul_trans}
We will use the representation of states similar to the one used in~\cite{B10a}. That is each state of the automaton corresponds to a prefix  and is represented as an integer . The main idea for accelerating transitions is to read the text into blocks of size  characters and then find the next destination state attained after reading those  characters using , , ,  and the  stabbing data structure. More precisely being at a state  and after reading next  characters of the text which form a string , we have to find next state which is the state  such that  is the longest element of  which is suffix of . For that purpose the  stabbing data structure is used in combination with  (which is queried on string ) in order to find  in case . Otherwise if no such  is found the data structure  will be used to find , where . 
The following lemma summarizes the time and the space of the data structures needed to simulate a transition.  
\begin{lemma}
\label{lemma:trans_lemma}
We can build a data structure occupying  bits of space such that if the automaton is in a state , the state  reached after doing all the transitions on  characters, can be computed in  time. 
\end{lemma}
The current state of the AC automaton is actually represented as a value . At the beginning the automaton is at state , and we read the text in blocks of  characters at each step. At the end of each step we have to determine the next state reached by the automaton which is represented by the number . We now show how the transitions of the AC automaton for a block of  characters are simulated. Suppose that we are at step  and the automaton is in the state  corresponding to a prefix . Now we have to read the substring  
and the next state to jump to after reading , is the state  corresponding to the longest element  of  which is a suffix of . 
\\
For simulating transitions we use ,, ,  and the  stabbing data structure. The table  associates to each element of  (each element of  whose length is in ) sorted in suffix lexicographic order the identifier of the states to which they correspond. That is for each  we set . We recall that given any element ,  can be obtained by querying  for the element .  
\\
The  stabbing data structure (lemma~\ref{lemma1}) which is built on numbers occupying  bits each, stores  intervals each of which is defined by two points, where each point is defined by a number which occupies  bits. 
Let  be decomposed by  where  is the suffix of  of length  and  is the prefix of  of length . Let  be the pointer associated with  in  (recall that  associates a unique pointer in  for each occurring factor  of elements of ). We store in the  stabbing data structure the interval , where  and  (recall that  is the number of elements of  which have  as a suffix). The  stabbing data structure naturally associates a unique integer identifier from  with each interval stored in it. We additionally use a table  of size  indexed with the interval identifiers. More precisely, let  be the identifier corresponding to the interval associated with the state  for . We let . That  way once we have found a given interval from the  stabbing data structure, we can index into table  in order to find the corresponding state. 
\\Now queries will happen in the following way: At step , we are at state  corresponding to a prefix  and we are to read the sequence , and must find the longest element of  which is a prefix of . For that, we do the following steps:
\begin{enumerate}
\item We first query  for the string  which will return a unique identifier  which is in fact a pointer to the ending position of a factor . Now, we compare  with . If they are not equal, we go to step , otherwise we continue with the next step. 
\item We query the  stabbing data structure for the point  This query returns the interval (identified by a variable ) which most tightly encloses the point  if it exists. This interval (if it exists) corresponds to a prefix  of  such that  and  is the longest element of  which is a prefix of . If the query returns no interval, we conclude that we have no element of  of length  which is a suffix of  and go to step , otherwise we continue with the next step.
\item We retrieve  which gives us the destination state which concludes the transition. 
\item At this step we are sure that no element of  of length at least  is a suffix of . We thus do a query on  for the string  in order to find the longest element of  which is a suffix of . Note that this element must be of length  and thus must be stored in  and also must be a suffix of . Let  be the identifier of the returned element. 
\item By reading  we retrieve the identifier of the destination state which is given by . This concludes the transition. 
\end{enumerate}
We now give a formal proof of lemma~\ref{lemma:trans_lemma}

\begin{proof} 
We now prove that the above algorithm effectively simulates  consecutive transitions in the automaton. Recall that we are looking for the state corresponding to the longest element  which is a suffix of . 
After we have read the string , we query the data structure  to retrieve a pointer  to a string  which is a factor of some string in  (or equivalently a suffix of some element in ). Then we compare  with  in time . Now we have two cases: 
\begin{itemize}
\item The comparison is not successful, we conclude that no prefix in  has  as a suffix and hence the element  must be shorter than  (otherwise it would have had  as a suffix). That means that  is a suffix of  ( is a suffix of  shorter than ) and hence has length at most . Hence we go the step  to query  for the string  in order to retrieve .
\item The comparison is successful, in which case we know there exists at least one element of , which has  as a suffix. Now we go to step  , querying the  stabbing for the point . The query returns an interval  where  and  for some prefixes  and . Now it can easily be proven that  and that  is the longest element of  which is a suffix of . This is proved by contradiction. By lemma~\ref{lemma:tech_lemma} we have that  must be a suffix of , and we suppose that the longest suffix is  having an associated interval  and . By definition  is a suffix of   and thus by lemma~\ref{lemma:tech_lemma}  which contradicts the fact that  is among all the intervals stored in the  data structure the one which most tightly encloses  (which is implied by lemma~\ref{lemma1}). 
\end{itemize}
Now, in the first case, we go to step  in order to find the longest prefix in  which is a suffix . In the second case, we go to step  looking among the elements which have  as a suffix for the longest one which is a suffix of . If the search is unsuccessful, we conclude that no such element  exists and thus  must be shorter than  and thus go to step  to find the longest prefix in  which is a suffix of . 
\\
The total space usage is clearly  bits as each of ,,, and the  stabbing data structure uses  bits. 
\\
Concerning the query time, it can easily be seen that the steps  and  take constant time, step  takes time , step  takes time  and finally step  takes  time. Summing up, the total time for a transition is .
\end{proof}

\subsection{Identifying matching occurrences}
\label{subsec:ident_occ}
In order to identify matching patterns the  stabbing data structure is used in combination with . 
\begin{lemma}
\label{lemma:report1_lemma}
Given a parameter  and a set  of variable length strings of total length  characters over an alphabet of size , we can build a data structure occupying space  bits, such that if the automaton is at a state  after reading  characters of a text , all the  matching occurrences of  which end at any position in  (or  if ) and begin at any position in  can be computed in  time. 
\end{lemma}
In order to find the matching pattern occurrences at each step, we use , the table  and the  stabbing data structure.
Initially the automaton is at state , we read the first  characters of the text,  and must recognize all occurrences which end in any position . Note that in this first step, any occurrence must end at position  (This is the case, because we have assumed that  is no longer than the length of the shortest pattern). Then at each subsequent step , we read a block  (or the block  in the last step) and must recognize all the occurrences which end at position  (or  in the last step). Suppose that at some step  we are at a state  corresponding to a prefix  and we are to read the block  ( in the last step). It is clear that any matching occurrence must be a substring of  (the string  concatenated to the string ) and moreover, that substring must end inside the string . In other words, any occurrence  is such that , where  is suffix of  and  is prefix of . 
\\
Identifying the pattern involves first computing a point  where  and  is computed by querying  for , then querying the  stabbing data structure in order to get all the rectangles which enclose  as integer identifiers, where each reported rectangle represents one occurrence of one of the patterns. Finally using the table , we can get the matching pattern identifiers along their starting and ending positions. 
We now describe how the set of rectangles is built. For each pattern  of length  we insert  rectangles. Namely, for each  we insert the rectangle which is defined by the two intervals:
\begin{itemize}
\item Let  be the prefix of  of length . Let  be the state corresponding to  (or equivalently the rank of  in suffix-lexicographic order relatively to the set ) and let  be the number of elements of  which have the string  as a suffix. The first interval is given by .
\item Let  be the suffix of  of length . Let  be the unique identifier returned by  for  (recall that  stores all suffixes of lengths at most  of elements of ). Let  be the number of elements of  which have  as a prefix. The second interval is given by . 
\end{itemize}
The  stabbing data structure returns a unique identifier  corresponding to each rectangle. Additionally with the rectangle, we associate a triplet  which is stored in table  at position , where  is the unique integer identifier of the pattern . This table thus uses  bits of space.
\\Now queries will happen in the following way: suppose that we are at state  corresponding to a prefix  and we are to read the block . We first query  for the string  giving us an identifier  corresponding to the longest element  such that  is prefix of . Then we do a  stabbing query for the point . Now for every found rectangle identified by an integer , we retrieve the triplet  from . Now the reported string has identifier , and matches the text at positions .  
We now give a formal proof of lemma~\ref{lemma:report1_lemma}

\begin{proof}
We now prove that the above procedure reports all (and only) matching occurrences. For that it suffices to prove that there exists a bijection between occurrence and reported rectangles. It is easy to see that each occurrence  which begins in  and ends in  can be decomposed as , where  is a suffix of  ( can possibly be the empty string) and  is a prefix of . Then as , we can easily deduce that  and . It is also easy to see that  is a suffix of . Let  be the longest element in  which is a prefix of . It is easy to see that  must be a prefix of . Thus according to lemma~\ref{lemma:tech_lemma} we have that  and . Now recall that  returns  and the  stabbing query is done precisely on the point , which will thus return all (and only) the rectangles corresponding to occurrences. 
\end{proof}
By combining lemma~\ref{lemma:report1_lemma} and lemma~\ref{lemma:trans_lemma}, we directly get theorem~\ref{theorem1} by setting  where  is the length of the shortest string in the set . At any step, we do the following: 
\begin{enumerate}
\item Read the characters , where we set  and we set  if  and  otherwise. 
\item Recognize all the pattern occurrences which start at positions any position  and which terminate at positions in  using lemma~\ref{lemma:report1_lemma}. 
\item Increment step  by setting . Then if , stop the algorithm immediately. 
\item Do a transition using lemma~\ref{lemma:trans_lemma} and return to action 1. 
\end{enumerate}
\subsection{Analysis}
Theorem~\ref{theorem1} is obtained by combining lemma~\ref{lemma:trans_lemma} with lemma~\ref{lemma:report1_lemma}. Namely by setting , where  is the shortest pattern in  in both lemmata we can simulate the running of the automaton in  steps at each step i, spending  to find the  matching occurrences (through lemma~\ref{lemma:report1_lemma}) and  time to simulate the transitions (through lemma~\ref{lemma:trans_lemma}). Summing up over all the  steps, we get the query time stated in the theorem.
We can now formally analyze the correctness and space usage of theorem~\ref{theorem1}.  
\paragraph{Correctness}
The correctness of the query is immediate. If can easily be seen that at each step , we are recognizing all occurrences which end at any position in  (or  in the last step). That is at any step  we can use lemma~\ref{lemma:report1_lemma} to recognize all the occurrence which end at any position in  (or  in the last step) and start at any position . Also at each step , we are at state  reached after reading  characters of the text, and lemma~\ref{lemma:trans_lemma} permits us to jump to the state  which is reached after reading  characters.
\paragraph{Space usage}
Summing up the total space usage by the theorem is  bits as both lemma~\ref{lemma:trans_lemma} and lemma~\ref{lemma:report1_lemma} use  bits. 


\subsection{Consequences}
Theorem~\ref{theorem1} states that we can use  bits of space to identify all occurrence of length at least  in a text  of length  in time . If we suppose that all the patterns are of length at least  bits ( characters), then by setting , we obtain an index which answers to queries in time . As we have  and , the query time simplifies to  . This gives us corollary~\ref{corollary2}. 
An important implication of theorem~\ref{theorem1} is that it is possible to attain the optimal  query time in case the patterns are of sufficient minimal length. Namely if each pattern is of length at least  words (that is  characters), then by setting  in theorem~\ref{theorem1}, we obtain a query time of .  This gives us corollary~\ref{corollary3}.  
\section{Tabulation based solution for multiple-string matching}
\label{section:tabul_sol}
We now prove theorem~\ref{theorem2}. 
A shortcoming of theorem~\ref{theorem1} is that it gives no speedup in case the length of the shortest string in  is too short. In this case we resort to tabulation in order to accelerate matching of short patterns. More specifically, in case, we have a specified quantity  of available memory space (where  as obviously we can not address more than  words of memory), we can precompute lookup tables using a standard technique known as the four russian technique~\cite{ADKF70} so that we can handle queries in time . In theorem~\ref{theorem1} our algorithm reads the text in blocks of size , where  is the length of the shortest pattern. In reality we can not afford to read more than  characters at the each step, because by doing so we may miss a substring of the block of length . Thus in order to be able to choose a larger block size , we must be able to efficiently identify all substrings of any block of (at most)  characters which belong to . The idea is then to use tabulation to answer to such queries in constant times (or rather in time linear in the number of reported occurrences). More in detail, for each possible block of  characters, we have a total of  substrings which could begin at all but the first position of the block. For each possible block of  characters, we could store a list of all substrings belonging to  and each list takes at most  pointers of length  bits. As we have a total of  possible characters, we can use a precomputed table of total size  bits. \begin{lemma}
\label{lemma:report2_lemma}
For a parameter  (where  is any constant such that ) and a set  of patterns where each pattern is of length at most , we can build a data structure occupying  bits of space such that given any string  of length , we can report all the  occurrences of patterns of  in  in  time. 
\end{lemma}
Theorem~\ref{theorem2} is obtained by combining lemmata~\ref{lemma:trans_lemma},~\ref{lemma:report1_lemma} and ~\ref{lemma:report2_lemma}.
Suppose we are given the parameter ; for implementing transitions, we can just use lemma~\ref{lemma:trans_lemma} in which we set , where the transitions are built on the set containing all the patterns. Now in order to report all the matching strings, we build an instance of lemma~\ref{lemma:report1_lemma} on the set  and in which we set  and also build  instances of lemma~\ref{lemma:report2_lemma} for every  such that . More precisely let  be the subset of strings in  of length at most , then the instance number  will be built on the set  using parameter  and will thus for all possible strings of length , store all matching patterns in  of length at most .  
\\ A query on a text of  will work in the following way: we begin at step  and the automaton is at state  which corresponds to the empty string. Recognizing the patterns will consist in the following actions done at each step : 
\begin{enumerate}
\item Read the substring , where  and  (or  if ). 
\item Recognize all the pattern occurrences which start at any position  and which terminate at any position  using lemma~\ref{lemma:report1_lemma}. 
\item Recognize all the matching strings of lengths at most  which are substrings of  using the instance number  of lemma~\ref{lemma:report2_lemma}.
\item Increment step  by setting . Then if , stop the algorithm immediately. 
\item Do a transition using lemma~\ref{lemma:trans_lemma} and return to action 1. 
\end{enumerate}
\subsection{Analysis}
\paragraph{Correctness}
The correctness of the transition is immediate. If can easily be seen that at each step , we are recognizing all occurrences which end at any position in  (or  in the last step). That is at any step :
\begin{itemize}
\item Lemma~\ref{lemma:report1_lemma} recognizes all occurrence which end at any position in  and start at any position .
\item Lemma~\ref{lemma:report2_lemma} recognizes all patterns which end at any position in  and start at any position  such that  using instance number  of the lemma for all but the last step. 
\item The last step of the algorithm recognizes all occurrences which end at any position in  and start at any position  using the instance number  of lemma~\ref{lemma:report2_lemma}. 
\end{itemize} 
Thus at the last step, we will have recognized all the occurrences of patterns in the text . 

\paragraph{Space usage}
It can easily be seen that the space used by lemma~\ref{lemma:trans_lemma} is in fact . The space used by lemma~\ref{lemma:report1_lemma} is  bits. The space used by all the instances of lemma~\ref{lemma:report2_lemma} is bounded above by . That is for each , the instance number  uses  for some constant . Thus the total space usage is upper bounded by  
As we have , the total space used by lemma~\ref{lemma:report2_lemma} can be upper bounded by . Summing up the space used by the three lemmata we get  bits of space.

\subsection{Consequences}
Corollary~\ref{corollary4} derives easily from the theorem. That is, in the case , we can set  for some constant  for any . Then space usage becomes .
\\
Similarly corollary~\ref{corollary5} derives immediately from the theorem. By setting  for some constant , the space usage becomes  bits of space.

\section{Single string matching}
We now turn to the proof of theorems~\ref{theorem3} and ~\ref{theorem4}. In both theorem we only have to match a single pattern  of length  against the text  of length . We first describe the matching algorithm used in theorem~\ref{theorem3} then sketch a possible way to construct the data structure used in the matching algorithm. We finally show the proof of~\ref{theorem4} which is based on the data structure of theorem~\ref{theorem3} combined with the use of the four russian technique.
\subsection{The matching algorithm}
Our string matching algorithm employs properties of periodic strings. For implementing the string matching we will use a sliding window of size  where  and at each step , we shift the window by  characters and spend time . Thus the total running of the string matching will clearly be . We note by  the set of all factors of the string  of length . Notice that  (possibly the same factor  could occur multiple time in the string  ). 
At any step  and we consider a text windows  and match every string which appears in the window. In other words match every string which starts at any position in  will be matched. Now let  (note that ). We will match  against all factors of  of length . Note that for a match to be possible we must necessarily find that  (if  is not a factor of  then we can not have a match). Every occurrence which begins at any position in  must end at a position in . 
If the pattern  occurs a single time in the windows , then we just have to do a single comparison which takes optimal time. Thus from now on we concentrate on the case where the factor  occurs multiple times in the pattern . Before detailing the way the matching is done, we first prove the following lemma:
\begin{lemma}
\label{lemma:substr_count}
If  appears  times in  and  times in , then  occurs at most  times in 
\end{lemma}
\begin{proof}
Let  be the sequence of consecutive appearances of  in . Then any occurrence of  in  must span exactly a sequence of  consecutive occurrences. As we exactly have  sequences of length  in a sequence of length , we deduce that we have at most  occurrence of  in . 
\end{proof}
Thus our first step of the matching will be to count the number of occurrences of  in  which gives us an upper bound on the number of occurrence of  in . 
\paragraph{Counting occurrences of  in }
First note that in case the factor  appears more than one time in , then its (shortest) period is necessarily shorter than    and can be uniquely decomposed as  where ,  and  is the length of the (shortest) period of ~\footnote{ in this case (it hash a period of length less than <2) is said to be periodic}. This can easily be explained: as  (that is we have ) then necessarily any two occurrences of  in  are separated by at most  characters. That means that   has one period of length at most  and thus the (shortest) period of  is at most . By a well known result (see Crochemore et al's book~\cite[Lemma 1.6]{CHL07}) we deduce that all periods of length at most  of  are multiple of the (shortest) period.
We now describe the way the number of occurrences of  in  are counted. Let ,  (that is ) and . For counting we first do a longest suffix repetition search for  in  and then do a longest prefix repetition search of  in  returning two numbers  and  respectively. We now deduce that we have exactly  occurrences of  in :
\begin{lemma}
The algorithm above correctly computes the number of occurrences of  in . 
\end{lemma}
\begin{proof}
The string  contains a substring . This substring contains at least  occurrences of . We store a perfect hashing function so that for each factor  we associate a triplet , where  is a pointer to first occurrence of the factor,  is number of occurrences of  and  is the period of  (emphasize that we need  only in case ). 
What remains is to prove that  contains no more than  occurrences of . The proof is by contradiction. Suppose that there is an occurrence  which was outside of the substring . We have two cases:
\begin{itemize}
\item The occurrence  is at the left of the occurrence , that is . In this case notice that . We have  meaning that  for any  such that . This means that  is a period of  which moreover must be multiple of the shortest period . From there we deduce that  for some integer  which means that the string  will be included in the substring  by means of the longest suffix matching. Hence we conclude that the occurrence   is inside the substring . 
\item The occurrence  is at the right of the occurrence . This is symmetric to the previous case and can be proved with a similar argument. 
\end{itemize}
\end{proof}
Let  be decomposed as  meaning that it contains  occurrences of , the last step in the matching consists in comparing  against  and compare  against . We now distinguish four cases: 
\begin{enumerate}
\item  is a suffix of  but  is not a prefix of . We require that ,  and moreover that . If the requirement is fulfilled then we have a single match of  with the fragment . Otherwise we do not have any match. 
\item  is a prefix of  but  is not a suffix of . This case is symmetric to the previous one. We also require the same three conditions: , and . If the requirement is fulfilled, then we have a single match of  with the fragment .
\item Neither  is suffix of  nor  is prefix of . In this case we require ,  and moreover that . If the requirement is fulfilled, we have a single match of  with the fragment , otherwise we have no match.
\item  is a prefix of  and  is suffix of . In this case we require that . Then in case both  and  we conclude that we have  matches where first match is  and the last match is  (the matches ). Otherwise we have  matches:
\begin{itemize}
\item In case  but , the first match is  and the last one is .
\item In case  but , the first match is  and the last one is .
\end{itemize}
Note that any two consecutive occurrences are separated by  characters and reporting all occurrences takes time linear in the number of occurrences. 
\end{enumerate}
\begin{lemma}
The algorithm above correctly computes the occurrences of  in .
\end{lemma}
\begin{proof}
By lemma~\ref{lemma:substr_count} we can not have more than  occurrences of  in . Thus to have at least a match we require that . Now consider the starting position  of an occurrence of  in . The only possible values of  are of the kind . If  we conclude that we have a single possible match and this match is handled by the case 1, where the match is verified by matching the substrings  and  against the substrings  and . Now in case , we could have more than one match. This case is handled by cases 2,3 and 4 in the algorithm above. 
We now prove that those 3 cases work correctly: we divide the set the matches into three categories, the leftest match, the rightest match and the  middle matches. It can easily be verified that the middle matches are only possible in case  is a prefix of  and  is suffix of . It can also be easily verified that leftest match is only be possible if  and that  is prefix of . Likewise the rightest match is only possible if  and that  is suffix of . It can easily be checked that the three last cases correctly account for the matches. 
\end{proof}
\paragraph{Implementation}
We now analyze in detail the data structures needed for the matching. The first step of the matching is to match the string  against all factors of  of length . Thus what we need is to build a dictionary on the set of factors of  of length . In addition, in case  occurs as factor in , we must proceed to a second step. In this second step we need to determine the factors  of . 
Thus the required dictionary must fulfill the following needs: take only space  bits, answer in optimal  and must return the necessary information to deduce the factors  of . 
For our implementation we can use a very simple data structure: we store a perfect hashing function so that for each factor  we associate a triplet , where  is a pointer to first occurrence of  in ,  is number of occurrences of  in  and  is the period of  (note that we need  only in case ). For implementing the first step we can just compared  with the factor . For implementing the second step the factors  of  are also factors of  and their positions in  are easily deduced as combinations of the parameters   and . 
\paragraph{Running time analysis}
We now analyze more precisely the running time of the matching. We can prove that all the running time of the matching on the window  take time  where  is the number of reported occurrences. First, notice that in any case we are doing a constant number of operations among the following ones:
\begin{itemize}
\item Matching  against the set . 
\item Longest suffix repetition search for  in .
\item Longest prefix repetition search of  in . 
\item Comparing  with  and  with . 
\end{itemize}
Now the matching of  can be done in optimal  time by means of perfect hashing. The longest suffix repetition matching of  in  takes . Likewise doing a longest prefix repetition matching of  in  takes . Finally comparisons of  with  and of  with  takes  as the compared strings are of length . Thus we have proved the following lemma:
\begin{lemma}
The matching of all occurrences of a string  of length  into a string  of length  where  can be done in optimal  time, where  is the number of occurrences.
\end{lemma}

\subsection{Construction of the data structure}
The dictionary described above can easily be constructed in  time. 
\paragraph{Determining the factors}
For determining the triplet  associated with each factor , we begin by building the suffix tree of . Then we do a DFS 	traversal of the suffix tree but in which the traversal is restricted to nodes of depth at most depth . More precisely during the DFS traversal, at each time we reach a node  whose path is of length at least  whose path is prefixed by factor , we know that all the nodes in the subtree of  will be have pointers to all occurrences of . Then it suffices to count the size of the subtree of  to get . For determining  and , it suffices to traverse the pointers and select the two smallest ones  and . Then if  we will have that . That is in case  occurs two times then the period of  is just the difference between the pointers to first and second occurrence of . In order to state that factors which are equal, we use a temporary table  which is set during the traversal of the suffix tree as well as a table  of triplets. More precisely we attribute consecutive identifiers starting from zero to different patterns during the traversal of the suffix tree when we have a pattern  we give it identifier  and for each occurrence of  at position  we set  and also set .
\paragraph{Construction of the perfect hash function}
We now turn our attention to the creation of the perfect hash function. The construction follows a standard procedure, that is first the set  is mapped injectively into a set of integers using a string hash function which transforms each string in  into an integer of  bits, followed by the computation of the perfect hash function on that set. For implementing the first step we do  traversals of the pattern . At each traversal  we will compute the hash values for the patterns which start at positions .  
At each traversal we compute the factor hash values in the following way: we conceptually divide each factor into blocks of size  except the last block potentially of size less than  which is padded with zero characters. Then we can compute the hash values using a polynomial hash function~\cite[section 5]{DGMP}  which is parametrized with a sufficiently large prime . That is we just use a table  in which we initialize  and then successively for increasing  set  (except at the last step where we set ). Then the hash value of associated with the factor occurring at position  will be given by . 
Then before doing the second step, we check whether the computed hash values for different factors (two factors  and  are considered distinct if their corresponding triplets  and  differ) are all distinct (if the hash function  is injective on the set ) and if it is not the case, redo the computation of the hash values using a newly chosen hash function  parametrized by a new randomly chosen prime . This procedure is repeated until we get a set of  distinct integer in which case we can proceed with the second step of the computation which consists in building the perfect hash function on the set of integers obtained in the first step. 
\subsection{Tabulation based solution}
We now give a proof of theorem~\ref{theorem4}. In the case , theorem~\ref{theorem3} already gives the required query time using just  bits of space. Thus we will focus on the case . For this case there is a very simple solution: consider that at each step  (where  is initialized as zero) we match the substring  against all occurrences of the pattern  (consider w.l.o.g that  is an even number) which start anywhere inside . Using the four russian technique we can for every possible string  of length , all positions of all occurences of  in  (if any) which start at any position in . Note that every such occurrence must end at any position in . As we have  strings of length  and we can have at most  positions of occurrences, the space used by the table which stores all those occurrences for every possible string is thus just  bits. 
\section{Conclusion}
In this paper, we have proposed four solutions to the problems of single and multiple pattern matching on strings in the RAM model. In this model we assume that we can read  consecutive characters of any string in  time. The first and third solutions have a query time which depends on the length of the shortest pattern (for multiple string matching) and the length of the only pattern respectively, in a way similar to that of the previous algorithms which aimed at average-optimal expected performance (not worst-case performance as in our case). Those two solutions achieve optimal query times if the shortest pattern (or the only pattern in the third solution) is sufficiently long. The second and fourth solutions have no dependence on the length of the shortest pattern but need to use additional precomputed space. They are interesting alternatives to the previous tabulation approaches by Bille~\cite{B09} and Fredriksson~\cite{F02}.
\\




This paper gives rise to two interesting open problems: 

\begin{itemize}
\item In order to obtain any speedup we either rely on the length of the shortest pattern being long enough (theorem~\ref{theorem1} and~\ref{theorem3}) or have to use additional precomputed space (theorems~\ref{theorem2} and~\ref{theorem4} ). An important open question is whether it is possible to obtain any speedup without relying on any of the two assumptions.
\item The space usage of both solutions is  bits, but the patterns themselves occupy just  bits of space. The space used is thus at least a factor  larger than the space occupied by the patterns. An interesting open problem is whether it is possible to obtain an acceleration compared to the standard AC automaton while using only  bits of space.
\end{itemize} 
\section*{Acknowledgements}
The author wishes to thank Mathieu Raffinot for his many helpful comments and suggestions on a previous draft and to two anonymous reviewers for their helpful remarks and corrections on a previous version of the paper. 
\small 
\bibliography{Fast_Aho_Corasick} 
\normalsize








\end{document}  
