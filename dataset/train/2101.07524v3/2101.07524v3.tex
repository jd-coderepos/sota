The appendix is organized as follows:
\squishlist
    \item Section A includes the omitted proofs for all theoretical conclusions in the main paper.
    \item Section B includes experiment details and additional experiment results.
\squishend

\section{Omitted Proofs}

\subsection{Proof of Proposition \ref{prop: opt_d}}\label{app:p1}
We firstly introduce Lemma 1 which helps with the proof of Proposition \ref{prop: opt_d}.

\begin{lemma}\label{lm:basic}
For any , the function  achieves its maximum in  at .
\end{lemma}
\begin{proof}
Denote by , clearly, when  or , . For , we have:

Note that  if  and  if . Thus, the maximum of  should be . And  achieves its maximum in  at .
\end{proof}
Now we proceed to prove Proposition \ref{prop: opt_d}. 
\paragraph{\textbf{Proof of Proposition \ref{prop: opt_d}}}
\begin{proof}
The trainer criterion for the discriminator , given any generator , is to maximize the quantity . Remember that:

We then have:
{
}
For , according to Lemma \ref{lm:basic}, the above objective function respectively achieves its maximum in  at: 

With the introduce of Duel Game, the distributions  and  in the Vanilla GAN got changed due to the appearance of . Thus, we define the corresponding updated distributions in \PG{} w.r.t. discriminator  as  and , respectively:

\end{proof}

\subsection{Proof of Theorem \ref{thm: global_min}}

\begin{proof}
When , for , we have:

This allows us to rewrite  as:


Note that , by subtracting this expression from , we have:

where KL is the Kullback-Leibler divergence. Note that:

and the Jensen-Shannon divergence between two distributions is always non-negative and zero only when they are equal, we have shown that  is the global minimum of . Thus, we need 

Given that , we have:



\end{proof}

\subsection{Proof of Theorem \ref{thm:stab1}}
\begin{proof}
Ignoring the weight , the duel term of discriminator  w.r.t. its diverged peer discriminator  becomes:
{}
where we define: 

for a clear presentation. Proceeding the previous deduction, we then have:

Thus, 

Note that:

Thus, given , the \textbf{Bias} term is cancelled out. When , we have:

and we further have:


\end{proof}

\subsection{Proof of Theorem \ref{thm: conv1}}
\begin{proof}
When , the overall min-max game becomes:

Since we assume enough capacity, the inner max game is achieved if and only if:
. To prove  converges to , only need to reproduce the proof of proposition 2 in \cite{gan}. We omit the details here.

\end{proof}




\section{Experiment Details and Additional Results}\label{app:exp}
\paragraph{Model Architectures}

For the small-scale datasets, we used a shallow version of generator and discriminator: three convolution layers in the generator and four layers in the discriminators. We use a deep version of generator and discriminator for natural scene and human face image generation, which have three convolution layers in the generator and seven layers in the discriminators. The deep version is the original design of DCGAN\cite{DCGAN}. The peer discriminator uses the duplicate version of the first one. 

\subsection{Architecture Comparison Between GAN, D2GAN and \PG{}}
Figure \ref{Fig:compare_arch} shows the architecture designs of single discriminator, dual discriminator, and our proposed \PG{}. Compared with Vanilla GAN, \PG{} has one more identical discriminator and a competitive Duel Game between two discriminators. The introduced Duel Game induces diversified generated samples by discouraging the agreement between  and . In D2GAN, although both discriminators are trained with different loss functions, they do not interfere with each other in the training. 
\begin{figure*}[!htb]
\vspace{-0.07in}
    \centering
    \vspace{-0.1in}
    {\includegraphics[width=0.72\textwidth]{figures/flowchart_gan.pdf}
    }
    \vspace{-0.1in}
    {\includegraphics[width=0.72\textwidth]{figures/flowchart_d2gan.pdf}
    }
    {\includegraphics[width=0.72\textwidth]{figures/flowchart_peergan.pdf}
    }
        \vspace{-5pt}
        \caption{Architecture comparisons between GAN based method (first row), dual discriminators GAN based method (second row) and \PG{} (third row).}
    \label{Fig:compare_arch}
\end{figure*}



\subsection{Additional Experiment Results}

StyleGAN-ADA \cite{karras2020training} is the state-of-the-art method in image generation. We applied our duel game to StyleGAN-ADA and further improves its performance. On CelebA \cite{CelebA} dataset, we improved FID from 4.85 to 4.52, and FFHQ-10k\cite{karras2019style} dataset improved FID from 7.24 to 6.01. We show the generated image results (trained on CelebA) in Figure \ref{Fig:celebA_sup}.
 \begin{figure}
 \vspace{-0.1in}
    \centering
    {\includegraphics[width=1.0\textwidth]{figures/celebA_sup.png}
    }
 \vspace{-0.15in}
    \caption{More CelebA image generation results of applying duel game on StyleGAN-ADA.}
 \vspace{-0.15in}
    \label{Fig:celebA_sup}
\end{figure}

\subsection{Additional Experiment Details}

\paragraph{Model Architectures}

For the small-scale datasets, we used a shallow version of generator and discriminator: three convolution layers in the generator and four layers in the
discriminators. We use a deep version of generator and discriminator for natural
scene and human face image generation, which have three convolution layers in
the generator and seven layers in the discriminators. The deep version is the
original design of DCGAN\cite{DCGAN}. The peer discriminator uses the duplicate version of the first one.


\paragraph{Hyper-Parameters}\label{app:para} 
 
\PG{} achieves low FID scores and high IS scores when  and  are simply set to constant values. However we found that we could obtain an approximately 10\% improvement through dynamic tuning. The parameter  controls the overall weight of , while  punishes the condition when  over-agrees with . In the early training phase when we have an unstable generator and discriminator, we set  and  to 0. As training progresses, we gradually increase these parameters to a max value, which helps with vanishing gradients. After the midpoint of training we decrease these parameters to help the discriminators converge, until the parameters reach approximately 0 at the end of the training process. We adopt 0.3, 0.5 as the max value for  and , respectively.

\subsection{Ablation Study of \PG{}}
During training, We initialize the  and  as 0, and gradually increase to the set maximum value. We experimentally discover =0.3 and =0.5 can achieve the best FID score in the datasets we tested on. Table 3 shows an thorough ablation of  different hyper-parameter setting on STL-10 dataset. The bold text are the best  setting when beta is fixed. 
 
 \begin{figure}
    \centering
    {\includegraphics[width=0.5\textwidth]{figures/beta_alpha_trend.pdf}
    }
    \caption{The trend of  in the training.}
    \end{figure}
\begin{table}
    \centering
  \vspace{-0.3in}
\begin{tabular}{l|cccccc}
\hline
            &=0.1    & =0.3   &=0.5   & \:\:\:\:=0.7\:\:\:\:\: &\:\:=0.9\:\:   \\ \hline
=0.25	& 60.88 & 56.01	& \textbf{51.86} & 58.17 & 60.91 \\
=0.50	& 58.77 & \textbf{51.37} & 58.45 & 55.16 & 57.75 \\
=0.75	& \textbf{55.07} & 59.58 &58.58 & 58.22 & 57.75 \\ \hline
\end{tabular}
\caption{Ablation study of max  and max  value tuning on STL-10 dataset (evaluate with FID score).}
\end{table}

\subsection{Stability of Training}\label{app:stab}
In this section, we empirically show the stability of \PG{} training procedure. We adopt STL-10 dataset and  for illustration. In Figure \ref{Fig:loss1} and \ref{Fig:loss2}, we visualize the loss of two discriminators during the training procedure of STL-10 dataset. The red lines indicate the smoothed trend of the loss evaluated on the generated images and real images. Real losses are represented by the shaded red lines. Although there exists certain unstable episodes (the difference between smoothed loss and the real loss is large) for both discriminators, the overall trend of both discriminators are stable. What is more, we do observe that  and  hardly experience unstable episodes at the same time. This phenomenon further validates our conclusion in Theorem 2: an unstable/diverged discriminator hardly disrupts the training of its peer discriminator!

\begin{figure*}[!htb]
\vspace{-0.15in}
    \centering
{\includegraphics[width=0.3\textwidth]{figures/loss1_3.pdf}
    }
{\includegraphics[width=0.3\textwidth]{figures/loss1_5.pdf} 
    }
    {\includegraphics[width=0.3\textwidth]{figures/loss1_7.pdf}
    }
        \vspace{-5pt}
        \caption{The loss of  in \PG{} with  on STL-10 dataset, left: ; middle: ; right: .}
    \label{Fig:loss1}
\end{figure*}

\begin{figure*}[!htb]
\vspace{-0.15in}
\centering
{\includegraphics[width=0.3\textwidth]{figures/loss2_3.pdf}
    }
{\includegraphics[width=0.3\textwidth]{figures/loss2_5.pdf} 
    }
    {\includegraphics[width=0.3\textwidth]{figures/loss2_7.pdf}
    }
        \vspace{-5pt}
        \caption{The loss of  in \PG{} with  on STL-10 dataset, left: ; middle: ; right: .}
    \label{Fig:loss2}
\end{figure*}

\paragraph{\textbf{Agreements Between Two Discriminators}}
We also empirically estimate the agreement level between two discriminators while training. In Figure \ref{Fig:agree}, the axis denotes the percentage of predictions that reach a consensus by  and . The smoothed curve depicts the overall change of the agreement level. At the initial stage,  is not encouraged to agree overly on its peer discriminator . As the training progresses, the agreement level gradually increases to a high value to help the convergence of the whole training process. The shaded red line means that the practical agreement level fluctuates around the smoothed line, incurs a certain degree of randomness and prevents discriminators from getting stuck in a local optimum.

\begin{figure*}[!htb]
\vspace{-0.15in}
    \centering
    {\includegraphics[width=0.3\textwidth]{figures/aggre_3.pdf}
    } 
    {\includegraphics[width=0.3\textwidth]{figures/aggre_5.pdf} 
    }
    {\includegraphics[width=0.3\textwidth]{figures/aggre_7.pdf}
    }
        \vspace{-5pt}
        \caption{The agreement level between  and  in \PG{} with  on STL-10 dataset, left: ; middle: ; right: .}
    \label{Fig:agree}
\end{figure*}

