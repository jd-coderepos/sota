The appendix is organized as follows:
\squishlist
    \item Section A includes the omitted proofs for all theoretical conclusions in the main paper.
    \item Section B includes experiment details and additional experiment results.
\squishend

\section{Omitted Proofs}

\subsection{Proof of Proposition \ref{prop: opt_d}}\label{app:p1}
We firstly introduce Lemma 1 which helps with the proof of Proposition \ref{prop: opt_d}.

\begin{lemma}\label{lm:basic}
For any $(a,b)\in \mathbb{R}^2\setminus \{0, 0\}$, the function $y\rightarrow a\log(y)+b\log(1-y)$ achieves its maximum in $[0,1]$ at $\frac{a}{a+b}$.
\end{lemma}
\begin{proof}
Denote by $f(y):=a\log(y)+b\log(1-y)$, clearly, when $y=0$ or $y=1$, $f(y)=-\infty$. For $y\in (0,1)$, we have:
\begin{align}
    f'(y)=0\Longleftrightarrow \frac{a}{y}-\frac{b}{1-y}=0 \Longleftrightarrow y=\frac{a}{a+b}.
\end{align}
Note that $f'(y)>0$ if $0<y<\frac{a}{a+b}$ and $f'(y)<0$ if $1>y>\frac{a}{a+b}$. Thus, the maximum of $f(y)$ should be $\max(f(a), f(\frac{a}{a+b}), f(b))=f(\frac{a}{a+b})$. And $f(y)$ achieves its maximum in $[0,1]$ at $\frac{a}{a+b}$.
\end{proof}
Now we proceed to prove Proposition \ref{prop: opt_d}. 
\paragraph{\textbf{Proof of Proposition \ref{prop: opt_d}}}
\begin{proof}
The trainer criterion for the discriminator $D_i$, given any generator $G$, is to maximize the quantity $\mathcal{L}(D_1, D_2,G)$. Remember that:
\begin{align}\label{eqn:19}
    \mathcal{L}(D_1, D_2, G)
    &= \E_{x\thicksim p_{\text{data}}}\left[\log D_1(x)\right]+\E_{x\thicksim p_{\text{data}}}\left[\log D_2(x)\right]\notag\\
    &+\E_{z\thicksim p_{z}}\left[\log \left(1-D_1\left(G(z)\right)\right)\right]+\E_{z\thicksim p_{z}}\left[\log \left(1-D_2\left(G(z)\right)\right)\right]\notag\\
    &+\beta \cdot \E_{x\thicksim \pd}\Bigg[\ell\Big(D_1(x),\BR\big(D_2(x)>\dfrac{1}{2}\big)\Big)-\alpha \cdot \ell\Big(D_1(x_{p_1}),\BR\big(D_2(x_{p_2})>\dfrac{1}{2}\big)\Big)\Bigg]\notag\\
     &+\beta \cdot \E_{x\thicksim \pd}\Bigg[\ell\Big(D_2(x),\BR\big(D_1(x)>\dfrac{1}{2}\big)\Big)-\alpha \cdot \ell\Big(D_2(x_{p_1}),\BR\big(D_1(x_{p_2})>\dfrac{1}{2}\big)\Big)\Bigg].
\end{align}
We then have:
{
\begin{align}
    \begin{split}
    \text{Eqn.}(\ref{eqn:19})&= \int_{x}p_{\text{data}}(x)\big[\log\big(D_1(x)\big)+\log\big(D_2(x)\big)\big]dx+\int_{z}p_{z}(z)\Big[\log\Big(1-D_1\big(G(z)\big)\Big)+\log\Big(1-D_2\big(G(z)\big)\Big)\Big]dz\\
    &+\beta \cdot \int_{x} \pd(x)\big(r_{2,G}(x)-\alpha \cdot p_{2,G}\big)\cdot \log\big(D_1(x)\big)dx+\beta \cdot \int_{x} \pd(x)\big(r_{1,G}(x)-\alpha \cdot p_{1,G}\big)\cdot \log\big(D_2(x)\big)\big]dx\\
    &+\beta\cdot \int_{x} \pd(x)\big(1-\alpha-r_{2,G}(x)+\alpha \cdot p_{2,G}\big)\cdot \log\big(1-D_1(x)\big)dx\\
    &+\beta \cdot \int_{x} \pd(x)\big(1-\alpha-r_{1,G}(x)+\alpha \cdot p_{1,G}\big)\cdot \log\big(1-D_2(x)\big)dx\\
    &= \int_{x}p_{\text{data}}(x)\big[\log\big(D_1(x)\big)+\log\big(D_2(x)\big)\big]dx+\int_{x}p_{g}(x)\big[\log\big(1-D_1(x)\big)+\log\big(1-D_2(x)\big)\big]dx\\
    &+\beta \cdot \int_{x} \pd(x)\big(r_{2,G}(x)-\alpha \cdot p_{2,G}\big)\cdot \log\big(D_1(x)\big)dx+\beta \cdot \int_{x} \pd(x)\big(r_{1,G}(x)-\alpha \cdot p_{1,G}\big)\cdot \log\big(D_2(x)\big)dx\\
    &+\beta\cdot \int_{x} \pd(x)\big(1-\alpha-r_{2,G}(x)+\alpha \cdot p_{2,G}\big)\cdot \log\big(1-D_1(x)\big)dx\\
    &+\beta \cdot \int_{x} \pd(x)\big(1-\alpha-r_{1,G}(x)+\alpha \cdot p_{1,G}\big)\cdot \log\big(1-D_2(x)\big)dx\\
    &= \int_{x} \big[p_{\text{data}}(x)+\beta \cdot \big(r_{2,G}(x)-\alpha \cdot p_{2,G}\big)\cdot \pd(x)\big] \cdot\log\big(D_1(x)\big) dx\\
    &+ \int_{x} \big[p_{g}(x)+\beta \cdot \big(1-\alpha-r_{2,G}(x)+\alpha \cdot p_{2,G}\big)\cdot \pd(x)\big] \cdot\log(1-D_1(x)) dx\\
    &+ \int_{x} \big[p_{\text{data}}(x)+\beta \cdot \big(r_{1,G}(x)-\alpha \cdot p_{1,G}\big)\cdot \pd(x)\big] \cdot\log\big(D_2(x)\big) dx\\
    &+ \int_{x} \big[p_{g}(x)+\beta \cdot \big(1-\alpha-r_{1,G}(x)+\alpha \cdot p_{1,G}\big)\cdot \pd(x)\big] \cdot\log\big(1-D_2(x)\big) dx.\\
    \end{split}
\end{align}}
For $D_1, D_2$, according to Lemma \ref{lm:basic}, the above objective function respectively achieves its maximum in $[0, 1], [0, 1]$ at: 
\begin{align}
D_{i,G}^*(x)=\dfrac{p_{\text{data}}(x)+\beta \cdot (r_{j,G}(x)-\alpha \cdot p_{j, G})\cdot \pd(x)}{p_{\text{data}}(x)+p_{g}(x)+\beta \cdot (1-\alpha)\cdot \pd(x)}, \qquad i\neq j.
\end{align}
With the introduce of Duel Game, the distributions $p_{\text{data}}$ and $p_{g}$ in the Vanilla GAN got changed due to the appearance of $\pd$. Thus, we define the corresponding updated distributions in \PG{} w.r.t. discriminator $D_i$ as $p_{\text{data}_i}$ and $p_{g_i}$, respectively:
\begin{align}
\label{eqn:new_dis}
    &p_{\text{data}_i}(x) :=\dfrac{p_{\text{data}}(x)+\beta\cdot \hat{r}^*_{j,G}(x)\cdot \pd(x)}{\int_{x}p_{\text{data}}(x)+\beta\cdot \hat{r}^*_{j,G}(x)\cdot \pd(x)dx},\\
    &p_{g_i}(x) :=\dfrac{p_{g}(x)+\beta\cdot \big(1-\hat{r}^*_{j,G}(x)\big)\cdot \pd(x)}{\int_{x}p_{g}(x)+\beta\cdot \big(1-\hat{r}^*_{j,G}(x)\big)\cdot \pd(x)dx}.
\end{align}
\end{proof}

\subsection{Proof of Theorem \ref{thm: global_min}}

\begin{proof}
When $\alpha=0, r_{j,G}(x)=\frac{1}{2}$, for $\alpha= 0, i=1,2$, we have:
\begin{align}
    D_{i,G}^*(x)&=\dfrac{p_{\text{data}}(x)+\beta \cdot \hat{r}_{j,G}^*(x)\cdot \pd(x)}{p_{\text{data}}(x)+p_{g}(x)+\beta\cdot \pd(x)}\rightarrow \dfrac{p_{\text{data}}(x)+\dfrac{\beta}{2} \cdot \pd(x)}{p_{\text{data}}(x)+p_{g}(x)+\beta\cdot \pd(x)}.
\end{align}
This allows us to rewrite $\frac{C(G)}{2}$ as:
\begin{align}
    \frac{C(G)}{2}=&\E_{x\thicksim p_{\text{data}_i}}\left[\log \frac{p_{\text{data}}(x)+\frac{\beta}{2}\cdot \pd(x)}{p_{\text{data}}(x)+p_{g}(x)+\beta\cdot \pd(x)}\right]+\E_{x\thicksim p_{g_i
     }}\left[\log \frac{p_{g}(x)+\frac{\beta}{2}\cdot \pd(x)}{p_{\text{data}}(x)+p_{g}(x)+\beta\cdot \pd(x)}\right].
\end{align}
$\Longrightarrow$
Note that $2\cdot \big(\E_{x\thicksim p_{\text{data}_i}}[-\log2]+\E_{x\thicksim p_{g_i}}[-\log2]\big)=-\log{16}$, by subtracting this expression from $C(G)$, we have:
\begin{align}
    C(G)=&-\log{16}+ 2\cdot KL\Big(p_{g}+ \dfrac{\beta}{2}\cdot \pd\Big|\Big|\dfrac{p_{\text{data}}+p_{g}+\beta\cdot \pd}{2}\Big)\notag\\
    &+2\cdot KL\Big(p_{\text{data}}+\dfrac{\beta}{2}\cdot \pd\Big|\Big|\dfrac{p_{\text{data}}+p_{g}+\beta\cdot \pd}{2}\Big),
\end{align}
where KL is the Kullback-Leibler divergence. Note that:
\begin{align}
    C(G)=-\log{16}+2\cdot  JSD\Big(p_{\text{data}}+\dfrac{\beta}{2}\cdot \pd\Big|\Big|p_{g}+\dfrac{\beta}{2}\cdot \pd\Big),
\end{align}
and the Jensen-Shannon divergence between two distributions is always non-negative and zero only when they are equal, we have shown that $C(G)^*=-\log{16}$ is the global minimum of $C(G)$. Thus, we need $$p_{\text{data}}+\dfrac{\beta}{2}\cdot \pd=p_{g}+\dfrac{\beta}{2}\cdot \pd\Leftrightarrow p_{\text{data}}=p_g.$$
$\Longleftarrow$
Given that $p_{\text{data}}=p_{g}$, we have:
\begin{align}
    C(G)=&\max_{D}\mathcal{L}(G, D_1, D_2)\notag\\
     =&2\cdot \E_{x\thicksim p_{\text{data}_i}}\left[\log \dfrac{p_{\text{data}}(x)+\dfrac{\beta}{2}\cdot \pd(x)}{p_{\text{data}}(x)+p_{g}(x)+\beta\cdot \pd(x)}\right]+2\cdot \E_{x\thicksim p_{g_i}}\left[\log \dfrac{p_{g}(x)+\dfrac{\beta}{2}\cdot \pd(x)}{p_{\text{data}}(x)+p_{g}(x)+\beta\cdot \pd(x)}\right]\notag\\
     =& 2\cdot \left(\log\dfrac{1}{2}+\log\dfrac{1}{2}\right)=-\log{16}.
\end{align}


\end{proof}

\subsection{Proof of Theorem \ref{thm:stab1}}
\begin{proof}
Ignoring the weight $\beta$, the duel term of discriminator $D_i$ w.r.t. its diverged peer discriminator $\tilde{D}_{j}$ becomes:
{\begin{align}\label{eqn:split}
    &\quad\text{Duel}(D_i)|_{\tilde{D}_{j}}:= \E_{x\thicksim \pd}\Big[\ell\Big(D_i(x),\BR\big(\tilde{D}_{j}(x)>\dfrac{1}{2}\big)\Big)-\alpha \cdot \ell\Big(D_i(x_{p_1}),\BR\big(\tilde{D}_{j}(x_{p_2})>\dfrac{1}{2}\big)\Big)\Big]\notag\\
    &=\E_{x\thicksim \pd,Y^*_j=1}\Big[\mathbb{P}(\tilde{Y}_j=1|Y^*_j=1)\cdot \ell\big(D_{i}(x),1\big)+\mathbb{P}(\tilde{Y}_j=0|Y^*_j=1)\cdot \ell\big(D_{i}(x),0\big)\Big]\notag
    \\
    &+\E_{x\thicksim \pd,Y^*_j=0}\Big[\mathbb{P}(\tilde{Y}_j=1|Y^*_j=0)\cdot \ell\big(D_{i}(x),1\big)+\mathbb{P}(\tilde{Y}_j=0|Y^*_j=0)\cdot \ell\big(D_{i}(x),0\big)\Big]\notag\\
    &-\alpha \cdot \E_{x_{p_1}\thicksim \pd}\Big[\mathbb{P}(\tilde{Y}_{j}=1)\cdot \ell\big(D_{i}(x_{p_1}),1\big)+\mathbb{P}(\tilde{Y}_{j}=0)\cdot \ell\big(D_{i}(x_{p_1}),0\big)\Big]\notag\\
    &=\E_{x\thicksim \pd,Y^*_j=1}\Big[(1-e_{\text{data}, j})\cdot \ell(D_{i}(x),1)+e_{\text{data}, j}\cdot \ell(D_{i}(x),0)\Big]
    \notag\\
    &+\E_{x\thicksim \pd,Y^*_j=0}\Big[e_{g,j}\cdot \ell(D_{i}(x),1)+(1-e_{g,j})\cdot \ell(D_{i}(x),0)\Big]\notag\\
    &-\alpha \cdot \E_{x_{p_1}\thicksim \pd}\Big[\big[\mathbb{P}(Y^*_j=1)\cdot (1-e_{\text{data},j})+\mathbb{P}(Y^*_j=0)\cdot e_{g,j}\big]\cdot \ell\big(D_{i}(x_{p_1}),1\big)\Big]\notag\\
    &-\alpha \cdot \E_{x_{p_1}\thicksim \pd}\Big[\big[\mathbb{P}(Y^*_j=1)\cdot e_{\text{data},j}+\mathbb{P}(Y^*_j=0)\cdot (1-e_{g,j})\big]\cdot \ell\big(D_{i}(x_{p_1}),0\big)\Big]\notag\\
    &=\E_{x\thicksim \pd,Y_j^*=1}\Big[(1-e_{\text{data}, j}-e_{g, j})\cdot \ell(D_{i}(x),1)+e_{\text{data}, j}\cdot \ell(D_{i}(x),0)+e_{g, j}\cdot \ell(D_{i}(x),1)\Big]
   \notag \\
    &+\E_{x\thicksim \pd,Y_j^*=0}\Big[(1-e_{\text{data}, j}-e_{g,j})\cdot \ell(D_{i}(x),0)+e_{data,j}\cdot \ell(D_{i}(x),0)+e_{g,j}\cdot \ell(D_{i}(x),1)\Big]\notag\\
      &-\alpha \cdot \E_{x_{p_1}\thicksim \pd}\Big[c_1\cdot \ell\big(D_{i}(x_{p_1}),1\big)\Big]\notag-\alpha \cdot \E_{x_{p_1}\thicksim \pd}\Big[c_2\cdot \ell\big(D_{i}(x_{p_1}),0\big)\Big],
\end{align}}
where we define: 
\begin{align*}
  &c_1:=\mathbb{P}(Y^*_j=1)\cdot (1-e_{\text{data},j}-e_{g,j})+\mathbb{P}(Y^*_j=0)\cdot e_{g,j}+\mathbb{P}(Y^*_j=1)\cdot e_{g,j},\\
  &c_2:=\mathbb{P}(Y^*_j=0)\cdot (1-e_{\text{data},j}-e_{g,j})+\mathbb{P}(Y^*_j=1)\cdot e_{\text{data},j}+\mathbb{P}(Y^*_j=0)\cdot e_{\text{data},j},
\end{align*}
for a clear presentation. Proceeding the previous deduction, we then have:
\begin{align}
    \text{Duel}(D_i)|_{\tilde{D}_{j}}&=(1-e_{\text{data}, j}-e_{g, j})\cdot\E_{x\thicksim \pd}\Big[ \ell\big(D_{i}(x),Y^*_j\big)\Big]+\E_{x\thicksim \pd}\Big[e_{\text{data}, j}\cdot \ell\big(D_{i}(x),0\big)+e_{g, j}\cdot \ell\big(D_{i}(x),1\big)\Big]\notag
    \\
    &-\alpha \cdot (1-e_{\text{data},j}-e_{g,j})\cdot  \E_{x\thicksim \pd}\Big[ \ell\big(D_{i}(x_{p_1}),Y^*_j\big)\Big]-\alpha \cdot \E_{x\thicksim \pd}\Big[e_{\text{data}, j}\cdot \ell\big(D_{i}(x),0\big)+e_{g, j}\cdot \ell\big(D_{i}(x),1\big)\Big].
\end{align}
Thus, 
\begin{align}
    \text{Duel}(D_i)|_{\tilde{D}_{j}}=&(1-e_{\text{data}, j}-e_{g, j})\cdot \text{Duel}(D_i)|_{D^*_{j,G}}\notag\\
    +&\underbrace{(1-\alpha)\cdot \E_{x\thicksim \pd} \big[e_{\text{data}, j}\cdot \ell\big(D_{i}(x),0\big)+e_{g, j}\cdot \ell\big(D_{i}(x),1\big)\big]}_{\textbf{Bias}}.
\end{align}
Note that:
\begin{align}
    \textbf{Bias}=(1-\alpha)\cdot \E_{x\thicksim \pd} \big[e_{\text{data}, j}\cdot \log\big(1-D_{i}(x)\big)+e_{g, j}\cdot \log\big(D_{i}(x)\big)\big].
\end{align}
Thus, given $\alpha=1$, the \textbf{Bias} term is cancelled out. When $e_{\text{data}, j}+e_{g, j}<1$, we have:
\begin{align}
    \text{Duel}(D_i)|_{\tilde{D}_{j}}=&(1-e_{\text{data}, j}-e_{g, j})\cdot \text{Duel}(D_i)|_{D^*_{j,G}},
\end{align}
and we further have:
\begin{align}
    \max_{D_{i}} \text{Duel}(D_i)|_{\tilde{D}_{j}}
    =& \max_{D_{i}} \text{Duel}(D_i)|_{D^*_{j,G}}.
\end{align}

\end{proof}

\subsection{Proof of Theorem \ref{thm: conv1}}
\begin{proof}
When $\beta=0$, the overall min-max game becomes:
\begin{align}
    &\min_{G}\max_{D_1,D_2}\mathcal{L}(D_1,D_2, G)\notag\\
    =& \min_{G} \max_{D_1,D_2} \E_{x\thicksim p_{\text{data}}}\big[\log D_1(x)\big]+\E_{z\thicksim p_{z}}\Big[\log \Big(1-D_1\big(G(z)\big)\Big)\Big]\notag\\
    &\qquad \qquad +\E_{x\thicksim p_{\text{data}}}\big[\log D_2(x)\big]+\E_{z\thicksim p_{z}}\Big[\log \Big(1-D_2\big(G(z)\big)\Big)\Big].
\end{align}
Since we assume enough capacity, the inner max game is achieved if and only if:
$D_1(x)=D_2(x)=\frac{p_{\text{data}}(x)}{p_{\text{data}}(x)+p_g(x)}$. To prove $p_g$ converges to $p_{\text{data}}$, only need to reproduce the proof of proposition 2 in \cite{gan}. We omit the details here.

\end{proof}




\section{Experiment Details and Additional Results}\label{app:exp}
\paragraph{Model Architectures}

For the small-scale datasets, we used a shallow version of generator and discriminator: three convolution layers in the generator and four layers in the discriminators. We use a deep version of generator and discriminator for natural scene and human face image generation, which have three convolution layers in the generator and seven layers in the discriminators. The deep version is the original design of DCGAN\cite{DCGAN}. The peer discriminator uses the duplicate version of the first one. 

\subsection{Architecture Comparison Between GAN, D2GAN and \PG{}}
Figure \ref{Fig:compare_arch} shows the architecture designs of single discriminator, dual discriminator, and our proposed \PG{}. Compared with Vanilla GAN, \PG{} has one more identical discriminator and a competitive Duel Game between two discriminators. The introduced Duel Game induces diversified generated samples by discouraging the agreement between $D_1$ and $D_2$. In D2GAN, although both discriminators are trained with different loss functions, they do not interfere with each other in the training. 
\begin{figure*}[!htb]
\vspace{-0.07in}
    \centering
    \vspace{-0.1in}
    {\includegraphics[width=0.72\textwidth]{figures/flowchart_gan.pdf}
    }
    \vspace{-0.1in}
    {\includegraphics[width=0.72\textwidth]{figures/flowchart_d2gan.pdf}
    }
    {\includegraphics[width=0.72\textwidth]{figures/flowchart_peergan.pdf}
    }
        \vspace{-5pt}
        \caption{Architecture comparisons between GAN based method (first row), dual discriminators GAN based method (second row) and \PG{} (third row).}
    \label{Fig:compare_arch}
\end{figure*}



\subsection{Additional Experiment Results}

StyleGAN-ADA \cite{karras2020training} is the state-of-the-art method in image generation. We applied our duel game to StyleGAN-ADA and further improves its performance. On CelebA \cite{CelebA} dataset, we improved FID from 4.85 to 4.52, and FFHQ-10k\cite{karras2019style} dataset improved FID from 7.24 to 6.01. We show the generated image results (trained on CelebA) in Figure \ref{Fig:celebA_sup}.
 \begin{figure}
 \vspace{-0.1in}
    \centering
    {\includegraphics[width=1.0\textwidth]{figures/celebA_sup.png}
    }
 \vspace{-0.15in}
    \caption{More CelebA image generation results of applying duel game on StyleGAN-ADA.}
 \vspace{-0.15in}
    \label{Fig:celebA_sup}
\end{figure}

\subsection{Additional Experiment Details}

\paragraph{Model Architectures}

For the small-scale datasets, we used a shallow version of generator and discriminator: three convolution layers in the generator and four layers in the
discriminators. We use a deep version of generator and discriminator for natural
scene and human face image generation, which have three convolution layers in
the generator and seven layers in the discriminators. The deep version is the
original design of DCGAN\cite{DCGAN}. The peer discriminator uses the duplicate version of the first one.


\paragraph{Hyper-Parameters}\label{app:para} 
 
\PG{} achieves low FID scores and high IS scores when $\alpha$ and $\beta$ are simply set to constant values. However we found that we could obtain an approximately 10\% improvement through dynamic tuning. The parameter $\beta$ controls the overall weight of $\dd$, while $\alpha$ punishes the condition when $D_1$ over-agrees with $D_2$. In the early training phase when we have an unstable generator and discriminator, we set $\alpha$ and $\beta$ to 0. As training progresses, we gradually increase these parameters to a max value, which helps with vanishing gradients. After the midpoint of training we decrease these parameters to help the discriminators converge, until the parameters reach approximately 0 at the end of the training process. We adopt 0.3, 0.5 as the max value for $\alpha$ and $\beta$, respectively.

\subsection{Ablation Study of \PG{}}
During training, We initialize the $\alpha$ and $\beta$ as 0, and gradually increase to the set maximum value. We experimentally discover $\alpha$=0.3 and $\beta$=0.5 can achieve the best FID score in the datasets we tested on. Table 3 shows an thorough ablation of  different hyper-parameter setting on STL-10 dataset. The bold text are the best $\alpha$ setting when beta is fixed. 
 
 \begin{figure}
    \centering
    {\includegraphics[width=0.5\textwidth]{figures/beta_alpha_trend.pdf}
    }
    \caption{The trend of $\alpha, \beta$ in the training.}
    \end{figure}
\begin{table}
    \centering
  \vspace{-0.3in}
\begin{tabular}{l|cccccc}
\hline
            &$\alpha$=0.1    & $\alpha$=0.3   &$\alpha$=0.5   & \:\:\:\:$\alpha$=0.7\:\:\:\:\: &\:\:$\alpha$=0.9\:\:   \\ \hline
$\beta$=0.25	& 60.88 & 56.01	& \textbf{51.86} & 58.17 & 60.91 \\
$\beta$=0.50	& 58.77 & \textbf{51.37} & 58.45 & 55.16 & 57.75 \\
$\beta$=0.75	& \textbf{55.07} & 59.58 &58.58 & 58.22 & 57.75 \\ \hline
\end{tabular}
\caption{Ablation study of max $\alpha$ and max $\beta$ value tuning on STL-10 dataset (evaluate with FID score).}
\end{table}

\subsection{Stability of Training}\label{app:stab}
In this section, we empirically show the stability of \PG{} training procedure. We adopt STL-10 dataset and $\beta=0.25$ for illustration. In Figure \ref{Fig:loss1} and \ref{Fig:loss2}, we visualize the loss of two discriminators during the training procedure of STL-10 dataset. The red lines indicate the smoothed trend of the loss evaluated on the generated images and real images. Real losses are represented by the shaded red lines. Although there exists certain unstable episodes (the difference between smoothed loss and the real loss is large) for both discriminators, the overall trend of both discriminators are stable. What is more, we do observe that $D_1$ and $D_2$ hardly experience unstable episodes at the same time. This phenomenon further validates our conclusion in Theorem 2: an unstable/diverged discriminator hardly disrupts the training of its peer discriminator!

\begin{figure*}[!htb]
\vspace{-0.15in}
    \centering
{\includegraphics[width=0.3\textwidth]{figures/loss1_3.pdf}
    }
{\includegraphics[width=0.3\textwidth]{figures/loss1_5.pdf} 
    }
    {\includegraphics[width=0.3\textwidth]{figures/loss1_7.pdf}
    }
        \vspace{-5pt}
        \caption{The loss of $D_1$ in \PG{} with $\beta=0.25$ on STL-10 dataset, left: $\alpha=0.3$; middle: $0.5$; right: $\alpha=0.7$.}
    \label{Fig:loss1}
\end{figure*}

\begin{figure*}[!htb]
\vspace{-0.15in}
\centering
{\includegraphics[width=0.3\textwidth]{figures/loss2_3.pdf}
    }
{\includegraphics[width=0.3\textwidth]{figures/loss2_5.pdf} 
    }
    {\includegraphics[width=0.3\textwidth]{figures/loss2_7.pdf}
    }
        \vspace{-5pt}
        \caption{The loss of $D_2$ in \PG{} with $\beta=0.25$ on STL-10 dataset, left: $\alpha=0.3$; middle: $0.5$; right: $\alpha=0.7$.}
    \label{Fig:loss2}
\end{figure*}

\paragraph{\textbf{Agreements Between Two Discriminators}}
We also empirically estimate the agreement level between two discriminators while training. In Figure \ref{Fig:agree}, the $y-$axis denotes the percentage of predictions that reach a consensus by $D_1$ and $D_2$. The smoothed curve depicts the overall change of the agreement level. At the initial stage, $D_i$ is not encouraged to agree overly on its peer discriminator $D_j$. As the training progresses, the agreement level gradually increases to a high value to help the convergence of the whole training process. The shaded red line means that the practical agreement level fluctuates around the smoothed line, incurs a certain degree of randomness and prevents discriminators from getting stuck in a local optimum.

\begin{figure*}[!htb]
\vspace{-0.15in}
    \centering
    {\includegraphics[width=0.3\textwidth]{figures/aggre_3.pdf}
    } 
    {\includegraphics[width=0.3\textwidth]{figures/aggre_5.pdf} 
    }
    {\includegraphics[width=0.3\textwidth]{figures/aggre_7.pdf}
    }
        \vspace{-5pt}
        \caption{The agreement level between $D_1$ and $D_2$ in \PG{} with $\beta=0.25$ on STL-10 dataset, left: $\alpha=0.3$; middle: $0.5$; right: $\alpha=0.7$.}
    \label{Fig:agree}
\end{figure*}

