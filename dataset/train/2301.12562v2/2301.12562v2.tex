\documentclass[sigconf, nonacm]{acmart}


\usepackage[T1]{fontenc}
\usepackage{soul}
\usepackage{url}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{array}

\usepackage{xspace}
\usepackage{hhline}
\usepackage{makecell}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{amsfonts}


\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\zuv}{\mathbf{Z}_{uv}}
\newcommand{\suvr}{\mathbf{S}_{uv}^{(i)}}
\newcommand{\mr}{\mathbf{M}^{(i)}}
\newcommand{\mruv}{\mathbf{M}^{(i)}_{uv}}
\newcommand{\xruv}{\mathbf{X}^{(i)}_{uv}}
\newcommand{\zruv}{\mathbf{Z}^{(i)}_{uv}}
\newcommand{\bftheta}{\pmb{\theta}}

\newcommand{\posplus}{\xspace}
\newcommand{\pos}{{P\lowercase{o}S}\xspace}
\newcommand{\sop}{{S\lowercase{o}P}\xspace}
\newcommand{\sopplus}{\xspace}


\newcommand{\ssgrl}{\textit{S3GRL}\xspace}
\newcommand{\scaled}{S\lowercase{ca}L\lowercase{ed}\xspace}
\newtheorem{defn}{Definition}
\newcommand{\first}{\textcolor[rgb]{0.9, 0.17, 0.31}}
\newcommand{\second}{\textcolor[rgb]{0.0, 0.0, 1.0}}
\newcommand{\third}{\textcolor[rgb]{0.55, 0.0, 0.55}}
\newcommand{\worst}{\cellcolor[rgb]{0.957,0.8,0.8}}
\newcommand{\best}{\cellcolor[rgb]{0.851,0.918,0.827}}
\newcommand{\base}{\cellcolor[rgb]{1,0.949,0.8}}
\newcommand{\grA}[1]{#1\textsf{x}}
\newcommand{\grB}[1]{#1\textsf{\,x}}

\DeclareMathOperator{\pool}{pool}



\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}










\begin{document}


\title{Simplifying Subgraph Representation Learning for Scalable Link Prediction}

\author{Paul Louis}
\email{paul.louis@ontariotechu.net}
\affiliation{\institution{Ontario Tech University}
  \city{Oshawa}
  \state{Ontario}
  \country{Canada}
}

\author{Shweta Ann Jacob}
\email{shweta.jacob@ontariotechu.net}
\affiliation{\institution{Ontario Tech University}
  \city{Oshawa}
  \state{Ontario}
  \country{Canada}
}

\author{Amirali Salehi-Abari}
\email{abari@ontariotechu.ca}
\affiliation{\institution{Ontario Tech University}
  \city{Oshawa}
  \state{Ontario}
  \country{Canada}
}

\renewcommand{\shortauthors}{Louis et al.}

\begin{abstract}
  Link prediction on graphs is a fundamental problem. Subgraph representation learning approaches (SGRLs), by transforming link prediction to graph classification on the subgraphs around the links, have achieved state-of-the-art performance in link prediction. However, SGRLs are computationally expensive, and not scalable to large-scale graphs due to expensive subgraph-level operations. To unlock the scalability of SGRLs, we propose a new class of SGRLs, that we call \textit{Scalable Simplified SGRL} (\ssgrl). Aimed at faster training and inference, \ssgrl simplifies the message passing and aggregation operations in each link's subgraph. \ssgrl, as a scalability framework, accommodates various subgraph sampling strategies and diffusion operators to emulate computationally-expensive SGRLs. We propose multiple instances of \ssgrl and empirically study them on small to large-scale graphs. Our extensive experiments demonstrate that the proposed \ssgrl models scale up SGRLs without significant performance compromise (even with considerable gains in some cases), while offering substantially lower computational footprints (e.g., multi-fold inference and training speedup).
\end{abstract}


\keywords{Graph Neural Networks, Link Predictions}


\settopmatter{printfolios=true}
\maketitle

\section{Introduction}


Graphs are ubiquitous in representing relationships between interacting entities in a variety of contexts, ranging from social networks \cite{chen2020friend} to polypharmacy \cite{zitnik2018modeling}. One of the main tasks on graphs is link prediction \cite{liben2003link}, which involves predicting future or missing relationships between pairs of entities, given the graph structure. Link prediction plays a fundamental role in impactful areas, including molecular interaction predictions \cite{huang2020skipgnn}, recommender systems \cite{ying2018graph}, and protein-protein interaction predictions \cite{zhong2022long}.



The early solutions to link prediction relied on hand-crafted, parameter-free heuristics based on the proximity of the source-target nodes \cite{page1999pagerank,adamic2003friends}.
However, these methods require extensive domain knowledge for effective implementation. Recently, the success of \textit{graph neural networks (GNNs)} in learning latent
representations of nodes \cite{kipf2017semi} has led to their application in link prediction, through aggregating the source-target nodes' representations as a link representation \cite{kipf2016variational,pan2018adversarially}. However, as GNNs learn a pair of nodes' representations independent of their relative positions to each other, aggregating independent node representations results in poor link representations \cite{zhang2021labeling}. To circumvent this, the state-of-the-art subgraph representation learning approaches (SGRLs) \cite{zhang2018link} learn enclosing subgraphs of pairs of source-target nodes, while augmenting node features with structural features.

The core issue hampering the deployment of GNNs and SGRLs is their high computational demands on large-scale graphs (e.g., OGB datasets \cite{hu2020ogb,hu2021ogb}). Many approaches have been proposed for GNNs to relieve this bottleneck, ranging from sampling \cite{chen2018fastgcn,graphsaint-iclr20} to simplification \cite{wu2019simplifying} techniques. However, these techniques fail to be applied directly in SGRL models, where the issue is even more adverse due to subgraph extractions pertaining to each link. Although recent work has focused on tackling SGRL scalability by sampling \cite{yin2022algorithm,louis2022sampling} or sketching \cite{chamberlain2023graph} approaches, little attention is given to simplification techniques. We focus on improving the scalability in SGRLs, by simplifying the underlying training mechanism.

Inspired by the simplification techniques in GNNs such as SGCN \cite{wu2019simplifying} and SIGN \cite{sign_icml_grl2020}, we propose \textit{Scalable Simplified  SGRL} (\ssgrl) framework, which extends the simplification techniques to SGRLs. Our \ssgrl is flexible in emulating many SGRLs by offering choices of subgraph sampling and diffusion operators, producing different-sized convolutional filters for each link's subgraph. Our \ssgrl benefits from precomputing the subgraph diffusion operators, leading to an overall reduction in runtimes while offering a multi-fold scale-up over existing SGRLs in training/inference times. We propose and empirically study multiple instances of our \ssgrl framework. Our extensive experiments show substantial speedups in \ssgrl models over state-of-the-art SGRLs while maintaining and sometimes surpassing their efficacies.


\section{Related Work}
\label{sec:relwork}
Graph representation learning (GRL) \cite{hamilton2020graph} has numerous applications in drug discovery \cite{xiong2019pushing}, knowledge graph completion \cite{zhang2020relational}, and recommender systems \cite{ying2018graph}. The key downstream tasks in GRL are node classification \cite{hamilton2017inductive}, link prediction \cite{zhang2018link} and graph classification \cite{zhang2018end}. The early work in GRL, \textit{shallow encoders} \cite{perozzi2014deepwalk,grover2016node2vec}, learn dense latent node representations by taking multiple random walks rooted at each node. However, shallow encoders were incapable of consuming node features and being applied in the inductive settings. This shortcoming led to growing interest in Message Passing Graph Neural Networks (MPGNNs)\footnote{We use MPGNNs and GNNs interchangeably even though GNNs represent the broader class in GRL.}
\cite{brunaspectral2014,defferrard2016convolutional} such as Graph Convolutional Networks (GCN) \cite{kipf2017semi}. In MPGNNs, node feature representations are iteratively updated by first aggregating neighborhood features and then updating them through non-linear transformations. MPGNNs differ in formulations of aggregation and update functions \cite{hamilton2020graph}.

Link prediction is a fundamental problem on graphs, where the objective is to compute the likelihood of the presence of a link between a pair of nodes (e.g., users in a social network) \cite{liben2003link}. Network heuristics such as Common Neighbors or Katz Index \cite{katz1953new} were the first solutions for link prediction. However, these predefined heuristics fail to generalize well on different graphs. To address this, MPGNNs have been used to learn representations of node pairs independently, then aggregate the representations for link probability prediction \cite{kipf2016variational,Mavromatis2021GraphIM}. However, this class of MPGNN solutions falls short in capturing graph automorphism and different structural roles of nodes in forming links \cite{zhang2021labeling}. SEAL \cite{zhang2018link} successfully overcomes this limitation by casting the link prediction problem as a binary graph classification on the enclosing subgraph about a link, and adding structural labels to node features. This led to the emergence of subgraph representation learning approaches (SGRLs) \cite{zhang2018link,li2020distance}, which offer state-of-the-art results on the link prediction task. However, a common theme impeding the practicality and deployment of SGRLs is the lack of its scalability to large-scale graphs. 


Recently, a new research direction has emerged in GNNs on scalable training and inference techniques, which mainly focuses on node classification tasks. A common practice is to use different sampling techniques---such as node sampling \cite{hamilton2017inductive}, layer-wise sampling \cite{zou2019layer}, and graph sampling \cite{graphsaint-iclr20}---to subsample the training data to reduce computation. Other approaches utilize simplification techniques, such as removing intermediate non-linearities \cite{wu2019simplifying,sign_icml_grl2020}, to speed up the training and inference. These approaches have been shown to fasten learning of MPGNNs (at the global level) for node and graph classification tasks, and even for link prediction \cite{10.1145/3546157.3546163}; however, they are not directly applicable to SGRLs which exhibit superior performance for link prediction.  
In this work, we take the scalability-by-simplification approach for SGRLs (rather than for MPGNNs) by introducing \textit{Scalable Simplified  SGRL} (\ssgrl). Allowing diverse definitions of subgraphs and subgraph-level diffusion operators for SGRLs, \ssgrl emulates SGRLs in generalization while speeding them up.


There is a growing interest in the scalability of SGRLs \cite{yin2022algorithm,louis2022sampling,chamberlain2023graph}, with a main emphasis on the sampling approaches.  SUREL \cite{yin2022algorithm}, by deploying random walks with restart, approximates subgraph structures; however, it does not use GNNs for link prediction. \scaled \cite{louis2022sampling} uses similar sampling techniques of SUREL, but to sparsify subgraphs in SGRLs for better scalability. ELPH/BUDDY \cite{chamberlain2023graph} uses subgraph sketches as messages in MPGNNs but does not learn link representations explicitly at a subgraph level. Most recently, Stochastic Subgraph Neighborhood Pooling (SSNP) \cite{jacob2023stochastic} has proposed to improve scalability in subgraph classification tasks \cite{alsentzer2020subgraph,wang2021glass}. Our work complements this body of research. Our framework can be combined with (or host) these methods for better scalability (see our experiments for an example). 




\section{Preliminaries}

Consider a graph , where  is the set of nodes (e.g., individuals, proteins),  represents their relationships (e.g., friendship or protein-to-protein interactions). We also let 
represent the \emph{adjacency matrix} of , where  if and only if  . We further assume each node possesses a -dimensional feature vector (e.g., user's profile, features of proteins, etc.) stored as a row in \emph{feature matrix} .  

\vskip 1mm
\noindent \textbf{Link Prediction.} The goal is to infer the presence or absence of an edge between \emph{target nodes}  given the observed matrices  and . The learning problem is to find a \emph{likelihood function}  such that it assigns a higher likelihood to those target nodes with a missing link. The likelihood function can be formulated by various neural network architectures (e.g., MLP \cite{guo2022linkless} or GNNs \cite{davidson2018hyperspherical}).

\vskip 1mm
\noindent \textbf{GNNs.} Graph Neural Networks (GNNs) typically take as input  and , apply  layers of convolution-like operations, and output , containing -dimensional representation for each node as a row. For example, the -layer output of Graph Convolution Network (GCN) \cite{kipf2017semi} is given by 


where \emph{normalized adjacency matrix} , ,  is the diagonal matrix with , and  is a non-linearity function (e.g., ReLU). Here,  and  is the  layer's learnable weight matrix. After  stacked layers of Eq.~\ref{eq:gcn}, GCN outputs  nodal-representation matrix . For any target nodes , one can compute its joint representation:

where ,  are 's and 's learned representations in , and  is a pooling/readout operation (e.g., Hadamard product or concatenation) to aggregate the pairs' representations. Then, the link probability  for the target is given by  , where  is a learnable non-linear function (e.g., MLP) transforming the target embedding  into a link probability.



\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{ijcai-figures/SG3RL_arch_final_width.pdf}
  \caption{Our \ssgrl framework: In the preprocessing phase (shown by the shaded blue arrow), first multiple subgraphs are extracted around the target nodes  and  (shaded in blue) by various sampling strategies.  Diffusion matrices are then created from extracted subgraph adjacency matrices by predefined diffusion operators (e.g., powers of subgraphs in this figure). Each diffusion process involves the application of the subgraph diffusion matrix on its nodal features to create the matrix . The operator-level node representations of selected nodes (with a red border in raw data) are then aggregated for all subgraphs to form the joint  matrix. The selected nodes in this example are the target nodes , and their common neighbor . In the learning phase (as shown by the shaded red arrow), the joint matrix  undergoes dimensionality reduction followed by pooling using center pooling (highlighted by blue-border box) and common neighbor pooling (highlighted by purple-border box). Finally, the target representation  is transformed by an MLP to a link probability .}
 
\label{fig:architecture}
\end{figure*}

\vskip 1mm
\noindent \textbf{SGRLs.} Subgraph representation learning approaches (SGRLs)  \cite{zhang2017weisfeiler,zhang2018link,zhang2021labeling,yin2022algorithm,louis2022sampling} treat link prediction as a binary graph classification problem on enclosing subgraph  around a target . SGRLs aim to classify if the enclosing subgraph  is \textit{closed} or \textit{open} (i.e., the link exists or not). For each , SGRLs produce its nodal-representation matrix  using the stack of convolution-like operators (see Eq.~\ref{eq:gcn}). Then, similar to Eq.~\ref{eq:quv}, the nodal representations would be converted to link probabilities, with the distinction that the pooling function is graph pooling (e.g., SortPooling \cite{zhang2018end}), operating over all node's representations (not just those of targets) to produce the fixed-size representation of the subgraph. To improve the expressiveness of GNNs on subgraphs, SGRLs augment node features with some structural features determined by the targets' positions in the subgraph. These augmented features are known as \textit{node labels} \cite{zhang2021labeling}. Node labeling techniques fall into \emph{zero-one} or \textit{geodesic distance-based} schemes \cite{huang2022boosting}. We refer to  as the matrix containing both the initial node features and labels generated by a valid labeling scheme \cite{zhang2021labeling}. The expressiveness power of SGRLs comes with high computational costs due to subgraph extractions, node labeling, and independent operations on overlapping large subgraphs. This computational overhead is exaggerated in denser graphs and deeper subgraphs due to the exponential growth of subgraph size. 


\noindent \textbf{SGCN and SIGN.} To improve the scalability of GNNs for node classification tasks, several attempts are made to simplify GNNs by removing their intermediate nonlinearities, thus making them shallower, easier to train, and scalable. Simplified GCN (SGCN) \cite{wu2019simplifying} has removed all but the last non-linearities in L-layer GCNs, to predict class label probabilities  for all nodes by
,
where  is softmax or logistic function, and  is the only learnable weight matrix. The term  can be precomputed once before training. Benefiting from this precomputation and a shallower architecture, SGCN improves scalability. SIGN has extended SGCN to be  more expressive yet scalable for node classification tasks. Its crux is to deploy a set of linear diffusion matrices  that can be applied to node-feature matrix . The class probabilities  are computed by  , where  

Here,  and  are non-linearities,  is the learnable weight matrix for diffusion operator ,  is a concatenation operation, and  is the learnable weight matrix for transforming node representations to class probabilities. Letting  be the identity matrix,   in Eq.~\ref{eq:sign_op_learning} allows the node features to contribute directly (independent of graph structure) into the node representation and consequently to the class probabilities. The  operator captures different heuristics in the graph. As with SGCN, the terms  in SIGN can be once precomputed before training. These precomputations and the shallow architecture of SIGN lead to substantial speedup during training and inference with limited compromise on node classification efficacy. Motivated by these efficiencies, our \ssgrl framework extends SIGN for link prediction on subgraphs while addressing the computational bottleneck of SGRLs.

\section{Scalable Simplified  SGRL (\ssgrl)}
We propose \textit{Scalable Simplified  SGRL} (\ssgrl), which benefits from the expressiveness power of SGRLs while offering the simplicity and scalability of SIGN and SGCN for link prediction. Our framework leads to a multi-fold speedup in both training and inference of SGRL methods while maintaining or boosting their state-of-the-art performance (see experiments below). 

Our \ssgrl framework consists of two key components: (i) \emph{Subgraph sampling strategy}  takes as an input the graph  and target pairs  and outputs the adjacency matrix  of the enclosing subgraphs  around the targets. The subgraph sampling strategy  can capture various subgraph definitions such as -hop enclosing subgraphs \cite{zhang2018link}), random-walk-sampled subgraphs \cite{yin2022algorithm,louis2022sampling}, and heuristic-based subgraphs \cite{zeng2021decoupling}; (ii) \emph{Diffusion operator}   takes the subgraph adjacency matrix  and outputs its diffusion matrix . A different class of diffusion operators are available: adjacency/Laplacian operators to capture connectivity, triangle/motif-based operators \cite{granovetter1983strength} to capture inherent community structure, personalized pagerank-based (PPR) operators \cite{gasteiger2019diffusion} to identify important connections. Each of these operators and their powers can constitute the different diffusion operators in \ssgrl.



In our \ssgrl framework, each model is characterized by the \emph{sampling-operator} set , where  and   are the  subgraph sampling strategy and diffusion operator, respectively. For a graph , target pair , and  sampling-operator pair , one can find 's sampled subgraph  and its corresponding diffusion matrix . For instance, one can define  to sample the random-walk-induced subgraph  rooted at  in . Then,  can compute the -th power of , to count the number of -length walks on the subgraph. \ssgrl computes the operator-level node representations of the selected subgraph  by 
 
Here,  is the node feature matrix for nodes in the selected subgraph  for the sampling-operator pair . Eq.~\ref{eq:operator-rep} can be viewed as feature smoothing where the diffusion matrix  is applied over node features . \ssgrl then concatenates the operator-level nodal-representation matrix  of all sampling-operator pairs to form the \textit{joint nodal-representation matrix}:

The concatenation between nodal-representation matrices with dimensionality mismatch should be done with care: the corresponding rows (belonging to the same node) should be inline, where missing rows are filled with zeros (analogous to zero-padding for graph pooling). The joint nodal-representation matrix  goes through a non-linear feature transformation (for dimensionality reduction) by learnable weight matrix  and non-linearity . This transformed matrix is then further downsampled by the graph pooling  to form the target's representation:
 
from which the link probability is computed by 
 
with  being a learnable non-linear function (e.g., MLP) to convert the target representation  into a link probability. 




Our \ssgrl exhibits substantial speedup in inference and training through precomputing  in comparison to more computationally-intensive SGRLs  \cite{li2020distance,pan2022neural}, designed based on multi-layered GCN or DGCNN \cite{zhang2018end} (see our experiments for details). Note that only Equations \ref{eq:simple_sgrl} and \ref{eq:link-prob-s3grl} are utilized in training and inference in \ssgrl (see also Figure \ref{fig:architecture}).  Apart from this computational speedup, \ssgrl offers other advantages analogous to other prominent GNNs:

\vskip 1mm
\noindent \textbf{Disentanglement of Data and Model.} The composition of subgraph sampling strategy  and diffusion operator  facilitates the disentanglement (or decoupling) of data (i.e., subgraph) and model (i.e., diffusion operator). This disentanglement resembles shaDow-GNN \cite{zeng2021decoupling}, in which the depth of GNNs (i.e., its number of layers) is decoupled from the depth of enclosing subgraphs. Similarly, in \ssgrl, one can explore high-order diffusion operators (e.g., adjacency matrix to a high power) in constrained enclosing subgraphs (e.g., ego network of the target pair). This combination simulates multiple message-passing updates between the nodes in the local subgraph, thus achieving local oversmoothing  \cite{zeng2021decoupling} and ensuring the final subgraph representation possesses information from all the nodes in the local subgraph.

\vskip 1mm
\noindent \textbf{Multi-View Representation.} Our \ssgrl framework via the sampling-operator pairs provides multiple views of the enclosing neighborhood of each target pair. This capability allows hosting models analogous to multi-view \cite{abu2020n,Cai_Ji_2020} models.


\section{\ssgrl Instances: \pos and \sop}
We introduce two instances of \ssgrl, differentiating in sampling-operator pairs. 




\vskip 1mm
\noindent \textbf{Powers of Subgraphs (\pos).} This instance intends to mimic a group of SGRLs with various model depths on a fixed-sized enclosing subgraph while boosting scalability and generalization (e.g., SEAL \cite{zhang2018link}). The sampling-operator set  is defined as follows: (i) the sampling strategy  for any  is constant and returns the adjacency matrix  of the -hop enclosing subgraph  about target . The subgraph  is a node-induced subgraph of  with the node set , including the nodes with the geodesic distance of at most  from either of the target pairs \cite{zhang2018link}; (ii) the -th diffusion operator  is the -th power of adjacency matrix . This operator facilitates information diffusion among nodes -length paths apart. Putting (i) and (ii) together, one can derive  for Eq.~\ref{eq:operator-rep}.  Note that  \textbf{I} allows the node features to contribute directly (independent of graph structure) to subgraph representation. \pos possesses two hyperparameters  and , controlling the number of diffusion operators and the number of hops in enclosing subgraphs, respectively. 

One can intuitively view \pos as equivalent to SEAL that uses a GNN model of depth  with ``skip-connections" \cite{xu2018representation} on the -hop enclosing subgraphs. The skip-connections property allows consuming all the intermediate learned representations of each layer to construct the final link representation. Similarly, \pos uses varying -th power diffusion operators combined with the concatenation operator in Eq.~\ref{eq:simple_sgrl} to generate the representation . In this light, the two hyperparameters of  and  (resp.) in \pos control the (virtual) depth of the model and the number of hops in enclosing subgraphs (resp.).  





\label{sec:pos}


\vskip 1mm
\noindent \textbf{Subgraphs of Powers (\sop).} This instance of \ssgrl, by transforming the global input graph G, brings long-range interactions into the local enclosing subgraphs. Let  be \textit{h-hop sampling strategy}, returning the enclosing -hop subgraph about the target pair . The \sop model defines  with  and . Here,  is the -th power of the input graph (computed by ), in which two nodes are adjacent when their geodesic distance in G is at most . In this sense, the power of graphs brings long-range interactions to local neighborhoods at the cost of neighborhood densification. However, as the diffusion operator is an identity function, \sop prevents overarching to the further-away information of indirect neighbors. \sop consists of two hyperparameters  and  that control the number of diffusion operators and hops in local subgraphs, respectively.  

\vskip 1mm
\noindent \textbf{\pos vs. \sop.} \pos and \sop are similar in capturing information diffusion among nodes (at most) -hop apart. But, their key distinction is whether long-range diffusion occurs in the global level of the input graph (for \sop) or the local level of the subgraph (for \pos). \sop is not a ``typical'' SGRL, however, it still uses subgraphs around the target pair on the power of graphs.



\vskip 1mm
\noindent \textbf{Graph Pooling.} Our proposed instances of \ssgrl are completed by defining the graph pooling function in Eq.~\ref{eq:simple_sgrl}. We specifically consider computationally-light pooling functions, which focus on pooling the targets' or their direct neighbors' learned representations.\footnote{Our focus is backed up by recent empirical findings \cite{chamberlain2023graph} showing the effectiveness of the target node's embeddings and the decline in the informativeness of node embeddings as the nodes get farther away from targets.} We consider simple \textit{center} pooling: , where  is the Hadamard product, and ,  are 's and 's learned representations in nodal-representation matrix . We also introduce \textit{common-neighbor} pooling: , where  is any invariant graph readout function (e.g., mean, max, or sum), and  and  are the direct neighborhood of targets ,  in the original input graph. We also define \textit{center-common-neighbor} pooling . In addition to their efficacy, these pooling functions allow one to further optimize the data storage and computations. As the locations of \textit{pooled nodes} (e.g., target pairs and/or their direct neighbors) for these pooling functions are specified in the original graph, one could just conduct necessary (pre)computations and store those data affecting the learned representation of pooled nodes, while avoiding unnecessary computations and data storage. Figure \ref{fig:architecture} shows this optimization through red-bordered boxes of rows for each . This information is the only required node representations in  utilized in the downstream  operation. We consider center pooling as the default pooling for \pos and \sop, and we refer to them as \posplus and \sopplus when center-common-neighbor pooling is deployed. We study both center and center-common-neighbor pooling for \pos. However, due to the explosive growth of -hop subgraph sizes in \sopplus, we limit our experiments on \sop to only center pooling. 



\vskip 1mm
\noindent \textbf{Inference Time Complexity.} Let  be the number of pooled nodes (e.g.,  for center pooling),  be the dimension of initial input features,  be the number of operators, and  be the reduced dimensionality in Eq.~\ref{eq:simple_sgrl}. The inference time complexity for any of \pos, \sop, and their variants is . Consider the dimensionality reduction of the joint matrix  by the weight matrix  in Eq.~\ref{eq:simple_sgrl}. As  is  by  and  is  by , their multiplication is in  time. The pooling for any proposed variants is . Assuming  in Eq.~\ref{eq:link-prob-s3grl} being an MLP with one -dimensional hidden layer, its computation is in  time.



\section{Experiments}
We carefully design an extensive set of experiments to assess the extent to which our model scales up SGRLs while maintaining their state-of-the-art performance. 
Our experiments intend to address these questions: \textbf{(Q1)} How effective is the \ssgrl framework compared with the state-of-the-art SGRLs for link prediction methods? \textbf{(Q2)} What is the computational gain achieved through \ssgrl in comparison to SGRLs? \textbf{(Q3)} How well does our best-performing model, \posplus, perform on the Open Graph Benchmark datasets for link prediction, graphs with millions of nodes and edges \cite{hu2020ogb,hu2021ogb}? \textbf{(Q4)} How complementary can \ssgrl be in combination with other scalable SGRLs (e.g., \scaled \cite{louis2022sampling}) to further boost scalability and generalization? 



\begin{table}[t]
\centering
\scalebox{1}{
\begin{tabular}{llcccc}  
\toprule
                                & \textbf{Dataset} & \multicolumn{1}{c}{\textbf{\# Nodes}} & \multicolumn{1}{c}{\textbf{\# Edges}} & \multicolumn{1}{c}{\textbf{Avg. Deg.}} & \textbf{\# Feat.}  \\ 
\midrule
\multirow{3}{*}{\begin{sideways}{\scriptsize Non-attr.}\end{sideways}} & \textrm{NS}               & 1,461                                  & 2,742                                  & 3.75                                   & NA                    \\
                                & \textrm{Power}            & 4,941                                  & 6,594                                & 2.67                                   & NA                    \\
                                & \textrm{Yeast}            & 2,375                                  & 11,693                                 & 9.85                                   & NA                    \\
                                
                                & \textrm{PB}               & 1,222                                  & 16,714                                 & 27.36                                  & NA                    \\ 
\cmidrule[0.75pt](r){2-6}
\multirow{4}{*}{\begin{sideways}{\scriptsize Attributed}\end{sideways}}     & \textrm{Cora}             & 2,708                                  & 4,488                                  & 3.31                                   & 1,433                  \\
                                & \textrm{CiteSeer}         & 3,327                                  & 3,870                                  & 2.33                                   & 3,703                  \\
                                & \textrm{PubMed}           & 19,717                                 & 37,676                                & 3.82                                   & 500                   \\
                                & \textrm{Texas}        & 183                                   & 143                                   & 1.56                                   & 1,703                  \\ 
                                & \textrm{Wisconsin}        & 251                                   & 197                                   & 1.57                                   & 1,703                  \\ 
\cmidrule[0.75pt](r){2-6}
\multirow{4}{*}{\begin{sideways}{\scriptsize OGB}\end{sideways}}            & \textrm{Collab}           & 235,868                                & 1,285,465                               & 8.2                                     & 128                   \\
& \textrm{DDI}           & 4,267                                & 1,334,889                               & 500.5                                     & NA                   \\
                                & \textrm{Vessel}           & 3,538,495                               & 5,345,897                               & 2.4                                   & 3                     \\
                                & \textrm{PPA}           &      576,289                          & 30,326,273 	                               & 73.7                                    & 58                     \\
                                & \textrm{Citation2}           & 2,927,963                               & 30,561,187                               & 20.7                                   & 128                     \\
\bottomrule
\end{tabular}


}


\caption{The statistics of the non-attributed, attributed, and OGB datasets.}
\label{table:dataset_comparison}
\end{table}


\begin{table*}[t]
\centering
\setlength{\extrarowheight}{-1pt}
\addtolength{\extrarowheight}{\aboverulesep}
\addtolength{\extrarowheight}{\belowrulesep}

\scalebox{0.95}{
\begin{tabular}{p{12pt}|l|llll|lllll} 
\toprule
\multicolumn{1}{l|}{}                                                    & \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{Non-attributed}}                                                                                                                                                                       & \multicolumn{5}{c}{\textbf{Attributed}}                                                                                                                                                                                                                                \\
\multicolumn{1}{l|}{}                                                    &                        & \multicolumn{1}{c}{\textbf{NS}}                           & \multicolumn{1}{c}{\textbf{Power}}                        & \multicolumn{1}{c}{\textbf{PB}}                           & \multicolumn{1}{c|}{\textbf{Yeast}}                    & \multicolumn{1}{c}{\textbf{Cora}}                         & \multicolumn{1}{c}{\textbf{CiteSeer}}                     & \multicolumn{1}{c}{\textbf{PubMed}}                       & \multicolumn{1}{c}{\textbf{Texas}}                        & \multicolumn{1}{c}{\textbf{Wisconsin}}                     \\ 
\hline
\multirow{3}{*}{{\begin{sideways}Heuristic\end{sideways}}}                                        &  AA                     & 92.14{\scriptsize0.77}                                     & 58.09{\scriptsize0.55}                                     & 91.76{\scriptsize0.56}                                     & 88.80{\scriptsize0.55}                                     & 71.48{\scriptsize0.69}                                     & 65.86{\scriptsize0.80}                                     & 64.26{\scriptsize0.40}                                     & 54.69{\scriptsize3.68}                                     & 55.60{\scriptsize3.14}                                      \\
            & CN                     & 92.12{\scriptsize0.79}                                     & 58.09{\scriptsize0.55}                                     & 91.44{\scriptsize0.59}                                     & 88.73{\scriptsize0.56}                                     & 71.40{\scriptsize0.69}                                     & 65.84{\scriptsize0.81}                                     & 64.26{\scriptsize0.40}                                     & 54.36{\scriptsize3.65}                                     & 55.08{\scriptsize3.08}                                      \\
    & PPR                    & 92.50{\scriptsize1.06}                                     & 62.88{\scriptsize2.18}                                     & 86.85{\scriptsize0.48}                                     & 91.71{\scriptsize0.74}                                     & 82.87{\scriptsize1.01}                                     & 74.35{\scriptsize1.51}                                     & 75.80{\scriptsize0.35}                                     & 53.81{\scriptsize7.53}                                     & 62.86{\scriptsize8.13}                                      \\ 
\hline
\multirow{3}{*}{{\begin{sideways}MPGNN\end{sideways}}}                                               & GCN                    & 91.75{\scriptsize1.68}                                     & 69.41{\scriptsize0.90}                                     & 90.80{\scriptsize0.43}                                     & 91.29{\scriptsize1.11}                                     & 89.14{\scriptsize1.20}                                     & 87.89{\scriptsize1.48}                                     & 92.72{\scriptsize0.64}                                     & 67.42{\scriptsize9.39}                                     & 72.77{\scriptsize6.96}                                      \\
            & GraphSAGE              & 91.39{\scriptsize1.73}                                     & 64.94{\scriptsize2.10}                                     & 88.47{\scriptsize2.56}                                     & 87.41{\scriptsize1.64}                                     & 85.96{\scriptsize2.04}                                     & 84.05{\scriptsize1.72}                                     & 81.60{\scriptsize1.22}                                     & 53.59{\scriptsize9.37}                                     & 61.81{\scriptsize9.66}                                      \\
             & GIN                    & 83.26{\scriptsize3.81}                                     & 58.28{\scriptsize2.61}                                     & 88.42{\scriptsize2.09}                                     & 84.00{\scriptsize1.94}                                     & 68.74{\scriptsize2.74}                                     & 69.63{\scriptsize2.77}                                     & 82.49{\scriptsize2.89}                                     & 63.46{\scriptsize8.87}                                     & 70.82{\scriptsize8.25}                                      \\ 
\hline
\multirow{2}{*}{{\begin{sideways}LF\end{sideways}}} & node2vec               & 91.44{\scriptsize0.81}                                     & 73.02{\scriptsize1.32}                                     & 85.08{\scriptsize0.74}                                     & 90.60{\scriptsize0.57}                                     & 78.32{\scriptsize0.74}                                     & 75.36{\scriptsize1.22}                                     & 79.98{\scriptsize0.35}                                     & 52.81{\scriptsize5.31}                                     & 59.57{\scriptsize5.69}                                      \\
              & MF                     & 82.56{\scriptsize5.90}                                     & 53.83{\scriptsize1.76}                                     & 91.56{\scriptsize0.56}                                     & 87.57{\scriptsize1.64}                                     & 62.25{\scriptsize2.21}                                     & 61.65{\scriptsize3.80}                                     & 68.56{\scriptsize12.13}                                    & 60.35{\scriptsize5.62}                                     & 53.75{\scriptsize9.00}                                      \\ 
\hline
\multirow{4}{*}{{\begin{sideways}AE\end{sideways}}}  & GAE                    & 92.50{\scriptsize1.71}                                     & 68.17{\scriptsize1.64}                                     & 91.52{\scriptsize0.35}                                     & 93.13{\scriptsize0.79}                                     & 90.21{\scriptsize0.98}                                     & 88.42{\scriptsize1.13}                                     & 94.53{\scriptsize0.69}                                     & 68.67{\scriptsize6.95}                                     & 75.10{\scriptsize8.69}                                      \\
                       & VGAE                   & 91.83{\scriptsize1.49}                                     & 66.23{\scriptsize0.94}                                     & 91.19{\scriptsize0.85}                                     & 90.19{\scriptsize1.38}                                     & 92.17{\scriptsize0.72}                                     & 90.24{\scriptsize1.10}                                     & 92.14{\scriptsize0.19}                                     & \textbf{\third{74.61{\scriptsize8.61}}}                                     & 74.39{\scriptsize8.39}                                      \\
                    & ARVGA                  & 92.16{\scriptsize1.05}                                     & 66.26{\scriptsize1.59}                                     & 90.98{\scriptsize0.92}                                     & 90.25{\scriptsize1.06}                                     & \base \textbf{\third{92.26{\scriptsize0.74}}}       & 90.29{\scriptsize1.01}                                     & 92.10{\scriptsize0.38}                                     & 73.55{\scriptsize9.01}                                     & 72.65{\scriptsize7.02}                                      \\
                   & GIC                    & 90.88{\scriptsize1.85}                                     & 62.01{\scriptsize1.25}                                     & 73.65{\scriptsize1.36}                                     & 88.78{\scriptsize0.63}                                     & 91.42{\scriptsize1.24}                                     & \base \textbf{\third{92.99{\scriptsize1.14}}}       & 91.04{\scriptsize0.61}                                     & 65.16{\scriptsize7.87}                                     & 75.24{\scriptsize8.45}                                      \\ 
\hline
\multirow{3}{*}{{\begin{sideways}SGRL\end{sideways}}}                                                   & SEAL                   & \textbf{\third{98.63{\scriptsize0.67}}}                                     & 85.28{\scriptsize0.91}                                     & \textbf{\third{95.07{\scriptsize0.35}}}                                     & \textbf{\second{97.56{\scriptsize0.32}}}                                     & 90.29{\scriptsize1.89}                                     & 88.12{\scriptsize0.85}                                     & 97.82{\scriptsize0.28}                                     & 71.68{\scriptsize6.85}                                     & 77.96{\scriptsize10.37}                                     \\
                 & GCN+DE                 & \textbf{\second{98.66{\scriptsize0.66}}}                                     & 80.65{\scriptsize1.40}                                     & \textbf{\second{95.14{\scriptsize0.35}}}                                     & 96.75{\scriptsize0.41}                                     & 91.51{\scriptsize1.10}                                     & 88.88{\scriptsize1.53}                                     & 98.15{\scriptsize0.11}                                     & \textbf{\second{76.60{\scriptsize6.40}}}                                     & \textbf{\second{74.65{\scriptsize9.56}}}                                      \\
              & WalkPool               & \base \textbf{\first{98.92{\scriptsize0.52}}}       & \base \textbf{\first{90.25{\scriptsize0.64}}}       & \base \textbf{\first{95.50{\scriptsize0.26}}}       & \base \textbf{\first{98.16{\scriptsize0.20}}}       & 92.24{\scriptsize0.65}                                     & 89.97{\scriptsize1.01}                                     & \base \textbf{\third{98.36{\scriptsize0.11}}}       & \base \textbf{\first{78.44{\scriptsize9.83}}}       & \base79.57{\scriptsize11.02}       \\ 
\hline
\multirow{3}{*}{{\begin{sideways}\ssgrl\end{sideways}}}                                                   & \pos (ours)                    & 97.23{\scriptsize1.38}                                     & \textbf{\third{86.67{\scriptsize0.98}}}                                     & 94.83{\scriptsize0.41}                                     & 95.47{\scriptsize0.54}                                     & \textbf{\second{94.65{\scriptsize0.67}}}                                     & \best \textbf{\first{95.76{\scriptsize0.59}}} & \textbf{\second{98.97{\scriptsize0.08}}}                                     & 73.75{\scriptsize8.20}                                     & \best \textbf{\first{82.50{\scriptsize5.83}}}  \\
             & \posplus (ours)                  & \best 98.37{\scriptsize1.26} & \best \textbf{\second{87.82{\scriptsize0.96}}} & \best 95.04{\scriptsize0.27} & \best \textbf{\third{96.77{\scriptsize0.39}}} & \best \textbf{\first{94.77{\scriptsize0.68}}} & \textbf{\second{95.72{\scriptsize0.56}}}                                     & \best \textbf{\first{99.00{\scriptsize0.08}}} & \best \textbf{\first{78.44{\scriptsize9.83}}} & \textbf{\third{79.17{\scriptsize10.87}}}                                     \\
            & \sop (ours)                    & 90.61{\scriptsize1.94}                                     & 75.64{\scriptsize1.33}                                     & 94.34{\scriptsize0.30}                                     & 92.98{\scriptsize0.58}                                     & 91.24{\scriptsize0.80}                                     & 88.23{\scriptsize0.73}                                     & 95.91{\scriptsize0.29}                                     & 69.49{\scriptsize7.12}                                     & 72.29{\scriptsize14.42}                                     \\ 
\hline
\multicolumn{2}{c|}{Gain}                                                                          & \multicolumn{1}{c}{-0.55}                        & \multicolumn{1}{c}{-2.43}                        & \multicolumn{1}{c}{-0.46}                        & \multicolumn{1}{c|}{-1.39}                       & \multicolumn{1}{c}{+2.51}                         & \multicolumn{1}{c}{+2.77}                         & \multicolumn{1}{c}{+0.64}                         & \multicolumn{1}{c}{0}                            & \multicolumn{1}{c}{+2.93}                          \\
\bottomrule
\end{tabular}
}
\caption{Average AUC for attributed and non-attributed datasets (over 10 runs). The top 3 models are indicated by \textbf{\first{First}}, \textbf{\second{Second}}, and \textbf{\third{Third}}. \colorbox[rgb]{0.851,0.918,0.827}{Green} is best model among our \ssgrl variants. \colorbox[rgb]{1,0.949,0.8}{Yellow} is the best baseline. Gain is AUC difference of \colorbox[rgb]{0.851,0.918,0.827}{Green} and \colorbox[rgb]{1,0.949,0.8}{Yellow}.}
\label{auc-values}
\end{table*}

\vskip 1mm
\noindent \textbf{Dataset.} For our experiments, we use undirected, weighted and unweighted, attributed and non-attributed datasets that are publicly available and commonly used in other link prediction studies \cite{zhang2018link,zhang2018end,li2020distance,pan2022neural,louis2022sampling,chamberlain2023graph}. Table \ref{table:dataset_comparison} shows the statistics of our datasets. We divide our datasets into three categories; non-attributed datasets, attributed datasets and the OGB datasets. The edges in the attributed and non-attributed dataset categories are randomly split into 85\% training, 5\% validation, and 10\% testing sets, except for Cora, CiteSeer and Pubmed datasets chosen for comparison in Table \ref{table:ogb-results}, where we follow the experimental setup in \cite{chamberlain2023graph} with a random split of 70\% training, 10\% validation, and 20\% testing sets. For the OGB datasets, we follow the dataset split provided by the OGB team \cite{hu2021ogb}.


\vskip 1mm
\noindent \textbf{Baselines.} For attributed and non-attributed datasets, we compare our \ssgrl models (\pos, \posplus, and \sop) with 15 baselines belonging to five different categories of link prediction models. Our \textit{heuristic} benchmarks include common neighbors (CN), Adamic Adar (AA) \cite{adamic2003friends}, and personalized pagerank (PPR). For \textit{message-passing GNNs (MPGNNs)}, we select GCN \cite{kipf2017semi}, GraphSAGE \cite{hamilton2017inductive} and GAT \cite{vaswani2017attention}. Our \textit{latent factor (LF)} benchmarks are  node2vec \cite{grover2016node2vec} and Matrix Factorization \cite{koren2009matrix}. The \textit{autoencoder (AE)} methods include GAE \& VGAE \cite{kipf2016variational}, adversarially regularized variational graph autoencoder  (ARVGA) \cite{pan2018adversarially} and Graph InfoClust (GIC) \cite{Mavromatis2021GraphIM}. Finally, our examined SGRLs include SEAL \cite{zhang2018link}, GCN+DE (distance encoding
\cite{li2020distance}), and WalkPool \cite{pan2022neural}.  For OGB datasets, we compare against BUDDY \cite{chamberlain2023graph} and its baselines which include a subset of our baselines outlined above and GraphSAGE \cite{hamilton2017inductive}. 


\begin{table*}[h!]
\centering
\setlength{\extrarowheight}{0pt}
\addtolength{\extrarowheight}{\aboverulesep}
\addtolength{\extrarowheight}{\belowrulesep}
\setlength{\aboverulesep}{0pt}
\setlength{\belowrulesep}{0pt}
\arrayrulecolor{black}

\scalebox{0.80}{
\begin{tabular}{l|cccc|cccc|cccc} 
\toprule
\textbf{Model}              & \textbf{Training}                                 & \textbf{Inference}                              & \textbf{Preproc.}                          & \textbf{Runtime}                            & \textbf{Training}                                & \textbf{Inference}                              & \textbf{Preproc.}                         & \textbf{Runtime}                           & \textbf{Training}                                & \textbf{Inference}                              & \textbf{Preproc.}                          & \textbf{Runtime}                             \\ 
\hline
\multicolumn{1}{l|}{}       & \multicolumn{4}{c|}{\textbf{NS }(non-attributed)}                                                                                                                                              & \multicolumn{4}{c|}{\textbf{Power }(non-attributed)}                                                                                                                                        & \multicolumn{4}{c}{\textbf{Yeast }(non-attributed)}                                                                                                                                            \\ 
\hline
SEAL                        & 4.91{\scriptsize0.23}                                       & 0.14{\scriptsize0.01}                                     & \worst17.86      & 275.28                                      & 11.73{\scriptsize0.02}                                     & 0.33{\scriptsize0.01}                                     & \worst44.48     & 658.14                                     & 24.03{\scriptsize0.40}                                     & 0.54{\scriptsize0.05}                                     & \worst115.02     & 1362.85                                      \\
GCN+DE                      & \best3.58{\scriptsize0.12}   & \best0.10{\scriptsize0.01} & \best11.73  & \best198.98  & \best8.62{\scriptsize0.27}  & \best0.25{\scriptsize0.01} & \best28.59 & \best479.4  & \best18.41{\scriptsize0.71} & \best0.46{\scriptsize0.06} & \best82.19  & \best1040.72  \\
WalkPool                    & \worst7.66{\scriptsize0.09}       & \worst0.41{\scriptsize0.02}     & 12.18                                      & \worst427.03      & \worst18.46{\scriptsize0.76}     & \worst0.87{\scriptsize0.06}     & 33.51                                     & \worst1024.55    & \worst174.80{\scriptsize1.06}    & \worst8.05{\scriptsize0.11}     & 90.75                                      & \worst9443.17      \\ 
\hline
\pos                        & \best2.24{\scriptsize0.15}   & \best0.06{\scriptsize0.01} & 34.86                                      & 152.23                                      & 5.58{\scriptsize0.48}                                      & \best0.14{\scriptsize0.01} & 97.71                                     & 388.97                                     & 9.95{\scriptsize1.45}                                      & 0.20{\scriptsize0.06}                                     & \worst259.96     & \worst775.58       \\
\posplus                       & \worst2.54{\scriptsize0.06}       & \worst0.07{\scriptsize0.00}     & \worst41.43      & \worst173.78      & \worst6.12{\scriptsize0.24}      & \worst0.16{\scriptsize0.01}     & \worst107.77    & \worst426.53     & \worst10.49{\scriptsize0.61}     & \best0.20{\scriptsize0.04} & 206.52                                     & 749.87                                       \\
\sop                         & 2.26{\scriptsize0.11}                                       & 0.06{\scriptsize0.00}                                     & \best24.67  & \best142.45  & \best5.41{\scriptsize0.23}  & 0.14{\scriptsize0.01}                                     & \best65.65 & \best347.62 & \best9.24{\scriptsize0.74}  & \worst0.22{\scriptsize0.04}     & \best117.23 & \best597.29   \\ 
\hline
\multicolumn{1}{c}{Speedup} & 3.42(1.41)                                        & 6.83(1.43)                                      & 0.72(0.28)                                 & \multicolumn{1}{c}{3.00(1.14)}                 & 3.41(1.41)                                       & 6.21(1.56)                                      & 0.68(0.27)                                & \multicolumn{1}{c}{2.95(1.12)}             & 18.92(1.76)                                      & 40.25(2.09)                                     & 0.98(0.32)                                 & 15.81(1.34)                                  \\ 
\hline
\multicolumn{1}{l|}{}       & \multicolumn{4}{c|}{\textbf{PB }(non-attributed)}                                                                                                                                              & \multicolumn{4}{c|}{\textbf{Cora }(attributed)}                                                                                                                                             & \multicolumn{4}{c}{\textbf{CiteSeer }(attributed)}                                                                                                                                             \\ 
\hline
SEAL                        & 64.62{\scriptsize5.59}                                      & 2.32{\scriptsize0.10}                                     & \worst531.79     & 3947.45                                     & 18.37{\scriptsize1.49}                                     & 0.73{\scriptsize0.12}                                     & \worst113.32    & \worst1090.94    & 12.54{\scriptsize0.69}                                     & 0.58{\scriptsize0.10}                                     & \worst93.52      & 768.72                                       \\
GCN+DE                      & \best55.82{\scriptsize1.59}  & \best2.01{\scriptsize0.09} & 398.81                                     & \best3346.80 & \best14.85{\scriptsize0.53} & \best0.62{\scriptsize0.08} & 80.48                                     & \best872.68 & \best11.43{\scriptsize0.71} & \best0.52{\scriptsize0.07} & 71.97                                      & \best685.98   \\
WalkPool                    & \worst133.30{\scriptsize0.52}     & \worst6.48{\scriptsize0.15}     & \best136.29 & \worst7291.50     & \worst18.53{\scriptsize0.91}     & \worst1.00{\scriptsize0.15}     & \best27.43 & 1034.33                                    & \worst15.32{\scriptsize0.54}     & \worst0.87{\scriptsize0.05}     & \best22.82  & \worst859.27       \\ 
\hline
\pos                         & 13.42{\scriptsize0.77}                                      & \worst0.33{\scriptsize0.04}     & 1754.88                                    & 2452.39                                     & 5.44{\scriptsize0.52}                                      & \best0.15{\scriptsize0.02} & \worst106.45    & 394.12                                     & \best4.82{\scriptsize0.22}  & \best0.15{\scriptsize0.01} & \worst78.62      & \worst335.37       \\
\posplus                       & \worst15.56{\scriptsize1.28}      & 0.29{\scriptsize0.05}                                     & \worst2527.23    & \worst3331.56     & \worst5.87{\scriptsize0.17}      & \worst0.17{\scriptsize0.01}     & 93.69                                     & \worst401.05     & 4.91{\scriptsize0.39}                                      & \worst0.17{\scriptsize0.02}     & 72.02                                      & 331.55                                       \\
\sop                         & \best13.32{\scriptsize0.72}  & \best0.25{\scriptsize0.06} & \best333.58 & \best1022.90 & \best5.04{\scriptsize0.17}  & 0.15{\scriptsize0.03}                                     & \best35.65 & \best300.10 & \worst4.96{\scriptsize0.14}      & 0.17{\scriptsize0.01}                                     & \best31.57  & \best293.96   \\ 
\hline
\multicolumn{1}{c}{Speedup} & 10.01(3.59)                                       & 25.92(6.09)                                     & 1.59(0.05)                                 & \multicolumn{1}{c}{7.13(1.00)}              & 3.68(2.53)                                       & 6.67(3.65)                                      & 3.18(0.26)                                & \multicolumn{1}{c}{3.64(2.18)}             & 3.18(2.30)                                        & 5.80(3.06)                                       & 2.96(0.29)                                 & 2.92(2.05)                                   \\ 
\hline
\multicolumn{1}{l|}{}       & \multicolumn{4}{c|}{\textbf{Pubmed }(attributed)}                                                                                                                                              & \multicolumn{4}{c|}{\textbf{Texas }(attributed)}                                                                                                                                            & \multicolumn{4}{c}{\textbf{Wisconsin }(attributed)}                                                                                                                                            \\ 
\hline
SEAL                        & \worst533.18{\scriptsize4.64}     & \worst38.46{\scriptsize1.08}    & 141.76                                     & \worst30150.31    & 0.32{\scriptsize0.01}                                      & \best0.01{\scriptsize0.00} & \worst2.55      & 20.46                                      & 0.47{\scriptsize0.01}                                      & \best0.02{\scriptsize0.00} & \worst3.29       & 29.27                                        \\
GCN+DE                      & 423.73{\scriptsize2.67}                                     & 34.44{\scriptsize1.21}                                    & \best106.00 & 24311.00                                    & \best0.31{\scriptsize0.01}  & 0.01{\scriptsize0.00}                                     & 1.87                                      & \best18.55  & \best0.43{\scriptsize0.01}  & 0.02{\scriptsize0.00}                                     & 2.63                                       & \best26.19    \\
WalkPool                    & \best150.27{\scriptsize6.22} & \best8.10{\scriptsize1.06} & \worst341.12     & \best8474.72 & \worst0.55{\scriptsize0.08}      & \worst0.03{\scriptsize0.01}     & \best0.92  & \worst32.54      & \worst0.85{\scriptsize0.04}      & \worst0.06{\scriptsize0.00}     & \best1.08   & \worst49.38        \\ 
\hline
\pos                         & 38.90{\scriptsize2.89}                                      & 0.79{\scriptsize0.10}                                     & \worst2986.74    & 5017.78                                     & 0.16{\scriptsize0.01}                                      & \best0.01{\scriptsize0.00} & 1.87                                      & \worst12.71      & \best0.21{\scriptsize0.02}  & \best0.01{\scriptsize0.00} & 2.65                                       & 15.86                                        \\
\posplus                       & \worst45.32{\scriptsize2.21}      & \worst0.92{\scriptsize0.11}     & 2976.80                                    & \worst5335.86     & \worst0.18{\scriptsize0.01}      & 0.01{\scriptsize0.00}                                     & \worst2.26      & 11.96                                      & \worst0.24{\scriptsize0.02}      & 0.01{\scriptsize0.00}                                     & \worst2.91       & \worst16.07        \\
\sop                         & \best38.38{\scriptsize2.90}  & \best0.75{\scriptsize0.15} & \best526.62 & \best2526.22 & \best0.15{\scriptsize0.01}  & \worst0.01{\scriptsize0.00}     & \best1.34  & \best9.61   & 0.22{\scriptsize0.01}                                      & \worst0.01{\scriptsize0.00}     & \best1.87   & \best13.75    \\ 
\hline
\multicolumn{1}{c}{Speedup} & 13.89(3.32)                                       & 51.28(8.80)                                     & 0.65(0.04)                                 & \multicolumn{1}{c}{11.93(1.59)}             & 2.62(1.72)                                       & 3.00(1.00)                                            & 1.90(0.41)                                 & \multicolumn{1}{c}{3.39(1.46)}             & 4.05(1.79)                                       & 6.00(2.00)                                            & 1.76(0.37)                                 & 3.59(1.63)                                   \\
\bottomrule
\end{tabular}
}
\caption{Computation time of SGRLs vs. our \ssgrl models: average training time (over 50 epochs), average inference time, preprocessing time, and total runtime (preprocessing, training, and inference time) for 50 epochs. \colorbox[rgb]{0.851,0.918,0.827}{Green} is the fastest and \colorbox[rgb]{0.957,0.8,0.8}{Red} is slowest for each group of SGRLs and \ssgrl. Max(min) speedup corresponds to the ratio of time taken by the slowest (fastest) SGRLs to our fastest (slowest) model.}
\label{resource-cons}
\end{table*}


\vskip 1mm
\noindent \textbf{Setup.} For SGRL and \ssgrl methods, we set the number of hops  for the non-attributed datasets (except WalkPool on the Power dataset with ),  on attributed datasets (except for WalkPool with  based on \cite{pan2022neural}), and  for OGB datasets. \footnote{All hyperparameters of baselines are optimally selected based on their original papers or the shared public implementations. When possible, we also select the \ssgrl hyperparameters to match the benchmarks' hyperparameters for a fair comparison.} In \ssgrl models, we set the number of operators  for all datasets except ogbl-collab, ogbl-ppa and ogbl-citation2 where . We also use the zero-one labeling scheme for all datasets except for ogbl-citation2 and ogbl-ppa, where DRNL is used instead. The  graph readout function in center-common-neighbor pooling is set to a simple mean aggregation, except for ogbl-vessel, ogbl-ppa and ogbl-citation2 where sum is used instead. In \ssgrl models, across the attributed and non-attributed datasets, we set the hidden dimension in Eq.~\ref{eq:simple_sgrl} to , and also implement  in Eq.~\ref{eq:link-prob-s3grl} as an MLP with one -dimensional hidden layer. For all models, we set the dropout to 0.5 and train them for 50 epochs with Adam \cite{kingma2014adam} and a batch size of 32 (except for MPGNNs with full-batch training on the input graph).





For the comparison against BUDDY on OGB and Planetoid datasets \cite{yang2016revisiting}, the results are taken from \cite{chamberlain2023graph}, except for ogbl-vessel dataset, where the baseline figures are taken from the publicly-shared leaderboards.\footnote{\href{https://ogb.stanford.edu/docs/leader\_linkprop}{https://ogb.stanford.edu/docs/leader\_linkprop} hosts the ogbl-vessel leaderboard from which we report the baseline results as of April 2, 2023, except for BUDDY, run by adding support from \href{https://github.com/melifluos/subgraph-sketching}{https://github.com/melifluos/subgraph-sketching}.}.





For all models, on the attributed and non-attributed datasets, we report the average of the area under the curve (AUC) of the testing data over 10 runs with different fixed random seeds.\footnote{We exclude Average Precision (AP) results due to their known strong correlations with AUC results \cite{pan2022neural}.} 
For the experiments against BUDDY on OGB and Planetoid datasets, we report the average of 10 runs (with different fixed random seeds) on the efficacy measures consistent with \cite{chamberlain2023graph}.

In each run, we test a model on the testing data with those parameters with the highest efficacy measure on the validation data. Our code is implemented in PyTorch Geometric \cite{Fey/Lenssen/2019} and PyTorch \cite{paszke2019pytorch}.\footnote{Our codes are available at \href{https://github.com/venomouscyanide/S3GRL\_OGB}{https://github.com/venomouscyanide/S3GRL\_OGB}. All experiments are run on servers with 50 CPU cores, 377 GB RAM, and 11 GB GTX 1080 Ti GPUs.} To compare the computational efficiency, we report the average training and inference time (over 50 epochs) and the total preprocessing and runtime for a fixed run on the attributed and non-attributed datasets. 












\vskip 1mm
\noindent \textbf{Results: Attributed and Non-attributed Datasets.} On the attributed datasets, the \ssgrl models, particularly \posplus and \pos, consistently outperform others (see Table \ref{auc-values}). Their gain/improvement, compared to the best baseline, can reach  (for Wisconsin) and  (for CiteSeer). This state-of-the-art performance of \posplus and \pos suggests that our simplification techniques do not weaken SGRLs' efficacy and even improve their generalizability. Most importantly, this AUC gain is achieved by multi-fold less computation: The \ssgrl models benefit \grA{2.3--13.9} speedup in training time for citation network datasets of Cora, CiteSeer, and Pubmed (see Table \ref{resource-cons}). Similarly, inference time witnesses \grA{3.1--51.2} speedup, where the maximum speedup is achieved on the largest attributed dataset Pubmed. Our \ssgrl models exhibit higher dataset preprocessing times compared to SGRLs (see Table \ref{resource-cons}). However, this is easily negated by our models' faster accumulative training and inference times that lead to  \grA{1.4--11.9} overall runtime speedup across all attributed datasets (min. for Texas and max. for Pubmed). 

We observe comparatively lower AUC values for \sop, which can be attributed to longer-range information being of less value on these datasets. Regardless, \sop still shows comparable AUC to the other SGRLs while offering substantially higher speedups. 

Despite only consuming the source-target information, \pos achieves first or second place in the citation networks indicating the power of the center pooling. Moreover, the higher efficacy of \posplus compared to \pos across a few datasets indicate added expressiveness provided by center-common-neighbor pooling. Finally, the autoencoder methods outperform SGRLs on citation networks. However, \ssgrl instances outperform them and have enhanced the learning capability of SGRLs.


For the non-attributed datasets, although WalkPool outperforms others, we see strong performance from our \ssgrl instances. Our instances appear second or third (e.g., Yeast or Power) or have a small margin to the best model (e.g., NS or PB). The maximum loss in our models is bounded to 2.43\% (see Power's gain in Table \ref{auc-values}). Regardless of the small loss of efficacy, our \ssgrl models demonstrate multi-fold speedup in training and inference times: training with \grA{1.4--18.9} speedup and inference with \grA{1.4--40.2} (the maximum training and inference speedup on Yeast). We see a similar pattern of higher preprocessing times for our models; however, it gets negated by faster training and inference times leading to an overall speedup in runtimes with the maximum of \grA{15.8} for Yeast. \sop shows a relatively lower AUC in the non-attributed dataset except for PB and Yeast, possibly indicating that long-range information is more crucial for them.

For all datasets, we usually observe higher AUC for \posplus than its \pos variant suggesting the expressiveness power of center-common-neighbor pooling over simple center pooling. Of course, these slight AUC improvements come with slightly higher computational costs (see Table \ref{resource-cons}). 




\begin{table*}
\centering


\scalebox{1.1}{
\begin{tabular}{l|c|c|c|c|c|c|c|c} 
\toprule
\textbf{Model} & \textbf{Cora} & \textbf{CiteSeer} & \textbf{Pubmed} & \textbf{Collab} & \textbf{DDI} & \textbf{Vessel}  & \textbf{Citation2} & \textbf{PPA} \\
               & HR@100        & HR@100            & HR@100          & HR@50           & HR@20        & roc-auc        & MRR  & HR@100 \\ 
\hline
CN             & 33.92{\scriptsize0.46}    & 29.79{\scriptsize0.90}        & 23.13{\scriptsize0.15}      & 56.44{\scriptsize0.00}      & 17.73{\scriptsize0.00}   & 48.49{\scriptsize0.00}  & 51.47{\scriptsize0.00} & 27.65{\scriptsize0.00} \\
AA             & 39.85{\scriptsize1.34}    & 35.19{\scriptsize1.33}        & 27.38{\scriptsize0.11}      & 64.35{\scriptsize0.00}      & 18.61{\scriptsize0.00}   & 48.49{\scriptsize0.00}  & 51.89{\scriptsize0.00} & 32.45{\scriptsize0.00} \\
GCN            & 66.79{\scriptsize1.65}    & 67.08{\scriptsize2.94}        & 53.02{\scriptsize1.39}      & 44.75{\scriptsize1.07}      & \textbf{\third{37.07{\scriptsize5.07}}}   & 43.53{\scriptsize9.61}    &  84.74{\scriptsize0.21} & 18.67{\scriptsize1.32} \\
SAGE           & 55.02{\scriptsize4.03}    & 57.01{\scriptsize3.74}        & 39.66{\scriptsize0.72}      & 48.10{\scriptsize0.81}      & \textbf{\second{53.90{\scriptsize4.74}}}   & 49.89{\scriptsize6.78}   &  82.60{\scriptsize0.36} & 16.55{\scriptsize2.40} \\
SEAL           & \textbf{\third{81.71{\scriptsize1.30}}}    & \textbf{\third{83.89{\scriptsize2.15}}}        & \textbf{\second{75.54{\scriptsize1.32}}}      & \textbf{\third{64.74{\scriptsize0.43}}}      & 30.56{\scriptsize3.86}   & \textbf{\second{80.50{\scriptsize0.21}}}   &  \textbf{\second{87.67{\scriptsize0.32}}}  & \textbf{\second{48.80{\scriptsize3.16}}} \\
BUDDY          & \textbf{\second{88.00{\scriptsize0.44}}}    & \textbf{\second{92.93{\scriptsize0.27}}}        & \textbf{\third{74.10{\scriptsize0.78}}}      & \textbf{\second{65.94{\scriptsize0.58}}}      & \textbf{\first{78.51{\scriptsize1.36}}}   & \textbf{\third{55.14{\scriptsize0.11}}}          &   \textbf{\third{87.56{\scriptsize0.11}}}   & \textbf{\first{49.85{\scriptsize0.20}}} \\
\posplus (ours)           & \textbf{\first{91.55{\scriptsize1.16}}}  & \textbf{\first{94.79{\scriptsize0.58}}}      & \textbf{\first{79.40{\scriptsize1.53}}}    & \textbf{\first{66.83{\scriptsize0.30}}}    & 22.24{\scriptsize3.36} & \textbf{\first{80.56{\scriptsize0.06}}}   &  \textbf{\first{88.14{\scriptsize0.08}}}  & \textbf{\third{42.42{\scriptsize1.80}}} \\
\bottomrule
\end{tabular}
}
\caption[Results for \posplus in comparison to the methodology set in BUDDY \cite{chamberlain2023graph}]{Results for \posplus in comparison to the datasets and baselines chosen in BUDDY \cite{chamberlain2023graph}. The top three performing models are \textbf{\first{First}}, \textbf{\second{Second}}, and \textbf{\third{Third}}.}
\label{table:ogb-results}

\end{table*}



\vskip 1mm
\noindent \textbf{Results: OGB and BUDDY.} As shown in Table \ref{table:ogb-results}, our \posplus model can easily scale to graph datasets with millions of nodes and edges. Under the experimental setup in BUDDY, our \posplus still outperforms all the baselines (including BUDDY) for all Planetoid datasets, confirming the versatility of our model to different dataset splits and evaluation criteria. Our \posplus outperforms others on ogbl-collab, ogbl-citation2, and ogbl-vessel. For ogbl-ppa, we come in third place. For ogbl-ddi, \posplus performs significantly lower than SEAL and BUDDY; but we still perform better than the heuristic baselines. This performance on ogbl-ddi might be a limitation of our \posplus, possibly because the common-neighbor information is noisy in denser ogbl-ddi, with the performance degradation exaggerated by the lack of node feature information. However, this set of experiments confirms the competitive performance of \posplus on the OGB datasets with large-scale graphs under different types of efficacy measurements.



\begin{table}[t!]
\centering
\setlength{\extrarowheight}{0pt}
\addtolength{\extrarowheight}{\aboverulesep}

\scalebox{0.83}{
\begin{tabular}{p{10pt}|lcccc} 
\toprule
\textbf{}                 & \textbf{Model} & \textbf{Training} &  \textbf{Preproc.} & \textbf{Runtime} & \textbf{AUC}  \\ 
\hline
\multirow{4}{*}{\begin{sideways}
    Cora\end{sideways}}     & \pos           & 4.82{\scriptsize0.25}               & 83.28{\scriptsize3.16}      & 336.70{\scriptsize2.47}    & 94.65{\scriptsize0.67}  \\
                          & \pos  + ScaLed  & 4.73{\scriptsize0.22}               & 60.32{\scriptsize3.50}      & 308.73{\scriptsize4.27}    & 94.35{\scriptsize0.52}  \\
                          \cmidrule{2-6}
                          & \posplus           & 5.59{\scriptsize0.31}               & 94.33{\scriptsize7.36}      & 387.95{\scriptsize7.81}    & 94.77{\scriptsize0.65}  \\
                          & \posplus+ ScaLed  & 5.49{\scriptsize0.28}             & 69.95{\scriptsize5.12}      & 358.40{\scriptsize7.08}    & 94.80{\scriptsize0.58}  \\ 
\midrule
\multirow{4}{*}{\begin{sideways}CiteSeer \end{sideways}} & \pos             & 4.66{\scriptsize0.20}              & 61.86{\scriptsize0.64}      & 308.20{\scriptsize2.86}    & 95.76{\scriptsize0.59}  \\
                          & \pos + ScaLed  & 4.65{\scriptsize0.21}               & 56.65{\scriptsize2.59}      & 302.45{\scriptsize3.95}    & 95.52{\scriptsize0.65}  \\
                          \cmidrule{2-6}
                          & \posplus           & 4.72{\scriptsize0.35}              & 79.14{\scriptsize5.34}      & 330.29{\scriptsize5.61}    & 95.60{\scriptsize0.52}  \\
                          & \posplus+ ScaLed  & 4.66{\scriptsize0.27}               & 65.56{\scriptsize5.13}      & 313.38{\scriptsize6.25}    & 95.60{\scriptsize0.53}  \\
\bottomrule
\end{tabular}}


\caption{Results for \ssgrl as a scalability framework: ScaLed's subgraph sampling combined with \pos and \posplus.}
\label{table:ksup-scaled}
\end{table}

\vskip 1mm
\noindent \textbf{\ssgrl as a scalability framework.}
To demonstrate the flexibility of \ssgrl as a scalability framework, we explore how easily it can host other scalable SGRLs. Specifically, we exchange the subgraph sampling strategy of \pos (and \posplus) with the random-walk induced subgraph sampling technique in \scaled. Through this process, we reduce our operator sizes and benefit from added regularization offered through stochasticity in random walks. We fixed its hyperparameters, the random walk length , and the number of random walks . Table \ref{table:ksup-scaled} shows the average computational time and AUC (over 10 runs).  The subgraph sampling of \scaled offers further speedup in \pos and \posplus in training, dataset preprocessing, and overall runtimes. For \posplus variants, we even witness AUC gains on Cora and no AUC losses on CiteSeer. The AUC gains of \posplus could be attributed to the regularization offered through sparser enclosing subgraphs. This demonstration suggests that \ssgrl can provide a foundation for hosting various SGRLs to further boost their scalability. 


\section{Conclusions and Future Work}
Subgraph representation learning methods (SGRLs), albeit comprising state-of-the-art models for link prediction, suffer from large computational overheads. In this paper, we propose a novel SGRL framework \ssgrl, aimed at faster inference and training times while offering flexibility to emulate many other SGRLs. We achieve this speedup through easily precomputable subgraph-level diffusion operators in place of expensive message-passing schemes. \ssgrl supports multiple subgraph selection choices for the creation of the operators, allowing for a multi-scale view around the target links. Our experiments on multiple instances of our \ssgrl framework show significant computational speedup over existing SGRLs, while offering matching (or higher) link prediction efficacies. For future work, we intend to devise learnable subgraph selection and diffusion operators, that are catered to the training dataset and computational constraints.



\bibliographystyle{ACM-Reference-Format}
\bibliography{references}



\end{document}
