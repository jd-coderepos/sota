



\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers,sort]{natbib}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks=true,linkcolor=red, citecolor=green]{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{adjustbox}
\usepackage{multirow}
\usepackage[flushleft]{threeparttable}
\usepackage{makecell}
\usepackage{tabulary}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage{kotex}
\DeclareMathOperator{\E}{\mathbb{E}}
\usepackage{bbold}
\usepackage{wrapfig}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{scalerel}
\usepackage{caption}


\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\usepackage{stfloats}

\newcommand{\xmark}{\text{\ding{55}}}
\newcommand{\cmark}{\text{\ding{51}}}





\def\cvprPaperID{4014} \def\confYear{CVPR 2021}




\begin{document}

\title{BBAM: Bounding Box Attribution Map for Weakly Supervised \\Semantic and Instance Segmentation}


\author{Jungbeom Lee ~~~~~~~ Jihun Yi ~~~~~~~ Chaehun Shin  ~~~~~~~  Sungroh Yoon\thanks{Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>.}\\
 Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea\\
 ASRI, INMC, ISRC, and Institute of Engineering Research, Seoul National University\\
{\tt\small \{jbeom.lee93, t080205, chaehuny, sryoon\}@snu.ac.kr}}

\maketitle


\begin{abstract}
Weakly supervised segmentation methods using bounding box annotations focus on obtaining a pixel-level mask from each box containing an object. Existing methods typically depend on a class-agnostic mask generator, which operates on the low-level information intrinsic to an image. In this work, we utilize higher-level information from the behavior of a trained object detector, by seeking the smallest areas of the image from which the object detector produces almost the same result as it does from the whole image. These areas constitute a bounding-box attribution map (BBAM), which identifies the target object in its bounding box and thus serves as pseudo ground-truth for weakly supervised semantic and instance segmentation. This approach significantly outperforms recent comparable techniques on both the PASCAL VOC and MS COCO benchmarks in weakly supervised semantic and instance segmentation. In addition, we provide a detailed analysis of our method, offering deeper insight into the behavior of the BBAM.
The code is available at: \url{https://github.com/jbeomlee93/BBAM}.

\end{abstract}

\thispagestyle{empty}
\section{Introduction}
Object segmentation is one of the most important steps in image recognition.
Advances in deep learning have greatly improved the performance of semantic and instance segmentation~\cite{he2017mask, chen2017deeplab} through the use of huge amounts of pixel-level annotated training data. 
However, annotating with pixel-level masks requires a lot of effort. According to Bearman \textit{et al.}~\cite{bearman2016s}, constructing a pixel-level mask for an image containing an average of 2.8 objects takes about 4 minutes.
This is why weakly supervised methods have been proposed, in which segmentation networks are trained using annotations that are less detailed than pixel-level masks, such as bounding boxes~\cite{song2019box, khoreva2017simple, dai2015boxsup}, or image-level tags~\cite{lee2019ficklenet, ahn2019weakly, ahn2018learning}.

The most easily obtainable annotation is the class label.
Labeling an image with class labels takes around 20 seconds~\cite{bearman2016s},
but it only indicates that objects of certain classes are depicted and gives no information about their locations in the image. Moreover, class labels provide no help in separating different objects of the same class, which is the goal of instance segmentation.


Bounding boxes provide information about individual objects and their locations.
Bounding box annotation takes about 38.1 seconds per image~\cite{bellver2019budget}, which is much more attractive than constructing pixel-level masks.
Many researchers have tackled semantic segmentation~\cite{dai2015boxsup, khoreva2017simple, song2019box, kulharia12356box2seg} and instance segmentation~\cite{khoreva2017simple, liao2019weakly, sun2020weakly, hsu2019weakly, arun2020weakly} using bounding box annotations as a search space in which a class-agnostic object mask can be found by an off-the-shelf object mask generator. These are mostly based on GrabCut~\cite{rother2004grabcut} or multiscale combinatorial grouping (MCG)~\cite{pont2016multiscale}.
Those mask generators operate on the low-level information of images, such as the color or brightness of pixels, and this limits the quality of the resulting mask. 
Thus, applying these mask generators to bounding box annotations requires additional steps such as estimating what proportion of the pixels in a bounding-box belong to the corresponding object~\cite{song2019box, kulharia12356box2seg}, iterative refinement of an estimated mask~\cite{dai2015boxsup}, and auxiliary attention modules~\cite{kulharia12356box2seg}.


We propose a pixel-level method of localizing a target object inside its bounding box using a trained object detector.
We make use of attribution maps obtained from the trained object detector, which highlight the image regions that the detector focuses on in conducting object detection.
Inspired by the perturbation methods used to explain the output of image classifiers~\cite{fong2017interpretable, fong2019understanding, dabkowski2017real}, we introduce a bounding box attribution map (BBAM) which provides an indication of the smallest areas of an image that are sufficient to make an object detector produce almost the same result as that from the original image.
The BBAM identifies the area occupied by the object in each bounding box predicted by the trained object detector.
Since this localization takes place at the pixel level, it can be used as a pseudo ground truth for weakly supervised learning of semantic and instance segmentation.







The main contributions of this paper can be summarized as follows.
\begin{itemize}
\vspace{-4pt}
	\item[] We propose a bounding box attribution map (BBAM), which can draw on the rich semantics learned by an object detector to produce pseudo ground-truth for training semantic and instance segmentation networks.
	\vspace{-5pt}
	\item[] Our technique significantly outperforms previous state-of-the-art methods of weakly supervised semantic and instance segmentation, assessed on the PASCAL VOC 2012 and MS COCO 2017 benchmarks.
	\vspace{-5pt}
	\item[] We analyze our method from various viewpoints, providing deeper insights into the properties of the BBAM.
\end{itemize}


\section{Related Work}
Fully supervised semantic and instance segmentation based on pixel-level annotations is highly reliable, but the manual annotation process is laborious. This requirement is overcome by weakly supervised methods based on inexact, but easily obtainable, annotations such as scribbles~\cite{tang2018normalized}, bounding boxes~\cite{song2019box, khoreva2017simple}, or class labels~\cite{lee2019ficklenet, ahn2019weakly, sun2020mining}. In this section, we briefly review some recently introduced weakly supervised approaches that use class labels (Section~\ref{W_imagelabel}) or bounding boxes (Section~\ref{W_boxes}). In addition, we describe some visual saliency methods related to our method (Section~\ref{XAI}).


\subsection{Learning with Class Labels}\label{W_imagelabel}
A class activation map (CAM)~\cite{zhou2016learning} is a widely adopted technique to obtain a localization map from class labels. However, a CAM only identifies the most discriminative regions of objects~\cite{lee2019ficklenet, lee2019frame}, and hence the majority of existing methods that use class labels~\cite{lee2019frame, lee2019ficklenet, li2019guided, huang2018weakly, ahn2018learning, hou2018self, lee2018robust, Shimoda_2019_ICCV, JiangOAAICCV19, fan2018cian, hong2017weakly} are primarily concerned with expanding the area of the target object activated by a CAM. For instance, erasure methods~\cite{hou2018self, wei2017object} iteratively find new regions of the target object by removing discriminative regions in an image.
Other methods~\cite{fan2018cian, sun2020mining} consider the information shared between several images by capturing cross-image semantic similarities and differences.
Seed growing and refinement techniques~\cite{ahn2018learning, ahn2019weakly, huang2018weakly} are typically used to expand the regions representing the target object imperfectly that are in the initial CAM, on the basis of relationships between pixels.
Other methods construct CAMs that embody the multi-scale semantic context in an image~\cite{lee2019ficklenet, lee2018robust, wei2018revisiting}.
Despite these efforts, the information available from class labels remains limited, so auxiliary information acquired from web images~\cite{shen2018bootstrapping} or videos~\cite{hong2017weakly, lee2019frame} can be used together.




\subsection{Learning with Bounding Boxes}\label{W_boxes}
Class labels have led to significant achievements in semantic segmentation, but they are inherently unhelpful in instance segmentation, which requires the separation of different objects of the same class.
In contrast, bounding boxes do provide information about the location of individual objects in an image, and they are still much cheaper than constructing pixel-level masks~\cite{bellver2019budget}.
Most existing methods utilized a bounding box as a search space to conduct low-level searches for object masks.
They create a pseudo mask within a box using off-the-shelf methods of mask proposal such as MCG~\cite{pont2016multiscale} or GrabCut~\cite{rother2004grabcut}. 
These processes can be guided by specifying the proportion of the pixels in a bounding box that are likely to belong to the object~\cite{song2019box, kulharia12356box2seg}. Iterative mask refinement techniques~\cite{dai2015boxsup} can also be applied.
However, these methods are largely based on low-level information in the image, and they ignore the semantics associated with the bounding boxes.
A rare exception is the multiple-instance learning formulation with a bounding box tightness prior~\cite{hsu2019weakly}: a crossing line within a box must contain at least one pixel of the target object.
The drawback with this approach is that only a small number of pixels are contributing to the localization of the object.


\vspace{-0.2em}
\subsection{Visual Saliency Methods}\label{XAI} 
\vspace{-0.2em}
Various methods have been proposed to visually explain the predictions of deep neural networks (DNNs)~\cite{fong2017interpretable, fong2019understanding, chang2018explaining, schulz2020restricting, zhou2016learning} in a form of a saliency map.
However, most studies have been concerned with classifiers, and only a few have looked at DNNs performing other tasks~\cite{hoyer2019grid, reif2019visualizing}. In particular, there have been no attempts to explain the predictions of object detectors, except Wu \textit{et al.}~\cite{wu2019towards}, who embedded interpretability inside the DNN, in this case Faster R-CNN~\cite{ren2015faster}. 
However, the explanation produced by their modified DNN is not immediately understandable because it is given as a form of tree, and thus it is not appropriate to generate pseudo ground truth for weakly supervised segmentation.
Gradient-based methods, such as SimpleGrad~\cite{zeiler2014visualizing}, SmoothGrad~\cite{smilkov2017smoothgrad}, and Grad-CAM~\cite{selvaraju2017grad}, can provide visual saliency maps of the results from classifiers, 
but these methods are not easily extended to object detectors, because of the structural difference between classifiers and object detectors. Nevertheless, gradient-based methods have a significant bearing on our approach, and we look at them in more detail in Section~\ref{sec_analysis}.


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.92\linewidth]{latex/Figures/stride_200604.pdf} \\label{eq_mask}
\mathcal{M}^{*} = \argmin_{\mathcal{M} \in [0, 1]^{\Omega}} \lambda \left\lVert \mathcal{M} \right\rVert_1 + \mathcal{L}_\text{perturb},
\label{eq_mask_perturb}
\begin{split}
\mathcal{L}_\text{perturb} = & \hspace{5pt}\mathbb{1}_{\text{box}} \left\lVert t^c - f^{\text{box}}(\Phi(I, \mathcal{M}), o) \right\rVert_1\\ &+ \mathbb{1}_{\text{cls}} \left\lVert p^c - f^{\text{cls}}(\Phi(I, \mathcal{M}), o) \right\rVert_1,
\end{split}
\label{eq_mask_weak}
\begin{split}
\mathcal{L}_{\text{perturb}} = \E_{o\in \mathcal{O}^{+}}[&\mathbb{1}_{\text{box}} \left\lVert t^c - f^{\text{box}}(\Phi(I, \mathcal{M}), o) \right\rVert_1\\ &+ \mathbb{1}_{\text{cls}} \left\lVert p^c - f^{\text{cls}}(\Phi(I, \mathcal{M}), o) \right\rVert_1].
\end{split}
\label{eq_mcg}
\begin{split}
&\mathcal{T}_{r}= \bigcup\limits_{i\in \mathcal{S}} m_{i} \text{,~~~where} \\
&\mathcal{S} = \{i~|m_i \subset \mathcal{T}\} \cup \{\argmax_i \text{IoU}(m_i, \mathcal{T})\}.
\end{split}
-0.95em]
     Method   & AP    & AP   & AP & AP & ABO\\
\hline\hline
    \-0.9em]
    \hline
    \-0.9em]
    \hline
    \-0.95em]
     Method   & sup.    & AP   & AP & AP \\
\hline\hline
    \-0.9em]
    \hline
    \-0.95em]
    Method  & \textit{val} & \textit{test}\\
    \hline\hline 
    \-0.9em]
\hline
    \-0.9em]
    \hline
    \-0.8em]
  \caption{\label{ins_sample} Examples of predicted instance masks for PASCAL VOC \textit{val} images of IRNet~\cite{ahn2019weakly}, Hsu \textit{et al.}~\cite{hsu2019weakly}, and ours.}
  \vspace{-1em}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{latex/Figures/semantic_seg_sample_cvpr.pdf} \-0.95em]
    
    MCG  & AP   & AP & AP  & AP   & AP   & AP \\
    \hline\hline
    \-0.9em]  
    \multicolumn{7}{l}{MS COCO \textit{val} images:} \\
      \xmark    &   23.5    &       47.9 &      20.3 &  10.4     &      24.9 & 36.5 \\
    \textcolor{red}{\cmark} &   \textbf{26.0}    &  \textbf{50.0}     & \textbf{23.9}      & \textbf{10.8}      &   \textbf{28.5}    & \textbf{40.3} \\
\Xhline{1pt}
    \end{tabular}\vspace{-1.2em}
  \label{tab:mcg}\end{table} 
The concurrent method, Box2Seg~\cite{kulharia12356box2seg}, achieved an mIoU of 76.4\% on the PASCAL VOC validation images, but it is based on UperNet~\cite{xiao2018unified}, which is a more powerful segmentation network than DeepLab-v2~\cite{pytorchdeeplab}.
For a fair comparison between Box2Seg~\cite{kulharia12356box2seg} and our BBAM, we attempt to relieve the benefit of UperNet~\cite{xiao2018unified} over DeepLab-v2~\cite{chen2017deeplab} by comparing the relative performance of the weakly supervised model to the fully supervised model.
Box2Seg achieves 88.4\% of the performance of its fully supervised equivalent (76.4 \textit{vs.} 86.4); but the corresponding figure for BBAM and its fully supervised equivalent is 96.7\% (73.7 \textit{vs.} 76.2).





\begin{table*}[t]
  \centering
\begin{minipage}{0.28\linewidth}
  \centering
  \includegraphics[width=\linewidth]{latex/Figures/effectiveness_of_each_head_white_v1.pdf}
  \vspace{-1.7em}
  \captionof{figure}{\label{fig_eachhead} Effect of each head on instance and semantic segmentation.}
  \end{minipage}\hfill
\begin{minipage}{0.34\linewidth}
\center
{
\begin{tabular}{c@{\hskip 0.15in}c@{\hskip 0.15in}c@{\hskip 0.2in}c@{\hskip 0.13in}c@{\hskip 0.09in}c}
     \Xhline{1pt}\-0.9em]
    0.2&0.2      &   \xmark    & 24.8      &  58.3     & 18.1  \\
    0.5&0.5  &   \xmark    &  28.3     &  59.5     &  24.7 \\

  0.8&0.8  &   \xmark    &  27.8     & 59.0      &  23.3 \\
    0.3   & 0.7   &    \xmark   &    28.1   &  59.5     & 24.0 \\
    0.3   & 0.7   &   \textcolor{red}{\cmark}    &  28.4   &    59.6  &  24.6  \\
    0.2   & 0.8   &    \xmark   &   28.6    &   60.4    & 24.0 \\
    0.2   & 0.8   &   \textcolor{red}{\cmark}    &  \textbf{29.6}     &  \textbf{61.9}     & \textbf{25.8} \\
    \Xhline{1pt}
    \end{tabular}}
    \vspace{-0.7em}
      \caption{Analysis of thresholds  and , and effect of the growing technique .}
  \label{table_params}\end{minipage}\hfill
\begin{minipage}{0.32\linewidth}
\centering
{
    \begin{tabular}{c@{\hskip 0.17in}c@{\hskip 0.13in}c@{\hskip 0.1in}c@{\hskip 0.1in}c}
    \Xhline{1pt}\-0.9em]
         0.001   &  26.6    &  58.7    &    21.1  &  67.9 \\
     0.003  &   28.1   &    59.9  & 22.8    &  69.7\\
     0.005    &   28.7   &   60.2    &   24.3    &  70.8 \\
     0.007   &  \textbf{29.6}     &      \textbf{61.9} &  \textbf{25.8}     &  \textbf{71.4}\\
     0.010   &  28.7    & 60.4     &     24.4  &  70.7 \\ 
     0.020   &   28.3   &    59.6  &     23.7  & 70.3 \\

    \Xhline{1pt}
    \end{tabular}}
    \vspace{-0.7em}
      \caption{Effect of  on instance (\texttt{Ins.}) and semantic (\texttt{Sem.}) segmentation.}\label{table_lambda}\end{minipage}
\vspace{-1.2em}
\end{table*} 
\vspace{-0.1em}
\subsection{Ablation Study}
\vspace{-0.1em}

\textbf{MCG proposals.}
Table~\ref{tab:mcg} shows how mask refinement with MCG proposals improves the instance segmentation performance of our method on the PASCAL VOC and MS COCO datasets.
Mask refinement with MCG proposals is particularly effective on masks for medium and large objects. 
The results obtained without MCG proposals offer the possibility of a fairer comparison with Hsu \textit{et al.}~\cite{hsu2019weakly}, which do not use MCG proposals. Our method produces better results than that of Hsu \textit{et al.}~\cite{hsu2019weakly} for both the PASCAL VOC and MS COCO datasets, which are shown in Tables~\ref{instance_voc} and \ref{instance_coco} respectively.
Hereinafter, to observe the contribution of each component of our system, we report results without using MCG proposals.

\textbf{\textit{Box} and \textit{cls heads}.}
BBAM can provide a separate attribution map for each head of the object detector by controlling the logical variables  and  in Eq.~\ref{eq_mask_weak}.
Figure~\ref{fig_eachhead} shows the effect of the BBAM obtained from each head on the performance of weakly supervised semantic and instance segmentation. Using the BBAM obtained from either the \textit{box head} ( and ) or the \textit{cls head} ( and ) shows competent performance, but the best performance is achieved when the two heads are used together. 
We attribute this to the complementary property of the two heads, which is examined in more detail in Section~\ref{sec_analysis}.







\textbf{Parameter sensitivity analysis.} Table~\ref{table_params} shows the effect of the thresholds  and , and the seed growing technique .
When  equals to , all pixels are assigned to either the foreground or the background.
We see that ignoring some pixels can improve the AP values, and the seed growing technique further improves performance.
We then studied the effect of , which controls the sparsity of the BBAM, on the performance of weakly supervised semantic and instance segmentation, with the results shown in Table~\ref{table_lambda}. 
Our method shows similar performance on semantic and instance segmentation over a broad range of values of .




\section{Detailed Analysis of the BBAM}\label{sec_analysis}



\textbf{Examples of BBAMs.}
Figure~\ref{BBAM_samples_total} shows BBAMs for validation images from PASCAL VOC~\cite{everingham2010pascal} and MS COCO~\cite{lin2014microsoft}.
The BBAMs have high values on the boundary and discriminative parts of each object, which are informative in conducting object detection.

\begin{figure}[t]
  \centering
\includegraphics[width=\linewidth]{latex/Figures/BBAM_samples_v1_print.pdf} \-0.7em]
  \caption{\label{fig_complement} Complementary operation of the \textit{box head} and the \textit{cls head}. (a) The definition of relative position. (b) Relative positions of the highly activated pixels from each head. (c) \textit{Box} and \textit{class} loss curves.}
  \vspace{-1.2em}
\end{figure*} 
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{latex/Figures/Object_size_v4.pdf}
\vspace{-1.5em}
\caption{\label{noise_robust} (a) Robustness against noisy box coordinate labels. (b) Localization accuracy by different strides. (c) Localization accuracy by different attribution methods.}
\vspace{-1em}
\end{figure} 
\textbf{Complementary operation of the \textit{box} and \textit{cls heads}.}
To determine which regions of an object are important to each head, we investigated the distribution of high-value pixels in the BBAM produced by each head.
In Figure~\ref{fig_complement}(a),  is the set of points on the contour of the object mask, and  is its centroid.
For each pixel , we determine  and .
Letting the angle between  and the -axis be , the position of the pixel  relative to  is .
In Figure~\ref{fig_complement}(b), we plot the relative positions of all the pixels with attribution values above 0.9 obtained from validation images of the PASCAL VOC dataset.
Pixels for which  are near the boundary of the object.
We observed that high values attributed by the \textit{box head} mainly occur near the boundary of the object, and those by the \textit{cls head} mainly occur in the interior.


Furthermore, we observed how much the prediction of each head changes when either of  and  is set to 1 during the optimization of Eq.~\ref{eq_mask}.
The extent of the change in prediction of each head can be inferred from the corresponding loss in Eq.~\ref{eq_mask_perturb}. Figure~\ref{fig_complement}(c) shows that applying the optimization of Eq.~\ref{eq_mask} to one of the heads increases the loss of the other head, implying that the discriminative area of the image necessary for each head is not sufficient for the other head to maintain the prediction.
These two observations suggest that the BBAM of each head provides complementary attributions. Examples of BBAMs obtained from each head are presented in the Appendix.




\textbf{Label noise in object detection.} We also looked at the robustness of our system against noisy box coordinate labels in instance segmentation. 
Hsu \textit{et al.}~\cite{hsu2019weakly} considered the effect of up to  of label noise: we extend this to . 
The validity of the bounding box tightness priors used by Hsu \textit{et al.}~\cite{hsu2019weakly} is seriously compromised by inaccurate box coordinates, with a considerable effect on performance, as shown in Figure~\ref{noise_robust}(a).
Our method shows better robustness than that of Hsu \textit{et al.}~\cite{hsu2019weakly}, whether the noise consists of expanded or contracted bounding box annotations.

\textbf{Effectiveness of an adaptive stride .}
As mentioned in Section~\ref{method_BBAM}, we use an adaptive stride  to cope with feature transformation due to RoI pooling.
Figure~\ref{noise_robust}(b) shows the IoU between the BBAM and ground truth mask on PASCAL VOC validation images, along with the results using fixed strides of 24 and 48.
Figure~\ref{noise_robust}(b) shows that a small fixed stride (=24) is ineffective with large objects, as is a large fixed stride (=48) with small objects. By contrast, an adaptive stride  can deal with objects of various sizes.


\textbf{Comparison with gradient-based methods.}
Gradient-based attribution methods, such as SimpleGrad~\cite{zeiler2014visualizing}, SmoothGrad~\cite{smilkov2017smoothgrad}, and Grad-CAM~\cite{selvaraju2017grad} can also provide attributions for the output of an object detector. 
However, since only the subset of features associated with the imperfect proposal is delivered to the \textit{cls} and \textit{box} heads, the gradients with respect to pixels, which exist outside the proposal yet essential for prediction, can vanish (but not completely, due to the receptive field). 
We provide empirical results supporting this analysis on the PASCAL VOC validation images: \underline{\textbf{(1)}} Figure~\ref{gradient_ex} shows examples in which SimpleGrad~\cite{zeiler2014visualizing} is applied to three similar predictions from different proposals. Pixels outside the proposal do indeed influence the predictions, but SimpleGrad's attributions mainly appear inside the proposal. 
\underline{\textbf{(2)}} We observed that the majority (87\%) of pixels with attribution values above 0.9 appear inside the imperfect proposal; the mean IoU between the set of positive proposals and the corresponding predictions is low (\textit{i.e.,} 0.56).
\underline{\textbf{(3)}} Figure~\ref{noise_robust}(c) shows that attribution maps from gradient-based attribution methods correlate poorly with ground truth masks.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{latex/Figures/gradient_v1_with_bar.pdf}
\vspace{-1.5em}
\caption{\label{gradient_ex} Examples of SimpleGrad~\cite{zeiler2014visualizing} for three similar predictions obtained from different proposals.}
\vspace{-1em}
\end{figure} 


\vspace{-0.15em}
\section{Conclusions}
\vspace{-0.15em}
We have introduced a bounding box attribution map (BBAM), which provides pixel-level localization of each target object in its bounding box by finding the smallest region that preserves the predictions of the object detector.
Our formulation is built on two-stage object detectors, but applying our method to one-stage object detectors is straightforward as long as they have \textit{box} and \textit{cls} heads.
Our experiments demonstrate that the BBAM achieves state-of-the-art performance on the PASCAL VOC and MS COCO benchmarks in weakly supervised semantic and instance segmentation.
We have also analyzed BBAMs from various viewpoints, and compared our technique with other attribution methods, to provide a deeper understanding of our approach.
We expect BBAMs to be a staple of future work on weakly supervised semantic and instance segmentation with bounding boxes, on a par with the CAM for class labels.  


\bigskip
\noindent\textbf{Acknowledgements:}
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) [2018R1A2B3001628], AIR Lab (AI Research Lab) in Hyundai \& Kia Motor Company through HKMC-SNU AI Consortium Fund, and the Brain Korea 21 Plus Project in 2021.




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

\clearpage

\renewcommand{\tabcolsep}{2pt}

\begin{table*}[t]
  \caption{Comparison of per-class mIoU scores.}
  \vspace{-1em}
\centering
  \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{lccccccccccccccccccccc|c}
    
        \Xhline{1pt}

        \-0.9em]

   \multicolumn{22}{l}{Results on validation images:}\\

    Shen \textit{et al.}~\cite{shen2018bootstrapping}~~~~~~&   86.8    &   71.2    &   32.4    &   77.0    &   24.4   &   69.8    &    85.3  &    71.9   &    86.5   &  27.6     &   78.9    &   40.7    &    78.5 &    79.1   &   72.7    &  73.1 &  49.6   &74.8  & 36.1  &  48.1 &   59.2~ &  63.0\\
    CIAN~\cite{fan2018cian}&   88.2    &   79.5    &   32.6    &   75.7    &   56.8   &   72.1    &    85.3  &    72.9   &    81.7   &  27.6     &   73.3    &   39.8    &    76.4 &    77.0   &   74.9    &  66.8 &  46.6   &    81.0  & 29.1  &  60.4 &   53.3~ &  64.3\\
    FickleNet~\cite{lee2019ficklenet} &   89.5    &   76.6    &   32.6    &   74.6    &   51.5   &   71.1    &    83.4  &    74.4   &    83.6   &  24.1     &   73.4    &   47.4    &    78.2 &    74.0   &   68.8    &  73.2 &  47.8   &    79.9  & 37.0  &  57.3 &   64.6~ &  64.9\\
    SSDD~\cite{Shimoda_2019_ICCV} &   89.0    &   62.5    &   28.9    &   83.7    &   52.9   &   59.5    &    77.6  &    73.7   &    87.0   &  34.0     &   83.7    &   47.6    &    84.1 &    77.0   &   73.9    &  69.6 &  29.8   &    84.0  & 43.2  &  68.0 &   53.4~ &  64.9\\
    Lee \textit{et al.}~\cite{lee2019frame} &   90.8    &   82.2    &   35.1    &  82.4    &   72.2   &   71.4    &    82.7  &    75.0   &    86.9   &  18.3     &   74.2    &   29.6    &    81.1&    79.2   &   74.7    &  76.4 &  44.2   &    78.6  & 35.4  &  72.8 &   63.0~ &  66.5\\

    
    BBAM (Ours) &   92.7    &   80.6    &   33.8    &  83.7    &   64.9   &   75.5    &    91.3  &    80.4   &    88.3   &  37.0     &   83.3    &   62.5    &    84.6 &    80.8   &   74.7    &  80.0 &  61.6   &    84.5  & 48.6  &  85.8 &   71.8~ &  73.7\\
    \hline\\label{eq_mask_supp}
\begin{split}
\mathcal{L}_{\mathcal{M}} = &\lambda \left\lVert \mathcal{M} \right\rVert_1 + \lambda_{\text{TV}} \left\lVert  \nabla \mathcal{M} \right\rVert_{\beta}^{\beta} \\ &+ \mathbb{1}_{\text{box}} \left\lVert t^c - f^{\text{box}}(\Phi(I, \mathcal{M}), o) \right\rVert_1 \\ &+ \mathbb{1}_{\text{cls}} \left\lVert p^c - f^{\text{cls}}(\Phi(I, \mathcal{M}), o) \right\rVert_1,
\end{split}
\label{sgd}
\mathcal{M}^{t+1} = \mathcal{M}^{t} - \xi \nabla_{\mathcal{M}^{t}} \mathcal{L}_{\mathcal{M}^{t}},
\vspace{-0.5em}
-0.5em]
  \caption{\label{BBAM_samples_faster} Examples of PASCAL VOC~\cite{everingham2010pascal} validation images with the results of object detection and corresponding BBAMs, obtained from Faster R-CNN~\cite{ren2015faster}.}
\end{figure*} \begin{figure*}[t]
  \centering
  \vspace{-0.2em}
  \includegraphics[width=\linewidth]{latex/Figures/supp_faster_coco_all_illu.pdf} \-0.5em]
  \caption{\label{seg_ex} Examples of predicted semantic masks for PASCAL VOC validation images of DSRG~\cite{huang2018weakly}, Shen \textit{et al.}~\cite{shen2018bootstrapping}, FickleNet~\cite{lee2019ficklenet}, Lee \textit{et al.}~\cite{lee2019frame}, and our method.}
\end{figure*} \begin{figure*}[t]
  \centering
  \vspace{-0.2em}
  \includegraphics[width=\linewidth]{latex/Figures/voc_ins_samples_final_illu.pdf} \-0.5em]
  \caption{\label{ins_seg_coco} Examples of predicted instance masks for MS COCO 2017 validation images of our method.}
\end{figure*} 


\end{document}
