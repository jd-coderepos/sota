\section{Proofs}\label{Appendix A}
    The combination of the following 2 Lemmas is a generalization of the geometric inequality proved by Liang et al. \cite{liang2015learning}. In many respects the scheme of the proof is similar.
    
    \begin{lemma}{(Geometric inequality for the exact  estimator in the second step)}\\
    \label{geom_ineq_lemma-1}
    Let  be -empirical risk minimizers from the first step of the  procedure,  be the exact minimizer from the second step of the  procedure. Then, for  the following inequality holds:
    
    \end{lemma}
\begin{proof}
    For any function  we denote the empirical  distance to be , empirical product to be  and the square of the empirical distance between  and  as
    . By definition of  estimator for some  we have:
        
    where  lies in a convex hull of -empirical risk minimizers .  Denote the balls centered at  to be ,  and . The corresponding spheres will be called . We have  and .
    Denote by  the conic hull of  with origin  and define the spherical cap outside the cone  to be .
    
    First,  and it is a contact point of  and . Indeed,  is necessarily on a line segment between  and a point outside  that does not pass through the interior of  by optimality of . Let  be the set of all contact points of  and  -- potential locations of .
    
    Second, for any , we have  i.e. any  is not in the interior of . Furthermore, let  be bounded subset cone  cut at . Thus  or , where .
    
    For any   consider the two dimensional plane  that passes through three points , depicted in Figure \ref{fig:geom_ineq_picture}. Observe that the left-hand side of the desired inequality \eqref{geom_ineq_ineq-1} is constant as  ranges over . The maximization of  over  is achieved by . 
    Hence, to prove the desired inequality, we can restrict our attention to the plane  and .
    Let  be the projection of  onto the shell .
By the geometry of the cone and triangle inequality we have:
    
    and, hence, .
    By the Pythagorean theorem,
    
    We can now extend this claim to . Indeed, due to the geometry of the projection  and the fact that  or   there are 2 possibilities:
    
    a) . Then ;
    
    b) . Then, since , we have
    
    In both cases, the following inequality is true
        
\end{proof}

\begin{figure}[ht!]
    \centering
    \includegraphics[scale=1]{geom_ineq_picture.png}
    \caption{The cut surface }
    \label{fig:geom_ineq_picture}
\end{figure}  



\begin{lemma}[Geometric Inequality for -empirical minimizers]
    \label{geom_ineq}
    Let  be -empirical risk minimizers from the first step of the  procedure, and  be the -empirical risk minimizer from the second step of the  procedure. Then, for any  and  the following inequality holds:
    
\end{lemma}
\begin{proof}
    Since Lemma \ref{geom_ineq_lemma-1} was actually proven for any , let  be the closest point to  from .      
    For this  the inequality \eqref{geom_ineq_ineq-1} holds. Similarly to Lemma \ref{geom_ineq_lemma-1}, there are 2 options: either , or .
    
    a) Let , then .
    Since  is -empirical risk minimizer, we have  . It means, that .
    
    b) Let , then by the cosine theorem (as depicted on Figure \ref{fig:geom_ineq_picture},   is the two dimensional plane which passes through ): 
    But  and .
    Then we have:
    
    
    Lemma \ref{geom_ineq_lemma-1} states:
    
    By using the triangle inequality and the convexity of the quadratic function,
    we can get the following bound
    
    Combining everything together,  we get the required result for the constant :
    
\end{proof}

    For convenience, we introduce a -excess risk
    
    then the following 2 statements are the direct consequences of the corresponding statements from the article \cite{liang2015learning}. The only difference is that in our case the geometric inequality has terms on the right side with minimization errors . Also our definition of the set  is different, but all that was needed from it was the property that  lies in . For brevity, we will not repeat the proofs, but only indicate the numbers of the corresponding results in the titles of the assertions. We will also proceed for statements the proofs for which we slightly modify or use without changes.

        \begin{corollary}[Corollary 3]
        Conditioned on the data , we have a deterministic upper bound for the  estimator:
        
        
        
    \end{corollary}
    \begin{theorem}[Theorem 4]
    \label{loss_expectation}
        The following expectation bound on excess loss of the  estimator holds:
        
        where  are independent Rademacher random variables, ,  and  almost surely.
    \end{theorem}
    
    \begin{theorem}[Theorem 7]
    \label{loss_probability}
        Assume the lower isometry bound in Definition \ref{isom_bound} holds with  and some  and  is the set defined in \ref{set_H}. Let .
        Define
        
        Then there exist two absolute constants  (which only depend on ), such that
        
        for any
        
        as long as .\\
    \end{theorem}

\begin{lemma}[Lemma 15]
\label{bound_complexity}
    The offset Rademacher complexity for  is bounded as:
    
    and with probability at least 
    
    where  
    
\end{lemma}
\begin{proof}
        Let  be the  -net of the  of size at most 
        and  be the closest point from this net for function , i.e. .
        By using the inequality
        
        we can get next upper bound:
        
    The right summarand is supremum over set of cardinality not more than .
    By using Lemma \ref{finite_bound}, we acquire the expected estimates.
\end{proof}


    We have now obtained, using the offset Rademacher complexity technique, the upper bound on
    excess risk in terms of the coverage size of the set .
    To get the desired result,  
    we need to obtain an upper bound on the size of the cover  in terms of the size of the cover .
\begin{lemma}
    \label{cover_ineq}
        For any scale , the covering number of  (where  is a sphere of radius one in space with norm ) and that of  are bounded in the sense:
        
    \end{lemma}
\begin{proof}
If we define as  the -net
        cardinality no more then ,
        then the following is true:  is -net for  . Hence,
        
    With this we can obtain the following upper bound

    
    But since  is the sum of  functions from  with coefficients in , by the inequaility \eqref{shmidt_norm}, we can cover this with a net of size no more than
    
\end{proof}

    Note that to obtain the required orders, we only need coverage with .
    
\begin{corollary}
\label{bound_H}
Let  defined in \ref{set_H} for , then for  defined in \ref{V} holds

where  is an indepedent constant.
\end{corollary}
\begin{proof}
By lemma \ref{cover_ineq} and inequality \ref{shmidt_cover}, we have


\end{proof}
    We are now fully prepared to prove the two main results.
    
        \begin{theorem}
        \label{premain_th}
        Let  be a  estimator and  be the set defined in \ref{set_H} for . The following expectation bound on excess loss holds:
        
        where  defined in \eqref{K,M} for constants 
        
        


    \end{theorem}
\begin{proof}
    By using Theorem \ref{loss_expectation} and inequality \ref{shmidt_norm} we have
        
        where ,  almost surely.
    
    
By using  Lemma \ref{bound_complexity} and corollary \ref{bound_H} we get desired result
    
\end{proof}

\begin{theorem}
    \label{main_th}
        Let  be a  estimator and let  be the set defined in \ref{set_H} for .
        Assume for  the lower isometry bound in Definition \ref{isom_bound} holds with  and some . Let .
        Define
        
        Then there exist 3 absolute constants  (which only depend on ), such that 
        
        as long as , 
        where 
        
        
        and  is an independent constant.
\end{theorem}

\begin{proof}



    By using Theorem \ref{loss_probability} for any
         we have
        

        as long as .
        
By using Lemmas \ref{bound_complexity} and \ref{bound_H} we have with probability no more than  for any 

    
    where  are defined in \eqref{K,M}.
Combining this inequality for  and ,  we get the required result.
\end{proof}

    \begin{lemma}[Lemma 9]
    \label{finite_bound}
        Let  be a finite set, . Then, for any 
        
        For any :
        
        where
        
    \end{lemma}

\clearpage    
\section{Result Tables}\label{Appendix B}

Here we additionally present tables with the results of numerical experiments. Particularly for runs with a small number of . It can be observed that the SnapStar algorithm is quite good with a strong budget constraint.
The results also include a relatively large run for the FASHION MNIST dataset. At the moment, ClassicStar (new warm-up) takes  place in the \fnurl{leaderboard}{https://paperswithcode.com/sota/image-classification-on-fashion-mnist} for this dataset.
Full versions of the following tables can be found in the \fnurl{repository}{https://github.com/mordiggian174/star-ensembling}.

\begin{table}[ht!]
\centering
\scalebox{1}{
\begin{tabular}{|l|llllll|}
\hline
Name                    & d & MSE          & MAE    &  & TRAIN MSE & TIME (sec) \\
\hline
\hline
Snap Star (shot warm-up)   & 5 & \textbf{10.881±0.575} & \textbf{2.229} & \textbf{0.869}     & 1.976     & 7.8  \\
Snap Star (new warm-up)    & 5 & 11.285±0.650 & 2.283  & 0.864     & 2.656     & 6.6  \\
Snap Ensemble           & 5 & 11.862±0.616 & 2.306   & 0.858     & 2.629     & 6.6  \\
Ensemble                & 5 & 12.568±0.878 & 2.399   & 0.849     & 4.220     & 6.8  \\
Classic Star (no warm-up)  & 5 & 11.365±0.410 & 2.278   & 0.864     & 2.978     & 7.2  \\
Classic Star (new warm-up) & 5 & 12.157±0.822 & 2.353   & 0.854     & 3.320     & 6.2  \\
Big NN                  & 5 & 12.068±0.860 & 2.411   & 0.855     & 3.644     & 4.0  \\
\hline
Snap Star (shot warm-up)   & 4 & \textbf{11.276±0.582} & \textbf{2.269} & \textbf{0.865}     & 2.329     & 6.2  \\
Snap Star (new warm-up)    & 4 & 11.598±0.729 & 2.292   & 0.861     & 2.739     & 5.0  \\
Snap Ensemble           & 4 & 11.819±0.341 & 2.316   & 0.858     & 2.819     & 5.0  \\
Ensemble                & 4 & 12.059±0.614 & 2.365  & 0.855     & 3.732     & 5.0  \\
Classic Star (no warm-up)  & 4 & 11.608±0.722 & 2.286  & 0.861     & 3.198     & 6.2  \\
Classic Star (new warm-up) & 4 & 11.890±0.966 & 2.319   & 0.857     & 3.093     & 5.2  \\
Big NN                  & 4 & 12.556±0.904 & 2.383   & 0.849     & 3.746     & 4.0  \\
\hline
\end{tabular}
}
\caption{BOSTON HOUSE PRICING. Part of results at 30 epochs, , }
\label{table:boston30}
\end{table} \begin{table}[ht!]
\centering
\begin{center}
\begin{tabular}{ |l|llllll|}
\hline
Name & d & MSE & MAE & R2 & TRAIN MSE & TIME (sec)\\
\hline
\hline
Snap Star (shot warm-up) & 5 &  & 5.97 & 0.362 & 70.64 & 733 \\
Snap Star (new warm-up) & 5 &  & 5.99 & 0.363 & 71.34 & 667 \\
Snap Ensemble & 5 &  & 6.02 & 0.361 & 70.03 & 543 \\
Ensemble & 5 &  & 6.05 & 0.361 & 72.05 & 711 \\
Classic Star (no warm-up) & 5 &  & 6.07 & 0.36 & 73.62 & 783 \\
Classic Star (new warm-up) & 5 &  & 6.00 & 0.364 & 72.59 & 807 \\
Big NN & 5 &  & 6.02 & 0.356 & 75.62 & 436 \\
\hline
Snap Star (shot warm-up) & 4 &  & 5.99 & 0.362 & 71.04 & 632 \\
Snap Star (new warm-up) & 4 &  & 6.01 & 0.363 & 71.78 & 565 \\
Snap Ensemble & 4 &  & 6.02 & 0.360 & 70.37 & 452 \\
Ensemble & 4 &  & 6.05 & 0.361 & 72.08 & 593 \\
Classic Star (no warm-up) & 4 &  & 6.04 & 0.36 & 73.76 & 652 \\
Classic Star (new warm-up) & 4 &  & 6.01 & 0.364 & 72.69 & 676 \\
Big NN & 4 &  & 6.03 & 0.355 & 75.63 & 375 \\
\hline
Snap Star (shot warm-up) & 3 &  & 5.98 & 0.361 & 71.62 & 530 \\
Snap Star (new warm-up) & 3 &  & 6.00 & 0.363 & 72.38 & 463 \\
Snap Ensemble & 3 &  & 6.02 & 0.360 & 70.77 & 362 \\
Ensemble & 3 &  & 6.05 & 0.361 & 72.12 & 473 \\
Classic Star (no warm-up) & 3 &  & 6.04 & 0.360 & 74.00 & 522 \\
Classic Star (new warm-up) & 3 &  & 6.01 & 0.363 & 72.81 & 546 \\
Big NN & 3 &  & 6.03 & 0.358 & 75.60 & 315 \\
\hline
\end{tabular}
\end{center}
\caption{MILLIION SONG. Part of results at 10 epochs}
\label{table:MSD10}
\end{table} \begin{table}[ht!]
\centering
\scalebox{1}{
\begin{tabular}{|l|llll|}
\hline
Name                    & d & accuracy    & entropy     & TIME (sec)    \\
\hline
\hline
Snap Star (shot warm-up)   & 3 & \textbf{0.900±0.002} & \textbf{0.284±0.008} & 340.333 \\
Snap Star (new warm-up)    & 3 & 0.898±0.002 & 0.285±0.008 & 313.0   \\
Snap Ensemble           & 3 & 0.897±0.003 & 0.290±0.009 & 272.667 \\
Ensemble                & 3 & 0.887±0.001 & 0.310±0.005 & 272.667 \\
Classic Star (no warm-up)  & 3 & 0.893±0.002 & 0.298±0.007 & 339.667 \\
Classic Star (new warm-up) & 3 & 0.893±0.002 & 0.297±0.007 & 285.667 \\
Big NN                  & 3 & 0.890±0.010 & 0.299±0.022 & 214.333 \\
\hline
Snap Star (shot warm-up)   & 2 & \textbf{0.894±0.007} & \textbf{0.294±0.020} & 248.667 \\
Snap Star (new warm-up)    & 2 & 0.892±0.001 & \textbf{0.294±0.006} & 230.333 \\
Snap Ensemble           & 2 & 0.891±0.006 & 0.302±0.021 & 203.667 \\
Ensemble                & 2 & 0.886±0.004 & 0.313±0.008 & 203.0   \\
Classic Star (no warm-up)  & 2 & 0.889±0.003 & 0.304±0.009 & 249.0   \\
Classic Star (new warm-up) & 2 & 0.889±0.004 & 0.303±0.008 & 203.667 \\
Big NN                  & 2 & 0.892±0.003 & 0.304±0.007 & 165.333 \\
\hline
Snap Star (shot warm-up)   & 1 & \textbf{0.891±0.002} & \textbf{0.299±0.006} & 159.0   \\
Snap Star (new warm-up)    & 1 & 0.885±0.001 & 0.318±0.008 & 149.333 \\
Snap Ensemble           & 1 & 0.889±0.001 & 0.304±0.007 & 136.0   \\
Ensemble                & 1 & 0.886±0.005 & 0.314±0.011 & 136.333 \\
Classic Star (no warm-up)  & 1 & 0.888±0.002 & 0.311±0.001 & 158.0   \\
Classic Star (new warm-up) & 1 & \textbf{0.891±0.002} & 0.302±0.005 & 122.333 \\
Big NN                  & 1 & 0.886±0.002 & 0.315±0.005 & 117.333 \\
\hline
\end{tabular}
}
\caption{FASHION MNIST. Part of results at 5 epochs, }
\label{table:fmnist5}
\end{table} \begin{table}
\centering
\scalebox{1}{
\begin{tabular}{|l|llll|}
\hline
Name                    & d & accuracy    & entropy     & TIME (sec)   \\
\hline
\hline
Snap Star (shot warm-up)   & 5 & 0.898 & 1.152 & 2588.0 \\
Snap Star (new warm-up)    & 5 & 0.898 & 1.136 & 2369.0 \\
Snap Ensemble           & 5 & 0.902 & 0.330 & 2036.0 \\
Ensemble                & 5 & 0.918 & 0.229 & 2052.0 \\
Classic Star (no warm-up)  & 5 & 0.922 & 0.229 & 2589.0 \\
Classic Star (new warm-up) & 5 & \textbf{0.923} & \textbf{0.228} & 2239.0 \\
Big NN                  & 5 & 0.910 & 0.481 & 1560.0\\
\hline
\end{tabular}
}
\caption{FASHION MNIST. All of results at 25 epochs, }
\label{table:fmnist25}
\end{table} 