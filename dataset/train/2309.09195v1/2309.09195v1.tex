\begin{figure*}
    \centering \includegraphics[scale = 0.279]{AccuracyPlotNormalWithOutSideInfo.pdf}
    \caption{Accuracy for different offloading costs () (\our{})}
    \label{fig:Accuracy_res_eesplit}
\end{figure*}

\begin{figure*}
    \centering \includegraphics[scale = 0.279]{CostPlotWithoutSideInfo.pdf}
    \caption{Cost (in  units) for different offloading cost (\our{})}
    \label{fig:cost_res_eesplit}
\end{figure*}
In this section, we first explain how to prepare an Early-Exit DNN and then provide the experimental details.
\subsection{Preparing a Multi-Exit DNN}\label{sec: ElasticBERT}
To evaluate \our{}, we require an early-exit neural network that has already been trained. We use the ElasticBERT-base backbone, which is based on the 12-transformer-layer BERT-base model. For anytime inference, ElasticBert has exits attached after each transformer layer. After attaching exits, the model is trained on a large text corpus with joint masked language modelling (MLM) and sentence order prediction (SOP) loss functions across all exits. After discarding all exits, the ElasticBERT-base model's backbone with learnt weights remains. For more details of the training procedure, we refer to \cite{liu2021elasticbert}.

After having a pre-trained model backbone, we attach task-specific exits (\textit{e.g.} classification heads) after all transformer layers along the backbone and fine-tune it using a labeled dataset (from a similar domain to the evaluation dataset). Sentence-level representations for sentence-level tasks are learned using the [\textit{CLS}] token. After each transformer layer, this token representation is connected to the classification heads. 
A sketch of the entire training procedure is provided in figure \ref{fig:ElasticBERT}.  and  represent token embeddings of the given input sequence. The head is attached to produce a representation that can be compared with the task label to compute the loss. Cross-entropy loss is the loss function that we select. Using learnable weights, the attached classification heads transform the [\textit{CLS}] token's \textit{q}-dimensional vector representation into a probability vector for direct comparison with the task label. 



\begin{table}
\caption{Information about the size of datasets. FT data is the dataset used to prepare the ElasticBERT backbone for the corresponding task and \#Samples is the number of samples in the dataset. E.data is the evaluation dataset.}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{E. Data} & \textbf{\#Samples} & \textbf{FT Data} & \textbf{\#Samples} \\ \hline
IMDb             & 25K                & SST-2            & 68K                \\ \hline
Yelp             & 560K               & SST-2            & 68K                \\ \hline
SciTail          & 24K                & RTE              & 2.5K               \\ \hline
QQP              & 365K               & MRPC             & 4K                 \\ \hline
SNLI             & 550K               & MNLI             & 433K               \\ \hline
\end{tabular}

\label{tab: dataset} 
\end{table}



\subsection{Experimental setup}\label{sec: exp_setup}
In this section, we explain the experimental setup and details of \our{}.
We have three major steps in the experimental setup which are summarized below:

\textbf{i) Training the backbone:} Initially, we train the ElasticBERT-base model with MLM and SOP heads attached after every transformer layer of the BERT-base model. After training on a large text corpus, we remove the MLM and SOP heads from the ElasticBERT model, leaving only the backbone. We directly import weights of the learned backbone, hence this part does not need any computation.

\textbf{ii) Fine-tuning and learning weights (Supervised):} In the backbone obtained by step (i), we attach task-specific exits (heads) after each transformer layer, and to learn weights for these heads, we perform supervised training. Since we assume that we do not have labels for the evaluation task. We utilize a labeled dataset with a similar kind of task but with a different distribution or from a different domain. For example, we evaluate \our{} on IMDb and Yelp datasets which are review classification datasets and learn the weights for heads using the SST-2 dataset which has a similar task of sentiment classification but with different latent data distribution. 

\textbf{iii) Online learning of optimal splitting layer (Unsupervised):} Finally we use weights from step (2) to learn the optimal splitting layer in an unsupervised and online setup for the evaluation tasks. We perform this step after the model has been deployed and ready for inference.

\begin{figure*}
    \centering \includegraphics[scale = 0.279]{AccuracyPlotNormalWithSideInfo.pdf}
    \caption{Accuracy for different offloading costs () (\our{}-S)}
    \label{fig:Accuracy_res_EESPlit s}
\end{figure*}

\begin{figure*}
    \centering \includegraphics[scale = 0.279]{CostPlotWithSideInfo.pdf}
    \caption{Cost (in  units) for different offloading cost (\our{}-S)}
    \label{fig:cost_res_EESPLIT-S}
\end{figure*} 

We perform experiments on single NVIDIA RTX 2070 GPU. Part (i) does not require any computation as we can directly import the weights from the backbone. Part (ii) takes a maximum of 10 GPU hour of runtime (on the MNLI dataset). Part (iii) is not computationally involved and could be executed in  hour of CPU runtime and does not requires GPU support on a single run.

We evaluated \our{} on five datasets covering three types of classification tasks. The datasets used for evaluation are:
\begin{enumerate}
    \item \textbf{Review classification on IMDb \cite{maas2011learning} and Yelp \cite{asghar2016yelp}:} IMDb is a movie review classification dataset and Yelp consists of reviews from various domains such as hotels, restaurants etc. For these two datasets, ElasticBERT is finetuned on \textbf{SST-2 (Stanford Sentiment classification)} dataset which is also a sentiment classification dataset.
    \item \textbf{Entailment classification on SciTail:} SciTail is an entailment classification dataset created from multiple questions from science and exams and web sentences. To evaluate SplitEE on SciTail, it is finetuned on \textbf{RTE(Recognizing Textual Entailment)} dataset which is also an entailment classification dataset.
    \item \textbf{Entailment classification on SNLI(Stanford Natural Language Inference) (Multi-class):} SNLI is a collection of human-written English sentence pairs manually labelled for balanced classification with labels \textit{entailment}, \textit{contradiction} and \textit{neutral}. For evaluation of this dataset, ElasticBERT is finetuned on \textbf{MNLI(Multi-Genre Natural Language Inference)} which also contains sentence pairs as premise and hypothesis, the task is the same as for SNLI.

    \item \textbf{Semantic equivalence classification on QQP(Quora Question Pairs)}: QQP is a semantic equivalence classification dataset which contains question pairs from the community question-answering website Quora. For this task, we finetuned ElasticBERT on \textbf{MRPC(Microsoft Research Paraphrase Corpus)} dataset which also has a semantic equivalence task of a sentence pair extracted from online news sources.
\end{enumerate}
  Details about the size of these datasets are in table \ref{tab: dataset}. Observe from the table that the size of the dataset used for fine-tuning is much smaller as compared to the size of the corresponding evaluation dataset. We do not split the evaluation dataset. Except for IMDb and Yelp, other datasets are a part of ELUE \cite{liu2021elasticbert} and GLUE \cite{wang2019glue} benchmark datasets.




We attach exits after every transformer layer in the ElasticBERT model. The predefined threshold  is directly taken from the ElasticBERT model which utilizes the validation split of fine-tuning data to learn the best threshold. The choice of action set depends on the number of layers of the DNN being used. The action set is  and for ElasticBERT . 
We have two types of costs: Computational cost and Offloading cost. As explained in section \ref{sec:problem setup}, the computational cost is proportional to the number of layers processed i.e.  where  could be interpreted as per-layer computational cost. We can split , where  and  resemble the per-layer processing cost and per-layer inference cost respectively. We relate  and  in terms of the number of matrix multiplications required to process and infer. We observe that  (5 matrix multiplications are needed for processing and 1 for inferring). Hence the cost for SplitEE-S will be  and  for SplitEE if th layer is chosen as the splitting layer.
Since offloading cost is also user-defined and depends on the communication network used (e.g. 3G, 4G, 5G and Wi-Fi). Hence in the experiments, we provide results on different offloading costs  from the set  as it is user-defined. With increasing stages of broadband mobile communication powers, we observe that offloading cost is at most five times the per-layer computational cost. For more details on how to compute the offloading cost, we refer to \cite{kuang2019partial}. For table \ref{tab: Results}, we use the fixed offloading cost as  (highest offloading cost). Without loss of generality, we choose  for conducting all the experiments. The cost accumulated is however left in terms of  which is user-specific as all the cost is now in terms of .




The trade-off factor  ensures that the reward function gives similar preferences to cost as well as accuracy. For the algorithm, we choose  to directly compare confidence and cost.
We repeat each experiment  times and in each run the samples are randomly reshuffled and then fed to the algorithm in an online manner. In each round, the algorithm chooses a splitting layer and accumulates the regret if the choice is not optimal. We plot the expected cumulative regret in figure \ref{fig:Regret_curve_1}. The accuracy and cost reported for \our{} and \our{}-S is computed considering the chosen splitting layer prediction in each round (i.e. for every sample) and then per-sample averaged for  runs.



\begin{table*}[]
\caption{Main Results: Results on different baselines across different datasets. Cost is left in terms of  units.  is user-defined. The offloading cost is taken 5 (worst-case).}
\label{tab: Results}
\begin{tabular}{ccccccccccc}
\hline
Model/Data  & \multicolumn{2}{c}{IMDb}         & \multicolumn{2}{c}{Yelp} & \multicolumn{2}{c}{SciTail} & \multicolumn{2}{c}{SNLI} & \multicolumn{2}{c}{QQP} \\ \hline
            & Acc           & Cost             & Acc            & Cost    & Acc             & Cost      & Acc            & Cost    & Acc           & Cost    \\ \hline
Final-exit  & 83.4          & 30.0             & 77.8           & 161.0   & 78.9            & 28.3      & 80.2           & 659.2   & 71.0          & 436.6   \\
Random-exit & -1.4          & -31.3\%          & -1.2  & -38.0\% & -0.7            & -31.8\%   & -2.0           & -41.5\% & -0.1          & -14.8\% \\
DeeBERT     & -6.1          & -43.3\%          & -2.5           & -59.0\% & -3.6            & -5.3\%    & -3.5           & -38.9\% & -6.7          & -50.1\% \\
ElasticBERT & -2.5          & -62.3\%          & -2.1           & -62.1\% & -0.1            & -40.2\%   & -2.7           & -61.4\% & -0.2          & -57.9\% \\
\our{} & -1.3 & \textbf{-66.6}\% & -1.1 & \textbf{-68.3}\% & 0.0 & -49.2\% & -1.6 & \textbf{-65.8}\% & -0.1 & \textbf{-59.1}\%\\
\our{}-S  & \textbf{-1.2} & -64.3\% & \textbf{-1.1}           & -65.2\% & \textbf{0.0}    & \textbf{-50.5\%}   & \textbf{-1.7}  & -62.5\% & \textbf{+0.1}  & -55.1\% \\ \hline
\end{tabular}
\end{table*}


\subsection{Baselines}
    \textbf{1) DeeBERT:} Similar to our setup, we fine-tune DeeBERT and then perform inference on the evaluation dataset.
    DeeBERT prepares the early exit model in two steps: (1) It learns the general weights and embeddings for the BERT backbone using the loss function attached only at the final layer, this part is similar to BERT fine-tuning. (2) After freezing the weights, it attaches a loss function after every transformer layer except the final layer. Note that DeeBERT does not have the option to offload. DeeBERT uses the entropy of the predicted vector as confidence. We fine-tune the entropy threshold in a similar fashion as used by DeeBERT. Since it does not make any difference, hence we keep the confidence of DeeBERT as the entropy of the predicted vector. Other parameters are kept the same as used by DeeBERT.


    
\textbf{2) ElasticBERT:} is also based on the BERT-base model, the only difference is ElasticBERT is jointly trained by attaching MLM and SOP heads after every transformer layer, Once the model is trained, it removes the heads leaving the backbone. More details are in section \ref{sec: ElasticBERT} and figure \ref{fig:ElasticBERT}. All the parameters are kept the same as the ElasticBERT setup.

\textbf{3) Random selection:} In random selection, we select a random exit point and then process the sample till chosen exit, if the confidence at chosen exit is above the threshold then exit, else offload. Then we calculate the cost and accuracy. We report the average accuracy and cost by running the above procedure 20 times.

\textbf{4) Final exit:} In this case, we process the sample till the final layer for inference. This setup has a constant cost of . This baseline is similar to the basic inference of neural networks. We also utilize this setup for benchmarking.

\vspace{-0.5cm}
\subsection{Need for offloading}
As explained in section \ref{sec: exp_setup}, the maximum possible offloading cost is -times the per-layer computational cost. Hence if a sample is not gaining sufficient confidence for classification till a pre-specified layer, we might want to offload it to the cloud. Previous methods process the sample throughout the DNN until it gains sufficient confidence. We observed that processing a sample beyond th layer was accumulating more processing cost than the offloading cost. While experimenting, we marked that on average DeeBERT processes  samples and ElasticBERT processes  samples beyond th exit layer. These many samples accumulate a large computational cost for edge devices. Since edge devices have fewer resources available, both DeeBERT and ElasticBERT might exploit these resources in terms of battery lifetime depletion and device lifetime.



While our setup decides on a splitting layer as a sample arrives. The sample is processed till chosen layer and if the sample gains sufficient confidence it exits the DNN else it offloads to the cloud for inference reducing cost drastically. Additionally, offloading helps in increasing accuracy as the last layer provides more accurate results on samples that were not gaining confidence initially. 



\begin{figure*}
    \centering
    \includegraphics[scale = 0.279]{RegretPlots.pdf}
    \caption{Regret for different models}
    \label{fig:Regret_curve_1}
\end{figure*}




\subsection{\our{} \ and \our{}-S}
From figures 
\ref{fig:Accuracy_res_eesplit},
\ref{fig:cost_res_eesplit}, \ref{fig:Accuracy_res_EESPlit s} and \ref{fig:cost_res_EESPLIT-S}, we observe that \our{} and \our{}-S have comparable performances. However, observe that \our{} does not utilize confidences of exits prior to chosen splitting layer. Hence we can directly process the sample to the splitting layer reducing the inference cost after each exit. \our{}-S uses the confidence available at all exits on the edge device to update rewards for multiple arms. The difference between the two is more evident in the regret curve (see fig.\ref{fig:Regret_curve_1}). \our-S curve saturates much earlier than \our{}. We also observed from the experiments that \our{} has a larger impact on the reshuffling of the dataset while \our{}-S is more robust to the reshuffling of the dataset as it needs less number of samples to learn the optimal splitting layer. Since in a real-world scenario, the size of the evaluation dataset might be small and we might need to adapt to changes in the distribution of data fast, in this case, SplitEE-S can be used. However, if the major concern is cost then \our{} will work better as it further reduces the inference cost (see table \ref{tab: Results}). Note that DeeBERT and ElasticBERT also incur the inference cost at all exits up to which a sample is processed.

\subsection{Analysis with different offloading cost}\label{sec:offloading}
Being user-defined, we analyse the behaviour of accuracy and cost on different offloading costs. Except for the QQP dataset, we observe a drop in accuracy as we increase the offloading cost. We explain the drop as more offloading cost will force more samples to exit early by choosing a deeper exit in DNN. More samples exiting early will make less accurate predictions.
Hence initially, when offloading cost is small it exits most samples from initial exits and offloads very few samples. When we increase offloading cost again it just goes deeper to gain confidence for those samples which were offloaded earlier. In terms of cost, it is evident that the cost of \our{} will go up as we increase the offloading cost. For the QQP dataset, we observe a reverse behaviour as there are very less samples that offload for QQP. We observed that there were many samples that exited the initial layers with miss-classifications (which is also an explanation for the lower cost of ElasticBERT).  As we increase the offloading cost \our{} looks for deeper exit layers to split hence a gain in accuracy.
Still, we are always better in terms of cost as well as accuracy when compared to ElasticBERT as shown in figure \ref{fig:cost_res_eesplit}, \ref{fig:cost_res_EESPLIT-S}, \ref{fig:Accuracy_res_eesplit} and \ref{fig:Accuracy_res_EESPlit s}. Detailed results are in table \ref{tab: Results}.









\subsection{Regret Performance}\label{sec: regret}
We repeat each experiment -times. Each time, a randomly reshuffled data is fed in an online manner to the algorithm. 
In each step, the algorithm selects a splitting layer and accumulates the regret if the choice is not optimal. In figure \ref{fig:Regret_curve_1}, we plot the expected cumulative regret along with a 95\% confidence interval. We choose the exploration parameter . While each plot shows the results for a specific dataset. \our{} and \our{}-S outperforms the considered alternatives, yielding a lower cumulative regret and achieving sub-linear regret growth. We also observe that the \our{}-S achieves lower regret than the \our{}. This is because the side information provides the algorithm with additional information about the environment, which can be used to learn the optimal splitting layer quickly. As a result, the algorithm with side information can converge to the optimal policy more quickly. As observed from the figure \ref{fig:Regret_curve_1}, the regret starts saturating after the first 2000 samples for \our{} and after 1000 samples for \our{}-S.
