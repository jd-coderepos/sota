\def\withcomments{1}
\def\full{1}
\documentclass[11pt,english]{article}
\usepackage{babel,comment}
\usepackage{amsmath, amsthm, amssymb, url}
\usepackage[numbers,longnamesfirst]{natbib}
\usepackage{verbatim}
\usepackage{cleveref,paralist}
\usepackage[usenames,dvipsnames]{color} \usepackage{multirow}
\usepackage{bigstrut}
\usepackage[disable]{todonotes}
\usepackage{fullpage, bbm}


\usepackage[ruled, noend, noline]{algorithm2e}

\ifnum\full=0
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{1ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}
\renewenvironment{itemize}[1]{\begin{compactitem}#1}{\end{compactitem}}
\renewenvironment{enumerate}[1]{\begin{compactenum}#1}{\end{compactenum}}
\fi

\newtheorem{openproblem}{Open Problem}
\newtheorem{question}{Question}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{sublemma}[theorem]{SubLemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{myclaim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{observation}{Observation}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}{Definition}[section]
\newtheorem{defn}[definition]{Definition}

\newtheorem{protocol}{Protocol}

\numberwithin{figure}{section}


\newcommand{\cD}{D} \newcommand{\D}{{\mathcal D}} \newcommand{\cR}{{\mathcal R}}
\newcommand{\cM}{{\mathcal M}}
\newcommand{\cP}{{\mathcal P}}
\newcommand{\cL}{{\mathcal L}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\C}{{\mathcal C}}
\newcommand{\eps}{{\epsilon}}
\newcommand{\ie}{{\textit i.e. }}
\newcommand{\AND}{\mbox{ and }}
\newcommand{\es}{\varnothing}
\newcommand{\MNM}{\mbox{\sc Mnm}_1}
\newcommand{\dis}{dist}
\newcommand{\Dis}{Dist}
\newcommand{\zeromset}{\{0,1,\dots,m-1\}}
\newcommand{\depth}{t}
\newcommand{\quest}{\text{\sf ?}\xspace}
\newcommand{\onevec}{{\mathbf 1}_d}
\newcommand{\kvec}{{\mathbf k}}
\newcommand{\ste}{{\epsilon\emph{-tester}}}
\newcommand{\tte}{{(\epsilon_{1},\epsilon_{2})\emph{-tester}}}
\newcommand{\de}{{\mydelta\emph{-additive-estimator}}}

\newcommand{\Accept}{\textbf{Accept}\xspace}
\newcommand{\accept}{\textbf{accept}\xspace}
\newcommand{\Reject}{\textbf{Reject}\xspace}
\newcommand{\reject}{\textbf{reject}\xspace}

\newcommand{\integerset}[1]{[0..{#1})}
\newcommand{\domain}{\integerset{n}^2}
\newcommand{\gp}{\text{GP}}
\newcommand{\side}{r}
\newcommand{\rblock}{-block\xspace}
\newcommand{\rblocks}{-blocks\xspace}
\newcommand{\blockxy}{\ensuremath{B_\side(x,y)}\xspace}
\newcommand{\lind}{t} 

\newcommand{\UH}{\text{UH}}
\newcommand{\LH}{\text{LH}}
\newcommand{\hull}{\text{Hull}}

\newcommand{\Best}{{\sf Best}\xspace}
\newcommand{\Stv}{{\sf StValid}\xspace}
\newcommand{\Compst}{{\sf ComputeStatus}\xspace}
\newcommand{\Constgr}{{\sf ConstructGraph}\xspace}

\newcommand{\mindist}{{\sf MinDist}\xspace}
\newcommand{\BestFixed}{{\sf Best For Fixed Base}\xspace}
\newcommand{\errle}{\dout_{\text{left}}}
\newcommand{\errri}{\dout_{\text{right}}}
\newcommand{\Tend}{{\bf T}_{\text{end}}}
\newcommand{\Tfin}{{\bf T}_{\text{fin}}}
\newcommand{\Tcut}{{\bf T}_{\text{cut}}}

\newcommand{\Tstart}{{\bf T}_0}
\newcommand{\stripset}{{\bf R}}
\newcommand{\triset}{{\bf T}}
\newcommand{\qset}{{\bf Q}}
\newcommand{\err}{\mbox{\it err}_S}
\newcommand{\con}{144}
\newcommand{\bm}{B_M}
\newcommand{\bmp}{B_{M'}}
\newcommand{\hp}[2]{H^{#1}_{#2}}\newcommand{\sepline}[2]{L^{#1}_{#2}}\newcommand{\hpi}[2]{M^{#1}_{#2}}\newcommand{\myerr}[1]{Err(#1)}

\renewcommand{\arraystretch}{2}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\avg}{\mathbb E}
\DeclareMathOperator*{\E}{\mathbb E}
\DeclareMathOperator*{\Var}{\mathrm Var}


\newcommand{\mydelta}{\epsilon} \newcommand{\bigdelta}{{\epsilon_0}} \newcommand{\dsquares}{d_{\rm squares}}
\newcommand{\dhatsquares}{\hat{d}_{\rm squares}}
\newcommand{\dout}{\hat{d}}

\newenvironment{myproof}{\begin{proof}}{
\end{proof}}
\newenvironment{proofof}[1]{\begin{myproof}[\ifnum\llncs=0 Proof \fi of {#1}]}{\end{myproof}}


\ifnum\withcomments=1
   \newcommand{\mnote}[1]{{\color{red}\footnote{{\color{red} {\bf M:} #1}}}}
   \newcommand{\pnote}[1]{{\color{green}\footnote{{\color{green} {\bf P:} #1}}}}
   \newcommand{\snote}[1]{{\color{black}\footnote{{\color{black} {\bf S:} #1}}}}
\else
   \newcommand{\mnote}[1]{}
   \newcommand{\pnote}[1]{}
   \newcommand{\snote}[1]{}
\fi

\newcommand{\mch}[1]{{\color{black}#1}}

\date{}
\ifnum\full=1
\title{Tolerant Testers of Image Properties\footnote{A preliminary version of this article was published in the proceedings of the 43rd International Colloquium on Automata, Languages, and Programming, ICALP, 2016~\cite{BMR16icalp}}}
\else
\title{Tolerant Testers of Image Properties}
\fi
\author{Piotr Berman\thanks{Pennsylvania State University, USA; {\tt berman@cse.psu.edu}.}\\
\and Meiram Murzabulatov\thanks{Pennsylvania State University, USA; {\tt meyram85@yahoo.co.uk}. This author was supported by NSF CAREER award CCF-0845701 and NSF award CCF-1422975.
}\\
\and Sofya Raskhodnikova\thanks{Pennsylvania State University, USA; {\tt sofya@cse.psu.edu}. This author was supported by NSF CAREER award CCF-0845701, NSF award CCF-1422975, and by Boston University's Hariri Institute for Computing and Center for Reliable Information Systems and Cyber Security and, while visiting the Harvard Center for Research on Computation \& Society, by a Simons Investigator grant to Salil Vadhan.
}
}
\begin{document}
\raggedbottom
\setlength{\parskip}{0pt}
\maketitle
\begin{abstract}
We initiate a systematic study of tolerant testers of image properties or, equivalently, algorithms that approximate the distance from a given image to the desired property (that is, the smallest fraction of pixels that need to change in the image to ensure that the image satisfies the desired property). Image processing is a particularly compelling area of applications for sublinear-time algorithms and, specifically, property testing. However, for testing algorithms to reach their full potential in image processing, they have to be tolerant, which allows them to be resilient to noise. Prior to this work, only one tolerant testing algorithm for an image property (image partitioning) has been published.

We design efficient approximation algorithms for the following fundamental questions: What fraction of pixels have to be changed in an image so that it becomes a half-plane? a representation of a convex object? a representation of a connected object? More precisely, our algorithms approximate the distance to three basic properties (being a half-plane, convexity, and connectedness) within a small additive error , after reading a number of pixels polynomial in  and independent of the size of the image. The running time of the testers for half-plane and convexity is also polynomial in .
Tolerant testers for these three properties were not investigated previously. For convexity and connectedness, even the existence of distance approximation algorithms with query complexity independent of the input size is not implied by previous work. (It does not follow from the VC-dimension bounds, since VC dimension of convexity and connectedness, even in two dimensions, depends on the input size. It also does not follow from the existence of non-tolerant testers.)

Our algorithms require very simple access to the input: uniform random samples for the half-plane property and convexity, and samples from uniformly random blocks for connectedness. However, the analysis of the algorithms, especially for convexity, requires many geometric and combinatorial insights. For example, in the analysis of the algorithm for convexity, we define a set of reference polygons   such that (1) every convex image has a nearby polygon in  and (2) one can use dynamic programming to quickly compute the smallest empirical distance to a polygon in . This construction might be of independent interest.

\end{abstract}
\section{Introduction}\label{sec:intro}
Image processing is a particularly compelling area of applications for sublinear-time algorithms and, specifically, property testing. Images are huge objects, and our visual system manages to process them very quickly without examining every part of the image. Moreover, many applications in image analysis have to process a large number of images online, looking for an image that satisfies a certain property among images that are generally very far from satisfying it. Or, alternatively, they look for a subimage satisfying a certain property in a large image (e.g., a face in an image where most regions are part of the background.) There is a growing number of proposed {\em rejection-based} algorithms that employ a quick test that is likely to reject a large number of unsuitable images (see, e.g., citations in \cite{KleinerKNB11}).

Property testing~\cite{RS96,GGR98} is a formal study of fast algorithms that accept objects with a given property and reject objects that are far. Testing image properties in this framework was first considered in \cite{Ras03}. Ron and Tsur~\cite{TsurR10} initiated property testing of images with a different input representation, suitable for testing properties of sparse images. Since these models were proposed, several sublinear-time algorithms for visual properties were implemented and used: namely, those by Kleiner et al.\ and Korman et al.\ \cite{KleinerKNB11,KormanRT,KormanRTA13}.

However, for sublinear-time algorithms to reach their full potential in image processing, they have to be resilient to noise: images are often noisy, and it is undesirable to reject images that differ only on a small fraction of pixels from an image satisfying the desired property. Tolerant testing was introduced by Parnas, Ron and Rubinfeld~\cite{PRR06}  exactly with this goal in mind---to deal with noisy objects. It builds on the property testing model and calls for algorithms that accept objects that are close to having a desired property and reject objects that are far. Another related task is approximating distance of a given object to a nearest object with the property within additive error~. (Distance approximation algorithms imply tolerant testers in a straightforward
\ifnum\full=1
way: see the remark after Definition~\ref{def:distance-approx-alg}).
\else
way.)
\fi
The only image problem for which tolerant testers were studied is the image partitioning problem investigated by Kleiner et al.~\cite{KleinerKNB11}.
\subparagraph{Our results.}
We design efficient approximation algorithms for the following fundamental questions: What fraction of pixels have to be changed in an image so that it becomes a half-plane? a representation of a convex object? a representation of a connected object?
In other words, we design algorithms that approximate the distance to being a half-plane, convexity and connectedness within a small additive error or, equivalently, tolerant testers for these properties. These problems were not investigated previously in the tolerant testing framework.
For all three properties, we give -additive distance approximation algorithms that run in constant time (i.e., dependent only on , but not the size of the image). We remark that even though it was known that these properties can be tested in constant time \cite{Ras03}, this fact does not necessarily imply constant-query tolerant testers for these properties. E.g., Fischer and Fortnow~\cite{FischerF06} exhibit a property (of objects representable with strings of length ) which is testable with a constant number of queries, but for which every tolerant tester requires  queries.
For convexity and connectedness, even the existence of distance approximation algorithms with query (or time) complexity independent of the input size does not follow from previous work. It does not follow from the VC-dimension bounds, since VC dimension of convexity and connectedness, even in two dimensions, depends on the input size\footnote{For  images, the VC dimension of convexity is  (this is the maximum number of vertices of a convex lattice polygon in an  lattice \cite{Barany00}); for connectedness, it is .}. Implications of the VC dimension bound on convexity are further discussed below.

Our results on distance approximation are summarized  in Table~\ref{table:all-results}. Our algorithm for convexity is the most important and technically difficult of our results, requiring a large number of new ideas to get running time polynomial in  To achieve this, we define a set of reference polygons   such that (1) every convex image has a nearby polygon in  and (2) one can use dynamic programming to quickly compute the smallest empirical distance to a polygon in . It turns out that the empirical error of our algorithm is proportional to the sum of the square roots of the areas of the regions it considers in the dynamic program. To guarantee (2) and keep our empirical error small, our construction ensures that the sum of the square roots of the areas of the considered regions is small.
This construction might be of independent interest.
\begin{table*}[t]
\begin{center}
\begin{tabular}{| l || c | c| c |}
\hline
\multicolumn{1}{|c||}{Property}& Sample Complexity &  Run Time & Access to Input
\\
\hline
Half-plane  &   &  & uniformly random pixels
\\
Convexity &   &  & uniformly random pixels
\\
Connectedness &   \ \ \ \ &  &uniformly random blocks of pixels
\\
\hline
\end{tabular}
\end{center}
\caption{Our results on distance approximation.
To get complexity of -tolerant testing, substitute .
}
\label{table:all-results}
\end{table*}
Our algorithms do not need sophisticated access to the input image: uniformly randomly sampled pixels suffice for our algorithms for the half-plane property and convexity. For connectedness, we allow our algorithms to query pixels from a uniformly random block. (See the end of Section~\ref{sec:defintions_notation} for a formal specification of the input access.)


Our algorithms for convexity and half-plane work by first implicitly learning the object\footnote{\label{fn:connection-to-learning}There is a known implication from learning to testing. As proved in \cite{GGR98}, a proper PAC learning algorithm for property 
with sampling complexity  implies a 2-sided error (uniform) property tester for  that takes  samples. There is an analogous implication from proper agnostic PAC learning to distance approximation with an overhead of  instead of . We choose to present our testers first and get learners as corollary because our focus is on testing and because we want additional features for our testers, such as 1-sided error, that do not automatically follow from the generic relationship.}.
\ifnum\full=1
PAC learning was defined by Valiant~\cite{Valiant84}, and agnostic learning, by Kearns et al.~\cite{KearnsSS94} and Haussler~\cite{Haussler92}.
\fi
 As a corollary of our analysis, we obtain fast proper agnostic PAC learners of half-planes and of convex sets in two dimensions that work under the uniform distribution.
The sample and time complexity\footnote{All our results are stated for error probability . To get results for general , by standard arguments, it is enough to multiply the complexity of an algorithm by .} of the PAC learners is as indicated in Table~\ref{table:all-results} for distance approximation algorithms for corresponding properties.



While the sample complexity of our agnostic half-plane learner (and hence our distance approximation algorithm for half-planes) follows from the VC dimension bounds, its running time does not. Agnostically learning half-spaces under the uniform distribution has been studied by \cite{KalaiKMS08}, but only for the hypercube  domains, not the plane.
Our PAC learner of convex sets, in contrast to our half-plane learner, \ dimension lower bounds on sample complexity. (The sample complexity of a PAC learner for a class
is at least proportional to the VC dimension of that class \cite{EHKV89}.) Since VC dimension of convexity of  images is
,
proper PAC learners of convex sets in two dimensions (that work under arbitrary distributions) must have sample complexity . However, one can do much better with respect to the uniform distribution.
Schmeltz~\cite{Sch92} showed that a non-agnostic learner for that task needs
 samples.
Surprisingly, it appears that this question has not been studied at all for agnostic learners. Our agnostic learner for convex sets in
\ifnum\full=1
two dimensions
\else
2D
\fi
under the uniform distribution needs
 samples and runs in time .

Finally, we note that for connectedness, we take a different approach. Our algorithms do not try to learn the object first; instead they rely on a combinatorial characterization of distance to connectedness. We show that distance to connectedness can be represented as an average of distances of sub-images to a related property.
\subparagraph{Comparison to other related work.}
Property testing has rich literature on graphs and functions, however, properties of images have been investigated very little. Even though superficially the inputs to various types of testing tasks might look similar, the problems that arise are different.
In the line of work on testing dense graphs, started by Goldreich et al.~\cite{GGR98}, the input is also an  binary matrix, but it represents an adjacency matrix of the dense input graph. So, the problems considered are different than in this work.
In the line of work on testing geometric properties, started by Czumaj, Sohler, and Ziegler~\cite{CzumajSZ00} and Czumaj and Sohler~\cite{CS01}, the input is a set of points represented by their coordinates. The allowed queries and the distance measure on the input space are different from ours. 


A line of work potentially relevant for understanding connectedness of images is on connectedness of bounded-degree graphs. Goldreich and Ron~\cite{GR02}  gave a tester for this property, subsequently improved by Berman et al.~\cite{BermanRY14}. Campagna et al.~\cite{CampagnaGR13} gave a tolerant tester for this problem. Even though we view our image as a graph in order to define connectedness of images, there is a significant difference in how distances between instances are measured (see~\cite{Ras03} for details). We also note, that unlike in~\cite{CampagnaGR13}, our tolerant tester for connectedness is fully tolerant, i.e., it works for all settings of parameters.


The only previously known tolerant tester for image properties was given by Kleiner et al.~\cite{KleinerKNB11}. They consider the following class of image partitioning problems, each specified by a  binary template matrix  for a small {\em constant} . The image satisfies the property corresponding to  if it can be partitioned by  horizontal and  vertical lines into blocks, where each block has the same color as the corresponding entry of . Kleiner et al.\ prove that  samples suffice for tolerant testing of image partitioning properties. Note that VC dimension of such a property is , so by Footnote~\ref{fn:connection-to-learning}, we can get a  bound. Our algorithms required numerous new ideas to significantly beat VC dimension bounds (for convexity and connectedness) and to get low running time.

For the properties we study, distance approximation algorithms and tolerant testers were not investigated previously. In the standard property testing model, the half-plane property can be tested in  time~\cite{Ras03}, convexity can be tested in  time~\cite{BMR16socg}, and connectedness can be tested in  time~\cite{Ras03,BermanRY14}. As we explained, property testers with running time independent of  do not necessarily imply tolerant testers with that feature.
Many new ideas are needed to obtain our tolerant testers. In particular, the standard testers for half-plane and connectedness are adaptive while the testers here need only random samples from the image, so the techniques used for analyzing them are different. The tester for convexity in~\cite{BMR16socg} uses only random samples, but it is not based on dynamic programming.




\subparagraph{Open questions.}
In this paper we give tolerant testers for several important problems on images.
It is open whether these testers are optimal. No nontrivial lower bounds are known for these problems. (For any non-trivial property, an easy lower bound on the query complexity of a distance approximation algorithm is . This follows from the fact that  coin flips are needed to distinguish between a fair coin and a coin that lands heads with probability .) Thus, our testers for half-plane and convexity are nearly optimal in terms of query complexity (up to a logorithmic factor in ). But it is open whether their running time can be improved.


\ifnum\full=1
\subparagraph{Organization.} We give formal definitions and notation in Section~\ref{sec:defintions_notation}. Algorithms for being a half-plane, convexity, and connectedness are given in Sections~\ref{sec:half-plane},~\ref{sec:convexity}, and~\ref{sec:connectedness}, respectively. The sections presenting algorithms for being a half-plane and convexity start by giving a distance approximation algorithm and conclude with the corollary about the corresponding PAC learner.
\else
\subparagraph{Organization.} We give formal definitions and notation in Section~\ref{sec:defintions_notation}, deferring some standard definitions to the {\color{black} full version}. Algorithms for being a half-plane, convexity, and connectedness are given in Sections~\ref{sec:half-plane},~\ref{sec:convexity}, and~\ref{sec:connectedness}, respectively.  We view our half-plane result as a good preparation for our distance approximation algorithm for convexity, the most technically difficult result in the paper. Corollaries about PAC learners as well as all omitted proofs and numerous figures can be found in the {\color{black} full version}.
\fi

\section{Definitions and Notation}\label{sec:defintions_notation}
We use  to denote the set of integers  and  to denote .
\ifnum\full=1
By  we mean the logarithm base 2, and by , the logarithm base .
\fi

\subparagraph{Image representation.}
We focus on black and white images. For simplicity, we only consider square images, but everything in this paper can be easily generalized to rectangular images.
We represent an image by an  binary matrix  of pixel values, where 0 denotes white and 1 denotes black.
We index the matrix by . The object is a subset of 
corresponding to black pixels; namely, .
\ifnum\full=1
The {\em left border of the image} is the set . The right, top and bottom borders are defined analogously. The image {\em border} is the set of pixels on all four borders.

For any region , we use  to denote its area.
\fi

\ifnum\full=0
The {\em absolute distance}, , between matrices  and  is the number of the entries on which they differ. The \emph{relative distance} between them is .
A property  is a subset of binary matrices.
\else
\subparagraph{Distance to a property.} The {\em absolute distance}, , between matrices  and  is the number of the entries on which they differ. The \emph{relative distance} between them is .
A property  is a subset of binary matrices. The distance of an image represented by matrix  to a property  is  . An image is {\em -far} from the property if its distance to the property is at least ; otherwise, it is -close to it.

\subparagraph{Computational Tasks.}
We consider several computational tasks: tolerant testing~\cite{PRR06}, additive approximation of the distance to the property, and proper (agnostic) PAC learning~\cite{Valiant84,KearnsSS94,Haussler92}.
\ifnum\full=1
Here we define them specifically for properties of images.
\fi
\begin{definition}[Tolerant tester]\label{def:tester}
 An {\em -tolerant tester} for a property  is a randomized algorithm that, given two parameters  such that  and access to an  binary matrix ,
\ifnum\full=0
(1) accepts with probability at least 2/3 if ;
(2) rejects with probability at least 2/3 if .
\else
\begin{enumerate}
\item accepts with probability at least 2/3 if ;
\item rejects with probability at least 2/3 if .
\end{enumerate}
\fi

\end{definition}



\begin{definition}[Distance approximation algorithm]\label{def:distance-approx-alg}
An {\em -additive distance approximation algorithm} for a property  is a randomized algorithm that, given an error parameter  and access to an  binary matrix  outputs a value  that with probability at least 2/3 satisfies .
\end{definition}

As observed in \cite{PRR06}, we can obtain an -tolerant tester for any property  by running a distance approximation algorithm for  with . Thus, all our distance approximation algorithms directly imply tolerant testers.

\begin{definition}[Proper agnostic PAC learner]

A proper agnostic PAC learning algorithm for class  that works under the uniform distribution is given a parameter  and access to an image . It can draw independent uniformly random samples   and obtain  and . With probability at least 2/3, it must output an image  such that .
\end{definition}
\fi
\subparagraph{Access to the input.}
A {\em query-based} algorithm accesses its  input matrix  by specifying a query pixel  and obtaining .
\ifnum\full=1
The query complexity of the algorithm is the number of pixels it queries.
A {\em query-based} algorithm is {\em adaptive} if its queries depend on answers to previous queries and {\em nonadaptive} otherwise.
\fi
A {\em uniform} algorithm accesses its  input matrix by drawing independent samples  from the uniform distribution over the domain (i.e., ) and obtaining .
A {\em block-uniform algorithm} accesses its  input matrix by specifying a block length . For a block length  of its choice, the algorithm draws  uniformly at random and obtains set  and  for all  in this set.
The sample complexity of a {\em uniform} or a {\em block-uniform} algorithm is the number of pixels of the image it examines.

\begin{remark}\label{remark:bernoulli}
Uniform algorithms have access to independent (labeled) samples from the uniform distribution over the domain.
\ifnum\full=1
Sometimes it is more convenient to design
{\em Bernoulli algorithms} that
\else
{\em Bernoulli algorithms}
\fi
only have access to (labeled) Bernoulli samples from the image: namely, each pixel appears in the sample with probability , where  is the sample parameter that controls the expected sample complexity.
By standard arguments, a Bernoulli algorithm with the sample parameter  can be used to obtain a uniform algorithm that takes  samples and has the same guarantees as the original algorithm (and vice versa).
\end{remark}


\section{Distance Approximation to the Nearest Half-Plane Image}\label{sec:half-plane}
{\color{black} 
{\color{black} An image is called a \emph{half-plane image}} if there exist an angle  and a real number  such that pixel  is black in the image iff . The line , denoted , is {\em a separating line} of the half-plane image, i.e., it
separates black and white pixels of the image. We call  the {\em direction} of the half-plane image (and ). Note that  is the oriented angle between the -axis and a line perpendicular to . For all  and , the half-plane image with a separating line  is denoted  and the closed half-plane whose every point  satisfies the inequality  is denoted . We can think of a half-plane image as a discretized half-plane.
 
}
\begin{theorem}\label{thm:half-plane-dist-appr}
For , there is a uniform -additive distance approximation algorithm for the half-plane property with sample complexity   and time complexity 
\end{theorem}

\begin{proof}
{\color{black}At a high level, our algorithm for approximating the distance to being a half-plane (Algorithm~\ref{alg:half-plane}) constructs a small set  of reference half-plane images. It samples pixels uniformly at random and outputs the empirical distance to the closest reference half-plane image. The core property of  is that the smallest empirical distance to a half-plane image in  can be computed quickly.

\begin{definition}[Reference directions and half-planes]\label{def:reference-half-planes}
Given , let . Let  be the set of directions of the form  for   called {\em reference directions}. The set of {\em reference half-plane images}, denoted , consists of every half-plane image for which  is a separating line, where  and  is an integer multiple of . \end{definition}
In other words, for every reference direction, we space separating lines of reference half-plane images distance  apart. By definition, there are at most  reference half-plane images for each direction in  and, consequently, .

\begin{algorithm}
\caption{Distance approximation to being a half-plane.}
\label{alg:half-plane}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameters , ; Bernoulli access to an  binary matrix .}
\DontPrintSemicolon
\BlankLine
\nl Sample a set  of  pixels uniformly at random with replacement.\;
\nl Let  be the sets of reference directions and half-planes, respectively (see Definition~\ref{def:reference-half-planes}) and .\;
\tcp{Compute ,
where :}
\nl\label{step:half-planes-foreach-dir} \ForEach {}\do{\tcp{Lines with direction  partition the image. Bucket sort samples by position in the partition:}
\nl\quad     Assign each sample  to bucket .\;
\nl\quad     For each bucket , compute  and , the number of white and black pixels it has.\;
\nl\label{step:half-planes-compute-estimate}\quad     For each , where , compute
.\;
}
\nl Output , the minimum of the values computed in Step~\ref{step:half-planes-compute-estimate}.\;
\end{algorithm}


\begin{lemma}\label{lem:properties-of-reference-HP}
 For every half-plane image , there is  such that .
\end{lemma}

\begin{proof}
We mentioned that a half-plane image can be viewed as a discretized half-plane. Next we define a set of half-planes that we use in the proof of the lemma.
\begin{definition}\label{def:hp}
The set of reference half-planes, denoted , consists of every half-plane , where  and  is an integer multiple of . 
\end{definition}
\begin{claim}\label{cl:properties-of-reference-HP}
For every half-plane , there is a half-plane  such that the area of the symmetric difference of  and  is at most .
\end{claim}
\begin{proof}
Consider a half-plane . Let  be a reference direction closest to . Then . 
We consider two cases. 
\ifnum\full=1
See Figures~\ref{fig:nearby-ref-halfplane} and~\ref{fig:nearby-ref-halfplane2}.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-Ref-halfplane.pdf}
\caption{Proof of Lemma~\ref{lem:properties-of-reference-HP}: triangular regions.}
\label{fig:nearby-ref-halfplane}
\end{minipage}
\hspace{0.1\linewidth}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-Ref-halfplane2.pdf}
\caption{Proof of Lemma~\ref{lem:properties-of-reference-HP}: triangular and quadrilateral regions.}
\label{fig:nearby-ref-halfplane2}
\end{minipage}
\end{figure}

\else
See the figures in the {\color{black} full version}.
\fi

{\bf Case 1:} Suppose that there is a reference half-plane  such that the lines  and  intersect inside inside . Note that the length of every line segment inside  is at most . The symmetric difference of  and  inside  consists of two regions
formed by lines  and . Each of these regions is either a triangle or (if it contains a corner of the image) a quadrilateral. First, suppose both regions are triangles. The sum of lengths of their bases, that lie on the same line, is at most , whereas the sum of their heights is at most . Hence, the sum of their areas is at most .

If exactly one of the regions is a quadrilateral, we add a line through the corner of the image contained in the quadrilateral and the intersection point of   and . It partitions the symmetric difference of  and  into two pairs of triangular regions. Let  (respectively, ) be the angle between the new line and  (respectively, ). Then . Applying the same reasoning as before to each pair of regions, we get that the sum of their areas is at most .
If both regions are quadrilaterals, we add a line as before for each of them and apply the same reasoning as before to the three resulting pairs of regions. Again, the area of the symmetric difference of  and  is at most . Thus,  is the required .

{\bf Case 2:} There exist reference half-planes  and  such that the line 
 is between  and . The region between  and  inside the image has
length at most  and width . Thus, its area is at
most . Partition it into two regions: between  and
 and between  and . One of the two regions has area at most
. Thus,  or  is the required .
\end{proof}

To complete the proof of the lemma we use the following theorem that relates the area of a lattice polygon and the number of integer points that the polygon covers. (A lattice polygon is a polygon whose vertices have integer coordinates.)
\begin{theorem}[Pick's theorem~\cite{pick}]\label{thm:pick}
For a simple lattice polygon , let  denote the number of lattice points in the interior of  and  denote the number of lattice points on the boundary of . Then .
\end{theorem}

\begin{definition}
For a polygon , let  denote the perimeter of  and  denote the number of pixels in , i.e., pixels in the interior of  and on its boundary.
\end{definition}
\begin{proposition}\label{prop:area-pixel}
Let  be a convex polygon. Then .
\end{proposition}
\begin{proof}
If all pixels in  are collinear then . This follows from the fact that the length of a line segment inside a polygon is at most half of the perimeter of the polygon and that the number of integer points on the line segment is at most the length of the line segment plus one. If not all pixels in  are collinear then consider the convex hull of all pixels in . Let  and  denote the number of pixels in the interior and on the boundary of that convex hull, respectively. (Note that the convex hull is a lattice polygon). By Theorem~\ref{thm:pick}, we obtain that  and .
\end{proof}


For some  and , half-plane image . Consider the half-plane . By Claim~\ref{cl:properties-of-reference-HP}, there is a half-plane  such that the area of the symmetric difference of  and  is at most , where  and  is  a multiple of . 

Recall that there are 4 cases for the symmetric difference of  and . More precisely, it consists of: 1) two triangles, 2) a triangle and a quadrilateral, 3) a quadrilateral, or 4) two quadrilaterals. We consider the last case (this is the hardest case and the three other cases are handled similarly). Let the symmetric difference of  and  consist of two quadrilaterals  and . (For reference, see Figure~\ref{fig:nearby-ref-halfplane2} where a triangle and a quadrilateral are shown.) Every line segment in the image has length at most . Thus, . By Proposition~\ref{prop:area-pixel}, we obtain that  (recall that ). This completes the proof.
\end{proof}
}
\subparagraph{Analysis of Algorithm~\ref{alg:half-plane}.}
Let  be the distance of  to being a half-plane. Then there exists a half-plane matrix  such that .
By a uniform convergence bound (see, e.g., \cite{Avrim-lecture-notes}), since  for all , we get that with probability at least 2/3,
 for all . Suppose this event happened. Then  because  for all half-planes . Moreover, by Lemma~\ref{lem:properties-of-reference-HP}, there is a matrix 
such that . For this matrix,  Thus,
 That is,  with probability 2/3, as required.


\subparagraph{Sample and time complexity.} The number of samples, , is . To analyze the running time, recall that . For each direction in , we perform a bucket sort of all samples in expected  time. The remaining steps in the {\bf foreach} loop of Step~\ref{step:half-planes-foreach-dir} can also be implemented to run in  time. The expected running time of Algorithm~\ref{alg:half-plane} is thus .
Remark~\ref{remark:bernoulli} implies a tester with the same worst case running time.
\end{proof}



\ifnum\full=1
\begin{corollary}\label{cor:half-plane-agnostic-learner}
The class of half-plane images is properly agnostically PAC-learnable with sample complexity   and time complexity  under the uniform distribution.
\end{corollary}
\begin{proof}
We can modify Algorithm~\ref{alg:half-plane} to output, along with , a reference half-plane  that minimizes it. By the analysis of Algorithm~\ref{alg:half-plane}, with probability at least 2/3, the output  satisfies 
\end{proof}
\fi


\section{Distance Approximation to the Nearest Convex Image}\label{sec:convexity}
An image is \emph{convex} if the convex hull of all black pixels contains only black pixels.
\begin{theorem}\label{thm:convexity_dist_appr}
For , there is a uniform -additive distance approximation algorithm for convexity with sample complexity  and running time .
\end{theorem}
\begin{proof}
The starting point for our algorithm for approximating the distance to convexity (Algorithm~\ref{alg:convexity-dist-approximation}) is similar to that of Algorithm~\ref{alg:half-plane} that approximates the distance to a nearest half-plane. We define a small set  of reference polygons. Algorithm~\ref{alg:convexity-dist-approximation} implicitly learns a nearby reference polygon and outputs the empirical distance from the image to that polygon. The key features of  is that (1) every convex image has a nearby polygon in , and (2) one can use dynamic programming (DP) to quickly compute the smallest empirical distance to a polygon in .

We start by defining reference directions, lines, points, and line-point pairs that are later used to specify our DP instances.  Reference directions are almost the same as in Definition~\ref{def:reference-half-planes}.





\begin{definition}[Reference lines, line-point pairs]\label{def:reference-lines}
Fix .  The set of {\em reference directions} is 
For every , define the set of {\em reference lines} \ where  is an integer multiple of  .
For each reference line, the set of {\em reference points on } contains points w.r.t.\ , which are inside , spaced exactly   apart (it does not matter how the initial point is picked). A {\em line-point pair} is a pair  where  is a reference line and  is a reference point w.r.t.\ . (Note that there could be reference points on  that were defined w.r.t.\ some other reference line. This is why we say ``a reference point w.r.t.\ '', and not ``a reference point on ''.)
\end{definition}

Roughly speaking, a reference polygon is a polygon whose vertices are defined by line-point pairs. There are additional restrictions that stem from the fact that we need to be able to efficiently find a nearby reference polygon for an input image. The actual definition specifies which actions we can take while constructing a reference polygon. Reference polygons are built starting from reference boxes, which are defined next.

\ifnum\full=1
\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-tolerant-ref-box.pdf}
\caption{ A reference box.}
\label{fig:bounding-box}
\end{minipage}
\hspace{0.1\linewidth}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-Triangles-of-T0.pdf}
\caption{ Triangles of the set .}
\label{fig:triangles-T0}
\end{minipage}
\end{figure}
\fi


\begin{definition}[Reference box]\label{def:reference-box}
A {\em reference box} is a set of four line-point pairs  for , where  are distinct horizontal lines, such that  is above , and  are distinct vertical lines, such that  is to the left of . The reference box defines a vertex set  and a triangle set  formed by removing the quadrilateral  from the rectangle delineated by the lines \ifnum\full=1.
See Figures~\ref{fig:bounding-box}-~\ref{fig:triangles-T0}.
\else
.
\fi
\end{definition}
\ifnum\full=1
Note that line-point pairs do not depend on the input.
\else

\fi
Intuitively, by picking a reference box, we decide to keep the area inside the quadrilateral  black, the area outside the rectangle formed by  white, and the triangles in  gray, i.e., undecided for now.

\begin{definition}
For points , let  denote the line that passes through  and . Let  denote the line segment between  and \ifnum\full=0 . \else\ and  denote the length of .\fi
\end{definition}

Reference polygons are defined next.  Intuitively, to obtain a reference polygon, we keep subdividing ``gray'' triangles in  into smaller triangles and deciding to color the smaller triangles black or white or keep them gray (i.e., undecided for now). We also allow ``cutting off'' a quadrilateral that is adjacent to black and coloring it black (a.k.a.\ ``the base change operation'').
\ifnum\full=1
The main recoloring operation from Definition~\ref{def:reference-polygons} is illustrated in Figure~\ref{fig:tolerant-reducing-area}.
\fi
Even though the definition of reference polygons is somewhat technical, the readers can check their understanding of this concept by following Algorithm~\ref{alg:convexity-dist-approximation}, as it chooses the best reference polygon to approximate the input image.

\begin{definition}[Reference polygon]\label{def:reference-polygons}
A {\em reference polygon} is an image of a polygon  where the set
 can be obtained from a reference box with a vertex set  and a triangle set  by the following recursive process.
Initially,  and . While  move a triangle  from  to  and
perform the following steps:


\begin{enumerate}
\item\label{item:ref-poly-definition} {\sf (Base Change).} Let  where
  Select reference point  on  w.r.t.\ line , and reference point  on  w.r.t.\ line . Add  to . (This corresponds to coloring the quadrilateral  black.) Let  be the height of  w.r.t.\ the base 



\item\label{item:ref-poly-definition-tall} {\sf (Subdivision Step)} If , choose whether to proceed with this step or go to Step~\ref{item:ref-poly-definition-short} (both choices correspond to a legal reference polygon); otherwise, go to Step~\ref{item:ref-poly-definition-short}. Let  be the angle between  and the -axis, and  be such that . Select a reference line-point pair  where the line  crosses  and , whereas  is in the triangle . Let  (resp., ) be the point of intersection of  and  (resp.,  and ). Let , \ifnum\full=1
, as shown on Figure~\ref{fig:tolerant-reducing-area}\fi
. Add  to  and triangles  to . (This represents coloring  black and keeping  and  gray.)

\item\label{item:ref-poly-definition-short} {\sf (End of Processing)} Do nothing. (This represents coloring  white).
\end{enumerate}
\end{definition}
\ifnum\full=1
\begin{figure}[ht]
\begin{minipage}[b]{0.55\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-tolerant-reducing-area.pdf}
\caption{ An illustration to Definition~\ref{def:reference-polygons}: Triangle .}
\label{fig:tolerant-reducing-area}
\end{minipage}
\hspace{0.01\linewidth}
\begin{minipage}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-Rectangle-and-regions-of-lines-0.pdf}
\caption{ Regions , , , and .}
\label{fig:w-regions}
\end{minipage}
\end{figure}
\fi

By Remark~\ref{remark:bernoulli}, to prove Theorem~\ref{thm:convexity_dist_appr}, it suffices to design a Bernoulli tester that takes  samples in expectation and runs in time . Our Bernoulli tester is Algorithm~\ref{alg:convexity-dist-approximation}.
In Algorithm~\ref{alg:convexity-dist-approximation}, we use the following notation for the (relative) empirical error with respect to an input image , a set of sampled pixels  and the size parameter . For an image , let
 For every region , we let
 and
 i.e., the empirical error if we make  black/white, respectively.

\begin{algorithm}
\caption{Bernoulli approximation algorithm for distance to convexity.}
\label{alg:convexity-dist-approximation}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameters , ; Bernoulli access to an  binary matrix .}
\DontPrintSemicolon
\BlankLine
\nl\label{step:convexity-approx:sample}
Set .
Include each image pixel in the sample  w.p.\ .\;
\tcp{Run the algorithm to find , the smallest fraction of samples misclassified by a reference polygon in . A dynamic programming implementation of the algorithm is given in \ifnum\full=0 Section~4.3 of the {\color{black} full version}.\else Section~\ref{sec:convexity-dist-approx-wrap-up}.\fi }
\nl
Let  (resp., ) be the set of pixels of the image  that lie either above  or to the left of  on  (resp., either below  or to the left of  on ). Let  (resp., ) be the set of pixels of  to the left of  (resp., to the right of ). \ifnum\full=1 (See Figure~\ref{fig:w-regions})\fi


\nl
Set .
\;
\nl\label{step:polygons-foreach-top-bottom}
        \ForAll {line-point pairs , where  are horizontal lines} \do{
\nl\quad\label{step:errle-init}     Set  \tcp*{\parbox[t]{4in}{The variable to compute the best error for the region to the left of , between  and .\;}}
\nl\quad\label{step:polygons-foreach-left}
\ForEach {line-point pair , where  is a vertical line} \do{
\nl\quad\quad Let  (resp., ) be the point where  intersects  (resp.,  intersects ).\;
\nl\quad\quad\label{step:errle-compute}
               \;
        }
\nl\quad        Similarly to Steps~\ref{step:errle-init}--\ref{step:errle-compute}, compute  .\tcp*{\parbox[t]{10in}{The best error for the region to the right of , between  and .\;}}

\nl\quad    Compute \;

    }
\nl         \Return .
\end{algorithm}

\ifnum\full=1
Subroutine \Best, presented next,
\else
Subroutine \Best
\fi
chooses the option with the smallest empirical relative error among those given in Definition~\ref{def:reference-polygons}, items~\ref{item:ref-poly-definition}-\ref{item:ref-poly-definition-short}.
\ifnum\full=0
Its pseudocode is in the {\color{black} full version}.
\else
\begin{algorithm}
\caption{Subroutine \Best used in Algorithm~\ref{alg:convexity-dist-approximation}.}
\label{alg:subroutine-best}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{triangle }
\DontPrintSemicolon
\BlankLine
\tcp{Use dynamic programming (see Section~\ref{sec:convexity-dist-approx-wrap-up} for implementation details).}
\nl Set 

\nl\quad\label{step:forall-best-type2}\ForAll {reference points  and  on the sides  and  respectively,}\do{

\nl\quad\quad Compute }

\nl\quad \Return 

\end{algorithm}


\begin{algorithm}\label{alg:subroutine-best-fixed-base}
\caption{Subroutine \BestFixed used in Algorithm~\ref{alg:subroutine-best}.}
\label{alg:subroutine-best-fixed-base}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{triangle }
\DontPrintSemicolon
\BlankLine
\nl Set \;
\nl \If {the height of  w.r.t.\ the base  is more than }{





\nl\quad\label{step:foreach-best-type1}\ForEach {line-point pair , where  (see Definition~\ref{def:reference-polygons}, item 2), ,
line  intersects the side  at some point  and the side  at some point , resp.
} \do{

\nl\quad\quad Compute 
}}

\nl \Return 

\end{algorithm}
\fi

\ifnum\full=0
Our set of reference polygons has two critical features. First, for each convex image there is a nearby reference polygon. It turns out that the empirical error for a region is proportional to the square root of its area. The second key feature of our reference polygons is that, for each of them, the set of considered triangles, , has small  where  denotes the area of triangle . The proofs of both features, as well as the analysis of the empirical error, are quite technical and appear in the {\color{black} full version}.\end{proof}

Here, we state and partially prove a lemma that puts together different parts of the analysis. It makes it clear why the empirical error of each region is proportional to the square root of its area which is, as explained in Footnote~\ref{fn:Pick}, a proxy for the number of pixels in it.


\else
Our set of reference polygons has two critical features. First, for each convex image there is a nearby reference polygon. This is proved in Section~\ref{sec:existence-of-nearby-ref-poly}. It turns out that the empirical error for a region is proportional to the square root of its area. The second key feature of our reference polygons is that, for each of them, the set of considered triangles, , has small  The proof of this fact, as well as the analysis of the empirical error appears in Section~\ref{sec:convexity-dist-approx-error-analysis}. Finally, Section~\ref{sec:convexity-dist-approx-wrap-up} completes the analysis of the algorithm, gives details of its implementation and presents the corollary about agnostic PAC learning of convex objects.

\subsection{Existence of a nearby reference polygon}\label{sec:existence-of-nearby-ref-poly}
\begin{lemma}\label{lem:nearby-reference-polygon}
 For every convex image , there exists  such that .
\end{lemma}


\begin{proof}
Consider a convex image . We will show how to construct a nearby reference polygon  using the recursive process in Definition~\ref{def:reference-polygons}. First, we obtain a reference box (see Definition~\ref{def:reference-box}) for  as follows. Let  be a line-point pair, where  is black in  and  is the topmost horizontal line that contains such a reference point. Similarly, define , replacing ``topmost'' with ``bottommost''. Analogously, define the two line-point pairs ,  with vertical lines. The four line-point pairs  for  define the reference box for , as shown in Figure~\ref{fig:reference-box}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\linewidth]{figure-reference-box.pdf}
\caption{ Reference box for a convex image .}
\label{fig:reference-box}
\end{figure}




Next we construct the set  from the reference box, as in Definition~\ref{def:reference-polygons}.  We also maintain two sets of line segments,  and , that are used in the analysis. Initially, . The colors of the points in the description below are with respect to the convex image . This is how we make the choices at each step of the recursive process in Definition~\ref{def:reference-polygons} to obtain our reference polygon:
\begin{enumerate}
\item {\sf (Base Change)} Choose  to be the furthest from  black reference w.r.t.\ lines  and , respectively. Recall that  is the height of  w.r.t.\ the base .

\item {\sf (Subdivision Step)} If , let  denote the convex hull of all black pixels in  and points  be the intersection points of  with  and , respectively. Choose a line-point pair  such that  is the furthest from  line that intersects  and ,  and  is black.  Let  intersect  and  at  and , respectively and let it intersect  at  and  as in Figure~\ref{fig:small-error-ref-poly-1}. Put the line segment  in  and  in . If no line in  contains a black reference point in  or if  go to Step 3.

\item {\sf (End of Processing)} Put the line segment  in  and  in . Triangle  is not subdivided and is called a {\em final} triangle.
\end{enumerate}



Observe that  and  differ only on three types of regions: outside of the reference box, inside the triangles in , and inside the triangles in . To show that  we prove in Claims~\ref{cl:error-in-strips}, \ref{cl:error-in-triangles}, and \ref{cl:small-area-above-line} that the number of disagreements in each of the three regions is small.
For any region , let .


Next claim
follows from the analysis of the convexity tester in \cite{Ras03}.
\begin{claim}
\label{cl:error-in-strips}
The number of black pixels in 
outside the reference box is at most .
\end{claim}
\mch{

\begin{claim}\label{cl:error-in-triangles}
Let  be a final triangle and points  be the points of intersection of  with  and , respectively. Then .
\end{claim}


\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{small-error-ref-polygon-1.pdf}
\caption{ An illustration to Subdivision Step in .}
\label{fig:small-error-ref-poly-1}
\end{figure}


\begin{proof}
By Proposition~\ref{prop:area-pixel}, . Note that  is obtuse.
\begin{proposition}
\label{prop:obtuse-height}
Let  be a triangle with sides  and . Let  be the angle opposite to side , and  be the height w.r.t.\ base  in . If  then .
\end{proposition}
\begin{proof}
By the cosine theorem, . Thus, , as claimed.
\end{proof}
If  then by Proposition~\ref{prop:obtuse-height}, the area . Since  we obtain that  and the claim holds (recall that ). Now assume that  and no line in  with a black reference point intersects the line segments  and  in .



\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{small-error-ref-polygon-2.pdf}
\caption{ An illustration of triangle .}
\label{fig:small-error-ref-poly-2}
\end{figure}

\begin{proposition}
\label{prop:small-error-above}
Let  be a triangle in which  is obtuse and  intersects the sides  and . Let  be a line that intersects  at  and , and it intersects  and  at  and , respectively. See Figure~\ref{fig:small-error-ref-poly-2}. Then .
\end{proposition}
\begin{proof}
Let  be a point (inside ) such that  is parallel to  and  is parallel to . Since  is convex, the portion of  in  is entirely inside . Angle  is obtuse since . Then by Proposition~\ref{prop:obtuse-height}, . Note that . Since  then by Proposition~\ref{prop:area-pixel}, .
\end{proof}

Let  be the line that does not intersect the line segment  and that is closest to it. Let  intersect the line segments  and  at  and . Then either  or . W.l.o.g. assume that .
Let  be the line that is parallel to  and that passes through point  as shown in Figure~\ref{fig:small-error-ref-poly-3}. Let  be the intersection point of  and the line segment . Denote the angle between  and  by . The distance between  and  is at  most . Otherwise there are two distinct lines from  that pass through the line segment . Since  the distance between the two lines is less than , contradiction.

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{small-error-ref-polygon-3.pdf}
\caption{ An illustration of line  in .}
\label{fig:small-error-ref-poly-3}
\end{figure}

Now we find an upper on the number of black pixels in . Let  intersect  at  and . Then . By Proposition~\ref{prop:small-error-above},  The number of black pixels in the rectangle  is at most . The area

The last inequality holds since  is obtuse. Let  (resp., ) denote the distance from the point  (resp., ) to the line . We find an upper bound on :

 Thus, . The height . By Proposition~\ref{prop:obtuse-height}, if  then . It implies that , contradiction. Therefore, . By the triangle inequality . Thus,
 The last inequality holds since . Note that  Thus, by Proposition~\ref{prop:area-pixel}, 
and  
The last inequality holds since . This completes the proof of Claim~\ref{cl:error-in-triangles}.
\end{proof}
\begin{claim}
\label{cl:small-area-above-line}
Let triangle  and line  be as defined in Step 2 of the recursive construction of . Let  and  denote the points of intersection of  and  and , respectively. Let  and  be the points of intersection of  and . Then .
\end{claim}
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{figure-error-above-line}
\caption{ An illustration of line  in triangle .}
\label{fig:error-above-l}
\end{figure}

\begin{proof}
If  then, by Proposition~\ref{prop:small-error-above},  Now assume that . Let  be the line at distance  from  closer to , as in Figure~\ref{fig:error-above-l}. Let  intersect  and  at  and , respectively. Then  is at most the number of black pixels in . Note that all black pixels in  are inside a rectangle with length . Thus, by Proposition~\ref{prop:area-pixel}, the number of black pixels in  is at most . The distance between the points of intersection of  with  is at most . Thus, by Proposition~\ref{prop:small-error-above},
 The last inequality holds since . This completes the proof of Claim~\ref{cl:small-area-above-line}.
\end{proof}


Observe that all points in  lie on the boundary of a convex polygon. Images  and  differ only on pixels outside of the reference box and inside the triangles  and . All the line segments in  are the sides of a convex polygon which is inside an  square. Thus, the sum of the lengths of the line segments in  is at most . Now we find an upper bound on . Note that in the process of constructing a reference polygon starting from triangles in , every triangle is subdivided into at most two new triangles. Fix a triangle . Consider a binary tree  rooted at , where every node is some triangle obtained during the reference polygon construction and every triangle in  has at most two children triangles obtained after subdivision of their parent (during the construction). Triangles in  correspond to the leaves of the binary tree. Thus, to upperbound  we need to find the maximum possible height of  and we need to assume that the tree is full. Recall  and  from the construction of a reference polygon. Triangle  is not subdivided if . By Proposition~\ref{prop:obtuse-height}, if  then . Thus, a triangle is not subdivided if its area drops below . Note that every triangle in  has area at most . Consider a triangle  in  with two children  and . Let  be the height of . By Claim~\ref{claim:sum-of-roots-of-areas}, .
Thus, every triangle in level  of  has area at most . The area of every triangle in level  of  is at least  (otherwise, non of the triangles in this level is subdivided and the height of  cannot be ). We obtain that  and thus, . Therefore, the number of leaves in  is at most  (recall that ) and . By Claims~\ref{cl:error-in-strips},~\ref{cl:error-in-triangles} and \ref{cl:small-area-above-line},

This completes the proof of Lemma~\ref{lem:nearby-reference-polygon}. }
\end{proof}




\subsection{Error analysis}\label{sec:convexity-dist-approx-error-analysis}

\begin{lemma}\label{lem:sum-of-roots-of-areas}
For each set  obtained in the construction of a reference polygon in Definition~\ref{def:reference-polygons},

\end{lemma}
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{figure-sum-of-roots}
\caption{ Triangle .}
\label{fig:sum-of-roots}
\end{figure}

\begin{proof}
All triangles in  are obtained by partitioning the four initial triangles in . The following claim analyzes how the area is affected by one step of partitioning.
\begin{claim}\label{claim:sum-of-roots-of-areas}
Let  and  be two gray triangles obtained from a triangle  in Subdivision Step of Definition~\ref{def:reference-polygons}. Then

\end{claim}

\begin{proof}
Observe that  is maximized when  and . W.l.o.g.\ we prove the lemma for this case. We use notation from Figure~\ref{fig:sum-of-roots}. Recall that a triangle  is partitioned only if its height  Since the sides of  are of length at most , the height is that large only if both angles adjacent to the base  are greater than . (To see this, consider an angle  between the base and a side of length . We get
.
Thus, )

First, we find the maximum value of  for a fixed line  on which position of point  varies. \mch{Let ,  and  be the angle between lines  and . W.l.o.g.\ assume that . Then  and . By the construction of triangles in ,  and . Let , ,  and , ,  ().}  Let\mch{

Thus, , where ,  are constants. By the  Cauchy-Schwarz inequality, .

Next, we find the maximum value of  varying position of  inside . We use the fact that
  to obtain

We need to show that . Let . Since  is constant and the geometric mean of two numbers is at most their arithmetic mean  We prove that  which is equivalent to . The latter inequality holds if  Function  is increasing on  thus,  Now we show that  If  the inequality holds. Let us assume that 
We need to prove that

Function  is decreasing on  thus,

The last inequality is equivalent to  which is true since . This completes the proof of Claim~\ref{claim:sum-of-roots-of-areas}}\end{proof}

Let  be the areas of the first four triangles in . Then .
By construction of triangles in , Claim~\ref{claim:sum-of-roots-of-areas}, and concavity of the square root function,

where .
This completes the proof of Lemma~\ref{lem:sum-of-roots-of-areas}.
\end{proof}


Let  be an input image,  be the set of samples obtained by the algorithm, and  be the parameter in the algorithm.
For any image , let  and

Also, for any region , let 
and 
\fi

\begin{lemma}\label{lem:accuracy-on-ref-polygons}
With probability at least  over the choice of the samples taken by Algorithm~\ref{alg:convexity-dist-approximation},
 for all reference polygons .
\end{lemma}
\begin{proof}

Consider a region  partitioned into two regions  and  such that in some step of the algorithm we are checking the assumption that  is black and  is white, i.e., evaluating  Let  be the set of all such regions . We will show that with probability at least 2/3, the estimates  are accurate on all regions in .

Fix . Let  be the set of misclassified pixels in , i.e., pixels in  which are white in  and pixels in  which are black in . Define . Algorithm~\ref{alg:convexity-dist-approximation} approximates  by  Equivalently, it uses the estimate  for  (recall that ). The error of the estimate is .

\begin{claim}\label{claim:error-for-R}
, where .
\end{claim}

\begin{proof}
For each pixel  we define random variables  and ,
where  is the indicator random variable for the event  (i.e., a Bernoulli variable with the probability parameter ), whereas .
Then our estimate of  is  whereas
. We use Bernstein inequality
\ifnum\full=1
(Theorem~\ref{thm:bernstein})
\else
(stated in the {\color{black} full version} for completeness)
\fi
with parameters
 and  to bound .
The variables  are identically distributed. The maximum value of  is . Note that
.
We assume w.l.o.g.\ that  (If  then  can never exceed , and the probability we are bounding is 0.)
By Bernstein inequality,


The second inequality holds because  and .
The equalities are obtained by substituting the expressions for  and , and simplifying.
By symmetry, .
\end{proof}
\ifnum\full=0
The rest of the proof appears in the {\color{black} full version}.
\end{proof}
\else
\begin{claim}\label{claim:convexity-approx-regions-count}
The number of regions in  is at most .
\end{claim}
\begin{proof}
Let . There are four types of regions in , each corresponding to a different call of the form  in the algorithm. The first type is a horizontal double strip of the form  and . There are  such strips.
The second type is where  is a black triangle  (or ) and  is a vertical strip  (respectively, ). For each horizontal double strip, there are  vertical strips. For each of them, there are  ways to choose a reference point on the vertical line that delineates the strip. So, overall, there are  regions of type 2. Type 1 and 2 together have at most  regions.
Regions of type 3 are black quadrilaterals of the form  Each quadrilateral is defined by two reference lines,  and  with two reference points on each. There are  ways to choose reference directions for the two lines. For each of them, there are at most  ways to choose a reference line and two reference points. Overall, the number of quadrilaterals in  is at most .
Finally, regions of type 4 are contained in triangles of the form ; they are of the form either  or , . In the former case, regions are defined by two line-point pairs  and . There are  pairs of reference directions. For each of them, there are at most  choices for each reference line and  choices for each reference points. In the latter case, they are defined by three reference line-point pairs:  and  but the direction of the line through  is determined by . As before, there are  pairs of reference directions. For each of them, there are at most  choices for each reference line and  choices for each reference points. Overall, the number of regions of type 4 is upper-bounded by .
Overall,  as claimed.
\end{proof}



By taking a union bound over all regions in  and applying Claims~\ref{claim:error-for-R}--\ref{claim:convexity-approx-regions-count}, we get that the probability that for one or more of them the error is larger than stated in Claim~\ref{claim:error-for-R} is at most ,
where the last inequality holds provided that  for some sufficiently large constant . We get that



Now suppose that event in (\ref{eq:low-error-on-all}) holds, that is, the error is low for all regions. Fix a reference polygon . Consider the partition of  into regions from  on which Algorithm~\ref{alg:convexity-dist-approximation} evaluates  while implicitly computing . Let  be the set of regions in the partition. Recall the four types of regions from the proof of Claim~\ref{claim:convexity-approx-regions-count}. Then   contains one region of type 1 and two regions of type 2, defined by the reference box of . Denote their areas by . For each triangle  created during the construction of  in Definition~\ref{def:reference-polygons}, the set  contains at most one region of type 3 and at most one region of type 4. They were implicitly colored, respectively, in Item~\ref{item:ref-poly-definition} and Items~\ref{item:ref-poly-definition-tall}-\ref{item:ref-poly-definition-short} of Definition~\ref{def:reference-polygons}, when triangle  was processed. Let  and  denote their respective areas.

Recall that  denotes the area of  and that an approximate (but precise enough for asymptotic analysis) upper bound on the number of misclassified pixels in  is . Since the event in (\ref{eq:low-error-on-all}) holds,

Since  and  for all ,  by concavity of the square root function,

We substitute these expressions in the previous inequality, use Lemma~\ref{lem:sum-of-roots-of-areas} and recall that :

This holds for all reference polygons  as long as the event in (\ref{eq:low-error-on-all}) happens, i.e., with probability at least 2/3. This completes the proof of Lemma~\ref{lem:accuracy-on-ref-polygons}.
\end{proof}

For completeness, we state Bernstein's inequality, which was used in the proof of Lemma~\ref{lem:accuracy-on-ref-polygons}.

\begin{theorem}[Bernstein's inequality]\label{thm:bernstein}
 Let  be  independent zero-mean random variables, where  for all . Then for all positive ,


\end{theorem}



\subsection{Wrapping up: proof of Theorem~\ref{thm:convexity_dist_appr} and corollary on agnostic learning}\label{sec:convexity-dist-approx-wrap-up}
\subparagraph{Analysis of Algorithm~\ref{alg:convexity-dist-approximation}.}
Let  be the distance of  to convexity. Then there exists a convex image  such that .
By Lemma~\ref{lem:nearby-reference-polygon}, there is a reference polygon  such that , and consequently, .
By Lemma~\ref{lem:accuracy-on-ref-polygons},
with probability at least 2/3 over the choice of the samples taken by Algorithm~\ref{alg:convexity-dist-approximation},
 for all reference polygons .
Suppose this event happened. Then  because  for all convex images . Moreover,   Thus,
 That is,  with probability at least 2/3, as required.

\subparagraph{Sample and time complexity of Algorithm~\ref{alg:convexity-dist-approximation}.} The number of samples taken by the algorithm is .

Next we explain how to implement it to run in time . Refer to Figure~\ref{fig:tolerant-reducing-area}.
Each instance triangle  of the dynamic programming in subroutine \Best is specified by two line-point pairs: . The number of line-point pairs is  because for each we select
the reference direction, the shift of the line, and the reference point, each
in  ways.  Hence, we have  entries in the dynamic
programming table for \Best.

In the process of solving an instance of  \Best, we consider  possibilities for points , that is,  possibilities over all instances. We show how to evaluate each of the possibilities in amortized time .
For that, we count white and black sample pixels in each sub-area in Figure~\ref{fig:tolerant-reducing-area} in amortized time .

First, we show how to do it for the entire triangle .  We have 
triangles that can be partitioned into  groups by specifying
the first line-point pair  and the second line (through  and ). That is, within each group, we vary
only point  on the second line.  We sort all sample points 
according to the angle of the segment . Similarly, we sort
the reference points  on the second line according to the angle of the segment .  After sorting, a single scan can establish the counts of white and black pixels in the triangles. Clearly,
we can sort in time . Thus, we
compute white/black counts for all instance triangles of \Best in time .

When we consider a possibility in \Best, the triangle
 is also an instance triangle, so we can find the
white/black counts for the quadrilateral  by computing the difference between the counts for entire triangle  and triangle , that is, in time .

When we consider a possibility in subroutine \BestFixed, we need the
counts for the four parts of . Since we already calculated the counts for  and because we can perform subtractions, it is
enough to do it for three parts.  Two of them,
 and
, are instance triangles for \Best, so we already calculated their counts. The third we choose is the triangle .  Note that this triangle is specified by three reference lines, so
there are  such triangles. We make a table for all of them.
To fill the table, we consider  groups:  we group together triangles for which line  has
a common direction. By sorting samples in , we can compute the counts for
each group in time . Thus, the time for filling the second table is . To summarize, Algorithm~\ref{alg:convexity-dist-approximation} runs in time . This completes the proof of Theorem~\ref{thm:convexity_dist_appr}.
\end{proof}



\begin{corollary}\label{cor:convexity-agnostic-learner}
The class of convex images is properly agnostically PAC-learnable with sample complexity   and time complexity  under the uniform distribution.
\end{corollary}
\begin{proof}
We can modify Algorithm~\ref{alg:convexity-dist-approximation} to output, along with , a reference polygon  with . With an additional DP table, we can compute which points became its vertices. By the analysis of Algorithm~\ref{alg:convexity-dist-approximation}, with probability at least 2/3, the output  satisfies 
\end{proof}
\fi




\section{Distance Approximation to the Nearest Connected Image}\label{sec:connectedness}
To define {\em connectedness}, we consider {\em the  image graph } of an image . The vertices of  are
, and two vertices  and  are connected by an edge if .
In other words, the image
graph consists of black pixels connected by the grid lines. The image is
{\em connected} if its image graph is connected.

\begin{theorem}\label{thm:connectedness_dist_appr}
There is a block-uniform -additive distance approximation algorithm for connectedness with sample complexity  and running time .
\end{theorem}

\ifnum\full=1
\subsection{Border Connectedness}\label{sec:border_connectedness}
\fi

The first idea in our algorithms for connectedness is that we can modify an image
\ifnum\full=1
in a relatively few places by superimposing a grid on it
(as shown in Figures~\ref{fig:Hawaii} and~\ref{fig:Hawaii-grided}),
and as a result obtain an image whose distance to connectedness is determined by the properties of individual squares into which the grid lines partition the image. The squares and the relevant property of the squares  are defined next.
\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-FranzLand.pdf}
\caption{An image .}
\label{fig:Hawaii}
\end{minipage}
\hspace{0.1\linewidth}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figure-FranzLand-grided.pdf}
\caption{A gridded image obtained from~.}
\label{fig:Hawaii-grided}
\end{minipage}
\end{figure}
\else
by superimposing a grid on it
(see figures in the {\color{black} full version}),
and as a result obtain a nearby image whose distance to connectedness is determined by the properties of individual squares into which the grid lines partition the image. The squares and the relevant property of the squares  are defined next.
\fi

For a set  and , we define
.

\begin{definition}[Squares and grid pixels]\label{def:squares} Fix a side length .
For all integers , where  and  are divisible by , the  image that
consists of all pixels in  is called an {\em -square} of . The set of all -squares of  is denoted .

The pixels that do not lie in any squares of , i.e., pixels  where  or  is divisible by  are called {\em grid pixels}. The set of all grid pixels is denoted by .
 \end{definition}

\begin{claim}\label{claim:GP-size}
.
\end{claim}
\ifnum\full=1
\begin{proof}
.
\end{proof}
\fi
Note that a square consists of pixels of an \rblock, with the pixels of the first row and column removed. Therefore, a block-uniform algorithm can obtain a uniformly random -square.

Recall the definition of the border of an image from Section~\ref{sec:defintions_notation}.
\begin{definition}[Border connectedness]\label{def:border_connectedness}
A (sub)image  is {\em border-connected} if for every black pixel  of , the image graph  contains a path from  to a pixel on the border. The property
\emph{border connectedness,} denoted , is the set of all border-connected images.
\end{definition}




\ifnum\full=1
\subsection{Proof of Theorem~\ref{thm:connectedness_dist_appr}}\label{sec:proof_of_thm}
\fi
The main idea behind Algorithm~\ref{alg:connectedness}, used to prove Theorem~\ref{thm:connectedness_dist_appr}, is to relate the distance to connectedness to the distance to another property, which we call {\em grid connectedness}. The latter distance is the average over squares of the distances of these squares to {\em border connectedness}. The average can be easily estimated by looking at a sample of the squares.




W.l.o.g.\ assume that . (Otherwise, we can pad the image with white pixels without changing whether it is connected and adjust the accuracy parameter.)

\begin{algorithm}\caption{Distance approximation to connectedness.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\label{alg:connectedness}
\Input{ and ; block-sample access to an  binary matrix .}
\DontPrintSemicolon
\BlankLine
\nl Sample  squares uniformly and independently from  (see Definition~\ref{def:squares}).\;
\tcp{This can be done by drawing random blocks from the -partition of .}
\nl\label{step:ave-dist-border-conn} For each such square ,
compute  (see \ifnum\full=1 Section~\ref{sec:border-con}\else the {\color{black} full version}\fi \ for details), where  is border connectedness (see Definition~\ref{def:border_connectedness}). Let  be the average of computed distances .\;
\nl \Return .
\end{algorithm}

\begin{definition}\label{def:grid_pixels} Fix . Let image  be a {\em gridded image} obtained from image  as follows:
\ifnum\full=0
\begin{center}
 is a grid pixel from 
\end{center}
\else

\fi
Let  be the set of all connected images. For  define \emph{grid connectedness}
M[i,j]=1(i,j)\in \gp_{4/\mydelta}
\end{definition}
\begin{lemma}\label{lem:distance_relation}
 Let  and  . Then . Moreover,

\end{lemma}
\begin{proof}
First, we prove that 
Let  be a connected image such that . Then , the gridded image obtained from , satisfies . Since , it follows that 

Now we show that . Let  be such that . Then  and, by Claim~\ref{claim:GP-size}, , implying , as required.

Finally, observe that to make  satisfy , it is necessary and sufficient to ensure that each square satisfies . In other words,

Since , the desired expression for  follows.
\end{proof}

\ifnum\full=1

\subparagraph{Analysis of Algorithm~\ref{alg:connectedness}.}
Let . Recall that  is the empirical average computed by the algorithm.
By the Chernoff-Hoeffding bound, . So, with probability at least 2/3, we have . If this event happens then
 because by Algorithm~\ref{alg:connectedness} and Lemma~\ref{lem:distance_relation}, respectively,  and , where . By Lemma~\ref{lem:distance_relation}, . Thus,
 holds with probability at least 2/3, as required.

\subparagraph{Query and time complexity.}
Algorithm~\ref{alg:connectedness} samples  squares containing  pixels each. Thus, the sample complexity is .

The most expensive step in Algorithm~\ref{alg:connectedness} is Step~\ref{step:ave-dist-border-conn} where the distance of a square  to border connectedness is calculated. By Theorem~\ref{thm:border-con} (see section \ref{sec:border-con}), the running time of this step for one square is  and it is called  times. Therefore, the running time of Algorithm~\ref{alg:connectedness} is , as claimed.
\else
The rest of the analysis is completed in the {\color{black} full version of this paper} using the Chernoff-Hoeffding bound.
\fi

\subsection{Algorithm for Border Connectedness}
\label{sec:border-con}

\begin{theorem}
\label{thm:border-con}
Let  be a  image. There is an algorithm that computes  (i.e., distance of  to border connectedness) in time .
\end{theorem}

\begin{proof}{\color{black}
To prove the theorem we give a dynamic programming algorithm that computes  in the following way: starting from row 1 of , it processes a row and proceeds to the next one. The algorithm stops after processing row . The information the algorithm computes and stores for each row is explained later in this section.

\begin{definition}
\label{def:block}
Let  be a vector. Call maximal consecutive runs of 's in  {\em 1-blocks} and let  denote the number of 1-blocks in . Let  (respectively, ) denote the string of  ones (respectively, zeros) and let .
\end{definition}
}

Consider a  image . Recall that  denotes the image graph of . For every , denote the subgraph of , induced by the first  rows in , by . Index 1-blocks in row  of  in the increasing order of indices of pixels they contain. For example, a row  contains two 1-blocks; the 1-block with three 's  has index 1, the 1-block with two 's has index 2. Each 1-block in row  has one of the following 5 statuses w.r.t.\ :
\begin{itemize}
\item{connected to the border of  (denoted by 1);}
\item{isolated, i.e., not connected to the border and to any other 1-block in its row (denoted by 0);}
\item{first 1-block in its connected component, i.e., it is in the connected component with other 1-blocks of row  and has the smallest index among them (denoted by );}
\item{intermediate 1-block in its connected component, i.e., has neither largest nor smallest index in its connected component (denoted by );}
\item{last 1-block in its connected component, i.e., it is in the connected component with other 1-blocks of row  and has the largest index among them (denoted by ).}
\end{itemize}

{\color{black}
\begin{definition}\label{def:config}
Let  denote the coloring of  in row , for some  (i.e., ). Statuses of 1-blocks of  w.r.t.  are captured by a {\em status vector} . The pair  is called the {\em configuration w.r.t.\ .}
\end{definition}

\begin{definition}\label{def:bc-sets}
For all , , and vectors  over  of length at most , define 
\end{definition}

For all  and  images , let  denote the number of pixels on which the first  rows of  and  differ. For all , and  define

Note that the number of all possible configurations for a row is at most . We show that if for some , the value of  is known for every configuration  in row , then for every configuration  in row , the cost  can be computed in time exponential in . This is a crucial ingredient that helps us to show that the running time of our algorithm is exponential in .

For a fixed , consider an image . Let  be an image which has the same  rows as . Let  denote the coloring of row  in . If  is border-connected then configuration  is {\em consistent} with coloring , i.e., every 1-block in  that has status other than 1 w.r.t.\  is connected to a 1-block in  w.r.t.\ . Moreover, for some status vector , image  and  can be determined from , and . Observe that if  is known for every configuration , then  can be computed. After computing costs for all configurations in each row,  can be found. In order to find , our algorithm uses subroutine \Compst. \Compst uses subroutine \Constgr that creates a graph whose nodes are 1-blocks of  and edges are defined according to the information provided by . Now we explain how  is found. }



Now we show how subroutine \Compst computes  from colorings ,, and the status vector  of  w.r.t\ , where  is an image from  for . Let  and . Index 1-blocks in  in the nondecreasing order of indices of pixels they contain. Let  be an image obtained from  by recoloring its row  to . Index 1-blocks in  in the nondecreasing order of indices of pixels they contain and add  to each index. Consider graph  where  and  has every edge of the following two types:
\begin{enumerate}
\item edges , where , , and  is not connected to any  in  .
\item , where , and  is connected to  in .
\end{enumerate}

\Compst uses subroutine \Constgr to construct graph . (\Constgr computes set .)
After graph  is constructed, \Compst checks whether configuration   and  are consistent w.r.t.\ . If they are not consistent it outputs a  symbol. If they are consistent then  (rows  in  can be recolored to all black rows and the resulting image is in ). \Compst finds vector  based on the connectivity information of graph  and information provided by vector .


To check whether  is consistent with  our algorithm uses subroutine \Compst (Algorithm~\ref{alg:status}). If it is consistent, to
find the status vector . Subroutine \Compst uses subroutine \Constgr that constructs a graph from  and  that helps to compute . The nodes in this graph are 1-blocks of  and edges are defined according to the information provided by . \Constgr is explained next.

For each , every image  has some configuration  in its 'th row w.r.t.\ . Vector  in this  configuration has the information about which 1-blocks in  are in the same connected component in  and which are connected to the border. Thus, if  is given we can construct a graph  whose nodes are all 1-blocks of  and the status vector of  w.r.t.\  is . Subroutine \Constgr (Algorithm~\ref{alg:graph}) constructs graph .

\begin{algorithm}\label{alg:graph}
\caption{Subroutine \Constgr used in Algorithm~\ref{alg:status}.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{vector , and .}
\DontPrintSemicolon
\BlankLine
\nl Index 1-blocks in  in the nondecreasing order of indices of pixels they contain. \\
\tcp{Let  and  (we maintain a stack)}

\nl \ForAll {indices } {
\nl\textbf{if}  and  \textbf{then} \Return \\
\nl\textbf{if}  and  \textbf{then} \Return \\

\nl\textbf{if}  \textbf{then} \\
\nl\textbf{if}  \textbf{then} \textbf{do} ; add  to  \textbf{until} \\
}
\nl \Return \\
\end{algorithm}



\begin{comment}
Let  denote the coloring of row  in an image . Then configuration  is {\em consistent} with coloring , i.e., every 1-block in  that has status other than 1 is connected to a 1-block in  w.r.t.\ . Moreover,  has a status vector  w.r.t.\  that can be determined from , and . Suppose  is known for every configuration . Then if we find status vector  for all colorings  consistent with , we can compute . {\color{black}After computing costs for all configurations in each row}, we find  which is equal to . Our algorithm that computes  as described and satisfies Theorem~\ref{thm:border-con} is Algorithm~\ref{alg:dist}. In Lemma~\ref{lm:correct}, we show that it correctly computes  for each . The algorithm uses subroutine \Compst (Algorithm~\ref{alg:conf}, analyzed at the end of this section) to find the status vector  of  w.r.t.\ .
\end{comment}
\begin{algorithm}
\caption{Distance to border connectedness of a square .}
\label{alg:dist}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{access to a  square .}
\DontPrintSemicolon
\BlankLine
\nl\label{st:init}\ForAll  {indices , vectors , and } \do{
\tcp{For , let  be a vector that corresponds to the  row of .}
\nl\quad\\}

\nl\label{st:subsequent}\ForAll {indices , vectors  and }
   \do{
\nl\quad \textbf{if}  \textbf{then} \\
\nl\quad\textbf{if}  \textbf{then} 

}
\nl \label{st:actual-min}\Return 
\end{algorithm}

\subparagraph{Analysis of Algorithm~\ref{alg:dist}.}
The following lemma shows that Algorithm~\ref{alg:dist} is correct.
\begin{lemma}
\label{lm:correct}
For all , , and , Algorithm~\ref{alg:dist} correctly computes .
\end{lemma}
\begin{proof} We prove the lemma inductively. For the first row, Algorithm~\ref{alg:dist} indeed computes the cost of every configuration. (Note that every 1-block in the first row is connected to the border and thus, every such 1-block has status . Therefore,  if , and , otherwise). Assume that the statement in the lemma holds for some row . We prove the statement for row . Note that if  for some , then . The algorithm correctly sets  to  and never changes it. If  consider an image  such that . Let  be the coloring of row  in  and  be the status vector of  w.r.t.\ . Then  and the configuration 
are consistent w.r.t. . Note that  and . Moreover, for every image , there is an image  such that . (In , recolor row  to  and all rows  to all black rows and obtain image . In image ,  and  are consistent w.r.t.\  and every 1-block in its rows  is border-connected.  Thus, image  is border-connected and .) Therefore, . At some point, the algorithm considers the configuration  for row  and the coloring  for row . By the inductive assumption, the algorithm correctly computes  which is equal to . The output of \Compst for the triple  will be the vector  and the algorithm sets  to  which is the correct value. This completes the proof.
\end{proof}

By Lemma~\ref{lm:correct}, Algorithm~\ref{alg:dist} computes the cost of every configuration in row . The algorithm outputs the minimum one among these costs. Let  be an image such that . Note that configurations of row  that are not possible for a border-connected image have unbounded costs (i.e., ). Thus, row  in  has some configuration which has the minimum cost among all configuration costs for the row. Note that the cost of a configuration in row  is equal to the cost of recoloring of  to some border connected square. Therefore, the output of Algorithm~\ref{alg:dist} is equal to .

\begin{algorithm}\label{alg:status}
\caption{Subroutine \Compst used in Algorithm~\ref{alg:dist}.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{index ; vectors , and .}
\DontPrintSemicolon
\BlankLine
\nl Construct graph \tcp{Let .}
\nl \textbf{if}  \textbf{then} \Return \\
\tcp{Let }
\nl Let \\
\nl For every \\
\nl\quad \textbf{if}  and  \textbf{then} increment  by 1\\
\nl\quad \textbf{if}  and  \textbf{then} increment  by 1\\
\nl\quad \textbf{if}  and  \textbf{then} add  to \\

\nl \textbf{if}  with  such that  for all  \textbf{then} \Return 

\nl \textbf{if}  \textbf{then} \\
\nl \textbf{else}\\ \nl\quad Let . Update  and 

\nl\quad For each edge , , \textbf{if}  \textbf{then} .

\nl\quad {Run BFS to find connected components in . For each pair , where \\ \quad , \textbf{if}  and  are connected and  \textbf{then} .}

\nl\quad{For each connected component of vertices  not marked by 1, where , update \\ \quad the corresponding entries of  with the corresponding symbols in  (i.e.,  if it is\\ \quad the vertex with the smallest index in the component,  if it is the vertex with the \\ \quad largest index in the component, and , otherwise).}

\nl \Return 

\end{algorithm}


\subparagraph{Query and Time Complexity of Algorithm~\ref{alg:dist}.}
The most expensive step in Algorithm~\ref{alg:dist} is Step 3. Note that there are at most  sets  for which subroutine \Compst is called in this step. \Compst uses subroutine \Constgr to construct graph . To construct type 1 and type 2 edges in  subroutine \Constgr performs  operations. Thus, the running time of \Constgr is  (recall that , ). Therefore, Steps 1-5 of \Compst run in time . Among the remaining steps (Steps 6-10) of \Compst the most expensive ones are Steps 8 and 9. Each of them runs in time  time. Thus, the running time of \Compst is  and Algorithm~\ref{alg:dist} runs in time , as claimed.
\end{proof}

\bibliographystyle{abbrvnat}\bibliography{visual-properties}

\iffalse
\section{Property Testers for Connectedness}\label{tester_for_connectedness}
In this section we give two -testers
for connectedness of an image: nonadaptive and adaptive. The latter has better
query and time complexity. Both algorithms work as follows: at each step they
partition the image into subimages of the same size, sample one of these
subimages and test it for  border connectedness (see Definition~
\ref{def:border_connectedness}). The nonadaptive algorithm uses subroutine \emph{Exhaustive-Square-Tester}
(Algorithm~\ref{alg:exhaustive_square_tester}) to test for border connectedness, whereas the adaptive algorithm uses \emph{Diagonal-Square-Tester} (Algorithm~\ref{alg:diagonal_square_tester}) for that task.

 \begin{theorem}\label{thm:connectedness_tester}
Given a proximity parameter , connectedness of  images, where  , can be -tested nonadaptively and with 1-sided error with query and time complexity .
It can be -tested adaptively and with 1-sided error with query and time complexity .
\end{theorem}

Our  samples pixels uniformly at random and
then constructs sets of subimages of different sizes, such that subimages of the
same size belong to the same set.
It then samples subimages from each set and within every sampled subimage
it uses one of the two subroutines to test for  (border
connectedness). If it finds a subimage that violates  and a black
pixel outside that subimage it reports that the image is -far from
connectedness. It reports that the image is connected otherwise. For simplicity
of the analysis of the algorithm we assume\footnote {This assumption can be made
w.l.o.g.\ because if  for some  , instead of the original image  we can consider a  image , which is equal to  on the corresponding coordinates and has white pixels everywhere else. Let
.
To -test  for connectedness, it suffices to -test  for connectedness.
The resulting tester for  has the desired query complexity because . If  for
some , to -test a property , it suffices to run an -test for  with  .} that  and  are powers of
.

\begin{definition}[Grid pixels and squares of different levels]
\label{def:Grid_pixels_squares_of_different_levels}
For  let .
Pixels of the set  or  are called
\emph{grid pixels of level  .} For all coordinates , which are divisible by ,
the  subimage that consists of pixels  is
called a \emph{square of level}  . The set of all squares of level   is denoted .
{\em Boundary pixels} of a square of level   are the pixels of the square which are adjacent to the grid pixels of level  . A square of level   that violates property  (see Definition~\ref{def:border_connectedness}) is called a \emph{witness}.
\end{definition}

\begin{algorithm}\label{alg:connectedness_tester}
\caption{ for connectedness.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameter  ; access to a  binary
matrix .}
\DontPrintSemicolon
\BlankLine
\nl
Query  pixels uniformly at random.
\;
\nl For  to  

 (a) Sample  squares of level   (see Definition~\ref{def:Grid_pixels_and_squares_of_different_levels})
    uniformly at random with replacement.

 (b) For every sampled square  from Step 1a, let
      be the set of its pixels.
Run a border-connectedness subroutine (e.g.,
Algorithm~\ref{alg:exhaustive_square_tester} or
Algorithm~\ref{alg:diagonal_square_tester}) with inputs . If the
subroutine rejects and Step 1 detects a black pixel outside , \reject.
\;
\nl
\Accept.
\;
\end{algorithm}
\begin{algorithm}\label{alg:exhaustive_square_tester}
\caption{Border-connectedness subroutine \emph{Exhaustive-Square-Tester}.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameters ,  and ; access to
 an  matrix .}
\DontPrintSemicolon
\BlankLine
Let  be a square of level  that consists of pixels 
\;
\nl
 Query all the pixels of .
 \;
\nl
Using breadth-first search (BFS) find all connected components of black pixels
in .
 \;
\nl
 If  violates , i.e., if there is a connected component of black
 pixels that does not have a pixel on the border, \reject; otherwise,
 \accept.
 \;
\end{algorithm}
Algorithm~\ref{alg:connectedness_tester} always accepts connected images
since it will see no violation of  in Step 1b. Assume that  is an image that is -far from connectedness. To prove that the
algorithm rejects  with probability at least  and that it has the
desired query and time complexity we use
Lemmas~\ref{lm:sum_of_local_costs} and~\ref{lm:success_probability}. The
main idea behind Lemma~\ref{lm:sum_of_local_costs} is as follows: if  is
-far from connectedness there will be enough witnesses (i.e., squares
that violate ) for Algorithm~\ref{alg:connectedness_tester} to
detect at least one of them in Step 2b. In order to prove this lemma we use Claim~\ref{cl:parent_children_cost_relation} and
Claim~\ref{cl:max_dist_to_border_connectedness}.

\begin{definition}[Local cost and effective local cost]
For a fixed value of  consider a square .
The \emph{local cost} of  is . The \emph{effective local cost} of  is
.
\end{definition}
\begin{claim}
\label{cl:parent_children_cost_relation}
For any square  of level , let  denote the set of its  children (i.e.,
squares of level  inside it).
Then .
\end{claim}
\begin{proof}
If  then  and since all costs are nonnegative
the inequality above becomes trivial.

Now assume that . Then . We can modify
 pixels in  such that all its children
satisfy property . Then we can make black all pixels of  that
partition it into its children, i.e., pixels  or . There are at most  such
pixels and after this modification  will satisfy . Hence,
.
\end{proof}
\begin{claim}
\label{cl:max_dist_to_border_connectedness}
Let  be a  square. Then .
\end{claim}
\begin{proof}
 If  contains at most  black pixels, we can make all of them white, i.e.,
 modify less than  pixels, and obtain an image that satisfies .
 Assume that there are more than   black pixels in . We partition all
 pixels of  into  groups such that group  contains all pixels , where . Each group has at most  elements and making black all the elements of one group produces an image that satisfies . By the pigeonhole principle one group has at least  black pixels which means it has at most  white pixels. After making all these white pixels black we obtain an image that satisfies  and this completes the proof.
\end{proof}


\begin{lemma}
\label{lm:sum_of_local_costs}
Let  be an  image that is -far from . Then the sum of
effective local costs of squares of all levels inside  is at least .
\end{lemma}
\begin{proof}
We obtain a connected image if we make all the grid pixels of level  black and modify pixels inside every square of  to satisfy property 
 (inside every square of that level).
Thus, . Thus,
it is enough to show that
. Let  be a square of level . We will prove by induction that for any integer 


 where
 denotes the set of all
squares of level  inside  (- contains
only ).

For  (base case) the statement above is true since it is equivalent to the
statement in Claim~\ref{cl:parent_children_cost_relation}. Assume
that the statement above is true for . We will prove its correctness for
.
By the induction hypothesis 
By Claim~\ref{cl:parent_children_cost_relation}
 Thus,
 and


We showed that for any square  of level 

By Claim~\ref{cl:max_dist_to_border_connectedness} in every square of the last
level the local cost is at most , i.e., it is equal to the effective local cost of that square. Hence,
 and

\end{proof}

\begin{definition}[Diagonal lattice pixels, diamonds and fences]
\label{def:diagonal_lattice_pixels_and_regions}
For a fixed value of  consider a square in  and let
. \emph{Diagonal lattice pixels} of
the square is the set of pixels  or . Let
 be a  image whose pixels with coordinates from  are black and the remaining pixels are white.
A set of pixels of the square whose corresponding pixels
in  form a connected component is called a \emph{diamond} of the square.
A set of all diagonal lattice pixels that have some neighboring
pixel(s) from a particular diamond is called \emph{fence} of that diamond.
\end{definition}


\begin{algorithm}
\label{alg:diagonal_square_tester}
\caption{Border-connectedness subroutine \emph{Diagonal-Square-Tester}.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameters ,  and ; access to
 an  matrix .}
\DontPrintSemicolon
\BlankLine
Let  be a square of level  that consists of pixels  and
.\;
\nl Query all the pixels from  (see
Definition~\ref{def:diagonal_lattice_pixels_and_regions}) in .\;
\nl
Initialize sets  and . Put all diamonds of  that contain
a border pixel in . Put the remaining diamonds in .
\;
\nl
While  and  such that  and
 have a common portion on their fences with a black pixel, remove 
from  and put it in \;
  \nl
Query  pixels in  uniformly at random

  (a) If a black pixel from a region in  is discovered, \reject.

  (b) If a black pixel from a region in  is discovered, let  be a natural
  number chosen from a distribution whose probability
  density function is ,  (i.e.,
  ).
  Perform a BFS starting from the black pixel.
  If the search is completed within  steps, \reject. Else if  black pixels
  were found for this component, stop the search and proceed with the remaining
  queried pixels.
\;
\nl
\Accept.
\end{algorithm}
\begin{lemma}
\label{lm:success_probability}
Fix  and let  be a witness that consists of pixels
.
A border-connectedness subroutine of
Algorithm~\ref{alg:connectedness_tester} rejects  with
probability , where  for \emph{Exhaustive-Square-Tester} and  for \emph{Diagonal-Square-Tester}.
\end{lemma}
\begin{proof}
\emph{Exhaustive-Square-Tester} will determine that  is a witness with
probability . Now we prove the claim
for \emph{Diagonal-Square-Tester}. Let  denote the number of connected components
in all the regions of the set  (see Steps 2 and 3 of
Algorithm~\ref{alg:diagonal_square_tester}) and  be the number
of black pixels in all the regions of the set  after Step 3 of the subroutine.
The probability that a pixel is selected by the subroutine in Step 4 is
.
Consider one of the  connected components above and let  be the number of
pixels in it.
The probability that \emph{Diagonal-Square-Tester} finds this component completely equals
, i.e., one of the  pixels
from this component was selected in Step 4 of the subroutine and  was
chosen in Step 4b. Each of the  connected components above can be
connected to the diagonal grid pixels by modifying at most  pixels
and we can make all the  pixels above white and the square will satisfy
. Thus, .

The subroutine determines that  is a witness if it finds one of the  connected components or one of the  black pixels above.
Thus, the sum of the probabilities of determining that  is a witness  is
 .
 Since  for , it follows that the
 probability of determining that  is a witness is at least .
\end{proof}


Now we complete the proof of the theorem. For any  there are
 squares in . Let  be a witness in .
The probability that the algorithm chooses  in Step~1b is
.
By Lemma~\ref{lm:success_probability} the probability that the algorithm rejects
 is at least . Since at least an 
fraction of pixels in  are black and any square contains at most
 pixels, the probability that the
algorithm will detect a black pixel outside  in Step 2 is at least
.
Thus, the probability that the algorithm finds a witness of level  in Step 1b
is at least .
By Lemma~\ref{lm:sum_of_local_costs} . Therefore, the probability that the algorithm detects at
least one witness of any level , and the probability
that the algorithm rejects  is at least 
(this holds for both values of ).


Now we prove that the algorithm has query
complexity  if it uses \emph{Exhaustive-Square-Tester}
as a subroutine.
The algorithm samples  squares of level
 and for each sampled square it calls
\emph{Exhaustive-Square-Tester} which makes
 queries in each sampled square of level .
Thus, the query complexity of the algorithm is .
When the algorithm uses \emph{Diagonal-Square-Tester} it queries
 inside each square of
level . Then it selects  pixels and a number
 such that  and then queries at
most  pixels for each of the selected pixel. Since  the
expected number of queries inside a square of level  is at most
. The expected total number of queries is
. The time
complexity of Step 1a and Step 2 of the algorithm is .
Therefore, the total time complexity of the algorithm is
+time complexity of Step 1b. In Step 1b the algorithm
uses either \emph{Exhaustive-Square-Tester} or \emph{Diagonal-Square-Tester}. Both of them perform a breadth first search within each sampled square.
Breadth first search is linear in the sum of the number of edges and the number of nodes of the graph.
Every pixel of a sampled square has at most  neighbouring pixels.
Thus, the number of edges in the image graph of every sampled square is linear in the number of pixels inside it and the time complexity of
Step 1b is linear in the number of all queried pixels, i.e.,
 for \emph{Exhaustive-Square-Tester} and
 for
\emph{Diagonal-Square-Tester}. This completes the proof of the theorem.
\section{Property Tester for Convexity}\label{sec:convexity-tester}
\begin{theorem}\label{thm:convexity-tester}
Given , convexity of  images can be -tested
(adaptively) with  queries and 1-sided error in time
.
\end{theorem}
Our -tester for convexity (Algorithm~\ref{alg:convexity-tester})
samples pixels uniformly at random and constructs a rectangle  that with high probability contains nearly all black pixels and whose sides include sampled black pixels.
Then it adaptively queries pixels of  in order to partition it into regions , 
and . The ``fence'' region  has a small area. If the image is convex,  contains only black pixels and  contains
only white pixels.  The algorithm queries a small number of random pixels in  and rejects
if it finds a misclassified pixel (i.e., a white pixel in  or
a black pixel in ), otherwise it accepts.

Since the number of black pixels outside  and the number of
pixels in  are small, if the image is -far from
convexity then there will be enough misclassified pixels in , and the algorithm
will detect at least one of them with high probability.

\begin{algorithm}\label{alg:convexity-tester}
\caption{ for convexity.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameter  ; access to a  binary
matrix .}
\DontPrintSemicolon
\BlankLine
\nl\label{step:convexity-init}
Query  pixels uniformly at random.
If all sampled pixels are white, \accept.
\;
\nl \label{step:convexity-test-construction_of_R}
Let  be the minimum axis-parallel rectangle that contains all sampled black
pixels. Let  (resp., ) be a sampled black pixel on the
top (resp., left, bottom, right) side of .
\;
\nl\label{step:convexity-test-walk}
\For { \textbf{ \em{to} } }{
    \nl\label{step:start-walk} Let  and .\tcp{Investigate the upper right corner
of .}
    \nl \While { is in }{
        \nl\lIf{ is black or below the line through  and }
            {.} \nl  \lElse{; \ \ .
    }}
    \nl\label{step:finish-walk} Let R the rotated
coordinates Rotate  clockwise by 90 degrees. \tcp{We rotate   to reuse lines \ref{step:start-walk}-\ref{step:finish-walk} of the pseudocode for investigating all four corners.}
}
\nl\label{step:convexity-test-B-and-W}
Let  be the convex hull of all black pixels discovered so far, and
.\;
\nl\label{step:misclassified} Query  pixels in . If a white pixel in  or a
black pixel in  was detected, \reject; otherwise, \accept.\;
\end{algorithm}

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{enclosing-box.pdf}
\caption{An illustration to Step~\ref{step:convexity-test-construction_of_R} of Algorithm~\ref{alg:convexity-tester}.}
\label{fig:enclosing-box}
\end{minipage}
\hspace{0.1\linewidth}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{walk-on-the-box.pdf}
\caption{An illustration to Steps~\ref{step:convexity-test-walk}--\ref{step:convexity-test-B-and-W} of Algorithm~\ref{alg:convexity-tester}.}
\label{fig:walk-on-the-box}
\end{minipage}
\end{figure}

\begin{proof}
We prove that Algorithm~\ref{alg:convexity-tester} satisfies
Theorem~\ref{thm:convexity-tester}. (Some steps of the algorithm are illustrated in Figures~\ref{fig:enclosing-box} and~\ref{fig:walk-on-the-box}).
First, we prove that Algorithm~\ref{alg:convexity-tester} always accepts if its input is a convex image . If  has no black pixels, Step~\ref{step:convexity-init} always accepts. Otherwise, all pixels in  are black by convexity of .
We will show that all pixels in  are
white.
For the sake of contradiction, suppose there is a black pixel  in . By definition of , there is a white pixel  in  such that  and . Thus, white pixel  is inside the
triangle , formed by three black pixels, contradicting convexity of . Analogously, there are no black pixels in  and . Since there are no white pixels in  and no black pixels in ,
Step~\ref{step:misclassified} of Algorithm~\ref{alg:convexity-tester} always .

Now assume that  is -far from convexity. First, we prove the two
lemmas below:

\begin{lemma}\label{lem:black-pixels-outside-R}
The probability that there are more than  black pixels outside
 after Step~\ref{step:convexity-test-construction_of_R} of Algorithm~\ref{alg:convexity-tester}
is at most .
\end{lemma}
\begin{proof}
Let  be a horizontal line with the largest -coordinate such that the image
 contains at least  black pixels on or above . The
probability that none of these pixels are sampled in Step~1 of Algorithm~\ref{alg:convexity-tester}
(and, consequently  lies below ) is at most
. Thus, the probability that
there are more than  of black pixels in the half-plane
above  is at most . The same bound holds for the half-planes to the left, to the right and below .
By a union bound the probability that there are more than  black pixels outside  is at most
.
\end{proof}
\begin{lemma}\label{lem:total_number_of_pixels_in_F}
Let . Then  contains at most  pixels.
\begin{proof}
Let  and  (see Step~\ref{step:convexity-test-construction_of_R} of
Algorithm~\ref{alg:convexity-tester}) for .
Call a subimage that consists of pixels , where  and
, a \emph{square}. Call squares that contain pixels from 
\emph{fence squares}. Let  be the third vertex of the triangle T.
We will find an upper bound on the number of fence squares inside .
Each pixel that Algorithm~\ref{alg:convexity-tester}
queries in Step 5 discovers at most one (new) square in . The
number of fence squares is at most the number of all discovered squares in the
triangle. The algorithm queries at most 
pixels in the triangle (thus, it discovers at most that many squares), since every time it either increases the -coordinate or decreases the -coordinate of the queried pixel.
Therefore, there are at most 
fence squares in this triangle. Similarly, we can
find an upper bound on the number of discovered squares in the remaining
triangles. Since the perimeter of  is at most , the sum of the upper bounds is at most
.
The number of pixels from  in a single
fence square is at most  and
the total number of pixels from  in all
fence squares is at most .
\end{proof}
\end{lemma}

We call a pixel {\em misclassified} if it is black and is in  or if it is white and in .
If we make all pixels in  black and all pixels outside of  white, we obtain a
convex image. Thus, by Lemma~\ref{lem:total_number_of_pixels_in_F},
 contains at least  misclassified pixels if there are
at most  black pixels outside of . If the latter is the
case, the probability that the algorithm will not detect a violated pixel
is at most . By
Lemma~\ref{lem:black-pixels-outside-R} the probability that 
contains less than  misclassified pixels is at most
.
Therefore, the probability Algorithm~\ref{alg:convexity-tester} accepts is at most
, as desired.
\subparagraph{Query complexity.} The algorithm queries pixels in Steps 1,5 and 9.
In Steps 1 and 9, the algorithm makes  queries. As we mentioned earlier, the
algorithm queries  pixels in Step~5. Thus, the overall query
complexity of the algorithm is .

\subparagraph{Running time.} The running time of the algorithm in Steps 1 through
7 and Step 9 is . In Step 8, one can
obtain the convex hull of  represented by a set of
 points lexicographically sorted by their coordinates
in time . In Step 10,  is represented
by at most 4 sets of  points. In each set points are sorted
by their -coordinates (either in decreasing or increasing order) and by
their -coordinates (either in decreasing or increasing order).
We need to check whether a point is inside  or  in this step. For a single point we can do
this in time  by a
binary search. Since we query  points the running
time of Step 10 is . Therefore, the running time
of the algorithm is .
\end{proof}
\begin{theorem}\label{thm:border-con}
The distance of a  image  to border connectedness can be computed
in time .
\end{theorem}
\begin{proof}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\col}{\chi}
\newcommand{\cc}{CC-partition}
To prove the theorem, we reduce the computation of the distance of  to border connectedness (see Definition~\ref{def:border_connectedness}), to the computation of a shortest path in a directed acyclic graph (DAG). More precisely, we construct a DAG  with two special nodes  and  such that there is a bijection between paths from  to  and  border-connected images. In addition to nodes  and , graph  will have at most  other (non-special) nodes. The out-degree of every node in  will be at most . We also define a node cost function  such that for
every - path  in , there is a border connected image  with . In graph , the cost of the shortest path from  to  is equal to  (recall that  denotes border connectedness). Our algorithm for computing  is Algorithm~\ref{alg:bord-con} which relies on the construction of .

We give an intuition behind the graph . Consider the following recoloring process of : Starting from the first row we recolor the current row in  and proceed to the next row. For every , let  denote the image that we obtain after recoloring row . If the subimage of , induced by its first  rows, can be extended to a border-connected image, by appropriately recoloring the remaining rows of , we say that  is {\em BC-extendable}. Observe that  is BC-extendable for all rows  iff the final image  is border-connected. We explain how a border-connected image corresponds to an - path in  in more details. The graph  is a layered graph with  layers of nodes and  layers of edges. Nodes  and  are in layers  and , respectively. All the remaining nodes of  are in layers . For every row  of a  image , there is a corresponding node  in layer  of . Directed edges are of the form  (edges in layer 0),  (edges in layer ), and  (edges in layer ), for . Moreover,  is border-connected iff the sequence  forms a path in .

Now we explain the construction of . For every , consider the image graph  of . We obtain a modified (undirected) image graph  from  by the addition of a new node  is adjacent to every node  of  such that . Note that  is border-connected iff the graph  is connected. Let  denote the subgraph of  induced by its nodes in the set . Let  denote the set of black pixels in row  of . Set  is a {\em coloring} of row  in  ( shows how each pixel of row  is colored). Consider the partition of the set  that corresponds to the connected components of . Call this partition a {\em connected components partition (\cc)} of . For each , we define the set of non-special nodes of layer  in . These nodes are triples of the form , where
\begin{itemize}
\item
 is a {\em row number};
\item
 is a coloring of row ;
\item
 encodes the \cc\ of . We describe  in more details later.
\end{itemize}

The cost of node  is , whereas
.


Recall that  is the coloring of row  in  and  encodes the \cc\ of . Let  be the coloring of row  in  and let  be the encoding of the \cc\ of . If both  and  are BC-extendable then the pair of nodes  is {\em good} and  is {\em consistent} with . To ensure a bijection between the set of  images and the set of - paths in , we only create edges for pairs of nodes in  that are good. A key observation is that , and the \cc\ of  define the \cc\ of . Thus, for every node  in layer , we can consider all colorings , that are consistent with , and find  such that the pair  is good. Therefore, we can create all the edges of layer . Note that for every coloring  of the first row, the \cc\ of  contains only the set . The encoding of such \cc\ is . We construct the edges of , layer by layer, starting from layer 1. When we process a node  in layer , we use subroutine \Compart in order to check whether a coloring  is consistent with . This is done by checking whether every node in  is connected either to  or to a node in  in the graph . If  and  are consistent, the subroutine returns , the encoding of the \cc\ of . If they are not consistent, it returns the  symbol (i.e., edge is not created). We analyze subroutine \Compart later. After some preliminary discussion, we define directed edges of . There are three types of edges in :
\begin{itemize}
\item
edges , for every  (edges of layer 0);
\item
edges , for every  (edges of layer );
\item
edges , for every  (edges of layer ).
\end{itemize}

Observe that, indeed, every  image  has a corresponding sequence of nodes , where  is a node in layer . The way we created edges assures that this sequence of nodes is a path iff  is border-connected. Moreover, the sum .

Now describe the encoding  of a \cc\ .
\begin{definition}\label{def:block}
For , let  be the coloring of row  in . Define graph  as the subgraph of  induced by the nodes in . Call every connected component in  {\em block}. Let  denote the number of blocks in .
\end{definition}
\begin{claim}\label{cl:part}
For , let  be the coloring of row  in . Then every possible \cc\ of  can be encoded as a binary string of length at most .
\end{claim}

Assume that Claim~\ref{cl:part} holds (we give its proof later) and that \Compart works correctly, we are ready to give the algorithm that compute the distance to border connectedness of the image  (Algorithm~\ref{alg:bord-con}).

\begin{algorithm}
\caption{Computing the distance to border connectedness.}
\label{alg:bord-con}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{a  image .}
\DontPrintSemicolon
\BlankLine
\tcp{Construct graph , where  and .
Initially , , , and , for all .}

\nl \ \tcp{ is the cost function of a node in }
\nl \ForAll {sets  and strings }
 \do{\nl\quad Put node  in  and edge  in \\
       \tcp{and compute the cost of node }
       \nl\quad \\

 }
\nl \ForAll {, , , and }
   \do {\nl\quad \\
          \nl\quad\textbf{if}  \textbf{then}\\
          \nl\quad\quad put  in  and  in \\
           \tcp{and compute the cost of node }
           \nl\quad\quad \\
}
\nl \ForAll {sets  and strings }
\do{\nl\quad Put  in \\}
\nl Run Dijkstra's algorithm to find the cost of the shortest path from  to  in  and output that cost divided by .\\
\end{algorithm}

\begin{algorithm}
\caption{Subroutine \Compart.}
\label{alg:encode}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{row number , sets , , and  .}
\DontPrintSemicolon
\BlankLine

\end{algorithm}






For , consider a connectivity partition . Recall the graph . Then nodes  and  of , that correspond to some black pixels in row  of , are in the same part of  iff they are in the same connected component of . To count all possible connectivity partitions  for node  of some - path, we observe that each such partition can be described as connected components of an outer-planar graph  that we define next:
The nodes of  are  and the edges are formed by the following rules:
\begin{enumerate}
\item for all , if  and  are both in  then there is an edge between them in , where .
\item for all , there is an edge between  and  in  if the nodes are in the same connected component in , where . Moreover, for any , such that  nodes  and  are not in the same connected component in .

Observe that graph  is outer-planar.


 and  are in the same component in , whereas  and 
\end{enumerate}
cycle edges that form the cycle  and
-{\it connections} which are
pairs  such that in  there is a path from  to .
To assure that  is planar we will properly select -connections
that span a connected component, so they never form
"crossing pairs" of -connections of the form
 and
 where .  For example, we can pick
a representative of a connected component and use connections of every
other member to the representative.
Note that if we have such a "crossing pair", the four nodes in this edge
pair are in the same component because these two edges correspond to
paths in  that have a node in common.

There are  row numbers and  possible colorings . It remains to count all possible partitions  for node  of some sequence



This graph has these types of edges:
(a)  such that  and , (b)
 such that ,
(c) 
such that  and  are in the same part of , (d) 
such that  and  are in the same part of .  Then partition 
of  must
correspond to the partition of  that is induced by
the connected components of . We add a condition on  to enforce that paths correspond to border
connected sets.  In the graph , every  must be in the same connected component either with  or with
a node from . In the case of edge , partition  has one set that
contains  and every  such that . Finally, edges  have
no restriction. Note that  can be constructed in time .

Now it remains to verify the size of .  The number of possible values
of  is clearly .  In directed edges  and
 the partition  is determined, hence
each node of  has at most  out-going edges.  It remains to count
the number of possible partitions  for a node of the form .

We observe that those partitions can be described as connected components
in an outer-planar graph with  nodes in which we
remove the edges of the outer face.  The lemma below shows that there
are at most  such partitions, which concludes the proof of the theorem.



\begin{lemma} Consider outer-planar graphs with node set  and with the
outer face being the cycle .  There are at most 
distinct partitions of the nodes of those graphs into connected components
made by the edges that are not on the outer face.\end{lemma}

\begin{proof}
For every node  we define two bits:  if this node is the first one
(has the minimal number) in its component, and  if this node is the
last one in its component.  Note that .  We can use those bits
and a stack to determine the partition.  The stack contains representatives of
the parts that contain at least one node that was not examined yet.  We process
bits in order ; if  we push  onto
the stack, otherwise we include it in the same component as the top of the
stac{k'}; if  we pop the stack.  Because we cannot pop empty stac{k'} this
clearly imposes further limitation on the encoding, for example,  iff
the stack is empty after processing of .
\end{proof}



{\bf Analysis of Algorithm~\ref{alg:bord-con}}. There are  rows and the number of all possible colorings is . By Claim~\ref{cl:part}, there are at most  encodings for a coloring of a row. Thus, the number of nodes in  is at most . Each node has out-degree at most  and thus, the number of edges in  is at most . Construction of  is done in  time. Since  is a DAG on at most  nodes and at most  edges the running time of the algorithm is , as claimed.
\end{proof}



\fi




\iffalse
\subsection{Improved Approximation for Distance to Connectedness}\label{sec:improved-approximation-for-connectedness}
\begin{theorem}
Fix parameters  and . There exists an algorithm ...
with query complexity .
\end{theorem}
At a high level, the algorithm partitions the image into smaller images of size  and samples  of them uniformly at random. Then the algorithm proceeds as the following: at each level  the algorithm partitions each square image of the previous level () into four equal sub-squares of the current level () and samples  of all squares of the current level uniformly at random. For each sampled image the algorithm evaluates the \emph{net-cost}. Based on the net-cost and a \emph{threshold value} that is evaluated at each level the algorithm computes the \emph{reported-cost} of each sampled square and takes the average. Both the net-cost and reported-cost of the sampled image are evaluated recursively from bottom to up by considering all the images in it starting from the smallest ones. The algorithm outputs the sum of all values(averages) that were computed at each level.

To compute the net-cost and reported-cost of a square we use a black box that takes   image, \emph{super pixels} and \emph{super pixels' cost} of the image as an input and outputs the distance to property . To evaluate the net-cost of each smallest square the black box takes only the square as an input. The reported-cost of each such square is  if its net-cost, where  is the perimeter of the image, and it is equal to the net-cost otherwise.
\fi

\begin{comment}
\subsection{Tester for Connectedness}\label{sec:connectedness-tester}
\begin{theorem}\label{thm:connectedness_tester}
There is a block-uniform (1-sided error) -tester for connectedness with sample and time complexity .
\end{theorem}
\begin{proof}
Observe that a tester can safely reject if it finds a small connected component and a black pixel outside it.
Our tester (Algorithm~\ref{alg:connectedness_tester-na}) looks for squares that contain small connected components, that is, are not border-connected, which we call witnesses. To find a witness, it samples -squares for  values of  which we call levels. In each subsequent level, the number of samples is doubled, but the side length of the squares is halved, i.e., the number of pixels in them is divided by 4.
If it finds a witness, it samples pixels to look for black pixels outside the witness.

\ifnum\full=1
For simplicity
of the analysis of the algorithm we assume\footnote {This assumption can be made
w.l.o.g.\ because if  for some , instead of the original image  we can consider a  image , which is equal to  on the corresponding coordinates and has white pixels everywhere else. Let
.
To -test  for connectedness, it suffices to -test  for connectedness.
The resulting tester for  has the desired query complexity because . If  for
some , to -test a property , it is sufficient to run an -test for  with  .}
\else
We can assume w.l.o.g.\
\fi
that 
and  are powers of
.
\begin{definition}[Levels, witnesses]
\label{def:Grid_pixels_and_squares_of_different_levels}
For  let  denote the length of level .
Pixels of the set  are called
\emph{grid pixels of level }, and squares in the set   are called \emph{squares of level} .
A square of level  which is not border-connected (see Definition~\ref{def:border_connectedness}) is called a \emph{witness}.
\end{definition}

\begin{algorithm}\label{alg:connectedness_tester-na}
\caption{ for connectedness.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{ and ; block-sample access to an  binary matrix .}

\DontPrintSemicolon
\BlankLine
\nl\label{step:border-connectedness-violation}
 For  to  

 (a) Sample  squares of level  (see Definition~\ref{def:Grid_pixels_and_squares_of_different_levels})
    uniformly at random with replacement.

 (b) For every sampled square  from
 Step~\ref{step:border-connectedness-violation}a,
use a breadth-first search (BFS)  to find all connected components of the image graph . If there is a connected component that does not have a pixel on the border of , mark  is a witness, and proceed to Step~\ref{step:conn-tester-pixel-sampling}.

\nl\label{step:conn-tester-pixel-sampling}
Sample  pixels uniformly at random. If a witness is found in Step~\ref{step:border-connectedness-violation} and the sample contains a black pixel outside the witness, \reject; otherwise, \accept.
\;
\end{algorithm}

We start the analysis of Algorithm~\ref{alg:connectedness_tester-na} by relating the distance of an image to connectedness to the fraction of witnesses it contains at different levels.
\begin{lemma}
\label{lm:repair_witnesses}
Consider an  image  and let  denote the fraction of witnesses among all squares in  for all  . Then
.
\end{lemma}
\begin{proof}
We will show how to make  connected by changing the required fraction of pixels.
First, we make all pixels in  black. By Claim~\ref{claim:GP-size}, there are at most  of them.
Then we recursively ``repair'' the witnesses, starting from the witnesses in , as follows. Inside each witness   we make the pixels in  black, and call the set of modified pixels the \emph{cross} of . Then we call the repair procedure on all witnesses in  which are inside . At each level, all pixels that belong to the central cross of a witness are connected to the grid pixels of level . After processing witnesses of level , which  are  squares, the image is connected.
There are  squares in .
 In each witness of level   we modify at most  pixels.
All together, at level   we modify at most  pixels. Overall, at most  fraction of pixels is modified in  to obtain a connected image, as claimed.
\end{proof}

\paragraph{Analysis of Algorithm~\ref{alg:connectedness_tester-na}.}
If the input image  is connected, the algorithm always accepts because there are no witnesses.

Consider an image  that is -far from connected, i.e., . Then by Lemma~\ref{lm:repair_witnesses}, .
For fixed   the probability that Algorithm~\ref{alg:connectedness_tester-na} fails to detect a witness of level   is
. Thus, the overall probability that it fails to detect a witness is at most


Assume that the algorithm detects a witness . There are at most  pixels  in . Since , there are at least  black pixels in , and at least   of them are outside .  The probability that Algorithm~\ref{alg:connectedness_tester-na} fails to detect at least one of these black pixels in Step~\ref{step:conn-tester-pixel-sampling} is at most . Thus, the algorithm detects a witness and a black pixel outside it with probability at least  as required.




\paragraph{Query and time complexity.}
The algorithm samples  squares of , for
, and inside each square queries
 pixels.
Thus, the query complexity of the algorithm is

The time complexity is linear in the number of samples, since BFS takes time linear in the number of vertices on bounded-degree graphs, and the remaining steps can be easily implemented to run in the time proportional to the number of sampled pixels.
\end{proof}
\end{comment}

\iffalse
\appendix

\section{Algorithm for Border Connectedness}
\label{sec:border-con}
\newcommand{\Out}{\mbox{\it Out}}
\newcommand{\cMb}{\overline{{\mathcal M}}}
\newcommand{\Be}{{\bf B}}

This section contains the proof of the following lemma.

\begin{lemma}
\label{thm:border-con}
Let  be a  image. There is an algorithm that computes  in time .
\end{lemma}

\begin{proof}
To prove the theorem we give a dynamic programming algorithm that computes  in the following way: Starting from row 1 of , it processes a row and proceeds to the next one. The algorithm stops after processing row . For each , it computes the cost of every recoloring of the first  rows of  according to the first  rows of every  border-connected image. Among the costs that it finds while processing the last row , the algorithm outputs the minimum one.

\begin{definition}
\label{def:block}
For , let  be a vector that corresponds to the  row of . Let  denote a recoloring for a row in . Call maximal consecutive runs of 's in  {\em 1-blocks} and let  denote the number of 1-blocks in . Let .
\end{definition}

Consider a  image . Recall that  denotes the image graph of . For every , denote the subgraph of , induced by the first  rows in , by . Index 1-blocks in row  of  in the nondecreasing order of indices of pixels they contain. For example, a row  contains two 1-blocks; the 1-block with three 's  has index 1, the 1-block with two 's has index 2. Each 1-block in row  has one of the following 5 statuses w.r.t.\ :
\begin{itemize}
\item{connected to the border of  (denoted by 1);}
\item{isolated, i.e., connected neither to the border nor to any other 1-block in its row (denoted by 0);}
\item{first 1-block in its connected component, i.e., it is in the same connected component with other 1-blocks of row  and has the smallest index among them (denoted by );}
\item{intermediate 1-block in its connected component, i.e., has neither largest nor smallest index in its connected component (denoted by ``'');}
\item{last 1-block in its connected component, i.e., it is in the same connected component with other 1-blocks of row  and has the largest index among them (denoted by ).}
\end{itemize}

The algorithm that we give is Algorithm~\ref{alg:dist}. The reader may look at its pseudocode at this point. Statuses of 1-blocks in  are captured by a {\em status vector} . Call the pair  {\em configuration}. Let  denote the coloring of row  in . Define , for , , and . Let  denote the fraction of pixels modified in  after recoloring its first  rows according to the first  rows of . Define . Let  denote the coloring of row  in . Configuration  is {\em consistent} with coloring , i.e., every 1-block in  that has status other than 1 is connected to a 1-block in  w.r.t.\ . Moreover,  has a status vector  that can be determined from , and . Suppose  is known for every configuration . Thus, if we find status vector  for all colorings  consistent with , we can compute . After computing costs for all sets , we find  which will be equal to .

\begin{algorithm}
\caption{Distance to border connectedness of a square .}
\label{alg:dist}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{access to a  square .}
\DontPrintSemicolon
\BlankLine
\nl\label{st:init}\ForAll  {indices , vectors , and } \do{
\tcp{Let  denote the string of  1's.}
\nl\quad\\}

\nl \label{st:subsequent} \ForAll {indices , vectors  and }
   \do{
\nl\quad \textbf{if}  \textbf{then}

\nl\quad\quad\\
\nl\quad\textbf{if}  \textbf{then}

\nl\quad\quad

}


\nl \label{st:actual-min}\Return 


\end{algorithm}

\subparagraph{Analysis of Algorithm~\ref{alg:dist}.}
We prove the following statement that shows correctness of the algorithm: For each , Algorithm~\ref{alg:dist} computes the minimum cost of each configuration in row . We prove this statement inductively. For the first row, the algorithm indeed computes minimum costs for every possible configuration. (Note that every 1-block in the first row is connected to the border and thus, every such 1-block has status .) Let us assume that the statement is true for some row . We prove the statement for row . Note that each 1-block of a coloring in row  has one of the 5 statuses that we introduced. Let  denote the configuration in row . The algorithm considers all consistent colorings  of row  and finds its status vector  by using subroutine \Compst. The algorithm updates the cost of  to \{cost of , cost of \}. Thus, for each configuration of row , the algorithm will eventually find the minimum cost. The algorithm computes the cost of every configuration in row  and outputs the minimum one among these costs. Let  be an image such that . Row  in  has some configuration which has the minimum cost among all configuration costs for the row. Note that the cost of a configuration in row  is equal to the cost of recoloring of  to some border connected square. Therefore, the output of the algorithm is equal to . This completes the correctness proof.

We show how subroutine \Compst computes . For each  and every pair of colorings ,, let  denote the image obtained from  after recoloring its row  and row  to  and , respectively. Let  be the status vector of . Note that  provides information about which 1-blocks in  are connected. Let  and . Index 1-blocks in  in the nondecreasing order of indices of pixels they contain. Index 1-blocks in  in the nondecreasing order of indices of pixels they contain and add  to each index. Construct graph  such that  and  has every edge of the following two types:
\begin{enumerate}
\item edges , where , , and  is not connected to any 
\item , where , and  is connected to  in .
\end{enumerate}

We say that  {\em corresponds to} , , and . Graph  can be constructed in  time.

\begin{algorithm}\label{alg:conf}
\caption{Subroutine \Compst used in Algorithm~\ref{alg:dist}.}
\label{alg:conf}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{index ; vectors , and .}
\DontPrintSemicolon
\BlankLine


\nl Construct graph  that corresponds to , , and . Let  and \\
\tcp{Let  (resp, ) denote the strings of  1's (resp, 0's). }
\nl \textbf{if}  let . Update  and  \textbf{else} 

\nl For every pair , , \textbf{if}  \text{then} .

\nl Run BFS to determine connected components in . For every pair , , \textbf{if}  and  are connected and  \textbf{then} .

\nl \textbf{if}  \textbf{then} for every connected component of vertices , where , that are not marked by 1, update the corresponding entries of  with the corresponding symbols in  (i.e.,  if it is the vertex with the smallest index in the component,  if it is the vertex with the largest index in the component, and , otherwise).

\nl \textbf{if} each  such that  is connected to a 1-block in row , \textbf{then} \Return 

\nl \textbf{else} \Return 

\end{algorithm}
The most expensive step in Algorithm~\ref{alg:dist} is Step 3. Note that there are at most  sets . Thus, Step 3 of the algorithm runs in time  (subroutine \Compst runs in time ). Thus, the running time of the algorithm is , as claimed.


\subsection{Improved Approximation for Distance to Connectedness}\label{sec:improved-approximation-for-connectedness}
\begin{theorem}
Fix parameters  and . There exists an algorithm ...
with query complexity .
\end{theorem}
At a high level, the algorithm partitions the image into smaller images of size  and samples  of them uniformly at random. Then the algorithm proceeds as the following: at each level  the algorithm partitions each square image of the previous level () into four equal sub-squares of the current level () and samples  of all squares of the current level uniformly at random. For each sampled image the algorithm evaluates the \emph{net-cost}. Based on the net-cost and a \emph{threshold value} that is evaluated at each level the algorithm computes the \emph{reported-cost} of each sampled square and takes the average. Both the net-cost and reported-cost of the sampled image are evaluated recursively from bottom to up by considering all the images in it starting from the smallest ones. The algorithm outputs the sum of all values(averages) that were computed at each level.

To compute the net-cost and reported-cost of a square we use a black box that takes   image, \emph{super pixels} and \emph{super pixels' cost} of the image as an input and outputs the distance to property . To evaluate the net-cost of each smallest square the black box takes only the square as an input. The reported-cost of each such square is  if its net-cost, where  is the perimeter of the image, and it is equal to the net-cost otherwise.



\end{proof}





\iffalse
\subsection{Improved Approximation for Distance to Connectedness}\label{sec:improved-approximation-for-connectedness}
\begin{theorem}
Fix parameters  and . There exists an algorithm ...
with query complexity .
\end{theorem}
At a high level, the algorithm partitions the image into smaller images of size  and samples  of them uniformly at random. Then the algorithm proceeds as the following: at each level  the algorithm partitions each square image of the previous level () into four equal sub-squares of the current level () and samples  of all squares of the current level uniformly at random. For each sampled image the algorithm evaluates the \emph{net-cost}. Based on the net-cost and a \emph{threshold value} that is evaluated at each level the algorithm computes the \emph{reported-cost} of each sampled square and takes the average. Both the net-cost and reported-cost of the sampled image are evaluated recursively from bottom to up by considering all the images in it starting from the smallest ones. The algorithm outputs the sum of all values(averages) that were computed at each level.

To compute the net-cost and reported-cost of a square we use a black box that takes   image, \emph{super pixels} and \emph{super pixels' cost} of the image as an input and outputs the distance to property . To evaluate the net-cost of each smallest square the black box takes only the square as an input. The reported-cost of each such square is  if its net-cost, where  is the perimeter of the image, and it is equal to the net-cost otherwise.
\fi


\iffalse
\section{Property Testers for Connectedness}\label{tester_for_connectedness}
In this section we give two -testers
for connectedness of an image: nonadaptive and adaptive. The latter has better
query and time complexity. Both algorithms work as follows: at each step they
partition the image into subimages of the same size, sample one of these
subimages and test it for  border connectedness (see Definition~
\ref{def:border_connectedness}). The nonadaptive algorithm uses subroutine \emph{Exhaustive-Square-Tester}
(Algorithm~\ref{alg:exhaustive_square_tester}) to test for border connectedness, whereas the adaptive algorithm uses \emph{Diagonal-Square-Tester} (Algorithm~\ref{alg:diagonal_square_tester}) for that task.

 \begin{theorem}\label{thm:connectedness_tester}
Given a proximity parameter , connectedness of  images, where  , can be -tested nonadaptively and with 1-sided error with query and time complexity .
It can be -tested adaptively and with 1-sided error with query and time complexity .
\end{theorem}

Our  samples pixels uniformly at random and
then constructs sets of subimages of different sizes, such that subimages of the
same size belong to the same set.
It then samples subimages from each set and within every sampled subimage
it uses one of the two subroutines to test for  (border
connectedness). If it finds a subimage that violates  and a black
pixel outside that subimage it reports that the image is -far from
connectedness. It reports that the image is connected otherwise. For simplicity
of the analysis of the algorithm we assume\footnote {This assumption can be made
w.l.o.g.\ because if  for some  , instead of the original image  we can consider a  image , which is equal to  on the corresponding coordinates and has white pixels everywhere else. Let
.
To -test  for connectedness, it suffices to -test  for connectedness.
The resulting tester for  has the desired query complexity because . If  for
some , to -test a property , it suffices to run an -test for  with  .} that  and  are powers of
.

\begin{definition}[Grid pixels and squares of different levels]
\label{def:Grid_pixels_squares_of_different_levels}
For  let .
Pixels of the set  or  are called
\emph{grid pixels of level  .} For all coordinates , which are divisible by ,
the  subimage that consists of pixels  is
called a \emph{square of level}  . The set of all squares of level   is denoted .
{\em Boundary pixels} of a square of level   are the pixels of the square which are adjacent to the grid pixels of level  . A square of level   that violates property  (see Definition~\ref{def:border_connectedness}) is called a \emph{witness}.
\end{definition}

\begin{algorithm}\label{alg:connectedness_tester}
\caption{ for connectedness.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameter  ; access to a  binary
matrix .}
\DontPrintSemicolon
\BlankLine
\nl
Query  pixels uniformly at random.
\;
\nl For  to  

 (a) Sample  squares of level   (see Definition~\ref{def:Grid_pixels_and_squares_of_different_levels})
    uniformly at random with replacement.

 (b) For every sampled square  from Step 1a, let
      be the set of its pixels.
Run a border-connectedness subroutine (e.g.,
Algorithm~\ref{alg:exhaustive_square_tester} or
Algorithm~\ref{alg:diagonal_square_tester}) with inputs . If the
subroutine rejects and Step 1 detects a black pixel outside , \reject.
\;
\nl
\Accept.
\;
\end{algorithm}
\begin{algorithm}\label{alg:exhaustive_square_tester}
\caption{Border-connectedness subroutine \emph{Exhaustive-Square-Tester}.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameters ,  and ; access to
 an  matrix .}
\DontPrintSemicolon
\BlankLine
Let  be a square of level  that consists of pixels 
\;
\nl
 Query all the pixels of .
 \;
\nl
Using breadth-first search (BFS) find all connected components of black pixels
in .
 \;
\nl
 If  violates , i.e., if there is a connected component of black
 pixels that does not have a pixel on the border, \reject; otherwise,
 \accept.
 \;
\end{algorithm}
Algorithm~\ref{alg:connectedness_tester} always accepts connected images
since it will see no violation of  in Step 1b. Assume that  is an image that is -far from connectedness. To prove that the
algorithm rejects  with probability at least  and that it has the
desired query and time complexity we use
Lemmas~\ref{lm:sum_of_local_costs} and~\ref{lm:success_probability}. The
main idea behind Lemma~\ref{lm:sum_of_local_costs} is as follows: if  is
-far from connectedness there will be enough witnesses (i.e., squares
that violate ) for Algorithm~\ref{alg:connectedness_tester} to
detect at least one of them in Step 2b. In order to prove this lemma we use Claim~\ref{cl:parent_children_cost_relation} and
Claim~\ref{cl:max_dist_to_border_connectedness}.

\begin{definition}[Local cost and effective local cost]
For a fixed value of  consider a square .
The \emph{local cost} of  is . The \emph{effective local cost} of  is
.
\end{definition}
\begin{claim}
\label{cl:parent_children_cost_relation}
For any square  of level , let  denote the set of its  children (i.e.,
squares of level  inside it).
Then .
\end{claim}
\begin{proof}
If  then  and since all costs are nonnegative
the inequality above becomes trivial.

Now assume that . Then . We can modify
 pixels in  such that all its children
satisfy property . Then we can make black all pixels of  that
partition it into its children, i.e., pixels  or . There are at most  such
pixels and after this modification  will satisfy . Hence,
.
\end{proof}
\begin{claim}
\label{cl:max_dist_to_border_connectedness}
Let  be a  square. Then .
\end{claim}
\begin{proof}
 If  contains at most  black pixels, we can make all of them white, i.e.,
 modify less than  pixels, and obtain an image that satisfies .
 Assume that there are more than   black pixels in . We partition all
 pixels of  into  groups such that group  contains all pixels , where . Each group has at most  elements and making black all the elements of one group produces an image that satisfies . By the pigeonhole principle one group has at least  black pixels which means it has at most  white pixels. After making all these white pixels black we obtain an image that satisfies  and this completes the proof.
\end{proof}


\begin{lemma}
\label{lm:sum_of_local_costs}
Let  be an  image that is -far from . Then the sum of
effective local costs of squares of all levels inside  is at least .
\end{lemma}
\begin{proof}
We obtain a connected image if we make all the grid pixels of level  black and modify pixels inside every square of  to satisfy property 
 (inside every square of that level).
Thus, . Thus,
it is enough to show that
. Let  be a square of level . We will prove by induction that for any integer 


 where
 denotes the set of all
squares of level  inside  (- contains
only ).

For  (base case) the statement above is true since it is equivalent to the
statement in Claim~\ref{cl:parent_children_cost_relation}. Assume
that the statement above is true for . We will prove its correctness for
.
By the induction hypothesis 
By Claim~\ref{cl:parent_children_cost_relation}
 Thus,
 and


We showed that for any square  of level 

By Claim~\ref{cl:max_dist_to_border_connectedness} in every square of the last
level the local cost is at most , i.e., it is equal to the effective local cost of that square. Hence,
 and

\end{proof}

\begin{definition}[Diagonal lattice pixels, diamonds and fences]
\label{def:diagonal_lattice_pixels_and_regions}
For a fixed value of  consider a square in  and let
. \emph{Diagonal lattice pixels} of
the square is the set of pixels  or . Let
 be a  image whose pixels with coordinates from  are black and the remaining pixels are white.
A set of pixels of the square whose corresponding pixels
in  form a connected component is called a \emph{diamond} of the square.
A set of all diagonal lattice pixels that have some neighbouring
pixel(s) from a particular diamond is called \emph{fence} of that diamond.
\end{definition}


\begin{algorithm}
\label{alg:diagonal_square_tester}
\caption{Border-connectedness subroutine \emph{Diagonal-Square-Tester}.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameters ,  and ; access to
 an  matrix .}
\DontPrintSemicolon
\BlankLine
Let  be a square of level  that consists of pixels  and
.\;
\nl Query all the pixels from  (see
Definition~\ref{def:diagonal_lattice_pixels_and_regions}) in .\;
\nl
Initialize sets  and . Put all diamonds of  that contain
a border pixel in . Put the remaining diamonds in .
\;
\nl
While  and  such that  and
 have a common portion on their fences with a black pixel, remove 
from  and put it in \;
  \nl
Query  pixels in  uniformly at random

  (a) If a black pixel from a region in  is discovered, \reject.

  (b) If a black pixel from a region in  is discovered, let  be a natural
  number chosen from a distribution whose probability
  density function is ,  (i.e.,
  ).
  Perform a BFS starting from the black pixel.
  If the search is completed within  steps, \reject. Else if  black pixels
  were found for this component, stop the search and proceed with the remaining
  queried pixels.
\;
\nl
\Accept.
\end{algorithm}
\begin{lemma}
\label{lm:success_probability}
Fix  and let  be a witness that consists of pixels
.
A border-connectedness subroutine of
Algorithm~\ref{alg:connectedness_tester} rejects  with
probability , where  for \emph{Exhaustive-Square-Tester} and  for \emph{Diagonal-Square-Tester}.
\end{lemma}
\begin{proof}
\emph{Exhaustive-Square-Tester} will determine that  is a witness with
probability . Now we prove the claim
for \emph{Diagonal-Square-Tester}. Let  denote the number of connected components
in all the regions of the set  (see Steps 2 and 3 of
Algorithm~\ref{alg:diagonal_square_tester}) and  be the number
of black pixels in all the regions of the set  after Step 3 of the subroutine.
The probability that a pixel is selected by the subroutine in Step 4 is
.
Consider one of the  connected components above and let  be the number of
pixels in it.
The probability that \emph{Diagonal-Square-Tester} finds this component completely equals
, i.e., one of the  pixels
from this component was selected in Step 4 of the subroutine and  was
chosen in Step 4b. Each of the  connected components above can be
connected to the diagonal grid pixels by modifying at most  pixels
and we can make all the  pixels above white and the square will satisfy
. Thus, .

The subroutine determines that  is a witness if it finds one of the  connected components or one of the  black pixels above.
Thus, the sum of the probabilities of determining that  is a witness  is
 .
 Since  for , it follows that the
 probability of determining that  is a witness is at least .
\end{proof}


Now we complete the proof of the theorem. For any  there are
 squares in . Let  be a witness in .
The probability that the algorithm chooses  in Step~1b is
.
By Lemma~\ref{lm:success_probability} the probability that the algorithm rejects
 is at least . Since at least an 
fraction of pixels in  are black and any square contains at most
 pixels, the probability that the
algorithm will detect a black pixel outside  in Step 2 is at least
.
Thus, the probability that the algorithm finds a witness of level  in Step 1b
is at least .
By Lemma~\ref{lm:sum_of_local_costs} . Therefore, the probability that the algorithm detects at
least one witness of any level , and the probability
that the algorithm rejects  is at least 
(this holds for both values of ).


Now we prove that the algorithm has query
complexity  if it uses \emph{Exhaustive-Square-Tester}
as a subroutine.
The algorithm samples  squares of level
 and for each sampled square it calls
\emph{Exhaustive-Square-Tester} which makes
 queries in each sampled square of level .
Thus, the query complexity of the algorithm is .
When the algorithm uses \emph{Diagonal-Square-Tester} it queries
 inside each square of
level . Then it selects  pixels and a number
 such that  and then queries at
most  pixels for each of the selected pixel. Since  the
expected number of queries inside a square of level  is at most
. The expected total number of queries is
. The time
complexity of Step 1a and Step 2 of the algorithm is .
Therefore, the total time complexity of the algorithm is
+time complexity of Step 1b. In Step 1b the algorithm
uses either \emph{Exhaustive-Square-Tester} or \emph{Diagonal-Square-Tester}. Both of them perform a breadth first search within each sampled square.
Breadth first search is linear in the sum of the number of edges and the number of nodes of the graph.
Every pixel of a sampled square has at most  neighbouring pixels.
Thus, the number of edges in the image graph of every sampled square is linear in the number of pixels inside it and the time complexity of
Step 1b is linear in the number of all queried pixels, i.e.,
 for \emph{Exhaustive-Square-Tester} and
 for
\emph{Diagonal-Square-Tester}. This completes the proof of the theorem.
\section{Property Tester for Convexity}\label{sec:convexity-tester}
\begin{theorem}\label{thm:convexity-tester}
Given , convexity of  images can be -tested
(adaptively) with  queries and 1-sided error in time
.
\end{theorem}
Our -tester for convexity (Algorithm~\ref{alg:convexity-tester})
samples pixels uniformly at random and constructs a rectangle  that with high probability contains nearly all black pixels and whose sides include sampled black pixels.
Then it adaptively queries pixels of  in order to partition it into regions , 
and . The ``fence'' region  has a small area. If the image is convex,  contains only black pixels and  contains
only white pixels.  The algorithm queries a small number of random pixels in  and rejects
if it finds a misclassified pixel (i.e., a white pixel in  or
a black pixel in ), otherwise it accepts.

Since the number of black pixels outside  and the number of
pixels in  are small, if the image is -far from
convexity then there will be enough misclassified pixels in , and the algorithm
will detect at least one of them with high probability.

\begin{algorithm}\label{alg:convexity-tester}
\caption{ for convexity.}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{parameter  ; access to a  binary
matrix .}
\DontPrintSemicolon
\BlankLine
\nl\label{step:convexity-init}
Query  pixels uniformly at random.
If all sampled pixels are white, \accept.
\;
\nl \label{step:convexity-test-construction_of_R}
Let  be the minimum axis-parallel rectangle that contains all sampled black
pixels. Let  (resp., ) be a sampled black pixel on the
top (resp., left, bottom, right) side of .
\;
\nl\label{step:convexity-test-walk}
\For { \textbf{ \em{to} } }{
    \nl\label{step:start-walk} Let  and .\tcp{Investigate the upper right corner
of .}
    \nl \While { is in }{
        \nl\lIf{ is black or below the line through  and }
            {.} \nl  \lElse{; \ \ .
    }}
    \nl\label{step:finish-walk} Let R the rotated
coordinates Rotate  clockwise by 90 degrees. \tcp{We rotate   to reuse lines \ref{step:start-walk}-\ref{step:finish-walk} of the pseudocode for investigating all four corners.}
}
\nl\label{step:convexity-test-B-and-W}
Let  be the convex hull of all black pixels discovered so far, and
.\;
\nl\label{step:misclassified} Query  pixels in . If a white pixel in  or a
black pixel in  was detected, \reject; otherwise, \accept.\;
\end{algorithm}

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{enclosing-box.pdf}
\caption{An illustration to Step~\ref{step:convexity-test-construction_of_R} of Algorithm~\ref{alg:convexity-tester}.}
\label{fig:enclosing-box}
\end{minipage}
\hspace{0.1\linewidth}
\begin{minipage}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{walk-on-the-box.pdf}
\caption{An illustration to Steps~\ref{step:convexity-test-walk}--\ref{step:convexity-test-B-and-W} of Algorithm~\ref{alg:convexity-tester}.}
\label{fig:walk-on-the-box}
\end{minipage}
\end{figure}

\begin{proof}
We prove that Algorithm~\ref{alg:convexity-tester} satisfies
Theorem~\ref{thm:convexity-tester}. (Some steps of the algorithm are illustrated in Figures~\ref{fig:enclosing-box} and~\ref{fig:walk-on-the-box}).
First, we prove that Algorithm~\ref{alg:convexity-tester} always accepts if its input is a convex image . If  has no black pixels, Step~\ref{step:convexity-init} always accepts. Otherwise, all pixels in  are black by convexity of .
We will show that all pixels in  are
white.
For the sake of contradiction, suppose there is a black pixel  in . By definition of , there is a white pixel  in  such that  and . Thus, white pixel  is inside the
triangle , formed by three black pixels, contradicting convexity of . Analogously, there are no black pixels in  and . Since there are no white pixels in  and no black pixels in ,
Step~\ref{step:misclassified} of Algorithm~\ref{alg:convexity-tester} always .

Now assume that  is -far from convexity. First, we prove the two
lemmas below:

\begin{lemma}\label{lem:black-pixels-outside-R}
The probability that there are more than  black pixels outside
 after Step~\ref{step:convexity-test-construction_of_R} of Algorithm~\ref{alg:convexity-tester}
is at most .
\end{lemma}
\begin{proof}
Let  be a horizontal line with the largest -coordinate such that the image
 contains at least  black pixels on or above . The
probability that none of these pixels are sampled in Step~1 of Algorithm~\ref{alg:convexity-tester}
(and, consequently  lies below ) is at most
. Thus, the probability that
there are more than  of black pixels in the half-plane
above  is at most . The same bound holds for the half-planes to the left, to the right and below .
By a union bound the probability that there are more than  black pixels outside  is at most
.
\end{proof}
\begin{lemma}\label{lem:total_number_of_pixels_in_F}
Let . Then  contains at most  pixels.
\begin{proof}
Let  and  (see Step~\ref{step:convexity-test-construction_of_R} of
Algorithm~\ref{alg:convexity-tester}) for .
Call a subimage that consists of pixels , where  and
, a \emph{square}. Call squares that contain pixels from 
\emph{fence squares}. Let  be the third vertex of the triangle T.
We will find an upper bound on the number of fence squares inside .
Each pixel that Algorithm~\ref{alg:convexity-tester}
queries in Step 5 discovers at most one (new) square in . The
number of fence squares is at most the number of all discovered squares in the
triangle. The algorithm queries at most 
pixels in the triangle (thus, it discovers at most that many squares), since every time it either increases the -coordinate or decreases the -coordinate of the queried pixel.
Therefore, there are at most 
fence squares in this triangle. Similarly, we can
find an upper bound on the number of discovered squares in the remaining
triangles. Since the perimeter of  is at most , the sum of the upper bounds is at most
.
The number of pixels from  in a single
fence square is at most  and
the total number of pixels from  in all
fence squares is at most .
\end{proof}
\end{lemma}

We call a pixel {\em misclassified} if it is black and is in  or if it is white and in .
If we make all pixels in  black and all pixels outside of  white, we obtain a
convex image. Thus, by Lemma~\ref{lem:total_number_of_pixels_in_F},
 contains at least  misclassified pixels if there are
at most  black pixels outside of . If the latter is the
case, the probability that the algorithm will not detect a violated pixel
is at most . By
Lemma~\ref{lem:black-pixels-outside-R} the probability that 
contains less than  misclassified pixels is at most
.
Therefore, the probability Algorithm~\ref{alg:convexity-tester} accepts is at most
, as desired.
\paragraph{Query complexity.} The algorithm queries pixels in Steps 1,5 and 9.
In Steps 1 and 9, the algorithm makes  queries. As we mentioned earlier, the
algorithm queries  pixels in Step~5. Thus, the overall query
complexity of the algorithm is .

\paragraph{Running time.} The running time of the algorithm in Steps 1 through
7 and Step 9 is . In Step 8, one can
obtain the convex hull of  represented by a set of
 points lexicographically sorted by their coordinates
in time . In Step 10,  is represented
by at most 4 sets of  points. In each set points are sorted
by their -coordinates (either in decreasing or increasing order) and by
their -coordinates (either in decreasing or increasing order).
We need to check whether a point is inside  or  in this step. For a single point we can do
this in time  by a
binary search. Since we query  points the running
time of Step 10 is . Therefore, the running time
of the algorithm is .
\end{proof}








\begin{comment}

Let  denote the parallel line to the base  that passes at distance  from it inside  and let  denote the length of  inside . Consider the line . It partitions  into a trapezoid and a triangle as shown in Figure~\ref{}. We consider the case when all black pixels of  in  lie inside the trapezoid.  The length of each leg of the trapezoid is at least  and thus, there exists at least one reference point on each leg, i.e, there exists a type-2 reference line that passes through the legs of the trapezoid. This reference line misclassifies at most all pixels inside the trapezoid. The area of the trapezoid is at most  and the claim holds.

Now let us assume that there are black pixels of  above the line . Let  be the closest to the
base type-1 reference line that does not cross the base and  be the closest to the
base type-1 reference line that crosses the base as shown in Figure~\ref{}. Let  intersect  and  at  and  respectively and  intersect  and  at  and  respectively. Let  denote the angle between  and the base. Then . Let  be the distance from  to the base and  denote the distance from  to the base. The distance  equals the length of  multiplied by . The length of  is at most , thus, . The distance between  and  is  and by the triangle inequality 
Consider the line . There are two cases:

Case 1: All black pixels of  lie inside the trapezoid below the line . Then the line misclassifies at most .

Case 2:  crosses the line . This implies that it also crosses the line . Let  cross the line  at points  and  (see Figure~\ref{}). The length of  inside  is at most  since no reference point was found on this portion of the line. Thus,  is also at most  (by convexity of ). Let  be the intersection point of the lines  and . By similarity the height of  is at most  and this triangle contains all black pixels of  above . Therefore,  misclassifies at most  black pixels of  and at most  white pixels of . This implies that  misclassifies at most  pixels of .

Therefore,  misclassifies at most  pixels of . Let us consider the closest type-2 reference line to  above it, i.e., there is now type-2 reference line that crosses the region between that line and . Analogously, consider the closest type-2 reference line below . The area of the region in  between these two type-2 reference lines is at most  and thus, either the region between  and the upper reference line or the region between  and lower reference line is at most . This implies that one of these type-2 reference lines misclassifies at most  pixels of M and such lines always exist since the distances from  to the base and from  to  are at least .

\begin{lemma}\label{lem:alg3_is_connectedness_tester}
Algorithm~\ref{alg:connectedness_tester-na} is a 1-sided error  for connectedness.
\end{lemma}
\begin{proof}
The algorithm accepts all connected images since in step 1b \emph{Exhaustive-Square-Tester} will see no violation of  inside each sampled square and will return TRUE every time.

Let us assume that  is -far from connectedness. Let  be the fraction of witnesses of level  and . The probability that all sampled squares are not witnesses is

Let . We will show that . There are  grid pixels of level . Let us make all white pixels among them black. We modify at most  pixels. Now, we will try repair witnesses (i.e., make them satisfy property ) bottom up starting from witnesses of level  up to witnesses of level . In each witness of level  we make black all the pixels in the middle row and the middle column of the witness. If a modified witness of level  still violates property  then we will see this violation in witnesses of level  and we will try to repair them at the next level (i.e., ). At each level we modify at most  pixels, where . We will reach witnesses of the last level . If we repair them  we will obtain a connected image. By Claim~\ref{cl:max_dist_to_border_connectedness} we can
repair a witness of this level by modifying at most . Since there are  squares of the last level we can repair all the witnesses of this level by modifying at most  pixels. Thus, we obtain a connected image by modifying at most



Since the image is -far from connectedness . Then . Thus, the probability that the algorithm detects a violation of  and rejects the image is at least .
\end{proof}

Analysis:
The algorithm makes  steps


It uses a subroutine that succeeds to find a violation in a square of type  with probability ,where  is the actual distance(in pixels) of the square to . This subroutine is called -tester of property  and  is a constant that will be chosen later.
At each level the algorithm queries all \emph{ lattice pixels} of the sampled square. Then it queries  of the remaining pixels, where , and for each discovered black pixel above it picks some number  such that  and performs a search until a component of size  is found or there are no more black pixels exist for this component.

\end{comment}
\fi
\fi
\end{document}
