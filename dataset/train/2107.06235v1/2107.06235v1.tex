\documentclass[times,twocolumn,preprint,3p,authoryear]{elsarticle}

\usepackage{ycviu}
\usepackage{framed,multirow}

\usepackage{amssymb}
\usepackage{latexsym}

\usepackage{url}
\usepackage{xcolor}
\definecolor{newcolor}{rgb}{.8,.349,.1}

\usepackage{subfiles}
\usepackage[caption=false]{subfig}
\usepackage[export]{adjustbox}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{boldline}
\usepackage{ulem}
\usepackage[ruled,lined,boxed]{algorithm2e}
\usepackage{nccmath}
\usepackage{makecell, tabularx}
\usepackage{rotating}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{pdfpages}



\newcommand*\rot{\rotatebox{90}}
\newcommand{\redul}[1]{{\color{red}\uline{{\color{black}#1}}}}
\newcommand{\blueul}[1]{{\color{blue}\uline{{\color{black}#1}}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\setlength{\floatsep}{3pt plus 2pt minus 2pt}
\setlength{\dblfloatsep}{3pt plus 2pt minus 2pt} \setlength{\textfloatsep}{3pt plus 2pt minus 2pt}
\setlength{\dbltextfloatsep}{3pt plus 2pt minus 2pt} \setlength{\intextsep}{3pt plus 2pt minus 2pt}
\setlength{\belowcaptionskip}{1pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{1pt plus 1pt minus 1pt}

\journal{Computer Vision and Image Understanding}

\begin{document}

\ifpreprint
  \setcounter{page}{1}
\else
  \setcounter{page}{1}
\fi

\begin{frontmatter}

\title{Exploiting Image Translations via Ensemble Self-Supervised Learning for Unsupervised Domain Adaptation}



\author[1]{\corref{cor1}Fabrizio J. \snm{Piva}}
\ead{f.j.piva@tue.nl}
\author[1]{Gijs \snm{Dubbelman}}

\cortext[cor1]{Manuscript under review at Computer Vision and Image Understanding}

\address[1]{Eindhoven University of Technology, Department of Electrical Engineering, Groene Loper 12, 5612AZ Eindhoven, The Netherlands}




\received{1 May 2013}
\finalform{10 May 2013}
\accepted{13 May 2013}
\availableonline{15 May 2013}
\communicated{S. Sarkar}


\begin{abstract}
    We introduce an unsupervised domain adaption (UDA) strategy that combines multiple image translations, ensemble learning and self-supervised learning in one coherent approach. We focus on one of the standard tasks of UDA in which a semantic segmentation model is trained on labeled synthetic data together with unlabeled real-world data, aiming to perform well on the latter. To exploit the advantage of using multiple image translations, we propose an ensemble learning approach, where three classifiers calculate their prediction by taking as input features of different image translations, making each classifier learn independently, with the purpose of combining their outputs by sparse Multinomial Logistic Regression. This regression layer known as meta-learner helps to reduce the bias during pseudo label generation when performing self-supervised learning and improves the generalizability of the model by taking into consideration the contribution of each classifier. We evaluate our method on the standard UDA benchmarks, i.e. adapting GTA V and Synthia to Cityscapes, and achieve state-of-the-art results in the mean intersection over union metric. Extensive ablation experiments are reported to highlight the advantageous properties of our proposed UDA strategy.
\end{abstract}\begin{keyword}
ensemble learning\sep self-supervised learning\sep unsupervised domain adaptation \sep image translations
\end{keyword}

\end{frontmatter}





\subfile{sections/introduction}
\subfile{sections/related_work}
\subfile{sections/method}
\subfile{sections/experiments}
\subfile{sections/conclusions}

\bibliographystyle{model2-names}
\bibliography{references}


\end{document}
