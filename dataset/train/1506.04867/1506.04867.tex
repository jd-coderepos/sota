\documentclass[prodmode,letterpaper]{acmsmall} 
\usepackage{setspace}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{float}
\usepackage{amsmath}
\usepackage[export]{adjustbox}
\floatplacement{figure}{H}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{varwidth}
\usepackage{xspace}

\newcommand{\rknn}{RNN\xspace}
\newcommand{\rstknn}{RSTNN\xspace}






\begin{document}

\markboth{}{Efficient Reverse  Nearest Neighbor evaluation for hierarchical index}

\title{Efficient Reverse  Nearest Neighbor evaluation for hierarchical index}
\author{Siddharth Dawar
\affil{Indraprastha Institute of Information Technology, India}
Vikram Goyal
\affil{Indraprastha Institute of Information Technology, India}
Debajyoti Bera
\affil{Indraprastha Institute of Information Technology, India}
}

\begin{abstract}
``Reverse Nearest Neighbor'' query finds applications in decision support
systems, profile-based marketing, emergency services etc.  In this paper,
we point out a few flaws in the branch and bound algorithms proposed
earlier for computing monochromatic \rknn queries over data points stored
in hierarchical index. We give suitable counter examples to validate our claims and propose a
correct algorithm for the corresponding problem.
We show that our algorithm is correct by identifying necessary
conditions behind correctness of algorithms for this problem.
\end{abstract}






\maketitle


\section{Introduction}\label{section:intro}
One important type of operation that is gaining popularity in database and
data-mining research
community is the {\em Reverse Nearest Neighbor Query} (\rknn)
\cite{korn2000influence}. Given a set of database objects  and a query
object , the \rknn query returns those objects in , for which 
is one of their  nearest neighbors; here the notion of neighborhood is with
respect to an appropriately defined notion of distance between the objects.
A classic example \rknn is in the domain of decision support systems where the task is to open
a new facility (like a restaurant) in an area such that it will be least influenced
by its competitors and attract good business. Another application is profile
based marketing \cite{korn2000influence}, where a company maintains profiles of
its customers and wants to start a new service which can attract the maximum
number of customers.
\rknn has also applications in clustering, where a cluster could be created by
identifying a group of objects, and clustering them around their common nearest
neighbor point -- this essentially involves finding cluster centers
with high cardinality of reverse nearest neighbor sets.
{\em Reciprocal nearest neighborhood}, in which data
points which are nearest neighbors of each other are clustered
together (and therefore, satisfy both nearest neighbor and reverse nearest
neighbor criteria), is another well-known technique in clustering \cite{lopez2012fastrnn}.

This important concept has seen a series of remarkable applications and
algorithms for processing different types of
objects, in various contexts and under variations \cite{kang2007continuous}, \cite{safar2009voronoi}, \cite{tran2009reverse}, \cite{taniar2011spatial},\cite{shang2011finding}, \cite{cheema2012continuous}, \cite{ghaemi2012continuous},\cite{li2013efficient}, \cite{emrich2014reverse}, \cite{cabello2010facility}, \cite{bhattacharya2013new} of the problem parameters.
The focus of this paper is
monochromatic \rknn queries -- in this version, all objects in
the database and the query belong to the same category, unlike the bichromatic
version in which the objects can belong to different categories. Furthermore, we want to
focus on queries where  is specified as part of a query, and want to support
objects from an arbitrary metric space.

This paper points out several fundamental inaccuracies in three papers published
earlier on the problem mentioned above.
\begin{itemize}
    \item Reverse -nearest neighbor search in dynamic and general metric databases \cite{achtert2009reverse}
    \item Reverse spatial and textual  nearest neighbor search \cite{lu2011reverse}
    \item Efficient algorithms and cost models for reverse spatial-keyword k-nearest neighbor search \cite{lu2014efficient}
\end{itemize}

Achtert et al.\cite{achtert2009reverse} proposed a branch-and-bound algorithm
for the above problem which could use any given hierarchical tree-like index
on data from any metric space. Lu et al. \cite{lu2011reverse} proposed a
similar algorithm, but specifically optimized for spatio-textual data, for answering
\rstknn queries using a specialized IUR tree as the
indexing structure. In a followup paper \cite{lu2014efficient}, they  proposed
an improvement of their algorithm (including correcting an error) and a
theoretical cost model to analyze the efficiency of their algorithm. However,
we observed several deficiencies in the algorithms mentioned above. In this
paper we will point out those inaccuracies, and discuss them more formally by
pointing out some key properties which
these algorithms violate, but are necessary for ensuring correctness of
these and other similar algorithms.  We will present detailed counter examples and suggest corrective
modifications to these algorithms.
Finally we will propose a correct algorithm for performing \rknn queries over a
hierarchical index and also present its proof of correctness.

The paper is organized as follows. In Section \ref{section:earlier} we explain
the three published approaches mentioned above in which we found
inaccuracies. In Section \ref{section:counter-ex} we describe our counter-examples with respect to them.
We present our modified algorithm in Section \ref{section:algo}, and its
proof of correctness in Section \ref{section:proof}.

\section{Earlier Results}\label{section:earlier}
The underlying algorithms for all three approaches mentioned above essentially
have the same structure and follow a branch-and-bound approach. The former work
is applicable on any kind of data with a distance measure that is a metric, and
uses any hierarchical tree-like index built on the data. The two latter work are
specifically concerned with \rknn query on spatio-textual data, which they
refer to as \rstknn query.

\begin{figure}[!tbh]
\begin{center}
\subfloat[]{\includegraphics[width=6.275cm]{./images/Diagram2}}
\subfloat[]{\includegraphics[width=6.275cm]{./images/Tables2intro}}
\caption{\small Example for illustrating \rstknn and \rknn}
\label{fig:example}
\end{center}
\end{figure}

In \rstknn, each object is
represented by a pair () where  is the spatial location and  is the
associated textual description which is represented by (word,weight(word)) pairs
for all words appearing in the database. Weight of a word
is calculated on the basis of TF-IDF scheme \cite{salton1988term}. Spatio-textual similarity () is defined by \cite{lu2011reverse} as follows:
 
The parameter  is used to define the relevance factor for spatial and
textual similarity while calculating the total similarity scores and is
specified in a query.  and  denote the minimum and maximum
distance between any two objects in the database and are used to normalize the
spatial similarity to the range . Similarly  and  denote
the minimum and maximum textual similarity between any two objects in the
database.  is the Euclidean Distance between  and  and
 is the
Extended Jaccard Similarity \cite{tan2011v} defined as:

where .vct= and .vct=.

As an example, consider Figure \ref{fig:example}. There, considering only
location attributes, and for , \rknn of  are objects  and .
However, if we consider both spatial and textual similarity, and taking 
and , \rstknn of  is ,  and .

Now we will describe the actual algorithm proposed by \cite{lu2011reverse} for 
\rstknn. It is important to present it in some detail -- this is required for
proper appreciation of the inaccuracies in this algorithm. This algorithm
requires its data to be organized as an hierarchical index called as
IUR-tree. IUR-Tree is a R-Tree \cite{guttman1984r}; where every node of the tree is embedded with Intersection and Union Vectors. The textual vectors contain the weight of every distinct item in the documents contained in the node. The weight of every item in the Intersection Vector (resp. Union Vector) is the minimum weight (resp. maximum weight) of all the items present in the documents contained in the node. During the execution of the algorithm, a lower and upper nearest-neighbor list/contribution list is created and maintained for each node in the IUR-Tree. The lower (resp. upper) contribution list stores the minimum (resp. maximum) similarity between the node and its neighbors.
\begin{figure}[!tbh]
\begin{center}
\subfloat[]{\includegraphics[width=6.275cm]{./images/Tree1}}
\subfloat[]{\includegraphics[width=6.275cm]{./images/TablesIUtry}}

\caption{\small IUR-Tree and Textual Vectors of Fig 1 \label{fig:IUR}}
\end{center}

\end{figure}

The IUR-Tree and Intersection and Union Vectors of the corresponding nodes is shown in the Figure \ref{fig:IUR}.
 These vectors along with the MBR's of nodes are used to compute the similarity approximations i.e. upper and lower bounds on the spatio-textual similarity between two groups of objects. 




\begin{spacing}{0.8}
\begin{algorithm}[tp]
\caption{\rstknn(: IUR-Tree root,: query) from \protect\cite{lu2011reverse}}
\begin{algorithmic}[1]
\State {\bfseries Output:} All objects , s.t  \rstknn(,,).
\State Initialize a priority queue , and lists ;
\State EnQueue();
\While{ is not empty}
	\State  DeQueue(); //Priority of  is 
	\For {each child node  of }
		\State Inherit();
		\If{IsHitOrDrop()==false}
			\For{each node  in } //see subsection \ref{subsec:locality}
			\State UpdateCL(); //update contribution lists of ;
			\If{IsHitOrDrop()=true} //see subsection \ref{subsec:completeness}
				\State{\bfseries break;}
			\EndIf
			\If{}
				\State UpdateCL(); //Update contribution Lists of  using .
				\If{IsHitOrDrop()=={\bfseries true}}
					\State Remove  from  or ;
				\EndIf
			\EndIf
			\If{ is not a hit or drop}
				\If{ is an index node}
					\State EnQueue();
				\Else
					\State .append(); //a database object
				\EndIf
			\EndIf
			\EndFor
		\EndIf
	\EndFor
\EndWhile
\State FinalVerification();
\algstore{myalg}
\end{algorithmic}
\end{algorithm}
\end{spacing}

We refer to an internal node or a point in the IUR-Tree as an entry. The algorithm takes as an input an IUR-Tree (Intersection Union tree) ,
query  and returns all database objects which are \rstknn of . The data structures used are: a priority queue () sorted
in decreasing order on , result list (), pruned list () and
candidate list ().  is the maximum spatial textual similarity of the entry  with the query point . The
algorithm dequeues the root of the IUR-Tree from the queue and for every child  of the root,  inherits the contribution list of its parent. The function UpdateCL() is invoked and the contribution list of  is updated with every  present in the candidate list, result list and the priority queue. After every invocation to UpdateCL(.), the algorithm checks based on the minimum and maximum bound similarity scores with the 
nearest neighbor, whether to add  to the results,
candidates or pruned list. If  can't be pruned or added to the results, the contribution list of  is updated with . This process is called the mutual effect. If  can be added to the results or pruned, it is removed from the queue or . After updating node  with all entries of ,  or , the function \textit{IsHitorDrop()} is again invoked. If  can't be added to the result or pruned list, a check is performed to find out whether  is a internal node or a point. If  is an
internal node, it is added to the queue, else to the candidate list.
When the queue becomes empty, there might be some objects left in the
candidate list. The function \textit{FinalVerification()} is invoked where the candidate objects are updated with all the entries present in  to decide whether they
belong to result or not.
\begin{spacing}{0.8}
\begin{algorithm}[tp]
\begin{algorithmic}[1]
\algrestore{myalg}
\Function{FinalVerification()}{}
\While{}
	\State Let  be an entry in  with the lowest level;
	\State ;
	\For {each object  in }
		\State UpdateCL(); //update contribution lists of .
		\If{IsHitOrDrop()==true} // see subsection \ref{subsec:completeness}
			\State ;
		\EndIf
	\EndFor
	\For{each child node  of }
		\State ; //access the children of  	
	\EndFor
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{spacing}

\section{Counter-examples}\label{section:counter-ex}
We describe three counter example in this section:
\begin{enumerate}
    \item Inaccuracy regarding computation of  and 
    \item Inaccuracy w.r.t. Locality Condition
    \item Inaccuracy w.r.t. Completeness Condition
\end{enumerate}
All these examples are
illustrated with respect to the algorithm described in \cite{lu2011reverse};
however we also
explain the concepts used in constructing these examples -- therefore these
examples can be easily modified to suit the other algorithms.
We observed that \cite{achtert2009reverse} proposed an algorithm which maintains
the locality condition, but violates the completeness condition. We recently
observed that \cite{lu2014efficient} modified their previous algorithm from
\cite{lu2011reverse} which now maintains the locality condition. However, their algorithm still violates the completeness condition.

\subsection{Inaccuracy regarding computation of  and }
The branch-and-bound algorithm presented in \cite{lu2011reverse} required
cleverly constructed lower and upper bounds on the textual similarity (and combined
textual-spatial similarity) between two groups of data objects. Its authors defined
 (minimum possible similarity) and  (maximum possible similarity)
and claimed that these definitions, when used in conjunction with upper and
lower bounds on spatial similarity, give valid upper and lower bounds on the
similarity between two groups of objects. To prove this claim, they used the
following crucial lemma. The first inaccuracy we report is regarding this lemma.

\begin{definition}[Similarity Preserving Function] \cite{lu2011reverse} Given
    two functions  and , where  denotes the
    domain of -element vectors and , the real numbers.  is
    a similarity preserving function w.r.t , such that for any three vectors
    , ,, if ,
    , then we have .
\label{defn:similarity_PF}
\end{definition}

\begin{lemma}\cite{lu2011reverse}
    \label{lemma:ej-wrong-lemma}
    Extended Jaccard is similarity preserving function wrt. function  for .
\end{lemma}


\paragraph{Counter Example}
Consider three points , ,  with textual vectors , , .
Using  as defined in Lemma \ref{lemma:ej-wrong-lemma},
observe that the given points satisfy 
the conditions for a similarity preserving
function, i.e., , . 
However,  which contradicts Definition
\ref{defn:similarity_PF}. The  and  formula given in the paper relied
on the above Lemma to be correct, which therefore become invalid.


We now present our approach to calculate  and  between two groups of
textual objects  and .
As explained earlier, every textual object is represented as a vector of term
frequencies. For any group of objects, their intersection vector (resp. union
vector) has been defined to be a vector whose every coordinate is the minimum (resp. maximum)
frequency among the corresponding coordinates of objects. Denoting the
intersection and union vectors of  as 
and , notice that for every , and
, . 
We propose the following formul\ae\ for .

The idea for computing  is that since it is a lower bound, we want to
minimize the term in the numerator and maximize the denominator of EJ to ensure
that  and .
Similarly formul\ae\ for  is given below:



\subsection{Inaccuracy w.r.t. Locality Condition}\label{subsec:locality}
\begin{figure}[!htb]
\begin{center}
\subfloat[]
{\includegraphics[width=0.4\linewidth]{./images/new_example}}
\subfloat[]{\includegraphics[width=0.6\linewidth]{./images/new_tree}}
\caption{\small Counter-example (Locality and Completeness conditions)
\label{fig:counter-ex}
 } 
\end{center}
\end{figure}

Consider the following counter-example for the dataset and IUR-Tree illustrated in Figure
\ref{fig:counter-ex}, and let  and . The minimum and maximum distance between any two points in the database is =7.07 and =142.21. The exact \rstknn of the
query point  is  and . The trace of the algorithm \cite{lu2011reverse} is shown in Table
\ref{Tab:rstknn_algo}. We will focus on step 1 here. The root of the tree is dequeued from the tree and node  is processed.  inherits the contribution lists of its parent, which is empty. Since ,  and  are empty,  is simply added to the queue. Now, node  is processed.  updates
its upper and lower contribution lists with  and invokes IsHitOrDrop. The upper and lower contribution lists of  upon invoking IsHitOrDrop is :\\
.CL=\\
.CL=\\
Since , which is more than the upper bound given by
.CL, at this point node  is accepted (wrongly) as the \rstknn of . \begin{table}[!htb]
\begin{tabular}{cc}\noindent\begin{minipage}[t]{\linewidth}
\begin{center}
\caption{Trace of \rstknn Algorithm (2011) \label{Tab:rstknn_algo}}
    \begin{tabular}{|l | p{4cm} | l | l | l | l |}
    \hline
    \bfseries{Steps} & \bfseries{Actions} & \bfseries{U}& \textbf{COL} & \textbf{ROL} & \textbf{PEL} \\
    \hline
    1 & Dequeue Root, Enqueue  &  && , , ,  & \\ 
    \hline
    2 & Dequeue  &  &  & ,,,  ,  & \\
    \hline
    \end{tabular}
\end{center}
\end{minipage}
 \end{tabular}
\end{table} 

We attribute this fault to the violation of the {\em Locality
Condition}, a property that, we claim, must have been followed by these
algorithms.

\paragraph{Locality Condition} Nearest neighbors of data points in a node may belong to
the node itself; hence, every node
should compute similarity with itself and include itself as a candidate (along
with other similar nodes) in any test to prune or accept the node as \rstknn of .

In the counter-example above, node  does not satisfy this condition
since its contribution lists do not contain itself or points inside it.

\subsection{Inaccuracy w.r.t. Completeness Condition}\label{subsec:completeness}
The trace of the algorithm \cite{lu2014efficient} is shown in Table \ref{Tab:rstknn_algo_2014}.
\begin{table}[!htb]
	\begin{tabular}{cc}\noindent\begin{minipage}[t]{\linewidth}
			\begin{center}
				\caption{Trace of \rstknn Algorithm (2014) \label{Tab:rstknn_algo_2014}}
				\begin{tabular}{|l | p{4cm} | l | l | l | l |}
					\hline
					\bfseries{Steps} & \bfseries{Actions} & \bfseries{U}& \textbf{COL} & \textbf{ROL} & \textbf{PEL} \\
					\hline
					1 & Dequeue Root, Enqueue , Enqueue  & ,  &&  & \\ 
					\hline
					2 & Dequeue  &  &  & ,  & \\
					\hline
					3 & Dequeue  &  &  & , , ,  & \\
					\hline
				\end{tabular}
			\end{center}
\end{minipage}
	\end{tabular}
\end{table} 

We will now focus on Step 2, when node  is dequeued from the priority queue,
and its children are now being processed.
Node  is now processed and it inherits the contribution lists of its parent . The function IsHitOrDrop is called, but   can't be pruned or added to the results. After invocation of IsHitOrDrop,  updates its contribution list with itself to maintain the locality condition.  further updates its contribution list with other entries present in ,  and  sorted in the decreasing order of the maximum spatio-textual similarity with . The upper and lower contribution list of  is shown below :\\ \\
.CL=\\
.CL=\\
Since , which is less than 0.68; so at this point 
 is accepted (wrongly) as \rstknn of .
We claim that this faulty
behaviour is due to not ensuring the {\em Completeness Condition}, viz.,
absence of  in contribution lists of .
This condition is discussed in more detail in Section \ref{sec:nn-u}. In this
example, the contribution lists of  is not complete.



\section{Proposed \rstknn Query Algorithm }\label{section:algo}
In this section, we present a modified algorithm to answer \rstknn
queries. We will illustrate our algorithm with an example, pointing
out the modifications and end this sections with a formal
proof of correctness. We begin by formalizing some notions which will be
used in the algorithm, and will be crucial in ensuring its correctness.

As explained earlier, the algorithms we considered worked on data that was stored in a hierarchical
tree-like index, where the leaf nodes are data points themselves (to
be represented by small letters) and internal nodes (to be represented by
CAPITAL letters) contain pointers to children
nodes.
Our modified algorithm will share backbone of these algorithms; however,
structually, it will bear resemblance to the algorithm presented in
\cite{lu2011reverse,lu2014efficient}. However, it will be presented in a
generalized manner which can be used to perform \rknn queries, given any value
of , on a wide variety of data and 
independent of the explicit indexing structure used.
The only requirement from the data and the index is a similarity measure 
among the data points, information about the of number of objects in each
node and estimates  and  among nodes (explained below).

\subsection{Contribution List a.k.a. -list}
We will use the following notation: if  is the  nearest neighbor of , then we will write  as .
We will use the convention that
a point is the  nearest neighbor of itself.
An immediate observation is the following: 
for any .

One way to answer \rknn queries
is by computing the list of nearest neighbors (-list) for every data point :
 is an ordered list of data points  such
that  is   is  and so on. Computing this list explicitly for
every data point could be very inefficient. 
The usual approach followed by branch-and-bound algorithms like
\cite{achtert2009reverse,lu2011reverse,lu2014efficient} is searching the index top-down
while maintaining two NN-lists with each node - one contains an
overestimate of its nearest neighbor, and another containing an underestimate of
the same. These estimated lists are constructed using two functions
 and  which must satisfy the
property below. The actual implementation of these functions depend crucially on
the type of data used and the index. For two nodes  and ,
\begin{itemize}
    \item  must give a lower bound for the minimum similarity between pairs of
points from  and  i.e.  ,,
.
    \item  must give an upper bound for the maximum similarity between pairs of points from  and  i.e.  , , .
\end{itemize}

Next, we will define the main component of our algorithm, a formalization of
{\em contribution lists} (CL) used in earlier algorithms.

\begin{definition}[NN-list \label{NN-list}]
An NN-list of a node  is a list of tuples: , where each  is a node and  is a positive integer.
\end{definition}

The NN-lists we will maintain per node are () and () whose tuples will
provide estimates to the similarity of  to its  nearest neighbor, for various
values of .

\subsection{Lower bound list }
The central idea behind the  list comes from the following observation.
Suppose for a set of  points  and
another point , we have that . Then, it is obvious that
if  does not belong to this set, ; and if  belongs
to this set, then . Extending this concept to nodes,
consider any node  with  data points; now, if  then,
 if   and  if . Notice that these bounds are tight.

We can even extend this idea to multiple nodes to get the following claim. Let
 be a data point and  be a collection of non-overlapping
nodes which do not contain , where the list is sorted in decreasing order of
. Let  denote the number of data points in , and let
 be a lower bound on . Then, for all . If  for some , then 
must be replaced with . We can generalize this even further by
considering a node instead of .


\begin{definition}[Lower NN-list \label{lower-contri-list}]
An NN-list  of non-overlapping nodes is a
valid  if:  
\begin{itemize}
\item the list is sorted in decreasing order of 
\item for all , if  does not overlap with , then  and if  overlaps with , then 
\end{itemize}
\end{definition}


The following lemma describes the use of lower NN-lists to get underestimates of
nearest neighbors. The proof is immediate from earlier definitions.
\begin{lemma}\label{lemma:NNL-lemma}
    For any  and  that satisfies  (including the case ), it holds that for all , .
\end{lemma}


\subsection{Upper bound list }\label{sec:nn-u}
We want to define  as an overestimation of nearest neighbors similar to
 and derive a similar lemma as Lemma \ref{lemma:NNL-lemma}; however, we
require an additional concept first.


\begin{definition}[Complete NN-list]
    We say that an NN-list  is {\em complete} if every data
    point is present in some node in the NN-list, and for every  in the list, 
\begin{itemize}
\item if  does not overlap with , then 
\item if  overlaps with , then 
\end{itemize}
\end{definition}

It must
be noted that an  list need not be complete for it to satisfy Lemma
\ref{lemma:NNL-lemma}. However,
similar arguments do not work for . Take for example, the example
situation similar to
the one described for : we have a set of points  and another point  (all distinct). But even if we know that
 for some  and for all , it is nevertheless not true
that , unless, all points other than  are in the set --
which is precisely what a complete NN-list specifies.

Now we can define similar concepts like .
\begin{definition}[Upper NN-list]
For a node , an NN-list  of non-overlapping nodes is a valid  when the following holds:
\begin{itemize}
\item the list is sorted in decreasing order of 
\item the list is complete
\end{itemize}
\end{definition}


Observe that the completeness condition requires that  must contain 
itself, or its parent node, or all its children nodes -- this is essentially
the locality condition we mentioned earlier (Section \ref{subsec:locality}).
However, we have chosen to specifically highlight the above condition separately
from the more general completeness condition.
The main working lemma for  follows next.
\begin{lemma}\label{lemma:NNU-lemma}
    For any  and  such that  (including the case when  and ), it holds that for all , .
\end{lemma}

\subsection{Branch-and-bound traversal}
A branch-and-bound algorithm traverses a hierarchical index by first visiting
the root, and then exploring its children nodes, and so on. For every node it
visits, the algorithm decides what to do next based on some estimate of the
relevance of the current node to the desired answer (here,  and 
lists). It may choose to further explore the node, add all the points
in the node to the result set and not explore the node further (aka. {\em
accepting} the node), or, simply not explore the node further because it decided
that the node does not contain any point that should be in the result set (aka.
{\em pruning} the node).

Suppose the query point is denoted by ; and suppose that a branch-and-bound
algorithm is currently visiting  during its traversal of the
index. Let  denote the (valid) lower NN-list of 
node, and  denote its (valid) upper NN-list. Also, suppose  is the smallest
index such that  for , and 
 is the smallest similar index for .

Here are the main theorems that give us sufficient conditions for accepting and
pruning certain nodes in the index during a branch-and-bound traversal.

\begin{theorem}[Accepting and Pruning
    Condition]\label{thm:accept-prune-condition}
\begin{enumerate}
    \item If , then  cannot have any node in  in
its \rknn set. Therefore,  can be pruned.
    \item If , then all nodes in  belong to \rknn of 
and so  can be accepted.
\end{enumerate}
\end{theorem}
The proofs for the two cases are immediate from
Lemma \ref{lemma:NNL-lemma} and \ref{lemma:NNU-lemma}, respectively.
\footnote{For accepting or pruning, in case there is a tie between
similarities between query point and a database point, we tie-break in favour of points in the database. The alternative
approach requires straight forward modification to the results in this
subsection.}.







\subsection{Algorithm}
Now we will discuss the modified algorithm for finding reverse nearest neighbors on
spatial-textual objects. Our algorithm is a modification of the one proposed in
\cite{lu2011reverse}, so we will mostly engage in highlighting the major changes.
Like the original algorithm, our algorithm uses the following data structures: a FIFO queue (), a result
list (), candidate list () and pruned list (). We use a FIFO
queue instead of a priority queue, as each entry of needs to update its
NN-list with every other entry present in every list  in order to
ensure completeness of lists. So, the order in which other entries are added is
irrelevant. We will frequently use NN-lists to refer to both the upper and lower
NN-lists of the corresponding entry.

As before, the algorithm initializes the lists and enqueues the root of the IUR-tree.
While the queue is not empty, an entry  is dequeued from the queue and its
parent is removed from its NN-list. The two key modifications we suggest are
stated next. First, if  is an internal node of the tree, it
adds itself to its NN-lists, thereby maintaining the {\em locality condition}
(line 12).
Then  updates its NN-lists with each entry  present in the queue and vice
versa. The updation of NN-list of  with every other entry in the queue
maintains the {\em completeness condition} (line 14). After this, IsHitorDrop is invoked to check if  can
be pruned or added to the results. If  can neither be pruned nor added to the
results, its children are added to the queue if  is an internal node;
otherwise,  is added to the candidate list. We continue with the optimisation
of having the children of  copy the NN-list of  before they are enqueued to .
When the queue
becomes empty, there might be some candidate points left in the candidate list.
The procedure FinalVerification is invoked to decide whether the points
present in the candidate list belong to the result list or the pruned list; this
procedure essentially checks every candidate point with other entries.

\begin{spacing}{0.8}
\begin{algorithm}[tp]
\caption{\rstknn(: IUR-Tree root,: query)}
\begin{algorithmic}[1]
\State {\bfseries Output:} All objects , s.t  RSTNN().
\State Initialize a FIFO queue , and lists ;
\State EnQueue();
\While{ is not empty}
	\State   DeQueue(); //FIFO Queue
	\For{each tuple }
		\If{ or }
			\State remove  from
			 and  ;
		\EndIf
	\EndFor
	\If(  is an internal node)
		\State Additself()				//Ensure locality condition
	\EndIf	
	\For{each entry  in }   // Ensure completeness condition   
		\State UpdateNN-list(); //mutual effect
		\State UpdateNN-list(); //mutual effect
	\EndFor
	\If{ is not a hit or drop}
		\If{ is an index node}
			\For{each child  of }
				\State Inherit();
				\State Inherit();
				\State EnQueue()				
			\EndFor
			
		\Else
			\State .append(); 
		\EndIf
	\EndIf
\EndWhile
\State FinalVerification();
\algstore{ouralg}
\end{algorithmic}
\end{algorithm}
\end{spacing} 
We illustrate the working of our algorithm on the example presented
earlier (Figure \ref{fig:counter-ex}) in
Table \ref{Tab:our_algo}. As expected, the algorithm now correctly returns  and 
as the only points in \rstknn of .

\begin{spacing}{0.8}
\begin{algorithm}[tp]
\begin{algorithmic}[1]
\algrestore{ouralg}
\Function{FinalVerification()}{}
\State 
\While{}
	\For {each point  in }\For{each point  in }
			\State UpdateNN-list(); 
		\EndFor
		\For(each point  in )
			\State UpdateNN-list(); 
		\EndFor
		\For(each point  in  )
			\State UpdateNN-list(); 
		\EndFor				
		\If{IsHitOrDrop()==true}
			\State ;
		\EndIf
	\EndFor
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
\end{spacing}


\begin{table}[tp]
\begin{tabular}{l}
\begin{minipage}[t]{\linewidth}
\begin{center}
\caption{Trace of our algorithm \label{Tab:our_algo}}
    \begin{tabular}{ |l| p{4cm} | l | l | l | p{2cm} |}
    \hline
    \bfseries{Steps} & \bfseries{Actions} & \bfseries{U}& \textbf{COL} & \textbf{ROL} & \textbf{PEL} \\ \hline
    1 & Dequeue Root, Enqueue , Enqueue  & ,  &&&\\ \hline
	2 & Dequeue  & , ,  & &&\\ \hline
	3 & Dequeue  & , , ,  && &  \\ \hline
	4 & Dequeue & , ,  &&& \\ \hline
	5 & Dequeue & ,  &&, & \\ \hline
	6 & Dequeue & , ,  &&, & \\ \hline
	7 & Dequeue & ,  &&, & \\ \hline
    8 & Dequeue &  &&, & \\ \hline
    9 & Dequeue &  &&, & , \\ \hline
    10 & Verify &  &&, & , , \\ \hline
    \end{tabular}
\end{center}
\end{minipage}
 \end{tabular}
\end{table} 

 
\subsection{Proof of Correctness}\label{section:proof}
We will now give a formal proof of correctness of our algorithm. Essentially, we
will show that, when an index node is checked (line 18) if it can be immediately accepted
or pruned (using Theorem \ref{thm:accept-prune-condition}),
its NN-lists (especially, upper NN-list) are complete (hence, valid).

First, we want to discuss a few observations. The first fact is, if at
any point of time, a
data point  not belonging to an entry  is covered in , then
 is covered subsequently in the NN-list of .
Since  is covered at this instant, some ancestor  of  must be present in
the NN-list of  at that instant. Observe that after an entry is added to the NN-list of ,
it is removed from the  NN-list of  only when the NN-list of  is updated
with the children of  (lines 21,22). This ensures that  is forever covered in the
NN-list of .

Similarly,  is covered subsequently
in the NN-lists of all (sub-)children of .
At line 18 of the algorithm, if  can't be added to the results or pruned,
after updating its NN-list with each entry present in ,its children are added
to the queue. However, each child of  inherits its NN-list i.e. simply copies
its NN-list (lines 21,22). Therefore, the children of  will also have  in
their NN-list.


\begin{figure}[tp]
\begin{center}
\subfloat[]{\includegraphics[width=0.3\linewidth]{./images/Triangle1}}
\subfloat[]{\includegraphics[width=0.3\linewidth]{./images/Triangle2}}
\caption{\small Indexing tree \label{fig:proof}}
\end{center}
\end{figure}

Now we present the key lemma for our proof of correctness.

\begin{lemma}\label{lemma:proof-3}
The upper NN-list of every entry , which is dequeued from the queue, is complete after line 17 of the \rstknn algorithm.
\end{lemma}
\begin{proof}
Consider an execution of the algorithm, and suppose the current node to be
dequeued from the queue is denoted by . Let  be any data point and 
denote the path from root to  in the tree.
We will prove that after line 17 of the
algorithm,  belongs to . 
There are two
possibilities (see Figure \ref{fig:proof} for reference):
\begin{description}
    \item [Case A]  belongs to sub tree of 
    \item [Case B]  does not belong to sub tree of 
\end{description}

Case A is trivial. If  belongs to sub tree of , it will be present in
 after line 17, since any internal node adds itself to its NN-lists
(line 12).

Let us now consider Case B.
Let  be the time when  is dequeued from the queue.
Now, one of these four different possibilities must be true at .
\begin{description}
    \item[Case B.1] Some node  on the path  belongs to the result list
	.
    \item[Case B.2] Some node  on the path  belongs to the pruned list
	.
    \item[Case B.3] Some node  on the path  belongs to the queue .
    \item[Case B.4]  belongs to the candidate list .
\end{description}
\paragraph{Case B.1}
Let  denote the time when line 17 was encountered after  was dequeued. Once again, there are two
possibilities.
\begin{description}
    \item[Case:  belongs to the queue at ] In this case,  will contain
	 through mutual effect (line 16) at . This implies
	that  is covered by  at .
    \item [Case:  does not belong to the queue at ]
If  does not belong to the queue, it implies that there exists some ancestor
of , say  (cannot be  because of condition of Case B.1) which
belongs to the queue at time . Then  contains  through
mutual effect (line 16). This implies that once
 contains , upper NN-lists of all its discendant nodes will also contain
.
\end{description}
The proof for Case B.2 and Case B.3 is similar to Case B.1.

We now consider the remaining Case B.4. Since  COL, it implies that
some ancestor  of  was dequeued from the queue prior to . All the
node present in the queue then contained  in their upper NN-list through
mutual effect. Therefore at   contained  in its
upper NN-list.
\end{proof}

\begin{theorem}
Given an integer , a query point , and an index tree , the algorithm 2 correctly returns all \rstknn points.
\end{theorem}
\begin{proof}
The correctness follows from the following observations that were made earlier.
\begin{itemize}
    \item Internal nodes are accepted or pruned (by IsHitOrDrop) only when the sufficient
conditions according to the Theorem
\ref{thm:accept-prune-condition} are met (using Lemma \ref{lemma:proof-3}).
    \item For the data points left in the candidate list , in FinalVerification,
the (complete) NN-lists of every such point are updated with every other object
(present in candidate, result and pruned list), before IsHitOrDrop being called
on the point for directly accepting or pruning. Our FinalVerification routine
implements this in a rather straight forward manner. In line 32 of this routine,
internal nodes present in  are replaced with their contained points to
ensure that operations in this routine directly involve points.

\end{itemize}
\end{proof}

\section{Conclusion and Future Work}\label{sec:Conclusion and Future Work}
\rknn is an important problem in facility location, operations research,
clustering and other domains. We observed that a few published algorithms are
not fully correct. In this paper we presented a correct algorithm to compute \rknn on a
general data set organised as a tree. We first discussed counter-examples to
illustrate where the earlier algorithms made an error, and then discussed
the necessity of maintaining locality and completeness conditions for ensuring
the correctness of results. We finished by modifying one of the proposed
algorithms along with an explanation why our algorithm is correct.

In the future, we would like to extend our algorithm for performing bichromatic
\rstknn algorithm. We would further like to develop algorithms where the objects
are dynamic (e.g., moving in space, or textual attributes getting updated).

\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{todsssubmission}

\end{document}
