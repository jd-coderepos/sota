

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{\begin{tabular}{l|llll}
\toprule[1pt]
\multirow{2}{*}{Dataset} & \multirow{2}{*}{\# Img} & Scene & Evaluation & Supervision \\
 &  & Type & Metric & Type \\ \hline
NYU &  & Indoor & AbsRel  \&  & Kinect \\ \hline
ScanNet &  & Indoor & AbsRel \&  & Kinect \\ \hline
2D-3D-S &  & Indoor & LSIV & LiDAR \\ \hline
\multirow{2}{*}{iBims-1} & \multirow{2}{*}{} & \multirow{2}{*}{Indoor} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}AbsRel \&\\   \&\end{tabular}} & \multirow{2}{*}{LiDAR} \\
 &  &  &  &  \\ \hline 
KITTI &  & Outdoor & AbsRel \&  & LiDAR \\ \hline
Sintel &  & Outdoor & AbsRel \&  & Synthetic \\ \hline
ETH3D &  & Outdoor & AbsRel \&  & LiDAR \\ \hline 
YouTube3D &  & In the Wild & WHDR & SfM, Ordinal pairs \\  \hline
\multirow{2}{*}{OASIS} & \multirow{2}{*}{} & \multirow{2}{*}{In the Wild} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}WHDR \\ \& LSIV\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}User clicks, \\ Small patches with GT\end{tabular}} \\
 &  &  &  &  \\ \hline
\multirow{2}{*}{DIODE} & \multirow{2}{*}{} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Indoor\\  \& Outdoor\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}AbsRel \& \\ \end{tabular}} & \multirow{2}{*}{LiDAR} \\
 &  &  &  &  \\ \toprule[1pt]
\end{tabular}}
\caption{Overview of the test sets in our experiments. \label{Tab: testing data details}}
\vspace{-0.5em}
\end{table}

\paragraph{Datasets and implementation details.}
\label{sec:data}
To train the PCM, we sampled K Kinect-captured depth maps from ScanNet, K LiDAR-captured depth maps from Taskonomy, and K synthetic depth maps from the 3D Ken Burns paper~\cite{Niklaus_TOG_2019}.
We train the network using SGD with a batch size of , an initial learning rate of , and a learning rate decay of . For parameters specific to PVCNN, such as the voxel size, we follow the original work~\cite{liu2019pvcnn}.

To train the DPM, we sampled K RGBD pairs from LiDAR-captured Taskonomy~\cite{zamir2018taskonomy}, K synthetic RGBD pairs from the 3D Ken Burns paper~\cite{Niklaus_TOG_2019},  K RGBD pairs from calibrated stereo DIML~\cite{kim2018deep}, K RGBD pairs from web-stereo Holopix50K~\cite{hua2020holopix50k}, and K web-stereo HRWSI~\cite{xian2020structure} RGBD pairs.
Note that when doing the ablation study about the effectiveness of PWN and ILNR, we sampled a smaller dataset which is composed of K images from Taskonomy, K images from DIML, and K images from HRWSI. 
During training,  images are withheld from all datasets as a validation set.
We use the depth prediction architecture proposed in Xian~\etal.~\cite{xian2020structure}, which consists of a standard backbone for feature extraction (e.g., ResNet50~\cite{he2016deep} or ResNeXt101~\cite{xie2017aggregated}), followed by a decoder, and train it using SGD with a batch size of , an initial learning rate  for all layer, and a learning rate decay of . 
Images are resized to  Ã— , and flipped horizontally with a  chance.
Following \cite{yin2020diversedepth}, we load data from different datasets evenly for each batch.

\paragraph{Evaluation details.}
The focal length prediction accuracy is evaluated on 2D-3D-S~\cite{armeni2017joint} following~\cite{hold2018perceptual}. 
Furthermore, to evaluate the accuracy of the reconstructed 3D shape, we use the Locally Scale Invariant RMSE (LSIV)~\cite{chen2020oasis} metric on both OASIS~\cite{chen2020oasis} and 2D-3D-S~\cite{armeni2017joint}. It is consistent with the previous work~\cite{chen2020oasis}. The OASIS~\cite{chen2020oasis} dataset only has the ground truth depth on some small regions, while 2D-3D-S has the ground truth for the whole scene.

\begin{table}[t]
\centering
\resizebox{1\linewidth}{!}{\begin{tabular}{l|ccccc}
\toprule[1pt]
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & ETH3D & NYU & KITTI & Sintel & DIODE \\
\multicolumn{1}{c|}{}                        & \multicolumn{5}{c}{AbsRel }           \\ \hline
Baseline    &     &     &   &   &    \\
Recovered Shift  &     &    &  &   &  \\ \toprule[1pt]
\end{tabular}}
\caption{Effectiveness of recovering the shift from 3D point clouds with the PCM. Compared with the baseline, the AbsRel is much lower after recovering the depth shift over all test sets.\label{Tab: effectiveness of shift prediction. }}
\vspace{-0.5em}
\end{table}

\begin{figure}[t]   
\centering
\includegraphics[width=\linewidth]{./Figs/focal_length_cmp_diff_proposal_S3DIS.pdf}
\caption{Comparison of recovered focal length on the 2D-3D-S dataset. Left, our method outperforms Hold-Geoffroy~\etal~\cite{hold2018perceptual}. Right, we conduct an experiment on the effect of the initialization of field of view (FOV). Our method remains robust across different initial FOVs, with a slight degradation in quality past  and .}
\vspace{-0.5em}
\label{Fig: cmp of focal length prediction.}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{./Figs/cmp_SOTA_PCD.pdf}
\caption{Qualitative comparison. We compare the reconstructed 3D shape of our method with several baselines. As MiDaS~\cite{Ranftl2020} does not estimate the focal length, we use the focal length recovered from \cite{hold2018perceptual} to convert the predicted depth to a point cloud.
``Ours-Baseline'' does not recover the depth shift or focal length and uses an orthographic camera, while ``Ours'' recovers the shift and focal length. We can see that our method better reconstructs the 3D shape, especially at edges and planar regions (see arrows).}
\label{Fig: cmp of SOTA 3D shape. }
\vspace{-0.5em}
\end{figure*}

To evaluate the generalizability of our proposed depth prediction method, we take  datasets which are unseen during training, including YouTube3D~\cite{chen2019learning}, OASIS~\cite{chen2020oasis}, NYU~\cite{silberman2012indoor}, KITTI~\cite{geiger2012we}, ScanNet~\cite{dai2017scannet}, DIODE~\cite{vasiljevic2019diode}, ETH3D~\cite{schops2017multi}, Sintel~\cite{Butler:ECCV:2012}, and iBims-1~\cite{Koch18:ECS}. 
On OASIS and YouTube3D, we use the Weighted Human Disagreement Rate (WHDR)~\cite{xian2018monocular} for evaluation. 
On other datasets, except for iBims-1, we evaluate the absolute mean relative error (AbsRel) and the percentage of pixels with . 
We follow Ranftl~\etal~\cite{Ranftl2020} and align the scale and shift before evaluation. 
To evaluate the geometric quality of the depth, i.e. the quality of edges and planes, we follow~\cite{Niklaus_TOG_2019, xian2020structure} and evaluate the depth boundary error~\cite{Koch18:ECS} () as well as the planarity error~\cite{Koch18:ECS} () on iBims-1. 
 and  evaluate the flatness and orientation
of reconstructed 3D planes compared to the ground truth 3D planes respectively, while  and  demonstrate the localization accuracy and the sharpness of edges respectively. More details as well as a comparison of these test datasets are summarized in Tab.~\ref{Tab: testing data details}

\subsection{3D Shape Reconstruction}

\paragraph{Shift recovery.}
To evaluate the effectiveness of our depth shift recovery, we perform zero-shot evaluation on  datasets unseen during training.
We recover a 3D point cloud by unprojecting the predicted depth map, and then compute the depth shift using our PCM. 
We then align the unknown scale~\cite{bian2019unsupervised, monodepth2} of the original depth and our shifted depth to the ground truth, and evaluate both using the AbsRel error.
The results are shown in Tab.~\ref{Tab: effectiveness of shift prediction. }, where we see that, on all test sets, the AbsRel error is lower after recovering the shift. 
We also trained a standard 2D convolutional neural network to predict the shift given an image composed of the unprojected point coordinates, but this approach did not generalize well to samples from unseen datasets. 





\paragraph{Focal length recovery.}

To evaluate the accuracy of our recovered focal length, we follow Hold-Geoffroy~\etal~\cite{hold2018perceptual} and compare on the 2D-3D-S dataset, which is unseen during training for both methods. 
The model of~\cite{hold2018perceptual} is trained on the in-the-wild SUN360~\cite{xiao2012recognizing} dataset. 
Results are illustrated in Fig.~\ref{Fig: cmp of focal length prediction.}, where we can see that our method demonstrates better generalization performance.
Note that PVCNN is very lightweight and only has  parameters, but shows promising generalizability, which could indicate that there is a smaller domain gap between datasets in the 3D point cloud space than in the image space where appearance variation can be large.


Furthermore, we analyze the effect of different initial focal lengths during inference. 
We set the initial field of view (FOV) from  to  and evaluate the accuracy of the recovered focal length, Fig.~\ref{Fig: cmp of focal length prediction.} (right). 
The experimental results demonstrate that our method is not particularly sensitive to different initial focal lengths.


\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{l|ll}
\toprule
\multirow{2}{*}{Method} & OASIS & 2D-3D-S \\
                        & LSIV  & LSIV  \\ \hline \hline  
\multicolumn{3}{c}{Orthographic Camera Model} \\ \hline \hline               
MegaDepth~\cite{li2018megadepth}               &       &       \\
MiDaS~\cite{Ranftl2020}               &       &       \\
Ours-DPM              &       &      \\ \hline \hline
\multicolumn{3}{c}{Pinhole Camera Model} \\ \hline \hline 
MegaDepth~\cite{li2018megadepth} + Hold-Geoffroy~\cite{hold2018perceptual} &  &      \\
MiDaS~\cite{Ranftl2020} + Hold-Geoffroy~\cite{hold2018perceptual}& &       \\
MiDaS~\cite{Ranftl2020} + Ours-PCM & &       \\
Ours                   &       &       \\ 
\bottomrule
\end{tabular}}
\vspace{0.4em}
\caption{Quantitative evaluation of the reconstructed 3D shape quality on OASIS and 2D-3D-S. Our method can achieve better performance than previous methods. Compared with the orthographic projection, our method using the pinhole camera model can obtain better performance. DPM and PCM refers to our depth prediction module and point cloud module respectively.
\label{Tab: shape evaluation on OASIS}}
\vspace{-2em}
\end{table}



\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{./Figs/cmp_SOTA.pdf}
\caption{Qualitative comparisons with state-of-the-art methods, including MegaDepth~\cite{li2018megadepth}, Xian~\etal\cite{xian2020structure}, and MiDaS~\cite{Ranftl2020}. It shows that our method can predict more accurate depths at far locations and regions with complex details. In addition, we see that our method generalizes better on in-the-wild scenes.\label{Fig: cmp of SOTA. }}
\vspace{-0.5em}
\end{figure*}

\paragraph{Evaluation of 3D shape quality.}
Following OASIS~\cite{chen2020oasis}, we use LSIV for the quantitative comparison of recovered 3D shapes on the OASIS~\cite{chen2020oasis} dataset and the 2D-3D-S~\cite{armeni2017joint} dataset. 
OASIS only provides the ground truth point cloud on small regions, while 2D-3D-S covers the whole 3D scene. Following OASIS~\cite{chen2020oasis}, we evaluate the reconstructed 3D shape with two different camera models, i.e. the orthographic projection camera model~\cite{chen2020oasis} (infinite focal length) and the (more realistic) pinhole camera model. 
As MiDaS~\cite{Ranftl2020} and MegaDepth~\cite{li2018megadepth} do not estimate the focal length, we use the focal length recovered from Hold-Geoffroy~\cite{hold2018perceptual} to convert the predicted depth to a point cloud.
We also evaluate a baseline using MiDaS instead of our DPM with the focal length predicted by our PCM (``MiDaS + Ours-PCM'').
From Tab.~\ref{Tab: shape evaluation on OASIS} we can see that with an orthographic projection, our method (``Ours-DPM'') performs  roughly as well as existing state-of-the-art methods. However, for the pinhole camera model our combined method significantly outperforms existing approaches. Furthermore, comparing ``MiDaS + Ours-PCM'' and ``MiDaS + Hold-Geoffroy'', we note that our PCM is able to generalize to different depth prediction methods. 

A qualitative comparison of the reconstructed 3D shape on in-the-wild scenes is shown in Fig.~\ref{Fig: cmp of SOTA 3D shape. }. 
It demonstrates that our model can recover more accurate 3D scene shapes. For example, planar structures such as walls, floors, and roads are much flatter in our reconstructed scenes, and the angles between surfaces (\eg walls) are also more realistic. Also, 
the shape of the car has less distortions.




\begin{table}[t]
\small
\centering
\begin{threeparttable}
\resizebox{1\linewidth}{!}{\begin{tabular}{l|ccccc}
\toprule[1pt]
\multirow{2}{*}{Method} & \multicolumn{4}{c}{iBims-1}                                    \\ \cline{2-6} 
        &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &AbsRel\\ \hline
Xian~\cite{xian2020structure}         &  & \multicolumn{1}{l|}{}         & & \multicolumn{1}{l|}{}   &    \\
MegaDepth~\cite{li2018megadepth}    &          & \multicolumn{1}{l|}{} &  &\multicolumn{1}{l|}{ } & \\
MiDaS~\cite{Ranftl2020}        & & \multicolumn{1}{l|}{} & & \multicolumn{1}{l|}{}    &    \\

3D Ken Burns~\cite{Niklaus_TOG_2019}   &  & \multicolumn{1}{l|}{} &    & \multicolumn{1}{l|}{} &   \\
\hline \hline
Ours\tnote{\dag} \, w/o PWN  &  & \multicolumn{1}{l|}{}  &   & \multicolumn{1}{l|}{} &   \\
Ours\tnote{\dag}    &  & \multicolumn{1}{l|}{}  &   & \multicolumn{1}{l|}{} &        \\
Ours Full   &  & \multicolumn{1}{l|}{}  &   & \multicolumn{1}{l|}{}  &       \\
\bottomrule
\end{tabular}}
\end{threeparttable}
\vspace{0.4em}
\caption{Quantitative comparison of the quality of depth boundaries (DBE) and planes (PE) on the iBims-1 dataset. We use  to indicate when a method was trained on the small training subset.\label{Tab: cmp of edges and planes}}
\vspace{-2em}
\end{table}

\subsection{Depth prediction}
In this section, we conduct several experiments to demonstrate the effectiveness of our depth prediction method, including a comparison with state-of-the-art methods, a comparison of our proposed image-level normalized regression loss with other methods, and an analysis of the effectiveness of our pair-wise normal regression loss. 

\begin{table*}[t]
\setlength{\tabcolsep}{2pt}
\resizebox{\linewidth}{!}{\begin{tabular}{l|l|ll|ll|ll|ll|ll|ll|ll|l}
\toprule[1pt]
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & OASIS & YT3D & \multicolumn{2}{c|}{NYU} & \multicolumn{2}{c|}{KITTI} & \multicolumn{2}{c|}{DIODE} & \multicolumn{2}{c|}{ScanNet} & \multicolumn{2}{c|}{ETH3D} & \multicolumn{2}{c|}{Sintel} & \multirow{2}{*}{Rank} \\
&   &\multicolumn{2}{c|}{WHDR}    & AbsRel     &      & AbsRel      &       & AbsRel      &       &AbsRel      &        &AbsRel     &       & AbsRel       &       &                       \\ \hline
OASIS~\cite{chen2020oasis}  &ResNet50  &  & & & & &  &   & & & & & & &  &  \\ 
MegaDepth~\cite{li2018megadepth}& Hourglass &  &  &&  & & & & & & &  & & &  & \\
Xian~\cite{xian2020structure} &ResNet50 & & & & &   &  & & & & & & & & &     \\
WSVD~\cite{wang2019web} &ResNet50 &  &  & & & & & & & & & & &  &  & \\
Chen~\cite{chen2019learning} &ResNet50  & &  &   &  &  &  & &  &  &   & & & &  &    \\
DiverseDepth~\cite{yin2020diversedepth}&ResNeXt50 & & & & & & & & & & & & & &  &  \\
MiDaS~\cite{Ranftl2020}&ResNeXt101 & & & & & & & & & &  &  & & & &  \\
\hline
Ours &ResNet50 & &  &  &  & & & & & & & & & & &   \\     
Ours &ResNeXt101 & &  &  &  & & & & & & & & & & &  
\\ \toprule[1pt]
\end{tabular}}
\caption{Quantitative comparison of our depth prediction with state-of-the-art methods on eight zero-shot (unseen during training) datasets. Our method achieves better performance than existing state-of-the-art methods across all test datasets. \label{Tab: Cmp with SOTA}}
\vspace{-1.5em}
\end{table*}



\paragraph{Comparison with state-of-the-art methods.}
In this comparison, we test on datasets unseen during training.
We compare with methods that have been shown to best generalize to in-the-wild scenes. Their results are obtained by running the publicly released code. Each method is trained on its own proposed datasets.
When comparing the AbsRel error, we follow Ranftl~\cite{Ranftl2020} to align the scale and shift before the evaluation. 
The results are shown in the Tab.~\ref{Tab: Cmp with SOTA}. Our method outperforms prior works, and using a larger ResNeXt101 backbone further improves the results. Some qualitative comparisons can be found in Fig.~\ref{Fig: cmp of SOTA. }



 \paragraph{Pair-wise normal loss.} 
 To evaluate the quality of our full method and dataset on edges and planes, we compare our depth model with previous state-of-the-art methods on the iBims-1 dataset.
 In addition, we evaluate the effect of our proposed pair-wise normal (PWN) loss through an ablation study. As training on our full dataset is computationally demanding, we perform this ablation on the small training subset.
The results are shown in Tab.~\ref{Tab: cmp of edges and planes}. 
 We can see that our full method outperforms prior work for this task.
 In addition, under the same settings, both edges and planes are improved by adding the PWN loss. 
We further show a qualitative comparison in Fig.~\ref{Fig: cmp of pcd with PWN. }.





\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{./Figs/PWN_effect.pdf}
\caption{Qualitative comparison of reconstructed point clouds. Using the pair-wise normal loss (PWN), we can see that edges and planes are better reconstructed (see highlighted regions).}
\label{Fig: cmp of pcd with PWN. }
\vspace{-1em}
\end{figure}

\paragraph{Image-level normalized regression loss.}
To show the effectiveness of our proposed image-level normalized regression (ILNR) loss, we compare it with the scale-shift invariant loss (SSMAE)~\cite{Ranftl2020} and the scale-invariant multi-scale gradient loss~\cite{wang2019web}. 
Each of these methods is trained on the small training subset to limit the computational overhead, and comparisons are made to datasets that are unseen during training. 
All models have been trained for  epochs, and we have verified that all models fully converged by then. 
The quantitative comparison is shown in Tab.~\ref{Tab. cmp of normalization loss}, where we can see an improvement of ILNR over other scale and shift invariant losses. 
Furthermore, we also analyze different options for normalization, including image-level Min-Max (ILNR-MinMax) normalization and image-level median absolute deviation (ILNR-MAD) normalization, and found that our proposed loss performs a bit better.


