\begin{abstract}
Image retrieval under varying illumination conditions, such as day and night images, is addressed by image preprocessing, both hand-crafted and learned. Prior to extracting image descriptors by a convolutional neural network, images are photometrically normalised in order to reduce the descriptor sensitivity to illumination changes.
We propose a learnable normalisation based on the U-Net architecture, which is trained on a combination of single-camera multi-exposure images and a newly constructed collection of similar views of landmarks during day and night.
We experimentally show that both hand-crafted normalisation based on local histogram equalisation and the learnable normalisation outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such as Oxford or Paris datasets.

\end{abstract}

\section{Introduction}

Since the first successful image retrieval methods~\cite{Sivic-ICCV03,Nister-CVPR06}, the field went through a rapid development. Numerous methods based on local features~\cite{Mikolajczyk-IJCV04,Matas-BMVC02} and their descriptors~\cite{Lowe-IJCV04} were improved in many directions, including spatial verification~\cite{Philbin-CVPR07,Jegou-ECCV08,Perdoch-CVPR09}, descriptor aggregation~\cite{Jegou-CVPR10,Perronnin-CVPR10}, and convolutional neural network (CNN) based feature detectors~\cite{Yi-ECCV16} and descriptors~\cite{Tian-CVPR17,Mishchuk-NIPS17}. Recently, image retrieval approaches based on global CNN descriptors~\cite{Arandjelovic-CVPR16,Gordo-ECCV16,Radenovic-TPAMI18} started to dominate due to their efficiency both in the search time and memory footprint. 

The challenges of image and particular object retrieval lie mainly in increasing the efficiency for large collections of images and in improving the quality of retrieved results. Scaling up to very large collections of images is addressed by efficient extraction of global CNN features and consequent efficient encoding~\cite{Jegou-PAMI11} and nearest neighbour search~\cite{Babenko-CVPR16,Johnson-17}.
Another direction of research considers retrieval of instances that exhibit significant geometric and/or photometric changes with respect to the query.

Various types of geometric changes appear in image collections, for example change of scale, such as when the query object covers only a small part of the database image, change in the view-point, and severe occlusion. Methods based on local features and efficient geometric verification~\cite{Stewenius-ECCV12} have shown good retrieval performance on significant geometric changes~\cite{Mikulik-SISAP13,Mikulik-ACCV14}.

\begin{figure}[t] \centering
    \includegraphics[scale=0.26]{illustrations/motivation/orig_query.png}
    \hspace{0.6cm}
    \includegraphics[scale=0.26]{illustrations/motivation/vgg_retrieved_1.png}
    \includegraphics[scale=0.26]{illustrations/motivation/vgg_retrieved_2.png}
    \includegraphics[scale=0.26]{illustrations/motivation/vgg_retrieved_3.png}
\\
    \includegraphics[scale=0.26]{illustrations/motivation/jointly_normalized_query.png} \hspace{0.6cm}
    \includegraphics[scale=0.26]{illustrations/motivation/jointly_retrieved_1.png}
    \includegraphics[scale=0.26]{illustrations/motivation/jointly_retrieved_2.png}
    \includegraphics[scale=0.26]{illustrations/motivation/jointly_retrieved_3.png}\3pt]

    \caption{For fine-tuning, the normalisation network (U-Net) is prepended to the embedding network (VGG) and both are trained in a Siamese manner on pairs of images. Each image of the input pair is first normalised and then embedded. A contrastive loss is applied to the distance between resulting descriptors.}
\label{fig:finetune}
\end{figure}
 
The main contribution of this paper is the introduction of the normalisation step. We propose performing photometric normalisation prior to extracting the descriptors. Both hand-crafted and learned normalisation is evaluated. We construct a training day-night dataset from existing 3D reconstructions which was made publicly available. Both the proposed normalisation and the constructed dataset is experimentally shown to improve the performance on challenging queries.


\section{Related work}

To reduce the sensitivity of local feature descriptors to illumination changes, an intensity normalisation step is introduced to the descriptor generation process, as in one of the most popular descriptors, SIFT~\cite{Lowe-IJCV04}. Another approach is based on geometric hashing~\cite{Lamdan-ICCV88,Chum-CVPR06} where the feature descriptor is not based on the appearance but on mutual positions of near-by features. 

Approaches making the local-feature descriptor insensitive to illumination changes alone are not sufficient to match difficult image pairs, as they rely on the feature detector to fire at the same locations despite the illumination change. One of the first approaches to learn illumination invariant feature detector was a Temporally Invariant Learned DEtector (TILDE)~\cite{Verdie-CVPR15}. In TILDE, the detector is trained on a dataset of images from 6 different scenes collected over time by still web cameras pointing out of a window. First, feature point candidates are selected. The selection criterion is stability across a number of roughly aligned webcam images collected over time. A regressor giving high responses in the candidate locations and low everywhere else is learned. 

The problem of day and night visual self-localisation using GPS-annotated Google StreetView images is addressed in~\cite{Torii-CVPR2015}.
The \Tokyo dataset of day, sunset and night images taken by a cell phone camera is used for query images. The authors demonstrate that for a dense VLAD descriptor~\cite{Jegou-CVPR10}, matching across large changes in the scene appearance becomes much easier when both the query image and the database image depict the scene from approximately the same viewpoint. To perform the visual localisation, StreetView panoramas and corresponding depth-maps are used to render a large number of virtual views by ray-tracing with view-points on a 5m  5m grid and 12 view directions at every view-point. Significant boost in performance is achieved when the queries are matched against the virtual views rather than the original panoramas.
The \Tokyo dataset is described in more detail in section~\ref{sec:experiments} as we use it for evaluation.

EdgeMAC~\cite{Radenovic-ECCV18} performs reasonable image matching in the presence of a significant change in illumination, especially when the colours and textures are corrupted. However, for a standard imaginary, dropping all the information but edges certainly degrades the performance, as already observed by \cite{Radenovic-ECCV18} and confirmed by our experiments.

Methods enhancing visual quality of images taken under bad light conditions were proposed. In~\cite{Chen-CVPR18}, raw output from the image sensor is taken and a neural network is used to enhance the visual appearance, as if the image was taken with long exposure. Camera (sensor) dependent models are learned from a dataset of multiple-exposure images of static scenes with qualitatively very impressive results.


\section{Photometric normalisation}

Image descriptors for image retrieval are extracted by a system of two components: photometric normalisation and embedding network. The normalisation translates images to an image domain less sensitive to illumination changes. The embedding network provides the mapping from the image to the descriptor space, in which  nearest neighbour search is used to retrieve similar images.
Two types of photometric normalisation are investigated, image preprocessing by hand-crafted normalisation and a normalisation network prepended to the embedding network.
 
\subsection{Hand-crafted normalisation}

Hand-crafted normalisation, specifically {\bf histogram equalisation}, {\bf CLAHE} and {\bf gamma correction}, is tested first in order to evaluate the need for a learnable normalisation network. We refer to~\cite{szeliski2010computer} for detailed  description of the algorithms. In the proposed pipeline, the image to be normalised is converted from RGB to LAB colour space, intensity transformation is applied on the lightness channel, and the image is converted back to RGB colour space before being used as an input to the embedding network.

In histogram equalisation, monotonic pixel intensity mapping is found, so that the histogram of mapped intensities is flat. 

In adaptive histogram equalisation, the image is divided into non-overlapping blocks and histogram equalisation is performed on each block independently. Each pixel intensity is then bilinearly interpolated from the four closest block mapped intensities, making transitions between blocks smooth. 
When a contrast limit is applied, the original histogram is mapped to a clipped histogram, which is not uniformly distributed in general.
The clipped histogram is constructed from the original histogram by uniformly redistributing pixels from the frequent intensity bins (bins whose value exceeds the clip limit)~\cite{szeliski2010computer}.
With clip limit equal to 1, the resulting histogram is flat, so the result is identical to histogram equalisation. The Contrast Limited Adaptive Histogram Equalisation (CLAHE) is a combination of all the techniques described above.

In gamma correction, pixel values in the range between 0 and 1 are raised to the power of chosen positive exponent. The exponent in gamma correction is chosen for each image such that the corrected image mean is equal to the dataset average. This is performed via a fast secant method that allows to perform it during image loading.

\paragraph{Implementation details.} For CLAHE, each image is split into a grid of 8x8 windows, so that the longer side of each window is 45px. The clip limit is set to 4 for all experiments which consistently yielded the best results.

\subsection{Learnable normalisation}

In this section, the architecture of the normalisation network is described and the details of its separate pre-training, including the description of the dataset used, are given.

\begin{figure*} \centering
    \includegraphics[width=4.1cm]{{illustrations/sid/fuji_00034_00_0.1s_short.400}.jpg}
    \includegraphics[width=4.1cm]{{illustrations/sid/fuji_00034_00_0.1s_0.2.400}.jpg}
    \includegraphics[width=4.1cm]{{illustrations/sid/fuji_00034_00_10s_long.400}.jpg}
    \includegraphics[width=4.1cm]{{illustrations/sid/fuji_00034_00_0.1s_3.0.400}.jpg}\4pt]

    \caption{Pre-training of the normalisation network on pixel-aligned image pairs. Each image pair is converted from RGB to LAB from which only the lightness channel is kept. The input image lightness channel (bottom-left) is transformed to the statistics of the target image lightness channel (top-left) through histogram matching. The resulting channel is concatenated with the input image RGB channels and fed to the normalisation network (U-Net). The loss function (MSE) is computed between the RGB image outputted by the network and the target RGB image.}
\label{fig:pretrain}
\end{figure}
 

\subsubsection{Pre-training}

The normalisation network is first trained on pixel-aligned pairs of images taken in different illumination conditions. The goal is, given one of the images (input) and statistics of the other (target) to reconstruct the target image. For the embedding network, we use the off-the-shelf pre-trained VGG retrieval CNN provided by~\cite{Radenovic-TPAMI18}.

Pre-training of the normalisation network is performed using the See-in-the-dark dataset. A pair of input and target image is chosen randomly from a set of images of each scene. No constraints are set on the pair, the target image can have both longer or shorter exposure time than the input image. The pre-training is summarised in Fig.~\ref{fig:pretrain}.

The loss function is the mean squared error between the predicted and target image, computed over all pixels. The network is trained for 44 thousand iterations with a batch size equal to 5. An SGD optimiser with learning rate of 0.001, momentum 0.1 and weight decay  is used.


\section{Fine-tuning for retrieval}
The proposed illumination-invariant retrieval method is fine-tuned in a two-stage process.
In the first stage, the embedding network is fine-tuned separately, without normalisation. In all experiments, the VGG network architecture with GeM pooling as provided by the authors of~\cite{Radenovic-TPAMI18} is used. The network is trained from off-the-shelf classification network~\cite{SZ14} minimising the contrastive loss on the image descriptors, following the procedure of~\cite{Radenovic-TPAMI18}.
In the second stage, normalisation is prepended to the embedding network and the final composition is fine-tuned also using the contrastive loss and in the same setup. This is common for both hand-crafted and learnable normalisation. In case of learnable normalisation, different scenarios are distinguished based on which network is trained.
A common practice in image retrieval is to apply whitening on the image descriptors extracted by the embedding network. Specific whitening is learned for each trained network following the procedure of~\cite{Radenovic-TPAMI18}. In all our experiments, retrieval is performed with whitened descriptors.

\subsection{Training datasets}
Two datasets were used to fine-tune our network - one of them is publicly available, the other one is newly created. In the following, we provide an overview of these datasets. Example images of these datasets are shown in Fig.~\ref{fig:dataset_sfm}.

\paragraph{Retrieval-SfM} dataset is used in~\cite{Radenovic-TPAMI18} to fine-tune a CNN for retrieval. We use the predefined geometrically validated image clusters and hard negative mining procedure as described in~\cite{Radenovic-TPAMI18}. However, most of the selected anchor and positive images are pairs of daylight images, occasionally a pair of night images is included, see Fig.~\ref{fig:dataset_sfm}~(a). 

\paragraph{Retrieval-SfM-N/D} is a novel dataset constructed from the same 3D reconstruction as Retrieval-SfM. We extracted hard positive image pairs with different lighting conditions, these hard positives are complementary to those provided in Retrieval-SfM. Example images are shown in Fig.~\ref{fig:dataset_sfm} (b). This dataset was made available on the project web page\footnote{http://cmp.felk.cvut.cz/daynightretrieval}.

In~\cite{Radenovic-TPAMI18}, in order to ensure the same surface is visible in positive image pairs, a certain number of features reconstructed to a common 3D point is required. However, even two geometrically very similar views with significant change in illumination may share only a small number of matching SIFT features. To find images observing the same scene surface, we approximate the surface visible in an image by a ball. The centre of this ball is equal to the mean of 3D points reconstructed for an image and the radius is given by a standard deviation of those points. To validate that two images depict the same part of the surface, the intersection over union of corresponding balls must be greater than 0.55. Furthermore, for a positive image pair, the angle between estimated camera optical axes is limited to 45 degrees.
The (relatively rough) ball approximation followed by volume intersection over union measure is very fast and exhaustively applicable to even large 3D models, providing satisfactory results for a wide range of objects without obvious false positives.

The procedure above assigns to each image participating in a 3D model a list of potential positive images.
The hard positive image pairs are chosen so that they maximise the difference in illumination among geometrically similar images. We measure the illumination difference as the difference in a trimmed-mean value of lightness in the LAB colour space where the lightest 40\% and darkest 40\% of pixels are dropped. This measure is robust to the presence of image frames, large occluding objects, \etc, which can be either light or dark.

We have constructed 20 thousand illumination-hard-positive image pairs with the largest difference in the illumination. The anchor image is chosen to be the darker image and a positive example the lighter. For the anchor images, a standard hard negative mining is performed during training~\cite{Radenovic-TPAMI18}. 

\subsection{Fine-tuning}

To fine-tune the composition of normalisation and embedding network, three approaches are compared. First, the embedding network is frozen and only the normalisation network is fine-tuned for retrieval. Second, the normalisation network is frozen, and the embedding network is trained. Finally, both networks are trained jointly with alternating update of the normalisation and the embedding network. All three approaches are trained on a mixture of Retrieval-SfM and Retrieval-SfM-N/D hard positives and mined hard negatives and this mixture is also used for consequent whitening.

In all three approaches, the training procedure of~\cite{Radenovic-TPAMI18} is followed. The training is performed for 4 thousand iterations, 10 epochs of 400 iterations each, with a batch size of 5. All images are downscaled to have the longer edge equal to 362 px for training and to 1024 px for validation. For each anchor image, five hard negative images are mined at the beginning of each epoch. In each epoch, hard negatives for 2 thousand query images are mined from the pool of 20 thousand images. The margin in the contrastive loss is set to 0.75.

\paragraph{Fine-tuning} of the {\bf normalisation} network. The gradient from the contrastive loss is backpropagated through the embedding network to the normalisation network. Weights of the embedding network are not updated during backpropagation, treating the embedding network as a loss function of the normalisation network. The learning parameters for the normalisation network remain the same as in pre-training.

\begin{figure}[t]
{\centering
    \includegraphics[width=.326\linewidth]{{illustrations/qualitative/00165_cropped_framed}.jpg}
    \includegraphics[width=.326\linewidth]{{illustrations/qualitative/00165_clahe_cropped_framed}.jpg}
    \includegraphics[width=.326\linewidth]{{illustrations/qualitative/00165_fixed_cropped_framed}.jpg}
    
    \includegraphics[width=.326\linewidth]{{illustrations/qualitative/00165_cutout}.jpg}
    \includegraphics[width=.326\linewidth]{{illustrations/qualitative/00165_clahe_cutout}.jpg}
    \includegraphics[width=.326\linewidth]{{illustrations/qualitative/00165_fixed_cutout}.jpg}

    \caption{Unaltered image from Tokyo 24/7 dataset (left), normalised by CLAHE (middle) and by U-Net from U-Net jointly N/D model (right). Best viewed on a computer screen.}
\label{fig:qualitative}}
\end{figure}
 
\paragraph{Fine-tuning} of the {\bf embedding} network is performed with the Adam optimiser with a learning rate of , weight decay of  and momentum parameters  and ~\cite{Radenovic-TPAMI18}.

\paragraph{Joint fine-tuning} uses a separate optimiser for each network, due to the sensitivity of both U-Net and pre-trained VGG to the optimiser choice. SGD is used to update the normalisation network while Adam is used to update the embedding network. The parameters for each optimiser are the same as in normalisation and embedding network fine-tuning. The training updates weights of only one network at a time, alternating the networks every 10 iterations.


\section{Experiments} \label{sec:experiments}

To evaluate the effect of the proposed image normalisation, we test all methods on two standard benchmarks for image retrieval, and propose a new evaluation protocol for image retrieval with severe illumination changes. We compare hand-crafted and learned normalisation with state-of-the-art baselines. The effects of hand-crafted and learned normalisation are visualised in Fig.~\ref{fig:qualitative}.

\newcommand{\doubledim}{{\color{red}\bf !}\xspace}
\newcommand{\firstsup}[1]{{\bf\color{red} #1}}
\newcommand{\secondsup}[1]{{\bf #1}}
\setlength{\tabcolsep}{6.8pt}
\begin{table}[t] \centering
\begin{tabular}{|l||r||r|r|r|}\hline
    Method & Avg & Tokyo & \roxf & \rpar \\\hline\hline
    VGG GeM~\cite{Radenovic-TPAMI18} & 69.9 & 79.4 & \firstsup{60.9} & 69.3 \\\hline
    EdgeMAC~\cite{Radenovic-ECCV18} & 45.6 & 75.9 & 17.3 & 43.5 \\\hline\hline
VGG GeM N/D & 71.1 & 83.5 & 60.0 & 69.8 \\\hline
    EdgeMAC+VGG \doubledim & 71.2 & 85.4 & 59.4 & 68.8 \\\hline\hline
Gamma corr. N/D & 70.9 & 84.6 & 59.5 & 68.7 \\\hline
    Histogram eq. N/D & 71.6 & \secondsup{86.8} & 59.6 & 68.3 \\\hline
    CLAHE & 71.6 & 84.1 & \secondsup{60.8} & 69.8 \\\hline
    CLAHE N/D & \firstsup{72.4} & \firstsup{87.0} & 60.2 & \firstsup{70.0} \\\hline\hline
U-Net embed N/D & 70.9 & 86.4 & 58.1 & 68.3\\\hline
    U-Net norm N/D & 71.0 & 83.2 & 60.0 & \secondsup{69.9} \\\hline
    U-Net jointly & 69.8 & 79.8 & 59.9 & 69.7 \\\hline
    U-Net jointly N/D & \secondsup{72.1} & 86.5 & 60.2 & 69.6 \\\hline
\end{tabular}\-0.7em]\hline
Edg+VGG 512 & 70.0 & 81.1 & {\bf 60.1} & 68.9 \\\hline
    Edg+VGG N/D 512 & 71.1 & 85.4 & 59.2 & 68.7 \\\hline\hline
    Edg+CLAHE N/D 512 & {\bf 72.4} & {\bf 88.4} & 59.4 & {\bf 69.3} \\\hline
    Edg+U-Net jointly N/D 512 & 72.1 & 87.8 & 59.8 & 68.7 \\\hline
\end{tabular}\3pt]

    \caption{The performance (measured by mAP) for chosen methods from Tab.~\ref{tab:results} on the \Tokyo dataset for different lighting conditions of the query and retrieved pair of images. Each column corresponds to the query image being taken either during day (D), sunset (S) or night (N) and the retrieved image being taken during one of the remaining two.}
\label{tab:titech_classes}
\end{table}
 
\subsection{Retrieval results}
The retrieval results are summarised in Tab.~\ref{tab:results}. Methods followed by ``ND'' were trained using a mixture of Retrieval-SfM and Retrieval-SfM-N/D datasets with the ratio 3:1 respectively, while other methods were trained using Retrieval-SfM only. Methods with a citation were taken from publicly available sources.

\paragraph{(i)} All image normalisation methods outperform the baseline methods with the same descriptor dimensionality (VGG GeM and EdgeMAC) on the \Tokyo dataset by a large margin, see Tab.~\ref{tab:results}. 
Combining VGG GeM and EdgeMAC descriptors delivers satisfactory results on the \Tokyo dataset at the cost of an increased descriptor dimensionality. However, the performance of the concatenated descriptors is slightly decreased on the Oxford and Paris datasets. 

\paragraph{(ii)} The effect of the newly introduced dataset, Retrieval-SfM-N/D, is visible in both cases, without the normalisation step -- comparing \mdl{VGG GeM} and \mdl{VGG GeM N/D}, and with the normalisation step -- comparing \mdl{CLAHE} and \mdl{CLAHE N/D}, or \mdl{U-Net jointly} and \mdl{U-Net jointly N/D} methods.

\paragraph{(iii)} An embedding network with no photometric normalisation fine-tuned on the novel dataset \mdl{VGG GeM N/D} performs better than the baseline \mdl{VGG GeM}, but is still inferior to methods using a photometric normalisation.

\paragraph{(iv)} The two best performing methods -- \mdl{CLAHE N/D} and \mdl{U-Net jointly N/D} -- preform similarly on all datasets, and are closely followed by another three methods \mdl{U-Net norm N/D}, \mdl{Histogram eq. N/D} and \mdl{CLAHE}.

\paragraph{(v)} Performance can be further increased by creating an ensemble of VGG GeM and EdgeMAC.
In all cases, methods trained with the proposed Retrieval-SfM-N/D dataset outperform comparable methods that do not use it. Similarly, the photometric normalisation always improves the results even when combined with EdgeMAC.

\vspace{.5\baselineskip} From the experiments, we conclude that the photometric normalisation significantly improves the performance (i), and that training the network on image pairs exhibiting illumination changes, such as Retrieval-SfM-N/D, is important (ii). The photometric normalisation enhances visual information that is difficult to capture for the embedding network alone, even when trained on data exhibiting illumination changes (iii). The currently proposed learnable photometric normalisation does not provide additional information over the CLAHE normalisation, that cannot be extracted later by the embedding network (iv). This is supported by the fact that freezing the normalisation network pre-trained for a different task (\mdl{U-Net embed N/D}) is beneficial for retrieval result on \Tokyo, comparably to \mdl{U-Net jointly N/D}.

We further analyse the performance on the \Tokyo dataset with respect to different light conditions of the query and retrieved images by breaking down the dataset illumination types. In Tab. \ref{tab:titech_classes}, we provide results for the six available combinations query-type  database-type, such as a night query retrieving a day image (denoted as ND).

It can be seen that the lowest scores are obtained for day-night image pairs, followed by sunset-night image pairs where the query is either one of the pair. For those four cases, presented methods bring the largest improvement.


\section{Conclusions}

In this work, we proposed a photometric normalisation step for image retrieval under varying illumination conditions. We have experimentally shown that such a normalisation significantly improves the performance in the presence of significant illumination changes, while preserving the state-of-the-art performance in similar illumination conditions. We have compared several methods, both hand-crafted and learnable. The best performing methods based on CLAHE and on the proposed learned normalisation with the U-Net architecture perform similarly well, while the hand-crafted method being significantly faster. 
Further, we have constructed a novel dataset Retrieval-SfM-N/D. The importance of fine-tuning the network on training data that exhibit significant changes in illumination was shown.
 