\documentclass{article}








\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2020}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      


\usepackage[english]{babel}

\usepackage{url}
\usepackage{enumitem}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{enumitem}


\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{ifthen}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{supertabular}
\usepackage{array}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{psfrag}
\usepackage{amsmath}
\usepackage{hhline}
\usepackage{type1cm}
\usepackage{lettrine}
\usepackage{algorithm}
\usepackage[normalem]{ulem}

\usepackage[noend]{algpseudocode}



\usepackage{pgfplots}
\usepackage{textcomp}



\usepackage{array,caption,tabularx,ragged2e,booktabs,ctable}
\usepackage{booktabs} \usepackage{braket}

\usepackage{comment}
\usepackage{multicol}
\usepackage{multirow}

\usepackage{mathalfa}



\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{multicol}
\usepackage{multirow}
\usepackage{array, tabularx,  ragged2e,  booktabs}
\usepackage{natbib}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}

\usepackage{ctable}
\usepackage{wrapfig}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows,trees,shapes,fit}

\usepackage[english]{babel} 




\usepackage{babel}
\usepackage[export]{adjustbox}
\usepackage{cleveref}



\crefformat{section}{\S#2#1#3} \crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}




\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathcal{S}}

\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\real}{\rm I\!R}

\let\ab\allowbreak
 \usepackage{xspace}

\newcommand{\comm}[1]{\textcolor{blue}{\noindent #1}}
\newcommand{\alert}[1]{\textcolor{red}{\noindent$\Rightarrow$ #1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\magenta}[1]{\textcolor{magenta}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\add}[1]{\textcolor{black}{#1}}


\newcommand{\ie}{{\em i.e.,}\xspace}
\newcommand{\eg}{{\em e.g.,}\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}

\newcommand{\otm}{{one-to-many}}
\newcommand{\mtm}{{many-to-many}}

\newcommand{\Ni}{({\em i})~}
\newcommand{\Nii}{({\em ii})~}
\newcommand{\Niii}{({\em iii})~}
\newcommand{\Niv}{({\em iv})~}
\newcommand{\Nv}{({\em v})~}
\newcommand{\Na}{({\em a})~}
\newcommand{\Nb}{({\em b})~}
\newcommand{\Nc}{({\em c})~}
\newcommand{\Nd}{({\em d})~}
\newcommand{\Ne}{({\em e})~}
\newcommand{\Nf}{({\em f})~}
 
\newcommand\BibTeX{B\textsc{ib}\TeX}


\title{Data Diversification: {A Simple Strategy For Neural Machine Translation}}


\author{Xuan-Phi Nguyen$^{1,3}$, Shafiq Joty$^{1,2}$, Wu Kui$^{3}$, Ai Ti Aw$^{3}$ \\
  $^1$Nanyang Technological University\\
  $^2$Salesforce Research\\
  $^3$Institute for Infocomm Research (I$^2$R), A*STAR \\
  Singapore \\
  \texttt{\{nguyenxu002@e.ntu,srjoty@ntu\}.edu.sg}\\
  \texttt{\{wuk,aaiti\}@i2r.a-star.edu.sg}
  }

\begin{document}

\maketitle
\begin{abstract}
We introduce Data Diversification: a simple but effective strategy to boost neural machine translation (NMT) performance. It diversifies the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset on which the final NMT model is trained. Our method is applicable to all NMT models. It does not require extra monolingual data like back-translation, nor does it add more computations and parameters like ensembles of models. Our method achieves state-of-the-art BLEU scores of 30.7 and 43.7 in the WMT'14 English-German and English-French translation tasks, respectively. It also substantially improves on 8 other translation tasks: 4 IWSLT tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala). We demonstrate that our method is more effective than knowledge distillation and dual learning, it exhibits strong correlation with ensembles of models, and it trades perplexity off for better BLEU score.
\end{abstract}

\section{Introduction} \label{sec:intro}
The invention of novel architectures for neural machine translation (NMT) has been fundamental to the progress of the field. From the traditional recurrent approaches \citep{SutskeverNIPS2014,luong2015effective}, NMT has advanced to self-attention method \citep{vaswani2017attention}, which is more efficient and powerful and has set the standard for many other NLP tasks \citep{devlin2018bert}. 
Another parallel line of research is to devise effective methods to improve NMT without intensive modification to model architecture, which we shall refer to as \emph{non-intrusive extensions}. Examples of these include the use of sub-word units to solve the out-of-vocabulary (OOV) problem \citep{sennrich2015neural} or exploiting extra monolingual data to perform semi-supervised learning using \emph{back-translation} \citep{backtranslate_sennrich-etal-2016-improving,understanding_backtranslation_scale}. One major advantage of these methods is the applicability to most existing NMT models as well as potentially future architectural advancements with little change. Thus, non-intrusive extensions are used in practice to avoid the overhead cost of developing new architectures and enhance the capability of existing state-of-the-art models.  


In this paper, we propose \emph{Data Diversification}\footnote{Code: \href{https://github.com/nxphi47/data_diversification}{https://github.com/nxphi47/data\_diversification}}, a simple but effective way to improve machine translation consistently and significantly. In this method, we first train multiple models on both backward (target$\rightarrow$source) and forward (source$\rightarrow$target) translation tasks. Then, we use these models to generate a diverse set of synthetic training data from both lingual sides to augment the original data. Our approach is inspired from and a combination of multiple well-known strategies: back-translation, ensemble of models, data augmentation and knowledge distillation for NMT. 

Our method establishes the state of the art (SOTA) in the WMT'14 English-German and English-French translation tasks with 30.7 and 43.7 BLEU scores, respectively.\footnote{As of submission deadline, we report SOTA in the standard WMT'14 setup without monolingual data.} Furthermore, it gives 1.0-2.0 BLEU gains in 4 IWSLT tasks (English$\leftrightarrow$German and English$\leftrightarrow$French) and 4 low-resource tasks (English$\leftrightarrow$Sinhala and English$\leftrightarrow$Nepali). {We demonstrate that data diversification outperforms other related methods -- knowledge distillation \citep{knowledge_distill_kim_rush_2016} and dual learning \cite{multiagent}}, and is complementary to back-translation \citep{backtranslate_sennrich-etal-2016-improving} in semi-supervised setup. Our analysis further reveals that the method is correlated with ensembles of models and it sacrifices perplexity for better BLEU.





\begin{comment}
Data Augmentation for Low-Resource Neural Machine Translation
https://www.aclweb.org/anthology/P17-2090/

SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation
https://www.aclweb.org/anthology/D18-1100/

Soft Contextual Data Augmentation for Neural Machine Translation
https://arxiv.org/pdf/1905.10523.pdf

Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation
https://arxiv.org/pdf/1910.03009.pdf

UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING
https://arxiv.org/pdf/1904.12848.pdf

QANet augmentation


Improving Neural Machine Translation Models with Monolingual Data
https://arxiv.org/pdf/1511.06709.pdf

On The Evaluation of Machine Translation Systems Trained With Back-Translation
https://arxiv.org/pdf/1908.05204.pdf

https://blog.statsbot.co/ensemble-learning-d1dcd548e936
\end{comment}

\begin{comment}
Data Augmentation for Low-Resource Neural Machine Translation
https://www.aclweb.org/anthology/P17-2090/

SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation
https://www.aclweb.org/anthology/D18-1100/

Soft Contextual Data Augmentation for Neural Machine Translation
https://arxiv.org/pdf/1905.10523.pdf

Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation
https://arxiv.org/pdf/1910.03009.pdf

UNSUPERVISED DATA AUGMENTATION
FOR CONSISTENCY TRAINING
https://arxiv.org/pdf/1904.12848.pdf

QANet augmentation


Improving Neural Machine Translation Models with Monolingual Data
https://arxiv.org/pdf/1511.06709.pdf

On The Evaluation of Machine Translation Systems
Trained With Back-Translation
https://arxiv.org/pdf/1908.05204.pdf


https://blog.statsbot.co/ensemble-learning-d1dcd548e936
\end{comment}




















 
\section{Background}\label{sec:background}
\paragraph{Novel Architectures}


The invention of novel neural architectures has been fundamental to scientific progress in NMT. Often they go through further refinements and modifications. For instance, \citet{self_relative} and \citet{weighted_transformer} propose minor modifications to improve the original Transformer  \citep{vaswani2017attention} with slight performance gains. \citet{scaling_nmt_ott2018scaling} propose scaling the training process to 128 GPUs to achieve more significant improvements. \citet{payless_wu2018} repeat the cycle with dynamic convolution.
Side by side, researchers also look for other complementary strategies to improve the performance of NMT systems, which are orthogonal to the advancements in model architectures.




\begin{wraptable}{r}{0.5\textwidth}
\vspace{-1em}
\caption{Estimated method comparison. $|\Theta|$ denotes the number of parameters, while $|D|$ denotes the size of \textit{actual} training data required.}
\vspace{-0.3em}
\resizebox{0.5\columnwidth}{!}{\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\bf Method} & \multicolumn{3}{c}{\bf Training} & \multicolumn{2}{c}{\bf Inference}     \\
\cmidrule{2-6}
{}                      & {\bf FLOPs}   & {\bf $|\Theta|$}  & {\bf $|D|$}   & {\bf FLOPs}   & {\bf $|\Theta|$}  \\
\midrule
\multicolumn{6}{c}{\textit{New Architectures}}\\
\midrule
Transformer             & $1\times$            & $1\times$                & $1\times$            & $1\times$            & $1\times$    \\
Dynamic Conv            & $1\times$            & $1\times$                & $1\times$            & $1\times$            & $1\times$    \\
\midrule
\multicolumn{6}{c}{\textit{Semi-supervised}}\\
\midrule
NMT+BERT                & {$\mathbf{>60\times}$}           & $3\times$                & {$\mathbf{>25\times}$} & $\mathbf{3\times}$            & $\mathbf{3\times}$    \\
Back-translation        & ${2\times}$            & $1\times$                & {$\mathbf{>50\times}$} & $1\times$            & $1\times$    \\
\midrule
\multicolumn{6}{c}{\textit{Evolution-based}}\\
\midrule
\citet{so2019evolved}   & {$\mathbf{>15000\times}$}            & $1\times$                & $1\times$ & $1\times$            & $1\times$    \\
\midrule
\multicolumn{6}{c}{\textit{Our Data Diversification}}\\
\midrule
Default Setup    & $7\times$            & $1\times$                & $1\times$            & $1\times$            & $1\times$    \\
\bottomrule
\end{tabular}
}
\vspace{-1em}
\label{table:method_compare}
\end{wraptable}


\vspace{-0.5em}
\paragraph{Semi-supervised NMT}

Semi-supervised learning offers considerable capabilities to NMT models. Back-translation \citep{backtranslate_sennrich-etal-2016-improving} is a simple but effective way to exploit extra monolingual data. 
Another effective strategy is to use pretrained models. \citet{bert_nmt} recently propose a novel way to incorporate pretrained BERT \citep{devlin2018bert} to improve NMT. 
Nonetheless, the drawback of both approaches is that they require huge extra monolingual data to train/pretrain. Acquiring enormous datasets is sometimes expensive, especially for low-resource scenarios (languages or domains). Moreover, in the case of using pretrained BERT, the packaged translation model incurs the additional computational cost of the pretrained model.

 
\vspace{-0.5em}
\paragraph{Resource Trade-offs}
Table \ref{table:method_compare} summarizes different types of costs for training and inference of different approaches to improve NMT. Developing new architectures, like dynamic convolution \citep{payless_wu2018}, offers virtually no measurable compromise for training and inference, but it may take time for new models to be refined and mature.
On the other hand, semi-supervised methods are often simpler, but require significantly more training data. In particular, \citet{understanding_backtranslation_scale} use back-translation with $50\times$ more training data. NMT+BERT \citep{bert_nmt} requires $60\times$ {more} computations and $25\times$ more data to train (including the pre-training stage). It also needs $3\times$ more computations and parameters during inference. 
Evolved-Transformer \citep{so2019evolved}, an evolution-based technique, requires more than 15,000 times more FLOPs to train. This may not be practical for common practitioners. 
 
On the other hand, our data diversification method is simple as back-translation, but it requires no extra monolingual data. It also has the same inference efficiency as the ``\textit{New Architectures}'' approach. However, it has to make compromise with extra computations in training.\footnote{Parameters $|\Theta|$ do not increase as we can discard the intermediate models after using them.}  


 
\section{Method}


\subsection{Data diversification} \label{subsec:datadiver}

Let $\gD = (S, T)$ be the parallel training data, where $S$ denotes the source-side corpus and $T$ denotes the target-side corpus. Also, {let $M_{S \rightarrow T}$ and $M_{T \rightarrow S}$} be the forward and backward NMT models, which translate from source to target and from target to source, respectively. In our case, we use the Transformer \citep{vaswani2017attention} as the base architecture. In addition, given a corpus $X_l$ in language $l$ and an NMT model $M_{l \rightarrow \hat{l}}$ which translates from language $l$ to language $\hat{l}$, we denote the corpus $M_{l \rightarrow \hat{l}}(X_l)$ as the translation of corpus $X_l$ produced by the model $M_{l \rightarrow \hat{l}}$. The translation may be conducted following the standard procedures such as maximum likelihood and beam search inference.



\begin{algorithm*}[t!]
\caption{Data Diversification: Given a dataset $\gD=(S,T)$, a diversification factor $k$, the number of rounds $N$; return a trained source-target translation model $\hat{M}_{S \rightarrow T}$.}
\label{alg:data_diverse}
\begin{algorithmic}[1]
\Procedure{Train}{$\gD = (S,T)$}
\State Train randomly initialized $M$ on $\gD=(S,T)$ until convergence
    \State \textbf{return} $M$
\EndProcedure
\end{algorithmic}
\begin{algorithmic}[1]
\Procedure{DataDiverse}{$\gD=(S,T), k, N$}
    \State $\gD_0 \gets \gD$  \algorithmiccomment{Assign original dataset to round-0 dataset.}
    \For{$r \in 1, \ldots, N$}
        \State $\gD_r = (S_r, T_r) \gets \gD_{r-1}$
        \For{$i \in 1, \ldots,k$}
            \State $M_{S \rightarrow T,r}^i \gets \textsc{Train}(\gD_{r-1}=(S_{r-1},T_{r-1}))$ \algorithmiccomment{Train forward model}
            \State $M_{T \rightarrow S,r}^i \gets \textsc{Train}(\gD'_{r-1}=(T_{r-1},S_{r-1}))$ \algorithmiccomment{Train backward model}
            \State $\gD_r \gets \gD_r \cup (S,M_{S \rightarrow T,r}^i(S))$  \algorithmiccomment{Add forward data}
            \State $\gD_r \gets \gD_r \cup (M_{T \rightarrow S,r}^i(T),T)$  \algorithmiccomment{Add backward data}
        \EndFor
    \EndFor
    \State $\hat{M}_{S \rightarrow T} \gets Train(\gD_N)$  \algorithmiccomment{Train the final model}
    \State \textbf{return} $\hat{M}_{S \rightarrow T}$
\EndProcedure
\end{algorithmic}
\end{algorithm*}



Our data diversification strategy trains the models in $N$ rounds. In the first round, we train $k$ forward models $(M^1_{S \rightarrow T,1},...,M^k_{S \rightarrow T,1})$ and $k$ backward models $(M^1_{T \rightarrow S,1},..,M^k_{T \rightarrow S,1})$, where $k$ denotes a diversification factor. 
Then, we use the forward models to translate the source-side corpus $S$ of the original data to generate synthetic training data. In other words, we obtain multiple synthetic target-side corpora as $(M^1_{S \rightarrow T,1}(S),...,M^k_{S \rightarrow T,1}(S))$. Likewise, the backward models are used to translate the target-side original corpus $T$ to synthetic source-side corpora as $(M^1_{T \rightarrow S,1}(T),...,M^k_{T \rightarrow S,1}(T))$.
After that, we augment the original data with the newly generated synthetic data, which is summed up to the new round-1 data $\gD_1$ as follows:
\begin{equation}
    \gD_1 = (S,T) \bigcup \cup_{i=1}^k (S, M^i_{S \rightarrow T,1}(S)) \bigcup \cup_{i=1}^k (M^i_{T \rightarrow S,1}(T), T)
\end{equation}
After that, if the number of rounds $N > 1$, we continue training round-2 models $(M^1_{S \rightarrow T,2},...,M^k_{S \rightarrow T,2})$ and $(M^1_{T \rightarrow S,2},..,M^k_{T \rightarrow S,2})$ on the augmented data $\gD_1$. The similar process continues until the final augmented dataset $\gD_N$ is generated. Eventually, we train the final model $\hat{M}_{S \rightarrow T}$ on the dataset $\gD_N$. For a clearer presentation, Algorithm \ref{alg:data_diverse} summarizes the process concretely. In the experiments, unless specified otherwise, we use the default setup of $k=3$ and $N=1$.

\subsection{Relation with existing methods} \label{subsec:rel}

Our method shares certain similarities with a variety of existing techniques, namely, data augmentation, back-translation, ensemble of models, knowledge distillation and multi-agent dual learning.

\vspace{-0.5em}
\paragraph{Data augmentation} Our approach is genuinely a data augmentation method. \citet{data_aug_low_resource_fadaee-etal-2017-data} proposed an augmentation strategy which targets rare words to improve low-resource translation. \citet{switchout} suggested to simply replace random words with other words in the vocabularies. 
Our approach is distinct from these methods in that it does not randomly corrupt the data and train the model on the augmented data on the fly. 
Instead, it transforms the data into synthetic translations, which follow different model distributions.



\vspace{-0.5em}
\paragraph{Back-translation} Our method is similar to back-translation, which has been employed to generate synthetic data from target-side extra monolingual data. \citet{backtranslate_sennrich-etal-2016-improving} were the first to propose such strategy, while \citet{understanding_backtranslation_scale} refined it at scale. Our method's main advantage is that it does not require any extra monolingual data. Our technique also differs from previous work in that it additionally employs forward translation, which we have shown to be important (see \cref{subsec:fwd}). 

\vspace{-0.5em}
\paragraph{Ensemble of models} Using multiple models to average the predictions and reduce variance is a typical feature of ensemble methods \citep{when_net_disaggree_perrone1992networks}. However, 
the drawback is that the testing parameters and computations are multiple times more than an individual model. {While our diversification approach correlates with model ensembles (\cref{subsec:ensemble})}, it does not suffer this disadvantage. 



\vspace{-0.5em}
\paragraph{Knowledge distillation} Knowledge distillation \citep{knowledge_distill_kim_rush_2016,bornagain_pmlr-v80-furlanello18a} involves pre-training a {big teacher} model and using its predictions (forward translation) to train a {smaller student} as the final model. In comparison to that, our method additionally employs back-translation and involves multiple backward and forward ``teachers''. We use all backward, forward, as well as the original data to train the final model without any  parameter reduction. We also repeat the process multiple times. 
 In this context, our method also differs from the ensemble knowledge distillation method  \citep{ensemble_distill_freitag2017}, which uses the teachers to jointly generate a single version of data. Our method on the other hand uses the teachers to individually generate various versions of synthetic data. 

\vspace{-0.5em}
\paragraph{Multi-agent dual learning} Multi-agent dual learning \citep{multiagent} involves leveraging duality with multiple forward and backward agents. Similar to \citep{ensemble_distill_freitag2017}, this method combines multiple agents in an ensembling manner to form forward ($F_{\alpha}$) and backward ($G_{\beta}$) teachers. Then, it simultaneously optimizes the reconstruction losses $\Delta_x(x,G_{\beta}(F_{\alpha}(x)))$ and $\Delta_y(y,F_{\alpha}(G_{\beta}(y)))$ to train the final dual models. As a result, the two models are coupled and entangled. On the other hand, our method does not combine the agents in this way, nor does it optimize any reconstruction objective.

Our approach is also related but substantially different from the mixture of experts for diverse MT \citep{mixture_model_nmt_shen2019}, iterative back-translation \citep{iterative-hoang-etal-2018} and copied monolingual data for NMT \citep{copied-currey-etal-2017}; 
see the Appendix for further details about these comparisons.


 \label{sec:data-diver}

\section{Experiments} \label{sec:experiments}
In this section, we present experiments to demonstrate that our data diversification approach improves translation quality in many translation tasks, encompassing WMT and IWSLT tasks, and high- and low-resource translation tasks. {Due to page limit, we  describe the setup for each experiment briefly in the respective subsections and give more details in the Appendix.} 



\subsection{WMT'14 English-German and English-French translation tasks}\label{sec:wmt}

\paragraph{Setup.} {We conduct experiments on the standard WMT'14 English-German (En-De) and English-French (En-Fr) translation tasks. The training datasets contain about 4.5M and 35M sentence pairs respectively. 
{The sentences are encoded with Byte-Pair Encoding (BPE) \citep{sennrich2015neural} with 32K operations.}We use newstest2013 as the development set, and newstest2014 for testing. {Both tasks are considered high-resource tasks as the amount of parallel training data is relatively large.} 
We use the Transformer \citep{vaswani2017attention} as our NMT model and follow the same configurations as suggested by \citet{scaling_nmt_ott2018scaling}.
When augmenting the datasets, we filter out the duplicate pairs, which results in training datasets of 27M and 136M pairs for En-De and En-Fr, respectively.\footnote{There were 14\% and 22\% duplicates, respectively. 
We provide a diversity analysis in the Appendix.} We do {not} use any extra monolingual data. 
}









\vspace{-0.5em}
\paragraph{Results.} From the results on WMT newstest2014 testset in Table \ref{table:data_diverse_wmt}, we observe that the scale Transformer \citep{scaling_nmt_ott2018scaling}, which originally gives 29.3 BLEU in the En-De task, now gives 30.7 BLEU with our data diversification  strategy, setting a new SOTA. Our approach yields an improvement of 1.4 BLEU over the without-diversification model and 1.0 BLEU over the previous SOTA reported on this task by \citet{payless_wu2018}.\footnote{We could not {reproduce} the results reported by \citet{payless_wu2018} using their \href{https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper\#wmt16-en-de}{code}. We only achieved 29.2 BLEU for this baseline, while our method applied to it gives 30.1 BLEU.} Our approach also outperforms other non-intrusive extensions, such as multi-agent dual learning and knowledge distillation by a good margin (0.7-3.1 BLEU).
Similar observation can be drawn for WMT'14 En-Fr task. Our strategy establishes a new SOTA of 43.7 BLEU, exceeding the previous (reported) SOTA by 0.5 BLEU. It is important to mention that while our method increases the overall training time (including the time to train the base models), training a single Transformer model for the same amount of time only leads to overfitting.



\subsection{IWSLT translation tasks}\label{sec:iwslt}

\paragraph{Setup.}  {We evaluate our approach in IWSLT'14 English-German (En-De) and German-English (De-En), IWSLT'13 English-French (En-Fr) and French-English (Fr-En) translation tasks. The IWSLT’14
En-De training set contains about 160K sentence
pairs. We randomly sample 5\% of the training data for validation and combine multiple test
sets IWSLT14.TED.\{dev2010, dev2012, tst2010,
tst1011, tst2012\} for testing. The IWSLT’13 En-Fr
dataset has about 200K training sentence pairs. We
use the IWSLT15.TED.tst2012 set for validation
and the IWSLT15.TED.tst2013 set for testing.}
We use BPE for all four tasks. 
We compare our approach against two baselines that do not use our data diversification: Transformer \citep{vaswani2017attention} and Dynamic Convolution \citep{payless_wu2018}. 

\begin{figure}[!t]
\begin{minipage}[t]{0.47\textwidth}
\centering
\captionof{table}{{BLEU scores on newstest2014 for WMT'14 English-German (En-De) and English-French (En-Fr) translation tasks. Distill (T$>$S) (resp. T$=$S) indicates the teacher model is larger than (resp. equal to) the student model. }
}
\label{table:data_diverse_wmt}
\resizebox{0.8\columnwidth}{!}{\setlength\tabcolsep{2pt}
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{\bf Method}   & \multicolumn{2}{c}{\bf WMT'14}\\
\cmidrule{2-3}
                           & \textbf{En-De} & \textbf{En-Fr}\\
\midrule
Transformer \citep{vaswani2017attention}$^\dagger$                    & 28.4 & 41.8\\
Trans+Rel. Pos \citep{self_relative}$^\dagger$                        & 29.2 & 41.5\\
Scale Transformer \citep{scaling_nmt_ott2018scaling}        & 29.3 & 42.7{\footnotemark{}}\\
Dynamic Conv \citep{payless_wu2018}$^\dagger$                  & 29.7 & 43.2\\
\midrule
\multicolumn{3}{l}{\bf Transformer with}\\
{Multi-Agent} \citep{multiagent}$^\dagger$                             & 30.0     &  -  \\
{Distill ({T$>$S})} \citep{knowledge_distill_kim_rush_2016}    & 27.6     & 38.6  \\
{Distill (T$=$S)} \citep{knowledge_distill_kim_rush_2016}     & 28.4     & 42.1  \\
{Ens-Distill} \citep{ensemble_distill_freitag2017}           & 28.9     & 42.5   \\
\midrule
\multicolumn{3}{l}{\bf Our Data Diversification with}\\
Scale Transformer \citep{scaling_nmt_ott2018scaling}        & \textbf{30.7} & \textbf{43.7}\\
\bottomrule
\end{tabular}
\setlength\tabcolsep{6pt}
}
\vspace{-1em}
\end{minipage}
\hfill
\begin{minipage}[t]{0.485\textwidth}
\centering
\captionof{table}{{BLEU scores on IWSLT'14 English-German (En-De), German-English (De-En), and IWSLT'13 English-French (En-Fr) and French-English (Fr-En) translation tasks. Superscript $^\dagger$ denotes the numbers are reported from the paper, others are based on our runs.}}
\label{table:data_diverse_iwslt}
\resizebox{0.86\columnwidth}{!}{\setlength\tabcolsep{2pt}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\bf Method}   & \multicolumn{2}{c}{\bf IWSLT'14}    & \multicolumn{2}{c}{\bf IWSLT'13}\\
\cmidrule{2-5}
{\bf }                                  & {\bf En-De} & {\bf De-En} & {\bf En-Fr} & {\bf Fr-En}\\
\midrule
\multicolumn{2}{l}{\bf Baselines}\\
Transformer                             & 28.6        & 34.7        & 44.0              & 43.3\\
Dynamic Conv                            & 28.7        & 35.0       & 43.8       & 43.5 \\
\midrule
\multicolumn{5}{l}{\bf Transformer with}\\
{Multi-Agent}$^\dagger$                 & 28.9     & 34.7   & -            & -     \\
{Distill (T$>$S)}                       & 28.0     & 33.6  & 43.4       & 42.9  \\
{Distill (T$=$S)}                       & 28.5     & 34.1  & 44.1        & 43.4  \\
{Ens-Distill}                           & 28.8     & 34.7  & 44.3       & 43.9  \\
\midrule
\multicolumn{5}{l}{\bf Our Data Diversification with}\\
Transformer                             & \textbf{30.6}        & {37.0}  & \textbf{45.5}              & \textbf{45.0}\\
Dynamic Conv                            & \textbf{30.6}        & \textbf{37.2}  & {45.2}              & {44.9}\\
\bottomrule
\end{tabular}
\setlength\tabcolsep{6pt}
}
\vspace{-1em}
\end{minipage}
\end{figure}
\footnotetext{\citet{scaling_nmt_ott2018scaling} reported 43.2 BLEU in En-Fr. However, we could achieve only 42.7 using their code, based on which our data diversification gives 1.0 BLEU gain.}


\vspace{-0.5em}
\paragraph{Results.} {From Table \ref{table:data_diverse_iwslt} we see that 
our method substantially and consistently boosts the performance in all the four translation  tasks.} In the En-De task, our method achieves up to 30.6 BLEU, which is 2 BLEU above the Transformer baseline. Similar trend can also be seen in the remaining De-En, En-Fr, Fr-en tasks.
The results also show that our method is agnostic to model architecture, with both the Transformer and Dynamic Convolution achieving high gains. In contrary, other methods like knowledge distillation and multi-agent dual learning show minimal improvements on these tasks.


\vspace{-0.5em}
\subsection{Low-resource translation tasks}

Having demonstrated the effectiveness of our approach in high-resource languages like English, German and French, we now evaluate our approach performs on low-resource languages. {For this, we use the English-Nepali and English-Sinhala low-resource setup proposed by \citet{flores}.}
Both Nepali (Ne) and Sinhala (Si) are challenging domains since the data sources are particularly scarce and the vocabularies and grammars are vastly different from high-resource language like English. 


\vspace{-0.5em}
\paragraph{Setup.} {We evaluate our method on the supervised setup of the four low-resource translation tasks: En-Ne, Ne-En, En-Si, and Si-En. We compare our approach against the baseline in \citet{flores}. The English-Nepali and English-Sinhala parallel datasets contain about 500K and 400K sentence pairs respectively. We replicate the same setup as done by \citet{flores} and use their dev set for development and devtest set for testing. We use $k=3$ in our data diversification experiments. }

\vspace{-1em}
\begin{table}[h!]
\begin{center}
\caption{Performances on low-resource translations. As done by \citet{flores}, the from-English pairs are measured in tokenized BLEU, while to-English are measured in detokenized SacreBLEU.}
\begin{tabular}{lcccc}
\toprule
{\bf Method}                  & {\bf En-Ne} & {\bf Ne-En} & {\bf En-Si} & {\bf Si-En}\\
\midrule
\citet{flores}                    & 4.3         & 7.6         & 1.0        & 6.7       \\
\midrule
Data Diversification                    & \textbf{5.7}         & \textbf{8.9}         & \textbf{2.2}        & \textbf{8.2}       \\
\bottomrule
\end{tabular}
\label{table:data_diverse_low_resource}
\end{center}
\end{table}


\vspace{-0.5em}
\paragraph{Results.} From the results in Table \ref{table:data_diverse_low_resource}, we can notice that our method consistently improves the performance by more than 1 BLEU in all four tested tasks. Specifically, the method achieves 5.7, 8.9, 2.2, and 8.2 BLEU for En-Ne, Ne-En, En-Si and Si-En tasks, respectively. In absolute terms, these are 1.4, 1.3, 2.2 and 1.5 BLEU improvements over the baseline model \citep{flores}. Without any monolingual data involved, our method establishes a new state of the art in all four low-resource tasks.





\section{Understanding data diversification}
\label{sec:analysis}

We propose several logical hypotheses to explain why and how data diversification works as well as provide a deeper insight to its mechanism. We conduct a series of experimental analysis to confirm or reject such hypotheses. As a result, certain hypotheses are confirmed by the experiments, while some others, though being intuitive, are experimentally rejected. In this section, we explain the hypotheses that are empirically verified, while we elaborate the failed hypotheses in the Appendix.


\begin{figure}[t!]
\begin{minipage}[b]{0.49\textwidth}
\centering
\captionof{table}{Diversification preserves the effects of ensembling, but does not change $|\Theta|$ and flops.}
\label{table:study:ensemble}
\resizebox{\columnwidth}{!}{\setlength\tabcolsep{2pt}
\begin{tabular}{lcccccc}
\toprule
{\bf}   &  {\bf $|\Theta|$}  & \multicolumn{2}{c}{\bf IWSLT'14}   & \multicolumn{2}{c}{\bf IWSLT'13}  & \multicolumn{1}{c}{\bf WMT}\\
\cmidrule{3-7}
{\bf }  & {\bf flops}       & {\bf En-De} & {\bf De-En} & {\bf En-Fr} & {\bf Fr-En} & {\bf En-De} \\
\midrule
Baseline    & 1x            & 28.6          & 34.7          & 44.0          & 43.3          & 29.3 \\
Ensemble    & \textbf{7x}   & 30.2          & 36.5          & \textbf{45.5} & 44.9          & 30.3\\
Ours        & {1x}          & \textbf{30.6} & \textbf{37.0} & \textbf{45.5} & \textbf{45.0} & \textbf{30.7}\\
\bottomrule
\end{tabular}
\setlength\tabcolsep{6pt}
}
\vspace{-0.5em}
\end{minipage}
\hfill
\begin{minipage}[b]{0.49\textwidth}
\centering
\captionof{table}{BLEU scores for forward and backward diversification in comparison to bidirectional diversification and the baseline on IWSLT'14 En-De and De-En tasks.}
\label{table:fw_bw_full}
\resizebox{\columnwidth}{!}{\setlength\tabcolsep{2pt}
\begin{tabular}{lcccc}
\toprule
{\bf Task}                  & {\bf Baseline} & {\bf Backward} & {\bf Forward} & {\bf Bidirectional}\\
\midrule
En-De	& 28.6	& 29.2	& 29.86	& 30.6\\
De-En	& 34.7	& 35.8	& 35.94	& 37.0\\
\bottomrule
\end{tabular}
\setlength\tabcolsep{6pt}
}
\vspace{-0.5em}
\end{minipage}
\end{figure}


\subsection{Ensemble effects} \label{subsec:ensemble}

\paragraph{Hypothesis} \emph{Data diversification exhibits a strong correlation with ensemble of models.}

\vspace{-0.5em}
\paragraph{Experiments}



To show this, we perform inference with an ensemble of seven (7) models and compare its performance with ours. We evaluate this setup on the WMT'14 En-De, and IWSLT'14 En-De, De-En, IWSLT'13 En-Fr and Fr-En translation tasks. The results are reported in Table \ref{table:study:ensemble}. We notice that the ensemble of models outdoes the single-model baseline by 1.3 BLEU in WMT'14 and 1.0-2.0 BLEU in IWSLT tasks. These results are particularly comparable to those achieved by our technique. This suggests that our method may exhibit an ensembling effect. However, note that an ensemble of models has a major drawback that it requires $N$ (7 in this case) times more computations and parameters to perform inference. In contrary, our method does not have this disadvantage.


\vspace{-0.5em}
\paragraph{Explanation} 


{Intuitively, different models (initialized with different random seeds) trained on the original dataset converge to different local optima.} As such, individual models tend to have high variance. Ensembles of models are known to help reduce variance, thus improves the performance. {Formally, suppose a single-model $M_i \in \{M_1,...,M_N\}$ estimates a model distribution $p_{M_i}$, which is close to the data generating distribution $p_{data}$.} An ensemble of models averages multiple $p_{M_i}$ (for $i= 1, \ldots, N$), which leads to a model distribution that is closer to $p_{data}$ and improves generalization.

Our strategy may achieve the same effect by forcing a single-model $\hat{M}$ to learn from the original data distribution $p_{data}$ as well as multiple synthetic distributions $\gD'_i \sim p_{M_i}$ for $i= 1, \ldots, N$, simultaneously. Following Jensen's Inequality \citep{jensen1906fonctions}, our method optimizes the upper bound:
\vspace{-0.5em}
\begin{equation}
    \E_{Y\sim U({M_1}(X),...,{M_N}(X)),X\sim p_{data}} \log p_{\hat{M}}(Y|X) \leq \sum_j \log [\frac{1}{N} \sum_{i}^N p_{\hat{M}}(y^i_j|y_{<j}^i,X)]
    \vspace{-0.2em}
\end{equation}
where $U$ is uniform sampling, $y^i_j = \text{argmax}_{y_j} p_{M_i}(y_j|y_{<j},X)$. Let $\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j},X)$ be the token-level probability of an ensemble of models $M_i$ and $V$ be the vocabulary. Experimentally, we observe that the final model $\hat{M}$ tends to outperform when the following condition is met:
\vspace{-0.5em}
\begin{equation}
    \E_{X\sim p_{data}} \Big[ \frac{1}{N}\sum_i^N p_{\hat{M}}(y^i_j|y_{<j}^i,X) \Big] \leq \E_{X\sim p_{data}} \Big[ \max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j},X) \Big] \mbox{  with  } y_j^k \in V \label{eqn:assump}
\end{equation}
Condition \ref{eqn:assump} can be met naturally at the beginning of the training process, but is not guaranteed at the end. We provide further analysis and supporting experiments in the Appendix.
 





\subsection{Perplexity vs. BLEU score} 

\paragraph{Hypothesis} \emph{Data diversification sacrifices perplexity for better BLEU score.
}

\vspace{-0.5em}
\paragraph{Experiments}

We tested this hypothesis as follows. We recorded the validation perplexity when the models fully converge for the baseline setup and for our data diversification method. We report the results  in Figure \ref{fig:ppl_vs_bleu} for WMT'14 En-De, IWSLT'14 En-De, De-En, IWSLT'13 En-Fr and Fr-En tasks. The left axis of the figure shows Perplexity (PPL) values for the models, which compares the \emph{dark} blue (baseline) and red (our) bars. Meanwhile, the right axis shows the respective BLEU scores for the models as reflected by the \emph{faded} bars.

\vspace{-0.5em}
\paragraph{Explanation}
Common wisdom tells that the lower perplexity often leads to better BLEU scores. In fact, our NMT models are trained to minimize perplexity (equivalently, cross entropy loss). However, existing research \citep{decoding_nmt} also suggests that sometimes sacrificing perplexity may result in better generalization and performance. As shown in Figure \ref{fig:ppl_vs_bleu}, our models consistently show higher perplexity compared to the baseline in all the tasks, though we did not have intention to do so. As a result, the BLEU score is also consistently higher than the baseline.


\begin{figure}
\vspace{-1em}
\begin{center}
\begin{subfigure}{.49\textwidth}
    \begin{center}
        \includegraphics[width=\textwidth]{img/ppl_bleu_2.png}
        \caption{Validation perplexity vs BLEU scores.}
        \label{fig:ppl_vs_bleu}
    \end{center}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \begin{center}
    \includegraphics[width=\textwidth]{img/init_bleu_2.png}
\caption{Random vs fixed parameters initialization.}
    \label{fig:init_vs_bleu}
  \end{center}
\end{subfigure}\end{center}
\vspace{-0.5em}
\caption{Relationship between validation perplexity vs the BLEU scores (\ref{fig:ppl_vs_bleu}) and the effects of random initialization (\ref{fig:init_vs_bleu}) in the IWSLT En-De, De-En, En-Fr, Fr-En and WMT'14 En-De tasks.}
\label{fig:ppl_vs_init_bleu}
\vspace{-1em}
\end{figure}



\subsection{Initial parameters vs. diversity}

\paragraph{Hypothesis} \emph{Models with different initial parameters increase diversity {in the augmented data}, while the ones with fixed initial parameters decrease it.} 

\vspace{-0.5em}
\paragraph{Experiments and Explanation} With the intuition that diversity in training data improves translation quality, we speculated that the initialization of model parameters plays a crucial role in data diversification. Since neural networks are susceptible to initialization, it is possible that different initial parameters may lead the models to different convergence paths \citep{goodfellow2016deep} and thus different model distributions, while models with the same initialization are more likely to converge in similar paths. To verify this, we did an experiment with initializing all the constituent models ($M_{S \rightarrow T,n}^i$, $M_{T \rightarrow S,n}^i,\hat{M}_{S \rightarrow T}$) with the same initial parameters to suppress data  diversity. We conducted this experiment on the IWSLT'14 English-German and German-English tasks. We used a diversification factor of $k=1$ only in this case. The results are shown in Figure \ref{fig:init_vs_bleu}. Apparently, the BLEU scores of the fixed (same) initialization drop compared to the randomized counterpart in both language pairs. However, its performance is still significantly higher than the single-model baseline. This suggests that initialization is not the only contributing factor to diversity. Indeed, even though we are using the same initial checkpoint, each constituent model is trained on a different dataset and and learns to estimate a different distribution.

\subsection{Forward-translation is important} \label{subsec:fwd}

\paragraph{Hypothesis} \emph{Forward-translation is as vital as back-translation.}

\paragraph{Experiments and Explanation} 
We separate our method into forward and backward diversification, in which we only train the final model ($\hat{M}_{S \rightarrow T}$) with the original data augmented by either the translations of the forward models ($M_{S \rightarrow T,n}^i$) or those of the backward models ($M_{T \rightarrow S,n}^i$) separately. We compare those variants with the bidirectionally diversified model and the single-model baseline. 
As shown in Table \ref{table:fw_bw_full}, the forward and backward methods perform worse than the bidirectional counterpart but still better than the baseline. However, it is worth noting that diversification with forward models outperforms the one with backward models, as recent research has focused mainly on back-translation where only backward models are used
\citep{backtranslate_sennrich-etal-2016-improving,understanding_backtranslation_scale}. 
Our finding is similar to \citet{exploit_src_monolingual}, where the authors used source-side monolingual data to improve BLEU score. 



\section{Unaffected by the translationese effect}

{
Data diversification is built on the foundation of back-translation \citep{backtranslate_sennrich-etal-2016-improving,understanding_backtranslation_scale}. However, in a recent work, \citet{eval_back_translation_translationese} point out that back-translation suffers from the \emph{translationese effect} \citep{Volansky-translationese}, where back-translation only improves the performance when the source sentences are translationese but does not offer any improvement when the sentences are natural text.\footnote{Translationese refers to the unique characteristics of translated text (e.g., simplification, explicitation, normalization) compared to text originally written in a given language. This happens because when translating a text, translators usually have to make trade-offs between fidelity (to the source) and fluency (in the target).} In other words, back-translation can be ineffective in practice because our goal is to translate natural text, not translated text. Thus, it is important to test whether our data diversification method also suffers from this effect.
}





{
To verify this, we measure the BLEU improvements of data diversification over the baseline \citep{scaling_nmt_ott2018scaling} in the WMT'14 English-German setup (\cref{sec:wmt}) in all 3 scenarios laid out by \citet{eval_back_translation_translationese}: \Ni natural source $\rightarrow$ translationese target ($X\rightarrow Y^*$), \Nii translationese source $\rightarrow$ natural target ($X^*\rightarrow Y$), and \Niii translationese of translationese of source to translationese of target ($X^{**}\rightarrow Y^*$). We use the same test sets provided by \citet{eval_back_translation_translationese} to conduct this experiment.\footnote{Note that our setup uses the WMT'14 training set while \citet{eval_back_translation_translationese} use the WMT'18 training set.}
}

{
As shown in \Cref{table:translationese}, out method consistently outperforms the baseline in all three scenarios while \citet{eval_back_translation_translationese} show that back-translation \citep{backtranslate_sennrich-etal-2016-improving} improves only in the $X^*\rightarrow Y$ scenario. Thus, our method is not effected by the translationese effect. Our explanation for this is that the mentioned back-translation technique \citep{backtranslate_sennrich-etal-2016-improving} is a \emph{semi-supervised} setup that uses extra natural monolingual data in the target. In our method, however, back-translation is conducted on the translationese part (target side) of the parallel data, and does not enjoy the introduction of extra natural text, which only benefits the $X^*\rightarrow Y$ scenario. Otherwise speaking, back-translation \emph{with} and \emph{without} monolingual data are two distinct setups that should not be confused or identically interpreted.
}


\begin{table}[hbt!]
\vspace{-0.5em}
\begin{center}
\caption{BLEU evaluation of our method and the baseline on the translationese effect \citep{eval_back_translation_translationese}, in the WMT'14 English-German setup.}
\begin{tabular}{lccc}
\toprule
{\bf WMT'14 En-De} & {\bf $X\rightarrow Y^*$}     & {\bf $X^*\rightarrow Y$}      & {\bf $X^{**}\rightarrow Y^*$} \\
\midrule
Baseline \citep{scaling_nmt_ott2018scaling}        & 31.35   & 28.47   & 38.59   \\
Our method      & 33.47   & 30.38   & 41.03\\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{table:translationese}
\end{center}
\end{table}


\section{Study on hyperparameters and back-translation}




\paragraph{Effect of different $k$ and $N$}

We first conduct experiments with the two hyper-parameters in our method -- the diversification factor $k$ and the number of rounds $N$, to investigate how they affect the performance. Particularly, we test the effect of different $N$ on the IWSLT'14 En-De and De-En tasks, while the effect of different $k$ is tested on the WMT'14 En-De task. As shown in Table \ref{table:ablation_n}, increasing $N$ improves the performance but the gain margin is insignificant. Meanwhile, Table \ref{table:ablation_k} shows that increasing $k$ significantly improves the performance until a specific saturation point. Note that increasing $N$ costs more than increasing $k$ while its gain may not be significant.

\begin{figure}[t!]
\vspace{-1em}
\begin{minipage}[t]{0.45\textwidth}
\centering
\captionof{table}{BLEU scores for different rounds $N$}
\label{table:ablation_n}
\resizebox{0.75\columnwidth}{!}{\setlength\tabcolsep{2pt}
\begin{tabular}{lccc}
\toprule
{\bf IWSLT'14}                  & {\bf $N=1$} & {\bf $N=2$} & {\bf $N=3$}\\
\midrule
En-De	& 30.4	& 30.6	& 30.6\\
De-En	& 36.8	& 36.9	& 37.0\\
\bottomrule
\end{tabular}
\setlength\tabcolsep{6pt}
}
\vspace{-0.5em}
\end{minipage}
\hfill
\begin{minipage}[t]{0.51\textwidth}
\centering
\captionof{table}{BLEU scores for different factors $k$.}
\label{table:ablation_k}
\resizebox{\columnwidth}{!}{\setlength\tabcolsep{2pt}
\begin{tabular}{lcccccc}
\toprule
{\bf WMT'14}                  & {\bf $k=1$} & {\bf $k=2$} & {\bf $k=3$} & {\bf $k=4$}  & {\bf $k=5$}  & {\bf $k=6$}\\
\midrule
En-De                     & 29.8      & 30.1       & 30.7   & 30.7   & 30.7   & 30.6\\
\bottomrule
\end{tabular}
\setlength\tabcolsep{6pt}
}
\vspace{-0.5em}
\end{minipage}
\end{figure}



\paragraph{Complementary to back-translation}


Our method is also complementary to back-translation (BT) \citep{backtranslate_sennrich-etal-2016-improving}. To demonstrate this, we conducted experiments on the IWSLT'14 En-De and De-En tasks with extra monolingual data extracted from the WMT'14 En-De corpus. In addition, we also compare our method against the back-translation baseline in the WMT'14 En-De task with extra monolingual data from News Crawl 2009. We use the \textit{big} Transformer as the final model in all our back-translation experiments. Further details of these experiments are provided in the Appendix. As reported in Table \ref{table:compare_bt}, using back-translation improves the baseline performance significantly. However, using our data diversification strategy with such monolingual data boosts the performance further with additional +1.0 BLEU over the back-translation baselines.



\begin{table}[hbt!]
\vspace{-0.5em}
\begin{center}
\caption{BLEU scores for models with and without back-translation (BT) on the IWSLT'14 English-German (En-De), German-English (De-En) and WMT'14 En-De tasks. Column $|D|$ shows the total data used in back-translation compared to the original parallel data.}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\bf Task}   & \multicolumn{2}{c}{\bf No back-translation}     & \multicolumn{3}{c}{\bf With back-translation} \\
\cmidrule{2-6}
{}                      & {\bf Baseline}    & {\bf Ours}    & {\bf $|D|$} & {\bf Baseline}  & {\bf Ours} \\
\midrule
IWSLT'14 En-De	& 28.6	& 30.6	& $29\times$   & 30.0 & \textbf{31.8}\\
IWSLT'14 De-En	& 34.7	& 37.0	& $29\times$   & 37.1  & \textbf{38.5}\\
\midrule
WMT'14 En-De   & 29.3  & 30.7  & $2.4\times$    & 30.8  & \textbf{31.8}\\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{table:compare_bt}
\end{center}
\end{table}

 
\section{Conclusion} \label{sec:conclusion}

We have proposed a simple yet effective method to improve translation performance in many standard machine translation tasks. 
The approach achieves state-of-the-art in the WMT'14 English-German translation task with 30.7 BLEU. It also improves in IWSLT'14 English-German, German-English, IWSLT'13 English-French and French-English tasks by 1.0-2.0 BLEU. Furthermore, it outperforms the baselines in the low-resource tasks: English-Nepali, Nepali-English, English-Sinhala, Sinhala-English. Our experimental analysis reveals that our approach exhibits a strong correlation with ensembles of models. It also trades perplexity off for better BLEU score. We have also shown that the method is complementary to back-translation with extra monolingual data as it improves the back-translation performance significantly.

\section*{Broader Impact}

Our work has a potential positive impact on the application of machine translation in a variety of languages. It helps boost performance in both high- and low-resource languages. The method is simple to implement and applicable to virtually all existing machine translation systems. Future commercial and humanitarian translation services can benefit from our work and bring knowledge of one language to another, especially for uncommon language speakers such as Nepalese and Sri Lankan. On the other hand, our work needs to train multiple models, which requires more computational power or longer time to train the final model.

\bibliography{neurips_2020t.bib}
\bibliographystyle{plainnat}


\newpage
\appendix
\section{Appendix}\label{sec:appendix}



{In the following supplementary materials, we discuss the other hypotheses that are not supported by the experiments. 
After that, we present mathematically our data diversification method is correlated to ensembles of models.
Finally, we describe the training setup for our back-translation experiments.} 

\section{Further comparison}\label{app:compare}
We continue to differentiate our method from other existing works. First, \citet{mixture_model_nmt_shen2019} also seek to generate diverse set of translations using mixture of experts, not to improve translation quality like ours. In this method, multiple experts are tied into a single NMT model to be trained to generate diverse translations through EM optimization. It does not employ data augmentation, neither forward nor backward translations. Our method does not train multiple peer models with EM training either. 

Second, iterative back-translation \citep{iterative-hoang-etal-2018} employs back-translation to augment the data in multiple rounds. In each round, a forward (or backward) model takes turn to play the ``back-translation'' role to train the backward (or forward) model. The role is switched in the next round. In our approach, both directions are involved in each round and multiple models to the achieve ensembling effect.

Third, \citet{copied-currey-etal-2017} propose to generate bitext from the target-side monolingual data by just copying the target sentence into the source sentence. In other words, source and target are identical. Our method does not use copying practice nor any extra monolingual data.


\section{Experimental setup details} \label{app:setup}

\paragraph{WMT'14 setup.} We conduct experiments on the WMT'14 English-German (En-De) and English-French (En-Fr) translation tasks. The training datasets contain about 4.5 million and 35 million sentence pairs respectively. {The sentences are encoded with Byte-Pair Encoding (BPE) \citep{sennrich2015neural} with 32,000 operations, which results in a shared-vocabulary of 32,768 tokens for En-De and 45,000 tokens for En-Fr.} We use newstest2013 as the development set, and newstest2014 for testing. Both tasks are considered as high-resource tasks as the amount of parallel training data is relatively large. 

We use the Transformer \citep{vaswani2017attention} as our NMT model and follow the same configurations as suggested by \citet{scaling_nmt_ott2018scaling}. The model has 6 layers, each of which has model dimension $d_{model}=1024$, feed-forward dimension $d_{ffn}=4096$, and 16 attention heads. 
Adam optimizer \citep{kingma2014adam} was used with the similar learning rate schedule as \citet{scaling_nmt_ott2018scaling} --- 0.001 learning rate, 4,000 warm-up steps, and a batch size of 450K tokens. We use a dropout rate of 0.3 for En-De and 0.1 for En-Fr. We train the models for 45,000 updates. The data generation process costs 30\% the time to train the baseline.

For data diversification, we use a diversification factor $k=3$ for En-De and $k=2$ for En-Fr. 
When augmenting the datasets, we filter out the duplicate pairs, which results in training datasets of 27M and 136M sentence pairs for En-De and En-Fr, respectively.\footnote{There were 14\% and 22\% duplicates, respectively. We provide a diversity analysis in the Appendix.} Note that we do {not} use any extra monolingual data. For inference, we average the last 5 checkpoints of the final model and use a beam size of 5 and a length penalty of 0.6. We measure the performance in standard tokenized BLEU.

\paragraph{IWSLT setup.} We also show the effectiveness of our approach in IWSLT'14 English-German (En-De) and German-English (De-En), IWSLT'13 English-French (En-Fr) and French-English (Fr-En) translation tasks. The IWSLT'14 En-De training set contains about 160K sentence pairs. We randomly sample 5\% of the training data for validation and combine multiple test sets IWSLT14.TED.\{dev2010, dev2012, tst2010, tst1011,  tst2012\} for testing. The IWSLT'13 En-Fr dataset has about 200K training sentence pairs.  We use the IWSLT15.TED.tst2012 set for validation and the IWSLT15.TED.tst2013 set for testing. We use BPE for all four tasks. This results in a shared vocabulary of 10,000 tokens for English-German pair and 32,000 tokens for English-French pair. 

We compare our approach against two baselines that do not use our data diversification: Transformer \citep{vaswani2017attention} and Dynamic Convolution \citep{payless_wu2018}. In order to make a fair comparison, for the baselines and our approach, we use the \textbf{base} setup of the Transformer model. Specifically, the models have 6 layers, each with model dimensions $d_{model}=512$, feed-forward dimensions $d_{ffn}=1024$, and $4$ attention heads. 
We use a dropout of $0.3$ for all our IWSLT experiments. 
The models are trained for 500K updates and selected based on the validation loss. Note that we do not perform checkpoint averaging for these tasks, rather we run the experiments for 5 times {with different random seeds} and report the mean BLEU scores to provide more consistent and stable results. 
For inference, we use a beam size of $5$, a length penalty of $1.0$ for En-De, $0.2$ for En-Fr, and $2.5$ for Fr-En pair. 

\paragraph{Low-resource setup.} We evaluate our data diversification strategy on the supervised setups of the four low-resource translation tasks: En-Ne, Ne-En, En-Si, and Si-En. We compare our approach against the baseline proposed in \citep{flores}. The  English-Nepali parallel dataset contains about 500K sentence pairs, while the English-Sinhala dataset has about 400K pairs. We use the provided dev set for development and devtest set for testing.

In terms of training parameters, we replicate the same setup as done by \citet{flores}. Specifically, we use the base Transformer model with 5 layers, each of which has 2 attention heads, $d_{model}=512$, $d_{ffn}=2048$. We use a dropout rate of 0.4, label smoothing of 0.2, weight decay of $10^{-4}$. We train the models for 100 epochs with batch size of 16,000 tokens. We select the inference models and length penalty based on the validation loss. The Nepali and Sinhala corpora are tokenized using the Indic NLP library.\footnote{\href{https://github.com/anoopkunchukuttan/indic\_nlp\_library}{https://github.com/anoopkunchukuttan/indic\_nlp\_library}} We reuse the provided shared vocabulary of 5000 tokens built by BPE learned with the \textit{sentencepiece} library.\footnote{\href{https://github.com/google/sentencepiece}{https://github.com/google/sentencepiece}} 

For inference, we use beam search with a beam size of 5, and a length penalty of 1.2 for Ne-En and  Si-En tasks, 0.9 for En-Ne and 0.5 for En-Si. We report tokenized BLEU for from-English tasks and detokenized SacredBLEU \citep{sacredbleu_post-2018-call} for to-English tasks. 
We use $k=3$ in our data diversification experiments. 



\section{Diversity analysis}\label{app:diversity}


As mentioned, by training multiple models with different random seeds, the generated translations from the training set yield only 14\% and 22\% duplicates for En-De and En-Fr, respectively. These results may be surprising as we might expect more duplicates. Therefore, we performed further diversity analysis. 

{To evaluate the diversity the teacher models brought in data diversification, we compare them using the BLEU/Pairwise-BLEU benchmark proposed by \citet{mixture_model_nmt_shen2019}. This benchmark measures the diversity and quality of multiple hypotheses, generated by multiple models or a mixture of experts, given a source sentence. Specifically, we use our forward models trained on WMT'14 English-German and English-French and measure the BLEU and Pairwise-BLEU scores in the provided test set \citep{mixture_model_nmt_shen2019}. The results are reported in Table \ref{table:diversity}, where we compare the diversity and quality of our method against the mixture of experts method provided by \citet{mixture_model_nmt_shen2019} (hMup) and other baselines, such as sampling \citep{biased_sampling_graves2013generating}, diverse bearm search \citep{diverse_beamli2016simple}.} All methods, ours and the baselines, use the big Transformer model.



{As it can be seen in En-De experiments, our method is less diverse than the mixture of experts (hMup) \citep{mixture_model_nmt_shen2019} and diverse beam search \citep{diverse_beamli2016simple} (57.1 versus 50.2 and 53.7 Pairwise-BLEU). However, translations of our method are of better quality (69.5 BLEU), which is very close to human performance. Meanwhile, our method achieve similar quality to beam, but yields better diversity than this approach. The same conclusion can be derived from the WMT'14 English-French experiments.}





\begin{table}[t]
\begin{center}
\caption{{WMT'14 English-German (En-De) and English-French (En-Fr) diversity performances in BLEU and Pairwise-BLEU scores, tested on the test set provided by \citet{mixture_model_nmt_shen2019}.} Lower Pairwise-BLEU means more diversity, higher BLEU means better quality.}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{\bf Method}   & \multicolumn{2}{c}{\bf Pairwise-BLEU}  & \multicolumn{2}{c}{\bf BLEU}\\
\cmidrule{2-5}
                           & \textbf{En-De} & \textbf{En-Fr}    & \textbf{En-De} & \textbf{En-Fr}\\
\midrule
Sampling            & 24.1 & 32.0  & 37.8 & 46.5 \\
Beam                & 73.0 & 77.1  & 69.9 & 79.8 \\
Div-beam            & 53.7 & 64.9  & 60.0 & 72.5 \\
hMup                & 50.2 & 64.0  & 63.8 & 74.6 \\
\midrule
Human               & 35.5 & 46.5  & 69.6 & 76.9 \\
\midrule
Ours                & 57.1 & 70.1  & 69.5 & 77.0  \\
\bottomrule
\end{tabular}
\label{table:diversity}
\end{center}
\end{table}


\section{Failed Hypotheses}\label{app:failed_hypo}
In addition to the successful hypotheses that we described in the paper, we speculated other possible hypotheses that were eventually not supported by the experiments despite being intuitive. We present them in this section for better understanding of the approach.


\paragraph{Effects of Dropout.} 
First, given that parameter initialization affects diversity, it is logical to assume that \textbf{Dropout} will magnify the diversification effects. {In other words, we expected that removing dropout would result in less performance boost offered by our method than when dropout is enabled.} However, our empirical results did not support this.


We ran experiments to test whether non-zero dropout magnify the improvements of our method over the baseline. 
{We trained the single-model baseline and our data diversification's teacher models (both backward, forward models) as well as the student model in cases of $\text{dropout}=0.3$ and $\text{dropout}=0.0$ in the IWSLT'14 English-German and German-English tasks.}
We used factor $k=1$ in these experiments. As reported in Table \ref{table:effect_dropout}, the no-dropout versions perform much worse than the non-zero dropout versions in all experiments. However, the gains made by our data diversification with dropout are not particularly higher than the non-dropout counterpart. This suggests that dropout may not contribute to the diversity of the synthetic data. 


\begin{table}[t]
\begin{center}
\caption{Improvements of data diversification under conditions with- and without- dropout in the IWSLT'14 English-German and German-English.}
\begin{tabular}{lccc}
\toprule
{\bf Task}                  & {\bf Baseline}    & {\bf Ours}    & {\bf Gain}\\
\midrule
\multicolumn{4}{c}{\bf Dropout = $0.3$} \\
\midrule
En-De	& 28.6	& 30.1	& +1.5 (5\%)\\
De-En	& 34.7	& 36.5	& +1.8 (5\%)\\
\midrule
\multicolumn{4}{c}{\bf Dropout = $0.0$} \\
\midrule
En-De	& 25.7	& 27.5	& +1.8 (6\%)\\
De-En	& 30.7	& 32.5	& +1.8 (5\%)\\

\bottomrule
\end{tabular}
\label{table:effect_dropout}
\end{center}
\end{table}



\paragraph{Effects of Beam Search.} 
We hypothesized that beam search would generate more diverse synthetic translations of the original dataset, thus increases the diversity and improves generalization. We tested this hypothesis by using greedy decoding ($\text{beam size}=1$) to generate the synthetic data and compare its performance against beam search ($\text{beam size}=5$) counterparts. We again used the IWSLT'14 English-German and German-English as a testbed. Note that for testing with the final model, we used the same beam search ($\text{beam size} = 5$) procedure for both cases. As shown in Table \ref{table:effect_beam}, the performance of greedy decoding is not particularly reduced compared to the beam search versions.

\begin{table}[t]
\begin{center}
\caption{Improvements of data diversification under conditions maximum likelihood and beam search in the IWSLT'14 English-German and German-English.}
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\bf Task}   & \multirow{2}{*}{\bf Baseline} & \multicolumn{2}{c}{\bf Ours}\\
\cmidrule{3-4}
{\bf }                  & {\bf }    & {\bf Beam=1}    & {\bf Beam=5}\\
\midrule
En-De	& 28.6	& 30.3	& 30.4\\
De-En	& 34.7	& 36.6	& 36.8\\
\bottomrule
\end{tabular}
\label{table:effect_beam}
\end{center}
\end{table}



\section{Correlation with Ensembling}\label{app:proof}
In this section, we show that our data diversification method is optimizing its model distribution to be close to the ensemble distribution under the condition that the final model is randomly initialized. We also show that such condition can be easily met in our experiments. Specifically, let the constituent models of an ensemble of models be $M_i \in \{M_1,...,M_N\}$ and each model $M_i$ produces a model distribution probability $p_{M_i}(Y|X)$ (for $i= 1, \ldots, N$) of the true probability $P(Y|X)$, $X$ and $Y=(y_1,...,y_m)$ be the source and target sentences sampled from the data generating distribution $p_{data}$, $\hat{M}$ be the final model in our data diversification strategy. 
In addition, without loss of generality, we assume that the final model $\hat{M}$ is only trained on the data generated by forward models $M_{S\rightarrow T,r}^i$. 
We also have token-level probability $P(y_j|y_{<j}, X)$ such that:
\begin{equation}
    \log P(Y|X) = \sum_j \log P(y_j|y_{<j}, X)
\end{equation}


For the sake of brevity, we omit $X$ in $P(y_j|y_{<j}, X)$ and use only $P(y_j|y_{<j})$ in the remaining of description. With this, our data diversification method maximizes the following expectation:

\begin{equation}
    \begin{array}{lll}
         \E_{Y\sim U({M_1}(X),...,{M_N}(X)),X\sim p_{data}} \log p_{\hat{M}}(Y|X) &=& \E_{Y\sim Y_c} \sum_{j}\log p_{\hat{M}}(y_j|y_{<j})\\ 
         &=& \sum_{Y\sim Y_c} \frac{1}{N} \sum_j \log p_{\hat{M}}(y_j|y_{<j})\\
         &=& \sum_j  \frac{1}{N} \sum_{Y\sim Y_c} \log p_{\hat{M}}(y_j|y_{<j}) \label{eqn:proof:our_expectation}
    \end{array}
\end{equation}
where $U$ denotes uniform sampling, $Y_c = U({M_1}(X),...,{M_N}(X))$. According to Jensen's Inequality \citep{jensen1906fonctions}, we have:
\begin{equation}
    \sum_j  \frac{1}{N} \sum_{Y\sim Y_c} \log p_{\hat{M}}(y_j|y_{<j}) \leq \sum_j \log [\frac{1}{N} \sum_{Y\sim Y_c} p_{\hat{M}}(y_j|y_{<j})] = \sum_j \log [\frac{1}{N} \sum_{i}^N p_{\hat{M}}(y^i_j|y_{<j}^i)]
\end{equation}
where $y^i_j = \text{argmax}_{y_j}p_{M_i}(y_j|y_{<j})$. Meanwhile, we define $\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j)$ as the averaged probability for output token $y_j^{e}=\text{argmax}_{y_j}\frac{1}{N}\sum_i^N p_{M_i}(y_j)$ of an ensemble of models. Then, since the maximum function is convex, we have the following inequality:
\begin{equation}
    \E \Big[\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j}) \Big] \leq \E\Big[ \frac{1}{N}\sum_i^N \max_{y_j^k} p_{M_i}(y^k_j|y_{<j}^i) \Big] = \E\Big[ \frac{1}{N}\sum_i^N p_{M_i}(y^i_j|y_{<j}^i) \Big]  \label{eqn:proof:ensemble}
\end{equation}
where $y_j^k \in V$ and $V$ is the vocabulary. In addition, with the above notations, by maximizing the expectation \ref{eqn:proof:our_expectation}, we expect our method to automatically push the final model distribution $p_{\hat{M}}$, with the term $\frac{1}{N} \sum_{i}^N p_{\hat{M}}(y^i_j|y_{<j}^i)$ close to the average model distribution of $\frac{1}{N}\sum_i^N p_{M_i}(y_j^i|y_{<j}^i$ (the right-hand side of Eqn. \ref{eqn:proof:ensemble}). 

Through experiments we will discuss later, we observe that our method is able to achieve high performance gain under the following conditions:
\begin{itemize}
    \item Both sides of Eqn. \ref{eqn:proof:ensemble} are tight, meaning they are almost equal. This can be realized when the teacher models are well-trained from the parallel data.
    \item The following inequality needs to be maintained and the training process should stop when the inequality no longer holds:
\end{itemize}
\begin{equation}
    \E\Big[ \frac{1}{N}\sum_i^N p_{\hat{M}}(y^i_j|y_{<j}^i) \Big] \leq \E \Big[ \max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j}) \Big] \leq \E\Big[ \frac{1}{N}\sum_i^N p_{M_i}(y^i_j|y_{<j}^i) \Big] \leq 1 \label{eqn:proof:assumption}
\end{equation}
The left-side equality of condition \ref{eqn:proof:assumption} happens when all $y^i_j$ are identical $\forall i \in \{1,...,N\}$ and $y^i_j= y^e_j \forall y_j$.
In the scenario of our experiments, condition \ref{eqn:proof:assumption} is easy to be met. 

First, when the constituent models $M_{i}$ are well-trained (but not overfitted), the confidence of the models is also high. This results in the expectation $\E\Big[\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j})\Big]$ and $\E\Big[ \frac{1}{N}\sum_i^N p_{M_i}(y^i_j|y_{<j}^i) \Big]$ comparably close to 1.0, compared to uniform probability of $1/|V|$ with $V$ is the target vocabulary and $|V|\gg N$. To test this condition, we empirically compute the average values of both terms over the IWSLT'14 English-German and IWSLT'13 English-French tasks. As reported in Table \ref{table:avg_en_prob_cond}, the average probability $\E\Big[\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j})\Big]$ is around 0.74-0.77, while the average probability $\E\Big[ \frac{1}{N}\sum_i^N p_{M_i}(y^i_j|y_{<j}^i) \Big]$ is always higher but close to the former term (0.76-0.79). Both of these terms are much larger than $1/|V| = 3\times 10^{-5}$. 



Second, as model $\hat{M}$ is randomly initialized, it is logical that $\E\frac{1}{N}\sum_i^N p_{\hat{M}}(y^i_j|y_{<j}^i) \approx 1/|V|$. Thus, under our experimental setup, it is likely that condition \ref{eqn:proof:assumption} is met. However, the condition can be broken when the final model $\hat{M}$ is trained until it overfits on the augmented data. This results in $\frac{1}{N}\sum_i^N p_{\hat{M}}(y^i_j|y_{<j}^i) \leq \E\Big[\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j)\Big]$. This scenario is possible because the model $\hat{M}$ is trained on many series of tokens $y_j^i$ with absolute confidence of $1$, despite the fact that model $M_i$ produces $y_j^i$ with a relative confidence $p_{M_i}(y_j^i|y_{<j}^i) < 1$. In other words, our method does not restrict the confidence of the final model $\hat{M}$ for the synthetic data up to the confidence of their teachers $M_i$. Breaking this condition may cause a performance drop. As demonstrated in Table \ref{table:avg_en_prob_cond}, the performances significantly drop as we overfit the diversified model $\hat{M}$ so that it is more confident in the predictions of $M_i$ than $M_i$ themselves (\ie\ 0.82 versus 0.76 probability for En-De). Therefore, it is recommended to maintain a final model's confidence on the synthetic data as high and close to, but not higher than the teacher models' confidence.


\begin{table}[t]
\begin{center}
\caption{The average value of $\E \Big[\frac{1}{N}\sum_i^N p_{M_i}(y^i_j|y_{<j}^i) \Big]$, $\E \Big[\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j}) \Big]$ and $\E \Big[\frac{1}{N}\sum_i^N p_{\hat{M}}(y^i_j|y_{<j}^i)\Big]$ in the IWSLT'14 English-German, German-English, IWSLT'13 English-French and French-English tasks.}
\begin{tabular}{lcccc}
\toprule
& {\bf En-De} & {\bf De-En} & {\bf En-Fr} & {\bf Fr-En}\\
\midrule
\multicolumn{5}{l}{\bf Teacher models $M_i$}\\
$\E \Big[\frac{1}{N}\sum_i^N p_{M_i}(y^i_j|y_{<j}^i) \Big]$             & 0.76  & 0.78   & 0.76    & 0.79\\
$\E \Big[\max_{y_j^k}\frac{1}{N}\sum_i^N p_{M_i}(y^k_j|y_{<j}) \Big]$   & 0.75  & 0.76   & 0.74    & 0.77\\
Test BLEU                                       & 28.6  & 34.7   & 44.0    & 43.3\\
\midrule
\multicolumn{5}{l}{\bf Diversified Model $\hat{M}$}\\
$\E \Big[\frac{1}{N}\sum_i^N p_{\hat{M}}(y^i_j|y_{<j}^i) \Big]$     & 0.74  & 0.74    & 0.73    & 0.75     \\
Test BLEU                                       & 30.6  & 37.0    & 45.5    & 45.0    \\
\midrule
\multicolumn{5}{l}{\bf Overfitted Diversified Model $\hat{M}$}\\
$\E \Big[\frac{1}{N}\sum_i^N p_{\hat{M}}(y^i_j|y_{<j}^i)\Big]$     & 0.82  & 0.86    & 0.84    & 0.89     \\
Test BLEU                                       & 28.6  & 34.8    & 43.8    & 43.3 \\
\bottomrule
\end{tabular}
\label{table:avg_en_prob_cond}
\end{center}
\end{table}


\section{Details on Back-translation experiments}\label{app:back_translation}
In this section, we describe the complete training setup for the back-translation experiments presented in Section 6. First, for the back-translation baselines \citep{backtranslate_sennrich-etal-2016-improving}, we train the backward models following the same setup for the baseline Transformer presented in Section 4.1 for the WMT'14 En-De experiment and Section 4.2 for IWSLT'14 De-En and En-De experiments. For IWSLT experiments, we use the target-side corpora (En and De) from the WMT'14 English-German dataset to augment the IWSLT De-En and En-De tasks. We use the BPE code built from the original parallel data to transform these monolingual corpora into BPE subwords. This results in a total dataset of 4.66M sentence pairs, which is 29 times larger than the original IWSLT datasets. For the final model trained on the augmented data, we use the big Transformer with the same hyper-parameters as described in Section 4.1. However, note that we use the same shared vocabulary from the baseline setup for the back-translation experiments. On the other hand, for the WMT'14 English-German experiment, we use the German corpus derived from News Crawl 2009, which contains 6.4M sentences. Similarly, we use the same BPE code and shared vocabulary built from the parallel data to transform and encode this monolingual corpus. The process produces a total dataset of 10.9M sentence pairs, which is 2.4 times larger than the original dataset. We use the big Transformer setup for all the WMT experiments.

Second, in terms of back-translation models trained with our data diversification strategy, we use the existing $k$ backward models to generate $k$ diverse sets of source sentences from the provided monolingual data. After that, we combine these datasets with the diverse dataset built by our method from the original parallel data. Then, we train the final model on this dataset. In addition, model setups and training parameters are identical to those used for the back-translation baselines.



 























































































































































\end{document}
