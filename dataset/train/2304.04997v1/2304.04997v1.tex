

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{array}
\usepackage{xspace}
\usepackage{bbm}


\usepackage[accsupp]{axessibility}
\newcommand{\ours}{MUREN}
\newcommand{\sh}[1]{\textcolor{black}{#1}}
\newcommand{\todo}[1]{\textcolor{black}{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{1500} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{Relational Context Learning for Human-Object Interaction Detection}

\author{Sanghyun Kim \hspace{0.8cm}  Deunsol Jung \hspace{0.8cm}  Minsu Cho \vspace{1.5mm}\\
Pohang University of Science and Technology (POSTECH), South Korea \\
{\tt\small \{sanghyun.kim, deunsol.jung, mscho\}@postech.ac.kr} \\
\small
\href{http://cvlab.postech.ac.kr/research/MUREN}{\url{http://cvlab.postech.ac.kr/research/MUREN}}
}


\maketitle

\begin{abstract}
Recent state-of-the-art methods for HOI detection typically build on transformer architectures with two decoder branches, one for human-object pair detection and the other for interaction classification.
Such disentangled transformers, however, may suffer from insufficient context exchange between the branches and lead to a lack of context information for relational reasoning, which is critical in discovering HOI instances.
In this work, we propose the multiplex relation network (MUREN) that performs rich context exchange between three decoder branches using unary, pairwise, and ternary relations of human, object, and interaction tokens. 
The proposed method learns comprehensive relational contexts for discovering HOI instances, achieving state-of-the-art performance on two standard benchmarks for HOI detection, HICO-DET and V-COCO. 


\end{abstract} 


\section{Introduction}
The task of Human-Object Interaction (HOI) detection is to discover the instances of \textit{human, object, interaction} from a given image, which reveal semantic structures of human activities in the image.
The results can be useful for a wide range of computer vision problems such as human action recognition~\cite{moon2021integralaction,bretti2021zero,zhang2019structured}, image retrieval~\cite{Wu_2022_CVPR,yoon2021image,gordo2017beyond}, and image captioning~\cite{Wu_2022_caption,yao2018exploring,herdade2019image} where a comprehensive visual understanding of the relationships between humans and objects is required for high-level reasoning.

With the recent success of transformer networks~\cite{vaswani2017attention} in object detection~\cite{detr,zhu2020deformable}, transformer-based HOI detection  methods~\cite{zhang2021cdn,chen2021asnet,kim2022mstr,kim2021hotr,tamura2021qpic,zou2021hoitrans,zhou2022distr}   have been actively developed to become a dominant base architecture for the task. 
Existing transformer-based methods for HOI detection can be roughly divided into two types: single-branch and two-branch.
The single-branch methods~\cite{tamura2021qpic,kim2022mstr,zou2021hoitrans} update a token set through a single transformer decoder and detect HOI instances using the subsequent FFNs directly. 
As a single transformer decoder is responsible for all sub-tasks (\textit{i.e.,} human detection, object detection, and interaction classification), they are limited in adapting to the different sub-tasks with multi-task learning, simultaneously~\cite{zhang2021cdn}.
\begin{figure}[t]
\begin{center}
   \includegraphics[]{Figure/teaser_fig_3.pdf}
\end{center}
   \caption{The illustration of relation context information in an HOI instance. We define three types of relation context information in an HOI instance: unary, pairwise, and ternary relation contexts.
   Each relation context provides useful information for detecting an HOI instance.
   For example, in our method, the unary context about an interaction (green) helps to infer that a human (yellow) and an object (red) are associated with the interaction, and vice versa. Our method utilizes the multiplex relation context consisting of the three relation contexts to perform context exchange for relational reasoning.
   }
   \vspace{-3mm}
\label{fig:teaser}
\end{figure}
To resolve the issue, the two-branch methods~\cite{zhang2021cdn,kim2021hotr,zhang2022upt,chen2021asnet, zhou2022distr} adopt two separated transformer decoder branches where one detects human-object pairs from a human-object token set while the other classifies interaction classes between  human-object pairs from an interaction token set.
However, the insufficient context exchange between the branches prevents the two-branch methods~\cite{kim2021hotr,zhang2021cdn,zhang2022upt} from learning relational contexts, which plays a crucial role in identifying HOI instances.
Although some methods~\cite{chen2021asnet, zhou2022distr} tackle this issue with additional context exchange, they are limited to propagating human-object context to interaction context.




To address the problem, we introduce the  \textbf{MU}tiplex \textbf{RE}lation \textbf{N}etwork~(MUREN) that performs rich context exchange using unary, pairwise, and ternary relations of human, object, and interaction tokens for relational reasoning.
As illustrated in Figure~\ref{fig:teaser}, we define three types of relation context information in an HOI instance: unary, pairwise, and ternary, each of which provides useful information to discover HOI instances.
The ternary relation context gives holistic information about the HOI instance while the unary and pairwise relation contexts provide more fine-grained information about the HOI instance. For example, as shown in Figure~\ref{fig:teaser}, the unary context about an interaction (\textit{e.g.,} `riding') helps to infer which pair of a human and an object is associated with the interaction in a given image, and the pairwise context between a human and an interaction (\textit{e.g.,} `human' and `riding') helps to detect an object (\textit{e.g.,} `bicycle').
Motivated by this, our multiplex relation embedding module constructs the context information that consists of the three relation contexts, thus effectively exploiting their benefits for relational reasoning.
Since each sub-task requires different context information for relational reasoning, our attentive fusion module selects requisite context information for each sub-task from multiplex relation context and propagates the selected context information for context exchange between the branches.
Unlike previous methods~\cite{kim2021hotr,chen2021asnet,zhang2021cdn,zhou2022distr}, we adopt three decoder branches which are responsible for human detection, object detection, and interaction classification, respectively.
Therefore, the proposed method learns discriminative representation for each sub-task.

We evaluate~MUREN~on two public benchmarks, HICO-DET~\cite{hico} and V-COCO~\cite{vcoco}, showing that MUREN~achieves state-of-the-art performance on two benchmarks.
The ablation study demonstrates the effectiveness of the multiplex relation embedding module and the attentive fusion module.
Our contribution can be summarized as follows:
\begin{itemize}
\item We propose multiplex relation embedding module for HOI detection, which generates context information using unary, pairwise, and ternary relations in an HOI instance.
  \item We propose the attentive fusion module that effectively propagates requisite context information for context exchange.
  \item We design a three-branch architecture to learn more discriminative features for sub-tasks, \textit{i.e.}, human detection, object detection, and interaction classification.
  \item Our proposed method, dubbed MUREN, outperforms state-of-the-art methods on HICO-DET and V-COCO benchmarks.
  
\end{itemize} \section{Related Work}
\subsection{CNN-based HOI Methods.}
Previous CNN-based HOI methods can be categorized into two groups: two-stage methods and one-stage methods.
Two-stage HOI methods~\cite{li2019tin,gao2020drg,gao2018ican,li2020IDN,qi2018dpnn,ulutan2020vsgnet,wang2020hetero,zhang2021spatially,hou2020vcl} first detect the human and the object instances using an off-the-shelf detector~(\textit{e.g.,} Faster R-CNN~\cite{ren2015faster}) and predict the interaction between all possible pairs of a human and an object. 
To create discriminative instance features for HOI detection, they additionally utilize spatial features~\cite{gao2018ican,spaital_learning,li2019tin}, linguistic features~\cite{liu2020consnet,gao2020drg}, and human pose features~\cite{li2019tin,gupta2019no} with visual features.
Some approaches~\cite{qi2018dpnn,gao2020drg,wang2020hetero,ulutan2020vsgnet,zhang2021spatially} utilize the graph structure and exchange the context information of the instance features for relational reasoning between the nodes.
DRG~\cite{gao2020drg} proposes human-centric and object-centric graphs to perform context exchange focused on relevant context information. SCG~\cite{zhang2021spatially} transforms and propagates the context information to the nodes in a graph conditioned on spatial relation.
On the other hand, previous one-stage HOI methods~\cite{kim2020uniondet,liao2020ppdm,fang2021dirv} detect human-object pairs and classify the interactions between human-object pairs in an end-to-end manner.
These methods utilize the interaction region to match the interaction and a pair of a human box and an object box. 
UnionDet~\cite{kim2020uniondet} proposes a union-level detector to find the union box of human and object for matching a human-object pair.
PPDM~\cite{liao2020ppdm} detects interaction centers and points to the center point of the human and object box to predict HOI instances.






\begin{figure*}[t]
\includegraphics[width=0.94\textwidth]{Figure/main_fig_3.pdf}
\caption{The overall architecture of MUREN. The proposed method adopts three-branch architecture: human branch, object branch, and interaction branch. Each branch is responsible for human detection, object detection, interaction classification. The input image is fed into the CNN backbone followed by the transformer encoder to extract the image tokens. A transformer decoder layer in each branch layer extracts the task-specific tokens for predicting the sub-task. The MURE takes the task-specific tokens as input and generates the multiplex relation context for relational reasoning. The attentive fusion module propagates the multiplex relation context to each sub-task for context exchange. The outputs at the last layer of each branch are fed into to predict the HOI instances.}
   \vspace{-3mm}
\label{fig:framework}
\end{figure*}


\subsection{Transformer-based HOI Methods.} 
Inspired by DETR~\cite{detr}, a number of work~\cite{zhou2022distr,zou2021hoitrans,chen2021asnet,tamura2021qpic,kim2022mstr,zhang2022upt,kim2021hotr} have adopted the transformer-based object detector to solve HOI detection. 
They can be divided into two folds: single-branch and two-branch methods.
The single-branch methods~\cite{tamura2021qpic,kim2022mstr,zou2021hoitrans} predict the HOI instances with a single transformer decoder.
MSTR~\cite{tamura2021qpic} utilizes multi-scale features to extract discriminative features for the HOI instances.
In contrast, two-branch methods~\cite{kim2021hotr,chen2021asnet,zhou2022distr,zhang2022upt,zhang2021cdn} adopt two transformer decoder branches, one is responsible for human-object pair detection and the other for interaction classification.
HOTR~\cite{kim2021hotr} detects the instances in an image in detection branch and predicts the interaction with additional offsets to associate humans and objects in interaction branch.
Although they extract discriminative features for each sub-task, there is no context exchange for relational reasoning, bringing performance degradation in HOI detection.
To alleviate this, AS-NET~\cite{chen2021asnet} and DisTR~\cite{zhou2022distr} perform the message passing for relational reasoning between two branches. 
However, they only propagate human-object context information for interaction classification.
In this paper, we exchange the context among branches with the multiplex relation context.
The multiplex relation context, which considers all relation contexts in an HOI instance, gives relational semantics for relational reasoning.
We also extract more discriminative features for each sub-task via three-branch.



 \section{Problem Definition}


Given an input image, the goal of HOI detection is to predict a visually-grounded set of HOI instances for object classes  and interaction classes . An HOI instance consists of four components: a bounding box of human , a bounding box of object , a one-hot vector of object label , and a one-hot
vector of interaction label , where  denotes the size of a set. The output of HOI detection is thus expressed by a set of HOI instances \{. 






\section{Method}





The proposed network, MUREN, is illustrated in Figure~\ref{fig:framework}.
Given an input image, it extracts image tokens via a CNN backbone followed by a transformer encoder. 
The image tokens are fed to three independent branches to perform three sub-task: human detection, object detection, and interaction classification.
In each branch, a transformer decoder layer refines  learnable tokens using the image tokens as keys and values to extract task-specific tokens.
Using the task-specific tokens of each branch, our multiplex relation embedding module~(MURE) generates the context information for relational reasoning. The attentive fusion module then integrates the context information across the task-specific tokens for human, object, and interaction branches, propagating the results to the next layer.
After repeating this process for  times, FFNs predict the set of HOI instances.
In the remainder of this section, we explain the details of each component in MUREN.


\subsection{Image Encoding}
Following the previous work~\cite{detr,tamura2021qpic,zou2021end}, we use a transformer encoder with a CNN backbone to extract image tokens.
The CNN backbone takes an input image to extract an image feature map.
The image feature map is fed into  convolution layer to reduce the channel dimension to , and the positional encoding~\cite{detr} is added to the image feature map to reflect the spatial configuration of the feature map. 
The feature map is then tokenized by flattening and fed into  
the transformer encoder 
to produce image tokens  for the subsequent networks, where  and  are the number of the image tokens and the channel dimension, respectively.

\subsection{HOI Token Decoding}
Different from previous two-branch methods~\cite{kim2021hotr,zhou2022distr,chen2021asnet}, we design an architecture consisting of three branches which is responsible for human detection, object detection, and interaction classification, respectively.
Each branch , consisting of  layers, takes the learnable tokens  and the image tokens  as inputs , where  indicates human, object, and interaction respectively. At each layer,  is refined through a transformer decoder layer followed by a MURE module and an attentive fusion module. Specifically, the three branches take learnable tokens  for human, object, and interaction branches, respectively.
In -th layer of the branch , a transformer decoder layer  updates , the output of previous layer of the branch , by attending  to generate task-specific tokens  which contain the context information for predicting a sub-task which the branch  is responsible for:


where  denotes a transformer decoder layer.













\subsection{Relational Contextualization}


As mentioned above, relational reasoning is crucial to identify HOI instances.
However, since the task-specific tokens are generated from the separated branches, the tokens suffer from a lack of relational context information.
To mitigate this issue, we propose multiplex relation embedding module~(MURE) which generates multiplex relation context for relational reasoning.
The multiplex relation context contains the unary, pairwise, and ternary relation contexts to exploit useful information in each relation context, as shown in Figure~\ref{fig:mure_fig}.


Specifically, the MURE first constructs the ternary relation context  for -th HOI instance by concatenating each  followed by an MLP layer.

where  is a concatenation operation.
We omit the subscript  for the sake of simplicity.
Since the ternary relation takes the overall understanding of each sub-task into account, it gives holistic context information about the HOI instance.
On the other hand, since the unary and the pairwise relations take a fine-grained level understanding of each sub-task into account, they give the fine-grained context information about the HOI instance.
To exploit both holistic and fine-grained context information, we embed the unary and the pairwise relation contexts within the ternary relation context with a sequential manner.



In detail, we apply a self-attention on a set of -th task-specific tokens  to consider the unary relation for -th HOI instance as Eq.~\ref{eq:sa_single}.
Then, the unary-relation context  is embedded into ternary relation context using a cross-attention as Eq.~\ref{eq:ca_single}:

where we denote  as a self-attention operation and  as a cross-attention operation for simplicity.
To embed the pairwise relation context within the ternary relation context, we extract the pairwise features of  for respective human-object, human-interaction, object-interaction relation as follows:

Similar to the above,
we apply the self attention on a set of pairwise features to consider the pairwise relation for -th HOI instance, and the cross attention to embed the pairwise relation contexts within ternary relation context:




Finally, the  is transformed to generate the multiplex relation context  as follows by attending the image tokens :


\sh{It is noteworthy that our high-order (ternary and pairwise) feature functions have a form of non-linear function, \textit{i.e.}, MLP, on top of a tuple of multiple inputs, which is not reducible to a sum of multiple functions of individual lower-order inputs in general. Such a high-order feature function thus can learn the structural relations of the inputs in the tuple, considering all the inputs jointly. For example, a ternary function of three coordinates  can compute the angle feature between {} and {}, which cannot be computed by an individual unary function, , , or  as well as their linear combination. In a similar vein, our ternary feature functions, \textit{i.e.}, Eq.~\ref{eq:triplet}, can effectively learn to capture structural relations which are not easily composable from unary and pairwise feature functions.
}


\begin{figure}[t!]
\centering
   \includegraphics[width=0.45\textwidth]{Figure/mure_fig_2.pdf}
   \caption{The architecture of the multiplex relation embedding module~(MURE). MURE takes -th task-specific tokens and the image tokens as input, and embed the unary and pairwise relation contexts into the ternary relation context. The multiplex relation context, the output of MURE, is fed into subsequent attentive fusion module for context exchange.}
\label{fig:mure_fig}
\vspace{-0.4cm}
\end{figure}


\subsection{Attentive Fusion}
Our attentive fusion module aims to propagate the multiplex relation context to the task-specific tokens for context exchange.
Since each sub-task requires different context information for relational reasoning, the multiplex relation context is transformed using MLP with each task-specific token to propagate the context information conditioned on each sub-task.
We further utilize the channel attention to select the requisite context information for each sub-task.
Then, the refined tokens , the output of -th layer of branch~, is generated by propagating the requisite context information to the task-specific tokens .
Formally, the channel attention  and the refined tokens  are formulated as follows:

where we denote  and  as element-wise multiplication, and sigmoid function, respectively.
As the refined tokens  is generated via context exchange with the multiplex relation context, it deduces the comprehensive relational understanding to discover HOI instances.


The , the output of last layer of branch , is fed into FFNs to predict a set of the HOI predictions.
Formally, given the , the MUREN~predicts a set of HOI predictions \{ using FFNs as follows:

where  is a softmax operation, and ,  are class probability of object and interaction, respectively. 
\subsection{Training Objective}
For training our proposed method, we follow previous transformer-based methods~\cite{tamura2021qpic,zhang2021cdn,zhou2022distr}. 
We adopt the Hungarian Matching~\cite{kuhn1955hungarian} to assign the ground-truth HOI instances to the predictions.
MUREN is trained with multi-task loss composed of four losses: L1 loss~\cite{ren2015faster}  and GIoU loss~\cite{rezatofighi2019generalized}  for the bounding box regression, cross-entropy loss  for the object classification, and focal loss~\cite{lin2017focal}  for the interaction classification. 
The total loss  is formulated as:

where , , , and  are the hyper-parameters for weighting each loss.
Additionally, we apply intermediate supervision for better representation learning.
Specifically, we attach the same FFNs to each decoding branch layer to calculate the intermediate loss.
This auxiliary loss is computed the same as .




\subsection{Inference}
 \sh{Given the set of HOI predictions, we generate a set of HOI instances , where ,  are one-hot vectors with the -th and -th index set to 1, respectively. Following ~\cite{zhang2021cdn}, we then select top- score HOI instances, where the score is given by .} 

 \section{Experiments}
\subsection{Datasets and Metrics}
We evaluate our model on the two public benchmark datasets: HICO-DET~\cite{hico} and V-COCO~\cite{vcoco}. 

\noindent
\textbf{HICO-DET} has 38,118 images for training and 9,658 images for testing.
It contains 80 object classes, 117 interaction classes and 600 HOI classes, which are a pair of an object class and an interaction class (\textit{e.g.,} `riding bicycle'). 
We evaluate the proposed method on Default and Known Object settings.
In the Default setting, the AP is calculated across all testing images for each HOI class.
The Known Object setting calculates the AP of an HOI class over the images containing the object in the HOI class (\textit{e.g.,} the AP of an HOI class `riding bicycle' is only calculated on the images which contain the object `bicycle').
Following the previous work~\cite{zhang2021cdn}, we report the mAP under three splits (Full, Rare, and Non-Rare) for each setting.
The Full, Rare, and Non-Rare splits contain all 600 HOI classes, 138 HOI classes, which have less than 10 training samples for each class, and 462 HOI classes, which have more than 10 training samples for each class, respectively.

\noindent
\textbf{V-COCO} is a subset of the MS-COCO~\cite{coco} dataset. It consists of 5400 and 4,946 images for training, and testing.
It has 80 object classes and 29 action classes. 
Following the evaluation settings in ~\cite{kim2021hotr}, we evaluate the proposed method on scenario 1 and scenario 2, and report role average precision under two scenarios ( for scenario 1 and  for scenario 2).
In scenario 1, the model should predict the bounding box of the occluded object as [0,0,0,0].
In contrast, the predicted bounding box of the occluded object is ignored on calculating the  in scenario 2.




\vspace{-1mm}
\subsection{Implementation Details}
The encoder in MUREN~adopts ResNet-50 as a CNN backbone followed by a 6-layer transformer encoder.
We set the number of branch layers  to 6.
For the training, we set the number of queries  to 64 for HICO-DET and 100 for V-COCO following~\cite{zhang2021cdn}.
The weight of loss , , ,  is set to 2.5, 1, 1, 1, respectively.
The network is initialized with the parameters of DETR~\cite{detr} pretrained on MS-COCO~\cite{coco}.
We optimize our network by AdamW~\cite{loshchilov2017decoupled} with the weight decay . 
We set the initial learning rate of the CNN backbone to  and the other component to . The model is trained with 100 epoch.
For the V-COCO, we freeze the CNN backbone to prevent overfitting, and set the learning rate to . All experiments are conducted with a batch size of 16 on 4 RTX 3090 GPUs.



\begin{table*}[!t]
\begin{center}
\scalebox{0.82}{
\begin{tabular}{ccc P{1.4cm}P{1.4cm}P{1.4cm} c P{1.4cm}P{1.4cm}P{1.4cm}}
\toprule 
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Feature} & \multicolumn{3}{c}{Default} && \multicolumn{3}{c}{Known Object} \\
&&& Full & Rare & Non-Rare && Full & Rare & Non-Rare \\
\midrule
\multicolumn{10}{l}{\textbf{CNN-based methods}} \\
\midrule
iCAN~\cite{gao2018ican} & R50 & A+S & 14.84 & 10.45 & 16.15 && 16.26 & 11.33 & 17.73  \\
TIN~\cite{li2019tin} & R50 & A+S+P & 22.90 & 14.97 & 25.26 && - & - & - \\
GPNN~\cite{qi2018dpnn} & R101 & A & 13.11 & 9.34 & 14.23 && - & -  & -\\
DRG~\cite{gao2020drg} & R50-FPN & A+S+L+M & 24.53 & 19.47 & 26.04 && 27.98 & 23.11 & 29.43 \\
VSGNet~\cite{ulutan2020vsgnet} & R152 & A+S & 19.80 & 16.05 & 20.91 && - & - & - \\
wang \textit{et al}.~\cite{wang2020hetero} & R50-FPN & A+S+M & 17.57 &16.85& 17.78&&21.00& 20.74 &21.08 \\
IDN~\cite{li2020IDN} & R50 & A+S & 26.29 & 22.61 & 27.39 && 28.24 & 24.47 & 29.37 \\ 
VCL~\cite{hou2020vcl} & R50 &  A & 23.63 & 17.21 & 25.55 && 25.98 & 19.12 & 28.03 \\
UnionDet~\cite{kim2020uniondet} & R50 & A & 17.58 & 11.72 & 19.33 && 19.76 & 14.68 & 21.27\\
GGNet~\cite{zhong2021glance} & HG104 & A & 28.83 & 22.13 & 30.84 && 27.36 & 20.23 &  29.48 \\
SCG~\cite{zhang2021spatially} & R50-FPN & A+S+M & 31.33 & 24.72 & 33.31 && 34.37 & 27.18 & 36.52 \\

\midrule
\multicolumn{5}{l}{\textbf{Transformer-based methods}} \\
\midrule
PST~\cite{dong2021pst} & R50 & A & 23.93 & 14.98 & 26.60 && 26.42 & 17.61 & 29.05 \\
HoiTrans~\cite{zou2021hoitrans} & R101 & A & 26.61 & 19.15 & 28.84 && 29.13 & 20.98 & 31.57 \\ 
HOTR~\cite{kim2021hotr} & R50 & A  & 25.10 & 17.34 & 27.42 && - & - & - \\
AS-Net~\cite{chen2021asnet} & R50 & A & 28.87 & 24.25 & 30.25 && 31.74 & 27.07 & 33.14 \\
QPIC~\cite{tamura2021qpic} & R101 & A & 29.90& 23.92& 31.69&& 32.38& 26.06 & 34.27 \\
MSTR~\cite{kim2022mstr} & R50 & A+M & 31.17 & 25.31 & 32.92 && 34.02 & 28.83 & 35.57 \\
CDN~\cite{zhang2021cdn} & R101 & A & 32.07 & 27.19 & \underline{33.53} && 34.79 & 29.48 & 36.38 \\
UPT~\cite{zhang2022upt} & R50 & A+S & 31.66 & 25.94 & 33.36 && 35.05 & 29.27 & \underline{36.77} \\
DisTR~\cite{zhou2022distr} & R50 & A & 31.75 & 27.45 & 33.03 && 34.50 & 30.13 & 35.81 \\
STIP~\cite{zhang2022stip} & R50 & A+S+L & \underline{32.22} & \underline{28.15} & 33.43 && \underline{35.29} & \textbf{31.43} & 36.45 \\  
\hline \midrule
Ours & R50 & A & \textbf{32.87} & \textbf{28.67} & \textbf{34.12} && \textbf{35.52} & \underline{30.88} & \textbf{36.91} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.45cm}
\caption{Performance comparison on the HICO-DET~\cite{hico} dataset. The letters in Feature column stand for A: Appearance/Visual features, S: Spatial features, L: Linguistic features, P: Human pose features, M: Multi-scale features.
The best score is highlighted in bold, and the second-best score is underscored.}
\vspace{-0.4cm}
\label{tab:hicodet-res}
\end{table*}

\vspace{-1mm}
\subsection{Comparison with State-of-the-Art}
Table~\ref{tab:hicodet-res} and Table~\ref{tab:vcoco-res} show the performance comparison of the proposed method with the previous HOI methods.
As shown in Table~\ref{tab:hicodet-res}, on the HICO-DET dataset, the proposed method achieves state-of-the-art performance on Default and Known Object settings against existing CNN- and transformer-based methods.
Compared with the previous CNN-based methods~\cite{zhang2021spatially,qi2018dpnn,gao2020drg,ulutan2020vsgnet,wang2020hetero}, which utilize the graph structure for context exchange, MUREN~shows significant improvements.
We also surpass the previous single-branch methods~\cite{zou2021hoitrans,kim2022mstr,tamura2021qpic}.
It illustrates that it is crucial extracting the task-specific tokens for each sub-task with different branches.
In particular, we outperform the previous two-branch methods~\cite{zhang2022upt,zhang2021cdn,zhou2022distr,chen2021asnet,kim2021hotr}.
DisTR~\cite{zhou2022distr} and AS-NET~\cite{chen2021asnet} perform context exchange for relational reasoning, but they only propagate the context information of the human and the object to the interaction branch for interaction classification.
Instead, we exchange the context information among the three branches, selecting requisite context information from the multiplex relation context for each sub-task.
These results illustrate the advantage of context exchange between each branch using the multiplex relation context for relational reasoning.
Moreover, MUREN~shows better performance without using any additional information (\textit{e.g.,} spatial and linguistic information) compared with~\cite{kim2022mstr,zhang2022stip,zhang2022upt,zhang2021spatially}.
We also outperform~\cite{zhang2021cdn,tamura2021qpic,zou2021hoitrans} which utilize a deeper backbone to extract discriminative features for each sub-task.
These results illustrate that three-branch architecture and context exchange with multiplex relation context for relational reasoning provide more discriminative features to predict each sub-task.
We further evaluate MUREN~on the V-COCO dataset and observe similar results as in the HICO-DET dataset.
As shown in Table~\ref{tab:vcoco-res}, MUREN~achieves state-of-the-art performances across all the metrics compared with existing methods.








\begin{table}[t]
\begin{center}
\scalebox{0.80}{
\begin{tabular}{ccccc}
\toprule
Method & Backbone & Feature &  &  \\
\midrule
\multicolumn{5}{l}{\textbf{CNN-based methods}} \\
\midrule
GPNN~\cite{qi2018dpnn}& R101 & A &  44.0 & - \\
iCAN~\cite{gao2018ican}& R50 &A+S& 45.3 & 52.4 \\
TIN~\cite{li2019tin} & R50 & A+S+P & 47.8 & 54.2 \\
VSGNet~\cite{ulutan2020vsgnet} & R152 & A+S & 51.8 & 57.0 \\
DRG~\cite{gao2020drg} & R50-FPN & A+S+L+M & 51.0 & - \\
VCL~\cite{hou2020vcl} & R101 & A & 48.3 & - \\ 
UnionDet~\cite{kim2020uniondet} & R50 & A & 47.5 & 56.2\\
GGNet~\cite{zhong2021glance}& HG104 & A & 54.7& -  \\
IDN~\cite{li2020IDN} & R50 & A+S & 53.3 & 60.3 \\ 
SCG~\cite{zhang2021spatially} & R50-FPN & A+S+M & 54.2 & 60.9 \\
\midrule
\multicolumn{5}{l}{\textbf{Transformer-based methods}} \\
\midrule
QPIC~\cite{tamura2021qpic} & R50 & A & 58.8 & 61.0 \\
MSTR~\cite{kim2022mstr} & R50 & A+M & 62.0 & 65.2 \\
HOTR~\cite{kim2021hotr} & R50 & A & 55.2 & 61.0 \\
AS-NET~\cite{chen2021asnet} & R50 & A & 53.9 & - \\
CDN~\cite{zhang2021cdn} & R101 & A & 63.9 & 65.9\\
UPT~\cite{zhang2022upt} & R50 & A & 59.0 & 64.5  \\
STIP~\cite{zhang2022stip} & R50 & A+S+L &  66.0 & \underline{70.7} \\  
DisTR~\cite{zhou2022distr} & R50 & A & \underline{66.2} & 68.5  \\
\hline\midrule
Ours & R50 & A &  \textbf{68.8} & \textbf{71.0} \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.4cm}
\caption{Performance comparison on V-COCO~\cite{vcoco} dataset. The letters in Feature column stand for A: Appearance/Visual features, S: Spatial features, L: Linguistic features, P: Human pose features, M: Multi-scale features.
The best score is highlighted in bold, and the second-best score is underscored.}
\vspace{-0.6cm}
\label{tab:vcoco-res}
\end{table}

\vspace{-1.5mm}
\subsection{Ablation Study}
We conduct various ablation studies on the V-COCO dataset to validate the effectiveness of~MUREN.

\noindent
\textbf{Impact of each relation context information on relational reasoning.}
We utilize the multiplex relation context, which contains the unary, pairwise, and ternary relation context, for relational reasoning.
To investigate the impact of each relation context information on relational reasoning, 
we gradually add each relation context information to the baseline, which predicts the HOI instances without context exchange among each branch for relational reasoning.
As shown in Table~\ref{tab:ablation-multiplex-res}, we observe that context exchange using the ternary relation context gives 4.55\%p, 4.22\%p improvement with a large margin in  and , respectively.
This result indicates that context exchange for relational reasoning is essential for discovering the HOI instance and ternary relation context promotes relational reasoning providing holistic information about the HOI instances.
Besides, when the model exploits ternary and unary relation contexts, the model shows an additional performance improvement.
We observe similar results on the model which utilizes both ternary and pairwise relation contexts.
It indicates that the fine-grained relation contexts provide useful information for relational reasoning to predict HOI instances.
When we use all the relation context information in HOI instance, the model shows a significant performance increase of 6.23\%p and 5.86\%p in  and , compared with the baseline.
It demonstrates that each relation context information complements the others, and thus the multiplex relation context provides rich information for relational reasoning and brings performance gain in HOI detection.

\noindent
\textbf{Impact of the multiplex relation context on each sub-task.}
For investigating the propagation impact of the multiplex relation context on the sub-tasks, we gradually add the propagation the multiplex relation context to each branch.
When we propagate the multiplex relation context to one of the detection branches (\textit{i.e.,} human branch and object branch), we observe that the model consistently shows performance improvement compared with the baseline, as shown in Table~\ref{tab:ablation-message-res}.
We also observe the performance gains when the model propagates the multiplex relation context to both human and object branch.
It indicates that relational context information is required to detect the human and the object in the HOI detection.
In particular, when the model propagates the multiplex relation context to the interaction branch, MUREN~shows the notable performance gains of 3.19\%p and 2.77\%p on scenario 1 and scenario 2.
It indicates that the multiplex relation context is essential to interaction classification which requires a comprehensive relational understanding between the human and the object.
The entire model of MUREN, which propagates the relation context information to all sub-tasks, achieves the highest performance with a significant margin compared with the other model variants.
The results demonstrate that context exchange among the three branches is essential to identify HOI instances and plays a crucial role in the comprehensive relational understanding.


\begin{table}[]
\begin{center}
\scalebox{0.88}{
\begin{tabular}{ccccccc}
\toprule
ternary & unary & pairwise &  &  \\
\midrule
- & - & - &  62.52 & 65.14  \\
\checkmark & - & -  &  67.07 & 69.36  \\
\checkmark & \checkmark & - &  68.12 & 70.31  \\
\checkmark & - &\checkmark & 67.67 & 70.02   \\
\hline\midrule
\checkmark & \checkmark &\checkmark & \textbf{68.75} & \textbf{71.00}  \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.4cm}
\caption{The impact of each relation context information on relational reasoning. The `ternary', `unary', and `pairwise' columns indicate the ternary, unary and pairwise relation context.}
\label{tab:ablation-multiplex-res}
\end{table}

\begin{table}[]
\begin{center}
\scalebox{0.82}{
\begin{tabular}{cccccc}
\toprule
human & object & interaction &  &  \\
\midrule
- & - & - &  62.52 & 65.14 \\
\checkmark & - & - &  64.44	& 66.62 \\
- & \checkmark & - &   63.66 & 66.00  \\
\checkmark & \checkmark & - &  65.29 & 67.5 \\
- & - &\checkmark &  65.71 & 67.91  \\
\hline\midrule
\checkmark & \checkmark &\checkmark & \textbf{68.75} & \textbf{71.00} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.4cm}
\caption{The impact of the multiplex relation context on each sub-task. The `human', `object', and `interaction' columns indicate the propagation of the multiplex relation context to human, object, and interaction branch, respectively.}
\vspace{-2mm}
\label{tab:ablation-message-res}
\end{table}

\begin{table}[]
\begin{center}
\scalebox{0.82}{
\begin{tabular}{cccc}
\toprule
conditioning & channel  &  &  \\
\midrule
- & - & 66.50 & 68.96 \\
\checkmark & - & 66.95 & 69.23 \\
- & \checkmark & 67.10  &  69.49 \\
\hline\midrule
\checkmark & \checkmark & \textbf{68.75} &  \textbf{71.00} \\
\bottomrule
\end{tabular}}
\end{center}
\vspace{-0.4cm}
\caption{Ablations studies on each component in the attentive fusion module. `conditioning` and `channel` indicate transforming multiplex relation context conditioned on a task-specific token} and channel attention mechanism.
\label{tab:ablation-att-res}
\vspace{-0.6cm}
\end{table}

\noindent
\textbf{Impact of attentive fusion module on context exchange.}
MUREN~exchanges relational context information between each branch via the attentive fusion module.
To investigate the impact of the attentive fusion module, we remove the attentive fusion module and fuse both the task-specific tokens and the multiplex relation context with an element-wise addition operation for the baseline.
As shown in Table~\ref{tab:ablation-att-res}, the performance drops by 2.25\%p and 2.04\%p in the two scenarios.
It shows the effectiveness of our attentive fusion module for context exchange between the branches.

\noindent
\textbf{Impact of the context information selection for each sub-task.}
In the attentive fusion module, we select requisite context information for each sub-task from the multiplex relation context.
We further analyze the impact of the context information selection as shown in Table~\ref{tab:ablation-att-res}.
To select the requisite context information for each sub-task, we utilize 1) transforming
multiplex relation context conditioned on a task-specific token (`conditioning' in Table~\ref{tab:ablation-att-res}) and 2) channel attention mechanism (`channel' in Table~\ref{tab:ablation-att-res}).
We observe that the model, which utilizes one of `conditioning' and `channel', gains performance improvement.
We also observe that the model with both `conditioning' and `channel' shows better performance than the other model variants.
The results demonstrate that each sub-task requires different context information for relational reasoning, and thus it is important to propagate the requisite context for each sub-task.
Our attentive fusion module effectively selects requisite context information for each sub-task.


\noindent
\textbf{Impact of disentangling human and object branches.}
Human plays a central and an active role for HOI, which is distinctive from a relatively passive role of object, and thus requires a dedicated module to capture relevant attributes and semantics such as pose and clothing. 
We evaluated in Table~\ref{tab:ablation-two-branch}
the effect of sharing parameters between human and object branches; we gradually increased the number of layers that share parameters between the two branches.
The results show that increasing the number of shared layers drops the performance and the full-sharing model, MUREN-(6), results in 2.2\%p and 1.9\%p decrease in performance at two scenarios, respectively, compared with non-sharing model, MUREN-(0).
This is a significant drop also compared to MUREN, which has a similar number of parameters with MUREN-(6) by adjusting the number of layer  of MUREN, indicating that separating human and object branches is important indeed for HOI detection. 









\begin{table}[!]
\begin{center}
\scalebox{0.82}{
\begin{tabular}{cccc}
\toprule
Method &  &  & Params (M) \\
\midrule
MUREN-(0) & 68.8 & 71.0 & 69.3 \\
MUREN-(3) & 67.1 & 69.3 & 64.3 \\
MUREN-(6) & 66.6 & 69.1 & 59.6 \\
MUREN & 68.3 & 70.6 & 59.6 \\ 
\bottomrule
\end{tabular}
}

\end{center}
\vspace{-0.4cm}
\caption{The Impact of disentangling human and object branches. MUREN-() denotes the sharing of parameters between the human and object branches across  layers. The parameters are shared only between corresponding layers.  MUREN is variant of MUREN by adjusting the number of layer .} 
\label{tab:ablation-two-branch}
\vspace{-0.5cm}
\end{table}

\subsection{Qualitative Results}
We visualize HOI detection results and the cross attention map of each branch and the multiplex relation embedding module~(MURE) in Fig.~\ref{fig:vis_att}.
As shown in Fig.~\ref{fig:vis_att}b, c, the human and the object branches focus on the instance extremities to detect the human and the object.
In the Fig.~\ref{fig:vis_att}d, we observe that the interaction branch attends to the regions where the interaction exists between the human and the object.
These results indicate that the task-specific tokens contain context information for predicting each sub-task.
We also observe that the cross-attention map in MURE highlights the overall region that contains the relational semantics about the HOI instance as shown in Fig.~\ref{fig:vis_att}e.
It demonstrates that MURE captures the context information about HOI instance for relational reasoning.








\begin{figure}[!]
\begin{center}
\includegraphics[width=0.432\textwidth]{Figure/vis_att_fig2.pdf}
\end{center}
\vspace{-0.45cm}
\caption{The visualization of the HOI detection results and the cross-attention map in each branch and the multiplex relation embedding module~(MURE). Best viewed in color.}

\vspace{-0.5cm}
\label{fig:vis_att}
\end{figure}

 \section{Conclusion}
\sh{We have proposed MUREN, a one-stage method that effectively performs the context exchange between the three branches for HOI detection. 
By leveraging relation contexts for relational reasoning in MURE and using the attention fusion module to select requisite context information for each sub-task, MUREN can learn discriminative features to predict each sub-task.
Our extensive experiments demonstrate the importance of context exchange between the branches and the effectiveness of MUREN, which achieves state-of-the-art performance on both HICO-DET and V-COCO benchmarks and its components.
}


\noindent \textbf{Acknowledgements.}
This work was supported by the IITP grants (2021-0-00537: Visual common sense through self-supervised learning for restoration of invisible parts in images (50\%), 2021-0-02068: AI Innovation Hub (40\%), and 2019-0-01906: AI graduate school program at POSTECH (10\%)) funded by the Korea government (MSIT).




%
 {\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
