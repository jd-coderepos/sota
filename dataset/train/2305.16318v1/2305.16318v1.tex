\documentclass{article}


    \PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[preprint]{neurips_2023}



\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{adjustbox}
\usepackage{xcolor}         \usepackage{color, colortbl}
\makeatletter
\newcommand\figcaption{\def\@captype{figure}\caption}
  \newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother
\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}
\definecolor{citecolor}{HTML}{2980b9}
\definecolor{linkcolor}{HTML}{c0392b}
\usepackage{subcaption}
\usepackage{makecell} 
\usepackage[hidelinks,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,linkcolor=linkcolor]{hyperref}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\title{Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation}





\author{\hspace{-0.25cm}\textbf{Shilin Yan$^{1,2*}$, Renrui Zhang$^{2,3*}$, Ziyu Guo$^{2*}$, Wenchao Chen$^{1}$}\\ 
  \hspace{-0.25cm}\textbf{Wei Zhang$^{1 \dagger}$, Hongyang Li$^{2\dagger}$, Yu Qiao$^{2}$, Zhongjiang He$^{4}$, Peng Gao$^{2\dagger}$}\vspace{0.3cm}\\
  \hspace{-0.25cm}$^1$School of Computer Science, Fudan University \\
  \hspace{-0.25cm}$^2$Shanghai Artificial Intelligence Laboratory\quad $^3$CUHK MMLab \\
  \hspace{-0.25cm}$^4$China Telecom Corporation Ltd. Data\&AI Technology Company\vspace{0.1cm}\\
  \texttt{\{tattoo.ysl\}@gmail.com, \{zhangrenrui, gaopeng\}@pjlab.org.cn} \\
}



\begin{document}

\blfootnote{\noindent $^{*}$Equal contribution. 
$^{\dagger}$ Corresponding author.}

\maketitle

\begin{abstract}

Recently, video object segmentation (VOS) referred by multi-modal signals, e.g., language and audio, has evoked increasing attention in both industry and academia. It is challenging for exploring the semantic alignment within modalities and the visual correspondence across frames.
However, existing methods adopt separate network architectures for different modalities, and neglect the inter-frame temporal interaction with references. In this paper, we propose \textbf{MUTR}, a \textbf{M}ulti-modal \textbf{U}nified \textbf{T}emporal transformer for \textbf{R}eferring video object segmentation. With a unified framework for the first time, MUTR adopts a DETR-style transformer and is capable of segmenting video objects designated by either text or audio reference. Specifically, we introduce two strategies to fully explore the temporal relations between videos and multi-modal signals. 
Firstly, for low-level temporal aggregation before the transformer, we enable the multi-modal references to capture multi-scale visual cues from consecutive video frames. This effectively endows the text or audio signals with temporal knowledge and boosts the semantic alignment between modalities.
Secondly, for high-level temporal interaction after the transformer, we conduct inter-frame feature communication for different object embeddings, contributing to better object-wise correspondence for tracking along the video.
On Ref-YouTube-VOS and AVSBench datasets with respective text and audio references, MUTR achieves \textbf{+4.2\%} and \textbf{+4.2\%} $\mathcal{J}\&\mathcal{F}$ improvements to \textit{state-of-the-art} methods, demonstrating our significance for unified multi-modal VOS. Code is released at \url{https://github.com/OpenGVLab/MUTR}.

\end{abstract}

\section{Introduction}\label{sec:introduction}
Multi-modal video object segmentation (VOS) aims to track and segment particular object instances across the video sequence referred by a given multi-modal signal, including referring video object segmentation (RVOS) with language reference, and audio-visual video object segmentation (AV-VOS) with audio reference.
Different from the vanilla VOS with only visual information, the multi-modal VOS is more challenging and in urgent demand, which requires a comprehensive understanding among different modalities and their temporal correspondence across frames.



There exist two main challenges in multi-modal VOS. Firstly, it requires to not only explore the rich spatial-temporal consistency in a video, but also align the multi-modal semantics among image, language and audio. Current approaches mainly focus on the visual-language or visual-audio modal fusion within independent frames, simply by cross-modal attention~\cite{chen2019see,hu2020bi,shi2018key} or dynamic convolutions~\cite{margffoy2018dynamic} for feature interaction. This, however, neglects the multi-modal temporal information across frames, which is significant for consistent object segmentation and tracking along the video.
Secondly, for the given references of two modalities, language and audio, existing works adopt different architecture designs and training strategies to separately tackle their modal-specific characteristics. Therefore, a powerful and unified framework for multi-modal VOS still remains an open question.


To address these challenges, we propose \textbf{MUTR}, a \textbf{M}ulti-modal \textbf{U}nified \textbf{T}emporal transformer for \textbf{R}eferring video object segmentation. Our approach, for the first time, presents a generic framework for both language and audio references, and enhances the interaction between temporal frames and multi-modal signals. In detail, we adopt a DETR-like~\cite{carion2020end} encoder-decoder transformer, which serves as the basic architecture to process visual information within different frames. On top of this, we introduce two attention-based modules respectively for low-level multi-modal temporal aggregation (MTA), and high-level multi-object temporal interaction (MTI).
Firstly before the transformer, we utilize the encoded multi-modal references as queries to aggregate informative visual and temporal features via the MTA module. We concatenate the visual features of adjacent frames and adopt sequential attention blocks for multi-modal tokens to progressively capture temporal visual cues of different image scales. This contributes to better low-level cross-modal alignment and temporal consistency.
Then, we regard the multi-modal tokens after MTA as object queries and feed them into the transformer for frame-wise decoding.
After that, we apply the MTI module to conduct inter-frame object-wise interaction, and maintain a set of video-wise query representation for associating objects across frames inspired by~\cite{heo2022vita}. Such a module enhances the instance-level temporal communication and benefits the visual correspondence for segmenting the same object in a video. Finally, we utilize a segmentation head following previous works~\cite{wu2022language,wu2021seqformer} to output the final object mask referred by multi-modality input.



To evaluate our effectiveness, we conduct extensive experiments on several popular benchmarks for multi-modal VOS.
RVOS with language reference (Ref-YouTube-VOS~\cite{seo2020urvos} and Ref-DAVIS 2017~\cite{khoreva2019video}), and one benchmark for AV-VOS with audio reference (AVSBench~\cite{zhou2022audio}). 
On Ref-YouTube-VOS~\cite{seo2020urvos} and Ref-DAVIS 2017~\cite{khoreva2019video} with language references, MUTR surpasses the state-of-the-art method ReferFromer~\cite{wu2022language} by +4.2\%  and +4.1\% $\mathcal{J}\&\mathcal{F}$ scores, respectively. On AV-VOS~\cite{zhou2022audio} with audio references, we also outperform Baseline~\cite{zhou2022audio} by +4.2\% $\mathcal{J}\&\mathcal{F}$ score.


Overall, our contributions are summaried as follows:

\begin{itemize}
   \item For the first time, we present a unified transformer architecture, termed as MUTR, to tackle video object segmentation referred by multi-modal inputs, i.e., either language or audio.

   \item To better align the temporal information with multi-modal signals, we propose two attention-based modules, MTA and MTI, respectively for low-level multi-scale aggregation and high-level multi-object interaction, achieving superior cross-modal understanding in a video.

   \item On benchmarks of two modalities, our approach both achieves state-of-the-art results, e.g., 
   +4.2 \% and +4.1\% $\mathcal{J}\&\mathcal{F}$ for Ref-YouTube-VOS  and  Ref-DAVIS 2017, +4.2\% $\mathcal{J}\&\mathcal{F}$ for AV-VOS. This fully indicates the significance and generalization ability of MUTR.

   
\end{itemize}


\section{Related Work}
\textbf{Referring video object segmentation.} Referring Video Object Segmentation (R-VOS) introduces the language expression for target object tracking and segmentation, so this will be a more challenging task than semi-supervised video object segmentation, which provides the ground truth for the first frame mask. Existing R-VOS methods can be broadly classified into three categories. One of the most straightforward ideas is to apply referring image segmentation methods~\cite{ding2021vision, feng2021encoder, hu2016segmentation, yang2022lavt, wang2022cris} independently to video frames, such as RefVOS~\cite{bellver2020refvos}. It is obvious that this approach disregards the valuable long-temporal information in videos, which makes it difficult to process common video challenges like object disappearance in reproduction and motion occlusion, blurring, etc.
Another approach involves propagating the target mask detected from several key frames throughout the video and selecting the object to be segmented based on a visual grounding model~\cite{kamath2021mdetr,luo2020multi,yu2018mattnet,zhang2017discriminative}. Although this method makes use of the temporal information to some extent and achieves significant performance improvement, its complex multi-stage training approach is not desirable. The very recent work MTTR~\cite{botach2022end} and ReferFormer~\cite{wu2022language} have employed query-based mechanisms. Nevertheless, they are end-to-end transformer frameworks, they perform R-VOS task utilizing image-level reference segmentation and lose the most valuable temporal information. In contrast to these methods, our unified framework fully explores video-level visual-attended language information for low-level temporal aggregation.

\textbf{Audio-visual video object segmentation.}  Audio-visual video object segmentation (AV-VOS) is a typical and challenging problem of predicting pixel-level individual positions based on a given sound signal. There is little previous work on audio-visual video object segmentation. Until recently~\cite{zhou2022audio} proposed the audio-visual video object segmentation dataset and a mechanism to this task. It adopts cross attention to learn the audio-visual correspondence. Different from it,~\cite{mo2023av} is based on the recent visual foundation model Segment Anything Model~\cite{kirillov2023segment,zhang2023personalize} to achieve audio-visual segmentation. However, all of them lack the temporal alignment between multi-modal information.


\textbf{Transformer.} Transformer~\cite{vaswani2017attention} was first proposed as a sequence-to-sequence attention-based building block for machine translation. Due to its powerful capabilities in global context modeling, it has become the cornerstone for most natural language process (NLP)~\cite{ahmed2017weighted,atienza2021vision,zhang2023llama,gao2023llama} and computer vision (CV)~\cite{dosovitskiy2020image,cheng2022masked,guo2023viewrefer} domains. More recently, DETR~\cite{carion2020end} and its variants ~\cite{zhu2020deformable,zheng2020end,gao2021fast} introduced the query-based paradigm for simplifying the traditional object detection pipeline to end-to-end while achieving performance comparable to that of CNN-based detectors~\cite{girshick2015fast}. DETR reformulates detection as a set prediction task and employs a set of learnable object queries as candidates to predict bounding boxes. MonoDETR~\cite{zhangmonodetr} introduces a depth-guided DETR for monocular 3D object detection, and iQuery~\cite{chen2022iquery} utilizes DETR-based architecture for audio-visual video segmentation.
VisTR~\cite{wang2021end} extends the idea behind DETR to the domain of video instance segmentation (VIS) , which solves the problem by supervising all instances of each frame in a video at the sequence level decoding manner. Inspired by these works, our work is also based on the query-based paradigm, but considers the alignment of multi-modal information on temporal, which is implemented by capturing informative visual cues from consecutive video frames, and then regard them as queries for the transformer.
\begin{figure*}[t!]
  \centering
    \includegraphics[width=\textwidth]{./pipeline.pdf}
    \vspace{0.05cm}
   \caption{\textbf{The Overall Pipeline of MUTR for referring video object segmentation.} We present a unified transformer architecture to tackle video object segmentation referred by multi-modal inputs. We propose MTA module and MTI module for low-level multi-scale aggregation and high-level multi-object interaction, respectively.}
    \label{pipeline}
    \vspace{-0.2cm}
\end{figure*}


\section{Method}
In this section, we illustrate the details of our MUTR for multi-modal video object segmentation. We first describe the overall pipeline in Section~\ref{overall}. Then, in Section~\ref{mta} and Section~\ref{mti}, we respectively
elaborate on the proposed designs of the multi-scale temporal aggregation module (MTA), and multi-object temporal interaction module (MTI).



\subsection{Overall Pipeline}
\label{overall}
The overall pipeline of MUTR is shown in Figure~\ref{pipeline}. We adopt a DETR-based~\cite{carion2020end} transformer as our basic architecture, including a visual backbone, a visual encoder and decoder, on top of which, two modules MTA and MTI are proposed for temporal multi-modal interaction. In this section, we successively introduce the pipeline of MUTR for video object segmentation.



\paragraph{Feature Backbone.}
Given an input video-text/audio pair, we first sample $T$ frames from the video clip, and utilize the visual backbone and a pre-trained text/audio backbone to extract the image and multi-modal features. Specifically, we utilize ResNet~\cite{he2016deep} or Swin Transformer~\cite{liu2021swin} as the visual backbone, and obtain the multi-scale visual features of the $2^{nd}, 3^{rd}, 4^{th}$ stages. Concurrently, for the text reference, we employ an off-the-shelf language model, RoBERTa~\cite{liu2019roberta}, to encode the linguistic embedding tokens. For the audio reference, we first process it as a spectrogram transform via a short-time Fourier Transform and then feed it into a pre-trained VGGish~\cite{hershey2017cnn} model. After the text/audio encoding, a linear projection layer is adopted to align the multi-modal feature dimension with the visual features. Note that, following previous work~\cite{wu2022language}, we adopt an early fusion module in the visual backbone to inject preliminary text/audio knowledge into visual features. 

\paragraph{MTA Module.}
On top of feature extraction, we feed the visual and text/audio features into the multi-scale temporal aggregation module (MTA). We concatenate the visual features of adjacent frames, and adopt cascaded cross-attention blocks to enhance the multi-scale and multi-modal feature fusion, which is specifically described in Section~\ref{mta}.

\paragraph{Visual Encoder-decoder Transformer.}
The basic transformer consists of a visual encoder and a visual decoder, which processes the video in a frame-independent manner to focus on the feature fusion within a single frame. In detail, the visual encoder adopts vanilla self-attention blocks to encode the multi-scale visual features. 
The visual decoder regards the encoded visual features as the key and value, and the output references from the MTA module as learnable object queries for decoding. 
Unlike the randomly initialized queries in traditional DETR~\cite{carion2020end}, ours are input-conditioned ones obtained via MTA module, which contains video-level multi-modal prior knowledge. With the visual decoder, the object queries gain rich instance information, which provide effective cues for the final segmentation process.


\paragraph{MTI Module.}
After the visual transformer, a multi-object temporal interaction (MTI) module is proposed for object-wise interaction, which is described in Section~\ref{mti}. In detail, we utilize an MTI encoder to communicate temporal features of the same object in different views. Then an MTI decoder is proposed to grasp information into a set of video-wise query representations for associating objects across frames, inspired by~\cite{heo2022vita}. 

\paragraph{Segmentation Head and Loss Function.}
On top of the components introduced above, we obtain the final mask predictions from the extracted multi-modal features via a segmentation head. We follow previous works~\cite{wu2022language,wu2021seqformer} to design the segmentation head that contains a bounding box head, a classification head, and a mask head. Then, we find the best assignment from the predictions of MUTR by using Hungarian Matching~\cite{carion2020end}. During training, we calculate three losses in MUTR, which are focal loss~\cite{lin2017focal} $\mathcal{L}_{c l s}$ on the predictions of referred object sequence, $\mathcal{L}_{\text {box }}$ on the bounding box of predicted instance, and $\mathcal{L}_{\text {mask }}$ on the predicted object masks. In detail, $\mathcal{L}_{\text {box }}$ is the combination of $L_1$ loss and GIoU loss~\cite{rezatofighi2019generalized}, and $\mathcal{L}_{\text {mask }}$ is the summation of the Dice~\cite{milletari2016v} and binary focal loss. The whole loss function is formulated as
\begin{align}
\label{loss}
\begin{split}
    & \mathcal{L} = \lambda_{cls}\ \mathcal{L}_{cls} + \lambda_{box}\ \mathcal{L}_{box} + \lambda_{mask}\ \mathcal{L}_{mask}\ ,
\end{split}
\end{align}
where $\lambda_{cls}$, $\lambda_{box}$ and $\lambda_{mask}$ denote the weights for $\mathcal{L}_{cls}$, $\mathcal{L}_{box}$ and $\mathcal{L}_{mask}$. See Section~\ref{implementation} for detailed settings.

\subsection{Multi-scale Temporal Aggregation}
\label{mta}
To boost both the multi-modal and multi-frame feature fusion, we introduce \textbf{M}ulti-scale \textbf{T}emporal \textbf{A}ggregation module for low-level temporal aggregation. The proposed MTA module generates a set of object queries that contain multi-modal knowledge for subsequent transformer decoding.

\paragraph{Multi-scale Temporal Transform.}
As shown in Figure~\ref{fig_mta}, the MTA module take the text/audio features $F_r$, and multi-scale visual features as input, i.e., the extracted features of $2^{nd}, 3^{rd}, 4^{th}$ stages from the visual backbone. We first utilize linear projection layers on the multi-scale features to transform them into the same dimension. Specifically, we separately utilize $1\times1$ convolution layers on the $2^{nd},3^{rd},4^{th}$ scale features, and an additional $3\times3$ convolution layer on the $4^{th}$ stage features to obtain the $5^{th}$ scale features. We denote the projected features as $\{F_{vj}^i\}$, where $2\le i \le 5,\ 1 \le j\le T$ represent the stage number and frame number. After that, we concatenate the visual features of adjacent frames for each scale, formulated as
\begin{align}
\label{mta_concat}
\begin{split}
    & F_{v}^i = \operatorname{Concat}(F_{v1}^i,\ F_{v2}^i,\ ...,\ F_{vj}^i,\ ...,\ F_{vT}^i), \\
\end{split}
\end{align} 
where $2\le i \le 5,\ 1 \le j\le T$, $F_{vj}^i$ represents the projected $j^{th}$ frame features of $i^{th}$ scale, and $\{F_{v}^i\}_{i=2}^{5}$ is the final transformed multi-scale visual feature. 
Then, the resulted multi-modal temporal features are regarded as the key and value in the following cross-attention blocks.

\paragraph{Multi-modal Cross-attention.}
On top of this, we adopt sequential cross-attention mechanisms for multi-modal tokens to progressively capture temporal visual cues of different image scales. 
We adopt four cross-attention blocks that are assigned to each scale respectively for multi-scale temporal feature extracting.
In each attention block, the text/audio features serve as the query, while the multi-scale visual features serve as the key and value. We formulate it as
\begin{align}
\label{mta_eq}
\begin{split}
    & F_f = \operatorname{Block}_{i-1}(F_r,\ F_{v}^i,\ F_{v}^i),\ 2 \le i \le 5, \\
\end{split}
\end{align}
where $\operatorname{Block}$ represents the sequential cross-attention blocks in MTA module, $F_f$ is the output multi-modal tokens that contain the multi-modal information. 

After that, we simply repeat the class token of $F_f$ for $T\times N$ times, where $T$ is the frame number and $N$ is the query number. We adopt them as the initialized queries fed into the visual transformer for frame-wise decoding. With the proposed MTA module, the pre-initialized input queries obtain prior multi-scale knowledge and temporal information for better multi-modal alignment during subsequent decoding.


\begin{figure*}[t!]
\begin{minipage}[t]{0.54\textwidth}
\includegraphics[width=\textwidth]{MTA.pdf}
\caption{\textbf{Multi-scale Temporal Aggregation.} For low-level multi-modal temporal aggregation, we propose MTA module for inter-frame interaction, which generates tokens with multi-modal knowledge as the input queries for transformer decoding.}
\label{fig_mta}
\end{minipage} 
\begin{minipage}[t]{0.434\textwidth}
\includegraphics[width=\textwidth]{MTI.pdf}
\caption{\textbf{Multi-object Temporal Interaction.} We introduce MTI module for inter-frame object-wise interaction, and maintain a set of video-wise query representations for associating objects across frames.}
\label{fig_mti}
\end{minipage}
\end{figure*}

\subsection{Multi-object Temporal Interaction}
\label{mti}
Since the visual transformer adopts a frame-independent manner and fails to interact information among multiple frames, we further introduce a \textbf{M}ulti-object \textbf{T}emporal \textbf{I}nteraction module to conduct inter-frame object-wise interaction. This module enhances the high-level temporal communication of objects, and benefits the visual correspondence for effective segmentation. The details of MTI module are shown in Figure~\ref{fig_mti}, which consists of an MTI encoder and an MTI decoder.

\paragraph{MTI Encoder.}
We obtain the object query outputs $P$ of each frame from the transformer decoder, and feed them into the MTI encoder, which contains a self-attention layer to conduct object-wise interaction across multiple frames, and a feed-forward network layer for feature transformation. To achieve more efficient implementation, we adopt shifted window-attention~\cite{liu2021swin} with linear computational complexity in the self-attention layer. The process of MTI encoder is formulated as
\begin{align}
\label{mti_encoder}
\begin{split}
    & P' = \operatorname{MTI\_Encoder}(P)\\
\end{split}
\end{align}
where $\operatorname{MTI\_Encoder}$ denotes the MTI encoder, and $P'$ is the outputs of MTI encoder.

\paragraph{MTI Decoder.}
Based on the MTI encoder, we maintain a set of video-wise query $Q$ for associating objects across frames, which are randomly initialized. We regard the outputs from MTI encoder as the key and value, and feed them and video-wise queries $Q$ into MTI decoder for video-wise decoding. The MTI decoder consists of a self-attention layer, a cross-attention layer, and a feed-forward network layer. We it them as
\begin{align}
\label{mti_decoder}
\begin{split}
& Q' = \operatorname{MTI\_Decoder}(Q,\ P',\ P')\\
\end{split}
\end{align}
where $\operatorname{MTI\_Decoder}$ represents the MTI decoder, $Q'$ is the outputs of MTI decoder. 
In this way, the proposed MTI module promotes high-level temporal fusion and enhances the connection and interaction of the same objects in different frames, which further contributes to effective segmentation.



\section{Experiments}

In Section~\ref{s4.1}, we first introduce the evaluation datasets and metrics. Then, we describe the implementation details in Section~\ref{implementation}.
After that, we present our quantitative results and qualitative results on R-VOS and AV-VOS benchmarks in Section ~\ref{s4.3} and \ref{s4.4}. Finally, in Section~\ref{s4.5} we conduct extensive ablation studies on Ref-YouTube-VOS~\cite{seo2020urvos} dataset.

\subsection{Datasets and Metrics.}\label{s4.1}

\textbf{Datasets.} We evaluate the effectiveness of MUTR on two common-used R-VOS benchmarks, i.e., Ref-DAVIS 2017~\cite{khoreva2019video} and Ref-YouTube-VOS~\cite{seo2020urvos}, and one challenging Audio-Visual Segmentation (AVS) benchmark AVSBench~\cite{zhou2022audio}. Ref-DAVIS 2017 is generated from DAVIS~\cite{pont20172017} by providing more than $1.5k$ referring descriptions with $90$ videos in total, which are divided into two subsets: training set with 60 videos and val set with 30 videos. Ref-YouTube-VOS is a large-scale benchmark that contains 3,978 videos and $15k$ referring descriptions with 3,471/202/305 videos in train/validation/test sets. 
The test set is only available during the competition, so all our experimental evaluation results are performed on the validation set.
AVSBench includes 4,932 videos, which are divided into 3,452/740/740 videos for train/val/test subsets. 

\textbf{Evaluation Metrics.} We adopt the standard evaluation protocol~\cite{perazzi2016benchmark,pont20172017} such as the region accuracy $\mathcal{J}$,  boundary accuracy  $\mathcal{F}$ and their average value ($\mathcal{J}\&\mathcal{F}$) for all benchmarks.
Ref-DAVIS 2017\footnote{https://github.com/davisvideochallenge/davis2017-evaluation} and AVSBench\footnote{https://github.com/OpenNLPLab/AVSBench} are evaluated by their official evaluation code, respectively. Ref-YouTube-VOS is evaluated by submitting the results to the official evaluation server\footnote{https://codalab.lisn.upsaclay.fr/competitions/3282}.

\subsection{Implementation details.}\label{implementation}
Following prior works~\cite{wu2022language}, we evaluate our models under various backbones including: ResNet~\cite{he2016deep}, Swin Transformer~\cite{liu2021swin} and Video Swin Transformer~\cite{liu2022video} and multi-modality inputs such as text or audio to verify the effectiveness of our method. In the visual encoder-decoder transformer, we adopt 4 encoder layers and 4 decoder layers, which are the same as Referformer~\cite{wu2022language}. The MTA module is composed of a 3-layer encoder and 3-layer decoder. 
We set the number of multi-modal queries to 5.
We train the model with AdamW~\cite{loshchilov2017decoupled} optimizer on 8 A100 GPUs.
The initial learning rate of the transformer and backbone are $1 \times 10^{-4}$ and $6 \times 10^{-6}$. The weight decay is set to $5 \times 10^{-4}$.
We set the batch size to 2 and 8 on R-VOS and AV-VOS, respectively. For losses, the coefficients are set as $\lambda_{\text {cls }}$ = 2, $\lambda_{L_1}$= 5, $\lambda_{\text {giou }}$ = 2, $\lambda_{\text {dice }}$ = 5, $\lambda_{\text {focal }}$ = 2. For all datasets, the frames are downsampled that the lower resolution with the shortest side ranges from 288 to 512, and the longest side is smaller than 640, referring to the setting in~\cite{wu2022language}.


\paragraph{R-VOS.} All of our models are trained for 12 epochs on the mixed data from Ref-YouTube-VOS and Ref-COCO~\cite{yu2016modeling}. The training video clip consists of 5 frames, while for the static images in Ref-COCO, a synthetic video sequence of 5 frames is generated using various data augmentations, including affine and perspective transformations.

The learning rate is scaled by the factor 0.1 on the 8th and 10th epoch.

\paragraph{AV-VOS.} Since only the first video frame in the AVSBench dataset is annotated, we perform data augmentations to the first frame to generate a pseudo video sequence of 5 frames for training. The models are trained for a total of 40 epochs, and then the learning rate decreases at the 30th epoch and the 35th epoch, where the drop coefficient is 0.1.

































\begin{table}[t!]
\vspace{-0.2cm}
\centering
\caption{\textbf{Performance of MUTR on Ref-YouTube-VOS and Ref-DAVIS 2017 Datasets.} We report the results of MUTR and prior works on multiple backbones, where our MUTR shows the \textit{state-of-the-art} performance on all datasets.
}
\label{tab:ref-ytb-davis}
\vspace{0.12cm}
\small
\centering
\setlength\tabcolsep{10pt}
\begin{tabular}{l|c|ccc|ccc}
\toprule[1.1pt]
    \makecell*[c]{\multirow{2}*{Method}} 
    &\makecell*[c]{\multirow{2}*{Backbone}} &  \multicolumn{3}{c|}{Ref-YouTube-VOS}  & \multicolumn{3}{c}{Ref-DAVIS 2017} \\
    & &$\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$ \\
\cmidrule(lr){1-8} CMSA~\cite{ye2019cross} & \makecell*[c]{\multirow{8}*{ResNet-50}} & 34.9 & 33.3 & 36.5 & 34.7 & 32.2 & 37.2 \vspace{-3pt}\\
CMSA\ +\ RNN~\cite{ye2019cross} & & 36.4 & 34.8 & 38.1 & 40.2 & 36.9 & 43.5 \\
URVOS~\cite{seo2020urvos} & & 47.2 & 45.3 & 49.2 & 51.5 & 47.3 & 56.0 \\
LBDT-4~\cite{ding2022language} & & 48.2 & 50.6 & 49.4 & - & - & - \\
YOFO~\cite{li2022you} & & 48.6 & 47.5 & 49.7 & 53.3 & 48.8 & 57.9  \\
MRLR~\cite{wu2022multi} & & 48.4 & 51.0 & 49.7 & 57.9 & 53.9 & 62.0   \\
ReferFormer~\cite{wu2022language} & & 58.7 & 57.4 & 60.1 & 61.1 & 58.0 & 64.1 \\
\textbf{MUTR} & & \textbf{61.9} & \textbf{60.4} & \textbf{63.4} & \textbf{65.3} & \textbf{62.4} & \textbf{68.2} \\
\cmidrule(lr){1-8}  
CITD~\cite{liang2021rethinking} &\makecell*[c]{\multirow{3}*{ResNet-101}} & 56.4 & 54.8 & 58.1 & - & - & -  \vspace{-3pt}\\
ReferFormer~\cite{wu2022language} & & 59.3 & 58.1 & 60.4 & 61.0 & 58.1 & 63.8 \\
\textbf{MUTR}& & \textbf{63.6} & \textbf{61.8} & \textbf{65.4} & \textbf{65.3} & \textbf{61.9}& \textbf{68.6} \\
\cmidrule(lr){1-8}  
ReferFormer~\cite{wu2022language} &\makecell*[c]{\multirow{2}*{Swin-L}} & 64.2 & 62.3 & 66.2 & 63.9 & 60.8 & 67.0 \vspace{-3pt}\\
\textbf{MUTR}& & \textbf{68.4} & \textbf{66.4} & \textbf{70.4} & \textbf{68.0} & \textbf{64.8} & \textbf{71.3} \\
\cmidrule(lr){1-8}  
MTTR~\cite{botach2022end} &\makecell*[c]{\multirow{4}*{Video-Swin-T}} & 55.3 & 54.0 & 56.6 & - & - & - \vspace{-3pt}\\
MANet~\cite{chen2022multi} & & 55.6 & 54.8 & 56.5 & - & - & - \\
ReferFormer~\cite{wu2022language} & & 62.6 & 59.9 & 63.3 & 62.8 & 60.8 & 67.0 \\
\textbf{MUTR}& & \textbf{64.0} & \textbf{62.2} & \textbf{65.8} & \textbf{66.5} & \textbf{63.0} & \textbf{70.0} \\
\cmidrule(lr){1-8}  
ReferFormer~\cite{wu2022language} &\makecell*[c]{\multirow{2}*{Video-Swin-S}} & 63.3 & 61.4 & 65.2 & 62.3 & 58.8 & 65.8 \vspace{-3pt}\\
\textbf{MUTR}& & \textbf{65.1} & \textbf{63.0} & \textbf{67.1} & \textbf{66.1} & \textbf{62.5} & \textbf{69.8} \\
\cmidrule(lr){1-8}  
VLT~\cite{ding2022vlt} &\makecell*[c]{\multirow{3}*{Video-Swin-B}} & 63.8 & 61.9 & 65.6 & 61.6 & 58.9 & 64.3 \vspace{-3pt}\\
ReferFormer~\cite{wu2022language} && 64.9 & 62.8 & 67.0 & 64.3 & 60.7 & 68.0 \\
\textbf{MUTR}& & \textbf{67.5} & \textbf{65.4} & \textbf{69.6}& \textbf{66.4} & \textbf{62.8} & \textbf{70.0} \\
\bottomrule[1.1pt]
\end{tabular}
\end{table}






 
\subsection{Quantitative Results}\label{s4.3}
\paragraph{Ref-YouTube-VOS.} 

As show in Table~\ref{tab:ref-ytb-davis}, MUTR outperforms the previous state-of-the-art methods by a large margin under on all datasets. On Ref-YouTube-VOS, MUTR with a lightweight backbone ResNet-50 achieves the superior performance with overall $\mathcal{J}\&\mathcal{F}$ of 61.9\%, an improvement of +3.2\% than the previous state-of-the-art method Referformer. 
By adopting a more powerful backbone Swin-Transformer~\cite{liu2021swin}, MUTR improves the performance to $\mathcal{J}\&\mathcal{F}$ 68.4\%, which is +4.2\%  than the previous method ReferFormer~\cite{wu2022language}. Using a more strong backbone, our method has a higher percentage of improvement, which better reflects the robustness of our method on the scaled-up model size. 
To reflect the powerful temporal modeling capability of MUTR, we therefore adopt the video Swin transformer~\cite{liu2022video} as the backbone, which is a spatial-temporal encoder that can effectively capture the spatial and temporal cues simultaneously, to compensate for the temporal limitations of the ReferFormer as discussed in~\cite{hu20221st}. It can be observed that our method significantly outperforms the ReferFormer, which demonstrates the effectiveness of the temporal consistency in our model.


\paragraph{Ref-DAVIS 2017.} On the Ref-DAVIS 2017, our method also achieves the best results under the same backbone setting. Since ReferFormer~\cite{wu2022language} does not include the results

on Ref-DAVIS 2017, we report its results using the official pre-trained models provided by ReferFormer.

\paragraph{AV-VOS.} Table~\ref{tab:avsbench} shows the performance of our MUTR on the AVSBench dataset. MUTR significantly surpasses all the previous best competitors  ($\mathcal{J}\&\mathcal{F}$ \textbf{83.0\% VS 78.8\%}) with the same ResNet-50 backbone. We also achieve a new state-of-the-art performance with Swin-L~\cite{liu2021swin} backbone.
By employing a stronger backbone, we observe consistent performance improvement of MUTR, indicating the strong generalization of our approach.


\begin{figure*}[t]

\begin{minipage}[t]{0.557\textwidth}
\captionof{table}{\textbf{Performance of MUTR on AVSBench Dataset.} MUTR surpasses the \textit{state-of-the-art} method.}
\vspace{0.05cm}
\label{tab:avsbench}
\resizebox{\textwidth}{!}{\begin{tabular}{l|c|ccc}
\toprule
 Methods  & Backbone & $\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$  \\
\cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-5}
LVS~\cite{chen2021localizing} & ResNet-18 & 44.5  & 37.9 & 51.0   \\
MSSL~\cite{qian2020multiple} & ResNet-18 &  55.6 & 44.9  & 66.3   \\
3DC~\cite{mahadevan2020making}  & ResNet-152 &66.5  & 57.1 & 75.9  \\
SST~\cite{duke2021sstvos}  & ResNet-50 &73.2  & 66.3 & 80.1  \\
iGAN~\cite{mao2021transformer} & - &69.7  & 61.6 & 77.8  \\
LGVT~\cite{zhang2021learning} & Swin-B &{81.1}  & {74.9} & {87.3}  \\
Baseline~\cite{zhou2022audio} &ResNet-50   &{78.8}  & {72.8} & {84.8}  \\
Baseline~\cite{zhou2022audio} &Pvt-v2   &{83.3}  & {78.7} & {87.9}  \\
\cmidrule(lr){1-5}
\multirow{6}*{MUTR}  & ResNet-50 &{83.0}  & {78.6} & {87.3}  \\
& ResNet-101 &{83.1}  & {78.5} & {87.6}  \\
& Swin-L &{85.7}  & {81.5} & {89.8}  \\
& Video-Swin-T &{83.0}  & {78.7} & {87.2}  \\
& Video-Swin-S &{84.1}  & {79.8} & {88.3}  \\
& Video-Swin-B &{85.7}  & {81.6} & {89.7}  \\
\bottomrule
\end{tabular}
}
\end{minipage}\hfill
\begin{minipage}[t]{0.42\textwidth}
    \begin{minipage}[t]{\linewidth}
\centering
\small
\tabcaption{Ablation Study of the MTA and MTI Modules of MUTR.}
\label{tab:mta_mti}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{c c|c c c}
    \toprule
         MTA  & MTI  & $\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ &  $\mathcal{F}$     \\ 
        \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-5}
        - &- & 60.2 & 58.7 & 61.7 \\
        - & \checkmark & 60.0 & 58.5 & 61.5 \\
        \checkmark &- & 61.5 & 60.1 & 63.0 \\
        \rowcolor{gray!10}\checkmark &\checkmark & \textbf{61.9} & \textbf{60.4} & \textbf{63.4} \\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{minipage}\qquad 
\begin{minipage}[t]{\linewidth}
\centering
\vspace{0.3cm}
\tabcaption{Ablation Study of Query Number in Visual Transformer and MTI Module.}
\label{tab:query}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{c|ccc}
    \toprule
         Query Number & $\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$ \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-4}
       1 & 61.2 & 59.6 & 62.9 \\
       3 & 61.3 & 59.9 & 62.7 \\
       \rowcolor{gray!10} \textbf{5} & \textbf{61.9} & \textbf{60.4} & \textbf{63.4} \\
       7 & 61.4 & 59.9 & 62.8 \\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{minipage}
\end{minipage}

\end{figure*}



\subsection{Qualitative Results}\label{s4.4}

The first two columns of Figure~\ref{qualitative results} visualize some qualitative results in comparison with ReferFormer~\cite{wu2022language}, which lacks inter-frame interaction in terms of temporal dimension. As demonstrated, along with multiple highly similar objects in the video, ReferFormer~\cite{wu2022language} is easier to misidentifies them. In contrast, our MUTR is able to associate all the objects in temporal, which can better track and segment all targets accurately.

The last column of Figure~\ref{qualitative results} visualizes the audio-visual result compared with Baseline~\cite{zhou2022audio} on AVSBbench dataset. With temporal consistency, MUTR can successfully track and segment challenging situations that are surrounded or occluded by similar instances.

\subsection{Ablation Studies}\label{s4.5}
In this section, we perform extensive experiments to analyze the main components and hyper-parameters of MUTR. All the ablation experiments are conducted with the ResNet-50 backbone and evaluate their impact by the Ref-YouTube-VOS performance.


\paragraph{Component Effectiveness Study.} Table~\ref{tab:mta_mti} demonstrates effectiveness of the Multi-scale Temporal Aggregation (MTA) and Multi-object Temporal Interaction (MTI) proposed in our framework.  The performance will be seriously degraded from  61.9\% to 60.2\% by removing MTA and MTI modules.

\paragraph{Ablation Study on MTA.}
In Table~\ref{tab:mta}, if either the single-scale temporal aggregation or multi-scale aggregation at the image level are adopted, the performance of MUTR would significantly drop to 60.4\% and 61.3\%, respectively, which demonstrates the necessity of the MTA module. We also ablate the number of MTA blocks. As seen in Table~\ref{tab:mta}, more MTA blocks cannot bring further performance improvement, since (1) not enough videos for training; (2) the embedding space of visual and reference is only 256-dimensional, which is difficult to optimize so many parameters.

\begin{figure*}[t!]
  \centering
    \includegraphics[width=\textwidth]{./vis.pdf}
   \caption{\textbf{Qualitative Results of MUTR.} We visualize the results between ReferFormer~\cite{wu2022language} and MUTR on R-VOS benchmarks and between Baseline~\cite{zhou2022audio} and MUTR on AV-VOS benchmark. Compared with ReferFormer, MUTR performs better on temporal consistency when segmenting multiple similar objects, i.e., fire truck in Ref-YouTube-VOS and box in Ref-DAVIS 2017. Also, compared with the baseline of AV-VOS~\cite{zhou2022audio} that denoted as `Baseline' in this figure, MUTR can handle serve occlusion.}
    \label{qualitative results}
    \vspace{0.2cm}
\end{figure*}


\begin{figure*}
\begin{minipage}[t!]{0.484\linewidth}
\centering
\tabcaption{Ablation Study of MTA Module.}
\label{tab:mta}
\vspace{0.1cm}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{cccccc}
    \toprule
    \multicolumn{2}{c}{Components} &\makecell*[c]{\multirow{2}*{\shortstack{\vspace*{1.2pt}\\Block\\\vspace*{0.3pt}\\Num.}}} 
    &\makecell*[c]{\multirow{2}*{$\mathcal{J\&F}$}} 
    &\makecell*[c]{\multirow{2}*{$\mathcal{J}$}}
    &\makecell*[c]{\multirow{2}*{$\mathcal{F}$}} \\
    \cmidrule(lr){1-2} 
      Multi-scale &Temporal &&&&\\
\cmidrule(lr){1-1}  \cmidrule(lr){2-2}  \cmidrule(lr){3-3}  \cmidrule(lr){4-4}  \cmidrule(lr){5-5}  \cmidrule(lr){6-6}  
     \checkmark &- &1 &61.3 &59.7 &62.7\\
     - &\checkmark &1 &60.4 &58.9 &61.9\\
     \rowcolor{gray!10}\checkmark &\checkmark &1 &\textbf{61.9} &\textbf{60.4} &\textbf{63.4}\\
     \checkmark &\checkmark &2 &60.7 &59.3 &62.2\\
     \checkmark &\checkmark &3 &60.4 &59.1 &61.7\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{minipage}\qquad
\begin{minipage}[t!]{0.451\linewidth}
\centering
\small
\tabcaption{Ablation Study of MTI Module.}
\label{tab:mti}
\vspace{0.1cm}
\begin{adjustbox}{width=\linewidth}
\begin{tabular}{cccccc}
    \toprule
    \multicolumn{2}{c}{Components} &\makecell*[c]{\multirow{2}*{\shortstack{\vspace*{1.2pt}\\Block\\\vspace*{0.3pt}\\Num.}}} 
    &\makecell*[c]{\multirow{2}*{$\mathcal{J\&F}$}} 
    &\makecell*[c]{\multirow{2}*{$\mathcal{J}$}}
    &\makecell*[c]{\multirow{2}*{$\mathcal{F}$}} \\
    \cmidrule(lr){1-2} 
      Encoder &Decoder &&&&\\
\cmidrule(lr){1-1}  \cmidrule(lr){2-2}  \cmidrule(lr){3-3}  \cmidrule(lr){4-4}  \cmidrule(lr){5-5}  \cmidrule(lr){6-6}  
     \checkmark &- &3 &60.3 &58.8 &61.9\\
     - &\checkmark &3 &61.2 &60.0 &62.6\\
     \rowcolor{gray!10}\checkmark &\checkmark &3 &\textbf{61.9} &\textbf{60.4} &\textbf{63.4}\\
     \checkmark &\checkmark &2 &61.1 &59.5 &62.6\\
     \checkmark &\checkmark &1 &60.8 &59.3 &62.3\\
    \bottomrule
\end{tabular}
\end{adjustbox}
\end{minipage}
\end{figure*}

\paragraph{Ablation Study on MTI.}
According to the results in Table~\ref{tab:mti}, the performance of MUTR is improved by using more MTI blocks. A possible reason is that the larger the MTI blocks, the more sufficient temporal communication between instance-level can be performed.
Moreover, using only the encoder or decoder, the performance of MUTR would both decline. 

\paragraph{Query Number.}
We also investigate the influence of different numbers of query on the final performance in Table~\ref{tab:query}. In our default setting, we set the query number $N$ to 5. As shown in Table~\ref{tab:query}, even in the case of $N$ = 1, MUTR still achieves very competitive results. It is observed that more queries enable the model to select from multiple candidate instances, which make it handle more challenging situations when the object to be referred is surrounded by other similar instances. However, as the number of queries increases (e.g. $N$ = 7), there is a clear degradation in performance. One possible reason is the imbalance of number between positive and negative samples, since only one object is referenced.

\section{Conclusion}
This paper proposes a MUTR, a \textbf{M}ulti-modal \textbf{U}nified \textbf{T}emporal transformer for \textbf{R}eferring video object segmentation. A simple yet and effective Multi-scale Temporal Aggregation (MTA) is introduced for multi-modal references to explore low-level multi-scale visual information in video-level. Besides, the high-level Multi-object Temporal Interaction (MTI) is designed for inter-frame feature communication to achieve temporal correspondence between the instance-level across the entire video. Aided by the MTA and MTI, our MUTR achieves new state-of-the-art performance on three R-VOS/AV-VOS benchmarks compared to previous solutions.
We hope the MTA and MTI will help ease the future study of multi-modal VOS and related tasks (e.g., referring video object tracking and video instance segmentation). We do not foresee negative social impact from the proposed work.

\bibliographystyle{splncs04}
\bibliography{references}
\clearpage

\appendix
\section{Overview}
\begin{itemize}
    \item \cref{B}: Additional implementation details.
    \item \cref{C}: Additional experiments.
    \item \cref{D}: Additional visualizations.
\end{itemize}

\section{Additional Experiment Details}
\label{B}
\subsection{Dataset Details}
\textbf{Ref-YouTube-VOS} is built upon YouTube-VOS~\cite{xu2018youtube} by providing 12,193 language descriptions for the training set of 3,471 videos and 202/305 videos with 2,096 expressions in validation/test set. Note that, the test set targets on competition that the server is currently inaccessible. Each object is annotated with two kinds of referring expressions for the \textit{first-frame} and \textit{full-video}.

\textbf{Ref-DAVIS 2017} is expanded from DAVIS~\cite{pont20172017} by providing 1,544 expression sentences describing 205 objects in total. The referred instance is annotated with two annotators and each of them gives the \textit{first-frame} and \textit{full-video} textual description with the same as Ref-YouTube-VOS. For fair comparisons, we report the results by averaging the scores with the same setting referring to ReferFormer~\cite{wu2022language}.

\textbf{AVSBench} is the first pixel-level audio-visual segmentation benchmark that contains 4,932 videos (5 frames each, 10,852 annotated frames) covering 23 categories including instruments, humans, animals, etc. Only the first frame is annotated in the training set, all the test set and validation set are annotated.

\subsection{Implementation Details}
\paragraph{R-VOS.} For the modality of language expression with the number words of $L$, we employ an off-the-shelf linguistic model, RoBERTa~\cite{liu2019roberta}, to extract the text feature ${F}_{r}~\in~\mathbb{R}^{L \times C_{l}}$, where $C_l$ = 768. The learning rate in the training phase is $1 \times 10^{-5}$. Note that, for Ref-DAVIS 2017, we directly report the results using the model trained on Ref-Youtube-VOS without fine-tuning referring to~\cite{wu2022language}.

\paragraph{AV-VOS.} Given an input of audio clip, the audio features ${F}_{r}~\in~\mathbb{R}^{L \times C_a}$ are extracted from  VGGish~\cite{hershey2017cnn} that is pre-trained on AudioSet~\cite{gemmeke2017audio}, where $C_a$ = 128 is the feature dimension. It should be noted that the audio encoder is frozen all the time following~\cite{zhou2022audio}.

\begin{table}[h]
\centering
\caption{\textbf{Performance of MUTR on Ref-YouTube-VOS and Ref-DAVIS 2017 Datasets.} We report the results between MUTR and UNINEXT on multiple backbones, where our MUTR shows the \textit{state-of-the-art} performance on all datasets.
}
\label{supptab:ref-ytb-davis}
\vspace{0.12cm}
\small
\centering
\setlength\tabcolsep{10pt}
\begin{tabular}{l|c|ccc|ccc}
\toprule[1.1pt]
    \makecell*[c]{\multirow{2}*{Method}} 
    &\makecell*[c]{\multirow{2}*{Backbone}} &  \multicolumn{3}{c|}{Ref-YouTube-VOS}  & \multicolumn{3}{c}{Ref-DAVIS 2017} \\
    & &$\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$ \\
\cmidrule(lr){1-8} UNINEXT~\cite{yan2023universal} &\makecell*[c]{\multirow{2}*{ResNet-50}} & 61.2 & 59.3 & 63.0 & 63.9 & 59.6 & 68.1 \\
\textbf{MUTR} & & \textbf{61.9} & \textbf{60.4} & \textbf{63.4} & \textbf{65.3} & \textbf{62.4} & \textbf{68.2} \\
\cmidrule(lr){1-8}  
UNINEXT~\cite{yan2023universal} &\makecell*[c]{\multirow{2}*{ConvNext-Large}} & 66.2 & 64.0 & 68.4 & 66.7 & 62.3 & 71.1 \vspace{-3pt}\\
\textbf{MUTR}& & \textbf{66.7} & \textbf{64.8} & \textbf{68.7}& \textbf{69.0} & \textbf{65.6} & \textbf{72.4} \\
\cmidrule(lr){1-8}  
\textbf{MUTR}&\makecell*[c]{\multirow{1}*{ConvMAE-Base}} & \textbf{66.9} & \textbf{64.7} & \textbf{69.1}& \textbf{69.2} & \textbf{65.6} & \textbf{72.8} \\

\bottomrule[1.1pt]
\end{tabular}
\end{table}

\section{Additional Experiments}
\label{C}
\paragraph{R-VOS.} All of our models are trained on the mixed dataset from image referring segmentation datasets Ref-COCO~\cite{yu2016modeling}, Ref-COCOg~\cite{yu2016modeling}, Ref-COCO+~\cite{mao2016generation} and referring video segmentation dataset Ref-YouTube-VOS. UNINEXT~\cite{yan2023universal} is pre-trained on the large-scale  object detection dataset Objects365~\cite{shao2019objects365}, and then finetune it on RefCOCO/g/+ and Ref-YouTube-VOS. For comparison, we further evaluate our model under ConvNext-Large and a ViT-based~\cite{dosovitskiy2020image} backbone ConvMAE~\cite{gao2022convmae}. The results are shown in Table~\ref{supptab:ref-ytb-davis}.


\begin{figure*}[t!]
  \centering
    \includegraphics[width=\textwidth]{./sup_text.pdf}
   \caption{\textbf{Qualitative results on Ref-YouTube-VOS between ReferFormer and MUTR.} }
    \label{sup_text}
    \vspace{-0.2cm}
\end{figure*}


\begin{table*}[t]
\centering
\caption{\textbf{Performance of MUTR on AVSBench Dataset.} We report the results between MUTR and Baseline on multiple backbones. $*$ represents the results of our own reproduction.
}
\label{supp:avsbench}
\begin{tabular}{c|c|ccc|ccc}
\toprule
 \multirow{2}*{Methods}  & \multirow{2}*{Backbone} & \multicolumn{3}{c|}{AVSBench Validation}& \multicolumn{3}{c}{AVSBench Test} \\
 \cmidrule(l){3-8}
 & & $\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J}\&\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$ \\
\midrule
Baseline~\cite{zhou2022audio} &ResNet-50   &{-}  & {-} & {-} &{78.8}  & {72.8} & {84.8} \\
Baseline$^{*}$~\cite{zhou2022audio}&ResNet-50 &{76.2} &{70.9} &{81.5} &{77.3} &{71.9} &{82.7} \\
\midrule
\multirow{7}*{MUTR}   & ResNet-50 &{82.5}  & {78.3} & {86.7} &{83.0}  & {78.6} & {87.3} \\
& ResNet-101 &{82.5}  & {78.1} & {86.8}  &{83.1}  & {78.5} & {87.6}  \\
  & Swin-L &{85.2}  & {81.2} & {89.1} &{85.7}  & {81.5} & {89.8}  \\
  & Video-Swin-T &{82.9}  & {78.8} & {83.0}  &{83.0}  & {78.7} & {87.2} \\
  & Video-Swin-S &{83.7}  & {79.3} & {88.0}  &{84.1}  & {79.8} & {88.3} \\
  & Video-Swin-B &{85.0}  & {81.1} & {88.9}  &{85.7}  & {81.6} & {89.7} \\
 & ConvMAE-B &{85.5}  & {81.2} & {89.7}  &{86.3}  & {82.2} & {90.3} \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{AV-VOS.} The AVSBench benchmark is split into three subsets, 3,452/740/740 for train/val/test sets, respectively. For verifying the effectiveness and robustness of our models, we still evaluate the performance on val set. The performance is shown in Table~\ref{supp:avsbench}. Note that, the Baseline~\cite{zhou2022audio} method does not publicly provide official weights, so we reproduced it with reference to the original paper settings. 


\begin{figure*}[t!]
  \centering
    \includegraphics[width=\textwidth]{./sup_audio.pdf}
   \caption{\textbf{Qualitative results on AVSBench between Baseline and MUTR.} }
    \label{sup_audio}
\end{figure*}


\section{Additional Visualizations}
\label{D}

\paragraph{Visualizations on Ref-YouTube-VOS.}
In Figure~\ref{sup_text}, we visualize the results compared with ReferFormer~\cite{wu2022language} on Ref-YouTube-VOS benchmark. From Figure~\ref{sup_text}, MUTR can successfully track and segment the referred instance even in challenging situations, where they are surrounded by similar instances (the first row), much deformation (the second row) and background interface (the third row).

\paragraph{Visualizations on AVSBench.} 
In Figure~\ref{sup_audio}, we visualize the results compared with Baseline~\cite{zhou2022audio} on AVSBench benchmark. With temporal consistency (Multi-object Temporal Interaction module), our model can still successfully segment the race car when a person walks past it. Our model can overcome the interference of close proximity to the referred object and similar color textures (Baby Laughter and Driving Bus), thanks in large part to our Multi-scale Temporal Aggregation module. 
\end{document}