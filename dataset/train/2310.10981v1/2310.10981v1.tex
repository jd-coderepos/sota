\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{bm}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{subcaption}



\title{Instructive Dialogue Summarization with Query Aggregations}


\author{
Bin Wang,
\  Zhengyuan Liu,
\  Nancy F. Chen \\
Institute for Infocomm Research (I$^2$R), A*STAR, Singapore\\
\texttt{wang\_bin@i2r.a-star.edu.sg}
}

\begin{document}
\maketitle
\begin{abstract}

    Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering and query-based summary generation. By training a unified model called \textbf{\emph{InstructDS}} (\textbf{Instruct}ive \textbf{D}ialogue \textbf{S}ummarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental results show that our approach outperforms the state-of-the-art models and even models with larger sizes. Additionally, our model exhibits higher generalizability and faithfulness, as confirmed by human subjective evaluations.
    
\end{abstract}

\section{Introduction}

    Both verbal and non-verbal conversations play a crucial role in the realm of communication. They serve as channels for humans to exchange information, ideas and emotions~\cite{kester2004conversation}. In an era of information explosion overload, dialogue summarization has become increasingly essential. The process involves extracting the key dialogue information, enabling people to grasp the essence of their daily interactions. 
    
    Conventional dialogue summarization models typically approach the problem as an unconstrained sequence-to-sequence task, treating dialogue-summary pairs as straightforward input-output pairs~\cite{shang-etal-2018-unsupervised,goo2018abstractive,chen-yang-2020-multi}. Although fine-tuning pre-trained language models such as BART~\cite{lewis-etal-2020-bart} has shown promising results, these models fail to consider the specific preferences of users who have distinct backgrounds, objectives, intents, and applications for the summaries they require.
    
\begin{figure}[t]
        \centering
         \includegraphics[width=0.5\textwidth]{figure1.pdf}
        \caption{Instructive dialogue summarization models, such as InstructDS, demonstrate multiple capabilities.}
        \label{fig:example_dialogue}
    \end{figure}


    In order to address this challenge, several methods have been proposed to integrate queries when generating summaries~\cite{dang2006duc,nema-etal-2017-diversity,su-etal-2021-improve,zhong-etal-2021-qmsum,zhu2022transforming,he-etal-2022-ctrlsum}. However, these models primarily concentrate on domains such as news~\cite{dang2006duc,he-etal-2022-ctrlsum}, Wikipedia~\cite{zhu2022transforming}, and meetings~\cite{zhong-etal-2021-qmsum}. The exploration of query-based summarization for dialogues remains limited. Furthermore, \citet{liu-chen-2021-controllable} propose controllable generation using personal named entity planning, and \citet{wang2022focused} suggest controlling summary conciseness. However, both methods focus on specific aspects of controllability and still lack the flexibility to incorporate user requirements as shown in Figure~\ref{fig:example_dialogue}.

    The primary obstacle in instruction-based dialogue summarization is the scarcity of training data. While existing datasets contain dialogue-summary pairs, creating query-based dialogue summarization datasets with limited human involvement is challenging due to high costs, limited diversity, and potential quality issues. In this work, shed light by Self-Instruct~\cite{selfinstruct}, we propose to synthesize query-dialogue-summary (QDS) triples by leveraging the conditional question generation and answering ability of general large language models (LLMs)~\cite{wei2023overview}. The process involves requesting LLMs to generate multiple candidate queries based on the reference summary. A filtering mechanism, employing text-based and semantic-based methods, is then applied to ensure the quality of collected queries. Finally, the query-based summarization is generated by triggering the question-answering ability of LLMs. This approach demonstrates a promising solution to generate query-based dialogue summarization triples while reducing human involvement and enhancing data diversity.

    The InstructDS framework is shown in Figure~\ref{fig:framework}. Through joint training with QDS triples, InstructDS can cater to user preferences by producing query-based summaries. This mixed training paradigm enhances the model's understanding of dialogue, which improves the factual quality of generated summaries. Our model exhibits superior domain generalizability by incorporating multiple datasets into a unified framework and the user's conciseness requirement can be fulfilled by our length-aware augmentations.

    Our main contributions are summarized as follows:
    \begin{itemize}
        \item We introduce \textbf{\emph{InstructDS}}, the pioneering instruction-following dialogue summarization model. It is a text generation model designed to summarize dialogues while explicitly considering user instructions.
        
        \item We present a straightforward yet effective approach to synthesize query-dialogue-summary triples from dialogue-summary pairs, facilitating query-based dialogue summarization. This method leverages the question generation and answering capabilities of large language models (LLMs). We validate its effectiveness through evaluations conducted by human annotators.
        

        \item We conducted an extensive evaluation on 3 dialogue summarization datasets and 1 dialogue comprehension dataset. The results demonstrate a substantial improvement over previous models. Additionally, according to human subjective test, our generated summaries exhibit comparable levels of factuality, fluency, informativeness, and conciseness to human written ones.

        
    \end{itemize}
    

\begin{figure*}[t]
        \centering
         \includegraphics[width=1.00\textwidth]{framework.pdf}
        \caption{Overall framework of our Instructive Dialogue Summarization (InstructDS) model. 
        }
        \label{fig:framework}
    \end{figure*}



\section{Related Work}

    \subsection{Dialogue Summarization}

        Dialogue summarization is the task of generating a concise and fluent summary of a conversation involving two or more participants. It has gained significant attention due to its broad applications and availability of relevant datasets~\cite{gliwa-etal-2019-samsum,chen-etal-2021-dialogsum,zhao2021todsum}. Solutions on dialogue summarization are mainly based on sequence-to-sequence models including the pointer-generation network~\cite{see-etal-2017-get}, T5~\cite{raffel2020exploring} and BART~\cite{lewis-etal-2020-bart}. However, it remains a challenging task due to the lengthy and unstructured nature of dialogues. \citet{chen-yang-2020-multi} proposes extracting dialogue structures from various perspectives before summarization. Other approaches attempt to incorporate co-reference information~\cite{liu-etal-2021-coreference} and leverage dialogue understanding objectives~\cite{liu-etal-2021-topic-aware} to enhance the factuality and informativeness~\cite{tang-etal-2022-confit,wang-etal-2022-analyzing}.

        
        Similar to text summarization, the process of generating dialogue summarization is uncontrollable and poses challenges in incorporating user preferences~\cite{zhong-etal-2021-qmsum,he-etal-2022-ctrlsum}. Efforts have been made to enhance the controllability of dialogue summarization. However, these approaches often have limited focus on personal named entities~\cite{liu-chen-2021-controllable,liu-chen-2022-entity,wang2022focused} and conciseness~\cite{wang2022focused}. The primary challenge in instructive dialogue summarization lies in the availability of suitable supervision. While QMSum~\cite{zhong-etal-2021-qmsum} introduces the first query-based meeting summarization, it focuses on lengthy meetings and consists of only 232 meeting samples. To address this limitation, we propose a methodology for synthesizing query-dialogue-summary triples leveraging summary-anchored techniques, to facilitate instructive dialogue summarization.

    \subsection{Instruction Tuning}

        Recently, instruction-finetuning on large language models has demonstrated remarkable generalizability towards unseen tasks by leveraging task descriptions~\cite{brown2020language,wang-etal-2022-super,chung2022scaling}. The availability of high-quality and diverse instructions unlocks the emerging capabilities of LLMs. For instance, Flan-series models are tuned with over 1800 tasks with diverse instructions~\cite{chung2022scaling}. However, dialogue tasks, being a sub-domain, have limited supervised data. This limitation leads to sub-optimal performance on query-based dialogue summarization using existing instruction-finetuned models. To mitigate the reliance on human-annotated instruction data, Self-Instruct~\cite{selfinstruct} uses GPT3 for generating diverse instructions and input-output pairs. In a similar vein, our study introduces diverse and high-quality augmentations of query-based dialogue summarization data for instructive dialogue summarization.
        
\section{Instructive Dialogue Summarization} 

    \subsection{Problem Definition}

        Unlike previous dialogue summarization approaches, instructive dialogue summarization involves a controllable generation process where the summary is dependent on both query and dialogue. In non-instructive dialogue summarization, given dialogue-summary pair $\{\bm{D}_{d,i},\bm{S}_{d,i}\}_{i=1}^{p_d}$ from dataset $d$ with $p_d~\geq~1$ pairs of instances, a model $\mathbf{M}$ is expected to generate a summary given the corresponding dialogue: $\mathbf{M}(\bm{D}_{d,i})=\bm{S}_{d,i}$. In instructive dialogue summarization, the focus shifts to structured triples. Given a query-dialogue-summary (QDS) triple $\{\bm{Q}_d,\bm{D}_d,\bm{S_d}\}_{i=1}^{t_d}$ from dataset $d$ with $t_d$ triples, an instructive model $\bm{M}$ should generate summary conditioned on both query and dialogue: $\bm{M}(\bm{Q}_{d,i},\bm{D}_{d,i})=\bm{S}_{d,i}$. Note that the same dialogue can be shared in several QDS triples with different queries.


\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{| l | c | c | c | c | c  c  c |}
        \toprule
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{\# Train} & \multirow{2}{*}{\# Validation} & \multirow{2}{*}{\# Test} & \multirow{2}{*}{\# QDS Triples} & \multicolumn{3}{c|}{Direct Exposure}  \\
        & & & & & \emph{Alpaca} & \emph{Flan-Series} & \emph{InstructDS} \\
        \hline
        SAMSum~\cite{gliwa-etal-2019-samsum} & 14,732 & 818 & 819 & 18,245 & \xmark & \cmark & \cmark  \\
        \hline
        DialogSum~\cite{chen-etal-2021-dialogsum} & 12,460 & 500 & 1,500 & 18,600 & \xmark & \xmark & \cmark \\
        \hline
        TODSum~\cite{zhao2021todsum} & 7,892 & 999 & 999 & 8,705 & \xmark & \xmark & \cmark \\
        \hline \hline
        DREAM~\cite{sun-etal-2019-dream} & 6,116 & 2,040 & 2,041 & - & \xmark & \cmark & \xmark \\ 
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{The dataset statistics include several dialogue summarization datasets such as SAMSum, DialogSum, and TODSum, as well as the DREAM dataset, which focuses on dialogue reading comprehension and contains natural query-dialogue-summary triples. The right part indicates the direct supervision exposures for Alpaca, Flan-Series, and InstructDS models.
        }
        \label{tab:datasets}
    \end{table*}



\begin{table}[htb]
            \centering
            \begin{adjustbox}{width=0.48\textwidth,center}
            \begin{tabular}{ |l c |}
            \toprule
            \textbf{Quality Review Question} & \textbf{Yes}\% (w/o filtering) \\
            \hline \hline
            \texttt{Does the query question answerable?}
            & 94\% (76\%) \\ \hline
            \begin{tabular}{@{}l@{}} 
            \texttt{Is the query differs from previous} \\ 
            \texttt{ones for the same dialogue? } 
            \end{tabular} 
            & 90\% (63\%) \\ \hline
            \begin{tabular}{@{}l@{}} 
            \texttt{Is the generated summary correct and} \\ 
            \texttt{acceptable for query and dialogue?} 
            \end{tabular} 
            & 83\% (71\%) \\
            \hline\hline
            \texttt{Both unique and correct.} & 75\% (45\%)\\
            \bottomrule
            \end{tabular}
            \end{adjustbox}
            \caption{Quality review for the generated queries and summaries from synthesized DQS triples. After (before) filtering results are shown. Examples of both kept and filtered QDS triples can be found in Table~\ref{tab:QDS_examples_SAMSUM},~\ref{tab:QDS_examples_dialogsum}.~\ref{tab:QDS_examples_todsum}.}
            \label{tab:QDS-quality-review}
        \end{table}


    \subsection{Synthesize QDS Triples}
    \label{sec:synthesize_qds_triples}

        The process of generating query-dialogue-summary (QDS) triples from dialogue-summary pairs in our pipeline involves three steps: 1) query generation from complete summary; 2) query filtering to ensure the validity and diversity; 3) query-guided summary generation.
        
        \textbf{Query Generation.} In order to capture a diverse range of potential queries, we deploy the question-generation ability of LLMs to generate multiple candidate queries. Specifically, we use \emph{Flan-T5-XL}~\cite{chung2022scaling} (refer as model $\bm{X}$) which has been trained on question generation datasets such as Quoref~\cite{dasigi-etal-2019-quoref}, MC-TACO~\cite{zhou-etal-2019-going} and CosmosQA~\cite{huang-etal-2019-cosmos}. We expect that its question-generation ability can be generalized to other narrative text. For each instance, we generate five candidate queries using the template shown in Table~\ref{tab:prompt-templates}.

        \textbf{Filtering and Postprocessing.} To ensure validity and diversity, we present two methods for query filtering. 1) \textbf{Text-based filtering.} Through an analysis of candidate queries, we observe that some queries are not answerable conclusively without hallucinations. Examples of such queries include \emph{`What will'} and \emph{`How would'} queries. Therefore, we utilize model $\bm{X}$ as a text-based binary classifier to determine the answerability of queries using the template in Table~\ref{tab:prompt-templates}. This filtering process eliminates around 45\% of generated queries that are likely to be unanswerable. 2) \textbf{Semantic-based filtering.} To avoid redundancy and ensure diversity, we remove similar queries for the same dialogue-summary pair through semantic similarity measurement. For instance, queries such as \emph{`What does Edward think about Bella?'} and \emph{`What does Edward think of Bella?'} are almost identical in meaning. We keep only the first query if the semantic similarity score, computed using normalized BERTScore~~\cite{bert-score}, is above 0.65. The semantic-based filtering process eliminates an additional 50\% of the queries.

        \textbf{Query-based Summary Generation.} Using the query and the complete summary as input, we generate the query-based summary with model $\bm{X}$. It is worth noting that generating query-based summaries from dialogues is challenging for model $\bm{X}$. In contrast, generating query-based summaries from condensed summaries is comparatively easier as it allows the model to extract information from a more concise and structured source, which further guarantees the quality.


        \textbf{Quality Check.} Finally, we collect QDS triples for three dialogue summarization datasets and present statistics in Table~\ref{tab:datasets}. On average, 1.3 QDS triples are generated for each dialogue-summary pair. To access quality and diversity, we enlist help from an expert to annotate 100 triples. Evaluation results in Table~\ref{tab:QDS-quality-review} demonstrate a significant improvement in the quality of synthesized triples after applying our filtering technique, with the quality score increasing from 45\% to 75\%. In the process, we incorporate the summary information as it represents a condense version of the information contained in the corresponding dialogues. The triples gathered are tend to have higher quality with fewer errors and cover more utterances as the comparison shown in Table~\ref{tab:QDS_examples_SAMSUM}.
        

    \subsection{Model Training}
    \label{sec:model_training}

        We perform instruction tuning with \emph{Flan-T5-XL} model as the initial checkpoint. The instructions are three-fold: 1) general dialogue summarization, 2) query-based dialogue summarization and 3) their length-aware augmentations. For query-based dialogue summarization, the query and dialogue are concatenated as the input with the template: ``\emph{\#\#\#Instruction: \{instruction\}. \#\#\# Input: \{dialogue\}}.'', where output is the summary. To account for length-aware generations, we append ``\emph{The generated summary should be around \{summary length\} words long.}'' to the original instruction.


\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{| l c | c  c  c | c  c  c | c  c  c | c | }
        \toprule
        
        \multirow{2}{*}{Models} & \multirow{2}{*}{Params} & \multicolumn{3}{c|}{ROUGE-1} & \multicolumn{3}{c|}{ROUGE-2} & \multicolumn{3}{c|}{ROUGE-L} & \multirow{2}{*}{BS} \\
        \cline{3-11}
         & & $F_1$ & $Pre$ & $Rec$ & $F_1$ & $Pre$ & $Rec$ & $F_1$ & $Pre$ & $Rec$ &  \\
        \hline
        \emph{Pointer-Generator} & - & 40.1 & - & - & 15.3 & - & - & 36.6 & - & - & - \\
        \emph{BART} & 400M & 53.0 & 59.0 & 52.8 & 28.4 & 32.1 & 28.2 & 44.2 & 49.3 & 44.0 & 53.3 \\
        \emph{MV-BART} & 400M & 53.9 & 55.7 & 57.4 & 28.4 & 29.3 & 30.6 & 44.4 & 45.7 & 47.5 & 53.6 \\
        \emph{Coref-BART} & 400M & 53.7 & 56.9 & 56.4 & 28.5 & 30.5 & 29.7 & 44.3 & 46.9 & 46.5 & 53.5 \\
        \emph{ConDigSum} & 400M & \underline{54.3} & 56.0 & 57.6 & 29.3 & 30.4 & 31.2 & 45.2 & 46.6 & 48.0 & \underline{54.0} \\
        \emph{GPT-3-finetune} & 175B$^*$ & 53.4 & - & - & \underline{29.8} & - & - & \underline{45.9} & - & - & -\\
        \hline
        \emph{Alpaca} & 7B & 28.2 & 26.0 & 39.8 & 5.7 & 5.1 & 8.3 & 20.5 & 19.2 & 29.0 & 19.4 \\
        \emph{Flan-T5-XXL} & 11B & 52.6 & 62.6 & 50.0 & 28.5 & 34.1 & 27.1 & 44.1 & 52.5 & 41.9 & 53.2 \\
        \emph{Flan-UL2} & 20B & 53.3 & 60.3 & 52.5 & 28.0 & 32.0 & 27.7 & 44.1 & 50.0 & 43.3 & 53.5 \\
        \emph{ChatGPT} & 175B & 32.7 & 22.4 & 70.2 & 12.3 & 8.4 & 27.1 & 24.7 & 16.9 & 53.6 & 32.5 \\
        \hline
        \emph{InstructDS} & 3B$^*$ & \textbf{55.3} & 58.8 & 57.5 & \textbf{31.3} & 33.5 & 32.6 & \textbf{46.7} & 49.7 & 48.6 & \textbf{55.5} \\ \hline \hline 
        \multicolumn{12}{|l|}{\emph{w/ reference summary length}}\\\hline
        \emph{ChatGPT} & 175B & 40.8 & 39.3 & 43.4 & 13.7 & 13.2 & 14.6 & 31.5 & 30.5 & 33.4 & 40.0 \\
        \emph{InstructDS} & 3B$^*$ & \textbf{58.4} & 58.5 & 58.8 & \textbf{32.8} & 32.9 & 33.0 & \textbf{48.9} & 49.0 & 49.2 & \textbf{58.5} \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{ROUGE scores on SAMSum test set. The results are divided into two blocks: dedicated dialogue summarization models and general-purpose LLMs. ``\emph{w/ reference summary length}'' indicates reference summary lengths are provided in instructions. $^*$ indicates 37.7M trainable parameters.}
        \label{tab:samsum_results}
    \end{table*}


        To enhance the generalizability across different dialogue types, we combine three dialogue summarization datasets to train a unified dialogue summarization model. From the synthesized QDS triples, we random sample 5k triples from each dataset. For length awareness, each sample is augmented once. This results in a total of $(14.7k+12.5k+7.9k+5k\times3)\times2=100k$ samples for training. It is important to note that all experimental results are obtained using the unified model without any specific tuning for individual datasets, which can potentially yield better results but at the cost of reduced generalizability. We employ LORA for parameter efficient training with a total of 37.7 million trainable parameters~\cite{hu2022lora}.


\section{Experiments}

    \subsection{Datasets, Metrics, and Baselines}

        We evaluate and benchmark our method on three dialogue summarization datasets including SAMSum~\cite{gliwa-etal-2019-samsum}, DialogSum~\cite{chen-etal-2021-dialogsum} and TODSum~\cite{zhao2021todsum}. These datasets are equipped with dialogues and human-written or verified summaries. Additionally, we explore dialogue reading comprehension with DREAM dataset~\cite{sun-etal-2019-dream} and we evaluate model accuracy without candidate choice exposures. In other words, we reframe DREAM as an open question-answering generation problem. The generated output is subsequently combined with BERTScore to determine the most possible choice. More details and examples are explained in Section~\ref{appendix:dream_eval}. For evaluation metrics, we include ROUGE\footnote{We utilize the \href{https://pypi.org/project/rouge-score/0.1.2/}{ROUGE-score} package from Google and compared with other implementations in Section~\ref{sec:rouge_implementation}.}~\cite{lin-2004-rouge}, BERTScore~\cite{bert-score}, human evaluation and ChatGPT (GPT-3.5-Turbo-0301) for comprehensive quality assessment.


        Dialogue summarization can be approached using dedicated dialogue summarization models, which are specifically finetuned with in-domain data, as well as general-purpose large language models (LLMs) that have larger model sizes and can perform various tasks by following instructions. The selected benchmarking models are \textbf{Dialogue Summarization Model.} 1) Pointer-Generator~\cite{see-etal-2017-get}, 2) BART~\cite{lewis-etal-2020-bart}, 3) MV-BART~\cite{chen-yang-2020-multi}, 4) Coref-BART~\cite{liu-etal-2021-coreference}, 5) ConDigSum~\cite{liu-etal-2021-topic-aware}, 6) GPT-3-finetuned~\cite{hu2022lora}. \textbf{General LLMs.} 7) FLAN-T5~\cite{chung2022scaling}, 8) FLAN-UL2~\cite{tay2022ul2}, 9) ALPACA, and 10) ChatGPT.

\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=0.82\textwidth,center}
        \begin{tabular}{ | l | c  c  c  c | c  c  c  c | }
        \toprule
        \multirow{2}{*}{Models} & \multicolumn{4}{c|}{DialogSum} & \multicolumn{4}{c|}{TODSum}\\ \cline{2-9}
         & R-1 & R-2 & R-L & BS & R-1 & R-2 & R-L & BS \\
        \hline
        \emph{Alpaca} & 25.5 & 4.9 & 18.8 & 18.0 & 33.6 & 6.9 & 21.8 & 14.6 \\
        \emph{Flan-T5-Large} & 38.8 & 14.4 & 30.9 & 38.7 & 37.3 & 13.4 & 25.3 & 23.6 \\
        \emph{Flan-T5-XXL} & 39.3 & 15.8 & 32.4 & 39.5 & 39.3 & 14.2 & 27.2 & 23.5 \\
        \emph{Flan-UL2} & 40.8 & 16.5 & 33.3 & 40.9 & 41.6 & 14.6 & 27.9 & 24.3 \\
        \emph{ChatGPT} & 38.4 & 12.9 & 29.8 & 38.8 & 39.8 & 11.8 & 24.5 & 24.9 \\
        \emph{BART} & 47.3 & 21.3 & 38.6 & 45.8 & 73.1 & 56.8 & 64.0 & 64.3 \\
        \hline
        \emph{InstructDS} & \textbf{47.8} & \textbf{22.2} & \textbf{39.4} & \textbf{47.0} & \textbf{89.3} & \textbf{78.9} & \textbf{85.4} & \textbf{85.5} \\ 
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{Results on DialogSum and TODSum dataset. BART results are computed from the outputs released by~\citet{chen-etal-2021-dialogsum} and~\citet{zhao2021todsum}.}
        \label{tab:dialogsum_tod}
    \end{table*}



    \subsection{Main Results}
    \label{sec:exp_main_results}

        The performance of different models on the SAMSum dataset is presented in Table~\ref{tab:samsum_results}, providing insights into their capabilities for general dialogue summarization. Notably, InstructDS outperforms others and establish the new SOTA for both ROUGE and BERTScore metrics. In general, dedicated summarization models show better performance because of their optimization specifically for the single task and dataset. In the case of general LLMs, Alpaca demonstrates less promising performance due to its optimization using synthesized instruction data, with limited involvement of dialogue summarization tasks. In contrast, as depicted in Table~\ref{tab:datasets}, FLAN-based models include the SAMSum dataset in their instruction tuning process, resulting in competitive performance. While ChatGPT is renowned for its versatility across various tasks, it is prone to generate lengthy summaries when not constrained by prompts~\cite{qin2023chatgpt}. Therefore, we further experiment with adding the reference summary length to instructions during summary generation. This approach significantly improves the performance of ChatGPT, achieving a balance between precision and recall. Meantime, InstructDS exhibits further performance boost, demonstrating its ability to follow length instructions.        

\begin{table}[t]
        \centering
        \begin{adjustbox}{width=0.42\textwidth,center}
        \begin{tabular}{| l | c |}
        \toprule
        Models & Multi-Choice Acc. \\
        \hline 
        \emph{Random} & 33.3\% \\
        \hline 
        \emph{Alpaca} & 51.3\% \\
        \emph{Flan-T5-Large} & 53.1\% \\ 
        \emph{Flan-T5-XXL} & 58.5\% \\ 
        \emph{Flan-UL2} & 56.8\% \\ 
        \emph{ChatGPT} & \textbf{60.8}\% \\ 
        \hline
        \emph{InstructDS} & 57.8\% \\
         + In-domain & \textbf{65.9}\% \\ 
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{Results on DREAM dataset. InstructDS is not trained with DREAM data. ``+In-domain'' indicates the inclusion of DREAM training data.}
        \label{tab:dream_results}
    \end{table}


        We conduct experiments on query-based dialogue summarization using the DREAM dataset and the results are presented in Table~\ref{tab:dream_results}. InstructDS can achieve comparable performance with Flan-T5 and underperform ChatGPT. It is important to note that InstructDS is only directly trained with synthesized QDS triples from other datasets, whereas FLAN-based models are directly trained with the DREAM data. This demonstrates the effectiveness and generalizability of our synthesized triples. Further, we explored the impact of incorporating in-domain DREAM training data into InstructDS. This resulted in a significant performance boost, surpassing ChatGPT by a considerable margin.
        

\begin{figure*}[t]
        \centering
         \includegraphics[width=1.0\textwidth]{ablation_study_1.pdf}
        \caption{Ablation study on different variants of InstructDS. Averaged ROUGE-1/2/L score is reported for dialogue summarization datasets, including SAMSum, DialogSum and TODSum. Accuracy is computed for DREAM dataset.}
        \label{fig:ablation}
    \end{figure*}




        To access the generalizability of InstructDS, we present results on DialogSum and TODSum datasets in Table~\ref{tab:dialogsum_tod}. InstructDS maintenances its outstanding performance over other models.  
        A significant gap exists between fine-tuned BART model and general-purpose LLMs. In contrast, InstructDS incorporates multiple data sources and augmented QDS triples. This comprehensive dialogue understanding framework contributes to its superior reasoning abilities across diverse dialogue domains and tasks.


    \subsection{Ablation Study}
    \label{sec:ablation}

        To provide insights into the effectiveness of InstructDS, we conduct ablation studies on InstructDS variants to answer two fundamental questions: 1) How do augmented QDS triples contribute to general and query-based dialogue summarization? 2) What is the effect of length awareness augmentation on general and length-controllable dialogue summarization? 

        \textbf{Model variants.} We examine five model variants of InstructDS across four evaluation datasets. Each variant is trained with distinct training data: 1) \textbf{MixDS}: Mixing three dialogue summarization datasets, 2) \textbf{MixDS+QDS}: MixDS and synthesized QDS triples, 3) \textbf{MixDS+Length}: MixDS and length augmentation, 4) \textbf{InstructDS}: MixDS, synthesized QDS triples and length augmentation, 5) \textbf{InstructDS+DREAM}: MixDS, synthesized QDS triples, human-written  QDS triples from DREAM, and length augmentation. All other hyperparameters and evaluation protocols are kept identical for fair comparisons.


\begin{figure}[tb]
            \centering
             \includegraphics[width=0.45\textwidth]{ablation_study_2.pdf}
            \caption{Ablation study on the number of included QDS triples. Performance on SAMSum (left, averaged ROUGE) and DREAM (right, accuracy) datasets are reported.}
            \label{fig:ablation2}
        \end{figure}


        The ablation results are presented in Table~\ref{fig:ablation}. First, on SAMSum and DialogSum datasets, the full InstructDS model demonstrates the best performance. Incorporating both synthesized QDS triples and length augmentation techniques contributes to an overall performance improvement. We attribute these performance boosts on the enhanced dialogue understanding and length awareness capabilities. On DREAM dataset, the inclusion of synthesized QDS triples leads to an improvement in query-based dialogue summarization performance, elevating it from 56.4 to 59.1. Notably, the performance is further enhanced when additional in-domain training data from the DREAM dataset. In TODSum dataset, we observe that augmented QDS triples do not yield better summarization results. However, the utilization of length augmentation techniques does improve the performance. It is because TODSum summaries are more templated, requiring less reasoning. In summary, our findings demonstrate that augmented QDS triples enhance dialogue understanding and reasoning capabilities, enabling query-based dialogue summarization with transferability to other dialogue datasets (such as DREAM). Length augmentation, on the other hand, enables not only length controllability but also improves length awareness in general summarization tasks.


\begin{figure}[tb]
            \centering
             \includegraphics[width=0.45\textwidth]{ablation_study_3.pdf}
            \caption{Ablation study on the percentage of length augmented instances. Performance on standard SAMSum and length-revealed SAMSum are reported. }
            \label{fig:ablation3}
        \end{figure}


\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.0\textwidth,center}
        \begin{tabular}{ | l | c c c c | c c c c | }
        \toprule
        \multirow{2}{*}{\textbf{Models}} & \multicolumn{4}{c|}{\textbf{Human Annotator}} & \multicolumn{4}{c|}{\textbf{ChatGPT}} \\ \cline{2-9}
         & Faithfulness & Fluency & Informativeness & Conciseness & Faithfulness & Fluency & Informativeness & Conciseness  \\
        \hline
        \emph{BART} & 3.85\textsubscript{(1.3)} & 4.36\textsubscript{(0.8)} & 3.22\textsubscript{(1.0)} & 4.30\textsubscript{(0.9)} & 4.22\textsubscript{(1.1)} & 4.80\textsubscript{(0.5)} & 3.37\textsubscript{(1.0)} & 4.93\textsubscript{(0.3)} \\
        \emph{Alpaca} & 3.24\textsubscript{(1.3)} & 3.77\textsubscript{(1.3)} & 3.45\textsubscript{(1.1)} & 3.11\textsubscript{(1.4)} & 3.59\textsubscript{(1.3)} & 4.07\textsubscript{(1.0)} & 3.19\textsubscript{(1.2)} & 4.29\textsubscript{(1.0)} \\
        \emph{Flan-UL2} & 4.00\textsubscript{(1.3)} & 4.38\textsubscript{(0.9)} & 3.03\textsubscript{(1.2)} & 4.29\textsubscript{(1.0)} & 4.45\textsubscript{(0.9)} & 4.78\textsubscript{(0.5)} & 3.52\textsubscript{(1.0)} & 4.91\textsubscript{(0.3)}  \\
        \emph{ChatGPT} & 4.52\textsubscript{(0.9)} & 4.38\textsubscript{(0.9)} & 4.62\textsubscript{(0.6)} & 2.77\textsubscript{(1.4)} & 4.94\textsubscript{(0.3)} & 4.94\textsubscript{(0.2)} & 4.78\textsubscript{(0.4)} & 4.89\textsubscript{(0.3)} \\ 
        \hline
        \emph{Human-written} & 4.34\textsubscript{(1.0)} & 4.54\textsubscript{(0.7)} & 3.58\textsubscript{(1.1)} & 4.36\textsubscript{(0.9)} & 4.49\textsubscript{(0.8)} & 4.81\textsubscript{(0.4)} & 3.74\textsubscript{(1.0)} & 4.95\textsubscript{(0.3)} \\
        \hline
        \emph{InstructDS} & 4.13\textsubscript{(1.1)} & 4.35\textsubscript{(0.8)} & 3.54\textsubscript{(1.0)} & 4.23\textsubscript{(1.0)} & 4.60\textsubscript{(0.8)} & 4.82\textsubscript{(0.4)} & 3.78\textsubscript{(0.9)} & 4.92\textsubscript{(0.3)}  \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
            \caption{Subjective quality evaluation with instances random sampled from SAMSum dataset (30 samples for Human Annotators, 200 samples for ChatGPT.). The Mean and standard deviation of evaluation scores are reported.}
        \label{tab:human_gpt_eval}
    \end{table*}


        \textbf{Number of QDA Triples.} To examine the impact of varying numbers of augmented QDS triples on InstructDS, we conduct experiments on SAMSum and DREAM datasets, as shown in Figure~\ref{fig:ablation2}. Consistent with previous observations, the inclusion of augmented QDS triples guides the model to reason more effectively, resulting in improved performance on general dialogue summarization. Additionally, the results from the DREAM dataset indicate that increasing the number of synthesized QDS triples leads to better performance in query-based summarization. As the number of QDS triples increases, the accuracy improves from 54 to over 58.

        \textbf{Length Augmentation Percentage.} The impact of varying the percentage of added length augmentation samples is shown in Figure~\ref{fig:ablation3}. It reveals that increasing the number of length augmentations can enhance the model's ability to control the generated summary length while a saturation point exists. Meantime, the effect on general summarization is diverse and relatively less consistent. 


    \subsection{Subjective Quality Evaluation}
    \label{sec:quality_evaluation}

        We conduct multidimensional evaluations to access the quality of generated summaries. This involves fine-grained Likert scores (scale 1 to 5, the higher the better) from both human and ChatGPT in four dimensions: Faithfulness, Fluency, Informativeness, and Conciseness~\cite{wang-etal-2022-analyzing,gao2023human}. Evaluations are performed on SAMSum dataset and we randomly sampled 30 instances for human evaluation and 200 instances for ChatGPT evaluation. The user interface and prompt template can be found in Figure~\ref{fig:human_eval_examples} and Table~\ref{tab:prompt-templates}, respectively. Each dialogue was accompanied by one human-written summary and five machine-generated ones. We engaged 12 volunteers, resulting in 792 labeled samples. On average, each dialogue-summary pair receives assessments from 4.4 annotators. The mean and standard deviation of Likert scores are presented in Table~\ref{tab:human_gpt_eval}.

        
        With human annotations, fluency is the best-performing metric. All models, except for Alpaca, demonstrate the ability to generate fluent summaries. Alpaca's relatively poor performance can be attributed to its unsupervised training and limited exposure to dialogue data. For informativeness and conciseness, ChatGPT and Alpaca produce the most informative summaries but receive the lowest conciseness scores. These models tend to generate longer summaries, including elaborate details, indicating a limited understanding of the desired compressiveness. Faithfulness evaluation emerges as a crucial factor in practical applications, where ChatGPT surpasses human performance. This can be attributed to potential inaccuracies in the annotations of the SAMSum dataset~\cite{wang-etal-2022-analyzing,gao2023reference}. ChatGPT's ability to generate detailed content, similar to the concept of Chain-Of-Thought~\cite{wei2022chain}, also contributes to higher faithfulness. Overall, InstructDS achieves comparable performance to human-written summaries in terms of fluency, informativeness, and conciseness. While InstructDS still falls short on human-level faithfulness, it demonstrates noticeable improvements compared to previous models.
        
        
        When using ChatGPT as an off-the-shelf evaluator, we observe that InstructDS is achieving on-pair or better performance over human written summaries on four dimensions. Especially for faithfulness, InstructDS is outperforming all other models except for ChatGPT. However, it is worth noting that ChatGPT exhibits biases towards its own outputs, resulting in potentially inflated evaluation scores. Similar patterns have also been found in other studies that involve ChatGPT evaluation. For example, \citet{zhang2023exploring} shows that ChatGPT always assigns higher scores to its own outputs when solving math problems, leading to significant concerns when using ChatGPT as an evaluator. Furthermore, in this work, we found that ChatGPT is not effective in evaluating the conciseness of summaries, which introduces a noticeable discrepancy compared to human evaluators in this aspect. We think it is because ChatGPT is not aware of the desired conciseness of summaries, which also attributes to the phenomenon that it is generating lengthy summaries. Further explorations are necessary for a robust ChatGPT evaluator in dialogue and other domains~\cite{wang2023large}.

        
\section{Discussions}

    \subsection{Relationship with Query-focused Summarization}

        Query-focused summarization is closely related to our instructive dialogue summarization concept~\cite{vig-etal-2022-exploring}. There are some similarities and differences. An ideal instructive dialogue summarization model should be capable of handling a wide range of instructions when generating summaries. As illustrated in Figure~\ref{fig:framework}, our current model can accommodate general dialogue summarization, query-based dialogue summarization, and dialogue summarization with length control. We anticipate that the range of instructions will be expanded in future research, encompassing diverse sets of instructions and multi-round dialogue summarization scenarios. In the meantime, as a domain-specific model, we anticipate that instructive dialogue summarization could exhibit emergent capabilities as shown in general instruction-tuned LLMs.

    \subsection{Long Dialogue (Meeting) Summarization}

        Summarizing dialogues in meetings, especially long ones, is a challenging task that requires a model capable of processing extended sequences. One promising avenue for research involves expanding our current method for summarizing lengthy dialogues, which can improve query-based meeting summarization and comprehension. Instead of relying on the entire lengthy dialogue, our approach generates queries from reference summaries. This approach addresses the difficulties of using pre-trained language models for long dialogue inputs in meeting scenarios.
        One of the obstacles in meeting summarization is the limited data availability, which limits the model's ability to generalize across different domains. Nevertheless, our method has the potential to alleviate data scarcity issues in the context of summarizing lengthy meetings. Another challenge is that meeting summarization is often associated with transcripts with ASR errors~\cite{jiang-etal-2023-speech}, which effect is under-explored in existing research.
        
\section{Conclusion}

    In this work, we present InstructDS, which is the first instructive dialogue summarization model that excels in both general dialogue summarization and query-based dialogue summarization. InstructDS can generate high-quality query-based summarization tailored to user requirements. This achievement is made possible through a combination of multi-dataset training, synthesized QDS triples and length-awareness augmentations. Experimental results demonstrate that InstructDS establishes new state-of-the-art performance across all four benchmark datasets.
    


\section*{Limitations}

    Dialogue summarization is a label-intensive task that demands substantial supervision and the collection of human-written summaries, which is both challenging and resource-intensive. Moreover, the transferability of annotations across different dialogue domains introduces additional complexity. Therefore, to develop a highly adaptable dialogue summarization model, leveraging unsupervised dialogue data becomes crucial. However, it is worth noting that InstructDS does not incorporate unlabelled dialogue data, leaving room for potential improvement.

    Another important aspect to consider in dialogue data is privacy. The sensitive nature of dialogues can hinder the accessibility and public availability of diverse dialogue datasets. Therefore, future enhancements of InstructDS should address privacy concerns and explore the utilization of advanced learning techniques such as federated learning, which can enable collaborative and privacy-preserving training processes.
    

    Automatically evaluating the quality of dialogue summarization poses significant challenges. Acquiring human annotators for model development is expensive and inefficient. Existing evaluation metrics heavily rely on ROUGE, and ChatGPT has emerged as a newly proposed method for evaluation. As discussed in Section~\ref{sec:quality_evaluation}, it still lacks transparency and robustness. Therefore, there is a pressing need for more effective evaluation methods specifically tailored for dialogue summarization. Multilingual and multicultural evaluation is crucial since dialogues are frequently intertwined with local norms, slang, code-switches and cultural nuances~\cite{wang2023seaeval}.
    

\section*{Acknowledgement}

    This research was supported by funding from the Institute for Infocomm Research (I$^2$R), A*STAR, Singapore. The computational work for this article was partially performed on resources of the National Supercomputing Centre (NSCC), Singapore.


\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Datasets and Metrics}

    In this section, we will provide a detailed explanation of the datasets and metrics that have been utilized in our work.
    
    \subsection{ROUGE Implementation}
    \label{sec:rouge_implementation}

    In the context of dialogue summarization, we observed that different studies incorporate different versions of the ROUGE metric, resulting in varied results. Specifically, we identified three widely used implementations of ROUGE:
    
        \begin{itemize}
            \item \href{https://github.com/andersjo/pyrouge/blob/master/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl}{ROUGE-1.5.5}: It is released by the author of the ROUGE paper. However, this implementation has certain limitations. It requires running the Perl script, which can be inconvenient when integrating it with other projects. Additionally, the last update for this implementation dates back to 2005.
            \item \href{https://pypi.org/project/py-rouge/}{Py-rouge}: A full Python implementation of ROUGE metric. It produces the same result as the Perl implementation. However, the package is not maintained since 2018. Because several dialogue summarization papers use this package, we also report the results using this package in this appendix.
            \item \href{https://github.com/google-research/google-research/tree/master/rouge}{ROUGE-score}: Another Python implementation of ROUGE. It is released and maintained by the Google research team. Recently, HuggingFace incorporate this package into their evaluation metrics package. Because of the influence of Google research and HuggingFace, we expect the package will receive significant attention across various research domains. Consequently, in this paper, we utilize this implementation to calculate the ROUGE score. It is also actively updated, with the latest update in March 2023.
        \end{itemize}
        
        This section presents the main results using the Py-rouge package. The results for SAMSum dataset are presented in Table~\ref{tab:appendix_samsum_results} while the results for DialogSum and TODSum are in Table~\ref{tab:appendix_dialogsum_todsum}. These results align with the patterns and conclusions discussed in Section~\ref{sec:exp_main_results}.

    \subsection{DialogSum Preprocessing}
    \label{sec:appendix_dialoguesum_preprocessing}


        In the original DialogSum paper~\cite{chen-etal-2021-dialogsum}, the authors used \#Person1\#, \#Person2\#, and so on to represent speakers because the original dialogue did not contain speaker information. However, this approach leads to inconsistencies as some names were already present in the original dialogue. To address this issue, we performed additional preprocessing on the data to align its format with SAMSum. Specifically, we employed the prompt template shown in Table~\ref{tab:prompt-templates} to prompt the \emph{FLAN-T5-XL} model to predict the name of the person. We then applied rule-based filtering to determine the appropriateness of the predicted names. This filtering process involved considering factors such as forbidden names labeled by humans, the length of the predicted name, the presence of special symbols, and whether the predicted name has appeared in the original dialogue. If the name did not meet the criteria according to our rule-based identification method, we used \emph{FLAN-T5-XL} again with the template from Table~\ref{tab:prompt-templates} to choose from a pool of ten candidate names, which consisted of five randomly sampled male names and five randomly sampled female names. Simultaneously, the name was correspondingly updated in the reference summary.


        Examples of preprocessed dialogues can be found in Table~\ref{tab:QDS_examples_dialogsum} and Table~\ref{tab:DialogSum_examples1}. To facilitate future research and development, we will make the name-replaced version available for public access.
        

    \subsection{Evaluation on DREAM}
    \label{appendix:dream_eval}

        DREAM~\cite{sun-etal-2019-dream} dataset is introduced for dialogue reading comprehension and the evaluation is designed as multi-choice questions. However, in real-world applications, where information queries are performed on dialogues, it is unlikely to have several candidate answers included as input. Real-world scenarios in dialogue reading comprehension are better represented as unconstrained text generation problems.

        our evaluation of the DREAM dataset is conducted in an unconstrained manner, without providing candidate choices to the model. To assess accuracy, we utilize the BERTScore package to measure the similarity between the generated output and the answer choices, selecting the highest-scoring option as the final answer. The evaluation process is illustrated in Figure~\ref{fig:dream_eval1} and Figure~\ref{fig:dream_eval2}.



\section{Templates and Examples}

    We provide more detailed illustrations of the human evaluation interface, instruction templates, synthesized QDS triples and case studies on model outputs.

    \begin{itemize}
        \item Figure~\ref{fig:human_eval_examples}: the interface used for subjective evaluation for annotators. The annotator is asked to label the quality of summaries in four dimensions.
        
        \item Table~\ref{tab:prompt-templates}: the templates used for several tasks including query generation, text-based filtering, ChatGPT evaluation and the preprocessing on DialogSum dataset.
        \item Table~\ref{tab:QDS_examples_SAMSUM},~\ref{tab:QDS_examples_dialogsum}~and~\ref{tab:QDS_examples_todsum}: shows the synthesized QDS triples including both kept and removed ones. The reason for filtering is also demonstrated.
        \item Table~\ref{tab:SAMSum_examples1},~\ref{tab:SAMSum_examples2},~\ref{tab:DREAM_examples},~\ref{tab:DialogSum_examples1}~and~\ref{tab:TODSum_examples1}: shows case studies on the generated summaries and query-based summaries on all four datasets including SAMSum, DREAM, DialogSum, and TODSum.
    \end{itemize}



\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | c | c |}
        \toprule
        \textbf{Task} & \textbf{Prompt Template} \\ 
        \hline
        \begin{tabular}{@{}c@{}}
        \textbf{Query Generation} \\
        (Sec.~\ref{sec:synthesize_qds_triples}) 
        \end{tabular} 
        & 
        \begin{tabular}{@{}c@{}} 
        \texttt{Generate an answerable and specific question based on the} \\ 
        \texttt{following context. Context: \$\{Summary\} } 
        \end{tabular} 
        \\\hline

        \begin{tabular}{@{}c@{}}
        \textbf{Text-based Filtering} \\
        (Sec.~\ref{sec:synthesize_qds_triples}) 
        \end{tabular} 
        & 
        \begin{tabular}{@{}c@{}} 
        \texttt{Can we get an answer from the context, yes or no?} \\ 
        \texttt{Question: \$\{Question\} Context: \$\{Summary\}} \\ \hline
        \texttt{Is the question fully answerable from the context without } \\ 
        \texttt{any guessing, yes or no? Question: \$\{Question\} Context: \$\{Summary\} } 
        \end{tabular} 
        \\ \hline

        \begin{tabular}{@{}c@{}}
        \textbf{ChatGPT Evaluation} \\
        (Sec.~\ref{sec:quality_evaluation}) 
        \end{tabular} 
        & 
        \begin{tabular}{@{}c@{}} 
        \texttt{Evaluate the quality of the abstractive summary from the dialogue.} \\ 
        \texttt{Please be extremely picky. Rate each summary on four dimensions:} \\
        \texttt{Faithfullness: whether the summary is correct according to dialogue,} \\
        \texttt{Fluency: Whether summary is grammarly correct, Informativeness:} \\
        \texttt{Whether the summary contains all essential information, Conciseness:} \\
        \texttt{Whether the summary is very concise (not verbose). Output should } \\
        \texttt{follow the template: `Faithfulness': value, `Fluency': value} \\ \texttt{`Informativeness': value, `Conciseness': value. You should rate on a} \\
        \texttt{scale from 1 (worst) to 5 (best). Do not give detailed explanations.} \\
        \texttt{Dialogue \$\{Dialogue\}. Summary: \$\{Summary\} }
        \end{tabular} 
        \\ \hline
        \begin{tabular}{@{}c@{}}
        \textbf{DialogSum Preprocessing} \\
        (Sec.~\ref{sec:appendix_dialoguesum_preprocessing}) 
        \end{tabular} 
        &
        \begin{tabular}{@{}c@{}} 
        \texttt{(1) Who is \#Person1\# in the following dialogue? \$\{Dialogue\}} \\ 
        \begin{tabular}{@{}c@{}}
        \texttt{(2) Select on proper name for \#Person1\# from \$\{candidate names\}} \\
        \texttt{in the following dialogue? \$\{Dialogue\}} 
        \end{tabular} 
        \\
        \end{tabular} 
        \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{Prompting templates for QDS triple generation, text-based query filtering, ChatGPT evaluation, and DialogSum preprocessing.}
        \label{tab:prompt-templates}
    \end{table*}




\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=0.90\textwidth,center}
        \begin{tabular}{| l | c  c  c | c  c  c | c  c  c |}
        \toprule
        
        \multirow{2}{*}{Models} & \multicolumn{3}{c}{ROUGE-1} & \multicolumn{3}{c}{ROUGE-2} & \multicolumn{3}{c|}{ROUGE-L} \\
        \cline{2-10}
         & $F_1$ & $Pre$ & $Rec$ & $F_1$ & $Pre$ & $Rec$ & $F_1$ & $Pre$ & $Rec$ \\
        \hline
        \emph{BART} & 53.2 & 59.3 & 53.1 & 28.6 & 32.3 & 28.4 & 50.3 & 54.9 & 49.9 \\
        \emph{MV-BART} & 54.0 & 55.8 & 57.5 & 28.5 & 29.5 & 30.7 & 50.6 & 51.5 & 53.2 \\
        \emph{Coref-BART} & 53.9 & 57.1 & 56.6 & 28.6 & 30.7 & 29.8 & 50.4 & 52.5 & 52.3 \\
        \emph{ConDigSum} & 54.4 & 56.2 & 57.8 & 29.4 & 30.5 & 31.4 & 51.3 & 52.3 & 53.7 \\
        \hline
        \emph{Alpaca} & 28.3 & 26.1 & 40.0 & 5.7 & 5.2 & 8.4 & 26.5 & 24.6 & 34.9 \\
        \emph{Flan-T5-Large} & 51.3 & 58.7 & 50.1 & 25.9 & 29.7 & 25.4 & 48.7 & 54.2 & 47.3 \\
        \emph{Flan-T5-XXL} & 52.8 & 62.8 & 50.2 & 28.6 & 34.2 & 27.3 & 50.2 & 57.8 & 47.7 \\
        \emph{Flan-UL2} & 53.4 & 60.4 & 52.6 & 28.1 & 32.2 & 27.8 & 50.2 & 55.5 & 49.1 \\
        \emph{ChatGPT} & 32.9 & 22.5 & 70.6 & 12.4 & 8.5 & 27.3 & 31.6 & 22.4 & 59.3 \\
        \hline
        \emph{InstructDS} & 55.6 & 59.1 & 57.8 & 31.4 & 33.6 & 32.8 & 52.7 & 55.1 & 54.2 \\ \hline \hline
        \multicolumn{10}{|l|}{\emph{w/ reference summary length}}\\\hline 
        \emph{ChatGPT} & 40.9 & 39.5 & 43.5 & 13.8 & 13.2 & 14.7 & 37.7 & 36.6 & 39.5 \\
        \emph{InstructDS} & 58.6 & 58.7 & 59.0 & 33.0 & 33.1 & 33.2 & 54.5 & 54.6 & 54.8 \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{SAMSum results using Py-rouge package.}
        \label{tab:appendix_samsum_results}
    \end{table*}



\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=0.75\textwidth,center}
        \begin{tabular}{ | l | c  c  c | c  c  c | }
        \toprule
        \multirow{2}{*}{Models} & \multicolumn{3}{c|}{DialogSum} & \multicolumn{3}{c|}{TODSum}\\ \cline{2-7}
         & R-1 & R-2 & R-L & R-1 & R-2 & R-L \\
        \hline
        \emph{Alpaca} & 25.6 & 4.9 & 24.6 & 33.4 & 6.9 & 28.1 \\
        \emph{Flan-T5-Large} & 38.7 & 14.3 & 37.2 & 37.2 & 13.3 & 31.6 \\
        \emph{Flan-T5-XXL} & 39.3 & 15.8 & 38.7 & 39.2 & 14.1 & 33.7 \\
        \emph{Flan-UL2} & 40.7 & 16.5 & 39.6 & 41.5 & 14.6 & 34.4 \\
        \emph{ChatGPT} & 38.4 & 12.8 & 36.3 & 39.6 & 11.7 & 30.8 \\
        \emph{BART} & 47.2 & 21.1 & 44.8 & 73.1 & 56.8 & 68.7 \\
        \hline
        \emph{InstructDS} & 47.7 & 21.8 & 45.3 & 89.2 & 79.8 & 87.5 \\ 
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{DialogSum and TODSum results using Py-rouge package.}
        \label{tab:appendix_dialogsum_todsum}
    \end{table*}



\begin{figure*}[htb]
            \centering
            \includegraphics[width=0.6\textwidth]{human_eval_example.png}
            \caption{An illustration of the user interface for human evaluation of summarization qualities.}
            \label{fig:human_eval_examples}
        \end{figure*}



\begin{figure*}[tb]
        \centering
        \includegraphics[width=0.8\textwidth]{dream_eval_1.pdf}
        \caption{Constrained DREAM evaluation as multi-choices question answering.}
        \label{fig:dream_eval1}
    \end{figure*}



\begin{figure*}[tb]
        \centering
        \includegraphics[width=0.8\textwidth]{dream_eval_2.pdf}
        \caption{Unconstrained DREAM evaluation as open-ended question answering.}
        \label{fig:dream_eval2}
    \end{figure*}



\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l |}
        \toprule
        \multicolumn{1}{|c|}{\textbf{Dialogue (SAMSum)}} \\ 
        \hline
        \texttt{Emma: We are going beach would you like to join in?} \\
        \texttt{Sharol: sure who else is coming?} \\
        \texttt{Emma: its me anna, emily, wendy, kate and you} \\
        \texttt{Sharol: would wendy be ok with me joining..} \\
        \texttt{Emma: i think she would be .. is there any problem between you guys?} \\
        \texttt{Sharol: i think she doesnt like me ... she always try to avoid me...} \\
        \texttt{Emma: really? then i think you should definetly join and sort things with her} \\
        \texttt{Sharol: hmm.. i dont want to be her friend forcefully...} \\
        \texttt{Emma: i know she doesnt dislike you there must b some misunderstanding...} \\ 
        \quad\quad~~~ \texttt{lets meet up and sort out.. be at my place at 11am} \\
        \texttt{Sharol: sure will be there} \\ \hline
        \texttt{General Summary: Sharol is going to go to the beach with Emma, anna, emily,} \\
        \quad\quad\quad\quad\quad\quad\quad\quad~~ \texttt{wendy and kate. Sharol is afraid that wendy doesn't like her. } \\

        \hline \hline
        \textbf{\textcolor{purple}{Dialogue-Guided Question Generation}}\\ 
        \hline
        \texttt{Query: Sharol will be at Emma's place at 11am.} \\
        \texttt{Query: What is Sharol going to do? } \\
        \texttt{Query: What is the problem between Sharol and Kate? } \\
        \texttt{Query: What is Emma going to do? } \\
        \texttt{Query: What will they do next? } \\

        \hline \hline
        \textbf{\textcolor{blue}{Kept QDS Triples}}\\ 
        \hline
        \texttt{Query: Who does Sharol go to the beach with? } \\
        \texttt{Summary: Sharol is going to go to the beach with Emma, Anna, Emily, Wendy and Kate.}\\ \hline
        \texttt{Query: What is Sharol afraid of?}\\
        \texttt{Summary: wendy doesn't like her }\\
        
        \hline \hline
        \textbf{\textcolor{red}{Filtered QDS Triples}} \\ 
        \hline
        \texttt{Query: Why is Sharol afraid Wendy doesn't like her?} \\
        \texttt{Reason for rejection: \textbf{text-based filtering}, the answer may not be answerable.} \\ \hline
        \texttt{Query: Who is Sharol going to the beach with?} \\
        \texttt{Reason for rejection: \textbf{semantic-based filtering}, duplicate question.} \\ \hline
        \texttt{Query: What is Sharol's relationship to Wendy?} \\
        \texttt{Reason for rejection: \textbf{text-based filtering}, the answer may not be answerable.} \\         
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{One example from SAMSum dataset on the synthesized Query-Dialogue-Summary triples.}
        \label{tab:QDS_examples_SAMSUM}
    \end{table*}

    



\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l |}
        \toprule
        \multicolumn{1}{|c|}{\textbf{Dialogue (DialogSum)}} \\ 
        \hline
        \texttt{Byron: Are you busy tomorrow night? I'm going over to the Workers Club for volleyball } \\ 
        \quad\quad\quad~ \texttt{if you'd like to come along.} \\
        \texttt{Jacquenette: Isn't that far away?} \\
        \texttt{Byron: Not really. If you take the No. 3 bus, you can get there in ten minutes. And} \\
        \quad\quad\quad~~ \texttt{if volleyball doesn't interest you, they've got a huge indoor swimming pool, a} \\
        \quad\quad\quad~~ \texttt{weight room, and indoor tracks. It's a great place to meet people. Would you} \\
        \quad\quad\quad~~ \texttt{like to go with me?} \\
        \texttt{Jacquenette: Now that you mentioned it, it would be nice to get away from the computer} \\
        \quad\quad\quad\quad\quad\quad~ \texttt{center for a change, and I really should get some more exercise. Working} \\
        \quad\quad\quad\quad\quad\quad~ \texttt{up a sweat in a ball game or the weight room would be nice. I've got so fat.} \\
        \texttt{Byron: Just look at me! You're not the only one. In high school I had a lot more time to} \\
        \quad\quad\quad~~ \texttt{do sports activities. Now what little spare time you have must be used in doing} \\
        \quad\quad\quad~~ \texttt{exercise. At least the club is open until 12 pm.} \\
        \texttt{Jacquenette: I guess it's worth a visit. Did you say you're going tomorrow night?}\\
        \texttt{Byron: Yeah.} \\
        \texttt{Jacquenette: OK, I'll come, too. How about meeting me in front of the cinema at eight,} \\
        \quad\quad\quad\quad\quad\quad~ \texttt{and we'll catch the bus there.} \\
        \texttt{Byron: Sure, see you then.} \\ \hline
        
        \texttt{General Summary: Lance invites Daffy to play volleyball together tomorrow night. Daffy} \\
        \quad\quad\quad\quad\quad\quad\quad\quad~~ \texttt{realizes it's time for more exercise so Daffy agrees. Lance begins to } \\
        \quad\quad\quad\quad\quad\quad\quad\quad~~ \texttt{talk about how much exercise Lance did in high school and how little} \\
        \quad\quad\quad\quad\quad\quad\quad\quad~~ \texttt{time Lance can put into exercise now. They decide to meet in front of} \\
        \quad\quad\quad\quad\quad\quad\quad\quad~~ \texttt{the cinema.}\\
        
        
        
        \hline \hline
        \textbf{\textcolor{blue}{Kept QDS Triples}}\\ 
        \hline
        \texttt{Query: What do Byron and Jacquenette decide to do?} \\
        \texttt{Summary: They think they have been getting too fat and decide to do more sports.} \\ \hline
        \texttt{Query: What does Byron invite Jacquenette to do tomorrow night?} \\
        \texttt{Summary: go over to the Workers Club for volleyball }\\
        \hline \hline
        \textbf{\textcolor{red}{Filtered QDS Triples}} \\ 
        \hline
        \texttt{Query: What sport do Byron and Jacquenette decide to do? }\\
        \texttt{Reason for rejection: \textbf{semantic-based filtering}, duplicate question.} \\ \hline
        \texttt{Query: What sport do Byron and Jacquenette decide to do more of? }\\
        \texttt{Reason for rejection: \textbf{text-based filtering}, the answer may not be answerable.} \\ \hline
        \texttt{Query: What does Byron invite Jacquenette to do tomorrow night? }\\
        \texttt{Reason for rejection: \textbf{semantic-based filtering}, duplicate question.} \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{One example from DialogSum dataset on the synthesized Query-Dialogue-Summary triples.}
        \label{tab:QDS_examples_dialogsum}
    \end{table*}


\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l |}
        \toprule
        \multicolumn{1}{|c|}{\textbf{Dialogue (TODSum)}} \\ 
        \hline
        \texttt{User: Can you find me a Chinese restaurant please?} \\
        \texttt{Agent: Sure.  There are 17 chinese restaurants in town.  Are you looking for a certain price} \\ 
        \quad\quad\quad~ \texttt{range or location?} \\
        \texttt{User: Yes.  I would like it to be in the expensive price range.} \\
        \texttt{Agent: I have found 5 that fit that description. Is there a particular area you would like?} \\
        \texttt{User: I would like to book a table for 6 at 15:15 on Tuesday.} \\
        \texttt{Agent: I was able to get a table at Hakka in the north. The reference number is 7KW8MVL3 and } \\ \quad\quad\quad~~\texttt{they'll keep the table for 15 minutes. Does that suit you?} \\
        \texttt{User: Yes, thank you. That's all for now. Bye!} \\
        \texttt{Agent: Thank you for contacting us and enjoy your meal at the hakka. Have a great day.} \\
        \hline
        \texttt{General Summary: the user wants to book the restaurant seat for 6 people at 14:15 on tuesday.} \\
        \quad\quad\quad\quad\quad\quad\quad\quad~~ \texttt{the specialty of the restaurant is chinese food and it is expensive. } \\
        
        \hline \hline
        \textbf{\textcolor{blue}{Kept QDS Triples}} \\ 
        \hline
        \texttt{Query: What is the price range of the restaurant?} \\
        \texttt{Summary: the user wants to book the seat for 6 people at 14:15 on tuesday. the specialty of } \\
        \quad\quad\quad\quad~ \texttt{the restaurant is chinese food and it is expensive.} \\ \hline
        \texttt{Query: What type of food does the restaurant serve?} \\
        \texttt{Summary: chinese} \\
        \hline
        \texttt{Query: What type of restaurant is the user interested in?} \\
        \texttt{Summary: chinese}\\
        \hline \hline
        \textbf{\textcolor{red}{Filtered QDS Triples}} \\ 
        \hline
        \texttt{Query: What type of food is the restaurant?} \\
        \texttt{Reason for rejection: \textbf{semantic-based filtering}, duplicate question.} \\ \hline
        \texttt{Query: What type restaurant is the user looking for?} \\
        \texttt{Reason for rejection: \textbf{semantic-based filtering}, duplicate question.} \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{One example from TODSum dataset on the synthesized Query-Dialogue-Summary triples.}
        \label{tab:QDS_examples_todsum}
    \end{table*}





\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l | }
        \toprule
        \multicolumn{1}{|c|}{\textbf{Case \#1 from SAMSum}} \\ \hline
        \textbf{\textcolor{blue}{Dialogue:}} \\ \hline
        
        \texttt{Marsha: Guys, we've planned the trip with John last night as we promised} \\ 
        \texttt{Cynthia: great, thank you for that} \\
        \texttt{Marsha: but of course you have to agree on that} \\
        \texttt{Mohammad: sure, but I really trust you} \\
        \texttt{Gavin: me too} \\
        \texttt{Marsha: so as we decided last time, we will spend a week just on the beach} \\
        \texttt{Marsha: we all wanted some calm, nice place, right?} \\
        \texttt{Gavin: yes!!! } \\
        \texttt{Marsha: John found this little hotel in Nosy Be} \\
        \texttt{Marsha: $<$file$\_$photo$>$ } \\
        \texttt{Marsha: and their website: $<$file$\_$other$>$ } \\
        \texttt{Marsha: it's quite basic the website, but it may actually be a good sign} \\
        \texttt{Gavin: I love it!} \\
        \texttt{Cynthia: it seems just perfect} \\
        \texttt{Marsha: I know} \\
        \texttt{Marsha: it's more expensive than hotels on the mainland} \\
        \texttt{Marsha: but it seems to be the most beautiful place in Madagascar} \\
        \texttt{Mohammad: so let's go there, it's still cheaper than Italy for example} \\
        \texttt{Marsha: exactly!} \\
        \hline \hline
        
        \textbf{Summary - Human Reference} \\ \hline
        \texttt{Marsha and John planned the trip. They will spend a week on the beach with Cynthia, Mohammad} \\ 
        \texttt{and Gavin. They will all stay in a hotel in Nosy Be.} \\ \hline
        
        \textbf{Summary - BART} \\ \hline 
        \texttt{Marsha, Cynthia, Mohammad and Gavin are going to Madagascar. They will spend a week on the } \\ 
        \texttt{beach. John found a hotel in Nosy Be.} \\ \hline \hline
        
        \textbf{Summary - Alpaca} \\ \hline
        \texttt{The group agreed to spend their vacation on a beach in the small hotel John had found inNosiv } \\ 
        \texttt{Be, Madgascar. It was more expensivethan hoteles on main land,but it was the best place according } \\
        \texttt{to the group. They all agreed that it would be worth the money.} \\ \hline \hline
        
        \textbf{Summary - Flan-UL2} \\ \hline
        \texttt{Marsha, Cynthia, Mohammad and Gavin are going to Madagascar. They will spend a week on the beach} \\
        \texttt{in Nosy Be.} \\ \hline \hline
        
        \textbf{Summary - ChatGPT} \\ \hline
        \texttt{Marsha informs the group that John had planned the trip, and they all agreed to spend a week on } \\ 
        \texttt{the beach. She shows them a picture of a hotel in Nosy Be and its website, which they all like} \\
        \texttt{despite it being more expensive than hotels on the mainland. Mohammad mentions that it is still} \\
        \texttt{cheaper than Italy and everyone agrees to go there.} \\ \hline \hline
        
        \textbf{Summary - InstructDS} \\ \hline
        \texttt{Marsha, John, Cynthia, Mohammad and Gavin will spend a week on the beach in Nosy Be. It's more } \\
        \texttt{expensive than hotels on the mainland, but it's still cheaper than Italy.} \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{The first case study from SAMSum dataset and the generated summaries from different models.}
        \label{tab:SAMSum_examples1}
    \end{table*}



\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l | }
        \toprule
        \multicolumn{1}{|c|}{\textbf{Case \#2 from SAMSum}} \\ \hline
        \textbf{\textcolor{blue}{Dialogue:}} \\ \hline
        
        \texttt{Ken: Hi, how are you?} \\
        \texttt{Ang: Just peachy! You?} \\
        \texttt{Ken: I'm okay...} \\
        \texttt{Ang: Just okay? What's wrong?} \\
        \texttt{Ken: Just stressed; work stuff, fighting with Brad, too much going on at mom's.} \\
        \texttt{Ang: Hang in there, it will get better!} \\
        \texttt{Ken: I know, but it's a lot.} \\
        \texttt{Ang: Can I do anything to help?} \\
        \texttt{Ken: You are! Listening to me vent! LOL!} \\
        \texttt{Ang: Are you at least doing anything fun this weekend?} \\
        \texttt{Ken: Show Saturday night, then seeing the grandkids on Sunday at the zoo.} \\
        \texttt{Ang: Sounds great! That will cheer you up!} \\
        \texttt{Ken: Gotta run, work calls. Love you!} \\
        \texttt{Ang: Love you too! Have a fantastic day!} \\
        \texttt{Ken: You too!} \\
        
        \hline \hline
        
        \textbf{Summary - Human Reference} \\ \hline
        \texttt{Ken feels stressed because of work and fighting with Brad. There is also too much going on at mom's.} \\
        \texttt{Ken is going to a show on Saturday night. On Sunday Ken is seeing the grandkids at the zoo.} \\ \hline
        
        \textbf{Summary - BART} \\ \hline 
        \texttt{Ken is stressed, because he has to deal with work issues and family issues. He is going to see a } \\
        \texttt{show on Saturday night and see the grandkids on Sunday.} \\ \hline \hline
        
        \textbf{Summary - Alpaca} \\ \hline
        \texttt{$<$noinput$>$}\\ \hline \hline
        
        \textbf{Summary - Flan-UL2} \\ \hline
        \texttt{Ken is stressed out. He is fighting with Brad and there is a lot going on at mom's. Ken will see } \\ 
        \texttt{Ang this weekend. They will go to zoo with the grandkids on Sunday and see the show on Saturday.}  \\ \hline \hline
        
        \textbf{Summary - ChatGPT} \\ \hline
        \texttt{Ken discusses with Ang about feeling stressed due to work, family issues and too much going on. Ang } \\
        \texttt{offers support and asks if there's anything they can do to help. Ken expresses gratitude for listening} \\
        \texttt{to him vent. They discuss plans for the weekend, including a show on Saturday and seeing grandkids at} \\
        \texttt{the zoo on Sunday. They end the conversation by exchanging love and well wishes. } \\ \hline \hline
        
        \textbf{Summary - InstructDS} \\ \hline
        \texttt{Ken is stressed because of work, fighting with Brad and too much going on at mom's. He will see a show} \\
        \texttt{on Saturday night and see the grandkids on Sunday at the zoo.} \\
        
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{The second case study from SAMSum dataset and the generated summaries from different models.}
        \label{tab:SAMSum_examples2}
    \end{table*}



\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l | }
        \toprule
        \multicolumn{1}{|c|}{\textbf{Case \#1 from DREAM}} \\ \hline
        \textbf{\textcolor{blue}{Dialogue:}} \\ \hline

        \texttt{W: What does the term ``alternative energy source'' mean?} \\
        \texttt{M: When we think of energy or fuel for our homes and cars, we think of petroleum, a fossil fuel } \\
        \quad~ \texttt{processed from oil removed from the ground, of which there is a limited supply. But alternative} \\
        \quad~ \texttt{fuels can be many things. Wind, sun and water can all be used to create fuel.} \\
        \texttt{W: Is the threat of running out of petroleum real?} \\
        \texttt{M: It has taken thousands of years to create the natural stores of petroleum we have available at} \\
        \quad~ \texttt{a much faster rate than it is being produced over time. The real controversy surrounding the } \\
        \quad~ \texttt{amount of petroleum we have is how much we need to keep in reserve for future use. Most experts} \\
        \quad~ \texttt{agree that by around 2025, the amount of petroleum we use will reach a peak. Then production } \\
        \quad~ \texttt{and availability will begin to seriously decline. This is not to say there will be no petroleum } \\
        \quad~ \texttt{at this point. But it'll become very difficult and therefore expensive to extract.} \\
        \texttt{W: Is that the most important reason to develop alternative fuel and energy sources?} \\
        \texttt{M: There are two very clear reasons to do so. One is that whether we have 60 or 600 years of fossil} \\
        \quad~ \texttt{fuels left, we have to find other fuel sources eventually. So the sooner we start, the better } \\
        \quad~ \texttt{off we will be. The other big argument is that when long time, which leads to some long-term} \\
        \quad~ \texttt{negative effects, like global warming and the greenhouse effect.} \\

        

        \hline
        \textbf{\textcolor{blue}{Queries:}} \\ \hline
        \texttt{(1) What do we usually refer to when we talk about energy according to the man?} \\
        \texttt{(2) What do most experts agree on according to the man?} \\
        \texttt{(3) What does the man think we should do now?} \\
        
        \hline \hline
        
        \textbf{Summary - Human Reference} \\ \hline
        \texttt{(1) Fuel refined from oil extracted from underground.} \\
        \texttt{(2) Oil production will begin to decline worldwide by 2025.} \\ 
        \texttt{(3) Start developing alternative fuels.} \\\hline \hline
    
        
        \textbf{Summary - Alpaca} \\ \hline
        \texttt{(1) When we refer energy, according the Man, it usually refers to petrol, natural gas, and other } \\
        \quad\quad \texttt{fossils fuells. These fuell sources are limited and will eventually run out. Alternative fuelles} \\
        \quad\quad \texttt{are being developed to reduce the dependence on these limited fuellers.} \\ 
        \texttt{(2) Most experTS agree THAT by AROUND } \\ 
        \texttt{(3) The man thinks we must develop alternate energy and fuel source as soon as possible because it } \\
        \quad\quad \texttt{will help us reduce our dependence on fossils fuel and limit the negative environmental impacts} \\
        \quad\quad \texttt{caused by burning them.} \\ \hline \hline
        
        \textbf{Summary - Flan-UL2} \\ \hline
        \texttt{(1) petroleum } \\ 
        \texttt{(2) the amount of petroleum we use will reach a peak in 2025 } \\ 
        \texttt{(3) Find alternative energy sources } \\ \hline \hline
        
        \textbf{Summary - ChatGPT} \\ \hline
        \texttt{(1) When we talk about energy according to the man, we usually refer to petroleum, a fossil fuel } \\
        \quad\quad \texttt{processed from oil removed from the ground, of which there is a limited supply.} \\ 
        \texttt{(2) Most experts agree that by around 2025, the amount of petroleum we use will reach a peak and } \\
        \quad\quad \texttt{then production and availability will begin to seriously decline.} \\ 
        \texttt{(3) There is no specific man mentioned in the given context who suggests something to do now.} \\
        \quad\quad \texttt{The conversation is about alternative energy sources and the need to develop them for a} \\
        \quad\quad \texttt{sustainable future.} \\  \hline \hline
        
        \textbf{Summary - InstructDS} \\ \hline
        \texttt{(1) petroleum, a fossil fuel processed from oil removed from the ground, of which there is a} \\
        \quad\quad \texttt{limited supply } \\ 
        \texttt{(2) that by around 2025, the amount of petroleum we use will reach a peak } \\ 
        \texttt{(3) develop alternative fuel and energy sources } \\
        
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{One case study from DREAM dataset and the generated query-based summaries from different models.}
        \label{tab:DREAM_examples}
    \end{table*}




\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l | }
        \toprule
        \multicolumn{1}{|c|}{\textbf{Case \#1 from DialogSum}} \\ \hline
        \textbf{\textcolor{blue}{Dialogue:}} \\ \hline

        \texttt{Whitaker: Hello, so how are we feeling today? } \\
        \texttt{Jill: Things are going well for me, doctor. } \\
        \texttt{Whitaker: Am I correct in thinking that you are here for your annual physical? } \\
        \texttt{Jill: Yes, I am applying for new health insurance, and I need a physical examination to qualify. } \\
        \texttt{Whitaker: Your basic physical exam will include lungs, heart, blood levels, and eyes, ears, and nose.} \\
        \texttt{Jill: I've been having a little trouble breathing. Would you look into that, please? } \\
        \texttt{Whitaker: We can do an allergy test, and later I can send you for an asthma test. } \\
        \texttt{Jill: I would appreciate it. When you give me a blood test, what are you looking for?} \\
        \texttt{Whitaker: I am going to check your cholesterol, blood sugar, and white blood cell count. } \\
        \texttt{Jill: I am expecting the tests to go well. I have been taking good care of myself.} \\
        
        \hline \hline
        
        \textbf{Summary - Human Reference} \\ \hline
        \texttt{Jill is going through an annual physical examination to apply for new health insurance, and Jill asks } \\
        \texttt{Whitaker to look into the breathing.} \\ \hline
        
        \textbf{Summary - BART} \\ \hline 
        \texttt{\#Person2\# (Jill) comes to \#Person1\# (Whitaker) 's  to have a physical examination for new health } \\
        \texttt{insurance and asks for an allergy test and an asthma test.}  \\ \hline \hline
        
        \textbf{Summary - Alpaca} \\ \hline
        \texttt{Doctor and patient discuss annual check-up, including labs, exam, insulin, asthmatic tests, chlolestrol,} \\
        \texttt{sugar and WBC count, allergies, healthy lifestyle, physical and mental health.}\\ \hline \hline
        
        \textbf{Summary - Flan-UL2} \\ \hline
        \texttt{Jill is seeing Whitaker for her annual physical. She needs it to qualify for new health insurance. } \\
        \texttt{Jilll has been having trouble breathing. Whitakker will check her cholesterol, blood sugar, and white} \\
        \texttt{blood cell count.}  \\ \hline \hline
        
        \textbf{Summary - ChatGPT} \\ \hline
        \texttt{Jill visits Whitaker for her annual physical and requests for Asthma test due to breathing problems.} \\
        \texttt{Basic physical exam includes checking of lungs, heart, blood levels, eyes, ears, and nose. Blood test } \\
        \texttt{will include cholesterol, blood sugar, and white blood cell count.} \\ \hline \hline
        
        \textbf{Summary - InstructDS} \\ \hline
        \texttt{Jill comes to Whitaker for Jill's annual physical because Jill needs a physical examination to qualify } \\
        \texttt{for new health insurance. Whitaker will do an allergy test and send Jill for an asthma test.} \\
        
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{One case study from DialogSum dataset and the generated summaries from different models.}
        \label{tab:DialogSum_examples1}
    \end{table*}




\begin{table*}[t]
        \centering
        \begin{adjustbox}{width=1.00\textwidth,center}
        \begin{tabular}{ | l | }
        \toprule
        \multicolumn{1}{|c|}{\textbf{Case \#1 from TODSum}} \\ \hline
        \textbf{\textcolor{blue}{Dialogue:}} \\ \hline

        \texttt{User: Hi, can you help me plan my Cambridge trip? I need a train from London Kings Cross to Cambridge.} \\
        \texttt{Agent: I can help you with that. One leaves friday at 05:17, is that time okay for you?} \\
        \texttt{User: Actually, I need to leave Tuesday after 10:30.} \\
        \texttt{Agent: I have a train that leaves Tuesday at 11:17 that will arrive at 12:08. Would you like to make} \\
        \quad\quad\quad~ \texttt{a reservation?} \\
        \texttt{User: Yes, please make a reservation for 6 people. I'll need the reference number. } \\
        \texttt{Agent: I was able to book your train. Your reference number is F13RS8CP. Is there anything else that} \\
        \quad\quad\quad~ \texttt{you need?} \\
        \texttt{User: I'm also looking for a place to eat. I would like an expensive south indian restaurant.} \\
        \texttt{Agent: It looks like we don't have any of those. Would you like to try something else?} \\
        \texttt{User: Hmm, are you sure? I want something south indian, expensive and in the centre of town.} \\
        \texttt{Agent: I have curry garden for Indian in the centre of town, but no south indian.} \\
        \texttt{User: What about one that serves mediterranean?} \\
        \texttt{Agent: I have two options. I can recommend la mimosa. Would you like me to reserve a table?} \\
        \texttt{User: You said you have two options. What is the other option available?} \\
        \texttt{Agent: Other than the La Mimosa Restaurant, I also have the Shiraz Restaurant. Both are located in the} \\
        \quad\quad\quad~ \texttt{center and are expensive. Which do you prefer? } \\
        \texttt{User: Lets go with the Shiraz please} \\
        \texttt{Agent: Ok perfect, would you like me to book a reservation for you?} \\
        \texttt{User: No. I think I am ok although I would like the postcode.} \\
        \texttt{Agent: The postcode is cb21dp. Is there anything else I can help you with today?} \\
        \texttt{User: No, that is all. Thank you so much.} \\
        \texttt{Agent: Let me know if you need anything else.} \\
        \texttt{User: I don't need anything else. You were a great help. Goodbye!} \\
        \texttt{Agent: Thank you for using our system!} \\


        \hline \hline
        
        \textbf{Summary - Human Reference} \\ \hline
        \texttt{the user wants the agent to help purchase 6 train tickets. this train leaves at 10:30 on tuesday, from } \\
        \texttt{london kings cross to cambridge. the user asks for the postcode of the restaurant. the restaurant is } \\
        \texttt{situated in the centre and has expensive mediterranean food.} \\ \hline
        
        \textbf{Summary - BART} \\ \hline 
        \texttt{user wants the agent to help purchase 6 train tickets. this train leaves at 10:30 on friday, from london} \\
        \texttt{kings cross to cambridge. the user wants to book the restaurant seat for 6 people at 12:15 on tuesday. } \\
        \texttt{the restaurant is located in the centre and the food there is expensive.}  \\ \hline \hline
        
        \textbf{Summary - Alpaca} \\ \hline
        \texttt{The user needs to travel from Kings X to Cambrige and is looking to depart on a Friday at the latest. } \\
        \texttt{The agent is able offer a 5.15am train which is acceptable to the user. They also need accommodation for } \\
        \texttt{the night before and the agent suggests the Curry Garden for Indian food in Cambridge, which they find } \\
        \texttt{acceptable. Finally, the travel agent provides the necessary reference numbers for their reservations.}\\ \hline \hline
        
        \textbf{Summary - Flan-UL2} \\ \hline
        \texttt{Agent helps the user book a train ticket from London Kings Cross to Cambridge. The user also wants to} \\
        \texttt{find an expensive restaurant in the centre of town.}  \\ \hline \hline
        
        \textbf{Summary - ChatGPT} \\ \hline
        \texttt{The user asks for assistance in planning a trip to Cambridge. The agent helps with booking a train and } \\
        \texttt{restaurant reservation with alternatives and provides reference numbers and postcodes. The user } \\
        \texttt{expresses gratitude and ends the conversation.} \\ \hline \hline
        
        \textbf{Summary - InstructDS} \\ \hline
        \texttt{the user wants to book 6 train tickets. this train leaves at 10:30 on tuesday, from london kings cross } \\
        \texttt{to cambridge. the user asks for the postcode of the restaurant. the restaurant offers the expensive} \\
        \texttt{mediterranean dishes, which is located in the centre.} \\
        
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        \caption{One case study from TODSum dataset and the generated summaries from different models.}
        \label{tab:TODSum_examples1}
    \end{table*}



\end{document}
