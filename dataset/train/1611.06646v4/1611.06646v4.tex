In this section we explain the experimental set up and the experimental results which validate the effectiveness of our odd-one () learning. 
We evaluate the usefulness of our odd-one-out learned features on the action classification task. 
Specifically, we use UCF101 and HMDB51 datasets for self-supervised feature learning from video data and then use the features for action classification.

The \textit{UCF101 dataset~\cite{soomro2012ucf101}} is an action recognition dataset of realistic action videos, collected from YouTube, consists of 101 action categories. It has 13,320 videos from 101 diverse action categories. The videos of this dataset is challenging which contains large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background and illumination conditions. It consist of three splits, in which we report the classification performance over all three splits as done in the literature.

The \textit{HMDB51 dataset~\cite{Kuehne2011}} is a generic action classification dataset consists of 6,766 video clips divided into 51 action classes. Videos and actions of this dataset are challenging due to various kinds of camera motions, viewpoints, video quality and occlusions. Following the literature, we use a one-vs-all multi-class classification strategy and report the mean classification accuracy over three standard splits provided by Kuehne~\etal~\cite{Kuehne2011}.

In rest of the sections, we perform several experiments to demonstrate different aspects of odd-one-out learning, network design choices, and the performance of different video-clip encoders when used with  networks. 
\subsection{Default odd-one-out training for videos.}
In this section, we explain the default odd-one-out learning process.
By default, we use questions that consist of six video sequences, five sequences with frames in the correct order and a sequence with frames in a wrong order. 
Each sampled video subsequence consists of six frames sampled according the sampling process described in section~\ref{sec.video}. 
Unless otherwise specified we use the random sampling as the default sampling process. 
We rely on the AlexNet architecture, however, the number of activations in the first fully connected layer is reduced to 128 unless otherwise specified. 
The \emph{sum of difference model} architecture (see section~\ref{sec.iqnetworks}) is our default activation fusion method. 
By default, we use the Dynamic Image~\cite{Bilen2016} as the temporal video-clip encoder. 
Experiments are run for 200 epochs without batch normalization and with a learning rate varying from 0.01 to 0.0001 in logarithmic manner. The batches are composed of 64 questions. 
Each question consist of six sub-videos and each sub-video has six frames. 
The self-supervised network is trained with stochastic gradient descent using MatConvNet~\cite{Vedaldi2015}. We use the first split of UCF101 datasets for training of the odd-one-out networks and also for validation. 
Temporal jittering is used to avoid over-fitting.
\subsection{Fine tuning for action recognition.}
Once we train the odd-one-out network, with default setting, we use that to initialize the supervised training. 
We initialize the fine-tuning network (AlexNet architecture~\cite{Donahue13} with standard 4096 activation at fully connected layers) with the convolutional filter weights obtained from the odd-one-out network. 
The fully connected layers are fine-tuned with a learning rate 10 times larger than the ones used for convolutional layers ( to ) and batches composed of 128 samples. 
Typically, the network takes sub-sequences of length six (six frames) as input (same size used in the the odd-one-out network).
We use temporal jittering and drop out rate of 0.8. 

During final inference, to compute the classification accuracy, we sample all non-overlapping sub-sequences (consists of six frames) and compute the maximum conditional probabilistic estimate per sequence. Mathematically, let us assume that given long video  we have sub-sample  subsequences of size , denoted by  where . 
Therefore the CNN returns the conditional probability of action category  for subsequence  which is denoted by .  
During the final inference, using i.i.d. assumption, the conditional log probability of the class  given video  is obtained by . 
We use the category that returns the maximum log conditional probability as the predicted class for that video.
\subsection{Evaluating sampling types for  learning.}
\begin{table}[t]
\small
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Method 				&  superv. acc.(\%) &  self.sup. acc.(\%)\\ \hline \hline
Random initialization 		&  47.0 		& n/a \\ \hline
-consec. samp. 		&  50.6 		& 27.4\\ \hline
-const. consec. samp. 	&  52.4 		& 29.0\\ \hline
-random sampling 		&  \tb{53.2} 		& \tb{29.6}\\ \hline



\end{tabular}
\end{center}
\caption{Comparing several odd-one-out sampling strategies with the random initialization for video action classification on UCF101 dataset.}
\label{tbl.sampling.di}
\end{table}
The objective of the first experiment is to evaluate the impact of sampling types used for odd-one-out networks. 
In this experiment, we use the default setting for odd-one-out () training for videos and use the default fine-tuning process explained earlier. 
Odd-one-out training is performed only on the training set of the UCF101 datasets first split.
Learned features are used to fine-tune all three splits of the UCF101 separately to evaluate the action classification accuracy.
We compare the three sampling types explained in section~\ref{sec.video}, namely \emph{a)} consecutive sampling, \emph{b)} random sampling and \emph{c)} constrained consecutive sampling.
We also compare our  initialization with the randomly initialized fine-tuning network results.
Results are reported in Table~\ref{tbl.sampling.di}.

As it can be seen from the results obtained in Table~\ref{tbl.sampling.di}, all three initialization methods that uses odd-one-out learning perform better than random initialization on the supervised action classification task. 
Random initialization obtains only 47.0\% over three splits where as  consecutive sampling obtains 50.6\% which is 3.6\% better than random initialization. 
Interestingly, the \emph{constrained consecutive sampling} process obtains better results compared to \emph{consecutive sampling} (52.4\%). 
The \emph{random sapling} process obtains the best results for both supervised and self-supervused tasks. 

The consecutive sampling is more confusing task and therefore the most difficult for the network to solve.
Videos typically have slow motions and in that case it may be difficult to tell apart correct vs incorrect ordering using consecutive sampling.  
With such a confusing task, the network might learn little. Moreover, learning from small motions means focusing on small subtelties and this may not generalize well to large general motion understading. This might be the reason for the poor performance of constrained consecutive sampling compared to random sampling.
After analysing these results, we conclude that odd-one-out learning obtains better features that potentially generalizes for other tasks and applications such as video action classification. 
Secondly, more general  tasks based on random question generations process such as \emph{random sampling} seems to generate more generalizable features.
Therefore, the odd-one-out learning does not need a carefully designed sampling process to learn good video features as apposed to methods such as~\cite{Misra2016}.
\subsection{Capacity of fully connected layers.}
We hypothesize that analogical reasoning tasks such as  learning generates useful features. 
However, if one wants to capture such information in the convolutional filters, perhaps it is better to limit the capacity of the fully connected layers. 
To attain this objective, we have introduced two design choices. 
First, we have reduced the number of activations at the fully connected layers to have only 128 instead of 4096.
Secondly, we use sum of difference (SOD) as the fusion method instead of simply concatenating (CON) the activations in our multi-branch network architecture. 
In this experiment, we evaluate the impact of both these design choices. 
We use the default experiment protocol but now use  with random sampling. 
First, we evaluate the impact of using 128 dimensional activations compared to 4096 using sum of difference model as the fusion method. Results are reported in Table~\ref{tbl.capacity.di}.
\begin{table}[t]
\scriptsize
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Method 		& self-sup. acc. & split- 1 & split 2 & split 3 & mean\\ \hline \hline
-4096-SOD 	& 25.7	&51.7     & 51.4    & 50.9    & 51.3 \\ \hline
-128-SOD	& 29.6	&\tb{54.1}     & \tb{51.9}    & \tb{53.6}    & \tb{53.2} \\ \hline 
-128-CON 	& \tb{33.6}	&49.7     & 50.3    & 50.4    & 50.1 \\ \hline
\end{tabular}
\end{center}
\caption{Comparing the impact of capacity of  networks fully connected layers on feature learning and action classification.}
\label{tbl.capacity.di}
\end{table}
Interestingly, a reduced capacity of 128 activations obtains better results than 4096 dimensional activations for both supervised learning and self-supervised learning. 
When the number of activations is reduced to 128, the self-supervised performance increase from 25.7\% to 29.6\% which is also reflected in supervised task where the supervised action classification performance improve from 51.3\% to 53.2\%.
It is also possible that this is partially, due to lack of over-fitting. 

Secondly, we compare the impact of multi branch fusion using feature concatenation (CON) with the sum of differences (SOD) fusion. 
Results are also reported in Table~\ref{tbl.capacity.di}. 
Now we are comparing -128-SOD with -128-CON.
Interestingly, the feature concatenation obtains the good results compared to sum of difference model for the self-supervised task.
However, the supervised action classification results for CON is not as good as the sum of difference (SOD) method. 
Even if sum of difference method (128-SOD) has relatively poor performance on the self-supervised task (29.6 compared to 33.6), intuitively it has the ability to push down the abstractions about analogical reasoning to the convolutional filters.
Therefore, the sum of difference model learns a better \emph{feature representations} in the expense of slight performance degradation on the task that it solves when used with odd-one-out learning.
\subsection{How big the  questions should be?}
In this experiment we evaluate the impact of  learning using different number of elements in each  question. We use the default experimental protocol with random sampling and train network with 2, 4, 6, 8 and 10 elements (subsequences) in each of the question and report the supervised and unsupervised performance on the validation set. Note that the self-supervised task is only trained on the split 1 of UCF101 dataset. Self-supervised task evaluated on the validation set of UCF101 split 1. Results are reported on Table~\ref{tbl.num.questions}. Note that  method with two elements reduces to what is similar to sequence verification method~\cite{Misra2016}.
\begin{table}[t]
\scriptsize
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Nq. 	& self.sup.acc. 	& split 1 & split 2 & split 3 & mean\\ \hline \hline
2	& \tb{73.0} 	& 49.3   	& 49.3    	& 49.4    & 49.3  	\\ \hline 
4	& 43.6 		& 52.1   	& 51.5    	& 51.3    & 51.6 	\\ \hline 
6	& 29.6 		& 54.1   	& 51.9    	& \tb{53.6}    & \tb{53.2} 	\\ \hline
8	& 21.3 		& \tb{54.5}   	& 52.5    	& 52.3    & 53.1	\\ \hline
10	& 16.6 		& 52.6   	& \tb{52.7}     & 53.2    & 52.8 	\\ \hline 
\end{tabular}
\end{center}
\caption{Impact of number of question (Nq.) on  learning on UCF101 dataset.}
\label{tbl.num.questions}
\end{table}
As it can be seen from the results in Table~\ref{tbl.num.questions}, as we increase the number of elements in the  question, the unsupervised task becomes harder. As a result, the unsupervised classification accuracy decreases. However, it is interesting to see that  task with two elements obtains only 49.3\% accuracy on the supervised classification task. However, with the increment of elements in each question, it tends to obtain better results for supervised classification task. On average best supervised results are obtained for a  question consist of six elements (\ie five related correct subsequences and one odd wrong subsequence). Results suggests as the task becomes very difficult (8 and 10 elements), the supervised results saturate and starts to decrease. 
This is because when tackling a very ambiguousand hard tasks the network may learn very little because it is not able to solve it which is also reflected in the poor performance.
When the task is too easy to solve, the network might also not learn much ( question size of 2). In Table~\ref{tbl.num.questions}, we see this effect.
\subsection{Video-clip encoding methods.}
In this section we compare the impact of  learning using three video-clip encoding methods discussed in the section~\ref{sec.dynamic}. 
We evaluate the sum of difference of frames (Sum-of-diff.) video-clip encoding, with the Dynamic Image~\cite{Bilen2016} encoding, and  
the stacking of sum of difference of frames (Stck.-of-diff.) video-clip encoder.
We compare the results for action recognition using UCF101 and HMDB51 datasets on Table~\ref{tbl.dynamic}.

For UCF101 (Table~\ref{tbl.dynamic}) the random initialization using the (Sum-of-diff.) video-clip encoding method (in section~\ref{sec.dynamic}) obtains only 43.4\% while for the same video-clip encoder, the  initialization obtains 54.3\% which is a significant improvement of 10.9\%.
Random initialization of dynamic images obtains better results than the sum of difference random initialization.
However, with  learning, the obtained results for dynamic images is 1.1\% worse than the sum of difference method.
Most interestingly, the stacking of difference of frames obtains the best results for both random initialization and  initialization.
Using  learning we improve the random initialization results for all three video-clip encoder methods with improvements of 10.9\%, 6.2\% and 9.8\% indicating the advantage of  learning for video representation learning. Similar trend can be seen for HMDB51 dataset as well (see Table~\ref{tbl.dynamic}). 
When the network is intialized with ImageNet pretrained models, we obtain 64.9 \%, 67.2 \%, 70.1 \% for sum-of-difference video-clip encoding, dynamic images and stack of difference methods respectively on UCF101.
\begin{table}[t]
\scriptsize
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Method 				& UCF101 & HMDB51 \\ \hline \hline
Rand Init. - Sum-of-diff.	& 43.4 & 21.8\\ \hline
 - Sum-of-diff.		& \ti{54.3} & \ti{25.9}\\ \hline \hline
Rand Init. - Dynamic image.	& 47.0 & 22.3\\ \hline
 - Dynamic image 		& \ti{53.2} & \ti{26.0}\\ \hline  \hline
Rand Init. - Stck.-of-diff. 	& 50.2 & 28.5 \\ \hline 
 - Stck.-of-diff. 		&  \tb{60.0} & \tb{32.4}\\ \hline
\end{tabular}
\end{center}
\caption{Impact of several video-clip encoder methods for odd-one-out learning using UCF101 dataset and HMDB51 over three splits.}
\label{tbl.dynamic}
\end{table}
\subsection{Visualizing the learned network filters}
In this section we visualize some of the network filters learned from sum of difference video-clip encoder (Fig.~\ref{fig:filters}(a)), Dynamic Image video-clip encoder (Fig.~\ref{fig:filters}(b)) and stack of difference of frames video-clip encoder (Fig.~\ref{fig:filters}(c)). It is not surprising that the learned filters for sum of difference video-clip encoder (Fig.~\ref{fig:filters}(a)) and Dynamic Image video-clip encoder (Fig.~\ref{fig:filters}(b)) is some what similar. Interestingly, the stack of difference of frames video-clip encoder (Fig.~\ref{fig:filters}(c)) has totally different set of filters (note we are visualizing only the mean of the first 5 depths of the conv. filters as the Red channel, the next 5 depths of the filters as Green channel and the last 5 as the Blue channel). All filters, are obtained using six frame clips and using six elements per question which is the default 03N setting.
\begin{figure*}[t]
  \centering
  \includegraphics[width=0.30\linewidth]{difference_of_frames_filters}  
  \includegraphics[width=0.30\linewidth]{dynamic_image_filters} 
  \includegraphics[width=0.30\linewidth]{stack_Of_difference_filters}
  \caption{Visualizing the learned first convolutional filter weights for (a) sum of difference video-clip encoder (b) Dynamic Image video-clip encoder (c) Stack of difference video-clip encoder.}
  \label{fig:filters}
\end{figure*}
\subsection{Comparing with state-of-the-art.}
\label{sec.soa}
In this section, we compare our -based self supervised results with the other state-of-the-art self-supervised methods. Specifically, we compare with DrLim~\cite{Hadsell2006}, TempCoh~\cite{Mobahi2009}, Obj. Patch~\cite{Wang2015} and Seq.Ver~\cite{Misra2016}. 
Results are reported in Table~\ref{tbl.soa}. Note that we use only the split 1 of UCF101 and HMDB51 so that we can compare with other published results~\cite{Misra2016}.
As it can be seen from the results, our  learning-based features obtains score almost 10\% higher in UCF101 than the second best method reported in the literature~\cite{Misra2016} that relies on sequential verification. Similarly, we obtain massive improvement of 12.7\% for HMDB51 dataset over~\cite{Misra2016}.

It should be noted that when relying on deep architectures pretrained on supervised datasets, like Imagenet~\cite{Deng2009}, the state-of-the art reaches about 94.2\% (\cite{Wang2016}) using optical flow, improved trajectory features and RGB data on UCF101. 
These accuracies from the state-of-the art action recognition methods are always obtained with the of inclusion of several other modalities, such as optical flow, as well as on massive supervised datasets like ImageNet~\cite{Deng2009}.
With the obtained results, we show some promising directions in self-supervised learning for video data, which contribute towards self-supervised deep networks that could be alternatives to fully supervised or semi-supervised networks in the future.
\begin{table}[t]
\scriptsize
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Method 			  & UCF101-split1 	& HMDB51-split1\\ \hline \hline
DrLim~\cite{Hadsell2006}  &     45.7 		&   16.3  	 \\ \hline
TempCoh~\cite{Mobahi2009} &     45.4 		&   15.9  	  \\ \hline 
Obj. Patch~\cite{Wang2015}&     40.7 		&   15.6   	  \\ \hline
Seq. Ver.~\cite{Misra2016}&     50.9 		&   19.8   	  \\ \hline 
Our - Stack-of-Diff.	  &     \textbf{60.3}	&      \textbf{32.5} 	  \\ \hline  \hline
Rand weights - Stack-of-Diff. &     51.3		& 28.3 \\ \hline
ImageNet weights - Stack-of-Diff. &     70.1		& 40.8 \\ \hline
\end{tabular}
\end{center}
\caption{Comparing with other state-of-the-art self-supervised learning methods for action classification using UCF101 and HMDB51 datasets.}
\label{tbl.soa}
\end{table}