\section{Experiments}
\label{experiment}
In this section, we first explain the experiment datasets and implementation details of the proposed method. Then extensive ablation study is conducted to analyze the effectiveness of the proposed SD block and hidden state adaptation module. Furthermore, the proposed method is compared with state-of-the-art video super-resolution methods in terms of both effectiveness and efficiency.

\subsection{Implementation Details}
\paragraph{\textbf{Datasets.}}
Some works~\cite{sajjadi2018frame,yi2019progressive} collect private training data from youtube on their own, which is not suitable for fair comparison with other methods. In this work, we adopt a widely used video processing dataset Vimeo-90K to train video super-resolution models. Vimeo-90K is a recent proposed large dataset for video processing tasks, which contains about  7-frame video clips with various motions and diverse scenes. About  video clips select out of  as the test set, termed as Vimeo-90K-T. To train our model, we crop patches of size  from HR video sequences as the target. Similar to~\cite{jo2018deep,sajjadi2018frame,Fuoli-arxiv19-rlsp,yi2019progressive}, the corresponding low-resolution patches are obtained by applying Gaussian blur with  to the target patches followed by  times downsampling.

To validate the effectiveness of the proposed method, we evaluate our models on several popular benchmark datasets, including Vimeo-90K-T~\cite{xue2019video}, Vid4~\cite{liu2013bayesian} and UDM10~\cite{yi2019progressive}. As mentioned above, Vimeo-90K-T contains a lot of video clips, but each clip has only 7 frames. Vid4 and UDM10 are long sequences with diverse scenes, which is suitable to evaluate the effectiveness of recurrent-based method in information accumulation~\cite{sajjadi2018frame,Fuoli-arxiv19-rlsp}.


{\flushleft{\textbf{Training Details.}}}
The base model of our method consists of 5 SD blocks where each convolutional layer has 128 channels,~\ie, RSDN 5-128. By adding more SD blocks, we can obtain RSDN 7-128 and RSDN 9-128. The performance can be further boosted with only small increase on computational cost and runtime. We adopt  for HSA module for efficiency. 
To fully utilize all given frames, we pad each sequence by reflecting the second frame at the beginning of the sequence.
When dealing with the first frame of a sequence, the previous estimated detail , structure  and hidden state feature  are all initialized with zeros. 
The model training is supervised with Charbonnier penalty loss function and is optimized with Adam optimizer~\cite{kingma2014adam} with  and . 
Each mini-batch consists of  samples. The learning rate is initially set to  and is later down-scaled by a factor of 0.1 every  epoch till  epochs. The training data is augmented by standard flipping and rotating. All experiments are conducted on a server with Python 3.6.4, PyTorch 1.1 and Nvidia Tesla V100 GPU. 

\begin{table}[t]
	\centering
	\scalebox{0.67}{	
		\begin{tabular}{c||c|c||c|c|c||c|c|c}
\hline
			\textbf{Method} & \multicolumn{2}{c||}{\begin{tabular}{@{}c@{}} One Stream\\7-256 \end{tabular}}   & \multicolumn{3}{c||}{\begin{tabular}{@{}c@{}}Two Stream\\7-128 \end{tabular}} & \multicolumn{3}{c}{\begin{tabular}{@{}c@{}} SD Block\\7-128 \end{tabular}}   \\ \hline \hline
			\textbf{Model} & Model 1 &Model 2   & Model 3  &Model 4  &Model 5  &Model 6  &Model 7  &Model 8  \\ 
			\textbf{HSA?} & w/o & w/ & w/  & w/o &w/  &w/ &w/o & w/ \\ 
			\textbf{Input}& Image &Image  &Image &S~\&~D &S~\&~D  &Image &S~\&~D  &S~\&~D 
			 \\ \hline
			PSNR/SSIM &27.58/0.8410  &27.65/0.8444   &27.70/0.8452 &27.64/0.8404  &27.68/0.8429   &27.73/0.8460  &27.76/0.8463 &{27.79}/{0.8474} \\ \hline
		\end{tabular}
	}
	\vspace{3mm}
	\caption{Ablation study on different network architecture.}
	\vspace{-5mm}
	\label{table:block}
\end{table}


{\flushleft{\textbf{Recurrent Unit.}}}
We compare three kinds of blocks for information flow in the recurrent unit,~\ie, the three blocks shown in Fig.~\ref{fig:SD_block}. 
For fair comparison among these blocks, we keep these three networks with almost the same parameters by setting the channel of convolutional layers in model 1 to 256, and setting the one in model 4 and 7 to 128.
\subsection{Ablation Study}
In this section, we conduct several ablation studies to analyze the effectiveness of the proposed SD block and the hidden state adaptation module. In addition, we also investigate the influence of different supervision on structure and detail components on the reconstruction performance.
\begin{wrapfigure}{R}{0.6\textwidth}
  \vspace{-30pt}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{img/model_ablated.jpg}	
  \end{center}  
  \vspace{-5mm}
 \caption{ Qualitative comparison between different network structures. Zoom in to see better visualization.}
  \label{fig:model_ablated}
  \vspace{-3mm}
\end{wrapfigure}
As shown in Tab.~\ref{table:block}, model 1 and model 4 achieves similar performance, with model 1 a little higher SSIM and model 4 a little higher PSNR. This implies that simply dividing the input into structure and detail components and processing each one individually does not work well. Although it seems that having two branches to process each component divides a difficult task into two easier ones, it makes each one blind to the other and can not make full use of the information in the input to reconstruct either component.

By introducing information exchange between structure and detail components, model 7 obtains better performance than model 1 and 4 in both PSNR and SSIM. Similar result can also found in comparison among model 2, 5 and 8. 
In addition, we experiment with taking the whole frames as input of both branches, that is, model 3 and model 6. By comparing model 3 and model 5 (and also model 6 and model 8), we show that the improvement comes not only from architecture of the two-stream block itself but also indeed from the decomposition into structure and detail components. The network with the proposed SD block allows each branch to explicitly focus on reconstructing a single component, which is easier than reconstructing a mixture of multiple components. Each branch makes use of the other one such that it can obtain enough information to reconstruct the high-resolution version for that component. The advantage of the proposed SD blocks can also be observed in the qualitative comparison as shown in Fig.~\ref{fig:model_ablated}.

In addition, we show in Tab.~\ref{table:block} that each model can gain further boost in performance with the proposed HSA module, about 0.04 dB in PSNR and 0.002 in SSIM on average. This module does not only work for the proposed network with SD blocks but also helps improve the performance for the ones with one-stream and two-stream residual blocks. The hidden state adaptation module allows the model to selectively use the history information stored in hidden state, which makes it robust to appearance change and error accumulation to some extent.

\begin{table}[t]
	\centering
	\scalebox{0.8}{	
		\begin{tabular}{c|c|c|c|c}
			\hline
			\multirow1{*}{\textbf{}}	
		     & &  &   &    \\ \hline \hline
			PSNR/SSIM  &27.56/0.8440 &27.77/0.8459 &27.73/0.8453  &27.79/0.8474
			\\	
			\hline
		\end{tabular}
	}
	\vspace{3mm}
	\caption{Ablation study on influence of different loss items.}
	\vspace{-5mm}
	\label{table:weights}
\end{table}
{\flushleft{\textbf{Influence of different components.}}}
The above experiment shows that decomposing the input into two components and processing them with the proposed SD blocks brings much improvement. We also investigate the relative importance of these two components by imposing different levels of supervision on the reconstruction of two components. It implies that the relative supervision strength applied to different components also plays an important role in the super-resolution performance. As shown in Tab.~\ref{table:weights}, when the weights for structure component, detail component and the whole frame are set to , it achieves a good performance of / in PSNR/SSIM. The performance degrades when the weigh for structure component more than the weight for detail component (\ie ), and verse vise (\ie ). The result of  is dB lower than that of , which means applying additional supervision on the combined image helps the training of the model. 

\begin{table}[t]
	\centering
	\scalebox{0.625}{	
		\begin{tabular}{l||c|c|c||c|c|c|c|c||c}
			\hline
			Vid4  &\#Frame &FLOPs &\#Param.  &Calendar (Y) &City (Y) &Foliage (Y)  &Walk (Y) &Average (Y)  &Average (RGB)  
			\\
			\hline \hline  	
			Bicubic & 1 &N/A &N/A &18.83/0.4936  &23.84/0.5234	&21.52/0.4438  &23.01/0.7096  & 21.80/0.5426   &20.37/0.5106	
			\\	
			SPMC ~\cite{tao2017detail}  & 3 & - & -   &~~-/-~~~~ &~~-/-~~~~ &~~-/-~~~~ &~~-/-~~~~ &25.52/0.76~~~ &-/-
			\\
			Liu~\cite{liu2017robust} & 5 &- &- &21.61/-~~~~~~~ &26.29/-~~~~~~~ &24.99/-~~~~~~~ &28.06/-~~~~~~~ &25.23/-~~~~~~~ &-/-
			\\
			TOFlow~\cite{xue2019video} & 7 &0.81T &1.41M &22.29/0.7273  &26.79/0.7446 &25.31/0.7118 &29.02/0.8799 &25.85/0.7659 &24.39/0.7438	
			\\
			DUF-52L~\cite{jo2018deep} & 7  &0.62T &5.82M  &24.17/0.8161 &28.05/0.8235 &26.42/0.7758 &30.91/0.9165	&27.38/0.8329 &25.91/0.8166
			\\
			RBPN~\cite{haris2019recurrent} & 7 &9.30T &12.2M &24.02/0.8088 &27.83/0.8045 &26.21/0.7579 &30.62/0.9111 &27.17/0.8205  &25.65/0.7997 
			\\
			EDVR-L~\cite{wang2019edvr} & 7 &0.93T &20.6M  &24.05/0.8147 &28.00/0.8122	 &26.34/0.7635  &{\color{blue}31.02}/0.9152  &27.35/0.8264  &25.83/0.8077   	
			\\	
			PFNL~\cite{yi2019progressive} &7 &0.70T &3.00M  &23.56/0.8232 &28.11/0.8366 &26.42/0.7761 &30.55/0.9103 &27.16/0.8365 &25.67/0.8189 
			\\
			TGA~\cite{isobe2020video} &7 &0.23T &5.87M  &{\color{blue}24.50}/0.8285 &28.50/0.8442 &26.59/0.7795 &30.96/0.9171 &27.63/0.8423 &26.14/0.8258
			\\
			FRVSR 10-128~\cite{sajjadi2018frame} &recurrent (2) &0.14T &5.05M &22.67/0.7844  &27.70/0.8063 &25.83/0.7541   &29.72/0.8971 &26.48/0.8104 &25.01/0.7917
			\\
			RLSP 7-256~\cite{Fuoli-arxiv19-rlsp} &recurrent (3) &0.09T &4.21M  &24.36/0.8235  &28.22/0.8362 &26.66/0.7821 &30.71/0.9134 &27.48/0.8388 &25.69/0.8153
			\\ 	\hline \hline 
			RSDN 5-128 & recurrent (2)  &0.08T &3.83M &24.34/0.8242 &28.73/0.8374  &26.66/0.7842 &30.73/0.9149   &27.61/0.8402 &26.13/0.8238
			\\
			RSDN 7-128 & recurrent (2)  &0.10T &5.01M &24.46/{\color{blue}0.8305}	 &{\color{blue}29.01}/{\color{blue}0.8480}  &{\color{blue}26.78}/{\color{blue}0.7921}  &30.92/{\color{blue}0.9189}   &{\color{blue}27.79}/{\color{blue}0.8474}  &{\color{blue}26.30}/{\color{blue}0.8314}
			\\
			RSDN 9-128 & recurrent (2)  &0.13T &6.19M &{\color{red}24.60}/{\color{red}0.8355}	 & {\color{red}29.20}/{\color{red}0.8527}  &{\color{red}26.84}/{\color{red}0.7931}  &{\color{red}31.04}/{\color{red}0.9210} &{\color{red}27.92}/{\color{red}0.8505}  &{\color{red}26.43}/{\color{red}0.8349}
			\\
			\hline
		\end{tabular}
	}
	\vspace{3mm}
	\caption{Quantitative comparison (PSNR (dB) and SSIM) on \textbf{Vid4} for  video super-resolution. {\color{red}Red} text indicates the best and {\color{blue} blue} text indicates the second best performance. Y and RGB indicate the luminance and RGB channels, respectively. FLOPs (MAC) are calculated on an HR image of size 720480. `' means the values are either taken from paper or calculated using provided models. } 
	\label{vid4_table}
\end{table}


\begin{table}[t]
	\centering
	\scalebox{0.42}
	{	
		\begin{tabular}{l||c|c|c|c|c||c|c||c|c}
			\hline   \hline  	
			\textbf{UDM10} &~~~~~~~\textbf{Bicubic}~~~~~~~  &~~~\textbf{TOFlow}~\cite{xue2019video}~~~ &~~~~\textbf{DUF-52L}~\cite{jo2018deep}~~~~   &~~~~~~\textbf{RBPN}~\cite{haris2019recurrent}~~~~~~ &~~~~\textbf{PFNL}~\cite{yi2019progressive}~~~~ &\textbf{FRVSR 10-128}~\cite{sajjadi2018frame} &~~\textbf{RLSP 7-256}~\cite{Fuoli-arxiv19-rlsp}~~&~~~~\textbf{RSDN 7-128}~~~~  &~~~~\textbf{RSDN 9-128}~~~~ 
			\\\hline
			FLOPs  [TMAC]   &N/A &2.17 &1.65 &24.81 &1.88  &0.36 &0.24 &0.28 &0.35 
			\\
			Runtime [ms]  &N/A &1693 &1413 &3567 &295  &137 &49 &79 &94
			\\ 
			Average (Y) &28.47/0.8523 &36.26/0.9438 &38.48/0.9605 &38.66/0.9596 &38.74/0.9627 &37.09/0.9522  &38.48/0.9606 &{\color{blue}39.13}/{\color{blue}0.9645} &{\color{red}39.35}/{\color{red}0.9653}
			\\
			Average (RGB)  &27.05/0.8267 &34.46/0.9298  &36.78/0.9514 &36.53/0.9462 &36.78/0.9514 &35.39/0.9403 &36.39/0.9465 &{\color{blue}37.26}/{\color{blue}0.9548} &{\color{red}37.46}/{\color{red}0.9557}
			\\ \hline  \hline
			\textbf{Vimeo-90K-T} &\textbf{Bicubic} &\textbf{TOFlow}~\cite{xue2019video} &\textbf{DUF-52L}~\cite{jo2018deep}  &\textbf{RBPN}~\cite{haris2019recurrent}~~ &\textbf{EDVR-L}~\cite{wang2019edvr}  &\textbf{FRVSR 10-128}~\cite{sajjadi2018frame} &~~\textbf{RLSP 7-256}~\cite{Fuoli-arxiv19-rlsp} &~~~~\textbf{RSDN 7-128}~~~~  &~~~~\textbf{RSDN 9-128}~~~~
			\\ \hline 
			FLOPs [TMAC]  &N/A &0.27 &0.20 &3.08 &0.30  &0.04 &0.03 &0.03 &0.04
			\\
			Runtime [ms] &N/A &215 &167 &470 &99 &28 &11 &13 &15
			\\ 
			Average (Y)  &31.30/0.8687 &34.62/0.9212   &36.87/0.9447 &37.20/0.9458  & {\color{red} 37.61}/{\color{red}0.9489}  &35.64/0.9319   &36.49/0.9403 & 37.05/0.9454 &{\color{blue}37.23}/{\color{blue}0.9471}
			\\
			Average (RGB) &29.77/0.8490 &32.78/0.9040  &34.96/0.9313 &{\color{blue}35.39}/0.9340 &{\color{red} 35.79}/{\color{red} 0.9374} &33.96/0.9192  & 34.56/0.9274 &35.14/0.9325 &35.32/{\color{blue}0.9344}
			\\ \hline
		\end{tabular}
	}
	\vspace{3mm}
	\caption{Quantitative comparison (PSNR(dB) and SSIM) on \textbf{UDM10} and \textbf{Vimeo-90K-T} for  video super-resolution, respectively. Flops and runtimes are calculated on an HR image size of  and  for UDM10 and Vimeo-90K-T, respectively. {\color{red}Red} text indicates the best and {\color{blue} blue} text indicates the second best performance. Y and RGB indicate the luminance and RGB channels, respectively. `' means the values are either taken from paper or calculated using provided models. } \label{SPMC_table}
	\vspace{-5mm}
\end{table}

\subsection{Comparison with State-of-the-arts}
In this section, we compare our methods with several state-of-the-art VSR approaches, including SPMC~\cite{tao2017detail}, TOFlow~\cite{xue2019video}, Liu ~\cite{liu2017robust}, DUF~\cite{haris2019recurrent}, EDVR~\cite{wang2019edvr}, PFNL~\cite{yi2019progressive}, TGA~\cite{isobe2020video}, FRVSR~\cite{sajjadi2018frame} and RLSP~\cite{Fuoli-arxiv19-rlsp}. The first seven methods super-resolve a single reference within a temporal sliding window. Among these methods, SPMC, TOFlow, Liu, RBPN and EDVR need to explicitly estimate the motion between the reference frame and other frames within the window, which requires redundant computation for several frames. DUF, PFNL and TGA skip the motion estimation process and partially ameliorate this issue. The last two methods FRVSR and RLSP super-resolve each frame in a recurrent way and are more efficient. 
We carefully implement most of these methods either on our own or by running the publicly available code, and manage to reproduce the results in their paper. 
The quantitative result of state-of-the-art methods on Vid4 is shown in Tab.~\ref{vid4_table}, where the number is either reported in the original papers or computed with our implementation. 
In addition, we also include the number of parameters and FLOPs for most methods when super-resolution is conducted on an LR image of size  in Tab.~\ref{vid4_table}. 

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{img/vid4.jpg}
	\caption{Qualitative comparison on \textbf{Vid4}, \textbf{UDM10} and \textbf{Vimeo-90K-T} test set for  SR. Zoom in for better visualization.}
	\label{fig:qualitative}
 	\vspace{-3mm}
\end{figure}
On Vid4, our model with only  SD block achieves dB PSNR in Y channel and dB PSNR in RGB channels, which already outperforms most of the previous methods by a large margin. By increasing the number of SD block to  and , our methods respectively gain another dB and dB PSNR in Y channel while with only a little increase in FLOPs. 
We also evaluate our method on other three popular test sets. The quantitative results on UDM10~\cite{yi2019progressive} and Vimeo-90K-T~\cite{xue2019video} two datasets are reported in Tab.~\ref{SPMC_table}. 
Our method achieves a very good balance between reconstruction performance and speed on these datasets.
On UDM10 test set, RSDN 9-128 achieves new state-of-the-art, and is about  and  times faster than DUF and RBPN, respectively. RSDN 9-128 outperforms the recent proposed PFNL, where this dataset is proposed by dB in PSNR in Y channel while being  times faster. 
The proposed method is also evaluated on Vimeo-90K-T, which only contains 7-frame in each sequence. In this case, although our method can not take full of its advantage because of the short length of the sequence, it only lags behind the large model EDVR-L but is  times faster.

We also show the qualitative comparison with other state-of-the-art methods. As shown in Fig.~\ref{fig:qualitative}, our method produces higher quality high-resolution images on all three datasets, including finer details and sharper edges. Other methods are either prone to generate some artifacts (e.g., wrong stripes in an image) or can not recover missing details (e.g., small windows of the building).
We also examine temporal consistency of the video super-resolution results in Fig.~\ref{fig:profile}, which is produced by extracting a horizontal row of pixels at the same position from consecutive frames and stacking them vertically. The temporal profile produced by our method is not only temporally smoother but also much sharper, satisfying both requirements of the video super-resolution task.

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{img/temporal_profile.pdf}
	\caption{Visualization of temporal profile for the green line on the calendar sequence. 
	}
	\label{fig:profile}
 	\vspace{-3mm}
\end{figure}