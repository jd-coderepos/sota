\appendix

\section{More Implementation Details}
\label{sec:appendix_detail}
We set the temperature to , the top  to , the max generation length to , and the timeout of executing a test case to  seconds. Specially, for baseline pass@, we use the greedy search setting with temperature . The number of sampling test cases for each problem is set to  for the HumanEval and MBPP benchmarks, and  for the APPS and CodeContests benchmarks. When scoring consensus sets in \ours, we use the square root of  to reduce the impact caused by code solutions. A supporting experiment can be found in Appendix \ref{appendix_sqrt}. For code solution post-processing, we follow \cite{chen2021evaluating} to truncate the generated content by five stop sequences: ``\textbackslash", ``\textbackslash", ``\textbackslash \#", ``\textbackslash", and ``\textbackslash".  
For the implementation of \incoder and \codegen, 
we use the HuggingFace transformers library \citep{Wolf2019HuggingFacesTS} and run both models with half precision.  
In addition, when the number of consensus sets in \ours is smaller than , the selection is done from the highest scoring consensus set to the lowest. When reaching the set with the lowest score, it repeats from the highest scoring consensus set. In most cases, the number of consensus sets is larger than , as shown in Figure \ref{fig:cluster_count}.

\section{Results on Original HumanEval}
\label{sec:appendix_origial_humaneval}
\begin{table}[t]
    \centering
    \scalebox{1}{
        \begin{tabular}{lllllllllllll}
        \toprule
        \multicolumn{1}{c}{{\textbf{Methods}}} & \multicolumn{3}{c}{{\textbf{Baseline}}} & \multicolumn{3}{c}{{\textbf{\ours}}} 
\\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-4}
        \cmidrule(lr){5-7}
\multicolumn{1}{c}{}&\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c}{}
\\
        \midrule
        \cushman & ~\improveorange{-1.8} & ~\improveorange{2.1} & ~\improveorange{6.7} & 
        ~\improveorange{14.1} & ~\improveorange{15.6} & ~\improveorange{14.4}
\\
        \davincione & ~\improveorange{-4.2} & ~\improveorange{2.4} &~\improveorange{3.1} & ~\improveorange{10.2}& ~\improveorange{10.2} & ~\improveorange{6.6}\\
\davincitwo & ~\improveorange{0.6} & ~\improveorange{3.9}  & ~\improveorange{0.6} &
        ~\improveorange{9.0} & ~\improveorange{7.8} & ~\improveorange{2.4}
\\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () on the original HumanEval benchmark with Codex models. The numbers in {\textcolor{orange}{orange}} indicate the absolute improvements of pass@ on the original benchmark over our modified benchmark in Table \ref{tab:main}. 
}
    \label{tab:modified_benchmark}
\end{table}
As mentioned in Section \ref{benckmarks}, for all benchmarks, we remove the example input-output cases from the original contexts to avoid exposing real test cases. To study the influence of such modification, we take HumanEval as an example and perform an additional experiment with its original contexts. The results are summarized in Table \ref{tab:modified_benchmark}.
On the one hand, the baseline pass@ and pass@ results on the original HumanEval benchmark outperform the modified version, which is reasonable because the example input-output cases may provide useful information for code generation. Nevertheless, the \passattop{1} results on the original benchmark are basically the same or even worse than the modified version, suggesting that the Codex models have not fully understood the semantics of the example input-output cases provided in the contexts. 
On the other hand, the performance of \ours is significantly improved using the original benchmark. This is as expected because the original contexts used for test case generation include real test cases, which could be borrowed by the models during the generation. Such real test cases will greatly empower \ours to distinguish correct code solutions. Hence, in our experiments, it is indispensable to remove the example input-output cases to avoid exposing the real test cases. In this way, the effectiveness of \ours can be fairly verified.

\section{Analysis on Code Solutions}
\label{appendix_sqrt}

\begin{figure}[t]
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{cluster_count_new.pdf}
  \captionof{figure}{The numbers of consensus sets that are produced by \cushman and \ours on the HumanEval benchmark.}
  \label{fig:cluster_count}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{cluster_size_multi_new.pdf}
  \captionof{figure}{The distribution of the code solution numbers for the top  consensus sets. The long tail distribution with number  is truncated.}
  \label{fig:cluster_size}
\end{minipage}
\end{figure}

In \ours, code solutions that can pass exactly the same test cases are considered consistent in functionality and are grouped into the same consensus set. Since we employ top  sampling with a rather high temperature of , the functionality of the code solutions may vary significantly, which results in more consensus sets.  We draw a histogram in Figure \ref{fig:cluster_count} to show the number of consensus sets produced by \cushman and \ours for each problem in the HumanEval benchmark. The average and median numbers are  and , respectively. We can find that most problems have less than  consensus sets, but the numbers have a high variance among different problems. We also draw the distribution of the numbers of code solutions for the top-ranked consensus sets in Figure \ref{fig:cluster_size}. The consensus sets ranked top  tend to have more code solutions with an average value of , and the numbers also have a high variance.

As mentioned in Appendix \ref{sec:appendix_detail}, we use the square root of  to reduce the impact caused by code solutions, because we believe passing more test cases is more important than having more code solutions with the same functionality. For example, there may be one code solution that can pass five test cases, whereas another five code solutions in a consensus set can pass only one test case. We intuitively consider that the former may be more likely correct. 
For validation, we perform an experiment by comparing the performance of \ours with the ``", ``" functions, and without any constraint (i.e., ``") on the number of code solutions. Figure \ref{fig:solution_importance} shows the results of three Codex models on the HumanEval benchmark. We can find that reducing the importance of code solutions can consistently improve the performance of \ours. Similar observations have been found in other models and benchmarks, where the performance of employing ``" is always better than or competitive to ``", indicating the rationality of our design.




\begin{figure}
\centering
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{solution_importance_2dim.pdf}
  \captionof{figure}{The \ours results of three Codex models with and without constraint on the number of code solutions.}
  \label{fig:solution_importance}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}{.49\textwidth}
  \centering
  \includegraphics[width=0.815\linewidth]{temperature.pdf}
  \captionof{figure}{The baseline pass@ and \ours \passattop{1} with \cushman at different temperature settings.}
  \label{fig:temperature}
\end{minipage}
\end{figure}

\iffalse
\section{More Implementation and Post-processing Details}
\label{sec:appendix_details}

For code solution post-processing, we follow \cite{chen2021evaluating} to truncate the generated content by five stop sequences: ``\textbackslash", ``\textbackslash", ``\textbackslash \#", ``\textbackslash", and ``\textbackslash". For test case post-processing, we extract the first five assertions that conform to the Python syntax for each generated sample. A valid assertion should start with ``" and contain the name of the corresponding entry point function. For experiments with \incoder and \codegen, we use the HuggingFace transformers library \citep{Wolf2019HuggingFacesTS}. The setup and post-processing procedure are the same as in the Codex experiments, except that the baseline pass@ results are obtained by picking the sample with the highest mean log-probability from  samples with a small temperature close to . To speed up our experiments, we run both models with half precision.  
\fi

\begin{table}[t]
    \centering
    \scalebox{1}{
        \begin{tabular}{ccllllll}
        \toprule
        \multicolumn{2}{c}{\textbf{De-duplication}} & \multicolumn{3}{c}{\textbf{HumanEval}} & 
        \multicolumn{3}{c}{\textbf{MBPP}} \\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-5}
        \cmidrule(lr){6-8}
        Solution & Test & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
        \midrule
        No & No &  &  &  & \textbf{55.4} &  &  \\
        No & Yes &  &  & \textbf{66.7} &  & \textbf{62.3} & \textbf{73.4} \\
        Yes & No & \textbf{46.9} & \textbf{52.5} &  &  &  &  \\
        Yes & Yes &  &  &  &  &  &  \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () on the HumanEval and MBPP benchmarks using \ours and \cushman with different de-duplication settings. The setting ``No No" in the first line means that neither the code solutions nor the test cases are de-duplicated, which is used in our main experiments.}
    \label{tab:deduplication}
\end{table}

\section{Influence of De-duplication}
\label{appendix_dedup}
Since we sample multiple times during generation, there is the chance that many of the generated code solutions and test cases are exactly the same. On the one hand, the number of duplicates may indicate the importance of a sample. On the other hand, duplicates may hinder the scoring of consensus sets in \ours when the quality of generation is unsatisfactory. Hence, we perform an ablation study to investigate the effects of removing duplicate code solutions and test cases.
Specifically, we first format the generated Python code to conform to the PEP 8 style guide\footnote{\url{https://peps.python.org/pep-0008}}, and then remove duplicate code solutions and test cases before performing \ours. The de-duplication results on the HumanEval and MBPP benchmarks using \ours and \cushman are shown in Table \ref{tab:deduplication}, where we can choose to de-duplicate the code solutions, or the test cases, or both. We can find that de-duplication has slight and inconsistent influence on the performance of \ours. For the HumanEval benchmark, the \passattop{1} results using code solution de-duplication alone are better than other settings. Nonetheless, for the MBPP benchmark, the best \passattop{1} results are achieved without de-duplication. Therefore, in our main experiments, we reserve all the generated code solutions and test cases when performing \ours and leave the study of more advanced de-duplication methods for future work.



\iffalse
\begin{table}[t]
    \centering
    \scalebox{0.92}{
        \begin{tabular}{cclll}
        \toprule
        \multicolumn{2}{c}{\textbf{De-duplication}} & \multicolumn{3}{c}{\textbf{\ours}} \\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-5}
        Solution & Test &  &  &  \\
        \midrule
        \grayline \multicolumn{5}{c}{\textbf{HumanEval}}\\
        No & No &  &  &  \\
        No & Yes &  &  & \textbf{66.7} \\
        Yes & No & \textbf{46.9} & \textbf{52.5} &  \\
        Yes & Yes &  &  &  \\
        \midrule
        \grayline \multicolumn{5}{c}{\textbf{MBPP}}\\
        No & No & \textbf{55.4} &  &  \\
        No & Yes &  & \textbf{62.3} & \textbf{73.4} \\
        Yes & No &  &  &  \\
        Yes & Yes &  &  &  \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () on the HumanEval and MBPP benchmarks using \ours and \cushman with different de-duplication settings.}
    \label{tab:deduplication}
\end{table}
\fi

\section{Sensitivity to the Temperature}
\label{appendix_temperature}
The hyper-parameter temperature has a great impact on the quality of generated code solutions and test cases when using top  sampling. We use a high temperature of  in our main experiments since \ours could benefit from a larger number of diverse samples. To investigate the sensitivity of \ours to the temperature, we perform an ablation study by using a range of temperatures to report the results of baseline pass@ and \ours \passattop{1}. Figure \ref{fig:temperature} shows the results of \cushman on the HumanEval benchmark at different temperature settings. We can find that a higher temperature does improve the baseline pass@ and \ours pass@, and \ours achieves a good performance when temperature is set to .

\section{Removing Trivial Code Solutions}
\label{appendix_remove_trivial}


\iffalse
\begin{table}[t]
    \centering
    \scalebox{0.98}{
        \begin{tabular}{ccccc}
        \toprule
        \multicolumn{1}{c}{Pass@} & \textsc{Introductory} & \textsc{Interview} & \textsc{Competition} & \textsc{CodeContests} \\
        \midrule
        \grayline \multicolumn{5}{c}{\textbf{\ours Raw}} \\
        \passattop{1} &  &  &  &  \\
        \passattop{2} &  &  &  & - \\
        \passattop{10} &  &  &  &  \\
        \passattop{100} & - & - & - &  \\
        \midrule
        \grayline \multicolumn{5}{c}{\textbf{\ours No-trivial}} \\
        \passattop{1} & ~\improve{0.3} & ~\improve{0.2} & ~\improve{0.3} & ~\improve{0.6} \\
        \passattop{2} & ~\improve{0.5} & ~\improve{0.4} & ~\improve{0.1} & - \\
        \passattop{10} & ~\improve{0.2} & ~\improve{0.1} & ~\improve{0.1} & ~\improve{0} \\
        \passattop{100} & - & - & - & ~\improve{0.1} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the zero-shot APPS (with respect to different levels of difficulty) and CodeContests benchmarks using \davincitwo with trivial code solutions filtered.}
    \label{tab:remove_trivial}
\end{table}
\fi

\iffalse
\begin{table}[t]
    \centering
    \scalebox{0.95}{
        \begin{tabular}{llllllllll}
        \toprule
        \multicolumn{2}{c}{{\textbf{Methods}}} & \multicolumn{4}{c}{{\textbf{\ours}}} & \multicolumn{4}{c}{{\textbf{\ours Non-trivial}}}\\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-6}
        \cmidrule(lr){7-10}
        \multicolumn{2}{c}{}&\multicolumn{1}{c}{} & \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{}\\
        \midrule
        \multirow{3}{*}{APPS} 
        & \textsc{Introductory} & \colorblue  &  & \coloryellow  &-& \colorblue ~\improve{0.3}& ~\improve{0.5}& \coloryellow ~\improve{0.2}& - \\
        & \textsc{Interview} & \colorblue  &  & \coloryellow  &-& \colorblue ~\improve{0.2}& ~\improve{0.4}& \coloryellow ~\improve{0.1}& - \\
        & \textsc{Competition} & \colorblue  &  & \coloryellow  &-& \colorblue ~\improve{0.3}& ~\improve{0.1}& \coloryellow ~\improve{0.1}& - \\
        \multicolumn{2}{c}{CodeContests} & \colorblue  &  & \coloryellow  & \colorgreen  & \colorblue ~\improve{0.6}& ~\improve{0.5}& \coloryellow ~\improve{0.0}& \colorgreen ~\improve{0.1} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the zero-shot APPS and CodeContests benchmarks using \davincitwo and \ours with/without the trivial code solutions filtered. The numbers in {\textcolor{red}{red}} indicate the absolute improvements after filtering the trivial solutions.}
    \label{tab:remove_trivial}
\end{table}
\fi

\begin{table}[t]
    \centering
    \scalebox{1}{
        \begin{tabular}{llllllll}
        \toprule
        \multicolumn{2}{c}{{\textbf{Methods}}} & \multicolumn{3}{c}{{\textbf{\ours}}} & \multicolumn{3}{c}{{\textbf{\ours (Remove Trivial)}}}\\
        \cmidrule(lr){1-2}
        \cmidrule(lr){3-5}
        \cmidrule(lr){6-8}
        \multicolumn{2}{c}{}&\multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c}{} &\multicolumn{1}{c}{}\\
        \midrule
        \multirow{3}{*}{APPS} 
        & \textsc{Introductory} & \colorblue  & \coloryellow  &-& \colorblue ~\improve{0.3}& \coloryellow ~\improve{0.2}& - \\
        & \textsc{Interview} & \colorblue  & \coloryellow  &-& \colorblue ~\improve{0.2}& \coloryellow ~\improve{0.1}& - \\
        & \textsc{Competition} & \colorblue  & \coloryellow  &-& \colorblue ~\improve{0.3}& \coloryellow ~\improve{0.1}& - \\
\cmidrule(lr){1-8}
        \multicolumn{2}{c}{CodeContests} & \colorblue  & \coloryellow  & \colorgreen  & \colorblue ~\improve{0.6} & \coloryellow ~\improve{0.0}& \colorgreen ~\improve{0.1} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the zero-shot APPS and CodeContests benchmarks using \davincitwo and \ours with/without the trivial code solutions filtered. The numbers in {\textcolor{red}{red}} indicate the absolute improvements after filtering the trivial solutions.}
    \label{tab:remove_trivial}
\end{table}

The problems in the APPS \textsc{Competition} and CodeContests benchmarks are of great difficulty compared to HumanEval and MBPP, leading to the poor performance of the most capable \davincitwo model. After checking the incorrect code solutions generated by \davincitwo, we identify many trivial solutions that just return the input argument or a constant value. Such solutions may hinder the ranking process of \ours if they can pass any generated test case. A trivial solution can be easily identified by its input arguments and returned values. If a solution always returns the same output value for different inputs, or its returned values are always the same as the inputs, it must be a trivial solution. To investigate the impact of trivial code solutions, we use \davincitwo on the zero-shot APPS and CodeContests benchmarks, and perform \ours after filtering out all the trivial solutions. 
As a result, we can remove an average of  () trivial solutions from the  () generated solutions per problem for the APPS (CodeContests) benchmark. However, as shown in Table \ref{tab:remove_trivial}, after removing a prominent percentage of trivial solutions, there is little performance gain, which could exactly demonstrate the robustness of \ours.


\section{Results on APPS and CodeContests in the One-shot Setting}
\label{appendix_oneshot}
\begin{table}[t]
    \centering
    \scalebox{0.88}{
        \begin{tabular}{lllllllllll}
        \toprule
\multicolumn{2}{c}{}&\multicolumn{1}{c}{} & \multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{} &\multicolumn{1}{c}{} &
        \multicolumn{1}{c}{} &\multicolumn{1}{c}{}\\
        \midrule
        \multicolumn{2}{c}{{\textbf{}}} & \multicolumn{5}{c}{{\textbf{Baseline}}} & \multicolumn{4}{c}{{\textbf{\ours}}}\\
        \cmidrule(lr){3-7}
        \cmidrule(lr){8-11}
        \multirow{3}{*}{APPS} & \textsc{Introductory} &\colorblue &\coloryellow &&-&-&\colorblue ~\improve{18.0}& &\coloryellow ~\improve{9.9}&-\\
        & \textsc{Interview} &\colorblue &\coloryellow &&-&-&\colorblue ~\improve{7.9}&&\coloryellow ~\improve{8.7}&-\\
        & \textsc{Competition}&\colorblue &\coloryellow &&-&-&\colorblue ~\improve{3.7}&&\coloryellow ~\improve{7.3}&-\\
\cmidrule(lr){1-11}
        \multicolumn{2}{c}{CodeContests} &\colorblue  & \coloryellow  &  &\colorgreen  &  & \colorblue ~\improve{2.2} &  & \coloryellow ~\improve{5.2} & \colorgreen ~\improve{3.5} \\
        \midrule
        \multicolumn{2}{c}{{\textbf{}}} & \multicolumn{5}{c}{{\textbf{Baseline Filter}}} & \multicolumn{4}{c}{{\textbf{\ours Filter}}}\\
        \cmidrule(lr){3-7}
        \cmidrule(lr){8-11}
        \multirow{3}{*}{APPS} & \textsc{Introductory} &\colorblue &\coloryellow &-&-&-&\colorblue ~\improve{6.0}& &\coloryellow ~\improve{0.8}&-\\
        & \textsc{Interview} &\colorblue &\coloryellow &-&-&-&\colorblue ~\improve{2.8}&&\coloryellow ~\improve{1.2}&-\\
        & \textsc{Competition}&\colorblue &\coloryellow &-&-&-&\colorblue ~\improve{0.9}&&\coloryellow ~\improve{0.8}&-\\
\cmidrule(lr){1-11}
        \multicolumn{2}{c}{CodeContests} &\colorblue  & \coloryellow  &  &\colorgreen  & - & \colorblue ~\improve{-0.3} &  & \coloryellow ~\improve{-0.8} & \colorgreen ~\improve{-0.7} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the APPS and CodeContests benchmarks using \davincitwo and the one-shot setting. The numbers in {\textcolor{red}{red}} indicate the absolute improvements of \ours (Filter) over Baseline (Filter) on pass@, pass@ and pass@.
    For \ours (Filter), temperature is set to  and sampling number is set to  for APPS and  for CodeContests. We do not report pass@ for ``Baseline Filter'' because the numbers of code solutions after filtering are less than the sampling numbers.
    }
    \label{tab:apps_code_oneshot}
\end{table}
Inspired by \cite{chen2021evaluating} and \cite{li2022competition}, we build one-shot versions of APPS and CodeContests by appending a single input-output example to the problem description as a formatting hint. After generation, we filter out the generated solutions that cannot pass the given example input-output cases, which we call the ``Baseline Filter'' method. After filtering, we can still perform \ours using the rest of code solutions, called the ``\ours Filter'' method.
Following the zero-shot experiments on APPS and CodeContests, we employ \davincitwo for generation and set the sampling number to  for APPS and  for CodeContests.

We summarize the experimental results in Table \ref{tab:apps_code_oneshot}, where we can find the one-shot performance using \ours is much better than that reported in Table \ref{tab:apps_code} in the zero-shot setting. The performance of the baselines can be significantly improved by filtering the solutions with the given example test cases. Moreover, ``\ours Filter'' can further outperform ``Baseline Filter'' on the APPS benchmark, especially for the introductory and interview problems. Nonetheless, for CodeContests and the competition level problems in APPS, ``\ours Filter'' has little performance improvement or even performs slightly worse than ``Baseline Filter''. After manual investigation, we blame such issue to the generated low-quality test cases, which hinder the scoring of consensus sets. This suggests the interest of future study on test case generation for more challenging programming problems.


\iffalse
\begin{table}[t]
    \centering
    \scalebox{0.98}{
        \begin{tabular}{lccc}
        \toprule
       \multicolumn{1}{c}{\textbf{Methods}} & \textsc{\textbf{Introductory}} & \textsc{\textbf{Interview}} & \textsc{\textbf{Competition}} \\
        \midrule
        Baseline Raw \passattop{1} &  &  &  \\
        Baseline Raw \passattop{10} &  &  &  \\
        Baseline Raw \passattop{50} &  &  &  \\
        \midrule
        \ours Raw \passattop{1} &  &  &  \\
        \ours Raw \passattop{2} &  &  &  \\
        \ours Raw \passattop{10} &  &  &  \\
        \midrule
        Baseline Filter \passattop{1} & \colorblue  & \colorblue  & \colorblue  \\
        Baseline Filter \passattop{2} & \colorgreen  & \colorgreen  & \colorgreen  \\
        Baseline Filter \passattop{10} & \coloryellow  & \coloryellow  & \coloryellow  \\
        \midrule
        \ours Filter \passattop{1} & \colorblue ~\improve{6.0} & \colorblue ~\improve{2.8} & \colorblue ~\improve{0.9} \\
        \ours Filter \passattop{2} & \colorgreen ~\improve{4.0} & \colorgreen ~\improve{2.6} & \colorgreen ~\improve{1.2} \\
        \ours Filter \passattop{10} & \coloryellow ~\improve{0.8} & \coloryellow ~\improve{1.2} & \coloryellow ~\improve{0.8} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the APPS benchmark (one-shot) using \davincitwo with respect to different levels of difficulty.}
    \label{tab:apps_oneshot}
\end{table}

\begin{table}[t]
    \centering
    \scalebox{1}{
        \begin{tabular}{ccccc}
        \toprule
        \multirow{1}{*}{\textbf{Methods}} & \multicolumn{4}{c}{{\textbf{Results}}} \\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-5}
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{} &\multicolumn{1}{c}{} & \multicolumn{1}{c}{}& \multicolumn{1}{c}{} \\
        \midrule
        Baseline Raw &  &  &  &  \\
        \ours Raw &  &  &  & - \\
        \midrule
        Baseline Filter & \colorblue  & \coloryellow  & \colorgreen  & -  \\
        \ours Filter & \colorblue ~\improve{-0.3} & \coloryellow ~\improve{-0.8} & \colorgreen ~\improve{-0.7} & - \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the CodeContests benchmark (one-shot) using \davincitwo.}
    \label{tab:codecontests_oneshot}
\end{table}
\fi


\section{More Analysis on Test Cases}
\label{appendix_test_case}
\subsection{Statistics on Test Cases}
\label{appendix_stat_test}
\iffalse
\begin{table}[t]
    \centering
    \scalebox{0.95}{
        \begin{tabular}{cccccc}
        \toprule
        \textbf{Models} & Average & Median \\
        \midrule
        \cushman &  &  \\
        \davincione &  &  \\
        \davincitwo &   &  \\
        \incoder &  &  \\
        \codegen &  &  \\
        \bottomrule
        \end{tabular}
    }
    \caption{The average and median numbers of syntactically correct test cases for each problem generated by various models on the HumanEval benchmark.}
    \label{tab:test_case_number}
\end{table}
\fi

\begin{table}[t]
\small
\begin{minipage}{0.48\linewidth}
\centering
    \scalebox{1}{
        \begin{tabular}{cccccc}
        \toprule
        \multirow{3}{*}{\textbf{Methods}} &
        \multicolumn{2}{c}{{\textbf{Test Case Number}}} \\
\cmidrule(lr){2-3}
        & Average & Median \\
        \midrule
        \cushman &  &  \\
        \davincione &  &  \\
        \davincitwo &   &  \\
        \incoder &  &  \\
        \codegen &  &  \\
        \bottomrule
        \end{tabular}
    }
    \caption{The numbers of extracted test cases for each problem generated by five models on the HumanEval benchmark.}
    \label{tab:test_case_number}
\end{minipage}
\hspace{2pt}
\begin{minipage}{0.48\linewidth}
\centering
\scalebox{1}{
    \begin{tabular}{ccc}
    \toprule
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{2}{c}{{\textbf{Code Coverage}}} \\
    \cmidrule(lr){2-3}
    & Statement & Branch \\
    \midrule
    \cushman &  &  \\
    \davincione &  &  \\
    \davincitwo &  &  \\
    \incoder &  &  \\
    \codegen &  &  \\
    \bottomrule
    \end{tabular}
}
    \caption{The Code Coverage () statistics of test cases generated by five models on the HumanEval benchmark.}
    \label{tab:coverage}
\end{minipage}
\end{table}

How many valid test cases do the models generate for \ours? Taking the HumanEval benchmark as an example, we sample  times for each problem when generating test cases. As illustrated in Figure \ref{fig:prelimilary}, at each time of sampling, we feed the \emph{context}  along with an \emph{instruction}  to the model and get the generated content that may contain multiple test cases. Then, as mentioned in Section \ref{sec:exp_test}, we further post-process the generated samples to get individual test cases that are syntactically correct. Finally, we only keep the first five valid test cases for each sample, which means a problem can be equipped with  test cases at most. Table \ref{tab:test_case_number} summarizes the average and median numbers of the extracted test cases for each problem. We can find that almost all the models could generate a considerable number of syntactically correct test cases, while \codegen generates plenty of unexpected noise.

\subsection{Code Coverage of Test Cases}
\label{appendix_coverage}
To further inspect the quality of generated test cases, we utilize the code coverage measurement and report two coverage criterias --- the statement coverage and the branch coverage. The statement coverage can be calculated as the percentage of statements in a code solution that are executed by test cases. The branch coverage is the percentage of executed branches for the control structure (e.g. the \textit{if} statement). We execute the canonical solution for each HumanEval problem on the test cases generated by five models, then collect the coverage results using Coverage.py\footnote{\url{https://coverage.readthedocs.io/en/6.4.2}}. As a result, the average numbers of statements and branches in the canonical solution of a problem are  and , respectively. As shown in Table \ref{tab:coverage}, all the models except \codegen have good performance on both statement and branch coverage, reaching an average of over  coverage. Such results may be attributed to the relatively short canonical solutions and the massive sampling number of test cases. Nevertheless, there are still corner cases that the models cannot cover, which calls for future improvements.


\begin{table}[t]
    \centering
    \begin{subtable}[t]{0.325\linewidth}
        \centering
        \scalebox{0.8}{
            \begin{tabular}{cllll}
            \toprule
            \multirow{3}{*}{\textbf{\textit{Limit}}} & \multicolumn{4}{c}{\textbf{Sampling Number}} \\
            \cmidrule(lr){2-5}
            &  &  &  &  \\
            \midrule
            \grayline \multicolumn{5}{c}{\textbf{\cushman}}\\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
            \midrule
            \grayline \multicolumn{5}{c}{\textbf{\davincitwo}}\\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
            \bottomrule
            \end{tabular}
        }
        \caption{\passattop{1}}
	\end{subtable}
	\begin{subtable}[t]{0.325\linewidth}
        \centering
        \scalebox{0.8}{
            \begin{tabular}{cllll}
            \toprule
            \multirow{3}{*}{\textbf{\textit{Limit}}} & \multicolumn{4}{c}{\textbf{Sampling Number}} \\
            \cmidrule(lr){2-5}
            &  &  &  &  \\
            \midrule
            \grayline \multicolumn{5}{c}{\textbf{\cushman}}\\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
            \midrule
            \grayline \multicolumn{5}{c}{\textbf{\davincitwo}}\\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
            \bottomrule
            \end{tabular}
        }
        \caption{\passattop{2}}
	\end{subtable}
	 \begin{subtable}[t]{0.325\linewidth}
	    \centering
	    \scalebox{0.8}{
            \begin{tabular}{cllll}
            \toprule
            \multirow{3}{*}{\textbf{\textit{Limit}}} & \multicolumn{4}{c}{\textbf{Sampling Number}} \\
            \cmidrule(lr){2-5}
            &  &  &  &  \\
            \midrule
            \grayline \multicolumn{5}{c}{\textbf{\cushman}}\\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
            \midrule
            \grayline \multicolumn{5}{c}{\textbf{\davincitwo}}\\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
             &  &  &  &  \\
            \bottomrule
            \end{tabular}
        }
        \caption{\passattop{10}}
	\end{subtable}
    \caption{Pass@ () on the HumanEval benchmark using \ours with different test case numbers. \textit{Sampling Number} is the number of test case samples we generate for each problem. Each sample may contain multiple assertion statements. These assertion statements are potential test cases, but we do not use all of them. Instead, we extract a \textit{Limit} number of syntactically correct assertion statements from each sample, and discard the rest.}
    \label{tab:differ_test_case_number}
\end{table}

\subsection{Results of Reducing the Number of Test Cases}
\label{appendix_test_case_number}
To investigate the performance of \ours using fewer test cases, we perform an ablation study on the number of test cases that participate in the dual execution agreement. As shown in Table \ref{tab:differ_test_case_number}, we report the results on the HumanEval benchmark using \cushman and \davincitwo with a range of test case numbers. The number of test cases is related to two hyper-parameters. One is the number of test case samples, which is set to  for HumanEval in our main experiments. The other one is \textit{Limit} that controls the amount of syntactically correct test cases we extract from each sample, which is set to  for all benchmarks in our main experiments.  Note that \textit{Limit} multiplied by the \textit{Sampling Number} is the maximum number of test cases for a problem, not the exact number, because not every sample contains the \textit{Limit} number of valid test cases. A valid test case (i.e., assertion statement) should start with ``" and contain the name of the corresponding entry point function.
We can conclude from the results that using more test cases in \ours could generally lead to better performance. While the performance gap narrows when \textit{Limit}  and the sampling number . Moreover, using only  test cases per problem for \ours can still improve the baseline \passattop{1} performance of \cushman by absolute  and \davincitwo by absolute . It demonstrates that \ours has high test case efficiency and we can use a smaller \textit{Sampling Number} in real-world application to balance the performance and computation cost.




\iffalse
\begin{table}[t]
    \centering
    \begin{tabular}{ccc}
    \toprule
    \multirow{3}{*}{\textbf{Methods}} & \multicolumn{2}{c}{{\textbf{Coverage}}} \\
    \cmidrule(lr){2-3}
    & Statement & Branch \\
    \midrule
    \cushman &  &  \\
\davincitwo &  &  \\
    \bottomrule
    \end{tabular}
    \caption{The Code Coverage () statistics of test cases generated by three Codex models on the HumanEval benchmark. Additionally, the average numbers of statements and branches in the canonical solution of a problem are \textcolor{orange}{} and \textcolor{orange}{}.}
    \label{tab:coverage}
\end{table}
\fi

\begin{table}[t]
    \centering
    \scalebox{0.95}{
        \begin{tabular}{lllllll}
        \toprule
        \multicolumn{1}{c}{{\textbf{Methods}}} & \multicolumn{3}{c}{\textbf{Code Solution Only }} 
        & \multicolumn{3}{c}{{\textbf{Test Case Only }}}\\
        \cmidrule(lr){1-1}
        \cmidrule(lr){2-4}
        \cmidrule(lr){5-7}
        \multicolumn{1}{c}{}&\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}&\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
        \midrule
\cushman & \improvedouble{41.2}{+7.7}{-3.3} & \improvedouble{49.2}{}{-0.9} & \improvedouble{61.9}{+7.6}{-3.8}& \improvedouble{29.9}{-3.6}{-14.6} & \improvedouble{36.6}{}{-13.5} & \improvedouble{59.5}{+5.2}{-6.2} \\
        \davincione & \improvedouble{44.4}{+5.4}{-5.8} & \improvedouble{54.7}{}{-4.2} & \improvedouble{69.0}{+8.4}{-6.8}& \improvedouble{35.0}{-4.0}{-15.2} & \improvedouble{46.0}{}{-12.9} & \improvedouble{70.2}{+9.6}{-5.6} \\
        \davincitwo & \improvedouble{55.9}{+8.9}{-9.9} & \improvedouble{67.0}{}{-8.1} & \improvedouble{82.7}{+7.8}{-3.9}& \improvedouble{58.4}{+11.4}{-7.4} & \improvedouble{65.1}{}{-10.0} & \improvedouble{86.1}{+11.2}{-0.5} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () on the HumanEval benchmark with ranking only on the number of code solutions () or test cases () in a consensus set. The numbers in {\textcolor{red}{red}} and {\textcolor[rgb]{0,0.392,0}{green}} indicate the absolute improvements over baseline and \ours, respectively.}
    \label{tab:naive_baseline}
\end{table}

\section{Ablation Study on the Score of Consensus Set}
\label{appendix_simplecount}
In \ours, the score of a consensus set is calculated as , where  and  are the code solutions and test cases in the consensus set, respectively. We can naturally derive two variants of scoring.
One is , in line with the idea of self-consistency~\citep{wang2022self}, which only considers the number of code solutions with the same functionality.
The other one is , which corresponds to simply counting the test cases that each code solution can pass. 
To evaluate the performance of these two variants, we perform an ablation study on the HumanEval benchmark using three Codex models. The experimental results are summarized in Table \ref{tab:naive_baseline}, from which we can observe that only considering the number of code solutions or test cases for consensus set scoring performs consistently worse than \ours, and even worse than the baseline. Therefore, it is essential to consider the importance of both code solutions and test cases, suggesting the reasonable design of our dual execution agreement.

As mentioned in Section \ref{sec:exp-setup}, AlphaCode~\citep{li2022competition} also includes a clustering method (denoted as AlphaCode-C) to select the generated code solutions, which shares a similar goal with our ablation method : clustering code solutions based on code functionality, and then scoring each cluster by size. AlphaCode-C requires a number of additional test inputs to produce outputs from code solutions, which are then used to determine the functional equivalence. AlphaCode-C relies on a separate test input generation model, which needs extra training and annotation. The model is unavailable and hard to replicate, as the paper does not provide sufficient details. We replicate AlphaCode-C by extracting test inputs from the test cases generated by \ours. We run all code solutions on the test inputs, and group them by outputs. The clusters are ranked by size and then we select the code solutions from each cluster in order. From Table \ref{tab:main} and Table \ref{tab:naive_baseline}, we can find that AlphaCode-C is inferior to , though they share the similar idea. The reason is that AlphaCode-C will group the trivial code solutions (e.g., solutions that always output ``", ``“, or an empty string with whatever inputs) together, leading to a large cluster of incorrect solutions that significantly affects performance. While such trivial code solutions are hard to pass the generated test cases in \ours, thus having lower consensus scores for ranking. This confirms the effectiveness of considering test case information.

\section{More examples for Case Study}
\label{appendix_code_example}

\begin{figure*}[t]
	\centering
	\begin{subfigure}[t]{0.95\linewidth}
		\centering
		\includegraphics[width=\linewidth]{case-appendix4.pdf}
		\caption{The first consensus set has fewer code solutions.}
		\label{subfig:case_appendix4}
	\end{subfigure}
\begin{subfigure}[t]{0.95\linewidth}
		\centering
		\includegraphics[width=\linewidth]{case-appendix5.pdf}
		\caption{The first consensus set has fewer test cases.}
		\label{subfig:case_appendix5}
	\end{subfigure}
	\caption{Two cases from the HumanEval benchmark, where \ours can find the correct consensus sets though they have (a) fewer code solutions, or (b) fewer test cases.}
	\label{fig:case_appendix4-5}
\end{figure*}

\begin{figure*}[t]
	\centering
\begin{subfigure}[t]{0.95\linewidth}
		\centering
		\includegraphics[width=\linewidth]{case-appendix2.pdf}
		\caption{Uncovered corner cases.}
		\label{subfig:case_appendix2}
	\end{subfigure}
	\begin{subfigure}[t]{0.95\linewidth}
		\centering
		\includegraphics[width=\linewidth]{case-appendix3.pdf}
		\caption{Failure of Problem Understanding.}
		\label{subfig:case_appendix3}
	\end{subfigure}
	\caption{Three incorrect cases from the HumanEval benchmark, where \ours cannot find the correct consensus sets due to (a) uncovered corner cases, or (b) failure of problem understanding.}
	\label{fig:case_appendix1-3}
\end{figure*}


Figure \ref{fig:case_appendix4-5} illustrates two cases that \ours can successfully find the correct consensus sets. Specifically, the case in Figure \ref{subfig:case_appendix4} requires to remove the vowels in the input text. There are  incorrect solutions and  test cases in the consensus set ranked , which forget to remove the upper-case vowels. Though the correct solutions in the top  consensus set are fewer (i.e., ), they can pass more test cases (i.e., ) and thus have a higher score. The case in Figure \ref{subfig:case_appendix5} is to decide when the balance of account will fall below zero. The functionality of the incorrect solutions in the second consensus set is to tell whether there are withdrawing operations. Nevertheless, the incorrect solutions can pass more test cases (i.e., ) than the correct solutions (i.e., ) in the top  consensus set. Fortunately, there are  correct solutions and only  incorrect solutions, making it possible for \ours to rank the correct consensus ahead. Both cases demonstrate the plausibility of using the dual execution agreement instead of solely considering the functional agreement between code solutions or the number of passed test cases.

Figure \ref{fig:case_appendix1-3} illustrates the cases that \ours fails to find the correct consensus sets. Specifically, Figure \ref{subfig:case_appendix2} demonstrates the situation that there are partially correct solutions that may fail at certain corner cases. In the example, there are  incorrect solutions in the top  consensus set that can pass  test cases, which will fail if the input is a string of length . The correct consensus set ranked  has more test cases (i.e., ), while it has a lower consensus score due to the small number of code solutions (i.e., ). The second example in Figure \ref{subfig:case_appendix3} shows the most common situation where \ours fails because the model cannot fully understand the problem. We can find that the incorrect solutions in the top  consensus set are totally missing the points of the given problem. While the model still tends to generate more incorrect solutions and test cases based on its wrong understanding. All the bad cases call for future improvements on the quality of generated code solutions and test cases.


\iffalse
\begin{lstlisting}[style=mypython,caption={Ambiguous Problem Description}, label={lst:code1}, basicstyle=\tiny]
def sort_array(array):
    """
    Given an array of non-negative integers, return a copy of the 
    given array after sorting, you will sort the given array in 
    ascending order if the sum( first index value, last index value) 
    is odd, or sort it in descending order if the sum( first index 
    value, last index value) is even.
    """
    # ground truth test cases
    # [2, 4, 3, 0, 1, 5] => [0, 1, 2, 3, 4, 5]
    # [2, 4, 3, 0, 1, 5, 6] => [6, 5, 4, 3, 2, 1, 0]
    
    # below is the incorrect solution rank 1
    # with 4 solutions, 138 test cases
    sum = 0
    for i in range(len(array)):
        sum += array[i]
    if sum array.sort(reverse=True)
    else:
        array.sort()
    return array
    
    # below is the correct solution rank 2
    # with 3 solutions, 158 test cases
    initial_sum = sum(array[0:1]) + sum(array[-1:])
    if initial_sum return sorted(array, reverse=True)
    else:
        return sorted(array)
\end{lstlisting}


\begin{lstlisting}[style=mypython,caption={Failing Corner Cases}, label={lst:code2}, basicstyle=\tiny]
def even_odd_count(num):
    """
    Given an integer. return a tuple that has the number of even
    and odd digits respectively.
    """
    # ground truth test cases
    # 3452 => (2, 2)
    # -78 => (1, 1)
    
    # below is the incorrect solution rank 1
    # with 45 solutions, 94 test cases
    evens = 0
    odds = 0
    while num != 0:
        x = num if x evens += 1
        else:
            odds += 1
        num = num // 10
    return (evens, odds)
    
    # below is the correct solution rank 2
    # with 41 solutions, 57 test cases
    even_count = 0
    odd_count = 0
    for n in str(num):
        if int(n) even_count += 1
        else:
            odd_count += 1
    return even_count, odd_count
\end{lstlisting}



\begin{lstlisting}[style=mypython,caption={Problem Misunderstanding}, label={lst:code3}, basicstyle=\tiny]
def minSubArraySum(nums):
    """
    Given an array of integers nums, find the minimum sum of any 
    non-empty sub-array of nums.
    """
    # ground truth test cases
    # [100, -33, 32, -1, 0, -2] => -33
    # [-1, -2, -3, 10, -5] => -6
    
    # below is the incorrect solution rank 1
    # with 16 solutions, 102 test cases
    if not nums:
        return 0
    total = nums[0]
    min_sum = total
    for i in range(1, len(nums)):
        if total < 0:
            total = nums[i]
        else:
            total += nums[i]
        min_sum = min(min_sum, total)
    return min_sum
    
    # below is the correct solution rank 2
    # with 7 solutions, 96 test cases
    if not nums:
        return 0
    min_sum = float('inf')
    n = len(nums)
    for i in range(n):
        curr_sum = 0
        for j in range(i, n):
            curr_sum += nums[j]
            min_sum = min(min_sum, curr_sum)
    return min_sum
    
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption={Effectiveness of \ours}, label={lst:positive_code_1}, basicstyle=\tiny]
def below_threshold(l: list, t: int):
    """
    Return True if all numbers in the list l are below threshold t.
    """
    # ground truth test cases
    # ([1, 2, 4, 10], 100) => True
    # ([1, 8, 4, 10], 10) => False
    
    # below is an example from the consensus set that
    # contains 61 correct solutions and 218 test cases
    if l == []:
        return True
    return l[0] < t and below_threshold(l[1:], t)
    
    # below is an example from the consensus set that
    # contains 30 incorrect solutions and 226 test cases
    for e in l:
        if e > t:
            return False
    return True
\end{lstlisting}

\begin{lstlisting}[style=mypython, caption={Effectiveness of \ours}, label={lst:positive_code_2}, basicstyle=\tiny]
def below_zero(operations: List[int]) -> bool:
    """
    You're given a list of deposit and withdrawal operations on a
    bank account that starts with zero balance. Your task is to
    detect if at any point the balance of account falls below zero, 
    and at that point function should return True. 
    Otherwise it should return False.
    """
    # ground truth test cases
    # [1, 2, -3, 1, 2, -3] => False
    # [1, 2, -4, 5, 6] => True
    
    # there are 79 correct solutions in the cluster ranked 1
    # there are 248 test cases that these solutions can pass
    # below is an example of correct solutions that ranked 1
    zero_balance = 0
    for operation in operations:
        zero_balance += operation
        if zero_balance < 0:
            return True
    return False
    
    # there are 6 incorrect solutions in the #2 ranked cluster
    # there are 255 test cases that these solutions can pass
    # below is an example of incorrect solutions that ranked 1
    return any(i < 0 for i in operations)
\end{lstlisting}
\fi



\iffalse
\begin{table}[t]
    \centering
    \scalebox{0.95}{
        \begin{tabular}{llll}
        \toprule
        \multicolumn{1}{c}{{\textbf{Methods}}} & \multicolumn{3}{c}{\textbf{Results}} \\
\cmidrule(lr){1-1}
        \cmidrule(lr){2-4}
\multicolumn{1}{c}{}&\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
        \midrule
\grayline \multicolumn{4}{c}{Only Using Test Case Number} \\
        \cushman & \improvedouble{29.9}{-3.6}{-14.6} &  & \improvedouble{59.5}{+5.2}{-6.2} \\
        \davincione & \improvedouble{35.0}{-4.0}{-15.2} &  & \improvedouble{70.2}{+9.6}{-5.6} \\
        \davincitwo & \improvedouble{58.4}{+11.4}{-7.4} &  & \improvedouble{86.1}{+11.2}{-0.5} \\
        \midrule
        \grayline \multicolumn{4}{c}{Only Using Code Solution Number} \\       \cushman & \improvedouble{41.2}{+7.7}{-3.3} &  & \improvedouble{61.9}{+7.6}{-3.8} \\
        \davincione & \improvedouble{44.4}{+5.4}{-5.8} &  & \improvedouble{69.0}{+8.4}{-6.8} \\
        \davincitwo & \improvedouble{55.9}{+8.9}{-9.9} &  & \improvedouble{82.7}{+7.8}{-3.9} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () on the HumanEval benchmark with ranking only on the number of test cases or code solutions in a consensus set. The numbers in {\textcolor{red}{red}} and {\textcolor[rgb]{0,0.392,0}{green}} indicate the absolute improvements over baseline and \ours respectively.}
    \label{tab:naive_baseline}
\end{table}
\fi




\iffalse
\section{Evaluation on the APPS and CodeContests Benchmark}


We build a zero-shot version of APPS to be in line with our settings of HumanEval and MBPP by removing the example input-output cases in the problem descriptions. We employ \davincitwo for code solution and test case generation. The sampling number is set to 50 for the APPS benchmark to save computation cost on the  testing problems. While we set the sampling number to  for the CodeContests benchmark to be comparable with the results reported by \cite{li2022competition}. 

From the results summarized in Table \ref{tab:apps_zeroshot} and Table \ref{tab:codecontests_zeroshot}, we can easily find the consistent performance improvements on both benchmarks using \davincitwo and \ours. We can achieve competitive and even better performance in the zero-shot setting compared to the previously reported results using few-shot prompts and model fine-tuning~\citep{chen2021evaluating, hendrycks2021measuring, li2022competition}.


\begin{table}[t]
    \centering
    \scalebox{0.98}{
        \begin{tabular}{lccc}
        \toprule
        & \textsc{Introductory} & \textsc{Interview} & \textsc{Competition} \\
        \midrule
        Baseline \passattop{1} & \colorblue  & \colorblue  & \colorblue  \\
        Baseline \passattop{10} &\coloryellow  &\coloryellow  &\coloryellow  \\
        Baseline \passattop{50} &  &  &  \\
        \midrule
        \ours \passattop{1} & \colorblue ~\improve{7.4} & \colorblue ~\improve{3.0} & \colorblue ~\improve{0.4} \\
        \ours \passattop{2} &  &  &  \\
        \ours \passattop{10} &\coloryellow ~\improve{6.6} &\coloryellow ~\improve{5.3} &\coloryellow ~\improve{3.7} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the APPS benchmark (zero-shot) using \davincitwo with respect to different levels of difficulty.}
    \label{tab:apps_zeroshot}
\end{table}

\begin{table}[t]
    \centering
    \scalebox{1}{
        \begin{tabular}{ccccccc}
        \toprule
        \multicolumn{4}{c}{{\textbf{Baseline}}} & \multicolumn{3}{c}{{\textbf{\ours}}} \\
        \cmidrule(lr){1-4}
        \cmidrule(lr){5-7}
        \multicolumn{1}{c}{Pass@} &\multicolumn{1}{c}{Pass@} & \multicolumn{1}{c}{Pass@}& \multicolumn{1}{c}{Pass@} & \multicolumn{1}{c}{Pass@} & \multicolumn{1}{c}{Pass@} & \multicolumn{1}{c}{Pass@} \\
        \midrule
        \colorblue  & \coloryellow  & \colorgreen  &  & \colorblue ~\improve{1.4} & \coloryellow ~\improve{2.3} & \colorgreen ~\improve{2.4} \\
        \bottomrule
        \end{tabular}
    }
    \caption{Pass@ () results on the CodeContests benchmark (zero-shot) using \davincitwo.}
    \label{tab:codecontests_zeroshot}
\end{table}
\fi