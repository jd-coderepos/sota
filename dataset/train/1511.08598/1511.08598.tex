\RequirePackage{fixltx2e}

\pdfoutput=1
\documentclass[
submission
, nomarks
]{dmtcs-episciences}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\synctex=-1
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}

\makeatletter
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{definition}
  \newtheorem{defn}[thm]{\protect\definitionname}
  \theoremstyle{remark}
  \newtheorem*{rem*}{\protect\remarkname}
  \theoremstyle{plain}
\newtheorem*{lem*}{\protect\lemmaname}
 \newtheorem{lem}{Lemma}
  \theoremstyle{plain}
\newtheorem*{thm*}{Main Theorem}

\newcounter{lemma}
\renewcommand{\thelemma}{Lemma}





\usepackage{amsfonts} \usepackage{hyperref}

\makeatother

\providecommand{\definitionname}{Definition}
  \providecommand{\lemmaname}{Lemma}
  \providecommand{\remarkname}{Remark}
  \providecommand{\theoremname}{Theorem}
\providecommand{\theoremname}{Theorem}




\def\nomath#1{\ifmmode\text{#1}\else#1\fi}


\def\interval#1#2{\left\langle#1,#2\right\rangle}
\def\intervalOC#1#2{\left(#1,#2\right\rangle}
\def\intervalOO#1#2{\left(#1,#2\right)}

\def\eqspaces{\;=\;}
\def\lespaces{\;\le\;}

\def\Prob#1{\Pr\!\left[#1\right]}
\def\ProbParam#1#2{\Pr_{#1}\!\left[#2\right]}
\def\Exp#1{\mathrm{E}\left[#1\right]}

\def\OO{\mathcal O}
\def\oo{\ensuremath o}

\def\diff#1{\mathrm{d}#1}
\def\euler{\mathrm{e}}
\def\natural{\mathbb{N}}

\def\emphDef#1{{\bf#1}}
\def\emphDefMath#1{\mathbf{#1}}

\def\setZeroTo#1{\{0,\dotsc,{#1}\}}



\title{Predecessor problem on smooth distributions}

\affiliation{
Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic \\
  CZ.NIC, z.\,s.\,p.\,o.
}

\author{Vladim\'ir \v{C}un\'at\affiliationmark{1,2}\thanks{This research was supported by the Czech Science Foundation under grant GA14-10799S.}
}

\received{2017-01-25}


\begin{document}

\maketitle

\begin{abstract}
We follow a research thread studying the predecessor problem on input
taken from ``smooth'' distributions family. We propose a conceptually
simpler solution, utilizing well-known results from much better studied
variant of the problem that assumes nothing about the input. As a
side effect, we are able to extend the range of input distributions
handled in  time, which are the most
studied cases, and we provide better insight into why the related
methods are faster on smooth inputs.
\end{abstract}

\keywords{data structures, computational complexity, search tree}




\section{Introduction}

The predecessor problem involves maintaining a set of linearly
ordered keys and performing queries on that set. The basic variant allows insertions, deletions, and -- more complex to perform -- \emph{predecessor} query: find the largest contained key that is less than a given value. 

This paper investigates the predecessor problem on inputs described by  a~well studied class of ``smooth'' probability distributions.
In the following section we formalize models that we use for input and computation,
and we introduce the class of smooth distributions. In~Section~\ref{sec:smooth-bucket} we
prove a key property of smooth distributions that seems to be
implicitly used in all related work (in weaker forms) as the most
important step to achieve the stated performance. In~Section~\ref{sec:main-results}
we show how to utilize this property directly with well-known distribution-independent
structures for the predecessor problem to get better performance in
a simpler way, essentially by converting to the case of polynomial-sized
universe.

\section{Definitions \label{sec:defs}}

We assume the usual word-RAM~\cite{HagerupT98}
as our computational model, and additionally we restrict keys to word-sized
integers. That can be shown not to be a~significant limitation, as
many other key types can be converted to the integer case cheaply,
for example standard floating-point numbers \cite[sec. 2.1.3]{Goldberg91}
and strings \cite{AndersT01}.

We study the behavior of the predecessor problem in cases where the input
has certain specific properties, which requires us to specify how we model the input:
\begin{defn}[input model]
Insertions take keys distributed according to a~density~ that
does not change during the whole life of the structure. Deletions
remove uniformly from the set of \emph{contained} elements. All the
operations are arbitrarily intermixed and mutually independent.
\end{defn}
This model is convenient because it preserves distribution of the
stored set~â€“ at any point of execution the set appears like a sample
chosen anew according to . That is usually essential for simplifying complexity
analyses in related structures. Various random deletion models were
thoroughly studied by Knuth~\cite{Knuth77}.

Most of related papers work with key distributions that can be described
as \emph{-smooth} for some
constants . Typically it is assumed that the particular
density of the distribution is not known, but  and 
are fixed known parameters.
\begin{rem*}
If the distribution \emph{was} known and its inverse cumulative distribution
function  was cheaply computable (or approximable), we could
convert the problem to the uniform case (or another bounded density).
We could simply store keys transformed by , which would preserve
the order. With any bounded input distribution, all operations can
be easily handled in constant time by simply splitting the the key
domain into  intervals and mapping each to an element
of an array.\footnote{We use  to denote the current size of the stored set, following
the custom in data structures.} This solution for bounded distributions was pointed out already by
Andersson and Mattsson \cite[sec. 5.2]{AndersM93}.
\end{rem*}
The concept of a smooth probability density was originally introduced by Mehlhorn, Tsakalidis \cite{MehlT93} and later generalized by Andersson, Mattsson \cite{AndersM93}. We only amend the definition given in \cite{AndersM93,KMSTTZ06} by allowing .\begin{defn}\label{def:smooth}
	Let  be the density of a continuous random variable  defined over an interval~. Given two functions  and ,  is called \emphDef{-smooth} if
	
\end{defn}

There is quite a simple intuition behind this complicated condition. It implies that if we cut some interval  into  subintervals and generate  values according to density  restricted to , every subinterval is expected to get  elements.

\begin{rem*}
This definition, as written, makes only sense for \emph{continuous} probability
distributions, yet any realistic models of computation can
\emph{not} assume handling arbitrary non-discrete values in constant
space and time per operation. Therefore we assume implicit rounding
when inserting a key, thus converting to the nearest value representable
in a single RAM word. It seems reasonable that the related research
assumed something similar without stating it explicitly.
\end{rem*}



\section{Static analysis of bucketing}
 \label{sec:smooth-bucket}
In this section we show how smoothness implies that an arbitrary sufficiently
short interval is expected to get only a~constant number of input keys.
\begin{lem*}
\refstepcounter{lemma}
\label{lem:smooth-bucket}
Given  and a positive
integer , let us independently draw  keys from a -smooth
distribution, and split its whole domain into at least 
equally long intervals. Then the expected number of keys in an~interval
is .
\end{lem*}
\begin{proof}
The smoothness (Definition~\ref{def:smooth}) over the domain  gives us:

We choose to cover the whole domain . The conditioning can be removed because it is always fulfilled.


Now we consider splitting the domain into  equally long intervals.
Let us choose  as the endpoint of an arbitrary
interval  and choose , so
 and thus the above probability covers at least the whole interval . That gives us:
 
The input keys are chosen independently, so the number of keys in  is given by the binomial distribution, and its expected value
 is in .
\end{proof}

\begin{rem*}
We showed that the number of keys in an interval is expected to be constant, but the tail of that binomial distribution can be bounded even more tightly, e.\,g.~by Chernoff bounds~\cite[chapter~4.1]{randomAlgs}, to guarantee that high values are exponentially rare.
We do not elaborate on a finer analysis, because we feel a more pressing problem that we do not solve in this paper~-- the unrealistic (unmotivated) character of the used input model where all keys are inserted \emph{independently} from a (partially) known distribution.
We use those assumptions for our analysis to enable comparison with known structures, as we have only found one published structure that uses a different model~\cite{DemaineJP04} and moreover the bounds implied in that case seem relatively weak.
\end{rem*}



\section{Combining with known results and comparing to related work \label{sec:main-results}}

We propose to split the bit representation of every key into two parts,
exactly as one step of the decomposition from van Emde Boas trees,
except that we split asymmetrically. In both parts we propose to use
standard structures that do not utilize distribution properties of
the input. The speed on the more significant bits will be due to the
keys being short, and for the less significant bits the number of
keys will be small in expected case due to smoothness.

\subsection{Reviewing standard vEBT decomposition}

Standard van Emde Boas trees solve the predecessor problem with all
operations needing  time when working with keys of 
bits. That assumes  is not asymptotically larger than the word
length of the used word-RAM.

The well-known decomposition can be viewed as performing binary search
for the longest prefix of bit-representation that is shared by the
searched key and some of the stored keys. The structure stores a mapping
from the more significant halves to corresponding subsets of the less
significant halves of the stored keys. These subsets are stored recursively
in the same way, and also the set of occurring more significant halves
is stored recursively. The operations on vEBT are carefully designed
to perform at most one nontrivial recursive call on each recursion
level, and all the rest are constant-time operations.

To keep the space linear\footnote{In word-RAM data structures, linear space means using  words
of memory.}, some modifications to the original structure are required. The mappings
can be represented by hash tables instead of simple arrays, and indirection
can be used: the whole set is split into -sized consecutive
clusters in a fashion similar to B-tree nodes, and the actual vEBT construction
is only applied to the whole clusters joined together by a doubly
linked list. These changes make the complexity of insertions and deletions
expected and amortized where the expectation is only over random bits.

A possible way of doing these modifications is described in more details
e.g.~in~\cite[chapter 4]{Cunat10}, including the algorithms for
operations which can also be found in textbooks \cite[chapter 20]{intro2alg}.

\subsection{Modifying the vEBT decomposition}

To cover the cases most studied in previous work, we focus on
-smooth distributions while using linear space.
To satisfy assumptions of the \ref{lem:smooth-bucket}, our top-level decomposition
needs to split away at least the most significant
 bits.\footnote{In case there are not that many bits in the word, we can ``split
all of them'', meaning we basically use vEB trees directly while
fitting into the stated complexity bounds.} If the values of parameters are unknown, we can split away more bits
to be asympotically certain, e.g. .

These bounds on the number of bits change during the life of the structure,
but we can easily ensure that we cut at least that many by performing
a full rebuild after every  modifying operations where 
denotes the size of the stored set during the last rebuild. It would
also be possible to deamortize that process by standard technique
of global rebuilding \cite{global-rebuilding}. Also note that 
only for  which is an unknown constant; thus
the performance will be constant in that base case.

We can use standard hashed vEBT on the more significant bits to get

expected amortized time per operation. Each substructure for the less
significant bits is expected to store  keys. The resulting
performance is essentially the same as that of~Kaporis et~al.~\cite{KMSTTZ06},
except that unlike all previous work we achieve linear space without
requiring , and our approach is conceptually much simpler.
\begin{thm*}There is a structure for solving the predecessor problem in 
expected time per operation and linear space on any -smooth
distribution for arbitrary . The distribution and
parameters do not need to be known.
\end{thm*}



\subsection{Concluding remarks}

If we wanted to cut down time on unfavorable input, we could use the
same approach as~Kaporis et~al. There are standard structures for
the predecessor problem working in time 
per operation on arbitrary input (considering the less precise bounds),
so these can be used for the less significant bits. Note that when
handling the more significant bits, the expected amortized time 
remains independent of the input, as it only comes from randomization
during hashing.

We feel that it is good to decouple most of the analysis from complex
conditions on the input, as especially in practice we can rarely assume
that all operations on the structure are independent and identically
distributed. For the approach of our paper to work, it is enough (informally)
that the information is only carried by a sufficiently short bit prefix
of the keys.

\section*{Acknowledgements}

Most of the results presented above were included in the author's master thesis~\cite{Cunat10}. The author wishes to dedicate this paper to the memory of his supervisor V\'aclav Koubek, who passed away before this work was completed.


\bibliographystyle{plain} \bibliography{refs}


\end{document}
