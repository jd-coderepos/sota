\section{Experiments}\label{sec:experiments}

\begin{table*}[t!]
\renewcommand{\arraystretch}{1.1}
\centering
\begin{tabular}{l*{9}{r}}
\toprule
&  \multicolumn{5}{c}{\textbf{Overlap}} & \multicolumn{1}{c}{\textbf{mAP}} & \multicolumn{2}{c} {\textbf{IoU}} \\
			& P@0.5 & P@0.6 & P@0.7 & P@0.8 & P@0.9 & 0.5:0.95 & Overall & Mean \\
\cmidrule(lr){2-6} \cmidrule(lr){7-7} \cmidrule(lr){8-9}
Hu~\etal~\cite{hu2016segmentation} & 7.7 & 3.9 & 0.8 & 0.0 & 0.0 & 2.0 & 21.3 & 12.8 \\
Li~\etal~\cite{li2017tracking} & 10.8 & 6.2 & 2.0 & 0.3 & 0.0 & 3.3 & 24.8 & 14.4  \\
Hu~\etal~\cite{hu2016segmentation}~ & 34.8 & 23.6 & 13.3 & 3.3 & 0.1 & 13.2 & 47.4 & 35.0 \\
Li~\etal~\cite{li2017tracking}~ & 38.7 & 29.0 & 17.5 & 6.6 & 0.1 & 16.3 & 51.5 & 35.4 \\
\cmidrule{1-9}
\textit{This paper: RGB} & 47.5 & 34.7 & 21.1 & 8.0 & 0.2 & 19.8 & 53.6 & 42.1 \\ 
\textit{This paper: RGB + Flow} & \textbf{50.0} & \textbf{37.6} & \textbf{23.1} & \textbf{9.4} & \textbf{0.4} & \textbf{21.5} & \textbf{55.1} & \textbf{42.6} \\
\bottomrule
\end{tabular}
\smallskip
\caption{Segmentation from a sentence on A2D Sentences. Object segmentation baselines~\cite{hu2016segmentation,li2017tracking} as proposed in the original papers, or fine-tuned on the A2D Sentences train split (denoted by ). Our model outperforms both baselines for all metrics. Incorporating Flow in our video model further improves results.}
\label{table:experiments:actoraction_res}
\end{table*}


\subsection{Ablation Study} 

In the first set of experiments we study the impact of individual components on our proposed model.

\textbf{Setup.} We select A2D Sentences for these set of experiments and use the train split for training and the test split for evaluation. The input to our model is a sentence describing what to segment and a video clip of  RGB frames around the frame to be segmented.

\textbf{Evaluation.} We adopt the widely used intersection-over-union (IoU) metric to measure segmentation quality. As aggregation metric we consider \textit{overall IoU}, which is computed as total intersection area of all test data over the total union area.

\textbf{Results on A2D Sentences.} We first evaluate the influence of the number of input frames on our visual encoder and the segmentation result. We run our model with  and we get , , , and  respectively in terms of \textit{overall IoU}. It reveals the important role of the large temporal context for actor and action video segmentation. Therefore, we choose  for all remaining experiments.

Next we compare our 1D convolutional textual encoder with an LSTM encoder. We follow the same setting for LSTM as in~\cite{hu2016segmentation,li2017tracking}, we use a final hidden state of LSTM as textual representation for the whole sentence. The dimension of the hidden state is set to . We represent words by the same word2vec embedding model for both models. We observe that our simple 1D convolutional textual encoder outperforms LSTM in terms of \textit{overall IoU}:  for our encoder and  for LSTM. We also experimented with bidirectional LSTM which slightly improves results over vanilla LSTM to . Therefore, we select the convolutional neural network to encode the textual input in the remaining experiments.

We further investigate the importance of our multi-resolution loss. We compare the setting when we are using all three resolutions to compute the loss () with the setting when only the highest resolution is used (). In terms of \textit{overall IoU} the multi-resolution setting performs  while single resolution performs . This demonstrates the benefit of the multi-resolution loss in our model.

\begin{figure*}[t!]
    \centering 
    \includegraphics[width=0.99\textwidth]{images/visualization.pdf} 
    \caption{Visualized segmentation results from our model on A2D Sentences. The first row shows a video with single actor and action, while the video in the second row contains similar types of actors performing the same action. In the third row, we illustrate a video with three sentences describing not only different actors, but also the same type of actor performing different actions. The colored segmentation masks are generated from the sentence with the same color above each video.}
    \label{fig:visualization}
\end{figure*}

In the last experiment we study the impact of the two-stream~\cite{SimonyanNIPS2014} approach for our task. We make a comparison for two type of inputs - RGB and Flow. For both streams we use 16 frames as input. The RGB stream produces better results than Flow:  for RGB and  for Flow. We then explore a fusion of RGB and Flow streams by computing a weighted average of the response maps from each stream. When we set the weight for RGB 2 times larger than Flow, it further improves our results to .


\subsection{Segmentation from a sentence}
\label{sec:experiments:actoraction}
In this experiment, we segment a video based on a given natural language sentence on the newly annotated A2D Sentences and J-HMDB Sentences datasets and compare our proposed model with the baseline methods.

\textbf{Setup.} As there is no prior work for video segmentation from a sentence, we select two methods~\cite{hu2016segmentation, li2017tracking}, which can be used for the related task of image segmentation from a sentence, as our baselines. To be precise, we compare with the segmentation model of~\cite{hu2016segmentation} and the lingual specification model of~\cite{li2017tracking}. We report baseline results in two training settings. In the first one, the baselines are trained solely on the ReferIt dataset~\cite{sahar2014referit}, as indicated in the original papers. In the second setting we further fine-tune the baseline models using the training videos from A2D Sentences. We train our model only on the train split of A2D Sentences. During test, we follow~\cite{xu2015fly} and evaluate the models on each frame of the test videos for which segmentation annotation is available - around one to three frames per video. The input to both baseline models is an RGB frame with a sentence description. For our model, we use the same sentence as input but instead of a single RGB frame we employ 16 frames around the frame to be segmented as this setting shows the best results in our ablation study.

\textbf{Evaluation.} In addition to \textit{overall IoU}, we also consider \textit{mean IoU} as aggregation. The \textit{mean IoU} is computed as the average over the IoU of each test sample.
While the \textit{overall IoU} favors large segmented regions, \textit{mean IoU} treats large and small regions equally. In addition, following~\cite{hu2016segmentation,li2017tracking}, we also measure precision at five different overlap values ranging from  to  as well as the mean average precision over ~\cite{lin2014microsoft}.


\textbf{Results on A2D Sentences.} In Table~\ref{table:experiments:actoraction_res}, we report the results on the A2D Sentences dataset. 
The model of~\cite{hu2016segmentation} and~\cite{li2017tracking}, pretrained on ReferIt~\cite{sahar2014referit}, performs modestly as this dataset contains rich sentences describing objects, but it provides less information about actions. Fine-tuning these two baselines on A2D Sentences helps improve their performance by incorporating the notion of actions into the models. Our model outperforms both baselines for all metrics using RGB frames as input, bringing  absolute improvement in ,  in \textit{overall IoU} and  in \textit{mean IoU}. Fusion of RGB and Flow streams further improves our results. The larger improvement in \textit{mean IoU} compared to \textit{overall IoU} indicates our model is especially better on segmenting small objects. The results in mAP show the benefit of our model for larger overlap values. We visualize some of the sentence-guided segmentation results in Figure~\ref{fig:visualization}. First of all, our model can tackle the scenarios when the actor is not in the frame, \eg in the second video. The model stops generating the segmentation once the man has left the camera's view.  Our model can also tackle the scenarios when the actor is performing an action which is different from the one specified in the sentence, \eg in the first video. The model doesn't output any segmentation for the frames in which the car is not in the \textit{jumping} state. It shows the potential of our model for spatio-temporal video segmentation. Second, in contrast to segmentation from actor-action labels, we can see from the second video that our segmentation from a sentence enables to distinguish the instances of the same actor-action pair by richer descriptions. In the third video, our model confuses two dogs, still we easily segment different types of actors.

\textbf{Results on J-HMDB Sentences.} We further evaluate the generalization ability of our model and the baselines. We test the models, finetuned or trained on A2D Sentences, on all  videos of J-HMDB Sentences dataset without any additional finetuning. For each video, we uniformly sample three frames for evaluation following the same setting as in the previous experiment. We report our results in Table~\ref{table:experiments:jhmdb_res}.

J-HMDB Sentences focuses exclusively on human actions and  out of  actions overlap with actions in A2D Sentences, namely \textit{climb stairs}, \textit{jump}, \textit{walk}, and \textit{run}. 
Consistent with the results on A2D Sentences, our method provides a more accurate segmentation for higher overlap values which is shown by \textit{mAP}. We attribute the better generalization ability to two aspects. The baselines rely on the VGG16~\cite{simonyan2014very} model to represent images, while we are using the video-specific I3D model. The second aspect comes from our textual representation, which can exploit similarity in descriptions of A2D Sentences and J-HMDB Sentences.

\begin{table*}[t]
\renewcommand{\arraystretch}{1.1}
\centering
\begin{tabular}{l*{9}{r}}
\toprule
&  \multicolumn{5}{c}{\textbf{Overlap}} & \multicolumn{1}{c}{\textbf{mAP}} & \multicolumn{2}{c}{\textbf{IoU}} \\
			& P@0.5 & P@0.6 & P@0.7 & P@0.8 & P@0.9 & 0.5:0.95 & Overall & Mean\\
\cmidrule(lr){2-6} \cmidrule(lr){7-7} \cmidrule(lr){8-9} 
Hu~\etal~\cite{hu2016segmentation} & 63.3 & 35.0 & 8.5 & 0.2 & 0.0 & 17.8 & \textbf{54.6} & 52.8  \\
Li~\etal~\cite{li2017tracking} & 57.8 & 33.5 & 10.3 & 0.6 & 0.0 & 17.3 & 52.9 & 49.1 \\
\cmidrule{1-9}
\textit{This paper} & \textbf{69.9} & \textbf{46.0} & \textbf{17.3} & \textbf{1.4} &  0.0 & \textbf{23.3} & 54.1 & \textbf{54.2} \\
\bottomrule
\end{tabular}
\smallskip
\caption{Segmentation from a sentence on J-HMDB Sentences using best settings per model on A2D Sentences, demonstrating generalization ability. Our model generates more accurate segmentations for higher overlap values.}
\label{table:experiments:jhmdb_res}
\end{table*}


\begin{table*}[t!]
\renewcommand{\arraystretch}{1.1}
\centering
\scalebox{0.84}{
\begin{tabular}{l*{10}{r}}
\toprule
&  \multicolumn{3}{c}{\textbf{Actor}} & \multicolumn{3}{c}{\textbf{Action}} & \multicolumn{3}{c}{\textbf{Actor and Action}} \\
			& Class-Average  & Global & Mean IoU & Class-Average & Global & Mean IoU & Class-Average & Global & Mean IoU\\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} 
Xu~\etal~\cite{xu2015fly} & 45.7 & 74.6 & - & 47.0 & 74.6 & - & 25.4 & 76.2 & -  \\
Xu~\etal~\cite{xu2016actor}  & 58.3 & 85.2 & 33.4 & 60.5 & 85.3 & 32.0 & 43.3 & 84.2 & 19.9   \\
Kalogeiton~\etal~\cite{kalogeiton2017joint} & \textbf{73.7} & 90.6 & 49.5 & 60.5 & 89.3 & 42.2 & 47.5 & 88.7 & 29.7  \\
\cmidrule{1-10}
\textit{This paper} & 71.4 & \textbf{92.8} & \textbf{53.7} & \textbf{69.3} & \textbf{92.5} & \textbf{49.4} & \textbf{52.4} & \textbf{91.7} & \textbf{34.8} \\
\bottomrule
\end{tabular}
}
\smallskip
\caption{Semantic segmentation results on the A2D dataset using actor, action and actor+action as input respectively. Even though our method is not designed for this setting, it outperforms the state-of-the-art in most of the cases.}
\label{table:experiments:semanticsegm_res}
\end{table*}



\subsection{Segmentation from actor and action pairs}
\label{sec:experiments:semantic}
Finally, we segment a video from a predefined set of actor and action pairs and compare it with the state-of-the-art segmentation models on the original A2D dataset~\cite{xu2015fly}.

\textbf{Setup.} Instead of input sentences, we train our model on the  valid actor and action pairs provided by the dataset, such as \textit{adult walking} and \textit{dog rolling}. We use these pairs as textual input to our model. Visual input is kept the same as before. 
As our model explicitly requires a textual input for a given video, we select a subset of pairs from all possible pairs as queries to our model. For this purpose, we finetune a multi-label classification network on A2D dataset and select the pairs with a confidence score higher than . We use this reduced set of pairs as queries to our model and pick the class label with the highest response for each pixel. The classification network contains an RGB and a Flow I3D model where the number of neurons in the last layer is set to  and the activation function is replaced by a  for multi-label classification. During training, we finetune the last inception block and the final layer of both models on random 64-frame video clips. We randomly flip each frame horizontally in the video clip and then extract a  random crop. We train for  iterations with the Adam optimizer and fix the learning rate to . During test, we extract 32-frame clips over the video and average the scores across all the clips and across RGB and Flow streams to obtain the final score for a given video. For this multi-label classification we obtain mean average precision of , compared to  in~\cite{xu2015fly}.

\textbf{Evaluation.} We report the class-average pixel accuracy, global pixel accuracy and \textit{mean IoU} as in~\cite{kalogeiton2017joint}. Pixel accuracy is the percentage of pixels for which the label is correctly predicted, either over all pixels (global) or first computed for each class separately and then averaged over classes (class-average).

\textbf{Results on A2D.} We compare our approach with the state-of-the-art in Table~\ref{table:experiments:semanticsegm_res}. Even though our method is not designed for this setting, it outperforms all the competitors for joint actor and action segmentation (last  columns of Table~\ref{table:experiments:semanticsegm_res}). Particularly, we improve the state-of-the-art by a margin of  in terms of class-average accuracy and  in terms of Mean IoU.
In addition to joint actor and action segmentation, we report results for actor and action segmentation separately. For actor segmentation the method by Kalogeiton~\etal~\cite{kalogeiton2017joint} is slightly better in terms of class-average accuracy, for all other metrics and settings our method sets a new state-of-the-art. Our improvement is particularly notable on action segmentation where we outperform the state-of-the-art by  in terms of class-average accuracy and  in terms of Mean IoU. It validates that our method is suitable for both actor and action segmentation, be it individually or combined.
