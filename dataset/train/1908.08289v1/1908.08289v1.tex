\documentclass{bmvc2k}

\title{Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation}

\addauthor{Jiahao Lin}{jiahao@comp.nus.edu.sg}{1}
\addauthor{Gim Hee Lee}{gimhee.lee@comp.nus.edu.sg}{1}

\addinstitution{
 Department of Computer Science,\\
 School of Computing, \\
 National University of Singapore\\
}

\runninghead{Lin \etal{}}{Trajectory Space Factorization for 3D Human Pose Estimation}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\usepackage{amsfonts}
\usepackage{commath}
\usepackage[caption=false]{subfig}
\usepackage{booktabs}
\urlstyle{same}
\begin{document}

\maketitle



\begin{abstract}

Existing deep learning approaches on 3d human pose estimation for videos are either based on Recurrent or Convolutional Neural Networks (RNNs or CNNs). However, RNN-based frameworks can only tackle sequences with limited frames because sequential models are sensitive to bad frames and tend to drift over long sequences. Although existing CNN-based temporal frameworks attempt to address the sensitivity and drift problems by concurrently processing all input frames in the sequence, the existing state-of-the-art CNN-based framework is limited to 3d pose estimation of a single frame from a sequential input.
In this paper, 
we propose a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation.  Our approach processes all input frames concurrently to avoid the sensitivity and drift problems, and yet outputs the 3d pose estimates for every frame in the input sequence.
More specifically, the 3d poses in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix.  
The trajectory bases matrix is precomputed from matrix factorization approaches such as 
Singular Value Decomposition (SVD) or Discrete Cosine Transform (DCT), and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. 
We demonstrate the effectiveness of our framework on long sequences by achieving state-of-the-art performances on multiple benchmark datasets.
Our source code is available at: \url{https://github.com/jiahaoLjh/trajectory-pose-3d}.

\end{abstract}



\section{Introduction}
\label{sec:intro}

3d human pose estimation from a 2d image is an important task in computer vision that has a wide range of applications such as human-computer interaction, augmented/virtual reality, camera surveillance, \textit{etc}.
Nonetheless, 3d pose estimation remains a challenging problem due to the inherent ambiguity in the ill-posed problem of lifting 3d pose from a 2d image.

Over the years, numerous research has been done on 3d pose estimation from a single image and a sequence of input frames, respectively. 
Despite the intuition that 3d pose estimation should be improved by having more information from the sequence of input frames, the performance of temporal-based frameworks are unexpectedly limited compared to its single frame counterpart. One of the reasons is sequential frameworks are mostly based on RNNs \cite{hossain2018exploiting, lin2017recurrent, lee2018propagating} that are very sensitive to the quality of the estimate at each time step. Just a few frames of bad estimates would lead to dire results in the estimation for all subsequent frames. 
Recently, CNN-based temporal frameworks \cite{collobert2016wav2letter, dauphin2017language, gehring2017convolutional, van2016wavenet} have shown advantages over RNNs in modeling temporal information. However, concurrently estimating all frames in a long sequence is an arduous task for data-driven approaches due to the increasing dimensionality of the output space with longer sequences, and the network for ``many-to-many mapping" requires significantly more data to train. As a result, the state-of-the-art temporal framework for 3d pose estimation \cite{pavllo20183d} only outputs the estimate of a single frame centered on an input sequence of a few hundred frames.

A key observation is that consecutive frames of human motions are highly correlated and abrupt changes in the motions are unlikely. This suggests that we do not need to estimate the 3d pose in each frame of a sequence independently. Instead, we can effectively reduce the output dimension by decomposing the uncorrelated components. To this end, we propose a deep learning-based framework that utilizes matrix factorization \cite{tomasi1992shape, bregler2000recovering, akhter2009nonrigid} for sequential 3d human poses estimation.
More specifically, the joints of the 3d pose estimates in all frames are represented as a motion matrix factorized into a trajectory bases matrix and a trajectory coefficient matrix. The trajectory bases matrix is precomputed with 
SVD or cosine bases from DCT, and the problem of sequential 3d pose estimation is reduced to training a deep network to regress the trajectory coefficient matrix. 
We show that a relatively small number of bases (compared to the large number of input frames) can effectively approximate the human motions.
We transform the extracted features from Cartesian space to trajectory space and regress the trajectory coefficients with a densely connected multilayer perceptron (MLP). The 3d poses can then be reconstructed from the linear combination of the trajectory bases with the trajectory coefficients regressed from the deep network.

Our main contributions are: (1) we present an effective framework to estimate 3d human poses for a long sequence in the trajectory space; (2) we show experimentally that a small number of trajectory bases are sufficient to model human motions which greatly reduces the complexity of the network training; and (3) our method gives state-of-the-art performance on multiple benchmark datasets.





\section{Related Work}
\label{sec:literature}
Early methods for
3d human pose estimation from 2d images
use a variety of hand-crafted features such as silhouette \cite{agarwal20043d}, shape \cite{mori2006recovering}, SIFT \cite{bo2008fast}, HOG \cite{rogez2008randomized} to estimate 3d human poses. More recently, a large number of works are based on the highly popular and effective deep neural networks. 
Li \etal \cite{li20143d} apply a deep convolutional neural network to jointly regress pose and detect body parts.
Tekin \etal \cite{tekin2016structured} propose to learn a high-dimensional latent pose representation accouting for joint dependencies using auto-encoder.
Pavlakos \etal \cite{pavlakos2017coarse} extend the idea of regressing heat-map from 2d to 3d and use a coarse-to-fine approach to estimate the 3d heat-map for joint estimation.
Sun \etal \cite{sun2017compositional} exploit the skeleton structure and define a compositional loss function to better learn joint interactions.
Lee \etal \cite{lee2018propagating} propose a structure-aware multi-stage architecture to regress body parts incrementally.
Training deep learning models directly from images requires huge amounts of data to be labeled with 3d ground truth. To alleviate this problem, a two-stage framework is proposed in several works. In particular, 2d poses are first estimated and subsequently used for 3d pose estimation.
Yasin \etal \cite{yasin2016dual} and Chen \etal \cite{chen20173d} exploit the idea of 3d pose retrieval by retrieving the nearest 3d poses in a 3d library based on the 2d poses.
Moreno-Noguer \cite{moreno20173d} estimates the 3d pose by regressing a distance matrix from 2d space to 3d space.
Martinez \etal \cite{martinez2017simple} propose a simple yet effective regression network based on fully-connected residual blocks.
Fang \etal \cite{fang2018learning} integrate kinematic knowledge into their framework and design a hierarchical grammar model.
Zhao \etal \cite{zhao2019semantic} propose to use graph neural networks to model structural information in human poses.
Our work also follows the two-stage approach.

The inherent depth ambiguity in 3d pose estimation from monocular images limits the estimation accuracy. Extensive research has been done to exploit extra information contained in temporal sequences.
Zhou \etal \cite{zhou2016sparseness} formulate an optimization problem to search for the 3d configuration with the highest probability given 2d confidence maps and solve the problem using Expectation-Maximization.
Tekin \etal \cite{tekin2016direct} use a CNN to align bounding box of consecutive frames and then generate a spatial-temporal volume based on which they extract 3d HOG features and regress the 3d pose for the central frame.
Mehta \etal \cite{mehta2017vnect} propose a real-time system for 3d pose estimation and apply temporal filtering to yield temporally consistent 3d poses.

Recently, RNN-based frameworks are used to deal with sequential input data.
Lin \etal \cite{lin2017recurrent} use a multi-stage framework based on Long Short-term Memory (LSTM) units to estimate the 3d pose from the extracted 2d features and estimated 3d pose in the previous stage.
Coskun \etal \cite{coskun2017long} propose to learn a human motion model using Kalman Filter and implement it with LSTMs.
Hossain \etal \cite{hossain2018exploiting} design a sequence-to-sequence network with LSTM units to first encode a sequence of motions in the form of 2d joint locations and then decode the 3d poses of the sequence.
However, RNNs are sensitive to erroneous inputs and tend to drift over long sequences.
To overcome the shortcomings of RNNs, a CNN-based framework is proposed by Pavllo \etal \cite{pavllo20183d} to aggregate temporal information using dilated convolutions. Despite being successful at regressing a single frame from a sequence of input, it cannot concurrently output the 3d pose estimations for all frames in the sequence. 

Inspired by matrix factorization methods commonly used in Structure-from-Motion (SfM) \cite{tomasi1992shape} and non-rigid SfM \cite{bregler2000recovering}, several works \cite{ramakrishna2012reconstructing, zhou20153d, zhou2016sparseness} on 3d human pose estimation factorize the sequence of 3d human poses into a linear combination of shape bases. Akhter \etal \cite{akhter2009nonrigid} suggest a duality of the factorization in the trajectory space. We extend the idea of 
matrix factorization to learning a deep network that estimates the coefficients of the trajectory bases from a sequence of 2d poses as inputs. The 3d poses of all frames are recovered concurrently as the linear combinations of the trajectory bases with the estimated coefficients.  



\section{Our approach}
\label{sec:approach}

In this section, we introduce our framework for sequential 3d pose estimation in detail. We formulate the estimation problem as a trajectory space coefficient regression task (Sec \ref{sec:factorization}). In particular, we use fixed trajectory bases, e.g., singular vectors on motion data or DCT bases, as the trajectory bases (Sec \ref{sec:trajectory-bases}), and propose a novel network architecture to effectively regress the trajectory coefficients from a sequence of 2d input (Sec \ref{sec:network}). An overview of our framework is shown in Figure \ref{fig:network}.

\subsection{Non-Rigid Structure Factorization}
\label{sec:factorization}

Given the 2d image coordinates  of  joints in  consecutive frames, the goal is to estimate the 3d Cartesian coordinates  of the  joints in all  frames. Let us denote the 3d coordinates of the  joints in the -th frame as

The entire sequence of  frames are concatenated to form the motion matrix,

where the column space is known as the \textit{trajectory space} \cite{akhter2009nonrigid}. Furthermore,  can be factorized into a linear combination of  trajectory bases, i.e., 

 is the trajectory bases matrix, where each column  is a trajectory basis vector.
Here, the number of trajectory bases  is no greater than the number of frames  since . Given a set of predefined trajectory bases , the estimation of a sequence of 3d poses  turns into the problem of finding the coefficient matrix .
On the other hand, the row space of  is known as the \textit{shape space}, where
the factorization in Equation \ref{equ: factorization} can also be interpreted as the product of a coefficient matrix  in the shape space and a shape bases matrix . The duality of the factorization in the row and column spaces is called \textit{shape-trajectory duality}. 
Unlike approaches \cite{ramakrishna2012reconstructing, zhou20153d, zhou2016sparseness} that estimate the 3d poses in shape space, we follow \cite{akhter2009nonrigid} to formulate the task in the trajectory space. This is because shape bases are highly motion specific, hence, large number of bases are required to achieve low reconstruction error for any arbitrary motion type \cite{ramakrishna2012reconstructing}. In contrast, trajectory space requires significantly fewer dominant bases to model human motions due to the physics constraining the motion acceleration. Additionally, reconstructed motions from linear combination of smooth trajectory bases are temporally consistent.

\subsection{Trajectory Bases}
\label{sec:trajectory-bases}

\begin{figure*}[t]
\centering
\subfloat[]{\includegraphics[width=0.48\linewidth]{images/svd}\label{fig:svd}}
\quad
\subfloat[]{\includegraphics[width=0.48\linewidth]{images/dct}\label{fig:dct}}
\caption{Trajectory bases for  frames. \textbf{(a)}: Singular vectors corresponding to the 3 largest singular values as trajectory bases. \textbf{(b)}: First 3 bases from DCT.}
\label{fig:bases}
\end{figure*}

\paragraph{Singular Value Decomposition (SVD).} The trajectory bases matrix  can be computed from the SVD of the motion matrix , i.e., ,
where  and  are the left and right singular vectors, and  is the diagonal matrix of singular values. 
More specifically,  is the  columns of the left singular vectors  that correspond to the  largest singular values.   
Multiple motion matrices are stacked into a larger matrix 
, where  denotes the total number of motion matrices. The same SVD operation is applied on  to get . A higher number of motion matrices, i.e., large , results in a more accurate trajectory bases matrix . However, in practice
we are limited by the memory needed to compute SVD. Hence, we
compute  from a small portion of the available large scale datasets \eg Human3.6M \cite{ionescu2014human3}. Nonetheless, the result is stable enough to show the trajectory pattern within human motion. 
Figure \ref{fig:svd} shows the plots of the bases computed by SVD on 10k trajectories from Human3.6M. 
Each plot is the  entries of a trajectory basis vector  (y-axis) against its index (x-axis) in the vector.  

\begin{figure*}[t]
\includegraphics[width=\linewidth]{images/truncate}
\caption{\textbf{Left:} Mean of absolute coefficient values corresponding to different DCT bases. The first coefficient corresponding to the DC component of a signal is discarded in the figure. \textbf{Right:} Reconstruction error when truncated to different number of DCT bases.}
\label{fig:truncate}
\end{figure*}

\paragraph{Discrete Cosine Transform (DCT).} The plots of the trajectory bases vectors from Figure \ref{fig:svd} show strong indication of sinusoidal form. Based on this observation, we also explore DCT that compactly model motion trajectories as pointed out in \cite{akhter2009nonrigid}.
For a discrete signal input  of finite length , the DCT is defined as

 stands for the coefficient corresponding to the -th cosine wave . The inverse DCT is used to transform the coefficients back to the original signal, i.e., 

Furthermore, the inverse DCT can be seen as a linear combination of the orthogonal bases  and  with the coefficients , and  is a column in the motion matrix . Limiting  gives us a set of  orthogonal bases that is similar to  mentioned earlier. The advantage of DCT is that the orthogonal bases are predefined without the need to learn from data. Figure \ref{fig:dct} shows the first 3 bases of DCT, which resemble the singular vectors in Figure \ref{fig:svd}.

Figure \ref{fig:truncate} (left) shows an example analysis on a subset of Human3.6M dataset (randomly sampling 100k trajectories with ). We see that bases with  have negligible corresponding coefficients, which is reasonable because human motions usually contain very sparse high-frequency movement. Figure \ref{fig:truncate} (right) further shows that it is sufficient to reconstruct the trajectory with relatively low error even if a very small proportion of bases are utilized. Hence, we only need to regress a low number of coefficients despite large number of frames because the trajectory bases already encodes the temporal correlation within the motion. This greatly reduces the complexity of training deep networks.

\subsection{Network Design and Implementation Details}
\label{sec:network}

\begin{figure*}
\includegraphics[width=\linewidth]{images/network.pdf}
\caption{Our Network Architecture.  frames of  2d joints are fed into a MLP for per frame feature extraction. Each feature channel along the temporal axis is transformed into trajectory space via a \textit{Transformer}. Coefficients from all feature channels are then concatenated and another MLP is applied to regress the  coefficients for all  trajectories.}
\label{fig:network}
\end{figure*}

Figure \ref{fig:network} shows an overview of our network. The input of the network is the 2d image coordinates of  joints from a sequence of  consecutive frames obtained from any off-the-shelf 2d pose estimators, \eg \cite{newell2016stacked, chen2018cascaded}. A MLP block with 4 repeated \textit{Linear-BatchNorm-ReLU-Dropout} structures is used to extract features for each frame. The weights within the block are shared across all frames. Each feature channel across the temporal axis is stacked into a "\textit{virtual feature trajectory}" and is transformed into the trajectory space via a "\textit{Transformer}". The "\textit{Transformer}" is simply an implementation of the DCT. An inner product is computed for the input trajectory and each of the fixed trajectory bases, and is further normalized by multiplying a scale factor of . This results in  coefficients for each feature channel, and all coefficients from all channels are concatenated and passed into another MLP block to regress the final  coefficients for all the  trajectories ( coordinates of  joints). The "\textit{Coefficient Regression}" MLP block consists of 5 repeated \textit{Linear-BatchNorm-ReLU-Dropout} structures, and differs from the "\textit{Feature Extraction}" block for both the linear layer size and dropout rate. Finally, we linearly combine the  trajectory bases using the regressed coefficients to reconstruct the  values for all  joints across  frames.

We empirically found that by densely connecting any two linear layers in the MLP block (originally proposed for convolutional layers by \cite{huang2017densely}), the network is capable of learning the length of the critical path automatically. This helps to boost the regression performance. The skip connection arrow in both "\textit{Feature Extraction}" block and "\textit{Coefficient Regression}" block indicates the usage of dense shortcut links.
Temporal-based frameworks usually suffer from noisy 2d input. To alleviate the detrimental impact from individual noisy input frames, an extra average pooling layer is added after feature extraction block to smoothen the "feature trajectories". We found that a pooling window of length 5 is suitable to reduce noise while preserving much of the underlying signal, and we report all the experiments results under this setting.
Our network is not restricted to any particular input sequence length. It can be adapted to any sequence length by simply substituting the trajectory bases in the "\textit{Transformer}". The number of trajectory bases can also be adjusted by adding (or removing) branches in the "\textit{Transformer}". Experiments on how the number of input frames and the number of trajectory bases affect the performance are shown in Section \ref{sec:result}.

We supervise the training of our network by minimizing \textit{L}1-loss over all  sequences

because \textit{L}1-loss is more robust to outliers which are common due to occlusions. Here  denotes the estimated 3d poses while  denotes the corresponding ground truth poses. We train our model using Adam \cite{kingma2014adam} optimizer for 100 epochs with initial learning rate of . We schedule the learning rate decay at epoch 60 and 85 with a shrink factor . We also perform data augmentation by horizontally flipping the human pose. During inference, estimated 3d poses for both original and flipped data are averaged as the final estimation.



\section{Experiments}
\label{sec:result}

\paragraph{Experimental setup.} We focus our evaluation on two widely used datasets: Human3.6M \cite{ionescu2014human3} and MPI-INF-3DHP \cite{mehta2016monocular}. We show both quantitative and qualitative results that demonstrate the effectiveness of our temporal estimation framework. \textbf{Human3.6M} is currently the largest publicly available dataset for human 3d pose estimation. The dataset consists of 3.6 million image frames captured by MoCap system in a constrained indoor studio environment. 11 actors perform 15 everyday activities such as walking, discussing, \textit{etc}. We follow two standard protocols for comparison with previous works. We use 5 subjects (S1, S5, S6, S7, S8) for training and 2 subjects (S9, S11) for testing. The evaluation metric is mean per joint position error (MPJPE) in mm after aligning the hip joint to the origin. We refer to this as {\bf protocol 1}. Several works \cite{bogo2016keep,pavlakos2017coarse,sun2017compositional,lee2018propagating,dabral2018learning,zhou2016sparseness,moreno20173d,nie2017monocular,martinez2017simple,fang2018learning,hossain2018exploiting, pavllo20183d} also report the error after aligning further with respect to the ground truth pose via Procrustes Analysis. We refer to this as {\bf protocol 2}. \textbf{MPI-INF-3DHP} is a recently released 3d dataset with both indoor environment (green background and studio background) and in-the-wild outdoor environment. Similar to \cite{mehta2016monocular}, we report the 3D Percentage of Correct Keypoints (PCK) with threshold of 150mm and Area Under Curve (AUC) for a range of PCK thresholds.

We follow \cite{pavllo20183d} in using 2d detections from the \textit{Cascaded Pyramid Network (CPN)} \cite{chen2018cascaded} as our network input. Instead of down-sampling, which is commonly done in many works that use a single-frame input, we keep the original frame rate (50fps for Human3.6M and 25fps for MPI-INF-3DHP) because having access to the complete sequence provides more detailed information. 
The trajectory bases are the SVD or DCT bases described in Section \ref{sec:trajectory-bases}
for any predefined number of input frames . More specifically, we perform SVD on the motion matrix formed with 10k randomly sampled trajectories from the training set to get the SVD bases. The DCT bases are the cosine bases defined in Equation \ref{equ:dct1} and \ref{equ:dct2}. 
We train the network with a fixed number of  trajectory bases and  input frames. Nonetheless, we use the ``sliding window approach" on input videos with  frames. In particular, we infer the 3d poses on a sliding window of  frames 
for an input video with an arbitrary  frames. We move the sliding window at a step of  frames, where  is set to 5 for all experiments shown in this paper.
Since the sliding window runs through most of the frames multiple times (except for the first and last frames), 
we compute the final pose for each frame as the average of all the 3d poses estimated for that frame. 
We note that  can range from 300 to 6000 frames for all the videos of the two datasets used in our experiments.


\renewcommand{\arraystretch}{1.1}

\begin{table*}[t]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l|rrrrrrrrrrrrrrr|r}
            \hline
            Protocol 1 & Direct. & Discuss & Eating & Greet & Phone & Photo & Pose & Purch. & Sitting & SitingD & Smoke & Wait & WalkD & Walk & WalkT & Avg \\
            
            \hline \hline
            
            Pavlakos \etal \cite{pavlakos2017coarse} & 67.4 & 71.9 & 66.7 & 69.1 & 72.0 & 77.0 & 65.0 & 68.3 & 83.7 & 96.5 & 71.7 & 65.8 & 74.9 & 59.1 & 63.2 & 71.9 \\
            Tekin \etal \cite{tekin2017learning} & 54.2 & 61.4 & 60.2 & 61.2 & 79.4 & 78.3 & 63.1 & 81.6 & 70.1 & 107.3 & 69.3 & 70.3 & 74.3 & 51.8 & 63.2 & 69.7\\
            Martinez \etal \cite{martinez2017simple} & 51.8 & 56.2 & 58.1 & 59.0 & 69.5 & 78.4 & 55.2 & 58.1 & 74.0 & 94.6 & 62.3 & 59.1 & 65.1 & 49.5 & 52.4 & 62.9\\
            Fang \etal \cite{fang2018learning} & 50.1 & 54.3 & 57.0 & 57.1 & 66.6 & 73.3 & 53.4 & 55.7 & 72.8 & 88.6 & 60.3 & 57.7 & 62.7 & 47.5 & 50.6 & 60.4\\
            Sun \etal \cite{sun2017compositional} & 52.8 & 54.8 & 54.2 & 54.3 & 61.8 & 67.2 & 53.1 & 53.6 & 71.7 & 86.7 & 61.5 & 53.4 & 61.6 & 47.1 & 53.4 & 59.1\\
            Yang \etal \cite{yang20183d} & 51.5 & 58.9 & 50.4 & 57.0 & 62.1 & 65.4 & 49.8 & 52.7 & 69.2 & 85.2 & 57.4 & 58.4 & 60.1 & 43.6 & 47.7 & 58.6\\
            Zhao \etal \cite{zhao2019semantic} & 47.3 & 60.7 & 51.4 & 60.5 & 61.1 & 67.8 & 49.9 & 47.3 & 68.1 & 86.2 & 55.0 & 61.0 & 60.6 & 42.1 & 45.3 & 57.6\\
            Lee \etal \cite{lee2018propagating} () & 43.8 & 51.7 & 48.8 & 53.1 & 52.2 & 74.9 & 52.7 & 44.6 & 56.9 & 74.3 & 56.7 & 66.4 & 68.4 & 47.5 & 45.6 & 55.8\\
            Dabral \etal \cite{dabral2018learning} (SAP-Net) & 46.9 & 53.8 & 47.0 & 52.8 & 56.9 & 63.6 & 45.2 & 48.2 & 68.0 & 94.0 & 55.7 & 51.6 & 55.4 & 40.3 & 44.3 & 55.5\\
            
            \hline \hline
            
            Zhou \etal \cite{zhou2016sparseness} & 87.4 & 109.3 & 87.1 & 103.2 & 116.2 & 143.3 & 106.9 & 99.8 & 124.5 & 199.2 & 107.4 & 118.1 & 114.2 & 79.4 & 97.7 & 113.0\\
            Lin \etal \cite{lin2017recurrent} & 58.0 & 68.2 & 63.3 & 65.8 & 75.3 & 93.1 & 61.2 & 65.7 & 98.7 & 127.7 & 70.4 & 68.2 & 72.9 & 50.6 & 57.7 & 73.1\\
            Hossain \& Little \cite{hossain2018exploiting} & 48.4 & 50.7 & 57.2 & 55.2 & 63.1 & 72.6 & 53.0 & 51.7 & 66.1 & 80.9 & 59.0 & 57.3 & 62.4 & 46.6 & 49.6 & 58.3\\
            Lee \etal \cite{lee2018propagating} () & \textbf{40.2} & 49.2 & 47.8 & 52.6 & 50.1 & 75.0 & 50.2 & 43.0 & \textbf{55.8} & 73.9 & 54.1 & 55.6 & 58.2 & 43.3 & 43.3 & 52.8\\
            Dabral \etal \cite{dabral2018learning} (TP-Net) & 44.8 & 50.4 & 44.7 & 49.0 & 52.9 & 61.4 & 43.5 & 45.5 & 63.1 & 87.3 & 51.7 & 48.5 & 52.2 & 37.6 & 41.9 & 52.1\\
            Pavllo \etal \cite{pavllo20183d} & 45.2 & 46.7 & 43.3 & 45.6 & \textbf{48.1} & \textbf{55.1} & 44.6 & 44.3 & 57.3 & 65.8 & \textbf{47.1} & 44.0 & 49.0 & \textbf{32.8} & \textbf{33.9} & 46.8\\
            
            Ours () & 43.2 & 46.7 & 45.5 & 46.1 & 51.5 & 59.2 & 44.5 & 42.9 & 58.0 & 66.2 & 49.2 & 45.5 & 50.3 & 36.5 & 39.6 & 48.8\\
            Ours () & 42.7 & 45.5 & 43.1 & 44.6 & 49.5 & 57.3 & 43.1 & 41.7 & 57.5 & 65.0 & 48.0 & 43.6 & 48.5 & 34.2 & 36.9 & 47.3\\
            Ours () & 42.5 & \textbf{44.8} & \textbf{42.6} & \textbf{44.2} & 48.5 & 57.1 & \textbf{42.6} & \textbf{41.4} & 56.5 & \textbf{64.5} & 47.4 & \textbf{43.0} & \textbf{48.1} & 33.0 & 35.1 & \textbf{46.6}\\
            \hline
        \end{tabular}
    }
    \caption{Results on Human3.6M under Protocol 1 (no rigid alignment for post-processing). Top half of the table are single-frame works. Bottom half of the table are multi-frame works. Bold-faced numbers indicate best results.}
    \label{tab:protocol1}
\end{table*}

\begin{table*}[t]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l|rrrrrrrrrrrrrrr|r}
            \hline
            Protocol 2 & Direct. & Discuss & Eating & Greet & Phone & Photo & Pose & Purch. & Sitting & SitingD & Smoke & Wait & WalkD & Walk & WalkT & Avg \\
            
            \hline \hline
            
            Pavlakos \etal \cite{pavlakos2017coarse} & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 51.9 \\
            Sun \etal \cite{sun2017compositional} & 42.1 & 44.3 & 45.0 & 45.4 & 51.5 & 53.0 & 43.2 & 41.3 & 59.3 & 73.3 & 51.0 & 44.0 & 48.0 & 38.3 & 44.8 & 48.3\\
            Martinez \etal \cite{martinez2017simple} & 39.5 & 43.2 & 46.4 & 47.0 & 51.0 & 56.0 & 41.4 & 40.6 & 56.5 & 69.4 & 49.2 & 45.0 & 49.5 & 38.0 & 43.1 & 47.7\\
            Lee \etal \cite{lee2018propagating} () & 38.0 & 39.3 & 46.3 & 44.4 & 49.0 & 55.1 & 40.2 & 41.1 & 53.2 & 68.9 & 51.0 & 39.1 & 56.4 & 33.9 & 38.5 & 46.2\\
            Fang \etal \cite{fang2018learning} & 38.2 & 41.7 & 43.7 & 44.9 & 48.5 & 55.3 & 40.2 & 38.2 & 54.5 & 64.4 & 47.2 & 44.3 & 47.3 & 36.7 & 41.7 & 45.7\\
            Dabral \etal \cite{dabral2018learning} (SAP-Net) & 32.8 & 36.8 & 42.5 & 38.5 & 42.4 & 49.0 & 35.4 & 34.3 & 53.6 & 66.2 & 46.5 & 34.1 & 42.3 & 30.0 & 39.7 & 42.2\\
            
            \hline \hline
            
            Hossain \& Little \cite{hossain2018exploiting} & 35.7 & 39.3 & 44.6 & 43.0 & 47.2 & 54.0 & 38.3 & 37.5 & 51.6 & 61.3 & 46.5 & 41.4 & 47.3 & 34.2 & 39.4 & 44.1\\
            Pavllo \etal \cite{pavllo20183d} & 34.1 & 36.1 & 34.4 & 37.2 & \textbf{36.4} & \textbf{42.2} & 34.4 & 33.6 & 45.0 & 52.5 & \textbf{37.4} & 33.8 & 37.8 & 25.6 & \textbf{27.3} & 36.5\\
            Dabral \etal \cite{dabral2018learning} (TP-Net) & \textbf{28.0} & \textbf{30.7} & 39.1 & \textbf{34.4} & 37.1 & 44.8 & \textbf{28.9} & \textbf{31.2} & \textbf{39.3} & 60.6 & 39.3 & \textbf{31.1} & 37.8 & \textbf{25.3} & 28.4 & \textbf{36.3}\\
            
            Ours () & 32.9 & 36.4 & 35.8 & 37.3 & 39.2 & 44.2 & 34.0 & 33.0 & 46.5 & 53.8 & 39.7 & 34.3 & 40.0 & 27.8 & 32.8 & 38.3\\
            Ours () & 32.8 & 35.7 & 34.4 & 36.1 & 38.1 & 43.3 & 33.0 & 32.5 & 46.4 & 52.7 & 38.7 & 33.1 & 38.5 & 26.3 & 30.7 & 37.3\\
            Ours () & 32.5 & 35.3 & \textbf{34.3} & 36.2 & 37.8 & 43.0 & 33.0 & 32.2 & 45.7 & \textbf{51.8} & 38.4 & 32.8 & \textbf{37.5} & 25.8 & 28.9 & 36.8\\
            \hline
        \end{tabular}
    }
    \caption{Results on Human3.6M under Protocol 2 (after rigid alignment for post-processing). Top half of the table are single-frame works. Bottom half of the table are multi-frame works. Bold-faced numbers indicate best results.}
    \label{tab:protocol2}
\end{table*}

\paragraph{Results.} Table \ref{tab:protocol1} shows the results on Human3.6M under protocol 1. We report the performance of our model for input length  (with  DCT bases),  (with  DCT bases), and  (with  DCT bases) corresponding to motion of 0.2s, 0.5s and 1s respectively. Our approach with  outperforms all previous single-frame (top half of the table) and temporal-based (bottom half of the table) approaches. Table \ref{tab:protocol2} further shows the results after rigid alignment with the ground truth. Under this protocol, we also show results that are on par with the existing state-of-the-art \cite{pavllo20183d,dabral2018learning}. It is interesting to note the significant improvements of non RNN-based frameworks (\cite{pavllo20183d, dabral2018learning} and ours) over the RNN-based framework \cite{hossain2018exploiting}. Nonetheless, \cite{pavllo20183d, dabral2018learning} estimate only one output frame from a sequence of input frames, while our approach generates stable 3d pose estimates for every frame of an input sequence.
We report the average error of each frame in an input length of  frames in Figure \ref{fig:sequence}. Here, we set the video length to be the same as our network input length, i.e., .  
The drop in performance for the frames near both ends are probably due to the small number of bases used in our network.
Overall, our framework is able to generate stable estimation for majority of the frames even for longer sequences.

\begin{table*}[t]
    \centering
    \subfloat[]{
        \scalebox{0.6}{
            \begin{tabular}[t]{l|c||l|c}
                \hline
                GT 2d & MPJPE & GT 2d & MPJPE \\
                \hline\hline
                Martinez \etal \cite{martinez2017simple} & 45.5 & Pavllo \etal \cite{pavllo20183d}  & 37.2\\
                Zhao \etal \cite{zhao2019semantic} & 43.8 & Ours (DCT) ()  & 34.4\\
                Lee \etal \cite{lee2018propagating} () & 40.9 & Ours (DCT) ()  & 33.0\\
                Hossain \& Little \cite{hossain2018exploiting}  & 39.2 & Ours (DCT) ()  & \textbf{32.8} \\
                Lee \etal \cite{lee2018propagating} ()  & 38.4 & Ours (SVD) ()  & 32.9\\
                \hline
            \end{tabular}
        }
        \label{tab:gt}
    }
    \quad
    \subfloat[]{
        \scalebox{0.6}{
            \begin{tabular}[t]{l|ccc}
                \hline
                MPI-INF-3DHP & PCK & AUC & MPJPE \\
                \hline \hline
                Mehta \etal \cite{mehta2016monocular} & 75.7 & 39.3 & - \\
                Mehta \etal \cite{mehta2017vnect} (ResNet 50)  & 77.8 & 41.0 & - \\
                Mehta \etal \cite{mehta2017vnect} (ResNet 100)  & 79.4 & 41.6 & - \\
                Ours ()  & \textbf{83.6} & \textbf{51.4} & \textbf{79.8} \\
                Ours ()  & 82.4 & 49.6 & 81.9\\
                \hline
            \end{tabular}
        }
        \label{tab:mpi}
    }
    \caption{\textbf{(a)}: Results on Human3.6M under Protocol 1 using ground truth 2d input. \textbf{(b)}: Results on MPI-INF-3DHP using ground truth 2d input.  indicates methods using temporal information (including ours). Bold-faced numbers indicate best results.}
\end{table*}

We further evaluate our approach with ground truth 2d poses on Human3.6M as the input. As shown in Table \ref{tab:gt}, our approach significantly improves previous best result by 4.4mm (11.8\%). This demonstrates that regression in the trajectory space is effective in generating highly accurate estimation even though the framework concurrently estimates the 3d poses for a long sequence of frames.
We also report the comparison of using SVD and DCT bases in Table \ref{tab:gt}. Although the SVD bases are computed on a subset of the dataset, it achieves similar result as DCT bases. This suggests that the model is not restricted to any specific bases. We leave the exploration on other bases as future work.
To demonstrate the generalizability of our framework, we evaluate the performance of our model on MPI-INF-3DHP in Table \ref{tab:mpi} with all previous works using ground truth 2d locations as input.  frames (with  DCT bases) and  frames (with  DCT bases) are reported for our model. Both settings outperform previous best results under all metrics. Qualitative visualizations are shown in Figure \ref{fig:seq-outdoor}.

\begin{figure*}[t]
\subfloat[]{\includegraphics[width=0.5\linewidth]{images/sequence_error}\label{fig:sequence}}
\subfloat[]{\includegraphics[width=0.5\linewidth]{images/frames_vs_bases}\label{fig:frames-vs-bases}}
\caption{\textbf{(a)}: Average per frame error within a sequence of different sequence lengths. \textbf{(b)}: Estimation error on Human3.6M for different numbers of frames and bases.}
\end{figure*}

\begin{figure*}[t]
\subfloat{\includegraphics[width=\linewidth]{images/h36m}}\\
\subfloat{\includegraphics[width=0.5\linewidth]{images/mpi_1}}
\subfloat{\includegraphics[width=0.5\linewidth]{images/mpi_2}}
\caption{Qualitative results for both indoor and outdoor videos. First row are 2d inputs. Second row are estimated 3d poses. Third row are ground truth 3d poses.}
\label{fig:seq-outdoor}
\end{figure*}

We also conduct experiments on how the number of input frames  and bases  affect the performance of the model. Estimation errors on Human3.6M for various  and  settings are shown in Figure \ref{fig:frames-vs-bases}.
We show the plots of  at  and  at  because the number of bases cannot be more than the number of frames, i.e., . The plots for  are truncated at  since the errors stabilized approximately after . We can also see that errors converged to mm for , where . This suggests that our network design is versatile and outputs consistent results over different length of input frames.
Although we empirically show the results of our network over  in all the previous results on Human3.6M, the results in Figure \ref{fig:frames-vs-bases} demonstrate that our network is not limited to any fixed number of input frames.




\section{Conclusion}
\label{sec:conclusion}
We proposed a deep learning-based framework that utilizes matrix factorization for sequential 3d human poses estimation. In particular, we showed that the 3d poses in all frames can be represented as a motion matrix factorized into a small number of trajectory bases, and a set of trajectory coefficients. We turned the problem of sequential 3d pose estimation into training a deep network to regress the set of trajectory coefficients from all the input frames. The effectiveness of our framework is demonstrated by achieving state-of-the-art performances on multiple benchmark datasets.


\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Triggs(2004)]{agarwal20043d}
Ankur Agarwal and Bill Triggs.
\newblock 3d human pose from silhouettes by relevance vector regression.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  volume~2, pages 882--888. IEEE, 2004.

\bibitem[Akhter et~al.(2009)Akhter, Sheikh, Khan, and
  Kanade]{akhter2009nonrigid}
Ijaz Akhter, Yaser Sheikh, Sohaib Khan, and Takeo Kanade.
\newblock Nonrigid structure from motion in trajectory space.
\newblock In \emph{Advances in neural information processing systems}, pages
  41--48, 2009.

\bibitem[Bo et~al.(2008)Bo, Sminchisescu, Kanaujia, and Metaxas]{bo2008fast}
Liefeng Bo, Cristian Sminchisescu, Atul Kanaujia, and Dimitris Metaxas.
\newblock Fast algorithms for large scale conditional 3d prediction.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 1--8. IEEE, 2008.

\bibitem[Bogo et~al.(2016)Bogo, Kanazawa, Lassner, Gehler, Romero, and
  Black]{bogo2016keep}
Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero,
  and Michael~J Black.
\newblock Keep it smpl: Automatic estimation of 3d human pose and shape from a
  single image.
\newblock In \emph{European Conference on Computer Vision}, pages 561--578.
  Springer, 2016.

\bibitem[Bregler et~al.(2000)Bregler, Hertzmann, and
  Biermann]{bregler2000recovering}
Christoph Bregler, Aaron Hertzmann, and Henning Biermann.
\newblock Recovering non-rigid 3d shape from image streams.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  volume~2, page 2690. IEEE, 2000.

\bibitem[Chen and Ramanan(2017)]{chen20173d}
Ching-Hang Chen and Deva Ramanan.
\newblock 3d human pose estimation= 2d pose estimation+ matching.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  volume~2, page~6. IEEE, 2017.

\bibitem[Chen et~al.(2018)Chen, Wang, Peng, Zhang, Yu, and
  Sun]{chen2018cascaded}
Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun.
\newblock Cascaded pyramid network for multi-person pose estimation.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 7103--7112. IEEE, 2018.

\bibitem[Collobert et~al.(2016)Collobert, Puhrsch, and
  Synnaeve]{collobert2016wav2letter}
Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve.
\newblock Wav2letter: an end-to-end convnet-based speech recognition system.
\newblock \emph{arXiv preprint arXiv:1609.03193}, 2016.

\bibitem[Coskun et~al.(2017)Coskun, Achilles, DiPietro, Navab, and
  Tombari]{coskun2017long}
Huseyin Coskun, Felix Achilles, Robert~S DiPietro, Nassir Navab, and Federico
  Tombari.
\newblock Long short-term memory kalman filters: Recurrent neural estimators
  for pose regularization.
\newblock In \emph{International Conference on Computer Vision}, pages
  5525--5533. IEEE, 2017.

\bibitem[Dabral et~al.(2018)Dabral, Mundhada, Kusupati, Afaque, Sharma, and
  Jain]{dabral2018learning}
Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer Afaque, Abhishek Sharma,
  and Arjun Jain.
\newblock Learning 3d human pose from structure and motion.
\newblock In \emph{European Conference on Computer Vision}, pages 668--683,
  2018.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  933--941, 2017.

\bibitem[Fang et~al.(2018)Fang, Xu, Wang, Liu, and Zhu]{fang2018learning}
Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, and Song-Chun Zhu.
\newblock Learning pose grammar to encode human body configuration for 3d pose
  estimation.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{gehring2017convolutional}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1243--1252, 2017.

\bibitem[Hossain and Little(2018)]{hossain2018exploiting}
Mir Rayat~Imtiaz Hossain and James~J Little.
\newblock Exploiting temporal information for 3d human pose estimation.
\newblock In \emph{European Conference on Computer Vision}, pages 69--86.
  Springer, 2018.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  volume~1, page~3. IEEE, 2017.

\bibitem[Ionescu et~al.(2014)Ionescu, Papava, Olaru, and
  Sminchisescu]{ionescu2014human3}
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
\newblock Human3. 6m: Large scale datasets and predictive methods for 3d human
  sensing in natural environments.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 36\penalty0 (7):\penalty0 1325--1339, 2014.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lee et~al.(2018)Lee, Lee, and Lee]{lee2018propagating}
Kyoungoh Lee, Inwoong Lee, and Sanghoon Lee.
\newblock Propagating lstm: 3d pose estimation based on joint interdependency.
\newblock In \emph{European Conference on Computer Vision}, pages 119--135,
  2018.

\bibitem[Li and Chan(2014)]{li20143d}
Sijin Li and Antoni~B Chan.
\newblock 3d human pose estimation from monocular images with deep
  convolutional neural network.
\newblock In \emph{Asian Conference on Computer Vision}, pages 332--347.
  Springer, 2014.

\bibitem[Lin et~al.(2017)Lin, Lin, Liang, Wang, and Cheng]{lin2017recurrent}
Mude Lin, Liang Lin, Xiaodan Liang, Keze Wang, and Hui Cheng.
\newblock Recurrent 3d pose sequence machines.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 5543--5552. IEEE, 2017.

\bibitem[Martinez et~al.(2017)Martinez, Hossain, Romero, and
  Little]{martinez2017simple}
Julieta Martinez, Rayat Hossain, Javier Romero, and James~J Little.
\newblock A simple yet effective baseline for 3d human pose estimation.
\newblock In \emph{International Conference on Computer Vision}, volume~1,
  page~5. IEEE, 2017.

\bibitem[Mehta et~al.(2017{\natexlab{a}})Mehta, Rhodin, Casas, Sotnychenko, Xu,
  and Theobalt]{mehta2016monocular}
Dushyant Mehta, Helge Rhodin, Dan Casas, Oleksandr Sotnychenko, Weipeng Xu, and
  Christian Theobalt.
\newblock Monocular 3d human pose estimation using transfer learning and
  improved cnn supervision. arxiv preprint.
\newblock pages 506--516, 2017{\natexlab{a}}.

\bibitem[Mehta et~al.(2017{\natexlab{b}})Mehta, Sridhar, Sotnychenko, Rhodin,
  Shafiei, Seidel, Xu, Casas, and Theobalt]{mehta2017vnect}
Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad
  Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt.
\newblock Vnect: Real-time 3d human pose estimation with a single rgb camera.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 36\penalty0 (4):\penalty0
  44, 2017{\natexlab{b}}.

\bibitem[Moreno-Noguer(2017)]{moreno20173d}
Francesc Moreno-Noguer.
\newblock 3d human pose estimation from a single image via distance matrix
  regression.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 1561--1570. IEEE, 2017.

\bibitem[Mori and Malik(2006)]{mori2006recovering}
Greg Mori and Jitendra Malik.
\newblock Recovering 3d human body configurations using shape contexts.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 28\penalty0 (7):\penalty0 1052--1062, 2006.

\bibitem[Newell et~al.(2016)Newell, Yang, and Deng]{newell2016stacked}
Alejandro Newell, Kaiyu Yang, and Jia Deng.
\newblock Stacked hourglass networks for human pose estimation.
\newblock In \emph{European Conference on Computer Vision}, pages 483--499.
  Springer, 2016.

\bibitem[Nie et~al.(2017)Nie, Wei, and Zhu]{nie2017monocular}
Bruce~Xiaohan Nie, Ping Wei, and Song-Chun Zhu.
\newblock Monocular 3d human pose estimation by predicting depth on joints.
\newblock In \emph{International Conference on Computer Vision}. IEEE, 2017.

\bibitem[Pavlakos et~al.(2017)Pavlakos, Zhou, Derpanis, and
  Daniilidis]{pavlakos2017coarse}
Georgios Pavlakos, Xiaowei Zhou, Konstantinos~G Derpanis, and Kostas
  Daniilidis.
\newblock Coarse-to-fine volumetric prediction for single-image 3d human pose.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 1263--1272. IEEE, 2017.

\bibitem[Pavllo et~al.(2018)Pavllo, Feichtenhofer, Grangier, and
  Auli]{pavllo20183d}
Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli.
\newblock 3d human pose estimation in video with temporal convolutions and
  semi-supervised training.
\newblock \emph{arXiv preprint arXiv:1811.11742}, 2018.

\bibitem[Ramakrishna et~al.(2012)Ramakrishna, Kanade, and
  Sheikh]{ramakrishna2012reconstructing}
Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh.
\newblock Reconstructing 3d human pose from 2d image landmarks.
\newblock In \emph{European Conference on Computer Vision}, pages 573--586.
  Springer, 2012.

\bibitem[Rogez et~al.(2008)Rogez, Rihan, Ramalingam, Orrite, and
  Torr]{rogez2008randomized}
Gr{\'e}gory Rogez, Jonathan Rihan, Srikumar Ramalingam, Carlos Orrite, and
  Philip~HS Torr.
\newblock Randomized trees for human pose detection.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 1--8. IEEE, 2008.

\bibitem[Sun et~al.(2017)Sun, Shang, Liang, and Wei]{sun2017compositional}
Xiao Sun, Jiaxiang Shang, Shuang Liang, and Yichen Wei.
\newblock Compositional human pose regression.
\newblock In \emph{International Conference on Computer Vision}, volume~2,
  page~7. IEEE, 2017.

\bibitem[Tekin et~al.(2016{\natexlab{a}})Tekin, Katircioglu, Salzmann, Lepetit,
  and Fua]{tekin2016structured}
Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent Lepetit, and Pascal
  Fua.
\newblock Structured prediction of 3d human pose with deep neural networks.
\newblock \emph{arXiv preprint arXiv:1605.05180}, 2016{\natexlab{a}}.

\bibitem[Tekin et~al.(2016{\natexlab{b}})Tekin, Rozantsev, Lepetit, and
  Fua]{tekin2016direct}
Bugra Tekin, Artem Rozantsev, Vincent Lepetit, and Pascal Fua.
\newblock Direct prediction of 3d body poses from motion compensated sequences.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 991--1000. IEEE, 2016{\natexlab{b}}.

\bibitem[Tekin et~al.(2017)Tekin, Marquez~Neila, Salzmann, and
  Fua]{tekin2017learning}
Bugra Tekin, Pablo Marquez~Neila, Mathieu Salzmann, and Pascal Fua.
\newblock Learning to fuse 2d and 3d image cues for monocular body pose
  estimation.
\newblock In \emph{International Conference on Computer Vision}, pages
  3941--3950. IEEE, 2017.

\bibitem[Tomasi and Kanade(1992)]{tomasi1992shape}
Carlo Tomasi and Takeo Kanade.
\newblock Shape and motion from image streams under orthography: a
  factorization method.
\newblock \emph{International Journal of Computer Vision}, 9\penalty0
  (2):\penalty0 137--154, 1992.

\bibitem[Van Den~Oord et~al.(2016)Van Den~Oord, Dieleman, Zen, Simonyan,
  Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu]{van2016wavenet}
A{\"a}ron Van Den~Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew~W Senior, and Koray
  Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{SSW}, 125, 2016.

\bibitem[Yang et~al.(2018)Yang, Ouyang, Wang, Ren, Li, and Wang]{yang20183d}
Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren, Hongsheng Li, and Xiaogang
  Wang.
\newblock 3d human pose estimation in the wild by adversarial learning.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  volume~1. IEEE, 2018.

\bibitem[Yasin et~al.(2016)Yasin, Iqbal, Kruger, Weber, and
  Gall]{yasin2016dual}
Hashim Yasin, Umar Iqbal, Bjorn Kruger, Andreas Weber, and Juergen Gall.
\newblock A dual-source approach for 3d pose estimation from a single image.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 4948--4956. IEEE, 2016.

\bibitem[Zhao et~al.(2019)Zhao, Peng, Tian, Kapadia, and
  Metaxas]{zhao2019semantic}
Long Zhao, Xi~Peng, Yu~Tian, Mubbasir Kapadia, and Dimitris~N Metaxas.
\newblock Semantic graph convolutional networks for 3d human pose regression.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 3425--3435. IEEE, 2019.

\bibitem[Zhou et~al.(2015)Zhou, Leonardos, Hu, and Daniilidis]{zhou20153d}
Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, and Kostas Daniilidis.
\newblock 3d shape estimation from 2d landmarks: A convex relaxation approach.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 4447--4455. IEEE, 2015.

\bibitem[Zhou et~al.(2016)Zhou, Zhu, Leonardos, Derpanis, and
  Daniilidis]{zhou2016sparseness}
Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos~G Derpanis, and
  Kostas Daniilidis.
\newblock Sparseness meets deepness: 3d human pose estimation from monocular
  video.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 4966--4975. IEEE, 2016.

\end{thebibliography}
 
\end{document}
