\section{Experimental Evaluation}
\subsection{Datasets and Preprocessing} To fully drive the network towards learning varieties of local D geometries and gain robustness to different noises present in real data, we use the DMatch Benchmark Dataset~\cite{zeng20163dmatch}. This dataset is a large ensemble of the existing ones such as Analysis-by-Synthesis \cite{valentin2016learning}, -Scenes \cite{shotton2013scene}, SUND \cite{xiao2013sun3d}, RGB-D Scenes v.2 \cite{lai2014unsupervised} and Halber and Funkhouser \cite{halber2016structured}. It contains  scenes in total, and we reserve  of them for training and validation.  are for benchmarking. DMatch already provided fragments fused from  consecutive depth frames of the  test scenes, and we follow the same pipeline to generate fragments from the training scenes. Test fragments lack the color information and therefore we resort to using only the D shape. This also makes our network insensitive to illumination changes.

Prior to operation, we downsample the fused fragments with spatial uniformity~\cite{birdal2017sampling} and compute surface normals using~\cite{Hoppe1992} in a -point neighborhood. A reference point and its neighbors within  cm vicinity form a local patch. The number of points in a local patch is thus flexible, which makes it difficult to organize data into regular batches. To facilitate training as well as to increase the representation robustness to noises and different point densities, each local patch is down-sampled. For a fair comparison with other methods in the literature, we use  points, but also provide an extended version that uses 5K since we are not memory bound, as for example, PPFNet~\cite{ppfnet} is. The preparation stage ends with the PPFs calculated for the assembled local patches.

\begin{table*}[t!]
  \centering
  \caption{Our results on the standard 3DMatch benchmark. \textit{Red Kitchen} data is from 7-scenes~\cite{shotton2013scene} and the rest imported from SUN3D~\cite{xiao2013sun3d}.}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccccc}
    \toprule
          & Spin Image~\cite{spin} & SHOT~\cite{shot}  & FPFH~\cite{fpfh}    &
          3DMatch~\cite{zeng20163dmatch} &
          CGF~\cite{Khoury_2017_ICCV} &
          PPFNet~\cite{ppfnet} &
          FoldNet~\cite{foldingnet} & 
          Ours &
          Ours-5K \\
    \midrule
    Kitchen & 0.1937 & 0.1779 & 0.3063  & 0.5751 & 0.4605 & \textbf{0.8972} &  0.5949 & 0.7352 & 0.7866 \\
    Home 1 & 0.3974 & 0.3718 & 0.5833  & 0.7372 & 0.6154 & 0.5577 & 0.7179 & 0.7564 & \textbf{0.7628} \\
    Home 2 & 0.3654 & 0.3365 & 0.4663  & \textbf{0.7067} & 0.5625 & 0.5913 & 0.6058 & 0.6250 & 0.6154 \\
    Hotel 1 & 0.1814 & 0.2080 & 0.2611 &  0.5708 & 0.4469 & 0.5796 & 0.6549 & 0.6593 & \textbf{0.6814} \\
    Hotel 2 & 0.2019 & 0.2212 & 0.3269 & 0.4423 & 0.3846 & 0.5769 & 0.4231 & 0.6058 & \textbf{0.7115} \\
    Hotel 3 & 0.3148 & 0.3889 & 0.5000 & 0.6296 & 0.5926 & 0.6111 & 0.6111 & 0.8889 & \textbf{0.9444} \\
    Study & 0.0548 & 0.0719 & 0.1541  & 0.5616 & 0.4075 & 0.5342 & \textbf{0.7123} & 0.5753 & 0.6199 \\
    MIT Lab & 0.1039 & 0.1299 & 0.2727  & 0.5455 & 0.3506 & \textbf{0.6364} &  0.5844 & 0.5974 & 0.6234 \\
    \midrule
    Average & 0.2267 & 0.2382 & 0.3589  & 0.5961 & 0.4776 & 0.6231 & 0.6130 & 0.6804 & \textbf{0.7182} \\
    \bottomrule
    \end{tabular}\label{tab:3dmatchbenchmark}}
\end{table*} \subsection{Accuracy Assessment Techniques} 

Let us assume that a pair of fragments  and  are aligned by an associated rigid transformation , resulting in a certain overlap. We then define a non-linear feature function  for mapping from input points to feature space, and in our case, this summarizes the PPF computation and encoding as a codeword. The feature for point  is , and  is the pool of features extracted for the points in . To estimate the rigid transformation between  and , the typical approach finds a set of matching pairs in each fragment and associates the correspondences. The inter point pair set  is formed by the pairs  that lie mutually close in the feature space by applying nearest neighbor search :

True matches set  is the set of point pairs with a Euclidean distance below a threshold  under ground-truth transformation  .

We now define an inlier ratio for  as the percentage of true matches in  as .
To successfully estimate the rigid transformation based on  via registration algorithms,  needs to be greater than . For example, in a common RANSAC pipeline, achieving  confidence in the task of finding a subset with at least  correct matches , with an inlier ratio  requires at least  iterations. Theoretically, given  , it is highly probable a reliable local registration algorithm would work, regardless of the robustifier. Therefore instead of using the local registration results to judge the quality of features, which would be both slow and not very straightforward, we define  with  votes for a correct match of two fragments.

Each scene in the benchmark contains a set of fragments. Fragment pairs  and  having an overlap above  under the ground-truth alignment are considered to match.  Together they form the set of fragment pairs  that are used in evaluations. The quality of features is measured by the recall  of fragment pairs matched in :


\begin{table*}[t!]
  \centering
  \caption{Our results on the rotated 3DMatch benchmark. \textit{Red Kitchen} data is from 7-scenes~\cite{shotton2013scene} and the rest imported from SUN3D~\cite{xiao2013sun3d}.}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccccc}
    \toprule
          & Spin Image~\cite{spin} & SHOT~\cite{shot}  & FPFH~\cite{fpfh}  & 
          3DMatch~\cite{zeng20163dmatch} &
          CGF~\cite{Khoury_2017_ICCV} &
          PPFNet~\cite{ppfnet} &
          FoldNet~\cite{foldingnet} &
          Ours &
          Ours-5K \\
    \midrule
    Kitchen & 0.1779 & 0.1779 & 0.2905 & 0.0040 & 0.4466 & 0.0020 & 0.0178& 0.7352 & \textbf{0.7885} \\
    Home 1 & 0.4487 & 0.3526 & 0.5897 & 0.0128 & 0.6667 & 0.0000 & 0.0321    & 0.7692 & \textbf{0.7821} \\
    Home 2 & 0.3413 & 0.3365 & 0.4712 & 0.0337 & 0.5288 & 0.0144 & 0.0337 & 0.6202 & \textbf{0.6442} \\
    Hotel 1 & 0.1814 & 0.2168 & 0.3009 & 0.0044 & 0.4425 & 0.0044 & 0.0133 & 0.6637 & \textbf{0.6770} \\
    Hotel 2 & 0.1731 & 0.2404 & 0.2981 & 0.0000     & 0.4423 & 0.0000 & 0.0096    & 0.6058 & \textbf{0.6923} \\
    Hotel 3 & 0.3148 & 0.3333 & 0.5185 & 0.0096 & 0.6296 & 0.0000 & 0.0370    & 0.9259 & \textbf{0.9630} \\
    Study & 0.0582 & 0.0822 & 0.1575 & 0.0000     & 0.4178 & 0.0000  & 0.0171   & 0.5616 & \textbf{0.6267} \\
    MIT Lab & 0.1169 & 0.1299 & 0.2857 & 0.0260 & 0.4156 & 0.0000 & 0.0260    & 0.6104 & \textbf{0.6753} \\
    \midrule
    Average & 0.2265 & 0.2337 & 0.3640 & 0.0113 & 0.4987 & 0.0026 & 0.0233 & 0.6865 & \textbf{0.7311} \\
    \bottomrule
    \end{tabular}\label{tab:rot3dmatchbenchmark}}
\end{table*}% \subsection{Results} 
\paragraph{Feature quality evaluation}
We first compare the performance of our features against the well-accepted works on the DMatch benchmark with  cm and . Tab.~~\ref{tab:3dmatchbenchmark} tabulates the findings. The methods selected for comparison comprise  handcrafted features (Spin Images~\cite{spin}, SHOT~\cite{shot}, FPFH~\cite{fpfh}) and  state-of-the-art deep learning based methods (DMatch~\cite{zeng20163dmatch}, CGF~\cite{Khoury_2017_ICCV}, PPFNet~\cite{ppfnet}, FoldingNet~\cite{foldingnet}). Note that FoldingNet has never been tested on local descriptor extraction before. It is apparent that, overall, our PPF-FoldNet could match far more fragment pairs in comparison to the other methods, except for scenes \textit{Kitchen} and \textit{Home}, where PPFNet and DMatch achieve a higher recall respectively. In all the other cases, PPF-FoldNet outperforms the state of the art by a large margin,  on average. PPF-FoldNet has a recall of  when using K sample points (the same as PPFNet), while PPFNet remains on . Moreover, because PPF-FoldNet has no memory bottleneck, it can achieve an additonal  improvement in comparison with the K version, when 5K points are used. Interestingly, FPFH is also constructed from a type of PPF features~\cite{fpfh}, but in a form of manual histogram summarization. Compared to FPFH, PPF-FoldNet has   and  higher recall using K and K points respectively. It demonstrates the unprecedented strength of our advanced method in compressing the PPFs. In order to optimally reconstruct PPFs in the decoder, the network forces the bottleneck codeword to be compact as well as distilling the most critical and distinctive information in PPFs. 

To illustrate that parameters in the evaluation metric are not tuned for our own good, we also repeat the experiments with different  and  values. The results are shown in Fig.~\ref{subfig:inlierthresh} and Fig.~\ref{subfig:distancethresh}. In Fig.~\ref{subfig:inlierthresh},  is fixed at  cm,  increases gradually from  to . When  is above , PPF-FoldNet always has a higher recall than the other methods. Below , some other methods may obtain a higher recall but this is too strict for most of the registration algorithms anyway. It is further noteworthy that when  is set to , the point where PPF-FoldNet still gets a recall above , the performance of the other methods falls below . This justifies that PPF-FoldNet is capable of generating many more sets of matching points with a high inlier ratio . This offers a tremendous benefit for the registration algorithms. In Fig.~\ref{subfig:distancethresh},  is fixed at ,  increases gradually from  cm to  cm. When  is smaller than  cm, PPF-FoldNet consistently generates higher recall. This finding indicates that PPF-FoldNet matches more point pairs with a small distance error in the Euclidean space, which could efficiently decrease the rigid transformation estimation errors.

\begin{figure*}[t!]
\centering
\subfigure[]{
\includegraphics[width=0.232505\linewidth, clip=true]{figures/match_inlier_thresh_cropped.pdf}
\label{subfig:inlierthresh}}
\subfigure[]{
\includegraphics[width=0.2315\linewidth, clip=true]{figures/match_distance_thresh_cropped.pdf}
\label{subfig:distancethresh}}
\subfigure[]{
\includegraphics[width=0.2325\linewidth, clip=true]{figures/sparsity_robustness.pdf}
\label{subfig:sparsity}}
\subfigure[]{
\includegraphics[width=0.23305\linewidth, clip=true]{figures/z_axis_rotation_cropped.pdf}
\label{subfig:zrotation}}
\caption{Evaluations on 3DMatch benchmark: \textbf{(a)} Results of different methods under varying inlier ratio threshold \textbf{(b)} Results of different methods under varying point distance threshold \textbf{(c)} Evaluating robustness again point density
\textbf{(d)} Evaluations against rotations around -axis}
\label{fig:evaluations}
\end{figure*} 
\paragraph{Tests on rotation invariance}
To demonstrate the outstanding rotation-invariance property of PPF-FoldNet, we take random fragments out of the evaluation set and gradually rotate them around the -axis from  to  in steps of . The matching results are shown in Fig.~\ref{subfig:zrotation}. As expected, both PPFNet and DMatch perform poorly as they operate on rotation-variant input representations. Hand crafted features or CGF also demonstrate robustness to rotations thanks to the reliance on the local reference frame (LRF). However, PPF-FoldNet stands out as the best approach with a much higher recall which furthermore does not require computation of local reference frames.

To further test how those methods perform under situations with severe rotations, we rotate all the fragments in DMatch benchmark with randomly sampled axes and angles over the whole rotation space, and introduce a new benchmark -- \textit{Rotated DMatch Benchmark}. The same evaluation is once again conducted on this new benchmark. Keeping the accuracy evaluations identical, our results are shown in Tab.~\ref{tab:rot3dmatchbenchmark}. DMatch and PPFNet completely failed under this new benchmark because of the variables introduced by large rotations. Once again, PPF-FoldNet, surpasses all other methods, achieving the best results in all the scenes, predominates the runner-up CGF by large margins of  and  respectively when using K and K points.
\paragraph{Sparsity evaluation}
Thanks to the sparse representation of our input, PPF-FoldNet is also robust in respect of the changes in point cloud density and noise. Fig.\ref{subfig:sparsity} shows the performance of different methods when we gradually decrease the points in the fragment from  to only . We can see that PPF-FoldNet is least affected by the decrease in point cloud density. In particular, when only  points are left in the fragments, the recall for PPF-FoldNet is still greater than  while PPFNet remains around  and the other methods almost fail. The results of PPFNet and PPF-FoldNet together demonstrate that PPF representation offers more robustness in respect of point densities, which is a common problem existing in many point cloud representations.    
\begin{table}[t!]
  \centering
  \setlength{\tabcolsep}{3pt}
  \caption{Accuracy comparison of different PPF representations.}
    \begin{tabular}{lccccccccc}
          & \multicolumn{1}{l}{Kitchen} & \multicolumn{1}{l}{Home 1} & \multicolumn{1}{l}{Home 2} & \multicolumn{1}{l}{Hotel 1} & \multicolumn{1}{l}{Hotel 2} & \multicolumn{1}{l}{Hotel 3} & \multicolumn{1}{l}{Study} & \multicolumn{1}{l}{MIT Lab} & \multicolumn{1}{l}{Average} \\
    \midrule
    PPFH  & 0.534 & 0.622 & 0.486 & 0.341 & 0.346 & 0.574 & 0.233 & 0.351 & 0.436 \\
     Bobkov1     & 0.514 & 0.635 & 0.510 & 0.403 & 0.433 & 0.611 & 0.281 & 0.481 & 0.483 \\
    Our-PPF & 0.506 & 0.635 & 0.495 & 0.350 & 0.385 & 0.667 & 0.267 & 0.403 & 0.463 \\
    \end{tabular}\label{tab:ppf}\end{table}\paragraph{Can PPF-FoldNet operate with different PPF constructions?} We now study 3 identical networks, trained for 3 different PPF formulations: ours, PPFH (the PPF used in FPFH~\cite{fpfh}) and Bobkov1 et al.~\cite{bobkov2018noise}. The latter has an added component of \textit{occupancy ratio} based on grid space. We use a subset of DMatch benchmark to train all networks for a fixed number of iterations and test on the rotated fragments. Tab.~\ref{tab:ppf} presents our findings: all features perform similarly. Thus, we do not claim the superiority of our PPF representation, but stress that it is simple, easy to compute, intuitive and easy to visualize. Due to the voxelization, \textit{Bobkov1} is significantly slower than the other methods, and due to the lack of an LRF, our PPF is faster than \textit{PPFH}'s. Using stronger pair primitives would favor PPF-FoldNet as our network is agnostic to the PPF construction.
\paragraph{Runtime} We run our algorithm on a machine loaded with NVIDIA TitanX Pascal GPU and an Intel Core i GHz CPU. On this hardware, computing features of an entire fragment via FPFH~\cite{fpfh} takes  seconds, whereas PPF-FoldNet achieves a  speed-up with  seconds, despite having similar theoretical complexity. In particular, our input preparation for PPF extraction runs in  seconds, and the inference in . This is due to 1) PPF-FoldNet requiring only a single pass over the input, 2) our efficient network accelerated on GPU powered Tensorflow.
\insertimageStar{1}{matching_visuals_cropped.pdf}{Qualitative results of matching across different fragments and for different methods. When severe transformations are present, only hand-crafted algorithms, CGF and our method achieves satisfactory matches. However, for PPF-FoldNet, the number of matches are significantly larger.}{fig:matching}{t!}
\insertimageStar{1}{evolution_cropped.pdf}{Visualizing signatures of reconstructed PPFs. As the training converges, the reconstructed PPF signatures become closer to the original signatures. Our network reveals the underlying structure of the PPF space.}{fig:evolution}{t!}
\subsection{Qualitative Evaluations} 
\label{sec:qualeval}
\paragraph{Visualizing the matching result} From the quantitative results, PPF-FoldNet is expected to have better and more correct feature matches, especially when arbitrary rigid transformations are applied. To show this visually, we run different methods and ours across several fragments undergoing varying rotations. In Fig.~\ref{fig:matching} we show the matching regions, over uniformly sampled~\cite{birdal2017sampling} keypoints on these fragments. It is clear that our algorithm performs the best among all others in discovering the most correct correspondences.
\paragraph{Monitoring network evolution} As our network is interpretable, it is tempting to qualitatively analyze the progress of the network. To do that we record the PPF reconstruction output at discrete time steps and visualize the PPFs as explained in ยง~\ref{sec:ppf}. Fig~\ref{fig:evolution} shows such a visualization for different local patches. First, thanks to the representation power, our network achieves high fidelity recovery of PPFs. Note that even though the network starts from a random initialization, it can quickly recover a desired point pair feature set, even after only a small number of iterations. Next, for similar local patches (top and bottom rows), the reconstructions are similar, while for different ones, different. 

\paragraph{Visualizing the latent space}
We now attempt to visualize the learned latent space and assess whether the embedding is semantically meaningful. To do so, we compute a set of codewords and the associated PPF signatures. We then run the Barnes Hut T-SNE algorithm~\cite{tsne,barneshut} on the extracted codewords and form a two-dimensional embedding space, as shown in Fig.~\ref{fig:tsne}. At each D location we paint the PPF signature and thereby illustrate the distribution of PPFs along the manifold. We also plot the original patches which generated the codewords and their corresponding signatures as cutouts. Presented in Fig.~\ref{fig:tsne}, whenever the patches are geometrically and semantically close, the computed descriptors are close, and whenever the patches have less physical similarity, they are embedded into different parts of the space. This provides insight into the good performance and meaningfulness in the relationships our network could learn.
\insertimageStar{1}{tsne_cropped.pdf}{Visualization of the latent space of codewords, associated PPFs and samples of clustered local D patches using TSNE~\cite{tsne,barneshut}.}{fig:tsne}{t!}
\insertimageStar{1}{feature_vis_cropped.pdf}{Visualization of the latent feature space on fragments fused from different views. To map each feature to a color on the fragment, we use TSNE embedding~\cite{tsne}. We reduce the dimension to three and associate each low dimensional vector with an RGB color.}{fig:rotation}{t!}
In a further experiment, we extract a feature at each location of the point cloud. Then, we reduce the dimension of the latent space to three via TSNE~\cite{tsne}, and colorize each point by the reduced feature vector. Qualitatively justifying the repeatibility of our descriptors, the outcome is shown in Fig.~\ref{fig:rotation}. Note that, descriptors extracted by the proposed approach lead to similar colors in matching regions among the different fragments. 