\pdfoutput=1
\documentclass{article}

\usepackage{amsmath, amssymb, amsthm}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}

\usepackage{xspace}
\usepackage{setspace}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{pgfplotstable}
\usepackage{tikz}
\pgfplotsset{compat=newest}
\usetikzlibrary{calc,positioning,shapes.geometric,math}

\usepackage{etoolbox}
\newtoggle{release}
\togglefalse{release}
\iftoggle{release}{
\newcommand{\alex}[1]{}
\newcommand{\gab}[1]{}
\newcommand{\adios}[1]{}
}
{
\newcommand{\alex}[1]{{\color{blue} A: #1}}
\newcommand{\gab}[1]{{\color{green} G: #1}}
\newcommand{\adios}[1]{{\color{magenta}Y: #1}}
}

\newcommand{\diffq}{\textsc{DiffQ}\xspace}
\newcommand{\I}{\mathbf{I}}
\newcommand{\V}{\mathbf{V}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\tQ}{\mathbf{\tilde{Q}}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\M}{\mathbf{M}}

\newcommand{\real}{\mathbb{R}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\esp}[1]{\mathbb{E}\left[#1\right]}
 
\newtoggle{arxiv}
\toggletrue{arxiv}

\iftoggle{arxiv}{
    \usepackage[accepted]{preprint}
}{
    \usepackage[accepted]{icml2021} 
}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}
\usepackage{array}
\usepackage{bibunits}







\icmltitlerunning{Differentiable Model Compression via Pseudo Quantization Noise}

\begin{document}

\twocolumn[
\icmltitle{Differentiable Model Compression via Pseudo Quantization Noise}







\icmlsetsymbol{equal}{}

\begin{icmlauthorlist}
\icmlauthor{Alexandre D\'efossez}{fair}
\icmlauthor{Yossi Adi}{fair}
\icmlauthor{Gabriel Synnaeve}{fair}
\end{icmlauthorlist}

\icmlaffiliation{fair}{Facebook AI Research}

\icmlcorrespondingauthor{Alexandre D\'efossez}{defossez@fb.com}
\icmlkeywords{Quantization, Differentiable Neural Architecture Search, Deep Learning}

\vskip 0.3in
]





\defaultbibliography{biblio}
\defaultbibliographystyle{icml2021}

\printAffiliationsAndNotice{\icmlEqualContribution} \begin{abstract}
We propose to add independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. This method, \textsc{DiffQ}, is differentiable both with respect to the unquantized parameters, and the number of bits used. Given a single hyper-parameter expressing the desired balance between the quantized model size and accuracy, \diffq can optimize the number of bits used per individual weight or groups of weights, in a single training. We experimentally verify that our method outperforms state-of-the-art quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the Wikitext-103 language modeling benchmark, \diffq compresses a 16 layers transformer model by a factor of 8, equivalent to 4 bits precision, while losing only 0.5 points of perplexity. Code is available at: {\color{magenta} \url{https://github.com/facebookresearch/diffq}}
\end{abstract}

\section{Introduction}
\label{intro}

\begin{figure}[h!]
  \centering
	\includegraphics[width=0.9\linewidth]{figs/imagenet_updated.pdf}
	\caption{ImageNet results using EfficientNet-B3 model. We plot the model size vs. model accuracy using different penalty levels. We additionally, present the uncompressed models (uncomp.) and Quantization Aware Training (QAT) using 4 and 8 bits.}
 	\label{fig:teaser}
 	\vskip -0.1in
\end{figure}

An important factor in the adoption of a deep learning model for real-world applications is how easily it can be pushed
to remote devices. It has been observed that larger models usually lead to better performance, for instance
with larger ResNets~\citep{he2016deep} achieving higher accuracies than smaller ones.
In response, the community has worked toward smaller, and more efficient models~\citep{tan2019efficientnet}. Yet
an EfficientNet-B3 is still almost 50MB, a considerable amount if the model is to be included in online applications,
or should be updated with limited network capabilities. For other applications, such as language modeling~\citep{vaswani2017attention} or source separation~\citep{defossez2019music}, the typical model size is closer to 1GB, which rules out any kind of mobile usage.

The simplest method to reduce model size consists is decreasing the number of bits used to encode individual weights. For instance, using 16 bits floating point numbers halves the model size, while retaining a sufficient approximation of the set of real numbers, , to train with first-order optimization methods~\citep{micikevicius2017mixed}.
When considering lower precision, for instance, 8 or 4 bits, the set of possible values is no longer a good approximation of , hence preventing the use of first-order optimization methods. Specifically, uniform quantization requires using the \texttt{round} function, which has zero gradients wherever it is differentiable.

Quantization can be done as a post-processing step to regular training. However, errors accumulate in a multiplicative fashion across layers, leading to a possibly uncontrolled decrease in the model accuracy.
\citet{krishnamoorthi2018quantizing} propose to use a gradient Straight-Through-Estimator (STE)~\citep{bengio2013estimating} in order to provide a non-zero gradient to the original weights. This allows the model to adapt to quantization during training and reduces the final degradation of performance. However, \citet{fan2020training} noticed instability and bias in the learned weights, as STE is not the true gradient to the function.

The nature of quantization noise has been extensively studied as part of Analog-to-Digital Converters (ADC). In particular, a useful assumption to facilitate the design of post-processing filters for ADC is
the independence of the input value and the ``Pseudo Quantization Noise'' (PQN), as formalized by \citet{widrow1996statistical}. In this work, we show that it also applies to deep learning model quantization, and provides a simple framework in which the output and the quantized model size are both differentiable, without any use of STE. This allows to optimally set the number of bits used per individual weight (or group of weights) to a trade-off between size and accuracy, in a single training and at almost no extra cost. Even when the number of bits to use is fixed, we show that unlike STE, using independent pseudo quantization noise does not introduce bias in the gradient and achieves higher performance.



\paragraph{Our Contribution:} (i) With \diffq, we propose to use pseudo quantization noise to approximate quantization at train time, as a differentiable alternative to STE, both with respect to the unquantized weights and number of bits used. \
\hat{w} = \displaystyle\frac{w - \min(w)}{\max(w) - \min(w)},

\label{eq:index}
    \I(x, B) = \mathrm{round}\left(x \cdot (2^{B} - 1)\right), \, \forall x\in[0, 1].

\label{eq:value}
    \V(q, B) = \frac{q}{2^{B} - 1},\, \forall q\in[2^B - 1].

\label{eq:uniform}
    \Q(x, B) = \V(\I(x, B), B),\,  \forall x\in[0, 1], B \in \nat_*.

    \min_{w\in{\real^d}} L(f_w),

    \min_{w\in{\real^d}} L(f_{\Q(w, B)}),

\label{counter_example_ste}
    \min_{w\in[0, 1]} L(w) := \esp{\frac{1}{2}\left(X\Q(w, B) - X w_*\right)^2}.

\label{counter_grad}
    G_n = \sigma^2 (\Q(w_n, B) - w_*).

\label{eq:diffq}
    \tQ(x, B) = x + \frac{\Delta}{2} \cdot \mathcal{U}[-1, 1],

G_n &= \esp{x \cdot \left(\left(w_n + \frac{\Delta}{2} \cdot \mathcal{U}[-1, 1]\right) x - w_* x\right)} \\
	&= \sigma^2 (w_n - w_*).

\label{eq:model_size}
\M(b) = \frac{g}{2^{23}}\sum_{s=1}^{d/g} b_s.

\begin{aligned}
     & \min_{w, b} L(f_{\Q(w, b)}),\\
    &\,\text{s.t.} \quad \M(b) \leq m.
\end{aligned}
\qquad \text{or} \qquad
\begin{aligned}
     & \min_{w, b} \M(b),\\
    &\,\text{s.t.} \quad L(f_{\Q(w, b)}) \leq l.
\end{aligned}

 \label{eq:opt_diffq}
 \min_{w, b} L(f_{\tQ(w, b)}) + \lambda(m) \M(b),
 
     b = b_\textrm{min} + \sigma(l) (b_\textrm{max} - b_\textrm{min}),
 
 \label{eq:true_model_size}
     \hspace{-0.1cm}\tilde{\M}(b) =  \frac{1}{2^{23}}\left(2 \cdot 32 + 8 + \frac{d}{g} \max(C) + g\sum_{s=1}^{d/g} b_s \right)
  
\vspace{-0.2cm}
\section{Results}
\label{results}
\renewcommand\UrlFont{\color{purple}\rmfamily}
We present experimental results for audio source separation, image classification, and language modeling. We show that \diffq{} can systematically provide a model with comparable performance to the uncompressed one while producing a model with a smaller footprint than the baseline methods (STE based).
We provide a finer analysis of different aspects of \diffq hyper-parameters and their impact on quantized models in Section~\ref{analysis}. Both experimental code, and a generic framework usable with any architecture in just a few lines, are available on our Github\footnote{\url{https://github.com/facebookresearch/diffq}}.
All hyper-parameters for optimization and model definition for all tasks are detailed in the Supplementary Material, Section \ref{app:xps}.



\noindent{\bf {\diffq{} hyper-parameters}}
For all experiments, we use , ,
 and Gaussian noise. 
We observed on most models that taking  is
unstable, with the notable exception of Resnet-20.
We use a separate Adam optimizer~\citep{adam} for the logit parameters controlling the number of bits used, with a default momentum  and decay .
We use the default learning rate  for all task,
except language modeling where we use .
The remaining hyper-parameters are , the amount of penalty
applied to the model size, and , the group size. When  is not mentioned, it is set to the default value , which we found to be the best trade-off between the model freedom and the overhead from storing the number of bits used for each group. 

\vspace{-0.1cm}
\subsection{Music Source Separation}
\vspace{-0.1cm}
We use the Demucs architecture by~\citet{defossez2019music} with  initial hidden channels.
The baseline is enhanced to account for recent improvement from speech source separation~\citep{defossez2020real} and pitch/tempo shift augmentation~\citep{cohen2019improving}. The model is trained on the standard MusDB benchmark~\citep{musdb} for 180 epochs, and evaluated with the Signal-To-Distortion Ratio (SDR) metric~\citep{measures}. The unquantized model is 1GB. We compare \diffq{} with QAT training with either 5 or 4 bits, with the results presented in Table~\ref{tab:speech}. With 5 bits, QAT is able to replicate almost the same performance as the uncompressed model.
However, trying to further compress the model to 4 bits precision leads to a sharp decrease of the SDR, losing 0.3 points, for a 130MB model. In contrast, \diffq{} achieves a model size of 120MB, with only
a drop of 0.03 point of SDR compared to the uncompressed baseline.

\vspace{-0.1cm}
\subsection{Language Modeling}
\vspace{-0.1cm}

We trained a 16 layers transformer~\citep{vaswani2017attention} based language model on the Wikitext-103 text corpus~\citep{merity2016pointer},
following~\citet{baevski2018adaptive},
 and using the Fairseq framework~\citep{ott2019fairseq}. 
 Results are presented in Table~\ref{tab:text}.
 We compare to the Quant-Noise method by~\citet{fan2020training}, but use a reduced
 layer-drop~\citep{fan2019reducing} of 0.1 instead of 0.2. This both improves the baseline, as well as the performance of \diffq models. In order to further improve the performance of \diffq with layer-drop, we explicitly set the gradient for the number of bits parameters to zero for all layers that have been dropped.
 In order to test the compatibility of \diffq with efficient int8 kernels, we further quantize the activations to 8 bits using PyTorch native support~\citep{Pytorch}.
 
 While QAT breaks down when trying to get
 to 4 bits precision (perplexity of 29.9), using \diffq allows to achieve an even lower model size (113MB vs. 118 MB 
 for QAT 4 bits) with a perplexity closer to the uncompressed one (18.6, vs.
 18.1 uncompressed). Even more interesting, with a lower penalty of 1, \diffq
 produces a model smaller than QAT 8 bits, and with a perplexity 
 better than the baseline.
While Quant-Noise significantly improves over QAT, it produces models that
are both larger, and with worse perplexity than \diffq.
Note that we did not use noise injection to emulate activation quantization, and it seems weight level noise injection was sufficient to make the model robust.
 
\begin{table}[t!]
\caption{
Language modeling results for a 16 layer Transformer trained on Wikitext-103. We also test combining weight and activation quantization. We compared \diffq to QAT and Quant-Noise (QN) method proposed by~\citet{fan2020training} (models with  were trained with a layer-drop of 0.2~\cite{fan2019reducing}.). Activations are quantized over 8 bits, with a per-channel scaling.}
\label{tab:text}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llcc}
\toprule
	Weights & Activation	 & PPL~ & M. S. (MB)~ \\
\midrule
Uncompressed & -  	   & 18.1 & 942\\
\midrule
    8 bits & 8 bits    & 18.3  & 236\\
QAT 8bits & 8 bits   	& 19.7  & 236 \\
QAT 4bits & 8 bits    	&   29.9   & 118 \\
\diffq  () & 8 bits & \textbf{18.0}          & 182 \\
\diffq  () & 8 bits & 18.6 & \textbf{113}\\
\midrule
Uncompressed  & -  	      & 18.3 & 942\\
QN 8 bits    & QN 8 bits & 18.7 & 236 \\
QN 4 bits    & QN 8 bits & 19.5 & 118 \\ 
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\subsection{Image Classification}

\begin{figure}[t!]
  \vskip 0.2in
  \centering
\subfigure[CIFAR-100]{\includegraphics[width=0.99\linewidth]{figs/cifar100.pdf}}
  \caption{Model accuracy and size on CIFAR-100 using MobileNet, ResNet-18, and WideResNet (WRN) models for various penalty levels
  using \diffq, QAT 4 and 8 models, and the baseline.}
  \vskip -0.2in
  \label{fig:cifar}
\end{figure}


\begin{table}[t!]
\caption{Image classification results for the ImageNet benchmark. Results are presented for \diffq and QAT using 4 and 8 bits using the DeiT model~\citep{touvron2020training}. We report Top-1 Accuracy (Acc.) together with Model Size (M.S.).}
\label{tab:imagenet}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|cc}
\toprule
		& Top-1 Acc. (\%)~ & M.S. (MB)~ \\
\midrule
Uncompressed    	                & 81.8 		   & 371.4 \\
\midrule
QAT 4bits    		                & 79.2         & 41.7  \\
QAT 8bits    		                & 81.6         & 82.9 \\
\midrule
\diffq ()  	    & \textbf{82.0}      & 45.7  \\
\diffq ()  	                & 81.5      & \textbf{33.02}  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Next, we evaluated three image classification benchmarks: ImageNet~\cite{imagenet_cvpr09}, CIFAR-10 and CIFAR-100~\cite{krizhevsky2009learning}. For CIFAR-10 and CIFAR-100 results are reported for MobileNet-v1~\cite{howard2017mobilenets}, ResNet-18~\cite{he2016deep}, and Wide-ResNet with 28x10, depth and width levels respectively ~\cite{zagoruyko2016wide}. ImageNet results are reported using EfficientNet-B3~\cite{tan2019efficientnet} and DeiT-B~\cite{touvron2020training} models. 
See the Supplementary Material, Section~\ref{app:xps}, for the hyper-parameters and augmentations used.

\noindent{\bf {CIFAR10 \& CIFAR-100.}}
Results for CIFAR-100 are depicted in Figure~\ref{fig:cifar}. We compare \diffq to QAT using 2, 3, and 4 bits quantization. Performance of the uncompressed model is additionally presented as an upper-bound. 
To understand the effect of the penalty level  on both model size and accuracy, we train models with \diffq using different penalty levels. Notice, as we decrease , model size increases together with model accuracy until it reaches a plateau in performance. 
Exact results are presented in Table~\ref{tab:cifar_supp}, in the Supplementary Material, along with the results on CIFAR-10, and a detailed analysis is in Section~\ref{supp:cifar}.

Results suggest \diffq models reach comparable performance to the QAT models while producing models with a smaller footprint. When considering QAT with 2 bits quantization, \diffq{} archives large accuracy improvements while producing a model with equivalent size.


\noindent{\bf {ImageNet.}}
Results for ImageNet using DeiT-B model are presented in Table~\ref{tab:imagenet}. We compared \diffq to QAT when training with 4 and 8 bits. Both QAT with 8 bits and \diffq reach comparable performance to the uncompressed model, while \diffq yields a model almost half of the size as QAT, however still bigger than QAT with 4 bits. When we increase , we get a smaller model-size than QAT with 4 bits but with better accuracy levels.

We evaluate the performance of \diffq{} on the memory-efficient architecture EfficientNet-B3 model on Figure~\ref{fig:teaser}.
Those results are also presented in Table~\ref{supp:imagenet} in the Supplementary Material.
Both QAT 8 bits and \diffq{} achieves similar performance
for equivalent model sizes, with a small drop compared to the uncompressed
baseline. However, when considering QAT 4 bits, \diffq{} produces a smaller
model with a significantly better accuracy level. In fact, for QAT 4, we
noticed considerable instability close to the end of the training, see Section~\ref{supp:imnet} in the Supplementary Material for a more detailed analysis.





\begin{table}[t!]
\caption{Comparison of \diffq against baslines presented in Section~\ref{relatedwork}. Each table section is for a specific dataset and model. We report accuracy (Acc.) and Model Size (M.S.). Sizes marked with  are
    reported after Huffman coding, following~\citet{polino2018model}.}
    \label{tab:related}
    \vskip 0.15in
    \centering
    \resizebox{0.9\columnwidth}{!}{
    \begin{tabular}{l|r|r}
        \toprule 
        \textsc{Method} & \textsc{Acc.}~ & M.S.~ \\
        \midrule 
        \multicolumn{3}{c}{CIFAR10 - ResNet-18} \\ 
        \midrule 
        \diffq (ours)   &  \textbf{93.9} & \textbf{2.7 MB} \\ 
        NICE \citep{baskin2018nice}       &   92.7 & 2.7 MB \\
        UNIQ \citep{baskin2018uniq}       &   89.1 & 2.7 MB \\
\midrule
        \multicolumn{3}{c}{CIFAR100 - Wide-ResNet} \\ 
        \midrule
        \diffq (ours)   &   \textbf{75.6} & \textbf{4.7 MB}\\ 
        Diff. Quant.~\citep{polino2018model}   &   49.3 & 7.9 MB\\ 
        \midrule
        \multicolumn{3}{c}{ImageNet - ResNet-18} \\ 
        \midrule 
        \diffq (ours) &   69.4 & 5.3 MB\\ 
DQ \citep{uhlich2020mixed}  &   \textbf{70.1} & 5.4 MB\\ 
Meta-Quant \citep{chen2019metaquant} &  60.3 & \textbf{1.3 MB}\\
        \midrule
\multicolumn{3}{c}{CIFAR-10 - ResNet-20} \\ 
        \midrule 
        \diffq (ours) &   \textbf{91.7} & \textbf{69 KB} \\ 
DQ \citep{uhlich2020mixed}  &   91.4 & 70 KB \\ 
\bottomrule
    \end{tabular}}
    \vskip -0.1in
\end{table}

\subsection{Comparison to related work}
\label{sec:compare_related}

On Table~\ref{tab:related}, we compare to some of the related work presented in Section~\ref{relatedwork}. Compared
with the NICE~\citep{baskin2018nice} and UNIQ~\citep{baskin2018uniq} methods, which also rely on additive noise, \diffq
achieves significantly better accuracy for the same model size.
We then compare to the differentiable quantization method by \citep{polino2018model}, which only optimizes the non uniform quantization points, not the pre-quantization weights. Following their practice, we report
numbers after Huffman coding. We achieve a model almost half as small, with a gap of 25\% in accuracy, which highlight that optimizing pre-quantization weights is more important than tuning a non uniform quantization grid.
Meta-Quant~\citep{chen2019metaquant} achieves smaller model size than \diffq, with 1 bit per weight, a regime where the PQN assumption breaks down, at the price of losing nearly 10\% of accuracy. 
Finally, compared with the D.Q. method by~\citet{uhlich2020mixed},
we achieve comparable results on ImageNet with ResNet-18, and both smaller and more accurate models on CIFAR-10 with ResNet-20.
 
\vspace{-0.2cm}
\subsection{Analysis}
\label{analysis}

In the following, we perform a finer analysis of the behavior of \diffq.
First we look at the distribution of the number of bits used for different layers for various architectures.
We then perform an ablation study, looking at the performance of \diffq without learning the number of bits, and the impact of the group size. Finally, we compare the difference in entropy of the learned models with QAT and \diffq.

\begin{figure}[t!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figs/ENB3_diffq.pdf}}
\centerline{\includegraphics[width=0.9\columnwidth]{figs/DeiT_diffq.pdf}}
\caption{Layer groups wise size of the baseline EfficientNet-B3 (above) and DeiT (below) models (floating point 32 bits) and the quantized model with \diffq ( for EfficientNet-B3,  for DeiT) with cumulative bits per weights. Layers are grouped in increasing depth (0 is close to the input), ``bits bits'' shows the capacity needed to encode the weights' bitwidth.}
\label{fig:hist}
\end{center}
\vskip -0.3in
\end{figure}





\begin{table}[t!]
\caption{A comparison between QAT and \diffq while we consider a fixed number of bits for all model parameters, specifically using 2, 3, and 4 bits. We report Accuracy (Acc.) and Model Size (M.S.) for a ResNet-18 model trained on CIFAR-100.}
\label{tab:image_abb}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{l|rr}
\toprule
&  Acc. (\%)~ &  M. S. (MB)~ \\
\midrule
Uncompressed    	            & 77.9 & 42.81 \\
\cmidrule(lr){1-3}
 QAT 2bits    		            & 58.7 & 2.72\\
 QAT 3bits    		            & 73.7 & 4.05  \\
 QAT 4bits    		            & 77.3 & 5.39  \\
\cmidrule(lr){1-3}
 \diffq 2bits	 			    & 66.6 & 2.72  \\
 \diffq 3bits				    & 76.7 & 4.05  \\
 \diffq 4bits				    & 77.5 & 5.39  \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.25in
\end{table}

\noindent{\bf {Bits Histogram}}
Figure~\ref{fig:hist} presents the weight bitwidth assignment over layer groups for the EfficientNet-B3 \cite{tan2019efficientnet} and DeiT \cite{touvron2020training} models trained on ImageNet. The capacity distribution over depth for ConvNets EfficientNet-B3) and Transformers (DeiT) are different (fp32 shows uncompressed capacity). Notice, that the quantization trends are different too: for the ConvNet, smaller bitwidths are used for deeper layers of the model while large bitwidth is more common in the first layers (except for the last linear layer which seems to need some precision). For the Transformer, this effect of varying quantization by layer is similar but less pronounced.

\noindent{\bf {Ablation}}
Recall, in Section~\ref{sec:counter} we demonstrated the instability of following STE for optimization. In Table~\ref{tab:image_abb} we compare QAT to \diffq for a ResNet-18 trained on CIFAR-100, where we consider a fixed number of bits for all model parameters.
\diffq outperforms QAT, where this is especially noticeable while using 2 bits quantization, in which training is less stable for QAT. 
The same occurs for other architectures (MobileNet, Wide-ResNet) and datasets (CIFAR-10) as shown in Table~\ref{supp:tab:app_image_abb},
in the Supplementary.

Next, we evaluated the affect of the group-size, , on model size and accuracy, by optimizing \diffq models using . When , we use a single group for the entire layer. Results for ResNet-18 using CIFAR-100 are summarized in Figure~\ref{tab:image_abb}. Interestingly, we observed that increasing , yields in a smaller model size on the expanse of a minor decrease in performance. However, when setting  model performance (model size and accuracy) is comparable to  for this task.

\noindent{\bf {\diffq maximizes entropy usage}}
The model size given by~\eqref{eq:true_model_size} is obtained with a traditional encoding of the quantized model.
However, more efficient coding techniques exist when the entropy of the data is low, such as Huffman coding~\citep{huffman}. Using the ZLib library, we obtain an estimate of the Huffman compressed model size after quantization. For instance, for the language model described in Table~\ref{tab:text}, the QAT 8 model gets further compressed from 236MB to 150MB, showing that the entropy of its quantized weight is significantly lower than the maximal one for 8 bits integers. However, the \diffq model naive size is 113MB, and after compression by ZLib, gets to 122MB.
This is a sign that the underlying entropy is close to its maximal value, with ZLib adding only overhead for no actual gain.

In \eqref{eq:opt_diffq}, we only penalize the naive number of bits used, while asking for
the best possible accuracy. In that case, the model maximally use the entropy capabilities for a given number of bits. An interesting line of research would be to replace the model size \eqref{eq:model_size} to account for the actual entropy of the data. We leave that for further research.




 
\begin{figure}[t!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.86\columnwidth]{figs/gs_ab.pdf}}
\caption{\diffq results with various groups sizes ().  refers to a single group for the entire layer. For reference, we report the accuracy of the uncompressed model (42.8 MB). 
Models are Resnet-18 trained on CIFAR-100.}
\label{fig:gs}
\end{center}
\vskip -0.35in
\end{figure}

\vspace{-0.1cm}
\section{Discussion}
\vspace{-0.1cm}
\label{discussion}

We presented \diffq, a novel and simple differentiable method for model quantization via pseudo quantization noise addition to models` parameters. Given a single hyper-parameter that quantifies the desired trade-off between model size and accuracy, \diffq can optimize the number of bits used for each trainable parameter or group of parameters during model training. 


We conduct expensive experimental evaluations on various domains using different model architectures. Results suggest that \diffq is superior to the baseline methods on several benchmarks from various domains. On ImageNet, Wikitext-103, and MusDB, we achieve a model size equivalent or smaller to a 4 bits quantized model, while retaining the same performance as the unquantized baseline.

For future work, we consider adapting the model size penalty to account for Huffman encoding, which could allow to further reduce the model size when it is gzipped. Another line of work would be using PQN to improve activation quantization. While 8-bits activations are well supported, this could open up the way to a 4-bits kernel. 
\clearpage
\bibliography{biblio}
\bibliographystyle{icml2021}


\renewcommand{\thesection}{\Alph{section}}
\onecolumn
\thispagestyle{empty}
\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}}
\begin{center}\Large \setstretch{1.2} Supplementary Material for \\
\textbf{Differentiable Model Compression via Pseudo Quantization Noise}
\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}}
\end{center}
\setcounter{section}{0}
\numberwithin{equation}{section}
\counterwithin{table}{section}
\renewcommand\theHsection{\Alph{section}}

We provide in Section~\ref{app:xps} all the details on the 
exact hyper-parameters, models, and datasets used for the results in Section~\ref{results} of the main paper.
Then, we provide supplementary results in Section~\ref{app:results},
in particular tables for the scatter plots given on Figures \ref{fig:teaser} and \ref{fig:cifar}.
Finally, we present the code provided along with this Supplementary Material, in the \texttt{code} folder.

\vspace{-0.2cm}
\section{Detailed experimental setup}
\vspace{-0.1cm}
\label{app:xps}

All experiments are conducted using NVIDIA V100 GPUs with either 16GB or 32GB RAM, depending on the applications
(with language modeling requiring larger GPUs.).
For all models trained with QAT or \diffq, we do not quantize tensors with a size under 0.01 MB (0.1 MB for the DeiT model).

\vspace{-0.2cm}
\subsection{Music Source Separation}
\vspace{-0.1cm}
We train a Demucs source separation model~\citep{defossez2019music} with a depth of 6 and 64 initial hidden channels, on the MusDB dataset~\citep{musdb}\footnote{\url{https://sigsep.github.io/datasets/musdb.html}}. Following~\citet{defossez2020real}, we upsample the input audio by a factor of 2 before feeding it to the model and downsample the output by a factor of 2 before computing the loss. Following~\citet{cohen2019improving}, we also use pitch-shift/tempo stretch data augmentation using the 
\texttt{soundstretch} package\footnote{\url{https://www.surina.net/soundtouch/soundstretch.html}}. A batch goes through
this transformation with a probability of 20\%, the tempo is changed by a uniform fractional amount between -12\% and +12\%.
The pitch is shifted by at most 2 semitones. . Those data augmentation strongly
improved the baseline. We train for 180 epochs, with a batch size of 64 and Adam~\citep{adam} with a learning rate of . All other training details are exactly as in \citep{defossez2019music}.

\vspace{-0.2cm}
\subsection{Language Modeling}
\vspace{-0.1cm}
We trained a 16 layers transformer~\citep{vaswani2017attention} based language model on the Wikitext-103 text corpus~\citep{merity2016pointer}\footnote{\url{https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/}},
following~\citet{baevski2018adaptive},
 and using the \texttt{Fairseq} framework~\citep{ott2019fairseq}. 
 We used the hyper-parameters and the script provided by \citep{fan2020training} in the \texttt{Fairseq} repository\footnote{\url{https://github.com/pytorch/fairseq/tree/master/examples/quant_noise}}, however, and unlike what they mention in their paper, this script does not include layer drop~\citep{fan2019reducing}.
 For \diffq, we tried the penalty levels  in , with group size , as well as  and .

\noindent{\bf Tied weights and \diffq.}
The model we trained was configured so that the word embedding in the first layer and the weight of the adaptive softmax are bound to the same value. It is important to detect such bounded parameters with \diffq, as otherwise, a different number of bits could be used for what is in fact, the very same tensor. Not only do we use a single bits logit parameter
when a parameter tensor is reused multiple times, but for each forward, we make sure that the pseudo quantization noise
is sampled only once and reused appropriately. Failure to do so led to a significant worsening of the performance
at validation time.

\vspace{-0.2cm}
\subsection{Image classification}
\vspace{-0.1cm}
\label{supp:image}

\noindent{\bf CIFAR10/100.}
On the CIFAR10/100 datasets, we train 3 different models: MobileNet-v1~\citep{howard2017mobilenets}, ResNet-18~\citep{he2016deep}, and a Wide-ResNet with 28x10 depth and width levels respectively~\citep{zagoruyko2016wide}.
All experiments are conducted on a single GPU with a batch size of 128, SGD with a learning rate of 0.1, momentum of 0.9,
weight decay of . The learning rate is decayed by a factor of 0.2 every 60 iterations.
To generate Figure~\ref{fig:cifar}, we evaluated \diffq for  in 
and the group size  in .

The dataset has been obtained from the \texttt{torchvision} package\footnote{\url{https://github.com/pytorch/vision}}.
The input images are augmented with a random crop of size 32 with padding of 4, and a random horizontal flip.
The RGB pixel values are normalized to mean 0 and standard deviation 1. We use the default split between
train and valid as obtained from the \texttt{torchvision} package.

\noindent{\bf ImageNet.}
We train an EfficientNet as implemented by~\citep{rw2019timm},
as well as a DeiT vision transformer~\citep{touvron2020training}  on the ImageNet dataset~\cite{imagenet_cvpr09}\footnote{\url{http://www.image-net.org/}}. We use the original dataset
split between train and valid. 
The images go through a random resize crop to 300px, a random horizontal flip, and pixel RGB values are normalized
to have zero mean and unit variance.

\noindent{\bf ImageNet - EfficientNet.}
We trained for 100 epochs, using RMSProp~\cite{tieleman2012lecture} as
implemented in the \texttt{timm} package\footnote{\url{https://github.com/rwightman/pytorch-image-models}}
with a learning rate of 0.0016, a weight decay of  and a momentum of 0.9. The learning rate is decayed
by a factor of 0.9875 with every epoch. As a warmup, the learning rate is linearly scaled from 0 to 0.0016 over the first 3 epochs. Following~\citep{rw2019timm}, we evaluate with an exponential moving average of the weights of the model, with
a decay of 0.99985. We use the random erase augmentation from \citep{rw2019timm}, as well as cutmix~\citep{yun2019cutmix},
with a probability of 0.2 and parameter to the beta distribution of 0.2.
All the models are trained on 8 GPUs. For \diffq, we used the penalties  in 

and the default group size .

\noindent{\bf ImageNet - DeiT.}
We use the official DeiT implementation by \citet{touvron2020training}\footnote{\url{https://github.com/facebookresearch/deit}}, with the default
training parameters, but without exponential moving averaging of the weights. More precisely, we trained for 300 epochs over 16 GPUs, with a batch size per GPU of 64, AdamW~\citep{loshchilov2017decoupled}, a weight decay of 0.05, learning rate of , cosine
learning rate scheduler, a learning rate warmup from  over 5 epochs and label smoothing~\citep{szegedy2016rethinking}. As data augmentation, we used color-jitter, random erase, and either cutmix or mixup~\citep{zhang2017mixup}.

For \diffq, we tested the penalty  in ,
and group size  in . We use a minimum number of bits of 3, instead of 2, as this led to better stability. 
We use Adam~\citep{adam} to optimize the bits parameters, with a learning rate of .

\vspace{-0.2cm}
\section{Supplementary results}
\vspace{-0.2cm}
\label{app:results}

\begin{table}[h!]
\caption{Detailed results of QAT and \diffq on the CIFAR-10/100 datasets. For each architecture and dataset,
we provide the performance of the baseline, QAT models with 2 to 4 bits, and two \diffq runs: v1. is the smallest model that is within a small range of the baseline performance, v2. is the best model of comparable size with QAT 2 bits, selected from the pool of candidates described in Section \ref{supp:image}. For Wide-ResNet, we report a single variant of \diffq, as it is both the smallest
and the one with the best accuracy.}
\label{tab:cifar_supp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|l|cc|cc|cc}
\toprule
	&	 & \multicolumn{2}{c}{MobileNet} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{WideResNet} \\
\midrule
	&	 & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ \\
\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{CIFAR-10}}
& Uncompressed    	        & 90.9 		   & 12.3 & 95.3  & 42.7  & 95.3    & 139.2\\
\cmidrule(lr){2-8}
& QAT 2bits    				& 78.1         & 0.88 & 87.2  & 2.70  & 70.8    & 8.81  \\
& QAT 3bits    				& 88.2         & 1.26 & 94.0  & 4.03  & 94.3    & 13.16 \\
& QAT 4bits    				& 90.1         & 1.64 & 95.0  & 5.36  & 94.4    & 17.50 \\
\cmidrule(lr){2-8}
 & \diffq v1 & 90.3 & 0.94 & 94.9 & 3.17 & 94.1 & 8.81\\
 & \diffq v2 & 87.9 & 0.91 & 93.9 & 2.71 & 94.1 & 8.81\\
\midrule\midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{CIFAR-100}}
& Uncompressed    	            & 68.1 		   & 12.6 & 77.9 & 42.8 & 76.2  & 139.4\\
\cmidrule(lr){2-8}
& QAT 2bits    		            & 10.9         & 0.91  & 58.7 & 2.72  & 46.5  & 8.83  \\
& QAT 3bits    		            & 59.7         & 1.29  & 73.7 & 4.05  & 75.0  & 13.18 \\
& QAT 4bits    		            & 66.9         & 1.69  & 77.3 & 5.39  & 75.5  & 17.53 \\
\cmidrule(lr){2-8}
 & \diffq v1 & 68.5 & 1.10 & 77.6 & 4.82 & 75.3 & 8.83\\
 & \diffq v2 & 64.6 & 0.94 & 71.7 & 2.72 & 75.6 & 8.84\\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\subsection{CIFAR-10/100}
\label{supp:cifar}
We report on Table~\ref{tab:cifar_supp} the results on the CIFAR10/100 datasets, which are shown for CIFAR100 in Figure~\ref{fig:cifar} in the main paper. Results are presented using MobileNet-v1, ResNet-18, and WideResNet. For CIFAR100 the presented results used for creating Figure~\ref{fig:cifar} in the main paper.
As we cannot show all the \diffq runs, we selected for each model and dataset two versions: v1 is the smallest model that has an accuracy
comparable to the baseline (accuracy is greater than  times the baseline accuracy), while v2 is the model with the highest accuracy
that is comparable in size with the QAT 2 bits model (size must be smaller than  times the baseline size, except for MobileNet, for which we had to allow a 4\% relative increase in size. The penalty and group size selected with this procedure is displayed on Table~\ref{table_hyper_supp}.

\begin{table}[h!]
\vspace{-0.4cm}
\caption{Penalty  and group size  for the v1 and v2 \diffq models reported on Table~\ref{tab:cifar_supp}}
\label{table_hyper_supp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|l|cc|cc|cc}
\toprule
	&	 & \multicolumn{2}{c}{MobileNet} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{WideResNet} \\
\midrule
	&	 &  &  &  &  &  &  \\
\midrule
\multirow{2}{*}{CIFAR-10}
 & \diffq v1 & 1 & 16 & 0.1 & 8 & 5 & 16\\
 & \diffq v2 & 5 & 8 & 5 & 4 & 5 & 16\\
\midrule\midrule
\multirow{2}{*}{CIFAR-100}
 & \diffq v1 & 1 & 16 & 0.05 & 4 & 5 & 16\\
 & \diffq v2 & 5 & 16 & 5 & 8 & 1 & 16\\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Looking first at v1 models, we achieve on all tasks and datasets
a model that is competitive with the baseline (sometimes even better),
with a model size that is smaller than a QAT 4 bits model (for instance more than 2MB saved on a ResNet-18 trained on CIFAR-10 compared to QAT 4 bits, for the same accuracy).

Now for v2, first note that as the minimum number of bits used by \diffq is exactly 2, it is not possible here to make a model smaller than QAT 2 bits. However, even with as little as 0.01 MB extra, \diffq can get up to 30\% increase in accuracy compared to QAT 2 bits (for a Wide ResNet). On all architectue and datasets,
the gain from \diffq over QAT 2 bits is at least 10\% accuracy.
This confirms in practice the bias of STE-based methods when the number
of bits is reduced, a bias that we already demonstrated in theory in Section~\ref{sec:counter}. In particular, it is interesting that the largest
improvement provided by \diffq is for the Wide ResNet model, which should be the easiest to quantize. But having the largest number of weights, it 
also likely the one that is the most sensitive to the oscillations of QAT quantized weights described in Section~\ref{sec:counter}.

Next, in Table~\ref{supp:tab:app_image_abb} we compare QAT against \diffq for model quantization using a fixed number of bits. These results are complementary to Table~\ref{tab:image_abb} in the main paper where we report results for CIFAR-100 using ResNet-18 model only. 

\begin{table*}[h!]
\vspace{-0.4cm}
\caption{A comparison between QAT and \diffq while we consider a fixed number of bits for all model parameters, specifically using 2, 3, and 4 bits. Results are reported for CIFAR-10 and CIFAR-100 using MobileNet-v1, ResNet-18. and WideResNet. We report Accuracy (Acc.) and Model Size (M.S.).}
\label{supp:tab:app_image_abb}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|l|cc|cc|cc}
\toprule
	&	 & \multicolumn{2}{c}{MobileNet} & \multicolumn{2}{c}{ResNet-18} & \multicolumn{2}{c}{WideResNet} \\
\midrule
	&	 & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ & Acc. (\%)~ &  M. S. (MB)~ \\
\midrule
\multirow{9}{*}{\rotatebox[origin=c]{90}{CIFAR-10}}
& Uncompressed    	        & 90.9 		   & 12.3 & 95.3  & 42.8  & 95.3    & 139.4\\
\cmidrule(lr){2-8}
& QAT 2bits    				& 78.1         & 0.88 & 87.2  & 2.70  & 70.8    & 8.81  \\
& QAT 3bits    				& 88.2         & 1.26 & 94.0  & 4.03  & 94.3    & 13.16 \\
& QAT 4bits    				& 90.1         & 1.64 & 95.0  & 5.36  & 94.4    & 17.50 \\
\cmidrule(lr){2-8}
& \diffq 2bits	 			& 84.1  	   & 0.88 & 92.3  & 2.70  & 94.4    & 8.81   \\
& \diffq 3bits				& 89.7         & 1.26 & 94.4  & 4.03  & 94.4    & 13.16  \\
& \diffq 4bits				& 90.4         & 1.64 & 95.1  & 5.36  & 94.6    & 17.50  \\
\midrule\midrule
\multirow{9}{*}{\rotatebox[origin=c]{90}{CIFAR-100}}
& Uncompressed    	            & 68.1 		   & 12.6 & 77.9 & 42.8 & 76.2  & 139.4\\
\cmidrule(lr){2-8}
& QAT 2bits    		            & 10.9         & 0.91  & 58.7 & 2.72  & 46.5  & 8.82  \\
& QAT 3bits    		            & 59.7         & 1.29  & 73.7 & 4.05  & 75.0  & 13.18 \\
& QAT 4bits    		            & 66.9         & 1.69  & 77.3 & 5.39  & 75.5  & 17.53 \\
\cmidrule(lr){2-8}
& \diffq 2bits	 			    & 17.2  	   & 0.91 & 66.6 & 2.72  & 72.8 & 8.82   \\
& \diffq 3bits				    & 60.1         & 1.29 & 76.7 & 4.05  & 76.9 & 13.18  \\
& \diffq 4bits				    & 66.8         & 1.69 & 77.5 & 5.39  & 76.9 & 17.53  \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\vspace{-0.2cm}
\subsection{EfficientNet-b3 on ImageNet}
\vspace{-0.1cm}
\label{supp:imnet}
On Table~\ref{supp:imagenet} we report the results for training
EfficientNet-b3~\cite{tan2019efficientnet} on the ImageNet dataset, matching the results reported on Figure~\ref{fig:teaser}.

\begin{table}[h!]
\caption{Image classification results for the ImageNet benchmark. Results are presented for \diffq and QAT using 4 and 8 bits using the EfficientNet-b3 model~\citep{tan2019efficientnet}. We report Top-1 Accuracy (Acc.) together with Model Size (M.S.).}
\label{supp:imagenet}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|cc}
\toprule
		& Top-1 Acc. (\%)~ & M.S. (MB)~ \\
\midrule
Uncompressed    	                & 81.6 		   & 46.7 \\
\midrule
QAT 4bits    		                &  57.3        & 6.3  \\
QAT 8bits    		                & \textbf{81.3}         & 12.0 \\
\midrule
\diffq ()  	                & 75.4      & \textbf{5.7} \\
\diffq ()  	    & 80.8      & 9.92  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

As previously, we selected two versions of \diffq, one matching the size of QAT 8bits, and one smallest than QAT 4 bits. At 8 bits, \diffq achieves the same accuracy as the uncompressed baseline, for a slightly smaller model than QAT 8bits. As we lower the number of bits, we again see a clear advantage for \diffq, with both a smaller model (5.7MB against 6.1MB) than QAT 4bits, and significantly higher accuracy (76.8\% vs. 57.3\%).

\begin{figure}[t!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{figs/oscillations_efnet.pdf}}
\caption{Model accuracy vs. epochs for ImageNet using EfficientNet-b3. Resutls are presented for both QAT4 and \diffq.}
\label{fig:app_oscillations}
\end{center}
\vskip -0.35in
\end{figure}

The lower accuracy for QAT4 on ImageNet led us to take a closer look at the model performance. Figure~\ref{fig:app_oscillations} depicts the model accuracy as a function of the number of epochs for both QAT4 and \diffq. Notice, similarly to the toy example presented in Section~\ref{sec:counter} training with QAT4 creates instability in the model optimization (especially near model convergence), which leads to significant differences in performance across adjacent epochs. When considering \diffq, model optimization is stable and no such differences are observed. 







\vspace{-0.2cm}
\subsection{Activation Quantization for Language Modeling}
\vspace{-0.1cm}
In Table~\ref{tab:app_text} we report language modeling results for a 16-layers Transformer models while applying activation quantization. Unlike the results in Table~\ref{tab:text} where we used per-channel activation quantization, here we report results with a histogram quantizer. Additionally when considering histogram quantizer, results suggest \diffq is superior to both QAT and QN when considering both model size and model performance.
\begin{table}[h!]
\vspace{-0.3cm}
\caption{
Language modeling results for a 16 layer Transformer trained on Wikitext-103. We also test combining weight and activation quantization using a histogram quantizer. We compared \diffq to QAT and Quant-Noise (QN) method proposed by~\citet{fan2020training} (models with  were trained with layer drop~\cite{fan2019reducing}.).}
\label{tab:app_text}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{llcc}
\toprule
	Weights & Activation	 & PPL~ & M. S. (MB)~ \\
\midrule
Uncompressed (Ours) & -  	   & 18.1 & 942\\
QAT 8bits    & -	& 18.2           & 236 \\
QAT 4bits   & - 	&   28.8         & 118 \\
\diffq  () & - & \textbf{18.0}          & 182 \\
\diffq  () & - & 18.5          & \textbf{113} \\
\midrule
    8 bits & 8 bits    & 19.5  & 236\\
QAT 8bits & 8 bits   	& 26.0  & 236 \\
QAT 4bits & 8 bits    	&   34.6   & 118 \\
\diffq  () & 8 bits & 19.1 & 182\\
\diffq  () & 8 bits & 19.2 & \textbf{113}\\
\midrule
Uncompressed  & -  	      & 18.3 & 942\\
QN 8 bits    & QN 8 bits & \textbf{18.7} & 236 \\
QN 4 bits    & QN 8 bits & 20.5 & 118 \\ 
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


%
 


\end{document}