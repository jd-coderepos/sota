\documentclass{article}



\usepackage[nonatbib, preprint]{neurips_2021}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[dvipsnames]{xcolor}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false, citecolor=ForestGreen]{hyperref}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathbbol}
\usepackage{algorithmic}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{multirow}
\usepackage{multicol} 
\usepackage{array}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{arydshln}

\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
	\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
	\newcommand{\innerproduct}[2]{\left\langle#1, #2\right\rangle}
	\newcommand{\fan}[1]{\Vert #1 \Vert}
	\newcommand{\qileft}{[\kern-0.15em[}
	\newcommand{\qiLeft}{\left[\kern-0.4em\left[}
	\newcommand{\qiright}{]\kern-0.15em]}
	\newcommand{\qiRight}{\right]\kern-0.4em\right]}
	\newcommand{\sign}{{\mbox{sign}}}
	\newcommand{\diag}{{\mbox{diag}}}
	\newcommand{\armin}{{\mbox{argmin}}}
	\newcommand{\rank}{{\mbox{rank}}}
	\renewcommand{\vec}{{\mbox{vec}}}
	\newcommand{\st}{{\mbox{s.t.}}}
\newcommand{\<}{\left\langle}
	\renewcommand{\>}{\right\rangle}
	\newcommand{\lbar}{\left\|}
	\newcommand{\rbar}{\right\|}
	\newcommand{\eg}{{\emph{e.g.}}}
	\newcommand{\ie}{{\emph{i.e.}}}
	\newcommand{\wrt}{{{w.r.t.}}}
	\newcommand{\etal}{{\emph{et.al.},}}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}} \renewcommand{\algorithmicensure}{\textbf{Output:}} \renewcommand{\Roman}[1]{\uppercase\expandafter{\romannumeral#1}}
	\newcommand{\red}[1]{{\color{red}{#1}}}
	\newcommand{\blue}[1]{{\color{blue}{#1}}}
	
	
\renewcommand{\a}{{\bm{a}}}
	\renewcommand{\b}{{\bm{b}}}
	\renewcommand{\d}{{\bm{d}}}
	\newcommand{\e}{{\bm{e}}}
	\newcommand{\f}{{\bm{f}}}
	\newcommand{\g}{{\bm{g}}}
	\renewcommand{\o}{{\bm{o}}}
	\newcommand{\p}{{\bm{p}}}
	\newcommand{\q}{{\bm{q}}}
	\renewcommand{\r}{{\bm{r}}}
	\newcommand{\rr}{{\bm{r}}}
	\newcommand{\s}{{\bm{s}}}
	\renewcommand{\t}{{\bm{t}}}
	\renewcommand{\u}{{\bm{u}}}
	\renewcommand{\v}{{\bm{v}}}
	\newcommand{\w}{{\bm{w}}}
	\newcommand{\x}{{\bm{x}}}
	\newcommand{\y}{{\bm{y}}}
	\newcommand{\z}{{\bm{z}}}
	\newcommand{\balpha}{{\bm{\alpha}}}
	\newcommand{\bbeta}{{\bm{\beta}}}
	\newcommand{\bmu}{{\bm{\mu}}}
	\newcommand{\bsigma}{{\bm{\sigma}}}
	\newcommand{\blambda}{{\bm{\lambda}}}
	\newcommand{\btheta}{{\bm{\theta}}}
	\newcommand{\bgamma}{{\bm{\gamma}}}
	\newcommand{\bxi}{{\bm{\xi}}}
	\newcommand{\bphi}{{\bm{\phi}}}
	
\newcommand{\ba}{{\bm{A}}}
	\newcommand{\bb}{{\bm{B}}}
	\newcommand{\bc}{{\bm{C}}}
	\newcommand{\bd}{{\bm{D}}}
	\newcommand{\be}{{\bm{E}}}
	\newcommand{\bg}{{\bm{G}}}
	\newcommand{\bi}{{\bm{I}}}
	\newcommand{\bj}{{\bm{J}}}
	\newcommand{\bl}{{\bm{L}}}
	\newcommand{\bo}{{\bm{O}}}
	\newcommand{\bp}{{\bm{P}}}
	\newcommand{\bq}{{\bm{Q}}}
	\newcommand{\bs}{{\bm{S}}}
	\newcommand{\bu}{{\bm{U}}}
	\newcommand{\bv}{{\bm{V}}}
	\newcommand{\bw}{{\bm{W}}}
	\newcommand{\bx}{{\bm{X}}}
	\newcommand{\by}{{\bm{Y}}}
	\newcommand{\bz}{{\bm{Z}}}
	\newcommand{\bTheta}{{\bm{\Theta}}}
	\newcommand{\bSigma}{{\bm{\Sigma}}}
	
\newcommand{\A}{{\mathcal{A}}}
	\newcommand{\B}{\mathcal{B}}
	\newcommand{\C}{\mathcal{C}}
	\newcommand{\D}{\mathcal{D}}
	\newcommand{\F}{\mathcal{F}}
	\renewcommand{\H}{\mathcal{H}}
	\newcommand{\I}{\mathcal{I}}
	\renewcommand{\L}{\mathcal{L}}
	\newcommand{\N}{\mathcal{N}}
	\renewcommand{\P}{\mathcal{P}}
	\newcommand{\X}{\mathcal{X}}
	\newcommand{\Y}{\mathcal{Y}}
	\newcommand{\W}{\mathcal{W}}
	


\title{ReSSL: Relational Self-Supervised Learning with Weak Augmentation}




\author{Mingkai Zheng$^1$ \quad Shan You$^{1,2}$\thanks{Corresponding author.}  \quad Fei Wang$^1$\\
	 \textbf{Chen Qian$^1$ \quad Changshui Zhang$^{2}$ \quad Xiaogang Wang$^{1,3}$ \quad Chang Xu$^4$}\\
	$^1$SenseTime Research\\
	 $^{2}$Department of Automation, Tsinghua University,\\
Institute for Artificial Intelligence, Tsinghua University (THUAI), \\
Beijing National Research Center for Information Science and Technology (BNRist)\\
	 $^3$The Chinese University of Hong Kong\\
	$^4$School of Computer Science, Faculty of Engineering, The University of Sydney\\
	 \texttt{\{zhengmingkai,youshan,wangfei,qianchen\}@sensetime.com}, \\
 \texttt{zcs@mail.tsinghua.edu.cn}, \texttt{xgwang@ee.cuhk.edu.hk},\texttt{c.xu@sydney.edu.au},\\ 
	
}


\begin{document}

\maketitle

\vspace{-10pt}
\begin{abstract}
\vspace{-10pt}


Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most of methods mainly focus on the instance level information (\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduced a novel SSL paradigm, which we term as relational self-supervised learning  (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. Moreover, to boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. Experimental results show that our proposed ReSSL significantly outperforms the previous state-of-the-art algorithms in terms of both performance and training efficiency. Code is available at \href{https://github.com/KyleZheng1997/ReSSL}{https://github.com/KyleZheng1997/ReSSL}



\end{abstract}

\section{Introduction}

Recently, self-supervised learning (SSL) has shown its superiority and achieved promising results for unsupervised visual representation learning in computer vision tasks \cite{cmc, deepinfomax, cpc, simclr, SimSiam, instance_discrimination, byol, moco}. The purpose of a typical self-supervised learning algorithm is to learn general visual representations from a large amount of data without human annotations, which can be transferred or leveraged in downstream tasks (\eg, classification, detection, and segmentation). Some previous works \cite{swav, byol} even have proven that a good unsupervised pretraining can lead to a better downstream performance than supervised pretraining.  

Among various SSL algorithms, contrastive learning \cite{instance_discrimination, alignment_uniformity, simclr} serves as a state-of-the-art framework, which mainly focuses on learning an invariant feature from different views. For example, instance discrimination is a widely adopted pre-text task as in \cite{simclr, moco, instance_discrimination},  which utilizes the noisy contrastive estimation (NCE) to encourage two augmented views of the same image to be pulled closer on the embedding space but pushes apart all the other images away. Deep Clustering \cite{deepclustering, Self-labelling, swav} is an alternative pre-text task that forces different augmented views of the same instance to be clustered into the same class. However, instance discrimination based methods will inevitably induce a class collision problem \cite{contrastive_theory, PCL, debiased},  where similar images should be pulled closer instead of being pushed away. Deep clustering based methods cooperated with traditional clustering algorithms to assign a label for each instance, which relaxed the constraint of instance discrimination, but most of these algorithms adopt a strong assumption, \ie, the labels must induce an equipartition of the data, which might introduce some noise and hurt the learned representations.




In this paper, we introduce a novel Relational Self-Supervised Learning framework (ReSSL), which does not encourage explicitly to push away different instances, but uses \textit{relation} as a manner to investigate the inter-instance relationships and highlight the intra-instance invariance. Concretely, we aim to maintain the consistency of pairwise similarities among different instances for two different augmentations. For example, if we have three instances  $\mathbf{x}^1$, $\mathbf{x}^2$, $\mathbf{y}$ and $\mathbf{z}$ where $\mathbf{x}^1$, $\mathbf{x}^2$ are two different augmentations of $\mathbf{x}$,  $\mathbf{y}$ and $\mathbf{z}$ are different samples. Then, if $\mathbf{x}^1$ is similar to $\mathbf{y}$ but different to $\mathbf{z}$, we wish $\mathbf{x}^2$ can maintain such relationship and vice versa. In this way, the relation can be modelled as a similarity distribution between a set of augmented images, and then use it as a metric to align the same images with different augmentations, so that the relationship between different instances could be maintained across different views. 





However, this simple manner induces unexpectedly horrible performance if we follow the same training recipe as other contrastive learning methods \cite{simclr, moco}. We argue that construction of a proper relation matters for ReSSL; aggressive data augmentations as in \cite{simclr, simclrv2, goodview} are usually leveraged by default to generate diverse positive pairs that increase the difficulty of the pre-text task. However, this hurts the reliability of the target relation. Views generated by aggressive augmentations might cause the loss of semantic information, so the target relation might be noisy and not that reliable. In this way, we propose to leverage weaker augmentations to represent the relation, since much lesser disturbances provide more stable and meaningful relationships between different instances. Besides, we also sharpen the target distribution to emphasize the most important relationship and utilize the memory buffer with a momentum-updated network to reduce the demand of large batch size for more efficiency. Experimental results on multiple benchmark datasets show the superiority of ReSSL in terms of both performance and efficiency. For example, with 200 epochs of pre-training, our ReSSL achieved 69.9\% on ImageNet \cite{imagenet_cvpr09} linear evaluation protocol, which is 2.4\% higher than our baseline method (MoCoV2 \cite{mocov2}). When working with the Multi-Crop strategy (200 epochs), ReSSL achieved new state-of-the-art 74.7\% Top-1 accuracy, which is 1.4\% higher than CLSA-Multi \cite{stronger}.










Our contributions can be summarized as follows.
\begin{itemize}
    \item We proposed a novel SSL paradigm, which we term it as relational self-supervised learning (ReSSL). ReSSL maintains the relational consistency between the instances under different augmentations instead of explicitly pushing different instances away.
    
    \item  Our proposed weak augmentation and sharpening distribution strategy provide a stable and high quality target similarity distribution, which makes the framework works well.
    
    \item  ReSSL is a simple and effective SSL framework since it replaces the widely adopted contrastive loss with our proposed relational consistency loss. It achieved state-of-the-art performance under the same training cost.
\end{itemize}



 \section{Related Works}
\textbf{Self-Supervised Learning}.
Early works in self-supervised learning methods rely on all sorts of pretext to learn visual representations. For example, colorizing gray-scale images \cite{img_color}, image jigsaw puzzle \cite{jigsaw}, image super-resolution \cite{srgan}, image inpainting \cite{inpainting}, predicting a relative offset for a pair of patches \cite{patch_prediction}, predicting the rotation angle \cite{rotation_selfsup}, and image reconstruction \cite{autoencoder, gan, biggan, bigbigan}. Although these methods have shown their effectiveness, they lack the generality of the learned representations. 

\textbf{Instance Discrimination}.
The recent contrastive learning methods \cite{cpc, cmc, simclr, moco, goodview, pirl, deepinfomax, mochi} have made a lot of progress in the field of self-supervised learning. Most of the previous contrastive learning methods are based on the instance discrimination \cite{instance_discrimination} task in which positive pairs are defined as different views of the same image, while negative pairs are formed by sampling views from different images. SimCLR \cite{simclr, simclrv2} shows that image augmentation (\eg Grayscale, Random Resized Cropping, Color Jittering, and Gaussian Blur), nonlinear projection head and large batch size plays a critical role in contrastive learning. Since large batch size usually requires a lot of GPU memory, which is not very friendly to most of researchers. MoCo \cite{moco, mocov2} proposed a momentum contrast mechanism that forces the query encoder to learn the representation from a slowly progressing key encoder and maintain a memory buffer to store a large number of negative samples. InfoMin \cite{goodview} proposed a set of stronger augmentation that reduces the mutual information between views while keeping task-relevant information intact. AlignUniform \cite{alignment_uniformity} shows that alignment and uniformity are two critical properties of contrastive learning. 

\textbf{Deep Clustering}. 
In contrast to instance discrimination which treats every instance as a distinct class, deep clustering \cite{deepclustering} adopts the traditional clustering method (\eg KMeans) to label each image iteratively. Eventually, similar samples will be clustered into the same class. Simply apply the KMeans algorithm might lead to a degenerate solution where all data points are mapped to the same cluster; SeLa \cite{Self-labelling} solved this issue by adding the constraint that the labels must induce equipartition of the data and proposed a fast version of the Sinkhorn-Knopp to achieve this. SwAV \cite{swav} further extended this idea and proposed a scalable online clustering framework. PCL \cite{PCL} reveals the class collision problem and simply performed instance discrimination and unsupervised clustering simultaneously; although it gets the same linear classification accuracy with MoCoV2, it has better performance on downstream tasks. 


\textbf{Contrastive Learning Without Negatives}. 
Most previous contrastive learning methods prevent the model collapse in an explicit manner (\eg push different instances away from each other or force different instances to be clustered into different groups.) BYOL \cite{byol} can learn a high-quality representation without negatives. Specifically, it trains an online network to predict the target network representation of the same image under a different augmented view and using an additional predictor network on top of the online encoder to avoiding the model collapse. SimSiam \cite{SimSiam} shows that simple Siamese networks can learn meaningful representations even without the use of negative pairs, large batch size, and momentum encoders.  
\label{section:method}
\section{Methodology}

In this section, we will first revisit the preliminary work on contrastive learning; then, we will introduce our proposed relational self-supervised learning framework. After that, the algorithm and the implementation details will also be explained.

\subsection{Preliminaries on Self-supervised Learning}
Given $N$ unlabeled samples $\mathbf{x}$, we randomly apply a composition of augmentation functions $T(\cdot)$ to obtain two different views $\mathbf{x}^{1}$ and $\mathbf{x}^{2}$ through $T(\mathbf{x}, \theta_{1})$ and $T(\mathbf{x}, \theta_{2})$ where $\theta$ is the random seed for $T$. Then, a convolutional neural network based encoder $\mathcal{F}(\cdot)$ is employed to extract the information from these samples, i.e.,   $\mathbf{h} = \mathcal{F}(T(\mathbf{x}, \theta))$. Finally, a two-layer non-linear projection head $g(\cdot)$ is utilized to map $\mathbf{h}$ into embedding space, which can be written as: $\mathbf{z} = g(\mathbf{h})$. SimCLR \cite{simclr} and MoCo \cite{moco} style framework adopt the noise contrastive estimation (NCE) objective for discriminating different instances in the dataset. Suppose $\mathbf{z}^{1}_{i}$ and $\mathbf{z}^{2}_{i}$ are the representations of two augmented views of $\mathbf{x}_{i}$ and $\mathbf{z}_{k}$ is a different instance. The NCE objective can be expressed by Eq. \eqref{equation:nce}, where the similarity function $sim(\cdot)$ represents the dot product between $L_2$ normalized vectors $sim(\mathbf{u}, \mathbf{v}) = \mathbf{u}^{T}\mathbf{v} / \lVert \mathbf{u} \lVert \lVert \mathbf{v} \lVert $ and $\tau$ is the temperature parameter. 
\vspace{-3pt}
\begin{equation}
    \label{equation:nce}
    \mathcal{L}_{NCE} = -\log \frac{\exp(sim(\mathbf{z}^{1}, \mathbf{z}^{2})/ \tau) }{ \exp(sim(\mathbf{z}_{i}^{1}, \mathbf{z}_{i}^{2}) / \tau ) + \sum_{k=1}^{N}  \exp(sim(\mathbf{z}_{i}^{1}, \mathbf{z}_{k}) / \tau ) }.
\end{equation}

BYOL \cite{byol} and SimSiam \cite{SimSiam} style framework add an additional non-linear predictor head $q(\cdot)$ which further maps $\mathbf{z}$ to $\mathbf{p}$. The model will minimize the negative cosine similarity (equivalent to minimize the L2 distance) between $\mathbf{z}$ to $\mathbf{p}$.  
\vspace{-4pt}
\begin{align}
    \label{equation:byol}
    \mathcal{L}_{cos} &= - \frac{\mathbf{p}^1}{\lVert \mathbf{p}^1 \lVert} \cdot \frac{\mathbf{z}^2}{\lVert \mathbf{z}^2 \lVert}, & \mathcal{L}_{mse} = \lVert  \mathbf{p}^1 - \mathbf{z}^2\lVert^2_2.
\end{align}
Tricks like stop-gradient and momentum teacher are often applied to avoid model collapsing.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Assets/framework.pdf}
    \vspace{-15pt}
    \caption{The overall framework of our proposed method. We adopt the student-teacher framework where the student is trained to predict the representation of the teacher, and the teacher is updated with a “momentum update” (exponential moving average) of the student. The relationship consistency is achieve by align the conditional distribution for student and teacher model. Please see more details in our method part.}
    \label{fig:framework}
    \vspace{-15pt}
\end{figure}

\vspace{-3pt}
\subsection{Relational Self-Supervised Learning}
In classical self-supervised learning, different instances are to be pushed away from each other, and augmented views of the same instance is expected to be of exactly the same features. However, both constrains are too restricted because of the existence of similar samples and the distorted semantic information if aggressive augmentation is adopted. In this way, we do not encourage explicit negative instances (those to be pushed away) for each instance; instead, we leverage the pairwise similarities as a manner to explore their relationships. And we pull the features of two different augmentations in this sense of relation metric. As a result, our method relaxes both \eqref{equation:nce} and \eqref{equation:byol}, where different instances do not always need to be pushed away from each other; and augmented views of the same instance only need to share the similar but not exactly the same features. 

Concretely, given a image $\mathbf{x}$ in a batch of samples , two different augmented views can be obtained by $\mathbf{x}^1 = T(\mathbf{x}, \theta_1)$, $\mathbf{x}^2 = T(\mathbf{x}, \theta_2)$ and calculate the corresponds embedding  $\mathbf{z}^1 = g(\mathcal{F} (\mathbf{x}^1))$, $\mathbf{z}^2 = g(\mathcal{F}(\mathbf{x}^2))$. Then, we calculate the similarities between the instances of the first augmented images. Which can be measured by $sim(\mathbf{z}^{1}, \mathbf{z}_{i})$. A softmax layer can be adopted to process the calculated similarities, which then produces a relationship distribution:
\begin{align}
    \mathbf{p}^1_i &= \frac{\exp(sim(\mathbf{z}^{1}, \mathbf{z}_{i})/ \tau_t) }{\sum_{k=1}^{K}  \exp(sim(\mathbf{z}^{1}, \mathbf{z}_{k}) / \tau_t, ) }. \label{equation:teacher}
\end{align}
where $\tau_t$ is the temperature parameter. At the same time, we can calculate the relationship between $\mathbf{x}^2$ and the $i$-th instance as $sim(\mathbf{z}^{2}, \mathbf{z}_{i})$. The resulting relationship distribution can be written as:
\begin{align}
    \mathbf{p}^2_i &= \frac{\exp(sim(\mathbf{z}^{2}, \mathbf{z}_{i})/ \tau_s) }{\sum_{k=1}^{K}  \exp(sim(\mathbf{z}^{2}, \mathbf{z}_{k}) / \tau_s, ) }. \label{equation:student}
\end{align}
where $\tau_s$ is a different temperature parameter. We propose to push the relational consistency between $p^{1}_i$ and $p^{2}_i$ by minimizing the Kullback–Leibler divergence, which can be formulated as:
\begin{equation}
    \label{equation:loss}
    \mathcal{L}_{relation} =  D_{KL} (\mathbf{p}^1 || \mathbf{p}^2) = H(\mathbf{p}^1, \mathbf{p}^2) - H(\mathbf{p}^1).
\end{equation}
Since the $\mathbf{p}^1$ will only be used as a target, the gradient will be clipped here to avoid the model collapsing, thus we only minimize the cross-entropy term $H(\mathbf{p}^1,\mathbf{p}^2)$ in our implementation.


\textbf{More efficiency with Momentum targets.} 
However, the quality of the target similarity distribution $\mathbf{p}^1$ is crucial, to make the similarity distribution reliable and stable, we usually require a large batch size which is very unfriendly to GPU memories. To resolve this issue, we utilize a ``momentum update" network as in \cite{moco, mocov2}, and maintain a large memory buffer $\mathcal{Q}$ of $K$ past samples $\{\mathbf{z}_k | k = 1, ... , K \}$ (follow the FIFO principle) for storing the feature embeddings from the past batches, which can then be used for simulating the large batch size relationship and providing a stable similarity distribution. 
\begin{equation} \label{equation:ema}
    \mathcal{F}_t \leftarrow m \mathcal{F}_t + (1 - m)\mathcal{F}_s, \quad  g_t \leftarrow m g_t + (1 - m) g_s,
\end{equation}
where $\mathcal{F}_s$ and $g_s$ denote the most latest encoder and head, respectively, so we name them as the student model with a subscript $s$. On the other hand, $\mathcal{F}_t$ and $g_t$ stand for ensembles of the past encoder and head, respectively, so we name them as the teacher model with a subscript $t$. $m$ represents the momentum coefficient which controls how fast the teacher $\mathcal{F}_t$ will be updated.


























\textbf{Sharper Distribution as Target}.
 Note, the value of $\tau_t$ has to be smaller than $\tau_s$ since $\tau_t$ will be used to generate the target distribution. A smaller $\tau$ will result in a ``sharper" distribution which can be interpreted as highlight the most similar feature for $\mathbf{z}^1$. Align $\mathbf{p}^2$ with $\mathbf{p}^1$ can be regarded as pulling $\mathbf{z}^2$ towards the features that are similar with $\mathbf{z}^1$.

\textbf{Weak Augmentation Strategy for Teacher}.
To further improve the quality and stability of the target distribution, we adopt a weak augmentation strategy for the teacher model since the standard contrastive augmentation is too aggressive, which introduced too many disturbances and will mislead the student network. Please refer to more details in our empirical study. 




\textbf{Compare with SEED and CLSA}. SEED \cite{seed} follows the standard Knowledge Distillation (KD) paradigm \cite{hinton2015distilling,romero2014fitnets,you2017learning} where it aims to distill the knowledge from a larger network into a smaller architecture. The knowledge transfer happens in the same view but between different models. In our framework, we are trying to maintain the relational consistency between different augmentations; the knowledge transfer happens between different views but in the same network. CLSA \cite{stronger} also introduced the concept of using weak augmentation to guide a stronger augmentation. However, the ``weak" augmentation in CLSA is equivalent to the ``strong" augmentation in our method (We do not use any stronger augmentations such as \cite{autoaugment, randaugment}). On the other hand, CLSA still adopts the InfoNCE loss \eqref{equation:nce} for instance discrimination, where our proposed method only utilized the relational consistency loss \eqref{equation:loss}. Finally, CLSA requires at least one additional sample during training, which will slow down the training speed.


\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{Input}
\Input{
$\mathbf{x} $: a batch of samples. $T_w(\cdot)$: Weak augmentation function. $T_c(\cdot)$: Contrastive augmentation function. $\mathcal{F}_t$ and $\mathcal{F}_s$: the teacher and student backbone network. $g_t$ and $g_s$ : the non-linear projection head for teacher and student. $Q$: the memory buffer  }
\While{network not converge} {
    \For{i=1 to step}{
        Fetch $\mathbf{x}$ from current batch $\mathcal{B}$
        
        $\mathbf{z}^1 = g_t( \mathcal{F}_t(T_w(\mathbf{x}, \theta_1)) )$;  \ \quad\quad $\mathbf{z}^2 = g_s( \mathcal{F}_s(T_c(\mathbf{x}, \theta_2)) )$;
        
        $\mathbf{p}^1$ = SoftMax($\mathbf{z}^1 Q^T$ / $\tau_t$ ); \quad\quad  $\mathbf{p}^2$ = SoftMax($\mathbf{z}^2 Q^T$ / $\tau_s$ )  \tcp*{ Eq. \eqref{equation:teacher}\eqref{equation:student}}
        
        Calculate $\mathcal{L}_{relation}$ loss by CrossEntropy($\mathbf{p}^1$, $\mathbf{p}^2$) \tcp*{ Eq. \eqref{equation:loss}}
        
        Update $\mathcal{F}_s$ and $g_s$ with loss $\mathcal{L}_{relation}$
        
        Update $\mathcal{F}_t$ and $g_t$ by $\mathcal{F}_t \leftarrow m \mathcal{F}_t + (1 - m) \mathcal{F}_s$,  $g_t \leftarrow m g_t + (1 - m) g_s$ \tcp*{ Eq. \eqref{equation:ema}}
        
        Update the memory buffer $Q$ by $\mathbf{z}^1$
    }
}
\SetKwInOut{Output}{Output}
\Output{The well trained model $\mathcal{F}_s$}
\caption{Relational Self-supervised Learning with Weak Augmentation (ReSSL)}
\label{alg:overall}
\end{algorithm}






 \vspace{-10pt}
\section{Empirical Study}
\vspace{-10pt}
In this section, we will empirically study our proposed method on 4 popular self-supervised learning benchmarks and compare to previous state-of-the-art algorithms (SimCLR \cite{simclr}, BYOL \cite{byol}, SimSiam \cite{SimSiam}, MoCoV2 \cite{mocov2}).

\textbf{Small Dataset}.
CIFAR-10 and CIFAR-100 \cite{cifar}. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. CIFAR-100 is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class.

\textbf{Medium Dataset}.
STL-10 \cite{stl10} and Tiny ImageNet \cite{tinyImagenet}. STL10 \cite{stl10} dataset is composed of 96x96 resolution images of 10 classes, 5K labeled training images, 8K validation images, and 100K unlabeled images. The Tiny ImageNet dataset is composed of 64x64 resolution images of 200 classes with 100K training images and 10k validation images.


\textbf{Implementation Details}
We adopt the ResNet18 \cite{resnet} as our backbone network. Because most of our dataset contains low-resolution images, we replace the first 7x7 Conv of stride 2 with 3x3 Conv of stride 1 and remove the first max pooling operation for a small dataset. For data augmentations, we use the random resized crops (the lower bound of random crop ratio is set to 0.2), color distortion (strength=0.5) with a probability of 0.8, and Gaussian blur with a probability of 0.5. The images from the small and medium datasets will be resized to 32x32 and 64x64 resolution respectively. Our method is based on MoCoV2 \cite{mocov2}; in order to simulate the shuffle BN trick on one GPU, we simply divide a batch of data into different groups and then calculate BN statistics within each group. The momentum value and memory buffer size are set to 0.99/0.996 and 4096/16384 for small and medium datasets respectively. Moreover, The model is trained using SGD optimizer with a momentum of 0.9 and weight decay of $5e^{-4}$. We linear warm up the learning rate for 5 epochs until it reaches $0.06 \times BatchSize / 256$, then switch to the cosine decay scheduler  \cite{cosine_lr}. For the implementation details of other methods, please refers to the supplementary material.

\textbf{Evaluation Protocol}.
All the models will be trained for 200 epochs. For testing the representation quality, we evaluate the pre-trained model on the widely adopted linear evaluation protocol - We will freeze the encoder parameters and train a linear classifier on top of the average pooling features for 100 epochs. To test the classifier, we use the center crop of the test set and computes accuracy according to predicted output. We train the classifier with a learning rate of 30, no weight decay, and momentum of 0.9. The learning rate will be times 0.1 in 60 and 80 epochs. Note, for STL-10; the pretraining will be applied on both labeled and unlabeled images. During the linear evaluation, only the labeled 5K images will be used.
\vspace{-5pt}

\renewcommand\arraystretch{1.12}
\begin{table}[h]
 \centering
 \setlength\tabcolsep{10pt}
 \small
 \caption{Compare to other SSL algorithms on small and medium dataset.}
 \vspace{-5pt}
 \label{table:compare_smalldata}
\begin{tabular}{c c c c c c c} 
\toprule 
Method & BackProp & EMA & CIFAR-10 & CIFAR-100 & STL-10 & Tiny ImageNet \\ \hline
Supervised            & -  & -   & 94.22 & 74.66 & 82.55 & 59.26 \\ \hline
SimCLR \cite{simclr}  & 2x & No  & 84.92 & 59.28 & 85.48 & 44.38 \\
BYOL \cite{byol}      & 2x & Yes & 85.82 & 57.75 & 87.45 & 42.70 \\
SimSiam \cite{SimSiam}& 2x & No  & 88.51 & 60.00 & 87.47 & 37.04 \\
MoCoV2  \cite{mocov2} & 1x & Yes & 86.18 & 59.51 & 85.88 & 43.36 \\
ReSSL (Ours)          & 1x & Yes & \textbf{90.20} & \textbf{63.79} & \textbf{88.25} & \textbf{46.60}  \\
\toprule 
\end{tabular}
\end{table}

\vspace{-5pt}
\textbf{Result}.
As we can see the result in Table \ref{table:compare_smalldata}, our proposed method outperforms the previous method on all four benchmarks. Reminder, most of the previous method requires twice back-propagation, which results in a much higher training cost than MoCoV2 and our method.


\subsection{A Properly Sharpened Relation is A Better Target}
The temperature parameter is very crucial in most contrastive learning algorithms. To verify the effective of $\tau_s$ and $\tau_t$ for our proposed method, we fixed $\tau_s = 0.1$ or $0.2$, and sweep over $\tau_t = \{0.01, 0.02, ..., 0.07\}$. The result is shown in Table \ref{table:ablation_t}. For $\tau_t$, the optimal value is either $0.04$ or $0.05$ across all different datasets. As we can see, the performance is increasing when we increase $\tau_t$ from $0$ to $0.04$ and $0.05$. After that, the performance will start to decrease. Note, $\tau_t \rightarrow 0$ correspond to the Top-1 or $argmax$ operation which produce a one-hot distribution as the target. On the other hand, when $\tau_t \rightarrow 0.1$, the target will be a much flatter distribution that cannot highlight the most similar features for students. Hence, $\tau_t$ can not be either too small or too large, but it has to be smaller than $\tau_s$ ($\mathbf{p}^1$ has to be sharper than $\mathbf{p}^2$) so that the target distribution can provide effective guidance to the student model.
\vspace{-5pt}

\renewcommand\arraystretch{1.15}
\begin{table}[h]
 \centering
 \setlength\tabcolsep{5pt}
 \small
 \caption{Effect of different $\tau_t$ and $\tau_s$ for ReSSL}
 \vspace{-5pt}
 \label{table:ablation_t}
\begin{tabular}{c c c c c c c c c } 
\toprule 
Dataset        & $\tau_s$ & $\tau_t = 0.01$ & $\tau_t = 0.02$ & $\tau_t = 0.03$ & $\tau_t = 0.04$ & $\tau_t = 0.05$ & $\tau_t = 0.06$ & $\tau_t = 0.07$ \\ \hline
CIFAR-10       & 0.1 & 89.35  & 89.74 & 90.09 & 90.04 & \textbf{90.20} & 90.18 & 88.67 \\
CIFAR-10       & 0.2 & 89.52  & 89.67 & 89.24 & 89.50 & 89.22 & 89.40 & 89.50 \\ \hline
CIFAR-100      & 0.1 & 62.34  & 62.79 & 62.71 & \textbf{63.79} & 63.46 & 63.20 & 61.31 \\
CIFAR-100      & 0.2 & 60.37  & 60.05 & 60.24 & 60.09 & 59.09 & 59.12 & 59.76 \\ \hline
STL-10         & 0.1 & 86.65  & 86.96 & 87.16 & 87.32 & \textbf{88.25} & 87.83 & 87.08 \\
STL-10         & 0.2 & 85.17  & 86.12 & 85.01 & 85.67 & 85.21 & 85.51 & 85.28 \\ \hline
Tiny ImageNet  & 0.1 & 45.20  & 45.40 & 46.30 & \textbf{46.60} & 45.08 & 45.24 & 44.18 \\
Tiny ImageNet  & 0.2 & 43.28  & 42.98 & 43.58 & 42.12 & 42.70 & 42.76 & 42.60 \\
\toprule 
\end{tabular}
\end{table}
For $\tau_t$, it is clearly to see that the result of $\tau_s = 0.1$ can always result a much higher performance than $\tau_s = 0.2$, which is different to MoCoV2 where $\tau_s = 0.2$ is the optimal value. According to \cite{NormFace, CosFace, ArcFace}, a greater temperature will result in a larger angular margin in the hypersphere. Since MoCoV2 adopts instance discrimination as the pretext task, a large temperature can enhance the compactness for the same instance and discrepancy for different instances. In contrast to instance discrimination, our method can be interpreted as pulling similar instances closer on the hypersphere; when the ground truth label is not available, the large angular margin might hurt the performance.


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{Assets/nn.pdf}
    \vspace{-10pt}
    \caption{Visualization of the 10 nearest neighbour of the query image. The top half is the result when we apply the weak augmentation. The bottom half is the case when the typical contrastive augmentation is adopted. Note, we use the red square to highlight the images that has different ground truth label with the query image.}
    \label{fig:nn}
    \vspace{-12pt}
\end{figure}


\subsection{Weak Augmentation Makes Better Relation}
As we have mentioned, the weaker augmentation strategy for the teacher model is the key to the success of our framework. Here, We implement the weak augmentation as a random resized crop (the random ratio is set to $(0.2, 1)$) and a random horizontal flip.  For temperature parameter, we simply adopt the same setting as in Table \ref{table:ablation_t} and report the performance of the best setting. The result is shown in Table \ref{table:ablation_weakaug}, as we can see that when we use the weak augmentation for the teacher model, the performance is significantly boosted across all datasets. We believe that this phenomenon is because relatively small disturbances in the teacher model can provide more accurate similarity guidance to the student model. To further verify this hypothesis, we random sampled three image from STL-10 training set as the query images, and then find the 10 nearest neighbour based on the weak / contrastive augmented query. We visualized the result in Figure \ref{fig:nn}, 

\renewcommand\arraystretch{1.15}
\begin{table}[h]
 \centering
 \setlength\tabcolsep{5pt}
 \small
 \vspace{-5pt}
 \caption{Effect of weak augmentation guided ReSSL }
 \vspace{-5pt}
 \label{table:ablation_weakaug}
\begin{tabular}{c c c c c c c } 
\toprule 
Teacher Aug & Student Aug & CIFAR-10 & CIFAR-100 & STL-10 & Tiny ImageNet \\ \hline
Contrastive & Contrastive  & 86.17 & 57.60 & 84.71 & 40.38 \\
Weak   & Contrastive  & \textbf{90.20} & \textbf{63.79} & \textbf{88.25} & \textbf{46.60}   \\ \hline
\toprule 
\end{tabular}
\vspace{-15pt}
\end{table}


\subsection{Dimension of the Relation}
Since we also adopt the memory buffer as in MoCo \cite{moco}, the buffer size will be equivalent to the dimension of the distribution $\mathbf{p}^1$ $\mathbf{p}^2$. Thus, it will be one of the crucial points in our framework. To verify the effect the memory buffer size, we simply keep $\tau_s = 0.1$ and $\tau_t = 0.04$, then varying the memory buffer size from 256 to 32768. The result is shown in Table \ref{table:ablation_buffer}, as we can see that a larger memory buffer can significantly boost the performance. However, a further increase in the buffer size can only bring a marginal improvement when the buffer is large enough.

\renewcommand\arraystretch{1.15}
\begin{table}[h]
\vspace{-5pt}
 \centering
 \setlength\tabcolsep{5pt}
 \small
 \caption{Effect of different memory buffer size on small and medium dataset}
 \vspace{-5pt}
 \label{table:ablation_buffer}
\begin{tabular}{c c c c c c c } 
\toprule 
Dataset (Small)     & $K$ = 256 & $K$ = 512 & $K$ = 1024 & $K$ = 4096 & $K$ = 8192 & $K$ = 16384  \\
CIFAR-10    & 89.37   & 89.53   & 89.83    & 90.04    & 90.15    & 90.35 \\ 
CIFAR-100   & 61.17   & 62.47   & 63.20    & 63.79    & 63.84    & 64.06 \\ \hline
Dataset (Medium)    & $K$ = 256 & $K$ = 1024 & $K$ = 4096 & $K$ = 8192 & $K$ = 16384 & $K$ = 32768 \\
STL-10          & 85.88   & 87.23    & 87.72 &  87.42   & 87.32     & 87.47 \\
Tiny ImageNet   & 43.08   & 45.32    & 45.78 &  45.42   & 46.60     & 46.48\\
\toprule 
\vspace{-25pt}
\end{tabular}
\end{table}



\subsection{Visualization of Learned Representations}
We also show the t-SNE \cite{tsne} visualizations of the representations learned by our proposed method and MoCov2 on the training set of CIFAR-10. Our proposed relational consistency loss leads to better class separation than the contrastive loss.

\begin{figure*}[h]
    \vspace{-10pt}
    \centering
    \subfigure[MoCoV2]{\label{fig:moco_tsne}\includegraphics[width=0.4\linewidth]{Assets/moco-cifar10.png}}
    \subfigure[Ours]{\label{fig:ours_tsne}\includegraphics[width=0.4\linewidth]{Assets/weak-cifar10.png}}
    \caption{t-SNE visualizations on CIFAR-10. Classes are indicated by colors.}
    \vspace{-10pt}
    \label{fig:tsne}
\end{figure*}


 \begin{table}[b]
 \vspace{-10pt}
 \centering
 \small
 \caption{Top-1 accuracy under the linear evaluation on ImageNet with the ResNet-50 backbone. The table compares the methods over 200 epochs of pretraining.}
 \label{table:200epoch}
 \vspace{-5pt}
\begin{tabular}{l  c c c c c c c} 
\toprule 
Method & Arch & Backprop & EMA & Batch Size & Param & Epochs & Top-1 \\
Supervised & R50 & 1x & No & 256 & 24 & 120 & 76.5 \\ \hline
\emph{1x Backprop Methods } \\
InstDisc \cite{instance_discrimination} & R50 & 1x & No  & 256 & 24 & 200 & 58.5 \\
LocalAgg \cite{local}                   & R50 & 1x & No  & 128 & 24 & 200 & 58.8 \\
MoCo v2 \cite{mocov2}                   & R50 & 1x & Yes & 256 & 24 & 200 & 67.5 \\
MoCHi \cite{mochi}                      & R50 & 1x & Yes & 512 & 24 & 200 & 68.0 \\
CPC v2 \cite{cpc}                       & R50 & 1x & No  & 512 & 24 & 200 & 63.8 \\
PCL v2 \cite{PCL}                       & R50 & 1x & Yes & 256 & 24 & 200 & 67.6 \\
AdCo \cite{adco}                        & R50 & 1x & Yes & 256 & 24 & 200 & 68.6 \\
ReSSL(Ours)                             & R50 & 1x & Yes & 256 & 24 & 200 & \textbf{69.9} \\
\hline
\emph{2x Backprop Methods } \\
CLSA-Single \cite{stronger}             & R50 & 2x & Yes & 256  & 24 & 200 & 69.4 \\
SimCLR \cite{simclr}                    & R50 & 2x & No  & 4096 & 24 & 200 & 66.8 \\
SwAV \cite{swav}                        & R50 & 2x & No  & 4096 & 24 & 200 & 69.1 \\
SimSiam \cite{byol} & R50 & 2x & No  & 256  & 24 & 200 & 70.0 \\
BYOL \cite{byol}    & R50 & 2x & Yes & 4096 & 24 & 200 & 70.6 \\
\hline
\toprule
\end{tabular}
\end{table}






\section{Results on Large-scale Datasets}
We also performed our algorithm on the large-scale ImageNet-1k dataset \cite{imagenet_cvpr09}. In the experiments, we adopt a learning rate of $0.05 * BatchSize / 256$, a memory buffer size of 130k, and a 2-layer non-linear projection head with a hidden dimension 4096 and output dimension 512. For $\tau_t$ and $\tau_s$, we simply adopt the best setting from Table \ref{table:ablation_t} where $\tau_t = 0.04$ and $\tau_s = 0.1$.

\textbf{Linear Evaluation}.
For the linear evaluation of ImageNet-1k, we strictly follow the setting in SwAV \cite{swav}. The result is shown in table \ref{table:200epoch}. As we can see clearly that in \emph{1x} backprop methods, our ReSSL is significantly better than all previous algorithms. Comparing to the \emph{2x} backprop methods, our ReSSL can outperform CLSA, SimCLR, and SwAV with much lesser training costs.

\textbf{Working with Multi-Crop Strategy}.
We also performed ReSSL with Multi-Crop strategy. The result is shown below in Table \ref{table:multi_crop}. Specifically, the result of 4 crops is trained with the resolution of $224 \times 224, 160 \times 160, 128 \times 128, 96 \times 96 $. For the result of 5 crops, we add an additional $192 \times 192$ image which is exactly the same with AdCo \cite{adco}. As we can see our proposed ReSSL is significantly better than previous state-of-the-art methods.

\begin{table}[h]
 \centering
 \small
 \caption{Working with Multi-Crop Strategy (Linear Evaluation on ImageNet)}
 \label{table:multi_crop}
 \vspace{-5pt}
\begin{tabular}{l  c c c c c c c} 
\toprule 
Method & Arch  & EMA & Batch Size & Param & Epochs & Top-1 \\
\hline
SwAV \cite{swav}                   & R50 & No & 256   & 24 & 200 & 72.7 \\
AdCo \cite{adco}                   & R50 & No & 256   & 24 & 200 & 73.2 \\
CLSA-Multi \cite{stronger}               & R50 & Yes & 256  & 24 & 200 & 73.3 \\
ReSSL (4 crops)                    & R50 & Yes & 256  & 24 & 200 & \textbf{73.8} \\
ReSSL (5 crops)                    & R50 & Yes & 256  & 24 & 200 & \textbf{74.7} \\
\toprule
\end{tabular}
\end{table}


\textbf{Working with Smaller Architecture}.
We also applied our proposed method on the smaller architecture (ResNet-18). The result is shown in Table \ref{table:res18}. Following the same training recipe of the ResNet-50 in above, our proposed method has a higher performance than SEED \cite{seed} without a larger pretrained teacher network.

\renewcommand\arraystretch{1.12}
\begin{table}[h]
 \centering
 \setlength\tabcolsep{10pt}
 \small
 \caption{Experiments on ResNet-18 (Linear Evaluation on ImageNet)}
 \vspace{-5pt}
 \label{table:res18}
\begin{tabular}{l c c c c }
\toprule 
Method & Epochs & Student & Teacher & Acc  \\ \hline
MoCo v2 & 200 & ResNet-18 & EMA & 52.2 \\
SEED & 200 & ResNet-18 & ResNet-50 (MoCoV2) & 57.6 \\
ReSSL & 200 & ResNet-18 & EMA &  \textbf{58.1} \\
\toprule 
\end{tabular}
\end{table}




\textbf{Training Cost}. 
Fair comparison should be performed under the same training cost. In table \ref{table:training_cost}, we show the training speed and linear evaluation accuracy on ImageNet.  We test all the methods on V100 GPU with 32G memory. It is clear to see that our ReSSL is just slightly slower (3\%) than MoCo v2, but we get 2.4\% improvements. On the other hand, the training speed of ReSSL (4 crops) is on pair with \emph{2x} backprop method (\ie SimCLR and BYOL), but our performance is significantly better than all state-of-the-art methods.


\begin{table}[h]
 \centering
 \small
 \caption{Training Cost Comparison, the result is reported in averaged GPU hours}
 \vspace{-5pt}
 \label{table:training_cost}
\begin{tabular}{l  c  c c c c c} 
\toprule 
Method & Epochs & Batch Size &  GPU & GPU Memory & (GPU·Time)/Epoch & Acc \\ \hline
MoCo v2       & 200  & 256   & 8 x V100  & 40 G & 2.25 & 67.5   \\
ReSSL (Ours)  & 200  & 256   & 8 x V100  & 42 G & 2.33 & \textbf{69.9}   \\ \hline
SimCLR        & 200  & 4096  & 32 x V100 & 858 G & 3.55 & 66.8  \\ 
BYOL          & 200  & 4096  & 32 x V100 & 863 G & 3.88 & 70.6   \\
ReSSL  (4 Crops) & 200  & 256  & 8 x V100 & 80 G & 3.62 & \textbf{73.8}    \\
\toprule 
\end{tabular}
\end{table}




\vspace{-5pt}
\textbf{Transfer Learning}. 
Finally, we further evaluate the quality of the learned representations by transferring them to other datasets. Following \cite{PCL}, we perform linear classification on the PASCAL VOC2007 dataset \cite{pascal-voc-2007}. Specifically, we resize all images to 256 pixels along the shorter side and taking a 224 × 224 center crop. Then, we train a linear SVM on top of corresponding global average pooled final representations. To study the transferability of the representations in few-shot scenarios, we vary the number of labeled examples $K$ and report the mAP. Table \ref{table:low-shot} shows the comparison between our method with previous works. We report the average performance over 5 runs (except for k=full).It's clearly to see that our proposed method is consistently outperform MoCo v2 and PCL v2 across all different $K$. 
\vspace{-5pt}

\renewcommand\arraystretch{1.15}
\begin{table*}[h]
 \centering
 \setlength\tabcolsep{7pt}
 \vspace{-8pt}
 \small
 \caption{Transfer learning on low-shot image classification}
 \label{table:low-shot}
\begin{tabular}{l  c c   c  c  c  c  c  c  c} 
\toprule 
Method & Epochs & ImageNet & $K$=16 & $K$=32 & $K$=64 & Full \\
Random & -      & -        & 10.10  & 11.34 & 11.96 & 12.42  \\
Supervised & 90 & 76.1     & 82.26  & 84.00 & 85.13 & 87.27  \\ \hline
MoCo V2 \cite{mocov2} & 200 &  67.5 & 76.14 & 79.16 & 81.52 & 84.60   \\
PCL V2 \cite{PCL} & 200 &  67.5  & 78.34 & 80.72 & 82.67 & 85.43 \\
ReSSL (Ours)     & 200 & 69.9 & \textbf{79.17} & \textbf{81.96} & \textbf{83.81} & \textbf{86.31} \\
\toprule 
\end{tabular}
\vspace{-15pt}
\end{table*}



 \section{Conclusion}

In this work, we propose relational self-supervised learning (ReSSL), a new paradigm for unsupervised visual representation learning framework that maintains the relational consistency between instances under different augmentations. Our proposed ReSSL relaxes the typical constraints in contrastive learning where different instances do not always need to be pushed away on the embedding space, and the augmented views do not need to share exactly the same feature. An extensive empirical study shows the effect of each component in our framework. The experiments on large-scaled datasets demonstrate the efficiency and state-of-the-art performance for unsupervised representation learning. 
{\small
\bibliographystyle{splncs04}
\bibliography{egbib}
}





\newpage
\appendix
\section{Implementation Details on Small and Medium Dataset}
We adopt the same backbone and data augmentation for all methods as we already described in Section 4. For SimCLR and BYOL, we use the LARS optimizer with a momentum of 0.9 and weight decay of $1e-4$; the learning rate will be linearly warmed up for 5 epochs until it reaches $1.0 \times BatchSize/256$. For linear evaluation, we use a standard SGD optimizer with a momentum of 0.9, weight decay of 0, and a learning rate of $0.2 \times BatchSize / 256$; the learning rate will be cosine decayed for 100 epochs. For SimSiam, the optimizer, learning rate, weight decay, and the linear evaluation details are the same as our MoCo and ReSSL implementation (as in Section 4).



\section{More Experiments on Temperature}
In this section, we add more experiments for different $\tau_t$ (an extension for Table \ref{table:ablation_t}). As we can see, when $\tau_t \rightarrow \tau_s$, the model is simply collapsed, which further verified that $\tau_t$ has to be properly sharpened. \emph{Note, as we have mentioned in Table \ref{table:ablation_t}, the optimal value for $\tau_t$ is 0.04$\sim$0.05.}  

\renewcommand\arraystretch{1.12}
\begin{table}[h]
 \centering
 \setlength\tabcolsep{10pt}
 \small
 \caption{More experiments for different $\tau_t$ (Top-1 accuracy on small and medium dataset)}
 \vspace{-5pt}
 \label{table:tau_t}
\begin{tabular}{c c c c c c} 
\toprule 
$\tau_s$ & $\tau_t$ & CIFAR-10 & CIFAR-100 & STL-10 & Tiny ImageNet \\ \hline
 0.1  & 0.08  & 10.00 & 1.00 & 83.05 & 39.38 \\
 0.1  & 0.09  & 10.00 & 1.00 & 10.00 & 0.50 \\
 0.1  & 0.10  & 10.00 & 1.00 & 10.00 & 0.50 \\
\toprule 
\end{tabular}
\end{table}


\section{More Experiments on Weak Augmentation}
Since the weak augmentation for the teacher model is one of the crucial points in ReSSL, we further analyze the effect of applying different augmentations on the teacher model. In this experiment, we simply set $\tau_t = 0.04$ and report the linear evaluation performance on the Tiny ImageNet dataset.  The results are shown in Table \ref{table:augmentation}. The first row is the baseline, where we simply resize all images to the same resolution (no extra augmentation is applied). Then, we applied random resized crops, random flip, color jitter, grayscale, gaussian blur, and various combinations. We empirically find that if we use no augmentation (\eg, no random resized crops) for the teacher model, the performance tends to degrade. This might result from that the gap of features between two views is way too smaller, which undermines the learning of representations. However, too strong augmentations of teacher model will introduce too much noise and make the target distribution inaccurate (see Figure \ref{fig:nn}). Thus mildly weak augmentations are better option for the teacher, and random resized crops with random flip is the combination with the highest performance as Table \ref{table:augmentation} shows.




\renewcommand\arraystretch{1.12}
\begin{table}[h]
 \centering
 \setlength\tabcolsep{10pt}
 \small
 \caption{Effect of different augmentation for teacher model (Tiny ImageNet)}
 \vspace{-5pt}
 \label{table:augmentation}
\begin{tabular}{c c c c c c} 
\toprule 
Random Resized Crops & Random Flip & Color Jitter & GrayScale & Gaussian Blur & Acc \\ \hline
 &   &  &  &  & 31.74 \\ \hline 
 \checkmark   &   &  &  &  & 46.00 \\ 
 & \checkmark &   &  &  & 30.98 \\
 & & \checkmark &  &  & 29.46 \\
 & & & \checkmark &   & 29.68 \\
 & & & & \checkmark & 30.10 \\ \hline
 \checkmark   & \checkmark  &  &  &  & \textbf{46.60} \\ \hline
 \checkmark   &   & \checkmark  &  &  & 44.44 \\
 \checkmark   &   &  & \checkmark &  & 42.28 \\
 \checkmark   &   &  &  & \checkmark & 44.88 \\
 \checkmark   & \checkmark  & \checkmark  &  &  & 43.70 \\
 \checkmark   & \checkmark  &  & \checkmark &  & 42.28 \\
 \checkmark   & \checkmark  &  &  & \checkmark & 44.52 \\
\toprule 
\end{tabular}
\end{table}











 



\end{document}
