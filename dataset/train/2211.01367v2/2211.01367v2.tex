\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}









\usepackage[final]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{CJKutf8}
\usepackage{comment}
\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\cf{\emph{cf.}} \def\Cf{\emph{Cf.}}
\def\etc{\emph{etc.}} \def\vs{\emph{vs.}}
\def\wrt{w.r.t.} \def\dof{d.o.f.}
\def\iid{i.i.d.} \def\wolog{w.l.o.g.}
\def\etal{\emph{et al.}}
\def \mbf{\mathbf}
\def \mbb{\mathbb}
\def \tbf{\textbf}
\newcommand{\sub}[1]{\textcolor{red}{#1}}
\newcommand{\del}[1]{\textcolor{blue}{#1}}
\newcommand{\ins}[1]{\textcolor{green}{#1}}

\usepackage{mathtools}
\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{#1\;\delimsize\|\;#2}
\newcommand{\infdiv}{D\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}


\title{Two-Stream Network for Sign Language Recognition and Translation}





\author{Yutong Chen\textsuperscript{\rm 1}\thanks{Equal contribution.} \quad Ronglai Zuo\textsuperscript{\rm 2}\footnotemark[1] \quad Fangyun Wei\textsuperscript{\rm 1}\footnotemark[1]~~\thanks{Corresponding author.}\quad Yu Wu\textsuperscript{\rm 1}\quad Shujie Liu\textsuperscript{\rm 1}\quad Brian Mak\textsuperscript{\rm 2}\\
  \textsuperscript{\rm 1}Microsoft Research Asia\quad \textsuperscript{\rm 2}The Hong Kong University of Science and Technology\\
  \texttt{chenytjudy@gmail.com}\quad \texttt{\{rzuo,mak\}@cse.ust.hk} \\
  \texttt{\{fawe,yuwu1,shujliu\}@microsoft.com} \\
}


\begin{document}


\maketitle


\begin{abstract}
Sign languages are visual languages using manual articulations and non-manual elements to convey information. For sign language recognition and translation, the majority of existing approaches directly encode RGB videos into hidden representations. RGB videos, however, are raw signals with substantial visual redundancy, leading the encoder to overlook the key information for sign language understanding. To mitigate this problem and better incorporate domain knowledge, such as handshape and body movement, we introduce a dual visual encoder containing two separate streams to model both the raw videos and the keypoint sequences generated by an off-the-shelf keypoint estimator. To make the two streams interact with each other, we explore a variety of techniques, including bidirectional lateral connection, sign pyramid network with auxiliary supervision, and frame-level self-distillation. The resulting model is called TwoStream-SLR, which is competent for sign language recognition (SLR). TwoStream-SLR is extended to a sign language translation (SLT) model, TwoStream-SLT, by simply attaching an extra translation network. Experimentally, our TwoStream-SLR and TwoStream-SLT achieve state-of-the-art performance on SLR and SLT tasks across a series of datasets including Phoenix-2014, Phoenix-2014T, and CSL-Daily. Code and models are available at: \url{https://github.com/FangyunWei/SLRT}.

\end{abstract}
 \section{Introduction}
Sign languages, which are the primary means of communication among the deaf and hard-of-hearing people, have been studied for a long time ~\cite{tamura1988recognition,starner1998real,bungeroth2004statistical}. In linguistic terms, sign languages are as rich and complex as any spoken language~\cite{armstrong1995gesture} and their word-order typology may differ from the spoken languages.
Moreover, there are limited parallel data for sign-to-text, and the gap between the visual modality and the language modality poses another challenge to developing a well-performing sign language translation (SLT) system~\cite{camgoz2018neural}.
To mitigate the problem, SLT often requires an intermediate representation between an input visual signal and the output text, which is a sequence of glosses\footnote[1]{Glosses are the word-for-word transcription of sign language where each gloss is a unique label for a sign.}, to achieve satisfactory translation results. The process of generating gloss sequences from given sign videos is termed as sign language recognition (SLR)~\cite{Cooper2011}, which does not have the word ordering problem in SLT. Figure~\ref{fig:teaser_SLRT} illustrates the relationship between SLR~\cite{P2014, STMC_MM, niu2020stochastic, fcn_eccv2020, CrossAug_MM2020, dnf_cui} and SLT~\cite{camgoz2020sign, camgoz2018neural, li2020tspnet, zhou2021improving, MMTLB_2022, STMC_MM}. In this paper, we work on both tasks.


The key to tackle SLR and SLT is to build a visual encoder to embed visual signals into hidden representations. 
Inspired by the promising progress of action recognition~\cite{ji20123d,tran2015learning,carreira2017quo,xie2018rethinking,liu2021video,arnab2021vivit}, a majority of works~\cite{Min_2021_ICCV,Hao_2021_ICCV,CrossAug_MM2020,fcn_eccv2020,niu2020stochastic,subunet_iccv2017} explore to directly model RGB videos to understand sign languages. However, a major problem with this kind of models is their robustness, and the models suffer from dramatic performance degradation when the background or signer is mismatched between training and testing. To alleviate the problem, we consider injecting proper domain knowledge (characteristic of sign languages) into learning.
Sign languages use two types of visual signals to convey information~\cite{koller2020quantitative}: manual elements that include handshape, palm orientation, etc., and non-manual markers such as facial expressions and movement of the body, head, mouth, eyes, and eyebrows. In this paper, we advocate involving keypoints of the face, hands, and upper body in the learning, and introduce a novel two-stream network named TwoStream-SLR to model both RGB videos and keypoint sequences for sign language recognition (SLR). The proposed TwoStream-SLR can be extended to handle sign language translation (SLT) by attaching an extra translation network. The resulting model is termed as TwoStream-SLT. An overview of TwoStream-SLR and TwoStream-SLT is shown in Figure~\ref{fig:teaser_overview}. Our contributions mainly lie in the proposed TwoStream-SLR; we summarize our design principles and key components of TwoStream-SLR as follows:


\begin{figure}[t]
     \begin{subfigure}{.44\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/teaser_SLRT.pdf}
         \caption{Illustration of SLR and SLT.}
         \label{fig:teaser_SLRT}
     \end{subfigure}
\hfill
\begin{subfigure}{.52\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/teaser_overview.pdf}
  \caption{Overview of TwoStream-SLR and TwoStream-SLT.}
  \label{fig:teaser_overview}
\end{subfigure}
\caption{(a) We select a sign video from the Phoenix-2014T~\cite{camgoz2018neural} dataset and visualize its gloss sequence and the corresponding text. Sign language recognition (SLR) seeks to train a model to recognize a gloss sequence from a sign video with weak sentence-level gloss annotations (\ie, gloss temporal boundaries are unknown). In contrast, sign language translation (SLT) aims to directly generate spoken languages (texts) with or without intermediate representations (glosses). (b) TwoStream-SLT is built upon TwoStream-SLR to enable SLT. We use HRNet~\cite{wang2020deep} trained on COCO-WholeBody~\cite{jin2020whole} to extract keypoints of the face, hands, and upper body. Keypoints are represented by heatmaps.}
\label{fig:fig}
\end{figure}
\begin{enumerate}[leftmargin=0.5cm]
    \item \textbf{Dual Visual Encoder.} We use two separate S3D~\cite{xie2018rethinking} backbones with lightweight head networks to encode RGB videos and keypoint sequences which are represented by a set of heatmaps. Since most sign language datasets do not provide keypoint annotations, we use an off-the-shelf keypoint estimator, HRNet~\cite{wang2020deep} which is trained on COCO-WholeBody~\cite{jin2020whole}, to generate pseudo keypoints of face, hands, and upper body for each frame. Both streams are supervised by the well-known connectionist temporal classification (CTC) loss~\cite{graves2006connectionist}. In contrast to existing methods which either utilize keypoints to crop concerned areas on feature maps~\cite{STMC_MM}/original videos~\cite{papadimitriou20_interspeech}, or provide supervision for multi-task learning~\cite{STMC_MM,Albanie2020bsl1k}, we directly apply a convolutional neural network to model keypoint sequences to avoid ad-hoc design~\cite{NSLT_keypoint, mseqgraph}. Our dual encoder architecture is different from two-stream networks for action recognition which encode either image (video)/optical flow~\cite{simonyan2014two,carreira2017quo}, or two videos of different frame rates~\cite{feichtenhofer2019slowfast}. We fuse different streams at late stage by averaging their predicted frame-wise gloss probabilities before feeding them to a CTC decoder to produce the final gloss sequences.  
\item \textbf{Information Interaction via Bidirectional Lateral Connection.} Constrained by the photographic apparatus, motion blur heavily exists in sign videos~\cite{P2014, camgoz2018neural}, resulting in inaccurate keypoint estimation. This motivates us to use the information extracted by the video stream to alleviate the negative impacts caused by inferior keypoints when modeling keypoint sequences. On the other hand, heavy redundancy and irrelevant information (\eg,~background and appearance of signer) in sign videos may lead the model to overlook the salient information. Thus, we propose to integrate the features extracted by the keypoint stream into the video stream as supplementary information. Thanks to the dual encoder design, intermediate representations of video and keypoint sequence streams could be easily integrated via a bidirectional lateral connection module~\cite{duan2021revisiting} for information exchange. \item\textbf{Alleviate Data Scarcity via Sign Pyramid Network and Auxiliary Loss.} Both SLR and SLT greatly suffer from data scarcity. For example, there are only around  parallel training samples in the widely used Phoenix-2014T~\cite{camgoz2018neural} dataset. In contrast, training an effective neural machine translation model usually requires a corpus of  parallel data~\cite{sennrich2019revisiting}. 
    Besides, similar to actions with different visual tempos in the action recognition field \cite{yang2020temporal}, glosses also have different temporal spans \cite{saunders2020progressive}. Thus, we present a sign pyramid network (SPN), which is inspired by the feature pyramid network~\cite{lin2017feature} in object detection and the temporal pyramid network~\cite{yang2020temporal} in action recognition, on top of the dual encoder to better capture glosses of various temporal spans in the low-data regime. The fused features yielded by SPN are further supervised by auxiliary CTC losses, which enable the shallow layers to learn meaningful features.
    
    
\item\textbf{Frame-Level Self-Distillation.}
     We treat the averaged ensemble predictions as pseudo-targets and propose an additional self-distillation loss calculated by the KL-divergence between each stream's predictions and pseudo-targets at the frame level. The self-distillation feeds rich knowledge in the late ensemble predictions back into each stream's learning. Moreover, compared with the CTC loss which only provides sentence-level supervision without temporal boundary information, the self-distillation loss is computed per frame to facilitate training with extra pseudo frame-level supervision.
\end{enumerate}
To enable sign language translation, we append a translation network to the TwoStream-SLR, yielding our translation model named TwoStream-SLT. The proposed TwoStream-SLR and TwoStream-SLT achieve state-of-the-art performance on both SLR and SLT across a series of benchmarks including Phoenix-2014~\cite{P2014}, Phoenix-2014T~\cite{camgoz2018neural}, and the recently published CSL-Daily~\cite{zhou2021improving}. 
 \section{Related Work}
\textbf{Sign Language Recognition and Translation.}
Sign language recognition (SLR) aims to transcribe an input sign video into a gloss sequence. An SLR model usually consists of two components, a video encoder that extracts frame-level features from an input video, and a decoder (or head network) that outputs gloss sequences from the extracted features.
The video encoder is usually based on CNNs including 3D-CNNs \cite{MMTLB_2022, Pu2019Iterative, li2020tspnet}, 2D-CNNs \cite{subunet_iccv2017, niu2020stochastic}, and 2D+1D CNNs \cite{STMC_MM, dnf_cui}, and both single-stream \cite{MMTLB_2022, Pu2019Iterative, li2020tspnet} and multi-stream architectures \cite{STMC_MM, dnf_cui, papadimitriou20_interspeech} have been adopted.
In this work, we utilize two separate S3D \cite{xie2018rethinking} backbones to model both RGB videos and keypoint sequences. In the design of gloss decoder, all recent works use either HMM \cite{koller2017re-sign, deep-sign, koller2019weak} or CTC \cite{fcn_eccv2020, STMC_MM, Min_2021_ICCV} following their success in automatic speech recognition. We adopt CTC due to its simplicity.
Since CTC loss just provides weak sentence-level supervision, 
\cite{dnf_cui, zhou2019dynamic} propose to iteratively generate fine-grained pseudo labels from CTC outputs to introduce stronger frame-level supervision, while \cite{Min_2021_ICCV} achieves frame-level knowledge distillation between the entire model and the visual encoder.
In this work, as a byproduct of the two-stream architecture, our frame-level distillation imparts the final ensemble knowledge into each individual stream and enhances the interaction and consistency between the two streams.
Sign language translation (SLT) directly predicts texts given sign videos. Most existing approaches formulate this task as a neural machine translation (NMT) problem by employing a visual encoder as tokenizer to extract visual features and forwarding them to a translation network for spoken text generation~\cite{MMTLB_2022,camgoz2020sign,li2020tspnet,zhou2021improving,Yin2020STMCTransf,camgoz2018neural}. We use mBART~\cite{liu2020multilingual} as our translation network due to its excellent SLT performance~\cite{MMTLB_2022}. To achieve satisfactory results, gloss supervision is commonly 
utilized in SLT by pretraining the visual encoder on SLR~\cite{camgoz2020sign,STMC_MM,zhou2021improving} and jointly training SLR and SLT~\cite{STMC_MM,camgoz2020sign}.



\textbf{Introduce Keypoints into SLR and SLT.}
How to leverage keypoints to boost the performance of SLR and SLT is still an open problem.
\cite{STMC_MM} and \cite{papadimitriou20_interspeech} utilize estimated keypoint coordinates to crop feature maps and RGB videos to process each cue (hands, face, and body) independently.
Some other works \cite{NSLT_keypoint, Camg2020Multichannel, mseqgraph} model keypoints from coordinates.
For example, the SLT system in \cite{NSLT_keypoint} feeds keypoint coordinates into an MLP followed by recurrent neural networks. \cite{mseqgraph} represents keypoints as skeleton graphs, which are further modeled by graph convolutional networks (GCNs).
However, all of the existing works treat keypoints as a set of coordinates, which is so sensitive to noise that a small perturbation may lead to wrong predictions~\cite{duan2021revisiting}.
Besides, extra efforts need to be paid to devise dedicated modules, \eg,~GCN.
In this work, we represent keypoints as heatmaps, which are robust to noise and can simply share the same architecture with the video stream.

\textbf{Multi-Stream Networks.} Multi-Stream networks are widely explored in the action recognition field~\cite{simonyan2014two, feichtenhofer2016convolutional, carreira2017quo, zolfaghari2017chained, feichtenhofer2019slowfast}. For example, I3D \cite{carreira2017quo} builds a two-stream 3D-CNN architecture and takes RGB video and optical flow as inputs. SlowFast~\cite{feichtenhofer2019slowfast} encodes videos with different frame rates. As for SLR and SLT, DNF \cite{dnf_cui} follows the idea of I3D to fuse information of RGB videos and optical flow, while STMC \cite{STMC_MM} models the multi-cue property of sign languages with a multi-stream architecture that takes cropped feature maps as inputs. In this work, our approach directly models RGB videos and keypoint heatmaps via a dual encoder. Besides, to alleviate the data scarcity issue and better capture glosses of various temporal spans, we present a sign pyramid network equipped with auxiliary supervision to drive the shallow layers to learn meaningful representations. 
How to model the interactions between different streams is non-trivial.
I3D \cite{carreira2017quo} uses a late fusion strategy by simply averaging the predictions of two streams.
Another way is to fuse the intermediate features of each stream in early stage by lateral connections~\cite{feichtenhofer2019slowfast}, concatenation~\cite{STMC_MM}, or addition~\cite{dnf_cui}.
In this work, we extend lateral connections in \cite{feichtenhofer2019slowfast} to bidirectional ones to make two streams complement each other.
In addition, our self-distillation integrates the knowledge of both streams to generate pseudo-targets, which can also be regarded as a kind of interaction. 
 \section{Method}
In this section, we introduce our TwoStream-SLR and TwoStream-SLT for the SLR and SLT tasks, respectively. Given a sign video  with  frames, our goal is to optimize TwoStream-SLR that can predict the gloss sequence  with  glosses (\ie, SLR). Appending a translation network onto TwoStream-SLR yields TwoStream-SLT, a model which is capable of predicting associated spoken language sentence  with  words (\ie, SLT) from sign videos. In general, . The proposed TwoStream-SLR and TwoStream-SLT are introduced in Section~\ref{sec:twostream-slr} and Section~\ref{sec:twostream-slt}, respectively.



\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/overview_SLR.pdf}
\caption{Overview of TwoStream-SLR, which is composed of: 1) a video encoder; 2) a keypoint sequence encoder; 3) a joint head; 4) a bidirectional lateral connection module; 5) two sign pyramid networks. The whole network is jointly supervised by the CTC losses and the frame-level self-distillation losses. Keypoint sequences are represented by heatmaps.}
\label{fig:overview_SLR}
\end{figure}

\subsection{TwoStream-SLR}
\label{sec:twostream-slr}
Figure~\ref{fig:overview_SLR} shows the overview of our TwoStream-SLR, which consists of five parts: 1) a video encoder; 2) a keypoint sequence encoder; 3) a joint head; 4) a bidirectional lateral connection module; 5) two sign pyramid networks with auxiliary supervision, to model RGB videos and keypoint sequences. 
\textbf{Video Encoder.} We use S3D~\cite{xie2018rethinking} with a lightweight head network as our video encoder. Only the first four blocks of S3D are used since our goal is to extract dense representations along the temporal dimension. We feed each  video into the encoder to extract its features, where  denotes the frame number,  and  represent the height and width of the sign video. We set  and  as 224 by default. The output feature of the last S3D block is spatially pooled into the size of  before it is fed into the head network. The goal of the head network is to further capture the temporal context. It consists of a temporal linear layer, a batch normalization layer, a ReLU layer, as well as a temporal convolutional block which contains two temporal convolutional layers with a temporal kernel size of 3 and a stride of 1, a linear translation layer, and a ReLU layer. The output feature which is named as gloss representation has a size of . Then a linear classifier and a Softmax function are applied to extract frame-level gloss probabilities. At last, we use connectionist temporal classification (CTC) loss  to optimize the video encoder.

\textbf{Keypoint Encoder.} To model keypoint sequences, we first utilize the HRNet~\cite{wang2020deep} which is trained on COCO-WholeBody~\cite{jin2020whole} to generate 42 hand keypoints, 68 face keypoints covering the mouth, eyes, and face contour, and 11 upper body keypoints covering shoulders, elbows, and wrists per frame. We empirically found that using only a subset of 26 face keypoints (10 mouth keypoints and 16 for other parts) performs well while saving computational resources. In total, 79 keypoints are used. We use heatmaps to represent the keypoints. 
Concretely, denoting the keypoint heatmap sequence with a size of  as , where  and  represent the spatial resolution of each heatmap, and  is the total number of keypoints, the elements of  are generated by a Gaussian function: , where  
denotes the coordinates of the -th keypoint in the -th frame, and  controls the scale of keypoints. We set  by default and  to reduce computational cost. The network architecture of the keypoint encoder is the same as the video encoder, except for the first convolutional layer which is substituted to fit the keypoint input. Note that weights of the video encoder and keypoint encoder are not shared. Similarly, a CTC loss is utilized to train the keypoint encoder, which is denoted as .

\textbf{Bidirectional Lateral Connection.}
To fuse the information of two streams, we propose lateral connection, which is explored in action recognition~\cite{feichtenhofer2019slowfast,christoph2016spatiotemporal,feichtenhofer2016convolutional,duan2021revisiting} and object detection~\cite{lin2017feature}. The lateral connection is implemented as an element-wise add operation between two feature maps of the same resolution. We apply lateral connection on the features (, , and ) generated by the first three blocks (, , and ) of the two S3D backbones. Since the spatial resolutions of intermediate features extracted by the two streams are different, we use spatially strided convolution and transposed convolution to align their spatial resolutions. For implementation, the lateral connection is bidirectional, and other variants such as unidirectional lateral connection are studied in Section~\ref{sec:ablation}.


\textbf{Joint Head and Late Ensemble.}
Both the video encoder and keypoint encoder have their own head networks. To fully explore the potential of our dual encoder architecture, we present an additional head network named joint head, which takes the concatenation of outputs of the two S3D networks as inputs. The architecture of the joint head is the same as the video head and keypoint head. The joint head is supervised by a CTC loss as well, which is denoted as . We average the frame-wise gloss probabilities predicted by the video head, keypoint head, and joint head and feed them to a CTC decoder to generate the gloss sequence . This late ensemble strategy fuses multi-stream results and improves over single-stream predictions as shown in our experiments.

\begin{figure}[t]
     \begin{subfigure}{.615\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/SPN.pdf}
         \caption{Architecture of sign pyramid network (SPN).}
         \label{fig:SPN}
     \end{subfigure}
\hfill
\begin{subfigure}{.365\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/overview_SLRT.pdf}
  \caption{Overview of TwoStream-SLT.}
  \label{fig:TSN-SLRT}
\end{subfigure}
\caption{(a) The sign pyramid network takes the features (, , and ) of the last three blocks of S3D backbone as inputs and generate the fused features  and . The construction of the sign pyramid involves a top-down pathway and a lateral connection where the transposed convolution is used to match both temporal and spatial dimensions of two feature maps. Two separate head networks of the same architecture are applied on  and  to generate frame-level gloss probabilities. We use the CTC loss as auxiliary supervision. Each stream in our dual encoder has an independent SPN. (b) Simply appending a translation network onto our TwoStream-SLR yields TwoStream-SLT, a framework for SLT. TwoStream-SLT is jointly supervised by the recognition loss and the translation loss.}
\label{fig:spn_tsn_slrt}
\end{figure}

\textbf{Sign Pyramid Network.}
To better capture glosses of different temporal spans and efficiently supervise the shallow layers to learn meaningful representations, we build a sign pyramid network (SPN) with auxiliary supervision upon the dual visual encoder. The architecture of SPN is illustrated in Figure~\ref{fig:SPN}. Specifically, we denote outputs of the last three blocks of the S3D backbone as , , and , respectively. Similar to~\cite{yang2020temporal}, the construction of the sign pyramid involves a top-down pathway and a lateral connection. We use element-wise add operation to fuse features extracted by adjacent S3D blocks, and the fused features are termed as  and  (see Figure~\ref{fig:SPN}). We use transposed convolution to match both temporal and spatial dimensions of two feature maps before element-wise addition. Then two separate head networks with the same architecture as the one used in the dual encoder are applied on  and  to extract frame-level gloss probabilities. Similarly, CTC losses are adopted to provide auxiliary supervision. Without loss of generality, we use two independent SPNs for the video and keypoint stream. The auxiliary CTC losses of two streams are denoted as  and , respectively. SPN is dropped in the inference stage.

\textbf{Frame-Level Self-Distillation.}
Existing datasets only provide sentence-level gloss annotations, where gloss temporal boundaries are not labeled. Thus, CTC~\cite{graves2006connectionist} loss is widely adopted to leverage this kind of weak supervision. However, once well optimized, the visual encoder is able to generate frame-wise gloss probabilities from which one can estimate the approximate temporal boundaries of glosses. This inspires us to use the predicted frame-wise gloss probabilities as pseudo-targets to provide extra fine-grained supervision in addition to the coarse-grained CTC loss. In accordance with our two-stream design, we use the averaged gloss probabilities from the three head networks as pseudo-targets to guide the learning of each stream. Formally, we minimize the KL-divergence between the pseudo-targets and the predictions of the three head networks. We call it frame-level self-distillation loss  as it not only provides frame-level supervision but also distills knowledge in the late ensemble back into each individual stream.



\textbf{Loss Function.}
The overall loss of TwoStream-SLR is composed of three parts: 1) the CTC losses applied on the outputs of the video encoder (), keypoint encoder (), and joint head (); 2) the auxiliary CTC losses ( and ) applied on the outputs of two sign pyramid networks; 3) the distillation loss (). We formulate the recognition loss as follows:

where  and  denote loss weights of the auxiliary CTC loss of the video stream and keypoint stream. Up to now, we have introduced all components of TwoStream-SLR. Once the training is finished, TwoStream-SLR is capable of predicting a gloss sequence by averaging predictions from the three head networks.

\subsection{TwoStream-SLT}
\label{sec:twostream-slt}
Previous approaches~\cite{camgoz2018neural, camgoz2020sign, li2020tspnet, STMC_MM, zhou2021improving,MMTLB_2022} formulate the SLT task as a neural machine translation (NMT) problem, where the input of the translation network is the output of the visual encoder. We follow this paradigm and append an MLP with two hidden layers and a subsequent translation network onto TwoStream-SLR to enable SLT. The resulting network is called TwoStream-SLT, which is illustrated in Figure~\ref{fig:TSN-SLRT}. We adopt mBART~\cite{liu2020multilingual} as our translation network due to its excellent SLT performance~\cite{MMTLB_2022}. To take advantage of our TwoStream design, we append an MLP and a translation network to each of the three heads of our TwoStream-SLR. The inputs of each MLP are the features (\ie,~gloss representations defined in Section~\ref{sec:twostream-slr}) encoded by the corresponding head network. The translation loss is a standard sequence-to-sequence cross-entropy loss~\cite{vaswani2017attention}. The overall translation loss  is the sum of three translation losses. TwoStream-SLT is jointly supervised by the recognition loss  defined in Eq.~\ref{eq:loss_slr} and the translation loss , which can be formulated as:
 
During inference, we adopt the fusion strategy for multi-source translation ensemble~\cite{firat2016zero} to combine predictions of the three translation networks. More details can be found in the supplementary materials.  
 \section{Experiment}
\noindent \textbf{Implementation Details.} 
The S3D backbone is first pretrained on Kinetics-400~\cite{kay2017kinetics}. Then we separately pretrain the video and keypoint encoder without the sign pyramid network under the supervision of a single CTC loss. Finally, TwoStream-SLR loads the pretrained weights of both encoders for SLR training with loss defined in Eq.~\ref{eq:loss_slr}. Following~\cite{MMTLB_2022}, we initialize our translation network with mBART-large-cc25\footnote[2]{https://huggingface.co/facebook/mbart-large-cc25} pretrained on CC25\footnote[3]{https://commoncrawl.org/} and freeze the S3D backbones during SLT training to prevent overfitting. Unless otherwise specified, we set  and  in Eq.~\ref{eq:loss_slr}, beam width as 5 for the CTC decoder and the SLT decoder during inference. We use cosine annealing schedule of 40 epochs and an Adam optimizer with weight decay , initial learning rate  for TwoStream-SLR and  for the MLP and translation network in TwoStream-SLT. We train our models on 8 Nvidia V100 GPUs. Please refer to the supplementary materials for more details.


\noindent \textbf{Datasets.} We use Phoenix-2014~\cite{P2014}, Phoenix-2014T~\cite{camgoz2018neural}, and CSL-Daily~\cite{zhou2021improving} to evaluate our method on SLR, while the last two datasets are also leveraged for SLT evaluation since they provide text annotations. All ablation studies are conducted on the Phoenix-2014T SLR task. \begin{itemize}[leftmargin=0.33cm]
    \item \textbf{Phoenix-2014} is a German SLR dataset with a vocabulary size of 1081 for glosses. It consists of 5672, 540, and 629 samples in the training, dev, and test set. 
\item \textbf{Phoenix-2014T} is an extension of Phoenix-2014 with a vocabulary size of 1066 for glosses and 2887 for German text. There are 7096, 519, and 642 samples in the training, dev, and test set.
\item \textbf{CSL-Daily} is a newly released large-scale Chinese sign language dataset with a vocabulary size of 2000 for glosses and 2343 for Chinese text. It consists of 18401, 1077, and 1176 samples in the training, dev, and test set.
\end{itemize}
\textbf{Evaluation Metrics.} Following previous works~\cite{STMC_MM, MMTLB_2022, camgoz2020sign, camgoz2018neural, zhou2021improving}, we adopt word error rate (WER) for SLR evaluation, and BLEU~\cite{papineni2002bleu} and ROUGE-L~\cite{lin2004rouge} to evaluate SLT. Lower WER indicates better recognition performance. For BLEU and ROUGE-L, the higher, the better. 


\begin{table}[t]
\setlength\tabcolsep{10pt}
    \caption{Comparison with previous works on \textbf{Sign Language Recognition (SLR)}. WER is adopted as the evaluation metric. Previous best results are \underline{underlined}.  denotes methods which adopt other modalities besides RGB videos such as pose keypoints~\cite{STMC_MM}, optical flow~\cite{dnf_cui}, or hand/mouth shape~\cite{koller2019weak}. The results of~\cite{subunet_iccv2017,LS-HAN_AAAI2018,dnf_cui,fcn_eccv2020,camgoz2020sign} on CSL-Daily are reproduced by SignBT~\cite{zhou2021improving}.}
    \label{tab:sota_cslr}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|cc|cc}
    \toprule
        \multirow{2}{*}{Method}  & \multicolumn{2}{c|}{Phoenix-2014} & \multicolumn{2}{c|}{Phoenix-2014T} & \multicolumn{2}{c}{CSL-Daily}  \\
        \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7}
        & Dev & Test &  Dev & Test &   Dev & Test \\
        \midrule
        SubUNets~\cite{subunet_iccv2017}&40.8&40.7&-&-&41.4&41.0 \\
        LS-HAN~\cite{LS-HAN_AAAI2018} &-&-&-&-&39.0&39.4 \\ IAN~\cite{Pu2019Iterative}&37.1&36.7&-&-&-&- \\
        ReSign~\cite{koller2017re-sign}&27.1&26.8&-&-&-&-\\
        CNN-LSTM-HMMs (Multi-Stream)~\cite{koller2019weak}&26.0&26.0&22.1&24.1&-&-  \\
        SFL~\cite{niu2020stochastic}&24.9&25.3&25.1&26.1&-&-\\
        DNF (RGB)~\cite{dnf_cui} &23.8&24.4&-&-&\underline{32.8}&32.4\\
        FCN~\cite{fcn_eccv2020}&23.7&23.9&23.3&25.1&33.2&33.5\\
        DNF (RGB+Flow)~\cite{dnf_cui} &23.1&22.9&-&-&-&-\\
        Joint-SLRT~\cite{camgoz2020sign}&-&-&24.6&24.5&33.1&\underline{32.0}\\
        VAC~\cite{Min_2021_ICCV}&21.2&22.3&-&-&-&-  \\
        LCSA~\cite{zuo22_interspeech} &21.4&21.9&-&-&-&-\\
        CMA~\cite{CrossAug_MM2020} &21.3&21.9&-&-&-&-\\
        SignBT~\cite{zhou2021improving}&-&-&22.7&23.9&33.2&33.2  \\
        MMTLB~\cite{MMTLB_2022}&-&-&21.9&22.5 &-&-\\
        SMKD~\cite{Hao_2021_ICCV} &20.8&21.0&20.8&22.4&-&-\\
        STMC-R (RGB+Pose)~\cite{STMC_MM}&21.1&20.7&\underline{19.6}&21.0&-&-\\
        CSLR (RGB+Pose) \cite{zuo2022c2slr} & \underline{20.5} & \underline{20.4} & 20.2 & \underline{20.4} &-&- \\
        \midrule
TwoStream-SLR (Ours)&\textbf{18.4}&\textbf{18.8}&\textbf{17.7}&\textbf{19.3}&\textbf{25.4}&\textbf{25.3}\\
    \bottomrule
    \end{tabular}}
\end{table}

\subsection{Comparison with State-of-the-art Methods on SLR and SLT}
For SLR, we compare our TwoStream-SLR with state-of-the-art methods on Phoenix-2014, Phoenix-2014T, and CSL-Daily, as shown in Table~\ref{tab:sota_cslr}. Our recognition network achieves a new state-of-the-art on all datasets and outperforms the previous best method by 1.6\% on Phoenix-2014, 1.1\% on Phoenix-2014T, and 6.7\% on CSL-Daily, respectively. For SLT, we compare our TwoStream-SLT with state-of-the-art methods on Phoenix-2014T and CSL-Daily as shown in Table~\ref{tab:sota_slt}. Following common practice, we evaluate our method in two settings: 1) Sign2Text (as described in Section~\ref{sec:twostream-slt}) which directly generates texts given sign videos; 2) Sign2Gloss2Text where we first use a recognition model to predict gloss sequences from sign videos, and then a translation model trained on gloss-text pairs to translate predicted gloss sequences to texts. Our TwoStream-SLT surpasses all previous methods in both settings. We believe that our better SLT performance is mainly attributed to the superior visual representations encoded by TwoStream-SLR. 


\begin{table}[t]
    \caption{Comparison with previous works on \textbf{Sign Language Translation (SLT)}. Sign2Gloss2Text indicates a two-staged pipeline and Sign2Text indicates end-to-end sign-to-text translation. Previous best results are \underline{underlined}.  denotes methods without using gloss annotations. (R: ROUGE, B: BLEU.) The results of~\cite{camgoz2018neural,camgoz2020sign} on CSL-Daily are reproduced by SignBT~\cite{zhou2021improving}.}
    \label{tab:sota_slt}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c c c c c|c c c c c}
    \toprule
    & \multicolumn{10}{c}{Phoenix-2014T}  \\
&\multicolumn{5}{c|}{Dev}& \multicolumn{5}{c}{Test}\\
    Sign2Gloss2Text&R&B1&B2&B3&B4&R&B1&B2&B3&B4\\
    \midrule
    SL-Luong~\cite{camgoz2018neural} & 44.14 & 42.88 & 30.30 & 23.02 & 18.40 & 43.80 & 43.29 & 30.39 & 22.82 & 18.13 \\
    Joint-SLRT~\cite{camgoz2020sign} & - & 47.73 & 34.82 & 27.11 & 22.11 & - & 48.47 & 35.35 & 27.57 & 22.45 \\
    SignBT~\cite{zhou2021improving} & 49.53 & 49.33 & 36.43 & 28.66 & 23.51 & 49.35 & 48.55 & 36.13 & 28.47 & 23.51 \\
    STMC-Transf~\cite{Yin2020STMCTransf} &46.31 & 48.27 & 35.20 & 27.47 & 22.47 & 46.77 & 48.73 &  36.53 & 29.03& 24.00 \\
    MMTLB~\cite{MMTLB_2022} & \underline{50.23} & \underline{50.36} & \underline{37.50} & \underline{29.69} & \underline{24.63} & \underline{49.59} & \underline{49.94} & \underline{37.28} & \underline{29.67} & \underline{24.60} \\
    TwoStream-SLT (Ours) &\textbf{52.01}&\textbf{52.35}&\textbf{39.76}&\textbf{31.85}&\textbf{26.47}&\textbf{51.59}&\textbf{52.11}&\textbf{39.81}&\textbf{32.00}&\textbf{26.71} \\
\midrule
    Sign2Text&R&B1&B2&B3&B4&R&B1&B2&B3&B4\\
    \midrule
    SL-Luong~\cite{camgoz2018neural} & 31.80 & 31.87 & 19.11 & 13.16 & 9.94 & 31.80 & 32.24 & 19.03 & 12.83 & 9.58 \\
    TSPNet~\cite{li2020tspnet} & - & - & - & - & - & 34.96 & 36.10 & 23.12 & 16.88 & 13.41 \\
    Joint-SLRT~\cite{camgoz2020sign} & - & 47.26 & 34.40 & 27.05 & 22.38 & - & 46.61 & 33.73 & 26.19 & 21.32 \\
    STMC-T~\cite{STMC_MM} &48.24 & 47.60 & 36.43 & 29.18 & 24.09 & 46.65 & 46.98 & 36.09 & 28.70 & 23.65 \\
    SignBT~\cite{zhou2021improving} & 50.29 & 51.11 & 37.90 & 29.80 & 24.45 & 49.54 & 50.80 & 37.75 & 29.72 & 24.32 \\
    MMTLB~\cite{MMTLB_2022} &\underline{53.10} & \underline{53.95} & \underline{41.12} & \underline{33.14} & \underline{27.61} & \underline{52.65} & \underline{53.97} & \underline{41.75} & \underline{33.84} & \underline{28.39} \\
    TwoStream-SLT (Ours) &\textbf{54.08}&\textbf{54.32}&\textbf{41.99}&\textbf{34.15}&\textbf{28.66}&\textbf{53.48}&\textbf{54.90}&\textbf{42.43}&\textbf{34.46}&\textbf{28.95}\\ 
    \bottomrule
    \toprule
& \multicolumn{10}{c}{CSL-Daily}  \\
    &\multicolumn{5}{c|}{Dev}& \multicolumn{5}{c}{Test}\\
    Sign2Gloss2Text&R&B1&B2&B3&B4&R&B1&B2&B3&B4\\
    \midrule
    SL-Luong~\cite{camgoz2018neural} & 40.18 & 41.46 & 25.71 & 16.57 & 11.06 & 40.05 & 41.55 & 25.73 & 16.54 & 11.03 \\
    Joint-SLRT~\cite{camgoz2020sign} & 44.18 & 46.82 & 32.22 & 22.49 & 15.94 & 44.81 & 47.09 & 32.49 & 22.61 & 16.24 \\
    SignBT~\cite{zhou2021improving} & 48.38 & \underline{50.97} & 36.16 & 26.26 & 19.53 & 48.21 & \underline{50.68} & 36.00 & 26.20 & 19.67 \\
    MMTLB~\cite{MMTLB_2022} & \underline{51.35} & 50.89 & \underline{37.96} & \underline{28.53} & \underline{21.88} & \underline{51.43} & 50.33 & \underline{37.44} & \underline{28.08} & \underline{21.46} \\
    TwoStream-SLT (Ours) &  \textbf{53.91} & \textbf{53.58} & \textbf{40.49} & \textbf{30.67} & \textbf{23.71} & \textbf{54.92} & \textbf{54.08} & \textbf{41.02} & \textbf{31.18} & \textbf{24.13} \\
    \midrule
    Sign2Text&R&B1&B2&B3&B4&R&B1&B2&B3&B4\\
    \midrule
    SL-Luong~\cite{camgoz2018neural} & 34.28 & 34.22 & 19.72 & 12.24 & 7.96 & 34.54 & 34.16 & 19.57 & 11.84 & 7.56 \\
    Joint-SLRT~\cite{camgoz2020sign} & 37.06 & 37.47 & 24.67 & 16.86 & 11.88 & 36.74 & 37.38 & 24.36 & 16.55 & 11.79 \\
    SignBT~\cite{zhou2021improving} & 49.49 & 51.46 & 37.23 & 27.51 & 20.80 & 49.31 & 51.42 & 37.26 & 27.76 & 21.34 \\
    MMTLB~\cite{MMTLB_2022} & \underline{53.38} & \underline{53.81} & \underline{40.84} & \underline{31.29} & \underline{24.42} & \underline{53.25} & \underline{53.31} & \underline{40.41} & \underline{30.87} & \underline{23.92} \\
    TwoStream-SLT (Ours) &\textbf{55.10}&\textbf{55.21}&\textbf{42.31}&\textbf{32.71}&\textbf{25.76}&\textbf{55.72}&\textbf{55.44}&\textbf{42.59}&\textbf{32.87}&\textbf{25.79}\\
    \bottomrule
    \end{tabular}}
\end{table}

\begin{table}[t]
    \caption{Study the effects of each component of TwoStream-SLR on the Phoenix-2014T SLR task. (V: video, K: keypoint, Bilateral: bidirectional lateral connection, SPN: sign pyramid network.)}
    \label{tab:abl_main}
    \centering
\begin{tabular}{cccccc|cc}
    \toprule
    V-Encoder & K-Encoder & Bilateral & SPN & Joint Head & Distillation & Dev & Test \\
    \midrule
    
    \checkmark & & & & & & 21.08 & 22.42 \\
    & \checkmark  & & & & & 27.14 & 27.19 \\
    \midrule
    \checkmark & \checkmark & & & & & 20.47 & 21.55 \\
    \checkmark &  \checkmark & \checkmark & & & & 19.03 & 20.12 \\
    \checkmark &  \checkmark & \checkmark  & \checkmark & & & 18.52 & 19.91 \\
    \checkmark &  \checkmark & \checkmark & \checkmark & \checkmark & & 18.36 & 19.49 \\ \checkmark &  \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{17.72} & \textbf{19.32} \\
    \bottomrule
    \end{tabular}\end{table}

\begin{table}[t]
    \caption{Ablation studies of: (a) lateral connection; (b) sign pyramid network (SPN); (c) weights of the auxiliary CTC losses; (d) the weight of the distillation loss; (e) self-distillation strategies, on the Phoenix-2014T SLR task. See Section~\ref{sec:twostream-slr} for the definition of  and . (V: video stream, K: keypoint stream.)}
    \centering
    \begin{subtable}[b]{0.48\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|cc}
    \toprule
    VK & KV & Connection & Dev & Test \\
    \midrule
    & & None & 18.57 & 20.03 \\
    \midrule
    \checkmark & &  & 17.88 & 19.61 \\
    & \checkmark &  & 18.82 & 19.93 \\
    \checkmark & \checkmark &   & \textbf{17.72} & \textbf{19.32} \\
    \checkmark & \checkmark &   & 17.91 & 19.54 \\
    \bottomrule
    \end{tabular}}
    \caption{Lateral connection.}
    \label{tab:lat}
    \end{subtable}
    \hfill
    \begin{subtable}[b]{0.48\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|cc}
    \toprule
    SPN-V & SPN-K & Level & Dev & Test \\
    \midrule
    \checkmark &  &  & 17.99 & 19.39 \\
    & \checkmark &  & 18.15 & 19.42 \\
    \midrule
    \checkmark & \checkmark &  & \textbf{17.72} & \textbf{19.32} \\
    \checkmark & \checkmark &  & 18.07 & 19.35 \\
    \checkmark & \checkmark &  &17.96 & 19.51 \\
    \bottomrule
    \end{tabular}}
    \caption{Sign pyramid network (SPN).}
    \label{tab:spn}
    \end{subtable}
    \newline
    \begin{subtable}[b]{0.25\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|cc}
    \toprule
     &  & Dev & Test \\
    \midrule
    0.1 & 0.5 & 17.85 & 19.42 \\
    0.2 & 0.5 & \textbf{17.72} & 19.32 \\
    0.5 & 0.5 & 17.99 & 19.30 \\
    0.2 & 0.1 & 17.80 & \textbf{19.14} \\
    0.2 & 0.2 & \textbf{17.72} & 19.49 \\
    \bottomrule
    \end{tabular}}
    \caption{Weights of the auxiliary CTC loss.}
    \label{tab:lossw}
    \end{subtable}
    \hfill
    \begin{subtable}[b]{0.3\textwidth}
\centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cc}
    \toprule
    Weight of   & Dev & Test \\
    \midrule
    0.2  & 18.41 & \textbf{19.21} \\
    0.5  & 18.20 & 19.63 \\
    1.0   & \textbf{17.72} & 19.32 \\
    1.5   & 18.28 & 19.93 \\
    2.0   & 17.83 & 19.28 \\
    \bottomrule
    \end{tabular}}
    \caption{The weight of the distillation loss.}
    \label{tab:weights_dist_loss}
    \end{subtable}
    \hfill
    \begin{subtable}[b]{0.4\textwidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccc|cc}
    \toprule
    Teacher & Students & Target & Dev & Test \\
    \midrule
    Joint Head & V, K & Soft & 18.82 & 19.93 \\
    Ensemble & V, K & Soft & 18.12 & 19.68 \\
    Ensemble & V, K, J & Soft & \textbf{17.72} & \textbf{19.32} \\
    Ensemble & V, K, J & Hard & 18.25 & 19.58 \\
    \bottomrule
    \end{tabular}}
    \caption{Distillation strategies. ``J'' denotes joint head.}
    \label{tab:dist}
    \end{subtable}
\end{table}



\subsection{Ablation Study}
\label{sec:ablation}
\textbf{Effects of Each Component.} We first show the effects of each proposed component of TwoStream-SLR in Table~\ref{tab:abl_main}. Without the dual architecture, the two single streams (one models RGB videos and the other models keypoint sequences) achieve 21.08\% and 27.14\% WER on the Phoenix-2014T Dev set. By averaging the final predictions from two streams, the WER is reduced to 20.47\%. The proposed bidirectional lateral connection brings in information interaction between two streams, leading to the significant improvement of 1.44\%. Introducing the sign pyramid network (SPN) with auxiliary CTC losses facilitates intermediate layers to learn more meaningful features, further boosting the performance to 18.52\%. As described in Section~\ref{sec:twostream-slr}, besides the individual head networks, we present a joint head to further integrate the encoded features from two streams. Our approach equipped with the joint head achieves 18.36 WER. At last, by adding the auxiliary frame-wise self-distillation loss, our framework attains the best result, yielding the WER of 17.72. 

\textbf{Study on Lateral Connection.} 
The goal of lateral connection is to provide information interaction such that the video stream and keypoint stream can complement each other. Here we compare our default configuration where the bidirectional lateral connection is conducted on , , and  (see Section~\ref{sec:twostream-slr} for their definitions) of two streams, with other variants including unidirectional lateral connection and different connection strategies. The comparison is shown in Table~\ref{tab:lat}. Without the lateral connection, the baseline model achieves 18.57 WER on the Phoenix-2014T Dev set. Thanks to the information interaction, both unidirectional (videokeypoint and keypointvideo) and bidirectional lateral connection strategies outperform the baseline. We adopt the bidirectional lateral connection performed on , , and  due to its best performance.


\textbf{Study on Sign Pyramid Network and Auxiliary Supervisions.}
Sign language understanding suffers from data scarcity. To capture glosses of various temporal spans and drive intermediate layers to learn more meaningful features, we propose a sign pyramid network (SPN) with auxiliary CTC losses. Here we study three key factors: 1) applying SPN on a single stream or both streams; 2) the levels of SPN; 3) the loss weights of auxiliary CTC losses of the two streams. The first two factors are studied in Table~\ref{tab:spn} while the last one in Table~\ref{tab:lossw}. We observe that applying SPN on both video and keypoint streams yields better results. We also find imposing extra CTC supervision on very shallow layers (\ie,~) hurts the performance, thus we advocate generating two levels of pyramid ( and ) for each stream. We set  and  in Eq.~\ref{eq:loss_slr} as 0.2 and 0.5 as they give better performance. 

\textbf{Study on Self-Distillation Strategies.} The proposed self-distillation loss provides frame-level supervision. There is a trade-off between the pseudo fine-grained supervision (self-distillation loss) and the coarse-grained supervision (CTC loss). Here we vary the weight of self-distillation loss  and show the results in Table \ref{tab:weights_dist_loss}. The best performance is obtained when the weight is set to 1.0.
Besides, we also study: 1) which prediction should be the pseudo-target (teacher) 2) which heads in our dual visual encoder should be taught by the pseudo-target (students); 3) whether to binarize the probabilities of the pseudo-target (soft target or hard target). Table~\ref{tab:dist} shows the results. As described in Section~\ref{sec:twostream-slr}, we present three head networks in the dual visual encoder, namely the video head, keypoint head, and joint head. We observe that using averaged gloss probabilities from three heads (Ensemble) as the pseudo-target outperforms using predictions of the joint head. We also find that applying the self-distillation loss on all three heads achieves better results than teaching only two heads, and using soft predictions as pseudo-targets outperforms the one with hard pseudo-targets.


\textbf{Study on Keypoint Inputs.}
Sign languages utilize multiple visual signals including handshape, facial expressions, the movement of body, head, mouth, and eyes, to convey information. To investigate the importance of various keypoints in SLR, we train several single-stream keypoint encoders using different combinations of keypoints as inputs and evaluate their performance on the Phoenix-2014T SLR task. We use the HRNet trained on COCO-WholeBody to generate 79 keypoints in total. These keypoints can be divided into 4 groups: 1) 11 upper body keypoints; 2) 42 hand keypoints; 3) 10 mouth keypoints; 4) 16 face keypoints (excluding the mouth). Step by step, we add each group of keypoints into model training and the results are shown in Table~\ref{tab:keypoint_comb}. It can be seen that all parts contribute to sign language understanding. As described in Section~\ref{sec:twostream-slr}, we use a Gaussian function with a hyper-parameter , which denotes keypoint scale, to generate a set of heatmaps, each of size  to represent keypoint sequences. Here we study the value of  and the resolution of heatmaps in Table~\ref{tab:heatmap_input}. We find that  and  achieves the best performance.



\begin{table}[t]
    \caption{Ablation studies of: (a) various combinations of keypoints as the inputs of our keypoint encoder; (b) the keypoint scale  of the Gaussian function and the resolution of the generated heatmaps, on the Phoenix-2014T SLR task.}
    \centering
    \begin{subtable}[b]{0.68\textwidth}
\centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc|cc}
    \toprule
    Upper body & Hand  & Mouth & Face & \#Keypoints & Dev & Test \\
    \midrule
    \checkmark &  &  &  & 11 & 49.11 & 48.46 \\
    \checkmark & \checkmark &  &  & 53 (+42) & 37.15 & 36.88 \\
    \checkmark & \checkmark & \checkmark &  & 63 (+10) & 28.42 & 28.20 \\\checkmark & \checkmark & \checkmark & \checkmark & 79 (+16)& \textbf{27.14} & \textbf{27.19} \\ \bottomrule
    \end{tabular}}
    \caption{The effects of different combinations of keypoints as inputs of the keypoint encoder.}
\label{tab:keypoint_comb}
    \end{subtable}
    \hfill
    \begin{subtable}[b]{0.3\textwidth}
\centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|cc}
    \toprule
     &  & Dev & Test \\
    \midrule
    1 & 56 & 29.78 & 28.72 \\ 2 & 56 & 30.10 & 29.02 \\ 2 & 112 & 27.22  & 27.52 \\ 4 & 112 & \textbf{27.14} & 27.19 \\ 6 & 112 & 27.94 & \textbf{27.10} \\ \bottomrule
    \end{tabular}}
    \caption{Keypoint scale  and heatmap resolution.}
\label{tab:heatmap_input}
    \end{subtable}
\end{table} \section{Conclusion}
In this paper, we concentrate on how to introduce domain knowledge into sign language understanding. To achieve the goal, we present a novel framework named TwoStream-SLR which adopts two streams to model RGB videos and keypoint sequences for sign language recognition. A variety of techniques are proposed to make the two streams interact with each other, including bidirectional lateral connection, sign pyramid network, and frame-level self-distillation. We further extend TwoStream-SLR to a sign language translation model by attaching an MLP and a translation network, yielding the translation framework named TwoStream-SLT. Our TwoStream-SLR and TwoStream-SLT achieve state-of-the-art performance on SLR and SLT tasks across a series of datasets including Phoenix-2014, Phoenix-2014T, and CSL-Daily. We hope that our approach can serve as a baseline to facilitate future research.

\textbf{Acknowledgements.} The work described in this paper was partially supported by a grant from the Research Grants Council of the HKSAR, China (Project No. HKUST16200118).

 

\small
\bibliographystyle{nips}
\bibliography{egbib}

\appendix

\section{Loss Formulation}
\textbf{CTC Loss.} We apply the CTC loss \cite{graves2006connectionist} not only on the outputs of the video head, pose head, and joint head, but also on the outputs of the sign pyramid networks for auxiliary training. The CTC loss considers all feasible alignments between two sequences.
Specifically, given a video  containing  frames and its annotated gloss sequence  containing  glosses, CTC yields  by marginalizing over all feasible decoding paths:

where  denotes a frame-level gloss path of length  in our model and  is the set of all feasible mappings between  and .
The probability  is computed by applying the Softmax function on the outputs of each head network. 
Finally, the CTC loss is defined as:


\textbf{Self-Distillation Loss.} Given a sign video , we use , , and  to denote its probability at timestamp  generated by the video head, keypoint head, and joint head, respectively. The averaged prediction  serves as the pseudo-target and it is calculated by:

Then the self-distillation loss  can be formulated as:

where  represents the KL-divergence between the probability distributions  and , and  is the length of the output.

\textbf{Translation Loss.} Given a sign video  and its associated spoken language sentence  with  words, the translation loss  is a sequence-to-sequence cross-entropy loss defined as:



\section{More Implementation Details}
\subsection{TwoStream-SLR}
\textbf{Training.}
The S3D backbone\footnote{https://github.com/kylemin/S3D} is pretrained on Kinetics-400~\cite{K400_dataset} by~\cite{min2019tased}. We separately train the video encoder and the keypoint encoder without the sign pyramid network via a single CTC loss. Then the weights of two pretrained encoders are loaded into our TwoStream-SLR for SLR training. The newly added components including the sign pyramid networks and joint head are randomly initialized. Data augmentations include spatial cropping in the range of [0.7-1.0] and frame-rate augmentation in the range of [-]. We adopt identical data augmentations for RGB videos and heatmap sequences to maintain spatial and temporal consistency. We use the first four blocks of the S3D backbone and freeze the first block during training. For each training stage, we train our model on 8 Nvidia V100 GPUs for 40 epochs with the initial learning rate , a cosine annealing schedule, and an Adam optimizer with weight decay .


\textbf{Inference.} We drop the sign pyramid networks in the inference stage. A CTC decoder is adopted to yield the final gloss predictions.
Specifically, denoting the gloss probabilities predicted by the video head, keypoint head, and joint head at the -th frame as , and , respectively, we first compute the average probability .
Then the CTC decoder takes  as inputs and outputs the final gloss prediction using the beam search algorithm with a beam width of 5. More details are available in \cite{graves2006connectionist}.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\textwidth]{figures/keypoint.pdf}
\caption{Illustration of keypoints used in our approach.}
\label{fig:keypoint}
\end{figure}
\subsection{TwoStream-SLT}
\textbf{Training.} We attach an MLP and a translation network onto each of the three heads in our TwoStream-SLR model to perform SLT. The resulting network is termed as TwoStream-SLT. Following~\cite{MMTLB_2022}, we use mBART-large-cc25\footnote{https://huggingface.co/facebook/mbart-large-cc25} as our translation network. Before SLT training, we pretrain our translation networks on gloss-text pairs. All text sequences are tokenized into subword tokens by the sentence-piece model used in mBART. A special language id token, \ie~<de\_DE> for German and <zh\_CN> for Chinese, is prepended to the text sequence to identify the target language. During SLT training, we freeze the S3D backbones and tune the video head, keypoint head, joint head, MLPs, and translation networks. We experimentally find that this strategy performs better than training the whole network while reducing computational cost. We train our TwoStream-SLT for 40 epochs, using a cosine annealing schedule and an Adam optimizer with weight decay . The initial learning rate is set to  for the visual heads and  for the MLPs and translation networks. 

\textbf{Inference.} We adopt the fusion strategy for multi-source translation ensemble proposed in~\cite{firat2016zero} to aggregate results of the three translation networks. Concretely, given a sign video  and tokens  before step , we average the predictions from the three independent translation networks to obtain the probability for the -th token as

where , , and  denote predictions from the translation networks appended to the video head, keypoint head, and joint head, respectively. The averaged probabilities are used to decode text sequences. We use a beam search decoder with a beam width of 5 and without length penalty.



\subsection{Keypoint Illustration.} We show the keypoints used in our TwoStream network in Figure~\ref{fig:keypoint}. There are 79 keypoints in total, including 26 face keypoints, 42 hand keypoints, and 11 upper body keypoints.



\begin{table}[t]
    \caption{Ablation studies of \textbf{Sign Language Translation (SLT)} on Phoenix-2014T and CSL-Daily. SingleStream-SLT which only utilizes a single video encoder without modelling keypoints serves as our baseline. TwoStream-SLT-V/K/J denotes the network where only one translation network is attached onto the video head/keypoint head/joint head. TwoStream-SLT-V/K/J denotes that we train TwoStream-SLT-V/K/J with three different random seeds, and predictions of the three independently trained models attached to the same head are averaged to generate final results.
}
    \label{tab:ablation_slt}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c c c c c|c c c c c}
    \toprule
    & \multicolumn{10}{c}{Phoenix-2014T}  \\
&\multicolumn{5}{c|}{Dev}& \multicolumn{5}{c}{Test}\\
    Framework&R&B1&B2&B3&B4&R&B1&B2&B3&B4\\
    \midrule
    SingleStream-SLT &52.60&53.35&40.75&32.92&27.59&52.15&53.85&41.33&33.53&28.16  \\
    \midrule
    TwoStream-SLT-V &53.11&53.62&41.02&33.02&27.53&53.28&54.38&42.04&34.14&28.68 \\
    TwoStream-SLT-K &53.21&53.88&41.14&33.26&27.83&52.87&54.58&41.78&33.60&27.98 \\
    TwoStream-SLT-J &53.05&53.94&41.37&33.50&28.11&52.74&54.00&41.66&33.72&28.23 \\
    \midrule
    TwoStream-SLT-V &53.89&54.05&41.68&33.69&28.21&53.27&54.57&42.24&34.25&28.70 \\
    TwoStream-SLT-K &53.32&53.66&41.31&33.55&28.10&53.19&54.22&41.72&33.82&28.42 \\
    TwoStream-SLT-J &53.51&54.03&41.51&33.51&27.97&53.17&54.11&41.78&33.88&28.38 \\        
    \midrule
    TwoStream-SLT &\textbf{54.08}&\textbf{54.32}&\textbf{41.99}&\textbf{34.15}&\textbf{28.66}&\textbf{53.48}&\textbf{54.90}&\textbf{42.43}&\textbf{34.46}&\textbf{28.95}\\ 
    \bottomrule
    \toprule
& \multicolumn{10}{c}{CSL-Daily}  \\
    &\multicolumn{5}{c|}{Dev}& \multicolumn{5}{c}{Test}\\
    Framework&R&B1&B2&B3&B4&R&B1&B2&B3&B4 \\
    \midrule
    SingleStream-SLT &53.33&53.29&40.37&30.81&23.95&53.32&52.84&40.12&30.57&23.70 \\
    \midrule
    TwoStream-SLT-V &54.50&54.22&41.46&31.87&24.93&54.89&54.12&41.32&31.64&24.66 \\
    TwoStream-SLT-K &53.42&53.68&40.83&31.22&24.43&53.95&54.24&41.34&31.64&24.58 \\
    TwoStream-SLT-J &54.00&54.39&41.25&31.50&24.61&54.45&54.73&41.54&31.68&24.64 \\
    \midrule
    TwoStream-SLT-V &54.67&54.66&41.79&32.21&25.28&55.18&54.75&41.90&32.21&25.15 \\
    TwoStream-SLT-K &54.03&54.43&41.60&31.95&25.01&55.07&55.34&42.36&32.58&25.42 \\
    TwoStream-SLT-J &54.26&54.84&41.79&32.01&24.95&54.94&54.91&42.04&32.32&25.25 \\
    \midrule
    TwoStream-SLT &\textbf{55.10}&\textbf{55.21}&\textbf{42.31}&\textbf{32.71}&\textbf{25.76}&\textbf{55.72}&\textbf{55.44}&\textbf{42.59}&\textbf{32.87}&\textbf{25.79}\\
    \bottomrule
    \end{tabular}}
\end{table}

\section{More Experiments}

\textbf{Ablation Study of Sign Language Translation.}
We show how our two-stream network can boost the performance of sign language translation in Table~\ref{tab:ablation_slt}. 
We leverage a network containing a single video encoder followed by an MLP and a translation network as our baseline.
We name this network as SingleStream-SLT. Note that our TwoStream-SLT appends an independent translation network onto the video head, keypoint head, and joint head, respectively. For a fair comparison with the baseline, we present three variants of TwoStream-SLT, namely TwoStream-SLT-V, TwoStream-SLT-K, and TwoStream-SLT-J. In each of the variants, only a single translation network is appended onto the video head, keypoint head, or joint head. We observe that almost all variants achieve comparable or better performance than SingleStream-SLT, showing the superiority of the two-stream design. To further demonstrate the benefits of our translation fusion strategy, we separately train three TwoStream-SLT-V/K/J models with different random seeds and average predictions of the three translation networks attached to the same head. The model with the fusion strategy is termed as TwoStream-SLT-V/K/J. Our TwoStream-SLT outperforms all three models with the fusion strategy showing that the improvement comes not only from the ensemble strategy, but also from our two-stream visual encoder.


\textbf{Ablation Study on Phoenix-2014 and CSL-Daily.} We conduct extra studies to show the effects of each proposed component on Phoenix-2014 and CSL-Daily SLR tasks as shown in Table~\ref{tab:supp_abl_main}.
\begin{table}[t]
    \caption{Ablation study on the effects of each proposed component in TwoStream-SLR on Phoenix-2014 and CSL-Daily datasets. The evaluation metric is WER in \%.}
    \label{tab:supp_abl_main}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccccc|cc}
    \toprule
    Dataset & RGB & Keypoints & Bilateral & Pyramid & Joint Head & Distillation & Dev & Test \\
    \midrule
    \multirow{7}{*}{Phoenix-2014} & \checkmark & & & & & & 22.44 & 23.32 \\ 
    & & \checkmark   & & & & & 28.56 & 28.00 \\
& \checkmark  &  \checkmark  & & & & & 22.02 & 22.76 \\
    &  \checkmark  &  \checkmark & \checkmark  & & & & 19.89 & 20.27 \\
& \checkmark &  \checkmark & \checkmark  & \checkmark & & & 19.42 & 19.62 \\
    & \checkmark &  \checkmark & \checkmark & \checkmark & \checkmark & & 18.97 & 19.04 \\
    & \checkmark &  \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{18.39} & \textbf{18.79} \\

    \midrule
    \multirow{7}{*}{CSL-Daily}   & \checkmark & & & & & & 28.88 & 28.50 \\ 
    &  & \checkmark & & & & & 34.59  & 34.08 \\ 
& \checkmark  &  \checkmark  & & & & & 28.44 & 28.43 \\
    & \checkmark & \checkmark & \checkmark &  & & & 27.55  & 27.01 \\
& \checkmark &  \checkmark & \checkmark  & \checkmark & & & 26.50 & 26.18 \\
    & \checkmark &  \checkmark & \checkmark & \checkmark & \checkmark & & 25.89 & 25.47 \\
    & \checkmark &  \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{25.44} & \textbf{25.33} \\
    
    \bottomrule
    \end{tabular}}
\end{table}





\section{Qualitative Results}
\textbf{Keypoint Heatmaps Generated by HRNet.}
\begin{figure}[t]
\centering
\includegraphics[width=1.0\textwidth]{figures/heatmaps.pdf}
\caption{Visualization for video frames and corresponding keypoint heatmaps generated by HRNet. The first two rows are from Phoenix-2014T, while the last two rows are from CSL-Daily.}
\label{fig:heatmap}
\end{figure}
As shown in Figure \ref{fig:heatmap}, we visualize the keypoint heatmaps generated by HRNet by randomly selecting five frames from the dev set of Phoenix-2014T and CSL-Daily, respectively. 
We find that the heatmaps are robust to signer appearance, hand positions, and palm orientation in most cases. 


\textbf{SLR Results.} As shown in Table \ref{tab:qual_s2g}, we conduct qualitative analysis for our TwoStream-SLR and show three samples from the dev set of Phoenix-2014T and CSL-Daily, respectively.
It is clear to see that using both video and keypoint streams (TwoStream-SLR) yields more accurate gloss predictions than using either one of them. The result suggests the effectiveness of integrating the features extracted from both the streams.

\begin{CJK*}{UTF8}{gbsn}
\begin{table}[ht!]
\caption{Qualitative results on Phoenix-2014T (Example (a,b,c)) and CSL-Daily (Example (d,e,f)). We use different colors to represent \sub{substitutions}, \del{deletions}, and \ins{insertions}, respectively.}
\label{tab:qual_s2g}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|c}
\toprule
\textbf{Example (a)} & & WER \\
\midrule
\multirow{2}{*}{Groundtruth} & SUED MEHR RUHIG FREUNDLICH BISSCHEN MILD & \multirow{2}{*}{-}\\
& (South More Calm Friendly Little Mild) & \\
\midrule
\multirow{2}{*}{Pred. (TwoStream-SLR)} & SUED MEHR RUHIG FREUNDLICH BISSCHEN MILD & \multirow{2}{*}{0.00} \\
& (South More Calm Friendly Little Mild) & \\
\midrule
\multirow{2}{*}{Pred. (Video only)} & SUED \del{MEHR} RUHIG FREUNDLICH BISSCHEN MILD & \multirow{2}{*}{16.67} \\
& (South \del{More} Calm Friendly Little Mild) & \\
\midrule
\multirow{2}{*}{Pred. (Keypoint only)} & SUED MEHR RUHIG FREUNDLICH \sub{TATSAECHLICH} \sub{BESSER} & \multirow{2}{*}{33.33} \\
& (South More Calm Friendly \sub{Indeed} \sub{Better}) & \\
\midrule
\midrule
\textbf{Example (b)} & & WER \\
\midrule
\multirow{2}{*}{Groundtruth} & AUCH NAH FLUSS UND ALPEN AUCH MOEGLICH NEBEL & \multirow{2}{*}{-}\\
& (Also Close Flow And Alps Also Possible Fog) & \\
\midrule
\multirow{2}{*}{Pred. (TwoStream-SLR)} & AUCH NAH FLUSS UND \ins{BERG} ALPEN AUCH MOEGLICH NEBEL & \multirow{2}{*}{12.50} \\
& (Also Close Flow And \ins{Mountain} Alps Also Possible Fog) & \\
\midrule
\multirow{2}{*}{Pred. (Video only)} & AUCH NAH FLUSS \del{UND} ALPEN \ins{BERG} AUCH MOEGLICH NEBEL & \multirow{2}{*}{25.00} \\
& (Also Close Flow \del{And} Alps \ins{Mountain} Also Possible Fog) & \\
\midrule
\multirow{2}{*}{Pred. (Keypoint only)} & AUCH NAH \sub{BISSCHEN} \sub{BERG} \sub{TAL} AUCH MOEGLICH NEBEL & \multirow{2}{*}{37.50} \\
& (Also Close \sub{Little} \sub{Mountain} \sub{Valley} Also Possible Fog) & \\ 
\midrule
\midrule
\textbf{Example (c)} & & WER \\
\midrule
\multirow{2}{*}{Groundtruth} & NACHT SYLT DREIZEHN GRAD MITTE BERG TAL NULL GRAD & \multirow{2}{*}{-}\\
& (Night Sylt Thirteen Degree Center Mountain Valley Zero Degree) & \\
\midrule
\multirow{2}{*}{Pred. (TwoStream-SLR)} & NACHT SYLT DREIZEHN GRAD MITTE BERG TAL NULL GRAD & \multirow{2}{*}{0.00} \\
& (Night Sylt Thirteen Degree Center Mountain Valley Zero Degree) & \\
\midrule
\multirow{2}{*}{Pred. (Video only)} & NACHT SYLT DREIZEHN GRAD MITTE BERG \del{TAL} NULL GRAD & \multirow{2}{*}{11.11} \\
& (Night Sylt Thirteen Degree Center Mountain \del{Valley} Zero Degree) & \\
\midrule
\multirow{2}{*}{Pred. (Keypoint only)} & NACHT \sub{DONNERSTAG} DREIZEHN GRAD MITTE BERG \del{TAL} NULL GRAD & \multirow{2}{*}{22.22} \\
& (Night \sub{Thursday} Thirteen Degree Center Mountain \del{Valley} Zero Degree) & \\
\midrule
\midrule
\textbf{Example (d)} & & WER \\
\midrule
Groundtruth & \ \ \ \ \ (Because Weather Bad Flight Canceled) & -\\
\midrule
Pred. (TwoStream-SLR) & \ \ \ \ \ (Because Weather Bad Flight Canceled) & 0.00 \\
\midrule
Pred. (Video only) & \ \ \ \del{}\ \ (Because Weather Bad \del{Flight} Canceled) & 20.00 \\
\midrule
Pred. (Keypoint only) & \ \ \ \sub{}\ \sub{}\ (Because Weather Bad \sub{Look} \sub{Deleted}) & 40.00\\
\midrule
\midrule
\textbf{Example (e)} & & WER \\
\midrule
Groundtruth & \ \ \ \ (Bath Feet Winter Good) & -\\
\midrule
Pred. (TwoStream-SLR) & \ \ \ \ (Bath Feet Winter Good) & 0.00 \\
\midrule
Pred. (Video only) & \ \ \del{}\ \ (Bath Feet \del{Winter} Good) & 25.00\\
\midrule
Pred. (Keypoint only) & \del{}\ \ \ \ (\del{Bath} Feet Winter Good) & 25.00\\
\midrule
\midrule
\textbf{Example (f)} & & WER \\
\midrule
\multirow{2}{*}{Groundtruth} & \ \ \ \ \ \ \ \ & \multirow{2}{*}{-}\\
& (Farmer To Dog Heart Love Result Dog Bite) & \\
\midrule
\multirow{2}{*}{Pred. (TwoStream-SLR)} & \ \ \ \ \ \ins{}\ \ \  & \multirow{2}{*}{12.50} \\
& (Farmer To Dog Heart Love \ins{At last} Result Dog Bite) & \\
\midrule
\multirow{2}{*}{Pred. (Video only)} & \sub{}\ \ \ \ \ \ \ \del{} & \multirow{2}{*}{25.00}\\
& (\sub{Rural} To Dog Heart Love Result Dog \del{Bite}) & \\
\midrule
\multirow{2}{*}{Pred. (Keypoint only)} & \sub{}\ \ \ \ \ \ins{}\ \ \ \del{} & \multirow{2}{*}{37.50}\\
& (\sub{Rural} To Dog Heart Love \ins{At last} Result Dog \del{Bite}) & \\
\bottomrule
\end{tabular}}
\end{table}
\end{CJK*}


\section{Broader Impact and Limitations}
In this paper, we propose a two-stream network for sign language recognition (SLR) and sign language translation (SLT) to improve communication between the hearing people and the deaf community. Deep learning benefits from large-scale training data while existing datasets of SLR and SLT only contain thousands of parallel data, which may make the training insufficient. There may be unpredictable failures, similar to most translation systems. Please do not use it for scenarios where failures will lead to serious consequences. The method is data-driven, and thus the performance may be affected by biases in the data. So please also be careful about the data collection process when using it. In addition, our approach relies on the keypoints estimator. Inaccurate estimations may hurt the performance. Improving the keypoints estimator is another promising way to facilitate sign language recognition and translation in our framework.


 



\end{document}
