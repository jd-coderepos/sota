
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{array}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs} \usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{varwidth}
\usepackage{pifont}
\usepackage{makecell}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\usepackage{wrapfig}
\usepackage[flushleft]{threeparttable}
\usepackage[vlined,linesnumbered,ruled,algo2e]{algorithm2e}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\newcommand\xinyu[1]{\textcolor{blue}{[Xinyu: #1]}}
\usepackage{varwidth}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{hyperref}

\newcommand{\finntk}{\hat\Theta}
\newcommand{\infntk}{\Theta}
\newcommand{\Id}{\textbf{I}}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{defi}{Definition}
\newtheorem{rem}{Remark}
\newtheorem{cor}{Corollary}
\newtheorem{ex}{Example}


\title{Neural Architecture Search on ImageNet in Four GPU Hours: \\ A Theoretically Inspired Perspective}



\author{Wuyang Chen, Xinyu Gong, Zhangyang Wang
\\
Department of Electrical and Computer Engineering\\
The University of Texas at Austin, Austin, TX, USA \\
\texttt{\{wuyang.chen,xinyu.gong,atlaswang\}@utexas.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle
\vspace{-1em}
\begin{abstract}
Neural Architecture Search (NAS) has been explosively studied to automate the discovery of top-performer neural networks. Current works require heavy training of supernet or intensive architecture evaluations, thus suffering from heavy resource consumption and often incurring search bias due to truncated training or approximations. Can we select the best neural architectures without involving any training and eliminate a drastic portion of the search cost?
We provide an affirmative answer, by proposing a novel framework called \textit{training-free neural architecture search} (\textbf{TE-NAS}). TE-NAS ranks architectures by analyzing the spectrum of the neural tangent kernel (NTK) and the number of linear regions in the input space. Both are motivated by recent theory advances in deep networks and can be computed without any training and any label. We show that: (1) these two measurements imply the \textit{trainability} and \textit{expressivity} of a neural network; (2) they strongly correlate with the network's test accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more flexible and superior trade-off between the trainability and expressivity during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes high-quality search but only costs \textbf{0.5} and \textbf{4} GPU hours with one 1080Ti on CIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in bridging the theoretical findings of deep networks and practical impacts in real NAS applications.
Code is available at: \url{https://github.com/VITA-Group/TENAS}.
\end{abstract}

\vspace{-1em}
\section{Introduction}
\vspace{-1em}
The recent development of deep networks significantly contributes to the success of computer vision. Thanks to many efforts by human designers, the performance of deep networks have been significantly boosted \citep{krizhevsky2012imagenet,simonyan2014very,szegedy2015going,he2016deep,xie2017aggregated}. However, the manual creation of new network architectures not only costs enormous time and resources due to trial-and-error, but also depends on the design experience that does not always scale up. 
To reduce the human efforts and costs, neural architecture search (\textbf{NAS}) has recently amassed explosive interests, leading to principled and automated discovery for good architectures in a given search space of candidates \citep{zoph2016neural,brock2017smash,pham2018efficient,liu2018progressive,chen2018searching,bender2018understanding,gong2019autogan,chen2020fasterseg,fu2020autogan}.

As an optimization problem, NAS faces two core questions: 1) ``\textbf{how to evaluate}'', i.e. the objective function that defines what are good architectures we want; 2) ``\textbf{how to optimize}'', i.e. by what means we could effectively optimize the objective function. 
These two questions are entangled and highly non-trivial, since the search spaces are of extremely high dimension, and the generalization ability of architectures cannot be easily inferred \citep{dong2020bench,dong2020nats}.
Existing NAS methods mainly leverage the validation set and conduct accuracy-driven architecture optimization. They either formulate the search space as a super-network (``supernet'') and make the training loss differentiable through the architecture parameters \citep{liu2018darts}, or treat the architecture selection as a sequential decision making process \citep{zoph2016neural} or evolution of genetics \citep{real2019regularized}.
However, these NAS algorithms suffer from heavy consumption of both time and GPU resources. Training a supernet till convergence is extremely slow, even with many effective heuristics for sampling or channel approximations \citep{dong2019searching,xu2019pc}. 
Approximated proxy inference such as truncated training/early stopping can accelerate the search, but is well known to introduce search bias to the inaccurate results obtained \citep{pham2018efficient,liang2019darts+,tan2020efficientdet}.
The heavy search cost not only slows down the discovery of novel architectures, but also blocks us from more meaningfully understanding the NAS behaviors.


On the other hand, the analysis of neural network's trainability (how effective a network can be optimized via gradient descent) and expressivity (how complex the function a network can represent) has witnessed exciting development recently in the deep learning theory fields.
By formulating neural networks as a Gaussian Process (no training involved), the gradient descent training dynamics can be characterized by the Neural Tangent Kernel (NTK) of infinite \citep{lee2019wide} or finite \citep{yang2019scaling} width networks, from which several useful measures can be derived to depict the network trainability at the initialization.   \citet{hanin2019complexity,hanin2019deep,xiong2020number} describe another measure of network expressivity, also without any training, by counting the number of unique linear regions that a neural network can divide in its input space. 
We are therefore inspired to ask:
\vspace{-0.5em}
\begin{itemize}[leftmargin=*]
    \item \emph{\textbf{How to optimize} NAS at network's initialization without involving any training, thus significantly eliminating a heavy portion of the search cost?}
    \item \emph{Can we define \textbf{how to evaluate} in NAS by analyzing the trainability and expressivity of architectures, and further benefit our understanding of the search process?}
\end{itemize}
\vspace{-0.5em}
Our answers are \textbf{yes} to both questions.
In this work, we propose TE-NAS, a framework for \underline{t}raining-fr\underline{e}e \underline{n}eural \underline{a}rchitecture \underline{s}earch. We leverage \textit{two indicators}, the condition number of NTK and the number of linear regions, that can decouple and effectively characterize the trainability and expressivity of architectures respectively in complex NAS search spaces. Most importantly, these two indicators can be measured in a training-free and label-free manner, thus largely accelerates the NAS search process and benefits the understanding of discovered architectures. To our best knowledge, TE-NAS makes the first attempt to bridge the theoretical findings of deep neural networks and real-world NAS applications. While we intend not to claim that the two indicators we use are the only nor the best options, we hope our work opens a door to theoretically-inspired NAS and inspires the discovery of more deep network indicators.
Our contributions are summarized as below:
\vspace{-0.5em}
\begin{itemize}[leftmargin=*]
    \item We identify and investigate two training-free and label-free indicators to rank the quality of deep architectures: the spectrum of their NTKs, and the number of linear regions in their input space. Our study finds that they reliably indicate the trainability and expressivity of a deep network respectively, and are strongly correlated with the network's test accuracy.
    \item We leverage the above two theoretically-inspired indicators to establish a training-free NAS framework, \textbf{TE-NAS}, therefore eliminating a drastic portion of the search cost. We further introduce a pruning-based mechanism, to boost search efficiency and to more flexibly trade-off between trainability and expressivity. 
    \item In NAS-Bench-201/DARTS search spaces, \textbf{TE-NAS} discovers architectures with a strong performance at remarkably lower search costs, compared to previous efforts. With just one 1080Ti, it only costs 0.5 GPU hours to search on CIFAR10, and 4 GPU hours on ImageNet, respectively, setting the new record for ultra-efficient yet high-quality NAS.
\end{itemize}\vspace{-1em}


\section{Related Works}\label{sec:related_works}\vspace{-0.5em}
\textbf{Neural architecture search (NAS)} is recently proposed to accelerate the principled and automated discovery of high-performance networks. However, most works suffer from heavy search cost, for both weight-sharing based methods \citep{liu2018darts,dong2019searching,liu2019auto,yu2020bignas,li2020sgas,Yang_2020_CVPR} and single-path sampling-based methods \citep{pham2018efficient,guo2019single,real2019regularized,tan2020efficientdet,li2020gp,yang2020hournas}. A one-shot super network can share its parameters to sampled sub-networks and accelerate the architecture evaluations, but it is very heavy and hard to optimize and suffers from a poor correlation between its accuracy and those of the sub-networks \citep{yu2020evaluating}. Sampling-based methods achieve more accurate architecture evaluations, but their truncated training still imposes bias to the performance ranking since this is based on the results of early training stages.

Instead of estimating architecture performance by direct training, people also try to predict network's accuracy (or ranking), called \textbf{predictor based NAS} methods \citep{liu2018progressive,luo2018neural,dai2019chamnet,luo2020semi}.
Graph neural network (GNN) is a popular choice as the predictor model \citep{wen2019neural,chen2020fitting}.
\citet{siems2020bench} even propose the first large-scale surrogate benchmark, where most of the architectures' accuracies are predicted by a pretrained GNN predictor.
The learned predictor can achieve highly accurate performance evaluation. However, the data collection step - sampling representative architectures and train them till converge - still requires extremely high cost. People have to sample and train 2,000 to 50,000 architectures to serve as the training data for the predictor. Moreover, none of these works can demonstrate the cross-space transferability of their predictors. This means one has to repeat the data collection and predictor training whenever facing an unseen search space, which is highly 
nonscalable.

The heavy cost of architecture evaluation hinders the \textbf{understanding} of the NAS search process. Recent pioneer works like 
\citet{shu2019understanding} observed that DARTS and ENAS tend to favor architectures with wide and shallow cell structures due to their smooth loss landscape. \citet{siems2020bench} studied the distribution of test error for different cell depths and numbers of parameter-free operators. \citet{chen2020stabilizing} for the first time regularizes the Hessian norm of the validation loss and visualizes the smoother loss landscape of the supernet. \citet{li2020neural} proposed to approximate the validation loss landscape by learning a mapping from neural architectures to their corresponding validate losses. 
Still, these analyses cannot be directly leveraged to guide the design of network architectures.

\citet{mellor2020neural} recently proposed a NAS framework that does not involve training, which shares the same motivation with us towards training-free architecture search at initialization. They empirically find that the correlation between sample-wise input-output Jacobian can indicate the architecture's test performance. However, why does the Jacobian work is not well explained and demonstrated. Their search performance on NAS-Bench-201 is still left behind by the state-of-the-art NAS works, and they did not extend to DARTs space.

Meanwhile, we see the evolving development of \textbf{deep learning theory} on neural networks.
NTK (neural tangent kernel) is proposed to characterize the gradient descent training dynamics of infinite wide \citep{jacot2018neural} or finite wide deep networks \citep{hanin2019finite}. Wide networks are also proved to evolve as linear models under gradient descent \citep{lee2019wide}. This is further leveraged to decouple the trainability and generalization of networks \citep{xiao2019disentangling}. Besides, a natural measure of ReLU network's expressivity is the number of linear regions it can separate in its input space \citep{raghu2017expressive,montufar2017notes,serra2018bounding,hanin2019complexity,hanin2019deep,xiong2020number}.
In our work, we for the first time discover two important indicators that can effectively rank architectures, thus bridging the theoretic findings and real-world NAS applications. Instead of claiming the two indicators we discover are the best, we believe there are more meaningful properties of deep networks that can benefit the architecture search process. We leave them as open questions and encourage the community to study.




\section{Methods}

The core motivation of our TE-NAS framework is to achieve architecture evaluation without involving any training, to significantly accelerate the NAS search process and reduce the search cost. In section \ref{sec:train_express} we present our study on two important indicators that reflect the trainability and expressivity of a neural network, and in section \ref{sec:pruning_search} we design a novel pruning-based method that can achieve a superior trade-off between the two indicators.

\subsection{Analyzing Trainability and Expressivity of Deep Networks}\label{sec:train_express}
Trainability and expressivity are distinct notions regarding a neural network \citep{xiao2019disentangling}. A network can achieve high performance only if the function it can represent is complex enough and at the same time, it can be effectively trained by gradient descent.


\subsubsection{Trainability by Condition Number of NTK}

The trainability of a neural network indicates how effective it can be optimized using gradient descent \citep{burkholz2019initialization,hayou2019impact,shin2020trainability}. Although some heavy networks can theoretically represent complex functions, they not necessarily can be effectively trained by gradient descent. One typical example is that, even with a much more number of parameters, Vgg networks \citep{simonyan2014very} usually perform worse and require more special engineering tricks compared with ResNet family \citep{he2016deep}, whose superior trainability property is studied by \citet{yang2017mean}.

Recent work~\citep{jacot2018neural,lee2019wide,chizat2019lazy} studied the gradient descent training of neural networks using a quantity called the neural tangent kernel (NTK).
The finite width NTK is defined by , where  is the Jacobian evaluated at a point  for parameter , and  is the output of the -th neuron in the last output layer .

\citet{lee2019wide} further proves that wide neural networks evolve as linear models using gradient descent, and their training dynamics is controlled by ODEs that can be solved as

\normalsize
for training data. Here  is the expected outputs of an infinitely wide network.  denotes the NTK between the training inputs, and  and  are drawn from the training set .
As the training step  tends to infinity we can see that Eq. \ref{eq:fc_ntk_recap_dynamics} reduce to .

The relationship between the conditioning of  and the trainability of networks is studied by \citet{xiao2019disentangling}, and we brief the conclusion as below. We can write Eq. \ref{eq:fc_ntk_recap_dynamics} in terms of the spectrum of :


\begin{wrapfigure}{r}{63mm}
\vspace{-1em}
\includegraphics[scale=0.25]{images/ntk_cond_together.pdf}
\centering
\vspace{-2em}
\caption{Condition number of NTK  exhibits negative correlation with the test accuracy of architectures in NAS-Bench201 \citep{dong2020bench}.}
\label{fig:ntk_201}
\vspace{-0.5em}
\end{wrapfigure}

where  are the eigenvalues of  and we order the eigenvalues .
As it has been hypothesized by \citet{lee2019wide} that the maximum feasible learning rate scales as , plugging this scaling for  into Eq. \ref{eq:fc_ntk_dynamics_eigen} we see that the  will converge exponentially at a rate given by , where  is the condition number. Then we can conclude that if the  of the NTK associated with a neural network diverges then it will become untrainable, so we use  as a metric for trainability:

 is calculated without any gradient descent or label.
Figure \ref{fig:ntk_201} demonstrates that the  is negatively correlated with the architecture's test accuracy, with the Kendall-tau correlation as . Therefore, minimizing the  during the search will encourage the discovery of architectures with high performance.\vspace{-0.5em}

\subsubsection{Expressivity by Number of Linear Regions}\vspace{-0.5em}

\begin{wrapfigure}{r}{37mm}
\vspace{-1.5em}
\centering     \includegraphics[scale=0.35]{images/linear_regions_example.pdf}
\vspace{-1em}
\caption{Example of linear regions divided by a ReLU network\protect\footnote[1] .}
\vspace{-1.5em}
\label{fig:linear_regions_example}
\end{wrapfigure}\footnotetext[1]{Plot is generated by us with the same method described by \citet{hanin2019complexity}.}

The expressivity of a neural network indicates how complex the function it can represent \citep{hornik1989multilayer,giryes2016deep}.
For ReLU networks, each ReLU function defines a linear boundary and divides its input space into two regions. Since the composition of piecewise linear functions is still piecewise linear, every ReLU network can be seen as a piecewise linear function. The input space of a ReLU network can be partitioned into distinct pieces (i.e. linear regions) (Figure \ref{fig:linear_regions_example}), each of which is associated with a set of affine parameters, and the function represented by the network is affine when restricted to each piece. Therefore, it is natural to measure the expressivity of a ReLU network with the number of linear regions it can separate.

Following \citet{raghu2017expressive,serra2018bounding,hanin2019complexity,hanin2019deep,xiong2020number}, we introduce the definition of activation patterns and linear regions for ReLU networks.

\textbf{Definition 1. Activation Patterns and Linear Regions (\citet{xiong2020number}) }\label{def:activation-regions}
\textit{
Let  be a ReLU CNN. 
An activation pattern of  is a function  from the set of neurons to , i.e., for each neuron  in , we have .   
Let  be a fixed set of parameters (weights and biases) in , and  be an activation pattern. The region corresponding to  and  is

where  is the pre-activation of a neuron .
Let  denote the number of linear regions of  at , i.e., 

}
\begin{wrapfigure}{r}{55mm}
\vspace{-2em}
\includegraphics[scale=0.28]{images/Linear_Region.pdf}
\centering
\vspace{-2em}
\caption{Number of linear regions  of architectures in NAS-Bench201 exhibits positive correlation with test accuracies.}
\label{fig:linear_regions_201}
\vspace{-2em}
\end{wrapfigure}

\vspace{-2em}
Eq. \ref{eq:linear_region} tells us that a linear region in the input space is a set of input data  that satisfies a certain fixed activation pattern , and therefore the number of linear regions  measures how many unique activation patterns that can be divided by the network.

In our work, we repeat the measurement of the number of linear regions by sampling network parameters from the Kaiming Norm Initialization \citep{he2015delving}, and calculate the average as the approximation to its expectation:
\begin{wrapfigure}{l}{.5\textwidth}

\end{wrapfigure}

\begin{wrapfigure}{r}{60mm}
\vspace{-1em}
\includegraphics[scale=0.38]{images/op_preference.pdf}
\centering \vspace{-1em}
\caption{ and  prefer different operators in NAS-Bench201.}
\label{fig:op_preference}
\vspace{-2em}
\end{wrapfigure}

\vspace{-0.5em}

We iterate through all architectures in NAS-Bench-201 \citep{dong2020bench}, and calculate their numbers of linear regions (without any gradient descent or label). Figure \ref{fig:linear_regions_201} demonstrates that the number of linear regions is positively correlated with the architecture's test accuracy, with the Kendall-tau correlation as . Therefore, maximizing the number of linear regions during the search will also encourage the discovery of architectures with high performance.

Finally, in Figure \ref{fig:op_preference} we analyze the operator composition of top 10\% architecture by maximizing  and minimizing , respectively. We can clearly see that  and  have different preferences for choosing operators. They both choose a large ratio of conv for high generalization performance. But meanwhile,  heavily selects conv, and  leads to skip-connect, favoring the gradient flow.

\vspace{-0.5em}

\subsection{Pruning-by-importance Architecture Search}\label{sec:pruning_search}

\vspace{-0.5em}

Given the strong correlation between the architecture's test accuracy and its  and , how to build an efficient NAS framework on top of them? We motivate this section by addressing two questions:

\vspace{-0.5em}
\textit{1. How to combine  and  together, with a good explicit trade-off?}

We first need to turn the two measurements  and  into one combined function, based on which we can rank architectures. As seen in Figure \ref{fig:ntk_201} and \ref{fig:linear_regions_201}, the magnitudes of  and  differ much. To avoid one overwhelming the other numerically, one possible remedy is normalization; but we cannot pre-know the ranges nor the value distributions of  and , before computing them over a search space. In order to make our combined function \textit{well defined before search} and \textit{agnostic to the search space}, instead of using the numerical values of  and , we could refer to their relative rankings. Specifically, each time by comparing the sampled set of architectures peer-to-peer, we can directly sum up the two relative rankings of  and  as the selection criterion. The equal-weight summation treats trainability and expressivity with the same importance conceptually\footnote{We tried some weighted summations of the two, and find their equal-weight summation to perform the best.} and delivers the best empirical result: we thus choose it as our default combined function. We also tried some other means to combine the two, and the ablation studies can be found in Appendix \ref{sec:appendix_prune_option}.







\vspace{-0.5em}
\textit{2. How to search more efficiently?}

Sampling-based methods like reinforcement learning or evolution can use rankings as the reward or filtering metric. However, they are inefficient, especially for complex cell-based search space. Consider a network stacked by repeated cells (directed acyclic graphs) \citep{zoph2018learning,liu2018darts}. Each cell has  edges, and on each edge we only select one operator out of  ( is the set of operator candidates). There are  unique cells, and for sampling-based methods,  networks have to be sampled during the search. The ratio  can be interpreted as the sampling efficiency: a method with small  can find good architectures faster. However, the search time cost of sampling-based methods still scales up with the size of the search space, i.e., .

Inspired by recent works on pruning-from-scratch \citep{lee2018snip,wang2020picking}, 
we propose a pruning-by-importance NAS mechanism to quickly shrink the search possibilities and boost the efficiency further, reducing the cost from  to .
Specifically, we start the search with a super-network  composed of all possible operators and edges. In the outer loop, for every round we prune one operator on each edge. The outer-loop stops when the current supernet  is a single-path network\footnote{Different search spaces may have different criteria for the single-path network. In NAS-Bench201 \citep{dong2020bench} each edge only keeps one operator at the end of the search, while in DARTS space \citep{liu2018darts} there are two operators on each edge in the searched network.}, i.e., the algorithm will return us the final searched architecture. For the inner-loop, we measure the change of  and  before and after pruning each individual operator, and assess its importance using the sum of two ranks. We order all currently available operators in terms of their importance, and prune the lowest-importance operator on each edge.


The whole pruning process is extremely fast. As we will demonstrate later, our approach is principled and can be applied to different spaces without making any modifications.
This pruning-by-importance mechanism may also be extended to indicators beyond   and .
We summarize our training-free and pruning-based NAS framework, TE-NAS, in Algorithm \ref{algo:pruning}. 


\vspace{-1em}
\begin{algorithm2e}[ht!]
\footnotesize
\textbf{Input:} supernet  stacked by cells, each cell has  edges, each edge has  operators, step . \\
	\While { is not a single-path network}{
	    \For {each operator  in } {
	         \Comment{the \underline{higher}  the more likely we will prune }\\
	         \Comment{the \underline{lower}  the more likely we will prune }\\
	    }
	    Get importance by :  index of  in \underline{descendingly} sorted list \\
	    Get importance by :  index of  in \underline{ascendingly} sorted list \\
	    Get importance \\
	    \\
	    \For {each edge , } {
	         \Comment{find the operator with \underline{greatest importance} on each edge.}\\
	        
	    }
	    
	}
	\Return{Pruned single-path network .}
	\caption{TE-NAS: Training-free Pruning-based NAS via Ranking of  and .}
	\label{algo:pruning}
\end{algorithm2e}
\vspace{-1em}


\subsubsection{Visualization of Search Process}


\begin{wrapfigure}{r}{65mm}
\vspace{-6em}
\begin{center}
	\label{fig:prune_201}\includegraphics[width=50mm]{images/pruning_process_201.pdf}\GPU sec.)}} & \textbf{\tabincell{c}{Search\\Method}} \\ \midrule
    ResNet \citep{he2016deep} & 93.97 & 70.86 & 43.63 & - & - \\ \midrule
    RSPS \citep{li2020random} &  &  &  & 8007.13 & random \\
    
    ENAS \citep{pham2018efficient} &  &  &  & 13314.51 & RL \\
    DARTS (1st) \citep{liu2018darts} &  &  &  & 10889.87 & gradient \\
    DARTS (2nd) \citep{liu2018darts} &  &  &  & 29901.67 & gradient \\
    GDAS \citep{dong2019searching} &  &  &  & 28925.91 & gradient \\ \midrule
    NAS w.o. Training \citep{mellor2020neural} &  &  &  & 4.8 & training-free \\
    TE-NAS (ours) &  &  &  & 1558 & training-free \\ \midrule
    \textbf{Optimal} & 94.37 & 73.51 & 47.31 & - & - \\
    \bottomrule
    \end{tabular}\label{table:nasbench201}}
    \vspace{-2mm}
\end{table}
\vspace{-0.5em}

We run TE-NAS for four independent times with different random seeds, and report the mean and standard deviation in Table \ref{table:nasbench201}. We can see that TE-NAS achieves the best accuracy on all three datasets, and largely reduces the search cost ( reduction).
Although \citet{mellor2020neural} requires even less search time (by only sampling 25 architectures), they suffer from much inferior accuracy performance, with notably larger deviations across different search rounds.


\vspace{-0.5em}
\subsection{Results on CIFAR-10 with DARTs Search Space}\label{sec:cifar10}

\begin{table}[b!]
    \vspace{-1em}
    \centering
    \caption{Comparison with state-of-the-art NAS methods on CIFAR-10.}
    \vspace{-1em}
    \scriptsize
    \begin{threeparttable}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Architecture} & \textbf{\tabincell{c}{Test Error\M)}} & \textbf{\tabincell{c}{Search Cost\M)}}} &
    \multirow{2}*{\textbf{\tabincell{c}{Search Cost\GPU days)}}} & 
    \multirow{2}*{\textbf{\tabincell{c}{Search\\Method}}} \\ \cline{2-3}
    
    & top-1 & top-5 & & & \\ \midrule
    
    
    NASNet-A \citep{zoph2018learning} & 26.0 & 8.4 & 5.3 & 2000 & RL \\
    AmoebaNet-C \citep{real2019regularized} & 24.3 & 7.6 & 6.4 & 3150 & evolution \\
    PNAS \citep{liu2018progressive} & 25.8 & 8.1 & 5.1 & 225 & SMBO \\
    MnasNet-92 \citep{tan2019mnasnet} & 25.2 & 8.0 & 4.4 & - & RL \\ \midrule
    
    DARTS (2nd) \citep{liu2018darts} & 26.7 & 8.7 & 4.7 & 4.0 & gradient \\
    SNAS (mild) \citep{xie2018snas} & 27.3 & 9.2 & 4.3 & 1.5 & gradient \\
    GDAS \citep{dong2019searching} & 26.0 & 8.5 & 5.3 & 0.21 & gradient \\
    BayesNAS \citep{zhou2019bayesnas} & 26.5 & 8.9 & 3.9 & 0.2 & gradient \\
    P-DARTS (CIFAR-10) \citep{chen2019progressive} & 24.4 & 7.4 & 4.9 & 0.3 & gradient \\ 
    P-DARTS (CIFAR-100) \citep{chen2019progressive} & 24.7 & 7.5 & 5.1 & 0.3 & gradient \\ 
    PC-DARTS (CIFAR-10) \citep{xu2019pc} & 25.1 & 7.8 & 5.3 & 0.1 & gradient \\
    
    TE-NAS (ours) & 26.2 & 8.3 & 6.3 & 0.05 & training-free \\ \midrule
    PC-DARTS (ImageNet) \citep{xu2019pc}\tnote{} & 24.2 & 7.3 & 5.3 & 3.8 & gradient \\
    ProxylessNAS (GPU) \citep{cai2018proxylessnas}\tnote{} & 24.9 & 7.5 & 7.1 & 8.3 & gradient \\
    TE-NAS (ours)\tnote{} & 24.5 & 7.5 & 5.4 & 0.17 & training-free \\ \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \item[] The architecture is searched on ImageNet, otherwise it is searched on CIFAR-10 or CIFAR-100.
    \end{tablenotes}
    \end{threeparttable}
    \label{tab:imagenet}
    \vspace{-2.5mm}
\end{table}

\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
The key questions in Neural Architecture Search (NAS) are ``what are good architectures'' and ``how to find them''. Validation loss or accuracy are possible answers but not enough, due to their search bias and heavy evaluation cost. Our work demonstrates that two theoretically inspired indicators, the spectrum of NTK and the number of linear regions, not only strongly correlate with the network's performance, but also benefit the reduced search cost and decoupled analysis of the network's trainability and expressivity. Without involving any training, our TE-NAS achieve competitive NAS performance with minimum search time. We for the first time bridge the gap between the theoretic findings of deep neural networks and real-world NAS applications, and we encourage the community to further explore more meaningful network properties so that we will have a better understanding of good architectures and how to search them.

\vspace{-0.5em}
\section*{Acknowledgement}
\vspace{-0.5em}
This work is supported in part by the NSF Real-Time Machine Learning program (Award Number: 2053279), and the US Army Research Office
Young Investigator Award (W911NF2010240).





























































































































\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}


\newpage

\appendix

\section{Implementation Details}\label{sec:implementations}

For  we sample one mini-batch of size 32 from the training set, and calculate . For  we sample 5000 images, forward them through the network, and collect the activation patterns from all ReLU layers. The calculation of both  and  are repeated three times in all experiments, where each time the network weights are randomly drawn from Kaiming Norm Initialization \citep{he2015delving} without involving any training (network weights are fixed).

Our retraining settings (after search) follow previous works \citep{xu2019pc,chen2019progressive,chen2020stabilizing}. On CIFAR-10, we train the searched network with cutout regularization of length 16, drop-path \citep{zoph2018learning} with probability as 0.3, and an auxiliary tower of weight 0.4.
On ImageNet, we also use label smoothing during training.
On both CIFAR-10 and ImageNet, the network is optimized by an SGD optimizer with cosine annealing, with learning rate initialized as 0.025 and 0.5, respectively.

\section{Searched Architecture}

\label{app:vis}
We visualize the searched normal and reduction cells in figure \ref{fig:TE_cifar10} and \ref{fig:TE_imagenet}, which is directly searched on CIFAR-10 and ImageNet respectively.

\begin{figure}[!htb]
\centering
\subfigure[Normal Cell]{\includegraphics[width=0.45\linewidth]{images/cifar_normal.pdf}}
\hfill
\subfigure[Reduction Cell]{\includegraphics[width=0.45\linewidth]{images/cifar_reduce.pdf}}
\caption{Normal and Reduction cells discovered by TE-NAS on CIFAR-10.}
\label{fig:TE_cifar10}
\end{figure}

\begin{figure}[!htb]
\centering
\subfigure[Normal Cell]{\includegraphics[width=0.45\linewidth]{images/imagenet_normal.pdf}}
\hfill
\subfigure[Reduction Cell]{\includegraphics[width=0.45\linewidth]{images/imagenet_reduce.pdf}}
\caption{Normal and Reduction cells discovered by TE-NAS on imageNet.}
\label{fig:TE_imagenet}
\end{figure}


\section{Depth and Width Preference of  and  in DARTs Space}

To analyze the impact of different architectures on trainability and expressivity in DARTs search space, we visualize  and  with different depths and width. Following \citet{shu2019understanding},  
the depth of a cell is defined as the number of connections on the longest path from input nodes to the output node, and the width of a cell is the summation of the edges of the intermediate nodes that are connected to the input nodes. We randomly sample 20,000 architectures in DARTs space, and plot the visualizations in Figure \ref{fig:depth_width_201}. Good architectures should exhibit low  (good trainability, blue dots in Figure \ref{fig:depth_width_ntk_201}) and high  (powerful expressivity, red dots in Figure \ref{fig:depth_width_linear_regions_201}). Therefore,  and  tell us that in DARTs space shallow but wide cells are preferred to favor both trainability and expressivity. This conclusion matches the findings by \citet{shu2019understanding}: existing NAS algorithms tend to favor architectures with wide and shallow cell structures, which enjoy fast convergence with smooth loss landscape and accurate gradient information.

\begin{figure}[h!]
\centering
\subfigure[]{\label{fig:depth_width_ntk_201}\includegraphics[width=0.49\linewidth]{images/NTK_darts.pdf}}
\subfigure[]{\label{fig:depth_width_linear_regions_201}\includegraphics[width=0.49\linewidth]{images/linear_regions_darts.pdf}}
\caption{Depth and width preference of (a)  and (b)  on DARTs Search Space.}
\label{fig:depth_width_201}
\end{figure}

\section{More Ablation Studies}
\subsection{Search with only  or }\label{sec:appendix_indicator_option}

As we observed in Table \ref{table:search_only_k_R}, searching with only  or  leads to inferior performance, which indicates the importance of maintaining both trainability and expressivity during the search.

\begin{table}[h!]
\caption{Search with only  or  on CIFAR-100 in NAS-Bench-201 space.}
\centering
\footnotesize
\begin{tabular}{cc}
\toprule
Methods & CIFAR-100 Test Accuracy \\ \midrule
Prune with only  & 69.25 (1.29) \\
Prune with only  & 70.48 (0.29) \\
TE-NAS & \textbf{71.24} (0.56) \\ \bottomrule
\end{tabular}
\label{table:search_only_k_R}
\end{table}


\subsection{Different combination options for  and }\label{sec:appendix_prune_option}

Pruning by  is not the only option (see Algorithm \ref{algo:pruning}). Here in this study we consider more:
\begin{enumerate}
    \item[1)] pruning by , i.e., pruning by the worst case.
    \item[2)] Pruninng by , i.e., pruning by the best case.
    \item[3)] pruning by summation of changes , i.e., directly use the numerical values of the changes.
\end{enumerate}

As we observed in Table \ref{table:prune_options}, our TE-NAS stands out of all options. This means a good trade-off between  and  are important, and also the ranking strategy is better than directly using numerical values.

\begin{table}[h!]
\caption{Search with only  or  on CIFAR-100 in NAS-Bench-201 space.}
\centering
\footnotesize
\begin{tabular}{cc}
\toprule
Methods & CIFAR-100 Test Accuracy \\ \midrule
Prune by  & 70.75 (0.73) \\
Prune by  & 70.33 (1.09) \\
Prune by  & 70.47 (0.68) \\
Prune by  (TE-NAS) & \textbf{71.24} (0.56) \\ \bottomrule
\end{tabular}
\label{table:prune_options}
\end{table}

\newpage

\subsection{Correlation between Test Accuracy and Combination of  and }

\begin{wrapfigure}{r}{80mm}
\vspace{-2.5em}
\includegraphics[scale=0.3]{images/NTK_Linear_Region_correlation.pdf}
\centering
\vspace{-1em}
\caption{Summation of ranking of  and  exhibits stronger (negative) correlation with the test accuracy of architectures in NAS-Bench201 \citep{dong2020bench}.}
\label{fig:combine_ntk_lr_201}
\vspace{-4em}
\end{wrapfigure}

Figure~\ref{fig:combine_ntk_lr_201} indicates that by using the summation of the ranking of both and , the combined metric achieves a much higher correlation with the test accuracy. The reason can be explained by Figure~\ref{fig:op_preference}, as  and  prefers different operators in terms of trainability and Expressivity. Their combination can filter out bad architectures in both aspects and strongly correlate with networks' final performance.
\vspace{4em}

\section{Generalization v.s. Test Accuracy}

\begin{wrapfigure}{r}{78mm}
\vspace{-2.5em}
\includegraphics[scale=0.3]{images/Test_vs_Train.pdf}
\centering
\vspace{-1em}
\caption{The correlation between test accuracy and the training accuracy in NAS-Bench201 \citep{dong2020bench}.}
\label{fig:test_vs_train}
\vspace{-1em}
\end{wrapfigure}


Conceptually, the generalization gap is the difference between a model’s performance on training data and its performance on unseen data drawn from the same distribution (e.g., testing set). In comparison, the two indicators  (trainability) and  (expressiveness) of a network determine how well the training set could be fit (i.e., training set accuracy), and do not directly indicate its generalization gap (or test set accuracy). Indeed, probing generalization of an untrained network at its initialization is a daunting, open challenge that seems to go beyond the current theory scope. 

In NAS, we are searching for the architecture with the best test accuracy. As shown in Figure~\ref{fig:test_vs_train}, in NAS-Bench201 the training accuracy strongly correlates with test accuracy. This also seems to be a result of the current standard search space design that could have implicitly excluded severe overfitting. This explains why  and , which only focuses on trainability and expressiveness during training, can still achieve good search results of test accuracy.


\end{document}
