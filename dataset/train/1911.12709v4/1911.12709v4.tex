We extensively evaluate our \imageSGD{} and \datasetSGD{} methods on several standard datasets as well as on aerial and medical images. These correspond to increasingly challenging adaptation scenarios.

\para{Adaptation scenarios.}
We first consider \textit{distribution shift}, where the training and test image sets come from the same general domain, consumer photos, but differ in their image and object statistics (Sec.~\ref{sec:in_domain_adaptation}).
This includes differences in image complexity, object size distribution, and when the test set contains object classes absent during training.
Then, we consider a \textit{\classspecialisation{}} scenario, where a sequence of objects of a single class has to be iteratively segmented 
(Sec.~\ref{sec:class_adaptation}).
Finally we test how our method handles large \textit{domain changes} where the imaging modality changes between training and testing. We demonstrate this by going from consumer photos to aerial and medical images (Sec.~\ref{sec:out_domain_adaptation}).

\para{Model Details.}  We use a strong re-implementation of~\cite{mahadevan18bmvc} as our interactive segmentation model (Sec.~\ref{sec:base_network}). We pre-train its parameters on \pascal{}~\cite{pascal-voc-2012} augmented with SBD~\cite{hariharan11iccv} (10582 images with 24125 segmented instances of 20 object classes).
As a baseline, we use this model as in~\cite{mahadevan18bmvc},~\ie without updating its parameters at test time.
We call this the \emph{\fixedmodellong}. This baseline already achieves state-of-the-art results on the PASCAL VOC12 validation set, simply by increasing the encoder resolution compared to~\cite{mahadevan18bmvc}
(3.44 clicks).
This shows that using a fixed set of model parameters works well when the train and test distributions match. 
We evaluate our proposed method by adapting the parameters of that same model at test time using \emph{single image adaptation} (IA), \emph{image sequence adaptation} (SA), and their combination (IA + SA).

\para{Evaluation metrics.}
We use
two standard metrics~\cite{xu16cvpr,liew17iccv,benard17arxiv,mahadevan18bmvc,li18cvpr,benenson19cvpr,jang19cvpr}:
(1) \textbf{\iouAtK{}}, the average intersection-over-union between the \gt{} and predicted segmentation masks, given $k$ corrections per image,
and (2) \textbf{\clicksAtIOU{}}, the average number of corrections needed to reach an IoU of $q\%$ on every image (thresholded at 20 clicks).
We always report mean performance over 10 runs (standard deviation is negligible at $\approx0.01$ for \clicksAtIOU{}).

\para{Hyperparameter selection.}
We optimize the hyperparameters for both adaptation methods on a subset of the \ade{} dataset \cite{zhou17cvpr,zhou18ijcv}.
Hence, the hyperparameters are optimized for adapting from \pascal{} to~\ade{}, which is distinct from the distribution shifts and domain changes we evaluate on.

\para{Implementation Details} are provided in the supplementary material.

\begin{table}[t]
	\centering
	\setlength{\tabcolsep}{3pt}
    \caption{\textbf{Adapting to distribution shifts.} 
	Mean number of clicks required to attain a particular mIoU score on Berkeley, YouTube-VOS and COCO datasets (Lower is better).
    Both of our adaptive methods, \imageSGD{}~(\imageadaptationshort{}) and \datasetSGD{}~(\sequenceadaptationshort{}) improve over the model that keeps the weights \fixedmodel{} at test time.
}
	\label{tab:distribution_adaptation}
	\resizebox{0.90\linewidth}{!}{\setlength{\tabcolsep}{6pt}	
	\begin{tabular}{|l|c|c|@{}C{1.4cm}@{}C{1.4cm}@{}C{1.4cm}|}
		\toprule
& \textbf{\berkeley{} } & \textbf{\youtubeVOS}& \multicolumn{3}{c|}{\textbf{COCO~\cite{lin14eccv}}} \\
& \multicolumn{1}{c|}{\cite{mcguinness10book}} & \multicolumn{1}{c|}{\cite{xu18arxiv}} & seen & unseen & unseen 6k\\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		Method   & \clicksAt{90}\ & \clicksAt{85} &  \multicolumn{3}{c|}{\clicksAt{85}} \\
		\hline
\uc \fixedmodellong{}~\cite{mahadevan18bmvc} & 5.4 & 7.9 & 10.0 & 11.9 & 13.2 \\ \arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
\imageadaptationshort{}  & 4.9& 7.0 & 9.1 & 10.7 & 10.6 \\
\sequenceadaptationshort{}  & 5.3 & 6.9  & 9.7 & 10.6 &  10.0\\
\combinedshort{} & \textbf{4.9} & \textbf{6.7}  & \textbf{9.1} & \textbf{9.9}    & \textbf{9.3} \\
        \arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
        $\Delta$ over \fixedmodellong{} 
		& \color{green!40!black}{8.5\%} & \color{green!40!black}{15.2\%}  & \color{green!40!black}{9.0\%} & \color{green!40!black}{16.8\%}    &\color{green!40!black}{29.5\%} \\
   		\bottomrule
	\end{tabular}
	}
\end{table}


\subsec{Adapting to distribution shift}
\label{sec:in_domain_adaptation}
We test how well we can adapt the model which is trained on \pascal{} to other consumer photos datasets.

\para{Datasets.}
We test on: (1) \emph{\berkeley{}}~\cite{mcguinness10book}, 100 images with a single foreground object.
(2) \emph{\youtubeVOS{}}~\cite{xu18arxiv}, a large video object segmentation dataset.
We use the test set of the 2019 challenge, where we take the first frame with ground truth (1169 objects, downscaled to $855\times480$ maximal resolution).
(3) \emph{\coco{}}~\cite{lin14eccv}, a large segmentation dataset with 80 object classes.
20 of those overlap with the ones in the \pascal{} dataset and are thus \emph{seen} during training. The other 60 are \emph{unseen}.
We sample 10 objects
per class from the validation set and separately report results for seen (200 objects) and unseen classes (600 objects) as in~\cite{xu16cvpr,majumder19cvpr}.
We also study how \datasetSGD{} behaves on longer sequences of 100 objects for each unseen class (named {\em \cocounseenlarge{}}).

\para{Results.}
We report our results in Tab.~\ref{tab:distribution_adaptation} and Fig.~\ref{fig:curve_collection_distribution}.
Both types of adaptation
improve performance on all tested datasets.
On the first few user corrections \textit{\imageSGD{}}~(\imageadaptationshort{}) performs similarly to the \fixedmodellong{} as it is initialized with the same parameters. But as more corrections are provided, it uses these more effectively to adapt its appearance model to a specific image.
Thus, it performs particularly well in the high-click regime,
which is most useful for objects that are challenging to segment (\eg due to low illumination, Fig.~\ref{fig:distribution_adaptations}),
or when~very~accurate~masks~are~desired.

\setlength{\intextsep}{2pt}\setlength{\columnsep}{10pt}\begin{wrapfigure}{r}{0.48\linewidth}
    \begin{subfigure}[t]{\linewidth}
    \centering    
    \begin{overpic}[trim={0 0cm 0cm 0.5cm},clip,width=0.95\linewidth]{figures/qual.pdf}
    \put(150,900){\makebox(0,-20){\textbf{\uc \fixedmodel{}}}}
    \put(500,900){\makebox(0,-20){\textbf{SA + IA}}}
    \put(850,900){\makebox(0,-20){\textbf{GT}}}
    \put(55,580){\makebox(0,-10){\color{white}\textbf{\scriptsize k=1}}\color{black}}
    \put(395,580){\makebox(0,-10){\color{white}\textbf{\scriptsize k=1}}\color{black}}
    \put(70,300){\makebox(0,-10){\color{white}\textbf{\scriptsize k=10}}\color{black}}
    \put(400,300){\makebox(0,-10){\color{white}\textbf{\scriptsize k=4}}\color{black}}
    \put(55,30){\makebox(0,-10){\color{white}\textbf{\scriptsize k=5}}\color{black}}
    \put(400,30){\makebox(0,-10){\color{white}\textbf{\scriptsize k=4}}\color{black}}
    \label{fig:coco_unseen_hard}
    \end{overpic}
    \end{subfigure}    
    \caption{\textbf{Qualitative results} of the frozen and our combined adaptation model.
     Red circles are negative clicks and green ones are positive.
     Green and red areas respectively show the pixels that turned to FG/BG with the latest clicks.
     Our method produces accurate masks with fewer clicks \textbf{k}.
     }
    \label{fig:distribution_adaptations}
\end{wrapfigure}
During \textit{\datasetSGD{}}~(\sequenceadaptationshort{}), the model adapts to the test image distribution and thus learns to produce good segmentation masks given just a few clicks~(Fig.~\ref{fig:coco_unseen_6k_iou_at_k}).
As a result, \sequenceadaptationshort{} outperforms using a \fixedmodellong{} on all datasets with distribution shifts (Tab.~\ref{tab:distribution_adaptation}). By adapting from images to the video frames of \youtubeVOS{}, \sequenceadaptationshort{} reduces the clicks needed to reach 85\% IoU by \att{15\%}.
Importantly, we find that our method adapts fast, making a real difference after just a few images, and then keeps on improving even as the test sequence becomes thousands of images long (Fig.~\ref{fig:coco_unseen_6k_over_time}).
This translates to a large improvement given a fixed budget of 4 clicks per object: on the \cocounseenlarge{} split it achieves \att{69\%} IoU compared to the \att{57\%} of the \fixedmodellong{} (Fig.~\ref{fig:coco_unseen_6k_iou_at_k}).

Generally, the curves for \datasetSGD{} grow faster in the low click regime than the \imageSGD{} ones, but then exhibit stronger diminishing returns in the higher click regime (Fig.~\ref{fig:coco_unseen_6k_iou_at_k}).
Hence, combining the two compounds their advantages
leading to a method that considerably improves over the \fixedmodellong{} on the full range of number of corrections and sequence lengths (Fig.~\ref{fig:coco_unseen_6k_iou_at_k}).
Compared to the \fixedmodel{} model, our combined method significantly reduces the number of clicks needed to reach the target accuracy on all datasets: from a \att{9\%} reduction on Berkeley and COCO seen, to a \att{30\%} reduction on COCO unseen 6k.

\subsection{Adapting to a specific class}
\label{sec:class_adaptation}
When a user segments objects of a single class at test-time, \datasetSGD{} naturally specializes its appearance model to that class.
We evaluate this phenomenon on 4 \coco{} classes.
We form 4 test image sequences, each focusing on a single class, containing objects of varied appearance. The classes are selected based on how \datasetSGD{} performs compared to the \fixedmodellong{} in Sec.~\ref{sec:in_domain_adaptation}.
We selected the following classes, with increasing order of difficulty for \datasetSGD{}:
(1) donut (2540 objects)
(2) bench (3500)
(3) umbrella (3979)
and (4) bed (1450). 

\para{Results.}
Tab.~\ref{tab:class_adaptation_comparison}, Fig.~\ref{fig:coco_unseen_donut_clicks_plot}  present results.
The class specialization brought by our image sequence adaptation~(\sequenceadaptationshort{}) leads to good masks from very few clicks.
For example, on the donut class it reduces \clicksAt{85} by \att{39\%} compared to the \fixedmodellong{} and by \att{44\%} when combined with \imageSGD{} (Tab.~\ref{tab:class_adaptation_comparison}).
Given just 2 clicks, \sequenceadaptationshort{} reaches \att{66\%} \iou{} for that class, compared to \att{25\%} \iou{} for the \fixedmodellong{} (Fig.~\ref{fig:coco_unseen_donut_clicks_plot}). The results for the other classes follow a similar pattern,
showing that \datasetSGD{} learns an effective appearance model for a single class.

\begin{figure}[t]
    \centering
    ~
   \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=1\linewidth]{plots/coco_unseen_large.pdf}
    \caption{Mean \iouAtK{} for varying $k$ on \cocounseenlarge{}.
    Both forms of adaptation significantly improve over a frozen model.
    }
    \label{fig:coco_unseen_6k_iou_at_k}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=1\linewidth]{plots/coco_unseen_6k_improvement_over_time.pdf}
    \caption{\iouAt{4} clicks as a function of the number of images processed.
    \sequenceadaptationshort{} quickly improves over the model with \fixedmodel{} weights.
    }
    \label{fig:coco_unseen_6k_over_time}
    \end{subfigure}
    ~    
    \begin{subfigure}[t]{0.3\textwidth}
    \includegraphics[width=1\linewidth]{plots/coco_unseen_donut_plots.pdf}
    \caption{
    \iouAtK{} for varying $k$ when specializing to \textit{donuts}.	
    \sequenceadaptationshort{} offers large gains
    by learning a class specific appearance model.
    }
    \label{fig:coco_unseen_donut_clicks_plot}
    \end{subfigure}
    \caption{\textbf{Results for adapting to dist. shift (a,b) or a specific class (c).}
    } 
    \label{fig:curve_collection_distribution}
\end{figure}
\begin{figure}[t!]   
    \begin{subfigure}[t]{0.50\textwidth}
    \includegraphics[width=0.49\linewidth]{plots/drions_db_plots.pdf}
    ~
    \includegraphics[width=0.47\linewidth]{plots/medical_improvement_over_time.pdf}
    \caption{\medical{} dataset.
    }
    \label{fig:drions_sequence_adaptation}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.50\textwidth}
    \includegraphics[width=0.49\linewidth]{plots/rooftop_plots.pdf}
    ~
    \includegraphics[width=0.47\linewidth]{plots/rooftop_improvement_over_time.pdf}
    \caption{\rooftop{} dataset.
    }
    \label{fig:rooftop_improvement_smoothed}
    \end{subfigure}
    \caption{\textbf{Results for domain change.}
    For each dataset, we show the mean IoU at $k$ corrections (left in \ref{fig:drions_sequence_adaptation}, \ref{fig:rooftop_improvement_smoothed}) and the number of clicks to reach the target IoU as a function of the number of images processed (right in \ref{fig:drions_sequence_adaptation}, \ref{fig:rooftop_improvement_smoothed}).
    \uc \imageSGD{} provides a consistent improvement over the test sequences.
    Instead, \datasetSGD{} adapts its appearance model to the new domain gradually, improving with every image processed (right in \ref{fig:drions_sequence_adaptation}, \ref{fig:rooftop_improvement_smoothed}).
    }
    \label{fig:domain_adapt}

\end{figure}
\raggedbottom
\begin{table}[b]
	\centering
    \begin{minipage}{0.95\linewidth}
	\centering
	\caption{\textbf{Class specialization.} We test segmenting objects of only one specific class.
	Our adaptive methods outperforms the \fixedmodel{} model on all tested classes.
	Naturally, gains are larger for \datasetSGD{}, as it can adapt to the class over time.
}
	\label{tab:class_adaptation_comparison}
	\resizebox{0.8\linewidth}{!}{\setlength{\tabcolsep}{6pt}
	\begin{tabular}{|p{3cm}r|c|c|c|}
		\toprule
        & \multicolumn{4}{c|}{\textbf{clicks @ 85\%~\iou{}}} \\
   & \multicolumn{1}{c|}{\textbf{Donut}} & \multicolumn{1}{c|}{\textbf{Bench}} & \multicolumn{1}{c|}{\textbf{Umbrella}} & \multicolumn{1}{c|}{\textbf{Bed}} \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\uc \fixedmodellong{}~\cite{mahadevan18bmvc} & 11.6 & 15.1 & 13.1 & 6.8 \\ \arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\imageadaptationshort{}~(Ours)          & 9.2 &   14.1 & 11.9 & 5.5 \\
		\sequenceadaptationshort{}~(Ours) & 7.1 & 14.0 & 11.1 & 5.5 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
        \combinedshort{}~(Ours) &  \textbf{6.5} & \textbf{13.3} & \textbf{10.2} & \textbf{5.0} \\
        \arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
        $\Delta$ over \fixedmodellong{} &  \color{green!40!black}{44.0\%} & \color{green!40!black}{11.9\%} & \color{green!40!black}{22.1\%} & \color{green!40!black}{26.5}\% \\
        \bottomrule

	\end{tabular}
	}
	\end{minipage}
\end{table}
\begin{table}[t]
	\centering
\begin{minipage}{0.90\linewidth}
	\centering
		\caption{\textbf{Domain change results}. We evaluate our model on 2 datasets that belong to different domains: aerial~(Rooftop) and medical~(DRIONS-DB). Both types of adaptation (\imageadaptationshort{} and \sequenceadaptationshort{})
		outperform the \fixedmodel{} model.}
	\label{tab:domain_adaptation_results}
	\resizebox{0.80\linewidth}{!}{\setlength{\tabcolsep}{6pt}
	\begin{tabular}{|p{3cm}cc|}
		\toprule
		& \textbf{DRIONS-DB~\cite{carmona08aim}} & \textbf{Rooftop~\cite{sun14eccv}} \\
		\textbf{Method}  & \clicksAt{90}~\iou{}  & \clicksAt{80}~\iou{}\\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\uc \fixedmodellong{}~\cite{mahadevan18bmvc} & 13.3 & 8.9  \\ \arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\imageadaptationshort{}~(Ours) & 11.4 & 6.3\\
		\sequenceadaptationshort{}~(Ours) &  3.6 & 3.6 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\combinedshort{}~(Ours) & \textbf{3.1}  & \textbf{3.6}  \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
        $\Delta$ over \fixedmodellong{} &  \color{green!40!black}{76.7\%} & \color{green!40!black}{59.6\%}  \\
		\bottomrule
	\end{tabular}
	}
	\end{minipage}
\end{table}
\subsec{Adapting to domain changes}
\label{sec:out_domain_adaptation}
We test our method's ability of adapting to domain changes by training on consumer photos (\pascal{}) and evaluating on aerial and medical imagery.  

\para{Datasets.} We explore two test datasets:
(1) \textit{\rooftop{}}~\cite{sun14eccv}, a dataset of 65 aerial images with segmented rooftops and 
(2) \textit{\medical}~\cite{carmona08aim}, a dataset of 110 retinal images with a segmentation of the optic disc of the eye fundus.
(we use the masks of the first expert). 
Importantly, the initial model parameters $\vec{\theta^*}$ were optimized for the \pascal{} dataset, which consists of consumer photos. Hence, we explore truly large domain changes here.

\para{Results.}
Both our forms of adaptation significantly improve over the \fixedmodellong{} (Tab.~\ref{tab:domain_adaptation_results}, Fig.~\ref{fig:domain_adapt}). 
\uc \imageSGD{} can only adapt to a limited extent, as it independently adapts to each object instance, always starting from the same initial model parameters $\vec{\theta^*}$.
Nonetheless, it offers a significant improvement, reducing the number of clicks needed to reach the desired IoU by \att{14\%-29\%}.
\uc \datasetSGD{}~(\sequenceadaptationshort{}) shows extremely strong performance, as its adaptation effects accumulate over the duration of the test sequence.
It reduces the needed user input by \att{60\%} for the \rooftop{} dataset and by over \att{70\%} for \medical{}.
When combining the two types of adaptation, the reduction increases to \att{77\%} for the \medical{} dataset (Tab.~\ref{tab:domain_adaptation_results}).
Importantly, our method adapts fast: on \medical{}
\clicksAt{90}
drops quickly and converges to just \att{2} corrections, as the length of the test sequence increases (Fig.~\ref{fig:drions_sequence_adaptation}). 
In contrast, the \fixedmodellong{} performs poorly on both datasets.
On the \rooftop{} dataset, it needs even more clicks than there are points in the ground truth polygons (\att{8.9} \vs \att{5.1}). This shows that even a state-of-the-art model like~\cite{mahadevan18bmvc} fails to generalize to truly different domains and highlights the importance of adaptation.

To summarize: We show that our method can bridge large domain changes spanning varied datasets and sequence lengths.
With just a single gradient descent step per image, our \datasetSGD{} successfully addresses a major shortcoming of neural networks, for the case of interactive segmentation: Their poor generalization to changing distributions~\cite{recht18arxiv,alcorn19cvpr}.
\subsec{Comparison to Previous Methods}
\label{sec:stoa_experiments_comparison}
\begin{table*}[t]
	\centering
	\caption{
The focus of our work is handling distribution shifts and domain changes between training and testing (Tab.~\ref{tab:distribution_adaptation},~\ref{tab:class_adaptation_comparison} \& \ref{tab:domain_adaptation_results}).
For completeness, we also compare our method against existing methods on standard datasets, where the distribution mismatch between training and testing is small.
At the time of initially releasing our work~\cite{kontogianni19arxiv}, our method outperformed all previous state-of-the-art models on all datasets. Later, {F-BRS}~\cite{sofiiuk20cvpr} (CVPR 2020) achieved even  better results.
}
	\label{tab:stoa_comparison}
	\resizebox{0.8\linewidth}{!}{\setlength{\tabcolsep}{2.5pt}
	\begin{tabular}{|p{5.2cm}|c|c|c|c|ccc|}
		\toprule
		& \textbf{VOC12~\cite{pascal-voc-2012}}  & \textbf{GrabCut~\cite{Rother04-tdfixed}} & \textbf{Berkeley~\cite{mcguinness10book}}& \multicolumn{1}{c|}{\textbf{DAVIS~\cite{perazzi16cvpr}}}\\
& \multicolumn{1}{c|}{validation} & & & 10\% of frames\\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\textbf{Method} & \clicksAt{85} & \clicksAt{90} & \clicksAt{90} & \clicksAt{85}\\
		\hline
		iFCN w/ GraphCut~\cite{xu16cvpr} & 6.88 & 6.04 & 8.65 &-\\
		RIS~\cite{liew17iccv} & 5.12 & 5.00 & 6.03 & - & \\		
		TSLFN~\cite{hu19nn} & 4.58 & 3.76 & 6.49&- & \\
		VOS-Wild~\cite{benard17arxiv} & 5.6\hspace{1.4mm} & 3.8\hspace{1.4mm} & - & - \\		
		ITIS~\cite{mahadevan18bmvc} & 3.80 & 5.60 & -  & -& \\
		CAG~\cite{majumder19cvpr} &  3.62 & 3.58 & 5.60&- & \\
		Latent Diversity~\cite{li18cvpr} & - & 4.79& -&   5.95 \\		
		BRS~\cite{jang19cvpr} & - & 3.60 & 5.08 & 5.58  \\ \arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		F-BRS~\cite{sofiiuk20cvpr} (Concurrent Work) & - & \color{white!30!black}{2.72} & \color{white!30!black}{4.57} & \color{white!30!black}{5.04}  \\ \hline
	    \arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		\combinedshort{} combined~(Ours) &  \textbf{3.18} & \textbf{3.07}& \textbf{4.94}& \textbf{5.16} \\
		\bottomrule
	\end{tabular}
	}
\end{table*}

While the main focus of our work is tackling challenging adaptation scenarios, we also compare our method
against state-of-the-art interactive segmentation methods on standard datasets.
These datasets are typically similar to \pascal{}, hence have a small distribution mismatch between training and testing.


\para{Datasets.}
(1) \berkeley{}, introduced in Sec.~\ref{sec:in_domain_adaptation}
(2) \textit{\grabcut{}}~\cite{Rother04-tdfixed}, 49 images with segmentation masks.
(3) \textit{\davis{}}~\cite{perazzi16cvpr}, 50 high-resolution videos out of which we sample 10\% of the frames uniformly at random as in~\cite{li18cvpr,jang19cvpr} (We note that the standard evaluation protocol of \davis{} favors adaptive methods, as the same objects appear repeatedly in the test sequence.) and 
(4) \textit{\pascal{} validation}, with 1449 images.


\para{Results.}
Tab.~\ref{tab:stoa_comparison} shows results. Our adaptation method achieves strong results:
At the time of initially releasing our work~\cite{kontogianni19arxiv}, it outperformed all previous state-of-the-art methods on all datasets (it was later overtaken by~\cite{sofiiuk20cvpr}).
It brings improvements even when the previous methods (which have \fixedmodel{} model parameters) already offers strong performance and need less than 4 clicks on average (\pascal{}, \grabcut{}). The improvement on \pascal{} further shows that our method helps even when the training and testing distributions match exactly (the \fixedmodellong{} needs \att{3.44} clicks).

Importantly, we find that our method outperforms~\cite{li18cvpr,jang19cvpr}, even though we use a standard segmentation backbone~\cite{chen18eccv} which predicts at $\frac{1}{4}$ of the input resolution. Instead~\cite{li18cvpr,jang19cvpr} propose specialized network architectures in order to predict at full image resolution, which is crucial for their good performance~\cite{jang19cvpr}.
We note that our adaptation method is orthogonal to these architectural optimizations and can be combined with them easily.

\subsec{Ablation Study}
\label{sec:ablation}
We ablate the benefit of treating corrections as training examples (on \cocounseenlarge{}).
For this, we selectively remove them from the loss (Eq.~\eqref{eq:adapt-loss}).
For \imageSGD{}, this leads to a parameter update that makes the model more confident in its current prediction, but this does not improve the segmentation masks. Instead, training on corrections improves \clicksAt{85} from \att{13.2} to \att{10.6}.
For \datasetSGD{}, switching off the corrections corresponds to treating the predicted mask as ground-truth and updating the model with it.
This approach implicitly contains corrections in the mask and thus improves ~\clicksAt{85} from \att{13.2} for the \fixedmodellong{} to \att{11.9}.
Explicitly using correction offers an additional gain of almost \att{2} clicks, down to \att{10}.
This shows that treating user corrections as training examples is key to our method:
They are necessary for \imageSGD{} and highly beneficial for \datasetSGD{}.

\subsection{Adaptation speed}
\begin{wrapfigure}{r}{0.38\linewidth}
    \centering
    \includegraphics[trim={0cm 0.35cm 0cm 0.2cm},clip,width=1\linewidth]{plots/iterations_improvement_over_frozen.pdf}
    \caption{Iterations \vs relative improvement over a \fixedmodellong{} (mean over all datasets).}
    \label{fig:adaptation_iterations}
\end{wrapfigure}
While our method updates the parameters at test time, it remains fast enough for interactive usage.
For the model used throughout our paper a parameter update step takes 0.16\,s (Nvidia V100 GPU, mixed-precision training, \berkeley{} dataset).
\uc\datasetSGD{} only needs a single update step, done \emph{after} an object is segmented (Sec.~\ref{sec:datasetSGD}).
Thus, the adaptation overhead is negligible here.
For single image adaptation we used 10 update steps, 
for a total time of 1.6\,s.
We chose this number of steps based on hyperparameter search (see supp. material).
In practice, fewer update steps can be used to increase speed,
as they quickly show diminishing returns (Fig.~\ref{fig:adaptation_iterations}). 
We recommend to use 3 update steps, reducing adaptation time to $0.5$\,s, with a negligible effect on the number of corrections required (average difference of less than \att{1\%}, over all datasets).
\newline
\hspace{\parindent}
To increase speed further, the following optimizations are possible:
(1) Using a faster backbone,~\eg with a ResNet-50~\cite{he15arxiv}, the time for an update step reduces to 0.06\,s;
(2) Using faster accelerators such as Google Cloud TPUs;
(3) Employing a fixed feature extractor and only updating a light-weight segmentation head~\cite{li18cvpr}.

