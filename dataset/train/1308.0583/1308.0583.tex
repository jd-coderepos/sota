\section{Experimental Results}
\label{sec:experiments}

In this  section, we describe two experiments  conducted to  evaluate the performance of \TS.
This section is structured as follows. In Subsection~\ref{subsec:rand_alg}, we describe an
 algorithm of random test generation that we compared with \TS. Some details of the implementation of \TS~we used
in experiments are given in Subsection~\ref{subsec:impl_tap_seq}. The first and second experiments
are described in Subsections~\ref{subsec:first_exper} and~\ref{subsec:second_exper} respectively.
\subsection{Random Algorithm We Used in Experiments}
\label{subsec:rand_alg}

\setlength{\intextsep}{10pt}
\setlength{\textfloatsep}{10pt}
\begin{figure}
\small
\begin{tabbing}
aaa\=bb\=cc\= dd \= eeeeee \= \kill
\Rnd\{\\
\tb{\scriptsize{1}}\> if () return(); \\
\tb{\scriptsize{2}}\> ; \\
\tb{\scriptsize{3}} \> ; ; \\
\tb{\scriptsize{4}} \> while () \{\\
\tb{\scriptsize{5}} \Tt if () \{ \\
\tb{\scriptsize{6}} \ttt  ; ; \\
\tb{\scriptsize{7}} \ttt ; \\
\tb{\scriptsize{8}} \ttt continue;\} \\
\tb{\scriptsize{9}}\Tt;\\
\tb{\scriptsize{10}} \Tt if  return(\ti{bug}); \\
\tb{\scriptsize{11}} \Tt ; \\
\tb{\scriptsize{12}}\Tt ; \\
\tb{\scriptsize{13}}\Tt ; \} \\
\tb{\scriptsize{14}} \> return(\ti{no\_bug\_found}); \} \\
\end{tabbing} 
\vspace{-10pt}
\caption{\ti{Algorithm for generation of  counterexamples randomly}}
\label{fig:rand_alg}
\end{figure}
 In this  subsection, we describe an algorithm of random test generation we used in the first experiment.
We will refer to this algorithm as \Rnd. 
The pseudocode of \Rnd~is shown in Figure~\ref{fig:rand_alg}.
The set of counterexamples generated by \Rnd~is controlled
by parameters \ti{max\_tries} and \ti{max\_length}. The value of \ti{max\_tries} limits the number
of generated  counterexamples while \ti{max\_length} sets the limit to the number of states
in a counterexample. The length of the current counterexample and the number of counterexamples 
generated so far are specified by variables \ti{length} and \ti{tries} respectively.

\Rnd~maintains variable \Curr~specifying a state reachable from the initial state
that is  currently processed by \Rnd. At the beginning, \Curr~is set to the initial state (line 2).
The main work is done in the 'while' loop (lines 4-13). If the value of \ti{length} 
exceeds \ti{max\_length}, a new counterexample is started  and the value
of \ti{tries} is incremented (lines 5-8). Otherwise, \Rnd~checks if \Curr~satisfies property .
If not, then  \Rnd~returns value \ti{bug}.
Otherwise, \Rnd~randomly generates an assignment
\pnt{x} to input variables  (line 11). Then \pnt{x} is used to generate a new state
that is the state to which the circuit switches from state \Curr~under input assignment \pnt{x}
(line 12). After that, the length of the current counterexample is incremented and a new iteration begins.
\subsection{Implementation of \TS}
\label{subsec:impl_tap_seq}
In the pseudocode of \TS~given in Figure~\ref{fig:tap_seq}, we did not clarify in what order states were extracted
from \Acts~in the 'while' loop. The two extremes are  depth-first and breadth-first orders. The depth-first order is to
 first process the state of \Acts~the is the farthest
from the initial state (in terms of transitions).  On the contrary, the breadth-first order, is to first process the state that is the
closest to the initial state. In the breadth-first variant of \TS, states are processed  one time frame after another. We assume here
that -th time frame consists of the states of \All~that can be reached from the initial state in  transitions.
That is, in the breadth-first variant, a state of \Acts~of -th time frame is processed only after every state
of every -th time frame where  has been processed and removed from \Acts.
Obviously, by imposing a particular order of extracting states from \Acts~one can also have modifications of \TS~ that are different
from the two extremes above. \ti{In this paper, we report results of  a breadth-first implementation of \TS}.


In the experiments, we ran two versions of \TS: randomized and non-randomized. The difference between these versions is in finding
boundary points used to encode proofs. In the randomized version, the internal SAT-solver called  to find boundary points
had some randomization in its decision making. Namely, the phase of every 10-th decision assignment was chosen randomly. 
The reason for such randomization is explained in Subsection~\ref{subsec:first_exper}.
\subsection{First Experiment: Comparison of \TS~with \Rnd}
\label{subsec:first_exper}
The objective of the first experiment was to compare \TS~with \Rnd.
In this comparison we used 314 buggy benchmarks of the HWMCC-10 competition.
 78 benchmarks of this set were trivial: the initial state did not satisfy the property to be verified. 
We excluded them from consideration.
The results of the  experiment on non-trivial benchmarks are summarized in Table~\ref{tbl:rand_vers_tap}.

\setlength{\abovecaptionskip}{2pt}   \setlength{\belowcaptionskip}{5pt}   \setlength{\intextsep}{1pt}
\setlength{\floatsep}{1pt}
\begin{table}[htb]
\caption{\ti{Solving non-trivial buggy HWMCC-10 benchmarks}. \ti{Maximum number of visited states  is limited to 1,000,000 for} \Rnd~\ti{and 40,000 for} \TS}
\vspace{-10pt}
\scriptsize
\begin{center}
\begin{tabular}{|p{30pt}|p{26pt}|p{22pt}|p{23pt}|p{22pt}|p{23pt}|l|} \hline
 \Ss{number of}  & \Rnd & \multicolumn{2}{|p{45pt}|}{\TS} & \multicolumn{2}{|p{45pt}|}{\TS} & \TS\\
 \Ss{benchmarks}  & \Ss{solved} & \multicolumn{2}{|p{45pt}|}{\Ss{unrandomized}} & \multicolumn{2}{|p{51pt}|}{\Ss{randomized}} & \Ss{total}\\
\cline{3-4}\cline{5-6} 
 &   & \Ss{solved.} & \Ss{converg.}     &\Ss{solved}  & \Ss{converg.}    & \Ss{solved} \\  \hline
\Ss{236} & \tb{43}   & 35   & 94  & 59 & 8 & \tb{69} \\ \hline
\end{tabular}
\label{tbl:rand_vers_tap}
\end{center}
\end{table}
 
The first column of Table~\ref{tbl:rand_vers_tap} shows the number of non-trivial benchmarks used in the first experiment.
The second column gives the number of benchmarks solved by \Rnd. The parameters  and  of \Rnd~were
set to 10,000 and 100 respectively. That is \Rnd~generated up to 10,000 counterexamples of length 100.
(So the total number of visited states was limited by 1,000,000. 
The counterexample length of 100  was large enough to solve
any benchmark solved by \TS.) For every benchmark, the time limit  for \Rnd~was set to 900 seconds. 

The next four columns show results of unrandomized and randomized versions of \TS.   For both versions,
the number of visited states (i.e. the size of \All) was limited by 40,000 and the time limit was set to 180 seconds.
For either version, we report the number of solved benchmarks and the number of benchmarks where a convergence point
was reached. (Recall that a convergence point is reached by \TS~when the set \Acts~becomes empty before a bug is found.)
 The last column gives the number of benchmarks
solved by at least one version of \TS.


The results of Table~\ref{tbl:rand_vers_tap} show that for many benchmarks the unrandomized version of \TS~  
reached a convergence point. This means that 
\TS, in its current form, needs some way to escape early convergence. In this experiment, we achieved this goal
by randomizing \TS~as described in Subsection~\ref{subsec:impl_tap_seq}. The randomized version
of \TS~solved more benchmarks and reached a convergence point only for 8 benchmarks. 
Overall,  the experiment showed that \TS~outperformed \Rnd~solving more benchmarks (69 versus 43) with much stricter limit on
the number of visited states. 

\subsection{Second Experiment: Bounded Model Checking and \TS}
\label{subsec:second_exper}
The objective of the second experiment was to show that some benchmarks solved by \TS~were 
hard for  Bounded Model Checking (BMC)~\cite{bmc}. 
In this experiment, we used a BMC tool built on top  of  the Aiger package~\cite{aiger} and Picosat~\cite{picosat}, a well-known SAT-algorithm.
In general, BMC is good at detecting shallow bugs but struggles to find deeper bugs even if these bugs are easy to detect.
This point is illustrated by results of the second experiment shown in Table~\ref{tbl:bmc_versus_tap}.  Notice that we do not claim
that the current implementation of \TS~outperforms BMC. The latter performed extremely well on shallow benchmarks
of the set we used in the first experiment  while \TS~could not solve many of them. We just
want to emphasize the promise of \TS~in finding deep bugs.

\setlength{\abovecaptionskip}{2pt}   \setlength{\belowcaptionskip}{5pt}   \setlength{\intextsep}{1pt}
\setlength{\floatsep}{1pt}
\begin{table}[htb]
\caption{\ti{Some benchmarks that are hard for BMC and easy for \TS}}
\vspace{-10pt}
\scriptsize
\begin{center}
\begin{tabular}{|p{49pt}|p{24pt}|p{30pt}|p{24pt}|p{30pt}|} \hline
 \Ss{benchmarks}  &  \multicolumn{2}{|p{54pt}|}{\ti{BMC}} & \multicolumn{2}{|p{54pt}|}{\TS} \\
\cline{2-3}\cline{4-5} 
  & \Ss{time (s.)} & \Ss{cex length}     &\Ss{time (s.)}  & \Ss{cex length}    \\  \hline
pdtswvroz10x6p0 & 118  & 58  & 1.2  & 88 \\ \hline
pdtswvsam6x8p0  & 116 & 48  & 7.7  &  48\\ \hline
pdtswvtma6x6p0  &95  & 57  & 0.8  & 57  \\ \hline
pdtswvtma6x4p0  & 70  & 57  & 0.9  & 57  \\ \hline
pdtswvroz8x8p0  & 65  & 48  & 1.1  & 72 \\ \hline
visbakery       & 925 & 59  & 28  & 61 \\ \hline
\end{tabular}
\label{tbl:bmc_versus_tap}
\end{center}
\end{table}
 
The first column of Table~\ref{tbl:bmc_versus_tap} gives benchmark names.
The next two columns show the time taken by the BMC tool to find a counterexample and the length of this counterexample.
The last two columns provide the same information for \TS.
The examples of Table~\ref{tbl:bmc_versus_tap} have the largest counterexample length among the benchmarks solved by \TS.
These are also the examples (among those solved by \TS) where the BMC tool had the longest run time. \TS~significantly
outperforms the BMC tool on these examples. Interestingly,
the first five benchmarks were also easy for \Rnd~(but \Rnd~failed to solve the 'visbakery' benchmark).
