\documentclass[11pt]{article}
\usepackage{latexsym, amssymb, amsmath, graphicx, amsthm, url}
\usepackage[boxed,noline,linesnumbered,noend]{algorithm2e}
\usepackage[top=1in,right=1in,left=1in,bottom=1in]{geometry}
\usepackage{cite}
\usepackage{enumitem}

\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{defn}{Definition}
\newtheorem{prob}{Problem}

\newcommand{\one}{{\bf 1}}
\newcommand{\tbar}[1]{\mbox{\sbox0{#1}\sbox2{\~{}}\ooalign{\hidewidth\raise\dimexpr\ht0-\ht2+.3ex\box2 \hidewidth\cr#1\cr}}}

\title{Universal Streaming}

\author{Vladimir Braverman
\thanks{
Department of Computer Science, Johns Hopkins University.
Email:
{\tt vova@cs.jhu.edu}.  Research supported in part by
DARPA grant N660001-1-2-4014. Its contents are solely the responsibility of the author and do not
represent the official view of DARPA or the Department of Defense.}
\and
Rafail Ostrovsky\thanks{
Department of Computer Science and Department of Mathematics, University of California, Los Angeles.
Email: {\tt rafail@cs.ucla.edu}.  Research supported in part by NSF grants CNS-0830803; CCF-0916574; IIS-1065276; 
CCF-1016540; CNS-1118126; CNS-1136174; US-Israel BSF grant 2008411, OKAWA Foundation Research Award, IBM 
Faculty Research Award, Xerox Faculty Research Award, B. John Garrick Foundation Award, Teradata 
Research Award, and Lockheed-Martin Corporation Research Award. This material is also based upon 
work supported by the Defense Advanced Research Projects Agency through the U.S. Office of Naval 
Research under Contract N00014-11-1-0392. The views expressed are those of the author and do 
not reflect the official policy or position of the Department of Defense or the U.S. Government.}
\and
Alan Roytman\thanks{
Department of Computer Science, University of California, Los Angeles.
Email: {\tt alanr@cs.ucla.edu}.}
}
\date{}

\begin{document}

\maketitle
\thispagestyle{empty}
\begin{abstract}
Given a stream of data, a typical approach in streaming algorithms is to design a sophisticated algorithm
with small memory that computes a specific statistic over the streaming data.  Usually, if one wants
to compute a different statistic {\em after} the stream is gone, it is impossible. But
what if we want to compute a different statistic after the fact?  In this paper, we consider the
following fascinating possibility: can we collect some small amount of {\em specific} data
during the stream that is ``universal,'' i.e., where we do not know anything about the statistics we
will want to {\em later} compute, other than the guarantee that had we known the statistic ahead of time,
it would have been possible to do so with small memory?  In other words, is it possible to collect
some data in small space during the stream, such that any other statistic that can be computed with comparable
space can be computed after the fact?  This is indeed what we introduce (and show) in this paper
with matching upper and lower bounds: we show that it is possible to collect {\em universal}
statistics of polylogarithmic size, and prove that these universal statistics allow us {\em after the fact}
to compute all other statistics that are computable with similar amounts of memory. We show that this is
indeed possible, both for the standard unbounded streaming model and the sliding window streaming model.
\end{abstract}

\section{Introduction}
With the vast amount of data being generated today, algorithms for data streams continue
to play an important role for many practical applications.
As the amount of data being generated
continues to grow at a staggering rate, streaming algorithms are increasingly becoming more important as a practical tool
to analyze and make sense of all the information.
Data streams have received a lot of attention
with good reason, as evidenced by the wide array of applications discussed in~\cite{A07,M05}.
Applications for streaming algorithms
which operate over input that arrives on the fly and use a small
amount of memory
are numerous, ranging from monitoring packets flowing across a network to analyzing
patterns in DNA sequences.  In practice, such applications generate vast amounts of data in a very short
period of time, so it is infeasible to store all of this information.
This presents a pressing
question: when is it possible to avoid storing all the information while still providing approximate
solutions with good theoretical guarantees?

Typically, algorithms are developed for data streams in the unbounded model, where some
statistic is maintained over the entire history of the stream.  For certain applications, it is useful
to only compute such statistics over recent data.  For instance, we may wish to analyze
stock market transactions in a particular timeframe~\cite{OMMGM02} or monitor packets
transmitted over a network in the last hour to identify suspicious activity~\cite{VSGB05}.
This framework is known as the sliding window model, where we maintain statistics
over the current window of size at most , which slides as time progresses.  In the sequence-based model,
exactly one element arrives and expires from the window per time step.
In the timestamp-based model, any number of elements may arrive or expire.
Clearly, the timestamp-based model is more general.

In a landmark paper that influenced the field as a whole, the work of Alon, Matias and Szegedy~\cite{AMS96}
studied the following fundamental framework.
For a universe  and an input stream (i.e., a sequence of integers drawn from ),
let  be the vector where each entry  denotes the frequency
with which element  appears in the stream.  At any point in time, the paper of~\cite{AMS96}
showed how to approximate various frequency moments in sublinear space.  Informally, for the  frequency
moment , it was shown that  and  can be approximated in polylogarithmic space,
while for , an upper bound of  was shown (the notation  hides polylogarithmic factors).
Moreover, a lower bound of  was shown for every .  As discussed in~\cite{AMS96},
such frequency functions are tremendously important in practice and have many applications in databases, as
they indicate the degree to which the data is skewed.
The fundamental work of Indyk and Woodruff~\cite{IW05} showed how to compute  for  in
space , which was the first optimal result for such frequency moments.
Their technique reduced the problem of computing  to computing heavy hitters, and indeed
our construction builds on their methods.
Their result was later improved up to polylogarithmic
factors by Bhuvanagiri, Ganguly, Kesh and Saha~\cite{BGKS06}.
Recently, Li, Nguy\tbar{\^e}n, and Woodruff~\cite{LNW14} showed that any one-pass streaming
algorithm that approximates an arbitrary function in the turnstile model can be implemented
via linear sketches.  Our work is related, since our algorithms are based on linear sketches
of~\cite{AMS96}.

Such works have opened a line of research
that is still extremely relevant today.  In particular,
what other types of frequency-based functions
admit efficient solutions in the streaming setting, and which functions are inherently difficult to approximate?
In our paper, we strive to answer this question for frequency-based, monotonically increasing
functions in the sliding window model.  We make progress on two significant, open problems
outlined in~\cite{sub30} by Nelson and~\cite{sub20} by Sohler.  Specifically, we are the first
to formalize the notion of universality for streaming over sliding windows.  Our main result is
the construction of a universal algorithm in the timestamp-based sliding window model for a broad class of
functions.  That is, we define a class of functions and design a single
streaming algorithm that produces a data structure with the following guarantee.  When querying
the data structure with a function  taken from the class, our algorithm approximates 
without knowing  in advance (here,  denotes the frequency that element  appears in the window).
Our data structure only collects statistics based on
the input stream itself without knowing .  The algorithm uses polylogarithmic memory in the
universe size  and the window size , and obtains a -approximation.
This is precisely the notion of universality that we develop in our paper, and
it is an important step forward towards resolving the problem in~\cite{sub30}.

Along the way, we design a zero-one law for a broader class of
monotonically increasing functions  which are zero at the origin that specifies when
 can be approximated with high probability in one pass, using polylogarithmic memory.
If  satisfies the conditions specified by the test, then given the function  we construct an
explicit, general algorithm that is able to approximate the summation to within a -factor
using polylogarithmic memory.  If the function  does not pass the test, then we provide a lower
bound which proves it is impossible to do so.
This result generalizes the work of~\cite{BO10} to the sliding window setting,
and makes important progress towards understanding the question posed in~\cite{sub20}.

\subsection{Contributions and Techniques}
Our contributions in this paper make progress on and give insight into two important problems:
\begin{enumerate}[itemsep=0mm]
\item We are the first to formally define the notion of universality in the streaming setting.  We define
a large class of functions  such that, for the entire class, we design a single, universal algorithm
for data streams in the sliding window model which maintains a data structure with the following guarantee.
When the data structure is queried with any function , it outputs a
-approximation of  without knowing  in advance (note
that the choice of  can change).  Our algorithm uses polylogarithmic memory (in  and ), makes one
pass over the stream, and succeeds with high probability.

\item We give a complete, algebraic characterization (i.e., a zero-one law) for the class of tractable functions over
sliding windows.  We define a broader set of functions 
such that, for any non-decreasing function  where , if , then we have an algorithm that gives
a -approximation to , uses polylogarithmic memory, makes one pass
over the stream, and succeeds with high probability.  Moreover, if ,
we give a lower bound which shows that super-polylogarithmic memory is necessary in order to approximate
 with high probability.  This generalizes the result of~\cite{BO10}
to the sliding window setting.
\end{enumerate}

Our algorithms work in the timestamp-based sliding window model and maintain the summation approximately for every window.
The value  can depend on  and , so that
the approximation improves as either parameter increases. Our construction
is very general, applying to many functions using the same techniques.
In particular, streaming algorithms tend to depend specifically on the function
to be approximated.  For instance, consider the specific algorithms for ~\cite{AMS96, I06},
~\cite{FM85, CDIM03, AMS96} and ~\cite{I06, L09, KNW10Soda} (where the 
norm is the  root of ).  The problems we study have been open
for several years, and our construction and proofs are
non-trivial.  Surprisingly, despite us using existing techniques, their solutions have remained
elusive.  The techniques we use allow us to avoid the strong pseudorandom
generator machinery developed by Nisan~\cite{N90}.  In fact, our construction only assumes
-wise independence, which might be useful in practice.

For our main result, item , it is useful to understand our techniques for solving
item .  Designing the correct characterization for ``tractable" functions is in itself a challenging task.
Indeed, one may think that the predicate from~\cite{BO10} is sufficient for designing an algorithm
in the sliding window model.  Unfortunately, this idea is difficult to carry through, and with good reason:
it turns out to be false!  Part of the novelty and difficulty of our techniques is the identification of an extra smoothing
assumption about the class of tractable functions over sliding windows.  If a function
does not satisfy our smoothing assumption, we show a super-polylogarithmic lower bound,
inspired by the proof of~\cite{DGIM02}.
We draw on the techniques of~\cite{BO10,BO13,IW05} for our positive result by first finding
heavy elements according to the function , and then reducing the sum problem to the heavy elements
problem.  Our work sheds light on the question posed in~\cite{sub20},
by exhibiting a strict separation result between the unbounded and sliding window models.

To obtain our main result, we observe that one can remove the assumption from our initial constructions
that  is given up front (so that all applications of  happen at the end of the window).  However,
some technical issues arise, as our construction relies on some parameters of  that stem from our
zero-one law.  To address these issues, we parameterize our class of functions  by a constant,
allowing us to build a single algorithm to handle the entire parameterized class.

\subsection{Related Work}
The paper of Braverman and Ostrovsky~\cite{BO10} is the most closely related to our paper.
We extend their result from the unbounded model to the timestamp-based sliding window model
(by formalizing a new characterization of tractable functions) and by designing a universal
algorithm for a large class of functions.  Our results build on~\cite{BO10,BO13,IW05}.

Approximating frequency moments and  norms is
well studied in the literature, as it has many applications,
and there are indeed a vast number of papers on the subject.  Compared to such works,
we make minimal assumptions and our results are extremely broad, as we design general algorithms that can not
only handle frequency moments, but other functions as well.
Flajolet and Martin~\cite{FM85} gave an algorithm to approximate 
(i.e., counting distinct elements),
and Alon, Matias, and Szegedy~\cite{AMS96} showed how to approximate  for
 using polylogarithmic memory, while for  they showed how to approximate
 using  memory.  They also showed an  lower bound for
.  Indyk~\cite{I06} used stable distributions to approximate  norms for
.  Indyk and Woodruff~\cite{IW05} gave the first optimal algorithm
for  (), where an  upper bound was developed.  In a
followup work, Bhuvanagiri, Ganguly, Kesh, and Saha~\cite{BGKS06} improved the space by
polylogarithmic factors.  For lower bounds, Bar-Yossef, Jayram, Kumar, and
Sivakumar~\cite{BJKS02} gave an  lower bound, which was
later improved to  by Chakrabarti, Khot, and Sun~\cite{CKS03} for any
streaming algorithm that makes one pass over the stream.  The literature is vast, and other
results for such functions
include~\cite{IW03,W04,BJKST02,CK04,CDIM03,FKSV99,G04,GC07,L09,KNW10Soda,KNW10Pods}.
There is also literature on finding frequent items, and this is indeed a problem we must solve
to achieve our results (although we must do so for a broad class of functions).
Examples of works which find frequent items include~\cite{CCF02,CH08,CM05ELS}.
Moreover, there has been a line of work in the literature
on estimating entropy and entropy norms, including~\cite{BG06,CBM06,CCM07,GMV06,HNO08,LSOXZ06}.

There is also a vast literature in streaming for sliding
windows.  In their foundational paper, Datar, Gionis, Indyk, and Motwani~\cite{DGIM02}
gave a general technique called exponential histograms that allows many fundamental
statistics to be computed in optimal space, including count, sum
of positive integers, average, and the  norm for .
Gibbons and Tirthapura~\cite{GT02} made improvements for the sum and count problem
with algorithms that are optimal in space and time.  Braverman and Ostrovsky~\cite{BO07}
gave a general framework for a large class of smooth functions, which include the  norm
for .  Our work complements their results, as
the functions they studied need not be frequency based.  Other problems include frequent itemsets~\cite{CWYM04}, frequency counts and
quantiles~\cite{AM04,LT06PODS}, rarity and similarity~\cite{DM02}, variance and -median~\cite{BDMO03},
diameter in multidimensional space and other geometric problems~\cite{FKZ05,CS04,AHV05}, and uniform
random sampling~\cite{BDM02}.  Many works have studied frequency estimation and frequent item identification,
including~\cite{GDDLM03,JQSYZ03,CM05TRANS}, along with -frequent elements,
including~\cite{HT08,ZG08,BAE07,HLT10,NL05}.  The recent work of~\cite{BGO13} gave an efficient
algorithm for computing -frequent elements over sliding windows.  Many of our constructions rely
on computing frequent elements, but we must do so under a broad class of functions.


\subsection{Roadmap}
In Section~\ref{sec:prob}, we describe notation used throughout this paper, give some definitions, and formalize the main
problems we study.  In Section~\ref{sec:lb}, we give a lower bound for functions that are not tractable (i.e., we show the ``zero"
part of our zero-one law).  In Section~\ref{sec:tract}, we give an algorithm for any tractable function (i.e., we show the ``one" part
of our zero-one law).  Finally, in Section~\ref{sec:universality}, we show the main result of this paper by giving a universal
streaming algorithm.

\section{Notation and Problem Definition}
\label{sec:prob}
We have a universe of  elements , where , and an integer .
A stream  is a (possibly infinite) sequence of integers , each from
the universe , where  is an upper bound on the size of the sliding window.  Specifically,
at each time step, there is a current window  that contains \emph{active} elements, where .
The window  contains the most recent elements of the stream,
and elements which no longer belong in the window are \emph{expired}.  We use the
timestamp-based model for sliding windows (i.e., any number of elements from the stream
may enter or leave the window at each time step).  We denote the frequency vector
by , where ) and each  is the frequency of element 
in window  (i.e., ).  For the
window , the  frequency moment .
For a vector , we let  be the -norm of , namely
.  For a vector  and a function , we define the
-Vector as .

We say that  is a -approximation of  if .  We define .  We say a probability  is negligible if .  Consider the following problem:

\begin{prob}[-Sum]
Let  be an arbitrary function which is non-decreasing such that .  For any stream
, any , and any , output a -approximation of  (where  is the multiplicity
of element ) for the current window .
\end{prob}

We first give some definitions which will be useful throughout the paper and help us define our notion
of tractability, beginning with the local jump:

\begin{defn}[Local Jump]
For  and , we define the local jump as

\end{defn}
\noindent That is,  is essentially the minimum amount needed to cause  to jump by a
-factor by shifting either to the left or to the right of .

\begin{defn}[Heavy Element]
For a vector , a function , and a parameter , we say that element  is -heavy with respect to  if
.
\end{defn}

\begin{defn}[Sampled Substream]
Let  be a stream and  be a function.  We denote by  the sampled substream of  consisting of
all elements that are mapped to 1 by the function .  More formally, .
\end{defn}

\begin{defn}[Residual Second Moment]
If there is an -heavy element  with respect to , we define the residual second moment as
.
\end{defn}


We are now ready to define our zero-one law.
\begin{defn}[Tractability]\label{defn:tractability}
We say a function  is tractable if  and:

\vspace{1mm}

\vspace{-4mm}

\vspace{-2mm}


\end{defn}
We let  be the set of functions which satisfy the above predicate.
Based on this definition, we formalize the notion of tractability for our universal setting.  It is similar to the definition of tractability,
except we need to upper bound some parameters by a constant.

\begin{defn}[Universal Tractability]\label{defn:utractability}
Fix a constant .  Let  denote the set of non-decreasing functions  where ,
, and:

\vspace{1mm}

\vspace{-4mm}

\vspace{-2mm}


\end{defn}

\begin{defn}[Universal Core Structure]\label{defn:ucs}
For a fixed vector , we say a data structure  is a universal core structure with
parameters , , , and a class of functions ,
where 
satisfies , if given any ,  outputs a set
 such that with probability at least  we have:
 For each , , and
 If there exists  such that  is -heavy with respect to , then
.
\end{defn}

\begin{defn}[Universal Core Algorithm]\label{defn:uca}
We say an algorithm  is a universal core algorithm with parameters , ,
,
and a class of functions , where 
satisfies , if, given any stream  as input,
 outputs a universal core structure for the vector  with the same parameters
, , , and~.
\end{defn}

\begin{defn}[Universal Sum Structure]\label{defn:uss}
For a fixed vector , we say a data structure  is a universal sum structure with
parameters , , and a class of functions , where 
satisfies , if given any ,  outputs a value 
such that with probability at least  we have:
.
\end{defn}

\begin{defn}[Universal Sum Algorithm]\label{defn:usa}
We say an algorithm  is a universal sum algorithm with parameters , ,
and a class of functions , where 
satisfies , if, given any stream  as input,
 outputs a universal sum structure for the vector  with parameters , ,
and~.
\end{defn}

In this paper, our main result is the proof of the following theorem:
\begin{thm}\label{thm:mainusum}
Fix a constant  and let  be the universally tractable set according to Definition~\ref{defn:utractability}.
There is a universal sum algorithm with parameters
 (for any ), ,
and .  The algorithm uses polylogarithmic space in  and , and
makes a single pass over the input stream .
\end{thm}

\noindent We can reduce the constant failure probability to inverse polynomial via standard methods.

Along the way, we also design a zero-one law (i.e., a test) which, given a function ,
determines if it is possible to solve the -Sum problem
using polylogarithmic space in  and  while making one pass over the stream .  If
 passes the test, we give an explicit algorithm which achieves a -approximation
except with negligible probability (making only one pass and using polylogarithmic memory).
To formalize our other main result, we define the following class {\sf STREAM-POLYLOG}:
\begin{defn}[{\sf STREAM-POLYLOG}]\label{def: strplg}
We say function  {\sf STREAM-POLYLOG} if ,  and
an algorithm  such that for any universe size , window size , ,
and stream :
  makes one pass over ,   uses  space, and 
For any window ,  maintains a -approximation of 
except with probability at most .
\end{defn}

\noindent Note that the constant error probability can be made to be as small as an inverse polynomial
by standard techniques.  Our other main result is the following theorem:
\begin{thm}\label{thm:zeroone}
Let  be a non-decreasing function such that :
.
\end{thm}

\section{Lower Bound for Sliding Windows}
\label{sec:lb}

In this section, we give a space lower bound for any non-tractable function .
We first show a deterministic lower bound for any algorithm that
approximates the -Sum problem.  Our technique is inspired by the lower bound proof in~\cite{DGIM02} for
estimating the number of 's for sliding windows.

\begin{thm}
Let  be a function such that .  Then, any deterministic algorithm that
solves the -Sum problem with relative error  (for some constant ) must
use space at least , where  is arbitrarily large.
\end{thm}

\begin{proof}
We construct a set of input streams such that, for any pair of data streams in the set, the algorithm must distinguish
between these two inputs at some point as the window slides.  Therefore, the space of the algorithm must be at least
logarithmic in the size of this set.

Since , in Definition~\ref{defn:tractability}, either
Predicate (\ref{eqn:tract1}) or (\ref{eqn:tract2}) does not hold.  If Predicate (\ref{eqn:tract1})
does not hold, then the lower bound from~\cite{BO10} applies and we are done.
Hence, we assume that
Predicate (\ref{eqn:tract2}) does not hold, implying:

Let  be given, and let  be arbitrarily large.  This negation implies that there are infinitely many
increasing points  and
corresponding values , where 
and .

Surprisingly, we will construct our lower bound with a universe of size , namely .  For each ,
we will construct a set of streams with a fixed, upper bounded window size of , and argue that the algorithm
must use memory at least  (note that, as the  are monotonically increasing, our lower bound will apply
for asymptotically large ).  We assume without loss of generality that
.  Our constructed streams will be as follows.
For each , note that our window
consists of elements which have arrived in the past  time steps.  For the first  time steps, we
construct many streams by choosing  of these time steps
(each choice defining a different stream).  For each chosen time step, we insert  's into
the stream, and for each time step that is not chosen, we insert zero elements.  For technical reasons, we pad
the last time step  in the first window with
 's.  Note that the
number of elements in the first window at time  is
.  We insert nothing at
time step .  For the remaining time steps , we simply repeat the first
 time steps of the stream (i.e., if time step  was chosen in the first  time steps,
, then we insert  's at time step ).

Now, we argue that for any such pair of constructed streams ,  which are different, any algorithm
with relative error smaller than  must distinguish between these two inputs.
To see this, consider the earliest time  when the two streams differ (note that ).
Let  be the window for stream  (and similarly  for stream ).
Let  be the number of chosen time steps in the first  time steps of stream .
Without loss of generality, we assume time step  was chosen in stream 
but not stream .  Hence, the number of chosen time steps in stream  up to time
 is .  Consider the windows at time step .  The number of elements in  at this time
is given by .  Moreover,
the number of elements in  is given by
.
Hence, the -Sum value at time  for  is .
As long as the algorithm has relative error , streams  and
 must be distinguished.

Thus, the algorithm's memory is lower bounded by the logarithm of the number of constructed streams, of which
there are  for each .  We have

If , we repeat the proof inserting two 's at each time step and the proof goes through.
Observing that  can be made arbitrarily large gives the proof.
\end{proof}

We now have a randomized lower bound by appealing to Yao's minimax principle~\cite{MR95} and building on top of our
deterministic lower bound, similarly to~\cite{DGIM02}.
\begin{thm}\label{thm:rlb}
Let  be a function where .  Then, any randomized algorithm that solves
-Sum with relative error smaller than  for some constant  and
succeeds with at least constant probability  must use memory , where 
is arbitrarily large.
\end{thm}


\section{An Algorithm for Tractable Functions}\label{sec:tract}
In this section, we complete the proof of Theorem~\ref{thm:zeroone} by first approximating heavy elements:
\begin{prob}[-Core]\label{prob:heavy}
We have a stream  and parameters~.  For each window , with probability at least ,
maintain a set  such that
 and there exists a set of indices  where 
for each .  If there is a -heavy element  with respect to , then .
\end{prob}

We begin solving the above problem with the following lemma (taken from~\cite{BO10}).

\begin{lem}\label{lem:majorseparation}
Let  be a vector with non-negative entries of dimension  and  be a pairwise independent random vector
of dimension  with entries  such that .   Denote
by  the vector with entries .  Let  be a constant, and let 
and .
If there is an -heavy element  with respect to , then:

If there is no -heavy element with respect to , then:

\end{lem}

We now prove some lemmas related to how approximating values can affect the function .

\begin{lem}\label{lem:Gapprox}
Let , and let  satisfy  and
, where .
Then .
\end{lem}
\begin{proof}
First, we note that  (recalling ).  We can
similarly get that .
Hence, we get that .

We conclude the proof by noting: .  Similarly, we get
.
\end{proof}


\begin{lem}\label{lem:Gapprox2}
Let  be such that , and let .  If
, then

\end{lem}

\begin{proof}
We have .
\end{proof}

We now give a useful subroutine over sliding windows which we will use in our main algorithm for
approximating heavy elements and prove its correctness
(there is a similar algorithm and proof in~\cite{BO10}, though it must be adapted to the sliding window setting).
\begin{algorithm}
\label{alg:res}
\DontPrintSemicolon
\For{ to } {
	\For{ to } {
		Generate a random hash function  with pairwise independent entries.\;
		Let  (i.e., , where  is the  entry of ). \;
		Consider the smooth histogram method for approximating  on sliding windows~\cite{BO07}. \;
		Let  be a -approximation of  on  (with negligible error probability). \;
		Let  be a -approximation of  on  (with negligible error probability). \;
		Let .
	}
	Let  (i.e.,  is the average of  independent 's).\;
}
Output  for the current window . \;
\caption{Residual-Approximation()}
\end{algorithm}
\begin{lem}\label{lem:resapprox}
Let  be any stream.  Algorithm Residual-Approximation makes a single pass over  and uses
polylogarithmic memory in  and .  Moreover, if the current window  contains an
-heavy element  with respect to , then the algorithm maintains and outputs
a value  such that  (except
with negligible probability).
\end{lem}
\begin{proof}
Assume the current window  has an -heavy element  with respect to .  Due to the properties of smooth
histograms from~\cite{BO07}, we know that , where  is the multiplicity vector
of the current window in substream  (similarly for ).  Hence, the random variable 
is a -approximation of the random variable  (here, 
is the indicator random variable which is 1 if  and 0 otherwise).  To see why, suppose that element  is mapped to  by ,
so that  belongs to the substream .  Then observe that

Hence, the minimum of  and  will indeed be a -approximation to ,
since this is the second moment of the vector  (the case is symmetric if element  is mapped to  by ).

Now, since  is pairwise independent, we have that .  In particular, since we always have ,
we can bound the variance by .  If we denote by  the random variable which is the average
of  independent 's, then we have .  Hence, if we choose  to be sufficiently
large, then by Chebyshev's inequality we have:

(for instance,  is sufficient).

Now, if we take the median  of  independent 's, then by Chernoff bound this would make the probability negligible.
That is, we have  except with negligible probability.  We can
repeat these arguments and consider the median of  averages (i.e., the 's) of  independent 's.  Since
there are only  's total (with each one being a -approximation to its corresponding random variable ,
except with negligible probability), then by the union bound all of the 's will be -approximations except with negligible probability
(since the sum of polylogarithmically many negligible probabilities is still negligible).  Therefore, the median of averages would give a
-approximation to .  Taking the square root guarantees that  (except with negligible
probability).

Note that the subroutine for computing an approximation to  on sliding windows using smooth histograms can be done in one pass and
in polylogarithmic space (even if we demand a -approximation and a negligible probability of failure).
\end{proof}

Now, we claim that Algorithm Compute-Hybrid-Major solves the following problem:
\begin{prob}[Hybrid-Major]
Given a stream  and , maintain a value  for each window  such
that:
 If , then  is a -approximation of  for some , and
 If the current window  has an element  such that
, then  is a -approximation of .
\end{prob}

\begin{algorithm}\label{alg:heavyapprox}
\DontPrintSemicolon
Let  be a -approximation of  for window  using the smooth histogram
method~\cite{BO07} (with negligible probability of error), where . \;

Repeat  times, independently and in parallel: \;
\Indp
Generate a uniform pairwise independent vector . \;
Maintain and denote by  a -approximation of the second moment for the window  using
a smooth histogram~\cite{BO07} (with negligible probability of error). \;
Similarly define  for the window . \;
If  and , then output  and terminate the algorithm. \;
\Indm
In parallel, apply Residual-Approximation() to maintain the residual second moment approximation,
let  denote the output of the algorithm. \;
If  or , then output . \;
Otherwise, output . \;
\caption{Compute-Hybrid-Major}
\end{algorithm}

Before delving into the proof, we first give the following useful lemma.

\begin{lem}\label{lem:heavyapprox}
Suppose the current window  contains an -heavy element .
Moreover, let  be a -approximation of the  norm of the current window , where .
Then .
\end{lem}
\begin{proof}
Since  is a -approximation of the  norm of , we have
.
Hence, we have that

Moreover, we have ,
which gives the lemma.
\end{proof}

\begin{lem}\label{lem:hybridmajor}
Compute-Hybrid-Major solves Hybrid-Major with negligible probability of error.
\end{lem}
\begin{proof}

First, we show that if there is no -heavy entry in the current window , then the output
is  except with negligible probability.  Consider a single iteration of the main loop of the algorithm.
Let  be the vector with entries  and denote
.
Since we have an  approximation over sliding windows, except with negligible probability,  and  are -approximations
of  and , respectively.  Hence,  and .
By Lemma \ref{lem:majorseparation}, except with probability at most :
 and .
Thus, the algorithm outputs  except with negligible probability.

Assume that there is an -heavy entry .
Then, applying Lemma~\ref{lem:heavyapprox} with some  to be set later, we know
 and 
(except with negligible probability).
By Lemma~\ref{lem:resapprox},
it follows that  except with negligible probability.
Hence, we have , since
 (assuming ).
Now, observe that if the algorithm outputs , then it must be that
.
Thus, by applying Lemma~\ref{lem:Gapprox2} with parameters , and ,
it follows that if the algorithm outputs , then  is a -approximation of .
Thus, the first condition of Hybrid-Major follows.

Finally, assume . By definition,
 and so  is -heavy with respect to .
By Lemma~\ref{lem:majorseparation}, we have (except with negligible probability):
.
Hence, except with negligible probability, the algorithm will not terminate before the last line.
Let  be the constant given by the definition of
tractability in Definition~\ref{defn:tractability}.  We assume  (otherwise
the number of elements in the window is constant).  Also, let 
be given by Definition~\ref{defn:tractability}.
By applying Lemma~\ref{lem:heavyapprox} with , we have
 for sufficiently large  (since 
is tractable) and
.
Since 
and ,
then by Lemmas~\ref{lem:Gapprox} and~\ref{lem:Gapprox2} (which we apply with the same parameters,
, and ), the algorithm outputs  which is a -approximation
of .  Thus, the second condition of Hybrid-Major follows.
\end{proof}

\noindent
The next two lemmas are from~\cite{BO10} (the proof of Lemma~\ref{lem:G-major} uses Predicate (\ref{eqn:tract1})
from Definition~\ref{defn:tractability}).
\begin{lem}\label{lem:tractcube}
If  is tractable, then
.
\end{lem}
\begin{lem}\label{lem:G-major}
Let  be a non-decreasing tractable function. Then for any , there
exists  such that for any  and for any 
the following holds. Let  be a stream and  be the current window.
If there is a -heavy element  with respect to , then there is
a set  such that  and:

\end{lem}

We now give the algorithm Compute--Core, which solves the -Core problem
(i.e., Problem~\ref{prob:heavy}), and prove its correctness.
A similar algorithm appears in~\cite{BO10}, we
repeat it here for completeness, and to help design and understand our main result on universality.

\begin{algorithm}\label{alg:heavy}
\DontPrintSemicolon
Generate a pairwise independent hash function , where . \;
, compute in parallel  = Compute-Hybrid-Major,
where . \;
Output . \;
\caption{Compute--Core}
\end{algorithm}

\begin{thm}\label{thm:gcore}
Algorithm Compute--Core solves -Core, except with probability asymptotically equal to .
Compute--Core uses  memory bits if  and 
for some .
\end{thm}

\begin{proof}
Let  denote the current window.  First, except with negligible probability, every positive  is a
-approximation of some distinct entry , which implies that
 is a -approximation of .  Second, assume that there exists a -heavy
entry  with respect to . Denote . By pairwise independence of , we have
.  By Lemma \ref{lem:G-major}, there exists a set  and  such that
 and:

Let  be the event that , and let  be the event that .
By Markov's inequality, by pairwise independence of , and by , there exists  such that:

If  occurs (which happens with probability at least ), then  is a -approximation of 
except with negligible probability (by Lemma~\ref{lem:hybridmajor}).  Thus, the final probability of error is approximately equal to .

It is not too hard to see that Algorithm Compute--Core uses polylogarithmic memory.  The subroutine depth is constant, and there are only
polylogarithmically many subroutine calls at each level.  At the lowest level, we only do direct computations on the stream that require polylogarithmic
space or a smooth histogram computation for  or , which also requires polylogarithmic space.
We get that for any constant , there exists a constant  such that we can solve -Core (except with probability ) using
 space, where .
\end{proof}

In Appendix~\ref{apx:sum}, we show how to reduce the -Sum problem to the -Core problem.  In
particular, we prove the following theorem.  The algorithm and proof of correctness follow from~\cite{BO13}.
We restate the algorithm and results using our notation for completeness.
\begin{thm}\label{thm:gsum from gcore}
If there is an algorithm that solves -Core using memory  and makes one pass over
 except with probability  for some , then there is an algorithm that
solves -Sum using memory  and makes one pass over  except with probability at most .
\end{thm}

\noindent Note that we can reduce the failure probability from constant to inverse polynomial using standard techniques.
Combining this with Theorem~\ref{thm:gcore} and Theorem~\ref{thm:rlb}, we have Theorem~\ref{thm:zeroone}.

\section{Universality}\label{sec:universality}

In this section, we show the main result of this paper, Theorem~\ref{thm:mainusum},
by designing a universal sum algorithm.  We first construct a universal core algorithm, which we call .
That is, given a data stream,
the algorithm produces a universal core structure with respect to the frequency vector 
defined by the current window  without knowing the function  to be approximated in advance.  Let
 be a constant and let  be the set according to Definition~\ref{defn:utractability}.
The structure will have the guarantee that, when queried with any function  taken from  (after
processing the stream), it outputs the set  according to Definition~\ref{defn:ucs}.


\paragraph{Universal Core Algorithm ():} The algorithm will construct a universal core structure
 and our techniques will build on the results from Section~\ref{sec:tract}.
Algorithm Residual-Approximation from Section~\ref{sec:tract} does not depend on the function ,
so it clearly carries over to our universal setting.

Algorithm Compute-Hybrid-Major does depend on , so we have to modify it accordingly.
We do not rewrite the whole algorithm, as there are only a few modifications.  In Step 1,
we set .  We get rid of Steps 8 and 9, and instead create a new
Step 8 where we find the index  of an -heavy element , if it exists (finding
such an index can be done using standard methods, the details of which we omit
for brevity).  We also create a new Step 9 where we output the triple  (assuming none of the parallel
copies from Step 2 outputs 0).

We also need to modify Algorithm Compute--Core.  In particular, the value
of  in Step 1 should depend on , and we set it to be .  Moreover,
we remove Step 3 from the algorithm and store  for each  as part of our
data structure  (recall that  is either 0 or a triple , where  are the values
computed in the  parallel instance of the subroutine Compute-Hybrid-Major and  is the index
of the corresponding -heavy element).

\paragraph{Querying the Structure:} Given a function  as a query to our universal
core structure, we now explain how to produce the set  according to Definition~\ref{defn:ucs}.

For each stored  in the data structure  (), if , then
we do not include it in our output set .  Otherwise, if  is a triple ,
then we include the pair  in our set  as long as

(recall ).


\begin{thm}\label{thm:ucore}
Fix a parameter  and let  be the set of tractable functions corresponding to the
definition of universal tractability.  Then  is a universal core algorithm with parameters
 (for any ), 
(for any ), , and .
\end{thm}

\begin{proof}
The correctness of  essentially follows from the proofs of the results in Section~\ref{sec:tract}.
In particular, Lemma~\ref{lem:resapprox} still holds since Algorithm Residual-Approximation is unchanged.

Lemma~\ref{lem:hybridmajor} still mostly holds without much modification.  Using the same notation
as in the original proof, if there
is no -heavy element, then the proof of Lemma~\ref{lem:hybridmajor} can still be applied
and the modified version of Compute-Hybrid-Major will output 
(except with negligible probability).  In such a case, the universal core structure will store the value .
If there is an -heavy element  and the structure stores , then again the
proof applies.  The reason is that, when
querying the universal core structure with a function , we check if
,
in which case the proof argues that  is a -approximation of .  In
the case that , the proof still goes through
since we apply Lemma~\ref{lem:heavyapprox} with , and we have
 (here, similarly to Lemma~\ref{lem:hybridmajor},  is the constant given by
the definition of universal tractability for , and hence ).

Finally, we must argue the correctness of Theorem~\ref{thm:gcore}.  Using some notation taken
from the proof, consider an output  (if , the data structure does not output it
to the set ) and observe that  for any  satisfying

is a -approximation of a distinct entry .  Moreover,
if there is a -heavy element , then we again have
.  In
fact, delving into the proof of Lemma~\ref{lem:G-major} (found in~\cite{BO10}), we see that the
specific value of  depends on , and is given by the definition of universal tractability for .
Since  and we choose , we get the probability of the bad
event  (using the same notation from Theorem~\ref{thm:gcore}) is bounded by:

The rest of the proof goes through in the same way, and hence this gives the theorem.
\end{proof}

We now argue how to use our universal core algorithm  as a subroutine to give the main result
of the paper.  The proof of the theorem below can be found in Appendix~\ref{apx:sumcore}, and the argument
follows a similar one found in~\cite{BO13} (we reproduce it in the appendix for completeness).

\begin{thm}\label{thm:usum}
Fix a parameter  and let  be the set of tractable functions corresponding to the
definition of universal tractability.  Suppose there is a universal core algorithm that makes a single pass over
, uses polylogarithmic memory in  and , and has parameters
 (for any ), 
(for any ), , and .  Then
there is a universal sum algorithm with parameters
 (for ), ,
and .  The algorithm uses polylogarithmic space in  and  and makes a single
pass over .
\end{thm}

\noindent We can reduce the failure probability of the universal sum algorithm to
inverse polynomial via standard methods.  Our main result, Theorem~\ref{thm:mainusum},
follows from Theorem~\ref{thm:ucore} and Theorem~\ref{thm:usum}.

\bibliographystyle{plain}
\bibliography{ZeroOne}

\appendix


\section{-Sum from -Core}\label{apx:sum}
In this section, we prove Theorem~\ref{thm:gsum from gcore}.
As mentioned, the algorithm and correctness follow from~\cite{BO13}.  We include the algorithm
here for completeness, and rephrase it and the results using notation from our paper.

Let  be a tractable function according to Definition~\ref{defn:tractability}, and let  be a stream given as input.
We show how to construct an algorithm that solves the -Sum problem by using our algorithm for -Core as
a subroutine.  In particular, let Compute--Core be our algorithm from Section~\ref{sec:tract} that solves
the -Core problem.  Note that for the output set 
 maintained by Compute--Core, using standard techniques one can easily obtain the explicit set
of indices  such that 
for each .  Hence, we assume that Compute--Core outputs a set of pairs of the form
.

In the language of~\cite{BO13}, Compute--Core produces a -cover with respect to the
vector  with probability at least , where 
(for any ) and  (for any ).  Given the tractable function
, our algorithm for -Sum is as follows:

\bigskip

\begin{algorithm}[H]\label{alg:gsum}
\DontPrintSemicolon
Generate  pairwise independent, uniform zero-one vectors ,
and let .
Let  be the substream defined by , and let  denote
 for the substream 
and window  (where ). \;

Maintain, in parallel, the cores  Compute--Core for each . \;

If , then output . \;

Otherwise, precisely compute . \;

For each , compute . \;
Output . \;
\caption{-Sum}
\end{algorithm}

Note that, in our paper, Compute--Core only takes three parameters (the stream , the
error bound , and the failure probability ), while the algorithm from~\cite{BO13} assumes
four parameters of the form Compute--Core.  Here, , , and 
have the same meaning as in our paper.  The parameter  controls how heavy an element needs to be
(according to the function ) in order to necessarily be in the output set of Compute--Core.  That is,
in the set  output by Compute--Core, if there is an  such that
 is -heavy with respect to , then .  In
Section~\ref{sec:tract}, we solve the problem for , but Algorithm -Sum needs the problem
solved for . However, using standard techniques, we can reduce the problem of
solving -Core for  to the same problem for .

\begin{thm}

For any tractable function , Algorithm -Sum computes a -approximation of 
except with probability at most , where  for any .
The algorithm uses memory that is polylogarithmic in  and .
\end{thm}

\begin{proof}
The proof of this theorem follows directly from Theorem 1 in~\cite{BO13}.
\end{proof}

\noindent Note that we can turn the constant failure probability into an inverse polynomial failure probability
using standard techniques.


\section{Universal Sum from Universal Core}\label{apx:sumcore}

In this section, we prove Theorem~\ref{thm:usum}. The algorithm and proof are similar to that of
Appendix~\ref{apx:sum}, except that we need to carry out the argument within our universal framework.
As mentioned, the algorithm and correctness follow from~\cite{BO13}.  We do not rewrite the whole algorithm,
but instead describe the necessary modifications that need to be made from Appendix~\ref{apx:sum}.

Let  be a stream given as input to our universal sum algorithm.
Let  be our universal core algorithm from Theorem~\ref{thm:ucore}, Section~\ref{sec:universality},
the parameters of which are specified in our universal sum algorithm description.

\paragraph{Universal Sum Algorithm:} We describe the modifications that need to be made to Algorithm
-Sum from Appendix~\ref{apx:sum}.

In Step 2, instead we need to maintain and store the
output  with parameters ,  (i.e., the one
given as input to our universal sum algorithm), , and
 for each  (in the  parallel iteration,  is
given the stream  as input).  As in Appendix~\ref{apx:sum}, we construct a universal core
structure for , but we can reduce the problem of  to .
Note that  is of the form  ( may have 's as well,
which we simply ignore).  For each such triple , we also store the value of .

In Step 3, instead we check if , and if so we store  (recall 
denotes the frequency vector  for the substream  induced by ).  We remove Steps
4, 5, and 6.

\paragraph{Querying the Structure:} Now, given a function , we explain how to query
the universal sum structure output by our universal sum algorithm to approximate .  In particular, for each  we first
query the universal core structure output by  to get a set .
Then, we compute  and, for each , we recursively compute
 according to:



\noindent Once each  has been computed for , we output .

\begin{thm}
Fix a parameter  and let  be the set of tractable functions corresponding to the
definition of universal tractability.  There is a universal sum algorithm with parameters
 (for ), ,
and .  The algorithm uses polylogarithmic space in  and  and makes a single
pass over .  When querying the universal sum structure (output by the universal sum algorithm)
with a function , it outputs a -approximation of  except
with probability at most .
\end{thm}

\begin{proof}
The proof of this theorem follows directly from Theorem 1 in~\cite{BO13}.
\end{proof}


\end{document}
