

\appendix\newpage

\onecolumn


\begin{center}
{\LARGE
\textbf{Supplementary material} \0.2cm]}

\vspace{0.5cm} \end{center}

This supplemental material provides complementary results referred in the main document, noticeably a presentation of the augmentation and regularization specificity in \textbf{timm} and some alternative training procedures.

 \FloatBarrier
 




\FloatBarrier


\section{Augmentations and Regularization in timm~\cite{pytorchmodels}}
\label{sec:augreg}

The \textbf{timm} library includes a variety of image augmentations, regularization techniques, optimizers, and learning rate schedulers that can be used to produce leading results on ImageNet classification and other 2D image tasks. Many \textbf{timm} training components have modifications and improvements from original implementations or papers describing them. One should be aware of these changes if using them. 

\paragraph{Data Augmentation} in \textbf{timm} includes implementations of RandAugment~\cite{Cubuk2019RandAugmentPA}, AutoAugment~\cite{Ekin2018AutoAugment}, AugMix~\cite{Hendrycks2020AugMixAS}, Random Erasing~\cite{Zhong2020RandomED}, and an integrated implementation of Mixup~\cite{Zhang2017Mixup} and CutMix~\cite{Yun2019CutMix}. The base for all augmentations is typically Random Resized Crop with horizontal flipping. 

\paragraph{RandAugment} is the most used of the AA (AutoAugment) variants in \textbf{timm} -- it also contains the most significant additions from the original paper and Tensorflow based implementations – so we will focus on that implementation. The original RandAugment specification has two hyper-parameters, M and N; where M is the distortion magnitude and N is the number of distortions uniformly sampled and applied per-image. The goal of RandAugment was that both M and N be human interpretable. However, that ended up not being the case for M. The scales of several augmentations were backwards or not monotonically increasing over the range such that increasing M does not increase the strength of all augmentations.

This is most visible for image enhancement blending operations (color, contrast, brightness, sharpness) where the argument value defines the behavior as follows: 

\begin{minipage}{0.8\linewidth}
\begin{itemize}
    \item [0.] selects the degenerate image
    \item [0.-1.0] interpolates between the degenerate and original image
    \item [1.0] returns original image
    \item [ 1.0] extrapolates the original image way from the degenerate
\end{itemize}
\end{minipage}


Taking sharpness as an example, magnitudes of M0, M5, and M10 are mapped in the original implementation to produce strong blurring (0.1), no-change (1.0), or strong sharpening (1.9) respectively.

The implementation in \textbf{timm} attempts to improve this situation by adding an ‘increasing’ mode (always enabled for recipes in this paper) where all augmentation strengths increase with magnitude; solarize and posterize increase with M (instead of decrease), and interpolation vs extrapolation for the blending operations is randomly chosen with a strength that increases with M. This makes increasing M more intuitive and allows an additional hyper-parameter to work well: \textbf{timm} adds a MSTD parameter which adds gaussian noise with the specified standard deviation to the M value per distortion application. Additionally, if MSTD is set to ‘-inf’, M is uniformly sampled from 0-M for each distortion. Without correcting the scales, one would often end up with completely empty or heavily inverted images in ranges of M that are supposed to be low in strength.

Care was taken in \textbf{timm}’s RandAugment to reduce impact on image mean, the normalization parameters can be passed as a parameter such that all augmentations that may introduce border pixels can use the specified mean instead of defaulting to 0 or a hard-coded tuple as in other implementations. And lastly, Cutout is excluded by default to favour separate use of \textbf{timm}’s Random Erasing implementation which has less impact on mean and standard deviation of the augmented images. 

\paragraph{Random Erasing} is another commonly used \textbf{timm} augmentation with modifications from the original paper. The implementation in timm follows the original but allows ‘erasing’ image regions with per-pixel gaussian noise (mean 0, std 1.0) instead of a uniform random or constant color (black or image mean) per-region. When applied to images at the recommended location in the augmentation pipeline -- after images have been normalized (standardized) -- this maintains image statistics and allows better results with stronger application of the augmentation. A count parameter was also added to \textbf{timm}’s Random Erasing such that multiple regions can be erased per-image. 

\paragraph{Mixup and CutMix} are cleanly integrated in \textbf{timm} in a manner not common in other implementations. Both can be enabled at the same time with a variety of different mixing strategies: 

\begin{description}
    \item [batchwise] CutMix vs Mixup selection, lambda and CutMix region sampling performed per-batch 
    \item [pairwise] mixing, lambda, and region sampling performed per mixing sample pair within batch
    \item [elementwise] mixing, lambda, and region sampling performed per sample within batch
    \item [half] the same as elementwise but one of each mixing pair is discarded so that each 
    sample is seen once per epoch
\end{description}

The default is to use either CutMix or Mixup with probability of 0.5 per-batch if both are enabled – this is the case for all mentioned training procedures in this paper. 

\paragraph{Regularization} in \textbf{timm} is standard. It allows use of similar regularization for many of the included models. Weight decay is available via either native PyTorch or \textbf{timm} optimizers. The ability to enable pre-classifier dropout is included in all model architectures. Stochastic-Depth has been added as an option to many of the most popular model architectures (via a layer named DropPath). Label-smoothing is included via a cross-entropy loss function and possible to use in combination with the label manipulation of CutMix and Mixup.  



\section{Alternative Training Procedures}
\label{sec:alternative}

The main training recipes in this paper uses the LAMB optimizer. Several sets of hyper-parameter variations with differing training costs were presented with leading results for the vanilla ResNet-50 architecture. Here, we introduce alternative training recipes that also produce results matching or exceeding the best existing ResNet-50. The reader may find these are better suited for use or adaptation for their specific model architecture, dataset, or task. The alternative recipes are: 
\begin{description}
    \item [Procedure \Bp~ --] RMSProp with EMA weight averaging and step LR decay;
    \item [Procedure \Cp~ --] SGD with Nesterov’s momentum, Adaptive Gradient Clipping, and a cosine learning rate decay. We have two variants of it (C1 and C2) depending on whether we use repeated augmentation or not; 
    \item [Procedure \Dp~ --] AdamP with a cosine learning rate decay and binary cross-entropy. 
\end{description}

The above procedures have been used to product excellent results for many pre-trained models in the \textbf{timm} library, including many non-ResNet architectures. Table~\ref{tab:others_training} summarizes their best ResNet-50 oriented settings. 


\begin{table}[t]
\centering
\scalebox{0.85}
{\begin{tabular}{l|cccc}
\toprule
Procedure  & \Bp & \Cp.1 & \Cp.2 & \Dp \\
\midrule
Train Res & 224 & 224 & 224 & 224 \\
Test Res & 224 & 224 & 224 & 224 \\
\midrule
Epochs  & 600 & 800 & 800 & 600 \\
\# of forward pass & 375k & 500k & 500k & 2,000k \\
\midrule
 Batch Size & 2048 & 2048 & 2048 & 384 \\
 Optimizer & RMSProp & SGD & SGD & AdamP \\
 Initial LR & 0.18 & 0.88 & 0.88 & 0.0033 \\
 LR Scheduler & step & cosine & cosine & cosine \\
 Decay Rate & 0.988 per 1-epoch & \xmarkg & \xmarkg & \xmarkg \\
 LR Noise (\% of training) & 0.45 to 1.0 & \xmarkg & \xmarkg & \xmarkg \\
  Weight Decay &  &  &  & 0.01\\
  Warmup Epochs & 5 & 5 & 5 & 5 \\
  
  \midrule
  Label Smoothing & 0.1 & 0.1 & 0.1 & 0.1 \\
  Dropout & 0.2 & 0.25 & 0.25 & 0.1 \\
  Stochastic Depth & 0.1 & 0.1 & 0.1 & 0.05 \\
  Repeated Augmentation & \xmarkg & \xmarkg & \cmark & \xmarkg \\
  Grad Clipping & \xmarkg  & AGC .025 & AGC .05 & \xmarkg  \\
  
  \midrule
  RandAugment (M/N/MSTD) & 8/2/1.0 & 7/3/1.0 & 7/3/1.0 & 7/3/1.0 \\
  Mixup & 0.2 & 0.2 & 0.2 & 0.2 \\
  CutMix & \xmarkg & 1.0 & 1.0 & 1.0 \\
  Random Erasing (Prob/Count) & 0.35/3 & 0.4/1 & 0.4/1 & .35/1 \\ 
  \midrule
  EMA weight averaging & 0.9999 & \xmarkg & \xmarkg & \xmarkg \\ 
  \midrule
  CE loss & \cmark & \cmark & \cmark & \xmarkg \\ 
  BCE loss  & \xmarkg & \xmarkg & \xmarkg & \cmark \\ 
  \midrule
  Top-1 acc. & 79.4\% & 79.8\% & 80.0\% & 79.8\% \\
  
  
 \bottomrule
\end{tabular}}
\smallskip
\caption{Alternative training procedures giving good performance with ResNet-50 architecture. 
\label{tab:others_training}}
\end{table}

\paragraph{Training procedure \Bp~ details (RMSProp)}
This procedure is inspired by the RandAugment~\cite{Cubuk2019RandAugmentPA} recipes used to train EfficientNet architectures but leverages features in \textbf{timm}’s implementation of RandAugment and Random Erasing. The step decay has been adjusted to decay every epoch (instead of every 2.4 as with EfficientNet, weight decay has been slightly decreased from EfficientNet defaults, and the learning rate is a bit higher. Additional augmentation was added in the form of per-pixel noise Random Erasing and Mixup. It should be noted that the RMSProp optimizer used is the \textbf{rmsprop\_tf} implementation in \textbf{timm} which carefully matches behaviours of the Tensorflow (before version 2.0) implementation. The native PyTorch RMSProp implementation will not produce the same results, even if adjusting for the epsilon location.

With long decay constants for the EMA weight averaging, it can be beneficial to perturb the learning rate (currently once per epoch) with noise in later stages of training (typically 40-50\% of the way through until the end). In exploration so far, learning rate noise appears to increase sensitivity of training results to random seed but has often produced the best result in (so far, limited) sweeps with the same hyper-parameters. Further analyzing the interplay between learning rate value, schedule, and noise, EMA decay constant, and random seed is a future objective for refining this training recipe.

This training strategy varies somewhat in effectiveness with batch size. Running experiments for this paper with larger batch sizes in the 1024-2048 range has often come slightly below (0.1 to 0.3 top-1) prior training runs with smaller sizes in the 256-768 range used for numerous \textbf{timm} pre-trained weights. It is unclear if this can be addressed with further hyper-parameter adjustments and different learning rate scaling (linear used by default) across batch sizes.

See Table~\ref{tab:procedure_b} for a summary of the procedure, including ranges of recommended of values to search over for applying to different classification task and architecture combinations. For larger model architectures it is advisable to focus on stronger augmentation and regularization values within the suggested ranges. Looking at Table~\ref{tab:mainres}, the original results for the \textbf{timm} specific EfficientNetV2-S~\cite{Tan2021EfficientNetV2SM} variant and ECA-ResNet-269-D were trained using this procedure, but with higher levels of augmentation and regularization than for ResNet-50.


\begin{table}[p]
\centering
\scalebox{0.85}
{\begin{tabular}{l|ll}
\toprule
 &  Recommended Range & ResNet-50 \\
 \midrule
 Epochs & 400-700 & 600 \\
 
 \midrule
 Initial LR (per batch size 256) & .01-.025 & 0.0225 \\
 LR Schedule & Step & Step \\
 LR Decay Rate & 0.97-0.99 per 1-3 epochs & 0.988 per 1-epoch \\
 LR Noise Active (\% of training) & 40-50\% to 100\% & 45\% to 100\% \\
 \midrule
 Grad Clipping & Off, global norm 1.0 & off \\
 \midrule
  Dropout & 0-0.4 & 0.2 \\ 
 Stoch Depth & 0-0.1 & 0.1 \\
 Repeated Augmentation & Off & Off \\
 \midrule
 RandAugment (M / N / MSTD) & 6-9 / 2-4 / 0.5-1.0 & 8 / 2 / 1.0 \\
 Random Erasing (Prob / Count) & 0.1-0.5 / 1-3 & 0.35 / 1 \\ 
  Mixup & 0.2, 0.5, 0.8 & 0.2 \\ 
 CutMix & Off, 0.8, 1.0 & 0 \\
 \midrule

 EMA Weight Averaging & On & On \\
 \midrule
 Loss & CE & CE \\ 

 \bottomrule
\end{tabular}}
\smallskip
\caption{Procedure \Bp~ summary 
\label{tab:procedure_b}}
\end{table}

\paragraph{Training procedure \Cp ~details (SGD with Nesterov's momentum and AGC)}

This recipe is based on the published procedure for training NFNets~\cite{Brock2021HighPerformanceLI}: using SGD with Nesterov’s momentum, Adaptive Gradient Clipping ~\cite{Brock2021HighPerformanceLI} (AGC), and heavy augmentation and regularization. AGC allows for stable large batch training at higher learning rates. Stronger default augmentation and regularization make up for the loss of batch normalization’s regularizing effect when paired with NFNets, but strengths can be relaxed when used with other architectures that use batch normalization. With some adjustments, this procedure has been useful training architectures in \textbf{timm} such as ECA-NFNet, ResNet, and EfficientNet variants to impressive performance levels. The original result for the \textbf{timm} EfficientNetV2-M~\cite{Tan2021EfficientNetV2SM} variant in Table~\ref{tab:mainres} was trained with the C.1 recipe, but with significantly higher augmentation and regularization than for ResNet-50.

The C.1 vs C.2 versions of this procedure seen in Table~\ref{tab:others_training} differ most significantly in the application of Repeated Augmentation. It should be noted that a shorter training length of 600 epochs also works quite well in both cases, with an expected drop of roughly 0.15-0.2 top-1 for the same seed. Table~\ref{tab:procedure_c} includes ranges of the ingredients for exploring with different tasks and architectures.



\begin{table}[p]
\centering
\scalebox{0.85}{
\begin{tabular}{l|ll}
\toprule
 &  Recommended Range & ResNet-50 \\
 \midrule
 Epochs & 300-800 & 800 \\
 \midrule
 Initial LR (per batch size 256) & .08-.12 & 0.11 \\
 LR Schedule & cosine & cosine \\
 \midrule
 Grad Clipping & AGC .01 - .05 & AGC .05 \\ 
 \midrule
  Dropout & 0-0.5 & 0.25  \\ 
 Stoch. Depth & 0-0.2 & 0.1  \\
 Repeated Augmentation & Off, On & On \\
 \midrule
 RandAugment (M / N / MSTD) & 6-10 / 2-4 / 0.5-1.0 & 7 / 3 / 1.0 \\
 Random Erasing (Prob / Count) & 0.1-0.5 / 1-3 & 0.4 / 1 \\ 
  Mixup & 0.2, 0.5, 0.8 & 0.2 \\ 
 CutMix & Off, 0.8, 1.0 & 1.0  \\
 \midrule
 Loss & CE & CE \\ 
 \bottomrule
\end{tabular}
}
\smallskip
\caption{Procedure \Cp~ summary 
\label{tab:procedure_c}}
\end{table}

\paragraph{Training Procedure \Dp ~ details (AdamP)}

Late in the process for this report a training trial using AdamP~\cite{adamp} showed promise. With limited runs so far a recipe based on AdamP has achieved a 79.8 top-1 on ImageNet-1k. Further experimentation is necessary, the trials so far were run at a comparatively small batch size, but the promising results warrant exploration. Note that unlike RMSProp or SGD (but similar to Adam and LAMB), it is recommended to use square root scaling when adjusting the learning rate for this recipe across different batch sizes.

Table~\ref{tab:procedure_d} contains the recommended ranges for the ingredients of this recipe. These ranges have not been explored extensively across different model architectures as with procedures B and C.


\begin{table}[p]
\centering
\scalebox{0.85}{
\begin{tabular}{l|ll}
\toprule
 &  Recommended Range & ResNet-50 \\
 \midrule
 Epochs & 300-600 & 600 \\
 \midrule
 Initial LR (per batch size 256) & .002 - .003 & 0.0027 \\
 LR Schedule & cosine & cosine \\
 \midrule
 Grad Clipping & None & None \\ 
 \midrule
  Dropout & 0-0.3 & 0.1  \\ 
 Stoch. Depth & 0-0.1 & 0.05 \\
Repeated Augmentation & Off, On & Off \\
 \midrule
 RandAugment (M / N / MSTD) & 6-9 / 2-4 / 0.5-1.0 & 7 / 3 / 1.0 \\
 Random Erasing (Prob / Count) & 0.1-0.5 / 1-3 & 0.35 / 1\\ 
  Mixup & 0.2, 0.5, 0.8 & 0.2 \\ 
 CutMix & Off, 0.8, 1.0 & 1.0  \\
 \midrule
 Loss & CE, BCE & BCE  \\ 
 \bottomrule
\end{tabular}
}
\smallskip
\caption{Procedure \Dp~ summary 
\label{tab:procedure_d}}
\end{table}

\paragraph{Other Recipes}

Undoubtedly, other training recipes with different combinations of optimizer, learning rate schedule, augmentation, and regularization exist that can match or surpass the performance of the procedures detailed in this report. Ingredients aside, putting in the time and effort to tune the recipe with the target architecture is key. The authors already have an AdamW recipe in the works that is looking promising. 

