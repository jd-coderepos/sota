

\documentclass{article}

\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure,overpic} 

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2017}


\icmltitlerunning{Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks}


\pgfplotsset{every axis/.append style={
                    label style={font=\tiny},
                    tick label style={font=\tiny}  
                    }}
                    
\usepgfplotslibrary{groupplots}                 

\newlength\figureheight
\newlength\figurewidth

\newcommand{\etal}{\textit{et al}. }
\newcommand{\ie}{\textit{i}.\textit{e}. }
\newcommand{\eg}{\textit{e}.\textit{g}. }

\begin{document} 

\twocolumn[
\icmltitle{Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Federico Monti}{usi}
\icmlauthor{Michael M. Bronstein}{usi,tau,intel,tum}
\icmlauthor{Xavier Bresson}{ntu}
\end{icmlauthorlist}

\icmlaffiliation{usi}{ICS USI Lugano, Switzerland}
\icmlaffiliation{tau}{Tel Aviv University, Israel}
\icmlaffiliation{intel}{Intel Perceptual Computing, Israel}
\icmlaffiliation{ntu}{NTU, Singapore}
\icmlaffiliation{tum}{TUM IAS, Germany}

\icmlcorrespondingauthor{Federico Monti}{federico.monti@usi.ch}

\icmlkeywords{geometric deep learning, matrix completion, recommender systems}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract} 


Matrix completion models are among the most common formulations of recommender
systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationarity structures of user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. This neural network system requires a constant number of parameters independent of the matrix size. We apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques.





















 
\end{abstract} 

\section{Introduction}


Recommender systems have become a central part of modern intelligent systems. Recommending movies on Netflix, friends on Facebook, furniture on Amazon, jobs on LinkedIn are a few examples of the main purpose of these systems. 
Two major approach to recommender systems are collaborative \cite{pro:BreeseHeckermanKadie98CollFilt} and content \cite{art:PazzaniBillsus07ContFilt} filtering techniques. Systems based on collaborative filtering use collected ratings of products by customers and offer new recommendations by finding similar rating patterns. Systems based on content filtering make use of similarities between products and customers to recommend new products. Hybrid systems combine collaborative and content techniques. 



\paragraph*{Matrix completion.}
Mathematically, a recommendation method can be posed as a {\em matrix completion} problem \cite{candes2012exact}, where columns and rows represent users and items, respectively, and matrix values represent a score determining whether a user would like an item or not. Given a small subset of known elements of the matrix, the goal is to fill in the rest. 
A famous example is the Netflix challenge \cite{art:KorenBellVolinsky09MatFac} offered in 2009 and carrying a 1M\\times\mathbf{X}\Omegay_{ij}\boldsymbol{\Omega}\Omega\circ\| \cdot \|_\star\ell_1\mathcal{G}_c = (\{1,\hdots, n\}, \mathcal{E}_c, \mathbf{W}_c)\mathbf{W}_c = (w^\mathrm{c}_{ij})w^\mathrm{c}_{ij} = w^\mathrm{c}_{ji}w^\mathrm{c}_{ij} = 0(i,j) \notin \mathcal{E}_cw^\mathrm{c}_{ij} > 0(i,j) \in \mathcal{E}_c\mathcal{G}_r = (\{1,\hdots, m\}, \mathcal{E}_r, \mathbf{W}_r )n\times n\boldsymbol{\Delta} = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2}\mathbf{D} = \mathrm{diag}\left(\sum_{j\neq i} w_{ij} \right)\boldsymbol{\Delta}_r\boldsymbol{\Delta}_c\mathbf{X}\mathcal{G}_c\mathcal{G}_r\| \mathbf{X} \|_{\mathcal{G}_{r}}^2=\mathrm{trace}(\mathbf{X}^\top \boldsymbol{\Delta}_r \mathbf{X})\| \mathbf{X} \|_{\mathcal{G}_{c}}^2=\mathrm{trace}(\mathbf{X} \boldsymbol{\Delta}_c \mathbf{X}^\top)nm\mathbf{X}\mathbf{W}\mathbf{H}^\topj_1j_2\hdotsj_3i_2\vdotsi_1nmj_1j_2\hdotsj_3m\times n\mathbf{X}\mathbf{X}=\mathbf{W}\mathbf{H}^\top\mathbf{W}, \mathbf{H}m\times rn\times rr\ll \min(m,n)\mathbf{W}, \mathbf{H}\mathcal{O}(mn)\mathcal{O}(m+n)\mathrm{rank}(\mathbf{W}\mathbf{H}^\top)\leq r\mathbf{X}
\boldsymbol{\Delta} = \boldsymbol{\Phi} \boldsymbol{\Lambda} \boldsymbol{\Phi}^\top
\boldsymbol{\Phi} = (\boldsymbol{\phi}_1, \hdots \boldsymbol{\phi}_n)\boldsymbol{\Lambda} = \mathrm{diag}(\lambda_1, \hdots, \lambda_n)\mathbf{x} = (x_1, \hdots, x_n)^\top\hat{\mathbf{x}} = \boldsymbol{\Phi}^\top\mathbf{x}\mathbf{x}, \mathbf{y}q', q\hat{\mathbf{Y}}_{ll'} = \mathrm{diag}(\hat{y}_{ll',1}, \hdots, \hat{y}_{ll',n})\xi\mathcal{O}(n^2)\boldsymbol{\Phi}, \boldsymbol{\Phi}^\top\mathbf{X}\mathbf{X}^{(t)}\tilde{\mathbf{X}}^{(t)}\mathbf{dX}^{(t)}\mathbf{X}^{(t+1)} = \mathbf{X}^{(t)} + \mathbf{dX}^{(t)}\mathbf{X}q\mathcal{O}(1)\mathcal{O}(mn)\tilde{\boldsymbol{\Delta}} = 2 \lambda_{n}^{-1}\boldsymbol{\Delta}  - \mathbf{I}\tilde{\boldsymbol{\Lambda}} = 2 \lambda_{n}^{-1} \boldsymbol{\Lambda}  - \mathbf{I}[-1,1]\boldsymbol{\theta}pT_j(\lambda) = 2\lambda T_{j-1}(\lambda) - T_{j-2}(\lambda)jT_1(\lambda) =\lambdaT_0(\lambda) =1T_j(\lambda) = \cos(j \cos^{-1}(\lambda))[-1, 1]jj+1j[-1,1]p\mathcal{O}(|\mathcal{E}|)|\mathcal{E}| = \mathcal{O}(n)k\mathcal{O}(n)\mathcal{O}(n^2)(p-1)p\mathbf{W}\mathbf{H}^\top\mathbf{H}^{(t)}\tilde{\mathbf{H}}^{(t)}\mathbf{W}^{(t)}\tilde{\mathbf{W}}^{(t)}\mathbf{dH}^{(t)}\mathbf{dW}^{(t)}\mathbf{W}^{(t+1)} = \mathbf{W}^{(t)} + \mathbf{dW}^{(t)}\mathbf{H}^{(t+1)} = \mathbf{H}^{(t)} + \mathbf{dH}^{(t)}\mathbf{W}\mathbf{H}^\topq\mathcal{O}(1)\mathcal{O}(m+n)\boldsymbol{\Phi}_c, \boldsymbol{\Phi}_r\boldsymbol{\Lambda}_c, \boldsymbol{\Lambda}_rn\times nm\times m\boldsymbol{\Delta}_c, \boldsymbol{\Delta}_r\hat{\mathbf{Y}}\mathcal{O}(mn)p\boldsymbol{\Theta} = (\theta_{jj'})(p+1)\times(p+1)\mathcal{O}(1)\mathbf{X}\mathcal{O}(mn)\tilde{\boldsymbol{\Delta}}_r\tilde{\boldsymbol{\Delta}}_c[-1,1]m\times n\mathbf{X}qm\times n \times q\mathbf{X} = \mathbf{W}\mathbf{H}^\top\hat{\mathbf{Y}}_r^{ll'} = \mathrm{diag}(\hat{y}^r_{ll',1}, \hdots, \hat{y}^r_{ll',m})\hat{\mathbf{Y}}_c^{ll'} = \mathrm{diag}(\hat{y}^c_{ll',1}, \hdots, \hat{y}^c_{ll',n})\mathcal{O}(m+n)2(p+1)qq'\mathbf{dX}\mathbf{X}\mathbf{dW}\mathbf{dH}\mathbf{W}\mathbf{H}m\times n\mathbf{X}^{(0)}t = 0 : T\mathbf{X}^{(t)}m\times n \times q\tilde{\mathbf{X}}^{(t)}q(i,j)\tilde{\mathbf{x}}^{(t)}_{ij} = (\tilde{x}^{(t)}_{ij1}, \hdots, \tilde{x}^{(t)}_{ijq})dx^{(t)}_{ij}\mathbf{X}^{(t+1)} = \mathbf{X}^{(t)} + \mathbf{dX}^{(t)}m\times r\mathbf{H}^{(0)}n\times r\mathbf{W}^{(0)}\mathbf{X}^{(0)}t = 0 : T\mathbf{H}^{(t)}n \times q\tilde{\mathbf{H}}^{(t)}j=1:n\tilde{\mathbf{h}}^{(t)}_j = (\tilde{h}^{(t)}_{j1}, \hdots, \tilde{h}^{(t)}_{jq})dh^{(t)}_{j}\mathbf{H}^{(t+1)} = \mathbf{H}^{(t)} + \mathbf{dH}^{(t)}\mathbf{W}^{(t)}m \times q\tilde{\mathbf{W}}^{(t)}i=1:m\tilde{\mathbf{w}}^{(t)}_i = (\tilde{w}^{(t)}_{i1}, \hdots, \tilde{w}^{(t)}_{iq})dw^{(t)}_{i}\mathbf{W}^{(t+1)} = \mathbf{W}^{(t)} + \mathbf{dW}^{(t)}\mathcal{O}(mn)\mathcal{O}(m+n)T\mathbf{X}^{(T)}_{\boldsymbol{\Theta},\boldsymbol{\sigma}}\boldsymbol{\Theta}\boldsymbol{\sigma}\boldsymbol{\theta}_c, \boldsymbol{\theta}_rp=5k=3232T=1010^{-3}r=1510\mathbf{H}\mathbf{W}\mathbf{X}^{(t)}m n m nm+nm+n\mathbf{1}\mathbf{m n}\mathbf{1}\mathbf{m+n}m+nm+n\mathbf{m}\mathbf{m+n}\lambda_r\lambda_c\lambda_r, \lambda_c3000 \times 3000$ entries for evaluating the performance. 

Tables~\ref{tab:results-movielens} and~\ref{tab:other-datasets} summarize the performance of different methods. RGCNN outperforms the competitors in all the experiments. 



\begin{table}[!h]
\caption{Performance (RMS error) of different matrix completion methods on the MovieLens dataset.\vspace{-2.25mm}}
\label{tab:results-movielens}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc}
\hline
\abovespace\belowspace
Method & RMSE \\
\hline
\abovespace 
Global Mean & 1.154\\
User Mean & 1.063\\
Movie Mean & 1.033\\
MC \cite{candes2012exact} & 0.973\\
IMC \cite{jain2013provable,xu2013speedup} & 1.653\\
GMC  \cite{kalofolias2014matrix} & 0.996\\
GRALS \cite{rao2015collaborative} & 0.945\\
{\bf sRGCNN} & {\bf 0.929} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\begin{table}[!h]
\caption{
Matrix completion results on several datasets (RMS error). For Douban and YahooMusic, a single graph (of users and items, respectively) was used. For Flixter, two settings are shown: users+items graphs / only users graph. \vspace{-2.25mm} 
}
\label{tab:other-datasets}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\hline
\abovespace\belowspace
Method & Flixster & Douban & YahooMusic \\
\hline
\abovespace 
GRALS & 1.3126 / 1.2447 & 0.8326 & 38.0423\\ 
{\bf sRGCNN} & {\bf 1.1788} / {\bf 0.9258} & {\bf 0.8012} & {\bf 22.4149} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}













 


\section{Conclusion}
\label{sec:conc}

In this paper, we presented a new deep learning approach for matrix completion based on a specially designed multi-graph convolutional neural network architecture. Among the key advantages of our approach compared to traditional methods is its low computational complexity and constant number of degrees of freedom independent of the matrix size. We showed that the use of deep learning for matrix completion allows to beat current state-of-the-art recommender system methods. To our knowledge, our work is the first application of deep learning on graphs to this class of problems. We believe that it shows the potential of the nascent field of geometric deep learning on non-Euclidean domains, and will encourage future works in this direction. 

 


\section{Acknowledgments}
FM and MB are supported in part by ERC Starting Grant No. 307047 (COMET), ERC Consolidator Grant No. 724228 (LEMAN), Google Faculty Research Award, Nvidia equipment grant, Radcliffe fellowship from Harvard Institute for Advanced Study, and TU Munich Institute for Advanced Study, funded by the German Excellence Initiative and the European Union Seventh Framework Programme under grant agreement No. 291763. XB is supported in part by NRF Fellowship NRFF2017-10. 


\bibliography{sections/refsx,sections/refs,sections/refs1}
\bibliographystyle{icml2017}

\end{document} 
