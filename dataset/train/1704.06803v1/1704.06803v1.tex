

\documentclass{article}

\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure,overpic} 

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2017}


\icmltitlerunning{Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks}


\pgfplotsset{every axis/.append style={
                    label style={font=\tiny},
                    tick label style={font=\tiny}  
                    }}
                    
\usepgfplotslibrary{groupplots}                 

\newlength\figureheight
\newlength\figurewidth

\newcommand{\etal}{\textit{et al}. }
\newcommand{\ie}{\textit{i}.\textit{e}. }
\newcommand{\eg}{\textit{e}.\textit{g}. }

\begin{document} 

\twocolumn[
\icmltitle{Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Federico Monti}{usi}
\icmlauthor{Michael M. Bronstein}{usi,tau,intel,tum}
\icmlauthor{Xavier Bresson}{ntu}
\end{icmlauthorlist}

\icmlaffiliation{usi}{ICS USI Lugano, Switzerland}
\icmlaffiliation{tau}{Tel Aviv University, Israel}
\icmlaffiliation{intel}{Intel Perceptual Computing, Israel}
\icmlaffiliation{ntu}{NTU, Singapore}
\icmlaffiliation{tum}{TUM IAS, Germany}

\icmlcorrespondingauthor{Federico Monti}{federico.monti@usi.ch}

\icmlkeywords{geometric deep learning, matrix completion, recommender systems}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract} 


Matrix completion models are among the most common formulations of recommender
systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationarity structures of user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. This neural network system requires a constant number of parameters independent of the matrix size. We apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques.





















 
\end{abstract} 

\section{Introduction}


Recommender systems have become a central part of modern intelligent systems. Recommending movies on Netflix, friends on Facebook, furniture on Amazon, jobs on LinkedIn are a few examples of the main purpose of these systems. 
Two major approach to recommender systems are collaborative \cite{pro:BreeseHeckermanKadie98CollFilt} and content \cite{art:PazzaniBillsus07ContFilt} filtering techniques. Systems based on collaborative filtering use collected ratings of products by customers and offer new recommendations by finding similar rating patterns. Systems based on content filtering make use of similarities between products and customers to recommend new products. Hybrid systems combine collaborative and content techniques. 



\paragraph*{Matrix completion.}
Mathematically, a recommendation method can be posed as a {\em matrix completion} problem \cite{candes2012exact}, where columns and rows represent users and items, respectively, and matrix values represent a score determining whether a user would like an item or not. Given a small subset of known elements of the matrix, the goal is to fill in the rest. 
A famous example is the Netflix challenge \cite{art:KorenBellVolinsky09MatFac} offered in 2009 and carrying a 1M\$ prize for the algorithm that can best predict user ratings for movies based on previous ratings. The size of the Netflix is 480k movies $\times$ 18k users (8.5B entries), with only 0.011\% known entries. 


Recently, there have been several attempts to incorporate geometric structure into matrix completion problems \cite{art:MaZhouLiuLyuKing11RecomSys,kalofolias2014matrix,rao2015collaborative,kuang2016harmonic}, e.g. in the form of column and row graphs representing similarity of users and items, respectively. Such additional information makes well-defined e.g. the notion of {\em smoothness} of data and was shown beneficial for the performance of recommender systems. 
These approaches can be generally related to the field of {\em signal processing on graphs} \cite{shuman2013emerging}, extending classical harmonic analysis methods to non-Euclidean domains. 


\paragraph*{Geometric deep learning.} 
Of key interest to the design of recommender systems are deep learning approaches. 
In the recent years, deep neural networks and, in particular, convolutional neural networks (CNNs) \cite{lecun1998gradient} have been applied with great success to numerous computer vision-related applications. However, original CNN models cannot be directly applied to the recommendation problem to extract meaningful patterns in users, items and ratings because these data are not Euclidean structured, i.e. they do not lie on regular lattices like images but irregular domains like graphs or manifolds. This strongly motivates the development of {\em geometric deep learning} \cite{review_new} techniques that can mathematically deal with graph-structured data, which arises in numerous applications, ranging from computer graphics and vision \cite{masci2015geodesic,WFT2015,add16,boscaini2016learning,monti2016geometric} to chemistry \cite{duv2015convolutional}. 


The earliest attempts to apply neural networks to graphs are due to Scarselli \etal \citeyear{gori2005new,GNN} (see more recent formulation \cite{GGSNN,comnets}). 
Bruna \etal \citeyear{bruna2013spectral,henaff2015deep} formulated CNN-like  deep neural architectures on graphs in the spectral domain, employing the analogy between the classical Fourier transforms and projections onto the eigenbasis of the graph Laplacian operator \cite{shuman2013emerging}. 
In a follow-up work, Defferrard \etal \citeyear{defferrard2016convolutional} proposed an efficient filtering scheme using recurrent Chebyshev polynomials, which reduces the complexity of CNNs on graphs to the same complexity of standard CNNs (on grids). This model was later extended to deal with dynamic data \cite{seo2016structured}. 
Kipf and Welling \citeyear{welling2016} proposed a simplification of Chebychev networks using simple filters operating on 1-hop neighborhoods of the graph.
Monti \etal \citeyear{monti2016geometric} introduced a spatial-domain generalization of CNNs to graphs local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs. 


\paragraph*{Main contribution.} In this work, we treat matrix completion problem as deep learning on graph-structured data. 
We introduce a novel neural network architecture that is able to extract local stationary patterns from the high-dimensional spaces of users and items, and use these meaningful representations to infer the non-linear temporal diffusion mechanism of ratings. 
The spatial patterns are extracted by a new CNN architecture designed to work on multiple graphs. The temporal dynamics of the rating diffusion is produced by a Long-Short Term Memory (LSTM) recurrent neural network (RNN) \cite{art:HochreiterSchmidhuber97LSTM}. 
To our knowledge, our work is the first application of graph-based deep learning to matrix completion problem. 


The rest of the paper is organized as follows. 
Section \ref{sec:rev} reviews the matrix completion models. Section \ref{sec:our_model} presents the proposed approach. Section \ref{sec:results} presents experimental results demonstrating the efficiency of our techniques on synthetic and real-world datasets, and Section~\ref{sec:conc} concludes the paper. 










 	



\section{Background}
\label{sec:rev}

\subsection{Matrix Completion}
\paragraph*{Matrix completion problem.}
Recovering the missing values of a matrix given a small fraction of its entries is an ill-posed problem without additional mathematical constraints on the space of solutions. A well-posed problem is to assume that the variables lie in a smaller subspace, i.e., that the matrix is of low rank, 
\begin{eqnarray}
\min_{\mathbf{X}} \ \textrm{rank}(\mathbf{X}) \quad \textrm{s.t.} \quad x_{ij}=y_{ij},\  \forall ij \in \Omega,
\label{eq:lowrank}
\end{eqnarray}
 where $\mathbf{X}$ denotes the matrix to recover, $\Omega$ is the set of the known entries and $y_{ij}$ are their values. To make   \eqref{eq:lowrank} robust against noise and perturbation, the equality constraint can be replaced with a penalty 
\begin{eqnarray}
 \min_{\mathbf{X}} \ \textrm{rank}(\mathbf{X}) + \frac{\mu}{2} \| \boldsymbol{\Omega} \circ (\mathbf{X}-\mathbf{Y}) \|_\mathrm{F}^2, 
  \end{eqnarray}
 where $\boldsymbol{\Omega}$ is the indicator matrix of the known entries $\Omega$ and $\circ$ denotes the Hadamard pointwise product. 


Unfortunately, rank minimization turns out an NP-hard  combinatorial problem  that is computationally intractable in practical cases. The tightest possible convex relaxation of the previous problem is  
  \begin{eqnarray}
 \min_{\mathbf{X}}  \ \| \mathbf{X} \|_\star + \frac{\mu}{2} \|\boldsymbol{\Omega} \circ (\mathbf{X}-\mathbf{Y}) \|_\mathrm{F}^2,
  \end{eqnarray}
where $\| \cdot \|_\star$ is the nuclear norm of a matrix equal to the sum of its singular values \cite{art:CandesRecht09MatrixComple}. 
Cand{\`e}s and Recht \citeyear{art:CandesRecht09MatrixComple} proved that the $\ell_1$ relaxation of the SVD lead to solutions that recover almost exactly the original low-rank matrix. 



\paragraph*{Geometric matrix completion}
An alternative relaxation of the rank operator in \eqref{eq:lowrank} is to constraint the space of solutions to be smooth w.r.t. some geometric structure of the matrix rows and columns \cite{art:MaZhouLiuLyuKing11RecomSys,kalofolias2014matrix,rao2015collaborative,art:BenziKalofoliasBressonVandergheynst16NMFTV}. 
The simplest model is proximity structure represented as an undirected weighted column graph 
 $\mathcal{G}_c = (\{1,\hdots, n\}, \mathcal{E}_c, \mathbf{W}_c)$ with {\em adjacency matrix} $\mathbf{W}_c = (w^\mathrm{c}_{ij})$, where $w^\mathrm{c}_{ij} = w^\mathrm{c}_{ji}$, $w^\mathrm{c}_{ij} = 0$ if $(i,j) \notin \mathcal{E}_c$ and $w^\mathrm{c}_{ij} > 0$ if $(i,j) \in \mathcal{E}_c$. 
In our notation, the column graph could be thought of as a social network capturing relations between users and the similarity of their tastes. 
The row graph  $\mathcal{G}_r = (\{1,\hdots, m\}, \mathcal{E}_r, \mathbf{W}_r )$ representing the similarities of the items is defined in a similar manner. 
 
 
On each of these graphs one can construct the (unnormalized) {\em graph Laplacian}, an $n\times n$ symmetric positive-semidefinite matrix $\boldsymbol{\Delta} = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2}$, where $\mathbf{D} = \mathrm{diag}\left(\sum_{j\neq i} w_{ij} \right)$ is the {\em degree matrix}. We denote the Laplacian associated with row and column graphs by $\boldsymbol{\Delta}_r$ and $\boldsymbol{\Delta}_c$, respectively. 
Considering the columns (respectively, rows) of matrix $\mathbf{X}$ as vector-valued functions on the column graph $\mathcal{G}_c$ (respectively, row graph $\mathcal{G}_r$), their smoothness can be expressed as the {\em Dirichlet norm} 
$\| \mathbf{X} \|_{\mathcal{G}_{r}}^2=\mathrm{trace}(\mathbf{X}^\top \boldsymbol{\Delta}_r \mathbf{X})$ (respecitvely, $\| \mathbf{X} \|_{\mathcal{G}_{c}}^2=\mathrm{trace}(\mathbf{X} \boldsymbol{\Delta}_c \mathbf{X}^\top)$). 
The {\em geometric matrix completion} problem thus boils down to minimizing 
 \begin{eqnarray}
\min_\mathbf{X} \ \| \mathbf{X} \|_{\mathcal{G}_r}^2 + \| \mathbf{X} \|_{\mathcal{G}_c}^2 + \frac{\mu}{2} \| \boldsymbol{\Omega} \circ (\mathbf{X} - \mathbf{Y}) \|_\mathrm{F}^2,
\label{eq:Dir}
\end{eqnarray}
























 

\begin{figure}[h!]
\centering
\scalebox{.88}{		
\begin{overpic}
	[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth]{Figures/architecture/matrices.pdf}
			\put(36,43){\footnotesize $n$ users}
			\put(1,75){\rotatebox{90}{\footnotesize $m$ items}}
\put(38,79.5){\footnotesize $\mathbf{X}$}			
			\put(20.5,16){\footnotesize $\mathbf{W}$}	
			\put(50,28){\footnotesize $\mathbf{H}^\top$}
\put(19,100){\footnotesize $j_1$}
			\put(27,100){\footnotesize $j_2$}
			\put(41,100){\footnotesize $\hdots$}
			\put(56.5,100){\footnotesize $j_3$}
\put(63,86){\footnotesize $i_2$}
			\put(63,79){\footnotesize $\vdots$}
			\put(63,74){\footnotesize $i_1$}
\put(48,3){\footnotesize $n$ users}
			\put(1,12){\rotatebox{90}{\footnotesize $m$ items}}
\put(30.5,36.5){\footnotesize $j_1$}
			\put(38.5,36.5){\footnotesize $j_2$}
			\put(52.5,36.5){\footnotesize $\hdots$}
			\put(68.5,36.5){\footnotesize $j_3$}
	\end{overpic}
	}
\caption{Full (top) and factorized (bottom) geometric matrix completion representations. 
The column and row graphs represent the relationships between users and items, respectively.  
 }
\label{fig:matrices}
\end{figure}	



\paragraph*{Factorized models. }
Matrix completion algorithms introduced in the previous section are well-posed as convex optimization problems, guaranteeing existence, uniqueness and robustness of solutions. Besides, fast algorithms have been developed in the context of compressed sensing to solve the non-differential nuclear norm problem. However, the variables in this formulation are the full  $m\times n$ matrix $\mathbf{X}$, making such methods hard to scale up to large matrices such as the notorious Netflix challenge. 

A solution is to use a factorized representation \cite{art:SrebroRennieJaakkola04MatFact,art:KorenBellVolinsky09MatFac,art:MaZhouLiuLyuKing11RecomSys,art:YanezBach14NMF,rao2015collaborative,art:BenziKalofoliasBressonVandergheynst16NMFTV} 
$\mathbf{X}=\mathbf{W}\mathbf{H}^\top$, 
where $\mathbf{W}, \mathbf{H}$ are $m\times r$ and $n\times r$ matrices, respectively, with $r\ll \min(m,n)$. The use of factors $\mathbf{W}, \mathbf{H}$ reduce the number of degrees of freedom from $\mathcal{O}(mn)$ to $\mathcal{O}(m+n)$; this representation is also attractive as solving the matrix completion problem often assumes the original matrix to be low-rank, and $\mathrm{rank}(\mathbf{W}\mathbf{H}^\top)\leq r$ by construction. 
Figure \ref{fig:matrices} shows the full and factorized settings of the matrix completion problem. 


The nuclear norm minimization problem in the previous section can be equivalently rewritten in a factorized form as \cite{art:SrebroRennieJaakkola04MatFact}:
 \begin{eqnarray}
\min_{\mathbf{W},\mathbf{H}} \ \frac{1}{2} \| \mathbf{W} \|_\mathrm{F}^2 + \frac{1}{2} \| \mathbf{H} \|_\mathrm{F}^2 + \frac{\mu}{2} \| \boldsymbol{\Omega} \circ (\mathbf{W}\mathbf{H}^\top - \mathbf{Y}) \|_\mathrm{F}^2.
\label{eq:candes_fact}
\end{eqnarray}
and the factorized formulation of the graph-based minimization problem \eqref{eq:Dir} as 
 \begin{eqnarray}
\min_{\mathbf{W},\mathbf{H}}  \ \frac{1}{2} \| \mathbf{W} \|_{\mathcal{G}_{r}}^2 + \frac{1}{2} \| \mathbf{H} \|_{\mathcal{G}_{c}}^2 + \frac{\mu}{2} \| \boldsymbol{\Omega} \circ (\mathbf{W}\mathbf{H}^\top - \mathbf{Y}) \|_\mathrm{F}^2.
\label{eq:Dir_fact}
\end{eqnarray}
The limitation of model \eqref{eq:Dir_fact} is to decouple the regularization process applied simultaneously on the rows and columns of $\mathbf{X}$ in \eqref{eq:Dir}, but the advantage is linear instead of quadratic complexity. 













 




\subsection{Deep learning on graphs}
The key idea to our work is {\em geometric deep learning}, an extension of the popular CNNs to graphs. 
A graph Laplacian admits a spectral eigendecomposition of the form 
$
\boldsymbol{\Delta} = \boldsymbol{\Phi} \boldsymbol{\Lambda} \boldsymbol{\Phi}^\top
$, 
where $\boldsymbol{\Phi} = (\boldsymbol{\phi}_1, \hdots \boldsymbol{\phi}_n)$ denotes the matrix of orthonormal eigenvectors and $\boldsymbol{\Lambda} = \mathrm{diag}(\lambda_1, \hdots, \lambda_n)$ is the diagonal matrix of the corresponding eigenvalues. The eigenvectors play the role of Fourier atoms in classical harmonic analysis and the eigenvalues can be interpreted as frequencies. 
Given a function $\mathbf{x} = (x_1, \hdots, x_n)^\top$ on the vertices of the graph, its {\em graph Fourier transform} is given by $\hat{\mathbf{x}} = \boldsymbol{\Phi}^\top\mathbf{x}$. 
The {\em spectral convolution} of two functions $\mathbf{x}, \mathbf{y}$ can be defined as the element-wise product of the respective  Fourier transforms,
\begin{equation} 
\label{spectral_conv}
\mathbf{x} \star \mathbf{y} = \boldsymbol{\Phi}(\boldsymbol{\Phi}^\top\mathbf{x}) \circ (\boldsymbol{\Phi}^\top\mathbf{y}) = \boldsymbol{\Phi}\, \mathrm{diag}(\hat{y}_1, \hdots, \hat{y}_n)\,\hat{\mathbf{x}}. 
\end{equation}






Bruna \etal \citeyear{bruna2013spectral} used the spectral definition of convolution~(\ref{spectral_conv}) to generalize CNNs on graphs. 
A spectral convolutional layer has the form 
\begin{equation} 
\label{spectral_construction_eq}
\tilde{\mathbf{x}}_l =   \xi \left(  \sum_{l'=1}^{q'} \boldsymbol{\Phi} \hat{\mathbf{Y}}_{ll'} \boldsymbol{\Phi}^\top \mathbf{x}_{l'} \right), \hspace{3mm} l = 1,\hdots, q,
\end{equation}
where $q', q$ denote the number of input and output channels, respectively,  
$\hat{\mathbf{Y}}_{ll'} = \mathrm{diag}(\hat{y}_{ll',1}, \hdots, \hat{y}_{ll',n})$ is a diagonal matrix of spectral multipliers representing a learnable filter in the spectral domain, and $\xi$ is a nonlinearity (e.g. ReLU) applied on the vertex-wise function values. 
Unlike classical convolutions carried out efficiently in the spectral domain using FFT, the computations of the forward and inverse graph Fourier transform incur expensive $\mathcal{O}(n^2)$ multiplication by the matrices $\boldsymbol{\Phi}, \boldsymbol{\Phi}^\top$, as there are no FFT-like algorithms on general graphs. 
Furthermore, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain, which is an important property of classical CNNs.  




\begin{figure*}[ht!]
\centering
\vspace{1mm}
\scalebox{.8}{
\vspace{5mm}
		\begin{overpic}
	[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth]{Figures/architecture/architecture2.pdf}
			\put(25,26){\footnotesize $\mathbf{X}$}	
\put(55,27){\footnotesize $\mathbf{X}^{(t)}$}	
			\put(76,27){\footnotesize $\tilde{\mathbf{X}}^{(t)}$}	
\put(64,25.25){\footnotesize MGCNN}	
			\put(86.5,25.25){\footnotesize RNN}	
\put(100,33.25){\footnotesize $\mathbf{dX}^{(t)}$}	
			\put(68,42.5){\footnotesize $\mathbf{X}^{(t+1)} = \mathbf{X}^{(t)} + \mathbf{dX}^{(t)}$}	
\put(70,20){\footnotesize \color{gray}{row+column filtering}}
	\end{overpic}\vspace{-2mm}
}
\caption{Recurrent GCNN (RGCNN) architecture using the full matrix completion model and operating simultaneously on the rows and columns of the matrix $\mathbf{X}$. The output of the Multi-Graph CNN (MGCNN) module is a $q$-dimensional feature vector for each element of the input matrix. The number of parameters to learn is $\mathcal{O}(1)$ and the learning complexity is $\mathcal{O}(mn)$.
 }
\label{fig:architectureX}
\end{figure*}	





To address these issues, Defferrard \etal \citeyear{defferrard2016convolutional} proposed using an explicit expansion in the Chebyshev polynomial basis to represent the spectral filters\vspace{-1mm}
\begin{equation} \label{eq:filt_cheby}
	\tau_{\boldsymbol{\theta}}(\tilde{\boldsymbol{\Delta}}) = \sum_{j=0}^{p-1} \theta_j T_j(\tilde{\boldsymbol{\Delta}}) = \sum_{j=0}^{p-1} \theta_j  \boldsymbol{\Phi} T_j(\tilde{\boldsymbol{\Lambda}})\boldsymbol{\Phi}^\top,
\end{equation}
where $\tilde{\boldsymbol{\Delta}} = 2 \lambda_{n}^{-1}\boldsymbol{\Delta}  - \mathbf{I}$ is the rescaled Laplacian such that its eigenvalues $\tilde{\boldsymbol{\Lambda}} = 2 \lambda_{n}^{-1} \boldsymbol{\Lambda}  - \mathbf{I}$ are in the interval $[-1,1]$, 
$\boldsymbol{\theta}$ is the $p$-dimensional vector of polynomial coefficients parametrizing the filter, and $T_j(\lambda) = 2\lambda T_{j-1}(\lambda) - T_{j-2}(\lambda)$ denotes the Chebyshev polynomial of degree $j$ defined in a recursive manner with $T_1(\lambda) =\lambda$ and $T_0(\lambda) =1$. \footnote{$T_j(\lambda) = \cos(j \cos^{-1}(\lambda))$ is an oscillating function on $[-1, 1]$ with $j$ roots, $j+1$ equally spaced extrema, and a frequency linearly dependent on $j$. Chebyshev polynomials form an orthogonal basis for the space of smooth functions on $[-1,1]$ and are thus convenient to compactly represent spectral filters.}
This approach benefits from several advantages. First, it does not require an explicit computation of the Laplacian eigenvectors, and due to the recursive definition of the Chebyshev polynomials, the computation of the filter incurs applying the Laplacian $p$ times. Multiplication by Laplacian has the cost of $\mathcal{O}(|\mathcal{E}|)$, and assuming the graph has $|\mathcal{E}| = \mathcal{O}(n)$ edges (which is the case for $k$-nearest neighbors graphs and most real-world networks), the overall complexity is $\mathcal{O}(n)$ rather than $\mathcal{O}(n^2)$ operations, which is the same complexity than standard CNNs.
Moreover, since the Laplacian is a local operator affecting only 1-hop neighbors of a vertex and accordingly its $(p-1)$st power affects the $p$-hop neighborhood, the resulting filters are spatially localized. 





\section{Our approach}
\label{sec:our_model}

In this paper, we propose formulating matrix completion as a learnable diffusion process applied to the score values. 
The deep learning architecture considered for this purpose consists of a spatial part extracting spatial features from the matrix (we consider two different approaches working on the full and factorized matrix models), and a temporal part using a recurrent LSTM network. The two architectures are summarized in Figures~\ref{fig:architectureX} and~\ref{fig:architectureWH} and described in details in the following.














 
\subsection{Multi-Graph CNNs}






\begin{figure*}[ht!]
\centering
\vspace{1mm}
\scalebox{.8}{
\vspace{5mm}
		\begin{overpic}
	[trim=0cm 0cm 0cm 0cm,clip,width=1\linewidth]{Figures/architecture/architecture1.pdf}
			\put(13.5,19){\footnotesize $\mathbf{W}$}	
			\put(34,27.5){\footnotesize $\mathbf{H}^\top$}	
\put(55,29.5){\footnotesize $\mathbf{H}^{(t)}$}	
			\put(76,29.5){\footnotesize $\tilde{\mathbf{H}}^{(t)}$}	
			\put(55,4.5){\footnotesize $\mathbf{W}^{(t)}$}	
			\put(76,4.5){\footnotesize $\tilde{\mathbf{W}}^{(t)}$}	
\put(64.75,27.75){\footnotesize GCNN}	
			\put(86.5,27.75){\footnotesize RNN}	
\put(64.75,2.75){\footnotesize GCNN}	
			\put(86.5,2.75){\footnotesize RNN}	
\put(100,31.25){\footnotesize $\mathbf{dH}^{(t)}$}	
			\put(100,6.25){\footnotesize $\mathbf{dW}^{(t)}$}
\put(68,11.75){\footnotesize $\mathbf{W}^{(t+1)} = \mathbf{W}^{(t)} + \mathbf{dW}^{(t)}$}	
			\put(68,36.5){\footnotesize $\mathbf{H}^{(t+1)} = \mathbf{H}^{(t)} + \mathbf{dH}^{(t)}$}	
\put(73,-1){\footnotesize {\color{red}{row filtering}}}	
			\put(72,23){\footnotesize {\color{blue}{column filtering}}}	
	\end{overpic}
	}
\caption{Separable Recurrent GCNN (sRGCNN) architecture using the factorized matrix completion model and operating separately on the rows and columns of the factors $\mathbf{W}$, $\mathbf{H}^\top$. The output of the GCNN module is a $q$-dimensional feature vector for each input row/column, respectively. The number of parameters to learn is $\mathcal{O}(1)$ and the learning complexity is $\mathcal{O}(m+n)$.
 }
\label{fig:architectureWH}
\end{figure*}


\paragraph*{Multi-graph convolution. }
Our first goal is to extend the notion of the aforementioned graph Fourier transform to matrices whose rows and columns are defined on row- and column-graphs. We recall that a classical two-dimensional Fourier transform of an image (matrix) can be thought of as applying a one-dimensional Fourier transform to its rows and columns. 
In our setting, the analogy of the two-dimensional Fourier transform has the form
\begin{equation}
\hat{\mathbf{X}} = \boldsymbol{\Phi}_r^\top \mathbf{X} \boldsymbol{\Phi}_c
\end{equation}
where $\boldsymbol{\Phi}_c, \boldsymbol{\Phi}_r$ and $\boldsymbol{\Lambda}_c, \boldsymbol{\Lambda}_r$  denote the $n\times n$ and $m\times m$ eigenvector- and eigenvalue matrices of the column- and row-graph Laplacians $\boldsymbol{\Delta}_c, \boldsymbol{\Delta}_r$, respectively. 
The multi-graph version of the spectral convolution~(\ref{spectral_conv}) is given by 
\begin{equation}
\mathbf{X} \star \mathbf{Y} =   \boldsymbol{\Phi}_r (\hat{\mathbf{X}} \circ \hat{\mathbf{Y}}) \boldsymbol{\Phi}_c^\top. 
\end{equation}
Representing the filters as their spectral multipliers $\hat{\mathbf{Y}}$ would yield $\mathcal{O}(mn)$ parameters, prohibitive in any practical application. 
To overcome this limitation, we resort to the representation of the filters in Chebychev polynomial bases of degree $p$, 
\begin{equation}
	\tau_{\boldsymbol{\Theta}}(\tilde{\lambda}_c, \tilde{\lambda}_r) = \sum_{j,j'=0}^{p} \theta_{jj'} T_j(\tilde{\lambda}_c) T_{j'}(\tilde{\lambda}_r),
\end{equation}
where $\boldsymbol{\Theta} = (\theta_{jj'})$ is the $(p+1)\times(p+1)$ matrix of coefficients, i.e., $\mathcal{O}(1)$ parameters.  
The application of such filters to the matrix $\mathbf{X}$ 
\begin{equation}
\tilde{\mathbf{X}} = \sum_{j,j' = 0}^p \theta_{jj'} T_{j}(\tilde{\boldsymbol{\Delta}}_r) \mathbf{X} T_{j'}(\tilde{\boldsymbol{\Delta}}_c) 
\label{eq:cheb-approx}
\end{equation}
results in $\mathcal{O}(mn)$ complexity. Here, as previously, $\tilde{\boldsymbol{\Delta}}_r$, $\tilde{\boldsymbol{\Delta}}_c$ denote the scaled Laplacians with eigenvalues in the interval $[-1,1]$. 




A {\em Multi-Graph CNN} (MGCNN) using this parametrization of filters~(\ref{eq:cheb-approx}) in the convolutional layer is applied to the $m\times n$ matrix $\mathbf{X}$ (single input channel), producing $q$ outputs (i.e., a tensor of size $m\times n \times q$). 


\paragraph*{Separable convolution. } 
A simplification of the multi-graph convolution is obtained considering the factorized form of the matrix $\mathbf{X} = \mathbf{W}\mathbf{H}^\top$ and applying one-dimensional convolution on the respective graph to each factor, 
\begin{eqnarray}
\tilde{\mathbf{w}}_{l} &=& \sum_{l'=1}^{q'} \boldsymbol{\Phi}_r \hat{\mathbf{Y}}_r^{ll'} \boldsymbol{\Phi}_r^\top{\mathbf{w}}_{l'}, \hspace{3mm} l = 1,\hdots, q\\
\tilde{\mathbf{h}}_{l} &=& \sum_{l'=1}^{q'} \boldsymbol{\Phi}_c \hat{\mathbf{Y}}_c^{ll'} \boldsymbol{\Phi}_c^\top{\mathbf{h}}_{l'}, \hspace{3mm} l = 1,\hdots, q,
\end{eqnarray}

where $\hat{\mathbf{Y}}_r^{ll'} = \mathrm{diag}(\hat{y}^r_{ll',1}, \hdots, \hat{y}^r_{ll',m})$ and $\hat{\mathbf{Y}}_c^{ll'} = \mathrm{diag}(\hat{y}^c_{ll',1}, \hdots, \hat{y}^c_{ll',n})$ are the row- and column-filters resulting in a total of $\mathcal{O}(m+n)$ parameters. 
Similarly to the previous case, we can express the filters resorting to Chebyshev polynomials, 
\begin{eqnarray}
\tilde{\mathbf{w}}_{l} &=& \sum_{l'=1}^{q'} \sum_{j=0}^p \theta_{ll', j}^r T_{j}(\tilde{\boldsymbol{\Delta}}_r) \mathbf{w}_{l'}\\
\tilde{\mathbf{h}}_{l} &=& \sum_{l'=1}^{q'} \sum_{j'=0}^p \theta_{ll', j'}^c T_{j'}(\tilde{\boldsymbol{\Delta}}_c) \mathbf{h}_{l'}
\label{eq:cheb-approxs}
\end{eqnarray}
with $2(p+1)qq'$ parameters in total. 










 
\subsection{Matrix diffusion with RNN}



The next step of our approach is to feed the features extracted from the matrix by the MGCNN (or alternatively, the row- and column-GCNNs) to a Recurrent Neural Network (RNN) implementing the score diffusion process. We use the classical Long-Short Term Memory (LSTM) RNN architecture \cite{art:HochreiterSchmidhuber97LSTM}, which has demonstrated to be highly efficient to learn the dynamical property of data sequences as LSTM is able to keep long-term internal states (in particular, avoiding the vanishing gradient issue). The input of the LSTM gate is given by the static features extracted from the MGCNN, which can be seen as a projection or dimensionality reduction of the original matrix in the space of the most meaningful and representative information (the disentanglement effect). This representation coupled with LSTM appears particularly well-suited to keep a long term internal state, which allows to predict accurate small changes $\mathbf{dX}$ of the matrix $\mathbf{X}$ (or $\mathbf{dW}$, $\mathbf{dH}$ of the factors $\mathbf{W}$, $\mathbf{H}$) that can propagate through the full temporal steps. 



Figures~\ref{fig:architectureX} and~\ref{fig:architectureWH} provides an illustration of the proposed matrix completion model. We also give a precise description of the two settings of our model in Algorithms~\ref{alg:RGCNN} and~\ref{alg:sRGCNN}. 
We refer to the whole architecture combining the MGCNN and RNN in the full matrix completion setting as Recurrent Graph CNN (RGCNN). The factorized version with two GCNNs and RNN is referred to as separable Recurrent Graph CNN (sRGCNN). 

\begin{algorithm}[!t]
\caption{Full matrix completion model using RGCNN}\label{alg:RGCNN}
\begin{algorithmic}[1]
\INPUT $m\times n$ matrix $\mathbf{X}^{(0)}$ containing initial values 
\FOR{$t = 0 : T$}
	\STATE Apply the Multi-Graph CNN \eqref{eq:cheb-approx} on $\mathbf{X}^{(t)}$ producing an $m\times n \times q$ output $\tilde{\mathbf{X}}^{(t)}$ containing a $q$-dimensional feature vector for each matrix element.
	\FOR {all elements $(i,j)$}
		\STATE Apply RNN to feature vector $\tilde{\mathbf{x}}^{(t)}_{ij} = (\tilde{x}^{(t)}_{ij1}, \hdots, \tilde{x}^{(t)}_{ijq})$ producing the predicted incremental value $dx^{(t)}_{ij}$
\ENDFOR
	\STATE Update $\mathbf{X}^{(t+1)} = \mathbf{X}^{(t)} + \mathbf{dX}^{(t)}$
\ENDFOR
\end{algorithmic}
\end{algorithm}



\begin{algorithm}[!t]
\caption{Factorized matrix completion model using sRGCNN}\label{alg:sRGCNN}
\begin{algorithmic}[1]
\INPUT $m\times r$ factor $\mathbf{H}^{(0)}$ and $n\times r$ factor $\mathbf{W}^{(0)}$ representing the matrix $\mathbf{X}^{(0)}$
\FOR{$t = 0 : T$}
	\STATE Apply the Graph CNN on $\mathbf{H}^{(t)}$ producing an $n \times q$ output $\tilde{\mathbf{H}}^{(t)}$.
	\FOR {$j=1:n$}
		\STATE Apply RNN to feature vector $\tilde{\mathbf{h}}^{(t)}_j = (\tilde{h}^{(t)}_{j1}, \hdots, \tilde{h}^{(t)}_{jq})$ producing the predicted incremental value $dh^{(t)}_{j}$
\ENDFOR
	\STATE Update $\mathbf{H}^{(t+1)} = \mathbf{H}^{(t)} + \mathbf{dH}^{(t)}$
\STATE Apply the Graph CNN on $\mathbf{W}^{(t)}$ producing an $m \times q$ output $\tilde{\mathbf{W}}^{(t)}$.
	\FOR {$i=1:m$}
		\STATE Apply RNN to feature vector $\tilde{\mathbf{w}}^{(t)}_i = (\tilde{w}^{(t)}_{i1}, \hdots, \tilde{w}^{(t)}_{iq})$ producing the predicted incremental value $dw^{(t)}_{i}$
\ENDFOR
	\STATE Update $\mathbf{W}^{(t+1)} = \mathbf{W}^{(t)} + \mathbf{dW}^{(t)}$
\ENDFOR
\end{algorithmic}
\end{algorithm}




The complexity of Algorithm \ref{alg:RGCNN} scales quadratically as $\mathcal{O}(mn)$ due to the use of MGCNN. For large matrices, we can opt for Algorithm \ref{alg:sRGCNN} that processes the rows and columns separately with standard GCNNs and scales linearly as $\mathcal{O}(m+n)$. 










 

\subsection{Training}
Training of the networks is performed by minimizing the loss 
\begin{equation}
\ell(\boldsymbol{\Theta},\boldsymbol{\sigma}) = 
\| \mathbf{X}^{(T)}_{\boldsymbol{\Theta},\boldsymbol{\sigma}} \|_{\mathcal{G}_r}^2 + \| \mathbf{X}^{(T)}_{\boldsymbol{\Theta},\boldsymbol{\sigma}} \|_{\mathcal{G}_c}^2 + \frac{\mu}{2} \| \boldsymbol{\Omega}\circ (\mathbf{X}^{(T)}_{\boldsymbol{\Theta},\boldsymbol{\sigma}} - \mathbf{Y}) \|_\mathrm{F}^2. 
\end{equation}
Here, $T$ denotes the number of diffusion iterations (applications of the RNN), and we use the notation $\mathbf{X}^{(T)}_{\boldsymbol{\Theta},\boldsymbol{\sigma}}$ to emphasize that the matrix depends on the parameters of the MGCNN (Chebyshev polynomial coefficients $\boldsymbol{\Theta}$) and those of the LSTM (denoted by $\boldsymbol{\sigma}$). 

In the factorized setting, we use the loss 
\begin{eqnarray}
\ell(\boldsymbol{\theta}_r, \boldsymbol{\theta}_c,\boldsymbol{\sigma}) &=& 
\| \mathbf{W}^{(T)}_{\boldsymbol{\theta}_r, \boldsymbol{\sigma}} \|_{\mathcal{G}_r}^2 + \| \mathbf{H}^{(T)}_{\boldsymbol{\theta}_c,\boldsymbol{\sigma}} \|_{\mathcal{G}_c}^2  \\
&+& \frac{\mu}{2} \| \boldsymbol{\Omega}\circ (\mathbf{W}^{(T)}_{\boldsymbol{\theta}_r,\boldsymbol{\sigma}} (\mathbf{H}^{(T)}_{\boldsymbol{\theta}_c,\boldsymbol{\sigma}} )^\top - \mathbf{Y}) \|_\mathrm{F}^2. \nonumber
\end{eqnarray}
where $\boldsymbol{\theta}_c, \boldsymbol{\theta}_r$ are the parameters of the two GCNNs. 

\section{Results}
\label{sec:results}

\label{sec:results}

\paragraph*{Experimental settings.} 
We closely followed the experimental setup of \cite{rao2015collaborative}, using five standard datasets: Synthetic dataset from \cite{kalofolias2014matrix}, MovieLens \cite{miller2003movielens}, Flixster \cite{jamali2010matrix}, Douban \cite{art:MaZhouLiuLyuKing11RecomSys}, and YahooMusic \cite{dror2012yahoo}. 
Classical Matrix Completion (MC) \cite{candes2012exact}, Inductive Matrix Completion (IMC) \cite{jain2013provable,xu2013speedup}, Geometric Matrix Completion (GMC) \cite{kalofolias2014matrix}, and Graph Regularized Alternating Least Squares (GRALS) \cite{rao2015collaborative} were used as baseline methods.  


In all the experiments, we used the following settings for our RGCNNs: Chebyshev polynomials of order $p=5$, outputting $k=32$-dimensional features, LSTM cells with $32$ features and $T=10$ diffusion steps. All the models were implemented in Google TensorFlow and trained using the Adam stochastic optimization algorithm \cite{KingmaB14} with learning rate $10^{-3}$. In factorized models, rank $r=15$ and $10$ was used for the synthetic and real datasets, respectively. 
For all methods, hyperparameters were chosen by cross-validation. 




\subsection{Synthetic data}
We start our experimental evaluation showing the performance of our approach on a small synthetic dataset, in which the user and item graphs have strong communities structure. Though rather simple, such a dataset allows to study the behavior of different algorithms in controlled settings. 
The performance of different matrix completion methods is reported in Table \ref{tab:results-synthetic-netflix}, along with their theoretical complexity. 
Our RGCNN model achieves the best accuracy, followed by the separable RGCNN. 
Different diffusion time steps of these two models are visualized in Figure \ref{fig:X-evolutions}. 
Figure \ref{fig:convergence-rate} shows the convergence rates of different methods. Figures \ref{fig:MGCNN-spectral-filter} and \ref{fig:GCNN-spectral-filter} depict the spectral filters learnt by the MGCNN and row- and column-GCNNs. 


We repeated the same experiment considering only the column (users) graph to be given. In this setting, the RGCNN cannot be applied, while the sRGCNN has only one GCNN applied on the factor $\mathbf{H}$, and the other factor $\mathbf{W}$ is free. 
Table~\ref{tab:results-synthetic-netflix-one-graph} summarizes the results of this experiment, again, showing that our approach performs the best. 




\begin{figure*}[!ht]
\centering
\setlength\figureheight{2.6cm}
\setlength\figurewidth{3.05cm}

\begin{minipage}[h]{1.0\linewidth}
\vspace{1mm}
\footnotesize
\vspace{-3mm}
\begin{minipage}[h]{0.085\linewidth}
\center
t = 0
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
1
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
2
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
3
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
4
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
5
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
6
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
7
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
8
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
9
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
10
\end{minipage}
\end{minipage}
\begin{tikzpicture}
\begin{groupplot}[
     group style = {group size = 11 by 1, horizontal sep=0.1cm}]
\nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_0/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_1/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_2/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_3/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_4/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_5/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_6/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_7/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_8/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_9/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X/time_10/X1.png};

   \end{groupplot}
\end{tikzpicture}
\begin{minipage}[h]{1.0\linewidth}
\footnotesize
\vspace{-3mm}
\begin{minipage}[h]{0.085\linewidth}
\center
2.26
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 1.89
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 1.60
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 1.78
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 1.31
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.52
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.48
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.63
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.38
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.07
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.01
\end{minipage}
\end{minipage}
\begin{tikzpicture}
\begin{groupplot}[
     group style = {group size = 11 by 1, horizontal sep=0.1cm}]
\nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_0/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_1/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_2/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_3/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_4/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_5/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_6/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_7/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_8/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_9/X1.png};

 \nextgroupplot[
xmin=-0.5, xmax=199.5,
ymin=-0.5, ymax=149.5,
axis on top,
xtick=\empty,
ytick=\empty,
width=\figurewidth,
height=\figureheight
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=199.5, ymin=149.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/evolutions_X_2_diff_conv/time_10/X1.png};

   \end{groupplot}
\end{tikzpicture}
\begin{minipage}[h]{1.0\linewidth}
\footnotesize
\vspace{-3mm}
\begin{minipage}[h]{0.085\linewidth}
\center
1.15
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 1.04
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.94
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.89
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.84
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.76
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.69
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.49
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.27
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.11
\end{minipage}
\hfill
\begin{minipage}[h]{0.085\linewidth}
\center
 0.01
\end{minipage}
\end{minipage}

\caption{Evolution of the matrix $\mathbf{X}^{(t)}$ with our architecture using full matrix completion model RGCNN (top) and factorized matrix completion model sRGCNN (bottom). Numbers indicate the RMS error. 
}
\label{fig:X-evolutions}
\end{figure*}
 
\begin{table}[!ht]
\caption{
Comparison of different matrix completion methods using {\it users+items graphs} in terms of number of parameters (optimization variables) and computational complexity order (operations per iteration). Rightmost column shows the RMS error on Synthetic dataset. 
}
\label{tab:results-synthetic-netflix}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\hline
\abovespace\belowspace
Method & Parameters & Complexity & RMSE\\
\hline
\abovespace
GMC   & $m n $ & $m n$ & 0.3693   \\
GRALS & $m+n$ & $m+n$ & 0.0114 \\
{\bf RGCNN}    & $\mathbf{1}$ & $\mathbf{m n}$ & {\bf 0.0053} \\
{\bf  sRGCNN}  & $\mathbf{1}$ &$\mathbf{m+n}$ & {\bf 0.0106} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[!ht]
\caption{Comparison of different matrix completion methods using {\it users graph only} in terms of number of parameters (optimization variables) and computational complexity order (operations per iteration). Rightmost column shows the RMS error on Synthetic dataset.}
\label{tab:results-synthetic-netflix-one-graph}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\hline
\abovespace\belowspace
Method & Parameters & Complexity & RMSE \\
\hline
\abovespace
GRALS & $m+n$ & $m+n$  & 0.0452 \\
{\bf sRGCNN} & $\mathbf{m}$ & $\mathbf{m+n}$ & {\bf 0.0362}\\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{figure}[!ht]
\setlength\figureheight{5.5cm}
\setlength\figurewidth{8.5cm}
\begin{tikzpicture}

\definecolor{color0}{rgb}{0,0.75,0.75}

\begin{axis}[
xlabel={Training iterations},
ylabel={RMSE},
xmin=0, xmax=50000,
ymin=7.87123281043023e-05, ymax=0.03,
ymode=log,
axis on top,
width=\figurewidth,
height=\figureheight,
legend entries={{GMC},{GRALS},{sRGCNN},{RGCNN}},
legend cell align={left},
legend style={font=\fontsize{7}{4}\selectfont, row sep=-1pt}
]
\addplot [blue]
table {0 0.00908201094716787
100 0.00863501150161028
200 0.00822887383401394
300 0.00785900559276342
400 0.00752236833795905
500 0.00721672410145402
600 0.00693994667381048
700 0.00668994151055813
800 0.00646465225145221
900 0.00626206444576383
1000 0.00608020089566708
1100 0.00591712305322289
1200 0.00577092776075006
1300 0.00563975190743804
1400 0.00552178453654051
1500 0.00541527662426233
1600 0.00531856017187238
1700 0.00523005845025182
1800 0.0051483022980392
1900 0.00507193943485618
2000 0.00499973772093654
2100 0.00493059400469065
2200 0.00486353132873774
2300 0.00479769613593817
2400 0.00473235826939344
2500 0.00466690538451076
2600 0.00460083549842238
2700 0.00453375466167927
2800 0.00446536857634783
2900 0.00439547700807452
3000 0.00432396866381168
3100 0.00425081187859178
3200 0.00417605182155967
3300 0.00409980118274689
3400 0.00402223598212004
3500 0.00394358672201633
3600 0.00386413326486945
3700 0.00378419528715312
3800 0.00370412901975214
3900 0.00362431537359953
4000 0.00354515388607979
4100 0.00346705527044833
4200 0.00339043070562184
4300 0.00331568554975092
4400 0.00324321095831692
4500 0.00317337433807552
4600 0.00310651399195194
4700 0.00304292887449265
4800 0.00298287463374436
4900 0.00292655802331865
5000 0.00287413247860968
5100 0.00282569485716522
5200 0.00278128776699305
5300 0.00274089677259326
5400 0.00270445551723242
5500 0.00267184735275805
5600 0.00264291511848569
5700 0.00261746253818274
5800 0.00259526562876999
5900 0.00257607852108777
6000 0.00255964184179902
6100 0.00254569016396999
6200 0.00253395759500563
6300 0.0025241847615689
6400 0.00251612346619368
6500 0.00250954087823629
6600 0.0025042206980288
6700 0.00249996641650796
6800 0.00249660317786038
6900 0.00249397498555481
7000 0.00249194633215666
7100 0.00249040126800537
7200 0.0024892408400774
7300 0.00248838309198618
7400 0.00248776003718376
7500 0.00248731579631567
7600 0.00248700683005154
7700 0.0024867975153029
7800 0.00248666014522314
7900 0.00248657469637692
8000 0.0024865239392966
8100 0.00248649693094194
8200 0.00248648505657911
8300 0.00248648202978075
8400 0.00248648575507104
8500 0.00248649204149842
8600 0.00248649925924838
8700 0.00248650554567575
8800 0.0024865111336112
8900 0.00248651788569987
9000 0.00248652254231274
9100 0.00248652580194175
9200 0.00248652906157076
9300 0.00248653069138527
9400 0.00248653185553849
9500 0.00248653255403042
9600 0.00248653325252235
9700 0.00248653395101428
9800 0.00248653464950621
9900 0.00248653464950621
10000 0.00248653488233685
10100 0.00248653464950621
10200 0.00248653488233685
10300 0.00248653488233685
10400 0.00248653488233685
10500 0.00248653464950621
10600 0.00248653488233685
10700 0.00248653464950621
10800 0.00248653464950621
10900 0.00248653488233685
11000 0.00248653464950621
11100 0.00248653464950621
11200 0.00248653464950621
11300 0.00248653464950621
11400 0.00248653464950621
11500 0.00248653464950621
11600 0.00248653464950621
11700 0.00248653464950621
11800 0.00248653464950621
11900 0.00248653464950621
12000 0.00248653488233685
12100 0.00248653464950621
12200 0.00248653464950621
12300 0.00248653464950621
12400 0.00248653464950621
12500 0.00248653464950621
12600 0.00248653464950621
12700 0.00248653464950621
12800 0.00248653464950621
12900 0.00248653464950621
13000 0.00248653464950621
13100 0.00248653464950621
13200 0.00248653464950621
13300 0.00248653464950621
13400 0.00248653464950621
13500 0.00248653464950621
13600 0.00248653464950621
13700 0.00248653464950621
13800 0.00248653464950621
13900 0.00248653464950621
14000 0.00248653464950621
14100 0.00248653464950621
14200 0.00248653464950621
14300 0.00248653464950621
14400 0.00248653464950621
14500 0.00248653464950621
14600 0.00248653464950621
14700 0.00248653464950621
14800 0.00248653464950621
14900 0.00248653464950621
15000 0.00248653464950621
15100 0.00248653464950621
15200 0.00248653464950621
15300 0.00248653464950621
15400 0.00248653464950621
15500 0.00248653464950621
15600 0.00248653464950621
15700 0.00248653464950621
15800 0.00248653464950621
15900 0.00248653464950621
16000 0.00248653464950621
16100 0.00248653464950621
16200 0.00248653464950621
16300 0.00248653464950621
16400 0.00248653464950621
16500 0.00248653464950621
16600 0.00248653464950621
16700 0.00248653464950621
16800 0.00248653464950621
16900 0.00248653464950621
17000 0.00248653464950621
17100 0.00248653464950621
17200 0.00248653464950621
17300 0.00248653464950621
17400 0.00248653464950621
17500 0.00248653464950621
17600 0.00248653464950621
17700 0.00248653464950621
17800 0.00248653464950621
17900 0.00248653464950621
18000 0.00248653464950621
18100 0.00248653464950621
18200 0.00248653464950621
18300 0.00248653464950621
18400 0.00248653464950621
18500 0.00248653464950621
18600 0.00248653464950621
18700 0.00248653464950621
18800 0.00248653464950621
18900 0.00248653464950621
19000 0.00248653464950621
19100 0.00248653464950621
19200 0.00248653464950621
19300 0.00248653464950621
19400 0.00248653464950621
19500 0.00248653464950621
19600 0.00248653464950621
19700 0.00248653464950621
19800 0.00248653464950621
19900 0.00248653441667557
20000 0.00248653464950621
20100 0.00248653464950621
20200 0.00248653464950621
20300 0.00248653464950621
20400 0.00248653464950621
20500 0.00248653464950621
20600 0.00248653464950621
20700 0.00248653464950621
20800 0.00248653464950621
20900 0.00248653441667557
21000 0.00248653464950621
21100 0.00248653488233685
21200 0.00248653464950621
21300 0.00248653464950621
21400 0.00248653464950621
21500 0.00248653464950621
21600 0.00248653464950621
21700 0.00248653464950621
21800 0.00248653464950621
21900 0.00248653464950621
22000 0.00248653464950621
22100 0.00248653441667557
22200 0.00248653464950621
22300 0.00248653464950621
22400 0.00248653488233685
22500 0.00248653464950621
22600 0.00248653464950621
22700 0.00248653464950621
22800 0.00248653464950621
22900 0.00248653464950621
23000 0.00248653464950621
23100 0.00248653464950621
23200 0.00248653464950621
23300 0.00248653464950621
23400 0.00248653464950621
23500 0.00248653464950621
23600 0.0024865351151675
23700 0.00248653464950621
23800 0.00248653464950621
23900 0.00248653441667557
24000 0.00248653464950621
24100 0.00248653464950621
24200 0.00248653464950621
24300 0.00248653464950621
24400 0.00248653464950621
24500 0.00248653464950621
24600 0.00248653464950621
24700 0.00248653464950621
24800 0.00248653464950621
24900 0.00248653441667557
25000 0.00248653464950621
25100 0.00248653464950621
25200 0.00248653464950621
25300 0.00248653464950621
25400 0.00248653488233685
25500 0.00248653464950621
25600 0.00248653488233685
25700 0.00248653441667557
25800 0.00248653488233685
25900 0.00248653441667557
26000 0.00248653418384492
26100 0.00248653464950621
26200 0.00248653464950621
26300 0.00248653441667557
26400 0.00248653464950621
26500 0.00248653464950621
26600 0.00248653464950621
26700 0.00248653464950621
26800 0.00248653488233685
26900 0.00248653464950621
27000 0.00248653464950621
27100 0.00248653464950621
27200 0.00248653464950621
27300 0.00248653441667557
27400 0.00248653488233685
27500 0.00248653441667557
27600 0.00248653464950621
27700 0.00248653441667557
27800 0.0024865351151675
27900 0.00248653441667557
28000 0.00248653464950621
28100 0.00248653464950621
28200 0.00248653488233685
28300 0.00248653488233685
28400 0.00248653418384492
28500 0.00248653441667557
28600 0.00248653488233685
28700 0.00248653464950621
28800 0.00248653464950621
28900 0.00248653418384492
29000 0.00248653464950621
29100 0.00248653464950621
29200 0.00248653464950621
29300 0.00248653464950621
29400 0.00248653464950621
29500 0.00248653464950621
29600 0.00248653464950621
29700 0.00248653464950621
29800 0.00248653488233685
29900 0.0024865351151675
30000 0.00248653418384492
30100 0.00248653441667557
30200 0.00248653488233685
30300 0.00248653464950621
30400 0.00248653418384492
30500 0.00248653464950621
30600 0.00248653464950621
30700 0.00248653464950621
30800 0.00248653464950621
30900 0.00248653464950621
31000 0.00248653464950621
31100 0.00248653464950621
31200 0.00248653488233685
31300 0.00248653441667557
31400 0.00248653464950621
31500 0.00248653464950621
31600 0.00248653464950621
31700 0.00248653464950621
31800 0.00248653464950621
31900 0.00248653464950621
32000 0.0024865351151675
32100 0.00248653488233685
32200 0.00248653464950621
32300 0.00248653464950621
32400 0.00248653464950621
32500 0.00248653464950621
32600 0.00248653488233685
32700 0.00248653464950621
32800 0.00248653464950621
32900 0.00248653464950621
33000 0.00248653464950621
33100 0.00248653464950621
33200 0.00248653464950621
33300 0.00248653464950621
33400 0.00248653418384492
33500 0.00248653464950621
33600 0.00248653464950621
33700 0.00248653464950621
33800 0.00248653464950621
33900 0.00248653464950621
34000 0.00248653441667557
34100 0.00248653464950621
34200 0.00248653464950621
34300 0.00248653441667557
34400 0.00248653464950621
34500 0.00248653441667557
34600 0.00248653488233685
34700 0.00248653441667557
34800 0.00248653464950621
34900 0.00248653488233685
35000 0.00248653488233685
35100 0.00248653464950621
35200 0.00248653464950621
35300 0.00248653488233685
35400 0.00248653488233685
35500 0.00248653464950621
35600 0.00248653464950621
35700 0.00248653488233685
35800 0.00248653464950621
35900 0.00248653464950621
36000 0.00248653464950621
36100 0.00248653464950621
36200 0.00248653441667557
36300 0.00248653464950621
36400 0.00248653441667557
36500 0.00248653464950621
36600 0.00248653464950621
36700 0.00248653464950621
36800 0.00248653488233685
36900 0.00248653464950621
37000 0.00248653464950621
37100 0.00248653464950621
37200 0.00248653464950621
37300 0.00248653464950621
37400 0.00248653464950621
37500 0.00248653464950621
37600 0.00248653464950621
37700 0.00248653441667557
37800 0.00248653464950621
37900 0.00248653464950621
38000 0.00248653464950621
38100 0.00248653464950621
38200 0.00248653464950621
38300 0.00248653441667557
38400 0.00248653464950621
38500 0.00248653464950621
38600 0.00248653488233685
38700 0.00248653464950621
38800 0.00248653441667557
38900 0.00248653488233685
39000 0.00248653464950621
39100 0.00248653441667557
39200 0.00248653488233685
39300 0.00248653464950621
39400 0.00248653464950621
39500 0.00248653464950621
39600 0.00248653464950621
39700 0.00248653488233685
39800 0.00248653464950621
39900 0.00248653464950621
40000 0.00248653464950621
40100 0.00248653464950621
40200 0.00248653464950621
40300 0.0024865351151675
40400 0.00248653464950621
40500 0.00248653488233685
40600 0.00248653464950621
40700 0.00248653464950621
40800 0.00248653464950621
40900 0.00248653441667557
41000 0.00248653464950621
41100 0.00248653441667557
41200 0.00248653488233685
41300 0.00248653464950621
41400 0.00248653464950621
41500 0.00248653441667557
41600 0.00248653464950621
41700 0.00248653464950621
41800 0.00248653464950621
41900 0.00248653464950621
42000 0.00248653464950621
42100 0.00248653464950621
42200 0.00248653464950621
42300 0.00248653464950621
42400 0.00248653464950621
42500 0.00248653464950621
42600 0.00248653464950621
42700 0.00248653464950621
42800 0.00248653464950621
42900 0.00248653464950621
43000 0.00248653441667557
43100 0.00248653441667557
43200 0.00248653464950621
43300 0.00248653488233685
43400 0.00248653464950621
43500 0.00248653464950621
43600 0.00248653464950621
43700 0.00248653418384492
43800 0.00248653488233685
43900 0.00248653441667557
44000 0.00248653464950621
44100 0.00248653488233685
44200 0.00248653441667557
44300 0.00248653464950621
44400 0.00248653488233685
44500 0.00248653464950621
44600 0.00248653464950621
44700 0.00248653441667557
44800 0.00248653464950621
44900 0.00248653464950621
45000 0.00248653488233685
45100 0.00248653441667557
45200 0.00248653464950621
45300 0.00248653464950621
45400 0.00248653464950621
45500 0.00248653488233685
45600 0.00248653418384492
45700 0.00248653488233685
45800 0.00248653464950621
45900 0.00248653488233685
46000 0.00248653464950621
46100 0.00248653464950621
46200 0.00248653488233685
46300 0.00248653464950621
46400 0.00248653464950621
46500 0.00248653488233685
46600 0.00248653441667557
46700 0.00248653464950621
46800 0.00248653464950621
46900 0.00248653464950621
47000 0.00248653464950621
47100 0.00248653464950621
47200 0.00248653464950621
47300 0.00248653464950621
47400 0.00248653464950621
47500 0.00248653441667557
47600 0.00248653464950621
47700 0.00248653464950621
47800 0.00248653464950621
47900 0.00248653464950621
48000 0.00248653464950621
48100 0.00248653464950621
48200 0.00248653441667557
48300 0.00248653464950621
48400 0.00248653441667557
48500 0.00248653464950621
48600 0.00248653464950621
48700 0.00248653441667557
48800 0.00248653464950621
48900 0.00248653464950621
49000 0.00248653464950621
49100 0.00248653441667557
49200 0.00248653441667557
49300 0.00248653464950621
49400 0.00248653488233685
49500 0.0024865351151675
49600 0.00248653464950621
49700 0.00248653441667557
49800 0.00248653464950621
49900 0.00248653464950621
};
\addplot [green!50.0!black]
table {0 1.96492886543274
100 1.88448584079742
200 1.798672914505
300 1.7185423374176
400 1.64367210865021
500 1.57360279560089
600 1.50792193412781
700 1.44625973701477
800 1.38828408718109
900 1.33369624614716
1000 1.28222680091858
1100 1.23363256454468
1200 1.1876939535141
1300 1.144211769104
1400 1.10300529003143
1500 1.06391036510468
1600 1.02677762508392
1700 0.991470813751221
1800 0.957865655422211
1900 0.925848543643951
2000 0.895315110683441
2100 0.866170167922974
2200 0.838325977325439
2300 0.811702013015747
2400 0.786223948001862
2500 0.76182359457016
2600 0.738437592983246
2700 0.716007769107819
2800 0.694480121135712
2900 0.673804581165314
3000 0.653934895992279
3100 0.634827792644501
3200 0.616443336009979
3300 0.598744332790375
3400 0.581695914268494
3500 0.565265774726868
3600 0.54942375421524
3700 0.534141480922699
3800 0.519392669200897
3900 0.505152404308319
4000 0.491397619247437
4100 0.478106498718262
4200 0.465258628129959
4300 0.452834725379944
4400 0.440816730260849
4500 0.429187685251236
4600 0.417931467294693
4700 0.407032996416092
4800 0.396477878093719
4900 0.386252701282501
5000 0.376344680786133
5100 0.36674165725708
5200 0.357432216405869
5300 0.348405450582504
5400 0.339651107788086
5500 0.331159383058548
5600 0.322920918464661
5700 0.314926952123642
5800 0.307168960571289
5900 0.299639016389847
6000 0.292329430580139
6100 0.285232871770859
6200 0.278342455625534
6300 0.271651417016983
6400 0.265153497457504
6500 0.258842527866364
6600 0.252712726593018
6700 0.24675852060318
6800 0.240974560379982
6900 0.235355660319328
7000 0.229896917939186
7100 0.224593609571457
7200 0.219441145658493
7300 0.214435204863548
7400 0.209571525454521
7500 0.204846024513245
7600 0.200254827737808
7700 0.195794120430946
7800 0.191460281610489
7900 0.187249794602394
8000 0.183159217238426
8100 0.179185271263123
8200 0.175324767827988
8300 0.171574622392654
8400 0.1679318100214
8500 0.164393454790115
8600 0.160956725478172
8700 0.157618850469589
8800 0.154377236962318
8900 0.151229217648506
9000 0.148172318935394
9100 0.145204082131386
9200 0.142322108149529
9300 0.139524042606354
9400 0.136807650327682
9500 0.134170681238174
9600 0.131610974669456
9700 0.1291264295578
9800 0.126714944839478
9900 0.124374508857727
10000 0.122103132307529
10100 0.119898915290833
10200 0.117759898304939
10300 0.115684270858765
10400 0.113670185208321
10500 0.111715883016586
10600 0.109819583594799
10700 0.107979580760002
10800 0.106194213032722
10900 0.104461826384068
11000 0.102780804038048
11100 0.101149566471577
11200 0.0995665639638901
11300 0.0980302765965462
11400 0.0965392217040062
11500 0.0950919538736343
11600 0.0936870351433754
11700 0.0923230648040771
11800 0.0909987017512321
11900 0.0897125899791718
12000 0.0884634330868721
12100 0.0872499421238899
12200 0.0860709100961685
12300 0.0849250853061676
12400 0.0838112905621529
12500 0.0827284008264542
12600 0.081675261259079
12700 0.0806507766246796
12800 0.0796539038419724
12900 0.0786835923790932
13000 0.077738843858242
13100 0.0768186748027802
13200 0.0759221613407135
13300 0.0750483497977257
13400 0.0741963908076286
13500 0.0733653977513313
13600 0.0725545585155487
13700 0.0717630609869957
13800 0.0709901303052902
13900 0.0702350288629532
14000 0.0694970265030861
14100 0.0687754452228546
14200 0.0680696070194244
14300 0.0673788785934448
14400 0.0667026489973068
14500 0.0660403221845627
14600 0.065391331911087
14700 0.0647551417350769
14800 0.0641312375664711
14900 0.063519112765789
15000 0.0629182979464531
15100 0.0623283423483372
15200 0.0617488250136375
15300 0.0611793212592602
15400 0.0606194399297237
15500 0.0600688122212887
15600 0.0595270842313766
15700 0.0589939169585705
15800 0.0584689863026142
15900 0.0579519867897034
16000 0.0574426390230656
16100 0.0569406598806381
16200 0.0564457811415195
16300 0.0559577718377113
16400 0.0554763861000538
16500 0.0550014078617096
16600 0.0545326210558414
16700 0.0540698356926441
16800 0.0536128543317318
16900 0.0531615018844604
17000 0.0527156069874763
17100 0.0522750094532967
17200 0.0518395639955997
17300 0.0514091216027737
17400 0.0509835444390774
17500 0.0505627132952213
17600 0.0501465015113354
17700 0.0497347936034203
17800 0.0493274889886379
17900 0.0489244721829891
18000 0.0485256500542164
18100 0.0481309480965137
18200 0.0477402582764626
18300 0.0473535135388374
18400 0.0469706282019615
18500 0.0465915352106094
18600 0.0462161637842655
18700 0.045844454318285
18800 0.0454763397574425
18900 0.0451117679476738
19000 0.0447506792843342
19100 0.04439302906394
19200 0.0440387725830078
19300 0.043687853962183
19400 0.0433402359485626
19500 0.0429958775639534
19600 0.0426547452807426
19700 0.0423167981207371
19800 0.041982002556324
19900 0.0416503250598907
20000 0.0413217358291149
20100 0.0409962013363838
20200 0.0406736992299557
20300 0.0403541922569275
20400 0.0400376617908478
20500 0.0397240743041039
20600 0.0394134074449539
20700 0.0391056425869465
20800 0.0388007387518883
20900 0.0384986810386181
21000 0.0381994470953941
21100 0.0379030033946037
21200 0.0376093350350857
21300 0.0373184084892273
21400 0.0370302014052868
21500 0.0367446914315224
21600 0.0364618524909019
21700 0.0361816547811031
21800 0.0359040722250938
21900 0.0356290824711323
22000 0.0353566557168961
22100 0.0350867584347725
22200 0.0348193682730198
22300 0.0345544628798962
22400 0.0342920050024986
22500 0.0340319685637951
22600 0.0337743274867535
22700 0.0335190407931805
22800 0.0332660898566246
22900 0.0330154336988926
23000 0.0327670536935329
23100 0.0325209088623524
23200 0.0322769656777382
23300 0.0320352055132389
23400 0.0317955873906612
23500 0.0315580777823925
23600 0.0313226468861103
23700 0.0310892630368471
23800 0.0308578927069902
23900 0.030628502368927
24000 0.030401062220335
24100 0.0301755368709564
24200 0.0299518946558237
24300 0.0297301020473242
24400 0.0295101292431355
24500 0.0292919371277094
24600 0.0290754996240139
24700 0.0288607813417912
24800 0.0286477524787188
24900 0.0284363757818937
25000 0.028226625174284
25100 0.0280184634029865
25200 0.027811860665679
25300 0.0276067852973938
25400 0.0274032019078732
25500 0.0272010862827301
25600 0.0270004011690617
25700 0.0268011204898357
25800 0.0266032088547945
25900 0.0264066383242607
26000 0.0262113772332668
26100 0.0260173976421356
26200 0.0258246678858995
26300 0.0256331600248814
26400 0.025442847982049
26500 0.0252536982297897
26600 0.0250656846910715
26700 0.0248787812888622
26800 0.0246929563581944
26900 0.0245081856846809
27000 0.0243244431912899
27100 0.0241416990756989
27200 0.0239599365741014
27300 0.023779122158885
27400 0.0235992334783077
27500 0.0234202444553375
27600 0.0232421346008778
27700 0.0230648778378963
27800 0.0228884536772966
27900 0.0227128341794014
28000 0.0225380044430494
28100 0.0223639365285635
28200 0.0221906136721373
28300 0.0220180079340935
28400 0.0218461100012064
28500 0.0216748882085085
28600 0.0215043276548386
28700 0.0213344097137451
28800 0.0211651138961315
28900 0.0209964234381914
29000 0.0208283141255379
29100 0.0206607766449451
29200 0.0204937867820263
29300 0.0203273259103298
29400 0.02016138471663
29500 0.0199959371238947
29600 0.0198309738188982
29700 0.0196664743125439
29800 0.0195024218410254
29900 0.0193388033658266
30000 0.0191756058484316
30100 0.0190128069370985
30200 0.0188503991812468
30300 0.0186883676797152
30400 0.0185266938060522
30500 0.0183653682470322
30600 0.0182043742388487
30700 0.0180437043309212
30800 0.0178833436220884
30900 0.0177232790738344
31000 0.0175634995102882
31100 0.0174039956182241
31200 0.0172447562217712
31300 0.0170857720077038
31400 0.0169270318001509
31500 0.0167685300111771
31600 0.0166102573275566
31700 0.0164522044360638
31800 0.0162943676114082
31900 0.0161367375403643
32000 0.0159793104976416
32100 0.0158220827579498
32200 0.0156650468707085
32300 0.01550820376724
32400 0.0153515487909317
32500 0.0151950819417834
32600 0.0150387976318598
32700 0.0148827023804188
32800 0.0147267943248153
32900 0.0145710716024041
33000 0.0144155425950885
33100 0.0142602082341909
33200 0.0141050713136792
33300 0.0139501411467791
33400 0.0137954195961356
33500 0.0136409178376198
33600 0.0134866405278444
33700 0.0133325979113579
33800 0.013178801164031
33900 0.0130252595990896
34000 0.0128719853237271
34100 0.0127189913764596
34200 0.0125662889331579
34300 0.012413895688951
34400 0.0122618228197098
34500 0.0121100889518857
34600 0.0119587080553174
34700 0.0118076987564564
34800 0.0116570796817541
34900 0.0115068648010492
35000 0.0113570764660835
35100 0.0112077314406633
35200 0.0110588502138853
35300 0.0109104514122009
35400 0.0107625564560294
35500 0.0106151830404997
35600 0.0104683516547084
35700 0.0103220827877522
35800 0.0101763941347599
35900 0.0100313080474734
36000 0.0098868403583765
36100 0.00974301155656576
36200 0.0095998402684927
36300 0.00945734139531851
36400 0.00931553356349468
36500 0.00917443260550499
36600 0.00903405249118805
36700 0.0088944099843502
36800 0.00875551719218493
36900 0.00861738529056311
37000 0.00848002638667822
37100 0.00834345072507858
37200 0.00820766855031252
37300 0.00807268545031548
37400 0.00793850980699062
37500 0.00780514627695084
37600 0.00767260044813156
37700 0.00754087511450052
37800 0.00740997213870287
37900 0.00727989291772246
38000 0.00715063745155931
38100 0.00702220434322953
38200 0.00689459266141057
38300 0.00676779914647341
38400 0.00664181867614388
38500 0.00651664705947042
38600 0.00639227824285626
38700 0.00626870477572083
38800 0.00614591967314482
38900 0.00602391408756375
39000 0.00590267637744546
39100 0.0057821967639029
39200 0.0056624636054039
39300 0.00554346293210983
39400 0.00542518170550466
39500 0.00530760316178203
39600 0.00519071333110332
39700 0.00507449358701706
39800 0.00495892856270075
39900 0.00484399916604161
40000 0.00472968770191073
40100 0.00461597787216306
40200 0.00450285151600838
40300 0.00439029559493065
40400 0.0042782942764461
40500 0.00416683964431286
40600 0.00405592238530517
40700 0.00394553877413273
40800 0.00383569160476327
40900 0.00372638623230159
41000 0.00361763476394117
41100 0.00350945605896413
41200 0.00340187596157193
41300 0.00329492706805468
41400 0.00318865012377501
41500 0.00308309332467616
41600 0.00297831441275775
41700 0.00287437927909195
41800 0.00277136266231537
41900 0.00266934977844357
42000 0.00256843492388725
42100 0.00246872170828283
42200 0.00237032189033926
42300 0.00227335491217673
42400 0.00217794580385089
42500 0.00208422308787704
42600 0.00199231575243175
42700 0.0019023516215384
42800 0.00181445525959134
42900 0.00172874669078737
43000 0.00164533930364996
43100 0.00156434089876711
43200 0.00148585240822285
43300 0.00140996789559722
43400 0.00133677339181304
43500 0.00126634689513594
43600 0.00119875662494451
43700 0.00113405950833112
43800 0.00107229850254953
43900 0.00101350271143019
44000 0.000957684067543596
44100 0.000904837506823242
44200 0.000854939804412425
44300 0.000807950331363827
44400 0.000763811287470162
44500 0.000722449622116983
44600 0.000683779071550816
44700 0.000647702720016241
44800 0.00061411497881636
44900 0.000582904845941812
45000 0.000553958117961884
45100 0.000527159660123289
45200 0.000502395094372332
45300 0.000479551672469825
45400 0.000458519672974944
45500 0.00043919196468778
45600 0.000421464443206787
45700 0.000405235652578995
45800 0.000390406901715323
45900 0.000376881420379505
46000 0.000364564766641706
46100 0.000353365408955142
46200 0.000343193882144988
46300 0.000333963893353939
46400 0.000325592583976686
46500 0.000318000820698217
46600 0.000311113748466596
46700 0.000304861372569576
46800 0.000299178063869476
46900 0.000294003344606608
47000 0.000289282033918425
47100 0.000284963520243764
47200 0.000281002197880298
47300 0.000277356593869627
47400 0.000273990270216018
47500 0.000270869844825938
47600 0.000267965660896152
47700 0.00026525161229074
47800 0.000262704386841506
47900 0.000260302913375199
48000 0.000258029234828427
48100 0.000255866820225492
48200 0.000253801117651165
48300 0.000251819350523874
48400 0.000249910139245912
48500 0.000248063268372789
48600 0.000246270385105163
48700 0.000244523369474337
48800 0.000242816313402727
48900 0.00024114349798765
49000 0.000239500994211994
49100 0.000237886371905915
49200 0.00023629906354472
49300 0.000234741004533134
49400 0.000233218510402367
49500 0.000231743732001632
49600 0.0002303375658812
49700 0.000229032390052453
49800 0.000227874115807936
49900 0.000226911753998138
};
\addplot [red]
table {0 0.0142594249919057
100 0.00328507157973945
200 0.00320161529816687
300 0.00298257684335113
400 0.0029080759268254
500 0.00286898319609463
600 0.00267827278003097
700 0.00244240323081613
800 0.002314681885764
900 0.0022952335420996
1000 0.00217611622065306
1100 0.0020442008972168
1200 0.00205519259907305
1300 0.00199888343922794
1400 0.00181393348611891
1500 0.0017628651112318
1600 0.00179227872285992
1700 0.00177244457881898
1800 0.00168580154422671
1900 0.00164998136460781
2000 0.00153581402264535
2100 0.0012976371217519
2200 0.00128749979194254
2300 0.00114743656013161
2400 0.00123104092199355
2500 0.000936322205234319
2600 0.000949768407735974
2700 0.000841374916490167
2800 0.00078981148544699
2900 0.000793184095527977
3000 0.000690105895046145
3100 0.000738960225135088
3200 0.00063604797469452
3300 0.000585959409363568
3400 0.000648958841338754
3500 0.000555491598788649
3600 0.000552104436792433
3700 0.000542375084478408
3800 0.000659562763758004
3900 0.000490173581056297
4000 0.000567781971767545
4100 0.000511298363562673
4200 0.000449502986157313
4300 0.000415420450735837
4400 0.000467780046164989
4500 0.000453706627013162
4600 0.000434712099377066
4700 0.000458413967862725
4800 0.000456042616860941
4900 0.000409810454584658
5000 0.000448865146609023
5100 0.000454130000434816
5200 0.000412710389355198
5300 0.000365639949450269
5400 0.00043005813495256
5500 0.000397038063965738
5600 0.000403032172471285
5700 0.000368160544894636
5800 0.000447071244707331
5900 0.000374807685147971
6000 0.000363827537512407
6100 0.000382893369533122
6200 0.000425598176661879
6300 0.000414198118960485
6400 0.000345010048476979
6500 0.000391132576623932
6600 0.000358787423465401
6700 0.000333039090037346
6800 0.000349147449014708
6900 0.000390499219065532
7000 0.000377370713977143
7100 0.000339594174874946
7200 0.000295215751975775
7300 0.000299231876851991
7400 0.000350774877006188
7500 0.000281534274108708
7600 0.000346990098478273
7700 0.000383692065952346
7800 0.00037469231756404
7900 0.000333838106598705
8000 0.000390590663300827
8100 0.000290276744635776
8200 0.000288607639959082
8300 0.000327912712236866
8400 0.000362828111974522
8500 0.000272674893494695
8600 0.000335466582328081
8700 0.000304153130855411
8800 0.000254952668910846
8900 0.00030918326228857
9000 0.00025848051882349
9100 0.000269459997070953
9200 0.000298429164104164
9300 0.000282908760709688
9400 0.000310110626742244
9500 0.00032892293529585
9600 0.000345880805980414
9700 0.000272721459623426
9800 0.000250804907409474
9900 0.000337138830218464
10000 0.000348279747413471
10100 0.000237125321291387
10200 0.00031323404982686
10300 0.00026406068354845
10400 0.000388422806281596
10500 0.000306043220916763
10600 0.000294892641250044
10700 0.000296202721074224
10800 0.000254128681262955
10900 0.000293629273073748
11000 0.000358584045898169
11100 0.000264604343101382
11200 0.00025186620769091
11300 0.000266636168817058
11400 0.000296895072096959
11500 0.000298009923426434
11600 0.000243981630774215
11700 0.00032354099676013
11800 0.00028068051324226
11900 0.000296748941764235
12000 0.000221648864680901
12100 0.000298826518701389
12200 0.000245405186433345
12300 0.000257863779552281
12400 0.000330478302203119
12500 0.000304698391119018
12600 0.000287132832454517
12700 0.000315730314468965
12800 0.000243119313381612
12900 0.000330123177263886
13000 0.000228101489483379
13100 0.00029602789436467
13200 0.000281604880001396
13300 0.000260380591498688
13400 0.000273351499345154
13500 0.00025188532890752
13600 0.000318091246299446
13700 0.000231835685553961
13800 0.000209071629797108
13900 0.000266535324044526
14000 0.000282682478427887
14100 0.00020137800311204
14200 0.000255969818681479
14300 0.000253471953328699
14400 0.000299808831186965
14500 0.000282101129414514
14600 0.000317903584800661
14700 0.000261639361269772
14800 0.000305396533804014
14900 0.000269398238742724
15000 0.000199595786398277
15100 0.000283678498817608
15200 0.000233810642384924
15300 0.000214269937714562
15400 0.000267894996795803
15500 0.000259146036114544
15600 0.000200432710698806
15700 0.000225777999730781
15800 0.000238939916016534
15900 0.000234251259826124
16000 0.000287230795947835
16100 0.000259032152825966
16200 0.000274614460067824
16300 0.000225334835704416
16400 0.000264369184151292
16500 0.000217849548789673
16600 0.000305610737996176
16700 0.000214605053770356
16800 0.000221685317228548
16900 0.000226033444050699
17000 0.000199261063244194
17100 0.000220751477172598
17200 0.000315748358843848
17300 0.000348550063790753
17400 0.000260871689533815
17500 0.000233050101087429
17600 0.000263922120211646
17700 0.000253339239861816
17800 0.000266073853708804
17900 0.000211072954698466
18000 0.0002643856278155
18100 0.000205465825274587
18200 0.00024311694141943
18300 0.000217309483559802
18400 0.000207861055969261
18500 0.000246055162278935
18600 0.000222842878429219
18700 0.000247940071858466
18800 0.000297306629363447
18900 0.000218588058487512
19000 0.000221025780774653
19100 0.000307537266053259
19200 0.000260211527347565
19300 0.000222979550017044
19400 0.000233366969041526
19500 0.000227580152568407
19600 0.000276280858088285
19700 0.000188314996194094
19800 0.000219934794586152
19900 0.00023277489526663
20000 0.000236448686337098
20100 0.000269619398750365
20200 0.000212296625250019
20300 0.00020144488371443
20400 0.000244566763285547
20500 0.000242018300923519
20600 0.000248691037995741
20700 0.00021920031576883
20800 0.000237010637647472
20900 0.000271254975814372
21000 0.000199947942746803
21100 0.000235848929150961
21200 0.000282857363345101
21300 0.000236239298828878
21400 0.000231391590205021
21500 0.000262849265709519
21600 0.000228149758186191
21700 0.000210577592952177
21800 0.00019974319729954
21900 0.000206044933293015
22000 0.000214394603972323
22100 0.00019867651280947
22200 0.000311965763103217
22300 0.000214460626011714
22400 0.000314258737489581
22500 0.000183167547220364
22600 0.000179417489562184
22700 0.000233561964705586
22800 0.000192112711374648
22900 0.000176457760971971
23000 0.000192724139196798
23100 0.000204754120204598
23200 0.000216374988667667
23300 0.000188971127499826
23400 0.000161585572641343
23500 0.000200744529138319
23600 0.000228744640480727
23700 0.000265464157564566
23800 0.00024248038243968
23900 0.000179631271748804
24000 0.000187273471965455
24100 0.00021134756389074
24200 0.000185431083082221
24300 0.000239989443798549
24400 0.000211344653507695
24500 0.000177641384652816
24600 0.000190930426470004
24700 0.000223704526433721
24800 0.000169623890542425
24900 0.000195197120774537
25000 0.000211518301512115
25100 0.000229481200221926
25200 0.000207308432436548
25300 0.000186921504791826
25400 0.000207372329896316
25500 0.000233909289818257
25600 0.00021946283231955
25700 0.000218666493310593
25800 0.000207609264180064
25900 0.000226071351789869
26000 0.000193032828974538
26100 0.000250746961683035
26200 0.000180024333531037
26300 0.000181667739525437
26400 0.000243351067183539
26500 0.000202597104362212
26600 0.000243572125327773
26700 0.000255254562944174
26800 0.000179050810402259
26900 0.000226984135224484
27000 0.00023510106257163
27100 0.00017728706006892
27200 0.000241433459450491
27300 0.000195963177247904
27400 0.000206412063562311
27500 0.000179225578904152
27600 0.000227843993343413
27700 0.000190659542568028
27800 0.000217393404454924
27900 0.000183048236067407
28000 0.000210033962503076
28100 0.000183289448614232
28200 0.000234452119912021
28300 0.000235880317632109
28400 0.000255273160291836
28500 0.00020363531075418
28600 0.000189747093827464
28700 0.000178594156750478
28800 0.000201859176740982
28900 0.000223680501221679
29000 0.000202887516934425
29100 0.000232467515161261
29200 0.000154672496137209
29300 0.000172894724528305
29400 0.00019047774549108
29500 0.000204296287847683
29600 0.000222519476665184
29700 0.000244892900809646
29800 0.000259494496276602
29900 0.000193230414879508
30000 0.000239425295148976
30100 0.000178502406924963
30200 0.000153805260197259
30300 0.000231357334996574
30400 0.000185309501830488
30500 0.000176809000549838
30600 0.000210418365895748
30700 0.000229018391110003
30800 0.000242243797401898
30900 0.000178445741767064
31000 0.00018072230159305
31100 0.000162504948093556
31200 0.000192609179066494
31300 0.000211949329241179
31400 0.000190357430255972
31500 0.000200439724721946
31600 0.000165891950018704
31700 0.000155831628944725
31800 0.000227207099669613
31900 0.00015213833830785
32000 0.000172117230249569
32100 0.000184569784323685
32200 0.000159964460181072
32300 0.000161442774697207
32400 0.000152850945596583
32500 0.000162772295880131
32600 0.000207845223485492
32700 0.000209124467801303
32800 0.000223761875531636
32900 0.00022477054153569
33000 0.000250669865636155
33100 0.000162509677466005
33200 0.000217030799831264
33300 0.000171916864928789
33400 0.000176508896402083
33500 0.00017558695981279
33600 0.000154495923197828
33700 0.00022878845629748
33800 0.000172947969986126
33900 0.000161427626153454
34000 0.000221162015805021
34100 0.000192899504327215
34200 0.000172026615473442
34300 0.000199461021111347
34400 0.000151448635733686
34500 0.000164310797117651
34600 0.000183810436283238
34700 0.000192089151823893
34800 0.000154356224811636
34900 0.000150485953781754
35000 0.000186144257895648
35100 0.000199666392290965
35200 0.000185226701432839
35300 0.000154165347339585
35400 0.00019654912466649
35500 0.000151221160194837
35600 0.000159858725965023
35700 0.000211606224183924
35800 0.000177417183294892
35900 0.000184374002856202
36000 0.000157197602675296
36100 0.000190446095075458
36200 0.000155973320943303
36300 0.00018571998225525
36400 0.00015837540559005
36500 0.000162002004799433
36600 0.000151369022205472
36700 0.000179939946974628
36800 0.000185364231583662
36900 0.000164356271852739
37000 0.000164850556757301
37100 0.000193773390492424
37200 0.000186134959221818
37300 0.000194250911590643
37400 0.000168431564816274
37500 0.00017158810805995
37600 0.000218385743210092
37700 0.00019097124459222
37800 0.000152216205606237
37900 0.000222981499973685
38000 0.00018967232608702
38100 0.000154203604324721
38200 0.000266434624791145
38300 0.000183851472684182
38400 0.000185106851859018
38500 0.000174880711711012
38600 0.000190484788618051
38700 0.000184952674317174
38800 0.000184249671292491
38900 0.000152887616422959
39000 0.000157020316692069
39100 0.000151330954395235
39200 0.000153033834067173
39300 0.000164235447300598
39400 0.000150295134517364
39500 0.000178240385139361
39600 0.000176574307261035
39700 0.0001648463512538
39800 0.000146669379319064
39900 0.00014136781101115
40000 0.000167371530551463
40100 0.000184251868631691
40200 0.000156564608914778
40300 0.000181406459887512
40400 0.000182030882569961
40500 0.000174081331351772
40600 0.000161022617248818
40700 0.000150511681567878
40800 0.000211617661989294
40900 0.000195019296370447
41000 0.00013757158012595
41100 0.0001712882949505
41200 0.000175050838151947
41300 0.000153135231812485
41400 0.000163140386575833
41500 0.000173332125996239
41600 0.000184884949703701
41700 0.000161274409038015
41800 0.000206908720429055
41900 0.000190647653653286
42000 0.000213744802749716
42100 0.000168425031006336
42200 0.000149734420119785
42300 0.000153558459714986
42400 0.000155513582285494
42500 0.000183305746759288
42600 0.00015483723836951
42700 0.000168673082953319
42800 0.000147486454807222
42900 0.000158764029038139
43000 0.000149092287756503
43100 0.00014205313345883
43200 0.00014814012683928
43300 0.000149033992784098
43400 0.000142591496114619
43500 0.000153501299791969
43600 0.000178884380147792
43700 0.000135432201204821
43800 0.000136256072437391
43900 0.000182607313035987
44000 0.000182484858669341
44100 0.000175249006133527
44200 0.000185414843144827
44300 0.000162818163516931
44400 0.000156463342136703
44500 0.000194112901226617
44600 0.000150353866047226
44700 0.000152900291141123
44800 0.000147328973980621
44900 0.000182187563041225
45000 0.000148976716445759
45100 0.000146290229167789
45200 0.000145996018545702
45300 0.00016853102715686
45400 0.000149271058035083
45500 0.000160692376084626
45600 0.00014525894948747
45700 0.0001621547708055
45800 0.00016259441326838
45900 0.00014191230002325
46000 0.000166426572832279
46100 0.000206823489861563
46200 0.000172658998053521
46300 0.000149696614244021
46400 0.000146205115015619
46500 0.00018041928706225
46600 0.000151274085510522
46700 0.000173726220964454
46800 0.000139183932333253
46900 0.000156320820678957
47000 0.000187091834959574
47100 0.000201965420274064
47200 0.000153832777868956
47300 0.000190092934644781
47400 0.000211392049095593
47500 0.000141554701258428
47600 0.000163542674272321
47700 0.000132669621962123
47800 0.000148774735862389
47900 0.000160151554155163
48000 0.000186288089025766
48100 0.000144097430165857
48200 0.000155118192196824
48300 0.000156139314640313
48400 0.000152934109792113
48500 0.000176975663634948
48600 0.000160189199959859
48700 0.000147949860547669
48800 0.000157837697770447
48900 0.000137010298203677
49000 0.00015891446673777
49100 0.000168324389960617
49200 0.000216546148294583
49300 0.000159258750500157
49400 0.000162933065439574
49500 0.000164362892974168
49600 0.000180463728611358
49700 0.000148893246660009
49800 0.000166178404469974
49900 0.000157636939547956
};
\addplot [color0]
table {0 0.0124469716101885
100 0.00614230893552303
200 0.00262880278751254
300 0.00143832433968782
400 0.00102286459878087
500 0.00104248244315386
600 0.000897188496310264
700 0.000680836674291641
800 0.000681549310684204
900 0.000681312056258321
1000 0.000546886469237506
1100 0.000448119477368891
1200 0.00046545997611247
1300 0.000415299378801137
1400 0.000554659229237586
1500 0.000470197293907404
1600 0.000322826788760722
1700 0.000743865908589214
1800 0.000337764475261793
1900 0.0003961862239521
2000 0.000312219519400969
2100 0.000308149261400104
2200 0.000326185690937564
2300 0.000347190711181611
2400 0.00026728303055279
2500 0.000392227899283171
2600 0.000367371627362445
2700 0.000318830803735182
2800 0.000320950930472463
2900 0.000386913074180484
3000 0.000301673455396667
3100 0.000307369773508981
3200 0.000311895535560325
3300 0.000564215879421681
3400 0.000235037703532726
3500 0.000230397170525976
3600 0.000199932255782187
3700 0.000276215694611892
3800 0.000185064869583584
3900 0.000189274447620846
4000 0.000178999311174266
4100 0.000195738844922744
4200 0.000307175796478987
4300 0.000184641539817676
4400 0.000221018228330649
4500 0.000162387892487459
4600 0.000155675777932629
4700 0.000199596848688088
4800 0.000288783427095041
4900 0.000236216437770054
5000 0.000193819811102003
5100 0.000232921898714267
5200 0.00022464532230515
5300 0.000324907246977091
5400 0.000160495736054145
5500 0.000189285681699403
5600 0.000252876779995859
5700 0.000197887697140686
5800 0.000140964373713359
5900 0.000132381450384855
6000 0.000187732366612181
6100 0.000177994414116256
6200 0.000170702944160439
6300 0.000129964580992237
6400 0.000154066234244965
6500 0.00013820875028614
6600 0.000138394680107012
6700 0.00021880041458644
6800 0.000151974003529176
6900 0.000158244394697249
7000 0.000180134375113994
7100 0.000160917683388107
7200 0.000176946909050457
7300 0.000156345180585049
7400 0.000196367313037626
7500 0.000130788743263111
7600 0.000189972619409673
7700 0.000200714508537203
7800 0.00012275691551622
7900 0.000227555487072095
8000 0.000155863293912262
8100 0.000124505910207517
8200 0.000217939508729614
8300 0.000166736543178558
8400 0.000153584565850906
8500 0.000151399857713841
8600 0.000150562016642652
8700 0.000206795608391985
8800 0.000150489679072052
8900 0.000195216372958384
9000 0.000155078552779742
9100 0.000127528168377466
9200 0.000136084127007052
9300 0.000151214961078949
9400 0.00015513641119469
9500 0.00017221130838152
9600 0.000130819476908073
9700 0.000121471668535378
9800 0.000151065105455928
9900 0.000190337203093804
10000 0.000116920324217062
10100 0.000113422458525747
10200 0.000123072386486456
10300 0.000127586914459243
10400 0.00011644287587842
10500 0.000171813342603855
10600 0.000124030688311905
10700 0.000227006938075647
10800 0.000147886807098985
10900 0.000196930923266336
11000 0.00015680021897424
11100 0.000227810218348168
11200 0.000288717041257769
11300 0.000144564342917874
11400 0.000145270707434975
11500 0.000190004036994651
11600 0.000110921093437355
11700 0.000132954126456752
11800 0.000115843853564002
11900 0.000161503587150946
12000 0.000122104756883346
12100 0.000194889726117253
12200 0.000183024414582178
12300 0.000122843470307998
12400 0.000169377322890796
12500 0.00011700508184731
12600 0.000153464920003898
12700 0.000131884647998959
12800 0.000114179849333595
12900 0.000122874946100637
13000 0.000126992337754928
13100 0.000263016787357628
13200 0.000123813035315834
13300 0.000138169023557566
13400 0.000140101823490113
13500 0.000122199999168515
13600 0.000128733168821782
13700 0.000133540437673219
13800 0.000125360733363777
13900 0.000113322697870899
14000 0.000233564584050328
14100 0.000162514174007811
14200 0.000119884556625038
14300 0.000113806527224369
14400 0.000117998904897831
14500 0.000110444467281923
14600 0.000248097203439102
14700 0.000204682946787216
14800 0.000136292190290987
14900 0.000131211112602614
15000 0.000116262024675962
15100 0.000165449877385981
15200 0.000198262234334834
15300 0.000122054720122833
15400 0.000112401488877367
15500 0.000161605072207749
15600 0.000153708373545669
15700 0.000107656320324168
15800 0.000155829882714897
15900 0.000139978132210672
16000 0.000196157619939186
16100 0.000170920087839477
16200 0.00011026140418835
16300 0.000120240038086195
16400 0.00016844151832629
16500 0.000139106428832747
16600 0.000112253772385884
16700 0.000173767330124974
16800 0.000130151907796971
16900 0.000129947962705046
17000 0.000128029016195796
17100 0.000111379158624914
17200 0.000138949428219348
17300 0.00012062525638612
17400 0.000145525264088064
17500 0.000161692369147204
17600 0.000106119587144349
17700 0.000136464528623037
17800 0.000139390336698852
17900 0.000119881748105399
18000 0.000236607578699477
18100 0.000116060960863251
18200 0.000106384854007047
18300 0.000107129591924604
18400 0.000118070114695001
18500 0.000125120699522085
18600 0.000104449885839131
18700 0.000141456417622976
18800 0.000105709637864493
18900 0.000114033246063627
19000 0.000132091256091371
19100 0.000109006978163961
19200 0.000130762593471445
19300 0.000118482443212997
19400 0.000111450885015074
19500 0.000101661113149021
19600 0.000141839816933498
19700 0.000144155230373144
19800 0.000127504375996068
19900 0.000108512424048968
20000 0.000123778285342269
20100 0.000118427080451511
20200 0.000104123319033533
20300 0.000120255746878684
20400 0.00012403646542225
20500 0.00014977902173996
20600 0.000131484717712738
20700 0.000127027451526374
20800 0.000120637916552369
20900 0.000125582489999942
21000 0.00010565575939836
21100 0.000129955195006914
21200 0.000116098468424752
21300 0.000105920218629763
21400 0.000119996431749314
21500 0.000120871911349241
21600 0.000102437552413903
21700 0.000122312107123435
21800 0.000105440347397234
21900 0.000101204546808731
22000 0.000107152940472588
22100 0.000107993342680857
22200 0.000135914466227405
22300 0.000103375692560803
22400 0.000101821395219304
22500 0.000112834597530309
22600 0.000204906929866411
22700 0.0001260277495021
22800 0.000111967470729724
22900 0.000109542546852026
23000 9.85862716333941e-05
23100 0.000119423762953375
23200 0.000143479992402717
23300 0.000111830129753798
23400 0.000114802453026641
23500 0.00012846699974034
23600 0.000155545669258572
23700 0.000113853762741201
23800 9.99718395178206e-05
23900 0.000108661370177288
24000 0.0001604906283319
24100 0.000118124866276048
24200 0.000114279246190563
24300 0.000125843624118716
24400 9.80345284915529e-05
24500 0.000144695994094945
24600 0.000105824758065864
24700 9.89681066130288e-05
24800 9.92802888504229e-05
24900 0.000120734177471604
25000 0.000118307019874919
25100 0.00012147623783676
25200 9.89710097201169e-05
25300 0.000100213059340604
25400 0.00011098688992206
25500 0.000110235436295625
25600 0.000175214270711876
25700 0.000106652827525977
25800 0.000108539519715123
25900 0.000142719567520544
26000 0.000113367095764261
26100 0.000144060861202888
26200 0.000100046119769104
26300 0.000122506287880242
26400 0.000111901244963519
26500 0.000104310725873802
26600 0.000107232655864209
26700 0.000130508080474101
26800 9.72942361840978e-05
26900 0.000104596947494429
27000 0.0001034806118696
27100 0.000119496202387381
27200 0.000101738354715053
27300 0.000116464492748491
27400 0.000106232881080359
27500 9.7803604148794e-05
27600 0.000161533680511639
27700 0.000208670986467041
27800 0.000124364363728091
27900 0.00010176398063777
28000 0.000117836352728773
28100 0.000104620703496039
28200 0.00010264253069181
28300 0.000112820096546784
28400 0.000116456205432769
28500 0.000106966348539572
28600 0.000198053850908764
28700 0.000102612677437719
28800 0.000161630843649618
28900 0.000106121238786727
29000 0.000112416928459425
29100 0.000165807825396769
29200 9.86364393611439e-05
29300 0.00011637806892395
29400 0.000102274585515261
29500 9.75476214080118e-05
29600 0.000115814007585868
29700 0.000202233015443198
29800 0.000125121456221677
29900 0.000101288060250226
30000 0.000108978892967571
30100 0.000107232175651006
30200 0.000111835477582645
30300 0.000102406353107654
30400 0.000114229980681557
30500 9.45308202062733e-05
30600 9.59348180913366e-05
30700 0.00024181921617128
30800 9.63521670200862e-05
30900 0.000101162906503305
31000 0.000102462749055121
31100 0.000109655302367173
31200 0.000109688946395181
31300 0.00011370921129128
31400 9.67585510807112e-05
31500 0.000122816913062707
31600 0.000155136702232994
31700 9.61124897003174e-05
31800 9.73450078163296e-05
31900 9.49297318584286e-05
32000 0.000103216938441619
32100 0.000126687649753876
32200 0.000107954969280399
32300 0.000125749385915697
32400 0.000107130515971221
32500 0.000136351707624272
32600 0.000130330794490874
32700 0.000109245796920732
32800 0.000178002490429208
32900 0.000130142638226971
33000 9.36586584430188e-05
33100 0.00010211460175924
33200 0.000170911807799712
33300 0.000103020196547732
33400 0.000124141370179132
33500 0.000152632957906462
33600 0.000122867946629412
33700 0.000125104066682979
33800 0.000197192683117464
33900 9.45740393945016e-05
34000 0.000129883163026534
34100 9.7815012850333e-05
34200 0.000114074493467342
34300 0.000100641023891512
34400 9.42969199968502e-05
34500 9.59907483775169e-05
34600 0.000108351763628889
34700 9.25548811210319e-05
34800 9.33882765821181e-05
34900 0.000139849827974103
35000 9.85506412689574e-05
35100 9.5300332759507e-05
35200 9.41213365877047e-05
35300 0.000109207794594113
35400 0.000138538409373723
35500 9.47233857004903e-05
35600 0.000119526463095099
35700 9.18319856282324e-05
35800 0.00014178799756337
35900 0.000113044661702588
36000 9.30288588278927e-05
36100 9.67730666161515e-05
36200 9.25693675526418e-05
36300 0.000100034500064794
36400 0.000103017191577237
36500 9.35921052587219e-05
36600 0.000153823377331719
36700 0.00010127588029718
36800 0.000103053505881689
36900 9.18341684155166e-05
37000 0.000101458746939898
37100 0.000127258739667013
37200 0.000125959471915849
37300 9.17019278858788e-05
37400 9.7172633104492e-05
37500 0.00011762559734052
37600 0.000152558510308154
37700 0.000115856142656412
37800 0.000104812184872571
37900 9.71937333815731e-05
38000 0.000100402139651123
38100 9.82783967629075e-05
38200 0.000143157565617003
38300 0.000104942351754289
38400 0.000105100793007296
38500 8.94549739314243e-05
38600 0.000101804434962105
38700 9.7812524472829e-05
38800 9.11650276975706e-05
38900 0.000104533792182337
39000 9.34529307414778e-05
39100 0.000113700683868956
39200 9.91928827716038e-05
39300 0.000124290134408511
39400 0.000138479910674505
39500 9.85321457847022e-05
39600 8.59079445945099e-05
39700 0.000127094463096
39800 0.000103472077171318
39900 9.37474324018694e-05
40000 0.000125916165416129
40100 9.21158498385921e-05
40200 0.000110808701720089
40300 0.00012167080421932
40400 0.000110267828858923
40500 8.52650773595087e-05
40600 0.00015418826660607
40700 0.00011720578186214
40800 0.000100400953670032
40900 0.000153945409692824
41000 0.000106804291135632
41100 0.000101309757155832
41200 8.77858765306883e-05
41300 9.86857558018528e-05
41400 0.000118427116831299
41500 0.000106185558252037
41600 0.000150575782754458
41700 0.000120337615953758
41800 9.33159244596027e-05
41900 8.32158621051349e-05
42000 0.000108509295387194
42100 9.56714065978304e-05
42200 9.38339217100292e-05
42300 0.000100435048807412
42400 0.000153760498506017
42500 0.000100385797850322
42600 0.000102496727777179
42700 0.000112496665678918
42800 8.92373718670569e-05
42900 8.75122277648188e-05
43000 0.000110301807580981
43100 0.000110441920696758
43200 0.000107919368019793
43300 0.000131559208966792
43400 9.36392752919346e-05
43500 0.000111037501483224
43600 8.87286732904613e-05
43700 0.000104638966149651
43800 0.000104084057966247
43900 9.62868653004989e-05
44000 0.000176698216819204
44100 8.29802374937572e-05
44200 8.84195469552651e-05
44300 0.000126087354146875
44400 0.000104669459688012
44500 0.000121880984806921
44600 8.23772206786089e-05
44700 8.9255474449601e-05
44800 0.000106505307485349
44900 0.000113058100396302
45000 8.53147139423527e-05
45100 8.9816669060383e-05
45200 8.99944570846856e-05
45300 0.000189780635992065
45400 8.23546433821321e-05
45500 8.67219205247238e-05
45600 0.000105638573586475
45700 0.000116046889161225
45800 9.80601253104396e-05
45900 0.000184067263035104
46000 0.000100303186627571
46100 9.06755522009917e-05
46200 0.000279369036434218
46300 0.000120097014587373
46400 0.000100557495898101
46500 0.000106817176856566
46600 0.000116546179924626
46700 7.99057525000535e-05
46800 0.000109636377601419
46900 8.44286696519703e-05
47000 0.000113645844976418
47100 8.27689946163446e-05
47200 0.000104219325294252
47300 0.000103738108009566
47400 0.000108188731246628
47500 0.00011422631359892
47600 0.000156588343088515
47700 8.48701238282956e-05
47800 9.86226950772107e-05
47900 0.000104402854049113
48000 8.31615761853755e-05
48100 0.000135991955175996
48200 8.04543524282053e-05
48300 0.000115719558380079
48400 0.000123495163279586
48500 9.51700858422555e-05
48600 8.00090710981749e-05
48700 0.000147150494740345
48800 8.35028404253535e-05
48900 7.87123281043023e-05
49000 0.000121407298138365
49100 9.85676015261561e-05
49200 0.000118042866233736
49300 9.21792889130302e-05
49400 0.000131009277538396
49500 0.000133492940221913
49600 0.000123219491797499
49700 9.25649219425395e-05
49800 9.74940267042257e-05
49900 7.95412779552862e-05
};
\end{axis}

\end{tikzpicture} \vspace{-5mm}
\caption{Convergence rates of the tested algorithms over the Synthetic Netflix dataset.}
\label{fig:convergence-rate}
\end{figure}

\begin{figure}[!ht]
\centering
\begin{minipage}[b]{0.2\linewidth}
\hspace*{-0.4cm}\raisebox{-0.1cm}{
\setlength\figureheight{3.2cm}
\setlength\figurewidth{3.2cm}
\begin{tikzpicture}

\begin{axis}[
xmin=0, xmax=20,
ymin=0, ymax=20,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick = {0, 8.5, 17},
xticklabels = {0,1,2},
ytick = {3, 11.5, 20},
yticklabels = {2,1,0},
xticklabel pos=right,
xlabel near ticks,
axis x line = right,
axis y line = left,
y axis line style = {stealth-},
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=-0.899999999999999,
point meta max=1,
colorbar style={ytick={-0.8,-0.6,-0.4,-0.2,0,0.2,0.4,0.6,0.8,1},yticklabels={0.8,0.6,0.4,0.2,0.0,0.2,0.4,0.6,0.8,1.0}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=0, xmax=17, ymin=20, ymax=3] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_0_absolute_value/filter1.png};
\end{axis};
\node at (-0.1, -0.1) {\tiny $\lambda_r$};
\node at (1.65, 1.45) {\tiny $\lambda_c$};

\end{tikzpicture} }
\end{minipage}
\hfill
\begin{minipage}[b]{0.73\linewidth}
\setlength\figureheight{2.7cm}
\setlength\figurewidth{2.98cm}
\begin{tikzpicture}
\begin{groupplot}[
     group style = {group size = 4 by 2, horizontal sep=0.1cm, vertical sep=0.1cm}]
     


\nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=0.00113467233532974,
point meta max=1.80726075219065,
colorbar style={ytick={0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8},yticklabels={0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_0_absolute_value/filter1.png};

%
 

\nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=0.00182024337306054,
point meta max=1.62648426677333,
colorbar style={ytick={0.2,0.4,0.6,0.8,1,1.2,1.4,1.6},yticklabels={0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_1_absolute_value/filter1.png};

%
 

\nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=0.00927264303043416,
point meta max=1.81756495874524,
colorbar style={ytick={0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8},yticklabels={0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_2_absolute_value/filter1.png};


%
 \nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=0.00171230213050688,
point meta max=1.65394239438446,
colorbar style={ytick={0.2,0.4,0.6,0.8,1,1.2,1.4,1.6},yticklabels={0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_3_absolute_value/filter1.png};


%
 
\nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=0.00209638960035366,
point meta max=3.51930867321789,
colorbar style={ytick={0.4,0.8,1.2,1.6,2,2.4,2.8,3.2},yticklabels={0.4,0.8,1.2,1.6,2.0,2.4,2.8,3.2}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_4_absolute_value/filter1.png};


%
 

\nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=0.000597093736481562,
point meta max=3.66198687441647,
colorbar style={ytick={0.4,0.8,1.2,1.6,2,2.4,2.8,3.2,3.6},yticklabels={0.4,0.8,1.2,1.6,2.0,2.4,2.8,3.2,3.6}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_5_absolute_value/filter1.png};


%
 

\nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=0.000898283861207845,
point meta max=1.84262938687627,
colorbar style={ytick={0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8},yticklabels={0.2,0.4,0.6,0.8,1.0,1.2,1.4,1.6,1.8}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_6_absolute_value/filter1.png};


%
 

\nextgroupplot[
xmin=-0.5, xmax=19.5,
ymin=-0.5, ymax=19.5,
axis on top,
width=\figurewidth,
height=\figureheight,
xtick=\empty,
ytick=\empty,
colormap={mymap}{[1pt]
  rgb(0pt)=(0,0,0.5);
  rgb(22pt)=(0,0,1);
  rgb(25pt)=(0,0,1);
  rgb(68pt)=(0,0.86,1);
  rgb(70pt)=(0,0.9,0.967741935483871);
  rgb(75pt)=(0.0806451612903226,1,0.887096774193548);
  rgb(128pt)=(0.935483870967742,1,0.0322580645161291);
  rgb(130pt)=(0.967741935483871,0.962962962962963,0);
  rgb(132pt)=(1,0.925925925925926,0);
  rgb(178pt)=(1,0.0740740740740741,0);
  rgb(182pt)=(0.909090909090909,0,0);
  rgb(200pt)=(0.5,0,0)
},
point meta min=2.89986801101242e-05,
point meta max=2.21810105764568,
colorbar style={ytick={0.25,0.5,0.75,1,1.25,1.5,1.75,2},yticklabels={0.25,0.50,0.75,1.00,1.25,1.50,1.75,2.00}}
]
\addplot graphics [includegraphics cmd=\pgfimage,xmin=-0.5, xmax=19.5, ymin=19.5, ymax=-0.5] {Figures/final_results/tikz/synthetic_netflix/2D_spectral_filters/filter_7_absolute_value/filter1.png};


%
 \end{groupplot}
\end{tikzpicture}
\end{minipage}
\caption{Absolute value of the first 8 spectral filters learnt by our bidimensional convolution. On the left the first filter with the reference axes associated to the row and column graph eigenvalues.}
\label{fig:MGCNN-spectral-filter}
\end{figure}


\begin{figure}[!h]
\setlength\figureheight{5.5cm}
\setlength\figurewidth{8.5cm}
\begin{tikzpicture}

\definecolor{color1}{rgb}{0.75,0,0.75}
\definecolor{color0}{rgb}{0,0.75,0.75}
\definecolor{color2}{rgb}{0.75,0.75,0}

\begin{axis}[
xlabel={$\lambda_r, \lambda_c$},
ylabel={Filter Response},
xmin=0, xmax=2,
ymin=0, ymax=0.9,
axis on top,
width=\figurewidth,
height=\figureheight
]
\addplot [blue]
table {0 0.136324610561132
0.1 0.210317468804121
0.2 0.228814396899939
0.3 0.205480297130346
0.4 0.152584236186743
0.5 0.0809994451701642
0.6 0.000203319591283919
0.7 0.0817225806295871
0.8 0.1580925311625
0.9 0.22361664326787
1 0.274400863796472
1.1 0.307946975189447
1.2 0.323152595478296
1.3 0.320311178284883
1.4 0.301112012821436
1.5 0.268640223890543
1.6 0.227376771885157
1.7 0.183198452788592
1.8 0.143377898174524
1.9 0.116583575206995
};
\addplot [green!50.0!black]
table {0 0.319112024269998
0.1 0.267679165128619
0.2 0.234495540849865
0.3 0.215986791376025
0.4 0.208853592418134
0.5 0.210071655455977
0.6 0.216891727738082
0.7 0.226839592281729
0.8 0.237716067872941
0.9 0.247597009066492
1 0.254833306185901
1.1 0.258050885323435
1.2 0.256150708340108
1.3 0.248308772865683
1.4 0.233976112298667
1.5 0.212878795806319
1.6 0.18501792832464
1.7 0.150669650558382
1.8 0.110385138981044
1.9 0.0649906058348718
};
\addplot [red]
table {0 0.87893633171916
0.1 0.373574490180612
0.2 0.0638341366112234
0.3 0.0931944779247045
0.4 0.136118144446611
0.5 0.0992406960576774
0.6 0.0125630079448224
0.7 0.0982170026212929
0.8 0.211704376286268
0.9 0.310807111611962
1 0.382736165076494
1.1 0.419005451074242
1.2 0.415431841915846
1.3 0.372135167828202
1.4 0.29353821695447
1.5 0.188366735354066
1.6 0.0696494270026688
1.7 0.0452820462077853
1.8 0.134793064469099
1.9 0.172946050056815
};
\addplot [color0]
table {0 0.289882898330688
0.1 0.0793396825909615
0.2 0.00113561244010929
0.3 0.019990401160717
0.4 0.104608240318298
0.5 0.227677799761295
0.6 0.365872227621078
0.7 0.499849150311947
0.8 0.614250672531128
0.9 0.697703377258777
1 0.74281832575798
1.1 0.746191057574749
1.2 0.708401590538025
1.3 0.634014420759678
1.4 0.531578522634507
1.5 0.413627348840237
1.6 0.296678830337525
1.7 0.201235376369953
1.8 0.151783874464035
1.9 0.17679569042921
};
\addplot [color1, dashed]
table {0 0.608784426935017
0.1 0.356468193937093
0.2 0.149512822747231
0.3 0.0129816011972724
0.4 0.13254165840894
0.5 0.211320595350116
0.6 0.252098324432969
0.7 0.258281424019486
0.8 0.233903138421476
0.9 0.183623377900571
1 0.112728718668223
1.1 0.0271324028857056
1.2 0.0666256613358853
1.3 0.161378899935633
1.4 0.249334072902798
1.5 0.322071274276823
1.6 0.370543932147324
1.7 0.3850788086541
1.8 0.355375999987126
1.9 0.270508936386556
};
\addplot [color2, dashed]
table {0 0.0335052609443665
0.1 0.0223346426725388
0.2 0.0353944054603576
0.3 0.120627729392052
0.4 0.217024522876739
0.5 0.310957327485084
0.6 0.391512032604217
0.7 0.450487875437736
0.8 0.482397441005707
0.9 0.484466662144661
1 0.456634819507599
1.1 0.401554541563988
1.2 0.324591804599762
1.3 0.233825932717324
1.4 0.140049597835541
1.5 0.056768819689751
1.6 0.000202965831756704
1.7 0.0107152483701706
1.8 0.0456602402687069
1.9 0.193688842749595
};
\addplot [black, dashed]
table {0 0.410960968583822
0.1 0.0308272948265077
0.2 0.217365965932607
0.3 0.360007638037205
0.4 0.420934387475252
0.5 0.421430721879005
0.6 0.380228990525007
0.7 0.313509384334087
0.8 0.234899935871363
0.9 0.155476519346237
1 0.0837628506124021
1.1 0.0257304871678354
1.2 0.0152011718451976
1.3 0.0381648856401443
1.4 0.0448455717861653
1.5 0.0394803062081337
1.6 0.028858323186636
1.7 0.0223210153579712
1.8 0.0317619337141513
1.9 0.0716267876029011
};
\addplot [blue, dashed]
table {0 0.292134643532336
0.1 0.00637941971868266
0.2 0.14157137349695
0.3 0.15948333171457
0.4 0.100516167972982
0.5 0.000570484437048305
0.6 0.114658232535422
0.7 0.218269923652708
0.8 0.293569645430148
0.9 0.328362725819647
1 0.316095733083785
1.1 0.255856475795806
1.2 0.152374002839625
1.3 0.016018603409827
1.4 0.137198192988336
1.5 0.285623616538941
1.6 0.401963657115399
1.7 0.45328306428045
1.8 0.401005347286165
1.9 0.200912775073947
};
\end{axis}

\end{tikzpicture} \vspace{-2mm}
\caption{Absolute value of the first four spectral filters learned by the user (solid) and items (dashed) GCNNs. }
\label{fig:GCNN-spectral-filter}
\end{figure}



\subsection{Real data}


Following \cite{rao2015collaborative}, we evaluated the proposed approach on the MovieLens, Flixster, Douban and YahooMusic datasets.  
For the MovieLens dataset we constructed the user and item (movie) graphs as unweighted 10-nearest neighbor graphs in the space of user and movie features, respectively. 
For Flixster, the user and item graphs were constructed from the scores of the original matrix. On this dataset, we also performed an experiment using only the users graph. 
For the Douban dataset, we used only the user graph (the provided social network of the user). 
For the YahooMusic dataset, we used only the item graph, constructed with unweighted 10-nearest neighbors in the space of item features (artists, albums, and genres). 
For the latter three datasets, we used a sub-matrix of $3000 \times 3000$ entries for evaluating the performance. 

Tables~\ref{tab:results-movielens} and~\ref{tab:other-datasets} summarize the performance of different methods. RGCNN outperforms the competitors in all the experiments. 



\begin{table}[!h]
\caption{Performance (RMS error) of different matrix completion methods on the MovieLens dataset.\vspace{-2.25mm}}
\label{tab:results-movielens}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lc}
\hline
\abovespace\belowspace
Method & RMSE \\
\hline
\abovespace 
Global Mean & 1.154\\
User Mean & 1.063\\
Movie Mean & 1.033\\
MC \cite{candes2012exact} & 0.973\\
IMC \cite{jain2013provable,xu2013speedup} & 1.653\\
GMC  \cite{kalofolias2014matrix} & 0.996\\
GRALS \cite{rao2015collaborative} & 0.945\\
{\bf sRGCNN} & {\bf 0.929} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}




\begin{table}[!h]
\caption{
Matrix completion results on several datasets (RMS error). For Douban and YahooMusic, a single graph (of users and items, respectively) was used. For Flixter, two settings are shown: users+items graphs / only users graph. \vspace{-2.25mm} 
}
\label{tab:other-datasets}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\hline
\abovespace\belowspace
Method & Flixster & Douban & YahooMusic \\
\hline
\abovespace 
GRALS & 1.3126 / 1.2447 & 0.8326 & 38.0423\\ 
{\bf sRGCNN} & {\bf 1.1788} / {\bf 0.9258} & {\bf 0.8012} & {\bf 22.4149} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{table}













 


\section{Conclusion}
\label{sec:conc}

In this paper, we presented a new deep learning approach for matrix completion based on a specially designed multi-graph convolutional neural network architecture. Among the key advantages of our approach compared to traditional methods is its low computational complexity and constant number of degrees of freedom independent of the matrix size. We showed that the use of deep learning for matrix completion allows to beat current state-of-the-art recommender system methods. To our knowledge, our work is the first application of deep learning on graphs to this class of problems. We believe that it shows the potential of the nascent field of geometric deep learning on non-Euclidean domains, and will encourage future works in this direction. 

 


\section{Acknowledgments}
FM and MB are supported in part by ERC Starting Grant No. 307047 (COMET), ERC Consolidator Grant No. 724228 (LEMAN), Google Faculty Research Award, Nvidia equipment grant, Radcliffe fellowship from Harvard Institute for Advanced Study, and TU Munich Institute for Advanced Study, funded by the German Excellence Initiative and the European Union Seventh Framework Programme under grant agreement No. 291763. XB is supported in part by NRF Fellowship NRFF2017-10. 


\bibliography{sections/refsx,sections/refs,sections/refs1}
\bibliographystyle{icml2017}

\end{document} 
