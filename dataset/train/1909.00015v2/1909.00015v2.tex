

\documentclass[11pt,a4paper]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{etoolbox}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{inconsolata}
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{mathtools}
\usepackage{empheq}
\usepackage{url}
\usepackage{microtype}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{setspace}
\usepackage{algorithmicx}
\usepackage{stmaryrd}
\usepackage{subcaption}
\usepackage[noend]{algpseudocode}
\algrenewcommand\alglinenumber[1]{{\textcolor{gray}{\sf\scriptsize#1}}}
\algrenewcommand\algorithmicindent{1.3em}\usepackage{soul}
\usepackage{marvosym}
\newcommand\markUnbabel{\Cancer}
\newcommand\markIT{\Leo}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}[lemma]

\def\QEDclosed{\mbox{\rule[0pt]{1.3ex}{1.3ex}}} \def\QEDopen{
    {\setlength{\fboxsep}{0pt}\setlength{
        \fboxrule}{0.2pt}\fbox{\rule[0pt]{0pt}{1.3ex}\rule[0pt]{1.3ex}{0pt}}}}
\def\QED{\QEDclosed} \def\proof{\noindent\hspace{0em}{\itshape Proof: }}
\def\endproof{\hspace*{\fill}~\QED\par\endtrivlist\unskip}
\def\lemmaproof{\noindent\hspace{0em}{\itshape Proof (of the Lemma): }}
\def\endlemmaproof{\hspace*{\fill}~\QED\par\endtrivlist\unskip}

\definecolor{vladcol}{rgb}{0.56, 0.27, 0.52}

\newtoggle{comms}

\toggletrue{comms}

\iftoggle{comms}{
\newcommand{\andre}[1]{\textcolor{purple}{[AM: #1]}}
\newcommand{\vlad}[1]{\textcolor{vladcol}{[VN: #1]}}
\newcommand{\goncalo}[1]{\textcolor{blue}{[GC: #1]}}
}{
\newcommand{\andre}[1]{}
\newcommand{\vlad}[1]{}
\newcommand{\goncalo}[1]{}
}

\newcommand*{\wrt}{\textit{w.\hspace{.07em}r\hspace{.07em}.t.}\@\xspace}
\newcommand*{\eg}{\textit{e.\hspace{.07em}g.}\@\xspace}
\newcommand*{\ie}{\textit{i.\hspace{.07em}e.}\@\xspace}
\newcommand*{\cf}{\textit{cf.}\@\xspace}
\newcommand*{\lhs}{\textit{l.\hspace{.07em}h\hspace{.07em}.s.}\@\xspace}
\newcommand*{\rhs}{\textit{r.\hspace{.07em}h\hspace{.07em}.s.}\@\xspace}

\newcommand{\tr}{\top}
\newcommand{\amap}{\bm{\pi}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\pfrac}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\simplex}{\triangle}
\newcommand\pp{p}
\newcommand\p{\bm{\pp}}
\newcommand\xx{z}
\newcommand\x{\bm{\xx}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\eqnref}[1]{Equation~\ref{eq:#1}}
\newcommand{\HHs}{\mathsf{H}^\textsc{S}}
\newcommand{\HHg}{\mathsf{H}^\textsc{G}}
\newcommand{\HHt}{\mathsf{H}^{\textsc{T}}}
\newcommand{\ones}{\bm{1}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\Loss}{\mathsf{L}}

\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\appref}[1]{Appendix~\ref{sec:#1}}
\newcommand{\DP}[2]{{#1}^\top{#2}}
\newcommand\thresh{\tau}

\DeclareMathOperator*{\sigmoid}{\mathsf{sigmoid}}
\DeclareMathOperator*{\argmax}{\mathsf{argmax}}
\DeclareMathOperator*{\argmin}{\mathsf{argmin}}
\DeclareMathOperator*{\diag}{\mathsf{diag}}
\DeclareMathOperator*{\cat}{\mathsf{cat}}
\DeclareMathOperator*{\softmax}{\mathsf{softmax}}
\DeclareMathOperator*{\sparsemax}{\mathsf{sparsemax}}
\DeclareMathOperator*{\supp}{\mathsf{supp}}
\newcommand*\entmaxtext{entmax\xspace}
\DeclareMathOperator*{\entmax}{\mathsf{\entmaxtext}}
\newcommand*\aentmax[1][\alpha]{\mathop{\mathsf{#1}\textnormal{-}\mathsf{\entmaxtext}}}

\DeclareMathOperator{\att}{\mathsf{Att}}
\DeclareMathOperator{\ath}{\mathsf{Head}}


\newcommand{\std}[1]{\footnotesize\textcolor{gray}{#1}}

\newcommand{\delang}{\textsc{de}\xspace}
\newcommand{\enlang}{\textsc{en}\xspace}
\newcommand{\rolang}{\textsc{ro}\xspace}
\newcommand{\jalang}{\textsc{ja}\xspace}

\newcommand{\langp}[2]{\textsc{#1}\textsc{#2}}
\newcommand{\langpb}[2]{\textsc{#1}\textsc{#2}}
\aclfinalcopy 




\title{Adaptively Sparse Transformers}
\author{
Gon\c{c}alo M. Correia\textsuperscript{\markIT{}} \\
\href{mailto:goncalo.correia@lx.it.pt}{\tt goncalo.correia@lx.it.pt}
\And
Vlad Niculae\textsuperscript{\markIT{}} \\
\href{mailto:vlad@vene.ro}{\tt vlad@vene.ro} \
    \att(\bm{Q}, \bm{K}, \bm{V}) = \amap
\left(\frac{\bm{Q}\bm{K}^\tr}{\sqrt{d}}\right) \bm{V},
    \label{eq:att_scaled_dot}
\label{eq:softmax}
    \softmax(\bm{z}) = \frac{\exp(z_j)}{\sum_{j'} \exp(z_{j'})}.
\label{eq:head}\hspace{-.01ex}\ath_i(\bm{Q}\!,\!\bm{K}\!,\!\bm{V})\!=\!\att(\bm{QW}_i^Q\!\!,\bm{KW}_i^K\!\!,\bm{VW}_i^V\!)\hspace{-1.5ex}\label{eq:define_entmax}
    \aentmax(\bm{z}) \coloneqq
    \argmax_{\p \in \simplex^d} \bm{p}^\top\bm{z} + \HHt_{\alpha}(\bm{p}),
\label{eq:tsallisdef}
    \HHt_{\alpha}(\bm{p})\!\coloneqq\!
\begin{cases}
\frac{1}{\alpha(\alpha-1)}\sum_j\!\left(p_j - p_j^\alpha\right)\!, &
\!\!\!\alpha \ne 1,\\
-\sum_j \pp_j \log \pp_j, &
\!\!\!\alpha = 1.
\end{cases}
\label{eq:entmax_form}
\aentmax(\x) = [(\alpha - 1){\x} - \thresh \ones]_+^{\nicefrac{1}{\alpha-1}},

\begin{aligned}
\pfrac{\aentmax(\x)}{\x} = \diag(\bm{s}) - \frac{1}{\sum_j s_j} \bm{ss}^\top, \\
\text{where}\quad s_i = \begin{cases}(\pp_i^\star)^{2-\alpha}, & \pp_i^\star > 0, \\
0,& \pp_i^\star = 0. \\\end{cases}
\end{aligned}

    \amap(\bm{Z})_{ij} = \aentmax(\bm{z}_i)_j
\label{eq:final_gradient_alpha_supp}
g_i =\begin{cases}
    \frac{p_i^{\star} - \tilde{\pp}_i}{(\alpha-1)^2} +
\frac{h_i - \tilde{\pp}_i\sum_j h_j}{\alpha-1}, & \alpha > 1,\
\end{proposition}
\noindent The proof uses implicit function differentiation and is given in Appendix \ref{sec:alpha_grad}.

Proposition~\ref{prop:grad_alpha} provides the remaining missing
piece needed for training adaptively sparse Transformers. In the
following section, we evaluate this strategy on neural machine
translation, and analyze the behavior of the learned attention heads.

\section{Experiments}
We apply our adaptively sparse Transformers on four machine translation tasks.
For comparison, a natural baseline is the standard Transformer
architecture using the softmax transform in its multi-head attention mechanisms.
We consider two other model variants in our experiments that make use of different
normalizing transformations:

\begin{itemize}
\item \textbf{1.5-\entmaxtext:} a Transformer with sparse \entmaxtext
attention with fixed  for all heads. This is a novel model,
since 1.5-\entmaxtext{} had only been proposed for
RNN-based NMT models~\citep{entmax}, but never
in Transformers, where attention modules are not just one single
component of the seq2seq model but rather an integral part of all of
the model components.\item \textbf{\boldmath -\entmaxtext:} an \textbf{adaptive}
Transformer with sparse \entmaxtext attention with a different,
learned  for each head.
\end{itemize}

The adaptive model has an additional scalar parameter per attention head per
layer for each of the three attention mechanisms (encoder self-attention,
context attention, and decoder self-attention), \ie,

and we set .
All or some of the  values can be tied if desired, but we
keep them independent for analysis purposes.

\begin{table*}[ht]

    \begin{center}
    \small
    \begin{tabular}{lrrrr}
    \toprule
    activation
    & \langp{de}{en} & \langp{ja}{en}
    & \langp{ro}{en} & \langp{en}{de}\\
    \midrule
    
    & 29.79
    & 21.57
    & 32.70
    & 26.02 \\
    
    & 29.83
    & \textbf{22.13}
    & \textbf{33.10}
    & 25.89 \\
    
    & \textbf{29.90}
    & 21.74
    & 32.89
    & \textbf{26.93} \\
    \bottomrule
    \end{tabular}
    \end{center}
    \caption{Machine translation tokenized BLEU test results
    on IWSLT 2017 \langp{de}{en},
    KFTT \langp{ja}{en}, WMT 2016 \langp{ro}{en} and
    WMT 2014 \langp{en}{de}, respectively.\label{table:mt}}
    \end{table*}

\paragraph{Datasets.} Our models were trained on 4 machine
translation datasets of different training sizes:

\begin{itemize}[itemsep=.5ex,leftmargin=2ex]
    \item IWSLT 2017 German  English
    \citep[\langp{de}{en},][]{cettolooverview}: ~200K sentence pairs.
    \item KFTT Japanese  English
    \citep[\langp{ja}{en},][]{neubig11kftt}: ~300K sentence pairs.
    \item WMT 2016 Romanian  English
    \citep[\langp{ro}{en},][]{bojar2016findings}: ~600K sentence pairs.
    \item WMT 2014 English  German
    \citep[\langp{en}{de},][]{bojar2014findings}: ~4.5M sentence pairs.
\end{itemize}

All of these datasets were preprocessed with byte-pair
encoding~\citep[BPE;][]{sennrich2016neural}, using joint
segmentations of 32k merge operations.

\paragraph{Training.}
We follow the dimensions of the Transformer-Base model of
\citet{vaswani2017attention}: The number of layers is  and
number of heads is  in the encoder self-attention, the context
attention, and the decoder self-attention. We use a mini-batch size
of 8192 tokens and warm up the learning rate linearly until 20k
steps, after which it decays according to an inverse square root
schedule. All models were trained until convergence of validation
accuracy, and evaluation was done at each 10k steps for
\langp{ro}{en} and \langp{en}{de} and at each 5k steps for
\langp{de}{en} and \langp{ja}{en}. The end-to-end computational
overhead of our methods, when compared to standard softmax, is
relatively small; in training tokens per second, the models using
-\entmaxtext and -\entmaxtext are, respectively, 
and  the speed of the softmax model.

\paragraph{Results.}
We report test set tokenized BLEU~\citep{papineni2002bleu} results in
Table \ref{table:mt}. We can see that replacing softmax by
\entmaxtext{} does not hurt performance in any of the datasets;
indeed, sparse attention Transformers tend to have slightly higher
BLEU, but their sparsity leads to a better potential for analysis. In
the next section, we make use of this potential by exploring the
learned internal mechanics of the self-attention heads.

\section{Analysis}

We conduct an analysis for the higher-resource dataset WMT 2014
English  German of the attention in the sparse adaptive
Transformer model (-\entmaxtext) at multiple levels: we
analyze high-level statistics as well as individual head behavior.
Moreover, we make a qualitative analysis of the interpretability
capabilities of our models.

\subsection{High-Level Statistics}
\label{sec:stats}

\paragraph{What kind of {\boldmath } values are learned?}
\figref{learning_alpha} shows the learning trajectories of the
 parameters of a selected subset of heads. We generally
observe a tendency for the randomly-initialized  parameters
to decrease initially, suggesting that softmax-like behavior may be
preferable while the model is still very uncertain. After around one
thousand steps, some heads change direction and become sparser,
perhaps as they become more confident and specialized. This shows
that the initialization of  does not predetermine its
sparsity level or the role the head will have throughout. In
particular, head  in the encoder self-attention layer  first
drops to around  before becoming one of the sparsest
heads, with .

The overall distribution of  values at convergence can be
seen in \figref{hist_alphas}. We can observe that the encoder
self-attention blocks learn to concentrate the  values in two
modes: a very sparse one around , and a dense
one between softmax and 1.5-\entmaxtext{}. However, the decoder self
and context attention only learn to distribute these parameters in a
single mode. We show next that this is reflected in the average
density of attention weight vectors as well.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/learning_alpha.pdf}
    \caption{\label{fig:learning_alpha}
    Trajectories of  values for a subset of the heads during
    training. Initialized at random, most heads become denser in the
    beginning, before converging. This suggests that dense attention may
    be more beneficial while the network is still uncertain, being
    replaced by sparse attention afterwards.}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/hist_alphas.pdf}
    \caption{\label{fig:hist_alphas}Distribution of learned  values per attention block.
    While the encoder self-attention has a bimodal distribution
    of values of ,
    the decoder self-attention and context attention have a single mode.}
\end{figure}

\paragraph{Attention weight density when translating.}
For any , it would still be possible for the weight
matrices in Equation~\ref{eq:head} to learn re-scalings so as to make
attention sparser or denser. To visualize the impact of adaptive
 values, we compare the empirical attention weight density
(the average number of tokens receiving non-zero attention) within
each module, against sparse Transformers with fixed .

\figref{hist_densities} shows that, with fixed , heads
tend to be sparse and similarly-distributed in all three attention
modules. With learned , there are two notable changes: (i) a
prominent mode corresponding to fully dense probabilities, showing
that our models learn to combine sparse and dense attention, and (ii)
a distinction between the encoder self-attention -- whose background
distribution tends toward extreme sparsity -- and the other two
modules, who exhibit more uniform background distributions. This
suggests that perhaps entirely sparse Transformers are suboptimal.

The fact that the decoder seems to prefer denser attention
distributions might be attributed to it being auto-regressive, only
having access to past tokens and not the full sentence. We speculate
that it might lose too much information if it assigned weights of
zero to too many tokens in the self-attention, since there are fewer
tokens to attend to in the first place.

Teasing this down into separate layers,
\figref{head_density_per_layer} shows the average (sorted) density of
each head for each layer. We observe that -\entmaxtext{} is
able to learn different sparsity patterns at each layer, leading to
more variance in individual head behavior, to clearly-identified
dense and sparse heads, and overall to different tendencies compared
to the fixed case of .

\begin{figure}[t]
    \centering
        \includegraphics[width=.95\columnwidth]{figures/hist_densities.pdf}
        \caption{\label{fig:hist_densities}
    Distribution of attention densities (average number of tokens
    receiving non-zero attention weight) for all attention heads and all
    validation sentences.
    When compared to 1.5-\entmaxtext{}, -\entmaxtext{}
    distributes the sparsity in a more uniform manner, with a clear mode
    at fully dense attentions, corresponding to the heads with low
    . In the softmax case, this distribution would lead to a
    single bar with density 1.}
\end{figure}

\begin{figure}[t]
    \centering
        \includegraphics[
            width=.95\columnwidth]{figures/head_density_per_layer.pdf}
        \caption{\label{fig:head_density_per_layer}
    Head density per layer for fixed and learned . Each line
    corresponds to an attention head; lower values mean that that
    attention head is sparser. Learned  has higher variance.
    }
\end{figure}

\paragraph{Head diversity.}
To measure the overall disagreement between attention heads, as a measure of
head diversity, we use the following generalization of the Jensen-Shannon
divergence:



where  is the vector of attention weights assigned by head
 to each word in the sequence, and  is the Shannon entropy,
base-adjusted based on the dimension of  such that . We average this measure over the entire validation set. The
higher this metric is, the more the heads are taking different roles
in the model.

\figref{js_divs}  shows that both sparse Transformer variants show more
diversity than the traditional softmax one. Interestingly, diversity seems to
peak in the middle layers of the encoder self-attention and context attention,
while this is not the case for the decoder self-attention.

The statistics shown in this section can be found for the other language pairs in
\appref{plots_lps}.

\begin{figure}[h]
    \includegraphics[width=\columnwidth]{figures/js_divs.pdf}
    \caption{\label{fig:js_divs}
Jensen-Shannon Divergence between heads at each layer. Measures the
disagreement between heads: the higher the value, the more the heads
are disagreeing with each other in terms of where to attend. Models
using sparse \entmaxtext have more diverse attention than the softmax
baseline.
}
\end{figure}

\subsection{Identifying Head Specializations}

Previous work pointed out some specific roles played by
different heads in the softmax Transformer
model~\citep{voita2018context,tang2018why,specialized}. Identifying
the specialization of a head can be done by observing the
type of tokens or sequences that the head often assigns most
of its attention weight; this is facilitated by sparsity.

\paragraph{Positional heads.}
One particular type of head, as noted by \citet{specialized}, is the
positional head. These heads tend to focus their attention on either
the previous or next token in the sequence, thus obtaining
representations of the neighborhood of the current time step. In
\figref{head_prev}, we show attention plots for such heads, found for
each of the studied models. The sparsity of our models allows these
heads to be more confident in their representations, by assigning the
whole probability distribution to a single token in the sequence.
Concretely, we may measure a positional head's \textbf{confidence} as
the average attention weight assigned to the previous token. The
softmax model has three heads for position , with median
confidence . The -\entmaxtext model also has three heads
for this position, with median confidence . The adaptive
model has four heads, with median confidences , the
lowest-confidence head being dense with , while the
highest-confidence head being sparse ().

For position , the models each dedicate one head, with confidence
around , slightly higher for \entmaxtext. The adaptive model
sets  for this head.

\begin{figure}[h]
    \includegraphics[width=\columnwidth]{figures/head_prev.pdf}
\caption{Self-attention from the most confidently previous-position head in
each model. The learned parameter in the -\entmaxtext model
is . Quantitatively more confident, visual inspection
confirms that the adaptive head behaves more consistently.}
\label{fig:head_prev}
\end{figure}

\paragraph{BPE-merging head.}
Due to the sparsity of our models, we are able to identify other head
specializations, easily identifying which heads should be further analysed.
In \figref{head_bpe} we show one such head where the  value
is particularly high (in the encoder, layer 1, head 4 depicted in
\figref{learning_alpha}). We found that this head most often looks at
the current time step with high confidence, making it a positional head
with offset . However, this head often spreads weight sparsely
over 2-3 neighboring tokens, when the tokens are part of the same BPE
cluster\footnote{BPE-segmented words are denoted by  in the
figures.} or hyphenated words. As this head is in the first layer, it
provides a useful service to the higher layers by combining
information evenly within some BPE clusters.

For each BPE cluster or cluster of hyphenated words,
we computed a score between 0 and 1 that corresponds to the
maximum attention mass assigned by any token to the rest of the
tokens inside the cluster in order to quantify the BPE-merging
capabilities of these heads.\footnote{If the
cluster has size 1, the score is the weight the token assigns to
itself.} There are not any attention heads in
the softmax model that are able to obtain a score over , while
for -\entmaxtext and -\entmaxtext there are two heads
in each ( and  for -\entmaxtext and  and
 for -\entmaxtext).

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/head_bpe}
    \caption{BPE-merging head  discovered in the
    -\entmaxtext model. Found in the first encoder layer,
    this head learns to discover some subword units and combine their
    information, leaving most words intact. It places  of
    its probability mass within the same BPE cluster as the current
    token: more than any head in any other model.}
    \label{fig:head_bpe}
\end{figure}

\begin{figure}[t]
    \begin{subfigure}[b]{\columnwidth}
    \includegraphics[width=\columnwidth]{figures/head_interro.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[width=\columnwidth]{figures/head_interro_not.pdf}
    \end{subfigure}
\caption{Interrogation-detecting heads in the three models. The top sentence
is interrogative while the bottom one is declarative but includes the
interrogative word ``what''. In the top example, these {\it
interrogation heads} assign a high probability to the question mark
in the time step of the interrogative word (with 
probability), while in the bottom example since there is no question
mark, the same head does not assign a high probability to the last
token in the sentence during the interrogative word time step.
Surprisingly, this head prefers a low , as can be seen
from the dense weights. This allows the head to identify the noun
phrase ``Armani Polo" better.}
\label{fig:head_interro}
\end{figure}

\begin{figure}[t]
    \includegraphics[
        width=\columnwidth]{figures/sparsity_difference.pdf}
\caption{Example of two sentences of similar length where the same head
() exhibits different sparsity. The longer phrase in the
example on the right ``a sexually transmitted disease'' is handled
with higher confidence, leading to more sparsity.
}
\label{fig:sparsity_difference}
\end{figure}

\paragraph{Interrogation head.}
On the other hand, in \figref{head_interro} we show a head for which our
adaptively sparse model chose an  close to 1, making it
closer to softmax (also shown in {\it encoder, layer 1, head 3}
depicted in \figref{learning_alpha}). We observe that this head
assigns a high probability to question marks at the end of the
sentence in time steps where the current token is interrogative, thus
making it an interrogation-detecting head. We also observe this type
of heads in the other models, which we also depict in
\figref{head_interro}. The average attention weight placed on the
question mark when the current token is an interrogative word is
 for softmax,  for -\entmaxtext, and 
for -\entmaxtext.

Furthermore, we can examine sentences where some tendentially sparse
heads become less so, thus identifying sources of ambiguity where the
head is less confident in its prediction. An example is shown in
\figref{sparsity_difference} where sparsity in the same head differs
for sentences of similar length.

\section{Related Work}\label{sec:related}

\paragraph{Sparse attention.}
Prior work has developed sparse attention mechanisms, including
applications to NMT~\citep{sparsemax, malaviya2018sparse, fusedmax,
shao2019ssn, maruf2019selective}. \citet{entmax} introduced the
\entmaxtext function this work builds upon. In their work, there is a
single attention mechanism which is controlled by a fixed .
In contrast, this is the first work to allow such attention mappings
to \emph{dynamically} adapt their curvature and sparsity, by
automatically adjusting the continuous  parameter. We also
provide the first results using sparse attention in a Transformer
model.

\paragraph{Fixed sparsity patterns.}
Recent research improves the scalability of Transformer-like networks
through static, fixed sparsity patterns
\citep{openai_sparse_transf,dynamic_conv}. Our adaptively-sparse
Transformer can dynamically select a sparsity pattern that finds
relevant words regardless of their position (\eg,
\figref{head_interro}). Moreover, the two strategies could be
combined. In a concurrent line of research, \citet{Sukhbaatar2019}
propose an adaptive attention span for Transformer language models.
While their work has each head learn a different contiguous span of
context tokens to attend to, our work finds different sparsity
patterns in the same span. Interestingly, some of their findings
mirror ours -- we found that attention heads in the last layers tend
to be denser on average when compared to the ones in the first
layers, while their work has found that lower layers tend to have a
shorter attention span compared to higher layers.

\paragraph{Transformer interpretability.}
The original Transformer paper~\citep{vaswani2017attention} shows
attention visualizations, from which some speculation can be made of
the roles the several attention heads have.
\citet{marecek-rosa-2018-extracting} study the syntactic abilities of
the Transformer self-attention, while \citet{raganato2018analysis}
extract dependency relations from the attention weights.
\citet{bert-rediscovers} find that the self-attentions in
BERT~\citep{devlin2018bert} follow a sequence of processes that
resembles a classical NLP pipeline. Regarding redundancy of heads,
\citet{specialized} develop a method that is able to prune heads of
the multi-head attention module and make an empirical study of the
role that each head has in self-attention (positional, syntactic and
rare words). \citet{li2018multi} also aim to reduce head redundancy
by adding a regularization term to the loss that maximizes head
disagreement and obtain improved results. While not considering
Transformer attentions, \citet{jain2019attention} show that
traditional attention mechanisms do not necessarily improve
interpretability since softmax attention is vulnerable to an
adversarial attack leading to wildly different model predictions for
the same attention weights. Sparse attention may mitigate these
issues; however, our work focuses mostly on a more mechanical aspect
of interpretation by analyzing head behavior, rather than on
explanations for predictions.

\section{Conclusion and Future Work}
We contribute a novel strategy for adaptively sparse attention, and,
in particular, for adaptively sparse Transformers. We present the
first empirical analysis of Transformers with sparse attention
mappings (\ie, \entmaxtext), showing potential in both translation
accuracy as well as in model interpretability.

In particular, we analyzed how the attention heads in the proposed
adaptively sparse Transformer can specialize more and with higher
confidence. Our adaptivity strategy relies only on gradient-based
optimization, side-stepping costly per-head hyper-parameter searches.
Further speed-ups are possible by leveraging more parallelism in the
bisection algorithm for computing -\entmaxtext.

Finally, some of the automatically-learned behaviors of our
adaptively sparse Transformers -- for instance, the near-deterministic
positional heads or the subword joining head -- may provide new ideas
for designing static variations of the Transformer.

\section*{Acknowledgments}
This work was supported by the European Research Council (ERC StG
DeepSPIN 758969), and by the Funda\c{c}\~ao para a Ci\^encia e
Tecnologia through contracts UID/EEA/50008/2019 and
CMUPERI/TIC/0046/2014 (GoLocal). We are grateful to Ben Peters for
the -\entmaxtext code and Erick Fonseca, Marcos Treviso,
Pedro Martins, and Tsvetomila Mihaylova for insightful group
discussion. We thank Mathieu Blondel for the idea to learn .
We would also like to thank the anonymous reviewers for their helpful
feedback.

\bibliography{refs}
\bibliographystyle{acl_natbib}

\clearpage
\onecolumn
\appendix
\begin{center}
{\huge \textbf{Supplementary Material}}
\end{center}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1.5ex plus 0.5ex minus .2ex}


\def\RR{{\mathbb{R}}}
\def\EE{{\mathbb{E}}}
\def\RRY{\RR^{|\cY|}}
\def\y{\bm{y}}
\def\triangleY{\triangle^{|\cY|}}
\def\sizeY{{|\cY|}}
\def\cC{{\mathcal{C}}}
\def\cD{{\mathcal{D}}}
\def\cX{{\mathcal{X}}}
\def\cY{{\mathcal{Y}}}

\section{High-Level Statistics Analysis of Other Language Pairs}
\label{sec:plots_lps}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_alphas_ro.pdf}
    \caption{\label{fig:hist_alphas_ro}WMT 2016 \langp{ro}{en}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_alphas_ja.pdf}
    \caption{\label{fig:hist_alphas_ja}KFTT \langp{ja}{en}.}
\end{subfigure}

\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_alphas.pdf}
    \caption{\label{fig:hist_alphas_en}WMT 2014 \langp{en}{de}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_alphas_de.pdf}
    \caption{\label{fig:hist_alphas_de}IWSLT 2017 \langp{de}{en}.}
\end{subfigure}
\caption{Histograms of  values.}
\label{fig:hist_alphas_lps}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_densities_ro.pdf}
    \caption{\label{fig:hist_densities_ro}WMT 2016 \langp{ro}{en}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_densities_ja.pdf}
    \caption{\label{fig:hist_densities_ja}KFTT \langp{ja}{en}.}
\end{subfigure}

\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_densities.pdf}
    \caption{\label{fig:hist_densities_en}WMT 2014 \langp{en}{de}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/hist_densities_de.pdf}
    \caption{\label{fig:hist_densities_de}IWSLT 2017 \langp{de}{en}.}
\end{subfigure}
\caption{Histograms of head densities.}
\label{fig:hist_densities_lps}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/js_divs_ro.pdf}
    \caption{\label{fig:js_divs_ro}WMT 2016 \langp{ro}{en}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/js_divs_ja.pdf}
    \caption{\label{fig:js_divs_ja}KFTT \langp{ja}{en}.}
\end{subfigure}

\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/js_divs.pdf}
    \caption{\label{fig:js_divs_en}WMT 2014 \langp{en}{de}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/js_divs_de.pdf}
    \caption{\label{fig:js_divs_de}IWSLT 2017 \langp{de}{en}.}
\end{subfigure}
\caption{Jensen-Shannon divergence over layers.}
\label{fig:js_divs_lps}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/head_density_per_layer_ro.pdf}
    \caption{\label{fig:head_density_per_layer_ro}WMT 2016 \langp{ro}{en}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/head_density_per_layer_ja.pdf}
    \caption{\label{fig:head_density_per_layer_ja}KFTT \langp{ja}{en}.}
\end{subfigure}

\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/head_density_per_layer.pdf}
    \caption{\label{fig:head_density_per_layer_en}WMT 2014 \langp{en}{de}.}
\end{subfigure}
\begin{subfigure}[b]{.47\linewidth}
    \includegraphics[width=\linewidth]{figures/head_density_per_layer_de.pdf}
    \caption{\label{fig:head_density_per_layer_de}IWSLT 2017 \langp{de}{en}.}
\end{subfigure}
\caption{Head densities over layers.}
\label{fig:head_density_per_layer_lps}
\end{figure}

\clearpage

\section{Background}

\subsection{Regularized Fenchel-Young prediction functions}
\begin{definition}[\citealt{blondel2019learning}]\label{def:rpf}

Let  be a strictly convex
regularization function. We define the prediction function  as

\end{definition}

\subsection{Characterizing the {\boldmath -\entmaxtext} mapping}\label{sec:bgform}

\begin{lemma}[\citealt{entmax}]
\label{lemma:tsallis_reduction}For any , there exists a unique  such that

\end{lemma}
\begin{proof}
From the definition of ,

we may easily identify it with a regularized prediction function
(Def.~\ref{def:rpf}):

We first note that for all ,

From the constant invariance and
scaling properties of 
\citep[Proposition~1, items~4--5]{blondel2019learning},

Using \citep[Proposition~5]{blondel2019learning}, noting that
 and ,
yields

Since  is strictly convex on the simplex,  has a unique
solution . \eqnref{rootform}
implicitly defines a one-to-one mapping between  and 
as long as ,
therefore  is also unique.
\end{proof}

\subsection{Connections to softmax and sparsemax}\label{sec:softmax}
The Euclidean projection onto the simplex, sometimes referred to, in the context of
neural attention, as sparsemax \citep{sparsemax},
is defined as

The solution can be characterized through the
unique threshold  such that  and \citep{Held1974}

Thus, each coordinate of the sparsemax solution is a piecewise-linear function.
Visibly, this expression is recovered when setting  in the
-\entmaxtext expression (\eqnref{entmax_form_supp}); for other
values of , the exponent induces curvature.

On the other hand, the well-known softmax is usually defined through the
expression

which can be shown to be the unique solution of the optimization problem

where  is the Shannon entropy.
Indeed, setting the gradient to  yields the condition
, where  and  are Lagrange
multipliers for the simplex constraints  and ,
respectively. Since the \lhs is only finite for ,
we must have  for all , by complementary
slackness. Thus, the solution must have the form , yielding \eqnref{softmax-supp}.
\section{Jacobian of {\boldmath }-\entmaxtext \wrt the shape parameter
{\boldmath }:
Proof of Proposition~\ref{prop:grad_alpha}}
\label{sec:alpha_grad}
Recall that the \entmaxtext transformation is defined as:

where  and  is the Tsallis entropy,

and  is the Shannon entropy.

In this section, we derive the Jacobian of  with respect to the scalar parameter .

\subsection{General case of {\boldmath }}

From the KKT conditions associated with the optimization problem in
Eq.~\ref{eq:entmax_form_supp}, we have that the solution  has the following form, coordinate-wise:

where  is a scalar Lagrange multiplier that ensures that
 normalizes to 1, \ie, it is defined implicitly by the condition:

For general values of , Eq.~\ref{eq:tau_condition} lacks a closed form solution. This makes the computation of the
Jacobian

non-trivial. Fortunately, we can use the technique of implicit differentiation
to obtain this Jacobian.

The Jacobian exists almost everywhere, and the expressions we derive
expressions yield a generalized Jacobian \citep{clarke_book} at any
non-differentiable points that may occur for certain (, ) pairs.
We begin by noting that  if
, because increasing  keeps sparse coordinates
sparse.\footnote{This follows from the margin property of 
 \citep{blondel2019learning}.}
Therefore we need to worry only
about coordinates that are in the support of . We will assume
hereafter that the \textsuperscript{th} coordinate of  is non-zero.
We have:

We can see that this Jacobian depends on , which we now compute using implicit differentiation.

Let ).
By differentiating both sides of Eq.~\ref{eq:tau_condition}, re-using some of
the steps in Eq.~\ref{eq:gradient_alpha_01}, and recalling Eq.~\ref{eq:p_kkt},
we get

from which we obtain:


Finally, plugging Eq.~\ref{eq:gradient_tau} into Eq.~\ref{eq:gradient_alpha_01}, we get:

where we denote by

The distribution  can be interpreted as a ``skewed''
distribution obtained from , which appears in the Jacobian of
 \wrt  as well~\cite{entmax}.


\subsection{Solving the indetermination for }

We can write Eq.~\ref{eq:gradient_alpha} as

When , we have , which leads to a  indetermination.

To solve this indetermination, we will need to apply L'H\^opital's rule twice.
Let us first compute the derivative of  with respect to . We have

therefore

Differentiating the numerator and denominator in Eq.~\ref{eq:gradient_alpha_fraction}, we get:

with

and

When ,  becomes again a  indetermination, which we can solve by applying again L'H\^opital's rule. Differentiating the numerator and denominator in Eq.~\ref{eq:B_shannon}:

Finally, summing Eq.~\ref{eq:A_shannon} and Eq.~\ref{eq:B_shannon_02}, we get


\subsection{Summary}

To sum up, we have the following expression for the Jacobian of  with respect to :




\end{document}
