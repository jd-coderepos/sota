
\begin{figure*}[t]
\centering
  \includegraphics[width=0.49\linewidth]{figs/ucf101_thres_impact1}	
  \includegraphics[width=0.49\linewidth]{figs/sparsity_comb1}
  \caption{Impact of video  and action  sparsity parameters, individually (\emph{left}) and when combined (\emph{right}) on UCF101 dataset.}
\label{fig:impact_sparse}
\end{figure*}


\section{Experiments} \label{sec:exp}
In this section, we employ the proposed object2action model on four recent action classification datasets.
We first describe these datasets and the text corpus used.
Second, we analyze the impact of applying the Fisher Word Vector over the baseline of the Average Word Vector for computing the affinity between objects and actions, and we evaluate the action and video sparsity parameters. 
Third, we report zero-shot classification results on the four datasets, and we compare against  the traditional zero-shot setting where actions are used during training. 
Finally, we report performance of zero-shot spatio-temporal action localization.

\subsection{Prior knowledge and Datasets}
Our method is based on freely available resources which we use as prior knowledge for zero-shot action recognition. For the four action classification datasets datasets used we only use the \emph{test} set.


\myparagraph{Prior knowledge}
We use two types of prior knowledge.
First, we use deep convolutional neural network trained from ImageNet images with objects~\cite{Krizhevsky_imagenetclassification} as visual representation.
Second, for the semantic embedding we train the skip-gram model of word2vec on the metadata (title, descriptions, and tags) of the YFCC100M dataset~\cite{thomee15yfcc100m}, this dataset contains about 100M Flickr images. 
Preliminary experiments showed that using visual metadata results in better performance than training on Wikipedia or GoogleNews data. 
We attribute this to the more visual descriptions used in the YFC100M dataset, yielding a semantic embedding representing visual language and relations.



\myparagraph{UCF101~\cite{ucf101}} 
This dataset contains 13,320 videos of 101 action classes. 
It has realistic action videos collected from YouTube and has large variations in camera motion, object appearance/scale, viewpoint, cluttered background, illumination conditions, \etc. 
Evaluation is measured using average class accuracy, over the three provided test-splits with around 3,500 videos each.

\myparagraph{THUMOS14~\cite{THUMOS14}} 
This dataset has the same 101 action classes as in UCF101, but the videos are have a longer duration and are temporally unconstrained. 
We evaluate on the testset containing 1,574 videos, using mean average precision (mAP) as evaluation measure.

\myparagraph{HMDB51~\cite{hmdb51}} 
This dataset contains 51 action classes and 6,766 video clips extracted from various sources, ranging from YouTube to movies, and hence this dataset contains realistic actions.
Evaluation is measured using average class accuracy, over the three provided test-splits with each 30 videos per class (1,530 videos per split).

\myparagraph{UCF Sports~\cite{Rodriguez:cvpr08}} 
This dataset contains 150 videos of 10 action classes. 
The videos are from sports broadcasts capturing sport actions in dynamic and cluttered environments.
Bounding box annotations are provided and this dataset is often used for spatio-temporal action localization. 
For evaluation we use the test split provided by~\cite{tian_iccv11} and performance is measured by average class accuracy.

\begin{table}[t]
\centering
{\small
\begin{tabular}{| c | l | c | c | }
\hline
{\bf Embedding} & {\bf Sparsity}	&  {\bf Best }   	&   {\bf Accuracy at}   \\	
		& 			&  {\bf accuracy}	&   =10, =100 	\\ \hline	
		&	Video  		&    18.0\%		&       17.5\%	  	\\	
AWV		&	Action 		&    22.7\%		&       21.9\% 		\\		
		&	Combine		&    22.7\%		&	 21.6\%		\\	\hline	
		&	Video 	  	&    29.1\%         	&      29.0\%     	\\	
FWV		&	Action 		&    30.8\%         	&      30.3\%	 	\\		
		&	Combine    	&    30.8\%    		&    	30.3\%		\\  \hline	
\end{tabular}}
\caption{Evaluating AWV and FWV for object to class affinity, and comparing action and video sparsity on UCF101 dataset.}
\label{table:sparse_comb}
\end{table}

\subsection{Properties of Objects2action}
\label{sec:params}


\begin{table*}[t]
\centering
{\small
\renewcommand{\tabcolsep}{3pt}
\begin{tabular}{| c | l | c | c | c | c | }
\hline
{\bf Embedding}		& {\bf Sparsity}  &    {\bf UCF101}   &   {\bf HMDB51}	&  {\bf THUMOS14} & {\bf UCF Sports}    \\      \hline
\multirow{3}{*}{AWV}		& None		&    16.7\% 	        &      8.0\%        &    4.4\%        & 13.9\%       \\	 	
					& Video		&    17.5\%        	&      7.7\%        &   10.7\%        & 13.9\%       \\
					& Action		&    21.9\%        	&      9.9\%        &   19.9\%        & 25.6\%       \\	\hline						
\multirow{3}{*}{FWV}	   	& None		&    28.7\%  	    	&     14.2\%        &    25.9\%        & 23.1\%       \\			
                    			& Video		&    29.0\%         	&     14.5\%        &   27.8\%        & 23.1\%       \\
					& Action		&   {\bf 30.3\%}    	&   {\bf 15.6\%}    & {\bf 33.4\%}    & {\bf 26.4\%}  \\    \hline \hline
\multicolumn{2}{|c|}{{Supervised}} &   63.9\%    &  35.1\%   	& 56.3\%		&  60.7\%	\\	\hline		
\end{tabular}}
\caption{{Evaluating semantic embeddings, action and video spartsity: Average accuracies (mAP for THUMOS14) for the four datasets. Action sparsity and FWV both boost the performance consistently. Supervised upper-bound using object scores as representation.}}
\label{table:cls_comp}
\end{table*}

\myparagraph{Semantic embedding} 
We compare the AWV with the FWV as semantic embedding.
For the FWV, we did a run of preliminary experiments to find suitable parameters for the number of components (varying ), the partial derivatives used (weight, mean, and/or variance)  and whether to use PCA or not.
We found them all to perform rather similar in terms of classification accuracy. 
Considering a label has only a few words (1 to 4), we therefore fix , apply PCA to reduce dimensionality by a factor of 2, and to use only the partial derivatives \wrt the mean (conforming the results in ~\cite{clinchant13ictir}).
Hence, the total dimensionality of FWV is , equivalent to the dimensionality of AWV, which allows for a fair comparison.
The two embeddings are compared in Table~\ref{table:sparse_comb} and Figure~\ref{fig:impact_sparse} (\emph{left}), and  FWV clearly outperforms AWV in all the cases.

\myparagraph{Sparsity parameters} 
In Figure~\ref{fig:impact_sparse},  we evaluate the action sparsity and video sparsity parameters.
The left plot shows average accuracy versus  and . 
It is evident that action sparsity, \ie, selecting most responsive object classes for a given action class leads to a better 
performance than video sparsity. 
The video sparsity (green lines) is more stable throughout and achieves best results in the range of 10 to 100 objects. 
Action sparsity is a bit sinuous, nevertheless it always performs better, independent of the type of embedding. 
Action sparsity is at its best in the range of selecting the 5 to 30 most related object classes. 
For the remaining experiments, we fix these parameters as  and .

We also consider the case when we apply sparsity on both video and actions (see the right plot). 
Applying sparsity on both sides does not improve performance, it is equivalent to the best action sparsity setting, showing that selecting the most prominent objects per action suffice for zero-shot action classification.
Table~\ref{table:sparse_comb} summarise the accuracies for the best and fixed choices of  and . 


\subsection{Zero-shot action classification}	 \label{sec:act_cls}
In this section we employ the obtained parameters of Object2action, from the previous section, for zero-shot action classification on the test splits of all four  datasets.
We evaluate the benefit of using the FWV over AWV, and the effect of using sparsity (video sparsity, action sparsity or no sparsity at all).
The results are provided in Table~\ref{table:cls_comp}. 
We observe that the FWV always outperforms AWV, and that it is always beneficial to apply sparsity, and action sparsity with FWV performs the best.
{We also provide the supervised upper-bounds using the same video representation of object classification scores in Table~\ref{table:cls_comp}. Here and for all the experiments, 
we power normalize () the video representations before applying  normalization.}

\myparagraph{Comparison to few-shot supervised learning}
In this experiment we compare the zero-shot classifier against few-shot supervised learning, on the THUMOS14 dataset.
For this we consider two types of video representation. 
The first representations, uses the state-of-the-art motion representation of~\cite{wang:imptraj13}, by encoding robust MBH descriptors along the improved trajectories~\cite{wang:imptraj13} using Fisher Vectors.
We follow the standard parameter cycle, by applying PCA, using a GMM with  Gaussians, employing power and  normalization.
The second representation uses the object scores  of a video, here also we apply power and  normalization. 
For both representations, we train one-vs-rest linear SVM classifiers and we average performance over 20 runs for every given number of train examples.

The results in mAP are shown in Figure~\ref{fig:thumos_cls}.
Interestingly, to perform better than our zero-shot classification, fully supervised classification setup requires 4 and 10 samples per class for object and motion representations respectively.


\begin{figure}[t]
\centering
  \includegraphics[width=0.9\linewidth]{figs/Thumos14_varTrain1}
  \caption{Our approach compared with the supervised classification with few examples per class: State-of-the-art object and motion representations respectively {require 4 and 10 examples per class} to catch up with our approach, which uses no example.}
\label{fig:thumos_cls}
\end{figure}


\begin{comment}
\begin{table*}[t]
\centering
{\small
\begin{tabular}{|l | c | c | c | c | }
\hline
{\bf Method} 		& {\bf UCF101}	&   {\bf HMDB51} &   {\bf THUMOS14}	&   {\bf UCFS}			\\	\hline
Supervised	 	&   63.9\%	&    35.1\%   	 & 54.7\%		&  60.7\%	\\	\hline				Transformed (AWV) 	&   50.0\%	&    27.9\%  	 &			&  	\\	
Transformed (FWV)	&   52.9\%	&    26.4\%  	 &			&	\\		\hline
Number of 	 	& 13320		& 5100		&  14330		& 103	\\		
training examples		&		&		&			&	\\		 \hline
\end{tabular}}
\caption{From supervised to zero-shot: . \cs{This table is not presented wisely. I am thinking a bar-graph with 3 types of dashed lines to indicate the supervised upper-bounds is better. Why emphasize this at all? We can also mention in text only.} \com{MJ: I am also not sure if this should be included, mentioning in text might be enough if we decide to keep it.}}
\label{table:superv_zero}
\end{table*}
\end{comment}


\myparagraph{Object transfer versus action transfer}
We now experiment with the more conventional setup for zero-shot learning, where we have training data for some action classes, disjoint from the set of test action classes. We keep half of the classes of a given dataset as train labels and the other half as our zero-shot classes. The action classifiers are learned from odd (or even) numbered classes and videos from the even (or odd) numbered classes are tested. 

{We evaluate two types of approaches for action transfer, \ie, when training classes are also actions. 
The first method uses the provided action attributes for zero-shot classification with direct attribute prediction~\cite{lampert09cvpr}. Since attributes are available only for 
UCF101, we experiment on this dataset. The train videos of the training classes are used to learn linear SVMs for the provided 115 attributes. 
The second method uses action labels embedded by FWV to compute affinity between train and test action labels. We use the same GMM with  components learned on 
ImageNet object labels. Here linear SVMs are learned for the training action classes. The results are reported for UCF101 and HMDB51 datasets.
For both the above approaches for action transfer, we use MBH descriptors encoded by Fisher vectors for video representation.
The results are reported in Table~\ref{table:obj_act_comp}.}

{For comparison with our approach, the same setup of testing on odd or even numbered classes is repeated with the object labels. The training set is ImageNet objects, so no video example 
is used for training.}
Table~\ref{table:obj_act_comp} compares object transfer and action transfer for zero-shot classification. 
{Object transfer leads to much better learning compared to both the methods for action transfer.}
The main reason for the inferior performance using actions is that there are not enough action labels or action attributes to describe the test classes, whereas from 15k objects there is 
a good chance to find a few related object classes.

\begin{comment}
\begin{table}[t]
\centering
{\small
\begin{tabular}{| l | c | c | c | c | }
\hline
{\bf Training}	&  \multicolumn{2}{c|} {\bf UCF101}	&	\multicolumn{2}{c|} {\bf HMDB51}	\\	\cline{2-5}
{\bf source} 	&    {\bf Odd} &   {\bf Even}		&  {\bf Odd} 	& {\bf Even}    \\      \hline
Actions		&    14.1\%      &    14.2\%        	&  8.2\% 	&  9.1\%  	\\	\hline
Objects		&    26.6\%      &    18.6\%        	& 14.3\% 	& 12.7\%    \\  \hline
\end{tabular}}
\caption{Object transfer compared with action transfer in a conventional zero-shot set-up. Note the supremacy of object without the need for action classes during training.}
\label{table:obj_act_comp}
\end{table}
\end{comment}


\begin{comment}
\begin{table}[t]
\centering
{\small
\begin{tabular}{| l | l | c | c | c | }
\hline
{\bf Labels} 	  &  {\bf Train set}	&  {\bf Test set} &  {\bf UCF101}	&  {\bf HMDB51}	\\	\hline
\multirow{ 2}{*}{Actions} &	Even		&	Odd	  &	14.1\%		&  8.2\%		\\
			  &	Odd		&	Even	  &	14.2\%		&  9.1\%		\\	\hline
\multirow{ 2}{*}{Objects} & \multirow{ 2}{*}{ImageNet}	&	Odd	  &	26.6\%		&  14.3\%		\\
			  &			&	Even	  &	18.6\%		&  12.7\%		\\	\hline
\end{tabular}}
\caption{Object transfer compared with action transfer in a conventional zero-shot set-up.}
\label{table:obj_act_comp}
\end{table}
\end{comment}

\begin{comment}
\begin{table}[t]
\centering
{\small
\begin{tabular}{| l | l | l | c | c | c | }
\hline
{\bf Labels}      	  &			   &  {\bf Train}    &  {\bf Test} &  {\bf UCF101}      &  {\bf HMDB51} \\      \hline
\multirow{ 4}{*}{Actions} &  \multirow{ 2}{*}{FWV} &     Even            &       Odd       &     15.4\%         & 12.8\%                \\
                          &			   &     Odd             &       Even      &     15.9\%         & 13.9\%                \\      \cline{2-6}
	 		  &  \multirow{ 2}{*}{DAP} &     Even            &       Odd       &     16.2\%         &  ---                \\
		          &			  &     Odd             &       Even      &     14.6\%          &   ---                \\      \hline
\multirow{ 2}{*}{Objects} & \multirow{ 2}{*}{FWV} & \multirow{ 2}{*}{ImageNet}  &      Odd   &     35.2\%       &  16.2\%   	\\
                          &			  &                     &       Even      &     38.7\%          &  24.2\%               \\      \hline
\end{tabular}}
\caption{Object transfer compared with action transfer in a conventional zero-shot set-up.}
\label{table:obj_act_comp}
\end{table}
\end{comment}

\begin{table}[t]
\centering
{\small
\renewcommand{\tabcolsep}{5pt}
\begin{tabular}{| c | l | l | c | c | }
\hline
{\bf Method}             &  {\bf Train}    &  {\bf Test} &  {\bf UCF101}      &  {\bf HMDB51} \\      \hline \hline
\multirow{ 2}{*}{Action attributes}        &     Even     &       Odd       	&     16.2\%         &  ---                \\
			                   &     Odd      &     Even  	  	&     14.6\%          &   ---                \\      \hline
\multirow{ 2}{*}{Action labels}   	 &     Even            &       Odd       &     16.1\%         & 12.4\%                \\
                 	 &     Odd      &       Even      &     14.4\%         & 13.4\%                \\      \hline \hline
\multirow{ 2}{*}{Objects2action}	&  \multirow{ 2}{*}{ImageNet}  &      Odd   	&     {\bf 37.3\%}       &  {\bf 15.0\%}       \\      
                    		&					&       Even      &     {\bf 38.9\%}     &  {\bf 24.5\%}            \\      \hline
\end{tabular}}
 \caption{{Object transfer versus action transfer in a conventional zero-shot set-up. Direct attribute prediction~\cite{lampert09cvpr} is used with action attributes, FWV is used to embed action labels, and in our objects2action.}}
\label{table:obj_act_comp}
\end{table}

{
\myparagraph{Zero-shot event retrieval}
We further demonstrate our method on the related problem of zero-shot event retrieval. We evaluate on the TRECVID13 MED~\cite{over2013trecvid} testset for EK0 task. 
There are 20 event classes and about 27,000 videos in the testset.
Instead of using the manually specified event kit containing the event name, a definition, and a precise description in terms of salient attributes, we only rely on the class label. In Table~\ref{table:med_comp}, we report mAP using event labels embedded by AWV and FWV. We also compare with the state-of-the-art approaches of Chen \etal~\cite{chenICMR14eventWeakTag} and Wu \etal~\cite{wuCVPR14zeroEventMultiModal} reporting their settings that are most similar to ours. They learn concept classifiers from images (from Flickr, Google) or YouTube video thumbnails, be it that they also use the complete event kit description. Using only the event labels, both of our semantic embeddings outperform these methods.} 

\begin{table}[t]
\centering
{\small
\begin{tabular}{| l | c |  c | }
\hline
\multicolumn{2}{|c|}{\bf Method}             						&    {\bf mAP}  \\      \hline 
\multicolumn{2}{|l|}{Wu \etal~\cite{wuCVPR14zeroEventMultiModal} (Google images)}             	&     1.21\%               \\      
\multicolumn{2}{|l|}{Chen \etal~\cite{chenICMR14eventWeakTag}    (Flickr images)}           	&     2.40\%               \\      
\multicolumn{2}{|l|}{Wu \etal~\cite{wuCVPR14zeroEventMultiModal} (YouTube thumbnails)}        	&     3.48\%               \\      \hline \hline
\multirow{ 2}{*}{Objects2action}   & 	AWV		         &     3.49\%                \\
			 &	FWV                      &     {\bf 4.21}\%               \\     
\hline
\end{tabular}}
\caption{{Zero-shot event retrieval on TRECVID13 MED testset: Comparison with the state-of-the-art methods having similar zero-shot setup as ours. Inspite of using only event labels and images, we outperform
methods that use event description and video thumbnails.}}
\label{table:med_comp}
\end{table}

\myparagraph{Free-text action search}
As a final illustration we show in Figure~\ref{fig:visualExamples} qualitative results from free-text querying of action videos from the THUMOS14 testset. We used the whole dataset for querying, and searched for actions that are not contained in the 101 classes of THUMOS14. Results show that free-text querying offers a tool to explore a large collection of videos. Results are best when the query is close to one or a few existing action classes, for example ``Dancing" retrieves results from ``salsa-spin" and other dancing clips. Our method fails for the query ``hit wicket", although it does find cricket matches. Zero shot action recognition through an object embedding unlocks free text querying without using any kind of expensive video annotations. 

\subsection{Zero-shot action localization}


\begin{figure}[t]
\centering
  \includegraphics[width=0.84\linewidth]{figs/auc_ucf}
  \caption{Action localization without video example on UCF Sports: AUCs for different overlap thresholds are shown for  and also for the fully supervised setting with motion and object representations. The performance is promising considering no example videos are used.}
\label{fig:ST_loc}
\end{figure}



\begin{figure*}[t]
\begin{minipage}{1\linewidth}
\begin{minipage}{0.19\linewidth}
\centering  {\bf Fight in ring}
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/fight-in-ring/rank01}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/fight-in-ring/rank02}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/fight-in-ring/rank03}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/fight-in-ring/rank04}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/fight-in-ring/rank05}
\end{minipage}
\hfill
\begin{minipage}{0.19\linewidth}
\centering  {\bf Dancing}
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/dancing/rank01}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/dancing/rank02}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/dancing/rank03}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/dancing/rank04}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/dancing/rank05}
\end{minipage}
\hfill
\begin{minipage}{0.19\linewidth}
\centering  {\bf Martial arts}
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/martial-arts/rank01}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/martial-arts/rank02}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/martial-arts/rank03}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/martial-arts/rank04}  \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/martial-arts/rank05}
\end{minipage}
\hfill
\begin{minipage}{0.19\linewidth}
\centering  {\bf Smelling food}
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/smelling-food/rank01}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/smelling-food/rank02}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/smelling-food/rank03}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/smelling-food/rank04}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/smelling-food/rank05}
\end{minipage}
\hfill
\begin{minipage}{0.19\linewidth}
\centering  {\bf Hit wicket}
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/hit-wicket/rank01}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/hit-wicket/rank02}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/hit-wicket/rank03}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/hit-wicket/rank04}          \\
\includegraphics[width=0.98\linewidth]{figs/action_retrieval/hit-wicket/rank05}
\end{minipage}
~ \smallskip \\
\end{minipage}

\caption{Illustration of never seen actions on THUMOS14 testset: For a given textual action query the top five retrieved videos are shown.
The 101 classes of THUMOS14 do not contain these five action label queries. The first two queries are somewhat close to classes `\emph{Sumo wrestling}' and `\emph{Salsa spin}'. All retrieved videos for the query: `Fight in ring' include sumo wrestling. The videos retrieved for the second query: `Dancing' also includes two instances of dancing other than salsa. All results for the these two queries are technically correct. The third query `Martial arts' finds  mostly gymnasts, and a karate match. The fourth query is: `Smelling food', where we still obtain cakes, food items and dining table in the background. For the fifth query: `hit wicket' (in cricket) we do not succeed but retrieve some cricket videos. This illustration shows the potential for free keyword querying of action classes without using any examples.}
\label{fig:visualExamples}
\end{figure*}





In our final experiment, we aim to localize actions in videos, \ie, detect when and where an action of interest occurs.
We evaluate on the UCF Sports dataset, following the latest convention to localize an action spatio-temporally as a sequence of bounding boxes~\cite{Jain:tubelets,tian_iccv11,Yicong:sdpm}.
For sampling the action proposal, we use the tubelets from~\cite{Jain:tubelets} and compute object responses for each tubelet of a given video. 
We compare with the fully supervised localization using the object and motion representations described in Section~\ref{sec:act_cls}.
The top five detections are considered for each video after non-maximum suppression. 

The three are compared in Figure~\ref{fig:ST_loc}, which plots area under the ROC (AUC) for varying overlap thresholds. 
We also show the results of another supervised method of Lan \etal~\cite{tian_iccv11}. It is interesting to see that for higher thresholds our approach performs better. Considering that we 
do not use any training example it is an encouraging result.
There are other state-of-the-art methods~\cite{Jain_15kObjAct,Jain:tubelets,Yicong:sdpm} not shown in the figure to avoid clutter. These methods achieve performance comparable to or lesser than
our supervised case.

For certain action classes many objects and scene from the context might not be present in the groundtruth tubelets. Still our approach finds enough object classes for recognizing the zero-shot classes in the tubelets, as we have large number of train classes. 
In contrast, finding atomic parts of actions such as `look-up', `sit-down', `lift-leg' etc are difficult to collect or annotate.
This is one of the most critical advantages we have with objects, that it is easier to find many object or scene categories.  



\section{Conclusion} 
We presented a method for zero shot action recognition without using any video examples. Expensive video annotations are completely avoided by using abundantly available object images and labels and a freely available text corpus to relate actions into an object embedding. In addition, we showed that modeling a distribution over embedded words with the Fisher Vector is beneficial to obtain a more precise sense of the unseen action class topic, as compared to a word embedding based on simple averaging. We explored sparsity both in the object embedding, as well as in the unseen action class, showing that sparsity is beneficial over mere feature-dimensionality. 

{We validate our approach on four action datasets and achieve promising results for action classification and localization. We also demonstrate our approach for action and event retrieval on THUMOS14 and TRECVID13 MED respectively.}
The most surprising aspect of our objects2action is that it can potentially find any action in video, without ever having seen the action before.

