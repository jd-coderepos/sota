\tableofcontents
\newpage






\subsection{Statistics of Code Corpus}

\tablename~\ref{tab:dataset-comp} summarizes the composition of \name's code corpus.

\begin{table}[htbp]
    \centering
    \caption{Composition of our code corpus for pre-training.}
    \resizebox{.7\columnwidth}{!}{
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{cccc}
    \toprule
    \textbf{Language} & \textbf{\# Tokens~(B)} & \textbf{\% Tokens~(\%)} & \textbf{Language Tag} \\
    \hline
    \textbf{C++} & 45.2283 & 28.4963 & // language: C++ \\
    \textbf{Python} & 42.3250 & 26.667 & \# language: Python \\
    \textbf{Java} & 25.3667 & 15.9824 & // language: Java \\
    \textbf{JavaScript} & 11.3165 & 7.13 & // language: JavaScript \\
    \textbf{C} & 10.6590 & 6.7157 & // language: C \\
    \textbf{Go} & 7.4774 & 4.7112 & // language: Go \\
    \textbf{HTML} & 4.9355 & 3.1096 & <!--language: HTML--> \\
    \textbf{Shell} & 2.7498 & 1.7325 & \# language: Shell \\
    \textbf{PHP} & 2.1698 & 1.3671 & // language: PHP \\
    \textbf{CSS} & 1.5674 & 0.9876 & /* language: CSS */ \\
    \textbf{TypeScript} & 1.1667 & 0.7351 & // language: TypeScript \\
    \textbf{SQL} & 1.1533 & 0.7267 & -- language: SQL \\
    \textbf{TeX} & 0.8257 & 0.5202 & \% language: TeX\\
    \textbf{Rust} & 0.5228 & 0.3294 & // language: Rust \\
    \textbf{Objective-C} & 0.4526 & 0.2851 & // language: Objective-C \\
    \textbf{Scala} & 0.3786 & 0.2385 & // language: Scala \\
    \textbf{Kotlin} & 0.1707 & 0.1075 & // language: Kotlin \\
    \textbf{Pascal} & 0.0839 & 0.0529 & // language: Pascal \\
    \textbf{Fortran} & 0.077 & 0.0485 & !language: Fortran \\
    \textbf{R} & 0.0447 & 0.0281 & \# language: R \\
    \textbf{Cuda} & 0.0223 & 0.014 & // language: Cuda \\
    \textbf{C\#} & 0.0218 & 0.0138 & // language: C\# \\
    \textbf{Objective-C++} & 0.0014 & 0.0009 & // language: Objective-C++ \\
    \bottomrule
    \end{tabular}}
    }
\label{tab:dataset-comp}
\end{table}

\subsection{Tokenization of \name}
\label{app:tokenization}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\columnwidth]{figures/tokenization.png}
    \caption{Illustration of tokenization in \name. \texttt{"\_"} represents a whitespace, and \texttt{"<|extratoken\_X|>"} represents concatenated whitespaces of different lengths.}
    \label{fig:tokenization}
\end{figure}

Given a code snippet as in~\figurename~\ref{fig:tokenization}, it is first separated into token pieces by the tokenizer.
Then, each token is mapped to an integer according to its ID in the pre-defined dictionary.
For example, 4 or 8 whitespaces (one or two indents in Python) are concatenated to \texttt{<|extratoken\_12|>} or \texttt{<|extratoken\_16|>}, respectively.
Note that in Figure~\figurename~\ref{fig:tokenization}, tokens are starting with \texttt{"\_"}, which represents whitespace and is often used to indicate if the token appears in the middle of a sentence. 
After tokenization, any code snippet or text description can be transformed into a vector of integers.

\subsubsection{Details of Budget Allocation Strategies}

We compare three strategies: \textbf{Best Single}, choose a single language with the best performance; \textbf{Uniform}, allocate the budget uniformly; \textbf{Weighted}, allocate the budget to multiple languages based their proportions in the training corpus. 
Detailed weights can be found in~\tablename~\ref{tab:app_alloc}.
The allocation of CodeGen-Multi-16B and InCoder-6.7B are extracted from the training corpora description in the original papers. 
The allocation of GPT-J-6B/GPT-NeoX-20B are from the number of tokens in the GitHub section of the Pile. 

\begin{table}[hbp]
\centering
\caption{Detailed assignment of budget allocation strategies. Given budget $k=100$, \textbf{Weighted} distribute the budgets according to the proportions of language in the training corpus of each model.}
\resizebox{.7\columnwidth}{!}{
\begin{tabular}{c|c|ccccc} 
\toprule
\textbf{Strategy}                           & \textbf{Model} & \textbf{Python} & \textbf{C++} & \textbf{Java} & \textbf{JavaScript} & \textbf{Go}  \\
\midrule
\textbf{Uniform}                   & All            & 20              & 20           & 20            & 20                  & 20           \\
\midrule
\multirow{5}{*}{\textbf{Weighted}} & GPT-J-6B          & 17              & 36           & 11            & 22                  & 14           \\
                                   & GPT-NeoX-20B       & 17              & 36           & 11            & 22                  & 14           \\
                                   & InCoder-6.7B        & 45              & 12           & 5             & 34                  & 4            \\
                                   & CodeGen-Multi-6B/16B        & 17              & 38           & 29            & 8                   & 8            \\
                                   & CodeGeeX-13B (ours)      & 32              & 33           & 20            & 9                   & 6            \\
\bottomrule
\end{tabular}}
\label{tab:app_alloc}
\end{table}

\subsection{Evaluation on \bench (Additional)}
\label{app:hx-additional}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.42\textwidth}
    \includegraphics[width=\textwidth]{figures/training_loss.png}
    \captionof{figure}{Training loss of \name.}
    \label{fig:training-loss}
    \end{minipage}
    \begin{minipage}{0.52\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/pass_rate_vs_iteration.png}
    \captionof{figure}{\bench pass rate vs. iteration. } 
    \label{fig:hx-iter}
    \end{minipage}
\end{figure}

\textbf{Pass rate vs. number of training iterations.} We show in~\figurename~\ref{fig:training-loss} that the cross entropy loss decreases steadily during training while the pass rate in \bench continues to improve for different languages in~\figurename~\ref{fig:hx-iter}.

\textbf{Pass rate distribution vs. Languages for other code generation models.} We show in~\figurename~\ref{fig:hx-distribution-other} that other code generation models also have various pass rate distribution for different languages.

\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pass_rate_vs_language_t02_incoder.png}
    \end{minipage}\begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pass_rate_vs_language_t08_incoder.png}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pass_rate_vs_language_t02_codegen6b.png}
    \end{minipage}\begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pass_rate_vs_language_t08_codegen6b.png}
    \end{minipage}
    \begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pass_rate_vs_language_t02_codegen16b.png}
    \end{minipage}\begin{minipage}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/pass_rate_vs_language_t08_codegen16b.png}
    \end{minipage}
    \caption{In \bench, each problem's pass rate varies when generating in different programming languages. \textbf{Left:} $t=0.2, p=0.95$; \textbf{Right:} $t=0.8, p=0.95$. \textbf{From top to bottom}: InCoder-6.7B, CodeGen-Multi-6B, CodeGen-Multi-16B.}
    \label{fig:hx-distribution-other}
\end{figure}

\subsection{Evaluation on Other Benchmarks}
\subsubsection{Evaluation on HumanEval}

The evaluation setting on HumanEval is the same as \bench.
We show that among multilingual code generation models, \name achieves the second highest performance on HumanEval, reaching 60\% in pass@100 (surpassed by PaLMCoder-540B).
We also notice that monolingual models outperforms multilingual ones with a large margin, indicating that multilingual models require a larger model capacity to master different languages.

\begin{table}[hp]
    \centering
    \caption{The results of \name on HumanEval benchmark. The metric is pass@$k$ introduced in \cite{chen2021codex} (* use the biased pass@$k$ from \cite{chowdhery2022Palm}). Nucleus sampling is used with top-p=0.95 and sampling temperature being 0.2/0.6/0.8 for @1/@10/@100 respectively.}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
    \textbf{Model} & \textbf{Size} & \textbf{Type} & \textbf{Available} & \textbf{pass@1} & \textbf{pass@10} & \textbf{pass@100} \\
    \midrule
    \textbf{CodeParrot}~\citep{tunstall2022codeparrot} & 1.5B & Multi & Yes & 4.00\% & 8.70\% & 17.90\% \\
    \textbf{PolyCoder}~\citep{xu2022polycoder} & 2.7B & Multi & Yes & 5.60\% & 9.80\% & 17.70\% \\
    \textbf{GPT-J}~\citep{wang2021gptj} & 6B & Multi & Yes & 11.60\% & 15.70\% & 27.70\% \\
    \textbf{CodeGen-Multi}~\citep{nijkamp2022codegen} & 6.1B & Multi & Yes & 18.16\% & 27.81\% & 44.85\% \\
    \textbf{InCoder}~\citep{fried2022incoder} & 6.7B & Multi & Yes & 15.20\% & 27.80\% & 47.00\% \\
    \textbf{GPT-NeoX}~\citep{black2022gptneox} & 20B & Multi & Yes & 15.40\% & 25.60\% & 41.20\% \\
    \textbf{LaMDA}~\citep{thoppilan2022lamda} & 137B & Multi & No & 14.00\%* & - & 47.30\%* \\
    \textbf{CodeGen-Multi}~\citep{nijkamp2022codegen} & 16.1B & Multi & Yes & 19.22\% & 34.64\% & 55.17\% \\
    \textbf{PaLM-Coder}~\citep{chowdhery2022Palm} & 540B & Multi & No & \textbf{36.00\%}* & - & \textbf{88.40\%}* \\
    \midrule
    \textbf{Codex}~\citep{chen2021codex} & 12B & Mono & No & 28.81\% & 46.81\% & 72.31\% \\
    \textbf{CodeGen-Mono}~\citep{nijkamp2022codegen} & 16.1B & Mono & Yes & 29.28\% & \textbf{49.86\%} & 75.00\% \\
    \midrule
    \textbf{\name (ours)} & 13B & Multi & Yes & 22.89\% & 39.57\% & 60.92\% \\
    \bottomrule
    \end{tabular}}
    \label{tab:humaneval}
\end{table}

\subsubsection{Evaluation on MBPP}

MBPP dataset is proposed by~\cite{austin2021program}, containing 974 problems in Python. 
Due to specific input-output format, MBPP need to be evaluated under a few-shot setting.
We follow the splitting in the original paper and use problems 11-510 for testing.
Under 1-shot setting, we use problem 2 in prompts.
Under 3-shot setting, we use problem 2,3,4 in prompts.
The metric is pass@$k$, $k\in\{1, 10, 80\}$.
For pass@1, the temperature is 0.2 and top-p is 0.95; for pass@10 and pass@ 80, the temperature is 0.8 and top-p is 0.95.
For baselines, we consider LaMDA-137B, PaLM-540B, Code-davinci-002 (online API version of OpenAI Codex), PaLMCoder-540B and InCoder-6.7B.

The results indicate that the model capacity is essential for multilingual code generation model.
With significantly more parameters, PaLM and Codex outperform \name with a large margin.
Meanwhile, we find that more shot in the prompts harm the performance of \name, the same phenomenon have also been discovered in InCoder~\citep{fried2022incoder}.
We assume that it is because smaller models do not have enough reasoning ability to benefit from the few-shot setting.

\begin{table}
\centering
\caption{The results of \name on MBPP dataset~\citep{austin2021program}. }
\resizebox{.9\columnwidth}{!}{
\begin{tabular}{clcccc}
\toprule
\textbf{Method} & \multicolumn{1}{c}{\textbf{Model}} & \textbf{Pass@1} & \textbf{Pass@10} & \textbf{Pass@80} \\
\midrule
\multirow{5}{*}{3-shot} & \textbf{LaMDA-137B}~\citep{austin2021program} & 14.80 & - & 62.40 \\
 & \textbf{PaLM-540B}~\citep{chowdhery2022Palm} & 36.80 & - & 75.00 \\
 & \textbf{Code-davinci-002}~\citep{chen2021codex} & \textbf{50.40} & - & \textbf{84.40} \\
 & \textbf{PaLMCoder-540B}~\citep{chowdhery2022Palm} & 47.00 & - & 80.80 \\
 & \textbf{CodeGeeX-13B (ours)} & 22.44 & 43.24 & 63.52 \\
\midrule
\multirow{2}{*}{1-shot} & \textbf{InCoder-6.7B}~\citep{fried2022incoder} & 19.40 & - & - \\
 & \textbf{CodeGeeX-13B (ours)} & 24.37 & 47.95 & 68.50 \\
\bottomrule
\end{tabular}
}
\label{tab:app_mbpp}
\end{table}

\subsubsection{Evaluation on CodeXGLUE}
CodeXGLUE is a benchmark proposed by \cite{lu2021codexglue}, containing multiple datasets to support evaluation on multiple tasks, using similarity-based metrics like CodeBLEU, BLEU and accuracy for generation tasks.
We test the performance of CodeGeeX on the \textbf{code summarization} task of CodeXGLUE. 
We first fine-tune the parameters of CodeGeeX on the given training set, mixing the training data in all languages to get one fine-tuned model. Then, we test the performance of the fine-tuned model on each language, using BLEU score for evaluation because the models generate natural language in summarization tasks.

For all languages, we set temperature to 0.2 and top-p to 0.95, and generate one summarization for each sample in the test set. 
We report the results in \tableautorefname~\ref{tab:cxg-summary}. 
CodeGeeX obtains an average BLEU score of 20.63, besting all baseline models. It is worth noting that CodeGeeX is not pre-trained on Ruby, and after removing the results on Ruby for all models, CodeGeeX outperforms the best baseline model (DistillCodeT5 from \cite{wang2021codet5}) by 1.88 in the average BLEU score.

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \caption{The results of \name on \textbf{code summarization} in CodeXGLUE benchmark~\citep{lu2021codexglue}. Six languages are considered, Ruby, JavaScript, Go, Python, Java, PHP. The metric is the BLEU score. * We don't have Ruby in the pretraining corpus.}
    \label{tab:cxg-summary}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccccc} 
    \toprule
    \textbf{Model} & \textbf{All} & \textbf{Ruby} & \textbf{JavaScript} & \textbf{Go} & \textbf{Python} & \textbf{Java} & \textbf{PHP} \\
    \midrule
    \textbf{CodeBERT}~\citep{feng2020codebert} & 17.83 & 12.16 & 14.90 & 18.07 & 19.06 & 17.65 & 25.16 \\
    \textbf{PLBART}~\citep{ahmad2021plbart} & 18.32 & 14.11 & 15.56 & 18.91 & 19.30 & 18.45 & 23.58 \\
    \textbf{ProphetNet-X}~\citep{qi2021prophetnet} & 18.54 & 14.37 & \textbf{16.60} & 18.43 & 17.87 & 19.39 & 24.57 \\
    \textbf{CoTexT}~\citep{phan2021cotext} & 18.55 & 14.02 & 14.96 & 18.86 & 19.73 & 19.06 & 24.68 \\
    \textbf{PolyglotCodeBERT}~\citep{feng2020codebert} & 19.06 & 14.75 & 15.80 & 18.77 & 18.71 & 20.11 & 26.23 \\
    \textbf{DistillCodeT5}~\citep{wang2021codet5} & 20.01 & \textbf{15.75} & 16.42 & 20.21 & 20.59 & \textbf{20.51} & 26.58 \\
    \textbf{\name (ours)} & \textbf{20.63} & 10.05* & 16.01 & \textbf{24.62} & \textbf{22.50} & 19.60 & \textbf{31.00} \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\subsubsection{Evaluation on XLCoST}
XLCoST is a benchmark proposed by \cite{zhu2022xlcost}, containing parallel multilingual code data, with code snippets aligned among different languages.
For generation tasks, XLCoST uses CodeBLEU, BLEU for evaluation.
We choose the \textbf{code translation} task of XLCoST for \name evaluation. 
We first fine-tune the parameters of CodeGeeX on the given training set, combining the training data in all 42 languages pairs to obtain one fine-tuned model. Then, we test the performance of the fine-tuned model on each language pair with CodeBLEU score.

For all language pairs, we set temperature to 0.2 and top-p to 0.95, and generate one translation for each sample in the test set. 
We report the results in \tableautorefname~\ref{tab:xlc-trans}. 
CodeGeeX performs better than all baseline models on all language pairs except for: PHP to Python on program level, C++ to Python on snippet level, and PHP to Python on snippet level. On average, CodeGeeX outperforms the baseline by 4.10 on program level and by 1.99 on snippet level.

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \caption{The results of \name on \textbf{code translation} in XLCoST benchmark. Six languages are considered, C++, Java, Python, C\#, JavaScript, PHP, C. The metric is CodeBLEU \citep{ren2020codebleu}. The results of baselines are adopted from the original paper~\citep{zhu2022xlcost}.}
    \label{tab:xlc-trans}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccccccc|ccccccc} 
    \toprule
     &  & \multicolumn{7}{c}{\textbf{Snippet-level}} & \multicolumn{7}{c}{\textbf{Program-level}} \\
     & \textbf{Model} & \textbf{C++} & \textbf{Java} & \textbf{Py} & \textbf{C\#} & \textbf{JS} & \textbf{PHP} & \textbf{C} & \textbf{C++} & \textbf{Java} & \textbf{Py} & \textbf{C\#} & \textbf{JS} & \textbf{PHP} & \textbf{C} \\
     \midrule
    \multirow{4}{*}{\textbf{C++}} & CodeBERT & – & 84.94 & 74.55 & 84.99 & 82.79 & 68.56 & 45.46 & – & 74.73 & 24.96 & 76.35 & 72.95 & 50.40 & 21.84 \\
     & PLBART & – & 83.85 & 74.89 & 84.57 & 83.19 & 68.62 & 83.95 & – & 75.26 & 70.13 & 78.01 & 61.85 & 67.01 & 72.59 \\
     & CodeT5 & – & 86.35 & \textbf{76.28} & 85.85 & 84.31 & 69.87 & 90.45 & – & 80.03 & 71.56 & 81.73 & 79.48 & 70.44 & 85.67 \\
     & \textbf{\name} & – & \textbf{86.99} & 74.73 & \textbf{86.63} & \textbf{84.83} & \textbf{70.30} & \textbf{94.04} & – & \textbf{84.40} & \textbf{73.89} & \textbf{84.49} & \textbf{82.20} & \textbf{71.18} & \textbf{87.32} \\
     \midrule
    \multirow{4}{*}{\textbf{Java}} & CodeBERT & 87.27 & – & 58.39 & 92.26 & 84.63 & 67.26 & 39.94 & 79.36 & – & 8.51 & 84.43 & 76.02 & 51.42 & 21.22 \\
     & PLBART & 87.31 & – & 58.30 & 90.78 & 85.42 & 67.44 & 72.47 & 81.41 & – & 66.29 & 83.34 & 80.14 & 67.12 & 63.37 \\
     & CodeT5 & 88.26 & – & 74.59 & 92.56 & 86.22 & 69.02 & 82.78 & 84.26 & – & 69.57 & 87.79 & 80.67 & 69.44 & 78.78 \\
     & \textbf{\name} & \textbf{89.08} & – & \textbf{74.65} & \textbf{92.94} & \textbf{86.96} & \textbf{69.77} & \textbf{88.44} & \textbf{87.07} & – & \textbf{73.11} & \textbf{91.78} & \textbf{84.34} & \textbf{70.61} & \textbf{81.07} \\
     \midrule
    \multirow{4}{*}{\textbf{Py}} & CodeBERT & 80.46 & 58.50 & – & 54.72 & 57.38 & 65.14 & 10.70 & 68.87 & 28.22 & – & 17.80 & 23.65 & 49.30 & 18.32 \\
     & PLBART & 80.15 & 74.15 & – & 73.50 & 73.20 & 66.12 & 62.15 & 74.38 & 67.80 & – & 66.03 & 69.30 & 64.85 & 29.05 \\
     & CodeT5 & 81.56 & 78.61 & – & 78.89 & 77.76 & 67.54 & 68.67 & 78.85 & 73.15 & – & 73.35 & 71.80 & 67.50 & 56.35 \\
     & \textbf{\name} & \textbf{82.91} & \textbf{81.93} & – & \textbf{81.30} & \textbf{79.83} & \textbf{67.99} & \textbf{82.59} & \textbf{82.49} & \textbf{79.03} & – & \textbf{80.01} & \textbf{77.47} & \textbf{68.91} & \textbf{71.67} \\
     \midrule
    \multirow{4}{*}{\textbf{C\#}} & CodeBERT & 86.96 & 90.15 & 56.92 & – & 84.38 & 67.18 & 40.43 & 78.52 & 82.25 & 10.82 & – & 75.46 & 51.76 & 21.63 \\
     & PLBART & 84.98 & 6.27 & 69.82 & – & 85.02 & 67.30 & 75.74 & 80.17 & 81.37 & 67.02 & – & 79.81 & 67.12 & 57.60 \\
     & CodeT5 & 88.06 & 91.69 & 73.85 & – & 85.95 & 68.97 & 81.09 & 83.59 & 85.70 & 69.52 & – & 80.50 & 69.63 & 77.35 \\
     & \textbf{\name} & \textbf{88.70} & \textbf{93.03} & \textbf{74.55} & – & \textbf{86.44} & \textbf{69.49} & \textbf{86.69} & \textbf{87.11} & \textbf{90.46} & \textbf{72.89} & – & \textbf{83.83} & \textbf{70.58} & \textbf{80.73} \\
     \midrule
    \multirow{4}{*}{\textbf{JS}} & CodeBERT & 84.38 & 84.42 & 52.57 & 84.74 & – & 66.66 & 33.29 & 75.43 & 72.33 & 9.19 & 75.47 & – & 52.08 & 19.79 \\
     & PLBART & 84.45 & 84.90 & 69.29 & 85.05 & – & 67.09 & 72.65 & 80.19 & 76.96 & 64.18 & 78.51 & – & 67.24 & 67.70 \\
     & CodeT5 & 85.06 & 85.48 & 73.15 & 85.96 & – & 68.42 & 80.49 & 82.14 & 79.91 & 68.42 & 81.77 & – & 68.76 & 74.57 \\
     & \textbf{\name} & \textbf{86.72} & \textbf{86.96} & \textbf{73.25} & \textbf{86.41} & – & \textbf{69.00} & \textbf{83.85} & \textbf{85.84} & \textbf{83.85} & \textbf{72.11} & \textbf{85.35} & – & \textbf{69.80} & \textbf{79.41} \\
     \midrule
    \multirow{4}{*}{\textbf{PHP}} & CodeBERT & 82.58 & 81.57 & 69.29 & 80.96 & 79.94 & – & 28.45 & 50.13 & 46.81 & 16.92 & 49.75 & 48.12 & – & 22.19 \\
     & PLBART & 83.87 & 81.66 & 71.17 & 78.00 & 82.94 & – & 57.39 & 79.40 & 72.77 & 61.26 & 74.16 & 44.26 & – & 56.23 \\
     & CodeT5 & 86.33 & 85.12 & \textbf{73.22} & 84.56 & 83.56 & – & 79.30 & 85.55 & 82.09 & \textbf{72.26} & 83.79 & 81.72 & – & 65.86 \\
     & \textbf{\name} & \textbf{86.75} & \textbf{86.24} & 71.37 & \textbf{85.58} & \textbf{84.17} & – & \textbf{83.89} & \textbf{87.23} & \textbf{83.90} & 71.02 & \textbf{85.34} & \textbf{82.81} & – & \textbf{78.76} \\
     \midrule
    \multirow{4}{*}{\textbf{C}} & CodeBERT & 45.84 & 39.69 & 13.55 & 39.71 & 29.85 & 38.88 & – & 21.70 & 21.27 & 21.10 & 19.50 & 15.64 & 31.71 & – \\
     & PLBART & 82.53 & 72.35 & 49.16 & 75.78 & 75.05 & 60.86 & – & 78.42 & 13.45 & 5.53 & 45.15 & 31.47 & 25.17 & – \\
     & CodeT5 & 90.26 & 81.81 & 63.81 & 83.05 & 79.73 & 66.32 & – & 88.17 & 76.12 & 56.32 & 80.20 & 76.50 & 64.28 & – \\
     & \textbf{\name} & \textbf{91.30} & \textbf{85.58} & \textbf{71.52} & \textbf{87.52} & \textbf{84.91} & \textbf{68.52} & – & \textbf{88.21} & \textbf{82.46} & \textbf{69.78} & \textbf{85.56} & \textbf{81.21} & \textbf{68.80} & – \\
    \bottomrule
    \end{tabular}}
\end{table}

\clearpage

\subsection{Examples of \name Generation}
\label{app:example}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/examples.png}
    \caption{Solutions (Problem 0 in \bench) generated by \name. Prompt and generated codes are separated by the 'Generation' line (added after the generation as an indicator).}
    \label{fig:hx_example}
    \vspace{-5mm}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/example_trans_id_95.png}
    \caption{Solutions (Problem 95 in \bench) translated by \name. Prompt and generated codes are separated by the 'Translation' line (added after the generation as an indicator).}
    \label{fig:hx_example_trans_id_95}
    \vspace{-5mm}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/example_id_109_1.png}
    \end{minipage}
    \begin{minipage}[t]{\textwidth}
    \includegraphics[width=\textwidth]{figures/example_id_109_2.png}
    \end{minipage}
    \caption{Solutions (Problem 109 in \bench) generated by \name. Prompt and generated codes are separated by the 'Generation' line (added after the generation as an indicator).}
    \label{fig:hx_example_id_109}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/example_id_13.png}
    \caption{Solutions (Problem 13 in \bench) generated by \name. Prompt and generated codes are separated by the 'Generation' line (added after the generation as an indicator).}
    \label{fig:hx_example_id_13}
    \vspace{-5mm}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/example_id_142.png}
    \caption{Solutions (Problem 142 in \bench) generated by \name. Prompt and generated codes are separated by the 'Generation' line (added after the generation as an indicator).}
    \label{fig:hx_example_id_142}
    \vspace{-5mm}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/example_trans_id_33.png}
    \caption{Solutions (Problem 33 in \bench) translated by \name. Prompt and generated codes are separated by the 'Translation' line (added after the generation as an indicator).}
    \label{fig:hx_example_trans_id_33}
    \vspace{-5mm}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/example_other_languages.png}
    \caption{Examples of \name generation with prompts in Chinese, French, Russia and Japanese. Prompt and generated codes are separated by multiple '\#'s (added after the generation as an indicator).}
    \label{fig:example_other_languages}
    \vspace{-5mm}
\end{figure}

%
