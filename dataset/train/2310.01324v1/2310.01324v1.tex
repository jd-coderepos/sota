\newpage
In this appendix, we provide more details of ZeroI2V from the following aspects:
\begin{itemize}

    \item Implementation details of our method are in \S~\ref{sec:appendix_a}.
    \item Experimental results with other pre-trained weights and backbone architectures, as well as training cost analysis, can be found in \S~\ref{sec:appendix_b}.
    \item Visualization of our proposed Spatial-Temporal Dual-Headed Attention (STDHA) is in \S~\ref{sec:vis}.
    \item Limitations and societal impact are in \S~\ref{sec:impact}
    \item License of the datasets and pre-trained models are in \S~\ref{sec:license}
\end{itemize}

\section{Implementation details of our method}
\label{sec:appendix_a}

\subsection{Model Details}
\begin{table}[ht]
\centering
\caption{\textbf{Model details.}  We use a multiset to represent the time offsets of different heads (\eg, "" means that there are 2 heads with . When , it represents a spatial head."Temporal RF" refers to temporal receptive field of a single STDHA. "Num. Adapters" refers to the number of linear adapters per ViT block.} 
\subfloat[Model details for Kinetics400.]{\tablestyle{4.8pt}{1.1}
\label{tab:models_details_k400}
\begin{tabular}{@{}x{50}x{30}y{160}x{30}x{30}@{}}
    \toprule
    Backbone & Frames &  of heads & Temporal RF &  Num. Adapters \\
    \midrule
    \multirow{3}{10mm}{ViT-B (=12)} & 8 &
     & 3    &  4    \\
  & 16 & & 4 & 4    \\
  & 32 &  & 6 & 4   \\
  \midrule
\multirow{3}{10 mm}{ViT-L (=16)}              &  8 &
     & 3  & 4   \\
  & 16 &  & 4  & 4    \\
  & 32 &  & 6  &  4   \\
  \bottomrule
  \end{tabular}}\hfill
\subfloat[Model details for Something-Something v2.]{\tablestyle{4.8pt}{1.1}
    \label{tab:models_details_sthv2}
       \begin{tabular}{@{}x{50}x{30}y{160}x{30}x{30}@{}}
    \toprule
    Backbone & Frames &  of heads & Temporal RF &  Num. Adapters \\
    \midrule
    \multirow{3}{10mm}{ViT-B (=12)} & 8 &
     & 3    &  6   \\
  & 16 & & 5 & 4    \\
  & 32 &  & 6 & 4   \\
  \midrule
\multirow{3}{10 mm}{ViT-L (=16)}              &  8 &
     & 3  & 4   \\
  & 16 &  & 5  & 4    \\
  & 32 &  & 6  &  4   \\
\midrule
\multirow{4}{12 mm}{Swin-B ( = 4, 8, 16 ,32)} & \multirow{4}{3 mm}{32} &  Stage 1:  & 2 & \multirow{4}{1.5 mm}{4}\\ 
   & & Stage 2:  & 3 & \\
   & & Stage 3:  & 5 & \\
   & & Stage 4:  &  5 &     \\
  \bottomrule
  \end{tabular}}
      \hfill
\end{table}

%
 
Our model details are shown in the Table \ref{tab:models_details_k400} and Table \ref{tab:models_details_sthv2}. Due to different requirements for spatial modeling and temporal modeling in different datasets, there are slight differences in the specific implementation settings.

\paragraph{Settings of STDHA}
For the settings of STDHA, we allocate 1/6 to 1/4 of the heads for temporal modeling based on the conclusions obtained from previous ablation experiments. For long inputs, we increase the absolute value of  to obtain a larger temporal receptive field. When using Swin-B as the backbone, due to its four stages and different numbers of heads in each stage, we simply allocate temporal heads to each stage at a ratio of 1/4. Since its input length is halved after patch embedding (from 32 frames to 16 frames), we set the value of  according to the best temporal receptive field of 16 frames, which is 5. Please note that we have not tried other configurations due to time constraints, so there may be better configurations.

\paragraph{Number of adapters}
Considering the balance between performance and training cost, we only assign different adapters for each weight for the minimum setting (ViT-B with 8-frame input) on the SSv2 dataset, which requires 6 adapters. For all other settings, we only use 4 adapters. And the bottleneck ratios of all adapters are set to 0.25.

\subsection{Training Details}


\begin{table}[h]
    \centering
    \caption{{\bf Training details of our method.}}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        dataset  & K400 & SSv2 \\
        \midrule
    \multicolumn{3}{@{}l}{\it Optimization settings} \\
        optimizer & \multicolumn{2}{c}{AdamW, learning rate=3e-4, weight decay=5e-2} \\
        batch size & \multicolumn{2}{c}{64} \\
        training epochs & 40   &  50 \\
         \midrule
         \multicolumn{3}{@{}l}{\it Sampling settings} \\
        crop size & \multicolumn{2}{c}{224} \\
       
        frame sampling rate &
  
        \begin{tabular}{c}16 (for 8-frame input)\\8 (for 16-frame input)\\4 (for 32-frame input)\end{tabular}
         &  uniformly sample as TSN~\citep{tsn}   \\
        
         num. testing views & 3 temporal  1 spatial & 1 temporal  3 spatial \\
        \midrule
         \multicolumn{3}{@{}l}{\it Data augmentation settings} \\
     RandAugment \citep{randaugment} & \multicolumn{2}{c}{m=7, n=4}  \\
        flip & \multicolumn{2}{c}{0.5} \\
        
        Random erasing \citep{rande} & - & 0.25 \\
        label smoothing & - & 0.1 \\

       
        \bottomrule
    \end{tabular}
    }
    \label{tab:train_details}
\end{table} 
As shown in the Table \ref{tab:train_details}, our training strategy is similar to the previous methods~\citep{stadapter, yangaim}. Considering that SSv2 requires stronger temporal modeling ability, we used a stronger data augmentation strategy following \cite{videoswin}. In addition, for the full finetuing experiment using Swin-B as the backbone (Swin-B with STDHA), we use exactly the same training strategy as video swin transformer~\citep{videoswin}.

\paragraph{Implementation details of adaptation strategies} The training configurations used for all the adaptation strategies are summarized as follows:
\begin{itemize}
    \item For the comparison experiment of full finetuning ViT with CLIP pretrained, we use 1/10 of the learning rate to avoid training collapse.
    \item For the comparison experiment of only tuning temporal head, we froze the parameters related to the spatial head (only training the part of the parameters related to the temporal head, in other words, we only trained , where  is the number of channels of the temporal head).
\end{itemize}

For any settings not explicitly mentioned, we assume they align with the training settings of the Linear Adapter.

\section{Additional experimental results}
\label{sec:appendix_b}


\begin{wraptable}[9]{r}{5cm}
\vspace{-0.5cm}
        \centering
        \caption{\textbf{Effect of bottleneck ratio of linear adapters.}}\label{tab:bottleneck}
        \begin{tabular}{@{}x{30}x{40}x{30}@{}}
            \toprule
           Ratio & Tunable Param(M) & Top-1 \\
           \midrule
           0.0625 & 3 & 64.2 \\
           0.125 & 7 & 65.0 \\
           0.25 & 14 & \textbf{66.0} \\
           0.5 & 28 & 65.8 \\
            \bottomrule
        \end{tabular}
\end{wraptable}





 \paragraph{Effect of bottleneck ratio} As shown in Table \ref{tab:bottleneck}, a bottleneck ratio of 0.25 achieves a good trade-off between performance and the number of parameters. Therefore, we choose 0.25 as the bottleneck ratio for all subsequent experiments. 

\begin{table}[h]
    \centering
    \caption{\textbf{Comparing the state-of-the-art video recognition methods on UCF101 and HMDB51.} We following ST-Adapter [1] test our method and report the 3-split mean Top-1 accuracy for both datasets.}
    \label{tab:ucfhmdb}
    \begin{tabular}{lccc}
        \toprule
        Method & Pretrain & UCF101 & HMDB51 \\
        \midrule
STC~\citep{stc} & K400 & 95.8 & 72.6 \\
ECO~\citep{eco} & K400 & 93.6 & 68.4 \\

        I3D~\citep{i3dk400} & ImageNet+K400 & 95.6 & 74.8 \\
        S3D~\citep{s3d} & ImageNet+K400 & 96.8 & 75.9 \\
        SlowOnly-8x8-R101~\citep{slowfast} & Kinetics+OmniSource & 97.3 & 79.0 \\
        VideoPrompt~\citep{videoprompt} & CLIP & 93.6 & 66.4 \\
\midrule
        ST-Adapter ViT-B/16~\citep{stadapter} & CLIP+K400 & 96.4 & 77.7 \\
        ST-Adapter ViT-L/14~\citep{stadapter} & CLIP+K400 & 98.1 & 81.7 \\
        \midrule
        ZeroI2V ViT-B/16 (Ours) & CLIP & 95.6 & 73.7 \\
        ZeroI2V ViT-B/16 (Ours) & CLIP+K400 & 97.7 & 78.5 \\
        ZeroI2V ViT-L/14  (Ours) & CLIP & 97.8 & 79.9 \\
        ZeroI2V ViT-L/14  (Ours) & CLIP+K400 & \textbf{98.6} & \textbf{83.4} \\
        \bottomrule
    \end{tabular}
    \label{tab:ucf_hmdb}
\end{table}

\begin{table}[ht] \centering
    \caption{\textbf{Results on K400 and SSv2 validation set with ImageNet21K pretrained.} Views = \#frames  \#spatial crops  \#temporal clips. “GFLOPs” means  FLOPs, "M" means . “Extra GLOPs” refers to the extra computation added to the original ViT under the same number of views. "New Params" refers to additional parameters 
 during inference besides the parameters of the original ViT backbone and linear classifier. Views for all methods are 813 for K400 and 831 for SSv2}
\tablestyle{4.8pt}{1.1}
    
\begin{tabular}{@{}y{130}x{30}x{30}x{30}x{30}x{30}x{30}x{25}x{25}@{}}
\toprule
Methods & Pretrain & GFLOPs & Extra GFLOPs  & Param(M) & New Param(M) & K400 Top-1 & SSv2 Top-1 \\
\midrule
\multicolumn{7}{@{}l}{\it Methods with full fine-tuning} \\
TimeSformer~\citep{timesformer} & IN21K &  590 & - & 121 & - & 78.0  & 59.5 \\
X-ViT~\citep{xvit}  & IN21K &  425 & - & 92 & - & 78.5  & 64.4 \\
\midrule
\multicolumn{7}{@{}l}{\it Methods with PETL \& CLIP ViT-B/16 } \\
EVL~\citep{evl} & IN21K &  454 & 32 & 115 & 29 & 75.4  & - \\
 ST-Adapter~\citep{stadapter} & IN21K &  455  & 33 & 93 & 7 & 76.6  & 62.8 \\
AIM~\citep{yangaim} & IN21K &  624 & 202 & 100 & 14 &  \textbf{78.8} & 62.0 \\

\textbf{ZeroI2V} & IN21K &  422 & 0 & 86 & 0   & 78.6 & \textbf{65.3} \\ \bottomrule
\end{tabular}


\label{tab:sota_in21k}
\end{table} 

\paragraph{Experiments with ImageNet21K pre-trained weights}
In order to investigate the adaptability of our method to different pre-trained weights, we conducted experiments using the same model and training settings on ImageNet21K pre-trained weights. The results are shown in Table \ref{tab:sota_in21k}. It can be seen that our method is still very effective under ImageNet21K weights and can surpass previous full fine-tuning methods. Compared to other PETL methods, our method shows stronger robustness. As shown in Figure \ref{fig:fig1}, when using ImageNet21K pre-trained weights, the advantage of our method over other PETL methods is even greater than when using CLIP pre-trained weights. For example, when using CLIP weights, our method slightly surpasses ST-Adapter~\citep{stadapter} (67.7 vs 67.1), while when using ImageNet21K weights, we have a clear advantage (65.3 vs 62.8).




\begin{table}[ht] \centering
    \caption{\textbf{Results on SSv2 validation set with Swin-B backbone.} K400\dag indicates that the model is pre-trained on both IN21K and K400. The other notations are the same as Table \ref{tab:sota_in21k}}.
\tablestyle{4.8pt}{1.1}
    
\begin{tabular}{@{}y{120}x{30}x{30}x{30}x{30}x{30}x{28}x{28}@{}}
\toprule
Methods & Pretrain  & Views & GFLOPs  & Param(M) & Tunable Param(M)  & Top-1 & Top-5 \\
\toprule

VideoSwin-B~\citep{videoswin} & K400\dag & 3231 & 963 & 89 & 89 &69.6  & \textbf{92.7}      \\
PST-B~\citep{tps} & IN21K & 3231 & 741 & 89 & 89 & 67.4 & 90.9 \\
SIFAR-B~\citep{tps} & IN21K & 3231 & 789 & 87 & 87 & 62.6 & 88.5 \\
Swin-B w/ \textbf{STDHA}  & IN21K & 3231 & 741  & 89 & 89 & \textbf{70.0}  & 92.1 \\
\textbf{ZeroI2V} Swin-B  & IN21K & 3231  & 741  & 89 &  14  & 67.8 & 91.4 \\ 
\bottomrule
\end{tabular}


\label{tab:sota_swin}
\end{table}

 \paragraph{Experiments with other backbone}
In order to verify the universality of our method, we conducted experiments using the Swin Transformer~\citep{swin} in Table \ref{tab:sota_swin}, which has a hierarchical structure and local window attention. As shown in Table 1, although our method was not specifically designed and adjusted for it, it can still achieve performance comparable or even better than other full fine-tuning methods. To our surprise, when we used a full fine-tuning strategy to train Swin-B using STDHA, we achieved a top-1 accuracy of \textbf{70\%}, which even surpassed VideoSwin-B~\citep{videoswin} pre-trained on the K400 dataset. From this, we can see that our designed STHDA is not only versatile but also has powerful temporal modeling capabilities. In addition, for backbones like Swin Transformer that have more inductive bias and have not been trained on large-scale image-text datasets, full fine-tuning may be able to better utilize the temporal modeling capabilities of STDHA.



\paragraph{Comparison of training cost}
 We compared the training cost of our method with previous methods in Table \ref{tab:train_cost}. It can be seen that compared to previous full fine-tuning methods such as Uniformer~\citep{uniformer} and ActionCLIP~\citep{actionclip}, our method significantly reduces training cost. However, due to the dense insertion of linear adapters during training, our training cost is slightly higher than that of previous PETL methods such as EVL~\citep{evl}. However, we believe that it is worthwhile and acceptable to exchange some training cost for a reduction in inference cost. We will also try to further reduce training cost by improving training strategies and adaptation strategies in the future.

\begin{table}[ht]
    \centering
        \tablestyle{4.8pt}{1.1}
    \caption{\textbf{Comparison of training cost}. Our results are obtained
     using a same V100-32G with PyTorch-builtin mixed precision,, following EVL~\citep{evl}.}
    \begin{tabular}{lccccc}
        \toprule
        Model (Frames)  & Pretrain  & Frames & \begin{tabular}{c}Training\\GPU Hours\end{tabular}  & \begin{tabular}{c}Tunable \\Param (M)\end{tabular}    & \begin{tabular}{c}K400\\Top-1\end{tabular} \\
        \midrule
Uniformer-B~\citep{uniformer}  & IN1K & 32 & 5000  V100 & 50  & 82.9 \\
        ActionCLIP ViT-B/16~\citep{actionclip}   & CLIP & 16& 480  RTX3090 & 142 & 82.6 \\
         
                
        EVL ViT-B/16\citep{evl}  & CLIP & 8 & 60  V100  & 29  & 82.9 \\
 
       \textbf{ZeroI2V ViT-B/16 (Ours)} & CLIP  & 8 & 85  V100	& \textbf{14}  & \textbf{83.0} \\
        \bottomrule
    \end{tabular}
    \vspace{-4mm}
    \label{tab:train_cost}
\end{table} 
\section{Visualization}
\label{sec:vis}

 The motivation behind the design of STDHA was to enable simultaneous spatial and temporal modeling in an independent way before information fusion (ie. decoupling spatial and temporal modeling). In order to more intuitively demonstrate the temporal modeling capabilities of our proposed STDHA, we visualized the attention map of the last layer of the network. As shown in Figure \ref{fig:vis}, note that we visualize the attention map of the last transformer layer in our figure. Due to the temporal receptive field increases with the network depth, both the spatial head and the temporal head of this last layer have a global temporal receptive field about the input frames. But we can still observe that the spatial heads pay more attention to the information of the current frame while the temporal head pays more attention to the information of other frames.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figs/vis.pdf}
    \caption{ \textbf{Visualization of attention maps of spatial heads, temporal heads and STDHA at the last layer generated by Grad-CAM~\citep{gradcam} on SSv2 validation set.}}
    \label{fig:vis}
    
\end{figure} 

\section{Limitations and Societal Impact}
\label{sec:impact}
\paragraph{Limitations}

Our method has the following two main limitations:
\begin{itemize}
    \item Although our method is very efficient during inference, the densely inserted linear adapters still need to participate in gradient calculation during training, which brings a non-negligible training cost. This makes our method still have a certain disadvantage in training cost compared to methods that use CLIP as an independent feature extractor (such as EVL~\citep{evl}). In the future, we need to consider more efficient training strategies and improve the structure of linear adapters to address this issue.
    \item Although STDHA has demonstrated powerful temporal modeling capabilities, it still requires consideration of the original number of heads in ViT and manual design of a head relocation strategy. Despite the ablation experiment results showing that our method’s performance is relatively stable across different head relocation strategies, achieving better results still necessitates some manual design. Obtaining optimal head relocation strategies through manual design is obviously challenging. In future work, we aim to investigate methods for automatically designing head relocation strategies.
\end{itemize}

\paragraph{Societal impact}
Our ZeroI2V method can apply existing image pre-trained transformers as powerful backbone networks for video tasks such as video classification, spatiotemporal action detection, and video segmentation. Although we do not provide direct applications, it still has the potential to be applied to many scenarios related to video tasks. On the positive side, a powerful video understanding backbone network can improve the performance of downstream tasks and thus enhance efficiency in various scenarios, such as in the fields of smart healthcare and intelligent transportation where video understanding is required. On the other hand, if applied improperly, advanced video networks may also have negative impacts, such as being used in terrorist military activities. Researchers need to carefully consider the potential risks and impacts when applying it to real-world scenarios.


\section{License of datasets and pre-trained models}
\label{sec:license}

All the datasets we used are commonly used datasets for academic purpose.  The license of the Kinetics-400\footnote{\url{https://www.deepmind.com/open-source/kinetics}} is CC BY-NC 4.0\footnote{\url{https://creativecommons.org/licenses/by/4.0}}. The license of the Something-Something V2\footnote{\url{https://developer.qualcomm.com/software/ai-datasets/something-something}} is custom. We used the publicly available CLIP pre-trained weights provided by OpenAI\footnote{\url{https://github.com/openai/CLIP}} and the Swin Transformer pre-trained weights provided by Microsoft\footnote{\url{https://github.com/microsoft/Swin-Transformer}}, both of which use the MIT License.

