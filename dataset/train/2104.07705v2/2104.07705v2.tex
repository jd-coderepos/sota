\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/mlm_24hr_dist_batch.pdf}
        \caption{Batch Size}
        \label{fig:bs_dist}
    \end{subfigure}
    ~~~~~~
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/mlm_24hr_dist_lr.pdf}
        \caption{Learning Rate}
        \label{fig:lr_dist}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/mlm_24hr_dist_warmup.pdf}
        \caption{Warmup Proportion}
        \label{fig:warmup_proportion_dist}
    \end{subfigure}
    ~~~~~~
    \begin{subfigure}[b]{0.2\textwidth}
        \includegraphics[width=\textwidth]{images/mlm_24hr_dist_days.pdf}
        \caption{Total Days}
        \label{fig:total_day_dist}
    \end{subfigure}
\caption{Distribution of the validation-set loss after 24 hours of training across different hyperparameters.}
    \label{fig:24_hour_sweep}
\end{figure}

\section{Hyperparameter Search}
\label{sec:hyperparameters}

Calibrating hyperparameters is key to increasing model performance in deep learning and NLP \cite{levy-etal-2015-improving, Liu2019RoBERTaAR}.
We re-tune core optimization hyperparameters to fit our low-resource setting, rather than the massive computation frameworks for which they are currently tuned.
Our hyperparameter search yields substantial improvements in MLM loss after 24 hours of training.


\subsection{Hyperparameters}
\label{subsection:hyperparams}



\paragraph{Batch Size (bsz)}
The number of examples (sequences up to 128 tokens) in each mini-batch.
We try batch sizes of 4k, 8k, and 16k examples, which are of a similar order of magnitude to the ones used by \citet{Liu2019RoBERTaAR}.
Since our hardware has limited memory, we achieve these batch sizes via gradient accumulation.
In terms of parameter updates, these batch sizes amount to approximately 23k, 12k, and 6k update steps in 24 hours, respectively.

\paragraph{Peak Learning Rate (lr)}
Our linear learning rate scheduler, which starts at 0, warms up to the peak learning rate, and then decays back to 0. We try 5e-4, 1e-3, and 2e-3.

\paragraph{Warmup Proportion (wu)}
We determine the number of warmup steps as a proportion of the total number of steps. Specifically, we try 0\%, 2\%, 4\%, and 6\%, which all reflect significantly fewer warmup steps than in BERT.


\paragraph{Total Days (days)}
The number of days it would take the learning rate scheduler to decay back to 0, as measured on our hardware. This is equivalent to setting the maximal number of steps.
Together with the warmup proportion, it determines where along the learning rate schedule the training process stops.
For a value of 1 day, the learning process will end when the learning rate decays back to 0.
We try setting the schedule according to 1, 3, and 9 days.


\subsection{Methodology}

We optimize our model using MLM loss with each hyperparameter setting.
Although there are 108 combinations in total, poor configurations are easy to identify early on.
After 3 hours, we prune configurations that did not reach a validation-set loss of 6.0 or less; this rule removes diverging runs, such as configurations with 0\% warmup.
After 12 hours, we keep the top 50\% of models with respect to the validation-set loss, and resume their runs until they reach 24 hours.


\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Configuration} &
\bf loss &
\bf bsz &
\bf lr &
\bf wu &
\bf days \\
\midrule
Search \#1 & 1.717 & 4096 & 2e-3 & 6\% & 1 \\
Search \#2 & 1.717 & 4096 & 1e-3 & 2\% & 1 \\
Search \#3 & 1.720 & 4096 & 1e-3 & 6\% & 1 \\
Search \#4 & 1.722 & 8192 & 1e-3 & 6\% & 1 \\
Search \#5 & 1.723 & 4096 & 2e-3 & 4\% & 1 \\
\midrule
\bertbase{}   & 2.050 & 256 & 1e-4 & 11.1\% & 3 \\
\bertlarge{}  & 2.318 & 256 & 1e-4 & 11.1\% & 8.3 \\
\bottomrule
\end{tabular}
\caption{Best hyperparameter configurations by MLM loss recorded after 24 hours of training.}
\label{tab:best_24hr_configs}
\end{table}
 
\begin{figure}
\centering
\includegraphics[width=\columnwidth]{images/bert_24hr_validation_reg.pdf}
\caption{The validation-set loss of 24hBERT compared to the original BERT model configurations.}
\label{fig:bert_comparison_in_24hr}
\end{figure}


\subsection{Results}

\begin{table*}[!ht]
\centering
\small
\begin{tabular}{@{}lccccccccc@{}}
\toprule
& \bf MNLI-m/mm & \bf QNLI & \bf QQP & \bf RTE & \bf SST-2 & \bf MRPC & \bf CoLA & \bf STS-B & \bf Avg. \\
\#Examples & 393k & 105k & 364k & 2.5k & 67k & 3.7k & 8.5k & 7k &  \\
\midrule 




\textbf{24hBERT}    & 84.4/83.8 & 90.6 & 70.7 & 75.3 & 93.0 & 88.5 & 57.1 & 86.8 & 81.1 \\
\bertbase           & 84.6/84.0 & 90.6 & 72.0 & 76.5 & 92.8 & 89.9 & 55.1 & 87.7 & 81.5 \\
\bertlarge          & 86.0/85.2 & 92.6 & 72.0 & 78.3 & 94.5 & 89.9 & 60.9 & 87.5 & 83.0 \\
\robertabase        & 87.0/86.5 & 92.4 & 72.5 & 79.6 & 95.8 & 89.7 & 58.8 & 88.3 & 83.4 \\

\bottomrule
\end{tabular}
\caption{Performance on GLUE test sets. Results for RTE, STS and
MRPC are reported by first finetuning on the MNLI model instead of the baseline pretrained model.
}


\label{tab:glue_tasks}
\end{table*} 
We first analyze the effect of each hyperparameter by plotting the distribution of the validation-set loss per value (Figure~\ref{fig:24_hour_sweep}).
We observe a clear preference towards synchronizing the learning rate schedule with the actual amount of training time in the budget (1 day), corroborating the results of \citet{Li2020BudgetedTR}.
We also find the smaller batch size to have an advantage over larger ones, along with moderate-high learning rates.
We suspect that the smaller batch size works better for our resource budget due to the trade-off between number of samples and number of updates, for which a batch size of 4096 seems to be a better fit.
Finally, there appears to be a preference towards longer warmup proportions; however, a closer look at those cases reveals that when the number of total days is larger (3 or 9), it is better to use a smaller warmup proportion (2\%), otherwise the warmup phase might take up a larger portion of the \textit{actual} training time.



Table~\ref{tab:best_24hr_configs} shows the best configurations by MLM
loss.
It is apparent that our calibrated models perform substantially better than models with BERTâ€™s default hyperparameters (which were tuned for 4 days on 16 TPUs). 
There is also relatively little
variance in performance among the top models.
We select the best model (Search \#1), and name it \textbf{24hBERT}.
Figure~\ref{fig:bert_comparison_in_24hr} compares 24hBERT with models using the default calibration, and shows that 24hBERT converges significantly faster. 















