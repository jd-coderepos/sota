

\section{Experimental Results}\label{sec:exp}


\subsection{Comparison with SoTA FR methods}~\label{sec:comparison}
\Paragraph{Datasets}
Following the experimental setting of~\cite{shi2021boosting}, we use \textbf{MS-Celeb-1M}~\cite{guo2016ms} as our labeled training dataset. MS-Celeb-1M is a large-scale public face dataset with web-crawled celebrity photos. For a fair comparison, we use the cleaned MSM-V (M images of K classes) from~\cite{shi2021boosting}.
For our target data, \textbf{WiderFace}~\cite{yang2016wider} is used. WiderFace is a dataset collected for face detection in challenging scenarios, with a diverse set of unconstrained variations. It is a suitable target dataset for training CFSM, as we aim to bridge the gap between the semi-constrained training faces and the faces in challenging testing scenarios. We follow~\cite{shi2021boosting} and use K face images from WiderFace.
For evaluation, we test on four \textbf{unconstrained} face recognition benchmarks: IJB-B, IJB-C, IJB-S and TinyFace. These  datasets represent real-world testing scenarios where faces are significantly different from the semi-constrained training dataset. 


 \textbf{IJB-B}~\cite{whitelam2017iarpa} contains both high-quality celebrity photos collected in the wild and low-quality photos or video frames with large variations.  
  It consists of  subjects with K still images and K frames from  videos. 
  
 \textbf{IJB-C}~\cite{maze2018iarpa} is an extension of IJB-B, which includes about  subjects with a total of  images and  unconstrained video frames.

 \textbf{IJB-S}~\cite{kalka2018ijb} is an extremely challenging benchmark where the images were collected in real-world surveillance environments. The dataset contains  subjects with an average of  videos per subject. Each subject also has  high-quality enrollment photos under different poses. We test on three protocols, Surveillance-to-Still (\textbf{V2S}), Surveillance-to-Booking (\textbf{V2B}) and Surveillance-to-Surveillance (\textbf{V2V}). The first/second notation in the protocol refers to the probe/gallery image source. `Surveillance' (V) refers to the surveillance video, `still' (S) refers to the frontal high-quality enrollment image and `Booking' (B) refers to the  high-quality enrollment images.

 \textbf{TinyFace}~\cite{cheng2018low} consists of  labelled facial identities given by  native low resolution face images, which is created to facilitate the investigation of unconstrained low-resolution face recognition.




\begin{table}[t]
\renewcommand\arraystretch{1.00}
  \caption{ Comparison with state-of-the-art methods on the IJB-B benchmark. \quad\quad\quad `*' denotes a subset of data selected by the authors.}
  \centering
  \resizebox{0.92\linewidth}{!}{
  \begin{tabular}{l |c |c || c |c |c |c |c}
    \hline
    \multirow{2}{*}{Method} & \multirow{2}{*}{ Train Data, \#labeled(+\#unlabeled)   }  & \multirow{2}{*}{Backbone}   & 
    \multicolumn{3}{c|}{Verification}   &    \multicolumn{2}{c}{Identification}    \\
    \cline{4-8}
    & & &  &  &  & Rank & Rank \\
   \hline
    VGGFace~\cite{cao2018vggface2}  & VGGFace, M & SE-ResNet- &  &  &  &  &  \\
    AFRN~\cite{kang2019attentional}  & VGGFace-*, M & ResNet-  &  &  &  &  & \\
    ArcFace~\cite{deng2019arcface}  & MSMV, M & ResNet- &  &   &   &  &  \\
    MagFace~\cite{meng2021magface}  & MSMV, M &  ResNet- &  &  &  &  &  \\
    \hline
    Shi~\emph{et al.}~\cite{shi2021boosting}  & cleaned MSMV2, M(+K) & ResNet-  &  &  &  &  & \\
    
    \hline
    \textbf{ArcFace}  & cleaned MSMV2, M & ResNet- &  &  &  &  &  \\
    \textbf{ArcFace+Ours}    & cleaned MSMV2, M(+K) & ResNet- &  &  &  &  &  \\
  \hline
  \end{tabular}
  }
  \label{tab:result_ijbb}
\end{table}




\begin{table}[t]

\renewcommand\arraystretch{1.00}
  \caption{ Comparison with state-of-the-art methods on the IJB-C benchmark.}
  \centering
  \resizebox{0.92\linewidth}{!}{
  \begin{tabular}{l |c |c || c |c |c |c |c}
    \hline
    \multirow{2}{*}{Method} & \multirow{2}{*}{ Train Data, \#labeled(+\#unlabeled)}  & \multirow{2}{*}{Backbone}   & 
    \multicolumn{3}{c|}{Verification}   &    \multicolumn{2}{c}{Identification}    \\
    \cline{4-8}
    & & &  &  &  & Rank & Rank \\
   \hline
    VGGFace~\cite{cao2018vggface2}  & VGGFace, M & SE-ResNet- & - &  &  &  &  \\
    AFRN~\cite{kang2019attentional}  & VGGFace-*, M & ResNet- & - &  &  &  &  \\
    PFE~\cite{shi2019probabilistic}  & MSM-, M & ResNet- & - &  &  &  &  \\
    DUL~\cite{chang2020data}  & MSM-, M & ResNet- & - &  &  &  &  \\
    ArcFace~\cite{deng2019arcface}  & MSMV, M & ResNet- &  &  &  &  &  \\
    MagFace~\cite{meng2021magface}  & MSMV, M &  ResNet- &  &  &  &  &  \\
    \hline
    Shi~\emph{et al.}~\cite{shi2021boosting}  & cleaned MSMV2, M(+K) & ResNet-  &  &  &  &  & \\
    
    \hline
    \textbf{ArcFace}  & cleaned MSMV2, M & ResNet- &  &  &  &  &  \\
    
    \textbf{ArcFace+ours}    & cleaned MSMV2, M(+K) & ResNet- &  &  &  &  &  \\
  \hline
  \end{tabular}
  }
  \label{tab:result_ijbc}
\end{table}



\begin{table}[t]
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}} 
\renewcommand\arraystretch{1.00}
  \caption{ Comparison with state-of-the-art methods on three protocols of the IJB-S and TinyFace benchmark. The performance is reported in terms of rank retrieval (closed-set) and TAR@FAR (open-set). It is worth noting that MARN~\cite{gong2019low} is a multi-mode aggregation method and is fine-tuned on UMDFaceVideo~\cite{bansal2017s}, a video dataset.}
  \centering
  \resizebox{1\linewidth}{!}{
  \begin{tabular}{l | c | c ||  c c c  c |c c  c c |c c  c c||c c}
    \hline
    \multirow{2}{*}{Method } & \multirow{2}{*}{\tabincell{c}{\textbf{Labeled}\\Train Data}} & \multirow{2}{*}{Backbone}  & \multicolumn{4}{c|}{IJB-S V2S}   &    \multicolumn{4}{c|}{IJB-S V2B}   &
    \multicolumn{4}{c||}{IJB-S V2V} &  \multicolumn{2}{c}{TinyFace}  \\
    \cline{4-7}
    \cline{8-11}
    \cline{12-15}
    \cline{16-17}
    & & &Rank & Rank &  &  
    &Rank & Rank &  & 
    &Rank & Rank &  &  
    &Rank & Rank\\
    \hline
    C-FAN~\cite{gong2019video} & MSM- & ResNet- &  &   &  &  
                 &  &   &  & 
                 &  &   &  & 
                 &  &  \\
    MARN~\cite{gong2019low} & MSM-  & ResNet- &  &  &  &  
                 &  &  &  & 
                 &  &  &  & 
                 &  &  \\
    PFE~\cite{shi2019probabilistic} & MSM- & ResNet-  &  &  &  &  
                 &  &  &  & 
                 &  &   &  & 
                 &  &  \\
    ArcFace~\cite{deng2019arcface} & MSMV2 & ResNet- &  &  &  &  
                    &  &  &  & 
                    &  &  &  & 
                    &  &  \\
    \hline
    Shi~\emph{et al.}~\cite{shi2021boosting} & MSMV2-* & ResNet- &  &  &  &  
                 &  &  &  & 
                 &  &  &  & 
                 &  &  \\\hline
    \textbf{ArcFace}~\cite{deng2019arcface} & MSMV2-*  & ResNet-   &  &  &  &  
    &  &  &  &  
    &  &  &  &  
    &  &  \\
    
    \textbf{ArcFace+Ours*} & MSMV2-* & ResNet-     &  &  &  &  
    &  &  &  &  
    &  &  &  &  
    &  &  \\
    
    \textbf{ArcFace+Ours} & MSMV2-* & ResNet-  &  &  &  &  
    &  &  &  &  
    &  &  &  &  
    &  &  \\ \hline
    
        \textbf{AdaFace}~\cite{kim2022adaface}  & WebFace12M & IResNet-   &  &  &  &  
    &  &  &  &  
    &  &  &  &  
    &  &  \\
    
        \textbf{AdaFace+Ours}  & WebFace12M & IResNet-   &  &  &  &  
    &  &  &  &  
    &  &  &  &  
    &  &  \\
    
    \hline
  \end{tabular}
  }
  \vspace{0mm}
  \label{tab:result_ijbs}
\end{table}





\Paragraph{Experiment Setting}
We first train CFSM with  of MS-Celeb-1M training data (M) as the source domain, and WiderFace as the target domain (K). The model is trained for  steps with a batch size of . Adam optimizer is used with  and  at a learning rate of .


For the FR model training, we adopt ResNet- as modified in~\cite{deng2019arcface} as the backbone and use ArcFace loss function~\cite{deng2019arcface} for training. 
We also train a model without using CFSM (\emph{i.e.,} replication of ArcFace) for comparison, denoted as \textbf{ArcFace}.
The efficacy of our method (\textbf{ArcFace+Ours}) is validated by training an FR model with the guided face synthesis as the auxiliary data augmentation during training according to Eq.~\ref{eqn:outer_min}. 




\Paragraph{Results.}
Tables~\ref{tab:result_ijbb} and~\ref{tab:result_ijbc} respectively show the face verification and identification results on IJB-B and IJB-C datasets. 
Our approach achieves SoTA performance on most of the protocols.
For IJB-B, performance increase from using CFSM (\textbf{ArcFace+Ours}) is  for TAR@FAR=, and  for TAR@FAR= on IJB-C. Since both IJB-B and IJB-C are a mixture of high quality images and low quality videos, the performance gains with the augmented data indicate that our model can generalize to both high and low quality scenarios.
In Tab.~\ref{tab:result_ijbs}, we show the comparisons on IJB-S and TinyFace. With our CFSM (\textbf{ArcFace+Ours}), ArcFace model outperforms all the baselines in both face identification and verification tasks, and achieves a new SoTA performance.



\begin{figure}[t]
\footnotesize
  \resizebox{1\linewidth}{!}{
\begin{tabular}{ c  c  c }

\raisebox{-.5\height}{\includegraphics[scale=0.3]{fig/main/ijbs_vs_rank1.pdf}} &
\raisebox{-.5\height}{\includegraphics[scale=0.3]{fig/main/ijbs_vb_rank1.pdf}} &
\raisebox{-.5\height}{\includegraphics[scale=0.3]{fig/main/ijbs_vv_rank1.pdf}} \\
  \vspace{-5mm}\\ 
\raisebox{-.5\height}{\includegraphics[scale=0.3]{fig/main/ijbs_vs_1v1.pdf}} &
\raisebox{-.5\height}{\includegraphics[scale=0.3]{fig/main/ijbs_vb_1v1.pdf}} &
\raisebox{-.5\height}{\includegraphics[scale=0.3]{fig/main/ijbs_vv_1v1.pdf}}
\end{tabular}
}
\vspace{0mm}
\caption{ Comparison on three IJB-S protocols 
with varied number of training data on the -axis (maximum is 3.9M). The plots shows that using guided CFSM as an augmentation (\textcolor{mygreen}{\textbf{ArcFace+Ours}}) can lead to higher performance in all settings. Note that CFSM trained on K unlabeled data is more useful than M original data as shown by the higher \textbf{V2V} performance of \textbf{Ours} with M than Baseline M. } 
\label{fig:diff_training}
\vspace{-2mm}
\end{figure}







\subsection{Ablation and Analysis}
In this experiment, we compare the face verification and identification performance on the \emph{most challenging} IJB-S and TinyFace datasets.  

\Paragraph{Large-scale Training Data vs.~Augmentation.} To further validate the applicability of our CFSM as an augmentation in different training settings, we adopt IResNet-~\cite{deng2019arcface} as the FR model backbone and utilize SoTA AdaFace loss function~\cite{kim2022adaface} and large-scale WebFace12M~\cite{zhu2021webface260m} dataset for training. As compared in Tab.~\ref{tab:result_ijbs}, our model still improves unconstrained face recognition accuracies by a promising margin (Rank1:  on the IJB-S V2V protocol and  on TinyFace) on a large-scale training dataset (WebFace12M).


\Paragraph{Effect of Guidance in CFSM.}
To validate the effectiveness of the proposed \emph{controllable} and \emph{guided} face synthesis model in face recognition, we train a FR model with CFSM as augmentation but with random style coefficients (\textbf{Ours*}), and compare with guided CFSM  (\textbf{Ours}). Tab.~\ref{tab:result_ijbs} shows that the synthesis with random coefficients does not bring significant benefit to unconstrained IJB-S dataset performance. However, when samples are generated with guided CFSM, the trained FR model performs much better. Fig.~\ref{fig:update_faces} shows the effect of guidance in the training images. For low quality images, too much degradation leads to images with altered identity. Fig.~\ref{fig:update_faces} shows that guided CFSM avoids synthesizing bad quality images that are unidentifiable.  




\begin{figure}[t]

\centering    
\includegraphics[width=10.0cm]{fig/main/dataset.png}
\vspace{-2mm}
\caption{ \textbf{(a)} Few examples from each dataset. Note differences in style. For example, AgeDB contains old grayscale photos, WiderFace has mostly low resolution faces, and IJB-S includes extreme unconstrained attributes (\emph{i.e.,} noise, motion blur or turbulence effect). \textbf{(b)} shows the pairwise distribution similarity scores among datasets that are calculated using the learned subspace via Eq.~\ref{eqn:dist_simil}. Note both IJB-B and WiderFace have high similarity scores with IJB-S. \textbf{(c)} The t-SNE plot of the learned  and mean style . The dots represent  and the stars denote .} 
\vspace{-2mm}
\label{fig:dataset}
\end{figure}
 






\begin{figure}[t]
\centering
\includegraphics[width=11cm]{fig/main/mag_examples.png}
\caption{ \textbf{Interpretable magnitude of the style coefficients}. Given an input image, we randomly sample two sets of style coefficients  (left) and  (right) for all  models (respectively trained with the IJB-S, WiderFace and AgeDB datasets as the target data). We dynamically adjust the magnitude of these two coefficients by , , , , , , where .
As can be seen, our model indeed realizes the goal of changing the degree of style synthesis with the coefficient magnitude.
}
\label{fig:mag_examples}
\end{figure}


\Paragraph{Effect of the Number of Labeled Training Data.}
To validate the effect of the number of labeled training data, we train a series of models by adjusting the number of labeled training samples from M to M and report results both on \textbf{ArcFace} and \textbf{ArcFace+Ours} settings. Fig.~\ref{fig:diff_training} shows the performance on various IJB-S protocols. For the full data usage setting, our model trained with guided CFSM as an augmentation outperforms the baseline by a large margin.
Also note that the proposed method trained with th (M) labeled data still achieves comparable performance, or even better than the baseline with M labeled data on \textbf{V2V} protocol. This is due to CFSM generating target data-specific augmentations, thus demonstrating the value of our controllable and guided face synthesis, which can significantly boost unconstrained FR performance. 




\subsection{Analysis and Visualizations of the Face Synthesis model}\label{sec:exp_dist_simi}
In this experiment, we quantitatively evaluate the distributional similarity between datasets based on the learned linear subspace model for face synthesis. To this end, we choose  face datasets that are publicly available and popular for face recognition testing. These datasets are LFW~\cite{huang2008labeled}, AgeDB-~\cite{moschoglou2017agedb}, CFP-FP~\cite{sengupta2016frontal}, IJB-B~\cite{whitelam2017iarpa}, WiderFace (WF)~\cite{yang2016wider} and IJB-S~\cite{kalka2018ijb}. 
Figure~\ref{fig:dataset}\textcolor{red}{(a)} shows examples from these  datasets. Each dataset has its own style. For example, CFP-FP includes profile faces, WiderFace has mostly low resolution faces, and IJB-S contains extreme unconstrained attributes. 
During training, for each dataset, we randomly select K images as our target data to train the synthesis model. For the source data, we use the same subset of MS-Celeb-M as in Sec.~\ref{sec:comparison}.

\Paragraph{Distribution Similarity}
Based on the learned dataset-specific linear subspace model, we calculate the pairwise distribution similarity score via Eqn.~\ref{eqn:dist_simil}. As shown in Fig.~\ref{fig:dataset}\textcolor{red}{(b)}, the score reflects the style correlation between datasets. For instance, strong correlations among IJB-B, IJB-S and WiderFace (WF) are observed. We further visualize the learned basis vectors  and the mean style  in Fig.~\ref{fig:dataset}\textcolor{red}{(c)}. The basis vectors are well clustered and the discriminative grouping indicates the correlation between dataset-specific models. 


\Paragraph{Visualizations of Style Latent Spaces}
Fig.~\ref{fig:mag_examples} shows face images generated by the learned CFSM. It can be seen that when the magnitude increases, the corresponding synthesized faces reveal more dataset-specific style variations. This implies the magnitude of the style code is a good  indicator of image quality.

We also visualize the learned  of  models in Fig.~\ref{fig:basis_examples}. As we move along a basis vector of the learned subspace, the synthesized images change their style in dataset-specific ways. For instance, with target as WiderFace or IJB-S, synthesized images show various low quality styles such as blurring or turbulence effect. CFP dataset contains cropped images, and the ``crop'' style manifests in certain directions.  Also, we can observe the learned  are different among datasets, which further verifies that our learned linear subspace model in CFSM is able to capture the variations in target datasets.





\begin{figure}[t]
\footnotesize
\resizebox{1\linewidth}{!}{
\begin{tabular}{ c  c c }
   { LFW} & { AgeDB} & { CFP}\\
\raisebox{-.5\height}{\includegraphics[scale=0.28]{fig/main/component_LFW.png}}  & \raisebox{-.5\height}{\includegraphics[scale=0.28]{fig/main/component_W_agedb.png}} & \raisebox{-.5\height}{\includegraphics[scale=0.28]{fig/main/component_W_cfp.png}}  \\
\vspace{-2mm} \\
   { IJB-B} & { WiderFace} & { IJB-S}\\
\raisebox{-.5\height}{\includegraphics[scale=0.28]{fig/main/component_IJBB.png}}  & \raisebox{-.5\height}{\includegraphics[scale=0.28]{fig/main/component_W_wf.png}} & \raisebox{-.5\height}{\includegraphics[scale=0.28]{fig/main/component_IJBS.png}}  \\
\end{tabular}
}
\caption{
Given a single input image, we visualize the synthesized images by traversing along with the learned orthonormal basis  in the  dataset-specific models. 
For each dataset, Rows  illustrate the first  basis vectors traversed. 
Columns  show the directions which are scaled to emphasize their effect, {\it i.e.}, only one element of style coefficient  varies from  to  while other  elements remain . }
\label{fig:basis_examples}
\end{figure}
 

