
\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{booktabs, tabularx}
\usepackage{subcaption}
\usepackage{comment}

\usepackage{xspace}
\usepackage{tikz}
\usepackage{caption}
\usetikzlibrary{automata,positioning}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{url}

\newcommand{\blank}{\textsc{[mask]}}
\newcommand{\clstoken}{\textsc{[cls]}}
\newcommand{\bert}{BERT}
\newcommand{\ctx}{\mathbf{x}}
\newcommand{\ack}{RELIC\xspace}
\newcommand{\tac}{TAC-KBP 2010}
\newcommand{\conll}{CoNLL-Aida}


\newcommand{\entityvec}{\mathbf{h}_e}
\newcommand{\contextvec}{\mathbf{h}_x}
\newcommand{\context}{g}
\newcommand{\concept}{f}
\newcommand{\hvec}{\mathbf{h}}


\title{Learning Cross-Context Entity Representations from Text}



\author{\centerline{Jeffrey Ling\quad Nicholas FitzGerald\quad Zifei Shan\quad Livio Baldini Soares}\\
{\centerline{\bf Thibault F\'evry\quad David Weiss\quad Tom Kwiatkowski}} \\
Google Research\\
 Work done as a Google AI Resident.\\ 
\texttt{\{\small jeffreyling,nfitz,zifeis,liviobs,tfevry,djweiss,tomkwiat\}@google.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}


\maketitle

\begin{abstract}
Language modeling tasks, in which words, or word-pieces, are predicted on the basis of a local context, have been very effective for learning word embeddings and context dependent representations of phrases.
Motivated by the observation that efforts to code world knowledge into machine readable knowledge bases or human readable encyclopedias tend to be entity-centric, we investigate the use of a fill-in-the-blank task to learn context independent representations of entities from the text contexts in which those entities were mentioned.
We show that large scale training of neural models allows us to learn high quality entity representations, and we demonstrate successful results on four domains: (1) existing entity-level typing benchmarks, including a 64\% error reduction over previous work on TypeNet (Murty et al., 2018); (2) a novel few-shot category reconstruction task; (3) existing entity linking benchmarks, where we match the state-of-the-art on CoNLL-Aida without linking-specific features and obtain a score of 89.8\% on TAC-KBP 2010 without using any alias table, external knowledge base or in domain training data and (4) answering trivia questions, which uniquely identify entities.
Our global entity representations encode fine-grained type categories, such as \emph{Scottish footballers}, and can answer trivia questions such as \emph{Who was the last inmate of Spandau jail in Berlin?}
\end{abstract}

\section{Introduction}



A long term goal of artificial intelligence has been the development and population of an entity-centric representation of human knowledge. 
Efforts have been made to create the knowledge representation with knowledge engineers \citep{lenat1986cyc} or crowdsourcers \citep{bollacker2008freebase}.
However, these methods have relied heavily on human definitions of their ontologies, which are both limited in scope and brittle in nature.
Conversely, due to recent advances in deep learning, we can now learn robust general purpose representations of words \citep{mikolov2013distributed} and contextualized phrases \citep{peters2018deep} directly from large textual corpora.
In particular, we observe that existing methods of building contextualized phrase representations capture a significant amount of local semantic context \citep{devlin2018bert}.
We hypothesize that by learning an \emph{entity encoder} which aggregates all of the textual contexts in which an entity is seen, we should be able to extract and condense general purpose knowledge about that entity.

Consider the following \emph{contexts} in which an entity mention has been replaced a \blank:


\begin{figure}[h]
    \centering
    \footnotesize
    \begin{tabular}{c}
        \dots the second woman in space, 19 years after \blank. \vspace{3pt}\\
        \dots \blank, a Russian factory worker, was the first woman in space \dots \vspace{3pt}\\
         \dots \blank, the first woman in space, entered politics \dots.
    \end{tabular}
\label{fig:my_label}
\end{figure}
\noindent
As readers, we understand that \emph{first woman in space} is a unique identifier, and we are able to fill in the blank unambiguously. 
The central hypothesis of this paper is that, by matching entities to the contexts in which they are mentioned, we should be able to build a representation for Valentina Tereshkova that encodes the fact that she was the first woman in space, that she was a politician, etc. and that we should be able to use these representations across a wide variety of downstream entity-centric tasks.

We present \ack~(Representations of Entities Learned in Context), a table of independent entity embeddings that have been trained to match fixed length vector representations of the textual context in which those entities have been seen.
We apply \ack~to entity typing (mapping each entity to its properties in an external, curated, ontology); entity linking (identifying which entity is referred to by a textual context), and trivia question answering (retrieving the entity that best answers a question). 
Through these experiments, we show that:

\begin{itemize}
    \item \ack~accurately captures categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies. We demonstrate significant improvements over previous work on established benchmarks, including a 64\% error reduction in the TypeNet low data setting. We also show that given just a few exemplar entities of a given category such as {\it Scottish footballers}~we can use \ack~to recover the remaining entities of that category with good precision.
    \item Using \ack~for entity linking can match state-of-the-art approaches that make use of non-local and non-linguistic information about entities. On the CoNLL-Aida benchmark, \ack~achieves a 94.9\% accuracy, matching the state-of-the-art of \cite{Raiman2018-hm}, despite not using any entity linking-specific features. On the TAC-KBP 2010 benchmark \ack~achieves 89.8\% accuracy, just behind the top ranked system \citep{Raiman2018-hm}, which makes use of external knowledge bases, alias tables, and task-specific hand-engineered features.
    \item \ack~learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned, and not the surface form of the mention itself. For entity linking, the opposite is true.
    \item We can treat the \ack~embedding matrix as a store of knowledge, and retrieve answers to questions through nearest neighbor search. We show that this approach correctly answers 51\% of the questions in the TriviaQA reading comprehension task \citep{joshi2017triviaqa} despite not using the task's evidence text at inference time. The questions answered correctly by \ack~are surprisingly complex, such as \emph{Who was the last inmate of Spandau jail in Berlin?}

\end{itemize}

\begin{comment}
In particular, we show that \ack~significantly outperforms previous work on the FIGMENT \citep{yaghoobzadeh2015corpus} and TypeNet \citep{murty2018hierarchical} entity-level fine-typing tasks with a fraction of the task-specific training data; \ack~achieves ??\% on the TAC-KBP 2010 entity linking benchmark; and \ack 
including a 73\% error reduction over previous work in TypeNet's low data setting.

To do this, we start with \bert~\citep{devlin2018bert}, a powerful pretrained text encoder, to encode contexts---Wikipedia text in which a hyperlinked span has been blanked out---and we train an entity encoder to match the \bert~representation of the entity's contexts.
We focus on entities that have Wikipedia pages and hyperlinked mentions to provide a clean setting in which we can isolate the effects of learning entity representations from textual context.
We experiment with a lookup table that maps each entity to a fixed length vector, which we call \textbf{\ack{}} (Representations of Entities Learned In Context).
We hypothesize that the dedicated entity representations in \ack~should be able to capture knowledge that is not present in \bert. To test this, we compare \ack~to two \bert-based entity encoders: one that encodes the entity's Wikipedia name, and one that encodes the first paragraph of the entity's Wikipedia page.


Ultimately, we would like \ack to encode all of the salient information about each entity. 
In this work, we study our representations' ability to capture categorical information encoded by human experts in the Freebase and Wikipedia category hierarchies. 
First, we show that \ack~significantly outperforms previous work on both the FIGMENT \citep{yaghoobzadeh2015corpus} and TypeNet \citep{murty2018hierarchical} entity-level fine-typing tasks with a fraction of the task-specific training data.
In particular, \ack~achieves a 73\% error reduction over previous work in TypeNet's low data setting.
Next, we introduce a novel few-shot category reconstruction task based on Wikipedia and TypeNet categories.
We show that given just a few exemplar entities of a category such as \emph{Giro d'Italia cyclists}, we can use \ack{} to recover the remaining entities of that category with good precision.
Finally, we observe that answers to trivia questions are usually well known entities, and we apply \ack{} to the TriviaQA reading comprehension task \citep{joshi2017triviaqa}. 
We show that without access to any evidence text, \ack{} can answer complex questions such as \emph{Who was the last inmate of Spandau jail in Berlin?} with surprising accuracy.
\end{comment}


























 
\section{Related work}
\label{sec:related}

\paragraph{Entity linking}
The most widely studied entity-level task is entity linking---mapping each entity mention onto a unique entity identifier. 
The Wikification task \citep{ratinov2011local, cheng2013relational}, in particular, is similar to the work presented in this paper, as it requires systems to map mentions to the Wikipedia pages describing the entities mentioned. 
There is significant previous work that makes use of neural context and entity encoders in downstream entity linking systems \citep{sun2015modeling, yamada2016joint, yamada-etal-2017-learning, gupta2017entity, murty2018hierarchical,kolistas2018end}, but that previous work focuses solely on discriminating between entities that match a given mention according to an external alias table. Here we go further in investigating the degree to which \ack~can capture world knowledge about entities.

\paragraph{Mention-level entity typing}
Another well studied task is mention-level entity typing \citep[e.g.][]{ling2012fine, choi2018ultra}. 
In this task, entities are labeled with types that are supported by the immediate textual context. 
For example, given the sentence {\it `Michelle Obama attended her book signing'}, Michelle Obama should be assigned the type {\it author} but not {\it lawyer}. 
Subsequently, mention-level entity typing systems make use of contextualized representations of the entity mention, rather than the global entity representations that we focus on here.


\paragraph{Entity-level typing}
An alternative notion of entity typing is entity-level typing, where each entity should be associated with all of the types supported by a corpus.
\citet{yaghoobzadeh2015corpus} and \citet{murty2018hierarchical} introduce entity-level typing tasks, which we describe more in Section~\ref{sec:entity_typing_results}. 
Entity-level typing is an important task in information extraction, since most common ontologies make use of entity type systems. Such tasks provide a strong method of evaluating learned global representations of entities.

\paragraph{Using knowledge bases} 
There has been a strong line of work in learning representations of entities by building knowledge base embeddings \citep{bordes2011learning, socher2013reasoning, yang2014embedding, toutanova2016compositional, vilnis2018box}, and by jointly embedding knowledge bases and information from textual mentions \citep{riedel2013relation, toutanova2015representing, hu2015entity}.
\citet{das2017question} extended this work to the {\sc spades} fill-in-the-blank task \citep{bisk2016evaluating}, which is a close counterpart to \ack's training setup.
However, we note that all examples in {\sc spades} correspond to a fully connected sub-graph in Freebase~\cite{bollacker2008freebase}. 
Subsequently, the contents 
are very limited in domain and \citet{das2017question} show that it is essential to use the contents of Freebase to do well on this task. 
We consider the unconstrained TriviaQA task \citep{joshi2017triviaqa}, introduced in Section~\ref{sec:trivia-qa}, to be a better evaluation for open domain knowledge representations.

\paragraph{Fill-in-the-blank tasks} 
There has been significant previous work in using fill-in-the-blank losses to learn context independent word representations \citep{mikolov2013distributed}, and context-dependent word and phrase representations \citep{dai2015semi,peters2018deep,radford2018improving,devlin2018bert}.
Cloze-style tasks, in which a system must choose which of a few entities best fill a blanked out span, have also been proposed as a method of evaluating reading comprehension \citep{Hermann:15, long2016leveraging, Onishi:16}. 
For entities, \citet{long2017world} consider a similar fill-in-the-blank task as ours, which they frame as rare entity prediction.
\citet{yamada2016joint} and \citet{yamada-etal-2017-learning} train entity representations using a fill-in-the-blank style loss and a bag-of-words representation of mention contexts. 
\citet{yamada2016joint, yamada-etal-2017-learning}~in particular take an approach that is very similar in motivation to \ack, but which focuses on learning entity representations for use as features in downstream classifiers that model non-linear interactions between a small number of candidate entities. 
In Section~\ref{sec:category_completion}, we show that \cite{yamada-etal-2017-learning}'s entity embeddings are good at capturing broad entity types such as {\it Tennis Player} but less good at capturing more complex compound types such as {\it Scottish Footballers}. 
In Section~\ref{sec:entity_linking_results}, we also show that by performing nearest neighbor search over the 818k entities in the TAC knowledge base, \ack can surpass \citealt{yamada-etal-2017-learning}'s performance on the \tac~entity linking benchmark \citep{Ji10overviewof}. This is despite the fact that \citeauthor{yamada-etal-2017-learning} massively restrict the linking search space with an externally defined alias table, and incorporate task-specific hand-engineered features. On the CoNLL-Aida benchmark, we show that \ack surpasses \citealt{yamada-etal-2017-learning} and matches \citealt{Raiman2018-hm} without using any entity linking-specific features.


































%
 
\section{Learning from context}\label{sec:learning_setup}

\newcommand{\entstart}{[\textsc{e}_s]}
\newcommand{\entend}{[\textsc{e}_e]}

\subsection{\ack~training input}
Let  be a predefined set of entities, and let  be a vocabulary of words.
A \emph{context}  is a sequence of words . 
Each context contains exactly one entity start marker  and one entity end marker , where . 
The sequence of words between these markers, , is the entity mention. 

Our training data is a corpus of (context, entity) pairs .
Each  identifies an entity that corresponds to the single entity mention in . 
We train \ack~to correctly match the entities in  to their mentions.
We will experiment with settings where the mentions are unchanged from the original corpus, as well as settings where with some probability  (the \textit{mask rate}) all of the words in the mention have been replaced with the uninformative  symbol. We hypothesize that this parameter will play a role in the effectiveness of learned representations in downstream tasks.

For clean training data, we extract our corpus from English Wikipedia\footnote{https://en.wikipedia.org}. See Section~\ref{sec:implementation} for details.

\subsection{context encoder}
We embed each context in  into a fixed length vector using a Transformer text encoder \citep{vaswani2017attention}, initialized with parameters from the BERT-base model released by \citealt{devlin2018bert}.
All parameters are then trained further using the objective presented below in Section~\ref{sec:ent-ctx-compat}.

We take the output of the Transformer corresponding to the initial  token in BERT's sequence representation as our context encoding, and we linearly project this into  using a learned weight matrix  to get a context embedding in the same space as our entity embeddings.

\subsection{entity embeddings}
Each entity  has a unique and abstract Wikidata QID\footnote{https://www.wikidata.org/wiki/Q43649390}.
\ack~maps these unique IDs directly onto a dedicated vector in  via a  dimensional embedding matrix.
In our experiments, we have a distinct embedding for every concept that has an English Wikipedia page, resulting in 5m entity embeddings overall.

\subsection{\ack~training loss}
\label{sec:ent-ctx-compat}
\ack~optimizes the parameters of the context encoder and entity embedding table to maximize the compatibility between observed (context, entity) pairs. 
Let  be a context encoder, and let  be an embedding function that maps each entity to its  dimensional representation via a lookup operation.
We define a compatibility score between the entity  and the context  as the scaled cosine similarity\footnote{In our experiments, we found cosine similarity to be more effective than dot product.}

where the scaling factor  is a learned parameter, following \citet{wang2018additive}.
Now, given a context , the conditional probability that  was the entity seen with  is defined as

and we train \ack~by maximizing the average log probability 


In practice, the definition of probability in Equation~\ref{eqn:entity-prob} is prohibitively expensive for large  (we use ). 
Therefore, we use a noise contrastive loss \citep{gutmann2012noise, mnih2013learning}.
We sample  negative entities from a noise distribution :

Denoting , we then compute our per-example loss using cross entropy:




In practice, we train our model with minibatch gradient descent and use all other entries in the batch as negatives.
That is, in a batch of size 4, entities for rows 1, 2, 3 will be used as negatives for row 0. This is roughly equivalent to  being proportional to entity frequency.


 


\section{Experimental setup}
\label{sec:implementation}

To train \ack, we obtain data from the 2018-10-22 dump of English Wikipedia. We take  to be the set of all entities in Wikipedia (of which there are over 5 million).
For each occurrence of a hyperlink, we take the context as the surrounding sentence, replace all tokens in the anchor text with a single \blank~symbol with probability  (see Section~\ref{sec:masking_ablation} for a discussion of different masking rates) and set the ground truth to be the linked entity.
We limit each context sentence to 128 tokens.
In this way, we collect a high-quality corpus of over 112M (context, entity) pairs.
Note in particular that an entity never co-occurs with text on its own Wikipedia page, since a page will not hyperlink to itself.
We set the entity embedding size to . 

We train the model using TensorFlow \citep{abadi2016tensorflow} with a batch size of 8,192 for 1M steps on Google Cloud TPUs. 


\section{Evaluation} \label{sec:evaluation}
We evaluate \ack's ability to: (1) solve the entity linking task without access to any task specific alias tables or features; (2) accurately capture entity properties that have been hand-coded into TypeNet and Wikipedia categories; (3) capture trivia knowledge specific to individual entities.

First we present results on established entity linking and entity typing tasks, to compare \ack's performance to established baselines and we show that the choice of masking strategy (Section~\ref{sec:learning_setup}) has a significant and opposite impact on performance on these tasks. 
We hypothesize that \ack~is approaching an upper bound on established entity-level typing tasks, and we introduce a much harder category completion task that uses \ack~to populate complex Wikipedia categories. We also apply \ack's context encoder and entity embeddings to the task of end-to-end trivia question answering, and we show that this approach can capture more than half of the answers identified by the best existing reading comprehension systems.

\subsection{Entity Linking} \label{sec:entity_linking_results}
\begin{table}[]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    System & \conll & \tac \\\hline
    \citealt{Sil2018-qo} & 94.0 & 87.4 \\
    \citealt{yamada2016joint} & 91.5 &  85.5 \\ 
    ~~~~~~~~ - entity linking features & 81.1 & 80.1 \\
    \citealt{yamada-etal-2017-learning}& 94.3 & 87.7 \\
    \citealt{Radhakrishnan2018-zj} & 93.0 & 89.6 \\
    \citealt{Raiman2018-hm} & 94.9 & 90.9 \\
    \ack & 81.9 & 87.5 \\
    \ack + \conll~tuning & 94.9\footnotemark & 89.8 \\
    \hline
    \end{tabular}
    \caption{\ack~achieves comparable precision to best performing dedicated entity-linking systems despite using no external resources or task specific features. When given a standard \conll~alias table and tuned on the \conll~training set, \ack's learned representations match the state-of-the-art DeepType system which relies on the large hand engineered Wikidata knowledge base.}
    \label{tab:entity_linking}
\end{table}
\ack~can be used to directly solve the entity linking problem.\footnotetext{Our finetuned CoNLL result uses the alias table of \cite{pprpershina} at inference time.}
We just need to find the single entity that maximizes the cosine similarity in Equation~\ref{eqn:entity_score} for a given context.
For the entity linking task, we create a context from the document's first 64 tokens as well as the 64 tokens around the mention to be linked.
This choice of context is well suited to the documents in the \conll~and \tac~datasets, since those documents tend to be news articles in which the introduction is particularly information dense.
In Table~\ref{tab:entity_linking}~we show performance for \ack~in two settings. 
First, we report the accuracy for the pure \ack~model with no in-domain tuning.
Then, we report the accuracy for a \ack~model that has been tuned on the CoNLL-Aida training set. 
On the \conll~benchmark, we also adopt a standard alias table \citep{pprpershina}~for this tuned model, as is commonly done in previous entity linking work. 

It is clear that for the \conll~benchmark in-domain tuning is essential.
We hypothesize that this is because of the dataset's bias towards certain types of news content that is very unlike our Wikipedia pre-training data---specifically sports reports. However, when we do adopt the standard CoNLL-Aida training set and alias table, \ack~matches the state of the art on this benchmark, despite using far fewer hand engineered resources (\cite{Raiman2018-hm}~use the large Wikidata knowledge base to create entity representations).
We do not make use of the \tac~training set or alias table, and we observe that \ack~is already competitive without these enhancements\footnote{We do reduce the candidate set from the 5m entities covered by \ack~to the 818k entities in the \tac~knowledge base to avoid ontological misalignment.}

It is significant that \ack~matches the performance of \cite{Raiman2018-hm}, which uses the large hand engineered Wikidata knowledge base to represent entities.
This supports our central hypothesis that it is possible to capture the knowledge that has previously been manually encoded in knowledge bases, using entity embeddings learned from textual contexts alone. In the rest of this section, we will show further support for our hypothesis by recreating parts of the Freebase and Wikipedia ontologies, and by using \ack~to answer trivia questions.

Finally, we believe that \ack's entity linking performance could be boosted even higher through the adoption of commonly used entity linking features.
As shown in Table~\ref{tab:entity_linking}, \cite{yamada2016joint} use a small set of well chosen discrete features to increase the performance of their embedding based approach by 10 points.
These features could be simply integrated into \ack's model, but we consider them to be orthogonal to this paper's investigation of purely learned representations.





\subsection{Entity-level fine typing} \label{sec:entity_typing_results}


We evaluate \ack's ability to capture entity properties on the FIGMENT \citep{yaghoobzadeh2015corpus} and TypeNet \citep{murty2018hierarchical} entity-level fine typing tasks which contain 102 and 1,077 types drawn from the Freebase ontology \citep{bollacker2008freebase}.
The task in both datasets is to predict the set of fine-grained types that apply to a given entity. We train a simple 2-layer feed-forward network that takes as input \ack's embedding  of the entity  and outputs a binary vector indicating which types apply to that entity.

Tables~\ref{tab:figment_results}, \ref{tab:typenet_results} show that \ack significantly outperforms prior results on both datasets.
For FIGMENT, \citet{yaghoobzadeh2018corpus} is an ensemble of several standard representation learning techniques: \texttt{word2vec} skip-gram contexts \citep{mikolov2013distributed}, \emph{structured} skip-gram contexts \citep{ling2015two}, and \texttt{FastText} representations of the entity names \citep{bojanowski2017enriching}.
For TypeNet, \citet{murty2018hierarchical} aggregate mention-level types and train with a structured loss based on the TypeNet hierarchy, but is still outperformed by our flat classifier of binary labels. We expect that including a hierarchical loss is orthogonal to our approach and could improve our results further.

The most striking results in Tables~\ref{tab:figment_results} and \ref{tab:typenet_results} are in the low data settings. 
On the low-data TypeNet setting of \citet{murty2018hierarchical}, \ack~achieves a 63\% error reduction over previous work, while \ack~also matches \citealt{yaghoobzadeh2018corpus}'s results on FIGMENT with 5\% of the training data.
\begin{table}[]
    \centering
    \begin{tabular}{|l|c|c|c|}
    \hline
        System & F1 & P@1 & Acc \\ \hline
        \citealt{yaghoobzadeh2018corpus} &  82.3 & 91.0 & 56.5 \\
        \ack & 87.9 & 94.8 & 68.3 \\
        \ack with 5\% of FIGMENT training data & 83.3 & 90.9 & 59.3 \\ \hline
    \end{tabular}
    \caption{Performance on FIGMENT. We report P@1 (proportion of entities whose top ranked types are correct), Micro F1 aggregated over all (entity, type) compatibility decisions, and overall accuracy of entity labeling decisions. \ack outperforms prior work, even with only 5\% of the training data.}
    \label{tab:figment_results}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
    System & TypeNet & TypeNet - Low Data (5\%) \\\hline
    \citealt{murty2018hierarchical} & 78.6 & 58.8 \\
    \ack & 90.1 & 85.3 \\\hline
    \end{tabular}
    \caption{Mean Average Precision on TypeNet tasks. \ack's gains are particularly striking in the low data setting from \cite{murty2018hierarchical}.}
    \label{tab:typenet_results}
\end{table}

\subsection{Effect of masking} \label{sec:masking_ablation}

\begin{figure}[t!]
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[scale=0.3]{images/typenet_mask_rates.png}
        \caption{TypeNet entity-level typing mAP on the development set for \ack models trained with different masking rates. A higher mask rate leads to better performance, both in low and high-data situations.}
        \label{fig:typenet_mask_rates}
    \end{minipage}~~~~~~~~~~~~~~~~~~~~~
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.3]{images/el_mask_rates.png}
        \caption{Entity linking accuracy for \ack models trained with different masking rates. No alias table nor in-domain fine-tuning is used. Higher mask rates lead to worse downstream performance in entity-linking tasks.}
        \label{fig:el_mask_rates}
    \end{minipage}
\end{figure}

In Section~\ref{sec:learning_setup} we introduced the concept of masking entity mentions, and predicting on the basis of the context in which they are discussed, not the manner in which they are named. 
Figures~\ref{fig:typenet_mask_rates} and \ref{fig:el_mask_rates} show the effect of training \ack~with different mask rates. 
It is clear that masking mentions during training is beneficial for entity typing tasks, but detrimental for entity linking. 
This is in accordance with our intuitions. Modeling mention surface forms is essential for linking, since these mentions are given at test time and names are extremely discriminative. However, once the mention is known the model only needs to distinguish between different entities with the same name (e.g. {\it President Washington, University of Washington, Washington State}) and this distinction rarely requires deep knowledge of each entity's properties. Subsequently, our best typing models are those that are forced to capture more of the context in which each entity is mentioned, because they are not allowed to rely on the mention itself.
The divergence between the trends in Figures~\ref{fig:typenet_mask_rates} and \ref{fig:el_mask_rates}~suggests that there may not be one set of entity embeddings that are optimum for all tasks. However, we would like to point out that that a mask rate of 10\%, \ack~nears optimum performance on most tasks. The optimum mask rate is an open research question, that will likely depend on entity frequency as well as other data statistics.


\subsection{Few-shot category completion} \label{sec:category_completion}
The entity-level typing tasks discussed above involve an in-domain training step. 
Furthermore, due to the incompleteness of the the FIGMENT and TypeNet type systems, we also believe that \ack's performance is approaching the upper bound on both of these supervised tasks.
Therefore, to properly measure \ack's ability to capture complex types from fill-in-the-blank training alone, we propose:

\begin{enumerate}
\item a new category completion task that does not involve any task specific optimization,
\item a new Wikipedia category based evaluation set that contains much more complex compound types, such as {\it Scottish footballers},
\end{enumerate}

We use this new task to compare \ack~to the embeddings learned by \citealt{yamada-etal-2017-learning}.

In the new category completion task, we represent each category by randomly sampling three exemplar entities, and calculating the centroid of their \ack~embeddings.
We then rank all other entities according to their dot-product with this centroid, and report the mean average precision (MAP) of the resultant ranking.


\begin{table}[]
    \centering
    \begin{tabular}{|l||c|c||c|c|}
    \hline
    & \multicolumn{2}{c||}{Yamada Subset} & \multicolumn{2}{c|}{All Entities} \\ \hline
    & TypeNet & Wikipedia & TypeNet & Wikipedia \\ \hline
    \# Entities & 291,663 & 707,588 & 323,347 & 3,667,933 \\ \hline
    Random & 2.7 & 0.1 & 2.5 & 0.1 \\
    \citealt{yamada-etal-2017-learning} & 25.9 & 8.0 & -- & -- \\
    \ack & 27.8 & 21.0 & 29.3 & 13.8 \\ \hline
\end{tabular}
    \caption{Mean average precision on exemplar-based category completion (Section~\ref{sec:category_completion}).
    The Yamada subset is filtered to only contain entities that are covered by \citealt{yamada-etal-2017-learning}, and categories are filtered to those
    which contain at least 300 entities (131 categories).
    For the "All Entities" setting, we use all Wikipedia entities covered by \ack, and filter to categories which contain at least 1000 entities (1083 categories).
    The embeddings learned by \citealt{yamada-etal-2017-learning} are competitive with \ack~on the task of populating TypeNet categories, but they are much worse at capturing the complex, and compound, typing information present in Wikipedia categories.}
    \label{tab:category_completion}
\end{table}


First, we apply this evaluation to the TypeNet type system introduced in \citep{murty2018hierarchical}.
These types are well-curated, but tend to represent high-level categories.
To measure the degree to which our entity embeddings capture finer grained type information, we construct an aditional dataset based on Wikipedia categories\footnote{We use the Yago 3.1~\citep{mahdisoltani2013yago3} dump of extracted categories that cover at least 1,000 entities, resulting in 1,083 categories.}.
These tend to be compound types, such as {\it Actresses from London}, which capture many aspects of an entity---in this case gender, profession, and place of birth.

From Table~\ref{tab:category_completion} we can see that the embeddings introduced by \citealt{yamada-etal-2017-learning} approach \ack's  performance on the TypeNet completion task, but they significantly underperform \ack~in completing the more complex Wikipedia categories.
Figure~\ref{tab:category_preds} shows example reconstructions for randomly sampled Wikipedia categories, two from TypeNet and three from Wikipedia. 
Both models achieve high precision on TypeNet categories, but on the finer-grained Wikipeida categories, the \citet{yamada-etal-2017-learning} model tends to
produce more broadly-related entites, whereas the \ack embeddings capture entities which are much closer to the exemplars.
In fact, we identify several false negatives in these examples.








\subsection{Trivia question answering}
\label{sec:trivia-qa}





\begin{table}[]
    \small
    \centering
    \begin{tabular}{|l|c|c|}
    \hline
          & Open-domain Unfiltered & Verified Web \\\hline
    Classifier baseline \citep{joshi2017triviaqa} & ---  & 30.2 \\
    SLQA \citep{wang2018multi}                    & ---  & 82.4 \\
    \hline
    \ack                                          & 35.7 & 51.2 \\ 
    ORQA \citep{lee-etal-2019-latent}             & 45.0 & ---  \\
    \hline
    \end{tabular}
    \caption{Answer exact match on TriviaQA. \ack's~fast nearest neighbor search over entities achieves 80\% of the performance of ORQA, which runs a BERT-based reading comprehesion model over multiple retrieved evidence passages. Unlike ORQA and \ack, the classifier baseline and SLQA have access to a single evidence document that is known to contain the answer. As a result they are solving a much easier task. } \label{tab:triviaqa_results}
\end{table}

\begin{comment}
\begin{table*}[]
    \small
    \centering
    \begin{tabular}{|c|cc|cc|cc|cc|}
    \hline
    & \multicolumn{2}{c|}{Web} & \multicolumn{2}{c|}{Web verified} & \multicolumn{2}{c|}{Wiki} & \multicolumn{2}{c|}{Wiki verified} \\
    & EM & F1 & EM & F1 & EM & F1 & EM & F1 \\
    \hline
    Classifier \citep{joshi2017triviaqa} & 24.0 & 28.4 & 30.2& 34.7 & 22.5 & 26.5 & 27.3 & 31.4 \\
    SLQA \citep{wang2018multi} & 68.7 & 73.1 & 82.4 & 85.4 & 66.6 & 71.4 & 74.8 & 78.7 \\
    \ack & 39.9 & 43.4 & 44.3 & 47.6 & 38.3 & 41.8 & 40.6 & 44.5 \\
    \hline
    \end{tabular}
    \caption{TriviaQA results. \ack~simply embeds the question and finds the closest entity. The other two approaches have access to evidence documents at  top rows build representations from (entity, context) pairs. The bottom two rows have access to evidence documents at test time.}
    \label{tab:triviaqa_results}
\end{table*}
\end{comment}

Our final experiment tests \ack's ability to answer trivia questions -- which can be considered high precision categories that only apply to a single entity -- using retrieval of encoded entities.
TriviaQA \citep{joshi2017triviaqa} is a question-answering dataset containing questions sourced from trivia websites, and the answers are usually entities with Wikipedia pages. The standard TriviaQA setup is a reading comprehension task, where answers are extracted from evidence documents.
Here, we answer questions in TriviaQA \emph{without access to any evidence documents at test time.}

\paragraph{Model and training} Given a question, we apply the context encoder  from Section~\ref{sec:ent-ctx-compat}, and retrieve 1 out of 5M entities using cosine similarity.
For training, we initialize both  and  from \ack training. We tune only 's parameters by optimizing the loss in Equation~\ref{eqn:batch_loss} applied to (question, answer entity) pairs, rather than the (context, entity) pairs seen during \ack's training.


\paragraph{Results}





TriviaQA results are shown in Table~\ref{tab:triviaqa_results}, and randomly sampled \ack~predictions are illustrated in Figure~\ref{tab:triviaqa_preds}.
All systems other than \ack~in Table~\ref{tab:triviaqa_results}~have access to evidence text at inference time.
In the open domain unfiltered setting, ORQA \citep{lee-etal-2019-latent} retrieves this text from a cache of Wikipedia. In the more standard verified-web reading comprehension task, the classifier baseline and SLQA are provided with a single document that is known to contain the answer. 

We consider ORQA to be the most relevant point of comparison for \ack. We observe that the retrieve-then-read approach taken by ORQA outperforms the direct answer retrieval approach taken by \ack.
However, ORQA runs a BERT based reading comprehension model over multiple evidence passages at inference time and we are encouraged to see that \ack's much faster nearest neighbor lookup captures 80\% of ORQA's performance.

It is also significant that \ack~outperforms \cite{joshi2017triviaqa}'s reading comprehension baseline by 20 points, despite the fact that the baseline has access to a single document that is known to contain the answer. However, \ack~is still far behind the reading comprehension upper bound set by \cite{wang2018multi} and there is a long way to go before \ack's embeddings can capture all of the facts that can be identified by question-dependent inference time reading.    


\begin{comment}
, but we are encouraged to see that \ack~can capture 80\% of the performance of ORQA.
This is despite the fact that , while \ack~simply performs a fast nearest neighbor search over entities.

However, the ORQA system presented by \cite{lee-etal-2019-latent} uses a learned model to retrieve this evidence text from a cache of Wikipedia, providing an interesting point of comparison for \ack. 


We mainly compare to the Classifier system from \citet{joshi2017triviaqa}, which trains a classifier on text spans from the evidence documents, and also compare to the best published results \citep{wang2018multi} as an upper bound.
It is important to note that, at test time, reading comprehension models like Classifier only need to select from the small set of entities that occur in the evidence documents, whereas \ack~must select one answer from the full set of 5M Wikipedia entities.
Even in this difficult retrieval setting, \ack significantly outperforms the Classifier baseline although this end-to-end approach is still nowhere near outperforming the most performant reading comprehension systems. 
We show some of \ack's predictions on the TriviaQA task in Figure~\ref{tab:triviaqa_preds}. We note that even when the top 1 prediction is incorrect, the model is able to retrieve entities that are semantically similar to the ground truth.
These results imply that there may be an opportunity to use \ack~along with more established approaches in open domain question answering \citep{chen2017reading}.
\end{comment}
 
\begin{figure*}[]
    \begin{subtable}[b]{\textwidth}
    \scriptsize
    \centering
    \begin{tabular}{|p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}|}
        \toprule
        \hline
        \multicolumn{1}{|c|}{Category and Exemplars} & \multicolumn{1}{c|}{\citealt{yamada-etal-2017-learning}} & \multicolumn{1}{c|}{\ack} \\ \hline
        \texttt{tennis\_player} \newline David Goffin \newline Yves Allegro \newline Flavia Pennatta & 1. \textbf{Ekaterina Makarova} \newline 2. \textbf{Vera Zvonareva} \newline 3. \textbf{Flavia Pennetta} \newline 4. \textbf{Max Mirnyi} \newline 5. \textbf{Lisa Raymond} \newline AP=71.87 & 1. \textbf{Prakash Amritraj} \newline 2. \textbf{Marco Chiudinelli} \newline 3. \textbf{Marc Gicquel} \newline 4. \textbf{Marius Copil} \newline 5. \textbf{Benjamin Balleret} \newline AP=56.87 \\ 
        \hline
        \texttt{exhibition\_producer} \newline Toledo Museum of Art \newline Egyptian Museum \newline San Jose Museum of Art &  1. \textbf{Smithsonian American Art Museum} \newline 2. \textbf{Honolulu Museum of Art} \newline 3. \textbf{Brooklyn Museum} \newline 4. \textbf{Whitney Museum of American Art} \newline 5. \textbf{Hirshhorn Museum and Sculpture Garden} \newline AP=38.41 & 1. \textbf{Cleveland Museum of Art} \newline 2. \textbf{Smithsonian American Art Museum} \newline 3. \textbf{Indianapolis Museum of Art} \newline 4. \textbf{Cincinnati Art Museum} \newline 5. \textbf{Museum of Fine Arts, Boston} \newline AP=52.52 \\ 
        \hline
        \hline
        \texttt{Scottish footballers} \newline Pat Crerand \newline Gerry Britton \newline Jim McLean &  1. Ayr United F.C. \newline 2. Clyde F.C. \newline 3. Scottish League Cup \newline 4. Stranraer F.C. \newline 5. Arbroath F.C. \newline AP=4.57 & 1. \textbf{Tommy Callaghan} \newline 2. \textbf{Gordon Wallace} \newline 3. \emph{David White}** \newline 4. Davie Dodds \newline 5. \textbf{John Coughlin} \newline AP=67.10 \\ 
        \hline
        \texttt{Number One Singles in Germany} \newline Lady Marmalade \newline Just Give Me a Reason \newline I'd Do Anything for Love (But I Won't Do That) &  1. Billboard Hot 100 \newline 2.  Grammy Award for Best Female Pop Vocal Performance \newline 3. Dance Club Songs \newline 4. Pop 100 \newline 5. Hot Latin Songs \newline AP=4.14 & 1. Try (Pink song) \newline 2. \emph{Whataya Want from Me}** \newline 3. Fuckin' Perfect \newline 4. Beautiful (Christina Aguilra song) \newline 5. Raise Your Glass \newline AP=4.59 \\ 
        \hline
        \texttt{2010 Albums} \newline This Is the Warning \newline Tin Can Trust \newline Bionic (Christina Aguilera album) &  1. FAA airport categories \newline 2. Rugby league county cups \newline 3. Digital Songs \newline 4. Country Airplay \newline 5. Swiss federal election, 2007 \newline AP=0.04 & 1. \emph{All I Want Is You}** \newline 2. Don't Mess with the Dragon \newline 3. \emph{Believe (Orianthi album)}** \newline 4. Sci-Fi Crimes \newline 5. \textbf{Interpol} \newline AP=6.78 \\ 
        \hline
\bottomrule


    \end{tabular}
    \caption{Top 5 predictions for a set of randomly selected categories, given 3 exemplars. The first two categories come from TypeNet, and the second two from our Wikipedia categorization dataset. Correct predictions are bolded. Predictions which are judged by the authors to be false negatives (predictions which properly belong to the target category) are indicated with asterisks**.}
    \label{tab:category_preds}
    \end{subtable}
    
    \vspace{0.2cm}

\begin{subtable}[t]{\textwidth}
\begin{framed}
\small
        \textbf{\underline{Q:}} Who was the last inmate of Spandau jail in Berlin? \\
        \textbf{\underline{A:}} \textbf{1. Rudolf Hess} 2. Adolf Hitler 3. Hermann Göring 4. Heinrich Himmler 5. Ernst Röhm \\
       
        \textbf{\underline{Q:}} Which fashionable London thoroughfare, about three quarters of a mile (1.2 km) long, runs from Hyde Park Corner to Marble Arch, along the length of the eastern side of Hyde Park? \\
        \textbf{\underline{A:}} \textbf{1. Park Lane} 2. Piccadilly 3. Knightsbridge 4. Leicester Square 5. Tottenham Court Road \\
    
        \textbf{\underline{Q:}} In which Lake District town would you find the Cumberland Pencil Museum? \\
        \textbf{\underline{A:}} \textbf{1. Keswick} 2. Hawkshead 3. Grasmere 4. Cockermouth 5. Ambleside \\
  
        \textbf{\underline{Q:}} The Wimbledon tennis tournament is held at which tennis club in London? \\
        \textbf{\underline{A:}} 1. Queen's Club \textbf{2. All England Lawn Tennis and Croquet Club} 3. Wimbledon Championships 4. Stade Roland-Garros 5. Wentworth Club 


\end{framed}
    \caption{TriviaQA predictions from retrieval. Questions are randomly sampled, and top 5 ranking answers are shown. Correct answer in bold. Note that even when the model is wrong, the predictions are all of the correct type.}
    \label{tab:triviaqa_preds}
    \end{subtable}
    
    \caption{Random example predictions drawn from category completion, and TriviaQA tasks.}

\end{figure*} 
\section{Conclusion}
\label{sec:conclusion}

In this paper, we demonstrated that the \ack fill-in-the-blank task allows us to learn context independent representations of entities with their own latent ontology.
We show successful entity-level typing results on FIGMENT \citep{yaghoobzadeh2015corpus} and TypeNet \citep{murty2018hierarchical}, even when only training on a small fraction of the task-specific training data.
We then introduce a novel few-shot category reconstruction task and when comparing to \citet{yamada-etal-2017-learning}, we found that \ack~is better able to capture complex compound types.  
Our method also proves successful for entity linking, where we match the state of the art on CoNLL-Aida despite not using linking-specific features and fare similarly to the best system on \tac~despite not using an alias table, any external knowledge bases, linking-specific features or even in-domain training data.
Finally, we show that our \ack~embeddings can be used to answer trivia questions directly, without access to any evidence documents. 
We encourage researchers to further explore the properties of our entity representations and BERT context encoder, which we will release publicly.



%
 
\clearpage

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\end{document}
