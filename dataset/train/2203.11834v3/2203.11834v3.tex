{\noindent
\textbf{\Large{Appendix}}}

\section{Background}
\label{app:background}
In this section, we briefly review the details of Sharpness-Aware Minimization (SAM) \cite{foret2020sharpness}, its adaptive version (ASAM) \cite{kwon2021asam} and Stochastic Weight Averaging (SWA) \cite{izmailov2018averaging}.
\subsection{SAM and ASAM: Overview}
\subsubsection{SAM} aims at finding the solution $\theta$ surrounded by a neighborhood having uniform low training loss $\mathcal{L}_\mathcal{D}(\theta)$, \textit{i.e.} located in a flat minimum. The \textit{sharpness} of a training loss function is defined as:
\begin{equation}
    \max_{||\epsilon||_p \leq \rho}  \mathcal{L}_\mathcal{D}(\theta+\epsilon) - \mathcal{L}_\mathcal{D}(\theta)
    \label{math:sharpness}
\end{equation}
where $\rho$ is an hyper-parameter defining the neighborhood size and $p\in[1,\infty)$. SAM aims at minimizing the sharpness of the loss solving the following minmax objective:
\begin{equation}
    \min_{\theta\in\mathbb{R}^d}\max_{||\epsilon||_p \leq \rho}  \mathcal{L}_\mathcal{D}(\theta+\epsilon) + \lambda ||\theta||_2^2
    \label{math:minmax_sam}
\end{equation}
where $\lambda$ is a hyper-parameter weighing the importance of the regularization term. In \cite{foret2020sharpness}, it is shown that $p=2$ is typically the optimal choice, hence, without loss of generality, we use the ${\ell}_2$-norm in the maximization over $\epsilon$ and omit the regularization term for simplicity. In order to obtain the exact solution of the inner maximization problem $\epsilon^* \triangleq \arg\max_{||\epsilon||_2 \leq \rho} \mathcal{L}(\theta+\epsilon)$, the authors propose to employ a first-order approximation of $\mathcal{L}(\theta+\epsilon)$ around $0$:
\begin{equation}
    \epsilon^* \approx \arg\max_{||\epsilon||_2 \leq \rho} \mathcal{L}_\mathcal{D}(\theta) + \epsilon^T \nabla_{\theta}\mathcal{L}_\mathcal{D}(\theta) =\rho\frac{\nabla_{\theta}\mathcal{L}_\mathcal{D}(\theta)}{||\nabla_{\theta}\mathcal{L}_\mathcal{D}(\theta)||_2} =: \hat{\epsilon}(\theta)
    \label{math:approx_sam}
\end{equation}
Under this computationally efficient approximation, 
$\hat{\epsilon}(\theta)$ is nothing more than a scaled gradient of the current parameters $\theta$. The sharpness-aware gradient is then defined as $\nabla_{\theta}\mathcal{L}_\mathcal{D}(\theta)|_{\theta + \hat{\epsilon}(\theta)}$ and used to update the model as 
\begin{equation}
\theta_{t+1} \leftarrow \theta_{t} - \gamma \nabla_{\theta_t}\mathcal{L}_\mathcal{D}(\theta_t)|_{\theta_t + \hat{\epsilon}_t}, \label{math:sam_update}
\end{equation}
where $\gamma$ is an appropriate learning rate and $\hat{\epsilon}_t = \hat{\epsilon}(\theta_t)$. This two-steps procedure is iteratively applied to solve Eq. \ref{math:minmax_sam}. Intuitively, SAM performs a first step of gradient ascent to estimate the point $(\theta_t + \hat{\epsilon}_t)$ at which the loss is approximately maximized and then applies gradient descent at $\theta_t$ using the just computed gradient.

\subsubsection{ASAM} In \cite{kwon2021asam}, the authors point out that sharpness defined in a rigid region with a fixed radius $\rho$ (Eq. \ref{math:sharpness}) is sensitive to parameter re-scaling, negatively affecting the connection between sharpness and generalization gap. If $A$ is a scaling operator acting on the parameters space without changing the loss function,
two neural networks with weights $\theta$ and $A\theta$ can have different values of sharpness while maintaining the same generalization gap, \textit{i.e.} the sharpness is \textit{scale-dependent}. As a solution, they introduce the concept of \textit{adaptive} sharpness, defined as  \begin{equation}
    \max_{||T_{\theta}^{-1}\epsilon||_p \leq \rho}  \mathcal{L}_\mathcal{D}(\theta+\epsilon) - \mathcal{L}_\mathcal{D}(\theta)
    \label{math:adaptive_sharpness}
\end{equation}
where $T_{\theta}^{-1}$ is the normalization operator of $\theta$ such that $T_{A\theta}^{-1}A = T_{\theta}^{-1}$. Eq. \ref{math:minmax_sam} can be rewritten to define the Adaptive Sharpness-Aware Minimization (ASAM) problem as follows:
\begin{equation}
    \min_{\theta\in\mathbb{R}^d}\max_{||T_{\theta}^{-1}\epsilon||_p \leq \rho}  \mathcal{L}_\mathcal{D}(\theta+\epsilon) + \lambda ||\theta||_2^2
    \label{math:asam}
\end{equation}
For improving stability, $T_{\theta}$ is substituted by $T_{\theta} + \eta I_w$, where $\eta>0$ is a hyper-parameter controlling the trade-off between stability and adaptivity, while $w$ is the number of weight parameters of the model.

\subsection{Stochastic Weight Averaging: Overview}
SWA averages weights proposed by SGD, while using a learning rate schedule to explore regions of the weight space corresponding to high performing networks. At each step $i$ of a cycle of length $c$, the learning rate is decreased from $\gamma_1$ to $\gamma_2$:
\begin{equation}
    \gamma(i) = \big(1-t(i)\big) \gamma_1 + t(i) \gamma_2, \quad t(i) = \frac{1}{c}\big(mod(i-1,c)+1\big) 
    \label{math:lr_swa}
\end{equation}
If $c=1$ the learning rate is constant ($\gamma_1$), otherwise for $c>1$ the learning schedule is cyclical. Starting from a pre-trained model $f_{\hat{\theta}}$, SWA captures all the updates $\theta$ at the end of each cycle and averages them as:
\begin{equation}
    \theta_{\text{SWA}} \leftarrow \frac{\theta_{\text{SWA}}\cdot n_{\text{models}} + \theta}{n_{\text{models}}+1}
    \label{math:swa}
\end{equation}
obtaining the final model $f_{\theta_{\text{SWA}}}$, where $n_\text{models}$ keeps track of the number of completed cycles.

In our method, SWA is applied on the server-side to make the learning process more robust. 
Adapting the scenario of \cite{izmailov2018averaging} to FL, from $75\%$ of the training onwards, the server keeps two models, $f_\theta$ and $f_{\theta_{\text{\swa}}}$ ($f$ and $f_{\text{\swa}}$ to simplify the notation). $f$ follows the standard FedAvg paradigm, while $f_{\text{\swa}}$ is updated every $c$ rounds (Eq. \ref{math:swa}). At each round, the cycling learning rate is computed (Eq. \ref{math:lr_swa}) and used for the clients' local training.

\subsection{Mixup and Cutout: Overview}
Mixup and Cutout are recent methods for data augmentation, aiming to improve the learned models' generalization. We apply one of the two in the client-side training.

\subsubsection{mixup} \cite{zhang2017mixup} trains the neural network on convex combinations of images and their labels, exploiting the prior knowledge that linear interpolation of features leads to linear interpolations of their corresponding targets. Given two input images $(x_i,x_j)$ and their corresponding one-hot label encodings $(y_i,y_j)$ drawn from the $k$-th client's training data $\mathcal{D}_k$, virtual training examples are constructed as follows:
\begin{equation}
\begin{split}
    \Bar{x} = \lambda x_i + (1-\lambda) x_j\\
    \Bar{y} = \lambda y_i + (1-\lambda) y_j
\end{split}
\end{equation}
with $\lambda\sim \text{Beta}(\alpha,\alpha)$ for $\alpha\in(0,\infty)$. 
\subsubsection{Cutout} \cite{devries2017improved} regularizes learning by randomly masking out square regions of the input during training. At the implementation level, this corresponds to applying a fixed-size zero-mask to a random location of the image. 

\section{Training in Heterogeneous Scenarios - Additional Material}
\begin{figure}[t]
    \centering
    \subfloat[][]{\includegraphics[width=.4\linewidth]{images/classifier.4_auto_scale_noname.pdf}}
    \subfloat[][]{\includegraphics[width=.4\linewidth]{images/classifier.4_auto_scale.pdf}}
    \caption{\textsc{Cifar100}. L2-norm of global classifier output features as rounds pass, after receiving as input each client's local data. \textbf{(a)} with $\alpha=0$, the model tends to focus on a different client's distribution, \textit{i.e.} on a single class, at each round. \textbf{(b)} when $\alpha=1000$, the model gives the same attention to each distribution.}
    \label{fig:clf_feats}
\end{figure}
In this section, we provide further analysis of the model's behavior in heterogeneous and homogeneous federated scenarios. As explained in Sec. \ref{sec:het_fl}, the model trained under a condition of statistical heterogeneity is subject to oscillations and loss in performance and generalization. Fluctuations in model predictions can also be noted by looking at its output features, defined as $f_\theta(x) \: \forall x\in\mathcal{X}$. Fig. \ref{fig:clf_feats} shows the L2-norm of the output features computed using the current global model $f_{\theta}^t \: \forall t\in[T]$, given as input the local clients' data $\mathcal{D}_k \: \forall k\in[K]$, where a higher norm value corresponds to greater attention paid to that class by the network. The uniformity of the features obtained in the homogeneous setting contrasts with the chaotic distribution of the ones resulting when $\alpha=0$, which significantly vary over time without following a constant trend.

\section{Experiments Details}
\label{app:exps}
Here we provide a detailed description of the datasets and models used in the paper, together with information regarding the chosen hyper-parameters and their fine-tuning intervals. All results presented in both the main text and the Appendix are averaged over the last 100 rounds for increased robustness and reliability. Unless otherwise specified, the framework is PyTorch \cite{NEURIPS2019_9015} and experiments were run on one NVIDIA GeForce GTX 1070.

\subsection{Datasets and Models}
 Table \ref{tab:stats} summarizes the tasks and the statistics of the number of clients and examples for each dataset.

\begin{table}[!t]\centering
\caption{Datasets statistics}\label{tab:stats}
\scriptsize
    \begin{tabular}{lccccc}
    \toprule
    Dataset & Task & Train clients & Size imbalance & Train samples & Test samples\\\midrule
    \textsc{Cifar10} &  Classification  & 100 & \ding{55} & 50,000 & 10,000\\
    \textsc{Cifar100} & Classification & 100 & \ding{55} & 50,000 & 10,000\\
    \textsc{Cifar100-Pam} &  Classification  &500 & \ding{55} & 50,000 & 10,000\\
    \textsc{Cifar10-C} & DG & - & - & - & 10,000\\
    \textsc{Cifar100-C} & DG & - & - & - & 10,000\\
    \textsc{Landmarks-User-160k} &  Classification  & 1,262 &  \ding{51} & 164,172 & 19,526\\
    \textsc{Cityscapes} (uniform) & SS & 146 &  \ding{51} & \multirow{2}{*}{2,975} & \multirow{2}{*}{500}\\
    \textsc{Cityscapes} (heterogeneous) &  SS & 144 & \ding{51}& & \\
    \textsc{Idda} (country) & SS+DG & 90 &  \ding{55} &  4,320 &1,920\\
    \textsc{Idda} (rainy) & SS+DG & 69 &  \ding{55} &  3,312 &2,928\\
    \bottomrule
    \end{tabular}
\end{table}

\subsubsection{CIFAR10 and CIFAR100} We replicate the federated version of the \textsc{Cifar} datasets proposed by \cite{hsu2019measuring}. Each dataset is split among 100 clients, receiving 500 images each according to the latent Dirichlet distribution (LDA) applied to the labels. The client's examples are selected following a multinomial distribution drawn from a symmetric Dirichlet distribution with parameter $\alpha$. The higher the value of $\alpha$ the larger the number of classes locally seen , \textit{i.e.} the more similar and homogeneous the clients' distributions are. We test $\alpha\in\{0,0.05,100\}$ on \textsc{Cifar10} and $\alpha\in\{0,0.5,1000\}$ on \textsc{Cifar100}. The task is image classification on 10 (\textsc{Cifar10}) and 100 (\textsc{Cifar100}) classes. 
\paragraph{Model:} We train a Convolutional Neural Network (CNN) similar to LeNet5 \cite{lecun1998gradient} on both datasets, following the setting of \cite{hsu2020federated}. The network has two 64-channels convolutional layers with kernel of size $5\times5$, each followed by a $2\times2$ max-pooling layer, ended by two fully connected layers with 384 and 192 channels respectively and a linear classifier. 
\paragraph{Data pre-processing:} The $32\times32$ input images are pre-processed following the standard pipeline: the training images are randomly cropped applying padding 4 with final size $32\times32$, randomly horizontally flipped with probability 0.5 and finally the pixel values are normalized with the dataset's mean and standard deviation; normalization is applied to test images as well.

\subsubsection{CIFAR100-PAM} We further extend our experiments to a more complex version of \textsc{Cifar100}, \textit{i.e.} \textsc{Cifar100-Pam} proposed by \cite{reddi2020adaptive}, reflecting the ``coarse" and ``fine" label structure of the dataset for a more realistic partition. The dataset is split among 500 clients - with 100 images each - following the Pachinko Allocation Method (PAM) \cite{li2006pachinko}, on the result of which LDA is applied. 
\paragraph{Model:} We train a modified ResNet18, replacing Batch Normalization~\cite{ioffe2015batch} layers with group normalization (GN) ones \cite{wu2018group}, as suggested by \cite{hsieh2020non}. We use two groups for each GN layer. Experiments have been run using \texttt{FedJAX}~\cite{ro2021fedjax} on a cluster with NVIDIA V100 GPUs.

\paragraph{Data pre-processing:} \textsc{Cifar100-Pam} images are pre-processed as the \textsc{Cifar} LDA versions described above.

\subsubsection{CIFAR10-C and CIFAR100-C} are the corrupted versions of the \textsc{Cifar} datasets. They are part of the benchmark proposed by \cite{hendrycks2019benchmarking}, used for testing the image classifiers' robustness. The $10k$ images-test set is modified according to a given \textit{corruption} and a corresponding level of \textit{severity}. There are 19 possible corruptions (brightness, contrast, elastic blur, elastic transform, fog, frost, Gaussian blur, Gaussian noise, glass blur, impulse noise, JPEG compression, motion blur, pixelate, saturate, short noise, snow, spatter, speckle noise, zoom blur), while the severity ranges from 1 (low) to 5 (high). 
\paragraph{Model:} The same model described for \textsc{Cifar10} and \textsc{Cifar100} is used here. To test the generalization ability of our method, we test the model trained with \textsc{Cifar10/100} on the corresponding corrupted dataset.
 
\subsubsection{Landmarks-User-160k} Introduced by \cite{hsu2020federated}, the Landmarks-User-160k dataset comprises 164,172 training images belonging to 2,028 landmarks. The dataset is created according to the authorship information from the large-scale dataset Google Landmarks v2 (GLv2) \cite{weyand2020google}. Each author owns at least 30 pictures depicting $5$ or more landmarks, while each location is depicted by at least 30 images and was visited by no less than $10$ users. The authors in the test set do not overlap with the ones appearing in the training split. 
\paragraph{Model:} 
We follow a setting similar to the one proposed by~\cite{hsu2020federated} and use a MobileNetV2~\cite{sandler2018mobilenetv2} network pre-trained on ImageNet~\cite{deng2009imagenet} with with GroupNorm layers in place of BatchNorm. Since no details on the model are available, we set the network feature multiplier $\alpha=1$ and use $8$ groups for the GN layers. We did not apply a bottleneck layer before the classifier as specified in~\cite{hsu2020federated}.
To reduce training time, we use \texttt{Flax}~\cite{flax2020github} for both pre-training and centralized baselines, and \texttt{FedJAX}~\cite{ro2021fedjax} for the implementation of the federated algorithms. Both libraries are based on \texttt{JAX}~\cite{jax2018github} and allow for efficient data parallelization. Implementation of the MobileNetV2 backbone used for all the experiments is available here\footnote{\url{https://github.com/rwightman/efficientnet-jax/tree/a65811fbf63cb90b9ad0724792040ce93b749303}}. All large-scale classification experiments have been performed using an NVIDIA DGX A100 40GB. 

The model trained on ImageNet reaches $\approx 68\%$ top-$1$ accuracy on the validation set. In our experience, GroupNorm tends to perform slightly worse than BatchNorm when trained on ImageNet. However, since we did not extensively tune the hyper-parameters, getting better final performance is possible. For the ImageNet training, we used $8$ GPUs with a total batch size of $2048$ images.

\paragraph{Data pre-processing:} We applied the same data augmentation for training the model on ImageNet and fine-tuning on GLv2: we crop and resize the input images to $224\times224$ with random scale and aspect ratio as described in \cite{szegedy2015going}. 
The data augmentation pipeline used for the experiments can be found here\footnote{\url{https://github.com/google/flax/blob/571018d16b42ce0a0387515e96ba07130cbf79b9/examples/imagenet/input_pipeline.py}}.
We also adapted the GLv2 TensorFlow Federated data pipeline\footnote{
\url{https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/gldv2/load_data}} to be compatible with \texttt{FedJAX}.

\subsubsection{Cityscapes} \cite{cordts2016cityscapes} is a popular dataset for Semantic Segmentation and contains 2,975 real photos taken in the streets of 50 different cities under good weather conditions. Annotations are provided for 19 semantic classes. We refer to the federated splits proposed in the FedDrive benchmark \cite{fantauzzo2022feddrive}. The \textit{uniform} version of the dataset randomly assigns each image to one of the 146 users. In order to account for the distribution heterogeneity appearing in real-world scenarios, an ulterior version is proposed, referred to as \textit{heterogeneous}: every client only accesses images from one of the 18 training cities. In both cases, the test set contains pictures of unseen cities.
\paragraph{Model:} As proposed by the authors of FedDrive, we employ the lightweight network BiSeNetv2 \cite{yu2021bisenet} for training, accounting for possible lower computational capabilities of the edge devices. 
\paragraph{Data pre-processing:} The images are randomly scaled in the range (0.5, 1.5) and cropped to a $512\times1024$ shape. 
 
\subsubsection{IDDA} \cite{alberti2020idda} is a synthetic dataset for semantic segmentation, specific for the field of autonomous driving. In addition to the annotations for 16 semantic classes, the driving conditions are further characterized by three axes: a city among the 7 available, ranging from Urban to Rural environments; one of 5 viewpoints, simulating different vehicles; an atmospheric condition among 3 possible choices (Noon, Sunset, Rainy), for a total of 105\textit{ domains}. As done for Cityscapes, we refer to FedDrive \cite{fantauzzo2022feddrive} for the federated splits. In the \textit{uniform} distribution of IDDA, each client has access to 48 images randomly drawn from the whole dataset. The \textit{heterogeneous} version is built so that every user only sees a single domain. Two distinct testing scenarios are proposed to assess the generalization abilities of the learned model: one with images belonging to domains likely already seen at training time (``seen'' in Table \ref{tab:ss_idda2} of the main text) and another one containing a never-seen one (``unseen''). The unseen domain either contains images taken in the countryside (``country'') to analyze the \textit{semantic} shift or in rainy conditions (``rainy'') for studying the shift in \textit{appearance}.

\paragraph{Model:} As done for Cityscapes, BiSeNetv2 is the model of choice.

\paragraph{Data pre-processing:} The images are randomly scaled in the range (0.5, 2.0) and cropped to a $512\times928$ shape. 

\subsection{Hyper-parameters Tuning}
We consider a different hyper-parameters setup for each dataset. The final choices of training hyper-parameters are summarized in Table \ref{tab:best_params}. Table \ref{tab:abl_sam} and \ref{tab:params_swa} respectively show the values used for \sam/\asam and \swa.

\begin{table}[t]\centering
\caption{Best performing training parameters}\label{tab:best_params}
\scriptsize
    \begin{tabular}{lccccccc}
    \toprule
    \multirow{2}{*}{Dataset} & Client &\multirow{2}{*}{Batch size} & \multirow{2}{*}{Weight decay} & \multirow{2}{*}{Epochs} & Client &\multirow{2}{*}{Rounds} & Clients\\
    & learning rate& & &&momentum& & per round\\
    \midrule
    \textsc{Cifar10} &  0.01 & 64 & $4\cdot10^{-4}$ & 1 & 0 &$10k$ & $\{5,10,20\}$ \\
    \textsc{Cifar100} & 0.01 & 64 & $4\cdot10^{-4}$ & 1 & 0 &$20k$ & $\{5,10,20\}$\\
    \textsc{Cifar100-Pam} & 0.01 &20 &$4\cdot10^{-4}$ &1-2 &0.9 &$10k$ & $\{10,20\}$\\
    \textsc{Landmarks-User-160k} & 0.1 &64 &$4\cdot10^{-5}$ &5 &0 &$5k$ &10 \\
    \textsc{Cityscapes} (unif.) & 0.05& 8& $5\cdot10^{-4}$& 2& 0.9&$1.5k$ & 5\\
    \textsc{Cityscapes} (het.) &  0.05& 8& $5\cdot10^{-4}$& 2&0.9& $1.5k$ & 5\\
    \textsc{Idda} (country) & 0.1& 8& 0&2 &0.9& $1.5k$ & 5\\
    \textsc{Idda} (rainy) & 0.1& 8& 0&2 &0.9& $1.5k$ & 5\\
    \bottomrule
    \end{tabular}
\end{table}

\subsubsection{CIFAR10 and CIFAR100} For both datasets, the training hyper-parameters follow the choice of \cite{hsu2020federated}. The client learning rate is tuned between the values $\{0.01, 0.1\}$ and set to 0.01, the batch size is 64, $E\in\{1,2\}$ is tested for the number of local epochs and the former is chosen. As for the weight decay the value $4\cdot10^{-4}$ leads to better performances than 0. The local optimizer is SGD with no momentum. No learning rate scheduler is used for simplicity. We optimize the cross-entropy loss. As for the server-side, we compare the behavior of different optimizers (\textit{i.e.} SGD, Adam, AdaGrad) with learning rates in $\{0.001, 0.01, 0.1, 1\}$ (results in Appendix \ref{app:server_optims}), following the setup of \cite{reddi2020adaptive}, and find out that \fedavg, \textit{i.e.} SGD with learning rate 1, is the best choice. When testing \fedavgm, the server-side momentum $\beta=0.9$. As for the other SOTAs, we choose $\mu=0.1$ in \fedprox and $\alpha=0.01$ in \feddyn from $\{0.001,0.01,0.1\}$; in \adabest, we tune $\beta\in\{0.8,0.9\}$ and $\mu\in\{0.01,0.02\}$ and pick $(0.9,0.02)$ for \textsc{Cifar10} and $(0.8,0.02)$ for \textsc{Cifar100}. The training proceeds for $10k$ rounds on \textsc{Cifar10} and $20k$ rounds on \textsc{Cifar100}.

\paragraph{Mixup/Cutout:} Following the setup of \cite{zhang2017mixup}, we fix $\alpha_\text{mixup}=1$, resulting in $\lambda$ uniformly distributed between 0 and 1. As for Cutout instead, we select a cutout size of $16\times16$ pixels for \textsc{Cifar10} and $8\times8$ for \textsc{Cifar100}, as done by \cite{devries2017improved}.  

\paragraph{SAM/ASAM:} The parameter $\rho$ of \sam is searched in $\{0.01,0.02,0.05,0.1,0.2,\\0.5\}$. As for \asam, the value of $\rho$ is tuned in $\{0.05,0.1,0.2,0.5,0.7,1.0,2.0\}$ and $\eta\in\{0.0,0.01,0.1,0.2\}$. The choices made for each dataset and $\alpha$ are shown in Table \ref{tab:abl_sam}. There is no distinction of values as clients vary per round.

\paragraph{SWA:} We test \swa's starting round in $\{5\%, 25\%, 50\%, 75\%\}$ of the rounds budget and as expected \cite{izmailov2018averaging} the best contribution is given if applied from $75\%$ of the training onwards (see Appendix \ref{app:abl}). We set the value of the learning rate $\gamma_1$ to 0.01 and test $\gamma_2\in \{10^{-5}, 10^{-4}, 10^{-3}\}$, selecting $\gamma_2=10^{-4}$. The cycle length $c$ is tested in $\{5,10,20\}$ and set to $10$ for \textsc{Cifar10} and $20$ for \textsc{Cifar100}. Table \ref{tab:params_swa} summarizes the choices. 

\begin{table}[t]\centering
\caption{\fedsam and \fedasam hyper-parameters}\label{tab:abl_sam}
\scriptsize
\setlength\tabcolsep{0.25cm}
    \begin{tabular}{llcccc}
    \toprule
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{Distribution}&\sam & \multicolumn{2}{c}{\asam}\\
    \cmidrule(l){3-3} \cmidrule(l){4-5}
    && $\rho$ & $\rho$ & $\eta$\\
    \midrule
    \multirow{3}{*}{\textsc{Cifar10}} & $\alpha=0$&0.1&0.7&0.2\\
    & $\alpha=0.05$&0.1&0.7&0.2\\
    & $\alpha=100$&0.02&0.05&0.2\\\multirow{3}{*}{\textsc{Cifar100}} & $\alpha=0$&0.02&0.5&0.2\\
    & $\alpha=0.5$&0.05&0.5&0.2\\
    & $\alpha=1000$&0.05&0.5&0.2\\\textsc{Cifar100-Pam} &$\alpha=0.1$& 0.05&0.5&0/0.2\\
    \textsc{Landmarks-User-160k} &-& 0.05&0.5&0/0.2\\
    \textsc{Cityscapes} & het/unif& 0.01&0.1&0.2\\
    \textsc{Idda} & het/unif&0.01&0.5&0.2\\
    \bottomrule
    \end{tabular}
\end{table}

\subsubsection{CIFAR100-PAM} The hyper-parameters follow the same choice of~\cite{reddi2020adaptive} (see Table~\ref{tab:best_params}). We report accuracy at $5K$ and $10K$ communication rounds.

\paragraph{Mixup/Cutout:} Same as \textsc{Cifar100}.

\paragraph{SAM/ASAM:} We search hyperpameters in the same values as \textsc{Cifar100}. For $\rho$ we found $0.05$ and $0.5$ to be the best values respectively for \sam and \asam in all configurations. For \asam we found that $\eta=0.2$ is working fine when cutout or no augmentations are applied, while $\eta=0$ works best in the case of Mixup.

\paragraph{SWA:} Same as \textsc{Cifar100}.

\subsubsection{Landmarks-User-160k} We start from the hyper-parameters proposed by~\cite{hsu2020federated}. In contrast with the original paper, we found that \fedavgm with momentum $\beta=0.9$ is unstable with $10$ participating clients and requires reducing the server learning rate to $0.1$ to train the model. Better performance and faster convergence can be obtained with $50$ clients per round and $\beta=0.9$. However, we use $10$ clients per round and \fedavg as the baseline because of our limited resources and to maintain consistency with other experiments. All hyper-parameters are described in Table~\ref{tab:best_params}.

\paragraph{SAM/ASAM:} The parameter $\rho$ of \sam is searched in $\{0.01,0.05,0.1\}$. As for \asam, the value of $\rho$ is tuned in $\{0.1,0.3,0.5\}$ and $\eta\in\{0.0,0.1,0.2\}$.

\paragraph{SWA:} We tested both \swa starting at the 75\% and 100\% of training, \textit{i.e.} the $3750$-\textit{th} and $5000$-\textit{th} rounds. We tested different combinations of cycle lengths $c \in \{5, 10, 20\}$ and learning rate $\gamma_2 \in \{ 10^{-2}, 10^{-3} ,10^{-4}\}$. The best performing learning rates $(\gamma_1,\gamma_2)$ are respectively $(10^{-1},10^{-3})$ and the cycle length is $5$.

\subsubsection{Cityscapes and IDDA} For both Cityscapes and IDDA, we maintain the choice of hyper-parameters of \cite{fantauzzo2022feddrive}. The clients' initial learning rate is 0.05 on Cityscapes and 0.1 on IDDA, the weight decay is $5\cdot10^{-4}$ on Cityscapes, while it is not used on IDDA, 2 local epochs, the client optimizer is SGD with momentum 0.9. Differently from \cite{fantauzzo2022feddrive}, we do not use mixed precision, thus the batch size is reduced from 16 to 8. A polynomial learning rate scheduler is applied locally, following \cite{yu2021bisenet}. The optimization is based on the Online Hard-Negative Mining \cite{shrivastava2016training}, which selects the 25\% of the pixels having the highest cross-entropy loss. The training is spanned across $1.5k$ rounds. 

\paragraph{SAM/ASAM:} The parameter $\rho$ of \sam is searched in $\{0.01,0.05,0.1\}$. As for \asam, the value of $\rho$ is tuned in the set $\{0.05,0.1,0.5\}$ and $\eta\in\{0.0,0.1,0.2\}$.

\paragraph{SWA:} Following the setup established for the \textsc{Cifar} datasets, \swa starts at the 75\% of training, \textit{i.e.} the 1125\textit{th} round. The learning rates $(\gamma_1,\gamma_2)$ are respectively $(10^{-1},10^{-3})$ for IDDA and $(5\cdot10^{-2}, 5\cdot10^{-4})$ for Cityscapes. The cycle length is 5 for both datasets.

\begin{table}[t]\centering
\caption{\swa hyper-parameters}\label{tab:params_swa}
\scriptsize
\setlength\tabcolsep{0.25cm}
    \begin{tabular}{lcccc}
    \toprule
    Dataset & $c$ & $\gamma_1$ & $\gamma_2$ & Start round\\
    \midrule
    \textsc{Cifar10} & 10 & $10^{-2}$ & $10^{-4}$ & 7500\\
    \textsc{Cifar100} & 20 & $10^{-2}$ & $10^{-4}$ & 15000\\
    \textsc{Cifar100-Pam} & 5 & $10^{-2}$ & $10^{-4}$ & 15000\\
    \textsc{Landmarks-User-160k} & 5 & $10^{-1}$ & $10^{-3}$ & 3750/5000\\
    \textsc{Cityscapes} & 5&$5\cdot10^{-2}$& $5\cdot10^{-4}$ & 1125\\
    \textsc{Idda} &5& $10^{-1}$ &$10^{-3}$& 1125\\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Plotting the Loss Landscapes}
In the main text, we introduced both 2-D (Fig. \ref{fig:fedavg_convergence} of the main text) and 3-D plots of the loss landscapes (Fig. \ref{fig:loss_landscape} of the main text). Implementation details follow.

\subsubsection{2D Loss Landscape} Following the indications of \cite{garipov2018loss,mirzadeh2020linear}:
\begin{enumerate}
    \item We choose three weight vectors $\theta_1, \theta_2, \theta_3$ and use them to obtain two basis vectors $\vec u = (\theta_2-\theta_1)$ and $\vec{v} = (\theta_3-\theta_1) - \frac{\langle\theta_3-\theta_1, \theta_2-\theta_1\rangle}{||\theta_2-\theta_1||^2}\cdot(\theta_2-\theta_1)$.
    \item Then, the normalized vectors $\hat{u}=\nicefrac{u}{||u||}$ and $\hat{v}=\nicefrac{v}{||v||}$ form an orthonormal basis in the plain containing $\theta_1, \theta_2, \theta_3$. 
    \item We now define a Cartesian grid of $N\times N$ points in the basis $\hat{u},\hat{v}$. In our case, $N=21$. 
    \item For each point of the grid, the corresponding weights are computed and the loss is consequently evaluated with the resulting network. For each point $P$ of the grid having coordinates $(x,y)$, the corresponding weights are computed as $P = \theta_1 + x \cdot \hat{u} + y \cdot \hat{v}$. As a consequence, $\theta_1$ is the reference and can be found in the origin $(0,0)$.
\end{enumerate}
We adapted the code of \cite{garipov2018loss}\footnote{\url{https://github.com/timgaripov/dnn-mode-connectivity}} to our scenario.

\subsubsection{3D Loss Landscape} The plots in Fig. \ref{fig:loss_landscape} in the main text are generated using the code of \cite{visualloss}\footnote{\url{https://github.com/tomgoldstein/loss-landscape}}, modified to fit our datasets and models. Given a network architecture and its pre-trained parameters, the loss surface is computed along random directions near the optimal parameters.  

\subsection{Computing Hessian Eigenvalues}
We refer to \cite{hessian-eigenthings} for computing both the local and the top 50 Hessian eigenvalues (Figs. \ref{fig:clients_eigs},{\color{red}{5}}
in the main text) with Stochastic Power Iteration method \cite{xu2018accelerated} with maximum 20 iterations per run.

\section{Results on Corrupted CIFAR10 and CIFAR100}
\label{app:corr}
In Fig. \ref{fig:cifar-c-supp}, we compare the performance obtained by \fedavg, \fedsam, \fedasam, \fedavg+ \swa, \fedsam+ \swa and \fedasam+ \swa on \textsc{Cifar10-C} and \textsc{Cifar100-C} as $\alpha$ varies. All results tell us that \asam (alone or combined with \swa) is the algorithm with the best generalization capabilities, as already seen in Sec. \ref{sec:dg} of the main text. 
\captionsetup[subfloat]{font=scriptsize,labelformat=empty}
\begin{figure}[!t]
    \centering
    \subfloat[][\textsc{Cifar10-C} $\alpha=0$]{\includegraphics[width=.49\linewidth]{images/cifar10_alpha0_k20_swa_all.pdf}}
    \subfloat[][\textsc{Cifar100-C} $\alpha=0$]{\includegraphics[width=.49\linewidth]{images/cifar100_alpha0_k20_swa_all.pdf}}\\
    \subfloat[][\textsc{Cifar10-C} $\alpha=0.5$]{\includegraphics[width=.49\linewidth]{images/cifar10_alpha0.05_k20_swa_all.pdf}}
    \subfloat[][\textsc{Cifar100-C} $\alpha=0.5$]{\includegraphics[width=.49\linewidth]{images/cifar100_alpha05_k20_swa_all.pdf}}\\
    \subfloat[][\textsc{Cifar10-C} $\alpha=100$]{\includegraphics[width=.49\linewidth]{images/cifar10_alpha100_k20_swa_all.pdf}}
    \subfloat[][\textsc{Cifar100-C} $\alpha=1000$]{\includegraphics[width=.49\linewidth]{images/cifar100_alpha1000_k20_swa_all.pdf}}
    \caption{\footnotesize{Domain generalization in FL. Results with 20 clients, severity level 5 on \textsc{Cifar10-C} and \textsc{Cifar100-C}.}}
    \label{fig:cifar-c-supp}
    \vspace{-0.5cm}
\end{figure}

\section{Ablation Studies}
\label{app:abl}
In this Section, we present our ablation studies on server-side optimizers, \sam, \asam and \swa, moved from the main text due to space constraints.

\subsection{Ablation Study on Server-Side Optimizers}
\label{app:server_optims}
\begin{table}[t]\centering
\caption{Final accuracy (\%) using different server-side optimizers with varying learning rate (LR) on \textsc{Cifar100} @ $20k$ rounds. 5\% clients participation. In bold the best results on both $\alpha=0$ and $\alpha=1k$.}\label{tab:server_optim}
\scriptsize
\setlength\tabcolsep{0.5cm}
    \begin{tabular}{lccc}
    \toprule
    Optimizer & LR & $\alpha=0$ & $\alpha=1k$\\
    \midrule
    \multirow{4}{*}{\texttt{SGD}} & 1 &\textbf{30.25}&\textbf{49.92}\\
    & 0.1 & 14.09 & 40.43\\
    & 0.01 & 2.67&11.35\\
    & 0.001 & 1.20 &1.12\\\midrule
    \multirow{4}{*}{\texttt{Adam}} & 1 & 1.00&{51.73}\\
    & 0.1 & 29.75&51.62\\
    & 0.01 & 13.72&40.12\\
    & 0.001 & 2.60&11.31\\\midrule
    \multirow{4}{*}{\texttt{AdaGrad}} & 1 & 1.00 &1.00\\
    & 0.1 & 1.77 &46.74\\
    & 0.01 & 26.25&51.44\\
    & 0.001 & 9.70&32.01\\
    \bottomrule
    \end{tabular}
\end{table} To choose the best server-side optimizer, we test SGD, Adam and AdaGrad on the heterogeneous ($\alpha=0$) and homogeneous ($\alpha=1k$) versions of \textsc{Cifar100} with 5 clients per round. Following \cite{reddi2020adaptive}, we set $\beta_1=\beta_2=0$ for AdaGrad and $\beta_1=0.9, \beta_2=0.99$ for Adam. As Table \ref{tab:server_optim} shows, SGD with learning rate 1, \textit{i.e.} \fedavg, is certainly the best choice to have acceptable performances both in the homogeneous scenario and above all in the heterogeneous one.

\subsection{Ablation Study on \sam and \asam}
We present here an analysis on the sensitivity of the model to the hyper-parameters $\rho$ and $\eta$ in \asam and $\rho$ in \sam (Fig. \ref{fig:sensitivity}), having as a reference the setting with 5\% clients participation on \textsc{Cifar100}. Regardless of the distribution, we can see that high values of \sam's $\rho$ lead to a fast decline in performance (Fig. \ref{fig:sensitivity}{\color{red}{a}}), meaning that the algorithm handles smaller neighborhoods better. On the other hand, \asam allows us to have more freedom and expand the size of the neighborhood up to the value of $\rho=0.5$ (Fig. \ref{fig:sensitivity}{\color{red}{b}}), index of the greater robustness of the method. In Fig. \ref{fig:sensitivity}{\color{red}{c}}, we notice that the performances improve linearly as $\eta$ increases, where $\eta$ is a hyper-parameter balancing the trade-off between stability and adaptivity.

\captionsetup[subfloat]{font=scriptsize,labelformat=parens}
\begin{figure}[]
    \centering
    \subfloat[][]{\includegraphics[width=.33\linewidth]{images/rho_sensitivity_sam.pdf}}
    \subfloat[][]{\includegraphics[width=.33\linewidth]{images/rho_sensitivity_asam.pdf}}
    \subfloat[][]{\includegraphics[width=.33\linewidth]{images/eta_sensitivity.pdf}}
    \caption{Results on \textsc{Cifar100}, 5\% clients participation. \textbf{(a)} Sensitivity to \sam's parameter $\rho$. \textbf{(b)-(c)} Sensitivity to \asam's parameters $\rho$ (with fixed $\eta=0.2$) and $\eta$ (with fixed $\rho=0.5$) as $\alpha$ varies.}
    \label{fig:sensitivity}
\end{figure}

\subsection{Ablation Study on \swa}
\label{app:abl_swa}
\swa adds two new concepts to the standard federated training: the average of stochastic weights collected along the trajectory of SGD (Eq. \ref{math:swa}) and the cyclical learning rate (Eq. \ref{math:lr_swa}), which decreases from $\gamma_1$ to $\gamma_2$ according to the cycle length $c$, transmitted as additional information to the clients of each round. Our ablation studies aim to understand which of these two components has the greatest impact on the achieved stability and increased model performance. We compare the results obtained by \swa with $c>1$ with those reached when the learning rate is kept constant, \textit{i.e.} $c=1$, and when the server-side average of the collected weights is not applied while maintaining $c>1$, \textit{i.e.} changing only the clients' learning rate cyclically (Table \ref{tab:abl_swa}). We point out that using $c=1$ and not applying the average brings us back to the standard federated setting. We discover that the server-side average gives the major contribution, which helps in stabilizing learning, while the cycle length does not particularly affect the results. Since the best results in the most difficult scenarios (\textit{i.e.} low value of both $\alpha$ and number of participating clients on \textsc{Cifar100}) are reached when $c>1$, we prefer the cyclical learning rate to the constant one in further experiments.

In addition, in Table \ref{tab:abl_start} we report the differences in results when applying \swa from $\{5\%, 25\%, 50\%, 75\%\}$ of the training onwards on \fedavg with 5 clients per round, showing that a longer pre-training of the network leads to the greater effectiveness of this algorithm.

\begin{table}[t]\centering
\caption{\swa ablation study: comparison between cyclical ($c>1$) and constant learning rate ($c=1$) and contribution given by averaging stochastic weights. Highlighted in bold the best result for each combination (Algorithm, $\alpha$, participating clients).}\label{tab:abl_swa}
\scriptsize
    \begin{tabular}{llcccccccccccc}
    \toprule
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{Algorithm} & \multirow{2}{*}{WeightsAvg} & \multirow{2}{*}{$c$} & \multicolumn{3}{c}{$\alpha=0$} & \multicolumn{3}{c}{$\alpha=0.5/0.05$} & \multicolumn{3}{c}{$\alpha=1k/100$}\\
    \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){11-13}
    & & && $5cl$ & $10cl$ & $20cl$ & $5cl$ & $10cl$ & $20cl$ & $5cl$ & $10cl$ & $20cl$\\
    \midrule
    \multirow{12}{*}{\textsc{Cifar100}} & \fedavg & \multirow{3}{*}{\ding{51}} & \multirow{3}{*}{20} & \textbf{39.34 }&39.74  & 39.85 & \textbf{43.90} &   \textbf{44.02}  &42.09 & 50.98&50.87 &50.92\\
    & \fedsam &  &  &  \textbf{39.30} &\textbf{ 39.51} & 39.24 & \textbf{47.96} & \textbf{46.76}  &\textbf{46.47} &{53.90} &53.67 &\textbf{54.36}\\
    & \fedasam & && {{42.01}} & {\textbf{42.64}} & {{41.62}}  &{\textbf{49.17}} &  {\textbf{48.72}} & {{48.27}}& 53.86& {54.79}&54.10\\\cmidrule{2-13}
    & \fedavg & \multirow{3}{*}{\ding{51}} & \multirow{3}{*}{1} & 38.86 & \textbf{39.82} & \textbf{40.19} & 43.86&43.93&\textbf{42.67}&\textbf{51.33}&\textbf{51.05}&\textbf{51.11}\\
    & \fedsam & & & 38.58 & 39.20 & \textbf{39.37}&47.29&46.34&46.40&53.88&\textbf{53.70}&\textbf{54.36}\\
    & \fedasam & & &\textbf{42.50}&42.40&\textbf{41.76}&48.67&48.50&47.95&{54.16}&\textbf{55.07}&{54.19}\\\cmidrule{2-13}
    & \fedavg & \multirow{3}{*}{\ding{55}}& \multirow{3}{*}{20} & 30.68&34.86&37.42&40.34&42.40&41.89&50.06&50.21&50.81\\
    & \fedsam & & & 31.51&35.87&37.81&44.08&45.80&46.43&53.76& 53.46&54.28\\
    & \fedasam & & & 36.85&39.76&41.03&46.34&48.06&\textbf{48.38}&{54.21}&55.06&{54.22}\\\cmidrule{2-13}
    & \fedavg & \multirow{3}{*}{\ding{55}} & \multirow{3}{*}{1} & 30.25 & 36.74 & 38.59 & 40.43 & 41.27 &  42.17 & 49.92 & 50.25& 50.66\\
    & \fedsam & & & 31.04 & 36.93 & 38.56 & 44.73 & 44.84 & 46.05  &\textbf{54.01} & 53.39 &53.97\\
    & \fedasam & & & {36.04} & {39.76} & {40.81} & {45.61} &{46.58} & {47.78}  & {\textbf{54.81}} & 54.97&{\textbf{54.50}}\\\midrule
    \multirow{12}{*}{\textsc{Cifar10}} & \fedavg & \multirow{3}{*}{\ding{51}} & \multirow{3}{*}{10} & 69.71 & 69.54 & 70.19 & 73.48 &  72.80   &\textbf{73.81} &84.35 & 84.32&84.47\\
    & \fedsam & & & 74.97 &73.73  & 73.06 & {{76.61}} & 75.84  & 76.22&84.23 & 84.37&84.63\\
    & \fedasam & & & {{76.44}} & {\textbf{75.51}} & {{76.36}}  & 76.12 & {{76.16}}  & {76.86}& {{84.88}}&{{84.80}} &{\textbf{84.79}}\\\cmidrule{2-13}
    & \fedavg & \multirow{3}{*}{\ding{51}} & \multirow{3}{*}{1} & \textbf{69.88}&\textbf{69.83}&\textbf{70.72}&\textbf{73.91}&\textbf{73.12}&73.07&\textbf{84.90}&84.47&\textbf{84.67}\\
    & \fedsam & & & \textbf{75.17}&\textbf{74.00}&\textbf{73.53}&\textbf{76.93}&\textbf{76.06}&\textbf{76.55}&\textbf{84.53}&84.54&84.77\\
    & \fedasam & & & \textbf{76.80}&75.48&\textbf{76.84}&\textbf{76.87}&\textbf{76.30}&\textbf{77.55}&\textbf{85.09}&\textbf{85.06}&84.73\\\cmidrule{2-13}
    & \fedavg & \multirow{3}{*}{\ding{55}} & \multirow{3}{*}{10} & 61.41&63.96&67.39&67.17&69.88&72.19&84.18&84.15&84.45\\
    & \fedsam & & & 70.66&71.14&73.04&73.93&74.96&76.20&84.23&84.40&84.69\\
    & \fedasam & & & 75.07&74.87&76.37&75.37&76.17&77.14&84.68&84.72&84.71\\\cmidrule{2-13}
    & \fedavg & \multirow{3}{*}{\ding{55}} & \multirow{3}{*}{1} &  65.00 & 65.54 & 68.52 & 69.24 & 72.50 & 73.07  &84.46 & \textbf{84.50}& 84.59\\
    & \fedsam & & & 70.16 & 71.09 & 72.90 & 73.52 & 74.81 & 76.04  &84.58 & \textbf{84.67} &{\textbf{84.82}}\\
    & \fedasam & & &{73.66} & {74.10} & {76.09} & {75.61} & {76.22} & {{76.98}}  & {84.77} &{84.72} &84.75\\
    \bottomrule
    \end{tabular}
\end{table}
 
\begin{table}[]\centering
\caption{\swa ablation study: comparison between \swa starting rounds when using \fedavg with 5 clients per round}\label{tab:abl_start}
\scriptsize
\setlength\tabcolsep{0.3cm}
    \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Dataset} &\multirow{2}{*}{$c$} & \multirow{2}{*}{Start round}  &   \multicolumn{3}{c}{Test Accuracy (\%)} \\
    \cmidrule(l){4-6} 
    & & & $\alpha=0$ &  $\alpha=0.5/0.05$ &  $\alpha=1k/100$\\
    \midrule
    \multirow{4}{*}{\textsc{Cifar100}} & \multirow{4}{*}{20} & 1000& 24.53&34.52&49.38\\
    & & 5000&30.66&39.71&\textbf{51.52}\\
    & & 10000&36.21&42.55&51.01\\
    & & 15000&\textbf{39.34}&\textbf{43.90}&50.98\\
    \midrule
    \multirow{4}{*}{\textsc{Cifar10}} & \multirow{4}{*}{10} & 500& 55.57&60.50&79.09\\
    & & 2500&60.34&65.72&81.49\\
    & & 5000&66.22&70.55&83.79\\
    & & 7500&\textbf{69.71}&\textbf{73.48}&\textbf{84.35}\\
    \bottomrule
    \end{tabular}
\end{table}

\section{Tables Omitted in the Main Text}
\subsection{Heterogeneous FL Benefits Even More from Flat Minima - Additional Material}
\label{app:omitted_benefits_sam}
Table \ref{tab:centr2} completes the analysis introduced in Sec. \ref{sec:cifar} regarding the gains obtained in the federated scenario w.r.t. the centralized one. Here we report the results for $\alpha\in\{0,1k\}$. As noted for $\alpha=0$ (Table \ref{tab:centr} in the main text), data augmentations fail in the federated heterogeneous scenarios ($\alpha\in\{0,0.5\}$), but reasonably work in the homogeneous ones. 
\definecolor{rosso}{HTML}{cc0000}
\definecolor{verde}{HTML}{279738}
\setlength{\tabcolsep}{4pt}
\begin{table}[h]
\begin{center}
\caption{\footnotesize{Comparison of improvements (\%) in centralized and federated scenarios ($\alpha\in\{0.5,1k\}$, 5 clients) on \textsc{Cifar100}, computed w.r.t. the reference at the bottom}}
\label{tab:centr2}
\tiny
\begin{tabular}{lccccccccc}
\toprule\noalign{\smallskip}
\multirow{2}{*}{Algorithm} & \multicolumn{3}{c}{Accuracy} & \multicolumn{3}{c}{Absolute Improvement} & \multicolumn{3}{c}{Relative Improvement}\\
\cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} 
 & Centr. & $\alpha=0.5$ & $\alpha=1k$ & Centr. & $\alpha=0.5$ & $\alpha=1k$ & Centr. & $\alpha=0.5$ & $\alpha=1k$\\
\midrule
\sam & 55.22&44.73&54.01&+3.02&{\color{verde}{+4.30}}&{\color{verde}{+4.01}}&+5.79&{\color{verde}{+10.64}}&{\color{verde}{+8.03}}\\
\asam & 55.66&45.61&54.81&+3.46&{\color{verde}{+5.18}}&{\color{verde}{\textbf{+4.89}}}&+6.63&{\color{verde}{+12.81}}&{\color{verde}{\textbf{+9.80}}}\\
\swa&52.72&43.90&50.98&+0.52&{\color{verde}{+3.47}}&{\color{verde}{+1.06}}&+1.00&{\color{verde}{+8.58}}&{\color{verde}{+2.12}}\\
\samswa&55.75&47.96&53.90&+0.55&{\color{verde}{+7.53}}&{\color{verde}{+3.98}}&+1.06&{\color{verde}{+18.63}}&{\color{verde}{+7.97}}\\
\asamswa&55.96&49.17&53.86&+3.76&{\color{verde}{\textbf{+8.74}}}&{\color{verde}{+3.94}}&+7.20&{\color{verde}{\textbf{+21.62}}}&{\color{verde}{+7.89}}\\
\mixup&58.01&35.10&55.34&+5.81&{\color{rosso}{-5.33}}&{+5.42}&+11.13&{\color{rosso}{-13.18}}&{+10.86}\\
\cutout&55.30&37.72&53.48&+3.10&{\color{rosso}{-2.71}}&{\color{verde}{+3.56}}&+5.94&{\color{rosso}{-6.70}}&{\color{verde}{+7.13}}\\
\midrule
\multicolumn{7}{l}{\texttt{Centralized}: \textbf{52.20} - \fedavg $\alpha=0.5$: \textbf{40.43},  $\alpha=1k$: \textbf{49.92}}\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt} 
\subsection{Data Augmentations with CIFAR10}
\label{app:augm}
Here we show the results obtained when applying Mixup and Cutout to \textsc{Cifar10} as the value of $\alpha$, clients participation and algorithm change (Table \ref{tab:augms_cifar10}). As demonstrated for \textsc{Cifar100} (Sec. \ref{sec:cifar}), data augmentations do not improve generalization in a federated context, but on the contrary they seem to inhibit learning, leading to sometimes even worse results than \fedavg. 
\setlength{\tabcolsep}{4pt}
\begin{table}[]
\begin{center}
\caption{\fedavg, \sam, \asam and \swa w/ strong data augmentations (\mixup, \cutout) on \textsc{Cifar10}}
\label{tab:augms_cifar10}
\tiny
\begin{tabular}{llccccccccccc}
\toprule\noalign{\smallskip}
 & \multirow{2}{*}{Algorithm} & \multirow{2}{*}{SWA}&\multirow{2}{*}{Aug} &\multicolumn{3}{c}{$\alpha=0$} &\multicolumn{3}{c}{$\alpha=0.5/0.05$} & \multicolumn{3}{c}{$\alpha=1000/100$}\\
\cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){11-13}
& & &&$5cl$& $10cl$ & $20cl$ & $5cl$& $10cl$ & $20cl$ & $5cl$& $10cl$ & $20cl$\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
\multirow{18}{*}{\rotatebox[origin=c]{90}{\textsc{Cifar10}}}&\fedavg&\ding{55}&\multirow{6}{*}{\rotatebox[origin=c]{90}{\texttt{None}}}&65.00 & 65.54 & 68.52 & 69.24 & 72.50 & 73.07  &84.46 & 84.50& 84.59\\
&\fedsam&\ding{55}&&70.16 & 71.09 & 72.90 & 73.52 & 74.81 & 76.04  &84.58 & 84.67 &\textbf{84.82}\\
&\fedasam&\ding{55}&&73.66 & 74.10 & 76.09 & 75.61 & 76.22 & 76.98  & 84.77 &84.72 &84.75\\
&\fedavg&\ding{51}&&69.71 & 69.54 & 70.19 & 73.48 &  72.80   &73.81 &84.35 & 84.32&84.47\\
&\fedsam&\ding{51}&&74.97 &73.73  & 73.06 & 76.61 & 75.84  & 76.22&84.23 & 84.37&84.63\\
&\fedasam&\ding{51}&&\textbf{76.44} & \textbf{75.51} & \textbf{76.36}  & \textbf{76.12} & \textbf{76.16}  & \textbf{76.86}& \textbf{84.88}&\textbf{84.80} &\textbf{84.79}\\
\cmidrule{2-13}
&\fedavg&\ding{55}&\multirow{6}{*}{\rotatebox[origin=c]{90}{\mixup}}&62.26&63.61&65.54&65.63&68.44&68.21&\textbf{82.38}&\textbf{84.46}&\textbf{83.58}\\
&\fedsam&\ding{55}&&67.35&69.32&69.78&70.34&72.98&72.54&81.88&82.24&82.25\\
&\fedasam&\ding{55}&&70.61&71.31&71.62&72.19&\textbf{72.84}&\textbf{72.72}&82.36&82.75&83.08\\
&\fedavg&\ding{51}&&66.31&66.89&66.26&69.79&69.12&68.80&82.27&82.88&82.67\\
&\fedsam&\ding{51}&&\textbf{72.42}&70.65&69.75&\textbf{73.36}&72.29&72.44&81.04&81.18&81.15\\
&\fedasam&\ding{51}&&72.37&\textbf{72.40}&\textbf{71.89}&72.54&72.36&72.32&81.86&81.70&81.92\\
\cmidrule{2-13}
&\fedavg&\ding{55}&\multirow{6}{*}{\rotatebox[origin=c]{90}{\cutout}}&61.12&64.47&64.20&66.45&69.09&68.99&\textbf{83.77}&83.91&\textbf{84.31}\\
&\fedsam&\ding{55}&&63.69&66.30&67.25&67.66&71.39&70.67&83.03&83.84&83.49\\
&\fedasam&\ding{55}&&68.50&69.26&69.75&69.23&\textbf{71.91}&\textbf{71.28}&83.73&\textbf{84.10}&84.00\\
&\fedavg&\ding{51}&&65.54&65.60&65.79&69.94&69.55&69.63&83.35&83.39&83.64\\
&\fedsam&\ding{51}&&69.40&68.45&67.36&71.36&71.56&70.99&82.61&82.75&82.52\\
&\fedasam&\ding{51}&&\textbf{71.30}&\textbf{71.12}&\textbf{70.91}&\textbf{72.79}&71.76&71.09&83.06&83.31&83.11\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt} 
\section{Figures Omitted in the Main Text}
All plots are best seen in colors.
\paragraph{Convergence plots} As shown in Sec. \ref{sec:cifar}, once combined with \fedavgm - \textit{i.e.} server-side momentum $\beta=0.9$ - \sam and \asam allow to reach convergence even in the most heterogeneous scenarios on both \textsc{Cifar10} and \textsc{Cifar100}. Fig. \ref{fig:convergence} shows the convergence plots of those runs. In addition, Fig. \ref{fig:conv_k5} compares the behavior of \fedavg, \fedsam, \fedasam and their combination with \swa on the most difficult setting, \textit{i.e.} $\alpha=0$ and 5 clients per round on both \textsc{Cifar} datasets, highlighting the stability and the positive gap in performance introduced by \swa. 
\paragraph{Loss Surfaces} Fig. \ref{fig:a05_client_conv} shows the convergence points of three local models trained with $\alpha=0.5$ on the corresponding test error surface, while Fig. \ref{fig:train_loss_plane} displays the train loss surfaces with $\alpha\in\{0,0.5,1000\}$. In addition, in Fig. \ref{fig:convg_algs} we compare the convergence points of \fedavg, \fedsam and \fedasam in the heterogeneous scenarios of \textsc{Cifar100}, \textit{i.e.} $\alpha\in\{0,0.5\}$, proving that \asam reaches the best local minimum. 
\paragraph{Hessian Eigenvalues} The top 50 eigenvalues of the global model trained with $\alpha=0.5$ are showed in Fig. \ref{fig:a05_eigs}. Fig. \ref{fig:clients_eigs_app} shows the complete comparison of the local Hessian eigenvalues partially shown in Sec. \ref{sec:het_fl}, introducing the values of $\lambda_{max}^k \: \forall k \in [K]$ resulting with \sam and $\alpha=0.5$. 

\captionsetup[subfloat]{font=scriptsize,labelformat=empty}
\begin{figure}
    \centering
    \subfloat[][\textsc{Cifar100}]{\includegraphics[width=.5\linewidth]{images/convergence_sam_fedavg_cifar100_a0.pdf}}
    \subfloat[][\textsc{Cifar10}]{\includegraphics[width=.5\linewidth]{images/convergence_asam_fedavg_cifar10_a0.pdf}}
    \caption{Convergence plots with $\alpha=0$, 20 clients. When combining \fedavgm or \fedsam (\textsc{Cifar100})/\fedasam (\textsc{Cifar10}) with \swa, convergence is reached even in the most heterogeneous scenarios. \fedavgmswa applied to \textsc{Cifar10} fails to learn, while adding momentum to \fedasam significantly speeds up training.}
    \label{fig:convergence}
\end{figure}

\begin{figure}
    \centering
    \subfloat[][\textsc{Cifar100}]{\includegraphics[width=.5\linewidth]{images/convergence_cifar100_a0_k5.pdf}}
    \subfloat[][\textsc{Cifar10}]{\includegraphics[width=.5\linewidth]{images/convergence_cifar10_a0_k5.pdf}}
    \caption{Convergence plots with $\alpha=0$, 5 clients, highlighting the positive gap in performance and the stability introduced by \swa (both if applied on \fedavg but especially on \fedasam) in the most difficult setting.}
    \label{fig:conv_k5}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.4\linewidth]{images/a05_test_error_plane.pdf}
    \caption{Test error surface computed on \textsc{Cifar100} using three distinct local models trained with $\alpha=0.5$ for $20k$ rounds.}
    \label{fig:a05_client_conv}
\end{figure}

\captionsetup[subfloat]{font=scriptsize,labelformat=parens}
\begin{figure}[!t]
    \centering
    \subfloat[][]{\includegraphics[width=.33\linewidth]{images/alpha0_clients_train_loss_plane.pdf}}
    \subfloat[][]{\includegraphics[width=.33\linewidth]{images/a05_train_loss_plane.pdf}}
    \subfloat[][]{\includegraphics[width=.33\linewidth]{images/alpha1k_clients_train_loss_plane.pdf}}
    \caption{Train cross-entropy loss surfaces computed with three local models after $20k$ training rounds on \textsc{Cifar100}. \textbf{(a)} $\alpha=0$ \textbf{(b)} $\alpha=0.5$ \textbf{(c)} $\alpha=1000$.}
    \label{fig:train_loss_plane}
\end{figure}

\begin{figure}[!t]
    \centering
    \subfloat[][]{\includegraphics[width=.25\linewidth]{images/algs_alpha0_train_loss_plane.pdf}}
    \subfloat[][]{\includegraphics[width=.25\linewidth]{images/algs_alpha0_test_error_plane.pdf}}
    \subfloat[][]{\includegraphics[width=.25\linewidth]{images/algs_alpha05_train_loss_plane.pdf}}
    \subfloat[][]{\includegraphics[width=.25\linewidth]{images/algs_alpha05_test_error_plane.pdf}}
    \caption{Loss surfaces comparing the convergence points of \fedavg, \fedsam and \fedasam after $20k$ training rounds on \textsc{Cifar100}. The minima reached by \sam and \asam are found within low-loss neighborhoods. \textbf{(a)} Train loss surface $\alpha=0$. \textbf{(b)} Test error surface $\alpha=0$. \textbf{(c)} Train loss surface $\alpha=0.5$. \textbf{(d)} Test error surface $\alpha=0.5$. }
    \label{fig:convg_algs}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=.4\linewidth]{images/eigs_cifar100_alpha05_k5.pdf}
    \caption{Top 50 eigenvalues of the global model with $\alpha=0.5$ on \textsc{Cifar100}.}
    \label{fig:a05_eigs}
\end{figure}

\captionsetup[subfloat]{font=scriptsize,labelformat=empty}
\begin{figure*}[!t]
\begin{minipage}{\columnwidth}
\centering
   \subfloat[][\fedavg $\alpha=0$]{\includegraphics[width=.25\linewidth]{images/eigs_a0_fedavg.png}}
   \subfloat[][\fedsam $\alpha=0$]{\includegraphics[width=.25\linewidth]{images/eigs_a0_sam.png}}
   \subfloat[][\fedasam $\alpha=0$]{\includegraphics[width=.25\linewidth]{images/eigs_a0_asam.png}}\\
    \subfloat[][\fedavg $\alpha=0.5$]{\includegraphics[width=.25\linewidth]{images/eigs_a05_fedavg.png}}
    \subfloat[][\fedsam $\alpha=0.5$]{\includegraphics[width=.25\linewidth]{images/eigs_a05_sam.png}}
    \subfloat[][\fedasam $\alpha=0.5$]{\includegraphics[width=.25\linewidth]{images/eigs_a05_asam.png}}\\
    \subfloat[][\fedavg $\alpha=1k$]{\includegraphics[width=.25\linewidth]{images/eigs_a1k_fedavg.png}}
    \subfloat[][\fedsam $\alpha=1k$]{\includegraphics[width=.25\linewidth]{images/eigs_a1k_sam.png}}
    \subfloat[][\fedasam $\alpha=1k$]{\includegraphics[width=.25\linewidth]{images/eigs_a1k_asam.png}}
    \caption{\footnotesize{Maximum Hessian eigenvalue computed for each client as rounds pass.}}
    \label{fig:clients_eigs_app}
\end{minipage}
\end{figure*}

