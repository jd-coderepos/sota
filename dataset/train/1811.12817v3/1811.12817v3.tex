\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{setspace}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{array}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{color}
\usepackage{placeins}
\usepackage{enumitem}
\usepackage{xcolor,colortbl}
\usepackage{xspace}

\usepackage[font={small}]{caption}
\captionsetup[table]{position=bottom}   

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\definecolor{tablegreen}{rgb}{0.09, 0.45, 0.27}
\definecolor{tablered}{rgb}{0.55, 0.0, 0.0}
\definecolor{lightgray}{rgb}{0.33, 0.41, 0.47}
\newcommand{\name}{L3C\xspace}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\updated}[1]{\textcolor{red}{#1}}
\newcommand{\lossidx}{i}
\newcommand{\lossparts}[1]{F^{(#1)}_\lossidx}
\newcommand{\plog}{p_\text{mix}}
\newcommand{\expectE}{\mathbb{E}}
\newcommand{\expect}[1]{\expectE\left[#1\right]}
\newcommand{\Preal}{\tilde{p}}
\newcommand{\zunquantized}{z'}
\newcommand{\R}{\mathbb{R}}
\newcommand{\levels}{\mathbb{L}}
\newcommand{\level}{\ell}
\newcommand{\sigmaq}{\sigma_q}
\newcommand{\X}{\mathcal{X}}
\newcommand{\nS}{{n_S}}
\newcommand{\Bicubic}{\mathcal{B}}
\newcommand{\prob}[1]{\text{Pr}\lbrack#1\rbrack}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\fskip}[1]{f^{(#1)}_{\text{int}}}
\newcommand{\fs}[1]{f^{(#1)}}
\newcommand{\rotated}[2]{{\multirow{#1}{*}{\fbox{\hphantom{1ex}\rotatebox[origin=c]{90}{\centering #2}}}}}

\newcommand{\pmix}{p_m}
\newcommand{\plogistic}{p_l}
\newcommand{\slower}[2]{\textcolor{tablegreen}{}}
\newcommand{\tablenote}[1]{}
\newcommand{\deftablenote}[1]{}

\newcommand{\raisek}{RAISE-1k\xspace}
\newcommand{\jpegk}{JPEG\kern0.2ex2000\xspace}

\definecolor{Gray}{gray}{0.9}
\newcommand{\cellh}[1]{\cellcolor{Gray}{#1}}


\setlength{\floatsep}{2ex minus 0.5ex}
\setlength{\textfloatsep}{2ex minus 0.5ex}
\setlength{\abovecaptionskip}{0.5ex minus 0.2ex}
\setlength{\belowcaptionskip}{0.5ex minus 0.5ex}


\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\z@}{2ex \@plus 1.25ex \@minus .5ex}{-1em}{\normalfont\normalsize\bfseries}}
\makeatother

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\begin{document}

\title{Practical Full Resolution Learned Lossless Image Compression}

\author{
    \begin{tabular}{ccccc}
        Fabian Mentzer &
        Eirikur Agustsson &
        Michael Tschannen &
        Radu Timofte &
        Luc Van Gool\\{\scriptsize mentzerf@vision.ee.ethz.ch} &
        {\scriptsize aeirikur@vision.ee.ethz.ch} &
        {\scriptsize michaelt@nari.ee.ethz.ch} &
        {\scriptsize timofter@vision.ee.ethz.ch} &
        {\scriptsize vangool@vision.ee.ethz.ch} 
    \end{tabular}\
H(\Preal, p) 
&= \expectE_{j \sim \Preal}\left[-\log p(j)\right] \nonumber \\
    &= -\sum_{j \in \X} \Preal(j) \log p(j) \label{eq:crossent}

    p(x, z^{(1)}, &\ldots, z^{(S)}) = \
where  is a uniform distribution. The feature representations can be hand designed or learned. Specifically, on one side, we consider an RGB pyramid with , where  is the bicubic (spatial) subsampling operator with subsampling factor . On the other side, we consider a learned representation  using a feature extractor . We use the hierarchical model shown in Fig.~\ref{fig:arch} using the composition 
,
where the  are feature extractor blocks and  is a scalar differentiable quantization function (see Sec.~\ref{sec:quantization}). The  in Fig.~\ref{fig:arch} are predictor blocks, and we parametrize  and  as convolutional neural networks. 

Letting ,
we parametrize the conditional distributions  
for all  as

using the predictor features
.\footnote{
            The final predictor only sees , i.e., we let .}
Note that  summarizes the information of . 


The predictor is based on the super-resolution architecture from EDSR~\cite{Lim_2017_CVPR_Workshops}, motivated by the fact that our prediction task is somewhat related to super-resolution in that both are dense prediction tasks involving spatial upsampling. We mirror the predictor to obtain the feature extractor, and follow~\cite{Lim_2017_CVPR_Workshops} in not using BatchNorm~\cite{ioffe2017batch}.
Inspired by the ``atrous spatial pyramid pooling'' from~\cite{chen2017rethinking}, we insert a similar layer at the end of : In , we use three atrous convolutions in parallel, with rates 1, 2, and 4, then concatenate the resulting feature maps to a -dimensional feature map. 



\subsection{Quantization}\label{sec:quantization}


We use the scalar quantization approach proposed in~\cite{mentzer2018cvpr} to quantize the output of : Given levels , we use nearest neighbor assignments to quantize each entry  as

but use differentiable ``soft quantization''

to compute gradients for the backward pass, where  is a hyperparameter relating to the ``softness'' of the quantization. 
For simplicity, we fix  to be  evenly spaced values in . 


\subsection{Mixture Model} \label{sec:mixture_model}

For ease of notation, let  again. We model the conditional distributions

using a generalization of the discretized logistic mixture model with  components proposed in~\cite{Salimans2017pcnnpp}, as it allows for efficient training: 
The alternative of predicting logits per \mbox{(sub-)pixel} has the downsides of requiring more memory, causing sparse gradients (we only get gradients for the logit corresponding to the ground-truth value), and does not model that neighbouring values in the domain of  should have similar probability. 

Let  denote the channel and  the spatial location. For all scales, we assume the entries of  to be independent across , given . 
For RGB (), we define

    where we use a weak autoregression over RGB channels to define the joint probability distribution via a mixture  (dropping the indices  for shorter notation):

    We define  as a mixture of logistic distributions  (defined in Eq.~\eqref{eq:plog} below). To this end, we obtain mixture weights\footnote{Note that in contrast to~\cite{Salimans2017pcnnpp} we do not share mixture weights  across channels. This allows for easier marginalization of Eq.~\eqref{eq:p_joint}.}
    , means , variances , as well as coefficients  from  (see further below), and get

    where we use the conditional dependency on previous  to obtain the updated means , as in~\cite[Sec. 2.2]{Salimans2017pcnnpp},

Note that the autoregression over channels in Eq.~\eqref{eq:p_joint} is only used to update the means  to .

For the other scales (), the formulation only changes in that we use no autoregression at all, i.e.,  for all . 
No conditioning on previous channels is needed, and Eqs.~\eqref{p_x_factorization}-\eqref{eq:p_mix} simplify to


For all scales, the individual logistics  are given as 

Here,  is the bin width of the quantization grid ( for  and  otherwise). The edge-cases  and  occurring for  are handled as described in \cite[Sec. 2.1]{Salimans2017pcnnpp}.

For all scales, we obtain the parameters of  from  with a  convolution that has  output channels (see Fig.~\ref{fig:arch_detail}).
For RGB, this final feature map must contain the three parameters  for each of the 3 RGB channels and  mixtures, as well as  for every mixture, thus requiring   channels. For , , since no  are needed.
With the parameters, we can obtain , which has dimensions  for RGB and  otherwise (visualized with cubes in Fig.~\ref{fig:arch}).


We emphasize that in contrast to~\cite{Salimans2017pcnnpp}, our model is \emph{not} autoregressive over pixels, i.e.,  are modelled as independent across  given  (also for ).




\begin{table*}[ht!]
\centering
\begin{tabular}{ll@{\hskip 6ex}r@{\hskip 1mm}l@{\hskip 8ex}r@{\hskip 1mm}l@{\hskip 8ex}r@{\hskip 1mm}l}
\toprule
        \footnotesize{[bpsp]} & Method & \multicolumn{2}{@{}l}{Open Images} & \multicolumn{2}{@{}l}{DIV2K}  & \multicolumn{2}{@{}l}{\raisek}  \\
\midrule
Ours & L3C & 2.991 & & 3.094 & & 2.387 &\\ \midrule
\multirow{2}{*}{\shortstack[l]{Learned\\Baselines}} & RGB Shared & 4.314 & \textcolor{tablegreen}{\footnotesize } & 4.429 & \textcolor{tablegreen}{\footnotesize } & 3.779 & \textcolor{tablegreen}{\footnotesize }\\
 & RGB & 3.298 & \textcolor{tablegreen}{\footnotesize } & 3.418 & \textcolor{tablegreen}{\footnotesize } & 2.572 & \textcolor{tablegreen}{\footnotesize }\\ \midrule
\multirow{4}{*}{\shortstack[l]{Non-Learned\\Approaches}} & PNG & 4.005 & \textcolor{tablegreen}{\footnotesize } & 4.235 & \textcolor{tablegreen}{\footnotesize } & 3.556 & \textcolor{tablegreen}{\footnotesize }\\
 & JPEG2000  & 3.055 & \textcolor{tablegreen}{\footnotesize } & 3.127 & \textcolor{tablegreen}{\footnotesize } & 2.465 & \textcolor{tablegreen}{\footnotesize }\\
 & WebP & 3.047 & \textcolor{tablegreen}{\footnotesize } & 3.176 & \textcolor{tablegreen}{\footnotesize } & 2.461 & \textcolor{tablegreen}{\footnotesize }\\
 & FLIF & 2.867 & \textcolor{tablered}{\footnotesize } & 2.911 & \textcolor{tablered}{\footnotesize } & 2.084 & \textcolor{tablered}{\footnotesize }\\ \bottomrule
\end{tabular}
\caption{\label{table:results_bpsp}Compression performance of our method (\name) and learned baselines (RGB Shared and RGB) to previous (non-learned) approaches, in bits per sub-pixel (bpsp). We emphasize the difference in percentage to our method for each other method in \emph{\textcolor{tablegreen}{green}} if \name outperforms the other method and in \emph{\textcolor{tablered}{red}} otherwise. \emph{Numbers are updated, see suppl.~\ref{sec:version3} for details.}}
\vspace{-1.5ex}
\end{table*}




\subsection{Loss}

We are now ready to define the loss, which is a generalization of the discrete logistic mixture loss introduced in~\cite{Salimans2017pcnnpp}. Recall from Sec.~\ref{sec:losslesscomp} that our goal is to model the true joint distribution of  and the representations , i.e.,  as accurately as possible using our model . Thereby, the  are defined using the learned feature extractor blocks , and  is a product of discretized (conditional) logistic mixture models with parameters defined through the , which are in turn computed using the learned predictor blocks . As discussed in Sec.~\ref{sec:losslesscomp}, the expected coding cost incurred by coding  w.r.t.\ our model  is the cross entropy .

We therefore directly minimize  w.r.t.\ the parameters of the feature extractor blocks  and predictor blocks  over samples. Specifically, given  training samples , let  be the feature representation of the -th sample.
We minimize
-2ex]
    &\hspace{5em} 
                  \boldsymbol{\cdot} \prod_{s=1}^{S} p
                              \big(\lossparts{s}|\lossparts{s+1}, \ldots, \lossparts{S}\big) 
        \Big) \nonumber\-2ex]
     & \hspace{3.5em}+ \sum_{s=1}^{S}\log p(\lossparts{s}|\lossparts{s+1}, \ldots, \lossparts{S})\Big)\label{eq:coding_cost}.
-0.8ex]
    5.207 bpsp &  \emph{stored: 0,1,2,3} &
    1.209 bpsp &    \emph{stored: 1,2,3} \\
    \multicolumn{2}{l}{\includegraphics[width=0.49\linewidth]{figs/generation/rgb+bn0_0_355}} &
    \multicolumn{2}{r}{\includegraphics[width=0.49\linewidth]{figs/generation/rgb+bn0+bn1_0_121}} \
            \hspace{-2ex}C(x_{1uv} | f^{(1)}) &= \sum_k \pi^k_{1uv} \; C_l(x_{1uv} | \tilde \mu^k_{1uv}, \sigma^k_{1uv}) \nonumber \\
            \hspace{-2ex}C(x_{2uv} | f^{(1)}, x_{1uv}) &= \sum_k \pi^k_{2uv} \; C_l(x_{2uv} | \tilde \mu^k_{2uv}, \sigma^k_{2uv}) \nonumber \\
            \hspace{-2ex}C(x_{3uv} | f^{(1)}, x_{1uv}, x_{2uv}) &= \sum_k \pi^k_{3uv} \; C_l(x_{3uv} | \tilde \mu^k_{3uv}, \sigma^k_{3uv}). \label{eq:C_mix}

            C(z^{(s)}_{cuv} | f^{(s+1)}) = \sum_k \pi^k_{cuv} C_l(z^{(s)}_c | \mu^k_{cuv}, \sigma^k_{cuv}).
            \label{eq:cdf_z_s_c}
         C_l(z | \mu, \sigma) = \sigmoid((z-\mu)/\sigma). 
        For each  the CDF  is a -dimensional matrix, where  for RGB and  otherwise, and . 
\item For each , encode each channel  of  with the predicted , using adaptive arithmetic coding (see Sec.~\ref{sec:losslesscomp}). To be able to uniquely decode, the sub-bitstream for  always starts with a triplet encoding its dimensions  as \textsc{uint16}. The final bitstream is the concatenation of all sub-bitstreams.
\end{enumerate}

\subsubsection*{Decoding}
\begin{enumerate}[leftmargin=*]
    \item Obtain the final  from the bitstream, which was encoded with a uniform prior.
    \item Feed  to  to obtain , and thereby also  for all . Since the decoder now has access to the same CDF as the encoder, we can decode  from the bitstream with our adaptive arithmetic decoder.
    \item Analogously, we repeat the previous step to obtain , as well as  using the accompanying CDFs.
    \item Given , which contains all parameters for the RGB scale (i.e., we know :  as well as , see Sec.~\ref{sec:mixture_model}), we can obtain the CDF for the first channel of  (, red channel), , and decode this first channel from the bitstream. Now we know , and with  we can obtain  via Eq.~\eqref{eq:updating_mus}. With this, we also know the CDF of the next channel, , and can decode  from the bitstream. In the same fashion, we can then obtain , then , and thus .
    \item Concatenating the channels , we finally obtain the decoded image .
\end{enumerate}

\subsubsection{Hardware Used}

Our timings were obtained on a machine with a Titan X (Pascal) GPU and Intel Xeon E5-2680 v3 CPU.


\subsubsection{Notes on Code Optimization} \label{sec:notes_optim}

The encoder can be run in parallel over all scales, as all CDFs are known after one forward pass. Further, we do not need to know the CDF for all symbols, but only for the symbols  we encode and , since this specifies the interval . 
The decoder is sequential in the scales since  is required to predict the distribution of . Still, for , the decoding of the channels of the  could be parallelized, as the channels are modelled fully independently.
However, we did not implement either of these improvements, keeping the code simple.

For both encoder and decoder, the CDFs must be available to the CPU, as the arithmetic coder runs there. However, the CDFs are huge tensors for real-world images ( for RGB, which amounts to MB for each channel of a  image). To save the expensive copying from GPU to CPU, we implemented our own CUDA kernel to store the claculated  directly into ``managed memory'', which can be accessed from both CPU and GPU. However, we did not optimize this CUDA kernel for speed.

Finally, while state-of-the-art adaptive entropy coders typically require on the order of milliseconds per MB (see \cite{duda2015use} and in particular \cite{giesen2014interleaved} for benchmarks on adaptive entropy coding), we implemented a simple arithmetic coding module to obtain the times in our tables.
Please see the code\textsuperscript{\ref{fn:github}} for details.

\newpage



\subsection{Comparison on ImageNet64} \label{sec:imgnet64_cmp}

We show a bpsp comparison on ImageNet64 in Table~\ref{table:results_imgnet64}. Similar to what we observed on ImageNet32 (see Section~\ref{sec:results_pixelcnn}), our outputs are  larger than MS-PixelCNN and  larger than the original PixelCNN, but smaller than all classical approaches. We note again that increase in bitcost is traded against orders of magnitude in speed. 

We also note that the gap between classical approaches and PixelCNN becomes smaller compared to ImageNet32. 

\begin{table}[ht!]
\centering
    \begin{tabular}{lcc} 
        \toprule
        \footnotesize{[bpsp]} & ImageNet64 &   Learned \\
        \midrule
        \name (ours) & 4.42 &  \checkmark \\
        PixelCNN~\cite{van2016pixel} & 3.57 &  \checkmark    \\
        MS-PixelCNN~\cite{reed2017parallel} & 3.70 &  \checkmark  \\
        \midrule
        PNG & 5.74 \\
        \jpegk & 5.07 \\
        WebP & 4.64 \\
        FLIF & 4.54 \\
        \bottomrule
\end{tabular}
    \captionsetup{width=.8\linewidth}
    \caption{\label{table:results_imgnet64}Comparing bits per sub-pixel (bpsp) on the  images from ImageNet64 of our method (\name) vs.\ PixelCNN-based approaches and classical approaches.}
\end{table}



\subsection{Note on Comparing Times for  Images} \label{sec:note_pcnn_cmp}
In Table~\ref{table:times}, we report run times for batch size 30 to be able to compare with the run times reported in~\cite{reed2017parallel}. However, this comparison is biased against us, as can be seen in Table~\ref{table:batchsize_32}: Since our network is fairly small, we can process up to 480 images of size  in parallel. We observe that the time to sample one image drops as the batch size increases, indicating that for BS=30, some overhead dominates.

\begin{table}[h!]
    \centering
    \begin{tabular}{p{0.39\linewidth}l}
        \toprule
        Batch Size & Time per image [s] \\
        \midrule
        30 &  \\
        60 &  \\
        120 &  \\
        240 &  \\
        480 &  \\
        \bottomrule
    \end{tabular}
    \captionsetup{width=.8\linewidth}
    \caption{\label{table:batchsize_32}Effect of varying the batch size.\hfill}
\end{table}



\clearpage


\subsection{Visualizing Representations} \label{sec:visualize_repr}

We visualize the representations  in Fig.~\ref{fig:bn}. It can be seen that the global image structure is preserved over scales, with representations corresponding to smaller  modeling more detail. This shows potential for efficiently performing image understanding tasks on partially decoded images similarly as described in \cite{torfason2018towards} for lossy learned compression: instead of training a feature extractor for a given task on , one could directly use the features  from our network.

\begin{figure}[ht]
\centering
    \includegraphics[width=\linewidth]{figs/bn/bn_3}\vspace{-0.9ex}
    \includegraphics[width=\linewidth]{figs/bn/scheme_hori}\vspace{-3.1ex}
    {\tiny \textcolor{white}{1\hfill25}}
    \caption{\label{fig:bn}Heatmap visualization of the first three channels for each of the representations , each containing values in , as indicated by the scale underneath.}
\end{figure}


\newpage

\subsection{Architectures of Baselines}
Figs.~\ref{fig:arch_rgb_shared},~\ref{fig:arch_rgb} show the architectures for the RGB Shared and RGB baselines. The dots in Fig.~\ref{fig:arch_rgb_shared} indicate that the model could in theory be applied more since  is used for every scale. 


\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{figs/arch_rgb_shared}
    \caption{\label{fig:arch_rgb_shared}Architecture for the RGB Shared baseline. Note that we train only one predictor .}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.8\linewidth]{figs/arch_rgb}
    \caption{\label{fig:arch_rgb}Architecture for the RGB baseline. Multiple predictors are trained.}
\end{figure}


\subsection{Encoding and Decoding Visualized}
We visualize the steps needed to encode the different  in Fig.~\ref{fig:encdecdetails} on the next page.

\begin{figure*}[hb]
\centering
    \vspace{-2em}
    Encoding\hspace{2em} Decoding\\
\includegraphics[width=0.8\textwidth]{figs/encdecdetails}
    \caption{\label{fig:encdecdetails}Visualizing encoding and decoding: At every step, the arithmetic coder (AC) takes a probability distribution and a .} 
\end{figure*}


\end{document}
