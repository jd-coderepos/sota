\documentclass{midl} 



\usepackage{mwe} \usepackage{multirow}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{adjustbox}
\usepackage{xcolor}
\jmlryear{2023}
\jmlrworkshop{Full Paper -- MIDL 2023 submission}


\title[Short Title]{Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation}



\midlauthor{\Name{Md Mostafijur Rahman} \Email{mostafijur.rahman@utexas.edu}\and
  \\ \Name{Radu Marculescu} \Email{radum@utexas.edu}\\
   \addr The University of Texas at Austin}










\begin{document}

\maketitle

\begin{abstract}
Transformers have shown great success in medical image segmentation. However, transformers may exhibit a limited generalization ability due to the underlying single-scale self-attention (SA) mechanism. In this paper, we address this issue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT) backbone network, which improves the generalizability of the model by computing SA at multiple scales. We also incorporate an attention-based decoder, namely Cascaded Attention Decoding (CASCADE), for further refinement of multi-stage features generated by MERIT. Finally, we introduce \textcolor{black}{an effective multi-stage feature mixing loss aggregation (MUTATION) method for better model training via implicit ensembling}. Our experiments on two widely used medical image segmentation benchmarks (i.e., Synapse Multi-organ, ACDC) demonstrate the superior performance of MERIT over state-of-the-art methods. Our MERIT architecture and MUTATION loss aggregation can be used with downstream medical image and semantic segmentation tasks.  
\end{abstract}

\begin{keywords}
Medical image segmentation, Vision transformer, Multi-scale transformer, Feature-mixing augmentation, Self-attention.
\end{keywords}

\section{Introduction}
\label{introduction}
Automatic medical image segmentation has become an important step in disease diagnosis nowadays. Since the emergence of UNet \cite{ronneberger2015u}, U-shaped convolutional neural networks (CNNs) \cite{oktay2018attention, huang2020unet, zhou2018unet++, fan2020pranet} have become de facto methods for medical image segmentation. By producing high-resolution segmentation maps through aggregating multi-stage features via skip connections, UNet variants, such as UNet++ \cite{zhou2018unet++} and UNet3Plus \cite{huang2020unet}, have shown good performance in medical image segmentation. However, the spatial context of the convolution operation limits the CNN-based methods ability to learn the long-range relations among pixels \cite{cao2021swin}. Some works \cite{chen2018reverse, oktay2018attention, fan2020pranet} try to address this issue by embedding attention mechanisms in the encoder or decoder. Despite the significant efforts made in this direction, the CNN-based methods still have insufficient ability to capture long-range dependencies.

With the emergence of Vision transformers \cite{dosovitskiy2020image}, many works \cite{cao2021swin, chen2021transunet, dong2021polyp, wang2022stepwise} try to address the above problem using a transformer encoder, specifically for medical image segmentation. Transformers capture long-range dependencies by learning correlations among all the input patches using self-attention (SA). Recently, hierarchical vision transformers, such as pyramid vision transformer (PVT) \cite{wang2021pyramid} with spatial reduction attention, Swin transformer \cite{liu2021swin} with window-based attention, and MaxViT \cite{tu2022maxvit} with multi-axis attention have been introduced to improve performance. Indeed, these hierarchical vision transformers are very effective for medical image segmentation tasks \cite{cao2021swin, dong2021polyp, wang2022stepwise}. However, these transformer-based architectures have two limitations: 1) self-attention is performed with a single attention window (scale) which has limited feature processing ability, and 2) the self-attention modules used in transformers have limited ability to learn spatial relations among pixels \cite{chu2021conditional}. 



More recently, PVTv2 \cite{wang2022pvt} embeds convolution layers in transformer encoders, while CASCADE \cite{rahman2023medical} introduces an attention-based decoder to address the limitation of learning spatial relations among pixels. Although these methods enable learning, the local (spatial) relations among pixels, they still have limited ability to capture features of multi-scale (e.g., small, large) organs/lesions/objects due to computing self-attention in a single-scale attention window. To address this limitation, we introduce a novel \textit{multi-scale hierarchical} vision transformer (MERIT) backbone which computes \textcolor{black}{self-attention} across \textit{multiple attention windows} to improve the generalizability of the model. We also incorporate multiple CASCADE decoders to produce better high-resolution segmentation maps by effectively aggregating and enhancing multi-scale hierarchical features. Finally, we introduce a novel effective multi-stage (hierarchical) feature-mixing loss aggregation (MUTATION) strategy \textcolor{black}{for implicit ensembling/augmentation} which produces new synthetic predictions by mixing hierarchical prediction maps from the decoder. The aggregated loss from these synthetic predictions improves the performance of medical image segmentation. Our contributions are as follows:
\vspace{-0.3cm} 

\begin{itemize}
  \item \textbf{Novel Network Architecture:} We propose a novel multi-scale hierarchical vision transformer (MERIT) for 2D medical image segmentation which captures both multi-scale and multi-resolution features. Besides, we incorporate a cascaded attention-based decoder for better hierarchical multi-scale feature aggregation and refinement.
\vspace{-0.5cm} \item \textbf{Multi-stage Feature-mixing Loss Aggregation:} We propose a new simple, yet effective way, namely MUTATION, to create synthetic predictions by mixing features during loss calculation; this improves the medical image segmentation performance.
\vspace{-0.1cm} \item \textbf{New State-of-the-art Results:} 
We perform rigorous experiments and ablation studies on two medical image segmentation benchmarks, namely Synapse multi-organ and ACDC cardiac diagnosis. Our implementation of MERIT using two instances (with different windows for SA) of MaxViT \cite{tu2022maxvit} backbone with CASCADE decoder and MUTATION loss aggregation strategy produces new state-of-the-art (SOTA) results on Synapse multi-organ and ACDC segmentation benchmarks.


\end{itemize}

\section{Related Work}
\label{sec:related_work}


\subsection{Vision transformers}



Dosovitskiy et al. \cite{dosovitskiy2020image} build the first vision transformer (ViT), which can learn long-range (global) relations among the pixels through SA. Recent works focus on improving ViT in different ways, such as designing new SA blocks \cite{liu2021swin, tu2022maxvit}, incorporating CNNs \cite{wang2022pvt, tu2022maxvit}, or introducing new architectural designs \cite{wang2021pyramid, xie2021segformer}. Liu et al. \cite{liu2021swin} introduce a sliding window attention mechanism in the hierarchical Swin transformer. In DeiT \cite{touvron2021training}, authors explore data-efficient training strategies to minimize the computational cost for ViT. 
SegFormer \cite{xie2021segformer} proposes a positional-encoding-free hierarchical transformer using Mix-FFN blocks. In PVT, authors \cite{wang2021pyramid} develop a pyramid vision transformer using a spatial reduction attention mechanism. The authors extend the PVT to PVTv2 \cite{wang2022pvt} by embedding an overlapping patch embedding, a linear complexity attention layer, and a convolutional feed-forward network. Recently, in MaxViT \cite{tu2022maxvit}, authors propose a multi-axis self-attention mechanism to build a hierarchical hybrid CNN transformer.

Although vision transformers have shown excellent promise, they have limited spatial information processing ability; also, there is little effort in designing multi-scale transformer backbones \cite{lin2022ds}. In this paper, we address these very limitations by introducing a multi-scale hierarchical vision transformer with attention-based decoding.










\subsection{Medical image segmentation}
Medical image segmentation can be formulated as a dense prediction task of classifying the pixels of lesions or organs in endoscopy, CT, MRI, etc. \cite{dong2021polyp, chen2021transunet}. U-shaped architectures \cite{ronneberger2015u, oktay2018attention, zhou2018unet++, huang2020unet, lou2021dc} are commonly used in medical image segmentation because of their sophisticated encoder-decoder architecture. Ronneberger et al. \cite{ronneberger2015u} introduce UNet, an encoder-decoder architecture that aggregates features from multiple stages through skip connections. In UNet++ \cite{zhou2018unet++}, authors use nested encoder-decoder sub-networks that are linked using dense skip connections. Besides, UNet3Plus \cite{huang2020unet} explores the full-scale skip connections having intra-connections among the decoder blocks.


Transformers are nowadays widely used in medical image segmentation \cite{cao2021swin, chen2021transunet, dong2021polyp}. In TransUNet \cite{chen2021transunet}, authors propose a hybrid CNN transformer architecture to learn both local and global relations among pixels. Swin-Unet \cite{cao2021swin} introduces a pure U-shaped transformer using Swin transformer \cite{liu2021swin} blocks. Recently, in CASTFormer \cite{you2022class}, authors introduce a class-aware transformer with adversarial training.

Some studies explore attention mechanisms with CNN \cite{oktay2018attention, fan2020pranet} and transformer-based architectures \cite{dong2021polyp} for medical image segmentation. In PraNet \cite{fan2020pranet}, authors utilize the reverse attention \cite{chen2018reverse}. PolypPVT \cite{dong2021polyp} uses PVTv2 \cite{wang2022pvt} as the encoder and adopts a CBAM \cite{woo2018cbam} attention block in the decoder with other modules. In CASCADE \cite{rahman2023medical}, authors propose a cascaded decoder using attention modules for feature refinement. Due to its remarkable performance in medical image segmentation, we incorporate the CASCADE decoder with our architecture.

\section{Method}
\label{sec:method}
In this section, we first introduce our proposed multi-scale hierarchical vision transformer (MERIT) backbone and decoder. We then describe an overall architecture combining our MERIT (i.e., MaxViT \cite{tu2022maxvit}) with the decoder (i.e., CASCADE \cite{rahman2023medical}). Finally, we introduce a new hierarchical feature-mixing loss aggregation method. 


\subsection{Multi-scale hierarchical vision transformer (MERIT)}
To improve the generalizability of the model across small and large objects in an image, we propose two designs based on the MERIT backbone network, i.e., Cascaded and Parallel. 

\begin{figure}[t]\floatconts
  {fig:cascaded_architecture}
  {\vspace{-0.7cm} 
\caption{Cascaded MERIT architecture. (a) cascaded MERIT backbone, (b) decoders with cascaded skip connections from the decoder 1, (c) prediction maps aggregation of two decoders. p1, p2, p3, and p4 are the aggregated multi-stage prediction maps.}}
  {\includegraphics[width=0.9\linewidth]{images/cascaded_merit_architecture.png}}
  \vspace{-0.5cm}
\end{figure}


\subsubsection{Cascaded MERIT}
\label{ssec:cascaded_merit}

In the cascaded design of our MERIT, we add (i.e., cascade) feedback from a backbone to the next backbone. We extract the hierarchical features from four different stages of the backbone network. Then, we cascade these features with the features from the previous backbone and pass them to the skip connections and bottleneck modules of respective decoders, except the first decoder. We also pass feedback from the decoder of one backbone (except the last) to the next backbone. This design captures the multi-scale, as well as multi-resolution features due to using multiple attention windows and hierarchical features. It also refines the features well due to adding feedback from the decoder of a backbone to the next backbone and using cascaded skip connections. Fig. \ref{fig:cascaded_architecture}(a) presents the Cascaded MERIT architecture with two backbone networks. For each backbone network, the images with resolution (H, W) are first put into a Stem layer (TB1 Stem, TB2 Stem in Fig. \ref{fig:cascaded_architecture}(a)) which reduces the feature resolution to (H/4, W/4). Afterward, these features are passed through four stages of transformer backbones (this reduces feature resolution by 2 times at each stage except the fourth). The features from the last stage of the first decoder are combined with the input image to cascade it (feature) with the second backbone in Fig. \ref{fig:cascaded_architecture}(a). To do this, we reduce the number of channels to one and produce logits by applying a 1x1 convolution followed by Sigmoid activation. \textcolor{black}{We also resize the feature map to the input resolution (i.e.,  in our implementation) of Backbone 2.}

\subsubsection{Parallel MERIT}
\label{ssec:parallel_merit}

Unlike Cascaded MERIT, in the parallel design of our MERIT backbone, we pass input images of multiple resolutions/scales in parallel into separate hierarchical transformer backbone encoders with different attention windows. Similar to Cascaded MERIT, we extract the hierarchical features from four different stages of the backbone networks and pass those features to the respective parallel decoders. This design also captures multi-scale features due to using hierarchical backbones with multiple attention windows. Fig. \ref{fig:parallel_architecture}(a) in Appendix \ref{asec:parallel_architecture} presents a design for the Parallel MERIT with two backbone networks. The input images are passed through similar steps in the backbone networks as in Cascaded MERIT. However, the Parallel MERIT shares information among the backbone networks only at the very end during the feature aggregation step (Fig. \ref{fig:parallel_architecture}(c) in Appendix \ref{asec:parallel_architecture}).  


\subsection{Decoder}
We propose using a separate decoder for each transformer backbone. As shown in Fig.  \ref{fig:cascaded_architecture}(b), we use cascaded skip connections in the decoder of our cascaded MERIT architecture. Here, we add the skip connections from the first backbone to the skip connections of the second backbone network. In this case, we share information across backbones in three phases, i.e., during backbone cascading, skip connections cascading, and aggregating prediction maps. This sharing of information helps to capture richer information than the single-resolution backbone, as well as our Parallel MERIT.

Unlike Fig. \ref{fig:cascaded_architecture}(b), in Fig. \ref{fig:parallel_architecture}(b) in Appendix \ref{asec:parallel_architecture}, we have two parallel decoders for our parallel backbones. Each decoder has four stages that correspond to four stages of the transformer backbone. We only aggregate the multi-stage prediction maps produced by the decoders in Fig. \ref{fig:parallel_architecture}(b) at the aggregation step shown in Fig. \ref{fig:parallel_architecture}(c). 


\subsection{Overall Architecture}
In our experiments, we use one of the most recent SOTA transformers, MaxViT \cite{tu2022maxvit}. We use two instances of MaxViT-S (standard) backbone with  and  attention windows to create our MERIT backbone. Each MaxViT backbone has two Stem blocks followed by four stages that consist of multiple (i.e., 2, 2, 5, 2) MaxViT blocks. Each MaxViT block is built with a Mobile Convolution Block (MBConv), a Block Attention having Block Self-Attention (SA) followed by a Feed Forward Network (FFN), a Grid Attention having a Grid SA followed by an FFN. We note that although we use the MaxViT backbone in our experiments, other transformer backbones can easily be used with our MERIT. 

Pure transformers have limited (spatial) contextual information processing ability among pixels. As a result, the transformer-based models face difficulties in locating discriminative local features. To address this issue, we adopt a recent attention-based cascaded decoder, CASCADE \cite{rahman2023medical}, for multi-stage feature refinement and aggregation. CASCADE \textcolor{black}{decoder} uses the attention gate (AG) \cite{oktay2018attention} for cascaded feature aggregation and the convolutional attention module (CAM) for robust feature map enhancement. CASCADE \textcolor{black}{decoder} has four CAM blocks for the four stages of hierarchical features from the transformer backbone and three AGs for three skip connections. CASCADE \textcolor{black}{decoder} aggregates the multi-resolution features by combining the upsampled features from the previous stage of the decoder with the features from the skip connections using AG. Then, CASCADE \textcolor{black}{decoder} processes the aggregated features using the CAM module (consists of channel attention \cite{hu2018squeeze} followed by spatial attention \cite{chen2017sca}) which groups pixels together and suppresses background information. Lastly, CASCADE \textcolor{black}{decoder} sends the output from the CAM block of each stage to a prediction head to produce prediction maps. 

We produce four prediction maps from the four stages of the CASCADE \textcolor{black}{decoder}. As shown in Fig. \ref{fig:cascaded_architecture}(c) and Fig. \ref{fig:parallel_architecture}(c) in Appendix \ref{asec:parallel_architecture}, we aggregate (additive) the prediction maps for each stage of our two decoders. We generate the final prediction map, , using Equation \ref{eq:output_aggregation}:

where , , , and  represent the prediction maps, and , , , and  are the weights of each prediction heads. We use the value of 1.0 for , , , and . Finally, we apply Softmax activation on  to get the multi-class segmentation output.

\subsection{Multi-stage feature-mixing loss aggregation (MUTATION)}
We now introduce a simple, yet effective multi-stage feature mixing loss aggregation strategy for image segmentation, which enables better model training. Our intention is to create new prediction maps combining the available prediction maps. So, we take all the prediction maps from different stages of a network as input and aggregate the losses of prediction maps generated using  non-empty subsets of  prediction maps. For example, if a network produces 4 prediction maps, our multi-stage feature-mixing loss aggregation produces a total of  prediction maps including 4 original maps. This mixing strategy is simple, it does not require additional parameters to calculate, and it does not introduce inference overheads. Due to its potential benefits, this strategy can be used with \textit{any} multi-stage image segmentation or dense prediction networks. Algorithm \ref{alg:msfmla} presents the steps to produce new prediction maps and loss aggregation.   

\begin{algorithm2e}
\caption{Multi-stage Feature-Mixing Loss Aggregation}
\label{alg:msfmla}
\LinesNumbered
\KwIn{ the ground truth mask A list ; , where each element is a prediction map}
\KwOut{; the aggregated loss}
\;
 find all non-empty subsets of prediction map indices, ;
\tcp{ is the set of non-empty subsets of }
\ForEach {}{

  ;  \tcp{ is a new prediction map}
  \ForEach {}{    
    \;
  }
  ; \textcolor{black}{\tcp{ is any loss function (e.g., CrossEntropy, DICE)}}
}
\end{algorithm2e}

\begin{table*}[t]
\centering
\floatconts
{tab:multi_organ_results}
{\caption{Results on Synapse multi-organ dataset. DICE scores (\%) are reported for individual organs. The results of UNet, AttnUNet, PolypPVT, and SSFormerPVT are taken from CASCADE \cite{rahman2023medical}. MERIT results are averaged over five runs for MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION.  denotes higher the better,  denotes lower the better. The best results are in bold.}
\vspace{-0.5cm}
} {\begin{adjustbox}{width=1\textwidth}
{\begin{tabular}{lrrrrrrrrrr}
\toprule
\multirow{2}{*}{Architectures} & \multicolumn{2}{c}{Average}                                                                                     & \multicolumn{1}{l}{\multirow{2}{*}{Aorta}} & \multicolumn{1}{l}{\multirow{2}{*}{GB}} & \multicolumn{1}{l}{\multirow{2}{*}{KL}} & \multicolumn{1}{l}{\multirow{2}{*}{KR}} & \multicolumn{1}{l}{\multirow{2}{*}{Liver}} & \multicolumn{1}{l}{\multirow{2}{*}{PC}} & \multicolumn{1}{l}{\multirow{2}{*}{SP}} & \multicolumn{1}{l}{\multirow{2}{*}{SM}} \\
                               & \multicolumn{1}{l}{DICE} & \multicolumn{1}{l}{HD95} &  \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{}                    & \multicolumn{1}{l}{}                         & \multicolumn{1}{l}{}                         & \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{}                    & \multicolumn{1}{l}{}                    & \multicolumn{1}{l}{}                    \\
\midrule
UNet \cite{ronneberger2015u}     &      70.11      &    44.69 
    &   84.00   &      56.70   &       72.41  &       62.64  &      86.98   &     48.73   &      81.48   &     67.96
    \\
AttnUNet \cite{oktay2018attention}    &  71.70     &  34.47 
     &  82.61       &  61.94        &  76.07   &  70.42  
    &  87.54        &  46.70   &  80.67      &  67.66

\\
R50+UNet \cite{chen2021transunet}                  & 74.68                    & 36.87                    & 84.18                                      & 62.84                                   & 79.19                                        & 71.29                                        & 93.35                                      & 48.23                                   & 84.41                                   & 73.92                                   
\\
R50+AttnUNet \cite{chen2021transunet}                   & 75.57                    & 36.97                    & 55.92                                      & 63.91                                   & 79.20                                        & 72.71                                        & 93.56                                      & 49.37                                   & 87.19                                   & 74.95                                   
\\
SSFormerPVT \cite{wang2022stepwise}                   & 78.01                    & 25.72                    & 82.78                                      & 63.74                                   & 80.72                                        & 78.11                                        & 93.53                                      & 61.53                                   & 87.07                                   & 76.61                                   \\ PolypPVT \cite{dong2021polyp}                       & 78.08                    & 25.61                    & 82.34                                      & 66.14                                   & 81.21                                        & 73.78                                        & 94.37                                      & 59.34                                   & 88.05                                   & 79.4                                    \\ TransUNet \cite{chen2021transunet}                    & 77.48                    & 31.69                     & 87.23                                      &  63.13                                   & 81.87                                       & 77.02                                        & 94.08                                     & 55.86                                   & 85.08                                   & 75.62                                      \\ SwinUNet \cite{cao2021swin}                      & 79.13                    & 21.55                    & 85.47                                      & 66.53                                  &  83.28                                        & 79.61                                        & 94.29                                      & 56.58                                   & 90.66                                   & 76.60                                  \\ 
MT-UNet \cite{wang2022mixed}                      & 78.59                    & 26.59                    & 87.92                                      & 64.99                                   & 81.47                                        & 77.29                                        & 93.06                                     & 59.46                                  & 87.75                                   & 76.81                                  \\
MISSFormer \cite{huang2021missformer}                      & 81.96                    & 18.20                    & 86.99                                      &  68.65                                   &  85.21                                        & 82.00                                        & 94.41                                      & 65.67                                   & 91.92                                   & 80.81                                   \\        
CASTformer \cite{you2022class}                      & 82.55                    & 22.73                    & \textbf{89.05}                                      &  67.48                                   & 86.05                                       & 82.17                                        & \textbf{95.61}                                      & 67.49                                   & 91.00                                   & 81.55                                   \\
PVT-CASCADE \cite{rahman2023medical}                  & 81.06                    & 20.23                   & 83.01                                      & 70.59                                   & 82.23                                        & 80.37                                        & 94.08                                      & 64.43                                   & 90.1                                    & 83.69                                   \\ TransCASCADE \cite{rahman2023medical}                 & 82.68                    & 17.34                    & 86.63                                      & 68.48                                   & 87.66                                        & 84.56                                        & 94.43                                      & 65.33                                   & 90.79                                   & 83.52                                   \\
\midrule
Parallel MERIT (Ours)                   & 84.22                    & 16.51                    & 88.38                                      & 73.48                                  & 87.21                                        & 84.31                                        & 95.06                                      & 69.97                                   & 91.21                                    & 84.15                                   \\
Cascaded MERIT (Ours)                   & \textbf{84.90}                    & \textbf{13.22}                    & 87.71                                      & \textbf{74.40}                                   & \textbf{87.79}                                       & \textbf{84.85}                                        & 95.26                                      & \textbf{71.81}                                   &  \textbf{92.01}                                    & \textbf{85.38}                                   \\
\bottomrule \end{tabular}}
\end{adjustbox}
}\footnotesize{ More details in Appendix \ref{assec:eval_metrics},  more details in Appendix \ref{assec:datasets}}\\
\vspace{-0.3cm}
\end{table*}

\section{Experiments}
In this section, we demonstrate the superiority of our proposed MERIT architectures by comparing the results with SOTA methods. We introduce datasets, evaluation metrics, and implementation details in \textbf{Appendix \ref{asec:experimental_setup}}. More experiments and ablation studies to answer questions related to our architectures are given in \textbf{Appendix \ref{assec:baseline_compare}-\ref{assec:loss_weight}}.








\subsection{Results on Synapse multi-organ segmentation}
Table \ref{tab:multi_organ_results} presents the results of Synapse multi-organ segmentation; it can be seen that both variants of our MERIT significantly outperform all the SOTA CNN- and transformer-based 2D medical image segmentation methods. Among all the methods, our Cascaded MERIT achieves the best average DICE score (84.90\%). Cascaded MERIT outperforms two popular methods on this dataset, such as TransUNet and SwinUNet by 7.42\% and 5.57\%, respectively, when compared to their original reported DICE scores. Cascaded MERIT achieves 2.22\% better DICE than the existing best method, TransCASCADE (82.68\% DICE), on this dataset. When we compare the HD95 distance of all the methods, we find that both variants of our MERIT achieve a lower HD95 distance. Cascaded MERIT has the lowest HD95 distance (13.22) which is 18.47 lower than TransUNet (HD95 of 31.69) and 4.12 lower than the best SOTA method, TransCASCADE (HD95 of 17.34).


If we look into the DICE score of individual organs, we observe that proposed MERIT variants significantly outperform SOTA methods on six out of eight organs. We also can conclude that Cascaded MERIT performs better both in large and small organs, though it exhibits greater improvement for small organs. We believe that both MERIT variants demonstrate better performance due to using the multi-scale hierarchical transformer encoder with cascaded attention-based decoding and MUTATION loss aggregation. 


\begin{table*}[t]
\centering
\floatconts
{tab:acdc_results}
{\caption{Results on the ACDC dataset. DICE scores (\%) are reported for individual organs. We present the results of MERIT averaging over five runs with the setting MERIT+CASCADE \textcolor{black}{decoder(Additive)}+MUTATION. The best results are in bold.}
\vspace{-0.5cm}
} {\small{
{\begin{tabular}{lrrrr}
\toprule
Architectures     & \multicolumn{1}{l}{Avg DICE} & \multicolumn{1}{l}{RV} & \multicolumn{1}{l}{Myo} & \multicolumn{1}{l}{LV} \\
\midrule
R50+UNet   \cite{chen2021transunet}       & 87.55                        & 87.10                  & 80.63                   & 94.92                  \\
R50+AttnUNet  \cite{chen2021transunet}  & 86.75                        & 87.58                  & 79.20                   & 93.47                  \\
ViT+CUP \cite{chen2021transunet}   & 81.45                        & 81.46                  & 70.71                   & 92.18                 \\
R50+ViT+CUP \cite{chen2021transunet} & 87.57                        & 86.07                  & 81.88                   & 94.75                  \\
TransUNet  \cite{chen2021transunet}       & 89.71                        & 88.86                  &  84.53                   & 95.73                  \\   
SwinUNet \cite{cao2021swin}         & 90.00                        & 88.55                  & 85.62                   & 95.83                  \\
MT-UNet \cite{wang2022mixed}         & 90.43                        & 86.64                  & 89.04                   & 95.62                  \\
MISSFormer \cite{huang2021missformer}         & 90.86                        & 89.55                  & 88.04                   & 94.99                  \\
PVT-CASCADE \cite{rahman2023medical}      & 91.46                        & 88.9                   & 89.97                   & 95.50                   \\
TransCASCADE \cite{rahman2023medical}    & 91.63                        & 89.14                 & \textbf{90.25}                   & 95.50 \\
\midrule
Parallel MERIT (Ours)      & \textbf{92.32}                        & \textbf{90.87}                   & 90.00                   & \textbf{96.08}                   \\
Cascaded MERIT (Ours)    & 91.85                      & 90.23                 & 89.53                   & 95.80 \\
\bottomrule \end{tabular}}
}}
\footnotesize{ More details in Appendix \ref{assec:datasets}}\\
\vspace{-0.3cm}
\end{table*}

\subsection{Results on ACDC cardiac organ segmentation}
Table \ref{tab:acdc_results} reports three cardiac organ segmentation results of different methods on the ACDC dataset for MRI data modality. Both our Parallel and Cascaded MERIT have better DICE scores than all other SOTA methods. Our Parallel MERIT achieves the best average DICE score (92.32\%) which outperforms TransUNet and SwinUNet by 2.61\% and 2.32\%, respectively. Parallel MERIT also shows the best DICE scores in RV (90.87\%) and LV (96.08\%) segmentation. We can conclude from these results that our method performs the best across different medical imaging data modalities. 

\section{Conclusion}
\label{sec:conclution}
In this paper, we have introduced a novel multi-scale hierarchical transformer architecture (MERIT) that can capture both the multi-scale and multi-resolution features necessary for medical image segmentation. We have also incorporated an attention-based cascaded decoder to further refine features. Moreover, we have proposed a novel multi-stage feature mixing loss aggregation (MUTATION) strategy \textcolor{black}{for implicit ensembling/augmentation} which ensures better model training and boosts the performance without introducing additional hyper-parameters and inference overhead. Our experimental results on two well-known multi-class medical image segmentation benchmarks demonstrate the superiority of our proposed method over all SOTA approaches. Finally, we believe that our proposed MERIT architectures and MUTATION loss aggregation strategy will improve other downstream medical image segmentation and semantic segmentation tasks.   




\bibliography{midl-samplebibliography}


\newpage

\appendix
\vspace{-0.5cm}
\section{Parallel MERIT Architecture}
\label{asec:parallel_architecture}
Due to the page limitation, our parallel MERIT architecture is given in Fig. \ref{fig:parallel_architecture}. This architecture is described in Section \ref{ssec:parallel_merit} of the main text.

\begin{figure}[t]\floatconts
  {fig:parallel_architecture}
  {\vspace{-0.6cm}\caption{Parallel MERIT architecture. (a) parallel MERIT backbone, (b) parallel decoders, (c) prediction maps aggregation from two decoders. p1, p2, p3, and p4 are the aggregated multi-stage prediction maps.}}
  {\includegraphics[width=0.9\linewidth]{images/parallel_merit_architecture.png}}\vspace{-0.5cm}
\end{figure}

\section{Experimental Setup}
\label{asec:experimental_setup}
This section first describes datasets, then introduces evaluation metrics, and finally provides the implementation details of our proposed architecture and experiments. 

\subsection{Datasets}
\label{assec:datasets}

\textbf{Synapse multi-organ dataset.} There are 30 abdominal CT scans with 3779 axial contrast-enhanced abdominal CT images in the Synapse multi-organ dataset\footnote{\href{https://www.synapse.org/\#!Synapse:syn3193805/wiki/217789}{https://www.synapse.org/\#!Synapse:syn3193805/wiki/217789 }}. Each CT scan has 85-198 slices of resolution  pixels, having a voxel spatial resolution of ([0:54-0:54]  [0:98-0:98][2:5-5:0]). We extract 2D slices from the CT scans and segment 8 abdominal organs, such as the aorta, gallbladder (GB), left kidney (KL), right kidney (KR), liver, pancreas (PC), spleen (SP), and stomach (SM). Following the experimental protocol of TransUNet \cite{chen2021transunet}, we split the dataset into 18 scans (2211 axial slices) for training, and 12 for validation.  

\textbf{ACDC dataset.} The ACDC dataset\footnote{\href{https://www.creatis.insa-lyon.fr/Challenge/acdc/}{https://www.creatis.insa-lyon.fr/Challenge/acdc/}} contains 100 cardiac MRI scans collected from different patients. We extract 2D slices from each MRI scan and segment three organs, such as the right ventricle (RV), left ventricle (LV), and myocardium (Myo). Following MT-UNet \cite{wang2022mixed}, we split the dataset into 70 (1304 axial slices), 10 (182 axial slices), and 20 cases for training, validation, and testing, respectively.




\subsection{Evaluation metrics.} 
\label{assec:eval_metrics}
In our experiments on the Synapse Multi-organ dataset, we use DICE and 95\% Hausdorff Distance (95\%HD) as the evaluation metrics. However, we use only DICE scores as an evaluation metric for the ACDC dataset. The DICE similarity scores  and 95\%HD distance  (95th percentile of the distances between boundary points in  and ) are calculated using Equations \ref{eq:dice} and \ref{eq:95hd}, respectively. 




where  and  are the ground truth mask and predicted segmentation map, respectively. 

\subsection{Implementation details}
\label{assec:impl_details}
We use PyTorch 1.12.0 with CUDA 11.6 in all of our experiments. Besides, we use a single NVIDIA RTX A6000 GPU with 48GB of memory to train all the models. We utilize the Pytorch pre-trained weights on ImageNet from timm library \cite{rw2019timm} for MaxViT backbone networks. We use the input resolutions and attention windows of  and , respectively, in our (dual-scale) MERIT. We augment data using only random rotation and flipping. We train our model using AdamW \cite{loshchilov2017decoupled} optimizer with a weight decay and learning rate of 0.0001. We optimize the combined DICE and Cross-Entropy (CE) loss  in Equation \ref{eq:loss} with  and  (weights are selected empirically in Appendix \ref{assec:loss_weight}) in all our experiments:

 
where  and  are the weight for the DICE () and CE () losses, respectively.

We train each model a maximum of 300 epochs with a batch size of 24 for Synapse multi-organ segmentation. For ACDC cardiac organ segmentation, we use a batch size of 12 and train each model for a maximum of 400 epochs.







\begin{table*}[t]
\centering
\floatconts
{tab:compare_baseline_results}
{\caption{Comparison with the baseline method on Synapse multi-organ and ACDC datasets. We report the results of our MERIT with the setting MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION. We report the average inference time (ms) over 5000 samples. All reported \textcolor{black}{DICE scores (\%) in columns Synapse Multi-organ and ACDC} are averaged over five runs. The best results are in bold.}} {\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm} \begin{tabular}{lrrrrr}
\toprule
Architectures    &  Input Resolutions   &    \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Params (M)/\\FLOPS (G)  \end{tabular}} &   
  \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Inference \\Time (ms)  \end{tabular}} &   \begin{tabular}[c]{@{}c@{}}Synapse \\Multi-organ  \end{tabular} & ACDC \\
\midrule
MaxViT            &          &  \textcolor{black}{65.25/10.43}	&  \textcolor{black}{19.79}  &  77.11  & 90.56 \\
MaxViT            &          &  \textcolor{black}{65.25/14.19}	  & \textcolor{black}{20.58}  & 78.53  & 90.98  \\
\midrule
\textcolor{black}{MaxViT with CASCADE decoder}           &  \textcolor{black}{}       &  \textcolor{black}{82.62/14.2}	&  \textcolor{black}{21.84}  &  \textcolor{black}{79.83}  & \textcolor{black}{90.87} \\
\textcolor{black}{MaxViT with CASCADE decoder}            &  \textcolor{black}{}        &  \textcolor{black}{82.62/19.11}	  & \textcolor{black}{23.07}  & \textcolor{black}{80.20}  & \textcolor{black}{91.15}  \\
\midrule
Parallel MERIT \textcolor{black}{(\textbf{our})}          & ,            &  \textcolor{black}{147.86/33.31}	&  \textcolor{black}{37.01}  & 84.22   & \textbf{92.32} \\
Cascaded MERIT \textcolor{black}{(\textbf{our})}          & ,           & \textcolor{black}{147.86/33.31}	&  \textcolor{black}{37.06}  & \textbf{84.90}   & 91.85 \\
\bottomrule \end{tabular}}
\end{adjustbox}
}\end{table*}


\section{Ablation Studies}
\label{asec:ablation_studies}
In this section, we present a wide range of ablation studies to answer different intrinsic questions related to our proposed architectures, loss aggregation, and experiments; these are described in the following subsections.

\subsection{Comparison with the baseline method}
\label{assec:baseline_compare}
We compare our proposed methods with baseline hierarchical MaxViT architecture. In the case of MaxViT, we do the same multi-stage prediction for a fair comparison. We also use a similar experimental setting except using MUTATION with our architectures. Table \ref{tab:compare_baseline_results} presents the results of these experiments. We can see from Table \ref{tab:compare_baseline_results} that our proposed architectures with MUTATION loss \textcolor{black}{(see "our" entries in Table \ref{tab:compare_baseline_results})} improve the baseline \textcolor{black}{hierarchical } resolution MaxViT \textcolor{black}{(see  row entries in Table \ref{tab:compare_baseline_results})} by 6.37\% and 1.34\% DICE scores \textcolor{black}{(with  more FLOPS and  longer inference time)} in Synapse multi-organ and ACDC datasets, respectively. \textcolor{black}{We can also see that our MERIT architecture has 147.86M parameters which is  larger than  resolution MaxViT with CASCADE decoder (see  row entries in Table \ref{tab:compare_baseline_results}), but with a 4.7\% better DICE score in Synapse multi-organ. We think that this increase of parameters/FLOPS/inference time worth it given the improvement in performance.}

\begin{table*}[]
\centering
\floatconts
{tab:multi_scale_results}
{\caption{Effect of multi-scale backbone on Synapse multi-organ dataset. We report the results of the backbone with CASCADE \textcolor{black}{decoder} (no MUTATION) to clarify the effect of multi-scale backbones. All reported results are averaged over five runs. The best results are in bold.}} {\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm} \begin{tabular}{lrrrrrr}
\toprule
Architectures    &  Input Resolutions   & \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Attention\\Windows\end{tabular}}	 & \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Params (M)/\\FLOPS (G)  \end{tabular}} & \begin{tabular}[c]{@{}c@{}}Avg \\DICE (\%) \end{tabular} \\
\midrule
\textcolor{black}{(single)} MaxViT            &         & \textcolor{black}{}	& \textcolor{black}{82.62/14.2} &  79.83 \\
\textcolor{black}{(single)} MaxViT            &         &  \textcolor{black}{}	 & \textcolor{black}{82.62/19.11}  & 80.20  \\
\midrule
Parallel Double MaxViT      & ,      & \textcolor{black}{, }	& \textcolor{black}{147.86/28.4} & 80.81   \\
Parallel Double MaxViT      & ,       &  \textcolor{black}{, }	& \textcolor{black}{147.86/38.22}  & 82.15  \\
Cascaded Double MaxViT      & ,       &  \textcolor{black}{, }	& \textcolor{black}{147.86/28.4}  &  81.06 \\
Cascaded Double MaxViT      & ,       &  \textcolor{black}{, }	& \textcolor{black}{147.86/38.22}  &  83.02 \\
\midrule
Parallel MERIT \textcolor{black}{(\textbf{our})}           & ,            &  \textcolor{black}{, }    & \textcolor{black}{147.86/33.31}  & \textbf{82.91}   \\
Cascaded MERIT \textcolor{black}{(\textbf{our})}          & ,          & \textcolor{black}{, }  & \textcolor{black}{147.86/33.31}  & \textbf{83.35}    \\
\bottomrule \end{tabular}}
\end{adjustbox}
}\end{table*}


\begin{table*}[t]
\centering
\floatconts
{tab:tiny_vs_small_results}
{\caption{\textcolor{black}{Comparison of Tiny MERIT vs. Small MaxViT architectures on Synapse multi-organ dataset. We report the results of the backbone with CASCADE \textcolor{black}{decoder} (no MUTATION) to clarify the effect of multi-scale backbones. All reported results are averaged over five runs. The best results are in bold.}}} {\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm}\begin{tabular}{lrrrr}
\toprule
\textcolor{black}{Architectures}              & \textcolor{black}{Input Resolution} & \textcolor{black}{\begin{tabular}[c]{@{}l@{}}Attention \\ Windows\end{tabular}} & \textcolor{black}{\begin{tabular}[c]{@{}l@{}}Params (M)/\\ FLOPS (G)\end{tabular}} & \textcolor{black}{Avg DICE (\%)} \\
\midrule
\textcolor{black}{MaxViT-Tiny}                & \textcolor{black}{224224}          & \textcolor{black}{77}                                                          & \textcolor{black}{36.86/6.57}                                                      & \textcolor{black}{77.84}     \\
\textcolor{black}{MaxViT-Tiny}                & \textcolor{black}{256256}          & \textcolor{black}{88}                                                          & \textcolor{black}{36.86/8.61}                                                      & \textcolor{black}{78.43}     \\
\midrule
\textcolor{black}{Parallel MERIT-Tiny (ours)} & \textcolor{black}{256256, 224224} & \textcolor{black}{88, 77}                                                     & \textcolor{black}{65.41/15.18}                                                     & \textcolor{black}{\textbf{81.34}}     \\
\textcolor{black}{Cascaded MERIT-Tiny (ours)} & \textcolor{black}{256256, 224224} & \textcolor{black}{88, 77}                                                     & \textcolor{black}{65.41/15.18}                                                     & \textcolor{black}{\textbf{81.82}}     \\
\midrule
\textcolor{black}{MaxViT-Small}               & \textcolor{black}{224224}          & \textcolor{black}{77}                                                          & \textcolor{black}{82.62/14.2}                                                      & \textcolor{black}{79.83}     \\
\textcolor{black}{MaxViT-Small}               & \textcolor{black}{256256}          & \textcolor{black}{88}                                                          & \textcolor{black}{82.62/19.11}                                                     & \textcolor{black}{80.20} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
}
\end{table*}

\subsection{Effect of multi-scale backbone}
\label{assec:effect_multi_scale}
We have conducted experiments on the Synapse multi-organ dataset to show the effect of our multi-scale backbone on medical image segmentation. In Table \ref{tab:multi_scale_results}, we present the results of all the methods with the CASCADE decoder (no MUTATION) to make a fair comparison of our proposed architecture. It can be seen from Table \ref{tab:multi_scale_results} that the input resolution has an impact on DICE score improvement. More precisely,  resolution backbones have better DICE scores than the  resolution backbones. \textcolor{black}{As shown, our Cascaded MERIT achieves the best DICE score (83.35\%) which improves the baseline  resolution MaxViT (see  row entries in Table \ref{tab:multi_scale_results}) by 3.15\%. When comparing with the double backbone architectures with the same input scale (attention window), we can see that our multi-scale (attention window) double backbone architectures achieve better DICE scores due to their additive advantage of multi-scale feature extraction. We note that our Parallel/Cascaded MERIT (33.31G) has a significantly lower computational complexity/FLOPS than the  resolution Parallel/Cascaded Double MaxViT (38.22G) due to using one  and another  resolution inputs. Despite that, our Parallel and Cascaded MERIT outperform the Double MaxViT by 0.76\% and 0.33\%, respectively. These improvements in the DICE score support the claim regarding the benefit of calculating SA in multiple scale attention windows.}

 \textcolor{black}{We have conducted an additional set of experiments by implementing a tiny version of MERIT using the tiny MaxViT backbones, to clarify that performance improvement is due to the effect of multi-scale SA, not because of using a model with more parameters. As shown in Table \ref{tab:tiny_vs_small_results}, when comparing against the Small MaxViT backbone which has more model parameters, both of our Tiny MERIT backbones perform better. Our Tiny Cascaded MERIT backbone outperforms the Small MaxViT backbone (see  row entries in Table \ref{tab:tiny_vs_small_results}) by up to 1.62\% DICE score for a 256256 input resolution, while having  smaller model parameters and  fewer FLOPS. Therefore, again, we can conclude from the empirical evaluation that our multi-scale SA calculation improves the performance of medical image segmentation.}

\begin{table*}[t]
\centering
\floatconts
{tab:cascade_mutation_effects_results}
{\caption{Effect of CASCADE \textcolor{black}{decoder} and MUTATION loss aggregation in MERIT on Synapse multi-organ dataset. We present the results of MERIT averaging over five runs. The best results are in bold.}} {{\vspace{-0.4cm}\begin{tabular}{lrrrr}
\toprule
Architectures    &  CASCADE \textcolor{black}{decoder}   &  MUTATION     & \multicolumn{1}{l}{Avg DICE (\%)} \\
\midrule
Parallel MERIT            &  No         & No    & 80.44  \\
Parallel MERIT           & No         & Yes                   & 81.06   \\
Parallel MERIT            &  Yes         & No            & 82.91  \\
Parallel MERIT \textcolor{black}{(\textbf{our})}          & Yes       & Yes                     & 84.22   \\
\midrule
Cascaded MERIT            &  No         & No    & 80.76  \\
Cascaded MERIT           & No         & Yes                   & 82.03   \\
Cascaded MERIT            &  Yes         & No            & 83.35  \\
Cascaded MERIT \textcolor{black}{(\textbf{our})}           & Yes       & Yes                     & \textbf{84.90}    \\
\bottomrule \end{tabular}}
}\end{table*}

\begin{figure}[t]\floatconts
  {fig:qualitative_results}
  {\caption{\textcolor{black}{Qualitative results on Synapse multi-organ dataset. (a) Ground Truth (GT), (b) MaxViT (input resolution 224224), (c) MaxViT (input resolution 256256), (d) Parallel MERIT, (e) Cascaded MERIT. We produce all the segmentation maps with the CASCADE decoder and overlay on top of original image/slice.}}}
  {\includegraphics[width=1\linewidth]{images/qualitative_results.png}}
\end{figure}

\subsection{Effect of CASCADE \textcolor{black}{decoder} and MUTATION loss aggregation in MERIT}
\label{assec:effect_cascade_mutation}
We have conducted some experiments on Synapse multi-organ dataset to demonstrate the effect of CASCADE \textcolor{black}{decoder} and MUTATION loss aggregation strategy on our MERIT architectures. Table \ref{tab:cascade_mutation_effects_results} presents the results of our Parallel MERIT with or without CASCADE \textcolor{black}{decoder} and MUTATION. We can see from Table \ref{tab:cascade_mutation_effects_results} that Parallel and Cascaded MERIT without both CASCADE \textcolor{black}{decoder} and MUTATION have the lowest DICE scores. CASCADE \textcolor{black}{decoder} significantly increases the DICE scores (2.47-2.59\%) \textcolor{black}{due to capturing the spatial (contextual) relations among pixels (usually limited in vision transformer)}, while MUTATION alone marginally improves the DICE (0.62-1.27\%). However, when MUTATION is used with the outputs from CASCADE \textcolor{black}{decoder}, it achieves the best DICE scores (84.22\%, 84.90\%) improving CASCADE \textcolor{black}{decoder} by 1.31-1.55\%. We believe the reason behind this is that MUTATION works well with the refined features of the CASCADE \textcolor{black}{decoder}. \textcolor{black}{Therefore, we can conclude that the synthesized prediction maps generated via combinatory aggregation (MUTATION) help us improve the performance of the model; this is why we prefer combinatory loss aggregation over linear aggregation. We believe that our combinatory loss aggregation (MUTATION) can be used as a beneficial ensembling/augmentation method in other downstream semantic and medical image segmentation tasks.}

\begin{table*}[t]
\centering
\floatconts
{tab:cascade_aggregation_results}
{\caption{Comparison of different aggregations in CASCADE \textcolor{black}{decoder} on Synapse multi-organ dataset. We present the results of MERIT averaging over five runs with the setting MERIT+CASCADE \textcolor{black}{decoder}+MUTATION. The best results are in bold.}} {{\vspace{-0.4cm} \begin{tabular}{lrrrr}
\toprule
Architectures    &  Aggregation in CASCADE \textcolor{black}{decoder}   &   \multicolumn{1}{l}{Avg DICE (\%)} \\
\midrule
Parallel MERIT            & Concatenation            & 84.18  \\
Parallel MERIT           & Concatenation                            & 84.22   \\
\midrule 
Cascaded MERIT         & Additive                    & 84.88      \\
Cascaded MERIT        & Additive                     & \textbf{84.90}   \\
\bottomrule \end{tabular}}
}\end{table*}

\subsection{\textcolor{black}{Qualitative results on Synapse Multi-organ Segmentation}}
\label{assec:qualitative_results}
\textcolor{black}{Fig. \ref{fig:qualitative_results} shows the qualitative results of the baseline hierarchical MaxViT and our proposed MERIT architectures. As shown in the figure, our MERIT architecture can segment the small organs (see the red rectangular box) well. In contrast, the single scale MaxViT architecture with both  and  input resolutions fail to segment that small organ. Our MERIT architecture also segments the larger organ much better than the single scale MaxViT. We believe the reason behind this better segmentation of both small and large organs by our MERIT architectures is the use of multi-scale SA. 
}


\subsection{Effect of different aggregations in CASCADE \textcolor{black}{decoder}} 
\label{assec:effect_aggregation_cascade}


Table \ref{tab:cascade_aggregation_results} presents the results of concatenation and additive aggregations in CASCADE \textcolor{black}{decoder} on Synapse multi-organ dataset. We can see from Table \ref{tab:cascade_aggregation_results} that our MERIT architectures with additive aggregation in CASCADE \textcolor{black}{decoder} are marginally better (0.04-0.02\%) than the concatenation. Therefore, we can conclude from these results that the aggregation techniques do not have much impact on the CASCADE decoder of our architectures while using MUTATION. However, concatenation aggregation-based methods usually have additional computational overheads due to increasing the number of channels after the aggregation, while additive aggregation keeps the number of channels the same. Consequently, we recommend using additive aggregation in our MERIT architectures due to its computational benefits.



\begin{table*}[t]
\centering
\floatconts
{tab:interpolations_results}
{\caption{Comparison of different interpolations in MERIT on Synapse multi-organ dataset. We present the results of MERIT averaging over five runs with the setting MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION. The best results are in bold.}} {{\vspace{-0.4cm}\begin{tabular}{lrrrr}
\toprule
Architectures    &  Interpolations    &   \multicolumn{1}{l}{Avg DICE (\%)} \\
\midrule
Parallel MERIT            &  nearest-exact            & 81.67  \\
Parallel MERIT           & area                            & 81.76   \\
Parallel MERIT            &  bicubic            & 83.58  \\
Parallel MERIT           & bilinear                            & 84.22   \\
\midrule
Cascaded MERIT         & nearest-exact                    & 82.27      \\
Cascaded MERIT        & area                     & 82.38   \\
Cascaded MERIT         & bicubic                    & 84.05      \\
Cascaded MERIT        & bilinear                     & \textbf{84.90}   \\
\bottomrule \end{tabular}}
}\end{table*}

\begin{figure}[t]\floatconts
  {fig:loss_curve}
  {\vspace{-0.7cm} \caption{Loss weight vs. DICE curve on Synapse multi-organ dataset. We report the results of our Cascaded MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION with different weights for DICE and CE losses. X-axis presents the weights of DICE loss, , while the weight for CE loss is . The value of 0.0 on the X-axis represents weights for DICE and CE losses of 0.0 and 1.0, respectively (i.e., only CE loss is used). While the value of 1.0 on the X-axis represents weights for DICE and CE losses of 1.0 and 0.0, respectively (i.e., only DICE loss is used)}}
  {\includegraphics[width=0.7\linewidth]{images/loss_curve.png}}
\end{figure}


\subsection{Effect of different interpolations in MERIT}
\label{assec:effect_interpolations}
We have conducted some experiments on Synapse multi-organ dataset to choose the best interpolations methods for our proposed MERIT architectures. Table \ref{tab:interpolations_results} presents the results of Parallel and Cascaded MERIT using \textit{nearest-exact} (nearest neighbor), \textit{area}, \textit{bicubic}, and \textit{bilinear} interpolation methods from Pytorch. The \textit{nearest-exact} interpolation shows the lowest DICE scores while \textit{bilinear} and \textit{bicubic} interpolation achieve the best and second best DICE scores, respectively. Therefore, we recommend using \textit{bilinear} interpolation in our proposed MERIT architectures to re-scale the features and prediction maps. 

\subsection{Choosing weight for DICE and CE losses}
\label{assec:loss_weight}
We optimize the combined DICE and CE loss during the training of our models. Here, we have conducted some experiments to choose the best weight pairs to combine these two losses. Fig. \ref{fig:loss_curve} presents the DICE scores for different weight pairs for losses. We can see in the graph that the model shows the worst DICE score when using only the CE loss. We get the best DICE score for the weights pair (, ) = (0.7, 0.3) which we have used in all of our experiments.   



\section{Supplementary Materials}
We make our source code publicly available at .









\end{document}
