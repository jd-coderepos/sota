\section{Experiments}
\label{sec:exp}
{
In the experiments, we denote our proposed method as ReViSE (Robust sEmi-supervised Visual-Semantic Embeddings). Extensive experiments on zero and few-shot image recognition and retrieval tasks are conducted using two benchmark datasets: Animals with Attributes ($\mathsf{AwA}$) \cite{lampert2014attribute} and Caltech-UCSD Birds 200-2011 ($\mathsf{CUB}$) \cite{WelinderEtal2010}. $\mathsf{CUB}$ is a fine-grained dataset in which the objects are both visually and semantically very similar, while $\mathsf{AwA}$ is a more general concept dataset. We use the same $\textit{training (+validation)/ test}$ splits as in \cite{akata2015evaluation,xian2016latent}. Table \ref{tbl:dataset_stat} lists the statistics of the datasets.

\begin{table}[t!]
\centering
\caption{\footnotesize The statistics of $\mathsf{AwA}$ and $\mathsf{CUB}$ datasets. Images and classes are disjoint across $\textit{training(+ validation)}$ / $\textit{test}$ split.}
\vspace{1mm}
\scalebox{0.8}
{
\begin{tabular}{|c||cc||cc|}
\hline
\multirow{2}{*}{$\mathsf{Dataset}$} & \multicolumn{2}{|c||}{$\textit{training (+ validation)}$} & \multicolumn{2}{c|}{$\textit{test}$}    \\ 
                         & \# of images          & \# of classes         & \# of images & \# of classes \\ \hline \hline
$\mathsf{AwA}$ \cite{lampert2014attribute}                     & 24293                 & 40                    & 6180         & 10            \\$\mathsf{CUB}$ \cite{WelinderEtal2010}                     & 8855                  & 150                   & 2931         & 50            \\ \hline
\end{tabular}
}
\label{tbl:dataset_stat}
\vspace{-4mm}
\end{table}


To verify the performance of our method, we consider two state-of-the-art deep-embeddings methods: CMT \cite{socher2013zero} and DeViSE \cite{frome2013devise}. CMT and DeViSE can be viewed as a special case of our proposed method with $\alpha = 0$ (without using unsupervised objective in eq.~\eqref{eq:all}). The difference between them is that DeViSE learns a nonlinear transformation on raw visual images and textual attributes for the alignment purpose, while CMT only learns the nonlinear transformation from visual to semantic embeddings. 

We choose GoogLeNet \cite{szegedy2015going} as the CNN model in DeViSE, CMT, and our architecture. For the textual attributes of classes, we consider three alternatives: human annotated attributes ($\mathit{att}$) \cite{lampert2014attribute}, Word2Vec attributes ($\mathit{w2v}$) \cite{mikolov2013distributed}, and Glove attributes ($\mathit{glo}$) \cite{pennington2014glove}. $\mathit{att}$ are continuous attributes judged by humans: $\mathsf{CUB}$ contains 312 attributes and $\mathsf{AwA}$ contains 85 attributes. $\mathit{w2v}$ and $\mathit{glo}$ are unsupervised methods for obtaining distributed text representations of words. We use the pre-extracted Word2Vec and Glove vectors from Wikipedia provided by \cite{akata2015evaluation,xian2016latent}. Both $\mathit{w2v}$ and $\mathit{glo}$ are 400-dim. features.

\subsection{Network Design and Training Procedure}
\label{subsec:train_pro}
{
	Please see Supplementary for the design details of ReViSE and its parameters. Note that we report results averaged over $10$ random trials.

}

\begin{table}[t!]
\centering
\caption{\footnotesize Zero-shot recognition using top-1 classification accuracy (\%). $\textit{att}$ attributes are used to describe each category.}
\vspace{1mm}
\scalebox{0.72}
{
\begin{tabular}{|c||ll||ll||c|}
\hline
$\mathsf{Dataset}$ & \multicolumn{2}{c||}{$\mathsf{AwA}$}                                                      & \multicolumn{2}{c||}{$\mathsf{CUB}$}                                                     & average                          \\
                    $\textit{recognition for}$    & \multicolumn{1}{c}{$\mathbf{V_{ut}}$} & \multicolumn{1}{c||}{$\mathbf{V_{te}}$} & \multicolumn{1}{c}{$\mathbf{V_{ut}}$} & \multicolumn{1}{c||}{$\mathbf{V_{te}}$} & top-1 acc. \\ \hline\hline
\multicolumn{6}{|c|}{using only labeled training data}                                                                                                                                                                                     \\ \hline \hline
DeViSE \cite{frome2013devise}                         &        60.8        &          63.0         &       38.9       &              36.8            &        49.9                       \\
CMT \cite{socher2013zero}                 &    59.3      &    61.6   &         41.1   &        40.6         &    50.7                 \\
ReViSE$^a$        &     60.3      &      61.2        &    46.4      &        45.0      &        53.2              \\
ReViSE$^b$        &     64.5           &     65.0         &     49.6         &              47.3            &      56.6               \\ \hline \hline
\multicolumn{6}{|c|}{using both labeled and unlabeled training data}   \\ \hline \hline
DeViSE* \cite{frome2013devise}             &   76.0        &       63.7       &     37.2    &          36.2          &    53.3             \\
CMT* \cite{socher2013zero}       &    77.8     &       58.5     &     39.9      &              39.8          &   54.0              \\
ReViSE$^c$        &   64.7         &     67.6        &   52.2     &    48.2   &       58.2        \\
ReViSE       &    {\bf 78.0  }      &   {\bf 68.6   }   &     {\bf 56.6   }   &  {\bf 49.6    }  &        {\bf 63.2}           \\ \hline
\end{tabular}
}
\label{tbl:induc_zero_recog}
\vspace{-1mm}
\end{table}

\begin{table}[t!]
\centering
\caption{\footnotesize Zero-shot retrieval using mean Average Precision (mAP) (\%). $\textit{att}$ attributes are used to describe each category.}
\vspace{1mm}
\scalebox{0.72}
{
\begin{tabular}{|c||ll||ll||c|}
\hline
$\mathsf{Dataset}$ & \multicolumn{2}{c||}{$\mathsf{AwA}$}                                                      & \multicolumn{2}{c||}{$\mathsf{CUB}$}                                                     & average                          \\
                    $\textit{retrieval for}$    & \multicolumn{1}{c}{$\mathbf{V_{ut}}$} & \multicolumn{1}{c||}{$\mathbf{V_{te}}$} & \multicolumn{1}{c}{$\mathbf{V_{ut}}$} & \multicolumn{1}{c||}{$\mathbf{V_{te}}$} & mAP \\ \hline\hline
\multicolumn{6}{|c|}{using only labeled training data}                                                                                                                                                                                     \\ \hline \hline
DeViSE \cite{frome2013devise}                         &        60.5        &      61.6       &         32.6           &              31.5            &        46.6           \\
CMT \cite{socher2013zero}                 &         58.8          &       61.4     &       35.8   &        37.0         &   48.3          \\
ReViSE$^a$        &     60.2        &    60.1       &    33.1      &       32.0    &       46.4          \\
ReViSE$^b$        &     64.4      &     63.2     &     36.0        &              37.4            &      50.3             \\ \hline \hline
\multicolumn{6}{|c|}{using both labeled and unlabeled training data}   \\ \hline \hline
DeViSE* \cite{frome2013devise}             &    65.6      &       58.9       &           35.4     &          31.3          &    47.8                \\
CMT* \cite{socher2013zero}       &    63.5    &    57.1    &     39.7     &              38.0          &   49.6            \\
ReViSE$^c$        &    66.8         &     63.6     &     39.4      &    37.5   &       51.8       \\
ReViSE       &    {\bf 74.2 }       &     {\bf 68.1 }   &    {\bf 47.6 }      &  {\bf 40.4  }    &        {\bf 57.6}      \\ \hline
\end{tabular}
}
\label{tbl:induc_zero_retr}
\vspace{-4mm}
\end{table}



\subsection{Zero-Shot Learning}
\label{subsec:zero_learn}
{
	Following the partitioning strategy of \cite{akata2015evaluation,xian2016latent}, we split $\mathsf{AwA}$ dataset into {\em 30/10/10} classes and $\mathsf{CUB}$ dataset into {\em 100/50/50} classes for {\em labeled training}/ {\em unlabeled training}/ {\em test} data. We adopt $\mathit{att}$ attributes as a textual description of each class. For zero-shot learning, not only the labels of images are unknown in the {\em unlabeled training} and {\em test} set, but classes are also disjoint across {\em labeled training}/ {\em unlabeled training}/ {\em test} splits. 


	To verify how unlabeled training data could benefit the learning of ReViSE, we provide four variants: ReViSE$^a$, ReViSE$^b$, ReViSE$^c$, and ReViSE. ReViSE$^a$ is when we only consider supervised objective. That is, $\alpha = 0$ in eq.~\eqref{eq:all}. ReViSE$^b$ is when we further take unsupervised objective in {\em labeled training data} into account; that is, only $\mathcal{L}_{reconstruct}$ and $\mathcal{L}_{MMD}$ are considered in $\mathcal{L}_{unsupervised}$ (see eq.~\eqref{eq:unsup}) for labeled training data. Next, for ReViSE$^c$, we consider {\em unlabeled training data} in $\mathcal{L}_{unsupervised}$ without unsupervised-data adaptation technique (setting $\beta = 0$). Last, ReViSE denotes our complete training architecture. 

	For completeness, we also consider the technique of unsupervised-data adaptation inference (see section~\ref{subsec:learning}) for DeViSE \cite{frome2013devise} and CMT \cite{socher2013zero}. In other words, we also evaluate how DeViSE and CMT benefit from the unlabeled training data. We adopt the same procedure as in eq.~\eqref{eq:test_adaptation} and report results as DeViSE* and CMT*, respectively.


	Similar to \cite{zhang2015zero,zhang2016zero1,zhang2016zero2}, the results and comparisons are reported using top-1 classification accuracy {\bf (top-1)} (see Table \ref{tbl:induc_zero_recog}) and mean average precision {\bf (mAP)} (see Table \ref{tbl:induc_zero_retr}) for recognition and retrieval tasks, respectively, on the unlabeled training and test images. To be more specific, we define the prediction score as \scalebox{0.9}{$\hat{y}_{i,c}^{(\cdot)} = \left \langle f'_v(\tilde{v}_{h,i}^{(\cdot)}), f'_t(t_{h,c}^{(\cdot)}) \right \rangle$} for a given image $v_i^{(\cdot)}$ and textual attributes $t_{c}^{(\cdot)}$ for class $c$. Results are provided after ranking $\hat{y}_{i,c}^{(\cdot)}$ on all unlabeled training or test classes. 

	Table \ref{tbl:induc_zero_recog} and \ref{tbl:induc_zero_retr} list the results for our zero-shot recognition and retrieval experiments. We first observe that {\bf NOT} all the methods benefit from using unlabeled training data during training. For example, in $\mathsf{AwA}$ dataset for test images $\mathbf{V_{te}}$, there is a $2.7\%$ retrieval deterioration from DeViSE to DeViSE* and a $3.1\%$ recognition deterioration from CMT to CMT*. On the other hand, our proposed method enjoys $2.6\%$ recognition improvement and $0.4\%$ retrieval improvement from ReViSE$^b$ to ReViSE$^c$. This shows that the learning method of our proposed architecture can actually benefit from unlabeled training data $\mathbf{V_{ut}}$ and $\mathbf{T_{ut}}$. 

	Next, we examine different variants in our proposed architecture. Comparing the average results from ReViSE$^a$ to ReViSE$^b$, we observe $3.4\%$ recognition improvement and $3.9\%$ retrieval improvement. This indicates that taking unsupervised objectives $\mathcal{L}_{reconstruct}$ and $\mathcal{L}_{MMD}$ into account results in learning better feature representations and thus yields a better recognition/ retrieval performance. Moreover, when unsupervised-data adaptation technique is introduced, we enjoy $5.0\%$ average recognition improvement and $5.8\%$ average retrieval improvement from ReViSE$^c$ to ReViSE. It is worth noting that the significant performance improvement for unlabeled training images $\mathbf{V_{ut}}$ further verifies that our unsupervised-data adaptation technique leads to a more accurate prediction on $\mathbf{V_{ut}}$.
}

\subsection{Transductive Zero-Shot Learning}
\label{subsec:new_zero_learn}
{

	In this subsection, we extend our experiments to a transductive setting, where test data are available during training. Therefore, the test data can now be regarded as the unlabeled training data ($\mathbf{V_{tr}} = \mathbf{V_{ut}}$ and $\mathbf{T_{tr}} = \mathbf{T_{ut}}$). To perform the experiments, as in Table \ref{tbl:dataset_stat}, we split $\mathsf{AwA}$ dataset into {\em 40/10} disjoint classes and $\mathsf{CUB}$ dataset into {\em 150/50} disjoint classes for {\em labeled training}/ {\em test} data.


	In order to evaluate different components in ReViSE, we further provide two variants: ReViSE$^{\dagger}$ and ReViSE$^{\dagger\dagger}$. ReViSE$^{\dagger}$ is when we consider no distributional matching between the codes across modalities ($\beta = 0$). ReViSE$^{\dagger\dagger}$ is when we further consider no contractive loss in our visual auto-encoder ($\beta = \gamma = 0$). Similar to previous subsection, we also consider DeViSE*, CMT*, and ReViSE$^{c}$ to evaluate the effect of our unsupervised-data adaptation inference.





\begin{table}[t!]
\centering
\caption{\footnotesize Transductive zero-shot recognition using top-1 classification accuracy (\%). }
\vspace{1mm}
\scalebox{0.72}
{
\begin{tabular}{|c||lll||lll||c|}
\hline
$\mathsf{Dataset}$ & \multicolumn{3}{c||}{$\mathsf{AwA}$}                                                      & \multicolumn{3}{c||}{$\mathsf{CUB}$}                                                     & average                          \\
                    $\textit{attributes}$    & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & top-1 acc. \\ \hline\hline
\multicolumn{8}{|c|}{test data not available during training}                                                                                                                                                                                     \\ \hline \hline
DeViSE \cite{frome2013devise}                  &           67.4               &           67.0             &        66.7                  &            40.8             &          28.8               &              25.6            &        49.3                         \\
CMT \cite{socher2013zero}                    &               67.6           &             69.5            &           68.0               &               42.4          &             29.6            &            25.7              &        50.5                         \\ \hline \hline
\multicolumn{8}{|c|}{test data available during training}                                                                                                                                                                                        \\ \hline \hline
DeViSE* \cite{frome2013devise}                &            90.7              &           84.8            &        88.0                  &           41.4              &            31.6             &              26.9            &       60.6                          \\
CMT* \cite{socher2013zero}                   &              89.4            &              87.8           &             81.8             &               43.1          &             31.8          &              28.9            &         60.5                       \\
ReViSE$^{\dagger\dagger}$             &            92.1              &             92.3            &         90.3      &       62.4          &       30.0            &         27.5       &           65.8                 \\
ReViSE$^{\dagger}$ &              92.8            &              92.6           &           91.7               &     62.7           &      31.8             &      28.9          &       66.8                     \\
ReViSE$^{c}$             &            73.0              &             67.0            &         73.4      &       53.7          &       26.4            &         28.2       &           53.6                \\
ReViSE                &           {\bf 93.4}               &           {\bf 93.5}      &            {\bf 92.2}       &         {\bf 65.4}         & {\bf 32.4}        &           {\bf 31.5}        &      {\bf 68.1}                 \\ \hline
\end{tabular}
}
\label{tbl:zero_recog}
\vspace{-4mm}
\end{table}



\vspace{0.1in}
\hspace{-5mm} {\bf Zero-Shot Recognition}:
Table \ref{tbl:zero_recog} reports top-1 classification accuracy. Observe that ReViSE clearly outperforms other state-of-the-art methods by a large margin. On average, we have at least $17\%$ gain compared to the methods without using unsupervised objective and $7.5\%$ gain compared to DeViSE* and CMT*. Note that all the methods work better on human annotated attributes ($\textit{att}$) than on unsupervised attributes ($\textit{w2v}$ and $\textit{glo}$) in $\mathsf{CUB}$ dataset. One possible reason is that for visually and semantically similar classes in a fine-grained dataset ($\mathsf{CUB}$), attributes obtained in an unsupervised way ($\textit{glo}$ word vectors) cannot fully differentiate between them. 
Nonetheless, for the more general concept dataset $\mathsf{AwA}$, using either supervised or unsupervised textual attributes, the performance does not differ by that much. 
For instance, our method achieves comparable performance using $\textit{att}$, $\textit{w2v}$, and $\textit{glo}$ ($93.4\%$, $93.5\%$, and $92.2\%$ top-1 classification accuracy) on $\mathsf{AwA}$ dataset.


The recognition performance for DeViSE* and CMT* ($60.6\%$ and $60.5\%$ on average) compared to DeViSE and CMT ($49.3\%$ and $50.5\%$ on average) further verifies that using unsupervised-data adaptation inference technique does benefit transductive zero-shot recognition. 
Furthermore, all of the variants of ReViSE using unsupervised-data adaptation inference (ReViSE$^{\dagger\dagger}$, ReViSE$^\dagger$, and ReViSE itself) have noticeable improvement over DeViSE* and CMT*. 
This shows that the proposed model succeeds in leveraging unsupervised information in test data for constructing more effective cross-modal embeddings.

Next, we evaluate the effects of different components designed in our architecture. First of all, we compare the results between ReViSE$^\dagger$ (set $\beta = 0$) and ReViSE. The performance gain ($66.8\%$ to $68.1\%$ on average) indicates that minimizing MMD distance between visual and textual codes enables our model to learn more robust visual-semantic embeddings. In other words, we can better associate cross-modal information when we match the distributions across visual and textual domains (please refer to Supplementary for the study of MMD distance). Second, we observe that, without contractive loss, performance slightly drop from $66.8\%$ (ReViSE$^\dagger$) to $65.8\%$ (ReViSE$^{\dagger\dagger}$). This is not surprising since the contractive auto-encoder aims at learning less varying features/codes with similar visual input, and therefore we can expect to learn more robust visual codes. 
Finally, similar to the observations found in comparing DeViSE/CMT to DeViSE*/CMT*, the {\em unsupervised-data adaptation inference} in ReViSE substantially improves the average top-1 classification 
accuracy from $53.6\%$ (ReViSE$^c$) to $68.1\%$ (ReViSE).
Please see Supplementary material for more detailed comparisons to the following non-deep-embeddings methods: SOC \cite{palatucci2009zero}, ConSE \cite{norouzi2013zero}, SSE \cite{zhang2015zero}, SJE \cite{akata2015evaluation}, ESZSL \cite{romera2015embarrassingly}, JLSE \cite{zhang2016zero1}, LatEm \cite{xian2016latent}, Sync \cite{changpinyo2016synthesized}, MTE \cite{bucher2016improving}, TMV \cite{fu2015transductive}, and SMS \cite{guo2016transductive}.




\vspace{0.1in}
\hspace{-5mm} {\bf Zero-Shot Retrieval}:
In Table \ref{tbl:zero_retr}, we report zero-shot retrieval results by measuring the retrieval performance by mean average precision (mAP). On average, methods that leverage unsupervised information yield better performance compared to the methods using no unsupervised objective. However, in few cases, the performance drops when we take unsupervised information into account. For example, on $\mathsf{CUB}$ dataset, DeViSE* performs unfavorably compared to DeViSE when $\textit{w2v}$ and $\textit{glo}$ word embeddings are used as textual attributes. 

Overall, our method does help improve zero-shot retrieval by at least $14.1\%$ compared to CMT*/DeViSE* and $21.5\%$ compared to CMT/DeViSE. It clearly demonstrates the effectiveness of leveraging unsupervised information for improving zero-shot retrieval (please see Supplementary for the plot of precision-recall curves).

In addition to quantitative results, we also provide qualitative results of ReViSE. Fig. \ref{fig:retr_image} is the image retrieval experiments
for classes {\em Chestnut\_sided\_Warbler} and {\em White\_eyed\_Vireo}. Given a class embedding, the nearest image neighbors are retrieved based on the cosine similarity between transformed visual and textual features. We consider two conditions: images from the same class and images from all test classes. In {\em Chestnut\_sided\_Warbler}, most of the images ($71.7\%$) are correctly classified, and we also observe that three nearest image neighbors are also in {\em Chestnut\_sided\_Warbler}. On the other hand, only $43.3\%$ images are correctly classified in {\em White\_eyed\_Vireo}, and two of the three nearest image neighbors are form wrong class {\em Wilson\_Warbler}.

 



\begin{figure}[t!]
\includegraphics[width=0.47\textwidth]{retrieved_image.pdf}
\caption{\footnotesize Images-retrieval experiments for $\mathsf{CUB}$ with $\textit{att}$ attributes.}
\label{fig:retr_image}
\vspace{-0.1in}
\end{figure}



\begin{table}[t!]
\centering
\caption{\footnotesize Transductive zero-shot retrieval using mean Average Precision (mAP) (\%).}
\vspace{1mm}
\scalebox{0.72}
{
\begin{tabular}{|c||lll||lll||c|}
\hline
$\mathsf{Dataset}$ & \multicolumn{3}{c||}{$\mathsf{AwA}$}                                                      & \multicolumn{3}{c||}{$\mathsf{CUB}$}                                                     & average                          \\
                    $\textit{attributes}$    & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & mAP \\ \hline\hline
\multicolumn{8}{|c|}{test data not available during training}                                                                                                                                                                                     \\ \hline \hline
DeViSE \cite{frome2013devise}                  &              67.5            &             67.6           &         66.2                 &            31.9             &           26.6              &           24.5               &        47.4              \\
CMT \cite{socher2013zero}                    &              66.3            &             70.6            &         69.5                 &             39.3            &             25.2            &           21.9              &     48.8                       \\ \hline \hline
\multicolumn{8}{|c|}{test data not available during training}                                                                                                                                                                                        \\ \hline \hline
DeViSE* \cite{frome2013devise}                &              82.3            &              78.0          &        84.4                  &             36.9            &            25.8             &             21.3             &       54.8               \\
CMT* \cite{socher2013zero}                   &              85.8            &             77.3            &       73.0                   &              44.1           &            28.9             &             28.1            &     56.2                     \\
ReViSE$^{\dagger\dagger}$&            96.7              &            96.8             &         95.1          &     60.7             &       29.4           &        27.2        &            67.7            \\
ReViSE$^{\dagger}$&              97.2            &               96.9          &          96.4                &    62.0           &       29.8            &        28.2       &        68.4          \\
ReViSE$^{c}$             &            73.0              &             67.0            &         73.4      &       53.7          &       26.4            &         28.2       &           53.6                \\
ReViSE                &          {\bf 97.4}                &          {\bf 97.4}               &         {\bf 96.7}            &      {\bf 68.9}           &       {\bf 30.5}      &        {\bf 30.9}          &          {\bf 70.3}           \\ \hline
\end{tabular}
}
\vspace{-4mm}
\label{tbl:zero_retr}
\end{table}

\vspace{0.1in}
\hspace{-5mm} {\bf Availability of Unlabeled Test Images}:
We next evaluate the performance of our method w.r.t. to the availability of test images for unsupervised objective (see Fig. \ref{fig:test_unsup}) on $\mathsf{CUB}$ dataset with $\textit{att}$ attributes. We alter the fraction $p$ of unlabeled test images used in the training stage from $0\%$ to $100\%$ by a step size of $10\%$. That is, in eq. \eqref{eq:unsup}, only $p$ portion (randomly chosen) of test images contributes to $\mathcal{L}_{unsupervised}$. 
Fig. \ref{fig:test_unsup} clearly indicates the performance increases when $p$ increases. That is, with more unsupervised information (test images) available, our model can better associate the supervised and unsupervised data.
Another interesting observation is that with only $40\%$ test images available, ReViSE achieves favorable performance on both transductive zero-shot recognition and retrieval. 

\hspace{-5mm} {\bf Expand the test-time search space:} Note that most of the methods \cite{frome2013devise,socher2013zero,norouzi2013zero,zhang2015zero,akata2015evaluation,zhang2016zero1,xian2016latent,changpinyo2016synthesized,fu2015transductive} consider that, at test time, queries come from only test classes. For $\mathsf{AwA}$ dataset with $\textit{att}$ attributes, we expand the test-time search space to all training and test classes and perform transductive zero-shot recognition for DeViSE*, CMT*, and ReViSE. We discover severe performance drops from $90.7\%$, $89.4\%$, and $93.4\%$ to $47.4\%$, $45.8\%$, and $42.5\%$. Similar results can also be observed in other non-deep-embeddings methods. Although challenging, it remains interesting to consider this generalized zero-/few-shot learning setting in our future work.


\begin{figure}[t!]
\centering
\includegraphics[width=0.43\textwidth]{test_unsup.pdf}
\caption{\footnotesize Fraction $p$ of test images used for training ReViSE on transductive (a) zero-shot recognition (b) zero-shot retrieval for $\mathsf{CUB}$ dataset with $\textit{att}$ attributes.}
\label{fig:test_unsup}
\vspace{-3mm}
\end{figure}


}

\begin{figure*}[t!]
\vspace{-3mm}
\centering
\includegraphics[width=0.8\textwidth]{orig_recon_code.pdf}
\caption{\footnotesize (a) Original CNN features (b) Reconstructed features (c) Visual codes for $\mathsf{AwA}$ dataset in ReViSE under transductive zero-shot setting. We use $\textit{glo}$ as our textual attributes for classes. Different colors denote different classes. Best viewed in colors.}
\label{fig:orig_recon_code}
\vspace{-4mm}
\end{figure*}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.8\textwidth]{devise_cmt_ours.pdf}
\caption{\footnotesize Output features of : (a) DeViSE* (b) CMT* (c) ReViSE. $\textit{glo}$ attributes are used on $\mathsf{AwA}$ dataset under transductive zero-shot setting. Different colors denote different classes. Best view in colors.}
\label{fig:devise_cmt_ours}
\vspace{-3mm}
\end{figure*}

\subsection{From Zero to Few-Shot Learning}
\label{subsec:few_shot}
{

In this subsection, we extend our experiments from transductive zero-shot to transductive few-shot learning. 
Compared to zero-shot learning, few-shot learning allows us to have a few labeled images in our test classes. Here, 3 images are randomly chosen to be labeled per test category. We use the same performance comparison metrics as in Sec. \ref{subsec:zero_learn} to report the results.

\vspace{0.1in}
\hspace{-5mm} {\bf Transductive Few-Shot Recognition and Retrieval}:
Tables \ref{tbl:few_recog} and \ref{tbl:few_retriev} list the results of transductive few-shot recognition and retrieval tasks. Generally speaking, ReViSE achieves the best performance compared to its variants and other methods. Moreover, as expected, when we compare the results with transductive zero-shot recognition (Table \ref{tbl:zero_recog}) and retrieval (Table \ref{tbl:zero_retr}), every methods perform better when few (i.e., $3$) labeled images are observed in the test classes. For example, for $\mathsf{CUB}$ dataset with $\textit{w2v}$ attributes, there is a $22.5\%$ recognition improvement for CMT* and a $32.3\%$ retrieval improvement for ReViSE. 

We also observe that the performance gap between our proposed ReViSE and other methods is reduced compared to transductive zero-shot learning. For instance, in average retrieval performance, ReViSE has $15.5\%$ mAP improvement over DeViSE* under zero-shot experiments, while only $9.3\%$ improvement under few-shot experiments.  


\begin{table}[t!]
\centering
\caption{\footnotesize Few-shot recognition comparison using top-1 classification accuracy (\%). For each test class, 3 images are randomly labeled, while the rest are unlabeled.}
\vspace{1mm}
\scalebox{0.72}
{
\begin{tabular}{|c||lll||lll||c|}
\hline
$\mathsf{Dataset}$ & \multicolumn{3}{c||}{$\mathsf{AwA}$}                                                      & \multicolumn{3}{c||}{$\mathsf{CUB}$}                                                     & average                          \\
                    $\textit{attributes}$    & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & top-1 acc. \\  \hline \hline
\multicolumn{8}{|c|}{test data not available during training}                                                                                                                                                                                        \\ \hline \hline
DeViSE \cite{frome2013devise}     &   80.9     &   75.3 &  79.4  & 54.0  &      45.7  &  46.0   &  63.6 \\
CMT \cite{socher2013zero}    &  85.1  &    83.4    &  84.3   &    56.7     &    53.4   &    52.0   & 69.2  \\ \hline \hline
\multicolumn{8}{|c|}{test data available during training}                                                                                                                                                                                        \\ \hline \hline
DeViSE* \cite{frome2013devise}     &    92.6      &      91.1  &   91.3     &   57.5     &      50.7   &   52.9       &    72.7   \\
CMT* \cite{socher2013zero}    &  90.6  &    90.2    &  91.1   &    62.5     &    54.3   &    55.4   &  74.0    \\
ReViSE$^{\dagger\dagger}$ &  93.3    &    93.3     &   93.1   &   66.9      &    57.6    &   59.0    &   77.2  \\
ReViSE$^{\dagger}$ &   93.3  &  93.8  &     93.5       &  67.7  &   59.6    &     60.0    &   78.0  \\
ReViSE$^{c}$ &   87.8 &  88.7 &    90.2     &  61.1 &   55.3  &  55.0   &  73.0 \\
ReViSE     &    {\bf 94.2}    &    {\bf 94.1}   &  {\bf 94.4}  &   {\bf 68.4}   &  {\bf 59.9} &  {\bf 61.7}    &    {\bf 78.8}    \\ \hline
\end{tabular}
}
\vspace{-4mm}
\label{tbl:few_recog}
\end{table}

}

\subsection{t-SNE Visualization}
\label{subsec:tsne}
{

Figure~\ref{fig:orig_recon_code} further shows the t-SNE \cite{maaten2008visualizing} visualization for the original CNN features, the reconstructed visual features $r_v(\tilde{v}^{(te)})$, and the visual codes $\tilde{v}_h^{(te)}$ on $\mathsf{AwA}$ dataset with $\textit{glo}$ attributes under transductive zero-shot setting. 
First of all, observe that both the reconstructed features and the visual codes have more separate clusters over different classes, which suggest ReViSE has learned useful representations. Another interesting observation is that the affinities between classes might change after learning visual codes. For example, ``leopard'' images (green dots) are near ``humpback whale'' images (light purple dots) in the original CNN feature space. However, in the visual code space, leopard images are far from humpback whale images. One possible explanation is that we know leopard is semantically distinct from humpback whale, and thus their semantic attributes must also be very different. This leads to different image clusters in our designed framework.

Next, we provide the t-SNE visualization on the output visual test scores $f_v(\mathbf{V_{te}})$ for DeViSE*, CMT*, and ReViSE in Fig. \ref{fig:devise_cmt_ours}. Clearly, ReViSE can better separate instances from different classes. 



}
\begin{table}[t!]
\centering
\caption{\footnotesize Few-shot retrieval comparison using mean Average Precision (mAP) (\%). For each test class, 3 images are randomly labeled, while the rest are unlabeled. }
\vspace{1mm}
\scalebox{0.72}
{
\begin{tabular}{|c||lll||lll||c|}
\hline
$\mathsf{Dataset}$ & \multicolumn{3}{c||}{$\mathsf{AwA}$}                                                      & \multicolumn{3}{c||}{$\mathsf{CUB}$}                                                     & average                          \\
                    $\textit{attributes}$    & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & \multicolumn{1}{c}{$\textit{att}$} & \multicolumn{1}{c}{$\textit{w2v}$} & \multicolumn{1}{c||}{$\textit{glo}$} & mAP \\  \hline \hline
\multicolumn{8}{|c|}{test data not available during training}                                                                                                                                                                                        \\ \hline \hline
DeViSE \cite{frome2013devise}     &   85.0  &   79.3  &  84.9  &   46.4    &    42.6   & 42.9       &  63.5  \\
CMT \cite{socher2013zero}    &  88.4  &    88.2    &  89.2   &    58.5     &    54.0   &    52.7   &  71.8   \\ \hline \hline
\multicolumn{8}{|c|}{test data available during training}                                                                                                                                                                                        \\ \hline \hline
DeViSE* \cite{frome2013devise}      &      96.7   &     95.5      &   95.8     &47.5     &   49.2   &   51.6    & 72.7   \\
CMT* \cite{socher2013zero}    &   95.3   &    94.8       &  95.8  &   60.0  &  54.7    &    56.4  &  76.2   \\
ReViSE$^{\dagger\dagger}$ &  97.2   &   97.1     &    97.1   &   71.2   &   59.4   &   61.4  &    80.6     \\
ReViSE$^{\dagger}$ &   97.3    &   97.5    &   97.4      &   72.5    &   61.4   &  62.5    &   81.4  \\
ReViSE$^{c}$ &  92.3  & 93.0  &  94.6    &  60.8 &  55.0 &  57.1   &  75.5 \\
ReViSE   &     {\bf 97.8}   &  {\bf 97.7}    &  {\bf 97.8}  &    {\bf 72.9}  &    {\bf 62.8}  &   {\bf 63.0}    &    {\bf 82.0} \\ \hline
\end{tabular}
}
\vspace{-4mm}
\label{tbl:few_retriev}
\end{table}



}
