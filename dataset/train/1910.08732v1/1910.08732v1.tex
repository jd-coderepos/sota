\section{Experiments}
\label{sec:experiments}
\noindent
\textbf{Implementation details}
The audio network  is based on that of \cite{kumar2018knowledge}, and is trained on the spectrogram of the audio clips in the train set of our dataset. We obtain the audio features after seven \texttt{conv} layers of the network, and average them to obtain D vector. The video network, denoted as  is an inflated 3D CNN network which is pretrained on the Kinetics dataset \cite{CarreiraCVPR2017} and a large video dataset of action recognition. We also obtain the  video features form the layer before the classification layer and average them to get a vector of D. Finally the text network, denoted as  is the well known \texttt{word2vec} network pretrained on Wikipedia \cite{mikolov2018advances} with output dimension of D.

The projection model for text embeddings was fixed to be a single layer network, where as for the audio and video was fixed to be a two layer network, with the output dimensions matching for all. In order to find the seen/unseen class bias parameter  we divide the maximum and minimum possible value of  into  equal intervals and then evaluate performances on the \textit{val} set. We chose the best performing  among those. 
\vspace{0.5em}\\
\textbf{Evaluation and performance metrics.}
We report the mean class accuracy (\% mAcc) for the classification task and the mean average precision (\% mAP) for the retrieval tasks. The performance for the seen classes (denoted as S) is classified (retrieved) over all the classes (seen and unseen), and that for the unseen classes (denoted as U) are also reported. The harmonic mean HM of S and U indicates how well the system performs on both seen and unseen categories on average. For classification, we classify each test example, and for retrieval, we perform leave-one-out testing, \ie each test example is considered as a query with the rest being the gallery. The performance reported is (mean class) averaged. 
\vspace{0.5em}\\
\textbf{Methods reported.}
We report performances of audio and video only methods, \ie only the respective modality is used to test and train. We also report a naive combination by concatenation of features from audio and video modalities before learning the projection to the common space. This method allows zeroshot classification and retrieval only when both the modalities are available, and it does not allow crossmodal retrieval at all. We then report performances with the proposed Coordinated Joint Multimodal Embeddings (CJME) method, when modality attention is used and when it is not used. In either of the cases, we can choose dominant modality (or not) based on the  value (eq.~\ref{eqn:alpha}). We report with both the cases.

We also compare our approach to two other baseline methods, namely pre-trained features and GCCA. In `Pre-trained' method the raw features obtained from the individual modality pre-trained network are directly used for retrieval as both are of same dimensions. This can be considered as one of the lower bound since no common projection is learned for the different modalities in this case. GCCA \cite{kettenring1971canonical} or Generalized Canonical Correlation Analysis is the standard extension of the Canonical Correlation Analysis (CCA) method from two-set method to multi-set method, where the correlation between the example pair from each sets are maximized. We use here the GCCA to maximize the correlation between all the three modalities (text, audio and video) for every example triplet in the dataset.


\subsection{Quantitative Evaluation}
\label{sec:quant}
\noindent
\textbf{Evaluation of calibrated stacking performance.} 
We have shown the improvement in performance with the approach of calibrated stacking in
Fig.~\ref{fig:bias_corr}. This shows the performances with different values of the bias parameter, \ie accuracies for seen and unseen classes, as well as their harmonic means.
We observe that the performance increases with the initial increase in bias, and then falls after a certain point as expected. We choose the best performing value of the class bias on the \emph{val} set and then fix it for the experiments on \emph{test} set.

\begin{figure}
\centering
	\includegraphics[width=0.49\columnwidth]{plots/layer-0_bias.png}
	\includegraphics[width=0.49\columnwidth]{plots/layer-1_bias.png}
\caption{Effect of classification performance for model M1 (left) and model M2 (right) with different values of bias parameter.
}
\label{fig:bias_corr}
\end{figure}
\begin{table}
\centering
\resizebox{.9\columnwidth}{!}{
\begin{tabular}{c|c|ccc}
\hline
Train Modality & Test Modality & S & U & HM \\
\hline \hline 
audio          & audio & 28.35 & 18.35 & 22.22 \\
CJME          & audio & 25.58  & 20.30 & 22.64 \\
\hline
video         & video & 43.27 & 27.11 & 33.34 \\
CJME  & video & 41.53 & 28.76 & 33.99 \\
\hline
both (concat) & both & 45.83 & 27.91 & 34.70 \\
CJME & both   & 30.29 & 31.30 & 30.79 \\
\hline
CJME (no attn) & audio or video & 31.72 & 26.31 & 28.76  \\
CJME (w/ attn) & audio or video & 41.07 & 29.58 & 34.39\\
\hline
\end{tabular}
}
\caption{Zeroshot classification performances (\% mAcc) achieved with audio only, video only, and both audio and video used for training and test. Note that the audio and video concatenation model requires both the modalities to be available during testing.}
\label{tab:zsc}
\vspace{-1 em}
\end{table}
\vspace{0.5em}
\textbf{Zeroshot audio-visual classification.}
Tab.~\ref{tab:zsc} gives the performances of the different models for the task of zeroshot classification. We make multiple observations here. The video modality performs better than the audio modality for the task ( \vs  HM), which is interesting as the original dataset was constructed for audio event detection. We also observe that when both audio and video modalities are used by simply concatenating the feature from the respective pre-trained networks, the performance increases to . This shows that adding the audio modality is helpful for zeroshot classification. Our coordinated joint multimodal embeddings (denoted CJME in the table) improves the performance of video and audio only models on the respective test sets by modest but consistent margins. This highlights the efficacy of the proposed method to learn joint embeddings which are comparable (slightly better) than individually trained models.

The performance of the proposed method is lower without attention learning and selective modality based test time prediction \cf the concatenated input model ( \vs ), but is comparable to it when trained and tested with attention (). Also, when we do not train for attention but use selective modality based prediction the performance falls (). Both these comparisons validate that the modality attention learning is an important addition to the base multimodal embedding learning framework.
\vspace{0.5em}\\
\textbf{Zeroshot audio-visual retrieval.}
Table~\ref{tab:zsr} compares the performances of different models for the task of zeroshot retrieval. 
The performance on the unseen classes are quite poor, albeit it is approximately three times the baseline pre-trained performance. This is because of the bias towards the seen classes in generalized ZSL. This happens for classification setting as well but is corrected for explicitly by reducing the scores of the seen classes. However, in a retrieval scenario, since the class of the gallery set member is not known in general, such correction can not be applied. We tried classifying the gallery sets first and then applying the seen/unseen class bias correction, however that did not improve results possibly because of erroneous classifications.

\begin{table}
\centering
\resizebox{.85\columnwidth}{!}{
\begin{tabular}{c|c|ccc}
\hline
Model & Test & S & U & HM \\
\hline \hline
pre-trained        & T\ra A & 3.83 &  1.66 & 2.32 \\
GCCA \cite{kettenring1971canonical}    & T\ra A & 49.84 &  2.39 & 4.56 \\
audio        & T\ra A & 43.16 & 3.34 & 6.20 \\
CJME & T\ra A & 48.24  &  3.32  &  6.21 \\
\hline
pre-trained         & T\ra V & 3.83 & 2.53 & 3.05 \\
GCCA \cite{kettenring1971canonical}    & T\ra V & 57.67 &  3.54 & 6.67 \\
video        & T\ra V & 48.62 & 5.25 & 9.47 \\
CJME & T\ra V & 59.39  & 5.55  & 10.15 \\
\hline
both (concat)& T\ra AV & 63.13 & 7.80 & 13.88 \\
CJME & T\ra AV & 65.45 & 5.40 & 9.97\\
\hline
CJME (no attn) & T\ra A or V & 65.74  &  5.09  &  9.45 \\
CJME (w/ attn) & T\ra A or V & 62.97 & 5.67 & 10.41\\
\hline
\end{tabular}
}
\caption{Zeroshot retrieval performances (\% mAP) achieved by models when audio only, video only, and both audio and video modalities are used for training and test. Note that the audio and video concatenation based model requires both modalities at test time also and can not predict using any single one.}
\label{tab:zsr}
\vspace{-1em}
\end{table}
We observe, from Tab.~\ref{tab:zsr}, similar trends as with zeroshot classification. The proposed CJME performs similar to audio only ( \vs ) and slightly better than video only ( \vs ) models but consistently outperforms both pre-trained and GCCA model. Compared to the audio and video features concatenated model, the performance without modality attention based training are lower ( \vs ) which improve upon using attention at training (), albeit staying a little lower \cf similar in the classification case. We thus conclude that CJME is as good as audio only or video only model and is competitive \cf concatenated features model, while allowing crossmodal retrieval, which we evaluate next.
\begin{table}
\centering
\resizebox{.9\columnwidth}{!}{
\begin{tabular}{c|c|ccc}
\hline
Model & Test & S & U & HM \\
\hline \hline
pre-trained & audio \ra video & 3.61 & 2.37 & 2.86  \\
GCCA \cite{kettenring1971canonical} & audio \ra video & 22.12 & 3.65  & 6.26 \\
CJME & audio \ra video & 26.87  &  4.31  &  7.43 \\
\hline 
pre-trained & video \ra audio & 4.22  & 2.57  & 3.19  \\
GCCA \cite{kettenring1971canonical} & video \ra audio & 26.68 & 2.98 & 5.26 \\
CJME & video \ra audio & 29.33  &  4.35  &  7.58 \\
\hline
\end{tabular}
}
\caption{Zeroshot crossmodal retrieval performances (\% mAP).}
\label{tab:zsr_cross}
\vspace{-1em}
\end{table}
\vspace{0.5em} \\
\textbf{Crossmodal retrieval.}
Since CJME learns to embed both audio and video modality in a common space, it allows for doing crossmodal retrieval from audio to video and vice-versa. Tab.~\ref{tab:zsr_cross} gives the performances of such crossmodal retrieval from audio and video domains. We observe that the retrieval accuracy in the case of crossmodal retrieval are  and  for audio to video and video to audio respectively. Due to the inability to do seen/unseen class bias correction, we observe a large gap between the retrieval performance of seen classes \cf unseen classes, which stays true in the case of crossmodal retrival as well. The performance is still three times better than the raw pre-trained features. We believe these are encouraging initial results on the challenging task of audio-visual crossmodal retrieval on real world unconstrained videos in zeroshot setting.
\begin{table}[t]
 \centering
\resizebox{.75\columnwidth}{!}{
 \begin{tabular}{c|c|c||ccc}
 \hline
 & &  & S & U & HM \\
 \hline \hline
 \xmark & \xmark & \cmark  &  1.26  &  10.13  &  2.24  \\
 \xmark&   \cmark    &   \xmark   &  3.00  &  4.18  &  3.49  \\
 \xmark&   \cmark&   \cmark   &  31.20  &  28.47  &  29.77   \\
 \cmark &   \xmark   &   \cmark  &  30.39  &  27.31  &  28.76  \\
 \cmark &   \cmark &   \xmark  &  30.07  &  25.06  &  27.33  \\
 \cmark &   \cmark &   \cmark  &  33.29  &  28.18  &  30.53  \\
 \hline
 \end{tabular}
 }
 \caption{Ablation study to verify the contribution of different loss terms. Performances for proposed CJME (with attention) method on zeroshot classification (\% mAcc)}
 \label{tab:ablation}
 \vspace{-1em}
 \end{table}
\vspace{0.5em} \\
\textbf{Ablation of the different loss components.} Tab.~\ref{tab:ablation} gives the performances in the different cases when we selectively turn off different combination of losses in the optimization objective eq.~\ref{eqn:finalobj}. We observe that all three losses contribute positively towards the performance. When either of the triplet loss is turned off, the performance drastically fall to , but when the crossmodal audio-video loss is added with one of the triplet losses turned off, they recover to reasonable values . Compared to the final performance of , when the text-audio, text-video and audio-video losses are turned off, the performances fall to ,  and  respectively. Thus we conclude that each component in the loss function is useful and that the networks (which are already pre-trained on auxiliary classification tasks) need to be trained for the current task to give meaningful results.

 
 
\begin{table*}
\centering
\begin{minipage}[b]{0.66\textwidth}
\resizebox{\textwidth}{!} {
\renewcommand{\arraystretch}{1.25}
\begin{tabular}{c|ccc|ccc|ccc|ccc}
\hline
& \multicolumn{3}{c|}{\textbf{SUN}} & \multicolumn{3}{c|}{\textbf{CUB}} & \multicolumn{3}{c|}{\textbf{AWA1}}& \multicolumn{3}{c}{\textbf{AWA2}} \\
\textbf{Method} & \textbf{U} & \textbf{S} & \textbf{HM} & \textbf{U} & \textbf{S} & \textbf{HM} & \textbf{U} & \textbf{S} & \textbf{HM} & \textbf{U} & \textbf{S} & \textbf{HM}  \\ \hline \hline
CONSE~\cite{norouzi2013zero} & 6.8 & 39.9 & 11.6 & 1.6 & 72.2 & 3.1 & 0.4 &88.6 &0.8 & 0.5 & 90.6 & 1.0 \\
DEVISE~\cite{frome2013devise} & 16.9 & 27.4 & 20.9 & 23.8 & 53.0 & 32.8 & 13.4 &68.7 &22.4 & 17.1 & 74.7 & 27.8  \\
SAE~\cite{kodirov2017semantic} & 8.8 & 18.0 & 11.8 & 7.8 & 54.0 & 13.6 & 1.8 &77.1 &3.5 & 1.1 & 82.2 & 2.2 \\
ESZSL~\cite{romera2015embarrassingly} & 11.0 & 27.9 & 15.8 & 12.6 & 63.8 & 21.0 &6.6&75.6&12.1& 5.9 & 77.8 & 11.0 \\
ALE~\cite{akata2016label} & 21.8 & 33.1 & 26.3 & 23.7 & 62.8 & {34.4} & 16.8 & 76.1 & 27.5 & 14.0 & 81.8 & 23.9 \\ 
\hline
CJME & 30.2 &23.7 & {26.6} & 35.6 & 26.1 & 30.1 &   29.8& 47.9 & {36.7} &51.9 &36.8 & {43.1} \\
\hline
\end{tabular}
}
\caption{Comparison with existing methods on standard datasets (projection based methods only, see sec.~\ref{sec:sota} for details)}
\label{tab:zsl_otherdata}
\end{minipage}
\hfill
\begin{minipage}[b]{.32\textwidth}
\resizebox{\textwidth}{!} {
\begin{tabular}{c|cc|ccc}
\hline
&\multicolumn{2}{c|}{\textbf{Modality}}&&&\\
\textbf{Method}& train & test   &\textbf{S} & \textbf{U} & \textbf{HM} \\
\hline \hline 
CONSE~\cite{norouzi2013zero}  & video & video &48.5 & 19.6 & 27.9  \\
DEVISE~\cite{frome2013devise} & video & video &39.8 & 26.0 & 31.5  \\
SAE~\cite{kodirov2017semantic}  & video & video &29.3 & 19.3 & 23.2 \\
ESZSL~\cite{romera2015embarrassingly} & video & video & 33.8 & 19.0 & 24.3 \\
ALE~\cite{akata2016label} & video & video & 47.9 & 25.2 & 33.0 \\
\hline
CJME & video & video & 43.2 & 27.1 & 33.3\\
CJME & both & video & 41.5 & 28.8 & 33.9 \\
CJME & both & both & 41.0 & 29.5 & {34.4} \\
\hline
\end{tabular}
}
\caption{Comparison with existing methods on proposed dataset (projection based methods only, see sec.~\ref{sec:sota} for details)}
\label{tab:zsl_owndata}
\end{minipage}
\end{table*}

 
\begin{figure*}
\centering

\includegraphics[width=0.97\textwidth]{images/qual_res_mod}
\caption{Qualitative crossmodal retrieval results with the proposed method. Each block of two rows from top to bottom corresponds to text to audio, text to video, audio to video and video to video respectively. The small icons on the left top of each image indicates the modality considered for that specific video. Please see detailed results video in the supplementary material.}
\label{fig:qualres}
\vspace{-0.4em}
\end{figure*}
 
 \subsection{Comparison with state of the art methods}
 \label{sec:sota}
 We address both possible issues , i.e.\ (i) our implementation is competitive \wrt other methods on standard datasets, and (ii) how do other methods compare on the proposed audio-visual ZSL dataset, by providing additional results. Tab.~\ref{tab:zsl_otherdata} gives the performance of our implementation on other datasets (existing method performances are taken from Xian et al.~\cite{xian2018zero}). 
 
 We observe that our method is competitive to other methods on an average. Tab.~\ref{tab:zsl_owndata} gives the classification performance of other methods using our features on the proposed dataset. We see that our method performs better than many existing methods (\eg ALE  vs.\ CJME ). Hence we conclude that our implementation and method, both, perform comparable to existing appearance based ZSL methods. Tab.~\ref{tab:zsl_owndata} also shows that adding audio improves the video only ZSL from  to  HM. 
 
 In these comparisons, we have not included some of the recent generative approaches \cite{verma2018generalized, xian2018feature} which handles the task by conditional generation of examples form the unseen classes. Although these approaches increase the performance but they come with the drawback of soft-max classification, which requires the classifier to be trained form scratch once again if a new class is added to the existing setup at test time. This also requires saving all the training data for generative approaches while in the projection based methods, this is not required. 
 

 \subsection{Qualitative evaluation}
 Fig.~\ref{fig:qualres} shows qualitative crossmodal retrieval results for all three pairs of modalities, \ie text to audio/video, audio to video and video to audio. We see that method makes acceptable mistakes, \eg for the car text query one of the audio retrieval contains motorbike due to the similar sound, for the bird video query the wrong retrieval is a cat purring sound which is similar to a pigeon sound. In the unseen class case, the bus text query return car, train and truck audio as the top false positives. Easier and distinct cases such as gunshot audio query gives very good video retrievals. We encourage the readers to look at the result videos available at \url{https://www.cse.iitk.ac.in/users/kranti/avzsl.html} for a better understanding of qualitative results. 