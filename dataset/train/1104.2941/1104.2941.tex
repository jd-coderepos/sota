In this section, we will derive the scaling laws for the performance of the system when transmissions are modeled as continuous. We model the arrivals at each receiver as a Poisson process and analyze the behavior of completion time as the number of receivers $n$ grow.

Each of the $n$ users needs to receive $k$ or more coded packets from a single transmitting node. In time $t$ packet lengths, each of the $n$ nodes \textit{independently} receives a number of packets that is Poisson distributed, on the time scale of integral numbers of packet lengths, with parameter $\lambda t$, where $\lambda = 1 - p_e$, and $p_e$ is the packet erasure probability. The probability that user $i$ receives $k$ or more coded packets within time $t$ is thus:
\begin{eqnarray}
Pr\{M_{i}^{t} = 0\} & = & 1 - \sum_{j = 0}^{k-1} \frac{(\lambda t)^j \exp(-\lambda t)}{j!} \label{one-user}
\end{eqnarray}

Hence the probability that all $n$ users receive at least $k$ coded packets in time $t$ or earlier is (\ref{one-user}) raised to the power of $n$. As in Section \ref{sec:Disc_Model} we define $\beta(t)$ to be the probability that \textit{all} of the $n$ users received $k$ or more coded packets within time $t$. This probability $\beta(t)$, which is also the probability that the transmitter can stop sending coded packets, is:
\begin{eqnarray}
\beta(t) & = & \left(1 - \sum_{j = 0}^{k-1} \frac{(\lambda t)^{j} \exp(-\lambda t)}{j!} \right)^{n} \label{eq:cont_model}
\end{eqnarray}

We select the first feedback time so that there is a significant probability that every receiver has completed the download and there is no need for retransmissions. In other words, $t^{*}$ is a time whose corresponding $\beta(t^*)$ has reached a certain reliability threshold. Let us use $\beta^{*}$ to denote this threshold. Thus: 
\begin{eqnarray}
t^{*} & = & t^{*}\big(\beta^{*},n\big) = \inf \left \{\hspace{0.05cm} t \hspace{0.2cm} | \hspace{0.2cm} \beta(t) \geq \beta^{*} \right \}
\end{eqnarray}

Rearranging terms in (\ref{eq:cont_model}) and substituting $\beta^{*}$ and $t^{*}$ for $\beta(t)$ and $t$ yields:
\begin{eqnarray}
\lambda t^{*} & = & \ln \left (\sum_{j = 0}^{k-1} \frac{(\lambda t^{*})^j}{j!}\right) - \ln \left( 1 - {\beta^{*}}^\frac{1}{n} \right) \label{exact-expression} \\
& = & \lambda t^{*} + \ln \left(\frac{\Gamma(k,\lambda t^{*})}{\Gamma(k)}\right) - \ln \left( 1 - {\beta^{*}}^\frac{1}{n} \right)
\end{eqnarray}

we then have:
\begin{eqnarray} \label{gamma_eqn}
\frac{\Gamma(k,\lambda t^{*})}{\Gamma(k)} & = &  \left( 1 - {\beta^{*}}^\frac{1}{n} \right)  \label{eq:exact_sensitivity_parameters}
\end{eqnarray}
where the Gamma functions are defined as:
\begin{eqnarray}
\Gamma(a,b) = \int_{b}^{\infty} t^{a-1} e^{-t} dt \nonumber \\ 
\Gamma(a) = \int_{0}^{\infty} t^{a-1} e^{-t} dt \nonumber 
\end{eqnarray}

\begin{figure} [ht]
\centering
\includegraphics[scale=0.63,clip=true]{Gamma_function_for_cont_model.eps}
\caption{Calculating $t^{*}$ from the $\frac{\Gamma(k,\lambda t)}{\Gamma(k)}$ function.}
\label{fig:Gamma_function_for_cont_model}
\end{figure}
Fig. \ref{fig:Gamma_function_for_cont_model} illustrates equation (\ref{gamma_eqn}). Notice that $\frac{\Gamma(k,\lambda t)}{\Gamma(k)}$ is strictly decreasing in $t$ and is thus invertible. As a result, given a set of parameters $(n,k,\beta^{*})$, a unique $t^{*}$ can be determined that is the amount of time that it takes for all $n$ users to receive the $k$ packet file, with probability $\beta^{*}$. The right hand side of (\ref{gamma_eqn}) corresponds to the horizontal line in Fig. \ref{fig:Gamma_function_for_cont_model}, and is the probability that any given user has not received the file by time $t^{*}$. For large $n$ and even a modest $\beta^{*}$, this probability, and hence the resulting horizontal line, would be quite low, resulting in the selection of a $t^{*}$ such as that shown in the figure. Alternatively, if the function $\frac{\Gamma(k,\lambda t)}{\Gamma(k)}$ is considered at time $t$, rather than at time $t^*$, then raising $1 - \frac{\Gamma(k,\lambda  t)}{\Gamma(k)}$ to the power of $n$ yields a continuous model version of the the probability function $\beta(t)$ plotted in Fig. \ref{fig:diff_k_and_n}. Taking the $n^{th}$ power of $1 - \frac{\Gamma(k,\lambda  t)}{\Gamma(k)}$ for large $n$ renders $\beta(t)$ close to 1 only if $1 - \frac{\Gamma(k,\lambda  t)}{\Gamma(k)}$ is very close to 1, thereby yielding the sharp transition in time seen in Fig. \ref{fig:diff_k_and_n}.

We are interested in sensitivity of $t^{*}$ to $n$ for a given value of $\beta^{*}$. A better understanding of this sensitivity can be achieved by looking at the reverse problem. Let us see how many nodes $n$ we can accommodate after $t$ transmissions for a given value of $\beta^{*}$. Rearranging terms in (\ref{eq:exact_sensitivity_parameters}) and solving for $n$ yields: 
\begin{eqnarray}
n & = & \frac{\ln \big(\beta^{*}\big) }{\ln \Big( 1 - \frac{\Gamma(k,\lambda t)}{\Gamma(k)} \Big)} \label{eq:exact_sensitivity_for_figures}
\end{eqnarray}

Figure \ref{fig:accomodated_nodes_for_a_given_t_k_100} provides the number of users that can be accommodated by time $t$, for a range of $\beta^{*}$. The figure was computed according to (\ref{eq:exact_sensitivity_for_figures}) for a file size of $k = 100$ packets and packet erasure probability of $p_{e} = 0.1$. Fig. \ref{fig:accomodated_nodes_for_a_given_t_k_100} can be used to determine the $t^{*}$ that will ensure a given reliability $\beta^{*}$ for a given $k$ and $n$. The dashed black lines in the figure illustrate how to determine this time for the example case of $\beta^{*}$ = .9 and $n = 1000$.

The number of nodes $n$ that can be accommodated increases rapidly, as emphasized by the logarithmic scale of the vertical axis and the linear scale of the horizontal axis. In fact a much larger group of users can be accommodated with a relatively short extra transmission time. For example, when $\beta^{*} = 0.1$, an increase of approximately $20$ in $t$ (from $110$ to $130$) can accommodate $100$ times as many users (from 10 to 1000 users). Because of the convexity exhibited in the figure, ever larger groups can be accommodated with the same number of extra transmissions. 

It should also be noted that $n$ is not very sensitive to $\beta^{*}$, and the sensitivity decreases as $t$ increases. For example, the figure shows that in order to accommodate $n = 10$ users, with reliabilities $\beta^{*} = 0.1$ and $\beta^{*} = 0.9$, we need $t = 108$ and $t = 125$ respectively (a 15.7\% increase in $t$ to reach the higher $\beta^{*}$). Accommodating $n = 1000$ users for the same values of $\beta^{*}$ will require $t = 130$ and $t = 142$ respectively (a 9.2\% increase in $t$). 
\begin{figure} [ht]
\centering
\includegraphics[scale=0.62,clip=true]{Affordable_number_of_nodes_vs_time_k_is_100.eps}
\vspace{-0.8cm}
\caption{The number of nodes $n$ that can be accommodated for a given transmission time $t$. The figure was computed according to (\ref{eq:exact_sensitivity_for_figures}) with $k=100$ packets and $p_{e} = 0.1$. An example of how the time $t^{*}\big(\beta^{*},n\big)$ can be obtained from these curves is illustrated by the dashed lines for the case of $\beta^{*}$ = .9 and $n = 1000$.}
\label{fig:accomodated_nodes_for_a_given_t_k_100}
\end{figure}

Similar numerical results hold for larger file sizes and can be verified by plotting (\ref{eq:exact_sensitivity_for_figures}) for larger values of $k$. As $k$ increases, the \textit{per packet} time required to reliably transmit a file to a fixed number of receivers decreases. This favorable gain comes from the ability to code across larger files, and shows the robustness of SMART to increases in the file size. 



