

\documentclass[journal,10pt]{IEEEtran}

\newtheorem{thm}{Theorem}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{rem}[thm]{Remark}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{coroll}[thm]{Corollary}
\newtheorem{assumption}[thm]{Assumption}
\newtheorem{problem}[thm]{Problem}
\newtheorem{algorithm}[thm]{Algorithm}


\IEEEoverridecommandlockouts                              \overrideIEEEmargins
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}      \usepackage{float} 
\usepackage{psfrag} 
\usepackage{url}
\usepackage{subfig}
\usepackage{cite}


\title{\LARGE \bf Subspace identification of large-scale interconnected systems}

\author{Aleksandar Haber and Michel Verhaegen\thanks{This research is supported by the Dutch Ministry of Economic Affairs and the Provinces of Noord-Brabant and Limburg in the frame of the "Pieken in de Delta" program.}\thanks{A. Haber and M. Verhaegen are with Delft Center for Systems and Control, Delft University of Technology, Delft, 2628 CD, The Netherlands, (e-mail: {\tt\small a.haber@tudelft.nl}; {\tt\small m.verhaegen@tudelft.nl}).}
}
\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
\maketitle
\begin{abstract}
We propose a decentralized subspace algorithm for identification of large-scale, interconnected systems that are described by sparse (multi) banded state-space matrices. First, we prove that the state of a local subsystem can be approximated by a linear combination of inputs and outputs of the local subsystems that are in its neighborhood. Furthermore, we prove that for interconnected systems with well-conditioned, finite-time observability Gramians (or observability matrices), the size of this neighborhood is relatively small. On the basis of these results, we develop a subspace identification algorithm that identifies a state-space model of a local subsystem from the local input-output data. Consequently, the developed algorithm is computationally feasible for interconnected systems with a large number of local subsystems. Numerical results confirm the effectiveness of the new identification algorithm. 
\end{abstract}
\begin{IEEEkeywords}
Identification, large-scale systems, subspace identification.
\end{IEEEkeywords}
\section{Introduction}
Large-scale interconnected systems consist of a large number of local subsystems that are distributed and interconnected in a spatial domain \cite{benner2004,siljak1991,bamieh02distributedcontrol,dandrea2003}. The classical identification methods, such as the Subspace Identification Methods (SIMs) \cite{verhaegen2007} or Prediction Error Methods (PEMs) \cite{lennart1999system}, are not suitable for identification of large-scale interconnected systems. This is mainly because the computational and memory complexity of these methods scale at least with  and , respectively, where  is the number of local subsystems of an interconnected system. Furthermore, the SIMs identify a state-space representation of an interconnected system, in which the interconnection structure is destroyed by unknown similarity transformation \cite{verhaegen2007}. However, for efficient distributed controller synthesis we need structured state-space models of interconnected systems \cite{paolo2009paper,bamieh02distributedcontrol,dandrea2003,justin2010distributed}.
\\
 On the other hand, the SIMs and the PEMs are centralized identification methods.
That is, these methods can be applied only if input-output data of all local subsystems can be collected and processed in one computing unit. In cases in which a large number of local sensors collect measurement data of local subsystems, it might not be possible to transfer local measurements to one centralized computing unit \cite{Rabbat2004}. In such situations, identification should be performed in a decentralized/distributed manner on a network of local computing units that communicate locally.  
\\
In \cite{paolo2009ident,massioniCirculant}, SIMs have been proposed for some special classes of interconnected systems. However, these identification algorithms are computationally infeasible for interconnected systems with a very large number of local subsystems.
\par
In this paper we propose a decentralized subspace algorithm for identification of large-scale, interconnected systems that are described by sparse (multi) banded state-space matrices. For example, these state-space models originate from discretization of 2D and 3D Partial Differential Equations (PDEs) \cite{benner2004,haberThesis}. First, we prove that the state of a local subsystem can be approximated by a linear combination of inputs and outputs of the local subsystems that are in its neighborhood.  Furthermore, we prove that for interconnected systems with well-conditioned, finite-time observability Gramians (or observability matrices), the size of this neighborhood is relatively small. On the basis of these results, we develop a subspace identification algorithm that identifies a state-space model of a local subsystem using only local input-output data. Consequently, the developed algorithm is computationally feasible for interconnected systems with an extremely large number of local subsystems (provided that the finite-time observability Gramian is well conditioned). We numerically illustrate the effectiveness of the new identification algorithm. 
\\
The paper is organized as follows. In Section \ref{problemFormulation}, we present the problem formulation. In Section \ref{mainTheorems}, we present theorems that are used in Section \ref{identSection} to develop the identification algorithm. In Section \ref{numericalSection} we present the results of numerical simulations and in Section \ref{sectionConclusion} we draw conclusions.
\section{Problem formulation}
\label{problemFormulation}
We briefly explain the notation used in this paper. A block diagonal matrix  with blocks  is denoted by . A column vector  is denoted by .
For the sake of presentation clarity, we consider the following state-space model: 
\begin{small}

\end{small}
\begin{small}

\end{small}
\begin{small}

\end{small}
The system  is referred to as \textit{the global system}, with \textit{the global state} , \textit{the global input} , \textit{the global measured output}  and \textit{the global measurement noise} . The system matrices ,  and  are referred to as \textit{the global system matrices}. The global system  is an interconnection of  \textit{local subsystems} : 
\begin{small}

\end{small}
where  is \textit{the local state} of the local subsystem ,  and  are the local states of \textit{the neighboring local subsystems}  and  respectively,  is \textit{the local input},  is \textit{the local measured output},  is \textit{the local measurement noise}. Without loss of generality, we assume that all local subsystems have identical local order . All matrices in \eqref{localSubSys} are constant matrices and are referred to as \textit{the local system matrices}. In \eqref{localSubSys}, the index  is referred to as \textit{the spatial index}. The spatial index takes the values from \textit{the spatial domain}  . For example, the global state-space model \eqref{globalSys} can be obtained by discretizing the 2D heat equation using the finite difference method \cite{benner2004,haberThesis}. The identification algorithm proposed in this paper can be generalized to large-scale interconnected systems with sparse (multi) banded system matrices (see Remark \ref{remarkGeneralization}).
\par
 The set of local subsystems  will be referred to as \textit{the neighborhood of }. \\
\begin{definition}
\label{structurePreservingSimDef}
\textit{The structure preserving similarity transformation} is the matrix , that transforms the global state-space model \eqref{globalSys}-\eqref{explanationGlobSys} into the following state-space model:
\begin{small}

\end{small}
where , ,  and . The matrix  has block bandwidth equal to 1 (the same sparsity pattern like ), while  and  are block diagonal matrices.\\
\end{definition} 
\begin{problem} \textit{Identification problem} 
\par
Consider the global system \eqref{globalSys} that consists of the interconnection of  local subsystems \eqref{localSubSys}. Then, using the sequence of the global input-output data ,
\begin{enumerate}
\item Estimate the order of the local subsystems .
\item Identify the global state-space model \eqref{globalSys} up to a structure preserving similarity transformation.
\end{enumerate} 
\label{identificationProblem}
\end{problem}
\section{Main theorems}
\label{mainTheorems}
Starting from  and by lifting \eqref{globalSys}  time steps, we obtain:
\begin{small}

\end{small}
where ,  and similarly we define . The matrix  is \textit{the -steps observability matrix} and the matrix  is \textit{the  steps impulse response matrix}\cite{verhaegen2007}. \\
On the other hand, from the state equation of the global state-space model \eqref{globalSys}, we have:
\begin{small}

\end{small}
where  is \textit{the  steps controllability matrix}. \textit{The parameter  should be selected such that it is much smaller than  ()}. This ensures that all lifted system matrices in \eqref{liftedClassicalOutput} and \eqref{liftedClassicalState} are sparse matrices.
\par
 Formation of the lifted equations \eqref{liftedClassicalOutput} and \eqref{liftedClassicalState} is the standard step in the classical SIMs. However, in this paper we use a different lifting technique. \textit{This lifting technique consists of first lifting the local outputs over the time domain and then lifting such lifted outputs over the spatial domain.} To explain this new lifting technique, we define the vector  as follows . In the same manner we define the lifted input vector  and the lifted measurement noise vector . Next, a column vector  is defined as follows: . In the same manner we define vectors  and . It can be easily proved that:
\begin{small}

\end{small}
where  and  are permutation matrices. By multiplying the lifted equation \eqref{liftedClassicalOutput} from left with  and keeping in mind that permutation matrices are orthogonal, we obtain:
\begin{small}

\end{small}
where the matrices  and  are defined as follows:
\begin{small}

\end{small}
The equation \eqref{liftedDataEq} will be called \textit{the global data equation}. On the other hand, using the orthogonality of the permutation matrix , from \eqref{liftedClassicalState} we have:
\begin{small}

\end{small}
where the matrix  is defined by . \textit{The matrix  is a sparse banded matrix, with the block bandwidth equal to . Similarly, the matrices  and  are sparse banded matrices with the block bandwidth equal to }. For more information about the structure of , , and , the interested reader is advised to consult Chapter 3 of \cite{haberThesis}.
\\
The following lemma will be used to prove Theorems \ref{stateEstimatorLemma} and \ref{mainTheoremSpatialDecay}. \\
\begin{lemma}
Assume that , where  is the observability index of the global system \cite{luenberger1971,haber2013mhe}. Then .
\label{observabilityTheorem}  \\
\end{lemma}
\textit{Proof}. 
The matrix  is defined by permuting the rows of . Since this permutation does not change the rank of , we have: . Now, let , where  is the observability index of the global system. This implies .  
\\ \par
Throughout the reminder of the paper, the matrix  will be called \textit{the structured observability matrix} of the global system \eqref{globalSys}. \textit{The finite-time observability Gramian} of the global system \eqref{globalSys}, denoted by , can be expressed as follows \cite{lim1996hankel,antoulas2005approximation}:
\begin{small}

\end{small} 
From \eqref{explanationLiftedDataEq} and \eqref{obsGramianGlobal}, we have:
\begin{small}

\end{small}
Taking into account that  has block bandwidth equal to , from \eqref{FFmatrix} we have that  is a sparse banded matrix with the block bandwidth equal to . Throughout the remainder of the paper, the condition number of  will be denoted by . The inverse of  will be denoted by . For the sequel we will assume that the matrix  is well-conditioned. \textit{As we will prove later, this assumption ensures that the local state of  can be approximated by a linear combination of input and output data of local subsystems that are in a relatively small neighborhood of }. \\
\begin{thm}
Let , where  is the observability index of the global system. Then,
\begin{small} 

\end{small}
\label{stateEstimatorLemma}
where .
\end{thm}
\textit{Proof}. From \eqref{liftedDataEq}, we have:
\begin{small}

\end{small}
Because , from Lemma \ref{observabilityTheorem} we have: . This implies that  is positive definite and invertible. Because of this, from \eqref{expresion1} we have:
\begin{small}

\end{small}
where . Substituting \eqref{expresion2} in  \eqref{liftedState} we arrive at \eqref{expression4Final}. 
\par
 Although  is a sparse banded matrix, in the general case,  is a dense matrix. However, by examining the entries of , it can be observed that the absolute values of the off-diagonal entries decay\footnote{The absolute values of off-diagonal elements can also oscillate. However, there should exist a decaying exponential function that bounds these oscillations.}, as they are further away from the main diagonal. This phenomena has been studied in \cite{benzi2007} and \cite{demko1984}, and it is illustrated in Fig. \ref{decayFinv}(a) in Section \ref{numericalSection}. Next, we define exponentially off-diagonally decaying matrices. \\
\begin{definition} \cite{benzi2007} We say that an  matrix  is an exponentially off-diagonally decaying matrix if there exist constants ,  and , such that:
\begin{small}

\end{small}
for all . \\ \end{definition} 
\begin{thm} Let , where  is the observability index of the global system, and consider the finite-time observability Gramian  and its inverse . The matrix  is an exponentially off-diagonally decaying matrix with:
\begin{small}

\end{small}
\label{mainTheoremSpatialDecay}
\end{thm}
where  is the bandwidth\footnote{The bandwidth  is defined by , where  is a constant in Eq. (2.6) in \cite{demko1984}.} of  ( is proportional to ).
\textit{Proof}. Because  from Lemma \ref{observabilityTheorem} it follows that  has full column rank. This implies that  is a symmetric positive definite matrix. Because of this, from \cite{demko1984} (see Theorem 2.4 and Proposition 2.2 in \cite{demko1984}, and for a more general proof, see \cite{benzi2007}), if follows that  is an exponentially off-diagonally decaying matrix with the
constants  and  given by \eqref{constantsDefinition}.  \\

Because by assumption the matrix  is well conditioned and , from \eqref{constantsDefinition} it follows that the off-diagonal decay of  is rapid. Because of this, the matrix  can be approximated by a sparse banded matrix \cite{benzi2007}. \\
\begin{definition} \cite{benzi2007} Let . The matrix  with its elements defined by:
\begin{small}

\end{small}
is an approximation of .
\label{blockBandedDefinition} \\
\end{definition}
\begin{proposition}
Consider the matrix  and its approximation . Then,
\begin{small}

\end{small}
where the constants  and  are defined in \eqref{constantsDefinition}. Moreover, the parameter  is an increasing function of .
\label{blockBandedApprox}
\end{proposition} 
\textit{Proof}. See the proof of Proposition 5.4 in \cite{haberThesis}.
 \\ \\
 Let us assume that the bandwidth of   is chosen such that , where  is a positive integer. Similarly to the partitioning of the matrix , defined in \eqref{explanationGlobSys}, we partition  into  blocks, where each block is a matrix of dimension . After this partitioning, the matrix  has the block bandwidth equal to . Accordingly, throughout the remainder of the paper, the matrix  will be denoted by .
\\
From Proposition \ref{blockBandedApprox} we see that the approximation accuracy increases as  increases or equivalently, as   increases. Furthermore, we see that the approximation accuracy is better when  is smaller. Because  is well conditioned, there exists , or equivalently, , for which the accuracy of approximating  by  is relatively good \cite{benzi2007}. For the sequel we assume that .
\par
By substituting  with  in \eqref{expression4Final}, we define an approximate global state:
\begin{small} 

\end{small}
 For the sequel we will partition  as follows: , where ,  . From \eqref{expresion32} we have that  is a linear combination of the lifted local inputs, lifted local outputs and lifted local measurement noises of the local subsystems belonging to the neighborhoods ,  and , respectively. 
Because , these neighborhoods are small. By substituting in \eqref{localSubSys} the local states  and  with their approximations,  and , we obtain the following approximate state-space model:
\begin{small}

\end{small}
\section{Identification algorithm}
\label{identSection}

The main idea of the identification algorithm it to use the approximate state-space model \eqref{unifiedSys} to estimate the state sequence of the local subsystem . This step has to be repeated for all  local subsystems. Because  and , the input  of \eqref{unifiedSys}, contains input-output data of local subsystems that are in a relatively small neighborhood of . Consequently, using the SIMs \cite{verhaegen2007} we can estimate the state sequence of \eqref{unifiedSys} in a computationally efficient manner. \textit{The computational complexity of estimating the state of \eqref{unifiedSys} is independent from the total number of local subsystems }. However, the problem lies in the fact that we do not know in advance the precise value of  that determines the form of the input . As it will be explained in Section \ref{commentsSubSection}, this problem can be solved by choosing several values of  and by computing the Variance Accounted For (VAF) of the identified models.
\\
Let the estimated state sequence of the approximate state-space model \eqref{unifiedSys} be denoted by . The state sequence  is approximately related to the ``true" state sequence of the local subsystem , via the following transformation:
\begin{small}

\end{small}
where  is a square, invertible matrix. We will denote the estimated state-sequences of the local subsystems  and  (that are estimated on the basis of \eqref{unifiedSys}) by  and , respectively. Similarly to \eqref{similarityTransformation12}, we have:
\begin{small}

\end{small}
where  and  are invertible matrices. By substituting \eqref{similarityTransformation12} and \eqref{similarityTransformation11} in \eqref{localSubSys}, and by multiplying the state-equation with , we obtain:
\begin{small}

\end{small}
State-space model \eqref{localSubSys4} tells us that once the local state sequences are estimated, the local system matrices  can be estimated by solving a least-squares problem formed on the basis of:
\begin{small}

\end{small}
Using the same principle, we can estimate the local system matrices of other local subsystems. Using the estimates of the local system matrices, we can form the estimates . Next, from \eqref{localSubSys4} we have:
\begin{small}

\end{small}
Since \eqref{localSubSys4} and \eqref{finalStrategy} hold for all , we conclude that , , , and ,
where  is the structure preserving similarity transformation (see Definition \ref{structurePreservingSimDef}). This shows that the identified model is (approximately) similar to the global state-space model \eqref{globalSys}. We are now ready to formally state the identification algorithm.
\\
\begin{algorithm} \textit{Identification of the global state-space model}\\
For , perform the steps 1 and 2: \\ 
1. Choose the parameters  and  and form the input vector   of the state space model \eqref{unifiedSys}.\\
 2. Estimate the local state sequence  of the state space model \eqref{unifiedSys} using the SIM.\\
After the steps 1 and 2 are completed, the state sequences , , are available. For , perform the following step:\\
  3. On the basis of \eqref{simpleLinearRegression} form a least-squares problem, and estimate the local system matrices . \\
  4. Using the estimates , , form the global system matrices .
\label{identificationAlgorithm}
\end{algorithm}
\subsection{Comments on the identification algorithm}
\label{commentsSubSection}
The theory developed in this paper predicts that for systems with well-conditioned, finite-time observability Gramians, there should exist  for which the matrix  can be accurately approximated by the sparse banded matrix . This implies that in the first step of Algorithm \ref{identificationAlgorithm}, we can select any  that satisfies .  
After that, the model should be identified and the VAF should be calculated. If the VAF value of the identified model is not high enough, then a new value of  needs to be chosen and identification procedure needs to be repeated (usually the new value should be larger than the previous one). This has to be repeated until a relatively high value of the VAF of the identified model is reached. Because the local subsystems are not identical, it might happen that the rate of the off-diagonal decay of  varies from one row to another. This implies that the matrix  can be approximated by a banded matrix, which bandwidth is row dependent. That is, for each local subsystem , it is possible to find a different parameter  that determines the input .  
\\
As it is shown in the next section, the form of the input  can cause ill-conditioning of the data  matrices used in the SIM. This is because  consists of the delayed inputs and outputs of the local subsystems. Some of the outputs might be depending on the past local inputs and local outputs. This problem can be resolved either by regularizing the data matrices used in the SIM or by eliminating certain outputs and inputs from . In this paper, we do not analyze the consistency of the identification algorithm. The consistency analysis is left for future research. The estimates of local subsystem matrices obtained using the proposed identification algorithm, can be used as initial guesses of the decision variables of a computationally efficient, parameter optimization method presented in Chapter 6 of \cite{haberThesis}. This way, the identification results can be additionally improved.
\begin{rem}
Algorithm \ref{identificationAlgorithm} can be generalized for global systems described by sparse, multi-banded, state-space matrices. For example, these systems originate from discretization of 3D PDEs using the finite difference method \cite{haberThesis}. Using the lifting technique presented in Section  \ref{mainTheorems}, it can be easily shown that the finite-time observability Gramian   of this class of systems, is a sparse, multi-banded matrix, for more details see chapters 2 and 3 of \cite{haberThesis}. In \cite{grote1997,benzi2007,haberThesis}, it has been shown that inverses  of sparse multi-banded matrices can be approximated by sparse multi-banded matrices. That is, the inverse of  can be approximated by a sparse multi-banded matrix. From the identification point of view, this implies that the state of a local subsystem can be identified using the local input-output data of local subsystems that are in its neighborhood. Depending on the interconnection pattern of local subsystems, this neighborhood can be a 2D or a 3D neighborhood. 
\label{remarkGeneralization}
\end{rem}

\section{Numerical experiments}
\label{numericalSection}
The data generating model, is a global state-space model consisting of  identical local subsystems. The local system matrices of each local subsystem are given by:
\begin{small}

\end{small}
The model \eqref{modelIdentification} is obtained using the finite-difference approximation of the heat equation, see Chapter 2 of \cite{haberThesis} (the thermal diffusivity constant is , and the temporal and spatial discretization steps are  and , respectively). Every local input is a zero-mean Gaussian white noise. Every local output is corrupted by a zero-mean Gaussian white noise. Signal to Noise Ratio (SNR) of each local output is  . In total 100 identification experiments are performed.
\par
For identification of the local state of \eqref{unifiedSys}, we use the SIM method summarized in \cite{haber2} (other SIMs can be also used). This SIM is a modified version of the SIM presented in \cite{jansson2003}. The SIM is applied with the past and future windows equal to  and , respectively.  Because all local subsystems have identical local system matrices \eqref{modelIdentification}, to identify the global state-space model we only need to perform three identification experiments. Namely, using \eqref{unifiedSys} we first estimate the state sequence of the local subsystem . Using the same methodology, we estimate the state sequence of . In the final identification step, we use the state-space model \eqref{simpleLinearRegression} (we set  in \eqref{simpleLinearRegression}) and the sequences , and , to form a least-squares problem. By solving this least-squares problem we estimate the local system matrices . Using  we form   and we compute the VAF of the identified model. 
\par
Figure \ref{decayFinv}(a) shows how the off-diagonal decay of  depends on  and . The results presented in Fig. \ref{decayFinv}(a) confirm that for well-conditioned , the off-diagonal decay of  is rapid. This figure also suggests that the accuracy of approximating  by , is relatively good for .
\begin{figure}[H]
\centering
 \subfloat[]{\includegraphics[width=4.4cm, height=3.9cm, trim=0mm 0mm 0mm 0mm,clip=true]{decay}}
 \subfloat[]{\includegraphics[width=4.2cm, height=3.9cm, trim=0mm 0mm 0mm 0mm ,clip=true]{singular}}\;\;
 \caption{\small{(a) The norm of the block elements  of the th block row of . (b) The singular values of the data matrix used to determine the order and to estimate the state-sequence of . The data matrix is formed on the basis of the input  in \eqref{inputSelections}.}}
\label{decayFinv}
\end{figure}
 To illustrate how the quality of the identified model depends on the selection of the input vector of \eqref{unifiedSys}, we identify the state-sequence of  using 5 different inputs:
\begin{small}

\end{small}
In the case of inputs  and , data matrices used to estimate the Markov parameters (impulse response parameters) of \eqref{unifiedSys} are ill conditioned. This ill-conditioning is caused by the fact that the local outputs, that are the elements of , are depending on delayed outputs and inputs. We use the regularization technique to improve the condition number of a data matrix used for identification of the Markov parameters of   (the regularization parameter is ). Local state order is selected by examining the singular values of the data matrix that is formed on the basis of . For each of the inputs defined in \eqref{inputSelections}, we form the data matrix and we select the local order . For illustration, in Fig. \ref{decayFinv}(b) we present the singular values of the data matrix formed on the basis of the input 3 (similar behavior of singular values can be observed for the inputs ,  and , while in the case of the input  the state order could not be uniquely determined). 
\\
Using the similar procedure, we determine the order and we estimate the state sequence of . The state sequence of , can be also identified on the basis of \eqref{localSubSys} (where  is set to 1). Namely, using  as known inputs in \eqref{localSubSys}, and using , we can directly identify  by solving a simple least-squares problem. \\
Next, we estimate the local system matrices . Using  we form the estimates of the global system matrices  and we compute the VAF of the global model. The average values of VAF (for ) are presented in Table \ref{tab:VAF}.
\par
\begin{tiny}\begin{table}[H]
\centering
\begin{tabular}{ |c | c | c | c |c|c|}
\hline
input                              &    1               &        2                &             3            &               4          &     5\\
\hline
VAF  (without reg.)                              &   40 \%          &         99.7 \%    &       5 \%             &           30 \%      &      20 \%  \\
\hline      
VAF (with reg.)        &     -              &          99.6 \%       &        97.7 \%         &         99.2 \%     &      98.5 \% \\
\hline
\end{tabular}
\caption{\small{The average values of VAF for 5 different inputs (reg. is the abbreviation for regularization).}}
\label{tab:VAF}
\end{table}\end{tiny}
From Table \ref{tab:VAF} we conclude that "best" identification results are obtained when the input 2 is used for identification. This input is formed by eliminating the delayed inputs and outputs that cause ill-conditioning. In Fig. \ref{fig:Nonoise}(a), we present the distribution of VAF values for the output of , when the input 2 is used for identification (in total  identification experiments are performed). Similar results are obtained for other local subsystems. The eigenvalues of , when the input 2 is used for identification, are given in Fig. \ref{eigenvalues}. Next, assuming that the local outputs are not corrupted by the noise, we perform identification using the input 2 (with regularization). The results are given in Fig. \ref{fig:Nonoise}(b).
\begin{figure}[H]
\centering
 \subfloat[]{\includegraphics[width=4.1cm, height=3.5cm,  trim=0mm 0mm 0mm 0mm ,clip=true]{vaf}}\;\;
 \subfloat[]{\includegraphics[width=4.1cm, height=3.5cm, trim=0mm 0mm 0mm 0mm,clip=true]{eigs}}\;\;
 \caption{\small{(a) Distribution of the VAF of . The identification of the state-sequence of  is performed using input 2, defined in \eqref{inputSelections}. (b) Eigenvalues of the estimated matrix  and , when the input 2 is used for identification. The outputs are not corrupted by noise.}}
\label{fig:Nonoise}
\end{figure}
As it can be seen from Fig. \ref{fig:Nonoise}(b), in the noise-free scenario we are able to obtain a relatively good identification results. Some of the eigenvalues are biased. This is mainly because of the approximation errors in the state-space model \eqref{unifiedSys} and because a part of useful information is "thrown away" by forming the input 2 (that is, some information is lost by eliminating the delayed inputs and outputs).
\begin{figure}[H]
\centering
 \subfloat[]{\includegraphics[width=4.1cm, height=3.5cm,  trim=0mm 0mm 0mm 0mm ,clip=true]{eig1}}\;\;
 \subfloat[]{\includegraphics[width=4.1cm, height=3.5cm, trim=0mm 0mm 0mm 0mm,clip=true]{eig2}}\;\;
 \caption{\small{(a) and (b): The distribution of the eigenvalues of  for 100 identification experiments. The SNR ratio is  . The circle with the big "X" corresponds to the eigenvalue of . Identification of the state-sequence of  is performed using input 2, defined in \eqref{inputSelections}.}}
\label{eigenvalues}
\end{figure}
\section{Conclusion}
\label{sectionConclusion}
In this paper we proposed a decentralized subspace algorithm for identification of state-space models of large-scale interconnected systems. To develop the identification algorithm, we proved that the state of a local subsystem can be approximated by a linear combination of inputs and outputs of local subsystems that are in its neighborhood. For systems with well-conditioned, finite-time observability Gramians, the size of this neighborhood is small. Consequently, the local subsystems can be estimated in a computationally efficient manner. The numerical experiments confirm the effectiveness of the proposed algorithm. 
\bibliographystyle{unsrt}
\bibliography{bibl}

\end{document}