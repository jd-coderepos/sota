\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{times}
\usepackage{hyperref}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}
\newcommand{\aB}{\mathbf{a}}
\newcommand{\bB}{\mathbf{b}}
\newcommand{\cB}{\mathbf{c}}
\newcommand{\dB}{\mathbf{d}}
\newcommand{\eB}{\mathbf{e}}
\newcommand{\fB}{\mathbf{f}}
\newcommand{\gB}{\mathbf{g}}
\newcommand{\hB}{\mathbf{h}}
\newcommand{\iB}{\mathbf{i}}
\newcommand{\jB}{\mathbf{j}}
\newcommand{\kB}{\mathbf{k}}
\newcommand{\lB}{\mathbf{l}}
\newcommand{\mB}{\mathbf{m}}
\newcommand{\nB}{\mathbf{n}}
\newcommand{\oB}{\mathbf{o}}
\newcommand{\pB}{\mathbf{p}}
\newcommand{\qB}{\mathbf{q}}
\newcommand{\rB}{\mathbf{r}}
\newcommand{\sB}{\mathbf{s}}
\newcommand{\tB}{\mathbf{t}}
\newcommand{\uB}{\mathbf{u}}
\newcommand{\vB}{\mathbf{v}}
\newcommand{\wB}{\mathbf{w}}
\newcommand{\xB}{\mathbf{x}}
\newcommand{\XB}{\mathbf{X}}
\newcommand{\yB}{\mathbf{y}}
\newcommand{\zB}{\mathbf{z}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}      
\usepackage{amsfonts}     
\usepackage{nicefrac} 
\usepackage{microtype}  
\usepackage{natbib}
\newcommand{\yrcite}[1]{\citeyearpar{#1}}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{placeins}
\usepackage[export]{adjustbox} 
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{verbatim}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{dsfont}


\newcommand{\maybebf}[0]{}
\newcommand{\amos}[1]{\textcolor{red}{#1}}
\newcommand{\matr}[1]{\mathbf{#1}} 


\usepackage{xr}
\makeatletter
\newcommand*{\addFileDependency}[1]{\typeout{(#1)}
  \@addtofilelist{#1}
  \IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{\externaldocument{#1}\addFileDependency{#1.tex}\addFileDependency{#1.aux}}

\myexternaldocument{suppmat}

\usepackage[accepted]{icml2021}


\icmltitlerunning{Neural Architecture Search without Training}

\begin{document}

\twocolumn[
\icmltitle{Neural Architecture Search without Training}


\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Joseph Mellor}{ush}
\icmlauthor{Jack Turner}{inf}
\icmlauthor{Amos Storkey}{inf}
\icmlauthor{Elliot J. Crowley}{eng}
\end{icmlauthorlist}

\icmlaffiliation{ush}{Usher Institute, University of Edinburgh}
\icmlaffiliation{inf}{School of Informatics, University of Edinburgh}
\icmlaffiliation{eng}{School of Engineering, University of Edinburgh}

\icmlcorrespondingauthor{Joseph Mellor}{joe.mellor@ed.ac.uk}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  



\begin{abstract}
The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be alleviated if we could partially predict a network's trained accuracy from its initial state. In this work, we examine the overlap of activations between datapoints in {\it untrained} networks and motivate how this can give a measure which is usefully indicative of a network’s {\it trained} performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201, and Network Design Spaces. Finally, our approach can be readily combined with more expensive search methods; we examine a simple adaptation of regularised evolutionary search that outperforms its predecessor. Code for reproducing our experiments is available at~\url{https://github.com/BayesWatch/nas-without-training}.
\end{abstract}



\section{Introduction}
The success of deep learning in computer vision is in no small part due to the insight and engineering efforts of human experts, allowing for the creation of powerful architectures for widespread adoption~\citep{krizhevsky2012imagnet,simonyan2015very,he2016deep,szegedy2016rethinking,huang2017densely}. However, this manual design is costly, and becomes increasingly more difficult as networks get larger and more complicated. Because of these challenges, the neural network community has seen a shift from designing architectures to designing algorithms that {\it search} for candidate architectures~\citep{elsken2019neural,wistuba2019survey}. These Neural Architecture Search (NAS) algorithms are capable of automating the discovery of effective architectures~\citep{zoph2017neural,zoph2018learning,pham2018efficient,tan2019mnasnet,liu2019darts,real2019regularized}.

\begin{figure*}[!h]

\centering
    \begin{subfigure}{.475\textwidth}
    \includegraphics[width=.99\textwidth]{figures/nasbench201_kernelplot_cifar10val_repeat1_batch128_augnone_rows5.pdf}
    \caption{NAS-Bench-201}
    \label{fig:activationhamming201}
    \end{subfigure}~
    \begin{subfigure}{.475\textwidth}
    \includegraphics[width=.99\textwidth]{figures/nds_darts_kernelplot_cifar10val_repeat1_batch128_augnone_rows5.pdf}
    \caption{NDS DARTS}
    \label{fig:activationhammingdarts}
    \end{subfigure}


\caption{{\color{black}  for a minibatch of 128 CIFAR-10 images for {\maybebf untrained} architectures in (a) NAS-Bench-201~\citep{Dong2020NAS-Bench-201} and (b) NDS DARTS~\citep{radosavovic2019network}.  in these plots is normalised so that the diagonal entry are . The  are sorted into columns based on the final CIFAR-10 validation accuracy {\maybebf when trained}. Darker regions have higher similarity. The profiles are distinctive; the  for good architectures in both search spaces have less similarity between different images. We can use  for an untrained network to predict its final performance without any training. Note that (b) covers a tighter accuracy range than (a), which may explain it being less distinctive.}}
    \label{fig:normhamming} 
\end{figure*}


NAS algorithms are broadly based on the seminal work of~\cite{zoph2017neural}. A controller network generates an architecture proposal, which is then trained to provide a signal to the controller through REINFORCE~\citep{williams1992simple}, which then produces a new proposal, and so on. Training a network for every controller update is extremely expensive; utilising 800 GPUs for 28 days in~\cite{zoph2017neural}. Subsequent work has sought to ameliorate this by (i) learning stackable cells instead of whole networks~\citep{zoph2018learning} and (ii) incorporating {\it weight sharing}; allowing candidate networks to share weights to allow for joint training~\citep{pham2018efficient}. These contributions have accelerated the speed of NAS algorithms e.g.\ to half a day on a single GPU in~\cite{pham2018efficient}.

For some practitioners, NAS is still too slow; being able to perform NAS quickly (i.e.\ in seconds) would be immensely useful in the hardware-aware setting where a separate search is typically required for each device and task~\citep{wu2019fbnet,tan2019mnasnet}. This could be achieved if NAS could be performed {\it without any network training}. In this paper {\maybebf we show that this is possible}. We explore NAS-Bench-101~\citep{ying2019bench}, NAS-Bench-201~\citep{Dong2020NAS-Bench-201}, and Network Design Spaces (NDS,~\citealp{radosavovic2019network}), and examine the overlap of activations between datapoints in a minibatch for an {\maybebf untrained} network. (Section~\ref{sec:scoring}). The linear maps of the network are uniquely identified by a binary code corresponding to the activation pattern of the rectified linear units. The Hamming distance between these binary codes can be used to define a kernel (which we denote by ) which is distinctive for networks that perform well; this is immediately apparent from visualisation alone across two distinct search spaces (Figure~\ref{fig:normhamming}). We devise a score based on  and perform an ablation study to demonstrate its robustness to inputs and network initialisation.

We incorporate our score into a simple search algorithm that {\maybebf doesn't require training} (Section~\ref{sec:nas}). This allows us to perform architecture search quickly, for example, on CIFAR-10~\citep{krizhevsky2009learning} we are able to search for a network that achieves  accuracy in 30 seconds within the NAS-Bench-201 search space; several orders of magnitude faster than traditional NAS methods for a modest change in final accuracy. Finally, we show that we can combine our approach with regularised evolutionary search (REA,~\citealp{pham2018efficient}) to produce a new NAS algorithm, {\it Assisted-REA} (AREA) that outperforms its predecessor on both NAS-Bench-101 and NAS-Bench-201. Code for reproducing our experiments is available at~\url{https://github.com/BayesWatch/nas-without-training}.

We believe this work is an important proof-of-concept for NAS without training, and shows that the large resource costs associated with NAS can be avoided. The benefit is two-fold, as we also show that we can integrate our approach into existing NAS techniques for scenarios where obtaining as high an accuracy as possible is of the essence. 



\begin{figure*}[!h]
\centering

    \includegraphics[width=.99\textwidth]{figures/naswot_locallinearregions_notext_smaller.pdf}
\caption{Visualising how binary activation codes of ReLU units correspond to linear regions. 1) Each ReLU node A splits the input into an active () and inactive region We label the active region 1 and inactive 0. 2) The active/inactive regions associated with each node A intersect. Areas of the input space with the same activation pattern are co-linear. Here we show the intersection of the A nodes and give the code for the linear regions. Bit  of the code corresponds to whether node A is active. 3) The ReLU nodes B of the next layer divides the space further into active and inactive regions. 4) Each linear region at a given node can be uniquely defined by the activation pattern of all the ReLU nodes that preceded it.}
    \label{fig:locallinearcodes} 
\end{figure*}


\section{Background}
\label{sec:lit}

Designing a neural architecture by hand is a challenging and time-consuming task. It is extremely difficult to intuit where to place connections, or which operations to use. This has prompted an abundance of research into neural architecture search (NAS); the automation of the network design process. In the pioneering work of~\cite{zoph2017neural}, the authors use an RNN controller to generate descriptions of candidate networks. Candidate networks are trained, and used to update the controller using reinforcement learning to improve the quality of the candidates it generates. This algorithm is very expensive: searching for an architecture to classify CIFAR-10 required 800 GPUs for 28 days. It is also inflexible; the final network obtained is fixed and cannot be scaled e.g.\ for use on mobile devices or for other datasets.

The subsequent work of~\cite{zoph2018learning} deals with these limitations. Inspired by the modular nature of successful hand-designed networks~\citep{simonyan2015very,he2016deep,huang2017densely}, they propose searching over neural building blocks, instead of over whole architectures. These building blocks, or {\it cells}, form part of a fixed overall network structure. Specifically, the authors learn a standard cell, and a reduced cell (incorporating pooling) for CIFAR-10 classification. These are then used as the building blocks of a larger network for ImageNet~\citep{russakovsky2015imagenet} classification. While more flexible---the number of cells can be adjusted according to budget---and cheaper, owing to a smaller search space, this technique still utilises 500 GPUs across 4 days.

ENAS~\citep{pham2018efficient} reduces the computational cost of searching by allowing multiple candidate architectures to share weights. This facilitates the simultaneous training of candidates, reducing the search time on CIFAR-10 to half a day on a single GPU. Weight sharing has seen widespread adoption in a host of NAS algorithms~\citep{liu2019darts,luo2018neural,cai2019proxylessnas,xie2019snas,brock2018smash}. However, there is evidence that it inhibits the search for optimal architectures~\citep{yu2020evaluating}. Moreover, random search proves to be an extremely effective NAS baseline~\citep{yu2020evaluating,li2019random}. This exposes another problem: the search space is still vast---there are  possible architectures in~\cite{pham2018efficient} for example---that it is impossible to isolate the best networks and demonstrate that NAS algorithms find them.

An orthogonal direction for identifying good architectures is the estimation of accuracy prior to training~\citep{deng2017peephole,istrate2019tapas}, although these differ from this work in that they rely on training a predictive model, rather than investigating more fundamental architectural properties. 
Since its inception others have explored our work and the ideas therein in interesting directions. Of most interest from our perspective are \citet{abdelfattah2021zerocost} who integrate training-free heuristics into existing more-expensive search strategies to improve their performance  as we do in this paper. 
\citet{park2020towards} use the correspondence between wide neural networks and Gaussian processes to motivate using as a heuristic the validation accuracy of a Monte-Carlo approximated neural network Gaussian process conditioned on training data. \citet{chen2021neural} propose two further heuristics---one based on the condition number of the neural tangent kernel~\citep{jacot2018neural} at initialisation and the other based on the number of unique linear regions that partition training data at initialisation---with a proposed strategy to combine these heuristics into a stronger one.





\subsection{NAS Benchmarks}
\label{sec:nasbench101}
A major barrier to evaluating the effectiveness of a NAS algorithm is that the search space (the set of all possible networks) is too large for exhaustive evaluation. This has led to the creation of several benchmarks~\citep{ying2019bench,Zela2020NAS-Bench-1Shot1:,Dong2020NAS-Bench-201} that consist of tractable NAS search spaces, and metadata for the training of networks within that search space. Concretely, this means that it is now possible to determine whether an algorithm is able to search for a good network. In this work we utilise NAS-Bench-101~\citep{ying2019bench} and NAS-Bench-201~\citep{Dong2020NAS-Bench-201} to evaluate the effectiveness of our approach. NAS-Bench-101 consists of 423,624 neural networks that have been trained exhaustively, with three different initialisations, on the CIFAR-10 dataset for 108 epochs. NAS-Bench-201 consists of 15,625 networks trained multiple times on CIFAR-10, CIFAR-100, and ImageNet-16-120~\citep{chrabaszcz2017downsampled}. Both benchmarks are described in detail in Appendix~\ref{appendix:nasbench}.

We also make use of the Network Design Spaces (NDS) dataset~\citep{radosavovic2019network}. Where the NAS benchmarks aim to compare search~\textit{algorithms}, NDS aims to compare the search~\textit{spaces} themselves. In particular, NDS uses the DARTS~\citep{liu2019darts} skeleton, and trains 1000 networks for each of the cell search spaces from AmoebaNet~\citep{real2019regularized}, DARTS~\citep{liu2019darts}, ENAS~\citep{pham2018efficient},  NASNet~\citep{zoph2017neural},
PNASNet~\citep{liu2018progressive}, ResNet~\citep{he2016deep}, and ResNeXt~\citep{xie2017aggregated}. That is to say, NDS describes all the possible cells from each search space, and stacks them into the DARTS skeleton for fair comparison. We use their collated results as a dataset of trained architectures to validate our methodology.


\section{Scoring Networks at Initialisation}
\label{sec:scoring}

Our goal is to devise a means to score a network architecture at initialisation in a way that is indicative of its final trained accuracy. This can either replace the expensive inner-loop training step in NAS, or better direct exploration in existing NAS algorithms.

Given a neural network with rectified linear units, we can, at each unit in each layer, identify a binary indicator as to whether the unit is inactive (the value is negative and hence is multiplied by zero) or active (in which case its value is multiplied by one). Fixing these indicator variables, it is well known that the network is now locally defined by a linear operator~\citep{hanin2019deep}; this operator is obtained by multiplying the linear maps at each layer interspersed with the binary rectification units. Consider a minibatch of data . Let us denote the linear map for input  by column vector , which maps the input through the network  to a final choice of scalar representation . The indicator variables from the rectified linear units in  at  form a binary code  that defines the linear region.

The intuition to our approach is that the more similar the binary codes associated with two inputs are then the more challenging it is for the network to learn to separate these inputs. When two inputs have the same binary code, they lie within the same linear region of the network and so are particularly difficult to disentangle. Figure~\ref{fig:locallinearcodes} visualises binary codes corresponding to linear regions. 

We use the Hamming distance  between two binary codes - induced by the untrained network at two inputs - as a measure of how dissimilar the two inputs are. 


We can examine the correspondence between binary codes for the whole minibatch by computing


where  is the number of rectified linear units. 



We empirically demonstrate this by computing  for a random subset of NAS-Bench-201~\citep{Dong2020NAS-Bench-201}  and NDS DARTS \citep{radosavovic2019network} networks {\bf at initialisation} for a minibatch of CIFAR-10 images.


\begin{figure*}[!ht]

     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nasbench101_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nasbench201_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nds_amoeba_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
     \end{subfigure}

\centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nds_darts_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nds_enas_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nds_nasnet_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
     \end{subfigure}

    
\centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nds_pnas_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
         \label{fig:amoeba}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nds_resnet_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
         \label{fig:pnas}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/naswot_hook_logdet_nds_resnext-b_cifar10_none_0.05_1_True_128_1_1.pdf}
         \caption{}
         \label{fig:resnet}
     \end{subfigure}
     

         \begin{tabular}{l p{.7\linewidth}}
             \raisebox{-.75\height}{\includegraphics[width=0.3\linewidth]{figures/naswot_hook_logdet_nds_darts_in_imagenette2_none_0.05_1_True_128_1_1.png}}
             & 
             \textit{Figure 3.} (a)-(i): Plots of our score for 1000 randomly sampled {\bf untrained} architectures in 
   NAS-Bench-101, NDS-DARTS, NDS-PNAS, NDS-Bench-201, NDS-ENAS, NDS-ResNet, NDS-Amoeba, NDS-NASNet, and NDS-ResNeXt-B
   against validation accuracy when trained. The inputs when computing the score and the validation accuracy for each plot are from CIFAR-10. (j): We include a plot from NDS-DARTS on the ImageNet dataset to illustrate that the score extends to more challenging datasets. We use a mini-batch from ImageNette2 which is a strict subset of ImageNet with only 10 classes. In all cases there is a noticeable correlation between the score for an untrained network and the final accuracy when trained.    \\
   \quad\quad\quad\quad\quad\quad\quad(j) & \\
         \end{tabular}
    
    
    \label{fig:scorevaccnasbench201} 

\end{figure*}

The normalised kernel plots are very distinct: high performing networks have fewer off-diagonal elements with high similarity. We can therefore use these normalised kernel plots to predict the final performance of untrained networks, in place of the expensive training step in NAS.
Specifically, we score networks using:



A higher score at initialisation implies improved final accuracy after training.


\begin{figure*}[!h]

\includegraphics[width=\linewidth]{figures/corrs_nds.pdf}
\caption{Kendall Tau across each of the NDS CIFAR-10 search spaces. We compare our method to two alternative measures: \texttt{grad norm} and \texttt{synflow}. The results for \texttt{grad norm} refer to the absolute Euclidean-norm of the gradients over one random minibatch of data. \texttt{synflow} is the gradient-based score defined by~\cite{tanaka2020pruning}, summed over each parameter in the network.}
\label{fig:nds_corrs}
\end{figure*}


We sample 1000 different architectures at random from NAS-Bench-101 and NAS-Bench-201, and the different spaces in NDS and plot our score on the {\it untrained} network versus their validation accuracies {\it when trained} for the datasets in these benchmarks in Figure 3.
We find in all cases there is a positive correlation between the validation accuracy and the score, being strongest in NAS-Bench-201, NDS-DARTS, and NDS-ResNeXt-B. We show the Kendall Tau correlation between our score and final accuracy on CIFAR-10 for NDS in Figure~\ref{fig:nds_corrs}. For comparison, we include the best-performing architecture scoring functions from~\cite{abdelfattah2021zerocost} ---~\texttt{grad\_norm} and~\texttt{synflow} --- as baselines. The first is the sum of the gradient norms for every weight in the network, and the second is the summed Synaptic Flow score derived in~\cite{tanaka2020pruning}. Our score correlates with accuracy across all of the search spaces, where the other two scores severely fluctuate. These results point to our score being effective on a wide array of neural network design spaces.


\subsection{Ablation Study}
\label{sec:ablation}
\begin{figure}[!t]
    

  \centering
  \includegraphics[width=\linewidth]{figures/naswot_ablationsmall_hook_logdet_nasbench201_cifar100_none_0.05_1_True_128_20_1.pdf}

\caption{Effect of different CIFAR-100 mini-batches, each of 128 images (left), and random input image mini-batches (right) for 10 randomly selected NAS-Bench-201 architectures (one in each  percentile range from 50-55, ..., 95-100). Random input data are normally distributed. For each network 20 samples were taken for each ablation. There is less variance in score when using different random input images mini-batches as opposed to different CIFAR-100 mini-batches.}
\label{fig:ablation}

\end{figure}


\textbf{How important are the images used to compute the score?\ } Since our method relies on randomly sampling a single minibatch of data, it is reasonable to question whether different minibatches result in different scores. To determine whether our method is dependent on minibatches, we randomly select 9 architectures in NAS-Bench-201 and compute the score separately for 10 random CIFAR-100 minibatches. The resulting box-and-whisker plot is given in Figure~\ref{fig:ablation}(left): different architectures vary in sensitivity, although the ranking of the scores is reasonably robust to the specific choice of images. In Figure~\ref{fig:ablation}(right) we compute our score using normally distributed random inputs, which seems to have little impact on the general trend. This leads us to believe the score captures a property of the network architecture, rather than something data-specific.

\textbf{Does the score change for different initialisations?\ } We empirically found that aggregating the score over multiple initialisations had little effect on the efficacy of the scoring function, supporting the idea that we are quantifying an architectural property rather than a function of the weights. 



\textbf{Does the size of the minibatch matter?\ } The size of  scales with minibatch size. To compare scores across mini-batch size we divide a given score by the minimum score using the same mini-batch size from the set of sampled networks.  
Figure~\ref{fig:ablationscore} in Appendix~\ref{appendix:ablate} presents this normalised score for different minibatch sizes. There was little variation in the normalised scores.



In Section~\ref{sec:nas} we demonstrate how our score can be used in a NAS algorithm for extremely fast search.





\section{Neural Architecture Search without Training - NASWOT}
\label{sec:nas}


\begin{table}
\caption{Mean  std. accuracy from NAS-Bench-101. NASWOT is our sampling algorithm (across 500 runs). REA uses evolutionary search to select an architecture (50 runs), Random selects one architecture (500 runs). AREA (assisted-REA) uses our score to select the starting population for REA (50 runs).  Search times for REA and AREA were calculated using the NAS-Bench-101 API.}
\vspace{2mm}
\label{table:benchmarking101}
\centering
    \begin{tabular}{@{}llc@{}} \hline 
    Method & Search (s)  & CIFAR-10 \\
    \midrule
    Random & N/A &  90.385.51 \\
    NASWOT (\texttt{N}=100) & 23  & 91.770.05 \\
    REA      &  12000  & 93.870.22 \\
    AREA (Ours) &  12000  & 93.910.29 \\
    \midrule
    \end{tabular}
\end{table}


\begin{table*}[!h]


\caption{Mean  std. accuracies on NAS-Bench-201 datasets. Baselines are taken directly from~\cite{Dong2020NAS-Bench-201}, averaged over 500 runs (3 for weight-sharing methods). Search times are given for a CIFAR-10 search on a single 1080Ti GPU.  NASWOT CIFAR10-search refers to searching on the CIFAR-10 dataset and then evaluating the final model on an alternative dataset. 
Performance of our training-free approach is given for different sample size~\texttt{N} (also 500 runs), along with that of our Assisted REA (AREA) approach. We also report the results for picking a network at random, and the best possible network from the sample.}

\label{table:benchmarking}
\centering


\begin{adjustbox}{width=2\columnwidth,center}
\begin{tabular}{@{}llllcllcll@{}} \hline 
\multirow{2}{*}{Method} & \multirow{2}{*}{Search (s)}  & \multicolumn{2}{c}{CIFAR-10} & \phantom{ab} & \multicolumn{2}{c}{CIFAR-100} & \phantom{ab} & \multicolumn{2}{c}{ImageNet-16-120} \\
\cmidrule{3-4} \cmidrule{6-7} \cmidrule{9-10}
& & validation & test && validation & test && validation & test \\
\midrule
\multicolumn{10}{c}{\textbf{Non-weight sharing}}\\
REA       &  12000 & 91.190.31 & 93.920.30 && 71.811.12 & 71.840.99 && 45.150.89 & 45.541.03 \\
RS        &  12000 & 90.930.36 & 93.700.36 && 70.931.09 & 71.041.07 && 44.451.10 & 44.571.25 \\
REINFORCE &  12000 & 91.090.37 & 93.850.37 && 71.611.12 & 71.711.09 && 45.051.02 & 45.241.18 \\
BOHB      &  12000 & 90.820.53 & 93.610.52 && 70.741.29 & 70.851.28 && 44.261.36 & 44.421.49 \\
\midrule

\multicolumn{10}{c}{\textbf{Weight sharing}}\\
RSPS                & 7587  & 84.161.69 & 87.661.69 && 59.004.60 & 58.334.34 && 31.563.28 & 31.143.88 \\
DARTS-V1            & 10890 & 39.770.00 & 54.300.00 && 15.030.00 & 15.610.00 && 16.430.00 & 16.320.00 \\
DARTS-V2            & 29902 & 39.770.00 & 54.300.00 && 15.030.00 & 15.610.00 && 16.430.00 & 16.320.00 \\
GDAS                & 28926 & 90.000.21 & 93.510.13 && 71.140.27 & 70.610.26 && 41.701.26 & 41.840.90 \\
SETN                & 31010 & 82.255.17 & 86.194.63 && 56.867.59 & 56.877.77 && 32.543.63 & 31.904.07 \\
ENAS                & 13315 & 39.770.00 & 54.300.00 && 15.030.00 & 15.610.00 && 16.430.00 & 16.320.00 \\
\midrule
\multicolumn{10}{c}{\textbf{Training-free}}\\


\color{Sepia}{NASWOT (\texttt{N}=10)}  &     3.05  & 89.16  1.13 &92.45  1.12 && 68.53  2.01 &68.66  2.02 &&41.13  3.94 &41.35  4.08 \\
\color{Sepia}{NASWOT (\texttt{N}=100)} &            30.01 & 89.55  0.82 & 92.83  0.91 && 69.31  1.68 &69.45  1.67 && 42.54  3.27 & 42.86  3.34 \\
\color{Sepia}{NASWOT (\texttt{N}=1000)} &          298.65 & 89.69  0.73 & 92.96  0.80 && 69.87  1.22 &70.03  1.16 && 43.99  2.05 &44.43  2.07 \\
\midrule
Random & N/A & 83.20  13.28 & 86.61  13.46 && 60.70  12.55 & 60.83  12.58 && 33.34  9.39 & 33.13  9.66 \\ 
Optimal (\texttt{N}=10) & N/A & 89.92  0.75 & 93.06  0.59 && 69.61  1.21 & 69.76  1.25 && 43.11  1.85 & 43.30  1.87 \\
Optimal (\texttt{N}=100) & N/A & 91.05  0.28 & 93.84  0.23 && 71.45  0.79 & 71.56  0.78 && 45.37  0.61 & 45.67  0.64 \\ 
\midrule
\color{Sepia}{AREA}  & 12000 & 91.20  0.27 & -  && 71.95  0.99 &  -  && 45.70   1.05 & - \\
\midrule
\end{tabular}
\end{adjustbox}

\end{table*}


In Section~\ref{sec:scoring} we derived a score for cheaply ranking networks at initialisation based on their expected performance (Equation~\ref{eq:score}). Here as a proof of concept, we integrate this score into a simple search algorithm and evaluate its ability to alleviate the need for training in NAS. Code for reproducing our experiments is available at~\url{https://github.com/BayesWatch/nas-without-training}.




Many NAS algorithms are based on that of~\cite{zoph2017neural}; it consists of learning a generator network which proposes an architecture. The weights of the generator are learnt by training the networks it generates, either on a proxy task or on the dataset itself, and using their trained accuracies as signal through e.g.\ REINFORCE~\citep{williams1992simple}. This is repeated until the generator is trained; it then produces a final network which is the output of this algorithm. The vast majority of the cost is incurred by having to train candidate architectures for every single controller update. Note that there exist alternative schema utilising e.g.\ evolutionary algorithms such as ~\citep{real2019regularized} or bilevel optimisation~\citep{liu2019darts} but all involve a training element.


We instead propose a simple alternative---NASWOT---illustrated in Algorithm~\ref{algo:ours}. Instead of having a neural network as a generator, we randomly propose a candidate from the search space and then rather than training it, we score it in its untrained state using Equation~\ref{eq:score}. We do this \texttt{N} times---i.e.\ we have a sample size of \texttt{N} architectures---and then output the highest scoring network.


{\tiny
\vspace{-4mm}
    \begin{algorithm}[H]
    \caption{NASWOT}
    \begin{algorithmic}[h]
          \State  generator = RandomGenerator() 
                \State best\_net, best\_score = None, 0 
    

          \For{\texttt{i=1:N}}
          
              \State net = generator.generate()
        \State \textcolor{teal}{score = net.score()} 
        \If{score  best\_score}
         \State best\_net, best\_score = net, score 
        \EndIf
        \EndFor
    \State chosen\_net = best\_network
    
    
    \end{algorithmic}
    
    \label{algo:ours}
    \end{algorithm}
    
    }




\textbf{NAS-Bench-101.\ } We compare NASWOT to 12000 seconds of REA \citep{real2019regularized} and random selection on NAS-Bench-101~\citep{ying2019bench} in Table~\ref{table:benchmarking101}. We can see that we are able to find a network with a final accuracy close to REA in under a minute on a single GPU. 



\textbf{NAS-Bench-201.\ }~\citet{Dong2020NAS-Bench-201} benchmark a wide range of NAS algorithms, both with and without weight sharing, that we compare to NASWOT. The weight sharing methods are random search with parameter sharing (RSPS,~\citealp{li2019random}), first-order DARTS (DARTS-V1,~\citealp{liu2019darts}), second order DARTS (DARTS-V2,~\citealp{liu2019darts}), GDAS~\citep{dong2019searching}, SETN~\citep{dong2019one}, and ENAS~\citep{pham2018efficient}. The non-weight sharing methods are random search with training (RS), REA~\citep{real2019regularized}, REINFORCE~\citep{williams1992simple}, and BOHB~\citep{falkner2018bohb}. For implementation details we refer the reader to~\cite{Dong2020NAS-Bench-201}. The hyperparameters in NAS-Bench-201 are fixed --- these results may not be invariant to hyperparameter choices, which may explain the low performance of e.g.\ DARTS.

We report results on the validation and test sets of CIFAR-10, CIFAR-100, and ImageNet-16-120 in Table~\ref{table:benchmarking}. Search times are reported for CIFAR-10 on a single GeForce GTX 1080 Ti GPU. As per the NAS-Bench-201 setup, the non-weight sharing methods are given a time budget of 12000 seconds. For NASWOT and the non-weight sharing methods, accuracies are averaged over 500 runs. For weight-sharing methods, accuracies are reported over 3 runs. We report NASWOT for sample sizes of \texttt{N}=10 and \texttt{N}=100. NASWOT is able to outperform all of the weight sharing methods while requiring a fraction of the search time.




The non-weight sharing methods do outperform NASWOT, though they also incur a large search time cost. It is encouraging however, that in a matter of seconds, NASWOT is able to find networks with performance close to the best non-weight sharing algorithms, suggesting that network architectures themselves contain almost as much information about final performance at initialisation as after training. 



Table~\ref{table:benchmarking} also shows the effect of sample size (\texttt{N}). We show the accuracy of networks chosen by our method for each~\texttt{N}. We list optimal accuracy for each~\texttt{N}, and random selection over the whole benchmark, both averaged over 500 runs. We observe that sample size increases NASWOT performance.

A key practical benefit of NASWOT is its execution time. Where time is a constraint, our method is very appealing. This may be important when repeating NAS several times, for instance for several hardware devices or datasets. This affords us the ability in future to specialise neural architectures for a task and resource environment cheaply, demanding only a few seconds per setup. 



\vspace{-4mm}
\section{Assisted Regularised EA - AREA}
\label{sec:area}

Our proposed score can also be incorporated into existing NAS algorithms. To demonstrate this we implemented a variant of REA~\citep{real2019regularized}, which we call Assisted-REA (AREA).

\vspace{-2mm}
{\tiny
      \begin{algorithm}[H]
    \caption{Assisted Regularised EA - AREA}
    \begin{algorithmic}[h]
           \State  population = []
                 \State  generator = RandomGenerator()
                 \For{\texttt{i=1:M}} 
              \State net = generator.generate() 
              \State \textcolor{teal}{scored\_net = net.score()}
              \State population.append(scored\_net)
          \EndFor
          \State Keep the top N scored networks in the population
          \State history = []
          \For{\texttt{net in population}} 
              
              \State \textcolor{purple}{trained\_net = net.train()}
              \State history.append(trained\_net)
          \EndFor
          \While{time limit not exceeded}
               
               \State Sample sub-population, S, without replacement from population
               \State Select network in S with highest accuracy as parent
               \State Mutate parent network to produce child
               \State Train child network
               \State Remove oldest network from population 
               \State population.append(child network)
               \State history.append(child network)
               
          \EndWhile
    \State chosen\_net = Network in history with highest accuracy
    \end{algorithmic}
    
    \label{algo:oursrea}
    \end{algorithm}
}


REA starts with a randomly-selected population (in our experiments the population size is 10). AREA instead randomly-samples a larger population (in our experiments we double the randomly-selected population size to 20) and uses our score to select the initial population (of size 10) for the REA algorithm. Pseudocode can be found in Algorithm~\ref{algo:oursrea}. Tables~\ref{table:benchmarking101} and~\ref{table:benchmarking} shows AREA improves upon REA in most cases across NAS-Bench-101 and NAS-Bench-201.


\section{Conclusion}

NAS has previously suffered from intractable search spaces and heavy search costs. Recent advances in producing tractable search spaces, through NAS benchmarks, have allowed us to investigate if such search costs can be avoided. In this work, we have shown that it is possible to navigate these spaces with a search algorithm---NASWOT---in a matter of seconds, relying on simple, intuitive observations made on initialised neural networks, that challenges more expensive black box methods involving training. Future applications of this approach to architecture search may allow us to use NAS to specialise architectures over multiple tasks and devices without the need for long training stages.
We also demonstrate how our score can be combined into an existing NAS algorithm. We find our score-assisted algorithm--- AREA---performs better than the original version on a number of benchmarks. 
This work is not without its limitations; our scope is restricted to convolutional architectures for image classification. However, we hope that this will be a powerful first step towards removing training from NAS and making architecture search cheaper, and more readily available to practitioners.



\paragraph{Acknowledgements.} This work was supported in part by the EPSRC Centre for Doctoral Training in Pervasive Parallelism and a Huawei DDMPLab Innovation Research Grant. The authors are grateful to Eleanor Platt, Massimiliano Patacchiola, and Paul Micaelli for their constructive comments. The authors would like to thank Xuanyi Dong for correspondence on NAS-Bench-201.




\bibliography{main}
\bibliographystyle{icml2021}
\clearpage 
\appendix

\section{Ablation Figures}
\label{appendix:ablate}






\begin{figure}[!h]
    \centering
    \includegraphics[width=.9\linewidth]{figures/naswot_ablation_hook_logdet_nasbench201_cifar100_none_0.05_1_True_128_20_1.pdf}
  \caption{Further ablation showing the effect of different CIFAR-100 image (top-left), initialisation (bottom-left), random input images (top-right), and mini-batch sizes (bottom-right) for 20 randomly selected NAS-Bench-201 architectures in the top 50 percentile CIFAR-100 accuracy range. For each network 20 samples were taken for each ablation.
  Effect of different CIFAR-100 image (top-left), initialisation (bottom-left), random input images (top-right), and mini-batch sizes (bottom-right) for 10 randomly selected NAS-Bench-201 architectures (one in each  percentile range from 50-55, ..., 95-100). Random input data are normally distributed. For each network 20 samples were taken for each ablation, using a mini-batch size of 128, except for mini-batch size for which mini-batch sizes of 32, 64, 128, and 256 were used. As the score depends on the mini-batch size we normalised the score by the minimum score of the sampled networks from the same mini-batch size.
  }
    \label{fig:ablationscore}  
\end{figure}

\FloatBarrier
\section{NAS Benchmarks}
\label{appendix:nasbench}



\begin{figure*}[!h]
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=.5\linewidth]{figures/nasbench101.pdf}
                \centering
                \caption{A NAS-Bench-101 cell.}
                \label{fig:nasdag101}
        \end{subfigure}\begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\linewidth]{figures/nasdag.pdf}
                \subcaption{A NAS-Bench-201 cell.}\label{fig:nasdag201}
                \includegraphics[width=.9\linewidth,right]{figures/skelegrow.pdf}
                \subcaption{The skeleton for NAS-Bench-101 and 201.}
                \label{fig:skelegrow}
        \end{subfigure}

\caption{(a): An example cell from NAS-Bench-101, represented as a directed acyclic graph. The cell has an input node, an output node, and 5 intermediate nodes, each representing an operation and connected by edges. Cells can have at most 9 nodes and at most 7 edges. NAS-Bench-101 contains 426k possible cells. By contrast, (b) shows a NAS-Bench-201~\citep{Dong2020NAS-Bench-201} cell, which uses nodes as intermediate states and edges as operations. The cell consists of an input node (A), two intermediate nodes (B, C) and an output node (D). An edge e.g.\ A B performs an operation on the state at A and adds it to the state at B. Note that there are 6 edges, and 5 possible operations allowed for each of these. This gives a total of  or 15625 possible cells. (c): Each cell is the constituent building block in an otherwise-fixed network skeleton (where N=5). As such, NAS-Bench-201 contains 15625 architectures.}
\label{fig:test}
\end{figure*}


\subsection{Nas-Bench-101}
In NAS-Bench-101, the search space is restricted as follows: algorithms must search for an individual cell which will be repeatedly stacked into a pre-defined skeleton, shown in Figure~\ref{fig:skelegrow}. Each cell can be represented as a directed acyclic graph (DAG) with up to 9 nodes and up to 7 edges. Each node represents an operation, and each edge represents a state. Operations can be chosen from:  convolution,  convolution,  max pool. An example of this is shown in Figure~\ref{fig:nasdag101}. After de-duplication, this search space contains 423,624 possible neural networks. These have been trained exhaustively, with three different initialisations, on the CIFAR-10 dataset for 108 epochs. 

\subsection{NAS-Bench-201}
\label{sec:nasbench201}


In NAS-Bench-201, networks also share a common skeleton (Figure~\ref{fig:skelegrow}) that consists of stacks of its unique {\it cell} interleaved with fixed residual downsampling blocks. Each cell (Figure~\ref{fig:nasdag201}) can be represented as a densely-connected DAG of 4 ordered nodes (A, B, C, D) where node A is the input and node D is the output. In this graph, there is an edge connecting each node to all subsequent nodes (A B, A C, A D, B C, B D, C D) for a total of 6 edges, and each edge can perform one of 5 possible operations (Zeroise, Identity,  convolution,  convolution,  average pool). The search space consists of every possible cell. As there are 6 edges, on which there may be one of 5 operations, this means that there are  possible cells. This makes for a total of 15625 networks as each network uses just one of these cells repeatedly. The authors have manually split CIFAR-10, CIFAR-100, and ImageNet-16-120~\citep{chrabaszcz2017downsampled} into train/val/test, and provide full training results across all networks for (i) training on train, evaluation on val, and (ii) training on train/val, evaluation on test. The split sizes are 25k/25k/10k for CIFAR-10, 50k/5k/5k for CIFAR-100, and 151.7k/3k/3k for ImageNet-16-120. We obtained these datasets via the NAS-Bench-201 repository~\citep{dong_2020}.

\end{document}
