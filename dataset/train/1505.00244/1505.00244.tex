\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amssymb,dsfont}
\usepackage{amsthm}
\usepackage{url,hyperref}

\usepackage{algorithm,algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}




\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{claim}{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{question}{Question}




\def\tr{{\rm tr}} \def\rank{{\rm rank}} 
\def\Pr{{\mathbb{P}}} \def\bin{{\rm bin}}
\def\ind{{\mathds{1}}}\def\I{{\mathcal{I}}} \def\SNR{{\scriptstyle{\bf SNR}}} \def\Ei{{\rm E}_{\rm i}} \def\rect{{\rm rect}} \def\sinc{{\rm sinc}} \def\N{{\mathbb{N}}} \def\Z{{\mathbb{Z}}} \def\R{{\mathbb{R}}} \def\Rplus{{\mathbb{R}^+}} \def\C{{\mathbb{C}}} 

\def\bbI{{\mathcal{\boldsymbol I}}}
\def\bbC{{\mathcal{\boldsymbol C}}}

\newcommand{\alg}{\mathcal{M}}
\newcommand{\mech}{\mathcal{M}}
\newcommand{\eps}{\varepsilon}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\univ}{U}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Lap}{Lap}
\DeclareMathOperator{\dif}{d}
\DeclareMathOperator{\diag}{diag}

\def\b0{{\bf 0}}


\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bof{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bl{{\bf l}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf Z}}


\def\bdelta{{\bf \delta}}
\def\bphi{{\bf \phi}}
\def\bmu{{\bf \mu}}

\def\bPsi{{\bf \Psi}}
\def\bPhi{{\bf \Phi}}
\def\bXi{{\bf \Xi}}
\def\bSigma{{\bf \Sigma}}
\def\bUpsilon{{\bf \Upsilon}}
\def\bLambda{{\bf \Lambda}}
\def\bGamma{{\bf \Gamma}}
\def\bOmega{{\bf \Omega}}
\def\bTheta{{\bf \Theta}}



\newcommand\junk[1]{}

\newcommand{\maps}{\colon} \DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\herdisc}{herdisc}
\DeclareMathOperator{\lindisc}{lindisc}
\DeclareMathOperator{\vecdisc}{vecdisc}
\DeclareMathOperator{\hvdisc}{hvdisc}
\DeclareMathOperator{\rdisc}{rdisc}
\DeclareMathOperator{\hrdisc}{hrdisc}
\DeclareMathOperator{\specLB}{SpecLB}
\DeclareMathOperator{\detlb}{detlb}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\opt}{opt}

\DeclareMathOperator{\polylog}{polylog}

\newcommand{\menote}[1]{}

\renewcommand{\SS}{\mathcal{S}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\II}{\mathcal{I}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\AP}{\mathcal{AP}}
\newcommand{\HAP}{\mathcal{HAP}}
\newcommand{\enorm}[1]{\|#1\|_{E\infty}}
\newcommand{\tra}{\intercal}

\newcommand{\rec}{\mathcal{R}}

\renewcommand{\univ}{\mathcal{U}}
\newcommand{\quer}{\mathcal{Q}}
\newcommand{\row}{e}
\newcommand{\NP}{\mathsf{NP}}
\renewcommand{\P}{\mathsf{P}}


\begin{document}
\title{An Improved Private Mechanism for Small Databases}
\author{Aleksandar Nikolov\\{Microsoft Research}\\{ Redmond, WA,
    USA}\\{alenik@microsoft.com}} 
\date{}
\maketitle              
\begin{abstract}
  We study the problem of answering a workload of linear queries
  , on a database of size at most  drawn from a
  universe  under the constraint of (approximate) differential
  privacy. Nikolov, Talwar, and Zhang~\cite{NTZ} proposed an efficient
  mechanism that, for any given  and , answers the queries
  with average error that is at most a factor polynomial in  and  worse than the best possible. Here we
  improve on this guarantee and give a mechanism whose competitiveness
  ratio is at most polynomial in  and , and has
  no dependence on . Our mechanism is based on the projection
  mechanism of~\cite{NTZ}, but in place of an ad-hoc noise
  distribution, we use a distribution which is in a sense optimal for
  the projection mechanism, and analyze it using convex duality and
  the restricted invertibility principle.
\end{abstract}

\section{Introduction}

The central problem of private data analysis is to characterize to
what extent it is possible to compute useful information from
statistical data without compromising the privacy of the individuals
represented in the dataset. In order to formulate this problem
precisely, we need a database model and a definition of what it means
to preserve privacy. Following prior work, we model a database as a
multiset  of  elements from a universe \junk{, i.e.~}, with each database element specifying the data of a
single individual. Defining privacy is more subtle. A definition which
has received considerable attention in recent years is
\emph{differential privacy}, which postulates that a randomized
algorithm preserves privacy if its distribution on outputs is almost
the same (in an appropriate metric) on any two input databases  and
 that differ in the data of at most a single individual. The
formal definition is as follows:
\begin{definition}[\cite{DMNS}]\label{def:DP}
  Two databases  and  are \emph{neighboring} if the size of
  their symmetric difference is at most one. A randomized algorithm
   satisfies \emph{-differential privacy} if for any
  two neighboring databases  and  and any measurable event 
  in the range of ,
  
  \junk{Above, probabilities are taken over the internal coin tosses of .}
\end{definition}
Differential privacy has a number of desirable properties: it is
invariant under post-processing, the privacy loss degrades smoothly
under (possibly adaptive) composition, and the privacy guarantees hold
in the face of arbitrary side information. We will adopt it as our
definition of choice in this paper. We will work in the regime , which is often called approximate differential privacy, to
distinguish it from pure differential privacy, which is the case
. Approximate differential privacy provides strong semantic
guarantees when  is  : roughly speaking,
it implies that with probability at least , an
arbitrarily informed adversary cannot guess from the output of the
algorithm if any particular user is represented in the
database. See~\cite{GantaKS08} for a precise formulation of this
semantic guarantee.

We then turn to the question of understanding the constraints imposed
by privacy on the kinds of computation we can perform. We focus on
computing answers to a fundamental class of database queries: the
\emph{linear queries}, which generalize counting queries.  A counting
query counts the number of database elements that satisfy a given
predicate; a linear query allows for weighted
counts. Formally, a linear query is specified by a function
 ( in the case of
counting queries); slightly abusing notation, we define the value of
the query as  (elements of
 are counted with multiplicity). We call a set  of linear
queries a \emph{workload}, and an algorithm that answers a query
workload a \emph{mechanism}. 

Since the work of Dinur and
Nissim~\cite{DN}, it has been known that answering queries too
accurately can lead to very dramatic privacy breaches, and this is
true even for counting queries. For example, in~\cite{DN,DMT07} it was
shown that answering  random counting queries with error
per query  allows an adversary to reconstruct a very
accurate representation of a database of size , which contradicts
any reasonable privacy notion. On the other hand, a simple mechanism
that adds independent Gaussian noise to each query answer achieves
-differential privacy and answers any set  of
counting queries with average error
~\cite{DN,DworkN04,DMNS}.\footnote{Here and in the
  remainder of the introduction we ignore dependence of the error on
   and .} While this is a useful guarantee for a small
number of queries, it quickly loses value when  is much
larger than the database size, and becomes trivial for 
queries. Nevetheless, since the seminal paper of Blum, Ligett and
Roth~\cite{BLR}, a long line of
work~\cite{DworkNRRV09,DworkRV10,RothR10,HardtR10,GuptaHRU11,HardtLM12,GuptaRU11}
has shown that even when , more sophisticated
private mechanisms can achieve error not much larger than
. For instance, there exist -differentially private mechanisms for linear queries that
acheive average error
~\cite{GuptaRU11}. There are sets of
counting queries for which this bound is tight up to factors
polylogarithmic in the size of the database~\cite{BunUV13}.

Specific query workloads allow for error which is much better than the
worst-case bounds. Some natural examples are queries counting the
number of points in a line interval or a -dimensional axis-aligned
box~\cite{dwork-continual,ChanSS10,XiaoWG10}, or a -dimensional
halfspace~\cite{halfspaces}. It is, therefore, desirable to have
mechanisms whose error bounds adapt \emph{both} to the query workload
and to the database size. In particular, if  is the
best possible average error\footnote{We give a formal definition later.}
achievable under differential privacy for the workload  on
databases of size at most , we would like to have a mechanism with
error at most a small factor larger than  for any 
and . The first result of this type is due to Nikolov, Talwar,
and Zhang~\cite{NTZ}, who presented a mechanism running in time
polynomial in , , and , with error at most
.

Here we improve the results from~\cite{NTZ}:
\begin{theorem}[Informal]\label{thm:main}
  There exists a mechanism that, given a database of size  drawn
  from a universe , and a workload  of linear queries,
  runs in time polynomial in ,  and , and has
  average error per query at most .
\end{theorem}

Notice that the competitiveness ratio in Theorem~\ref{thm:main} is
\emph{independent of the number of queries}, which can be
significantly larger than both  and . This type of
guarantee is easier to prove when , because in that case
there exist nearly optimal mechanisms that are oblivious of the
database size~\cite{NTZ}. Therefore, we focus on the more
challenging regime of small databases, i.e.~. 

It is worth making a couple of remarks about the strength of
Theorem~\ref{thm:main}. First, in many applications the queries in
 are represented compactly rather than by a truth table, and
 is exponentially large in the size of a natural
representation of the input. In such cases, running time bounds which
are polynomial in  may be prohibitive. Nevertheless, our work
still gives interesting information theoretic bounds on the optimal
error, and, moreover, our mechanism can be a starting point for
developing more efficient variants. Furthermore, under a plausible
complexity theoretic hypothesis, our running time guarantee is the
best one can hope for without making further assumptions on
~\cite{Ullman13}. A second remark is that our optimal error
guarantees are in terms of \emph{average} error, while many papers in
the literature consider worst-case error. Proving a result analogous
to Theorem~\ref{thm:main} for worst-case error remains an interesting
open problem.

\junk{Another
interesting problem is to remove the dependence on the universe size
in the competitiveness ratio. It is plausible that this can be done
with the projection mechanism and a well-chosen Gaussian noise
distribution, but we would need tighter lower bounds, possibly based
on fingerprinting codes as in~\cite{BunUV13}.}






\paragraph{Techniques.}
Following the ideas of~\cite{NTZ}, our starting point is a
generalization of the well-known Gaussian noise mechanism, which adds
appropriately scaled correlated Gaussian noise to the queries. By
itself, this mechanism is sufficient to guarantee privacy, but its
error is too large when . The main insight
of~\cite{NTZ} was to use the knowledge that the database is small to
reduce the error via a post-processing step. The post-processing is a
form of regression: we find the vector of answers that is closest to the
noisy answers while still consistent with the database size bound.
(In fact the estimator is slightly more complicated and related to the
hybrid estimator of Zhang~\cite{Zhang13-hybrid}.)  Intuitively, when
 is small compared to the number of queries, this regression step
cancels a significant fraction of the error.

Our first novel contribution is to analyze the error of this mechanism
for arbitrary noise distributions and formulate it as a convex
function of the covariance matrix of the noise. Then we write a convex
program that captures the problem of finding the covariance matrix for
which the performance of the mechanism is optimized on the given query
workload and database size bound. We use Gaussian noise with this
optimal covariance in place of the recursively constructed ad-hoc
noise distribution\footnote{The distribution in~\cite{NTZ} is
  independent of the database size bound. This could be a reason why
  their guarantees scale with  rather than .}
from~\cite{NTZ}. Finally, we relate the dual of the convex program to
a spectral lower bound on  via the restricted
invertibility principle of Bourgain and Tzafriri~\cite{bour-tza}. We
stress that while the restricted invertibility principle was used
in~\cite{NTZ} as well, here we need a new argument which works for the
optimal covariance matrix we compute and gives a smaller
competitiveness ratio.

In addition to the improvement in the competitiveness ratio, our
approach here is more direct and we believe it gives a better
understanding of the performance of the regression-based mechanism for
small databases.

\section{Preliminaries}

We use capital letters for matrices and lower-case letters for vectors
and scalars. We use  for the standard
inner product between vectors in . For a matrix  and a set , we use  for the
submatrix consisting of the columns of  indexed by elements of
. We use the notation  to denote that  is a positive
definite matrix, and  to denote that it is positive
semidefinite. We use  for the smallest singular
value of , i.e.~. We use  for the trace operator, and
 for the  operator norm of ,
i.e.~.

The distribution of a multivariate Gaussian
with mean  and covariance  is denoted .

\subsection{Histograms, the Query Matrix, and the Sensitivity Polytope}

It will be convenient to encode the problem of releasing answers to
linear queries using linear-algebraic notation. A common and very
useful representation of a database  is the
\emph{histogram representation}: the histogram of  is a vector  such that for any ,  is equal to
the number of copies of  in . Notice that  and
also that if  and  are respectively the histograms of two
neighboring databases  and , then  (here
 is the standard 
norm). Linear queries are a linear transformation of . More
concretely, let us define the \emph{query matrix}  associated with a set of linear queries  by
. Then it is easy to see that the vector 
gives the answers to the queries  on a database  with
histogram .

Since this does not lead to any loss in generality, for the remainder
of this chapter we will assume that databases are given to mechanisms
as histograms, and workloads of linear queries are given as query
matrices. We will identify the space of size- databases with
histograms in the scaled  ball , and we will identify neighboring
databases with histograms  such that . 


The \emph{sensitivity polytope}  of a query matrix  is the convex hull of the columns of  and
the columns of . Equivalently, ,
i.e.~the image of the unit  ball in  under
multiplication by . Notice that  is
the symmetric convex hull\footnote{The symmetric convex hull of a set
  of points  is equal to the convex hull of .} of the possible vectors of query answers to the
queries in  on databases of size at most .


\subsection{Measures of Error and the Spectral Lower Bound}

As our basic notion of error we will consider mean squared error.
For a mechanism  and a subset , let us
define the error with respect to the query matrix  as

where the expectation is taken over the random coins of
.
We also write  as . The optimal
error achievable by any -differentially private
mechanism for the query matrix  and databases of size up to  is

where the infimum is taken over all -differentially
private mechanisms .

Arguing directly about  appears
difficult. For this reason we use the following spectral lower bound
from~\cite{NTZ}. This lower bound was implicit in previous papers, for example~\cite{shiva2010}.

\begin{theorem}[\cite{NTZ}]\label{thm:speclb-small}
  There exists a constant  such that for any query matrix , any small enough , and any 
  small enough with respect to , , where
  

\end{theorem}

\subsection{Composition and the Gaussian Mechanism}

An important basic property of differential privacy is that the
privacy guarantees degrade smoothly under composition and are not
affected by post-processing.

\begin{lemma}[\cite{DMNS,odo}]
  \label{lm:simple-composition}
  Let  satisfy -differential privacy, and
   satisfy -differential
  privacy for any fixed . Then the mechanism
   satisfies
  -differential privacy.
\end{lemma}

A basic method to achieve -differential privacy is the
Gaussian mechanism. We  use the following generalized variant,
introduced in~\cite{NTZ}.

\begin{theorem}[\cite{DN,DworkN04,DMNS,NTZ}]\label{thm:gaussian}
  Let  be a set of queries with query matrix , and let
  , , be such that
   for all columns  of . Then the
  mechanism  where  and  satisfies -differential privacy.
\end{theorem}

\section{The Projection Mechanism}

A key element in our mechanism is the use
of least squares estimation to reduce error on small databases. In this section we
introduce and analyze a mechanism based on least squares estimation,
similar to the hybrid estimator of~\cite{Zhang13-hybrid}. Essentially
the same mechanism was used in~\cite{NTZ}, but the definition and
analysis were tied to a particular noise distribution.

\begin{algorithm}
  \caption{Projection Mechanism    }\label{alg:proj}
  \begin{algorithmic}[1]
    \REQUIRE \emph{(Public)} Query matrix ; matrix  such that  for all columns  of .
    \REQUIRE \emph{(Private)} Histogram  of a database of size
    .
    
    \STATE Run the generalized Gaussian mechanism
    (Theorem~\ref{thm:gaussian}) to compute ;

    \STATE Let  be the orthogonal projection operator onto the
    span of the eigenvectors corresponding to the   largest eigenvalues of 

    \STATE Compute , where  is the sensitivity
    polytope of , and  is
    

    \ENSURE  Vector of answers .
  \end{algorithmic}
\end{algorithm}

As shown in~\cite{NTZ,conjunctions}, Algorithm~\ref{alg:proj} can be
efficiently implemented using the ellipsoid algorithm or the
Frank-Wolfe algorithm.

To analyze the error of the Projection Mechanism, we use the following
key lemma, which appears to be standard in statistics (we refer
to~\cite{NTZ,conjunctions} for a proof). Recall that for a convex body
(compact convex set with non-empty interior) , the
\emph{Minkowski norm} (\emph{gauge function}) is defined by  for any . The \emph{polar body} is  and the
corresponding norm is also equal to the \emph{support function} of
: . When  is symmetric around  (i.e.~), the Minkowski norm
and support function are both norms in the usual sense.

\begin{lemma}[\cite{NTZ,conjunctions}]\label{lm:lse}
  Let  be a symmetric convex body, and let . Let, finally, . We have 
\end{lemma}

The next lemma gives our analysis of the error of the Projection
Mechanism.

\begin{lemma}\label{lm:proj-err}
  Assume  is such that  for all columns  of . Then the Projection Mechanism
   in Algorithm~\ref{alg:proj} is -differentially private. Moreover, for ,
   where  are the eigenvalues of .
\end{lemma}
\begin{proof} 
  To prove the privacy guarantee, observe that the output of
   is just a post-processing of the
  output of , i.e.~the algorithm does not access 
  except to pass it to . The privacy then follows
  from Theorem~\ref{thm:gaussian} and
  Lemma~\ref{lm:simple-composition}.

  Next we bound the error. Let  be the true answers to
  the queries, and let  be the random noise introduced by the
  generalized Gaussian mechanism.  By the Pythagorean theorem and
  linearity of expectation, the expected total squared error of the
  projection mechanism is
  
  Above and in the remainder of the proof expectations are taken
  with respect to the randomness in the choice of .  We bound the
  two terms on the right hand side separately. We will show:
  
  \eqref{eq:top-eig} and \eqref{eq:bott-eig} together imply the error
  bound in the theorem.

  To prove \eqref{eq:top-eig}, observe that . By the definition of , the
  non-zero eigenvalues of  are  where . We have
  

  To prove \eqref{eq:bott-eig} we appeal to Lemma~\ref{lm:lse}. Define
  . With  in the place of
  , the lemma implies that
  
  where we used the simple fact
  
   is the convex hull of the columns of  and the
  columns of . For any such column  we have
  
  The first inequality is by the assumption on ; the second
  follows because ;
  the third inequality is due to the fact that the smallest eigenvalue
  of  restricted to the range of  is
   by the choice of . Therefore,
  . Since a linear
  functional attains its maximum value over a polytope at a vertex, we
  have . Each inner product  is a centered Gaussian random variable with variance  By the choice of , the largest
  eigenvalue of  is . From this fact and the inequality , we have that the variance of  is at most . By a
  standard concentration argument, we can bound the expectation of the
  maximum absolute value of the inner products as
  
  Plugging this into \eqref{eq:err-bnd-lse}, we get
  
  To show that this implies \eqref{eq:bott-eig}, observe that, by
  averaging, .  Since , . This finishes the
  proof of \eqref{eq:bott-eig}, and, therefore, of the theorem.
\end{proof}


\section{Optimality of the Projection Mechanism}

In this section we show that we can choose a covariance matrix
 so that  has nearly optimal
error:
\begin{theorem}\label{thm:main-smalldb}
  Let  be a small enough constant and let  be small enough with respect to . For any
  query matrix , and any database size
  bound , there exists a covariance matrix  such
  that the Projection Mechanism  in
  Algorithm~\ref{alg:proj} is -differentially private
  and has error
  
  Moreover,  can be computed in time polynomial in .
\end{theorem}

Theorem~\ref{thm:main-smalldb} is the formal statement of
Theorem~\ref{thm:main}. (Recall again that Algorithm~\ref{alg:proj}
can be implemented in time polynomial in ,  and , as shown in~\cite{NTZ,conjunctions}.)


To prove the theorem, we optimize over the choices of  that
ensure -differential privacy, and use convex duality
and the restricted invertibility principle to relate the optimal
covariance to the spectral lower bound.

\subsection{Minimizing the Ky Fan Norm}

Recall that for an  matrix  with
eigenvalues , and a
positive integer , the Ky Fan -norm is defined as
.  The
covariance matrix  we use in the projection mechanism will be
the one achieving , where  is the column of the query matrix
 associated with the universe element . This choice is directly
motivated by Lemma~\ref{lm:proj-err}.  We can write this optimization
problem in the following way.

The program above has a geometric meaning. For a positive definite
matrix , the set  is an ellipsoid centered at the origin. The constraint
\eqref{eq:ellips-enclose-small} means that  has to contain all
columns of the query matrix . The objective function
\eqref{eq:ellips-obj-small} is equal to the sum of squared lengths of
the  longest major axes of . Therefore, we are looking for
the smallest  ellipsoid centered at the origin that contains the
columns of , where the ``size'' of the ellipsoid is the sum of
squared lengths of the  longest major axes. We will not use this
geometric interpretation in the rest of the paper.

We will show that
\eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} is a
convex optimization problem. This will allow us to use general tools
such as the ellipsoid method to find an optimal solution, and also to
use duality theory in order to analyze the value of the optimal
solution.

To show that
\eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} is convex
we will need the following well-known result of Fan.

\begin{lemma}[\cite{Fan49}]\label{lm:kyfan}
  For any  real symmetric matrix , 
  
\end{lemma}

With this result in hand, we can prove that
\eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} is a
convex optimization problem.

\begin{lemma}\label{lm:ellips-program-small}
  The objective function \eqref{eq:ellips-obj-small} and constraints
  \eqref{eq:ellips-enclose-small} are convex over .
\end{lemma}
\begin{proof}
  The objective function and the constraints
  \eqref{eq:ellips-enclose-small} are affine, and therefore convex. It
  remains to show that the objective \eqref{eq:ellips-obj-small} is
  also convex. Let  and  be two feasible solutions and
  define  for some . Because the matrix inverse is operator convex (see
  e.g.~\cite{Bhatia-MA}), . Let  be such that  and . Such a  exists by by
  Lemma~\ref{lm:kyfan}. We have, again using Lemma~\ref{lm:kyfan},
  
  This finishes the proof. 
\end{proof}

Since the program
\eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} is
convex, its optimal solution can be approximated in polynomial time within
any given degree of accuracy using the ellipsoid
algorithm~\cite{GLS-ellipsoid}.

\subsection{A Special Function}

Before we continue, we need to introduce a somewhat complicated
function of the singular values of a matrix. This function will turn
out to be the objective funciton in a maximization problem which is
dual to \eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small}.
The next lemma is needed to argue that this function is
well-defined. The lemma was proved in~\cite{simplex}.

\begin{lemma}[\cite{simplex}]\label{lm:singvals-thresh}
  Let  be non-negative reals,
  and let  be a positive integer. There exists a unique integer
  , , such that
  
  with the convention . 
\end{lemma}

We are now ready to define the function:
\begin{definition}
  Let  be an  positive semidefinite
  matrix with singular values , and
  let  be a positive integer. The function  is
  defined as
  
  where  is the unique integer such that 
  
\end{definition}
Lemma~\ref{lm:singvals-thresh} guarantees that  is a
well-defined real-valued function. In the next lemma we also show that
it is continuous.

\begin{lemma}\label{lm:hk-contin}
  The function  is continuous over positive semidefinite
  matrices with respect to the operator norm.  
\end{lemma}
\begin{proof}
  Let  be a  positive semidefinite matrix with singular
  values  and let , , be the
  unique integer for which . If , then setting  small enough ensures that, for
  any  such that ,  and  are
  computed with the same value of . In this case, the proof of
  continuity follows from the continuity of the square root
  function. Let us therefore assume that  for some . Then  for any integer ,
  
  We then have
  
  For any  such that  for a small enough
  , we have
  
for some integer   in . Continuity then follows from
  \eqref{eq:hk-irrev-t}, and the
  continuity of the square root function. 
\end{proof}


\subsection{The Dual of the Ky Fan Norm Minimization Problem}

Our next goal is derive a dual characterization of
\eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small}, which we
will then relate to the spectral lower bound . It is
useful to work with the dual, because it is a maximization problem, so
to prove optimality we just need to show that any feasible solution of
the dual gives a lower bound on the optimal error under differential
privacy.


The next theorem gives our dual characterization in terms of the
special function  defined in the previous section.

\begin{theorem}\label{thm:nuclear-small}
  Let  be a rank
   matrix, and let  be the optimal value of
  \eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small}. Then,
  
\end{theorem}

Since the objective of
\eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} is not
necessarily differentiable, in order to analyze the dual and prove
Theorem~\ref{thm:nuclear-small}, we need to recall the concepts of
subgradients and subdifferentials. A \emph{subgradient} of a convex
function  at , where  is some open subset
of , is a vector  so that for every  we
have

The set of subgradients of  at  is denoted  and
is known as the \emph{subdifferential}. When  is differentiable at
, the subdifferential is a singleton set containing only the
gradient . If  is defined by ,
where  , then . A basic fact in convex analysis is that 
achieves its minimum at   if and only if . For
more information on subgradients and subdifferentials, see the
classical text of Rockafellar~\cite{Rockafellar}. 

Overton and Womersley~\cite{OvertonW93-kyfan} analyzed the
subgradients of functions which are a composition of a differentiable
matrix-valued function with a Ky Fan norm. The special case we need
also follows from the results of Lewis~\cite{Lewis95-matfunc}.
\begin{lemma}[\cite{OvertonW93-kyfan},\cite{Lewis95-matfunc}]\label{lm:kyfan-subgr}
  Let  for a positive definite
  matrix . Let  be the
  singular values of  and let  be the diagonal matrix
  with the  on the diagonal. Assume that for some , . Then the subgradients of  are
  given by
  
  where  is the submatrix of  indexed by . 
\end{lemma}

We use the following well-known characterization of the convex
hull of boolean vectors of Hamming weight . For a proof,
see~\cite{schrijver-combop-B}. 

\begin{lemma}\label{lm:unik-poly}
  Let . Then
  . 
\end{lemma}

Before we prove Theorem~\ref{thm:nuclear-small}, we need one more
technical lemma.

\begin{lemma}\label{lm:subgr-soln}
  Let  be an  positive semidefinite matrix of rank at
  least . Then there exists an  positive definite matrix
   such that , and . 
\end{lemma}
\begin{proof}
  Let , and let  be
  the non-zero singular values of . Let  be
  some singular value decomposition of :  is an orthonormal
  matrix and  is a diagonal matrix with the  on the
  diagonal, followed by s.

  Assume that , , is the integer (guaranteed by
  Lemma~\ref{lm:singvals-thresh}) for which  and define . Since  and
 we assumed  has rank at least , we have .  Define
  
  and set  be the diagonal matrix with the  on the diagonal.
  We set  to be an arbitrary number satisfying . Let us set . By
  Lemma~\ref{lm:unik-poly} and and the choice of , the vector
   is an element of the polytope
  . Then  is an element of . Since this set is a subset of , we
  have . A calculation shows that
  . This completes
  the proof. 
\end{proof}

\begin{proof}[of Theorem~\ref{thm:nuclear-small}]
  We will use standard notions from the theory of convex duality. For
  a reference, see the book by Boyd and Vandenberghe~\cite{BoydV-cvx}.

  Let us define  to be the domain for the
  constraints \eqref{eq:ellips-enclose-small} and the objective
  function \eqref{eq:ellips-obj-small}. This makes the constraint  implicit.  The optimization problem is convex by
  Lemma~\ref{lm:ellips-program-small}. Is is also always feasible: for
  example for  an upper bound on the Euclidean norm of the longest
  column of ,  is a feasible
  solution. Slater's condition is therefore satisfied, since the
  constraints are affine, and, therefore, strong duality holds.

  The Lagrange dual function for
  \eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} is
  
  with dual variables , . Equivalently, we
  can define the diagonal matrix , , with entries , and the dual function becomes
  
  Since the terms  and  are
  non-negative for any , . Therefore, the effective domain  of
   is . Since we have
  strong duality, .
  
  By the additivity of subgradients, a matrix   achieves the minimum
  in \eqref{eq:g-raw-small} if and only if ,
  where .  Consider first the case in which
   has rank at least . Then, by Lemma~\ref{lm:subgr-soln},
  there exists an  such that  and
  . Observe that, if  is an  matrix such
  that  and ,
  then 
  
  Since, by Lemma~\ref{lm:kyfan-subgr} and ,  is a convex combination of matrices  with  as above, it follows that . Then we have
  

  If  is such that  has rank less than , we can reduce
  to the rank  case by a continuity argument. Fix any non-negative
  diagonal matrix  and for  define . For any ,
   has rank , since  has rank
   by assumption, and, therefore, . Then, by Corollary
  7.5.1.~in~\cite{Rockafellar}, and \eqref{eq:g-small-raw}, we have
   
  The final equality follows from the continuity of , proved in
  Lemma~\ref{lm:hk-contin}. 
  
  Let us define new variables  and , where  and . Because  is homogeneous with exponent , we can
  re-write  as .
  From the first-order optimality condition , we see that maximum of  is achieved
  when  and is equal to
  . Therefore maximizing  over diagonal
  positive semidefinite  is equivalent to the optimization problem
  \eqref{eq:nuclear-obj2-small}--\eqref{eq:nuclear-pos2-small}. Since,
  by strong duality, the maximum of  is equal to the optimal
  value of
  \eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small}, this
  completes the proof. 
\end{proof}

\subsection{Proof of  Theorem~\ref{thm:main-smalldb}}

Our strategy will be to use the dual formulation in
Theorem~\ref{thm:nuclear-small} and the restricted invertibility
principle to give a lower bound on . First we state the
restricted invertiblity principle and a consequence of it proved in~\cite{apx-disc}.

\begin{theorem}[\cite{bour-tza,bt-constructive}]\label{thm:bt}
  Let , let  be an  real matrix, and
  let  be an  diagonal matrix such that 
  and . For any integer  such that  there exists a subset
   of size  such that
  . 
\end{theorem}

For the following lemma, which is a consequence of
Theorem~\ref{thm:bt}, we need to recall the definition of the trace
(nuclear) norm of a matrix :  is equal to the
sum of singular values of .

\begin{lemma}[\cite{apx-disc}]\label{lm:bt-lb}
  Let  be an  by  real matrix of rank , and let  be a diagonal matrix such that . Then there exists a
  submatrix  of , , such that
  , for a universal constant .
\end{lemma}


\begin{proof}[of Theorem~\ref{thm:main-smalldb}]
  Given a database size  and a query matrix , we compute the
  covariance matrix  as follows. We compute a matrix  which
  gives an (approximately) optimal solution to
  \eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} for  , and we
  set . Since
  \eqref{eq:ellips-obj-small}--\eqref{eq:ellips-enclose-small} is a
  convex optimization problem, it can be solved in time polynomial in
   to any degree of accuracy using the ellipsoid
  algorithm~\cite{GLS-ellipsoid} (or the algorithm of Overton and
  Womersley~\cite{OvertonW93-kyfan}). By Lemma~\ref{lm:proj-err} and
  the constraints \eqref{eq:ellips-enclose-small},
   is -differentially
  private with this choice of .

  By Lemma~\ref{lm:proj-err},
  
  By Theorem~\ref{thm:nuclear-small}, the optimal solution  of
  \eqref{eq:nuclear-obj2-small}--\eqref{eq:nuclear-pos2-small}
  satisfies
  
  where  are the eigenvalues
  of  and , , is an integer such that . At least one of  and  must be bounded from below by
  . Next we consider these two cases separately.

  Assume first that . Let  be the orthogonal projection
  operator onto the eigenspace of 
  corresponding to . Then, because
   are the nonzero singular
  values of , we have . By Lemma~\ref{lm:bt-lb} applied to
  the matrices  and , there exists a set  of size at most ,
  such that
  
  for an absolute constant .

  For the second case, assume that . Let  now be an orthogonal
  projection operator onto the eigenspace of  corresponding
  to . By the choice of , we
  have
  
  By Theorem~\ref{thm:bt}, applied with , , and
  , there exists a set  of size
   so that
  
  The theorem follows from \eqref{eq:proj-err}, the fact that at least
  one of \eqref{eq:main-lb-small-1} or \eqref{eq:main-lb-small-2} holds,
  and Theorem~\ref{thm:speclb-small}. 
\end{proof}

\section{Conclusion}

Several natural problems remain open. Probably the most important one
is to prove results analogous to ours for worst case, rather than
average, error. In that case the simple post-processing strategy of
the projection mechanism will likely not be sufficient. Another
interesting problem is to remove the dependence on the universe size
in the competetiveness ratio. It is plausible that this can be done
with the projection mechanism and a well-chosen Gaussian noise
distribution, but we would need tighter lower bounds, possibly based
on fingerprinting codes as in~\cite{BunUV13}.

\section*{Acknowledgments}
The author would like to thank the anonymous reviewers of ICALP 2015 for helpful comments.

\bibliographystyle{alpha}
\bibliography{Privacy,mypapers}







\end{document}
