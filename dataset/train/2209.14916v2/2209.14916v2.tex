\vspace{-5pt}
\section{Experiments}
\label{sec:experiments}
\vspace{-5pt}

\begin{figure}[t!]
\centering
\begin{overpic}[width=\textwidth]{figures/edit.pdf}
\end{overpic}
\vspace{-5pt}
\caption{
\textbf{Editing applications.} Light blue frames represent motion input and bronze frames are the generated motion. Motion in-betweening (left+center) can be performed conditioned on text or without condition by the same model. Specific body part editing using text is demonstrated on the right: the lower body joints are fixed to the input motion while the upper body is altered to fit the input text prompt. 
}
\vspace{-10pt}
\label{fig:edit}
\end{figure}


We implement \ourmethod{} for three motion generation tasks: Text-to-Motion(\ref{sec:t2m}), Action-to-Motion(\ref{sec:a2m}) and unconditioned generation(\ref{sec:uncond}. Each sub-section reviews the data and metrics of the used benchmarks, provides implementation details, and presents qualitative and quantitative results. Then, we show implementations of motion in-betweening (both conditioned and unconditioned) and body-part editing by adapting diffusion inpainting to motion (\ref{sec:edit}).
Our models have been trained with $T=1000$ noising steps and a cosine noise schedule.
All of them have been trained on a single \textit{NVIDIA GeForce RTX 2080 Ti} GPU for a period of about $3$ days. 


\vspace{-5pt}
\subsection{Text-to-Motion}
\label{sec:t2m}
\vspace{-5pt}

Text-to-motion is the task of generating motion given an input text prompt. The output motion is expected to be both implementing the textual description, and a valid sample from the data distribution (i.e. adhering to general human abilities and the rules of physics). In addition, for each text prompt, we also expect a distribution of motions matching it, rather than just a single result. We evaluate our model using two leading benchmarks - KIT~\citep{plappert2016kit} and HumanML3D~\citep{guo2022generating}, over the set of metrics suggested by \citet{guo2022generating}: 
 \textit{R-precision} and \textit{Multimodal-Dist} measure the relevancy of the generated motions to the input prompts, \textit{FID} measures the dissimilarity between the generated and ground truth distributions (in latent space), \textit{Diversity} measures the variability in the resulting motion distribution, and \textit{MultiModality} is the average variance given a single text prompt. For the full implementation of the metrics, please refer to \citet{guo2022generating}.
We use HumanML3D as a platform to compare different backbones of our model, discovering that the diffusion framework is relatively agnostic to this attribute.
In addition, we conduct a user study comparing our model to current art and ground truth motions. 

\textbf{Data.}
HumanML3D is a recent dataset, textually re-annotating motion capture from the AMASS~\citep{AMASS:ICCV:2019} and HumanAct12~\citep{guo2020action2motion} collections. 
It contains $14,616$ motions annotated by $44,970$ textual descriptions.
In addition, it suggests a redundant data representation including a concatenation of root velocity, joint positions, joint velocities, joint rotations and the foot contact binary labels. We also use in this section the same representation for the KIT dataset, brought by the same publishers. 
Although limited in the number ($3,911$) and the diversity of samples, most of the text-to-motion research is based on KIT, hence we view it as important to evaluate using it as well. 

\textbf{Implementation.}
In addition to our Transformer encoder-only backbone (Section~\ref{sec:method}), we experiment \ourmethod{} with three more backbones: (1) \textit{Transformer decoder} injects $z_{tk}$ through the cross-attention layer, instead of as an input token. (2) \textit{Transformer decoder + input token}, where $z_{tk}$ is injected both ways, and (3) \textit{GRU}~\citep{cho2014learning} concatenate $z_{tk}$ to each input frame (Table~\ref{tab:humanml3d}). 
Our models were trained with batch size $64$, $8$ layers (except GRU that was optimal at $2$), and latent dimension $512$. To encode the text we use a frozen \textit{CLIP-ViT-B/32} model.
Each model was trained for $500K$ steps, afterwhich a checkpoint was chosen that minimizes the FID metric to be reported. Since foot contact and joint locations are explicitly represented in HumanML3D, we don't apply geometric losses in this section. We evaluate our models with guidance-scale $s=2.5$ which provides a diversity-fidelity sweet spot (Figure~\ref{fig:user_study_and_sweep}).


\textbf{Quantitative evaluation.} We evaluate and compare our models to current art (JL2P~\citet{ahuja2019language2pose}, Text2Gesture~\citep{bhattacharya2021text2gestures}, and T2M~\citep{guo2022generating}) with the metrics suggested by \citet{guo2022generating}. 
As can be seen, \ourmethod{} achieves state-of-the-art results in \textit{FID}, \textit{Diversity}, and \textit{MultiModality}, indicating high diversity per input text prompt, and high-quality samples, as can also be seen qualitatively in Figure~\ref{fig:teaser}.

\textbf{User study.} We asked 31 users to choose between \ourmethod{} and 
state-of-the-art works
in a side-by-side view, with both samples generated from the same text prompt randomly sampled from the KIT test set. We repeated this process with 10 samples per model and 10 repetitions per sample. 
This user study enabled a comparison with the recent TEMOS model~\citep{petrovich22temos}, which was not included in the HumanML3D benchmark.
Fig.~\ref{fig:user_study_and_sweep} shows that most of the time, \ourmethod{} was preferred over the compared models, and even preferred over ground truth samples in $42.3\%$ of the cases.


\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{


















\begin{tabular}{lccccc}
\toprule

\multirow{2}{2cm}{\centering Method} & 
\multirow{2}{2.cm}{\centering R Precision (top 3)$\uparrow$} & 
\multirow{2}{1.5cm}{\centering FID$\downarrow$} & \multirow{2}{2.5cm}{\centering Multimodal Dist$\downarrow$} & \multirow{2}{2cm}{\centering Diversity$\rightarrow$} & \multirow{2}{2cm}{\centering Multimodality$\uparrow$} \\
\\


\midrule
Real & $0.797^{\pm.002}$ & $0.002^{\pm.000}$ & $2.974^{\pm.008}$ & $9.503^{\pm.065}$ & -\\ 
\midrule

JL2P &  $0.486^{\pm.002}$ & $11.02^{\pm.046}$ & $5.296^{\pm.008}$ & $7.676^{\pm.058}$ & - \\

Text2Gesture &  $0.345^{\pm.002}$ & $7.664^{\pm.030}$ & $6.030^{\pm.008}$ & $6.409^{\pm.071}$ & - \\



T2M & $\mathbf{0.740^{\pm.003}}$ & $1.067^{\pm.002}$ & $\mathbf{3.340^{\pm.008}}$ & $9.188^{\pm.002}$ & $2.090^{\pm.083}$ \\


\midrule


\ourmethod{} (ours) &  ${0.611^{\pm.007}}$ & $\mathbf{0.544^{\pm.044}}$ & ${5.566^{\pm.027}}$ & $\mathbf{9.559^{\pm.086}}$ & ${2.799^{\pm.072}}$ \\

\ourmethod{} (decoder) & ${0.608^{\pm.005}}$ & ${0.767^{\pm.085}}$ & ${5.507^{\pm.020}}$ & ${9.176^{\pm.070}}$ & $\mathbf{2.927^{\pm.125}}$ \\

 \,\,\,\,\, + input token & ${0.621^{\pm.005}}$ & ${0.567^{\pm.051}}$ & ${5.424^{\pm.022}}$ & ${9.425^{\pm.060}}$ & $2.834^{\pm.095}$ \\

\ourmethod{} (GRU) & ${0.645^{\pm.005}}$ & ${4.569^{\pm.150}}$ & ${5.325^{\pm.026}}$ & ${7.688^{\pm.082}}$ & $1.2646^{\pm.024}$ \\

\bottomrule
\end{tabular} }
\caption{\textbf{Quantitative results on the HumanML3D test set.} All methods use the real motion length from the ground truth. `$\rightarrow$' means results are better if the metric is closer to the real distribution. We run all the evaluation 20 times (except \textit{MultiModality} runs 5 times) and $\pm$ indicates the 95\% confidence interval. \textbf{Bold} indicates best result.}
\label{tab:humanml3d}
\end{table}

\begin{table*}[ht]
\centering

\resizebox{\textwidth}{!}{










\begin{tabular}{lccccc}
\toprule

\multirow{2}{2cm}{\centering Method} & 
\multirow{2}{2.cm}{\centering R Precision (top 3)$\uparrow$} & 
\multirow{2}{1.5cm}{\centering FID$\downarrow$} & \multirow{2}{2.5cm}{\centering Multimodal Dist$\downarrow$} & \multirow{2}{2cm}{\centering Diversity$\rightarrow$} & \multirow{2}{2cm}{\centering Multimodality$\uparrow$} \\
\\



\midrule
Real & $0.779^{\pm.006}$ & $0.031^{\pm.004}$ & $2.788^{\pm.012}$ & $11.08^{\pm.097}$ & -\\ 
\midrule

JL2P & $0.483^{\pm.005}$ & $6.545^{\pm.072}$ & $5.147^{\pm.030}$ & $9.073^{\pm.100}$ & - \\

Text2Gesture & $0.338^{\pm.005}$ & $12.12^{\pm.183}$ & $6.964^{\pm.029}$ & $9.334^{\pm.079}$ & - \\



T2M & $\mathbf{0.693^{\pm.007}}$ & ${2.770^{\pm.109}}$ & $\mathbf{3.401^{\pm.008}}$ & $\mathbf{10.91^{\pm.119}}$ & $1.482^{\pm.065}$ \\
\midrule


\ourmethod{} (ours) & ${0.396^{\pm.004}}$ & $\mathbf{0.497^{\pm.021}}$ & ${9.191^{\pm.022}}$ & ${10.847^{\pm.109}}$ & $\mathbf{1.907^{\pm.214}}$\\




\bottomrule
\end{tabular} }
\caption{\textbf{Quantitative results on the KIT test set.} }
\label{tab:kit}
\end{table*}


\begin{figure}\centering
    \subfloat[\centering KIT User Study]{{\includegraphics[width=5cm]{figures/user_study.pdf} }}\qquad
    \subfloat[\centering Classifier-free scale sweep]{{\includegraphics[width=5cm]{figures/scale_sweep.pdf} }}\caption{\textbf{(a) Text-to-motion user study for the KIT dataset}. Each bar represents the preference rate of \ourmethod{} over the compared model. \ourmethod{} was preferred over the other models in most of the time, and $42.3\%$ of the cases even over ground truth samples. The dashed line marks $50\%$. \textbf{(b) Guidance-scale sweep for HumanML3D dataset.} \textit{FID} (lower is better) and \textit{R-precision} (higher is better) metrics as a function of the scale $s$, draws an accuracy-fidelity sweet spot around $s=2.5$.}
\vspace{-10pt}
    \label{fig:user_study_and_sweep}
\end{figure}



\begin{table}[tbh]
\centering


\resizebox{.95\textwidth}{!}{

    \begin{tabular}{lccccc}
    \toprule
    
    \multirow{2}{2cm}{\centering Method} & 
    \multirow{2}{1.5cm}{\centering FID$\downarrow$} &
    \multirow{2}{1.5cm}{\centering Accuracy$\uparrow$} &
    \multirow{2}{2cm}{\centering Diversity$\rightarrow$} &
    \multirow{2}{2cm}{\centering Multimodality$\rightarrow$} \\
    \\
    
    \midrule
    Real (INR) & $0.020^{\pm.010}$ & $0.997^{\pm.001}$ & $6.850^{\pm.050}$ & $2.450^{\pm.040}$\\ 

    Real (ours) & $0.050^{\pm.000}$ & $0.990^{\pm.000}$ & $6.880^{\pm.020}$ & $2.590^{\pm.010}$\\   

    
    \midrule
    
    Action2Motion~\citeyearpar{guo2020action2motion} & $0.338^{\pm.015}$ & $0.917^{\pm.003}$ & $6.879^{\pm.066}$ & $\underline{2.511^{\pm.023}}$\\ 
    
    ACTOR~\citeyearpar{petrovich21actor} & $0.120^{\pm.000}$ &$0.955^{\pm.008}$ &$\mathbf{6.840^{\pm.030}}$ &$2.530^{\pm.020}$ \\ 

    INR~\citeyearpar{cervantes2022implicit} &  $\underline{0.088^{\pm.004}}$ & $\underline{0.973^{\pm.001}}$ & $6.881^{\pm.048}$ & $2.569^{\pm.040}$ \\ 
    \midrule
    
    

    


    \ourmethod{} (ours) &
        $0.100^{\pm.000}$ & $\mathbf{0.990^{\pm.000}}$ & $\underline{6.860^{\pm.050}}$ & $2.520^{\pm.010}$\\

    \quad\quad w/o foot contact  & 
        $\mathbf{0.080^{\pm.000}}$ & $\mathbf{0.990^{\pm.000}}$ & $6.810^{\pm.010}$ & $\mathbf{2.580^{\pm.010}}$ \\  

    \bottomrule
    \end{tabular}
}
\caption{\textbf{Evaluation of action-to-motion on the HumanAct12 dataset.} Our model leads the board in three out of four metrics. 
Ground-truth evaluation results are slightly different for each of the works, due to implementation differences, such as python package versions. 
It is important to assess the diversity and multimodality of each model using its own ground-truth results, as they are measured by their distance from GT. We show the GT metrics measured by our model and by the leading compared work, INR~\citep{cervantes2022implicit}.
\textbf{Bold} indicates best result, $\underline{\text{underline}}$ indicates second best, $\pm$ indicates 95\% confidence interval, $\rightarrow$ indicates that closer to real is better.}
\label{tab:humanact12}
\end{table}
 
\begin{table}[tbh]
\centering


\resizebox{\textwidth}{!}{

    \begin{tabular}{lccccc}
    \toprule
    
    \multirow{2}{2cm}{\centering Method} & 
    \multirow{2}{1.5cm}{\centering $\text{FID}_{\text{train}}\downarrow$} &
    \multirow{2}{1.5cm}{\centering $\text{FID}_{\text{test}}\downarrow$} &
    \multirow{2}{1.5cm}{\centering Accuracy$\uparrow$} &
    \multirow{2}{2cm}{\centering Diversity$\rightarrow$} &
    \multirow{2}{2cm}{\centering Multimodality$\rightarrow$} \\
    \\
    
    \midrule
    Real & $2.92^{\pm.26}$ & $2.79^{\pm.29}$ & $0.988^{\pm.001}$ & $33.34^{\pm.320}$ & $14.16^{\pm.06}$\\ 
    \midrule
    
    ACTOR~\citeyearpar{petrovich21actor} & $20.49^{\pm2.31}$ & $23.43^{\pm2.20} $ &$0.911^{\pm.003}$ & $31.96^{\pm.33}$ & $14.52^{\pm.09}$ \\ 

    INR~\citeyearpar{cervantes2022implicit} (best variation) &  $\mathbf{9.55^{\pm.06}}$ & $15.00^{\pm.09}$ &$0.941^{\pm.001}$ & $31.59^{\pm.19}$ & $14.68^{\pm.07}$ \\ 
    \midrule
    
    
    


    \ourmethod{} (ours)  & 
        $9.98^{\pm1.33}$ & $\mathbf{12.81^{\pm1.46}}$ & $\underline{0.950^{\pm.000}}$ & $\underline{33.02^{\pm.28}}$ & $\mathbf{14.26^{\pm.12}}$ \\

    \quad\quad w/o foot contact & 
        $\underline{9.69^{\pm.81}}$ & $\underline{13.08^{\pm2.32}}$ & $\mathbf{0.960^{\pm.000}}$ & $\mathbf{33.10^{\pm.29}}$ & $\mathbf{14.06^{\pm.05}}$\\
    

    \bottomrule
    \end{tabular}

} \caption{\textbf{Evaluation of action-to-motion on the UESTC dataset.} The performance improvement with our 
model shows a clear gap from state-of-the-art. 
\textbf{Bold} indicates best result, $\underline{\text{underline}}$ indicates second best, $\pm$ indicates 95\% confidence interval, $\rightarrow$ indicates that closer to real is better.}
\label{tab:uestc}
\end{table}
\vspace{-10pt}
 
\subsection{Action-to-Motion}
\vspace{-5pt}

\label{sec:a2m}
Action-to-motion is the task of generating motion given an input action class, represented by a scalar. The output motion should faithfully animate the input action, and at the same time be natural and reflect the distribution of the dataset on which the model is trained. Two dataset are commonly used to evaluate action-to-motion models: HumanAct12~\citep{guo2020action2motion} and UESTC~\citep{ji2018large}. We evaluate our model using the set of metrics suggested by \citet{guo2020action2motion}, namely Fr\'{e}chet Inception Distance (FID), action recognition accuracy, diversity and multimodality. The combination of these metrics makes a good measure of the realism and diversity of generated motions.

\textbf{Data.}
HumanAct12~\citep{guo2020action2motion} offers approximately 1200 motion clips, organized into 12 action categories, with 47 to 218 samples per label. 
UESTC~\citep{ji2018large} consists of 40 action classes, 40 subjects and 25K samples, and is split to train and test. 
We adhere to the cross-subject testing protocol used by current works, with 225-345 samples per action class.
For both datasets we use the sequences provided by \citet{petrovich21actor}.

\textbf{Implementation.}
The implementation presented in Figure~\ref{fig:overview} holds for all the variations of our work. In the case of action-to-motion, the only change would be the substitution of the text embedding by an action embedding. Since action is represented by a scalar, its embedding is fairly simple; each input action class scalar is converted into a learned embedding of the transformer dimension.

The experiments have been run with batch size $64$, a latent dimension of $512$, and an encoder-transformer architecture. Training on HumanAct12 and UESTC has been carried out for $750K$ and $2M$ steps respectively. In our tables we display the evaluation of the checkpoint that minimizes the FID metric. 

\textbf{Quantitative evaluation.}
Tables \ref{tab:humanact12} and \ref{tab:uestc} reflect \ourmethod{}'s performance on the HumanAct12 and UESTC datasets respectively. We conduct 20 evaluations, with 1000 samples in each, and report their average and a 95\% confidence interval.
We test two variations, with and without foot contact loss.
Our model leads the board for both datasets. 
The variation with no foot contact loss attains slightly better results; nevertheless, as shown in our supplementary video, the contribution of foot contact loss to the quality of results is important, and without it we witness artifacts such as shakiness and unnatural gestures. 



\vspace{-5pt}
\section{Additional Applications}
\vspace{-5pt}
\subsection{Motion Editing} \label{sec:edit}
\vspace{-5pt}

In this section we implement two motion editing applications - \textbf{in-betweening} and \textbf{body part editing}, both using the same approach in the temporal and spatial domains correspondingly. 
For \textbf{in-betweening}, we fix the first and last $25\%$ of the motion, leaving the model to generate the remaining $50\%$ in the middle. For \textbf{body part editing}, we fix the joints we don't want to edit and leave the model to generate the rest. In particular, we experiment with editing the upper body joints only.
In figure~\ref{fig:edit} we show that in both cases, using the method described in Section~\ref{sec:method} generates smooth motions that adhere both to the fixed part of the motion and the condition (if one was given).




\begin{table}[tbh]
\centering


\small

    \begin{tabular}{lcccccc}
    \toprule
    
    \multirow{2}{2cm}{\centering Method} & 
    \multirow{2}{1.5cm}{\centering FID$\downarrow$} &
    \multirow{2}{2cm}{\centering KID$\downarrow$} &
    \multirow{2}{2cm}{\centering Precision$\uparrow$ Recall$\uparrow$} &
    \multirow{2}{2cm}{\centering Multimodality$\uparrow$} \\
    \\

    \midrule

    ACTOR~\citeyearpar{petrovich21actor} & 48.80 & 0.53 & 0.72,   0.74 & 14.10 \\ 

    MoDi~\citeyearpar{raab2022modi} & \textbf{13.03} & \textbf{0.12} & \textbf{0.71},   \textbf{0.81} & \textbf{17.57}\\ 
    
    \midrule

    \ourmethod{} (ours)  & 31.92 & 0.36 & 0.66, 0.62 & 17.00\\  

    \bottomrule
    \end{tabular}
\caption{\textbf{Evaluation of unconstrained synthesis on the HumanAct12 dataset.} We test \ourmethod{} in the challenging unconstrained setting, and compare with MoDi~\citep{raab2022modi}, a work that was specially designed for such setting. We demonstrate that in addition to being able to support any condition, we can achieve plausible results in the unconstrained setting.
\textbf{Bold} indicates best result.}
\label{tab:unconstrained}
\end{table}
 \vspace{-5pt}
\subsection{Unconstrained Synthesis} \label{sec:uncond}
\vspace{-5pt}
The challenging task of unconstrained synthesis has been studied by only a few~\citep{holden2016deep, raab2022modi}. 
In the presence of data labeling, e.g., action classes or text description, the labels work as a supervising factor, and facilitate a structured latent space for the training network. 
The lack of labeling make training more difficult.
The human motion field possesses rich unlabeled datasets~\citep{mixamo}, and the ability to train on top of them is an advantage.
Daring to test \ourmethod{} in the challenging unconstrained setting, we follow MoDi\citep{raab2022modi} for evaluation. We use the metrics they suggest (FID, KID, precision/recall and multimodality), and run on an unconstrained version of the HumanAct12~\citep{guo2020action2motion} dataset.

\textbf{Data.}
Although annotated, we use HumanAct12 (see Section \ref{sec:a2m}) in an unconstrained fashion, ignoring its labels. The choice of HumanAct12 rather than a dataset with no labels (e.g., Mixamo~\citep{mixamo}), is for compatibility with previous publications. 

\textbf{Implementation.}
Our model uses the same architecture for all forms of conditioning, as well as for the unconstrained setting.
The only change to the structure shown in Figure~\ref{fig:overview}, is the removal of the conditional input, such that $z_{tk}$ is composed of the projection of $t$ only.
To simulate an unconstrained behavior, ACTOR~\citet{petrovich21actor} has been trained by \citep{raab2022modi} with a labeling of one class to all motions.


\textbf{Quantitative evaluation.}
The results of our evaluation are shown in table \ref{tab:unconstrained}.
We demonstrate superiority over works that were not designed for an unconstrained setting, and get closer to MoDi~\citep{raab2022modi}. MoDi is carefully molded for unconstrained settings, while our work can be applied to any (or no) constrain, and also provides editing capabilities.


