
\documentclass{article} \usepackage[table]{xcolor}
\usepackage{iclr2022_conference,times}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage[colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue]{hyperref}
    
    


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{url}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{color, colortbl}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{booktabs} 
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{bm}

\usepackage{tikz}
\usepackage{siunitx}
\usepackage{scalerel}
\usetikzlibrary{shapes.geometric,shapes.symbols}
\newcommand{\tikzcircle}[2][red,fill=red]{\tikz[baseline=-0.5ex]\draw[#1,radius=#1] (0,0) circle ;}\newcommand{\tikzsymbol}[2][circle]{\tikz[baseline=-0.5ex]\node[innersep=1pt,shape=#1,draw,#2]{};}

\definecolor{myblue}{HTML}{0070C0}

\usepackage[marginal]{footmisc}











\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{multirow}

\newcommand{\x}{\mathbf{x}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\dkl}{\mathbb{D}_{\rm{KL}}}

\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bpsi}{\boldsymbol{\psi}}

\newcommand{\cs}[1]{\textcolor{magenta}{[\textbf{CS}: #1]}}

\newcommand{\A}[1]{\textcolor{blue}{[\textbf{A}: #1]}}

\newcommand{\tikzdiamond}[1][red,fill=red]{\scalerel*{\tikz \draw[rounded corners=0.6pt,#1] (-5pt,0)--++(45:5pt)--++(-45:5pt)--++(-180+45:5pt)--cycle;}{\diamond}}


\title{Learning to Generalize across Domains \\ on Single Test Samples}




\author{
Zehao Xiao\textsuperscript{1},
Xiantong Zhen\textsuperscript{1,2},
Ling Shao\textsuperscript{3},
Cees G. M. Snoek\textsuperscript{1} \\
\textsuperscript{1}AIM Lab, University of Amsterdam \textsuperscript{2}Inception Institute of Artificial Intelligence \\ \textsuperscript{3}National Center for Artificial Intelligence, Saudi Data and Artificial Intelligence Authority}

\iclrfinalcopy
\begin{document}

\maketitle

\begin{abstract} 
We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. 
We propose learning to generalize across domains on single test samples. 
We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time. 
We formulate the adaptation to the single test sample as a variational Bayesian inference problem, which incorporates the test sample as a conditional into the generation of model parameters. 
The adaptation to each test sample requires only one feed-forward computation at test time without any fine-tuning or self-supervised training on additional data from the unseen domains. 
Extensive ablation studies demonstrate that our model learns the ability to adapt models to each single sample by mimicking domain shifts during training. 
Further, our model achieves at least comparable -- and often better -- performance than state-of-the-art methods on multiple benchmarks for domain generalization
\footnote{Code available: \url{https://github.com/zzzx1224/SingleSampleGeneralization-ICLR2022}}.
\end{abstract}

\section{Introduction}



Despite their widespread adoption and success in academia and industry alike, deep convolutional neural networks suffer from a fundamental flaw: they have insufficient generalizability to test data that is out of their training distributions \citep{recht2019imagenet}.
Improving the generalization of machine learning methods therefore remains a challenging problem  \citep{moreno2012unifying,recht2019imagenet,krueger2021out}.
To deal with the distribution shift, domain adaptation \citep{saenko2010adapting,long2015learning,lu2020stochastic,li2021learning} and domain generalization  \citep{muandet2013domain,li2017deeper,li2020domain} have been extensively investigated.
In domain adaptation, the key assumption is that the target data, either labeled or unlabeled, is accessible during \textit{training}, which allows the model to be adapted to the target domains. 
However, this assumption does not necessarily hold in realistic scenarios where the target domain data is not available.
By contrast, domain generalization strives to learn a model on source domains that can generalize well to unseen target domains without any access to the target domain data during training. We aim for domain generalization.


Previous domain generalization approaches have successfully explored domain-invariant learning \citep{muandet2013domain,chattopadhyay2020learning} or domain augmentation \citep{shankar2018generalizing,zhou2020learning} to handle the domain shift between the source and target domains. 
However, since those models are trained on source domains, there will always be an ``adaptivity gap'' when applying them to target domains without further adaptation~\citep{dubey2021adaptive}.
Thus, it is necessary to train models that are able to further adapt to target domains without ever using target data during training.


Recently, further adapting a pre-trained model using target domain data by fine-tuning or self-supervised training at test time has shown effectiveness in improving performance on domain generalization \citep{sun2020test,wang2021tent}.
To achieve proper adaptation to the target domain, these methods typically rely on 
extra fine-tuning operations on target data or extra networks \citep{dubey2021adaptive} at test time.
Although some of these methods can achieve adaptation with a single test sample \citep{d2019learning,sun2020test,banerjee2021self}, batches of target data are required for good performance.
This prevents these methods from 
effectively adapting
to the target domain when provided with very few target samples or exposed to test samples from multiple target domains without domain identifiers.
Different from these works, we aim to learn the ability to adapt to a single sample such that the model can further adapt to each test sample from any target domain.

\begin{wrapfigure}{r}{0.6\textwidth}
\vspace{-4mm}
\centering 
\centerline{\includegraphics[width=0.59\textwidth]{Illustration5.png}} 
\vspace{-1mm}
\caption{
\textbf{Illustration of single sample generalization.}
Different colors denote different classes. Circles denote the classifier prototypes for different categories. 
With our adaptation, the source features and the single target feature together define new classifier prototypes, which are more adapted to the target sample. 
}
\label{illustrate}
\vspace{-4mm}
\end{wrapfigure} 
In this paper, we propose \textit{learning to generalize on single test samples} for domain generalization. 
Since a single sample from the target distribution cannot inform much about the whole distribution, we consider each target sample a domain by itself. Correspondingly, we propose that each target sample adjusts the trained model in its own way. 
This avoids the difficulty of using the limited information available to adapt the model to the entire target domain distribution.
An illustration of the method is shown in Figure~\ref{illustrate}.
Using meta-learning \citep{li2017deeper}, we train our model to acquire the ability to adapt with each single sample on source domains such that it can then adapt to each individual sample in the target domain to handle the domain shift at test time. 
Unlike previous methods, our model does not need to be fine-tuned using target data or train extra networks when generalized to the unseen target domains. 
Once it is trained on source domains, our model is able to adapt to each single target sample with just a feed-forward computation at test time, without the need for any extra target data. 
Note that not a single target sample is seen during training, which does not forgo the strict source and target separation setting of domain generalization. 
Moreover, in contrast to previous test-time adaptation methods \citep{sun2020test,wang2021tent,dubey2021adaptive}, our method does not need to 
re-adapt the model to
each target domain.
Our model effortlessly handles test samples from multiple domains without 
knowing their domain identifiers. 

\begin{table}[t]
\small
\vspace{-4mm}
\centering
\caption{
\textbf{Training and test-time settings of learning methods for generalization across domains.} $(X_s, Y_s)$ and $X_{t}$ denote the labeled source data and unlabeled target data. $X$ indicates batches of samples, while $\x$ denotes just one sample. 
Our method does not need any target data during training, like (one-shot) domain adaptation methods, or fine-tuning operations during inference. It just uses single test samples to do per-sample adaptation for domain generalization. 
}
\vspace{-2mm}
\begin{adjustbox}{max width =\textwidth}
\begin{tabular}{llllccc}
\toprule
& & \textbf{Training} & \multicolumn{4}{c}{\textbf{Test-time}} \\
\cmidrule(lr){3-3} 
\cmidrule(lr){4-7}
\textbf{Task} & \textbf{Method} & Data & Data & Fine-tune & Extra model & Adaptive   \\ \midrule
Domain & Common, e.g.,~\citep{long2015learning} & $X_s, Y_s, X_{t}$  & $\x_{t}$ & $\times$ & $\times$ & $\checkmark$\\
Adaptation & One-shot, e.g.,~\citep{luo2020adversarial} & $X_s, Y_s, \x_{t}$ & $\x_{t}$ & $\times$ & $\times$ & $\checkmark$ \\
\midrule
 & Common, e.g.,~\citep{seo2020learning} & $X_s, Y_s$ & $\x_{t}$ & $\times$ & $\times$ & $\times$\\
Domain & Test-time training \citep{sun2020test} & $X_s, Y_s$ & $\x_{t}$ & $\checkmark$ & $\checkmark$ & $\checkmark$\\
Generalization & Test-time adaptation \citep{wang2021tent} & $X_s, Y_s$ & $X_{t}, \x_{t}$ & $\checkmark$ & $\times$ & $\checkmark$\\
 & Domain-adaptive method \citep{dubey2021adaptive} & $X_s, Y_s$ & $X_{t}, \x_{t}$ & $\times$ & $\checkmark$ & $\checkmark$ \\
& \textit{\textbf{Single sample generalization}} & $X_s, Y_s$ & $\x_{t}$ & $\times$ & $\times$ & $\checkmark$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-2mm}
\label{settings}
\vspace{-4mm}
\end{table}

To be specific, we build our model under the meta-learning framework and formulate the single sample generalization as a variational inference problem.
In the training stage, we divide the source domains into several meta-source domains and a meta-target domain and explore the adaptive model by incorporating the information of the meta-target sample into the generation of the model parameters. 
For any given meta-target sample, we propose a variational distribution generated by this sample and the meta-source data to approximate the model distribution obtained by the meta-target data. 
In the test phase, the adapted models for the samples from the real target domains are generated on the fly by the variational model distribution. 
The random splits of the meta-source and meta-target domains expose the model to domain shifts and mimic the real generalization process from source domains to the target domain. 
Thus, the model is endowed with the ability to adapt to any unseen sample by end-to-end training with the source data only.
By doing so, our method does not need to introduce any extra target data or fine-tuning operations on the target domain. 
A comparison of the training/test-time settings among different domain generalization (and domain adaptation) methods is summarized in Table~\ref{settings}.

We demonstrate the effectiveness of our single sample generalization by conducting experiments on commonly-used benchmarks for domain generalization. 
Our ablation studies under different domain generalization settings demonstrate the advantages of our method in comparison to other recent adaptive methods.

\vspace{-2mm}
\section{Related Work}
\vspace{-1mm}

\textbf{Domain adaptation} To handle the domain shift between source and target domains, considerable efforts have been devoted to developing methods for domain adaptation \citep{long2015learning,lu2020stochastic,hoffman2018cycada,kumar2010co,tzeng2017adversarial,luo2019taking}.
However, the assumption that target data is accessible during training is often invalid. 
Recently, some even more challenging settings were proposed in domain adaptation, e.g., few-shot  \citep{motiian2017few} and one-shot domain adaptation \citep{dong2018domain,luo2020adversarial}, but there are still a few or one target sample available during training.
In this work, we focus on the more strict setting, domain generalization, that assumes not a single target sample is accessible during training. We construct our model under a meta-learning framework to learn the ability of adaptation to single samples across source domains. 
At test time, no more learning is needed and we generate an adapted model for each individual target sample by only a forward pass, without any extra fine-tuning operation.

\textbf{Domain generalization} The problem of domain generalization was introduced by \cite{blanchard2011generalizing}, and formally posed by~\cite{muandet2013domain}.  \cite{muandet2013domain} and  \cite{ghifary2016scatter} both considered domain-invariant learning by minimizing the dissimilarity of features across source domains. Following these works, many methods explored domain invariant learning for domain generalization \citep{motiian2017unified,rojas2018invariant,li2018domainb,seo2020learning,zhao2020domain}. 
Alternatively, augmenting the source domain data has been explored to simulate more domain shift during training \citep{shankar2018generalizing,volpi2018generalizing,zhou2020learning,qiao2020learning,zhou2021mix}. 
Our method tackles domain shift by learning to adapt to each unseen target sample, which is orthogonal to domain-invariant and domain augmentation methods.

\textbf{Domain meta-learning} Meta-learning methods have also been studied for domain generalization \citep{balaji2018metareg,du2020learning,xiao2021bit, du2020metanorm}. \cite{li2018metalearning} introduced the model agnostic meta-learning \citep{finn2017model} into domain generalization. \cite{dou2019domain} further enhanced the method with global and local regularization. 
We follow the experimental settings of previous methods \citep{du2020learning,xiao2021bit} that split source domains into meta-source and meta-target domains. By mimicking the adaptation process at training time, our model learns to learn the adaptable classifiers from meta-source domains and the meta-target sample.


\textbf{Test-time adaptation} Recently, several methods have proposed to adapt a model trained on source data to unseen target data at test time \citep{d2019learning,sun2020test,wang2021tent,dubey2021adaptive}.  \cite{sun2020test} trained their model on source domains with both supervised and self-supervised objective functions, and then fine-tuned it on target data with only the self-supervised one.
\cite{wang2021tent} further improved this method by removing the self-supervised module, while fine-tuning the model with an entropy minimization loss.
Differently,  \cite{dubey2021adaptive} trained a domain embedding network with unsupervised source data, 
which is utilized to generate the target domain embedding by hundreds of unlabeled target samples
to adapt the model at test time, without fine-tuning or further training. 
These adaptive methods all need extra steps, e.g., fine-tuning the model or training embedding networks, and \textit{batches} of target samples for good adaptation. 
In contrast, our method learns to adapt to each target sample using just its own information without any extra target samples and fine-tuning operations.
The adaptation is achieved by generating the model for each target sample with only one forward pass.
Another recent work utilized test-time augmentation to improve robustness at test time  \citep{chai2021ensembling}. 
While it is of interest to adapt a test-time sample to source domains with such test-time augmentations, our method aims to adapt the model trained by source domains to each target sample.


\vspace{-3mm}
\section{Methodology}
\label{method}
\vspace{-2mm}


In domain generalization, several domains are defined as different data distributions in the joint space $\mathcal{X} \times \mathcal{Y}$, where $\mathcal{X}$ and $\mathcal{Y}$ denote the input space and the label space. 
The domains are divided into non-overlapping source domains, collectively denoted as $\mathcal{S}$ and the target domains $\mathcal{T}$. 
During training, only data from source domains is accessible. Data from target domains is never seen. Our goal is to learn a model on the source domains only that generalizes well to the target domains. As we focus on homogeneous domain generalization \citep{zhou2021domain} in this work, all domains share the same label space. 

Since the target data is inaccessible during training, we design a meta-learning framework with episodic training \citep{li2019episodic,balaji2018metareg} that endows the model with the capability of learning to generalize on single test samples from any unseen domain.
To this end, according to the precise domain annotations (if available) or image clusters, we divide the source domain(s) $\mathcal{S}$ into several meta-source domains $\mathcal{S'}$  and a meta-target domain $\mathcal{T'}$ in each iteration to expose the model to domain shifts. After training, the learned model is applied to the target domain $\mathcal{T}$ for evaluation.
In this case, the model is exposed to different domain shifts to learn the ability to handle it.
After training, the learned model is applied to the target domain $\mathcal{T}$ for evaluation.








\textbf{Variational learning} We develop our model under the probabilistic framework. Assume we have a single sample $\x_{t'}$ from the meta-target domain $\mathcal{T'}$, and we would like to predict its class label $\y_{t'}$. We consider the conditional predictive log-likelihood $\log p(\y_{t'}|\x_{t'},\mathcal{T'})$. By incorporating the model $\btheta_{t'}$ into the prediction distribution, we have
\begin{equation}
\label{eq1}
    \log p(\y_{t'}|\x_{t'}, \mathcal{T'}) = \log \int p(\y_{t'}|\x_{t'}, \btheta_{t'})p(\btheta_{t'}|\mathcal{T'})d\btheta_{t'},
\end{equation}
where we condition the prediction on  $\mathcal{T'}$ to leverage information from the meta-target data and $(\x_{t'}, \y_{t'})$ denotes the input-output pair of the meta-target sample. We refer to $p(\btheta_{t'}|\mathcal{T'})$ as the meta-prior distribution since it is conditioned on the meta-target data. 

However, in practice, the labeled data from the target domain is inaccessible, leading to an intractable distribution $p(\btheta_{t}|\mathcal{T})$ during inference.
To address the problem, we propose to approximate the model function $p(\btheta_{t}|\mathcal{T})$ with the accessible source data by the variational inference approach, which provides a framework to approximate intractable distributions via optimization.
By training under the meta-learning framework, our method will learn the ability to approximate the model functions of unseen target domains by source domain data.


\textbf{Variational domain generalization} 
We introduce  a variational distribution $q(\btheta_{t'}|\mathcal{S'})$ to approximate the true posterior over the model parameters $\btheta$. By incorporating $q(\btheta_{t'}|\mathcal{S'})$ into eq.~(\ref{eq1}), we derive a lower bound of the conditional predictive log-likelihood:
\begin{equation}
\begin{aligned}
\label{baseline}
    \log p(\y_{t'}|\x_{t'}, \mathcal{T'}) & = \log \int p(\y_{t'}|\x_{t'}, \btheta_{t'})p(\btheta_{t'}|\mathcal{T'})d\btheta_{t'} \\
    & \geq \mathbb{E}_{q(\btheta_{t'})}[\log p(\y_{t'}|\x_{t'}, \btheta_{t'})] - \dkl [q(\btheta_{t'}|\mathcal{S'}) || p(\btheta_{t'}|\mathcal{T'})].
\end{aligned}
\end{equation}
The introduced Kullback-Leibler (KL) divergence term is minimized to reduce the distance between $q(\btheta_{t'}|\mathcal{S'})$ and $p(\btheta_{t'}|\mathcal{T'})$, which encourages the model $\btheta$ inferred by $q(\btheta_{t'}|\mathcal{S'})$ from meta-source domains to be generalizable to the meta-target domain.


Although eq.~(\ref{baseline}) reduces the difference between the variational distributions $q(\btheta_{t'}|\mathcal{S'})$ and $p(\btheta_{t'}|\mathcal{T'})$, 
$q(\btheta_{t'}|\mathcal{S'})$ still has limited capacity of handling the meta-target samples due to ``adaptivity gap'' \citep{dubey2021adaptive}.
Since only little information about the meta-target domain $\mathcal{T'}$ and the meta-target sample $\x_{t'}$ is available in $\mathcal{S'}$, there is no guarantee that the variational distribution $q(\btheta_{t'}|\mathcal{S'})$ will be adapted to the meta-target domain $\mathcal{T'}$. 
To reduce the ``adaptivity gap'', the model needs to acquire the ability to effectively use the target information.
Thus, we further propose to learn to adapt the model $\btheta_{t'}$ by taking more information of the meta-target data into account.

\textbf{Single sample generalization} In domain generalization, especially for real applications, when given data from an unseen target domain, the only accessible information of the target data is from the target sample, without any annotations.
We therefore propose to utilize the information contained in the given target sample to construct the adapted model. 
Instead of utilizing $q(\btheta_{t'}|\mathcal{S'})$ as the variational distribution,
we incorporate the meta-target sample $\x_{t'}$ into the variational distribution $q(\btheta_{t'}|\x_{t'}, \mathcal{S'})$ to approximate the true posterior distribution. Given an unlabeled sample $\x_{t'}$ from the meta-target domain $\mathcal{T'}$, we have a new evidence lower bound (ELBO) as follows:
\begin{equation}
\begin{aligned}
    \log p(\y_{t'}|\x_{t'}, \mathcal{T'}) & = \log \int p(\y_{t'}|\x_{t'}, \btheta_{t'})p(\btheta_{t'}|\mathcal{T'})d\btheta_{t'} \\
    & \geq \mathbb{E}_{q(\btheta_{t'})}[\log p(\y_{t'}|\x_{t'}, \btheta_{t'})] - \dkl [q(\btheta_{t'}|\x_{t'}, \mathcal{S'}) || p(\btheta_{t'}|\mathcal{T'})].
\end{aligned}
\label{elbo}
\end{equation}


By conditioning on the given sample $\x_{t'}$ and the source data $\mathcal{S'}$, the adapted variational distribution $q(\btheta_{t'}|\x_{t'}, \mathcal{S'})$ contains both the categorical information provided by the meta-source data and the domain information given by the meta-target sample. 
This to a large extent guarantees the inferred model to be discriminative and adapted to the given meta-target domain sample. 
Moreover, the KL divergence in eq.~(\ref{elbo}) further acts as a regularizer to guide the variational distribution to be similar to the parameter distribution $p(\btheta_{t'}|\mathcal{T'})$ obtained by the meta-target data, which is assumed to be the most suitable parameter for the meta-target sample $\x_{t'}$. This makes the model better adapted to $\x_{t'}$




Based on the meta-learning framework and eq.~(\ref{elbo}), our model learns the ability to adapt the meta-source model to each meta-target instance across different domain shifts.
The learned ability is exploited to handle domain shifts at test time by adapting the source model to each target instance.
During inference, our method directly uses the variational distributions $q(\btheta_t|\x_t, \mathcal{S})$ as the adapted model generated by the information of the given target sample $\x_t$ and the data from all source domains $\mathcal{S}$. Thus, the method is able to generalize to each unseen target sample without the requirement of any extra data from the target domains or fine-tuning operations. The derivations of eq.~(\ref{baseline}) and eq.~(\ref{elbo}), as well as more discussions on their tightness are provided in Appendix~\ref{app1}. 



\begin{wrapfigure}{r}{0.6\textwidth}
\vspace{-4.5mm}
\centering 
\centerline{
\includegraphics[width=0.58\textwidth]{architecture4.png}
} 
\vspace{-4mm}
\caption{\textbf{Architecture of single sample generalization.} $\mathcal{S'}$ and $\mathcal{T'}$ denote  meta-source and meta-target. $\x_{t'}$ and $\y_{t'}$ denote the input image and prediction of the single test sample. 
}
\label{model}
\vspace{-6mm}
\end{wrapfigure}

\vspace{-0.5mm}
\textbf{Learning to adapt classifiers}
The variational inference framework in eq.~(\ref{elbo}) indicates that we can learn to adapt all model parameters $\btheta$ to the target sample with its own information. 
This could, however, be computationally infeasible due to the large number of parameters in the network.
Thus, for computational efficiency and feasibility we utilize the features from the feature extractor as the sample-specific information to construct the adapted classifier, which contains much fewer parameters.


To do so, we divide the model $\btheta$ into a feature extractor $\bphi$ and a classifier $\w$. The feature extractor is shared across domains, while the classifier is trained to be adapted to each single sample by eq.~(\ref{elbo}). 
Both the meta-prior distribution $p(\w_{t'}|\mathcal{T'})$ and the variational posterior distribution $q(\w_{t'}|\x_{t'}, \mathcal{S'})$ of the classifier $\w_{t'}$ are generated by amortized inference using the amortization technique~\citep{kingma2013auto, gershman2014amortized,gordon2018meta, shen2021variational}.
The amortization technique provides a natural way to generate model parameters by feature representations, which is one appropriate solution for our method to incorporate the target feature into the construction of the adapted variational posterior distribution. Moreover, the amortization networks can be trained and evaluated together with the model, without introducing extra training steps and fine-tuning operations to achieve the adaptation ability.
Specifically, we use the center features of samples in each class from the meta-target domain and meta-source domains as the representatives of $\mathcal{T'}$ and $\mathcal{S'}$. 
We generate the meta-prior distribution $p(\w_{t'}|\mathcal{T'})$ by taking the center features of $\mathcal{T'}$ as input to the amortized inference network $\btheta_{a}$. For the variational distribution $q(\w_{t'}|\x_{t'}, \mathcal{S'})$, the input to the amortized inference network is the concatenation of the center features of the meta-source domain $\mathcal{S'}$ with the features of the target sample $\x_{t'}$ for adaptation. 
The amortized inference network $\btheta_{a}$ is shared for the inference of both the meta-prior and posterior distributions.

However, the direct concatenation operation is not necessarily optimal as the input for the amortized inference network. 
We propose to generate the parameter distribution using the meta-source domains and then adapt it with the given meta-target sample.
To do so, we first generate a classifier distribution $p(\w_{s'}|\mathcal{S'})$ using the center features of the meta-source data $\mathcal{S'}$.
After that, we take $p(\w_{s'}|\mathcal{S'})$ as a prior and combine it with the meta-target sample to estimate the variational posterior, as follows:
\begin{equation}
\label{hierar}
    q(\w_{t'}|\x_{t'}, \mathcal{S'}) = \int{p(\w_{t'}|\x_{t'}, \w_{s'})} p(\w_{s'}|\mathcal{S'}) d\w_{s'}.
\end{equation}
This establishes a hierarchical variational inference of the posterior distribution. We show in our experiments that the hierarchical inference yields better results than the direct concatenation.
In practice, we approximate the integral by:
\begin{equation}
q(\w_{t'}|\x_{t'}, \mathcal{S'}) = \sum^{L}_{\ell=1}p(\w_{t'}|\x_{t'}, \w^{(\ell)}_{s'}),
    \label{qws}
\end{equation}
where $\w^{(\ell)}_{s'} \sim p(\w_{s'}|\mathcal{S'})$ and $L$ is the number of Monte Carlo samples.

The negative KL divergence term in eq.~(\ref{elbo}) tries to close the gap between the variational adaptive distribution $q(\w_{t'}|\x_{t'}, \mathcal{S'})$ and the meta-prior distribution $p(\w_{t'}|\mathcal{T'})$, aiming to make the variational distribution adapt better to the target data. However, there is no guarantee that the classifier produced by the meta-prior is discriminative enough for classification.
To address this issue, we introduce an intermediate supervision based on the meta-prior distribution to make it as predictive as possible, by maximizing: 
\begin{equation}
\label{prior}
    \mathbb{E}_{p(\w_{t'}|\mathcal{T'})}\big[\log p(\y_{t'}|\x_{t'}, \w_{t'})\big].
\end{equation}

This is also implemented as a cross-entropy loss using Monte Carlo sampling from the meta-prior.
An illustration of our architecture is provided in Figure~\ref{model}. We also provide an algorithm in Appendix~\ref{algorithm}.


\begin{figure*}[t] 
\centering 
\centerline{\includegraphics[width=0.9\textwidth]{singlesample3.png}} 
\vspace{-2mm}
\caption{\textbf{Illustration of single sample generalization using meta-learning.} Through variational Bayesian inference, we incorporate the single sample as a conditional during the generation of model parameters. The dashed line indicates training only.  } 
\label{adapt}
\vspace{-2mm}
\end{figure*} 

\vspace{-2mm}
\paragraph{Empirical objective function} 
By incorporating eq. (\ref{qws}) and eq. (\ref{prior}) into eq. (\ref{elbo}), we obtain the final empirical objective function:
\begin{equation}
\label{finalloss}
    \mathcal{L} {=} \sum^M_{m=1} \log p(\y_{t'}|\x_{t'},\w^{(m)}_{t'}) + \sum^N_{n=1} \log p(\y_{t'}|\x_{t'},\w^{(n)}_{t'}) - \mathbb{D}_{\rm{KL}}(\sum^{L}_{\ell=1}p(\w_{t'}|\x_{t'}, \w^{(\ell)}_{s'})||p(\w_{t'}|\mathcal{T'}))
\end{equation}
where we use Monte Carlo sampling: $\w^{(m)}_{t'} \sim \sum^{L}_{\ell=1}q(\w_{t'}|\x_{t'}, \w^{(\ell)}_{s'})$ and $\w^{(n)}_{t'} \sim p(\w_{t'}|\mathcal{T'})$.

As illustrated in Figure~\ref{adapt}, at training time (a), we train the model episodically 
on meta-source domains $\mathcal{S}'$ and a meta-target domain $\mathcal{T}'$. At inference time (b), given a single sample $\mathbf{x}_t$ from the target domain $\mathcal{T}$, we use this sample to adapt the $\w_{s}$ generated by the source data. 
The adaptation is achieved by generating the adapted classifiers $\w_{t}$ for each target sample with only one forward pass using the learned amortization inference network $\btheta_{a}$.


\vspace{-1mm}
\section{Experiments}
\vspace{-1mm}

We conduct our experiments on four widely used benchmarks for domain generalization: \textbf{PACS} \citep{li2017deeper}, \textbf{Office-Home} \citep{venkateswara2017deep}, \textbf{rotated MNIST}, and \textbf{Fashion-MNIST} \citep{piratla2020efficient}.
The details of the datasets and our implementations are provided in Appendix~\ref{appimp}.











\textbf{Benefit of single sample generalization}
We first investigate the effectiveness of our single sample generalization on the PACS dataset.
To demonstrate the benefits of all components of our method, we consider four settings: 
(i) the ``variational amortized classifier'' with eq.~(\ref{baseline}) as the objective function, (ii) our single sample generalization method without hierarchical architecture (eq.~\ref{qws}), (iii) single sample generalization without supervision on the meta-prior distribution (eq.~\ref{prior}), and (iv) the entire single sample generalization method, with the objective function in eq.~(\ref{finalloss}). 
Results based on the ResNet-18 backbone are shown in Table~\ref{ablations}.



By comparing rows 1 and 2 of Table~\ref{ablations}, it is clear that our single sample generalization brings an improvement in accuracy over the invariant amortized classifier, especially on the ``art-painting'' and ``cartoon'' domains. 
These results demonstrate the benefits of introducing the information of the given target sample into the model generation. The comparison also demonstrates that our model learns the capability to adapt to the target sample using its own information.
Comparing the last two rows shows the benefits of the intermediate supervision in eq.~(\ref{prior}), which provides a more discriminative meta-prior distribution during training to learn more adapted variational distribution, especially on the ``art-painting'', ``cartoon'' and ``sketch'' domains.
Results in rows 2 and 4 of Table~\ref{ablations} show that the hierarchical architecture for the variational adaptive classifier $q(\w_{t'})$ provides a more adapted model for each target sample and achieves the best performance on all domains.




\begin{table}[t]
\begin{minipage}[t]{\linewidth}
\vspace{-1mm}
\centering
\caption{\textbf{Benefits of single sample generalization.} Ablation study on PACS using ResNet-18 averaged over five runs. Best runs within a 95\% confidence margin bolded. Single sample generalization is more effective than the invariant amortized classifier. The hierarchical architecture further improves the performance. Intermediate supervision on the prior distribution is also important. 
}
\vspace{-2mm}
\centering
\label{ablations}
	\resizebox{0.99\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{llllll}
\toprule
Settings  & Photo    & Art-painting         & Cartoon        & Sketch         & \textit{Mean}       \\ \midrule
Invariant amortized classifier (eq.~\ref{baseline})    & 95.19 \scriptsize{$\pm$0.30} & 79.34 \scriptsize{$\pm$0.65} & 78.04 \scriptsize{$\pm$0.85} & 76.21 \scriptsize{$\pm$0.54} & 82.19 \scriptsize{$\pm$0.35} \\
Single sample generalization (w/o eq.~\ref{qws})
& \textbf{95.69} \scriptsize{$\pm$0.20} & 81.06 \scriptsize{$\pm$0.33} & \textbf{79.20} \scriptsize{$\pm$0.49} & 77.10 \scriptsize{$\pm$0.64} & 83.26 \scriptsize{$\pm$0.31} \\
Single sample generalization (w/o eq.~\ref{prior})
& \textbf{95.47} \scriptsize{$\pm$0.20} & 80.00 \scriptsize{$\pm$0.61} & 77.87 \scriptsize{$\pm$0.91} & 77.20 \scriptsize{$\pm$0.88} & 82.64 \scriptsize{$\pm$0.32} \\
Single sample generalization (eq.~\ref{finalloss})
& \textbf{95.87} \scriptsize{$\pm$0.24}         & \textbf{82.02} \scriptsize{$\pm$0.36}  & \textbf{79.73} \scriptsize{$\pm$0.49} & \textbf{78.96} \scriptsize{$\pm$0.67} & \textbf{84.15} \scriptsize{$\pm$ 0.21} \\ 
\bottomrule
\end{tabular}
}
\end{minipage}
\begin{minipage}[t]{\linewidth}
\vspace{1mm}
\centering
\caption{
\textbf{Importance of appropriate domain shift during training} on PACS using ResNet-18 averaged over five runs. The more appropriate the domain shift encountered during training, the better our method will perform during inference.
}
\vspace{-2mm}
\centering
\label{domainsplit}
	\resizebox{0.88\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{llllll}
\toprule
Division of source domains  & Photo    & Art-painting         & Cartoon        & Sketch         & \textit{Mean}       \\ \midrule
Split images randomly
& \textbf{95.75} \scriptsize{$\pm$0.24} & 78.30 \scriptsize{$\pm$0.33} & 78.52 \scriptsize{$\pm$0.81} & 75.69 \scriptsize{$\pm$0.64} & 82.07 \scriptsize{$\pm$0.11} \\
Split by image clusters
& \textbf{95.67} \scriptsize{$\pm$0.39} & 80.05 \scriptsize{$\pm$0.56} & \textbf{79.39} \scriptsize{$\pm$0.60} & 77.12 \scriptsize{$\pm$1.00} & 83.06 \scriptsize{$\pm$0.39} \\
Split by domain annotations
& \textbf{95.87} \scriptsize{$\pm$0.24}         & \textbf{82.02} \scriptsize{$\pm$0.36}  & \textbf{79.73} \scriptsize{$\pm$0.49} & \textbf{78.96} \scriptsize{$\pm$0.67} & \textbf{84.15} \scriptsize{$\pm$0.21} \\ 
\bottomrule
\end{tabular}
}
\end{minipage}
\begin{minipage}[t]{\linewidth}
\vspace{1mm}
\centering
\caption{
\textbf{Ablation of meta-learning setting.} The experiments are conducted on PACS using ResNet-18 averaged over five runs. Under the meta-learning framework (second row), the method performs better on all domains.
}
\centering
\label{metaablation}
	\resizebox{0.8\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{llllll}
\toprule
Settings  & Photo    & Art-painting         & Cartoon        & Sketch         & \textit{Mean}       \\ \midrule
w/o Meta-learning
&94.61 \scriptsize{$\pm$0.65} & 79.97 \scriptsize{$\pm$0.43} & 78.45 \scriptsize{$\pm$0.29} & 75.83 \scriptsize{$\pm$0.79} & 82.21 \scriptsize{$\pm$0.36} \\
Meta-learning
& \textbf{95.87} \scriptsize{$\pm$0.24}         & \textbf{82.02} \scriptsize{$\pm$0.36}  & \textbf{79.73} \scriptsize{$\pm$0.49} & \textbf{78.96} \scriptsize{$\pm$0.67} & \textbf{84.15} \scriptsize{$\pm$0.21} \\ 
\bottomrule
\end{tabular}
}
\vspace{-6mm}
\end{minipage}
\end{table}


\textbf{Importance of appropriate domain shift during training}
To show the importance of mimicking an appropriate domain shift during training, we conduct experiments with different divisions of source domains during training. 
As shown in Table~\ref{domainsplit}, when we randomly split images into source domains (row 1), the domain shift between them is too small to mimic the domain shift between source and target domains. Thus, the performance are not so good, especially on ``art-painting'' and ``sketch''.
When we split the source domains with image clusters (row 2) and precise domain annotations (row 3), the performance is improved,
demonstrating the importance of constructing appropriate domain shift during training.
By better mimicking domain shifts during training, our method achieves better performance.
Importantly, the experiments also show that our method can improve the performance without the need to have multiple domains and annotations by a simple unsupervised domain split method, e.g., clustering. 
The conclusion is also demonstrated in the single-source domain experiments that are provided in Table~\ref{ssdg}, Appendix~\ref{extraex}.




\textbf{Ablation of meta-learning setting}
To show the importance of the meta-learning framework, we also implement a non-meta-learning version of our method.
We directly generate the adapted classifiers by features from all source domains and the feature representations of the single sample. 
The results are shown in Table~\ref{metaablation}. Without the meta-learning framework, the model has difficulty to learn the ability to adapt to unseen domains and tends to overfit to the source domains. Thus the performance on all domains is worse than the meta-learning-based model.

\textbf{Influence of backbone}
To show the influence of backbone models on the single sample generalization, we further compare our proposal with a baseline empirical risk minimization (ERM) method on PACS by varying the backbone model size. 
As shown in Table~\ref{diffbackbone}, our method achieves more obvious performance gains with larger backbone models compared to the ERM baseline. 
We explain this by our use of an amortized inference network that generates the classifier by the source domain features and the target sample feature. Implying an increase in backbone capacity has a direct effect on our classifier capacity, enabling better adaptation to the target sample.

\begin{table}[t]
\centering
\caption{
\textbf{Influence of backbone}. The experiments are conducted on PACS over five runs. Only the average accuracy of four domains is shown in the Table. With larger and larger backbone models, our method achieves better and better performance gaps compared to the ERM baseline.
}
\vspace{-2mm}
\centering
\label{diffbackbone}
    \resizebox{0.8\columnwidth}{!}{\setlength\tabcolsep{4pt} 
    \begin{tabular}{lllll}
    \toprule
        Method & AlexNet & ResNet-18 & ResNet-34  & ResNet-50 \\
        \midrule
        ERM baseline & 71.23 \scriptsize{$\pm$0.38} & 81.45 \scriptsize{$\pm$0.21} & 82.58 \scriptsize{$\pm$0.36} & 84.09 \scriptsize{$\pm$0.13} \\
        \textbf{\textit{Single sample generalization}} & 72.50 \scriptsize{$\pm$0.28} & 84.15 \scriptsize{$\pm$0.21} & 85.91 \scriptsize{$\pm$0.38} & 87.51 \scriptsize{$\pm$0.22} \\ \bottomrule
    \end{tabular}
    }
\vspace{-2mm}
\end{table}

\begin{figure*}[t] 
\centering 
\centerline{\includegraphics[width=.9\textwidth]{figure2s4.pdf}} 
\vspace{-3mm}
\caption{\textbf{Visualization of single sample generalization} on the ``cartoon'' domain from PACS. 
Different shapes denote different categories. Samples are in red, classifiers in blue. 
Single sample generalization generates adapted classifiers for different target samples, leading to better classification. 
} 
\label{visual}
\vspace{-6mm}
\end{figure*} 


\textbf{Visualization of single sample generalization}
To illustrate the benefit of incorporating the test sample into classifier generation, we visualize the the target features and the classifiers of both the ``variational amortized classifier'' (baseline) and our single sample generalization method in Figure~\ref{visual}. 
We treat the vectors of different categories in the classifier as prototypes with the same dimension as the test features and map the classifier vectors and features into the same 2D space. 
As shown in the left figure (baseline), the classifier parameters of the ``invariant amortized classifier'' are trained on source domains and fixed for all target samples. 
In contrast, as shown in the three figures on the right, our method generates specific, adapted classifiers for each test target sample. For instance, the test sample ($\Large \tikzdiamond[red, fill=red]$) has the shortest distance to the adapted classifier of its own class ($\Large \tikzdiamond[color=myblue, fill=myblue]$), producing the correct predictions. More visualizations, including failure cases, are provided in Appendix~\ref{appvis}.

\begin{figure*}[t] 
\centering 
\centerline{\includegraphics[width=0.9\textwidth]{figure3-3.pdf}} 
\vspace{-2mm}
\caption{\textbf{Single sample generalization vs. Tent} \citep{wang2021tent} for different settings on rotated-MNIST.
Tent shows good performance with a large batch of samples from a single domain (left).
However, when provided only one sample or given samples from different domains without the domain id (right), Tent suffers.
More detailed comparisons are provided in Appendix~\ref{apptent}. 
} 
\label{tent}
\vspace{-4mm}
\end{figure*} 



\textbf{Single sample generalization vs. Tent\footnote{All Tent results are obtained by running the author-provided code accompanying  \citep{wang2021tent}.}}
Next we conduct an experiment on rotated-MNIST to compare our method with Tent by \cite{wang2021tent}, which is a recent test-time adaptation method that fine-tunes the model with batches of target samples. 
We follow the setting in \citep{piratla2020efficient} and generalize seven domains by rotating the samples by $0^\circ$ to  $90^\circ$ in intervals of $15^\circ$. 
To make the target domains more different from each other, we use images rotated by $0^\circ$, $45^\circ$ and $90^\circ$ as target domains,  $15^\circ$, $30^\circ$, $60^\circ$ and  $75^\circ$ as source domains.
For fair comparisons, we train the base model of Tent using data from all source domains, which is the same amount of train and validation data as in our method.
Then, the trained base model is adapted by the target samples as in the original paper of Tent. The other settings are the same as our methods.
As shown in Figure~\ref{tent}, we design two settings for Tent for the comparisons: ``single target domain'' (left) and ``multiple target domains'' (right). In the ``single target domain'' setting, we adapt the base model to each of the target domains separately using Tent and take the average accuracy of different domains. For ``multiple target domains'', Tent is required to adapt to all three target domains jointly without the domain identifiers. 
Since our method is able to adapt to each sample, the performance is consistent under both settings.

As expected, Tent achieves good adaptation for large batch sizes (e.g., 128) from a single target domain. However, when there are only very few samples, and ultimately just one sample, the adaptation of Tent starts to suffer, and may even hurt accuracy. Tent, like other test-time adaptive domain generalization methods, relies on a batch of samples to adapt to the target domain. In other words, the test samples have known domain identifiers. 
However, in practical settings, the test samples might come from different domains and the domain identifiers may be inaccessible. Thus, we also explore the effectiveness under the ``multiple target domains'' setting, as shown in Figure~\ref{tent} (right). In this scenario, the adaptability of test-time adaptation methods like Tent, starts to decline, even with more samples. In contrast, our method learns the capability to adapt to each sample from any target domain, overcoming the lack of target samples while being robust under multiple target domains.
We also compare our method with Tent on PACS and Office-Home. The results are shown in Table~\ref{pacsoff}. We perform Tent with a baseline model trained on all source domain data, which is then fine-tuned by 128 target samples for 100 iterations. 
The conclusion is the same as on rotated MNIST. Our method achieves better performance. 
One reason can be that our model adapts the model to each individual target sample rather than the entire target domain. Thus, different adaptations are conducted on different target samples. Alternatives are sensitive to the chosen batch of samples, which is not necessarily generalizable to the entire domain, especially when the batch is small. Another reason is our choice for meta-learning. By mimicking the adaptation process under the meta-learning framework, the model learns the ability to adapt to a single target sample. Without meta-learning, as shown in Table~\ref{metaablation}, the performance drops.





\begin{table}[t]
\begin{center}
\caption{\textbf{Comparison on PACS and Office-Home.} Our method achieves best mean accuracy with a ResNet-50 backbone and is competitive with ResNet-18. 
Notably, it surpasses the adaptive methods by \cite{wang2021tent} and \cite{dubey2021adaptive}, despite them using more data at test-time (Table \ref{settings}). 
}
\label{pacsoff}
\vspace{-2mm}
	\resizebox{0.8\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{lllll}
\toprule
~ & \multicolumn{2}{c}{\textbf{PACS}} & \multicolumn{2}{c}{\textbf{Office-Home}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} 
 Method  & ResNet-18 & ResNet-50 & ResNet-18 & ResNet-50\\ \midrule
\cite{carlucci2019domain} & 80.51 & -  & 61.20 & - \\
\cite{dou2019domain} & 81.04 & 82.67 & - & - \\
\cite{zhao2020domain} & 81.46 & 85.34 & - & - \\
\cite{zhou2020learning}   & 82.83 & 84.90 & 65.63 & 67.66 \\
\cite{seo2020learning} & \textbf{85.11} & 86.64 & 62.90 & - \\
\cite{gulrajani2020search}  & - & 85.50 & - & 66.50 \\
\cite{dubey2021adaptive}  & - & 84.50 & - & 68.90 \\
\cite{wang2021tent}{$^\dagger$}  & 83.09 \scriptsize{$\pm$0.13} & 86.23 \scriptsize{$\pm$0.22} & 64.13 \scriptsize{$\pm$0.16} & 67.99 \scriptsize{$\pm$0.22} 
\\
\textit{\textbf{Single sample generalization}}  & 84.15 \scriptsize{$\pm$0.21} & \textbf{87.51} \scriptsize{$\pm$0.22} & \textbf{66.02} \scriptsize{$\pm$0.28} & \textbf{71.07} \scriptsize{$\pm$0.31}\\ 
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-6mm}
\end{table}





\textbf{Comparisons with state of the art} Finally, we compare our method with the state-of-the-art on four datasets. The results on PACS and Office-Home are reported in Table \ref{pacsoff}. Our method achieves good performance with both the ResNet-18 and ResNet-50 backbones. Compared with others, our method shows larger advantages with ResNet-50 than ResNet-18 on both PACS and Office-Home.
This is reasonable since we generate the adaptive model using the features of source and single target samples, which contain more information with ResNet-50, leading to more adapted models.
This is also demonstrated in Table~\ref{diffbackbone}.
Results per domain, as well as the results on rotated-MNIST and Fashion-MNIST are provided in Appendix~\ref{sec:detailcompare}, with similar results and conclusions.









\vspace{-2mm}
\section{Conclusion}
\vspace{-1mm}
We propose learning to generalize by single test samples for domain generalization.
Our model is able to adapt to any unseen target sample by leveraging the information of the given sample to generate the model.
We formulate the adaptation process as a variational inference problem and train the model under the meta-learning framework.
After training on the source data, our method can adapt to each sample from the unseen target domains without any fine-tuning operations or extra target data. Ablation studies and further experiments on several benchmarks show the effectiveness of our method.


\section*{Acknowledgment}
This work is financially supported by the Inception Institute of Artificial Intelligence, the University of Amsterdam and the allowance 
Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy.




\section*{Ethics Statement}


With the capability to handle the domain shift without the accessibility of target data during training and fine-tuning operations, our method has potential positive impacts in the applications that often face unseen domains in practice, e.g., medical imaging and automatic driving.
Accordingly, our method would also potentially face some negative social impacts accompanying with applications, e.g., lack of fairness with the model trained by incomplete data and the privacy of patients in medical imaging.


\section*{Reproducibility Statement}
To reproduce our method, the details of the method and loss functions are provided in Section~\ref{method}. 
We also provide the derivations and discussions about the evidence lower bound used in our method in Appendix~\ref{app1}.
The datasets used in this paper and the detailed implementations of the method are reported in Appendix~\ref{appimp}. An illustration of the architecture is shown in Figure~\ref{model}.
We also provide a link to the source code of our method in the Abstract.

\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}



\newpage
\appendix

\section{Derivations}
\label{app1}

\paragraph{Derivation of variational domain generalization} 
In domain generalization, since the target domain data is inaccessible, the distribution $p(\btheta_{t}|\mathcal{T})$ is intractable.
We leverage the meta-learning setting to mimic the domain generalization process. In particular, we episodically divide the source domains into meta-source domains $\mathcal{S'}$ and a meta-target domain $\mathcal{T'}$ and learn the ability to generalize to the target data.
Thus, during training, we generate the parameter distribution by the meta-source data $\mathcal{S'}$ to approximate the distribution $p(\btheta_{t'}|\mathcal{T'})$ generated by the meta-target data.
Then at inference, the model is able to approximate the distribution $p(\btheta_{t}|\mathcal{T})$ with the parameters generated by the source data $\mathcal{S}$.
To do so, we introduce  a variational distribution $q(\btheta_{t'}|\mathcal{S'})$ to approximate the true posterior over the model parameter $\btheta$. The full derivation of eq.~(\ref{baseline}) in the main paper is as follows:
\begin{equation}
\begin{aligned}
\label{supbase}
    \log p(\y_{t'}|\x_{t'}, \mathcal{T'}) & = \log \int p(\y_{t'}|\x_{t'}, \btheta_{t'})p(\btheta_{t'}|\mathcal{T'})d\btheta_{t'} \\
    & = \log \int p(\y_{t'}|\x_{t'}, \btheta_{t'}) \frac{p(\btheta_{t'}|\mathcal{T'})}{q(\btheta_{t'}|\mathcal{S'})} q(\btheta_{t'}|\mathcal{S'}) d\btheta_{t'}
    \\
    & = \log \Big[ \mathbb{E}_{q(\btheta_{t'}|\mathcal{S'})} \big[ p(\y_{t'}|\x_{t'}, \btheta_{t'}) \frac{p(\btheta_{t'}|\mathcal{T'})}{q(\btheta_{t'}|\mathcal{S'})} \big] \Big]\\
    & \geq \mathbb{E}_{q(\btheta_{t'}|\mathcal{S'})} \Big[ \log \big[ p(\y_{t'}|\x_{t'}, \btheta_{t'}) \frac{p(\btheta_{t'}|\mathcal{T'})}{q(\btheta_{t'}|\mathcal{S'})} \big] \Big] \\
    & = \mathbb{E}_{q(\btheta_{t'}|\mathcal{S'})}[\log p(\y_{t'}|\x_{t'}, \btheta_{t'})] - \dkl [q(\btheta_{t'}|\mathcal{S'}) || p(\btheta_{t'}|\mathcal{T'})].
\end{aligned}
\end{equation}

\paragraph{Derivation of single sample generalization} Since there is little information available about the target domain or target data in the variational distribution $q(\btheta_{t'}|\mathcal{S'})$ in eq.~(\ref{baseline}), the resultant model is not adaptive to the target domain data. To generate a model adapted to the target sample, we introduce information of the target data, and incorporate the meta-target sample $\x_{t'}$ into the variational distribution $q(\btheta_{t'}|\x_{t'}, \mathcal{S'})$. The detailed derivation is:
\begin{equation}
\begin{aligned}
    \log p(\y_{t'}|\x_{t'}, \mathcal{T'}) & = \log \int p(\y_{t'}|\x_{t'}, \btheta_{t'})p(\btheta_{t'}|\mathcal{T'})d\btheta_{t'} \\
    & = \log \int p(\y_{t'}|\x_{t'}, \btheta_{t'}) \frac{p(\btheta_{t'}|\mathcal{T'})}{q(\btheta_{t'}|\x_{t'}, \mathcal{S'})} q(\btheta_{t'}|\x_{t'}, \mathcal{S'}) d\btheta_{t'}
    \\
    & = \log \Big[ \mathbb{E}_{q(\btheta_{t'}|\x_{t'}, \mathcal{S'})} \big[ p(\y_{t'}|\x_{t'}, \btheta_{t'}) \frac{p(\btheta_{t'}|\mathcal{T'})}{q(\btheta_{t'}|\x_{t'}, \mathcal{S'})} \big] \Big] \\
    & \geq \mathbb{E}_{q(\btheta_{t'}|\x_{t'}, \mathcal{S'})} \Big[ \log \big[ p(\y_{t'}|\x_{t'}, \btheta_{t'}) \frac{p(\btheta_{t'}|\mathcal{T'})}{q(\btheta_{t'}|\x_{t'}, \mathcal{S'})} \big] \Big] \\
    & = \mathbb{E}_{q(\btheta_{t'}|\x_{t'}, \mathcal{S'}))}[\log p(\y_{t'}|\x_{t'}, \btheta_{t'})] - \dkl [q(\btheta_{t'}|\x_{t'}, \mathcal{S'}) || p(\btheta_{t'}|\mathcal{T'})].
\end{aligned}
\label{supelbo}
\end{equation}

\paragraph{Comparison of the two evidence lower bounds}
The main difference between eq.~(\ref{supbase}) and eq.~(\ref{supelbo}) is the variational posterior $q(\btheta_{t'})$.
Compared with eq.~(\ref{supbase}), our method (eq.~\ref{supelbo}) introduces the target sample $\x_{t'}$ into the generation of the variational posterior, which therefore achieves more adaptive classifiers for the target sample.
Moreover, the tightness of the bound is also related to the variational posterior.
The approximation gap between our bound (eq.~\ref{elbo}) and the objective log-likelihood corresponds to the KL divergence between the variational posterior $q(\btheta_{t'}|\x_{t'}, \mathcal{S'})$ and the true posterior $p(\btheta_{t'}|\x_{t'},\y_{t'}, \mathcal{T'})$, as shown in the following equation: 
\begin{equation}
\begin{aligned}
    log p(\y_{t'}|\x_{t'}, \mathcal{T'}) & = \underbrace{ \mathbb{E}_{q(\btheta_{t'})} [\log p(\y_{t'}|\x_{t'}, \btheta_{t'})] - \mathbb{D}_{\rm{KL}}[q(\btheta_{t'}|x_{t'}, \mathcal{S'}) || p(\btheta_{t'}|\mathcal{T'})]}_{ELBO} \\
    & + \underbrace{\mathbb{D}_{\rm{KL}} [q(\btheta_{t'}|\x_{t'}, \mathcal{S'}) || p(\btheta_{t'}|\x_{t'},\y_{t'}, \mathcal{T'})]}_{Approximation~Gap}.
\end{aligned}
\end{equation}
Compared with eq.~(\ref{baseline}), whose approximation gap is formulated as $\mathbb{D}_{\rm{KL}} [q(\btheta_{t'}|\mathcal{S'}) || p(\btheta_{t'}|\x_{t'},\y_{t'}, \mathcal{T'})]$, our method takes the given sample $\x_{t'}$ as one of the conditions, leading to a tighter lower bound.


\section{Algorithm of Single Sample Generalization}
\label{algorithm}

We describe the the detailed training and test algorithm of our single sample generalization method in Algorithm~\ref{alg:1}
\begin{algorithm}[h]
\small
\caption{Single-Test-Sample Generalization}
\label{alg:1}
\begin{algorithmic}
\STATE \underline{TRAINING TIME}
\STATE {\textbf{Require:}} Source domains $\mathcal{S}=\left \{ D_{s} \right \}^{S}_{s=1}$ each with $n$ sample pairs ${(\x_s, \y_s})$
\STATE {\textbf{Require:}} Learning rate $\lambda$; the number of iterations $N_{iter}$.
\STATE Initialize $\boldsymbol{\Theta} = \{\bphi, \btheta_{a}, \bpsi \}$. \textcolor{gray}{$\bphi$: ImageNet-pretrained feature extractor; 
$\bpsi$: multiple layer module.}
\FOR{\textit{iter} in $N_{iter}$}
\STATE $\mathcal{T'}$ $\leftarrow$ Randomly Sample($\left \{ D_{s} \right \}^{S}_{s=1}$, $t'$); 
\\
$\mathcal{S'}$ $\leftarrow$ $\left \{ D_{s} \right \}^{S}_{s=1}$ $\backslash$ $\mathcal{T'}$;
\STATE Sample datapoints $\{(\mathbf{x}_{t'}^{(k)}, \mathbf{y}_{t'}^{(k)})\} \sim \mathcal{T'}$.
\STATE $\mu_{\w^{(m)}_{t'}}, \sigma_{\w^{(m)}_{t'}} = f_{\btheta_{a}}(\mathcal{T'})$; $\w^{(m)}_{t'} \sim \mathcal{N}(\mu_{\w^{(m)}_{t'}}, \sigma_{\w^{(m)}_{t'}})$. \textcolor{gray}{// Compute meta-prior $p(\w_{t'}|\mathcal{T'})$ as eq.(\ref{prior})}
\STATE $\mu_{\w_{s'}}, \sigma_{\w_{s'}} = f_{\bpsi}(\mathcal{S'})$; $\w_{s'} \sim \mathcal{N}(\mu_{\w_{s'}}, \sigma_{\w_{s'}})$. \textcolor{gray}{// Compute $p(\w_{s'}|\mathcal{S'})$ in eq.(\ref{hierar})}
\STATE $\mu_{\w^{(n)}_{t'}}, \sigma_{\w^{(n)}_{t'}} = f_{\btheta_{a}}(\w_{s'}, \bphi(\x^{(k)}_{t'}))$; $\w^{(n)}_{t'} \sim \mathcal{N}(\mu_{\w^{(n)}_{t'}}, \sigma_{\w^{(n)}_{t'}})$. \textcolor{gray}{// Compute $q(\w_{t'}|\x^{(k)}_{t'}, \mathcal{S'})$ as eq.(\ref{qws})}
\STATE $\mathcal{L}=\sum\limits_{k}$ \Big($\sum\limits_{m=1}^{M} $ CrossEntropy $((\w^{(m)}_{t'}\cdot \bphi(\x^{(k)}_{t'})), \mathbf{y}_{t'}^{(k)})$ + $\sum\limits_{n=1}^{N} $ CrossEntropy $((\w^{(n)}_{t'}\cdot\x^{(k)}_{t'}), \mathbf{y}_{t'}^{(k)})$ 
\\ \qquad + $\mathbb{D}_{\rm{KL}} [q(\w_{t'}|\x^{(k)}_{t'}, \mathcal{S'})||p(\w_{t'}|\mathcal{T'})]$ \Big). \textcolor{gray}{// Compute loss as eq.(\ref{finalloss})}
\STATE Update parameters: $\boldsymbol{\Theta} \leftarrow \boldsymbol{\Theta} -\lambda \nabla_{\Theta} \mathcal{L}$. \textcolor{gray}{// Gradient descent step}
\ENDFOR
\STATE {\hrulefill}
\STATE \underline{TEST TIME}
\STATE {\textbf{Require}}: Source domains $\mathcal{S}=\left \{ D_{s} \right \}^{S}_{s=1}$; target samples $\left \{ \x_{t} \right \}^{T}_{t=1}$ from the target domain $\mathcal{T}$; trained model with weight $\boldsymbol{\Theta} = \{\bphi, \btheta_{a}, \bpsi\}$.
\STATE $\mu_{\w_{s}}, \sigma_{\w_{s}} = f_{\bpsi}(\mathcal{S})$; $\w_{s} \sim \mathcal{N}(\mu_{\w_{s}}, \sigma_{\w_{s}})$. \textcolor{gray}{// Compute $p(\w_{s}|\mathcal{S})$}
\STATE $\mu_{\w^{(n)}_{t}}, \sigma_{\w^{(n)}_{t}} = f_{\btheta_{a}}(\w_{s}, \bphi(\x_{t}))$; $\w^{(n)}_{t} \sim \mathcal{N}(\mu_{\w^{(n)}_{t}}, \sigma_{\w^{(n)}_{t}})$. \textcolor{gray}{// Compute $q(\w_{t}|\x^{(k)}_{t}, \mathcal{S})$}
\STATE \textbf{return} $\y_{t} = \w^{(n)}_{t} \cdot \x_{t}$.
\end{algorithmic}
\end{algorithm}

\section{Datasets and Implementation Details}
\label{appimp}

\paragraph{Datasets}
\textbf{PACS} \citep{li2017deeper} is a widely used dataset consisting of 9,991 images of seven classes from four domains, i.e., \textit{photo}, \textit{art-painting}, \textit{cartoon} and \textit{sketch}. We use the same training and validation split as in \citep{li2017deeper} and follow the ``leave-one-out'' protocol from~\citep{li2017deeper,li2019episodic,carlucci2019domain}. 
\textbf{Office-Home} \citep{venkateswara2017deep} has 15,500 images of 65 categories. The images are also from four domains, i.e., \textit{art}, \textit{clipart}, \textit{product} and \textit{real-world}. We use the same experimental protocol as for PACS. 
\textbf{Rotated MNIST and Fashion-MNIST} are utilized in \cite{piratla2020efficient}.
We follow their recommended settings and randomly select a subset of 2,000 images from MNIST and 10,000 images from Fashion-MNIST. The subset of images is rotated by $0^\circ$ through $90^\circ$ in intervals of $15^\circ$, creating seven domains. We use the subsets with rotation angles from $15^\circ$ to $75^\circ$ as the source domains, and images rotated by $0^\circ$ and $90^\circ$ as the target domains. 
For comparison, we evaluate our method on both in-distribution and out-of-distribution data.

\vspace{-2mm}
\paragraph{Implementation details}
We evaluate our method on PACS with both ResNet-18 and ResNet-50 \citep{he2016deep} pretrained on ImageNet \citep{deng2009imagenet} as the backbone.
During training, we use Adam optimization and train the model for 10,000 iterations. 
The learning rate of the backbone is set to 0.00005 for ResNet-18 and 0.00001 for ResNet-50, while the learning rate of the network for generating the classifier is set to 0.0001 consistently.
The batch size is 128. To generate the classifier, in each iteration we select 10 samples from each category in each meta-source domain for generating the center features $\mathcal{S'}$, and the same number of samples from the meta-target domain for $\mathcal{T'}$. The model with the highest validation accuracy is utilized for evaluation on the target domain.
Most of the experimental settings and hyperparameters on Office-Home are the same as on PACS. Since there are more categories in Office-Home, the number of samples per category per domain is set to 5 to fit the memory footprint.
The learning rate of the backbone is set to 0.00001 for both ResNet-18 and ResNet-50.
For fair comparisons, we evaluate the rotated MNIST and Fashion-MNIST with ResNet-18, following \citep{piratla2020efficient}.
The other experimental settings are also the same as PACS.
We train all models 
on an NVIDIA Tesla V100 GPU. 
The detailed information about other hyperparameters are summarized in Table~\ref{param}.




\begin{table}[t]
\caption{\textbf{Implementation details of our method per dataset and backbone.} ``Number of source samples'' denotes the number of samples per class per source domain for generating the adapted classifier. 
}
\centering
\label{param}
	\resizebox{0.99\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{lcccc}
\toprule
Dataset & Backbone & Classifier learning rate & Backbone learning rate & Number of source samples  \\ \midrule
\multirow{2}*{PACS} & ResNet-18  & 0.0001 & 0.00005 & 10 \\ 
~ & ResNet-50  & 0.0001 & 0.00001 & 10 \\ \midrule
\multirow{2}*{Office-Home} & ResNet-18   & 0.0001 & 0.00001 & 5 \\
~ & ResNet-50   & 0.0001 & 0.00001 & 5 \\ \midrule
Rotated MNIST & ResNet-18  & 0.0001 & 0.00005 & 10 \\ \midrule
Fashion-MNIST & ResNet-18  & 0.0001 & 0.00005 & 10 \\
\bottomrule
\end{tabular}
}
\end{table}







\section{Extra Experiments}
\label{extraex}












\subsection{Detailed comparisons with state of the art}
\label{sec:detailcompare}

\begin{table}[t]
\vspace{-1mm}
\caption{\textbf{Comparison on PACS.} Our method achieves best mean accuracy with a ResNet-50 backbone and is competitive with ResNet-18. Notably, it surpasses the adaptive test-time methods by \cite{wang2021tent} and \cite{dubey2021adaptive}, despite them using more data at test-time (see Table \ref{settings}).
$\mathbf{\dagger}$ denotes the reimplemented results in both this table and Table~\ref{office}.
}
\vspace{-2mm}
\centering
\label{pacs}
	\resizebox{0.85\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{lllllll}
\toprule
Backbone & Method  & Photo    & Art-painting         & Cartoon        & Sketch         & \textit{Mean}       \\ \midrule
\multirow{8}*{ResNet-18} & \cite{carlucci2019domain} &  96.03   & 79.42                 & 75.25          & 71.35          & 80.51         \\
~   & \cite{dou2019domain}    &  94.99   & 80.29                 & 77.17          & 71.69          & 81.04         \\
~   & \cite{zhao2020domain}    & \textbf{96.65}          & 80.70          & 76.40          & 71.77          & 81.46         \\
~   & \cite{zhou2020learning}    & 96.20 & 83.30          & 78.20          & 73.60          & 82.83 \\
~   & \cite{wang2021tent}{$^\dagger$}  &  95.49 \scriptsize{$\pm$0.27} & 81.55 \scriptsize{$\pm$0.35} & 77.67 \scriptsize{$\pm$0.39} & 77.64 \scriptsize{$\pm$0.27} & 83.09 \scriptsize{$\pm$0.13}        \\
~   & \cite{zhou2021mix}   & 96.10 & 84.10          & 78.80          & 75.90          & 83.70 \\
~   & \cite{seo2020learning}   & 95.87 & \textbf{84.67}          & 77.65           & \textbf{82.23}          & \textbf{85.11} \\
~   & \textit{\textbf{This paper}}  & 95.87 \scriptsize{$\pm$0.24}         & 82.02 \scriptsize{$\pm$0.36}  & \textbf{79.73} \scriptsize{$\pm$0.49} & 78.96 \scriptsize{$\pm$0.67} & 84.15 \scriptsize{$\pm$ 0.21} \\
\midrule
\multirow{8}*{ResNet-50} & \cite{dou2019domain} & 95.01 & 82.89 & 80.49 & 72.29 & 82.67 \\
~ & \cite{dubey2021adaptive}  & -  & - & - & - & 84.50 \\
~   & \cite{zhou2020learning}{$^\dagger$} & 97.55 &
85.21 &
80.33 &
76.53 &
84.90 \\
~   & \cite{zhao2020domain} & \textbf{98.25} & 87.51 & 79.31 & 76.30 & 85.34 \\
~ & \cite{gulrajani2020search}  & 97.20 & 84.70 & 80.80 & 79.30 & 85.50 \\
~ & \cite{wang2021tent}{$^\dagger$}  & 97.96 \scriptsize{$\pm$0.27} & 86.30 \scriptsize{$\pm$0.26} & 82.53 \scriptsize{$\pm$0.69} & 78.11 \scriptsize{$\pm$0.73} & 86.23 \scriptsize{$\pm$0.22} \\
~   & \cite{seo2020learning} & 95.99 & 87.04 & 80.62 & \textbf{82.90} & 86.64 \\
~   & \textit{\textbf{This paper}}  & 97.88 \scriptsize{$\pm$0.15}         & \textbf{88.09} \scriptsize{$\pm$0.26}  & \textbf{83.83} \scriptsize{$\pm$0.68} & 80.21 \scriptsize{$\pm$0.66}         & \textbf{87.51} \scriptsize{$\pm$0.22} \\ 
\bottomrule
\end{tabular}
}
\vspace{-3mm}
\end{table}
\textbf{PACS} In Table~\ref{pacs}, we conduct experiments with both ResNet-18 and ResNet-50 backbones.
Our method achieves good performance using the ResNet-18 backbone.
For each individual domain, we are competitive with the state of the art and slightly better on the ``cartoon'' domain when using ResNet-18. 
On the ``art-painting'' and ``cartoon'' domains with ResNet-50, our method also achieves good performance. Although it delivers competitive, or sometimes even better, performance on most domains, our method is not as good on the ``sketch'' domain. One reason might be that the images and features from this domain carry less information than other domains, leading to less adaptive models.

\begin{table}[t]
\begin{center}
\caption{\textbf{Comparison on Office-Home.} Our method achieves the best mean accuracy using both a ResNet-18 and ResNet-50 backbone. 
Notably, it surpasses the adaptive test-time methods by \cite{wang2021tent} and \cite{dubey2021adaptive}, despite them using more data at test-time (see Table \ref{settings}). 
}
\label{office}
\vspace{-2mm}
	\resizebox{0.85\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{lllllll}
\toprule
Backbone & Method  & Art    & Clipart         & Product        & Real World         & \textit{Mean}       \\ \midrule
\multirow{6}*{ResNet-18} & \cite{carlucci2019domain} & 53.04 & 47.51 & 71.47 & 72.79  & 61.20 \\
~   & \cite{seo2020learning} & 59.37 & 45.70 & 71.84 & 74.68 & 62.90 \\
~   & \cite{huang2020self} & 58.42 & 47.90 & 71.63 & 74.54 & 63.12
\\
~ & \cite{wang2021tent}{$^\dagger$}  & 56.45 \scriptsize{$\pm$0.30} & 52.06 \scriptsize{$\pm$0.18} & 73.19 \scriptsize{$\pm$0.42} & 74.82 \scriptsize{$\pm$0.27} & 64.13 \scriptsize{$\pm$0.16} 
\\
~   & \cite{zhou2020learning}   & \textbf{60.60} & 50.10 & \textbf{74.80} & \textbf{77.00} & 65.63 \\
~   & \textit{\textbf{This paper}}  & 59.39 \scriptsize{$\pm$0.43}        & \textbf{53.94} \scriptsize{$\pm$0.45}  & \textbf{74.68} \scriptsize{$\pm$0.33} & 76.07 \scriptsize{$\pm$0.17} & \textbf{66.02} \scriptsize{$\pm$0.28} \\ 
\midrule
\multirow{6}*{ResNet-50} & \cite{gulrajani2020search}  & 61.30 & 52.40 & 75.80 & 76.60 & 66.50 \\
& \cite{zhou2020learning}{$^\dagger$}  & 63.62 &
51.48 &
76.57 &
78.95 &
67.66 \\
~ & \cite{wang2021tent}{$^\dagger$}  & 62.12 \scriptsize{$\pm$0.32} & 56.65 \scriptsize{$\pm$0.49} & 75.61 \scriptsize{$\pm$0.57} & 77.58 \scriptsize{$\pm$0.42} & 67.99 \scriptsize{$\pm$0.22} \\
~ & \cite{dubey2021adaptive}  & -  & - & - & - & 68.90 \\
~ & \cite{sun2016deep} & 65.30 & 54.40 & 76.50 & 78.40 & 68.70 \\
~   & \textit{\textbf{This paper}}  & \textbf{67.21} \scriptsize{$\pm$0.59}	 & \textbf{57.97} \scriptsize{$\pm$0.37}	& \textbf{78.61} \scriptsize{$\pm$0.59} & \textbf{80.47} \scriptsize{$\pm$0.16} & \textbf{71.07} \scriptsize{$\pm$0.31}
 \\ 
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

\textbf{Office-Home} As shown in Table~\ref{office}, our method again achieves good overall performance with both the ResNet-18 and ResNet-50 backbones. 
With ResNet-18 as the backbone, we outperform other methods by a good margin on the ``clipart'' domain, while delivering competitive performance on the other domains.
When utilizing ResNet-50 as the backbone, our method achieves good performance on all four domains.

\textbf{Rotated MNIST and Fashion-MNIST.} 
On rotated MNIST and Fashion-MNIST, for fair comparison, we use ResNet-18 as the backbone following \citep{piratla2020efficient} and evaluate the method on both in-distribution and out-of-distribution sets.
Our method achieves best performance on both datasets as shown in Table~\ref{mnist}, especially for the out-of-distribution setting. Moreover, our method demonstrates less performance drop from in-distribution to out-of-distribution data in comparison to other methods. 


\begin{table}[t]
\begin{center}
\caption{\textbf{Comparison on rotated MNIST and Fashion-MNIST.} 
In-distribution performance is evaluated on the test sets of MNIST and Fashion-MNIST with rotation angles of $15^\circ$, $30^\circ$, $45^\circ$, $60^\circ$ and $75^\circ$, while the out-of-distribution performance is evaluated on test sets with angles of $0^\circ$ and $90^\circ$. We achieve the best performance on both the in-distribution and out-of-distribution test sets.}
\label{mnist}
\resizebox{0.9\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{lllll}
\toprule
& \multicolumn{2}{c}{\textbf{MNIST}} & \multicolumn{2}{c}{\textbf{Fashion-MNIST}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} 
 & In-distribution & Out-of-distribution  & In-distribution & Out-of-distribution\\ \midrule
\cite{dou2019domain}      & 98.2 & 93.2  & 86.9   & 72.4 \\
\cite{piratla2020efficient}     & 98.4 & 94.7 & 89.7  & 78.0      \\
\textit{\textbf{This paper}} & \textbf{98.9} \scriptsize{$\pm$0.1}   & \textbf{95.8} \scriptsize{$\pm$0.1}  & \textbf{90.9} \scriptsize{$\pm$0.2}   & \textbf{80.8} \scriptsize{$\pm$0.5}  \\ \bottomrule
\end{tabular}
}
\end{center}
\end{table}


\subsection{Single source domain generalization}
As our method learns the adaptation ability by mimicking the domain shift during training, it requires at least two source domains to support the meta-learning scheme, which is a limitation of the method.


We conduct an experiment that uses SVHN as the single source domain and MNIST as the target. We generate source domains by two simple methods: random image split and clustering of images (K-means). The results in Table~\ref{ssdg} show that our method performs slightly better with larger domain shift simulated in the single source domain. To learn better adaptation ability, auxiliary methods are needed to generate larger domain shifts, for example by domain augmentation techniques like \cite{qiao2020learning}.
\begin{table}[t]
\vspace{-2mm}
\centering
\caption{
Experiments on single source domain generalization. We use SVHN as the source domain and MNIST as the target. Our method performs slightly better with larger domain shift simulated in the single source domain.
}
\vspace{-1mm}
\centering
\label{ssdg}
	\resizebox{0.6\columnwidth}{!}{\setlength\tabcolsep{4pt} 
\begin{tabular}{lc}
\toprule
Method  & SVHN $\rightarrow$ MNIST        \\ \midrule
Baseline
&82.05 \scriptsize{$\pm$0.34}\\
Random split by images
& \textbf{82.45} \scriptsize{$\pm$0.70}\\ 
Split by cluster
& \textbf{83.01} \scriptsize{$\pm$0.36}\\ 
\bottomrule
\end{tabular}
}
\end{table}



\section{Visualizations}
\label{appvis}

\paragraph{More comparisons with baseline}
\begin{figure*}[t] 
\vspace{-3mm}
\centering 
\centerline{\includegraphics[width=\textwidth]{sup1ss.pdf}} 
\caption{\textbf{Visualization of single sample generalization on PACS.} We plot the test samples and classifiers in the same 2D plane.  Different shapes denote different categories. Samples are in red, classifiers in blue. 
The baseline method produces the same classifier for all samples from each domain, while our method generates the classifier adapted to each target sample.
The two methods use the same test samples from each domain. The test sample is shown in the left-top corner of each sub-figure.
The visualization shows why our model performs better.
} 
\label{supvisual}
\vspace{-2mm}
\end{figure*} 
To further demonstrate the effectiveness of our method, we provide more visualizations of the classifier parameters and feature representations from different domains in Figure.~\ref{supvisual}.
The left column shows the visualizations of the “variational amortized classifier” (baseline), and the three columns on the right show our results.
The same conclusion can be drawn here as in the main paper.
Our method is able to generate adapted classifiers for each test sample, providing better predictions than the fixed classifiers in the baseline method.

\vspace{-2mm}
\paragraph{Failure cases}
\begin{figure*}[t] 
\centering 
\centerline{\includegraphics[width=\textwidth]{sup3s.pdf}} 
\caption{\textbf{Failure case  visualizations of our method.} The visualization settings are the same as Figure~\ref{supvisual}. Our method makes wrong predictions, be it that the correct classifiers are also close to the samples, which shows that the classifiers generated by our method are still adapted to the given sample in these cases.
} 
\label{fail}
\vspace{-6mm}
\end{figure*} 
To gain insights into our method, we also provide some failure cases in Figure~\ref{fail}. 
Our method gets confused when samples have objects of different categories in the same image, as shown in the first row.
Although failing in these cases, our model provides correct predictions for the other objects in the image. 
In the visualizations, the classifier with the same category of the labeled object is also close to the feature, which demonstrates the effectiveness of our method.
In the middle two rows, our method struggles with the samples that contain multiple objects or complex backgrounds.
A possible reason is that to take full advantage of the given sample, we utilize multi-level features of the target sample to generate the adapted classifier. 
However, the complex background and multiple objects in the image bring too much noisy information, leading to less adapted classifiers for the given sample.
A solution can be extracting information of the target sample selectively to reduce noise in the features.
Moreover, in the ``sketch'' domain, as shown in the last row, the samples usually have less information than other domains.
This makes our method more sensitive to the given information.
When there is noisy information, e.g., the chair in the left figure, or object with unobvious features, e.g., the elephant in the right figure, the method fails to generate proper classifiers for the sample.


\section{More comparisons with Tent}
\label{apptent}

\begin{figure*}[t] 
\centering 
\centerline{\includegraphics[width=\textwidth]{sup2ss.pdf}} 
\vspace{-2mm}
\caption{\textbf{Single sample generalization vs. Tent for different settings on rotated-MNIST.}
Tent shows good performance with a large batch of samples from a single domain (left).
When provided only one sample or when given samples from different domains, without their domain id (right), Tent suffers.
In contrast, our method is independent to the number of target samples and domains. } 
\label{suptent}
\end{figure*} 

We provide more detailed comparisons with Tent \cite{wang2021tent} in Figure~\ref{suptent} to further show the effectiveness of our method.
The three rows in Figure~\ref{suptent} show the accuracy of different settings on $0^\circ$, $45^\circ$, and $90^\circ$ target domain, respectively. The mean accuracy is shown in Figure~\ref{tent} in the main paper.
The same as in the main paper, the left column shows results under the ``single target domain'' setting, while the right column shows the ``multiple target domains'' setting.
In the ``single target domain'' setting, Tent is utilized to adapt the base model trained on source domains to each target domain independently.
By contrast, in the ``multiple target domains'' setting, the base model is adapted and evaluated on three target domains jointly, without the domain identifiers.
The base model is adapted by Tent using different number of samples, e.g., 1, 32, 128, with different optimization steps, e.g., 1, 10, 100.
The same conclusion can be drawn here as in the main paper.
Compared with Tent, our method adapts the model to each target sample individually by a single and efficient feed-forward pass. 
Thus, our performance is independent to the settings and optimization steps.

Specifically, as shown in the left column, Tent achieves better performance with both larger numbers of samples and fine-tuning steps, especially on $0^\circ$ and $90^\circ$ domains.
However, with fewer target samples being available, and ultimately just one sample, the adaptation of Tent starts to suffer, or even hurt the accuracy with more optimization steps.
When adapted to three target domains jointly, as shown in the right column, the adapted performance is similar with the ``single target domain'' settings with few target samples.
With more target samples, the performance degrades, worse than the left column. 
Accuracy on $0^\circ$ domain improves slightly, while on $45^\circ$ and $90^\circ$ domain even drops as the adaptation is affected by samples from other domains.
In contrast, since our method adapts the model to each target sample individually, we achieve consistently good performance regardless of how many domains the test samples cover. This demonstrates the benefit of our single sample generalization.





\end{document}
