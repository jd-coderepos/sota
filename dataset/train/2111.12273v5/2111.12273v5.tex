\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\caption{Performance comparisons of different methods \jing{with} ResNet-18, ResNet-34 and ResNet-50 on ImageNet. We obtain DoReFa-Net results from~\cite{choi2018pact}. ``W/A'' refers to the bitwidth of weights and activations, respectively. ``FP'' represents the Top-1 accuracy of the full-precision models. ``-'' denotes that the results are not reported. \revise{We do not apply advanced techniques to boost the performance as mentioned in the compared methods in Section~\ref{sec:experiments}.} 
}
\vspace{-0.1in}
\centering
\scalebox{0.72}
{
\begin{tabular}{cccccccc}
\toprule
\multirow{2}{*}{Method} & Bitwidth  & 
\multicolumn{2}{c}{Accuracy (\%)} & Bitwidth  & 
\multicolumn{2}{c}{Accuracy (\%)} \\
& (W/A) & Top-1 & Top-5 & (W/A) & Top-1 & Top-5 \\
\midrule
\multicolumn{7}{c}{ResNet-18 (FP: 70.7)} \\
\cdashline{1-7}
DoReFa-Net$^{*}$~\cite{zhou2016dorefa} & 4/4 & 68.1 & 88.1 & 2/2 & 62.6 & 84.4 \\
PACT$^{*}$~\cite{choi2018pact} & 4/4 & 69.2 & 89.0 & 2/2 & 64.4 & 85.6 \\
LQ-Nets$^{*}$~\cite{zhang2018lq} & 4/4 & 69.3 & 88.8 & 2/2 & 64.9 & 85.9 \\
DSQ~\cite{gong2019differentiable} & 4/4 & 69.6 & - & 2/2 & 65.2 & - \\
BRECQ~\cite{li2021brecq} & 4/4 & 69.6 & - & 2/2 & - & - \\
FAQ~\cite{mckinstry2019discovering} & 4/4 & 69.8 & 89.1 & 2/2 & - & - \\
QIL$^{*}$~\cite{jung2019learning} & 4/4 & 70.1 & - & 2/2 & 65.7 & - \\
LLT$^{*}$~\cite{wang2022learnable} & 4/4 & 70.4 & 89.6 & 2/2 & 66.0 & 86.2 \\
Auxi~\cite{zhuang2020training} & 4/4 & - & - & 2/2 & 66.7 & 87.0 \\
DAQ$^*$~\cite{kim2021distance} & 4/4 & 70.5 & - & 2/2 & 66.9 & - \\
EWGS$^*$~\cite{lee2021network} & 4/4 & 70.6 & -  & 2/2 & 67.0 & - \\
BR~\cite{han2021improving} & 4/4 & 70.8 & 89.6  & 2/2 & 67.2 & 87.3 \\
APOT~\cite{Li2020Additive} & 4/4 & 70.7 & 89.6  & 2/2 & 67.3 & 87.5 \\
LSQ~\cite{Esser2020LEARNED} & 4/4 & 71.1 & \textbf{90.0} & 2/2 & 67.6 & \textbf{87.6} \\
SAQ (Ours) & 4/4 & \textbf{71.6} & \textbf{90.0} & 2/2 & \textbf{67.8} & \textbf{87.6} \\
\midrule
\multicolumn{7}{c}{ResNet-34 (FP: 74.1)} \\
\cdashline{1-7}
LQ-Nets$^{*}$~\cite{zhang2018lq} & 4/4 & - & -  & 2/2 & 69.8 & 89.1 \\
DSQ~\cite{gong2019differentiable} & 4/4 & 72.8 & -  & 2/2 & 70.0 & - \\
FAQ~\cite{mckinstry2019discovering} & 4/4 & 73.3 & 91.3  & 2/2 & - & - \\
QIL$^{*}$~\cite{jung2019learning} & 4/4 & 73.7 & -  & 2/2 & 70.6 & - \\
APOT~\cite{Li2020Additive} & 4/4 & 73.8 & 91.6  & 2/2 & 70.9 & 89.7 \\
DAQ$^*$~\cite{kim2021distance} & 4/4 & 73.7 & -  & 2/2 & 71.0 & - \\
Auxi~\cite{zhuang2020training} & 4/4 & - & - & 2/2 & 71.2 & 89.8 \\
EWGS$^*$~\cite{lee2021network} & 4/4 & 73.9 & -  & 2/2 & 71.4 & - \\
LSQ~\cite{Esser2020LEARNED} & 4/4 & 74.1 & 91.7 & 2/2 & 71.6 & 90.3 \\
SAQ (Ours) & 4/4 & \textbf{75.0}& \textbf{92.3}  & 2/2 & \textbf{71.8} & \textbf{90.4} \\
\midrule
\multicolumn{7}{c}{ResNet-50 (FP: 76.8)} \\
\cdashline{1-7}
DoReFa-Net$^{*}$~\cite{zhou2016dorefa} & 4/4 & 71.4 & 89.8  & 2/2 & 67.1 & 87.3 \\
LQ-Net$^{*}$~\cite{zhang2018lq}  & 4/4 & 75.1 & 92.4  & 2/2 & 71.5 & 90.3\\
FAQ~\cite{mckinstry2019discovering} & 4/4 & 76.3 & 93.0  & 2/2 & - & - \\
PACT$^{*}$~\cite{choi2018pact}  & 4/4 & 76.5 & 93.2  & 2/2 & 72.2 & 90.5 \\
APOT~\cite{Li2020Additive} & 4/4 & 76.6 & 93.1  & 2/2 & 73.4 & 91.4 \\
LSQ~\cite{Esser2020LEARNED} & 4/4 & 76.7 & 93.2  & 2/2 & 73.7 & 91.5 \\
Auxi~\cite{zhuang2020training} & 4/4 & - & -  & 2/2 & 73.8 & 91.4 \\
SAQ (Ours) & 4/4 & \textbf{77.6} & \textbf{93.6} & 2/2 & \textbf{74.5} & \textbf{91.9} \\
\bottomrule
\end{tabular}
}
\begin{tablenotes}
     \item \footnotesize $^{*}$ denotes that the first and last layers are not quantized.
\end{tablenotes}
\label{table:results_on_imagenet_resnet}
\vspace{-0.3in}
\end{table}

\noindent\textbf{Datasets and evaluation metrics.} We evaluate our method on  ImageNet~\cite{deng2009imagenet} which is a large-scale dataset containing 1.28 million training images and 50k validation samples with 1k classes.
We measure the performance of different methods using the Top-1 and Top-5 accuracy. 

\noindent\textbf{Implementation details.}
Our implementations are based on PyTorch~\cite{paszke2019pytorch}. We apply \methodshortname to CNNs and vision Transformers, including ResNet-18~\cite{he2016deep}, ResNet-34, ResNet-50,  MobileNetV2~\cite{sandler2018inverted} and ViT~\cite{dosovitskiy2020image}. 
We first train the full-precision models and use them to initialize the low-precision ones. Following LSQ~\cite{Esser2020LEARNED}, we quantize both weights and activations for all matrix multiplication layers, including convolutional layers, fully-connected layers, and self-attention layers. For the first and last layers, we quantize both weights and activations to 8-bit to preserve the performance. \revise{We do not apply quantization to the input images since they have been quantized to 8-bit during image preprocessing.} 

For CNNs, we use the \jing{uniform} quantization method mentioned in Section~\ref{sec:quantization}. Relying on SGD with the momentum term of 0.9, we apply \methodshortname with Case 3 to train the quantized models with a mini-batch size of 512 unless otherwise specified. Following APOT~\cite{Li2020Additive}, we use weight normalization before quantization. We initialize the clipping levels to 1. We fine-tune 90 epochs for ResNet-18, ResNet-34, and ResNet-50. We set weight decay to $1\times10^{-4}$ by default except for 2-bit ResNet-18, for which we set it to $2.5\times10^{-5}$ following~\cite{Esser2020LEARNED}.
For MobileNetV2, we fine-tune 140 epochs 
\revise{following~\cite{park2020profit}}. We set the weight decay to $4\times10^{-5}$.
The learning rate is initialized to 0.02 and decreased to 0 following the cosine annealing~\cite{loshchilov2016sgdr}. For ViTs, we use LSQ+~\cite{bhalgat2020lsq+} \jing{uniform} quantization following Q-ViT~\cite{li2022q}. We initialize the clipping levels by minimizing the quantization error following~\cite{li2021fixed}. \revise{Relying on AdamW~\cite{loshchilov2018decoupled}, we apply SAQ with Case 3 to train ViTs.} The learning rate is initialized to $2 \times 10^{-4}$ and decreased to 0 using the cosine annealing. We train the quantized model for 150 epochs with a mini-batch size of 1,024. 
We do not apply the automatic mixed-precision training following~\cite{li2022q}. For the hyperparameter $\rho$, we conduct grid search over \{0.02, 0.05, 0.1, 0.15, 0.2, $\dots$, 1.0\} to find appropriate values following the common practice in SAM~\cite{foret2021sharpnessaware} and GSAM~\cite{zhuang2021surrogate}. More details regarding $\rho$ and its sensitivity analysis can be found in the supplementary material.
Following LookSAM~\cite{liu2022towards}, we set hyperparameter $\beta$ and update frequency $\tau$ to 0.7 and 4, respectively. 
\revise{Due to limited space, we put more implementation details in the supplementary material.}

\noindent\textbf{Compared methods.} 
We compare with enormous \jing{fixed-point} quantization methods, including 
DoReFa-Net~\cite{zhou2016dorefa}, PACT~\cite{choi2018pact}, LQ-Nets~\cite{zhang2018lq}, DSQ~\cite{gong2019differentiable},  FAQ~\cite{mckinstry2019discovering}, QIL~\cite{jung2019learning}, Auxi~\cite{zhuang2020training}, PROFIT~\cite{park2020profit}, LSQ~\cite{Esser2020LEARNED}, APOT~\cite{Li2020Additive},  LSQ+~\cite{bhalgat2020lsq+}, LLSQ~\cite{Zhao2020Linear}, DAQ~\cite{kim2021distance}, BRECQ~\cite{li2021brecq}, EWGS~\cite{lee2021network}, BR~\cite{han2021improving}, OOQ~\cite{nagel2022overcoming}, and LLT~\cite{wang2022learnable}.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Performance comparisons in terms of ViT-S/32, ViT-S/16, ViT-B/32, ViT-B/16, and MobileNetV2 on ImageNet. We obtain the results of PACT from~\cite{wang2019haq}. \revise{We do not apply iterative training with weight freezing and progressive quantization in PROFIT~\cite{park2020profit} to improve the performance of the quantized models.}
}
\vspace{-0.1in}
\centering
\scalebox{0.75}
{
\begin{tabular}{cccccccc}
\toprule
Network & Method & \tabincell{c}{Bitwidth \\ (W/A)}  & \tabincell{c}{Top-1 \\ Acc. (\%)} & \tabincell{c}{Top-5 \\ Acc. (\%)} \\
\midrule
\multirow{2}{*}{\tabincell{c}{ViT-S/32 \\ (FP: 68.5)}} & LSQ+~\cite{bhalgat2020lsq+} & 4/4 & 68.0 & 88.1 \\
& SAQ (Ours) & 4/4 & \textbf{68.6} & \textbf{88.4} \\
\midrule
\multirow{2}{*}{\tabincell{c}{ViT-S/16 \\ (FP: 75.9)}} & LSQ+~\cite{bhalgat2020lsq+} & 4/4 & 76.1 & 93.0 \\
& SAQ (Ours) & 4/4 & \textbf{76.9} & \textbf{93.5} \\
\midrule
\multirow{2}{*}{\tabincell{c}{ViT-B/32 \\ (FP: 70.7)}} & LSQ+~\cite{bhalgat2020lsq+} & 4/4 & 72.1 & 90.4 \\
& SAQ (Ours) & 4/4 & \textbf{72.7} & \textbf{90.7} \\
\midrule
\multirow{2}{*}{\tabincell{c}{ViT-B/16 \\ (FP: 77.2)}} & LSQ+~\cite{bhalgat2020lsq+} & 4/4 & 78.0 & 93.4 \\
& SAQ (Ours) & 4/4 & \textbf{79.2} & \textbf{94.2} \\
\midrule
\multirow{9}{*}{\tabincell{c}{MobileNetV2 \\ (FP: 71.9)}} & PACT~\cite{choi2018pact} & 4/4 & 61.4 & 83.7 \\
& DSQ$^{*}$~\cite{gong2019differentiable} & 4/4 & 64.8 & - \\
& BRECQ~\cite{li2021brecq} & 4/4 & 66.6 & - \\
& LLSQ$^{*}$~\cite{Zhao2020Linear} & 4/4 & 67.4 & 88.0 \\
& EWGS~\cite{lee2021network} & 4/4 & 70.3 & - \\
& BR~\cite{han2021improving} & 4/4 & 70.4 & 89.4 \\
& OOQ~\cite{nagel2022overcoming} & 4/4 & 70.6 & - \\
& PROFIT~\cite{park2020profit} & 4/4 & 71.6 & \textbf{90.4} \\
& SAQ (Ours) & 4/4 & \textbf{72.0} & \textbf{90.4} \\
\bottomrule
\end{tabular}
}
\begin{tablenotes}
     \item \footnotesize $^{*}$ denotes that the first and last layers are not quantized.
\end{tablenotes}
\vspace{-0.25in}
\label{table:results_imagenet_mobilenet}
\end{table}

\revise{Unless otherwise specified, we do not apply advanced techniques to boost performance such as pre-activation in LSQ, non-uniform quantization in APOT, weight regularization in BR and OOQ, knowledge distillation in Auxi and PROFIT, iterative training with weight freezing in OOQ and PROFIT, gradient scaling in EWGS, progressive quantization in PROFIT, batch normalization re-estimation in PROFIT and OOQ, asymmetric quantization in PROFIT and LSQ+. More advanced techniques used in other quantization methods are discussed in the supplementary.}


\subsection{Main Results}
We apply \methodshortname to quantize ResNet-18, ResNet-34, and ResNet-50 on ImageNet. From Table~\ref{table:results_on_imagenet_resnet}, \methodshortname outperforms existing SOTA \jing{uniform quantization} methods by a large margin. The improvement is more obvious with the increase of bitwidth. For example, for 2-bit ResNet-34, the Top-1 accuracy improvement of \methodshortname over LSQ is 0.2\% while for the 4-bit one is 0.9\%. We speculate that the loss landscape of the quantized models becomes sharper with the decrease of bitwidths due to the discretization operation in quantization as shown in the supplementary. As a result, smoothing the loss landscapes of the 2-bit quantized models is harder than the 4-bit counterparts. Moreover, for 2-bit quantization, deeper models show more obvious accuracy improvement over the SOTA methods. For instance, \methodshortname surpasses Auxi by 0.7\% on 2-bit ResNet-50 while only bringing 0.2\% Top-1 accuracy improvement over LSQ on 2-bit ResNet-18.
\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Performance comparisons of different cases. We report the results of ResNet-50 on ImageNet. $\lambda_{max}$ denotes the largest eigenvalue of the Hessian of the converged quantized model. Lower $\lambda_{\mathrm{max}}$ indicates flatter loss landscapes.}
\vspace{-0.1in}
\centering
\scalebox{0.7}
{
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Method} & Bitwidth  & 
\multicolumn{2}{c}{Acc. (\%)} & \multirow{2}{*}{$\lambda_{\mathrm{max}}$} & Bitwidth  & \multicolumn{2}{c}{Acc. (\%)} & \multirow{2}{*}{$\lambda_{\mathrm{max}}$} \\
& (W/A) & Top-1 & Top-5 & & (W/A) & Top-1 & Top-5 \\
\midrule
\multicolumn{9}{c}{\tabincell{c}{ResNet-50}} \\
\cdashline{1-9}
SGD & 4/4 & 76.5 & 93.1 & 71.8 & 2/2 & 73.9 & 91.6 & 60.1 \\
Case 1 & 4/4 & 77.3 & 93.5 & 6.6  & 2/2 & 74.3 & 91.8 & 12.6 \\
Case 2 & 4/4 & 77.0 & 93.3 & 14.0 & 2/2 & 74.2 & 91.8 & 24.4 \\
Case 3 & 4/4 & \textbf{77.6} & \textbf{93.6} & \textbf{6.3} & 2/2 & \textbf{74.5} & \textbf{91.9} & \textbf{9.5} \\
\bottomrule
\end{tabular}
}
\vspace{-0.12in}
\label{table:comparisons_different_cases}
\end{table}
Remarkably, our 4-bit ResNet-34 surpasses the full-precision model by 0.9\% on the Top-1 accuracy. One possible reason is that performing quantization with \methodshortname helps to remove redundancy and regularize the networks. Similar phenomena are also observed in LSQ. 

To show the effectiveness of our method on lightweight models, we apply \methodshortname to quantize MobileNetV2. From Table~\ref{table:results_imagenet_mobilenet}, our \methodshortname yields better performance than the SOTA \jing{uniform quantization} methods. For example, SAQ exceeds PROFIT by 0.4\% on the Top-1 accuracy. We also apply \methodshortname to ViT~\cite{dosovitskiy2020image}. We implement LSQ+ following~\cite{li2022q} and compare our method with it. From Table~\ref{table:results_imagenet_mobilenet}, our \methodshortname shows consistently superior performance over the baseline LSQ+ (\eg, 1.2\% Top-1 accuracy improvement on ViT-B/16). 


\begin{figure}[!t]

        \begin{minipage}[t]{0.22\textwidth}
        \centering
        \includegraphics[height=1.0in]{figures/vitb_16_w4a4_loss.pdf}
        \vspace{-0.1in}
        \caption{\revise{The training (dashed line) and validation (solid line) losses for 4-bit ViT-B/16 on ImageNet. The $\lambda_{max}$ of SGD and SAQ obtained quantized models are 14,564 and 676, respectively.}}
        \label{fig:curve_comparisons}
        \end{minipage}
        \hspace{0.05in}
        \begin{minipage}[t]{0.23\textwidth}
        \centering
        \includegraphics[height=1.0in]{figures/r18_cosine_distance_gradient.pdf}
        \vspace{-0.1in}
        \caption{Effect of introducing the vanilla quantization loss (VQL). We visualize the cosine similarity between $\nabla_{\bu} \mL(\bw + \boldsymbol{\epsilon}_q + \hat{\boldsymbol{\epsilon}}_s)$ and $\nabla_{\bu} \mL( Q_w(\bw) )$ in terms of 2-bit and 4-bit ResNet-18 on ImageNet.}
        \label{fig:cosine_distance_two_loss}
        \end{minipage}
    \vspace{-0.3in}
\end{figure}

\subsection{Ablation Studies}
\label{sec:ablation_studies}
\noindent\textbf{Performance comparisons of different cases.} To investigate the effectiveness of different cases introduced in Section~\ref{sec:saq_different_cases}, we apply different methods to quantize ResNet-50 on ImageNet. We use ``SGD'' to represent training the quantized models with the vanilla SGD. 
To measure the loss curvature, we report the largest eigenvalue $\lambda_{\mathrm{max}}$ of the Hessian of the converged quantized models following~\cite{chen2022when,foret2021sharpnessaware}. From Table~\ref{table:comparisons_different_cases}, Case 1, Case 2, and Case 3 all yield significantly higher accuracy and lower $\lambda_{\mathrm{max}}$ than the SGD counterpart. This strongly shows that our method is able to smooth the loss landscape and improve the generalization performance of the quantized models. Among the three cases, Case 2 performs the worst with the lowest accuracy and \jing{the} highest $\lambda_{\mathrm{max}}$, which \jing{suggests} that the perturbations introduced by SAM might be diminished due to the discretization, leading to suboptimal performance. Moreover, Case 3 consistently performs better than Case 1. For example, on 4-bit ResNet-50, Case 3 exceeds Case 1 by 0.3\% on the Top-1 accuracy \jing{as well as achieving lower $\lambda_{\mathrm{max}}$}. These results \jing{indicate} that the perturbation mismatch issue in Case 1 \jing{might degrade} the quantization performance.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Effect of different losses in the objective function in Eq.~(\ref{eq:unified_equation_2}) on ImageNet.  ``VQL'' represents the vanilla quantization loss $\mL(Q_w(\bw))$ and ``PQL'' denotes the perturbed quantization loss $\mL (\bw + \boldsymbol{\epsilon}_q + \hat{\boldsymbol{\epsilon}}_s)$. 
}
\vspace{-0.1in}
\centering
\scalebox{0.72}
{
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{VQL} & \multirow{2}{*}{PQL} & Bitwidth  & \multicolumn{2}{c}{Accuracy (\%)} & Bitwidth  & \multicolumn{2}{c}{Accuracy (\%)} \\
& & (W/A) & Top-1 & Top-5 & (W/A) & Top-1 & Top-5 \\
\midrule
\multicolumn{8}{c}{ResNet-18} \\
\cdashline{1-8}
\checkmark & & 2/2 & 67.3 & 87.4 & 4/4 & 71.1 & 89.8 \\
& \checkmark & 2/2 & 67.5 & 87.5 & 4/4 & 71.3 & 90.0 \\
\checkmark & \checkmark & 2/2 & \textbf{67.8} & \textbf{87.6} & 4/4 & \textbf{71.6} & \textbf{90.1}\\
\bottomrule
\end{tabular}
}
\label{table:effect_different_components}
\vspace{-0.25in}
\end{table}


\revise{Besides, to investigate the generalization capability of SGD and our SAQ, we show the training and validation losses of 4-bit ViT-B/16 on ImageNet in Figure~\ref{fig:curve_comparisons}. Compared with SGD, SAQ achieves lower training and validation losses at the beginning of training. At the end of training, SAQ shows slightly higher training loss but much lower validation loss and $\lambda_{{\mathrm{max}}}$ than SGD. These results justify that \methodshortname converges to a flatter local minimum and thus achieves better generalization performance. 
}


\noindent\textbf{Effect of different losses in the objective function.} To investigate the effect of different components in the objective in Eq.~(\ref{eq:unified_equation_2}), we apply different methods to quantize ResNet-18. 
From Table~\ref{table:effect_different_components}, using the perturbed quantization loss $\mL (\bw + \boldsymbol{\epsilon}_q + \hat{\boldsymbol{\epsilon}}_s)$ surpasses the one equipped with the vanilla quantization loss $\mL(Q_w(\bw))$ by 0.2\% on the Top-1 accuracy for 2-bit and 4-bit quantization, supporting that smoothing the loss landscape improves the generalization performance of the quantized models. By combining $\mL(Q_w(\bw))$ and $\mL (\bw + \boldsymbol{\epsilon}_q + \hat{\boldsymbol{\epsilon}}_s)$, we observe Top-1 accuracy improvement of 0.3\% for both 2-bit and 4-bit quantization. 

To further show the effect of $\mL(Q_w(\bw))$, we visualize the cosine similarity between $\nabla_{\bu} \mL(\bw + \boldsymbol{\epsilon}_q + \hat{\boldsymbol{\epsilon}}_s)$ and $\nabla_{\bu} \mL( Q_w(\bw) )$ of the quantized ResNet-18 in Figure~\ref{fig:cosine_distance_two_loss}. We observe that the cosine similarities of 2-bit models are lower than those of 4-bit ones during training. By introducing $\mL( Q_w(\bw) )$, all quantized models achieve higher similarities. These results justify that introducing $\mL(Q_w(\bw))$ helps to reduce the gap between two losses and improve the performance of the quantized models.

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\caption{Effect of the efficient training (ET) strategy on ImageNet. We measure the training throughput on 4 NVIDIA V100 GPUs with a mini-batch size of 512.
}
\vspace{-0.1in}
\centering
\scalebox{0.72}
{
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Network} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Accuracy (\%)} & Train Throughput \\
& & Top-1 & Top-5 & (images/s) \\
\midrule
\multirow{3}{*}{4-bit ResNet-34} & SGD & 74.4 & 91.9 & \textbf{1632} \\
& SAQ w/o ET & \textbf{75.1} & 92.2 & 841 \\
& SAQ w/ ET & 75.0 & \textbf{92.3} & 1454 \\
\bottomrule
\end{tabular}
}
\label{table:effect_efficient_SAQ}
\vspace{-0.12in}
\end{table}


\noindent\textbf{Effect of the efficient training strategy.} To investigate the effect of the efficient training strategy mentioned in Section~\ref{sec:fast_optimization}, we apply SAQ to train 4-bit ResNet-34 with and without the efficient training strategy on ImageNet. The training throughput is quantified by the number of processed images per second on 4 NVIDIA V100 GPUs with a mini-batch size of 512. From Table~\ref{table:effect_efficient_SAQ}, we observe that our SAQ with efficient training strategy is only $\sim$11\% slower than SGD while achieving nearly the same performance as SAQ without the efficient training strategy.


\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Effect of jointly performing quantization and loss landscape smoothing on ImageNet.
The Top-1 accuracy of the full-precision ResNet-18 obtained by SAM is 70.9\%. 
}
\vspace{-0.1in}
\centering
\scalebox{0.72}
{
\begin{tabular}{cccccccc}
\toprule
\multirow{2}{*}{Method} & Bitwidth  & 
\multicolumn{2}{c}{Accuracy (\%)} & Bitwidth  & 
\multicolumn{2}{c}{Accuracy (\%)} \\
& (W/A) & Top-1 & Top-5 & (W/A) & Top-1 & Top-5 \\
\midrule
\multicolumn{7}{c}{ResNet-18 (FP: 70.7)} \\
\cdashline{1-7}
SGD & 2/2 & 67.3 & 87.4 & 4/4 & 71.1 & 89.8 \\
SAM $\rightarrow$ SGD & 2/2 & 67.4 & 87.5 & 4/4 & 71.1 & 89.9 \\
SAQ (Ours) & 2/2 & \textbf{67.8} & \textbf{87.6} & 4/4 & \textbf{71.6} & \textbf{90.1} \\
\bottomrule
\end{tabular}
}
\label{table:SAQ_vs_flat_quantize}
\vspace{-0.25in}
\end{table}

\noindent\textbf{SAQ vs. train flat and then quantize.}
\revise{
To further investigate the effectiveness of \methodshortname, we compare our method with ``SAM $\rightarrow$ SGD'' that first obtains a full-precision model with SAM and then trains a quantized model with SGD using full-precision model weights as initialization. 
We also include ``SGD'' that trains the quantized models with SGD for comparisons. We report the results of different methods with ResNet-18 on ImageNet.
As seen from Table~\ref{table:SAQ_vs_flat_quantize}, for the full-precision model, SAM outperforms SGD by 0.2\% on the Top-1 accuracy. However, for 2-bit quantization, SAM $\rightarrow$ SGD only yields 0.1\% Top-1 accuracy improvement compared with SGD.}
We speculate that smoothing the loss landscape of the pre-trained models provides a better weight initialization for the quantized models.
Nevertheless, due to the large distribution gap between the quantized weights and full-precision weights, the performance gain over SGD is limited.
In contrast, SAQ performs better than SAM $\rightarrow$ SGD, which shows the superiority of jointly performing quantization and loss landscape smoothing. For example, on ResNet-18, SAQ exceeds SAM $\rightarrow$ SGD by 0.4\% on the Top-1 accuracy.
These results suggest that the improvement comes not only from SAM but also from our SAM customization for network quantization.

\begin{table}[t]
\renewcommand{\arraystretch}{1.3}
\caption{Transfer performance comparisons on downstream tasks. \revise{We measure the performance of different methods on 4-bit ResNet-50 using the Top-1 accuracy (\%).}
}
\vspace{-0.1in}
\centering
\scalebox{0.72}
{
\begin{tabular}{ccc}
\toprule
Method & SGD & SAQ (Ours) \\
\midrule
CIFAR-10~\cite{krizhevsky2009learning} & 97.0$\pm$0.0 & \textbf{97.1$\pm$0.1} \\
CIFAR-100~\cite{krizhevsky2009learning} & 82.4$\pm$0.2 & \textbf{83.1$\pm$0.2} \\
Oxford Flowers-102~\cite{parkhi2012cats} & 96.1$\pm$0.2 & \textbf{96.4$\pm$0.4} \\
Oxford-IIIT Pets~\cite{nilsback2008automated} & 94.9$\pm$0.2 & \textbf{95.9$\pm$0.2} \\
\bottomrule
\end{tabular}
}
\label{table:transfer_results}
\vspace{-0.22in}
\end{table}


\noindent\textbf{More results on transfer learning.}
To evaluate the transfer power of different quantized models, we conduct transfer learning \jing{experiments on new} datasets, including CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100, Oxford-IIIT Pets~\cite{parkhi2012cats}, and Oxford Flowers-102~\cite{nilsback2008automated}. We use the quantized models trained on ImageNet to initialize the model weights and then fine-tune all layers using SGD. We repeat the experiments 5 times and report the mean as well as the standard deviation of the Top-1 accuracy. More implementation details can be found in the supplementary. From Table~\ref{table:transfer_results}, \methodshortname leads to much better transfer performance. For example, on Oxford-IIIT Pets, SAQ quantized 4-bit ResNet-50 brings 1.0\% Top-1 accuracy improvement over the SGD counterpart. These results justify that \methodshortname improves the generalization performance by smoothing the loss landscape of the quantized models.