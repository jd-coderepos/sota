	


Our model consists of a pretrained encoder, a biaffine scoring module, and a TreeCRF model. 
Given a sentence, we obtain the contextualized representations from the encoder, feed the representations to the biaffine layer to get the log potentials for the TreeCRF model, then use the TreeCRF to decode a constituency tree. 
To calculate the probability of partial trees, we propose the \textsc{Masked Inside} algorithm as a simple, efficient algorithm for partial marginalization. 


\subsection{The Base Biaffine Scoring Architecture}
\label{ssec:base_biaffine}
Given a sentence $x$ as a sequence of words: $x = [x_1, x_2, ..., x_n], x_i \in \mathcal{V}$, $\mathcal{V}$ is the vocabulary and $n$ is the  sentence length. 
We use a base biaffine encoder similar to the biaffine dependency parser in \citet{Dozat2017DeepBA} to predict span scores: 

\begin{align}
  e_1, ..., e_n &= \text{FF}(\text{Enc}(x)) \\ 
  s_{ijk} &= e_i^\intercal U_k^{(1)} e_j + (e_i + e_j)^\intercal U_k^{(2)} + b_k \label{eq:biaffine}
\end{align} 
Where Enc$(\cdot)$ denotes a pretrained encoder, FF$(\cdot)$ denotes a feed-forward network, $e_i$ denotes the contextualized embedding for word $x_i$, $U_k^{(1)}$, $U_k^{(2)}$ and $b_k$ are the parameters for the biaffine scoring mechanism, and $s_{ijk}$ means the score for a constituent spanning from word $x_i$ to $x_j$ (inclusive) with label $k$ where
$i, j \in [1, 2, ..., n], k \in [1, 2, ..., |\mathcal{L}|]$, $\mathcal{L}$ is the set of labels for the constituents. 
We further note $\mathcal{L}$ is a union of observed labels $\mathcal{L}_o$ and latent labels $\mathcal{L}_l$, as we will explain later.

\begin{figure}
  \begin{center}		
    \includegraphics[width=3.25in]{figures/example_tree}
  \end{center}	
  \caption{\label{fig:example_tree} An example symbol tree.
  Left: observed partial tree (lower) and its corresponding symbol tree (upper) with different types of nodes induced from the observed tree.
  The symbol tree notation further enables us to build different types of masks for the \textsc{Masked Inside} algorithm. 
  Right: a full tree compatible with the left partial tree (lower) by realizing the latent nodes from \scalebox{0.8}{$\square$} to \scalebox{0.8}{$\blacksquare$} (upper). 
  An entity with dashed lines corresponds to a realized latent node \scalebox{0.8}{$\blacksquare$}. 
  A partial tree may correspond to different realized full trees. 
  }
\end{figure} 

\subsection{Partially-Observed TreeCRFs}
\label{ssec:po_treecrfs}

A constituency TreeCRF is a probabilistic discriminative model that defines a distribution over constituency trees $T$ given  sentence $x$. 
We represent a labeled constituency tree as a rank-3 binary tensor $T$ where $T_{ijk} = 1$ means that there is a span from word $x_i$ to $x_j$ with label $k$. 
We use the biaffine scores $s_{ijk}$ as the log potentials, and the probability of a tree is given by the Gibbs distribution: 

\begin{align}
  s(T) &= \sum_{ijk} T_{ijk}  s_{ijk} \\
  p(T | x) &= \frac{\exp(s(T))}{Z} \\
  Z &= \sum_T \exp(s(T)) = \textsc{Inside}(s) 
\end{align}
Where $Z$ is the partition function that sums over all possible tree structure $T$, which can be computed exactly with the Inside algorithm \citep{Eisner2016InsideOutsideAF}.  


For nested NER, the annotations of trees are incomplete so we only get partial trees. 
With a slight abuse of notation, we still use $T$ to denote partial trees,
and there are locations in $T$ that might be 1 but filled in with 0 because it is not observed (latent). 
To better understand the nature of different nodes in a partial tree, 
we introduce a convenient \textit{symbol tree}  notation $\bar{T}$ given a partial tree $T$. 
$\bar{T}$ is a $n \times n$ matrix with difference types of nodes.
Algorithm \ref{alg:build_mask} gives details for building symbol trees $\bar{T}$ (we delay the discussion about the mask $M$ returned from Algorithm \ref{alg:build_mask} to the next section).
Nodes in $\bar{T}$ can only be one of $\bullet$, \scalebox{0.8}{$\square$} or $\circ$. 
Figure \ref{fig:example_tree} left gives an example of $\bar{T}$. 
A $\bullet$ in $\bar{T}$ means an observed node (a labeled entity) in $T$, 
a $\circ$ means a node that is incompatible with observed nodes because it overlaps with an observed entity 
(e.g., there cannot be a latent entity [BC] because B is already in the observed entity [AB] and the boundaries of entities cannot cross)
and 
a \scalebox{0.8}{$\square$} means a latent node that is possible to be realized in a full tree (e.g., there is a possible entity [ABC] with some latent label). 
We note leaf nodes and the root can only be observed $\bullet$ or latent \scalebox{0.8}{$\square$}. 

\begin{algorithm}[t!]
  \small
  \caption{\label{alg:build_mask} \textsc{Symbol Tree and Mask Construction}}
  \begin{algorithmic}[1]
  
  \State Input: partial tree $T$, $\mathcal{L}_o$ observed labels, $\mathcal{L}_l$ latent labels. 
  \State $\triangleright$ Initialize all nodes as latent \scalebox{0.8}{$\square$}:
  \State \begin{varwidth}[h]{\linewidth} \label{alg:line_latent_mask}
      For all $i, j \in \{1, 2, ..., n\}$: \par
      \hskip\algorithmicindent  $\bar{T}[i, j] = \text{\scalebox{0.8}{$\square$}}$ \par
      \hskip\algorithmicindent  $\forall k_1 \in \mathcal{L}_o, M[i, j, k_1] = 0;\;\; \;\forall k_2 \in \mathcal{L}_l, M[i, j, k_2] = 1$
    \end{varwidth}
  \For{$i, j \gets 1$ to $n$}    
    \If{$\exists k, T_{ijk} = 1$} \Comment{Observe entity $(i, j)$ with label $k$}
      \State $\bar{T}[i, j] = \bullet$ 
      \State $M[i, j, k] = 1$, $\forall m \in \mathcal{L}, m \ne k, M[i, j, m] = 0$ \label{alg:line_observed_mask}
\State $\triangleright$ For all spans with crossed boundaries with $(i, j)$:
      \State \begin{varwidth}[h]{\linewidth} \label{alg:line_rejected_mask_1}
        $\forall (i', j'), i' < i \; \&\; i \le j' < j$: \par
          \hskip\algorithmicindent  $\bar{T}[i', j'] = \circ, \;\;\forall m \in \mathcal{L}, M[i', j', m] = 0$ \par
        \end{varwidth}
\State \begin{varwidth}[h]{\linewidth} \label{alg:line_rejected_mask_2}
        $\forall (i', j'),  i < i' \le j \; \&\; j < j'$: \par
          \hskip\algorithmicindent  $\bar{T}[i', j'] = \circ, \;\;\forall m \in \mathcal{L}, M[i', j', m] = 0$ \par
        \end{varwidth}
    \EndIf
  \EndFor

  \State Return: symbol tree $\bar{T}$, mask $M$
  
\end{algorithmic}
\end{algorithm} 
Given the partial tree $T$, a full tree $\tilde{T}$ compatible with $T$ can be constructed by realizing \scalebox{0.8}{$\square$} to \scalebox{0.8}{$\blacksquare$} in $\bar{T}$
(Figure \ref{fig:example_tree} right. A \scalebox{0.8}{$\blacksquare$} in the upper part denotes a realized latent span corresponding to an entity with a dashed line in the lower part). 
We further denote the set of labels for latent spans $\mathcal{L}_{l}$ as opposed to the labels for observed entities $\mathcal{L}_{o}$. 
We restrict the labels for the latent spans to be within $\mathcal{L}_{l}$. 
For example, the labels for the constituents [ABC] and [DEF] are only allowed to be in $\mathcal{L}_{l}$ because they are latent. 
This separation would allow us to decode partial trees during inference by dropping out entities with latent labels. 
Since there are multiple ways to complete a partial tree, we use $\tilde{\mathcal{T}}$ to denote the set of full trees completed from $T$. 

To train the TreeCRF with partial trees, 
we maximize the conditional probability of $p(T| x)$ computed by marginalizing all latent nodes \scalebox{0.8}{$\square$} out:

\begin{align}
  &\log p(T | x) = s(T) - \log Z  \label{eq:log_prob} \\ 
  &s(T) = \log \sum_{\tilde{T} \in \tilde{\mathcal{T}}} \exp( s(\tilde{T})) \label{eq:summation}
\end{align}
This objective could equivalently be viewed as the average probability of the observed partial tree $T$ over its all possible compatible full trees in $\tilde{\mathcal{T}}$. 








\begin{algorithm}[t]
  \small
  \caption{\label{alg:partial_summation} \textsc{Inside for Partial Marginalization}}
  \begin{algorithmic}[1]
  
  \State Input: Scores $s$, partial tree $T$ and its corresponding $\bar{T}$
  \For{$i \gets 1$ to $n$}  
    \If{$\bar{T}[i, i] = \bullet$} \Comment{Observed leaf}
      \State $\exists k \in \mathcal{L}_o, T_{iik} = 1, \beta[i, i, k] = \exp(s_{iik})$   \label{alg:line_observed_1}  
      \State $\forall m \ne k, \beta[i, i, m] = 0 $
    \ElsIf{$\bar{T}[i, i] = \text{\scalebox{0.8}{$\square$}}$} \label{alg:line_latent_1} \Comment{Latent leaf} 
      \State $\forall k \in \mathcal{L}_o, \beta[i, i, k] = 0 $ \label{alg:line_latent_o_1}
      \State $\forall k \in \mathcal{L}_l, \beta[i, i, k] = \exp(s_{iik}) $ \label{alg:line_latent_l_1}
    \EndIf
  \EndFor

  \For{$d \gets 1$ to $n - 1$}       
    \For{$i \gets 1$ to $n - d$}       
      \State $j = i + d$
      \If{$\bar{T}[i, j] = \bullet$}  \Comment{Observed}
        \State $\exists k \in \mathcal{L}_o, T_{ijk} = 1$ 
        \State \begin{varwidth}[h]{\linewidth}
          $\beta[i, j, k] = \exp(s_{ijk}) \cdot$ \par
            \hskip\algorithmicindent $ \sum_{l = i}^{j - 1} \sum_{k_1, k_2 \in \mathcal{L}} \beta[i, l, k_1] \beta[l + 1, j, k_2]$
          \end{varwidth} \label{alg:line_observed_2}
        \State $\forall m \ne k, \beta[i, j, m] = 0 $
      \ElsIf{$\bar{T}[i, j] = \text{\scalebox{0.8}{$\square$}}$} \label{alg:line_latent_2} \label{alg:line_latent_2} \Comment{Latent}
        \State \begin{varwidth}[h]{\linewidth} \label{alg:line_latent_l_2}
          $\forall k \in \mathcal{L}_l, \beta[i, j, k] = \exp(s_{ijk}) \cdot$ \par
            \hskip\algorithmicindent $ \sum_{l = i}^{j - 1} \sum_{k_1, k_2 \in \mathcal{L}} \beta[i, l, k_1] \beta[l + 1, j, k_2]$
          \end{varwidth}
        \State $\forall k \in \mathcal{L}_o, \beta[i, j, k] = 0 $ \label{alg:line_latent_o_2}
      \ElsIf{$\bar{T}[i, j] = \circ$} \label{alg:line_rejected} \Comment{Rejected}
        \State $\forall k \in \mathcal{L}, \beta[i, j, k] = 0 $ \label{alg:line_sum_end}
      \EndIf
    \EndFor
  \EndFor

  \If{$\bar{T}[1, n] = \bullet$}  \label{alg:line_observed_3}  \Comment{Observed root}
    \State $\exists k \in \mathcal{L}_o, T_{1nk} = 1$. Return $s(T) = \beta[1, n, k]$
  \ElsIf{$\bar{T}[1, n] = \text{\scalebox{0.8}{$\square$}}$} \label{alg:line_latent_3} \Comment{Latent root}
    \State Return $s(T) = \log(\sum_{k \in \mathcal{L}_l}\beta[1, n, k])$ \label{alg:line_latent_l_3}
  \EndIf
  
\end{algorithmic}
\end{algorithm} 
\subsection{Masked Inside for Efficient Partial Marginalization}
\label{ssec:masked_inside}
To compute the partial marginalization in equation~(\ref{eq:summation}), we give a tailored Inside algorithm that supports different inference operations for different nodes.
As is shown in Algorithm \ref{alg:partial_summation}, during the summation process,
if the current node is:
(a) an observed~$\bullet$, then we evaluate (add) its corresponding score (line \ref{alg:line_observed_1} and \ref{alg:line_observed_2}); 
(b) a latent~\scalebox{0.8}{$\square$} whose label can only be in $\mathcal{L}_l$.
So we reject (do not add) all the scores corresponding to observed labels $\mathcal{L}_o$  (line \ref{alg:line_latent_o_1} and \ref{alg:line_latent_o_2}), and sum over all scores corresponding to latent labels $\mathcal{L}_l$ for this node (line \ref{alg:line_latent_l_1} and \ref{alg:line_latent_l_2}); 
(c) a rejected~$\circ$, we reject all scores corresponding to this node (line \ref{alg:line_sum_end}). 

However, a naive implementation of Algorithm~\ref{alg:partial_summation} can be inefficient with $O(n^3)$ complexity. 
Such inefficiency has previously restricted the application of TreeCRFs in parsing literature. 
More severely, Algorithm~\ref{alg:partial_summation} does not support batch computation over sentences, making it more impractical.
Recent works show that, for the original Inside algorithm, it is possible to parallelize it on modern hardware architectures with efficient batch computation, reducing the complexity from $O(n^3)$ to at least $O(n^2)$, and could further be $O(n\log n)$\footnote{
The complexity discussed here does not include the summation over $k_1, k_2$ in line~\ref{alg:line_masked_sum}. If the summation over $l$ is implemented with a binary tree shaped summation then the complexity is $O(n \log n)$, if it is sequential then the complexity is $O(n^2)$. The optimization of this summation is usually implemented inside tensor computation libraries.
}
\citep{Rush2020TorchStructDS, Zhang2020FastAA}. 
It would be ideal if we could use similar batchification techniques for Algorithm~\ref{alg:partial_summation}, which motivates our proposed \textsc{Masked Inside} algorithm for efficient partial marginalization. 







\begin{algorithm}[t!]
  \small
  \caption{\label{alg:masked_inside} \textsc{Masked Inside}}
  \begin{algorithmic}[1]
  
  \State Input: Scores $s$, mask $M$
  \For{$i \gets 1$ to $n$}    
    \State $\beta[i, i, k] = M[i, i, k] \cdot \exp(s_{iik})$ \label{alg:line_masked_leaf} \Comment{Masked leaf scores} 
  \EndFor
  \For{$d \gets 1$ to $n - 1$}       
\State \begin{varwidth}[h]{\linewidth} 
      Parallelization on $i$, tensor operation on $l, k, k_1, k_2$ \label{alg:line_parallel} \par
        \hskip\algorithmicindent  $1 \le i \le n - d$; $\;\; j = i + d$;  $\;\; k, k_1, k_2 \in \{1, ..., |\mathcal{L}|\}$
      \end{varwidth}
\State \begin{varwidth}[h]{\linewidth} \label{alg:line_masked_sum}
      $\beta[i, j, k] = (M[i, j, k] \exp(s_{i j k})) \; \cdot$ \Comment{Masked scores} \par
        \hskip\algorithmicindent $\sum_{l = i}^{j - 1} \sum_{k_1, k_2 \in \mathcal{L}} \beta[i, l, k_1] \beta[l + 1, j, k_2]$
      \end{varwidth}
  \EndFor

  \State Return: $s(T) = \log(\sum_{k \in \mathcal{L}} \beta[1, n, k])$
  
\end{algorithmic}
\end{algorithm}

 

As is shown in Algorithm~\ref{alg:masked_inside}, the key insight of \textsc{Masked Inside} is that \textit{all if-else statements for partial summation in Algorithm~\ref{alg:partial_summation} 
can be re-written in a unified masked summation} (line~\ref{alg:line_masked_leaf} and \ref{alg:line_masked_sum} in Algorithm~\ref{alg:masked_inside}) with a pre-computed mask $M$ from Algorithm~\ref{alg:build_mask}. 
To be specific, in Algorithm~\ref{alg:build_mask}: 
(a) for an observed node~$\bullet$, we mask out all scores except the score of its observed label (line~\ref{alg:line_observed_mask}), 
which corresponds to lines~\ref{alg:line_observed_1}, \ref{alg:line_observed_2} and \ref{alg:line_observed_3} in Algorithm~\ref{alg:partial_summation}; 
(b) for a rejected node~$\circ$, we mask out all its scores (lines \ref{alg:line_rejected_mask_1}-\ref{alg:line_rejected_mask_2}), which corresponds to line~\ref{alg:line_rejected} in Algorithm~\ref{alg:partial_summation}; 
(c) for a latent node~\scalebox{0.8}{$\square$}, we mask out the scores for all observed labels $\mathcal{L}_o$, and retain scores for all latent labels $\mathcal{L}_l$ (line~\ref{alg:line_latent_mask}), which corresponds to lines~\ref{alg:line_latent_1}, \ref{alg:line_latent_2}, and \ref{alg:line_latent_3} in Algorithm~\ref{alg:partial_summation}. 
Applying different masks to Algorithm~\ref{alg:masked_inside}, we can recover all if-else statements in Algorithm~\ref{alg:partial_summation}. 
As two special cases, if all masks are 1 (i.e., not masked), we recover the original Inside algorithm; if the masks are constructed from a full tree, we recover the original bottom-up evaluation. 

\begin{table*}[t]
  \centering
  \begin{tabular}{@{} l  ccc  ccc  ccc @{}}
    \toprule
    & \multicolumn{3}{c}{ACE2004} & \multicolumn{3}{c}{ACE2005} & \multicolumn{3}{c}{ GENIA} \\ 
    &Train & Dev & Test &Train & Dev & Test &Train & Dev & Test \\ 
    \hline			
    \# sentences & 7,078 & 859 & 922 & 7,194 & 969 &1,047 & 14,836 & 1,855 & 1,855 \\ 			
    {with nested entities } & 2,691 & 290 & 377 & 2,691 & 338 & 330 & 3,199 & 362 & 448 \\			
    \# entities & 22,172 &2,510 & 3,024 & 24,441 & 3,200 & 2,993 & 46,473 & 5,014 & 5,600 \\			
    {\# nested entities} & 10,080 &1,086 & 1,410 &9,389 & 1,112& 1,118 & 8,337 & 903 & 1,217 \\			
    avg length & 20.38 & 20.69 & 20.96 & 19.21 & 18.93 &17.19 &30.13 & 29.17 &30.48 \\
    \bottomrule
  \end{tabular}
  \caption{Statistics of ACE2004, ACE2005, and GENIA datasets.}
  \label{dataset}
\end{table*} 
	\begin{table*}[t]
		\begin{center}
			\begin{tabular}{@{}l c c c c c c c c c @{}}
				\toprule
				& \multicolumn{3}{c}{ACE2004} 	& \multicolumn{3}{c	}{ACE2005} & \multicolumn{3}{c}{GENIA} \\ 
Model & P & R & F1 & P & R & F1 & P & R & F1 \\
				\hline						
				LSTM-CRF~\cite{lample2016neural} & 71.3 & 50.5 & 58.3 & 70.3 & 55.7 & 62.2 & 75.2 & 64.6 & 69.5 \\
FOFE(c=6)~(Xu et al. 2017) & 68.2& 54.3 & 60.5 & 76.5 & 66.3 & 71.0 & 75.4 & 67.8 & 71.4 \\
Transition~\cite{wang2018transition} & 74.9 &71.8 & 73.3& 74.5 & 71.5 & 73.0 & 78.0 & 70.2 & 73.9 \\
				Cascaded-CRF~(Ju et al. 2018) &-&-& - & 74.2 & 70.3 & 72.2 & 78.5 & 71.3 & 74.7 \\		
SH(c=n)~\cite{wang2018hyper} & 77.7 &72.1&74.5 & 76.8 & 72.3 & 74.5 & 77.0 & 73.3 & 75.1 \\  				
ML~\cite{fisher2019merge} &-&-&- & 75.1& 74.1 &74.6 & - &- &- \\
				BENSC~\cite{tan2020boundary} & 78.1 & 72.8 & 75.3 & 77.1 & 74.2 & 75.6 & 78.9 & 72.7 & 75.7\\
				Pyramid \cite{jue2020pyramid} & 81.1 & 79.4 & 80.3 & 80.0 & 78.9 & 79.4 & 78.6 & 77.0 & 77.8 \\
				\hline
				\multicolumn{8}{@{}l}{with Pretrained LM } \\	
MGNER (ELMo)~\cite{xia-etal-2019-multi}& 81.7& 77.4& 79.5 & 79.0 & 77.3 & 78.2 &-&-&-  \\
				ML (ELMo) \cite{fisher2019merge} &- &- &-& 79.7 & 78.0 & 78.9 &- &- &- \\
				ML (BERT) \cite{fisher2019merge} &- &- &- & 82.7 & 82.1 & 82.4 &- &- &- \\
				Seq2seq \cite{strakova-etal-2019-neural} &- &- & 84.3 &- &- & 83.4 &- &- & 78.2 \\
				BENSC (BERT) \cite{tan2020boundary} & 85.8 & 84.8 & 85.3 & 83.8& 83.9 & 83.9 & 79.2 & 77.4 & 78.3 \\
				Pyramid (BERT) \cite{jue2020pyramid} & 86.1 & 86.5 & 86.3 & 84.0 & 85.4 & 84.7 & 79.5 & 78.9 & 79.2 \\
\hline
				\multicolumn{8}{@{}l}{with Additional Supervision } \\
				DYGIE \cite{luan-etal-2019-general} &- &- & 84.7 &- & - & 82.9 & - & - & 76.2 \\ 
				\citet{yu-etal-2020-named}  & 87.3 & 86.0 & 86.7 & 85.2 & 85.6 & 85.4 & 81.8 & 79.3 & 80.5 \\
				BERT-MRC \cite{li-etal-2020-unified} & 85.0 &  86.3 & 86.0  & 87.2 & 86.6 & 86.9 & 85.2 & 81.1 & 83.8 \\ 
				\hline
				\multirow{2}{5cm}{\bf PO-TreeCRFs (ours)} & 86.7 & 86.5 & 86.6 &  84.5 	  &  86.4 & 85.4 & 78.2 & 78.2 & 78.2 \\
				& \small{$\pm$0.4}  & \small{$\pm$0.4}  & \small{$\pm$0.3}  & \small{$\pm$0.4}  & \small{$\pm$0.2}  & \small{$\pm$0.1}  & \small{$\pm$0.7}  & \small{$\pm$0.8}  & \small{$\pm$0.1}   \\
				\hline
				\multicolumn{8}{@{}l}{\bf PO-TreeCRFs Ablation Study} \\	
Change Biaffine to Bilinear & 86.0 & 86.7 & 86.4 & 83.0 & 86.5 & 84.7 & 79.9 & 75.5 & 77.6   \\
				W/o. Structure Smoothing & 86.1 & 86.4 & 86.2 & 83.5 & 85.8 & 84.6 & 78.7 & 76.5 & 77.6 \\
				W/o. Potential Normalization and Structure Smoothing & 86.0 & 85.3 & 85.7 & 82.7 & 86.2 & 84.4 & 76.5 & 78.1 & 77.3 \\
				W/o. TreeCRFs & 84.4 & 85.4 & 84.9 & 82.0 & 86.4 & 84.1 & 80.5 & 74.5 & 77.4 \\ 
				\bottomrule
			\end{tabular}		
		\end{center}
		\caption{Main results and ablation studies on three datasets. We report the average scores of 5 runs for main results. } 
		\label{result}
	\end{table*} 
As an efficient alternative for Algorithm~\ref{alg:partial_summation}, the advantages of Algorithm~\ref{alg:masked_inside} is that it is (a) conceptually much simpler and (b) highly parallelizable.
The later allows us to fully exploit the computational power of modern hardware architectures (like GPUs) and highly optimized tensor operation libraries (like Pytorch).
We note that it the parallelization on $i$ in Algorithm~\ref{alg:masked_inside} that reduces the complexity to at least $O(n^2)$, 
Furthermore, as an equivalent implementation to Algorithm~\ref{alg:masked_inside}, we can apply masks to scores in the logarithm scale\footnote{In our implementation, we use $-10^6$ to substitute the undefined $\log 0$ for numerical stability.}
then feed the masked scores to a normal Inside algorithm (rather than multiplying the masks inside Algorithm~\ref{alg:masked_inside}): 

\begin{align}
  s(T) &= \textsc{MaskedInside}(s, M) \nonumber \\
  &= \textsc{Inside}( \log M + s) \label{eq:masked_inside}\\ 
  p(T | x) &= \frac{\exp(s(T))}{Z} \label{eq:log_prob_efficient}
\end{align}
In practice, we compute the masks $M$ in the data-processing stage.
For training, by reusing existing implementations of the Inside algorithm in highly-optimized structured prediction libraries like Torch-Struct~\citep{Rush2020TorchStructDS},
we can \textit{implement the \textsc{Masked Inside} with one single line of code}, as is shown in equation~(\ref{eq:masked_inside}), thus significantly reducing the implementation complexity required by Algorithm~\ref{alg:partial_summation}. 





\subsection{Regularization} 
\label{ssec:regularization}
We propose two regularization techniques for TreeCRFs: (a)
\textit{potential normalization}, which is inspired by batch normalization \citep{Ioffe2015BatchNA}, and (b) \textit{structure smoothing}, which is inspired by label smoothing  \citep{Mller2019WhenDL}. 
Potential normalization (PN) is simple: we normalize the scores $s$ to an empirical distribution of zero mean and one variance. 
The difference with batch-norm (BN) is that we apply PN at an instance-level, rather than a batch-level. 
In our experiments, we observe that PN gives a slightly better convergence. 

Structure smoothing regularizes TreeCRFs by putting a small portion of weights to nodes that are marginalized out. 
Specifically, during the partial marginalization, instead of using a zero mask that does not include the weights of rejected nodes, we change the mask to a small value $\epsilon$

\begin{align}
  M[i, j, k] = 0 \to M[i, j, k] = \epsilon \quad \text{for rejected $\circ$}
\end{align}
This would effectively add the weights of all rejected nodes to $s(T)$ with a multiplier $\epsilon$. 
This is similar to label smoothing which adds a small portion of weights to all labels other than the target label. 
The reason that we call it structure smoothing is that it not only smooths over the labels, but also smooths over different tree structures\footnote{Additionally, if we randomize $\epsilon$ and make them i.i.d. Gumbel samples, we recover the Stochastic Softmax Trick~\citep{paulus2020gradient}, which generalize Gumbel-Softmax to combinatorial structures.}. 
We further observe that structure smoothing should be based on potential normalization for numerical stability, otherwise it does not converge in our experiments. 
The implementation of structure smoothing is still easy and aligns with previous discussions about equation~(\ref{eq:masked_inside})
as one only needs to change the zeros in $M$ to $\epsilon$. 





\subsection{Training and Inference} 
\label{ssec:training_inference}
During training,
we maximize the log conditional probability $\log p(T|x)$ efficiently computed by equation~(\ref{eq:masked_inside}) and~(\ref{eq:log_prob_efficient}).
During inference, we use CKY decoding to decode a full tree with the maximum probability. 
We only include nodes whose labels are in the observed label set $\mathcal{L}_o$, and dismiss nodes whose labels are in the latent set $\mathcal{L}_l$.
This would allow us to decode nested entities (partial trees) for evaluation. 



%
