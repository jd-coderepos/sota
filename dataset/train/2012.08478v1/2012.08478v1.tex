	


Our model consists of a pretrained encoder, a biaffine scoring module, and a TreeCRF model. 
Given a sentence, we obtain the contextualized representations from the encoder, feed the representations to the biaffine layer to get the log potentials for the TreeCRF model, then use the TreeCRF to decode a constituency tree. 
To calculate the probability of partial trees, we propose the \textsc{Masked Inside} algorithm as a simple, efficient algorithm for partial marginalization. 


\subsection{The Base Biaffine Scoring Architecture}
\label{ssec:base_biaffine}
Given a sentence  as a sequence of words: ,  is the vocabulary and  is the  sentence length. 
We use a base biaffine encoder similar to the biaffine dependency parser in \citet{Dozat2017DeepBA} to predict span scores: 

 
Where Enc denotes a pretrained encoder, FF denotes a feed-forward network,  denotes the contextualized embedding for word , ,  and  are the parameters for the biaffine scoring mechanism, and  means the score for a constituent spanning from word  to  (inclusive) with label  where
,  is the set of labels for the constituents. 
We further note  is a union of observed labels  and latent labels , as we will explain later.

\begin{figure}
  \begin{center}		
    \includegraphics[width=3.25in]{figures/example_tree}
  \end{center}	
  \caption{\label{fig:example_tree} An example symbol tree.
  Left: observed partial tree (lower) and its corresponding symbol tree (upper) with different types of nodes induced from the observed tree.
  The symbol tree notation further enables us to build different types of masks for the \textsc{Masked Inside} algorithm. 
  Right: a full tree compatible with the left partial tree (lower) by realizing the latent nodes from \scalebox{0.8}{} to \scalebox{0.8}{} (upper). 
  An entity with dashed lines corresponds to a realized latent node \scalebox{0.8}{}. 
  A partial tree may correspond to different realized full trees. 
  }
\end{figure} 

\subsection{Partially-Observed TreeCRFs}
\label{ssec:po_treecrfs}

A constituency TreeCRF is a probabilistic discriminative model that defines a distribution over constituency trees  given  sentence . 
We represent a labeled constituency tree as a rank-3 binary tensor  where  means that there is a span from word  to  with label . 
We use the biaffine scores  as the log potentials, and the probability of a tree is given by the Gibbs distribution: 


Where  is the partition function that sums over all possible tree structure , which can be computed exactly with the Inside algorithm \citep{Eisner2016InsideOutsideAF}.  


For nested NER, the annotations of trees are incomplete so we only get partial trees. 
With a slight abuse of notation, we still use  to denote partial trees,
and there are locations in  that might be 1 but filled in with 0 because it is not observed (latent). 
To better understand the nature of different nodes in a partial tree, 
we introduce a convenient \textit{symbol tree}  notation  given a partial tree . 
 is a  matrix with difference types of nodes.
Algorithm \ref{alg:build_mask} gives details for building symbol trees  (we delay the discussion about the mask  returned from Algorithm \ref{alg:build_mask} to the next section).
Nodes in  can only be one of , \scalebox{0.8}{} or . 
Figure \ref{fig:example_tree} left gives an example of . 
A  in  means an observed node (a labeled entity) in , 
a  means a node that is incompatible with observed nodes because it overlaps with an observed entity 
(e.g., there cannot be a latent entity [BC] because B is already in the observed entity [AB] and the boundaries of entities cannot cross)
and 
a \scalebox{0.8}{} means a latent node that is possible to be realized in a full tree (e.g., there is a possible entity [ABC] with some latent label). 
We note leaf nodes and the root can only be observed  or latent \scalebox{0.8}{}. 

\begin{algorithm}[t!]
  \small
  \caption{\label{alg:build_mask} \textsc{Symbol Tree and Mask Construction}}
  \begin{algorithmic}[1]
  
  \State Input: partial tree ,  observed labels,  latent labels. 
  \State  Initialize all nodes as latent \scalebox{0.8}{}:
  \State \begin{varwidth}[h]{\linewidth} \label{alg:line_latent_mask}
      For all : \par
      \hskip\algorithmicindent  \square \par
      \hskip\algorithmicindent  
    \end{varwidth}
  \For{ to }    
    \If{} \Comment{Observe entity  with label }
      \State  
      \State ,  \label{alg:line_observed_mask}
\State  For all spans with crossed boundaries with :
      \State \begin{varwidth}[h]{\linewidth} \label{alg:line_rejected_mask_1}
        : \par
          \hskip\algorithmicindent   \par
        \end{varwidth}
\State \begin{varwidth}[h]{\linewidth} \label{alg:line_rejected_mask_2}
        : \par
          \hskip\algorithmicindent   \par
        \end{varwidth}
    \EndIf
  \EndFor

  \State Return: symbol tree , mask 
  
\end{algorithmic}
\end{algorithm} 
Given the partial tree , a full tree  compatible with  can be constructed by realizing \scalebox{0.8}{} to \scalebox{0.8}{} in 
(Figure \ref{fig:example_tree} right. A \scalebox{0.8}{} in the upper part denotes a realized latent span corresponding to an entity with a dashed line in the lower part). 
We further denote the set of labels for latent spans  as opposed to the labels for observed entities . 
We restrict the labels for the latent spans to be within . 
For example, the labels for the constituents [ABC] and [DEF] are only allowed to be in  because they are latent. 
This separation would allow us to decode partial trees during inference by dropping out entities with latent labels. 
Since there are multiple ways to complete a partial tree, we use  to denote the set of full trees completed from . 

To train the TreeCRF with partial trees, 
we maximize the conditional probability of  computed by marginalizing all latent nodes \scalebox{0.8}{} out:


This objective could equivalently be viewed as the average probability of the observed partial tree  over its all possible compatible full trees in . 








\begin{algorithm}[t]
  \small
  \caption{\label{alg:partial_summation} \textsc{Inside for Partial Marginalization}}
  \begin{algorithmic}[1]
  
  \State Input: Scores , partial tree  and its corresponding 
  \For{ to }  
    \If{} \Comment{Observed leaf}
      \State    \label{alg:line_observed_1}  
      \State 
    \ElsIf{\square} \label{alg:line_latent_1} \Comment{Latent leaf} 
      \State  \label{alg:line_latent_o_1}
      \State  \label{alg:line_latent_l_1}
    \EndIf
  \EndFor

  \For{ to }       
    \For{ to }       
      \State 
      \If{}  \Comment{Observed}
        \State  
        \State \begin{varwidth}[h]{\linewidth}
           \par
            \hskip\algorithmicindent 
          \end{varwidth} \label{alg:line_observed_2}
        \State 
      \ElsIf{\square} \label{alg:line_latent_2} \label{alg:line_latent_2} \Comment{Latent}
        \State \begin{varwidth}[h]{\linewidth} \label{alg:line_latent_l_2}
           \par
            \hskip\algorithmicindent 
          \end{varwidth}
        \State  \label{alg:line_latent_o_2}
      \ElsIf{} \label{alg:line_rejected} \Comment{Rejected}
        \State  \label{alg:line_sum_end}
      \EndIf
    \EndFor
  \EndFor

  \If{}  \label{alg:line_observed_3}  \Comment{Observed root}
    \State . Return 
  \ElsIf{\square} \label{alg:line_latent_3} \Comment{Latent root}
    \State Return  \label{alg:line_latent_l_3}
  \EndIf
  
\end{algorithmic}
\end{algorithm} 
\subsection{Masked Inside for Efficient Partial Marginalization}
\label{ssec:masked_inside}
To compute the partial marginalization in equation~(\ref{eq:summation}), we give a tailored Inside algorithm that supports different inference operations for different nodes.
As is shown in Algorithm \ref{alg:partial_summation}, during the summation process,
if the current node is:
(a) an observed~, then we evaluate (add) its corresponding score (line \ref{alg:line_observed_1} and \ref{alg:line_observed_2}); 
(b) a latent~\scalebox{0.8}{} whose label can only be in .
So we reject (do not add) all the scores corresponding to observed labels   (line \ref{alg:line_latent_o_1} and \ref{alg:line_latent_o_2}), and sum over all scores corresponding to latent labels  for this node (line \ref{alg:line_latent_l_1} and \ref{alg:line_latent_l_2}); 
(c) a rejected~, we reject all scores corresponding to this node (line \ref{alg:line_sum_end}). 

However, a naive implementation of Algorithm~\ref{alg:partial_summation} can be inefficient with  complexity. 
Such inefficiency has previously restricted the application of TreeCRFs in parsing literature. 
More severely, Algorithm~\ref{alg:partial_summation} does not support batch computation over sentences, making it more impractical.
Recent works show that, for the original Inside algorithm, it is possible to parallelize it on modern hardware architectures with efficient batch computation, reducing the complexity from  to at least , and could further be \footnote{
The complexity discussed here does not include the summation over  in line~\ref{alg:line_masked_sum}. If the summation over  is implemented with a binary tree shaped summation then the complexity is , if it is sequential then the complexity is . The optimization of this summation is usually implemented inside tensor computation libraries.
}
\citep{Rush2020TorchStructDS, Zhang2020FastAA}. 
It would be ideal if we could use similar batchification techniques for Algorithm~\ref{alg:partial_summation}, which motivates our proposed \textsc{Masked Inside} algorithm for efficient partial marginalization. 







\begin{algorithm}[t!]
  \small
  \caption{\label{alg:masked_inside} \textsc{Masked Inside}}
  \begin{algorithmic}[1]
  
  \State Input: Scores , mask 
  \For{ to }    
    \State  \label{alg:line_masked_leaf} \Comment{Masked leaf scores} 
  \EndFor
  \For{ to }       
\State \begin{varwidth}[h]{\linewidth} 
      Parallelization on , tensor operation on  \label{alg:line_parallel} \par
        \hskip\algorithmicindent  ; ;  
      \end{varwidth}
\State \begin{varwidth}[h]{\linewidth} \label{alg:line_masked_sum}
       \Comment{Masked scores} \par
        \hskip\algorithmicindent 
      \end{varwidth}
  \EndFor

  \State Return: 
  
\end{algorithmic}
\end{algorithm}

 

As is shown in Algorithm~\ref{alg:masked_inside}, the key insight of \textsc{Masked Inside} is that \textit{all if-else statements for partial summation in Algorithm~\ref{alg:partial_summation} 
can be re-written in a unified masked summation} (line~\ref{alg:line_masked_leaf} and \ref{alg:line_masked_sum} in Algorithm~\ref{alg:masked_inside}) with a pre-computed mask  from Algorithm~\ref{alg:build_mask}. 
To be specific, in Algorithm~\ref{alg:build_mask}: 
(a) for an observed node~, we mask out all scores except the score of its observed label (line~\ref{alg:line_observed_mask}), 
which corresponds to lines~\ref{alg:line_observed_1}, \ref{alg:line_observed_2} and \ref{alg:line_observed_3} in Algorithm~\ref{alg:partial_summation}; 
(b) for a rejected node~, we mask out all its scores (lines \ref{alg:line_rejected_mask_1}-\ref{alg:line_rejected_mask_2}), which corresponds to line~\ref{alg:line_rejected} in Algorithm~\ref{alg:partial_summation}; 
(c) for a latent node~\scalebox{0.8}{}, we mask out the scores for all observed labels , and retain scores for all latent labels  (line~\ref{alg:line_latent_mask}), which corresponds to lines~\ref{alg:line_latent_1}, \ref{alg:line_latent_2}, and \ref{alg:line_latent_3} in Algorithm~\ref{alg:partial_summation}. 
Applying different masks to Algorithm~\ref{alg:masked_inside}, we can recover all if-else statements in Algorithm~\ref{alg:partial_summation}. 
As two special cases, if all masks are 1 (i.e., not masked), we recover the original Inside algorithm; if the masks are constructed from a full tree, we recover the original bottom-up evaluation. 

\begin{table*}[t]
  \centering
  \begin{tabular}{@{} l  ccc  ccc  ccc @{}}
    \toprule
    & \multicolumn{3}{c}{ACE2004} & \multicolumn{3}{c}{ACE2005} & \multicolumn{3}{c}{ GENIA} \\ 
    &Train & Dev & Test &Train & Dev & Test &Train & Dev & Test \\ 
    \hline			
    \# sentences & 7,078 & 859 & 922 & 7,194 & 969 &1,047 & 14,836 & 1,855 & 1,855 \\ 			
    {with nested entities } & 2,691 & 290 & 377 & 2,691 & 338 & 330 & 3,199 & 362 & 448 \\			
    \# entities & 22,172 &2,510 & 3,024 & 24,441 & 3,200 & 2,993 & 46,473 & 5,014 & 5,600 \\			
    {\# nested entities} & 10,080 &1,086 & 1,410 &9,389 & 1,112& 1,118 & 8,337 & 903 & 1,217 \\			
    avg length & 20.38 & 20.69 & 20.96 & 19.21 & 18.93 &17.19 &30.13 & 29.17 &30.48 \\
    \bottomrule
  \end{tabular}
  \caption{Statistics of ACE2004, ACE2005, and GENIA datasets.}
  \label{dataset}
\end{table*} 
	\begin{table*}[t]
		\begin{center}
			\begin{tabular}{@{}l c c c c c c c c c @{}}
				\toprule
				& \multicolumn{3}{c}{ACE2004} 	& \multicolumn{3}{c	}{ACE2005} & \multicolumn{3}{c}{GENIA} \\ 
Model & P & R & F1 & P & R & F1 & P & R & F1 \\
				\hline						
				LSTM-CRF~\cite{lample2016neural} & 71.3 & 50.5 & 58.3 & 70.3 & 55.7 & 62.2 & 75.2 & 64.6 & 69.5 \\
FOFE(c=6)~(Xu et al. 2017) & 68.2& 54.3 & 60.5 & 76.5 & 66.3 & 71.0 & 75.4 & 67.8 & 71.4 \\
Transition~\cite{wang2018transition} & 74.9 &71.8 & 73.3& 74.5 & 71.5 & 73.0 & 78.0 & 70.2 & 73.9 \\
				Cascaded-CRF~(Ju et al. 2018) &-&-& - & 74.2 & 70.3 & 72.2 & 78.5 & 71.3 & 74.7 \\		
SH(c=n)~\cite{wang2018hyper} & 77.7 &72.1&74.5 & 76.8 & 72.3 & 74.5 & 77.0 & 73.3 & 75.1 \\  				
ML~\cite{fisher2019merge} &-&-&- & 75.1& 74.1 &74.6 & - &- &- \\
				BENSC~\cite{tan2020boundary} & 78.1 & 72.8 & 75.3 & 77.1 & 74.2 & 75.6 & 78.9 & 72.7 & 75.7\\
				Pyramid \cite{jue2020pyramid} & 81.1 & 79.4 & 80.3 & 80.0 & 78.9 & 79.4 & 78.6 & 77.0 & 77.8 \\
				\hline
				\multicolumn{8}{@{}l}{with Pretrained LM } \\	
MGNER (ELMo)~\cite{xia-etal-2019-multi}& 81.7& 77.4& 79.5 & 79.0 & 77.3 & 78.2 &-&-&-  \\
				ML (ELMo) \cite{fisher2019merge} &- &- &-& 79.7 & 78.0 & 78.9 &- &- &- \\
				ML (BERT) \cite{fisher2019merge} &- &- &- & 82.7 & 82.1 & 82.4 &- &- &- \\
				Seq2seq \cite{strakova-etal-2019-neural} &- &- & 84.3 &- &- & 83.4 &- &- & 78.2 \\
				BENSC (BERT) \cite{tan2020boundary} & 85.8 & 84.8 & 85.3 & 83.8& 83.9 & 83.9 & 79.2 & 77.4 & 78.3 \\
				Pyramid (BERT) \cite{jue2020pyramid} & 86.1 & 86.5 & 86.3 & 84.0 & 85.4 & 84.7 & 79.5 & 78.9 & 79.2 \\
\hline
				\multicolumn{8}{@{}l}{with Additional Supervision } \\
				DYGIE \cite{luan-etal-2019-general} &- &- & 84.7 &- & - & 82.9 & - & - & 76.2 \\ 
				\citet{yu-etal-2020-named}  & 87.3 & 86.0 & 86.7 & 85.2 & 85.6 & 85.4 & 81.8 & 79.3 & 80.5 \\
				BERT-MRC \cite{li-etal-2020-unified} & 85.0 &  86.3 & 86.0  & 87.2 & 86.6 & 86.9 & 85.2 & 81.1 & 83.8 \\ 
				\hline
				\multirow{2}{5cm}{\bf PO-TreeCRFs (ours)} & 86.7 & 86.5 & 86.6 &  84.5 	  &  86.4 & 85.4 & 78.2 & 78.2 & 78.2 \\
				& \small{0.4}  & \small{0.4}  & \small{0.3}  & \small{0.4}  & \small{0.2}  & \small{0.1}  & \small{0.7}  & \small{0.8}  & \small{0.1}   \\
				\hline
				\multicolumn{8}{@{}l}{\bf PO-TreeCRFs Ablation Study} \\	
Change Biaffine to Bilinear & 86.0 & 86.7 & 86.4 & 83.0 & 86.5 & 84.7 & 79.9 & 75.5 & 77.6   \\
				W/o. Structure Smoothing & 86.1 & 86.4 & 86.2 & 83.5 & 85.8 & 84.6 & 78.7 & 76.5 & 77.6 \\
				W/o. Potential Normalization and Structure Smoothing & 86.0 & 85.3 & 85.7 & 82.7 & 86.2 & 84.4 & 76.5 & 78.1 & 77.3 \\
				W/o. TreeCRFs & 84.4 & 85.4 & 84.9 & 82.0 & 86.4 & 84.1 & 80.5 & 74.5 & 77.4 \\ 
				\bottomrule
			\end{tabular}		
		\end{center}
		\caption{Main results and ablation studies on three datasets. We report the average scores of 5 runs for main results. } 
		\label{result}
	\end{table*} 
As an efficient alternative for Algorithm~\ref{alg:partial_summation}, the advantages of Algorithm~\ref{alg:masked_inside} is that it is (a) conceptually much simpler and (b) highly parallelizable.
The later allows us to fully exploit the computational power of modern hardware architectures (like GPUs) and highly optimized tensor operation libraries (like Pytorch).
We note that it the parallelization on  in Algorithm~\ref{alg:masked_inside} that reduces the complexity to at least , 
Furthermore, as an equivalent implementation to Algorithm~\ref{alg:masked_inside}, we can apply masks to scores in the logarithm scale\footnote{In our implementation, we use  to substitute the undefined  for numerical stability.}
then feed the masked scores to a normal Inside algorithm (rather than multiplying the masks inside Algorithm~\ref{alg:masked_inside}): 


In practice, we compute the masks  in the data-processing stage.
For training, by reusing existing implementations of the Inside algorithm in highly-optimized structured prediction libraries like Torch-Struct~\citep{Rush2020TorchStructDS},
we can \textit{implement the \textsc{Masked Inside} with one single line of code}, as is shown in equation~(\ref{eq:masked_inside}), thus significantly reducing the implementation complexity required by Algorithm~\ref{alg:partial_summation}. 





\subsection{Regularization} 
\label{ssec:regularization}
We propose two regularization techniques for TreeCRFs: (a)
\textit{potential normalization}, which is inspired by batch normalization \citep{Ioffe2015BatchNA}, and (b) \textit{structure smoothing}, which is inspired by label smoothing  \citep{Mller2019WhenDL}. 
Potential normalization (PN) is simple: we normalize the scores  to an empirical distribution of zero mean and one variance. 
The difference with batch-norm (BN) is that we apply PN at an instance-level, rather than a batch-level. 
In our experiments, we observe that PN gives a slightly better convergence. 

Structure smoothing regularizes TreeCRFs by putting a small portion of weights to nodes that are marginalized out. 
Specifically, during the partial marginalization, instead of using a zero mask that does not include the weights of rejected nodes, we change the mask to a small value 


This would effectively add the weights of all rejected nodes to  with a multiplier . 
This is similar to label smoothing which adds a small portion of weights to all labels other than the target label. 
The reason that we call it structure smoothing is that it not only smooths over the labels, but also smooths over different tree structures\footnote{Additionally, if we randomize  and make them i.i.d. Gumbel samples, we recover the Stochastic Softmax Trick~\citep{paulus2020gradient}, which generalize Gumbel-Softmax to combinatorial structures.}. 
We further observe that structure smoothing should be based on potential normalization for numerical stability, otherwise it does not converge in our experiments. 
The implementation of structure smoothing is still easy and aligns with previous discussions about equation~(\ref{eq:masked_inside})
as one only needs to change the zeros in  to . 





\subsection{Training and Inference} 
\label{ssec:training_inference}
During training,
we maximize the log conditional probability  efficiently computed by equation~(\ref{eq:masked_inside}) and~(\ref{eq:log_prob_efficient}).
During inference, we use CKY decoding to decode a full tree with the maximum probability. 
We only include nodes whose labels are in the observed label set , and dismiss nodes whose labels are in the latent set .
This would allow us to decode nested entities (partial trees) for evaluation. 



%
