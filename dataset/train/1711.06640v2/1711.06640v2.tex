We investigate scene graph parsing: the task of producing graph representations of real-world images that provide semantic summaries of objects and their relationships.
For example, the graph in Figure~\ref{fig:teaser} encodes the existence of key objects such as people (``man'' and ``woman''), their possessions (``helmet'' and ``motorcycle'', both possessed by the woman), and their activities (the woman is ``riding'' the ``motorcycle'').
Predicting such graph representations has been shown to improve natural language based image tasks~\cite{johnson_image_2015,Teney2016GraphStructuredRF,Yin2017Obj2TextGV} and has the potential to significantly expand the scope of applications for computer vision systems.
Compared to object detection ~\cite{ren_faster_2015, redmon_yolo9000:_2016} , object interactions ~\cite{yao2010modeling,chao:iccv2015} and activity recognition ~\cite{2014survey}, scene graph parsing poses unique challenges since it requires reasoning about the complex dependencies across all of these components.

\begin{figure}
    \centering
    \includegraphics[scale=.23]{teaser.pdf}
    \caption{A ground truth scene graph containing entities, such as \texttt{woman}, \texttt{bike} or \texttt{helmet}, that are localized in the image with bounding boxes, color coded above, and the relationships between those entities, such as \texttt{riding}, the relation between \texttt{woman} and \texttt{motorcycle} or \texttt{has} the relation between \texttt{man} and \texttt{shirt}.
    }
    \label{fig:teaser}
\end{figure}

Elements of visual scenes have strong structural regularities.
For instance, people tend to wear clothes, as can be seen in Figure~\ref{fig:teaser}.
We examine these structural repetitions, or \emph{motifs}, using the Visual Genome~\cite{visualgenome} dataset, which provides annotated scene graphs for 100k images from COCO~\cite{mscoco}, consisting of over 1M instances of objects and 600k relations.
Our analysis leads to two key findings.
First, there are strong regularities in the local graph structure such that
the distribution of the relations is highly skewed once the corresponding object categories are given, but not vice versa.
Second, structural patterns exist even in larger
subgraphs; we find that over half of images contain previously occurring graph motifs. 

Based on our analysis, we introduce a simple yet powerful baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set.
The baseline improves over prior state-of-the-art by 1.4 mean recall points (3.6\% relative), suggesting that an effective scene graph model must capture both the asymmetric dependence between objects and their relations, along with larger contextual patterns.

We introduce the \term{\modellong~(\model)}, a new neural network architecture
that complements existing approaches to scene graph parsing.
We posit that the key challenge in modeling scene graphs lies in devising an efficient mechanism to encode the global context that can directly inform the local predictors (i.e., objects and relations).
While previous work has used graph-based inference to propagate information in both directions between objects and relations~\cite{xu_scene_2017, li2017msdn, li_vip-cnn:_2017}, our analysis suggests strong independence assumptions in local predictors limit the quality of global predictions. Instead, our model predicts graph elements by staging bounding box predictions, object classifications, and relationships such that the global context encoding of all previous stages establishes rich context for predicting subsequent stages, as illustrated in Figure~\ref{fig:ourmodel}.
We represent the global context via recurrent sequential architectures such as Long Short-term Memory Networks (LSTMs) \cite{Hochreiter:1997:LSM:1246443.1246450}.

Our model builds on Faster-RCNN~\cite{ren_faster_2015}  for predicting bounding regions, fine tuned and adapted for Visual Genome.
Global context across bounding regions is computed and propagated through bidirectional LSTMs, which is then used by another LSTM that labels each bounding region conditioned on the overall context and all previous labels.
Another specialized layer of bidirectional LSTMs then computes and propagates information for predicting edges given bounding regions, their labels, and all other computed context.
Finally, we classify all  edges in the graph, combining globally contextualized representations of head, tail, and image representations using using low-rank outer products~\cite{Kim2016HadamardPF}.
The method can be trained end-to-end.











Experiments on Visual Genome demonstrate the efficacy of our approach. First, we update existing work by pretraining the detector on Visual Genome, setting a new state-of-the-art (improving on average across evaluation settings 14.0 absolute points).
Our new simple baseline improves over previous work, using our updated detector, by a mean improvement of 1.4 points.
Finally, experiments show {\modellong}s is effective at modeling global context, with a mean improvement of 2.9 points (7.1\% relative improvement) over our new strong baseline. 




















































