

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{gensymb}

\newcommand{\zpc}[1]{\textcolor{blue}{{[Zhaopeng: #1]}}}
\newcommand{\yz}[1]{\textcolor[rgb]{1,0.6,0.1}{{[Yinda: #1]}}}
\newcommand{\zc}[1]{\textcolor{black}{{#1}}}
\newcommand{\beforecaption}{\vspace{-1.2em}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\setlength{\textfloatsep}{6pt}



\usepackage[colorlinks=true,allcolors=black,urlcolor=blue,pagebackref=true,breaklinks=true,bookmarks=false]{hyperref}
\let\orgautoref\autoref
\providecommand{\Autoref}[1]{\def\figureautorefname{Figure}\orgautoref{#1}}
\renewcommand{\autoref}[1]{\def\figureautorefname{Fig.}\orgautoref{#1}}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}

\def\cvprPaperID{****} \def\confYear{CVPR 2021}



\begin{document}

\title{Holistic 3D Scene Understanding from a Single Image \\ with Implicit Representation }
\author{
    Cheng Zhang\ \ \ Zhaopeng Cui\ \ \ Yinda Zhang\thanks{Equal contribution}\ \ \ Bing Zeng\ \ \ Marc Pollefeys \ \ \ Shuaicheng Liu\thanks{Corresponding author}\\
 State Key Lab of CAD \& CG, Zhejiang University \\
     University of Electronic Science and Technology of China \quad
     Google \quad ETH Z\"{u}rich \\
}


\maketitle

\begin{abstract}
We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shapes, object poses, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. 
We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine the 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features.
A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.\blfootnote{Project webpage:
\href{https://chengzhag.github.io/publication/im3d/}{https://chengzhag.github.io/publication/im3d/}}
   
\end{abstract}

\section{Introduction}
3D indoor scene understanding is a long-lasting computer vision problem and has tremendous impact on several applications, e.g., robotics, virtual reality. 
Given a single color image, the goal is to reconstruct the room layout as well as each individual object and estimate its semantic type in the 3D space. 
Over decades, there are plenty of works consistently improving the performance of such a task over two focal points of the competition.
One is the \textbf{3D shape representation} preserving fine-grained geometry details, evolving from the 3D bounding box, 3D volume, point cloud, to the recent triangulation mesh.
The other is the joint inference of multiple objects and layout in the scene leveraging \textbf{contextual information}, such as co-occurring or relative locations among objects of multiple categories.
However, the cluttered scene is a double-blade sword, which unfortunately increases the complexity of 3D scene understanding by introducing large variations in object poses and scales, and heavy occlusion.
Therefore, the overall performance is still far from satisfactory.


\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=\textwidth]  
		{figure/scnrecon/ours/845_bbox}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=\textwidth]  
		{figure/scnrecon/ours/845_recon}
	\end{subfigure}
	\vspace{0.5em}
	\beforecaption
	\caption{Our proposed pipeline takes a single image as input, estimates layout and object poses, then reconstructs the scene with Signed Distance Function (SDF) representation.}
	\label{fig:teaser}
\end{figure}

In this work, we propose a deep learning system for holistic 3D scene understanding, which predicts and refines object shapes, object poses, and scene layout jointly with \textbf{deep implicit representation}.
At first, similar to previous methods, we exploit standard Convolutional Neural Networks (CNN) to learn an initial estimation of 3D object poses, scene layout as well as 3D shapes. 
Different from previous methods using explicit 3D representation like volume or mesh, we utilize the local structured implicit representation of shapes motivated by~\cite{genova2020local}. Instead of taking depth images as input like \cite{genova2020local}, we design a new local implicit shape embedding network to learn the latent shape code directly from images, which can be further decoded to generate the implicit function for 3D shapes. Due to the power of implicit representation, the 3D shape of each object can be reconstructed with higher accuracy and finer surface details compared to other representations.






Then, we propose a novel graph-based scene context network to gather information from local objects, i.e., bottom-up features extracted from the initial predictions, and learn to refine the initial 3D pose and scene layout via scene context information with the implicit representation.
Being one of the core topics studied in scene understanding, the context has been achieved in the era of deep learning mainly from two aspects - the model architecture and the loss function.
From the perspective of model design, we exploit the graph-based convolutional neural network (GCN) to learn context since it has shown competitive performance to learn context~\cite{yang2018graph}. With the deep implicit representation, the learned local shape latent vectors are naturally a compact and informative feature measuring of the object geometries, which results in more effective context models compared to features extracted from other representations such as mesh.


Not only the architecture, but the deep implicit representation also benefits the context learning on the loss function.
One of the most basic contextual information yet still missing in many previous works - objects should not intersect with each other, could be easily applied as supervision by penalizing the existence of 3D locations with negative predicted SDF in more than one objects\footnote{The object interior is with negative SDF, and thus no location should be inside of two objects.}.
We define this constraint as {a novel physical violation loss} and find it particularly helpful in preventing intersecting objects and producing reasonable object layouts.







Overall, our contributions are mainly in four aspects.
First, we design a two-stage single image-based holistic 3D scene understanding system that could predict object shapes, object poses, and scene layout with deep implicit representation, then optimize the later two.
Second, a new image-based local implicit shape embedding network is proposed to extract latent shape information which leads to superior geometry accuracy.
Third, we propose a novel GCN-based scene context network to refine the object arrangement which well exploits the latent and implicit features from the initial estimation. 
Last but not least, we design a physical violation loss, thanks to the implicit representation, to effectively prevent the object intersection.
Extensive experiments show that our model achieves the state-of-the-art performance on the standard benchmark.

\section{Related works}

\noindent\textbf{Single Image Scene Reconstruction.}
As a highly ill-posed problem, single image scene reconstruction sets a high bar for learning-based algorithms, especially in a cluttered scene with heavy occlusion. 
The problem can be divided into layout estimation, object detection and pose estimation, and 3D object reconstruction. 
A simple version of the first problem is to simplify the room layout as a bounding box \cite{hedau2009recovering, lee2009geometric, mallya2015learning, dasgupta2016delay, ren2016coarse}. 
To detect objects and estimate poses in 3D space, Recent works \cite{du2018learning, huang2018cooperative, chen2019holistic++} try to infer 3D bounding boxes from 2D detection by exploiting relationships among objects with a graph or physical simulation.
At the same time, other works \cite{izadinia2017im2cad, hueting2017seethrough, huang2018holistic} further extend the idea to align a CAD model with similar style to each detected object.
Still, the results are limited by the size of the CAD model database which results in an inaccurate representation of the scene.
To tackle the above limitations of previous works, Total3D \cite{nie2020total3dunderstanding} is proposed as an end-to-end solution to jointly estimate the layout box and object poses while reconstructing each object from the detection and utilizing the reconstruction to supervise the pose estimation learning.
However, they only exploit relationships among objects with features based on appearance and 2D geometry.

\noindent\textbf{Shape Representation.}
In the field of computer graphics, traditional shape representation methods include mesh, voxel, and point cloud. 
Some of the learning-based works try to encode the shape prior into a feature vector but stick to the traditional representations by decoding the vector into mesh \cite{groueix2018papier, wang2018pixel2mesh, pan2019deep, smith2019geometrics, gkioxari2019mesh}, voxel \cite{wu20153d, choy20163d, brock2016generative, wu2016learning, stutz2018learning} or point cloud \cite{lin2017learning, achlioptas2018learning, yang2019pointflow}. 
Others try to learn structured representations which decompose the shape into simple shapes \cite{li2017grass, gao2019sdm, paschalidou2019superquadrics}. 
Recently, implicit surface function \cite{mescheder2019occupancy, park2019deepsdf, xu2019disn, saito2019pifu, peng2020convolutional, saito2020pifuhd} has been widely used as a new representation method to overcome the disadvantages of traditional methods (i.e. unfriendly data structure to neural network of mesh and point cloud, low resolution and large memory consumption of voxel). 
Most recent works \cite{genova2019learning, genova2020local, wu2020pq} try to combine the structured and implicit representation which provides a physically meaningful feature vector while introducing significant improvement on the details of the decoded shape.

\noindent\textbf{Graph Convolutional Networks.}
Proposed by \cite{gori2005new}, graph neural networks or GCNs have been widely used to learn from graph-structured data. 
Inspired by convolutional neural networks, convolutional operation has been introduced to graph either on spectral domain \cite{bruna2013spectral, defferrard2016convolutional, kipf2016semi} or non-spectral domain \cite{hamilton2017inductive} which performs convolution with a message passing neural network to gather information from the neighboring nodes. 
Attention mechanism has also been introduced to GCN and has been proved to be efficient on tasks like node classification \cite{velivckovic2017graph}, scene graph generation \cite{yang2018graph} and feature matching \cite{sarlin2020superglue}. 
Recently, GCN has been even used on super-resolution \cite{zhou2020cross} which is usually the territory of CNN. In the 3D world which interests us most, GCN has been used on classification \cite{wang2019dynamic} and segmentation \cite{te2018rgcnn, wang2019graph, wang2019dynamic} on point cloud, which is usually an enemy representation to traditional neural networks. 
The most related application scenario of GCN with us is 3D object detection on points cloud. 
Recent work shows the ability of GCN to predict relationship \cite{avetisyan2020scenecad} or 3D object detections \cite{najibi2020dops} from point cloud data. 





\begin{figure*}
	\centering
	\vspace{-1.5em}
	\includegraphics[width=\textwidth]  
		{figure/pipeline}
	\vspace{-0.8em}
	\beforecaption
	\caption{Our proposed pipeline. We initialize the layout estimation and 3D object poses with LEN and ODN from prior work \cite{nie2020total3dunderstanding}, then refine them with Scene Graph Convolutional Network (SGCN). We utilize a Local Implicit Embedding Network (LIEN) to encode latent code for LDIF decoder \cite{genova2020local} and to extract implicit features for SGCN. With the help of LDIF and marching cube algorithm, object meshes are extracted then rotated, scaled, and put into places to construct the scene.}
	\vspace{-1em}
	\label{fig:pipeline}
\end{figure*}

\section{Our method}

As shown in \autoref{fig:pipeline}, the proposed system consists of two stages, i.e., the initial estimation stage, and the refinement stage. In the initial estimation stage, similar to \cite{huang2018cooperative, nie2020total3dunderstanding}, a 2D detector is first adopted to extract the 2D bounding box from the input image, followed by an Object Detection Network (ODN) to recover the object poses as 3D bounding boxes and a new Local Implicit Embedding Network (LIEN) to extract the implicit local shape information from the image directly, which can further be decoded to infer 3D geometry.
The input image is also fed into a Layout Estimation Network (LEN) to produce a 3D layout bounding box and relative camera pose.
In the refinement stage, a novel Scene Graph Convolutional Network (SGCN) is designed to refine the initial predictions via the scene context information.
As 2D detector, LEN, ODN has the standard architecture similar to prior works \cite{huang2018cooperative, nie2020total3dunderstanding}, in this section, we will describe the details of the novel SGCN and LIEN in detail. Please refer to our supplementary materials for the details of our 2D detector, LEN, ODN.






\subsection{Scene Graph Convolutional Network}

As shown in \autoref{fig:pipeline}, motivated by Graph R-CNN \cite{yang2018graph}, we model the whole 3D scene as a graph , in which the nodes represent the objects, the scene layout, and their relationships.
The graph is constructed starting from a complete graph with undirected edges between all objects and layout nodes, which allows information to flow among objects and the scene layout. Then, we add relation nodes to each pair of neighboring object/layout nodes.
Considering the nature of directional relation \cite{krishna2017visual}, we add two relation nodes between each pair of neighbors in different directions. 




It is well known that the input features are the key to an effective GCN \cite{wang2018pixel2mesh}.
For different types of nodes, we design features carefully from different sources as follows.
For each node, features from different sources are flattened and concatenated into a vector, then embedded into a node representation vector with the same length using MLP. 

\noindent\textbf{Layout Node.} 
We use the feature from the image encoder of LEN, which encodes the appearance of layout, and the parameterized output of layout bounding box and camera pose from LEN, as layout node features.
We also concatenate the camera intrinsic parameters normalized by the image height into the feature to add camera priors. 

\noindent\textbf{Object Node.} 
We collect the appearance-relationship feature \cite{nie2020total3dunderstanding} from ODN, and the parameterized output of object bounding box from ODN, along with the element centers in the world coordinate and analytic code from LIEN (which we will further describe in the next section). 
We also use the one-hot category label from the 2D detector to introduce semantic information to SGCN.

\noindent\textbf{Relationship Node.} 
For nodes connecting two different objects, the geometry feature \cite{hu2018relation, vaswani2017attention} of 2D object bounding boxes and the box corner coordinates of both connected objects normalized by the image height and width are used as features. The coordinates are flattened and concatenated in the order of source-destination, which differentiate the relationships of different directions.
For nodes connecting objects and layouts, since the relationship is presumably different from object-object relationship, we initialize the representations with constant values, leaving the job of inferring reasonable relationship representation to SGCN.

 
For a graph with  objects and  layout, object-layout nodes and relationship nodes can then be put into two matrices  and .

Since the graph is modeled with different types of nodes, which makes a difference in the information needed from different sources to destinations, we define independent message passing weights for each of the source-destination types.
We denote the linear transformation and the adjacent matrix from source node to destination node with type  and  as  and , in which node types can be source object (or layout) , destination object (or layout) , and relationships . 
Thus, 
the representation of object and layout nodes can be updated as

and the relationship node representations can be updated as

After four steps of message passing, independent MLPs are used to decode object node representations into residuals for corresponding object bounding box parameters , and layout node representation into residuals for initial layout box  and camera pose . Please refer to our supplementary or
\cite{nie2020total3dunderstanding} for the details of the definition.
The shape codes can be also refined in the scene graph, while we find that it doesn't improve empirically as much as for the layout and object poses in our pipeline because our local implicit embedding network, which will be introduced in the following, is powerful enough to learn accurate shapes.


\subsection{Local Implicit Embedding Network}
\label{sec:lien}
With a graph constructed for each scene, we naturally ask what features help SGCN effectively capture contextual information among objects. 
Intuitively, we expect features that well describe 3D object geometry and their relationship in 3D space. 
Motivated by Genova \etal \cite{genova2020local}, we propose to utilize the local deep implicit representation as the features embedding object shapes due to its superior performance for single object reconstruction.
In their model, 
the function is a combination of 32 3D elements (16 with symmetry constraints), with each element described with 10 Gaussian function parameters analytic code and 32-dim latent variables (latent code). The Gaussian parameters describe the scale constant, center point, radii, and Euler angle of every Gaussian function, which contains structured information of the 3D geometry.
We use analytic code as a feature for object nodes in SGCN, which should provide information on the local object structure.
Furthermore, since the centers of the Gaussian functions presumably correspond to centers of different parts of an object, we also transform them from the object coordinate system to the world coordinate system as a feature for every object node in SGCN.
The transformation provides global information about the scene, which makes SGCN easier to infer relationships between objects.
The above two features make up the implicit features of LIEN.

As LDIF \cite{genova2020local} is designed for 3D object reconstruction from one or multiple depth images, we design a new image-based Local Implicit Embedding Network (LIEN) to learn the 3D latent shape representation from the image which is obviously a more challenging problem.
Our LIEN consists of a Resnet-18 as image encoder, along with a three-layer MLP to get the analytic and latent code. 
Additionally, in order to learn the latent features effectively, we concatenate the category code with the image feature from the encoder to introduce shape priors to the LIEN, which improves the performance greatly. Please refer to our supplementary material for the detailed architecture of the proposed LIEN. 

\subsection{Loss Function}

\noindent\textbf{Losses for Initialization Modules.} 
When training LIEN along with LDIF decoder individually, we follow \cite{genova2020local} to use the shape element center loss  with weight  and point sample loss,

where  and  evaluates  losses for near-surface samples and uniformly sampled points.
When training LEN and ODN, we follow \cite{huang2018cooperative,nie2020total3dunderstanding} to use classification and regression loss for every output parameter of LEN and ODN,




\noindent\textbf{Joint Refinement with Object Physical Violation Loss.} 
For the refinement stage, we aim to optimize the scene layout and object poses using the scene context information by minimizing the following loss function,

Besides ,  and cooperative loss  \cite{nie2020total3dunderstanding}, we propose a novel physical violation loss as a part of joint loss for the scene graph convolutional network to make sure that objects will not intersect with each other.
The neural SDF representation used by local implicit representation gives us a convenient way to propagate gradient from undesired geometry intersection back to the object pose estimation. To achieve this,
we first sample points inside objects.
For each object , we randomly sample points inside the bounding box of each object, along with the center points of Gaussian elements as point candidates.
We then queue these candidates into LDIF decoder of the object and filter out points outside object surfaces to get inside point samples . 
Finally, we queue  into the LDIF decoder of the k-nearest objects  to verify if they have intersection with other objects (if the predicted label is "inside"). We follow \cite{genova2020local} to compute a  loss between the predicted labels of intersected points with the ground truth surface label (where we use , ,  for "outside", "inside", "surface" labels). The object physical violation loss can be defined as:

where  is the LDIF for object  to decode a world coordinate point  into LDIF value.
A sigmoid is applied on the LDIF value (scaled by ) to get the predicted labels, and a ReLU is applied to consider only the intersected points.
As shown in \autoref{fig:lphy}, the loss punishes intersected sample points thus push both objects away from each other to prevent intersections.


\begin{figure}[t]
	\centering
	\vspace{-1em}
	\includegraphics[width=0.4\textwidth]
		{figure/Lphy}
	\vspace{0.5em}
	\beforecaption
	\caption{Object physical violation loss. Based on the insight that objects should not intersect, we punish points inside neighboring objects (demonstrated as , which has negative LDIF values in both object A and object B). With error back-propagated through the LDIF decoder, intersected objects should be pushed back from each other, reducing intersection resulting from bad object pose estimation.}
	\label{fig:lphy}
\end{figure}

\section{Experiments}
In this section, we compare our method with state-of-the-art 3D scene understanding methods in various aspects and provide an ablation study to highlight the effectiveness of major components. 

\subsection{Experiment Setup}

\begin{figure}[!ht]
	\centering
	\vspace{-1em}
	\begin{subfigure}[t]{0.1\textwidth}
\includegraphics[width=\textwidth]
		{figure/objrecon/input/bookcase_IKEA_EXPEDIT_3_0131}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/input/bookcase_IKEA_HEMNES_0182}
		\includegraphics[width=\textwidth]
		{figure/objrecon/input/chair_IKEA_STEFAN_3403}
\includegraphics[width=\textwidth]  
		{figure/objrecon/input/desk_IKEA_HEMNES_0234}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/input/sofa_IKEA_EKTORP_2_0003}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/input/table_IKEA_NESNA_1291}
		\vspace{-6mm}
		\caption{Input}
	\end{subfigure}
		\begin{subfigure}[t]{0.1\textwidth}
\includegraphics[width=\textwidth]
		{figure/objrecon/AtlasNet/bookcase_IKEA_EXPEDIT_3_0131}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/AtlasNet/bookcase_IKEA_HEMNES_0182}
		\includegraphics[width=\textwidth]
		{figure/objrecon/AtlasNet/chair_IKEA_STEFAN_3403}
\includegraphics[width=\textwidth]  
		{figure/objrecon/AtlasNet/desk_IKEA_HEMNES_0234}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/AtlasNet/sofa_IKEA_EKTORP_2_0003}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/AtlasNet/table_IKEA_NESNA_1291}
		\vspace{-6mm}
		\caption{AtlasNet}
		\label{fig:objrecon_atalasnet}
	\end{subfigure}
	\begin{subfigure}[t]{0.1\textwidth}
\includegraphics[width=\textwidth]
		{figure/objrecon/MGN/bookcase_IKEA_EXPEDIT_3_0131}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/MGN/bookcase_IKEA_HEMNES_0182}
		\includegraphics[width=\textwidth]
		{figure/objrecon/MGN/chair_IKEA_STEFAN_3403}
\includegraphics[width=\textwidth]  
		{figure/objrecon/MGN/desk_IKEA_HEMNES_0234}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/MGN/sofa_IKEA_EKTORP_2_0003}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/MGN/table_IKEA_NESNA_1291}
		\vspace{-6mm}
		\caption{MGN}
		\label{fig:objrecon_mgn}
	\end{subfigure}
	\begin{subfigure}[t]{0.1\textwidth}
\includegraphics[width=\textwidth]
		{figure/objrecon/ours/bookcase_IKEA_EXPEDIT_3_0131}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/ours/bookcase_IKEA_HEMNES_0182}
		\includegraphics[width=\textwidth]
		{figure/objrecon/ours/chair_IKEA_STEFAN_3403}
\includegraphics[width=\textwidth]  
		{figure/objrecon/ours/desk_IKEA_HEMNES_0234}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/ours/sofa_IKEA_EKTORP_2_0003}
		\includegraphics[width=\textwidth]  
		{figure/objrecon/ours/table_IKEA_NESNA_1291}
		\vspace{-6mm}
		\caption{Ours}
		\label{fig:objrecon_ours}
	\end{subfigure}
	\vspace{0.5em}
	\beforecaption
	\caption{Object reconstruction qualitative comparison. We use the implementation from \cite{nie2020total3dunderstanding} for AtlasNet \cite{groueix2018}. Our results contain finer details and have more smooth surfaces.}
	\vspace{-0.2em}
	\label{fig:objrecon}
\end{figure}

\begin{table*}[!h]
    \vspace{-1.5em}
	\begin{center}
		\begin{tabular}{|l|c c c c c c c c c|c|}
			\hline
			Category & bed & bookcase & chair & desk & sofa & table & tool & wardrobe & misc & mean\\
			\hline\hline
			AtlasNet \cite{groueix2018} & 9.03 & 6.91 & 8.37 & 8.59 & 6.24 & 19.46 & 6.95 & 4.78 & 40.05 & 12.26\\
			TMN \cite{pan2019deep} & 7.78 & 5.93 & 6.86 & 7.08 & 4.25 & 17.42 & 4.13 & 4.09 & \textbf{23.68} & 9.03\\
			MGN \cite{nie2020total3dunderstanding} & 5.99 & 6.56 & \textbf{5.32} & \textbf{5.93} & \textbf{3.36} & 14.19 & 3.12 & \textbf{3.83} & 26.93 & 8.36\\
			Ours & \textbf{4.11} & \textbf{3.96} & 5.45 & 7.85 & 5.61 & \textbf{11.73} & \textbf{2.39} & 4.31 & 24.65 & \textbf{6.72}\\
			\hline
		\end{tabular}
	\end{center}
	\vspace{-0.5em}
	\beforecaption
	\caption{Object reconstruction comparison. We report the Chamfer distance scaled with the factor of . We follow \cite{nie2020total3dunderstanding} to align the reconstructed mesh to ground-truth with ICP then sample 10K points from the output and the ground-truth meshes. Although trained on watertight meshes with more noise, our results still shows better results.}
	\vspace{-0.5em}
	\label{tbl:objrecon}
\end{table*}

\begin{table*}[!h]
    \vspace{-0.5em}
	\begin{center}
		\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			Method & bed & chair & sofa & table & desk & dresser & nightstand & sink & cabinet & lamp & mAP\\
			\hline\hline
			3DGP \cite{choi2013understanding} & 5.62 & 2.31 & 3.24 & 1.23 & - & - & - & - & - & - & - \\
			HoPR \cite{huang2018holistic} & 58.29 & 13.56 & 28.37 & 12.12 & 4.79 & 13.71 & 8.80 & 2.18 & 0.48 & 2.41 & 14.47 \\
			CooP \cite{huang2018cooperative} & 57.71 & 15.21 & 36.67 & 31.16 & 19.90 & 15.98 & 11.36 & 15.95 & 10.47 & 3.28 & 21.77\\
			Total3D \cite{nie2020total3dunderstanding} & 60.65 & 17.55 & 44.90 & 36.48 & 27.93 & 21.19 & 17.01 & 18.50 & 14.51& 5.04& 26.38\\
			Ours & \textbf{89.32} & \textbf{35.14} & \textbf{69.10} & \textbf{57.37} & \textbf{49.03} & \textbf{29.27} & \textbf{41.34} & \textbf{33.81} & \textbf{33.93} & \textbf{11.90} & \textbf{45.21}\\
			\hline
		\end{tabular}
	\end{center}
	\vspace{-0.5em}
	\beforecaption
	\caption{3D object detection comparison. For CooP, we report the better results from \cite{nie2020total3dunderstanding} trained on NYU-37 object labels. Our method outperforms SOTA, benefiting from a better understanding of the object relationships and the scene context.}
	\vspace{-1em}
	\label{tbl:3ddetection}
\end{table*}

\noindent \textbf{Datasets.} 
We follow \cite{nie2020total3dunderstanding} to use two datasets to train each module individually and jointly. We use two datasets for training and evaluation.
1) \textbf{Pix3D} dataset \cite{sun2018pix3d} is presented as a benchmark for shape-related tasks including reconstruction, providing 9 categories of 395 furniture models and 10,069 images with precise alignment.
We use the mesh fusion pipeline from Occupancy Network \cite{mescheder2019occupancy} to get watertight meshes for LIEN training and evaluate LIEN on original meshes.
2) \textbf{SUN RGB-D} dataset \cite{song2015sun} contains 10K RGB-D indoor images captured by four different sensors and is densely annotated with 2D segmentation, semantic labels, 3D room layout, and 3D bounding boxes with object orientations.
Follow Total3D \cite{nie2020total3dunderstanding}, we use the train/test split from \cite{gkioxari2019mesh} on the Pix3D dataset and the official train/test split on the SUN RGB-D dataset. 
The object labels are mapped from NYU-37 to Pix3D as presented by \cite{nie2020total3dunderstanding}.

\noindent \textbf{Metrics.} 
We adopt the same evaluation metrics with \cite{huang2018cooperative,nie2020total3dunderstanding}, including average 3D Intersection over Union (IoU) for layout estimation; mean absolute error for camera pose; average precision (AP) for object detection; and chamfer distance for single-object mesh generation from single image. 

\noindent \textbf{Implementation.} 
We use the outputs of the 2D detector from Total3D as the input of our model. 
We also adopted the same structure of ODN and LEN from Total3D. 
LIEN is trained with LDIF decoder on Pix3D with watertight mesh, using Adam optimizer with a batch size of 24 and learning rate decaying from 2e-4 (scaled by 0.5 if the test loss stops decreasing for 50 epochs, 400 epochs in total) and evaluated on the original non-watertight mesh. 
SGCN is trained on SUN RGB-D, using Adam optimizer with a batch size of 2 and learning rate decaying from 1e-4 (scaled by 0.5 every 5 epochs after epoch 18, 30 epochs in total). 
We follow \cite{nie2020total3dunderstanding} to train each module individually then jointly.
When training SGCN individually, we use  without , and put it into the full model with pre-trained weights of other modules.
In joint training, we adopt the observation from \cite{nie2020total3dunderstanding} that object reconstruction depends on clean mesh for supervision, to fix the weights of LIEN and LDIF decoder.




\subsection{Comparison to State-of-the-art}
\label{sec:comp}
In this section, we compare to the state-of-the-art methods for holistic scene understand from aspects including object reconstruction, 3D object detection, layout estimation, camera pose prediction, and scene mesh reconstruction.





\noindent \textbf{3D Object Reconstruction.} 
We first compare the performance of LIEN with previous methods, including AtlasNet \cite{groueix2018}, TMN \cite{pan2019deep}, and Total3D \cite{nie2020total3dunderstanding}, for the accuracy of the predicted geometry on Pix3D dataset.
All the methods take as input a crop of image of the object and produce 3D geometry.
To make a fair comparison, the one-hot object category code is also concatenated with the appearance feature for AtlasNet \cite{groueix2018} and TMN \cite{pan2019deep}.
For our method, we run a marching cube algorithm on 256 resolution to reconstruct the mesh.
The quantitative comparison is shown in \autoref{tbl:objrecon}.
Our method produces the most accurate 3D shape compared to other methods, yielding the lowest mean Chamfer Distance across all categories.
Qualitative results are shown in \autoref{fig:objrecon}.
AtlasNet produces results in limited topology and thus generates many undesired surfaces.
MGN mitigates the issue with the capability of topology modification, which improves the results but still leaves obvious artifacts and unsmooth surface due to the limited representation capacity of the triangular mesh.
In contrast, our method produces 3D shape with correct topology, smooth surface, and fine-grained details, which clearly shows the advantage of the deep implicit representation.







\begin{table}[t]
    \vspace{-0.25em}
	\begin{center}
	    \resizebox{0.8\columnwidth}{!}{
    		\begin{tabular}{|l|c|c|c|}
    			\hline
    			Method & Layout IoU & Cam pitch & Cam roll \\
    			\hline\hline
    			3DGP \cite{choi2013understanding} & 19.2 & - & - \\
    			Hedau \cite{hedau2009recovering} & - & 33.85 & 3.45\\
    			HoPR \cite{huang2018holistic} & 54.9 & 7.60 & 3.12 \\
    			CooP \cite{huang2018cooperative} & 56.9 & 3.28 & 2.19\\
    			Total3D \cite{nie2020total3dunderstanding} & 59.2 & 3.15 & \textbf{2.09}\\
    			Ours & \textbf{64.4} & \textbf{2.98} & 2.11 \\
    			\hline
    		\end{tabular}
    	}
	\end{center}
	\vspace{-0.5em}
	\beforecaption
	\caption{3D layout and camera pose estimation comparison. Our method outperforms SOTA by 5.2\% in layout estimation while on par with SOTA on camera pose estimation.}
	\label{tbl:layout}
\end{table}

\begin{figure*}[!ht]
	\centering
	\scriptsize
	\newcommand{\rgb}[1]{\raisebox{-0.5\height}{\includegraphics[width=.15\linewidth]{#1}}}
	\newcommand{\oblique}[1]{\raisebox{-0.5\height}{\includegraphics[width=.15\linewidth,clip,trim=48 12 48 12]{#1}}}
	\newcommand{\rot}[1]{\rotatebox[origin=c]{90}{#1}}
	\def\arraystretch{0.5}\begin{tabular}{c|c*{6}{c@{\hspace{1px}}}}
    	\multicolumn{8}{l}{\hspace{34px}\includegraphics[width=.91\linewidth]{figure/scnrecon/legend}\vspace{5px}} \\
        \rule{0pt}{5px}&\rot{Input}& 
    	    \rgb{figure/scnrecon/input/000724} &
    	    \rgb{figure/scnrecon/input/000276} &
    	    \rgb{figure/scnrecon/input/001140} &
    	    \rgb{figure/scnrecon/input/000765} &
\rgb{figure/scnrecon/input/001109} &
\rgb{figure/scnrecon/input/001149} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \oblique{figure/scnrecon/Total3D/724_oblique_3d} &
        	    \oblique{figure/scnrecon/Total3D/276_oblique_3d} &
        	    \oblique{figure/scnrecon/Total3D/1140_oblique_3d} &
        	    \oblique{figure/scnrecon/Total3D/765_oblique_3d} &
\oblique{figure/scnrecon/Total3D/1109_oblique_3d} &
\oblique{figure/scnrecon/Total3D/1149_oblique_3d} \\
        	\rot{Oblique View}& \rot{Ours}& 
        	    \oblique{figure/scnrecon/ours/724_oblique_3d} &
        	    \oblique{figure/scnrecon/ours/276_oblique_3d} &
        	    \oblique{figure/scnrecon/ours/1140_oblique_3d} &
        	    \oblique{figure/scnrecon/ours/765_oblique_3d} &
\oblique{figure/scnrecon/ours/1109_oblique_3d} &
\oblique{figure/scnrecon/ours/1149_oblique_3d} \\
        	&\rot{GT}& 
        	    \oblique{figure/scnrecon/gt/724_oblique_3d} &
        	    \oblique{figure/scnrecon/gt/276_oblique_3d} &
        	    \oblique{figure/scnrecon/gt/1140_oblique_3d} &
        	    \oblique{figure/scnrecon/gt/765_oblique_3d} &
\oblique{figure/scnrecon/gt/1109_oblique_3d} &
\oblique{figure/scnrecon/gt/1149_oblique_3d} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \rgb{figure/scnrecon/Total3D/724_bbox} &
        	    \rgb{figure/scnrecon/Total3D/276_bbox} &
        	    \rgb{figure/scnrecon/Total3D/1140_bbox} &
        	    \rgb{figure/scnrecon/Total3D/765_bbox} &
\rgb{figure/scnrecon/Total3D/1109_bbox} &
\rgb{figure/scnrecon/Total3D/1149_bbox} \\
            \rot{Camera View}&\rot{Ours}& 
        	    \rgb{figure/scnrecon/ours/724_bbox} &
        	    \rgb{figure/scnrecon/ours/276_bbox} &
        	    \rgb{figure/scnrecon/ours/1140_bbox} &
        	    \rgb{figure/scnrecon/ours/765_bbox} &
\rgb{figure/scnrecon/ours/1109_bbox} &
\rgb{figure/scnrecon/ours/1149_bbox} \\
        	&\rot{GT}& 
        	    \rgb{figure/scnrecon/gt/724_bbox} &
        	    \rgb{figure/scnrecon/gt/276_bbox} &
        	    \rgb{figure/scnrecon/gt/1140_bbox} &
        	    \rgb{figure/scnrecon/gt/765_bbox} &
\rgb{figure/scnrecon/gt/1109_bbox} &
\rgb{figure/scnrecon/gt/1149_bbox} \\
    	\hline
    	\multirow{2}{*}{\rot{Scene Reconstruction}}
        	&\rot{Total3D}& 
        	    \rgb{figure/scnrecon/Total3D/724_recon} &
        	    \rgb{figure/scnrecon/Total3D/276_recon} &
        	    \rgb{figure/scnrecon/Total3D/1140_recon} &
        	    \rgb{figure/scnrecon/Total3D/765_recon} &
\rgb{figure/scnrecon/Total3D/1109_recon} &
\rgb{figure/scnrecon/Total3D/1149_recon} \\
        	&\rot{Ours}& 
        	    \rgb{figure/scnrecon/ours/724_recon} &
        	    \rgb{figure/scnrecon/ours/276_recon} &
        	    \rgb{figure/scnrecon/ours/1140_recon} &
        	    \rgb{figure/scnrecon/ours/765_recon} &
\rgb{figure/scnrecon/ours/1109_recon} &
\rgb{figure/scnrecon/ours/1149_recon} \\
    \end{tabular}
\caption{Qualitative comparison on object detection and scene reconstruction. We compare object detection results with Total3D \cite{nie2020total3dunderstanding} and ground truth in both oblique view and camera view. The results show that our method gives more accurate bounding box estimation and with less intersection. We compare scene reconstruction results with Total3D in camera view and observe more reasonable object poses.}
	\label{fig:scnrecon}
\end{figure*}

\noindent \textbf{3D Object Detection.} 
We then evaluate the 3D object detection performance of our model.
Follow \cite{nie2020total3dunderstanding,huang2018cooperative}, we use mean average precision (mAP) with the threshold of 3D bounding box IoU set at 0.15 as the evaluation metric.
The quantitative comparison to state-of-the-art methods \cite{choi2013understanding,huang2018holistic,huang2018cooperative,nie2020total3dunderstanding} is shown in \autoref{tbl:3ddetection}.
Our method performs consistently the best over all semantic categories and significantly outperforms the state-of-the-art (i.e. improving AP by 18.83\%).
\Autoref{fig:scnrecon} shows some qualitative comparison.
Note how our method produces object layout not only more accurate but also in reasonable context compared to Total3D, e.g. objects are parallel to wall direction.




\noindent \textbf{Layout Estimation.} 
We also compare the 3D room layout estimation with Total3D \cite{nie2020total3dunderstanding} and other state-of-the-arts \cite{choi2013understanding,huang2018holistic,huang2018cooperative}.
The quantitative evaluation is shown in \autoref{tbl:layout} (Layout IoU).
Overall, our method outperforms all the baseline methods.
This indicates that the GCN is effective in measuring the relation between layout and objects and thus benefits the layout prediction.



\noindent \textbf{Camera Pose Estimation.} 
\autoref{tbl:layout} also shows the comparison over camera pose prediction, following the evaluation protocol of Total3D.
Our method achieves 5\% better camera pitch and slightly worse camera roll.







\noindent \textbf{Holistic Scene Reconstruction.}
To our best knowledge, Total3D \cite{nie2020total3dunderstanding} is the only work achieving holistic scene reconstruction from a single RGB, and thus we compare to it.
Since no ground truth is presented in SUN RGB-D dataset, we mainly show qualitative comparison in \autoref{fig:scnrecon}. 
Compared to Total3D, our model has less intersection and estimates more reasonable object layout and direction.
We consider this as a benefit from a better understanding of scene context by GCN. Our proposed physical violation loss  also contributes to less intersection.



\subsection{Ablation Study}

In this section, we verify the effectiveness of the proposed components for holistic scene understanding. 
As shown in \autoref{tbl:ablation}, we disable certain components and evaluate the model for 3D layout estimation and 3D object detection, We do not evaluate the 3D object reconstruction since it is highly related to the usage of deep implicit representation, which has been already evaluated in \autoref{sec:comp}.

\noindent \textbf{Does GCN Matter?}
To show the effectiveness of GCN, we first attach the GCN to the original Total3D to improve the object and scene layout (\autoref{tbl:ablation}, Total3D+GCN).
For the difference between MGN of Total3D and LIEN of ours, we replace deep implicit features with the feature from image encoder of MGN and use their proposed partial Chamfer loss  instead of .
Both object bounding box and scene layout are improved.
We also train a version of our model without the GCN (Ours-GCN), and the performance drops significantly.
Both experiments show that GCN is effective in capturing scene context.

\noindent \textbf{Does Deep Implicit Feature Matter?}
As introduced in \autoref{sec:lien}, the LDIF representation provides informative node features for the GCN.
Here we demonstrate the contribution from each component of the latent representation.
Particularly, we remove either element centers or analytic code from the GCN node feature (Ours-element, Ours-analytic), and find both hurts the performance.
This indicates that the complete latent representation is helpful in pursuing better scene understanding performance.

\noindent \textbf{Does Physical Violation Loss Matter?}
\zc{
Additionally, 
we evaluate the effectiveness of the physical violation loss.
We train our model without it (Ours-), and also observe performance drop 
for both scene layout and object 3D bounding box 
in \autoref{tbl:ablation}.
We refer to supplementary material for qualitative comparison.
}

\noindent \textbf{Evaluating on Other Metrics.}
\zc{
We also test our method in other aspects including supporting relation, geometry accuracy, and room layout as shown in \autoref{tbl:more_ablation}.
1) We calculate the mean distance between the predicted bottom of on-floor objects and the ground truth floor to measure the supporting relationship.
As ground truth, an object is considered to be on-floor if its bottom surface is within 15cm to the floor.
While GCN significantly improves the metric,  slightly hurts possibly because it tends to push objects away.
Further qualitative results are shown in the supplementary material.
Besides, we also measure the average volume of the collision per scene between objects (Coll Vol), and our full model effectively prevent collision.
2) We follow Total3D \cite{nie2020total3dunderstanding} to evaluate the alignment between scene reconstruction and ground truth depth map with global loss , and our full model performs the best.
3) We also project the predicted layout onto the image and evaluate with image based metrics \cite{dasgupta2016delay, ren2016coarse}.
Our full model achieves the best on both corner and pixel errors.
Overall, the GCN and  benefit on all these aspects.
}


\begin{table}[!t]
    \vspace{-0.2em}
	\begin{center}
	    \resizebox{0.8\columnwidth}{!}{
    		\begin{tabular}{|l|c|c|c|}
    			\hline
    			Setting & Layout IoU  & Detection mAP \\
    			\hline\hline
    			Total3D & 59.25 & 26.38\\
    			Total3D+GCN & 62.49 & 37.04 \\
    			\hline
    			Ours-GCN & 60.04 & 27.47 \\
    			Ours-element & 64.22 & 42.05 \\
    			Ours-analytic & 63.76 & 43.10 \\
    			Ours- & 63.52 & 43.33 \\
    			\textbf{Full} & \textbf{64.41} & \textbf{45.21}\\
    			\hline
    		\end{tabular}
    	}
	\end{center}
	\vspace{-0.7em}
	\beforecaption
	\caption{Ablation study. We evaluate layout estimation with layout IoU and 3D object detection with mAP.}
	\label{tbl:ablation}
\end{table}

\begin{table}[!t]
    \vspace{-1em}
	\begin{center}
    	\resizebox{1\columnwidth}{!}{
    		\begin{tabular}{|l|c|c|c|c|c|}
    			\hline
    			Setting & \makecell[c]{Sup Err \\ } &  & \makecell[c]{Coll Vol \\ } & \makecell[c]{Corner Err \\ (\%)} & \makecell[c]{Pixel Err \\ (\%)} \\
    			\hline\hline
    			Total3D & 26.72 & 1.43 & - & 13.29 & 20.51\\
\hline
Ours-GCN & 24.18 & 1.41 & 16.64 & 13.17 & 20.05 \\
    			Ours- & \textbf{13.35} & 1.14 & 13.65 & 11.60 & 17.91 \\
    			\textbf{Full} & 14.71 & \textbf{1.11} & \textbf{13.55} & \textbf{11.45} & \textbf{17.60}\\
    			\hline
    		\end{tabular}
    	}
	\end{center}
	\vspace{-0.7em}
	\beforecaption
	\caption{\zc{Ablation study on other metrics. We compare on supporting error,  (in units of ), average collision volume, corner error, and pixel error.}}
	\label{tbl:more_ablation}
\end{table}

\begin{figure}[!t]
	\centering
    \vspace{-1em}
	\begin{subfigure}[t]{0.15\textwidth}
		\includegraphics[width=\textwidth]  
		{figure/generalizeRebuttal/n02818832_589/n02818832_589}
		\includegraphics[width=\textwidth]  
		{figure/generalizeRebuttal/IMG_0991/IMG_0991}
		\vspace{-6mm}
		\caption{Input}
	\end{subfigure}
	\begin{subfigure}[t]{0.15\textwidth}
		\includegraphics[width=\textwidth]  
		{figure/generalizeRebuttal/n02818832_589/3dbbox}
		\includegraphics[width=\textwidth]  
		{figure/generalizeRebuttal/IMG_0991/3dbbox}
		\vspace{-6mm}
		\caption{3D Detection}
	\end{subfigure}
	\begin{subfigure}[t]{0.15\textwidth}
		\includegraphics[width=\textwidth]  
		{figure/generalizeRebuttal/n02818832_589/recon}
		\includegraphics[width=\textwidth]  
		{figure/generalizeRebuttal/IMG_0991/recon}
		\vspace{-6mm}
		\caption{Reconstruction}
	\end{subfigure}
	\vspace{0.2em}
	\beforecaption
	\caption{Qualitative results on ObjectNet3D dataset \cite{xiang2016objectnet3d} (row 1) and the layout estimation dataset in \cite{hedau2009recovering} (row 2).}
\label{fig:rebuttal_genralize}
\end{figure}


\subsection{Generalization to other datasets}

\zc{
We also show qualitative results of our method tested on the 3D detection dataset ObjectNet3D \cite{xiang2016objectnet3d} and the layout estimation dataset in \cite{hedau2009recovering} without fine-tuning in \autoref{fig:rebuttal_genralize}.
Our method shows good generalization capability and performs reasonably well on these unseen datasets.
}


\section{Conclusion}
We have presented a deep learning model for holistic scene understanding by leveraging deep implicit representation.
Our model not only reconstructs accurate 3D object geometry, but also learns better scene context using GCN and a novel physical violation loss, which can deliver accurate scene and object layout.
Extensive experiments show that our model improves various tasks in holistic scene understanding over existing methods.
A promising future direction could be exploiting object functionalities for better 3D scene understanding.

\vspace{0.5em}
\noindent \textbf{Acknowledgement:} This research was supported by National Natural Science Foundation of China (NSFC) under grants No.61872067 and No.61720106004.








{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}



\clearpage

\newpage\appendix
\begin{center}
{\Large \textbf{Supplementary Material}}
\end{center}


\maketitle

In this supplementary material, we provide the detailed network architecture, implementation details, 3D detection on all categories, more qualitative results, and discussion of failure cases. 


\section{Architecture of Our Pipeline}
We show the architecture of LEN, ODN, LIEN, and SCGN in \autoref{fig:architecture}.

\noindent\textbf{2D Detector, LEN, ODN.} 
Following \cite{nie2020total3dunderstanding, huang2018cooperative}, we use Faster RCNN \cite{ren2016faster} trained on COCO dataset \cite{lin2014microsoft} and fine-tuned on SUN RGB-D~\cite{song2015sun} as 2D detector.
The 2D detection results on SUN RGB-D are filtered and matched with the ground-truth 3D object bounding box during the data preparation procedure provided by \cite{nie2020total3dunderstanding}.
During the initialization stage, we use LEN and ODN architecture shown in \autoref{fig:architecture} similar with \cite{nie2020total3dunderstanding}. 

\noindent\textbf{LIEN.} 
Our proposed LIEN consists of an image encoder followed by a three-layer MLP to embed a single image into a code. 
When evaluating on SUN RGB-D, the category labels are mapped to the ones used by Pix3D and concatenated to the image feature following \cite{nie2020total3dunderstanding}.
To construct the shape elements for LDIF decoder, we follow \cite{genova2020local} to reshape the 1344-dim vector into a 32x42 array, which corresponds to 42-dim (10 for analytic code and 32 for latent code) codes of the 32 shape elements.

\noindent\textbf{SGCN.}
Before being fed into each node, the features from different sources are flattened, concatenated, and embedded into a 512-dim representation using FC layers.
The weights of the embedding network for layout, object, and relationship nodes are independent of each other.
After updated with four steps of message passing, the representations of layout and object nodes are decoded into parameters with the networks specially designed for each of the node types.
The decoding networks follow the design of LEN and ODN, and refine the parameterized initial outputs of them.


\begin{figure*}
	\centering
	\includegraphics[width=.9\textwidth]  
		{figure/architecture}
	\caption{Architecture of LEN, ODN, LIEN, and SGCN. Our pipeline takes features from LEN, ODN, LIEN and other sources and embeds them into node representations. The parameter  initialized by LEN and ODN is then refined with residual  decoded from updated node representations, . Variables  are parameterized following \cite{nie2020total3dunderstanding, huang2018holistic}. We set dropout rate to 0.5 for all dropout blocks.}
	\label{fig:architecture}
\end{figure*}


\section{Implementation details}
\noindent\textbf{Data Processing.} 
For the training of LIEN, watertight meshes~\cite{genova2020local} must be used to retrieve the ground-truth values of inside-outside labels. 
However, the models of Pix3D~\cite{sun2018pix3d} are not that clean with inverted surface normals and holes occasionally, which causes failure with the traditional flood fill algorithm. 
To get more robust results, we utilize the mesh fusion pipeline \cite{mescheder2019occupancy} which generates watertight meshes by fusing signed distance fields from several virtual cameras and applying the marching cube algorithm on it.
Although the mesh fusion pipeline makes the model thicker and introduces noise to the ground-truth sample points, we evaluate it on the original mesh to directly compare with previous works.

\noindent\textbf{SGCN Outputs.}\zc{
As mentioned in main paper Section 3.1, our SGCN predicts residuals to refine the parameters of object bounding boxs, layout box, and camera pose.
We follow \cite{nie2020total3dunderstanding} to set the origin of the world coordinate frame at the camera center, with the y-axis up and perpendicular to the floor, and the x-axis aligned to the orientation of the camera forward.
Thus the camera pose can be parameterized as , where  is the camera pitch and  is the camera roll.
Also, a bounding box can be parameterized as 3D center , size  and orientation .
Specifically, a layout box can be represented as , and a object box can be represented as , where  is the offset between 2D projection of 3D center and detected 2D object bounding box center, and  is the distance between 3D center and camera center.
}

\noindent\textbf{Hyper Parameters.}
When training LIEN, we use 1024 near-surface samples and 1024 uniformly samples, and set their loss weight  and .
For shape element center loss, we let .
Following \cite{nie2020total3dunderstanding, huang2018holistic}, classification and regression loss is used for parameters of both LEN and ODN, which we denote as . Other parameters of LEN and ODN are using only regression loss.
For camera parameters, we set , and .
For layout box parameters, we set , and .
For object box parameters, we set , and .
When training with cooperative loss and object physical violation loss, we set .


\noindent\textbf{Scene Mesh Reconstruction.}
Since our LIEN is trained on Pix3D with only 9 categories like MGN of Total3D, we suffer from the same problem with them when testing on SUN RGB-D, that our LIEN can not generalize to some of the categories.
For the accuracy of the scene reconstruction, we follow \cite{nie2020total3dunderstanding} to only consider certain categories of objects (i.e. cabinet, bed, chair, sofa, table, door, bookshelf, desk, shelves, dresser, refrigerator, television, box, whiteboard, nightstand).
As a result, the reconstructed scene mesh has fewer objects than 3D detections.



\section{3D Detection on all categories}
In this section, we report the average precision of 3D object detection on all categories of SUN RGB-D for a full comparison in \autoref{tbl:3ddetectionSupp}.
Our method achieves the best performance for 27 over 33 categories and a significantly better mean average precision.


\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{l|c|c|c}
			Method & CooP \cite{huang2018cooperative} & Total3D \cite{nie2020total3dunderstanding} & Ours \\
			\hline
			cabinet & 10.47 & 14.51 & \textbf{33.93} \\
			bed & 57.71 & 60.65 & \textbf{89.34} \\
			chair & 15.21 & 17.55 & \textbf{35.14} \\
			sofa & 36.67 & 44.90 & \textbf{69.10} \\
			table & 31.16 & 36.48 & \textbf{57.37} \\
			door & 0.14 & 0.69 & \textbf{5.82} \\
			window & 0.00 & \textbf{0.62} & 0.00 \\
			bookshelf & 3.81 & 4.93 & \textbf{18.33} \\
			picture & 0.00 & 0.37 & \textbf{1.04} \\
			counter & 27.67 & 32.08 & \textbf{57.02} \\
            blinds & \textbf{2.27} & 0.00 & 1.69 \\
            desk & 19.90 & 27.93 & \textbf{49.03} \\
            shelves & 2.96 & 3.70 & \textbf{16.68} \\
            curtain & 1.35 & 3.04 & \textbf{7.38} \\
            dresser & 15.98 & 21.19 & \textbf{29.27} \\
            pillow & 2.53 & 4.46 & \textbf{11.41} \\
            mirror & 0.47 & 0.29 & \textbf{0.87} \\
            clothes & 0.00 & 0.00 & 0.00 \\
            books & 3.19 & 2.02 & \textbf{5.44} \\
			fridge & 21.50 & 24.42 & \textbf{39.12} \\
			tv & 5.20 & 5.60 & \textbf{11.17} \\
			paper & 0.20 & \textbf{0.97} & 0.03 \\
			towel & 2.14 & 2.07 & \textbf{7.73} \\
			shower curtain & \textbf{20.00} & \textbf{20.00} & 0.00 \\
			box & 2.59 & 2.46 & \textbf{6.71} \\
			whiteboard & 0.16 & 0.61 & \textbf{2.39} \\
			person & 20.96 & \textbf{31.29} & 20.82 \\
			nightstand & 11.36 & 17.01 & \textbf{41.34} \\
			toilet & 42.53 & 44.24 & \textbf{70.81} \\
		    sink & 15.95 & 18.50 & \textbf{33.81} \\
		    lamp & 3.28 & 5.04 & \textbf{11.90} \\
		    bathtub & 24.71 & 21.15 & \textbf{53.64} \\
		    bag & 1.53 & 2.47 & \textbf{6.82} \\
		    \hline
		    mAP & 12.23 & 14.28 & \textbf{24.10} \\
		\end{tabular}
	\end{center}
	\vspace{-1em}
	\caption{Average precision of 3D object detection on all categories. For CooP, we report the better results from \cite{nie2020total3dunderstanding} trained on NYU-37 object labels.}
	\label{tbl:3ddetectionSupp}
\end{table}

\section{More Qualitatively Comparison with MGN on Object Mesh Reconstruction}
In this section, we show more results on the object reconstruction in \autoref{fig:objreconSupp}.
Compared to MGN in Total3D\cite{nie2020total3dunderstanding}, our method produces more accurate geometry preserving high-quality details especially on chairs, bookshelves, and those shapes with relatively more complex topology.




\begin{figure}[!ht]
\centering
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=\textwidth]  
		{figure/scnreconLp/woLp/33_recon}
\includegraphics[width=\textwidth]
		{figure/scnreconLp/woLp/845_recon}
		\caption{Ours-}
	\end{subfigure}
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=\textwidth]  
		{figure/scnreconLp/wLp/33_recon}
\includegraphics[width=\textwidth]
		{figure/scnreconLp/wLp/845_recon}
		\caption{Full: with }
	\end{subfigure}
	\vspace{-0.5em}
	\caption{\zc{Scene reconstruction samples of Ours- and Full. We observe more intersections between objects without physical violation loss in some scenes.}}
	\label{fig:lphy_qualitative}
\end{figure}

\begin{figure}[!ht]
\centering
	\newcommand{\front}[1]{\includegraphics[width=\textwidth,clip,trim=50 20 50 80]{#1}}
	\begin{subfigure}[t]{0.155\textwidth}
		\front{figure/scnreconRebuttal/Total3D/276_front_3d}
		\vspace{-2em}
		\caption{Total3D}
	\end{subfigure}
	\begin{subfigure}[t]{0.155\textwidth}
		\front{figure/scnreconRebuttal/ours/276_front_3d}
		\vspace{-2em}
		\caption{Ours}
	\end{subfigure}
	\begin{subfigure}[t]{0.155\textwidth}
		\front{figure/scnreconRebuttal/gt/276_front_3d}
		\vspace{-2em}
		\caption{GT}
	\end{subfigure}
	\vspace{-1em}
	\caption{\zc{Qualitative comparison of supporting relation.
	We take the front view of object 3D detection of main paper Figure 5 column 4 as an example. 
	We observe fewer flying objects in our results than Total3D, which shows a better understanding of supporting relation.
	}}
	\label{fig:sup_qualitative}
\end{figure}


\section{More Qualitatively Comparison on 3D Detection and Scene Reconstruction}
In main paper Section 4.2, we show qualitative results of the 3D object detection and scene reconstruction.
Here, we show more results in \autoref{fig:scnreconSupp1}, \autoref{fig:scnreconSupp2}, and \autoref{fig:scnreconSupp3}.
We can observe that compared to the state-of-the-art method \cite{nie2020total3dunderstanding}, our method produces significantly more accurate object pose estimation with fewer flying objects (\autoref{fig:scnreconSupp1}e, \autoref{fig:scnreconSupp2}a), fewer objects intersected with each other (\autoref{fig:scnreconSupp1}a, \autoref{fig:scnreconSupp2}d, \autoref{fig:scnreconSupp3}e), and more accurate object orientation estimation (\autoref{fig:scnreconSupp1}c, \autoref{fig:scnreconSupp2}e, \autoref{fig:scnreconSupp3}c). We also observe fewer objects intersected with the layout box (\autoref{fig:scnreconSupp1}d, \autoref{fig:scnreconSupp2}e, \autoref{fig:scnreconSupp3}a). 





\section{Qualitative Comparison of Ablation Study}

\zc{
In main paper Section 4.3, we quantitatively compare the improvement of our proposed .
While exhibiting a small gap from the metric, we show in qualitative results (\autoref{fig:lphy_qualitative}) that the visual difference is relatively large.
Objects are more likely to intersect with each other when trained without , which disobeys physical context severely.
On the contrary, training with  effectively prevents these errors in the results.
}

\zc{
We also quantitatively compare the supporting relation, in main paper Section 4.3.
Here in \autoref{fig:sup_qualitative}, we qualitatively compare the understanding of supporting relation in the front view of object 3D detection.
}


\section{Failure Cases}
We also show some failure cases in \autoref{fig:scnreconFail}. We observe that although our LIEN performs well on Pix3D and is generalized to SUN RGB-D, it still cannot make plausible reconstruction for some objects in rarely seen shapes (i.e. the desks of (a) and (b), the bookshelves of (b) and (c), the bed of (d)). For object detection, our pipeline fails to correctly estimate the pose of the bed in (e), which might result from the clustered scenes. Also, in some extreme cases, heavy occlusion might cause our pipeline to fail like in (f).




\begin{figure*}[!ht]
	\centering
	\scriptsize
	\newcommand{\verti}[1]{\raisebox{-.5\height}{\includegraphics[width=.14\linewidth]{#1}}}
	\newcommand{\squa}[1]{\raisebox{-.5\height}{\includegraphics[width=.1\linewidth]{#1}}}
	\newcommand{\hori}[1]{\raisebox{-.5\height}{\includegraphics[width=.05\linewidth]{#1}}}
\begin{tabular}{*{9}{c@{\hspace{5px}}}}
	    \verti{figure/objreconSupp/input/0002} &
	    \verti{figure/objreconSupp/MGN/0002} &
	    \verti{figure/objreconSupp/ours/0002} &
	    \squa{figure/objreconSupp/input/1369} &
	    \squa{figure/objreconSupp/MGN/1369} &
	    \squa{figure/objreconSupp/ours/1369} &
	    \hori{figure/objreconSupp/input/0094} &
	    \hori{figure/objreconSupp/MGN/0094} &
	    \hori{figure/objreconSupp/ours/0094} \\
	    
	    \verti{figure/objreconSupp/input/0030} &
	    \verti{figure/objreconSupp/MGN/0030} &
	    \verti{figure/objreconSupp/ours/0030} &
	    \squa{figure/objreconSupp/input/0112} &
	    \squa{figure/objreconSupp/MGN/0112} &
	    \squa{figure/objreconSupp/ours/0112} &
	    \hori{figure/objreconSupp/input/0119} &
	    \hori{figure/objreconSupp/MGN/0119} &
	    \hori{figure/objreconSupp/ours/0119} \\
	    
	    \verti{figure/objreconSupp/input/0128} &
	    \verti{figure/objreconSupp/MGN/0128} &
	    \verti{figure/objreconSupp/ours/0128} &
	    \squa{figure/objreconSupp/input/0143} &
	    \squa{figure/objreconSupp/MGN/0143} &
	    \squa{figure/objreconSupp/ours/0143} &
	    \hori{figure/objreconSupp/input/0228_2} &
	    \hori{figure/objreconSupp/MGN/0228_2} &
	    \hori{figure/objreconSupp/ours/0228_2} \\
	    
	    \verti{figure/objreconSupp/input/0138} &
	    \verti{figure/objreconSupp/MGN/0138} &
	    \verti{figure/objreconSupp/ours/0138} &
	    \squa{figure/objreconSupp/input/0177} &
	    \squa{figure/objreconSupp/MGN/0177} &
	    \squa{figure/objreconSupp/ours/0177} &
	    \hori{figure/objreconSupp/input/0261} &
	    \hori{figure/objreconSupp/MGN/0261} &
	    \hori{figure/objreconSupp/ours/0261} \\
	    
	    \verti{figure/objreconSupp/input/0228} &
	    \verti{figure/objreconSupp/MGN/0228} &
	    \verti{figure/objreconSupp/ours/0228} &
	    \squa{figure/objreconSupp/input/0204} &
	    \squa{figure/objreconSupp/MGN/0204} &
	    \squa{figure/objreconSupp/ours/0204} &
	    \hori{figure/objreconSupp/input/0321} &
	    \hori{figure/objreconSupp/MGN/0321} &
	    \hori{figure/objreconSupp/ours/0321} \\
        
	    \verti{figure/objreconSupp/input/0320} &
	    \verti{figure/objreconSupp/MGN/0320} &
	    \verti{figure/objreconSupp/ours/0320} &
	    \squa{figure/objreconSupp/input/0246} &
	    \squa{figure/objreconSupp/MGN/0246} &
	    \squa{figure/objreconSupp/ours/0246} &
	    \hori{figure/objreconSupp/input/1605} &
	    \hori{figure/objreconSupp/MGN/1605} &
	    \hori{figure/objreconSupp/ours/1605} \\
	    \vspace{5px}
	    \verti{figure/objreconSupp/input/0388} &
	    \verti{figure/objreconSupp/MGN/0388} &
	    \verti{figure/objreconSupp/ours/0388} &
	    \squa{figure/objreconSupp/input/0315} &
	    \squa{figure/objreconSupp/MGN/0315} &
	    \squa{figure/objreconSupp/ours/0315} &
	    \hori{figure/objreconSupp/input/1728} &
	    \hori{figure/objreconSupp/MGN/1728} &
	    \hori{figure/objreconSupp/ours/1728} \\
	    \vspace{5px}
	    \verti{figure/objreconSupp/input/0474} &
	    \verti{figure/objreconSupp/MGN/0474} &
	    \verti{figure/objreconSupp/ours/0474} &
	    \squa{figure/objreconSupp/input/0853} &
	    \squa{figure/objreconSupp/MGN/0853} &
	    \squa{figure/objreconSupp/ours/0853} &
	    \hori{figure/objreconSupp/input/1920} &
	    \hori{figure/objreconSupp/MGN/1920} &
	    \hori{figure/objreconSupp/ours/1920} \\
	    
	    \verti{figure/objreconSupp/input/0700} &
	    \verti{figure/objreconSupp/MGN/0700} &
	    \verti{figure/objreconSupp/ours/0700} &
	    \squa{figure/objreconSupp/input/0922} &
	    \squa{figure/objreconSupp/MGN/0922} &
	    \squa{figure/objreconSupp/ours/0922} &
	    \hori{figure/objreconSupp/input/2122} &
	    \hori{figure/objreconSupp/MGN/2122} &
	    \hori{figure/objreconSupp/ours/2122} \\
	    
	    \verti{figure/objreconSupp/input/0737} &
	    \verti{figure/objreconSupp/MGN/0737} &
	    \verti{figure/objreconSupp/ours/0737} &
	    \squa{figure/objreconSupp/input/0988} &
	    \squa{figure/objreconSupp/MGN/0988} &
	    \squa{figure/objreconSupp/ours/0988} &
	    \hori{figure/objreconSupp/input/2213} &
	    \hori{figure/objreconSupp/MGN/2213} &
	    \hori{figure/objreconSupp/ours/2213} \\
	    \vspace{2px}
	    \verti{figure/objreconSupp/input/1324} &
	    \verti{figure/objreconSupp/MGN/1324} &
	    \verti{figure/objreconSupp/ours/1324} &
	    \squa{figure/objreconSupp/input/1760} &
	    \squa{figure/objreconSupp/MGN/1760} &
	    \squa{figure/objreconSupp/ours/1760} &
	    \hori{figure/objreconSupp/input/0032} &
	    \hori{figure/objreconSupp/MGN/0032} &
	    \hori{figure/objreconSupp/ours/0032} \\
	    
	    Input & MGN \cite{nie2020total3dunderstanding} & Ours & 
	    Input & MGN \cite{nie2020total3dunderstanding} & Ours & 
	    Input & MGN \cite{nie2020total3dunderstanding} & Ours \\
    \end{tabular}
\caption{More qualitative comparisons on object reconstruction. We compare with MGN from Total3D \cite{nie2020total3dunderstanding}.}
	\label{fig:objreconSupp}
\end{figure*}

\begin{figure*}[!ht]
	\centering
	\scriptsize
	\newcommand{\rgb}[1]{\raisebox{-0.5\height}{\includegraphics[width=.175\linewidth]{#1}}}
	\newcommand{\oblique}[1]{\raisebox{-0.5\height}{\includegraphics[width=.175\linewidth,clip,trim=48 12 48 12]{#1}}}
	\newcommand{\rot}[1]{\rotatebox[origin=c]{90}{#1}}
	\def\arraystretch{0.5}\begin{tabular}{c|c*{5}{c@{\hspace{1px}}}}
        &\rot{Input}& 
    	    \rgb{figure/scnreconSupp/input/000033} &
    	    \rgb{figure/scnreconSupp/input/000042} &
    	    \rgb{figure/scnreconSupp/input/000182} &
    	    \rgb{figure/scnreconSupp/input/000559} &
    	    \rgb{figure/scnreconSupp/input/000368} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \oblique{figure/scnreconSupp/Total3D/33_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/42_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/182_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/559_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/368_oblique_3d} \\
        	\rot{Oblique View}& \rot{Ours}& 
        	    \oblique{figure/scnreconSupp/ours/33_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/42_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/182_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/559_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/368_oblique_3d} \\
        	&\rot{GT}& 
        	    \oblique{figure/scnreconSupp/gt/33_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/42_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/182_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/559_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/368_oblique_3d} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \rgb{figure/scnreconSupp/Total3D/33_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/42_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/182_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/559_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/368_bbox} \\
            \rot{Camera View}&\rot{Ours}& 
        	    \rgb{figure/scnreconSupp/ours/33_bbox} &
        	    \rgb{figure/scnreconSupp/ours/42_bbox} &
        	    \rgb{figure/scnreconSupp/ours/182_bbox} &
        	    \rgb{figure/scnreconSupp/ours/559_bbox} &
        	    \rgb{figure/scnreconSupp/ours/368_bbox} \\
        	&\rot{GT}& 
        	    \rgb{figure/scnreconSupp/gt/33_bbox} &
        	    \rgb{figure/scnreconSupp/gt/42_bbox} &
        	    \rgb{figure/scnreconSupp/gt/182_bbox} &
        	    \rgb{figure/scnreconSupp/gt/559_bbox} &
        	    \rgb{figure/scnreconSupp/gt/368_bbox} \\
    	\hline
    	\multirow{2}{*}{\rot{Scene Mesh}}
        	&\rot{Total3D}& 
        	    \rgb{figure/scnreconSupp/Total3D/33_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/42_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/182_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/559_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/368_recon} \\
        	&\rot{Ours}& 
        	    \rgb{figure/scnreconSupp/ours/33_recon} &
        	    \rgb{figure/scnreconSupp/ours/42_recon} &
        	    \rgb{figure/scnreconSupp/ours/182_recon} &
        	    \rgb{figure/scnreconSupp/ours/559_recon} &
        	    \rgb{figure/scnreconSupp/ours/368_recon} \vspace{2px}\\
        \multicolumn{2}{c}{}& (a) & (b) & (c) & (d) & (e) \\
    \end{tabular}
\caption{Qualitative comparisons on object detection and scene reconstruction.}
	\label{fig:scnreconSupp1}
\end{figure*}

\begin{figure*}[!ht]
	\centering
	\scriptsize
	\newcommand{\rgb}[1]{\raisebox{-0.5\height}{\includegraphics[width=.175\linewidth]{#1}}}
	\newcommand{\oblique}[1]{\raisebox{-0.5\height}{\includegraphics[width=.175\linewidth,clip,trim=48 12 48 12]{#1}}}
	\newcommand{\rot}[1]{\rotatebox[origin=c]{90}{#1}}
	\def\arraystretch{0.5}\begin{tabular}{c|c*{5}{c@{\hspace{1px}}}}
        &\rot{Input}& 
    	    \rgb{figure/scnreconSupp/input/000718} &
    	    \rgb{figure/scnreconSupp/input/000769} &
    	    \rgb{figure/scnreconSupp/input/000794} &
    	    \rgb{figure/scnrecon/input/000845} &
    	    \rgb{figure/scnreconSupp/input/000875} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \oblique{figure/scnreconSupp/Total3D/718_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/769_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/794_oblique_3d} &
        	    \oblique{figure/scnrecon/Total3D/845_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/875_oblique_3d} \\
        	\rot{Oblique View}& \rot{Ours}& 
        	    \oblique{figure/scnreconSupp/ours/718_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/769_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/794_oblique_3d} &
        	    \oblique{figure/scnrecon/ours/845_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/875_oblique_3d} \\
        	&\rot{GT}& 
        	    \oblique{figure/scnreconSupp/gt/718_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/769_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/794_oblique_3d} &
        	    \oblique{figure/scnrecon/gt/845_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/875_oblique_3d} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \rgb{figure/scnreconSupp/Total3D/718_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/769_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/794_bbox} &
        	    \rgb{figure/scnrecon/Total3D/845_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/875_bbox} \\
            \rot{Camera View}&\rot{Ours}& 
        	    \rgb{figure/scnreconSupp/ours/718_bbox} &
        	    \rgb{figure/scnreconSupp/ours/769_bbox} &
        	    \rgb{figure/scnreconSupp/ours/794_bbox} &
        	    \rgb{figure/scnrecon/ours/845_bbox} &
        	    \rgb{figure/scnreconSupp/ours/875_bbox} \\
        	&\rot{GT}& 
        	    \rgb{figure/scnreconSupp/gt/718_bbox} &
        	    \rgb{figure/scnreconSupp/gt/769_bbox} &
        	    \rgb{figure/scnreconSupp/gt/794_bbox} &
        	    \rgb{figure/scnrecon/gt/845_bbox} &
        	    \rgb{figure/scnreconSupp/gt/875_bbox} \\
    	\hline
    	\multirow{2}{*}{\rot{Scene Mesh}}
        	&\rot{Total3D}& 
        	    \rgb{figure/scnreconSupp/Total3D/718_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/769_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/794_recon} &
        	    \rgb{figure/scnrecon/Total3D/845_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/875_recon} \\
        	&\rot{Ours}& 
        	    \rgb{figure/scnreconSupp/ours/718_recon} &
        	    \rgb{figure/scnreconSupp/ours/769_recon} &
        	    \rgb{figure/scnreconSupp/ours/794_recon} &
        	    \rgb{figure/scnrecon/ours/845_recon} &
        	    \rgb{figure/scnreconSupp/ours/875_recon} \vspace{2px}\\
        \multicolumn{2}{c}{}& (a) & (b) & (c) & (d) & (e) \\
    \end{tabular}
\caption{Qualitative comparisons on object detection and scene reconstruction.}
	\label{fig:scnreconSupp2}
\end{figure*}

\begin{figure*}[!ht]
	\centering
	\scriptsize
	\newcommand{\rgb}[1]{\raisebox{-0.5\height}{\includegraphics[width=.16\linewidth]{#1}}}
	\newcommand{\oblique}[1]{\raisebox{-0.5\height}{\includegraphics[width=.16\linewidth,clip,trim=48 12 48 12]{#1}}}
	\newcommand{\rot}[1]{\rotatebox[origin=c]{90}{#1}}
	\def\arraystretch{0.5}\begin{tabular}{c|c*{5}{c@{\hspace{1px}}}}
        &\rot{Input}& 
    	    \rgb{figure/scnrecon/input/002435} &
    	    \rgb{figure/scnreconSupp/input/002652} &
    	    \rgb{figure/scnreconSupp/input/003187} &
    	    \rgb{figure/scnreconSupp/input/003194} &
    	    \rgb{figure/scnreconSupp/input/004925} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \oblique{figure/scnrecon/Total3D/2435_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/2652_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/3187_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/3194_oblique_3d} &
        	    \oblique{figure/scnreconSupp/Total3D/4925_oblique_3d} \\
        	\rot{Oblique View}& \rot{Ours}& 
        	    \oblique{figure/scnrecon/ours/2435_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/2652_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/3187_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/3194_oblique_3d} &
        	    \oblique{figure/scnreconSupp/ours/4925_oblique_3d} \\
        	&\rot{GT}& 
        	    \oblique{figure/scnrecon/gt/2435_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/2652_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/3187_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/3194_oblique_3d} &
        	    \oblique{figure/scnreconSupp/gt/4925_oblique_3d} \\
    	\hline
    	    &\rot{Total3D}& 
        	    \rgb{figure/scnrecon/Total3D/2435_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/2652_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/3187_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/3194_bbox} &
        	    \rgb{figure/scnreconSupp/Total3D/4925_bbox} \\
            \rot{Camera View}&\rot{Ours}& 
        	    \rgb{figure/scnrecon/ours/2435_bbox} &
        	    \rgb{figure/scnreconSupp/ours/2652_bbox} &
        	    \rgb{figure/scnreconSupp/ours/3187_bbox} &
        	    \rgb{figure/scnreconSupp/ours/3194_bbox} &
        	    \rgb{figure/scnreconSupp/ours/4925_bbox} \\
        	&\rot{GT}& 
        	    \rgb{figure/scnrecon/gt/2435_bbox} &
        	    \rgb{figure/scnreconSupp/gt/2652_bbox} &
        	    \rgb{figure/scnreconSupp/gt/3187_bbox} &
        	    \rgb{figure/scnreconSupp/gt/3194_bbox} &
        	    \rgb{figure/scnreconSupp/gt/4925_bbox} \\
    	\hline
    	\multirow{2}{*}{\rot{Scene Mesh}}
        	&\rot{Total3D}& 
        	    \rgb{figure/scnrecon/Total3D/2435_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/2652_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/3187_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/3194_recon} &
        	    \rgb{figure/scnreconSupp/Total3D/4925_recon} \\
        	&\rot{Ours}& 
        	    \rgb{figure/scnrecon/ours/2435_recon} &
        	    \rgb{figure/scnreconSupp/ours/2652_recon} &
        	    \rgb{figure/scnreconSupp/ours/3187_recon} &
        	    \rgb{figure/scnreconSupp/ours/3194_recon} &
        	    \rgb{figure/scnreconSupp/ours/4925_recon} \vspace{2px}\\
        \multicolumn{2}{c}{}& (a) & (b) & (c) & (d) & (e) \\
    \end{tabular}
\caption{Qualitative comparisons on object detection and scene reconstruction.}
	\label{fig:scnreconSupp3}
\end{figure*}



\begin{figure*}[!ht]
	\centering
	\scriptsize
	\newcommand{\rgb}[1]{\raisebox{-0.5\height}{\includegraphics[width=.14\linewidth]{#1}}}
	\newcommand{\oblique}[1]{\raisebox{-0.5\height}{\includegraphics[width=.14\linewidth,clip,trim=48 12 48 12]{#1}}}
	\newcommand{\rot}[1]{\rotatebox[origin=c]{90}{#1}}
\begin{tabular}{c|c*{6}{c@{\hspace{1px}}}}
        &\rot{Input}& 
    	    \rgb{figure/failure/input/000342} &
    	    \rgb{figure/failure/input/000575} &
    	    \rgb{figure/failure/input/000657} &
    	    \rgb{figure/failure/input/000877} &
    	    \rgb{figure/failure/input/000983} &
    	    \rgb{figure/failure/input/001298} \\
    	\hline
        	\multirow{2}{*}{\rot{Oblique View}}& 
        	\rot{Ours}& 
        	    \oblique{figure/failure/ours/342_oblique_3d} &
        	    \oblique{figure/failure/ours/575_oblique_3d} &
        	    \oblique{figure/failure/ours/657_oblique_3d} &
        	    \oblique{figure/failure/ours/877_oblique_3d} &
        	    \oblique{figure/failure/ours/983_oblique_3d} &
        	    \oblique{figure/failure/ours/1298_oblique_3d} \\
        	&\rot{GT}& 
        	    \oblique{figure/failure/gt/342_oblique_3d} &
        	    \oblique{figure/failure/gt/575_oblique_3d} &
        	    \oblique{figure/failure/gt/657_oblique_3d} &
        	    \oblique{figure/failure/gt/877_oblique_3d} &
        	    \oblique{figure/failure/gt/983_oblique_3d} &
        	    \oblique{figure/failure/gt/1298_oblique_3d} \\
    	\hline
            \multirow{2}{*}{\rot{Camera View}}&
            \rot{Ours}& 
        	    \rgb{figure/failure/ours/342_bbox} &
        	    \rgb{figure/failure/ours/575_bbox} &
        	    \rgb{figure/failure/ours/657_bbox} &
        	    \rgb{figure/failure/ours/877_bbox} &
        	    \rgb{figure/failure/ours/983_bbox} &
        	    \rgb{figure/failure/ours/1298_bbox} \\
        	&\rot{GT}& 
        	    \rgb{figure/failure/gt/342_bbox} &
        	    \rgb{figure/failure/gt/575_bbox} &
        	    \rgb{figure/failure/gt/657_bbox} &
        	    \rgb{figure/failure/gt/877_bbox} &
        	    \rgb{figure/failure/gt/983_bbox} &
        	    \rgb{figure/failure/gt/1298_bbox} \\
    	\hline
    	    \rot{Scene Mesh}&
    	    \rot{Ours}& 
        	    \rgb{figure/failure/ours/342_recon} &
        	    \rgb{figure/failure/ours/575_recon} &
        	    \rgb{figure/failure/ours/657_recon} &
        	    \rgb{figure/failure/ours/877_recon} &
        	    \rgb{figure/failure/ours/983_recon} &
        	    \rgb{figure/failure/ours/1298_recon} \vspace{2px}\\
        \multicolumn{2}{c}{}& (a) & (b) & (c) & (d) & (e) & (f) \\
    \end{tabular}
\caption{Failure Cases. Possible reasons might be unseen object shapes (a, b, c, d), heavy occlusion (f), cluttered scene (e).}
	\label{fig:scnreconFail}
\end{figure*}

\end{document}
