\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{epsfig}
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{xcolor, colortbl}
\usepackage{mathtools}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{rotating}
\RequirePackage{fix-cm}
\usepackage{booktabs}
\usepackage{kotex}
\usepackage{pifont}
\usepackage{blindtext}
\usepackage{xr}
\usepackage[toc]{multitoc}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage[accsupp]{axessibility}  \usepackage[capitalize]{cleveref}
\newcommand{\sftype}[1]{{\textsf{\small #1}}}
\newcommand{\sftypes}[1]{{\textsf{\tiny #1}}}
\newcommand{\expnum}[2]{{#1}\mathrm{e}{-#2}}





\usepackage{soul}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{xcolor, colortbl}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{diagbox}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{orcidlink}
\usepackage{pict2e,picture}
\newtheorem{theorem}{Theorem}
\def\ie{\emph{i.e.}}
\def\eg{\emph{e.g.}}
\def\etal{\emph{et al.}}
\def\wrt{\emph{w.r.t.}}
\definecolor{grey}{rgb}{0.9, 0.9, 0.9}
\newcommand{\ccol}{\cellcolor{grey}}
\definecolor{brown}{rgb}{0.65, 0.16, 0.16}
\newcommand{\ready}[1]{{\color{brown}{#1}}}
\newcommand{\sy}[1]{{\color{purple}{#1}}}

\usepackage{pict2e,picture}

\makeatletter
\DeclareRobustCommand{\shortarrow}[1][]{\check@mathfonts
  \if\relax\detokenize{#1}\relax
    \settowidth{\dimen@}{}\else
    \setlength{\dimen@}{#1}\fi
  \sbox\z@{\usefont{U}{lasy}{m}{n}\symbol{41}}\begin{picture}(\dimen@,\ht\z@)
  \roundcap
  \put(\dimexpr\dimen@-.7\wd\z@,0){\usebox\z@}
  \put(0,\fontdimen22\textfont2){\line(1,0){\dimen@}}
  \end{picture}}
\makeatother



\usepackage[accsupp]{axessibility}  




\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{4593}  

\title{Cross-Domain Ensemble Distillation \linebreak for Domain Generalization} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Cross-Domain Ensemble Distillation for Domain Generalization}
\author{Kyungmoon Lee

\inst{1,2}\orcidlink{0000-0001-6597-3928} \qquad
Sungyeon Kim\inst{2}\orcidlink{0000-0002-6919-4822} \qquad
Suha Kwak\inst{2}\orcidlink{0000-0002-4567-9091}}\authorrunning{Kyungmoon Lee, Sungyeon Kim, Suha Kwak}
\institute{NALBI Inc, Seoul, Korea \qquad POSTECH, Pohang, Korea \\
\email{kyungmoon@nalbi.ai, \{sungyeon.kim, suha.kwak\}@postech.ac.kr }\\
{\tt\small
\url{http://cvlab.postech.ac.kr/research/XDED/}
}
}

\maketitle

\begin{abstract}
Domain generalization is the task of learning models that generalize to unseen target domains. We propose a simple yet effective method for domain generalization, named cross-domain ensemble distillation (XDED), that learns domain-invariant features while encouraging the model to converge to flat minima, which recently turned out to be a sufficient condition for domain generalization. To this end, our method generates an ensemble of the output logits from training data with the same label but from different domains and then penalizes each output for the mismatch with the ensemble. Also, we present a de-stylization technique that standardizes features to encourage the model to produce style-consistent predictions even in an arbitrary target domain. Our method greatly improves generalization capability in public benchmarks for cross-domain image classification, cross-dataset person re-ID, and cross-dataset semantic segmentation. Moreover, we show that models learned by our method are robust against adversarial attacks and image corruptions.






\keywords{domain generalization, knowledge distillation, flat minima}

\end{abstract} \section{Introduction}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{CameraReady/figure/cr_fig1.pdf}
    \caption{Illustration of cross-domain ensemble distillation (XDED). Although the four images share the same class label, their predictions manifest different inter-class relations due to the visual gap between domains. XDED constructs an ensemble by averaging all predictions and matches it with each prediction.
}
    \label{fig:teaser}
\end{figure*}

Deep neural networks (DNNs) have brought remarkable advances in a number of research areas such as image classification~\cite{Alexnet}, image synthesis~\cite{goodfellow2014generative}, and reinforcement learning~\cite{mnih2013playing}. The huge success of DNNs depends heavily on the assumption that training and test data are sampled under the independent and identically distributed (i.i.d.)~condition. However, this assumption often does not hold in real-world scenarios; a large error occurs due to the discrepancy between training and test data, also known as the domain shift problem. As a solution to this problem, domain generalization, the task of learning models that generalize to unseen target domains, is in the spotlight.
A key to the success of domain generalization is to learn invariant features across domains. To this end, most previous methods align feature distributions of multiple domains by adversarial training~\cite{li2018domain,li2018deep}, minimizing the dissimilarity between the distributions of source domains~\cite{muandet2013domain}, or contrastive learning~\cite{kim2021selfreg}. Then, a classifier is trained to predict the labels for the aligned source features in hopes that it will also generalize well for any target domain. 
However, this approach often drops performance when the target domain differs substantially from the source domains as the model is prone to overfit to the source domains.


Meanwhile, the relationship between the geometry of loss landscapes and generalization ability has attracted increasing attention~\cite{dziugaite2017computing,foret2021sharpness,jiang2020fantastic,keskar2016large}.
In particular, converging to flat minima in loss landscapes is known as a key to achieve robustness against the
loss landscape shift between training and test datasets. 
Inspired by the observation that higher posterior entropy helps a model converge to flat minima~\cite{chaudhari2017entropy,pereyra2017regularizing,zhang2018deep}, entropy regularization techniques like self-knowledge distillation~\cite{zhang2019your} and entropy maximization~\cite{cha2020cpr} have been proposed to increase entropy rather than forcing a model to completely fit training data (\ie, one-hot labels) to induce low entropy.
Since the degree
of loss landscape shift is generally expected to be bigger in the case of domain generalization, it is more important to converge to flat minima 
in domain generalization.
However, the benefit of flat minima in terms of domain generalization has not been actively studied yet.


In this paper, we propose a novel method, named \emph{cross-domain ensemble distillation} (XDED), that learns domain-invariant features while encouraging convergence to flat minima for domain generalization.
Specifically, XDED generates an ensemble of the output logits for the data with the same label but from different domains, and then penalizes each output for the mismatch with the ensemble (Fig.~\ref{fig:teaser}). 
By doing so, it enables a model to learn domain-invariant features by enforcing prediction consistency between the data with the same label but from different domains. 
Also, 
XDED increases the posterior entropy of each output distribution, which helps the model converge to flat minima as the entropy regularization does.
To the best of our knowledge, XDED is the first to achieve these two objectives simultaneously for domain generalization, and this contribution leads to significant performance improvement.

Since XDED is still limited to exploiting the information of only source domains, there is further room to reduce the domain gap with the target domain. Hence, we also introduce a de-stylization technique well-suited to domain generalization, called UniStyle. UniStyle suppresses domain-specific style bias simply by standardizing intermediate feature maps of input image during both training and test time. Thanks to UniStyle, our model produces style-consistent predictions not only for the source domains but also for the target domain, which greatly reduces the domain gap and boosts the effect of XDED.

Based on the recent theoretical result on the relationship between the domain generalization and the flatness of local minima~\cite{cha2021swad}, we first empirically show that the proposed framework can improve generalization capability by achieving two goals: promoting flat minima and reducing the domain gap. Next, we further demonstrate the superiority of our method through extensive experimental results. 
On the standard public benchmarks for cross-domain image classification, XDED significantly enhances generalization ability in both multi-source and single-source settings. 
We also validate the effectiveness of our method in various domain generalization scenarios by showing the non-trivial improvement on the DomainBed~\cite{gulrajani2020search}, cross-dataset person re-ID~\cite{zheng2015scalable,zheng2017unlabeled}, and cross-dataset semantic segmentation experiments. Moreover, we demonstrate that models learned by our method also help achieve robustness against adversarial attacks and unseen image corruptions.
 \section{Related Work}
\label{relatedwork}

\noindent \textbf{Domain generalization.} 
The goal of domain generalization is to learn domain-invariant features that well generalize to unseen target domains. 
For the purpose, existing methods match feature distributions of different domains by adversarial feature alignment~\cite{li2018domain,li2018deep} or reducing the difference between feature distributions of diverse source domains~\cite{muandet2013domain}. 
Recently, meta-learning frameworks~\cite{balaji2018metareg,dou2019domain,li2019episodic} have been introduced to simulate the domain shift by dividing the meta-train and meta-test domains from source domains. 
Also, data augmentation methods have been proposed to generate more diverse data beyond those of given source domains~\cite{StyleNeophile,kim2021wedge,shankar2018generalizing,xu2021fourier,zhou2020learning}. 
Most similar to our framework, ensemble methods for domain generalization have been proposed~\cite{xu2014exploiting,seo2020learning,zhou2021dael}. They all train multiple modules such as exemplar SVMs~\cite{xu2014exploiting}, domain-specific BN~\cite{Batchnorm} layers~\cite{seo2020learning} or classifiers~\cite{zhou2021dael}, and exploit the ensemble of learned modules for prediction in testing.
However, we remark that our XDED utilizes the ensemble of model predictions as the soft label and transfers it to the model itself. Therefore, it does not demand any additional module during both training and testing. 


\noindent \textbf{Knowledge distillation (KD).}
KD
was originally studied to transfer the knowledge of a deep model to a shallow model for model compression~\cite{hinton2015distilling}. 
It has been also used for other purposes such as metric learning~\cite{park2019relational,kim2021embedding} and network regularization~\cite{xu2019data,zhang2019your,yun2020regularizing}.
In particular for network regularization, self-knowledge distillation (self-KD) has been studied; it distills knowledge from the model itself and enforces prediction consistency between a sample and its perturbed one or other samples.
KD has been used for domain adaptation~\cite{meng2018adversarial,feng2020kd3a}, and such method trains several teacher models from the source domains and distills the ensemble of their predictions to the student model. 
It unfortunately requires large memory due to multiple teachers, and are difficult to be extended to domain generalization as they demand target images in training.
In contrast, our method improves generalization capability of a model on unseen domains without the need for target images and additional teacher models.

\noindent \textbf{Flat minima in loss landscapes.}
Recent analyses have revealed that finding flat minima is crucial for model generalization~\cite{keskar2016large,dziugaite2017computing,foret2021sharpness}. 
In this context,
multiple methods have been proposed to promote flat minima in loss landscapes since flat minima have an advantage over sharp minima in robustness against the loss landscape shift between training and test data.Among literature on ways of promoting flat minima (\eg, weight averaging~\cite{izmailov2018averaging,cha2021swad} and training strategies~\cite{foret2021sharpness,chaudhari2017entropy}), we focus on the high entropy-seeking approaches, on which XDED is based. 
Maximum Entropy~\cite{pereyra2017regularizing,cha2020cpr} maximizes the entropy of an output distribution from a classifier.
Similarly, KD-based methods also aim at inducing high entropy of the output distribution by penalizing the mismatch with the output distribution from that of another classifier such as differently initialized peer networks~\cite{zhang2018deep} or subnetworks within a network itself~\cite{zhang2019your}. Although SWAD~\cite{cha2021swad} has introduced the importance of flat minima in the area of domain generalization, we remark that SWAD belongs to weight averaging but does not focus on learning domain-invariant features, whereas XDED belongs to entropy regularization as well as is designed for learning domain-invariant features.



\noindent \textbf{Bias towards styles.}
Recent studies~\cite{brendel2019approximating,geirhos2018imagenet} revealed that DNNs overly depend on a strong bias towards styles, and it is also confirmed in the domain generalization literature~\cite{choi2021robustnet,zhou2021domain,StyleNeophile} that a visual domain is highly correlated to feature statistics.
Hence, previous work defines image styles as the bias and attempts to remove the bias by style augmentation in the space of feature statistics~\cite{zhou2021domain,StyleNeophile}, using another model that is intentionally biased to styles~\cite{nam2021reducing}, or minimizing a whitening loss~\cite{choi2021robustnet}.
Distinct from these techniques, we show that a simple yet effective de-stylization technique leads to a smaller divergence measure between target and source domains without bells and whistles.

 \section{Our Method}
\label{method}
\subsection{Cross-Domain Ensemble Distillation}
\noindent \textbf{Review of knowledge distillation (KD).}
The goal of KD~\cite{hinton2015distilling} is to transfer knowledge of a teacher model  to a student model , usually a wide and deep model to a smaller one, for the purposes of model compression or model regularization. Given input data point  and its label , we denote the output logit of model as . The posterior predictive distribution of  is then formulated as:

where the model is parameterized by  and  is a temperature scaling parameter. KD enforces to match the predictive distributions of  and . Specifically, it is achieved by minimizing the Kullback-Leibler (KL) divergence between their predictive distributions as follows:

where  is a batch of input data,  and  are the parameters of a teacher and a student, respectively.




\noindent \textbf{Cross-domain ensemble distillation.} We propose a new KD method for domain generalization, called cross-domain ensemble distillation (XDED). XDED aims to construct the domain-invariant knowledge from the data of multiple domains. Specifically, XDED generates an ensemble of logits from the data with the same label but from different domains. Next, XDED penalizes each logit for the mismatch with the ensemble which is not biased towards a specific domain, which encourages learning domain-invariant features.
Unlike the conventional KD, XDED does not require an additional network that increases training complexity (\eg, extra parameters and training time) but distills the ensemble constructed by multiple samples to the model itself in the form of self-KD. 


Formally, let  denote the set of samples that have the same class label  in a mini-batch. Then, we obtain an ensemble of logits from  by simply taking an average as:

Then, the predictive distribution for the ensemble created from data  is as:

The loss function of XDED is defined as follows:

where  is a fixed copy of the parameter . Following \cite{miyato2018virtual,yun2020regularizing}, we stop the gradient to be propagated through  to prevent the model from falling into some trivial solutions. To sum up, we set our objective function as

where  is a batch of input images,  is a batch of corresponding class labels,  denotes the vanilla cross-entropy loss, and  is a hyperparameter to balance  and .  and  are 5.0 and 4.0 throughout this paper.

\subsection{UniStyle: removing and unifying style bias} To further regularize the model to produce style-consistent predictions, we propose a de-stylization technique that is well-suited to domain generalization. As source domain styles are not expected to appear at test time, we propose UniStyle to prevent the model from being biased towards the domain-specific styles, which reduces the domain gap with the target domain.

More specifically, following existing methods based on style transfer~\cite{dumoulin2016learned,huang2017arbitrary,ulyanov2016instance}, we first represent a neural style as statistics of intermediate feature maps from the feature extractor. Formally, let  denote an  intermediate feature map of an image. Then, a neural style of the image is represented as the combination of channel-wise mean  and standard deviation  of  as:

and

where  and . Next, we simply standardize each feature to have constant channel-wise statistics,  and  as:

where  and  (\ie, zero-mean standardization). 
Technically, UniStyle is a special case of InstanceNorm (IN)~\cite{ulyanov2016instance}. Nevertheless, we remark that UniStyle aims to remove domain-specific information without any learnable parameters to reduce the domain gap while IN learns channel-wise scaling and bias parameters for style transfer.
Also, note that we empirically observed that UniStyle is effective when being applied at multiple early layers, which is aligned with recent studies~\cite{dumoulin2016learned,huang2017arbitrary} suggesting that the style information is usually captured at the early layers.\footnote{See the supplementary material for further analyses.}



\subsection{Analysis of Our Method}
In this section, we analyze the effectiveness of XDED, especially through the link to the theoretical result and the supporting empirical evidences. We first begin with a theorem related to domain adaptation~\cite{ben2010theory,ben2007analysis}, which shows that the expected risk on the target domain is bounded by that on the source domain and the divergence between these domains.
To find a model parameter  for domain generalization, Cha \etal~\cite{cha2021swad} considered a robust empirical loss:

where  is an empirical risk over source domains  and  is a radius which defines neighbor parameters of . Then, Cha \etal~\cite{cha2021swad} proved that finding flat minima reduces the domain gap through the theorem below:
\begin{theorem}
Consider a set of N covers  such that the hypothesis space  where  and  is dimension of . Let  be a VC dimension of each . Then, for any , the following bound holds with probability at least ,
    
where  is the number of training samples and  is the divergence between the source domain  and the target domain .
\label{theorem:swad}
\end{theorem}
We remark that, in Eq.~(\ref{eq:swad}), the test loss  is bounded by three terms: (1) the robust empirical loss , (2) the divergence , and (3) a confidence bound depending on the radius  and the number of training samples . In the rest of this section, according to the above theorem, we provide a theoretical interpretation that our method enhances the generalization ability by lowering both  and  with the empirical evidences. 

\begin{table}[!t]
    \centering
    \caption{
    Comparison of the entropy values. When each model is converged, the entropy value is calculated by averaging over all training samples.
    }
    \fontsize{8.5}{10.5}\selectfont
    \begin{tabularx}{0.7 \textwidth}{
       >{\centering\arraybackslash}X|
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X|
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X}
    
    \hline
    \multicolumn{1}{l|}{} &
    \multicolumn{2}{c|}{OfficeHome (Clipart)} & \multicolumn{2}{c}{PACS (Cartoon)} \\
    \multicolumn{1}{l|}{Methods} & {Entropy} & {Accuracy} & {Entropy} & {Accuracy} \\
    
    \hline
    
    \multicolumn{1}{l|}{ResNet-18} & 0.25 & 49.4 & 0.01 & 75.9\\
    \multicolumn{1}{l|}{MixStyle~\cite{zhou2021domain}} & 0.35 & 53.4 & 0.03 & 78.8 \\
 
    \multicolumn{1}{l|}{\ccol XDED} & \ccol \textbf{0.92} & \ccol \textbf{55.2} & \ccol \textbf{0.38} & \ccol \textbf{81.7} \\
        \hline
    \end{tabularx}
    \label{tab:supple_comparison_entropy}
\end{table} \begin{figure*}[t]
    \centering
\includegraphics[width=0.85\textwidth]{CameraReady/figure/cr_fig2_4_ver2.pdf}
    \caption{
    Train/Test losses versus the weight perturbation while varying the standard deviation of the added Gaussian noise. Note that the results are produced with the target domain (Art of PACS) and the rest source domains, and the loss values are log-scaled.
}
    \label{fig:a_dist}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{CameraReady/figure/cr_fig3_ver2.pdf}
\caption{
    Comparison to existing methods promoting flat minima. Each model is evaluated on Cartoon of PACS after being trained on the rest source domains. \textbf{Left}: The divergence (-distance) between the source domains and the target domain, \textbf{Right}: Generalization performance on the target domain.
    }
    \label{fig:supple_a_dist}
\end{figure*}

\noindent \textbf{Promoting flat minima.} 
We remark that XDED is motivated by recent entropy regularization methods~\cite{cha2020cpr,zhang2019your,zhang2018deep} in pursuit of flat minima. It has been empirically demonstrated that these methods promote flat minima by inducing higher posterior entropy. It can be interpreted as relaxing the training procedure to learn richer information encoded in soft labels, which helps the model converge to flat minima more than forcing the model to completely fit one-hot labels. In this context, we also demonstrate that XDED clearly induces higher entropy as shown in Table~\ref{tab:supple_comparison_entropy}. Considering that XDED is motivated by the observation that different domains manifest different inter-class relations due to the domain gap (Fig.~\ref{fig:teaser}), this is natural since our ensembles would integrate meaningful inter-class relations from multiple domains and the model learned with them would be led towards high entropy.

Next, to investigate whether the model learned with XDED converges to flat minima indeed, we quantify the flatness of the local minima where the model converged by measuring the increase of loss values between  and its neighborhoods, assuming that the model converged in flat minima would have smaller increases. Following \cite{cha2021swad,cha2020cpr,zhang2019your,zhang2018deep}, we measure the losses of the learned models before and after adding Gaussian noises to model parameters while varying the standard deviation of the noise  (\ie,  where ) with 100 runs. As a result, XDED demonstrates its robustness against the weight perturbation with smaller loss increases as shown in Fig.~\ref{fig:a_dist}. 


\noindent \textbf{Domain-invariant feature learning.} 
Here, we highlight that XDED also learns domain-invariant features via regularizing the consistency between the predictions from the data with the same label but from different domains and their ensemble. Thus, we compare XDED with existing methods promoting flat minima, which are dedicated to the flatness of local minima only. Specifically, to examine the effectiveness in reducing the divergence , we measure -distance~\cite{ben2010theory,kifer2004detecting}. Due to the computational intractability, we calculated an approximated one~\cite{long2015learning,nam2021reducing}~\footnote{It is defined as  where  is the generalization error of a SVM-based two-class classifier trained to distinguish between target and source domains.}
As shown in Fig.~\ref{fig:supple_a_dist} (Left), we observe that the existing methods promoting flat minima fail to reduce the distance while XDED clearly lowers the distance and UniStyle further enhances the result. Naturally, that result is connected to the quantitative superiority of our framework over existing flat minima-promoting methods (Fig.~\ref{fig:supple_a_dist} (Right)). 
 \section{Experiments}
\label{experiments}
\subsection{Generalization in image classification}
\label{dg_cls}

\noindent \textbf{Multi-source domain generalization.}
Specifically, for a fair comparison, we follow the leave-one-domain-out protocol~\cite{li2017deeper} where we train a model on three domains and evaluate it on the remaining domain. For the benchmark datasets, we employ the PACS~\cite{li2017deeper} and OfficeHome~\cite{venkateswara2017deep} that are widely-used benchmarks for domain generalization in image classification. PACS contains 9,991 images of 7 classes over 4 domains: Art Painting, Cartoon, Photo, and Sketch. OfficeHome includes 15,500 images of 65 classes over 4 domains: Artistic, Clipart, Product, and Real. We use ResNet-18~\cite{resnet} as the backbone, and our UniStyle is applied to output feature maps of the first and second residual blocks for PACS and the first one only for OfficeHome.

\noindent \textbf{Results.}
As summarized in Table.~\ref{tab:cr_pacs_officehome}, we observe that our method not only significantly enhances the vanilla but also outperforms the latest competing methods. In particular, our method outperforms the second-best method on Cartoon of PACS and Clipart of OfficeHome by about 4.0\% and 2.0\%, respectively. these results justify the superiority of our method, which is simple yet effective.

\begin{table}[!t]
    \centering
    \caption{
    Leave-one-domain-out generalization results on PACS and OfficeHome.
    }
\fontsize{8}{10}\selectfont
    \begin{tabularx}{1.0 \textwidth}{
       >{\centering\arraybackslash}X|
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
    }
    
    \hline
    \multicolumn{1}{l|}{} &
    \multicolumn{5}{c|}{PACS} & \multicolumn{5}{c}{OfficeHome} \\
    \hline
\multicolumn{1}{l|}{Methods} & {Art} & {Cartoon} & {Photo} & {Sketch} & \multicolumn{1}{|c|}{Avg.} & {Artistic} & {Clipart} & {Product} & {Real} & \multicolumn{1}{|c}{Avg.} \\
    \hline
    \multicolumn{1}{l|}{ResNet-18} & {77.0} & {75.9} & {96.0} & {69.2} & \multicolumn{1}{|c|}{79.5} & {58.9} & {49.4} & {74.3} & {76.2} & \multicolumn{1}{|c}{64.7} \\
    \multicolumn{1}{l|}{MMD-AE~\cite{li2018domain}} & {75.2} & {72.7} & {96.0} & {64.2} & \multicolumn{1}{|c|}{77.0} & {56.5} & {47.3} & {72.1} & {74.8} & \multicolumn{1}{|c}{62.7} \\
    \multicolumn{1}{l|}{JiGen~\cite{carlucci2019domain}} & {79.4} & {75.3} & {96.0} & {71.6} & \multicolumn{1}{|c|}{80.5} & {53.0} & {47.4} & {71.4} & {72.7} & \multicolumn{1}{|c}{61.2} \\
    \multicolumn{1}{l|}{CrossGrad~\cite{shankar2018generalizing}} & {79.8} & {76.8} & {96.0} & {70.2} & \multicolumn{1}{|c|}{80.7} & {58.4} & {49.4} & {73.9} & {75.8} & \multicolumn{1}{|c}{64.4} \\
    
    \multicolumn{1}{l|}{MASF~\cite{dou2019domain}} & {80.2} & {77.1} & {94.9} & {71.6} & \multicolumn{1}{|c|}{81.0} & {-} & {-} & {-} & {-} & \multicolumn{1}{|c}{-} \\
    \multicolumn{1}{l|}{Epi-FCR~\cite{li2019episodic}} & {82.1} & {77.0} & {93.9} & {73.0} & \multicolumn{1}{|c|}{81.5} & {-} & {-} & {-} & {-} & \multicolumn{1}{|c}{-} \\
    \multicolumn{1}{l|}{EISNet~\cite{wang2020learning}} & {81.8} & {76.4} & {95.9} & {74.3} & \multicolumn{1}{|c|}{82.1} & {-} & {-} & {-} & {-} & \multicolumn{1}{|c}{-} \\

    
    \multicolumn{1}{l|}{L2A-OT~\cite{zhou2020learning}} & {83.3} & {78.2} & {\underline{96.2}} & {73.6} & \multicolumn{1}{|c|}{82.8} & {\underline{60.6}} & {50.1} & {\underline{74.8}} & {\textbf{77.0}} & \multicolumn{1}{|c}{65.6} \\
    
    \multicolumn{1}{l|}{SagNet~\cite{nam2021reducing}} & {83.5} & {77.6} & {95.4} & {76.3} & \multicolumn{1}{|c|}{83.2} & {60.2} & {45.3} & {70.4} & {73.3} & \multicolumn{1}{|c}{62.3} \\
    \multicolumn{1}{l|}{SelfReg~\cite{kim2021selfreg}} & {82.3} & {78.4} & {\underline{96.2}} & {77.4} & \multicolumn{1}{|c|}{83.6} & {-} & {-} & {-} & {-} & \multicolumn{1}{|c}{-} \\

    
    \multicolumn{1}{l|}{MixStyle~\cite{zhou2021domain}} & {84.1} & {78.8} & {96.1} & {75.9} & \multicolumn{1}{|c|}{83.7} & {58.7} & {53.4} & {74.2} & {75.9} & \multicolumn{1}{|c}{65.5} \\
    
    \multicolumn{1}{l|}{L2D~\cite{wang2021learning}} & {81.4} & {79.5} & {95.5} & {80.5} & \multicolumn{1}{|c|}{84.2} & {-} & {-} & {-} & {-} & \multicolumn{1}{|c}{-} \\

    
    \multicolumn{1}{l|}{FACT~\cite{xu2021fourier}} & {\underline{85.3}} & {78.3} & {95.1} & {79.1} & \multicolumn{1}{|c|}{84.5} & {60.3} & {54.8} & {74.4} & {\underline{76.5}} & \multicolumn{1}{|c}{\underline{66.5}} \\
    \multicolumn{1}{l|}{DSON~\cite{seo2020learning}} & {84.6} & {77.6} & {95.8} & {\underline{82.2}} & \multicolumn{1}{|c|}{85.1} & {59.3} & {45.7} & {71.8} & {74.6} & \multicolumn{1}{|c}{62.9} \\
    \multicolumn{1}{l|}{RSC~\cite{huang2020self}} & {83.4} & {\underline{80.3}} & {95.9} & {80.8} & \multicolumn{1}{|c|}{85.1} & {58.4} & {47.9} & {71.6} & {74.5} & \multicolumn{1}{|c}{63.1} \\
    \multicolumn{1}{l|}{StyleNeophile~\cite{StyleNeophile}} & {84.4} & {79.2} & {94.9} & {\textbf{83.2}} & \multicolumn{1}{|c|}{\underline{85.4}} & {59.5} & {\underline{55.0}} & {73.5} & {75.5} & \multicolumn{1}{|c}{65.8} \\
    \hline
    \multicolumn{1}{l|}{\ccol Ours} & {\ccol \textbf{85.6}} & {\ccol \textbf{84.2}} & {\ccol \textbf{96.5}} & {\ccol 79.1} & \multicolumn{1}{|c|}{\ccol \textbf{86.4}} & {\ccol \textbf{60.8}} & {\ccol \textbf{57.1}} & {\ccol \textbf{75.3}} & {\ccol \underline{76.5}} & \multicolumn{1}{|c}{\ccol \textbf{67.4}} \\

    \hline
    \end{tabularx}
\label{tab:cr_pacs_officehome}
\end{table} 

\noindent \textbf{Single-source domain generalization}
\label{single_src_dg}
Thanks to the simple design of our proposed method, which does not explicitly require domain labels, our method can be transparently incorporated with single-source domain generalization where we only have access to a single source domain during training. Therefore, to further evaluate the impact of our method on single-source domain generalization, our model is trained on each single domain of PACS and evaluated on the remaining target domains.

\noindent \textbf{Results.} As shown in Table.~\ref{tab:single_source}, our model, on average, significantly outperforms other baselines by 8.7\% in average accuracy. Besides, in all cases except for the case of , our model shows its superiority in performance. We believe this interesting result stems from the fact that our method is still able to help the model converge to flat minima and exploit the fine-grained relations between intra-domain samples even if only a single source domain is given.

\begin{table*}[!t]
    \centering
\caption{
        Single-source domain generalization accuracy (\%) on PACS with a ResNet-18. (A: Art Painting, C: Cartoon, S:Sketch, P:Photo).
    }
    \fontsize{7}{9}\selectfont
\begin{tabularx}{1.0\textwidth}{
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X|
    >{\centering\arraybackslash}X
    }
    \hline
\multicolumn{1}{l|}{Methods} & {A }\shortarrow[.12cm] {C}&
     {A }\shortarrow[.12cm] {S} &  {A }\shortarrow[.12cm] {P} &  {C }\shortarrow[.12cm] {A} &  {C }\shortarrow[.12cm] {S} &  {C }\shortarrow[.12cm] {P} &  {S }\shortarrow[.12cm] {A} &  {S }\shortarrow[.12cm] {C} &  {S }\shortarrow[.12cm] {P} &  {P }\shortarrow[.12cm] {A} &  {P }\shortarrow[.12cm] {C} &  {P }\shortarrow[.12cm] {S} &  {Avg.} \\
    \hline


    \multicolumn{1}{l|}{ResNet-18} & 62.3 & 49.0 & 95.2 & 65.7 & 60.7 & 83.6 & 28.0  & 54.5 & 35.6 & 64.1 & 23.6 & 29.1 & 54.3 \\
\multicolumn{1}{l|}{JiGen~\cite{carlucci2019domain}} & 57.0 & 50.0 & 96.1 & 65.3 & 65.9 & 85.5 & 26.6 & 41.1 & 42.8 & 62.4 & 27.2 & 35.5 & 54.6 \\
    \multicolumn{1}{l|}{MixStyle~\cite{zhou2021domain}} & 65.5 & 49.8 & \underline{96.7} & 69.9 & 64.5 & 85.3 & 27.1 & 50.9 & 32.6 & 67.7 & \underline{38.9} & 39.1 & 57.4 \\
    \multicolumn{1}{l|}{RSC~\cite{huang2020self}} & 62.5 & 53.1 & 96.2 & 68.9 & \textbf{70.3} & 85.8 & 37.9 & 56.3 & \underline{47.4} & 66.3 & 26.4 & 32.0 & 58.6 \\
    \multicolumn{1}{l|}{SelfReg~\cite{kim2021selfreg}} & 65.2 & 55.9 & 96.6 & 72.0 & \underline{70.0} & \underline{87.5} & 37.1 & 54.0 & 46.0 & 67.7 & 28.9 & 33.7 & 59.5 \\
    \multicolumn{1}{l|}{SagNet~\cite{nam2021reducing}} & \underline{67.1} & \underline{56.8} & 95.7 & \underline{72.1} & 69.2 & 85.7 & \underline{41.1} & \underline{62.9} & 46.2 & \underline{69.8} & 35.1 & \underline{40.7} & \underline{61.9} \\

    


    \hline

    \multicolumn{1}{l|}{\ccol Ours} & \ccol \textbf{74.6} & \ccol \textbf{58.1} & \ccol \textbf{96.8} & \ccol \textbf{74.4} & \ccol 69.6 & \ccol \textbf{87.6} & \ccol \textbf{43.3} & \ccol \textbf{65.6} & \ccol \textbf{50.3} & \ccol \textbf{71.4} & \ccol \textbf{54.3} & \ccol \textbf{51.5} & \ccol \textbf{66.5} \\
    
    \hline
\end{tabularx}
\label{tab:single_source}
\end{table*} 










\begin{table*}[!t]
    \centering
\caption{Domain generalization accuracy (\%) on DomainBed. 
The column ``Terra'' stands for TerraIncognita dataset. Note that we adopt leave-one-domain-out cross-validation as a model selection criteria. 
    }
\adjustbox{max width=1.0\textwidth}
    {    
    \fontsize{8}{10}\selectfont
    \begin{tabularx}{1.\textwidth} {
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X
    >{\centering\arraybackslash}X|
    >{\centering\arraybackslash}X
    }
    \hline
    \multicolumn{8}{c}{Model selection: leave-one-domain-out cross-validation} \\
\hline
\multicolumn{1}{l|}{Methods}
    & {CMNIST} & {RMNIST} & {VLCS} & {PACS} & {OfficeHome} & {Terra} & {Avg.} \\
\hline
\multicolumn{1}{l|}{ERM~\cite{vapnik1998statistical}} & 36.7 & 97.7 & 77.2 & 83.0 & 65.7 & 41.4 & 66.9 \\
\multicolumn{1}{l|}{IRM~\cite{arjovsky2019invariant}} & 40.3 & 97.0 & 76.3 & 81.5 & 64.3 & 41.2 & 66.7 \\
\multicolumn{1}{l|}{GroupDRO~\cite{sagawa2020distributionally}} & 36.8 & 97.6 & \underline{77.9} & 83.5 & 65.2 & 44.9 & 66.7 \\
\multicolumn{1}{l|}{Mixup~\cite{zhang2017mixup}} & 33.4 & \underline{97.8} & 77.7 & 83.2 & 67.0 & \textbf{48.7} & 67.9 \\
\multicolumn{1}{l|}{MLDG~\cite{li2018mldg}} & 36.7 & 97.6 & 77.2 & 82.9 & 66.1 & 46.2 & 67.7 \\
\multicolumn{1}{l|}{CORAL~\cite{sun2016coral}} & 39.7 & \underline{97.8} & \textbf{78.7} & 82.6 & \textbf{68.5} & 46.3 & \textbf{68.9} \\
\multicolumn{1}{l|}{MMD~\cite{li2018domain}} & 36.8 & \underline{97.8} & 77.3 & 83.2 & 60.2 & 46.5 & 66.9 \\
\multicolumn{1}{l|}{DANN~\cite{ganin2016dann}} & \underline{40.7} & 97.6 & 76.9 & 81.0 & 64.9 & 44.4 & 67.5 \\
\multicolumn{1}{l|}{CDANN~\cite{li2018deep}} & 39.1 & 97.5 & 77.5 & 78.8 & 64.3 & 39.9 & 66.1 \\
\multicolumn{1}{l|}{MTL~\cite{blanchard2021domain}} & 35.0 & \underline{97.8} & 76.6 & \underline{83.7} & 65.7 & 44.9 & 67.2 \\
\multicolumn{1}{l|}{SagNet~\cite{nam2021reducing}} & 36.5 & 94.0 & 77.5 & 82.3 & \underline{67.6} & \underline{47.2} & 67.5 \\
\multicolumn{1}{l|}{ARM~\cite{zhang2020adaptive}} & 36.8 & \textbf{98.1} & 76.6 & 81.7 & 64.4 & 42.6 & 66.7 \\
\multicolumn{1}{l|}{VREx~\cite{krueger2021out}} & 36.9 & 93.6 & 76.7 & 81.3 & 64.9 & 37.3 & 65.1 \\
\multicolumn{1}{l|}{RSC~\cite{huang2020self}} & 36.5 & 97.6 & 77.5 & 82.6 & 65.8 & 40.0 & 66.6 \\
\hline
\multicolumn{1}{l|}{\ccol Ours} & \ccol \textbf{46.5} & \ccol 97.7 & \ccol 74.8 & \ccol \textbf{83.8} & \ccol 65.0 & \ccol 42.5 & \ccol \underline{68.4} \\

\hline

    \end{tabularx}
    }
\label{tab:domainbed}
\end{table*}

%
 \noindent \textbf{DomainBed.}
We also conduct extensive experiments on the DomainBed~\cite{gulrajani2020search} which is a testbed for domain generalization to compare state-of-the-art methods across several benchmark datasets. The rationale behind the DomainBed is that the domain generalization performances are too much dependent on the hyperparameter tuning. For a fair comparison, we follow its standard protocols for training and evaluation.

\noindent \textbf{Results.} As shown in Table.~\ref{tab:domainbed}, our method generally shows competitive performances and ranks second out of 15 methods on average accuracy. In particular, on CMNIST, our method substantially outperforms other competing methods. Since CMNIST is designed to simulate the domain shift by correlating the digit colors with the class labels, we conjecture that our improvement on CMNIST is attributed to the de-stylization effect of UniStyle, which would help the model decorrelate between the colors and labels.

\subsection{Generalization in person re-ID}
\label{dg_reid}
In this section, we further evaluate our method on person re-identification (re-ID), which is to match pedestrians across non-overlapping camera views.



\noindent \textbf{Experimental setup.}
Here, we address domain generalization for person re-ID, where the test data is collected from cameras of the unseen dataset rather than from those of the training dataset. Specifically, the model trained to match people in the source dataset is evaluated by how well it matches pedestrian data of the unseen test set, which are disjoint from the source dataset. For datasets, we adopt two widely-used benchmarks: Market1501 (Market)~\cite{zheng2015scalable} and DukeMTMC-reID (Duke)~\cite{ristani2016performance,zheng2017unlabeled}. We use 32,668 images of 1,501 identities collected from 6 cameras and 36,411 images of 1,812 identities from 8 cameras for Market1501 and Duke, respectively. As for performance measures, we adopt mean average precision (mAP) and Recall@K (R@K). Following the prior work~\cite{zhou2021domain}, we adopt ResNet-50~\cite{resnet} as a backbone architecture. In these experiments, we apply UniStyle to the 1st, 2nd, and 3rd residual blocks of a model.





























    






\begin{table}[!t]
    \centering
\caption{
        Generalization results on the cross-dataset person re-ID.
    }
    \fontsize{8}{10}\selectfont
    \begin{tabularx}{0.75 \textwidth}{
       >{\centering\arraybackslash}X|
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X|
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X}
    
\hline
    \multicolumn{1}{l|}{} &
    \multicolumn{2}{c|}{Market  Duke} & \multicolumn{2}{c}{Duke  Market} \\
    \multicolumn{1}{l|}{Methods} & {mAP} & {R@1} &  {mAP} & {R@1} \\
    
    \hline
    






    \multicolumn{1}{l|}{ResNet-50} & 19.3 & 35.4 & 20.4 & 45.2\\
    \multicolumn{1}{l|}{RandomErase~\cite{zhong2020random}} & 14.3 & 27.8 & 16.1 & 38.5\\
    \multicolumn{1}{l|}{DropBlock~\cite{ghiasi2018dropblock}} & 18.2 & 33.2  & 19.7 & 45.3\\


    \multicolumn{1}{l|}{MixStyle~\cite{zhou2021domain}} & 23.4 & 43.3 & 24.7 & 53.0 \\
    
    \multicolumn{1}{l|}{StyleNeophile~\cite{StyleNeophile}} & \underline{26.3} & \underline{46.5} & \underline{27.2} & \underline{55.0} \\
    




    \multicolumn{1}{l|}{\ccol Ours} & \ccol \textbf{27.4} & \ccol \textbf{49.3} & \ccol \textbf{30.1} & \ccol \textbf{59.0} \\


    
\hline
    
    \end{tabularx}
\label{tab:person_reid}
\end{table} \noindent \textbf{Comparison to other regularization methods.}
As shown in Table.~\ref{tab:person_reid}, our method substantially outperforms other methods in mAP and Recall@1. Although RandomErase and Dropblock are effective for learning discriminative features, they fail to improve performance when encountering unseen domain data. Furthermore, by exploiting inter-class relations provided by different cameras, our method shows its superiority over MixStyle and StyleNeophile which are designed for domain generalization but utilizes one-hot labels only.

\begin{table}[!t]
    \centering
    \caption{
    mIoU (\%) results on the cross-dataset semantic segmentation. GTA5 is for training, and Cityscapes, SYNTHIA, BDD, and Mapillary are test sets.
}
    \fontsize{8}{10}\selectfont
    \begin{tabularx}{0.9 \textwidth}{
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X}
    \hline
    \multicolumn{1}{l|}{Methods (GTA5)} & Cityscapes & BDD & Mapillary & SYNTHIA \\
    \hline
    \multicolumn{1}{l|}{DeepLabV3+~\cite{chen2018encoder}} & 28.9 & 25.1 & 28.1 & 26.2 \\
    \multicolumn{1}{l|}{SW~\cite{pan2019switchable}} & 29.9 & 27.4 & 29.7 & 27.6 \\
    \multicolumn{1}{l|}{DRPC~\cite{yue2019domain}} & \underline{37.4} & 32.1 & 34.1 & \underline{28.0} \\
\multicolumn{1}{l|}{RobustNet~\cite{choi2021robustnet}} & 36.5 & \textbf{35.2} & \textbf{40.3} & \textbf{28.3} \\
    \multicolumn{1}{l|}{\ccol Ours} & \ccol \textbf{39.2} & \ccol \underline{32.4} & \ccol \underline{37.1} & \ccol \underline{28.0} \\
    \hline
    \end{tabularx}
    \label{tab:dg_segmentation}
\end{table}

\subsection{Generalization in semantic segmentation}
\label{dg_segmentation}
\noindent \textbf{Experimental setup.} Lastly, to investigate whether our method can be extended to the dense prediction task, evaluation on semantic segmentation is addressed here. Following the mainstream protocol, we train models on a synthetic dataset and evaluate them on several datasets which mainly belong to real-world. Specifically, we adopt GTA5~\cite{richter2016playing} as a source dataset which consists of 24,966 images. For target datasets, Cityscapes~\cite{cityscapes}, BDD~\cite{yu2018bdd100k}, and Mapillary~\cite{neuhold2017mapillary} are real-world datasets whose image sizes are 5,000, 10,000, and 25,000, respectively. Lastly, SYNTHIA~\cite{ros2016synthia} has 9,400 images. Note that ResNet-50 is used as the backbone and the common 19 classes are used across all datasets.

\noindent \textbf{Results.}
We remark that XDED constructs an ensemble by simply averaging all the logits from the pixels whose gt is the same in a mini-batch. As shown in Table~\ref{tab:dg_segmentation}, ours outperforms the competing methods overall, even if those are dedicated to this task only. We show that our method can be extended to the pixel-wise classification with little modification on XDED. Also, the results support our claim that our method is simple yet effective in a wide range of tasks.


\subsection{In-depth Analysis}
\label{sec:experiment_ablation}
\begin{table}[!t]
\centering
\caption{
Ablation study of the proposed components on cross-domain tasks of image classification (Accuracy) and person re-ID (mAP).
}
\fontsize{8}{10}\selectfont
\begin{tabularx}{0.8 \textwidth}{
   >{\centering\arraybackslash}X
   >{\centering\arraybackslash\hsize=.5\hsize}X
   >{\centering\arraybackslash\hsize=.5\hsize}X|
   >{\centering\arraybackslash}X}
\hline

\multicolumn{1}{l|}{\multirow{1}{*}{Methods}}& \multicolumn{1}{c}{Art} & \multicolumn{1}{c|}{Clipart} & {Market  Duke}\\ 







\hline
\multicolumn{1}{l|}{Vanilla}  & 77.0 & 49.4 & 19.3 \\
\multicolumn{1}{l|}{w/ UniStyle}  & 81.2 & 50.4 & \underline{26.2} \\
\multicolumn{1}{l|}{w/ XDED}  & \underline{83.3} & \underline{55.2} & 24.2 \\
\multicolumn{1}{l|}{\ccol Ours}  & \ccol \textbf{85.6} & \ccol \textbf{57.1} & \ccol \textbf{27.4} \\

\hline

\end{tabularx}
\label{tab:ablation_components}
\end{table} \noindent \textbf{Ablation study.} To investigate the impact of each component in our method, we conduct an ablation study which is summarized in Table~\ref{tab:ablation_components}. The result reveals that two components are complementary and consistently help the model improve the generalization ability. For image classification, XDED contributes most to the performance, and UniStyle boosts the effect of XDED. On both domains, XDED uniformly improves the vanilla method by about 6\%, whereas UniStyle shows different degrees of improvement. It is because the image style discrepancy between domains in OfficeHome is less severe than that in PACS. Interestingly, for the task of person re-ID, UniStyle reveals more impact than does XDED. Due to the inherent characteristics of the task itself, the effect of XDED on collecting meaningful knowledge of the same pedestrian from different cameras may become less significant.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.92\textwidth]{CameraReady/figure/cr_fig4.pdf}
    \caption{Visualization results of the loss landscapes incorporating the vanilla method and XDED on the PACS dataset. Note that each loss landscape is visualized on the data of source domains, not the data of the marked target domain. Blue and red surfaces are from the vanilla method and XDED, respectively.
    }
\label{fig:loss_surface}
\end{figure}

\noindent \textbf{Loss surface visualization.}
To further illustrate how XDED leads to flat minima in the loss landscapes, we provide qualitative results that visualize the loss landscapes. Following \cite{cha2020cpr}, we plot the loss landscapes on data of source domains per each case by perturbing the model parameters across the first and second Hessian eigenvectors which are provided by PyHessian~\cite{yao2020pyhessian} which is a framework for Hessian-based analysis of neural networks. As shown in Fig.~\ref{fig:loss_surface}, we observe that the loss landscapes incorporating XDED clearly become flatter than those incorporating the vanilla method for all cases. We argue that these qualitative results also consistently support that XDED promotes flat minima.



\begin{table}[!t]
    \centering
    \caption{
    Multi-source domain generalization accuracy (\%) on Photo of PACS before and after applying given adversarial attacks.
    }
    \fontsize{8}{10}\selectfont
    \begin{tabularx}{0.75 \textwidth}{
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X}
    \hline
    \multicolumn{1}{p{33mm}|}{Methods} & Photo & w/ FGSM & w/ PGD \\ \hline
\multicolumn{1}{l|}{ResNet-18} & 96.0 & 39.6 & 16.3\\
    \multicolumn{1}{l|}{Label smoothing~\cite{szegedy2016rethinking}} & 95.6 & 43.5 & 20.2\\
\multicolumn{1}{l|}{Mixup~\cite{zhang2017mixup}} & 95.8 & 46.5 & 21.9\\
    \multicolumn{1}{l|}{Manifold mixup~\cite{verma2019manifold}} & 93.5 & \underline{46.6} & \underline{23.8}\\

    \multicolumn{1}{l|}{MixStyle~\cite{zhou2021domain}} & \underline{96.1} & 41.4 & 22.7\\


\multicolumn{1}{l|}{ \ccol Ours} & \ccol \textbf{96.5} & \ccol \textbf{55.4} & \ccol \textbf{30.4}\\

    \hline
    \end{tabularx}
    \label{tab:comparison_adversarial_attack}
    \vspace{-2mm}
\end{table} \noindent \textbf{Robustness to adversarial examples.}
Recent studies have demonstrated that convergence on flat minima strengthens the adversarial robustness~\cite{wu2020adversarial,stutz2021relating}. To revalidate that our method promotes flat minima, we evaluate the adversarial robustness of learned models. Specifically, we trained models on source domains and added adversarial perturbations on images of the unseen target domain by using existing adversarial attack methods: FGSM~\cite{goodfellow2014explaining} and PGD~\cite{madry2018towards}. Table~\ref{tab:comparison_adversarial_attack} shows that our method outperforms other regularization methods in terms of robustness against both unseen data and adversarial attacks. Considering that adversarial attacks are made to maximize the loss value, we argue that our superiority in adversarial robustness is also attributed to the capability of promoting flat minima as desired, even though our method has no direct connection to adversarial training.

\begin{table}[!t]
\centering
    \caption{
    Average classification error (\%) on the corruption benchmarks.
}
    \fontsize{8}{10}\selectfont
    \begin{tabularx}{0.7 \textwidth}{
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X
       >{\centering\arraybackslash}X}
    \hline
    \multicolumn{1}{p{21mm}|}{Methods} & CIFAR-10-C & CIFAR-100-C \\
    \hline
    \multicolumn{1}{l|}{40-2 WRN~\cite{zagoruyko2016wide}} & 26.9 & 53.3 \\
    \multicolumn{1}{l|}{Cutout~\cite{devries2017improved}} & 26.8 & 53.5 \\
    \multicolumn{1}{l|}{Mixup~\cite{zhang2017mixup}} & 22.3 & 50.4 \\
    \multicolumn{1}{l|}{CutMix~\cite{yun2019cutmix}} & 27.1 & 52.9 \\
    \multicolumn{1}{l|}{AutoAug~\cite{cubuk2018autoaugment}} & 23.9 & 49.6 \\
    \multicolumn{1}{l|}{AugMix~\cite{hendrycks2019augmix}} & \textbf{11.2} & \textbf{35.9} \\

    \multicolumn{1}{l|}{ \ccol Ours} & \ccol \underline{18.5} & \ccol \underline{46.6} \\
    \hline
    \end{tabularx}
    \label{tab:comparison_corruption}
\end{table} \noindent \textbf{Results on corruption benchmarks.} We further measure the resilience of learned models to image corruptions. Following the protocol provided by \cite{hendrycks2019benchmarking}, we trained models on the original training dataset, and evaluated them on the test dataset constructed by corrupting the original test dataset through predefined corruption types. Table.~\ref{tab:comparison_corruption} shows that our method outperforms all regularization methods except AugMix~\cite{hendrycks2020augmix}. Considering AugMix is a state of the art that is dedicated to corruption robustness while ours is not, we argue that our method still shows its significant robustness against image corruptions. \section{Conclusion}
We have presented a simple yet effective framework for domain generalization. XDED first generates an ensemble of output distributions for the data with the same label but from different domains, and then penalizes each output distribution for the mismatch with the ensemble in the form of self-knowledge distillation. With this approach, our model can learn domain-invariant features and also easily converges to flat minima. Besides, the proposed UniStyle suppresses domain-specific style bias to boost the effect of XDED and encourage style-consistent predictions. Furthermore, we empirically validate the generalization ability of the proposed method from the perspective of flat minima and reduced divergence between source and target. Through extensive experimental results, we demonstrate the superiority of the proposed framework. 



\vfill
{\small
\noindent \textbf{Acknowledgement.} 
This work was supported by 
the NRF grant and  the IITP grant funded by Ministry of Science and ICT, Korea
(NRF-2021R1A2C3012728,  IITP-2019-0-01906,     IITP-2022-0-00926,     IITP-2022-0-00290).    }
 



\bibliographystyle{splncs04}
\bibliography{cvlab_kwak}
\end{document}