



\documentclass[12pt]{article}

\usepackage{tikz}  \usepackage{amsmath}\usepackage{lmodern}

\usepackage{hyperref}\hypersetup{breaklinks,ocgcolorlinks,
   colorlinks=true,linkcolor=[rgb]{0.45,0.0,0.0},citecolor=[rgb]{0,0,0.45}
}

\usepackage[hyperref, amsmath,thmmarks]{ntheorem}
\theoremseparator{.}


\usepackage{titlesec}
\titlelabel{\thetitle. }

\usepackage{graphicx}
\usepackage{ifpdf}
\usepackage{xspace}
\usepackage{paralist}
\usepackage{picins}
\usepackage[noend]{algorithmic}
\usepackage{xspace}
\usepackage{color}

\usepackage{euscript}
\usepackage{amssymb}


\usepackage{wide}

\DefineNamedColor{named}{RedViolet} {cmyk}{0.07,0.90,0,0.34}

\newcommand{\AlgorithmI}[1]{{\textcolor[named]{RedViolet}{\texttt{\bf{#1}}}}}
\newcommand{\Algorithm}[1]{{\AlgorithmI{#1}\index{algorithm!#1@{\AlgorithmI{#1}}}}}

\newcommand{\newtext}[1]{\textcolor{red}{#1}}

\definecolor{darkblue}{rgb}{0,0,0.35}

\newcommand{\Snewtext}[1]{\textcolor{darkblue}{#1}}



\newcommand{\XSays}[2]{{
      {\fbox{\tt
            #1:} }
      #2
      \marginpar{#1}
      {\fbox{\tt
            end}}
   }
}


\DefineNamedColor{named}{OliveGreen}{cmyk}{0.64, 0, 0.95, 0.40}
\providecommand{\ComplexityClass}[1]{{{\textcolor[named]{OliveGreen}{\textsc{#1}}}}}
\providecommand{\NPHard}{{\ComplexityClass{NP-Hard}}\index{NP!hard}\xspace}


\newcommand{\Anne}[1]{{\XSays{Anne}{#1}}}




\newtheorem{theorem}{Theorem}[section]


\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}\newtheorem{fact}[theorem]{Fact} \newtheorem{lemma}[theorem]{Lemma}


\newtheorem{datastructure}[theorem]{Data-Structure}

\newtheorem{observation}[theorem]{Observation}
\newtheorem{invariant}[theorem]{Invariant}
\newtheorem{question}[theorem]{Question} 

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{openproblem}[theorem]{Open Problem}




\theorembodyfont{\rm}
\theoremheaderfont{\normalfont\bfseries}
\newtheorem{remark}[theorem]{Remark}\newtheorem{defn}[theorem]{Definition}
\theoremheaderfont{\em}\theorembodyfont{\upshape}\theoremstyle{nonumberplain}\theoremseparator{}\theoremsymbol{\rule{2mm}{2mm}}\newtheorem{proof}{{P}roof:}


\newcommand{\atgen}{\symbol{'100}}
\newcommand{\SarielThanks}[1]{\thanks{Department of Computer
      Science; 
      University of Illinois; 
      201 N. Goodwin Avenue;
      Urbana, IL, 61801, USA;
      {\tt sariel\atgen{}uiuc.edu}; {\tt
         {{http://www.uiuc.edu/\string~sariel/}}.} #1}}

\providecommand{\si}[1]{#1}

\newcommand{\apndlab}[1]{\label{app:#1}}
\newcommand{\apndref}[1]{Appendix~\ref{app:#1}}

\newcommand{\seclab}[1]{\label{sec:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\secrefpage}[1]{Section~\ref{sec:#1}}
        

\newcommand{\thmlab}[1]{{\label{theo:#1}}}
\newcommand{\thmref}[1]{Theorem~\ref{theo:#1}}
\newcommand{\thmrefpage}[1]{Theorem~\ref{theo:#1}}

\newcommand{\remlab}[1]{\label{rem:#1}}
\newcommand{\remref}[1]{Remark~\ref{rem:#1}}
\newcommand{\remrefpage}[1]{Remark~\ref{rem:#1}}

\newcommand{\lemlab}[1]{\label{lemma:#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lemma:#1}}
\newcommand{\lemrefpage}[1]{Lemma~\ref{lemma:#1}}

\newcommand{\figlab}[1]{\label{fig:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}

\newcommand{\deflab}[1]{\label{def:#1}}
\newcommand{\defref}[1]{Definition~\ref{def:#1}}

\newcommand{\eqlab}[1]{\label{equation:#1}}
\newcommand{\Eqref}[1]{Eq.~(\ref{equation:#1})}
\newcommand{\Eqrefpage}[1]{Eq.~(\ref{equation:#1})}

\newcommand{\factref}[1]{Fact~\ref{fact:#1}}
\newcommand{\factlab}[1]{\label{fact:#1}}

\newcommand{\dsref}[1]{Data-Structure~\ref{ds:#1}}
\newcommand{\dslab}[1]{\label{ds:#1}}

\newcommand{\obslab}[1]{\label{observation:#1}}
\newcommand{\obsref}[1]{Observation~\ref{observation:#1}}

\newcommand{\itemlab}[1]{\label{item:#1}}
\newcommand{\itemref}[1]{(\ref{item:#1})}

\newcommand{\corlab}[1]{\label{cor:#1}}
\newcommand{\corref}[1]{Corollary~\ref{cor:#1}}

\newcommand{\aseclab}[1]{\label{a:sec:#1}}
\newcommand{\asecref}[1]{Appendix Section~\ref{a:sec:#1}}

\newcommand{\athmlab}[1]{{\label{a:theo:#1}}}
\newcommand{\athmref}[1]{Theorem~\ref{a:theo:#1}}

\newcommand{\alemlab}[1]{\label{a:lemma:#1}}
\newcommand{\alemref}[1]{Lemma~\ref{a:lemma:#1}}

\newcommand{\afiglab}[1]{\label{a:fig:#1}}
\newcommand{\afigref}[1]{Figure~\ref{a:fig:#1}}

\newcommand{\adeflab}[1]{\label{a:def:#1}}
\newcommand{\adefref}[1]{Definition~\ref{a:def:#1}}

\newcommand{\aeqlab}[1]{\label{a:equation:#1}}
\newcommand{\aEqref}[1]{Eq.~(\ref{a:equation:#1})}

\newcommand{\afactref}[1]{Fact~\ref{a:fact:#1}}
\newcommand{\afactlab}[1]{\label{a:fact:#1}}

\newcommand{\aobslab}[1]{\label{a:observation:#1}}
\newcommand{\aobsref}[1]{Observation~\ref{a:observation:#1}}

\newcommand{\aitemlab}[1]{\label{a:item:#1}}
\newcommand{\aitemref}[1]{(\ref{a:item:#1})}





\newcommand{\cardin}[1]{\left| {#1} \right|}

\newcommand{\myqedsymbol}{\rule{2mm}{2mm}}



\newcommand{\aftermathA}{\par\vspace{-\baselineskip}}

\newcommand{\pbrc}[2][\!\!]{#1\left[ {#2} \MakeBig \right]}

\newcommand{\ts}{\hspace{0.6pt}}

\newcommand{\IncGraphPage}[4][]{ \ifpdf
   \IncludeGraphics[page=#4,#1]{\File{#2/#3}}
   \else
   \IncludeGraphics[#1]{\File{#2/#3/#3_#4}}
   \fi } \newcommand{\IncludeGraphics}[2][]{\includegraphics[#1]{#2}
}
\newcommand{\File}[1]{#1}
\newcommand{\etal}{\textit{et~al.}\xspace}

\newcommand{\emphind}[1]{\emph{#1}\index{#1}}
\definecolor{blue25}{rgb}{0,0,0.55}
\newcommand{\emphic}[2]{\textcolor{blue25}{\textbf{\emph{#1}}}\index{#2}}

\newcommand{\emphi}[1]{\emphic{#1}{#1}}




\newcommand{\emphOnly}[1]{\emph{\textcolor{blue25}{\textbf{#1}}}}


\newcommand{\Frechet}{Fr\'{e}c{h}e{}t\xspace}\providecommand{\Arr}{\mathop{\mathrm{\EuScript{A}}}}
\newcommand{\ArrVD}{\mathop{\mathrm{\Arr_{||}}}}
\newcommand{\ArrX}[1]{\mathop{\mathrm{\EuScript{A}}}\pth{#1}}

\newcommand{\widthX}[2]{{\mathrm{width}}_{#1}\pth{#2}}
\newcommand{\distf}[3]{\mathsf{d}_{#1}\pth{#2, #3}}
\newcommand{\distFr}[2]{\mathsf{d}_{\EuScript{F}}\pth{#1, #2}}
\newcommand{\distSFr}[3]{\mathsf{d}_{\EuScript{S}}\pth{#1,#2, #3}}
\newcommand{\distoSFr}[2]{\mathsf{d}_{\EuScript{S}}\pth{#1,#2}}

\newcommand{\distSFrSym}[3]{\mathsf{d}_{\EuScript{F}_{s}
      \leftrightarrow}\pth{#1,#2, #3}}\newcommand{\distCmd}[1]{\left\| {#1} \right\|}
\newcommand{\distX}[2]{\distCmd{#1 - #2}}

\newcommand{\LeftOver}{L}
\newcommand{\Starts}{S}
\newcommand{\Reached}{R}
\newcommand{\Queue}{Q}


\newcommand{\pnt}{\mathsf{p}}
\newcommand{\pntA}{\mathsf{q}}
\newcommand{\pntB}{\mathsf{r}}
\newcommand{\pntBa}{\mathsf{r'}}
\newcommand{\pntC}{\mathsf{u}}
\newcommand{\pntCa}{\mathsf{u}'}
\newcommand{\pntCb}{\mathsf{u}''}
\newcommand{\pntD}{\mathsf{v}}
\newcommand{\pntE}{\mathsf{z}}
\newcommand{\pntF}{\mathsf{f}}
\newcommand{\pntG}{\mathsf{g}}


\newcommand{\x}{x}
\newcommand{\y}{y}

\newcommand{\ACoord}[1]{\x_{#1}}
\newcommand{\BCoord}[1]{\y_{#1}}


\newcommand{\xPnt}{\ACoord{\pnt}}
\newcommand{\yPnt}{\BCoord{\pnt}}

\newcommand{\xPntA}{\ACoord{\pntA}}
\newcommand{\yPntA}{\BCoord{\pntA}}

\newcommand{\xPntB}{\ACoord{\pntB}}
\newcommand{\yPntB}{\BCoord{\pntB}}

\newcommand{\xPntBa}{\ACoord{\pntBa}}
\newcommand{\yPntBa}{\BCoord{\pntBa}}

\newcommand{\xPntC}{\ACoord{\pntC}}
\newcommand{\yPntC}{\BCoord{\pntC}}

\newcommand{\xPntCa}{\ACoord{\pntCa}}
\newcommand{\yPntCa}{\BCoord{\pntCa}}

\newcommand{\xPntCb}{\ACoord{\pntCb}}
\newcommand{\yPntCb}{\BCoord{\pntCb}}

\newcommand{\xPntD}{\ACoord{\pntD}}
\newcommand{\yPntD}{\BCoord{\pntD}}

\newcommand{\opt}{\mathrm{opt}}

\newcommand{\IEdge}[2]{\mathcal{I}_{\mathrm{edge}}\pth{#1, #2}}



\providecommand{\nfrac}[2]{#1/#2}



\newcommand{\sccurveA}{\overline{\pi}}
\newcommand{\sbcurveA}{\widehat{\overline{\pi}}}

\newcommand{\SimplifyX}[1]{#1}
\newcommand{\SubCurvifyX}[1]{\widehat{#1}}

\newcommand{\SubCurvifyY}[1]{{#1}}

\newcommand{\subcZ}{\SubCurvifyX{\cZ}}
\newcommand{\crvs}{\SimplifyX{\cZ}}\newcommand{\cQ}{\mathsf{Q}}
\newcommand{\cS}{\mathsf{S}}
\newcommand{\cZ}{{Z}}


\newcommand{\ShortcutX}[1]{#1'}


\newcommand{\cXBase}{X} \newcommand{\cYBase}{Y} \newcommand{\crvCBase}{{\pi}}
\newcommand{\crvDBase}{{\eta}}


\newcommand{\cX}{\SimplifyX{\cXBase}}
\newcommand{\cXOrig}{\pmb{\cXBase}}
\newcommand{\subCXS}{\SubCurvifyY{\cXS}}
\newcommand{\subcX}[1]{\SubCurvifyY{\cX}_{#1}}
\newcommand{\subcXOrig}[1]{\SubCurvifyY{\cXOrig}_{#1}}

\newcommand{\cY}{\SimplifyX{\cYBase}}
\newcommand{\cYOrig}{\pmb{\cYBase}}
\newcommand{\cYSC}{\ShortcutX{\cYBase}}
\newcommand{\subsegYS}{\overline{\cY}}
\newcommand{\subsegY}[1]{\overline{\cY}_{#1}}
\newcommand{\subsegYOrig}[1]{\overline{\cYOrig}_{#1}}
\newcommand{\subcY}[1]{\SubCurvifyY{\cY}_{#1}}
\newcommand{\subcYOrig}[1]{\SubCurvifyY{\cYOrig}_{#1}}


\newcommand{\cubcY}{\SubCurvifyX{\cY}}




\newcommand{\crvC}{\pmb{\crvCBase}}
\newcommand{\crvCSimp}{\SimplifyX{\crvCBase}}
\newcommand{\subsegCSimp}{\overline{\crvCSimp}}
\newcommand{\subsegC}{\overline{\crvC}}
\newcommand{\subcC}{\SubCurvifyX{\crvC}}
\newcommand{\subcCX}[1]{\SubCurvifyX{\crvC_{#1}}}


\newcommand{\crvD}{\pmb{\crvDBase}}
\newcommand{\crvDSimp}{\SimplifyX{\crvDBase}}
\newcommand{\subcDSimp}{\SubCurvifyX{\crvDSimp}}
\newcommand{\subcD}{\SubCurvifyX{\crvD}}
\newcommand{\subcDX}[1]{\SubCurvifyX{\crvD_{#1}}}



\newcommand{\sbcurveB}{\widehat{\overline{\sigma}}}
\newcommand{\subcurveD}{\widehat{\eta}}

\newcommand{\sccurveD}{\overline{\eta}}

\newcommand{\linelab}[1]{\label{line:#1}}
\newcommand{\lineref}[1]{Line~\ref{line:#1}}



\newcommand{\seg}{\mathsf{h}}
\newcommand{\segA}{\mathsf{u}}
\newcommand{\subsegA}{\widehat{\mathsf{u}}}
\newcommand{\segB}{\mathsf{v}}
\newcommand{\subsegB}{\widehat{\mathsf{v}}}

\newcommand{\ball}[2]{B(#1,#2)}
\newcommand{\BallC}{\mathbf{b}}
\newcommand{\BallX}[1]{\mathbf{b}\pth{#1}}
\newcommand{\SphereX}[1]{\mathbb{S}\pth{#1}}

\newcommand{\lenX}[1]{\left\|{#1} \right\|}
\newcommand{\pntOn}[2]{#1[{#2}]}
\newcommand{\FDXxxx}{\EuScript{D}}
\newcommand{\VDleq}[1]{V_{\leq #1}}
\newcommand{\FDleqXR}[2]{\ensuremath{\EuScript{R}_{\leq #1}(#2)}}
\newcommand{\FDleqXL}[1]{\ensuremath{\EuScript{R}_{\leftarrow}(#1)}}
\newcommand{\FDleq}[1]{\FDleqC_{\leq #1}}
\newcommand{\FDleqI}[2]{\ensuremath{\EuScript{R}^{#1}_{\leq #2}}}


\newcommand{\ShowComments}[1]{}

\newcommand{\FullFDleqC}{\EuScript{D}}
\newcommand{\FullFDleq}[1]{\FullFDleqC_{\leq #1}}
\newcommand{\NleqC}{\EuScript{N}}
\newcommand{\Nleq}[1]{\NleqC_{\leq #1}}
\newcommand{\NleqApprox}[4]{{\NleqC'_{\leq #1}}\pth{#3,#4, #2}}

\newcommand{\BEventsC}{\nabla}
\newcommand{\BEvents}[1]{\BEventsC_{#1}}

\newcommand{\Graph}{\mathsf{G}}
\newcommand{\Cone}{\mathcal{V}}
\newcommand{\Family}{\mathcal{F}}
\providecommand{\pth}[2][\!]{#1\left({#2}\right)}
\providecommand{\brc}[1]{\left\{ {#1} \right\}}
\providecommand{\sep}[1]{\,\left|\, {#1} \MakeBig\right.}
\providecommand{\MakeBig}{\rule[-.2cm]{0cm}{0.4cm}}

\newcommand{\diameterX}[2][\!]{\mathrm{d{}i{}am}\pth[#1]{#2}}
\newcommand{\constA}{c_1}
\newcommand{\constB}{c_2}
\newcommand{\constC}{c_3}
\newcommand{\constD}{c_4}
\newcommand{\constE}{c_5}
\newcommand{\constF}{c_6}
\newcommand{\constG}{c_7}
\newcommand{\constH}{c_8}
\newcommand{\constI}{c_8}


\newcommand{\PntSet}{{\mathsf{P}}}
\newcommand{\PntSetA}{{\mathsf{A}}}
\newcommand{\PntSetB}{{\mathsf{B}}}

\newcommand{\sRadius}{\mu}
\newcommand{\Radius}{\mathsf{r}}
\newcommand{\Grid}{\EuScript{G}}
\newcommand{\CellXY}[2]{C_{#1,#2}}
\newcommand{\ICellXY}[2]{I_{#1,#2}}
\newcommand{\MICellXY}[2]{M_{#1,#2}}

\providecommand{\MakeBig}{\rule[-.2cm]{0cm}{0.4cm}}
\providecommand{\MakeSBig}{\rule[0.0cm]{0.0cm}{0.35cm}} 

\newcommand{\IVCellXY}[2]{I^v_{#1,#2}}
\newcommand{\IHCellXY}[2]{I^h_{#1,#2}}
\newcommand{\RVCellXY}[2]{R^v_{#1,#2}}
\newcommand{\RHCellXY}[2]{R^h_{#1,#2}}

\newcommand{\KSet}{\EuScript{K}}
\newcommand{\veEvents}{Z}
\newcommand{\sEvents}{Z}

\providecommand{\eps}{{\varepsilon}}\renewcommand{\Re}{{\rm I\!\hspace{-0.025em} R}}
\newcommand{\reals}{\Re}
\newcommand{\NULL}{\ensuremath{\mathsf{null}}}

\providecommand{\ceil}[1]{\left\lceil {#1} \right\rceil}
\providecommand{\floor}[1]{\left\lfloor {#1} \right\rfloor}

\newcommand{\WSPDRep}{\ensuremath{\mathcal{W}}}
\newcommand{\WSPD}{\textsf{WSPD}\xspace}

\newcommand{\listX}[1]{\mathrm{list}\pth{#1}}
\newcommand{\nodeX}[1]{\mathrm{node}\pth{#1}}
\newcommand{\QTree}{\EuScript{T}}
\newcommand{\GridBall}[1]{\mathrm{region}\pth{#1}}
\newcommand{\parent}{\overline{\mathrm{p}}}
\newcommand{\parentX}[1]{\parent\pth{#1}}

\newcommand{\density}{\phi}
\renewcommand{\th}{t{}h\xspace}

\newcommand{\concatOp}{\oplus}
\newcommand{\pbrcZ}[1]{\left[ \MakeSBig {#1} \right]}





\newcommand{\SC}[3]{{#1}\!\!\ts\ts \left\langle  #2, #3 \right\rangle}

\newcommand{\SubCrv}[3]{{#1}\!\!\ts\ts\pbrcZ{#2, #3}}
\newcommand{\ScutCrv}[3]{\overline{#1}\!\pbrcZ{#2, #3}}
\newcommand{\SpCrv}[2]{{#1}_{#2}}

\newcommand{\Tree}{T}
\newcommand{\node}{\nu}

\newcommand{\VertexSet}[1]{{V}\pth{#1}}
\newcommand{\VtxSet}{{V}}

\newcommand{\cNode}[1]{\mathrm{c{}r}\pth{#1}}
\newcommand{\pNode}[1]{\mathrm{p{}e{}r{}m}\pth{#1}}
\newcommand{\kNode}[2]{\mathrm{s{}i{}m{}p{}l_{#1}}\pth{#2}}
\newcommand{\segNode}[1]{\mathrm{s{e}g}\pth{#1}}
\newcommand{\frNode}[1]{\mathrm{d}_{#1}}
\newcommand{\kFrNode}[2]{\mathrm{f{}r_{#1}}\pth{#2}}
\newcommand{\lNode}[1]{\mathrm{l}\pth{#1}}

\newcommand{\VertexInd}[2]{v_{#1}\pbrc{#2}}
\newcommand{\dEdge}[2]{{#1 \rightarrow #2}}


\newcommand{\ObjSet}{\mathsf{X}}
\newcommand{\levelC}{\mathsf{l}}
\newcommand{\levelX}[1]{\levelC\pth{#1}}
\newcommand{\Simplex}{\rho}
\newcommand{\GridLevel}[1]{G_{#1}}
\newcommand{\CellSetX}[1]{C\pth{#1}}
\newcommand{\Cube}{\mathsf{C}}
\newcommand{\Cell}{\Box}
\newcommand{\CellX}[1]{\Cell_{#1}}
\newcommand{\query}{{\mathtt{q}}}
\newcommand{\Term}[1]{\textsf{#1}}
\newcommand{\ANN}{\Term{ANN}\index{approximate nearest neighbor}\xspace}
\newcommand{\Na}{{\rm I\!\hspace{-0.025em} N}}
\newcommand{\simpX}[1]{\mathrm{s{i}m{p}l}\pth{#1}}
\newcommand{\spineX}[1]{\mathrm{s{}p{}i{}n{}e}\pth{#1}}


\DefineNamedColor{named}{RedViolet}{cmyk}{0.07,0.90,0,0.34}


\newcommand{\tunnelTest}{\Algorithm{tunnel}\xspace}
\newcommand{\kdeciderFr}{\Algorithm{-decider}\xspace}
\newcommand{\deciderFr}{\Algorithm{decider}\xspace}
\newcommand{\DeciderFr}{\Algorithm{Decider}\xspace}
\newcommand{\kDeciderFr}{\Algorithm{-Decider}\xspace}
\newcommand{\approxFr}{\Algorithm{a{}p{}r{}x{}\Frechet}\xspace}

\newcommand{\BFS}{\Algorithm{B{F}S}\xspace}



\newcommand{\edge}{\mathsf{e}}

\newcommand{\frVal}[1]{\mathsf{d}\pth{#1}}

\newcommand{\constSC}{3}
\newcommand{\constDec}{4}

\newcommand{\XPath}{P}


\newcommand{\tunnel}{tunnel\xspace}
\newcommand{\Tunnel}{Tunnel\xspace}
\newcommand{\tunnels}{tunnels\xspace}
\newcommand{\Tunnels}{Tunnels\xspace}

\newcommand{\tunnelLtr}{\mathsf{\tau}}
\newcommand{\xtunnel}[2]{\tunnelLtr\pth{ #1,  #2}}
\newcommand{\xTunnels}[4]{\mathcal{T}\pth{#1, #2, #3, #4}}
\newcommand{\xTunnelsLeq}[5]{\mathcal{T}_{\leq #5}\pth{#1, #2, #3, #4}}

\newcommand{\scPrice}[2]{{\mathrm{p{r}c}}\pth{\xtunnel{#1}{#2}}}


\newcommand{\rCreate}[4]{\mathrm{r}_{\mathrm{crt}}\pth{#1, #2, #3,#4}}
\newcommand{\rMinCreate}[4]{\mathrm{r}_{\min}\pth{#1, #2, #3,#4}}
\newcommand{\bCanonical}[4]{\mathrm{\tau}_{\min}\pth{#1, #2, #3,#4}}

\newcommand{\pShortcut}[3]{\mathrm{p{r}c}_{\min}\pth{#1, #2, #3}}
\newcommand{\pShortcutC}{\gamma}
\newcommand{\priceX}[1]{\mathrm{p{r}c}\pth{#1}}
\newcommand{\chordX}[1]{\mathrm{chord}\pth{#1}}
\newcommand{\rMonotone}[3]{r_{\mathrm{mono}}\pth{#1, #2, #3}}
\newcommand{\Interval}{\mathcal{I}}
\newcommand{\distSet}[2]{\mathsf{d}\pth{#1, #2}}

\newcommand{\AlphaBetaCovered}{-covered\xspace}

\newcommand{\nn}[2]{\mathrm{n{}n}\pth{#1, #2}}
\newcommand{\bisector}[3]{\mathrm{mid}\pth{#1, #2, #3}}

\newcommand{\wu}{\widehat{u}}
\newcommand{\wv}{\widehat{v}}

\newcommand{\uopt}{u'_\mathrm{opt}}
\newcommand{\vopt}{v'_\mathrm{opt}}

\newcommand{\xa}[1]{u_{#1}'}
\newcommand{\xb}[1]{v_{#1}'}
\newcommand{\xc}[1]{w_{#1}'}

\newcommand{\xA}{u_1'}
\newcommand{\xB}{v_1'}
\newcommand{\xC}{x_3}
\newcommand{\xD}{v'}
\newcommand{\xE}{v''}
\newcommand{\revent}{\delta}

\newcommand{\permut}[1]{\left\langle {#1} \right\rangle}

\newcommand{\vtxA}{u}
\newcommand{\vtxB}{v}
\newcommand{\vtxC}{w}
\newcommand{\vtxD}{x}
\newcommand{\vtxE}{y}
\newcommand{\vtxF}{z}










\newcommand{\FigWidth}{0.9\textwidth}

\newcommand{\RTBoundedK}{O\pth{ c^2 k n \log^3 n }}\newcommand{\RTUnbounded}{O\pth{ c^2 n \log^2 n \pth{\log n
                                 +\eps^{-2d}\log(1/\eps)} }}

\newcommand{\DOT}{}

\newcommand{\const}{\ensuremath{\frac{11}{10}}}
\newcommand{\constN}{\ensuremath{({11}/{10})}}

\newcommand{\constR}{\ensuremath{\frac{10}{11}}}
\newcommand{\constEps}{\ensuremath{1/10}}
\newcommand{\weightIdx}[2]{\ensuremath{\phi_{#1}\pth{#2}}}
\newcommand{\weight}[1]{\ensuremath{\phi\pth{\vtx_{#1}}}}
\newcommand{\weightC}{\ensuremath{\phi}}
\newcommand{\weightCX}[1]{\ensuremath{\phi}_{#1}}
\newcommand{\vtx}{\vtxB}

\usepackage{pifont}\usepackage[symbol*]{footmisc}

\newcommand*\circled[1]{\footnotesize\tikz[baseline=(char.base)]{
    \node[shape=circle,draw,inner sep=0.2pt] (char) {#1};}}



\DefineFNsymbols*{stars}[text]{{\ding{172}} {\ding{173}} {\ding{174}} {\ding{175}} {\ding{176}} {\ding{177}} {\ding{178}} {\ding{179}} {\ding{180}} {{\protect\circled{{\tiny 10}}}}{{\protect\circled{{\tiny 11}}}}{{\protect\circled{{\tiny 12}}}}{{\protect\circled{{\tiny 13}}}}{{\protect\circled{{\tiny 14}}}}{{\protect\circled{{\tiny 15}}}}{{\protect\circled{{\tiny 16}}}}{{\protect\circled{{\tiny 17}}}}{{\protect\circled{{\tiny 18}}}}{{\protect\circled{{\tiny 19}}}}{{\protect\circled{{\tiny 20}}}}{{\protect\circled{{\tiny 21}}}}{{\protect\circled{{\tiny 22}}}}}

\newcommand{\segX}[2]{{#1}{#2}}

\newcommand{\DSX}[1]{\mathcal{D}_{#1}}

\newcommand{\prevX}[1]{{#1}^{-}}
\newcommand{\nextX}[1]{{#1}^{+}}

\newcommand{\heap}{\mathcal{H}}

\newcommand{\SNumVertices}[1]{2#1 - 1}

\newcommand{\cYk}{\cY_k^*}




\numberwithin{figure}{section}
\numberwithin{equation}{section}

\newcommand{\postdecessor}{successor\xspace}


\newcommand{\GridCompl}{\ensuremath{\chi}}
\newcommand{\NgridCompl}{\ensuremath{\eps^{-d}\log(1/\eps)}}

\newcommand{\vrestricted}{vertex-restricted}
\newcommand{\scunrestricted}{unrestricted}
\newcommand{\unrestricted}{continuous}
\newcommand{\asymmetric}{directed}
\newcommand{\symmetric}{undirected}


\newcommand{\CodeComment}[1]{\textcolor{blue}{\texttt{#1}}}
\newcommand{\tunnelPrice}{\Algorithm{price}\xspace} 
\newcommand{\matching}{matching\xspace}
\newcommand{\cubcZ}{\widehat{\cZ}}

\newcommand{\Eqlab}[1]{\label{equation:#1}}
\newcommand{\gridMin}{\ensuremath{\eps L/4}}
\newcommand{\gridMinFrac}{\ensuremath{\frac{\eps L}{4}}}
\newcommand{\ds}{\displaystyle}
\newcommand{\sidelengthX}[1]{\mathrm{sidelength}\pth{#1}}


 
\begin{document}

\title{Jaywalking your Dog --\\
   Computing the \Frechet Distance with Shortcuts\thanks{A preliminary version of this paper appeared in {\em \si{Proc.}
         23rd ACM-SIAM \si{Sympos.} Discrete Algorithms}, pages
      318--337, 2012.  The latest full version of this paper is
      available online \cite{dh-jydfd-11}. }}

\author{Anne Driemel\thanks{Department of Information and Computing Sciences; Utrecht
      University; The Netherlands;
      \texttt{an{}n{}e}\hspace{0cm}\texttt{\atgen{}\si{cs.uu.nl}}.
      This work has been supported by the Netherlands
      \si{Organisation} for Scientific Research (\si{NWO}) under
      \si{RIMGA} (Realistic Input Models for Geographic
      Applications).} \and Sariel Har-Peled\SarielThanks{Work on this paper was partially supported by NSF AF
      award CCF-0915984.}  }


\date{\today}

\maketitle

\setfnsymbol{stars}


\begin{abstract}
    The similarity of two polygonal curves can be measured using the
    \Frechet distance. We introduce the notion of a more robust
    \Frechet distance, where one is allowed to shortcut between
    vertices of one of the curves. This is a natural approach for
    handling noise, in particular batched outliers. We compute a
    -approximation to the minimum \Frechet distance over all
    possible such shortcuts, in near linear time, if the curve is
    -packed and the number of shortcuts is either small or
    unbounded.
    
    To facilitate the new algorithm we develop several new tools:
    \begin{compactenum}[\qquad(A)] \item A data structure for
        preprocessing a curve (not necessarily -packed) that
        supports -approximate \Frechet distance queries
        between a subcurve (of the original curve) and a line segment.
        
	\item A near linear time algorithm that computes a permutation
        of the vertices of a curve, such that any prefix of
         vertices of this permutation, form an
        optimal approximation (up to a constant factor) to the
        original curve compared to any polygonal curve with 
        vertices, for any .
        
	\item A data structure for preprocessing a curve that supports
        approximate \Frechet distance queries between a subcurve and
        query polygonal curve. The query time depends quadratically on
        the complexity of the query curve, and only (roughly)
        logarithmically on the complexity of the original curve.
    \end{compactenum}
    To our knowledge, these are the first data structures to support
    these kind of queries efficiently.
\end{abstract}


\section{Introduction}
\seclab{introduction}


Comparing the shapes of polygonal curves -- or sequenced data in
general -- is a challenging task that arises in many different
contexts. The \Frechet{} distance and its variants (e.g., dynamic
time-warping \cite{kp-sudtw-99}) have been used as similarity measures
in various applications such as matching of time series in databases
\cite{kks-osmut-05}, comparing melodies in music information retrieval
\cite{sgh-csi-08}, matching coastlines over time
\cite{mdbh-cmpdfd-06}, as well as in map-matching of vehicle tracking
data \cite{bpsw-mmvtd-05, wsp-anmms-06}, and moving objects analysis
\cite{bbg-dsfm-08, bbgll-dcpcs-08}.  Informally, the \Frechet distance
between two curves is defined as the maximum distance a point on the
first curve has to travel as this curve is being continuously deformed
into the second curve.  Another common description uses the following
``leash'' metaphor: Imagine traversing the two curves simultaneously
and at each point in time the two positions are connected by a leash
of a fixed length. During the traversal you can vary the speeds on
both curves independently, but not walk backwards. The \Frechet
distance corresponds to the minimum length of a leash that permits
such a traversal.

\parpic[r]{\IncGraphPage[width=0.3\linewidth]{figs}{\si{hiking}}{1}}

The \Frechet distance captures similarity under small non-affine
distortions and for some of its variants also spatio-temporal
similarity \cite{mssz-fdsl-2011}.  However, it is very sensitive to
local noise, which is frequent in real data.  Unlike similarity
measures such as the root-mean-square deviation (RMSD), which averages
over a set of similarity values, and dynamic time warping, which
minimizes the sum of distances along the curves, the \Frechet distance
is a so-called \emph{bottleneck measure} and can therefore be affected
to an extent which is generally unrelated to the relative amount of
noise across the curves.  In practice, curves might be generated by
physical tracking devices, such as GPS, which is known to be
inaccurate when the connection to the satellites is temporarily
disturbed due to atmospheric conditions or reflections of the
positioning signal on high buildings. Such inaccurate data points are
commonly referred to as ``outliers''.  Note that outliers come in
batches if they are due to such a temporary external condition.
Similarly, in computer vision applications, the silhouette of an
object could be partially occluded, and in sound recordings, outliers
may be introduced due to background sounds or breathing.  Detecting
outliers in time series has been studied extensively in the literature
\cite{mmy-rs-06}.  One may also be interested in outliers as a
deviation from a certain expected behavior or because they carry some
meaning.  It could be, for instance, that trajectories of two hikers
deviate locally, because one hiker chose to take a detour to a
panoramic view point, see the example in the figure above. Outlier
detection is inherently non-trivial if not much is known about the
underlying probability distributions and the data is sparse
\cite{ay-eeaod-05}.  We circumvent this problem in the computation of
the \Frechet distance by minimizing over all possibilities for
outlier-removal.  In a sense, our approach is similar to computing a
certain notion of partial similarity.  Unlike other partial distance
measures, the distance measure we propose is parameter-free. For
comparison, in the \emph{partial \Frechet distance}, as it was studied
by Buchin \etal \cite{bbw-eapcm-09}, one is interested in maximizing
the portions of the curves which can be matched within a certain
\Frechet distance (the parameter). In this case, the dissimilar
portions of the curves are ignored. In our case, they are replaced by
\emph{shortcuts}, which have to be matched under the \Frechet
distance.

\paragraph{The task at hand.} We are given two polygonal curves 
and  in , which we perceive as a sequence of linearly
interpolated measurement points. We believe that  is similar to
 but it might contain considerable noise that is occluding this
similarity. That is, it might contain erroneous measurement points
(outliers), which need to be ignored when assessing the similarity.
We would like to apply a few edit operations to  so that it
becomes as similar to  as possible. In the process hopefully
removing the noise in  and judging how similar it really is to
.  To this end, we -- conceptually -- remove subsequences of
measurement points, which we suspect to be outliers, and minimize over
all possibilities for such a removal. This is formalized in the
shortcut \Frechet distance.


\paragraph{Shortcut \Frechet distance.}
A shortcut replaces a subcurve between two vertices by a straight
segment that connects these vertices.  The part being shortcut is not
ignored, but rather the new curve with the shortcuts has to be matched
entirely to the other curve under the \Frechet distance.  As a
concrete example, consider the figure below.  The \Frechet distance
between  and  is quite large, but after we shortcut the
outlier ``bump'' in , the resulting new curve  has a
considerably smaller \Frechet distance to .  We are interested in
computing the minimum such distance allowing an unbounded number of
shortcuts.

Naturally, there are many other possibilities to tackle the task at
hand, for example:
\begin{compactenum}[(i)]
    \item bounding the number of shortcuts by a parameter ,
    \item allowing shortcuts on both curves,
    \item allowing only shortcuts between vertices that are close-by
    along the curve,
    \item ignoring the part being shortcut and maximizing the length
    of the remaining portions,
    \item allowing shortcuts to start and end anywhere along the
    curve,
    \item allowing curved shortcuts, etc.
\end{compactenum}



\parpic[r]{\begin{minipage}{0.25\linewidth}\includegraphics[page=2,width=.9\linewidth]{figs/hiking}
       \figlab{shortcut:example}\end{minipage}}


If one is interested in (iii) then the problem turns into a
map-matching problem, where the start and end points are fixed and the
graph is formed by the curve and its eligible shortcuts. For this
problem, results can be found in the literature \cite{cdgnw-amm-11,
   aerw-mpm-03}. A recent result by Har-Peled and Raichel
\cite{hr-fdre-11} is applicable to the variant where one allows such
shortcuts on both curves, i.e. (ii)+(iii).  The version in (iv) has
been studied under the name of \emph{partial \Frechet distance}
\cite{bbw-eapcm-09}.

In this paper, we concentrate on the \asymmetric{} \vrestricted{}
shortcut \Frechet distance (see \secref{shortcut:frechet} for the
exact definition) because computing it efficiently seems like a first
step in understanding how to solve some of the more difficult
variants, e.g.,~(v).  Surprisingly, computing this simpler version of
the shortcut \Frechet distance is already quite challenging,
especially if one is interested in an efficient algorithm.  A more
recent result by Buchin \etal~\cite{bds-jnp-13, d-raapg-13} shows that
computing the shortcut \Frechet distance exactly is weakly \NPHard for
variant (v), where we allow shortcuts to start and end anywhere along
the curve.  Furthermore, our algorithms can be extended to
variant~(i), i.e., where at most  shortcuts are allowed, see
\remrefpage{k:shortcut}.

Note that allowing shortcuts on both curves does not always yield a
meaningful measure, {especially if shortcuts on both curves may be
   matched to each other.  In particular, if one of the two curves is
   more accurately sampled and can act as a model curve, allowing
   shortcuts on only one of the two curves seems reasonable. }



\paragraph{Input model.} 
A curve  is \emphi{-packed} if the total length of 
inside any ball is bounded by  times the radius of the ball.
Intuitively, -packed curves behave reasonably in any resolution.
The boundary of convex polygons, algebraic curves of bounded maximum
degree, the boundary of \AlphaBetaCovered shapes \cite{e-cuabc-05},
and the boundary of -fat shapes \cite{d-ibucf-08} are all
-packed (under the standard assumption that they have bounded
complexity).  Interestingly, the class of -packed curves is closed
under simplification, see \cite{dhw-afdrc-12}. This makes them
attractive for efficient algorithmic manipulation.

Another input model which is commonly used is called low density
\cite{bksv-rimga-02}.  We call a set of line segments
-\emphi{dense}, if for any ball the number of line segments that
intersect this ball and which are longer than the radius of the ball
is bounded by . It is easy to see by a simple packing argument
that -packed curves are -dense.



\paragraph{Informal restatement of the problem.} 

In the parametric space of the two input curves, we are given a
terrain defined over a grid partitioning , where the height
at each point is defined as the distance between the two associated
points on the two curves. The grid is induced by the vertices of the
two curves. As in the regular \Frechet distance, we are interested in
finding a path between  and  on the terrain, such that
the maximum height on the path does not exceed some  (the
minimum such  is the desired distance). This might not be
possible as there might be ``mountain chains'' blocking the way.  To
overcome this, we are allowed to introduce \tunnels that go through
such obstacles. Each of these \tunnels connect two points that lie on
the horizontal lines of the grid, as these correspond to the vertices
of one curve. Naturally, we require that the starting and ending
points of such a \tunnel have height at most  (the current
distance threshold being considered), and furthermore, the price of
such a \tunnel (i.e., the \Frechet distance between the corresponding
shortcut and subcurve) is smaller than~. Once we introduce
these \tunnels, we need to compute a \emph{monotone} path from 
to  in the grid which uses \tunnels.  Finally, we need to
search for the minimum  for which there is a feasible
solution.

\paragraph{Challenge and ideas.}
Let  be the total number of vertices of the input curves. A priori
there are potentially  horizontal edges of the grid that might
contain endpoints of a \tunnel, and as such, there are potentially
 different families of \tunnels that the algorithm might have
to consider.  A careful analysis of the structure of these families
shows that, in general, it is sufficient to consider one (canonical)
\tunnel per family.  Using -packedness and simplification, we can
reduce the number of relevant grid edges to near linear.  This in turn
reduces the number of potential \tunnels that need to be inspected to
.  This is still insufficient to get a near linear time
algorithm. Surprisingly, we prove that if we are interested only in a
constant factor approximation, for every horizontal edge of the grid
we need to inspect only a constant number of \tunnels.  Thus, we
reduce the number of \tunnels that the algorithm needs to inspect to
near linear. And yet we are not done, as naively computing the price
of a \tunnel requires time near linear in the size of the associated
subcurve.  To overcome this, we develop a new data structure, so that
after preprocessing we can compute the price of a \tunnel in
polylogarithmic time per \tunnel. Now, carefully putting all these
insights together, we get a near linear time algorithm for the
approximate decision version of the problem.

However, to compute the minimum , for which the decision
version returns true -- which is the shortcut \Frechet distance -- we
need to search over the critical values of . To this end, we
investigate and characterize the critical values introduced by the
shortcut version of the problem.  Using the decision procedure, we
perform a binary search of several stages over these values, in the
spirit of \cite{dhw-afdrc-12}, to get the required approximation.


\subsection*{Our results}

\begin{compactenum}[(A)]
    \item \textbf{Computing the shortcut \Frechet distance.} For a prescribed parameter , we present an algorithm for
    computing a -approximation to the \asymmetric{}
    \vrestricted{} shortcut \Frechet distance between two given
    -packed polygonal curves of total complexity , see
    \defref{shortcut:f:r} for the formal definition of the distance
    being approximated.
    
    If we allow an unbounded number of shortcuts the running time of
    the new algorithm is , see \thmref{main} for the
    exact result. A variant of this algorithm can also handle the case
    where we allow only  shortcuts, with running time
    , see \remrefpage{k:shortcut}.  In the analysis of
    these problems we use techniques developed by Driemel \etal{} in
    \cite{dhw-afdrc-12} and follow the general approach used in the
    parametric search technique of devising a decision procedure which
    is used to search over the critical events for the \Frechet
    distance.  The shortcuts introduce a new type of critical event,
    which we analyze in \secref{shortcut:price}.  The presented
    approximation algorithms can be easily modified to yield
    polynomial-time exact algorithms for the same problems (and for
    general polygonal curves). As such, the main challenge in devising
    the new algorithm was to achieve near linear time performance.
    Furthermore, the algorithm uses a new data structure (described
    next) that is interesting on its own merit.
    
    \item \textbf{\Frechet-distance queries between a segment and a
       subcurve.} We present a data structure that preprocesses a given polygonal
    curve , such that given a query segment , and two
    points  on  (and the edges containing them), it
    -approximates the \Frechet distance between  and
    the subcurve of  between  and . Surprisingly,
    the data structure works for any polygonal curve (not necessarily
    packed or dense), requires near linear preprocessing time and
    space, and can answer such queries in polylogarithmic time
    (ignoring the dependency on ). See
    \thmref{segment:queries:f:r} for the exact result.
    
    \item \textbf{Universal vertex permutation for curve
       simplification.} We show how to preprocess a polygonal curve in near-linear time
    and space, such that, given a number , one can compute
    a simplification in  time which has 
    vertices (of the original curve) and is optimal up to a constant
    factor with respect to the \Frechet distance to the original
    curve, compared to any curve which uses 
    vertices. Surprisingly, this can be done by computing a
    permutation of the vertices of the input curve, such that this
    simplification is the subcurve defined by the first  vertices
    in this permutation. Namely, we compute an ordering of the
    vertices of the curves by their \Frechet ``significance''.  See
    \thmref{f:r:simpl:result} for the exact result.
    
    \item \textbf{\Frechet-distance queries between a curve with 
       vertices and a subcurve.}  We use the above universal vertex
    permutation, to extend the data structure in (B) to support
    queries with polygonal curves of multiple segments (as opposed to
    single segments) and obtain a constant factor approximation with
    polylogarithmic query time, see \thmref{k:seg:query:result}. The
    query time is quadratic in the query curve complexity and
    logarithmic in the input curve complexity.
    
\end{compactenum}


\paragraph{Related work.}Assume we are given two polygonal curves of total complexity  and
we are interested in computing the \Frechet distance between these
curves. The problem has been studied in many variations.  We only
discuss the results which we deem most relevant and refer the reader
to \cite{bbmm-fswd-12} for additional references.

Driemel \etal presented a near linear time -approximation
algorithm for the \Frechet distance assuming the curves are well
behaved \cite{dhw-afdrc-12}; that is, -packed.  In general,
computing the \Frechet distance exactly takes roughly quadratic time.
After publication in the seminal paper by Alt and Godau
\cite{ag-cfdbt-95}, their -time algorithm remained the
state of the art for more than a decade.  This lead Alt to conjecture
that the problem of deciding whether the \Frechet distance between two
curves is smaller or equal a given value is 3SUM-hard.  However,
recently, there has been some progress in improving upon the quadratic
running time of the decision algorithm.  First, Agarwal \etal
presented a subquadratic time algorithm for a specific variant of the
\Frechet distance \cite{aaks-dfst-13}.  Buchin \etal build upon their
work and give an algorithm for the original \Frechet distance
\cite{bbmm-fswd-12}.  Their algorithm is randomized and takes  expected time overall to compute the \Frechet
distance. The decision algorithm they present is deterministic and
takes subquadratic time.  The only lower bound known for the decision
problem is  and was given by Buchin \etal
\cite{bbkrw-wtd-07}.  A randomized algorithm simpler than the one by
Alt and Godau, which has the same running time, but avoids parametric
search, was recently presented by Har-Peled and Raichel
\cite{hr-fdre-13}.

Buchin \etal \cite{bbw-eapcm-09} showed how to compute the
\emph{partial \Frechet distance} under the  and 
metric.  Here, one fixes a threshold , and computes the
maximal length of subcurves of the input curves that match under
\Frechet distance .  The running time of their algorithm is
roughly .  For the problem of counting the number
of subcurves that are within a certain \Frechet distance, a recent
result by \si{de Berg} \etal provides a data structure to answer such
queries up to a constant approximation factor \cite{bcg-ffq-11}.  To
the best of our knowledge the problem of computing the \Frechet
distance when one is allowed to introduce shortcuts has not been
studied before.


\paragraph{Previous work on curve simplification.} There is a large
body of literature on curve simplification. Since this is not the main
subject of the paper, we only discuss a selection of results which we
consider most relevant, since they use the \Frechet distance as a
quality measure.  Agarwal \etal \cite{ahmw-nltaa-05} give a
near-linear time approximation algorithm to compute a simplification
which is in \Frechet distance  to the original curve and of
which the size is at most the size of the optimal simplification with
error .  Abam \etal \cite{abh-sals-10} study the problem in
the streaming setting, where one wishes to maintain a simplification
of the prefix seen so far.  Their algorithm achieves an 
competitive ratio using  additional storage and maintains a
curve with  vertices which has a smaller \Frechet distance to the
prefix than the optimal \Frechet simplification with  vertices.
Bereg \etal \cite{bjwyz-spcdfd-08} give an exact 
algorithm that minimizes the number of vertices in the simplification,
but using the discrete \Frechet distance, where only distances between
the vertices of the curves are considered. Simplification under the
\Frechet distance has also been studied by Guibas \etal
\cite{ghms-apsml-93}.


\paragraph{Organization.}

In \secref{prelims} we describe some basic definitions and results. In
particular, the formal problem statement and the definition of the
\emph{\asymmetric{} \vrestricted{} shortcut \Frechet distance} between
two curves is given in \secref{shortcut:frechet}.  We also discuss
some basic tools needed for the algorithms.
In \secref{algo:unbounded:general}, we describe the approximation
algorithm for the shortcut \Frechet distance.  Here, we devise an
approximate decision procedure in \secref{algo:decider} that is used
in the main algorithm, described in \secref{algo:main}, to search over
an approximate set of candidate values.  The analysis of this
algorithm is given in \secref{analysis:unbounded}.  Since the
shortcuts introduce a new set of candidate values, we provide an
elaborate study of these new events in \secref{shortcut:price}.  The
main result for approximating the shortcut \Frechet distance is stated
in \thmref{main}.  In the remaining sections we describe the new data
structures.  In \secref{f:r:query} we describe a data structure for a
fixed curve, that answers queries for the \Frechet distance between a
subcurve and a given segment.  In \secref{f:r:simpl}, we use this data
structure to compute the universal vertex permutation. The extension
to query curves with more than two vertices are described in
\secref{k:seg:query}.  We conclude with discussion and some open
problems in \secref{conclusions}.


\section{Preliminaries}\seclab{prelims}


\paragraph{Notation.}
A \emphi{curve}  is a continuous mapping from  to ,
where  denotes the point on the curve parameterized by .  Given two curves  and  that share an endpoint, let
 denote the \emphi{concatenated} curve.  We denote
with  the subcurve of  from  to
 and with  the subcurve of 
between the two points .  Similarly,
 denotes the line segment between the points
 to , we call this a \emphi{shortcut} of
. For a set of numbers , an \emphi{atomic interval} is a maximum
interval of  that does not contain any point of  in its
interior.

\subsection{Background and standard definitions}
\seclab{background}

Some of the material covered in this section is standard, and follows
the presentation in Driemel \etal \cite{dhw-afdrc-12}.  A
\emphi{reparameterization} is a one-to-one and continuous function
. It is \emphi{orientation-preserving} if it
maps  and .  The \Frechet distance is defined only for
oriented curves, as we need to match the start and end points of the
curves. The orientation of the curves we use would be understood from
the context.

\begin{defn}\deflab{width:f}

    Let  and  be two
    polygonal curves. We define the \emphi{width} of an
    orientation-preserving reparametrization , with
    respect to  and , as
    
    The \emphi{\Frechet distance} between the two curves is
    
\end{defn}




\begin{defn}\deflab{elevation}Let  and  be two
    polygonal curves.  The square  represents their
    \emphi{parametric space}.  For a point , we define its \emphi{elevation} to be  Let  be a parameter,
    the -\emphi{free space} of  and  is defined as
    
\end{defn}

\paragraph{Free space diagram.} We are interested only in polygonal
curves, which we assume to have uniform parameterizations.  The
parametric space can be broken into a (not necessarily uniform) grid
called the \emphi{free space diagram}, where a vertical line
corresponds to a vertex of  and a horizontal line corresponds to
a vertex of .

\parpic[r]{\includegraphics[scale=0.7]{figs/free_space_cell}}

Every two segments of  and  define a \emphi{free space cell}
in this grid.  In particular, let  denote the free space cell that corresponds to the
\th edge of  and the \th edge of . The cell
 is located in the \th column and \th row of this
grid.

It is known that the free space, for a fixed , inside such a
cell  (i.e., ) is the clipping of an affine transformation of a disk
to the cell \cite{ag-cfdbt-95}, see the figure on the right; as such,
it is convex and of constant complexity.  Let  denote
the horizontal \emphi{free space interval} at the top boundary of
, and  denote the vertical free space
interval at the right boundary.

We define the \emphi{complexity} of the relevant free space, for
distance , denoted by , as the total
number of grid cells that have a non-empty intersection with
.

\begin{observation}\obslab{f:r:segments}Given two segments  and , it holds , .  To
    see this, consider the uniform parameterization  and , for . It is
    easy to verify that  is convex, and
    as such , for any .
\end{observation}

\paragraph{Free space events.}

To compute the \Frechet distance consider increasing  from 
to . As  increases, structural changes happen to the
free space. We are interested in the radii (i.e., the value of
) of these events.

\parpic[l]{\includegraphics{figs/creation_time}}

Consider a segment  and a vertex , a
\emphi{vertex-edge event} corresponds to the minimum radius of a ball
centered at , such that  is tangent to the ball, see the
figure on the left.  In the free space diagram, this corresponds to
the event that a free space interval consists of one point only.  The
line supporting this boundary edge corresponds to the vertex, and the
other dimension corresponds to the edge.  Naturally, the event could
happen at a vertex of .  The second type of event, a
\emphi{monotonicity event}, corresponds to a value  for which
a monotone subpath inside the -free space becomes feasible.
Geometrically, this corresponds to the common distance of two vertices
on one curve to the intersection point of their bisector with a
segment on the other curve.


\subsection{The -shortcut \Frechet distance}
\seclab{shortcut:frechet}

\begin{defn}\deflab{shortcut:curve}For a polygonal curve , we refer to any order-preserving
    concatenation of  non-overlapping (possibly empty) subcurves
    of  with  shortcuts connecting the endpoints of the
    subcurves in the order along the curve, as a -\emphi{shortcut
       curve} of .  Formally, for values , the shortcut curve is defined as
    . If
    each  is a vertex of  we refer to the shortcut
    curve as being \emphi{\vrestricted{}}, otherwise we say it is
    \emphi{\scunrestricted}.
\end{defn}

\begin{defn} \deflab{shortcut:f:r}Given two polygonal curves  and , we define their
    \emphi{continuous} -shortcut \Frechet distance as the minimal
    \Frechet distance between the curve  and any unrestricted
    -shortcut curve of .  We denote it with
    .  If we do not want to bound the number of
    shortcuts, we omit the parameter  and denote it with
    .  The \emphi{\vrestricted{} -shortcut
       \Frechet distance} is defined as above using only
    \vrestricted{} shortcut curves of .  Furthermore, note that
    in all cases we allow only one of the input curves to be shortcut,
    namely , thus we call the distance measure
    \emphi{\asymmetric{}}.
\end{defn}

In this paper, we study the \asymmetric{} \vrestricted{} -shortcut
\Frechet distance for the case of bounded and unbounded .  In the
following, we will omit the predicates \asymmetric{} and
\vrestricted{} when it is clear from the context.


\paragraph{Free space.}

The \emphi{-reachable free space} 
is

This is the set of points that have an -monotone path from
 that stays inside the free space and otherwise uses at most
 \tunnels, which are defined in the next subsection.



\begin{figure*}[\si{bt}]\center
\includegraphics{figs/simple_example_swap} \caption{(A) Example of
       two dissimilar curves that can be made similar by shortcutting
       one of them.  (B) A \tunnel  corresponds
       to a shortcut and a subcurve matched to each other and (C)
       their free space diagram.  (D) The \tunnel connects previously
       disconnected components of the free space.  (E) The curve 
       resulting from shortcutting . Its (regular) \Frechet
       distance from  is dramatically reduced.  }
    \figlab{simple:example}
\end{figure*}

\subsection{\Tunnels and gates -- definitions}\seclab{def:tunnels}

\subsubsection{\Tunnels}\seclab{tunnels:real:def}

In the parametric space, a shortcut  and
the subcurve , that it is being matched
to, correspond to a the rectangle with corners  and ,
where  and .
By shortcutting the curve on the vertical axis, we are collapsing this
rectangle to a single row, see \figref{simple:example}~(C).  More
precisely, this is the free space diagram of the shortcut and the
subcurve.  We call this row a \emphi{\tunnel{}} and denote it by
.  We require  and  for monotonicity.  \figref{simple:example} shows the full
example of a \tunnel. We call the \Frechet distance of the shortcut
segment to the subcurve the \emphi{price} of this \tunnel and denote
it with .
A \tunnel  is \emphi{feasible} for  if
it holds that  and , i.e., if .
(Note that in turn the feasibility of a monotone path in the free
space of the tunnel is determined by the price of the tunnel.)  Now,
let  and  and let  be the edge of
 that contains  (resp.,  the edge that
contains ) for the \tunnel .  We
denote with  the \emphi{family of
   \tunnels{}} that  belongs to.  Furthermore,
let  denote the subset of
these \tunnels that are feasible for .

\begin{defn}\deflab{min:radius}\deflab{canonical:price}The \emphi{canonical \tunnel{}} of the \tunnel family
    , denoted by , is the \tunnel that matches the shortcut  to
    the subcurve , such that  and  are the values
    realizing
        
    We refer to  as the
    \emphi{minimum radius} of this family. The canonical tunnel may
    not be uniquely defined if only one of the two values  or 
    determines the minimum radius.  In this case, we define  and
     as the values minimizing  and
     for  and ,
    individually.  We call the price of the canonical \tunnel the
    \emphi{canonical price} of this \tunnel family.
\end{defn}

Clearly, one can compute the canonical \tunnel
 in constant time. In particular, the
price of this canonical \tunnel is


We emphasize that a shortcut is always a segment connecting two
vertices of the curve , and a \tunnel always lies in the
parametric space; that is, they exist in two completely different
domains.

\begin{observation}\obslab{min:radius:eq}The minimum radius of a tunnel family
     corresponds to either
    \begin{inparaenum}[(i)]
        \item the distance of  to its closest point on ,
        \item the distance of  to its closest point on , or
        \item the common distance of  and  to the intersection
        of their bisector with the edge  (i.e., a monotonicity
        event). Note that the event in case (iii) can only happen if
        .
    \end{inparaenum}
\end{observation}

\subsubsection{Gates}\vspace{-0.2cm}

\parpic[r]{\includegraphics{figs/gates}}\noindent Let  be a subset of the parametric space that is convex
in every cell.  Let  be a free space interval. We
call the left endpoints of  the \emphi{left
   gate} of  in the cell , and similarly the right
endpoint is the \emphi{right gate}.  The figure to the right shows an
example of gates  and .  The \emphi{set of gates} of 
are the gates with respect to all cells in the free space diagram.  We
define the \emphi{canonical gate} of a vertex-edge pair as the point
in parametric space that minimizes the vertex-edge distance. Note that
canonical gates serve as endpoints of canonical tunnels that span
across columns in the free-space diagram.

\subsection{Curve simplification}

We use the following simple algorithm for the simplification of the
input curves. It is easy to verify that the curve simplified with
parameter  is in \Frechet distance at most  to the
original curve, see \cite{dhw-afdrc-12}.

\begin{defn}\deflab{simplification}Given a polygonal curve  and a parameter .
    First mark the initial vertex of  and set it as the
    current vertex. Now scan the polygonal curve from the current
    vertex until it reaches the first vertex that is in distance at
    least  from the current vertex. Mark this vertex and set
    it as the current vertex. Repeat this until reaching the final
    vertex of the curve, and also mark it.  We refer to the resulting
    curve  that connects only the marked vertices, in their order
    along , as a \emphi{-simplification} of
     and we denote it with .
\end{defn}

\begin{figure}\centering
    \includegraphics[width=0.3\textwidth]{figs/no_triangle_inequality}
    \caption{The \asymmetric{} -shortcut \Frechet does not
          satisfy the triangle inequality. In the depicted
          counter-example it holds that  for any value
          of  and for  unbounded. This holds true in the
          \vrestricted{} and in the \unrestricted{} case.}
       \figlab{triangle:inequality}
\end{figure}

During the course of the algorithm we will simplify the input curves
in order to reduce the complexity of the free space.  The -shortcut
\Frechet distance does not satisfy the triangle inequality, as can be
seen by the counter-example shown in \figref{triangle:inequality}.
Therefore, we need the next lemma to ensure that the computed distance
between the simplified curves approximates the distance between the
original curves.  The proof is straightforward and can be found in
\cite{d-raapg-13}.

\begin{lemma}[\cite{d-raapg-13}]\lemlab{t:r:i:n:e:q:s:u:b}Given a simplification parameter  and two polygonal
    curves  and , let  and 
    denote their -simplifications, respectively.  For any , it holds that .  Similarly, .
\end{lemma}





\begin{lemma}[\cite{dhw-afdrc-12}]\lemlab{complexity:l:e:q}For any two -packed curves  and  in 
    of total complexity , and two parameters  and
    , we have that
    , where .
\end{lemma}


\subsection{Building blocks for the algorithm}

The algorithm uses
the following two non-trivial data structures.
\begin{datastructure}\dslab{d:s:magic:B}Given a polygonal curve  with  vertices in , one
    can build a data structure, in 
    time, using  space, where
    , that supports a procedure
     which receives two points 
    and  in the parametric space of  and  and returns
    a value , such that  in  time.  See
    \secrefpage{f:r:query} and \thmrefpage{segment:queries:f:r}.
\end{datastructure}

\begin{datastructure}
    \dslab{d:s:magic:A}For given parameters  and , and two -packed
    curves  and  in , let  and ,
    where .  One can compute all the
    vertex-edge pairs of the two simplified curves  and  in
    distance at most  from each other, in time . See below for details.
\end{datastructure}








We describe here how to realize \dsref{d:s:magic:A}.  Observe that
 and  have density .  Now, we build the
data structure of \si{de Berg} and Streppel \cite{bs-arsbsp-06} for
the segments of  (with ). For each vertex of  we
compute all the segments of  that are in distance at most
 from it, using the data structure \cite{bs-arsbsp-06}. Each
query takes  time, where  is the number of
edges reported.  \lemref{complexity:l:e:q} implies that the total sum
of the 's is .  We now repeat this for the other
direction. This way, one can realize \dsref{d:s:magic:A}.




\subsection{Monotonicity of the prices of tunnels}

The following two lemmas imply readily that under certain conditions
the prices of tunnels which share an endpoint are approximately
monotone with respect to the -coordinate of their starting
point. We will exploit this in the approximation algorithm that
computes the reachability in the free-space diagram. We will see in
\secref{tunnel:test} that this drastically reduces the number of
tunnels that need to be inspected in order to decide if a particular
cell is reachable.

\begin{lemma}\lemlab{monotone:shortcut:base}Given a value  and two curves  and
    , such that  is a subcurve of ,
    and given two line segments  and , such
    that  and the start
    (resp., end) point of  is in distance  to the
    start (resp., end) point of , then
    .
\end{lemma}




\noindent
\begin{minipage}{0.8\textwidth}
    \begin{proof}
        Let  denote the subsegment of  that is
        matched to  under an optimal \Frechet mapping
        between  and .  We know that
         by this mapping.  The
        start point of  is in distance  to the
        start point of , since they are both in distance
         to the start point of  and the same holds
        for the end points. This implies that
        .  Now, by the
        triangle inequality, .
    \end{proof}
\end{minipage}\begin{minipage}{0.19\textwidth}
    \hfill {\includegraphics[width=0.9\textwidth]{figs/base_lemma}}
\end{minipage}

\begin{lemma}\lemlab{monotone:shortcut}Consider two polygonal curves  and , three points  and  in their free space, and let . If  then
    .\footnote{Here,  is the elevation of , see
       \defref{elevation}, and  is the tunnel
       between  and , see \secref{tunnels:real:def}.}
\end{lemma}

\begin{proof}
    Let  be the subcurve , and
    let .  Similarly, let
     be the shortcut  and
    let . By
    \lemref{monotone:shortcut:base}  for .
\end{proof}


We will see in \secref{second:stage} that the monotonicity property of
\lemref{monotone:shortcut} also enables a faster search over tunnel
events. The property holds even if the \tunnel{}s under consideration
are not valid. For example if  and 
then the tunnel  is not a valid tunnel and it
cannot be used by a valid solution. Nevertheless,
 has a well defined price, and these prices
have the required monotonicity property.


\begin{lemma}
    \lemlab{monotone:matrix}For a parameter , let  be
     points in the -free space in increasing order by their
    -coordinates, and let  for any .
    Then, we have:
    \begin{compactenum}[\qquad (A)]
        \item If  then for all , we have
        .
        
        \item If  then for all , we have
        .
    \end{compactenum}
\end{lemma}

\begin{proof}
    To see the first part of the claim, note that by
    \lemref{monotone:shortcut}, . As for the second
    part, we have by the same lemma that , and
    thus .
\end{proof}



\section{Approximating the shortcut \Frechet distance}\seclab{algo:unbounded:general}

We describe the algorithm to approximate the \asymmetric{} \vrestricted{}
shortcut \Frechet distance between two given polygonal curves  and 
where the number of shortcuts that can be used in a solution is unbounded.
In \secref{analysis:unbounded} we prove the correctness and analyze the
complexity of this algorithm.



\subsection{The \tunnelTest{} procedure}
\seclab{tunnel:test}

A key element in the decision procedure is the \tunnelTest{} procedure
depicted in \figref{tunnel:test}.  During the decision procedure, we
will repeatedly invoke the \tunnelTest procedure with a set of gates
, for which we already know that they are contained in the
reachable free space , and the
left gate associated with a horizontal free space interval of
, in order to determine, if and to which
extent this interval is reachable.

Intuitively, this procedure receives as input a set of reachable
points in the parametric space and a free space interval (in the form
of the left gate) and we are asking if there exists an
\emphi{affordable} \tunnel connecting a reachable point to the
interval. Here, affordable means that its price is less than .
More precisely, the procedure receives a set of gates  and a
gate  as input and returns the endpoint of an (approximately)
affordable tunnel that starts at a gate of  and ends either
at  or the leftmost point to the right of  in the same
free space interval.  If a tunnel between a gate in  and the
free space interval of  exists, which has price less than
, then the algorithm will return the endpoint of a tunnel of
price less than or equal to .  If the algorithm
returns \NULL, then we know that no such tunnel of price less than
 exists.

The main idea of the \tunnelTest procedure is the following.  For a
given \tunnel, we can -approximate its price, using a data
structure which answers these queries in polylogarithmic time, see
\dsref{d:s:magic:B}.  The desired tunnel could be a vertical tunnel
which starts at a gate of , or a tunnel between a gate of
 and .  Naively, one could test all tunnels that start
from a gate in  and end in , however, this takes time
at least linear in the size of . Since we are only
interested in a constant factor approximation, it is sufficient, by
\lemref{monotone:shortcut}, to test only the \tunnel which corresponds
to the shortest subcurve of .  The corresponding gates can be
found in polylogarithmic time using a two dimensional range tree,
which is built on the set  and we assume is available to
us. We can maintain the range tree during the decision procedure
depicted in \figref{decider:infty:inner}. The technical details are
described in the proof of \lemrefpage{s:y:m:f:s}.  An alternative
solution that uses a balanced binary search tree only is described in
\cite{d-raapg-13}.


\begin{figure}[t]\begin{center}\fbox{\begin{minipage}{0.76\linewidth}\tunnelTest{}
               \begin{algorithmic}[1]
                   \STATE Let  point in
                    with  value of ,\\
                   ~~~~~~~~~~ such that  and
                   , where .
                   \linelab{range:search}
                   
		   \STATE , see
                   \dsref{d:s:magic:B}. \linelab{tunnel:compute:phi}
\IF {  }
                   \linelab{tunnel:test}
                   
                   \STATE Return  ~~~ ~ \CodeComment{// \tunnel
                      } \linelab{X:i}
                   
                   \ENDIF


                   \STATE Compute  such that 
                   \linelab{col:index}
                   
                   \STATE Let  point in
                    with  value of ,\\
                   ~~~~~~~~~~ such that ,
                   , and 
                   \linelab{range:search:2}
                   
                   \IF{  does not exist}\STATE Return \NULL.
                   \ENDIF 

                   \STATE 
                   
                   \IF{ }
                   
                   \STATE Return  \linelab{X:i:2} ~~~~~\CodeComment{// vertical \tunnel
                      }
                   
                   \ELSE
                   
                   \STATE Return \NULL.
                   
                   \ENDIF
                   
               \end{algorithmic}
           \end{minipage}}
    \end{center}\vspace{-0.5cm}\caption{The \tunnelTest procedure receives a set of gates 
       and a gate  in the parametric space and returns the
       endpoint of an affordable tunnel between  and 
       (or a point close to it) if it exists. The technical details of
       the range queries in \lineref{range:search} and
       \lineref{range:search:2} are described in the proof of
       \lemrefpage{s:y:m:f:s}.  }
    \figlab{tunnel:test}\end{figure}


\subsection{The decision algorithm}
\seclab{algo:decider}

In the decision problem we want to know whether the shortcut \Frechet
distance between two curves,  and , is smaller or equal a
given value .  The free space diagram
 may consist of a certain number of
disconnected components and our task is to find a monotone path from
 to , that traverses these components while using
shortcuts between vertices of  to ``bridge'' between points in
different components or where there is no monotone path connecting
them (see \figref{simple:example}).  The decision algorithm exploits
the monotonicity of the tunnel prices shown in
\lemref{monotone:shortcut} and is based on a breadth first search in
the free space diagram (a similar idea was used
in \cite{dhw-afdrc-12}, but here the details are more involved).

\begin{figure}[t]
    \begin{center}
        \fbox{\begin{minipage}{\FigWidth}\deciderFr{}
               \begin{algorithmic}[1]
\STATE Assert that  and 
                   
                   \STATE Let  be a min-priority queue for
                   nodes  with keys 
                   
                   \STATE Compute and enqueue the cells
                    that have non-empty
                    or .
                   
                   \STATE Let .
                   
                   \WHILE{}
                   
                   \STATE Dequeue node  and its copies from
                   
                   
                   \STATE Let  be the left gate of
                   
                   
                   \STATE  \linelab{decider:t:t}
                   
                   \STATE Compute  and
                    from ,
                   , ,
                    and 
                   
                   \IF{}
                   
                   \STATE Enqueue  and insert edge between
                    and 
                   
                   \ENDIF
                   
                   \IF{}
                   
                   \STATE Enqueue  and insert edge between
                    and 
                   
                   \STATE Add gates of  to 
                   
                   \ENDIF
                   






                   \ENDWHILE
                   
                   \IF{}
                   
                   \STATE Return ``''
                   
                   \ELSE
                   
                   \STATE Return ``''
                   
                   \ENDIF
                   
               \end{algorithmic}
           \end{minipage}}\end{center}
    \vspace{-0.6cm}\caption{The decision procedure \deciderFr{} for the shortcut
       \Frechet distance.  }
    \figlab{decider:infty:inner}
\end{figure}


Given two curves  and , and parameters  and ,
the algorithm may output an answer equivalent to ``yes'' if there
exists a shortcut curve  of , such that
 and an answer equivalent ``no'' if
there exists no shortcut curve such that .



\subsubsection{Detailed description of the decision procedure}
\seclab{b:f:s:decider}

The decision algorithm is depicted in \figref{Decider:infty} (and
\figref{decider:infty:inner}).  The algorithm uses a directed graph
 that has a node  for every free space cell
 whose boundary has a non-empty intersection with the
free space .  These intersections are
defined as the free space intervals ,
,  and , see
\secref{background}.  For any path along the edges of the graph
 from  to , there exists a monotone path that
traverses the corresponding cells of 
while using zero or more affordable tunnels.  A node  can have
an incoming edge from another node , if  and  and either  is a neighboring node, or the two cells
can be connected by an affordable tunnel which starts at the lower
boundary of the cell corresponding to  and ends at the upper
boundary of the cell corresponding to .  The idea of the
algorithm is to propagate reachability intervals  and 
while traversing a sufficiently large subgraph starting from ,
and computing the necessary parts of this subgraph on the fly.  We
store these intervals with the cell  that has them on the top
(resp., right) boundary.  The reachability intervals 
being computed satisfy

and an analogous statement applies to .  The aim is
to determine if either  or .  Throughout the whole algorithm
we also maintain a set of gates , which represents the
endpoints of the horizontal reachability intervals computed so far.

We will traverse the graph by handling the nodes in a row-by-row
order, thereby handling any node  only after we handled the
nodes , where ,  and .
To this end we keep the nodes in a min-priority queue where the node
 has the key .  The correctness of the computed
reachability intervals will follow by induction on the order of these
keys.  Furthermore, it will ensure that we handle each node at most
once and that we traverse at most three of the incoming edges to each
node of the graph.

The queue is initialized with the entire node set at once.  To compute
this initial node set and the corresponding free space intervals we
use \dsref{d:s:magic:A}.  The algorithm then proceeds by handling
nodes in the order of extraction from this queue.  When dequeuing
nodes from the queue, the same node might appear three times
(consecutively) in this queue. Once from each of its direct neighbors
in the grid and once from the initial enqueuing.

In every iteration, the algorithm dequeues the one or more copies of
the same node  and merges them into one node if necessary.
Assume that  has an incoming edge that corresponds to an
affordable tunnel.  Let  be the left gate of
. We invoke \tunnelTest{} to
test if this is the case.  If the call returns \NULL, then there is no
such affordable tunnel.  Otherwise, we know that the returned point
 is contained in .
If there were more than one copies of this node in the queue, we also
access the reachability intervals of the one or two neighboring
vertices (i.e.,  and ).  Using
the reachability information from the at most three incoming edges
obtained this way, we can determine if the cells  and
 are reachable, by computing the resulting
reachability intervals  at the top side and
 the right side of the cell .  Since
the free space within a cell is convex and of constant complexity,
this can be done in constant time.

Now, if  we create a node ,
connect it to  by an edge, we enqueue it, and add the gates of
 to .  If 
we create a node , connect it to  by an edge, and we
enqueue it.  If we discover that the top-right corner of the free
space diagram is reachable this way, we output the equivalent to
``yes'' and the algorithm terminates. In this case we must have added
 as a gate to .  The algorithm may also terminate
before this happens if there are no more nodes in the queue, in this
case we output that no suitable shortcut curve exists.


\begin{figure}[t]
    \begin{center}
        \fbox{\begin{minipage}{\FigWidth}\DeciderFr{}
               \begin{algorithmic}[1]
                   \STATE Let 
                   
                   \STATE Compute  and
                    with
                   
	       
                   \STATE Call 
                   with 
                   \linelab{decider:replace}
                   
                   \STATE Return either ``'' or
                   ``''
                   
               \end{algorithmic}
           \end{minipage}}\end{center}
    \vspace{-0.6cm}\caption{The resulting decision procedure \DeciderFr{}.  A
       detailed description of the complete algorithm is given in
       \secref{b:f:s:decider}. }
    \figlab{Decider:infty}
\end{figure}



\subsection{The main algorithm}
\seclab{algo:main}

The given input is two curves  and .  We want to use the
approximate decision procedure \DeciderFr, described above, in a
binary search like fashion to compute the shortcut \Frechet distance.
Conceptually, one can think of the decider as being exact. In
particular, the algorithm would, for a given value of , call
the decision procedure twice with parameters  and  (using ). If the two calls agree, then
we can make an exact decision, if the two calls disagree, then we can
output a -approximation of the shortcut \Frechet distance.


The challenge is how to choose the right subset of candidate values to
guide this binary search.  Some of the techniques used for this search
have been introduced in previous papers. In particular, this holds for
the search over vertex-vertex, vertex-edge and monotonicity events
which we describe as preliminary computations in \secref{first:stage}.
This stage of the algorithm eliminates the candidate values that also
need to be considered for the approximation of the standard \Frechet
distance and it is almost identical to the algorithm presented in
\cite{dhw-afdrc-12}.

As mentioned before, a monotone path could also become usable by
taking a tunnel.  There are two types of events associated with a
\tunnel family: The first time such that any \tunnel in this family is
feasible, which is the \emphi{creation radius}.  Fortunately, the
creation radii of all \tunnels are approximated by the set of
vertex-vertex and vertex-edge event radii, and our first stage search
(see \secref{first:stage}) would thus take care of such events.

The other events we have to worry about are the first time that the
feasible family of \tunnel{}s becomes usable via a \tunnel (i.e., the
price of some tunnel in this family is below the distance threshold
).  Luckily, it turns out that it is sufficient to search over
the price of the canonical \tunnel{} associated with such a family.
The price of a specific \tunnel can be approximated quickly using
\dsref{d:s:magic:B}.  However, there are  \tunnel
families, and potentially the algorithm has to consider all of them.
Fortunately, because of -packedness, only  of these events
are relevant.  A further reduction in running time is achieved by
using a certain monotonicity property of the prices of these
\tunnel{}s and our ability to represent them implicitly to search over
them efficiently.



\subsubsection{The algorithm -- First stage}
\seclab{first:stage}

We are given two -packed polygonal curves  and 
with total complexity .  We repeatedly compute sets of event values
and perform binary searches on these values as follows.

We compute the set of vertices  of the two curves, and using
well-separated pairs decomposition, we compute, in  time,
a set  of  distances that, up to a factor of two, represents
any distance between any two vertices of . Next, we use \DeciderFr
(with fixed ) to perform a binary search for the atomic
interval in  that contains the desired distance. Let  denote this interval.  If  then we are
done, since we found a constant size interval that contains the
\Frechet distance. Otherwise, we use the decision procedure to verify
that the desired radius is not in the range  and
.  For , , let
 denote the obtained interval.

We now continue the search using only \deciderFr and the simplified
curves  and , where .  We extract the
vertex-edge events of  and  that are smaller than ,
see \secref{background}. To this end, we compute all edges of 
that are in distance at most  of any vertex of  and vice
versa using \dsref{d:s:magic:A}.  Let  be the set of resulting
distances.  We perform a binary search, using \deciderFr to find the
atomic interval  of  that contains the shortcut \Frechet distance between 
and .

Finally, we again search the margins of this interval, so that either
we found the desired approximation, or alternatively we output the
interval ,


\subsubsection{Second Stage -- Searching over tunnel prices}
\seclab{second:stage}




It remains to search over the canonical prices of tunnel families
, where \footnote{Since for the case where  the canonical price
   coincides with the creation event value.}.  After the first stage,
we have an interval , and
simplified curves  and  of which the shortcut \Frechet
distance is contained in  and approximates
.  By \lemref{complexity:l:e:q}, the
number of vertex-edge pairs in distance  is bounded by
.  The corresponding horizontal grid edges in the
parametric space contain the canonical gates which are feasible for
any value in .  Let  denote the
 points in the parametric space that correspond to the
canonical gates of these vertex-edge pairs; that is, for every feasible pair  (a vertex of ) and
 (an edge of ), we compute the closest point  on
 to , and place the point corresponding to  in the free space into .


It is sufficient to consider the tunnel families between these
vertex-edge pairs, since all other families are not feasible in the
remaining search interval.  Thus, if we did not care about the running
time, we could compute and search over the prices of the tunnels
, using \dsref{d:s:magic:B}. Naively, this
would take roughly quadratic time. Instead, we use a more involved
implicit representation of these \tunnel{}s to carry out this task.

\paragraph{Implicit search over \tunnel prices.}

Consider the implicit matrix of tunnel prices  where the entry  is a -approximation to the
price of the canonical tunnel . By
\lemref{monotone:matrix}, the first  values of the \th row of
this matrix are monotonically decreasing up to a constant factor,
since they correspond to tunnels that share the same endpoint 
and are ordered by their starting points  (we ignore the
values in this matrix above the diagonal).  Using \dsref{d:s:magic:B}
we can -approximate a value in the matrix in polylogarithmic
time per entry.  Similarly, the lower triangle of this matrix is
sorted in increasing order in each column. As such, this matrix is
sorted in both rows and columns and one can apply the algorithm of
Frederickson and Johnson \cite{fj-gsrsm-84} to find the desired
value. This requires  calls to \DeciderFr, the evaluation
of  entries in the matrix, and takes  time otherwise.
Here, we are using \DeciderFr as an exact decision procedure.  The
algorithm will terminate this search with the desired constant factor
approximation to the shortcut \Frechet distance.



\section{Analysis}
\seclab{analysis:unbounded}

\subsection{Analysis of the \tunnelTest{} procedure}

\begin{lemma}\lemlab{tunnel:test}Given the left gate  of a free space interval
     and a set of gates , and parameters
     and , the algorithm 
    depicted in \figref{tunnel:test} outputs one of the following:
    \begin{compactenum}[(i)]
        \item A point , such that there
        exists a tunnel  of price
         from a gate
        , or
        
        \item \NULL, in this case, there exists no tunnel of price
        less than or equal to  between a gate of 
        and a point in .
    \end{compactenum}
    Furthermore, in case , there exists no other point  that is the endpoint of a tunnel from 
    with price less than or equal to .
\end{lemma}
\begin{proof}
    The correctness of this procedure follows from the monotonicity of
    the tunnel prices, which is testified by
    \lemref{monotone:shortcut}.  Let  be the
    -approximation to the price of the tunnel, that we
    compute in \lineref{tunnel:compute:phi}.  This tunnel starts at a
    point in  and ends in  and it corresponds to the
    shortest subcurve  of  over any such tunnel.
    \lemref{monotone:shortcut} implies that if  then
    there can be no other tunnel of price less than , which
    corresponds to a subcurve of  that contains .
    Therefore, the price of any tunnel from a point , which lies in the lower left quadrant of , to a
    point that lies in the upper right quadrant of  has a price
    larger than . In particular, this holds for those tunnels
    that end to the right of  in the same free space interval.
    The only other possibility for a tunnel from  to
     is a vertical tunnel that lies to the right of
    .  Observe that a vertical tunnel which is feasible for
     always has price at most , since it corresponds
    to a subcurve of  that is equal to a point which is in
    distance  to the shortcut edge.  In \lineref{col:index}
    and \lineref{range:search:2} we compute the leftmost gate of
     in the lower right quadrant of  which lies in the
    same column as . If there exists such a point with a
    vertical tunnel that ends in the free space interval
    , then we return the endpoint of this tunnel.
    Otherwise we can safely output the equivalent to the answer that
    there exists no tunnel of price less than .
\end{proof}


\subsection{Analysis of the decision procedure}

Clearly, the priority queue operations take  time and
 space, where  is the size of the
node set, which corresponds to the complexity of the free space
diagram.  We invoke the \tunnelTest procedure once for each node.
Since we add at most a constant number of gates for every cell to
, the size of this set is also bounded by .
Therefore, after the initialization the algorithm takes time near
linear in the complexity of the free space diagram.  We can reduce
this complexity by first simplifying the input curves with
 before invoking the \deciderFr
procedure, thereby paying another approximation factor. We denote the
resulting wrapper algorithm with \DeciderFr, it is depicted in
\figref{Decider:infty}.  Now, the initial computation of the nodes
takes near-linear time by \dsref{d:s:magic:A} and therefore the
overall running time is near linear.  A more detailed analysis of the
running time can be found in the following.

\begin{lemma}\lemlab{s:y:m:f:s}Given parameters  and  and two
    -packed polygonal curves  and  {in }
    of total complexity . The algorithm \DeciderFr depicted in
    \figref{Decider:infty} outputs one of the following:
    \begin{inparaenum}[(i)]
        \item ``'', or
	\item ``''.
    \end{inparaenum}
    In any case, the output returned is correct. The running time is
    , where .
\end{lemma}

\begin{proof}
    The algorithm  computes the simplified curves  and 
    with , before invoking the algorithm
    \deciderFr described in \figref{decider:infty:inner} on these
    curves.  By the correctness of the \tunnelTest{} procedure (i.e.,
    \lemref{tunnel:test}), one can argue by induction that the subsets
    of points of  intersecting a
    grid edge are sufficiently approximated by the reachable intervals
    computed by \deciderFr (see \Eqrefpage{reachable:good}).  By
    \lemref{t:r:i:n:e:q:s:u:b}, this approximates the decision with
    respect to the original curves sufficiently.
    
    It remains to analyze the running time. By
    \lemref{complexity:l:e:q}, the size of the node set of the graph
     is bounded by .  This also bounds the
    size of the point set  and the number of calls to the
    \tunnelTest procedure, as those are at most a constant number per
    node.  During the \tunnelTest procedure, which is depicted in
    \figref{tunnel:test}, we\smallskip \begin{compactenum}[\;\;\;(A)]
        \item approximate the price of one \tunnel in
        \lineref{tunnel:compute:phi}, and
        \item invoke two orthogonal range queries on the set
         in \lineref{range:search} and
        \lineref{range:search:2}.
    \end{compactenum}
    \smallskip As for (A), building the data structure that supports this kind of
    queries takes  time by \dsref{d:s:magic:B}. Since we perform  such
    queries, this takes  time overall.  As for
    (B), again, the set of gates  is a finite set of two
    dimensional points and we can use two dimensional range trees
    (with fractional cascading as described in \cite{bcko-cgaa-08}) to
    support the orthogonal range queries. We want to build this tree
    by adding  points throughout the algorithm execution.  Since
    the range tree is a static data structure, we have to make it
    dynamic, but we only need to support insertions, and no deletions.
    This can be easily done by using the logarithmic method if we
    allow an additional logarithmic factor to the running time, see
    also \cite{bs-dspsd-80, o-ddds-83}.  In this method, the point set
    is distributed over  static range trees, which need to
    be queried independently and which are repeatedly rebuilt
    throughout the algorithm.  Overall, maintaining this data
    structure and answering the orthogonal range queries takes  time.
    
    During the algorithm, we maintain a priority queue, where each
    node is added and extracted at most three times. As such, the
    priority queue operations take time in .  The initial
    computation of the node set takes 
    by \dsref{d:s:magic:A}.
    
    Therefore, the overall running time is ,
    which is
where .
\end{proof}





\begin{observation}
    \obslab{reparametrizations}It is easy to modify the \deciderFr algorithm such that it also
    outputs the respective shortcut curve and reparametrization which
    satisfies the \Frechet distance. We would modify the tunnel
    procedure such that it returns not only the endpoint, but also the
    starting point of the computed tunnel. During the algorithm, we
    then insert an edge for each computed tunnel, thereby creating at
    most three incoming edges to each node.  After the algorithm
    terminates, we can trace any path backwards from  to
     in the subgraph computed this way. This path encodes the
    shortcut curves as well as the reparametrizations.\end{observation}

\subsection{Analysis -- understanding \tunnel events}
\seclab{shortcut:price}

The main algorithm uses the procedure \DeciderFr to perform a binary
search for the minimum  for which the decision procedure
returns ``yes''.  In the problem at hand we are allowed to use
\tunnels to traverse the free space diagram, and it is possible that a
path becomes feasible by introducing a \tunnel.  The algorithm has to
consider this new type of critical events.






Consider the first time (i.e., the minimal value of ) that a
decision procedure would try to use a \tunnel of a certain family.


\begin{defn}\deflab{r:create}Given a \tunnel family , we
    call the minimal value of  such that
     is non-empty the
    \emphi{creation radius} of the \tunnel family and we denote it
    with .  (Note, that the price of
    a \tunnel might be considerably larger than its creation radius.)
\end{defn}


\begin{lemma}\lemlab{r:create}The creation radius , see \defref{min:radius}.
\end{lemma}

\begin{proof}
    Recall that the creation radius of the \tunnel family is the
    minimal value of  such that any \tunnel in this family is
    feasible. Let  be the closest point on  to , and
     the closest point on  to .  If  appears
    before  on , then the canonical \tunnel is realized by
     and  and the claim holds. In
    particular, this is the case if .
    
    Now, the only remaining possibility is that  appears after
     on . It must be that , therefore let
    .  Observe that in this case any \tunnel in
    the family which is feasible for  also has a price that is
    smaller or equal to .
Consider the point  realizing the quantity
    
    Note that  is the subcurve of  corresponding to the
    (vertical) canonical \tunnel in this case.  We claim that for any
    subsegment  (agreeing with the
    orientation of ) we have that .  If  then the claim trivially
    holds.
    
    \begin{figure}[\si{tb}]\center
        \includegraphics[scale=0.8]{figs/stupid_1}
        ~~~~~~\includegraphics[scale=0.8]{figs/stupid_2}
        \caption{Two cases:  appears either before or after 
           along , assuming that  appears after  on
           .}
        \figlab{stupid}
    \end{figure}
    
    Assume that  appears after  along  (the case
    depicted in \figref{stupid}). Since  appears after  along
    , we have that , as
    moving away from  only increases the distance from
    . Therefore,
    
    
    Otherwise, if  appears before  along , as depicted
    in \figref{stupid} on the right, then
    
    since moving away from  only increases the distance from .
    
    This implies that the minimum  for a \tunnel in
     to be feasible is at least
    .  And
     testifies that there is a \tunnel in this family that is
    feasible for this value.
\end{proof}

The following lemma describes the behavior when  rises above a
\tunnel price, such that the area in the free space that lies beyond
this \tunnel potentially becomes reachable by using this \tunnel. More
specifically, it implies that the first time (i.e., the minimal value
of ) that any \tunnel of a family
 is usable (i.e., its price is less
than ), any \tunnel in the feasible set
 associated with this
family will be usable.
\begin{lemma}\lemlab{tunnel:event}Given a value , we have for any \tunnel
     in the feasible subset of a given \tunnel
    family , that
    \begin{compactenum}[(i)]
        \item if , then
        ,
        
        \item otherwise, .
    \end{compactenum}
\end{lemma}
\begin{proof}
    We first handle the case that .  Let  and .
    
\parpic[r]{\includegraphics{figs/c_bridge_lemma_2}}
    
    
    Let  and  be some points on
    these edges, that correspond to  and , respectively.
    Observe that since this is a feasible \tunnel in this family, we
    have that
    
    Consider the optimal \Frechet matching of 
    with , and let  and  be the points on 
    that are matched to  and  by this optimal
    \Frechet matching.  Let , where
     is the subsegment of  minimizing .
    
    We have, by \obsref{f:r:segments}, that
    

    For , this implies , where  is equal for all \tunnels in the
    family.  Now, if   then we have
    
    and
    This proves (i).  Otherwise, we have
    . Which
    implies that , but then , implying (ii).
    
    If  then the \Frechet distance is between the shortcut
    segment and a subsegment of . But this distance is the
    maximum distance between the corresponding endpoints, by
    \obsref{f:r:segments}. As the distance between endpoints of
    shortcuts and subcurves corresponding to \tunnels of
     is at most
    , and by \lemref{r:create} the claim follows.
\end{proof}




\lemref{monotonicity} below implies that the set of creation radii of
all \tunnels is approximated by the set of vertex-vertex and
vertex-edge event radii.  A similar lemma was shown in
\cite{dhw-afdrc-12}, to prove this property for the monotonicity event
values. Therefore, the algorithm eliminates these types of events in
the first stage, in addition to eliminating the vertex-vertex and
vertex-edge events.

\begin{lemma}\lemlab{monotonicity}Consider an edge  of a curve , and
    two vertices  and  of a curve . We have that , where  is in the
    set .
\end{lemma}
\begin{proof}
    First, observe that , as it is the maximum distance of some point on
     from both  and . In particular, if
     then we are
    done.
    
    As such, it must be that . Assume that  is closer to  than , and
    let  be the closest point on  to . By the triangle
    inequality, the distance of  from  is in the range
    .  Observe that  and .  Thus,
    . Note that if
     then we are done, as this
    implies that  is in the range
    . Otherwise,
     is in the range . In either case, the claim
    follows.
    
    The case that  is closer to  than  follows by
    symmetry.
\end{proof}

\subsection{Analysis of the main algorithm}
\seclab{algo:main:analysis}

The following lemma can be obtained using similar arguments as in the
analysis of the main algorithm in \cite{dhw-afdrc-12}. We provide a
simplified proof for the case here, where we are only interested in a
constant factor approximation.


\begin{lemma}\lemlab{first:stage}Given two -packed polygonal curves  and  in
     with total complexity , the first stage of the
    algorithm (see \secref{first:stage}) outputs one of the following:
    \begin{compactenum}[(A)]
        \item a -approximation to the shortcut \Frechet distance
        between  and ;
        \item an interval , and curves  and
         with the following properties:
        \begin{compactenum}[(i)]
            \item  is contained in
             and ,
            
            \item  contains no vertex-edge,
            vertex-vertex, or monotonicity event values and no \tunnel
            creation radii (as defined in \secref{shortcut:price}) of
             and .
        \end{compactenum}
    \end{compactenum}
    The running time is .
\end{lemma}

\begin{proof}
    We first prove the correctness of the algorithm as stated in the
    claim.  The set  approximates the vertex-vertex distances of
    the vertices of  and  up to a factor of
    two. Therefore, the interval , which we
    obtain from the first binary search, contains no vertex-vertex
    distance of  that is more than a factor of two away from
    its boundary.  This implies that the simplification
     results in the same curve for any
    .  An analogous statement holds for
    . Unless, a constant factor approximation is found either
    in the interval  or the interval , the algorithm continues the search using the procedure
    \deciderFr and the curves simplified with .
    
    
    It is now sufficient to search for a constant factor approximation
    to  in the interval
    , since this will approximate the
    desired \Frechet distance by a constant factor.  Indeed, by the
    result of the initial searches, we have that .  \lemref{t:r:i:n:e:q:s:u:b}
    imply that  On the other hand, the
    same lemma implies that  This implies, that
    , since
    .  Note that
    this also proves the correctness of , since the returned
    interval is contained in .
    
    Observe that the set of vertex-vertex distances of  and 
    is contained in the set of vertex-vertex distances of 
    and . Clearly,  cannot contain any
    vertex-vertex distances of  and .  The algorithm
    therefore extracts the remaining vertex-edge events  from the
    free space diagram and performs a binary search on them.  We
    obtain the atomic interval , which
    contains no vertex-edge events of  and
    . Note that by \Eqrefpage{c:tunnel:endpoints} and \lemref{r:create},
    the monotonicity event values, as described in
    \secref{background}, coincide with the values of  where a
    tunnel within a column of the parametric space becomes feasible,
    that is, with the quantity .  By
    \lemref{monotonicity}, these event values would have to lie within
    a factor two of the boundaries of the interval .
    Therefore, we again search the margins of this interval, so that
    either we found the desired approximation, or alternatively, it
    must be in the interval ,
    which now contains no vertex-vertex, vertex-edge, monotonicity or
    tunnel creation events of  and .  Since 
    is the interval that the algorithm returns, unless it finds a
    constant factor approximation to the desired \Frechet distance,
    the above argumentation implies  and .
    
    As for the running time, computing the set  using
    well-separated pairs decomposition can be done in 
    time, see \cite{dhw-afdrc-12}.  Computing the set  takes time
    in , by \dsref{d:s:magic:A} with
     and .  The algorithm invokes the
    decision procedure  times, and this dominates the
    overall running time, see \lemref{s:y:m:f:s}.
\end{proof}

\begin{lemma}\lemlab{rand:algo}Given two -packed polygonal curves  and  in
     of total complexity , one can compute a constant factor
    approximation to .  The running time
    is .
\end{lemma}

\begin{proof}
    First, the algorithm performs the preliminary computations as
    described in \secref{first:stage}. By \lemref{first:stage}, we
    either find a constant factor approximation, or we obtain an
    interval  and simplified curves  and
    . Furthermore, the interval  does not contain
    any vertex-vertex, vertex-edge, monotonicity, or tunnel creation
    events of  and .  Let  be the canonical gates
    that are feasible in the \mbox{-free} space of  and
    . We have that  and we can compute
    them using \dsref{d:s:magic:A} in  time, for
    .  Thus, the running time up to this stage is bounded by
    , by \lemref{first:stage}.
    
    Now, we invoke the second stage of the algorithm described in
    \secref{second:stage} on the matrix of implicit tunnel prices
    defined by  and return the output as our solution.
    
    Consider a monotone path in the parametric space that corresponds
    to the optimal solution. If the price of this path is determined
    by either a vertex-vertex, a vertex-edge or a monotonicity event
    then we have found an approximation to the shortcut \Frechet
    distance already in the first stage of the search algorithm.  If
    it is dominated by a tunnel price and this tunnel has both
    endpoints in the same column of the free space, then by
    \obsref{f:r:segments} it is a creation radius. By
    \lemref{r:create} this is equivalent to the minimum radius of the
    corresponding tunnel family. By \obsref{min:radius:eq} the minimum
    radius corresponds to either a vertex-edge event or a monotonicity
    event. Thus, it lies outside the interval , since
    by \lemref{first:stage} these critical values were eliminated in
    the first stage. Otherwise, this critical tunnel has to be between
    two columns.  Let  be the price of this \tunnel (which is
    also the price of the whole solution).
    
    Consider what happens to this path if we slightly decrease
    . Since  is optimal, then the critical tunnel
    either ceases to be feasible or its price is not affordable
    anymore.
    
    If the critical tunnel is no longer feasible, then one of its
    endpoints is also an endpoint of the free space interval it lies
    on. Consider the modified path in the free space, which uses the
    new endpoint of the free space interval.  If the free space
    interval is empty, then this corresponds to a vertex edge event,
    and this is not possible inside the interval .
    The other possibility is that the path is no longer
    monotone. However, this corresponds to a monotonicity event, which
    again we already handled because of \lemref{first:stage}.
    
    If the tunnel is still feasible, then it must be that the
    endpoints of this \tunnel are contained in the interior of the
    free space interval and not on its boundary.  Now
    \lemref{tunnel:event} (i) implies that the price of this \tunnel
    is equal to the price of the canonical tunnel.  As such, the price
    of the optimal solution is being approximated correctly in this
    case.
    
    
    
    
    Observe that in the second stage we are searching over all tunnel
    events that lie in the remaining search interval (whether they are
    relevant in our case or not).  Hence, the search would find the
    correct critical value, as it is one of the values considered in
    the search.
    
    The running time of second stage is bounded by:
    \begin{compactenum}[(A)]
        \item  time to compute the needed
        entries in the matrix, using \dsref{d:s:magic:B}.
        
        \item  time for the
         calls to \DeciderFr.
        
        \item  for other computations.
    \end{compactenum}
    
    Therefore, the overall running time of the algorithm is .
\end{proof}




\subsection{Result}

The following theorem states the main result for approximating the
shortcut \Frechet distance.


\begin{theorem}\thmlab{main}Given two -packed polygonal curves  and  in
    , with total complexity , and a parameter ,
    the algorithm of \secref{algo:unbounded:general} computes a
    -approximation to the shortcut \Frechet distance between
     and  in  time.  The algorithm also
    outputs the shortcut curve of  and the reparametrizations
    that realize the respective shortcut \Frechet distance.
\end{theorem}

\begin{proof}
    The result follows from \lemref{rand:algo}.  This yield an
    interval  that contains the value of the optimal
    solution. We can turn any constant factor approximation into a
    -approximation, using \DeciderFr with  by
    invoking it over a constant number of subintervals of the form
    , where . These intervals
    are required to cover , and as such, \DeciderFr would
    return the desired approximation for one of them (the running time
    of each call to \DeciderFr is stated in \lemref{s:y:m:f:s}).
    
    It is easy to modify the algorithm, such that it also outputs the
    shortcut curve and the reparametrizations realizing the
    approximate \Frechet distance, see \obsref{reparametrizations}.
\end{proof}


\begin{remark}\remlab{k:shortcut}One can extend the algorithm of \thmref{main} so that it
    approximates the \Frechet distance where only  shortcuts are
    allowed. The basic algorithm is similar, except that we keep track
    for the points of  how many shortcuts were used in
    computing them. The resulting algorithm has running time
     (for  a constant).  This version of the
    algorithm is described in the first author's thesis, see
    \cite{d-raapg-13}.
\end{remark}



\section{Data structures for \Frechet-distance queries}
\seclab{single:segment:query} 

Given a polygonal curve  in , we build a data structure
that supports queries for the \Frechet distance of subcurves of 
to query segments . We describe the data structure in three
stages.  After establishing some basic facts in \secref{query:helper},
we first describe a data structure that achieves a constant factor
approximation in \secref{queries:stage:one}.  We proceed by describing
a data structure that answers queries for the \Frechet distance of the
entire curve to a query segment up to an approximation factor of
 in \secref{queries:stage:two}.  Finally, we describe how to
combine these two results to obtain the final data structure for
segment queries in \secref{queries:stage:three}.


\subsection{Useful lemmas for curves and segments}
\seclab{query:helper}

\begin{defn}\deflab{spine}
    For a curve , the segment connecting its endpoints is its
    \emphi{spine}, denoted by .
\end{defn}

The following is a sequence of technical lemmas that we need later
on. These lemmas testify that:
\begin{compactenum}[\quad (A)]
    \item The spine of a curve is, up to a factor of two, the closest
    segment to this curve with respect to the \Frechet distance, see
    \lemref{f:r:basic}.
    
    \item The \Frechet distance between a curve and its spine is
    monotone, up to a factor of two, with respect to subcurves, see
    \lemref{monotone:subcurve}.
    
    \item Shortcutting a curve cannot increase the \Frechet distance
    of the curve to a line segment, see \lemref{shortcut:segment} or
    \cite{bbw-cfdsp-08}.
\end{compactenum}

\begin{lemma}\lemlab{f:r:basic}Let  be a segment and  be a curve.  Then,
    \begin{inparaenum}[(i)]
        \item \itemlab{shortcut} 
        , and
        \item .
    \end{inparaenum}
\end{lemma}

\begin{proof}
    Let  and  be the endpoints of ; that is
    .
    
    (i) Since in any \matching{} of  with  it must be
    that  is matched to , and  is matched to
    , it follows that , by
    \obsref{f:r:segments}.
    
    (ii) By (i) and the triangle inequality, we have that
     which implies the claim.
\end{proof}


\begin{lemma}\lemlab{monotone:subcurve}Given two curves  and , such that  is a
    subcurve of .  Then, we have that
    .
\end{lemma}


\begin{proof}
    Consider the \matching{} that realizes the \Frechet distance
    between  and . It has to match the endpoints of
     to points  and  on .  We have
    that .  By \lemref{f:r:basic} (i), we have
    .
    Now, by the triangle inequality, we have that
    
    \aftermathA
\end{proof}

\begin{lemma}
    \lemlab{shortcut:segment}Let  be a polygonal curve, 
    be a segment, and let  be any two indices. Then, for , we have .
\end{lemma}

\begin{proof}
    Consider the \matching{} realizing ,
    and break it into three portions:
    \begin{compactitem}
        \item the portion matching  with a
        ``prefix'' ,
        \item the portion matching  with a
        subsegment , and
        \item the portion matching  with a
        ``suffix'' .
    \end{compactitem}
    Now, by \lemref{f:r:basic} \itemref{shortcut}, we have that
    
    \aftermathA
\end{proof}




\subsection{Stage 1: Achieving a constant-factor approximation}
\seclab{f:r:query:constant}\seclab{queries:stage:one}


In this section we describe a data structure that preprocesses a curve
 to answer queries for the \Frechet distance of a subcurve of
 to a query segment up to a constant approximation factor. This
data structure will be the basis for later extensions.

A query is specified by points  and . Here  and
 are points on  (and we are also given the edges of 
containing these two points), and the points  and  define
the query segment. Our goal is to approximate .

\subsubsection{The data structure}

\paragraph{Preprocessing.}
Build a balanced binary tree  on the \emph{edges} of .
Every internal node  of  corresponds to a subcurve of
, denoted by . Let  denote the
spine of  (\defref{spine}).  For every node, we
precompute its \Frechet distance of the curve  to the
segment . Let  denote this distance.

\paragraph{Answering a query.}
For the time being, assume that  and  are vertices of .  In
this case, one can compute, in  time,  nodes
 of , such that . We compute the polygonal curve , and
compute its \Frechet distance from the segment . We denote
this distance by . We return

as the approximate distance between {} and the subcurve
.

\subsubsection{Analysis}

\begin{lemma}
    \lemlab{easy:shortcut}Given a polygonal curve  with  edges, one can preprocess
    it in  time, such that for any pair  of
    vertices of  and a segment , one can compute, in
     time, a -approximation to
    .
\end{lemma}

\begin{proof}
    The construction of the data structure and how to answer a query
    is described above. For the preprocessing time, observe that
    computing the \Frechet distance of a segment to a polygonal curve
    with  segments takes  time
    \cite{ag-cfdbt-95}. Hence, the distance computations in each level
    of the tree  take  time, and 
    time overall.
    
    As for the query time, computing  takes  time,
    and computing its \Frechet distance from  takes  time \cite{ag-cfdbt-95}.
    
    Finally, observe that the returned distance  is a
    realizable \Frechet distance, as we can take the \matching{}
    between  and , and chain it with the \matching{}
    of every edge of  with its corresponding subcurve of
    . Clearly, the resulting \matching{} has width at most
    .
    
    Let  be the index realizing . Then, by repeated application of
    \lemref{shortcut:segment}, we have that . Thus,
    
    To see the last step, consider the \matching{} realizing
    , and consider the subsegment  of  that is being matched to . Clearly, .
\end{proof}


\begin{theorem}\thmlab{data:structure:constant}Given a polygonal curve  with  edges, one can preprocess
    it in  time and using  space, such that,
    given a query specified by
    \begin{compactenum}[\quad(i)]
        \item a pair of points  and  on the curve ,
        \item the edges containing these two points, and
        \item a pair of points  and ,
    \end{compactenum}
    one can compute, in  time, a
    -approximation to .
\end{theorem}

\begin{proof}
    This follows by a relatively minor modification of the above
    algorithm and analysis. Indeed, given  and  (and the edges
    containing them), the data structure computes the two vertices
     that are endpoints of these edges that lie between 
    and  on the curve. The data structure then concatenates the
    segments  and  to the approximation  (here
     is computed for the vertices  and ).  The remaining
    details are as described above.
\end{proof}


\subsection{Stage 2: A segment query to the entire curve}
\seclab{queries:stage:two}

In this section we describe a data structure that preprocesses a curve
to answer queries for the \Frechet distance of the entire curve to a
query segment up to an approximation factor of . We will use
this data structure as a component in our later extensions.



\subsubsection{The data structure}

We need the following relatively easy construction of an exponential
grid. \figref{exp:sprinkle} illustrates the idea.
The details can be found in \cite{d-raapg-13}. 
\begin{lemma}[\cite{d-raapg-13}]\lemlab{exp:grid}
    Given a point , a parameter  and an interval
     one can compute in  time and space an exponential grid of points , 
    such that for any point  with ,
    one can compute in constant time a grid point  with 
    .
\end{lemma}

\paragraph{Preprocessing.}
We are given a polygonal curve  in  with  segments, and
we would like to preprocess it for -approximate \Frechet
distance queries against a query segment. To this end, let , where  is the spine of .  We construct an
exponential grid  of points around  with the range 
 as described in \lemref{exp:grid}
 and illustrated in \figref{exp:sprinkle}.  
We construct the same grid  around the vertex~.

\begin{figure}\centering
\includegraphics{figs/exponential_grid_sprinkle}
\caption{{We build an exponential grid around each endpoint of the curve, such that 
for any point , which has distance to the endpoint in the range
, there exists a grid point  which is relatively close by.}}
\figlab{exp:sprinkle}
\end{figure}

Now, for every pair of points  we
compute the \Frechet distance  and store it. Thus, we take 
 time to build a data structure 
that requires  space, where .

\paragraph{Answering a query.}
Given a query segment , we compute the distance

If , then we return  as the approximation
to the distance .  
If  then we return  as the approximation. 
Otherwise, let  (resp., ) be the nearest neighbor to
 in  (resp., ). We return the distance

as the approximation.

\subsubsection{Analysis}

\begin{lemma}
    \lemlab{epsilon:query:w:curve}Given a polygonal curve  with  vertices in , one
    can build a data structure, in  
    time, that uses  space, such that given a query segment 
    one can -approximate  in
     time, where .
\end{lemma}
\begin{proof}
    The data structure is described above. 
    Given  we compute the distance of the
    endpoints of this segment from the endpoints of . If they are too
    close, or if one of them is too far away, then we are done since in this
    case the \Frechet distance is dominated either by these distances or by the
    precomputed value .  Otherwise, we find the two cells in the exponential
    grid that contain  and  (that is, the indices of the grid
    points that are close to them) as described above. 
    Using the indices of the grid points, we can directly look-up the
    approximation of the \Frechet distance in constant time.
    
    Now, we argue about the quality of the approximation using the notation
    which is also used above. 
    There are three cases: either
    \begin{inparaenum}[(i)]
    \item , or
    \item , or
    \item .
    \end{inparaenum}
    Let  be the returned value. We claim that in all three cases, it
    holds that
    
    First note that by the triangle inequality, 
    
    Now, in case (i) above,  dominates the distance value and we return .
    Thus, \Eqref{sprinkle:claim} follows from \Eqref{yatq}.

    In case (ii),  dominates the distance value and we return
    . Since  is at most ,  again
     \Eqref{sprinkle:claim} follows from \Eqref{yatq}.

    In case (iii), the precomputed \Frechet distance of  to
     dominates the distance. Recall that we return 
      in
    this case.
    Again, by the triangle inequality, it holds
    that
    
    Since  is at least  and by \obsref{f:r:segments}, \lemref{exp:grid} implies that 
    
    thus, since also  is at most  it follows by \Eqref{t:eq:2} that 
    
    This implies the claim.
\end{proof}











\subsection{Stage 3: A segment query to a subcurve}
\seclab{queries:prelim}\seclab{queries:stage:three}\seclab{f:r:query}

In this section we describe a data structure that preprocesses a curve
 to answer queries for the \Frechet distance of a subcurve of
 to a query segment up to an approximation factor of
. For this we combine the data structures developed in the
previous sections.

As in \secref{f:r:query:constant}, a query is defined by two points
 and  on  and a segment with endpoints  and
. The goal is now a -approximation to .

\subsubsection{The data structure}

\paragraph{Preprocessing.}
Let  be a given polygonal curve with  vertices. We build the
data structure of \thmref{data:structure:constant}. Next, for each
node of the resulting tree , we build for its subcurve the data
structure of \lemref{epsilon:query:w:curve} using .

\begin{figure}\centering
    \includegraphics{figs/multigraph}
    \caption{Schematic illustration of the graph  on the
       vertex set .}
    \figlab{vi:graph}
\end{figure}


\paragraph{Answering a query.}
Using the data structure of \thmref{data:structure:constant} we first
compute a -approximation  to ; that is, .  This query also
results in a decomposition of  into 
subcurves. Let  be the vertices
of these subcurves, where  and  are subsegments of
.

We want to find points on  that can be matched to
 under a -approximate \Frechet matching.
To this end, we uniformly partition the segment  into
segments of length at most {, where  is a
   sufficiently large constant which we define later.}  Let  be
the set of vertices of this implicit partition.  For each vertex
, for , we compute its nearest point on , and let  be the set of all vertices in
 that are in distance at most  from .  The set  is
the set of candidate points to match  in the \matching{} that
realizes the \Frechet distance.

Now, we build a graph  where  is the multiset
of vertices. Two points  and  are connected
by a direct edge in this graph if and only if  is after  in the
oriented segment . See \figref{vi:graph} for a schematic
illustration. The price of such an edge  is a
-approximation to the \Frechet distance between
 and .  The portion
 of the curve corresponds to a node in
, and this node has an associated data structure that can
answer such queries in constant time (see
\lemref{epsilon:query:w:curve}).  For any point , we
directly compute the \Frechet distance  with . Similarly, we compute, for each , the \Frechet
distance of the segment  to the segment . We add
the corresponding edges to  together with the vertices 
and .

Using a variant of Dijkstra's algorithm for bottleneck shortest paths,
we now compute a path in this graph which minimizes the maximum cost
of any single edge visited by the path, connecting  with
. The cost of this path is returned as the approximation to the
\Frechet distance between  and .
Intuitively, this path corresponds to the cheapest matching of
 (broken into subcurves by the vertices ) with , where
, , and every subcurve
 is matched with two points in the
corresponding sets  and .


\subsubsection{Analysis}

\paragraph{Query time.}
Computing the set of vertices  takes  time.  The graph  has  vertices and they
can be computed in  time. In particular, the number of
vertices in  is bounded by , since they are spread
apart on a line segment by  and contained inside a
ball of radius . Thus, the graph has 
edges connecting  with  and  edges in
total. The cost of each edge can be computed in constant time, see
\lemref{epsilon:query:w:curve}.  Computing the cheapest path between
 and  in  can be done in  time, using Dijkstra's
algorithm for bottleneck shortest paths. Overall, the query time is


\begin{figure}\centering
    \includegraphics{figs/snapping}
    \caption{Illustration of the error introduced by snapping. }
    \figlab{snapping-error}
\end{figure}

\paragraph{Quality of approximation.}

Consider the \matching{} that realizes the \Frechet distance between
the query segment  and the subcurve , and
break it at the vertices of . Now, snap the matching
such that the endpoints of  are mapped to
their closest vertices in  and , respectively, for all
. This introduces an error of at most , if we choose , see \figref{snapping-error} for an illustration.  We get
another factor of  error since we are
approximating the price of these portions using
\lemref{epsilon:query:w:curve}. Therefore, the approximation has price
at most



\paragraph{Preprocessing time and space.}
Building the data structure described in
\thmref{data:structure:constant} takes  time. For each
node  of this tree, building the data structure of
\lemref{epsilon:query:w:curve} takes  time per node, where  is the number of
vertices of the curve stored in the subtree of . As such, overall,
the preprocessing time is . For each
node, this data structure requires  space and
thus the overall space usage is , where
.




Putting the above together, we get the following result.

\begin{theorem}\thmlab{segment:queries:f:r}Given a polygonal curve  with  vertices in , one
    can build a data structure, in 
    time, that uses  space, such that for a
    query segment , and any two points  and  on the
    curve (and the segments of the curve that contain them), one can
    -approximate the distance
     in  time, and .
\end{theorem}

We emphasize that the result of \thmref{segment:queries:f:r} assumed
nothing on the input curve . In particular, the curve  is
not necessarily -packed.



\section{Universal vertex permutation and its applications}
\seclab{f:r:simpl}


We would like to extend the data structure described in
\secref{queries:stage:one} to support queries with curves of more than
one segment. For this, we first introduce a new method to represent a
polygonal curve in a way such that we can extract a simplification
with a small number of segments quickly. We describe this method in
\secref{u:simpl} and we describe the extension of the data structure
in \secref{k:seg:query}.


\subsection{Universal vertex permutation}
\seclab{u:simpl}

We use the data structure described in \secref{queries:prelim} to
preprocess , such that, given a number of vertices ,
we can quickly return a simplification of  which has
\begin{compactenum}[(i)]
    \item  vertices of the original curve and
    \item minimal \Frechet distance to , up to a constant factor,
    compared to any simplification of  with only  vertices.
\end{compactenum}
The idea is to compute a permutation of the vertices, such that the
curve formed by the first  vertices in this permutation is a good
approximation to the optimal simplification of a curve using (roughly)
 vertices.


\begin{defn}
    Let  be a polygonal curve with vertices
    . Let  be a
    subset of the vertices that contains the endpoints of .  We
    call the polygonal curve obtained by connecting the vertices in
     in their order along  a \emphi{spine curve} of
     and we denote it with . Additionally
    we may call  a \emphi{k-spine curve} of
     if it has  vertices.
\end{defn}


\begin{defn}
    Given a polygonal curve  and a permutation  of the vertices of , where
     and  are the endpoints of , let
     be the subset  of
    the vertices for any .  We call  a
    \emphi{universal {vertex} permutation} if it holds that
    \begin{compactenum}[\quad(i)]
        \item , for any , and
        \item , for any polygonal curve  with
         vertices,
    \end{compactenum}
    where ,  and  are constants larger than
    one which do not depend on .
\end{defn}


\subsubsection{Construction of the permutation}

We compute a universal vertex permutation of .  The idea of the
algorithm is to estimate for each vertex the error introduced by
removing it, and repeatedly remove the vertex with the lowest error in
a greedy fashion.

Specifically, for each vertex  that is not an endpoint of ,
let  be its predecessor on  and let 
be its \postdecessor on . Let  be a
\constN-approximation of
.  Insert the vertex  with
weight  into a min-heap .  Repeat this for all
the internal vertices of .

At each step, the algorithm extracts the vertex  from the heap
 having minimum weight. Let  and
 be the predecessor and
\postdecessor of  in the curve ,
respectively, where  denotes the set of vertices currently in
the heap with the addition of the two endpoints of .

The algorithm removes  from  and updates the weight of
 and  in  (if the vertex being updated is an
endpoint of  its weight is  and its weight is not being
updated). Updating the weight of a vertex  is done by computing
its predecessor and \postdecessor vertices in the current curve
 (i.e.,  and ) and approximating the \Frechet
distance of the subcurve of (the \emph{original} curve)  between
these two vertices and the segment . Formally, the updated weight of  is
, which is a \constN-approximation to

The updated weight of  is computed in a similar fashion.

The algorithm stops when  is empty. Reversing the order of the
handled vertices, results in a permutation
, where  and  are the
two endpoints of .








\paragraph{Implementation details.}

Using \thmref{segment:queries:f:r}, the initialization takes  time overall, using .  In addition, the
algorithm keeps the current set of vertices of  in a doubly
linked list in the order in which the vertices appear along the
original curve . In each iteration, the algorithm performs one
extract-min from the min-heap , and calls the data structure of
\thmref{segment:queries:f:r} twice to update the weight of the two
neighbors of the extracted vertex. As such, overall, the running time
of this algorithm is .

\paragraph{Extracting a spine curve quickly.}
Given a parameter , we would like to be able to quickly compute the
spine curve , where . To this end, we compute for , the spine curve  by removing the unused
vertices from .  Naturally, we also store
the original curve . Clearly, one can store these 
curves in  space, and compute them in linear time. Now, given
, one can find the first curve in this collection that has more
vertices than , copy it, and remove from it all the unused
vertices. Clearly, this query can be answered in  time.


\subsubsection{Analysis}


\begin{lemma}\lemlab{spine:curve:f:r}Let  be the permutation computed
    above. Consider a value , and let  be an ordering of the vertices of  by their order along . Then, it holds that
    .
\end{lemma}
\begin{proof}
    This is immediate as one can combine for , the
    \matching{}s realizing  to obtain \matching{}s of
     and , and such that the \Frechet distance
    is the maximum used in any of these \matching{}s.
\end{proof}


Let  be the permutation of the vertices of 
as computed in the preprocessing stage, and let  denote
weight of vertex  at the time of its extraction.  We have the
following three lemmas to prove that the computed permutation is
universal.

\begin{lemma}\lemlab{universal:0}For any , it holds that .
\end{lemma}
\begin{proof}
    We show that the weight of a vertex at the time of extraction is
    at most  times smaller than the final weight of any of the
    vertices extracted before this vertex.  Let  be a vertex
    and let  be the weight of this vertex at
    the time of extraction of some other vertex , with . Clearly, , since the algorithm extracted 
    with the minimum weight at the time.  If  then the claim
    holds.
    
    Otherwise, if , then there must be a vertex which caused
    the weight of  to be updated. Let  be the minimum index
    such that  and
    .  We have that
     is a -approximation of the \Frechet distance
     for
    two vertices  and .  Similarly, we have that
     is a -approximation of the
    \Frechet distance
     for
    two vertices  and .  Observe that since the
    extraction of  caused the weight of  to be
    updated, it must be that  is a
    subcurve of .  Hence, by
    \lemref{monotone:shortcut:base}, we have that
    
    Now it follows that , which proves the claim.
\end{proof}


\begin{lemma}\lemlab{universal:i}For any  it holds that .
\end{lemma}
\begin{proof}
    Let  be the vertices in  in
    the order in which they appear on .
    Consider the mapping between  and this spine curve, which
    associates every edge  of
     with the subcurve
    . Clearly, it holds that
    
    Indeed, if  is the \postdecessor of  on
    , then
    , otherwise, there must be a vertex which
    appears on  in between  and , which is
    contained in  and the weight of
    this vertex is the approximation of this distance at the time of
    extraction.  Now it follows by \lemref{universal:0} that
    .
\end{proof}


\begin{lemma}\lemlab{universal:ii}For any , let  be the curve with the
    smallest \Frechet distance from  with  vertices (note,
    that  is not restricted to have its vertices lying on
    ). We have that , where .
\end{lemma}
\begin{proof}
    Let  be the mapping realizing the \Frechet
    distance between  and . Let , for .
    
    \parpic[r]{\includegraphics[scale=1]{figs/universal}}
    
    Since  has only  vertices, it breaks  into 
    subcurves. Since, , there must be three
    consecutive vertices  on
     and two vertices 
    of , such that the vertices  appear on the subcurve , see the figure on the
    right.
    
    Now,  and by \lemref{f:r:basic}, we have
    
    as the simplification algorithm removed the minimum weight vertex
    at time  (i.e., ).
\end{proof}

\subsubsection{The result}

\begin{theorem}
    \thmlab{f:r:simpl:result}Given a polygonal curve  with  edges, we can preprocess it
    using  space and  time, such that, given
    a parameter , we can output in  time a
    -spine curve  of  and a value
    , such that \smallskip
    \begin{compactenum}[\quad(i)]
        \item ,
and \smallskip
        \item ,
    \end{compactenum}
    \smallskip where  is the polygonal curve with  vertices with minimal
    \Frechet distance from~.  (For  we output 
    and ).
\end{theorem}

\begin{proof}
    The algorithm computing the universal vertex permutation and its
    associated data structure is described above, for .  Specifically, it returns the spine curve
     as the required approximation, with the
    value . Computing  takes  time.
    By \lemref{universal:i} and \lemref{universal:ii}, we have that
     and  satisfy the claim.
    
    Building the data structure takes  time, and
    it uses  space using . Each query to this
    data structure takes  time. We perform a
    constant number of these queries to the data structure per
    extraction from the heap, thus getting the claimed preprocessing
    time.
\end{proof}


\subsection{Extending the data structure for \Frechet-distance queries}
\seclab{k:seg:query}

We use the universal vertex permutation described in the previous
section to extend our data structure of \secref{queries:stage:one} to
support queries with more than one segment.

\subsubsection{The data structure}

The input is a polygonal curve  with  vertices.

\paragraph{Preprocessing.}
Similar to the algorithm of \secref{f:r:query:constant}, build a
balanced binary tree  on . For every internal node 
of  construct the data structure of \thmref{f:r:simpl:result}
for , denoted by , and store it at
.

\paragraph{Answering a query.}
Given any two vertices  and  of , and a query polygonal
curve  with  segments, the task is to approximate
. We initially proceed as in
\secref{f:r:query:constant}, computing in  time,  nodes  of , such that
.  Now, extract a simplified curve
with  vertices from , denoted by
, for , where .  For , let  denote the
simplification error (as returned by ), where
 and
 is a lower bound to the \Frechet distance of \emph{any}
curve with at most  vertices from , for
 (see \thmref{f:r:simpl:result}).

Next, compute the polygonal curve , and its \Frechet distance from
; that is, .  We return

as the approximate distance between  and .

\subsubsection{Analysis}

\paragraph{Query time.}
Extracting the  relevant nodes takes  time.
Querying these  data structures for the simplification of the
respective subcurves, takes  overall, by
\thmref{f:r:simpl:result}.  Computing the \Frechet distance between
the resulting simplification  of , which has  edges, and  takes  time
\cite{ag-cfdbt-95}.  Thus the overall time used for answering a query
is bounded by .

\paragraph{Preprocessing time and space.}
Building the initial tree  takes  time and it requires
 space.  Let  denote the number of vertices of
.  For each node~, computing the additional
information and storing it requires  space and
 time.  Recall that
 is a balanced binary tree and for the nodes  contained in one level of the tree it holds that . Thus, computing and storing the
additional information takes an additional  time
and  space by \thmref{f:r:simpl:result}.



\paragraph{Quality of approximation.}
By the following lemma the data structure achieves a constant-factor
approximation.


\begin{lemma}\lemlab{k:seg:query:vertices}Given a polygonal curve  and a query curve  with 
    segments, the value  (see \Eqref{delta}) returned by the
    above data structure is a constant-factor approximation to
    .
\end{lemma}
\begin{proof}
    Clearly,  bounds the required distance from above, as one
    can extract a \matching{} of  and  realizing
    . As such, we need to prove that , where .
    
    So, let  be the mapping
    realizing , and let , for . Clearly, .  Since  has at
    most  vertices, by \thmref{f:r:simpl:result}, we have
    
    for .  In particular, we have .
    Now, by the triangle inequality, we have that
    
    As such, .  Now, .
\end{proof}

\paragraph{The result.}
Putting the above together, we get the following result. We emphasize
that  is being specified together with the query curve, and the
data structure works for any value of .

\begin{theorem}
    \thmlab{k:seg:query:result}Given a polygonal curve  with  edges, we can preprocess it
    in  time and  space, such that, given
    a query specified by
    \begin{compactenum}[\quad(i)]
        \item a pair of points  and  on the curve ,
        \item the edges containing these two points, and
        \item a query curve  with  segments,
    \end{compactenum}
    one can approximate  up to a
    constant factor in  time.
\end{theorem}

\begin{proof}
    The preprocessing is described and analyzed above. The query
    procedure needs to be modified slightly since the  and  are
    not necessarily vertices of . However, this can be done the
    same way as for the initial data structure in
    \thmref{data:structure:constant}.  Let  be the first and
    last vertices of  contained in . We now
    extract the  nodes  of
    , such that
     We continue with the procedure as described
    above using this node set.  The analysis of
    \lemref{k:seg:query:vertices} applies with minor modifications.
\end{proof}






\section{Conclusions}
\seclab{conclusions}


In this paper, we presented algorithms for approximating the \Frechet
distance when one is allowed to perform shortcuts on the original
curves. More specifically the presented algorithms approximate the
\asymmetric{} \vrestricted{} shortcut \Frechet distance.
Surprisingly, for -packed curves it is possible to compute a
constant factor approximation in a running time which is near linear
in the complexity of the input curves.

We also presented a way to compute an ordering of the vertices of the
curve, such that any prefix of this ordering serves as a good
approximation to the curve in the \Frechet distance, and it is optimal
up to constant factors.
We used this universal vertex permutation to develop a data
structure that can quickly approximate (up to a constant factor) the
(regular) \Frechet distance between a query curve and the input
curve. Surprisingly, the query time is logarithmic in the complexity
of the original curve (and near quadratic in the complexity of the
query curve).

There are many open questions for further research. The most immediate
questions being how to extend our result to the other definitions of a
shortcut \Frechet distance mentioned in the introduction and how to
improve the approximation factor. The work in this paper is a step
towards solving these more difficult questions.  

As for exact computations, it is easy to see that one can obtain
polynomial-time algorithms by modifying the algorithms presented in this paper
even for general polygonal curves, see also \cite{d-raapg-13}.
Surprisingly, a more recent result shows that if the requirement that
shortcuts have to start and end at input vertices is dropped, the
problem of computing the shortcut \Frechet distance becomes
\NPHard~\cite{bds-jnp-13, d-raapg-13}.




\paragraph*{Acknowledgments.}

The authors thank Mark d{}e Berg, Marc van Kreveld, Benjamin Raichel,
Jessica Sherette, and Carola Wenk for insightful discussions on the
problems studied in this paper and related problems.  The authors also
thank the anonymous referees for their detailed and insightful
comments.

\bibliographystyle{alpha}\bibliography{shortcut}


\end{document}
