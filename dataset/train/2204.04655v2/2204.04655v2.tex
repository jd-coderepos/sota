\section{Experiment}
\label{sec:experiment}




\begin{table*}[t]
\centering
\begin{adjustbox}{width=0.95\textwidth}
\begin{tabular}{ll||cc|ccc|c||ccc}
\bottomrule
 &  & \multicolumn{3}{c|}{\textbf{PQ}} & \multicolumn{3}{c}{\textbf{PartPQ}} \\ 
\textbf{Panoptic seg. method}  &  \textbf{Part seg. method}  & All & P & NP & All & P & NP \\ 
\midrule

\textit{\textbf{Cityscapes Panoptic Parts} validation set} \\
UPSNet \cite{xiong2019upsnet}(ResNet50) & DeepLabv3+ \cite{deeplabv3plus}(ResNet50)  & 59.1 & 57.3 & 59.7  & 55.1 & 42.3 & 59.7 \\
DeepLabv3+(ResNet50) \& Mask R-CNN(ResNet50) \cite{maskrcnn} & DeepLabv3+ \cite{deeplabv3plus} (Xception-
65)  & 61.0 & 58.7 & 61.9 &  56.9 & 43.0 & 61.9  \\
\hline
\textbf{Panoptic-PartFormer (ResNet50)}  & & \textbf{61.6} & \textbf{60.0} & \textbf{62.2} & \textbf{57.4} & \textbf{43.9} & \textbf{62.2} \\ 
\hline

EfficientPS \cite{mohan2021efficientps}(EfficientNet)~\cite{tan2019efficientnet} &  BSANet \cite{Zhao2019BSANet}(ResNet101)   & 65.0 & 64.2 & 65.2   & 60.2 & \textbf{46.1} & 65.2  \\
HRNet-OCR (HRNetv2-W48)~\cite{ocrnet,wang2020deep} \& PolyTransform~\cite{liang2020polytransform} &  BSANet \cite{Zhao2019BSANet}(ResNet101)  & 66.2 & 64.2 & 67.0  & 61.4 & 45.8 & 67.0 \\
\hline


\textbf{Panoptic-PartFormer (Swin-base)} & & \textbf{66.6}  & \textbf{65.1}  & \textbf{67.2}  & \textbf{61.9}  & \textbf{45.6} & \textbf{68.0}  \\

\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\small \textbf{Experiment Results on CPP.} Previous works combine results from commonly used (top), and state-of-the-art methods (bottom) for semantic segmentation, instance segmentation, panoptic segmentation and part segmentation. Metrics split into \textit{P} and \textit{NP} are evaluated on scene-level classes with and without parts, respectively.}
\label{tab:experiments_res_city}
\end{table*}


\noindent
\textbf{Datasets.}
We mainly carry out experiments on two datasets including Cityscapes Panoptic Parts (CPP) and PASCAL Panoptic Parts (PPP), which are based on the established scene understanding datasets Cityscapes~\cite{cordts2016cityscapes} and PASCAL VOC~\cite{Everingham2010Pascal}, respectively. The CPP extends with part-level semantics the Cityscapes dataset~\cite{cordts2016cityscapes} and is annotated with 23 part-level semantic classes. In particular, 5 scene-level semantic classes from the {human} and {vehicle} high-level categories are annotated with parts. The CPP contains 2975 training and 500 validation images. PPP extends the PASCAL VOC 2010 benchmark~\cite{Everingham2010Pascal} with part-level and scene-level semantics. PPP has 4998 training and 5105 validation images. To perform fair comparison, following previous settings~\cite{degeus2021panopticparts,Everingham2010Pascal}, we perform experiments 59 scene-level classes (20 things, 39 stuff), and 57 part classes.  We further report Cityscapes Panoptic Segmentation validation set~\cite{cordts2016cityscapes} results as sub-task comparison.


\noindent
\textbf{Experiment Settings.} ResNet~\cite{resnet} and Swin Transformer~\cite{liu2021swin} are adopted as the backbone networks and other layers use Xavier initialization~\cite{xavier_init}. The optimizer is AdamW~\cite{ADAMW} with weight decay 0.0001. The training batch size is set to 16 and all models are trained with 8 GPUs. For PPP datasets, we first pretrain our model on COCO dataset~\cite{coco_dataset} since most previous baselines~\cite{degeus2021panopticparts} are pretrained on COCO. For PPP dataset, we adopt the multiscale training~\cite{detr} by resizing the input images from the scale 0.5 to scale 2.0. We also apply random crop augmentations during training, where the train images are cropped with probability 0.5. For CPP dataset, we follow the similar setting in Panoptic-Deeplab~\cite{cheng2020panoptic} where we resize the images with scale rang from 0.5 to 2.0 and randomly crop the whole image during training with batch size 16. All the results are obtained via single scale inference. 

\noindent
\textbf{Metric.}
We report PartPQ~\cite{degeus2021panopticparts} and PQ~\cite{kirillov2019panoptic} as the main metrics, since PartPQ is a unified metric that contain both scene-level output and part-level output. Part segmentation results such as mIoU can be found in the appendix file. The PartPQ per scene-level class  is formalized as .  is true positive,  is false positive, and  is false negative segments, receptively. The definition of these is based on the Intersection Over Union (IOU) between a predicted segment  and a ground-truth segment  for a class  (where ,  is the label set). A prediction is a  if it has an overlap with a ground-truth segment with an . An  is a predicted segment that is not matched with the ground-truth, and an  is a ground-truth segment not matched with a prediction.  contains two cases (part and non-part): 




\subsection{Main Results}

\noindent
\textbf{Results on Cityscape Panoptic Part Dataset.} In Tab.~\ref{tab:experiments_res_city}, we compare our Panoptic-PartFormer with previous baselines. All the models use the single scale inference without test time augmentation. Our method with ResNe50 backbone achieves 57.4\% PartPQ which outperforms the previous work using complex pipelines~\cite{deeplabv3plus,maskrcnn} with even stronger backbone~\cite{chollet2017xception}. For the same backbone, our method results in 2.3\% PartPQ gain over the previous baseline. 
For large model comparison, our method with Swin-Transformer achieves 61.9 \%PartPQ. It outperforms previous works that use state-of-the-art individual models~\cite{ocrnet,wang2020deep,Zhao2019BSANet,liang2020polytransform} by 0.5\%. Note that the best model from HRNet~\cite{ocrnet} is pretrained using Mapillary dataset~\cite{neuhold2017mapillary}.
We follow the same pipeline for fair comparison. Both settings prove the effectiveness of our approaches.

\noindent
\textbf{GFlops and Parameter Comparison.} Since our method is one single unified model, our Panoptic-PartFormer has advantages on both GFlops and Parameters. Since the work~\cite{liang2020polytransform} is not public available, we estimate the lower bound by its baseline model~\cite{maskrcnn}.
As shown in Tab.~\ref{tab:cpp_detailed}, our model obtain around 60\% GFlops drop and 70\% parameter drop.

\noindent
\textbf{Results on Pascal Panoptic Part Dataset.} We further compare our method with previous work on the Pascal Panoptic Part dataset in Tab.~\ref{tab:experiments_res_ppp}. For different settings, our methods achieve state-of-the-art results on both PQ and PartPQ with a very significant gain. For ResNet backbone, our methods achieve \textbf{6-7\% gains} on PartPQ. Moreover, our resnet101 model can achieve better results than previous work using \textit{larger} backbone~\cite{qiao2021detectors}.
Using Swin Transformer base as backbone~\cite{liu2021swin}, our method achieves \textbf{47.4\% PartPQ} which shows the generalization ability on large model. 


\noindent
\textbf{Results on Cityscapes Panoptic Segmentation.}
We also compare our method with several previous works on cityscapes panoptic validation set. As shown in Tab.~\ref{tab:experiments_res_cityscapes}, our Panoptic-PartFormer also achieves state-of-the-art results compared with previous works~\cite{li2020panopticFCN,cheng2020panoptic}. This proves the generalization ability of our framework. 

\begin{table}[!t]\setlength{\tabcolsep}{6pt}
	\centering
	\begin{threeparttable}
		\scalebox{0.60}{
			\begin{tabular}{l c c c c}
				\toprule[0.15em]
			Method &  PQ & PartPQ  & Param(M) & GFlops \\
				\toprule[0.15em]
		UPSNet + DeepLabv3+ (ResNet50) & 59.1 & 55.1 & 87 &  890 \\ \textbf{Panoptic-PartFormer (ResNet50)} & \textbf{61.6} & \textbf{57.4}  & \textbf{37.35} & \textbf{185.84}  \\ 
		\hline
HRNet(OCR) +PolyTransform + BSANet & 66.2 & 61.4 & 181 & 1154 \\
        \textbf{Panoptic-PartFormer (Swin-base)} & \textbf{66.6}  & \textbf{61.9}  & \textbf{100.32} & \textbf{408.52} \\
	\bottomrule[0.1em]
	\end{tabular}}
		\caption{\small More detailed comparison on Cityscapes PPS dataset. GFlops are measured with 1200  800 input.}
		\label{tab:cpp_detailed}
	\end{threeparttable}
\end{table}



\begin{table}[t]
\centering
\begin{adjustbox}{width=0.50\textwidth}
\begin{tabular}{ll||cc}
\toprule[0.15em]
\textbf{Panoptic seg. method}  &  \textbf{Part seg. method}  & PQ & PartPQ \\ 
\midrule
\textit{\textbf{Pascal Panoptic Parts} validation set} \\
DeepLabv3+ \& Mask R-CNN \cite{maskrcnn}(ResNet50) & DeepLabv3+ \cite{deeplabv3plus}(ResNet50) & 35.0  &  31.4   \\ 
DLv3-ResNeSt269~\cite{zhang2020resnest} \& DetectoRS \cite{qiao2021detectors} &  BSANet \cite{Zhao2019BSANet}  & 42.0  & 38.3  \\ \hline
\textbf{Our Unified Approach} \\
\textbf{Panoptic PartFormer (ResNet50)} &  & \textbf{47.6} & \textbf{37.8} \\
\textbf{Panoptic PartFormer (ResNet101)} & & \textbf{49.2} & \textbf{39.3} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{\textbf{Experiment Results on PPP dataset.} All the methods use single scale inference.}
\label{tab:experiments_res_ppp}
\end{table}

\begin{table*}[!t]
    \footnotesize
	\centering
    \scalebox{0.70}{\subfloat[Effect of each component. DD: Decoupled Decoder. DC: Dynamic Convolution.
    SA: Self Attention. I: Interaction number.
    ]{
        \small
        \label{tab:effect_component}
	    \begin{tabularx}{0.40\textwidth}{c c c c c c c} 
		        				\toprule[0.15em]
    		DD & DC  & SA & I=1 & I=3 & PQ & PartPQ  \\
            \midrule[0.15em]
    		\checkmark & \checkmark & \checkmark &  - & \checkmark & 61.6 & 57.4 \\
    		- & \checkmark & \checkmark & - & \checkmark & 61.2 & 55.9 \\ 
    		\checkmark & - & \checkmark & - & \checkmark & 57.0 & 52.2 \\ 
    		\checkmark & \checkmark & - &- & \checkmark & 57.3 & 53.4 \\ 
    		\checkmark & \checkmark & \checkmark &\checkmark & - & 58.3 & 54.2 \\ 
        	\bottomrule[0.1em]
	    \end{tabularx}
    }} \hfill
    \scalebox{0.70}{
    \subfloat[Ablation on Query Reasoning Design]{
        \label{tab:query_reasoning}
		\begin{tabularx}{0.40\textwidth}{c c c} 
			\toprule[0.15em]
		  Setting & PQ & PartPQ \\
			\midrule[0.15em]
		  Joint Reasoning & 61.6 & 57.4 \\
          Separate Reasoning & 61.1 & 56.8 \\
          Sequential Reasoning & 60.8 & 56.3 \\
			\bottomrule[0.1em]
		\end{tabularx}
    }} \hfill
    \scalebox{0.70}{
    \subfloat[Dense Prediction or Query Prediction on Part. DP: Dense Prediction.
    w: with. ASPP: Atrous Spatial Pyramid Pooling~\cite{deeplabv3}.    
    ]{
        \label{tab:dp_vs_jq}
		\begin{tabularx}{0.45\textwidth}{c c c} 
			\toprule[0.20em]
			Method & PQ & PartPQ \\
			\midrule[0.15em]
			Joint Query  & 61.6 & 57.4 \\ 
            DP-Based & 59.8 & 55.9 \\ 
            DP-Based w ASPP~\cite{deeplabv3} & 59.9 & 56.1 \\ 
			\bottomrule[0.1em]
		\end{tabularx}
    }} \hfill
    \scalebox{0.65}{
    \subfloat[Effect of Aligned Decoder Design.]{
        \label{tab:aligned_decoder}
	    \begin{tabularx}{0.80\textwidth}{c c c c c} 
		        				\toprule[0.15em]
    		 Settings  & PQ & PartPQ & P & NP \\
    		\toprule[0.15em]
    		w Aligned Part Decoder & 61.6 & 57.4 & 43.9 & 62.2\\
    		w/o Aligned Part Decoder & 61.4 & 56.3 & 41.2 & 62.1  \\
    		Aligned Part Decoder on Both Features &  61.4 & 57.2 & 43.7 & 62.0 \\
        	\bottomrule[0.1em]
	    \end{tabularx}
    }} \hfill
    \scalebox{0.70}{
    \subfloat[Upper bound Analysis. GT: Ground Truth]{
        \label{tab:upper_bound}
	    \begin{tabularx}{0.55\textwidth}{c c c c c} 
		  \toprule[0.15em]
    	    Setting & Panoptic-GT  & Part-GT  & PartPQ & P \\
    		\toprule[0.15em]
    	       baseline & - & - & 57.4 & 43.9 \\
    	        & - & \checkmark & 61.6 & 56.1 \\
    	        & \checkmark &  - & 88.4 & 56.4 \\
        	\bottomrule[0.1em]
	    \end{tabularx}
    }} \hfill
	\caption{Ablation studies and analysis on Cityscapes Panoptic Part validation set with ResNet50 as backbone. Best view it in color.}
\end{table*}



\begin{table}[t]
\centering
\begin{adjustbox}{width=0.42\textwidth}
\begin{tabular}{c c c c c}
\toprule[0.15em]
\textbf{Method}  &  Backbone  &  &  &   \\ 
\midrule[0.15em]
Panoptic FPN~\cite{kirillov2019panopticfpn} & ResNet101 & 58.1 & 52.0 & 62.5 \\
  UPSNet~\cite{xiong2019upsnet} & ResNet50 & 59.3 & 54.6 & 62.7 \\
  SOGNet~\cite{yang2019sognet} & ResNet50 & 60.0 & { 56.7} & 62.5 \\
  Seamless~\cite{porzi2019seamless} & ResNet50 & 60.2 & 55.6 & 63.6 \\
  Unifying~\cite{li2020unifying} & ResNet50 & 61.4 & 54.7 & 66.3 \\
  Panoptic-DeepLab~\cite{cheng2020panoptic} & ResNet50 & 59.7 & - & - \\
  Panoptic FCN~\cite{li2020panopticFCN} & ResNet50 & {\ 61.4} & 54.8 & { 66.6} \\
  Panoptic FCN++~\cite{li2021fully} & Swin-large & 64.1 & 55.6 & 70.2 \\
  \hline
Panoptic-PartFormer & ResNet50 & 61.6 & 54.9 & 66.8 \\
Panoptic-PartFormer & Swin-base & 66.6 & 61.7 & 70.3 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{ \small \textbf{Experiment Results on Cityscapes Panoptic validation set.} indicates using DCN~\cite{deformablev2}.}
\label{tab:experiments_res_cityscapes}
\end{table}





\subsection{Ablation Study and Model Design}
\label{sec:ablation}
In this section, we present ablation study and several model designs of our Panoptic-PartFormer. 

\noindent
\textbf{Effectiveness of each component.} As shown in Tab.~\ref{tab:effect_component}, we start with the effectiveness of each component of our framework by removing it from the original design. Removing Decoupled Decoder (DD) results in a 1.4 \% drop on PartPQ. Removing Dynamic Convolution (DC) or Self Attention (SA) results in a large drop on PQ which means both are important for the interaction between queries and corresponding query features. Decreasing stage number to 1 also leads to a significant drop. Performing more interaction results in more accurate feature location for each query, which is the same as previous works~\cite{zhang2021knet,peize2020sparse}.


\noindent
\textbf{Whether the part query depends more on thing query?} With our framework, we can easily analyze the relationship among stuff query, thing query, and part query. We present several ways of reasoning and fusing different queries. From intuitive thought, part information is more related to thing query. We design two different query interaction methods, shown in Tab~\ref{tab:query_reasoning}. For separate reasoning, we adopt DD and SA on two query pairs, including stuff-thing query and thing-part query. For sequential reasoning, we perform DD and SA with thing-part query first and stuff-thing query second. However, we find the best model is the joint reasoning, which is the default setting described in method part. We argue that better part segmentation needs the whole scene context rather than thing features only. 

\begin{table*}[!t]
    \footnotesize
	\centering
    \scalebox{0.70}{\subfloat[Effect on Position Encoding.
    ]{
        \small
        \label{tab:pos_enc}
	    \begin{tabularx}{0.45\textwidth}{c c c} 
		 	\toprule[0.2em]
				Method & PQ & PartPQ \\
				\midrule[0.15em]
				baseline &  61.6  & 57.4 \\
				w/o positional encoding & 59.0  & 55.1  \\
				\bottomrule[0.1em]
	    \end{tabularx}
    }} \hfill
    \scalebox{0.70}{
    \subfloat[Effect of adding boundary supervision]{
        \label{tab:boundary_loss}
		\begin{tabularx}{0.40\textwidth}{c c c} 
		\toprule[0.2em]
				Method &  PQ  & PartPQ  \\  
				\midrule[0.15em]
				baseline &  61.6  & 57.4 \\
			    + boundary loss & 61.5  & 57.2  \\
			\bottomrule[0.1em]
		\end{tabularx}
    }} \hfill
    \scalebox{0.70}{
    \subfloat[Effect on adding part annotations.]{
        \label{tab:part_anno}
		\begin{tabularx}{0.45\textwidth}{c c c} 
		\toprule[0.2em]
				Method &  PQ  & PartPQ  \\  
				\midrule[0.15em]
				things and stuff only  &  61.2  & - \\
				+ part annotation (ours) & 61.6  & 57.4  \\
			    \bottomrule[0.1em]
		\end{tabularx}
    }} \hfill
     \caption{\small More analysis using our Panoptic-PartFormer.}
\end{table*}


\noindent
\textbf{Choose joint query modeling or separate modeling on part?} Following PanopticFPN settings~\cite{kirillov2019panopticfpn}, we also adopt semantic-FPN like model for part segmentation. Dense Prediction (DP) is the baseline method shown in Fig.~\ref{fig:teaser_01}(b). We adopt the same merging process for panoptic segmentation and part segmentation. As shown in Tab.~\ref{tab:dp_vs_jq}, our joint query based method achieves the better results and outperforms previous dense prediction based approach and its improved version~\cite{deeplabv3}. This indicates that joint learning benefits part segmentation a lot, which proves the effectiveness of our framework.  

\noindent
\textbf{Aligned decoder is more important for part segmentation.} As shown in Tab.~\ref{tab:aligned_decoder}, using aligned part decoder results in better PartPQ especially for the things with Part (P). Adding both paths with aligned decoder does not bring extra gain. This verifies our motivation: part segmentation needs more detailed information, while thing and stuff predictions do not need it.


\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.75\linewidth]{fig/pano_part_exp_vis.pdf}
	\caption{\small Visualization of our Panoptic-PartFormer. Top: results on Cityscapes PPS validation set. Bottom left: prediction on BDD dataset~\cite{yu2020bdd100k}. Bottom right: prediction on Mapillary dataset~\cite{neuhold2017mapillary}. Best view it on screen. }
	\label{fig:exp_vis}
\end{figure*}



\noindent
\textbf{Necessity of Positional Encoding on  and .} 
In Tab.~\ref{tab:pos_enc}, removing positional encoding leads to inferior results on both PQ and PartPQ which indicates the importance of position information~\cite{wang2020solov2,zhang2021knet,detr}.

\noindent
\textbf{Will boundary supervision help for part segmentation?} In Tab.~\ref{tab:boundary_loss}, we also add boundary supervision for part segmentation where we use the dice loss~\cite{milletari2016v} and binary cross entropy loss. However, we find no gains on this since our mask is generated from aligned decoder since it already contains detailed information.


\noindent
\textbf{Will joint training help for panoptic segmentation?} As shown in Tab.~\ref{tab:part_anno}, joint learning benefits the panoptic segmentation baseline. However, the benefit is \textit{limited} since both thing and stuff prediction does not need much detailed information. 

\subsection{Analysis and Visualization}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.75\linewidth]{fig/pano_part_results_pascal_contex_pss.pdf}
	\caption{\small More visualization results on Pascal Context Panoptic Part validation set. Best viewed in color and by zooming in. Note that stuff classes have the same color, while thing classes are not.}
	\label{fig:vis_results_pascal_pss}
\end{figure*}
\noindent
\textbf{Visualization and Generalization.} We give several visualization examples using our model on Cityscapes PPS validation set. Moreover, we also visualize several examples on the Mapillary dataset~\cite{neuhold2017mapillary} and BDD dataset~\cite{yu2020bdd100k} to show the generalization ability of our method. As shown in the first row of Fig.~\ref{fig:exp_vis}, our method achieves considerable results. Moreover, on the Mapillary~\cite{neuhold2017mapillary} and BDD datasets~\cite{yu2020bdd100k} which do not have part annotations, our method can still work well as shown in the last row of the Fig.~\ref{fig:exp_vis}. Moreover, we also visualize the results on PPP datasets. The first two rows show the crowded human scene and outdoor scene. Both cases show that our model can obtain the convincing results. The last row shows the small objects cases. The failure cases are due to tiny objects including their parts. 

\noindent
\textbf{Upper bound analysis of Our Model.} In Tab.~\ref{tab:upper_bound}, we give the upper bound analysis to our model by replacing the panoptic segmentation Ground Truth or part segmentation Ground Truth into our prediction. Replacing panoptic segmentation GT leads to a huge gain on PartPQ while replacing part segmentation only results in a limited gain. That indicates PartPQ is more \textit{sensitive to panoptic segmentationt than part segmentation on CPP dataset}. We conclude that a stronger panoptic segmentation model maybe the key for better PPS results. 



%
