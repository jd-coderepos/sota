\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_2020}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{graphicx}



\usepackage{xcolor}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{soul}
\usepackage{caption}
\usepackage{bm}
\captionsetup{font=small}\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=cyan,
}



\title{Cross-directional Feature Fusion Network for Building Damage Assessment from Satellite Imagery}



\author{Yu Shen,
Sijie Zhu, Taojiannan Yang, Chen Chen \\
 University of North Carolina at Charlotte, Nanjing University of Science and Technology\\
  \texttt{shenyu@njust.edu.cn}, \texttt{\{szhu3,tyang30,chen.chen\}@uncc.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Fast and effective responses are required when a natural disaster (e.g., earthquake, hurricane, etc.) strikes. Building damage assessment from satellite imagery is critical before an effective response is conducted. High-resolution satellite images provide rich information with pre- and post-disaster scenes for analysis. However, most existing works simply use pre- and post-disaster images as input without considering their correlations. In this paper, we propose a novel cross-directional fusion strategy to better explore the correlations between pre- and post-disaster images. Moreover, the data augmentation method CutMix is exploited to tackle the challenge of hard classes. The proposed method achieves state-of-the-art performance on a large-scale building damage assessment dataset -- xBD.


\end{abstract}

\section{Introduction}
\label{secIntro}
Natural disasters, such as earthquakes, floods and tsunami, cause serious social and economic devastation. When a natural disaster strikes, accurate and immediate responses are required in Humanitarian Assistance and Disaster Response (HADR) for saving thousands of lives \cite{sidrane2019machine, doshi2019firenet}. Before these responses, rescue planning and preparations are conducted based on the damage analysis \cite{weber2020detecting}. With the rapid development of remote sensing technology, high resolution satellite images are now available for damage analysis. Traditionally, these images of disaster areas are analyzed by experts, which might be time-consuming if the areas are large. Therefore, automatic information extraction from satellite images, such as building segmentation and damage assessment, is imperative under time-critical situations. 

Building damage assessment plays a pivotal role in HADR, which aims at predicting the building damage level for each pixel based on building segmentation. With a pair of pre- and post-disaster images, the extent of the damage to buildings can be learned by machine learning algorithms. Recently, deep learning-based methods have shown their effectiveness in building damage assessment. Xu et al. \cite{xu2019building} investigated the capability of convolutional neural networks (CNN) for building damage detection by identifying damaged and undamaged buildings. To evaluate the damage levels more precisely, Weber et al. \cite{weber2020building} considered building damage assessment as a semantic segmentation task. 


With a pair of pre- and post-disaster images for building damage assessment, a key question would be how to effectively model the correlations between these images? Unfortunately, there are only a few works have explored this direction.  
Hao et al. \cite{hao2020attention} simply concatenated the features from pre- and post-disaster images and fed them into non-local attention modules. Gupta et al. \cite{gupta2020rescuenet} developed a framework that uses the difference of pre- and post-disaster features as input of a network.

Another challenge of building damage assessment from satellite imagery lies in the visual similarity between certain classes (e.g., \textit{no damage} and \textit{minor damage}). These classes are considered as hard classes. To better explain this problem, we use the xBD \cite{gupta2019xbd} dataset, which is the largest dataset for building damage assessment to date, as an example. Fig. \ref{fig:figImage} shows a pair of images from this dataset. 
\begin{wrapfigure}{r}{0.35\textwidth}
    \vspace{-0.2cm}
    \includegraphics[width=\linewidth]{Fig-1.png} 
\caption{A pair of images and their annotations from the xBD dataset.}
\label{fig:figImage}
\end{wrapfigure}
Based on the visual observation, it is difficult to distinguish between classes such as \textit{no damage} and \textit{minor damage} due to high visual similarities. To further verify this observation, Table \ref{tabConfusion} reports the classification results of baseline method (ResNet-50) on xBD. From the classification confusion matrix, about 24.2\% of \emph{minor damage} are mis-classified as \emph{no damage}. \begin{wraptable}{r}{0.4\textwidth}
\vspace{-\intextsep}
  \caption{Classification confusion matrix (\%)  of ResNet-50 baseline on xBD test set.}
  \vspace{-\intextsep}
  \scriptsize
  \begin{center}
  \setlength\tabcolsep{3.0pt} 
  \renewcommand{\arraystretch}{0.6}\begin{tabular}{rccccc}
    \toprule
Damage Level  & C0 & C1 & C2  & C3 & C4 \\
    \midrule
    Background (C0) &\textbf{98.6} &	0.9 &	0.2&	0.2 &	0.1 \\
    No damage (C1) & 7.1&	\textbf{88.7} &	3.2&	0.8&	0.1\\
    Minor Damage (C2) & 6.3 &	\textcolor{blue}{24.2}&	\textbf{60.0} &	9.2&	0.4\\
    Major Damage (C3)  & 3.0 &	6.6	& \textcolor{blue}{14.9}&	\textbf{73.1} &	2.4\\
    Destroyed (C4) & 5.5 &	2.4&	1.2&	8.9&	 \textbf{82.1} \\
    \bottomrule
  \end{tabular}
  \vspace{-\intextsep}
  \end{center}
 \label{tabConfusion}
\end{wraptable}
One effective strategy to cope with hard classes and improve the model performance is data augmentation \cite{shorten2019survey,cubuk2019autoaugment}. Data augmentation has been widely used as a pre-processing technique to artificially increase the size of dataset in segmentation tasks \cite{hernandez2018data, myronenko20183d, zhang2017mixup}. Recently, CutMix \cite{yun2019cutmix} is proposed as a new data augmentation technique, which generates a new image by combining two image samples, to enhance the generalization ability of neural networks. CutMix directly cuts and pastes image patches from one image to another, which can be easily used in segmentation tasks.

Motivated by the above observations, we introduce a two-stage U-Net \cite{ronneberger2015u} based framework that integrates pre- and post-disaster features for building damage assessment. First, a single U-Net is used for building segmentation. Then a two-branch U-Net is applied for damage assessment using the weights from building segmentation for fine tuning. In the network, a cross-directional fusion model is proposed to explore the correlations between features from pre- and post-disaster images. By leveraging channel-wise and spatial-wise correlations, the fused features can be further enhanced. Moreover, to tackle the hard classes problem, CutMix is employed for data augmentation. 
Specifically, we only apply CutMix to hard classes to make the network pay more attention to those classes, thereby learning more robust representations for hard classes. In the experiments, we show that this strategy yields superior classification performance over simply adopting CutMix for all classes. 

In summary, this work makes two key contributions. (1) We present a new framework that integrates pre- and post-disaster images for building damage assessment. The proposed cross-directional fusion model effectively aggregates the feature representations from two images. (2) We unveil the challenge of hard classes in building damage assessment and explore a data augmentation strategy CutMix to address this problem. The proposed framework achieves state-of-the-art performance on a large-scale building damage assessment benchmark -- xBD. 



\begin{figure}
  \centering
    \subfloat{
	\includegraphics[width=0.89\linewidth]{framework2.png} 
	} \\
  \caption{(a) Overall framework of the proposed method. In the building segmentation stage, only pre-disaster images and the upper U-Net branch are used. In the damage assessment stage, pre- and post disaster images are fed into the two network branches separately. (b) Architecture of the cross-directional fusion model.}
  \label{figFramework}
\end{figure}

\section{Proposed Method}
\label{secMethod}
\textbf{Overview.} 
As shown in Fig. \ref{figFramework}(a), the whole framework consists of two stages: building segmentation (stage 1) and damage assessment (stage 2). In stage 1, a single U-Net branch (i.e., the upper one) is used for building segmentation. This U-Net branch uses only pre-disaster images as input and produces segmentation masks of building objects. In stage 2, the pre- and post-disaster images are fed into the two network branches separately.
The weights are shared in the two-branch U-Net to reduce the computational cost. The network weights from stage 1 are used as initialization for network fine tuning in stage 2. To further enhance the feature representations, a cross-directional fusion model and CutMix data augmentation are utilized in the proposed framework.

\textbf{Cross-directional fusion model.} To further explore the correlations between pre- and post-disaster features, the proposed cross-directional fusion (CDF) model is added in the framework. Inspired by the squeeze and excitation (SE) block \cite{guha2018recalibrating}, the proposed CDF model focuses on recalibrating features from channel and spatial dimensions. 
Moreover, the channel and spatial information from  pre- and post-disaster features is aggregated together in a cross manner and then embedded in the network respectively. The model details are depicted in Fig. \ref{figFramework}(b). Let  and  be the feature maps obtained from the two branches of U-Net respectively, the channel information can be extracted by

where  denotes the concatenation of feature maps,  represents the global average pooling,   is the sigmoid function and 
is a feature vector of  channels after dimension reduction from . Then the new features from two branches can be formulated as

where  denote the channel-wise multiplication between the input feature maps and vector . The output of channel feature fusion are then used in the spatial feature fusion. We concatenate  and  and feed them into a  convolution  as follows

where .
Then the output features are 

where  and  denote the spatial-wise multiplication. As a result, channel and spatial information from pre- and post-disaster branches are effectively aggregated in the cross-directional fusion model.
The proposed fusion model consists of only simple convolution and matrix operations, which is easy to implement and integrate with existing CNN architectures. 


\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{CutMix.png} \\
   \includegraphics[width=0.48\linewidth]{legend.png}
  \caption{Data augmentation with CutMix for hard classes.}
  \vspace{-0.1cm}
  \label{figCutmix}
\end{figure}

\textbf{Data augmentation with CutMix.} 
As discussed in Sec. \ref{secIntro}, there are several damage levels that are difficult to distinguish from each other due to object visual similarities in satellite images. To address this challenge, we leverage the CutMix data augmentation scheme to increase the sample sizes of hard classes, hoping to build better feature representations for these classes.  
Specifically, image patches are cut from samples that contain hard classes (i.e., \textit{minor} and \textit{major} damage classes for the xBD dataset based on the results in Table \ref{tabConfusion}), and then pasted into any random sample images. The CutMix procedure is illustrated in Fig. \ref{figCutmix}. Let  and  be a randomly selected training sample (an image pair) and label, () be a randomly selected sample from hard classes, where  is the channel number of images. Then the CutMix operation in this task can be defined as

where  denotes a binary mask indicating where to cut out and fill in from two image samples,  is an element-wise multiplication, and  represents the generated new sample. With the increased sample sizes of hard classes using CutMix, the network is forced to pay more attention to these classes and learn more robust representations for them.

\section{Experiments}
\begin{wraptable}{r}{0.3\textwidth}
\vspace{-\intextsep}
\footnotesize
  \caption{xBD dataset splits and annotation numbers.}
  \vspace{-0.3cm}
  \label{tabDataset}
  \begin{center}
  \setlength\tabcolsep{2.0pt} 
  \renewcommand{\arraystretch}{0.8}\begin{tabular}{ccc}
    \toprule
    {Split}     & {Image No.}     & {Polygons No.} \\
    \midrule
    Train & 18336  &  632228     \\
    Test    & 1866  & 109724     \\
    \bottomrule
  \end{tabular}
  \vspace{-0.3cm}
  \end{center}
\end{wraptable}
\textbf{Dataset description.} The xBD dataset is a large-scale public dataset of satellite images for building segmentation and damage assessment \cite{gupta2019xbd}. It covers a variety of disasters (such as hurricanes, floods, wildfire and earthquakes) and locations with more than 800,000 building annotations across the world. The dataset consists of image pairs (pre- and post-disaster) with a size of  pixels. The damage assessment contains 4 levels, including no damage, minor damage, major damage and destroyed. The training and testing sets are listed in Table \ref{tabDataset}. It is worth mentioning that the data is imbalanced and the damage level is highly skewed toward ``no damage''. The number of each damage level's polygons is reported in Table \ref{tabDamageLevel}.
\begin{wraptable}{r}{0.4\textwidth}
\vspace{-\intextsep}
\footnotesize
  \caption{Damage level annotations.}
  \vspace{-0.3cm}
  \begin{center}
  \setlength\tabcolsep{2.0pt} 
  \renewcommand{\arraystretch}{0.7}\begin{tabular}{ccccc}
    \toprule
    \textbf{}     & No damage    & Minor & Major & Destroyed \\
    \midrule
   \textbf{No.} & 313003  &  36860    & 29904 & 31560 \\
    \bottomrule
  \end{tabular}
  \vspace{-0.3cm}
  \end{center}
    \label{tabDamageLevel}
\end{wraptable}

All experiments are evaluated on xBD dataset with metric  for building segmentation, which is defined as:

where ,  and  are the number of true-positive, false-positive and false-negative pixels of segmentation results. The  metric for damage assessment is in a similar manner with . The overall score  of building segmentation and damage assessment is defined as:


\textbf{Implementation details.}
We use Pytorch framework to build the networks. All experiments are conducted on a machine with an Intel i9-9920X CPU and two NVIDIA TITAN-V GPUs. All images are cropped to a size of  pixels for training. Apart from CutMix, basic data augmentation is used, such as flip and rotation. The cross-entropy loss is used for both building segmentation and damage assessment. The optimization method is Adam. In the building segmentation stage, the learning rate is 0.00015 and the number of epoch is 120. In the building damage assessment stage, the learning rate is 0.0002 and the number of epoch is 20.

\textbf{Results analysis.}
To validate the effectiveness of the proposed framework, we employ state-of-the-art methods for comparison on xBD. All the models adopt Res50 as backbone. A Res50 baseline without cross-directional fusion and CutMix is also used for comparison. As shown in Table \ref{tabResults}, the proposed framework using the vanilla U-Net structure alone (i.e., Res50 (baseline)) is able to outperform the existing methods in terms of all three metrics. With the proposed cross-directional fusion model and CutMix data augmentation, it further improves the accuracy for damage assessment (0.778 vs. 0.757 in ), and the improvement is consistent for all the damage levels. 
Several visual examples of building segmentation and damage assessment results of our method are presented in the \textbf{Appendix}. 
\begin{table}
  \caption{Building segmentation and damage assessment performance comparison on xBD dataset.}
  \footnotesize
  \centering
  \begin{tabular}{cccccccc}
    \toprule
         &  (overall) &  &   & No damage    & Minor & Major & Destroyed \\
    \midrule
    Gupta et al. \cite{gupta2020rescuenet} & 0.741 & 0.835 &	0.697 &	0.906 &	0.493 &	0.722 &	0.837 \\
   Weber et al. \cite{weber2020building}  & 0.770 &	0.840 &	0.740 &	0.885 &	0.563 &	0.771 &	0.808   \\
   Res50 (baseline) & 0.789 & 0.864 &	0.757 &	0.923 &	0.578 &	0.760 &	0.869  \\
   Res50 (ours) &  \textbf{0.804} &	0.864 &	0.778 &	0.927 &	0.610 &	0.781 &	0.873   \\
    \bottomrule
  \end{tabular}
    \label{tabResults}
\end{table}



\begin{wraptable}{r}{0.37\textwidth}
\vspace{-\intextsep}
\footnotesize
  \caption{Parameters (M) and FLOPs (G) of our method and baseline.}
  \vspace{-0.3cm}
  \begin{center}
  \setlength\tabcolsep{2.0pt} 
  \renewcommand{\arraystretch}{0.4}\begin{tabular}{cccc}
    \toprule
    \textbf{Method}     & Params    & FLOPs  \\
    \midrule
     Res50 (baseline)   & 32.5  & 92.9  \\
     Ours    & 33.1  &  107.6     \\
    \bottomrule
  \end{tabular}
  \vspace{-0.3cm}
  \end{center}
\label{tabParam}
\end{wraptable}

We also compare the parameter size and computational cost (FLOP) between the proposed method and the baseline. As shown in Table \ref{tabParam}, although the cross-directional fusion model is added in the network, it brings less than 1M additional parameters. And this results in only slight increase in computational cost (107.6 vs. 92.9 GFLOPs). 
\begin{wraptable}{r}{0.7\textwidth}
\vspace{-\intextsep}
\footnotesize
  \caption{Ablation study on cross-directional fusion and CutMix.}
  \vspace{-0.3cm}
  \begin{center}
  \setlength\tabcolsep{2.0pt} 
  \renewcommand{\arraystretch}{0.7}\begin{tabular}{cccccccc}
    \toprule
    \textbf{Method}     &     &  &  & No damage & Minor & Major & Destroyed \\
    \midrule
     Res50 (baseline) & 0.789 & 0.864 &	0.757 &	0.923 &	0.578 &	0.760 &	0.869 \\
     Ours w/o CutMix   & 0.795  &  0.864    & 0.765  & 0.923 & 0.592 & 0.769 &0.871\\
     Ours w/o Fusion   & 0.802  &  0.864    & 0.775  & 0.926 & 0.605 & 0.779 & 0.872 \\
    \bottomrule
  \end{tabular}
  \vspace{-0.3cm}
  \end{center}
\label{tabAbfusion}
\end{wraptable}

\textbf{Ablation study.}
To investigate the contribution of cross-directional fusion model and CutMix data augmentation, we perform ablation studies on these two components. From Table \ref{tabAbfusion}, we can observe that the cross-directional fusion model brings 0.6\% (0.789 vs. 0.795 in ) improvement compared with the baseline. Moreover, the CutMix strategy on hard classes boosts the overall performance () by 1.3\%, which is a considerable gain. In particular, the accuracy of \textit{minor damage} class is improved by 2.7\% (0.578 vs. 0.605) with CutMix. 

\begin{wraptable}{r}{0.5\textwidth}
\vspace{-\intextsep}
  \caption{Ablation study of CutMix on different classes.}
  \vspace{-0.2cm}
  \footnotesize
  \centering
  \setlength\tabcolsep{2.0pt} 
  \renewcommand{\arraystretch}{0.7}\begin{tabular}{cccccc}
    \toprule 
         Backbone & No damage & Minor & Major & Destroyed  &  \\
    \midrule
    Res50 & & & & & 0.789  \\
    Res50 & \checkmark  & \checkmark &  \checkmark  &   \checkmark  & 0.790\\
    Res50 &  & \checkmark &  \checkmark  &   \checkmark  & 0.797\\
    Res50 &   & \checkmark &  \checkmark  &     & \textbf{0.802}\\
    Res50 &   & \checkmark &    &     & 0.795\\
    \bottomrule
  \end{tabular}
 \label{tabAbcut}
\end{wraptable}
For the xBD dataset, \textit{minor damage} and \textit{major damage} are the most difficult classes based on the results in Table \ref{tabAbfusion}. Therefore, in our proposed method, CutMix data augmentation is mainly focused on these two damage levels to generate more training samples for them. We further conduct experiments to investigate the influence of CutMix on different damage levels. Table \ref{tabAbcut} reports the overall scores when CutMix is applied on different damage levels. We can clearly observe that there is barely any improvement when CutMix is used on the entire dataset (i.e., considering all classes). In contrast, a significant improvement is achieved when only \textit{minor} and \textit{major} damage levels are considered in the data augmentation. This hard-class biased data augmentation strategy facilitates the network to learn better representations for those classes. 
\vspace{-0.2cm}
 

\section{Conclusion}
In this work, we propose a two-stage U-Net framework that integrates cross-directional fusion model for building damage assessment. In the fusion model, channel and spatial information from pre- and post-disaster image features is aggregated and embedded in the network, which enables the network to learn feature representations more effectively. Moreover, a data augmentation strategy CutMix is used to mitigate the challenge of hard classes. Experimental results show that significant improvements can be achieved when CutMix is applied on hard damage levels. The proposed method also yields state-of-the-art building damage assessment performance on the xBD dataset. 








\bibliographystyle{unsrt}
\bibliography{neurips}











\clearpage
\appendix
\section{Appendix}


\subsection{Visual Examples}
Figures \ref{figExample1}-\ref{figExample4} provide visual comparisons of the building damage assessment results of our proposed method and the baseline for different disasters. We can observe that our proposed framework achieves better performance in damage level assessment. For example, in Figure \ref{figExample1}, some regions of \textit{major damage} (orange color) and \textit{no damage} (green color) are mis-classified as \textit{destroyed} (red color) and \textit{minor damage} (yellow color) respectively by the baseline, while our proposed method yields much less mis-classification and produces a segmentation map that is very close to the ground-truth.

\begin{figure}[h]
\centering
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/2rgb_pre.png}
	} 
	\subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/2loc_gt.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/2loc_base.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/2loc_ours.jpg}
    } \\
    \vspace{-0.2cm}
    \subfloat[pre and post-disaster]{
		\includegraphics[width=0.22\linewidth]{VisEx/2rgb_post.png}
	} 
	\subfloat[ground-truth]{
		\includegraphics[width=0.22\linewidth]{VisEx/2dmg_gt.jpg}
    }  
    \subfloat[baseline]{
		\includegraphics[width=0.22\linewidth]{VisEx/2dmg_base.jpg}
    }  
    \subfloat[ours]{
		\includegraphics[width=0.22\linewidth]{VisEx/2dmg_ours.jpg}
    }  \\
    \vspace{-0.2cm}
    \subfloat{
		\includegraphics[width=0.65\linewidth]{legend.png}
    } 
	\caption{A visual comparison of results of the baseline and the proposed method over a hurricane disaster image. }
\label{figExample1}
\end{figure}

\begin{figure}[h]
\centering
	\subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/1rgb_pre.png}
	} 
	\subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/1loc_gt.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/1loc_base.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/1loc_ours.jpg}
    } \\ 
    \vspace{-0.2cm}
    \subfloat[pre and post-disaster]{
		\includegraphics[width=0.22\linewidth]{VisEx/1rgb_post.png}
	} 
	\subfloat[ground-truth]{
		\includegraphics[width=0.22\linewidth]{VisEx/1dmg_gt.jpg}
    }  
    \subfloat[baseline]{
		\includegraphics[width=0.22\linewidth]{VisEx/1dmg_base.jpg}
    }  
    \subfloat[ours]{
		\includegraphics[width=0.22\linewidth]{VisEx/1dmg_ours.jpg}
    }  \\
    \vspace{-0.2cm}
    \subfloat{
		\includegraphics[width=0.65\linewidth]{legend.png}
    } 
	\caption{A visual comparison of results of the baseline and the proposed method over a hurricane disaster image.}
\label{figExample2}
\end{figure}

\begin{figure}
\centering
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/3rgb_pre.png}
	} 
	\subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/3loc_gt.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/3loc_base.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/3loc_ours.jpg}
    } \\ 
    \vspace{-0.2cm}
    \subfloat[pre and post-disaster]{
		\includegraphics[width=0.22\linewidth]{VisEx/3rgb_post.png}
	} 
	\subfloat[ground-truth]{
		\includegraphics[width=0.22\linewidth]{VisEx/3dmg_gt.jpg}
    }  
    \subfloat[baseline]{
		\includegraphics[width=0.22\linewidth]{VisEx/3dmg_base.jpg}
    }  
    \subfloat[ours]{
		\includegraphics[width=0.22\linewidth]{VisEx/3dmg_ours.jpg}
    }  \\
    \vspace{-0.2cm}
    \subfloat{
		\includegraphics[width=0.65\linewidth]{legend.png}
    } 
	\caption{A visual comparison of results of the baseline and the proposed method over a tsunami disaster image. }
\label{figExample3}
\end{figure}


\begin{figure}
\centering
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/4rgb_pre.png}
	} 
	\subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/4loc_gt.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/4loc_base.jpg}
    }  
    \subfloat{
		\includegraphics[width=0.22\linewidth]{VisEx/4loc_ours.jpg}
    } \\ 
    \vspace{-0.2cm}
    \subfloat[pre and post-disaster]{
		\includegraphics[width=0.22\linewidth]{VisEx/4rgb_post.png}
	} 
	\subfloat[ground-truth]{
		\includegraphics[width=0.22\linewidth]{VisEx/4dmg_gt.jpg}
    }  
    \subfloat[baseline]{
		\includegraphics[width=0.22\linewidth]{VisEx/4dmg_base.jpg}
    }  
    \subfloat[ours]{
		\includegraphics[width=0.22\linewidth]{VisEx/4dmg_ours.jpg}
    }  \\
    \vspace{-0.2cm}
    \subfloat{
		\includegraphics[width=0.65\linewidth]{legend.png}
    } 
	\caption{A visual comparison of results of the baseline and the proposed method over a wildfire disaster image. }
\label{figExample4}
\end{figure}




\end{document}
