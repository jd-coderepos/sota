\documentclass{article}
\usepackage{iclr2016_conference,times}

\usepackage[page,header]{appendix}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{mathtools}
\usepackage{bm}\usepackage{relsize}
\usepackage{natbib}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl} \usepackage{bigdelim}

\usepackage{rotating}
\usepackage{color, colortbl}

\definecolor{mColor1}{rgb}{0.95,0.95,0.95}


\usepackage{hyperref}

\usepackage{array}



\newcommand{\theHalgorithm}{\arabic{algorithm}}



\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}


\newcommand\Ba{\bm{a}}
\newcommand\Bb{\bm{b}}
\newcommand\Bc{\bm{c}}
\newcommand\Bd{\bm{d}}
\newcommand\Be{\bm{e}}
\newcommand\Bf{\bm{f}}
\newcommand\Bg{\bm{g}}
\newcommand\Bh{\bm{h}}
\newcommand\Bi{\bm{i}}
\newcommand\Bj{\bm{j}}
\newcommand\Bk{\bm{k}}
\newcommand\Bl{\bm{l}}
\newcommand\Bm{\bm{m}}
\newcommand\Bn{\bm{n}}
\newcommand\Bo{\bm{o}}
\newcommand\Bp{\bm{p}}
\newcommand\Bq{\bm{q}}
\newcommand\Br{\bm{r}}
\newcommand\Bs{\bm{s}}
\newcommand\Bt{\bm{t}}
\newcommand\Bu{\bm{u}}
\newcommand\Bv{\bm{v}}
\newcommand\Bw{\bm{w}}
\newcommand\Bx{\bm{x}}
\newcommand\By{\bm{y}}
\newcommand\Bz{\bm{z}}
\newcommand\BA{\bm{A}}
\newcommand\BB{\bm{B}}
\newcommand\BC{\bm{C}}
\newcommand\BD{\bm{D}}
\newcommand\BE{\bm{E}}
\newcommand\BF{\bm{F}}
\newcommand\BG{\bm{G}}
\newcommand\BH{\bm{H}}
\newcommand\BI{\bm{I}}
\newcommand\BJ{\bm{J}}
\newcommand\BK{\bm{K}}
\newcommand\BL{\bm{L}}
\newcommand\BM{\bm{M}}
\newcommand\BN{\bm{N}}
\newcommand\BO{\bm{O}}
\newcommand\BP{\bm{P}}
\newcommand\BQ{\bm{Q}}
\newcommand\BR{\bm{R}}
\newcommand\BS{\bm{S}}
\newcommand\BT{\bm{T}}
\newcommand\BU{\bm{U}}
\newcommand\BV{\bm{V}}
\newcommand\BW{\bm{W}}
\newcommand\BX{\bm{X}}
\newcommand\BY{\bm{Y}}
\newcommand\BZ{\bm{Z}}
\newcommand\Bal{\bm{\alpha}}
\newcommand\Bbe{\bm{\beta}}
\newcommand\Bla{\bm{\lambda}}
\newcommand\Bep{\bm{\epsilon}}
\newcommand\Bga{\bm{\gamma}}
\newcommand\Bmu{\bm{\mu}}
\newcommand\Bnu{\bm{\nu}}
\newcommand\Brh{\bm{\rho}}
\newcommand\Bth{\bm{\theta}}
\newcommand\Bxi{\bm{\xi}}
\newcommand\Bka{\bm{\kappa}}
\newcommand\Bsi{\bm{\sigma}}
\newcommand\Bta{\bm{\tau}}
\newcommand\BLa{\bm{\Lambda}}
\newcommand\BPh{\bm{\Phi}}
\newcommand\BPs{\bm{\Psi}}
\newcommand\BSi{\bm{\Sigma}}
\newcommand\BUp{\bm{\Upsilon}}
\newcommand\BXi{\bm{\Xi}}
\newcommand\BGa{\bm{\Gamma}}
\newcommand\BTh{\bm{\Theta}}
\newcommand\BOn{\bm{1}}
\newcommand\BZe{\bm{0}}
\newcommand\BLOn{\mathlarger{\mathlarger{\bm{1}}}}
\newcommand\BLZe{\mathlarger{\mathlarger{\bm{0}}}}
\newcommand\bbR{\mathbb{R}}
\newcommand\bbN{\mathbb{N}}
\newcommand\bbC{\mathbb{C}}
\newcommand\Xcal{\mathcal{X}}
\newcommand\Fcal{\mathcal{F}}
\newcommand\Ecal{\mathcal{E}}
\newcommand\Ncal{\mathcal{N}}
\newcommand\Bcal{\mathcal{B}}
\newcommand\Lcal{\mathcal{L}}
\newcommand\Scal{\mathcal{S}}
\newcommand\Qcal{\mathcal{Q}}
\newcommand\xs{x}
\newcommand\zs{z}
\newcommand\xsp{\left(x_{\phi}\right)}
\newcommand\xp{\bm{x}_{\phi}}
\newcommand\zp{\bm{z}_{\omega}}
\newcommand\zps{\left(z_{\omega}\right)}
\newcommand\sign{\mathop{\mathrm{sign}\,}}
\newcommand\diag{\mathop{\mathrm{diag}\,}}
\newcommand\abs{\mathop{\mathrm{abs}\,}}
\newcommand\EXP{\mathbf{\mathrm{E}}}
\newcommand\PR{\mathbf{\mathrm{Pr}}}
\newcommand\PP{\mathbf{\mathrm{P}}}
\newcommand\VAR{\mathbf{\mathrm{Var}}}
\newcommand\COV{\mathbf{\mathrm{Cov}}}
\newcommand\Inf{\mathbf{\mathrm{I}}}
\newcommand\Hent{\mathbf{\mathrm{H}}}
\newcommand\TR{\mathbf{\mathrm{Tr}}}
\newcommand\ALs{{\mbox{\boldmath }}}
\newcommand\XIs{{\mbox{\boldmath }}}
\newcommand\sgn{\mathop{\mathrm{sgn}\,}}
\newcommand\oo{\mathrm{old}}
\newcommand\nn{\mathrm{new}}
\newcommand\rank{\mathrm{rank}}


\newcommand{\R}{\textsf{R~}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\texttt{#1}}}

\iclrfinalcopy

\title{Fast and Accurate Deep Network Learning by
  Exponential Linear Units (ELUs)}


\author{Djork-Arn{\'e} Clevert,
Thomas Unterthiner
\&
Sepp Hochreiter\\
Institute of Bioinformatics\\
Johannes Kepler University, Linz, Austria\\
\texttt{\{okko,unterthiner,hochreit\}@bioinf.jku.at}
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}



\begin{document}

\maketitle


\begin{abstract}
We introduce the ``exponential linear unit'' (ELU)
which speeds up learning in deep neural networks and
leads to higher classification accuracies.
Like rectified linear units (ReLUs), leaky ReLUs
(LReLUs) and parametrized ReLUs (PReLUs),
ELUs alleviate the vanishing gradient problem via the identity
for positive values.
However ELUs have improved learning characteristics compared to
the units with other activation functions.
In contrast to ReLUs, ELUs have negative values which allows them
to push mean unit activations closer to zero like
batch normalization but with lower computational complexity.
Mean shifts toward zero speed up learning by bringing
the normal gradient closer to the unit natural gradient
because of a reduced bias shift effect.
While LReLUs and PReLUs have negative values, too, they
do not ensure a noise-robust deactivation state.
ELUs saturate to a negative value with smaller inputs and thereby
decrease the forward propagated variation and information.
Therefore ELUs code the degree of presence of
particular phenomena in the input, while they
do not quantitatively model the degree of their absence.

In experiments, ELUs lead not only to faster learning, but also to
significantly better generalization performance than ReLUs and LReLUs
on networks with more than 5 layers.
On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch
normalization while batch normalization does not improve ELU networks.
ELU networks are among the top 10
reported CIFAR-10 results and yield the best published result on CIFAR-100,
without resorting to multi-view evaluation or model averaging.
On ImageNet, ELU networks considerably speed up learning
compared to a ReLU network with the same architecture, obtaining
less than 10\% classification error for a single crop,
single model network.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Currently the most popular activation function for neural networks is
the rectified linear unit (ReLU), which was first proposed for
restricted Boltzmann machines \citep{Nair:10} and then successfully used
for neural networks \citep{Glorot:11}.
The ReLU activation function is the identity for positive
arguments and zero otherwise.
Besides producing sparse codes,
the main advantage of ReLUs is that they alleviate the vanishing
gradient problem \citep{Hochreiter:98fuzzy, Hochreiter:2001}
since the derivative of 1 for positive
values is not contractive \citep{Glorot:11}.
However ReLUs are non-negative and, therefore, have a mean
activation larger than zero.

Units that have a non-zero mean activation act as bias for
the next layer.
If such units do not cancel each other out,
learning causes a {\em bias shift} for units in next layer.
The more the units are correlated, the higher their bias shift.
We will see that Fisher optimal learning, i.e., the natural
gradient \citep{Amari:98},
would correct for the bias shift
by adjusting the weight updates.
Thus, less bias shift brings the standard gradient closer to
the natural gradient and speeds up learning.
We aim at activation functions that push activation means closer to zero
to decrease the bias shift effect.

Centering the activations at zero
has been proposed in order to keep the off-diagonal entries of the
Fisher information matrix small \citep{Raiko:12}.
For neural network it is known that centering the activations
speeds up learning \citep{LeCun:91,LeCun:98,Schraudolph:98}.
``Batch normalization'' also centers activations with the goal
to counter the internal covariate shift \citep{Ioffe:15}.
Also the Projected Natural Gradient Descent algorithm (PRONG)
centers the activations by implicitly whitening them
\citep{Desjardins:15}.


An alternative to centering is to push the mean activation toward zero by
an appropriate activation function.
Therefore  has been preferred over
logistic functions \citep{LeCun:91,LeCun:98}.
Recently ``Leaky ReLUs'' (LReLUs)
that replace the negative part of the ReLU with a linear function
have been shown to be superior to ReLUs \citep{Maas:13}.
Parametric Rectified Linear Units (PReLUs) generalize LReLUs
by learning the slope of the negative part which yielded
improved learning behavior on large image benchmark data sets \citep{He:15}.
Another variant are
Randomized Leaky Rectified Linear Units (RReLUs) which randomly sample
the slope of the negative part which raised the performance on
image benchmark datasets and convolutional networks \citep{Xu:15}.

In contrast to ReLUs, activation functions like
LReLUs, PReLUs, and RReLUs do not
ensure a noise-robust deactivation state.
We propose an activation function that has negative
values to allow for mean activations close to zero,
but which saturates to a negative value with smaller arguments.
The saturation decreases the variation of the units if
deactivated, so the precise deactivation argument is less relevant.
Such an activation function can code the degree of presence of
particular phenomena in the input, but does
not quantitatively model the degree of their absence.
Therefore, such an activation function is more robust to noise.
Consequently, dependencies between
coding units are much easier to model and much easier to interpret
since only activated code units carry much information.
Furthermore, distinct concepts are much less likely to interfere with
such activation functions since the deactivation state is
non-informative, i.e.\ variance decreasing.

\section{Bias Shift Correction Speeds Up Learning}
\label{sec:naturalGradient}
To derive and analyze the bias shift effect mentioned in
the introduction, we utilize the natural gradient.
The natural gradient corrects the gradient direction with
the inverse Fisher information matrix and, thereby, enables
Fisher optimal learning, which ensures
the steepest descent in the Riemannian parameter manifold
and Fisher efficiency for online learning \citep{Amari:98}.
The recently introduced Hessian-Free Optimization
technique \citep{Martens:10}
and the Krylov Subspace Descent methods \citep{Vinyals:12}
use an extended Gauss-Newton approximation of the Hessian,
therefore they can be interpreted as versions of natural
gradient descent \citep{Pascanu:14}.


Since for neural networks the
Fisher information matrix is typically too expensive to compute,
different approximations
of the natural gradient have been proposed.
Topmoumoute Online natural Gradient
Algorithm (TONGA) \citep{LeRoux:08} uses a
low-rank approximation of natural gradient descent.
FActorized Natural Gradient (FANG) \citep{Grosse:15}
estimates the natural gradient via an approximation of
the Fisher information matrix by a Gaussian graphical model.
The Fisher information matrix can be approximated by a block-diagonal matrix,
where unit or quasi-diagonal natural gradients are used \citep{Olivier:13}.
Unit natural gradients or ``Unitwise Fisher's scoring'' \citep{Kurita:93}
are based on natural gradients for perceptrons \citep{Amari:98,Yang:98}.
We will base our analysis on the unit natural gradient.

We assume a parameterized probabilistic model 
with parameter vector  and data .
The training data are  with
, where  is the
input for example  and  is its label.
 is the loss of example  using model
. The average loss on the
training data  is the empirical risk .
Gradient descent updates the
weight vector  by

where  is the learning rate.
The {\em natural gradient} is the inverse Fisher
information matrix  multiplied by the gradient of the empirical
risk:
.
For a  multi-layer perceptron
 is the unit activation vector and  is the
bias unit activation.
We consider the ingoing weights to unit , therefore we drop the
index :  for the weight from unit 
to unit ,  for the activation, and  for the bias weight of unit .
The activation function  maps
the net input  of unit  to its activation
.
For computing the Fisher information matrix, the derivative of
the log-output probability
 is required.
Therefore we define the  at unit  as
,
which can be computed via backpropagation, but using
the log-output probability instead of the conventional loss function.
The derivative is
.

We restrict the Fisher information matrix to weights leading to
unit  which is the {\em unit Fisher information matrix} .
 captures only the interactions of weights to
unit . Consequently, the unit natural gradient only corrects
the interactions of weights to unit , i.e.\ considers the
Riemannian parameter manifold only in a subspace.
The unit Fisher information matrix is

Weighting the activations by
 is equivalent to
adjusting the probability of drawing inputs . Inputs  with large
 are drawn with higher probability.
Since , we can define a
distribution  :

Using , the entries of  can be expressed as second moments:


If the bias unit is  with weight 
then the weight vector can be divided
into a bias part  and the rest : .
For the row 
that corresponds to the bias weight, we have:

The next Theorem~\ref{th:th1} gives the correction of the standard
gradient by the unit natural gradient where
the bias weight is treated separately (see also \citet{Yang:98}).
\begin{theorem}
\label{th:th1}
The unit natural gradient corrects the weight update
 to a unit  by
following affine transformation of
the gradient  :

where 
is the unit Fisher information matrix without row 0 and column 0
corresponding to the bias weight.
The vector  is the zeroth column of
 corresponding to the bias weight,
and the positive scalar  is

where  is the vector of activations of units with weights to unit  and
.
\end{theorem}

\begin{proof}
Multiplying the inverse Fisher matrix  with the separated
gradient 
gives the weight update :

where


The previous formula is derived in
Lemma~\ref{th:lemma1} in the appendix.
Using  in the update gives

The right hand side is obtained by inserting
 in the left hand side
update.
Since ,
,
and , we obtain

Applying Lemma~\ref{th:lemma2} in the appendix gives the formula for .
\end{proof}

The {\em bias shift} (mean shift) of unit  is the change of unit 's
mean value due to the weight update. Bias shifts of unit 
lead to oscillations and impede learning. See Section~4.4 in \citet{LeCun:98} for
demonstrating this effect at the inputs and in \citet{LeCun:91} for explaining
this effect using the input covariance matrix.
Such bias shifts are mitigated or even prevented
by the unit natural gradient.
The {\em bias shift correction} of the unit natural gradient is the
effect on the bias shift due to  which captures
the interaction between the bias unit and the incoming units.
Without bias shift correction, i.e.,  and ,
the weight updates are  and .
As only the activations
depend on the input, the bias shift can be computed by multiplying the weight
update by the mean of the activation vector .
Thus we obtain the bias shift
.
The bias shift strongly depends on the correlation of the incoming
units which is captured by .

Next, Theorem~\ref{th:th2} states that the bias shift correction by the
unit natural gradient can be
considered to correct the incoming mean  proportional
to  toward zero.
\begin{theorem}
\label{th:th2}
The bias shift correction by the unit natural gradient is equivalent to
an additive correction of the incoming mean by

and a multiplicative correction of the bias unit by , where

\end{theorem}
\begin{proof}
Using
, the bias shift is:


The mean correction term, indicated by an underbrace in previous
formula, is

The expression Eq.~\eqref{eq:k} for  follows from
Lemma~\ref{th:lemma2} in the appendix.
The bias unit correction term is
.
\end{proof}


In Theorem~\ref{th:th2} we can reformulate
. Therefore  increases with the length of
 for given variances and covariances.
Consequently the bias shift correction through the
unit natural gradient is governed by the length of .
The bias shift correction is zero for  since
 does not correct the bias unit multiplicatively.
Using Eq.~\eqref{eq:b},  is split into an
offset and an information containing term:

In general, {\em smaller positive  lead to smaller
positive , therefore to smaller corrections.}
The reason is that in general the largest absolute components
of  are positive,
since activated inputs will activate the unit  which in turn will have
large impact on the output.


To summarize,
the unit natural gradient corrects the bias shift of unit 
via the interactions of incoming units with the bias unit
to ensure efficient learning.
This correction is equivalent to shifting the mean activations
of the incoming units toward zero and scaling up the bias unit.
To reduce the undesired bias shift effect without the natural gradient,
either the
(i) activation of incoming units can be
centered at zero
or (ii) activation functions with negative values can be used.
We introduce a new activation function with negative values while
keeping the identity for positive arguments where it is not contradicting.


\section{Exponential Linear Units (ELUs)}
\label{sec:elu}



The {\em exponential linear unit} (ELU) with  is

The ELU hyperparameter  controls the value to
which an ELU saturates for negative net inputs (see Fig.~\ref{fig:ActivationFunction}).
ELUs diminish the vanishing gradient effect
as rectified linear units (ReLUs) and leaky ReLUs
(LReLUs) do.
The vanishing gradient problem is alleviated because
the positive part of these functions is the identity,
therefore their derivative is one and not contractive.
In contrast,  and sigmoid activation functions are contractive
almost everywhere.

\begin{wrapfigure}{r}{0.45\textwidth}
\vspace*{-10pt}
\begin{center}
\includegraphics[width=0.45\textwidth]{ActivationFunction}
\end{center}
\caption{The rectified linear unit (ReLU), the leaky ReLU (LReLU, ), the shifted ReLUs (SReLUs),
and the exponential linear unit (ELU, ). \label{fig:ActivationFunction}}
\vspace*{-5pt}
\end{wrapfigure}
In contrast to ReLUs, ELUs have negative values which
pushes the mean of the activations closer to zero.
Mean activations that are closer to zero enable faster learning as
they bring the gradient closer to the natural gradient (see
Theorem~\ref{th:th2} and text thereafter).
ELUs saturate to a negative value when the argument gets smaller.
Saturation means a small derivative
which decreases the variation and the information that is
propagated to the next layer. Therefore the representation
is both noise-robust and low-complex \citep{Hochreiter:99nc}.
ELUs code the degree of presence of input concepts,
while they neither quantify the degree of their absence nor distinguish
the causes of their absence.
This property of non-informative deactivation states is also present at ReLUs
and allowed to detect biclusters corresponding to
biological modules in gene expression datasets \citep{Clevert:15nips} and
to identify toxicophores in toxicity prediction \citep{Unterthiner:15,Mayr:15}.
The enabling features for these interpretations
is that activation can be clearly distinguished from deactivation
and that only active units carry relevant information and can crosstalk.


\section{Experiments Using ELUs}
\label{sec:exp}
\begin{figure}[!ht]
\begin{center}
\subfigure[Average unit activation]{
\includegraphics[angle=0,width= 0.49\textwidth]{mnist-avg-unit-activation_new}}
\subfigure[Cross entropy loss]{
\includegraphics[angle=0,width= 0.49\textwidth]{mnist-crossentropy_new}}
\end{center}
\caption{ELU networks evaluated at MNIST. Lines are
the average over five runs with different random initializations, error
bars show standard deviation.
Panel (a): median of the average unit activation for different
activation functions.
Panel (b): Training set (straight line) and validation set (dotted line)
 cross entropy loss. All lines stay flat after epoch 25.
\label{fig:mnistplots}}
\end{figure}

In this section, we assess the performance of exponential linear units (ELUs)
if used for unsupervised and supervised learning of deep autoencoders and
deep convolutional networks. ELUs with   are compared to
(i) Rectified Linear Units (ReLUs) with activation ,
(ii) Leaky ReLUs (LReLUs) with activation  (), and
(iii) Shifted ReLUs (SReLUs) with activation .
Comparisons are done with and without batch normalization.
The following benchmark datasets are used:
(i) {\em MNIST} (gray images in 10 classes, 60k train and 10k test),
(ii) {\em CIFAR-10} (color images in 10 classes, 50k train and 10k test),
(iii) {\em CIFAR-100} (color images in 100 classes, 50k train and 10k test), and
(iv) {\em ImageNet} (color images in 1,000 classes, 1.3M train and 100k tests).


\subsection{MNIST}

\subsubsection{Learning Behavior}
We first want to verify that ELUs keep the mean activations closer to
zero than other units. Fully connected deep neural networks
with ELUs (), ReLUs, and LReLUs () were
trained on the MNIST digit classification
dataset while each hidden unit's activation was tracked.
Each network had eight hidden layers of 128 units each, and was trained
for 300 epochs by
stochastic gradient descent with
learning rate  and mini-batches of size 64.
The weights have been initialized according to \citep{He:15}.
After each epoch we calculated the units' average activations on a fixed
subset of the training data.
Fig.~\ref{fig:mnistplots} shows the median over all units along learning.
ELUs stay have smaller median throughout the training process.
The training error of ELU networks decreases much more rapidly than for the other
networks.

Section~\ref{sec:varianceMean} in the appendix compares the variance
of median activation in ReLU and ELU networks.
The median varies much more in ReLU networks.
This indicates that ReLU networks continuously try to correct
the bias shift introduced by previous weight updates
while this effect is much less prominent in ELU networks.

\subsubsection{Autoencoder Learning}

\begin{figure}[!ht]
\begin{center}
\subfigure[Training set]{
\includegraphics[angle=0,width= 0.49\textwidth]{mnist-autoencoder_new}}
\subfigure[Test set]{
\includegraphics[angle=0,width= 0.49\textwidth]{mnist-autoencoder_valid_new}}
\end{center}
\caption{Autoencoder training on MNIST: Reconstruction error for the test and training  data set over
epochs, using different activation functions and learning rates. The results are medians
over several runs with different random initializations.
\label{fig:mnistAuto}}
\end{figure}




To evaluate ELU networks at unsupervised settings, we followed
\citet{Martens:10} and \citet{Desjardins:15} and trained a
deep autoencoder on the MNIST dataset.
The encoder part consisted of four
fully connected hidden layers with sizes 1000, 500, 250 and 30, respectively.
The decoder part was symmetrical to the encoder.
For learning we applied stochastic gradient descent with mini-batches of 64 samples for 500 epochs
using the fixed learning rates ().
Fig.~\ref{fig:mnistAuto} shows, that ELUs outperform the competing
activation functions in terms of training / test set reconstruction error for all learning rates.
As already noted by \citet{Desjardins:15}, higher learning rates
seem to perform better.



\subsection{Comparison of Activation Functions}
\label{sec:compFunctions}

In this subsection we show that ELUs indeed possess a superior
learning behavior compared to other activation functions as
postulated in Section~\ref{sec:elu}.
Furthermore we show that ELU networks perform better than ReLU
networks with batch normalization.
We use as benchmark dataset CIFAR-100 and use a relatively
simple convolutional neural network (CNN) architecture to keep the
computational complexity reasonable for comparisons.


\begin{figure*}[!ht]
\begin{center}
\subfigure[Training loss]{
\includegraphics[angle=0,width= 0.32\textwidth]{TrainLoss}}
\subfigure[Training loss (start)]{
\includegraphics[angle=0,width= 0.32\textwidth]{TrainLossStart}}
\subfigure[Training loss (end)]{
\includegraphics[angle=0,width= 0.32\textwidth]{TrainLossEnd}}\-2.0ex]
\caption{Comparison of ReLUs, LReLUs, and SReLUs on CIFAR-100.
Panels (a-c) show the training loss, panels (d-f) the test classification
error. The ribbon band show the mean and standard
deviation for 10 runs along the curve. ELU networks achieved lowest test
error and training loss.
\label{fig:resCIFAR100}}
\end{center}
\vspace*{-5pt}
\end{figure*}


\begin{figure*}[!ht]
\begin{center}
\subfigure[ELU - ReLU ]{
\includegraphics[angle=0,width= 0.32\textwidth]{TestError_bn_elu}}
\subfigure[ELU - SReLU ]{
\includegraphics[angle=0,width= 0.32\textwidth]{TestError_bn_srelu}}
\subfigure[ELU - LReLU ]{
\includegraphics[angle=0,width= 0.32\textwidth]{TestError_bn_leaky}}\
\BM^{-1} \ &= \
\begin{pmatrix}
\BA & \Bb \\
\Bb^T & c
\end{pmatrix}^{-1} \ = \
\begin{pmatrix}
\BK & \Bu \\
\Bu^T & s
\end{pmatrix} \ ,

\BK \ &= \ \BA^{-1}  \ + \  \Bu \ s^{-1} \Bu^T \\
\Bu \ &= \ - \  s \ \BA^{-1} \ \Bb \\
s \ &= \ \left( c \ - \ \Bb^T\BA^{-1} \Bb \right)^{-1} \ .

\begin{pmatrix}
\BA & \BB \\
\BB^T & \BC
\end{pmatrix}^{-1} \ &= \
\begin{pmatrix}
\BK & \BU \\
\BU^T & \BS
\end{pmatrix} \ ,

\BK \ &= \ \BA^{-1}  \ + \  \BA^{-1} \ \BB \ \left( \BC \ - \ \BB^T\BA^{-1} \BB
\right)^{-1} \BB^T \ \BA^{-1} \\
\BU \ &= \ - \  \BA^{-1} \ \BB \ \left( \BC \ - \ \BB^T\BA^{-1} \BB
\right)^{-1} \\
\BU^T \ &= \ - \ \left( \BC \ - \ \BB^T\BA^{-1} \BB
\right)^{-1}  \BB^T \ \BA^{-1} \\
\BS \ &= \ \left( \BC \ - \ \BB^T\BA^{-1} \BB
\right)^{-1} \ .

\BK   \ &= \ \BA^{-1} \ + \ \BU \ \BS^{-1} \BU^T \ .

\begin{pmatrix}
\BA & \Bb \\
\Bb^T & c
\end{pmatrix}^{-1} \ &= \
\begin{pmatrix}
\BK & \Bu \\
\Bu^T & s
\end{pmatrix} \ ,

\BK \ &= \ \BA^{-1}  \ + \  \BA^{-1} \ \Bb \ \left( c \ - \ \Bb^T\BA^{-1} \Bb
\right)^{-1} \Bb^T \ \BA^{-1} \\
\Bu \ &= \ - \  \BA^{-1} \ \Bb \ \left( c \ - \ \Bb^T\BA^{-1} \Bb
\right)^{-1} \\
\Bu^T \ &= \ - \ \left( c \ - \ \Bb^T\BA^{-1} \Bb
\right)^{-1}  \Bb^T \ \BA^{-1} \\
s \ &= \ \left( c \ - \ \Bb^T\BA^{-1} \Bb
\right)^{-1} \ .

\BK   \ &= \ \BA^{-1} \ + \ \Bu \ s^{-1} \Bu^T \ .

\BK \ &= \ \BA^{-1}  \ + \  \Bu \ s^{-1} \Bu^T \\
\Bu \ &= \ - \  s \ \BA^{-1} \ \Bb \\
\Bu^T \ &= \ - \ s \ \Bb^T \ \BA^{-1} \\
s \ &= \ \left( c \ - \ \Bb^T\BA^{-1} \Bb \right)^{-1} \ .

&\EXP^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba) \ \leq \ 1

&\left( 1 \ - \ \EXP^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)
\right)^{-1} \ = \ 1 \ + \ \EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba) \ .

&\left( 1 \ - \ \EXP^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)
\right)^{-1} \ \left(1 \ - \ \EXP_p^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)\right)
\\ \nonumber
&= \ 1 \ + \  \left( \EXP(\Ba) \ - \ \EXP_p(\Ba) \right)^T
\VAR^{-1}(\Ba) \ \EXP(\Ba) \ .

&\left(\BA \ + \ \Bb \ \Bc^T \right)^{-1} \ = \ \BA^{-1} \ - \ \frac{\BA^{-1}
    \ \Bb \ \Bc^T \ \BA^{-1}}{1 \ + \ \Bc^T \BA^{-1} \Bb} \ .

\label{eq:sherman}
&\Bc^T \left(\BA \ + \ \Bb \ \Bb^T\right)^{-1} \Bb \ = \ \Bc^T \BA^{-1} \Bb \ - \ \frac{\Bc^T \BA^{-1}
    \ \Bb \ \Bb^T \ \BA^{-1} \Bb}{1 \ + \ \Bb^T \BA^{-1} \Bb} \\ \nonumber
&= \ \frac{\Bc^T \BA^{-1} \Bb \ \left( 1 \ + \ \Bb^T \BA^{-1} \Bb
  \right) \ - \ \left(\Bc^T \BA^{-1}
    \ \Bb\right)\left(\Bb^T \BA^{-1}
    \ \Bb\right)  }{1 \ + \ \Bb^T \BA^{-1} \Bb} \\ \nonumber
&= \ \frac{\Bc^T \BA^{-1} \Bb}{1 \ + \ \Bb^T \BA^{-1} \Bb} \ .

&\EXP(\Ba \ \Ba^T) \ = \ \VAR(\Ba) \ + \ \EXP(\Ba) \ \EXP^T(\Ba)

&\EXP^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)
\ = \ \EXP^T(\Ba) \ \left( \VAR(\Ba) \ + \ \EXP(\Ba) \ \EXP^T(\Ba)
\right)^{-1} \ \EXP(\Ba) \\ \nonumber
&= \ \frac{\EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}{1 \ + \
  \EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}
\ \leq \ 1 \ .

&\left( 1 \ - \ \EXP^T(\Ba) \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)
\right)^{-1} \ = \ 1 \ + \ \EXP^T(\Ba) \VAR^{-1}(\Ba) \ \EXP(\Ba) \ .

&\EXP_p^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)
\ = \ \EXP_p^T(\Ba) \ \left( \VAR(\Ba) \ + \ \EXP(\Ba) \ \EXP^T(\Ba)
\right)^{-1} \ \EXP(\Ba) \\ \nonumber
&= \ \frac{\EXP_p^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}{1 \ + \
  \EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}
\ .

&1 \ - \ \EXP_p^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)
\ = \ 1 \ - \ \frac{\EXP_p^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}{1 \ + \
  \EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}
 \\ \nonumber
&= \ \frac{1 \ + \  \EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba) \ - \
  \EXP_p^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}{1 \ + \ \EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}
\ = \ \frac{1 \ + \  \left( \EXP(\Ba) \ - \ \EXP_p(\Ba) \right)^T
  \VAR^{-1}(\Ba) \ \EXP(\Ba) }{1 \ + \ \EXP^T(\Ba) \ \VAR^{-1}(\Ba) \ \EXP(\Ba)}
\ .

&\left( 1 \ - \ \EXP^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)
\right)^{-1} \ \left(1 \ - \ \EXP_p^T(\Ba) \ \EXP^{-1}(\Ba \ \Ba^T) \ \EXP(\Ba)\right)
\\ \nonumber
&= \ 1 \ + \  \left( \EXP(\Ba) \ - \ \EXP_p(\Ba) \right)^T
\VAR^{-1}(\Ba) \ \EXP(\Ba) \ .


\end{proof}

\section{Variance of Mean Activations in ELU and ReLU Networks}
\label{sec:varianceMean}
To compare the variance of median activation in ReLU and ELU networks,
we trained a neural network with 5 hidden layers of 256 hidden units for 200
epochs using a learning rate of 0.01, once using ReLU and once using ELU
activation functions on the MNIST dataset.
After each epoch, we calculated the median activation
of each hidden unit on the whole training set.
We then calculated the variance of these changes, which is depicted in
Figure~\ref{fig:varianceofchanges}\, . The median varies much more in ReLU networks.
This indicates that ReLU networks continuously try to correct
the bias shift introduced by previous weight updates
while this effect is much less prominent in ELU networks.
\begin{figure*}[!ht]
\begin{center}
\includegraphics[width= 0.7\textwidth]{VariationOfChange}
\caption{Distribution of variances of the median hidden unit activation
after each epoch of MNIST training. Each row represents the units in
a different layer of the network.
\label{fig:varianceofchanges}}
\end{center}
\end{figure*}


\end{document}
