


\documentclass[svgnames,usenames,preprint,nocopyrightspace]{sigplanconf}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{xspace}
\usepackage[colorlinks=true]{hyperref}

\usepgflibrary{shapes.geometric}
\usepgflibrary[shapes.geometric]
\usetikzlibrary{shapes.geometric}
\usetikzlibrary[shapes.geometric]
\usetikzlibrary[matrix]
\usetikzlibrary{arrows, decorations, decorations.markings}
\usetikzlibrary{positioning}

\usepackage{listings}

\newcommand*\lstinputpath[1]{\lstset{inputpath=#1}}
\lstinputpath{code}


\usepackage{array,arydshln}


\usepackage{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{light-red}{rgb}{1.0,0.8,0.8}
\definecolor{yellow-orange}{rgb}{1.0,0.85,0.05}
\definecolor{gray-blue}{rgb}{0.57,0.72,0.84}
\definecolor{light-gray}{gray}{0.80}

\definecolor{DarkGreen}{rgb}{0.0, 0.2, 0.13}

\definecolor{apricot}{rgb}{0.98, 0.81, 0.69}
\definecolor{bananayellow}{rgb}{1.0, 0.88, 0.21}
\definecolor{babyblueeyes}{rgb}{0.63, 0.79, 0.95}

\usepackage{soul}
\sethlcolor{titlebgcolor}

\newcommand\red[1]{\setlength\fboxsep{0pt}\colorbox{light-red}{\strut #1}}
\newcommand\blue[1]{\setlength\fboxsep{0pt}\colorbox{cyan}{\strut #1}}
\newcommand\green[1]{\setlength\fboxsep{0pt}\colorbox{GreenYellow}{\strut #1}}
\newcommand\brown[1]{\setlength\fboxsep{0pt}\colorbox{apricot}{\strut #1}}
\newcommand\yellow[1]{\setlength\fboxsep{0pt}\colorbox{bananayellow}{\strut #1}}
\newcommand\grey[1]{\setlength\fboxsep{0pt}\colorbox{babyblueeyes}{\strut #1}}

\def\redcolor{\color{red}}
\def\blackcolor{\color{black}}


\newcommand{\cL}{{\cal L}}


\usepackage{wrapfig}
\usepackage{times} \usepackage{stmaryrd} \usepackage{centernot}


\usepackage{comment}
\excludecomment{enclosedreminders}


\usepackage[
     disable,
    textwidth=0.3\textwidth,textsize=scriptsize]{todonotes}
    
\newcommand{\gvcommin}[1]{\todo[inline,color=lightgray,bordercolor=gray,linecolor=gray]{GV: #1}}
\newcommand{\gvcomm}[1]{\todo[color=lightgray,bordercolor=gray,linecolor=gray]{GV: #1}}
\newcommand{\stcomm}[1]{\todo[color=red!20,bordercolor=red,linecolor=red,size=\scriptsize, caption={}]{ST: #1}}
\newcommand{\stcommin}[1]{\todo[inline,color=red!20,bordercolor=red,linecolor=red,size=\scriptsize, caption={}]{ST: #1}}
\newcommand{\mclcomm}[1]{\todo[color=Beige,bordercolor=DarkGoldenrod,linecolor=DarkGoldenrod]{MCL: #1}}
\newcommand{\mclcommin}[1]{\todo[inline,color=Beige,bordercolor=DarkGoldenrod,linecolor=DarkGoldenrod]{MCL: #1}}
\newcommand{\jmcomm}[1]{\todo[color=blue!20,bordercolor=blue,linecolor=blue,size=\scriptsize, caption={}]{JM: #1}}
\newcommand{\jmcommin}[1]{\todo[inline,color=blue!20,bordercolor=blue,linecolor=blue,size=\scriptsize, caption={}]{JM: #1}}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinestyle{cstyle}{
  frame=tb,
  escapechar={@},
  language=c,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
basicstyle={\footnotesize\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  keywordstyle=[2]\color{dkgreen},
  keywordstyle=[3]\color{magenta},
  commentstyle=\color{gray},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
}

\lstdefinestyle{hstyle}{
  frame=tb,
  escapechar={\scriptstyle #1\scriptscriptstyle \i\scriptscriptstyle \ix_{\x}\scriptstyle h_{\h}\scriptstyle 0y_{\t}\textbf{c}=a\textbf{v}+b\textbf{v}\forall x, y .~ f(x,y) = f(y,x)\forall x, y, z .~ f(f(x,y),z) = f(x,f(y,z))\forall x, y, z .~ g(f(x,y),z) = f(g(x,z),g(y,z))iiiii+0ii0length(\mathtt{v})s \nowrites\ lsll \notin \writes(s)s \noreads\ lsls_1 \nowrites\ s_2s_1s_2s_1 \noreads\ s_2s_1s_2s_1 \nowritesexcarrays\ s_2s_1 \nowritesarray\ s_2s_1s_2e~\puree\writes(s)sg~\distributesover~f\forall x, y, z .~ g(f(x,y),z) \approx f(g(x,z),g(y,z))l~\freshlle_{ini}rel(l,e_{end})mod(l)s_1le_{ini}rel(l,e_{end})mod(l)s_2le_{ini}rel(l,e_{end})mod(l)s_1s_2rel~\pure(s_1s_2) \nowrites \{l, e_{ini}, e_{end}\}\writes(mod(l)) \subseteq \{l\},c_i, r_i) =
        \NewCode(c_{i -1}, \{r_{i -1}\})\neg \IsFinal(c_i)\SelectRule\NewCode\Code_i\Rule_i\SelectRuleCode_iRule_iSelectRuleRule_iSelectRule$ is
implicitelly selecting a position.








\section{Producing Code for Heterogeneous Systems}
\label{sec:translation}

In the second phase of the tool (Figure~\ref{fig:code_trans_seq}),
code for a given platform is produced starting from the result of the
transformation process.  The destination platform of a fragment of
code can be specified using annotations like this:

\medskip

\texttt{\#pragma polca mpi}

\medskip

This information is relevant, on one hand, to hint at what
transformations should be applied and also to decide when the code is
ready for translation.  In fact, the decisions taken by the machine
learning-based tool we mentioned before 
is partly directed by the destination architecture. 




The translation to a final code for a given architecture is in most
cases straightforward as it needs only to introduce the ``idioms''
necessary for the architecture or to perform a syntactical
translation.  As a consequence, there is no search or decision
process: for each input code given to the translation, there is only
one output code
which is obtained via predefined transformations or glue code
injection.




Some of the translations need specific information: for instance,
knowing if a statement is performing I/O is necessary when translating
to MPI, because  executing this operation might need to be done in a
single thread.  It is often the case that this can be deduced by
syntactical inspection, but in other cases (e.g., if the operation is
part of a library function) it may need explicit annotations.






\section{Implementation Notes}
\label{sec:implementation}



The transformation phase, which obtains C code that could be easily
translated into the source language for the destination platform, is a
key part of the tool.  As a large part of the system was experimental
(including the definition of the language, the properties, the
generation of the final code, and the search / rule selection
procedures), we needed a flexible and expressive implementation
platform.

We initially tested well-known infrastructures such as Clang / LLVM.
While they are very well supported and tested, we found that they
understandably were primarily designed for compilation instead of for
source-to-source program transformation, which is our main goal.  When
implementing complex source-to-source program transformation routines
in Clang, we found that the interface offered was not really designed
to perform AST transformations, and that the design documents warned
that the interface could not be assumed to be stable.  Additionally,
the methods to transform source code had to be coded in C++ which made
them verbose and full of low-level details which we did not want to
deal with.  Compiling rules to C++ was of course an option, but even
this compilation was not going to be easy, due to the conceptual
distance between the nature of the rules and the API for code
manipulation, and it was dependant upon an unstable interface.  Even
in that case, the whole Clang would have to be recompiled after
introducing new rules, which made project development and testing
cumbersome, and would make the addition user-defined rules complicated.

Therefore we decided to switch to a declarative language and implement
the tool in Haskell.  Parsing the input code is done by means of the 
\texttt{Language.C}~\cite{LanguageC} library, which returns the AST as
a data structure which is easy to manipulate.  In particular, we used
the Haskell facilities to deal with generic data structures through
the \emph{Scrap Your Boilerplate} (SYB) library~\cite{DataGenerics}.  This
allows us to easily extract information from the AST or modify it with
a generic traversal of the whole structure. 

The rules themselves are written in a subset of C, and can therefore
be also parsed using \texttt{Language.C}.  After reading them in, they
are automatically compiled into Haskell code (contained in the file
\texttt{Rules.hs} ---see Figure~\ref{fig:ana-trans-tool}) which
performs the traversal and (when applicable) the transformation of the
AST.  This module is loaded together with the rest of the tool,
therefore avoiding the extra overhead of interpreting the rules.  The
declarative nature of Haskell and facilities such as completely
automatic memory management makes this compilation much easier than it
would be in the case of compiling into C.














When it comes to rule compilation, \stml rules can be divided into two
classes: those which operate at the expression level (which are easier
to implement) and those which can manipulate both expressions and
(sequences of) statements.  In the latter case, sequences of
statements (\texttt{cstmts}) of an unknown size have to be considered:
for example, in Figure~\ref{fig:stml-rule-JoinAssignments},
\texttt{s1}, \texttt{s2}, and \texttt{s3} can be sequences of any
number of statements (including the empty sequence), and the rule has
to try all the possibilities to determine if there is a match which
meets the rule conditions. For this, Haskell code that explicitly
performs an AST traversal needs to be generated.  In the case of
expressions, they are syntactically bound, and the translation of the
rule is much easier.


When generating Haskell code, the rule sections (\texttt{pattern},
\texttt{condition}, \texttt{generate}, \texttt{assert}) generate the 
corresponding LHS's, guards, and RHS's of a Haskell function.
If the conditions to apply a rule are met, the result is returned in a
triplet \ihaskell{(rule\_name, old\_code, new\_code)} where the two
last components are, respectively, the matched and transformed
sections of the AST.  Note that \ihaskell{new\_code} may contain new
properties if the \texttt{generate} section of the rule defines them.

Since several rules can be applied at several locations of the AST,
every rewriting step can actually return a list of tuples --- one for
each rule
and location where that rule can be applied.  As mentioned elsewhere
(Section~\ref{sec:oracle-selection}), besides the possibility of
interacting with a user, we are studying the usage of an external oracle
which 
determines the best candidate to apply in the next step.  The
transformation halts when either no more rules are
applicable or when a stop condition is found, according to the
oracle. 


The tool is divided into four main modules: 

\begin{itemize}

\item \texttt{\textbf{Main.hs}} 
implements the main  workflow of the tool: it calls the parser on the input C
  code to build the AST, links the pragmas to the AST, executes the 
  transformation sequence (interactive or automatically)
and outputs the transformed code.

\item \texttt{\textbf{PragmaLib.hs}} reads pragmas and links them to
  their corresponding node in the AST.  It also restores or injects
  pragmas in the transformed code.


\item \texttt{\textbf{Rul2Has.hs}} translates \stml rules (stored in
  an external file) into Haskell functions which actually perform the
  AST manipulation.  It also reads and loads \stml rules as an AST and
  generates the corresponding Haskell code in the
  \texttt{\textbf{Rules.hs}} file.



\item \texttt{\textbf{RulesLib.hs}} contains supporting code used by
  \texttt{Rules.hs} to identify whether some \stml rule is or not
  applicable (e.g., there is matching code, the preconditions hold,
  etc.) and to execute the implementation of the rule (including AST
  traversal, transformation, \ldots).
\end{itemize}









\section{Conclusion}
\label{sec:conclusions}

We have presented a transformation toolchain that uses semantic
information, in the form of user- or machine-provided annotations, to
produce code for different platforms.  It has two clearly separated
phases: a source-to-source transformation which generates code with
the style appropriate for the destination architecture and a
translation from that code to the one used in the specific platform.

We have focused until now in the initial phase, which included the
specification of a DSL (\stml) to define rules and code properties, a
translator from this language into Haskell, a complete engine to work with
these rules, and an interface to interact with external oracles (such
as a reinforcement learning tool which we are developing) to guide the
transformation.

The translation phase is still in an preliminary stage. However, and
while it is able to translate some input code, it needs to be improved
in order to support a wider range of input code.  
We have compared, using several metrics, the code obtained using our
tool and the corresponding initial code and the results are
encouraging. 


As future work, we plan to improve the usability of the \stml
language.
At the same time, we are modifying Cetus to automatically obtain more
advanced\,/\,specific properties, and we are integrating profiling
techniques in the process to make it easier to evaluate the whole
transformation system and give feedback on it.

Simultaneously, we are investigating other analysis tools which can
be used to derive more precise properties.  Many of these
properties are related to data dependencies and pointer behavior.  We
are considering, on one hand, tools like
PLuTo~\cite{Bondhugula:2008} and PET~\cite{poly_pet} (two polytope
model-based analysis tools) or the dependency analyzers for the
Clang\,/\,LLVM compiler.  However, since they fall short to derive
dependencies (e.g., alias analysis) in code with pointers, we are also
considering tools based on separation
logic~\cite{DBLP:conf/csl/OHearnRY01,DBLP:conf/lics/Reynolds02} such
as VeriFast~\cite{Jacobs08theverifast,DBLP:conf/nfm/JacobsSPVPP11}
which can reason on dynamically-allocated, mutable structures.






\acks

Work partially funded by EU FP7-ICT-2013.3.4 project 610686 POLCA, Comunidad de Madrid project S2013/ICE-2731 N-Greens Software, and MINECO Projects TIN2012-39391-C04-03 / TIN2012-39391-C04-04 (StrongSoft), TIN2013-44742-C4-1-R (CAVI-ROSE), and TIN2015-67522-C3-1-R (TRACES).



\bibliographystyle{abbrvnat}
\softraggedright
\bibliography{../../BiBTeX/hpc_transformations,../../BiBTeX/polca_refs,../../BiBTeX/polca_deliverables,../../BiBTeX/c_a_t}







\end{document}
