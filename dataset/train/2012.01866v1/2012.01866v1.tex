\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[final]{neurips_2020}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{caption}
\usepackage{subfig}
\usepackage{siunitx}
\usepackage{color}
\usepackage{comment}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{makecell}

\usepackage{xparse}

\ExplSyntaxOn
\cs_new:Nn\__avercalc_plus:n{
  + ( 100 * #1 )
}
\NewExpandableDocumentCommand{\avercalc}{O{1}+m}{\fp_eval:n {
    round(
      ( 0 \clist_map_function:nN { #2 } \__avercalc_plus:n ) / max(1, \clist_count:n { #2 })
      , #1
    )
  }
}
\ExplSyntaxOff


\usepackage{array}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}



\usepackage[ruled,vlined,linesnumbered]{algorithm2e} \SetKwInput{KwInput}{Input}                \SetKwInput{KwOutput}{Output}              





\newcommand{\lau}[1][magenta]{\textcolor{#1}}
\newcommand{\tim}[1][red]{\textcolor{#1}}
\newcommand{\alj}[1][cyan]{\textcolor{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand*{\dotcup}{\mathbin{\dot{\cup}}}
\newcommand{\TODO}[1]{\textcolor{red}{TODO #1}}

\newcommand*{\eg}{\emph{e.g.}\@\xspace}
\newcommand*{\ie}{\emph{i.e.}\@\xspace}
\newcommand*{\cf}{\emph{c.f.}\@\xspace}
\newcommand*{\etal}{\emph{et al.}\@\xspace}
 
\newif\ifarxiv
\arxivtrue

\unless\ifarxiv
  \usepackage{xr}
  \externaldocument{supp}
\fi







\title{Make One-Shot Video Object Segmentation \\ Efficient Again}



\author{Tim Meinhardt \\
  Technical University of Munich \\
  \texttt{tim.meinhardt@tum.de} \\
  \And
  Laura Leal-Taix{\'e} \\
  Technical University of Munich \\
  \texttt{leal.taixe@tum.de} \\
}

\begin{document}

\maketitle





\begin{abstract}

    Video object segmentation (VOS) describes the task of segmenting a set of objects in each frame of a video.
In the semi-supervised setting, the first mask of each object is provided at test time.
Following the one-shot principle, fine-tuning VOS methods train a segmentation model separately on each given object mask.
However, recently the VOS community has deemed such a test time optimization and its impact on the test runtime as unfeasible.
To mitigate the inefficiencies of previous fine-tuning approaches, we present~\textit{efficient One-Shot Video Object Segmentation} (e-OSVOS).
In contrast to most VOS approaches, e-OSVOS decouples the object detection task and predicts only local segmentation masks by applying a modified version of Mask R-CNN.
The one-shot test runtime and performance are optimized without a laborious and handcrafted hyperparameter search.
To this end, we meta learn the model initialization and learning rates for the test time optimization.
To achieve optimal learning behavior, we predict individual learning rates at a neuron level. Furthermore, we apply an online adaptation to address the common performance degradation throughout a sequence by continuously fine-tuning the model on previous mask predictions supported by a frame-to-frame bounding box propagation. e-OSVOS provides state-of-the-art results on DAVIS 2016, DAVIS 2017, and YouTube-VOS for one-shot fine-tuning methods while reducing the test runtime substantially.
\\
Code is available at~\url{https://github.com/dvl-tum/e-osvos}.

\end{abstract} 

\section{Introduction}

    Video object segmentation (VOS) describes a two-class (foreground-background) pixel-level classification task on each frame of a given video sequence.
Multiple objects are discriminated by predicting individual foreground-background pixel masks.
In this work, we address a variant of VOS which is semi-supervised at test time.
To this end, the ground truth foreground-background segmentation mask of the first frame is provided for each object. Machine learning methods that tackle semi-supervised VOS are categorized by their utilization of the provided object ground truth masks.


    We focus on fine-tuning methods~\cite{OSVOS,OSVOS-S,MVOS,premvos,onavos}, which exploit the transfer learning capabilities of neural networks and follow a multi-step training procedure: (i) {\it pre-training steps}: learn general image and segmentation features from training the model on images and video sequences , and (ii) {\it fine-tuning}: one-shot test time optimization which enables the model to learn foreground-background characteristics specific to each object and video sequence.
While elegant through their simplicity, fine-tuning methods face important shortcomings: (i) pre-training is fixed and not optimized for the subsequent fine-tuning, (ii) the hyperparameters of the test time optimization are often excessively handcrafted and fail to generalize between datasets.
The common existing fine-tuning setups~\cite{OSVOS,premvos} are inefficient and suffer from a high test runtime with as many as 1000 training iterations per segmented object.
As a consequence, recent methods refrain from such optimization at test time and instead opt for solutions such as template matching~\cite{BlazinglyFV,VideoMatch} and mask propagation~\cite{Cheng_favos_2018,Cheng_ICCV_2017,oh2018fast,Perazzi2017,8325298,Yang2018EfficientVO} for semi-supervised VOS.


    In this work, we revisit the concept of one-shot fine-tuning for VOS, and show how to leverage the power of meta learning to overcome the aforementioned issues.
To this end, we propose three key design choices which make one-shot fine-tuning for VOS efficient again:



    \begin{figure}[t]
    \centering

    \includegraphics[width=0.6\textwidth]{figures/teaser_img.pdf}


    \caption{Performance versus runtime comparison of modern video object segmentation (VOS) approaches on the DAVIS 2017 validation set.
We only show methods with publicly available runtime information.
Our~\textbf{e-OSVOS} approach demonstrates the relevance of fine-tuning for VOS and its inherent flexibility as we apply the same meta learned optimization for a varying number of iterations and with online adaptation (OnA).
}
    \label{fig:teaser}
\end{figure} 
    \noindent{\bf Learning the Model Initialization}
The common pre-training~\cite{OSVOS,OSVOS-S,premvos,onavos} yields a segmentation model not specifically optimized for the subsequent fine-tuning task and requires an \textit{unlearning} of potential false positive objects.
Therefore, we propose to meta learn the pre-training step, i.e., we learn the best initialization of the segmentation model for a subsequent fine-tuning to any object.

    \noindent{\bf Learning Neuron-Level Learning Rates}
We replace the laborious and handcrafted hyperparameter search from~\cite{OSVOS,OSVOS-S,premvos,onavos} and additionally optimize learning rates for each neuron of the model.
In contrast to a single learning rate for the entire model~\cite{how_train_maml} or millions for all of its parameters~\cite{MVOS}, this allows for an ideal balance between individual learning behavior and additional trainable parameters.

    \noindent{\bf Optimization of Model with Object Detection}
To account for the foreground-background pixel imbalance and the challenging object discrimination by individual fine-tuning, previous fine-tuning methods~\cite{OSVOS-S,premvos,onavos} rely on additional mask proposal or bounding box prediction methods.
In contrast, we directly fine-tune Mask R-CNN~\cite{MaskRCNN} with its separate end-to-end trainable object detection head which limits mask predictions to local object bounding boxes.





    This leads to our {\it efficient one-shot video object segmentation} (e-OSVOS) approach, which achieves state-of-the-art segmentation performance on the DAVIS-2016, DAVIS-2017, and YouTube-VOS benchmarks compared to all previous fine-tuning methods, at a much lower test runtime, see Figure~\ref{fig:teaser}.
Overall, our results combat the negative preconceptions with respect to fine-tuning as a principle for semi-supervised VOS, and are intended to motivate future research in this direction.




\subsection{Related Work}
We categorize VOS methods by their application of one-shot fine-tuning for semi-supervised VOS.

    \noindent{\bf Without Fine-Tuning}
Several methods~\cite{BlazinglyFV, VideoMatch,voigtlaender2019feelvos} pose VOS as the task of pixel retrieval in the learned embedding space.
After the embedding learning, no fine-tuning is necessary during the inference -- pixels are simply their respective nearest neighbors in the learned embedding space~\cite{BlazinglyFV, VideoMatch} or used as a guide to the segmentation network~\cite{voigtlaender2019feelvos}.
Other methods propagate segmentation masks using optical flow or point trajectories~\cite{Cheng_ICCV_2017, 8325298, RANet} or segment, propagate and combine object parts~\cite{Cheng_favos_2018}.
The authors of~\cite{oh2018fast} propagate and decode segmentation masks based on the first- and query-frame embeddings.
STM~\cite{STM_19} leverages a memory network to capture the information of the object in the past frames which is then decoded to predict the current frame mask.
They achieve state-of-the-art performance but fail to capture small objects and require a large GPU memory for sequences with many objects.


    \noindent{\bf With Fine-Tuning}
The concept of fine-tuning for semi-supervised VOS was first introduced in OSVOS~\cite{OSVOS}.
This family of methods fine-tunes a pre-trained segmentation model to the first frame ground truth mask of a given object and predicts segmentation masks for the remaining video frames.
OnAVOS~\cite{onavos} extends this approach by adapting the target appearance model online on consecutive frames using heuristics-based fine-tuning policies.
While conceptually elegant, the aforementioned methods have no notion of individual objects, shapes, or motion consistency.
To remedy this issue, OSVOS-S~\cite{OSVOS-S} and PReMVOS~\cite{luiten2018premvos} leverage object detection and instance segmentation methods (\eg, MaskRCNN~\cite{MaskRCNN}) during the inference as additional object guidance cues.
This approach is akin to the~\textit{tracking-by-detection} paradigm, commonly followed in the multi-object tracking community.
Fine-tuning methods for VOS all share one major drawback -- the online fine-tuning process requires extensive manual hyperparameter search and so far numerous training iterations during the inference (up to 1000 in the original OSVOS~\cite{OSVOS} method).
Hence, recent methods refrain from such optimization at test time due to its impact on the runtime.

    \noindent{\bf Towards Efficient Fine-Tuning}
        Ideally, we would like to learn an appearance model and perform as-few-as-possible training steps to during the inference.
One viable approach consists of posing the video object segmentation task as a meta learning problem and optimizing the fine-tuning policies (\eg, generic model initialization, learning rates, and the number of fine-tuning iterations).
The first attempt in this direction, MVOS~\cite{MVOS}, proposes to learn the initialization and learning rates per model parameter.
However, this approach is impractical for modern large-scale detection/segmentation networks.
In this paper, we revisit the concept of meta learning for VOS and propose several critical design choices which yield state-of-the-art results and vastly outperform MVOS~\cite{MVOS} and other fine-tuning methods.

    \noindent{\bf Meta Learning for Few-Shot Learning.}
Previous works have addressed analogous issues for image classification. The authors of MAML~\cite{MAML} propose to learn the model initialization for an optimal subsequent fine-tuning at test time.
Such initialization is supposed to benefit the fine-tuning beyond the traditional transfer learning which merely internalizes the training data.
The MAML++~\cite{how_train_maml} and Meta-SGD~\cite{Meta-SGD} approaches suggest several improvements to MAML and compliment the model initialization by learning the optimal learning rate.
However, both approaches limit their potential by optimizing only a single global learning rate for the entire model.
The authors of~\cite{memorization} conduct an analysis of the meta learning for few-shot scenarios problem and address the memorization problem with a specifically tailored loss function.
Other approaches, such as~\cite{Ravi2017OptimizationAA}, suggest to not only predict the learning rate but apply a parameterized model to predict the entire update step.
However, these approaches so far are limited in their applicability to large-scale neural networks.
 

\section{One-Shot Fine-Tuning for Video Object Segmentation} \label{sec:osvos}

    For a given video sequence $n$ with $I_n$ image frames $\{\mathbf{x}_i^n : 0 \leq i < I_n \}$ and $K_n$ objects, video object segmentation (VOS) predicts individual object masks $\mathbf{y}_i^{n,k}$ of all frames $\mathbf{x}_i^n$.
In the case of semi-supervised VOS, the ground truth mask $\mathbf{\hat{y}}_0^{n,k}$ of a single frame is provided at test time for each object.
For simplicity, we assume that the given frame always corresponds to the first frame $i=0$ of the video.
However, potentially a video might contain multiple objects entering the sequence at different frames.
The common approach for one-shot fine-tuning of a segmentation model $f$ and its parameters $\boldsymbol \theta_f$ follows the three-step optimization pipeline presented in~\cite{OSVOS}:
(i) {\it Base network:} Learn general object features by training the feature extractor backbone of $f$ on a large scale image recognition challenge, e.g. ImageNet~\cite{imagenet}.
(ii) {\it Parent network:} Train $f$ on a segmentation dataset, e.g., DAVIS-17 training set~\cite{DAVIS17semi}, to learn the foreground-background segmentation problem.
(iii) {\it Fine-tuning:} Learn object and sequence-specific features by separately fine-tuning the parent network to each object of a given video sequence.
It should be noted that one-shot learning by nature runs full-batch updates, hence iteration and epoch are often used interchangeably.
For a sequence $n$, the fine-tuning yields $K_n$ separately trained models $f^{n,k}$ with parameters $\boldsymbol \theta_f^{n,k}$.
The final object masks $\mathbf{y}_i^{n,k} = f^{n,k}(\mathbf{x}_i^n)$ are obtained from the maximum over the predicted pixel probabilities over all objects $K_n$.
The steps (ii) and (iii) minimize the segmentation loss $\mathcal{L}_{seg}(\mathcal{D}, \boldsymbol \theta_f)$, e.g., binary cross-entropy, of the model $f$ on a given training dataset $\mathcal{D}$.
For clarity, we omit the sequence and object indices on $f$ and $\boldsymbol \theta_f$ in future references and refer to a problem solved by an optimization $g$ as:
\begin{align}
        \boldsymbol \theta_f^{g} = \operatorname*{argmin}_{\boldsymbol \theta_f \text{ with } {g}} \mathcal{L}_{seg}(\mathcal{D}, \boldsymbol \theta_f)
    \end{align}
Such optimization is defined by several hyperparameters, including, the model initialization, number of training iterations and the type of stochastic gradient descent (SGD) method as well as its learning rate(s).

\section{Efficient One-Shot Video Object Segmentation} \label{sec:eosvos}

    We describe the key design choices of e-OSVOS, namely, the model choice, meta learning of the fine-tuning optimization, and two additional test-time modifications to further enhance our performance.

\subsection{Optimization of Model with Object Detection} \label{sec:mask_rcnn}

    Fine-tuning a fully-convolutional model for VOS suffers from two major issues: (i) the imbalance between foreground and background pixels and (ii) the challenging object discrimination by individual fine-tuning.
Typically, the latter requires an \textit{unlearning} of potential false positive pixels.Several fine-tuning approaches~\cite{OSVOS-S,premvos,onavos} tackle these issues by including separate mask proposal or bounding box prediction methods.
We propose to directly fine-tune Mask R-CNN~\cite{MaskRCNN} which decouples the object detection and requires demanding pixel-wise segmentation only to bounding boxes.


    Mask R-CNN consists of a feature extraction backbone, a Region Proposal Network (RPN) and two network heads, namely, the bounding box object detection and mask segmentation heads.
The RPN produces potential bounding box candidates, also known as proposals, on the intermediate feature representation provided by the backbone.
The box detection head predicts the object class and regresses the final bounding boxes for each proposal. Finally, the segmentation head provides object masks for each object class and bounding box.
The segmentation loss mentioned in Section~\ref{sec:osvos} corresponds to the multi-task Mask R-CNN loss: $\mathcal{L}_{seg} = \mathcal{L}_{RPN} + \mathcal{L}_{box} + \mathcal{L}_{mask}$.


    We adapt Mask R-CNN for the VOS task by replacing the pixel-wise cross-entropy $\mathcal{L}_{mask}$ loss with the Lov{\'a}sz-Softmax~\cite{berman2018lovasz} loss.
The Lov{\'a}sz-Softmax loss directly optimizes the intersection-over-union and demonstrates superior performance in our one-shot fine-tuning setting.
In contrast to commonly applied batch normalization, group normalization~\cite{groupnorm} allows for fine-tuning even on single sample (frame) batches.
Therefore, we replace all normalization layers of the backbone with group normalization.

\subsection{Meta Learning the One-Shot Test Time Optimization}

    As outlined by~\cite{Ravi2017OptimizationAA}, meta learning is of particular interest for semi-supervised or few-shot learning scenarios. In this work, we extend this idea from image classification to VOS and meta learn steps (ii) and (iii) of the optimization pipeline from Section~\ref{sec:osvos}.
To this end, we learn differentiable components of the test time optimization, specifically, the model initialization and SGD learning rate(s).


    \subsubsection{Meta Tasks}
In order to meta learn the optimization, we formulate the VOS fine-tuning problem as a meta task.
A task represents the fine-tuning optimization on a single object of a video sequence.
Given a set of $N$ unique video sequences each with $K_n$ objects, we define the corresponding taskset $\mathcal{T} = \{T_{n, k} : 0 \leq k < K_n | 0 \leq n < N \}$ with $T_{n,k} = \{\mathcal{D}_{train}^{n, k}, \mathcal{D}_{test}^{n, k} \}$.
As illustrated in Figure~\ref{fig:meta_tasks}, an individual task is created by splitting each sequence into a training and test dataset consisting of disjoint sets of video frames.
The goal of task $T_{n, k}$ is to minimize the test loss $\mathcal{L}_{seg}(\mathcal{D}_{test}^{n, k}, f)$ of the model $f$.
The datasets $\mathcal{D}_{train}^{n, k} = \{\mathbf{x}_0^n, \mathbf{\hat{y}}_0^{n, k} \}$ and $\mathcal{D}_{test}^{n, k} = \{ \{\mathbf{x}_i^n, \mathbf{\hat{y}}_i^{n, k} \} : 1 \leq i < I_n \}$ include the first and all consecutive frames, respectively.
We train e-OSVOS on the
         $\mathcal{T}_{train}$ such that the fine-tuning optimization on any $\mathcal{D}_{train}^{n, k}$ yields optimal results on the corresponding $\mathcal{D}_{test}^{n, k}$.
This involves two optimizations, namely, the inner fine-tuning and outer meta optimization.
As for all machine learning methods, the final generalization to the test taskset $\mathcal{T}_{test}$ is paramount.
In future references of the datasets $\mathcal{D}_{train}$ and $\mathcal{D}_{test}$, we again omit the sequence and object indices $n, k$.

        \begin{figure*}[ht]
    \centering
    \hspace*{-0.22cm}
    \subfloat[Meta Taskset]{
        \label{fig:meta_tasks}
        \includegraphics[width=0.49\textwidth]{figures/meta_tasks.pdf}}
    \subfloat[Meta Optimization]{
        \label{fig:meta_optimization}
        \includegraphics[width=0.49\textwidth]{figures/meta_optimization_img.pdf}}
\caption{The test time optimization $g$ of e-OSVOS is meta learned on a VOS taskset structured as in \protect\subref{fig:meta_tasks}.
Each task represents a video sequence with its frames split into training $\mathcal{D}_{train}^n$ and test $\mathcal{D}_{test}^n$ datasets.
The optimization $g$ depicted in \protect\subref{fig:meta_optimization} consists of the model initialization and a set of learning rates applied with vanilla stochastic gradient descent.
Both of which are meta learned by backpropagation of the final test loss $\mathcal{L}_{seg}(\mathcal{D}_{test}, \boldsymbol \theta_f^T)$.
}
\end{figure*} 
    \subsubsection{Meta Optimization}
In analogy to \textit{How to train your MAML}~\cite{how_train_maml}, our test time optimization consists of a vanilla SGD with two trainable components, namely, the initialization of the model $f$ and learning rates $\boldsymbol \lambda$ which are applied for a fixed number of iterations.
We refer to the trainable parameters of such an optimization $g$ with $\boldsymbol \theta_g$.
Learning a single task involves the following bi-level optimization problem for $\boldsymbol \theta_g$ and $\boldsymbol \theta_f$:
\begin{align}
            \boldsymbol \theta_g^* = & \operatorname*{argmin}_{\boldsymbol \theta_g} \mathcal{L}_{seg}(\mathcal{D}_{test}, \boldsymbol \theta_f^g) \label{eq:bilevel1},\\
\text{s.t.} \,\ \boldsymbol \theta_f^g = & \operatorname*{argmin}_{\boldsymbol \theta_f \text{ with } g} \mathcal{L}_{seg}(\mathcal{D}_{train}, \boldsymbol \theta_f) \label{eq:bilevel2}.
        \end{align}
The outer optimization in Equation~\eqref{eq:bilevel1} is handcrafted and performed on batches of tasks from $\mathcal{T}_{train}$.
The bi-level optimization aims to maximize the generalization from a given training to its corresponding test dataset.
In practice, one step of Equation~\eqref{eq:bilevel1} includes multiple steps in Equation~\eqref{eq:bilevel2}.
This corresponds to fine-tuning the model $f$ for multiple iterations on the first frame $\mathcal{D}_{train}$. The optimization $g$ is trained by \textit{Backpropagation Through Time} (BPTT) of the test loss after $T$ training iterations:
\begin{align}
\mathcal{L}_{BPTT} = & \mathcal{L}_{seg}(\mathcal{D}_{test}, \boldsymbol \theta_f^T) \label{eq:bptt1}, \\
\text{with} \,\ \boldsymbol \theta_f^{t+1} = & \,\ g(\nabla_{\boldsymbol \theta_f^t} \mathcal{L}_{seg}(\mathcal{D}_{train}, \boldsymbol \theta_f^t), \boldsymbol \theta_f^t) \label{eq:bptt2}.
        \end{align}
As illustrated in Figure~\ref{fig:meta_optimization}, $g$ connects the computational graph of each iteration over time.
The optimization applies a gradient descent step with respect to $\mathcal{D}_{train}$ and updates $f$.
To this end, the optimization receives the current model parameters $\boldsymbol \theta_f^t$ and their gradients $\nabla_{\boldsymbol \theta_f^t} \mathcal{L}_{seg}(\mathcal{D}_{train}, \boldsymbol \theta_f^t)$.
After $T$ updates, the optimization itself is updated to minimize $\mathcal{L}_{BPTT}$ with respect to the updated model $f$.
As Equation~\eqref{eq:bptt2} already requires the computation of model parameter gradients, the outer backpropagation of $\mathcal{L}_{BPTT}$ introduces second order derivatives. To reduce the computational effort, these can be omitted which is equivalent to ignoring the dashed edges of the graph in Figure~\ref{fig:meta_optimization}.




    \subsubsection{Learning the Segmentation Model Initialization}
Meta learning the model initialization for a subsequent optimization (fine-tuning) yields superior performance compared to classic transfer learning approaches (parent network training).
The initialization not only internalizes the data of the tasks, but benefits the subsequent fine-tuning step.
Previous works~\cite{MAML,Meta-SGD,MVOS}, have applied this successfully to few-shot image classification.
For semi-supervised VOS, meta learning provides a model initialization $\boldsymbol \theta_f^0$ for the fine-tuning optimization in Equation~\eqref{eq:bptt2}.
Such an initialization avoids biases for specific objects and eases the individual fine-tuning to each object significantly.
In addition to the classic overfitting to $\mathcal{T}_{train}$, meta learning is prone to zero-shot collapsing, also called the \textit{memorization problem} \cite{memorization}.
For image classification, this is avoided by randomly shuffling the class labels for each training task.
For multi-object VOS we tackle the issue of zero-shot collapsing by separating objects of the same sequence to multiple tasks.
The first two example tasks in Figure~\ref{fig:meta_tasks} demonstrate the necessity of a one-shot optimization for segmenting different objects given the same input image.

    \subsubsection{Learning Neuron-Level Learning Rates}
The optimization $g$ performs Equation~\eqref{eq:bptt2} with a vanilla SGD step and updates the segmentation model by applying a set of meta learned learning rates $\boldsymbol \lambda$ for a fixed number of iterations.
The entire set of trainable optimization parameters is denoted as $\boldsymbol \theta_g = \{\boldsymbol \theta_f^0, \boldsymbol \lambda\}$. Previous meta learning for few-shot approaches applied learning rates for different parameter hierarchy levels, from a single global learning rate for the entire model in~\cite{Meta-SGD}, to learning rates for all model parameters $\boldsymbol \theta_f$ in MVOS~\cite{MVOS}.
The latter is unfeasible for many modern state-of-the-art segmentation networks as it effectively doubles the number of trainable optimization parameters ($|\boldsymbol \theta_g| \approx 2|\boldsymbol \theta_f^0|$).


        Therefore, we propose an ideal balance between individual learning behavior and additional trainable parameters by optimizing a set of learning rates at the neuron level.
A common linear neural network layer consists of multiple neurons, or kernels for convolution layers, where each neuron applies a weight tensor and corresponding scalar bias.
We predict a pair of learning rates for each neuron of the model $f$, i.e., a single rate for each weight tensor and bias scalar.
The amount of additional trainable parameters is neglectable for modern segmentation models as their total number of parameters typically exceeds~\num{e7}.
In Algorithm~\ref*{alg:meta_training} of the supplementary, we illustrate the full e-OSVOS training pipeline for a given VOS taskset $\mathcal{T}_{train}$.

\subsection{Online Adaption and Bounding Box Propagation} \label{sec:online_adaptation}
    By nature, fine-tuning methods are prone to overfit on the given single frame dataset $\mathcal{D}_{train}^{n, k} = \{\mathbf{x}_0^n, \mathbf{\hat{y}}_0^{n, k} \}$.
For sequences with changing object appearance or new similar objects entering the scene, such overfitting often results in degrading recognition performance or drifting of the segmentation mask.
However, e-OSVOS incorporates two test time techniques to overcome those problems.

  {\bf Online adaptation}
Inspired by~\cite{onavos}, we apply an online adaptation (OnA) which continuously fine-tunes the segmentation model on both the given first frame ground truth and past mask predictions.
First, we fine-tune the model for $T$ iterations only on the first frame which yields $\boldsymbol \theta_f^T$ and then continue the fine-tuning every $I_{OnA}$ frames for $T_{OnA}$ additional iterations on the combined online dataset $\mathcal{D}_{train}^{n, k} = \{\mathbf{x}_0^n, \mathbf{\hat{y}}_0^{n, k} \} \cup \{\mathbf{x}_{i}^n, \mathbf{y}_{i}^{n, k} \}$.
In contrast to~\cite{onavos}, our efficient test time optimization allows for a reset of the model before every additional fine-tuning to the first-frame model state $\boldsymbol \theta_f^T$.
Such a reset avoids the accumulation of false positive pixels wrongly considered as ground truth.
Our learned optimization $g$ generalizes to such an online adaptation without any additional meta learning.


   \noindent{\bf Bounding Box Propagation}
In analogy to~\cite{tracktor_2019_ICCV}, we extend the RPN proposals with the detected object boxes of the previous frame.
To account for the changing position of the object, we augment the previous boxes with random spatial transformations.
Starting with the first frame ground truth boxes, the frame-to-frame propagation facilitates the tracking of each object over the sequence.
 \section{Experiments}

    We demonstrate the applicability of e-OSVOS on three semi-supervised VOS benchmarks, namely, DAVIS 2016~\cite{DAVIS16}, DAVIS 2017~\cite{DAVIS17semi}, and YouTube-VOS~\cite{Xu2018YouTubeVOSAL}.
The tasksets $\mathcal{T}$ for training and evaluation of e-OSVOS are constructed from the corresponding training, validation, and test video sequences of each benchmark.

\subsection{Datasets and Evaluation Metrics}
    \noindent{\bf DAVIS 2016}
        The DAVIS 2016~\cite{DAVIS16} benchmark consists of a training and validation set with 30 and 20 single object video sequences, respectively.
Every sequence is captured at 24 frames per second (FPS) and semi-supervision is achieved by providing the respective first frame object mask.


    \noindent{\bf DAVIS 2017}
        The DAVIS 2017~\cite{DAVIS17semi} benchmark extends DAVIS-16 with 100 additional sequences including dedicated test-dev and test sets.
The validation, test-dev, and test set each consist of 30 sequences.
The extended train set contains the remaining 60 video sequences.
Furthermore, DAVIS 2017 contains a mix of single and multi-object sequences with varying image resolutions.

    \noindent{\bf YouTube-VOS}
        Our largest benchmark, YouTube-VOS~\cite{Xu2018YouTubeVOSAL}, consists of 4453 video sequences including dedicated test and validation sets with 508 and 474 sequences, respectively.
As DAVIS 2017, this benchmark contains single and multi-object sequences in multiple resolutions but provides segmentation ground truth only at 6 FPS.
In general, ~\cite{Xu2018YouTubeVOSAL} requires stronger tracking capabilities as objects enter in the middle of the sequence or leave and reenter the frame entirely.

    \noindent{\bf Evaluation Metrics}  \label{sec:eval_metrics}
We evaluate the standard VOS metrics defined by~\cite{DAVIS16}.
For the intersection over union (IoU )between predicted and ground truth masks, also known as Jaccard index $\mathcal{J}$ in \%, we evaluate the mean as well as decay over the sequence.
Furthermore, we report the mean contour accuracy $\mathcal{F}$ in \%, the mean combination metric $\mathcal{J} \& \mathcal{F}$ in \% and the frames per second (FPS) in Hz.




\subsection{Implementation Details}  \label{sec:imp_details}

    For all experiments, we apply a Mask R-CNN with ResNet50~\cite{DBLP:journals/corr/HeZRS15} and FPN~\cite{FPN_2017_CVPR} pre-trained on the COCO~\cite{ms_coco} segmentation dataset.
In order to optimize the learning rates and model initialization jointly without overfitting, we follow previous VOS approaches such as ~\cite{voigtlaender2019feelvos,stm} and train e-OSVOS on YouTube-VOS combined with DAVIS 2017.
To improve generalization, we construct training tasks $T_{n, k} \in \mathcal{T}_{train}$ by randomly sampling a single frame from a sequence and augmenting it with spatial and color transformations for each train and test dataset.
Furthermore, for both DAVIS datasets, we fine-tune the meta learning of the model initialization for each dataset while keeping the previously learned learning rates fixed.
For the outer optimization we apply RAdam~\cite{liu2019radam} with a fixed learning rate $\beta$, as shown in Algorithm~\ref*{alg:meta_training} of the supplementary, on batches of 4 training tasks each distributed to a Quadro RTX 6000 GPU for a total of 4 days.
To limit the computational effort, we ignore second order derivatives and fine-tune for $T=5$ BPTT iterations. The learning rates are clamped to be non-negative after each meta update.

    The online adaptation (OnA) is applied every $I_{OnA}=5$ steps for $T_{OnA}=10$ iterations.
To further boost inner-sequence generalization, we apply spatial random transformations as in~\cite{OSVOS} during the initial fine-tuning but not for the online adaptation.
While the iterations are fixed to $T=5$ during the meta learning e-OSVOS generalizes to varying numbers of iterations and the online adaptation without any further learning.
To indicate different versions of e-OSVOS, we denote the application of online adaptation and the number of initial fine-tuning iterations.

\subsection{Ablation Study}

    \begin{table*}
    \caption{
        \textbf{Ablation study} of each e-OSVOS component on the DAVIS 2017 validation set.
The first row represents a handcrafted equivalent of our test time optimization.
We present performance gains componentwise for 10 fine-tuning iterations and iteration-wise for our final e-OSVOS version.
}
    \label{tab:ablation_study}
    \centering
    \resizebox{0.6\columnwidth}{!}{
    \begin{tabular}{lcclH}
        \toprule
        Method & Iterations ($T$) & \multicolumn{2}{c}{$\mathcal{J}\&\mathcal{F} \uparrow$} & FPS $\uparrow$ \tabularnewline
        \midrule

        \multirow{4}{*}{\makecell[l]{Mask R-CNN \\ + parent training + single LR search}}     & 10 & $33.6$ & & 9.09\tabularnewline
                                                                                  & 50 & $39.7$ & & 4.55\tabularnewline
                                                                                  & 100 & $41.6$ & & 2.94\tabularnewline
                                                                                  & 1000 & $42.7$ & & 0.44\tabularnewline
        \midrule
        Mask R-CNN && \tabularnewline
        + Learn model initialization and single LR                     & 10 & $64.4$ & + 30.8 & 9.09 \tabularnewline
        + Learn neuron level learning rates                                       & 10 & $67.2$ & + 2.8 & 8.74  \tabularnewline
        + Group normalization + Lov{\'a}sz-Softmax                           & 10 & $69.4$ & + 2.2  & 8.33  \tabularnewline

        \midrule

        + Bounding box propagation (e-OSVOS)                                      & 10 & $69.9$ & + 0.5 & 8.30\tabularnewline
        + Online adaption (e-OSVOS-OnA)                                           & 10 & $71.2$ & + 1.3 & 1.75 \tabularnewline

        \midrule

\multirow{2}{*}{e-OSVOS-$T$}                            & 50 & $71.3$ & & 5.26\tabularnewline
                                                            & 100 & $71.2$ & & 3.45\tabularnewline

        \midrule

\multirow{2}{*}{e-OSVOS-$T$-OnA}                   & 50 & $73.7$\tabularnewline
                                                            & 100 & $74.8$\tabularnewline













\bottomrule
    \end{tabular}
    }
\end{table*}     \begin{figure}[t]
    \centering
    \caption{
We evaluate e-OSVOS for increasing~\textbf{number of initial fine-tuning iterations} $T$ on the DAVIS 2017 validation set.
The first iterations yield the largest performance gains while still running at comparatively large frames per second rates.
}
    \includegraphics[width=0.6\textwidth]{figures/e-OSVOS-T_img.pdf}
    \label{fig:e-OSVOS-T}
\end{figure} 


    We demonstrate the effect of the individual e-OSVOS components on the DAVIS 2017 validation set in Table~\ref{tab:ablation_study}.
Both the parent and meta training utilize the combined dataset of YouTube-VOS and DAVIS 2017.
For a fair comparison between a varying number of fine-tuning iterations, we refrained from any spatial random transformations at test time.
The first row shows a handcrafted equivalent of the e-OSVOS test time optimization for which we apply a grid search to find the optimal global fine-tuning learning rate.
Note, this baseline is not representative for state-of-the-art fine-tuning VOS approaches as we omitted any additional handcrafted test time improvements~\cite{OSVOS,OSVOS-S,premvos,onavos}, such as, layer-wise learning rates, learning rate scheduling, contour snapping.
The handcrafted approach is inferior to meta learning the initialization and a single global learning rate even for substantially more iterations.
The neuron-level learning rates and additional modifications to the Mask R-CNN motivated in Section~\ref{sec:eosvos} both yield substantial segmentation performance gains.
While the improvement from bounding box propagation is comparatively small, it only adds an insignificant amount of additional runtime.
The marginal improvement from e-OSVOS-50 to e-OSVOS-100 motivates the application of an online adaption to combat overfitting to the first frame and degrading performance over the course of the sequence.
In Figure~\ref{fig:e-OSVOS-T}, we further demonstrate the efficiency of e-OSVOS on the DAVIS 2017 validation set.
The meta learning enables large gains in segmentation performance after only a few fine-tuning iterations without suffering from low frames per second rates.
With an increasing number of iterations, the actual inference time of the sequence becomes neglectable.

\subsection{Benchmark Evaluation}

    \begin{table*}
    \centering
\caption{VOS performance evaluation on the \textbf{DAVIS 2016 and 2017} benchmarks.
We categorize methods by their application of fine-tuning (FT) and post-processing (PP) of the predicted masks and label methods with an ensemble of models with $\dagger$.
The table is ordered by ${\mathcal{J}}\&\mathcal{F}$ on DAVIS 2017 validation.
The evaluation metrics are detailed in Sec.~\ref{sec:eval_metrics}.
If not publicly available we adopted the runtime (FPS) from~\cite{meta_words}.
    }
\resizebox{1.0\textwidth}{!}{
    \begin{tabular}[t]{lcc cccHc cccc cccc}
        \toprule
        &  &  & \multicolumn{5}{c}{DAVIS 2016 - validation} & \multicolumn{4}{c}{DAVIS 2017 - validation} & \multicolumn{4}{c}{DAVIS 2017 - test-dev} \\
\cmidrule(lr){4-8} \cmidrule(lr){9-12} \cmidrule(lr){13-16}
Method                              & FT & PP & ${\mathcal{J}} \uparrow$ & ${\mathcal{J}}$ Decay $\downarrow$ & ${\mathcal{F}}\uparrow$ &  ${\mathcal{J}}\&\mathcal{F} \uparrow$ & FPS $\uparrow$ & ${\mathcal{J}}\uparrow$ & ${\mathcal{J}}$ Decay $\downarrow$ & ${\mathcal{F}}\uparrow$ &  ${\mathcal{J}}\&\mathcal{F}\uparrow$ & ${\mathcal{J}}\uparrow$ & ${\mathcal{J}}$ Decay $\downarrow$ & ${\mathcal{F}}\uparrow$ &  ${\mathcal{J}}\&\mathcal{F}\uparrow$\\

        \cmidrule(r){1-16} \cmidrule(lr){4-8} \cmidrule(lr){9-12} \cmidrule(lr){13-16}
FAVOS~\cite{Cheng_favos_2018}        & &$\times$ & 82.4 & \textbf{4.5}  & 79.5 & 80.9 & 0.56 & 54.6 & 14.1  & 61.8 & 58.2 & 42.9 & 18.1 & 44.2 & 43.6 \\
RGMP~\cite{oh2018fast}                  & & & 81.5 & 10.9  & 82.0 & 81.7 & \textbf{7.70} & 64.8 & 18.9 & 68.6 & 66.7 & 51.3 & 34.3 & 54.4 & 52.8 \\
        RVOS~\cite{oh2018fast}                  & & & -- & --  & -- & -- & -- & 57.5 & 24.9 & 63.6 & 60.6 & 47.9 & 35.7 & 52.6 & 50.3\\
        MetaVOS~\cite{meta_words}               & & & 81.5 & 5.0  & 82.7 & 82.1 & 4.0 & 63.9 & 14.4 & 70.7 & 67.3  &--&--&--&--\\
        RANet~\cite{Ziqin2019RANet}             & & & 86.6 & 7.4 & 87.6 & 87.1 & 30.3 & 63.2 & 18.6 & 68.2 & 65.7 & 53.4 & 21.9 & 57.3 & 55.4\\
        FEELVOS~\cite{voigtlaender2019feelvos}  & & & 81.1 & 13.7  & 82.2 & 81.7 & 2.22 & 69.1 & 17.5 & 74.0 & 71.5  & 55.1 & 29.8 & 60.4 & 57.8\\
        MHP-VOS~\cite{xu2019mhp}                & & & 87.6 & 6.9  & 89.5 & 88.6 & 0.01 & 73.4  & 17.8 & 78.9 & 76.1  & 66.4 & 18.0 & 72.7 & 69.5\\
        STM~\cite{STM_19}                       & & & \textbf{88.7} & 5.0  & \textbf{90.1} & \textbf{89.4} & 6.25 & \textbf{79.2} & \textbf{8.0} & \textbf{84.3} & \textbf{81.7} & \textbf{69.3} & \textbf{16.9} & \textbf{75.2} & \textbf{72.2}\\
\cmidrule(r){1-16}
$\textrm{CINM}^{\dagger}$~\cite{cinm}                               &$\times$ &$\times$ & 83.4 & 12.3  & 85.0 & 84.2 & 0.01 & 67.2 & 24.6 & 74.4 & 70.7 & 64.5 & 20.0 & 70.5 & 67.5\\
        Lucid~\cite{lucid}                                                  &$\times$ &$\times$ &83.9 & 9.1  & 82.0 & 83.0 & 0.005 & -- & --  & -- & -- & 63.4 & 19.5 & 69.9 & 66.6\\
        MVOS~\cite{MVOS}                                                &$\times$ &$\times$ & 83.3 & --  & 84.1 & -- & 4.0 & 56.3 & --  & 62.1 & 59.2  &--&--&--&--\\
        OSVOS~\cite{OSVOS}                                                &$\times$ &$\times$ & 79.8 & 14.9  & 80.6 & 80.2 & 0.11 & 56.6 & 26.1  & 63.9 & 60.3 & 47.0 & 19.2 & 54.8 & 50.9\\
        $\textrm{OSVOS-S}^{\dagger}$~\cite{OSVOS-S}                       &$\times$ &$\times$ & 85.6 & 5.5  & 87.5 & 86.5 & 0.22 & 64.7 & 15.1  & 71.3 & 68.0 & 52.9 & 24.1  & 62.1 & 57.5\\
        OnAVOS~\cite{onavos}                                                &$\times$ &$\times$ & 86.1 & 5.2  & 84.9 & 85.5 & 0.08 & 61.6 & 27.9  & 69.1 & 65.3 & 49.9 & 23.0 & 55.7 & 52.8\\

$\textrm{PReMVOS}^{\dagger}$~\cite{luiten2018premvos}   &$\times$ & & 84.9 & 8.8  & {\bf 88.6} & {\bf 86.8} & 0.01 & 73.9 & 16.2 & \textbf{81.8} & {\bf 77.8} & \textbf{67.5} & \textbf{21.7} & \textbf{75.8} & \textbf{71.6}\\

        \cmidrule(r){1-3} \cmidrule(lr){4-8} \cmidrule(lr){9-12} \cmidrule(lr){13-16}

        e-OSVOS-10                       & $\times$ & & 85.1 & 5.0  & 84.8 & 84.9 & \textbf{5.3} & 69.2  & 18.5 & 74.6 & 71.9 &--&--&--&--\\
        e-OSVOS-50                       & $\times$ & & 85.5 & 5.0  & 85.8 & 85.6 & 1.64 & 70.7  & 18.6 & 75.9 & 73.3  &--&--&--&--\\

        \cmidrule(r){1-3} \cmidrule(lr){4-8} \cmidrule(lr){9-12} \cmidrule(lr){13-16}

        e-OSVOS-50-OnA                   & $\times$ & & 85.9 & 5.2  & 85.9 & 86.0 & 0.35 & 73.0  & 13.6 & 78.3 & 75.6 & 60.9 & 22.1 & 68.6 & 64.8\\
        e-OSVOS-100-OnA                   & $\times$ & & \textbf{86.6} & \textbf{4.5}  & 87.0 & XXX & 0.29 & \textbf{74.4}  & \textbf{13.0} & 80.0 & 77.2  &--&--&--&--\\
        \bottomrule
    \end{tabular}
    }
\label{tab:comparison_all}
\end{table*}     \begin{table*}
    \centering
\caption{
        VOS performance evaluated on the \textbf{YouTube-VOS} validation set.
This benchmark additionally evaluates the performance on completely unseen object classes.
Results of other methods are copied from~\cite{STM_19}.
}
\resizebox{0.8\columnwidth}{!}{
    \begin{tabular}[t]{lcc ccccc }
        \toprule
        &  &  & \multicolumn{5}{c}{YouTube-VOS - validation} \\
\cmidrule(lr){4-8}
Method                              & FT & PP & Overall $\uparrow$ & ${\mathcal{J}}$ Seen $\uparrow$ & ${\mathcal{F}}$ Seen $\uparrow$ &  ${\mathcal{J}}$ Unseen $\uparrow$ & ${\mathcal{F}}$ Unseen $\uparrow$ \\

        \cmidrule(r){1-3} \cmidrule(lr){4-8}
OSMN~\cite{osmn}                  & & & 51.2 &60.0 &60.1 &40.6 &44.0 \\
        MSK~\cite{msk}                  & & & 53.1 & 59.9 & 59.5 & 45.0 & 47.9 \\
        RGMP~\cite{oh2018fast}                  & & & 53.8 & 59.5 & -- & 45.2 & -- \\
        RVOS~\cite{rvos}             & & & 56.8 & 63.6 & 67.2 & 45.5 & 51.0  \\
        S2S~\cite{s2s}                          & & & 64.4 & 71.0 & 70.0 & 55.5 & 61.2 \\
        A-GAME~\cite{agame}                & & & 66.1 & 67.8 & -- & 60.8 & -- \\
        STM~\cite{STM_19}                       & & & \textbf{79.4} & \textbf{79.7} & \textbf{84.2} & \textbf{72.8} & \textbf{80.9}  \\
\cmidrule(r){1-3} \cmidrule(lr){4-8}
OnAVOS~\cite{onavos}                                                &$\times$ &$\times$ & 55.2 & 60.1 & 62.7 & 46.6 & 51.4  \\
OSVOS~\cite{OSVOS}                                                &$\times$ &$\times$ & 58.8  & 59.8  & 60.5  & 54.2 & 60.7  \\
$\textrm{PReMVOS}^{\dagger}$~\cite{luiten2018premvos}   &$\times$ & & 66.9 & 71.4  & \textbf{75.9} & 56.5 & 63.7 \\

        \cmidrule(r){1-3} \cmidrule(lr){4-8}
e-OSVOS-50-OnA                     & $\times$ & & \textbf{71.4} & \textbf{71.7}  & 66.0 & \textbf{74.3} & \textbf{73.8} \\
        \bottomrule
    \end{tabular}
    }
\label{tab:youtube}
\end{table*} 
    We present state-of-the-art VOS results for fine-tuning methods on DAVIS 2016 and 2017 in Table~\ref{tab:comparison_all} and for YouTube-VOS in Table~\ref{tab:youtube}.
We focus our evaluation on fine-tuning, hence separating the results of methods without fine-tuning (FT).
The overall state-of-the art method STM~\cite{STM_19}, which does not leverage fine-tuning, currently surpasses all existing approaches in terms of performance and runtime.
Nevertheless, we want to motivate fine-tuning as a concept applicable to further boost results of methods like STM without harming its efficiency.

    \noindent{\bf DAVIS 2016 and 2017}
In terms of the important $\mathcal{J}$ metric, we outperform all previous one-shot fine-tuning approaches on the validation set while reducing the runtime multiple orders of magnitude.
It is important to note, that unlike our approach all previous fine-tuning methods rely on post-processing or an ensemble of models to achieve optimal results.
We even surpass PReMVOS~\cite{luiten2018premvos}, the long-time state-of-the-art VOS method, with a much simpler and more efficient fine-tuning approach.
PReMVOS applies an additional contour snapping, as in~\cite{OSVOS}, which explains its superiority in terms of contour accuracy $\mathcal{F}$.
On the test-dev set all methods achieve substantially worse results in all metrics compared to the validation set.
This is due to more sequences with challenging identity preservation scenarios.
We do not achieve state-of-the-art results for fine-tuning methods on the test-dev set.
However, our approach still demonstrates the potential of fine-tuning as we surpass most none-fine-tuning methods without applying any post-processing or an ensemble of methods.

    \noindent{\bf YouTube-VOS}
On the more challenging YouTube-VOS dataset, our approach yields overall better results compared to all previous fine-tuning methods.
In particular, PReMVOS suffers from inferior performance on unseen object classes.
This indicates that our meta learned initialization provides a superior fine-tuning initialization which is less prone to overfitting.
It should be noted that some methods were evaluated on an earlier version of the YouTube-VOS benchmark which causes slight variations in the final results. \section{Conclusion}

This works demonstrates the application of meta learning to VOS fine-tuning and makes one-shot video object segmentation efficient again.
We first motivate our model choice to be a modified Mask R-CNN instead of a fully convolutional segmentation model.
Furthermore, we meta learn the model initialization and a set of neuron-level learning rates.
e-OSVOS works in addition to common test-time techniques which mitigate performance degradation, such as online adaptation with continuous fine-tuning and a bounding box propagation.
We demonstrate the best performance amongst fine-tuning methods, and aspire to reignite research in this promising approach to semi-supervised VOS.
 \clearpage

\ifarxiv
    \pagenumbering{gobble}
    \setcounter{section}{0}
    \setcounter{figure}{0}
    \setcounter{table}{0}
    \setcounter{equation}{0}
    \setcounter{footnote}{0}

    \title{Make One-Shot Video Object Segmentation \\ Efficient Again \\ {\normalfont Supplementary Material}}
    \clearpage
    \maketitle
    \let\maketitle\relax

    \begin{abstract}
The supplementary material complements our work with the training algorithm of our efficient One-Shot Video Object Segmentation (e-OSVOS) and additional implementation as well as training details.
Furthermore, we provide a more detailed comparison to PremVOS, a state-of-the-art fine-tuning method, including selected visual results.
\end{abstract}

\section{Implementation Details}

    In Algorithm~\ref{alg:meta_training} we provide a structured overview of the meta learning algorithm for the e-OSVOS test time optimization.
The optimization is defined by its trainable parameters $\boldsymbol \theta_g = \{\boldsymbol \theta_f^0, \boldsymbol \lambda\}$ consisting of the model initialization and neuron-level learning rates.
Given a taskset $\mathcal{T}_{train}$ of training tasks we sample batches of tasks, fine-tune a model for $T$ iterations, and update $\boldsymbol \theta_g$ to achieve an optimal generalization to the respective test sets $\mathcal{D}_{test}^{n,k}$ of sequence $n$ and object $k$.

    For the sake of completeness and to facilitate the reproduction of our results, we provide additional implementation details to Section~\ref{sec:imp_details} of the main paper.
We do not apply regularization, such as weight decay or dropout, neither during the meta training nor at test time.
To improve segmentation results, we double the default RoIAlign~\cite{MaskRCNN} pooling window size to 28.
As the YouTube-VOS~\cite{Xu2018YouTubeVOSAL} validation dataset does not provide publicly available ground truth, we extract 100 sequences of the training set to monitor our meta learning progress.

    

\begin{algorithm}
    \footnotesize
    \KwInput{
        Optimization $g$ with model initialization and neuron-level learning rates $\boldsymbol \theta_g = \{\boldsymbol \theta_f^0, \boldsymbol \lambda\}$. Meta learning rate $\beta$. Fine-tuning iterations $T$.

}
    \KwData{Taskset $\mathcal{T}_{train}$}
    \KwOutput{$\boldsymbol \theta_g^*$ }
\While{not done}
    {
        Sample tasks $\mathcal{T}_{n,k} = \{\mathcal{D}_{train}^{n,k}, \mathcal{D}_{test}^{n,k} \} \sim \mathcal{T}_{train}$ \\
        \ForAll{$\mathcal{T}_{n,k}$}
        {
\For{$t \gets 0$ \KwTo $T - 1$}
            {
Update $\boldsymbol \theta_f^{t+1} = \,\ g(\nabla_{\boldsymbol \theta_f^t} \mathcal{L}_{seg}(\mathcal{D}_{train}^{n,k}, \boldsymbol \theta_f^t), \boldsymbol \theta_f^t)$ }
            $\mathcal{L}_{BPTT}^{n,k} = \mathcal{L}_{seg}(\mathcal{D}_{test}^{n,k}, \boldsymbol \theta_f^T)$
}
        Update $\boldsymbol \theta_g \leftarrow \boldsymbol \theta_g - \beta \nabla_{\theta_g} \sum_{\mathcal{T}_j}  \mathcal{L}_{BPTT}^{n,k}$ \\
}

    \caption{Meta learning the e-OSVOS test time optimization}
    \label{alg:meta_training}
\end{algorithm}
 
\section{DAVIS 2017 Sequence Analysis}

    

\begin{table*}[t!]
    \centering
    \small
    \caption{VOS performance comparison per sequence on the \textbf{DAVIS 2017 validation} benchmark between e-OSVOS and $\textrm{PReMVOS}$~\cite{luiten2018premvos} which resembles the state-of-the-art for fine-tuning methods.
We present the mean Intersection over Union ${\mathcal{J}}$ averaged over the number of objects indicated in parentheses.
    }
\resizebox{\textwidth}{!}{

    \begin{tabular}[t]{l r *{30}{c}} 
        \toprule
Method                              & 
          \rotatebox[origin=l]{90}{Bike-Packing (2)} &
          \rotatebox[origin=l]{90}{Blackswan} &
          \rotatebox[origin=l]{90}{Bmx-Trees (2)} &
          \rotatebox[origin=l]{90}{Breakdance} &
          \rotatebox[origin=l]{90}{Camel} &
          \rotatebox[origin=l]{90}{Car-Roundabout} &
          \rotatebox[origin=l]{90}{Car-Shadow} &
          \rotatebox[origin=l]{90}{Cows} &
          \rotatebox[origin=l]{90}{Dance-Twirl} &
          \rotatebox[origin=l]{90}{Dog} &
          \rotatebox[origin=l]{90}{Dogs-Jump (3)} &
          \rotatebox[origin=l]{90}{Drift-Chicane} &
          \rotatebox[origin=l]{90}{Drift-Straight} &
          \rotatebox[origin=l]{90}{Goat} &
          \rotatebox[origin=l]{90}{Gold-Fish (5)} &
          \rotatebox[origin=l]{90}{Horsejump-High (2)} &
          \rotatebox[origin=l]{90}{India (3)} &
          \rotatebox[origin=l]{90}{Judo (2)} &
          \rotatebox[origin=l]{90}{Kite-Surf (3)} &
          \rotatebox[origin=l]{90}{Lab-Coat (5)} &
          \rotatebox[origin=l]{90}{Libby} &
          \rotatebox[origin=l]{90}{Loading (3)} &
          \rotatebox[origin=l]{90}{Mbike-Trick (2)} &
          \rotatebox[origin=l]{90}{Motocross-Jump (2)} &
          \rotatebox[origin=l]{90}{Paragliding-Launch (3)} &
          \rotatebox[origin=l]{90}{Parkour} &
          \rotatebox[origin=l]{90}{Pigs (3)} &
          \rotatebox[origin=l]{90}{Scooter-Black (2)} &
          \rotatebox[origin=l]{90}{Shooting (3)} &
          \rotatebox[origin=l]{90}{Soapbox (3)} &
        \\
        
        \cmidrule(r){1-1} \cmidrule(lr){2-31}


        $\textrm{PReMVOS}$~\cite{luiten2018premvos}   &
            \textbf{\avercalc{0.7747, 0.7669}} &
            \textbf{\avercalc{0.9582}} &
            \textbf{\avercalc{0.4884, 0.7304}} &
            \avercalc{0.8076} &
            \textbf{\avercalc{0.9259}} &
            \textbf{\avercalc{0.9754}} &
            \textbf{\avercalc{0.9712}} &
            \textbf{\avercalc{0.9411}} &
            \avercalc{0.7976} &
            \textbf{\avercalc{0.9403}} &
            \avercalc{0.8784, 0.8824, 0.8836} &
            \textbf{\avercalc{0.8820}} &
            \textbf{\avercalc{0.9487}} &
            \textbf{\avercalc{0.8882}} &
            \textbf{\avercalc{0.8740, 0.6747, 0.8571, 0.9325, 0.9406}} &
            \textbf{\avercalc{0.8571, 0.8216}} &
            \textbf{\avercalc{0.6593, 0.5162, 0.4586}} &
            \textbf{\avercalc{0.8959, 0.7656}} &
            \textbf{\avercalc{0.3194, 0.2620, 0.7523}} &
            \textbf{\avercalc{0.2790, 0.0724, 0.8580, 0.9283, 0.8798}} &
            \textbf{\avercalc{0.8682}} &
            \avercalc{0.9524, 0.5378, 0.8505} &
            \textbf{\avercalc{0.8369, 0.7772}} &
            \avercalc{0.6202, 0.4448} &
            \avercalc{0.2732, 0.5106, 0.1289} &
            \textbf{\avercalc{0.9242}} &
            \textbf{\avercalc{0.6161, 0.6049, 0.8673}} &
            \textbf{\avercalc{0.7877, 0.8643}} &
            \textbf{\avercalc{0.6262, 0.7900, 0.8943}} &
            \avercalc{0.8491, 0.6860, 0.7332} &
            \\
        e-OSVOS-100-OnA   &
            \avercalc{0.6753, 0.7936} &
            \avercalc{0.9307} &
            \avercalc{0.4082, 0.6951} &
            \textbf{\avercalc{0.8212}} &
            \avercalc{0.8970} &
            \avercalc{0.9600} &
            \avercalc{0.9612} &
            \avercalc{0.9239} &
            \textbf{\avercalc{0.8652}} &
            \textbf{\avercalc{0.9401}} &
            \textbf{\avercalc{0.8635, 0.9181, 0.8929}} &
            \avercalc{0.8788} &
            \avercalc{0.9298} &
            \avercalc{0.8812} &
            \avercalc{0.8612, 0.7008, 0.8617, 0.9224, 0.9311} &
            \avercalc{0.8276, 0.7991} &
            \avercalc{0.6421, 0.5223, 0.4542} &
            \avercalc{0.8432, 0.7810} &
            \avercalc{0.2208, 0.1268, 0.7293} &
            \avercalc{0.1172, 0.0938, 0.9380, 0.9199, 0.9205} &
            \avercalc{0.8479} &
            \textbf{\avercalc{0.9432, 0.5504, 0.8511}} &
            \avercalc{0.8122, 0.7733} &
            \textbf{\avercalc{0.6383, 0.7549}} &
            \textbf{\avercalc{0.7694, 0.5008, 0.0752}} &
            \avercalc{0.9174} &
            \avercalc{0.5988, 0.5336, 0.9174} &
            \avercalc{0.7691, 0.8457} &
            \avercalc{0.4941, 0.8202, 0.8495} &
            \textbf{\avercalc{0.8513, 0.8010, 0.8395}} &
            \\
        \bottomrule
    \end{tabular}  

    }
\label{tab:eval_seqs}
\end{table*}     \begin{figure*}[t]
    \centering
\subfloat[Frame 1 (Ground Truth)]{
        \begin{tabular}[b]{c}\includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/blackswan/00000_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/kite-surf/00000_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/shooting/00000_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/india/00000_merged.png}
        \end{tabular}}
    \subfloat[Frame 2]{
        \begin{tabular}[b]{c}\includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/blackswan/00001_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/kite-surf/00001_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/shooting/00001_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/india/00001_merged.png}
        \end{tabular}}
    \subfloat[Frame 40]{
        \begin{tabular}[b]{c}\includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/blackswan/00040_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/kite-surf/00040_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/shooting/00039_merged.png}\\
            \includegraphics[width=0.29\textwidth]{figures/vis_comparison_images/india/00040_merged.png}
        \end{tabular}}

    \caption{Visualization of our e-OSVOS-50-OnA results on the \textit{blackswan}, \textit{kite-surf}, \textit{shooting} and \textit{india} sequences from the DAVIS 2017 validation set.
We illustrate the output of the Mask R-CNN~\cite{MaskRCNN} mask head as well as the corresponding object detection bounding box predictions.
    }
    \label{fig:vis_comparison}
\end{figure*} 
    In Table~\ref{tab:eval_seqs}, we provide a per sequence comparison between our e-OSVOS-100-OnA and PReMVOS~\cite{luiten2018premvos} on the DAVIS 2017 validation set.
The e-OSVOS-50-OnA variant applies $T=100$ first frame fine-tuning and subsequently $T_{OnA}=10$ iterations on past mask predictions every fifth frame ($I_{OnA}=5$).
PReMVOS applies up to 1000 iterations and achieves state-of-the-art results with an ensemble of additional methods including a re-identification and contour snapping module.
Nevertheless, we surpass the results of PReMVOS on many sequences while running orders of magnitude faster (0.29 vs. 0.01 frames per second).
In Figure~\ref{fig:vis_comparison}, we present visual results on four sequences for which we achieve worse results compared to PReMVOS.
Due to the RoIAlign feature pooling in the Mask R-CNN~\cite{MaskRCNN} architecture and without an additional contour snapping, we fail to produce highly detailed masks as observable on the \textit{blackswan} sequence (first row).
Furthermore, the \textit{kite-surf} (second row) and \textit{india} (fourth row) sequences demonstrate that our e-OSVOS approach is likely to yield false positive bounding box detections once an object is not visible anymore.
 \fi

\clearpage
\noindent{\bf Acknowledgements}

This research was funded by the Humboldt Foundation through the Sofja Kovalevskaja Award. 

\noindent{\bf Broader Impact}

Authors are asked to include a section in their submissions discussing the broader impact of their work, including possible societal consequences, both, positive and negative.


Many methods for video object segmentation or multiple object tracking rely on appearance models of objects.
In this work, we have shown that one can rely on the simple but elegant solution of fine-tuning of a model as a way to build appearance models.


Semi-supervised video object segmentation is often used to automatize video editing, e.g., to remove one object from a video.
While it is clear that more automatic methods would have a positive impact in reducing the manual work needed to perform such video edits, there is also potential to misuse such technology. One could imagine the creation of fake videos, where objects are taken out or put on the scene to create out-of-context content that might lead viewers to misinterpret the situation.
Nonetheless, we believe the technology is still in an early stage and far from being able to create fake content without substantial knowledge and manual work.
Therefore, we believe that, for this particular task, the benefits outweigh the potential harms of the technology.


Appearance models are also key towards tackling multi-object tracking and segmentation, important for applications such as robotics.
For example, social robots are often tasked with following one specific person, hence the robot has to learn fast an on the fly the appearance of the specific person that it has to follow.
This can be extended to multiple people tracking, where each model would be fine-tuned to a specific person on the scene.
Segmentation of an object of interest becomes also key for robotic tasks such as grasping or any object-robot interaction.
But multi-object tracking and video object segmentation also have a dark side, with applications such as illegal surveillance. We want to note, that our method does not make use of any kind of identifying characteristic of a person (if the person would be our object to follow and segment).
Therefore, we believe our technology does not directly contribute nor promote these kinds of misuses.


We believe that the simple concept of fine-tuning a model to a specific object is incredibly powerful. With our work, we hope to inspire researchers to continue with that paradigm, now that we can properly train it to achieve state-of-the-art results.
Looking at the impact that these tools can have for society, one can see extremely positive things such as the realization of social robots that could help the elderly in their daily chores.
 \clearpage
\bibliography{paper}
\bibliographystyle{plainnat}
 
\end{document}
