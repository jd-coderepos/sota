\sectiontinyvert{Experiments and evaluation} \label{sec:experiments}



We present quantitative results on the Human3.6M~\cite{h36m_pami,IonescuSminchisescu11} and Ski-Pose PTZ-Camera~\cite{ski_ptz} datasets. We present qualitative results on the Human3.6M,  KTH Multi-view Football II~\cite{footballDS} and Ski-Pose PTZ-Camera~\cite{ski_ptz} datasets,
and on synthetic 
videos captured by dynamic cameras. Detailed description of these datasets can be found in the supplementary material.

\paragraphtinyvert{Quantitative results} 

\begin{table*}[t!]

\setlength{\abovecaptionskip}{5pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-2pt plus 3pt minus 2pt}

\caption{Protocol \#1 MPJPE error on Human3.6M. Legend:
 is a \textbf{non} ep-free algorithm. In case parameters are not  given, we imitate their computation by perturbing the GT params by an unrealistically small perturbation amount; 
() exploit temporal information; () extra training data. In \bluebold{blue} - best result when camera parameters are not  given, in \textbf{bold} - best result per method group.
}

\resizebox{\textwidth}{!}{
\Large \begin{tabular}{c|ccccccccccccccc|c}
\toprule
\textbf{Method} &\textbf{Dir.} & \textbf{Disc.}&  \textbf{Eat} & \textbf{Greet} & \textbf{Phone} & \textbf{Photo} & \textbf{Pose} & \textbf{Purch.} &\textbf{Sit}& \textbf{SitD.}& \textbf{Smoke} &\textbf{Wait}& \textbf{WalkD.}& \textbf{Walk}& \textbf{WalkT.} & \textbf{Mean}\\
\midrule
\multicolumn{10}{l}{\textbf{Monocular} methods} & & & & & &\\
\midrule


Shi \etal~\shortcite{shi2020motionet}() &47.3 &53.1 &50.3 &53.9 &53.5 &52.8 &52.0 &55.4 &64.2 &54.8 &66.8 &55.0 &50.3 &59.1 &50.3 &54.6 \\
Llopart~\shortcite{llopart2020liftformer}() &42.2 &44.5 &42.6 &43.0 &46.9 &53.9 &42.5 &41.7 &55.2 &62.3 &44.9 &42.9 &45.3 &31.8 &31.8 &44.8 \\
Reddy \etal~\shortcite{Reddy2021TesseTrackEL}() &38.4 &46.2 &44.3 &43.2 &44.8 &48.3 &52.9 &\textbf{36.7} &\textbf{45.3} &54.5 &63.4 &44.4 &41.9 &46.2 &39.9 &44.6\\
{Li \etal~\shortcite{li2021exploiting}()} &39.9 &43.4 &40.0 &40.9 &46.4 &50.6 &42.1 &39.8 &55.8 &61.6 &44.9 &43.3 &44.9 &29.9 &30.3 &43.6 \\
{Hu \etal~\shortcite{hu2021conditional}()} &\textbf{35.5} &41.3 &\textbf{36.6} &39.1 &42.4 &49.0 &39.9 &37.0 &51.9 &63.3 &\textbf{40.9} &\textbf{41.4} &\textbf{40.3} &\textbf{29.8} &28.9 &41.1 \\
Cheng \etal~\shortcite{cheng20203d}~() &36.2 &\textbf{38.1} &42.7 &\textbf{35.9} &\textbf{38.2} &\textbf{45.7} &\textbf{36.8} &42.0 &45.9 &\textbf{51.3} &41.8 &41.5 &43.8 &33.1 &\textbf{28.6} &\textbf{40.1}  \\
\midrule
\midrule
\multicolumn{10}{l}{\textbf{Multi-view} methods, extrinsic camera parameters are \textbf{ given} } & & & & & &\\
\midrule
Tome \etal\shortcite{tome2018rethinking}~() &43.3 &49.6 &42.0 &48.8 &51.1 &64.3 &40.3 &43.3 &66.0 &95.2 &50.2 &52.2 &51.1 &43.9 &45.3 &52.8 \\
\makecell{Kadkhodamohammadi \\ and  Padoy~\shortcite{kadkhodamohammadi2019generalizable}} &39.4 &46.9 &41.0 &42.7 &53.6 &54.8 &41.4 &50.0 &59.9 &78.8 &49.8 &46.2 &51.1 &40.5 &41.0 &49.1 \\
He \etal~\shortcite{he2020epipolar} &25.7 &27.7 &23.7 &24.8 &26.9 &31.4 &24.9 &26.5 &28.8 &31.7 &28.2 &26.4 &23.6 &28.3 &23.5 &26.9
\\
Qiu \etal~\shortcite{qiu2019cross}~() &24.0 &26.7 &23.2 &24.3 &24.8 &22.8 &24.1 &28.6 &32.1 &26.9 &31.0 &25.6 &25.0 &28.0 &24.4 &26.2\\
Ma \etal~\cite{ma2021transfusion}() &24.4 &26.4 &23.4 &21.1 &25.2
&23.2 &24.7 &33.8 &29.8 &26.4 &26.8 &24.2 &23.2 &26.1 &23.3 &25.8\\
Iskakov \etal~\shortcite{iskakov2019learnable} &19.9 &20.0 &18.9 &18.5 &20.5 &19.4 &18.4 &22.1 &22.5 &28.7 &21.2 &20.8 &19.7 &22.1 &20.2 &20.8\\
Reddy \etal~\shortcite{Reddy2021TesseTrackEL}() &\textbf{17.5} &\textbf{19.6} &\textbf{17.2} &\textbf{18.3} &\textbf{18.2} &\textbf{17.7} &\textbf{18.0} &\textbf{18.0} &\textbf{20.5} &\textbf{20.3} &\textbf{19.4} &\textbf{17.2} &\textbf{18.9} &\textbf{19.0} &\textbf{17.8} &\textbf{18.7}\\
\midrule
\midrule
\multicolumn{10}{l}{\textbf{Multi-view} methods, extrinsic camera parameters are \textbf{not  given} } & & & & & &\\
\midrule
Chu and Pan~\shortcite{chu_and_pan_semisupervised}() &49.1 &63.6 &48.6 &56.0  &57.4 &69.6 &50.4 &62.0 &75.4 &77.4 &57.2 &53.5 &57.7 &37.6 &38.1 &56.9 \\
{\color{gray}
\makecell{Iskakov \etal~\shortcite{iskakov2019learnable} \\ param. perturb by 4\% }} &{\color{gray}30.2} &{\color{gray} 37.2} &{\color{gray} 32.7} &{\color{gray} 33.2} &{\color{gray} 38.8} &{\color{gray}43.7} &{\color{gray}29.7} &{\color{gray}43.0} &{\color{gray}49.4} &{\color{gray}67.6} &{\color{gray}38.0} &{\color{gray} 33.1} &{\color{gray}42.1} &{\color{gray}27.2} &{\color{gray}29.3} &{\color{gray}38.4} \\
{\color{gray}\makecell{Iskakov \etal~\shortcite{iskakov2019learnable} \\ param. perturb by 3\% }} &{\color{gray}27.6}  &{\color{gray}30.3}  &{\color{gray}29.0}  &{\color{gray}29.4}  &{\color{gray}33.1} &{\color{gray}{36.5}} 
&{\color{gray}27.4}  &{\color{gray}34.8}  &{\color{gray}39.1}  &{\color{gray}54.0} &{\color{gray}{34.4}}  &{\color{gray}30.7} &{\color{gray}36.2} &{\color{gray}{26.2}}  &{\color{gray}28.4}  &{\color{gray}33.1} \\
\textbf{Ours}() &\bluebold{22.0} &\bluebold{23.6} &\bluebold{24.9} &\bluebold{26.7} &\bluebold{30.6} &\bluebold{35.7} &\bluebold{25.1} &\bluebold{32.9} &\bluebold{29.5} &\bluebold{32.5} &\bluebold{32.6} &\bluebold{26.5} &\bluebold{34.7} &\bluebold{26.0} &\bluebold{27.7} &\bluebold{30.2}
\end{tabular}}

\label{tab:quant_human36}

\setlength{\abovecaptionskip}{-20pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-0pt plus 3pt minus 2pt}
\caption*{}

\end{table*}













 
We show quantitative results using the Mean Per Joint Position Error (MPJPE)~\cite{h36m_pami,IonescuSminchisescu11}, 
and report standard protocol \#1 MPJPE (that is, error relative to the pelvis), in millimeters.

\Cref{tab:quant_human36} presents a quantitative comparison of the MPJPE metric on the Human3.6M~\cite{h36m_pami} dataset. 
\sr{We present monocular methods, followed by multi-view ones that are split into ones that are acquainted with camera parameters and ones that are not.}
We show that in the absence of camera parameters, our model outperforms state-of-the-art methods by a large margin, and that even when camera parameters are available, 
FLEX is among the top methods. Note that these achievements are although FLEX aims at a slightly different task, which is motion reconstruction rather than pose estimation.

Being the only ep-free algorithm, we have no methods to compare to directly. However, algorithms can mitigate the lack of extrinsic camera parameters by estimating them. In the following comparisons, we show that when extrinsic parameters are not given, using estimated ones induces larger prediction errors, due to the innate inaccuracy of predicted values. On the other hand, FLEX is not affected by the lack of extrinsic parameters, since it does not use them whatsoever.
We compare FLEX with two models:
\begin{enumerate}[nosep,leftmargin=0cm,itemindent=0.5cm,labelwidth=\itemindent,labelsep=0cm,align=left]
\item[(1)]
There are two methods that do not use given camera parameters~\cite{chu_and_pan_semisupervised,kocabas2019selfsupervised}. 
They are not ep-free since they use estimated camera parameters, but we can still use them in settings where camera parameters are not given.
Only one of them~\cite{chu_and_pan_semisupervised} publishes MPJPE protocol \#1 results, and we significantly outperform it
(See \Cref{tab:quant_human36}). 
This gap is mostly because of the inaccuracy of parameter prediction and partially because their model is semi-supervised. 
    
\item[(2)] 
For comparing with the best available method, 
we have chosen the current state-of-the-art multi-view algorithm of Iskakov \etal~\cite{iskakov2019learnable} (TesseTrack~\cite{Reddy2021TesseTrackEL} is marginally better, but it does not provide code). Since their algorithm is \emph{not} ep-free, we
imitate parameter estimation by running a controlled perturbation of the camera parameters.
We re-train their method with distorted data to simulate an environment where camera distortion parameters are unknown. 
In addition, we perturb the 
extrinsic parameters by Gaussian noise with an extremely small standard deviation of 3\% of each parameter's 
value. That is, for a parameter , we sample  and use  as the input extrinsic parameter. 
We show that increasing the standard deviation from 3\% to 4\% yields a significant increase in the error, reflecting the sensitivity of non ep-free methods to  inaccuracy in camera parameters.
To obtain an equivalent environment, we compare FLEX to the method of Iskakov \etal~\shortcite{iskakov2019learnable} after using their own 2D pose estimation.
The lower part of \Cref{tab:quant_human36} shows that FLEX outperforms the non ep-free state-of-the-art, even when perturbation percentage is extremely small.
Their results
in that lower part 
are grayed out, to emphasize that we simulate an unrealistic setting.
\sr{Next, we show that a 3\% perturbation, rather than estimation, is fairer toward the compared method, as estimation induces larger inaccuracy.
We estimate the extrinsic camera parameters with two leading frameworks, COLMAP~\cite{schoenberger2016sfm} and OpenCV-SFM~\cite{opencv_library}, and obtain errors of 5.5\% and 8.6\%, respectively. The error is the mean value of  for all extrinsic values  and their estimation . Moreover, the estimation process involves friction: OpenCV-SFM 
strongly depends on an initial guess, and COLMAP requires that each pair of cameras observes partially overlapping images, a limiting factor that prevents its usage in settings where the cameras face each other.}
\end{enumerate}





\begin{wraptable}{r}{0.48\textwidth}
\setlength{\abovecaptionskip}{-5pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-24pt plus 3pt minus 2pt}

\ifeccv
\caption{
MPJPE 
on the Ski-PTZ dataset,
measured for methods trained when extrinsic parameters are \emph{not} given. 
 is self/weakly-supervised. 
}
\fi
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{MPJPE
} \\
\hline
CanonPose~\cite{wandt2020canonpose} () & 128.1 \\
Chen \etal~\cite{chen2021deductive} () & 99.4 \\
Ours & \textbf{65.5} \\
\hline
\end{tabular}
\end{center}

\ifeccv
\else
\caption{Protocol \#1 MPJPE error on the Ski-PTZ dataset,
measured for methods that are trained when extrinsic parameters are \textbf{not} given. 
Legend:  is self-supervised. 
 is weakly supervised.}
\fi


\label{tab:ski_quantitative}


\setlength{\abovecaptionskip}{-50pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-0pt plus 3pt minus 2pt}
\caption*{}

\end{wraptable}
 In addition to the comprehensive comparison on the Human3.6 dataset, 
in \Cref{tab:ski_quantitative} we show a quantitative comparison on the Ski-Pose PTZ-Camera~\cite{ski_ptz} dataset, for methods that are trained when camera parameters are \emph{not} given.
These methods are comparable in settings that lack extrinsic parameters because they estimate them. 
However, since they still use (estimated) parameters, they are not ep-free.
FLEX leads the table with a large gap.
This gap is mostly because parameter estimation induces an inevitable inaccuracy, and partially because the compared models are self/semi-supervised. 

\begin{table}[t!]
\setlength{\abovecaptionskip}{-5pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-5pt plus 3pt minus 2pt}

    \hfill
    \begin{minipage}{.35\linewidth}
        \centering
\caption{Smoothness, measured by acceleration error (), on Human3.6M.\\
        ():~2D pose from \cite{iskakov2019learnable}.\hfill\\ ():~ground-truth 2D poses.}
        \begin{center}
\begin{tabular}{|c|c|}
\hline
\ifeccv \else \kern-3pt \fi
\textbf{Method} & \textbf{Acc. Err.} \\ \hline
VIBE\cite{kocabas2020vibe} & 18.3 \\ \hline
MEVA\cite{luo20203d} & 15.3 \\ \hline
HMMR\cite{kanazawa2019learning} & 9.1 \\ \hline
TCMR\cite{choi2021static} & 5.3 \\ \hline
Iskakov\cite{iskakov2019learnable} & 3.9\\ \hline
Shi\cite{shi2020motionet}  & 3.6() / 2.0() \\ \hline
FLEX  & \textbf{1.6}() / \textbf{0.9}() \\ \hline

\end{tabular}
\end{center}















         \ifeccv
        \else
        \caption{Smoothness, measured by acceleration error (), on Human3.6M.\\
        ():~2D pose from \cite{iskakov2019learnable}.\hfill\\ ():~ground-truth 2D poses.}
        \fi
        \label{tab:acc_error}
    \end{minipage}\hfill
    \begin{minipage}{.36\linewidth}
      \centering
\caption{Attention impact.\\ TE: Transformer Encoder.\hfill \\MHA: Multi-head Attention.\hfill\\ : no. of stacked layers.\\ : no. of attention heads.}
        \begin{center}
\begin{tabular}{|c|c|}
\hline

\textbf{Method} & \textbf{MPJPE} \\

\hline
Conv. layer & 31.9 \\
\hline
TE - 1, 64 & 30.9 \\
\hline
TE - 2, 64 & 37.8 \\
\hline
MHA - 128 & 30.5 \\
\hline
MHA - 64 & \textbf{30.2} \\
\hline
MHA - 32 & 30.6 \\
\hline
MHA - 16 & 30.9 \\
\hline

\end{tabular}
\end{center}
         \label{tab:fusion_arch}
    \end{minipage}
    \hfill
    \hphantom{.} 

\setlength{\abovecaptionskip}{-35pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-0pt plus 3pt minus 2pt}
\caption*{}

\end{table} 
A known strength of predicting rotation angles rather than locations, is the \emph{smoothness} of predicted motion. 
In \Cref{tab:acc_error} we show that FLEX's smoothness result outperforms others by a large margin. Following Kanazawa \etal~\cite{kanazawa2019learning}, we measure smoothness using the acceleration error of each joint. 



\paragraphtinyvert{Qualitative results}


In the following figures we show rigs, that is, bone structure 
from reconstructed animation videos, selecting challenging scenes. Videos of the reconstructed motions are  
\ifanonymous{provided \iftog{as well,}\else{in the sup. mat.,}\fi}
\else{available on our project page,}
\fi
presenting the smoothness of motion and the naturalness of rotations. 
\Cref{fig:football_teaser,fig:quality_h36,fig:ski_ptz_qualitative}
 show scenes from the KTH Multi-view Football II~\cite{footballDS}, the Human3.6M~\cite{h36m_pami,IonescuSminchisescu11} and the Ski-Pose PTZ-Camera~\cite{ski_ptz} datasets, respectively. 
Each row depicts three views of one time frame. To the right of each image, we place a reconstructed rig, which is sometimes zoomed in for better visualization. 
Notice the occluded and blurry scenes in the football figure (\ref{fig:football_teaser}). The KTH Football dataset is filmed using dynamic (moving) cameras, 
a setting where extrinsic parameters are rarely given, thus disqualifying methods that require camera parameters.
Our algorithm is agnostic to the lack of camera parameters and attains good qualitative results. 

In \Cref{fig:competitors} we show qualitative results of FLEX, compared to current non ep-free  multi-view state-of-the-art \cite{iskakov2019learnable}, and to our monocular baseline~\cite{shi2020motionet}. 
Note that the method in \cite{iskakov2019learnable} produces unnatural poses such as a huge leg in the first row and a backward-bent elbow in the last row. 

\begin{figure}[t]
\setlength{\abovecaptionskip}{5pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-5pt plus 3pt minus 2pt}
\centering
\begin{minipage}[b]{0.48\linewidth}
    \centering
\includegraphics[width=\textwidth]{./images/H36M_results_big_font.pdf}
    \caption{Our results on videos from the Human3.6M dataset.}
    \label{fig:quality_h36}
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\textwidth]{./images/Ski_results.pdf}
    \caption{Our results on videos from the Ski-Pose PTZ-Camera dataset.}
    \label{fig:ski_ptz_qualitative}
\end{minipage}
\end{figure}

 \begin{figure}[t]
\setlength{\abovecaptionskip}{5pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-15pt plus 3pt minus 2pt}
\centering
\begin{minipage}[b]{0.68\linewidth}
    \centering
    \includegraphics[width=\textwidth, height=5.0cm]{./images/competitors_grid.jpg}
    \caption{Qualitative comparison of our work vs. non ep-free state-of-the-art (Iskakov \etal~\cite{iskakov2019learnable}) and vs. our single-view baseline (Shi \etal~\cite{shi2020motionet}).}
    \label{fig:competitors}
\end{minipage}
\hfill
\begin{minipage}[b]{0.27\linewidth}
    \centering
\includegraphics[width=0.8\textwidth,height=5.0cm]{./images/root_color_vert2.jpg}
    \caption{Global root position. Ground-truth is in thin black.}
    \label{fig:root_pos}
\end{minipage}
\end{figure}

 \begin{figure*}[htb]
\setlength{\abovecaptionskip}{5pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-15pt plus 3pt minus 2pt}
\centering
\includegraphics[width=\linewidth]{./images/Fight_results_with_circles_2_ROWS_new_design.jpg}
\caption{Results on multi-person synthetic videos.
\sr{In the zoomed-in circular images we depict 2D pose estimations, which are erroneous due to occlusion. A matching circle in the center rectangular image shows that 
our method reconstructs correct 3D motion although it takes inaccurate 2D joints for input. 
}
} 
\label{fig:fight_results_with_circles}
\end{figure*} 

\paragraphtinyvert{Multi-person captured by dynamic cameras} 
We evaluate our algorithm on
a setting with dynamic cameras, with multi-person scenes introducing
severe inter-person occlusions.
Recall that the term \emph{dynamic} refers to moving cameras that occasionally change their location and rotation.
There are several multi-view datasets. Most of them are not fully dynamic: Human3.6M~\cite{h36m_pami,IonescuSminchisescu11}, CMU Panoptic~\cite{CMU:mocap} and TUM Shelf  Campus~\cite{campus_shelf} contain static scenes only, while Tagging~\cite{tagging_dataset} and Ski-Pose PTZ-Camera~\cite{ski_ptz} contain  rotating cameras whose locations are fixed. KTH~\cite{footballDS} is fully dynamic, but it is too blurry and does not provide ground-truth for all subjects.
Despite its limitations, we use the KTH dataset for qualitative analysis, but we cannot use it for thorough research.
To mitigate the lack of a dynamic dataset, we generate synthetic videos using animated characters downloaded from Mixamo~\cite{mixamo}, an online dataset of character animation. Then, we generate video sequences of two interacting characters using Blender~\cite{blender}, which is a 3D creation suite. The newly created data is available on our project page.
Our "synthetic studio" is illustrated at the \ifarxiv{appendix}\else{sup. mat.}\fi, where two interacting figures are video-filmed by multiple dynamic cameras.
Using Blender, we obtain a rendered video stream from the view angle of each synthetic camera. 
Recall that the input to our algorithm is 2D joint locations, hence it is agnostic to the video appearance, and to whether the input image is real or synthetic.

The 2D backbone we use over the rendered video sequences is Alphapose~\cite{alphapose}, a state-of-the-art multi-person 2D pose estimator.
Once obtaining the 2D joint locations, we use a na\"ive heuristic, which is not part of the suggested algorithm, to associate each detected person with its ID: for each frame, we associate the detected 2D pose with the one that is geometrically closest to it in the previous frame. 
In \Cref{fig:fight_results_with_circles} we depict qualitative results of two boxers. We emphasize several viewpoints where the 2D estimator attains large errors. Yet, FLEX compensates for these errors by fusing multi-view information. In the \ifarxiv{appendix }\else{sup. mat. }\fi we show additional characters and the predicted 2D pose for all the viewpoints.

\paragraphtinyvert{Global position}

In \Cref{fig:root_pos} we draw the global position of the scaled predicted root joint along time.
Ground-truth is depicted using a thin black curve, and our prediction is an overlay on top of it, changing from light to dark as time progresses. The start and the end of each trajectory are signaled by the letters S and E, respectively.
Depicted motions are evaluated on the test set of Human3.6M, on the motions of walking, talking on the phone, and eating.
Note that our predictions almost completely overlap the ground-truth curve. 
Recall we use weak perspective to bypass dependency on intrinsic parameters, resulting in up-to-scale global position accuracy. 
\sr{Quantitatively, our MPJPE on the H36M validation
set is , outperforming Iskakov \etal~\cite{iskakov2019learnable} (perturbed by 3\%) that attain . The other ep-free work \cite{chu_and_pan_semisupervised} does not solve global locations.}




\paragraphtinyvert{Ablation study}

\ifeccv{
    \begin{table}[t]
\setlength{\abovecaptionskip}{0pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{10pt plus 3pt minus 2pt}
    \caption{Ablation studies: The impact of (a) Number of views; (b) 2D backbone, and (c) Fusion method \sr{(refer to the sup. mat. for details regarding fusion)}.}
\begin{minipage}{.32\linewidth}
\setlength{\abovecaptionskip}{0pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-10pt plus 3pt minus 2pt}
        \centering
\caption*{\small(a)}
        



\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{\textbf{\#Views}} & \multicolumn{2}{c|}{\textbf{2D backbone}} \\ \cline{2-3} 
 & \rule{0pt}{2ex}\textbf{GT } & \textbf{\cite{iskakov2019learnable}}\\
\hline
1 & 47.7 & 56.3 \\
\hline
2 & 33.9 & 41.4\\
\hline
3 & 26.3 & 34.6 \\
\hline
4 & \textbf{22.9} & \textbf{30.2} \\
\hline
\end{tabular}

         \label{tab:num_views}
    \end{minipage}
    \hfill
    \begin{minipage}{.32\linewidth}
\setlength{\abovecaptionskip}{-10pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 3pt minus 2pt}
        \centering
\caption*{\small(b)}
        





\begin{center}

\begin{tabular}{|c|c|}
\hline 
\ifeccv \else \kern-3pt \fi
\makecell{\textbf{2D} \\ 
\textbf{backbone}} & 
\ifeccv \else \kern-3pt \fi 
\textbf{MPJPE }
\ifeccv \else \kern-3pt \fi
\\
\hline
\cite{Cao:2018} &38.6\\
\hline
\cite{chen2018cascaded} &31.7\\
\hline
\cite{iskakov2019learnable} &30.2\\
\hline
GT &\textbf{22.9}\\
\hline

\end{tabular}
\end{center}
         \label{tab:2D_backbone}
    \end{minipage}
    \hfill
    \begin{minipage}{.32\linewidth}
\setlength{\abovecaptionskip}{-11pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-21pt plus 3pt minus 2pt}
        \centering
\caption*{\small(c)}
        \begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{MPJPE } \\
\hline
Averaged  views & 36.4 \\
\hline
Late fusion & 31.0 \\
\hline
FLEX & \textbf{22.9} \\
\hline
\end{tabular}
\end{center}         \label{tab:ablation}
    \end{minipage}
\setlength{\abovecaptionskip}{-35pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{-0pt plus 3pt minus 2pt}
\caption*{}
\end{table}
 }
\else{
    \input{./figures/double_tables}
    \input{./figures/ablation}
}
\fi

We evaluate the impact of different settings on the performance of FLEX using various ablation tests. 
\Cref{tab:fusion_arch} compares different multi-view fusion architectures.  
Note that using attention rather than convolution yields a 2mm improvement.
The performance degrades with the transformer encoder due to its large number of parameters, which require more data for training than what is available in our case.

\Cref{tab:num_views} measures MPJPE on Human3.6M in several studies.
\Cref{tab:num_views}(a) studies a varying number of views, where the 2D pose is once given and once estimated. It confirms that a larger number of views induces more accurate results. Note that the gap between the two columns decreases once the number of views increases. It shows that using several views compensates for the inaccuracy of estimated 2D poses. 
\Cref{tab:2D_backbone}(b) compares 2D pose estimation backbones, and justifies our use of Iskakov \etal~\cite{iskakov2019learnable}.
Finally, in \Cref{tab:ablation}(c) we explore
two variations, both with ground-truth 2D inputs. 
The first variation runs FLEX as a monocular method (K=1) and averages the monocular predictions.
The second changes the fusion layers,  and , to use late fusion instead of an early one. 
We conclude that the configuration used by FLEX is better than both variations.


\begin{wrapfigure}{R}{0.5\textwidth}
\centering

\setlength{\abovecaptionskip}{-32pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 3pt minus 2pt}
\caption*{}
\includegraphics[width=0.5\textwidth]{./images/rebutall_2cams_b.png}

\setlength{\abovecaptionskip}{-33pt plus 3pt minus 2pt}
\setlength{\belowcaptionskip}{0pt plus 3pt minus 2pt}
\caption*{}
\label{fig:generalization_inset}

\end{wrapfigure} 
\setcolor{violet}
\paragraphtinyvert{Generalization}
We exhibit generalization by training on one dataset and evaluating on a different, more challenging one. The train dataset is Human3.6M, and the evaluation ones are the KTH Football dataset, and the synthetic videos.
For quantitative measurement, we train our model on two of the four cameras of the Human3.6M dataset.
We test it using the other two cameras, on which the model has not been trained. 
We repeat this process for all possible camera pairs and obtain an average MPJPE of 148mm. Note that this error is not large compared to the human body size, and indeed we attain pleasing visual results as shown in the inset on the right.
\setcolor{black}

