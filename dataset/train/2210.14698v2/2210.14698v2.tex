\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2022}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}
\usepackage{csquotes}

\usepackage{microtype}
\usepackage{makecell,multirow}
\usepackage{amsmath,amssymb,amsthm}

\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{inconsolata}
\usepackage{todonotes}




\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}
\restylefloat{table}
\usepackage{booktabs}

\DeclareMathOperator*{\topK}{Top-K}

\newcommand{\pairb}[1]{\left( #1 \right)}
\newcommand{\bigO}[1]{\mathcal{O}(#1)}

\newcommand{\ryanchange}[1]{{\color{blue} #1}}


\newcommand{\xra}[1]{\overset{#1}{\rightsquigarrow}}
\newcommand{\xla}[1]{\overset{#1}{\leftsquigarrow}}

\newcommand{\openm}[0]{\texttt{<m>}}
\newcommand{\closem}[0]{\texttt{<\textbackslash m>}}
\newcommand{\pseq}[1]{P_{\mathrm{seq}}\left(#1\right)}

\newcommand*{\defeq}{\mathrel{\stackrel{\textnormal{\tiny def}}{=}}}
\newcommand{\defn}[1]{\textbf{#1}}
\newcommand*{\bw}{\mathbf{w}}

\newcommand*{\asp}{\texttt{ASP}}
\newcommand*{\tfive}{\textsc{t{\scriptsize 5}}}
\newcommand*{\tzero}{\textsc{t{\scriptsize 0}}}
\newcommand*{\flant}{\textsc{flan}\text{-}\textsc{t{\scriptsize 5}}}


\definecolor{mygreen}{HTML}{7FC07F}
\definecolor{myred}{HTML}{DF7F7F}
\definecolor{mygray}{HTML}{BAB7B7}
\definecolor{pine}{HTML}{BCE672}
\definecolor{lotus}{HTML}{E4C6D0}
\definecolor{gold}{HTML}{F2BE45}
\definecolor{bamboo}{HTML}{789262}
\definecolor{jade}{HTML}{3DE1AD}
\definecolor{blueblue}{HTML}{4C8DAE}

\newcommand*{\actioncopy}{\texttt{{\setlength{\fboxsep}{2.5pt}\colorbox{mygray}{copy}}}}
\newcommand*{\actionlsb}{\texttt{{\setlength{\fboxsep}{2pt}\colorbox{mygreen}{[}}}}
\newcommand*{\actionrsb}{\texttt{{\setlength{\fboxsep}{2pt}\colorbox{myred}{]}}}}

\newcommand*{\calA}{\mathcal{A}}
\newcommand*{\calB}{\mathcal{B}}
\newcommand*{\calC}{\mathcal{C}}
\newcommand*{\calE}{\mathcal{E}}
\newcommand*{\actionSet}{\mathcal{A}}
\newcommand*{\calZ}{\mathcal{Z}}
\newcommand*{\calY}{\mathcal{Y}}
\newcommand*{\calYn}{\mathcal{Y}_n}
\newcommand*{\calL}{\mathcal{L}}
\newcommand*{\calM}{\mathcal{M}}
\newcommand*{\calR}{\mathcal{R}}
\newcommand*{\calT}{\mathcal{T}}
\newcommand*{\calU}{\mathcal{U}}
\newcommand*{\ptheta}{p_{\boldsymbol{\theta}}}
\newcommand*{\stheta}{s_{\boldsymbol{\theta}}}

\usepackage{cleveref}
\crefname{section}{\S}{\S\S}
\Crefname{section}{\S}{\S\S}
\crefname{table}{Tab.}{}
\crefname{figure}{Fig.}{Figs.}
\crefname{algorithm}{Algorithm}{}
\crefname{equation}{Eq.}{Eqs.}
\crefname{line}{Line}{}
\crefname{appendix}{App.}{}
\crefformat{section}{\S#2#1#3}
\crefname{thm}{Theorem}{}
\crefname{cor}{Corollary}{}
\crefname{prop}{Proposition}{}
\crefname{def}{Definition}{}

\newcommand{\tianyuchange}[1]{{\color{blue}  #1}}

\makeatletter
\newcommand*\iftodonotes{\if@todonotes@disabled\expandafter\@secondoftwo\else\expandafter\@firstoftwo\fi}
\makeatother
\newcommand{\noindentaftertodo}{\iftodonotes{\noindent}{}}
\newcommand{\fixme}[2][]{\todo[color=yellow,size=\scriptsize,fancyline,caption={},#1]{#2}} \newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} \newcommand{\ryan}[2][]{\note[#1]{ryan}{violet!40}{#2}}
\newcommand{\Ryan}[2][]{\ryan[inline,#1]{#2}\noindentaftertodo}
\newcommand{\eleanor}[2][]{\note[#1]{eleanor}{yellow!40}{#2}}
\newcommand{\Eleanor}[2][]{\eleanor[inline,#1]{#2}\noindentaftertodo}

\newcommand{\mrinmaya}[2][]{\note[#1]{mrinmaya}{cyan}{#2}}

\newcommand{\tianyu}[2][]{\note[#1]{tianyu}{pine}{#2}}
\newcommand{\Tianyu}[2][]{\tianyu[inline,#1]{#2}\noindentaftertodo}

\newcommand{\nicholas}[2][]{\note[#1]{nicholas}{lime}{#2}}

\newcommand{\zee}[2][]{\note[#1]{zee}{olive}{#2}}
 

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}

\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\boldsymbol{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\boldsymbol{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\newcommand*{\ethz}{}
\newcommand*{\google}{}



\setlength\titlebox{4.15cm}
\setlength{\belowcaptionskip}{-10pt}


\title{Autoregressive Structured Prediction with Language Models}

\author{Tianyu Liu\textsuperscript{\ethz} \qquad Yuchen Eleanor Jiang\textsuperscript{\ethz} \\ \bf{Nicholas Monath}\textsuperscript{\google} \qquad \bf{Ryan Cotterell}\textsuperscript{\ethz} \qquad \bf{Mrinmaya Sachan}\textsuperscript{\ethz} \\ 


\setlength{\fboxsep}{2.pt}\setlength{\fboxrule}{2.pt}

\fcolorbox{white}{white}{
\textsuperscript{\ethz}ETH Zürich \qquad \textsuperscript{\google}Google Research
} \\

\fcolorbox{white}{white}{
  \texttt{\href{mailto:tianyu.liu@inf.ethz.ch}{tianyu.liu},} \texttt{\href{mailto:yuchen.jiang@inf.ethz.ch}{yuchen.jiang}}\texttt{@inf.ethz.ch}
} \\
\fcolorbox{white}{white}{
\texttt{\href{mailto:nmonath@google.com}{nmonath@google.com}} \qquad \texttt{\href{mailto:ryan.cotterell@inf.ethz.ch}{ryan.cotterell},} \texttt{\href{mailto:mrinmaya.sachan@inf.ethz.ch}{mrinmaya.sachan}}\texttt{@inf.ethz.ch}
}
}

\begin{document}
\maketitle

\begin{abstract}
    In recent years, NLP has moved towards the application of language models to a more diverse set of tasks.
    However, applying language models to structured prediction, e.g., predicting parse trees, taggings, and coreference chains, is not straightforward.
    Prior work on language model-based structured prediction typically flattens the target structure into a string to easily fit it into the language modeling framework.
    Such flattening limits the accessibility of structural information and can lead to inferior performance compared to approaches that overtly model the structure.
    In this work, we propose to construct a conditional language model over sequences of structure-building actions, rather than over strings in a way that makes it easier for the model to pick up on intra-structure dependencies.
    Our method sets the new state of the art on named entity recognition, end-to-end relation extraction, and coreference resolution.\looseness=-1

    \vspace{1.5em}
    {\includegraphics[width=1.25em,height=1.25em]{logos/github.png}\hspace{1.5em}\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{\url{https://github.com/lyutyuh/ASP}}}
    \vspace{-1.0em}
\end{abstract}


\begin{figure*}[!ht]
     \centering
     \includegraphics[width=\textwidth,trim={0.3cm 0.cm 0.5cm 0.1cm} ,clip]{figs/illustration_cr.pdf}
        \caption{Illustration of the target outputs of our framework on coreference resolution (\textbf{\textsc{coref}}) and end-to-end relation extraction (\textbf{\textsc{ere}}). The lower part illustrates the decoding process of our model. The actions  are color-coded as \actionrsb{}, \actionlsb{} and \actioncopy{}. The structure random variables  are presented along with coreference links or relation links. We present words in the \actioncopy{} cells merely as an illustration. \looseness=-1 } 
        \label{fig:illustration}
\end{figure*}

\section{Introduction}
Many common NLP tasks, e.g., named entity recognition, relation extraction, and coreference resolution are naturally taxonomized as structured prediction, the supervised machine-learning task of predicting
a structure from a large\footnote{Typically, large means exponential in the size of the input.} set.
To generalize well to held-out data in a structured prediction problem, the received wisdom has been that it is necessary to correctly model complex dependencies between different pieces of the structure.
However, a recent trend in structured prediction for language has been to forgo explicitly modeling such dependencies \citep[\textit{inter alia}]{ma-hovy-2016-end, lee-etal-2017-end, he-etal-2017-deep}, and, instead, to apply an expressive black-box model, e.g., a neural network, with the hope that the model picks up on the dependencies without explicit instruction.\looseness=-1


Framing structured prediction as conditional language modeling is an increasingly common black-box technique for building structured predictors that has led to empirical success \citep[\emph{inter alia}]{NIPS2015_277281aa, raffel-t5,athiwaratkun-etal-2020-augmented,decao2020autoregressive,tanl}. 
The idea behind the framework is to encode the target structure as a string, flattening out the structure.
Then, one uses a conditional language model to predict the flattened string encoding the structure.
For instance, \citet{NIPS2015_277281aa} flatten parse trees into strings and predict the strings encoding the flattened trees from the sentence with a machine translation architecture.
The hope is that the autoregressive nature of the language model allows it to \emph{learn} to model the intra-structure dependencies and the necessary hard constraints that ensure the model even produces well-formed structures.
Additionally, many modelers make use of pre-trained language models \cite{lewis,raffel-t5} to further improve the language models.\looseness=-1

However, despite their empirical success, simply hoping that a black-box approach correctly models intricate intra-structure dependencies is often insufficient for highly structured tasks \citep[\S1]{tanl}.
Indeed, the act of flattening a structured object into a string
makes properly modeling the intra-structure dependencies harder for many tasks, e.g., those that involve nested spans or long-distance dependencies. 
For instance, in coreference resolution, a conference link between two mentions can stretch across thousands of words, and a coreference chain can also contain over a hundred mentions \citep{pradhan-etal-2012-conll}.
Flattening such a large amount of structured information into a string makes the task more difficult to model.\looseness=-1

In this paper, we propose a simple framework that augments a conditional language model with explicit modeling of structure.
Instead of modeling strings that encode a flattened representation of the target structure, we model a constrained set of actions that build the target structure step by step; see \cref{fig:illustration} for an example of our proposed framework.
Training a conditional language model to predict structure-building actions exposes the structure in a way that allows the model to pick up on the intra-structure dependencies more easily while still allowing the modeler to leverage pre-trained language models.
We conduct experiments on three structured prediction tasks: named entity recognition, end-to-end relation extraction, and coreference resolution. 
On each task, we achieve state-of-the-art results \emph{without} relying on data augmentation or task-specific feature engineering.\looseness=-1
 
\section{Autoregressive Structured Prediction} \label{sec:model}

In this section, we describe our proposed approach, which we refer to as \textbf{autoregressive structured prediction} (\asp).
Unlike previous approaches for structured prediction based on conditional language modeling, we represent structures as sequences of \defn{actions}, which build pieces of the target structure step by step.
For instance, in the task of coreference resolution, the actions build spans as well as the relations between the spans, contiguous sequences of tokens.
We give an example in \cref{fig:illustration}.


\subsection{Representing Structures with Actions}
 While our approach to structured prediction, \asp, is quite general, our paper narrowly focuses on modeling structures that are expressible as a set of dependent spans, and we couch the technical exposition in terms of modeling spans and relationships among spans.
Our goal is to predict an action sequence , where each action  is chosen from an \defn{action space} .
In this work, we take  to be factored, i.e., , where  is a set of structure-building actions,  is the set of bracket-pairing actions, and  is a set of span-labeling actions. 
Thus, each  may be expressed as a triple, i.e., .
We discuss each set in a separate paragraph below.\looseness=-1


\paragraph{Structure-Building Actions.} 
We first define a set of structure-building actions  that allow us to encode the span structure of a text, e.g.,  in \cref{fig:illustration} encodes that \textit{Delaware} is a span of interest.
More technically, the action  refers to a right bracket that marks the right-most part of a span.
The action  refers to a left bracket that marks the left-most part of a span.
The superscript  on  is inspired by the Kleene star and indicates that it is a placeholder for 0 or more consecutive left brackets\footnote{In our preliminary experiments, we observe unsatisfactory performance when the model has to generate consecutive left brackets. We leverage  as an engineering workaround. We hypothesize that this phenomenon is due to the inability of transformers to recognize \texttt{Dyck} languages~\citep{hahn-2020-theoretical, hao-etal-2022-formal}.}.
Finally,  refers to copying a word from the input document. 
To see how these actions come together to form a span, consider the subsequence in \cref{fig:illustration}, , which is generated from a sequence of structure-building actions , , and .\looseness=-1

\paragraph{Bracket-Pairing Actions.}
Next, we develop the set of actions that allow the model to match left and right brackets; we term these bracket-pairing actions. 
The set of bracket-pairing actions consists of
all previously constructed left brackets, i.e.,

Thus, in general,  is .
However, it is often the case that domain-specific knowledge can be used to prune .
For instance, coreference mentions and named entities rarely cross sentence boundaries, which yields a linguistically motivated pruning strategy \citep{liu-etal-2022-structured}.
Thus, in some cases, the cardinality of  can be significantly smaller.
When we decode action sequences  into a structure, unpaired  and  can be removed ensuring that the output of the model will not contain unpaired brackets.\looseness=-1


\paragraph{Span-Labeling Actions.}
Finally, we add additional symbols  associated with each  that encode a labeling of a single span or a relationship between two or more spans.
For instance, see \Cref{sec:instantiation} for an example.
We denote the set of all  as

where  is the set of previous spans, which allows the model to capture intra-span relationships, and  denotes the set of possible labelings of the current span and the relationship between the adjoined spans. 
In general, designing  requires some task-specific knowledge in order to specify the label space.
However, we contend it requires less effort than designing a flattened string output where different levels of structures may be intertwined \cite{tanl}. \looseness=-1


\subsection{Model Parameterization}
Let  be an input document of  sentences where  denotes the  sentence in .
We first convert the structure to be built on top of  into an action sequence, which we denote as  where . 
Now, we model the sequence of actions  as a conditional language model\looseness=-1

The log-likelihood of the model is then given by .
We model the local conditional probabilities  as a softmax over
a \emph{dynamic} set  that changes as a function of the history , i.e.,

where  is a parameterized score function; we discuss several specific instantiations of  in \cref{sec:instantiation}.
Finally, we note that the use of a dynamic vocabulary stands in contrast to most conditional language models where the vocabulary is held constant across time steps, e.g., \citeposs{sutskever2014sequence} approach to machine translation.\looseness=-1

\paragraph{Greedy Decoding.}
We determine the approximate best sequence  using a greedy decoding strategy.
At decoding step , we compute

The chosen  will then be verbalized as a token as follows: If , then we copy the next token from the input that is not present in the output. 
Otherwise, if  or , we insert  or  into the output sequence, respectively. 
The verbalized token is then fed into the conditional language model at the next step. 
The decoding process terminates when the model copies a distinguished symbol \textsc{eos} symbol from the input.
The end of the procedure yields an approximate argmax .

\looseness=-1

\paragraph{Computational Complexity.}
\cref{eq:loglinear} can be computed quite efficiently using our framework, as the cardinalities of  is , and the size of  and  are both .
A tighter analysis says the cardinalities of  and  are roughly linear in the number of spans predicted. 
In practice, we have  where  is the size of vocabulary, which is the step-wise complexity of \cite{tanl}.
A quantitative analysis of the number of mentions in coreference can be found in \cref{appendix:mention_recall}. \looseness=-1

\paragraph{Generality.}
Despite our exposition's focus on tasks that involve assigning labels to span or span pairs, our method is quite general. 
Indeed, almost any structured prediction
task can be encoded by a series of structure-building actions.
For tasks that involve labeling tuples of spans, e.g., semantic role labeling makes use of tree-tuples that consist of the subject, predicate, and object, \cref{eq:support_general} can be easily extended with a new space of categorical variables  to model the extra item.

\subsection{Task-specific Parameterizations} \label{sec:instantiation}
We now demonstrate how to apply \asp{} to three language structured prediction tasks: 
named entity recognition, coreference resolution, and end-to-end relation extraction.

\paragraph{Named Entity Recognition.} 
Named entity recognition is the task of labeling all mention spans   in a document  that refers to named entities. 
Since named entity recognition only requires labeling spans (and not linking them), we only need our task-specific  to encode the entity type, which is canonically taken from a set of pre-defined categories .
The function  in \cref{eq:loglinear} is implemented by a feed-forward network

where  is the decoder hidden state at step , a column vector, and  represents the mention that corresponds to .
Note that each  and  represent independent feed-forward networks with \emph{no} shared parameters.\looseness=-1

\paragraph{End-to-End Relation Extraction.} 
End-to-end relation extraction is the task of jointly extracting a set of entities alongside a set of relations between pairs of extracted entities.
Formally, given a set of pre-defined entity categories  and a set of pre-defined relations .
The goal is (i) to identify all possible entities  in  that could be associated with one of the entity types  in  and (ii) to identify all possible triples  in  where  are the head and tail entity and  is the relation between  and . 
Here, the support of  takes the form of \cref{eq:support_general}, where  is instantiated as . 
And  kept the same as in \cref{eq:step_scorer_ner}.

\paragraph{Coreference Resolution.} 

The task of coreference resolution involves identifying all mention spans  in  and then clustering them.
However, in addition to identifying the mention spans, the task of coreference resolution requires us to assign an antecedent to every possible mention in .
To encode coreference resolution in our framework, we consider the task-specific  from the set\looseness=-1

where we follow the convention set in \citet{lee-etal-2017-end} that the antecedent of the first mention in each coreference chain is defined to be .
Again, we define  as in \cref{eq:step_scorer_ner} with the exception that, when , we define . \looseness=-1
 
\section{Experiments}
We experiment on three NLP structured prediction tasks: named entity recognition, end-to-end relation extraction, and coreference resolution.
We are primarily interested in understanding whether ASP provides advantages over two existing formalisms: (i) conditional language models \citep{athiwaratkun-etal-2020-augmented, tanl} that flatten the structure into a string (augmented language models), and (ii) the classic discriminative models whose autoregressivity is bounded.
We experiment with three pre-trained language models, T5 \cite{raffel-t5}, T0 \cite{sanh2021multitask}, and Flan-T5 \cite{flant5} for the three tasks under consideration.
Additional experimental details are given in \cref{appendix:exp_setting} and \cref{appendix:datasets}.\looseness=-1

\subsection{Named Entity Recognition}
First, we evaluate our model on the CoNLL-03 English NER task. Following previous work, we report the micro precision, recall, and F1 score. 
As shown in \cref{tab:result_test_ner_conll03}, our model using T0-3B backbone outperforms all other models without data augmentation or ensembling.

\begin{table}[t]
\centering
\resizebox{0.5\textwidth}{!}{
\small
\begin{tabular}{lccc}
\toprule
            & Prec. & Rec. & F1 \\ \midrule
 \citet{ma-hovy-2016-end} & 91.4 & 91.1  & 91.2 \\
  \citeauthor{devlin-etal-2019-bert} & - & - &  92.8 \\  \citeauthor{ye-etal-2022-packed} & - & - & 94.0  \\ 
\citeauthor{athiwaratkun-etal-2020-augmented} & - & - & 91.5 \\ 
\citeauthor{tanl}  & - & - & 91.7 \\ \midrule
\texttt{ASP} & 91.4 & 92.2 & 91.8 \\ 
{\texttt{ASP}} & 92.1  & 93.4  & 92.8 \\
{\texttt{ASP}} & {93.8}  & {94.4} &  \textbf{94.1} \\ \bottomrule
\end{tabular}} 
\caption{Test F1 scores of named entity recognition on the CoNLL-03  test set.}
\label{tab:result_test_ner_conll03}
\end{table} 
\subsection{End-to-End Relation Extraction} 
We compare \asp{} on the CoNLL-04 and ACE-05 English end-to-end relation extraction datasets.
The results are shown in \cref{tab:result_test_ere_conll04} and \cref{tab:result_test_ere_ace05}.
Our proposed approach achieves state-of-the-art results on both datasets using T5-3B as the backbone.
In particular, it outperforms the flattened-string model of \citet{tanl} by a large margin ( F1).
We hypothesize that this is due to relations requiring higher-order dependencies between spans. 


\begin{table}[ht]
\centering
\resizebox{0.5\textwidth}{!}{
\small
\begin{tabular}{lcc}
\toprule
                 & Ent  & Rel  \\ \midrule
\citet{eberts2019span} & 88.9 & 71.5   \\ 
 \citet{ijcai2020-546} & 88.9 & 71.9    \\ 
 \citeauthor{wang-lu-2020-two} & 90.1 & 73.8    \\ 
\citeauthor{tanl} & 89.4 & 71.4  \\ \midrule
 \texttt{ASP}& 89.5  & 73.2   \\ 
 \texttt{ASP} & \textbf{90.3}  & \textbf{76.3}   \\ \bottomrule
\end{tabular}} 
\caption{\textbf{Micro} F1 scores of entity extraction and relation extraction on the CoNLL-04 joint entity relation extraction test set.}
\label{tab:result_test_ere_conll04}
\end{table}

\begin{table}[ht]
\centering
\resizebox{0.5\textwidth}{!}{
\small
\begin{tabular}{lccc}
\toprule
                   & Ent & Rel & Rel+ \\ \midrule
\citeauthor{wang-lu-2020-two} & 89.5 & 67.6  & 64.3 \\
\citeauthor{zhong-chen-2021-frustratingly} & 90.9 & 69.4 &  67.0 \\  
\citeauthor{ye-etal-2022-packed}\footnotemark{} & 91.1 & 72.4 & 70.3  \\ 
\citeauthor{tanl} & 88.9 & 63.7 & - \\ \midrule
 \texttt{ASP} & 90.7  & 71.1  & 68.6 \\ 
 \texttt{ASP} & 91.3  & 71.9  & 69.4 \\ 
 \texttt{ASP}\footnotemark{} & \textbf{91.3}  & \textbf{72.7} &  \textbf{70.5} \\ \bottomrule
\end{tabular}} 
\caption{Test F1 scores of entity and relation extraction on the ACE-05 joint entity relation extraction task.}
\label{tab:result_test_ere_ace05}
\end{table}
\footnotetext[\numexpr\value{footnote}-1]{\citet{ye-etal-2022-packed} counts symmetric relations twice for evaluation, which is inconsistent with previous work. We report the re-evaluated scores under the standard metric.}
\footnotetext{On ACE-05, we observe inferior performance using T0-3B instead of T5-3B. We suspect this is due to systematic deficiencies in dataset preprocessing, e.g., errors during sentencization and tokenization as well as inconsistent capitalization.\looseness=-1}
 
\subsection{Coreference Resolution}
We then conduct experiments on the standard OntoNotes benchmark in the CoNLL-12 English shared task dataset \cite{pradhan-etal-2012-conll}.
\Cref{tab:result_test_coref} reports the results. Again, our model achieves state-of-the-art performance among systems without any data augmentation\footnote{We achieve  F1 score on the development set, outperforming the result without pretraining on additional data reported by \citet[Tab. 5]{wu-etal-2020-corefqa}. 
In addition, training our model does not require the usage of TPUs.}, outperforming the previous state of the art by 1.5 F1 score.
We also observe that our \asp{} models substantially outperform discriminative models that make use of the same PLM.
Further analysis is provided in \cref{appendix:mention_recall}.

\begin{table}[t]
\centering
\resizebox{0.5\textwidth}{!}{
\small
\begin{tabular}{lcccc}
\toprule
                     & MUC    & B   & CEAF  &      Avg. F1    \\ \midrule
\citet{lee-etal-2017-end} & 75.8 & 65.0  & 60.8          & 67.2          \\ 
 \citet{joshi-etal-2020-spanbert} & 85.3   & 78.1         & 75.3          & 79.6           \\
 \citeauthor{joshi-etal-2020-spanbert}  \hspace{-0mm} &  79.8         & 70.2          & 66.8          & 72.3       \\ 
 \citeauthor{joshi-etal-2020-spanbert}  & 81.4          & 73.1         & 73.1          & 74.9     \\ 
 

\citeauthor{urbizu-etal-2020-sequence} & 
64.9 &  66.5 & 65.3 & 65.6   \\ 
 \citeauthor{tanl}  & 81.0 & 69.0 & 68.4 & 72.8   \\ 
 \citeauthor{dobrovolskii-2021-word} & 86.3          & 79.9         & 76.6          & 81.0     \\ \midrule
\texttt{ASP} & 82.3 & 75.1 & 72.5 &  76.6  \\ 
\texttt{ASP} & 84.7 & 77.7  & 75.2 & 79.3  \\ 
\texttt{ASP} & 86.9  & 81.5  & 78.4 & 82.3 \\ 
\texttt{ASP} & 87.2 & 81.7  & 78.6 & \textbf{82.5}\\ \bottomrule
\end{tabular}} 
\caption{Results on the CoNLL-12 English test set. Avg. F1 denotes the average F1 of MUC, B{}, and CEAF{}. 
Models marked with  are our re-implementation. Other results are taken from their original papers. The full results are in \Cref{tab:result_test_conll}.}
\label{tab:result_test_coref}
\end{table} 

\section{Related Work}

Most similar to our approach is the model of  \cite{tanl}, which also predicts structures in an iterative manner using conditional language models.
Similar approaches exist for constituency parsing \cite{NIPS2015_277281aa,dyer-etal-2016-recurrent}, entity retrieval \cite{decao2020autoregressive},
semantic parsing \cite{xiao2016sequence}, slot labeling, and intent classification \cite{athiwaratkun-etal-2020-augmented}. 
Earlier work on search-based \cite{daume2009search,doppa2014structured,chang2015learning} and greedy-based approaches \cite{swayamdipta2016greedy} applied to structured prediction also predict the structure in a sequential fashion as we do.\looseness=-1
Other work such as energy-based models \cite[\textit{inter alia}]{belanger2016structured,tu2018learning} and graphical models \cite{durrett-klein-2014-joint, ganea-hofmann-2017-deep} predict structures more holistically.\looseness=-1
 
\section{Conclusion}
In this paper, we propose a novel framework for structured prediction
that encodes a structure as a series of structure-building actions that obtains state-of-the-art performance across three tasks.
In contrast to past approaches for structured prediction, our approach is compatible with pre-trained large language models.
This allows us to reduce structured prediction to the problem of fine-tuning pre-trained language models over an enlarged alphabet.
We show empirically that \asp{} outperforms previous structured prediction models by a large margin. 
Indeed, we set the new state of the art on three tasks: named entity recognition, end-to-end relation extraction, and coreference resolution.


\section*{Acknowledgements}
We acknowledge support from an ETH Z\"urich Research grant (ETH-19 21-1) and a grant from the Swiss National Science Foundation (project \#201009) for this work. We also thank Zeerak Talat and Peter Szemraj for their feedback on this manuscript. 

\section*{Ethical Considerations}
To consider the ethical implications of our work,
we consider the tasks and models used
and our proposed approach. The tasks considered, 
named entity recognition, relation extraction, and coreference resolution
are often used in a pipeline of approaches (say for automatically
building knowledge bases). Understanding the biases, errors, and failure cases of these tasks and their models and how they affect downstream use cases of the knowledge base would be important to consider. That said, to our knowledge the proposed approach does not exacerbate (or lessen) or introduce new considerations to the ones known about tasks/models more generally. 


\section*{Limitations}

\paragraph{Autoregressive Modeling Assumption.}
The decoder model, which is autoregressive, introduces an inductive bias on the structured prediction approach.
Specifically, the left-to-right approach requires the model to model dependencies in a specific order.
This could account for some of the reduction in performance compared to task-specific discriminative models. Understanding the implications of the autoregressive decision is indeed an interesting question, but one that we felt was out of scope for this short paper.

\paragraph{Efficiency.}
In our experiments, we reduce the burden of finding many mention spans in two-stage approaches.  
On sentence-level tasks, e.g., entity and relation extraction, the number of decoding steps is relatively small. 
For instance, the average number of words in an input sentence is 20. Our system has a lighter memory trace as opposed to discriminative models. This extra time cost can be partially compensated with larger batch sizes.
However, on document-level tasks, e.g., coreference resolution, the number of decoding steps is too large to be compensated with parallelism. More efficient methods for inference such as non-autoregressive decoding \cite{gu2018nonautoregressive} remain to be explored in future work.

\paragraph{Decoding Algorithms.}
In this work, we use greedy decoding in all the experiments. 
Alternative decoding algorithms might further improve the quality of the generated sequences, e.g., beam search \cite{zhang-clark-2008-tale,goldberg-etal-2013-efficient}.


\paragraph{Choice of Pretrained Language Models.} 
In this work, the choice of T5 and its variants as the conditional language model backbone of our model is largely motivated by their ability to handle arbitrarily long sequences. Unlike BART and GPT, T5 uses relative position encoding. On document-level tasks such as coreference resolution, the ability to process long sequences is extremely important.
However, other pretrained conditional language models, either with encoder--decoder structures or decoder-only structures, can be used as a backbone. It might be interesting to explore techniques that generalize fixed-length position encoding to longer sequences.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}
\clearpage
\appendix

\section{Experimental Details}
\subsection{Experimental Settings} \label{appendix:exp_setting}
In our experiments, the \{T5,T0,flan-T5\}-base, \{T5,T0,flan-T5\}-large, \{T5,T0,flan-T5\}-\{3B,XL\}, \{T5,T0,flan-T5\}-\{11B,XXL\} have 220 million, 770 million, 3 billion, and 11 billion parameters respectively\footnote{\url{https://github.com/google-research/text-to-text-transfer-transformer}}. The feedforward neural networks described in \cref{sec:instantiation} have one hidden layer of size 150 for ACE-05, 4096 for CoNLL-03, CoNLL-04, and CoNLL-12.\looseness=-1

We follow the same preprocessing procedure and train/dev/test split of previous work on all datasets. For all the experiments, we use the AdamW optimizer \cite{kingma2015adam}. 
We train 40 epochs on CoNLL-12 for coreference resolution with batch size 1. For end-to-end relation extraction on CoNLL-04 and ACE-05, we train 100 epochs with batch size 8. The initial learning rates are set to 5e-5 for \{T5,T0,flan-T5\}-base and \{T5,T0,flan-T5\}-large models, 3e-5 for \{T5,T0,flan-T5\}-\{3B,XL,11B,XXL\} models.\looseness=-1


We apply bfloat16 training in our experiments. One single A100-40GB GPU is used for training models that use \{T5,T0,flan-T5\}-base and \{T5,T0,flan-T5\}-large. Two A100-40GB GPUs are required to train models that use 3B or XL. Six A100-80GB GPUs are required to train models that use 11B or XXL models.
It takes around 0.1 seconds for base-scale models and 1 second per updating step for \{3B,11B,XXL\} models.\looseness=-1

\subsection{Datasets} \label{appendix:datasets}

\subsubsection{Named Entity Recognition}
\paragraph{CoNLL-03.}
We use the CoNLL-03 dataset \cite{tjong-kim-sang-de-meulder-2003-introduction} to evaluate our model on named entity recognition. This dataset consists of 946 training articles, 216 development articles, and 231 test sentences. We evaluate under the document-level settings, which means we feed the entire document into the model instead of the individual sentences.


\subsubsection{End-to-End Relation Extraction}
\paragraph{CoNLL-04.}
The CoNLL-04 dataset contains four types of entities (location, organization, person, other) and five types of relations (work for, kill, organization based in, live in, located in).
We split the dataset as the training (922 sentences), validation (231 sentences), and test (288 sentences) as in previous work.
For the ACE-05 dataset, we follow the train/dev/test split of previous work \cite{zhong-chen-2021-frustratingly}.



\paragraph{ACE-05.}
The ACE-05 dataset \cite{walker2005ace} contains 511 documents in total collected from multiple domains including newswire, broadcast, discussion forums, etc. We follow \citet{luan-etal-2019-general}'s preprocessing script\footnote{https://github.com/luanyi/DyGIE/tree/master/preprocessing} and split the dataset into train/dev/test set.
ACE-05 contains inconsistently capitalized data. The newswire portion collected from CNN are entirely lowercased, which involves around 20 documents. Previous works \cite{zhong-chen-2021-frustratingly,ye-etal-2022-packed} that use case-insensitive encoders such as ALBERT are not affected by this deficiency. 
However, the T5 model and its variants are case-sensitive. We use the python \texttt{truecase} package\footnote{https://pypi.org/project/truecase/} to restore the correct capitalization during preprocessing.


\subsubsection{Coreference Resolution}
\paragraph{CoNLL-12.}
The CoNLL-12 English shared task dataset for coreference resolution \cite{pradhan-etal-2012-conll} contains 2802 documents for training, 343 for validation, and 348 for testing. 
During training, we chunk the documents into segments of 2048 maximum words. 
In total, there are 2830 segments for training.
During the evaluation, we use the entire document as the input to the model.


\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[scale=0.9]
    \begin{axis}[
        xlabel= Ratio,
        ylabel= Recall (\%),
        y label style={at={(axis description cs:0.05,0.5)}},
        x label style={at={(axis description cs:0.5,0.03)}},
        xmajorgrids=true,
        ymajorgrids=true,
        legend cell align={left},
        legend style={at={(1,0.3)}},
        height=7cm
    ]
      \addplot [mark=none,color=blue] 
      coordinates {
        (0.1,  65.91)
        (0.2,  90.36)
        (0.25, 93.74)
        (0.3,  95.04)
        (0.4,  96.24)
        (0.5,  96.75)
        (0.6,  97.15)
    };
    \addplot [mark=*,color=blue,only marks] 
      coordinates {
        (0.4,  96.24)
    };
    \addplot [mark=square*,color=red,only marks] 
      coordinates {
        (0.0959,  89.43)
    };
    \legend{\texttt{Joshi} (various ratio)\\ \texttt{Joshi} (actual ratio)\\ \\}
    \end{axis}
\end{tikzpicture} 
\end{center}
\caption{Recall rate of gold mentions. 
The ratio on the -axis refers to the number of predicted mentions divided by . \texttt{Joshi} refers to the two-stage model of \citep{joshi-etal-2020-spanbert}.}
\label{fig:mention_detection}
\end{figure}


\begin{table*}[!ht]
\centering \small
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
                     & \multicolumn{3}{c}{MUC}     & \multicolumn{3}{c}{B}       & \multicolumn{3}{c}{CEAF}    &        \\ \cline{2-10} 
                   & P    & R    & F             & P    & R    & F             & P    & R    & F             &  Avg. F1        \\ \midrule
\citet{lee-etal-2017-end} & 78.4 & 73.4 & 75.8          & 68.6 & 61.8 & 65.0          & 62.7 & 59.0 & 60.8          & 67.2      \\ 
\citet{lee-etal-2018-higher} & 81.4 & 79.5 & 80.4          & 72.2 & 69.5 & 70.8          & 68.2 & 67.1 & 67.6          & 73.0      \\
\citet{joshi-etal-2019-bert} & 84.7 & 82.4 & 83.5          & 76.5 & 74.0 & 75.3          & 74.1 & 69.8 & 71.9          & 76.9      \\
 \citet{joshi-etal-2020-spanbert} & 85.8 & 84.8 & 85.3          & 78.3 & 77.9 & 78.1          & 76.4 & 74.2 & 75.3          & 79.6    \\
 \citeauthor{joshi-etal-2020-spanbert}  \hspace{-0mm} & 82.4 & 77.4 & 79.8         & 72.3 & 68.2 & 70.2          & 70.5 & 63.5 & 66.8          & 72.3       \\ 
 \citeauthor{joshi-etal-2020-spanbert}  & 85.5 & 77.7 & 81.4          & 78.3 & 68.5 & 73.1          & 75.0 & 65.9 & 73.1          & 74.9     \\ 
 \citeauthor{dobrovolskii-2021-word} & 84.9 & 87.9 & 86.3  & 77.4 & 82.6 &   79.9  & 76.1 & 77.1     & 76.6          & 81.0     \\ 
\midrule
\citeauthor{urbizu-etal-2020-sequence} & 
- & - & 64.9 &  - & - & 66.5 & - & - & 65.3 & 65.6   \\ 
 \citeauthor{tanl} & - & - & 81.0 &  - & - & 69.0 & - & - & 68.4 & 72.8   \\ 
\texttt{ASP} & 81.7  & 82.8 & 82.3 & 74.2 & 76.1 & 75.1 & 74.5 & 70.6 & 72.5 &  76.6  \\ 
\texttt{ASP} &  83.3 &  86.2 & 84.7 & 75.9 & 79.5 & 77.7 & 75.8 & 74.5 & 75.2 & 79.3  \\ 
\texttt{ASP} & 83.5 & 87.6  & 85.5 & 76.3 & 81.8 & 79.0 & 76.0 & 76.2 & 76.1 & 80.2  \\ 
\texttt{ASP} & 85.8  &  88.3 & 86.9 & 79.6 & 83.3 & 81.5 & 78.3 & 78.5 & 78.4 & 82.3   \\ 
\texttt{ASP} & 84.9  & 88.7 & 86.7 & 78.5 & 83.8 & 81.1 & 78.4 & 78.5 & 78.4 & 82.2 \\
\texttt{ASP} & 86.1  & 88.4 & 87.2 & 80.2 & 83.2 & 81.7 & 78.9 & 78.3 & 78.6 & \textbf{82.5} \\ \bottomrule
\end{tabular}} 
\caption{Full results on the CoNLL-12 English test set. Avg. F1 denotes the average F1 of MUC, B{}, and CEAF{}. 
Models marked with  are our re-implementation. Other results are taken from their original papers.}
\label{tab:result_test_conll}
\end{table*}
 
\section{Coreference Resolution} \label{appendix:mention_recall}
In this section, we analyze the performance of mention detection for coreference resolution of our model in \cref{fig:mention_detection}. This analysis casts light on how our model \emph{plans globally} in an autoregressive manner.
In the task of coreference resolution, only the entities that are mentioned more than once in a document are annotated as mentions. This is to say, an utterance of an entity should only be labeled if that entity is referred to again afterward. Thus, in previous coreference resolution models, a dedicated mention detection module that enumerates candidate textual spans (e.g., noun phrases and pronouns) for mentions is indispensable. 
However, our model is able to directly predict the exact set of mentions that we require, even if the target sequence is generated from left to right. We conclude that this results from the cross-attention mechanism which enables the model to look at relevant parts in the input document during decoding.
Given an input document of  words, our model predicts only  mentions with a  recall rate of gold mentions. This refrained mention detection strategy imposes a limit on the cardinality of  in \cref{eq:support_general}. As a result, this relatively small constant factor (compared to 0.4 used in most previous work) keeps our model tractable without the need for pruning strategies as in the models based on \cite{lee-etal-2017-end}. 


\section{Modeling More Restricted Structures} \label{appendix:parsing}
In this work, we tackled three tasks that are traditionally considered structured prediction problems. Named entity recognition and relation extraction consider labeling spans with a set of given types. 
Coreference resolution has long-range dependencies and has to model relationships between spans. 
However, there are structured prediction problems that require more restricted outputs. For instance, in dependency parsing, a spanning tree connecting every word in the input sentence is the desired output \cite{dependencyparsing}. While in constituency parsing, a parse tree in Chomsky Normal Form is supposed to be a complete binary tree except for the leaf nodes \cite{handbook-clnlp}.
Modeling such types of structures requires a more specified definition of task-specific actions. In future work, we aim to explore the abilities and limitations of our method.\looseness=-1


\section{Experiments with Flan-T5}
We conduct additional experiments with the latest pretrained language model Flan-T5 \cite{flant5}. Flan-T5 is pretrained on more supervised tasks and achieves better performance than the original T5 on multiple NLU tasks. 
The results are shown in \cref{tab:result_test_conll}, \cref{tab:result_test_ner_conll03_flant5}, and \cref{tab:result_test_ere_conll04_flant5}. We find that with the same size of the model, Flan-T5 performs better than T5 in general.

\begin{table}[t]
\centering
\resizebox{0.45\textwidth}{!}{
\small
\begin{tabular}{lccc}
\toprule
             & Prec. & Rec. & F1 \\ \midrule
\texttt{ASP} & 91.4 & 92.2 & 91.8 \\ 
{\texttt{ASP}} & 92.7 & 93.8 & 93.3 \\ 
{\texttt{ASP}} & 92.1  & 93.4  & 92.8 \\
{\texttt{ASP}} & 93.3 & 94.3 & 93.8 \\  
{\texttt{ASP}} & {93.8}  & {94.4} &  \textbf{94.1} \\ \bottomrule 
\end{tabular}} 
\caption{Test F1 scores of named entity recognition on the CoNLL-03 test set.}
\label{tab:result_test_ner_conll03_flant5}
\end{table}






\begin{table}[t]
\centering

\begin{tabular}{lcc}
\toprule
             & Ent & Rel \\ \midrule
\texttt{ASP} & 89.5 & 73.2\\ 
{\texttt{ASP}} & 89.4 & 73.8\\ 
{\texttt{ASP}} & 90.5 & 76.2 \\ \bottomrule
\end{tabular}
\caption{Test F1 scores of named entity recognition on the CoNLL-04 test set.}
\label{tab:result_test_ere_conll04_flant5}
\end{table}

\begin{table*}[h]
    \small
    \centering 
\resizebox{\textwidth}{!}{
    \begin{tabular}{p{1.6cm} p{\textwidth-1.6cm}} \toprule
       \textbf{named entity recognition}  &  \texttt{GUNMEN WOUND TWO  MANCHESTER UNITED  FANS IN  AUSTRIA .  VIENNA  1996-12-06 Two  Manchester United  soccer fans were wounded by unidentified gunmen on Friday and taken to hospital in the  Austrian  capital, police said. " The four  Britons  were shot at from a  Mercedes  car at around 1 a.m., " a spokeswoman told  Reuters . The two men were hit in the pelvis and leg. Police said their lives were not in danger. The fans, in  Austria  to watch their team play  Rapid Vienna  last Wednesday, may have been involved in a pub brawl earlier, the spokeswoman said.  Manchester United  won 2-0.</s>} \\ \midrule
       \textbf{end-to-end relation extraction}  &  \texttt{And this final story: retired  Senator   Strom Thurmond  has never made a secret about  his  fondness for young pretty  women  .</s>} \\ \midrule
       \textbf{coreference resolution}  &  \texttt{<speaker> - </speaker>  Al Gore  won't be the next U.S. President, but  he   has a slim chance of becoming  the next President at  Harvard  .  Gore  holds a degree from  the university , and is one of about 500 people nominated for  the job .  A school official  talked about  the Vice President's  chances during an interview with " the Boston Globe. "   He  says it's unlikely  Gore  will be selected,  because  he  doesn't have enough experience in the academic world.</s>} \\ \cmidrule{2-2}
       & \texttt{<speaker> - </speaker>  Violence between Israelis and Palestinians  continued in  its  third month, though at a slightly reduced level overall.  Israeli and Palestinian negotiators  met separately at the White House with President Bill Clinton in hopes of restarting direct negotiations between  them  for a final settlement.</s>} \\
       \bottomrule
    \end{tabular}
    }
    \caption{Predicted sequences from CoNLL-03, ACE-05, and CoNLL-12 dataset.}
    \label{tab:qualitative_example}
\end{table*}

\section{Decoding Examples}
We provide decoding examples from the tasks we experiment on in \cref{tab:qualitative_example}. The  actions are verbalized into tokens.



\end{document}
