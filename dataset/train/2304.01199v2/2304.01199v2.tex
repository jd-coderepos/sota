\section{Experiments}
\label{sec:experiments}

We evaluate our method on AVA~\cite{gu2018ava} in various settings. AVA~\cite{gu2018ava} poses an action detection problem, where people are localized in a spatio-temporal volume with action labels. It provides annotations at 1Hz, and each actor will have 1 pose action, up to 3 person-object interactions (optional), and up to 3 person-person interaction (optional) labels. For the evaluations, we use AVA v2.2 annotations and follow the standard protocol as in ~\cite{gu2018ava}. We measure mean average precision (mAP) on 60 classes with a frame-level IoU of 0.5. In addition to that, we also evaluate our method on AVA-Kinetics~\cite{li2020ava} dataset, which provides spatio-temporal localized annotations for Kinetics videos.

 \pdfoutput=1
We use PHALP~\cite{rajasegaran2022tracking} to track people in the AVA dataset. PHALP falls into the tracking-by-detection paradigm and uses Mask R-CNN~\cite{he2017mask} for detecting people in the scene. At the training stage, where the bounding box annotations are available only at 1Hz, we use Mask R-CNN detections for the in-between frames and use the ground-truth bounding box for every 30 frames. For validation, we use the bounding boxes used by ~\cite{pan2021actor} and do the same strategy to complete the tracking. We ran, PHALP on Kinetics-400~\cite{kay2017kinetics} and AVA~\cite{gu2018ava}. Both datasets contain over 1 million tracks with an average length of 3.4s and over 100 million detections. In total, we use about 900 hours length of tracks, which is about 40x more than previous works~\cite{kanazawa2019learning}. See Table~\ref{tbl:phalp_tracks} for more details. 

Tracking allows us to train actions densely. Since, we have tokens for each actor at every frame, we can supervise every token by assuming the human action remains the same in a \textit{1 sec} window~\cite{gu2018ava}. First, we pre-train our model on Kinetics-400 dataset~\cite{kay2017kinetics} and AVA~\cite{gu2018ava} dataset. We run MViT~\cite{fan2021multiscale} (pre-trained on MaskFeat~\cite{wei2022masked}) at 1Hz on every track in Kinetics-400 to generate pseudo ground-truth annotations. Every 30 frames will share the same annotations and we train our model end-to-end with binary cross-entropy loss. Then we fine-tune the pretrained model, with \textit{tracks} generated by us, on AVA ground-truth action labels. At inference, we take a track, and randomly sample  of other tracks from the same video and pass it through the model. We take an average pooling on the prediction head over a sequence of  frames, and evaluate at the center-frame. For more details on model architecture, hyper-parameters, and training procedure/training-time please see Appendix A1.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/FIG1.png}
    \caption{\textbf{Class-wise performance on AVA:} We show the performance of JMRN~\cite{shah2022pose} and \methodnameA \ on 60 AVA classes (average precision and relative gain). For pose based classes such as \textit{standing}, \textit{sitting}, and \textit{walking} our 3D pose model can achieve above 60 mAP average precision performance by only looking at the 3D poses over time. By modeling multiple trajectories as input our model can understand the interactions among people. For example, activities such as \textit{dancing} (\textcolor{teal}{+30.1\%}), \textit{martial art} (\textcolor{teal}{+19.8\%}) and \textit{hugging} (\textcolor{teal}{+62.1\%}) have large relative gains over state-of-the-art pose only model. We only plot the gains if it is above or below 1 mAP.}
    \label{fig:single_ve_multi_pose}
    \vspace{-0.4cm}
\end{figure*}


\begin{table}[!t]
\begin{center}
\small
\vspace{5pt}
\begin{tabular}{l c c c }
\toprule[0.4mm]
Dataset & \# clips & \# tracks & \# bbox \\ \midrule
AVA~\cite{gu2018ava} & 184k & 320k & 32.9m \\
Kinetics~\cite{kay2017kinetics} & 217k & 686k & 71.4m \\ \midrule
Total & 400k & 1m & 104.3m\\
\bottomrule[0.4mm]
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{Tracking statistics on AVA~\cite{gu2018ava} and Kinetics-400~\cite{kay2017kinetics}:} We report the number tracks returned by PHALP~\cite{rajasegaran2022tracking} for each datasets (m: million). This results in over 900 hours of tracks, with a mean length of 3.4 seconds (with overlaps). }
\label{tbl:phalp_tracks}
\end{table}


\subsection{Action Recognition with 3D Pose}
\label{sec:experiments_pose_only}

In this section, we discuss the performance of our method on AVA action recognition, when using 3D pose cues, corresponding to Section~\ref{sec:method_pose}. We train our 3D pose model \textbf{\methodnameA}, on Kinetics-400 and AVA datasets. For Kinetics-400 tracks, we use MaskFeat~\cite{wei2022masked} pseudo-ground truth labels and for AVA tracks, we train with ground-truth labels. We train a single person model and a multi-person model to study the interactions of a person over time, and person-person interactions. Our method achieves 24.1 mAP on multi-person (N=5) setting (See Table~\ref{tbl:pose_only}). While this is well below the state-of-the-art performance, this is a first time a 3D model achieves more than 15.6 mAP on AVA dataset. Note that the first reported performance on AVA was 15.6 mAP~\cite{gu2018ava}, and our 3D pose model is already above this baseline. 


\begin{table}[!t]
\begin{center}
\small
\begin{tabular}{l c c c c c}
\toprule[0.4mm]
Model & Pose &  OM  & PI & PM & mAP \\ \midrule
PoTion~\cite{choutas2018potion}        & 2D        & - & - & - & 13.1 \\ 
JMRN~\cite{shah2022pose}               & 2D        & 7.1   & 17.2 & 27.6 & 14.1 \\ \midrule 
\methodnameA                                   & 3D (n=1)    &  12.0 & 22.0 & 46.6 & 22.9 \\
\methodnameA                                   & 3D (n=5)    &  13.3 & 25.9 & 48.7 & 24.1 \\
\bottomrule[0.4mm]
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{AVA Action Recognition with 3D pose:} We evaluate human-centric representation on AVA dataset~\cite{gu2018ava}. Here \textit{OM} : Object Manipulation, \textit{PI} : Person Interactions, and \textit{PM} : Person Movement. \methodnameA can achieve about 80\% performance of MViT models on person movement tasks without looking at scene information.}
\vspace{-0.2cm}
\label{tbl:pose_only}
\end{table}

We evaluate the performance of our method on three AVA sub-categories (Object Manipulation (\textit{OM}), Person Interactions (\textit{PI}), and Person Movement(\textit{PM})). For the person-movement task, which includes actions such as \textit{running}, \textit{standing}, and \textit{sitting} etc., the 3D pose model achieves 48.7 mAP. In contrast, MaskFeat performance in this sub-category is 58.6 mAP. This shows that the 3D pose model can perform about 80\% good as a strong state-of-the-art model. On the person-person interaction category, our multi-person model achieves a gain of +2.1 mAP compared to the single-person model, showing that the multi-person model was able to capture the person-person interactions. As shown in the Fig~\ref{fig:single_ve_multi_pose}, for person-person interactions classes such as \textit{dancing}, \textit{fighting}, \textit{lifting a person} and \textit{handshaking} etc., the multi-person model performs much better than the current state-of-the-art pose-only models. For example, in \textit{dancing} multi-person model gains +39.8 mAP, and in \textit{hugging} the relative gain is over +200\%. In addition to that, the multi person model has the largest gain compared to the single person model in the person interactions category.

On the other hand, object manipulation has the lowest score among these three tasks. Since we do not model objects explicitly, the model has no information about which object is being manipulated and how it is being associated with the person. However, since some tasks have a unique pose when interacting with objects such as \textit{answering a phone} or \textit{carrying an object}, knowing the pose would help in identifying the action, which results in 13.3 mAP.

\subsection{Actions from Appearance and 3D Pose}

While the 3D pose model can capture about 50\% performance compared to the state-of-the-art methods, it does not reason about the scene context. To model this, we concatenate the human-centric 3D representation with feature vectors from MaskFeat~\cite{wei2022masked} as discussed in Section~\ref{sec:method_pose_apperance}. MaskFeat has a MViT2~\cite{li2021improved} as the backbone and it learns a strong representation about the scene and contextualized appearance. First, we pretrain this model on Kinetics-400~\cite{kay2017kinetics} and AVA~\cite{gu2018ava} datasets, using the pseudo ground truth labels. Then, we fine-tune this model on AVA tracks using the ground-truth action annotation. 

In Table~\ref{tbl:results_sota} we compare our method with other state-of-the-art methods. Overall our method has a gain of \textbf{+2.8 mAP} compared to Video MAE~\cite{feichtenhofer2022masked, tong2022videomae}. In addition to that if we train with extra annotations from AVA-Kinetics our method achieves \textbf{42.3 mAP}. Figure~\ref{fig:results_sota} show the class-wise performance of our method compared to MaskFeat~\cite{wei2022masked}. Our method overall improves the performance of 56 classes in 60 classes. For some classes (e.g. \textit{fighting}, \textit{hugging}, \textit{climbing}) our method improves the performance by more than +5 mAP. In Table~\ref{tbl:AVA-K} we evaluate our method on AVA-Kinetics~\cite{li2020ava} dataset. Compared to the previous state-of-the-art methods our method has a gain of +1.5 mAP.

In Figure~\ref{fig:results_qualitative}, we show qualitative results from MViT~\cite{fan2021multiscale} and our method. As shown in the figure, having explicit access to the tracks of everyone in the scene allow us to make more confident predictions for actions like \emph{hugging} and \emph{fighting}, where it is easy to interpret close interactions. In addition to that, some actions like \emph{riding a horse} and \emph{climbing} can benefit from having access to explicit 3D poses over time. Finally, the amodal nature of 3D meshes also allows us to make better predictions during occlusions. 


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/FIG_FINAL.png}
    \caption{\textbf{Comparison with State-of-the-art methods:} We show class-level performance (average precision and relative gain) of MViT~\cite{fan2021multiscale} (pretrained on MaskFeat~\cite{wei2022masked}) and ours. Our methods achieve better performance compared to MViT on over 50 classes out of 60 classes. Especially, for actions like \textit{running}, \textit{fighting}, \textit{hugging}, and \textit{sleeping} etc., our method achieves over \textbf{+5 mAP}. This shows the benefit of having access to explicit tracks and 3D poses for action recognition. We only plot the gains if it is above or below 1 mAP.}
    \label{fig:results_sota}
\end{figure*}

\begin{table}[!h]
\begin{center}
\small
\begin{tabular}{l c l}
\toprule[0.4mm]
Model & Pretrain & mAP \\ \midrule
SlowFast R101, 8×8~\cite{feichtenhofer2019slowfast}        & \multirow{2}{*}{K400}  & 23.8 \\
MViTv1-B, 64×3~\cite{fan2021multiscale}                            &                                          & 27.3 \\ \midrule
SlowFast 16×8 +NL~\cite{feichtenhofer2019slowfast}       &  \multirow{5}{*}{K600} & 27.5 \\
X3D-XL~\cite{feichtenhofer2020x3d}                                    &                                          & 27.4 \\
MViTv1-B-24, 32×3~\cite{fan2021multiscale}                      &                                           & 28.7 \\
Object Transformer~\cite{wu2021towards}                         &                                           & 31.0 \\
ACAR R101, 8×8 +NL~\cite{pan2021actor}                         &                                          & 31.4 \\ \midrule
ACAR R101, 8×8 +NL~\cite{pan2021actor}                         &  \multirow{1}{*}{K700} & 33.3 \\ \midrule
MViT-L↑312, 40×3~\cite{li2021improved},                           &  IN-21K+K400       & 31.6 \\ 
MaskFeat~\cite{wei2022masked}                                          &  K400                               & 37.5 \\
MaskFeat~\cite{wei2022masked}                                          &  K600                               & 38.8 \\
Video MAE~\cite{feichtenhofer2022masked,tong2022videomae}                                  &  K600                               & 39.3 \\ 
Video MAE~\cite{feichtenhofer2022masked,tong2022videomae}                                  &  K400                               & 39.5 \\  
Hiera~\cite{ryali2023hiera}                                  &  K700                               & 42.3 \\  \midrule
\methodnameB - MViT                                               &  K400                               & 42.6 \\         
\methodnameB - Hiera                                               &  K700                               & 45.1 \\         
\bottomrule[0.4mm]
\end{tabular}
\end{center}
\vspace{-0.2cm}
\caption{\textbf{Comparison with state-of-the-art methods on AVA 2.2:}. Our model uses features from MaskFeat~\cite{wei2022masked} with full crop inference. Compared to Video MAE~\cite{feichtenhofer2022masked, tong2022videomae} our method achieves a gain of \textbf{+2.8 mAP}, and with Heira~\cite{ryali2023hiera} backbone our model achieves 45.1 mAP with a gain of \textbf{+2.5 mAP}.}
\vspace{-5pt}
\label{tbl:results_sota}
\end{table}



\begin{table}[!h]
\begin{center}
\small
\begin{tabular}{l c}
\toprule[0.4mm]
Model & mAP \\ \midrule
SlowFast~\cite{feichtenhofer2019slowfast} & 32.98 \\ \midrule
ACAR~\cite{pan2021actor} & 36.36  \\ \midrule
RM~\cite{feng2021relation} & 37.34 \\ \midrule
\methodnameB+MViT & \textbf{38.91}  \\
\bottomrule[0.4mm]
\end{tabular}
\end{center}
\vspace{-5pt}
\caption{\textbf{Performance on AVA-Kinetics Dataset.} We evaluate the performance of our model on AVA-Kinetics~\cite{li2020ava} using a single model (no ensembles) and compare the performance with previous state-of-the-art single models. }
\vspace{-0.5cm}
\label{tbl:AVA-K}
\end{table}

\subsection{Ablation Experiments}

\paragraph{Effect of tracking:} All the current works on action recognition do not associate people over time, explicitly. They only use the mid-frame bounding box to predict the action. For example, when a person is \textit{running} across the scene from left to right, a feature volume cropped at the mid-frame bounding box is unlikely to contain all the information about the person. However, if we can track this person we could simply know their exact position over time and that would give more localized information to the model to predict the action. 


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{figs/Q_4_1.pdf}
    \caption{\textbf{Qualitative Results:} 
    We show the predictions from MViT~\cite{fan2021multiscale} and our model on validation samples from AVA v2.2. The person with the colored mesh indicates the person-of-interest for which we recognise the action and the one with the \tikzmarknode[draw,inner sep=2pt,rounded corners,fill=gray!30]{A}{gray mesh} indicates the supporting actors.
    The first two columns demonstrate the benefits of having access to the action-tubes of other people for action prediction. In the first column, the \tikzmarknode[draw,inner sep=2pt,rounded corners,fill=orange!30]{A}{orange person} is very close to the other person with hugging posture, which makes it easy to predict \emph{hugging} with higher probability. Similarly, in the second column, the explicit interaction between the multiple people, and knowing others also fighting increases the confidence for the \emph{fighting} action for the \tikzmarknode[draw,inner sep=2pt,rounded corners,fill=green!30]{A}{green person} over the 2D recognition model. The third and the fourth columns show the benefit of explicitly modeling the 3D pose over time (using tracks) for action recognition. Where the \tikzmarknode[draw,inner sep=2pt,rounded corners,fill=yellow!30]{A}{yellow person} is in riding pose and \tikzmarknode[draw,inner sep=2pt,rounded corners,fill=Thistle!30]{A}{purple person} is looking upwards and legs on a vertical plane. The last column indicates the benefit of representing people with an amodal representation. Here the hand of the \tikzmarknode[draw,inner sep=2pt,rounded corners,fill=RoyalBlue!30]{A}{blue person} is occluded, so the 2D recognition model does not see the action as a whole. However, SMPL meshes are amodal, therefore the hand is still present, which boosts the probability of predicting the action label for \emph{closing the door}.}
    \label{fig:results_qualitative}
\end{figure*}


To this end, first, we evaluate MaskFeat~\cite{wei2022masked} with the same detection bounding boxes~\cite{pan2021actor} used in our evaluations, and it results in 40.2 mAP. With this being the baseline for our system, we train a model which only uses MaskFeat features as input, but over time. This way we can measure the effect of tracking in action recognition. Unsurprisingly, as shown in Table~\ref{tbl:main_ablation} when training MaskFeat with tracking, the model performs +1.2 mAP better than the baseline. This clearly shows that the use of tracking is helpful in action recognition. Specifically, having access to the tracks help to localize a person over time, which in return provides a second order signal of how joint angles changes over time. In addition, knowing the identity of each person also gives a discriminative signal between people, which is helpful for learning interactions between people.

\paragraph{Effect of Pose:} The second contribution from our work is to use 3D pose information for action recognition. As discussed in Section~\ref{sec:experiments_pose_only} by only using 3D pose, we can achieve 24.1 mAP on AVA dataset. While it is hard to measure the exact contribution of 3D pose and 2D features, we compare our method with a model trained with only MaskFeat and tracking, where the only difference is the use of 3D pose. As shown in Table~\ref{tbl:main_ablation}, the addition of 3D pose gives a gain of +0.8 mAP. While this is a relatively small gain compared to the use of tracking, we believe with more robust and accurate 3D pose systems, this can be improved. 


\begin{table}[!h]
\begin{center}
\small
\vspace{5pt}
\begin{tabular}{c c c c l}
\toprule[0.4mm]
Model & OM & PI & PM & mAP \\ \midrule
MViT                   & 32.2 & 41.1 & 58.6 & 40.2 \\
MViT + Tracking        & 33.4 & 43.0 & 59.3 & 41.4 \\
MViT + Tracking + Pose & 34.4 & 43.9 & 59.9 & 42.3 \\
\bottomrule[0.4mm]
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{Ablation on the main components:} We ablate the contribution of tracking and 3D poses using the same detections. First, we only use MViT features over the tracks to evaluate the contribution from tracking. Then we add 3D pose features to study the contribution from 3D pose for action recognition.} 
\vspace{-0.4cm}
\label{tbl:main_ablation}
\end{table}




\subsection{Implementation details}
In both the pose model and pose+appearance model, we use the same vanilla transformer architecture~\cite{vaswani2017attention} with 16 layers and 16 heads. For both models the embedding dimension is . We train with 0.4 mask ratio and at test time use the same mask token to in-fill the missing detections. The output token from the transformer is passed to a linear layer to predict the AVA action labels. We pre-train our model on kinetics for 30 epochs with MViT~\cite{fan2021multiscale} predictions as pseudo-supervision and then fine-tune on AVA with AVA ground truth labels for few epochs. We train our models with AdamW~\cite{loshchilov2017decoupled} with base learning rate of  and . We use cosine annealing scheduling with a linear warm-up. For additional details please see the Appendix.



\subsection{Hiera backbone results}
\label{sec:hiera_results}


In this section, we show the class-wise performance when Hiera~\cite{ryali2023hiera} is used to extract contextualized appearance features. Our model achieves \textbf{2.5 mAP} gain over the Hiera baseline. Overall over method achieves 45.1 mAP and significant gains on multiple action classes. Table~\ref{tbl:hiera_table} shows task wise performance of the Hiera model and the performance of LART. In all three categories, LART performs much better than the baseline model, and LART shows a significant gain in person-interaction tasks.


\begin{table}[!h]
\begin{center}
\small
\begin{tabular}{l c c c c}
\toprule[0.4mm]
Model &  OM  & PI & PM & mAP \\ \midrule
MViT~\cite{wei2022masked}              & 32.2 & 41.1 & 58.6 & 40.5 \\ 
\methodnameB+MViT                      & 34.5 & 44.0 & 59.9 & 42.3 \\ \midrule 
Hiera~\cite{ryali2023hiera}            & 34.5 & 43.0 & 62.3 & 42.6 \\
\methodnameB+Hiera                     & 37.3 & 45.7 & 63.8 & 45.1 \\
\bottomrule[0.4mm]
\end{tabular}
\end{center}
\vspace{-10pt}
\caption{\textbf{Effect of Backbones:} AVA dataset~\cite{gu2018ava} contains \textit{OM} : Object Manipulation, \textit{PI} : Person Interactions, and \textit{PM} : Person Movement tasks. We compare the action detection performance of the backbones (MViT~\cite{wei2022masked} and Hiera~\cite{ryali2023hiera}) and the performance when combined with LART. }
\vspace{-0.2cm}
\label{tbl:hiera_table}
\end{table}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.98\textwidth]{figs/FIG_FINAL_Hiera2.png}
    \caption{\textbf{Comparison with Hiera~\cite{ryali2023hiera}:} We show class-level performance (average precision and relative gain) of Hiera~\cite{ryali2023hiera} (pre-trained on using MAE~\cite{he2021masked}) and ours. Our methods achieve better performance compared to Hiera in 53 classes out of 60 classes. Especially, for actions involving object interactions, and pose changes such as running. fighting, our method achieves over 10\% relative gain on mean average precision. This shows the benefit of having access to explicit tracks and 3D poses for action recognition. We only plot the gains if it is above or below 1 mAP.}
    \label{fig:results_sota_mvit}
\end{figure*}

