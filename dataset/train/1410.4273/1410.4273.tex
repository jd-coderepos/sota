

\documentclass[final,leqno,onefignum,onetabnum]{siamltex1213}

\usepackage{hyperref}
\usepackage{url}
\usepackage{array}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{breqn}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algorithmic}
\usepackage{arydshln}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{float}
\usepackage{listings,url,verbatim}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{afterpage}
\newtheorem{prop}[theorem]{Proposition}

\title{An Efficient Algorithm for Unweighted Spectral Graph Sparsification\thanks{This work was partially supported by NSF Award CCF-131912}}


\author{David G. Anderson\thanks{Department of Mathematics, University of California, Berkeley, California 94720 (\email{anderson@math.berkeley.edu}, \email{mgu@math.berkeley.edu}, \email{melgaard@math.berkeley.edu}).} \and Ming Gu\footnotemark[2] \and Christopher Melgaard\footnotemark[2]}


\begin{document}
\maketitle
\slugger{sicomp}{xxxx}{xx}{x}{x--x}

\begin{abstract}
Spectral graph sparsification has emerged as a powerful
tool in the analysis of large-scale networks by reducing the overall
number of edges, while maintaining a comparable graph Laplacian
matrix. In this paper, we present an efficient algorithm for the
construction of a new type of spectral sparsifier, the
\emph{unweighted} spectral sparsifier. Given a general undirected and unweighted
graph , and an integer  (the
number of edges in ), we compute an unweighted graph  with  and  such that for every  
 
where  and  are the Laplacian matrices for  and ,
respectively, and  is a slowly-varying
function of  and . This work addresses the
open question of the existence of \emph{unweighted} graph
sparsifiers for unweighted
graphs~\cite{ramanujansparse}.  Additionally, our algorithm efficiently computes
unweighted graph sparsifiers for weighted graphs, leading to
sparsified graphs that retain the weights of the original graphs.
\end{abstract}

\begin{keywords}graph sparsification, spectral graph theory, spectral sparsification, unweighted graph sparsification\end{keywords}

\begin{AMS} 68R10, 90C35, 15A18, 15B34, 15B48\end{AMS}


\pagestyle{myheadings}
\thispagestyle{plain}
\markboth{AN ALGORITHM FOR UNWEIGHTED GRAPH SPARSIFICATION}{D. G. ANDERSON, M. GU, AND C. MELGAARD}

\section{Introduction}

Graph sparsification seeks to approximate a graph  with a graph  on the same vertices, but with fewer edges.  Called a sparsifier,  requires less storage than  and serves as a proxy for  in computations where  is too large, evoking the effectiveness of sparsifiers in wide-ranging applications of graphs, including social networks, conductance, electrical networks, and similarity \cite{conf/soda/ChierichettiLP10,DBLP:conf/stoc/ChristianoKMST11,conf/kdd/MathioudakisBCGU11,st1}.  In some applications, graph sparsification improves the quality of the graph, such as in the design of information networks and the hardwiring of processors and memory in parallel computers \cite{journals/cacm/BatsonSST13,4495}.  Sparsifiers have also been utilized to find approximate solutions of symmetric, diagonally-dominant linear systems in nearly-linear time \cite{journals/cacm/BatsonSST13,journals/corr/cs-DS-0310036,st1,journals/corr/abs-cs-0607105}.

Recent work on graph sparsification includes \cite{agm,conf/innovations/KapralovP12,ss1,st1,spectsim}.  Batson, Spielman, and Srivastava \cite{ramanujansparse} prove that for every graph there exists a spectral sparsifier where the number of edges is linear in the number of vertices.  They further provide a polynomial-time, deterministic algorithm for the sparsification of weighted graphs, which could produce weights that differ greatly from the weights of the original graph.  The work of Avron and Boutsidis \cite{fasterSub} explores unweighted sparsification in the context of finding low-stretch spanning trees.  They provide a greedy edge removal algorithm and a volume sampling algorithm with theoretical guarantees.  In comparison, our novel greedy edge selection algorithm has tighter theoretical bounds for both spanning trees and in the more general context of unweighted graph sparsification.

Our work introduces a deterministic, greedy edge selection algorithm to calculate sparsifiers for weighted and unweighted graphs.  Our algorithm selects a subset of edges for the sparse approximation , without assigning or altering weights.  While the Dual Set algorithms of \cite{fasterSub, ramanujansparse, nearoptcol} reweight all selected edges for computing \emph{weighted sparsifiers}, our algorithm produces \emph{unweighted sparsifiers} for an unweighted input graph, and can create a weighted sparsifier for a weighted input graph by assigning the original edge weights to the sparsifier.  Hence our concept of unweighted sparsification applies to both unweighted and weighted graphs.  To formalize:

\begin{definition} 
Let  be a given graph\footnote{Note that any unweighted graph  induces a weighted graph  where  if  and
 otherwise.
}. We define an
{\bf unweighted sparsification} of  to be any graph of the form , where 

is the indicator function and
 is the Hadamard product, i.e.

\end{definition}

Several definitions have been proposed for the notion in which a sparsifier approximates a dense graph.  Benzc\'{u}r and Karger \cite{conf/stoc/BenczurK96} introduced cut sparsification, where the sum of the weights of the edges of a cut dividing the set of vertices is approximately the same for the dense graph and the sparsifier.  Spielman and Teng \cite{spectsim} proposed spectral sparsification, a generalization of cut sparsification, which seeks sparsifiers with a Laplacian matrix close to that of the input graph.  We follow the work of \cite{ramanujansparse,spectsim} and base our work on spectral sparsification, for which we now present a rigorous definition.

Given an undirected graph , define the signed edge-vertex incidence matrix  as

where all edges are randomly assigned a direction, and  is an edge from  to .  Define the diagonal weight matrix 

The Laplacian of the graph is

Note that 

for a vector .  To compare Laplacians of graphs  and  defined
on the same set of nodes we denote

\begin{definition} 
The graph  is a {\bf -approximation} of  if

\end{definition}
Because our unweighted sparsification algorithm does not change the weights of
the edges kept in , it is immediate that :
\begin{prop} \label{prop:upp}
If  is an unweighted sparsification of , then

\proof

for all .
\hspace{2 em}\endproof
\end{prop}

Our algorithm does not operate directly on the Laplacian matrix.  Rather, we consider the SVD of .



\noindent where  is a diagonal matrix containing all non-zero singular values of ; and where  is a row orthonormal matrix, with , and  being the number of connected components in . For the unweighted graph,  is simply the
identity matrix.  plays a similar role to that of the matrix
 in~\cite{ramanujansparse} and the matrix  in \cite{fasterSub}.  Our algorithm utilizes the column-orthogonality of , highlighting the reason for not working directly with the Laplacian matrix.  We note, nevertheless, that this algorithm can be adapted to any orthogonal decomposition of .

We are now in a position to present our main results.  In section 2 we present the unweighted column selection algorithm, as well as the spectral bounds for the sparsifiers it calculates.  Section 3 provides supporting theory.  Comparisons to other modern algorithms are made in section 4.  In section 5 we demonstrate one application of graph sparsification, graph visualization, by applying our algorithm to real autonomous systems data.  Some observations and concluding remarks are offered in sections 6 and 7.

\section{The Unweighted Column Selection (UCS) Algorithm}
Our algorithm selects edges for a sparsifier based on the columns  of ,

where  is the number of edges, and , as above. Therefore, the edges of  that
are included in the sparsifier are exactly the columns of  that our algorithm selects. Denote
the number of edges kept as .  Let  denote the set of selected edges after  iterations.

We propose the following greedy algorithm for column selection on .  Initially set  and , and
choose a constant . At step :
\begin{itemize}
\item Solve for the unique  such that

\item Solve for the unique  such that

where  is the  largest eigenvalue of the symmetric matrix .
\item Find an index  such that 

\item Update  and . 
\end{itemize}

While equations (\ref{step1}) and (\ref{step2}) are relatively straightforward to justify and solve, equation (\ref{step3}) requires careful consideration, and is the focus of much of section 3.  Note that equation () can be solved in  operations, equation () in  operations, and equation () in  operations.  This last complexity count follows because testing the inequality scales with , and potentially all remaining indices must be tested.  Thus the total complexity of selecting  columns is .

While this procedure will work for any , we will show that an effective choice is

where 

and where  is the minimizer of , given as

Our spectral bounds are derived using this choice of .  We summarize this procedure in the Unweighted Column Selection algorithm.

\begin{algorithm}
{\sc Algorithm: Unweighted Column Selection (UCS)}

\renewcommand{\algorithmicrequire}{\textbf{Inputs:}}
\renewcommand{\algorithmicensure}{\textbf{Outputs:}}
\renewcommand{\algorithmicprint}{\textbf{break}}
\refstepcounter{algorithm}
    \label{algo:unweighted}
    \begin{algorithmic}[1]
    \REQUIRE , , .
    \ENSURE 
    \STATE Calculate the column-orthogonal matrix 
    \STATE Set , 
    \FOR{}
        \STATE Solve for  using equation ()
        \STATE Calculate  using equation ()
        \STATE Find  such that inequality () is satisfied \label{find_i}
        \STATE Update 
        \STATE Update 
    \ENDFOR
		\STATE Let  be the selected edges
    \end{algorithmic}
\end{algorithm}

Theorem~\ref{thm:unweighted} below confirms the correctness of the Unweighted Column Selection Algorithm. This theorem, along with other properties of 
the UCS algorithm, will be discussed and proved in Section~\ref{Sec:Properties}.
\begin{theorem} \label{thm:unweighted}
Let  and let . Then the sparsified graph  produced by 
the UCS algorithm satisfies

where 

\end{theorem}
 


\section{Correctness and Performance of the UCS Algorithm}\label{Sec:Properties}
The goal of this section is to prove Theorem~\ref{thm:unweighted}.  Section \ref{sec:31} establishes that the UCS algorithm is well-defined.  Section \ref{sec:32} proves a lower bound for the minimum singular value of the submatrix selected by the UCS algorithm, and provides a good choice for the input parameter .  In section \ref{sec:33}, the UCS algorithm is shown to be a graph sparsification algorithm.

\subsection{The Existence of a Solution to Equation~(\ref{step3})}\label{sec:31}
The next two lemmas show that equation~(\ref{step3}) always has a solution.
\begin{lemma}\label{lemma:1}
At a given iteration  in the UCS algorithm, at step  define

Then there exists , with , such that .  Furthermore,
\small

\end{lemma}
\normalsize
\proof
Clearly .  Although  is undefined at
, let , where
.  Note that

because the last term in each sum will dominate the rest of the sum.  Furthermore,

Hence for small , we have , and, therefore,  exists, with , and  via the Intermediate Value Theorem.  Note that if there exists  such that , then we repeat the same argument replacing the expression  with .\\

Now we prove inequality (\ref{eqn:gz}).  We use the following version of the Cauchy-Schwartz formula: for  then . Consequently

where the last step comes from 
. The strict inequality above holds
because  After some simple algebra,
\small

\normalsize
which implies our desired inequality because .
\hspace{2 em}\endproof

Next, we show that our algorithm is well defined in the sense we can always find a new index  for each iteration that
satisfies .
\begin{lemma}\label{lemma:2}
An index  can always be found to satisfy line
 of the UCS algorithm for .
\end{lemma}
\proof Note the two following partial fraction results


Using the fact that , followed by the inequality of Lemma , we have
\footnotesize
 	
\normalsize
where the last line follows from equations (\ref{eqn1}) and (\ref{eqn2}).  After some rearranging:


This inequality can be rewritten using the trace property  and the identity :
\footnotesize

\normalsize
Moving terms to the right and dividing by  (because )  gives

For this to be true, there must exist an  such that

This last relation gives

where the last line was accomplished with the trace property previously indicated and the Sherman-Morrison formula.
\hspace{2 em}\endproof

\subsection{Lower Bound on }\label{sec:32}

Lemma~\ref{lemma:2} ensures that the UCS algorithm can
indeed find all  indices. We now estimate an eigenvalue lower
bound on . Let , 
and  represent the values of ,
 and , respectively, determined in
iteration .  Then note that by the definitions of  and
 we have

Define the following quantity and functions:

To bound , we first establish a recurrence relation on . 
\begin{lemma} \label{lem:tech}
After the last iteration of the UCS algorithm, we have

\end{lemma}
\proof
Remember that , and note that 

The equation  gives

Applying equation (\ref{eqn:part}) to both sides:

Since 

we have

Inequality (\ref{eqn:dec}) follows by noting that the
terms in the sum are decreasing in .  The final
substitution is necessary because solving the preceding recurrence
relation is impractical.  To further simplify calculations, we define

Therefore,


Next, to demonstrate the effectiveness of the algorithm, we derive a lower bound for  after  iterations.  This analysis will
involve selecting an appropriate  to maximize the lower bound.
\begin{lemma} \label{lem:lowbound}
If , then 

\end{lemma}
\proof A key observation is that  is \emph{strictly} convex in ,
which is easily verified by showing that the second derivative
 is positive by our assumptions that
 and . Next, we apply Jensen's
Inequality for discrete sums~\cite{zorich} to the recurrence relation in Lemma \ref{lem:tech}:

Along with  from Lemma \ref{lemma:1},  this finally leads to 
 
The expression on the right-hand side of~(\ref{eqn:lF}) is monotonically increasing
in . So, maximizing  will also
maximize the lower bound on .
\begin{lemma} \label{lem:max}
The function  is maximized at

\end{lemma}
\proof
Setting the derivative of  to zero:

Solving for the desired root:

We see that  is the global maximum on the region  via the first derivative test since  for  and  for .
\hspace{2 em}\endproof

We remark that combining () and () implies that the UCS algorithm should choose 
 for effective column selection. We are now ready to estimate .
\begin{theorem} \label{thm:lower}
If  is chosen according to Lemma \ref{lem:max} in the UCS algorithm, then

where  is defined in~(\ref{Eqn:alpha}).
\end{theorem} 
\proof We wish to apply our choice of  to Lemma \ref{lem:lowbound}. We satisfy the assumption

Therefore, plugging  into (\ref{eqn:lF}) of Lemma \ref{lem:lowbound}: 



\subsection{Correctness of the Unweighted Column Selection Algorithm}\label{sec:33}

We are now in a position to prove Theorem \ref{thm:unweighted}. Our
arguments are similar to those of the weighted sparsifier algorithm in~\cite{ramanujansparse}.

{\em Proof of Theorem \ref{thm:unweighted}.} By
Proposition~\ref{prop:upp}, we only need to show . Consider the SVD of  in equation~(\ref{PolarMat}), and let  be any vector such that . Then 


 
It follows that 
 
On the other hand, by construction we have 

With equation~(\ref{Eqn:y}), the Courant-Fisher min-max property gives

where the last line is due to Theorem \ref{thm:lower}.
\hspace{2 em}\endproof

\section{Performance Comparison of UCS and Other Algorithms}
This section compares the bound (\ref{Eqn:alpha}) to bounds of other current methods.  

\subsection{Comparison with Twice-Ramanujan Sparsifiers}
Given a weighted graph , the algorithm of \cite{ramanujansparse} produces a sparsified graph , where  is a subset of  and  contains new edge weights, such that 

where the parameter  is defined via the equation . 

By choosing  to be a moderate and dimension-independent constant,
equation (\ref{Eqn:Hw}) asserts that every graph  has a \emph{weighted} spectral sparsifier with a number of
edges linear in . This strong result, nevertheless, is obtained by
allowing unrestricted changes in the graph weights.  Such changes may be undesirable, especially if  is unweighted, and the UCS algorithm may be preferred.

To compare the effectiveness of these two types of sparsifiers, we 
simplify equation~(\ref{Eqn:alpha}):

It follows that for , a
dimension-independent constant, we must choose . This
is the price one must pay to retain the original weights. For , the UCS algorithm computes a sparsified graph with
a  that grows at most linearly with .  The algorithm of \cite{ramanujansparse} runs in time , which is equivalent to UCS.

\subsection{Further Comparisons of Column Selection Algorithms}
The algorithm of \cite{ramanujansparse} has been generalized in \cite{nearoptcol} to a column selection algorithm for computing CX decompositions.  In this work, Boutsidis, Drineas, and Magdon-Ismail prove that, given row-orthonormal matrices  and  then for a given  there exist weights  with at most  of them nonzero such that

 and
 
In the context of CX decompositions,  is understood to be the loadings matrix of a data matrix , i.e.  is the SVD of  (although the algorithm could be applied to other matrices for other applications).  Their work includes an algorithm for finding the weights, Deterministic Dual Set Spectral Sparsification (DDSSS).

\begin{theorem}\label{bnd:old}
Let  denote a matrix that chooses the  columns selected by the DDSSS algorithm.  The inequalities (\ref{weighted0}) and (\ref{weighted1}) imply

\end{theorem}
\proof
We interpret these inequalities as a bound on  by first partitioning

where  is a permutation matrix that orders the selected columns first. Then, using a CS decomposition \cite{loan}, we can write

where  and  are diagonal matrices with non-negative entries such that .  Furthermore, because  and  are orthogonal, by inspection  contains the singular values of .  Hence

Now let  be a weight matrix, whose diagonal entries are , the weights from above.  Define

Then
\footnotesize

\normalsize
Therefore


Rearranging


\begin{corollary}
Let  be as defined in equation (\ref{Eqn:alpha}).  Then

\end{corollary}
\proof
\footnotesize

\normalsize
This suggests the UCS algorithm may find a better subset than the column selection algorithm in \cite{nearoptcol}. Observe that typically . For the purpose of finding a well-conditioned subset of columns in , requiring the whole matrix  is computationally expensive. On the other hand, an even better subset can be obtained by applying the UCS algorithm directly to , at considerable savings in computational time and memory usage.  This algorithm runs in time , far slower than UCS.

\section{A Numeric Example: Graph Visualization}
We test the UCS algorithm on the Autonomous systems \mbox{AS-733} dataset in \cite{snapnets}\footnote{File {\tt as19981229}}.  The data is undirected, unweighted, and contains 493 nodes and 1189 edges.  To visualize the data, nodes are plotted using coordinates determined by the force-directed Fruchterman-Reingold algorithm.  This algorithm treats the edges of a graph as forces (similar to springs), and perturbs node coordinates until the graph appears to be near an equilibrium state \cite{FruRei91}.

We apply the force-directed algorithm with two methodologies.  First, the force-directed algorithm is run on the whole graph to determine a fixed set of node coordinates.  Using these coordinates, the original graph is plotted with various sparsifiers in Figure \ref{fig:2}.  Second, we run the force-directed algorithm on each sparsifier to determine node coordinates for that sparsifier, and plot both the sparsifier and the original graph on these coordinates (Figure \ref{fig:3}).  While this requires rerunning the force-directed algorithm for each sparsifier, the algorithm converges faster because of the reduced number of edges.

\begin{figure}[t]
\centering
    \includegraphics[width=0.5\textwidth]{all.jpg}
\caption[]{Autonomous System Example: Original Graph}
\label{fig:1}
\vspace{-0.5cm}
\end{figure}

\afterpage{
\begin{figure}[t]
\centering
\subfigure[984 Edges]{
    \includegraphics[width=0.19\textwidth]{pi984_orig.jpg}
}
\subfigure[738 Edges]{
    \includegraphics[width=0.19\textwidth]{pi738_orig.jpg}
}
\subfigure[615 Edges]{
    \includegraphics[width=0.19\textwidth]{pi615_orig.jpg}
}
\subfigure[Spanning Tree\protect\footnotemark]{
  \includegraphics[width=0.19\textwidth]{asst_orig.jpg}
}
\subfigure{
  \includegraphics[width=0.10\textwidth]{legcol1b.png}
}
\caption[]{Autonomous Systems Graph with Sparsifiers of Various Cardinalities (node coordinates calculated from whole graph)}
\label{fig:2}


\subfigure[984 Edges]{
    \includegraphics[width=0.19\textwidth]{pi984.jpg}
}
\subfigure[738 Edges]{
    \includegraphics[width=0.19\textwidth]{pi738.jpg}
}
\subfigure[615 Edges]{
    \includegraphics[width=0.19\textwidth]{pi615.jpg}
}
\subfigure[{Spanning Tree\protect\footnotemark[3]}]{
  \includegraphics[width=0.19\textwidth]{asst.jpg}
}
\subfigure{
  \includegraphics[width=0.10\textwidth]{legcol2b.png}
}
\caption[]{Autonomous Systems Graph with Sparsifiers of Various Cardinalities (node coordinates recalculated for each sparsifier)}
\label{fig:3}

\subfigure[984 Edges]{
    \includegraphics[width=0.19\textwidth]{bounds984c.png}
}
\subfigure[738 Edges]{
    \includegraphics[width=0.19\textwidth]{bounds738c.png}
}
\subfigure[615 Edges]{
    \includegraphics[width=0.19\textwidth]{bounds615c.png}
}
\subfigure[493 Edges]{
  \includegraphics[width=0.19\textwidth]{bounds493c.png}
}
\subfigure{
  \includegraphics[width=0.10\textwidth]{legb.png}
}




\caption[]{Progress During Iteration and Theoretical Singular Value Lower Bound for Sparsifiers of Various Cardinalities}
\label{fig:4}

\vspace{-0.5cm}
\end{figure}
\footnotetext[3]{Calculated by running UCS algorithm with  and omitting the final edge.  In general, a spanning tree for a connected graph can be found by selecting  edges and removing an edge from a loop created by the UCS algorithm.}
}

Although the original graph can be considered sparse, visualization of the graph is difficult.  In Figure \ref{fig:1}, a few nodes are seen to have high degree, but little information is readily available about important edges in the graph or about how important nodes are related.  Figure \ref{fig:2} shows that plotting the sparsifier on the original graph provides incremental benefit.  The sparser graphs begin to highlight important nodes and important edges connecting them, but visualization remains difficult.  Rerunning the force-directed algorithm on the sparsifiers, nevertheless, evokes an easily interpretable structure, where important nodes, clusters, and important edges connecting clusters are readily visible (Figure \ref{fig:3}).  

\section{Relationship to the Kadison-Singer Problem} 
Let  be an integer, and let  be a matrix that satisfies

where . Equation~(\ref{Eqn:delta}) implies that  is
a row-orthonormal matrix and that each column of  is uniformly
bounded away from  in norm. Marcus {\em et al.} \cite{Marcus2014} show that there exists a partition 

of  such that 

When the graph  is sufficiently dense, equation~(\ref{Eqn:Partition}) implies the existence of an unweighted graph sparsifier (see~Batson, {\em et al.}~\cite{ramanujansparse}) .

\section{Conclusion} We have presented an efficient algorithm for the construction of unweighted spectral sparsifiers for general weighted and unweighted graphs, addressing the open question of the existence of such graph sparsifiers for general graphs~\cite{ramanujansparse}.  Our algorithm is supported by strong theoretical spectral bounds.  Through numeric experiments, we have demonstrated that our sparsification algorithm can be an effective tool for graph visualization, and anticipate that it will prove useful for wide-ranging applications involving large graphs.  An important feature of our sparsification algorithm is the deterministic unweighted column selection algorithm on which it is based.  An open question is the existence of a larger lower spectral bound, either with the same  or a new one.\\

\bibliographystyle{plain}
\bibliography{ucs_sparsifiers}










\end{document}
