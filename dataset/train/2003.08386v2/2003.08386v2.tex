
\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{hyperref}
\usepackage{breakcites}
\hypersetup{
	colorlinks=true,
	citecolor=blue,
	linkcolor=blue
}

\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}

\usepackage{booktabs} 
\usepackage{pifont}\usepackage{enumitem}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}


\begin{document}
	\pagestyle{headings}
	\mainmatter
	\def\ECCVSubNumber{794}  

	\title{DLow: Diversifying Latent Flows for Diverse Human Motion Prediction} 

	
	
	\titlerunning{DLow for Diverse Human Motion Prediction}
	\author{Ye Yuan \and Kris Kitani}
	\authorrunning{Y. Yuan and K. Kitani}
	\institute{Robotics Institute, Carnegie Mellon University\\
		\email{\{yyuan2, kkitani\}@cs.cmu.edu}}
	\maketitle
	\vspace{-4mm}
	
	\begin{abstract}
		Deep generative models are often used for human motion prediction as they are able to model multi-modal data distributions and characterize diverse human behavior. While much care has been taken into designing and learning deep generative models, how to efficiently produce diverse samples from a deep generative model \emph{after} it has been trained is still an under-explored problem. To obtain samples from a pretrained generative model, most existing generative human motion prediction methods draw a set of independent Gaussian latent codes and convert them to motion samples. Clearly, this random sampling strategy is not guaranteed to produce diverse samples for two reasons: (1)~The independent sampling cannot force the samples to be diverse; (2)~The sampling is based solely on likelihood which may only produce samples that correspond to the major modes of the data distribution. To address these problems, we propose a novel sampling method, Diversifying Latent Flows (DLow), to produce a diverse set of samples from a pretrained deep generative model. Unlike random (independent) sampling, the proposed DLow sampling method samples a single random variable and then maps it with a set of learnable mapping functions to a set of correlated latent codes. The correlated latent codes are then decoded into a set of correlated samples. During training, DLow uses a diversity-promoting prior over samples as an objective to optimize the latent mappings to improve sample diversity. The design of the prior is highly flexible and can be customized to generate diverse motions with common features (e.g., similar leg motion but diverse upper-body motion). Our experiments demonstrate that DLow outperforms state-of-the-art baseline methods in terms of sample diversity and accuracy. Our code is released on the project page: \href{https://www.ye-yuan.com/dlow}{\texttt{https://www.ye-yuan.com/dlow}}.
		\vspace{-3mm}
	\end{abstract}
	\vspace{-9mm}
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.9\textwidth]{figs/teaser.pdf}
		\vspace{-4mm}
		\caption{In the latent space of a conditional variational autoencoder (CVAE), samples (stars) from our method DLow are able to cover more modes (colored ellipses) than the CVAE samples. In the motion space, DLow generates a diverse set of future human motions while the CVAE only produces perturbations of the motion of the major mode.}
		\label{fig:teaser}
		\vspace{-7mm}
	\end{figure}
	
	
	\section{Introduction}
	\vspace{-1mm}
	
	Human motion prediction, i.e., predicting the future 3D poses of a person based on past poses, is an important problem in computer vision and has many useful applications in autonomous driving~\cite{paden2016survey}, human robot interaction~\cite{koppula2013anticipating} and healthcare~\cite{troje2002decomposing}. It is a challenging problem because the future motion of a person is potentially diverse and multi-modal due to the complex nature of human behavior. For many safety-critical applications, it is important to predict a diverse set of human motions instead of just the most likely one. For examples, an autonomous vehicle should be aware that a nearby pedestrian can suddenly cross the road even though the pedestrian will most likely remain in place. This diversity requirement calls for a generative approach that can fully characterize the multi-modal distribution of future human motion.
	
	Deep generative models, e.g., variational autoencoders (VAEs)~\cite{kingma2013auto}, are effective tools to model multi-modal data distributions. Most existing work~\cite{walker2017pose,lin2018human,barsoum2018hp,ruiz2018human,kundu2019bihmp,yan2018mt,aliakbarian2020stochastic} using deep generative models for human motion prediction is focused on the design of the generative model to allow it to effectively learn the data distribution. After the generative model is learned, little attention has been paid to the sampling method used to produce \emph{motion samples} (predicted future motions) from the \emph{pretrained} generative model (weights kept fixed). Most of prior work predicts a set of motions by randomly sampling a set of latent codes from the latent prior and decoding them with the generator into motion samples. We argue that such a sampling strategy is not guaranteed to produce a diverse set of samples for two reasons: (1) The samples are independently drawn, which makes it difficult to enforce diversity; (2) The samples are drawn based on likelihood only, which means many samples may concentrate around the major modes (which have more observed data) of the data distribution and fail to cover the minor modes (as shown in Fig.~\ref{fig:teaser} (Bottom)). The poor sample efficiency of random sampling means that one needs to draw a large number of samples in order to cover all the modes which is computationally expensive and can lead to high latency, making it unsuitable for real-time applications such as autonomous driving and virtual reality.  This prompts us to address an overlooked aspect of diverse human motion prediction --- the sampling strategy.
	
	We propose a novel sampling method, Diversifying Latent Flows (DLow), to obtain a diverse set of samples from a pretrained deep generative model. For this work, we use a conditional variational autoencoder (CVAE) as our pretrained generative model but other generative models can also be used with our approach. DLow is inspired by the two previously mentioned problems with random (independent) sampling. To tackle problem (1) where sample independence limits model diversity, we introduce a new random variable and a set of learnable deterministic mapping functions to correlate the motion samples. We first transform the random variable with the mappings functions to generate a set of correlated latent codes which are then decoded into motion samples using the generator. As all motion samples are generated from a common random factor, this formulation allows us to model the joint sample distribution and offers us the opportunity to impose diversity on the samples by optimizing the parameters of the mapping functions. To address problem (2) where likelihood-based sampling limits diversity, we introduce a diversity-promoting prior (loss function) on the samples during the training of DLow. The prior follows an energy-based formulation using an energy function based on pairwise sample distance. We optimize the mapping functions during training to minimize the cross entropy between the joint sample distribution and diversity-promoting prior to increase sample diversity. To strike a balance between diversity and likelihood, we add a KL term to the optimization to enhance the likelihood of each sample. The relative weights between the prior term and the KL term represent the trade-off between the diversity and likelihood of the generated motion samples.
	Furthermore, our approach is highly flexible in that by designing different forms of the diversity-promoting prior we can impose a variety of structures on the samples besides diversity. For example, we can design the prior to ask the motion samples to cover the ground truth better to achieve higher sample accuracy. Additionally, other designs of the prior can enable new applications, such as controllable motion prediction, where we generate diverse motion samples that share some common features (e.g., similar leg motion but diverse upper-body motion).
	
	The contributions of this work are the following: 
	(1) We propose a novel perspective for addressing sample diversity in deep generative models --- designing sampling methods for a \emph{pretrained} generative model. 
	(2) We propose a principled sampling method, DLow, which formulates diversity sampling as a constrained optimization problem over a set of learnable mapping functions using a diversity-promoting prior on the samples and KL constraints on the latent codes, which allows us to balance between sample diversity and likelihood.
	(3)~Our approach allows for flexible design of the diversity-promoting prior to obtain more accurate samples or enable new applications such as controllable motion prediction. 
	(4)~We demonstrate through human motion prediction experiments that our approach outperforms state-of-the-art baseline methods in terms of sample diversity and accuracy.
	
	
	\vspace{-5mm}
	\section{Related Work}
	\vspace{-2mm}
	\noindent\textbf{Human Motion Prediction.} Most previous work takes a deterministic approach to modeling human motion and regress a single future motion from past 3D poses \cite{fragkiadaki2015recurrent,jain2016structural,butepage2017deep,li2017auto,ghosh2017learning,martinez2017human,pavllo2018quaternet,chiu2019action,gopalakrishnan2019neural,aksan2019structured,wang2019imitation,mao2019learning} or video frames \cite{chao2017forecasting,zhang2019predicting,yuan2019egopose}. While these approaches are able to predict the most likely future motion, they fail to model the multi-modal nature of human motion, which is essential for safety-critical applications. More related to our work, stochastic human motion prediction methods start to gain popularity with the development of deep generative models. These methods~\cite{walker2017pose,lin2018human,barsoum2018hp,ruiz2018human,kundu2019bihmp,yan2018mt,aliakbarian2020stochastic,yuan2020residual} often build upon popular generative models such as conditional generative adversarial networks (CGANs;~\cite{goodfellow2014generative}) or conditional variational autoencoders (CVAEs;~\cite{kingma2013auto}). The aforementioned methods differ in the design of their generative models, but at test time they follow the same sampling strategy --- randomly and independently sampling trajectories from the pretrained generative model without considering the correlation between samples. In this work, we propose a principled sampling method that can produce a diverse set of samples, thus improving sample efficiency compared to the random sampling typically used in prior work.
	
	\vspace{1pt}
	\noindent\textbf{Diverse Inference.} Producing a diverse set of solutions has been investigated in numerous problems in computer vision and machine learning. A branch of these diversity-driven methods stems from the M-Best MAP problem~\cite{nilsson1998efficient,seroussi1994algorithm}, including diverse M-Best solutions~\cite{batra2012diverse} and multiple choice learning~\cite{guzman2012multiple,lee2016stochastic}. Alternatively, submodular function maximization has been applied to select a diverse subset of garments from fashion images~\cite{hsiao2018creating}. Another type of methods~\cite{kulesza2011k,gong2014diverse,gillenwater2014expectation,huang2015we,azadi2017learning,yuan2019diverse,WengYuan2020} seeks diversity using determinantal point processes (DPPs;~\cite{macchi1975coincidence,kulesza2012determinantal}) which are efficient probabilistic models that can measure the global diversity and quality within a set. Similarly, Fisher information~\cite{rissanen1996fisher} has been used for diverse feature~\cite{gu2012generalized} and data~\cite{sourati2017probabilistic} selection. Diversity has also been a key aspect in generative modeling. A vast body of work has tried to alleviate the mode collapse problem in GANs~\cite{che2016mode,chen2016infogan,srivastava2017veegan,arjovsky2017wasserstein,gulrajani2017improved,elfeki2018gdpp,lin2018pacgan,yang2019diversity} and the posterior collapse problem in VAEs~\cite{zhao2017infovae,tolstikhin2017wasserstein,kim2018semi,bhattacharyya2018accurate,liu2019cyclical,he2019lagging}. Normalizing flows~\cite{rezende2015variational} have also been used to promote diversity in trajectory forecasting~\cite{rhinehart2018r2p2,guan2020generative}. This line of work aims to improve the diversity of the data distribution learned by deep generative models. We address diversity from a different angle by improving the strategy for producing samples from a pretrained deep generative model. 
	\vspace{-2mm}
	\section{Diversifying Latent Flows (DLow)}
	\label{sec:dlow}
	\vspace{-5pt}
	
	For many existing methods on generative vision tasks such as multi-modal human motion prediction, the primary focus is to learn a good generative model that can capture the multi-modal distribution of the data. In contrast, once the generative model is learned, little attention has been paid to devising sampling strategies for producing diverse samples from the \emph{pretrained} generative model.
	
	In this section, we will introduce our method, Diversifying Latent Flows (DLow), as a principled way for drawing a diverse and likely set of samples from a pretrained generative model (weights kept fixed). To provide the proper context, we will first start with a brief review of deep generative models and how traditional methods produce samples from a pretrained generative model.
	
	\vspace{2mm}
	\noindent\textbf{Background: Deep Generative Models.} 
	Let  denote data (e.g., human motion) drawn from a data distribution  where  is some conditional information (e.g., past motion). One can reparameterize the data distribution by introducing a latent variable  such that , where  is a Gaussian prior distribution. Deep generative models learn  by modeling the conditional distribution , and the generative process can be described as sampling  and mapping them to data samples  using a deterministic \emph{generator} function  as
	
	where the generator  is instantiated as a deep neural network parametrized by . This generative process produces samples from the implicit sample distribution  of the generative model, and the goal of generative modeling is to learn a generator  such that . There are various approaches for learning the generator function , which yield different types of deep generative models such as variational autoencoders (VAEs;~\cite{kingma2013auto}), normalizing flows (NFs;~\cite{rezende2015variational}), and generative adversarial networks (GANs;~\cite{goodfellow2014generative}). Note that even though the discussion in this work is focused on conditional generative models, our method can be readily applied to the unconditional case.
	
	\vspace{2mm}
	\noindent\textbf{Random Sampling.} 
	Once the generator function  is learned, traditional approaches produce samples from the learned data distribution  by first randomly sampling a set of latent codes  from the latent prior  (Eq.~\eqref{eq:p_z}) and decode  with the generator  into a set of data samples  (Eq.~\eqref{eq:G_theta}). We argue that such a sampling strategy may result in a less diverse sample set for two reasons: (1) Independent sampling cannot model the repulsion between samples within a diverse set; (2) The sampling is only based on the data likelihood and many samples can concentrate around a small number of modes that have more training data. As a result, random sampling can lead to low sample efficiency because many samples are similar to one another and fail to cover other modes in the data distribution.
	
	\begin{figure*}[t]
		\centering
		\includegraphics[width=\textwidth]{figs/overview.pdf}
		\vspace{-7mm}
		\caption{\textbf{Overview of our DLow framework applied to diverse human motion prediction.} The network  takes past motion  as input and outputs the parameters of the mapping functions . Each mapping  transforms the random variable  to a different latent code  and also warps the density  to the latent code density . Each latent code  is decoded by the CVAE decoder into a motion sample .}
		\label{fig:overview}
		\vspace{-5mm}
	\end{figure*}
	
	\vspace{2mm}
	\noindent\textbf{DLow Sampling.} To address the above issues with the random sampling approach, we propose an alternative sampling method, Diversifying Latent Flows (DLow), that can generate a diverse and likely set of samples from a pretrained deep generative model. Again, we stress that the weights of the generative model are kept fixed for DLow. We later apply DLow to the task of human motion prediction in Sec.~\ref{sec:motion_pred} to demonstrate DLow's ability to improve sample diversity.
	
	Instead of sampling each latent code  independently according to , we introduce a random variable  and conditionally generate the latent codes  and data samples  as follows:
	
	where  is a Gaussian distribution,  are latent mapping functions with parameters , and each  maps  to a different latent code .
	The above generative process defines a joint distribution  over the samples  and latent codes , where  is the conditional distribution induced by the generator . Notice that in our setup,  depends only on  as the generator parameters  are learned in advance and are kept fixed. The data samples  can be viewed as a sample from the joint sample distribution  and the latent codes  can be regarded as a sample from the joint latent distribution  induced by warping  through . If we further marginalize out all variables except for  from , we obtain the marginal sample distribution  from which each sample  is drawn. Similarly, each latent code  can be viewed as a latent sample from the marginal latent distribution .
	
	The above distribution reparametrizations are illustrated in Fig.~\ref{fig:overview}. We can see that all latent codes  and data samples  are correlated as they are uniquely determined by , and by sampling  one can easily produce  and  from the joint latent distribution  and joint sample distribution . Because  and  are controlled by the latent mapping functions , we can impose structural constraints on  and  by optimizing the parameters  of the latent mapping functions.
	
	
	To encourage the diversity of samples , we introduce a diversity-promoting prior  (specific form defined later) and formulate a constrained optimization problem:
	\vspace{-2mm}
	
	where we minimize the cross entropy between the sample distribution  and the diversity-promoting prior .
	However, the objective in Eq.~\eqref{eq:obj} alone can result in very low-likelihood samples  corresponding to latent codes  that are far away from the Gaussian prior .
	To ensure that each sample  also has high likelihood under the generative model ,  we add constraints in Eq.~\eqref{eq:constraint} on the KL divergence between  and the Gaussian prior  (same as ) to make  and thus  where  and . To optimize this constrained objective, we soften the constraints with the Lagrangian function:
	\vspace{-2mm}
	
	where we use the same Lagrangian multiplier  for all constraints. Despite having similar form, the above objective is very \emph{different} from the objective function of -VAE~\cite{higgins2017beta} in many ways: (1) our goal is to learn a diverse sampling distribution  for a pretrained generative model rather than learning the generative model itself; (2) The first part in our objective is a diversifying term instead of a reconstruction term; (3) Our objective function applies to most deep generative models, not just VAEs. In this objective, the softening of the hard KL constraints allows for the trade-off between the diversity and likelihood of the samples . For small ,  is allowed to deviate from  so that  can potentially attend to different regions in the latent space as shown in Fig.~\ref{fig:overview} (latent space) to further improve sample diversity. For large , the objective will focus on minimizing the KL term so that  and , and thus the sample  will have high likelihood under . 
	
	
	The overall DLow objective is defined as:
	
	where  and  are the first and second term in Eq.~\eqref{eq:dlow_opt} respectively.
	In the following, we will discuss in detail how we design the latent mapping functions  and the diversity-promoting prior .
	
	
	
	\vspace{2mm}
	\noindent\textbf{Latent Mapping Functions.}
	Each latent mapping  transforms the Gaussian distribution  to the marginal latent distribution  for latent code  where  is also conditioned on . As  should stay close to the Gaussian latent prior , it would be ideal if the mapping  makes  also a Gaussian. Thus, we design  to be an invertible affine transformation:
	
	where the mapping parameters ,  is a nonsingular matrix,  is a vector, and  is the number of dimensions for  and . As shown in Fig.~\ref{fig:overview} and Fig.~\ref{fig:network} (Right), we use a -head network  to output , and the parameters  of the network  are the parameters to be optimized with the DLow objective in Eq.~\eqref{eq:dlow_overall}.
	
	Under the invertible affine transformation ,  becomes a Gaussian distribution . This allows us to compute the KL divergence terms in  analytically:
	
	The KL divergence is minimized when  which implies that  and . Geometrically, this means that  is in the orthogonal group , which includes all rotations and reflections in an -dimensional space. This means any mapping  that is a rotation or reflection operation will minimize the KL divergence. As mentioned before, there is a trade-off between diversity and likelihood in Eq.~\eqref{eq:dlow_overall}. To improve sample diversity (minimize ) without compromising likelihood (KL divergence), we can optimize  to be different rotations or reflections to map  to different feasible points  in the latent space. This geometric understanding sheds light on the mapping space admitted by the hard KL constraints. In practice, we use soft KL constraints in the DLow objective to further enlarge the feasible mapping space which allows us to achieve lower  and better sample diversity.
	
	
	\vspace{2mm}
	\noindent\textbf{Diversity-Promoting Prior.} In the DLow objective, a diversity-promoting prior  on the joint sample distribution is used to guide the optimization of the latent mapping functions .  
	With an energy-based formulation, the prior  can be defined using an energy function :
	
	where  is a normalizing constant.  Dropping the constant , the first term in Eq.~\eqref{eq:dlow_opt} can be rewritten as
	
	To promote sample diversity of , we design an energy function  based on a pairwise distance metric :
	
	where we use the Euclidean distance for  and an RBF kernel with scale . Minimizing  moves the samples towards a lower-energy (diverse) configuration.
	 can be evaluated efficiently with the reparametrization trick~\cite{kingma2013auto}.
	
	
	Up to this point, we have described the proposed sampling method, DLow, for generating a diverse set of samples from a pretrained generative model . By introducing a common random variable , DLow allows us to generate correlated samples . Moreover, by introducing learnable mapping functions , we can model the joint sample distribution  and impose structural constraints, such as diversity, on the sample set  which cannot be modeled by random sampling from the generative model. 
	
	
	
	\section{Diverse Human Motion Prediction}
	\label{sec:motion_pred}
	
	Equipped with a method to generate diverse samples from a pretrained deep generative model, we now turn our attention to the task of diverse human motion prediction.
	Suppose the pose of a person is a -dimensional vector consisting of 3D joint positions, we use  to denote the past motion of  time steps and  to denote the future motion over a future time horizon of . Given a past motion , the goal of diverse human motion prediction is to generate a diverse set of future motions .
	
	To capture the multi-modal distribution of the future trajectory , we take a generative approach and use a conditional variational autoencoder (CVAE) to learn the future trajectory distribution . Here we use the CVAE for its stability over other popular approaches such as CGANs, but other suitable deep generative models could also be used. The CVAE uses a varitional lower bound~\cite{jordan1999introduction} as a surrogate for the intractable true data log-likelihood:
	
	where  is an -parametrized approximate posterior distribution.
	We use multivariate Gaussians for the prior, posterior (encoder distribution) and likelihood (decoder distribution): ,  , and  where  is a hyperparameter.
	Both the encoder and decoder are implemented as recurrent neural networks (RNNs). As shown in Fig.~\ref{fig:network}, the encoder network  outputs the parameters of the posterior distribution: ; the decoder network  outputs the reconstructed future trajectory . The CVAE is learned via jointly optimizing the encoder and decoder with Eq.~\eqref{eq:cvae}.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figs/networks.pdf}
		\vspace{-7mm}
		\caption{\textbf{Network architectures} for the CVAE and DLow. We use GRUs~\cite{chung2014empirical} to extract motion features.  and  denotes the -th pose in  and  respectively.}
		\label{fig:network}
		\vspace{-5mm}
	\end{figure}
	
	\subsection{Diversity Sampling with DLow}
	Once the CVAE is learned, we follow the DLow framework proposed in Sec.~\ref{sec:dlow} to optimize the network  (Fig.~\ref{fig:network} (Right)) and learn the latent mapping functions . Before doing this, to fully leverage the DLow framework, we will look at one of DLow's key feature, i.e., the design of the diversity-promoting prior  in  can be flexibly changed by modifying the underlying energy function . This allows us to impose various structural constraints besides diversity on the sample set . Below, we will provide two examples of such prior designs that (1) improve sample accuracy or (2) enable new applications such as controllable motion prediction.
	
	\vspace{1mm}
	\noindent\textbf{Reconstruction Energy.}
	To ensure that the sample set  is both diverse and accurate, i.e., the ground truth future motion  is close to one of the samples in , we can modify the prior's energy function  in Eq.~\eqref{eq:dlow_energy} by adding a reconstruction term :
	
	where  is a weighting factor and we use Euclidean distance as the distance metric . As DLow produces a correlated set of samples  instead of independent samples, the network  can learn to distribute samples in a way that are both diverse and accurate, covering the ground truth better. We use this prior design for our main experiments.
	
	\vspace{1mm}
	\noindent\textbf{Controllable Motion Prediction.} 
	Another possible design of the diversity-promoting prior  is one that promotes diversity in a certain subspace of the sample space. In the context of human motion prediction, we may want certain body parts to move similarly but other parts to move differently. For example, we may want leg motion to be similar but upper-body motion to be diverse across motion samples.
	We call this task controllable motion prediction, i.e., finding a set of diverse samples that share some common features, which can allow users or down-stream systems to explore variations of a certain type of samples.
	
	Formally, we divide the human joints into two sets,  and , and ask samples in  to have similar motions for joints  but diverse motions for joints . 
	We can slice a motion sample  into two parts:  where  and  correspond to  and  respectively. Similarly, we can slice the sample set  into two sets:  and . We then define a new energy function  for the prior :
	
	where we add another energy term  weighted by  to minimize the motion distance between samples for joints , and we only compute the diversity-promoting term  using motions of joints .
	After optimizing  using the DLow objective with the new energy , we can produce diverse samples  that have similar motions for joints .
	
	Furthermore, we may also want to use a reference motion sample  to provide the desired features. To achieve this, we can treat  as the first sample  in . We first find its corresponding latent code  using the CVAE encoder: . We can then find the common variable  for generating  using the inverse mapping :
	
	With  known, we can generate  that includes .
	In practice, we force  to be an identity mapping to enforce  so that  covers the posterior distribution of . Otherwise, if  lies outside of the high density region of , it may lead to low-likelihood  after the inverse mapping.
	
	
	\vspace{-3mm}
	\section{Experiments}
	\vspace{-2mm}
	
	\noindent\textbf{Datasets.}
	We perform evaluation on two public motion capture datasets: Human3.6M~\cite{ionescu2013human3} and HumanEva-I~\cite{sigal2010humaneva}. Human3.6M is a large-scale dataset with 11 subjects (7 with ground truth) and 3.6 million video frames in total. Each subject performs 15 actions and the human motion is recorded at 50 Hz. Following previous work \cite{martinez2017simple,luvizon20182d,yang20183d,pavllo20193d}, we adopt a 17-joint skeleton and train on five subjects (S1, S5, S6, S7, S8) and test on two subjects (S9 and S11). HumanEva-I is a relatively small dataset, containing only three subjects recorded at 60 Hz. We adopt a 15-joint skeleton~\cite{pavllo20193d} and use the same train/test split provided in the dataset. By using both a large dataset with more variation in motion and a small dataset with less variation, we can better evaluate the generalization of our method to different types of data. For Human3.6M, we predict future motion for 2 seconds based on observed motion of 0.5 seconds. For HumanEva-I, we forecast future motion for 1 second given observed motion of 0.25 seconds.
	
	\vspace{1mm}
	\noindent\textbf{Baselines.}
	To fully evaluate our method, we consider three types of baselines: (1) Deterministic motion prediction methods, including \textbf{ERD}~\cite{fragkiadaki2015recurrent} and \textbf{acLSTM}~\cite{li2017auto}; (2) Stochastic motion prediction methods, including CVAE based methods, \textbf{Pose-Knows}~\cite{walker2017pose} and \textbf{MT-VAE}~\cite{yan2018mt}, as well as a CGAN based method, \textbf{HP-GAN}~\cite{barsoum2018hp}; (3) Diversity-promoting methods for generative models, including \textbf{Best-of-Many}~\cite{bhattacharyya2018accurate}, \textbf{GMVAE}~\cite{dilokthanakul2016deep}, \textbf{DeLiGAN}~\cite{gurumurthy2017deligan}, and \textbf{DSF}~\cite{yuan2019diverse}.
	
	\vspace{2mm}
	\noindent\textbf{Metrics.} We use the following metrics to measure both sample \emph{diversity} and \emph{accuracy}. (1) \textbf{Average Pairwise Distance (APD)}: average  distance between all pairs of motion samples to measure diversity within samples, which is computed as . (2) \textbf{Average Displacement Error (ADE)}: average  distance over all time steps between the ground truth motion  and the closest sample, which is computed as . (3) \textbf{Final Displacement Error (FDE)}:  distance between the final ground truth pose  and the closest sample's final pose, which is computed as . (4) \textbf{Multi-Modal ADE (MMADE)}: the multi-modal version of ADE that obtains multi-modal ground truth future motions by grouping similar past motions. (5) \textbf{Multi-Modal FDE (MMFDE)}: the multi-modal version of FDE.
	
	In these metrics, APD has been used to measure sample diversity~\cite{aliakbarian2020stochastic}. ADE and FDE are common metrics for evaluating sample accuracy in trajectory forecasting literature~\cite{alahi2016social,lee2017desire,gupta2018social}. MMADE and MMFDE~\cite{yuan2019diverse} are metrics used to measure a method's ability to produce multi-modal predictions.
	
	\vspace{1mm}
	\noindent\textbf{Implementation Details.} We use a batch size of 64 and set the latent dimensions  to 128 in all experiments. For the CVAE, we sample 5000 training examples every epoch and train the networks for 500 epochs using Adam~\cite{kingma2014adam} and a learning rate of~\mbox{1e-3}. The DLow objective in Eq.~\eqref{eq:dlow_overall} can be rewritten as: . We set  to  for Human3.6M and  for HumanEva-I. For the mappings , we specify  to be diagonal to reduce the output size of . This design is mainly for computational efficiency, as we do find that using a full parametrization of  improves performance. The RBF kernel scale  is set to 100 for Human3.6M and 20 for HumanEva-I. For both datasets, we sample 5000 training examples every epoch and train  for 500 epochs using Adam with a learning rate of 1e-4.
	
	
	\begin{table}[ht]
		\footnotesize
		\centering
		\resizebox{\columnwidth}{!}{
			\begin{tabular}{@{\hskip 0mm}lcccclcccccl@{\hskip 0mm}}
				\toprule
				& \multicolumn{5}{c}{Human3.6M~\cite{ionescu2013human3}} & & \multicolumn{5}{c}{HumanEva-I~\cite{sigal2010humaneva}} \\ \cmidrule{2-6} \cmidrule{8-12}
				Method & APD  & ADE  & FDE  & MMADE  & MMFDE  & & APD  & ADE  & FDE  & MMADE  & MMFDE  \\ \midrule
				DLow (Ours) & \textbf{11.741} & \textbf{0.425} & \textbf{0.518} & \textbf{0.495} & \textbf{0.531} &  & \textbf{4.855} & \textbf{0.251} & \textbf{0.268} & \textbf{0.362} & \textbf{0.339} \\
				ERD \cite{fragkiadaki2015recurrent}                     & 0  & 0.722 & 0.969 & 0.776 & 0.995 &  & 0 & 0.382 & 0.461 & 0.521 & 0.595 \\
				acLSTM \cite{li2017auto}                      & 0  & 0.789 & 1.126 & 0.849 & 1.139 &  & 0 & 0.429 & 0.541 & 0.530 & 0.608 \\
				Pose-Knows \cite{walker2017pose}              & 6.723  & 0.461 & 0.560 & 0.522 & 0.569 &  & 2.308 & 0.269 & 0.296 & 0.384 & 0.375 \\
				MT-VAE \cite{yan2018mt}           & 0.403  & 0.457 & 0.595 & 0.716 & 0.883 &  & 0.021 & 0.345 & 0.403 & 0.518 & 0.577 \\
				HP-GAN \cite{barsoum2018hp}           & 7.214  & 0.858 & 0.867 & 0.847 & 0.858 &  & 1.139 & 0.772 & 0.749 & 0.776 & 0.769 \\
				Best-of-Many \cite{bhattacharyya2018accurate} & 6.265  & 0.448 & 0.533 & 0.514 & 0.544 &  & 2.846 & 0.271 & 0.279 & 0.373 & 0.351 \\
				GMVAE \cite{dilokthanakul2016deep}            & 6.769  & 0.461 & 0.555 & 0.524 & 0.566 &  & 2.443 & 0.305 & 0.345 & 0.408 & 0.410 \\
				DeLiGAN \cite{gurumurthy2017deligan}          & 6.509  & 0.483 & 0.534 & 0.520 & 0.545 &  & 2.177 & 0.306 & 0.322 & 0.385 & 0.371 \\
				DSF \cite{yuan2019diverse}                    & 9.330  & 0.493 & 0.592 & 0.550 & 0.599 &  & 4.538 & 0.273 & 0.290 & 0.364 & 0.340 \\
				\bottomrule
			\end{tabular}
		}
		\vspace{1mm}
		\caption{\textbf{Quantitative results} on Human3.6M and HumanEva-I.}
		\label{table:quan}
		\vspace{-5mm}
	\end{table}
	
	\begin{table}[ht]
		\footnotesize
		\centering
		\resizebox{\columnwidth}{!}{
			\begin{tabular}{@{\hskip 1mm}ccc@{\hskip 2mm}cccclcccccl@{\hskip 1mm}}
				\toprule
				\multicolumn{2}{c}{Energy} & & \multicolumn{5}{c}{Human3.6M~\cite{ionescu2013human3}} & & \multicolumn{5}{c}{HumanEva-I~\cite{sigal2010humaneva}} \\ \cmidrule{1-2} \cmidrule{4-8} \cmidrule{10-14}
				 & & & APD  & ADE  & FDE  & MMADE  & MMFDE  & & APD  & ADE  & FDE  & MMADE  & MMFDE  \\ \midrule
				\cmark & \cmark & & 11.741 & \textbf{0.425} & \textbf{0.518} & \textbf{0.495} & \textbf{0.531} & & 4.855 & \textbf{0.251} & \textbf{0.268} & \textbf{0.362} & \textbf{0.339}\\  
				\cmark & \xmark & & \textbf{13.091} & 0.546 & 0.663 & 0.599 & 0.669 & & \textbf{4.927} & 0.263 & 0.281 & 0.368  & 0.347\\  
				\xmark & \cmark & & 6.844 & 0.432 & 0.525 & 0.500 & 0.539 & & 2.355 & 0.252 & 0.277 & 0.376 & 0.366 \\  
				\xmark & \xmark & & 6.383 & 0.520 & 0.629 & 0.577 & 0.638 & & 2.247 & 0.281 & 0.317 & 0.395 & 0.393 \\  
				\bottomrule
			\end{tabular}
		}
		\vspace{1mm}
		\caption{\textbf{Ablation study} on Human3.6M and HumanEva-I.}
		\label{table:ablation}
		\vspace{-10mm}
	\end{table}
	
	\vspace{-3mm}
	\subsection{Quantitative Results}
	\vspace{-1mm}
	We summarize the quantitative results on Human3.6M and HumanEva-I in Table~\ref{table:quan}. The metrics are computed with the sample set size . For both datasets, we can see that our method, DLow, outperforms all baselines in terms of both sample diversity (APD) and accuracy (ADE, FDE) as well as covering multi-modal ground truth (MMADE, MMFDE). Determinstic methods like ERD~\cite{fragkiadaki2015recurrent} and acLSTM~\cite{li2017auto} do not perform well because they only predict one future trajectory which can lead to mode averaging. Methods like MT-VAE~\cite{yan2018mt} produce trajectories samples that lack diversity so they fail to cover the multi-modal ground-truth (indicated by high MMADE and MMFDE) despite having decently low ADE and FDE. We would also like to point out the closest competitor DSF~\cite{yuan2019diverse} can only generate one deterministic set of samples, while our method can produce multiple diverse sets by sampling . We also show how each metric changes against various  in Appendix~\ref{sec:metrics_vs_k}.
	
	\vspace{1mm}
	\noindent\textbf{Ablation Study.}
	We further perform an ablation study (Table~\ref{table:ablation}) to analyze the effects of the two energy terms  and  in Eq.~\eqref{eq:human_prior}. First, without the reconstruction term , the DLow variant is able to achieve higher diversity (APD) at the cost of sample accuracy (ADE, FDE, MMADE, MMFDE). This is expected because the network only optimizes the diversity term  and focuses solely on diversity. Second, for the variant without , both sample diversity and accuracy decrease. It is intuitive to see why the diversity (APD) decreases. To see why the sample accuracy (ADE, FDE, MMADE, MMFDE) also decreases, we should consider the fact that a more diverse set of samples have a better chance at covering the ground truth. Finally, when we remove both  and  (i.e., only optimize ), the results are the worst, which is expected.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figs/comp_base_new.pdf}
		\vspace{-7mm}
		\caption{\textbf{Qualitative Results} on Human3.6M and HumanEva-I.}
		\label{fig:comp_base}
		\vspace{-3mm}
	\end{figure}
	
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figs/comp_beta.pdf}
		\vspace{-7mm}
		\caption{\textbf{Varying  in DLow} allows us to balance between diversity and likelihood. }
		\label{fig:beta}
		\vspace{-5mm}
	\end{figure}
	
	
	\vspace{-3mm}
	\subsection{Qualitative Results}
	\vspace{-1mm}
	To visually evaluate the diversity and accuracy of each method, we present a qualitative comparison in Fig.~\ref{fig:comp_base} where we render the start pose, the end pose of the ground truth future motion, and the end pose of 10 motion samples. Note that we do not model the global translation of the person, which is why some sitting motions appear to be floating. For Human3.6M, we can see that our method DLow can predict a wide array of future motions, including standing, sitting, bending, crouching, and turning, which cover the ground truth bending motion. In contrast, the baseline methods mostly produce perturbations of a single motion --- standing. For HumanEva-I, we can see that DLow produces interesting variations of the fighting motion, while the baselines produce almost identical future motions.
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.9\linewidth]{figs/variation_new.pdf}
		\vspace{-5mm}
		\caption{\textbf{Effect of varying } on motion samples. }
		\label{fig:var}
		\vspace{-3mm}
	\end{figure}
	
	\vspace{1mm}
	\noindent\textbf{Diversity vs. Likelihood.} As discussed in the approach section, the  in Eq.~\eqref{eq:dlow_opt} represents the trade-off between sample diversity and likelihood. To verify this, we trained three DLow models with different  (1, 10, 100) and visualize the motion samples generated by each model in Fig.~\ref{fig:beta}. We can see that a larger  leads to less diverse samples which correspond to the major mode of the generator distribution, while a smaller  can produce more diverse motion samples covering other plausible yet less likely future motions.
	
	\vspace{1mm}
	\noindent\textbf{Effect of varying .} A key difference between our method and DSF~\cite{yuan2019diverse} is that we can generate multiple diverse sets of samples while DSF can only produce a fixed diverse set. To demonstrate this, we show in Fig.~\ref{fig:var} how the motion samples of DLow change with different . By comparing the four sets of motion samples, one can conclude that changing  varies each set of samples but preserves the main structure of each motion.
	
	\vspace{1mm}
	\noindent\textbf{Controllable Motion Prediction.}
	As highlighted before, the flexible design of the diversity-promoting prior enables a new application, controllable motion prediction, where we predict diverse motions that share some common features. We showcase this application by conducting an experiment using the energy function defined in Eq.~\eqref{eq:human_ctrl}. 
	The network is trained so that the leg motion of the motion samples is similar while the upper-body motion is diverse.
	The results are shown in Fig.~\ref{fig:control}. We can see that given a reference motion, our method can generate diverse upper-body motion and preserve similar leg motion, while random samples from the CVAE cannot enforce similar leg motion. Please refer to Appendix~\ref{sec:control_res} for more results.
	
	
	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{figs/control_new.pdf}
		\vspace{-7mm}
		\caption{\textbf{Controllable Motion Prediction.} DLow enables samples to have more similar leg motion to the reference.}
		\label{fig:control}
		\vspace{-6mm}
	\end{figure}
	
	\vspace{-2mm}
	\section{Conclusion}
	\vspace{-1mm}
	We have proposed a novel sampling strategy, DLow, for deep generative models to obtain a diverse set of future human motions. We introduced learnable latent mapping functions which allowed us to generate a set of correlated samples, whose diversity can be optimized by a diversity-promoting prior. Experiments demonstrated superior performance in generating diverse motion samples. Moreover, we showed that the flexible design of the diversity-promoting prior further enables new applications, such as controllable human motion prediction. We hope that our exploration of deep generative models through the lens of diversity will encourage more work towards understanding the complex nature of modeling and predicting future human behavior.
	
	
	\bibliographystyle{splncs04}
	\bibliography{main}
	
	\appendix
	
\clearpage
\section{Additional Human3.6M Results}
In this section, we show more qualitative results on Human3.6M, including additional comparison with baselines (Fig.~\ref{fig:supp_h36m_comp}) and additional examples of DLow (Fig.~\ref{fig:supp_h36m}). Please refer to the \href{https://youtu.be/64OEdSadb00}{video} to see the whole motion sequences.
\vspace{-3mm}
\subsection{Additional Comparison with Baselines on Human3.6M}
\begin{figure}[ht!]
    \vspace{-7mm}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/human36m_comp.pdf}
    \vspace{-5mm}
    \caption{\textbf{Additional comparison with the baselines on Human3.6M.} We show the start pose, the end pose of the ground truth future motion, and the end pose of 10 motion samples by each method.}
    \label{fig:supp_h36m_comp}
\end{figure}

\clearpage
\subsection{Additional Examples of DLow on Human3.6M}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figs/human36m_dlow.pdf}
    \caption{\textbf{Additional examples of DLow on Human3.6M.} Each row corresponds to a different sequence, where we show the start pose, the end pose of the ground truth future motion, and the end pose of 10 motion samples.}
    \label{fig:supp_h36m}
\end{figure}

\clearpage
\section{Additional Controllable Motion Prediction Results}
\label{sec:control_res}
In Fig.~\ref{fig:control_supp}, we show additional results on controllable motion prediction using Human3.6M, where we use DLow to constrain the motion samples to have similar leg motion to the reference motion but diverse upper-body motion. Notice that DLow is able to produce samples with similar leg motion, while CVAE (random) samples cannot enforce similar leg motion. We further show some quantitative results in Table~\ref{table:control}, where we compute the average leg motion distance from motion samples to the reference motion and the APD for upper-body motion.

\vspace{1mm}\noindent\textbf{Implementation Details.}
We use the same networks in Fig.~3 of the main paper and the same hyperparmeters and training procedure given in the implementation details of the main paper. The main modification is that we use Eq.~24 in the paper for the energy function  of the prior , and the DLow objective in Eq.~12 can be rewritten as: . We set  to . We also use a full parametrization of  instead of a diagonal one.

\begin{table}[ht!]
\footnotesize
\centering
\vspace{-5mm}
\begin{tabular}{c@{\hskip 2mm}c@{\hskip 2mm}c}
\toprule
Method & Leg Dist  & Upper-body APD  \\ \midrule
DLow & \textbf{1.071} & \textbf{12.741} \\  
CVAE & 2.958 & 6.051 \\  
\bottomrule
\end{tabular}
\vspace{2mm}
\caption{\textbf{Quantitative results} for controllable motion prediction.}
\label{table:control}
\vspace{-10mm}
\end{table}

\begin{figure}[ht!]
\vspace{-7mm}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/controllable_supp_new.pdf}
    \vspace{-3mm}
    \caption{\textbf{Additional results on controllable motion prediction.} DLow can produce motion samples that have similar leg motion to the reference (Ref) yet diverse upper-body motion, while CVAE (random) samples cannot enforce similar leg motion.}
    \label{fig:control_supp}
    \vspace{-10mm}
\end{figure}

\clearpage
\section{Metrics vs. Number of Samples }
\label{sec:metrics_vs_k}
\begin{figure}[ht!]
    \vspace{-5mm}
    \centering
    \includegraphics[width=\textwidth]{figs/plot.pdf}
    \caption{\textbf{Metrics vs. Number of Samples } on both Human3.6M (Left) and HumanEva-I (Right).}
    \label{fig:metrics}
    \vspace{-5mm}
\end{figure}



\clearpage
\section{Additional HumanEva-I Results}
We also show more qualitative results on HumanEva-I which is a much smaller dataset with less motion variation. We present additional comparison with baselines (Fig.~\ref{fig:supp_heva_comp}) and additional examples of DLow (Fig.~\ref{fig:supp_heva}).
\subsection{Additional Comparison with Baselines on HumanEva-I}
\begin{figure}[ht!]
    \vspace{-5mm}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/humaneva_comp.pdf}
    \vspace{-5mm}
    \caption{\textbf{Additional comparison with the baselines on HumanEva-I.} We show the start pose, the end pose of the ground truth future motion, and the end pose of 10 motion samples by each method.}
    \label{fig:supp_heva_comp}
    \vspace{-5mm}
\end{figure}

\clearpage
\subsection{Additional Examples of DLow on HumanEva-I}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figs/humaneva_dlow.pdf}
    \caption{\textbf{Additional examples of DLow on HumanEva-I.} Each row corresponds to a different sequence, where we show the start pose, the end pose of the ground truth future motion, and the end pose of 10 motion samples.}
    \label{fig:supp_heva}
\end{figure}

 	
\end{document}
