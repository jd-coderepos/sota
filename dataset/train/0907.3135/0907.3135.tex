\documentclass{article}
\usepackage[applemac]{inputenc}
\usepackage{cite}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm, algorithmic}
\usepackage{graphicx}
\usepackage{relate}
\setlength{\relateright}{5pt}
\setlength{\relateleft}{0pt}

\newcommand{\polylog}{\ensuremath{\mathrm{polylog}}}
\newcommand{\leaf}{\ensuremath{\mathrm{leaf}}}
\newcommand{\occ}{\ensuremath{\mathrm{occ}}}

\newcommand{\rank}{\ensuremath{\mathrm{rank}}}
\newcommand{\depth}{\ensuremath{\mathrm{depth}}}
\newcommand{\leaves}{\ensuremath{\mathrm{leaves}}}
\newcommand{\roots}{\ensuremath{\mathrm{root}}}
\newcommand{\parent}{\ensuremath{\mathrm{parent}}}
\newcommand{\child}{\ensuremath{\mathrm{child}}}
\newcommand{\lab}{\ensuremath{\mathrm{label}}}
\newcommand{\nca}{\ensuremath{\mathrm{nca}}}
\newcommand{\lnca}{\ensuremath{\mathrm{lnca}}}
\newcommand{\pre}{\ensuremath{\mathrm{pre}}}
\newcommand{\post}{\ensuremath{\mathrm{post}}}
\newcommand{\lcp}{\ensuremath{\mathrm{lcp}}}
\newcommand{\sort}{\ensuremath{\mathrm{sort}}}
\newcommand{\op}{\ensuremath{\mathrm{op}}}
\newcommand{\str}{\ensuremath{\mathrm{str}}}
\newcommand{\strdepth}{\ensuremath{\mathrm{strdepth}}}
\newcommand{\LCP}{\ensuremath{\mathrm{LCP}}}
\newcommand{\LCPP}{\ensuremath{\mathrm{LCPP}}}
\newcommand{\LDP}{\ensuremath{\mathrm{LDP}}}


\newcommand{\hindex}{\ensuremath{\mathrm{hindex}}}
\newcommand{\lindex}{\ensuremath{\mathrm{lindex}}}
\newcommand{\iindex}{\ensuremath{\mathrm{index}}}
\newcommand{\bin}{\ensuremath{\mathrm{bin}}}
\newcommand{\even}{\ensuremath{\mathrm{even}}}
\newcommand{\odd}{\ensuremath{\mathrm{odd}}}
\newcommand{\bitonic}{\ensuremath{\mathrm{bitonic}}}
\newcommand{\smear}{\ensuremath{\mathrm{smear}}}
\newcommand{\rmb}{\ensuremath{\mathrm{rmb}}}
\newcommand{\lmb}{\ensuremath{\mathrm{lmb}}}
\newcommand{\lsmear}{\ensuremath{\mathrm{lsmear}}}
\newcommand{\rsmear}{\ensuremath{\mathrm{rsmear}}}



\newcommand{\lcps}{\ensuremath{\mathrm{lcps}}}
\newcommand{\lcpp}{\ensuremath{\mathrm{lcpp}}}
\newcommand{\ldp}{\ensuremath{\mathrm{ldp}}}

\newcommand{\rdp}{\ensuremath{\mathrm{rdp}}}
\newcommand{\revlex}{\ensuremath{\mathrm{revlex}}}
\newcommand{\revlexmin}{\ensuremath{\mathrm{revlexmin}}}
\newcommand{\rlm}{\ensuremath{\mathrm{rlm}}}


\newcommand{\dom}{\ensuremath{\mathrm{dom}}}
\newcommand{\fail}{\ensuremath{\mathrm{fail}}}
\newcommand{\num}{\ensuremath{\mathrm{num}}}


\newcommand{\light}{\ensuremath{\mathrm{light}}}
\newcommand{\heavy}{\ensuremath{\mathrm{heavy}}}

\newcommand{\lex}{\ensuremath{\mathrm{lex}}}
\newcommand{\new}{\ensuremath{\mathrm{new}}}
\newcommand{\enc}{\ensuremath{\mathrm{enc}}}
\newcommand{\hforward}{\ensuremath{\mathrm{hforward}}}
\newcommand{\hfail}{\ensuremath{\mathrm{hfail}}}
\newcommand{\accept}{\ensuremath{\mathrm{accept}}}

\newcommand{\Zip}{\ensuremath{\textsc{Zip}}}
\newcommand{\Unzip}{\ensuremath{\textsc{Unzip}}}
\newcommand{\Move}{\ensuremath{\textsc{Move}}}
\newcommand{\Simulate}{\ensuremath{\textsc{Simulate}}}
\newcommand{\Match}{\ensuremath{\textsc{Match}}}
\newcommand{\Pref}{\ensuremath{\textsc{Pref}}}
\newcommand{\Update}{\ensuremath{\textsc{Update}}}
\newcommand{\Search}{\ensuremath{\textsc{Search}}}
\newcommand{\Prefix}{\ensuremath{\textsc{Prefix}}}
\newcommand{\Next}{\ensuremath{\textsc{Next}}}
\newcommand{\Path}{\ensuremath{\textsc{Path}}}


\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\renewcommand{\angle}[1]{\langle{#1}\rangle}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newcommand{\qed}{\hfill\ensuremath{\Box}\medskip\\\noindent}
\newenvironment{proof}{\noindent\emph{Proof. }}

\title{Fast Searching in Packed Strings\footnote{An externded abstract of this paper appeared at the 20th Annual Symposium on Combinatorial Pattern Matching.}}



\author{Philip Bille\thanks{Supported by the Danish Agency for Science, Technology, and Innovation.}  \\  {\tt phbi@imm.dtu.dk}}






\begin{document}
\maketitle

\begin{abstract}
Given strings  and  the (exact) string matching problem is to find all positions of substrings in  matching . The classical Knuth-Morris-Pratt algorithm [SIAM J. Comput., 1977] solves the string matching problem in linear time which is optimal if we can only read one character at the time. However, most strings are stored in a computer in a packed representation with several characters in a single word, giving us the opportunity to read multiple characters simultaneously. In this paper we study the worst-case complexity of string matching on strings given in packed representation. Let  be the lengths  and , respectively, and let  denote the size of the alphabet. On a standard unit-cost word-RAM with logarithmic word size we present an algorithm using time 

Here  is the number of occurrences of  in . For 
this improves the  bound of the Knuth-Morris-Pratt algorithm.
Furthermore, if  our algorithm is optimal
since any algorithm must spend at least 
time to read the input and report all occurrences. The result is
obtained by a novel automaton construction based on the
Knuth-Morris-Pratt algorithm combined with a new compact
representation of subautomata allowing an optimal tabulation-based
simulation. \end{abstract}



\section{Introduction}
Given strings  and  of length  and , respectively, the
\emph{(exact) string matching problem} is to report all positions of
substrings in  matching . The string matching problem is perhaps
the most basic problem in combinatorial pattern matching and also one
of the most well-studied, see e.g.,~\cite{KMP1977, BM1977,KR1987,
  BYG1992} for classical textbook algorithms and the surveys
in~\cite{Gusfield1997, NR2002}. The first worst-case  algorithm
(we assume w.l.o.g. that ) is the classical
Knuth-Morris-Pratt algorithm~\cite{KMP1977}. If we assume that we can
read only one character at the time this bound is optimal since we
need  time to read the input. However, most strings are
stored in a computer in a \emph{packed representation} with several
characters in a single word. For instance, DNA-sequences have an
alphabet of size  and are therefore typically stored using  bits
per character with  characters in a -bit word. On packed
strings we can read multiple characters in constant time and hence
potentially do better that the  lower bound for string
matching. In this paper we study the worst-case complexity of packed
string matching and present an algorithm to beat the  lower
bound for almost all combinations of  and .


\subsection{Setup and Results}
We assume a standard unit-cost word RAM with word length  and a standard instruction set including arithmetic
operations, bitwise boolean operations, and shifts. The space
complexity is the number of words used by the algorithm, not counting
the input which is assumed to be read-only. All strings in this paper
are over an alphabet  of size . The \emph{packed
  representation} of a string  is obtained by storing  characters per word thus representing  in
 words. If  is
given in the packed representation we simply say that  is a
\emph{packed string}. The \emph{packed string matching problem} is
defined as above except that  and  are packed strings. In the
worst case any algorithm for packed string matching must examine all
of the words in the packed representation of the input strings. The
algorithm must also report all occurrences of  in  and therefore
must spend at least  time, where  denotes the number of occurrences of
 in . In this paper we present an algorithm with the following
complexity.
\begin{theorem}\label{thm:main}
For packed strings  and  of length  and , respectively,
with characters from an alphabet of size , we can solve the
packed string matching problem in time  and space  for any
constant , .  
\end{theorem}
For  this improves the  bound of the
Knuth-Morris-Pratt algorithm. Furthermore, if 
our algorithm matches the lower bound and is therefore optimal. In
practical situations  is typically much smaller than  and
therefore this condition is almost always satisfied.

\subsection{Techniques}\label{sec:techniques}
The KMP-algorithm~\cite{KMP1977} may be viewed as simulating an
automaton  according to the characters from  in a left-to-right
order. At each character in  we use  to maintain the longest
prefix of  matching the current suffix of . Several improvements
of automaton simulation based algorithms are known, see
e.g~\cite{MP1980, Myers1992,BYG1992, WMM1995}. Typically, these
algorithms partition the automaton into many small subautomata which
can then be quickly simulated fast using either precomputed and
tabulated information and/or the arithmetic and logical operations of
the machine. The idea is then to use the fast simulation for the small
subautomata to obtain a faster simulation of the entire automata. This
approach is often called the ``Four Russian Technique''
~\cite{ADKF1970} when tabulation is used and ``word-level
parallelism'' or ``bitparallelism''~\cite{BaezaYates1989} when only
arithmetic and logical instructions are used. There are (at least) two
central components needed to make this approach effective. First, the
partition into subautomata should allow for efficient distribution of
the computation among the subautomata. Secondly, for tabulation or
word-level parallelism to work efficiently the subautomata must be
encoded compactly.


If we attempt to apply this idea to the KMP-algorithm the above
challenges pose major problems. First, the structure of the
transitions in  does not in general allow us to partition  into
subautomata such that a simulation does not change subautomata too
often. Indeed, for any partition we might be forced to repeatedly
change subautomaton after every group of  characters of  and
hence end up using  time. Secondly, even if we could design
a suitable partition of  into subautomata, a compact encoding of
the transitions of a subautomata is non-trivial to obtain. An explicit
list of such transitions will not suffice to achieve the bound of
Theorem~\ref{thm:main}. The main contribution of this paper are two
new ideas to overcome these problems.


First, we present the \emph{segment automaton}, , derived from .
In , the states of  are grouped into overlapping intervals of  states from  such that (almost all
of) the states in  are duplicated in . We show how to
selectively ``copy'' the transitions from  to  such that the
total number of transitions between subautomata never exceeds 
in the simulation on . Secondly, we show how to exploit structural
properties of the transitions to represent subautomata optimally. This
allows us to tabulate paths of transitions for all subautomata of size
 using  space and preprocessing time. Plugging in
, for constant ,
this is  space and preprocessing time. The
simulation can then be performed in time  leading to Theorem~\ref{thm:main}.

This main contribution of this paper is theoretical, however, we
believe that both the segment automaton and the compact representation
of automata may prove very useful in practice if combined with ideas
from other algorithms for packed matching.


\subsection{Related Work}
Exploiting packed string representations to speed-up string matching
is not a new idea and is even mentioned in the early papers by Knuth
et al. and Boyer and Moore~\cite{KMP1977, BM1977}. More recently,
several packed string matching algorithms have
appeared~\cite{BaezaYates1989b, TP1997,
  Fredriksson2002,Fredriksson2003, KBN2007,FL2009, FL2009a}. However,
none of these improve the worst-case  bound of the classical
KMP-algorithm.

It is possible to extend the ``super-alphabet'' technique by
Fredriksson~\cite{Fredriksson2002, Fredriksson2003} to obtain a simple
trade-off for packed string matching. The idea is to build an
automaton that, similar to the KMP-automaton, maintains the longest
prefix of  matching the current suffix of  but allows  to be
processed in groups of  characters. Each state has 
outgoing transitions corresponding to all combinations of 
characters. This algorithm uses  time and
 space. Choosing  this
is  time and 
space. Compared to Theorem~\ref{thm:main} this is a factor 
worse in space and only improves the  time bound of the
KMP-algorithm when .



Packed string matching is closely related to the area of
\emph{compressed pattern matching} introduced by Amir and
Benson~\cite{AB1992,AB1992a}. Here the goal is to search for an
uncompressed pattern in a compressed text without decompressing it
first. Furthermore, the search should be faster than the naive
approach of decompressing the text first and then using the fastest
algorithm for the uncompressed problem. In \emph{fully compressed
  pattern matching} the pattern is also given in compressed form.
Several algorithms for (fully) compressed string matching are known,
see e.g., the survey by Rytter~\cite{Rytter1999}. For instance, if 
is compressed with the Ziv-Lempel-Welch scheme~\cite{Welch1984} into a
string  of length , Amir et al.~\cite{ABF1996} showed how to
find all occurrences of  in time . The packed
representation of a string may be viewed as the most basic way to
compress a string. Hence, in this perspective we are studying the
fully compressed string matching problem for packed strings. Note that
our result is optimal if the pattern is not packed.


As mentioned above, speeding up automaton based algorithms for string
matching is a well-known idea that has been succesfully applied to a
number of string matching problems. For instance,
Myers~\cite{Myers1992} showed how speed up the simulation of
Thompson's~\cite{Thomp1968} automaton construction for the regular
expression matching problem. The improvement here is from updating a
set of states in a non-deterministic automaton efficiently, whereas we
obtain an improvement by processing multiple characters quickly.  With
few exceptions, see e.g.,~\cite{MP1980, BT2009}, most of the known
improvements of automata-based algorithms are based on updating sets
of states. 


\subsection{Outline}
We first briefly review the KMP-algorithm and how it can be viewed as
simulating an automaton in Sec.~\ref{sec:KMP}. We then present the
two major components of our algorithm. Specifically, in
Sec.~\ref{sec:segment} we present the segment automaton and in
Sec.~\ref{sec:representation} we show how to compactly represent
and efficiently tabulate subautomata. In Sec.~\ref{sec:algorithm}
we put the components together and present the complete
algorithm. Finally, in Sec.~\ref{sec:remarks} we conclude with some
remarks and open problems.



\section{The Knuth-Morris-Pratt Automaton and String Matching}\label{sec:KMP}
We briefly review the KMP-algorithm~\cite{KMP1977} which will be the
starting point of our new algorithm. The algorithm that we describe
and use is a slightly simpler version of the KMP-algorithm, often
referred to as the ``Morris-Pratt'' algorithm~\cite{MP1970}, which
suffices to achieve our results.


Let  be a string of length  on an alphabet . The
character at position  in  is denoted  and the substring
from position  to  is denoted by . The substrings
 and  are the \emph{prefixes} and \emph{suffixes}
of , respectively.

The Knuth-Morris-Pratt automaton (KMP-automaton), denoted , for
 consists of  states identified by the integers  each corresponding to a prefix of . From state  to state
,  there is a \emph{forward transition} labeled
. We call the rightmost forward transition from  to  the
\emph{accepting transition}. From state , , there is
a \emph{failure transition} to a state denoted  such that
 is the longest prefix of  matching a proper suffix
of . Fig.~\ref{fig:segment}(a) depicts the KMP-automaton for
the pattern . \begin{figure}[t]
  \centering \includegraphics[scale=.5]{segment}
  \caption{(a) The Knuth-Morris-Pratt automaton  for the pattern
    . Solid lines are forward transitions and
    dashed lines are failure transitions. (b)-(c) The corresponding
    segment automaton  for  consisting of  segments
    with , , and  states. The light transitions are shown in
    (b) and the heavy transition transitions in (c).}
   \label{fig:segment}
\end{figure}

The failure transitions form a tree with root in state  and with
the property that  for any state . Since the longest
prefixes of  and  matching a suffix of  can
increase by at most one character we have the following property of
failure transitions.
\begin{lemma}\label{lem:fail}
  Let  be a string of length  and  be the KMP-automaton
  for . For any state , .
\end{lemma}
We will exploit this property in Sec.~\ref{sec:encoding} to
compactly encode subautomata of the KMP-automaton. The KMP-automaton
can be constructed in time ~\cite{KMP1977}.


To find the occurrences of  in  we read the characters of 
from left-to-right while traversing  to maintain the longest
prefix of  matching a suffix of the current prefix of  as
follows. Initially, we set the state of  to . Suppose that we
are in state  after reading the  characters of , i.e., the
longest prefix of  matching a suffix of  is . We
process the next character  as follows. If 
matches the label of the forward transition from  the next state is
. Furthermore, if this transition is the accepting transition
then  is the endpoint of a substring of  matching  and we
therefore report an occurrence. Otherwise, ( does not match
the label of the forward transition from  to ) we recursively
follow failure transitions from  until we find a state  whose
forward transition is labeled  in which case the next state is
, or if no such state exist we set the next state to be . We
define the \emph{simulation} of  on  to be sequence of
transitions traversed by the algorithm.

Each time the simulation on  follows a forward transition we
continue to the next character and hence the total number of forward
transitions is at most . Each failure transition strictly decreases
the current state number while forward transitions increase the state
number by . Since we start in state  the number of failure
transition is therefore at most the number of forward
transitions. Hence, the total number of transitions is at most 
and therefore the searching takes  time. In total the
KMP-algorithm uses time .

\section{The Segment Automaton}\label{sec:segment}
In this section we introduce a simple automaton called the
\emph{segment automaton}. The segment automaton for  is equivalent
to  in the sense that the simulation on  at each step
provides the longest prefix  matching a suffix of the current
prefix of . The segment automaton gives a decomposition of 
into subautomata of a given size  such that the simulation on 
passes through at most  subautomata. In our packed string
matching algorithm, we will simulate the segment automaton using a fast
algorithm for simulating subautomata of size , leading to the bound of Theorem~\ref{thm:main}. 



Let  be the KMP-automaton for . For a even integer
parameter ,  we define the segment automaton with
parameter , denoted , as follows. Define a \emph{segment}
 to be an interval , , of
states in  and let  denote the size of
. Divide the  states of  into a set of  overlapping segments, denoted , where  is defined by

Thus, each segment in  consists of  consecutive states from
, except the last segment, , which may be smaller. Any
state  in  appears in at most  segments and adjacent segments
share  states.


The segment automaton  is obtained by adding  states
for each segment  and then selectively ``copying''
transitions from  to . Specifically, the states of  is the
set of pairs given by
 
We view each state  of  as the th state of the th
segment, i.e., state  corresponds to the state  in
. Hence, each state in  is represented by  or  states in
 and each state in  uniquely corresponds to a state in .


We copy transitions from  to  in the following way. Let  be a transition in . For each segment  such that  we have the following transitions in :
\begin{itemize}
\item If  there is a \emph{light transition} from  to . 
\item If  there is a \emph{heavy transition} from  to , where either  is the unique segment containing  or if two segments contain , then  is the segment such that , i.e., the segment containing  in the leftmost half.
\end{itemize}
If  is a forward transition with label  it is
also a forward transition in  with label , if  is a
failure transition it is also a failure transition in , and if 
is the accepting transition it is also an accepting transition in .
The segment automaton with  corresponding to the KMP-automaton
of Fig.~\ref{fig:segment}(a) is shown in Fig.~\ref{fig:segment}(b) and
(c) showing the light and heavy transitions, respectively. From the
correspondence between  and  we have that each accepting
transition in a simulation of  on  corresponds to an occurrence
of  in . Hence, we can solve string matching by simulating 
instead of .

We will use the following key property of the . 
\begin{lemma}\label{lem:heavy}
For a string  of length  and even integer parameter , the simulation of the segment automaton, , on a string  of length  contains at most  heavy and accepting transitions. 
\end{lemma}
\begin{proof}
  Consider the sequence  of transitions in the simulation of  on . Let  denote the number of accepting
  transitions, and let  and  denote the
  number of heavy forward and heavy failure transitions,
  respectively. Each accepting transition in  corresponds to an
  occurrence and therefore . For a state 
  in  we will refer to  as the \emph{segment number}. Since a
  forward transition in  increases the state number by  in  a
  heavy forward transition increases the segment number by  or 
  in . A heavy failure transition strictly decrease the segment
  number. Hence, since we start the simulation in segment , we can
  have at most  heavy failure transitions for each heavy forward
  transition in  and therefore


If  the results trivially follows. Hence, suppose
that . Before the first heavy forward transition in
 there must be at least  light transitions in order to reach
state . Consider the subsequence of transitions  in 
between an arbitrary heavy transition  and a forward heavy
transition . The heavy transition  cannot end in segment 
since there is no heavy forward transition from here. All other heavy
transitions have an endpoint in the leftmost half of a segment and
therefore at least  light transitions are needed before a heavy
forward transition can occur. Recall that the total number of
transition in  is at most  and therefore the number of heavy
forward transitions in  is bounded by

Combining the bound on  with \eqref{eq:Nfail} and
\eqref{eq:Nforward} we have that the total number of heavy and
accepting transitions is
\qed
\end{proof}

\section{Representing Segments}\label{sec:representation}
We now describe how to efficiently encode segments how to use the
encoding to efficiently tabulate transitions within segments. In the
following section, we show how to combine this with a simulation of
the segment automaton, leading to the full algorithm for packed
string matching. 



\subsection{A Compact Encoding}\label{sec:encoding}
Let  be a segment with  states over an alphabet of size
. We show how to compactly represent all light transitions in
 using  bits. To represent forward transitions we
simply store the labels of the  light forward transitions in 
using  bits. Next consider the
failure transitions. A straightforward approach is to explicitly store
for each state  a bit indicating if its failure pointer is
light or heavy and, if it is light, a pointer to . Each
pointer requires  bits and hence the total cost for
this representation is  bits. We show how to improve this
to  bits in the following.


First, we locally enumerate the states in  to . Let \}, , be the
set of states in  with a light failure transition and let  be the set of failure pointers for
the states in . We encode  as a bit string  of length 
such that  iff . This uses  bits. To represent
 compactly we encode  and the sequence of differences
between consecutive elements ,
where . We represent 
explicitly using  bits. Our representation of 
consists of  bit strings. The first string, denoted , is the
concatenation of the binary encoding of the numbers in , i.e., , where  denotes
standard two's complement binary encoding (the differences may be
negative) and  denotes concatenation. Each number  uses at
most  bits and therefore the size of the  is at
most

where . The second bit string, denoted
, represents the boundaries of the numbers in , i.e.,
 iff  is the start of a new number in . Thus,
. Note that with , , and  we can
uniquely decode . The total size of the representation is
 bits.


To bound the size of the representation we show that 
implying that the representation uses  bits as desired. We first bound the sum . Recall from Lemma~\ref{lem:fail} that the failure function
increases by at most  between consecutive states in . Hence,
over the subsequence  of  of failure pointers in the range
 the total increase of the failure function can be at most
. Hence, . Furthermore, if ,
for some , the total decrease of  over a segment of
 states is at most  plus the total increase and therefore
. Hence,

Combining \eqref{eq:bd} and \eqref{eq:diff} we have that 

The second inequality follows from Jensen's inequality combined with
the fact that the logarithm is a concave function. In summary, we have
the following result.
\begin{lemma}\label{lem:encoding}
  All light forward and failure transitions of a segment of size 
  can be encoded using  bits.
\end{lemma}



\subsection{Simulating Light Transitions}
We now show how to efficiently simulate multiple light transition within
segments. First, we first introduce a number of important concepts. 


Let  be the segment automaton, and consider the path 
of states in the simulation on  from a state  on some string
. Define the \emph{longest light path} from  on  to be
the longest prefix of  consisting entirely of light non-accepting
transitions in segment . Furthermore, define the \emph{string
  length} of  to be number of forward transitions in , i.e., the
number of characters of  traversed in . For example, consider
state  in segment  in Fig.~\ref{fig:segment}. The longest
light path on the string  is the path . The transition on c from state  is a
heavy failure transition and therefore not included in . The string
of  is 2.



Our goal is to quickly compute the string length and endpoint of
longest light paths. Let  be the compact encoding of a
segment  as described above including the label of the forward
heavy transition from the rightmost state in  (if any) and a bit
indicating whether or not the rightmost light transition is accepting
or not. Furthermore, let  be a state in , let  be a string,
and define
\begin{relate}
\item[:] Return the pair , where 
  and  is the string length and final state, respectively, of the longest
  light path in  from  matching a prefix of .
\end{relate}
For example, if  is the encoding of segment 
in Fig.~\ref{fig:segment}, then 
returns the pair . 


We will efficiently tabulate  for arbitrary strings  of
length  (the maximum number of light forward transitions in a
segment of size ) as follows. Let  be the total number of bits
needed to represent the input to . The string  uses  bits and by Lemma~\ref{lem:encoding}  uses

bits. Furthermore, the state number  uses  bits and
hence . 

We tabulate all possible results for  using a table 
containing  entries as follows. Let  be any input to 
encoded using the above  bit representation and let 
denote the non-negative integer in  represented by
. The table stores at position  the result of
 on input . We compute each entry using a standard
simulation in  time and therefore we construct  in  time and space. Hence, if we have  space
available for  we may set , where  is an upper bound on the constant
appearing in the  expression above. Hence, the total space
and preprocessing time now becomes .


With  precomputed and stored in memory we can now answer arbitrary
 queries for arbitrary encoded segments and strings of length
at most  in constant time by table lookup.  


\section{The Algorithm}\label{sec:algorithm}
We now put the pieces from the previous sections together to obtain
our main result of Theorem~\ref{thm:main}. Assume that we have  space available and choose  as
above for the tabulation. We first preprocess  by computing the
following information:
\begin{itemize}
\item The segment automaton  with parameter  and  segments .
\item The compact encoding  for each segment .
\item The tabulated  function for segments with  states and
  input string of length .
\end{itemize}
We compute the segment automaton and the compact encodings in 
time and space. The tabulation for  uses  time and space
and hence the preprocessing uses  time and space.

We find the occurrences of  in  using the algorithm described
below. The main idea is to simulate the segment automaton using the
tabulated  function with the segment automaton. At each
iteration of the algorithm we traverse light transitions until we
either have processed  characters from  or encounter a heavy or
accepting transition. We then follow the single next transition and
report an occurrence if the transition is accepting. We repeat this
process until we have read all of . We note that to implement the
algorithm we need to be able to extract any substring of 
characters from  quickly even if the substring does not begin at a
word boundary. We use standard shifts to extract such a substring in
constant time.

\paragraph{\bf Algorithm S}(\emph{Packed String Search}). Let  be a
preprocessed string for a parameter  as above. Given a string 
of length  the algorithm finds all occurrences of  in .
\begin{description}
\item[S1.] [Initialize] Set  and .
\item[S2.] [Do up to  light transitions] Compute . At this point  is
  the state in the traversal of  on  after reading the prefix
  . All transitions on the string  are light and
  non-accepting by the definition of .
\item[S3.] [Done?] If  then terminate. In this case step S2
  exhausted remaining characters from .
\item[S4.] [Do single transition] Compute  by traversing the
  transition  from  on character . If
   is a failure transition, set  and
  otherwise set . If  is an
  accepting transition report an occurrence ending at position .
\item[S5.] [Repeat] Update  and  and repeat from step S2.
\end{description}
For example, consider running Algorithm S on the segment automaton in
Fig.~\ref{fig:segment} with string . Here,  and we therefore process up to  characters from  in a
single iteration. Initially, we start at state  in step S1. In
step S2, we compute the longest path of light and non-accepting
transitions matching . Since there is such a path
from  in this segment matching aba, we process all of aba and
end at . Since more characters remain in , we continue from
step S3. In step S4, we traverse the light failure transition from
 to  since . In step S5, we update
 and  and repeat from step
S2. In step S2, we process all characters from 
ending in . In step S4, we traverse the light forward
transition from  to  on . In iteration
3, we process characters  in step S2, ending at state
. The character  is not traversed since the
corresponding transition is heavy. Instead, in step S5, we traverse b
to . In iteration 4, we process the character  in
step S2, ending at . We cannot traverse the character  since the corresponding transition is accepting. Step S5 traverses
the accepting transition and reports an occurrence. In the final
iteration, no light transitions are traversed in step S2 and the
algorithm terminates in step S3.


It is straightforward to verify that Algorithm S simulates  on
 and reports occurrences whenever we encounter an accepting
transition. In each iteration we either read  characters from 
and/or perform a heavy or accepting transition. We can process 
characters from  on light transitions at most  and by
Lemma~\ref{lem:heavy} the total number of heavy and accepting
transitions is . Hence, the total number of iterations
is . Since each iteration takes constant time this also
bounds the running time. Adding the preprocessing time and plugging in
 the time becomes

with space . Hence we have the following result.
\begin{theorem}
  Let  and  be packed strings of length  and ,
  respectively. For a parameter  we can solve the packed
  string matching problem in time  and space .
\end{theorem}
Note that the tabulation is independent of . Hence, if we want to
support multiple searches it suffices to precompute the tables
once. If we plugin , for ,
we obtain an algorithm using time  and space
 thereby showing Theorem~\ref{thm:main}.

\section{Remarks and Open Problems}\label{sec:remarks}
We have presented an almost optimal solution for the packed string
matching problem on a unit-cost RAM with logarithmic word-length. We
conclude with two challenging open problems.

\begin{itemize}
\item Our algorithm relies on tabulation to compute the 
  function, and therefore its speed is limited by the amount of space
  we have for tables. Consequently, it cannot take advantage of a
  large word length . We wonder if it is possible to
  obtain a packed string matching algorithm that achieves a speed-up
  over the KMP-algorithm that depends on  rather than . In
  particular, it might be possible to come up with an algorithm based
  on \emph{word-level parallelism} (a.k.a. \emph{bit
    parallelism}~\cite{BaezaYates1989}), that uses the arithmetic and
  logical instructions in the word-RAM instead of tables to perform
  the computation.
\item It would be interesting to obtain fast algorithms for related
  packed problems. For instance, we wonder if it is possible to obtain
  a similar speed-up for the multi-string matching
  problem~\cite{AC1975}. The Aho-Corasick algorithm~\cite{AC1975} for
  multi-string matching uses an automaton that generalizes the
  KMP-automaton, however, it appears difficult to extend our
  techniques to this automaton.
\end{itemize}


\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}
