\appendix

\section*{Supplementary Material for HAMT}

Section~\ref{sec:model_details} provides additional details for the model. The experimental setup is described in Section~\ref{sec:expr_details}, including datasets, metrics and implementation details.
Section~\ref{sec:more_results} presents computation time on R2R dataset and full experimental results on RxR and REVERIE datasets. Section~\ref{sec:more_ablation_results} includes more ablations. 
Finally, Section~\ref{sec:viz_results} illustrates qualitative results.

\section{Model details}
\label{sec:model_details}
\subsection{Proxy tasks in training}
We employ five proxy tasks to train HAMT and introduced SAP/SAR and SPREL in Section~\ref{sec:model_pretrain}. In the following, we present the other three proxy tasks, which are all based on the input pair , where  is the textual instruction and  is the full trajectory with length .

\noindent\textbf{Masked Language Modeling (MLM).}
The task predicts masked words based on contextual words and the full trajectory.
We randomly mask out tokens in  with the probability of 15\% with a special token \verb|[mask]| as in BERT, and predict the word distribution  where  is the masked instruction,  is the output embedding of the masked word  and  is a two-layer fully-connected network.
The objective is to minimize the negative log-likelihood of original words: .
The task is beneficial to learn grounded language representations and cross-modal alignment.

\noindent\textbf{Masked Region Modeling (MRM).}
The task aims to predict semantic labels of masked observations in the trajectory given an instruction and neighboring observations.
We zero out observations in  15\% of the time. The target of a masked  is the class probability predicted by an image classification model pretrained on ImageNet. We use ViT-B/16~\cite{dosovitskiy2020image} in this work. 
Suppose  is the target class probability for a masked , we predict  where  is the output embedding of masked , and minimize the KL divergence between the two probability distributions: .
In order to solve the task,  should capture temporal continuity in the history sequence and align with relevant instructions. 

\noindent\textbf{Instruction Trajectory Matching (ITM).}
The task predicts whether a pair of instruction and trajectory is aligned.
We predict the alignment score as , where  is element-wise multiplication and  are output embeddings for the text \verb|[cls]| token and the history \verb|[cls]| token respectively.
We sample 4 negative trajectories for each positive instruction-trajectory pair during training, in which two negative trajectories are randomly selected from other positive pairs in the mini-batch, two are obtained by temporally shuffling the positive trajectory.
The objective is the Noisy Contrastive Estimation loss~\cite{gutmann2010noise}: .
The model is supposed to learn cross-modal alignment and be sensitive to temporal orders of history to solve the task.


\subsection{Structure variants in fine-tuning}
\label{sec:supp_hamt_nov2t}
\begin{wrapfigure}{r}{0.5\textwidth}
	\centering
	\vspace{-4em}
	\includegraphics[width=\linewidth]{figures/hamt_encdec_variant}
	\caption{Comparison of the original cross-modal transformer layer (left) and the encoder-decoder based variant (right).}
	\label{fig:hamt_encdec_variant}
	\vspace{-4em}
\end{wrapfigure}


We present the encoder-decoder variant of HAMT in fine-tuning on the right of Figure~\ref{fig:hamt_encdec_variant}.
Compared to the original cross-modal transformer on the left, the variant removes text-to-vision cross-modal attention. 
The encoder encodes the texts to obtain textual embeddings. Then the decoder reuses the same text embeddings in vision-to-text attention layer at each navigation step.
In this way, the variant is more efficient when instructions are long \eg in R4R and RxR datasets.


\section{Experimental setup}
\label{sec:expr_details}
\subsection{Dataset details}
Table~\ref{tab:dataset_stats} summarizes details of the dataset split.
The proposed R2R-Back and R2R-Last setups consider exactly the same splits as the R2R dataset.
We present details to construct R2R-Back and R2R-Last in the following.

\begin{table}[h]
	\centering
	\small
	\caption{Dataset statistics. \#traj, \#instr  denote the number of trajectories and instructions respectively.}
	\label{tab:dataset_stats}
	\begin{tabular}{ccccccccc} \toprule
		\multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Train} & \multicolumn{2}{c}{Val Seen} & \multicolumn{2}{c}{Val Unseen} & \multicolumn{2}{c}{Test Unseen} \\
		& \#traj & \#instr & \#traj & \#instr & \#traj & \#instr & \#traj & \#instr \\ \midrule
		R2R \cite{anderson2018vision} & 4,675 & 14,039 & 340 & 1,021 & 783 & 2,349 & 1,391 & 4,173 \\
		RxR \cite{ku2020room} & 11,077 & 79,467 & 1,244 & 8,813 & 1,517 & 13,652 & - & 11,888 \\ \midrule
		R4R \cite{jain2019stay} & 25,921 & 233,532 & 115 & 1,035 & 5,026 & 45,234 & - & - \\
		R2R-Back & 4,675 & 14,039 & 340 & 1,021 & 783 & 2,349 & - & - \\ \midrule
		CVDN \cite{thomason2020vision} & 4,742 & 4,742 & 382 & 382 & 907 & 907 & 1,384 & 1,384 \\ \midrule
		R2R-Last & 4,675 & 14,039 & 340 & 1,021 & 783 & 2,349 & - & - \\
		REVERIE \cite{qi2020reverie} & 4,150 & 10,466 & 515 & 1,423 & 1,328 & 3,521 & 2,304 & 6,292 \\ \bottomrule
	\end{tabular}
\end{table}


\noindent\textbf{R2R-Back.} We append a returning command at the end of annotated instructions in R2R to create new instructions for R2R-Back. The returning command is randomly sampled from the following sentences: ``walk back to the start'', ``return by the way you came'', ``double back to where you start'', ``backtrack to the start'', ``back the way you came'', ``return to the starting point''. The original target location is viewed as a middle stop point. The groundtruth trajectory in R2R-Back is the concatenation of the original and its inverse trajectory.

\noindent\textbf{R2R-Last.} We use spacy toolkit\footnote{\url{https://spacy.io/}} to split sentences for instructions in R2R. We only select the last sentence in each instruction as the new high-level instruction. It mainly describes where the goal location is \eg ``stop in front of the vent'', requiring the agent to explore houses without step-by-step textual guidance. The groundtruth trajectory is the same as R2R.

\subsection{Evaluation Metrics}
In R2R, RxR, R4R and R2R-Last datasets, a predicted trajectory is considered to be successful if the agent arrives 3 meters near to the final destination.
However, such definition would make a motionless agent achieve 100\% success rate (SR) on R2R-Back dataset as the final destination is the same as the starting location.
Therefore, in R2R-Back evaluation, we define the success as that an agent firstly arrives 3 meters near to the original destination and then returns 3 meters near to its starting location.
The groundtruth length in the SPL metric is also modified as the total traversed distance in groundtruth trajectory rather than the shortest distance between start and target location. 
As the REVERIE task aims for remote object grounding, the success on REVERIE is defined as arriving at a viewpoint where the target object is visible.

\subsection{Implementation Details}

\noindent\textbf{Training with proxy tasks.}
We sample proxy tasks for each mini-batch to train the HAMT model. The sampling ratio is MLM:MRM:ITM:SAP:SAR:SPREL=5:2:2:1:1:1.
The optimizer is AdamW \cite{loshchilov2017decoupled}.
In the end-to-end training stage, we use image augmentation and regularization techniques to avoid overfitting of the ViT model, including RandAugment \cite{cubuk2020randaugment} and stochastic depth \cite{huang2016deep}.

\noindent\textbf{Fine-tuning for sequential action prediction.}
Due to different goals in various VLN tasks, we design different rewards in reinforcement learning for each downstream VLN dataset.
In R2R, RxR and R4R datasets, the reward is introduced in Section~\ref{sec:model_finetune} to take both goal distance and path fidelity into account.
In R2R-Last, REVERIE and CVDN datasets where the instruction may not describe detailed navigation path, we only use the reduced distance to the goal viewpoints as rewards. We normalize the reduced distance in the same way as in the R2R dataset.
In R2R-Back dataset, we use a different fine-tune strategy to avoid trivial motionless solutions.
We require the agent to predict stop actions twice for the original destination (midpoint) and its starting point (final destination) respectively.
Before arriving at the midpoint, the RL reward is computed based on distances to the midpoint.
If the agent predicts a wrong location to stop for the midpoint, the episode is stopped; otherwise the agent continues its task while receiving rewards based on the distance to the final destination for fine-tuning.
We run each experiment twice for ablation study and use the best result on the validation unseen split for the state-of-the-art comparison.

\section{Experimental results}
\label{sec:more_results}

\subsection{Computation Efficiency}

\begin{wraptable}{r}{0.4\textwidth}
\centering
\small
\vspace{-1.5em}
\caption{Computation time in inference on R2R val unseen split.}
\label{tab:supp_r2r_time}
\begin{tabular}{lccc} \toprule
 & \begin{tabular}[c]{@{}c@{}}Inference\\ Time (s)\end{tabular} & SR & SPL \\ \midrule
RecBERT \cite{hong2020recurrent} & 69 & 63 & 57 \\
HAMT & 104 & 66 & 61 \\
HAMT noT2V & 76 & 65 & 60 \\ \bottomrule
\end{tabular}
\vspace{-1em}
\end{wraptable}

To assess the influence of history encoding on the inference time, we compare HAMT with RecBERT \cite{hong2020recurrent}. The HAMT and RecBERT use the same number of layers in the language transformer and cross-modal transformer. The main difference of two models is in the history encoding and the attended length of history for action prediction. We run each model on the R2R val unseen split (2349 instructions) and report inference times averaged over two runs using a single Tesla P100 GPU. 
For our method we compare variants with and without Text-to-Vision Attention (see Section~\ref{sec:supp_hamt_nov2t}), denoted here as HAMT and HAMT noT2V respectively.
We can see that HAMT and its noT2V variant are only 1.5x and 1.1x slower compared to RecBERT, suggesting that attending to the whole history does not increase the inference time significantly. Moreover, while HAMT noT2V is only 10\% slower compared to \cite{hong2020recurrent}, it still outperforms \cite{hong2020recurrent} in SR and SPL on val unseen split.

\subsection{RxR dataset}

\begin{wraptable}{r}{0.6\textwidth}
\centering
\small
\tabcolsep=0.1cm
\vspace{-1em}
\caption{Navigation performance on RxR test split.}
\label{tab:supp_rxr_test_cmpr}
\begin{tabular}{lccccc} \toprule
 & PL & SR & SPL & nDTW & SDTW \\ \midrule
Multilingual Baseline \cite{ku2020room} & 16.88 & 20.98 & 18.55 & 41.05 & 20.59 \\
Monolingual Baseline \cite{ku2020room} & 17.05 & 25.40 & 22.59 & 41.05 & 20.59 \\
CLIP-ViL & 15.43 & 38.34 & 35.17 & 51.10 & 32.42 \\
CLEAR-CLIP & 16.46 & 40.29 & 36.57 & 53.69 & 34.86 \\
Multilingual HAMT & 19.77 & \textbf{53.12} & \textbf{46.62} & \textbf{59.94} & \textbf{45.19} \\ \midrule
Human & 20.78 & 93.92 & 74.13 & 79.48 & 76.90 \\ \bottomrule
\end{tabular}
\end{wraptable}

As shown in Table~\ref{tab:dataset_stats}, RxR dataset contains much more instructions than R2R dataset. Therefore, we directly use RxR in training proxy tasks rather than R2R with augmented data.
As there are three different languages in RxR, we take advantage of pretrained multilingual BERT \cite{conneau2020unsupervised} to initialize the unimodal language encoder, so we are able to deal with multilingual instructions using the same HAMT model.
We employ the encoder-decoder variant of HAMT for computational efficiency.
For fair comparison with other approaches in RxR testing leaderboard\footnote{\url{https://ai.google.com/research/rxr/competition?active_tab=leaderboard} (25/10/2021).} which adopt pretrained CLIP \cite{radford2021learning} features, we use the same visual features without end-to-end optimization.
Table~\ref{tab:supp_rxr_test_cmpr} presents navigation performances on RxR test split. Our multilingual HAMT model achieves 12.83\% and 6.25\% gains on SR and nDTW respectively than the second place. Nevertheless, there is still a large gap compared to the human performance.
We further present results on val seen and val unseen splits in Table~\ref{tab:supp_rxr_val_cmpr}.

\begin{table}
\centering
\small
\caption{Navigation performances on RxR val seen and val unseen splits.}
\label{tab:supp_rxr_val_cmpr}
\begin{tabular}{lcccccccc} \toprule
 & \multicolumn{4}{c}{Val Seen} & \multicolumn{4}{c}{Val Unseen} \\
 & SR & SPL & nDTW & SDTW & SR & SPL & nDTW & SDTW \\ \midrule
Multilingual Baseline~\cite{ku2020room} & 25.2 & - & 42.2 & 20.7 & 22.8 & - & 38.9 & 18.2 \\
Monolingual Baseline~\cite{ku2020room} & 28.8 & - & 46.8 & 23.8 & 28.5 & - & 44.5 & 23.1 \\ \midrule
Multilingual HAMT & \textbf{59.4} & \textbf{58.9} & \textbf{65.3} & \textbf{50.9}  & \textbf{56.5} & \textbf{56.0} & \textbf{63.1} & \textbf{48.3} \\ \bottomrule
\end{tabular}
\end{table}


\subsection{REVERIE dataset}
The remote object localization task in REVERIE dataset requires both navigation and object grounding. To support the two subtasks in HAMT, we concatenate object features with original view image features for each viewpoint, and add an object grounding head to predict the target object given output embeddings of all object tokens.
We fine-tune HAMT that is end-to-end pretrained on R2R dataset, and use the optimized ViT to extract object features given groundtruth object bounding boxes in REVERIE dataset.
As shown in Table~\ref{tab:supp_reverie_cmpr}, HAMT achieves better navigation performance (SR and SPL), but the object grounding performance (RGS and RGSPL) on test split is worse than state of the art. 
Since HAMT can more effectively encode observed visual scenes and actions in the history sequence, it is able to better understand house environments and navigate to target viewpoints more efficiently as shown in the much higher SPL score.
However, as we use ViT optimized on R2R dataset to extract object features, the object representation might not be as generalizable as object features used in previous works which are pretrained on large-scale object detection datasets.


\begin{table}
\small
\centering
\tabcolsep=0.08cm
\caption{Navigation and object grounding performances on REVERIE val unseen and test splits.}
\label{tab:supp_reverie_cmpr}
\begin{tabular}{lcccccccccccc} \toprule
\multirow{3}{*}{Methods} & \multicolumn{6}{c}{Validation Unseen} & \multicolumn{6}{c}{Test Unseen} \\
\multicolumn{1}{c}{} & \multicolumn{4}{c}{Navigation} & \multicolumn{2}{c}{Grounding} & \multicolumn{4}{c}{Navigation} & \multicolumn{2}{c}{Grounding}  \\ 
 & TL & SR & OSR & SPL & RGS & RGSPL & TL & SR & OSR & SPL & RGS & RGSPL \\ \midrule Seq2Seq \cite{anderson2018vision} & 11.07 & 4.20 & 8.07 & 2.84 & 2.16 & 1.63 & 10.89 & 3.99 & 6.88 & 3.09 & 2.00 & 1.58 \\
RCM \cite{wang2019reinforced} & 11.98 & 9.29 & 14.23 & 6.97 & 4.89 & 3.89  & 10.60 & 7.84 & 11.68 & 6.67 & 3.67 & 3.14 \\
SMNA \cite{ma2019self} & 9.07 & 8.15 & 11.28 & 6.44 & 4.54 & 3.61  & 9.23 & 5.80 & 8.39 & 4.53 & 3.10 & 2.39 \\
FAST-MATTN \cite{qi2020reverie} & 45.28 & 14.40 & 28.20 & 7.19 & 7.84 & 4.67 & 39.05 & 19.88 & 30.63 & 11.6 & 11.28 & 6.08 \\
SIA \cite{lin2021scene} & 41.53 & 31.53 & \textbf{44.67} & 16.28 & \textbf{22.41} & 11.56 & 48.61 & \textbf{30.80} & \textbf{44.56} & 14.85  & \textbf{19.02} & 9.20 \\
RecBERT \cite{hong2020recurrent} & 16.78 & 30.67 & 35.02 & 24.90 & 18.77 & 15.27 & 15.86 & 29.61 & 32.91 & 23.99 & 16.50 & \textbf{13.51} \\ \midrule
HAMT & 14.08 & \textbf{32.95} & 36.84 & \textbf{30.20} & 18.92 & \textbf{17.28} & 13.62 & 30.40 & 33.41 & \textbf{26.67} & 14.88  & 13.08 \\ \bottomrule
\end{tabular}
\end{table}




\section{Additional ablations}
\label{sec:more_ablation_results}
\subsection{History in training with proxy tasks}

We show that the history input plays a critical role for training with proxy tasks.
We compare HAMT with history input and PREVALENT \cite{hao2020towards} without history.
For fair comparison, we re-implement PREVALENT which only takes instruction  and single-step observation  as input and the other architectures are set the same as HAMT.
We train PREVALENT with all proxy tasks except the ITM task because there is no trajectory input in PREVALENT for instruction-trajectory matching.
ViT features pretrained on ImageNet are used in this experiment.


\begin{wrapfigure}{r}{0.4\textwidth}
\centering
	\includegraphics[width=\linewidth]{figures/pretrain_sap_cmpr}
	\caption{SAP accuracy of PREVALENT (w/o history) and HAMT (w/ history) on R2R dataset.}
	\label{fig:pretrain_sap_cmpr}
\end{wrapfigure}
In Figure~\ref{fig:pretrain_sap_cmpr}, we present the single-step action prediction (SAP) accuracy of HAMT and PREVALENT during the training.
The SAP accuracies on val seen split are similar for the two models, however, PREVALENT performs much worse on the val unseen split than HAMT.
Due to the capacity of large-scale transformer, PREVALENT is likely to memorize the map structure of seen houses, and thus achieves comparable performance to HAMT.
However, such knowledge cannot be transferred to unseen houses because the structure and visual observations are distinct for seen and unseen houses.
Feeding history as inputs avoids the model simply cramming the structure of seen houses, and enables it to align the history with an instruction to predict actions for better generalization.
After fine-tuning the two models on R2R dataset, we obtain SPL 57.5 on val unseen split for HAMT, while 52.7 for PREVALENT without history input.
As the same proxy tasks are used in training, the large gains of our HAMT model contribute to the history encoding.
Therefore, \textbf{the proposed history encoding can largely improve the navigation performance on top of training proxy tasks.}

\subsection{Visual features in training with proxy tasks}
\begin{wraptable}{r}{0.4\textwidth}
	\centering
	\small
	\tabcolsep=0.1cm
	\vspace{-2.5em}
    \caption{Comparison of features (same notations as Table~\ref{tab:r2r_vit_e2e}).}
    \label{tab:supp_r2r_vit_e2e}
    \begin{tabular}{ccccccc} \toprule
     &  &  & \multicolumn{2}{c}{Val Seen} & \multicolumn{2}{c}{Val Unseen} \\
    Features & PT & e2e & SR & SPL & SR & SPL \\ \midrule
    \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Resnet \\ 152\end{tabular}} &  &  & 65.5 & 61.3 & 54.4 & 48.7 \\
    & \checkmark &  & 69.3 & 64.8 & 63.5 & 57.5 \\ \midrule
    \multirow{3}{*}{ViT} &  &  & 68.8 & 66.1 & 56.3 & 52.5 \\
     & \checkmark &  & \textbf{75.7} & \textbf{72.5} & 64.4 & 58.8 \\
     & \checkmark & \checkmark & 75.0 & 71.7 & \textbf{65.7} & \textbf{60.9} \\ \bottomrule
    \end{tabular}
	\vspace{-1em}
\end{wraptable}
Table~\ref{tab:supp_r2r_vit_e2e} provides an additional experiment in the third row compared to Table~\ref{tab:r2r_vit_e2e}.
It demonstrates that ViT features outperform ResNet152 features with and without training proxy tasks.
Comparing the last two rows in Table~\ref{tab:supp_r2r_vit_e2e}, end-to-end feature optimization improves SPL by 2.1\% on val unseen split but decreases SPL by 0.8\% on val seen split. Note that we follow previous VLN works \cite{hong2020recurrent,tan2019learning} to select the best model based on val unseen and use the same model for val seen split. We observe that the performance on val seen split can be improved with longer training time. After optimizing visual representations, HAMT converges faster on val unseen split and achieves the best performance at earlier iterations. Therefore, the performance on val seen split is slightly worse than no end-to-end optimization. If training longer, the performance with optimized ViT features on val seen split can be higher.

\subsection{Different proxy tasks in end-to-end training}
\begin{wraptable}{l}{0.4\textwidth}
\centering
\small
\vspace{-1em}
\tabcolsep=0.1cm
\caption{Comparison of different proxy tasks in end-to-end optimization.}
\label{tab:supp_proxy_e2e_ablations}
\begin{tabular}{cccccc} \toprule
 &  & \multicolumn{2}{c}{Val Seen} & \multicolumn{2}{c}{Val Unseen} \\
SAP(R) & SPREL & SR & SPL & SR & SPL \\ \midrule
 &  & 70.1 & 65.9 & 63.3 & 57.7 \\
\checkmark &  & 72.5 & 69.2 & 64.5 & 59.4 \\
\checkmark & \checkmark & \textbf{75.0} & \textbf{71.7} & \textbf{65.7} & \textbf{60.9} \\ \bottomrule
\end{tabular}
\end{wraptable}

In Table~\ref{tab:r2r_proxy_tasks} of the main paper, we fix ViT features to ablate contributions of different proxy tasks in training.
We further present the ablation results in a fully end-to-end training setup in Table~\ref{tab:supp_proxy_e2e_ablations}, where different proxy tasks are used to train HAMT including the ViT features.
The results show the same trend as Table~\ref{tab:r2r_proxy_tasks}, where our proposed two new proxy tasks (SAP/R and SPREL) are beneficial.
Moreover, we can see that the end-to-end ViT features are superior to fixed ViT features in Table~\ref{tab:r2r_proxy_tasks} on val unseen split for all the three proxy task combinations.

\subsection{Two-stage end-to-end (e2e) training strategy}
We compare our two-stage e2e training strategy with a single-stage e2e training of HAMT. 
However, single-stage e2e training achieves inferior performance to the two-stage training or even no e2e training. 
When trained for 25k iterations and evaluated on the val unseen split, the single-stage e2e training of HAMT results in SPL 53.5 while no e2e training achieves SPL 56.5. We hypothesize that the single-stage e2e training is less effective for VLN given (a) the limited training data available for the VLN task and (b) the higher complexity of VLN compared to common vision and language tasks.

\subsection{History encoding in long-horizon VLN task}

We compare different history encoding approaches on the R2R-Back dataset to show that the history information is more beneficial for the long-horizon VLN task.
Table~\ref{tab:r2rback_ablation} presents navigation results. All the models are initialized from weights after training with proxy tasks.
In order to successfully return back, the agent should remember the way it comes to the targets. The recurrent state is insufficient to capture all the information and achieves the worst navigation performance.
Encoding agent's oriented view at each step in temporal-only model improves over the recurrent approach. However, as the oriented view of the agent in backward trajectory is different from the view in forward trajectory, temporal-only model does not take advantage of the full memory in previous exploration and performs inferior to our hierarchical history encoding model.
It demonstrates the effectiveness of our proposed method in long-horizon VLN task that requires long-term dependency.
We also show that using the end-to-end trained ViT features further benefits the navigation performance.


\begin{table}[h]
	\centering
	\small
	\vspace{-1em}
	\tabcolsep=0.12cm
	\caption{Navigation results for R2R-Back dataset.}
	\label{tab:r2rback_ablation}
	\begin{tabular}{cccccccccccc} \toprule
		\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}History \\ Encoding\end{tabular}} & \multirow{2}{*}{e2e} & \multicolumn{5}{c}{Val Seen} & \multicolumn{5}{c}{Val Unseen} \\ 
		&  & TL & SR & SPL & nDTW & SDTW & TL & SR & SPL & nDTW & SDTW \\ \midrule
		Recurrent &  & 22.33 & 51.4 & 48.4 & 67.3 & 45.7 & 23.35 & 41.1 & 37.7 & 58.2 & 35.6 \\
		Temporal-only &  & 22.70 & 51.6 & 49.6 & 67.8 & 46.7 & 22.93 & 45.1 & 42.9 & 62.7 & 40.2 \\
		Hierarchical &  & 23.52 & \textbf{66.8} & \textbf{63.5} & \textbf{73.8} & \textbf{60.4} & 24.58 & 56.5 & 51.7 & 63.6 & 48.4 \\
		Hierarchical & \checkmark & \textbf{22.76} & 64.8 & 61.8 & 73.7 & 58.9 & \textbf{23.78} & \textbf{57.2} & \textbf{53.1} & \textbf{65.1} & \textbf{49.5} \\ \bottomrule
	\end{tabular}
\end{table}


\subsection{Structure variants in fine-tuning}
\begin{wraptable}{r}{0.4\linewidth}
	\centering
	\small
	\vspace{-2.5em}
	\tabcolsep=0.1cm
	\caption{Comparison of using different tokens in  in fine-tuning.}
	\label{tab:act_token_expr}
	\begin{tabular}{ccccccc} \toprule
		\multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Action \\ Prediction Token\end{tabular}} & \multicolumn{2}{c}{Val Seen} & \multicolumn{2}{c}{Val Unseen} \\ \midrule
		obs & txt & hist & SR & SPL & SR & SPL \\ \midrule
		\checkmark &  &  & 76.1 & 72.8 & \textbf{66.0} & 60.3 \\
		\checkmark & \checkmark &  & 75.0 & 71.7 & 65.7 & \textbf{60.9} \\
		\checkmark &  & \checkmark & \textbf{78.0} & \textbf{75.9} & 65.5 & 60.2 \\
		\checkmark & \checkmark & \checkmark & 76.3 & 73.4 & 65.5 & 60.9 \\ \bottomrule
	\end{tabular}
	\vspace{-1em}
\end{wraptable}
Our model reuses the  in training proxy tasks to sequentially predict action in fine-tuning.
In Table~\ref{tab:act_token_expr}, we compare using different input tokens for the action prediction in , including different combinations of the observation token , global history token  and special text token .
We can see that the performance varies little on the val unseen split, which indicates that the cross-modal transformer in our model is able to effectively fuse different modalities so that the performance is influenced little by tokens used in prediction.






\section{Qualitative results}
\label{sec:viz_results}

Figures~\ref{fig:ex_02}-\ref{fig:ex_01} illustrate trajectories obtained by our HAMT model and compare them to results of the state-of-the-art RecBERT~\cite{hong2020recurrent} model.
We can see that HAMT enables to better interpret instructions (Figure~\ref{fig:ex_02}), recognize the scene (Figure~\ref{fig:ex_03}), follow the correct direction (Figure~\ref{fig:ex_04}), and align the current observation with the instruction (Figure~\ref{fig:ex_01}). We also provide some failure cases in Figures~\ref{fig:fail_02}-\ref{fig:fail_01}, where the HAMT model still needs improvements on scene and object recognition.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_02_rec}
		\caption{Predicted trajectory by RecBERT~\cite{hong2020recurrent} (failed).}
		\label{fig:ex_02_rec}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_02_hamt}
		\caption{Predicted trajectory by HAMT (succeed).}
		\label{fig:ex_02_hamt}
	\end{subfigure}
	\caption{Examples in R2R val unseen split. Navigation steps inside red box are incorrect. The instruction is ``Walk to the right of the stairs. Continue past and to the right of the stairs that go down. Turn right and stop in the doorway of the double glass doors.'' (id: 697\_0). The RecBERT misunderstands the instruction and goes down the stairs instead of turning right. Our HAMT is better to understand the instruction and spatial relation related to the stairs to turn to the right of the stairs.}
	\label{fig:ex_02}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_03_rec}
		\caption{Predicted trajectory by RecBERT~\cite{hong2020recurrent} (failed).}
		\label{fig:ex_03_rec}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_03_hamt}
		\caption{Predicted trajectory by HAMT (succeed).}
		\label{fig:ex_03_hamt}
	\end{subfigure}
	\caption{Examples in R2R val unseen split. Navigation steps inside red box are incorrect. The instruction is ``Walk into the kitchen area. Walk by the sink and oven. Walk straight into the hallway. Turn right into the little room. Turn left and walk into the bedroom. Stop by the corner of the bed.'' (id: 155\_0). The RecBERT fails to recognize the kitchen area and navigates back and forth in wrong locations. Our HAMT correctly recognizes the kitchen and follows the instruction.}
	\label{fig:ex_03}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_04_rec}
		\caption{Predicted trajectory by RecBERT~\cite{hong2020recurrent} (failed).}
		\label{fig:ex_04_rec}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_04_hamt}
		\caption{Predicted trajectory by HAMT (succeed).}
		\label{fig:ex_04_hamt}
	\end{subfigure}
	\caption{Examples in R2R val unseen split. Navigation steps inside red box are incorrect. The instruction is ``Walk straight until you get to a room that has a black table on the left with flowers on it. Wait there.'' (id: 4182\_2). The RecBERT takes the wrong direction at the first step, while our HAMT follows the instruction and successfully stops.}
	\label{fig:ex_04}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_01_rec}
		\caption{Predicted trajectory by RecBERT~\cite{hong2020recurrent} (failed).}
		\label{fig:ex_01_rec}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ex_01_hamt}
		\caption{Predicted trajectory by HAMT (succeed).}
		\label{fig:ex_01_hamt}
	\end{subfigure}
	\caption{Examples in R2R val unseen split. Navigation steps inside red box are incorrect. The instruction is ``Walk out of the bathroom and turn right. Turn left and walk down the hallway. Turn right and stop by the end table.'' (id: 5153\_0). The RecBERT correctly performs the first two turns but fails to track the third turn right action and stops incorrectly. Our HAMT is better to align the current state with the instruction to correctly perform  the third turn right action.}
	\label{fig:ex_01}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/failure_cases_02_gt}
		\caption{groundtruth trajectory.}
		\label{fig:fail_02_gt}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/failure_cases_02_hamt}
		\caption{Predicted trajectory by HAMT (failed).}
		\label{fig:fail_02_hamt}
	\end{subfigure}
	\caption{Failure cases in R2R val unseen split. The instruction is ``Go stand underneath the stairs, next to the liquor shelf. '' (id: 36968\_2). Though HAMT correctly goes towards the direction, it fails to recognize the liquor shelf and results in exploring further the room until reaching the maximum number of navigation steps.}
	\label{fig:fail_02}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/failure_cases_01_gt}
		\caption{Groundtruth trajectory.}
		\label{fig:fail_01_gt}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/failure_cases_01_hamt}
		\caption{Predicted trajectory by HAMT (failed).}
		\label{fig:fail_01_hamt}
	\end{subfigure}
	\caption{Failure cases in R2R val unseen split. The instruction is ``With the low stone or concrete barrier behind you, walk parallel to the board covering the floor and turn left before reaching the end. Move forward to leave the wooden flooring and when on the stone flooring, turn right and stand in front of the doors leading out of the room.'' (id: 5873\_1). As the scene is unusual, HAMT fails to locate itself in the correct direction at the first step.}
	\label{fig:fail_01}
\end{figure}

