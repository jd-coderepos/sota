\documentclass[a4paper,USenglish,numberwithinsect]{lipics}
\usepackage{graphicx}
\usepackage{thmtools}
\usepackage{enumerate}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,automata}
\usepackage{multirow}
\usepackage{hyperref}

\theoremstyle{plain}
\newtheorem{lemma-indexed-by-theorem}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{example-indexed-by-theorem}[theorem]{Example}

\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1\right\rceil}
\newcommand{\suchthat}{\:|\:}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathsf{P}}
\newcommand{\sharpP}{\ensuremath{\mathsf{\# P}}}
\newcommand{\NP}{\ensuremath{\mathsf{NP}}}
\newcommand{\SAT}{SAT}
\newcommand{\ApproxMC}{\ensuremath{\mathsf{ApproxMC}}}
\newcommand{\UniGen}{\ensuremath{\mathsf{UniGen}}}

\newcommand{\improvs}{I}
\newcommand{\valids}{A}
\newcommand{\creative}{admissible}
\newcommand{\wref}{w_{\text{ref}}}



\begin{document}

\title{Control Improvisation\footnote{This is a preliminary version of the paper, which has since been substantially extended: please see Fremont et al.~2017 \cite{jacm-preprint}.}}
\author{Daniel J. Fremont}
\author{Alexandre Donz\'e}
\author{Sanjit A. Seshia}
\author{David Wessel}
\affil{University of California, Berkeley\\\{\texttt{dfremont}, \texttt{donze}, \texttt{sseshia}\}\texttt{@berkeley.edu}}
\Copyright{Daniel J. Fremont, Alexandre Donz\'e, Sanjit A. Seshia, and David Wessel}
\subjclass{F.4.3 Formal Languages, G.3 Probability and Statistics, F.2.2 Nonnumerical Algorithms and Problems}
\keywords{finite automata, random sampling, Boolean satisfiability, testing, computational music, control theory}

\maketitle

\begin{abstract} 
We formalize and analyze a new automata-theoretic
problem termed {\em control improvisation}. Given an automaton, the
problem is to produce an \emph{improviser}, a probabilistic algorithm
that randomly generates words in its language, subject to two
additional constraints: the satisfaction of an \emph{admissibility}
predicate, and the exhibition of a specified amount of
randomness. Control improvisation
has multiple applications, including, for example, 
generating musical improvisations
that satisfy rhythmic and melodic constraints, where admissibility is
determined by some bounded divergence from a reference melody. We
analyze the complexity of the control improvisation problem, giving
cases where it is efficiently solvable and cases where it is
\sharpP-hard or undecidable. We also show how symbolic techniques
based on Boolean satisfiability (SAT) solvers can be used to approximately solve some of the
intractable cases.
\end{abstract} 

\section{Introduction}
\label{sec:intro}

We introduce and formally characterize a new automata-theoretic problem termed {\em control
improvisation}. Given an automaton, the problem is to produce an \emph{improviser}, a probabilistic
algorithm that randomly generates words in the language of the automaton, subject to two additional
constraints: each generated word must satisfy an \emph{admissibility} predicate, and the improviser
must exhibit a specified amount of randomness.

The original motivation for this problem arose from a topic known as {\em machine improvisation of
music}~\cite{rowe-2001}.  Here, the goal is to create algorithms which can generate variations of a
reference melody like those commonly improvised by human performers, for example in jazz.  Such an
algorithm should have three key properties.  First, the melodies it generates should conform to rhythmic
and melodic constraints typifying the music style (e.g. in jazz, the melodies should follow the
harmonic conventions of that genre). Second, the algorithm should be sufficiently randomized that
running it several times produces a variety of different improvisations. Finally, the generated
melodies should be actual variations on the reference melody, neither reproducing it exactly nor being
so different as to be unrecognizable. In previous work~\cite{donze-icmc14}, we identified these
properties in an initial definition of the control improvisation problem, and applied it to the
generation of monophonic (solo) melodies over a given jazz song harmonization\footnote{Examples of
improvised melodies can be found at the following URL:\\
\url{http://www.eecs.berkeley.edu/~donze/impro_page.html}.}.
 
These three properties of a generation algorithm are not specific to music.  Consider
\emph{black-box fuzz testing} \cite{fuzzing-book}, which produces many inputs to a program hoping to
trigger a bug.  Often, constraints are imposed on the generated inputs, e.g. in \emph{generative}
fuzz testing approaches which enforce an appropriate format so that the input is not rejected
immediately by a parser.  Also common are \emph{mutational} approaches which guide the generation
process with a set of real-world seed inputs, generating only inputs which are variations of those
in the set. And of course, fuzzers use randomness to ensure that a variety of inputs are tried.
Thus we see that the inputs generated in fuzz testing have the same general requirements as music
improvisations: satisfying a set of constraints, being appropriately similar/dissimilar to a
reference, and being sufficiently diverse.

We propose control improvisation as a precisely-defined theoretical problem capturing these
requirements, which are common not just to the two examples above but to many other generation
problems.  Potential applications also include home automation mimicking typical occupant behavior (e.g.,
randomized lighting control obeying time-of-day constraints and limits on energy
usage~\cite{lee-personal13}) and randomized variants of the supervisory control problem
\cite{lafortune06}, where a controller keeps the behavior of a system within a safe operating region
(the language of an automaton) while adding diversity to its behavior via randomness.  A typical example
of the latter is surveillance: the path of a patrolling robot should satisfy various constraints
(e.g. not running into obstacles) and be similar to a predefined route, but incorporate some
randomness so that its location is not too predictable \cite{lafortune-personal15}.

Our focus, in this paper, is on the {\em theoretical characterization of
control improvisation}. Specifically, we give a precise theoretical
definition and a rigorous characterization of the complexity of the
control improvisation problem under various conditions on the inputs
to the problem. While the problem is distinct from any other we have
encountered in the literature, our methods are closely connected to
prior work on random sampling from the languages of automata and
grammars \cite{hickey-cohen,denise2006,sharpNFA}, and sampling from
the satisfying assignments of a Boolean formula~\cite{unigen}.
Probabilistic programming techniques~\cite{probprog} could be used
for sampling under constraints, but the present methods cannot be used
to construct improvisers meeting our definition.

In summary, this paper makes the following novel contributions:
\begin{itemize}
\item Formal definitions of the notions of control improvisation (CI) and a
polynomial-time improvisation scheme (Sec.~\ref{sec:prelim});
\item A theoretical characterization of the conditions under which
improvisers exist (Sec.~\ref{section:existence});
\item A polynomial-time improvisation scheme for a practical class of
CI instances, involving finite-memory admissibility predicates
(Sec.~\ref{section:finite-memory}); 
\item \sharpP-hardness and undecidability results for more general classes of
the problem (Sec.~\ref{section:complex-automata}); 
\item A symbolic approach based on Boolean satisfiability (SAT)
solving that is useful in the case when the automata
are finite-state but too large to represent explicitly (Sec.~\ref{section:symbolic}).
\end{itemize}
We conclude in Sec.~\ref{section:conclusion} with a
synopsis of results and directions for future work.
For lack of space, we include only selected proofs and proof sketches in the main body
of the paper; complete details may be found in the Appendix.

\section{Background and Problem Definition}
\label{sec:prelim}

In this section, we first provide some background on a previous
automata-theoretic method for music improvisation based on a data
structure called the {\em factor oracle}. We then provide a formal
definition of the control improvisation problem while explaining the
choices made in this definition.

\subsection{Factor Oracles}

An effective and practical approach to machine improvisation of music (used for example in the prominent OMax system \cite{omax}) is based on a data structure
called the factor oracle \cite{AssayagD04,Cleophas03constructingfactor}. Given a word
 of length  that is a symbolic encoding of a reference melody, a factor oracle  is an
automaton constructed from  with the following key properties:  has  states, all
accepting, chained linearly with direct transitions labelled with the letters in , and with
potentially additional forward and backward transitions. Figure \ref{figure:factor-oracle} depicts
 for .  A word  accepted by  consists of concatenated ``factors'' of ,
and its dissimilarity with  is correlated with the number of non-direct transitions. By
assigning a small probability  to non-direct transitions,  becomes a generative Markov
model with tunable ``divergence'' from . In order to impose more musical structure on the
generated words, our previous work~\cite{donze-icmc14} additionally requires that improvisations
satisfy rules encoded as deterministic finite automata, by taking the product of the generative
Markov model and the DFAs. While this approach is heuristic and lacks any formal guarantees, it has
the basic elements common to machine improvisation schemes: (i) it involves randomly generating
strings from a formal language typically encoded as an automaton, (ii) it enforces diversity in the
generated strings, and (iii) it includes a requirement on which strings are admissible based on their
divergence from a reference string. The definition we propose below captures these elements in a
rigorous theoretical manner, suitable for further analysis.  In Sec.~\ref{section:finite-memory}, we
revisit the factor oracle, sketching how the notion of divergence from  that it represents can be
encoded in our formalism.
 
 {
\setlength{\intextsep}{8pt}
\setlength{\belowcaptionskip}{-5pt}
\setlength{\abovecaptionskip}{0pt}
\begin{figure}[tb]
\centering
\begin{tikzpicture}[initial text=, transform shape, scale=0.8]

 \node[accepting, state, initial] (s0) {}; 
 \node[accepting, state, right= of s0] (s1) {}; 
 \node[accepting, state, right= of s1] (s2) {};
 \node[accepting, state, right= of s2] (s3) {}; 
 \node[accepting, state, right= of s3] (s4) {};

\path[->] 
 (s0) edge node [above] {} (s1)
 (s1) edge node [above] {} (s2)    
 (s2) edge node [above] {} (s3)
 (s3) edge node [above] {} (s4) 
(s0) edge [bend left=40] node [above] {} (s3)
 (s0) edge [bend left=50] node [above] {} (s4)
 (s1) edge [bend left] node [above] {} (s3)
(s1) edge [bend left] node [above] {} (s0) 
 (s2) edge [bend left ] node [above] {} (s1) 
 (s3) edge [bend left] node [above] {} (s0) 
 (s4) edge [bend left] node [above] {} (s0); 

\end{tikzpicture}
\caption{Factor oracle constructed from the word .}
\label{figure:factor-oracle}
\end{figure}
}
 
\subsection{Problem Definition}

We abbreviate deterministic and nondeterministic finite
automata as DFAs and NFAs respectively. 
We use the standard definition of probabilistic finite automata from
\cite{rabin-pfas}, where a string is accepted iff it causes the
automaton to reach an accepting state with probability greater than a
specified \emph{cut-point} . 
We call a probabilistic finite automaton, together with a choice of
cut-point so that its language is definite, a PFA. 
We write  for the probability of event
 given that the random variable  is drawn from the
distribution . 

\begin{definition}
An \emph{improvisation automaton} is a finite automaton (DFA, NFA, or
PFA)  over a finite alphabet . An
\emph{improvisation} is any word , and
 is the set of all improvisations. 
\label{defn:improv-auto}
\end{definition}

\begin{definition}
An \emph{admissibility predicate} is a computable predicate . An improvisation  is
\emph{\creative} if . We write  for the set of
all {\creative} improvisations. 
\label{defn:admiss}
\end{definition}

\begin{subparagraph}{Running Example.}
Our concepts will be illustrated with a simple 
example. Our aim is to produce variations of the binary string  of length 3, subject to the constraint that there cannot be two
consecutive 1s. So , and 
 is a DFA which accepts all length-3 strings 
that do not have two 1s in a row. To ensure that our
variations are similar to , we let our admissibility predicate
 be 1 if the Hamming distance between  and  is at
most 1, and 0 otherwise. Then the improvisations are the strings
, , , , and , of which , , and
 are admissible. \\
\end{subparagraph}

Intuitively, an improviser samples from the set of improvisations according to some
distribution. But what requirements must one impose on this distribution?  Since we want a variety
of improvisations, we require that each one is generated with probability at most some bound
. By choosing a small value of  we can thus ensure that many different improvisations
can be generated, and that no single one is output too frequently. Other constraints are possible,
e.g. requiring that every improvisation have nonzero probability, but we view this as too
restrictive: if there are a large number of possible improvisations, it should be acceptable for an
improviser to generate many but not all of them.
Another possibility would be to ensure variety by imposing some minimum distance between the improvisations.
This could be reasonable in a setting (such as music) where there is a natural metric on the space of improvisations, but we choose to keep our setting general and not assume such a metric.
Finally, we require our generated improvisation to
be admissible with probability at least  for some specified . When the
admissibility predicate encodes a notion of similarity to a reference string, for example, this allows us to
require that our improvisations usually be similar to the reference. Combining these requirements,
we obtain our definitions of an acceptable distribution over improvisations and thus of an
improviser:

\begin{definition}
Given 
with  and  as in
Definitions~\ref{defn:improv-auto} and~\ref{defn:admiss},  an error probability, and  a
probability 
bound, a distribution  with support  is an
\emph{-improvising distribution} if:
\begin{itemize}
\item 
\item 
\item 
\end{itemize}
If there is an -improvising distribution, we say that
 is -\emph{feasible} (or simply {\em feasible}). An 
\emph{-improviser} (or simply {\em improviser}) for a feasible  
is an expected finite-time
probabilistic algorithm generating strings in 
whose output distribution (on empty
input) is an -improvising distribution.
\label{defn:feasible}
\end{definition}

To summarize, if  is feasible, there
exists a distribution satisfying the requirements in Definition~\ref{defn:feasible}, and an
improviser is a probabilistic algorithm for sampling from one.

\begin{subparagraph}{Running Example.}
For our running example,  is not feasible since  
means we can only generate admissible improvisations, and since there
are only 3 of those we cannot possibly give them all probability at
most . Increasing  to  would make 
feasible. Increasing  to  would also work, allowing us
to return an inadmissible improvisation  of the time:
an algorithm uniformly sampling from  would be
an improviser for . 
\end{subparagraph}

\begin{definition}
Given ,
the \emph{control improvisation (CI)} problem is to decide whether
 is feasible, and if so to generate an improviser for
. 
\end{definition}

Ideally, we would like an efficient algorithm to solve the CI problem. Furthermore,
the improvisers our algorithm produces should themselves be efficient, in the sense
that their runtimes are polynomial in the size of the original CI
instance. This leads to our last definition: 

\begin{definition}
A \emph{polynomial-time improvisation scheme} for a class
 of CI instances is a polynomial-time algorithm 
with the following properties: 
\begin{itemize}
\item for any , if  is
feasible then  is an improviser for , and
otherwise   
\item there is a polynomial 
such that if , then 
has expected runtime at most . 
\end{itemize}
\end{definition}

A polynomial-time improvisation scheme for a class of CI instances is
an efficient, uniform way to solve the control improvisation problem
for that class. In Sections \ref{section:finite-memory} and
\ref{section:complex-automata} we will investigate which classes have
such improvisation schemes. 

\section{Existence of Improvisers} \label{section:existence}

It turns out that the feasibility of an improvisation problem is completely determined by the sizes of  and :

\begin{theorem} \label{theorem:feasibility}
For any , the following are equivalent:
\begin{enumerate}[\quad (a)]
\item  is feasible.
\item  and .
\item There is an improviser for .
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{description}
\item[(a)(b):] Suppose  is an -improvising distribution with support . Then , so . We also have , so .

\item[(b)(c):] Defining , we have . If , then there is a subset  with . Since , the uniform distribution on  is a -improvising distribution. Since this distribution has finite support and rational probabilities, there is an expected finite-time probabilistic algorithm sampling from it, and this is a -improviser. If instead , defining  we have . Since , there are disjoint subsets  and  with  and . Let  be the distribution on  where each element of  has probability  and each element of  has probability . Then , so  is a -improvising distribution. As above there is an expected finite-time probabilistic algorithm sampling from , and this is an -improviser.

\item[(c)(a):] Immediate. \qedhere
\end{description} 
\end{proof}

\begin{remark}
In fact, whenever  is feasible, the construction in the proof of Theorem \ref{theorem:feasibility} gives an improviser which works in nearly the most trivial possible way: it has two finite lists  and , flips a (biased) coin to decide which list to use, and then returns an element of that list uniformly at random.
\end{remark}

A consequence of this characterization is that when there are infinitely-many admissible improvisations, there is an improviser with zero error probability:

\begin{corollary}
If  is infinite,  is feasible for any .
\end{corollary}

In addition to giving conditions for feasibility, Theorem \ref{theorem:feasibility} yields an algorithm which is guaranteed to find an improviser for any feasible CI problem.

\begin{corollary}
If  is feasible, an improviser for  may be found by an effective procedure.
\end{corollary}
\begin{proof}
The sets  and  are clearly computably enumerable, since  is computable. We enumerate  and  until enough elements are found to perform the construction in Theorem \ref{theorem:feasibility}. Since  is feasible, the theorem ensures this search will terminate.
\end{proof}

We cannot give an upper bound on the time needed by this algorithm without knowing something about the admissibility predicate . Therefore although as noted in the remark above whenever there are improvisers at all there is one of a nearly-trivial form, actually finding such an improviser could be difficult. In fact, it could be faster to generate an improviser which is \emph{not} of this form, as seen for example in Sec.~\ref{section:finite-memory}.

\begin{corollary}
The set of feasible CI instances is computably enumerable but not computable.
\end{corollary}
\begin{proof}
Enumerability follows immediately from the previous Corollary. If checking whether  is feasible were decidable, then so would be checking if , but this is undecidable since  can be an arbitrary computable predicate.
\end{proof}

\section{Finite-Memory Admissibility Predicates} \label{section:finite-memory}

In order to bound the time needed to find an improviser, we must
constrain the admissibility predicate . Perhaps the simplest
type of admissibility predicate is one which can be computed by a DFA,
i.e., one such that there is some DFA  which accepts a
word  iff . This captures the notion of
a \emph{finite-memory} admissibility predicate, where admissibility of
a word can be determined by scanning the word left-to-right, only
being able to remember a finite number of already-seen symbols. An
example of a finite-memory predicate  is one 
such that  iff each subword of 
of a fixed constant length satisfies some condition. By the
pumping lemma, such predicates have the property that continually
repeating some section of a word can produce an infinite family of
improvisations, which could be a disadvantage if looking for
``creative'', non-repetitive improvisations. However, in applications
such as music we impose a maximum length on improvisations, so this is
not an issue. 

\begin{example-indexed-by-theorem}[Factor Oracles]
Recall that one way of measuring the divergence of an improvisation
 generated by the factor oracle  built from a word  is by
counting the number of non-direct transitions that  causes  to
take. Since DFAs cannot count without bound, we can use a sliding window of some
finite size . Then our admissibility predicate  can be that
at any point as  processes , the number of the previous 
transitions which were non-direct lies in some interval 
with . This predicate can be encoded as a DFA
of size  (see the Appendix for details). The size of
the automaton grows exponentially in the size of the window, but for
small windows it can be reasonable. 
\end{example-indexed-by-theorem}

When the admissibility predicate is finite-memory and the automaton
 is a DFA, there is an efficient procedure to test
if an improviser exists and synthesize one if so. The construction is
similar to that of Theorem \ref{theorem:feasibility}, but avoids explicit enumeration of all
improvisations to be put in the range of the improviser. To avoid
enumeration we use a classic method of uniformly sampling from the
language of a DFA  (see for example
\cite{hickey-cohen,denise2006}). The next few lemmas summarize the
results we need, proofs being given in the Appendix for
completeness. The first step is to determine the size of the
language. 

\begin{restatable}{lemma-indexed-by-theorem}{lemmaDFACounting} \label{lemma-dfa-counting}
If  is a DFA,  can be computed in polynomial time.
\end{restatable}

Once we know the size of  we can efficiently sample
from it, handling infinite languages by sampling from a finite subset
of a desired size. 

\begin{restatable}{lemma-indexed-by-theorem}{lemmaDFAPumpSamp} \label{lemma-dfa-pump-samp}
There is a polynomial  such that for any  and DFA  with infinite language, there is a probabilistic algorithm  which uniformly samples from a subset of  of size  in expected time at most , and which can be constructed in the same time.
\end{restatable}

\begin{restatable}{lemma-indexed-by-theorem}{lemmaDFAUnifSamp} \label{lemma-dfa-unif-samp}
There is a polynomial  such that for any DFA  with finite language, there is a probabilistic algorithm  which uniformly samples from  in expected time at most , and which can be constructed in the same time.
\end{restatable}

Using these sampling techniques, we have the following:

\begin{theorem} \label{theorem-dfa-scheme}
The class of CI instances  where  is
a DFA and  is computable by a DFA has a polynomial-time
improvisation scheme.  
\end{theorem}
\begin{proof}
The proof considers five cases. We first define some notation. 
Let  denote the DFA giving . Letting
 be the product of 
and , we have . This
product can be computed in polynomial time since the automata are both
DFAs, and  is polynomial in  and
. In some of the cases below we will also use a DFA
 which is the synchronous product of 
and the complement of . Clearly , and the size of  and the
time needed to construct it are also polynomial in  and
. 

Next we compute  and  in polynomial time using Lemma
\ref{lemma-dfa-counting}. There are now several cases (illustrated in Figure
\ref{figure-proof-cases}): 

{
\setlength{\intextsep}{8pt}
\setlength{\belowcaptionskip}{-5pt}
\setlength{\abovecaptionskip}{2pt}
\begin{figure}
\centering
\begin{tikzpicture}[scale=1.3]

\path [fill=gray] (0,0) -- (0,3) -- (3,3) -- (0,0); \path [fill=lightgray] (0,0) -- (2,2) -- (2,1) -- (3,1) -- (3,0) -- (0,0); \path [fill=yellow] (2,2) -- (3,2) -- (3,1) -- (2,1) -- (2,2); \draw [green, fill=green] (2,2) -- (3,3) -- (3,2) -- (2,2); 

\draw (0,0) -- (3,0) node[anchor=north] {\large };
\draw (0,0) node[anchor=north east] {\large }
	(1,0) node[anchor=north] {\large }
	(2,0) node[anchor=north] {\large };
\draw (1.5, -0.5) node[anchor=north] {\normalsize };

\draw (0,0) -- (0,3) node[anchor=east] {\large };
\draw (0,1) node[anchor=east] {\large }
	(0,2) node[anchor=east] {\large };
\draw (-0.5, 1.5) node[anchor=east] {\normalsize };

\draw [fill] (0,3) circle (0.04)
	(3,0) circle (0.04);

\draw (0,0) -- (3,3); \draw [dashed] (1,0) -- (1,3);
\draw [dashed] (2,0) -- (2,3);
\draw [dashed] (3,0) -- (3,3);
\draw [dashed] (0,1) -- (3,1);
\draw [dashed] (0,2) -- (3,2);
\draw [dashed] (0,3) -- (3,3);

\draw [red, line width=3] (3,1) -- (3,2); \draw [blue, fill=blue] (3,3) circle (0.04); 

\node at (1.5,0.5) {\normalsize (E)};
\node at (3,3) [blue, anchor=west] {\normalsize (A)};
\node at (2.7,2.3) {\normalsize (B)};
\node at (3,1.5) [red, anchor=west] {\normalsize (C)};
\node at (2.5,1.5) {\normalsize (D)};

\end{tikzpicture}
\caption{Cases for Theorem \ref{theorem-dfa-scheme}. The dark gray
region cannot occur.}
\label{figure-proof-cases}
\end{figure}
}

\begin{enumerate}[(A)]
\item \label{case:pump-v} : Applying Lemma
\ref{lemma-dfa-pump-samp} to  with , we obtain a probabilistic algorithm  which uniformly samples from a
subset of  of size
. Since , we have that  is
a -improviser and return it. 

\item \label{case:unif-v} : Applying Lemma \ref{lemma-dfa-unif-samp} to , we obtain a probabilistic algorithm  which uniformly samples from . Since , we have that  is a -improviser and return it.

\item \label{case:pump-i-unif-v}  and : Applying Lemma \ref{lemma-dfa-unif-samp} to  we obtain  as in the previous case. Defining , we have . Applying Lemma \ref{lemma-dfa-pump-samp} to  with  yields a probabilistic algorithm  which uniformly samples from a subset of  of size . Let  be a probabilistic algorithm which with probability  executes , and otherwise executes . Then since  and  are disjoint, every word generated by  has probability either  (if it is in ) or  (if it is in ). Also,  outputs a member of  with probability , so  is an -improviser and we return it.

\item \label{case:unif-i-unif-v} : As in the previous case, except obtaining  by applying Lemma \ref{lemma-dfa-unif-samp} to . Since , we have  and so  as constructed above is an -improviser.

\item \label{case:infeasible}  or : By Theorem \ref{theorem:feasibility},  is not feasible, so we return .
\end{enumerate}

This procedure takes time polynomial in , , and , so it is polynomial-time. Also, a fixed polynomial in these quantities bounds the expected runtime of the generated improviser, so the procedure is a polynomial-time improvisation scheme.
\end{proof}

\begin{subparagraph}{Running Example.}
Recall that for our running example , we have  and . 
Since  and , we are in case (\ref{case:unif-i-unif-v}) of Theorem \ref{theorem-dfa-scheme}.
So our scheme uses Lemma \ref{lemma-dfa-unif-samp} to obtain  and  uniformly sampling from  and  respectively.
It returns a probabilistic algorithm  that executes  with probability  and otherwise executes .
So  returns , , and  with probability  each, and  and  with probability  each.
The output distribution of  satisfies our conditions, so it is an improviser for .
\end{subparagraph}

\section{More Complex Automata} \label{section:complex-automata}

While counting the language of a DFA is easy, in the case of an NFA it
is much more difficult, and so there are unlikely to be
polynomial-time improvisation schemes for more complex automata. Let
 and  be the classes of CI instances
where  or  respectively are given by an
NFA, and the other is given by a DFA. Then denoting by  
either of these classes, we have (deferring full proofs from this section to the Appendix): 

\begin{restatable}{theorem}{theoremNFAHardness} \label{theorem:nfa-hardness}
Determining whether  is feasible is -hard.
\end{restatable}
\begin{proof}[Proof sketch]
The problem of counting the language of an NFA, which is
-hard \cite{sharpNFA}, is polynomially reducible to that of
checking if  is feasible.
\end{proof}
\begin{remark}
Determining feasibility of -instances is not a counting
problem, so it is not -complete, but it is clearly in
: we construct the automata  and
 as in Theorem \ref{theorem-dfa-scheme} (now they
can be NFAs), count their languages using , and apply Theorem
\ref{theorem:feasibility}. 
\end{remark}

\begin{corollary}
If there is a polynomial-time improvisation scheme for , then .
\end{corollary}

This result indicates that in general, the control improvisation
problem is probably intractable in the presence of NFAs. Some special
cases could still be handled in practice: for example, if the NFA is
very small it could be converted to a DFA. Another tractable case is
where although one of  or  (as
in Theorem \ref{theorem-dfa-scheme}) is an NFA, it has infinite
language (this can clearly be detected in polynomial time). If
 is an NFA with infinite language we can use case
(\ref{case:pump-v}) of the proof of Theorem \ref{theorem-dfa-scheme}, since an NFA
can be pumped in the same way as a DFA. If instead 
is a DFA with finite language but  is an NFA with
infinite language, one of the other cases (\ref{case:unif-v}),
(\ref{case:pump-i-unif-v}), or (\ref{case:infeasible}) applies, and in
case (\ref{case:pump-i-unif-v}) we can sample  by pumping  enough to ensure we get a
string longer than any accepted by . Table
\ref{table:complexities} in Section \ref{section:conclusion}
summarizes these cases. 

For still more complex automata, the CI problem becomes even
harder. In fact, it is impossible if we allow either
 or  to be given by a PFA. Let
 and  be the classes of CI instances
where each of these respectively are given by a PFA, and the other is
given by a DFA. Then letting  be either of these classes,
we have: 

\begin{restatable}{theorem}{theoremPFAHardness} \label{theorem:pfa-hardness}
Determining whether  is feasible is undecidable.
\end{restatable}
\begin{proof}[Proof sketch]
Checking the feasibility of  amounts to counting the language of a PFA, but determining whether the language of a PFA is empty is undecidable \cite{nasu-honda,condon-lipton}.
\end{proof}

\section{Symbolic Techniques} \label{section:symbolic}

Previously we have assumed that the automata defining a control
improvisation problem were given explicitly. However, in practice
there may be insufficient memory to store full transition tables, in
which case an implicit representation is required. This prevents us from using
the polynomial-time improvisation scheme of Theorem
\ref{theorem-dfa-scheme}, so we must look for alternate methods. These
will depend on the type of implicit representation used. We focus on
representations of DFAs and NFAs by propositional formulae, as used
for example in bounded model checking \cite{bmc}. 
\begin{definition}
A \emph{symbolic automaton} is a transition system over states  and inputs  represented by:
\begin{itemize}
\item a formula  which is true iff  is an initial state,
\item a formula  which is true iff  is an accepting state, and
\item a formula  which is true iff there is a transition from  to  on input  .
\end{itemize}
A symbolic automaton accepts words in  according to the usual definition for NFAs.
\end{definition}

Given a symbolic automaton, it is straightforward to generate a
formula whose models correspond, for example, to accepting paths of at
most a given length (see \cite{bmc} for details). A SAT solver can
then be used to find such a path. We refer to the length of the
longest simple accepting path as the \emph{diameter} of the
automaton. This will be an important parameter in the runtime of our
algorithms. In some cases an upper bound on the diameter is known
ahead of time: for example, if we only want improvisations of up to
some maximum length, and have encoded that constraint in
. If the diameter is not known, it can be found
iteratively with SAT queries asserting the existence of a simple
accepting path of length , increasing  until we find no such
path exists. The diameter could be exponentially large compared to the
symbolic representation, but this is a worst-case scenario. 

Our approach for solving the control improvisation problem with symbolic automata will be to adapt the procedure of Theorem \ref{theorem-dfa-scheme}, replacing the counting and sampling techniques used there with ones that work on symbolic automata. For language size estimation we use the following:
\begin{restatable}{lemma-indexed-by-theorem}{lemmaSymbolicCount} \label{lemma:symbolic-count}
If  is a symbolic automaton with diameter , for any  we can compute an estimate of  accurate to within a factor of  in time polynomial in , , , and  relative to an {\NP} oracle.
\end{restatable}

Sampling from infinite languages can be done by a direct adaptation of the method for explicit DFAs in Lemma \ref{lemma-dfa-pump-samp}.
\begin{restatable}{lemma-indexed-by-theorem}{lemmaSymbPumpSamp} \label{lemma:symb-pump-samp}
There is a polynomial  such that for any  and symbolic automaton  with infinite language and diameter , there is a probabilistic oracle algorithm  which uniformly samples from a subset of  of size  in expected time at most  and which can be constructed in the same time.
\end{restatable}

To sample from a finite language, we use techniques for almost-uniform
generation of models of propositional formulae. In theory uniform
sampling can be done exactly using a SAT solver \cite{bgp}, but the
only algorithms which work in practice are \emph{approximate} uniform
generators such as {\UniGen} \cite{unigen}. This algorithm guarantees
that the probability of returning any given model is within a factor
of  of the uniform probability, for any given 
(the constant is for technical reasons specific to
{\UniGen}). {\UniGen} can also do projection sampling, i.e., sampling
where two models are considered identical if they agree on the set of
variables being projected onto. Henceforth, for simplicity, we will
assume we have a generic almost-uniform generator that can do
projection, and will ignore the  restriction imposed by
{\UniGen} (although we might want to abide by this in practice in
order to be able to use the fastest available algorithm). We assume
that the generator runs in time polynomial in  and the size of
the given formula relative to an {\NP} oracle, and succeeds with at
least some fixed constant probability. 
\begin{restatable}{lemma-indexed-by-theorem}{lemmaSymbUnifSamp} \label{lemma:symb-unif-samp}
There is a polynomial  such that for any  and symbolic automaton  with finite language and diameter , there is a probabilistic oracle algorithm  which samples from  uniformly up to a factor of  in expected time at most , and which can be constructed in the same time.
\end{restatable}

Now we can put these methods together to get a version of Theorem \ref{theorem-dfa-scheme} for symbolic automata. The major differences are that this scheme requires an {\NP} oracle, has some probability of failure (which can be specified), and returns an improviser with a slightly sub-optimal value of . The proof generally follows that of Theorem \ref{theorem-dfa-scheme}, so we only sketch the differences here (see the Appendix for a full proof).
\begin{restatable}{theorem}{theoremSymbolicScheme}
There is a procedure that given any CI problem  where  and  are given by symbolic automata with diameter at most , and any , , and , if  is -feasible returns an -improviser with probability at least . Furthermore, the procedure and the improvisers it generates run in expected time given by some fixed polynomial in , , , and  relative to an {\NP} oracle.
\end{restatable}
\begin{proof}[Proof sketch]
We first compute estimates  and  of  and  respectively using Lemma \ref{lemma:symbolic-count}. Then we break into cases as in Theorem \ref{theorem-dfa-scheme}:
\begin{enumerate}[(A)]
\item : As in case (\ref{case:pump-v}) of Theorem \ref{theorem-dfa-scheme}, using Lemma \ref{lemma:symb-pump-samp} in place of Lemma \ref{lemma-dfa-pump-samp}. We obtain a -improviser.

\item : As in case (\ref{case:unif-v}) of Theorem \ref{theorem-dfa-scheme}, using Lemma \ref{lemma:symb-unif-samp} in place of Lemma \ref{lemma-dfa-unif-samp}. Since we can do only approximate counting and sampling, we obtain a -improviser.

\item  and : As in case (\ref{case:pump-i-unif-v}) of Theorem \ref{theorem-dfa-scheme}, using Lemmas \ref{lemma:symb-pump-samp} and \ref{lemma:symb-unif-samp} in place of Lemmas \ref{lemma-dfa-pump-samp} and \ref{lemma-dfa-unif-samp}. Our use of approximate counting/sampling means we obtain only an -improviser.

\item : We cannot use the procedure in case (\ref{case:unif-i-unif-v}) of Theorem \ref{theorem-dfa-scheme}, since it may generate an element of  with too high probability if our estimate  is sufficiently small. Instead we sample almost-uniformly from  with probability , and from  with probability . This yields an -improviser.

\item \label{case:symb-infeasible}  or : We return .
\end{enumerate}

If  is -feasible, case (\ref{case:symb-infeasible}) happens with probability less than  by Theorem \ref{theorem:feasibility}. Otherwise, we obtain an -improviser.
\end{proof}

Therefore, it is possible to {\em approximately} solve the control
improvisation problem when the automata are given by a succinct
propositional formula representation. This allows working with general
NFAs, and very large automata that cannot be stored explicitly, but
comes at the cost of using a SAT solver (perhaps not a heavy cost
given the dramatic advances in the capacity of SAT solvers) and
possibly having to increase  by a small factor. 

\section{Conclusion} \label{section:conclusion}

In this paper, we introduced control improvisation, the problem of
creating improvisers that randomly generate variants of words in the
languages of automata. We gave precise conditions for when improvisers
exist, and investigated the complexity of finding improvisers for
several major classes of automata. In particular, we showed that the
control improvisation problem for DFAs can be solved in polynomial
time, while it is intractable in most cases for NFAs and undecidable 
for PFAs. These results are summarized in Table
\ref{table:complexities}. Finally, we studied the case where the
automata are presented symbolically instead of explicitly, and showed
that the control improvisation problem can still be solved
approximately using SAT solvers. 

One interesting direction for future
work would be to find other tractable cases of the control
improvisation problem deriving from finer structural properties of the
automata than just determinism. Extensions of the theory to other classes of
formal languages, for instance context-free languages represented by 
pushdown automata or context-free grammars, are also worthy of study. 
Finally, we are investigating
further applications, particularly in the areas of testing, security,
and privacy. 

\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.2}
\begin{table}[tb]
\begin{center}
\begin{tabular}{cr||c|c|c|c|}
 &  & \textbf{DFA} & \multicolumn{2}{|c|}{\textbf{NFA}} & \textbf{PFA} \\
 &  &  &  &  &  \\
\hline
\hline
\textbf{DFA} &  & \multicolumn{2}{c|}{\multirow{2}{*}{poly-time}} & \multirow{3}{*}{\sharpP-hard} & \multirow{4}{*}{} \\
\cline{1-2}
\multirow{2}{*}{\textbf{NFA}} &  & \multicolumn{2}{c|}{} & &  \\
\cline{2-4}
&  & \sharpP-hard & - &   &  \\
\cline{1-5}
\textbf{PFA} &  & \multicolumn{4}{r|}{undecidable} \\
\hline \\
\end{tabular}
\caption{Complexity of the control improvisation problem when  and  are given by various different types of automata. The cell marked `-' is impossible since .}
\label{table:complexities}
\end{center}
\label{defaulttable}
\end{table}

\subparagraph*{Acknowledgements}
The first three authors dedicate this paper to the memory of the fourth author, David Wessel, who passed away while it was being written.
We would also like to thank Ben Caulfield, Orna Kupferman, Markus Rabe, and the anonymous reviewers for their helpful comments.
This work is supported in part by the National Science Foundation Graduate Research Fellowship Program 
under Grant No.~DGE-1106400, by the NSF Expeditions grant CCF-1139138, and 
by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.

\bibliographystyle{plain}
\bibliography{main}

\appendix

\section{Proofs}

\setcounter{section}{4}
\begin{example-indexed-by-theorem}[Factor Oracles]
The factor oracle-based admissibility predicate  described above can be encoded as a DFA of size  as follows: we have a copy of , denoted , for every string , each bit of  indicating whether the corresponding previous transition (out of the last ) was non-direct. As each new symbol is processed, we execute the current copy of  as usual, but move to the appropriate state of the copy of  corresponding to the new -transition history, i.e., if we were in , we move to  where  consists of the last  bits of  followed by a 0 if the transition we took was direct and a 1 otherwise. Making the states of  accepting iff the number of 1s in  is in , this automaton represents  as desired.
\end{example-indexed-by-theorem}

\lemmaDFACounting*
\begin{proof}
First we prune irrelevant states unreachable from the initial state or from which no accepting state can be reached (this pruning can clearly be done in polynomial time). If the resulting graph contains a cycle (also detectable in polynomial time), we return . Otherwise  is a DAG with multiple edges, and every sink is an accepting state. For each accepting state  we add a new vertex and an edge to it from . Then there is a one-to-one correspondence between accepting words of  and paths from the initial state to a sink. Now we can compute for each vertex  the number of paths  from it to a sink using the usual linear-time DAG algorithm (traversal in reverse topological order) modified slightly to handle multiple edges. We return  with  the initial state.
\end{proof}

\lemmaDFAPumpSamp*
\begin{proof}
Having pruned  as in Lemma \ref{lemma-dfa-counting}, since  is infinite there must be some state  of  such that
\begin{itemize}
\item there is a word  which takes  from its initial state to ,
\item there is a nonempty word  which takes  from  to itself, and
\item there is a word  which takes  from  to an accepting state.
\end{itemize}
We can find  as above with , in time polynomial in . Then we have  for any . We form a probabilistic algorithm  which acts as follows: it prints , then picks an integer uniformly at random from  and prints that many copies of , before finally printing . Clearly the output of  is a uniform sample from a subset of  of size . Constructing  takes time polynomial in  (as this bounds the sizes of , , and ) and , and  runs in expected time bounded by a fixed polynomial in these values.
\end{proof}

\lemmaDFAUnifSamp*
\begin{proof}
Prune  and compute the path counts  as in Lemma \ref{lemma-dfa-counting}. To every edge  in  assign the weight . It is clear that at every vertex the sum of the weights of the outgoing edges is 1 (unless the vertex is a sink). We prove by induction along reverse topological order that treating these weights as transition probabilities, starting from any state  and talking a random walk until a sink is reached we obtain a uniform distribution over all paths from  to a sink. If  is a sink this holds trivially. If  has a nonempty set of children , then by the inductive hypothesis for every  starting a walk at  gives a uniform distribution over the  paths from  to a sink. Therefore the probability of following any such path starting at  is . So the result holds by induction. In particular, if we start from the initial state we obtain a uniform distribution over all paths to a sink, and thus a uniform distribution over . Since all probabilities are rational with denominators bounded by , this walk can be performed by a probabilistic algorithm  of size polynomial in , with expected time bounded by a fixed polynomial in . Then  returns a uniform sample from , and it can be constructed in time polynomial in .
\end{proof}

\theoremNFAHardness*
\begin{proof}
We prove this for  --- the other case is analogous. As shown in \cite{sharpNFA}, the problem of determining  given an NFA  over an alphabet  and  in unary is -complete. We give a polynomial-time (Cook) reduction from this problem to checking feasibility of a CI instance in .

As noted in \cite{sharpNFA}, we can in polynomial time (in , which is acceptable since  is given in unary) construct an NFA  such that . Then we construct the trivial DFA  accepting all of , and consider the CI instances . Clearly for these instances we have . By Theorem \ref{theorem:feasibility},  is feasible iff , and since  for any  we can determine whether this is the case. Since , using binary search we can find the exact value of  with polynomially-many such queries.
\end{proof}

\theoremPFAHardness*
\begin{proof}
We prove this for , the other case being similar. Given a PFA  with cut-point  over an alphabet  with at least two symbols, determining whether the language of  is empty (i.e. whether it accepts no words with probability greater than ) is undecidable \cite{nasu-honda,condon-lipton}. For any , we can construct a PFA  by adding new states and deterministic transitions to  so that there are exactly  words taking  from its initial state to the initial state of , and any word which does not have one of these as a prefix causes  to reject with probability 1. Then  is  with one of the  prefixes added to each word. Therefore  iff , and so it is undecidable to determine whether the language of a PFA has at least  elements.

Constructing the trivial DFA  accepting all of , the CI instance  satisfies . By Theorem \ref{theorem:feasibility},  is feasible iff . Since checking this latter condition is undecidable, so is determining feasibility of -instances.
\end{proof}

\lemmaSymbolicCount*
\begin{proof}
Recall that the algorithm in Lemma \ref{lemma-dfa-counting} detected
accepting cycles by finding words  taking the automaton to some
state , from  to  along at least one transition, and to an
accepting state respectively. If we impose the additional constraint
, then we can check the existence of such words
with a single SAT query. So to test whether  is
infinite, we use one query for each : since  is the
diameter, we are guaranteed to find an accepting cycle if one
exists. Thus if any query is satisfiable, we return . 

If instead all the queries fail, then all words in 
have length at most . So the SAT query  for accepting paths
of length at most  in fact matches every accepting path. Models of
this formula then correspond to accepting words if we project onto the
input variables (i.e. ignore the values of the variables which encode
states). Therefore  is equal to the number of models
of  after projection. The general problem of counting models of
a propositional formula (even without projection) is \sharpP-complete,
but using the SAT solver we can get probabilistic bounds. An
approximate model counter such as {\ApproxMC} \cite{approxmc} can
return an estimate of the number of models of  which is accurate
to within a factor of  with probability at least . In fact {\ApproxMC} can be easily modified to do projection
counting (see \cite{unigen}), giving us the required estimate of
. 

The first stage of this process clearly takes time polynomial in  and  relative to the oracle. {\ApproxMC} runs in time polynomial in , , and  relative to the oracle, so this procedure does as well.
\end{proof}

\lemmaSymbPumpSamp*
\begin{proof}
We look for an accepting cycle using the method in Lemma \ref{lemma:symbolic-count}. One will be found since  is infinite, and then we can pump it to get  different words just as in Lemma \ref{lemma-dfa-pump-samp}.
\end{proof}

\lemmaSymbUnifSamp*
\begin{proof}
As noted in Lemma \ref{lemma:symbolic-count}, since the language is finite every word in it has length at most . Constructing the formula  from that lemma, once we project onto the input variables there is a one-to-one correspondence between accepting words and models of . So we need to almost-uniformly generate projected models of a propositional formula. We use an almost-uniform generator as described above, whose runtime will be polynomial in  and  relative to the oracle.
\end{proof}

\theoremSymbolicScheme*
\begin{proof}
The procedure begins by deriving the symbolic representations of  and  (the product of  and the complement of ). Next we estimate  and  using Lemma \ref{lemma:symbolic-count} with a confidence of . Then with probability at least , both these estimates are within a factor of  of the true values. We assume this is the case for the rest of the proof, making no guarantees otherwise. We now break into the same cases as Theorem \ref{theorem-dfa-scheme}, using our estimates  and  of  and  respectively.

\begin{enumerate}[(A)]
\item : In this case we must have . We proceed as in case (\ref{case:pump-v}) of Theorem \ref{theorem-dfa-scheme}, but using Lemma \ref{lemma:symb-pump-samp} with  in place of Lemma \ref{lemma-dfa-pump-samp} to obtain a -improviser.

\item : Then . We proceed as in case (\ref{case:unif-v}) of Theorem \ref{theorem-dfa-scheme}, using Lemma \ref{lemma:symb-unif-samp} in place of Lemma \ref{lemma-dfa-unif-samp}. Since we are using an almost-uniform generator instead of a uniform one, some words could have probability as high as , and so this gives us a -improviser.

\item  and : Then , and . We proceed along the same lines as case (\ref{case:pump-i-unif-v}) of Theorem \ref{theorem-dfa-scheme}. We use Lemma \ref{lemma:symb-unif-samp} as in the previous case to generate a probabilistic algorithm  almost-uniformly sampling from . Defining , we have . We can use Lemma \ref{lemma:symb-pump-samp} in place of Lemma \ref{lemma-dfa-pump-samp} to get a probabilistic algorithm  uniformly sampling from a subset of  of size . Let  be a probabilistic algorithm which with probability  executes , and otherwise executes . Then since  and  are disjoint, every word generated by  has probability either at most  (if it is in ) or at most  (if it is in ). Also  outputs a member of  with probability , so  is an -improviser and we return it.

\item : Then , and . As in the previous case, use Lemma \ref{lemma:symb-unif-samp} to produce a probabilistic algorithm  almost-uniformly sampling from . Since  is finite, we can use the same technique to get a probabilistic algorithm  almost-uniformly sampling from . Let  be a probabilistic algorithm which with probability  executes , and otherwise executes . Then  generates each  with probability at most , and each  with probability at most . Furthermore  outputs a member of  with probability at least , so  is an -improviser and we return it.

\item  or : It is possible that the problem is not -feasible, so the procedure returns .
\end{enumerate}
In the cases where an almost-uniform generator is used, there is some constant probability that the generator will fail. If that happens, the improviser just runs the generator again: since the failure probability is a fixed constant, so is the expected number of repetitions needed, and thus the expected runtime of the improviser is just multiplied by an overall constant.

Now if  is -feasible, by Theorem \ref{theorem:feasibility} we have  and  with probability at least . So with probability  case (\ref{case:symb-infeasible}) does not happen, and the procedure returns an -improviser.
\end{proof}

\end{document}
