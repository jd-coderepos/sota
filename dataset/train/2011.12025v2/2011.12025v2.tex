

\section{Experiments}
We integrate our dynamic processing method in SwiftNet~\cite{orsic_defense_2019}, a state of the art network for real-time semantic segmentation of road scenes. We test semantic segmentation on the Cityscapes~\cite{cordts_cityscapes_2016}, Camvid~\cite{brostow2009semantic_camvid}, and Mapillary Vistas~\cite{neuhold_mapillary_2017} datasets. In addition, we provide an ablation study, analyzing the overhead of block-based processing. Our method is implemented in PyTorch 1.5 and CUDA 10.2, without TensorRT optimizations. We report two model complexity metrics: the number of computations and the frames per second.

{The number of computations} is reported in billions of \emph{multiply-accumulates (GMACs)} per image, being half the number of floating-point operations (FLOPS). The FLOPS metric is often used to compare model complexity in a platform-agnostic manner, since the efficiency of the implementation has no impact. 
For our dynamic method, where the number of computations varies per image, we report the average GMAC count, over the validation or test set.

\emph{Frames per second (FPS)} is a more practical metric to measure inference speed, but depends largely on the implementation of building blocks. 
Therefore, both FPS and GMACs metrics should be taken into account for fair comparison. 
We measure the inference speed on both a high-end Nvidia GTX 1080 Ti 11GB GPU and low-end Nvidia GTX 1050 Ti 4 GB GPU, paired with an Intel i7 processor. The model is warmed up on the first half of the image set, and the speed is measured on the second half. To compare the inference speed to those reported by others, we normalize the FPS numbers (\textit{norm FPS}) of different GPUs based on their relative performance, using the same scaling factors as Orsic \emph{et al.}~\cite{orsic_defense_2019}.

\subsection{Cityscapes semantic segmentation}

\begin{figure}[tb]
\centering
\includegraphics[width=1\linewidth, trim= 0 0 0 0, clip]{img/exp/cityscapes_results}
\caption{Cityscapes validation results and comparison with static baselines of similar complexity. For a given computational complexity (GMACs), our adaptive resolution method SegBlocks consistently outperforms static baseline networks (SwiftNet) of similar complexity. Each backbone (RN50, RN18, Eff-L1) is trained and evaluated with  for dynamic models and resolutions , ,  for static baselines.  }
\label{fig:cityscapes_results}
\end{figure}


\begin{table*}[tb]
\caption{Results on Cityscapes semantic segmentation. Our SegBlocks models are based on the respective SwiftNet baselines, and integrate block-based dynamic resolution processing in those networks \label{tab:cityscapes_results}. {The symbol `-' indicates that the metric was not reported. For our method, only several test set results are reported due to submission limitations on Cityscapes test evaluation.}}
\centering
\begin{tabular}{@{}clcccccZ@{}}
\toprule
& \textbf{Method}                & \textbf{mIoU \textit{val}} & \textbf{\textit{test}} & \textbf{GMACs}  & \textbf{FPS} & \textbf{norm FPS} & \textbf{memory (max)} \\ \midrule

\multirow{9}{*}{\rotatebox[origin=c]{90}{Our method}} &
SwiftNet-RN50 (baseline, our impl.)               & 78.0 &           & 206.3     &     16.3     @ 1080 Ti,\quad  3.8  @ 1050 Ti         &   16.3          & 2724  MB            \\
& SegBlocks-RN50 (=0.4)  & 77.5 &       76.1           &  127.2  &    23.3   @ 1080 Ti,\quad 5.7 @ 1050 Ti       &           23.3       &    2957  MB        \\
& SegBlocks-RN50 (=0.2)  & 76.2 &                  &  88.5  &    30.0    @ 1080 Ti,\quad 7.3 @ 1050 Ti       &            30.0        &    2044 MB            \0.5ex]
\cdashline{2-8}\noalign{\vskip 0.5ex}

 & SwiftNet-EffL1 (baseline, our impl.)               & 76.6 &           & 24.9     &      17.4   @ 1080 Ti,\quad   7.1 @ 1050 Ti         &       17.4     &   1338 MB            \\
 & SegBlocks-EffL1 (=0.4)            &  76.3 &  74.1        & 15.6                 &        30.4   @ 1080 Ti,\quad  8.4 @ 1050 Ti         &        30.4          &  1210   MB            \\
 & SegBlocks-EffL1 (=0.2)            &  75.3 &              & 11.4                 &      35.6     @ 1080 Ti,\quad  10.6 @ 1050 Ti         &        35.6          &  1210   MB            \\
\midrule

\multirow{5}{*}{\rotatebox[origin=c]{90}{\parbox{1.1cm}{\centering Dynamic\\networks}}} & Patch Proposal Network (AAAI2020)~\cite{wu_patch_2020}                    & 75.2 &         -  &  -           & 24 @ 1080 Ti  & 24               &   1137 MB               \\
 & Huang et al. (MVA2019)~\cite{huang_uncertainty_2019}  & 76.4 &     -      & -           & 1.8 @ 1080 Ti   & 1.8                &        -           \\
 & Wu et al.~\cite{wu_real-time_2017}    & 72.9 &      -     & -           & 15 @ 980 Ti   & 33    & -                  \\ 
 & Learning Downsampling (ICCV2019)~\cite{marin_efficient_2019}    & 65.0 &   -        &  34           & -   &-    & -                  \\ 
 & HyperSeg-M~\cite{nirkin2021hyperseg} & 76.2 & 75.8 & 7.5 & 36.9 @ 1080 Ti & 36.9 & - MB \\
 & Stochastic Sampling~\cite{xie_spatially_2020} & 80.6 & - & 373.2 & no GPU implementation &- & - \\
\midrule

\multirow{8}{*}{\rotatebox[origin=c]{90}{\parbox{2cm}{\centering Efficient\\static networks}}}
 & ShelfNet18-lw (CVPR2019)~\cite{zhuang2019shelfnet}     & - &      74.8     & 95        & 36.9 @ 1080 Ti  & 36.9               &      -           \\
 & SFNet (ResNet-18) (ECCV2020)~\cite{zhuang2019shelfnet}     & - &      78.9     & 247        & 26 @ 1080 Ti  & 26               &      -           \\
 & SwiftNet-RN18 (CVPR2019)~\cite{orsic_defense_2019}     & 75.6 &      75.5     & 104.0        & 39.9 @ 1080 Ti  & 39.9               &      -           \\
  & ESPNetV2 (CVPR2019)~\cite{mehta2019espnetv2}     & 66.4 & 66.2          & 2.7        & -  & -               &      -           \\
 & DABNet (BMVC2019)~\cite{li2019dabnet}  & 70.1 &      -     & -        & 27.7 @ 1080 Ti  & 27.7               &      -           \\
 & BiSeNet-RN18 (ECCV2018)~\cite{ferrari_bisenet_2018}     & 74.8 &    74.7        & 67       & 65.5 @ Titan Xp & 58.5               &        -       \\
 & BiSeNetV2-RN18 (IJCV2021)~\cite{yu2021bisenet}     & 75.8 &    75.3        & 118.5       & 47.3 @ 1080 Ti & 47.3               &        -       \\
 & ICNet (ECCV2018)~\cite{ferrari_icnet_2018}                     &   -   & 69.5          &   30          & 30.3 @ Titan X  & 49.7               &   -               \\
 & GUNet (BMVC2018)~\cite{mazzini2018guided}                     &    -  & 70.4          &   -          & 33.3 @ Titan Xp  & 29.7               &   -               \\
 & ERFNet (TITS2017)~\cite{romera_erfnet_2018}                       & 72.7 &    69.7       & 27.7         & 11.2 @ Titan X  & 18.4                   &      -            \\ 

\bottomrule

\end{tabular}
\end{table*}



\begin{table*}[tb]
\scriptsize \setlength{\tabcolsep}{2pt}
\caption{IoU per class on the Cityscapes validation set: our method improves the IoU score for most classes compared to lower-resolution baselines with similar complexity. Classes such as person, rider, car, bus and train benefit more from high-resolution processing. The percentage of pixels processed in high-resolution, given per class, shows that our method mostly processes road and sky regions at low resolution. } 
\label{tab:cityscapes_classes}
\centering
\begin{tabular}{lllllllllllllllllllll}
\toprule
{} & {road} & {side-} & {buil-} & {wall} & {fence} & {pole} & {traffic} & {traffic} & {vege-} & {terrain} & {sky} & {per-} & {rider} & {car} & {truck} & {bus} & {train} & {motor-} & {bicycle} & mIoU\\
{} & {} & { walk} & {ding} & {} & {} & {} & {light} & {sign} & {tation} & {} & {} & {son} & {} & {} & {} & {} & {} & {cycle} & {}  & {}
\\ \midrule

{SwiftNet-RN18 }   & 97.8          & 82.9              & 91.8              & 50.9          & 56.2           & 63.0          & 69.0                   & 78.1                  & 92.2                & 61.4             & 94.7         & 80.5            & 60.0           & 94.8         & 78.3           & 84.3         & 75.0           & 6.5                & 76.3  &      76.2    \\ 
{{()}}       &  &     &   &     &             &        &    &    &    &   &   &   &   &    &   &  &     &   &    &      \\
\midrule


{SwiftNet-RN18}       & \textbf{97.8}   & \textbf{83.0}                 & 91.5                & 54.1            & 52.8             & 60.5            & 65.5                             & \textbf{76.4}                   & 91.9                            & \textbf{62.1}      & 94.6           & 79.2              & 58.2             & 94.2           & 75.5             & 80.5           & 69.6             & 57.6                            & 74.9     &      74.7    \\ 
{()}       &  &     &   &     &             &        &    &    &    &   &   &   &   &    &   &  &     &   &    &      \\
{SegBlocks-RN18 } & \textbf{97.8}   & 82.9                          & \textbf{91.9}       & \textbf{59.5}   & \textbf{57.6}    & \textbf{61.1}   & \textbf{67.0}                    & 76.2                            & \textbf{92.1}                   & 61.9               & \textbf{94.4}  & \textbf{80.4}     & \textbf{59.9}    & \textbf{94.4}  & \textbf{77.2}    & \textbf{82.4}  & \textbf{77.2}    & \textbf{60.1}                   & \textbf{75.7}  & \textbf{76.3}  \\
{(=0.4)}       &  &     &   &     &             &        &    &    &    &   &   &   &   &    &   &  &     &   &        &  \\

{\quad \textit{\% high-res pixels}}       & \textit{ 8.5} & \textit{45.8} & \textit{64.0} & \textit{67.0} & \textit{88.5} & \textit{86.3} & \textit{85.6} & \textit{82.2} & \textit{59.7} & \textit{68.2} & \textit{36.7} & \textit{93.3} & \textit{96.2} & \textit{66.2} & \textit{61.7} & \textit{68.9} & \textit{85.0} & \textit{90.9} & \textit{93.2}   & -   \\  \bottomrule
\end{tabular}
\end{table*}


\begin{table}[tb]
\scriptsize
\centering
\caption{Memory analysis for SegBlocks-RN18 running in 16-bit floating point, as reported by PyTorch. By storing some image areas at lower resolution, SegBlocks can reduce the memory usage, especially when using larger batch sizes.}
\label{tab:memory}
\begin{tabular}{@{}l|ccc@{}}
\toprule
\textbf{model} & \multicolumn{3}{c}{\textbf{Batch size}} \\
               & 1           & 2           & 4           \\ \midrule
SwiftNet-RN18 (static)         & 480 MB        & 880 MB         & 1681 MB        \\
SegBlocks-RN18 ()   & 571 MB        & 901 MB         & 1586 MB        \\
SegBlocks-RN18 ()   & 340 MB        & 721 MB         & 940 MB         \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Dataset and setup}

The Cityscapes~\cite{cordts_cityscapes_2016} dataset for semantic segmentation consists of 2975 training, 500 validation and 1525 test images of  pixels. We use the standard 19 classes for semantic segmentation and do not use the additional coarsely labeled images. 

We train SwiftNet~\cite{orsic_defense_2019} with three different backbones: ResNet-18 (RN18), ResNet-50 (RN50)~\cite{he_deep_2016}, and EfficientNet-Lite1 (EffL1), which corresponds to EfficientNet-B1~\cite{tan2019efficientnet} without squeeze-and-excite and swish modules. Our dynamic processing method is integrated in these baseline networks and we trained models for different , indicating the desired percentage of high-resolution blocks. 
The models are trained on  crops, augmented by image scaling between factor 0.5 and 2, slight color jitter and random horizontal flip. The optimizer is Adam, the learning rate is cosine annealed from   to  over 350 epochs, weight decay is set to  and the batch size is 8. Standard cross entropy loss is used for models with a ResNet backbone, whereas the EfficientNet models use a bootstrapped cross entropy loss~\cite{reed2014training_bootstrap}.  The backbone is pre-trained on ImageNet~\cite{deng_imagenet_2009}. Our method uses a block size of  pixels, resulting in a block grid of  blocks per image that can adapt the processing resolution to the content. 

\subsubsection{Results and comparison}

Figure~\ref{fig:cityscapes_results} shows the mIoU accuracy of models at various computational costs (GMACs). Static baseline networks of different computational complexity are obtained by adjusting the training and evaluation resolution to ,  and . The complexity of our SegBlocks methods is determined by the number of high-resolution blocks, changed by training for different values of hyperparameter . 
Our SegBlocks methods consistently outperform the static baseline networks, showing that dynamic resolution processing is more beneficial than sampling all regions at lower resolution. For instance, SegBlocks-RN18 with  reduces the computational complexity of the baseline network by , without decreasing the mIoU. In contrast, a static baseline network of similar complexity, trained on images of  pixels, achieves 1.7\% lower mIoU. 


\begin{figure}[tb!]
\centering
\includegraphics[width=1\linewidth, trim= 0 0 0 0,clip]{img/exp/distr.pdf}
\caption{Percentage of high-resolution blocks per image, as a histogram over 500 Cityscapes validation images, for SegBlocks-RN18 (). Simple images use fewer high-res blocks, down to 20\%, and complex images have up to 55\% high-res blocks (colored yellow).}
\label{fig:cityscapes_distr}
\end{figure}

\begin{figure*}[tb]
\centering
\includegraphics[width=1\linewidth, trim= 0 0 0 0]{img/exp/resolution_decision_examples.pdf}
\caption{Examples of resolution decisions made by the policy network and corresponding segmentation outputs, for SegBlocks-RN18 with  and . High-resolution blocks are colored in yellow. 
}
\label{fig:cityscapes_policy_examples}
\end{figure*}


Table~\ref{tab:cityscapes_results} compares the performance of our method with other dynamic methods and non-dynamic efficient segmentation architectures. Compared to other dynamic methods, our method achieves higher mIoU results at faster inference speeds. Patch Proposal Network~\cite{wu_patch_2020} is the most competitive method, and uses an architecture with a global branch and patch-based refinement branch. Our method achieves better accuracy and FPS due to our efficient block-based execution framework. This enables more fine-grained block-based execution, with in total 128 blocks ( grid) compared to only 16 large patches, resulting in better adaptability and performance. Similarly, the method of Wu et al.~\cite{wu_real-time_2017} refines regions with a high-resolution branch on large patches. Huang et al.~\cite{huang_uncertainty_2019} use a cascade of two existing network architectures (e.g. lightweight ICNet and accurate PSPNet), where the second network is only applied on complex regions. However, this introduces additional latency and does not re-use any features of the lightweight network, resulting in low efficiency and only 1.8 FPS. The method of Marin et al.~\cite{marin_efficient_2019} (Learning Downsampling) adaptively samples the input image, to more densely sample near segmentation boundaries. Only the theoretical computational cost (GMACs) is reported. HyperSeg~\cite{nirkin2021hyperseg}, based on the EfficientNet backbone~\cite{tan2019efficientnet}, adjusts the weights of the decoder dynamically per image. Stochastic sampling~\cite{xie_spatially_2020} dynamically interpolates features spatially, but does not have an accelerated GPU implementation.

We are also competitive with state of the art architectures for fast semantic segmentation, such as BiSeNet~\cite{ferrari_bisenet_2018} or ERFNet~\cite{romera_erfnet_2018}. It is worthwhile to note that our proposed dynamic resolution method is complementary to further improvements in network architectures.


The table also demonstrates the inference speed improvement achieved using our efficient implementation.
For instance, our method improves the inference speed of SwiftNet-RN50 from 16 FPS to 30 FPS on a GTX 1080 Ti, achieving real-time performance. This increase of 84 percent is obtained by reducing the theoretical complexity (GMACs) by 63\%,  while the mean IoU is decreased by 1.8\%.
Memory usage is reported in Table~\ref{tab:memory} as PyTorch's peak allocated memory during inference over the validation set, and indicates that our method also reduces memory consumption by storing low-complexity regions at lower resolution.

Our method adapts the operations to each individual image, and Fig.~\ref{fig:cityscapes_distr} shows the distribution of operations over images.
Qualitative results are shown in Fig.~\ref{fig:cityscapes_policy_examples}, visualizing the resolution decisions of the policy network and the respective segmentation output, for  set to 0.4 and 0.2. The policy network, trained with reinforcement learning, properly selects complex regions for high-resolution processing. 

\subsubsection{Per-class analysis}
The IoU result per class is provided in Table~\ref{tab:cityscapes_classes}, as well as the percentage of pixels processed in high-resolution for each class. Our method mainly processes classes such as road, sky and vegetation in low resolution, whereas rider, motorcycle and bicycle are typically sampled in high resolution.




\subsection{CamVid semantic segmentation}

\begin{table}[t]
\scriptsize
\centering
\caption{{Results on CamVid's test set for semantic segmentation~\cite{brostow2009semantic_camvid}. All methods are pretrained on ImageNet~\cite{deng_imagenet_2009}, without Cityscapes pretraining~\cite{cordts_cityscapes_2016}. Results annotated with   are determined using open-source implementations. FPS measured on Nivida GTX 1080 Ti.  }}\label{tab:results_camvid}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Method}              & \textbf{mIoU} & \textbf{GMACs} & \textbf{FPS}                    \\  \midrule
SwiftNet-EffL1~\cite{orsic_defense_2019} (our impl.)    & 75.1          & 13.4           & 48                  \\
SegBlocks-EffL1 ( = 0.4)& 74.6  (-0.5\%)& 9.1 (-32\%) & 58  (+20\%) \\
SegBlocks-EffL1 ( = 0.2)& 73.4  (-1.7\%)& 6.6 (-50\%) & 64  (+33\%) \\
\midrule

BiSeNet~\cite{ferrari_bisenet_2018}                     & 68.7          &  32.4          & 116                  \\
BiSeNetV2~\cite{yu2021bisenet}                    & 72.4          &                & 125                       \\
BiSeNetV2-L~\cite{yu2021bisenet}                  & 73.2          &                & 33                        \\
HyperSeg-S (ResNet-18)~\cite{nirkin2021hyperseg}   & 77.0          &                & 32.5          \\
HyperSeg-S (EfficientNet-B1)~\cite{nirkin2021hyperseg}   & 78.4        &  6.3       & 38.0            \\
SFNet (DF2)~\cite{li2020semantic}                  & 70.4          &                & 134                      \\
SFNet (ResNet-18)~\cite{li2020semantic}            & 73.8          &   41.2         & 36                      \\                 \bottomrule
\end{tabular}
\end{table}

Another popular benchmark for real-time semantic segmentation is CamVid~\cite{brostow2009semantic_camvid}, having 701 densely annotated frames, divided in 367 training, 101 validation and 233 test images. Following the methodology of related works~\cite{ferrari_bisenet_2018, orsic_defense_2019, yu2021bisenet}, we train on the training and validation subsets and report test scores on 11 semantic classes. We evaluate on images of  pixels. We use the same hyperparameters and data augmentations as on Cityscapes, but with crops of 704 pixels. The backbone is pretrained on ImageNet, but we did not pretrain on Cityscapes.
Dynamic methods use a block size of 64 pixels, resulting in a grid of  blocks. In order to adjust for the lower input resolution compared to Cityscapes, we remove the stride of the last residual block in the backbone, leading to higher-resolution representations in the spatial pyramid pooling module.

Table~\ref{tab:results_camvid} shows that our results are competitive with other real-time semantic segmentation methods, and our dynamic execution (SegBlocks) improves inference speed over the static baseline network (SwiftNet). However, due to the smaller image dimensions and block size, the speedup is more modest compared to the high-res Cityscapes experiments.


\subsection{Mapillary Vistas semantic segmentation}
Mapillary Vistas~\cite{neuhold_mapillary_2017} is a large dataset containing high-resolution road scene images with labels for semantic and instance segmentation. We use the standard 15000/2000 train/val split, resize all images to  pixels and use the 65 classes for semantic segmentation. We report validation results, since the test server is not available. SegBlocks usees blocks of 128 pixels, resulting in an adaptive grid of  blocks. The hyperparameters are the same as the Cityscapes experiments, with the batchsize reduced to 4 as the number of classes requires more memory, and we train SegBlocks with an EfficientNet-Lite1 backbone for 100 epochs.
Table~\ref{tab:mapillary_results} demonstrates that SegBlocks reduces the number of operations and improves inference speed for the segmentation model on this dataset.

\begin{table}[tb]
\scriptsize
\centering
\caption{Mapillary Vistas validation results. Results with  are reported in \cite{hao2021real_wfcdnet} and benchmark FPS with an Nvidia GTX2080 Ti GPU. Other results are benchmarked on an Nvidia GTX1080 Ti.}
\label{tab:mapillary_results}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Network}              & \textbf{Input size}                & \textbf{FPS}                & \textbf{GMACs}               & \textbf{mIoU(\%)}            \\ \midrule
SwiftNet-EffL1 (our impl.) & 15361152    &   20.7   &  21.5     &  41.7    \\ 
\cdashline{1-5}\noalign{\vskip 0.5ex}
SegBlocks-EffL1 ( = 0.4)  & 15361152    &   33.0   &  13.7     &  40.5    \\
SegBlocks-EffL1 ( = 0.2)  & 15361152    &   40.1   &  11.5     &  39.8    \\ \midrule
AGLNet~\cite{zhou2020aglnet}    & 20481024 & 53.0   & 24.1G & 30.7 \\
WFDCNet~\cite{hao2021real_wfcdnet}       & 20481024 & 53.1 & 12.5G & 30.5 \\
FPENet~\cite{liu2019feature_fpenet}    & 20481024 & {92.5} & {3.1G}  & {28.3} \\
DABNet~\cite{li2019dabnet}            & 20481024 & {78.3} & {20.9G} & {29.6} \\ \bottomrule
\end{tabular}
\end{table}







\subsection{Ablation studies}

\paragraph*{\textbf{Module execution time analysis}}



\begin{table}[tb!]
\scriptsize
\centering
\caption{Time profiling of block modules, as total time taken during the inference of 250 validation images on an Nvidia GTX 1080 Ti GPU.}
\label{tab:timings}
\begin{tabular}{@{}l|cccc@{}}
\toprule
                  & \multicolumn{2}{c}{\textbf{SegBlocks-RN50}} & \multicolumn{2}{c}{\textbf{SegBlocks - RN18}} \\
                      & =0.4       & =0.2      & =0.4        & =0.2       \\ \midrule
{mIoU}         & 77.5\%           & 76.2\%          & 76.3\%            & 75.9\%           \\ \cdashline{1-5}\noalign{\vskip 0.5ex}
{GMACs}        & 127.2            & 88.6            & 60.5              & 43.5             \\
{policy GMACs} & 0.34             & 0.34            & 0.34              & 0.34             \\ \cdashline{1-5}\noalign{\vskip 0.5ex}
{Runtime}      & {11.6s}         & {9.3s}         & {5.8s}           & {4.9s}          \\
{Policy Net}   & 0.04s (\textless1\%) & 0.04s (\textless1\%) & 0.04s (\textless1\%) & 0.04s (\textless1\%) \\
{BlockSample}  & 0.12s (1\%)     & 0.13s (1\%)    & 0.11s (2\%)      & 0.12s (2\%)     \\
{BlockPad}     & 0.75s (6\%)     & 0.61s (7\%)    & 0.68s (12\%)     & 0.56s (11\%)    \\
{BlockCombine} & 0.04s (\textless1\%) & 0.03s (\textless1\%) & 0.02s (\textless1\%) & 0.02s (\textless1\%) \\ \bottomrule
\end{tabular}
\end{table}

We profile the time characteristics of our block modules to analyze their overhead. Table~\ref{tab:timings} compares the execution time of the Policy Net, BlockSample, BlockPad and BlockCombine modules with the total runtime. The overhead of the Policy Net, BlockSample and BlockCombine modules is negligible, and the overhead of the BlockPad operation is reasonable with around 10 percent of the total execution time. The BlockPad module has less impact on the ResNet-50 backbone, as its  convolutions in the bottleneck function do not require padding. Note that the cost of the BlockPad operation scales with the total number of processed pixels, and thus with the number of high-resolution blocks. 

\paragraph*{\textbf{Reinforcement learning policy}}
We compare the policy trained using reinforcement learning with other baselines in Fig.~\ref{fig:policy_ablation}. The `random' policy randomly selects blocks for high-resolution processing, with the number of selected blocks given by the target percentage. The heuristic policy was proposed in~\cite{verelst2020segblocks} and selects the regions with the highest visual change, based on the average L2 distance between high- and low-resolution versions of a block. 

\begin{figure}[tb]
\centering
\includegraphics[width=0.5\linewidth, trim=2 5 2 1, clip]{img/exp/ablation/policy_comparison}
\caption{
{Comparison of block execution policies. The reinforcement learning policy is proposed in this work. The random policy randomly selects a percentage of blocks per frame for high-resolution processing. The heuristic policy is given in~\cite{verelst2020segblocks}. }
}
\label{fig:policy_ablation}
\end{figure}

\paragraph*{\textbf{Policy network architecture}}
The policy network, determining whether blocks should be executed with high- or low-resolution processing, should be lightweight but powerful enough for the selection task. The ablation (Table~\ref{tab:ablation_policynet}) shows that the size of this network does not significantly impact the results. For our experiments, we use an input resolution of 512x256 with a 4 layer network having 64 features in each layer.


\begin{table}[tb]
\scriptsize
\centering
\caption{Policy network ablation: adjusting the input resolution, number of layers and number of features for the policy network.}
\label{tab:ablation_policynet}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Policy input res.} & \textbf{\# Layers} & \textbf{\# Features} & \textbf{mIoU} & \textbf{Policy GMACs} \\ \midrule
 & 2 & 64  & 75.3 & 0.01 \\
 & 3 & 64  & 75.5 & 0.07 \\
 & 4 & 64  & 76.3 & 0.34 \\
 & 5 & 64  & 76.1 & 1.51 \\
\midrule

 & 4 & 32  & 76.2 & 0.11 \\
 & 4 & 64  & 76.3 & 0.34 \\
 & 4 & 128 & 76.1 & 1.21 \\ \bottomrule
\end{tabular}
\end{table}




\paragraph*{\textbf{Training steps and progress}}
We compare the training speed of a standard model with a dynamic SegBlocks model in Fig.~\ref{fig:training_progress}, which shows that the reinforcement learning of the policy does not significantly impact the required number of training steps.

\begin{figure}[tb]
\centering
\includegraphics[width=0.5\linewidth, trim=0 1 20 18, clip]{img/exp/training_progress}
\caption{Training progress given by validation mIoU for non-dynamic baseline (SwiftNet-RN18) and our dynamic method (SegBlocks-RN18). Learning the policy does not significantly impact the training progress.} 

\label{fig:training_progress}
\end{figure}


\paragraph*{\textbf{Block size}}
Table~\ref{tab:blocksize} compares the impact of the method's block sizes on accuracy and performance. Figure~\ref{fig:blocksize} demonstrates the granularity of each block size on Cityscapes. Larger block sizes offer better inference speeds, by padding fewer pixels and having less overhead, whereas smaller block sizes offer finer control of adaptive processing.


\begin{table}[!tb]
\scriptsize
\caption{Block size comparison for SegBlocks-RN18 with  on the Cityscapes validation set \label{tab:blocksize}.}
\centering
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Block size} & \textbf{mIoU} & \textbf{GMACs} & \textbf{FPS (1050 Ti)} \\ \midrule
 & 76.3\% & 59.3 & 11.3 \\
 & 76.3\% & 60.5 & 13.5\\
 & 75.2\% & 56.4 & 14.6\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[!tb]
\centering
  \includegraphics[width=0.32\linewidth]{img/exp/blocksize/p64}
   \includegraphics[width=0.32\linewidth]{img/exp/blocksize/p128}
  \includegraphics[width=0.32\linewidth]{img/exp/blocksize/p256}
\caption{Granularity of blocks of 64, 128, and 256 pixels on Cityscapes' images of  pixels.}
\label{fig:blocksize}
\end{figure}



\paragraph*{\textbf{Sampling pattern}}
As described in Section~\ref{sec:blockpad}, the BlockPad module pads individual blocks by sampling their neighbors. When low-resolution blocks are padded with features of high-resolution blocks, these features should be subsampled. Table~\ref{tab:sampling} compares average-sampling with strided subsampling and shows that average-sampling achieves better accuracy (mIoU) with only a slight increase in overhead. In addition, we show that using zero-padding has a significant impact on accuracy, highlighting the importance of BlockPad when processing images in blocks. 

\begin{table}[!tb]
\scriptsize
\caption{Comparison of BlockPad sampling patterns and zero-padding for SegBlocks-RN18 () \label{tab:sampling}}
\centering
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Method} & \textbf{mIoU} & \textbf{Runtime} & \textbf{BlockPad time} \\ \midrule
Avg-sampling & 76.3\% & 5.8 s & 0.68 s (12\%) \\
Strided sampling & 75.6\% & 5.7 s & 0.63 s (11\%) \\
Zero-padding & 70.2\% & 5.2 s & N.A. (integrated in conv.)\\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Single residual block analysis}
The overhead introduced by our block modules strongly depends on the block size: larger blocks have fewer pixels in the padding, and require fewer copy operations. Moreover, standard operations, implemented in PyTorch and cuDNN, are often optimized for larger spatial dimensions. We analyze a single residual block to measure the impact of both block size and the percentage of high-resolution blocks.
Figure~\ref{fig:speed_vs_blocksize} studies the impact of the block size, when evaluating half of the blocks at high and half of the blocks at low resolution. The number of floating-point operations is reduced by 56\%, resulting in a theoretical 2.3 times speed increase. In practice, we measure 1.92 times faster inference of the residual block when the block size is larger than 32. Block sizes smaller than 4 pixels result in {slower} inference. Note that, even though block sizes are typically large at the network input (e.g. ) downsampling in the network reduces the block size by the same factor. The SwiftNet network downsamples by a factor 32 throughout the network, resulting in block sizes of  for the deepest network layers.
Figure~\ref{fig:speed_vs_blocksize} shows that, when evaluating using block size 8, the percentage of high-res blocks should be lower than 75\% to achieve practical speedup. For lower percentages, the practical speedup of our implementation is around 70 percent of the theoretical one.

\begin{figure}[!tb]
\centering
\subfloat[Block size impact]{\includegraphics[width=0.4\linewidth]{img/exp/speed_blocksize}\label{fig:speed_vs_blocksize}}
\hfil
\subfloat[Percentage impact]{\includegraphics[width=0.4\linewidth]{img/exp/speed_percentage}\label{fig:speed_vs_percentage}}
\caption{Comparison of theoretical versus practical speedup for a single residual block of ResNet-18, studying the impact of the block size and the percentage of high-resolution blocks on the performance.}
\end{figure}
