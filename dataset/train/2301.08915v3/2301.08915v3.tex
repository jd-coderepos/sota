
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak

\def\varcls{{\omega}}
\def\varreg{{\theta}} 
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor={blue},citecolor={purple},urlcolor={red}} 
\usepackage{url}
\usepackage{soul}
\usepackage{comment}
\clearpage{}





\usepackage{times}
\usepackage{microtype} \usepackage{graphicx} \usepackage{subfigure} 
\usepackage{balance} 


\usepackage{xspace}
\newcommand*{\eg}{e.g.\@\xspace}
\newcommand*{\ie}{i.e.\@\xspace}

\makeatletter
\newcommand*{\etc}{\@ifnextchar{.}{etc}{etc.\@\xspace}}
\makeatother



\usepackage{algorithm}
\usepackage{algorithmic}




\usepackage{hyperref}



\usepackage{helvet}
\usepackage{courier}



\usepackage{makecell}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}
\usepackage{booktabs}

\def\support{\mbox{supp}}
\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\etal{{\em et al.\/}\, }
\def\card{{\mbox{Card}}}
\def\ie{\mbox{\textit{i.e.}, }}
\def\eg{\mbox{\textit{e.g.}, }}
\def\wrt{\mbox{\textit{w.r.t. }}}
\def\vol{{\mbox{vol}}}
\def\diam{\mbox{diam}}

\def\balpha{\mbox{{\boldmath }}}
\def\bbeta{\mbox{{\boldmath }}}
\def\bzeta{\mbox{{\boldmath }}}
\def\bgamma{\mbox{{\boldmath }}}
\def\bdelta{\mbox{{\boldmath }}}
\def\bmu{\mbox{{\boldmath }}}
\def\bftau{\mbox{{\boldmath }}}
\def\beps{\mbox{{\boldmath }}}
\def\blambda{\mbox{{\boldmath }}}
\def\bLambda{\mbox{{\boldmath }}}
\def\bnu{\mbox{{\boldmath }}}
\def\bomega{\mbox{{\boldmath }}}
\def\bfeta{\mbox{{\boldmath }}}
\def\bsigma{\mbox{{\boldmath }}}
\def\bzeta{\mbox{{\boldmath }}}
\def\bphi{\mbox{{\boldmath }}}
\def\bxi{\mbox{{\boldmath }}}
\def\bvphi{\mbox{{\boldmath }}}
\def\bdelta{\mbox{{\boldmath }}}
\def\bvarpi{\mbox{{\boldmath }}}
\def\bvarsigma{\mbox{{\boldmath }}}
\def\bXi{\mbox{{\boldmath }}}
\def\bmW{\mbox{{\boldmath }}}
\def\bmY{\mbox{{\boldmath }}}

\def\bPi{\mbox{{\boldmath }}}

\def\bOmega{\mbox{{\boldmath }}}
\def\bDelta{\mbox{{\boldmath }}}
\def\bPi{\mbox{{\boldmath }}}
\def\bPsi{\mbox{{\boldmath }}}
\def\bSigma{\mbox{{\boldmath }}}
\def\bUpsilon{\mbox{{\boldmath }}}

\def\mA{{\mathcal A}}
\def\mB{{\mathcal B}}
\def\mC{{\mathcal C}}
\def\mD{{\mathcal D}}
\def\mE{{\mathcal E}}
\def\mF{{\mathcal F}}
\def\mG{{\mathcal G}}
\def\mH{{\mathcal H}}
\def\mI{{\mathcal I}}
\def\mJ{{\mathcal J}}
\def\mK{{\mathcal K}}
\def\mL{{\mathcal L}}
\def\mM{{\mathcal M}}
\def\mN{{\mathcal N}}
\def\mO{{\mathcal O}}
\def\mP{{\mathcal P}}
\def\mQ{{\mathcal Q}}
\def\mR{{\mathcal R}}
\def\mS{{\mathcal S}}
\def\mT{{\mathcal T}}
\def\mU{{\mathcal U}}
\def\mV{{\mathcal V}}
\def\mW{{\mathcal W}}
\def\mX{{\mathcal X}}
\def\mY{{\mathcal Y}}
\def\mZ{{\mathcal{Z}}}



\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}


\def\bmA{{\mathbfcal A}}
\def\bmB{{\mathbfcal B}}
\def\bmC{{\mathbfcal C}}
\def\bmD{{\mathbfcal D}}
\def\bmE{{\mathbfcal E}}
\def\bmF{{\mathbfcal F}}
\def\bmG{{\mathbfcal G}}
\def\bmH{{\mathbfcal H}}
\def\bmI{{\mathbfcal I}}
\def\bmJ{{\mathbfcal J}}
\def\bmK{{\mathbfcal K}}
\def\bmL{{\mathbfcal L}}
\def\bmM{{\mathbfcal M}}
\def\bmN{{\mathbfcal N}}
\def\bmO{{\mathbfcal O}}
\def\bmP{{\mathbfcal P}}
\def\bmQ{{\mathbfcal Q}}
\def\bmR{{\mathbfcal R}}
\def\bmS{{\mathbfcal S}}
\def\bmT{{\mathbfcal T}}
\def\bmU{{\mathbfcal U}}
\def\bmV{{\mathbfcal V}}
\def\bmW{{\mathbfcal W}}
\def\bmX{{\mathbfcal X}}
\def\bmY{{\mathbfcal Y}}
\def\bmZ{{\mathbfcal Z}}



\def\0{{\bf 0}}
\def\1{{\bf 1}}

\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf{Z}}}


\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bff{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

\def\hy{\hat{y}}
\def\hby{\hat{{\bf y}}}


\def\mmE{{\mathbb E}}
\def\mmP{{\mathbb P}}
\def\mmQ{{\mathbb Q}}
\def\mmB{{\mathrm B}}
\def\mmR{{\mathbb R}}
\def\mmS{{\mathbb S}}
\def\mmV{{\mathbb V}}
\def\mmN{{\mathbb N}}
\def\mmZ{{\mathbb Z}}
\def\mMLr{{\mM_{\leq k}}}

\def\tC{\tilde{C}}
\def\tk{\tilde{r}}
\def\tJ{\tilde{J}}
\def\tbx{\tilde{\bx}}
\def\tbK{\tilde{\bK}}
\def\tL{\tilde{L}}
\def\tbPi{\mbox{{\boldmath }}}
\def\tw{{\bf \tilde{w}}}



\def\barx{\bar{\bx}}

\def\pd{{\succ\0}}
\def\psd{{\succeq\0}}
\def\vphi{\varphi}
\def\trsp{{\sf T}}

\def\mRMD{{\mathrm{D}}}
\def \DKL{{D_{KL}}}
\def\st{{\mathrm{s.t.}}}
\def\nth{{\mathrm{th}}}
\def\Pr{{\mathrm{Pr}}}
\def\patch{{\mathrm{patch}}}


\def\bx{{\bf x}}
\def\bX{{\bf X}}
\def\by{{\bf y}}
\def\bY{{\bf Y}}
\def\bw{{\bf w}}
\def\bW{{\bf W}}
\def\balpha{{\bm \alpha}}
\def\boldeta{{\bm \eta}}
\def\boldEta{{\bm \Eta}}
\def\bGamma{{\bf \Gamma}}
\def\bmu{{\bm \mu}}

\def\bK{{\bf K}}
\def\bb{{\bf b}}
\def\bg{{\bf g}}
\def\bp{{\bf p}}
\def\bP{{\bf P}}
\def\bh{{\bf h}}
\def\bc{{\bf c}}
\def\bz{{\bf z}}

\def\st{{\mathrm{s.t.}}}
\def\tr{\mathrm{tr}}
\def\grad{{\mathrm{grad}}}
\def\Id{{\mathrm{Id}}}
\def\ReLU{{\mathrm{ReLU}}}

\usepackage{ntheorem}
\newtheorem{coll}{Corollary}
\newtheorem{deftn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem*{*thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem*{*lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{ass}{Assumption}
\newtheorem{prob}{Problem}
\newenvironment*{proof}{\textbf{Proof}\quad}{\hfill \par\vspace{2pt}}

\renewcommand\theprob{\Roman{prob}}

\usepackage{ dsfont }
\def\mm1{{\mathds{1}}}
\def\supp{\mbox{supp}}
\def\bvarepsilon{{\boldmath{\varepsilon}}}
\def\Lip{\mbox{Lip}}
\def\bkappa{\mbox{{\boldmath }}}

\usepackage{enumitem}




\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  
\newcommand{\yes}{\textcolor[rgb]{0,0.6,0}{\large{\checkmark}}}
\newcommand{\no}{\textcolor[rgb]{0.6,0,0}{\large{{\bf\times}}}}

\def\red{\textcolor{black}}
\def\blue{\textcolor{black}}
\def\green{\textcolor{black}}






\clearpage{}
\newcommand{\SH}[1]{{\color{blue} {\bf (SH: #1)}}}
\newcommand{\AY}[1]{{\color{purple} {\bf (AY: #1)}}}
\newcommand{\LY}[1]{{\color{green} {\bf (LY: #1)}}}

\def\shihao{\textcolor{black}}

\title{Improving Deep Regression with Ordinal Entropy}






\author{Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, Angela Yao \\
National University of Singapore, Singapore, \\ 
Huawei International Pte Ltd, Singapore
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
In computer vision, it is often observed that formulating regression problems as a classification task yields better performance.  
We investigate this curious phenomenon and provide a derivation to show that classification, with the cross-entropy loss, outperforms 
regression with a mean squared error loss in its ability to learn high-entropy feature representations.
Based on the analysis, we propose an ordinal entropy \shihao{regularizer} to encourage higher-entropy feature spaces while maintaining ordinal relationships to improve the performance of regression tasks.
Experiments on synthetic and real-world regression tasks demonstrate the 
importance and benefits of increasing entropy for regression. Code can be found here: \url{https://github.com/needylove/OrdinalEntropy}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Classification and regression are two fundamental tasks of machine learning. The choice between the two usually depends on the categorical or continuous nature of the target output. Curiously, in computer vision, specifically with deep learning, it is often preferable to solve regression-type problems as classification tasks. A simple and common way is to discretize the continuous labels; each bin is then treated as a class. After converting regression into classification, the ordinal information of the target space is lost. Discretization errors are also introduced to the targets. Yet for a diverse set of regression problems, including depth estimation~\citep{cao2017estimating}, age estimation~\citep{rothe2015dex}, crowd-counting~\citep{liu2019counting} and keypoint detection~\citep{li20212d}, classification yields better performance.


The phenomenon of classification outperforming regression on inherently continuous estimation tasks naturally begs the question of why. Previous works have not investigated the cause, although they hint at task-specific reasons. 
For depth estimation, both~\cite{cao2017estimating} and \cite{fu2018deep} postulate that it is easier to estimate a quantized range of depth values rather than one precise depth value. 
For crowd counting, regression suffers from inaccurately generated target values~\citep{xiong2022discrete}. Discretization helps alleviate some of the imprecision. For pose estimation, classification allows for the denser and more effective heatmap-based supervision~\citep{zhang2020distribution,gu2021removing,gu2021dive}. 


Could the performance advantages of classification 
run deeper than task-specific nuances? In this work, we posit that regression lags in its ability to learn high-entropy feature representations. We arrive at this conclusion by analyzing the differences between classification and regression from a mutual information perspective.   
According to~\citet{shwartz2017opening}, deep neural networks during learning aim to maximize the mutual information between the learned representation  and the target . The mutual information between the two can be defined as  . 
 is large when the marginal entropy  is high, \ie features  are as spread as possible, and the conditional entropy  is low, \ie features of common targets are as close as possible. 
Classification accomplishes both objectives~\citep{boudiaf2020unifying}.  This work, as a key contribution, shows through derivation that regression minimizes  but ignores .  Accordingly, the learned representations  from regression have a 
lower marginal entropy (see Fig.~\ref{fig_train}).  A t-SNE visualization of the features (see Fig.~\ref{fig_regression} and\ref{fig_classification}) confirms that features learned by classification have more spread than features learned by regression. More visualizations are shown in Appendix \ref{appendix_C}.

\begin{figure}[!t]
\centering

\subfigure[Entropy of feature space] {
 \label{fig_train}
\includegraphics[width=0.33\columnwidth]{fig/entropy_training.png}
}
\subfigure[Regression] {
 \label{fig_regression}
\includegraphics[width=0.25\columnwidth]{fig/regression2.png}
}
\subfigure[Classification] {
\label{fig_classification}
\includegraphics[width=0.28\columnwidth]{fig/classification256.png}
}
\caption{Feature learning of regression versus classification for depth estimation.
Regression keeps features close together and forms an ordinal relationship, while classification spreads the features (compare (b) vs. (c) ), leading to a higher entropy feature space. Features are colored based on their predicted depth. Detailed experimental settings are given in Appendix \ref{appendix_C}.
}
\label{fig_motivation1}
\end{figure}

The difference in entropy between classification and \shihao{regression} stems from the different losses. We postulate that the lower entropy features learned by  losses in regression explain the performance gap compared to classification.  Despite its overall performance advantages, classification lags in the ability to capture ordinal relationships.  As such, simply spreading the features for regression to emulate classification will break the inherent ordinality of the regression target output.  

To retain the benefits of both high entropy and ordinality for feature learning, we propose, as a second contribution, an 
ordinal entropy \shihao{regularizer}
for regression.  
Specifically, we capture ordinal relationships as a weighting based on the distances between samples in both the representation and target space. 
Our ordinal entropy \shihao{regularizer} increases the distances between representations, while weighting the distances to preserve the ordinal relationship.



The experiments on various regression tasks demonstrate the effectiveness of our proposed method.
Our main contributions are three-fold: 
\begin{itemize}
    \item To our best knowledge, we are the first to analyze regression's reformulation as a classification problem, especially in the view of representation learning. {We find that regression lags in its ability to learn high-entropy features, which in turn leads to the lower mutual information between the learned representation and the target output.}
    \item Based on our theoretical analysis, we design an ordinal entropy \shihao{regularizer} to learn high-entropy feature representations that preserve ordinality.
    \item Benefiting from our ordinal entropy loss, our methods achieve significant improvement on synthetic datasets for solving ODEs and stochastic PDEs as well as real-world regression tasks including depth estimation, crowd outing and age estimation. 
\end{itemize}


\section{Related work}\label{sec:relatedwork}

\textbf{Classification for Continuous Targets.} 
Several works formulate regression problems as classification tasks to improve performance.  They focus on different design aspects such as label discretization and uncertainty modeling. 
To discretize the labels, \citet{cao2017estimating,fu2018deep} and \citet{liu2019counting} convert the continuous values into discrete intervals with a pre-defined interval width. 
To improve class flexibility,~\citet{bhat2021adabins} followed up with an adaptive bin-width estimator. 
Due to inaccurate or imprecise regression targets, several works have explored modeling the uncertainty of labels with classification. \citet{liu2019counting} proposed estimating targets that fall within a certain interval with high confidence.~\citet{tompson2014joint} and~\cite{newell2016stacked} propose modeling the uncertainty by using a heatmap target in which each pixel represents the probability of that pixel being the target class.  This work, instead of focusing on task-specific designs, explores the difference between classification and regression from a learning representation point of view.  By analyzing mutual information, we reveal a previously underestimated impact of high-entropy feature spaces. 


\shihao{\textbf{Ordinal Classification.} 
Ordinal classification aims to predict ordinal target outputs.
Many works exploit the distances between labels~\citep{castagnos-etal-2022-simple,10.1007/978-3-031-12053-4_12, gong2022ranksim} to preserve ordinality. Our ordinal entropy regularizer also preserves the ordinality by exploiting the label distances, while it mainly aims at encouraging a higher-entropy feature space.
}

\textbf{Entropy.}
The entropy of a random variable reflects its uncertainty and can be used to analyze and regularize a feature space. 
With entropy analysis, \cite{boudiaf2020unifying} has shown the benefits of the cross-entropy loss, \ie encouraging features to be dispersed while keeping intra-class features compact. Moreover, existing works~\citep{pereyra2017regularizing,dubey2017regularizing} have shown that 
many regularization terms, like confidence penalization~\citep{pereyra2017regularizing} and label smoothing~\citep{muller2019does}, are actually regularizing the entropy of the output distribution. Inspired by these works, we explore the difference in entropy between classification and regression. Based on our entropy analysis, we design an entropy term (\ie ordinal entropy) for regression and bypass explicit classification reformulation with label discretization.

\begin{figure}[!t]
\centering
\subfigure[{Illustration of frameworks}] {
\label{fig:framework}
\includegraphics[width=0.32\columnwidth]{fig/framework2.pdf}
}
\subfigure[{Tightness and diversity of ordinal entropy}] {
\label{fig_ordinalEntropy}
\includegraphics[width=0.64\columnwidth]{fig/ordinalentropyIllustrate.png}
}
\caption{Illustration of (a) regression and classification for continuous targets, and the use of our ordinal entropy for regression, (b) the pull and push objective of tightness and diversity on the feature space. The tightness part encourages features to be close to their feature centers while the diversity part encourages feature centers
to be far away from each other.}
\label{fig_weightentropy}
\end{figure}

\section{A Mutual-Information Based Comparison 
on Feature Learning}
\label{sec:theory}
\subsection{Preliminaries}
Suppose we have a dataset  with  input data  and their corresponding labels . In a typical regression problem for computer vision,  
is an image or video,  
while  takes a continuous value in the label space .
The target of regression is to recover  by encoding the image to feature  with encoder  and then mapping  to a predicted target  with a regression function  parameterized by . 
The encoder  and  are learned by minimizing a regression loss function such as the mean-squared error . 

To formulate regression as a classification task with  classes, the continuous target  can be converted to classes  with some discretizing mapping function, where  is a categorical target. The feature  is then mapped to the categorical target  with classifier  parameterized by .  The encoder  and  are learned by minimizing the cross-entropy loss , where   .
Fig.~\ref{fig:framework} visualizes the symmetry of the two formulations. 

The entropy of a random variable can be loosely defined as the amount of ``information'' associated with that random variable. One approach for estimating entropy  for a random variable  is the meanNN entropy estimator~\citep{faivishevsky2008ica}.  
It can accommodate higher-dimensional s, and is commonly used in high-dimensional space~\citep{faivishevsky2010nonparametric}. 
The meanNN estimator relies on the distance between samples to approximate . For a -dimensional , it is defined as




\subsection{Feature Learning with a Cross-Entropy Loss (Classification)} 

Given that mutual information between target  and feature  is defined as , it follows that  can be maximized by minimizing the second term  and maximizing the first term . \citet{boudiaf2020unifying} showed that minimizing the cross-entropy loss accomplishes both 
by approximating the standard cross-entropy loss  with a pairwise cross-entropy loss .   serves as a lower bound for , and can be defined as 



where 
 is to make sure   is a convex function with respect to .
Intuitively,  can be understood as being composed of both a pull and a push objective more familiar in contrastive learning. We interpret the pulling force as a tightness term.  It encourages higher values for  and closely aligns the feature vectors within a given class. This results in features clustered according to their class, \ie lower conditional entropy .  The pushing force from the second term encourages lower  while forcing classes' centers  to be far from the origin.  This results in diverse features that are spread apart, or higher marginal entropy . Note that the tightness term corresponds to the numerator of the Softmax function in , while the diversity term corresponds to the denominator. 

\subsection{Feature Learning with an  Loss (Regression)} 

In this work, we find that minimizing , as done in regression, is a proxy for minimizing , without increasing .  Minimizing  does not increase the marginal entropy  and therefore limits feature diversity. The link between classification and regression is first established below in Lemma~\ref{Lemma_discretize}.  We assume that we are dealing with a linear regressor, as is commonly used in deep neural networks.

\begin{lemma}
\label{Lemma_discretize}
We are given dataset , where  is the input and  is the label,
	and linear regressor  parameterized by 
	.  
	Let  denote the corresponding feature. 
	Assume that the label space  is discretized into bins with maximum width , and  is the center of the bin to which  belongs. Then for any , there exists  such that:

\end{lemma}

The detailed proof of Lemma~\ref{Lemma_discretize} is provided in Appendix~\ref{appendix_A}. 

The result of Lemma~\ref{Lemma_discretize} says that the discretization error from replacing a regression target  with  can be made arbitrarily small if the bin width  is sufficiently fine. As such, the  can be directly approximated by the second term of Eq.~\ref{eq:lemma1} \ie  . With this result, it can be proven that minimizing  is a proxy for \shihao{minimizing} .

\begin{thm}
Let  denote the \shihao{center of the features} corresponding to bin center , and  be the angle between  and . Assume that  is normalized, , where  is the distribution of  and that  is fixed. 
Then, minimizing  can be seen as a proxy for \shihao{minimizing}  \shihao{ without increasing }.
\label{thm:1}
\end{thm}

\begin{proof}
Based on Lemma~\ref{Lemma_discretize}, we have



Note,  exist unless  and .
Since it is assumed that , the term  can be interpreted as a conditional cross entropy between  and , as it satisfies

where  denotes equal to, up to a multiplicative and an additive constant.  The  denotes Monte Carlo sampling from the  distribution, allowing us to replace the expectation by the mean of the samples. Subsequently, we can show that 

The {result} in Eq.~\ref{eq:proofresult} shows that  is an upper bound of the tightness term in mutual information.  
If , then  is equal to 0 and the bound is tight \ie .
Hence, minimizing  is a proxy for \shihao{minimizing} .


\shihao{Apart from , the relation in Eq. \ref{eq:proofresult} also contains the KL divergence between the two conditional distributions  and , where  are feature centers of Z.  Minimizing this divergence will either force  closer to the centers , or move the centers  around. By definition, however, the cluster centers  cannot expand beyond â€™s coverage, so features Z must shrink to minimize the divergence. As such, the entropy  will not be increased by this term.}
\end{proof}

Based on 
Eq. \ref{eq:pce_reformulated}
and Theorem~\ref{thm:1}, we draw the conclusion that regression, with an MSE loss, overlooks the marginal entropy  and results in a less diverse feature space than classification with a cross-entropy loss.





It is worth mentioning that the Gaussian distribution assumption, \ie , is standard in the literature when analyzing features \citep{yang2021free,salakhutdinov2012one} and entropy \citep{misra2005estimation}, and  is a constant value at each iteration.

\section{Ordinal Entropy}

Our theoretical analysis in Sec.~\ref{sec:theory} shows that learning with only the MSE loss does not increase the marginal entropy  and results in lower feature diversity.  To remedy this situation, we propose a novel
regularizer to encourage a higher entropy feature space. 

Using the distance-based entropy estimate from Eq.~\ref{equ_entropyestimation}, 
one can then minimize the 
\shihao{the negative distances between feature centers  to maximize the entropy of the feature space.  are calculated by taking a mean over all the features  which project to the same .}
Note that as feature spaces are unbounded, the features \shihao{} must first be normalized, and below, we assume all the features \shihao{} are already normalized with an L2 norm:




where  is the number of feature centers \shihao{in a batch of samples or a sampled subset sampled from a batch}. We consider each feature as a feature center when the continuous labels of the dataset are precise enough.

While the \shihao{regularizer}  indeed spreads features to a larger extent, it also breaks ordinality in the feature space (see Fig.~\ref{fig_directlyentropy}). As such, we opt to weight the feature norms in  with , where  are the distances in the label space :


As shown in Figure \ref{fig_wd},  spreads the feature while also preserve preserving ordinality.  Note that  is a special case of  when  are all equal.

To further minimize the conditional entropy , we introduce an additional tightness term that directly considers the distance between each feature  with 
\shihao{its} centers  in the feature space:

\shihao{where  is the batch size.} Adding this tightness term further encourages features \shihao{close to its centers} (compare Fig.~\ref{fig_wd} with Fig.~\ref{fig_w}).  
Compared with features from standard regression (Fig.~\ref{fig_reg2}), the features in Fig.~\ref{fig_w} are more spread, \ie the lines formed by the features are longer. 


We define the ordinal entropy \shihao{regularizer} as , with a diversity term  and a tightness term .  achieves similar effect as classification in that it spreads  while tightening features  corresponding to . \shihao{Note, if the continuous labels of the dataset are precise enough and each feature is its own center, then our ordinal entropy regularizer will only contain the diversity term, \ie .}
We show our regression with ordinal entropy (red dotted arrow) in Fig.~\ref{fig:framework}.

The final loss function  is defined as:

where  is the task-specific regression loss and  and  are trade-off parameters.

\begin{figure}[!t]
\centering
\subfigure[Regression] {
 \label{fig_reg2}
\includegraphics[width=0.2\columnwidth]{fig/regression2.png}
}
\subfigure[Regression ] {
 \label{fig_directlyentropy}
\includegraphics[width=0.2\columnwidth]{fig/directlyentropy.png}
}
\subfigure[Regression ] {
 \label{fig_wd}
\includegraphics[width=0.2\columnwidth]{fig/weightentropy2.png}
}
\subfigure[
Regression ] {
 \label{fig_w}
\includegraphics[width=0.23\columnwidth]{fig/weightentropyTightness2.png}
}
\caption{t-SNE visualization 
of features from the depth estimation task. (b) Simply spreading the features () leads to a higher entropy feature space, while the ordinal relationship is lost. (c) By further exploiting the ordinal relationship in the label space (), the features are spread and the ordinal relationship is also preserved. (d) Adding the tightness term () further encourages features close to its centers.}
\label{fig_vis}
\end{figure}



\section{Experiments}
\label{sec:experiment}

\subsection{Datasets,  Metrics \& Baseline Architectures}
We conduct experiments on four tasks: operator learning, a synthetic dataset and three real-world regression settings of depth estimation, crowd counting, age estimation. 

For \textbf{operator learning}, we follow the task from DeepONet~\citep{lu2021learning} and use a two-layer fully connected neural network with 100 hidden units. See Sec.~\ref{sec:synthetic} for details on data preparation. 

For \textbf{depth estimation}, NYU-Depth-v2 \citep{silberman2012indoor} provides indoor images with the corresponding depth maps at a pixel resolution . 
We follow~\citep{lee2019big} and use ResNet50~\citep{he2016deep} as our baseline architecture unless otherwise indicated.  We use the train/test split given used by previous works~\citep{bhat2021adabins,yuan2022new}, 
and evaluate with the standard metrics of threshold accuracy ,  average relative error (REL), root mean squared error (RMS)  and average  error.  

For \textbf{crowd counting}, we evaluate on SHA and SHB of the ShanghaiTech crowd counting dataset (SHTech)~\citep{zhang2015cross}. Like previous works, we adopt density maps as labels and evaluate with mean absolute error (MAE) and mean squared error (MSE).  We follow~\cite{li2018csrnet} and use CSRNet with ResNet50 as the \shihao{regression} baseline architecture.

For \textbf{age estimation}, we use AgeDB-DIR~\citep{yang2021delving} and also implement their \shihao{regression} baseline model, which uses ResNet-50 as a backbone. Following~\cite{liu2019large}, we report results on three disjoint subsets (\ie,Many, Med. and Few), and also overall performance (\ie ALL). We evaluate with MAE and geometric mean (GM).


\textbf{Other Implementation Details:} We follow the settings of previous works DeepONet~\citep{lu2021learning} for operator learning, Adabins~\citep{bhat2021adabins} for depth estimation, CSRNet~\citep{li2018csrnet} for crowd counting, and~\cite{yang2021delving} for age estimation.  See Appendix~\ref{appendix_E} for details.

 and  are set empirically based on the scale of the task loss .  We use the trade-off parameters  the same value of  for operator learning, depth estimation, crowd counting and age estimation, respectively.


\subsection{Learning Linear and Nonlinear Operators}\label{sec:synthetic}

We first verify our {method} on the synthetic task of operator learning. In this task, an (unknown) operator maps input functions into output functions and the objective is to regress the output value.  We follow~\citep{lu2021learning} and generate data for both a linear and non-linear operator. For the linear operator, we aim to learn the integral operation :

where  is the input function, and  is the target function. The data is generated with a mean-zero Gaussian random field function space: , where the covariance kernel  is the radial-basis function kernel with a length-scale parameter . The function  is represented by the function values of  fixed locations . 
The data is generated as , where  is sampled from the domain of . We randomly sample 1k data as the training set and test on the testing set with 100k samples.


For the nonlinear operator, we aim to learn the following {stochastic partial differential equation}, which maps  of different correlation lengths  to a solution :

where , 
 from the random space 
with Dirichlet boundary conditions , and . The randomness comes from the diffusion coefficient .  The function  is modelled as a Gaussian random process , with mean  and .
We randomly sample 1k training samples and 10k test samples.

\begin{table*}[t]
	\caption{Ablation studies on linear and nonlinear operators learning with synthetic data and depth estimation on NYU-Depth-v2. For operator learning, we report results as mean  standard variance over 10 runs. 
    \textbf{Bold} numbers indicate the best performance.}
	\label{tab:depth_ablation}
	\centering
	\scalebox{0.95}{
		\begin{tabular}{l|c|c|cccc}
			\hline
			\multirow{2}[0]{*}{Method} & \multicolumn{1}{c|}{Linear}& \multicolumn{1}{c|}{Nonlinear} & \multicolumn{4}{c}{NYU-Depth-v2}\\
 			\cline{2-7}
			& ()~ & ()~ & ~ & REL~ & RMS~ & ~ \\
			\hline
			Baseline  & 3.0  0.93 &  2.5 2.0 & 0.793  & 0.148 & 0.502 & 0.064 \\
			Baseline +  & 2.2  0.42 & 0.8  0.3 & 0.795  & 0.147 & 0.505 & 0.063  \\
			Baseline +  &  \textbf{1.6}  \textbf{0.34} & 0.5  0.3 & 0.808 & 0.144 & 0.483 & 0.061 \\
			Baseline +    & - & - & \textbf{0.811} & \textbf{0.143} & \textbf{0.478} & \textbf{0.060} \\
			\midrule
			w/ cosine distance  & 1.7  0.33 & 0.6  0.5 & 0.806 & 0.147 & 0.488 & 0.061 \\
			w/o normalization  & 5.3  1.10  & 15.5  0.4 & 0.790 & 0.153 & 0.510 & 0.064\\	\midrule
  & 2.0   0.60 & \textbf{0.4}  \textbf{0.2} & 0.800 & 0.149 & 0.498 & 0.063\\
  & 1.8  0.50  & 0.5 0.2 & 0.799 & 0.147 & 0.496 & 0.062\\
			\hline
		\end{tabular}}
\end{table*}

For operator learning, we set  as the task-specific baseline loss for both the linear and non-linear operator. Table~\ref{tab:depth_ablation} shows that even without ordinal information, adding the diversity term \ie  to   already improves performance.  The best gains, however, are achieved by incorporating the weighting with , which decreases  by 46.7\% for the linear operator and up to 80\% for the more challenging non-linear operator. The corresponding standard variances are also reduced significantly. 

Note that we do not verify  on operator learning due to the 
high data precision on synthetic datasets, it is difficult to sample points belonging to the same . Adding , however, is beneficial for the three real-world tasks (see Sec.~\ref{subsection_depth}).  

\subsection{Real-World Tasks: Depth Estimation, Crowd Counting \& Age Estimation}
\label{subsection_depth}

\textbf{Depth Estimation:} 
Table~\ref{tab:nyu} shows that adding the ordinal entropy terms boosts the performance of the regression baseline and the state-of-the-art regression method~NeW-CRFs~\citep{yuan2022new}.  NeW-CRFs with ordinal entropy achieves the highest values for all metrics, decreasing  and REL errors by  and , respectively.
Moreover, higher improvement can be observed when adding the ordinal entropy into a simpler baseline, \ie ResNet-50.


\begin{table*}[t]
	\caption{{Quantitative comparison} of depth estimation results with NYU-Depth-v2. \textbf{Bold} numbers indicate the best performance.}
	\label{tab:nyu}
	\centering
		\begin{tabular}{l|cccc}
			\hline
			\multirow{1}[0]{*}{Method}
			& ~ & REL~ & RMS~ & ~ \\
			\hline
Laina et al. \citep{laina2016deeper} & 0.811 & 0.127 & 0.573 & 0.055  \\
			DORN \citep{fu2018deep} & 0.828 & 0.115 & 0.509 & 0.051  \\
			BTS \citep{lee2019big} & 0.885  & 0.110 & 0.392 & 0.047  \\
			Adabins \citep{bhat2021adabins} & 0.903 & 0.103 & 0.364 & 0.044 \\
			\midrule
			NeW-CRFs \citep{yuan2022new} & 0.922  & 0.095 & 0.334 & 0.041 \\
			NeW-CRFs +  & ~~\bf{0.932}~~ & ~~\bf{0.089}~~ & ~~\bf{0.321}~~ &~~\bf{0.039}~~\\
			Baseline (ResNet-50) & 0.793  & 0.148 & 0.502 & 0.064\\
			Baseline (ResNet-50) +  & 0.808 & 0.144 & 0.483 & 0.061\\
			Baseline (ResNet-50) +  & {0.811} & {0.143} & {0.478} & {0.060}\\
			\hline
		\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption{Results on SHTech. \textbf{Bold} numbers indicate the best performance.}
	\label{tab:counting}
	\centering
		\begin{tabular}{c|cc|cc}
			\hline
			\multirow{2}[0]{*}{Method} & \multicolumn{2}{c|}{MAE~} & \multicolumn{2}{c}{MSE~}  \\
 			\cline{2-5}
			& SHA & SHB &  SHA & SHB\\
			\hline
			\shihao{Regression} Baseline~\citep{li2018csrnet} & 68.2 & 10.6 & 115.0 & 16.0 \\
			+ & 66.9 & \textbf{9.1} & 107.5 & 14.7 \\
			+ +  & \textbf{65.6} & \textbf{9.1} & \textbf{105.0} & \textbf{14.5} \\
			\hline
		\end{tabular}
\end{table*}


\begin{table*}[t]
	\caption{Results on AgeDB-DIR. 
\textbf{Bold} numbers indicate the best performance.
	}
	\label{tab:age}
	\centering
		\begin{tabular}{c|cccc|cccc}
			\hline
			\multirow{2}[0]{*}{Method} & \multicolumn{4}{c|}{MAE~} & \multicolumn{4}{c}{GM~}  \\
 			\cline{2-9}
 			
			& ALL & Many & Med. & Few & ALL & Many & Med. & Few\\
			\hline	Baseline~\citep{yang2021delving} & 7.77 & \textbf{6.62} & 9.55 & 13.67 & 5.05 & 4.23 & 7.01 & 10.75 \\
 & 7.60 & 6.79 & 8.55 & 12.70 & 4.76 & 4.15 & 5.95 & \textbf{9.60} \\
			  & \textbf{7.46} & 6.73 & \textbf{8.18} & \textbf{12.38} & \textbf{4.72} & \textbf{4.21} & \textbf{5.36} & 9.70 \\
			\hline
		\end{tabular}
\end{table*}



\textbf{Crowd Counting:} Table~\ref{tab:counting} shows that adding  and  each contribute to improving the baseline.  Adding both terms has the largest impact and for SHB, the improvement is up to 14.2\% on MAE and 9.4\% on MSE.  

\textbf{Age Estimation:}
Table~\ref{tab:age} shows that with  we achieve a significant 0.13 and 0.29 overall improvement (\ie ALL) on MAE and GM, respectively. Applying  achieves a further overall improvement over  only, including 0.14 on MAE and 0.04 on GM.

\subsection{Ablation Studies}
Ablation results on both operator learning and depth estimation are shown in Table~\ref{tab:depth_ablation} and Figure~\ref{fig:ablation}. 

\textbf{Ordinal Relationships:} Table \ref{tab:depth_ablation} shows that using the unweighted diversity term `Baseline+', which ignores ordinal relationships, is worse than the weighted version `Baseline+' for both operator learning and depth estimation. 


\textbf{Feature Normalization:} As expected, normalization is important, as performance will decrease (compare `w/o normalization' to `Baseline+' in Table~\ref{tab:depth_ablation}) for both operator learning and depth estimation.  Most interestingly, normalization also helps to lower variance for operator learning. 

\textbf{Feature Distance} : We replace the original feature distance L2 with cosine distance (see `w/ cosine distance') and the cosine distance is slightly worse than L2 for all the cases.

\textbf{Weighting Function} : The weight as defined in Eq.~\ref{eq:ordinal_entropy} is based on an L2 distance. Table~\ref{tab:depth_ablation} shows that L2 is best for linear operator learning and depth estimation but is slightly worse than 

for nonlinear operator learning.


\textbf{Sample Size (M)}
In practice, we estimate the entropy from a limited number of regressed samples and this is determined by the batch size. For certain tasks, this may be sufficiently large, \eg depth estimation (number of pixels per image  batch size) or very small, \eg age estimation (batch size).  We investigate the influence of  from Eq. \ref{eq:ordinal_entropy} on linear operator learning (see Fig.\ref{fig_lamda_MSE}).
In the most extreme case, when  = 2, the performance is slightly better than the baseline model ( vs. ), suggesting that our ordinal \shihao{regularizer} terms are effective even with 2 samples. 
As  increases, the MSE and its variance steadily decrease as the estimated entropy likely becomes more accurate.  However, at a certain point, MSE and variance start to increase again. This behavior is not surprising; with too many samples under consideration, it likely becomes too difficult to increase the distance between a pair of points without decreasing the distance to other points, \ie there is not sufficient room to maneuver. 
The results for the nonlinear operator learning are given in Appendix \ref{appendix_nonlinear}.


\textbf{Hyperparameter  and :} 
Fig.~\ref{fig_lamda_MSE} plots the MSE for linear operator learning versus the trade-off hyper-parameter   applied to the diversity term .  Performance remains relatively stable up to , after which this term likely overtakes the original learning objective  and causes MSE to decrease.
\shihao{The results for the nonlinear operator and analysis on  are given in Appendix \ref{appendix_nonlinear} and \ref{sec_lambdat}.}

\textbf{Marginal Entropy :} We show the marginal
entropy of the testing set from different methods during training (see Fig.~\ref{fig_entropy_methods}). We can see that the marginal entropy of classification is always larger than that of regression, which has a downward trend. 
Regression with only diversity achieves the largest marginal entropy, which verifies the effectiveness of our diversity term.
With both diversity and tightness terms, as training goes, its marginal entropy continues to increase and larger than that of regression after the  epoch.
\shihao{More experiment results can be found in Appendix \ref{app_m}.}

\begin{figure}[!t]
	\centering
	\subfigure[MSE with number of samples]{	
	\label{fig_sample_MSE}
	\includegraphics[width=0.30\linewidth]{fig/mse_samples2.png}}
	\subfigure[MSE with ]{	
	\label{fig_lamda_MSE}
	\includegraphics[width=0.30\linewidth]{fig/mse_weight2.png}}
	\subfigure[Entropy of different methods]{	
	\label{fig_entropy_methods}
	\includegraphics[width=0.33\linewidth]{fig/mv_6.png}}
	\caption{Based on the linear operator learning and depth estimation, we show (a) the effect of the number of samples on MSE, (b) the performance analysis with different  and (c) the entropy curves of different methods during testing. The results for the nonlinear operator learning are given in Appendix \ref{appendix_nonlinear}. }
	\label{fig:ablation}
\end{figure}


\section{Conclusion}
\label{sec:conclusion}

In this paper, we dive deeper into the trend of solving regression-type problems as classification tasks by comparing the difference between regression and classification from a mutual information perspective. We conduct a theoretical analysis and show that regression with an MSE loss lags in its
ability to learn high-entropy feature representations. Based on the findings, we propose an ordinal entropy \shihao{regularizer} for regression, which not only keeps an ordinal relationship in feature space like regression, but also learns a high-entropy feature representation like classification. Experiments on different regression tasks demonstrate that our entropy \shihao{regularizer} can serve as a plug-in component for regression-based methods to further improve the performance. 

\textbf{Acknowledgement}. This research / project is supported by the Ministry of Education, Singapore, under its MOE Academic Research Fund Tier 2 (STEM RIE2025 MOE-T2EP20220-0015).

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage

\renewcommand\thefigure{\Alph{figure}} 
\setcounter{figure}{0}

\textbf{Appendix}
\appendix
\section{Proof of lemma \ref{Lemma_discretize}}
\label{appendix_A}

\begin{proof}
    
Since , we have:

For :

\end{proof}


\section{Visualization}
\label{appendix_C}

\textbf{Experimental setting}
We train the regression and classification models on the NYU-Depth-v2 dataset for depth estimation. We modify the last layer of a ResNet-50 model to a convolution operation with kernel size , and train the modified model with  as our regression model. For the classification models, we modify the last layer of two ResNet-50 models to output  channels, where  is the number of classes, and train the modified models with cross-entropy. The classes are defined by uniformly discrete ground-truth depths into  bins. The entropy of feature space is estimated using Eq \ref{equ_entropyestimation} on pixel-wise features over the training and test set of NYU-Depth-v2. After training, we visualize the pixel-wise features of an image from the test set using t-distributed stochastic neighbor embedding (t-SNE), and features are colored based on their predicted depth. 

The visualization results are shown in Figure A. We exploit three entropy estimators to estimate the entropy of feature space . Entropy in the first row of Figure A is estimated with the meanNN entropy estimator Eq.~\ref{equ_entropyestimation}. Entropy in the second row is also estimated with the meanNN entropy estimator, where the input is the features normalized with the L2 norm. Entropy in the third row is estimated with the diversity part of our ordinal entropy Eq.~\ref{eq:ordinal_entropy}.

 

We make several interesting observations from the visualization results: (1) On both training and testing, compared with classification, regression always has a lower estimated entropy based on all three entropy estimators; (2) benefiting from the diversity part of our ordinal entropy , regression produces a higher entropy space than the classification model; (3)  with the tightness term  can result in feature space with a similarly estimated entropy compared with classification. 


\begin{figure}[h]
\centering
\subfigure[Training] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_1.png}
}
\subfigure[Testing] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_2.png}
}
\subfigure[Testing] {
	\includegraphics[width=0.31\columnwidth]{fig/mv3.png}
}
\subfigure[Training(feature normalized)] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_4.png}
}
\subfigure[Testing(feature normalized)] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_5.png}
}
\subfigure[Testing(feature normalized)] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_6.png}
}
\subfigure[Training(ordinal entropy)] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_7.png}
}
\subfigure[Testing(ordinal entropy)] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_8.png}
}
\subfigure[Testing(ordinal entropy)] {
	\includegraphics[width=0.31\columnwidth]{fig/mv_9.png}
}
\label{fig:fig_mv}
\caption{Visualization results with different entropy estimators on training and testing set.}
\end{figure}

\section{Results on the nonlinear operator learning problem}
\label{appendix_nonlinear}
We analyze the effect of the number of samples on MSE loss and the performance with  on the nonlinear operator learning task. The results are shown in Figure~B. The results corroborate the conclusions derived from the linear task. 

\begin{figure}[h]
\centering
\subfigure[MSE with number of samples] {
	\includegraphics[width=0.45\columnwidth]{fig/mse_samples_nonlinear.png}
}
\subfigure[MSE with ] {
	\includegraphics[width=0.45\columnwidth]{fig/mse_weight_nonlinear.png}
}
\label{fig:result_nonlinear}
\caption{Based on the nonlinear operator learning problem, we show (a) the effect of the number of samples on MSE loss, (b) the performance analysis with different .}
\end{figure}


\section{Evaluation Metrics}
\label{appendix_E}
Here we introduce the definition of the evaluation metrics for depth estimation, crowd counting, and age estimation.

\textbf{Depth Estimation.} We denote the predicted depth at position  as  and the corresponding ground truth depth as , the total number of pixels is . 
The metrics are: 
1) threshold accuracy  \% of , where ;
2) average relative error (REL): ;
3) root mean squared error (RMS):  ;
4) average ( error): .

\textbf{Crowd Counting.} Given  images for testing, \shihao{} are the estimated count and the ground truth for the -th image, respectively. We exploit two widely used metrics as measurements:
1) Mean Absolute Error (MAE), and 
2) Mean Squared Error (MSE) .

\textbf{Age Estimation.} Given  images for testing,  and  are the -th prediction and ground-truth, respectively. The evaluation metrics include 
1)MAE: , and 
2)Geometric Mean (GM): .


\section{Effect of }
\label{sec_lambdat}

We analyze the effect of  with an ablation study on the age estimation with Age-DB-DIR.
Table~\ref{tab:age_lambda_t} shows that the final performance is not sensitive to the change of , and  is effective even with a small , \ie 0.1.


\begin{table*}[h]
	\caption{Results on AgeDB-DIR. 
\textbf{Bold} numbers indicate the best performance.
	}
	\label{tab:age_lambda_t}
	\centering
		\begin{tabular}{c|cccc|cccc}
			\hline
			\multirow{2}[0]{*}{} & \multicolumn{4}{c|}{MAE~} & \multicolumn{4}{c}{GM~}  \\
 			\cline{2-9}
 			
			& ALL & Many & Med. & Few & ALL & Many & Med. & Few\\
			\hline
			0 & 7.60 & 6.79 & 8.55 & 12.70 & 4.76 & 4.15 & 5.95 & \textbf{9.60} \\
0.1 & 7.50 & \textbf{6.53} & 8.64 & 13.50 & \textbf{4.71} & \textbf{4.04} & 5.92 & 10.62 \\
			1 & \textbf{7.46} & 6.73 & \textbf{8.18} & \textbf{12.38} & 4.72 & 4.21 & \textbf{5.36} & 9.70 \\
			10 & 7.49 & 6.70 & 8.27 & 12.86 & 4.79 & 4.23 & 5.61 & 10.21 \\
			\hline
		\end{tabular}
\end{table*}

\section{Influence of the Sample Size (M)}
\label{app_m}


Efficiency-wise, the computing complexity of the regularizer is quadratic with respect to .  The synthetic experiments on operator learning (Table \ref{tab:app_m}) use a 2-layer MLP, so the regularizer adds significant computing time when M gets large. However, the real-world experiments on depth estimation (Table \ref{tab:app_m2}) use a ResNet-50 backbone, and the added time and memory are negligible (27\% and 0.3\%, respectively), even with . We exploit  in our depth estimation experiments, where  is a 16x subsampling of the total number of pixels in an image. Note that these increases are only during training and do not add computing demands for inference. In addition, the added time and memory with  are also negligible (0.08\% and ~0\%, respectively), even with M=3536.



\begin{table*}[h]
	\caption{Quantitative comparison of the time consumption and memory consumption on linear operator learning with synthetic data.}
	\label{tab:app_m}
	\centering
		\begin{tabular}{l|cccc}
			\hline
			\multirow{1}[0]{*}{M}
			& Training time (s) & Memory taken (MB)  \\
			\hline
			0 & 155 & 2163    \\
			2 & 291 & 2163    \\
			10 & 296 & 2163    \\
			100 & 327 & 2163   \\
			1000 & 1033 & 2205    \\			
			\hline
		\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption{Quantitative comparison of the time consumption and memory consumption on depth estimation with NYU-v2. The training time is  one epoch training time.}
	\label{tab:app_m2}
	\centering
		\begin{tabular}{lr|ccc}
			\hline
			\multirow{1}[0]{*}{M}
			& Regularizer & Training time (s) & Memory taken (MB) \\
			\hline
			0 & no regularizer & 1836 & 12363   \\
			100 &  & 1877 & 12379   \\
			1000 &  & 1999 & 12395   \\
			3526  &  & 2342 & 12405   \\
			3526 &  & 2344 & 12405  \\
			\hline
		\end{tabular}
\end{table*}

\end{document}
