\section{\dataset\ Evaluation Framework}
\label{sec:eval}


Previous evaluations of scientific document representations in the literature tend to focus on small datasets over a limited set of tasks, and extremely high (99\%+) AUC scores are already possible on these data for English documents \cite{Chen2019ImprovingTN, Wang2019improving}. New, larger and more diverse benchmark datasets are necessary.  Here, we introduce a new comprehensive evaluation framework to measure the effectiveness of scientific paper embeddings, which we call \dataset. The framework consists of diverse tasks, ranging from citation prediction, to prediction of user activity, to document classification and paper recommendation. 
Note that \sys will not be further fine-tuned on any of the tasks; we simply plug in the embeddings as features for each task.
Below, we describe each of the tasks in detail and the evaluation data associated with it. In addition to our training data, we release all the datasets associated with the evaluation tasks. 

\subsection{Document Classification}
An important test of a document-level embedding is whether it is predictive of the class of the document.  Here, we consider two classification tasks in the scientific domain:

\paragraph{MeSH Classification \hspace{1em}}
In this task, the goals is to classify scientific papers according to their Medical Subject Headings (MeSH) \cite{lipscomb2000medical}.\footnote{\url{https://www.nlm.nih.gov/mesh/meshhome.html}} We construct a dataset consisting of 23K academic medical papers, where each paper is assigned one of 11 top-level disease classes such as cardiovascular diseases, diabetes, digestive diseases derived from the MeSH vocabulary.
The most populated category is Neoplasms (cancer) with 5.4K instances (23.3\% of the total dataset) while the category with least number of samples is Hepatitis (1.7\% of the total dataset). We follow the approach of \citet{feldman2019medical} in mapping the MeSH vocabulary to the disease classes. 

\paragraph{Paper Topic Classification  \hspace{1em}}
This task is predicting the topic associated with a paper using the predefined topic categories of the Microsoft Academic Graph (MAG)~\cite{Sinha2015AnOO}\footnote{\url{https://academic.microsoft.com/}}. MAG provides a database of papers, each tagged with a list of topics. The topics are organized in a hierarchy of 5 levels, where level 1 is the most general and level 5 is the most specific. For our evaluation, we derive a document classification dataset from the level 1 topics, where a paper is labeled by its corresponding level 1 MAG topic.
We construct a dataset of 25K papers, almost evenly split over the 19 different classes of level 1 categories in MAG.


\subsection{Citation Prediction}

As argued above, citations are a key signal of relatedness between papers.  We test how well different paper representations can reproduce this signal through citation prediction tasks. In particular, we focus on two sub-tasks: predicting \textit{direct citations}, and predicting \textit{co-citations}.
We frame these as ranking tasks and evaluate performance using \map and \ndcg, standard ranking metrics. 

\paragraph{Direct Citations} 
In this task, the model is asked to predict which papers are cited by a given query paper from a given set of candidate papers. The evaluation dataset includes approximately 30K total papers from a held-out pool of papers, consisting of 1K query papers and a candidate set of up to 5 cited papers and 25 (randomly selected) uncited papers. The task is to rank the cited papers higher than the uncited papers. For each embedding method, we require only comparing the L2 distance between the raw embeddings of the query and the candidates, without any additional trainable parameters. 

\paragraph{Co-Citations}
This task is similar to the \textit{direct citations} but instead of predicting a cited paper, the goal is to predict a highly co-cited paper with a given paper. Intuitively, if papers A and B are cited frequently together by several papers, this shows that the papers are likely highly related and a good paper representation model should be able to identify these papers from a given candidate set. The dataset consists of 30K total papers and is constructed similar to the \textit{direct citations} task.

\subsection{User Activity}
The embeddings for similar papers should be close to each other; 
we use user activity as a proxy for identifying similar papers and test the model's ability to recover this information. 
Multiple users consuming the same items as one another is a classic relatedness signal and forms the foundation for recommender systems and other applications \cite{schafer2007collaborative}. In our case, we would expect that when users look for academic papers, the papers they view in a single browsing session tend to be related.  Thus, accurate paper embeddings should, all else being equal, be relatively more similar for papers that are frequently viewed in the same session than for other papers.  
To build benchmark datasets to test embeddings on user activity, we obtained logs of user sessions from a major academic search engine. We define the following two tasks on which we build benchmark datasets to test embeddings:

\paragraph{Co-Views}
Our co-views dataset consists of approximately 30K papers.  To construct it, we take 1K random papers that are not in our train or development set and associate with each one up to 5 frequently co-viewed papers and 25 randomly selected papers (similar to the approach for citations). Then, we require the embedding model to rank the co-viewed papers higher than the random papers by comparing the L2 distances of raw embeddings.  We evaluate performance using standard ranking metrics, \ndcg and \map.

\paragraph{Co-Reads}
If the user clicks to access the PDF of a paper from the paper description page, this is a potentially stronger sign of interest in the paper.  In such a case we assume the user will read at least parts of the paper and refer to this as a ``read'' action.  Accordingly, we define a ``co-reads'' task and dataset analogous to the co-views dataset described above. This dataset is also approximately 30K papers.



\subsection{Recommendation}

In the recommendation task, we evaluate the ability of paper embeddings to boost performance in a production recommendation system.  Our recommendation task aims to help users navigate the scientific literature by ranking a set of ``similar papers'' for a given paper.  
We use a dataset of user clickthrough data for this task which consists of 22K clickthrough events from a public scholarly search engine. We partitioned the examples temporally into train (20K examples), validation (1K), and test (1K) sets.  As is typical in clickthrough data on ranked lists, the clicks are biased toward the top of original ranking presented to the user.  To counteract this effect, we computed propensity scores using a swap experiment \cite{Agarwal2019EstimatingPB}.  The propensity scores give, for each position in the ranked list, the relative frequency that the position is over-represented in the data due to exposure bias.  We can then compute de-biased evaluation metrics by dividing the score for each test example by the propensity score for the clicked position.  We report propensity-adjusted  versions of the standard ranking metrics Precision@1 () and Normalized Discounted Cumulative Gain ().

We test different embeddings on the recommendation task by including cosine embedding distance\footnote{Embeddings are L2 normalized and in this case cosine distance is equivalent to L2 distance.} as a feature within an existing recommendation system that includes several other informative features (title/author similarity, reference and citation overlap, etc.). Thus, the recommendation experiments measure whether the embeddings can boost the performance of a strong baseline system on an end task.  For \sys , we also perform an online A/B test to measure whether its advantages on the offline dataset translate into improvements on the online recommendation task (\S\ref{sec:results}).

\section{Experiments}
\label{sec:exp-details}

\setlength{\dashlinedash}{0.2pt}
\setlength{\dashlinegap}{1pt}
\setlength{\arrayrulewidth}{0.4pt}

\paragraph{Training Data}

To train our model, we use a subset of the Semantic Scholar corpus \cite{Ammar2018ConstructionOT} consisting of about 146K query papers (around 26.7M tokens) with their corresponding outgoing citations, and we use an additional 32K papers for validation.
For each query paper we construct up to 5 training triples comprised of a query, a positive, and a negative paper. The positive papers are sampled from the direct citations of the query, while negative papers are chosen either randomly or from citations of citations (as discussed in \S\ref{subsec:negative-sampling}). We empirically found it helpful to use 2 hard negatives (citations of citations) and 3 easy negatives (randomly selected papers) for each query paper. This process results in about 684K training triples and 145K validation triples.

\paragraph{Training and Implementation}
We implement our model in AllenNLP \cite{gardner-etal-2018-allennlp}. We initialize the model from SciBERT pretrained weights \cite{Beltagy2019SciBERT} since it is the state-of-the-art pretrained language model on scientific text. We continue training all model parameters on our training objective (Equation \ref{eq:loss}). We perform minimal tuning of our model's hyperparameters based on the performance on the validation set, while baselines are extensively tuned. Based on initial experiments, we use a margin  for the triplet loss. For training, we use the Adam optimizer \cite{Kingma2014AdamAM} following the suggested hyperparameters in \citet{Devlin2018BERTPO} (LR: 2e-5, Slanted Triangular LR scheduler\footnote{Learning rate linear warmup followed by linear decay.}~\cite{howard-ruder-2018-universal} with number of train steps equal to training instances and cut fraction of 0.1). We train the model on a single Titan V GPU (12G memory) for 2 epochs, with batch size of 4 (the maximum that fit in our GPU memory) and use gradient accumulation for an effective batch size of 32. Each training epoch takes approximately 1-2 days to complete on the full dataset. We release our code and data to facilitate reproducibility.
\footnote{\url{https://github.com/allenai/specter}}

\paragraph{Task-Specific Model Details}
For the classification tasks, we used a linear SVM  where embedding vectors were the only features. The  hyper-parameter was tuned via a held-out validation set.  For the recommendation tasks, we use a feed-forward ranking neural network that takes as input ten features designed to capture the similarity between each query and candidate paper, including the cosine similarity between the query and candidate embeddings and manually-designed features computed from the papers' citations, titles, authors, and publication dates.



\begin{table*}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{4.9pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}lrrrrrrrrrrrrr@{}}
\toprule
Task                  & \multicolumn{2}{c}{Classification} & \multicolumn{4}{c}{User activity prediction}                        & \multicolumn{4}{c}{Citation prediction}                & \multicolumn{2}{c}{\multirow{2}{*}{\begin{tabular}{@{}c@{}}Recomm.\end{tabular}}} & 
\multirow{3}{*}{\begin{tabular}{@{}c@{}}Avg.\end{tabular}}
\\ \cmidrule(lr){2-3} \cmidrule(lr){4-7} \cmidrule(lr){8-11} 
Subtask               & MAG              & MeSH            & \multicolumn{2}{c}{Co-View} & \multicolumn{2}{c}{Co-Read} & \multicolumn{2}{c}{Cite} & \multicolumn{2}{c}{Co-Cite} & \multicolumn{2}{c}{} &                   \\ \cdashline{1-13}
Model  / Metric                & F1               & F1              & \map        & \ndcg        & \map         & \ndcg        & \map       & \ndcg       & \map         & \ndcg        &         &        &  \\ \midrule
Random                             & 4.8  & 9.4  & 25.2 & 51.6 & 25.6 & 51.9 & 25.1 & 51.5 & 24.9 & 51.4 & 51.3 & 16.8 & 32.5 \\ 
Doc2vec \citeyearpar{doc2vec}      & 66.2 & 69.2 & 67.8 & 82.9 & 64.9 & 81.6 & 65.3 & 82.2 & 67.1 & 83.4 & 51.7 & 16.9 & 66.6 \\
Fasttext-sum \citeyearpar{fasttext}        & 78.1 & 84.1 & 76.5 & 87.9 & 75.3 & 87.4 & 74.6 & 88.1 & 77.8 & 89.6 & 52.5 & 18.0 & 74.1 \\
SIF \citeyearpar{sif}                  & 78.4 & 81.4 & 79.4 & 89.4 & 78.2 & 88.9 & 79.4 & 90.5 & 80.8 & 90.9 & 53.4 & 19.5 & 75.9 \\
ELMo \citeyearpar{elmo}                 & 77.0 & 75.7 & 70.3 & 84.3 & 67.4 & 82.6 & 65.8 & 82.6 & 68.5 & 83.8 & 52.5 & 18.2 & 69.0 \\
Citeomatic \citeyearpar{citeomatic}           & 67.1 & 75.7 & 81.1 & 90.2 & 80.5 & 90.2 & 86.3 & 94.1 & 84.4 & 92.8 & 52.5 & 17.3 & 76.0 \\ 
SGC \citeyearpar{sgc}               & 76.8 & 82.7 & 77.2 & 88.0 & 75.7 & 87.5 & \bf{91.6} & \bf{96.2} & 84.1 & 92.5 & 52.7 & 18.2 & 76.9 \\
SciBERT \citeyearpar{Beltagy2019SciBERT} & 79.7 & 80.7 & 50.7 & 73.1 & 47.7 & 71.1 & 48.3 & 71.7 & 49.7 & 72.6 & 52.1 & 17.9 & 59.6 \\ 
Sent-BERT \citeyearpar{sentence_bert}           & 80.5 & 69.1 & 68.2 & 83.3 & 64.8 & 81.3 & 63.5 & 81.6 & 66.4 & 82.8 & 51.6 & 17.1 & 67.5 \\ \hdashline
\sys (Ours)          &  \bf{82.0} &	\bf{86.4} & \bf{83.6} & \bf{91.5} & \bf{84.5} & \bf{92.4} & 88.3 &	94.9 & \bf{88.1} & \bf{94.8} & \bf{53.9}& \bf{20.0} & \bf{80.0} \\
\bottomrule
\end{tabular}
\caption{Results on the \dataset evaluation suite consisting of 7 tasks. 
}
\label{tab:results}
\end{table*}


\paragraph{Baseline Methods}

Our work falls into the intersection of textual representation, citation mining, and graph learning, and we evaluate against state-of-the-art baselines from each of these areas. 
We compare with several strong textual models: SIF \cite{sif}, a method for learning document representations by removing the first principal component of aggregated word-level embeddings which we pretrain on scientific text; SciBERT \cite{Beltagy2019SciBERT} a state-of-the-art pretrained Transformer LM for scientific text; and Sent-BERT \cite{sentence_bert}, a model that uses negative sampling to tune BERT for producing optimal sentence embeddings. We also compare with Citeomatic \cite{citeomatic}, a closely related paper representation model for citation prediction which trains content-based representations with citation graph information via dynamically sampled triplets, and SGC \cite{sgc}, a state-of-the-art graph-convolutional approach.
For completeness, additional baselines are also included; due to space constraints we refer to Appendix \ref{appendix:a} for detailed discussion of all baselines. We tune hyperparameters of baselines to maximize performance on a separate validation set.



