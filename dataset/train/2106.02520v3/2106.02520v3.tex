

\section{Experiments}
\subsection{Implementation Details}\label{sec:4.1}
For backbone feature extractor, we use ResNet-101~\cite{he2016deep} pre-trained on ImageNet~\cite{deng2009imagenet}, and following~\cite{min2019hyperpixel}, extract the features from the best subset layers. Other backbone features can also be used, which we analyze the effect of various backbone features in the following ablation study. For the hyper-parameters for Transformer encoder, we set the depth as 1 and the number of heads as 6. We resize the spatial size of the input image pairs to 256256 and a sequence of selected features are resized to 1616. We use a learnable positional embedding~\cite{dosovitskiy2020image}, instead of fixed~\cite{vaswani2017attention}. We implemented our network using PyTorch~\cite{paszke2017automatic}, and AdamW~\cite{loshchilov2017decoupled} optimizer with an initial learning rate of 3e5 for the CATs layers and 3e6 for the backbone features are used, which we gradually decrease during training. 
\begin{table}[!t]
    \caption{\textbf{Quantitative evaluation on standard benchmarks~\cite{min2019spair,ham2016proposal,ham2017proposal}.} Higher PCK is better. The best results are in bold, and the second best results are underlined. CATs means CATs without fine-tuning feature backbone.~\emph{Feat.-level: Feature-level, FT. feat.: Fine-tune feature.} 
    }\label{tab:main_table}\vspace{-5pt}
    
    \begin{center}
    
    \scalebox{0.8}{
    \begin{tabular}{l|c|c|c|c|ccc|ccc}
            \hlinewd{0.8pt}
             \multirow{3}{*}{Methods}&\multirow{3}{*}{Feat.-level} &\multirow{3}{*}{FT. feat.} & \multirow{3}{*}{Aggregation}& SPair-71k~\cite{min2019spair} & \multicolumn{3}{c|}{PF-PASCAL~\cite{ham2017proposal}} & \multicolumn{3}{c}{PF-WILLOW~\cite{ham2016proposal}}  \\
          
              &&& &PCK @  & \multicolumn{3}{c|}{PCK @ }  & \multicolumn{3}{c}{PCK @ }   \\ 
        
              & & & &0.1 & 0.05 & 0.1 & 0.15 & 0.05 & 0.1 & 0.15  \\ 
             \midrule
             WTA &Single &\xmark&-& 25.7&35.2&53.3&62.8&24.7&46.9&59.0\\\midrule
             CNNGeo~\cite{rocco2017convolutional} &Single&\xmark&- &20.6 &41.0 &69.5 &80.4 &36.9 &69.2 &77.8\\
             A2Net~\cite{paul2018attentive} &Single&\xmark&- &22.3 &42.8 &70.8 &83.3 &36.3 &68.8 &84.4\\
              WeakAlign~\cite{rocco2018end} &Single&\xmark&- &20.9 &49.0 &74.8 &84.0 &37.0 &70.2 &79.9\\
              RTNs~\cite{kim2018recurrent} &Single &\xmark & - &25.7 &55.2 &75.9 &85.2 &41.3 &71.9 &86.2\\ 
              SFNet~\cite{lee2019sfnet} &Multi &\xmark & - &- &53.6 &81.9 &90.6 &46.3 &74.0 &84.2\\
            
             \midrule
             NC-Net~\cite{rocco2018neighbourhood} &Single &\cmark &4D Conv. &20.1 &54.3 &78.9 &86.0 &33.8 &67.0 &83.7\\


             DCC-Net~\cite{huang2019dynamic} &Single &\xmark &4D Conv. &- &55.6 &82.3 &90.5 &43.6 &73.8 &86.5\\
             
             HPF~\cite{min2019hyperpixel} &Multi &- &RHM&28.2 &60.1 &84.8 &92.7 &45.9 &74.4 &85.6\\
             GSF~\cite{jeon2020guided} &Multi &\xmark &2D Conv. &36.1&65.6 &87.8 &95.9 &49.1 &78.7 &\underline{90.2}\\
             ANC-Net~\cite{li2020correspondence} &Single &\xmark &4D Conv. &- &- &86.1 &- &- &- &-\\
             DHPF~\cite{min2020learning} &Multi &\xmark&RHM &37.3 &\underline{75.7} &90.7 &\underline{95.0} &49.5 &77.6 &89.1 \\
             SCOT~\cite{liu2020semantic} &Multi &- &OT-RHM &35.6 &63.1 &85.4 &92.7 &47.8 &76.0 &87.1\\
             CHM~\cite{min2021convolutional} &Single &\xmark & 6D Conv. &\underline{46.3} &\textbf{80.1} &\underline{91.6} &94.9 &\textbf{52.7} &\textbf{79.4} &87.5\\\midrule
             CATs &Multi &\xmark &Transformer &42.4 &67.5 &89.1 &94.9 &46.6 &75.6 &87.5\\
             CATs &Multi &\cmark &Transformer&\textbf{49.9} &75.4 &\textbf{92.6} &\textbf{96.4} &\underline{50.3} &\underline{79.2} &\textbf{90.3}\\
            \hlinewd{0.8pt}
    \end{tabular}
    }
    \end{center}
\end{table}

\begin{table}[!t]
    
    \caption{\label{tab:hpftable} \textbf{Per-class quantitative evaluation on SPair-71k~\cite{min2019spair} benchmark.}}\vspace{-10pt}
    
    \begin{center}
        \scalebox{0.61}{
        \begin{tabular}{l|cccccccccccccccccc|c}
        \hlinewd{0.8pt}
         Methods & aero. & bike & bird & boat & bott. & bus & car & cat & chai. & cow & dog & hors. & mbik. & pers. & plan. & shee. & trai. & tv & all\\
        \midrule
        
       
        CNNGeo~\cite{rocco2017convolutional} &  23.4 & 16.7 & 40.2 & 14.3 & 36.4 & 27.7 & 26.0 & 32.7 & 12.7 & 27.4 & 22.8 & 13.7 & 20.9 & 21.0 & 17.5 & 10.2 & 30.8 & 34.1 & 20.6  \\
      
        A2Net~\cite{paul2018attentive} & 22.6 & {18.5} & 42.0 & {16.4} & {37.9} & {30.8} & {26.5} & 35.6 & 13.3 & 29.6 & 24.3 & 16.0 & 21.6 & {22.8} & {20.5} & 13.5 & 31.4 & {36.5} & 22.3 \\
      
       WeakAlign~\cite{rocco2018end} &  22.2 & 17.6 & 41.9 & {15.1} & {38.1} & {27.4} & {27.2} & 31.8 & 12.8 & 26.8 & 22.6 & 14.2 & 20.0 & 22.2 & 17.9 & 10.4 & {32.2} & 35.1 & 20.9 \\
    
       NC-Net~\cite{rocco2018neighbourhood} & 17.9 & 12.2 & 32.1 & 11.7 & 29.0 & 19.9 & 16.1 & 39.2 & 9.9 & 23.9 & 18.8 & 15.7 & 17.4 & 15.9 & 14.8 & 9.6 & 24.2 & 31.1 & 20.1   \\\-2.3ex]
        
        {SCOT}~\cite{liu2020semantic} & {34.9} & {20.7} & {63.8} & {21.1} & {43.5} & {27.3} & {21.3} & {63.1} & {20.0} & {42.9} & {42.5} & {31.1} & {29.8} & {35.0} & {27.7} & {24.4} & {48.4} & {40.8} & {35.6} \\
       
       {DHPF}~\cite{min2020learning} & {38.4} & {23.8} & {68.3} & {18.9} & {42.6} & {27.9} & {20.1} & {61.6} & {22.0} & {46.9} & {46.1} & {33.5} & {27.6} & {40.1} & {27.6} & {28.1} & {49.5} & {46.5} & {37.3} \\
        
        
        {CHM}~\cite{min2021convolutional} & \underline{49.6} & \underline{29.3} & {68.7} & \underline{29.7} & \underline{45.3} & \underline{48.4} & \underline{39.5} & {64.9} & \underline{20.3} & \underline{60.5} & \underline{56.1} & {46.0} & \underline{33.8} & \underline{44.3} & \underline{38.9} & \underline{31.4} & \underline{72.2} & \underline{55.5} & \underline{46.3} \\\midrule
        CATs & {46.5} & {26.9} & \underline{69.1} & {24.3} & {44.3} & {38.5} & {30.2} & \underline{65.7} & {15.9} & {53.7} & {52.2} & \underline{46.7} & {32.7} & {35.2} & {32.2} & {31.2} & {68.0} & {49.1} & {42.4} \\
        CATs  & \textbf{52.0} & \textbf{34.7} & \textbf{72.2} & \textbf{34.3} & \textbf{49.9} & \textbf{57.5} & \textbf{43.6} & \textbf{66.5} & \textbf{24.4} & \textbf{63.2} & \textbf{56.5} & \textbf{52.0} & \textbf{42.6} & \textbf{41.7} & \textbf{43.0} & \textbf{33.6} & \textbf{72.6} & \textbf{58.0} & \textbf{49.9} \\
        \hlinewd{0.8pt}
        
        \end{tabular}}
    \end{center}\vspace{-10pt}
\end{table}

\subsection{Experimental Settings}
In this section, we conduct comprehensive experiments for semantic corrspondence, by evaluating our approach through comparisons to state-of-the-art methods including CNNGeo~\cite{rocco2017convolutional}, A2Net~\cite{paul2018attentive}, WeakAlign~\cite{rocco2018end}, NC-Net~\cite{rocco2018neighbourhood},
RTNs~\cite{kim2018recurrent}, SFNet~\cite{lee2019sfnet}, HPF~\cite{min2019hyperpixel}, DCC-Net~\cite{huang2019dynamic}, ANC-Net~\cite{li2020correspondence},  DHPF~\cite{min2020learning}, SCOT~\cite{liu2020semantic}, GSF~\cite{jeon2020guided}, and CHMNet~\cite{min2021convolutional}. In Section 4.3, we first evaluate matching results on several benchmarks with quantitative measures, and then provide an analysis of each component in our framework in Section 4.4. For more implementation details, please refer to our implementation available at ~\url{https://github.com/SunghwanHong/CATs}. \vspace{-5pt}

\paragraph{Datasets.}
SPair-71k~\cite{min2019spair} provides total 70,958 image pairs with extreme and diverse viewpoint, scale variations, and rich annotations for each image pair, e.g., keypoints, scale difference, truncation and occlusion difference, and clear data split. Previously, for semantic matching, most of the datasets are limited to a small quantity with similar viewpoints and scales~\cite{ham2016proposal,ham2017proposal}. As our network relies on Transformer which requires a large number of data for training, SPair-71k~\cite{min2019spair} makes the use of Transformer in our model feasible. 
we also consider PF-PASCAL~\cite{ham2017proposal} containing 1,351 image pairs from 20 categories and PF-WILLOW~\cite{ham2016proposal} containing 900 image pairs from 4 categories, each dataset providing corresponding ground-truth annotations. \vspace{-5pt}

\paragraph{Evaluation Metric.}
For evaluation on SPair-71k~\cite{min2019spair}, PF-WILLOW~\cite{ham2016proposal}, and PF-PASCAL~\cite{ham2017proposal}, we employ a percentage of correct keypoints (PCK), computed as the ratio of estimated keypoints within the threshold from ground-truths to the total number of keypoints. Given predicted keypoint  and ground-truth keypoint , we count the number of predicted keypoints that satisfy following condition: , where  denotes Euclidean distance;  denotes a threshold which we evaluate on PF-PASCAL with , SPair-71k and PF-WILLOW with ;  and  denote height and width of the object bounding box or entire image, respectively. 

\subsection{Matching Results}\label{sec:4.3}
For a fair comparison, we follow the evaluation protocol of~\cite{min2019hyperpixel} for SPair-71k, which our network is trained on the training split and evaluated on the test split. Similarly, for PF-PASCAL and PF-WILLOW, following the common evaluation protocol of~\cite{han2017scnet,kim2018recurrent,huang2019dynamic,min2019hyperpixel,min2020learning}, we train our network on the training split of PF-PASCAL~\cite{ham2017proposal} and then evaluate on the test split of PF-PASCAL~\cite{ham2017proposal} and PF-WILLOW~\cite{ham2016proposal}. All the results of other methods are reported under identical setting.

Table~\ref{tab:main_table} summarizes quantitative results on SPair-71k~\cite{min2019spair}, PF-PASCAL~\cite{ham2017proposal} and PF-WILLOW~\cite{ham2016proposal}. We note whether each method leverages multi-level features and fine-tunes the backbone features in order to ensure a fair comparison. We additionally denote the types of cost aggregation. Generally, our CATs outperform other methods over all the benchmarks. This is also confirmed by the results on SPair-71k, as shown in Table~\ref{tab:hpftable}, where the proposed method outperforms other methods by large margin. Note that CATs reports lower PCK than that of CHM, and this is because CHM fine-tunes its backbone networks while CATs does not.  \figref{fig:spair_quali} visualizes qualitative results for extremely challenging image pairs. We observe that compared to current state-of-the-art methods~\cite{liu2020semantic,min2020learning}, our method is capable of suppressing noisy scores and find accurate correspondences in cases with large scale and geometric variations. 

It is notable that CATs generally report lower PCK on PF-WILLOW~\cite{ham2016proposal} compared to other state-of-the-art methods. This is because the Transformer is well known for lacking some of inductive bias. When we evaluate on PF-WILLOW, we infer with the model trained on the training split of PF-PASCAL, which only contains 1,351 image pairs, and as only relatively small quantity of image pairs is available within the PF-PASCAL training split, the Transformer shows low generalization power. This demonstrates that the Transformer-based architecture indeed requires a means to compensate for the lack of inductive bias, e.g., data augmentation. 

\begin{figure}[!t]
    \centering
    \renewcommand{\thesubfigure}{}
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/scot_qualitative1.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/scot_qualitative2.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/scot_qualitative3.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/scot_qualitative4.png}}\hfill\\
	\vspace{-20.5pt}
	\subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/dhpf_qualitative1.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/dhpf_qualitative2.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/dhpf_qualitative3.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/dhpf_qualitative4.png}}\hfill\\
	\vspace{-20.5pt}
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/qualitative1.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/qualitative2.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/qualitative3.png}}\hfill
    \subfigure[]
	{\includegraphics[width=0.247\linewidth]{figure/img/qualitative4.png}}\hfill\\
	\vspace{-10pt}
    \caption{\textbf{Qualitative results on SPair-71k~\cite{min2019spair}:} (from top to bottom) keypoints transfer results by SCOT~\cite{liu2020semantic}, DHPF~\cite{min2020learning}, and CATs. Note that green and red line denotes correct and wrong prediction, respectively, with respect to the ground-truth.}\label{fig:spair_quali}\vspace{-10pt}
  
\end{figure}

\subsection{Ablation Study}\label{sec:4.4}
In this section we show an ablation analysis to validate critical components we made to design our architecture, and provide an analysis on use of different backbone features, and data augmentation. We train all the variants on the training split of SPair-71k~\cite{min2019spair} when evaluating on SPair-71k, and train on PF-PASCAL~\cite{ham2017proposal} for evaluating on PF-PASCAL. We measure the PCK, and each ablation experiment is conducted under same experimental setting for a fair comparison. \vspace{-5pt}

\paragraph{Network Architecture.}
Table~\ref{tab:architecture} shows the analysis on key components in our architecture. There are four key components we analyze for the ablation study, including appearance modelling, multi-level aggregation, swapping self-attention, and residual connection. 

\begin{wraptable}{r}{5.8cm}
\caption{\textbf{Ablation study of CATs.}}
\label{tab:architecture}\vspace{+5pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{cl|c}
        \hlinewd{0.8pt}
        &\multirow{2}{*}{Components} &SPair-71k \\
        && = 0.1 \\
        \midrule
        \textbf{(I)} &Baseline  &26.8 \\
        \textbf{(II)} &+ Appearance Modelling  &33.5 \\
        \textbf{(III)} &+ Multi-level Aggregation &35.9 \\
        \textbf{(IV)} &+ Swapping Self-Attention &38.8 \\
        \textbf{(V)} &+ Residual Connection &42.4 \\
        \hlinewd{0.8pt}
\end{tabular}}
\end{wraptable}
We first define the model without any of these as baseline, which simply feeds the correlation map into the self-attention layer. We evaluate on SPair-71k benchmark by progressively adding the each key component. From \textbf{I} to \textbf{V}, we observe consistent increase in performance when each component is added. \textbf{II} shows a large improvement in performance, which demonstrates that the appearance modelling enabled the model to refine the ambiguous or noisy matching scores. Although relatively small increase in PCK for \textbf{III}, it proves that the proposed model successfully aggregates the multi-level correlation maps. Furthermore, \textbf{IV} and \textbf{V} show apparent increase, proving the significance of both components. \vspace{-5pt}

\paragraph{Feature Backbone.}
\begin{wraptable}{r}{7cm}
\vspace{-15pt}
\caption{\textbf{Ablation study of feature backbone.}}
\label{tab:feature-backbone}\vspace{+5pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c}
 \hlinewd{0.8pt}
\multirow{2}{*}{Feature Backbone} &SPair-71k &PF-PASCAL\\

 & = 0.1 & = 0.1 \\
               \midrule
DeiT-B~\cite{touvron2020deit}&32.1 &76.5\\
DeiT-B~\cite{touvron2020deit}&38.2 &87.5\\\midrule
DINO w/ ViT-B/16~\cite{caron2021emerging} &39.5 &88.9 \\
DINO w/ ViT-B/16~\cite{caron2021emerging} &42.0 &88.9 \\\midrule
ResNet-101~\cite{he2016deep} & 37.4 &87.3  \\
ResNet-101~\cite{he2016deep} &42.4 &89.1\\
 \hlinewd{0.8pt}
\end{tabular}}
\end{wraptable}
As shown in Table~\ref{tab:feature-backbone}, we explore the impact of different feature backbones on the performance on SPair-71k~\cite{min2019spair} and PF-PASCAL~\cite{ham2017proposal}. We report the results of models with backbone networks frozen. The top two rows are models with DeiT-B~\cite{touvron2020deit}, next two rows use DINO~\cite{caron2021emerging}, and the rest use ResNet-101~\cite{he2016deep} as backbone. Specifically, subscript \texttt{single} for DeiT-B and DINO, we use the feature map extracted at the last layer for the single-level, while for subscript \texttt{all}, every feature map from 12 layers is used for cost construction. For ResNet-101 subscript \texttt{single}, we use a single-level feature cropped at , while for \texttt{multi}, we use the best layer subset provided by~\cite{min2019hyperpixel}. Summarizing the results, we observed that leveraging multi-level features showed apparent improvements in performance, proving effectiveness of multi-level aggregation introduced by our method. It is worth noting that DINO, which is more excel at dense tasks than DeiT-B, outperforms DeiT-B when applied to semantic matching. This indicates that fine-tuning the feature could enhance the performance. To best of our knowledge, we are the first to employ Transformer-based features for semantic matching. It would be an interesting setup to train an end-to-end Transformer-based networks, and we hope this work draws attention from community and made useful for future works. \vspace{-5pt}

\paragraph{Data Augmentation.}
\begin{wraptable}{r}{5cm}
\vspace{-7mm}
\caption{~\textbf{Effects of augmentation.}}
\label{tab:aug}\vspace{+5pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c}
\toprule
&\multirow{2}{*}{Augment.} &SPair-71k\\

& & = 0.1\\
               \midrule
DHPF~\cite{min2020learning} &\xmark &37.3\\
DHPF~\cite{min2020learning} &\cmark &39.4\\
\midrule
CATs &\xmark &{43.5}\\
CATs &\cmark &{49.9}\\
\bottomrule
\end{tabular}} \\
\centering
\end{wraptable}
In Table~\ref{tab:aug}, we compared the PCK performance between our variants and DHPF~\cite{min2020learning}. We note if the model is trained with augmentation. For a fair comparison, we evaluate both DHPF~\cite{min2020learning} and CATs trained on SPair-71k~\cite{min2019spair} using strong supervision, which assumes that the ground-truth keypoints are given. The results show that compared to DHPF, a CNN-based method, data augmentation has a larger influence on CATs in terms of performance. This demonstrates that not only we eased the data-hunger problem inherent in Transformers, but also found that applying augmentations for matching has positive effects. Augmentation technique would bring a highly likely improvements in performance, and we hope that the future works benefit from this. 

\paragraph{Serial swapping.}
It is apparent that Equation~\ref{eq2} is not designed for an order-invariant output. Different from NC-Net~\cite{rocco2018neighbourhood}, we let the correlation map undergo the self-attention module in a serial manner. We conducted a simple experiment to compare the difference between each approach. From experiments, we obtained the results of parallel and serial processing on SPair-71k with  = 0.1, which are PCK of 40.8 and 42.4, respectively. In light of this, although CATs may not support order invariance, adopting serial processing can obtain higher PCK as it has a better capability to reduce inconsistent matching scores by additionally processing the already processed cost map, which we finalize the architecture to include serial processing.

\begin{figure*}[t]
\centering
\renewcommand{\thesubfigure}{}
\subfigure[]
{\includegraphics[width=0.163\textwidth]{figure/figure6/src.png}}\hfill
\subfigure[]
{\includegraphics[width=0.163\textwidth]{figure/figure6/trg.png}}\hfill
\subfigure[]
{\includegraphics[width=0.163\textwidth]{figure/figure6/level1.png}}\hfill
\subfigure[]
{\includegraphics[width=0.163\textwidth]{figure/figure6/level2.png}}\hfill
\subfigure[]
{\includegraphics[width=0.163\textwidth]{figure/figure6/level3.png}}\hfill
\subfigure[]
{\includegraphics[width=0.163\textwidth]{figure/figure6/level4.png}}\hfill\\
\vspace{-10pt}
\caption{\textbf{Visualization of self-attention:} (from left to right) source and target images, and multi-level self-attentions. Note that each attention map attends different aspects, and CATs aggregates the cost leveraging hierarchical semantic representations. }
\label{fig:attention}\vspace{-10pt}
\end{figure*} 

\subsection{Analysis}
\paragraph{Visualizing Self-Attention.}
We visualize the multi-level attention maps obtained from the Transformer aggregator. 
As shown in~\figref{fig:attention}, the learned self-attention map at each level exhibits different aspect. With these self-attentions, our networks can leverage multi-level correlations to capture hierarchical semantic feature representations effectively. 
\vspace{-5pt}

\paragraph{Memory and run-time.}
\begin{wraptable}{r}{6.8cm}
\vspace{-7mm}
\caption{\textbf{Memory and run-time comparison.} Inference time for aggregator is denoted by .}
\label{tab:runtime-main-paper}\vspace{+5pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|c|c}
\hlinewd{0.8pt}
&Aggregation &Memory [GB] &Run-time [ms]  \\
\midrule
NC-Net~\cite{rocco2018neighbourhood}& 4D Conv.&\textbf{1.2} &193.3\ (166.1)  \\
SCOT~\cite{liu2020semantic}&OT-RHM &4.6&146.5\ (81.6) \\
DHPF~\cite{min2020learning}&RHM &1.6 &57.7\ (\underline{29.5})  \\
CHM~\cite{min2021convolutional}&6D Conv &\underline{1.6}&\underline{47.2}\ (38.3)\\
\midrule
CATs &Transformer &1.9&\textbf{34.5}\ (\textbf{7.4})  \\
\hlinewd{0.8pt}
\end{tabular}} \\
\centering
\end{wraptable}
In Table~\ref{tab:runtime-main-paper}, we show the memory and run-time comparison to NC-Net~\cite{rocco2018neighbourhood}, SCOT~\cite{liu2020semantic}, DHPF~\cite{min2020learning} and CHM~\cite{min2021convolutional} with CATs. For a fair comparison, the results are obtained using a single NVIDIA GeForce RTX 2080 Ti GPU and Intel Core i7-10700 CPU. We measure the inference time for both the process without counting feature extraction, and the whole process. Thanks to Transformers' fast computation nature, compared to other methods, our method is beyond compare. We also find that compared to other cost aggregation methods including 4D, 6D convolutons, OT-RHM and RHM, ours show comparable efficiency in terms of computational cost. Note that NC-Net utilizes a single feature map while other methods utilize multi-level feature maps. We used the standard self-attention module for implementation, but more advanced and efficient transformer~\cite{liu2021swin} architectures could reduce the overall memory consumption.

\subsection{Limitations}
One obvious limitation that CATs possess is that when applying the method to non-corresponding images, the proposed method would still deliver correspondences as it lacks power to ignore pixels that do not have correspondence at all. A straightforward solution would be to consider including a module to account for pixel-wise matching confidence. Another limitation of CATs would be its inability to address a task of finding accurate correspondences given multi-objects or non-corresponding objects. Addressing such challenges would be a promising direction for future work.