\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{notes2bib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[justification=centering]{caption}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\title{Efficient Algorithms for the Closest Pair Problem and Applications}
\author{Sanguthevar~Rajasekaran,
        Sudipta~Pathak\\
Department of CSE, University of Connecticut, Storrs, CT}
        

\date{July 2014}

\begin{document}
\maketitle
\begin{abstract}
The closest pair problem (CPP)  is one of the well studied and fundamental problems in computing. Given a set of points in a metric space, the problem is to identify the pair of closest points. Another closely related problem is the fixed radius nearest neighbors problem (FRNNP). Given a set of points and a radius , the problem is, for every input point , to identify all the other input points that are within a distance of  from . A naive deterministic algorithm can solve these problems in quadratic time. CPP as well as FRNNP play a vital role in computational biology, computational finance, share market analysis, weather prediction, entomology, electro cardiograph, N-body simulations, molecular simulations, etc. As a result, any improvements made in solving CPP and FRNNP will have immediate implications for the solution of numerous problems in these domains. We live in an era of big data and processing these data take large amounts of time. Speeding up data processing algorithms is thus much more essential now than ever before. In this paper we present algorithms for CPP and FRNNP that improve (in theory and/or practice) the best-known algorithms reported in the literature for CPP and FRNNP. These algorithms also improve the best-known algorithms for related applications including time series motif mining and the two locus problem in Genome Wide Association Studies (GWAS).
\end{abstract}

\section{Introduction}
\label{intro}
The closest pair problem (CPP) has a rich history and has been extensively studied. On an input set of  points, the problem is to identify the closest pair of points. A straight forward algorithm for CPP takes quadratic (in ) time. Most of the algorithms proposed in the literature are concerned with the Euclidean space. In the rest of this paper, unless otherwise mentioned, we imply the Euclidean space. In his seminal paper, Rabin proposed a randomized algorithm with an expected run time of  \cite{RAB76} (where the expectation is in the space of all possible outcomes of coin flips made in the algorithm). Rabin's algorithm used the floor function as a basic operation. In 1979, Fortune and Hopcroft presented a deterministic algorithm with a run time of  assuming that the floor operation takes  time  \cite{FH79}. Both of these algorithms assume a constant-dimensional space (and the run times have an exponential dependency on the dimension). The algorithm of Preparata and Shamos for points in 2D is deterministic and runs in  time \cite{PS86}. Yao has proven a lower bound of  on the algebraic decision tree model (for any dimension) \cite{YO91}. This lower bound holds under the assumption that the floor function is not allowed.
The algorithm of Khuller and Matias is also randomized and has an expected linear run time utilizing the floor operation \cite{SY95}. There are two steps in the algorithm. In the first step, the distance between the closest pair of points is estimated within a factor of 3. In the second step, the neighborhood of each point  is explored to identify those points that are within a distance of  from , where  is the estimate that step 1 comes up with. Using this neighborhood information, the correct pair is identified.



One of the major issues with the above algorithms is the fact that their run times are exponentially dependent on the dimension.   For example, the expected run time of \cite{SY95}'s algorithm is  on  points from a -dimensional space. So, even for a moderate value of , the algorithm may be very slow in practice. There are numerous applications for which the dimension is very large. In the following sections, we consider two such application domains.



Time series motif mining (TSMM) is a crucial problem that can be thought of as CPP in a large dimensional space. In one version of the TSMM problem, we are given a sequence  of real numbers and an integer . The goal is to identify two subsequences of  of length  each that are the most similar to each other (from among all pairs of subsequences of length  each). These most similar subsequences are referred to as {\em time series motifs}. Let  be a collection of all the -mers of . (An -mer is nothing but a contiguous subsequence of  of length ). Clearly, the -mers in  can be thought of as points in . As a result, the TSMM problem is the same as CPP in . Any of the above mentioned algorithms can thus be used to solve the TSMM problem. A typical value for  of practical interest is several hundreds (or more). For these values of , the above algorithms (\cite{RAB76},\cite{FH79},\cite{PS86},\cite{SY95}) will take an unacceptable amount of time (because of the exponential dependence on the dimension). Designing an efficient practical and exact algorithm for the TSMM problem remains an ongoing challenge.

Mueen, et al. have presented an elegant exact algorithm called MK for TSMM \cite{AEQSB09}. MK improves the performance of the brute-force algorithm with a novel application of the triangular inequality. MK is currently the best-performing algorithm in practice for TSMM.
A number of probabilistic as well as approximate algorithms are also known for solving this problem (see e.g., \cite{PMPS08,BES03,TCM07,JJMY08,DCIT07,SG04,YKK05}). For instance, the algorithm of \cite{BES03} exploits algorithms proposed for finding -motifs from biological data. The idea here is to partition the time series data into frames of certain width. Followed by this, the mean value in each frame is computed. This mean is quantized into four intervals and as a result, the original time series data is converted into a string of characters from an alphabet of size 4. Finally, any -motif finding algorithm is applied on the transformed string to identify the time series motifs.

An application of great interest in bioinformatics is Genome Wide Association Study (GWAS). A lot of effort has been spent to identify mappings between phenotypical traits and genomic data. Due to the advent of next generation high throughput sequencing technologies, nowadays it is possible to study the genomic structure of individuals in detail. Single Nucleotide Polymorphisms (SNPs) are  positions in the genome where nucleotides vary among individuals \cite{T07}. SNPs in the human genome are considered to be responsible for different phenotypical traits. In GWAS,  two different problems have been focused on. In single locus association study, researchers try to find out the association between phenotypical traits and individual SNPs. In two locus association study, the goal is to figure out the association between pairs of SNPs and phenotypical traits. A major task in this study is that of identifying the most correlated pair of SNPs. Two locus associations are also known as gene-gene interactions.
Such interactions are believed to be major factors responsible for many complex phenotypical traits \cite{SDNRCNHD07,CDDBDADLMDCJE05,JDLCMPMLYJBEJSTS98,H09,NMDPCGA99,JCSEBSAKLDPJW04}.

Given that the number of SNPs found in humans is  to , a brute force way of scanning through every possible pair of SNPs to identify the most correlated pair is not feasible in practice. A number of algorithms for the two locus problem can be found in the literature. For instance, genetic algorithms are used in \cite{NYH01} and \cite{YXKR10}. The algorithms proposed in  \cite{XSFW10} and \cite{XFW08} take  time, where  is the number of SNPs and  is the number of subjects. An algorithm with an expected run time of , where  is a constant, has been presented in \cite{PBK11}. This algorithm exploits an algorithm known for the  Light Bulb Problem \cite{RSJ89} and Locality Sensitive Hashing (LSH) \cite{M02}.

In this paper we present efficient algorithms for CPP. Our algorithms improve the results reported in several papers including \cite{AEQSB09}, \cite{BES03}, and  \cite{PBK11}. Our main contributions can be summarized as follows:
\begin{enumerate}
\item We improve the CPP algorithm of \cite{AEQSB09} by introducing a novel idea. Specifically, \cite{AEQSB09}'s algorithm is based on two basic ideas. We contribute a third idea that results in an improvement of the run time for exact TSMM by a factor of around 1.5. We also show how to extend our algorithm to solve the fixed radius nearest neighbors problem (FRNNP).
\item We present an algorithm for CPP when the domain of interest has strings of characters (from a finite alphabet) and the metric is Hamming distance. It turns out that MK does not perform well for the case of Hamming distance, and to be fair, we note that the authors of MK do not claim it might. A comparison of our algorithm with MK reveals that our algorithm outperforms MK (by a factor of around 200). Our algorithm can also be used in approximate TSMM. Specifically, instead of using -motif search algorithms, our algorithm can be used in \cite{BES03}. In this case, the run time of the algorithm in \cite{BES03} will improve significantly, since exact algorithms for solving the -motif search problem take time that is exponential in  and . Our algorithm is a modified version of the light bulb algorithm of \cite{RSJ89}.
\item The light bulb algorithm of \cite{RSJ89} finds the most correlated pair of bulbs. The light bulb problem can be thought of as CPP in the space of {\bf binary} strings with Hamming distance as the metric. We extend this algorithm when the strings are from an arbitrary (finite) alphabet. More importantly, we present an algorithm for finding the {\bf least} correlated pair of strings (from an arbitrary alphabet). The algorithm of \cite{PBK11} also solves this problem utilizing Locality Sensitive Hashing (LSH) \cite{M02}. Our algorithm does not use LSH. Instead, it uses a novel deterministic mapping function that we have come up with.
\item Using the above algorithm for finding the least correlated pair, we present a novel algorithm for solving the two locus GWAS problem. An experimental comparison reveals that our algorithm is four times faster than the algorithm of \cite{PBK11}. We note here that the authors of \cite{PBK11} use Pearson's correlation coefficient to measure the similarity between a pair of SNPs, whereas we use the complement of the Hamming distance as the measure of similarity.
\end{enumerate}



The rest of this paper is organized as follows. In Section~\ref{sec2} we provide some notations. In Section~\ref{sec3} we present our improved algorithm for CPP (called MPR) and compare it with MK. In this section we also provide an analysis of MK and experimentally compare the performances of MK and MPR. Section~\ref{sec4} deals with the case of character strings and Hamming distance. Specifically, we show how to modify the light bulb algorithm of \cite{RSJ89} to get an algorithm for finding the most correlated pair of strings from an arbitrary finite alphabet. We compare this algorithm with MK experimentally. In Section~\ref{sec5} we present an algorithm for finding the least correlated pair of strings. This algorithm is based on a novel mapping function that we have come up with. Section~\ref{sec6} is devoted to the problem of two locus association in GWAS. In particular, we present a novel algorithm for this problem and compare our algorithm with that of \cite{PBK11}. Some concluding remarks are given in Section~\ref{sec7}.

\section{Notations and Definitions}\label{sec2}
Let  be a sequence of real numbers (or characters from a finite alphabet). An -mer of  is nothing but a subsequence of  of  contiguous elements of . The -mers of  are , for .

If the elements of  are real numbers, then the Euclidean distance between  and , denoted as , is .

If the elements of  are characters from an alphabet , then the Hamming distance between  and , denoted as , is  where  if  and  if  (for any ). A sequence of characters can be thought of as a string of characters, since we can obtain a string from the sequence by concatenating the characters. Thus we'll use the terms `a sequence of characters' and `a string of characters' interchangeably.

Let  and  be two sequences of characters. Also, let the Hamming distance between  and  be . Then, by the {\em number of matches} between  and  we mean . Also, the {\em correlation} between  and  is defined to be .

\section{Time Series Motif Mining Algorithm}\label{sec3}
The input for this problem are a sequence  and an integer . The goal is to find two -mers of   that are the closest to each other (from among all the pairs of -mers of ). A general version of this problem is one where the input consists of  points from  and we want to identify the two closest points.
A straight forward algorithm will compute the distance between every pair of -mers and output the pair with the least distance. Since we can compute the distance between two -mers in  time, this simple algorithm for TSMM will run in a total of  time.

\subsection{MK Algorithm}
The MK algorithm of \cite{AEQSB09} speeds up the brute force method by pruning off a large number of pairs that cannot possibly be the closest. There are two main ideas used in MK.  The first idea in the algorithm is to speedup the computation of distances. Let  and  be any two -mers. To compute the distance between  and , the algorithm keeps adding  for . When the sum exceeds , this pair is immediately dropped (without completing the rest of the distance computation). This technique is known as {\em early abandoning}.

The second idea in MK uses the triangular inequality in a novel way.
Let  and  be any two -mers.  At any stage in the algorithm, we have an upper bound  on the distance between the closest pair of -mers. If  can be inferred to be greater than , then we can drop the pair  from future consideration (since this pair cannot be the closest). Ideally, we would like to calculate  exactly for every pair of -mers  and . But this will take too much time. MK circumvents this problem by {\bf estimating} the distance between  and  via the triangular inequality. In particular, a random reference -mer   is chosen and the distance between each -mer and  is computed.  The -mers are kept in an ascending order of their distances to . From thereon,  is used as a lower bound on . If this lower bound is , then  is dropped from future consideration.

The above algorithm is generalized to employ multiple reference -mers. The use of multiple references speeds up the algorithm.

\subsection{An Analysis of the MK Algorithm and Our New Idea}
In this section we provide an (informal) analysis of the MK algorithm to explain why the algorithm has a very good performance. Specifically, if we choose multiple random reference points, the algorithm achieves a much better run time than having a single reference point. We explain why this is the case.

For ease of understanding consider the 2D Euclidean space. The analysis can be extended to points in . For any two -mers  and , the closer  is to , the better will be our estimate and hence the better will be our chance of dropping  (if  is not the closest pair). It turns out that the quality of the lower bound  is decided by two factors: 1) the angle  and 2) . We illustrate this with an example. Let  and . Consider a reference point  (Figure~\ref{figure1}(a)). Note that  is at a distance of  from . In this case . Also, . Let  be the point we get by keeping the distance between the reference point and  the same, but changing this angle to  (Figure~\ref{figure1}(b)). In this case,   improves to . As another example, if the reference point  lies on the perpendicular bisector of  and , then 

\begin{figure}[h]
\includegraphics[scale=.50]{triangles.eps}
\caption{Reference Points}\label{figure1}
\label{fig:1}       \end{figure}

For any two input points  and , if we pick multiple reference points randomly, then we would expect that at least one of these reference points  will be such that the angle  will be such that  will be `large'. In contrast, if we have only one reference point, for some pairs of points the corresponding angles may be `good', but for a good percentage of the pairs, the angles may not be `good'. (A reference point  is `'good' if  is close to ).

The effect of  on  can be seen with the same examples. Consider the example of Figure~\ref{figure1}(a). Assume that we keep the angle the same but increase  to  and get the reference point  (Figure~\ref{figure1}(c)). In this case,  improves to . Also, in the example of Figure~\ref{figure1}(b), say we keep the angle the same but increase  to  and get the reference point . In this case,  improves to . Of course, if the reference point  lies on the perpendicular between  and , then however large  could be,  will continue to be zero! However, the probability of this happening is low. For a given angle , we can compute the limit of  as  tends to . For instance when the angle is  (Figure~\ref{figure1}(a)), this limit is .

\subsection{Our algorithm}
Our proposed new algorithm indeed exploits the relationship between  and . In particular, we pick a collection  of random reference points and project each of these points out by multiplying each coordinate value of each point by a factor of . For example,  could be . The rest of the algorithm is the same as MK.

A pseudocode for our algorithm, called {\em Motif discovery with Projected Reference points (MPR)},  is given below.

\noindent {\bf Algorithm} {\sf MPR}\\
{\em Input:}  and an integer , where each  is a real number (for ). Input are also  and , where  is the number of references and  is the projection factor.\\
{\em Output:} The two closest -mers of .\\

\vspace{-0.15in}
 \noindent {\bf 1)} Pick  random -mers of  as references; Project these
references by multiplying each element in each -mer
 by . Let these projected references be .\\
 {\bf 2)} Compute the distance between every -mer of  and
 every projected reference -mer.\\
{\bf 3)} Sort the -mers of  with respect to their distances
 to . Let the sorted -mers be
.\\
{\bf 4)} Let ; Let ;\\
{\bf 5) for}  {\bf to}  {\bf do}\\
\hspace*{0.4in} {\bf for}  {\bf to}  {\bf do}\\
\hspace*{0.7in} ;\\
\hspace*{0.7in} {\bf for}  {\bf to}  {\bf do}\\
\hspace*{0.9in} {\bf if}  {\bf then}\\
\hspace*{1.1in} ; {\bf exit};\\
\hspace*{0.7in} {\bf if}  {\bf then exit else}\\
\hspace*{0.9in} Compute ; \\
\hspace*{0.9in} {\bf if}  {\bf then}\\
\hspace*{1.1in} ; ;\\
{\bf 6) Output} .

\noindent{\bf Observation:} Please note that even though the above algorithm has been presented for solving the TSMM problem, it is straight forward to extend it to solve CPP in .

\subsection{An analysis of our algorithm}
In this section we show why our idea of projecting reference points improves the performance of the algorithm. Let the input points be from  for some integer . Consider any two input points  and . Let  be any reference point. Note that any three points are coplanar. Consider any hyperplane  containing  and . If we multiply every coordinate of  by the same number, then the resultant point will also lie in . This is because the equation defining  will be of the form . Thus, in order to see how  changes with a scaling of , it suffices to consider the case that these three points are in 2D.

Without loss of generality let  be  and  be , for some real number . There are two cases to consider for the position of  relative to  and : 1)  is to the right of the perpendicular bisector of  and ; 2)  is to the left of the perpendicular bisector between  and . These two cases are illustrated in Figures~\ref{fig2}(a) and \ref{fig2}(b), respectively. Note that when  lies on the perpendicular bisector of  and ,  will be zero.

\begin{figure}[h]
\includegraphics[scale=.50]{fig5.eps}
\caption{The effect of scaling on reference points}\label{fig2}      \end{figure}

In case 1, let  be ,  being a scaling factor. , , and . As a result,  . Using the fact that  (when ), . Thus, . Clearly, when  and  are the same, the value of  increases when  increases.

In case 2, let  be , for a scaling factor of . Clearly, , . Thus, . Also, . Using the approximation mentioned in case 1, we see that . Therefore, . In this case, when  and  are the same, the value of  increases when  decreases.

But for a given reference point, and two input points  and , we do not know which of the two cases will hold. But we can expect that half of the randomly chosen reference points will fall under case 1 and the other half will be expected to fall under case 2. If we only employ a scaling factor  that is greater than one, then, for an expected half of the reference points we expect to see an improvement in the estimate of a lower bound for . This explains why our algorithm performs better than MK.

The above analysis can also be used to better understand the MK algorithm.

\subsection{Fixed radius nearest neighbors problem (FRNNP)}
In this problem we are given  points  in  and a radius  (which is a real number) and the problem is to identify the -neighborhood of each input point. If  is an input point, its -neighborhood is defined to be the set of all input points that are within a distance of  from . FRNNP has numerous applications. One of the applications of vital importance is that of molecular simulations.

We can modify {\sf MPR} to solve this problem as well. The modified version is given below. Let  denote the -neighborhood of .

\noindent {\bf Algorithm} {\sf MPR-FRNNS}\\
{\em Input:}  and , where each  is a point in  (for ) and  is a real number. Input are also  and , where  is the number of references and  is the projection factor.\\
{\em Output:} , for .\\

\vspace{-0.15in}
\noindent {\bf 1)} Pick  random points of  as references; Project these \\
references by multiplying each coordinate of each reference\\
point by . Let these projected references be .\\
 {\bf 2)} Compute the distance between every point of  and \\
 every projected reference point.\\
{\bf 3)} Sort the points of  with respect to their distances\\
 to . Let the sorted points be
.\\
{\bf 4) for}  {\bf to}  {\bf do} ;\\
{\bf 5) for}  {\bf to}  {\bf do}\\
\hspace*{0.4in} {\bf for}  {\bf to}  {\bf do}\\
\hspace*{0.7in} ;\\
\hspace*{0.7in} {\bf for}  {\bf to}  {\bf do}\\
\hspace*{0.9in} {\bf if}  {\bf then}\\
\hspace*{1.1in} ; {\bf exit};\\
\hspace*{0.7in} {\bf if}  {\bf then exit else}\\
\hspace*{0.9in} Compute ; \\
\hspace*{0.9in} {\bf if}  {\bf then} add  to  and \\
\hspace*{0.9in} add  to ;\\
{\bf 6) Output}  for .

\subsection{An experimental comparison of MK and MPR}
A typical algorithm in the literature for CPP has two phases. In the first phase pairs of points that cannot possibly be the closest are eliminated. In the second phase distance is computed between every pair of points that survive the first phase. The time spent in the first phase is typically very small and hence is negligible (compared to the time spent in the second phase). Also, the time needed to process the pairs in the second phase is linear in the number of surviving pairs. As a result, it suffices to report the number of pairs (to be processed in the second phase) as a measure of performance (see e.g., \cite{PBK11}). In this paper also we use this measure of performance throughout.

We have experimentally compared the performance of MK and MPR on different data sets. The machine we have used has an
Intel(R) Core(TM) i7-2640M 2.8 GHz CPU with 8GB RAM running Windows 7 (64 bit). The same machine has been used for all the experiments reported in this paper.

As mentioned in \cite{AEQSB09}, random walk data set is the most difficult case for time series mining algorithms since the probability of the existence of very close motif pairs is very low.
We have also used the same data for our comparison.
In particular, we have used 10 different random walk data sets of sizes ranging from 10K to 100K. We have also varied the motif length to see how the performances change. Our algorithm performs better than MK for higher motif lengths. Both the algorithms have been run 10 times and the averages computed. We do not perform any comparison with the brute force  method as that has already been done in \cite{AEQSB09}.\newline

\begin{table}[ht]
\centering  \scalebox{0.6}{
\begin{tabular}{|c|c|c|c|c|} \hline
& & & &\\                        Dataset Size & Pairs (MK) & Runtime in sec(MK) & Pairs (MPR) & Runtime in sec(MPR)\\ [0.2ex] \hline                  & & & &\\
 &  &  &  & \\ [0.2ex] \hline                  & & & &\\
 &  &  &  & \\ [0.2ex] \hline                  & & & &\\
 &  &  &  & \\ \hline                  & & & &\\
 &  &  &  & \\ [1ex]      \hline                  & & & &\\
 &  &  &  & \\ [1ex]      \hline                  & & & &\\
 &  &  &  & \\ [1ex]      \hline                  & & & &\\
 &  &  &  & \\ [1ex]      \hline
& & & &\\
 &  &  &  & \\ [1ex]      \hline
& & & &\\
 &  &  &  & \\ [1ex]      \hline
& & & &\\
 &  &  &  & \\ [1ex]      \hline \end{tabular}}
\vspace*{0.2in}
\caption{Number of Pairs and Runtime comparison: Euclidean case}\label{table1} \end{table}

In Table~\ref{table1} we show the number of pairs processed (in the second phase) in MK and MPR.  The size (i.e., the length)
 of the time series data varies from 10K to 100K, the motif length being 1024. The following parameter values have been used:  and .
 From this table we see that MK processes around 1.5 times the number of pairs processed by MPR.
Figure~\ref{figure2} presents a graphical plot of the runtime requirements of MK and MPR algorithms. This figure shows that the run time of MK is around 1.5 times the run time of MPR. This improvement is quite significant for the following reason: There are two ideas used in MK, namely, early abandoning and the use of random reference points. As the authors point out in \cite{AEQSB09}, the difference between using early abandoning alone and both the ideas is small, especially on random walk data sets. However, MK performs much better than using early abandoning alone on real datasets.

\begin{figure}[h]
\includegraphics[scale=.50]{runtime.eps}
\caption{Runtime Comparison between MK and MPR: Euclidean case}\label{figure2}
\end{figure}

Table~\ref{table2} and Figure~\ref{figure3} show how the number of pairs  reduces with an increase in the number  of the reference points for MK and MPR algorithms. From these, we note that as  increases, the difference between MK and MPR widens.

\begin{table}[ht]
\centering  \scalebox{0.6}{
\begin{tabular}{|c|c|c|c|c|} \hline
& & & &\\                        No of Ref & No of Pairs(MK) & Runtime(MK) & No of Pairs(MPR) & Runtime(MPR) \\ [0.2ex] \hline                  & & & &\\
 &  & 18.93 sec &  & 13.17 sec\\ [0.2ex] \hline                  & & & &\\
 &  & 10.43 sec &  & 5.03 sec\\ [0.2ex] \hline                  & & & &\\
 &  & 5.74 sec &  & 3.15 sec\\ \hline                  & & & &\\
 &  & 4.29 sec &  & 2.23 sec\\ [1ex]      \hline                  & & & &\\
 &  & 3.15 sec &  & 2.16 sec\\ [1ex]      \hline                  & & & &\\
 &  & 2.88 sec &  & 1.90 sec\\ [1ex]      \hline                  & & & &\\
 &  & 2.73 sec &  & 1.73 sec\\ [1ex]      \hline
& & & &\\
 &  & 2.69 sec &  & 1.68 sec\\ [1ex]      \hline
& & & &\\
 &  & 2.45 sec &  & 1.60 sec\\ [1ex]      \hline
& & & &\\
 &  & 2.46 sec &  & 1.51 sec\\ [1ex]      \hline & & & &\\
 &  & 2.37 sec &  & 1.32 sec\\ [0.2ex] \hline                  & & & &\\
 &  & 2.06 sec &  & 1.18 sec\\ [0.2ex] \hline                  & & & &\\
 &  & 1.99 sec &  & 1.14 sec\\ \hline                  & & & &\\
 &  & 1.96 sec &  & 1.09 sec\\ [1ex]      \hline                  & & & &\\
 &  & 2.00 sec &  & 1.12 sec\\ [1ex]      \hline                  & & & &\\
 &  & 2.01 sec &  & 1.12 sec\\ [1ex]      \hline                  & & & &\\
 &  & 2.12 sec &  & 1.12 sec\\ [1ex]      \hline
& & & &\\
 &  & 2.17 sec &  & 1.19 sec\\ [1ex]      \hline
& & & &\\
 &  & 2.19 sec &  & 1.19 sec\\ [1ex]      \hline
\end{tabular}}
\vspace{0.2in}
\caption{Number of pairs as a function of \protect\linebreak the number of references: Euclidean case}\label{table2} \end{table}

The bar graph in Figure~\ref{figure3} pictorially represents this comparison. The blue and red bars represent the number of pairs processed by MK and MPR, respectively.

\begin{figure}[h]
\includegraphics[scale=.50]{refBar.eps}
\caption{Number of pairs as a function of the number of reference points: Euclidean case}\label{figure3}
\end{figure}

We have run both MK and MPR algorithms on some of the real data sets from \url{http://www.cs.ucr.edu/~mueen/MK/}. Table~\ref{table3} shows the performance of MK and MPR algorithms. On dataset 1, MPR is around 2 times faster than MK and on dataset 2, MPR is around 1.5 times faster than MK. In this Table, `-' indicates that the algorithm did not stop within 40 minutes.

\begin{table}[ht]
\centering  \scalebox{0.6}{
\begin{tabular}{|c|c|c|c|c|c|} \hline
& & & & &\\                        Dataset & Length & No. of Pairs (MK) & Runtime(MK) & No. of Pairs (MPR) & Runtime(MPR)\\ [0.2ex] \hline                  & & & & &\\
1 & 33,021 &  & 43.944 sec &  & 25.214 sec\\ [0.2ex] \hline                  & & & & &\\
2 & 18,667 &  & 485.591 sec &  & 355.531 sec\\ \hline                  & & & & &\\
3 & 78,254 &  & 6018.072 sec &  & 3857.739 sec\\ [1ex]      \hline                  \end{tabular}}
\vspace{0.2in}
\caption{Comparison with Real Data sets. \protect\newline Data set 1: ; \protect\newline Data set 2: ; \protect\newline Data set 3: }\label{table3} \end{table}
\section{The Case of Character Strings}\label{sec4}
In this section we consider the CPP when the space is one of character strings and the metric is Hamming distance. An algorithm for this version of CPP has numerous applications including approximate TSMM (see e.g., \cite{BES03}). When the alphabet is , the light bulb algorithm of \cite{RSJ89} can be used to solve this problem. In this section we show how to modify the light bulb algorithm for the case of generic alphabets. Before presenting details on the modification, we provide a brief summary of the light bulb problem.

\subsection{The light bulb problem}
The light bulb problem is that of identifying the most correlated pair of bulbs from out of  given bulbs \newline . This problem is solved by observing the state of each bulb in  discrete time steps (for some relevant value of ). The states of bulb  in these  time steps can be represented as a vector  (for ). We can think of  as a sample from a probability distribution that governs the state of bulb . It can be shown that if  is sufficiently large (e.g., ), then the pair of bulbs that is the most correlated in the samples is also the most correlated pair with high probability. Thus the light bulb problem can be stated as follows: We are given  Boolean vectors . The problem is to find the pair of vectors that are the most similar (i.e., the Hamming distance between them is the smallest). From hereon, we will use this formulation of the problem.

   Note that, given two vectors, we can find the Hamming distance between them in  time. A straight forward algorithm to identify the most correlated pair of bulbs takes  time. This algorithm computes the Hamming distance between every pair of bulbs. The algorithm of \cite{RSJ89} takes subquadratic time. In particular, the expected run time of this algorithm is \newline  assuming that . Here,  is the correlation between the most correlated pair of bulbs and  is the correlation between the second most correlated pair of bulbs. Note that if the correlation between two bulbs  and  is  then the expected Hamming distance between  and  is . Equivalently, the similarity (i.e., the number of matches) between  and  is .

\subsection{The light bulb algorithm}
Consider a matrix  of size , such that the th row of  is , for . The algorithm of \cite{RSJ89} iteratively collects pairs of bulbs that are candidates to be the most correlated. Once it collects enough pairs, it computes the distance between each pair in this collection and outputs the closest. There are  iterations in the algorithm and in each iteration, some candidate pairs are generated and added to the collection . In any iteration, the algorithm picks  columns of  at random (for some constant ). The rows are sorted based on the characters in the randomly chosen columns. As a result of this sorting, the bulbs get partitioned into buckets such that all the bulbs with equal values (in the  random columns) fall into the same bucket. A pair of bulbs  will get added to  in any iteration if they fall into the same bucket in this iteration.

The authors of \cite{RSJ89} show that after  iterations,  will have the most correlated pair of bulbs with high probability (i.e., with a probability of ). The above algorithm has been proposed for the case of binary strings. We can modify this algorithm to handle the case of an arbitrary (finite) alphabet and get the following theorem.

\begin{theorem}\label{theorem0}
Let  be a matrix of size . Each entry in this matrix is an element from some set  of cardinality . We can find the most correlated pair of columns of  in an expected  time where  is the correlation between the most correlated pair of columns,  is the correlation between the second most correlated pair of columns, and  is the word length of the machine.  This expected run time will be  if we use a general sorting algorithm. (Here correlation is based on Hamming distance. For example,  is the largest fraction of rows in which any two columns agree).
\end{theorem}



\noindent{\bf Proof:} The only difference in the algorithm is that instead of sorting binary strings we will have to sort strings from an arbitrary alphabet. Without loss of generality, let  be the alphabet under concern. In the original algorithm, one has to sort  -bit integers in every iteration. For a generic alphabet, we have to sort  -bit integers. If one uses any comparison based sorting algorithm, this sorting takes  time. If we use an integer sorting algorithm, this sorting can be done in  time where  is the word length of the machine. This is the time spent in each iteration of the algorithm. Therefore, the total expected run time is . 

Let this modified version of the light bulb algorithm be called MLBA.

\subsection{An experimental comparison}
The algorithm of \cite{BES03} for approximate TSMM partitions the input time series data  based on a window of size  (for an appropriate value of ), computes the mean of every window, and discretizes the mean into four possible values. As a result, the time series data is transformed into a string  of characters form the alphabet . It then uses any -motif finding algorithm to find the motifs in . However, all the exact algorithms for finding -motifs take time that is exponential on  and . Note that the last step of finding  motifs can be replaced with a problem of finding time series motifs in  which is nothing but CPP in the domain of strings of characters, the motif length being .

One could employ MK to solve CPP in the domain of character strings. The only difference is that we have to replace Euclidean distance with Hamming distance. We have implemented this algorithm. It turns out that MK does not perform well for the case of Hamming distance. To be fair, the authors of MK have not tested MK for this case. We have compared MK with MLBA and the results are shown in Table~\ref{table4}. As this Table reveals, MLBA is around 200 times faster than MK. It is also clear that if we employ MLBA in place of -motif finding algorithms, the performance of the approximate TSMM algorithm given in \cite{BES03} will improve significantly.

\begin{table}[ht]
\centering  \scalebox{0.6}{
\begin{tabular}{|c|c|c|c|c|} \hline
& & & &\\                        Dataset Size & Pairs(MK) & Runtime in sec(MK) & Pairs(MLBA) & Runtime in sec(MLBA)\\ [0.1ex] \hline                  & & & &\\
 & 176,799 &  & 2,012 & \\ [0.1ex] \hline                  & & & &\\
 & 1,981,048 &  & 7,849 & \\ [0.1ex] \hline                  & & & &\\
 & 5,783,132 &  & 17,795 & \\ \hline                  & & & &\\
 & 11,586,390 &  & 31,337 & \\ [1ex]      \hline                  & & & &\\
 & 9,693,660 &  & 48,867 & \\ [1ex]      \hline                  & & & &\\
 & 14,594,238 &  & 71,164 & \\ [1ex]      \hline                  & & & &\\
 & 20,494,808 &  & 96,539 & \\ [1ex]      \hline
& & & &\\
 & 27,395,331 &  & 125,343 & \\ [1ex]      \hline
& & & &\\
 & 35,295,659 &  & 317,220 & \\ [1ex]      \hline
& & & &\\
 & 44,195,948 &  & 602,326 & \\ [1ex]      \hline \end{tabular}}
\vspace{0.2in}
\caption{Number of Pairs and Runtime comparison on \protect\linebreak strings and Hamming distance}\label{table4} \end{table}

Figure~\ref{figure4} shows a runtime comparison of MK and MLBA for the case of character strings from a finite alphabet.

\begin{figure}[h]
\includegraphics[scale=.50]{MK_MLBA_RTComparison.eps}
\caption{Runtime comparison between MK and MLBA \protect\linebreak for character strings}\label{figure4}
\end{figure}

\section{Identification of the Least Correlated Pair of Strings}\label{sec5}
The light bulb algorithm of \cite{RSJ89} identifies the closest pair of strings, from out of  given binary strings. An interesting question is if we can use the same algorithm to identify the {\bf furthest} pair of strings. This problem has relevance in many problems including the two locus problem in GWAS.
The authors of \cite{PBK11} present an elegant adaptation of the light bulb algorithm to solve this problem when the strings are binary. They also show how to solve this problem for arbitrary alphabets using Locality Sensitive Hashing (LSH) \cite{M02}. They map the input strings into binary strings using LSH. LSH closely preserves similarities with a high probability. In this section we show how to avoid LSH. In particular, we present novel deterministic mappings of the input strings to binary strings such that similarities are preserved deterministically.  Our experimental comparison shows that our algorithm has a  significantly better run time than that of \cite{PBK11}.


\subsection{Some notations}
 Let  stand for the number of matches between two strings (of equal length)  and . For instance, if  and , then  (since they match in positions 2 and 4). Let  and  be two sequences of strings (each string having the same length). We define  to be .

   Consider the sequences , for , where each  is  or  (for ). Note that each  is a sequence of strings where each string is of length 1. Let .

Each  can be thought of as a string from the alphabet . In the application of GWAS, we can let  correspond to the SNP , for . Specifically,  is the value of the th SNP in subject , for . If we are interested in finding the two most correlated SNPs, then we can use MLBA to identify this pair (as shown in Section~\ref{sec4}). On the other hand, if our goal is to identify the least correlated pair, then, it is not clear how to do this using MLBA. To solve the two locus GWAS problem, we have to identify not only the most correlated pair of bulbs but also the least correlated pair.

\subsection{Finding the least correlated pair -- the case of zeros and ones}
 The authors of \cite{PBK11} present an elegant solution for this problem when each  has only zeros and ones. In this case, each  can be thought of as a light bulb. The idea is to construct a matrix  of size  where each column of  corresponds to either a bulb or its `complement', Specifically, the first  columns correspond to the bulbs and the next  columns correspond to the complements of the bulbs. In other words, , for  and  for . Here, if  is any bit, then,  denotes its complement. Let  and . The algorithm of \cite{PBK11} for finding the least correlated pair works as follows. Consider all the pairs of columns  such that  and . From out of these pairs, identify the pair  of columns with the maximum number of matches.  If  and , then  is the least correlated pair of bulbs. Finding such a pair  can be done using the light bulb algorithm of \cite{RSJ89}. The correctness of this algorithm follows from the fact that if the two bulbs  and  have the least number of matches, then, column  and the complement of column  will have the most number of matches.

\subsection{Finding the least correlated pair - the case of zeros, ones, and twos}\label{mapping}
 It is not clear how to extend the above idea when the sequences have three (or more) possible elements. The authors of \cite{PBK11} reduce such general cases to the case of zeros and ones using locality sensitive hashing (LSH).
 The measure of correlation used by \cite{PBK11} is different from what we use in this paper. We define the correlation between two strings  and  as . In contrast, \cite{PBK11} use Pearson's correlation coefficient.



 In this section we present an elegant algorithm for the problem of identifying the least correlated pair of strings without employing LSH. The idea of \cite{PBK11} is to map input strings into Boolean vectors. If  and  are any two strings, then the sequences  and  are mapped to Boolean vectors  and  by LSH such that the distance between  and  will be nearly the same as the distance between  and  with some probability. The larger the length of  is, the better will be the accuracy of LSH in preserving distances.

 Our algorithm also maps each  into a Boolean vector  deterministically such that , for .

 Consider an alphabet  with three strings where . Clearly,  if  and  if  for any . Also,  if  and  if . Here  stands for the string obtained from  by complementing each bit. For example, if  then .

 Consider the sequences , for , where each  is  or  (for ). Note that each  is a sequence of strings where each string is of length 1. Let . Assume now that we encode each  as follows (for  and ): : and . Let the encoded version of  be denoted as  for . Note that , for any .

 It is easy to see that , for any  and  (). For any , let , for . Clearly,  for any .

Clearly, the following statement is true: If, from out of all the pairs of strings,  has the largest correlation, i.e.,  is the largest, then from out of all the Boolean vectors generated,  and  will have the largest correlation. Also, if  is the smallest, then,  and  will have the largest correlation (from out of the pairs ).

 We can now form a matrix  of size  where the first  columns correspond to (transformed) strings and the next  columns correspond to complements of (transformed) strings. Let  and .  Consider all the pairs of columns  such that  and . From out of these pairs, identify the pair  of columns with the maximum number of matches.  If  and , then  is the least correlated pair of strings. Finding such a pair  can be done using the light bulb algorithm of \cite{RSJ89}.

 \subsection{Run Time Analysis}\label{mapanalysis}
\begin{theorem}\label{theorem1}
 Given  strings, we can find the closest pair of strings in an expected time of , where  and  are the largest and the second largest correlation values, respectively. Also, we can find the least correlated pair of strings in an expected time of \protect\linebreak , where  and  are the smallest and the next smallest correlation values, respectively.
 \end{theorem}

\noindent{\bf Proof:}
 When we transform input strings to binary sequences, the ordering of pairs is preserved in terms of correlations as we have shown before. Let  be the correlation of the largest correlated pair and  be the correlation of the second largest correlated pair. How do these values change in the transformed domain? If  and  are the transformed values of these correlations, respectively, it can be seen that  and .

 If  and  are the correlations of the smallest and the second smallest correlated pairs, respectively, and if  and  are the transformed values of these, respectively, then we can see that:  and . To find the largest correlated pair, we can use MLBA (Theorem~\ref{theorem0}). We use the mapping only to find the least correlated pair. 

 \subsection{The case of a general alphabet}
 We have thus far considered the case where the alphabet is . We can extend the mapping to a general alphabet and get the following theorem.

\begin{theorem}
 Given  strings, we can find the largest correlated pair of strings in an expected time of  \newline , where  and  are the largest and the second largest correlation values, respectively. Also, we can find the least correlated pair of strings in an expected  time of \protect\linebreak , where  and  are the smallest and the next smallest correlation values, respectively, and there are  characters in the alphabet.
 \end{theorem}

\noindent{\bf Proof:} Consider sequences from the alphabet . In this case we map each element of this alphabet to a binary string of length  where there is only one 1. Specifically, we use the following mapping:  etc. As before, we don't need any mapping if our goal is to find the largest correlated pair. The mapping is used only to find the least correlated pair. 

 We can improve the above theorem by employing a random mapping as follows: We will use a binary string of length  to encode each symbol in the alphabet. The encoding for each symbol is obtained by (uniformly) randomly choosing each bit in the string (of length ). Let  and  be any two symbols in the alphabet (with ) and let  and  be their encodings, respectively. Then, clearly, the expected value of  is . Also, the expected value of  is . If  is the correlation between a pair of strings and if  is the transformed value, then, it follows that the expected value of  is . An application of the Chernoff bounds will readily imply that the value of  will indeed be very close to this expected value with a probability of . Therefore, we get:

  \begin{theorem}
 Given  strings,  we can find the least correlated pair of strings in an expected time of \protect\linebreak \newline , where  and  are the smallest and the next smallest correlation values, respectively, and there are  characters in the alphabet. 
 \end{theorem}

\section{Two Locus Association Problem}\label{sec6}
The two locus association problem is defined as follows.
Input is a matrix  of size  where  is the number of patients (subjects) each with  SNPs. Here  is the number of cases and  is the number of controls. There are three possible values for each SNP, namely,  or .  The cases are of phenotype  and the controls are of phenotype . Rows  through  of  correspond to cases. Let this submatrix be called . Rows  through  of  correspond to controls and let this submatrix be called . Each column of  corresponds to an SNP.  The two locus association problem is to identify the pair of SNPs whose statistical correlation with phenotype is maximally different between cases and controls. As mentioned in \cite{PBK11}, the goal is to identify the pair:



If  is any matrix, then,  stands for the correlation between the columns  and  of .

The algorithm of \cite{PBK11} exploits the light bulb algorithm of \cite{RSJ89} and locality sensitive hashing (LSH) \cite{M02}. They use LSH to transform matrices  and  to  and , respectively. In particular, each column  of  is converted to a column  of zeros and ones. The size of  is  and the size of  is chosen to be . The matrix  is also transformed into  in a similar manner using LSH. Followed by this, the pair of interest is identified.

To be precise, using  and , the matrix  is formed where


where  is obtained from  by complementing every element of . Note that  is of size . Let  and . Consider all the pairs of columns  such that  and . From out of these pairs, identify the pair  of columns with the maximum number of matches.  If  and , then  is the pair of interest. This pair can be found using the light bulb algorithm of \cite{RSJ89}.

We can use our mapping ideas to get the following theorem.

\begin{theorem}\label{theorem6.1}
We can find the pair  of SNPs that maximizes  in an expected time of , where  and  are the smallest and the next smallest values of , respectively, over all possible pairs  of SNPs.
 \end{theorem}

\noindent{\bf Proof:} Our algorithm also uses the same method except that instead of using LSH to map  and  to  and , respectively, we employ the deterministic mapping we have proposed in Section~\ref{mapping}.

Let  and  be two SNPs (i.e., two columns in  and ). Let  and . Now consider columns  and  of . What can we say about the correlation of these columns? From the discussion in Section~\ref{mapanalysis}, we realize that this correlation is . This also proves the correctness of our algorithm. Let  be the maximum value of  over all possible pairs  and let  be the second largest value. Then, the run time follows from Theorem~\ref{theorem0}. 


 In a similar manner we can also find the pair that maximizes  and hence identify the pair that maximizes .
\subsection{An experimental comparison}
The notion of similarity (between two SNPs) used in \cite{PBK11} is Pearson's correlation coefficient. In this paper the similarity we use is based on the Hamming distance.  Specifically, the complement of the Hamming distance is the measure of similarity we employ.
The authors of \cite{PBK11} have tested their algorithms on different data sets (including random data). Since we do not have access to either these data sets or their algorithms, the only comparison we can do was on the random data. As explained in \cite{PBK11}, we have also generated SNPs from binomial distributions. In particular, for each subject, the value of each SNP is chosen uniformly randomly to be either 0 or 1 with equal probability. This dataset is called NOISE Data in \cite{PBK11} (c.f. Table~\ref{table6} in \cite{PBK11}).  Like in \cite{PBK11}, we have also generated data of sizes K, K, and K. For each size we have generated two different data sets and computed the average number of pairs to be processed. We compare these numbers with the ones reported in \cite{PBK11}. As can be seen from Table~\ref{table5}, our algorithm is around 4 times faster than the one in \cite{PBK11}. Note that this a significant improvement since the typical processing times for the two locus problem are quite high. For example, the authors of \cite{PBK11} report that on some of the data sets (with no more than  SNPs), the brute force algorithm for the two locus problem took several days on 1000 CPUs! Thus any improvement in the run time could make a noticeable difference.

How does one ensure that the output of an algorithm for the two locus problem is correct? For small data sizes, one could run the exhaustive brute force algorithm to identify the correct pair and use it to verify correctness. In fact when the number of SNPs is either K or K, we first found the correct answer and then used it to measure the run time of our algorithm as follows. We'll run our algorithm one iteration at a time until the correct pair(s) is (are) picked up by our algorithm. At this point we will stop and report the total number of pairs collected. The numbers shown in Table~\ref{table5} have been obtained in this manner. We could not use this method for K, since the brute force algorithm was taking too much time.

When  is very large, we {\em inject} pairs with known correlations. As an example, consider the problem of finding the largest correlated pair of columns in a  matrix . Say we generate each column by picking each element to be either 0 or 1 with equal probability. For any two columns, clearly, the expected correlation is . We can perform a probabilistic analysis to get a high probability bound on the largest correlation between any two columns. One could also get this estimate empirically. For example, for , we generated several random data sets and computed the largest correlation in each and calculated an average. The average maximum correlation was . Let  be this value. To inject a pair with a correlation of  where  is  we generate a column  with all ones and another column  with  ones and  zeros. We then replace (any) two columns of  with  and . Clearly, the correlation between  and  is . The expected correlation between  and any other column of  (other than ) is . Similarly, the expected correlation between  any other column of  (other than ) is . Thus the pair  is likely to be the winner with high probability. We stop our algorithm when this pair is picked by our algorithm. We have picked a value for  that is only slightly larger than  so as to get an accurate estimate on the run time.

We have used a similar technique to inject pairs for the two locus problem as well. In Table~\ref{table6} we show the results for our algorithm. In these cases we have generated the SNPs randomly from a binomial distribution as before. We have employed 200 cases and 200 controls. Clearly, the expected value of  is zero for this data. If we map this data using our deterministic mapping, then the expected correlation between any two SNPs will be . Here again we empirically found that the largest correlation was around 58.3\% (when  was K). Therefore, we  have injected a pair whose correlation was \%. This pair is likely to be the winner. Our algorithm was run until this pair was picked. At this time, the algorithm was stopped. We also checked if the algorithm picked any pair whose correlation was better than that of the injected one and found none. In all the datasets we tried, we were always able to find a pair (other than the injected one) whose correlation was very close to 60\% and hence the numbers shown in Table~\ref{table6} are very close to the case with no injections.

\begin{table}[ht]
\centering  \scalebox{0.6}{
\begin{tabular}{|c|c|c|c|c|} \hline
& & & &\\                        Dataset Size & Pairs (GWAS) & Pairs (MLBA) & Top 10 & Top 100\\ [0.2ex] \hline                  & & & &\\
 & 7,082,458 & 1,904,999 & 0.6 & 0.53\\ [0.2ex] \hline                  & & & &\\
 & 13,626,181 & 53,907,259 & 0.4 & 0.34\\ [0.2ex] \hline                  \end{tabular}}
\vspace{0.2in}
\caption{No of pairs comparison on \protect\linebreak NOISE data set}\label{table5} \end{table}

\begin{table}[ht]
\centering  \scalebox{0.6}{
\begin{tabular}{|c|c|} \hline
&\\                        Dataset Size & No. of Pairs (MLBA) \\ [0.2ex] \hline                  &\\
 & 560426 \\ [0.2ex] \hline                  & \\
 & 706197 \\ [0.2ex] \hline                  \end{tabular}}
\vspace{0.2in}
\caption{No of pairs comparison on \protect\linebreak synthetic data with injected pairs}\label{table6} \end{table}


\noindent{\bf Practical Considerations:} Another important question is for how long we should run the program before we can be sure that the correct pair has been obtained (with a high confidence). Please note that we do not know the values of  and  (c.f. Theorem~\ref{theorem6.1}). Theorem~\ref{theorem6.1} suggests that the run time of our algorithm is  for some relevant . We can empirically estimate . The idea is to measure the run time of the algorithm for various values of  (as explained above), the maximum value of  being as much as possible (for the given computing platform and time constraints). Then we could use any numerical procedure to estimate .

\section{Conclusions}\label{sec7}
In this paper we have presented novel algorithms for the closest pair problem (CPP). CPP is a ubiquitous problem that has numerous applications in varied domains. We have offered algorithms for the cases of Euclidean as well as Hamming distances. We have applied our algorithms for two well studied and important problems, namely, time series motif mining and two locus genome wide association study. Our algorithms significantly improve the best-known algorithms for these problems. Specifically, we improve the results presented in many prior papers including \cite{AEQSB09}, \cite{BES03}, and  \cite{PBK11}.

\section{Acknowledgement}
This research has been supported in part by the NIH grant NIH-R01-LM-010101.


















\bibliographystyle{elsarticle-num}
\begin{thebibliography}{}
\bibitem{PBK11}P. Achlioptas, B. Scholkopf and K. Borgwardt, Two-locus association mapping in subquadratic runtime, ACM SIGKDD international conference on Knowledge discovery and data mining (KDD) 2011.
\bibitem{CDDBDADLMDCJE05}C.E. Aston, D.A. Ralph, D.P. Lalo, S. Manjeshwar, B.A. Gramling, D.C. DeFreese, A.D. West, D.E. Branam, L.F. Thompson, M.A. Craft, D.S. Mitchell, C.D. Shimasaki, J.J. Mulvihill, and E.R. Jupe, Oligogenic combinations associated with breast cancer risk in women under 53 years of age, {\em Human Genetics}, 116(3):208-221, Feb. 2005.
\bibitem{PMPS08}P. Beaudoin, M. van de Panne, P. Poulin and S. Coros, Motion-Motif Graphs, Symposium on Computer Animation 2008.
\bibitem{M02}M. S. Charikar, Similarity estimation techniques from rounding algorithms, Proc. ACM Symposium on Theory of Computing (STOC), 2002.
\bibitem{BES03}B. Chiu, E. Keogh, and S. Lonardi, Probabilistic discovery of time series motifs, Proc. of the 9th International Conference on Knowledge Discovery and Data mining (KDD), pp. 493-498, 2003.
\bibitem{JDLCMPMLYJBEJSTS98}J.H. Cho, D.L. Nicolae, L.H. Gold, C.T. Fields, M.C. LaBuda, P.M. Rohal, M.R. Pickles, L. Qin,
Y. Fu, J S. Mann, B.S. Kirschner, E.W. Jabs, J. Weber, S.B. Hanauer, T.M. Bayless, and S.R. Brant. Identification of novel susceptibility loci for inflammatory bowel disease on chromosomes 1p, 3q, and 4q: evidence for epistasis between 1p and IBD1 Proceedings of the National Academy of Sciences of the United States of America, 95(13):7502-7507, June 1998.
\bibitem{T07}T. H. Consortium. A second generation human haplotype map of over 3.1 million SNPs. Nature, 449(7164):851-61, Oct. 2007.
\bibitem{H09}H. J. Cordell. Detecting gene-gene interactions that underlie human diseases. Nat Rev Genet, 10(6):392-404, June 2009.
\bibitem{NMDPCGA99}N.J. Cox, M. Frigge, D.L. Nicolae, P. Concannon, C.L. Hanis, G.I. Bell, and A. Kong, Loci on chromosomes 2 (NIDDM1) and 15 interact to increase susceptibility to diabetes in mexican americans, Nature Genetics, 21(2):213-215, Feb. 1999.
\bibitem{FH79}S. Fortune and J. S. Hopcroft, \emph{A note on Rabin's nearest-neighbor algorithm}, Information Processing Letters, 8(1), pp. 20-23, 1979.
\bibitem{TCM07}T. Guyet, C. Garbay and M. Dojat, Knowledge construction from time series data using a collaborative exploration system, Journal of Biomedical Informatics 40(6): 672-687 (2007).
\bibitem{SY95}S. Khuller and Y. Matias, \emph{A Simple Randomized Sieve Algorithm for the Closest-Pair Problem}, Information and Computation, Vol 188 (1), pp. 34--37, 1995.
\bibitem{JJMY08}J. Meng, J.Yuan, M. Hans and Y. Wu, Mining Motifs from Human Motion, Proc. of EUROGRAPHICS, 2008.
\bibitem{DCIT07}D. Minnen, C.L. Isbell, I. Essa, and T. Starner, Discovering Multivariate Motifs using Subsequence Density Estimation and Greedy Mixture Learning, Proc. 22nd Conference on Artificial Intelligence (AAAI?07), 2007.
\bibitem{AEQSB09} A. Mueen, E. Keogh, Q. Zhu, S. Cash,  B. Westover, \emph{"Exact Discovery of Time Series Motifs"}, SIAM International Conference on Data Mining (SDM) 2009.
\bibitem{SDNRCNHD07}S. K. Musani, D. Shriner, N. Liu, R. Feng, C. S. Coffey, N. Yi, H. K. Tiwari, and D. B. Allison. Detection of gene x gene interactions in genome-wide association studies of human population data. Human Heredity , 63(2):67-84, 2007.
\bibitem{NYH01}R. Nakamichi, Y. Ukai, and H. Kishino, Detection of closely linked multiple quantitative trait loci using a genetic algorithm, Genetics, 158(1):463-475, May 2001.
\bibitem{RSJ89} R. Paturi, S. Rajasekaran, and J. Reif. The light bulb problem. In Proc. 2nd Annu. Workshop on Comput. Learning Theory , pp. 261-268, San Mateo, CA, 1989. Morgan Kaufmann.
\bibitem{PS86}F. Preparata and M. Shamos, \emph{Computational Geometry}, Springer Verlag, 1986.
\bibitem{RAB76}M. Rabin, \emph{Probabilistic Algorithms}, Algorithms and Complexity, Recent Results and New Directions, Academic Press, pp 21-39, 1976.
\bibitem{SG04}S. Rombo and G. Terracina, Discovering representative models in large time series databases, Proc. 6th International Conference on Flexible Query Answering Systems, pp. 84-97, 2004.
\bibitem{YKK05}Y. Tanaka, K. Iwamoto, and K. Uehara, Discovery of time-series motif from multi-dimensional data based on MDL principle, Machine Learning, 58(2-3):269-300, 2005.
\bibitem{S07}S. Tata, Declarative Querying For Biological Sequences, Ph.d Thesis, The University of Michigan, 2007. (Advisor Jignesh M. Patel).
\bibitem{YXKR10}Y. Wang, X. Liu, K. Robbins, and R. Rekaya, AntEpiSeeker: detecting epistatic interactions for case-control studies using a two-stage ant colony optimization algorithm, BMC Bioinformatics, 3:117-117, 2010.
\bibitem{JCSEBSAKLDPJW04}J. Xu, C.D. Langefeld, S.L. Zheng, E.M. Gillanders, B. Chang, S.D. Isaacs, A.H. Williams, K E. Wiley, L. Dimitrov, D.A. Meyers, P.C. Walsh, J.M. Trent, and W.B. Isaacs, Interaction effect of PTEN and CDKN1B chromosomal regions on prostate cancer linkage, Human Genetics, 115(3):255-262, Aug. 2004.
\bibitem{YO91}A.C. Yao, \emph{Lower Bounds for algebric computation trees with integer inputs}, SIAM J. Comput., 20:4, pp. 655-668, 1991.
\bibitem{XSFW10}X. Zhang, S. Huang, F. Zou, and W. Wang. TEAM: efficient two-locus epistasis tests in human genome-wide association study. Bioinformatics (Oxford, England), 26(12):i217-227, June 2010.
\bibitem{XFW08}X. Zhang, F. Zou, and W. Wang, Fastanova: an efficient algorithm for genome-wide association study, in Proc. 14th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), pages 821-829, Las Vegas, Nevada, USA, 2008.










\end{thebibliography}
\end{document}
