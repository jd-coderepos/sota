\RequirePackage{fix-cm}
\documentclass[twocolumn,compsoc,tikz]{cvm}
\pdfoutput=1
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{import}
\usetikzlibrary{quotes,arrows.meta}
\usetikzlibrary{positioning}

\def\edgecolor{rgb:blue,4;red,1;green,4;black,3}
\newcommand{\midarrow}{\tikz \draw[-Stealth,line width =0.8mm,draw=\edgecolor] (-0.3,0) -- ++(0.3,0);}

\usepackage{Ball}
\usepackage{Box}
\usepackage{Block}
\usepackage{RightBandedBox}

 \usetikzlibrary{positioning}
\usetikzlibrary{3d} \usepackage{graphicx,wrapfig,lipsum}
\usepackage{adjustbox}
\usepackage{hyperref}
\setcounter{page}{1}
\graphicspath{{figures/},{figures/cvm/}}
\usepackage{etoolbox}

\def\ManuscriptInfo{Manuscript received: 2020-02-20; accepted: 2020-04-24.}

\def\PaperTitle{iSeeBetter: Spatio-temporal video super-resolution using recurrent generative back-projection networks}

\def\ConvColor{rgb:yellow,5;red,2.5;white,5}
\def\ConvReluColor{rgb:yellow,5;red,5;white,5}
\def\PoolColor{rgb:red,1;black,0.3}
\def\UnpoolColor{rgb:blue,2;green,1;black,0.3}
\def\FcColor{rgb:blue,5;red,2.5;white,5}
\def\FcReluColor{rgb:blue,5;red,5;white,4}
\def\SoftmaxColor{rgb:magenta,5;black,7}

\newcommand{\copymidarrow}{\tikz \draw[-Stealth,line width=0.8mm,draw={rgb:blue,4;red,1;green,1;black,3}] (-0.3,0) -- ++(0.3,0);}
\newcommand{\dd}[1]{\mathrm{d}#1}

\def\AuthorNames{Aman Chadha(\rm{\Letter}), John Britto, and M. Mani Roja}

\def\AuthorAdress{ &Department of Computer Science, Stanford University, 450 Serra Mall, Stanford, CA 94305, USA. E-mail: aman@amanchadha.com; amanc@stanford.edu (\rm{\Letter}).\\
     & Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, USA. E-mail: jnadar@umass.edu.\\
    & Department of Electronics and Telecommunication Engineering, University of Mumbai, Mumbai, Maharashtra 400032, India. E-mail: maniroja@tsec.edu\\
}

\def\firstheader{DOI 10.1007/s41095-xxx-xxxx-x\hfill Vol. x, No. x, month year, xx--xx\
\hspace*{-0.75em}
MSE_t = \frac{1}{{WH}}\sum\limits_{x = 0}^W {\sum\limits_{y = 0}^H {{{({{(H{R_t})}_{x,y}} - {G_{{\theta _G}}}{{(L{R_t})}_{x,y}})}^2}} }

\hspace*{-0.5em}
\begin{array}{l}
PerceptualLoss_t = \\
\frac{1}{{{W_{i,j}}{H_{i,j}}}}\sum\limits_{x = 1}^{{W_{i,j}}} {\sum\limits_{y = 1}^{{H_{i,j}}} {{{\left( \begin{array}{l}
VG{G_{i,j}}{\left( {H{R_t}} \right)_{x,y}} - \\
VG{G_{i,j}}{({G_{{\theta _G}}}(L{R_t}))_{x,y}}
\end{array} \right)}^2}} }
\end{array}\

AdversarialLoss_t = - log ({D_{{\theta _D}}}({G_{{\theta _G}}}(L{R_t}))

\begin{array}{*{20}{l}}
{TVLos{s_t} = }\\
{\frac{1}{{WH}}\sum\limits_{i = 0}^W {\sum\limits_{j = 0}^H {\sqrt {\begin{array}{*{20}{l}}
{{{({G_{{\theta _G}}}{{(L{R_t})}_{i,j + 1,k}} - {G_{{\theta _G}}}{{(L{R_t})}_{i,j,k}})}^2} + }\\
{{{({G_{{\theta _G}}}{{(L{R_t})}_{i + 1,j,k}} - {G_{{\theta _G}}}{{(L{R_t})}_{i,j,k}})}^2}}
\end{array}} } } }
\end{array}

Loss{_G}_{{{_\theta }_{_{_G}}}}(SR_t)=\begin{array}{*{20}{l}}
{\rm{ }}\,\,\,\,\,\alpha{\rm{ }} \times {\rm{ }}MSE\left( {SR_t, HR_t} \right)\\
{+ {\rm{ }}\,\beta {\rm{ }} \times {\rm{ }}PerceptualLoss\left({SR_t,HR_t}\right)}\\
{+ \,\gamma \times {\rm{ }} {\rm{ }}AdversarialLoss\left( {SR_t} \right)}\\
{+\, \delta {\rm{ }}\, \times {\rm{ }}TVLoss\left( {SR_t,{\rm{ }}HR_t} \right)}
\end{array}

Los{s_D}_{{{_\theta }_{_{_D}}}}(SR_t) = {\rm{ }}1 - {D_\theta }_{_D}(HR_t) + {D_\theta }_{_D}(SR_t)\\

\begin{array}{l}
Los{s_G}_{{{_\theta }_{_{_G}}}} = {\rm{ }}\frac{1}{N}\sum\limits_{t = 1}^N {(Los{s_G}_{{{_\theta }_{_{_G}}}}(S{R_t}))} \\
Los{s_D}_{{{_\theta }_{_{_D}}}} = {\rm{ }}\frac{1}{N}\sum\limits_{t = 1}^N {(Los{s_D}_{{{_\theta }_{_{_D}}}}(S{R_t}))} 
\end{array}\


\section{Experimental evaluation}\label{sec:evaluation}

To train the model, we used an Amazon EC2 P3.2xLarge instance with an NVIDIA Tesla V100 GPU with 16GB VRAM, 8 vCPUs and 64GB of host memory. We used the hyperparameters from RBPN and SRGAN. Tab. \ref{tab:3} compares iSeeBetter with six state-of-the-art VSR algorithms: DBPN \cite{haris2018deep}, B + T \cite{liu2017robust}, DRDVSR \cite{tao2017detail}, FRVSR \cite{sajjadi2018frame}, VSR-DUF \cite{jo2018deep} and RBPN/6-PF \cite{haris2019recurrent}. Tab. \ref{tab:4} offers a visual analysis of VSR-DUF and iSeeBetter. Tab. \ref{tab:5} shows ablation studies to assess the impact of using a generator-discriminator architecture and the four-fold loss as design decisions.

\begin{table*}[hb]
    \caption{Visually inspecting examples from Vid4, SPMCS and Vimeo-90k comparing VSR-DUF and iSeeBetter. We chose VSR-DUF for comparison because it was the state-of-the-art at the time of publication. Top row: fine-grained textual features that help with readability; middle row: intricate high-frequency image details; bottom row: camera panning motion.}
\label{tab:4}
\begin{center}
\begin{tabular}{ccccc}
    \toprule
        Dataset & Clip Name & VSR-DUF \cite{jo2018deep} & \textbf{iSeeBetter} & Ground Truth \\ 
        \hline
        \noalign{\vskip 2mm}
Vid4 & Calendar & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{CALRB}}} & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{CALiSB}}} & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{CALGT}}}\\
        \noalign{\vskip 2mm}
SPMCS & Pagoda & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{PRB}}} & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{PiSB}}} & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{PGT}}}\\
        \noalign{\vskip 2mm}
Vimeo90K & Motion & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{MotionRB}}} & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{MotioniSB}}} & \adjustbox{valign=m}{\resizebox{115pt}{!}{\includegraphics{MotionGT}}}\\
        \noalign{\vskip 1mm}
        \bottomrule
    \end{tabular}
        \end{center}
\end{table*}

\begin{table*}[hb]
\caption{Ablation analysis for iSeeBetter using the ``City'' clip from Vid4.}
\begin{adjustbox}{width=0.9\textwidth,center}
\begin{tabular}
    {lcc}
        \toprule
iSeeBetter Config & PSNR\\
\hline
RBPN baseline with L1 loss & 27.73 \\
RBPN baseline with MSE loss & 27.77 \\
RBPN generator + SRGAN discriminator with adversarial loss & 28.08 \\
RBPN generator + SRGAN discriminator with adversarial + MSE loss & 28.12 \\
RBPN generator + SRGAN discriminator with adversarial + MSE + perceptual loss & 28.27 \\
RBPN generator + SRGAN discriminator with adversarial + MSE + perceptual + TV loss & 28.34 \\
        \bottomrule
    \end{tabular}
\label{tab:5}
\end{adjustbox}
\end{table*}





\section{Conclusions and future work}\label{sec:conclusions}
We proposed iSeeBetter, a novel spatio-temporal approach to VSR that uses recurrent-generative back-projection networks. iSeeBetter couples the virtues of RBPN and SRGAN. RBPN enables iSeeBetter to generate superior SR images by combining spatial and temporal information from the input and neighboring frames. In addition, SRGAN's discriminator architecture fosters generation of photo-realistic frames. We used a four-fold loss function that emphasizes perceptual quality. Furthermore, we proposed a new evaluation protocol for video SR by collating diverse datasets. With extensive experiments, we assessed the role played by various design choices in the ultimate performance of iSeeBetter, and demonstrated that on a vast majority of test video sequences, iSeeBetter advances the state-of-the-art.

To improve iSeeBetter, a couple of ideas could be explored. In visual imagery the foreground recieves much more attention than the background since it typically includes subjects such as humans. To improve perceptual quality, we can segment the foreground and background, and make iSeeBetter perform ``adaptive VSR'' by utilizing different policies for the foreground and background. For instance, we could adopt a wider span of the number of frames to extract details from for the foreground compared to the background. 
Another idea is to decompose a video sequence into scenes on the basis of frame-similarity and make iSeeBetter assign weights to adjacent frames based on which scene they belong to. Adjacent frames from a different scene can be weighed lower compared to frames from the same scene, thereby making iSeeBetter focus on extracting details from frames within the same scene -- \`a la the concept of attention applied to VSR. 
\CvmAck{The authors would like to thank Andrew Ng's lab at Stanford University for their guidance on this project. In particular, the authors express their gratitude to Mohamed El-Geish for the idea-inducing brainstorming sessions throughout the project.}

\bibliographystyle{CVM}

{\normalsize  \bibliography{iSeeBetter}}

\Author{ACAD}{Aman Chadha}
{has held positions at some of the world's leading semiconductor/product companies. He is currently based out of Cupertino (Silicon Valley), California and is currently pursuing his graduate studies in Artificial Intelligence from Stanford University. He has published in prestigious international journals and conferences, and has authored two books. His publications have garnered about 200 citations. He currently serves on the editorial boards of several international journals including IJATCA, IJLTET, IJCET, IJEACS and IJRTER. He has served as a reviewer for IJEST, IJCST, IJCSEIT and JESTEC. Aman graduated with an M.S. from the University of Wisconsin-Madison with an outstanding graduate student award in 2014 and his B.E. with distinction from the University of Mumbai in 2012. His research interests include Computer Vision (particularly, Pattern Recognition), Artificial Intelligence, Machine Learning and Computer Architecture. Aman has 18 publications to his credit.}

\Author{JBRITTO}{John Britto}
{is pursuing his M.S. in Computer Science from the University of Massachusetts, Amherst. He completed his B.E. in Computer Engineering from the University of Mumbai in 2018. His research interests lie in Machine Learning, Natural Language Processing, and Artificial Intelligence.}

\Author{MMROJA}{M. Mani Roja}
{is a full professor in the Electronics and Telecommunication Department at the University of Mumbai since the past 30 years. She received her Ph.D. in Electronics and Telecommunication Engineering from Sant Gadge Baba Amravati University and her Masters in Electronics and Telecommunication Engineering from the University of Mumbai. She has collaborated across the years in the fields of Image Processing, Speech Processing, and Biometric recognition.}


\end{document}
