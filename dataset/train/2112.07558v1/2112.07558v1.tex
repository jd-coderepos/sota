We present in this section our numerical experiments to assess the benefit of multimodality for crop mapping with temporal attention-based networks. We evaluate several modality-fusion schemes and several mapping tasks. We also introduce a new large-scale open-access  and multimodal  dataset with annotations fit for all tasks.
\subsection{Pastis-R}
To evaluate the benefit of multimodality, we extend the open-access PASTIS dataset \citep{pastis} with corresponding Sentinel-1 observations.
PASTIS is composed of  time series of  multi-spectral patches sampled in four different regions of France. Each patch has a spatial extent of kmkm and contains all available Sentinel-2 observations for the  season for a total of k images.
Note that PASTIS does not filter out  observations with high cloud cover, hence, certain patches can be partially or entirely obstructed by clouds.


We use Sentinel-1 in Ground Range Detected format processed into {  backscatter coefficient } in decibels, {orthorectified at a } m spatial resolution with Orfeo Toolbox \citep{christophe2008orfeo}. We do not apply any spatial or temporal speckle filtering, {nor radiometric terrain correction}: {following the deep learning paradigm, we limit data preprocessing to the minimum.} We assemble each Sentinel-1 observation into a -channel image: vertical polarization (VV), horizontal polarisation (VH), and the ratio of vertical over horizontal polarization (VV/VH). We separate observations made in ascending and descending orbit into two distinct time series. Indeed, the incidence angle of space-borne radar can significantly influence the return signal \citep{singhroy1999effects}. As represented in \figref{fig:pastis}, each time series comprises around  radar acquisitions { for each of the  patches. This amounts to } a total of k added radar images.
We use the annotations of PASTIS: semantic class and instance identifier for each pixel, allowing us to evaluate models for parcel-based classification, semantic segmentation, and panoptic segmentation. We make the PASTIS-R dataset \citep{pastis-r} publicly available at: \url{github.com/VSainteuf/pastis-benchmark} . 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth, trim=0cm 12.5cm 0cm 0cm,clip]{gfx/pastis_figv2.002.jpeg}
    \caption{\textbf{{Pastis-R.}} We extend the PASTIS dataset with radar time series corresponding to ascending and descending orbits of Sentinel-1. For each square patch of kmkm, PASTIS-R thus provides the image time series of  different modalities, along with semantic and instance annotation for each pixel.}
    \label{fig:pastis}
\end{figure}

\subsection{Implementation details}
As detailed in \secref{sec:implem}, we use the official PyTorch implementations of PSE+LTAE and U-TAE with default hyperparameters.
We  use the official  cross-validation folds of PASTIS \citep{garnot2021utae} to evaluate the performance of the different models. We train our models using the Adam optimizer \citep{kingma2014adam} with default parameters ,  unless specified otherwise, and train all networks on a single TESLA V100 GPU with Gb of VRAM. 

\paragraph{\bf Multimodality Configuration}  We consider the two orbits of Sentinel-1 as separate modalities to account for their difference in incident angle, which corresponds to . When using auxiliary loss terms, we set  for all modalities. When using temporal dropout, we set  for the optical modality and  for the radar time series. For early fusion, we interpolate the Sentinel-1 observations to the dates of the Sentinel-2 time series. Indeed, the opposite interpolation strategy would imply tripling the temporal length of the Sentinel-2 time series, which would significantly increase memory usage. Interpolation is computed on the fly when loading dataset samples. 

\paragraph{\bf Parcel Classification} For this problem, we train the models for  epochs in batches of  parcels. We use the  class nomenclature of PASTIS and report the classification Intersection over Union macro-averaged over the class set (mIoU) to evaluate the parcel-level predictions. 

\paragraph{\bf Semantic Segmentation} We train the semantic segmentation models for  epochs in batches of  multi-temporal patches. In this setting, the models also predict \emph{background} pixels, resulting in a  class nomenclature. We report the mIoU of the pixel-level predictions: 

with ,, and  the count of true positives, false positives, and false negatives for the binary class predictions defined by a class .
\paragraph{\bf Panoptic Segmentation} We follow the training procedure recommended by \citet{garnot2021utae} to train the PaPs network: the learning rate starts at  for the first  epochs, and decreases to  for the last  epochs. We report the class-averaged panoptic metrics introduced in \citet{kirillov2019panoptic}: Segmentation Quality (SQ), Recognition Quality (RQ), and Panoptic Quality (PQ). 
The RQ corresponds to the  score for the problem of combined detection and classification: to be counted as a true positive, a parcel must be both detected (the intersect over union of the predicted and true instance masks is above ) and have its crop type correctly classified.
The SQ corresponds to the intersect over union between the true and predicted masks for correctly detected and classified parcels. Finally, the PQ is the product of both values, thus simultaneously combining information on the quality of detection, classification, and delineation. We report the unweighted classwise average of the three quality measurements. We refer the reader to \citet{kirillov2019panoptic} for more details on these metrics.



\subsection{Parcel Classification Experiment}
We first implement and evaluate the different fusion schemes and their training enhancements for parcel classification. In this setting, the contour of parcels is known in advance, and the model predicts the type of crop cultivated during the period covered by the SITS.

\begin{table}[ht!]
\caption{\textbf{Parcel Classification.} We evaluate the performance of models operating on a single modality (top) and for different fusion strategies for parcel-based classification (bottom). We evaluate each model's baseline performance and the impact of the temporal dropout and auxiliary classifiers enhancements, when applicable. We report the 5-fold cross-validated classification scores in terms of mean classwise Intersect over Union, the base model's parameter count, and, when relevant, of the model with auxiliary classifiers.}
\label{tab:xp:parcelbased}
\begin{tabular}{lccccccc}
\toprule
         & \multicolumn{2}{c}{\multirow{2}{*}{Base}} &\,& Temp. & Auxiliary   & Auxiliary \&     &      Parameter      \\
         &       &&                  & dropout  & supervision & Temp. dropout &      Count      \\  \cline{2-3} \cline{5-7}
         &OA&mIoU& \multicolumn{4}{c}{mIoU} \\
         \midrule
S2       & 91.7 &73.9                  && 74.5     & -           & -                &       114k    \\
S1D      & 87.0&64.5                  && 64.7     & -           & -                &       114k     \\
S1A      & 86.4&63.3                  && 62.9     & -           & -                &       114k    \\\midrule
Early Fusion  &91.8  &74.9                  && 76.5       & -           & -                &       117k     \\
Mid Fusion   & 92.0 &75.1                  && 75.9     &      75.0       & 76.5             &       152k/185k    \\
Late Fusion  & 91.1&73.0                  &&   73.6       & 76.1        & \textbf{77.2}             &       254k/287k    \\
Decision Fusion & 91.0 &72.5                  &&   72.8       & 75.2        & 75.8            &        259k    \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{\bf Analysis}
In \tabref{tab:xp:parcelbased}, we report the performance of all fusion schemes with and without  enhancements.
We first observe that the optical satellite S2 significantly outperforms the two radar time series by a margin of almost  points of mIoU, confirming the relevance of Sentinel-2 for crop type mapping.
We remark that, without enhancement (first column), multimodal models trained with early or mid-fusion schemes improve the performance {compared to} single optical modality network{s}, {while} decision and late fusion perform slightly worse consistently with the results of \citet{pelletier2021fusion}. This highlights the benefit of learning {to mix} modality features early on. 
In contrast, auxiliary supervision and temporal dropout improve the later models. This shows that these enhancements can encourage attention-based models to combine features and decisions efficiently from different modalities, as observed in \citet{ienco2019combining} for recurrent networks. 
All things considered, late fusion with both enhancements performs best with  mIoU compared to  {a network operating purely on the optical modality}, see \figref{fig:perclass_parcel} for {a classwise comparison}. {Mid-}fusion without enhancement provides good performances with a lower parameter count and none of the preprocessing necessary for early fusion. In practice, the {mid-fusion scheme} is \% faster at inference time than late fusion, making it a valid choice when operating with limited computational resources.

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{gfx/Fusion_per_class_parcel.pdf}
    \caption{{\bf Classwise Performance for Parcel Classification.} We report the IoU of the late fusion model with auxiliary supervision and temporal dropout and {of} the model traine{d} purely on the optical modality. {Multimodality brings a consistent benefit across all classes, which is more notable for some of the most challenging classes such as \emph{Potatoes}, or \emph{Winter triticale}.}}
    \label{fig:perclass_parcel}
\end{figure} 
\paragraph{\bf Auxiliary Supervision and Gradient Flow}
Motivated by the impact of auxiliary supervision on the performance of the late fusion approach, we propose to study its effect on the learning process further.  
Specifically, we wish to evaluate the different spatio-temporal encoders' contribution to the reduction of the objective loss , with and without auxiliary supervision, and for the parcel classification task.
Note that, as auxiliary decisions are not computed at inference time, we only consider the decrease of : a decrease in the auxiliary losses does not directly affect the model's performance.

Following the insights of  \citet{wang2020picking}, we consider the following first-order approximation of the decrease of  incurred by taking a gradient step:

with  the current learning rate.The term  of the scalar product in \eqref{eq:flow} corresponds to the step size in the gradient descent and the term  to the slope of the objective loss. Their scalar product approximates the decrease in objective loss when taking a single gradient step. Note that this approximation, called gradient flow, is only valid when using stochastic gradient descent (SGD) and does not hold for momentum or adaptive optimization schemes {such as ADAM \citep{kingma2014adam}}. We thus retrain the late fusion model with SGD for parcel classification. By considering each term in the scalar product in \equaref{eq:flow}, we can estimate the contribution of each parameter of the network to the decrease of the objective loss .
\begin{figure}[h]
    \centering
    \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth, trim=0cm 1cm 0cm 0cm, clip]{gfx/gflow_base.pdf}
    \caption{Training without auxiliary supervision: ~.}
    \label{fig:gflow:base}
    \end{subfigure}
    \vfill
    \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{gfx/gflow_aux.pdf}
    \caption{Training with auxiliary supervision: ~.}
    \label{fig:gflow:aux}
    \end{subfigure}
    \caption{{\bf Gradient Flow.} Evolution of the gradient flow for different modules of the late fusion model. The contribution of each modality is plotted as a fraction of the total flow, without auxiliary loss terms (top) and with the additional  term (bottom). We report the flow for the spatial encoders (PSE), temporal encoders (LTAE), and the MLP-based decoders.}
    \label{fig:gflow}
\end{figure}

In \figref{fig:gflow}, we represent the evolution of the gradient flow for different modules of our architecture by summing the contribution of their corresponding parameters.
We observe that, as expected, the gradient flow is concentrated in the modules dedicated to the optical modality. Interestingly, the spatial encoders contribute as much or even more than the temporal encoders despite having four times fewer parameters.

We remark that auxiliary losses lead the model to a different training regime. 
While auxiliary supervision results in an increase of the proportion of gradient flow in some radar modules such as PSE-S1A, the flow also increases in proportion in some optical modules as well. We conclude that auxiliary supervision affects all modalities, not only the weaker modalities.

\subsection{Semantic Segmentation}
In this section, we evaluate the performance of the late fusion scheme compared to single modality baselines for semantic segmentation. While the  {mid-fusion} scheme yields promising results on parcel-based experiments, its implementation into a semantic segmentation architecture is not trivial. Indeed,  the state-of-the-art network for this task \citep{garnot2021utae} relies on a U-Net architecture with temporal encoding. In this architecture, spatial and temporal encoding are performed conjointly. After several unsuccessful attempts, we limit our study to the other fusion schemes for this task.

\begin{table}[h]
\caption{\textbf{Semantic Segmentation Experiment}. We evaluate the  semantic segmentation performance of models operating on a single modality and multimodal models trained with early, late, and decision fusion strategies. We evaluate each model's baseline performance and the impact of temporal dropout and auxiliary classifiers, when applicable. We report the 5-fold cross-validated classification scores in mean classwise Intersect over Union (- not applicable). Note that temporal dropout is necessary for the late and decision fusion models to fit in memory.}
\label{tab:semantic}
\begin{center}
\begin{tabular}{lcccc}
\toprule
         & \multirow{2}{*}{Base} & Temporal   & Auxiliary \&     &      Parameter      \\
         &                       & dropout  & Temporal dropout &      Count      \\ \midrule
S2       & 63.1                  & 63.6            & -                &       1\;087k   \\
S1D      &     54.9             &   54.7           & -                &      1\;083k     \\
S1A      & 53.8                  & 53.3          & -                &       1\;083k    \\
Early Fusion        & 64.9        & 65.8         & -     & 1\;602k\\
Late Fusion  & -                 &    65.8          & \textbf{66.3}             &       1\;709k     \\
Decision Fusion  &-                &   64.7          & 64.3          &       1\;742k     \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\paragraph{\bf Analysis} We report the performance of the different models in \tabref{tab:semantic}. In our experimental setup, the late fusion model with {over}  total {multimodal} observations did not fit in {the Gb of memory of our GPU} with a batch size of  image time series. {By reducing the size of the input sequences, temporal dropout allowed us to  train this memory-intensive model.}
The late fusion model improves the performance of the unimodal models by  mIoU points. The performance is further improved by {another}  point with the addition of auxiliary supervision. {The early fusion model performs slightly below late fusion, even with temporal dropout. }
As represented in \figref{fig:qualisem}, the radar {modality allows for prediction with crisper contours, in particular between adjacent or nearly adjacent parcels}. This suggests that {the image rugosity}  {of the radar} acquisitions is {can be} valuable to detect inter-parcel zones. {These areas, often of sub-pixel extent,} may display optical reflectances {similar to their} neighboring parcels {but often present surfaces such as fences or groves with a volumetric scatter and thus a distinct radar response . }

Note that the performance of our models on semantic segmentation is around pts below that for parcel classification. This result was expected as {the semantic segmentation task prevents us from exploiting knowledge about the contour of parcels and has the supplementary class \emph{background}, corresponding to non-agricultural land.}


\begin{figure}[th!]
    \centering
\begin{tabular}{c}
    \begin{tabular}{rlrlrlrl}
 \definecolor{tempcolor}{rgb}{0,0,0}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Background} 
           &
           \definecolor{tempcolor}{rgb}{0.6823529411764706, 0.7803921568627451, 0.9098039215686274}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Meadow}
           &
           \definecolor{tempcolor}{rgb}{1.0, 0.4980392156862745, 0.054901960784313725}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Soft W. wheat}
           &
           \definecolor{tempcolor}{rgb}{1.0, 0.7333333333333333, 0.47058823529411764}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Corn}
           \\
           \definecolor{tempcolor}{rgb}{0.17254901960784313, 0.6274509803921569, 0.17254901960784313}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{W. barley} 
           &
           \definecolor{tempcolor}{rgb}{0.596078431372549, 0.8745098039215686, 0.5411764705882353}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{W. rapeseed}
           &
           \definecolor{tempcolor}{rgb}{0.8392156862745098, 0.15294117647058825, 0.1568627450980392}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Spring barley}
           &
           \definecolor{tempcolor}{rgb}{1.0, 0.596078431372549, 0.5882352941176471}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Sunflower}
           \\
           \definecolor{tempcolor}{rgb}{0.5803921568627451, 0.403921568627451, 0.7411764705882353}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Grapevine} 
           &
           \definecolor{tempcolor}{rgb}{0.7725490196078432, 0.6901960784313725, 0.8352941176470589}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Beet}
           &
           \definecolor{tempcolor}{rgb}{0.5490196078431373, 0.33725490196078434, 0.29411764705882354}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{W. triticale}
           &
           \definecolor{tempcolor}{rgb}{0.7686274509803922, 0.611764705882353, 0.5803921568627451}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{W. durum wheat}
           \\
           \definecolor{tempcolor}{rgb}{0.8901960784313725, 0.4666666666666667, 0.7607843137254902}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Fruits, veg., flow.} 
           &
           \definecolor{tempcolor}{rgb}{0.9686274509803922, 0.7137254901960784, 0.8235294117647058}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Potatoes}
           &
           \definecolor{tempcolor}{rgb}{0.4980392156862745, 0.4980392156862745, 0.4980392156862745}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Leguminous fodder}
           &
           \definecolor{tempcolor}{rgb}{0.7803921568627451, 0.7803921568627451, 0.7803921568627451}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Soybeans}
           \\
           \definecolor{tempcolor}{rgb}{0.7372549019607844, 0.7411764705882353, 0.13333333333333333}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Orchard} 
           &
           \definecolor{tempcolor}{rgb}{0.8588235294117647, 0.8588235294117647, 0.5529411764705883}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Mixed cereal}
           &
           \definecolor{tempcolor}{rgb}{0.09019607843137255, 0.7450980392156863, 0.81176470588235291}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1); 
           & \footnotesize{Sorghum}
           &
           \definecolor{tempcolor}{rgb}{1,1,1}
           \tikz \fill[fill=tempcolor, scale=0.3, draw=black] (0,0) rectangle (1,1);
           & \footnotesize{Void label}
    \end{tabular}
 \\
\begin{tabular}{ccccc}
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_1_RGB.png}
     &
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_1_RAD.png}
     &
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_1_S2.png}
     & 
     \begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {  \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_1_FUS.png}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
            \draw[cyan,ultra thick] (0.52,0.34) circle (0.12);
        \end{scope}
     \end{tikzpicture}
  
     &
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_1_GT.png}
     \\
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_2_RGB.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_2_RAD.png}
     &
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_2_S2.png}
     &
     \begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {   \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_2_FUS.png}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
            \draw[magenta,ultra thick] (0.85,0.85) circle (0.12);
            \draw[cyan,ultra thick] (0.25,0.3) circle (0.10);
        \end{scope}
     \end{tikzpicture}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_2_GT.png}
     \\
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_4_RGB.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_4_RAD.png}
     &
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_4_S2.png}
     &
      \begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {       \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_4_FUS.png}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
            \draw[magenta,ultra thick] (0.82,0.43) circle (0.08);
            \draw[cyan,ultra thick] (0.66,0.45) circle (0.08);
        \end{scope}
     \end{tikzpicture}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/VIZ_4_GT.png}
     \\
     \begin{subfigure}{.19\textwidth}
    \caption{Optical}
    \label{fig:qualisem:s2}
    \end{subfigure}
    &
    \begin{subfigure}{.19\textwidth}
    \caption{Radar}
    \label{fig:qualisem:s1}
    \end{subfigure}

    &
    \begin{subfigure}{.19\textwidth}
    \caption{S2 Prediction}
    \label{fig:qualisem:mono}
    \end{subfigure}
    &
    \begin{subfigure}{.19\textwidth}
    \caption{Fusion Prediction}
    \label{fig:qualisem:late}
    \end{subfigure}
    &
     \begin{subfigure}{.19\textwidth}
    \caption{Ground Truth}
    \label{fig:qualisem:gt}
    \end{subfigure}
    \end{tabular}
  \end{tabular}
    \caption{{\bf Qualitative Results for Semantic Segmentation.} We show one observation from the optical time series in \Subfigref{fig:qualisem:s2} and {one} from the radar time series in \Subfigref{fig:qualisem:s1}. The prediction for the unimodal optical model is represented in \Subfigref{fig:qualisem:mono}, a{our late fusion multimodal model} in \Subfigref{fig:qualisem:late}, and finally the ground truth in \Subfigref{fig:qualisem:gt}. We observe that the multimodal model produces results with clearer and more distinct borders between close parcels (cyan circle \protect\tikz \protect\node[circle, thick, draw = cyan, fill = none, scale = 0.7] {};). The multimodal model also displays fewer errors for hard and ambiguous parcels, showing the benefit of learning intermodal features (magenta circle \protect\tikz \protect\node[circle, thick, draw = magenta, fill = none, scale = 0.7] {};). Crop types are represented according to the color code above (W. stands for Winter). The same legend is {used} in all subsequent figures representing crop labels. }
    \label{fig:qualisem}
\end{figure}

\paragraph{ \bf Varying Cloud Cover Experiment}
One of the motivations for using both optical and radar images in the context of crop type mapping is to exploit the imperviousness of radar signals to cloud cover. This potentially allows our model to rely on the radar signal when optical observations are obstructed by clouds, which is particularly crucial in countries with pervasive cloud cover, such as subtropical regions \citep{orynbaikyzy2020crop}. The parcel-based and semantic experiments allow for a first exploration of this capacity, but remain {bound} to the specific cloud conditions of the French metropolitan territory and the {year} of acquisition ({2019}). We propose to further investigate this benefit of multimodality by artificially simulating increased cloud obstruction {on the test set}. To do so, we evaluate the performance of models when removing random optical acquisitions while leaving the radar time series unchanged. We report the performance of the models in Figure \ref{fig:london}, for different ratios of remaining optical observations, corresponding to different levels of cloud obstruction.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=.8\linewidth]{gfx/LondonFogv5.pdf}
    \caption{Parcel-based classification}
    \label{fig:london:parcel}
    \end{subfigure}
    \vfill
    \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=.8\linewidth]{gfx/LondonFog_semsegv2.pdf}
    \caption{Semantic segmentation}
    \label{fig:london:semantic}
    \end{subfigure}
    \caption{\textbf{Varying Cloud Cover Experiment.} We {evaluate} the different models with {varying} ratios optical observations {remaining}. In both parcel-based classification \Subfigref{fig:london:parcel} and semantic segmentation \Subfigref{fig:london:semantic}, the fusion models prove more robust to a reduced number of optical observations.  }
    \label{fig:london}
\end{figure} 

As expected, the performance of the S2-only model drops drastically as the number of available optical observations decreases for both parcel classification and semantic segmentation, performing worse than unimodal radar models for {a ratio of} \% of artificial occlusion. 
Multimodal fusion models can maintain an almost constant level of performance for up to \% missing optical acquisitions. For more extreme ratios, the performances of the multimodal models eventually drop. The magnitude of the drop seems to be related to the amount of interplay between modalities in the network.
{Early fusion proves the least robust to missing optical observations.} Mid-fusion, and to a lesser extent, the late fusion are also affected by obstruction. These models rely on multimodal encoders and decoders, which are likely to be affected by a severe decrease in the quality of the optical sequence. In contrast, the decision fusion scheme comprises independent classifiers and proves to be the most resilient: even with \% of optical images removed, it still outperforms the radar modality by several mIoU points.
We conclude that such models should be favored in regions with pervasive or inconsistent cloud cover.

We also observe that auxiliary supervision and temporal dropout make both unimodal and multimodal models  more resilient to missing optical acquisitions for semantic segmentation. The same phenomenon can be observed for parcel classification but was not represented for clarity.

\subsection{Panoptic segmentation experiment}
In this section, we evaluate the performance of the {early and} late fusion schemes compared to single modality baselines for panoptic segmentation. We do not assess auxiliary losses on the late fusion model  as the use of auxiliary decoders in this setting comes at a prohibitive computational cost. Indeed, the auxiliary decoders would be PaPs instance segmentation modules which already significantly impact training times on single modality architectures. Decision fusion is not evaluated here for the same reason.  Like in the semantic segmentation experiment, temporal dropout proved necessary to train the late fusion model.

\begin{table}[]
\centering
\caption{\textbf{Panoptic Segmentation Experiment}. We evaluate the panoptic segmentation performance of models operating on a single modality and multimodal models trained with the early and late fusion strategy with temporal dropout.}
\label{tab:xp:pano}
\begin{tabular}{lcccc}
\toprule
                    & SQ & RQ & PQ & Parameter count\\\midrule
S2                  &  81.3  & 49.2   & 40.4 & 1\;318k  \\
S1D                 &  77.0  &  39.3  &   30.9 & 1\;318k  \\
S1A                 &  77.4  &  38.8   &  30.6 & 1\;318k  \\
Early Fusion + Tdrop        &   \textbf{82.2 }&  \textbf{50.6} &  \textbf{42.0} &1\;791k \\
Late Fusion + Tdrop        &   81.6 &  50.5 &  41.6 & 2\;390k \\\bottomrule
\end{tabular}
\end{table}

\paragraph{\bf Analysis} We report the results of this experiment on \tabref{tab:xp:pano}. Overall, the early and late fusion schemes increase the panoptic quality by pt and pt, respectively, compared to the optical baseline. This improvement is mostly driven by an increase in recognition quality, while the segmentation quality remains almost unchanged. This suggests that the radar modality helps correctly detect additional agricultural parcels rather than refining the delineation of their boundaries. Although modest, this improvement is valuable for this notoriously complex task.

We show on \figref{fig:qualipano} the qualitative evaluation of the panoptic fusion model compared to the optical baseline. In practice, the fusion model seems to retrieve more agricultural parcels successfully and manages to retrieve small parcels missed by the optical model. We also display the predictions made by the unimodal models and the predictions of the fusion model in \figref{fig:qualipanomod}. These qualitative results show how the radar modality helps detect more parcels than the optical baseline or improve the fusion model's semantic predictions. Additionally, given the relative noisiness of radar observations, the radar-only models retrieve surprisingly well the parcel boundaries. {As mentioned previously, this could be attributed to the distinct volumetric radar response on parcel boundaries.}  We report the per-class performances on \figref{fig:perclass_pano}. 

Regarding robustness to clouds, when performing inference on only \% of the optical observations, the S2 baseline model drops to  PQ. In contrast, our late fusion model maintains a score of  PQ. Consistently with the previous experiments, the addition of the radar modality helps improve the panoptic predictions with reduced availability of optical observations.








\begin{figure}[h]
\centering
\begin{tabular}{ccccc}
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_1_RGB.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_1_RAD.png}
     &
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_1_S2.png}
     &
   \begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {       \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_1_FUS.png}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
            \draw[magenta,ultra thick] (0.1,0.35) circle (0.08);
            \draw[magenta,ultra thick] (0.26,0.14) circle (0.08);
        \end{scope}
    \end{tikzpicture}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_1_GT.png}
     \\
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_3_RGB.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_3_RAD.png}
     &
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_3_S2.png}
     &
    \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {       \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_3_FUS.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[cyan,ultra thick] (0.25,0.75) circle (0.2);
    \end{scope}
    \end{tikzpicture}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_3_GT.png}
     \\
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_5_RGB.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_5_RAD.png}
     &
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_5_S2.png}
     &
    \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {       \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_5_FUS.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[green,ultra thick] (0.8,0.75) circle (0.18);
        \draw[green,ultra thick] (0.8,0.4) circle (0.15);
    \end{scope}
    \end{tikzpicture}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/pano/PANOVIZ_5_GT.png}
     \\
     \begin{subfigure}{.19\textwidth}
    \caption{Optical}
    \label{fig:qualipano:s2}
    \end{subfigure}
    &
    \begin{subfigure}{.19\textwidth}
    \caption{Radar}
    \label{fig:qualipano:s1}
    \end{subfigure}

    &
    \begin{subfigure}{.19\textwidth}
    \caption{S2 Prediction}
    \label{fig:qualipano:mono}
    \end{subfigure}
    &
    \begin{subfigure}{.19\textwidth}
    \caption{Fusion Prediction}
    \label{fig:qualipano:late}
    \end{subfigure}
    &
     \begin{subfigure}{.19\textwidth}
    \caption{Ground Truth}
    \label{fig:qualipano:gt}
    \end{subfigure}
    \end{tabular}
    
    
    
\caption{{\bf Qualitative Results for Panoptic Segmentation.}  We show one observation from the optical time series in \Subfigref{fig:qualipano:s2} and {one} from the radar time series in \Subfigref{fig:qualipano:s1}. The prediction for the unimodal optical model is represented in \Subfigref{fig:qualipano:mono} and the multimodal model in \Subfigref{fig:qualipano:late}, and finally the ground truth in \Subfigref{fig:qualipano:gt}, with the same colormap as in \figref{fig:qualisem}. The fusion model {retrieves} more parcels (cyan circle 
\protect\tikz \protect\node[circle, thick, draw = cyan, fill = none, scale = 0.7] {};), and even {small parcels that were missed by the purely optical model} (magenta circle 
\protect\tikz \protect\node[circle, thick, draw = magenta, fill = none, scale = 0.7] {};). We also note that the fusion model seems to handle parcels with internal subdivisions (green circle 
\protect\tikz \protect\node[circle, thick, draw = green, fill = none, scale = 0.7] {};) better than the optical model.}
\label{fig:qualipano}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{gfx/Fusion_per_class_pano.pdf}
    \caption{{\bf Classwise Performance for Parcel Classification.} We report the Panoptic Quality of the late fusion model with temporal dropout and the model trained purely on the optical modality. The classes are ordered as in \figref{fig:perclass_parcel}. In the panoptic setting, the radar modality is also specifically beneficial for hard classes such as \emph{Winter triticale}.}
    \label{fig:perclass_pano}
\end{figure}





\begin{figure}[h]
\centering
\begin{tabular}{ccccc}
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_6_S1A.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_6_S1D.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_6_S2.png}
     & 
     \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {           \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_6_FUS.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[green,ultra thick] (0.15,0.23) circle (0.12);
        \draw[red,ultra thick] (0.8,0.62) circle (0.12);

    \end{scope}
    \end{tikzpicture}
     & 
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_6_GT.png}
 \\
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_7_S1A.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_7_S1D.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_7_S2.png}
     & 
         \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {               \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_7_FUS.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[cyan,ultra thick] (0.15,0.43) circle (0.09);
    \end{scope}
    \end{tikzpicture}
    
     & 
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_7_GT.png}
 \\
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_8_S1A.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_8_S1D.png}
     & 
    \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_8_S2.png}
     & 
    \begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {               \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_8_FUS.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[green,ultra thick] (0.3,0.7) circle (0.2);
    \end{scope}
    \end{tikzpicture}
     & 
     \includegraphics[width=.18\textwidth, trim=1cm 0.5cm 0cm 0cm, clip]{gfx/panomod/PANOMODVIZ_8_GT.png}
    \\
     \begin{subfigure}{.19\textwidth}
    \caption{S1A Prediction}
    \label{fig:qualipano:s2}
    \end{subfigure}
    &
    \begin{subfigure}{.19\textwidth}
    \caption{S1D Prediction}
    \label{fig:qualipano:s1}
    \end{subfigure}

    &
    \begin{subfigure}{.19\textwidth}
    \caption{S2 Prediction}
    \label{fig:qualipano:mono}
    \end{subfigure}
    &
    \begin{subfigure}{.19\textwidth}
    \caption{Fusion Prediction}
    \label{fig:qualipano:late}
    \end{subfigure}
    &
     \begin{subfigure}{.19\textwidth}
    \caption{Ground Truth}
    \label{fig:qualipano:gt}
    \end{subfigure}
\end{tabular}
    
    
    
\caption{{\bf Qualitative Results for Panoptic Segmentation.} We compare the predictions made by unimodal models operating on S1A (a), S1D (b), S2 (c), and the predictions made by the late fusion model (d). We also show the ground truth annotations (e). We observe cases where the optical model does not detect parcels but successfully predicted by the radar-only models and by the fusion model as well (green circle 
\protect\tikz \protect\node[circle, thick, draw = green, fill = none, scale = 0.7] {};). We also note that the optical model detects some parcels, but the crop type is corrected by the addition of the radar modality
(red circle 
\protect\tikz \protect\node[circle, thick, draw = red, fill = none, scale = 0.7] {};).
Conversely, some parcels are detected by the radar-only model with an incorrect crop type and not detected by the optical model. Combining both modalities in the fusion model leads to a correct prediction. (cyan circle 
\protect\tikz \protect\node[circle, thick, draw = cyan, fill = none, scale = 0.7] {};)}

\label{fig:qualipanomod}
\end{figure}