\vspace{-1mm}
\subsection{Image Tagging}
\vspace{-1mm}
We have collected image-level annotation on six weather conditions, six scene types, and three distinct times of day, for each image. The videos contain large portions of extreme weather conditions, such as snow and rain. They also include a diverse number of different scenes across the world. Notably, our dataset contains approximately an equal number of day-time and night-time videos. Such diversity allows us to study domain transfer and generalize our object detection model well on new test sets. Detailed distributions of images with weather, scene, and day hours tags are shown in the Appendix. We provide image tagging classification results using DLA-34~\cite{dla} in Figure~\ref{fig:tagging}. The average classification accuracy across different weather and scenes are around 50 to 60.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/tagging.pdf}
    \vspace{-5mm}
    \caption{\small Image tagging classification results using DLA-34.\vspace{-4mm}}
    \label{fig:tagging}
\end{figure}

\begin{figure*}[tp]
    \centering
    \adjustbox{width=0.7\linewidth}{
    \begin{tabular}{c@{\hspace{1pt}}c@{\hspace{1pt}}c}
        \includegraphics[width=0.3\linewidth]{images/lanes_jpg/006a7635-c42f9f97.jpg} & 
        \includegraphics[width=0.3\linewidth]{images/lanes_jpg/000d35d3-41990aa4.jpg} & 
        \includegraphics[width=0.3\linewidth]{images/lanes_jpg/00268999-a4b8e39d.jpg} \\
    \end{tabular}
    }
    \caption{\small Examples of lane marking annotations. Red lanes are vertical and blue lanes are parallel. Left: we label all the visible lane boundaries. Middle: not all marking edges are lanes for vehicles to follow, such as pedestrian crossing. Right: parallel lanes can also be along the current driving direction.\vspace{-2mm}}
    \label{fig:lane_examples}
\end{figure*}

\begin{figure*}[tp]
    \centering
    \adjustbox{width=.9\linewidth}{
    \begin{tabular}{c@{\hspace{1pt}}c@{\hspace{1pt}}c@{\hspace{1pt}}c}
        \includegraphics[width=0.3\linewidth]{images/drivable_jpg/b20b69d2-6e2b9e73.jpg} & 
        \includegraphics[width=0.3\linewidth]{images/drivable_jpg/b22a4d9f-48b2e986.jpg} &
        \includegraphics[width=0.3\linewidth]{images/drivable_jpg/b20b69d2-bd242bf0.jpg} &
        \includegraphics[width=0.3\linewidth]{images/drivable_jpg/b2102d00-a8c09be1.jpg} \\
    \end{tabular}
    }
    \caption{\small Examples of drivable areas. Red regions are directly drivable and the blue ones are alternative. Although drivable areas can be confined within lane markings, they are also related to locations of other vehicles shown in the right two columns.\vspace{-4mm}}
    \label{fig:drivable_examples}
\end{figure*}

\subsection{Object Detection}
Locating objects is a fundamental task for not only autonomous driving but the general visual recognition.
We provide bounding box annotations of 10 categories for each of the reference frames of 100K videos. The instance statistics is shown in Figure~\ref{fig:bbox_instance}. We provide visibility
attributes including ``occluded'' and ``truncated'' in Figure~\ref{fig:occlusion} and
Figure~\ref{fig:truncation}. 

\subsection{Lane Marking}

The lane marking detection is critical for vision-based vehicle localization and trajectory planning. However,
available datasets are often limited in scale and diversity. For example, the Caltech Lanes
Dataset~\cite{Aly2008Real} only contains 1,224 images, and the Road Marking Dataset~\cite{Wu2012A} has 1,443
images labeled in 11 classes of lane markings. The most recent work, VPGNet~\cite{lee2017vpgnet}, consists of
about 20,000 images taken during three weeks of driving in Seoul. 

Our lane markings (Figure~\ref{fig:lane_examples}) are labeled with 8 main categories: road curb,
crosswalk, double white, double yellow, double other color, single white, single yellow, single other color. The \textit{other} categories are ignored during evaluation.
We label the attributes of continuity (full or dashed) and direction (parallel or perpendicular). Shown
in Table~\ref{tab:lane_label_comp}, our lane marking annotations cover a diverse set of classes. Detailed
distributions of types of lane markings and drivable areas are shown in the Appendix.
\vspace{-2mm}
\begin{table}[htp]
    \centering
    \adjustbox{width=.8\linewidth}{
    \begin{tabular}{l@{\hspace{2pt}}|@{\hspace{2pt}}c@{\hspace{2pt}}c@{\hspace{2pt}}c}
    \toprule
    Datasets & Training & Total & Sequences\\ \midrule
    Caltech Lanes Dataset~\cite{Aly2008Real} & - & 1,224 & 4 \\
        Road Marking Dataset~\cite{Wu2012A} & - & 1,443 & 29 \\
         KITTI-ROAD~\cite{fritsch2013new} & 289 & 579 & - \\
         VPGNet~\cite{lee2017vpgnet} & 14,783 & 21,097 & - \\
         \midrule
         BDD100K & 70,000 & 100,000 & 100,000\\
         \bottomrule
    \end{tabular}}
    \caption{\small Lane marking statistics. Our lane marking annotations are significantly richer and are more diverse.\vspace{-2mm}}
    \label{tab:lane_label_comp}
\end{table}

\subsection{Drivable Area}

Lanes alone are not sufficient to decide road affordability for driving. Although most of the time, the vehicle should stay between the lanes, it is common that no clear lane marking exists. In addition, the road area is shared with all other vehicles, but a lane can not be driven on if occupied. All these conditions beyond lane markings direct our driving decisions and thus are relevant for designing autonomous driving algorithms.

Our drivable areas are divided into two different categories: \emph{directly drivable area} and \emph{alternatively drivable area}. The directly drivable area is what the driver is currently driving on -- it is also the region where the driver has priority over other cars or the right of the way. In contrast, alternatively drivable area is a lane the driver is currently not driving on, but able to do so via changing lanes. Although the directly and alternatively drivable areas are visually indistinguishable, they are functionally different, and require the algorithms to recognize blocking objects and scene context. Some examples are shown in Figure~\ref{fig:drivable_examples}. The distribution of drivable region annotations is shown in the Appendix. Not surprisingly, on highways or city streets, where traffic is closely regulated, drivable areas are mostly within lanes and they do not overlap with the vehicles or objects on the road. However, in residential areas, the lanes are sparse. Our annotators can find the drivable areas based on the surroundings.

\subsection{Semantic Instance Segmentation}

We provide fine-grained, pixel-level annotations for images from each of the 10,000 video clips randomly sampled from the whole dataset. Each pixel is given a label and a corresponding identifier denoting the instance number of that object label in the image. Since many classes (e.g., sky) are not amenable to being split into instances, only a small subset of class labels are assigned instance identifiers. The entire label set 
consists of 40 object classes, which are chosen to capture the diversity of objects in road scenes as well as maximizing the number of labeled pixels in each image. Besides a large number of labels, our dataset exceeds previous efforts in terms of scene diversity and complexity. The whole set is split into 3 parts: 7K images for training, 1K images for validation, and 2K images for testing. The distribution of classes in the semantic instance segmentation dataset is shown in the Appendix.


\subsection{Multiple Object Tracking}

To understand the temporal association of objects within the videos, we provide a multiple object tracking (MOT) dataset including 2,000 videos with about 400K frames. Each video is approximately 40 seconds and annotated at 5 fps, resulting in approximately 200 frames per video. We observe a total number of 130.6K track identities and 3.3M bounding boxes in the training and validation set. The dataset splits are 1400 videos for training, 200 videos for validation and 400 videos for testing. Table~\ref{tab:box_tracking_label_comp} shows the comparison of BDD100K with previous MOT datasets. Our tracking benchmark provides one order-of-magnitude bigger than the previously popular tracking dataset, MOT17~\cite{mot16}. A recent dataset released by Waymo~\cite{waymo_2019} has fewer tracking sequences (1150 vs 2000) and fewer frames (230K vs 398K) in total, compared to ours. But Waymo data has more 2D boxes (9.9M vs 4.2M), while ours has better diversity including different weather conditions and more locations. Distributions of tracks and bounding boxes by category are shown in the Appendix.


\begin{table}[htp]
\centering
\adjustbox{width=.78\linewidth}{
\small
\begin{tabular}{l@{\hspace{4pt}}|c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}}
    \toprule
   Datasets          & Frames & Sequences & Identities & Boxes     \\ \midrule
KITTI~\cite{kitti}  & 8K       & 21            & 917           & 47K \\
MOT17~\cite{mot16} & 34K & 21            & 1,638      & 337K   \\
\midrule
BDD100K & 318K         & 1,600      &           131K    & 3.3M \\ 
\bottomrule
\end{tabular}}
\caption{\small MOT datasets statistics of training and validation sets. Our dataset has more sequences, frames, identities as well as more box annotations.\vspace{-2mm}}
    \label{tab:box_tracking_label_comp}
\end{table}

\begin{figure}[tp]
    \centering
    \vspace{4mm}
    \setlength{\tabcolsep}{1pt}
    \adjustbox{width=\linewidth}{
    \begin{tabular}{c@{\hspace{2pt}}c@{\hspace{2pt}}c}
        \includegraphics[height=0.35\linewidth]{images/box-track/size-1.png} &
        \includegraphics[height=0.35\linewidth]{images/box-track/size-2.png} &
        \includegraphics[height=0.35\linewidth]{images/box-track/length.png} \\
    \end{tabular}
    }
    \caption{\small Cumulative distributions of the box size (left), the ratio between the max and min box size for each track (middle) and track length (right). Our dataset is more diverse in object scale.\vspace{-1mm}}
    \label{fig:mot-track}
\end{figure}

\begin{figure}[htp]
    \vspace{-2mm}

    \centering
    \setlength{\tabcolsep}{1pt}

    \begin{tabular}{cc}
        \includegraphics[width=0.45\linewidth]{images/box-track/occlusion-1.pdf} & \hspace{3mm}
        \includegraphics[width=0.45\linewidth]{images/box-track/occlusion-2.pdf} \\
    \end{tabular}
    \vspace{-2mm}
    \caption{\small Number of occlusions by track (left) and number of occluded frames for each occlusion (right).  Our dataset covers complicated occlusion and reappearing patterns. \vspace{-2mm}}
    \label{fig:mot-occlusion}
    
\end{figure}

BDD100K MOT is diverse in object scale. Figure~\ref{fig:mot-track} (left) plots the cumulative distribution of box size, defined as  for a bounding box with width  and height . Figure~\ref{fig:mot-track} (middle) shows the cumulative distribution of the ratio between the maximum box size and the minimum box size along each track, and  Figure~\ref{fig:mot-track} (right) shows that of the length of each track. The distributions show that the MOT dataset is not only diverse in visual scale among and within tracks, but also in the temporal range of each track.

Objects in our tracking data also present complicated occlusion and reappearing patterns are shown in Figure~\ref{fig:mot-occlusion}. An object may be fully occluded or move out of the frame, and then reappear later. We observe 49,418 occurrences of occlusion in the dataset, or one occurrence of occlusion every 3.51 tracks. Our dataset shows the real challenges of object re-identification for tracking in autonomous driving.


\subsection{Multiple Object Tracking and Segmentation}

We further provide a multiple object tracking and segmentation (MOTS) dataset with 90 videos. We split the dataset into 60 training videos, 10 validation videos, and 20 testing videos.


\begin{table}[htp]
\centering
\adjustbox{width=0.9\linewidth}{
\begin{tabular}{l@{\hspace{4pt}}|c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}c@{\hspace{4pt}}}
    \toprule
  Datasets   & Frames & Seq. & Identities & Ann. & Ann. / Fr.     \\ \midrule
KITTI MOTS~\cite{mots}  & 8K          & 21             & 749        & 38K & 4.78  \\
MOTS Challenge~\cite{mots} & 2.9K     & 4             & 228        & 27K & 9.40  \\
\midrule
DAVIS 2017~\cite{davis17} & 6.2K      & 90                & 197           &  - & -      \\
YouTube VOS~\cite{ytvos} & 120K      & 4.5K                & 7.8K           &  197K & 1.64      \\
\midrule
BDD100K MOTS & 14K     & 70              & 6.3K      & 129K & 9.20 \\
\bottomrule
\end{tabular}
}
\caption{\small Comparisons with other MOTS and VOS datasets. }
\label{tab:seg_tracking_label_comp}
\end{table}

\begin{figure*}[htp]
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccc}        \includegraphics[width=0.24\textwidth]{images/segmentation/1b5c5134-ba91a0e3_img} &
        \includegraphics[width=0.24\textwidth]{images/segmentation/1b5c5134deadcf8624425b89eee2be92-ba91a0e3-b11a-4a15-80fa-966048a7e76d_city} &
        \includegraphics[width=0.24\textwidth]{images/segmentation/1b5c5134deadcf8624425b89eee2be92-ba91a0e3-b11a-4a15-80fa-966048a7e76d_bdd} &
        \includegraphics[width=0.24\textwidth]{images/segmentation/1b5c5134-ba91a0e3_gt} \\
        
        \includegraphics[width=0.24\textwidth]{images/segmentation/00e9be89-00001245_img} &
        \includegraphics[width=0.24\textwidth]{images/segmentation/00e9be89d500bd6d6e50577eca7ebbef_1245_city} &
        \includegraphics[width=0.24\textwidth]{images/segmentation/00e9be89d500bd6d6e50577eca7ebbef_1245_bdd} &
        \includegraphics[width=0.24\textwidth]{images/segmentation/00e9be89-00001245_gt} \\
        (a) \small{Our Image} & \small{(b) Trained on Cityscapes} & \small{(c) Trained on Ours} & \small{(d) Ground Truth} \\
    \end{tabular}
    \vspace{-2mm}
    \caption{\small Visual comparisons of the same model (DRN~\cite{drn}) trained on different datasets. We find that there is a dramatic domain shift between Cityscapes and our new dataset. For example, due to infrastructure difference, the model trained on Cityscapes is confused by some simple categories such as sky and traffic signs.}
    \label{fig:cityscapes_vis}
\end{figure*}

\begin{table*}[htb]
\centering
\adjustbox{width=.65\linewidth}{
\begin{tabular}{l|ccc|l|ccc}
\toprule
\backslashbox{Train}{Test}& City & Non-City & Val  &\backslashbox[30mm]{Train}{Test} & Daytime & Non-Daytime & Val \\ \midrule
City-30K & 29.5 & 26.5 & 28.8  & Daytime-30K & 30.6 & 23.6 & 28.1\\
Non-City-30K & 24.9 & 24.3 & 24.9 & Non-Daytime-30K & 25.9 & 25.3 & 25.6\\
Random-30K & 28.7 & 26.6 & 28.3 & Random-30K & 29.5 & 26.0 & 28.3\\
\bottomrule
\end{tabular}}
\caption{\small Domain discrepancy experiments with object detection. We take the images from one domain and report testing results in AP on the same domain or the opposite domain. We can observe significant domain discrepancies, especially between daytime and nighttime.\vspace{-5mm}}
\label{tab:detection_transfer}
\end{table*}

Table~\ref{tab:seg_tracking_label_comp} shows the details of the BDD MOTS dataset and the comparison with existing multiple object tracking and segmentation (MOTS) and video object segmentation (VOS) datasets. MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge~\cite{mots} require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS \cite{ytvos} dataset. Detailed distributions of the MOTS dataset by category are shown in the Appendix.

\subsection{Imitation Learning}

GPS/IMU recordings in our dataset show the human driver action given the visual input and the driving trajectories. We can use those recordings as a demonstration supervision for the imitation learning algorithms and use perplexity to measure the similarity of driving behaviors on the validation and testing set. We refer to Xu~\etal~\cite{xu2017end} for details on the evaluation protocols. Visualizations of the driving trajectories are shown in the Appendix.