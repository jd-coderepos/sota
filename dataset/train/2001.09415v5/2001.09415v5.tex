\def\year{2021}\relax
\documentclass[letterpaper]{article} \usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  

\usepackage{soul}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\urlstyle{same}
\usepackage{subfigure}
\usepackage{amsmath, amssymb}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{footnote}
\usepackage{amssymb}
\usepackage{nameref}

\usepackage{times}
\usepackage{latexsym}
\usepackage[switch]{lineno}  

\pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2021.1)
} 



\setcounter{secnumdepth}{0} 







\iffalse
\title{My Publication Title --- Single Author}
\author {
Author Name \\
}

\affiliations{
    Affiliation \\
    Affiliation Line 2 \\
    name@example.com
}
\fi


\title{DUMA: Reading Comprehension with Transposition Thinking }
\author {
Pengfei Zhu,\textsuperscript{\rm 1,2,3}
Hai Zhao,\textsuperscript{\rm 1,2,3,\footnote{Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cutting-edge Machine reading comprehension and language model.}}
Xiaoguang Li\textsuperscript{\rm 4} \\
}
\affiliations {
\textsuperscript{\rm 1}Department of Computer Science and Engineering, Shanghai Jiao Tong University\\
\textsuperscript{\rm 2}Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\\
\textsuperscript{\rm 3}MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\\
\textsuperscript{\rm 4}Huawei Noah's Ark Lab\\
}

\begin{document}


\maketitle

\begin{abstract}
Multi-choice Machine Reading Comprehension (MRC) requires model to decide the correct answer from a set of answer options when given a passage and a question. Thus in addition to a powerful Pre-trained Language Model (PrLM) as encoder, multi-choice MRC especially relies on a matching network design which is supposed to effectively capture the relationships among the triplet of passage, question and answers. While the newer and more powerful PrLMs have shown their mightiness even without the support from a matching network, we propose a new \textbf{DU}al \textbf{M}ulti-head Co-\textbf{A}ttention (DUMA) model, which is inspired by human's transposition thinking process solving the multi-choice MRC problem: respectively considering each other's focus from the standpoint of passage and question. The proposed DUMA has been shown effective and is capable of generally promoting PrLMs. Our proposed method is evaluated on two benchmark multi-choice MRC tasks, DREAM and RACE, showing that in terms of powerful PrLMs, DUMA can still boost the model to reach new state-of-the-art performance.

\end{abstract}

\section{Introduction}
Machine Reading Comprehension has been a heated topic and challenging problem, and various datasets and models have been proposed in recent years \cite{newsqa,ecawe,msmarco,squad,teach_machine_read,dream,race,dua,lingke,zhang2020sg,bhargav2020translucent,hu2019read+}. For the tasks of MRC, given passage and question, the task can be categorized as \textit{generative} and \textit{selective} according to its answer style \cite{mrc_survey}. \textit{Generative} tasks require the model to generate answers according to the passage and question, not limited to spans of the passage, while \textit{selective} tasks give model several candidate answers to select the best one. Multi-choice MRC is a typical task in \textit{selective} type, xwhich is the focus of this paper. Figure \ref{tab:example_dream} shows one example of DREAM dataset \cite{dream}, whose task is to select the best answer among three candidates given particular passage and question. 

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{dream_example_back.pdf}
	\caption{\label{tab:example_dream} An example of DREAM dataset.}
\end{figure}

The kernel method for a model to solve MRC problem is a two-level hierarchical process, 1) representation encoding which is done by an encoder such as PrLM; and 2) capturing the relationship among the triplet of passage, question and answer which has to be carefully handled by various matching networks such as OCN \cite{ocn} and DCMN \cite{dcmn}. With the development of PrLMs, matching network design tends to become more complicated for more effective improvements. 

\makesavenoteenv{tabular}
\makesavenoteenv{table}
\begin{table}[h]\small
\renewcommand\arraystretch{1.3}
	\centering
	{
		\begin{tabular}{@{}p{1.9cm}|@{}p{0.45cm}|l|l|p{1.3cm}}
			\hline		
			 		& & +OCN & +WAE & +DCMN \\
			\hline
			\hline
			BERT  & \,71.0 & 71.7(+0.7) & 73.1(+2.1)& 75.8(+4.8) \\
			XLNet& \,80.1 & 80.9(+0.8) & 81.8(+1.7)& 82.8(+2.7) \\
			ALBERT  & \,86.6  & 87.2(+0.6) & 87.3(+0.7) & 85.7(-0.9) \\
			ELECTRA  &  \,86.1 & 86.3(+0.2) & 86.9(+0.8)&  84.9(-1.2)\\
			\hline
		\end{tabular}
		
	}
	\caption{Improvements of several prior models for representative PrLMs (sorted by releasing time) on RACE dataset.}
	\label{lm_and_net_compare}
\end{table}

Table \ref{lm_and_net_compare} shows that as the newer variant of the PrLM such as ALBERT \cite{albert} has shown its powerfulness even without the support from a proper matching network, in the meantime, the previous models\footnote{We re-implement OCN and WAE, and obtain codes of DCMN through personal communication with its authors. Besides, results denoted with  are from original papers.} \cite{ocn, bert_wae, dcmn} either brings very limited improvements or even cause drop on the PrLM's \cite{bert, xlnet, albert, clark2020electra} performance, which motivates us to develop an more effective mechanism to support the powerful enough PrLMs. Instead of designing more complicated matching network patterns, we choose a going-back-to-the-basic way to have obtained inspiration from human experience on solving MRC problems, which intuitively is to first \textbf{1)} quickly read through the overall content of passage, question and answer options to build up a global impression, followed by a \textbf{transposition thinking} process: \textbf{2)} based on dedicated information from question and answer options, re-considerate details of the passage and collect supporting evidences for question and answer options, \textbf{3)} based on dedicated information from passage, re-considerate the question and answer options to decide the correct option and exclude wrong options. When humans are re-reading the passage, they tend to extract key information according to their impression of question and answer options, and it is the same when re-reading question and answer options. It can be regarded as a bi-directional process in terms of transposition thinking pattern, and we adopt an attention inside network design to simulate this procedure, whose details are shown in the following Section \textit{\nameref{sec_model}}.

Since attention mechanism was proposed \cite{nmt_attention} originally for Neural Machine Translation, it has been widely used in MRC tasks to model the relationships between passage and question, and effectively enhances nearly all kinds of tasks \cite{bidaf,dua,dcmn}. Attention mechanism computes relationships of each word representation in one sequence to a target word representation in another sequence and aggregates them to form a final representation, which is commonly named as passage-to-question attention or question-to-passage attention.

Transformer \cite{transformer} uses self-attention mechanism to represent dependencies and relationships of different positions in one single sequence, which is an effective method to obtain representations of sentences for global encoding. Since \cite{gpt,bert} use it to improve the structure of PrLMs \cite{elmo}, many kinds of PrLMs has been proposed to constantly refresh records of all kinds of tasks \cite{roberta,albert}. For PrLMs, the more layer and bigger hidden size they use, the better performance they achieve. Benefited from large-scale unlabeled training data and multiple stacked layers, PrLMs are able to encode sentences into very deep and precise representations. Moreover, \cite{albert} reveals the importance of generalization for models, that is parameter sharing among layers can efficiently improve the performance. However, training a LM has been a time and labor consuming work, which usually needs amounts of engineering works to explore parameter settings. The bigger the model is, the more resource it consumes and the harder it can be implemented. Moreover, despite the great success they achieve in different tasks, we find that for MRC tasks, using self-attention of the Transformer to model sequences is far from enough. No matter how deep the structure is, it suffers from the nature of self-attention, which is only drawing a global relationship, while for MRC tasks the passage and the question are remarkable different in contents and literal structures and the relationship between them necessarily needs to be carefully considered. However, previous models \cite{nmt_attention,bidaf,dcmn} only obtain limited improvement when applied on the top of PrLMs even though they use very complicated structure.

Rather than seeking a complicated matching network pattern, we are inspired by the human thinking experience solving MRC problems and put forward a new network design named as \textbf{DU}al \textbf{M}ulti-head Co-\textbf{A}ttention (DUMA) to sufficiently capture relationships among passage, question and answer options for multi-choice MRC, as a result it may effectively improve the performance when cooperating with newer and more powerful PrLMs. Our model is based on the Multi-head Attention module, which is the kernel module of Transformer. Similar to BiDAF \cite{bidaf} and DCMN \cite{dcmn}, we use the bi-directional way to obtain sufficient modeling of relationships. The contributions can be summarized as:

1) For multi-choice MRC tasks, we investigate effects of previous models over Pre-trained Language Models.

2) We propose a new \textbf{DU}al \textbf{M}ulti-head Co-\textbf{A}ttention (DUMA) model which well simulates the procedure human solving MRC tasks, and show its effectiveness and superiority to previous models through extensive experiments.

3) We have reached new state-of-the-art on two benchmark multi-choice MRC tasks, DREAM and RACE.

\section{Related Works}\label{related}
\cite{nmt_attention} first propose attention mechanism for Neural Machine Translation. The jointly learning of alignment and translation effectively improves the performance. Since then, attention model has been introduced to all kinds of Natural Language Processing tasks and various of architectures has been proposed \cite{tu2020select,chen2019convolutional,gao2019generating,yan2019deep}. \cite{bidaf} uses a multi-stage architecture to hierarchically model representation of the passage, and uses a bi-directional attention flow. These works are before PrLMs was proposed, and are able to model the representations well on the top of traditional encoder such as Long Short-Term Memory (LSTM) \cite{lstm}. In fact, the experimental results show that they can still improve the representations of PrLMs, but the improvements are suboptimal. 

Based on PrLMs, \cite{ocn} propose a method to model relationship and interaction among answer options to the benefit of distinguishing them. \cite{bert_wae} ensemble a model which learns to select the wrong answer. \cite{dcmn} propose a sentence selection method to select more important sentences from passage to improve the matching representations, and considers interactions among answers for multi-choice MRC tasks. Even though the matching network design becomes more complicated, it cannot fully exploit powerful PrLMs and even cause drop on performance when applied on newer PrLMs\footnote{As shown in Table \ref{lm_and_net_compare}.}.

In a word, when applied on the top of PrLMs, previous models are not effective enough to improve the performance by a large margin. Thus inspired by the experience human solving MRC problems we design a new model which can effectively utilize well-modeled representations of PrLMs for even better performance.


\section{Task Definition}
Multi-choice MRC tasks have to handle a triplet of passage , question  and answer . When given the passage and question, the model is required to make a correct answer. The passage consists of multiple sentences, and its content can be dialogue, story, news and so on, depending on the domain of the dataset. The questions and corresponding answers are single sentences, which are usually much shorter than the passage. The target of multi-choice MRC is to select the correct answer from the candidate answer set  for a given passage and question pair , where  is the number of candidate answers. Formally, the model needs to learn a probability distribution function .

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.45 \textwidth]{overview_aaai.pdf}
	\caption{The overall architecture. Our proposed DUMA is between the Encoder and Decoder.}
	\label{fig:overview}
\end{figure}

\section{Model}
\label{sec_model}

Figure \ref{fig:overview} illustrates the overall architecture of our model. An encoder takes text input to form a global sequence representation, which is similar to human reading through the whole content for the first time to obtain an overall impression, and a decoder is to perform the answer prediction which is similar to human aggregating all the information to select the correct answer option. Our proposed Dual Multi-head Co-Attention (DUMA) layer is between the encoder and the decoder, which simulates human transposition thinking process to capture relationships of key information from passage, question and answer options.

\subsection{Encoder}
To encode input tokens into representations, we take PrLMs as the encoder. To get global contextualized representation, for each different candidate answer, we concatenate it with its corresponding passage and question to form one sequence and then feed it into the encoder. Let , ,  respectively denote the sequences of passage, question and a candidate answer, where , ,  are tokens. The adopted encoder with encoding function  takes the concatenation of ,  and  as input, namely . The encoding output  has a form , where  is a vector of fixed dimension  that represents the respective token.

\subsection{Dual Multi-head Co-Attention}



We use our proposed Dual Multi-head Co-Attention module to calculate attention representations of passage and question-answer. Figure \ref{fig:overview} shows the details of our proposed DUMA, which may be stacked as  layers. The following formula takes  for simplicity. Our model is based on the Multi-head Attention module \cite{transformer}. The proposed DUMA reuses the architecture of Multi-head Attention, while for the inputs,  and  are the same but  is another sequence representation (Note this  here denotes \textit{Query} from the original paper, different from previous  in this paper. And ,  are \textit{Key}, \textit{Value} respectively). We first separate the output representation from Encoder to obtain  and , where ,  denote the -th and -th token representation of passage and question-answer respectively and ,  are the length. Then we calculate the attention representations in a bi-directional way, that is, take 1)  as \textit{Query},  as \textit{Key} and \textit{Value}, and 2)  as \textit{Query},  as \textit{Key} and \textit{Value}.




\noindent where , , ,  are parameter matrices, , ,  denote the dimension of \textit{Query} vectors, \textit{Key} vectors and \textit{Value} vectors,  denotes the number of heads,  denotes Multi-head Attention and  denotes our Dual Multi-head Co-Attention. The  function first uses mean pooling to pool the sequence outputs of , and then aggregates the two pooled outputs through a fusing method. In Subsection \textit{\nameref{fuse_investigate}}, we investigate three fusing methods, namely element-wise multiplication, element-wise summation and concatenation. 

As shown in the Figure \ref{fig:overview}, the left part of DUMA calculates question-answer-aware passage representation, which simulates human re-reading details in the passage with impression of question and answer, and the right part calculates passage-aware question-answer representation, which simulates re-considering the question-answer with deeper understanding of the passage. The  function means fusing all the key information before deciding which is the best answer option.

\subsection {Decoder}
Our model decoder takes the outputs of DUMA and computes the probability distribution over answer options. Let  denote the -th answer option,  denote the output of -th  triplet, and  denote the correct answer option, the loss function is computed as:

where  is a learnable parameter and  denotes the number of candidate answer options.

\section{Experiments}
\begin{table}[t]\small
\renewcommand\arraystretch{1.1}
	\centering
	{
		\begin{tabular}{l|l|l}
			\hline		
			  & DREAM & RACE  \\
			\hline
			\hline
			\# of source documents & 6,444 & 27,933    \\
			\# of total questions & 10,197 & 97,687 \\
			Train/Dev/Test split & 3:1:1 &18:1:1 \\
			Extractive (\%)   & 16.3 & 13.0   \\
			Abstractive (\%)  & 83.7  & 87.0 \\
			Average answer length  & 5.3 & 5.3  \\
			\# of answers per question & 3 & 4 \\
			Avg./Max. \# of turns per dialogue & 4.7 / 48 & -  \\
			Avg. passage length & 85.9 & 321.9  \\
			Vocabulary size  & 13,037 & 136,629 \\
			\hline
		\end{tabular}
		
	}
	\caption{\label{statistics} Statistical data of DREAM and RACE dataset. \# denotes the number. ``Extractive" means the answers are spans of the passage, and ``Abstractive" means the answers are not spans.}
\end{table}

Our proposed method is evaluated on two benchmark multi-choice MRC tasks, DREAM and RACE. Table \ref{statistics} shows their data statistics, which indicates RACE is a large-scale dataset covering a broad range of domains, and DREAM is a small dataset presenting passage in a form of dialogue.

\paragraph{DREAM} DREAM \cite{dream} is a dialogue-based dataset for multiple-choice reading comprehension, which is collected from English exams. Each dialogue as the given passage has multiple corresponding questions and each question has three candidate answers. The most important feature of the dataset is that most of the questions are non-extractive and need reasoning from more than one sentence, so the dataset is small but still challenging. 
\paragraph{RACE} RACE \cite{race} is a large dataset collected from middle and high school English exams. Most of the questions also need reasoning, and domains of passages are diversified, ranging from news, story to ads.

\subsection{Evaluation}
For multi-choice MRC tasks, the evaluation criteria is accuracy, , where  denotes the number of examples the model selects the correct answer, and  denotes the number of the whole evaluation examples.

\begin{table}[t]\small
\renewcommand\arraystretch{1.3}
	\centering
	{
		\begin{tabular}{p{4.4cm}|@{}p{0.45cm}|@{}p{0.45cm}|@{}p{1.3cm}}
			\hline		
			 model &\;dev & \;test &\;source  \\
			\hline
			\hline
			BERT\cite{bert}  & \,66.0 & \,66.8 &\multirow{6}*{\,leaderboard}   \\
			BERT+WAE& \multirow{2}*{\,-} & \multirow{2}*{\,69.0}   \\
			\cite{bert_wae} &&\\
			XLNet\cite{xlnet} & \,-  & \,72.0 \\
			RoBERTa+MMM& \multirow{2}*{\,88.0} & \multirow{2}*{\,88.9}\\ \cite{mmm} &&  \\
			\hline
			ALBERT\cite{albert} & \,89.2  & \,88.5 \\
			\hline
			ALBERT+DUMA & \textbf{\,89.9} & \textbf{\,90.5}& \multirow{2}*{\,our\;\;model} \\
			\;\;\;+multi-task learning\cite{dumamulti}&\,- & \textbf{\,91.8} \\
			\hline
		\end{tabular}
	}
	\caption{\label{tab:dream_result} Results on DREAM dataset. Results with multi-task learning are reported by \cite{dumamulti}.}
\end{table}

\begin{table}[t]\small
\renewcommand\arraystretch{1}
	\centering
	{
		\begin{tabular}{p{3.3cm}|p{1.9cm}|@{}p{1.2cm}}
			\hline		
			 model  & test (M/H) & \;source  \\
			\hline
			\hline
			HAF \cite{haf}  &46.0(45.0/46.4) & \multirow{10}*{\;publication}   \\
			MRU \cite{mru}    & 50.4(57.7/47.4)   \\
			HCM \cite{hcm}   & 50.4(55.8/48.2) \\
			MMN \cite{mmn}  & 54.7(61.1/52.2)  \\
			GPT \cite{gpt} & 59.0(62.9/57.4)  \\
			RSM \cite{rsm} & 63.8(69.2/61.5)  \\
			OCN \cite{ocn} & 71.7(76.7/69.6) \\
			XLNet \cite{xlnet} & 81.8(85.5/80.2) \\
			XLNet + DCMN+ & \multirow{2}*{82.8(86.5/81.3)} \\
			\cite{dcmn} & \\
			\hline
RoBERTa + MMM & \multirow{2}*{85.0(89.1/83.3)} & \multirow{13}*{\;leaderboard}\\
			\cite{mmm}&&\\
			ALBERT (single) & \multirow{2}*{86.5(89.0/85.5)} &~\\
			\cite{albert} &&\\
			T5\cite{t5} & 87.1(-/-)&~ \\
			UnifiedQA &\multirow{2}*{89.4(-/-)} &~ \\
			\cite{unifiedqa}&&\\
			ALBERT(ensemble) & \multirow{2}*{89.4(91.2/88.6)} & ~  \\
			\cite{albert} &&\\
			Megatron-BERT (single) & \multirow{2}*{\textbf{89.5(91.8/88.6)}} &~ \\
			\cite{Megatron}&&\\
			Megatron-BERT (ensemble)\cite{Megatron} & \multirow{2}*{\textbf{90.9(93.1/90.0)}} &\multirow{2}*{~} \\
			\cline{1-3}
			ALBERT & \multirow{2}*{86.6(89.0/85.5)}\\
			 \cite{albert}  &  \\
			\hline
			ALBERT+DUMA & \textbf{88.0(90.9/86.7)} & \multirow{3}*{\;our model} \\
			ALBERT+DUMA&\multirow{2}*{\textbf{89.8(92.6/88.7)}}\\
			(ensemble) &  &  \\
			\hline
		\end{tabular}
		
	}
	\caption{\label{tab:race_result} Results on RACE dataset.}
\end{table}

\begin{table}[h]\small
\renewcommand\arraystretch{1.3}
	\centering
	{
		\begin{tabular}{l|l|l}
			\hline		
			 model & dev & test (M/H)  \\
			\hline
			\hline
			ALBERT  & 87.4  & 86.6(89.0/85.5) \\
			\hline
			ALBERT&\multirow{2}*{\textbf{88.1}(+0.7)}&\multirow{2}*{\textbf{88.0(90.9/86.7)}(+1.4)}\\
			+DUMA &  &  \\
			\hline
		\end{tabular}
		
	}
	\caption{\label{tab:race_result_1} Comparison with ALBERT baseline on RACE dataset.}
\end{table}

\begin{table}[t]\small
\renewcommand\arraystretch{1.1}
	\centering
	{
		\begin{tabular}{l|l|l}
			\hline		
			 model & dev & test  \\
			\hline
			\hline
			ALBERT & 64.51 & 64.43	 \\
			\hline
			\;\;+Vanilla SA&66.27&66.34\\
			\;\;+DUMA & 67.06 & \textbf{67.56}\\
			\;\;+TB-DUMA & \textbf{67.79} & 67.17 \\
			\hline
		\end{tabular}
		
	}
	\caption{\label{trm_and_mha} Comparison among vanilla Multi-head Self-attention, DUMA and TB-DUMA on DREAM dataset.}
\end{table}




\subsection{Experimental Settings}
Our model takes ALBERT as encoder, and use  layers of DUMA. We make the left and right part of DUMA and all the layers share parameters. Using the PrLM, our model training is done through a fine-tuning way for both tasks.

Our codes are written based on Transformers\footnote{https://github.com/huggingface/transformers}, and results of ALBERT \cite{albert}, ELECTRA \cite{clark2020electra} and BERT \cite{bert} models as baselines are our re-running unless otherwise specified.

For DREAM dataset, the learning rate is 1e-5, batch size is 8 and the warmup steps are 100. We train the model for 2 epochs in 4 hours. For RACE dataset, the learning rate is 1e-5, the batch size is 8 and the warmup steps are 1000. We train the model for 3 epochs in 2 days. For each dataset, we use FP16 training from Apex\footnote{https://github.com/NVIDIA/apex} for accelerating the training process. We train the models on eight nVidia P40 GPUs. In the following Section \textit{\nameref{ablation_sec}}, for other re-running or re-implementation including PrLM baselines and PrLM plus other models for comparison, we use the same learning rate, warmup steps and batch size as mentioned above. 

We choose the result on dev set that has stopped increasing for three checkpoints (382 steps for DREAM and 3000 steps for RACE). To obtain stable results, we run experiments 5 times with different random seeds and select the median as the ultimate performance.

\subsection{Results}
Tables \ref{tab:dream_result}, \ref{tab:race_result} and \ref{tab:race_result_1} show the experimental results. Megatron-BERT \cite{Megatron} is a variant of BERT \cite{bert} which has 8.3 billion parameters and is nearly 40 times bigger than the largest size of ALBERT, so usually it is very hard applied in practice with present common computation power and its results are not strictly comparable to our ALBERT+DUMA. Except for this, our model both achieves state-of-the-art performance on RACE leaderboard\footnote{http://www.qizhexie.com/data/RACE\_leaderboard} and DREAM leaderboard\footnote{https://dataset.org/dream/}, and it can be further improved with multi-task learning method MMM \cite{dumamulti, mmm}.

\section{Analysis Studies}\label{ablation_sec}

We perform ablation experiments on the DREAM dataset to investigate key features of our proposed DUMA, such as attention modeling ability, structural simplicity, bi-directional setting and low coupling.

\subsection{Comparison with Vanilla Self-attention and Transformer Block}\label{trm_block}

We investigate whether the improvements are simply caused by the increase of parameters. Thus we conduct the experiments of ALBERT plus vanilla Multi-head Self-attention \cite{transformer}, whose inputs , ,  are all concatenation of passage, question and answer. Results shown in Table \ref{trm_and_mha} indicate the effectiveness of our bi-directional co-attention model design.

Moreover, we observe that the original Transformer Block (TB) \cite{transformer} consists not only \textit{Multi-head Attention} module but also \textit{Layer Normalization (LN)} and \textit{Feed-Forward Network (FFN)}.  In consideration of the extensive application and great success of TB for global encoding \cite{bert,roberta,albert}, we investigate whether the Transformer Block better model the co-attention relationships than Multi-head Attention using TB-based DUMA (TB-DUMA). Experimental results shown in Table \ref{trm_and_mha} indicate TB-DUMA has no obvious difference with our DUMA in modeling relationships. However, our proposed DUMA holds more brief structure and equally effective performance.


\subsection{Comparison with Related Models}
\begin{table}[t]\small
\renewcommand\arraystretch{1.3}
	\centering
	{
		\begin{tabular}{@{}p{2.65cm}|l|p{1.85cm}}
			\hline		
			 model & ALBERT  & ELECTRA   \\
			\hline
			\hline
			baseline & 64.4/88.5 & 88.2	 \\
			\hline
			+Soft\,Attention\shortcite{nmt_attention}& 65.4(+1.0)/88.9(+0.4) & 88.8(+0.6) \\
			+BiDAF\shortcite{bidaf} & 65.6(+1.2)/89.3(+0.8) & 89.1(+0.9)\\
			+OCN\shortcite{ocn} &65.8(+1.4)/89.2(+0.7)  & 89.0(+0.8) \\
			+WAE\shortcite{bert_wae} & 66.5(+2.1)/89.9(+1.4) & 89.5(+1.3) \\
			
			+DCMN\footnote{The results of ALBERT+DCMN are our re-running of the official codes which we obtained through personal communication with its authors.}\shortcite{dcmn} & 63.3(-1.1)/87.8(-0.7)  & 87.7(-0.5)\\
			\hline
			+DUMA & \textbf{67.6(+3.2)/90.5(+2.0)} & \textbf{89.8(+1.6)}  \\
			\hline
		\end{tabular}
	}
	\caption{\label{attention_comparison} Comparison among different models on DREAM dataset.}
\end{table}

We compare our attention model with several representative works, which have been discussed in Section \textit{\nameref{related}}. Soft Attention \cite{nmt_attention} and BiDAF \cite{bidaf} are originally based on traditional encoder such as LSTM \cite{lstm}, and DCMN \cite{dcmn}, OCN \cite{ocn}, WAE\cite{bert_wae} are based on BERT \cite{bert}. For fair comparison with Soft Attention, we simply use it to replace the attention score computing in our model. 

Table \ref{attention_comparison} compares the effectiveness of various model designs, and our proposed DUMA outperforms all other models. The performance of Soft Attention is much lower than our DUMA, which indicates the DUMA's similarity in structure with ALBERT (both use Multi-head Attention) makes it better to utilize information from encoded representation. Even though BiDAF has been a successful attention model since a long time ago, it is suboptimal for PrLMs. WAE uses an ensemble model design with nearly twice sized parameters as our model. DCMN adopts a much more complicated model structure design for better matching, but the result with ALBERT and ELECTRA is not satisfactory, which indicates it may be specially optimized for specific PrLM, while our DUMA achieves the absolutely highest accuracy with a intuitive structure design. In fact, our DUMA has nice generalization ability because it also works well with many kinds of PrLMs.

\subsection{Investigation of Fusing Method} \label{fuse_investigate}

\begin{table}[t]\small
\renewcommand\arraystretch{1.3}
	\centering
	{
		\begin{tabular}{l|l|l}
			\hline		
			 model & dev & test  \\
			\hline
			\hline
			ALBERT & 64.51 & 64.43	 \\
			\hline
			element-wise multiplication & 65.29 &64.58\\
			element-wise summation & 66.32 & 65.51 \\
			concatenation & \textbf{67.06} & \textbf{67.56}\\
			\hline
		\end{tabular}
		
	}
	\caption{\label{fusing_methods} Comparison among different implementation of the fusing method on DREAM dataset. The last three rows are our DUMA applying three kinds of implementations.}
\end{table}


We investigate different implementations of fusing function from equation (\ref{eq_fuse}), namely element-wise multiplication, element-wise summation and concatenation. The results are shown in Table \ref{fusing_methods}. We see that concatenation is optimal because it retains the matching information and lets network learn to fuse them dynamically.


\subsection{Number of Parameters}
\begin{table}[t]\small
\renewcommand\arraystretch{1.3}
	\centering
	{
		\begin{tabular}{p{3.15cm}|p{3.2cm}}
			\hline		
			 model & para. num.  \\
			\hline
			\hline
			ALBERT & 11.7M	 \\
			\hline
			\;+Soft Attention\shortcite{nmt_attention} & 13.5M\;(+1.8M)\;(+15.4\%)   \\
			\;+BiDAF\shortcite{bidaf} &  12.0M\;(+0.3M)\;(+2.6\%) \\
			\;+OCN\shortcite{ocn} & 14.8M\;(+3.1M)\;(+26.5\%)  \\
			\;+WAE\shortcite{bert_wae} & 23.4M\;(+11.7M)\;(+100\%)  \\
			\;+DCMN\shortcite{dcmn} & 19.4M\;(+7.7M)\;(+65.8\%)  \\
			\hline
			\;+DUMA & 13.5M\;(+1.8M)\;(+15.4\%) \\
			\hline
		\end{tabular}
		
	}
	\caption{\label{parameter_comparison} Comparison of number of parameters among different models. The models are same as listed in Table \ref{attention_comparison}.}
\end{table}
We compare number of parameters among different models in Table \ref{parameter_comparison}. BiDAF requires the least model enlargement, however it is far less effective than our model. Besides, our model enlargement is far less than DCMN. In a word, our DUMA can obtain the best performance while requiring a little model enlargement.



\begin{figure}[h]\small\centering
	\subfigure[]{
		\begin{minipage}[]{0.45\linewidth}
			\includegraphics[width=1\textwidth]{multi_layer.png}\label{multi_layer}
		\end{minipage}
	}
	\subfigure[]{
		\begin{minipage}[]{0.45\linewidth}
			\includegraphics[width=1\textwidth]{multi_layer_1.png}\label{multi_layer_1}
		\end{minipage}
	}
	\caption{(a) Different numbers of DUMA layers on DREAM dataset. (b) Different numbers of TB-DUMA layers on DREAM dataset.} 
\end{figure} 
\subsection{Number of DUMA Layers}




We stack 2 layers of our DUMA, that is to make passage and question-answer interact more than once to obtain deeper representations. Besides, we make different layers share parameters, which is the same as ALBERT. 

Figure \ref{multi_layer} shows the results. We can see that as the number of layers increases the performance fluctuates, and too many layers even lead to slight drop. It is much like when human solving MRC tasks, excessive thinking and hesitation may make them misunderstand the meaning of some information. For the network with current number of parameters, it shows that interacting twice is enough to capture the key information, and stacking too many layers may disorder the well-modeled representations and make the model harder trained. 

Note that PrLMs \cite{bert,roberta,albert} stacks Transformer Blocks (described in Subsection \textit{\nameref{trm_block}}) instead of Multi-head Attention modules, which raises a doubt that it is the lack of LN and FFN that makes the DUMA improper for stacking deeper network. Thus we further conduct an experiment with TB-DUMA (the same as described in Subsection \textit{\nameref{trm_block}}). Experimental results in Table \ref{multi_layer_1} show the same performance trend as the original DUMA, which again verifies the effectiveness of our DUMA design. 

\subsection{Effect of Bi-direction}
\begin{table}[t]\small
\renewcommand\arraystretch{1.1}
	\centering
	{
		\begin{tabular}{@{}p{1.7cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}}
			\hline		
			 \;model & dev & test & avg \\
			\hline
			\hline
			\;ALBERT & 64.51 & 64.43 &64.47 \\
			\hline
			\;P-to-Q & 64.61\;(+0.10) & 64.72\;(+0.29) & 64.67\;(+0.20) \\
			\;Q-to-P & 66.76\;(+2.25) & 66.29\;(+1.86) & 66.53\;(+2.06)\\
			\;Both(DUMA) & 67.06\;(\textbf{+2.55}) & 67.56\;(\textbf{+3.13}) & 67.31\;(\textbf{+2.84})\\
			\hline
		\end{tabular}
		
	}
	\caption{\label{single_direction} Bi-directional vs. uni-directional attentions on DREAM dataset.}
\end{table}

As figured out by \cite{dcmn}, bi-directional matching is a very important feature for sufficiently modeling the relationship between passage and question. To investigate the effect, we perform experiments on two settings, namely P-to-Q only and Q-to-P only. In other words, we respectively remove the right part and left part of our DUMA. Table \ref{single_direction} shows the results. We see that for bi-directional model, the overall improvement is 2.84\%, while for uni-directional model the improvement is only 2.06\% at most. The setting of bi-direction effectively improves the performance, which reveals its efficiency for modeling the relationship and agrees to the conclusion of \cite{dcmn}. Also it is the same as our intuitive understanding that all the passage, question and answer options should be deliberated.

\subsection{Cooperation with PrLMs} \label{with_bert}
\begin{table}[t]\small
\renewcommand\arraystretch{1.2}
	\centering
	{
		\begin{tabular}{p{1.6cm}|p{1.6cm}|p{1.6cm}|p{1.6cm}}
			\hline		
			 model & dev & test &avg \\
			\hline
			\hline
			ALBERT & 64.51 & 64.43 &64.47 \\
			\;\;+DUMA &  67.06\;(+2.55) & 67.56\;(+3.13) & 67.31\;(+2.84)\\
			\hline
			BERT & 61.18  &  61.54 & 61.36 \\
			\;\;+DUMA & 64.82\;(+3.64) & 64.03\;(+2.49) & 64.43\;(+3.07) \\
			\hline
		\end{tabular}
		
	}
	\caption{\label{bert_result} Results using BERT as encoder on DREAM dataset. Results of BERT are our re-running.}
\end{table}

Though the proposed DUMA is supposed to enhance state-of-the-art PrLM like ALBERT and ELECTRA, we claim that it is generally effective for less advanced models. Thus we simply replace the adopted ALBERT by its early variant BERT to examine the effectiveness of DUMA. Table \ref{bert_result} shows the results. We see that our model can be easily transferred to other PrLMs, thus it can be seemed as an effective module for modeling relationships among passage, question and answer for Multi-choice MRC.

\subsection{Effect of Self-attention}
\begin{table}[t]\small
\renewcommand\arraystretch{1.1}
	\centering
	{
		\begin{tabular}{l|l|l}
			\hline		
			 model & dev & test  \\
			\hline
			\hline
			ALBERT (SA) & 64.51 & 64.43 \\
			ALBERT (SA) + DUMA (CA) & \textbf{67.06} & \textbf{67.56} \\
			ALBERT (SA+CA) & 41.18  &  40.08 \\
			\hline
		\end{tabular}
		
	}
	\caption{\label{only_co_attention} Results with and without self-attention on DREAM dataset. ``SA" means self-attention and ``CA" means co-attention. ``SA+CA" means straightforwardly using CA to replace SA in ALBERT.}
\end{table}
Our overall architecture can be split into two steps from the view of attention, of which the first is self-attention (ALBERT) and the second is co-attention (DUMA). To examine whether the structure can be further simplified, that is only using co-attention, we straightforwardly change all of the Multi-head Self-attention of ALBERT model to our Dual Multi-head Co-attention, while still using its pre-trained parameters. The results are shown in Table \ref{only_co_attention}, showing that putting co-attention directly into ALBERT model may lead to much poorer performance compared to the original ALBERT and our ALBERT+DUMA integration way. To conclude, a better way for modeling is our PrLM plus DUMA model, which is to firstly build a global relationship using self-attention of the well trained encoder and then further enhance the relationship between passage and question-answer and distill more matching information using co-attention.
\section{Comparison of Predictions}

Table \ref{prediction_comparison} shows a hard example which needs to capture important relationships and matching information. Benefited from well-modeled relationship representations, DUMA can better distill important matching information between passage and question-answer.

\begin{table}[h]\small
\renewcommand\arraystretch{1.3}
	\centering
	\begin{tabular}{p{1.2cm}|p{1.1cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}}
		\hline
		\multirow{4}*{Passage} & \multicolumn{4}{l}{Woman: \textit{So, you have three days off, what}}\\ 
		& \multicolumn{4}{l}{\textit{are you going to do?}} \\
		& \multicolumn{4}{l}{Man: \textit{Well, I probably will rent some movi-}} \\
		& \multicolumn{4}{l}{\textit{es with my friend bob.}} \\
		\hline
		Question & \multicolumn{4}{l}{\textit{What will the man probably do?}} \\
		\hline
		\multirow{3}{1cm}{Answer options}& \multicolumn{4}{l}{1) \textit{Ask for a three-day leave.}} \\ 
		& \multicolumn{4}{l}{2) \textit{Go out with his friend.}} \\ 
		& \multicolumn{4}{l}{3) \textit{Watch films at home.}\;\;\;} \\
		\hline
		ALBERT&+BiDAF&+Sf Att&+DCMN&+DUMA \\
		\hline
		Prediction&\multicolumn{1}{c|}{1)}&\multicolumn{1}{c|}{1)}&\multicolumn{1}{c|}{2)}&\multicolumn{1}{c}{3)\;\;}\\
		\hline
	\end{tabular}
	\caption{\label{prediction_comparison} Predictions of different models which are same as in Table \ref{attention_comparison}. ``Sf Att'' means Soft Attention.}
	\end{table}
\section{Conclusion}

In this paper, we simulates human transposition thinking experience when solving MRC problems and propose a novel \textbf{DU}al \textbf{M}ulti-head Co-\textbf{A}ttention (DUMA) to model the relationships among passage, question and answer for multi-choice MRC tasks, which is able to cooperate with popular large-scale Pre-trained Language Models and brings effective performance improvements. Besides, we investigate previous attention mechanisms or matching networks applied on the top of PrLMs, and our model is shown as optimal through extensive experiments, which achieves the best performance with an intuitive motivated structure design. Our proposed DUMA enhancement has been verified effective on two benchmark multi-choice MRC tasks, DREAM and RACE, which achieves new state-of-the-art over strong PrLM baselines.

\bibliography{aaai2021_arxiv.bib}
\bibliographystyle{aaai21}
\end{document}
