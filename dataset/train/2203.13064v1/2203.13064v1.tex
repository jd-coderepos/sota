\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{authblk}

\usepackage{acl}


\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{multirow}


\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}



\usepackage{lipsum}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}



\title{Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction}




\author{Maksym Tarnavskyi\thanks{\ This research was performed during Maksym Tarnavskyi's work on Ms.Sc. thesis at Ukrainian Catholic University \cite{tar2021}.}}
\author[]{Artem Chernodub}
\author[]{Kostiantyn Omelianchuk}
\affil[]{Ukrainian Catholic University, Faculty of Applied Sciences, \texttt{tarnavskyi@ucu.edu.ua}}
\affil[]{Grammarly, \texttt{firstname.lastname@grammarly.com}}

\begin{document}

\maketitle
\begin{abstract}
In this paper, we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations. We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size. Our best ensemble achieves a new SOTA result with an  score of 76.05 on BEA-2019 (test), even without pre-training on synthetic datasets. In addition, we perform knowledge distillation with a trained ensemble to generate new synthetic training datasets, "Troy-Blogs" and "Troy-1BW". Our best single sequence tagging model that is pretrained on the generated Troy- datasets in combination with the publicly available synthetic PIE dataset achieves a near-SOTA\footnote{To the best of our knowledge, our best single model gives way only to much heavier T5 model \cite{rothe2021a}.} result with an  score of 73.21 on BEA-2019 (test). The code, datasets, and trained models are publicly available.\footnote{\url{https://github.com/MaksTarnavskyi/gector-large}}
\end{abstract}

\section{Introduction}

The purpose of the Grammatical Error Correction (GEC) task is to correct grammatical errors in natural texts. This includes correcting errors in spelling, punctuation, grammar, morphology, word choice, and others. An intelligent GEC system receives text containing mistakes and produces its corrected version. The GEC task is complicated and challenging: the accuracy of edits, inference speed, and memory limitations are topics of intensive research.

Currently, Machine Translation (MT) is the mainstream approach for GEC. In this setting, errorful sentences correspond to the source language, and error-free sentences correspond to the target language. Early GEC-MT methods leveraged  phrase-based statistical machine translation (PBSMT) \cite{yuan-felice-2013-constrained}. Then this approach rapidly evolved to seq2seq Neural Machine Translation (NMT) based on gated recurrent neural networks \cite{yuan2016grammatical} and recent powerful Transformer-based seq2seq models. Transformer-based models autoregressively capture the full dependency among output tokens; however, inference can be slow due to sequential decoding. \citet{grundkiewicz-etal-2019-neural} leveraged a Transformer model  \cite{vaswani2017attention} that was pre-trained on synthetic GEC data and right-to-left re-ranking for ensemble. \citet{kaneko2020encoder} adopted several strategies of BERT \cite{devlin2018bert} usage for GEC. Recently, \citet{rothe2021a} built their system on top of T5 \cite{xue-etal-2021-mt5}, a xxl version of the T5 Transformer encoder-decoder model and reached new state-of-the-art results (11B parameters). 

While still not as widespread as MT, the sequence tagging approach for GEC, which generates a sequence of text edit operations encoded by tags for errorful input text is becoming more common. LaserTagger \cite{malmi-etal-2019-encode} is a sequence tagging model that casts text generation as a text editing task.  Corrected texts are reconstructed from the inputs using three main edit operations: keeping a token, deleting it, and adding a phrase before the token. LaserTagger combines a BERT encoder with an autoregressive Transformer decoder, which predicts edit operations. The Parallel Iterative Edit (PIE) model \cite{awasthi2019parallel} does parallel decoding, achieving quality that is competitive with the seq2seq models.\footnote{\url{http://nlpprogress.com/english/grammatical_error_correction}} It predicts edits instead of tokens and iteratively refines predictions to capture dependencies.\blfootnote{The paper has been accepted for publication at 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022).} A similar approach is presented in \cite{omelianchuk2020gector}. The GECToR system achieves competitive results using various Transformers as an encoder; and linear layers with softmax for tag prediction and error detection. By replacing an autoregressive decoder with linear output layers, itâ€™s also potentially several times faster than seq2seq systems.

Today, the generation of synthetic data is becoming significant for most GEC models. Natural languages are rich, and their grammars contain many rules and exceptions; therefore, professional linguists are often utilized to annotate high-quality corpora for further training ML-based systems mostly in a supervised manner \cite{dahlmeier2013building}, \cite{bryant2019the}. However, human annotation is expensive, so researchers are working on methods for augmentation of training data, synthetic data generation, and strategies for its efficient usage \cite{lichtarge2019corpora}, \cite{kiyono2019an}, \cite{stahlberg2021synthetic}. The majority of GEC systems today use synthetic data to pre-train Transformer-based components of their models.

In this work, we are focusing on exploring sequence tagging models and their ensembles. Although most of our developments may eventually be applied to other languages, we work with English only in this study. Being a resource-rich language, English is a highly competitive area for the GEC task.



\section{Base System Overview}
\subsection{GECToR architecture} 
Our tagging models are inherited from GECToR \cite{omelianchuk2020gector}. To date, GECToR shows near-SOTA results on CoNLL-2014 and BEA-2019 benchmarks. It is based on AllenNLP \cite{Gardner2017AllenNLP} and  HuggingFace Transformers \cite{wolf2019huggingface}, and its source code is freely available.\footnote{\url{https://github.com/grammarly/gector}}

GECToR is a sequence tagging model that contains a Transformer-based encoder stacked with two output linear layers that are responsible for error detection and error correction. The model is trained with a cross-entropy loss function to produce tags that encode token-level edits. Then iterative postprocessing is performed. GECToR predicts the tag-encoded transformations for each token in the input sequence; it can then apply these transformations to get the modified output sequence. 

Since some corrections in a sentence may depend on others, applying the GEC sequence tagger only once may not be enough to correct the sentence entirely. Therefore, GECToR uses an iterative correction approach, modifying  the sentence by repeatedly running it through the model (up to four times) (Fig. \ref{fig_gector}).

\begin{figure}[!h]
\centering
\includegraphics[width=1.0\columnwidth]{images/gector_architecture.png} 
\caption{The GECToR model's iterative pipeline for sequence tagging and sentence modification.} \label{fig_gector}
\end{figure}

\subsection{Tag-encoded edit operations} As in GECToR, our primary edit operations are encoded by the following tags: \textit{\DELETE} (delete the current token), \textit{\t_{1}t_{1}REPLACE}\_  (replace the current token with the token ). GECToR also has special edit operations, such as changing the case of a token, changing the verb form to express a different number or tense, or converting singular nouns to plural, and other.  We refer to \cite{omelianchuk2020gector} for the details of edit transformations.

\subsection{Our contributions} 

We claim the following contributions:

1. We empirically investigate and improve the GECToR sequence tagging system \cite{omelianchuk2020gector} by upgrading the Transformer encoders to Large configurations, leveraging an advanced tokenizer, performing additional filtering of edits-free sentences, and increasing the vocabulary size.

2. We show that the ensembling of sequence taggers by majority votes on output edit spans provides better performance compared to ensembling by averaging output tag probabilities while staying tolerant to the models' architecture and vocabulary sizes.

3. We apply the knowledge distillation method to produce annotated data using ensemble of sequence taggers. When trained on the distilled data, single GEC tagging models show competitive performance.

4. We make the code, datasets, and trained models publicly available.

\section{Datasets}
 \begin{table}
 \scriptsize
 \begin{tabular}{|l|c|c|c|c|c|c|}
 \hline

 \textbf{Dataset} & \textbf{Type}  &  \textbf{Part} & \textbf{\# Sent.} & \textbf{\# Tokens} & \textbf{\% Edits} \\

  \hline
        Lang-8  & Ann        & Train          & 1.04M             & 11.86M & 42\%\\
 \hline
       NUCLE    & Ann        & Train          & 57k               & 1.16M & 62\% \\
 \hline
        FCE      & Ann        & Train          & 28k               & 455k & 62\% \\
  \hline
                &                   & Train          & 34.3k             & 628.7k &  67\%  \\
         W\&I   & Ann         & Dev            & 3.4k              & 63.9k &  69\% \\
                &                   & Test           & 3.5k              & 62.5k &   N/A \\
  \hline

         LOCNESS    & Ann         & Dev            & 1k              & 23.1k & 52\% \\
                &                   & Test           & 1k              & 23.1k &   N/A\\
  \hline
        1BW     & Mon       & N/A           &  115M  &  0.8B & N/A \\
 \hline        
        Blogs     & Mon      & N/A           & 13.5M  & 171M & N/A \\
\hline
        Troy-1BW     & Dis       & Train           &  1.2M  & 30.88M & 100\% \\
 \hline        
        Troy-Blogs     & Dis      & Train           & 1.2M  & 21.49M & 100\% \\
\hline
PIE     & Syn        & Train           & 1.2M  & 30.1M & 100\%\\


\hline
\end{tabular}
\caption{Description and statistics of datasets used in this work. Dataset types: (Ann)otated, (Syn)thetic, (Mon)olingual, and (Dis)tilled. Combined, these datasets form the \textit{Joint Train Dataset}. BEA-2019 dev/test parts are concatenations of W\&I and LOCNESS dev/test parts. Only parts of the original corpora from the cited sources are used in our work. }\label{training-data-table-new}
\end{table} 




\subsection{Annotated data} For training single models and ensembles, we use parallel annotated data from the Lang-8 Corpus of Learner English (Lang-8)\footnote{\url{https://sites.google.com/site/naistlang8corpora}} \cite{tajiri2012tense}, the National University of Singapore Corpus of Learner English (NUCLE)\footnote{\url{https://www.comp.nus.edu.sg/~nlp/corpora.html}} \cite{dahlmeier2013building}, the First Certificate in English dataset (FCE)\footnote{\url{https://ilexir.co.uk/datasets/index.html}} \cite{yannakoudakis2011new}, and the Write \& Improve (W\&I) Corpus \cite{bryant2019bea}.\footnote{\url{https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz}} Please, see Table \ref{training-data-table-new} for details.

\subsection{Monolingual data, distilled data} For knowledge distillation from the ensemble, we use parts of two monolingual datasets: the One Billion Word Benchmark (1BW)\footnote{\url{http://statmt.org/wmt11/training-monolingual.tgz}} \cite{one2013bw} and the Blog Authorship Corpus (Blogs)\footnote{\url{https://www.kaggle.com/rtatman/blog-authorship-corpus}} \cite{schl2005effec}. Corresponding distilled datasets have prefixes "Troy-"; see more details about their generation in Section 6.
\subsection{Synthetic data} 
After knowledge distillation for the final training of the student model we also use parallel sentences with synthetically generated grammatical errors from the PIE dataset \cite{awasthi-etal-2019-parallel}.\footnote{\url{https://github.com/awasthiabhijeet/PIE/tree/master/errorify}}
\subsection{Evaluation} We report , , and   metrics computed by ERRANT scorer \cite{bryant2017automatic} on dev and test datasets from the W\&I + LOCNESS Corpus from the BEA-2019 GEC Shared Task \cite{bryant2019the}. 



\section{Our System's Design}

\subsection{Tokenization} In the original GECToR system, the Byte-Pair Encoding (BPE) tokenizer \cite{sennrich-etal-2016-neural} uses a custom implementation.\footnote{\url{https://github.com/google/sentencepiece}} This was chosen because the out-off-the-box AllenNLP tokenizer was too slow, and HuggingFace Transformers' tokenizers did not provide a BPE-to-words mapping. Our work is fully implemented with Transformers from the HuggingFace Transformers library. In particular, we moved to the recently released fast tokenizers from HuggingFace. Now, our encoders have the same tokenizers for fine-tuning as they had for initial pretraining, which leads to better quality after fine-tuning.

\subsection{Initialization and training setup} Our encoder is loaded with its default pretrained weights; the linear layers' weights are initialized with random numbers. Our models are trained by Adam optimizer \cite{kingma1412adam} with default hyperparameters. We use a multi-class categorical cross-entropy loss function. The early stopping technique is used: Stopping criteria is 3 epochs without improving the loss function on the dev set, which is a random 2\% sample from the same source as training data and is different for each stage.


\subsection{Training stages} Model training is performed in several stages (Table \ref{train-stages}). In Stage I, the model is pretrained on synthetic datasets; this stage is optional. Then, in Stage II, we carry out warm-up training on the \textit{Joint Train Dataset}, which contains the Lang-8, NUCLE, FCE, and W\&I datasets (Table \ref{training-data-table-new}). Thus, we perform coarse fine-tuning on a large amount of diverse GEC data. Datasets are used sequentially with no shuffling. In order not to adversely impact the out-of-box pretrained weights of the encoder, during the first two epochs we train only the linear layers (so-called "cold epochs"); later, we make all model's weights trainable. 

In Stage III, we continue fine-tuning on the W\&I Train dataset, which contains only the highest-quality data. Another difference between Stages II and III is the share of edit-free sentences in the training data. We observed that too many sentences in training data without edits lead to reducing the appearance rate of the tagger and deteriorating the overall quality. Therefore, we filter out edit-free sentences from the Joint Train Dataset, which is used in Stage II. In Stage III, we fine-tune the model on the unfiltered version of the W\&I Train dataset. 

\begin{table}[htbp]
\scriptsize
\centering
\begin{tabular}{|l|ccc|ccc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Training}\\ \textbf{stage} \#\end{tabular}}} & \multicolumn{3}{c|}{\textbf{Base}} & \multicolumn{3}{c|}{\textbf{Large}} \\ \cline{2-7} 
\multicolumn{1}{|c|}{}                                                                            & \textbf{P}           & \textbf{R}           &            & \textbf{P}          & \textbf{R}          &           
\\ \hline
Stage I.  &   N/A & N/A	 & N/A   &   N/A & N/A & N/A   \\ 
Stage II.  &   50.12 & 34.04	 & 45.79    &   52.11 & 37.34 & 48.29   \\ 
Stage III.   &  53.77 & \textbf{39.23}& 50.06      &   54.85 & \textbf{42.54} & 51.85   \\ 
Inf. tweaks &\textbf{62.49} & 32.26	 & \textbf{52.63}  & \textbf{65.76} & 33.86 &\textbf{55.33}   \\

\hline
\end{tabular}
\caption{\label{train-stages} Performance of our system with a RoBERTa encoder (in Base and Large configurations) after each training stage and inference tweaks on BEA-2019 (dev). Pre-training on synthetic data (Stage I) as was done in \cite{omelianchuk2020gector} is not performed.}
\end{table}

The final stage is inference tweaks \cite{omelianchuk2020gector} for balancing between the model's precision and recall. This is done by introducing additional hyperparameters: additional confidence (AC) to the probability for the \textit{\\mathbf{F_{0.5}}\mathbf{F_{0.5}}F_{0.5}APPEND\_it}, \textit{\\mathbf{P}\mathbf{R}\mathbf{F_{0.5}}^{(L)}_{5K}^{(L)}_{5K}^{(L)}_{5K}^{(L)}_{10K}^{(L)}_{10K}^{(L)}_{10K}\mathbf{P}\mathbf{R}\mathbf{F_{0.5}}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(B)}^{(L)}^{(B)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(B)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}\mathbf{\oplus}"+"\oplusF_{0.5}\mathbf{P}\mathbf{R}\mathbf{F_{0.5}}^{(L)}^{(L)}^{(L)}^{(L)}\oplus^{(L)}\oplus^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}\oplus^{(L)}\oplus^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}\oplus^{(L)}\oplus^{(L)}^{(L)}^{(L)}^{(L)}^{(L)}\oplus^{(L)}\oplus^{(L)}\oplusN_{min}N_{min}1 \leq N_{min} \leq N_{single\_models}N_{min}N_{min} = 1N_{min}N_{min}N_{single\_models} - 1\mathbf{N_{single\_models}}\mathbf{N_{min}}\mathbf{P}\mathbf{R}\mathbf{F_{0.5}}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(B)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}N_{min}\mathbf{P}\mathbf{R}\mathbf{F_{0.5}}^{(L)}_{5K}\oplus^{(L)}_{5K}\oplus^{(L)}_{5K}^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{10K}^{(L)}_{5K}\oplus^{(L)}_{10K}\oplus^{(L)}_{5K}^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{5K}^{(L)}_{10K}\oplus^{(L)}_{10K}\oplus^{(L)}_{5K}F_{0.5}76.05F_{0.5}=73.70^{(L)}_{5K}^{(L)}_{5K}^{(L)}_{5K}^{(L)}_{5K}F_{0.5}^{(L)}_{5K}\mathbf{F_{0.5} = 73.21}F_{0.5} = 71.5^{(B)}_{5K}F_{0.5} = 72.4^{(B)}_{5K}\mathbf{F_{0.5}}^{(L)}_{5K}^{(L)}_{5K}_{10K}^{(L)}\oplus_{10K}^{(L)}\oplus_{5K}^{(L)}^{(L)}_{5K}F_{0.5} = 55.81F_{0.5} = 72.69F_{0.5} = 76.05F_{0.5} = 73.21/72.69\mathbf{F_{0.5}}^{(L)}_{5K}^{(L)}_{5K}_{10K}^{(L)}\oplus_{10K}^{(L)}\oplus_{5K}^{(L)}$
 (this work) & 76.1 & 41.6 & 65.3 \\
\hline
\end{tabular}
\caption{\label{table_related_nucle} Comparison of our best single tagging models and ensembles with related work on CoNLL-14 (test).}
\end{table}

\end{document}
