\section{Experiments}
\label{sec:exp}

\myparagraph{Datasets.} We evaluate our method on the two popular object detection datasets, \ie, COCO~\cite{lin2014microsoft} and LVIS~\cite{gupta2019lvis}. For the COCO dataset, we follow OV-RCNN~\cite{zareian2021open} to split the object categories to 48 base categories and 17 novel categories. For the LVIS dataset, we follow ViLD~\cite{gu2021open} to split the 337 rare categories into novel categories and the rest common and frequent categories into base categories. For brevity, we denote the open-vocabulary benchmarks based on COCO and LVIS as OV-COCO and OV-LVIS. 

\myparagraph{Evaluation Metrics.} We evaluate the detection performance on both base and novel categories for completeness. 
For OV-COCO, we follow OV-RCNN~\cite{zareian2021open} to report the box AP at IoU threshold 0.5, noted as AP. 
For OV-LVIS, we report both the mask and box AP averaged on IoUs from 0.5 to 0.95, noted as mAP.
The AP of novel categories (AP) and mAP of rare categories (AP) are the main metrics that evaluate the open-vocabulary detection performance on OV-COCO and OV-LVIS, respectively. 

\myparagraph{Implementation Details.} 
We build \method~on Faster R-CNN~\cite{ren2015faster} with ResNet50-FPN~\cite{lin2017fpn}.
For a fair comparison with existing methods, we initialize the backbone network with weights pre-trained by SOCO~\cite{wei2021aligning} and apply synchronized Batch Normalization (SyncBN)~\cite{megdet} following DetPro~\cite{Du_2022_CVPR}. We choose the  schedule (180, 000 iterations) for the main experiments on COCO and LVIS~\cite{mmdetection, wu2019detectron2}. For the pre-trained VLM, we choose the CLIP~\cite{radford2021learning} model based on ViT-B/32~\cite{ViT}. For the adaptation to caption supervision, we base our method on Faster R-CNN with ResNet50-C4 backbone~\cite{ren2015faster} and adopt the  schedule. For the prompt of category names, we use the hand-crafted prompts in ViLD~\cite{gu2021open} for all our experiments by default.
We use learned prompt only when comparing with DetPro~\cite{Du_2022_CVPR}.

\begin{table*}[t]
  \small
  \centering
\caption{Comparison with state-of-the-art methods on OV-LVIS. * denotes the re-implemented ViLD~\cite{gu2021open} reported in DetPro~\cite{Du_2022_CVPR}.
}
\vspace{-8pt}
 {	
 \tablestyle{3pt}{1.1}
  \scalebox{0.9}{
\begin{tabular}{l|c|c|c>{\color{gray}}c>{\color{gray}}c>{\color{gray}}c|c>{\color{gray}}c>{\color{gray}}c>{\color{gray}}c}
  \toprule
 \multirow{2}{*}{Method}&\multirow{2}{*}{Ensemble}&\multirow{2}{*}{Learned Prompt} &\multicolumn{4}{c|}{Object Detection}&\multicolumn{4}{c}{Instance segmentation}\\
  &&&AP&AP&AP&AP&AP&AP&AP&AP\\ \shline
ViLD~\cite{gu2021open}& - & - & 16.3 &21.2 &31.6& 24.4 &16.1 &20.0 &28.3 &22.5\\
OV-DETR~\cite{zang2022open}& - &- & - & - & - & - & 17.4 &25.0& 32.5&26.6 \\
BARON (Ours)& - & - & \textbf{17.3} &25.6 &31.0 &26.3 &\textbf{18.0}&24.4 &28.9 &25.1 \\ \hline
ViLD~\cite{gu2021open} & \checkmark &- &16.7&26.5&34.2&27.8 & 16.6 & 24.6 & 30.3 & 25.5\\
ViLD*~\cite{gu2021open} & \checkmark &- &17.4&27.5&31.9&27.5&16.8&25.6&28.5&25.2\\
BARON (Ours) &\checkmark & -& \textbf{20.1} &28.4 &32.2 &28.4 &\textbf{19.2} &26.8 &29.4 &26.5\\\hline
DetPro~\cite{Du_2022_CVPR} &\checkmark & \checkmark& 20.8 &27.8&32.4&28.4&19.8 &25.6&28.9&25.9\\
BARON (Ours) & \checkmark & \checkmark &\textbf{23.2}&29.3&32.5&29.5&\textbf{22.6}&27.6&29.8&27.6\\
\bottomrule
\end{tabular}
}


 }
\label{tab:lvis_main}
\end{table*}

\begin{table*}[t]
  \small
  \centering
\vspace{-2pt}
\caption{Comparison of the transfer ability of the model trained on OV-LVIS. * denotes the re-implemented ViLD~\cite{gu2021open} reported in DetPro~\cite{Du_2022_CVPR}.  denotes that we use hand-crafted prompts for a fair comparison with ViLD.}
\vspace{-8pt}
  { \tablestyle{3pt}{1.1}
  \scalebox{0.9}{
  \begin{tabular}{l|cc|cccccc|cccccc}
  \toprule
 \multirow{2}{*}{Method}&\multicolumn{2}{c|}{Pascal VOC}&\multicolumn{6}{c|}{COCO}&\multicolumn{6}{c}{Objects365}\\
  &AP&AP&AP&AP&AP&AP&AP&AP&AP&AP&AP&AP&AP&AP\\
  \midrule
Supervised~\cite{Du_2022_CVPR}&78.5&49.0&46.5&67.6&50.9&27.1&67.6&77.7&25.6&38.6&28.0&16.0&28.1&36.7\\
  \midrule
ViLD*~\cite{gu2021open}&73.9&57.9&34.1&52.3&36.5&21.6&38.9&46.1&11.5&17.8&12.3&4.2&11.1&17.8\\
BARON (Ours)&\textbf{74.5}&\textbf{57.9}&\textbf{36.3}&\textbf{56.1}&\textbf{39.3}&\textbf{25.4}&\textbf{39.5}&\textbf{48.2}&\textbf{13.2}&\textbf{20.0}&\textbf{14.0}&\textbf{4.8}&\textbf{12.7}&\textbf{20.1}\\
 \midrule
DetPro~\cite{Du_2022_CVPR} &74.6&57.9&34.9&53.8&37.4&22.5&39.6&46.3&12.1&18.8&12.9&4.5&11.5&18.6\\
BARON (Ours) &\textbf{76.0}&\textbf{58.2}&\textbf{36.2}&\textbf{55.7}&\textbf{39.1}&\textbf{24.8}&\textbf{40.2}&\textbf{47.3}&\textbf{13.6}&\textbf{21.0}&\textbf{14.5}&\textbf{5.0}&\textbf{13.1}&\textbf{20.7}\\

\bottomrule
\end{tabular}
} }
\vspace{-10pt}
\label{tab:transfer}
\end{table*}

\subsection{Benchmark Results}
\myparagraph{OV-COCO.}
We report the comparison with previous methods in Table~\ref{tab:coco_main}.
BARON surpasses previous state of the arts with either pre-trained VLMs or COCO captions~\cite{chen2015microsoft}, indicating its effectiveness and flexibility.
Note that OV-DETR is based on the Deformable DETR~\cite{zhu2020deformable}, which is stronger than the Faster R-CNN~\cite{ren2015faster} with higher performance on base categories. 
But BARON still outperforms OV-DETR by 4.6 AP on novel categories.
When using caption supervision, BARON even outperforms PB-OVD~\cite{gao2021towards} that uses sophisticated pseudo-labeling. Combining CLIP image features, COCO captions, and MAVL~\cite{maaz2022class} proposals, BARON significantly outperforms Rasheed \etal~\cite{rasheedbridging} by a large margin.

\myparagraph{OV-LVIS.}
We compare BARON with other methods on the OV-LVIS benchmark in Table~\ref{tab:lvis_main}.
Because ViLD~\cite{gu2021open} is trained with large-scale jittering~\cite{simple_copy_paste} and a prohibitive  schedule, DetPro~\cite{Du_2022_CVPR} re-implemented it with backbone weights pre-trained by SOCO~\cite{wei2021aligning} and a regular  schedule.
DetPro also proposes learned prompts for the category's names. Besides, an ensembling strategy for classification scores is adopted in ViLD and DetPro. 
For fair comparison, we respectively implement our method on OV-LVIS with and without the these tricks. 
\method~achieves the best performance in all the scenarios and can even surpass ViLD that adopts the ensembling strategy without these tricks.


\myparagraph{Transfer to Other Datasets.}
We transfer the open-vocabulary detector trained on OV-LVIS to three other datasets, including Pascal VOC 2007~\cite{everingham2010pascal} test set, COCO~\cite{lin2014microsoft} validation set and Objects365~\cite{shao2019objects365} v2 validation set. We compare our results with DetPro~\cite{Du_2022_CVPR} and the ViLD~\cite{gu2021open} implemented in DetPro. The comparison is fair since all the models are based on the same object detector and training schedule. 
As shown in Table~\ref{tab:transfer}, our approach exhibits better generalization ability on all of the three datasets.

\subsection{Ablation Study}
In this section, we ablate the effectiveness of components in \method on OV-COCO benchmark.


\begin{table*}[h]
\centering
\hspace{1mm}
\begin{minipage}[t]{0.31\textwidth}
\centering
\caption{Effectiveness of main components of \method}
\vspace{-6pt}
\scalebox{0.9}{ \tablestyle{3pt}{1.1}
  \scalebox{0.9}{

\begin{tabular}{c |c |c| c|c c c}
 \midrule
\#  & & & PE & AP & AP & AP \\
 \midrule
1 & \checkmark & - & - & 25.7&59.6 &50.6 \\
2 & -  & \checkmark & - &25.7& 59.4&50.5 \\
3 & - & \checkmark  & \checkmark &32.8 & 60.1 &53.0\\
4 & \checkmark & \checkmark & \checkmark &\textbf{34.0} &60.4 &53.5 \\

\hline
\end{tabular}
} }
\label{tab:ablation_overall}
\end{minipage}
\hspace{6mm}
\begin{minipage}[t]{0.31\textwidth}
\centering
\caption{Exploring sampling strategies to obtain bag of regions}
\vspace{-6pt}
\scalebox{0.9}{ \tablestyle{3pt}{1.1}
  \scalebox{0.9}{\begin{tabular}{c|c| cc c}
 \midrule
&Sampling strategy & AP & AP & AP\\
 \midrule
1&Grid & 25.4&58.0 & 49.5 \\
2&Random & 27.3 & 53.3 & 46.5 \\
3&Ours (reduced) & \textbf{32.2} &58.3 &51.5\\
4&Ours & \textbf{34.0} &60.4 &53.5\\
\hline
\end{tabular}
} }
\label{tab:sampling_baseline}
\end{minipage}
\hspace{1mm}
\begin{minipage}[t]{0.31\textwidth}
\centering
\caption{Overlap (IOF) between regions}
\vspace{-6pt}
\scalebox{0.916}{ \tablestyle{3pt}{1.1}
  \scalebox{0.9}{
  \begin{tabular}{c|c| cc c}
 \midrule
\#&Overlap & AP & AP & AP\\
 \midrule
1 & -0.1& 32.5 & 59.7 & 52.6\\
2 & 0.0 & 33.6 & 60.2 & 53.2\\
3 & 0.1 & \textbf{34.0} & 60.4 & 53.5\\
4 & 0.2 & 33.8 & 59.8 & 53.0\\
5 & 0.3 & 33.7 & 60.0 & 53.1\\
\hline
\end{tabular}
} }
\label{tab:overlap}
\end{minipage}
\end{table*}

\begin{table*}[h]
\vspace{-4pt}
\hspace{1mm}
\centering
\begin{minipage}[t]{0.31\textwidth}
\centering
\caption{Ablation study on the sampling probability }
\vspace{-8pt}
\scalebox{0.95}{ \tablestyle{3pt}{1.1}
  \scalebox{0.9}{
\begin{tabular}{c| cc c}
 \midrule
p & AP & AP & AP\\
 \midrule
0.1 & 33.7&59.8 &52.9\\
0.3 & \textbf{34.0} &60.4 &53.5\\
0.5 & 33.2 &60.0 &53.0\\
\hline
\end{tabular}
} }
\label{tab:probability}
\end{minipage}
\hspace{6mm}
\begin{minipage}[t]{0.31\textwidth}
\centering
\caption{Number of sampled bags per region proposal}
\vspace{-8pt}
\scalebox{0.95}{ \tablestyle{3pt}{1.1}
  \scalebox{0.9}{
\begin{tabular}{c| cc c}
 \midrule
\#\text{bags} & AP & AP & AP\\
 \midrule
1 & 32.6 &58.5 &51.7\\
3 & \textbf{34.0} &60.4 &53.5\\
5 & 33.2 &60.0 &53.0\\
\hline
\end{tabular}
} }
\label{tab:groups}
\end{minipage}
\hspace{1mm}
\begin{minipage}[t]{0.31\textwidth}
\centering
\caption{Number of pseudo words}
\vspace{-8pt}
\scalebox{0.96}{ \tablestyle{3pt}{1.1}
  \scalebox{0.9}{
  \begin{tabular}{c|c| cc c}
 \midrule
\#& \#words & AP & AP & AP\\
 \midrule
1 &2 & 31.6 & 59.5 & 52.2 \\
2 &4 & 33.1 & 60.1 & 53.0 \\
3 &6 & \textbf{34.0} & 60.4 & 53.5 \\
4 &8 & 33.5 & 59.9 & 53.0 \\
\hline
\end{tabular}
} }
\label{tab:word_num}
\end{minipage}
\vspace{-7pt}
\end{table*}



\myparagraph{Effectiveness of Aligning Bag of Regions.} We start from a baseline that only uses the individual-level loss . As shown in Table~\ref{tab:ablation_overall}(), the individual-level baseline achieves 25.7 mAP on novel categories. We then replace   with the loss for bag of regions  (Table~\ref{tab:ablation_overall}()). Without considering the spatial information of region boxes, the performance on novel categories is compatible with aligning individual regions.
When adding the positional embedding of the region boxes' spatial information, the performance on novel categories (Table~\ref{tab:ablation_overall}()) dramatically increases by 7.1 mAP.
This means that the spatial information is essential to effectively exploit the compositional structure of co-occurring visual concepts in a bag of regions.
Finally, in Table~\ref{tab:ablation_overall}(), we find that the individual-level loss is complementary to the bag-of-regions alignment, which brings 1.2 mAP performance gain on novel categories.

\myparagraph{Sampling Strategies.}
We explore two baselines to sample bags of regions to support the rationale of our neighborhood sampling strategy. The first is to equally split an image into grids (dubbed as grid sampling) like the pre-training stage in OVR-CNN~\cite{zareian2021open} such that the fixed grids form a bag of regions. And the second is to
randomly sample region proposals to form a bag of regions (dubbed as random sampling). For a fair comparison, we keep the number of sampled regions in each sampling strategy roughly the same. Concretely, we split the images to  grids for the grid sampling strategy and sample 9 region proposals for the random sampling strategy. For these two strategies, we take 4 permutations of the regions to obtain richer bag-of-regions embeddings so that there would be 36 regions for each image. For the neighborhood sampling, we introduce a reduced version of the strategy that restricts the number of region proposals per image to 12 and takes 1 bag per proposal. This is because we record that the average number of regions in a bag is 3 with  and , meaning the average sampled regions for each image in the neighborhood sampling strategy is close to . 

The grid sampling strategy, whose fixed regions may either contain too many objects or only small parts of an object, achieves 25.4 mAP on the novel categories as shown in Table~\ref{tab:sampling_baseline}(). For the random sampling strategy, regions in a bag have different sizes and shapes and the distance between the regions can be large, which hinders the image encoder to exactly represent the bag of regions. It achieves 27.3 mAP as shown in Table~\ref{tab:sampling_baseline}(). While our neighborhood sampling strategy utilizes the same amount of regions, we record 32.2 mAP on the novel categories in Table~\ref{tab:sampling_baseline}(). Compared to these two baselines, our neighborhood sampling strategy captures potential objects in the vicinity of region proposals and ensures the teacher embedding exactly represents the bag of regions by sampling the neighboring boxes of region proposals. Note that our result in Table~\ref{tab:sampling_baseline}() is achieved with 3 groups per region proposal and no limitation on the number of region proposals. More details on how we develop the sampling strategy are in the appendix.


\myparagraph{Box Overlap between Regions in a Bag.} We let the sampled regions in a group overlap at a certain IOF. In Table~\ref{tab:overlap}, we show how the overlap between boxes affects the performance. The scalar smaller than 0.0 in Table~\ref{tab:overlap}() means that we keep an interval between the sampled boxes. The best performance (34.0 mAP) comes with an overlap of 0.1. Table~\ref{tab:overlap}() indicate that the regions in a group need to have a continuity of semantics. Table~\ref{tab:overlap}() indicate that the regions also need to cover diverse image contents.


\myparagraph{Sampling Probability.} We study the effect of the probability  to sample the candidate boxes, which affects the number of regions in a bag. The details are in Sec.~\ref{sec:context_sampling}. Note that the  will be adjusted by the aspect ratio scaled with the scale factor . We fix  as 3.0 and sample 3 bags for each region proposal. We observe that the best result on novel categories (34.0 mAP) is achieved with  in Table~\ref{tab:probability}.


\myparagraph{Number of Bags Per Proposal.} We show the effect of the number of sampled bags (bags) for each region proposal in Table~\ref{tab:groups}. For details, please refer to Sec~\ref{sec:context_sampling}. We fix  as 3.0 and the sampling probability  as 0.3. We observe that the best result on novel categories (34.0 mAP) is achieved with three bags of regions for each region proposal in Table~\ref{tab:groups}. 


\myparagraph{Number of Pseudo Words Per Region.} As an object category often needs many words to reach a precise description, we study the number of pseudo words (wordss) predicted for each region of interest. Table~\ref{tab:word_num}() show that the performance on novel categories increases with the number of pseudo words, indicating that stacking more pseudo words to a certain extent can strengthen the detector's ability to distinguish object categories. However, the results in Table~\ref{tab:word_num}() show that further increasing the number of pseudo words does not bring performance gain, and redundant words can even do harm to the performance.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/visualization.jpg}
    \vspace{-24pt}
    \caption{Qualitative comparisons of BARON and the individual-level baseline in Table~\ref{tab:ablation_overall}().
    \textbf{Top:} Red boxes are for the novel categories and blue for the base categories. \textbf{Bottom:} Feature map's responses to the queried novel object categories. From (a) to (c), the queried novel categories are `skateboard', `airplane' and `dog', respectively. 
    \method~detects objects of novel categories that are missed by the baseline. 
    }
    \label{fig:vis}
    \vspace{-13pt}
\end{figure*}


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/composition.jpg}
    \vspace{-12pt}
    \caption{For each image, we incrementally add the object categories that appear in it to the text description. The similarity score between the image and text embeddings increases as the text description becomes more complete and precise.}
    \label{fig:motivation}
 \vspace{-7pt}
\end{figure}

\subsection{Further Analysis}
We show qualitative results in this section to further analyze the effectiveness of our method.

\myparagraph{Co-occurrence of Objects.} We first illustrate how a pre-trained VLM~\cite{radford2021learning} captures the co-occurrence of objects by comparing the image-text similarity in Fig.~\ref{fig:motivation}. 
We use the CLIP-ViT-B/32 model pre-trained on more than 400 million image-text pairs to obtain the image and text embeddings.
For each image, we incrementally add the object categories that appear to the text description. 
We observe that the similarity score between the image and text embeddings increases as the text description includes more concepts as shown in Fig.~\ref{fig:motivation}.
The examples reveal that the large-scale VLMs could capture the co-occurrence of multiple concepts in an image, although they are not explicitly trained to do so.
We believe this is because each image-text pair naturally contains multiple concepts and the VLMs could implicitly learn the underlying connections when training on massive-scale image-text pairs. We also find simple relationship between objects can also be captured by the VLMs, \eg, the similarity increases when we add the relation `in front of' to the text description in the second example of Fig~\ref{fig:motivation}.

\myparagraph{Visualization.} We further visualize the predictions of detectors learned through \method~and the individual-level baseline in Fig.~\ref{fig:vis}. 
The images are from COCO's validation set. We visualize the feature map's response to the novel categories using the Grad-CAM++~\cite{chattopadhay2018grad}.
We find that the model learned through \method~generates responses at locations of novel categories while the individual-level baseline induces weaker, incomplete or diffused responses.
We also notice that semantically related objects can respond to the queried object category.
For example, in Fig~\ref{fig:vis}(c) the location of a flock of sheep chased by a dog responds to the queried object `dog'. Note the `dog' and the `flock of sheep' are not in neighboring regions and the `dog' is much smaller in size. Even though we form bags of regions by neighboring regions of equal size in training, the model has the ability to capture the relationship between objects with imbalanced sizes or large distance. This phenomenon resembles the generalization ability of human language, where learned concepts can be applied to describe or recognize new things.
