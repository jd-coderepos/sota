\documentclass{article}
\usepackage[preprint]{neurips_2019}


\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage{amssymb}
\newcommand{\mathbold}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
\usepackage[ttdefault=true]{AnonymousPro}



\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{nicefrac}
\usepackage{bm}



\usepackage[english]{babel}
\usepackage[parfill]{parskip}
\usepackage{afterpage}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{xspace}



\usepackage{lineno}
\renewcommand\linenumberfont{\normalfont
                             \footnotesize
                             \sffamily
                             \color{SkyBlue}}

\newcommand{\parnum}{\bfseries\P\arabic{parcount}}
\newcounter{parcount}
\newcommand\p{\stepcounter{parcount}\leavevmode\marginpar[\hfill\parnum]{\parnum}}



\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{shadecolor}{gray}{0.9}

\newcommand{\red}[1]{\textcolor{BrickRed}{#1}}
\newcommand{\orange}[1]{\textcolor{BurntOrange}{#1}}
\newcommand{\green}[1]{\textcolor{OliveGreen}{#1}}
\newcommand{\blue}[1]{\textcolor{MidnightBlue}{#1}}
\newcommand{\sky}[1]{\textcolor{SkyBlue}{#1}}
\newcommand{\gray}[1]{\textcolor{black!60}{#1}}



\renewcommand{\labelenumi}{\color{black!67}{\arabic{enumi}.}}
\renewcommand{\labelenumii}{{\color{black!67}(\alph{enumii})}}
\renewcommand{\labelitemi}{{\color{black!67}\textbullet}}



\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage{subcaption}



\usepackage{booktabs, array}



\usepackage{listings}
\usepackage{fancyvrb}
\fvset{fontsize=\small}

\usepackage{natbib}
\usepackage[colorlinks,linktoc=all]{hyperref}
\usepackage[all]{hypcap}
\hypersetup{citecolor=MidnightBlue}
\hypersetup{linkcolor=MidnightBlue}
\hypersetup{urlcolor=MidnightBlue}

\usepackage[nameinlink]{cleveref}
\creflabelformat{equation}{#2\textup{#1}#3}  



\usepackage
[acronym,smallcaps,nowarn,section,nonumberlist]{glossaries}
\glsdisablehyper{}



\definecolor{strings}{rgb}{.624,.251,.259}
\definecolor{keywords}{rgb}{.224,.451,.686}
\definecolor{comment}{rgb}{.322,.451,.322}

\lstdefinelanguage{python}{
  morekeywords={from, import, as, for, in, while, def, return, =, +, if, elif, else, with,
  -, /, *, lambda, global, del},
  keywords=[2]{1,2,3,4,5,6,7,8,9,0, __init__, STACK, class, super, self},
  keywords=[3]{list,len},
  morecomment=[l]{\#},
  morecomment=[s]{"""}{"""},
  morestring=[b]',
  morestring=[b]",
  alsoletter={<>=-+/*},
  sensitive=true
}

\lstset{
  language=python,
  keywordstyle=\color{BrickRed}\bfseries\ttfamily,
  keywordstyle=[2]\color{MidnightBlue}\ttfamily,
  keywordstyle=[3]\color{comment}\ttfamily,
  commentstyle=\color{comment}\ttfamily,
  stringstyle=\color{strings}\ttfamily,
  basicstyle=\fontsize{9pt}{9.25pt}\selectfont\ttfamily,
  basewidth=0.5em,
  stepnumber=1,
  columns=fixed,
  xleftmargin=2ex,firstnumber=1,
  showstringspaces=false,
  mathescape=true,
  keepspaces=True,
  tabsize=2,
  escapechar=@,
}







\newcommand{\draftdisclaimer}{\begin{center}\begin{framed} DRAFT: DO
NOT CITE OR DISTRIBUTE \end{framed}\end{center}} 

\newcommand{\g}{\,|\,}
\renewcommand{\gg}{\,\|\,}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}
\newcommand{\nestedmathbold}[1]{{\mathbold{#1}}}



\newcommand{\mba}{\nestedmathbold{a}}
\newcommand{\mbb}{\nestedmathbold{b}}
\newcommand{\mbc}{\nestedmathbold{c}}
\newcommand{\mbd}{\nestedmathbold{d}}
\newcommand{\mbe}{\nestedmathbold{e}}
\newcommand{\mbg}{\nestedmathbold{g}}
\newcommand{\mbh}{\nestedmathbold{h}}
\newcommand{\mbi}{\nestedmathbold{i}}
\newcommand{\mbj}{\nestedmathbold{j}}
\newcommand{\mbk}{\nestedmathbold{k}}
\newcommand{\mbl}{\nestedmathbold{l}}
\newcommand{\mbm}{\nestedmathbold{m}}
\newcommand{\mbn}{\nestedmathbold{n}}
\newcommand{\mbo}{\nestedmathbold{o}}
\newcommand{\mbp}{\nestedmathbold{p}}
\newcommand{\mbq}{\nestedmathbold{q}}
\newcommand{\mbr}{\nestedmathbold{r}}
\newcommand{\mbs}{\nestedmathbold{s}}
\newcommand{\mbt}{\nestedmathbold{t}}
\newcommand{\mbu}{\nestedmathbold{u}}
\newcommand{\mbv}{\nestedmathbold{v}}
\newcommand{\mbw}{\nestedmathbold{w}}
\newcommand{\mbx}{\nestedmathbold{x}}
\newcommand{\mby}{\nestedmathbold{y}}
\newcommand{\mbz}{\nestedmathbold{z}}

\newcommand{\mbA}{\nestedmathbold{A}}
\newcommand{\mbB}{\nestedmathbold{B}}
\newcommand{\mbC}{\nestedmathbold{C}}
\newcommand{\mbD}{\nestedmathbold{D}}
\newcommand{\mbE}{\nestedmathbold{E}}
\newcommand{\mbF}{\nestedmathbold{F}}
\newcommand{\mbG}{\nestedmathbold{G}}
\newcommand{\mbH}{\nestedmathbold{H}}
\newcommand{\mbI}{\nestedmathbold{I}}
\newcommand{\mbJ}{\nestedmathbold{J}}
\newcommand{\mbK}{\nestedmathbold{K}}
\newcommand{\mbL}{\nestedmathbold{L}}
\newcommand{\mbM}{\nestedmathbold{M}}
\newcommand{\mbN}{\nestedmathbold{N}}
\newcommand{\mbO}{\nestedmathbold{O}}
\newcommand{\mbP}{\nestedmathbold{P}}
\newcommand{\mbQ}{\nestedmathbold{Q}}
\newcommand{\mbR}{\nestedmathbold{R}}
\newcommand{\mbS}{\nestedmathbold{S}}
\newcommand{\mbT}{\nestedmathbold{T}}
\newcommand{\mbU}{\nestedmathbold{U}}
\newcommand{\mbV}{\nestedmathbold{V}}
\newcommand{\mbW}{\nestedmathbold{W}}
\newcommand{\mbX}{\nestedmathbold{X}}
\newcommand{\mbY}{\nestedmathbold{Y}}
\newcommand{\mbZ}{\nestedmathbold{Z}}

\newcommand{\mbalpha}{\nestedmathbold{\alpha}}
\newcommand{\mbbeta}{\nestedmathbold{\beta}}
\newcommand{\mbdelta}{\nestedmathbold{\delta}}
\newcommand{\mbepsilon}{\nestedmathbold{\epsilon}}
\newcommand{\mbchi}{\nestedmathbold{\chi}}
\newcommand{\mbeta}{\nestedmathbold{\eta}}
\newcommand{\mbgamma}{\nestedmathbold{\gamma}}
\newcommand{\mbiota}{\nestedmathbold{\iota}}
\newcommand{\mbkappa}{\nestedmathbold{\kappa}}
\newcommand{\mblambda}{\nestedmathbold{\lambda}}
\newcommand{\mbmu}{\nestedmathbold{\mu}}
\newcommand{\mbnu}{\nestedmathbold{\nu}}
\newcommand{\mbomega}{\nestedmathbold{\omega}}
\newcommand{\mbphi}{\nestedmathbold{\phi}}
\newcommand{\mbpi}{\nestedmathbold{\pi}}
\newcommand{\mbpsi}{\nestedmathbold{\psi}}
\newcommand{\mbrho}{\nestedmathbold{\rho}}
\newcommand{\mbsigma}{\nestedmathbold{\sigma}}
\newcommand{\mbtau}{\nestedmathbold{\tau}}
\newcommand{\mbtheta}{\nestedmathbold{\theta}}
\newcommand{\mbupsilon}{\nestedmathbold{\upsilon}}
\newcommand{\mbvarepsilon}{\nestedmathbold{\varepsilon}}
\newcommand{\mbvarphi}{\nestedmathbold{\varphi}}
\newcommand{\mbvartheta}{\nestedmathbold{\vartheta}}
\newcommand{\mbvarrho}{\nestedmathbold{\varrho}}
\newcommand{\mbxi}{\nestedmathbold{\xi}}
\newcommand{\mbzeta}{\nestedmathbold{\zeta}}

\newcommand{\mbDelta}{\nestedmathbold{\Delta}}
\newcommand{\mbGamma}{\nestedmathbold{\Gamma}}
\newcommand{\mbLambda}{\nestedmathbold{\Lambda}}
\newcommand{\mbOmega}{\nestedmathbold{\Omega}}
\newcommand{\mbPhi}{\nestedmathbold{\Phi}}
\newcommand{\mbPi}{\nestedmathbold{\Pi}}
\newcommand{\mbPsi}{\nestedmathbold{\Psi}}
\newcommand{\mbSigma}{\nestedmathbold{\Sigma}}
\newcommand{\mbTheta}{\nestedmathbold{\Theta}}
\newcommand{\mbUpsilon}{\nestedmathbold{\Upsilon}}
\newcommand{\mbXi}{\nestedmathbold{\Xi}}

\newcommand{\mbzero}{\nestedmathbold{0}}
\newcommand{\mbone}{\nestedmathbold{1}}
\newcommand{\mbtwo}{\nestedmathbold{2}}
\newcommand{\mbthree}{\nestedmathbold{3}}
\newcommand{\mbfour}{\nestedmathbold{4}}
\newcommand{\mbfive}{\nestedmathbold{5}}
\newcommand{\mbsix}{\nestedmathbold{6}}
\newcommand{\mbseven}{\nestedmathbold{7}}
\newcommand{\mbeight}{\nestedmathbold{8}}
\newcommand{\mbnine}{\nestedmathbold{9}}



\newcommand{\diag}{\textrm{diag}}
\newcommand{\supp}{\textrm{supp}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{}\mkern2mu{#1#2}}}

\newcommand{\cD}{\mathcal{D}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cI}{\mathcal{I}} \usepackage{tikz}

\usetikzlibrary{bayesnet} 

\pgfdeclarelayer{edgelayer}
\pgfdeclarelayer{nodelayer}
\pgfsetlayers{edgelayer,nodelayer,main}

\definecolor{hexcolor0xbfbfbf}{rgb}{0.749,0.749,0.749}

\tikzset{>=latex}
\tikzstyle{none}   = [inner sep=0pt]
\tikzstyle{line}   = [ -, thick, shorten <=1pt, shorten >=1pt ]
\tikzstyle{arrow}  = [ ->, thick, shorten <=1pt, shorten >=1pt ]
\tikzstyle{ardash} = [ dashed, ->, thick, shorten <=1pt, shorten >=1pt ]

\tikzstyle{empty}=[circle,opacity=0.0,text opacity=1.0,inner sep=0pt]
\tikzstyle{box}=[rectangle,fill=White,draw=Black]
\tikzstyle{filled}=[circle,thick,fill=hexcolor0xbfbfbf,draw=Black]
\tikzstyle{hollow}=[circle,thick,fill=White,draw=Black]
\tikzstyle{param}=[rectangle,fill=Black,draw=Black,inner sep=0pt,minimum width=4pt,minimum height=4pt]
\tikzstyle{paramhollow}=[rectangle,thick,fill=White,draw=Black,inner sep=0pt,minimum
width=4pt,minimum height=4pt]

\usepackage{pgfplots}                               \pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\newlength\figureheight
\newlength\figurewidth
\setlength\figureheight{1.75in}
\setlength\figurewidth{2.5in}


\newlength\figureheightsmall
\newlength\figurewidthsmall
\setlength\figureheightsmall{1.5in}
\setlength\figurewidthsmall{1.85in}

\definecolor{POSTcolor}{rgb}{0.48, 0.20, 0.58} \definecolor{Qcolor}{rgb}{0.00, 0.53, 0.22}  \usepackage{soul}


\newcommand{\pp}{\textcolor{Plum}{\P\,}}

\newcommand{\q}[1]{\red{{Q | #1}}}

\newcommand{\note}[1]{\red{#1}}

\newcommand{\bcom}[1]{\sidenote{bp: #1}}
\newcommand{\lcom}[1]{\sidenote{ld: #1}}
\usepackage{ragged2e}
\DeclareRobustCommand{\sidenote}[1]{\marginpar{
                                    \RaggedRight
                                    \textcolor{Plum}{\textsf{#1}}}}
\setlength{\marginparwidth}{1in} 


\title{Discrete Flows: Invertible Generative \\ Models of Discrete Data}

\author{Dustin Tran\quad
Keyon Vafa\thanks{Work done as an intern at Google Brain. Supported by NSF grant DGE-1644869.}\quad
Kumar Krishna Agrawal\thanks{Work done as an AI resident.}\quad
Laurent Dinh\quad
Ben Poole \\
Google Brain\quad Columbia University
}

\begin{document}

\maketitle


\begin{abstract}
While normalizing flows have led to significant advances in modeling high-dimensional continuous distributions, their applicability to discrete distributions remains unknown. In this paper, we show that flows can in fact be extended to discrete events---and under a simple change-of-variables formula not requiring log-determinant-Jacobian computations. Discrete flows have numerous applications. We consider two flow architectures: discrete autoregressive flows that enable bidirectionality, allowing, for example, tokens in text to depend on both left-to-right and right-to-left contexts in an exact language model; and discrete bipartite flows that enable efficient non-autoregressive generation as in RealNVP. Empirically, we find that discrete autoregressive flows outperform autoregressive baselines on synthetic discrete distributions, an addition task, and Potts models; and bipartite flows can obtain competitive performance with autoregressive baselines on character-level language modeling for Penn Tree Bank and text8.
\end{abstract}

\vspace{-1ex}
\section{Introduction}
\label{sec:introduction}
\vspace{-1ex}

There have been many recent advances in normalizing flows, a technique for constructing high-dimensional continuous distributions from invertible transformations of simple distributions
\citep{rezende2015variational,tabak2013family,rippel2013high}. Applications for high-dimensional continuous distributions are widespread: these include latent variable models with expressive posterior approximations \citep{rezende2015variational,ranganath2016hierarchical,kingma2016improved}, parallel image generation \citep{dinh2017density,kingma2018glow}, parallel speech synthesis \citep{oord2017parallel,ping2018clarinet,prenger2018waveglow},
and general-purpose density estimation \citep{papamakarios2017masked}.


Normalizing flows are based on the change-of-variables formula, which derives a density given an invertible function applied to continuous events. There have not been analogous advances for discrete distributions, where flows are typically thought to not be applicable. Instead, most research for discrete data has focused on building either latent-variable models with approximate inference \citep{bowman2015generating}, or increasingly sophisticated autoregressive models that assume a fixed ordering of the data \citep{bengio2003neural,vaswani2017attention}.

In this paper, we present an alternative for flexible modeling of discrete sequences by extending continuous normalizing flows to the discrete setting. We construct discrete flows with two architectures:
\begin{enumerate}
\item
\textbf{Discrete autoregressive flows} enable multiple levels of autoregressivity. For example, one can design a bidirectional language model of text where each token depends on both left-to-right and right-to-left contexts while maintaining an exact likelihood and sampling.
\item
\textbf{Discrete bipartite flows} (i.e., with coupling layers similar to RealNVP \citep{dinh2017density}) enable flexible models with parallel generation. For example, one can design nonautoregressive text models which maintain an exact likelihood for training and evaluation.
\end{enumerate}

We evaluate discrete flows on a number of controlled problems: discretized mixture of Gaussians, full-rank discrete distributions, an addition task, and Potts models. In all settings we find that stacking discrete autoregressive flows yields improved performance over autoregressive baselines, and that bipartite flows can reach similar performance as autoregressive baselines while being fast to generate. Finally, we scale up discrete bipartite flows to character-level language modeling where we reach 1.38 bits per character on Penn Tree Bank and 1.23 bits per character on text8; their generation speed is over 100x faster than state-of-the-art autoregressive models.

\vspace{-1ex}
\subsection{Related Work}
\label{sub:related}


\paragraph{Bidirectional models.}
Classically, bidirectional language models such as log-linear models and Markov random fields have been pursued, but they require either approximate inference \citep{mnih2012fast,jernite2015fast} or approximate sampling \citep{berglund2015bidirectional}.
Unlike bidirectional models, autoregressive models must impose a specific ordering, and this has been shown to matter across natural language processing tasks \citep{vinyals2015order,ford2018importance,xia2017deliberation}. Bidirectionality such as in encoders have been shown to significantly improve results in neural machine translation \citep{britz2017massive}. Most recently, BERT has shown bidirectional representations can significantly improve transfer tasks \citep{devlin2018bert}.
In this work, discrete autoregressive flows enable bidirectionality while maintaining the benefits of a (tractable) generative model.

\paragraph{Nonautoregressive models.}
There have been several advances for flexible modeling with nonautoregressive dependencies, mostly for continuous distributions \citep{dinh2014nice,dinh2017density,kingma2018glow}. For discrete distributions, \citet{reed2017parallel} and \citet{stern2018blockwise} have considered retaining blockwise dependencies while factorizing the graphical model structure in order to simulate hierarchically. \citet{gu2018non} and \citet{kaiser2018fast} apply latent variable models for fast translation, where the prior is autoregressive and the decoder is conditionally independent. \citet{lee2018deterministic} adds an iterative refinement stage to initial parallel generations.
\citet{ziegler2019latent} also apply latent variable models and with continuous non-autoregressive normalizing flows as the prior.
In this work, discrete bipartite flows enable nonautoregressive generation while maintaining an exact density---analogous to RealNVP advances for image generation \citep{dinh2017density}.
Most recently, \citet{hoogeboom2019integer} proposed integer discrete flows, a concurrent unpublished work with similar ideas as discrete flows but with a flow transformation for ordinal data and applications to image compression and image generation. We find their results complement ours in illustrating the advantages of invertible functions which do not require log determinant Jacobians and apply an approximate gradient.


\vspace{-1ex}
\section{Background}
\label{sec:background}

\subsection{Normalizing Flows}

Normalizing flows transform a probability distribution using an invertible function
\citep{tabak2013family,rezende2015variational,rippel2013high}. Let  be a -dimensional continuous random variable whose density can be computed efficiently. Given an invertible function , the change-of-variables formula provides an explicit construction of the induced distribution on the function's output, :

The transformation  is referred to as a flow and  is referred to as the base distribution. Composing multiple flows can induce further complex distributions that increase the expressivity of  \citep{rezende2015variational, papamakarios2017masked}.

\subsection{Flow Transformation}

For an arbitrary invertible , the determinant of the Jacobian incurs an  complexity, which is infeasible for high-dimensional datasets. Thus, normalizing flows are designed so that the determinant of the flow's Jacobian can be computed efficiently. Here, we review two popular flow transformations.

\paragraph{Autoregressive flows.}
Autoregressive functions such as recurrent neural networks and Transformers \citep{vaswani2017attention} have been shown to successfully model sequential data across many domains.
Specifically, assume a base distribution . With  and  as autoregressive functions of , i.e. , and  for all , the flow computes a location-scale transform \citep{papamakarios2017masked, kingma2016improving},

The transformation is invertible and the inverse can be vectorized and computed in parallel:

In addition to a fast-to-compute inverse, the autoregressive flow's Jacobian is lower-triangular, so its determinant is the product of the diagonal elements, .
This enables autoregressive flow models to have efficient log-probabilities for training and evaluation.


\paragraph{Bipartite flows.}
Real-valued non-volume preserving (RealNVP) uses another type of invertible transformation \citep{dinh2017density} that nonlinearly transforms subsets of the input. For some , a coupling layer follows a bipartite rather than autoregressive factorization:

where  and  are functions of  with . Due to the bipartite nature of the transformation in coupling layers, we refer to them as bipartite flows. By changing the ordering of variables between each flow, the composition of bipartite flows can learn highly flexible distributions. By design, their Jacobian is lower-triangluar, with a determinant that is the product of diagonal elements, .

Bipartite flows are not as expressive as autoregressive flows, as a subset of variables do not undergo a transformation. However, both their forward and inverse computations are fast to compute, making them suitable for generative modeling where fast generation is desired.

\section{Discrete Flows}
\label{sec:discrete}

Normalizing flows depend on the change of variables formula (\Cref{eq:change-of-variables}) to compute the change in probability mass for the transformation. However, the change of variables formula applies only to continuous random variables. We extend normalizing flows to discrete events.


\subsection{Discrete Change of Variables}

Let  be a discrete random variable and  where  is some function of . The induced probability mass function of  is the sum over the pre-image of :

where  is the set of all elements such that . For an invertible function , this simplifies to

This change of variables formula for discrete variables is similar to
the continuous change of variables formula (\Cref{eq:change-of-variables}), but without the log-determinant-Jacobian. Intuitively, the log-determinant-Jacobian corrects for changes to the volume of a continuous space; volume does not exist for discrete distributions so there is no need to account for it. Computationally, \Cref{eq:discrete-change-of-variables} is appealing as there are no restrictions on  such as fast Jacobian computations in the continuous case, or tradeoffs in how the log-determinant-Jacobian influences the output density compared to the base density.

\subsection{Discrete Flow Transformation}

\begin{figure}[t!]
\centering
\begin{subfigure}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{img/ar-flow.pdf}
\end{subfigure}\hspace{5em}\begin{subfigure}[t]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{img/bipartite_flow.pdf}
\end{subfigure}\hfill \caption{Flow transformation when computing log-likelihoods.
\textbf{(a)} Discrete autoregressive flows stack multiple levels of autoregressivity.
The receptive field of output unit 2 (\red{red}) includes left and right contexts.
\textbf{(b)} Discrete bipartite flows apply a binary mask (\blue{blue} and \green{green}) which determines the subset of variables to transform.
With 2 flows, the receptive field of output unit 2 is .
}
\label{fig:flow-transformations}
\end{figure}

Next we develop discrete invertible functions. To build intuition, first consider the binary case.
Given a -dimensional binary vector , one natural function applies the XOR bitwise operator,

where  is a function of previous outputs, ;  is the XOR function (0 if  and  are equal and 1 otherwise).
The inverse is . We provide an example next.

\paragraph{Example.}
Let  where  is defined by the following probability table:

The data distribution cannot be captured by a factorized one . However, it can with a flow: set ;
 with probabilities ; and  with probabilities .
The flow captures correlations that cannot be captured alone with the base.
More broadly, discrete flows perform a multi-dimensional relabeling of the data
such that it's easier to model with the base. This is analogous to continuous flows, which whiten the data such that it's easier to model with the base (typically, a spherical Gaussian).

\textbf{Modulo location-scale transform.}
To extend XOR to the categorical setting,
consider a -dimensional vector , each element of which takes on values in . One can perform location-scale transformations on the \emph{modulo integer space},

Here,  and  are autoregressive functions of  taking on values in  and  respectively. For this transformation to be invertible,  and  must be coprime (an explicit solution for  is Euclid's algorithm). An easy way to ensure coprimality is to set  to be prime; mask noninvertible  values for a given ; or fix . Setting  and , it's easy to see that the modulo location-scale transform generalizes XOR. (We use  for all experiments except character-level language modeling.)

The idea also extends to the bipartite flow setting: the functions  are set to  for a subset of the data dimensions, and are functions of that subset otherwise. Invertible discrete functions are widely used in random number generation, and could provide inspiration for alternatives to the location scale transformation for constructing flows \citep{salmon2011parallel}.



\paragraph{Example.}
\Cref{fig:flow-mixtures} illustrates an example of using flows to model correlated categorical data. Following \citet{metz2016unrolled}, the data is drawn from a mixture of Gaussians with 8 means evenly spaced around a circle of radius 2. The output variance is , with samples truncated to be between  and , and we discretize at the  level, resulting in two categorical variables (one for  and one for ) each with 90 states. A factorized base distribution cannot capture the data correlations, while a single discrete flow can. (Note the modulo location-scale transform does not make an ordinal assumption. We display ordinal data as an example only for visualization; other experiments use non-ordinal data.)

\begin{figure}[t!]
\centering
\begin{subfigure}[t]{0.3\textwidth}
\centering
\includegraphics[scale=0.25]{img/quantized_ring_data.pdf}
\caption{Data}
\end{subfigure}\hfill \begin{subfigure}[t]{0.3\textwidth}
\centering
\includegraphics[scale=0.25]{img/quantized_ring_no_flows.pdf}
\caption{Factorized Base}
\end{subfigure}\hfill \begin{subfigure}[t]{0.3\textwidth}
\centering
\includegraphics[scale=0.25]{img/quantized_ring_1_flow.pdf}
\caption{1 Flow}
\end{subfigure}
\caption{Learning a discretized mixture of Gaussians with maximum likelihood. Discrete flows help capture the multi-dimensional modes, which a factorized distribution cannot. (Note because the data is 2-D, discrete autoregressive flows and discrete bipartite flows are equivalent.)}
\label{fig:flow-mixtures}
\end{figure}



















\subsection{Training Discrete Flows}
With discrete flow models,
the maximum likelihood objective per datapoint is

where the flow  has free parameters according to its autoregressive or bipartite network, and the base distribution  has free parameters as a factorized (or itself an autoregressive) distribution. Gradient descent with respect to base distribution parameters is straightforward. To perform gradient descent with respect to flow parameters, one must backpropagate through the discrete-output function  and . We use the straight-through gradient estimator \citep{bengio2013estimating}. In particular, the (autoregressive or bipartite) network outputs two vectors of  logits  for each dimension , one for the location and scale respectively. For the scale, we add a mask whose elements are negative infinity on non-invertible values such as 0. On the forward pass, we take the argmax of the logits, where for the location,

Because the argmax operation is not differentiable, we replace \Cref{eq:argmax} on the backward pass with the softmax-temperature function:

As the temperature , the softmax-temperature becomes close to the argmax and the bias of the gradient estimator disappears. However, when  is too low, the gradients vanish, inhibiting the optimization.
Work with the Gumbel-softmax distribution indicates that this approximation works well when the number of classes  \citep{maddison2016concrete,jang2017categorical}, which aligns with our experimental settings; we also fix
.









\section{Experiments}
\label{sec:experiments}

We perform a series of synthetic tasks to better understand discrete flows, and also perform character-level language modeling tasks. For all experiments with discrete autoregressive flows, we used an autoregressive Categorical base distribution where the first flow is applied in reverse ordering. (This setup lets us compare its advantage of bidirectionality to the baseline of an autoregressive base with 0 flows.) For all experiments with discrete bipartite flows, we used a factorized Categorical base distribution where the bipartite flows alternate masking of even and odd dimensions.
We implement and make available discrete flows as part of Bayesian Layers \citep{tran2018bayesian}.






\begin{table*}[!t]
\centering
\begin{tabular}{lcccc} \\  \toprule
& Autoregressive Base & Autoregressive Flow & Factorized Base & Bipartite Flow \\ \midrule
  & \textbf{0.9} & \textbf{0.9} & 1.3 & \textbf{1.0} \\
  & 7.7 & \textbf{7.6} & 8.0 & \textbf{7.9} \\
 & 10.7 & \textbf{10.3} & 11.5 & \textbf{10.7} \\
 & 15.9 & \textbf{15.7} & 16.6 & \textbf{16.0} \\
\bottomrule
\end{tabular}
\caption{Negative log-likelihoods for the full rank discrete distribution (lower is better). Autoregressive flows improve over its autoregressive base. Bipartite flows improve over its factorized base and achieve nats close to an autoregressive distribution while remaining parallel.}
\label{table:full_rank}
\end{table*}






\begin{table*}[!t]
\centering
\begin{tabular}{lcccc} \\  \toprule
& AR Base & AR Flow \\ \midrule
\textbf{number of states = 3} & & \\
\hfill  ,   & 9.27 & \textbf{9.124} \\
\hfill  ,   & 3.79 & \textbf{3.79} \\
\hfill  ,   & 16.66 & \textbf{11.23} \\
\hfill  ,   & 6.30 & \textbf{5.62} \\

\textbf{number of states = 4} & & \\
\hfill ,   & 11.64 & \textbf{10.45} \\
\hfill ,   & 5.87 & \textbf{5.56} \\

\textbf{number of states = 5} & & \\
\hfill ,   & 13.58 & \textbf{10.25} \\
\hfill ,   & 7.94 & \textbf{7.07} \\
\bottomrule
\end{tabular}
\caption{{Negative log-likelihoods on the square-lattice Potts model (lower is better).}  denotes dimensionality. Higher coupling strength  corresponds to more spatial correlations.}
\label{table:potts}
\end{table*}



\subsection{Full-rank Discrete Distribution}
To better understand the expressivity of discrete flows, we examined how well they could fit random full-rank discrete distributions. In particular, we sample a true set of probabilities for all  dimensions of  classes according to a Dirichlet distribution of size , .
For the network for the autoregressive base distribution and location parameters of the flows, we used a Transformer with 64 hidden units. We used a composition of 1 flow for the autoregressive flow models, and 4 flows for the bipartite flow models.

\Cref{table:full_rank} displays negative log-likelihoods (nats) of trained models over data simulated from this distribution.
Across the data dimension  and number of classes , autoregressive flows gain several nats over the autoregressive base distribution, which has no flow on top.
Bipartite flows improve over its factorized base and in fact obtain nats competitive with the autoregressive base while remaining fully parallel for generation.


\subsection{Addition}
Following \citet{zaremba2014learning}, we examine an addition task: there are two input numbers with  digits (each digit takes  values), and the output is their sum with  digits (we remove the  digit if it appears).
Addition naturally follows a right-to-left ordering: computing the leftmost digit requires carrying the remainder from the rightmost computations.
Given an autoregressive base which poses a left-to-right ordering, we examine whether the bidirectionality that flows offer can adjust for wrong orderings. While the output is determnistic, the flexibility of discrete flows may enable more accurate outputs.
We use an LSTM to encode both inputs, apply 0 or 1 flows on the output, and then apply an LSTM to parameterize the autoregressive base where its initial state is set to the concatenation of the two encodings. All LSTMs use 256 hidden units for , and 512 hidden units for .

For , an autoregressive base achieves \textbf{4.0} nats;
an autoregressive flow achieves \textbf{0.2} nats (i.e., close to the true deterministic solution over all pairs of 10-digit numbers). A bipartite model with 1, 2, and 4 flows achieves \textbf{4.0}, \textbf{3.17}, and \textbf{2.58} nats respectively.
For , an autoregressive base achieves \textbf{12.2} nats;
an autoregressive flow achieves \textbf{4.8} nats. A bipartite model with 1, 2, 4, and 8 flows achieves \textbf{12.2}, \textbf{8.8}, \textbf{7.6}, and \textbf{5.08} nats respectively.

\subsection{Potts Model}
Given the bidirectional dependency enabled by discrete flows, we examined how they could be used for distilling undirected models with tractable energies but intractable sampling and likelihoods. We sampled from Potts models (the Categorical generalization of Ising models), which are a 2d Markov random field with pairwise interactions between neighbors (above/below, left/right, but not diagonally connected) \citep{wu1982potts}. To generate data we ran 500 steps of Metropolis-Hastings, and evaluated the NLL of baselines and discrete flows as a function of the coupling strength, . Low coupling strengths correspond to more independent states, while high coupling strengths result in more correlated states across space. For the base network, we used a single layer LSTM with 32 hidden units. For the flow network, we used an embedding layer which returns a trainable location parameter for every unique combination of inputs.

\Cref{table:potts} displays negative log-likelihoods (nats) of trained models over data simulated from Potts models with varying lattice size and coupling strength. As Potts models are undirected models, the autoregressive base posits a poor inductive bias by fixing an ordering and sharing network parameters across the individual conditional distributions. Over data dimension  and coupling , autoregressive flows perform as well as, or improve upon, autoregressive base models. \Cref{appendix:potts} includes samples from the model; they are visually indistinguishable from the data.

\subsection{Character-Level Penn Tree Bank}

\begin{table*}[!t]
\centering
\begin{tabular}{lcccc}
\\  \toprule
& Test NLL (bpc) & Generation \\ \midrule
3-layer LSTM \citep{merity2018analysis}  & \textbf{1.18} &  3.8 min \\
\citet{ziegler2019latent} (AF/SCF) &  1.46 & - \\
\citet{ziegler2019latent} (IAF/SCF) &  1.63 & - \\
Bipartite flow & 1.38 & \textbf{0.17 sec} \\
\bottomrule
\end{tabular}
\caption{Character-level language modeling results on Penn Tree Bank. }
\label{table:ptb}
\end{table*}

We follow the setup of \citet{ziegler2019latent}, which to the best of our knowledge is the only comparable work with nonautoregressive language modeling. We use Penn Tree Bank with minimal processing from \citet{mikolov2012subword}, consisting of roughly 5M characters and a vocabulary size of .
We split the data into sentences and restrict to a max sequence length of 288.
The LSTM baseline of \citet{merity2018analysis} uses 3 layers, truncated backpropagation with a sequence length of 200, embedding size of 400, and hidden size of 1850.\footnote{The LSTM results are only approximately comparable as they do not apply the extra preprocessing step of removing sentences with 288 tokens.} \citet{ziegler2019latent}'s nonautoregressive models  have two variants, in which they use a specific prior with a conditionally independent likelihood and fully factorized variational approximation. AF/SCF uses a prior  over latent time steps and hidden dimension that's autoregressive in  and nonautoregressive in ; and IAF/SCF is nonautoregressive in both  and . For the bipartite flow, we use 8 flows each with an embedding of size 400 and an LSTM with 915 hidden units.

\Cref{table:ptb} compares the test negative log-likelihood in bits per character as well as the time to generate a 288-dimensional sequence of tokens on a NVIDIA P100 GPU. The bipartite flow significantly outperforms \citet{ziegler2019latent}, including their autoregressive/nonautoregressive hybrid. In addition, the generation time is over 1000x faster than the LSTM baseline. Intuitively, the use of bipartite flows means that we only have to perform one forward pass over the model as opposed to the 288 forward passes for a typical autoregressive model.

\subsection{Character-Level text8}
\begin{figure}[t!]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{img/num-flows.png}
\end{minipage}\hfill \begin{minipage}{0.54\textwidth}
\centering
\begin{tabular}{ccc}
\\  \toprule
& bpc & Gen. \\ \midrule
LSTM
(\blue{Coojimans+2016})
& 1.43 &  19.8s \\
64-layer Transformer
(\blue{Al-Rfou+2018})
&  \textbf{1.13} & 35.5s \\
Bipartite flow (4 flows, w/ ) & 1.60 & \textbf{0.15s} \\
Bipartite flow (8 flows, w/o ) & 1.29 & \textbf{0.16s} \\
Bipartite flow (8 flows, w/ ) & 1.23 & \textbf{0.16s} \\
\bottomrule
\end{tabular}
\end{minipage}
\caption{Character-level language modeling results on text8. The test bits per character decreases as the number of flows increases. More hidden units  and layers  in the Transformer per flow, and applying a scale transformation instead of only location, also improves performance.
}
\label{fig:text8}
\end{figure}

We also evaluated on text8, using the preprocessing of \citet{mikolov2012subword,zhang2016architectural} with 100M characters and a vocabulary size of . We split the data into 90M characters for train, 5M characters for dev, and 5M characters for test. For discrete bipartite flows, we use a batch size of 128, sequence length of 256, a varying number of flows, and parameterize each flow with a Transformer with 2 or 3 layers, 512 hidden units, 2048 filter size, and 8 heads.

\Cref{fig:text8} compares the test negative log-likelihood in bits per character as well as the time to generate one data point, i.e., a 256-dimensional sequence, on a NVIDIA P100 GPU. The bipartite flow reaches competitive performance, i.e., better than an LSTM baseline but not as good as the state-of-the-art bpc from the 235M parameter 64-layer Transformer (we're unfamiliar with previous nonautoregressive results to compare to). We also find that having a flexible scale (``w/ '') improves performance over fixing  and only learning the location transform . The bipartite flows' generation times are significantly faster than the baselines with upwards of a 100x speedup.







\section{Discussion}

We describe discrete flows, a class of invertible functions for flexible modeling of discrete data. Discrete autoregressive flows enable bidirectionality by stacking multiple levels of autoregressivity, each with varying order. Discrete bipartite flows enable nonautoregressive generation by flexibly modeling data with a sequence of bipartite-factorized flow transformations. Our experiments across a range of synthetic tasks and character-level text data show the promise of such approaches.

As future work, we're also investigating discrete inverse autoregressive flows, which enable flexible variational approximations for discrete latent variable models. An open question remains with scaling discrete flows to large numbers of classes: in particular, the straight-through gradient estimator works well for small numbers of classes such as for character-level language modeling, but it may not work for (sub)word-level modeling where the vocabulary size is greater than 5,000. In such settings, alternative gradient estimators or data representations may be fruitful. Lastly, there may be other invertible discrete functions used in cryptography and random number generation that could be leveraged for building up alternative forms for discrete flows \citep{salmon2011parallel}.

\paragraph{Acknowledgements}
Keyon Vafa is supported by NSF grant DGE-1644869.
We thank Michael W. Dusenberry, Kevin Murphy, Wei Ping, and Jakob Uszkoreit for helpful comments and feedback.


\bibliographystyle{apalike}
\bibliography{bib}
\clearpage

\appendix

\section{Samples from Potts Model}
\label{appendix:potts}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{img/potts_model_discrete_flows.png}
\caption{Samples from Potts model, for 4x4 grid with 3 states and coupling strength () = 0.1}
\end{figure}






\end{document}
