\documentclass{article}











\usepackage[final]{neurips_2020}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      


\usepackage{amsfonts}       \usepackage{nicefrac}       

\usepackage{amsmath, amsthm, amssymb}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage[font=small]{caption}

\definecolor{citecolor}{HTML}{0071bc}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,citecolor=citecolor,linkcolor=blue,bookmarks=false]{hyperref}



\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\task}{\mathcal{T}}


\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}

\newcommand{\bd}[1]{\textbf{#1}}
\newcommand{\app}{\raise.17ex\hbox{$\scriptstyle\sim$}}
\newcommand{\ncdot}{{\mkern 0mu\cdot\mkern 0mu}}
\def\x{$\times$}
\newcommand{\dt}[1]{\fontsize{8pt}{.1em}\selectfont \emph{#1}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcommand{\myparagraph}[1]{{\vspace{0.5em} \noindent \bf #1}}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother

\newcommand{\pol}[0]{\pmb{\pi}}
\newcommand{\cpol}[0]{\pmb{\mu}}
\newcommand{\dd}{\frac{\partial}{\partial \theta_i}}


\newcommand{\yw}[1]{\textcolor{red}{(YW: #1)}} 

\usepackage{graphicx}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 

\title{Multi-Task Reinforcement Learning with \\ Soft Modularization}

\author{
    Ruihan Yang$^{1}$
    \quad Huazhe Xu$^{2}$
    \quad Yi Wu$^{3}$
    \quad Xiaolong Wang$^{1}$ \\
    {${ }^{1}$UC San Diego 
    \qquad ${ }^{2}$ UC Berkeley 
    \qquad ${ }^{3}$ IIIS, Tsinghua }\\
}

\begin{document}

\maketitle


\begin{abstract}

Multi-task learning is a very challenging problem in reinforcement learning. While training multiple tasks jointly allow the policies to share parameters across different tasks, the optimization problem becomes non-trivial:  It remains unclear what parameters in the network should be reused across tasks, and how the gradients from different tasks may interfere with each other. Thus, instead of naively sharing parameters across tasks, we introduce an explicit modularization technique on policy representation to alleviate this optimization issue. Given a base policy network, we design a routing network which estimates different routing strategies to reconfigure the base network for each task. Instead of directly selecting routes for each task, our task-specific policy uses a method called \emph{soft modularization} to softly combine all the possible routes, which makes it suitable for sequential tasks. 
We experiment with various robotics manipulation tasks in simulation and show our method improves both sample efficiency and performance over strong baselines by a large margin. Our project page with code is at  \url{https://rchalyang.github.io/SoftModule/}.

\end{abstract} 
\vspace{-0.1in}
\section{Introduction}\label{sec:intro}
\vspace{-0.1in}

Deep Reinforcement Learning (RL) has recently demonstrated extraordinary capabilities in multiple domains, including playing games~\cite{mnih2013playing} and robotic control and manipulation~\cite{lillicrap2015continuous,levine2016end}. Despite its successful applications, 
Deep RL still requires a large amount of data for training complex tasks.
On the other hand, while the current deep RL methods can learn individual policies for specific tasks such as robot grasping and pushing, it remains very challenging to train a single network that generalizes across all possible robotic manipulation tasks. 

In this paper, we study multi-task RL as one step forward towards skill sharing across diverse tasks and ultimately building robots that can generalize. Training deep networks with multiple tasks jointly, agents can learn to share and re-use components across different tasks, which further leads to improved sample efficiency. This is particularly important when we want to adopt RL algorithms in real-world applications. Multi-task learning also provides a natural curriculum since learning easier tasks can be beneficial for learning of more challenging tasks with shared parameters~\cite{pinto2017learning}. 


However, multi-task RL remains a hard problem. It becomes even more challenging when the number of tasks increases. For instance, it has been shown by \cite{yu2019meta} that training with diverse robot manipulation tasks jointly with a sharing network backbone and multiple task-specific heads for actions hurt the final performance comparing to independent training in each task. One major reason is that multi-task learning introduces optimization difficulties: It is unclear how the tasks will affect each other when trained jointly, and optimizing some tasks can bring negative impacts on the others~\cite{teh2017distral}. 

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/teaser3-4-update.pdf}
\vspace{-0.2in}
\caption{\small{Our multi-task policy network with soft modularization. Given different tasks, our network generate different soft combination of network modules. Gray squares represent network modules and red lines represent the connection between modules (Darker red indicates larger weight).}}
\label{fig:teaser}
\vspace{-0.27in}
\end{figure}

For tackling this problem,  compositional models with multiple modules were introduced~\cite{andreas2017modular,haarnoja2018composable}. For example, researchers proposed to train modular sub-policies and task-specific high-level policies jointly in a Hierarchical Reinforcement Learning (HRL) framework~\cite{andreas2017modular}. The sub-policies can be shared and selected by different high-level policies with a learned policy composition function. 
However, HRL introduces an optimization challenge on jointly training sub-policies and high-level task-specific policies while training sub-policies separately often require predefined subtasks or some sophisticated way to discover subgoals, which are typically infeasible for real-world applications. 


In this paper, instead of designing individual modules explicitly for each sub-policy, we propose a soft modularization method that generates soft combinations of different modules for different tasks automatically without explicitly specifying the policy structure. This approach consists of two networks: a base policy network and a routing network. The base policy network, which is composed of multiple modules, takes the state as input and outputs an action for the task.  The routing network takes a task embedding and the current state as input and estimates the routing strategy. 

Given a task, the modules in the base policy network will be reconfigured by the routing network. This is visualized in Figure~\ref{fig:teaser}. Furthermore, instead of taking hard assignments on modules, which is hard to optimize in sequential tasks, our routing network outputs a probability distribution over module assignments for each task. A task-specific base network can be viewed as a weighted combination of the shared modules according to the probability distribution. We benefit from this design to directly back-prop through the routing weights and train both networks jointly over multiple tasks. The advantage is that we can modularize the networks according to tasks without specifying policy hierarchies explicitly (e.g., HRL). The role of each module automatically emerged after training and the routing network 
determines which modules should be used more for different tasks.

We perform experiments in Meta-World~\cite{yu2019meta}, which contains 50 robotic manipulation tasks. With soft modularization, we achieve significant improvements in both sample efficiency and final performance over previous state-of-the-art multi-task policies. For example, we almost double the manipulation success rate for learning with 50 tasks compared to the multi-task baselines. Our approach utilizes far less training data compared to training individual policies for each task while achieving learned policy that is able to perform closely to the individually trained policies. This shows that enforcing soft modularization can improve the generalization across different tasks in RL. 
 
\vspace{-0.1in}
\section{Related Work}\label{sec:related}
\vspace{-0.1in}
\label{related}
\textbf{Multi-task learning.} Multi-task learning~\cite{caruana1997multitask} is one of the core machine learning problems. Researchers have shown learning with multiple objectives can make different tasks benefit from each other in robotics and RL ~\cite{wilson2007multi,pinto2016curious,pinto2017learning,riedmiller2018learning,hausman2018learning,sax2019learning}. While sharing parameters across tasks can intuitively improve data efficiency, gradients from different tasks can interfere negatively with each other.
One way to avoid it is to use policy distillation~\cite{parisotto2015actor,rusu2015policy,teh2017distral, teh2017distral}.
However, these approaches still require separate networks for different policies and an extra distillation stage. 
Researchers also propose to explicitly model the similarity between gradients from different tasks~\cite{zhang2014regularization,chen2017gradnorm,kendall2018multi,lin2019adaptive,sener2018multi,du2018adapting,Yu2020surgery,hu2019learning}. For example, it is proposed in~\cite{chen2017gradnorm} to normalize the gradients from different tasks for balancing multi-task losses. Besides adjusting the losses, a recent work~\cite{Yu2020surgery} proposes to directly reduce the gradient conflicts by gradient projection. However, optimization relying on the gradient similarity is usually unstable, especially when there is a large gradient variance within each task itself. 



\textbf{Compositional learning and modularization.} Instead of directly enforcing the gradients to align, a natural way to avoid the conflicts of the gradient is using compositional models. By utilizing different modules across different tasks, it reduces the interference of gradients on each module and allows better generalization~\cite{singh1992transfer,devin2017learning,andreas2017modular,rusu2016progressive,yuzhe2020,peng2019mcp,haarnoja2018composable,sahni2017learning,Goyal2020Reinforcement}. For example, the policy is decomposed to task-specific and robot-specific modules in~\cite{devin2017learning}, and the policy is able to solve unseen tasks by re-combining the pre-defined modules. However, the pre-defining modules and manual specification of the combination are not scalable. Instead of defining and pre-training the modules or sub-policies, our approach utilizes soft combinations over modules, which allows fully end-to-end training. 


There are also related works learning a routing module in the supervised tasks.  Rosenbaum et. al~\cite{rosenbaum2017routing,rosenbaum2019routing} proposes to use RL to learn a routing policy, which can be considered as a \emph{hard} version of our soft modularization method. This ``hard modularization'' approach is infeasible for RL settings because joint training a multitask control policy and a routing policy suffers from exponentially higher variance in policy gradient due to the temporal nature in RL and leads to severe training instability. For RL, ``hard modularization'' approach would further introduce high variance along with the exploration in the environment. Whereas, our soft version doesn't introduce additional variance, significantly stabilizes RL training, and produces much improved empirical performances. In addition, the following works inspire our work in different ways:  Purushwalkam et. al \cite{purushwalkam2019task} consider zero-shot compositional learning in vision; Wang et.al \cite{wang2019tafe} consider weight generation for multi-task learning; Li et.al \cite{li2020learning} alleviate the scale variance in semantic representation with dynamic routing.

\textbf{Mixture of experts.} Our method is also related to works on mixture of experts~\cite{MI1993,gomi1993,jacobs1991,singh1992transfer,NIPS1993_750,Ma2018MMoE}. For example, it is proposed in Satinder et. al~\cite{singh1992transfer} to train a gating function to select different Q functions (experts) for different tasks. Instead of performing one-time selection among the individual expert (which is usually pre-defined), the modules we propose are organized in multiple layers in our base policy network, with multiple layers of selection guided by the routing network. While each module alone is not functioning as a policy, this increases the flexibility of sharing the modules across different tasks. At the same time, it reduces the mutual interference between the modules because the modules are only connected via the routing network. 

 
\vspace{-0.1in}
\section{Background}\label{sec:background}
\vspace{-0.1in}
We consider a finite horizon Markov decision processe (MDP) for each task $\task$ and there are $M$ tasks in total, which can be represented by $(S, A, P, R, H, \gamma)$, where the state $s \in S$ and action $a \in A$ are continuous. $P(s_{t+1}|s_t, a_t)$ represents the stochastic transition dynamics. $R(s_t, a_t)$ represents the reward function. $H$ is the horizon and  $\gamma$ is the discount factor. We use $\pi_\phi (a_t|s_t)$ to represent the policy parameterized by $\phi$
and the goal is to learn a policy maximizing the expected return.
In multi-task RL, tasks are sampled from a distribution $p(\task)$, and different tasks have different MDPs. 

\vspace{-0.07in}
\subsection{Reinforcement Learning with Soft Actor-Critic} 
\vspace{-0.07in}

In this paper, we train policy with Soft Actor-Critic (SAC)~\cite{DBLP:journals/corr/abs-1801-01290}. SAC is an off-policy actor-critic deep reinforcement learning approach, where actor aims to succeed at the task as well as act as randomly as possible. We consider the parameterized soft Q-function is $Q_\theta(s_t, a_t)$ where $Q$ is parameterized by $\theta$. There are three types of parameters to optimize in SAC: The policy parameters $\phi$, the parameters of Q-function $\theta$ and a temperature $\alpha$. The objective of policy optimization is:
\vspace{-0.05in}
\begin{align}
J_\pi(\phi) = \E_{s_t\sim\mathcal{D}}\left[\E_{a_t\sim\pi_\phi}[\alpha \log \pi_\phi(a_t|s_t) - Q_\theta(s_t, a_t)]\right],
\label{eq:reparam_objective}
\end{align}
where $\alpha$ is a learnable temperature served as an entropy penalty coefficient. It can be learned to maintain the entropy level of the policy, using:
\vspace{-0.05in}
\begin{align}
J(\alpha)  = \E{a_t \sim \pi_\phi} \left[ - \alpha\log\pi_\phi(a_t|s_t) - \alpha \bar{\mathcal{H}}\right],
\label{eq:ecsac:alpha_objective}
\end{align}
where $\bar{\mathcal{H}}$ is a desired minimum expected entropy. If $\log \pi_t(a_t|s_t)$ is optimized to increase its value, and the entropy is becoming smaller, $\alpha$ will be adjusted to increase in the process. 


\vspace{-0.07in}
\subsection{Multi-task Reinforcement Learning}
\vspace{-0.07in}

We extend SAC from single task to multi-task by learning a single, task-conditioned policy $\pi(a|s, z)$, where $z$ represents a task embedding. We optimize the policy to maximize the average expected return across all tasks sampled from $p(\task)$.
The objective of policy optimization is,
\vspace{-0.05in}
\begin{align}
J_\pi(\phi)=\mathbb{E}_{\task \sim p(\task)} \left[ J_{\pi, \task}(\phi) \right],
\label{eq:jphi}
\end{align}
where $J_{\pi, \task}(\phi)$ is adopted directly from Eq.~\ref{eq:reparam_objective} with task $\task$. Similarly for  Q-function, the objective is:
\vspace{-0.05in}
\begin{align}
J_Q(\theta)=\mathbb{E}_{\task \sim p(\task)} \left[ J_{Q, \task}(\theta) \right].
\label{eq:jtheta}
\end{align}
 
\vspace{-0.1in}
\section{Method}\label{sec:method}
\vspace{-0.1in}

We propose to perform multi-task reinforcement learning using a single base policy network with multiple modules. As visualized in Figure ~\ref{fig:model}, instead of finding discrete routing paths to connect the modules for different tasks, we perform soft modularization: we utilize another routing network (right side of Figure~\ref{fig:model}) which takes the task identity embedding and observed state as inputs and outputs the probabilities to weight the modules in a soft manner.

With soft modularization, it allows task-specific policies to learn and discover what modules to share across different tasks. Since the soft combination process is differentiable, both policy network and the routing network can be trained together in an end-to-end manner. Note that the network for the soft Q-function follows the similar structure but initialized and trained independently. 

\begin{wrapfigure}{il}{0.35\textwidth}
  \vspace{-0.25in}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{figs/methodv5-short.pdf}
  \end{center}
    \caption{\small{Our framework contains a base policy network with multiple modules (left) and a routing network (right) generating connections between modules in the base policy network. }}
    \label{fig:model}
  \vspace{-0.45in}
\end{wrapfigure}
Although the soft modularization provides a differentiable way to modularize and share the network across tasks, different tasks can still learn and converge with different training speed based on the difficulties of the tasks. For example, learning ``reaching'' policy is usually much faster than learning ``pick and place'' policy. To tackle this problem, we introduce a simple way to automatically adjust the losses for different tasks to balance the training across tasks.

In the following subsections, we will first introduce our network architecture with soft modularization, and then training objective for multi-task learning with this architecture. 

\vspace{-0.07in}
\subsection{Soft Modularization}
\vspace{-0.07in}

As shown in Figure~\ref{fig:model}, our model of multi-task policy contains two networks: the base policy network and the routing network. At each time stage, the network takes the input of the current state $s_t$ and the task embedding $z_\task$ as inputs. We use an one-hot vector for $z_\task$ representing each task. We forward $s_t$ to a 2-layer MLP and obtain a $D$-dimension representation $f(s_t)$, which is then used as inputs for the modules as well as the routing network. We extract the representation for the task embedding by one fully connected layer as $h(z_\task)$, which is also in $D$-dimension. 

\textbf{Routing Network.} The depth of our routing network is corresponding to number of module layers in the base policy network. Supposed we have $L$ module layers and each layer has $n$ modules in the base policy network. The routing network will have $L-1$ layers to output the probabilities to weight the modules and the dimension of the probability vector is $n \times n$. We define the output probability vector for the $l$th layer as $p^l \in \mathbb{R}^{n^2}$. The probability vector for the next layer can be represented as,
{{
\begin{align}
p^{l+1} = \mathcal{W}^{l}_d ({\rm ReLU}(\mathcal{W}^{l}_u p^{l} \cdot( f(s_t) \cdot h(z_\task))) ),
\end{align}}}
where $\mathcal{W}^{l}_u$ is a fully connected layer in $\mathbb{R}^{D \times n^2}$ dimensions, which converts the probability vector to an embedding which has the same dimension as the task embedding representation and observation representation. We perform element-wise multiplication between these three embeddings to obtain a new feature representation, combining the information from the probabilities of previous layer, the observation and the task information. This feature is then forwarded to another fully connected layer $\mathcal{W}^{l}_d \in \mathbb{R}^{n^2 \times D}$, which leads to the probability vector for the next layer $p^{l+1}$. We visualize this process on computing $p^{l=2}$ from $p^{l=1}$ in Figure~\ref{fig:model}. To compute the first layer of probabilities, we use the inputs from both the task embedding and the state representation as,
\begin{align}
p^{l=1} = \mathcal{W}^{l=1}_d ({\rm ReLU}(f(s_t) \cdot h(z_\task)) ),
\end{align}
where $f(s_t)$ is the feature representation of the state with $D$ dimensions. To weight modules in the base policy network, we use softmax function to normalize $p^{l}$ as,
\vspace{-0.05in}
\begin{align}
\hat{p}^{l}_{i,j} = \frac{\exp{(p^{l}_{i,j})}}{\sum_{j=1}^{n}\exp{(p^{l}_{i,j})}},
\label{eq:prop}
\end{align}
which is the probability of weighting the $j$th module in the $l$th layer for contributing to the $i$th module in the $l+1$ layer. We will illustrate how this is used in the base policy network in the following.

\textbf{Base Policy Network.} As shown in the left side of Figure~\ref{fig:model}, our base policy network has $L$ layers of modules, and each layer contains $n$ modules. We denote that the input for the $j$th module in the $l$th layer is a $d$-dimensional feature representation $g_j^l \in \mathbb{R}^{d}$. The input feature representation for the $i$th module in the $l+1$ layer can be represented as,
\vspace{-0.1in}
\begin{align}
g_{i}^{l+1} = \sum_{j=1}^n \hat{p}^{l}_{i,j} ({\rm ReLU}(W_j^l g_j^l)),
\end{align}
where $W_j^l \in \mathbb{R}^{d \times d}$ represents the module parameters. We compute a weighted sum of the module outputs with the routing probability outputs. Recall from Eq.~\ref{eq:prop} that $\hat{p}^{l}_{i,j}$ represents the probability connecting the $j$th module in layer $l$ to the $i$th module in layer $l+1$ and it is normalized to $\sum_j \hat{p}^{l}_{i,j} = 1$. 
Given the final layer module outputs, we compute the mean and variance of the action as the outputs,
\vspace{-0.1in}
\begin{align}
\mu, \sigma = \sum_{j=1}^n W_j^L g_j^L,
\end{align}
where $W_j^L \in \mathbb{R}^{d \times o}$ are the module parameters in the last layer, $o$ represents the output dimension. 

Note that although we have only introduced the network architectures for policies so far, we adopt similar architectures with soft modularization for Q-function as well. The weights for both the base policy network and the routing network are not shared or reused in the Q-function. 


\vspace{-0.07in}
\subsection{Multi-task Optimization}
\vspace{-0.07in}

We focus on the problem of balancing the learning across different tasks, as easier tasks usually converge faster than the harder ones. We scale the training objectives for the policy network with different weights for different tasks. These weights are learned automatically: the objective weight will be small if the confidence of the policy for the task is high, and be large if the confidence is low. 

This loss weight is directly related to the temperature parameter $\alpha$ in SAC, trained via Eq.~\ref{eq:ecsac:alpha_objective}: When value of $\log \pi_\phi(a_t|s_t)$ become larger, which means entropy become smaller, $\alpha$ will become larger to encourage exploration. On the other hand, $\alpha$ will become small if $\log \pi_\phi(a_t|s_t)$ is small. We have different temperature parameters for $M$ different tasks: $\{\alpha_i\}_{i=1}^M$. The objective weights $w_i$ for task $i$ are proportional to the exponential of negative $\alpha_i$,
\vspace{-0.1in}
\begin{align}
w_i  = \frac{\exp{(-\alpha_i)}}{\sum_{j=1}^M \exp{(-\alpha_j)}}.
\label{eq:ecsac:multitask_alpha_objective}
\end{align}
We then adjust the optimization objective from Eq.~\ref{eq:jphi} as,
$J_\pi(\phi)=\mathbb{E}_{\task \sim p(\task)} \left[w_\task \cdot J_{\pi, \task}(\phi) \right],$
and the objetive for Q-fuction from Eq.~\ref{eq:jtheta} is adjusted as,
$
J_Q(\theta)=\mathbb{E}_{\task \sim p(\task)} \left[w_\task \cdot J_{Q, \task}(\theta) \right].$
 
\vspace{-0.1in}
\section{Experiments}\label{sec:expr}
\vspace{-0.1in}
We perform experiments on multi-task robotics manipulation. We discuss the experiment environment, benchmark, and baselines,  compare our method with baselines and conduct ablation study. 

\vspace{-0.07in}
\subsection{Environment}
\vspace{-0.07in}
We evaluate our approach with the recent proposed Meta-World~\cite{yu2019meta} environment. This environment contains 50 different robotics continuous control and manipulation tasks with a sawyer arm in the MuJoCo environment~\cite{todorov2012mujoco}. There are two challenges for multi-task learning in this environment: MT10 and MT50 challenge, which requires learning 10 and 50 manipulation tasks simultaneously. Building on top of these two challenges, we further extend the tasks to be goal-conditioned tasks. More specifically, the original MT10 and MT50 tasks are manipulation tasks with fixed goals. To make the tasks more realistic, we extend the tasks to have flexible goals. We name the two extensions as \textbf{MT10-Conditioned} and \textbf{MT50-Conditioned} tasks meanwhile we denote the original MT10 challenge as \textbf{MT10-Fixed} and the original MT50 challenge as \textbf{MT50-Fixed}.
 





\vspace{-0.07in}
\subsection{Baselines and Experimental Settings}
\vspace{-0.07in}

\textbf{Baselines.} We train our model with SAC~\cite{DBLP:journals/corr/abs-1801-01290}. We compare to five baselines with SAC without using our network architecture as following: (i) \textbf{Single-task SAC}: Individual policy for each task in MT10-Conditioned.
(ii) \textbf{Multi-task SAC (MT-SAC)}: Using a one-hot task ID with the state as inputs. (iii) \textbf{Multi-task multi-head SAC (MT-MH-SAC)}: Built upon MT-SAC with independent heads for tasks. The same MT-SAC and MT-MH-SAC baselines are also proposed in~\cite{yu2019meta}, and we reproduce their results. (iv) \textbf{Mixture of Experts (Mix-Expert)}: It consists of four experts with the same architecture as MT-SAC, and a learned gating network for expert combination~\cite{jacobs1991}. (v) \textbf{Hard Routing}: It consists of four module layers with four modules each layer. For each layer, agent selects one module to use according to the controller/router depending on the task, following~\cite{rosenbaum2017routing}.



\textbf{Variants of Our Approach.} We conduct all experiments under two settings of our method. We ablate different numbers of module layer and modules in each layer: \textbf{Ours (Shallow)} contains $L=2$ module layers, $n=2$ modules per layer and each module outputs a $d=256$ representation; \textbf{Ours (Deep)} contains $L=4$ module layers, $n=4$ modules per layer and each module outputs a $d=128$ representation. The number of parameters is the same in both cases.


\textbf{Evaluation Metrics.} We evaluate the policies based on the success rate of executing the tasks, which is well-defined in the Meta-World environment\cite{yu2019meta}. We use the average success rate cross tasks to measure the performance. For each experiment, we train all methods with 3 random seeds. To plot the training curves, we plot the success rate of the polices across time with variance. For the final performance, we directly evaluate the final policy for each approach. We sample 100 episodes per task per seed. We compute the success rate for all these trials and report the averaged results. 

\textbf{Training samples.} For MT-SAC and MT-MH-SAC baselines, we train them with 20 million samples on the MT10 setting and 100 million samples on the MT50 setting. For our method, Mix-Expert and Hard Routing Baselines, they converge much faster, and we train it with 15 million samples for MT10 and with 50 million samples for MT50 tasks. 


\vspace{-0.1in}
\subsection{Routing Network Visualization}
\vspace{-0.1in}

We perform visualization on the networks trained with Ours (Deep) on the MT10-Conditioned setting. 

\begin{figure}
\centering
\includegraphics[width=0.22\textwidth, clip]{figs/compare-drawer-close-v1-ped-insert-side-v1.pdf}
\includegraphics[width=0.22\textwidth, clip]{figs/compare-push-v1-window-open-v1.pdf}
\includegraphics[width=0.22\textwidth, clip]{figs/compare-reach-v1-pick-place-v1.pdf}
\includegraphics[width=0.22\textwidth, clip]{figs/compare-door-v1-drawer-open-v1.pdf}
\vspace{-0.05in}
    \caption{\small{Sampled observation and corresponding routing. Each column shows two different tasks sharing similar routing. 
The shared parts are highlighted with blue boxes.}
    }
	\label{figure:demo}
    \vspace{-0.2in}
 \end{figure}


\begin{wrapfigure}{li}{0.42\textwidth}
\vspace{-0.2in}
\begin{center}
\includegraphics[width=0.42\columnwidth,clip]{figs/routing_visualization.png}
\vspace{-0.2in}
\caption{Probabilities from the routing network for different tasks are extracted and visualized with t-NSE. Routing probabilities from different tasks are grouped in different clusters.}
\label{figure:routing_tsne}
\vspace{-0.3in}
\end{center}
\end{wrapfigure}

\textbf{Probability Visualization.} We visualize the probabilities $p^l$ predicted by the routing network. Ours (Deep) contains $l=4$ module layers with $n=4$ modules per layer. As shown in Figure~\ref{figure:demo}, we plot $p^l$ as the connections between different modules and use deep red color to represent large probability and light red color for small probability. For each column, we visualize the routing networks for two different tasks. We can see that even for different tasks, they could share similar module connections. It shows that our soft modularization method allows the reuse of skills across different tasks. 

\textbf{t-SNE Visualization.} We visualize the routing probabilities for different tasks via t-SNE~\cite{Maaten08visualizingdata} in Figure~\ref{figure:routing_tsne}. We run the policy on each task in MT10-Conditioned multiple times to collect routing samples. We combine all the routing probabilities from all layers into a $(l-1)n^2=48$ dimensional vector representing the routing path and visualize via t-SNE. We find clear boundaries between tasks, indicating that the agent can distinguish different tasks and choose corresponding skillset for each task. Besides, we notice that those tasks sharing similar task structures (e.g., drawer-open-v1 and drawer-close-v1, window-open-v1 and window-close-v1) are close in the plot. 

\vspace{-0.1in}
\subsection{Quantitative Results}
\vspace{-0.1in}


\textbf{Results on MT10-Fixed.} As shown in Table~\ref{tab:MT50andMT10}, our re-implementation of multi-task multi-head SAC performs very close to the reported results in~\cite{yu2019meta}. Although the final success rate of our method is only $2\%$ better than our best baseline implementation, our method converges faster than the baselines, as shown in the 2nd plot in Figure~\ref{figure:general_comparison}. We are not getting a significant gain in the final success rate is because training 10 tasks with fixed goals is quite simple. We move forward to a more practical and challenging setting with training  goal-conditioned policies. 

\textbf{Results on MT10-Conditioned.} As task difficulty increases, we can see from Table~\ref{tab:MT50andMT10} that our approach (Ours (Shallow)) achieves more than $4\%$ improvement over the baseline. Our approaches continue to improve the sample efficiency over the MT-MH-SAC baselines (1st plot in Figure~\ref{figure:general_comparison}). 

\begin{figure}[t]
\begin{center}
    \centering
        \includegraphics[width=\textwidth, clip]{figs/all_training_curve_nips.pdf}
        \vspace{-0.25in}
        \caption{Training curves of different methods on all benchmarks (Concrete lines: the average over 3 seeds; Shaded areas: the standard deviation over 3 seeds). For MT10, our method converges much faster than the baselines. For MT50, we achieve a large gain on sample efficiency and performance.}
    \label{figure:general_comparison}
\end{center}
\vspace{-0.1in}
\end{figure}

\textbf{Results on MT50-Fixed and MT50-Conditioned.} When we are moving from joint training with 10 tasks to 50 tasks, the problem becomes more challenging. As shown in Table~\ref{tab:MT50andMT10} and the last two plots in Figure~\ref{figure:general_comparison}, our method achieves a significant improvement over the baseline methods (around $24\%$) in both the fixed goal and goal-conditioned settings. We also observe that in MT50 environments, Ours (Deep) performs better than Ours (Shallow) approach, while it is the opposite in the MT10 setting. The reason for this phenomenon might be: (i) for a smaller number of task (MT10), simple network topology facilitates more on information sharing across tasks; (ii) for larger number of task (MT50), more complex network topology provides more routing choices and prevents different tasks from harming the performance of each other. It is also worthy of mentioning that our method achieves better success rates in MT50-Conditioned environments than MT50-Fixed. The reason is that MT50-Conditioned provides more examples in training for better generalization.

\textbf{Mixture of Experts and Hard Routing baselines.} We notice that although the performance of Mixture of Experts is only close to MT-SAC on MT10, it performs better than MT-MH-SAC on MT50. The reason is that when the number of tasks is small, Mixture of Experts is easy to degenerate to MT-SAC (with a single network). When the number of tasks becomes larger, the gating network in mixture of experts can learn to cluster the tasks into different sets corresponding to different experts. Similarly, while the Hard Routing baseline performs poorly in MT10, it catches up with the MT-SAC when applied to 50 tasks. It shows that routing still helps when task number increases. However, the optimization with hard routing is extremely challenging (see discussions in Section~\ref{related}). Both baselines perform significantly worse than our method in both MT10 and MT50 tasks. 


\begin{table*}[t]
\vspace{-0.05in}
\centering
\tablestyle{2pt}{1.05}
\begin{tabular}{l|x{50}x{80}x{50}x{80}}
\multicolumn{1}{c|}{Method} & MT10-Fixed & MT10-Conditioned & MT50-Fixed & MT50-Conditioned \\
\shline
MT-SAC$^{*}$        & 39.5\%          & -      & 28.8\%  & - \\
MT-SAC              & 44.0\%          & 42.6\% & 31.4\%  & 28.3\% \\
MT-MH-SAC$^{*}$     & \textbf{88.0\%} & -      & 35.9\%  & - \\
MT-MH-SAC           & 85.0\%          & 67.4   & 35.5\% & 34.2\% \\
Mix-Expert          & 42.8\%  & 40.0\% & 36.1\% & 37.5\% \\   
Hard Routing        & 20.8\%  & 27.0\% & 22.9\% & 29.1\% \\

\hline
Ours (Shallow) & 87.0\% & \textbf{71.8\%} & 59.5\% & 60.4\%  \\
Ours (Deep)    & 86.7\% & 68.4\% & \textbf{60.0\%} &  \textbf{61.0\%} \\
\end{tabular}
\vspace{-0.1in}
\caption{Comparisons on average success rates for MT10 and MT50 tasks. MT-SAC$^*$, MT-MH-SAC$^*$ indicate results reported in \cite{yu2019meta}. Approaches without $^*$ indicate baselines of our own implementation. \label{tab:MT50andMT10}}

\vspace{-0.2in}
\end{table*}


\vspace{-0.07in}
\subsection{Effects on Network Capacity}
\vspace{-0.07in}

We conduct experiments to see how the capacity of the network (number of parameters) can influence the performance of the baseline methods. We compare our approach with baselines using different numbers of parameters for MT50-Fixed in Table~\ref{tab:network_ablation}. We ablate different number of network layers and the number of hidden units in each layer. We denote MT-MH-SAC-$l$ as the multi-task multi-head SAC baseline with $l$ layers. We also ablate more hidden unites for each layer and name the methods with ``Wide''. The detailed configurations for different ablations are shown in Table~\ref{tab:network_ablation}. 

\begin{figure*}[t]
    \begin{center}
    \subfloat[MT50-Fixed]{
        \includegraphics[width=0.3\textwidth,clip]{figs/ablation_networksize_width_and_depth.png}
        \label{exp:capacity}
    }
    \subfloat[MT10-Conditioned]{
        \includegraphics[width=0.3\textwidth, clip]{figs/ablation.png}
        \label{exp:ablation1}
    }
    \subfloat[MT10-Conditioned]{
        \includegraphics[width=0.3\textwidth, clip]{figs/ablation_mtmhsac_balance.png}
        \label{exp:ablation2}
    }
    \vspace{-0.1in}
    \caption{\small{(a) Compare Ours (Deep) and baselines with different network capacity for MT50-Fixed. (b) Analyse balancing training samples and using observation for routing network in Ours (Shallow) for  MT10-Conditioned. (c) Analyse balancing training samples in the baseline for MT10-Conditioned.}}
	\label{figure:curve}
    \end{center}{}
\vspace{-0.1in}
\end{figure*}


\begin{table}
\tablestyle{2pt}{1.05}
\begin{tabular}{l|x{50}x{30}x{30}x{30}}
\multicolumn{1}{c|}{Method} & MT50-Fixed & Params & layers & units\\
\shline
MT-MH-SAC$^{*}$  & 35.9\% & 1.2x & 3 & 400\\
MT-MH-SAC        & 35.5\% & 1.2x & 3 & 400\\
MT-MH-SAC-4      & 46.7\% & 1.6x & 4 & 400\\
MT-MH-SAC-5      & 45.2\% & 2.0x & 5 & 400\\
MT-MH-SAC-6      & 45.0\% & 2.4x & 6 & 400\\

MT-MH-SAC-4-Wide$^{*}$     & 50.7\% & 3.3x & 4 & 600 \\
MT-MH-SAC-5-Wide$^{*}$     & 50.3\% & 4.2x & 5 & 600\\
\hline
Ours (Deep)    & \textbf{60.0\%} & 1x & - & - \\
\end{tabular}
\caption{Comparison with baselines using different number of parameters for MT50-Fixed.\label{tab:network_ablation}}
\vspace{-0.25in}
\end{table}


We observe that even our model uses the smallest number of parameters, we can still achieve much better results. For example, our method is around $10\%$ better than the baseline (MT-MH-SAC-5-Wide) which has $4.2$x number of parameters compared to our method. We also observe that the gain saturates very fast as we make the network larger and larger: The baseline with $4.2$x capacity is slightly worse than the baseline with $3.3$x capacity. We visualize the training curve in Figure~\ref{exp:capacity}: our method converges faster and has much better performance than large capacity baselines. 



\vspace{-0.07in}
\subsection{Comparison with Single Task Policy}
\vspace{-0.07in}


A substantial advantage of multi-task learning is with sample efficiency. We compare our policy with single task policy on MT10-Conditioned. Given 15 million samples, Ours (Shallow) achieve $71.8\%$ average success rate, while by average single task policy achieved $78.5\%$ success rate.
Though the single task policy can overfit easily given enough training examples and achieve a very good result for one specific task but our method can still perform reasonably close to the single task policy, even we train with much fewer examples with much fewer parameters via a shared network. It shows that for each task, using data from other tasks along with our method can significantly improve sample efficiency, and skills learned by the soft modular policy can be shared between tasks with routing.

\vspace{-0.07in}
\subsection{Analysing Learning Components}
\vspace{-0.07in}
We analyze the importance of two learning components in our method with MT10-Conditioned: (i) Balance the training across different tasks using temperature parameters (Eq.~\ref{eq:ecsac:multitask_alpha_objective}); (ii) Use observation representation as the inputs for the routing network. 
We report the comparison results in Figure~\ref{exp:ablation1} and Figure~\ref{exp:ablation2}. In Figure~\ref{exp:ablation2}, we ablate our method in the Ours (Shallow) setting, and remove the balance training (Ours (Shallow, w/o Balance)) as well as remove both the balance training and observation inputs for the routing network (Ours (Shallow, w/o Obs $\&$ Balance)).
If we remove one or both learning components, the success rate is reduced by a large margin. Thus both components play an important role in our approach.
The importance of encoding observation for our routing network might also be a reason for the poor performance of Hard Routing baseline, since the controller of Hard Routing is parameterized by tabular lookup table which can not encode high dimensional information like observation.
We also apply the balance training strategy to the baseline
in Figure~\ref{exp:ablation2} as MT-MH-SAC-Balance. Interestingly, we find that the baseline approach is not affected as much with the new optimization strategy. Thus we do not apply balance training for baselines. 
 
\vspace{-0.1in}
\section{Conclusion}
\vspace{-0.1in}
In this paper, we propose multi-task RL with soft modularization for robotics manipulation tasks. Our method improves the sample efficiency as well as the success rate over the baselines by a large margin. The advantage becomes more obvious when given more diverse tasks. This shows that soft modularization allows effective sharing and reusing network components across tasks, which opens up future opportunities to generalize the policy to unseen tasks in a zero-shot manner. 

 
\vspace{-0.1in}
\section{Potential Broader Impact}
\vspace{-0.1in}
Our work provided a simple and effective framework for skill and component reuse in the multi-task RL domain, which the community can build off. With the learned skill module, Our work can also inspire work on zero-shot skill transferring and sharing. 

With improved sample efficiency and potential zero-shot skill transfer, the community might be able to use reinforcement learning to solve tasks that not feasible before and build the robots that can generalize to different tasks. 
These robots could potentially bring lots of new possibilities in almost every aspect of people's daily life, e.g., self-driving cars and house-hold robots.  Besides, general learned robotics can also be useful for unseen or urgent out-of-distribution scenes. For instance, when it comes to performing a rescue under the earthquake, the robot should have the ability to cope with different conditions. 

In the deep learning era, collecting samples and training large models could consume a lot energy and release a massive amount of carbon dioxide. With better sample efficiency, training reinforcement learning policy for real-world settings can be much more environment-friendly. Meanwhile, better sample efficiency can also lower the bar and be more accessible for inexperienced researchers to get into the field.

{\noindent {\bf Acknowledgement}: This work is supported, in part, by grants from DARPA LwLL, NSF 1730158 CI-New: Cognitive Hardware and Software Ecosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349 CC*DNI Pacific Research Platform,  a research grant from Qualcomm, and gift from TuSimple.}

 

\bibliography{reference}

\bibliographystyle{plain}
\setcitestyle{numbers}

\end{document}
