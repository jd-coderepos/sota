\documentclass{LMCS}

\makeatletter
\@ifundefined{lhs2tex.lhs2tex.sty.read}{\@namedef{lhs2tex.lhs2tex.sty.read}{}\newcommand\SkipToFmtEnd{}\newcommand\EndFmtInput{}\long\def\SkipToFmtEnd#1\EndFmtInput{}}\SkipToFmtEnd

\newcommand\ReadOnlyOnce[1]{\@ifundefined{#1}{\@namedef{#1}{}}\SkipToFmtEnd}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\newcommand{\tex}[1]{\text{\texfamily#1}}	

\newcommand{\Sp}{\hskip.33334em\relax}


\newcommand{\Conid}[1]{\mathit{#1}}
\newcommand{\Varid}[1]{\mathit{#1}}
\newcommand{\anonymous}{\kern0.06em \vbox{\hrule\@width.5em}}
\newcommand{\plus}{\mathbin{+\!\!\!+}}
\newcommand{\bind}{\mathbin{>\!\!\!>\mkern-6.7mu=}}
\newcommand{\rbind}{\mathbin{=\mkern-6.7mu<\!\!\!<}}\newcommand{\sequ}{\mathbin{>\!\!\!>}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\usepackage{polytable}

\@ifundefined{mathindent}{\newdimen\mathindent\mathindent\leftmargini}{}

\def\resethooks{\global\let\SaveRestoreHook\empty
  \global\let\ColumnHook\empty}
\newcommand*{\savecolumns}[1][default]{\g@addto@macro\SaveRestoreHook{\savecolumns[#1]}}
\newcommand*{\restorecolumns}[1][default]{\g@addto@macro\SaveRestoreHook{\restorecolumns[#1]}}
\newcommand*{\aligncolumn}[2]{\g@addto@macro\ColumnHook{\column{#1}{#2}}}

\resethooks

\newcommand{\onelinecommentchars}{\quad-{}- }
\newcommand{\commentbeginchars}{\enskip\{-}
\newcommand{\commentendchars}{-\}\enskip}

\newcommand{\visiblecomments}{\let\onelinecomment=\onelinecommentchars
  \let\commentbegin=\commentbeginchars
  \let\commentend=\commentendchars}

\newcommand{\invisiblecomments}{\let\onelinecomment=\empty
  \let\commentbegin=\empty
  \let\commentend=\empty}

\visiblecomments

\newlength{\blanklineskip}
\setlength{\blanklineskip}{0.66084ex}

\newcommand{\hsindent}[1]{\quad}\let\hspre\empty
\let\hspost\empty
\newcommand{\NB}{\textbf{NB}}
\newcommand{\Todo}[1]{\textbf{To do:}~#1}

\EndFmtInput
\makeatother
\ReadOnlyOnce{polycode.fmt}\makeatletter

\newcommand{\hsnewpar}[1]{{\parskip=0pt\parindent=0pt\par\vskip #1\noindent}}

\newcommand{\hscodestyle}{}



\newcommand{\sethscode}[1]{\expandafter\let\expandafter\hscode\csname #1\endcsname
   \expandafter\let\expandafter\endhscode\csname end#1\endcsname}



\newenvironment{compathscode}{\par\noindent
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \par\noindent
   \ignorespacesafterend}

\newcommand{\compaths}{\sethscode{compathscode}}



\newenvironment{plainhscode}{\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\hspre\pboxed}{\endpboxed \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

\newenvironment{oldplainhscode}{\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}



\newcommand{\plainhs}{\sethscode{plainhscode}}
\newcommand{\oldplainhs}{\sethscode{oldplainhscode}}
\plainhs



\newenvironment{arrayhscode}{\hsnewpar\abovedisplayskip
   \advance\leftskip\mathindent
   \hscodestyle
   \let\\=\@normalcr
   \hsnewpar\belowdisplayskip
   \ignorespacesafterend}

\newcommand{\arrayhs}{\sethscode{arrayhscode}}



\newenvironment{mathhscode}{\parray}{\endparray}

\newcommand{\mathhs}{\sethscode{mathhscode}}



\newenvironment{texthscode}{}

\newcommand{\texths}{\sethscode{texthscode}}



\def\codeframewidth{\arrayrulewidth}
\RequirePackage{calc}

\newenvironment{framedhscode}{\parskip=\abovedisplayskip\par\noindent
   \hscodestyle
   \arrayrulewidth=\codeframewidth
   \tabular{@{}|p{\linewidth-2\arraycolsep-2\arrayrulewidth-2pt}|@{}}\hline\framedhslinecorrect\\{-1.5ex}\let\endoflinesave=\\
   \let\\=\@normalcr
   \framedhslinecorrect\endoflinesave{.5ex}\hline
   \endtabular
   \parskip=\belowdisplayskip\par\noindent
   \ignorespacesafterend}

\newcommand{\framedhslinecorrect}[2]{#1[#2]}

\newcommand{\framedhs}{\sethscode{framedhscode}}



\newenvironment{inlinehscode}{1][]{}\def\fromto##1##2##3{##3}\def\nextline{}}{\prooftree
\ifinsideprooftree\let}}} 

\newcommand{\of}{\!:\!}
\newcommand{\subtype}{\mathrel{\mathord{<}\mathord{:}}}
\newcommand{\derN}{\vdash}
\newcommand{\FV}{\mathsf{FV}}
\newcommand{\ru}{\dfrac}
\newcommand{\NN}{\mathbb{N}}
\def\bprg#1\eprg{}


\newcommand{\ABwhere}[3]{#1 \; \mathsf{where}\; [#2] \leftarrow #3}

\newcommand{\lambdaSing}{\texorpdfstring{\ensuremath{\lambda^{\mathsf{Sing}}}}{Singletons}}
\newcommand{\lambdaPI}{\texorpdfstring{\ensuremath{\lambda^{\mathsf{Irr}}}}{Proof-irrelevance}}
\newcommand{\lambdaIrr}{\lambdaPI}

\newcommand{\Vecraw}{\mathsf{Vec}}
\newcommand{\tVec}[1]{\Vecraw\;#1\;}
\newcommand{\tleqraw}{\mathsf{leq}}
\newcommand{\tleq}[1]{\tleqraw\;#1\;}
\newcommand{\Trueraw}{\mathsf{True}}
\newcommand{\TrueC}{\Trueraw\;}
\newcommand{\tlookupraw}{\mathsf{lookup}}
\newcommand{\tlookup}[4]{\tlookupraw\;#1\;#2\;#3\;#4\;}
\newcommand{\funT}[2]{(#1 \of #2) \to}
\newcommand{\funS}[2]{(#1 : #2) \to}
\newcommand{\sigT}[2]{(#1 \of #2) \times}
\newcommand{\Bool}{\mathsf{Bool}}
\newcommand{\ttrue}{\mathsf{true}}
\newcommand{\tfalse}{\mathsf{false}}
\newcommand{\Ltraw}{\mathsf{Lt}}
\newcommand{\Lt}[1]{\Ltraw\,#1\,}
\newcommand{\tmagicraw}{\mathsf{magic}}
\newcommand{\tmagic}[1]{\tmagicraw\,#1\,}

\newcommand{\den}{\semc}
\newcommand{\Den}{\semc}
\newcommand{\Lam}{\mathsf{Lam}}
\newcommand{\App}{\mathsf{App}}
\newcommand{\D}{\mathsf{D}}
\newcommand{\dbindex}[1]{\tq\,\tp^{#1}}
\newcommand{\tq}{\mathsf{q}}
\newcommand{\tp}{\mathsf{p}}
\newcommand{\up}{\upa}
\newcommand{\down}{\da}
\newcommand{\dapp}{\cdot}

\newcommand{\A}{\mathcal{A}}
\newcommand{\seq}[1]{#1^*}
\newcommand{\Aseq}{\seq\A}
\newcommand{\tfst}{\mathsf{fst}}
\newcommand{\tsnd}{\mathsf{snd}}
\newcommand{\tPair}{\mathsf{Pair}}
\newcommand{\update}[3]{#1[#3/#2]}

\newcommand{\tyrule}[3]{\inferrule* [right=(#1)] {#2} {#3}} 
\newcommand{\tyrulenl}[3]{\inferrule* [right=(#1)] {#2} {#3}}

\newcommand{\algrule}[2]{\inferrule* {#1} {#2}} 

\newcommand{\rulename}[1]{\ensuremath{\mbox{\textsc{(#1)}}}}

\newcommand{\dashVv}{-\!\!\!\!\parallel}

\newcommand{\ctx}{\mathsf{Ctx}}
\newcommand{\type}[1]{\mathsf{Type}(#1)}
\newcommand{\term}[2]{\mathsf{Term}(#1,#2)}

\newcommand{\into}{\rightarrow}

\newcommand{\ectx}{\diamond} 
\newcommand{\ctxe}[2]{#1.#2} 

\newcommand{\idsubs}[1]{\mathsf{id}_{#1}}
\newcommand{\esubs}{\langle\,\!\rangle}
\newcommand{\exsubs}[2]{( #1, #2)}
\newcommand{\subsc}[2]{#1\,#2}
\newcommand{\subsTm}[2]{#1\,#2}
\newcommand{\subsTy}[2]{#1\,#2}

\newcommand{\TmU}{\mathsf{U}}
\newcommand{\F}[2]{\mathsf{Fun}\,#1\,#2}
\newcommand{\DSum}[2]{\Sigma\,#1\,#2}
\newcommand{\p}{\mathsf{p}}
\newcommand{\q}{\mathsf{q}}
\newcommand{\appTm}[2]{\mathsf{app}\;#1\;#2} \newcommand{\singTm}[2]{\{#1\}_{#2}}
\newcommand{\depair}[2]{(#1,#2)}
\newcommand{\dfst}[1]{\mathsf{fst}\;#1}
\newcommand{\dsnd}[1]{\mathsf{snd}\;#1}
\newcommand{\natty}{\mathsf{Nat}}
\newcommand{\ztm}{\mathsf{zero}}
\newcommand{\suctm}[1]{\mathsf{suc}\;#1}
\newcommand{\natrecraw}{\mathsf{natrec}}
\newcommand{\natrec}[4]{\natrecraw\;#1\;#2\;#3\;#4}
\newcommand{\unit}{()}
\newcommand{\boxty}[1]{[#1]}
\newcommand{\boxtm}[1]{[#1]}
\newcommand{\subid}[2]{\exsubs{\idsubs{#1}}{#2}}
\newcommand{\ind}[1]{\mathsf{v}_{#1}} 

\newcommand{\dctx}[1]{#1\vdash} 
\newcommand{\dtype}[2]{#1\vdash#2} 
\newcommand{\dterm}[3]{#1\vdash#3:#2} 
\newcommand{\dsubs}[3]{#1\vdash#3:#2} 
\newcommand{\deqctx}[2]{\vdash#1=#2} 
\newcommand{\deqtype}[3]{#1\vdash#2=#3} 
\newcommand{\deqterm}[4]{#1\vdash#3=#4:#2} 
\newcommand{\deqsubs}[4]{#1\vdash #3=#4: #2} 



\newcommand{\vdashp}{\vdash^{\oprf}}\newcommand{\sdctx}[1]{#1\vdashp} 
\newcommand{\sdtype}[2]{#1\vdashp#2} 
\newcommand{\sdterm}[3]{#1\vdashp#3:#2} 
\newcommand{\sdsubs}[3]{#1\vdashp#3:#2} 
\newcommand{\sdeqctx}[2]{\vdashp#1=#2} 
\newcommand{\sdeqtype}[3]{#1\vdashp#2=#3} 
\newcommand{\sdeqterm}[4]{#1\vdashp#3=#4:#2} 
\newcommand{\sdeqsubs}[4]{#1\vdashp #3=#4: #2} 

\newcommand{\der}[2]{#1\!\!::\!\!#2} 

\newcommand{\lift}[2]{\subsTm{#2}{\p^{#1}}}
\newcommand{\Da}[1]{\mathop{\Downarrow} #1} 
\newcommand{\upa}[2]{\mathop{\uparrow}\nolimits_{#1}{#2}} 
\newcommand{\da}[2]{\mathop{\downarrow}\nolimits_{#1}{#2}}
\newcommand{\reify}[2]{\mathsf{R}_{#1}\,#2}
\newcommand{\reifyC}[2]{\mathsf{R}_{|#1|}\,#2}

\newcommand{\calR}{\mathcal{R}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\smashed}[1]{|#1|}
\newcommand{\singD}[2]{\{\!\!\{#1\}\!\!\}_{#2}}
\newcommand{\singDU}[2]{\{\!\!\{#2\}\!\!\}_{#1}}
\newcommand{\sigD}[2]{\coprod\,#1\,#2}

\newcommand{\dom}{\mathit{dom}}
\newcommand{\vdom}{\dom}
\newcommand{\perD}{\mbox{PER}(D)}
\newcommand{\perne}{\mathord{\mathcal{N}\!\mathit{e}}}
\newcommand{\pernf}{\mathord{\mathcal{N}\!\mathit{f}}}
\newcommand{\perU}{\mathcal{U}}
\newcommand{\perT}{\mathcal{T}}
\newcommand{\Per}{\mathsf{Per}}

\newcommand{\Var}{\mathit{Var}}

\newcommand{\semc}[1]{[\![#1]\!]} 
\newcommand{\rel}{\sim} 


\newcommand{\one}{\mathbf{1}}
\newcommand{\dashV}{-\!\!\parallel}
\newcommand{\pairD}[2]{( #1,#2)}
\newcommand{\pairF}[2]{(#1,#2)}

\newcommand{\iLam}[1]{\mathsf{Lam}\,#1}
\newcommand{\iNe}[2]{\mathsf{App}\,#1\,#2}
\newcommand{\iPair}[2]{(#1 , #2)}
\newcommand{\iVar}[1]{\mathsf{Var}\,x_{#1}}
\newcommand{\iO}{\top}
\newcommand{\iU}{\mathsf{U}}
\newcommand{\iPi}[2]{\mathsf{Fun}\,#1\,#2}
\newcommand{\iSing}[2]{\mathsf{Sing}\,#1\,{#2}}
\newcommand{\iDs}[2]{\mathsf{Sum}\,#1\,#2}
\newcommand{\iDp}[2]{\iota_\mathsf{Dep} (#1,#2)}
\newcommand{\Fst}{\mathsf{Fst}}
\newcommand{\Snd}{\mathsf{Snd}}
\newcommand{\iBoolF}[1]{\Fst\;#1}
\newcommand{\iBoolT}[1]{\Snd\;#1}



\newcommand{\fst}{\tfst}\newcommand{\snd}{\tsnd}\newcommand{\appD}{\mathsf{appD}}
\newcommand{\fstnew}{\tfst\,}\newcommand{\sndnew}{\tsnd\,}

\newcommand{\ertype}[1]{\overline{#1}}
\newcommand{\ersubs}[1]{\overline{\overline{#1}}}

\newcommand{\terms}{\mathit{Terms}}

\newcommand{\neterms}{\mathit{Ne}}
\newcommand{\nfterms}{\mathit{Nf}}
\newcommand{\nfsubs}{\mathit{Ns}}
\newcommand{\nfctxt}{\mathit{Nc}}

\newcommand{\chkctx}[1]{#1\Leftarrow}
\newcommand{\chktype}[2]{#1 \vdash #2\, \mathord{\Leftarrow}}
\newcommand{\chkterm}[3]{#1\vdash#3\Leftarrow #2}
\newcommand{\inftype}[3]{#1\vdash #2\Rightarrow #3}

\newcommand{\nbe}[1]{\mathbf{nbe}(#1)}
\newcommand{\nbety}[2]{\mathbf{nbe}_{#1}(#2)}
\newcommand{\nbetm}[3]{\mathbf{nbe}_{#1}^{#2}(#3)}
\newcommand{\norm}[1]{\mathbf{nbe}(#1)}
\newcommand{\normty}[2]{\mathbf{nbe}_{#1}(#2)}
\newcommand{\normtm}[3]{\mathbf{nbe}_{#1}^{#2}(#3)}

\newcommand{\ruleref}[1]{(\RefTirName{#1})}
\newcommand{\oprf}{\dprf}
\newcommand{\dprf}{\star}
\renewcommand{\boxty}[1]{\mathsf{Prf}\,#1}
\newcommand{\boxpar}[1]{\mathsf{Prf}\,(#1)}
\newcommand{\prfraw}{\mathsf{Prf}}
\newcommand{\prf}[1]{\mathsf{Prf}\,#1}
\newcommand{\recty}[1]{\mathsf{Rec}(#1)}
\newcommand{\whereraw}{\mathsf{where}}
\newcommand{\wheretm}[3]{#1\,\whereraw\!^{#3}\,#2}
\newcommand{\choice}[2]{\upepsilon_{#1}\,#2}

\newcommand{\elimraw}{\mathsf{case}}
\newcommand{\Elimraw}{\mathsf{Case}}
\newcommand{\enum}[1]{\mathsf{N}_{#1}}
\newcommand{\elim}[4]{\elimraw\!^{#1}\;#2\;#3\;#4}
\newcommand{\elimn}[3]{\elimraw\;#1\;#2\;#3}
\newcommand{\const}[2]{\mathsf{c}^{#1}_{#2}}
\newcommand{\constn}[1]{\mathsf{c}_{#1}}
\newcommand{\constD}[2]{\mathsf{c}^{#1}_{#2}}
\newcommand{\elimD}[4]{\Elimraw\!^{#1}(#2, #3, #4)}
\newcommand{\enumD}[1]{\mathsf{N}_{#1}}
\newcommand{\tuple}[1]{\langle #1\rangle}
\newcommand{\erecraw}{\elimraw}
\newcommand{\erec}[4]{\erecraw^{#1}(#2,#3,#4)}

\newcommand{\enumrule}[2][n]{n\ensuremath{_{#1}}-{#2}}
\newcommand{\C}[1]{\ensuremath{\mathcal{C}_{#1}}}
\newcommand{\tnrecd}{[D\into D]\times D\times[D\into [D\into D]]\times D}
\newcommand{\iZero}{\ztm}
\newcommand{\iNat}{\natty}
\newcommand{\perNat}{\mathcal{N}}
\newcommand{\nrecraw}{\mathsf{natrec}}
\newcommand{\Nrecraw}{\mathsf{Natrec}}
\newcommand{\iNrec}[4]{\Nrecraw(#1,#2,#3,#4)}
\newcommand{\drec}[4]{\nrecraw(#1,#2,#3,#4)}
\newcommand{\iSuc}[1]{\mathsf{suc}\,#1}
\ifdefined\LONGVERSION
  \relax
\else
\newcommand{\LONGVERSION}[1]{}
\newcommand{\SHORTVERSION}[1]{#1}
\fi
\newcommand{\LONGSHORT}[2]{\LONGVERSION{#1}\SHORTVERSION{#2}}

\newcommand{\REDUNDANT}[1]{}\newcommand{\EXPLAINREDUNDANT}[1]{#1}

\newcommand{\PrfIrrTitle}{\subsection{Calculus \lambdaPI with proof irrelevance}}


\newcommand{\para}[1]{
\LONGSHORT{\paragraph{\it #1.}}
          {\vspace{1ex}\noindent{\it #1.}}
}
\newcommand{\localpara}[1]{\noindent #1.}
 
\def\doi{7 (2:4) 2011}
\lmcsheading {\doi}
{1--57}
{}
{}
{Nov.~15, 2009}
{May~\phantom{.~0}4, 2011}
{}   

\begin{document}

\title[A Modular Type-Checking Algorithm for Type Theory]{A Modular
  Type-Checking Algorithm for \\ Type Theory with Singleton Types and
  \\ Proof Irrelevance}

\author[A.~Abel]{Andreas Abel\rsuper a}	\address{{\lsuper a}Ludwig-Maximilians-Universit\"{a}t M\"{u}nchen}	\email{\abelmail}  \thanks{{\lsuper a}Supported by INRIA 
as guest researcher in the PI.R2 team, PPS,
Paris, France, from October 2009 to March 2010.}	

\author[T.~Coquand]{Thierry Coquand\rsuper b}	\address{{\lsuper b}G\"{o}teborg University}	\email{\coquandmail}  

\author[M.~Pagano]{Miguel Pagano\rsuper c} \address{{\lsuper c}Universidad Nacional de C\'{o}rdoba} \email{\paganomail} \thanks{{\lsuper c}Partially supported by CONICET, Argentina.} 





\keywords{type theory, type-checking, normalisation-by-evaluation,
  singleton types, proof-irrelevance} 

\subjclass{F.4.1}





\begin{abstract}
  \noindent We define a logical framework with singleton types and one
  universe of small types. We give the semantics using a PER model; it
  is used for constructing a normalis\-ation-by-evaluation algorithm.
  We prove completeness and soundness of the algorithm; and get as a
  corollary the injectivity of type constructors. Then we give the
  definition of a correct and complete type-checking algorithm for
  terms in normal form. We extend the results to proof-irrelevant
  propositions.
\end{abstract}

\maketitle

\newtheorem{lemma}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{remark}[thm]{Remark}
\newtheorem{definition}[thm]{Definition}
\newtheorem{theorem}[thm]{Theorem}
\section{Introduction and Related Work}
\label{sec:intro}
\setcounter{footnote}{0} 

\noindent One of the raisons d'\^{e}tre of
proof-checkers like Agda \cite{norell:PhD}, Coq \cite{coq81}, and
Epigram \cite{mcBride:afp04} is to decide if a given term has some
type (either checking for a given type or inferring one); i.e., if
a term corresponds to a proof of a proposition
\cite{harperHonsellPlotkin:LF}.  Hence, the convenience of such a
system is, in part, determined by the types for which the system can
check membership. We extend the decidability of type-checking done in
previous works \cite{abelAehligDybjer:mfps07,abelCoquandDybjer:lics07}
for Martin-L\"{o}f type theories \cite{mlitt,nordstroem:mltt} by
considering singleton types and proof-irrelevant propositions.

\LONGVERSION{We consider a type theory with a \emph{universe}, which
  allows large eliminations, i.e., types defined by recursion on
  natural numbers. 
The universe of small types was introduced by
  Martin-L\"{o}f \cite{martinlof72} for formalising category theory.
Martin-L\"of presents universes in two different styles \cite{mlitt}: 
  {\em \`a la Russell} (the one considered here), and {\em \`a la Tarski}.}

\emph{Singleton types} were introduced by Aspinall \cite{aspinall:csl94} in
the context of specification languages. An important use of singletons
is as definitions by abbreviations (see
\cite{aspinall:csl94,coquandPollackTakeyama:fundinf05}); they were
also used to model translucent sums in the formalisation of SML
\cite{harper:popl07}.  It is interesting to consider singleton types
because beta-eta phase separation fails: one cannot do eta-expansion before
beta-normalisation of types because the shape of the types at which to
eta-expand is still unknown at this point; and one cannot postpone
eta-expansion after beta-normalisation, because eta-expansion at
singleton type can
trigger new beta-reductions. 
Stone and Harper \cite{stoneHarper:tocl06} decide type checking in a
logical framework (LF) with singleton types and subtyping.  Yet it is not clear
whether their method extends to computation on the type level.  As far
as we know, our work is the first where singleton types are considered
together with a universe.

De Bruijn proposed the concept of \emph{irrelevance of proofs} \cite{aut-4},
for reducing the burden in the formalisation of mathematics.  As shown
by Werner \cite{werner:strengthProofIrrelevance}, the use of
proof-irrelevance types together with sigma types is one way to get
subset types \`a la PVS \cite{pvs-sub} in type-theories having the
eta rule.  This style of subset types was also explored by Sozeau
\cite[Sec.~3.3]{sozeau:types06}\LONGVERSION{; for another presentation of subset types in
  Martin-L\"{o}f type-theory see \cite{sambin}}.
\LONGVERSION{Berardi conjectured that
  (impredicative) type-theory with proof-irrelevance is equivalent to
  constructive mathematics \cite{berardi}.}

































Checking dependent types relies on checking types for equality.  To
this end, we compute -long normal forms using
\emph{normalisation by evaluation} (NbE) \cite{martinlof:jaist04}.  
Syntactic expressions are evaluated into a semantic domain and then
\emph{reified} back to expressions in normal form.
To handle functional and
open expressions, the semantic domain has to be equipped
with variables; a major challenge in rigorous treatments of NbE has
been the problem to generate fresh identifiers.
Solutions include term families \cite{bergerSchwichtenberg:lics91},
liftable de Bruijn terms \cite{aehlig:nbe}, or Kripke semantics
\cite{abelCoquandDybjer:mpc08}.  In this work we present a novel
formulation of NbE which avoids the problem completely: reification is
split into an -expansion phase  in the semantics,
followed by a read back function ()
into the syntax which is indexed by
the number of already used variables.  This way, a standard PER
model is sufficient, and technical difficulties are avoided.

\paragraph{Outline.} 
In Section~\ref{sec:cwf-gat}, we first present \lambdaSing,
Martin-L\"of's logical framework with one universe and singleton
types, as a generalized algebraic theory \cite{cartmell}.  Secondly,
we introduce \lambdaPI, Martin-L\"of type theory with natural numbers,
sigma types, and proof-irrelevant propositions.  In
Section~\ref{sec:examples}, we show some examples using singleton types and
proof-irrelevant types.  In Section~\ref{sec:nbe-primer}, we present
briefly NbE for untyped and simply typed lambda calculi; in particular
we illustrate our novel approach to generate fresh identifiers.  In
Section~\ref{sec:semantics}, we define the semantics of the type
theories by a PER model and prove the soundness of the inference rules.
We use this model to introduce a normalization algorithm
, for which we prove completeness (if  is derivable,
then  and  are identical). 
The soundness of the algorithm (i.e.,  is derivable)
is proven by logical relations in Section~\ref{sec:logrel}.
In Section~\ref{sec:proof-alg}, we define a bi-directional algorithm for
checking the type of normal forms and inferring the type of neutral
terms.  More related work is discussed in Section~\ref{sec:related}.
\LONGVERSION{ The Haskell programs corresponding to the NbE, and
  type-checking algorithms are shown in the
  appendices~\ref{sec:nbealg} and~\ref{sec:alg}, respectively.  }

 \section{The Calculus as a Generalised Algebraic Theory}
\label{sec:cwf-gat}
 
\noindent In this section, we introduce the type theory.  In order to show the
modularity of our approach, we present it as two calculi 
and : the first one has dependent function spaces,
singleton types, and a universe closed under function spaces and
singletons. In the second calculus we leave out singleton types and we
add natural numbers, sigma types, and proof-irrelevant propositions.
It is not clear if singleton types can be combined with
proof-irrelevant propositions without turning the system inconsistent.


We present the calculi using the formalism proposed by Cartmell for
generalised algebraic theories (GAT) \cite{cartmell}.  A GAT consists
of \emph{sort symbols} and \emph{operator symbols}, each with
a dependent typing, and \emph{equations} between \emph{sort
  expressions} and \emph{terms} (``operator expressions'').
Following Dybjer \cite{dybjer:internalTypeTheory}, we are using
``informal syntax'' where redundant arguments to operators are left
implicit. 

\LONGSHORT{
\subsection{Calculus \lambdaSing with singleton types}
\label{sec:calc-sing}
}{
\subsubsection*{Calculus with singleton types.}
\label{sec:calc-sing}
}

\LONGVERSION{
We use capital Greek letters () for variables ranging
  over contexts; capital letters from the beginning of the Latin
  alphabet () for variables ranging over types; small Greek
  letters () are used for variables denoting
  substitutions; and minuscule Latin characters () for
  variables on terms. Words in sans face denote constants (e.g.,
  ).  
}


\subsubsection{Sorts}
\label{sec:sorts}
The set of sort symbols is  and their formation rules, in the sense of Cartmell's
GATs, are:
2ex]
  \tyrule{type-sort}{\Gamma \in \ctx }{\type{\Gamma} \mbox{ is a
      type}}
\qquad
  \tyrule{term-sort}{\Gamma \in \ctx\\ A\in \type{\Gamma}}
  {\term{\Gamma}{A} \mbox{ is a type}}

  \tyrule{empty-ctx}{ }{\ectx \in \ctx} \qquad
  \tyrule{ext-ctx}{\Gamma \in \ctx\\ A\in \type{\Gamma}}{\ctxe{\Gamma}{A}\in \ctx}

  \tyrule{empty-subs}{\Gamma \in \ctx
        }{\esubs\in\Gamma\into\ectx}
\qquad
  \tyrule{ext-subs}{\REDUNDANT{\Gamma,\Delta \in \ctx\\ } 
          \sigma\in\Gamma\into\Delta\\
          \REDUNDANT{A\in \type{\Delta}\\ } 
          t\in \term{\Gamma}{\subsTy{A}{\sigma}}
        }{\exsubs{\sigma}{t} \in\Gamma\into\ctxe{\Delta}{A}}

  \tyrule{id-subs}{\Gamma \in \ctx
        }{\idsubs{\Gamma}\in\Gamma\into\Gamma} 
\qquad
  \tyrule{comp-subs}{\REDUNDANT{\Gamma,\Delta,\Theta \in \ctx\\ }
          \delta\in\Gamma\into\Theta\\ 
          \sigma\in\Theta\into\Delta
        }{\subsc{\sigma}{\delta}\in\Gamma\into\Delta}
\\
  \tyrule{fst-subs}{\REDUNDANT{\Gamma \in \ctx\\ } 
          A\in \type{\Gamma}
        }{\p \in \ctxe{\Gamma}{A} \into\Gamma}

  \tyrule{u-f}{\Gamma \in \ctx }{\TmU \in \type{\Gamma}}  
\quad
  \tyrule{u-el}{\REDUNDANT{\Gamma \in \ctx\\ } 
          A\in \term{\Gamma}{\TmU}
        }{A \in \type{\Gamma}}
\quad
  \tyrule{fun-f}{\REDUNDANT{\Gamma \in \ctx\\ } 
          A\in \type{\Gamma}\\ 
          B\in \type{\ctxe{\Gamma}{A}}
        }{\F{A}{B} \in \type{\Gamma}} 
\

\para{Terms}
\label{sec:terms}
The set of operators for terms is . 
It includes function space  and singleton  as
small-type constructors in .  Lambda terms with explicit
substitutions are obtained via the constructions , , , and .
Since we have used juxtaposition for composition and application of
substitutions, we have the explicit  for term application.  Note
that  stands for the top (th) variable, the th variable is
expressed as . 

2ex]
  \tyrule{fun-el}{\REDUNDANT{\Gamma \in \ctx\\ A\in \type{\Gamma} \\ }
          B \in \type{\ctxe{\Gamma}{A}}\\
          t\in \term{\Gamma}{\F{A}{B}}\\ 
          u\in \term{\Gamma}{A} 
        }{\appTm{t}{u}\in \term{\Gamma}{\subsTy{B}{\subid{\Gamma}{u}}}}
\
2ex]
    \tyrule{sing-el}{\REDUNDANT{\\Gamma \in \ctx\\ A\in \type{\Gamma}\\ }
            a \in \term{\Gamma}{A}\\
            t\in \term{\Gamma}{\singTm{a}{A}}
          }{t \in \term{\Gamma}{A}}

   \inferrule{A = B \in \type{\Gamma}\\ \gamma = \delta \in
  \Delta\into\Gamma}{\subsTy{A}{\gamma} = \subsTy{B}{\delta}\in
  \type{\Delta}}\label{eq:example} \enspace .

  \ru{t \in \term \Gamma A \qquad
      A = A' \in \type \Gamma
    }{t \in \term \Gamma {A'}}

  \idsubs{\ectx} &= \esubs& 
  \idsubs{\ctxe{\Gamma}{A}} & =  \exsubs{\p}{\q}& 
\\
  \subsc{\idsubs{}}{\sigma} &=  \sigma & 
  \subsc{\sigma}{\idsubs{}} &=  \sigma & 
\\
  \subsc{(\subsc{\sigma}{\delta})}{\gamma} &=  \subsc{\sigma}{(\subsc{\delta}{\gamma})} & 
  \subsc{\p}{\exsubs{\sigma}{t}} & =  \sigma & 
\\
  \subsc{\esubs}{\delta} &=  \esubs& 
  \subsc{\exsubs{\sigma}{t}}{\delta} &=
    \exsubs{\subsc{\sigma}{\delta}}{\subsTm{t}{\delta}}&

  \appTm{(\lambda t)}{r} &=  \subsTm{t}{\subid{\Gamma}{r}} &
  \lambda (\appTm{(\subsTm{t}{\p})}{\q})& =  t &
\\
  \subsTy{\TmU}{\sigma} &=  \TmU& 
  \subsTy{(\singTm{t}{A})}{\sigma} &=
    \singTm{\subsTm{t}{\sigma}}{\subsTy{A}{\sigma}}&
\\
  \subsTy{(\F{A}{B})}{\sigma} &=
    \F{(\subsTy{A}{\sigma})}{(\subsTy{B}{\exsubs{\subsc{\sigma}{\p}}}{\q})} & 
  \subsTm{(\lambda t)}{\sigma} &=  \lambda (\subsTm{t}
    {\exsubs{\subsc{\sigma}{\p}}{\q}})& 
\\
  \subsTm{(\appTm{r}{s})}{\sigma} &=
    \appTm{(\subsTm{r}{\sigma})}{(\subsTm{s}{\sigma})}&
  \subsTm{(\subsTm{t}{\delta})}{\sigma}& =
    \subsTm{t}{(\subsc{\delta}{\sigma})}  &
\\
  \subsTm{\q}{\exsubs{\sigma}{t}}&=  t& 
  \subsTm{t}{\idsubs{}} &=  t & 
  
  \tyrule{sing-eq-i}{t,t'\in \term{\Gamma}{\singTm{a}{A}}}
  {t = t'  \in \term{\Gamma}{\singTm{a}{A}}}
\qquad
  \tyrule{sing-eq-el}{t = t' \in \term{\Gamma}{\singTm{a}{A}}}
  {t = t'  \in \term{\Gamma}{A}}

  \label{eq:eq-fst-q}
  \deqterm{\ctxe{\Gamma}{\singTm{\lambda t}{\F{A}{B}}}}{\subsTy{B}{\subid{}{a}}}
  {\appTm{\q}{a}}{\subsTm{t}{\subid{}{a}}}

  \ind i = \left\{
    \begin{array}{ll}
      \q     & \mbox{ if } i \leq  0 \\
      \q\p^i & \mbox{ if } i > 0 . \\
    \end{array}
\right.

    \neterms \ni k &::= \ind i \mid \appTm{k}{v}&\\
    \nfterms \ni v, V,W &::= \TmU \mid \F{V}{W} \mid \singTm{v}{V}
    \mid \lambda v\mid k&

\tyrule{sum-u-i}{A\in \term{\Gamma}{\TmU}\\ B\in \term{\ctxe{\Gamma}{A}}{\TmU}}
  {\DSum{A}{B}\in \term{\Gamma}{\TmU}}
\
\tyrule{sum-f}{\REDUNDANT{\Gamma\in \ctx\\ }A \in \type{\Gamma} \\ 
    B \in \type{\ctxe{\Gamma}{A}}}
  {\DSum{A}{B}\in \type{\Gamma}}
\1ex]
  \tyrule{sum-el1}{
    \REDUNDANT{\Gamma \in \ctx\\ A\in \type{\Gamma}\\ B\in \type{\ctxe{\Gamma}{A}}\\}
    t \in \term{\Gamma}{\DSum{A}{B}}}{\dfst{t} \in \term{\Gamma}{A}}
\qquad
  \tyrule{sum-el2}{
    \REDUNDANT{\Gamma \in \ctx\\ A\in \type{\Gamma}\\ B\in \type{\ctxe{\Gamma}{A}}\\}
    t \in \term{\Gamma}{\DSum{A}{B}}}
  {\dsnd{t} \in \term{\Gamma}{\subsTy{B}{\subid{}{\dfst{t}}}}
  }

  \dfst{\depair{a}{b}} &= a &
  \dsnd{\depair{a}{b}} &= b &
  \depair{\dfst{t}}{\dsnd{t}} &= t
\\
  \subsTm{(\dfst{t})}{\sigma} &= \dfst{(\subsTm{t}{\sigma})}&
  \subsTm{(\dsnd{t})}{\sigma} &= \dsnd{(\subsTm{t}{\sigma})}&
  \subsTm{\depair{a}{b}}{\sigma}  &=
  \depair{\subsTm{a}{\sigma}}{\subsTm{b}{\sigma}} &
\\
& &
    \subsTy{(\DSum{A}{B})}{\sigma} &=  \DSum{(\subsTy{A}{\sigma})}
                       {{(\subsTy{B}{\exsubs{\subsc{\sigma}{\p}}{\q}})}\hspace*{-6ex}} &

  \tyrule{nat-u-i}{\Gamma\in\ctx}
  {\natty\in \term{\Gamma}{\TmU}}
\
  \tyrule{nat-z-i}{
    \Gamma \in \ctx}{\ztm \in \term{\Gamma}{\natty}}
\
  \tyrule{nat-s-i}{
    \REDUNDANT{\Gamma \in \ctx\\} t\in \term{\Gamma}{\natty}
  }{\suctm{t} \in \term{\Gamma}{\natty} }
\
Here, we used  as an abbreviation for
  
which in conventional notation reads .
Since  is a big type, it can mention the universe , thus, we
can define small types by recursion via .  This so
called \emph{large elimination} excludes normalization
proofs which use induction on type expressions 
\cite{crary:lfmtp08,courant:itrs02}.  We add the usual computation
laws for primitive recursion.


\para{Enumeration sets}  The type  has the  canonical
inhabitants , \dots, , which can be
eliminated by the dependent case distinction   with  branches.
1ex]
  \tyrule{\enumrule{e}
  }{
    \REDUNDANT{\Gamma \in \ctx\\}
    B\in \type{\ctxe{\Gamma}{\enum{n}}}
      \\ t\in \term{\Gamma}{\enum{n}}
      \\\\ t_{0}\in \term{\Gamma}{\subsTy{B}{\subid{}{\const{n}{0}}}}\ \cdots\ 
      t_{n-1}\in \term{\Gamma}{\subsTy{B}{\subid{}{\const{n}{n-1}}}}
  }{\elim{n}{B}{t_{0} \cdots t_{n-1}}{t}\in
    \term{\Gamma}{\subsTy{B}{\subid{}{t}}}}  

\elim{n}{B}{t_{0} \cdots t_{n-1}}{\const{n}{i}}&= t_{i} \\
\elim{n}{\enum n}{\const{n}{0} \cdots \const{n}{n-1}}{t}&= t 

  \tyrule{{\enumrule[0]tm}}{t \in \term \Gamma {\enum 0}
        }{\oprf \in \term \Gamma {\enum 0}}
\quad
  \tyrule{{\enumrule[0]eq}}{t,t' \in \term \Gamma {\enum 0}}{t = t' \in \term \Gamma {\enum 0}}
\quad
   \tyrule{{\enumrule[1]eq}}{t,t' \in \term \Gamma {\enum 1}}{t = t' \in \term \Gamma {\enum 1}}
  
  \tyrule{prf-f}{\REDUNDANT{\Gamma \in \ctx\\ } 
    A \in \term{\Gamma}{\TmU}
  }{\boxty{A} \in \term{\Gamma}{\TmU}}
\qquad
  \tyrule{prf-f}{\REDUNDANT{\Gamma \in \ctx\\ } 
    A \in \type{\Gamma}
  }{\boxty{A} \in \type{\Gamma}}
\\
  \tyrule{prf-i}{\REDUNDANT{\Gamma \in \ctx\\ A \in \type{\Gamma}\\ }
    a\in\term{\Gamma}{A}
  }{\boxtm{a} \in \term{\Gamma}{\boxty{A}}}
\qquad
  \tyrule{prf-tm}{\REDUNDANT{\Gamma \in \ctx\\ A \in \type{\Gamma}\\ }
    a\in\term{\Gamma}{A}
  }{\oprf \in \term{\Gamma}{\boxty{A}}}
\\
  \tyrule{prf-eq}{\REDUNDANT{\Gamma \in \ctx\\ } 
    A \in \type{\Gamma}\\
    t,t'\in\term{\Gamma}{\boxty{A}}
  }{t = t' \in \term{\Gamma}{\boxty{A}}}

  \dfrac{\Gamma \derN t : \boxty A \qquad
            \Gamma \derN B \qquad
            \Gamma, x \of A \derN b : B \qquad
            \Gamma, x \of A, y \of A \derN b = b[y/x] : B
          }{\Gamma \derN \ABwhere b x t : B}

  \dfrac{\Gamma \derN t : \boxty A \qquad 
            \Gamma \derN B \qquad
            \Gamma, x \of A \derN b : \boxty B 
          }{\Gamma \derN \ABwhere b x t : \boxty B}

  \ABwhere b x {\boxtm a} & = b[a/x] \\
  \ABwhere {b[\boxtm x/y]} x t & =  b[t/y] \\
  \ABwhere a x (\ABwhere b y c) & = \ABwhere {(\ABwhere a x b)} y c &
   \mbox{if } y \not\in\FV(a)

  \tyrule{prf-el}{t \in \term{\Gamma}{\boxty{A}} \\
    B\in \type{\Gamma}\\ 
    b \in \term{\ctxe{\Gamma}{A}}{\subsTy{B}{\p}}\\
    \subsTm b \p = \subsTm b {\exsubs{\subsc \p \p}{\q}}
      \in \term{\ctxe{\ctxe \Gamma A} {\subsTy A \p}}{\subsTy B {\subsc \p \p}}
  }{\wheretm{b}{t}{B} \in \term{\Gamma}{B}}  

  \wheretm{b}{\boxtm{a}}{B} 
      & = \subsTm{b}{\subid{}{a}} 
    & \rulename{prf-} \\
  \wheretm{\subsTm b {\exsubs \p {\boxtm \q}}}{t}{B}
    & = \subsTm b {\subid{} t} 
      & \rulename{prf-}\\
  \wheretm a {(\wheretm b c B)} A 
    & = \wheretm {(\wheretm {\subsTm a {\exsubs {\subsc \p \p} \q}} b {\subsTy A \p})} c B
      & \rulename{prf-assoc}\\

      \neterms \ni k ::= &\ldots \mid \dfst{k} \mid \dsnd{k} \mid
        \natrec{V}{v}{v'}{k} \mid \elim{n}{V}{v_0 \cdots v_{n-1}}{k} \mid \wheretm{v}{k}{V} \mid \oprf &\\
      \nfterms \ni v,V ::= &\ldots \mid \DSum{V}{W}\mid \natty\mid
      \enum{n} \mid \boxty{V}\mid \depair{v}{v'} \mid \ztm \mid \suctm{v} \mid 
      \const{n}{i} \mid \boxtm{v} &
    
  \neterms \ni k &::= \ldots \mid \wheretm{v}{k}{V} &\\
  \nfterms \ni v,V &::= \ldots \mid \boxty{V}\mid \boxtm{v} \mid \oprf &

    \proofLine{\sdterm{\Gamma}{\boxty{A}}{\oprf}}{hypothesis}\tag{*}\label{eq:lift-ex-1}\\
    \proofLine{\sdterm{\Gamma}{A}{t}}{by inversion on \eqref{eq:lift-ex-1}}
      \tag{\textdagger}\label{eq:lift-ex-2} \\
    \proofLine{\dterm{\Gamma'}{A'}{t'}}{by ind. hyp. is a good lifting of \eqref{eq:lift-ex-2}} \\
    \proofLine{\dterm{\Gamma'}{\boxty{A'}}{\boxtm{t'}}}{by \ruleref{prf-i}, is a good lifting of\eqref{eq:lift-ex-1}}      
  
    \proofLine{\dterm{\Gamma''}{B}{s}}{hypothesis, be other good lifting of \eqref{eq:lift-ex-2}}\tag{**}\label{eq-lift-ex-3}\\
    \proofLine{\dtype{\Gamma''}{B}}{by inversion, good lifting of }\\
    \proofLine{\deqtype{\Gamma'}{B''}{\boxty{A'}}}{by ind. hyp.}\\
    \proofLine{\deqterm{\Gamma'}{\boxty{A'}}{\boxtm{t'}}{s}}{by \ruleref{prf-eq} and \ruleref{conv}}.
  
\begin{prooftree}
  
  
  
  \justifies
  \derN\oprf\of\boxty{\TyZero}
  
  \justifies
  \derN \ABwhere x x \oprf = \czero \of \TyZero
  \quad
  
  
  
  \justifies
  \derN\oprf\of\boxty{\TyOne}
  
  \justifies
  \derN \ABwhere x x \oprf = \cone \of \TyOne
  
  \justifies
  \derN \czero = \cone \of \TyN
\end{prooftree}
1ex]
  \psi^\Sigma & : & \boxpar{\sigT x A B} \to \sigT x {\boxty A} {\boxty B}  
\\
  \psi^\Sigma & = & \lambda q.\, \ABwhere 
     {(\boxtm{\dfst p},\, \boxtm{\dsnd p})} p q
\1ex]
  \psi^\Pi & : & \boxpar{\funT x A B} \to \funT x A {\boxty B}
\\
  \psi^\Pi & = & \lambda f \lambda x.\, \ABwhere {\boxtm{g\,x}} g f
\end{array}
\eprg
We would like to put  for the  in , but this is not
well typed, since we do not have  for arbitrary .  It
seems that  is not definable in , as it is not
definable in computational lambda-calculus \cite{complambda}
for an arbitrary monad
.  Awodey and Bauer also have only , which is trivial.


For arbitrary  we have

In the opposite direction,  by proof irrelevance.  Thus,  and  establish an
isomorphism, which means that  distributes over .


\subsection{On subtyping in \lambdaSing}

Subtyping can be defined in several ways, for instance,  is a
subtype of  in , written ,
iff .
Most presentations of singleton types include
subtyping \cite{aspinall:csl94,courant:itrs02,stoneHarper:tocl06}, so
it is natural to ask whether the usual rules hold in our calculus.
Using the principle  iff , it is easy to
see that we have Aspinall's two axioms \cite{aspinall:csl94}:

Also, singleton formation is compatible with subtyping, if  then .  Contravariant subtyping, however, only holds up to
-equality.  If we relax the definition of subtyping  to  where
 denotes any -expansion of , then we get
contravariant subtyping

Furthermore, we have following two axioms, which hold definitionally in Stone and
Harper's system \cite{stoneHarper:tocl06}

The first axiom is one of Courant's subtyping rules \cite{courant:itrs02}. 




 

\section{From Untyped to Typed Normalisation by Evaluation}
\label{sec:nbe-primer}

\noindent In this section, we give a short introduction into
normalisation-by-evaluation for typed lambda calculi, with a special
emphasis on our novel method for generation of fresh identifiers during
reification.

\subsection{Fresh Name Generation in NbE}

The basic idea of NbE is to \emph{evaluate} a term of type  into a
suitable semantics  from which its \emph{normal form} can be
extracted by \emph{reification}.  In case of the simply-typed
lambda-calculus, this is possible if we choose for base types 
the set of terms of type  and for function types  a
suitable subset of the function space .  During
reification of a function  to a term , the identifier  has to be chosen \emph{fresh} to avoid capture
of names in the body of the function .  However, since  is a
semantic object, it is a non-trivial problem to compute a name which
is fresh for .  Garillot and Werner \cite{garillotWerner:tphols07}
solve it by first letting  be a dummy identifier, computing the
free variables in the reified function body , and then reify 
again with a name  which is fresh for .  This is, of course,
horribly inefficient, and there are other solutions.  In the original
publication on NbE by Berger and Schwichtenberg
\cite{bergerSchwichtenberg:lics91}, base types  are interpreted by \emph{term families}.  These are
functions  from the natural numbers into a \emph{de Bruijn level}
representation of terms such that all instances  are
-equivalent but in  the bound variables are levels
starting with . In this setting, the reification of a function  is not a term but a term family, mapping  to the
term family ,
where  denotes the reification function and  is
the variable  seen as an element in . 
Note that every  in , 
the body of the reified abstraction, will bind a variable 
from the set .

When considering NbE for the \emph{untyped} lambda calculus, the type
semantics collapses to a single domain  which contains terms and functions,\footnote{
Let us notice
here the \emph{tagging} introduced by the disjoint sum operator .
Indeed, in the absence of a type structure, \emph{tagless
  normalisation} seems impossible.  
}
as observed by Filinski and Rhode \cite{filinskiRohde:untypedNbE}.  
Aehlig and Joachimski \cite{aehlig:nbe} replace term families by
functions  from natural numbers to a \emph{de Bruijn index}
representation of terms, where  shifts all \emph{free} indices
by . 

In this paper, instead of having term families  in
the semantics, we have a notion of \emph{neutral (``term-like'')
  value} built up from free variables  and application of the
free variables to sequences of values .  The free variables
are \emph{de Bruijn levels} in spirit, thus, no shifting is needed,
just like in the \emph{locally nameless} approach
\cite{pollack:alpha}.  The second author has given a semantics with
neutrals before \cite{coquand:type}, calling the free variables
\emph{generic values}.  Also, this approach has been used by the first
two authors together with Dybjer \cite{abelCoquandDybjer:mpc08} for
NbE without a reflection operation, and independently by L\"oh,
McBride, and Swierstra
\cite{loehMcBrideSwierstra:tutorialDependentlyLambda}.  In this
article, we put the technique to a novel use by defining typed
reification \emph{and reflection} for this semantics.


\subsection{Untyped NbE}
\label{sec:untyped-nbe}
Let  be a denumerable set of variables.
We consider a set  and a notion of function space 
with an embedding constructor  and two
further constructors  and  for neutral values.  An application function  is given by

Such a  can be realised by solving the recursive domain equation 
 or, for
the practically minded, by defining a Haskell data type
\begin{verbatim}
    data D where
      Lam : (D -> D) -> D
      Var : Var -> D
      App : D -> D -> D
\end{verbatim}
and programming  by pattern matching.  Our definition of
 is a bit ``too big'' since it does not restrict  to the
construction of neutral values
 but we have also
.  However, we can ignore these unwanted elements
since our NbE algorithm never produces any. 

\begin{rem}
\label{rem:comp-adequacy}
  The relationship between the denotational model\/  and the
  Haskell data type  is not without subtleties.  Domain
  theoretic functions such as application  correspond to
  Haskell programs if our denotational semantics is computationally
  adequate for Haskell's operational semantics
  \cite{plotkin:adequacy}. Filinski and Rhode
  \cite{filinskiRohde:untypedNbE} formally relate a NbE
  function on a reflexive domain\/  to a NbE program written in an
  ML-like, call-by-value language, by exploiting computational
  adequacy.  We do not formally prove this connection for Haskell in
  this article, this is deferred to future work.  
\end{rem}

Untyped NbE is now given by a standard evaluator 
of terms  in environments  and a readback function  from values  at de Bruijn level  to terms
\cite{gregoireLeroy:compiledReduction}.   For the sake of
readability, we use names instead of de Bruijn indices in the syntax
of untyped terms.

To normalise a closed term , compute .  To
normalise an open term  with free variables 
compute  with environment .

To prepare for applying our method to  and ,
let us switch to de Bruijn representation.  Environments become tuples
and variables de Bruijn indices .

To read back a de Bruijn level  as a de Bruijn index, we have
to take the current length  of the variable context into account.
While de Bruijn levels are absolute references, they are numbered
 in a context of length , de Bruijn
indices are relative to the length of the context, they are enumerated
from right to left: 
.  
The formula  (assuming ) converts level 
into the corresponding index.

\subsection{Typed NbE}

While untyped NbE returns a -normal form (if it exists), typed
normalisation by evaluation yields a -normal form, usually
the -long form.  To obtain the -long form, we have to
modify our reification procedure.  One method is to make read-back
type directed \cite{abelCoquandDybjer:mpc08}, which corresponds to
postponing -expansion after -normalisation. 
However, this strategy
is not sufficient in the case of , because
-expansions at singleton types can trigger new
-reductions.   The other method is to divide -expansion
into \emph{reflection} and \emph{``reification''}, the first expanding
variables to enable new reductions, and the second expanding the
result of -normalisation to obtain an -long
form.\footnote{In tagless normalisers
  \cite{bergerSchwichtenberg:lics91}, 
  reflection is necessary to
  inject variables  of non-base types  into the semantics
  .  However, for languages beyond pure type systems it is
  hard to obtain tagless normalisation.  Classic is the problem of
  disjoint sum types \cite{altenkirchDybjerHofmannScott:lics01}: to
  display a free variable of type  as either a left or a right
  injection, we need control structures
  \cite{balatDiCosmoFiore:popl04}.  Alternatively, one can replace
  data types by their Church encodings.  None of these approaches fit
  our purposes, thus, we are currently not aiming at tagless normalisation.}


The novel approach of this article is to do reflection  and
``reification'' , hence, -expansion, completely at
the level of
the semantics .  Since our value domain  allows us to
construct functions via , the process of -expansion is
independent of any fresh name considerations.

To compute the long normal form of a closed term  of type , 
run .  For an open term , execute , where .




 \section{Semantics}
\label{sec:semantics}

\noindent In this section we define a domain  for denoting types,
  terms, and substitutions. Then we introduce a partial function
   for
  reifying elements of the domain into the calculus; this function
  takes an extra argument  indicating the next free variable.  We
  continue by defining PERs over the domain; these PERs denote the
  axioms for types, terms, and substitutions. We need PERs for the
  evaluation function is defined over syntactical entities and not for
  typing judgements. We also introduce PERs  and  
  whose elements are
  invariably, in every context, reified as normal forms and
  neutral terms respectively.  Using these PERs we define a family
  (indexed by denotations of types) of functions for ``normalising''
  in the domain. We conclude this section proving completeness for
  this family of normalisation functions; here completeness means that
  two terms in the theory are read back as the same normal form.
  In this section we define a PER model of the calculus presented in
  the previous section. The model is used to define a normalisation
  function later.

\subsection{PER semantics}
\label{sec:persem}

\LONGVERSION{
  \newcommand{\pairapp}[2]{\mathsf{Pair}\ #1\ #2}
\newcommand{\singapp}[2]{\mathsf{Sing}\ #1\ #2}
\newcommand{\per}[1]{\mbox{PER}(#1)}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\semcs}[1]{{}^{\mathsf{s}}\kern-0.2ex\semc{#1}}
\newcommand{\semctx}[1]{(\![{#1}]\!)}

In this subsection we introduce the abstract notion of PER models for
our theory. This subsection does not introduce any novelty (except for
some notational issues). We refer the reader to \cite{319872} for a
short report on the historical developments of PER models.

\begin{defi}[Partial Equivalence Relations]\label{sec:pers}
A partial equivalence relation (PER) over a set  is a binary
relation over  which is symmetric and transitive.

If  is a PER over , and  then it is
clear that . We define ; clearly,  is an equivalence relation
over .  If , sometimes we will write , and  if . We denote
with  the set of all PERs over . Given two PERs 
and  over , we say  is included in  if
 implies ; we denote this
inclusion with .

If  and , we say that  is {\em a family of PERs indexed by
  }\/ iff for all , . If  is a family indexed by , we write .
\end{defi}


\begin{defi}[Applicative structure]
  An applicative structure is given by a pair ,
  where  is a set and  is a binary operation on .
\end{defi}


The following definitions are standard (\eg\
\cite{aspinall:csl94,coquandPollackTakeyama:fundinf05}) in definitions
of PER models for dependent types. The first one is even standard for
non-dependent types (\cf\ \cite{DBLP:books/el/leeuwen90/Mitchell90})
and ``F-bounded polymorphism'' (\cite{143230}); its definition clearly
shows that equality is interpreted extensionally for dependent
function spaces. The second one is the PER corresponding to the
interpretation of singleton types; it has as its domain all the
elements related to the distinguished element of the singleton, and it
relates everything in its domain.

\begin{defi} Let  be an applicative structure,
  , and .
  \begin{enumerate}[(1)]
  \item ;
  \item .
  \end{enumerate}
\end{defi}

 Besides interpreting function spaces and singletons we need PERs for
the denotation of the universe of small types, and for the set of
large types; jointly with these PERs we need functions assigning a PER
for each element in the domain of these universe PERs. Note that this
forces the applicative structure to have some distinguished elements.


\begin{defi}[Universe]
  Given an applicative structure  with distinguished elements
   and , a universe  is a
  PER  over  and a family 
  with the condition that  is closed under function and 
  singleton types.  This means:
  \begin{enumerate}[(1)]
  \item Whenever  and
    for all , , then , with .
  \item Whenever  and , then  and .
  \end{enumerate}
\end{defi}\medskip

\noindent An applicative structure paired with one universe for small
types and one universe for large types is the minimal structure needed
for having a model of our theory.

\begin{defi}[PER model]
  Let  be an applicative structure with distinguished 
  elements , and ; a {\em
    PER model} is a tuple  satisfying:
  \begin{enumerate}[(1)]
  \item , such that
     and  are
    both universes, and
  \item , with .
  \end{enumerate}
\end{defi}

In the following definition we introduce an abstract concept for
environments: since variables are represented as projection functions
from lists (think of  as taking the head of a list, and  as
taking the tail), it is enough having sequences together with
projections.

\begin{defi}[Sequences]
  Given a set , a set  \emph{has sequences} over  if
  there are distinguished operations , , ,  and  such that
  
\end{defi}

Now we need to extend the notion of PERs over  to PERs over
 for interpreting substitutions.\footnote{The reader is invited
  to think of  as the terminal object of the category of PERs
  over  and PER preserving morphisms; looked this way our
  definition for  does not differ very much from others
  \cite{aspinall:csl94,coquandPollackTakeyama:fundinf05}.} 

\begin{defi} Let  be an applicative structure and let 
  have sequences over ; moreover let  and .
  \begin{enumerate}[(1)]
   \item ;
   \item ;
  \end{enumerate}
\end{defi}

Until here we have introduced semantic concepts. Now we are going to
axiomatise the notion of evaluation, connecting the syntactic realm
with the semantic one.

\begin{defi}[Environment model]\label{def:syn-app-str}
  Let  be a PER model and let  have
  sequences over .  We call  an {\em environment model} if
  the {\em evaluation functions}  and  satisfy:
  \newlength{\originalArrayColSep} \setlength{\originalArrayColSep}{\arraycolsep} 
  \setlength\arraycolsep{0.3em}
  
  \setlength\arraycolsep{\originalArrayColSep}
Since no ambiguities arise, we shall henceforth
write  instead of .
\end{defi}

Once we have an environment model  we can define the denotation
for contexts. The second clause in the next definition is not
well-defined a priori; its totality is a corollary of
Thm. \ref{thm:soundness}.

\begin{defi}Given an environment model , we define recursively
  the semantic of contexts :
  \begin{enumerate}[(1)]
  \item ,
  \item .
  \end{enumerate}  
\end{defi}\medskip

\noindent We use PERs for validating equality judgements and the domain of each
PER for validating typing judgements.

\begin{defi}[Validity] Let  be an environment model. We
  define inductively the predicate of satisfability of judgements by
  the model, denoted with :
  \begin{enumerate}[(1)]
  \item  iff true
  \item  iff 
  \item  iff 
  \item  iff  and for all , 
  \item  iff 
  \item  iff  and for all , 
  \item  iff 
  \item  iff ,
    , and for all ,
    .
  \end{enumerate}
\end{defi}


\begin{thm}[Soundness of the Judgements]
  \label{thm:soundness} Let  be a model. If ,
  then .
\end{thm}
\begin{proof} By easy induction on . 
\end{proof}

 
  \subsection{A concrete PER model} In this subsection we define a
  concrete PER model over a Scott domain. The definition of the
  evaluation function is post-poned to the next subsection after
  introducing the NbE machinery.}

\begin{defi}
  \label{def:domain}
  We define a domain 
   where  is a denumerable set
  of variables (as usual we write  and assume  if
  , for ), 
   is lifting,  
  is the Sierpinski
  space,  is the set of continuous functions from  to
  ,  is the coalesced sum\LONGSHORT{ (this is the
    disjoint union where all the bottoms elements are identified),}{,}
  and  is the Cartesian product of 
  \cite{dom-theory}. 



\end{defi}
An element of
 which is not  can be of one of the forms:
 
Elements of the form  and  are called
\emph{neutral}; in this section, we reuse the letter  to denote
neutral elements of .


In order to define an environment model over , we endow it with an
applicative structure. Note also that  has pairing, letting us to
take the set of sequences over  simply as  with
. We define application  and the projections  by
 
We define a partial function  which reifies elements from the model into terms; this
function is similar to Gr\'egoire and Leroy's read-back function
\cite{gregoireLeroy:compiledReduction}. 
\begin{defi}[Read-back function]
  \label{def:reify}

\end{defi}

As explained in Sect.~\ref{sec:untyped-nbe}, the reification of
variables turns de Bruijn levels into de Bruijn indices. 
Note that in case  we
return the th de~Bruijn index just not to be undefined;
we will come back to this later. 




The next PERs contain those elements of the domain  whose
reification is defined for any context length. Moreover, their
elements are reified as neutral terms and normal forms, respectively;
allowing us to reason semantically about normal forms. Remember that
 denotes that  is syntactically equal to  and .

\begin{defi}[(Semantic) neutral terms and normal forms]
\label{def:pernf}
  
\end{defi}\medskip

\noindent Notice that if the case  were undefined in the clause for
variables in \ref{def:reify}, then for any  the application
 would be undefined; hence  and, consequently,  would be empty. Since we depend on
having a semantic representation of variables and neutrals we add the
case . This case will not arise in our use of the readback
function.

\LONGVERSION{
\begin{rem}
  \label{rem:pers}
  These are clearly PERs over : symmetry is trivial and
  transitivity follows from transitivity of the syntactical equality.
\end{rem}

\begin{lem}[Closure properties of  and ]
\label{rem:presnf} \bla
  \begin{enumerate}[\em(1)]
  \item .
  \item Let . If  for all , then .
  \item If  and , then .
  \item If  for all , then .
  \item  for all .
  \item If  and , then . 
  \end{enumerate}
\end{lem}
}





\SHORTVERSION{
  The following definitions are standard
  \cite{aspinall:csl94,coquandPollackTakeyama:fundinf05} (except for
  ); they will be used in the definition of the model.  
\begin{defi} Let  and .
  \begin{enumerate}[\em(1)]
  \item ;
  \item ;
  \item ;
  \item .
  \end{enumerate}
\end{defi}
}

We define  and  using Dybjer's schema of inductive-recursive definition 
\cite{dybjer:jsl00}. We show then that  is a family of PERs
over .

\begin{defi}[PER model]\label{def:peru}\hfill
\begin{enumerate}[(1)]
\item Inductive definition of .\hfill
  \begin{enumerate}[(a)]
  \item ,
  \item if  and , then ,
  \item if  and for all ,  then .
  \end{enumerate}
\item Inductive definition of .\hfill
  \begin{enumerate}[(a)]
  \item ,
  \item ,
  \item if , and  then ,
  \item if , and for all , , then .
  \end{enumerate}
\item Recursive definition of .\hfill
  \begin{enumerate}[(a)]
  \item ,
  \item ,
  \item ,
  \item , in all other cases.
  \end{enumerate}
\end{enumerate}
\end{defi}


\LONGVERSION{
\begin{rem}
  \label{rem:ord-pert}
  The generation order  on  is well-founded. The
  minimal elements are , and elements in ; , and for all , ;
  and, finally, .
\end{rem}
}

\begin{lem}
  \label{lem:famperd}
\LONGSHORT{
  The function  is a family of
   over , i.e., .
}{
  The function  is a family of  over .  
}
\end{lem}

\LONGVERSION{
  \begin{proof} By induction on . 
See Appendix~\ref{prf:famperd}.
  \end{proof}

The previous lemma leads us to the definition of a PER model over
. Note also that  has all the distinguished elements needed to
call it a syntactical applicative structure.

\begin{cor}
  The tuple  is a PER model. \qed
\end{cor}
}
\newcommand{\etatitle}{\texorpdfstring{}{eta}}
\subsection{Normalisation and \etatitle-Expansion in the Model}
\label{sec:norm-model}



In the following, we adopt the NbE algorithm outlined in
Section~\ref{sec:nbe-primer} to the dependent type theory
.  Since read-back has already be defined, we only
require reflection, reification and evaluation functions.





\LONGSHORT{
\begin{defi}[Reflection and reification]
  \label{def:up-down}
  The partial functions  and  are given as follows: 


\end{defi}
}{
\begin{defi}
  \label{def:up-down}
  The partial functions  and  are given as follows: 
  
  
\end{defi}
}\medskip

\LONGVERSION{\noindent In the following lemma we show that reflection 
  corresponds to Berger and Schwichtenberg's ``make self evaluating''
  and both reification functions  and
   correspond to``inverse of the evaluation function''
  \cite{bergerSchwichtenberg:lics91}.
  Note that they are indexed by types values instead of 
  syntactic types, since we are dealing with dependent instead of
  simple types.
}

\newcommand{\uptitle}{\texorpdfstring{}{\textuparrow}}
\newcommand{\downtitle}{\texorpdfstring{}{\textdownarrow}}
\newcommand{\Downtitle}{\texorpdfstring{}{Down}}

\begin{lem}[Characterisation of \uptitle, \downtitle, and \Downtitle]
  \label{lem:reify}
  Let , then\hfill
  \begin{enumerate}[\em(1)]
  \item if  then ;
  \item if , then ;
  \item and also .
  \end{enumerate}
\end{lem}
\begin{proof}
  By induction on . See \ref{prf:reify}.
\end{proof}







Let us recapitulate what we have achieved: we have defined a PER model
over the domain ; then we defined a family of functions 
indexed over denotation of types with the property that when applied
to elements in the corresponding PER we get back elements which will
be reified as normal forms. In fact, we have the stronger result that
whenever we apply  to two related elements 
we get elements to be reified as the same term.

Now we define evaluation which clearly satisfies the environment model
conditions in Def.~\ref{def:syn-app-str}; hence, we have a model and, using 
Thm.~\ref{thm:soundness}, we conclude completeness for our normalisation
algorithm.

\begin{defi}[Semantics] Evaluation of substitutions and terms
  into  is defined inductively by the following equations.
  
\end{defi}

\SHORTVERSION{
\begin{defi}[Validity] \hfill
  \begin{enumerate}[\em(1)]
  \item  iff true
  \item  iff 
  \item  iff 
  \item  iff  and for all , 
  \item  iff 
  \item  iff  and for all , 
  \item  iff 
  \item  iff ,
    , and for all ,
    .
  \end{enumerate}
\end{defi}
}

\SHORTVERSION{
\begin{thm}[Soundness of the Judgements]
  \label{thm:soundness}
  if , then .
\end{thm}
\begin{proof} By easy induction on . 
\end{proof}
}

\begin{thm}[Completeness of NbE]
  \label{thm:completeness}
  Let  and let also , then\linebreak
  .
\end{thm}
\begin{proof}
  By Thm.~\ref{thm:soundness} we have 
  and we conclude by Lem.~\ref{lem:reify}.
\end{proof}


\PrfIrrTitle

\label{sec:pi-model}

\LONGVERSION{
We extend all the definitions concerning the construction of the model.

\begin{defi}[Extension of domain ]
  
\end{defi}

}
\SHORTVERSION{
  We extend all the definition concerning the construction of the model;
  
  the new inhabitants will be written as , and ,
  respectively.  The read-back function is extended by the equations
   and
  .  We add a new clause in the definition
  of ,

The definitions of normalisation and expansion are extended for
,
  
  The semantic equations for the new constructions are
  
} 


\LONGVERSION{
\noindent
We use the following notations for the injections into :


\noindent
In this extension, the injections , , , and
 construct neutral elements . Soundness for the calculus
 requires the canonical element for proof-irrelevant types
 to be in every PER; thus we need to redefine application
 to have :
 We also redefine the projections
 and  to account for neutrals and because they are
used in the definition of , which will be used as the denotation
of sigma types.
 

\setlength{\originalArrayColSep}{\arraycolsep} 
\setlength\arraycolsep{0.2em}

\begin{defi}[Read-back function]\hfill

   
\end{defi}
\setlength{\arraycolsep}{\originalArrayColSep}



\noindent
We define inductively new PERs for interpreting naturals and finite
types. Note that  and  are irrelevant, in this way we can
model -expansion for  and ;  is
also irrelevant, even when  distinguishes its elements.

\begin{defi}[More semantic types] \label{def:pernat}\bla
  \begin{enumerate}[(1)]
  \item  is the smallest PER over , such that
    \begin{enumerate}[(a)]
    \item 
    \item 
    \item , if 
    \end{enumerate}
  \item If  then .
\item ,
   \item ,
  \item 
    , for .
  \end{enumerate}
\end{defi}

We add new clauses in the definitions of the partial equivalences for
universe and types, these clauses do not affect the well-foundedness
of the order  defined in \ref{rem:ord-pert}, but now we
have that  and  are also minimal elements for that
order.

\begin{defi}[Extension of  and ]\hfill
\begin{enumerate}[(1)]
 \item Inductive definition of .
  \begin{enumerate}[(a)]
  \item If , and for all , , then
    .
  \item ,
  \item ,
  \item if , then .
  \end{enumerate}
\item Inductive definition of .
  \begin{enumerate}[(a)]
  \item If , and for all , , then .
  \item if , then .
  \end{enumerate}
\item Recursive definition of .
  \begin{enumerate}[(a)]
\item ,
  \item  
\item ,
  \item if , then .
  \end{enumerate}
\end{enumerate}
\end{defi}\medskip

\noindent Note that in the PER model, all propositions  are inhabited.
In fact, all types are inhabited, for there is a reflection from
variables into any type, be it empty or not.  So, the PER model is
unsuited for refuting propositions.  However, the logical relation we
define in the next section will only be inhabited for non-empty types.

\begin{rem}
  It can be proved by induction on  that .
\end{rem}

\begin{defi}[Reflection and reification, \cf~\ref{def:up-down}]


\end{defi}\medskip






\noindent For giving semantics to eliminators for data types we need to define
partial functions , and .
\begin{defi}[Eliminations on ]\bla
  \label{def:recd}
  \begin{enumerate}[(1)]
  \item Elimination operator for naturals.

  \item Elimination operator for finite types.
  
\end{enumerate}
\end{defi}

\begin{rem}
  If for all , , and , and for all  and , , and  then .
\end{rem}


With these new definitions we can now give the semantic equations for
the new constructs. 

\begin{defi}[Extension of interpretation]
  
  
\end{defi}


\begin{lem}[Laws of proof elimination]
  \label{lem:soundness-irr} , , and
  associativity for  are modeled by the extended
  applicative structure.
\end{lem}
\begin{proof}See \ref{prf:soundness-irr}.
\end{proof}
}


\begin{rem}
\LONGSHORT{
 All of lemmata \ref{lem:famperd}, \ref{lem:reify}, and theorems
  \ref{thm:soundness}, and \ref{thm:completeness} are valid for the
  extended calculus.
}{
  All of lemmata \ref{lem:famperd}, \ref{lem:reify}, and theorems
  \ref{thm:soundness}, and \ref{thm:completeness} are valid for the
  calculus with proof-irrelevance.
}
\end{rem}

Note that we have defined a \emph{proof-irrelevant}
semantics for  that collapses
all elements of  to , which leads to a more efficient
implementation of the normalisation function. However, this semantics
is not sound if \lambdaIrr\ is extended with singleton types
interpreted analogously to , i.e., , because it does not model
\ruleref{sing-eq-el}.  (We have  for all
, but not necessarily .)
On the other hand, \lambdaIrr\ without 
can be extended to singleton types as explained in the following
remark.

\begin{rem}[Extending \lambdaIrr\ by singleton types] 
  \label{rem:proof-relevant-model}
  Singleton types can be added straightforwardly if we employ a
  \emph{proof-relevant} semantics:

  The domain  is not changed; in particular we have ,
  and it is readback as before, ; hence
  .



  All the enumerated types are modelled in a uniform way:
  ; proof-irrelevance types  are interpreted as
  the irrelevant PER with the same domain as the PER for : 
  . Reflection
  and reification for  are defined respectively as
  
  With these definitions it is clear that the corresponding
  result for Lem.~\ref{lem:reify} is still valid.
  
  Since , introduction and elimination of
  proofs can be interpreted as follows
  
  \noindent this model is sound with respect to the calculus
   extended with singleton types; hence
  Thm. \ref{thm:completeness} is valid.
\end{rem}

\begin{rem}
  As was previously said we cannot use this PER model for proving
  that there is no closed term in . Instead, one can build
  up a PER model, in the sense of \ref{sec:persem}, of closed values,
  where . By soundness (Thm.~\ref{thm:soundness}) it follows
  that there is no possible derivation of .
\end{rem}



 \section{Correctness of NbE}
\label{sec:logrel}

\LONGSHORT{

  \noindent In Thm.~\ref{thm:completeness} we have proved that the NbE algorithm
  is complete with respect to the judgemntal equality of our calculi;
  a corollary of that fact is totality of NbE.

  \begin{rem} Let . Given some , we can conclude
     is a well-defined term 
    in normal form.
  \end{rem}

  In this section we prove correctness, with respect to the typing
  rules, for our NbE algorithm. This means that given a typing
  , when NbE is applied to , the resulting
  normal form , is provable equal to ; i.e.
  .

  Let us anticipate the main results of this section. As a corollary of
  Thm.~\ref{thm:logrel} we show that a term is related to its
  denotation with respect to some canonical environment (to be defined
  in Def.~\ref{def:canonical-env}). Previously we prove in Lem.
  \ref{lem:judgeq} that if a term is logically related with some
  semantic element, then its reification will be judgmentally equal
  to the term. Composing these facts we obtain correctness. As a
  consequence of having correctness and completeness for NbE, one gets
  decidability for judgmentally equality: normalise both terms and
  check they are syntactically the same. Another important corollary
  is injectivity for constructors.

  \subsection{Logical relations}
  \label{sec:logrel-sing}
  In this subsection we define logical relations and prove some
  technical lemmas about them. As is standard with logical relations
  one defines them by induction on types (here we define by induction
  on semantics of types, \ie\  elements of ) and for basic types
  they are defined by prescribing the property to be proved; while for
  higher order types they are defined using the relations of the 
  domain and image types.

}{ 

  In order to prove soundness of our normalisation algorithm we
  define logical relations \cite{kripke-models} between types and
  elements in the domain of , and between terms and elements in
  the domain of the PER corresponding to elements of .

}


\begin{defi}[Logical relations]
  \label{def:logrel} We define simultaneously two families of binary
   relations:
  \begin{enumerate}[(a)]
  \item If  then  
    shall be a -indexed family of relations
    between well-formed syntactic types  and type values .
  \item If  then 
    shall be a -indexed family of relations between
    terms  of type  and values  in PER .
  \end{enumerate}
These relations are defined simultaneously by induction on .
  \begin{enumerate}[(1)]
  \item Neutral types: .
    \begin{enumerate}[(a)]
    \item  iff
      for all ,
      .
    \item  iff  , and
      for all ,
      .
    \end{enumerate}
  \item Universe.
    \begin{enumerate}[(a)]
    \item  iff
      .
    \item  iff
      , and .
    \end{enumerate}
  \item Singletons.
    \begin{enumerate}[(a)]
    \item  iff
       and
      .
    \item  iff
       and
      , and
      .
    \end{enumerate}
  \item Function spaces.
    \begin{enumerate}[(a)]
    \item  iff
      , and , and
      
      for all  and
      .
    \item  iff
      , ,
      and  for all
       and
      .
    \end{enumerate}
  \end{enumerate}
\end{defi}\medskip


\noindent The following technical lemmata show that the logical relations are
preserved by judgmental equality, weakening of the judgement, and the
equalities on the corresponding PERs.   These lemmata are proved
simultaneously for types and terms.


\begin{lem}[Closure under conversion]
  \label{lem:logrelEqTy} Let  
  and .  Then,
  \begin{enumerate}[\em(a)]
  \item , and
  \item if  and  
  then .
  \end{enumerate}
\end{lem}
\LONGVERSION{
\begin{proof}By induction on . See \ref{prf:logrelEqTy}.
\end{proof}
}

\begin{lem}[Monotonicity]
  \label{lem:logrelMon}
  Let , then
  \begin{enumerate}[\em(a)]
  \item if , then
    ; and
  \item if , then
    .
  \end{enumerate}
\end{lem}
\LONGVERSION{
  \begin{proof}By induction on . See \ref{prf:logrelMon}.
  \end{proof}
}

\begin{lem}[Closure under PERs]
  \label{lem:logrelEqD}
  Let , then
  \begin{enumerate}[\em(a)]
  \item if , then ;
    and
  \item if  and 
    , then .
  \end{enumerate}
\end{lem}
\LONGVERSION{
\begin{proof}By induction on . See \ref{prf:logrelEqD}.
\end{proof}
}

The following lemma plays a key r\^ole in the proof of soundness. It
proves that if a term is related to some element in (some PER), then
it is convertible to the reification of the corresponding element in
the PER of normal forms.


\begin{lem}\label{lem:judgeq}
  Let .  Then, 
  \begin{enumerate}[\em(a)]
  \item ,
  \item if  then 
     ; and
  \item if  and for all ,
    ,
    then .
  \end{enumerate}
\end{lem}
\LONGVERSION{
\begin{proof}By induction on . See \ref{prf:judgeq}.
\end{proof}
}

In order to finish the proof of soundness we have to prove that each
well-typed term (and each well-formed type) is logically related to
its denotation; with that aim we extend the definition of logical
relations to substitutions and prove the fundamental theorem of
logical relations.

\begin{defi}[Logical relation for substitutions] \hfill
  \label{logrelsubs}
  \begin{enumerate}[(1)]
  \item  always holds.
  \item  iff
    ,
    , and
    .
  \end{enumerate}
\end{defi}\medskip

\LONGSHORT{

\noindent By the way this relation is defined, the counterparts of
~\ref{lem:logrelEqTy},~\ref{lem:logrelMon}, and~\ref{lem:logrelEqD} are
easily proved by induction on the co-domain of the substitutions.

\begin{rem}
  \label{lem:logrelEqSub}
  If , and , then .
\end{rem}
\begin{rem}
  \label{lem:monsubs}
  If , then
  for any ,
  .
\end{rem}
\begin{rem}
  \label{lem:logrelSubEqD}
  If , and , then .
\end{rem}
}{
  After proving the counterparts of \ref{lem:logrelEqTy},
  \ref{lem:logrelMon} and \ref{lem:logrelEqD} for substitutions, we
  can proceed with the proof of the main theorem of logical relations.
}

\begin{thm}[Fundamental theorem of logical relations]
  \label{thm:logrel}
  Let .
  \begin{enumerate}[\em(1)]
  \item If , then
    ;
  \item if , then
    ; and
  \item if  then
    .
  \end{enumerate}
\end{thm}
\LONGVERSION{
\begin{proof}By mutual induction on the derivations. See \ref{prf:logrel}.
\end{proof}
}

\LONGSHORT{ We define for each context  an element
   of . This environment will be used to define the
  normalisation function.

}{
We define for each context  an element  of ,
that is, by construction, logically related to . This
environment will be used to define the normalisation function; also
notice that if we instantiate Thm.~\ref{thm:logrel} with
, then a well-typed term under  will be logically
related to its denotation.
}
\begin{defi}[Canonical environment]
  \label{def:canonical-env}
\LONGSHORT{
We define  by induction on  as follows:
      \label{eq:env-ctx}
    
}{
  Let , where
       and
       with .


Then  for .
}
\end{defi}\medskip

\LONGVERSION{
\noindent By an immediate induction on contexts we can check the following.

\begin{lem}\label{lem:logrelIdSub} If  then 
  . 
\end{lem}
\begin{proof}By induction on . See \ref{prf:logrelIdSub}
\end{proof}

\subsection{Main results}

Now we can define concretely the normalisation function as the
composition of reification with normalisation after evaluation under
the canonical environment. The following corollaries just instantiate
previous lemmata and theorems concluding correctness of NbE.

}

\begin{defi}[Normalisation algorithm] 
  \label{def:nbe-alg}
  Let , and .
  
\end{defi}\medskip

\LONGSHORT{


  \noindent Notice that if we instantiate Thm.~\ref{thm:logrel} with
  , then a well-typed term  under  will be
  logically related to its denotation. Finally, using the key lemma
  \ref{lem:judgeq} we conclude correctness for NbE.

  \begin{cor}
    Let , and , then by
    fundamental theorem of logical relations (and Lem.~\ref{lem:logrelEqTy}),
    \begin{enumerate}[\em(1)]
    \item ; and
    \item ,
    \end{enumerate}
  \end{cor}

  \begin{cor}[Soundness of NbE]\label{rem:nbe-eq}
    By way of Lem.~\ref{lem:judgeq}, it follows immediately
    \begin{enumerate}[\em(1)]
    \item , and
    \item .
    \end{enumerate}
  \end{cor}

}{ The first point of soundness is a direct consequence of
  Thm.~\ref{thm:logrel} and Lem.~\ref{lem:logrelEqTy}; and the second
  point is obtained using Lem.~\ref{lem:judgeq}.
  \begin{cor}[Soundness of NbE]
      \label{rem:nbe-eq}
      Let , and , then
      \begin{enumerate}
\item , and
        ; and
      \item , and
        .
      \end{enumerate}
  \end{cor}
}
\LONGSHORT{

  We have now a decision procedure for judgmental equality; for
  deciding , put both terms in normal
  formal and check if they are syntactically equal.

  \begin{cor}
    \label{cor:termrel}
    If , and , then we can decide
    . Also if , and
    , we can decide .
  \end{cor}

  As a byproduct we can conclude that type constructors are injective;
  this result is exploited in the next section where we introduce the
  type-checking algorithm.  Injectivity of  plays a key
  r\^ole in all versions of dependent type theory with equality as
  judgement;  \cf\ Adams' \cite{adams} proof of equivalence between
  PTS with equality as a judgement and equality taken as a relation
  between untyped terms, improved by Siles and 
  Herbelin~\cite{silesHerbelin:lics10}.
  
  \begin{rem}
    \label{rem:nbe-hom}
    By expanding definitions, we easily check 
    \begin{enumerate}[(1)]
    \item , and
    \item .
    \end{enumerate}
  \end{rem}

  \begin{cor}[Injectivity of  and of ]
    \label{injcons}
    If , then
    , and
    .  Also
    , then
    , and .
  \end{cor}


}{

  \begin{rem}
    \label{rem:nbe-hom}
    By expanding the definitions, we easily check 
    \begin{enumerate}
    \item , and
    \item .
    \end{enumerate}
  \end{rem}

  \begin{cor}
    \label{cor:termrel}
    If , and , then we can decide
    . Also if , and
    , we can decide .
  \end{cor}

  \begin{cor}[Injectivity of  and of ]
    \label{injcons}
    If , then
    , and
    .  Also
    , then
    , and .
  \end{cor}
}



\PrfIrrTitle
\label{sec:pi-nbe}


\LONGSHORT{ 

  In this section we introduce the logical relations for the new types
  in \lambdaIrr. We skip the re-statement of the results given for
  \lambdaSing\ in \ref{sec:logrel-sing}, instead we present in Appendix
  \ref{sec:proofs} the proof for some of the new cases arising in this
  calculus for each of lemmata \ref{lem:logrelEqTy},
  \ref{lem:logrelMon}, \ref{lem:logrelEqD}, \ref{lem:judgeq} and
  theorem \ref{thm:logrel}.

\begin{defi}[\cf\ \ref{def:logrel}]\hfill
  \begin{enumerate}[(1)]
  \item Sigma types.
    \begin{enumerate}[(a)]
    \item  iff
       and
       and for all  and ,
      .
    \item  iff
       and
       and
      .
    \end{enumerate}
  \item Natural numbers.
    \begin{enumerate}[(a)]
    \item  iff
      .
    \item  iff
       and for all ,
      .
\end{enumerate}
  \item Finite types.
    \begin{enumerate}[(a)]
    \item  iff
      .
    \item  iff
       and for all ,
      .
\end{enumerate}
  \item Proof-irrelevance types.
    \begin{enumerate}[(a)]
    \item  iff
       and .
    \item  iff
      .
    \end{enumerate}
  \end{enumerate}
\end{defi}
}{
  We add the corresponding cases in the definition of logical relations,
  
}



\LONGSHORT{
  






















  \begin{rem}\hfill
    \begin{enumerate}[(1)]
    \item ;
    \item ;
    \item .
    \item .
    \end{enumerate}
  \end{rem}
}{
  \begin{rem}
    All the lemmata \ref{lem:logrelEqTy}, \ref{lem:logrelMon},
    \ref{lem:logrelEqD}, \ref{lem:judgeq}, theorem \ref{thm:logrel},
    and remarks \ref{rem:nbe-eq}, \ref{rem:nbe-hom} are still
    valid. Moreover we also have .
  \end{rem}
}

\LONGVERSION{
\begin{cor}
  If , then
  , and .
\end{cor}
}
   \section{Type-checking algorithm}
  \label{sec:proof-alg}
\LONGSHORT{
  \noindent In this section, we define a couple of judgements that represent a
  bidirectional type checking algorithm for terms in normal form; its
  implementation in Haskell can be found in the appendix.
The algorithm is similar to previous ones 
  \cite{coquand:type,abelCoquandDybjer:flops08},
  in that it proceeds by analysing the possible types for each normal
  form, and succeeds only if the type's shape matches the one required
  by the introduction rule of the term.
The only difference is introduced by the presence of singleton types;
  now we should take into account that a normal form can also have
  a singleton as its type. 

  This situation can be dealt in two possible ways; either one checks
  that the deepest tag of the normalised type (see Def.~\ref{def:erase})
  has the form of the type of the introductory rule; or one adds a rule
  for checking any term against singleton types. The first approach
  requires to have more rules (this is due to the combination of
  singletons and a universe). We take the second approach, which
  requires to compute the eta-long normal form of the type before
  type-checking. We also note that the proof of completeness is more
  involved, because now the algorithm is not only driven by the term
  being checked, but also by the type.

  Our algorithm depends on having a \emph{good} normalisation function;
  note that this function does not need to be based on normalisation
  by evaluation. Also note that the second point asks for having correctness
  and completeness of the normalisation function.

\begin{defi}[Good normalisation function]\hfill
    \label{def:nbe-prop}
    \begin{enumerate}[(1)]
    \item , and
      ; 
    \item  if and only if 
      , and 
      , if and only if
      .
    \end{enumerate}
  \end{defi}\medskip

  \noindent From these properties we can prove the injectivity of which
  is crucial for completeness of type checking -abstractions.
}{ 

  In this section we define a bi-directional type-checking algorithm
  for terms in normal form, and a type-inference algorithm for neutral
  terms. We prove its correctness and completeness.

  The algorithm is similar to previous ones
  \cite{coquand:type,abelCoquandDybjer:flops08}.  The only difference is
  due to the presence of singleton types. We deal with this by
  -normalising the type, and considering first if the normalised
  type is a singleton (side-condition in type-checking of neutrals);
  in that case we check that the term is typeable with the tag of the
  singleton type, and that it is equal to the term of the singleton.

  We stress the importance of having a normalisation function with the
  property stated in Rem.~\ref{rem:nbe-hom}, and also to have
  decidability of equality. In fact, it is enough to have a function
   such that:
  
    \begin{enumerate}
    \item , and
      ; 
    \item  if and only if 
      , and 
      , if and only if
      .
    \end{enumerate}  
}







\subsection{Type-checking \lambdaSing}

In this section, let , and .
\LONGSHORT{
For obtaining the deepest tag of a singleton type, 
we define an operation on types, which
is essentially the same as the one defined by Aspinall~\cite{aspinall:csl94}.
  \begin{defi}[Singleton's tag] 
    
  \end{defi}
}{ 
  We define a function to get the deepest tag of a singleton, that is
  essentially the same as in \cite{aspinall:csl94},
    
}\medskip

  \noindent The predicates for type-checking are defined mutually inductively,
  together with the function for inferring types.

  \begin{defi}[Type-checking and type-inference]
    \label{alg:typecheking}
We define three mutually inductive algorithmic judgements

All three judgements presuppose and maintain the invariant the
input  is a well-formed context. The procedures  and  expect
their inputs  and  in -normal form.  Inference  expects a neutral term  and returns its principal type
 in long normal form. 
\vspace{0.7em}

\localpara{Well-formedness checking of types }
    
\localpara{Type checking terms } 

\localpara{Type inference } 

    \end{defi}\medskip
  
\noindent Bidirectional type checking for dependent function types is
well-understood 
\cite{coquand:type,loehMcBrideSwierstra:tutorialDependentlyLambda};
let us illustrate briefly how it works for singleton types, by
considering the type checking problem
.  Here is
a skeletal derivation of this judgement, which is at the same time an
execution trace of the type checker:

  
  \qquad
  \deqtype{\singTm{\ztm}{\natty}}{\ertype{\singTm{\ztm}{\natty}}}{\natty} 
  \justifies
  \chkterm{\singTm{\ztm}{\natty}}{\natty}{\q}
  
    Since the type to check against is a singleton, 
    the algorithm proceeds by checking
     and
    . Now the type
    of the neutral  is inferred and its tag compared to the given
    type ; as the tag is also , the check succeeds. 
    The remaining equation
     is derivable
    by \ruleref{sing-eq-el}. Of course, the equations are checked
    by the  function; for example, by using
    our own function for normalisation we have
    .


  \begin{thm}[Correctness of type-checking]\hfill
    \label{thm:corr-tc}
    \begin{enumerate}[\em(1)]
    \item If , then .
    \item If , then .
    \item If , then .
    \end{enumerate}
  \end{thm}
\LONGSHORT{
  \begin{proof}By simultaneous induction on 
    , , and . See \ref{prf:corr-tc}.
  \end{proof}
}
{
  \begin{proof}
    By simultaneous induction on the type-checking judgement.
  \end{proof}
}

In order to prove completeness we define a lexicographic order on
pairs of terms and types, in this way we can make induction over the
term, and the type.
  
\begin{defi}
  \label{def:wf-compl}
  Let , and , then  is the lexicographic order on . The corresponding orders are  iff  is
  an immediate sub-term of ; and , iff
  .
\end{defi}
  
  \begin{thm}[Completeness of type-checking]\hfill
    \label{thm:compl-tc}
    \begin{enumerate}[\em(1)]
    \item If , then .
    \item If , then
      .
    \item If , and ,
      then .
    \end{enumerate}
  \end{thm}
\LONGSHORT{
  \begin{proof}
    We prove these three statements simultaneously by well-founded
    induction on the order .  The respective measures are (1)
    , (2) , and (3) .
    Details are in the Appendix~\ref{prf:compl-tc}.
\end{proof}
}
{
  \begin{proof}
    By simultaneous induction on , and well-founded induction on
    .
  \end{proof}
}




\PrfIrrTitle

\LONGSHORT{

  We give additional rules for type-checking and type-inference
  algorithms for the constructs added in
  Sect.~\ref{sec:pi-calc}. Remember that we distinguished two calculi:
  the calculus  has rules (\textsc{\enumrule[0]tm}) and
  (\textsc{prf-tm}); while  lacks those rules.





}{
}

\begin{defi}[Type-checking and type-inference]
  \label{alg:nat-tc}

\LONGSHORT{
\renewcommand{\para}[1]{\noindent #1.} 

\para{-types}



\para{Natural numbers}


\para{Finite types}
  

\para{Proof types}
  

}{  

}
\end{defi}\medskip



\LONGSHORT{

\noindent  We do not show the proof for correctness, because nothing is to be
  gained from it; suffice it to say that we can prove correctness with
  respect to .

\begin{thm}
\label{thm:corr-tc-pi}
  The type-checking algorithm is sound with respect to the calculus
  .
\end{thm}  
\begin{proof}
  By simultaneous induction on the derivability of the type-checking
  judgements.
\end{proof}


It is clear that the given rules are not complete for checking
, because there is no rule for checking
. Note that it is not possible to have a
sound and complete type-checking algorithm with respect to
, for it would imply the decidability of
type-inhabitation. Since type checking happens always \emph{before}
normalisation, we can still use a good normalisation function with
respect to the calculus  for normalising types or deciding
equality. Indeed, if the term to type-check does not contain ,
the need of checking  will never arise;
this is clearly seen by verifying that only sub-terms are
type-checked in the premises.

\begin{thm}
  The type-checking algorithm is complete with respect to the calculus
  .
\end{thm}  
\begin{proof}
  By simultaneous induction on the normal form of types and terms,
  using inversion on the typing judgement and correctness of .
\end{proof}

}{ \begin{rem}
    Thm.~\ref{thm:corr-tc} is still valid for the calculus with
    \ruleref{prf-tm}. Moreover, Thm.~\ref{thm:compl-tc} is valid if
    we add the axiom 
  \end{rem}
}



\begin{cor}
  The type-checking algorithm is correct (by Thm.~\ref{thm:corr-tc-pi}
  and Cor.~\ref{cor:cons-prf-tm}) and complete with respect to the
  calculus . \end{cor}






 
\section{Conclusion}

\noindent The main contributions of the paper are the definition of a correct
and complete type-checking algorithm, and a simpler solution to the
problem of generating fresh identifiers in the NbE algorithm for a
calculus with singletons, one universe, and proof-irrelevant
types. The type-checker is based on the NbE algorithm which is used to
decide equality and to prove the injectivity of the type
constructors. We emphasise that the type-checking algorithm is modular
with respect to the normalisation algorithm. All the results can be
extended to a calculus with annotated lambda abstractions, yielding a
type-checking algorithm for terms not necessarily in normal forms.
The NbE algorithm can be implemented fairly easily in Haskell (\cf\
Appendix \ref{sec:nbealg}), but the correctness of the implementation
depends on proving the computational adequacy of the domain semantics
with respect to Haskell's operational semantics. We have not developed
this proof in this article and leave it for to future work.




\subsection{Related and Further Work on Singleton Types}
\label{sec:related}

\noindent Singleton types are used to model the SML module system and records
with manifest fields \cite{coquandPollackTakeyama:fundinf05}.

Aspinall \cite{aspinall:csl94} presents a logical framework with
singleton types and subtyping and shows its consistency via a PER
model, yet not decidability.  
The second author, Pollack, and Takeyama
\cite{coquandPollackTakeyama:fundinf05} extend the Aspinall's
framework by -equality and records and a type checking algorithm
which is correct wrt.\ the PER model. This work is unconventional since
there is no complete syntactical specification of the LF in terms of
syntax, typing and equality rules.  Instead, in the style of
Martin-L\"of meaning explanations, they list a number of inference
rules which are valid in the semantics and prove that type-checked
expressions evaluate to values of the correct semantic type.

Courant \cite{courant:itrs02} shows strong normalization for a variant of
Aspinall's system with equality defined by reduction.  He uses a
typed Kripke model of strongly normalizing terms, a variant of
Goguen's typed operational semantics \cite{goguen:PhD}.  

Stone and Harper \cite{stoneHarper:tocl06} extend Aspinall's framework
by sigma types and eta-equality, which allows them to reduce
singletons at higher types to singletons at base type.  Their decision
procedure is type-directed, its completeness is shown via a Kripke
model.  Crary \cite{crary:lfmtp08} gives a simplified decision
procedure via hereditary substitutions and proves its correctness in
Twelf, without the need for a model construction.  His purely
syntactical approach does not scale to universes, since he cannot
handle types defined by recursion.  Goguen \cite{goguen:syntacticEta}
follows a similar agenda, he shows decidability for singleton types
in the presence of eta by an eta-expanding translation into a logical
framework with beta-equality only.  He works with fully annotated
terms in the sense of Streicher \cite{streicher:PhD}.  He stresses
that his approach does not scale to computation on the type level.

In the continuation of this work we want to investigate whether our
type-checking algorithm can be simplified if we implement Stone and
Harper's insight that singleton types at higher types can be defined
in terms of singleton base types.  Further, we would like to integrate
subtyping in our calculus, which should not be too difficult, since
the PER model already supports subtyping
\cite{aspinall:csl94,coquandPollackTakeyama:fundinf05}. 


\subsection{Related and Further Work on Proof Irrelevance}

Pfenning \cite{pfenning:intextirr} presents a logical framework with
proof irrelevance that supports irrelevant function arguments,
with function introduction rule 
(writing  in our syntax):  

He proves decidability using erasure, mentioning that
his technique does not scale to universes.  
Elimination of irrelevance is implicitly handled by annotating
variables to ensure proof variables 
appear only in proofs,
in contrast to our explicit use of  in the style of Awodey and Bauer
\cite{awodeyBauer:propositionsAsTypes}.  However, we believe that
Pfenning's proof irrelevance can be modeled via bracket types 
, with the weaker ``monadic'' rule for  (see
section~\ref{sec:cwf-gat}). 

Barras and Bernardo's \cite{DBLP:conf/fossacs/BarrasB08} presentation of
proof irrelevant functions

diverges from Pfenning's that they allow irrelevant variables  to
be relevant in types .  (In  the variable  might only
appear irrelevantly, expressed by the side condition that  may not be
free in the relevant parts  of .)
Barras and Bernardo justify their calculus by erasing into Miquel's
Implicit Calculus of Constructions (ICC) \cite{miquel:tlca01}. The ICC
style irrelevance seems more expressive than Awodey and Bauer's or
Pfenning's, but the exact relationship is unclear to us.



Berger's Uniform Heyting Algebra \cite{berger:uniformHA} features
uniform quantification   (and  to
obtain optimized programs by extraction from proofs.  A proof of a
uniform universal

may not mention term variable  in a computational relevant
position.  Since the shape of formulas does not depend on terms,
Berger's calculus can be seen as logical counterpart of
either Pfenning's or Bruno and Bernardo's type system.

We see two interesting questions about the different approaches to
proof irrelevance above:
\begin{enumerate}[(1)]
\item How can Barras and Bernardo's ICC be understood in
  terms of judgmental equality \`a la Pfenning?
\item How can ICC and the calculus of Pfenning be extended to full
  bracket types \`a la Awodey and Bauer without explicit use of .
\end{enumerate}

 
\para{Acknowledgments} The authors have led many discussions with Peter
Dybjer on normalization by evaluation, generalized algebraic theories,
and presentation of type theory as categories with families.  The
authors thank the referees of a previous version of this article for
their helpful comments. The authors are also grateful for the
suggestions made by the anonymous referees on this long version. 


\bibliographystyle{hplain}\bibliography{biblio,auto-lmcs}

\appendix

\section{Normalisation by evaluation}
\label{sec:nbealg}

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}\column{5}{@{}>{\hspre}l<{\hspost}@{}}\column{6}{@{}>{\hspre}l<{\hspost}@{}}\column{8}{@{}>{\hspre}l<{\hspost}@{}}\column{9}{@{}>{\hspre}l<{\hspost}@{}}\column{10}{@{}>{\hspre}l<{\hspost}@{}}\column{11}{@{}>{\hspre}l<{\hspost}@{}}\column{12}{@{}>{\hspre}l<{\hspost}@{}}\column{13}{@{}>{\hspre}l<{\hspost}@{}}\column{14}{@{}>{\hspre}l<{\hspost}@{}}\column{15}{@{}>{\hspre}l<{\hspost}@{}}\column{16}{@{}>{\hspre}l<{\hspost}@{}}\column{19}{@{}>{\hspre}l<{\hspost}@{}}\column{20}{@{}>{\hspre}l<{\hspost}@{}}\column{22}{@{}>{\hspre}l<{\hspost}@{}}\column{23}{@{}>{\hspre}l<{\hspost}@{}}\column{24}{@{}>{\hspre}l<{\hspost}@{}}\column{25}{@{}>{\hspre}l<{\hspost}@{}}\column{29}{@{}>{\hspre}c<{\hspost}@{}}\column{29E}{@{}l@{}}\column{30}{@{}>{\hspre}l<{\hspost}@{}}\column{31}{@{}>{\hspre}l<{\hspost}@{}}\column{32}{@{}>{\hspre}l<{\hspost}@{}}\column{35}{@{}>{\hspre}l<{\hspost}@{}}\column{36}{@{}>{\hspre}l<{\hspost}@{}}\column{37}{@{}>{\hspre}l<{\hspost}@{}}\column{42}{@{}>{\hspre}l<{\hspost}@{}}\column{43}{@{}>{\hspre}l<{\hspost}@{}}\column{44}{@{}>{\hspre}l<{\hspost}@{}}\column{45}{@{}>{\hspre}l<{\hspost}@{}}\column{47}{@{}>{\hspre}l<{\hspost}@{}}\column{53}{@{}>{\hspre}l<{\hspost}@{}}\column{62}{@{}>{\hspre}c<{\hspost}@{}}\column{62E}{@{}l@{}}\column{65}{@{}>{\hspre}l<{\hspost}@{}}\column{E}{@{}>{\hspre}l<{\hspost}@{}}\>[B]{}\mathbf{type}\;\Conid{Type}{}\<[13]\>[13]{}\mathrel{=}{}\<[16]\>[16]{}\Conid{Term}{}\<[E]\\
\>[B]{}\mathbf{data}\;\Conid{Term}{}\<[13]\>[13]{}\mathrel{=}{}\<[16]\>[16]{}\Conid{U}{}\<[44]\>[44]{}\mbox{\onelinecomment  universe}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Fun}\;\Conid{Type}\;\Conid{Type}{}\<[44]\>[44]{}\mbox{\onelinecomment  dependent function space}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Singl}\;\Conid{Term}\;\Conid{Type}{}\<[44]\>[44]{}\mbox{\onelinecomment  singleton type ()}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{App}\;\Conid{Term}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  application}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Lam}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  abstraction}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Q}{}\<[44]\>[44]{}\mbox{\onelinecomment  variable}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Sub}\;\Conid{Term}\;\Conid{Subst}{}\<[44]\>[44]{}\mbox{\onelinecomment  substitution}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Sigma}\;\Conid{Type}\;\Conid{Type}{}\<[44]\>[44]{}\mbox{\onelinecomment  dependent pair type}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Fst}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  first projection}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Snd}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  second projection}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Pair}\;\Conid{Term}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  dependent pair }{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Nat}{}\<[44]\>[44]{}\mbox{\onelinecomment  naturals}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Zero}{}\<[44]\>[44]{}\mbox{\onelinecomment  0}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Suc}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  +1}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Natrec}\;\Conid{Type}\;\Conid{Term}\;\Conid{Term}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  elimination for Nat}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Prf}\;\Conid{Type}{}\<[44]\>[44]{}\mbox{\onelinecomment  proof (with proof irrelevance)}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Box}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  a term in \ensuremath{\Conid{Prf}\;\Conid{A}}}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Star}{}\<[44]\>[44]{}\mbox{\onelinecomment  canonical element of \ensuremath{\Conid{Prf}\;\Conid{A}}}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Where}\;\Conid{Type}\;\Conid{Term}\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  \ensuremath{\Conid{Box}} elimination}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Enum}\;\Conid{Int}{}\<[44]\>[44]{}\mbox{\onelinecomment  \ensuremath{\Conid{Enum}\;\Varid{n}} has n elements}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Const}\;\Conid{Int}\;\Conid{Int}{}\<[44]\>[44]{}\mbox{\onelinecomment  \ensuremath{\Conid{Const}\;\Varid{n}\;\Varid{i}} is the \ensuremath{\Varid{i}}th element}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Case}\;\Conid{Int}\;\Conid{Type}\;[\mskip1.5mu \Conid{Term}\mskip1.5mu]\;\Conid{Term}{}\<[44]\>[44]{}\mbox{\onelinecomment  elimination for \ensuremath{\Conid{Enum}\;\Varid{n}}}{}\<[E]\\
\>[16]{}\mathbf{deriving}\;(\Conid{Eq},\Conid{Show}){}\<[E]\\blanklineskip]\>[B]{}\mathbf{type}\;\Conid{DT}{}\<[13]\>[13]{}\mathrel{=}{}\<[16]\>[16]{}\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  semantic types}{}\<[E]\\
\>[B]{}\mathbf{data}\;\Conid{D}{}\<[13]\>[13]{}\mathrel{=}{}\<[16]\>[16]{}\Conid{T}{}\<[44]\>[44]{}\mbox{\onelinecomment  terminal object (empty context)}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Ld}\;(\Conid{D}\to \Conid{D}){}\<[44]\>[44]{}\mbox{\onelinecomment  function}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{FunD}\;\Conid{DT}\;(\Conid{D}\to \Conid{DT}){}\<[44]\>[44]{}\mbox{\onelinecomment  dependent function type}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{UD}{}\<[44]\>[44]{}\mbox{\onelinecomment  universe}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{SingD}\;\Conid{D}\;\Conid{DT}{}\<[44]\>[44]{}\mbox{\onelinecomment  singleton type}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{Vd}\;\Conid{Int}{}\<[44]\>[44]{}\mbox{\onelinecomment  free variable}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{AppD}\;\Conid{D}\;\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  neutral application}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{SumD}\;\Conid{DT}\;(\Conid{D}\to \Conid{DT}){}\<[44]\>[44]{}\mbox{\onelinecomment  dependent pair type}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{PairD}\;\Conid{D}\;\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  context comprehension}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{FstD}\;\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  first projection of neutral}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{SndD}\;\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  second projection of neutral}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{NatD}{}\<[44]\>[44]{}\mbox{\onelinecomment  natural number type}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{ZeroD}{}\<[44]\>[44]{}\mbox{\onelinecomment  0}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{SucD}\;\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  +1}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{NatrecD}\;(\Conid{D}\to \Conid{DT})\;\Conid{D}\;\Conid{D}\;\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  recursion on neutrals}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{PrfD}\;\Conid{DT}{}\<[44]\>[44]{}\mbox{\onelinecomment  proof type}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{StarD}{}\<[44]\>[44]{}\mbox{\onelinecomment  don't care}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{EnumD}\;\Conid{Int}{}\<[44]\>[44]{}\mbox{\onelinecomment  enumeration type}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{ConstD}\;\Conid{Int}\;\Conid{Int}{}\<[44]\>[44]{}\mbox{\onelinecomment  constants in \ensuremath{\Conid{EnumD}}}{}\<[E]\\
\>[13]{}\mid {}\<[16]\>[16]{}\Conid{CaseD}\;\Conid{Int}\;(\Conid{D}\to \Conid{DT})\;[\mskip1.5mu \Conid{D}\mskip1.5mu]\;\Conid{D}{}\<[44]\>[44]{}\mbox{\onelinecomment  elimination on neutrals}{}\<[E]\\blanklineskip]\>[B]{}\Varid{pi1},\Varid{pi2}\mathbin{::}{}\<[14]\>[14]{}\Conid{D}\to \Conid{D}{}\<[E]\\
\>[B]{}\Varid{pi1}\;{}\<[6]\>[6]{}(\Conid{PairD}\;\Varid{d}\;\Varid{d'}){}\<[20]\>[20]{}\mathrel{=}{}\<[23]\>[23]{}\Varid{d}{}\<[E]\\
\>[B]{}\Varid{pi1}\;{}\<[6]\>[6]{}\Conid{StarD}{}\<[20]\>[20]{}\mathrel{=}\Conid{StarD}{}\<[E]\\
\>[B]{}\Varid{pi1}\;{}\<[6]\>[6]{}\Varid{k}{}\<[20]\>[20]{}\mathrel{=}{}\<[23]\>[23]{}\Conid{FstD}\;\Varid{k}{}\<[E]\\
\>[B]{}\Varid{pi2}\;{}\<[6]\>[6]{}(\Conid{PairD}\;\Varid{d}\;\Varid{d'}){}\<[20]\>[20]{}\mathrel{=}{}\<[23]\>[23]{}\Varid{d'}{}\<[E]\\
\>[B]{}\Varid{pi2}\;{}\<[6]\>[6]{}\Conid{StarD}{}\<[20]\>[20]{}\mathrel{=}\Conid{StarD}{}\<[E]\\
\>[B]{}\Varid{pi2}\;{}\<[6]\>[6]{}\Varid{k}{}\<[20]\>[20]{}\mathrel{=}{}\<[23]\>[23]{}\Conid{SndD}\;\Varid{k}{}\<[E]\\blanklineskip]\>[B]{}\Varid{neutralD}\mathbin{::}\Conid{D}\to \Conid{Bool}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;(\Conid{Vd}\;\anonymous ){}\<[30]\>[30]{}\mathrel{=}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;(\Conid{AppD}\;\anonymous \;\anonymous ){}\<[30]\>[30]{}\mathrel{=}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;(\Conid{FstD}\;\anonymous ){}\<[30]\>[30]{}\mathrel{=}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;(\Conid{SndD}\;\anonymous ){}\<[30]\>[30]{}\mathrel{=}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;(\Conid{NatrecD}\;\anonymous \;\anonymous \;\anonymous \;\anonymous ){}\<[30]\>[30]{}\mathrel{=}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;(\Conid{CaseD}\;\anonymous \;\anonymous \;\anonymous \;\anonymous ){}\<[30]\>[30]{}\mathrel{=}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;\Conid{StarD}{}\<[30]\>[30]{}\mathrel{=}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{neutralD}\;\anonymous {}\<[30]\>[30]{}\mathrel{=}\Conid{False}{}\<[E]\\blanklineskip]\>[B]{}\Varid{downs}\mathbin{::}\Conid{Int}\to (\Conid{D}\to \Conid{DT})\to [\mskip1.5mu \Conid{D}\mskip1.5mu]\to \Conid{Int}\to [\mskip1.5mu \Conid{D}\mskip1.5mu]{}\<[E]\\
\>[B]{}\Varid{downs}\;{}\<[8]\>[8]{}\anonymous \;{}\<[11]\>[11]{}\anonymous \;{}\<[14]\>[14]{}[\mskip1.5mu \mskip1.5mu]\;{}\<[22]\>[22]{}\anonymous {}\<[25]\>[25]{}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]\\
\>[B]{}\Varid{downs}\;{}\<[8]\>[8]{}\Varid{n}\;{}\<[11]\>[11]{}\Varid{f}\;{}\<[14]\>[14]{}(\Varid{d}\mathbin{:}\Varid{ds})\;{}\<[22]\>[22]{}\Varid{i}{}\<[25]\>[25]{}\mathrel{=}\Varid{down}\;(\Varid{f}\;(\Conid{ConstD}\;\Varid{n}\;\Varid{i}))\;\Varid{d}\mathbin{:}\Varid{downs}\;\Varid{n}\;\Varid{f}\;\Varid{ds}\;(\Varid{i}\mathbin{+}\mathrm{1}){}\<[E]\\blanklineskip]\>[B]{}\Varid{caseD}\mathbin{::}\Conid{Int}\to (\Conid{D}\to \Conid{DT})\to [\mskip1.5mu \Conid{D}\mskip1.5mu]\to \Conid{D}\to \Conid{D}{}\<[E]\\
\>[B]{}\Varid{caseD}\;\Varid{n}\;\Varid{b}\;\Varid{ds}\;\Conid{StarD}{}\<[62]\>[62]{}\mathrel{=}{}\<[62E]\>[65]{}\Conid{StarD}{}\<[E]\\
\>[B]{}\Varid{caseD}\;\Varid{n}\;\Varid{b}\;\Varid{ds}\;(\Conid{ConstD}\;\Varid{m}\;\Varid{i})\mid \Varid{n}\equiv \Varid{m}\mathrel{\wedge}\Varid{i}\mathbin{<}\Varid{n}{}\<[62]\>[62]{}\mathrel{=}{}\<[62E]\>[65]{}\Varid{ds}\mathbin{!!}\Varid{i}{}\<[E]\\
\>[B]{}\Varid{caseD}\;\Varid{n}\;\Varid{b}\;\Varid{ds}\;\Varid{d}\mid {}\<[19]\>[19]{}\Varid{neutralD}\;\Varid{d}\mathrel{\wedge}{}\<[E]\\
\>[19]{}\Varid{and}\;[\mskip1.5mu \Varid{constD}\;\Varid{n}\;\Varid{i}\;(\Varid{ds}\mathbin{!!}\Varid{i})\mid \Varid{i}\leftarrow [\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\Varid{n}\mathbin{-}\mathrm{1}\mskip1.5mu]\mskip1.5mu]{}\<[62]\>[62]{}\mathrel{=}{}\<[62E]\>[65]{}\Varid{up}\;(\Varid{b}\;\Varid{d})\;\Varid{d}{}\<[E]\\
\>[B]{}\Varid{caseD}\;\Varid{n}\;\Varid{b}\;\Varid{ds}\;\Varid{d}\mid {}\<[19]\>[19]{}\Varid{neutralD}\;\Varid{d}{}\<[62]\>[62]{}\mathrel{=}{}\<[62E]\>[65]{}\Varid{up}\;(\Varid{b}\;\Varid{d}){}\<[E]\\
\>[19]{}\hsindent{16}{}\<[35]\>[35]{}(\Conid{CaseD}\;\Varid{n}\;{}\<[45]\>[45]{}(\lambda \Varid{e}\to \Varid{downT}\;(\Varid{b}\;\Varid{e}))\;{}\<[E]\\
\>[45]{}(\Varid{downs}\;\Varid{n}\;\Varid{b}\;\Varid{ds}\;\mathrm{0})\;{}\<[E]\\
\>[45]{}\Varid{d}){}\<[E]\\
\>[B]{}\Varid{up}\mathbin{::}\Conid{DT}\to \Conid{D}\to \Conid{D}{}\<[E]\\
\>[B]{}\Varid{up}\;(\Conid{SingD}\;\Varid{a}\;\Varid{x})\;{}\<[19]\>[19]{}\Varid{k}{}\<[22]\>[22]{}\mathrel{=}\Varid{a}{}\<[E]\\
\>[B]{}\Varid{up}\;(\Conid{FunD}\;\Varid{a}\;\Varid{f})\;{}\<[19]\>[19]{}\Varid{k}{}\<[22]\>[22]{}\mathrel{=}\Conid{Ld}\;(\lambda \Varid{d}\to \Varid{up}\;(\Varid{f}\;\Varid{d})\;(\Conid{AppD}\;\Varid{k}\;(\Varid{down}\;\Varid{a}\;\Varid{d}))){}\<[E]\\
\>[B]{}\Varid{up}\;(\Conid{SumD}\;\Varid{a}\;\Varid{f})\;{}\<[19]\>[19]{}\Varid{k}{}\<[22]\>[22]{}\mathrel{=}\Conid{PairD}\;{}\<[31]\>[31]{}(\Varid{up}\;\Varid{a}\;(\Conid{FstD}\;\Varid{k}))\;{}\<[E]\\
\>[31]{}(\Varid{up}\;(\Varid{f}\;(\Varid{up}\;\Varid{a}\;(\Conid{FstD}\;\Varid{k})))\;(\Conid{SndD}\;\Varid{k})){}\<[E]\\
\>[B]{}\Varid{up}\;(\Conid{PrfD}\;\Varid{a})\;{}\<[19]\>[19]{}\Varid{k}{}\<[22]\>[22]{}\mathrel{=}\Conid{StarD}{}\<[E]\\
\>[B]{}\Varid{up}\;(\Conid{EnumD}\;\mathrm{0})\;{}\<[19]\>[19]{}\Varid{k}{}\<[22]\>[22]{}\mathrel{=}\Conid{StarD}{}\<[E]\\
\>[B]{}\Varid{up}\;(\Conid{EnumD}\;\mathrm{1})\;{}\<[19]\>[19]{}\Varid{k}{}\<[22]\>[22]{}\mathrel{=}\Conid{ConstD}\;\mathrm{1}\;\mathrm{0}{}\<[E]\\
\>[B]{}\Varid{up}\;\Varid{d}\;{}\<[19]\>[19]{}\Varid{k}{}\<[22]\>[22]{}\mathrel{=}\Varid{k}{}\<[E]\\blanklineskip]\>[B]{}\Varid{downT}\mathbin{::}\Conid{DT}\to \Conid{DT}{}\<[E]\\
\>[B]{}\Varid{downT}\;(\Conid{SingD}\;\Varid{a}\;\Varid{x}){}\<[22]\>[22]{}\mathrel{=}\Conid{SingD}\;{}\<[31]\>[31]{}(\Varid{down}\;\Varid{x}\;\Varid{a})\;{}\<[43]\>[43]{}(\Varid{downT}\;\Varid{x}){}\<[E]\\
\>[B]{}\Varid{downT}\;(\Conid{FunD}\;\Varid{a}\;\Varid{f}){}\<[22]\>[22]{}\mathrel{=}\Conid{FunD}\;{}\<[31]\>[31]{}(\Varid{downT}\;\Varid{a})\;{}\<[43]\>[43]{}(\lambda \Varid{d}\to \Varid{downT}\;(\Varid{f}\;(\Varid{up}\;\Varid{a}\;\Varid{d}))){}\<[E]\\
\>[B]{}\Varid{downT}\;(\Conid{SumD}\;\Varid{a}\;\Varid{b}){}\<[22]\>[22]{}\mathrel{=}\Conid{SumD}\;{}\<[31]\>[31]{}(\Varid{downT}\;\Varid{a})\;{}\<[43]\>[43]{}(\lambda \Varid{d}\to \Varid{downT}\;(\Varid{b}\;(\Varid{up}\;\Varid{a}\;\Varid{d}))){}\<[E]\\
\>[B]{}\Varid{downT}\;(\Conid{PrfD}\;\Varid{a}){}\<[22]\>[22]{}\mathrel{=}\Conid{PrfD}\;{}\<[31]\>[31]{}(\Varid{downT}\;\Varid{a}){}\<[E]\\
\>[B]{}\Varid{downT}\;\Varid{d}{}\<[22]\>[22]{}\mathrel{=}\Varid{d}{}\<[E]\\blanklineskip]\>[B]{}\mbox{\onelinecomment  Evaluation}{}\<[E]\\blanklineskip]\>[B]{}\Varid{eval}\mathbin{::}\Conid{Term}\to \Conid{Env}\to \Conid{D}{}\<[E]\\
\>[B]{}\Varid{eval}\;\Conid{U}\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Conid{UD}{}\<[E]\\
\>[B]{}\Varid{eval}\;(\Conid{Fun}\;\Varid{t}\;\Varid{f})\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Conid{FunD}\;(\Varid{eval}\;\Varid{t}\;\Varid{d})\;(\lambda \Varid{d'}\to \Varid{eval}\;\Varid{f}\;(\Conid{PairD}\;\Varid{d}\;\Varid{d'})){}\<[E]\\
\>[B]{}\Varid{eval}\;(\Conid{Singl}\;\Varid{t}\;\Varid{a})\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Conid{SingD}\;(\Varid{eval}\;\Varid{t}\;\Varid{d})\;(\Varid{eval}\;\Varid{a}\;\Varid{d}){}\<[E]\\
\>[B]{}\Varid{eval}\;(\Conid{Lam}\;\Varid{t})\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Conid{Ld}\;(\lambda \Varid{d'}\to \Varid{eval}\;\Varid{t}\;(\Conid{PairD}\;\Varid{d}\;\Varid{d'})){}\<[E]\\
\>[B]{}\Varid{eval}\;(\Conid{App}\;\Varid{t}\;\Varid{r})\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}(\Varid{eval}\;\Varid{t}\;\Varid{d})\mathbin{`\Varid{ap}`}(\Varid{eval}\;\Varid{r}\;\Varid{d}){}\<[E]\\
\>[B]{}\Varid{eval}\;\Conid{Q}\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Varid{pi2}\;\Varid{d}{}\<[E]\\
\>[B]{}\Varid{eval}\;(\Conid{Sub}\;\Varid{t}\;\Varid{s})\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Varid{eval}\;\Varid{t}\;(\Varid{evalS}\;\Varid{s}\;\Varid{d}){}\<[E]\\blanklineskip]\>[B]{}\Varid{eval}\;\Conid{Nat}\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Conid{NatD}{}\<[E]\\
\>[B]{}\Varid{eval}\;\Conid{Zero}\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Conid{ZeroD}{}\<[E]\\
\>[B]{}\Varid{eval}\;(\Conid{Suc}\;\Varid{t})\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Conid{SucD}\;(\Varid{eval}\;\Varid{t}\;\Varid{d}){}\<[E]\\
\>[B]{}\Varid{eval}\;(\Conid{Natrec}\;\Varid{b}\;\Varid{z}\;\Varid{s}\;\Varid{t})\;{}\<[24]\>[24]{}\Varid{d}\mathrel{=}\Varid{natrec}\;{}\<[36]\>[36]{}(\lambda \Varid{e}\to \Varid{eval}\;\Varid{b}\;(\Conid{PairD}\;\Varid{d}\;\Varid{e}))\;{}\<[E]\\
\>[36]{}(\Varid{eval}\;\Varid{z}\;\Varid{d})\;{}\<[E]\\
\>[36]{}(\Varid{eval}\;\Varid{s}\;\Varid{d})\;{}\<[E]\\
\>[36]{}(\Varid{eval}\;\Varid{t}\;\Varid{d}){}\<[E]\\blanklineskip]\>[B]{}\Varid{evalS}\mathbin{::}\Conid{Subst}\to \Conid{Env}\to \Conid{Env}{}\<[E]\\
\>[B]{}\Varid{evalS}\;\Conid{E}\;{}\<[20]\>[20]{}\Varid{d}\mathrel{=}\Conid{T}{}\<[E]\\
\>[B]{}\Varid{evalS}\;\Conid{Is}\;{}\<[20]\>[20]{}\Varid{d}\mathrel{=}\Varid{d}{}\<[E]\\
\>[B]{}\Varid{evalS}\;(\Conid{Ext}\;\Varid{s}\;\Varid{t})\;{}\<[20]\>[20]{}\Varid{d}\mathrel{=}\Conid{PairD}\;(\Varid{evalS}\;\Varid{s}\;\Varid{d})\;(\Varid{eval}\;\Varid{t}\;\Varid{d}){}\<[E]\\
\>[B]{}\Varid{evalS}\;\Conid{P}\;{}\<[20]\>[20]{}\Varid{d}\mathrel{=}\Varid{pi1}\;\Varid{d}{}\<[E]\\
\>[B]{}\Varid{evalS}\;(\Conid{Comp}\;\Varid{s}\;\Varid{s'})\;{}\<[20]\>[20]{}\Varid{d}\mathrel{=}(\Varid{evalS}\;\Varid{s}\mathbin{\circ}\Varid{evalS}\;\Varid{s'})\;\Varid{d}{}\<[E]\\blanklineskip]\>[B]{}\Varid{nbeTy}\mathbin{::}\Conid{Type}\to \Conid{Type}{}\<[E]\\
\>[B]{}\Varid{nbeTy}\;\Varid{ty}\mathrel{=}\Varid{readback}\;\mathrm{0}\;(\Varid{downT}\;(\Varid{eval}\;\Varid{ty}\;\Conid{T})){}\<[E]\\blanklineskip]\>[B]{}\Varid{nbeOpenTy}\mathbin{::}\Conid{Ctx}\to \Conid{Type}\to \Conid{Type}{}\<[E]\\
\>[B]{}\Varid{nbeOpenTy}\;\Varid{ctx}\;\Varid{ty}{}\<[19]\>[19]{}\mathrel{=}\Varid{readback}\;\Varid{n}\;(\Varid{downT}\;(\Varid{eval}\;\Varid{ty}\;\Varid{env})){}\<[E]\\
\>[B]{}\hsindent{5}{}\<[5]\>[5]{}\mathbf{where}\;{}\<[12]\>[12]{}\Varid{n}{}\<[19]\>[19]{}\mathrel{=}\Varid{length}\;\Varid{ctx}{}\<[E]\\
\>[12]{}\Varid{env}{}\<[19]\>[19]{}\mathrel{=}\Varid{mkenv}\;\Varid{n}\;\Varid{ctx}{}\<[E]\\blanklineskip]\>[B]{}\Varid{mkvar}\mathbin{::}\Conid{Int}\to \Conid{Term}{}\<[E]\\
\>[B]{}\Varid{mkvar}\;\Varid{n}{}\<[10]\>[10]{}\mid \Varid{n}\equiv \mathrm{0}{}\<[23]\>[23]{}\mathrel{=}\Conid{Q}{}\<[E]\\
\>[10]{}\mid \Varid{otherwise}{}\<[23]\>[23]{}\mathrel{=}\Conid{Sub}\;\Conid{Q}\;(\Varid{subs}\;(\Varid{n}\mathbin{-}\mathrm{1})){}\<[E]\\blanklineskip]\>[B]{}\Varid{chkTerm}\mathbin{::}\Conid{Ctx}\to \Conid{Type}\to \Conid{Term}\to \Conid{Bool}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}\Conid{U}\;{}\<[26]\>[26]{}(\Conid{Fun}\;\Varid{t}\;\Varid{t'}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;\Conid{U}\;\Varid{t}\mathrel{\wedge}{}\<[E]\\
\>[44]{}\Varid{chkTerm}\;(\Varid{t}\mathbin{:}\Varid{ts})\;\Conid{U}\;\Varid{t'}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}\Conid{U}\;{}\<[26]\>[26]{}(\Conid{Singl}\;\Varid{e}\;\Varid{t}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;\Conid{U}\;\Varid{t}\mathrel{\wedge}{}\<[E]\\
\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;\Varid{t}\;\Varid{e}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}\Conid{U}\;{}\<[26]\>[26]{}(\Conid{Sigma}\;\Varid{t}\;\Varid{t'}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;\Conid{U}\;\Varid{t}\mathrel{\wedge}{}\<[E]\\
\>[44]{}\Varid{chkTerm}\;(\Varid{t}\mathbin{:}\Varid{ts})\;\Conid{U}\;\Varid{t'}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}\Conid{U}\;{}\<[27]\>[27]{}\Conid{Nat}{}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}(\Conid{Fun}\;\Varid{t}\;\Varid{t'})\;{}\<[26]\>[26]{}(\Conid{Lam}\;\Varid{e}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;(\Varid{t}\mathbin{:}\Varid{ts})\;\Varid{t'}\;\Varid{e}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}(\Conid{Singl}\;\Varid{e}\;\Varid{t})\;{}\<[26]\>[26]{}\Varid{e'}{}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;(\Varid{nbeOpenTy}\;\Varid{ts}\;\Varid{t})\;\Varid{e'}\mathrel{\wedge}{}\<[E]\\
\>[44]{}(\Varid{nbeOpen}\;\Varid{ts}\;\Varid{e}\;\Varid{t})\equiv (\Varid{nbeOpen}\;\Varid{ts}\;\Varid{e'}\;\Varid{t}){}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}(\Conid{Sigma}\;\Varid{t}\;\Varid{r})\;{}\<[26]\>[26]{}(\Conid{Pair}\;\Varid{e}\;\Varid{e'}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;\Varid{t}\;\Varid{e}\mathrel{\wedge}{}\<[E]\\
\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;(\Varid{nbeOpenTy}\;\Varid{ts}\;(\Varid{sgSub}\;\Varid{r}\;\Varid{e}))\;\Varid{e'}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}\Conid{Nat}\;{}\<[26]\>[26]{}\Conid{Zero}{}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Conid{True}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}\Conid{Nat}\;{}\<[26]\>[26]{}(\Conid{Suc}\;\Varid{t}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;\Conid{Nat}\;\Varid{t}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}(\Conid{Prf}\;\Varid{t})\;{}\<[26]\>[26]{}(\Conid{Box}\;\Varid{e}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkTerm}\;\Varid{ts}\;\Varid{t}\;\Varid{e}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}(\Conid{Enum}\;\Varid{n})\;{}\<[26]\>[26]{}(\Conid{Const}\;\Varid{m}\;\Varid{i}){}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{m}\equiv \Varid{n}\mathrel{\wedge}\Varid{i}\mathbin{<}\Varid{n}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\Varid{ts}\;{}\<[13]\>[13]{}\Varid{t}\;{}\<[26]\>[26]{}\Varid{e}\mid \Varid{neutral}\;\Varid{e}{}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Varid{chkNeTerm}\;\Varid{ts}\;\Varid{t}\;\Varid{e}{}\<[E]\\
\>[B]{}\Varid{chkTerm}\;\anonymous \;{}\<[13]\>[13]{}\anonymous \;{}\<[26]\>[26]{}\anonymous {}\<[41]\>[41]{}\mathrel{=}{}\<[41E]\>[44]{}\Conid{False}{}\<[E]\\blanklineskip]\>[B]{}\Varid{erase}\mathbin{::}\Conid{Type}\to \Conid{Type}{}\<[E]\\
\>[B]{}\Varid{erase}\;(\Conid{Singl}\;\Varid{e}\;\Varid{t}){}\<[20]\>[20]{}\mathrel{=}\Varid{erase}\;\Varid{t}{}\<[E]\\
\>[B]{}\Varid{erase}\;\Varid{t}{}\<[20]\>[20]{}\mathrel{=}\Varid{t}{}\<[E]\\blanklineskip]\>[B]{}\Varid{chkNeTerm}\mathbin{::}\Conid{Ctx}\to \Conid{Type}\to \Conid{Term}\to \Conid{Bool}{}\<[E]\\
\>[B]{}\Varid{chkNeTerm}\;\Varid{ts}\;\Varid{t}\;\Varid{e}\mathrel{=}{}\<[21]\>[21]{}\mathbf{case}\;{}\<[27]\>[27]{}\Varid{maybeEr}\;(\Varid{infType}\;\Varid{ts}\;\Varid{e})\;\mathbf{of}{}\<[E]\\
\>[21]{}\Conid{Just}\;\Varid{t'}{}\<[30]\>[30]{}\to \Varid{t}\equiv \Varid{t'}{}\<[E]\\
\>[21]{}\Conid{Nothing}{}\<[30]\>[30]{}\to \Conid{False}{}\<[E]\ColumnHook
\end{hscode}\resethooks


\subsection*{Inferring the types of neutral terms}\bla\par

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}\column{9}{@{}>{\hspre}l<{\hspost}@{}}\column{13}{@{}>{\hspre}l<{\hspost}@{}}\column{16}{@{}>{\hspre}l<{\hspost}@{}}\column{17}{@{}>{\hspre}l<{\hspost}@{}}\column{19}{@{}>{\hspre}l<{\hspost}@{}}\column{22}{@{}>{\hspre}l<{\hspost}@{}}\column{29}{@{}>{\hspre}l<{\hspost}@{}}\column{30}{@{}>{\hspre}c<{\hspost}@{}}\column{30E}{@{}l@{}}\column{33}{@{}>{\hspre}l<{\hspost}@{}}\column{35}{@{}>{\hspre}l<{\hspost}@{}}\column{37}{@{}>{\hspre}l<{\hspost}@{}}\column{39}{@{}>{\hspre}l<{\hspost}@{}}\column{43}{@{}>{\hspre}l<{\hspost}@{}}\column{47}{@{}>{\hspre}l<{\hspost}@{}}\column{50}{@{}>{\hspre}l<{\hspost}@{}}\column{52}{@{}>{\hspre}l<{\hspost}@{}}\column{53}{@{}>{\hspre}l<{\hspost}@{}}\column{55}{@{}>{\hspre}l<{\hspost}@{}}\column{57}{@{}>{\hspre}l<{\hspost}@{}}\column{E}{@{}>{\hspre}l<{\hspost}@{}}\>[B]{}\Varid{nbeType}\mathbin{::}\Conid{Ctx}\to \Conid{Type}\to \Conid{Maybe}\;\Conid{Type}{}\<[E]\\
\>[B]{}\Varid{nbeType}\;\Varid{ctx}\;\Varid{t}\mathrel{=}\Conid{Just}\;(\Varid{nbeOpenTy}\;\Varid{ctx}\;\Varid{t}){}\<[E]\\blanklineskip]\>[B]{}\Varid{infType}\;\Varid{ts}\;(\Conid{App}\;\Varid{e}\;\Varid{e'}){}\<[30]\>[30]{}\mathrel{=}{}\<[30E]\>[33]{}\mathbf{case}\;\Varid{maybeEr}\;(\Varid{infType}\;\Varid{ts}\;\Varid{e})\;\mathbf{of}{}\<[E]\\
\>[33]{}\hsindent{2}{}\<[35]\>[35]{}\Conid{Just}\;(\Conid{Fun}\;\Varid{t}\;\Varid{t'})\to {}\<[E]\\
\>[35]{}\hsindent{4}{}\<[39]\>[39]{}\mathbf{if}\;\Varid{chkTerm}\;\Varid{ts}\;\Varid{t}\;\Varid{e'}{}\<[E]\\
\>[35]{}\hsindent{4}{}\<[39]\>[39]{}\mathbf{then}\;\Varid{nbeType}\;\Varid{ts}\;(\Varid{sgSub}\;\Varid{t'}\;\Varid{e'}){}\<[E]\\
\>[35]{}\hsindent{4}{}\<[39]\>[39]{}\mathbf{else}\;\Conid{Nothing}{}\<[E]\\
\>[33]{}\hsindent{2}{}\<[35]\>[35]{}\anonymous \to \Conid{Nothing}{}\<[E]\\blanklineskip]\>[B]{}\Varid{infType}\;\anonymous \;\anonymous {}\<[30]\>[30]{}\mathrel{=}{}\<[30E]\>[33]{}\Conid{Nothing}{}\<[E]\\blanklineskip]\>[B]{}\Varid{infCtx}\mathbin{::}\Conid{Ctx}\to \Conid{Subst}\to \Conid{Ctx}{}\<[E]\\
\>[B]{}\Varid{infCtx}\;{}\<[9]\>[9]{}(\Varid{t}\mathbin{:}\Varid{ts})\;{}\<[17]\>[17]{}\Conid{P}{}\<[29]\>[29]{}\mathrel{=}\Varid{ts}{}\<[E]\\
\>[B]{}\Varid{infCtx}\;{}\<[9]\>[9]{}(\Varid{t}\mathbin{:}\Varid{ts})\;{}\<[17]\>[17]{}(\Conid{Comp}\;\Conid{P}\;\Varid{s}){}\<[29]\>[29]{}\mathrel{=}\Varid{infCtx}\;\Varid{ts}\;\Varid{s}{}\<[E]\ColumnHook
\end{hscode}\resethooks



\vfill\eject

\section{Proofs}
\label{sec:proofs}

\newcounter{prfcnt}
\renewcommand{\theprfcnt}{\Alph{section}.\arabic{prfcnt}}

\newcommand{\DisplayProof}[4]{
\refstepcounter{prfcnt}\label{prf:#1}
\proof[\theprfcnt .\ Proof of #2 \ref{#3}.]#4
}

\DisplayProof{famperd}{Lemma}{lem:famperd}{By induction on . We do not show the base cases,
  for they are trivial.
  \begin{enumerate}[(1)]
  \item Let .
    
  \item Let .
    
  \end{enumerate}
}

\DisplayProof{reify}{Lemma}{lem:reify}{By induction on .
  \begin{enumerate}[(a)]
\item Case .
    \begin{enumerate}[(1)]
    \item The partial function  maps neutrals to related elements
      in the corresponding PER.
      
    \item The partial function  maps related elements to related
      normal forms.
      
    \item The function  maps related elements in  to normal forms.
      
    \end{enumerate}
  \item Case .
    \begin{enumerate}[(1)]
    \item The partial function  maps neutrals to related elements
      in the corresponding PER.
      
      \item The partial function  maps related elements to related
      normal forms.
              
    \item The function  maps related elements in  to normal forms.
      
    \end{enumerate}
  \end{enumerate}
}

\DisplayProof{soundness-irr}{Lemma}{lem:soundness-irr}{The proofs of
  soundness for \rulename{prf-} and \rulename{prf-} have
  the same structure, so we show only the first one.
  \begin{enumerate}[\rulename{prf-assoc}]
  \item[\rulename{prf-}] 
    
    
\item[\rulename{prf-assoc}]
  

\end{enumerate}
}

\DisplayProof{logrelEqTy}{Lemma}{lem:logrelEqTy}{By induction on . 
  \begin{enumerate}[(a)]
  \item Types; in all cases we use symmetry and transitivity to show 
    the conditions. We only show the case for .
    \begin{enumerate}[(1)]
    \item : 
      
    \item .
    
    \end{enumerate}
  \item Terms. As in the case for types, we use symmetry and
    transitivity. We show only the case for singletons and functions.
    \begin{enumerate}[(1)]
    \item : 
      
  \item : 
      
    \end{enumerate}
  \end{enumerate}
}

\DisplayProof{logrelMon}{Lemma}{lem:logrelMon}{By induction on .  This property is
    trivial for the base cases; for singletons is obtained by applying
    the i.h. We show two cases.
    \begin{enumerate}
    \item Let .
      
    \item .  As mentioned earlier if
       then
       is non-empty if and
      only if  is not empty.
      
    \end{enumerate}
  We do not show proofs for the second part, since the most involved
  case is dealt analogously to the case for .\qed  
}

\DisplayProof{logrelEqD}{Lemma}{lem:logrelEqD}{By induction on . Note that the first part for the base cases is trivial;
  the second point is also trivial for . Thus we do not
  show those parts of the proof.
  \begin{enumerate}[(a)]
  \item Types.
    \begin{enumerate}[(1)]
    \item . 
      
    \item . 
      
    \end{enumerate}
  \item Terms.
    \begin{enumerate}[(1)]
    \item .
            
  \item .
      
      By i.h. on \eqref{eq:cong-d-eq-f1t} and \eqref{eq:cong-d-eq-f3t}
      and monotonicity \ref{lem:logrelMon}
      
      By i.h. on \eqref{eq:cong-d-eq-f2t}
      
  \item .
    
  \end{enumerate}
\end{enumerate}
}

\DisplayProof{judgeq}{Lemma}{lem:judgeq}{By induction on .
  By induction on . For a better organisation of the proof
  we show the proofs for each point separately.
  \begin{enumerate}[(a)] 
  \item . We skip the
    part for the minimal elements in .
    \begin{enumerate}[(1)] 
    \item :
      
    \item :
      
    \end{enumerate}
  \item . We skip
    the part for the minimal elements in .
    \begin{enumerate}[(1)]
    \item : 
      
      \item :
        
    \end{enumerate}
  \item\hfill
    \begin{enumerate}[(1)]
    \item :
      
    \item : 
      
    \end{enumerate}
  \end{enumerate}
}

\DisplayProof{logrelIdSub}{Lemma}{lem:logrelIdSub}{By induction on
  ; we show only the inductive case.  Let
  .
      
}

\DisplayProof{logrel}{Theorem}{thm:logrel}{
 We note that for
  terms we show only the cases when the last rule used was the
  introductory rule, or the rule for introducing elements in
  singletons; for the case of the conversion rule,
  we can conclude by i.h., and lemma \ref{lem:logrelEqTy}.
  \begin{enumerate}[(a)]
  \item Types. We show only the case for \ruleref{fun-f}.

\item Terms. We show the case for application
      \ruleref{fun-el} and for \ruleref{\enumrule{e}}. The case for abstraction \ruleref{fun-i} is
      analogous to \ruleref{fun-f}.\hfill
    \begin{enumerate}[(1)]
\item \ruleref{fun-el}
      
    \item \ruleref{\enumrule{e}}
      

\end{enumerate}    
  \item Substitutions. Only the proof for \ruleref{ext-subs} is shown.

\end{enumerate}
}



\DisplayProof{corr-tc}{Theorem}{thm:corr-tc}{By simultaneous induction on 
    , , and . 
    \begin{enumerate}[(1)]
    \item Types:
      \begin{enumerate}[]
      \item the case for  is also obtained directly from the
        derivations we get using the i.h.\ on ,
        and ; and use them for deriving
        
      \item for , we can apply the same reasoning as
        before: by i.h.\ on , and
         we know that there are, respectively,
        derivations with conclusions , and
        ; from which we can conclude
        
      \item here we'll consider the three cases when  is a neutral
        term, because the reasoning is the same. By i.h.\ on
        , we have a derivation with
        conclusion ; hence we use
        \ruleref{u-el}.
      \end{enumerate}
    \item Terms:
      \begin{enumerate}[]
      \item let , and . By i.h.
        , and
        , and using both
        derivations we can derive .
      \item consider , and . by i.h.\ on
        , and
        , we have
        , and ,
        and using conversion we derive ; and
        these are the premises we need to show
        .
      \item , and : we have
        . From this we can
        conclude by i.h.\ ; and this is
      the key premise for concluding
      .
    \item : by hypothesis we know
      , and , and
      ; by the i.h. on the second one we
      get ; then we can conclude using
      \ruleref{sing-i}.
    \item , and : let
      , then we distinguish the cases when
       is a singleton, and when  is not a singleton. In the
      latter case, the derivation is obtained directly from the
      correctness of type-inference.  In the first case we use the
      rule \ruleref{sing-el}, with the derivation obtained by i.h.\
      and then we conclude with conversion.
      \end{enumerate}
    \item Inference:
      \begin{enumerate}[]
      \item for , if , then we use \ruleref{hyp},
        and conversion; if , then we have a derivation with
        conclusion , and clearly
        , hence by
        \ruleref{subs-term}, we have
        ,
        we conclude by correctness of  and by conversion.
      \item by i.h. we have derivations with conclusions
        , with , hence we
        have a derivation  (using
        \ruleref{sing-el} if necessary) and ,
        hence by the rule \ruleref{fun-el}, we have
        .
        We conclude by conversion and correctness of .\qed
      \end{enumerate}
    \end{enumerate}
}

\DisplayProof{compl-tc}{Theorem}{thm:compl-tc}{We prove simultaneously
  all the points. The first point is by induction on the structure of
  the type. In the last two points we use well-founded induction on
  the order .
  \begin{enumerate}[(1)]
   \item Types:
     \begin{enumerate}[]
     \item ; by inversion we know
       , and ;
       hence by i.h.\ we have respectively ,
       and .
     \item : by inversion we have
       , and , hence by
       i.h.\ we have both , and
       .
     \item , we have to show
       . By lemma~\ref{lem:invneut}, we know
       ; hence by i.h.\ we have
       , and ,
       hence .
     \end{enumerate}
   \item Terms: We omit the trivial cases, e.g. ; we
     have re-arranged the order of the cases for the sake of clarity.
     \begin{enumerate}[]
     \item : 
       \begin{enumerate}[(a)]
       \item either ,
         , and
         ; hence, by i.h.\ we know
         both , and
         ; hence we can conclude
         .
       \item Or ,
         , and ,
         hence by i.h. we know
         , by conversion we also have
         and transitivity of the equality
         , hence
         .
       \end{enumerate}
     \item : 
       \begin{enumerate}[(a)]
       \item , and
         . From those derivations we have by
         i.h.\ , and
         , respectively; from which 
         we conclude 
       \item , with
         , and ,
         hence by i.h. we know . We can
         also derive , hence
         .
       \end{enumerate}
     \item 
       \begin{enumerate}[(a)]
       \item , and
         ; from this we can conclude
          by ind. hyp. we get
         ; therefore
         .
       \item Or ,
         , and ,
         hence by i.h. we know
         , by conversion we also have
         and transitivity of the equality
         , hence
         .
       \end{enumerate}
     \item : then we do case analysis on .
       \begin{enumerate}[(a)]
       \item If , then by soundness of
         , and conversion we have
         ; and by inversion of
         singletons we have , and also
         . Clearly ,
         hence we can apply the inductive hypothesis and conclude
         ; from that and , we conclude 
         , i.e.,
         .
       \item If , then . We use the last clause for concluding
         ; but we need to show that if
         , then
         ; we show this in
         the next point.
         \end{enumerate}
     \end{enumerate}
   \item Inference: let ,
     , and . Show
     .
     \begin{enumerate}[]
     \item let us consider first the case when ; by
       inversion we have derivations , and
       .  Hence by i.h. we know that
       , and
       .
     \item Now we consider the case when  is not a singleton, and
       ; this case is trivial because by inversion
       we know that
       .
     \item the last case to consider is  and 
       not a singleton. By inversion we know
       , and
       , hence
       , and
       , hence .  By
       i.h.\ we know that if , then
       , and also
       . Hence we can conclude
       . And
        (by correctness of the
        algorithm).\qed
     \end{enumerate}
   \end{enumerate}
}

 \end{document}
