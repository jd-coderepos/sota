

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{amsmath,amssymb} \usepackage{color}


\usepackage{wrapfig}

\usepackage[accsupp]{axessibility}  


\usepackage{hyperref}




\usepackage{amsmath,amssymb,amsbsy,xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}


\makeatother

\let\svthefootnote\thefootnote
\newcommand\freefootnote[1]{\let\thefootnote\relax \footnotetext{#1}\let\thefootnote\svthefootnote }

\renewcommand{\thefootnote}{*}


\DeclareUnicodeCharacter{2192}{}




\begin{document}
\pagestyle{headings}
\mainmatter


\title{Streaming Multiscale Deep Equilibrium Models} 



\titlerunning{StreamDEQ}

\author{Can Ufuk Ertenli\inst{}\orcidID{0000-0001-7795-3617} \and
Emre Akbas\inst{*}\orcidID{0000-0002-3760-6722} \and
Ramazan Gokberk Cinbis\inst{*}\orcidID{0000-0003-0962-7101}}
\authorrunning{U. Ertenli et al.}

\institute{Middle East Technical University (METU), Department of Computer Engineering
\email{\{ufuk.ertenli,eakbas,gcinbis\}@metu.edu.tr} }

\maketitle




\begin{abstract}
We present StreamDEQ, a method that infers frame-wise representations on videos with minimal per-frame computation. In contrast to conventional methods where compute time grows at least linearly with the network depth, we aim to update the representations in a continuous manner. For this purpose, we leverage the recently emerging implicit layer models, which infer the representation of an image by solving a fixed-point problem. Our main insight is to leverage the slowly changing nature of videos and use the previous frame representation as an initial condition on each frame. This scheme effectively recycles the recent inference computations and greatly reduces the needed processing time. Through extensive experimental analysis, we show that StreamDEQ is able to recover near-optimal representations in a few frames time, and maintain an up-to-date representation throughout the video duration. Our experiments on video semantic segmentation and video object detection show that StreamDEQ achieves on par accuracy with the baseline (standard MDEQ) while being more than  faster. Code and additional results are available at \url{https://ufukertenli.github.io/streamdeq/}.


\keywords{Implicit layer models, video analysis and understanding,
video object detection, video semantic segmentation}
\end{abstract}


\footnotetext{Equal contribution for senior authorship.}
\freefootnote{To appear at ECCV 2022.}

\section{Introduction}\label{sec:intro}


Modern convolutional deep networks excel at numerous recognition tasks. It is commonly observed that deeper models tend to outperform their shallower counterparts~\cite{he2016deep,huang2017densely,zagoruyko2016wide}, e.g. the prediction quality tends to increase with network depth using the architectures with residual connections~\cite{he2016deep}. Due to the sequential nature of the layer-wise calculations, however, increasing the network depth results in longer inference times. While the increase in inference duration can be acceptable for various offline recognition problems, it is typically of concern for many streaming video analysis tasks. For example, in perception modules of autonomous systems, it is not only necessary to keep up with the frame rate but also desirable to minimize the computational burden of each recognition component to reduce the hardware requirements and/or save resources for additional tasks. Similar concerns arise in large-scale video analysis tasks, e.g. on video sharing platforms, a small increase in per-frame calculations can add up to great increments in total consumption. 



Various techniques have been proposed to speed up the inference in deep networks. A widely studied idea is to apply a large model to selected {\em key-frames} and then either interpolate its features to the intermediate frames~\cite{zhu2017dff,zhu2018towards} or apply a smaller model to them~\cite{xu2018dynamic,liu2019looking}. However, such approaches come with several potential complications: (i) each time the larger model is applied, the model lags behind, the handling of which demands a complicated system design. (ii) Most methods require optical flow or motion estimates~\cite{zhu2017dff,zhu2018towards}, which brings in an additional estimation problem and an additional point of failure. In addition, the time cost of the flow estimation naturally reduces the time budget for all dependent steps. (iii) Special techniques need to be developed to maintain the compatibility of the representations and/or confidence scores obtained across the key and intermediate frames. (iv) The training schemes tend to be complicated due to the need for training over video mini-batches. It is also noteworthy that several models, e.g.~\cite{kang2017tubelets,wang2018manet}, rely on forward \textbf{and} backward flow estimates, making them less suitable for streaming recognition problems due to non-causal processing.


A related approach is to select a subset of each frame to process. These methods typically aim to identify the most informative regions in the input~\cite{mnih2014ram,ba2015dram,cordonnier2021differentiable}. For static images, the region selection process can continue until the model becomes confident about its predictions. However, when applied to videos, such subset selection strategies share shortcomings similar to approaches relying on flow-based intra-frame prediction approximations. The inputs change over time, therefore, the models have to choose between relying on optical flow to warp the rest of the features or to omit them entirely, which may result in obsolete representations over time~\cite{zhu2018towards}.


In this context, the recently introduced {\em implicit layer models}, pioneered by the work on Deep Equilibrium Model (DEQ)~\cite{bai2019deq} and Multiscale Deep Equilibrium Model (MDEQ)~\cite{bai2020mdeq}, offer a fundamentally different alternative to deep neural networks. DEQ (and MDEQ) shows that by using the fixed-points of a network as the representation, one can gain the representation power of deep models, using a network with only a few layers. The potential of DEQ to eliminate long chains of computations over network layers, therefore, renders it an attractive candidate towards building efficient streaming recognition models. 


However, while DEQ provides a way to learn deep representations using shallow networks, the test-time inference process involves iterative fixed-point estimation algorithms, such as root finding or fixed-point iterations. Since each iteration can be interpreted as an increment in the network depth, DEQ effectively constructs deep networks for inference, and therefore, can still suffer from the run-time costs as in explicit deep networks. 
 
Our main insight is the potential to speed up the inference process, \ie fixed-point estimation, by exploiting the temporal smoothness across neighboring frames in videos.\footnote{We do not refer to a mathematical definition of smoothness, but rather emphasize that the changes between neighboring frames are small.} More specifically, we observe that the fully estimated MDEQ representation of a frame can be used for obtaining the approximate representations of the following frames, using only a few inference iterations. We further develop the idea, and show that even without fully estimating the representation at any time step, the implicit layer representation can be kept up-to-date by running the inference iterations over the iteration steps {\em and} video time steps, in a continuous manner. The final scheme, starting from scratch, accumulates and transfers the extracted information throughout the video duration. We, therefore, refer to the proposed method as Streaming DEQ, or StreamDEQ for short.


The main difference between standard DEQ and StreamDEQ is illustrated in Figure~\ref{fig:intro_figure}. While DEQ typically requires a large number of inference iterations, StreamDEQ enables inference with only a few iterations per frame by leveraging the relevance of the most recent frame's representation. On the start of a new stream, or after a major content change (e.g. a shot change), StreamDEQ quickly adapts to the video in a few frames, much like a person adapting her/his focus and attention when watching a new video. In the following frames, it continuously updates the representation to adapt to minor changes (e.g. objects moving, entering or exiting the scene). 


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/fig_intro_v2.pdf}
\caption{Our method, StreamDEQ, exploits the temporal smoothness between successive frames and extracts features via a small number of solver iterations, starting from the previous frame’s representation as initial solution. StreamDEQ accumulates and transfers the extracted information continuously over successive frames; effectively sharing computations across video frames in a causal manner}
\label{fig:intro_figure}
\end{figure}


Overall, StreamDEQ provides a simple and {\em lean} solution to the streaming recognition with implicit layer models, where a single model naturally performs cost-effective recognition, without relying on external inputs and heuristics, such as optical flow~\cite{horn1981determining}, post-processing methods (Seq-NMS~\cite{han2016seq} or tubelet re-scoring~\cite{kang2016object,kang2017tubelets}). Our method also maintains the causality of the system, and executes in a continuous manner. We also note that it allows dynamic time budgeting; the duration of the inference process can be tuned on-the-fly by a controller, depending on the instantaneous compute system load, which can be a desirable feature in real-world scenarios.


We verify the effectiveness of the proposed method through extensive experiments on video semantic segmentation and video object detection. Our experimental results show that StreamDEQ recovers near-optimal representations at much lower inference costs. More specifically, on the ImageNet-VID video object detection task, StreamDEQ converges to the mAP scores of  and  using only  and  inference iterations per frame, respectively. In comparison, the standard DEQ inference scheme yields only  and  mAP scores using  and  iterations, respectively. Similarly, on the Cityscapes semantic segmentation task, using StreamDEQ instead of the standard DEQ inference scheme improves the converged streaming mIoU score from  to  when  inference iterations are used per frame, and from  to  when  iterations are used per frame. 


\section{Related Work}\label{sec:related_work}


Here, we summarize efficient video processing methods, video object detection and segmentation models. Furthermore, we discuss saliency based techniques for video processing. Finally, we give an overview of implicit models (DEQs).


\subsubsection{Efficient Video Processing and Inference.}


There have been many efforts to improve video processing efficiency to reach real-time processing speeds. Most of these works take a system-oriented approach~\cite{carreira2018massively,narayanan2019pipedream,li2020towards}. For example, Carreira et al.~\cite{carreira2018massively} develop an efficient parallelization scheme over multiple GPUs and process different parts of a model in separate GPUs to improve efficiency while sacrificing accuracy due to frame delays. Narayanan et al.~\cite{narayanan2019pipedream} propose a novel scheduling mechanism that efficiently schedules and divides forward and backward passes over multiple GPUs. In another work, Li et al.~\cite{li2020towards} use a dynamic scheduler in which the model chooses to skip a frame when the delays build up to the point where it would be impossible to calculate the results of the next frame in the allotted time.


We also note that works on low-cost network designs, such as MobileNets~\cite{howard2017mobilenets,sandler2018mobilenetv2} and low-resolution networks~\cite{liu2018mobile,zhao2018deep}, are also relevant. Such efforts are valuable especially for replacing network components with more compute-friendly counterparts. However, the advantages of such techniques can also be limited due to natural trade-offs between speed and performance~\cite{zhu2020review} as the lower-cost network components tend to have lower expressive power. Nevertheless, one can easily incorporate low-cost model design principles into DEQ or StreamDEQ models, thanks to the architecture-agnostic definition of implicit layer models. While such efforts may bring reductions in inference wall-clock time, they are outside the scope of our work.


\subsubsection{Video Semantic Segmentation.}


Semantic segmentation is a costly, spatially dense prediction task. Its application to videos remains relatively limited. To reduce the computational cost, most works rely on exploiting temporal relations between frames using methods such as feature warping~\cite{gadde2017semantic,xu2018dynamic,huang2018efficient,jain2019accel}, feature propagation~\cite{shelhamer2016clocknet,li2018low}, feature aggregation~\cite{hu2020temporally}, and knowledge distillation~\cite{liu2020efficient}. 


Gadde et al.~\cite{gadde2017semantic} propose warping features of the previous frame at different depths based on optical flow. Xu et al.~\cite{xu2018dynamic} evaluate regions of the input frame and decide whether to warp the features with a cheap flow network or use the large segmentation model based on a confidence score. Huang et al.~\cite{huang2018efficient} keep a moving average over time by combining the segmentation maps from the current frame with the warped map from the previous frame. Jain et al.~\cite{jain2019accel} warp high-quality features from the previous key-frame and fuse them with lower quality features calculated on the current frame to make predictions. 


Shelhamer et al.~\cite{shelhamer2016clocknet} propose an adaptive method that schedules updates to the multi-level feature map so that features of layers with smaller changes are carried forward (without any transformation). Li et al.~\cite{li2018low} introduce an adaptive key-frame scheduling method based on the deviation of low-level features compared to the previous key-frame and if the deviation is small, the features are propagated with spatially variant convolution. 


Hu et al.~\cite{hu2020temporally} use a set of shallow networks, each calculating features of consecutive frames starting from scratch. Then, these features are aggregated at the current frame with an attention-based module. Liu et al.~\cite{liu2020efficient} propose to use an expensive network during training including optical flow and applies knowledge distillation on a student network to benefit from the high capacity of a teacher network while cutting computational costs thanks to a smaller and more efficient student network which the authors use during inference. 


In contrast to all these approaches, the proposed StreamDEQ method directly leverages the similarities across video frames, without requiring any ad-hoc video handling strategies, as a way to adapt the implicit layer inference mechanism to efficient streaming video analysis.


\subsubsection{Video Object Detection.}


Most modern video object detection methods exploit temporal information to improve the accuracy and/or efficiency. To this end, optical flow~\cite{zhu2017dff,zhu2017fgfa,kang2017tubelets,wang2018manet}, feature aggregation~\cite{zhu2017fgfa,bertasius2018object,wu2019selsa,chen2020mega} and post-processing techniques~\cite{han2016seq,kang2016object} are prominently used. 


Zhu et al.~\cite{zhu2017dff} use optical flow estimates to warp features on selected key-frames to intermediate frames for increased efficiency. Zhu et al.~\cite{zhu2017fgfa} also propose FGFA that uses optical flow to warp features of nearby frames to the current frame and aggregates these features adaptively based on a weight calculated from feature similarity. Kang et al.~\cite{kang2017tubelets} create links between objects through time (tubelets) from the predictions calculated with optical flow across a video linking objects through time and apply tubelet re-scoring to keep detections of high-confidence. Wang et al.~\cite{wang2018manet} adds an instance level calibration module to FGFA~\cite{zhu2017fgfa} and combines them to generate better predictions. 


Bertasius et al.~\cite{bertasius2018object} sample features from neighboring support frames via deformable convolution that learns object offsets between frames and aggregates these features over these frames. Wu et al.~\cite{wu2019selsa} focus on linking object proposals in a video according to their semantic similarities. Chen et al.~\cite{chen2020mega} propose a model that aggregates local and global information with a long-range memory.


Another common way to improve performance is to apply a post-processing method. For example, Han et al.~\cite{han2016seq} introduce Seq-NMS to exploit temporal consistency by constructing a temporal graph to link objects in adjacent frames. With a similar idea, Kang et al.~\cite{kang2016object} generate tubelets by combining single image detections through the video and use a tracker to re-scored the tubelets as a post-processing to improve temporal consistency. 


\subsubsection{Saliency Based Techniques.}


To reduce computational cost, another viable approach is to select important regions in an image and process only those small patches~\cite{mnih2014ram,ba2015dram,cordonnier2021differentiable,liu2022confidence}. Video extensions of these models also exist~\cite{bazzani2011learning,denil2012learning,zhu2018towards}.


Mnih et al.~\cite{mnih2014ram} and Ba et al.~\cite{ba2015dram} model human eye movements by capturing {\em glimpses} from images with a recurrent structure and process those glimpses at each step. Cordonnier et al.~\cite{cordonnier2021differentiable} propose selecting most important regions to process by first processing a downsampled version of the image. Liu et al.~\cite{liu2022confidence} stops processing for regions with high-confidence predictions at an earlier stage.


Bazzani et al.~\cite{bazzani2011learning} and Denil et al.~\cite{denil2012learning} approach video processing in a human-like manner where the model {\em looks at} a different patch around the objects of interest at each frame and tracks them. Zhu et al.~\cite{zhu2018towards} takes a key-frame based approach. At each key-frame they process the full input and at intermediate frames, they update the feature maps partially based on temporal consistency.


\subsubsection{Implicit Layer Models.}


Implicit layer models have seen a recent surge of interest and have been successful at numerous tasks. DEQs~\cite{bai2019deq} are a recent addition to the implicit model family aimed at solving sequence modeling tasks. DEQs pose a fixed-point solving problem as a root finding problem and utilize an iterative root finding algorithm to find a solution. Multiscale Deep Equilibrium Models (MDEQ)~\cite{bai2020mdeq} are the extension of the base DEQ to image-based models. Bai et al.~\cite{bai2021stabilizing} propose adding a Jacobian regularization term to improve model training.


In addition, Huang et al.~\cite{huang2021impsq} propose re-using the fixed-point across training iterations with the drawback of having to stay in full-batch mode for the training. Also, Bai et al.~\cite{bai2021neural} suggests a new initialization scheme that is realized through a small network. Furthermore, inferring information from the last few iterations reduces the number of solver iterations required for convergence.


\section{Proposed Method}\label{sec:method}


We build our method on the Deep Equilibrium Model (DEQ). In this section, we first give an overview of DEQ and then present the details of our method. 


\subsection{DEQ Overview}


Weight-tied networks are models where some or all layers share the same weights~\cite{bai2018trellis,dehghani2018universal}. A DEQ is essentially a weight-tied network with only one shallow block. DEQ leverages the fact that continuously applying the same layer to its output tends to guide the output to an equilibrium point, \ie a fixed-point. Let  represent the model’s input,  the equilibrium point and  the applied shallow block, then a DEQ can be described as

Since the {\em depth} of processing is obtained through repeatedly applying the same layer(s), these models are also called {\em implicit deep models}. DEQ’s fundamental difference from a standard weight-tied model is that the fixed-point is found by root finding algorithms in both forward and backward passes as in Eq. \ref{eq:root_find}: 

DEQ uses the Broyden's method~\cite{broyden1965class} for this purpose.  The accuracy of the solution depends on the number of Broyden iterations~\cite{bai2021neural,bai2021stabilizing}.  While more iterations yield better accuracy, they increase computation cost. 
DEQs have been successfully adapted to computer vision tasks, too, with the introduction of Multiscale Deep Equilibrium Models (MDEQ)~\cite{bai2020mdeq}. MDEQ is  a multiscale model where each scale is driven to equilibrium together with other scales again by using Broyden's method. Iterations start with  and continue  times to obtain the final solution, .  is set to  for ImageNet classification and  for Cityscapes semantic segmentation in MDEQ~\cite{bai2020mdeq}. 


\subsection{Streaming DEQ}


Let  be a  dimensional tensor representing a video where  is the temporal dimension. We represent the frame at time  with  which is a  tensor. It should be noted that, we primarily target videos with temporal continuity, without too frequent shot changes. But we also study the effects of shot changes in Section~\ref{sec:experiments}. 


To process a video, DEQ can be applied to each video frame  to obtain , the representation of that frame. This amounts to running the Broyden solver for  iterations starting from  for each frame. 


However, we know a priori that transitions between subsequent video frames are typically smooth, \ie . From this observation, we hypothesize that the corresponding fixed-points, \ie representations  and , are likely to be similar. Therefore, the representation of the previous frame can be used effectively as a starting point for inferring the representation of the current frame. To validate this hypothesis, we run an analysis on the ImageNet-VID~\cite{imagenetviddataset} dataset using the ImageNet pretrained MDEQ model. We assume that at each frame , we have access to the reference representation, , of the previous frame. Reference representations are obtained by running the MDEQ model until convergence ( iterations). At each frame, we use the reference representation of the previous frame as the starting point of the solver: 




\begin{wrapfigure}[17]{R}{0.48\textwidth}
    \includegraphics[width=0.48\textwidth]{figures/l2_vs_iter}
  \caption{Squared Euclidean approximation error as a function of inference steps, when the solver is initialized with the reference representation of the preceding frame\label{fig:l2vsiter}}
\end{wrapfigure}




\noindent and run the solver for various but small numbers of iterations, . To analyze the amount of change in representations over time, we use an ImageNet-pretrained model (MDEQ-XL), since ImageNet representations are known to be useful in many transfer learning tasks. In  Figure~\ref{fig:l2vsiter}, we show the squared Euclidean distance between  and   for various  values, when the solver is started with Eq. \ref{eq:model1}. Dashed lines correspond to the squared Euclidean distance between MDEQ-XL’s reference and -iteration based representations.


From the results presented in Figure~\ref{fig:l2vsiter}, we observe that when the solver is initialized with the preceding frame's fixed-point, the inference process quickly converges towards the reference representation.  We also observe that after starting from the reference representation of the previous frame and performing only  iteration on the current frame, the approximate representation is already more similar  to the reference representation than starting from scratch and performing  iterations. 


Next, we examine the case where the inference method is given access to the reference representations only at certain frames. To simulate this case, at each video clip, we compute the reference representation only at the first frame , \ie . In all following ones, we initialize the solver with the estimated representation of the preceding frame and run the solver for  iterations. That is, 
 
We present the results of this scheme for  in the left hand side of Figure~\ref{fig:l2_iter_frame}. We observe that starting with the reference representation on the initial frame is still useful but for longer clips its effect diminishes. Still, this scheme helps us maintain a stable performance even after several frames. For example, starting with the reference representation and then applying  iterations per frame throughout the following  frames yields a representation that is closer to the reference representation of the final frame than the one given by baseline DEQ inference with  solver iterations. This result shows that the -step inference scheme is able to keep up with the changes in the scene by starting from a good initial point. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.495\textwidth]{figures/l2_iter_frame_ref.pdf}
    \hfill
    \includegraphics[width=0.495\textwidth]{figures/l2_iter_frame.pdf} 
\caption{Distance between the reference representations and StreamDEQ estimations for varying number of iterations, when StreamDEQ is initialized with reference representations (on the left) or just zeros (on the right) on the very first frame}
\label{fig:l2_iter_frame}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/fig_method.pdf}
\caption{StreamDEQ applied to a streaming video, performing two iterations per frame. The representation inference process is initialized with zeros in the very first frame (), and with the most recent representation () in the rest of the stream. This scheme effectively recycles all recent computations for time-efficient inference on a new frame, and therefore, allows approximating a long inference chain (\ie a deep network) by a few inference steps (\ie a few layers) throughout the video stream}
\label{fig:model_picture}
\end{figure}


While this scheme can provide efficient inference on novel frames, we would still need the reference representations of the initial frames, or key-frame(s), which would share the same problems with key-frame based video recognition approaches, e.g.~\cite{zhu2017dff,xu2018dynamic,liu2019looking}. To address this problem, we further develop the idea, and hypothesize that we can start from scratch (\ie all zeros), do a limited number of iterations per frame, and pass the representation to the next frame as the starting point. That is, 
 
We present the representation distance results for this final scheme in Figure~\ref{fig:l2_iter_frame} (right). The representation distances to the reference representations stabilize in  frames. Converged distance values (in  frames) are almost the same with those of the previous scheme (Eq.\ref{eq:model2}). Additionally, the initial representations have relatively large distances but these differences get smaller as new frames arrive. We call this final scheme as StreamDEQ. This scheme avoids any heavy processing in any one of the frames, and completely avoids the concept of key-frames. The number of Broyden iterations can be tuned, which allows easy control over the time-vs-accuracy trade-off.  Therefore, the inference iterations can be run as much as the time budget allows. An illustration of the StreamDEQ inference process is given in Figure~\ref{fig:model_picture}.


\section{Experimental Results} \label{sec:experiments}


We evaluate our method on video semantic segmentation and video object detection. In the following, we provide technical details regarding training and inference setups, the datasets used, and present our experimental findings. We use the PyTorch~\cite{pytorch} framework for all experiments. 


\subsection{Video Semantic Segmentation}


\subsubsection{Experimental Setup.}


We use the Cityscapes semantic segmentation dataset~\cite{cityscapesdataset}, which consists of K finely annotated and K coarsely annotated images. These finely annotated images are divided into train, val and test set, each containing , , and  images, respectively. They correspond to frames extracted from video clips where each annotated image is the th frame of its respective clip. To evaluate over videos, we use these clips up to the th frame, which has fine annotations, and perform the evaluation on that frame.


We use the pretrained MDEQ-XL segmentation model from the MDEQ paper~\cite{bai2020mdeq} and do not perform any additional training. We also do not make any changes to its evaluation setup or hyperparameters, perform the evaluation on Cityscapes \texttt{val} and report {\em mean intersection over union} (mIoU) results. For further details, we refer the reader to MDEQ~\cite{bai2020mdeq}.


\subsubsection{Results.}


We present the results of StreamDEQ for two scenarios. The first scenario corresponds to Eq. \ref{eq:model2}, where we use the reference representations of the first frame to initialize the solver, and apply StreamDEQ then on. Results of this experiment in Figure~\ref{fig:seg_f_iou_iter} (left) show that as the offset of the evaluated frame increases, mIoU starts decreasing, which is expected because the further we move away from the first frame the more irrelevant its representation will become. However, mIoU then stabilizes at a value that is proportional to the number of Broyden iterations (the more iterations, the better the mIoU). This shows that StreamDEQ is able to extract better features over time. StreamDEQ’s performance with  iterations is still comparable with the baseline (MDEQ) with  iterations.


The second scenario corresponds to our final StreamDEQ proposal (\ie Eq. \ref{eq:model3}), where we initialize the solver from scratch, \ie with all zeros, and apply StreamDEQ. Results of this case are shown in Figure~\ref{fig:seg_f_iou_iter} (right). As the videos progress, it might be expected that the Broyden solver cannot keep up with the changing scenes. However, we observe that even after  frames, the accuracy does not drop. Additionally, the impact of this method is clearer for lower numbers of iterations. For example, performing  iteration on every frame without our method would only yield an mIoU score of . However, StreamDEQ obtains a mIoU score of  in  frames. This is an improvement of over . For  iterations, StreamDEQ is able to obtain  mIoU in  frames whereas the non-streaming baseline achieves  only  mIoU. Moreover, the converged mIoU values (at larger frame offsets) are similar in Figure \ref{fig:seg_f_iou_iter} (left) and Figure \ref{fig:seg_f_iou_iter} (right). Therefore, we conclude that the initial point where we start the solver becomes less important as the video streams and the performance stabilizes at some value higher than the non-streaming case.


We also illustrate these results qualitatively in Figure~\ref{fig:seg_qual}. For  iteration, while the baseline cannot produce any meaningful segmentation, StreamDEQ starts capturing many segments correctly at the  frame. With  iterations, while the DEQ baseline still produces poor results, StreamDEQ starts to yield accurate predictions in early frames compared to the single iteration case. With  iterations, while both models provide rough but relevant predictions in the first frame, StreamDEQ predictions start to become clearly more accurate in the following frames; for example, tree trunks and the sky become visible only when StreamDEQ is applied.


We examine the effects of increasing the number of iterations on inference speed in Table~\ref{tab:inference_speed}. We note that our method does not introduce any computation overhead other than the time it takes to store the fixed-point representation of the previous frame. Therefore, we observe a linear increase in compute times as the number of iterations increases. StreamDEQ with  iterations achieves an mIoU score of  at  ms per image. MDEQ with  iterations can only achieve  mIoU. Additional inference results can be found in the videos provided on our project page.


\begin{figure}[t]
    \centering
    \includegraphics[width=.495\linewidth]{figures/IoU_iter_frame_ref.pdf}
    \hfill
    \includegraphics[width=.495\linewidth]{figures/IoU_iter_frame.pdf} 
\caption{StreamDEQ semantic segmentation results (in mIoU) on the Cityscapes dataset as a function of solver iterations when the first frame representation is initialized with the reference representation (left) versus zeros (right)}
\label{fig:seg_f_iou_iter}
\end{figure}


\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\begin{center}
\caption{Inference time comparisons of the proposed method with differing number of iterations on Cityscapes and ImageNet-VID datasets}
\label{tab:inference_speed}
\begin{tabular}{cccccc}
\hline\noalign{\smallskip}
 & & \multicolumn{2}{c}{Cityscapes} & \multicolumn{2}{c}{ImageNet-VID} \\
\hline\noalign{\smallskip}
Model & \# iterations & mIoU & FPS & mAP@50 & FPS \\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
StreamDEQ & 1 & 45.5 & 4.3 & 9.1 & 10.3 \\
StreamDEQ & 2 & 57.9 & 2.9 & 39.5 & 9.2 \\
StreamDEQ & 4 & 71.5 & 1.9 & 50.4 & 6.2 \\
StreamDEQ & 8 & 78.2 & 1.1 & 54.8 & 3.5 \\
MDEQ (Baseline) & 27/26 & 79.7 & 0.3 & 55.0 & 1.2 \\
\hline
\end{tabular}
\end{center}
\end{table}
\setlength{\tabcolsep}{1.4pt}


\begin{figure}[t]
\centering
\includegraphics[width=0.78\linewidth]{figures/seg_qual_new_200.png}
\caption{Qualitative comparison of the baseline with StreamDEQ with different numbers of iterations on the Cityscapes dataset}
\label{fig:seg_qual}
\end{figure}


\subsubsection{Effect of Shot Changes.}


We also study the effects of shot changes for the video semantic segmentation task. For this purpose, we initialize the solver with the reference representations from a random frame from the Cityscapes or ImageNet-VID datasets and run StreamDEQ starting from the representations of that frame. Results of ImageNet-VID to Cityscapes shot change experiments can be found in Figure~\ref{fig:seg_shot_change}. In this case, even though at first, the obtained scores are lower, following a similar trajectory to the ones in Figure~\ref{fig:seg_f_iou_iter} (right), mIoU scores stabilize to a value close to our original experiment. We conclude that, even with occasional shot changes, our method is able to adapt to the new scene in a few frames. The Cityscapes-to-Cityscapes shot change experiments, with similar results, are provided in the supplementary material.


\begin{figure*}[t]
  \begin{minipage}[t]{0.47\linewidth}
    \includegraphics[width=\linewidth]{figures/IoU_iter_frame_shot_change.pdf}
    \caption{mIoU results of StreamDEQ with shot changes from ImageNet-VID}
    \label{fig:seg_shot_change}
  \end{minipage}\hfill \begin{minipage}[t]{0.47\linewidth}
    \includegraphics[width=\linewidth]{figures/map_iter_frame.pdf}
    \caption{mAP@50 results of StreamDEQ for various number of iterations after initialization with zeros from the beginning of a clip on the ImageNet-VID dataset}
    \label{fig:map_iter_frame}
  \end{minipage}
\end{figure*}


\subsection{Video Object Detection}


\subsubsection{Experimental Setup.}


For the video object detection task, we evaluate our method on the ImageNet-VID dataset~\cite{imagenetviddataset} utilizing the MMDetection~\cite{mmdetection} and MMTracking~\cite{mmtracking} frameworks. ImageNet-VID dataset consists of  training and  validation videos from  classes that are a subset of the  classes of the ImageNet-DET dataset. The frames and annotations for each video are available at a rate of - FPS per video. Note that the ImageNet-DET dataset consists only of images rather than videos. We follow the widely used protocol~\cite{zhu2017fgfa,wang2018manet,deng2019relation,wu2019selsa,chen2020mega} and train our model on the combination of ImageNet-VID and ImageNet-DET datasets using the  overlapping classes. We use a mini-batch size of , distributed to  NVIDIA A100 GPUs. We resize each image to have a shorter side of  pixels and train the model for a total of  epochs in  stages. We initialize the learning rate to  and reduce it by a factor of  at epochs  and . We test the model on ImageNet-VID \texttt{val} and report mAP@50 scores following the common practice.


We adopt Faster R-CNN~\cite{ren2015faster} by replacing its ResNet backbone with the MDEQ-XL model. To incorporate multi-level representations, we also use a Feature Pyramid Network (FPN)~\cite{lin2017feature} module after MDEQ-XL. Without any additional modifications, we directly use the model while keeping the model hyperparameters and remaining architecture details same  as in Faster R-CNN models for ResNet backbones. Exceptionally, we only modify the number of channels for the FPN module to match that of MDEQ-XL. We start training with the ImageNet pretrained MDEQ-XL model from MDEQ~\cite{bai2020mdeq}. During training, we use  solver iterations for both forward and backward passes of the MDEQ, following the ImageNet classification experiments in MDEQ~\cite{bai2020mdeq}. Unlike the video object detection models~\cite{zhu2017fgfa,deng2019relation,chen2020mega}, we train our model in the causal single-frame setting, meaning we do not use any temporal information for improved training. Furthermore, we also incorporate the Jacobian regularization loss for MDEQs introduced by Bai et al.~\cite{bai2021stabilizing} to stabilize training.


\subsubsection{Results.}


To the best of our knowledge, this is the first time that an implicit model (MDEQ) is used for an object detection task. We achieve 55.0 mAP@50 on ImageNet-VID \texttt{val}. We are aware that Faster R-CNN with ResNet-18 backbone yields 64.9 mAP@50, however, Faster R-CNN is highly optimized to perform well with ResNet backbones. Yet, we use this same setting with an MDEQ without any parameter optimization, as our focus is not on constructing a  MDEQ-based state-of-the-art video object detector. We believe that there is room for improvement in detector design and tuning details, which we leave for future work. 


Similar to the video segmentation task, we run StreamDEQ with different numbers of iterations. We present the results of this experiment in Figure~\ref{fig:map_iter_frame}. We observe the same trends with the segmentation task. Over time, detection performance increases and stabilizes at a value proportional to the number of Broyden iterations. The  and  iteration cases do not produce any detection results in the non-streaming mode, but if we perform  iterations with StreamDEQ, we improve the performance from  to  mAP@50 in  frames. 


In addition, we also compare the inference speed of StreamDEQ with our baseline for different number of iterations in Table~\ref{tab:inference_speed}. In the  iteration case, we obtain a score of  with StreamDEQ which is only  lower than our baseline model, while being almost  times faster.


\section{Conclusions \& Future Work}


In this paper, we proposed StreamDEQ, an efficient streaming video application of the multiscale implicit deep model, MDEQ. We showed that our model can start from scratch (\ie all zeros) and efficiently update its representations to reach near-optimal representations as the video streams. We validated this claim on video semantic segmentation and video object detection tasks with thorough experiments. StreamDEQ presents a viable approach for both  real-time video analysis and off-line large-scale methods. StreamDEQ is not specific to segmentation or object detection, and can be used as a drop-in replacement for most other structured prediction problems on streaming videos as long as the prediction task is performed by an implicit model. In addition, application to time series prediction and event camera based recognition tasks can be interesting future work directions.


\subsubsection{Acknowledgments.}


The numerical calculations were partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA) and METU Robotics and AI Technologies Research Center (ROMER) resources. Dr. Cinbis is supported by a Google Faculty Research Award. Dr. Akbas is supported by the BAGEP Award of the Science Academy, Turkey.




\par\vfill\par


\clearpage
\bibliographystyle{splncs04}
\bibliography{arxiv}


\end{document}
