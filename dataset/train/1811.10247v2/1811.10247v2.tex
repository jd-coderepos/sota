\section{Experiment}
\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{figs/loc_error_curve}
	\caption{3D Localization Errors in the horizontal, vertical and depth dimensions according to the distances between objects and camera centers.}
	\label{fig:localization3d}
\end{figure}
\begin{table*}[t]   
	\centering
	\setlength{\tabcolsep}{2.1mm}{  
		\begin{spacing}{1.5}
			\scalebox{0.68}{
				\begin{tabular}{|p{2cm}<{\centering}|p{1cm}<{\centering}|c||ccc||ccc||ccc|}
					\hline
					\multirow{2}{*}{Method} & \multirow{2}{*}{Type} & \multirow{2}{*}{Time (s)}   & \multicolumn{3}{c||}{AP\textsubscript{3D} / AP\textsubscript{BEV} (IoU=0.3)} & \multicolumn{3}{c||}{AP\textsubscript{3D} / AP\textsubscript{BEV} (IoU=0.5)} & \multicolumn{3}{c|}{AP\textsubscript{3D} / AP\textsubscript{BEV} (IoU=0.7)}                  \\ \cline{4-12} 
					&                       &                       & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard              & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard  & \multicolumn{1}{c|}{Easy} & \multicolumn{1}{c|}{Moderate} & Hard           \\ \hline \hline
					3DOP                    & Stereo                &4.2  &69.79 / 71.41 &52.22 / 57.78 &\textbf{49.64} / \textbf{51.91} & 46.04 / \textbf{55.04}                     & 34.63 / \textbf{41.25}                & 30.09 / \textbf{34.55} & 6.55 / 12.63                     & 5.07 / 9.49                         & 4.10 / 7.59          \\      
					\hline       
					Mono3D                  & Mono                  & 3  &28.29 / 32.76 &23.21 / 25.15  &19.49 / 23.65 & 25.19 / 30.50                      & 18.20 / 22.39                        & 15.22 / 19.16         & 2.53 / 5.22                     & 2.31 / 5.19                         & 2.31 / 4.13          \\ 
					
					MF3D                    & Mono                  & 0.12 &/ &/ &/              & 47.88 / 55.02            & 29.48 / 36.73                        & 26.44 / 31.27         & 10.53 / 22.03                     & 5.69 / 13.63                         & 5.39 / 11.60          \\ 
					Ours                    & Mono                  & \textbf{0.06}  &\textbf{72.17} / \textbf{73.10} &\textbf{59.57} / \textbf{60.66} &46.08 / 46.86             &\textbf{50.51} / 54.21                   & \textbf{36.97} / 39.69                         & \textbf{30.82} / 33.06         & \textbf{13.88} / \textbf{24.97}           & \textbf{10.19} / \textbf{19.44}               & \textbf{7.62} / \textbf{16.30} \\ \hline
					
				\end{tabular}  
			} 
	\end{spacing}}
	\caption{\textbf{3D Detection Performance.} Average Precision of 3D bounding boxes on the same KITTI validation set and the inference time per image. Note that the stereo-based method 3DOP is not compared but listed for reference.
	}
	\label{tab:3dap} 
\end{table*}
We evaluate the proposed network on the challenging KITTI dataset~\cite{geiger2012kitti}, which contains 7481 training images and 7518 testing images with calibrated camera parameters. Detection is evaluated in three regimes: easy, moderate and hard, according to the occlusion and truncation levels. 
We compare our method with the state-of-art monocular 3D detectors, MF3D ~\cite{xu2018multifusion} and Mono3D~\cite{chen2016monocular}. We also present the results of a stereo-based 3D detector 3DOP~\cite{chen20153dop} for reference. For a fair comparison, we use the train1/val1 split following the setup in~\cite{chen2016monocular,chen2017multiview}, where each set contains half of the images.


\paragraph{Metrics.} 
For evaluating 3D localization performance, we use the mean errors between the central location of predicted 3D bounding boxes and their nearest ground truths.
For 3D detection performance, we follow the official settings of KITTI benchmark to evaluate the 3D Average Precision (AP\textsubscript{3D}) at different Intersection of Union (IoU) thresholds.  

\paragraph{3D Localization Estimation.}
We evaluate the three dimensional location errors (horizontal, vertical and depth) according to the distances between the targeting objects and the camera centers. The distances are divided into intervals of 10 meters. The errors are calculated as the mean differences between the predicted 3D locations and their nearest ground truths in meters. Results are presented in \fig{\ref{fig:localization3d}}. The errors, especially for in the depth dimension, increase as the distances grow because far objects presenting small scales are more difficult to learn.

The results indicate that our approach (red curve) outperforms Mono3D by a significant margin, and is also superior to 3DOP, which requires stereo images as input. Another finding is that, in general, our model is less sensitive to the distances. When the targets are 30 meters or farther away from the camera, our performance is the most stable, indicating that our network deals with far objects(containing small image regions) best.

Interestingly, horizontal and vertical errors are an order of magnitude smaller than that of depth, \ie, the depth error dominants the overall localization error. This is reasonable because the depth dimension is not directly observed in the 2D image but is reasoned from geometric features. The proposed IDE module performs superior to the others for the easy and moderate regimes and comparable to the stereo-based method for the hard regime. 






\paragraph{3D Object Detection.}
3D detection is evaluated using the AP\textsubscript{3D} at 0.3, 0.5 and 0.7 3D IoU thresholds for the car class. We compare the performance with two monocular methods, Mono3D and MF3D. The results are reported in \tab{\ref{tab:3dap}}. Since the authors of MF3D have not published their validation results, we only report the AP\textsubscript{3D} at 0.5 and 0.7 presented in their paper. Experiments show that our method outperforms the state-of-art monocular detectors mostly and is comparable to the stereo-based method.

Our network is designed for efficient applications and a fast 2D detector with no region proposal is adopted. The inference time achieves about 0.06 seconds per image on a Geforce GTX Titan X, which is much less than the other three methods. On the other hand, this design at some degree sacrifices the accuracy of 2D detection. Our 2D AP of the moderate regime at 0.7 IoU threshold is 78.14\%, about 10\% lower than the region proposal based methods that generate a large number of object proposals to recall as many groundtruth as possible. Despite using a relatively weak 2D detector, our 3D detection achieves the state-of-the-art performance, resorting to our IDE and 3D localization module. Note that the 2D detection is a replaceable submodule in our network, and is not our main contribution. 





\begin{figure*}[t] 
	\centering
	\scriptsize
	\begin{tabular}{c@{\hspace{0.14cm}}c@{\hspace{0.14cm}}c}
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007062_proj} &
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007011_proj} &
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007014_proj} \\
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007062_scan_with_truth} &
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007011_scan_with_truth} &
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007014_scan_with_truth} \\
		(a)  & (b)  & (c) \\
		
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007033_proj} &
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007065_proj} &
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007025_proj} \\
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007033_scan_with_truth} &
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007065_scan_with_truth} &
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007025_scan_with_truth} \\
		(d)  & (e)  & (f) \\
		
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007046_proj} &
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007069_proj} &
		\includegraphics[width=0.32\linewidth]{figs/visualize_all/007273_proj} \\
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007046_scan_with_truth} &
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007069_scan_with_truth} &
		\includegraphics[width=0.32\linewidth,trim={0cm 6cm 0cm 1cm},clip]{figs/visualize_all/007273_scan_with_truth} \\
		(g)  & (h)  & (i)
		
	\end{tabular}
	\caption{\textbf{Qualitative Results.} Predicted 3D bounding boxes are drawn in orange, while ground truths are in blue. Lidar point clouds are plotted for reference but not used in our method. Camera centers are at the bottom-left corner. (a), (b) and (c) are common cases when predictions recall the ground truths, while (d), (e) and (f) demonstrate the capability of our model handling truncated objects outside the image. (g), (h) and (i) show the failed detections when some cars are heavily occluded.}
	\label{fig:visualize3d}
\end{figure*}

    

\paragraph{Local 3D Bounding Box Regression.}
We evaluate the regression of local 3D bounding boxes with the size (height, width, length) and orientation metrics.
The height, width, length of a 3D bounding box can be easily calculated from its eight corners. The orientation is measured by the azimuth angles in the camera coordinate frame. We present the mean errors in \tab{\ref{tab:3dparams}}. Our network demonstrates a better capability to learn the size and orientation of a 3D bounding box from merely photometric features. 
It is worth noting that in our local corner regression module, after RoiAlign layers, all the objects of interest are rescaled to the same size to introduce scale-invariance, yet the network still manages to learn their real 3D sizes. 
This is because our network explores the image features that convey projective geometries and semantic information including types of objects (\eg, SUVs are generally larger than cars) to facilitate the size and orientation estimation.


\paragraph{Qualitative Results.}
 Qualitative visualization is provided for three typical situations, shown in \fig{\ref{fig:visualize3d}}. In common street scenes, our predictions are able to successfully recall the targets. It can be observed that even though the vehicles are heavily truncated by image boundaries, our network still outputs precise 3D bounding boxes. Robustness to such a corner case is important in the scenario of autonomous driving to avoid collision with lateral objects. For cases where some vehicles are heavily occluded by others, \ie, in (g), (h) and (i), our 3D detector can handle those visible vehicles but fails in detecting invisible ones. In fact, this is a general limitation of perception from monocular RGB images, which can be solved by incorporating 3D data or multi-view data to obtain informative 3D geometric details.

\begin{table}[]
		\setlength{\tabcolsep}{3mm}{
		\begin{spacing}{1.3}
\scalebox{0.88}{
	\begin{tabular}{|cccc|c|}
		\hline
		\multicolumn{1}{|c||}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{Size (m)}                                                                          & \multirow{2}{*}{Orientation (rad)} \\ \cline{2-4}
		\multicolumn{1}{|c||}{}                        & Height                           & Width                            & Length                           &                                    \\ \hline \hline
		\multicolumn{1}{|c||}{3DOP}                  & 0.107                           & 0.094                           & 0.504                           & 0.580                            \\
		\multicolumn{1}{|c||}{Mono3D}                    & 0.172                          & 0.103                          & 0.582                          &  0.558                            \\
\multicolumn{1}{|c||}{Ours}                    &\textbf{0.084}  &\textbf{0.084}  &\textbf{0.412}  & \textbf{0.251}   \\ \hline
	\end{tabular}
}
\end{spacing}}
\caption{{3D Bounding Box Parameters Error.}} 
\label{tab:3dparams}
\end{table}




\subsection{Ablation Study}
A crucial step for localizing 3D center  is estimating its 2D projection , since  is analytically related to . Although the 2D bounding box's center  can be close to , as is illustrated in \fig{\ref{fig:notation}} (a), it does not have such 3D significance. When we replace  with  in 3D reasoning, the horizontal location error rises from 0.27m to 0.35m, while the vertical error increases from 0.09m to 0.69m. Moreover, when an object is truncated by the image boundaries, its projection  can be outside the image, while  is always inside. In this case, using  for 3D localization can result in a severe discrepancy. Therefore, our subnetwork for locating the projected 3D center is indispensable.



In order to examine the effect of coordinate transformation before local corner regression, we directly regress the corners offset in camera coordinates without rotating the axes. It shows that the average orientation error increases from 0.251 to 0.442 radians, while the height, width and length errors of the 3D bounding box almost remain the same. This phenomenon corresponds to our analysis that switching to object coordinates can reduce the rotation ambiguity caused by projection, and thus enables more accurate 3D bounding box estimation.

