\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{float}
\usepackage{color}
\usepackage{pifont}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\newlength\savedwidth
\newcommand\whline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 0.8pt}\hline\noalign{\global\arrayrulewidth\savedwidth}}

\newcommand{\enze}[1]{\textcolor{red}{[enze: #1]}}
\newcommand{\fdp}[1]{\textcolor{red}{[DP: #1]}}
\newcommand{\implus}[1]{\textcolor{red}{[implus: #1]}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\definecolor{mygray}{gray}{.92}
	
\def\ie{\emph{i.e.}}
\def\eg{\emph{e.g.}}
\def\etc{\emph{etc}}
\def\etal{{\em et al.~}}

\begin{document}

\title{Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions}

\author{
    Wenhai Wang, 
    Enze Xie,
    Xiang Li,
    Deng-Ping Fan,\\
    Kaitao Song,
    Ding Liang,
    Tong Lu,
    Ping Luo,
    Ling Shao\\
    Nanjing University~~~
    The University of Hong Kong~~~\\
    Nanjing University of Science and Technology~~~
    IIAI~~~
    SenseTime Research\\
    ,
    
}

\ificcvfinal\thispagestyle{empty}\fi


\twocolumn[{\maketitle
\vspace{-10mm}
\begin{figure}[H]
\hsize=\textwidth
\centering
\hspace{-0.2in}
\begin{subfigure}{0.30\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figure/p1.pdf}
    \caption{CNNs: VGG~\cite{simonyan2014very}, ResNet~\cite{he2016deep}, \etc.}
    \label{fig:1a}	
\end{subfigure}    
\hspace{0.2in}
\begin{subfigure}{0.30\textwidth}
     \centering
     \includegraphics[width=1\textwidth]{figure/p2.pdf}
     \caption{Vision Transformer~\cite{dosovitskiy2020image}}
     \label{fig:1b}
\end{subfigure}
\hspace{0.2in}
\begin{subfigure}{0.30\textwidth}
     \centering
     \includegraphics[width=1\textwidth]{figure/p3.pdf}
     \caption{Pyramid Vision Transformer (ours)}
     \label{fig:1d}
\end{subfigure}
\vspace{-3mm}
\caption{\textbf{Comparisons of different architectures,} where ``Conv'', and ``TF-E'', represent convolution, and Transformer Encoder, respectively. 
(a) shows that many CNN backbones use pyramid structure for dense prediction tasks such as object detection (DET), semantic and instance segmentation (SEG).
(b) shows that the recently proposed Vision Transformer (ViT)~\cite{dosovitskiy2020image} is a ``columnar'' structure specially-designed for image classification (CLS). (c) illustrates that by incorporating the pyramid structure from CNNs, we present Pyramid Vision Transformer (PVT), which can be used as a versatile backbone for many computer vision tasks, broadening the scope and impact of ViT.
Moreover, our experiments also show that PVT can be easily combined with DETR~\cite{carion2020end} to build an end-to-end object detection system without convolutions and hand-crafted components such as dense anchors and non-maximum suppression (NMS).
}
\label{fig:pipeline}
\end{figure}
\vspace{2mm}
}]



\begin{abstract}
Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions.
Unlike the recently-proposed Transformer model (\eg, ViT) that is specially designed for image classification, we propose  Pyramid Vision Transformer~(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts.
(1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution,
which is important for dense prediction, but also using a progressive shrinking pyramid  to reduce computations of large feature maps.
(2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. 
(3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, \eg~object detection, semantic and instance segmentation.
For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP (see Figure \ref{fig:cmp}).
We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches.
Code is available at {\footnotesize{\url{https://github.com/whai362/PVT}}}.

\end{abstract}

\section{Introduction}
Convolution Neural Network (CNN) achieves remarkable successes in  computer vision, and becomes a versatile and dominating method in almost all tasks of computer vision~\cite{deng2009imagenet,lin2014microsoft,everingham2010pascal,zhou2017scene,simonyan2014very,he2016deep,xie2017aggregated,ren2015faster,he2017mask,lin2017focal,chen2018encoder,kirillov2019panoptic}. 
Nevertheless, this work is trying to explore new versatile  backbone network without convolutions. We investigate an alternative model beyond CNN for the tasks of dense predictions such as object detection, semantic and instance segmentation, other than image classification.

Inspired by the success of Transformer~\cite{vaswani2017attention} in natural language processing (NLP), many researchers are trying to explore  applications of Transformer in computer vision.
For example, some works~\cite{carion2020end,zhu2020deformable,xie2021segmenting,sun2020transtrack,hu2021Transformer} model the vision task as a dictionary lookup problem with learnable queries, and use the Transformer decoder as a task-specific head on the top of the CNN backbone, such as VGG~\cite{simonyan2014very} and ResNet~\cite{he2016deep}.
While some prior arts have incorporated  the attention modules~\cite{wang2018non,ramachandran2019stand,zhao2020exploring} into CNNs, 
as far as we know, exploring a clean and convolution-free Transformer backbone to address dense prediction tasks in computer vision is rarely studied.



Recently, Dosovitskiy \etal\cite{dosovitskiy2020image} employs  Transformer for image classification.
This is an interesting and meaningful attempt to replace the CNN backbone by a convolution-free model. 
As shown in Figure~\ref{fig:pipeline} (b), ViT has a columnar structure with coarse image patches (\ie, dividing image with a large patch size)
as input\footnote{Due to resource constraints, ViT cannot use fine-grained image patches (\eg,  per patch) as input, and can only receive coarse patches (\eg,  per patch) as input, which leads to its low output resolution (\eg, 32-stride).}.
Although ViT is applicable to image classification, \textbf{it is challenging to be directly 
adapted to pixel-level dense predictions}, \eg, object detection and segmentation, because (1) its output feature map has only a single scale with low resolution and (2) its computations and memory cost are relatively high even for common input image size (\eg, shorter edge of 800 pixels in COCO detection benchmark). 


\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{figure/cmp.pdf}
	\hspace{-42.5mm}\resizebox{.48\columnwidth}{!}{\tablestyle{2pt}{1}
	\begin{tabular}[b]{r|c|c}
Backbone & \#Param (M) & AP \\
	\whline
	R18~\cite{he2016deep} & 21.3 & 31.8 \\
	\rowcolor{mygray}
	PVT-T (ours) & 23.0 & 36.7 \\
	\hline
	R50~\cite{he2016deep} & 37.7 & 36.3 \\
	\rowcolor{mygray}
	PVT-S (ours) & 34.2 & 40.4 \\
	\hline
	R101~\cite{he2016deep} & 56.7 & 38.5 \\
	X101-32x4d~\cite{xie2017aggregated} & 56.4 & 39.9 \\
	ViT-S/32~\cite{dosovitskiy2020image} & 60.8  & 31.7 \\
	\rowcolor{mygray}
	PVT-M (ours) & 53.9 & 41.9 \\
	\hline
	X101-64x4d~\cite{xie2017aggregated} & 95.5 & 41.0 \\
	\rowcolor{mygray}
	PVT-L (ours) & 71.1 & 42.6 \\
\multicolumn{3}{l}{
			\vspace{8.3mm}}\\
\end{tabular} 	}
\caption{\textbf{Performance comparison on COCO  of different backbones using RetinaNet for object detection}. We see that when the number of parameters among different models are comparable, the PVT variants significantly outperform their corresponding counterparts in COCO such as ResNets (R)~\cite{he2016deep}, ViT \cite{dosovitskiy2020image} and ResNeXts (X) \cite{xie2017aggregated}.
    For instance, with comparable numbers of parameters, PVT-S outperforms R50 by 4.1 AP (40.4 \vs 36.3), where ``T'', ``S'', ``M'' and ``L'' denote our PVT models with tiny, small, medium and large size. 
    We also see that the original ViT models could be limited in object detection.
}
\label{fig:cmp}
\end{figure}


To compensate the above limitations, this work proposes a convolution-free backbone network using Transformer model,  termed Pyramid Vision Transformer (PVT), which can serve as a versatile backbone in many 
downstream tasks, including image-level prediction as well as  pixel-level dense predictions.
Specifically, as illustrated in Figure~\ref{fig:pipeline} (c), 
different from ViT, PVT overcomes the difficulties of conventional Transformer by (1) taking fine-grained image patches (\ie,  per patch) as input to learn high-resolution representation, which is essential for dense prediction tasks, 
(2) introducing a progressive shrinking pyramid to reduce the sequence length of Transformer when the depth of network is increased, significantly reducing the computational consumption, and (3) adopting a spatial-reduction attention (SRA) layer to further reduce the resource cost to learn high-resolution feature maps.




Overall, the proposed PVT possesses the following merits.
Firstly, compared to the conventional CNN backbones (see Figure~\ref{fig:pipeline} (a)) where the receptive field increases when the depth increases, PVT always produces a global receptive field (by attentions among all small patches), which is more suitable for detection and segmentation than CNNs' local receptive field.
Secondly, compared to ViT (see Figure~\ref{fig:pipeline} (b)), due to the advance of the pyramid structure, our method is easier to be plugged into many representative dense prediction pipelines, \eg, RetinaNet~\cite{lin2017focal} and Mask R-CNN.
Thirdly, with PVT, we can build a convolution-free pipeline by combining PVT with other Transformer decoders designed for different tasks, such as PVT+DETR~\cite{carion2020end} for object detection.
For example, to the best of our knowledge, \emph{our experiments present the first end-to-end object detection pipeline, PVT+DETR}, which is entirely convolution-free. It achieves 34.7 on the COCO , outperforming the original DETR based on ResNet50.

The main \textbf{contributions} of this work are listed below.
\begin{itemize}
    \item We propose Pyramid Vision Transformer (PVT), which is the first backbone designed for various pixel-level dense prediction tasks without convolutions. Combining PVT and DETR, we can build an end-to-end object detection system without convolutions and hand-crafted components such as dense anchors and non-maximum suppression (NMS).
    
    \item We overcome many difficulties when porting Transformer to dense pixel-level predictions, by designing 
progressive shrinking pyramid and spatial-reduction attention (SRA), which are able to reduce the resource consumption of using Transformer, making PVT flexible to learn multi-scale and high-resolution feature maps.
    
    \item We verify PVT by applying it to many different tasks, \eg,  image classification, object detection, and semantic segmentation, and compare it with the well-designed ResNets~\cite{he2016deep} and ResNeXts~\cite{xie2017aggregated}. As shown in Figure \ref{fig:cmp}, we see that PVT with different numbers of parameters can consistently improve performance compared to prior arts.
    For example, RetinaNet+PVT-Small achieves  40.4 AP on COCO , outperforming RetinaNet+ResNet50 by 4.1 AP (40.4 \vs 36.3). 
    Moreover, RetinaNet+PVT-Large achieves 42.6 AP, which is 1.6 better than RetinaNet+ResNeXt101-64x4d, as well as  reducing its number of parameters by 30\%.
\end{itemize}




\section{Related Work}

\subsection{Convolutional Backbones in Computer Vision}
Convolutional block is the work-horse of deep neural networks in visual recognition. The standard and fundamental convolutional block was first introduced in \cite{lecun1998gradient} to distinguish hand-writing numbers. The block contains convolutional kernels with a certain receptive field that captures favorable visual context. To introduce the translation equivariance, the weights of convolutional kernels are shared over the entire image space. With the rapid development of the computational resources (\eg, GPUs), the successful training for a stack of a few convolutional blocks~\cite{krizhevsky2012imagenet,simonyan2014very} on large-scale image classification dataset (\eg, ImageNet \cite{russakovsky2015imagenet}) becomes possible. GoogLeNet \cite{szegedy2015going} demonstrated that a convolutional operator containing multiple kernel paths can achieve very competitive performance. 
The effectiveness of multi-path convolutional block was further validated in Inception series \cite{szegedy2016rethinking,szegedy2017inception}, ResNeXt \cite{xie2017aggregated}, DPN \cite{chen2017dual}, MixNet \cite{wang2018mixed} and SKNet \cite{li2019selective}.
Further, ResNet \cite{he2016deep} proposed skip connections in convolutional blocks, which makes very deep networks possible and has impressive effects in the field of computer vision.
DenseNet \cite{huang2017densely} introduced densely connected topology, which connects each convolutional block to its previous blocks. 
More recent advances can be found in recent survey/review papers~\cite{khan2020survey,shorten2019survey}.

\subsection{Dense Prediction Tasks}
\paragraph{Preliminary.} The dense prediction task aims to perform pixel-level classification or regression on the feature map.
Object detection and semantic segmentation are two representative dense prediction tasks.

\paragraph{Object Detection.} In the deep learning era, CNN~\cite{lecun1998gradient} has become the dominant framework for object detection, which includes single-stage detectors (\eg, SSD~\cite{liu2016ssd}, RetinaNet \cite{lin2017focal}, FCOS \cite{tian2019fcos}, GFL \cite{li2020generalized}, PolarMask \cite{xie2020polarmask} and OneNet \cite{sun2020onenet}) and multiple-stage detectors (Faster R-CNN \cite{ren2015faster}, Mask R-CNN \cite{he2017mask}, Cascade R-CNN \cite{cai2018cascade} and Sparse R-CNN \cite{sun2020sparse}).
Most of these popular object detectors are built on high-resolution or multi-scale feature maps, to obtain a good detection performance.
Recently, DETR~\cite{carion2020end} and deformable DETR~\cite{zhu2020deformable} combine the CNN backbone and the Transformer decoder to build an end-to-end object detector.
Like the previous CNN-based detectors, they also require high-resolution or multi-scale feature maps for accurate object detection.

\paragraph{Semantic Segmentation.} CNN also plays an important role in semantic segmentation.
In the early stage, FCN \cite{long2015fully} introduced fully convolutional architectures to generate a spatial segmentation map for a given image of any size.
After that, deconvolution operation was introduced by Noh \etal\cite{noh2015learning} and achieved impressive performance on PASCAL VOC 2012 dataset \cite{shetty2016application}.
Inspired by FCN, U-Net \cite{ronneberger2015u} is proposed for especially the medical image segmentation domain, which bridges the information flow between corresponding low-level and high-level feature maps with the same spatial sizes. 
To explore richer global context representation, Zhao \etal\cite{zhao2017pyramid} designs a pyramid pooling module over various pooling scales, and Kirillov \etal\cite{kirillov2019panoptic} develop a lightweight segmentation head termed Semantic FPN, based on FPN~\cite{lin2017feature}.
DeepLab family \cite{chen2017deeplab,liu2019auto} applies the dilated convolution to enlarge the receptive field while keeping the feature map resolution.
Similar to object detection methods, semantic segmentation methods also rely on high-resolution or multi-scale feature maps.








\begin{figure*}[t]
		\centering
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=0.95\textwidth]{./figure/arch.pdf}}
		\caption{\textbf{Overall architecture of the proposed Pyramid Vision Transformer (PVT).} The entire model is divided into four stages, and each stage is comprised of a patch embedding layer, and a -layer Transformer encoder. Following the pyramid structure, the output resolution of the four stages progressively shrinks from 
4-stride
		to 
32-stride.}
		\label{fig:arch}
\end{figure*}
\subsection{Self-Attention and Transformer in Vision}
As convolutional filter weights are usually fixed after training, it is inflexible for them to adapt dynamically to input variation. Therefore, many methods have been proposed to alleviate this problem via using self-attention operations.
The non-local block \cite{wang2018non} attempted to model long-range dependencies in both space and time, which is shown beneficial for accurate video classification.
Despite its success, the non-local operator suffers from the high memory and computation cost. Criss-cross \cite{huang2019ccnet} further reduced the complexity by generating sparse attention maps only through the criss-cross path. Ramachandran \etal\cite{ramachandran2019stand} proposed stand-alone self-attention  was propose to replace convolutional layers with local self-attention units. AANet \cite{bello2019attention} achieves competitive results when combining the self-attention and convolutional operations.
DETR \cite{carion2020end} utilize the Transformer decoder to model object detection as an end-to-end dictionary lookup problem with learnable queries,
successfully removing the hand-crafted process such as Non-Maximal Suppression (NMS). 
Based on DETR, deformable DETR \cite{zhu2020deformable} further introduces a deformable attention layer to focus on a sparse set of contextual elements which obtains fast convergence and better performance.
Recently, Vision Transformer (ViT) \cite{dosovitskiy2020image} employs a pure Transformer~\cite{vaswani2017attention} model to make image classification by treating an image as a sequence of patches.
DeiT \cite{touvron2020training} further extends ViT by using a novel distillation approach. 
Different from previous methods, this work try to introduce the pyramid structure into Transformer and design a pure Transformer backbone for dense prediction tasks.



\section{Pyramid Vision Transformer (PVT)}
\subsection{Overall Architecture}
Our goal is to introduce the pyramid structure into Transformer, so that it can generate multi-scale feature maps for dense prediction tasks (\eg, object detection and semantic segmentation).
An overview of PVT is depicted in Figure~\ref{fig:arch}.
Similar to CNN backbones~\cite{he2016deep}, our method has four stages that generate feature maps of different scales.
All stages share a similar architecture, which consists of a patch embedding layer and  Transformer encoder layers.

In the first stage, given an input image with the size of , we first divide it into  patches\footnote{Same as ResNet, we keep the highest resolution of our output feature map at 4-stride.}, and the size of each patch is .
Then, we feed the flattened patches to a linear projection and get embedded patches with size of .
After that, the embedded patches along with position embedding pass through a Transformer encoder with  layer, and the output is reshaped to a feature map , and its size is . 
In the same way, using the feature map from the prior stage as input, we obtain the following feature maps , , and , whose strides are 8, 16, and 32 pixels with respect to the input image.
With the feature pyramid , our method can be easily applied to most downstream tasks, including image classification, object detection, and semantic segmentation.

\subsection{Feature Pyramid for Transformer}
Unlike CNN backbone networks~\cite{he2016deep} that use convolution stride to obtain multi-scale feature maps, our PVT use \textit{progressive shrinking strategy} to control the scale of feature maps by patch embedding layers.

Here, we denote the patch size of the -th stage as .
At the beginning of the stage , we first evenly divide the input feature map  into  patches, 
and then each patch is flatten and projected to a -dim embedding. 
After the linear projection, the shape of the embedded patches can be viewed as , where the height and width are  times smaller than the input.

In this way, we can flexibly adjust the scale of the feature map in each stage, making it possible to construct a feature pyramid for Transformer. 

\subsection{Transformer Encoder}
\begin{figure}
		\centering
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=0.45\textwidth]{./figure/att.pdf}}
		\caption{\textbf{Multi-head attention (MHA) \vs spatial-reduction attention (SRA).} 
		With spatial-reduction operation, the computational/memory cost of our SRA could be much lower than that of MHA, so our SRA is more friendly to high-resolution feature maps.
		}
		\label{fig:att}
\end{figure}
For the Transformer encoder in the stage , it has  encoder layers, and each encoder layer is composed of an attention layer and a feed-forward layer~\cite{vaswani2017attention}.
Since our method needs to process high-resolution (\eg, 4-stride) feature maps, 
we propose a spatial-reduction attention (SRA) layer to replace the traditional multi-head attention (MHA) layer~\cite{vaswani2017attention} in the encoder.


Similar to MHA, our SRA also receives a query , a key , and a value  as input, and outputs a refined feature.
The difference is that our SRA will reduce the spatial scale of  and  before the attention operation as shown in Figure~\ref{fig:att}, which largely reduces the computation/memory overhead.
Details of SRA in the stage  can be formulated as follows:


where ,
,
, and

are parameters of linear projections. 
 is the head number of the Transformer encoder in the stage . 
Therefore, the dimension of each head  is equal to .
 is the spatial-reduction operation, which is defined as:

Here,  denotes the reduction ratio of the attention layers in the stage .
 is the operation of reshaping the input 

to the sequence with the size of .

is a linear projection that reduces the dimension of input sequence to .  refers to layer normalization~\cite{ba2016layer}.
Same as Transformer~\cite{vaswani2017attention},
 is the attention operation that is calculated as:

Through these formulas, we can figure out that the computational/memory costs of our  operation is  times lower than that of MHA, and thus it can handle larger input feature maps/sequences with limited resources.



\subsection{Model Details}
In summary, the hyper parameters of our method are listed as follows:
\begin{itemize}
    \item : the patch size of the stage ;
    \item : the channel number of the output of the stage ;
    \item : the number of encoder layers in the stage ;
    \item : the reduction ratio of the SRA in the stage ;
    \item : the head number of the SRA in the stage ;
    \item : the expansion ratio of the feed-forward layer~\cite{vaswani2017attention} in the stage ;
\end{itemize}
Following the design rules of ResNet~\cite{he2016deep}, we (1) use small output channel numbers in shallow stages; and (2) concentrate the major computation resource
in intermediate stages.

To provide instances for discussion, we describe a series of PVT models with different scales, namely PVT-Tiny, -Small, -Medium, and -Large, in Table~\ref{tab:arch}.
More details of employing these models in specific downstream tasks will be introduced in Section~\ref{sec:task}.
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{1.0mm}
    \begin{tabular}{c|c|c|c|c|c|c}
& Output Size & Layer Name & PYT-Tiny & PYT-Small & PYT-Medium  & PYT-Large \\
	\whline
\multirow{2}{*}[-2.5ex]{Stage 1} & \multirow{2}{*}[-2.5ex]{\scalebox{1.3}{}} & Patch Embedding & \multicolumn{4}{c}{;\ \ \ } \\
	\cline{3-7}
	& & \tabincell{c}{Transformer\\Encoder} & 
	 &
	 &
	 &
	 \\
	\hline
	\multirow{2}{*}[-2.5ex]{Stage 2} & \multirow{2}{*}[-2.5ex]{\scalebox{1.3}{}} & Patch Embedding & \multicolumn{4}{c}{;\ \ \  } \\
	\cline{3-7}
	& & \tabincell{c}{Transformer\\Encoder} &
	 &
	 &
	 &
	\\
	\hline
	\multirow{2}{*}[-2.5ex]{Stage 3} & \multirow{2}{*}[-2.5ex]{\scalebox{1.3}{}} & Patch Embedding & \multicolumn{4}{c}{;\ \ \  } \\
	\cline{3-7}
	& & \tabincell{c}{Transformer\\Encoder} &
	 &
	 &
	 &
	\\
	\hline
	\multirow{2}{*}[-2.5ex]{Stage 4} &  \multirow{2}{*}[-2.5ex]{\scalebox{1.3}{}} & Patch Embedding & \multicolumn{4}{c}{;\ \ \ } \\
	\cline{3-7}
	& & \tabincell{c}{Transformer\\Encoder} & 
	 &
	 &  & \\
\end{tabular}     \caption{\textbf{Detailed settings of PVT series.} The design follows the two rules of ResNet~\cite{he2016deep}: (1) with the growth of network depth, the hidden dimension gradually increases, and the output resolution progressively shrinks; (2) the major computation resource is concentrated in Stage 3.
    }
    \label{tab:arch}
\end{table*}



\subsection{Discussion}
The most related work to our method is ViT~\cite{dosovitskiy2020image}. Here we discuss in detail the relationship and differences between them.

Both PVT and ViT are pure Transformer models without convolution operation. The main difference between them is the pyramid structure. 
Similar to the traditional Transformer~\cite{vaswani2017attention}, the length of ViT's output sequence is the same as the input, which means that the output of ViT is single-scale (see Figure~\ref{fig:pipeline} (b)).
Moreover, due to the limited resource., the output of ViT is coarse-grained (\eg, the patch size is 16 or 32 pixels), and thus its output resolution is relatively low (\eg, 16-stride or 32-stride).
As a result, it is difficult to directly apply ViT in dense prediction tasks that require high-resolution or multi-scale feature maps.



Our PVT breaks the routine of Transformer by introducing a progressive shrinking pyramid.
It can generate multi-scale feature maps like a traditional CNN backbone.
In addition, we also designed a simple but
effective attention layer---SRA, to process high-resolution feature maps and reduce computation/memory costs.
Benefiting from the above designs, our method has the following advantages over ViT:
1) more flexible---can generate feature maps of different scales, channels in different stages;
2) more versatile---can be easily plugged and played in most downstream task models;
3) more friendly to computation/memory---can process the feature map with higher resolution.

\section{Applied to Downstream Tasks}
\label{sec:task}
\subsection{Image-Level Prediction}
Image classification is the most representative task of image-level prediction. Following ViT~\cite{dosovitskiy2020image} and DeiT~\cite{touvron2020training}, we append a learnable
classification token to the input of the last stage, and then use a fully connected layer to make classification on the top of the classification token.

\subsection{Pixel-Level Dense Prediction}
In addition to image-level prediction, dense prediction that requires performing pixel-level classification or regression on the feature map is also often seen in downstream tasks.
Here, we discuss two typical tasks, namely object detection, and semantic segmentation.

\paragraph{Object Detection.} 
We apply our PVT models to two representative object detection methods, namely RetinaNet~\cite{lin2017focal} and Mask R-CNN~\cite{he2017mask}.
RetinaNet is a widely-used single-stage detector, and Mask R-CNN is one of the mainstream two-stage instance segmentation frameworks. 
The implementation details are listed as follows: (1) Same as ResNet, we directly use the output feature pyramid  as the input of FPN~\cite{lin2017feature}, and then the refined feature maps are fed to the follow-up detection or instance segmentation head.
(2) In object detection, the input can be an arbitrary shape, so the position embeddings pre-trained on ImageNet may no longer be meaningful. Therefore, we perform bilinear interpolation on the pre-trained position embeddings according to the input image. 
(3) During the training of the detection model, all layers in PVT will not be frozen.

\paragraph{Semantic Segmentation.}
We choose Semantic FPN~\cite{kirillov2019panoptic} as the baseline, which is a simple segmentation method without special operations (\eg, dilated convolution). Therefore, using it as the baseline can well examine the original effectiveness of backbones. Similar to the implementation in target detection, we feed the feature pyramid directly to the semantic FPN, and use bilinear interpolation to resize the pre-trained position embedding.


\section{Experiments}
We compare our PVT with the two most representative CNN backbones, namely ResNet~\cite{he2016deep}, and ResNeXt~\cite {xie2017aggregated}, which are widely used in benchmarks of many downstream tasks.


\subsection{Image Classification}
\label{sec:cls}
\paragraph{Experiment Settings.}
We perform image classification experiments on the ImageNet dataset. 
The ImageNet 2012 dataset \cite{russakovsky2015imagenet} comprises 1.28 million training images and 50K validation images from 1,000 categories. 
We train models on the training set, and report the top-1 error on the validation set.
For a fair comparison, we follow DeiT~\cite{touvron2020training} to perform the random-size cropping to 224 224, random horizontal flipping \cite{szegedy2015going}, and mixup~\cite{zhang2017mixup} for data augmentation. Label-smoothing regularization \cite{szegedy2016rethinking} is used during training. We use  AdamW~\cite{loshchilov2017decoupled} with the momentum of 0.9, a mini-batch size of 128, and a weight decay of  by default. The initial learning rate is set to  and decreases following the cosine schedule~\cite{loshchilov2016sgdr}.  All models are trained for 300 epochs from scratch on 8 V100 GPUs.
To benchmark, we apply a center crop on the validation set, where  pixels are cropped for evaluating the recognition accuracy. 

\paragraph{Results.}
\begin{table}[t]
    \centering
\setlength{\tabcolsep}{1.0mm}
    \begin{tabular}{l|c|c|c}
Method & \#Param (M) & GFLOPs & Top-1 (\%)  \\
	\whline
R18*~\cite{he2016deep} & 11.7 & 1.8 & 30.2  \\
	R18~\cite{he2016deep}  & 11.7 & 1.8 & 31.5 \\
	DeiT-Tiny/16~\cite{touvron2020training} & 5.7 & 1.3 & 27.8 \\
	\rowcolor{mygray}
	PVT-Tiny (ours) & 13.2 & 1.9 &24.9 \\
	\hline
	R50*~\cite{he2016deep}   &25.6 &4.1 &23.9 \\
	R50~\cite{he2016deep}  &25.6 &4.1 & 21.5 \\
	X50-32x4d*~\cite{xie2017aggregated} &25.0 &4.3 &22.4  \\
	X50-32x4d~\cite{xie2017aggregated} &25.0 &4.3 & 20.5 \\
	DeiT-Small/16~\cite{touvron2020training}  & 22.1 & 4.6 & 20.1 \\
	\rowcolor{mygray}
	PVT-Small (ours)  & 24.5 & 3.8 & 20.2 \\
	\hline
	R101*~\cite{he2016deep}  &44.7 & 7.9 &22.6\\
	R101~\cite{he2016deep}  &44.7 & 7.9 & 20.2\\
	X101-32x4d*~\cite{xie2017aggregated} & 44.2 & 8.0 &21.2 \\
	X101-32x4d~\cite{xie2017aggregated} & 44.2  & 8.0 & 19.4 \\
	ViT-Small/16~\cite{dosovitskiy2020image} & 48.8 & 9.9 & 19.2 \\
	\rowcolor{mygray}
	PVT-Medium (ours) & 44.2 & 6.7 & 18.8\\
	\hline
X101-64x4d*~\cite{xie2017aggregated} & 83.5 & 15.6 & 20.4\\
	X101-64x4d~\cite{xie2017aggregated} & 83.5 & 15.6 & 18.5\\
	ViT-Base/16~\cite{dosovitskiy2020image} & 86.6 & 17.6 & 18.2 \\
	DeiT-Base/16~\cite{touvron2020training} & 86.6 & 17.6 & 18.2 \\
	\rowcolor{mygray}
	PVT-Large (ours) & 61.4 & 9.8 & 18.3 \\
\end{tabular}     \caption{\textbf{Image classification performance on the ImageNet validation set}.
    ``Top-1'' denotes the top-1 error rate.
    ``\#Param'' refers to the number of parameters. ``GFLOPs'' is calculated under the input scale of . ``*'' indicates the performance of the method trained with the strategy in its original paper.}
    \label{tab:cls}
\end{table}
In Table~\ref{tab:cls}, we find that our PVT models are superior to conventional CNN backbones under similar parameter numbers and computation budgets.
For example, when the GFLOPs are roughly similar, the top-1 error of PVT-Small reaches 20.2, which is 1.3 higher than that of ResNet50~\cite{he2016deep} (21.5).
Meanwhile, under similar or relatively lower complexity, our PVT family archives performance comparable (\ie, 18.2 \vs 18.3) to the recently proposed Transformer-based models, such as ViT~\cite{dosovitskiy2020image} and DeiT~\cite{touvron2020training}.
This result is within expectations, because the pyramid structure may be beneficial to dense prediction tasks, but the gains it brings to image classification are limited.

Note that, ViT and DeiT may have limitations as they are particularly designed for classification tasks, which are not suitable for dense prediction tasks that usually require effective feature pyramids.

\subsection{Object Detection}
\label{sec:det}
\paragraph{Experiment Settings.}
We conduct object detection experiments on the challenging COCO benchmark~\cite{lin2014microsoft}.
All models are trained on the COCO \texttt{train2017} (118k images) and evaluated on the \texttt{val2017} (5k images).
We evaluate our PVT backbones on two standard detectors: RetinaNet~\cite{lin2017focal} and Mask R-CNN~\cite{he2017mask}.
During training, we first use the pre-trained weights on ImageNet to initialize the backbone and Xavier~\cite{glorot2010understanding} to initialize the newly added layers.
Our models are trained with the batch size of 16 on 8 V100 GPUs and optimized by AdamW~\cite{loshchilov2017decoupled} with the initial learning rate of .
Following the common setting~\cite{lin2017focal,he2017mask,chen2019mmdetection}, we adopt 1 or 3 training schedule~(\ie, 12 or 36 epochs) to train all detection models.
The training image is resized to the shorter side of 800 pixels, while the longer side does not exceed 1333 pixels.
When using 3 training schedule, we also randomly resize the shorter side of the input image within the range of .
In the testing phase, the shorter side of the input image is fixed to 800 pixels.

\begin{table*}[t]
    \centering
\setlength{\tabcolsep}{1mm}
    

\begin{tabular}{l|c|lcc|ccc|lcc|ccc}
\multirow{2}{*}{Backbone} & \multirow{2}{*}{\tabincell{c}{\#Param \M)}} &\multicolumn{6}{c|}{Mask R-CNN 1x} &\multicolumn{6}{c}{Mask R-CNN 3x + MS} \\
\cline{3-14} 
& &AP &AP &AP &AP &AP &AP &AP &AP &AP &AP &AP &AP \\
\whline
ResNet18~\cite{he2016deep} & {31.2} & 34.0 & 54.0 & 36.7 & 31.2 & 51.0 & 32.7 & 36.9 & 57.1 & 40.0 & 33.6 & 53.9 & 35.7\\
\rowcolor{mygray}
PVT-Tiny (ours) & 32.9 & {36.7}\gbf{+2.7} & {59.2} & {39.3} & {35.1}\gbf{+3.9} & {56.7} & {37.3} & {39.8}\gbf{+2.9} & {62.2} & {43.0} & {37.4}\gbf{+3.8} & {59.3} & {39.9} \\
\hline
ResNet50~\cite{he2016deep} & 44.2& 38.0 & 58.6 & 41.4 & 34.4 & 55.1 & 36.7 & 41.0 & 61.7 & 44.9 & 37.1 & 58.4 & 40.1\\
\rowcolor{mygray}
PVT-Small (ours) &{44.1} & {40.4}\gbf{+2.4} & {62.9} & {43.8} & {37.8}\gbf{+3.4} & {60.1} & {40.3} & {43.0}\gbf{+2.0} & {65.3} & {46.9} & {39.9}\gbf{+2.8} & {62.5} & {42.8}\\
\hline
ResNet101~\cite{he2016deep} & 63.2 & 40.4 & 61.1 & 44.2 & 36.4 & 57.7 & 38.8 & 42.8 & 63.2 & 47.1 & 38.5& 60.1& 41.3\\
ResNeXt101-32x4d~\cite{xie2017aggregated} &{62.8} & 41.9\gbf{+1.5} & 62.5 & {45.9} & 37.5\gbf{+1.1} & 59.4 & 40.2 & 44.0\gbf{+1.2} & 64.4 & 48.0 & 39.2\gbf{+0.7} & 61.4 & 41.9 \\
\rowcolor{mygray}
PVT-Medium (ours) &63.9 & {42.0}\gbf{+1.6} &{64.4} &45.6 &{39.0}\gbf{+2.6}& {61.6}& {42.1} & {44.2}\gbf{+1.4} & {66.0} & {48.2} & {40.5}\gbf{+2.0} & {63.1} & {43.5}\\
\hline
ResNeXt101-64x4d~\cite{xie2017aggregated} &101.9 & 42.8 & 63.8 & {47.3} & 38.4 & 60.6 & 41.3 & 44.4 & 64.9 & 48.8 & 39.7& 61.9 & 42.6 \\
\rowcolor{mygray}
PVT-Large (ours) &81.0 & {42.9}\gbf{+0.1}& {65.0} & 46.6 &{39.5}\gbf{+1.1}& {61.9}& {42.5} & 44.5\gbf{+0.1} &66.0& 48.3 &40.7\gbf{+1.0} &63.4& 43.7 \\
\end{tabular}     \caption{\textbf{Object detection and instance segmentation performance on the COCO .} ``\#Param'' refers to parameter number. AP and AP denote bounding box AP and mask AP, respectively. ``MS'' means using multi-scale training~\cite{lin2017focal,he2017mask}.}
    \label{tab:mask}
\end{table*}

\paragraph{Results.}
As shown in Table~\ref{tab:retina}, using RetinaNet for object detection, we find that when the parameter number is comparable, the PVT variants significantly surpasses their counterparts, showing that our PVT can be a good alternative to the CNN backbone for object detection.
For example, with 1 training schedule, RetinaNet+PVT-Tiny is 4.9 AP better than RetinaNet+ResNet18~\cite{he2016deep}~(36.7 \vs 31.8).
Moreover, with 3 training schedule and multi-scale training, RetinaNet+PVT-Large archive 43.4 AP, surpassing RetinaNet+ResNeXt101-64x4d~\cite{xie2017aggregated} 41.8 AP, while our parameter number is 30\% fewer~(71.1M \vs 95.5M).

Similar results are found in the instance segmentation experiment using Mask R-CNN, as shown in Table~\ref{tab:mask}.
With 1 training schedule, Mask R-CNN+PVT-Tiny achieves 35.1 mask AP (AP), which is 3.9 AP better than Mask R-CNN+ResNet18 and even 0.7 AP higher than Mask R-CNN+ResNet50. 
The best AP obtained by Mask R-CNN+PVT-Large 40.7, which is 1.0 AP higher than the model based on ResNeXt101-64x4d.


\subsection{Semantic Segmentation}
\label{sec:seg}
\begin{table}[t]
    \centering
\setlength{\tabcolsep}{2.6mm}
    \begin{tabular}{l|c|l}
\multirow{2}{*}{Backbone} & \multicolumn{2}{c}{Semantic FPN}\\
	\cline{2-3}
	& \#Param (M) & mIoU (\%)   \\
	\whline
ResNet18~\cite{he2016deep} & {15.5} & 32.9 \\
	\rowcolor{mygray}
	PVT-Tiny (ours) & 17.0 & {35.7}\gbf{+2.8} \\
	\hline
ResNet50~\cite{he2016deep} & 28.5 & 36.7\\
\rowcolor{mygray}
PVT-Small (ours) & {28.2}& {39.8}\gbf{+3.1}\\
\hline
ResNet101~\cite{he2016deep} & 47.5& 38.8\\
ResNeXt101-32x4d~\cite{xie2017aggregated} & {47.1} & 39.7\gbf{+0.9} \\
\rowcolor{mygray}
PVT-Medium (ours) & 48.0 & {41.6}\gbf{+2.8}\\
\hline
ResNeXt101-64x4d~\cite{xie2017aggregated} & 86.4 & 40.2\\
\rowcolor{mygray}
PVT-Large (ours) & {65.1} & {42.1}\gbf{+1.9} \\
\end{tabular}     \caption{\textbf{Semantic segmentation performance of different backbones on the ADE20K validation set.} ``\#Param'' refers to parameter number. 
    }
    \label{tab:seg}
\end{table}

\paragraph{Experiment Settings.}
We choose ADE20K~\cite{zhou2017scene}, a challenging scene parsing benchmark for semantic segmentation. ADE20K contains 150 fine-grained semantic categories, where there are 20210, 2000, and 3352 images for training, validation and, testing, respectively. 
We evaluate our PVT backbone by applying it to Semantic FPN~\cite{kirillov2019panoptic}, a simple segmentation method without dilated convolutions~\cite{yu2015multi}.
In the training phase, the backbone is initialized with the pre-trained weights on ImageNet~\cite{deng2009imagenet}, and other newly added layers are initialized with Xavier~\cite{glorot2010understanding}. 
We optimize our models by AdamW~\cite{loshchilov2017decoupled} with the initial learning rate of 1e-4.
Following the common settings~\cite{kirillov2019panoptic,chen2017deeplab}, we train our models for 80k iterations with the batch size of 16 on 4 V100 GPUs.
The learning rate is decayed according to the polynomial decay schedule with the power of 0.9.
We randomly resize and crop the training image to  and scale the image to the shorter side of 512 during testing.


\paragraph{Results.} 
As shown in Table~\ref{tab:seg}, our PVT consistently outperforms ResNet~\cite{he2016deep} and ResNeXt~\cite{xie2017aggregated} under different parameter scales, using Semantic FPN for semantic segmentation. 
For example, with almost the same parameter number, our PVT-Tiny/Small/Medium is at least 2.8 mIoU higher than ResNet-18/50/101. 
In addition, although the parameter number of our Semantic FPN+PVT-Large is 20\% lower than that of Semantic FPN+ResNeXt101-64x4d, the mIoU is still 1.9 higher~(42.1 \vs 40.2), showing that for semantic segmentation, our PVT can extract better features than the CNN backbone benefitting from the global attention mechanism.


\subsection{Pure Transformer Dense Prediction}
We replace ResNet50 with PVT on DETR~\cite{carion2020end} for detection and Trans2Seg~\cite{xie2021segmenting} for segmentation, building pure Transformer dense prediction pipelines. The results prove that fully Transformer without convolution also works well on both object detection and semantic segmentation.


\paragraph{PVT+DETR.}
\begin{table}[t]
    \centering
\setlength{\tabcolsep}{0.6mm}
    \begin{tabular}{l|lcc|ccc}
\multirow{2}{*}{Method} & \multicolumn{6}{c}{DETR}\\
	\cline{2-7}
 &AP &AP &AP &AP &AP &AP  \\
	\whline
	ResNet50 & 32.3 & 53.9 & 32.3 & 10.7 & 33.8 & 53.0  \\
\rowcolor{mygray}
	PVT-Small (ours) & 34.7\gbf{+2.4} & 55.7 & 35.4 & 12.0 & 36.4 & 56.7 \\
\end{tabular}     \caption{\textbf{Performance of the pure Transformer object detection pipeline.} We build a pure Transformer detector by combining PVT and DETR~\cite{carion2020end}, whose AP is 2.4 higher than the original DETR based on ResNet50~\cite{he2016deep}.}
    \label{tab:detr}
\end{table}
We build a pure Transformer model for object detection by combining our PVT with DETR~\cite{carion2020end}, which has a Transformer-based detection head. We train models on the COCO  for 50 epochs with the initial learning rate of . The learning rate is divided by 10 at the 33rd epoch. We use random flip and random scale as the data augmentation. All other experiment settings is the same as that of Sec.~\ref{sec:det}. As reported in Table \ref{tab:detr}, PVT+DETR archieve 34.7 AP on the COCO , outperforming ResNet50+DETR by 2.4 AP (34.7 \vs 32.3).

\paragraph{PVT+Trans2Seg.}
\begin{table}[t]
    \centering
\setlength{\tabcolsep}{0.7mm}
    \begin{tabular}{l|c|c|l}
Method & \#Param (M) &GFLOPs & mIoU (\%)   \\
	\whline
	R50-d8+DeeplabV3+  &26.8&120.5 &41.5\\
	R50-d16+DeeplabV3+  &26.8&45.5 &40.6\\
	\hline
	R50-d16+Trans2Seg  & 56.1&79.3 & 39.7\\
\rowcolor{mygray} PVT-Small+Trans2Seg   &32.1 &31.6&42.6\gbf{+2.9}\\
	
\end{tabular}     \caption{\textbf{Performance of the pure Transformer semantic segmentation pipeline.} We build a pure Transformer detector by combining PVT and Trans2Seg~\cite{xie2021segmenting}. It is 2.9\% higher than ResNet50-d16+Trans2Seg and 1.1\% higher than ResNet50-d8+DeeplabV3+ with lower GFlops. ``d8'' and ``d16'' means dilation 8 and 16, respectively. }
    \label{tab:trans2seg}
\end{table}
We build 
a
pure Transformer model for semantic segmentation by combining our PVT with Trans2Seg~\cite{xie2021segmenting}, a 
segmentation head based on Transformer decoder.
According to the experiment settings in Sec.~\ref{sec:seg}, we perform experiments on ADE20K~\cite{zhou2017scene} dataset with 40k iterations training, single scale testing, and compare it with ResNet50+Trans2Seg, DeeplabV3+ with dilation 8 (d8) and 16 (d16), as shown in Table \ref{tab:trans2seg}. 

We find that PVT-Small+Trans2Seg achieves 42.6 mIoU, outperforming ResNet50-d8+DeeplabV3+~(41.5).
Note that, ResNet50-d8+DeeplabV3+ has 120.5 GFLOPs due to the high computation cost of dilated convolution, and our PVT-Small+Trans2Seg has only 31.6 GFLOPs.
PVT-Small+Trans2Seg also performs better than ResNet50-d16+Trans2Seg~(31.6G Flops \vs 79.3 GFLOPs, 42.6 \vs 39.7, only 40\% GFLOPs and 2.9 mIoU higher).



\subsection{Ablation Study}
\paragraph{Experiment Settings.}
We conduct ablation studies on ImageNet~\cite{deng2009imagenet} and COCO\cite{lin2014microsoft} datasets.
The experimental settings on ImageNet are the same as the settings in Sec. \ref{sec:cls}.
On the COCO dataset, all models are trained using 1 training schedule (\ie, 12 epochs), and other experimental settings follow the settings in Sec. \ref{sec:det}.

\begin{table}[t]
    \centering
\begin{tabular}{l|c|ccc}
\multirow{2}{*}{Method} & \multirow{2}{*}{\tabincell{c}{\#Param \M)}} & \multirow{2}{*}{Top-1} & \multicolumn{3}{c}{RetinaNet 1x}\\
	\cline{4-6}
	& & & AP &AP &AP  \\
	\whline
	PVT-Small-Wide & 46.8 & 19.3 & 40.8 & 61.8 &43.3 \\
PVT-Medium & 44.2 & {18.8} & {41.9} & {63.1} & {44.3}  \\
\end{tabular}     \caption{\textbf{Deeper \vs Wider.} ``Top-1'' denotes the top-1 error on the ImageNet validation set. ``AP'' denotes the bounding box AP on the COCO . The deep model (\ie, PVT-Medium) obtains better performance than the wide model (\ie, PVT-Small-Wide ) under comparable parameter number.}
    \label{tab:deep_wide}
\end{table}



\paragraph{Deeper \vs Wider.} 
The problem of whether the CNN backbone should go deeper or wider has been extensively discussed in previous work~\cite{he2016deep,zerhouni2017wide}. Here, we explore this problem in our PVT.
For fair comparisons, we multiply the hidden dimensions  of PVT-Small
by a scale factor 1.4 to make it have an equivalent parameter number to the deep model (\ie, PVT-Medium). As shown in Table \ref{tab:deep_wide}, the deep model (\ie, PVT-Medium) consistently works better than the wide model (\ie, PVT-Small-Wide) on both ImageNet and COCO. Therefore, going deeper is more effective than going wider in the design of PVT. Based on this observation, in Table~\ref{tab:arch}, we develop PVT models with different scales by increasing the model depth.


\paragraph{Pre-trained Weights.} Most dense prediction models (\eg, RetinaNet~\cite{lin2017focal}) rely on the backbone whose weights are pre-trained on ImageNet. We also discuss this problem in our PVT. In the top of Figure \ref{fig:pretrain}, we plot the validation AP curves of RetinaNet-PVT-Small w/ ({red curves}) and w/o ({blue curves}) pre-trained weights.
We find that the model w/ pre-trained weights converges better than the one w/o pre-trained weights, 
and the gap between their final AP reaches 13.8 under the 1 training schedule and 8.4 under the 3 training schedule and multi-scale training.
Therefore, like CNN-based models, pre-training weights can also help PVT-based models converge faster and better.
Moreover, in the bottom of Figure \ref{fig:pretrain}, we also see that the convergence speed of PVT-based models ({red curves}) is faster than that of ResNet-based models ({green curves}).


\paragraph{Computation Cost.}
\begin{figure}
		\centering
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=0.45\textwidth]{./figure/flops.pdf}}
		\caption{\textbf{GFLOPs under different input scales.} The growth rate of GFLOPs: ViT-Small/16~\cite{dosovitskiy2020image}ViT-Small/32~\cite{dosovitskiy2020image}PVT-SmallResNet50~\cite{he2016deep}. When the input scale is less than , the GFLOPs of PVT-Small and ResNet50~\cite{he2016deep} are close.}
		\label{fig:flops}
\end{figure}
\begin{table}[t]
    \centering
\setlength{\tabcolsep}{1.0mm}
    \begin{tabular}{l|c|c|c|ccc}
\multirow{2}{*}{Method} &  
	\multirow{2}{*}{Scale} & 
	\multirow{2}{*}{GFLOPs} & \multirow{2}{*}{\tabincell{c}{Time\ms)}} & \multicolumn{3}{c}{RetinaNet 1x}\\
	\cline{5-7}
	&&&&AP &AP &AP  \\
	\whline
	R50~\cite{he2016deep} &800&239.3&55.9 & 36.3 & 55.3 & 38.6\\
	\hline
	\multirow{2}{*}{PVT-S (ours)} & 640 &157.2& 51.7  & 38.7 & 59.3 & 40.8 \\
	 & 800 &285.8 &76.9 & 40.4 & 61.3 & {43.0} \\
	
\end{tabular}     \caption{\textbf{Latency and AP under different input scales.} ``Scale'' and ``Time'' denote the input scale and time cost per image. When the shorter side of the input image is 640 pixels, the PVT-S+RetinaNet has lower GFLOPs and time cost than R50~\cite{he2016deep}+RetinaNet, while obtaining 2.4 better AP (38.7 \vs 36.3). Note that, the time cost is tested on one V100 GPU with the batch size of 1.}
    \label{tab:speed}
\end{table}
With the increment of input scale, the growth rate of GFLOPs of our method is greater than ResNet~\cite{he2016deep}, but less than ViT~\cite{dosovitskiy2020image}, as shown in Figure \ref{fig:flops}, which means that our PVT is more suitable for tasks with medium-resolution input (\eg, shorter size does not exceed 800 pixels).
On COCO, the shorter side of the input image is 800 pixels. Under this condition, the inference speed of RetinaNet based on PVT-Small is slower than the one based on ResNet50, as reported in Table \ref{tab:speed}.

A direct solution for this problem is to reduce the input scale. When reducing the shorter side of the input image to 640 pixels, the model based on PVT-Small runs faster than the ResNet50-based model, but our AP is 2.4 higher (38.7 \vs 36.3).
Another possible solution is to develop a novel self-attention layer with lower computational complexity for visual tasks. This is a direction worth exploring, and we will pay consistent efforts in it in the future.

\section{Conclusions and Future Work}

In this paper, we introduce PVT, a pure Transformer backbone for dense prediction tasks such as object detection and semantic segmentation. 
We develop a progressive shrinking pyramid and a spatial-reduction attention layer to obtain multi-scale feature maps under limited computation/memory resources.
Extensive experiments on object detection and semantic segmentation benchmarks verify that our PVT is stronger than well-designed CNN backbones under comparable numbers of parameters.




Although PVT can serve as an alternative to the CNN backbone (\eg, ResNet, ResNeXt), there are still some specific modules and operations designed for CNNs but not considered in this work, such as 
SE~\cite{hu2018squeeze}, SK~\cite{li2019selective}, dilated convolution~\cite{yu2015multi}, and NAS~\cite{tan2019efficientnet}.
Moreover, with years of rapid developments, there are many well-engineered CNN backbones such as Res2Net~\cite{gao2019res2net}, EfficientNet~\cite{tan2019efficientnet}, and ResNeSt~\cite{zhang2020resnest}.
On the contrary, the Transformer-based model in computer vision is still in its early stage of development. 
Therefore, 
we believe there are many potential technologies to be explored in the future. We hope that our method could serve as a good starting point.










{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
