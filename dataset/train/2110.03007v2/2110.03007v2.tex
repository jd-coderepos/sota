\section{Experiments and Results}

\subsection{Experimental Setup}


\begin{table}[h]
\caption{Comparison of binary accuracy and f1 weighted average metrics against literature models for sentiment classification on MOSEI. With ↑ and ↓ we denote the distance from the minimum and the maximum performance respectively\label{mosei_sota}}
\resizebox{.45\textwidth}{!}{\begin{tabular}{ccc}
{ \textbf{Model (Reference, Evaluation)}}                                   & { \textbf{Acc2}} & { \textbf{F1-weighted}} \\ \hline
{ MV-LSTM (\cite{rajagopalan2016extending}, \cite{zadeh2018multimodal})}    & { 76.4}          & { 76.4}                 \\
{ MFN (\cite{zadeh2018memory}, \cite{zadeh2018multimodal})}                 & { 76}            & { 76}                   \\
{ Graph-MFN (\cite{zadeh2018multimodal}, \cite{tsai2019multimodal})}        & { 76.9}          & { 77.0}                 \\
{ RAVEN (\cite{wang2019words}, \cite{tsai2019multimodal})}                  & { 79.1}          & { 79.5}                 \\
{ MCTN (\cite{pham2019found}, \cite{tsai2019multimodal})}                   & { 79.8}          & { 80.6}                 \\
{ CIA (\cite{chauhan-etal-2019-context}, \cite{chauhan-etal-2019-context})} & { 80.4}          & { 78.2}                 \\
{ MulT (\cite{tsai2019multimodal}, \cite{tsai2019multimodal})}              & { \textbf{82.5}} & { \textbf{82.3}}        \\ \hline
{ Average}                                                                  & { 77.6}          & { 76.9}                 \\ \hline
{ CAE-LR (Ours)}                                                            & { 78 (2↑ 4.5↓)}  & { 76.3 (0.3↑ 4.3↓)}    
\end{tabular}}
\end{table}

\begin{table*}
\caption{Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition on IEMOCAP. With ↑ and ↓ we denote the distance from the minimum and the maximum performance respectively\label{iemocap_sota}}
\resizebox{\textwidth}{2.5cm}{
\begin{tabular}{ccccccccc}
\multicolumn{1}{l}{{ }}                                            & \multicolumn{2}{c}{{ \textbf{Happy}}}                              & \multicolumn{2}{c}{{ \textbf{Sad}}}                               & \multicolumn{2}{c}{{ \textbf{Angry}}}                             & \multicolumn{2}{c}{{ \textbf{Neutral}}}                              \\ \cline{2-9} 
{ \textbf{Model (Reference, Evaluation)}}                          & { \textbf{Acc2}}     & { \textbf{F1-weighted}} & { \textbf{Acc2}}    & { \textbf{F1-weighted}} & { \textbf{Acc2}}    & { \textbf{F1-weighted}} & { \textbf{Acc2}}       & { \textbf{F1-weighted}} \\
{ DF  (\cite{nojavanasghari2016deep}, \cite{wang2019words})}       & { 86}             & { 81}                & { 81.8}            & { 81.2}                & { 75.8}            & { 65.4}                & { 59.1}               & { 44}                \\
{ MV-LSTM (\cite{rajagopalan2016extending}, \cite{wang2019words})} & { 85.9}             & { 81.3}                & { 80.4}            & { 74}                & { 85.1}            & { 84.3}                & { 67}               & { 66.7}                \\
{ BC-LSTM (\cite{poria2017context}, \cite{wang2019words})}         & { 84.9}             & { 81.7}                & { 83.2}            & { 81.7}                & { 83.5}            & { 84.2}                & { 67.5}               & { 64.1}                \\
{ MARN (\cite{zadeh2018multi}, \cite{wang2019words})}              & { 86.7}             & { 83.6}                & { 82}            & { 81.2}                & { 84.6}            & { 84.2}                & { 66.8}               & { 65.9}                \\
{ TFN (\cite{zadeh2017tensor}, \cite{zadeh2017tensor})}              & { -}             & { 83.6}                & { -}            & {82.8 }                & { -}            & { 84.2}                & {}               & { 65.4}                \\
{ RMFN (\cite{liang2018multimodal}, \cite{wang2019words})}         & { 87.5}             & { 85.8}                & { 83.8}            & { 82.9}                & { 85.1}            & { 84.6}                & { 69.5}               & { 69.1}                \\
{ MFN (\cite{zadeh2018memory}, \cite{wang2019words})}              & { 90.2}             & { 85.8}                & { 88.4}            & { 86.1}                & { \textbf{87.5}}   & { 86.7}                & { 72.1}               & { 68.1}                \\
{ MCTN (\cite{pham2019found}, \cite{tsai2019multimodal})}          & { 84.9}             & { 83.1}                & { 80.5}            & { 79.6}                & { 79.7}            & { 80.4}                & { 62.3}               & { 57}                \\
{ MulT (\cite{tsai2019multimodal}, \cite{tsai2019multimodal})}     & { \textbf{90.7}}    & { \textbf{88.6}}       & { 86.7}            & { 86}                & { 87.4}            & { \textbf{87}}       & { \textbf{72.4}}      & { \textbf{70.7}}       \\
{ ICCN (\cite{sun2020learning}, \cite{sun2020learning})}           & { 87.4}             & { 84.7}                & { \textbf{88.6}}   & { \textbf{88}}       & { 86.3}            & { 85.9}                & { 69.7}               & { 68.5}                \\
\hline
{ Average}                                                         & { 87.13}             & { 83.92}                & { 83.93}            & { 82.35}                & { 83.89}            & { 82.69}                & { 67.38}               & { 63.95}                \\ \hline
{ CAE-LR (Ours)}                                                   & { 85.8  (0.9↑ 4.9↓)} & { 81.60 (0.6↑ 7↓)}      & { 82.3 (3.4↑ 6.3↓)} & { 82.13 (8.13↑ 5.87↓)}  & { 85.6 (9.8↑ 1.9↓)} & { 85.5 (20.1↑ 1.5↓)}    & { 63.85 (4.75↑ 8.55↓)} & { 60.91 (16.91↑ 9.79↓)}
\end{tabular}}
\end{table*}


The MOSEI \cite{zadeh2018multimodal} and IEMOCAP \cite{busso2008iemocap}  datasets have been used for representation learning, as well as Sentiment Analysis and Emotion Recognition respectively. The processed version of IEMOCAP consists of 7318 segments of recorded dyadic dialogues annotated for the presence of the human emotions happiness, sadness, anger and neutral, while MOSEI is a large scale sentiment analysis dataset made up of 22,777 movie review video clips from more than 1000 online Youtube speakers. The data and feature extraction, as well as the train, validation and test splits were obtained from the widely used in the literature CMU-MultimodalSDK repository (\href{https://github.com/A2Zadeh/CMU-MultimodalSDK}{https://github.com/A2Zadeh/CMU-MultimodalSDK}). 

After feature extraction was performed (COVAREP \cite{degottex2014covarep} for audio, GloVe \cite{pennington2014glove} for text and Facet \cite{imotions} for visual) each segment was represented by a 74-d acoustic, a 35-d visual and 300-d textual feature vector. With the use of the aforementioned word-level alignment and concatenation, a matrix, , of 20 x 409 dimensions is obtained for each utterance. After performing a sequence of standard and min-max scaling for each of the 409 features across all 20 timestamps and all dataset instances, the 2-dimensional inputs are properly prepossessed.

In order to train the multimodal representations in an unsupervised manner, we initialized a Convolutional AE with a 4-layer Encoder and its corresponding 4-layer Decoder. The kernel size of the first 2 layers of the Encoder was 3x3, while for the last two layers two 5x5 kernels were used. 2x2 padding along with 1x1 stride and 2x2 max-pooling were used in all layers. The channels of the layers were 32, 64, 128 and 10 respectively. This yields in a (flattened)  representation of 250 dimensions in the code part of the AE. For better representation ability we used the Gelu activation function, while batch normalization was included across all layers. We trained the Convolutional AE using the mean-squared error as loss function, while Adam was chosen to be the optimizer with initial learning rate of 0.002 and a reduce-on-plateau learning rate scheduler. We also performed normal weight initialization and early stopping based on the mean-squared error of the validation set.

For the unsupervised experimental results, we gathered the train sets of MOSEI and IEMOCAP to form the train set and the corresponding validation sets in order to create a multi-dataset validation set. For different kinds of pre-training as well as the performance of the Convolutional AE in terms of the mean-squared error function, we refer the reader to section \ref{ablation_study}.

\subsection{Results on Downstream Classification}


Using the Encoder part of the Convolutional AE as a feature extractor, we get a 250-d feature vector for each utterance. This means that the initial 20 x 409 (=8180 feature values) matrix is reduced to a 250-d vector (32 times less elements). These embeddings have been used to train a Logistic Regression model for the two downstream tasks of Sentiment Analysis and Emotion Recognition. Following the existing literature \cite{zadeh2018multi, zadeh2018memory, zadeh2017tensor, wang2019words, zadeh2018multimodal, liu2018efficient, tsai2019multimodal, tsai2019learning,  chauhan-etal-2019-context, sun2020learning}, we report the binary accuracy and weighted averaged f1 metrics on sentiment for MOSEI, and on each of the 4 emotions of IEMOCAP in an one-vs-all manner.

In tables \ref{mosei_sota} and \ref{iemocap_sota} we compare our results with state-of-the-art and well established architectures in the tasks of Sentiment Analysis and Emotion Recognition. More specifically, we compare to MV-LSTM \cite{rajagopalan2016extending}, MFN \cite{zadeh2018memory}, Graph-MFN \cite{zadeh2018multimodal}, RAVEN \cite{wang2019words}, MCTN \cite{pham2019found}, CIA \cite{chauhan-etal-2019-context}, MulT \cite{tsai2019multimodal}, DF \cite{nojavanasghari2016deep}, BC-LSTM \cite{poria2017context}, MARN \cite{zadeh2018multi}, TFN \cite{zadeh2017tensor}, RMFN \cite{liang2018multimodal} and ICCN \cite{sun2020learning} which, with the use of the CMU-MultimodalSDK repository, are trained and evaluated on the exact same sets. For each model we reference both the original work and the one that evaluated the method on the MOSEI or IEMOCAP datasets. For detailed explanation and comparison of the aforementioned architectures, we refer the reader to detailed reviews on Multimodal Sentiment Analysis \cite{gkoumas2021makes} and Multimodal Emotion Recognition \cite{app11177962}. 

As seen in \ref{mosei_sota} and \ref{iemocap_sota} the proposed generic mutlimodal representations achieve competitive performance with the use of a basic Machine Learning algorithm (Logistic Regression). More specifically, our lightweight method achieves an average (calculated on state-of-the art competitive models) performance on sentiment analysis with 2 absolute points above minimum and 4.5 below maximum binary accuracy. It also performs close to average and always above minimum for the task of Emotion Recognition. Therefore, a very simple simple classification algorithm can score the average performance of the SotA models, which clearly indicates that the learned multimodal language embeddings have strong representation power and can be used across different multimodal language tasks, despite the different recording set-up across datasets. The code for all experiments can be found in the mlr repository (\href{https://github.com/lobracost/mlr}{https://github.com/lobracost/mlr}). 

\subsection{Ablation Study}\label{ablation_study}

\begin{table}
\centering
\caption{Convolutional AE pretraining for different modalities and dataset combinations\label{cae_pretrain}}
\resizebox{.45\textwidth}{!}{
\begin{tabular}{ccc}
{ \textbf{Modalities}}   & { \textbf{Embeddings Training}} & { \textbf{MSE} ( x )}  \\ \hline
{ Audio}                 & { MOSEI \& IEMOCAP}  & { 32.34} \\
{ Vision}                & { MOSEI \& IEMOCAP}  & { 22.67} \\
{ Text}                  & { MOSEI \& IEMOCAP}  & { 44.22} \\
{ [Audio, Vision]}        & { MOSEI \& IEMOCAP}  & { 30.21}  \\
{ [Vision, Text]}         & { MOSEI \& IEMOCAP}  & { 24.34} \\
{ [Audio, Text]}          & { MOSEI \& IEMOCAP}  & { 29.94} \\
{ [Audio, Vision, Text]} & { MOSEI \& IEMOCAP}  & { 24.11} \\
{ [Audio, Vision, Text]} & { MOSEI}            & { 23.12} \\
{ [Audio, Vision, Text]} & { IEMOCAP}              & { 20.58}
\end{tabular}}

\centering
\caption{Performance on MOSEI sentiment classification using embeddings trained on different modality combinations\label{modalities_pretrain}}
\begin{tabular}{ccc}
{ \textbf{Modalities}}   & { \textbf{Acc2}}  & { \textbf{F1-weighted}} \\ \hline
{ Audio}                 & {71.06}          & {59.1}                \\
{ Visual}                & {71.06}          & {59.1}                \\
{ Text}                  & {75.4}          & {72.63}                \\
{ [Audio, Visual]}       & {71.06}          & {59.87}                \\
{ [Visual, Text]}        & {71.04}          & {59.01}                \\
{ [Audio, Text]}         & {77.86}          & {76.09}                \\
{ [Audio, Visual, Text]} & { \textbf{78}} & { \textbf{76.3}}      
\end{tabular}

\centering
\caption{Performance on MOSEI sentiment classification using embeddings trained on different dataset combinations\label{dataset_pretrain}}
\begin{tabular}{ccc}
\multicolumn{1}{l}{{ \textbf{Pretrained representations}}} & { \textbf{Acc2}}  & { \textbf{F1-weighted}} \\ \hline
{ MOSEI}                                                   & { 77.2}          & { 75.4}                \\
{ IEMOCAP}                                                 & { 76.7}          & { 74.5}                \\
{ MOSEI \& IEMOCAP}                                        & { \textbf{78}} & { \textbf{76.3}}      
\end{tabular}

\end{table}


In this section, we examine the role of the modalities combinations, as well as different datasets used for the unsupervised training of the embeddings. To this end, we trained the Convolutional AE on a range of different modality  and training data combinations. For each combination, we report the mean-squared error in table \ref{cae_pretrain}. In that way we can gain an insight on the quality of the learnt information compression for each modality combination. It is easily derived that the learnt embeddings of our method are more informative with respect to the original visual modality and less collective for the textual one.


\subsubsection{The role of different modalities}

Using the produced embeddings of each modality combination in order to perform downstream classification, we end up with the results of table \ref{modalities_pretrain} for the MOSEI dataset. As it can be clearly seen \textit{(i)} using all three modalities enriches the representation power, and \textit{(ii)} the textual modality is the most informative for the task of Multimodal Language Analysis, a fact that is also known in the literature \cite{sun2020learning}. Thus our method achieves to effectively express unimodal information and learn multimodal interactions of temporal language sequences. 

\subsubsection{Cross-domain generalization ability}\label{cross-dataset}


In order to examine whether information of one dataset can be used to other tasks, we performed different pretraining to the Convolutional Autoencoder and record the performance on downstream classification for the MOSEI dataset. The two crucial remarks from table \ref{dataset_pretrain} that make our work widely useful is that \textit{(i)} our methodology leads to embeddings that can easily generalize to new data that have been recorded in a different way and for different tasks, since the performance for Sentiment Analysis when using embeddings pretrained on IEMOCAP is 0.5 absolute points (in terms of binary accuracy) below the ones trained in the MOSEI train data and \textit{(ii)} our embeddings can be enriched from information of different datasets, since the classification using the embeddings trained on MOSEI \& IEMOCAP performs better than the ones trained on just the MOSEI training set. That clearly indicates that the proposed Encoder can serve as feature extractor in a range of Multimodal Language tasks and used for generalization in unseen data formats.

\subsection{Model Complexity}

\begin{table}
\centering
\caption{Comparison of model parameters\label{complexity_comparison}}
\begin{tabular}{ccc}
{ \textbf{}}                          & \multicolumn{2}{c}{{ \textbf{Number of Parameters}}}        \\
{ \textbf{Model}}                     & { \textbf{MOSEI}} & { \textbf{IEMOCAP}} \\ \hline
{ TFN}                                & { 6,804,859}      & { 23,198,398}       \\
{ RMFN}                               & { -}              & { 1,732,884}        \\
{ MFN}                                & { 415,521}        & { 1,325,508}        \\
{ MulT}                               & { 874,651}        & { 1,074,998}        \\
{ CAE-LR (ours)} & { 256,453}        & { 257,206}         
\end{tabular}
\end{table}

In \cite{gkoumas2021makes} the authors retrained 11 of the most powerful and widely used models for Multimodal Language Analysis and list the number of parameters for some of them. However, this study, due to different pretraining, reports smaller amount of parameters for some models (eg. \cite{mai2021analyzing} reports 1,549,321 parameters for MulT on the MOSEI dataset with a different pretraining procedure). However, for the sake of a complete comparison, we choose to report the results of \cite{gkoumas2021makes}, though this may underestimate the number of parameters of the reported models. In table \ref{complexity_comparison} we report the number of parameters of the proposed methods against the models reported in \cite{gkoumas2021makes}. It can be noticed that the Encoder used for feature extraction uses the same number of parameters across both datasets, which is not the case for other architectures where the parameter difference across tasks ranges from ~123\% to ~340\% (probably due to the number of classes). That is a direct outcome from the fact that our method does not require extensive fine-tuning techniques in order to be generalized to other tasks. Combining the feature extraction (256,202 parameters for both tasks) with the inference part (251 parameters for MOSEI and 1004 for IEMOCAP) we end up with an end-to-end method that includes parameters in the range 256,202-257,206 while the rest architectures are highly greedy in terms of parameter amounts since they range from 415,521 to 23,198,398. Thus the CAE-LR is a lightweight method that can be easily generalized to other tasks, without impact in the number of parameters (shown in Table \ref{complexity_comparison}), nor significant performance loss (shown in section \ref{cross-dataset}).