

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{makecell}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{10150} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}

\title{CR-LSO: Convex Neural Architecture Optimization in the Latent Space of Graph Variational Autoencoder with Input Convex Neural Networks}

\author{Xuan Rao$^{1}$, Bo Zhao$^{2}$\footnote{Corresponding author}, Songyi Xiao$^{1}$, Derong Liu$^{3,4}$\\
$^{1}$Guangdong University of Technology, $^{2}$Beijing Normal University \\ $^{3}$Southern University of Science and Technology, $^{4}$University of Illinois at Chicago\\
{\tt\small 2112004117@mail2.gdut.edu.cn;  zhaobo@bnu.edu.cn;  liudr@sustech.edu.cn}
}
\maketitle

\begin{abstract}
In neural architecture search (NAS) methods based on latent space optimization (LSO), a deep generative model is trained to embed discrete neural architectures into a continuous latent space. In this case, different optimization algorithms that operate in the continuous space can be implemented to search neural architectures. However, the optimization of latent variables is challenging for gradient-based LSO since the mapping from the latent space to the architecture performance is generally non-convex. To tackle this problem, this paper develops a convexity regularized latent space optimization (CR-LSO) method, which aims to regularize the learning process of latent space in order to obtain a convex architecture performance mapping. Specifically, CR-LSO trains a graph variational autoencoder (G-VAE) to learn the continuous representations of discrete architectures. Simultaneously, the learning process of latent space is regularized by the guaranteed convexity of input convex neural networks (ICNNs). In this way, the G-VAE is forced to learn a convex mapping from the architecture representation to the architecture performance. Hereafter, the CR-LSO approximates the performance mapping using the ICNN and leverages the estimated gradient to optimize neural architecture representations. Experimental results on three popular NAS benchmarks show that CR-LSO achieves competitive evaluation results in terms of both computational complexity and architecture performance. \footnote{Codes are available at https://github.com/RaoXuan-1998/CR-LSO.}
\end{abstract}

\section{Introduction}
\label{sec:intro}
Many real-world problems, such as the function network design \cite{DBLP:conf/nips/AstudilloF21}, the functional protein prediction (e.g., the AlphaFOLD \cite{jumper2021highly}), etc., can be formulated as optimizing graphs that represent proper computational performance on given tasks. Neural architecture search (NAS) can also be described as a graph optimization problem since a neural architecture is a directed acyclic graph (DAG) from the view of data streams. Thus, finding an optimal architecture is equivalent to determining the optimal topology of the DAG. However, many efficient optimization methods, such as Bayesian optimization (BO), simulated annealing, and gradient-based optimization, which operate in continuous space primarily, are not applicable to the optimization of neural architectures directly due to the discreteness of graphs.

To optimize neural architectures in the continuous space, this paper resorts to a special technique called latent space optimization (LSO), where a deep generative model is trained to embed the discrete architecture into a relatively smooth latent space. In this way, the search space of NAS is transformed from discrete to continuous. The objective of LSO-based NAS can be formulated as  
\begin{align}
	\label{eq:lso}
	\begin{split}
		& {\rm max}_{z\in \mathcal{Z}} \ \ {\rm Performance} (\alpha) \\
		& {\rm s.t.} \ \ \ \alpha = {\rm Decode}(z) \ \ {\rm and} \ \ \alpha \in \mathcal{I},
	\end{split}
\end{align}
where $\mathcal{I}$ is the discrete search space, $\mathcal{Z}$ is the continuous latent space, and ${\rm Decode}(z)$ is the decoding function which converts the continuous representation $z$ to the discrete architecture $\alpha$. From \eqref{eq:lso}, we know that the architecture is searched in the continuous space $\mathcal{Z}$ by optimizing $z$ but is evaluated in the discrete space $\mathcal{I}$ by evaluating $\alpha$. Although some applications of LSO have received success in NAS tasks \cite{luo2018neural, DBLP:conf/nips/ZhangJCGC19, chatzianastasis2021graph}, most of them discussed how to design an encoder-decoder model to better learn the continuous representations of neural architectures, and only a few of them discussed the subsequent optimization method for architecture latent variables. 

It is worth pointing out that this paper focuses on improving the effectiveness of gradient-based LSO for architecture search \cite{luo2018neural}, where the gradient of the architecture performance with respect to the latent variable is estimated by a performance predictor. However, gradient-based optimization is challenging as follows. First, the mapping from the latent space to the architecture performance is generally non-convex. Although the similarity of different architectures can be measured directly in the Euclidean latent space of deep generative models, the performance mapping is not guaranteed to be a convex function, which raises challenges for the gradient-based optimization. Second, most conventional function approximators, e.g., the multi-layer perceptron (MLP), the graph neural network (GNN), the radial basis function (RBF), etc., are not convex from input to output. It means that even if the underlying performance mapping is convex, one may not obtain a convex predictor due to the non-convexity of function approximators. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.50\textwidth]{cr_lso/cr_lso.jpg}
	\caption{The conceptual visualization of the proposed CR-LSO. By using a graph variational autoencoder, CR-LSO transforms the discrete search space of NAS into a continuous latent space. Simultaneously, CR-LSO utilizes the guaranteed convexity of the ICNN to regularize the learning process of latent space, so as to obtain a convex architecture performance mapping, which makes the gradient-based LSO more effective.}
	\label{fig:cr_lso}
\end{figure}

Can the (approximate) optimality of gradient-based LSO be guaranteed? This paper tries to answer this question by developing a convexity regularized latent space optimization (CR-LSO) method, whose main process is shown in Figure \ref{fig:cr_lso}. First, the CR-LSO builds a graph variational autoencoder (G-VAE) to learn the continuous representations of discrete architectures. Simultaneously, the learning process of latent space is regularized by the guaranteed convexity of input convex neural networks (ICNNs) \cite{amos2017input}. In this way, the G-VAE has to learn a (approximate) convex performance mapping. Otherwise, the prediction loss of architecture performance cannot decrease to a sufficiently low level.\footnote{Note that the ICNN can approximate convex functions only. If the architecture performance mapping is not convex, the prediction loss will be large. Then, if the G-VAE expects to decrease the loss, the embedding method should be changed to make the underlying mapping convex. This indicates ``regularizing the learning process of latent space with the guaranteed convexity of the ICNN".} However, the learning process of convexity regularized latent space requires amounts of labeled architectures. To avoid this problem, a GNN-based predictor is built to predict the performance of unlabeled architectures. Thus, the learning process of latent space is indeed based on a semi-supervised manner. Hereafter, the gradient optimization of latent variables is executed with a convex performance predictor in a convex space. The main contributions of this paper are summarized as follows.
\begin{itemize}
	\item Neural architectures are searched in the continuous space by learning a G-VAE which embeds the discrete architecture into the latent space. In this cause, the similarity of neural architectures can be utilized to improve the search efficiency of NAS methods.
	\item The CR-LSO method, a convex architecture optimization framework, is developed to improve LSO-based NAS. By regularizing the learning process of the latent space of G-VAE using the guaranteed convexity of the ICNN, the architecture performance mapping is formalized as a convex function, which makes the gradient-based LSO more competitive.
	\item Experimental results on three NAS benchmarks, i.e., the NAS-Bench-101 \cite{DBLP:conf/icml/YingKCR0H19}, the NAS-Bench-201 \cite{DBLP:conf/iclr/Dong020}, and the NAS-Bench-301 \cite{siems2020bench}, show that the proposed CR-LSO achieves competitive NAS results compared to other state-of-the-art NAS methods and hyper-parameter optimization algorithms in terms of computational complexity and architecture performance.
\end{itemize}

\section{Related Works}
\subsection{Input convex neural network (ICNN)}
An ICNN is a scalar-valued neural network with constraints on network parameters such that the network mapping from input to output is a convex function. Originally, it was used to allow the convex inference in various problems, such as structured prediction, data imputation, and continuous action reinforcement learning \cite{amos2017input}. Furthermore, a convex optimal control for complex systems is achieved with implementing ICNNs \cite{chen2018optimal}, and the optimal transport between two distributions is improved by ICNNs \cite{DBLP:conf/icml/MakkuvaTOL20}. 
	\subsection{Neural architecture search (NAS)}
NAS aims to advance the representation learning of deep neural networks by automatically designing architectures that facilitate the performance on given tasks \cite{he2021automl, elsken2019neural}. The discreteness of neural architectures limits the direct employment of gradient-based optimization. This problem urged many NAS methods seeking for black-box optimization, such as reinforcement learning (RL) \cite{zoph2016neural, DBLP:journals/ivc/JaafraLDN19, DBLP:conf/icml/PhamGZLD18, DBLP:conf/aaai/ChengWZCW022}, EA \cite{real2019regularized, DBLP:conf/eccv/NingZZWY20, liu2021survey}, and BO \cite{zhou2019bayesnas, DBLP:conf/aaai/WhiteNS21, DBLP:conf/iclr/RuW0O21} in previous works. In the differentiable architecture search (DARTS) \cite{liu2018darts, xu2021partially, chen2021progressive, DBLP:conf/bmvc/Chu021}, the discrete search space is relaxed by replacing the operator selection with the softmax mixture of all candidate operators to realize gradient-based NAS. After the relaxation, the task of NAS is transformed into the joint optimization of architecture parameters and network weights. Different from DARTS-based NAS, a deep generative model is trained in LSO-based NAS to transform the search space from discrete to continuous \cite{DBLP:conf/nips/YanZAZ020}. In this case, neural architectures are searched in the continuous latent space and are evaluated in the discrete search space \cite{luo2018neural, DBLP:conf/nips/ZhangJCGC19, chatzianastasis2021graph}.
\section{Methodology}
This section describes the CR-LSO. The first part introduces the G-VAE which learns the continuous representations of discrete architectures. Then, the second part describes how to regularize the learning process of latent space to obtain a convex performance mapping. Finally, the gradient-based architecture optimization is presented.
\subsection{G-VAE}
As the basic work of LSO, the G-VAE transforms the search space of NAS from discrete to continuous in order to allow architecture optimization in the continuous space. The main framework of G-VAE is established based on the variational autoencoder (VAE) \cite{kingma2013auto, doersch2016tutorial} except that the G-VAE is designed for graph representation learning specially.

\subsubsection{Variational autoencoder}
Let $\alpha=(A, X, E)$ be the graph representation of an architecture $\alpha$, where $A$, $X$ and $E$ are adjacency matrix, node attributes, and edge attributes, respectively. For an architecture $\alpha$, the optimization objective of G-VAE is to learn its continuous representation $z\in \mathcal{Z}$ by learning an approximate posterior distribution $q_{\phi}(z|\alpha)$ (the encoder) and a generative model (the decoder) $p_{\theta}(\alpha|z)$, where $\phi$ and $\theta$ are learnable parameters of the encoder and decoder, respectively. This goal is achieved by Bayesian approximate inference, i.e., by minimizing the variational upper bound of the negative log-likelihood $-{\rm log} \; p(\alpha)$ as 
\begin{align}
	\label{eq:vae_loss}
	\begin{split}
		\mathcal{L}_{\phi, \theta}(\alpha) =   \ D_{\rm KL} \big( q_{\phi}(z|\alpha) \Vert p(z)\big) \\
		 + \ \mathbb{E}_{q_{\phi}(z|\alpha)} \big[ -{\rm log} \: p_{\theta}(\alpha|z) \big],
	\end{split}
\end{align}
where $D_{\rm KL} \big( q_{\phi}(z|\alpha) \Vert p(z)\big)$ is the Kullback–Leibler (KL) divergence between the posterior $q_{\phi}(z|\alpha)$ and a prior isotropic Gaussian distribution $p(z)$. 

\subsection{GNN-based encoder} 
The encoder of the G-VAE is an $L$-layer graph neural network (GNN) using the synchronous message passing scheme \cite{fey2019fast}. Let $N(v)$ be the neighborhood of node $v$ in $\alpha$. Denoting $\mathbf{x}_{v}^{(k-1)}$ as the feature vector of node $v$ in layer $(k-1)$ and $e_{uv}$ as the edge feature from $u$ to $v$, the updating function of node $v$ in layer $k$ is designed as
\begin{equation}
	\label{eq:nodeupdate}
	\mathbf{x}_{v}^{(k)} = \Theta^{(k)}\mathbf{x}_{v}^{(k-1)} + {\psi}^{(k)} \big( \mathbf{x}_{v}^{(k-1)}, \mathbf{h}_{v}^{(k)} \big),
\end{equation}
where the incoming messages $\mathbf{h}_{v}^{k}$ of node $v$ is calculated by an aggregate function as
\begin{equation}
	\label{eq:aggregate}
	\mathbf{h}_{v}^{(k)} = \sum_{u\in N(v)} {\varphi}^{(k)}(\mathbf{x}_{v}^{(k-1)}, \mathbf{x}_{u}^{(k-1)}, \mathbf{e}_{uv}).
\end{equation}
In \eqref{eq:nodeupdate} and \eqref{eq:aggregate}, $\Theta^{(k)}$ is a learnable parameter set, and both ${\psi}^{(k)}(\cdot)$ and ${\varphi}^{(k)}(\cdot)$ are MLPs with nonlinear activation functions. The first term of the right hand side of \eqref{eq:nodeupdate} $\Theta^{(k)}\mathbf{x}_{v}^{(k-1)}$ is the residual information of node updating, which allows us to build a deeper GNN encoder by avoiding the vanishing gradients \cite{DBLP:conf/cvpr/HeZRS16}. Since the aggregate function (\ref{eq:aggregate}) is invariant to the permutation of incoming nodes, the encoder of G-VAE embeds an architecture into the latent space injectively. In addition, \eqref{eq:aggregate} includes an extra edge feature, which allows us to deal with the operator-on-node (e.g., the NAS-Bench-101) and operator-on-edge (e.g., the NAS-Bench-301) search spaces consistently using the same encoder. The representation of an architecture is designed as the sum of feature vectors of all nodes as 
\begin{equation}
	\textbf{h}_{\alpha} = {\rm sum}\big( \big\{  \mathbf{h}_{v}^{L} | v\in V   \big\} \big).
\end{equation}
After calculating $\mathbf{h}_{\alpha}$, two linear layers are used to output the mean $\mu(\alpha)$ and the variance $\sigma(\alpha)$ of a normal distribution, which characterize the posterior distribution $q_{\phi}(z|\alpha)$.\footnote{As long as the posterior is close to the prior Gaussian, it is said that the search space of NAS has been transformed into a continuous one.}

\subsection{Decoder} 
The decoder is a generative model to generate discrete architectures given latent variables $z$. However, decoding is much more difficult than encoding due to the non-Euclidean property of graphs. For methods which generate a new graph in a sequential manner, a growing graph is constructed by adding nodes and edges iteratively \cite{DBLP:conf/nips/ZhangJCGC19, you2018graphrnn}. However, these methods have to design the generation rules of discrete graphs, which is not an easy task. Inspired by \cite{simonovsky2018graphvae}, an MLP-based probabilistic graph decoder is introduced in G-VAE to generate the entire graph at once. This method is effective if the generated graphs are small, e.g., the cell-based neural architectures. 

\subsection{Convexity regularized latent space}
Note that minimizing \eqref{eq:vae_loss} does not mean that the mapping from latent space to architecture performance is convex. Thus, the latent space has to be regularized somehow to lead the underlying performance mapping to convex. In this part, the guaranteed convexity of ICNNs is leveraged to regularize the learning process of latent space. 	

\subsubsection{ICNN}
Considering a $k$-layer MLP-like neural network $f_{\rm ICNN}(z)$, the architecture of an ICNN is defined as
\begin{equation}
	y_{i+1} = h_{i}\Big( W_{i}^{(y)}y_{i} + W_{i}^{(z)}z + b_{i} \Big), \: \ f_{\rm ICNN}(z) = y_{k},
\end{equation} 
where $i=0,...,k-1$, $y_{i}$ and $b_{i}$ are the activation output and the bias of the layer $i$, $\beta = \{ W_{0:k-1}^{(z)}, W_{0:k-1}^{(y)}, b_{0:k-1} \}$ is the learnable parameter set of ICNNs, and $h_{i}$ is nonlinear activation function. Specifically, $y_{0}$ and $W_{0}^{(y)}$ are set to zero. Then, for such a network, a lemma is necessary to be provided \cite{amos2017input, chen2018optimal}:

\emph{Lemma 1:} \emph{The function $f_{\rm ICNN}(z)$ is convex from input to output if all elements of $W_{1:k-1}^{y}$ are non-negative, and all activation functions $h_{i}$ are convex and non-decreasing (e.g., ReLU and LeakyReLU).}

\subsubsection{Regularizing the learning process of latent space}
To regularize the learning process of latent space, an ICNN-based predictor is used to predict the performance of neural architectures and is optimized together with the G-VAE. Then, the joint loss function of G-VAE and ICNN is 
\begin{align}
	\label{eq:vae_loss_all}
	\mathcal{L}_{\phi, \theta, \beta} & (\alpha,s) =   D_{\rm KL}\big( q_{\phi}(z|\alpha)\vert\vert  p(z) \big) + \nonumber \\ & \mathbb{E}_{q_{\phi}(z|\alpha)} \big[ -{\rm log} \ p_{\theta}(\alpha|z) + \big(s - f_{\rm ICNN}(z)\big)^{2}  \big],
\end{align}
where $s$ is the performance of architecture $\alpha$ on a particular task (e.g., the accuracy in image classification). 

Note that, the convexity of ICNN and the additional prediction loss $\big(s-f_{\rm ICNN}(z)\big)^{2}$ force the G-VAE to learn a convex performance mapping in the latent space. If the underlying performance mapping is a non-convex function, the prediction loss will not decrease to a sufficiently low level because the ICNN predictor can only approximate a convex function. In this case, in order to decrease the prediction loss, the G-VAE has to optimize the parameters of encoder and decoder to learn an appropriate embedding method which embeds the discrete architectures into a continuous convex space. 

From the above analysis, we know that the supervised training of G-VAE requires the performance of architectures. However, when a small number of labeled architectures are provided only, it is difficult for the G-VAE to learn the continuous representations of discrete architectures.\footnote{Generally speaking, the training of G-VAE needs amounts of unlabeled architectures. Otherwise, the G-VAE cannot learn the effective continuous representations of architectures and cannot decode the valid discrete architectures when given the latent variables.} To tackle this problem, we leverage a semi-supervised approach, where a GNN predictor $g_{\rm GNN}(\alpha)$ is used to predict the architecture based on the discrete $\alpha$ to predict the performance of unlabeled architectures. 

Overall, the training process of G-VAE is summarized in Algorithm \ref{alg:gvae}.
\begin{algorithm}[htbp]
\caption{The training process of G-VAE}
{\textbf{Input:} The labeled architecture set $\mathcal{D} = \{(\alpha_{i},s_{i})\}$}, the G-VAE, the ICNN $f_{\rm ICNN}(z)$ and the GNN predictor $g_{\rm GNN}(\alpha)$. \\
{\textbf{Begin:}}
\begin{algorithmic}[1]
		\STATE{Using $\mathcal{D}$, train $g_{\rm GNN}(\alpha)$ until convergence to predict the performance of unlabeled architectures.} \\
		\STATE{Construct a pseudo-labeled set $\mathcal{D'} = \{(\alpha_{i}',s'_{i})\}$ with $|\mathcal{D'}| \gg |\mathcal{D}|$ and $s'_{i} =  g_{\rm GNN}(\alpha_{i}')$ }. \\
		\STATE{Construct an enlarged architecture set $\mathcal{D}_{\rm Big} = \mathcal{D} \cup \mathcal{D'}$ }. \\
		\STATE{Using $\mathcal{D}_{\rm Big}$, train the G-VAE and $f_{\rm ICNN}(z)$ by minimizing \eqref{eq:vae_loss_all} until convergence.}
		 
	\end{algorithmic}
	\textbf{Output}: The trained G-VAE and $f_{\rm ICNN}(z)$ . 
	\label{alg:gvae}
\end{algorithm}
\subsection{Convex Neural architecture optimization}
\subsubsection{Architecture inference.} 
The inference for preferable architectures is executed in the latent space $\mathcal{Z}$. Given an initial discrete architecture $\alpha$, we first obtain its continuous representation $z$ by the encoder. Note that $z$ is not the optimal point from the view of the predictor $f_{\rm ICNN}(z)$. Then, by moving $z$ along the gradient direction of $f_{\rm ICNN}$, we obtain a probably better architecture representation $z'$ as
\begin{equation}
	\label{eq:gradient_ascent}
	z' = z + \eta \frac{\partial f_{\rm ICNN}}{\partial z},
\end{equation}
where $\eta$ is a reasonably small step size.\footnote{In the architecture optimization phase, random noise is injected to the architecture representation $z$ to explore the search space, i.e., $z \leftarrow z + \varepsilon \mathcal{N}(0,I)$, where $\varepsilon$ is the parameter to control the variance.} The process of \eqref{eq:gradient_ascent} can be performed multiple times by iteratively assigning a new $z$, thereby we obtain a set of new architecture representations $\{ z' \}$. Given each $z'$, we get a discrete architecture by the decoder. Note that, $\alpha'$ is better than $\alpha$ at least in the prediction of $f$, and this is the way to continuously obtain (probably) better architectures. 
\subsubsection{Approaching the performance highland.}
Although $\alpha'$ is better than $\alpha$ from the view of the predictor, the conclusion may not hold in practice, since $f_{\rm ICNN}(z)$ is a rough predictor based on the limited and noisy labeled architecture set. To obtain an accurate predictor, a natural way is to sample countless architectures from the search space and evaluate their performance on given tasks. However, it is impracticable when the algorithm is executed in a huge search space such as the DARTS \cite{liu2018darts}, where the quantity of candidate architectures reaches $10^{18}$. 

Do we really need an accurate predictor throughout the whole search space? For the purpose of architecture optimization, the answer is No, since it is not necessary to consider architectures with poor performance. Note that we are interested in high-performance architectures only, thus it is enough as long as the predictor is accurate in the performance highland. Nonetheless, it requires the NAS algorithms to continuously explore better architectures until a satisfactory architecture is obtained. However, the performance highland cannot be approached at one stroke by a static, rough prediction model, thus it should be completed by alternately fine-tuning the prediction model and collecting preferable candidates by step-by-step architecture inference. Along with the above analysis, the implementation of CR-LSO is summarized in Algorithm \ref{alg:cr_lso}. 

\begin{algorithm}[htbp]
	\caption{The implementation of CR-LSO}
	{\textbf{Input:} The search space $\mathcal{I}$, the initial evaluation number $Q_{\rm start}$, the maximum evaluation number $Q_{\rm max}$}, the size $K$ of seed set for architecture optimization. \\
	{\textbf{Begin:}}
	\begin{algorithmic}[1]
		\STATE{Construct a labeled architecture set $\mathcal{D}=\{ (\alpha_{i}, s_{i}) \}$ where $\alpha_{i}$ is sampled from $\mathcal{I}$ randomly, $s_{i}$ is the performance of $\alpha_{i}$, and $|\mathcal{D}| = Q_{\rm start}$.}  \\
		\STATE{Obtain the G-VAE and $f_{\rm ICNN}(z)$ by Algorithm \ref{alg:gvae}.} \\
		\WHILE{$|\mathcal{D}| < Q_{\rm max}$}
		\STATE{ Obtain the equivalent set $\tilde{\mathcal{D}} = \{(z_{i},s_{i}) \}$ with $z_{i}$ the continuous representation of $\alpha_{i}$.} \\
		\STATE{ Fine-tune the $f_{\rm ICNN}(z)$ by minimizing the prediction loss on  $\tilde{\mathcal{D}}$.} \\
		\STATE{ Obtain the continuous seed set $\mathcal{S}_{\rm seed} = \{ z_{j} \}$ with $|\mathcal{S}_{\rm seed}| = K$, where $z_{j}$ is the continuous representation of the top-$j$ architecture in $\mathcal{D}$.} 
		\STATE{ For each $z\in \mathcal{S_{\rm seed}}$, obtain a better representation $z'$ by \eqref{eq:gradient_ascent}, and obtain the discrete architecture $\alpha'$ by the decoder of G-VAE. If $\alpha'$ is not new to $\mathcal{D}$, increase the step size $\eta$ of \eqref{eq:gradient_ascent} until a new $\alpha'$ is obtained.}
		\STATE{ For each new $\alpha'$, evaluate its performance $s'$ on the given task, and add $(\alpha', s')$ to $\mathcal{D}$.}
		\ENDWHILE
	\end{algorithmic}
	\textbf{Output}: The best architecture $\alpha^{*}$ in $\mathcal{D}$. 
	\label{alg:cr_lso}
\end{algorithm}
\section{Experiments}
In this section, the effectiveness of CR-LSO is demonstrated on three NAS benchmarks, which are NAS-Bench-101 \cite{DBLP:conf/icml/YingKCR0H19}, NAS-Bench-201 \cite{DBLP:conf/iclr/Dong020}, and NAS-Bench-301 \cite{siems2020bench}. 

The architectures of these search spaces are represented by sparse graphs \cite{fey2019fast}. The encoder of G-VAE is a 3-layer GNN with 512 channels (exception: 256 channels in NAS-Bench-101). The dimension of latent space is 8, 64 and 128 on NAS-Bench-101, 201 and 301, respectively, based on the architecture complexity of different search spaces. The ICNN predictor is set to 3-layer with 256 hidden neurons. The decoder of G-VAE, which is a 3-layer MLP with 512 hidden neurons, outputs the predictive node types, edge connections, and edge types of neural architectures at once. The semi-supervised predictor is the same as the encoder, but is followed by a MLP to output the predicted accuracy. All these models are optimized by an Adam optimizer with the initial learning rate $1e^{-4}$. More training details are included in the Appendix \ref{implementation details}. 
\subsection{Architecture search in NAS-Bench-101}
\begin{table}[htbp]
	\centering
	\caption{Comparison of NAS evaluations between the CR-LSO and the state-of-the-art methods on NAS-Bench-101. The means and standard deviations are reported ($\%$). }
	\begin{tabular}{lccc}
		\toprule
		Methods & Query & Test Acc. & Ranking \\
		\midrule
		RE \cite{real2019regularized} & 2000 & 93.96 $\pm$ 0.05 & 89 \\
		NAO \cite{luo2018neural} & 300 & 93.69 $\pm$ 0.06 & 1191 \\
		NAO \cite{luo2018neural} & 2000 & 93.90 $\pm$ 0.03 & 169 \\
		Yehui $et \ al.$ \cite{DBLP:conf/cvpr/TangWXCSXX0X20}  & - & 94.01 $\pm$ 0.12 & 47 \\
		SemiNAS \cite{DBLP:conf/nips/Luo0WQCL20} & 300 & 93.89 $\pm$ 0.06 & 197 \\
		SemiNAS \cite{DBLP:conf/nips/Luo0WQCL20} & 2000 & 94.02 $\pm$ 0.05 & 43 \\
		ReNAS \cite{DBLP:conf/cvpr/Xu00TJX021} & 4236 & 93.95 $\pm$ 0.11 & 93 \\ 
		\midrule
		CR-LSO (ours) & 5000 & \textbf{94.23 $\pm$ 0.00} & \textbf{2} \\ 
		CR-LSO (ours) & 2000 & 94.22 $\pm$ 1e-5 & 3 \\ 
		CR-LSO (ours) & 1000 & 94.15 $\pm$ 1e-3 & 8\\
		CR-LSO (ours) & 500 & 93.97 $\pm$ 2e-3 & 72 \\ 
		\bottomrule
	\end{tabular}
	\label{tab:nas_ben_101}
\end{table}
The NAS-Bench-101 employs a cell-based search space which contains 423,624 architectures. All architectures are trained on CIFAR10 for 3 times and their performances are saved in a look-up table. Thus, querying the accuracy of an architecture in the table is equivalent to training and evaluating the architecture. 

To test the effectiveness of CR-LSO under different computational complexity, 4 different $Q_{\rm max}$ are used, which are 5000, 2000, 1000 and 500. The corresponding $Q_{\rm start}$ are 4000, 1500, 600 and 300, respectively. The size of seed set for architecture optimization $K=10$. All these experiments are run for 16 times independently. The architecture with the highest test accuracy among the validation top-50 architectures is reported. Table \ref{tab:nas_ben_101} shows the performance comparison in terms of query number. It can be seen that the proposed CR-LSO achieves the optimal NAS results. For example, when $Q_{\max} = 2000$, CR-LSO can find out the average top-3 architecture steadily. In addition, it is impressive that the standard deviation of CR-LSO is small extraordinary, which may benefit from the robustness of convexity-guaranteed gradient optimization.
\subsubsection{Latent space visualization with PCA}
\begin{figure}
	\centering
	\begin{subfigure}{0.499 \linewidth}
		\includegraphics[width=1.0\textwidth]{pca/unconstraied_space.jpg}
		\caption{Unconstrained latent space.}
		\label{fig:pca-a}
		\end{subfigure}
	\hfill
		\begin{subfigure}{0.48 \linewidth}
		\includegraphics[width=1.0\textwidth]{pca/convexity_space.jpg}
		\caption{Convexity regularized latent space.}
		\label{fig:pca-b}
	\end{subfigure}
	\caption{The PCA visualizations of unconstrained and convexity regularized latent spaces in NAS-Bench-101. The architectures with blue colors own higher rankings than those with red colors.}
	\label{fig:pca}
\end{figure} 
\begin{table*}[htbp]
	\centering
	\caption{Comparison of NAS evaluation between the CR-LSO and the state-of-the-art methods on NAS-Bench-201. The means and standard deviations are reported ($\%$). }
	\begin{tabular}{lcccccc}
		\toprule
		{} & \multicolumn{2}{c}{CIFAR10} & \multicolumn{2}{c}{CIFAR100} & \multicolumn{2}{c}{ImageNet16-120}   \\
		\cline{2-7} \specialrule{0em}{1.5pt}{2.0pt}
		Methods & Validation & Test & Validation & Test & Validation & Test  \\
		\midrule
		DARTS \cite{liu2018darts} & 39.77 $\pm$ 0.00 & 54.30 $\pm$ 0.00 & 15.03 $\pm$ 0.00 & 15.61 $\pm$ 0.00 & 16.43 $\pm$ 0.00 & 16.32 $\pm$ 0.00 \\
		ENAS \cite{DBLP:conf/icml/PhamGZLD18} & 37.51 $\pm$ 3.19 & 53.89 $\pm$ 0.58 & 13.37 $\pm$ 2.35 & 13.96 $\pm$ 2.33 & 15.06 $\pm$ 1.95 & 14.84 $\pm$ 2.10 \\
		GDAS \cite{DBLP:conf/cvpr/DongY19} & 89.89 $\pm$ 0.08 & 93.61 $\pm$ 0.09 & 71.34 $\pm$ 0.04 & 70.70 $\pm$ 0.30 & 41.59 $\pm$ 1.33 &  41.71 $\pm$ 0.98 \\
		RS \cite{bergstra2012random} & 90.93 $\pm$ 0.36 & 93.70 $\pm$ 0.36 & 70.93 $\pm$ 1.09 & 71.04 $\pm$ 1.07 & 44.45 $\pm$ 1.10 & 44.57 $\pm$ 1.25 \\
		REINFORCE \cite{williams1992simple} & 91.09 $\pm$ 0.37 & 93.85 $\pm$ 0.37 & 71.61 $\pm$ 1.12 & 71.71 $\pm$ 1.09 & 45.05 $\pm$ 1.02 & 45.24 $\pm$ 1.18 \\
		\midrule
		BOHB \cite{DBLP:conf/icml/FalknerKH18} & 90.82 $\pm$ 0.53 & 93.61 $\pm$ 0.52 & 72.59 $\pm$ 0.82 & 72.37 $\pm$ 0.90 & 45.44 $\pm$ 0.70 & 45.26 $\pm$ 0.83 \\
		ReNAS \cite{DBLP:conf/cvpr/Xu00TJX021} & 90.90 $\pm$ 0.31 & 93.99 $\pm$ 0.25 & 71.96 $\pm$ 0.99 & 72.12 $\pm$ 0.79 & 45.85 $\pm$ 0.47 & 45.97 $\pm$ 0.49 \\
		FairNAS \cite{DBLP:conf/iccv/Chu0X21} & 90.07 $\pm$ 0.57 & 93.23 $\pm$ 0.18 & 70.94 $\pm$ 0.94 & 71.00 $\pm$ 1.46 & 41.90 $\pm$ 1.00 & 42.19 $\pm$ 0.31 \\
		arch2vec-BO \cite{DBLP:conf/nips/YanZAZ020} & 91.41 $\pm$ 0.22 & 94.18 $\pm$ 0.24 & 73.35 $\pm$ 0.32 & 73.37 $\pm$ 0.30 & 46.34 $\pm$ 0.18 & 46.27 $\pm$ 0.37 \\
		CR-LSO (ours) & \textbf{91.54 $\pm$ 0.05} & \textbf{94.35 $\pm$ 0.05} & \textbf{73.44 $\pm$ 0.17} & \textbf{73.47 $\pm$ 0.14} & \textbf{46.51 $\pm$ 0.05} & \textbf{46.98 $\pm$ 0.35} \\
		\midrule
		Optimal & 91.61 & 94.37 & 73.49 & 73.51 & 46.77 & 47.31 \\
		\bottomrule
	\end{tabular}
	\label{tab:nas_bench_201}
\end{table*}
To illustrate the influence of convexity regularization of ICNN, we visualize the unconstrained and the convexity regularized latent spaces respectively in Figure \ref{fig:pca} by projecting the architecture representations into a two-dimensional space using the PCA algorithm. It can be seen that without any constraints, the architectures with high and poor performances are highly mixed in the two-dimensional space, which means that the G-VAE has not learned a convex architecture performance mapping (Figure \ref{fig:pca-a}). However, with the convexity regularization, the location difference of high-performance and poor-performance architectures is obvious, which means that the G-VAE has learned an approximate convex mapping in the latent space (Figure \ref{fig:pca-b}). 
\subsection{Architecture search in NAS-Bench-201}
NAS-Bench-201 is a small NAS benchmark with 15,625 different architectures and their corresponding train, validation, and test accuracies on CIFAR10, CIFAR100, and ImagetNet16-120 datasets. We set $Q_{\rm max} = 500$ and $K=5$ to verify the effectiveness of CR-LSO. Correspondingly, we set $Q_{\rm start} = 300$ to train the GNN predictor and G-VAE. The experiments are run for 32 times independently and the evaluation results are summarized in Table \ref{tab:nas_bench_201}. It can be seen that for all datasets, CR-LSO achieves the highest validation and test accuracies. What's more, the standard derivations of CR-LSO evaluations are significantly lower than other state-of-the-art methods. 
\subsubsection{Architecture similarity in the latent space}
\begin{figure}
	\centering
	\begin{subfigure}{1.0 \linewidth}
		\includegraphics[width=1.0\textwidth]{cr_lso/unsupervised_similarity_2.jpg}
		\caption{Architecture similarity in unconstrained latent space.}
		\label{fig:short-a}
	\end{subfigure}
	\hfill
	\begin{subfigure}{1.0 \linewidth}
		\includegraphics[width=1.0\textwidth]{cr_lso/semisupervised_similarity_2.jpg}
		\caption{Architecture similarity in convexity regularized latent space.}
		\label{fig:short-a}
	\end{subfigure}
	\caption{The cosine similarity of architecture latent representations in the unconstrained latent space (top) and the convexity regularized latent space (bottom) in NAS-Bench-201. Left: The architecture similarity among the optimal 100 architectures. Right: The architecture similarity between the optimal 100 and the worst 100 architectures. }
	\label{fig:short}
\end{figure} 
In this part, we test whether the architectures with similar performance are embedded into similar positions in the latent space. We pick up the optimal 100 and the worst 100 architectures from NAS-Bench-201 and visualize their cosine similarity of latent representations in Figure \ref{fig:short}. We can see that no matter in the unconstrained or the convexity regularized latent spaces, the similarity of the optimal 100 architectures is higher than that between the top 100 and the worst 100 architectures. However, with the convexity regularization of CR-LSO, the high-performance architectures share higher similarity among themselves and share lower similarity with the worst ones. 
\subsection{Architecture search in NAS-Bench-301}
\begin{table*}[htbp]
	\centering
	\caption{Comparison of the average accuracy of top-5 architectures searched by CR-LSO and other hyper-parameter optimizations methods on NAS-Bench-301. The means and standard deviations are reported ($\%$).}
	\begin{tabular}{llccccc}
		\toprule
		{} & \textsc{{}} & \multicolumn{5}{c}{Query numbers of architectures}   \\
		\cline{3-7} \specialrule{0em}{1.5pt}{2.0pt}
		Search spaces & Methods &  200 & 400 & 800 & 1600 & 3200  \\
		\midrule
		Discrete & US & 94.16 $\pm$ 0.02 & 94.22 $\pm$ 0.05 & 94.29 $\pm$ 0.03 & 94.35 $\pm$ 0.04 & 94.41 $\pm$ 0.03 \\   
		\midrule
		\multirow{3}{*}{LS} & RS \cite{bergstra2012random} & 94.04 $\pm$ 0.06 & 94.19 $\pm$ 0.08 & 94.30 $\pm$ 0.05 & 94.36 $\pm$ 0.08 & 94.40 $\pm$ 0.05  \\
		{} & TPE \cite{bergstra2013making} & 94.42 $\pm$ 0.07 & 94.51 $\pm$ 0.07 & 94.63 $\pm$ 0.10 & 94.70 $\pm$ 0.10 & 94.77 $\pm$ 0.06 \\
		{} & CMA-ES \cite{nomura2021warm} & 94.34 $\pm$ 0.05 & 94.59 $\pm$ 0.15 & 94.83 $\pm$ 0.09 & 94.89 $\pm$ 0.08 & 94.92 $\pm$ 0.06 \\
		\midrule 
		\multirow{5}{*}{CR-LS} & RS \cite{bergstra2012random} & 94.17 $\pm$ 0.06 & 94.23 $\pm$ 0.05 & 94.31 $\pm$ 0.06 & 94.35 $\pm$ 0.04 & 94.42 $\pm$ 0.03 \\ 
		{} & TPE \cite{bergstra2013making} & 94.50 $\pm$ 0.06 & 94.65 $\pm$ 0.03 & 94.76 $\pm$ 0.04 & 94.81 $\pm$ 0.09 & 94.88 $\pm$ 0.06 \\ 
		{} & CMA-ES \cite{nomura2021warm} & 94.37 $\pm$ 0.09 & 94.60 $\pm$ 0.06 & 94.82 $\pm$ 0.08 & 94.93 $\pm$ 0.08 & 94.98 $\pm$  0.08 \\ 
		{} & NAO \cite{luo2018neural} & 94.49 $\pm$ 0.01 & 94.55 $\pm$ 0.04 & 94.72 $\pm$ 0.08 & 94.71 $\pm$ 0.03 & 94.75 $\pm$ 0.04 \\ 
		{} & CR-LSO (Ours) & \textbf{94.53 $\pm$ 0.04} & \textbf{94.80 $\pm$ 0.06} & \textbf{94.89 $\pm$ 0.06} & \textbf{94.94 $\pm$ 0.08} & \textbf{94.98 $\pm$ 0.02} \\ 
		\bottomrule
	\end{tabular}
	\label{tab:top-5_performance}
\end{table*}
In this section, we further verify the transfer ability of CR-LSO in a much larger NAS benchmark, i.e., the NAS-bench-301 \cite{siems2020bench}, which is a surrogate benchmark  based on the search space of DARTS \cite{liu2018darts} and contains about $10^{18}$ architectures.
\subsubsection{Baselines and experiment setup}
For fair comparison, three hyper-parameter optimization algorithms are employed as baselines including random search (RS) \cite{bergstra2012random}, BO-based tree-structured Parzen Estimator (TPE) \cite{bergstra2013making,ozaki2020multiobjective}, and EA-based covariance matrix adaptation evolution strategy (CMA-ES) \cite{nomura2021warm, hansen2016cma}. These baselines are employed under the framework of Optuna \cite{akiba2019optuna}. In addition, NAO \cite{luo2018neural} is applied as a baseline of gradient-based LSO. The uniform sampling (US) of the discrete search space is used as the lower bound of all these methods. In particular, $K=10$ and $\rho = Q_{\rm start}/Q_{\rm max} = 0.5$ are unchanged for NAO and CR-LSO. To verify whether a convexity regularized latent space (CR-LS) can promote the downstream optimization strategies, we also test the ability of RS, BO and CMA-ES in the constrained latent space (denoted by LS). In particular, we set $Q_{\rm max} = 200, 400, 800, 1600$ and $3200$ to test the performance of these methods under different computational complexity. The means and standard deviation of the average accuracy of the top-5 architectures are recorded. For each experiment, the result is calculated by five independent runs. The statistical results are summarized in Table \ref{tab:top-5_performance}.
\subsubsection{Analysis.} 
\emph{\textbf{Observation 1}: CR-LS may promote some downstream optimization strategies.} The average accuracy of TPE in LS is higher than that in CR-LS. It means that the approximate convex mapping of CR-LS assists the TPE to find a better solution. Nevertheless, EA-based CMA-ES obtains similar performance in both LS and CR-LS, which implies that it is robust to the structure of latent space.

\emph{\textbf{Observation 2}: CR-LSO is superior to almost all listed methods in terms of sample efficiency and architecture performance.} When $Q_{\rm max} = 200$, CR-LSO achieves an average accuracy of $97.53\%$, which is slightly higher than TPE and NAO and much higher than CMA-ES. The performance of CR-LSO is improved dramatically when $Q_{\rm max} = 400$. It obtains the highest average accuracy of $97.80\%$, which significantly outperforms all listed methods. As $Q_{\rm max}$ continuously increases, CR-LSO makes no compromises on architecture performance though CMA-ES has shown strong competitiveness. 
\subsubsection{Visualization of t-SNE projection.} \begin{figure}[htbp]
	\centering
	\includegraphics[width=0.42\textwidth]{tnse/tnse_2d_all.jpg}
	\includegraphics[width=0.40\textwidth]{tnse/tnse_2d.jpg}
	\caption{Two-dimensional t-SNE projection of latent representations of architectures searched by different methods. Architectures with lower rankings (darker colors) own higher performances on NAS-Bench-301.}
	\label{fig:tnse_2d}
\end{figure}
In Figure \ref{fig:tnse_2d}, the architectures searched by different methods ($Q_{\rm max} = 3200$) are visualized by projecting the latent representations of architectures into the two-dimensional space using the t-SNE algorithm \cite{van2008visualizing}. The architectures with darker colors own better rankings. Assuming that the main characteristics of these latent representations have been preserved, we analyze the distribution difference of architectures explored by different methods. It can be seen that the architecture distributions of RS and US are quite similar in both the distribution shape and architecture performance, which matches the results in Table \ref{tab:top-5_performance}. NAO has a similar distribution shape to RS and US, but it obtains better architecture rankings because it moves the representations toward better regions along the approximated gradients. But due to the non-convexity of conventional neural networks, it falls into the local optimal regions. Benefiting from the convexity-guaranteed gradient optimization, CR-LSO escapes from the local optimal region and explores more competitive architectures. The architectures of CR-LSO are mainly located at the boundary of the main point cloud, which might because of the step-by-step optimization of gradients. Derivative-free methods, e.g., TPE and CMA-ES, however, share less similarity with gradient-based methods and explore architectures out of the main distribution of search space. Nevertheless, only CMA-ES explores architectures with similar competitiveness as CR-LSO.   
\subsubsection{Hyper-parameter sensitivity evaluation}
\begin{table}[htbp]
	\centering
	\caption{The sensitivity evaluation of $\rho = Q_{\rm start}/Q_{\rm max}$}.
	\begin{tabular}{cccc}
		\toprule
		{} & \multicolumn{3}{c}{$\rho = Q_{\rm start}/Q_{\rm max}$} \\
		\cline{2-4} \specialrule{0em}{1.5pt}{2.0pt}
		$Q_{\rm max}$ & 25$\%$ & 50$\%$ & 75$\%$ \\
		\midrule
		400 & 94.70 $\pm$ 0.03 & 94.77 $\pm$ 0.06 & 94.72 $\pm$ 0.06 \\ 
		800 & 94.85 $\pm$ 0.07 & 94.86 $\pm$ 0.12 & 94.83 $\pm$ 0.09 \\ 
		1600 & 94.97 $\pm$ 0.04 & 94.95 $\pm$ 0.06 & 94.94 $\pm$ 0.05 \\
		3200 & 95.02 $\pm$ 0.03 & 95.00 $\pm$ 0.07 & 94.95 $\pm$ 0.05 \\
		\bottomrule
	\end{tabular}
	\label{tab:proportion_effect}
\end{table}
In this part, the sensitivity of CR-LSO to some hyper-parameters is evaluated. The influence of the training set proportion for building the initial predictor ($\rho = Q_{\rm start}/Q_{\rm max}$) is analyzed by setting $\rho = 25\%$, $50\%$ and $75\%$. All experiments are run five times independently. Here, $K=10$ remains unchanged. From the results summarized in Table \ref{tab:proportion_effect}, we see that for $\rho =25\%$, $50\%$ and $75\%$, CR-LSO achieves similar accuracy, which means that it is robust to the change of $\rho$. 
\begin{table}[htbp]
	\centering
	\caption{The sensitivity evaluation of hyper-parameter $K$.}
	\begin{tabular}{cccc}
		\toprule
		{} & \multicolumn{3}{c}{The number of $K$} \\
		\cline{2-4} \specialrule{0em}{1.5pt}{2.0pt}
		$Q_{\rm max}$ & 5 & 10 & 15 \\
		\midrule
		200 & 94.57 $\pm$ 0.05 & 94.59 $\pm$ 0.05 & 94.50 $\pm$ 0.09 \\
		400 & 94.71 $\pm$ 0.06 & 94.77 $\pm$ 0.06 & 94.73 $\pm$ 0.06 \\ 
		800 & 94.87 $\pm$ 0.02 & 94.86 $\pm$ 0.12 & 94.88 $\pm$ 0.04 \\
		1600 & 94.96 $\pm$ 0.05 & 94.95 $\pm$ 0.06 & 94.90 $\pm$ 0.09 \\
		3200 & 95.00 $\pm$ 0.05 & 95.00 $\pm$ 0.07 & 94.99 $\pm$ 0.02 \\
		\bottomrule
	\end{tabular}
	\label{tab:k_influence}
\end{table}
Next, the influence of different $K$ for architecture inference is analyzed by setting $K=5$, $10$ and $15$. Similarly, all experiments are run five times independently. Here, $\rho = 0.5$ remains unchanged. From the results summarized in Table \ref{tab:k_influence}, we see that for $K=5$, $10$ and $15$, CR-LSO achieves similar performance, which means that it is robust to the change of $K$.


\section{Conclusion}
In this paper, neural architectures are searched in a continuous space by learning a G-VAE which embeds the discrete architectures into the latent space. Simultaneously, to improve the gradient-based LSO, the CR-LSO approach is developed to obtain a convex architecture performance mapping, which is achieved by regularizing the learning process of latent space with the guaranteed convexity of ICNNs. The evaluation results compared with other state-of-the-art NAS methods and hyper-parameter optimization algorithms on three popular NAS benchmarks demonstrate the effectiveness of the proposed CR-LSO in terms of computational complexity and architecture performance. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\section{More implementation details.}
\label{implementation details}
\subsection{NAS-Bench-101}
As the first rigorous NAS benchmark, the NAS-Bench-101 uses a cell-based search space with 423,624 architectures, each of which consists of at most 7 nodes and at most 9 edges \cite{DBLP:conf/icml/YingKCR0H19}. In fact, the NAS-Bench-101 has an operator-on-node search space, where a node denotes a specific operator and an edge denotes a connection between two distinct nodes. For example, the first and final nodes denote the input and output nodes, respectively, and other nodes denote the computational operators such as $1\times1$ convolution, $3\times3$ convolution and $3\times3$ max-pooling. Edges of NAS-Bench-101 denote the information flows between nodes and are not so meaningful compared with those of NAS-Bench-201 and NAS-Bench-301. We represent architectures of NAS-Bench-101 using sparse graphs \cite{fey2019fast} whose nodes denote types of operators and edges denote the node connections. Since there is no edge feature for NAS-Bench-101 architectures, we let $e_{uv}=0$.  
\subsection{NAS-Bench-201}
The NAS-Bench-201 is another popular cell-based NAS benchmark which contains 15,625 unique architectures \cite{DBLP:conf/iclr/Dong020}. Different from NAS-Bench-101, NAS-Bench-201 employs an operator-on-edge strategy where nodes denote the feature tensors of neural networks and edges denote the computational operators. Each cell consists of 4 nodes (including an input node, an output node, and three intermediate nodes) and 5 operators (including $1\times1$ convolution, $3\times3$ convolution, $3\times3$ average pooling, skip connection and zero). The training, validation and test accuracies of each architecture are provided for three datasets including CIFAR10, CIFAR100 and ImageNet16-120. When representing an architecture of NAS-Bench-201 using the sparse graph, we denote the node attributes as the node orders of the architecture and the edge attributes as the operator types. 
\subsection{DARTS search space and NAS-Bench-301}
The DARTS search space \cite{liu2018darts}  which obtains about $10^{18}$ architectures is one of the most popular NAS spaces. The architecture configuration is represented by two types of cells, namely normal cell and reduction cell, each of which has 7 nodes (including 2 input nodes, 4 intermediate nodes and 1 output node). Each intermediate node has two incoming edges from previous nodes, and each edge takes one of the 7 operators including $3\times3$ separable convolution, $5\times5$ separable convolution, $3\times3$ dilated convolution, $5\times5$ dilated convolution, $3\times3$ average pooling, $3\times3$ max pooling skip connections. However, the search space of DARTS is so large that it is impracticable to construct a tabular-based NAS benchmark just like NAS-Bench-101 and NAS-Bench-201. In this background, the NAS-Bench-301 is proposed by building a surrogate-based NAS benchmark to cover the search space of DARTS \cite{siems2020bench}. Specifically, NAS-Bench-301 trains 60K architecture sampled from the DARTS search space and build the surrogate benchmark by XGBoost and graph isomorphism networks (GINs). Then, by the surrogate NAS benchmark, the performance of neural architectures can be evaluated by querying the surrogate model directly, which removes the constraints in exhaustively evaluating the search space. Similar to NAS-Bench-201 experiments, we denote the node attributes of a graph as the node orders of the corresponding architecture and the edge attributes as the operator types. Since the normal cell and reduction cell are the same in meta-topology, we train a cell-based G-VAE to capture the continuous representation of a cell and concatenate the representations of the normal cell and the reduction cell together as the whole representation of the architecture.
\subsection{The training of G-VAE} 
The architectures of NAS-Bench-101, NAS-Bench-201, and NAS-Bench-301 benchmarks are represented by sparse graphs \cite{fey2019fast}. The encoder of G-VAE is a 3-layer GNN with 512 channels (exception: 256 channels in NAS-Bench-101 due to the small scale). The dimension of latent space is 8, 64 and 128 on NAS-Bench-101, NAS-Bench-201 and NAS-Bench-301, respectively, based on the architecture complexity of different search spaces. The predictor used to regularize the learning process of latent space is a 3-layer ICNN with 256 hidden neurons. The decoder of G-VAE, which is a 3-layer MLP with 512 hidden neurons, outputs the predictive node types, edge connections, and edge types (exception: for NAS-Bench-101, there is no edge types need to be predicted) of neural architectures at once. The semi-supervised predictor is the same as the encoder but is followed by an MLP to output the predictive scalar accuracy. The semi-supervised predictor is trained for 200 epochs by an Adam optimizer with $\beta_{1} = 0.0$, $\beta_{2} = 0.5$ (not to be confused with the $\beta$ of ICNNs), batch size 32, and initial learning rate $1e^{-4}$. The learning rate is annealed to zero following a cosine scheduler. After training, the parameters of semi-supervised learning are frozen. Then, we collect the unlabeled architectures by sampling neural architectures in the corresponding search space and label them using the semi-supervised predictor. For NAS-Bench-101 and NAS-Bench-201, all architectures in the search space are used to train the G-VAE. For NAS-Bench-301, 50,000 architectures are sampled from the search space randomly and used to train the G-VAE. The G-VAE and the ICNN predictor are trained for 200 epochs by an Adam optimizer with same hyper-parameters as the semi-supervised predictor except the batch size 512. After training, the parameters of the encoder and decoder of the G-VAE are also frozen. 
\begin{table}
	\centering
	\caption{The validation loss of G-VAE on 10K architectures after 200-epoch training. Reconstruction denotes the reconstruction loss of discrete neural architectures, and KLD denotes the KL divergence between the posterior distribution and prior distribution.}
	\begin{tabular}{c|c|c}
		\toprule
		Model & Reconstruction & KLD \\
		\midrule
		G-VAE & $1.336\times10^{-8}$ & $2.43\times10^{-3}$  \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Architecture inference} 
Before architecture inference, the ICNN predictor is fine-tuned for 50 epochs by an Adam optimizer with the newly updated labeled set and same hyper-parameters as the semi-predictor. Then, the architecture inference is executed by the step-by-step gradient optimization in the latent space as $z^{\prime} = z + \eta \frac{\ \partial f(z)}{\partial z}$. The step size $\eta$ cannot be too small unless the discrete architecture of $z^{\prime}$ may be the same as the one of $z$. Specifically, we set the initial value of $\eta$ as $\eta_{0}=0.02$ and the step size interval $\Delta\eta= 0.02$. Then, when the decoded architecture is not new to the labeled set, the step size is increased by $\eta \leftarrow \eta + \Delta \eta$. For most experiments, the size of seed set for architecture inference is set as $K=10$ (exception: $K=5$ when $Q_{\rm start} = 300$ in NAS-Bench-101 experiments). In addition, Gaussian noise is injected to $z$ to explore the search space as $z \leftarrow z + \epsilon \mathcal{N}(0,{\rm I})$. Specifically, we set $\epsilon=0.05$ for NAS-Bench-101 and NAS-Bench-201 and NAS-Bench-301. Whenever a discrete architecture which is not new to the labeled set is derived, we evaluate its performance by querying the architecture on the corresponding NAS benchmark, then add the result to the labeled set for further architecture inference.  

\section{More Latent Space Visualization}
In this section, we visualize the latent spaces of NAS-Bench-101 and NAS-Bench-201 experiments in more detail. The results will show how CR-LSO can regularize the learning process of latent spaces to obtain more regular performance mappings.
\subsection{NAS-Bench-101}
\subsubsection{PCA visualization}
\begin{figure*}
	\centering
	\begin{subfigure}{0.33 \linewidth}
		\includegraphics[width=1.0\textwidth]{pca_101/unconstraied-space-2D_00.jpg}
		\caption{Unconstrained latent space.}
	\end{subfigure}
	\begin{subfigure}{0.33 \linewidth}
		\includegraphics[width=1.0\textwidth]{pca_101/convexity-space-100-2D_00.jpg}
		\caption{CR-LS ($Q_{\rm start} = 100$, Kendall' $s$ = 0.21).}
		\label{fig:pca_101_100}
	\end{subfigure}
	\begin{subfigure}{0.33 \linewidth}
	\includegraphics[width=1.0\textwidth]{pca_101/convexity-space-300-2D_00.jpg}
	\caption{CR-LS ($Q_{\rm start} = 300$, Kendall' $s$ = 0.43).}
	\end{subfigure}
	\begin{subfigure}{0.33 \linewidth}
	\includegraphics[width=1.0\textwidth]{pca_101/convexity-space-600-2D_00.jpg}
	\caption{CR-LS ($Q_{\rm start} = 600$, Kendall' $s$ = 0.51).}
	\end{subfigure}
	\begin{subfigure}{0.33 \linewidth}
	\includegraphics[width=1.0\textwidth]{pca_101/convexity-space-1500-2D_00.jpg}
	\caption{CR-LS ($Q_{\rm start} = 1500$, Kendall' $s$ = 0.60).}
	\end{subfigure}
	\begin{subfigure}{0.33 \linewidth}
	\includegraphics[width=1.0\textwidth]{pca_101/convexity-space-4236-2D_00.jpg}
	\caption{CR-LS ($Q_{\rm start} = 4000$, Kendall' $s$ = 0.73).}
	\end{subfigure}
	\caption{The PCA visualizations of unconstrained and convexity regularized latent spaces in NAS-Bench-101. The architectures with blue colors own higher rankings than those with yellow or red colors (blue $>$ yellow $>$ red).}
	\label{fig:pca_101}
\end{figure*} 
The PCA projections of latent spaces in NAS-Bench-101 experiments are visualized in Figure \ref{fig:pca_101}. Architectures with blue colors have higher rankings than those with yellow or red colors (blue $>$ yellow $>$ red). The Kendall correlation coefficients of semi-supervised GNN predictors for training the G-VAE are concluded in the figure legends. It can be seen that without any constraints, the high-performing and poor-quality architectures are highly mixed. It means a non-convex architecture performance mapping is obtained in the latent space. When being regularized by the convexity of ICNNs, the G-VAE learns latent spaces with more ranking distinctions, which allows for more stable gradient-based optimization. Even if the semi-supervised predictor is low-quality (when only a small number of labeled architectures are available), the G-VAE tries to separate architectures with different rankings (Figure \ref{fig:pca_101_100}). Thus, it is emphasized that a stronger semi-supervised predictor will improve the performance of CR-LSO significantly.  
\subsubsection{Architecture cosine similarity}
\begin{figure}
	\centering
	\begin{subfigure}{1.05 \linewidth}
		\includegraphics[width=1.0\textwidth]{cos_101/unsupervised_2_00.jpg}
		\caption{Architecture cosine similarity in unconstrained latent space.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{1.05 \linewidth}
		\includegraphics[width=1.0\textwidth]{cos_101/100_2_00.jpg}
		\caption{Architecture cosine similarity in CR-LS ($Q_{\rm start} = 100$, Kendall' $s$ = 0.21).}
	\end{subfigure}
	\begin{subfigure}{1.05 \linewidth}
		\includegraphics[width=1.0\textwidth]{cos_101/600_2_00.jpg}
		\caption{Architecture cosine similarity in CR-LS ($Q_{\rm start} = 600$, Kendall' $s$ = 0.51).}
	\end{subfigure}
	\caption{The cosine similarity of architecture latent representations in the unconstrained latent space (top) and the convexity regularized latent space (bottom) in NAS-Bench-101 experiments. Left: The architecture similarity among the optimal 100 architectures. Right: The architecture similarity between the optimal 100 and the worst 100 architectures. }
	\label{fig:cos_101}
\end{figure} 
The architecture cosine similarity is visualized in Figure \ref{fig:cos_101}. We can see that with the regularization of the convexity of ICNNs, the cosine similarity between the optimal 100 architectures and the worst 100 ones is significantly lower than that without any constraints.   
\subsection{NAS-Bench-201} 
\subsubsection{PCA visualization}
\begin{figure}
	\centering
	\begin{subfigure}{1.0 \linewidth}
		\includegraphics[width=1.0\textwidth]{pca_201/unsupervised.png}
		\caption{Unconstrained latent space.}
		\label{fig:pca_201_a}
	\end{subfigure}
	\begin{subfigure}{0.98 \linewidth}
		\includegraphics[width=1.0\textwidth]{pca_201/Semisupervised_300.png}
		\caption{CR-LS ($Q_{\rm start} = 300$, Kendall' $s$ = 0.68).}
		\label{fig:pca_201_b}
	\end{subfigure}
	\caption{The PCA visualizations of latent spaces in NAS-Bench-101. The architectures with blue colors own higher rankings than those with yellow or red colors (blue $>$ yellow $>$ red); (a) Unconstrained latent space; (b) Convexity regularized latent space.}
	\label{fig:pca_201}
\end{figure} 
The PCA projections of latent spaces in NAS-Bench-201 experiments are visualized in Figure \ref{fig:pca_201}. Without any constraints, the G-VAE learns a latent space where the architectures with distinct rankings are mixed highly (Figure \ref{fig:pca_201_a}). With the convexity regularization, the G-VAE tries to learn a convex performance mapping, which results in a latent space with obvious architecture rankings (Figure \ref{fig:pca_201_b}).
\subsubsection{Architecture cosine similarity} 
\begin{figure}
	\centering
	\begin{subfigure}{1.05 \linewidth}
		\includegraphics[width=1.0\textwidth]{cr_lso/unsupervised_similarity_2.jpg}
		\caption{Architecture similarity in unconstrained latent space.}
	\end{subfigure}
	\begin{subfigure}{1.05 \linewidth}
		\includegraphics[width=1.0\textwidth]{cr_lso/semisupervised_similarity_2.jpg}
		\caption{CR-LS ($Q_{\rm start} = 300$, Kendall' $s$ = 0.79).}
	\end{subfigure}
	\caption{The cosine similarity of architecture latent representations in the unconstrained latent space (top) and the convexity regularized latent space (bottom) in NAS-Bench-201 experiments. Left: The architecture similarity among the optimal 100 architectures. Right: The architecture similarity between the optimal 100 and the worst 100 architectures. }
	\label{fig:cos_201}
\end{figure} 
The architecture cosine similarity is visualized in Figure \ref{fig:cos_201}. 
\section{Prediction performance of ICNNs}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.236\textwidth]{cr_lso/icnn_prediction.jpg}
	\includegraphics[width=0.236\textwidth]{cr_lso/mlp_prediction.jpg}
	\caption{Scatter plots of the prediction performance against the true performance ($N=1600$). The horizontal axis represents the true performance, and the vertical axis represents the predicted performance. The prediction performances of the ICNN and the MLP are almost the same.}
	\label{fig:prediction_performance}
\end{figure}
\begin{table}[htbp]
	\centering
	\caption{Comparison of prediction performance.}
	\begin{tabular}{lccc}
		\toprule
		Models & $N$ & Pearson's $r$ & Kendall's $r$ \\
		\midrule
		\multirow{3}{*}{ICNN} & 100 & 0.831 $\pm$ 0.009 & 0.634 $\pm$ 0.011 \\
		{} & 400 & 0.932 $\pm$ 0.002 & 0.777 $\pm$ 0.003 \\
		{} & 1600 & 0.950 $\pm$ 0.002 & 0.812 $\pm$ 0.004 \\		
		\midrule
		\multirow{3}{*}{MLP} & 100 & 0.823 $\pm$ 0.012 & 0.629 $\pm$ 0.013 \\
		{} & 400 & 0.930 $\pm$ 0.003 & 0.773 $\pm$ 0.006 \\
		{} & 1600 & 0.948 $\pm$ 0.002 & 0.812 $\pm$ 0.004 \\
		\bottomrule
	\end{tabular}
	\label{tab:prediction_performance}
\end{table}
An accurate predictor is critical to the optimization of latent variables. One may doubt whether the ICNN still has the ability to capture the underlying performance mapping. This part eliminates such doubts by analyzing the prediction performance of the ICNN quantitatively on NAS-Bench-301 \cite{siems2020bench}. To test the approximation ability, we sample 31.6K architectures from the search space of DARTS randomly and label them using the query values of NAS-Bench-301. The first 1.6K architectures are used as the training set and the other 30K ones are used as the test set. For each experiment, the training set is used to train a GNN-based predictor. Then, the predictor is used to train the G-VAE and the ICNN in a semi-supervised manner. Hereafter, the ICNN predictor is verified on the test set. Pearson and Kendall correlation coefficients are utilized to quantify the evaluation results. For comparison, the MLP is used as a baseline. If the evaluation results of the ICNN are as good as MLP's, we say that the ICNN does not lose the approximation ability. Experiments are accomplished in three different settings with the architecture number of the training set (denoted by $N$) being 100, 400 and 1600, respectively. All experiments are repeated five times independently. Means and standard deviations are recorded. The results are summarized in Table \ref{tab:prediction_performance}. Figure \ref{fig:prediction_performance} shows the scatter plots of the prediction performance against the true performance. As we can see, for $N=100$, $400$ and $1600$, the ICNN achieves similar performance as the MLP, which indicates a solid foundation for the subsequent architecture search.

\section{The representation ability of ICNNs} 
The convexity of ICNNs follows the fact that the non-negative sums of convex functions are also convex, and the composition of a convex and convex non-decreasing function is also convex. The condition that $g_{i}$ must be convex and non-decreasing is not very restrictive, since many nonlinear activation functions such as $\rm ReLU$, $\rm LeakyReLU$ and $\rm SiLU$ already satisfy the condition. The condition that $W_{i}^{(z)}$ must be non-negative is somewhat restrictive, but since the bias terms and the bypass $W_{i}^{(y)}$ terms can be negative, the networks still have substantial representation power. Notably, a lemma which describes the representation ability of ICNNs deserves our attention \cite{chen2018optimal}. 

\emph{Lemma 2:} \emph{For any Lipschitz convex function $f:  \mathbb{R}^{\rm n} \rightarrow \mathbb{R}$ over a compact domain, there is a neural network $\hat{f}$ with non-negative weights and ReLU activations that approximate $f$ within $\epsilon$, i.e., $|f(x) - \hat{f}(x)| < \epsilon$ for any $\epsilon > 0$ and any $x$ in the compact domain.}
\section{A toy example to understand CR-LSO}
In this part, a toy example is presented to show how we can transform a non-convex function into a convex function by leveraging a simplified version of the proposed CR-LSO. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{toy_example/function.png}
	\caption{The surface plot of function $y = h(\boldsymbol{x})$.}
	\label{fig:target_function}
\end{figure}
Suppose that we expect to maximize a two-dimensional black box function $y=h(\boldsymbol{x})$ with
\begin{align}
	\begin{split}
		h(\boldsymbol{x}) = -0.64(x_{1}-1)^{2} -0.64(x_{2}+0.2)^{2} - \\
		2{\rm sin}(\pi(x_{1}-0.2)) - 2{\rm cos}(\pi(x_{2}-0.1)) - 4,
	\end{split}
\end{align}
where $\boldsymbol{x} = [x_{1},x_{2}]^{\rm T} \in \mathbb{R}^{2}$. The surface plot of $h(\boldsymbol{x})$ is shown in Figure \ref{fig:target_function}.
Now, we expect to maximize $h(\boldsymbol{x})$ by gradient-based optimization with the assistance of a neural network predictor. However, the multi-model
function $h(\boldsymbol{x})$ prevents us from obtaining the optimal solution by gradient-based optimization unless an appropriate initial point is selected. Even if $h(\boldsymbol{x})$ is a convex function, the problem cannot be solved since the conventional neural network is not guaranteed to be a convex function. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{toy_example/prediction_icnn.jpg}
	\caption{The surface plot of the predicted $h(\boldsymbol{x})$ of the ICNN predictor.}
	\label{fig:prediction_icnn}
\end{figure}

For the time being, a single ICNN $f_{\beta}(\boldsymbol{x})$ is used to approximate $h(\boldsymbol{x})$. In this case, the problem of the convexity of prediction model is solved. Now, all we hope is that the optimal point of $f_{\beta}(\boldsymbol{x})$ locates near the optimum of $h(\boldsymbol{x})$. At least, the two points should be close enough. Unfortunately, things go out of the expectation. Figure \ref{fig:prediction_icnn} shows the surface plot of $f_{\beta}(\boldsymbol{x})$. As we can see, the ICNN makes inevitable compromises to maintain the convexity and characterizes the rough shape of $h(\boldsymbol{x})$ only. The optimal point of $f_{\beta}(\boldsymbol{x})$, however, is not close enough to the optimal point of $h(\boldsymbol{x})$, which means that maximize $f_{\beta}(\boldsymbol{x})$ does not maximize $h(\boldsymbol{x})$ simultaneously. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{toy_example/prediction_crlso.jpg}
	\caption{The surface plot of predicted $h(\boldsymbol{x})$ of CR-LSO in original space, i.e., the surface plot of $y = f_{\beta}(g_{\theta}(\boldsymbol{x}))$. We can see that the CR-LSO approximates $h(\boldsymbol{x})$ almost everywhere.}
	\label{fig:prediction_crlso}
\end{figure}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{toy_example/prediction_crlso_latent_2d.jpg}
	\caption{The surface plot of predicted $h(\boldsymbol{x})$ of CR-LSO in two-dimensional latent space, i.e., the surface plot of $y = f_{\beta}(\boldsymbol{u})$. We can see that in the latent space, the prediction model is a convex function, which leads the gradient-based optimization more stable.}
	\label{fig:prediction_crlso_latent}
\end{figure}

Here, the proposed CR-LSO is used to approximate $h(\boldsymbol{x})$ and constraint the underlying mapping from latent space to predictions as a convex function. For simplicity, the auto-encoder, rather than the standard variational auto-encoder, is employed. Now, let $\boldsymbol{u}\in \mathbb{R}^{\rm d}$ be the variable of latent space, and $\hat{\boldsymbol{x}} = g^{-1}_{\phi}(\boldsymbol{u})$ be the re-constructed $\boldsymbol{x}$, where $g_{\theta}(\boldsymbol{x})$ and $g^{-1}_{\phi}(\boldsymbol{u})$ are the encoder and decoder respectively. The ICNN is modified as $f_{\beta}(\boldsymbol{u})$ to receive inputs from the latent space. The training objective of the auto-encoder and the ICNN is to minimize the loss function 
\begin{align}
	\begin{split}
		& \mathcal{L}_{\theta,\phi,\beta}(\boldsymbol{x},y) =  \Vert \boldsymbol{x} - g_{\phi}^{-1}(g_{\theta}(\boldsymbol{x}))\Vert^{2} + \Vert y - f_{\beta}(g_{\theta}(\boldsymbol{x}))) \Vert^{2}.
	\end{split}
\end{align}
Figures \ref{fig:prediction_crlso} and \ref{fig:prediction_crlso_latent} show the surface plots of CR-LSO's prediction in the original space and the two-dimensional latent space, respectively. Compared with the single ICNN, the CR-LSO almost approximates $h(\boldsymbol{x})$ everywhere in the original space. And in the latent space, the CR-LSO learns a convex prediction function, which leads the gradient-based optimization more stable. Now, the concern is how well the decoder decodes the variable of latent space. If the auto-encoder transforms and restores $\boldsymbol{x}$ appropriately, i.e., $\boldsymbol{x} \approx g_{\phi}^{-1}(g_{\theta}(\boldsymbol{x}))$, we may obtain an appropriate solution in the original space by maximizing $f_{\beta}(\boldsymbol{u})$ in the latent space. We empirically find that the loss term $\Vert \boldsymbol{x} - g_{\phi}^{-1}(g_{\theta}(\boldsymbol{x}))\Vert^{2}$ decreases to a sufficiently low level with the dimension of latent space increasing. 
\begin{table}[htbp]
	\centering
	\caption{Empirical comparison of different models for fitting $h(\boldsymbol{x})$. Row dimension means the dimension of latent space used by CR-LSO. Row prediction means the square deviation between the true value and predicted value, i.e., $\Vert y - f_{\beta}(g_{\theta}(\boldsymbol{x}))) \Vert^{2}$. Row reconstruction means the square deviation between the reconstructed $\hat{\boldsymbol{x}}$ and $\boldsymbol{x}$, i.e., $\Vert \boldsymbol{x} - g_{\phi}^{-1}(g_{\theta}(\boldsymbol{x}))\Vert^{2}$.}
	\begin{tabular}{lcccccc}
		\toprule
		Model & Dimension & Prediction & Reconstruction \\
		\midrule
		MLP & - & 0.134 & -  \\
		ICNN & - & 4.382 & -  \\
		\midrule
		\multirow{6}{*}{CR-LSO} & 1 & 0.171 & 1.829 \\
		{} & 2 & 0.014 & 0.087  \\
		{} & 3 & 0.004 & 0.007  \\
		{} & 4 & 0.006 & 0.003 \\
		{} & 5 & 0.005 & 0.001  \\
		\bottomrule
	\end{tabular}
	\label{tab:empirical_comparison}
\end{table}

The statistical losses for fitting $h(\boldsymbol{x})$ are summarized in Table \ref{tab:empirical_comparison}. In an extreme example, we try to transform the two-dimensional non-convex function into an one-dimensional convex function. The empirical evidence has shown that it is impracticable, since both the prediction and the reconstruction losses are very large. When the dimension of latent space is larger than that of original space, both the prediction loss and reconstruction loss are small. It implies that the neural networks has learned a nonlinear transformation that projects $\boldsymbol{x}$ into a higher dimensional space where the predictive function is convex.  

At the end of this toy example, it is worth pointing out that the proposed CR-LSO is developed for graph optimization problems where the input space is discrete and structured (e.g., neural architecture search), rather than continuous optimization problems. Although the above example seems to show that the CR-LSO may transform a continuous non-convex function into a convex function, the optimization performance for continuous optimization is unevaluated, and is beyond the discussion topic of this paper. 
\end{document}
