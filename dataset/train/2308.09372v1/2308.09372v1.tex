
\section{Results}


\begin{table}
	\small
	\centering
	\begin{tabular}{l|cl|cc}
		Model & \makecell[c]{DeiT \\ Based} & \makecell[c]{Orig. \\ Acc.} & \makecell[c]{New \\ Acc.} & \makecell[c]{}\\
		\hline
		ViT-S (DeiT)            & \checkmark & 79.8  & 82.54 & + 2.74 \\
		ViT-S (DeiT III)        &            & 82.6  & 82.54 & - 0.06 \\
		XCiT-S                  & \checkmark & 82.0  & 83.65 & + 1.65 \\
		Swin-S                  & \checkmark & 83.0  & 84.87 & + 1.87 \\
		SwinV2-Ti               &            & 81.7  & 83.09 & + 1.39 \\
		Wave-ViT-S              &            & 82.7  & 83.61 & + 0.91 \\
		Poly-SA-ViT-S           &            & 71.48 & 78.34 & + 6.86 \\
		EfficientFormer-V2-S0   &            & 75.7 & 71.53 & - 4.17 \\
		CvT-13                  &            & 83.3 & 82.35 &- 0.95\\
		CoaT-Ti                 & \checkmark & 78.37 &  78.42  & + 0.05 \\
		GFNet-S                 &            & 80.0  & 81.33 & + 1.33 \\
		FocalNet-S              &            & 83.4  & 84.91 & + 1.51 \\
		DynamicViT-S            &            & 83.0 & 81.09 & - 1.91 \\
		EViT (delete)           & \checkmark & 79.4  & 82.29 & + 2.89 \\
		EViT (fuse)             & \checkmark & 79.5  & 81.96 & + 2.46 \\
		ToMe-ViT-S              & \checkmark & 79.42 & 82.11 & + 2.69 \\
		CaiT-S24                & \checkmark & 82.7  & 84.91 & + 2.21 \\
		TokenLearner-ViT-8      &            & 77.87 & 80.66 & + 2.79 \\
		STViT-Swin-Ti           & \checkmark & 80.8  & 82.22 & + 1.42 \\
	\end{tabular}
	\caption{ImageNet-1k accuracy differences of the original papers and the new training pipeline. 
		Models are trained on  pixel images, unless marked with  or , in which case they are trained on  and  pixels, respectively. Results marked with  are obtained using knowledge distillation.}
	\label{table:pipeline_comparisons}
\end{table}

\subsection{ImageNet-1k Accuracy}
To ensure a fair evaluation and comparable results across models, we first compare the ImageNet accuracy reported in the original papers and the one obtained after training with our pipeline in \Cref{table:pipeline_comparisons}.
We can see that a substantial fraction of these pipelines is based on the one from \cite{Touvron2021b}, making them a good fit for training with the updated version of that pipeline.


Overall, we observe that almost all the models trained with our pipeline achieve a higher accuracy; 1.35\% on average.
Additionally, all the models for which the original pipeline significantly outperformed ours were trained using knowledge distillation or fine-tuned at a higher resolution. 
The highest improvement is +6.86\% for 
Poly-SA, indicating possible training instabilities in the original paper, similar to ones we faced with the other Kernel Attention models.

Most taxonomy classes contain models that achieve peak accuracies of approximately 85\% (see the supplementary material). However, two notable exceptions are observed: the \emph{kernel attention} class, which poses challenges in optimizing models, and the \emph{fixed attention} class, where the use of a constant attention matrix inherently results in a lower accuracy.
In summary, the pipeline provides a strong and comparable baseline for all models, regardless of architectural differences.

\begin{figure}
	\centering
	\includegraphics{model_vs_acc_per_param_and_acc_excerpt.pdf}
	\caption{Accuracy and accuracy per parameter of different models at a resolution of  pixels ordered by model accuracy. We grouped 14 models of intermediate accuracies into the `others` column, for which the light gray bar  shows the range.}
	\label{fig:model_vs_acc_per_param_and_acc_excerpt}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics{parameters_vs_acc_excerpt.pdf}
	\caption{Pareto front of number of parameters vs. accuracy for models with at most 30 million parameters. Legend in \Cref{fig:throughput_vs_acc_size_imsize}.}
	\label{fig:parameters_vs_acc_excerpt}
\end{figure}

\subsection{Number of Parameters}
The number of parameters 
is
used in the literature as a proxy metric for tracking the model complexity, and the overall computational cost of using a model.
When analyzing the parameter efficiency of different models (\Cref{fig:model_vs_acc_per_param_and_acc_excerpt}), it is evident that for most smaller models, the accuracy per parameter remains relatively constant at about . 
However, this value is approximately halved for larger models, which indicates diminishing returns on scaling the model size.
The smaller hybrid attention models, EfficientFormerV2-S0 and CoaT-Ti, exhibit the highest accuracy per parameter,
significantly outperforming
other attention-based models as well as ResNet50, a CNN. This 
suggests that the combination of attention and convolutions allows for the development of 
very parameter-efficient models.
For
the baseline ViT, there is a noticeable drop in accuracy per parameter across 
the
model sizes. While ViT-Ti 
outperforms
models of comparable accuracy, and ViT-S performs on par with similar models, ViT-B slightly underperforms when compared to other, larger models. 

\Cref{fig:parameters_vs_acc_excerpt} highlights the Pareto boundary between accuracy and the number of parameters, showing models with fewer than  million parameters.
It is noteworthy that the majority of Pareto optimal models are fine-tuned at a higher resolution of  pixels, which contributes to an increase in accuracy without a any growth in the parameter count. 
At the smallest model sizes, we observe once again, that EfficientFormerV2-S0 and CoaT-Ti are Pareto optimal models,
while the TokenLearner is a Pareto optimal choice between ViT-Ti and the also Pareto optimal ViT-S.


\subsection{Speed}
The inference speed is a critical metric of significant importance for practitioners when making decisions regarding model deployment. 
Whether driven by strict requirements for real-time processing 
or the desire to obtain model outputs within reasonable timeframes, inference speed directly impacts the usability and effectiveness of deployed models. 
The models we evaluated often claim a superior 
throughput vs. accuracy trade-off
compared to the original ViT. 
However, our comprehensive evaluation (see \Cref{fig:throughput_vs_acc_size_imsize}) reveals that ViT remains Pareto optimal at all sizes.
Additionally, it becomes evident that only a subset of models, namely Synthesizer-FR and 
some sequence reduction models
demonstrate improvements in the Pareto front when compared to a ViT of the corresponding size.

Moreover, our observations indicate that fine-tuning at a higher resolution of  pixels is not an efficient strategy. 
While it may result in improved model accuracy, it entails a significant increase in computational cost, leading to a substantial reduction in throughput. 
Consequently, opting for the next larger model turns out to be more efficient. 
Although a larger model may involve more floating-point operations, these are parallelized more effectively, resulting in higher overall throughput as well as a higher  accuracy.

We observe a substantial correlation of 0.81 between inference time and fine-tuning time, which is supported by the overall similarities we see in the Pareto fronts.
Additionally, in our analysis of fine-tuning time, the TokenLearner model emerges as a standout performer, demonstrating the fastest fine-tuning speed while achieving a commendable accuracy of 77.35\%.
For more details, see the supplementary material.


\begin{figure}[ht]
	\centering
	\includegraphics{inference_memory_vs_acc_size_imsize_excerpt_small.pdf}
	\caption{Excerpt of the Pareto front of inference memory and accuracy for models which need less then 1.25GB of VRAM. Legend in \Cref{fig:throughput_vs_acc_size_imsize}.}
	\label{fig:inference_memory_vs_acc_size_imsize}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics{training_memory_vs_acc_size_imsize_excerpt_small.pdf}
	\caption{Excerpt of the Pareto front of training memory and accuracy for models which need less than 225GB of VRAM for training. Legend in \Cref{fig:throughput_vs_acc_size_imsize}.}
	\label{fig:training_memory_vs_acc_size_imsize}
\end{figure}

\subsection{Memory}
VRAM imposes a significant constraint in deep learning research and practice, as it enforces hard limitations on the usable models.
When optimizing for VRAM usage during inference, the analysis presented in \Cref{fig:inference_memory_vs_acc_size_imsize} reveals the remarkable performance of the \emph{hybrid attention} models CoaT and CvT.
Notably, Wave-ViT, 
EViT and ToMe, fine-tuned at 
 px, and CaiT form the Pareto front for larger sizes. 
Importantly, ViT fails to achieve Pareto optimality in this metric.
This observation suggests that, similar to the number of parameters, hybrid attention models excel in low memory environments.
In contrast, the aspect of training memory (\Cref{fig:training_memory_vs_acc_size_imsize}) exhibits a similar pattern as observed in throughput, and consequently we measured a relatively high correlation of 0.71 between training memory and inference time (see the supplementary material).
Here, CoaT and CvT need more memory than other models of comparable accuracy, and again 
ViT maintains its Pareto optimality, along with the sequence reduction models EViT, ToMe, TokenLearner, and Synthesizer-FR, and XCiT. 

\subsection{Correlation of Metrics}
\begin{table}
	\centering
	\begin{tabular}{l|cccccc}
		      &  & \makecell[c]{\textit{FT} \\ \textit{Time}} & \makecell[c]{\textit{FFwd} \\ \textit{Time}} & \makecell[c]{\textit{FFwd} \\ \textit{Mem}} & \makecell[c]{\textit{Train} \\ \textit{Mem}} \\
		\hline
		\textit{FLOPS}				& 0.30 & \textbf{0.72} & 0.48          & 0.42 & \textbf{0.85} \\
		\textit{}           &      & 0.05          & 0.02          & 0.40 & 0.18          \\
		\textit{FT Time}            &      &               & \textbf{0.81} & 0.17 & \textbf{0.89} \\
		\textit{FFwd Time}     		&      &               &               & 0.13 & \textbf{0.71} \\
		\textit{FFwd Memory}       	&      &               &               &      & 0.48          \\
	\end{tabular}
	\caption{Correlation between the number of floating point operations (FLOPS), number of parameters (), fine-tuning time (\textit{FT Time}), inference time (\textit{FFwd Time}), inference memory (\textit{FFwd Mem}) and training memory (\textit{Train Mem}).}
	\label{table:correlation_of_metrics}
\end{table}


We discovered the highest correlation coefficient of 0.89 between fine-tuning time and training memory. This strong correlation suggests a common underlying factor or bottleneck, possibly related to the necessity of memory reads during training. 
Understanding this relationship can provide valuable insights into the factors influencing training efficiency.
Intriguingly, the highest correlation coefficient of a theoretical metric is 0.85, found between FLOPS and training memory, suggesting that the VRAM required for training can be roughly estimated based on the theoretical FLOPS using the following approximation (plot in the supplementary material):

Surprisingly, the other evaluated metrics 
exhibit relatively weak correlations when considering different model architectures as seen in \Cref{table:correlation_of_metrics}, which highlights the limited reliability of estimating computational costs solely based on theoretical metrics. 
Consequently, assessing model efficiency in practical scenarios requires the measurement of throughput and memory requirements for novel architectures.

