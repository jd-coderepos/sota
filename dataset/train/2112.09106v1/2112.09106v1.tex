\section{Experiments}
\label{sec:experiments}

Our models are primarily evaluated on transfer learning for open-vocabulary object detection. We also present results of zero-shot inference for object detection. Finally, we present ablation study on different model components.


\smallskip
\noindent \textbf{Datasets}. For pretraining, we use the image-text pairs from Conceptual Caption dataset (CC3M)~\cite{sharma2018conceptual} which collects 3 millions of image-text pairs from the web. We also consider a smaller dataset COCO Caption (COCO Cap)~\cite{chen2015microsoft} to pretrain our model when conducting ablation study. COCO Cap contains 118k images with each image annotated by human for 5 captions. We parsed object concepts from COCO/CC3M dataset and filtered the concepts whose frequency is lower than 100, resulting in 4764/6790 concepts. 

For transfer learning of open-vocabulary object detection, we train detectors with base categories of COCO detection dataset~\cite{lin2014microsoft} and LVIS dataset (v1)~\cite{gupta2019lvis}, respectively. On COCO, We follow the data split of \cite{bansal2018zero} with 48 base categories and 17 novel categories which are subsets of COCO object classes. We use the processed data from \cite{zareian2021open} with 107,761 training images and 4,836 test images. On LVIS, following \cite{gu2021zero}, we use the training/validation images for training/evaluation and adopt the category split with 866 base categories (common and frequent objects) and 337 novel categories (rare objects). 

We evaluate object detection performance on COCO and LVIS for both transfer learning and zero-shot inference.



\begin{table*}[]
\centering
\resizebox{0.73\textwidth}{!}{\begin{tabular}{lll|ll|ccccc}
\toprule
\multicolumn{3}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Visual Encoder Pretraining\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Detector Training \end{tabular}}} & \multicolumn{5}{c}{COCO} \\ \multicolumn{3}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Novel \\ (17)\end{tabular}}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Base \\ (48)\end{tabular}}} & \multicolumn{3}{c}{Generalized (17+48)} \\ Method & Dataset & Backbone & Method & Backbone & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & Novel & Base & All \\ \midrule
Cls-ResNet~\cite{he2016deep} & ImageNet & RN50 & FR-CNN~\cite{faster-rcnn} & RN50-C4 & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{54.5} & - & - & - \\
Cls-IncRN~\cite{szegedy2017inception} & ImageNet & IncRNv2 & SB~\cite{bansal2018zero} & IncRNv2 & \multicolumn{1}{c}{0.70} & \multicolumn{1}{c}{29.7} & 0.31 & 29.2 & 24.9 \\
Cls-DarkNet~\cite{redmon2016you} & ImageNet & DarkNet19 & DELO~\cite{Zhu_2020_CVPR} & DarkNet19 & \multicolumn{1}{c}{7.60} & \multicolumn{1}{c}{14.0} & 3.41 & 13.8 & 13.0 \\
Cls-ResNet~\cite{he2016deep} & ImageNet & RN50 & PL~\cite{Rahman_aaai2020} & RN50-FPN & \multicolumn{1}{c}{10.0} & \multicolumn{1}{c}{36.8} & 4.12 & 35.9 & 27.9 \\
OVR~\cite{zareian2021open} & COCO Cap & RN50 & OVR~\cite{zareian2021open} & RN50-C4 & \multicolumn{1}{c}{27.5} & \multicolumn{1}{c}{46.8} & 22.8 & 46.0 & 39.9 \\
OVR~\cite{zareian2021open} & CC3M & RN50 & OVR~\cite{zareian2021open} & RN50-C4 & \multicolumn{1}{c}{16.7} & \multicolumn{1}{c}{43.0} & - & - & 34.3 \\
CLIP~\cite{radford2021learning} & CLIP400M & ViT-B/32 & ViLD*~\cite{gu2021zero} & RN50-FPN & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 27.6 & \textbf{59.5} & \textbf{51.3} \\ \midrule
CLIP~\cite{radford2021learning} & CLIP400M & RN50 & Ours & RN50-C4 & \multicolumn{1}{c}{22.5} & \multicolumn{1}{c}{53.1} & 14.2 & 52.8 & 42.7 \\
Ours & COCO Cap & RN50 & Ours & RN50-C4 & \multicolumn{1}{c}{30.8} & \multicolumn{1}{c}{55.2} & 26.8 & 54.8 & 47.5 \\
Ours & CC3M & RN50 & Ours & RN50-C4 & \multicolumn{1}{c}{\textbf{35.2}} & \multicolumn{1}{c}{\textbf{57.6}} & \textbf{31.4} & 57.1 & 50.4 \\ \midrule
Ours & CC3M & RN50x4 & Ours & RN50x4-C4 & \multicolumn{1}{c}{\textbf{43.3}} & \multicolumn{1}{c}{\textbf{61.9}} & \textbf{39.3} & \textbf{61.6} & \textbf{55.7} \\ \bottomrule
\end{tabular}} \vspace{-2mm}
\caption{Open-vocabulary object detection results on COCO dataset. Initialized by our pretrained visual encoder, our detector outperforms published works on all metrics by a remarkable margin, and outperforms the unpublished work ViLD* on novel categories. ViLD* trains the detector with data augmentation of copy-paste~\cite{ghiasi2021simple} and a much longer training schedule (16x). Notations: Cls denotes the image classification pretraining on ImageNet~\cite{imagenet}, RN50 means ResNet50, IncRNv2 is Inception-ResNet-V2.}
\label{tab:zeroshot-main}
\end{table*}


\begin{table*}[]
\centering
\resizebox{0.92\textwidth}{!}{\begin{tabular}{lll|llll|llll}
\toprule
\multicolumn{3}{c|}{Visual Encoder Pretraining} & \multicolumn{4}{c|}{Detector Training} & \multicolumn{4}{c}{LVIS} \\ Method & Dataset & Backbone & Method & Backbone & Training Strategy & Supervision & APr & APc & APf & mAP \\ \midrule
- & - & - & Mask RCNN~\cite{he2017mask} & RN50-FPN & 16x+Copy-paste~\cite{ghiasi2021simple} & Base+Novel & 13.0 & 26.7 & \textbf{37.4} & 28.5 \\
Cls-ResNet~\cite{he2016deep} & ImageNet & RN50 & Mask RCNN~\cite{he2017mask} & RN50-C4 & 1x+Standard & Base+Novel & 11.9 & 22.0 & 29.7 & 23.3 \\ \midrule
CLIP~\cite{radford2021learning} & CLIP400M & ViT-B/32 & ViLD*~\cite{gu2021zero} & RN50-FPN & 16x+Copy-paste~\cite{ghiasi2021simple} & Base & 16.7& 26.5& 34.2& 27.8\\
Ours & CC3M & RN50 & Ours & RN50-C4 & 1x+Standard & Base & 17.1& 27.4& 34.0& 28.2\\ \midrule
CLIP~\cite{radford2021learning} & CLIP400M & ViT-B/32 & ViLD*~\cite{gu2021zero} & RN152-FPN & 16x+Copy-paste~\cite{ghiasi2021simple} & Base & 19.8 & 27.1 & 34.5  & 28.7 \\ 
Ours & CC3M & RN50x4 & Ours & RN50x4-C4 & 1x+Standard & Base & \textbf{22.0}
& \textbf{32.1}
& 36.9 
& \textbf{32.3} \\ \bottomrule
\end{tabular}} \vspace{-2mm}
\caption{Open-vocabulary object detection results on LVIS dataset. Without sophisticated training strategy, our detector still outperforms ViLD* on most metrics. Using same training strategy, our open-vocabulary detector beats the fully-supervised Mask RCNN for all metrics.} \label{tab:zeroshot-concurrent}
\end{table*}


\smallskip
\noindent \textbf{Evaluation protocol and metrics}. We adopt the standard object detection metrics: mean Average Precision (AP) and AP50 (AP at an intersection over union of 0.5). We evaluate our models on two benchmarks for open-vocabulary object detection, including COCO and LVIS. On COCO, we report AP50 and follow the evaluation settings in \cite{zareian2021open}: (1) only predicting and evaluating novel categories (Novel), (2) only predicting and evaluating base categories (Base), (3) a generalized setting that predicts and evaluates all categories (Generalized). On LVIS, we follow the benchmark of \cite{gu2021zero} where the rare objects are defined as novel categories. We report AP for novel categories (APr), base categories (APc, APf) and all categories (mAP), respectively.


\smallskip
\noindent \textbf{Implementation details}.
\textit{During pretraining}, the default student model and teacher model were both ResNet50~\cite{he2016deep} of pretrained CLIP. The RPN used in pretraining was trained with the base categories of LVIS dataset. Our default model was pretrained on CC3M dataset with the concepts parsed from COCO Cap. SGD was used with the image batch of 96, initial learning rate of 0.002, maximum iteration of 600k, and 100 regions per image. 
\textit{For transfer learning} of object detection, our detectors were developed on Detectron2~\cite{wu2019detectron2} using Faster RCNN~\cite{faster-rcnn} with ResNet50-C4 architecture. The RPN used in transfer learning was trained by the base categories of target dataset (\eg, the transfer learning on COCO used the RPN trained on COCO). SGD was used with image batch of 16, initial learning rate 0.002, and 1x schedule. The weight of background category was set to 0.2/0.8 on COCO/LVIS. Focal scaling was particularly applied to COCO training with  as 0.5. 
\textit{For zero-shot inference} of object detection, RPN was the same as pretraining stage and NMS threshold was set to 0.9. For all experiments, the temperature  was 0.01.


\subsection{Transfer Learning for Object Detection}
We present the results of transfer learning for open-vocabulary object detection on COCO and LVIS datasets. Additionally, we report results for fully supervised setting where all categories are used during training.


\begin{table*}[]
\centering
\resizebox{0.81\textwidth}{!}{\begin{tabular}{lll|ll|cc|cccc}
\toprule
\multicolumn{3}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Visual Encoder Pretraining\end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Detector Training\end{tabular}}} & \multicolumn{2}{c|}{COCO} & \multicolumn{4}{c}{LVIS} \\
\multicolumn{3}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{Train: 80, Test: 80} & \multicolumn{4}{c}{Train: 1203, Test: 1203} \\ Method & Dataset & Backbone & Method & Backbone & AP50 & mAP & APr & APc & APf & mAP \\ \midrule
Cls-ResNet~\cite{he2016deep} & ImageNet & RN50 & FR-CNN~\cite{faster-rcnn} & RN50-C4 & 55.9 & 35.7 & 11.9 & 22.0 & 29.7 & 23.3 \\
CLIP~\cite{radford2021learning} & CLIP400M & RN50 & Ours & RN50-C4 & 56.3 & 36.4 & 16.0 & 25.0 & 32.0 & 26.2 \\
Ours & CC3M & RN50 & Ours & RN50-C4 & {59.8} & {38.8} & {18.6} & {27.8} & {34.8} & {29.0} \\ Ours & CC3M & RN50x4 & Ours & RN50x4-C4 & \textbf{64.4} & \textbf{42.7} & \textbf{24.5} & \textbf{32.0} & \textbf{36.5} & \textbf{32.5} \\ 
\bottomrule
\end{tabular}} \vspace{-2mm}
\caption{Fully supervised object detection results on COCO and LVIS datasets. Our detector initialized by our pretrained visual encoder converges faster and significantly outperforms the petrained backbones of ImageNet and CLIP on all metrics at 1x schedule.}
\label{tab:fully-supervised}
\end{table*}


\begin{table*}[]
\centering
\resizebox{0.70\textwidth}{!}{\begin{tabular}{lll|c|ccc|cccc}
\toprule
\multicolumn{3}{c|}{Visual Encoder Pretraining} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Region \\ Proposals\end{tabular}} & \multicolumn{3}{c|}{COCO} & \multicolumn{4}{c}{LVIS} \\ Method & Dataset & Backbone &  & Novel & Base & All & APr & APc & APf & mAP \\ \midrule
OVR~\cite{zareian2021open} & COCO Cap & RN50 & GT & 46.7 & \multicolumn{1}{l}{43.7} & \multicolumn{1}{l|}{44.5} & - & - & - & - \\
CLIP~\cite{radford2021learning} & CLIP400M & RN50 & GT & 58.6 & 58.2 & 58.3 & 40.3 & 41.7 & 43.6 & 42.2 \\
Ours & CC3M & RN50 & GT & {60.5} & {61.7} & {61.4} & {40.7} & {43.5} & {47.0} & {44.4} \\ 
Ours & CC3M & RN50x4 & GT & \textbf{65.2} & \textbf{65.6} & \textbf{65.5} & \textbf{50.1} & \textbf{50.1} & \textbf{51.7} & \textbf{50.7} \\ \midrule
OVR~\cite{zareian2021open} & COCO Cap & RN50 & RPN & 24.6 & 17.9 & 19.6 & - & - & - & - \\
CLIP~\cite{radford2021learning} & CLIP400M & RN50 & RPN & 29.7 & 24.0 & 25.5 & {11.6} & 9.6 & 7.6 & 9.2 \\
Ours & CC3M & RN50 & RPN & {31.4} & {25.2} & {26.8} & 10.9 & {10.4} & {8.2} & {9.6} \\ 
Ours & CC3M & RN50x4 & RPN & \textbf{34.6} & \textbf{27.9} & \textbf{29.6} & \textbf{13.8} & \textbf{12.1} & \textbf{9.4} & \textbf{11.3} \\\bottomrule
\end{tabular}}  \vspace{-2mm}
\caption{Zero-shot inference with ground-truth (GT) boxes or RPN boxes on COCO and LVIS dataset. All models use RoIAlign to extract visual representation of proposed image regions. Our pretrained model outperforms baselines by a clear margin across datasets.
} \label{tab:zeroshot-inference}
\end{table*}




\subsubsection{Open-Vocabulary Object Detection}

\noindent \textbf{Setup}. The detectors are trained by base categories while evaluated on base and novel categories (\eg, 48/866 base categories and 17/337 novel categories on COCO/LVIS). To compare with ViLD~\cite{gu2021zero}, all experiments on LVIS additionally use mask annotation to train detector.

\smallskip
\noindent \textbf{Baselines}. We consider several baselines as follows:
\squishlist
\item \textbf{Zero-shot object detectors} (SB~\cite{bansal2018zero}, DELO~\cite{Zhu_2020_CVPR}, PL~\cite{Rahman_aaai2020}): Zero-shot object detection is the closest area to open-vocabulary object detection. These detectors usually rely on the pretrained word embeddings of object classes for generalization to novel categories. 
\item \textbf{Open-vocabulary object detectors} (OVR~\cite{zareian2021open}, ViLD~\cite{gu2021zero}): These detectors leverage pretrained vision-language models that have learned a large vocabulary from image-text pairs. OVR is our close competitor in the sense that we both pretrain visual encoders and use them as the detector initialization. ViLD is a recent unpublished work that focuses on detector training by distilling visual features of a pretrained model from CLIP. ViLD specially uses the data augmentation of copy-paste~\cite{ghiasi2021simple} with 16x training schedule. 
\item \textbf{Fully supervised detectors}: On COCO, we include the supervised baseline from OVR which is a Faster RCNN~\cite{faster-rcnn} trained by the base categories with 1x schedule. On LVIS, we include the supervised baseline from ViLD which is a Mask RCNN~\cite{he2017mask} trained by base and novel categories with special data augmentation as ViLD. We additionally report a Mask RCNN trained in standard 1x schedule from Detectron2~\cite{wu2019detectron2}. 
\item \textbf{Our detector variants}: We consider initializing our detector with different pretrained visual encoders, including CLIP and our model pretrained on COCO Cap.
\squishend


\noindent \textbf{Results}. Table~\ref{tab:zeroshot-main} and Table~\ref{tab:zeroshot-concurrent} show the results on COCO and LVIS datasets, respectively.

On COCO dataset, initialized by our pretrained backbone, our detector significantly outperforms previous published SoTA method OVR~\cite{zareian2021open} on all metrics (\eg, 31.4 vs.\ 22.8 on novel categories). Compared with the CLIP backbone from which we start our region-based pretraining, our model brings a remarkable gain across all metrics, particularly +17.2 AP50 on novel categories. When compared with ViLD, an unpublished SoTA method with sophisticated training strategy, our model is still comparable on Base and All, while substantially better on Novel (\eg, 31.4 vs.\ 27.6) which is the main focus in open-vocabulary detection. 
On LVIS dataset, with comparable backbone size (RN50x4-C4 of ours: 83.4M, RN152-FPN of ViLD: 84.1M), our detector outperforms ViLD by a large margin (\eg, +2.2 APr and +3.6 mAP). Note that these superior detection results on COCO and LVIS are achieved by using a single pretrained backbone, with standard data augmentation and 1x training schedule. These results suggest that our region-based vision-language pretraining has learned better alignment between image regions and object concepts, and thus facilitates open-vocabulary object detection.  



\subsubsection{Fully Supervised Object Detection}

\noindent \textbf{Setup}. Detection annotation of all object categories are used during training and evaluation. Again, all experiments on LVIS additionally use mask annotation to train detector.

\smallskip
\noindent \textbf{Baselines}. We consider the following baselines: (1) Faster RCNN~\cite{faster-rcnn} intialized by ImageNet pretrained backbone: This is a common object detector in the community. (2) Our detector initialized by pretrained CLIP. This baseline is to validate our proposed pretraining method.

\smallskip
\noindent \textbf{Results}. In Table~\ref{tab:fully-supervised}, the detector initialized by our pretrained visual backbone largely outperforms the baselines that are initialized by ImageNet and CLIP backbones (\eg, +2.4 mAP on COCO and +2.8 mAP on LVIS). These results suggest that our proposed pretraining method helps the fully supervised detector converge faster and achieves better performance at 1x schedule. 
Again, when using RN50x4 as the backbone for both teacher model and student model, the performance is significantly improved (eg, 38.8 vs.\ 42.7 mAP on COCO, 29.0 vs.\ 32.5 on LVIS).


\subsection{Zero-shot Inference for Object Detection}

\noindent \textbf{Setup}. 
Without finetuning on the detection annotation, the pretrained vision-language models are directly used to recognize the proposed regions. We use the same evaluation datasets and metrics as the experiments in transfer learning. We consider two types of region proposals: (1) The ground-truth bounding boxes are used as region proposals. This setting aims at evaluating the recognition performance by eliminating the localization error. (2) The region proposals come from a RPN which is also used in pretraining. The performance in this setting is dependent on both the quality of RPN and recognition ability.

\smallskip
\noindent \textbf{Baselines}. We consider two baselines: (1) OVR~\cite{zareian2021open} pretrains visual backbone on image-text pairs of COCO Cap which has close object concepts as COCO detection dataset. We evaluate the pretrained model provided in their code base.  (2) CLIP~\cite{radford2021learning} is pretrained on 400M image-text pairs. Both OVR and CLIP pretrain model on the image-text pairs while our pretraining leverages the created region-text pairs for learning visual region representation. 

\smallskip
\noindent \textbf{Results}. Table~\ref{tab:zeroshot-inference} summarizes the results. With ideal region proposals, our pretrained model outperforms CLIP baseline by a clear margin across datasets (\eg, 61.4 vs.\ 58.3 All AP50 on COCO, 44.4 vs.\ 42.2 mAP on LVIS). When compared with OVR, our model demonstrates a much larger margin (\eg, 61.4 vs.\ 44.5 All AP50 on COCO), not to mention that OVR is pretrained on the same dataset as evaluation. Even if using RPN proposals, our model still clearly outperforms CLIP and OVR (\eg, 26.8 vs.\ 19.6 \& 25.5 on COCO, 9.6 vs.\ 9.2 on LVIS). These promising results suggest that our pretraining method with region-text alignment improves the visual recognition ability for image regions. 
With RN50x4 architecture as the backbones of teacher and student models, the zero-shot inference performance is further improved across datasets and different types of region proposals (\eg, +6.3 mAP on LVIS with GT boxes, +2.8 All on COCO with RPN boxes). 


\begin{figure*}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/zeroshot_inference_vis.pdf} \vspace{-2mm}
    \caption{Visualization of zero-shot inference on COCO dataset with {\it ground-truth boxes}. Without finetuning, the pretrained models (top: CLIP, bottom: Ours) are directly used to recognize image regions into 65 categories in COCO. (Image IDs: 9448, 9483, 7386, 4795)} \vspace{+2mm}
    \label{fig:visualization}
\end{figure*}


\begin{table}[]
\centering
\resizebox{0.47\textwidth}{!}{\begin{tabular}{c|c|cc|ccc}
\toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}  Region-text \\ Pairs \end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}} Image-text \\ Pairs \end{tabular}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}COCO\\ Zero-shot Inference\end{tabular}}} & \multicolumn{3}{c}{COCO} \\
 &  & \multicolumn{2}{c|}{} & \multicolumn{3}{c}{Generalized (17+48)} \\ &  & All (RPN) & All (GT) & Novel & Base & All \\ \midrule
\checkmark &  & 26.7 & 60.4 & 21.4 & 55.5 & 46.6 \\
\checkmark & \checkmark & 28.0 & 62.8 & 26.8 & 54.8 & 47.5 \\ \bottomrule
\end{tabular}}  \vspace{-2mm}
\caption{Ablation study on pretraining supervision. All models are pretrained on COCO Cap.}
\label{tab:image-region}
\end{table}

\begin{table}[]
\centering
\resizebox{0.42\textwidth}{!}{\begin{tabular}{cc|cc|ccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Region Type \end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}COCO\\ Zero-shot Inference\end{tabular}}} & \multicolumn{3}{c}{COCO} \\
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{3}{c}{Generalized (17+48)} \\ Random & RPN & All (RPN) & All (GT) & Novel & Base & All \\ \midrule
\checkmark &  & 27.1 & 60.8 & 25.2 & 54.5 & 46.9 \\
 & \checkmark & 28.0 & 62.8 & 26.8 & 54.8 & 47.5 \\ \bottomrule
\end{tabular}}  \vspace{-2mm}
\caption{Ablation study on the type of regions used during pretraining. All models are pretrained on COCO Cap.} \label{tab:ablation-region}
\end{table}


\subsection{Ablation Study}
The evaluation in this section uses COCO dataset and the same metrics as zero-shot inference and transfer learning.

\smallskip
\noindent \textbf{Pretraining supervision}. Table~\ref{tab:image-region} studies the effect of different pretraining supervisions. Accordingly, though using the region-text pairs already attains plausible results, the additional supervision from image-text pairs can further improve the performance (\eg, +2.4 AP50 with GT boxes on zero-shot inference, +5.4 Novel AP50 on transfer learning). We suspect that image-text pairs provide extra contextual information from global image description which compensates our created region descriptions.

\smallskip
\noindent \textbf{Types of image regions}. Table~\ref{tab:ablation-region} studies the effects of region proposal quality during pretraining. We replace the RPN proposals by sampling the same number of image regions with random location and random aspect ratio. Random boxes hurt zero-shot inference (-2.0 AP50 with GT boxes) while reserve comparable performance in transfer learning (46.9 vs.\ 47.5 All AP50). These results indicate that our pretraining is robust to the quality of region proposals. Zero-shot inference benefits from higher quality of proposals but the gap becomes smaller when human supervision is available to finetune the model.

\smallskip
\noindent \textbf{Pretraining dataset and concept pool}. In Table~\ref{tab:ablation-concept}, using COCO Cap dataset or using the COCO concepts achieves better zero-shot inference performance (62.8 vs.\ 61.4 vs.\ 60.8 AP50 with GT boxes). We hypothesize that COCO Cap has a smaller domain gap to COCO detection dataset. However, the model pretrained on CC3M achieves significant boost on transfer learning (47.5 vs.\ 50.4 All AP50). We conjecture that the model learns more generic visual representation from a larger number of images in CC3M. 


\smallskip
\noindent \textbf{Pretraining losses}. Table~\ref{tab:ablation-loss} studies the effects of different losses. With both contrastive loss and distillation loss, the model achieves close results as distillation-only model on zero-shot inference (\eg, 62.8 vs.\ 63.1 AP50 with GT boxes) while the best performance on transfer learning (\eg, 26.8 Novel AP50). These results suggest that two losses play different roles. Distillation loss helps to inherit the visual-semantic knowledge from the teacher model, while contrastive loss enforces more discriminative representations for transfer learning. 



\begin{table}[]
\centering
\resizebox{0.465\textwidth}{!}{\begin{tabular}{c|c|cc|ccc}
\toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Pretraining\\ Dataset\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Concept\\ Pool Source\end{tabular}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}COCO\\ Zero-shot Inference\end{tabular}}} & \multicolumn{3}{c}{COCO} \\
 &  & \multicolumn{2}{c|}{} & \multicolumn{3}{c}{Generalized (17+48)} \\ &  & All (RPN) & All (GT) & Novel & Base & All \\ \midrule
COCO Cap & COCO Cap & 28.0 & 62.8 & 26.8 & 54.8 & 47.5 \\
CC3M & COCO Cap & 26.8 & 61.4 & 31.4 & 57.1 & 50.4 \\
CC3M & CC3M & 26.5 & 60.8 & 29.1 & 56.0 & 49.0 \\ \bottomrule
\end{tabular}}  \vspace{-2mm}
\caption{Ablation study on the pretraining datasets and the source of concept pool.}
\label{tab:ablation-concept}
\end{table}


\begin{table}[]
\centering
\resizebox{0.46\textwidth}{!}{\begin{tabular}{cc|cc|ccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Pretraining Loss \end{tabular}}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}COCO\\ Zero-shot Inference\end{tabular}}} & \multicolumn{3}{c}{COCO} \\
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} & \multicolumn{3}{c}{Generalized (17+48)} \\ Contrastive & Distillation & All (RPN) & All (GT) & Novel & Base & All \\ \midrule
\checkmark &  & 25.2 & 58.2 & 21.8 & 54.2 & 45.8 \\
 & \checkmark & 27.8 & 63.1 & 24.1 & 54.6 & 46.7 \\
\checkmark & \checkmark & 28.0 & 62.8 & 26.8 & 54.8 & 47.5 \\ \bottomrule
\end{tabular}}  \vspace{-2mm}
\caption{Ablation study on losses during pretraining. All models use image-level contrastive loss pretrained on COCO Cap.} \label{tab:ablation-loss}
\end{table}



\smallskip
\noindent \textbf{Teacher model and student model}. Table~\ref{tab:teacher} studies the effects of using different teacher and student models. Compared with the default setting at first row, using ResNet50x4 as the teacher model can largely improve the zero-shot inference performance (+4.2 AP50 with GT boxes). However, in the transfer learning setting, the performance using a stronger teacher remains roughly the same (both are 50.4 AP50 for All). When we further replace the student model with ResNet50x4, the transfer learning performance is significantly boosted (+5.3 AP50 for All), but the zero-shot inference performance remains (29.6 vs.\ 29.3 AP50 with RPN boxes). Based on these results, we conjecture that zero-shot inference performance relies on the teacher model that guides the region-text alignment, while transfer learning is more likely constrained by the capacity of student model. 



\begin{table*}[!ht]
\begin{minipage}{0.65\linewidth}
\setlength{\tabcolsep}{3.8pt}
\centering
\footnotesize
\begin{tabular}{c|c|cc|ccc}
\toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Teacher\\ Backbone\end{tabular}} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Student\\ Backbone\end{tabular}} & \multicolumn{2}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}COCO\\ Zero-shot Inference\end{tabular}}} & \multicolumn{3}{c}{COCO} \\
 &  & \multicolumn{2}{c|}{} & \multicolumn{3}{c}{Generalized (17+48)} \\ &  & All (RPN) & All (GT) & Novel & Base & All \\ \midrule
RN50 & RN50 & 26.8 & 61.4 & 31.4 & 57.1 & 50.4 \\
RN50x4 & RN50 & 29.3 & 65.6 & 30.8 & 57.3 & 50.4 \\ 
RN50x4 & RN50x4 & 29.6 & 65.5 & 39.3 & 61.6 & 55.7 \\ \bottomrule
\end{tabular}\vspace{-2mm}
\captionof{table}{Ablation study on COCO with different teacher and student models in pretraining. All models are pretrained on CC3M dataset.}
\label{tab:teacher}
\end{minipage}\hfill
\begin{minipage}{0.3\linewidth}
\setlength{\tabcolsep}{2.8pt}
\centering
\footnotesize
\begin{tabular}{c|ccc}
\toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Focal Scaling\end{tabular}} & \multicolumn{3}{c}{COCO} \\
 & \multicolumn{3}{c}{Generalized (17+48)} \\ & Novel & Base & All \\ \midrule
 & 22.6 & 58.5 & 49.1 \\
\checkmark & 31.4 & 57.1 & 50.4 \\ \bottomrule
\end{tabular}\vspace{-2mm}
\captionof{table}{Ablation study on effects of focal scaling during transfer learning for object detection.}
\label{tab:ablation-focal}
\end{minipage}
\end{table*}

\smallskip
\noindent \textbf{Focal scaling}. Table~\ref{tab:ablation-focal} studies the effects of focal scaling during transfer learning. With focal scaling, the finetuned detector achieves a better balance between novel categories and base categories on COCO dataset. We conjecture that the detector overfits to the small set of base categories in COCO (\eg, 48 base categories), which hurts the generalization on novel categories. Focal scaling effectively alleviates the potential overfitting.




\subsection{Discussion}

\noindent \textbf{Visualization}. Fig.~\ref{fig:visualization} visualizes the results of zero-shot inference with ground-truth boxes and 65 categories from COCO dataset. Our model predicts more reasonable categories than CLIP (\eg, the blue regions in 1st and 2nd columns are correctly predicted as ``umbrella'' and ``person'' by our model). These results suggest that our proposed region-based vision-language pretraining can help to recognize image regions precisely.

\begin{figure}
	\centering
	\includegraphics[width=0.80\linewidth]{figures/supplement_vis.pdf} \caption{Visualization of zero-shot inference on COCO dataset with {\it ground-truth boxes}. Without finetuning, the pretrained models are asked to predict 1203 categories from LVIS dataset. We show the top-3 predicted categories from our pretrained model and pretrained CLIP model. (Image IDs: 776, 13597, 17029)} \label{fig:supplement_visualization}
\end{figure}

Further, the pretrained models can predict the customized object concepts by simply replacing the language embeddings of target categories. Fig.~\ref{fig:supplement_visualization} visualizes results of zero-shot inference with ground-truth boxes and 1203 categories from LVIS dataset, instead of the small set of 65 categories from COCO dataset. We show the {\it top-3} predictions for each region with their confidence scores. 

As shown by the successful cases in Fig.~\ref{fig:supplement_visualization}, our pretrained model can correctly recognize the image regions while the CLIP model often fails to predict the correct labels (\eg, ``teddy bear'' is predicted by our model with a high confidence score 99.5\%). Interestingly, other than the most-confident category, our model can also predict reasonable categories with top-3 scores (\eg, ``bear'' in 1st example and ``truffle chocolate'' in 2nd example). Even in the failure case where both CLIP and our model fail to recognize the dog as most-confident category, our model can still recognize the image region as visually similar concepts (\eg, ``ferret'' and ``cub'') or a fine-grained type of dog (\eg, ``shepherd dog''). On the contrary, CLIP predicts less visually similar concepts, such as ``grizzly'' and ``gorilla''.

\smallskip
\noindent \textbf{Limitations}. Our work has several limitations that can be further investigated.
(1) We focus on learning the object concepts without explicitly attending to other information in natural language, such as object attributes and object relationships, which are beneficial to some vision tasks (\eg, visual grounding). Learning comprehensive region representations can be a future work. 
(2) Our method relies on CLIP's visual-semantic space and has not updated the language encoder. When given similar scale of data as CLIP, unfreezing the language encoder may bring more gain in our region-based language-image pretraining. 

