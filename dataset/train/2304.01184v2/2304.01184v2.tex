\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{booktabs}

\usepackage{subfiles} \usepackage{makecell} \usepackage{multirow} \usepackage{subcaption}
\usepackage{color,xcolor}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{000} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{WeakTr: Exploring Plain Vision Transformer for Weakly-supervised\\Semantic Segmentation}

\newcommand{\name}{WeakTr}

\author{Lianghui Zhu, Yingyue Li, Jiemin Fang, Yan Liu, Hao Xin, Wenyu Liu, Xinggang Wang\\
School of EIC, Huazhong University of Science \& Technology\\
 Ant Group\\
}

\ificcvfinal\thispagestyle{empty}\fi


\maketitle

{
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{These authors contributed equally to this work.}
\footnotetext[2]{Corresponding author.}
}










\begin{abstract}
  This paper explores the properties of the plain Vision Transformer (ViT) for Weakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM) is of critical importance for understanding a classification network and launching WSSS. We observe that different attention heads of ViT focus on different image areas. Thus a novel weight-based method is proposed to end-to-end estimate the importance of attention heads, while the self-attention maps are adaptively fused for high-quality CAM results that tend to have more complete objects. Besides, we propose a ViT-based gradient clipping decoder for online retraining with the CAM results to complete the WSSS task. We name this plain \underline{\textbf{Tr}}ansformer-based \underline{\textbf{Weak}}ly-supervised learning framework \name{}. It achieves the state-of-the-art WSSS performance on standard benchmarks, \ie, 78.4\% mIoU on the val set of PASCAL VOC 2012 and 50.3\% mIoU on the val set of COCO 2014.
  Code is available at \url{https://github.com/hustvl/WeakTr}.
\end{abstract}



\section{Introduction}
\label{sec:intro}
\subfile{sub_files/1intro.tex}
\section{Related Work}
\label{sec:relatedw}
\subfile{sub_files/2relatedw.tex}
\section{Weakly-supervised Semantic Segmentation Transformer (\name)}
\label{sec:method}
\subfile{sub_files/3method.tex}
\section{Experiments}
\label{sec:experiments}
\subfile{sub_files/4experiments.tex}
\section{Conclusion}
\label{sec:conclusion}
\subfile{sub_files/5conclusion.tex}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\newpage

\begin{figure*}[htp]
\centering
    \includegraphics[width=1.\textwidth]{figure/CA_PAcomparisonv1.0.pdf}
\caption{Comparison of the mean-sum results and the weight-based results. (a) shows the cross-attention maps from the different attention heads for the ``plane" category. (b) shows the result obtained by the original mean-sum approach. (c) shows the result obtained by our proposed weight-based approach. (d) shows the patch-attention maps from the different attention heads corresponding to the ``background" point. (We denote the query point with the ``'') (e) shows the result obtained by the original mean-sum approach. (f) shows the result obtained by our proposed weight-based approach.}
    \label{fig:attention_maps}
    
\end{figure*}

\appendix

\section{Appendix}


\subsection{Implementation Details}
\label{sec:imple}
\subfile{sub_files/Aimpl.tex}



\subsection{Additional Ablations}

\label{sec:ablation}
\subfile{sub_files/Babl.tex}
\subsection{Additional Quantitative Results}
\label{sec:results}
\subfile{sub_files/Cresults.tex}
\subsection{Additional Visualization Results}
\label{sec:visual}
\subfile{sub_files/Dvisualresults.tex}


\end{document}
