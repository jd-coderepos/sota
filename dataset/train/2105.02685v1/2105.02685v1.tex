\subsection{Binary Sentence Generation}



\subsubsection{Human Evaluation} 
In \autoref{tab:human_annotation}, we report the performances of systems when evaluated by humans on the polarity transfer task. 100 sentences are generated by each system and 3 english native speakers are asked to annotate each sentence along 3 dimensions (\textit{i.e} fluency, sentiment and content preservation). Turkers assign binary labels to fluency and sentiment (following the protocol introduced in \citet{jalalzai2020heavy}) while content is evaluated on a likert scale from 1-5. For content preservation, both the input sentence and the generated sentence are provided to the turker. The annotator agreement is measure by the Krippendorff Alpha\footnote{Krippendorff Alpha measures of inter-rater reliability in :  is perfect disagreement and  is perfect agreement.} \cite{krippendorff2018content}. The Krippendorff Alpha is:  on the sentiment classification,  for fluency and  for content preservation.

\begin{table}[t]
\centering
   \begin{tabular}[h]{c|ccc}
   \hline 
   Model &  Fluency & Content & Sentiment\\\hline
Human& 0.80 & 3.4 & 0.78 \\\hline
 & 0.60& 2.4 & 0.63 \\
 & 0.62& 2.6 & 0.65 \\
  & 0.68 & 2.6 &  0.63 \\
 & 0.70 & 2.4 &  0.65  \\
 &0.68 & 2.9 &  0.70  \\
 &0.76& 3.0 &  0.58 \\\hline
\end{tabular}
\caption{Human annotation of generated samples. For this comparison we rely on the sentences provided in \url{https://github.com/rpryzant/delete_retrieve_generate}. Human annotations are also provided by \citet{li2018delete}. We have reprocessed the provided sentence using a tokenizer based on SentencePiece \cite{tok_0,tok_1}. Since there is a trade-off between automatic evaluation metrics (\textit{i.e} BLEU, Perplexity and Accuracy of Style Transfer), we set
minimum thresholds on BLEU and on style transfert accuracy. The best model that met the threshold on validation is selected. We will release--along with our code--new generated sentences for comparison.}
\label{tab:human_annotation}
\end{table}
\subsection{Content preservation using Cosine Similarity}
\begin{figure} 

\centering     \includegraphics[width=0.45\textwidth]{style_sent/cossim_sent_True.pdf}
\caption{Content preservation measured by the cosine similarity. }\label{fig:add_style_transfert_sentiment}
\end{figure} 

\autoref{fig:add_style_transfert_sentiment} measures the content preservation measured using cosine similarity for the sentence generation task using sentiment labels. As with the BLEU score, we observe that as the learnt representation becomes more entangled ( increases) less content is preserved. Similarly to BLEU the model using the KL bound conserves outperforms other models in terms of content preservation for .

\subsection{Example of generated sentences}
\autoref{tab:example_sentences_st} gathers some sentences generated by the different sentences for different values of . 

\textbf{Style transfert.} From \autoref{tab:example_sentences_st}, we can observe that the impact of disentanglement on a qualitative point of view. For small values of  the models struggle to do the style transfer (see example 2 for instance). As  increases disentanglement becomes easier, however, the content becomes more generic which is a known problem (see \cite{mmi} for instance). 



\textbf{Example of ``degeneracy" for large values of .} For sentences generated with the baseline model a repetition phenomenon appears for greater values of . For certain sentences, models ignore the style token (\textit{i.e.}, the sentence generated with a positive sentiment is the same as the one generated with the negative sentiment). We attribute this degeneracy to the fact that the model is only trained with  sharing the same sentiment which appears to be an intrinsic limitation of the model introduced  by \cite{text}.

\textbf{Analysis of performances of vCLUB-S} Similarly to what can be observed with automatic evaluation \autoref{tab:example_sentences_st} shows that the system based on vCLUB-S has only two regimes: “light” disentanglement and strong disentanglement. With light disentanglement the decoder fail at transferring the polarity and for strong disentanglement few content features remain and the system tends to output generic sentences.

\section{Additional Results on Multi class Sentence Generation}\label{sec:addition_multu}
Results on the multi-class style transfer and on are reported in \autoref{fig:accuracy_category_st} Similarly than in the binary case there exists a trade-off between content preservation and style transfer accuracy. We observe that the BLEU score in this task is in a similar range than the one in the gender task, which is expected because data come from the same dataset where only the labels changed.


\begin{table}
\resizebox{0.5\textwidth}{!}{\begin{tabular}{ll|l} \hline
 & Model	& Sentence	\\\hline
\iffalse
\multirow{7}{*}{0.1} &\textbf{Input}  & \textbf{the food was the best food i've ever experienced.}  \\\hline
&Adv  &  the food was the best i've ever had in. \\
&vCLUB-S  &  the food was the best i've ever had. \\
&KL  &  	the food was the best food i've ever experienced. \\
&  & the food was the best food i've experienced.  \\
&  &  the food was so good and the best i ever had. \\
&  & the food is so good i will be going back.  \\\hline
\multirow{7}{*}{1} &Input  & the food was the best food i've ever experienced.  \\\hline
&Adv  &  	the food was the best i've ever eaten here. \\
&vCLUB-S  & the food was the best.  \\
&KL  &  	the food was the best food i've ever experienced. \\
&  & 	the food was the best i've ever eaten at.  \\
&  & the food was amazing as well as i am extremely satisfied.  \\
&  &  	the food was very good and the service good. \\\hline
\multirow{7}{*}{5} &Input  & the food was the best food i've ever experienced.  \\\hline
&Adv  & 	i love this place.  \\
&vCLUB-S  & i love it. \\
&KL  &   the food was the best i've ever eaten here.\\
&  &  the food is ok, but the service is terrible. \\
&  &  	the food is always good but the service is always bad. \\
&  &  the food was ok and very good. \\\hline
\multirow{7}{*}{10} &Input  & the food was the best food i've ever experienced.  \\\hline
&Adv  & i love this place.  \\
&vCLUB-S  & i love it. \\
&KL  &  the food was excellent, but i love this food. \\
&  & the food was worst at best.  \\
&  & the food was not well cooked with the sauce.  \\
&  & 	the food wasn't bad but it was not good.  \\\hline\hline
\fi \multirow{7}{*}{0.1} &\textbf{Input}  &  	\textbf{It's freshly made, very soft and flavorful.} \\\hline
&Adv  & it's crispy and too nice and very flavor.  \\
&vCLUB-S  & It's freshly made, and great. \\
&KL  & it's a huge, crispy and flavorful.  \\
&  &  it's hard, and the flavor was flavorless. \\
&  & it's very dry and not very flavorful either.  \\
&  & it's a good place for lunch or dinner.  \\ \hline
\multirow{7}{*}{1} &Input  & 	it's freshly made, very soft and flavorful.  \\\hline
&Adv  &  it's not crispy and not very flavorful flavor. \\
&vCLUB-S  & It's bad. \\
&KL  & 	it's very fresh, and very flavorful and flavor.  \\
&  & it's not good, but the prices are good.  \\
&  &  it's not very good, and the service was terrible. \\
& &	it was a very disappointing experience and the food was awful.   \\\hline
\multirow{7}{*}{5} &Input  & 	it's freshly made, very soft and flavorful.  \\\hline
&Adv  &  	i hate this place. \\
&vCLUB-S  & i hate it. \\
&KL  &  	it's very fresh, flavorful and flavorful. \\
&  &  	it's not worth the money, but it was wrong. \\
&  &  it's not worth the price, but not worth it. \\
&  & 	it's hard to find, and this place is horrible.  \\\hline
\multirow{7}{*}{10} &Input  & 	it's freshly made, very soft and flavorful.  \\\hline
&Adv  & i hate this place.  \\
&vCLUB-S  & i hate it. \\
&KL  & 	it's a little warm and very flavorful flavor.  \\
&  & it was a little overpriced and not very good.  \\
&  &  	it's a shame, and the service is horrible. \\
&  & 	it's not worth the \D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}\lambdaD_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}D_{\alpha=1.3}D_{\alpha=1.5}D_{\alpha=1.8}$  & we ordered NUM wings, NUM of NUM tacos and we waited. \\ \hline\hline
\caption{Sequences generated by the different models on the binary sentiment conditional sentence generation task.}
\label{tab:example_sentences_cg}
\end{longtable}
\fi
\twocolumn
\iffalse
\subsubsection{Results on Conditional Sentence Generation}
\begin{figure*} \centering
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{style_sent/bleu_sent_False.pdf}\vspace{-.3cm}
\caption{}\label{fig:bleu_st_cg}  
\end{subfigure} \begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{style_sent/accu_sent_False.pdf}\vspace{-.3cm}
\caption{}\label{fig:accuracy_sentiment_cg} \end{subfigure} \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{style_sent/ppl_sent_False.pdf}\vspace{-.3cm}
        \caption{}\label{fig:ppl_sentiment_cg}
    \end{subfigure}\vspace{-.3cm}
    \caption{Numerical experiments on conditional sentence generation. Results include BLEU (\autoref{fig:bleu_st_cg}), style transfer accuracy (\autoref{fig:accuracy_sentiment_cg}) and sentence fluency (\autoref{fig:ppl_sentiment_cg}).}\label{fig:conditionnal_sentence_sentiment}
\end{figure*}
\fi