\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/fig1_moti_negattn_1111.pdf}
\vspace{-0.6cm}
    \caption{
Comparison of highlight-ness~(saliency score) when relevant and non-relevant queries are given.
        We found that the existing work only uses queries to play an insignificant role, thereby may not be capable of detecting negative queries and video-query relevance; saliency scores for clips in ground-truth~(GT) moments are low and equivalent for positive and negative queries.
On the other hand, query-dependent representations of QD-DETR result in corresponding saliency scores to the video-query relevance and precisely localized moments.
}
    \label{fig:motivation_ex}
\end{figure}


\section{Introduction}
Along with the advance of digital devices and platforms, video is now one of the most desired data types for consumers~\cite{apostolidis2021video,wu2017deep}.
Although the large information capacity of videos might be beneficial in many aspects, e.g., informative and entertaining, inspecting the videos is time-consuming, so that it is hard to capture the desired moments~\cite{anne2017localizing,apostolidis2021video}. 



Indeed, the need to retrieve user-requested or highlight moments within videos is greatly raised.
Numerous research efforts were put into the search for the requested moments in the video~\cite{anne2017localizing, gao2017tall, liu2015multi, escorcia2019temporal} and summarizing the video highlights~\cite{zhang2016video, mahasseni2017unsupervised, badamdorj2022contrastive, wei2022learning}.
Recently, Moment-DETR~\cite{momentdetr} further spotlighted the topic by proposing a QVHighlights dataset that enables the model to perform both tasks, retrieving the moments with their highlight-ness, simultaneously.



















When describing the moment, one of the most favored types of query is the natural language sentence~(text)\cite{anne2017localizing}. 
While early methods utilized convolution networks~\cite{zhang2020learning, gao2021fast, wang2020temporally}, recent approaches have shown that deploying the attention mechanism of transformer architecture is more effective to fuse the text query into the video representation.
For example, Moment-DETR~\cite{momentdetr} introduced the transformer architecture which processes both text and video tokens as input by modifying the detection transformer~(DETR), and UMT~\cite{umt} proposed transformer architectures to take multi-modal sources, e.g., video and audio. 
Also, they utilized the text queries in the transformer decoder.
Although they brought breakthroughs in the field of MR/HD with seminal architectures, they overlooked the role of the text query.
To validate our claim, we investigate the Moment-DETR~\cite{momentdetr} in terms of the impact of text query in MR/HD~(Fig.\ref{fig:motivation_ex}).
Given the video clips with a relevant positive query and an irrelevant negative query, we observe that the baseline often neglects the given text query when estimating the query-relevance scores, i.e., saliency scores, for each video clip.
























To this end, we propose Query-Dependent DETR~(QD-DETR) that produces query-dependent video representation.
Our key focus is to ensure that the model's prediction for each clip is highly dependent on the query.
First, to fully utilize the contextual information in the query, we revise the transformer encoder to be equipped with cross-attention layers at the very first layers.
By inserting a video as the query and a text as the key and value of the cross-attention layers, our encoder enforces the engagement of the text query in extracting video representation.
Then, in order to not only inject a lot of textual information into the video feature but also make it fully exploited, we leverage the negative video-query pairs generated by mixing the original pairs.
Specifically, the model is learned to suppress the saliency scores of such  negative~(irrelevant) pairs.
Our expectation is the increased contribution of the text query in prediction since the videos will be sometimes required to yield high saliency scores and sometimes low ones depending on whether the text query is relevant or not.
Lastly, to apply the dynamic criterion to mark highlights for each instance, we deploy a saliency token to represent the entire video and utilize it as an input-adaptive saliency criterion. 
With all components combined, our QD-DETR produces query-dependent video representation by integrating source and query modalities.
This further allows the use of positional queries~\cite{dabdetr} in the transformer decoder.
Overall, our superior performances over the existing approaches validate the significance of the role of text query for MR/HD.





























%
