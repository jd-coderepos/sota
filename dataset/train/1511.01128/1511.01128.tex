\documentclass[3p,11pt,preprint]{elsarticle}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bbm}

\newcommand{\rqedhere}{\tag*{\hspace{-1em}\square}}

\biboptions{authoryear}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{decorations.pathreplacing}

\newdefinition{definition}{Definition}
\newdefinition{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newproof{proof}{Proof}

\let\set\mathbbm
\def\k{\set D}
\newcommand{\qk}{\set{K}}
\newcommand{\D}{x}
\newcommand{\alg}{\k[\D;\sigma,\delta]}
\newcommand{\qalg}{\qk[\D;\sigma,\delta]}
\newcommand{\lc}{\operatorname{lc}}
\newcommand{\gcrd}{\operatorname{gcrd}}
\newcommand{\syl}{\operatorname{Syl}}
\newcommand{\cont}{\operatorname{cont}}
\newcommand{\pquo}{\operatorname{pquo}}
\newcommand{\prem}{\operatorname{prem}}
\newcommand{\pres}{\operatorname{sres}}
\newcommand{\prs}[3]{(#1_i)_{i\in \{#2,\dots,#3\}}}
\newcommand{\cc}{\operatorname{tc}}

\journal{Journal of Symbolic Computation}

\begin{document}
\begin{frontmatter}
  \title{Improved Polynomial Remainder Sequences for Ore~Polynomials}

  \author{Maximilian Jaroschek\fnref{fn1}}
  \ead{mjarosch@risc.jku.at}
  \address{Research Institute for Symbolic Computation\\ Johannes Kepler University\\ A4040 Linz, Austria}
  \fntext[fn1]{Supported by the Austrian Science Fund (FWF) grant Y464-N18}

\begin{abstract}
Polynomial remainder sequences contain the intermediate results of the Euclidean algorithm when applied to (non-)commutative polynomials. The running time of the algorithm is dependent on the size of the coefficients of the remainders. Different ways have been studied to make these as small as possible. The subresultant sequence of two polynomials is a polynomial remainder sequence in which the size of the coefficients is optimal in the generic case, but when taking the input from applications, the coefficients are often larger than necessary. We generalize two improvements of the subresultant sequence to Ore polynomials and derive a new bound for the minimal coefficient size. Our approach also yields a new proof for the results in the commutative case, providing a new point of view on the origin of the extraneous factors of the coefficients.
\end{abstract}

\begin{keyword}
Ore polynomials \sep greatest common right divisor \sep polynomial remainder sequences \sep subresultants

11A05 \sep 68W30
\end{keyword}

\end{frontmatter}

\section{Introduction}

When given a system of differential equations, one might be interested in finding the common solutions of these equations. In order to do so, one can compute another differential equation whose solution space is the intersection of the solution spaces of the equations in the original system. One way to do this is to translate the equations into operators and use the Euclidean algorithm to compute their greatest common right divisor. The solution space of the greatest common right divisor then consists of the desired elements. 

Similarly, given a sequence of numbers  that satisfies two different recurrence equations, the Euclidean algorithm is used in applications to find a reasonable candidate for the least order equation of which  is a solution.

Carrying out Euclid's algorithm applied to two polynomials over a domain  usually requires~a~prediction of the denominators that might appear in the coefficients of the remainders in order to bypass costly computations in the quotient field of . While such a prediction can be done easily, the growth of the coefficients of the remainders can be tremendous, which might result in an unnecessary high running time. This can be avoided by dividing out possible content of the remainders to make their coefficients as small as possible. For commutative polynomials as well as for non-commutative operators, different ways have been extensively studied to find factors of the content in the sequence of remainders without computing the GCD of the coefficients of each element of the sequence. Most notably in this respect are subresultant sequences, where the growth of the coefficients can be reduced from exponential to linear in the number of reduction steps in the Euclidean algorithm. When taking generic, randomly 
generated input, the coefficient size in the subresultant sequence is usually optimal, but when taking the input from applications in e.g. combinatorics or physics, the remainders still have non-trivial content in many cases. 

For commutative polynomials, some ways are known to improve on subresultants. In this article we generalize two of these results to Ore polynomials and we also give a new proof for the commutative case that is based on the structure of subresultants as matrix determinants. Furthermore, we use these results to derive a new bound for the coefficient size of the content-free remainders.

In Section~\ref{prelimsec} the basic notions of Ore polynomial rings are stated. A precise definition and examples of polynomial remainder sequences are given in Section~\ref{prssec} and further details on the subresultant sequence are then presented in Section~\ref{sressec}. The main results of this article can be found in Sections~\ref{contentsec}~and~\ref{improvsec}, where we first describe how additional content in the subresultant sequence can emerge and then use these results to improve on the Euclidean algorithm and to get~a~new bound for the size of the coefficients.

\section{Preliminaries}
\label{prelimsec}

The algebraic framework for different kinds of operators that we consider here are Ore polynomial rings, which were introduced by \O ystein Ore in the 1930's. We provide an overview of some basic facts that suffice our needs and that can be found in \citet{ore} and \citet{bronstein}.
\begin{definition}
Let  be a commutative domain,  the set of univariate polynomials over  and let  be an injective endomorphism.
\begin{enumerate}
 \item  A map  is called pseudo-derivation w.r.t. , if for any  

 \item Suppose that  is a pseudo-derivation w.r.t. . We define the Ore polynomial ring  with componentwise addition and the unique distributive and associative extension of the multiplication rule

 to arbitrary polynomials in . To clearly distinguish this ring from the standard polynomial ring over , we denote it by .
\end{enumerate}
\end{definition}

 Elements of an Ore polynomial ring are called operators and are denoted by capital letters. We refer to the leading coefficient of an operator  as , to the coefficient of  in  as  and to the polynomial degree of  in  as the order  of . 

\begin{example}
\label{oreex} Commonly used Ore polynomial rings are:
 \begin{enumerate}
  \item , the ring of commutative polynomials over .
  \item , the ring of linear ordinary differential operators.
  \item If  is the forward shift in , i.e. , then  is the ring of linear ordinary recurrence operators.
  \item If  is the -shift in , i.e. , then  is the ring of Jackson's -derivative operators.
 \end{enumerate}
\end{example}

In this article, we consider the following situation: Let  be a Euclidean domain with degree function  and let  be an Ore polynomial ring where  is an automorphism. For any operator~, we define  to be the maximal coefficient degree of . The content  of  is the greatest common divisor of all the coefficients of  and it is defined to be  if  is a field. It is possible to extend  to an Ore polynomial ring over the quotient field~ of  by setting  and~ for   (see \cite{zli2}, Proposition 2.2.1). We will denote this ring by  without making it explicit that the automorphism and the pseudo-derivation are extensions of the functions used in . It is well known that for any two operators , there exists a greatest common right divisor (GCRD) and it can be made unique (up to units in ) by 
setting  to a nonzero -
left multiple of any GCRD of  and~ that has coefficients in  but does not have any content in . 

Throughout this article, we let ,  be such that  and~ is the GCRD of  and~.

\begin{definition}
 For  and~,  is obtained by applying  times  to  and~, where  is the inverse map of . The th -factorial of  is defined as the product 
\end{definition}

\section{Polynomial Remainder Sequen\-ces for Ore Polynomials}
\label{prssec}

The greatest common right divisor of  and~ can be computed by using the Euclidean algorithm. If we multiply any intermediate result that appears during the execution of the  algorithm by an element of , the final output will be a -left multiple of . This amount of freedom allows us to optimize the running time by choosing these factors appropriately. In order to be able to formulate improvements of this kind, the notion of polynomial remainder sequences has been introduced. Each element of such a sequence corresponds to a remainder computed in one iteration of the Euclidean algorithm.
\begin{definition}
 Let  and~ 
be sequences in ,  a sequence in  and let  and~ be sequences in  such that
 
and all  are nonzero except for . We call the sequence  a polynomial remainder sequence (PRS) of  and~. 
\end{definition}

A PRS of  and~ is uniquely determined by specifying the  and~. Whenever we talk about~a~PRS , we allow ourselves to refer to the related sequences ,  etc. as in the above definition without explicitly introducing them. 

In order to efficiently compute , one wants to make sure that all the remainders are elements of  rather than . This can be achieved by choosing the  in a way such that the quotient of any two consecutive remainders has coefficients in . To this extent, for  set  and division with remainder yields  and~ in  with:

We call  the pseudo-quotient of  and~ and~ the pseudo-remainder of  and~.

The  are used to make sure that computations can be done in  and the  control the coefficient growth in a PRS. We want  to contain as many factors of the content of  as possible without much computational overhead needed to obtain these factors.

\begin{example}
\label{prsex}
 Set  and
\begin{enumerate}
 \item . This is called the pseudo PRS of  and~. Here, no content will be divided out.
 \item . This is called the primitive PRS of  and~. The coefficients of the remainders will be as small as possible, but it is necessary to compute the GCD of the coefficients of each remainder in order to get the .
 \item \label{sresprs}The subresultant PRS of  and~ (see Section~\ref{sressec}) is given by

where

In this PRS, the content that is generated systematically by pseudo-remaindering will be cleared from the remainders.
\end{enumerate}
\end{example}

While in all of the above PRSs the remainders are elements of , the degrees of the coefficients differ drastically, as illustrated in the following example. It can be shown that the degrees of the coefficients in the pseudo PRS grow exponentially with , which renders this PRS practically useless. The growth in the subresultant and primitive PRS is linear in .

\begin{example}
\label{mainex}
 Assume we are given a finite sequence of rational numbers that comes from a sequence  which admits a linear recurrence equation with polynomial coefficients. If the amount of data is sufficiently large, we are able to guess recurrence operators of some fixed order and maximal coefficient degree that annihilate , i.e. the operators applied to the sequence give zero. (For details on guessing and a Mathematica implementation of the method, see \cite{kauers}.)
 For example, consider 
 
 Given the first 300 terms of this sequence, we can find two operators  and~ in  with ,  and maximal coefficient degree ,  resp. Both operators annihilate the given sequence, but none of them is of minimal order. To get an annihilating minimal order operator, we compute the GCRD of  and~ in . Table 1 shows the maximal coefficient degrees of the remainders for different PRSs of  and~.
\begin{center}
\begin{tabular}{l|c|c|c|c|c|c|c}
PRS &  &  &  &  &  &  & \\
\hline
 pseudo & 11 & 22 & 49 & 114 & 271 & 650 & 1565\\
\hline
 subresultant & 11 & 16 & 21 & 26 & 31 & 36 & 41\\
\hline
 primitive & 9 & 12 & 15 & 18 & 21 & 24 & 21\\
\end{tabular}\\
\vspace{0.1cm}\scriptsize{Table 1: Maximal coefficient degrees for different PRSs.}
\end{center}
The example confirms that the degrees in the pseudo PRS grow exponentially, whereas the subresultant PRS and the primitive PRS show linear growth. At the same time, the degrees in the subresultant PRS are not as small as possible. This behavior is typical not only for this pair~~and~, but in general for operators coming from applications. For randomly generated operators, the subresultant PRS and the primitive PRS usually coincide. Our goal is to understand the difference between randomly generated input and the operators  and~ as above and to identify the source of some (and most often all) of the additional content in the subresultant PRS. To make use of this knowledge, we will then adjust the formulas for  and~ from Example~\ref{prsex}.\ref{sresprs} so that we get a PRS with smaller degrees without having to compute the content of every remainder.
\end{example}

\section{Subresultant Theory for Ore Polynomials}
\label{sressec}

For commutative polynomials, the theory of subresultants was intensively studied by \cite{brown}, \cite{browntraub}, \cite{collins} and \cite{loos}. The main idea is to translate relations between the elements of a PRS like the B\'ezout relation or the (pseudo-)remainder formula into linear algebra. A central tool in this context is the Sylvester matrix, which, roughly speaking, contains the coefficients of all the monomial multiples of the input polynomials that are necessary to compute remainders of any possible degree. The remainders in the subresultant sequence turn out to be polynomials whose coefficients are determinants of certain submatrices of this matrix. \cite{zli} generalized these results to Ore polynomials. 

\begin{center}
\begin{tikzpicture}
{
\matrix (m) [matrix of math nodes,left delimiter=(\hspace{0.7cm},right delimiter={)}]
  {
   \lc(x^{d_B-1}A) & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cc(x^{d_B-1}A)      \\
                   & \text{\llap{}}\hspace{0.5cm} &        &        &        &        &        & \vdots \\
                   &        & \text{\llap{}}\hspace{0.3cm} & \cdots & \cdots & \cdots & \cdots & \cc(A)      \\ 
   \lc(x^{d_A-1}B) & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cc(x^{d_A-1}B)      \\
                   & \text{\llap{}}\hspace{0.5cm} &        &        &        &        &        & \vdots \\
                   &        & \text{\llap{}}\hspace{0.3cm} &        &        &        &        & \vdots \\
                   &        &        & \text{\llap{}}\hspace{0.3cm} & \cdots & \cdots & \cdots & \cc(B)      \\ 
  };

\fill[fill=gray!50, opacity=0.3 ] (-2.3,0.5) -- (-5.2,2.5) -- (4.5,2.5) -- (4.5,0.5) -- cycle ;
\fill[fill=gray!50, opacity=0.3 ] (-1.6,-2.5) -- (-5.2,0.5) -- (4.5,0.5) -- (4.5,-2.5) -- cycle ;

\draw [decorate,decoration={brace,amplitude=6pt,mirror}]
(5.0,0.45) -- (5.0,2.5) node [black,midway,xshift=0.6cm] 
{\footnotesize };

\draw [decorate,decoration={brace,amplitude=6pt,mirror}]
(5.0,-2.5) -- (5.0,0.35) node [black,midway,xshift=0.6cm] 
{\footnotesize };

}
\end{tikzpicture}\\
\vspace{0.1cm}\scriptsize{Figure 1: The form of the Sylvester matrix of  and~. Entries outside of the gray area are zero.}
\end{center}
The Sylvester matrix  is defined to be the matrix of size  with the following entries: If  and~, the entry in the th row and~th column is the th coefficient of . If  and~, the entry in the th row and~th column is the th coefficient of . 

For  with , the matrix  is obtained from  by removing the rows  to , the rows  to , the columns  to  and the last  columns except for the column . 
\begin{center}
\begin{tikzpicture}
{
\matrix (m) [matrix of math nodes,left delimiter=(\hspace{0.7cm},right delimiter={)}]
  {
   \phantom{\lc(x^{d_B-1}A)} & \phantom{\Big(} & \phantom{\cdots} & \phantom{\cdots} & \phantom{\cdots} & \phantom{\cdots} & \phantom{\cdots} & \phantom{\cc(x^{d_B-1}A)}\hspace{0.5cm}\quad      \\
   \phantom{\lc(x^{d_B-1}A)}\\ 
  \phantom{\lc(x^{d_B-1}A)}\\
\phantom{\lc(x^{d_B-1}A)}\\
\phantom{\lc(x^{d_B-1}A)}\\
\phantom{\lc(x^{d_B-1}A)}\\
\phantom{\lc(x^{d_B-1}A)}\\
  };


\fill[fill=gray!50, opacity=0.3 ] (-2.3,0.5) -- (-5.2,2.5) -- (4.5,2.5) -- (4.5,0.5) -- cycle ;
\fill[fill=gray!50, opacity=0.3 ] (-1.6,-2.5) -- (-5.2,0.5) -- (4.5,0.5) -- (4.5,-2.5) -- cycle ;

\draw[fill=black] (-5.2,2.22) -- (4.6,2.22) ;
\draw[fill=black] (-5.2,1.72) -- (4.6,1.72) ;

\draw[fill=black] (-5.2,0.3) -- (4.6,0.3) ;
\draw[fill=black] (-5.2,-0.2) -- (4.6,-0.2) ;

\draw[fill=black] (-4.8,2.62) -- (-4.8,-2.5) ;
\draw[fill=black] (-4.2,2.62) -- (-4.2,-2.5) ;

\draw[fill=black] (4.25,2.62) -- (4.25,-2.6) ;
\draw[fill=black, decorate, decoration={border, segment length=10pt, angle=0} ] (3.65,2.6) -- (3.65,-2.6) ;
\draw[fill=black] (2.95,2.62) -- (2.95,-2.6) ;
}
\end{tikzpicture}\\
\vspace{0.1cm}\scriptsize{Figure 2: Sketch of . The lines indicate the removed rows and columns. The column under the dotted line is added again.}
\end{center}
\begin{definition}
 For , the polynomial  is called the th (polynomial) subresultant of  and~.
 If the order of  is strictly less than , the th subresultant of  and~ is called defective, otherwise it is called regular.
 The subresultant sequence of  and~ of the first kind is the subsequence of  that contains , , the trailing zero and all nonzero  for which  is regular.
\end{definition}

\begin{theorem}[\cite{zli}]
\label{zlithm}
 The polynomial remainder sequence given by  and~ as in Example~\ref{prsex}.\ref{sresprs}, the subresultant PRS, is equal to the subresultant sequence of  and~ of the first kind.
\end{theorem}

\section{Identifying Content of Polynomial Subresultants}
\label{contentsec}

The representation of subresultants in terms of determinants of the matrices  makes it possible to identify content by exploiting the special form of these matrices as well as the correspondence between rows of the Sylvester matrix and monomial multiples of  and~. For the case of commutative polynomials, some results are known for detecting such additional content. We generalize two results to the Ore setting. The first (Theorem~\ref{simpleimprov}) is a generalization of an observation mentioned in \cite{brown}, which carries over quite easily to the Ore case. The second (Theorem~\ref{mainthm}) usually performs better in terms of coefficient size of the remainders, but a heuristic argument is necessary to use it algorithmically (see Section~\ref{improvsec}). 

\begin{theorem}
\label{simpleimprov}
 With  and~ for , we get: 
\end{theorem}
\begin{proof}
 Let  be fixed. The coefficients of  are the determinants of the matrices  for . The first column of all of these matrices is

Laplace expansion along this column proves the claim.\qed
\end{proof}

Not all of the subresultants of  and~ are in the subresultant PRS of  and~. To make use of Theorem~\ref{simpleimprov} for a new PRS, we need a minor specialisation of the statement:

\begin{corollary}
\label{cor1}
  Let  be the subresultant PRS of  and~ (not necessarily normal). If we choose  then  for .
\end{corollary}
\begin{proof}
 Suppose  is the th subresultant of  and~. Then, by the definition of the subresultant sequence of the first kind and Theorem~\ref{zlithm}, the st subresultant of  and~ is regular. Because of this and the subresultant block structure (see \cite{zli}),  is of order  and so  is equal to . By Theorem~\ref{simpleimprov}, the content of  is divisible by . 
 It is easy to see that  is equal to .\qed
\end{proof}

In the commutative case, a second source of additional content was determined, although this result is not widely known. The following theorem can be found in \cite{knuth}:

\begin{theorem}
\label{knuththm}
 Let  be such that the subresultant PRS of  and~ is normal, i.e.  for , and let  be the GCD of  and~. Then  for .
\end{theorem}

A generalization of Theorem~\ref{knuththm} to Ore polynomials is not straightforward, as Example~\ref{knuthex} shows.

\begin{example}[Example~\ref{mainex} cont.]
 \label{knuthex}
If we take  and~ as in Example~\ref{mainex}, then the leading coefficient of the GCRD of  and~ is ,
where  is a polynomial of degree 17. The subresultant PRS of  and~ turns out to be normal and~ is of order . By Theorem~\ref{knuththm}, if the polynomials were elements of ,  would be divisible by  and a naive translation of the theorem to the non-commutative case suggests divisibility by a polynomial of degree at least~36. The (monic) content of , however, is only , which is contained in, but not equal to, .
\end{example}

Again in the commutative case, let ,  be such that  and . \cite{knuth} proves Theorem~\ref{knuththm} by showing that if  is the subresultant PRS~of~~and~ and~ is the subresultant PRS of , , then . This approach is problematic for Ore polynomials, because there the 's and the 's have coefficients in  and not necessarily in . This means that even after showing that a quotient ~is~a -left multiple of some subresultant  of  and~, the left factor and the denominators in the coefficients of  might not be coprime and thus lead to cancellation. Therefore we will not only describe why in the non-commutative case only some factors of  appear as content, but we also present a new proof of Theorem~\ref{knuththm} that makes it more explicit where the additional content comes from. Moreover, we won't require the 
remainder sequence to be normal. 

In , if  is a multiple of the primitive polynomial , then their quotient will always have coefficients in , and therefore, the leading coefficient of  contains all the factors of the leading coefficient of . For Ore polynomials, this is not necessarily true, since the quotient of  and  might be an element of . Still, different left multiples of  in  may share some common factors in their leading coefficients, as described in Lemma \ref{lcthm}.

\begin{lemma}
\label{lcthm}
 Let  be fixed, let   be a left ideal and let  be any element of  of order  such that, among all the operators of order  in , its leading coefficient~ is minimal with respect to the degree. Then  is independent of the choice of  (up to multiplication by units in ) and for any  with  we have . 
\end{lemma}
\begin{proof}
 Assume there are  for which the claim  does not hold.
 We let  and get , thus  by assumption.
 Division with remainder yields nonzero  such that 
 Hence the operator  is an element of  whose leading coefficient has degree less than . This contradicts the choice of .

\noindent For the uniqueness, let  be any other operator of order  with minimal leading \mbox{coefficient} degree. By what was just shown above, we get  and , so  and  are associates.
\qed
\end{proof}

 \begin{definition}
  Consider ,  and  from Lemma~\ref{lcthm}. The shift  of the leading coefficient of~ is called the essential part of  at order~. If there is no operator in  for some order , the essential part of  at order  is defined to be .
 \end{definition}

Let  and  where  is the left ideal generated by~. We give an informal explanation of essential parts of  in terms of solutions of~, i.e. functions that are annihilated by . Any non-removable singularity of a solution of  corresponds to a root of the leading coefficient of , but not for any root of  there has to be a solution with a non-removable singularity at that point. Any solution of  is also a solution of every operator in~ and it can happen that there are nonzero -left multiples of  in  that have strictly smaller leading coefficient degree than . If such a \textit{desingularized} operator exists, it means that some of the roots of  can be removed by multiplying  with another operator from the left. These removable roots are called the \textit{apparent singularities} of 
. It is shown in \cite{mythesis} that there exists a unique minimal (w.r.t.\ degree) essential part of  that appears in the essential parts of  at every order greater than . This minimal essential part of  is a polynomial whose roots are exactly the non-apparent singularities of , and it turns out that for each root of the essential part of , there is at least one solution of  that does not admit an analytic continuation at that point. A more detailed description of desingularization and apparent singularities of differential equations can be found in \cite{ince}. Further references and recent results on desingularization of Ore operators can be found in \cite{chen}.

Note that for commutative polynomials, by Gau\ss' Lemma, the essential part of a nonzero ideal at any order is equal to the leading coefficient of the primitive greatest common divisor of the ideal elements. 

For the remaining part of this article, let  be the left ideal generaed by  and~. We formulate our Ore generalization of Theorem~\ref{knuththm}, where now some of the essential parts of  play the role of the leading coefficient of the GCRD of  and .

\begin{theorem}
\label{mainthm}
 Let  and~. If  is the essential part of  at order~ for , then

\end{theorem}

\begin{proof}
 For any ,  is of size  and if the last column is removed, the resulting matrix does not depend on  anymore. For , let  be the set of all  matrices obtained by removing the last  columns and any  rows from . The th coefficient of  is the determinant of  and Laplace expansion along the last column shows that it is a -linear combination of the elements of . By induction on~ we show that the determinant of any element of  is divisible by . The theorem is then proven by setting .

 For , the only entry in a matrix in  is either zero or the leading coefficient of a monomial left multiple of  or  of order , so the claim follows from Lemma~\ref{lcthm}.

 Now suppose the claim is true for  and let  be any element of . If the determinant of  is zero, then there is nothing to show. Consider the case where . 
Then there is a  such that . By Cramer's rule, the th component~ of  is of the form  where  is the determinant of some element of . By induction hypothesis it is divisible by . 
 Every row in  corresponds to an operator of the form  or  for , minus some of the lower order terms. For the th row, , we denote the corresponding operator by . By the definition of , the operator  will have order  and leading coefficient .
 So if we set  and~, then  is an element in  of order  and its leading coefficient is . Lemma~\ref{lcthm} yields that  is divisible by , so we get in total .\qed
\end{proof}

In practice, the essential parts of  will most likely be the same at every order  with . In that case, Theorem~\ref{mainthm} is equivalent to the following simplification, where only the essential part of  at order  needs to be known.

\begin{corollary}
\label{oneespart}
 Let  and~. If  is the essential part of  at order~, then

\end{corollary}

\begin{proof}
 According to Lemma~\ref{lcthm},  divides the essential part of  at order  for any . If , then the th subresultnat of  and  is zero. Otherwise, Theorem~\ref{mainthm} yields that  is divisible by 
 
\end{proof}




Like for Theorem~\ref{simpleimprov}, an adjustment of Corollary~\ref{oneespart} to the block structure of the subresultant sequence of the first kind is needed in order to construct a new PRS.

\begin{corollary}
\label{cor2}
 Let  be the subresultant PRS of  and~ (not necessarily normal) and let  be the essential part of  at order . If we set  and  then  for .
\end{corollary}
\begin{proof}
 Suppose  is the th subresultant of  and~. As in the proof of Corollary~\ref{cor1}, we have that~ is equal to . So by Corollary~\ref{oneespart}, the content of  is divisible by . Simple hand calculation shows that this is equal to .\qed
\end{proof}

\section{Improved Polynomial Remainder Sequence}
\label{improvsec}

We now derive formulas for the  and~ that take into account the potential additional content characterized by Theorems~\ref{simpleimprov} and~\ref{mainthm}. For this we need the following lemma:

\begin{lemma}
\label{quothm}
 For :
 
\end{lemma}
\begin{proof}
 By Lemma 2.3 in \cite{zli}, the pseudo-remainder of  and~ is the st subresultant of  and~ (up to sign). Consequently, its coefficients are determinants of submatrices of  that contain one row corresponding to the operator  and~ rows corresponding to operators of the form , . Thus, by Lemma~2.2 in~\cite{zli}, it follows that (up to sign)
 
The pseudo-remainder formula \eqref{premformula} applied to   and~ is
Combining this with \eqref{eq4} and dividing the resulting equation by  from the left gives the desired result. \qed
\end{proof}

This now allows us to state  and~ for improved polynomial remainder sequences:

\begin{theorem}
\label{newabs}
 Suppose  is the subresultant PRS of  and~ and~ is any sequence in  with . Set . Then  is a PRS of  and~ with: 

where

\end{theorem}

\begin{proof}
 From the definition of  and the equations

it follows that

For the first summand on the right hand side, Lemma~\ref{quothm} yields

For the second summand, observe that since  equals , we have that  equals  for all . Thus

The proof is concluded by combining \eqref{eq1}, \eqref{eq2} and \eqref{eq3} and dividing the resulting equation by  from the left.\qed
\end{proof}

Two possible choices for  were presented in Corollary~\ref{cor1} and \ref{cor2}. The computation of~ in Corollary~\ref{cor1} is straightforward, but in Corollary~\ref{cor2}, the essential part of  (the ideal generated by  and ) at order~ is usually not known. A simple heuristic can solve this problem in most cases: As was shown in Lemma~\ref{lcthm}, the essential part of  at order~ appears in a shifted version in the leading coefficient of every nonzero ideal element with order less than or equal to . In particular it is contained in  and~. Thus, if  is the essential part of~ at order , we have

and in most cases, we not only have divisibility but equality. In fact, in all the examples we looked at that came from combinatorics or physics, this guess for the essential part turned out to be correct.

\begin{example}[Example~\ref{knuthex} cont.]
 We now use Theorem~\ref{newabs} and Corollaries~\ref{cor1}~and~\ref{cor2} to compute new PRSs of  and~ as in Example~\ref{mainex}. The essential part of  at order~ is , so , which is also the guess given by the right hand side of \eqref{eq5}. Applying Corollary \ref{cor1} yields the factors

whereas Corollary \ref{cor2} gives

 The improvements from Corollary~\ref{cor1}  are marginal, while the degrees in the improved PRS with the results from Corollary~\ref{cor2} are equal to the degrees in the primitive PRS, except for the very last step:
 \begin{center}
\begin{tabular}{l|c|c|c|c|c|c|c}
PRS &  &  &  &  &  &  & \\
\hline
 subresultant & 11 & 16 & 21 & 26 & 31 & 36 & 41\\
\hline
 improved (Cor.~\ref{cor1}) & 10 & 15 & 20 & 25 & 30 & 35 & 40\\
\hline
 improved (Cor.~\ref{cor2})& 9 & 12 & 15 & 18 & 21 & 24 & 27\\
\hline
 primitive & 9 & 12 & 15 & 18 & 21 & 24 & 21\\
\end{tabular}\\
\vspace{0.1cm}\scriptsize{Table 2: Maximal coefficient degrees for the subresultant, improved and primitive PRS.}
\end{center}
\end{example}

\begin{example}
 Although the remainders in the PRS based on Corollary~\ref{cor2} are usually primitive when starting from randomly generated operators or operators that come from some applications, it is not guaranteed that this is always the case. As an example, consider 

The second subresultant of  and~ is , so , but in the improved PRS, no content will be found. 

As mentioned, it may also happen that the guess for the essential part of  at order  is too large, for example:

Here, ) in the subresultant PRS is , but a factor  is predicted. The mistake in predicting the essential part can be noticed on the fly during the execution of the algorithm as soon as a remainder with coefficients in  appears. It is then possible to either switch to another PRS or to refine the guess of the essential part. One strategy to do so is to remove all the factors from the guess that could be responsible for the appearance of denominators. Let  be the guess for the essential part of  at  and let  be the non-trivial common denominator of the coefficients of a remainder  in the improved PRS. Furthermore let  be the set of all integers~ such that . Update ,  and  with

and continue the computation with these new values. For differential operators in , we have  and for recurrence operators in ,  contains all the integer roots of the polynomial .
\end{example}

\begin{example}
 We can guess two operators  and  in  of order , , resp. that annihilate the sequence

The GCRD of  and  is of order  and the essential part of  at  is of degree . The essential part of  at order , however, is of degree , so here we are in the rare case where the essential part of  at order  is only contained but not equal to the essential part at lower orders. Formula \eqref{eq5} only predicts the essential part of  at order   and during the GCRD computation, content that comes from lower order essential parts emerges
 \begin{center}
\begin{tabular}{l|c|c|c|c|c|c|c}
PRS &  &  &  &  &  &  & \\
\hline
 improved (Cor.~\ref{cor2})& 31 & 44 & 57 & 70 & 83 & 96 & 109 \rlap{\quad}\\
\hline 
 primitive & 31 & 44 & 50 & 56 & 62 & 68 & 74\\
\end{tabular}\\
{\vspace{0.1cm}\scriptsize{Table 3: Maximal coefficient degrees for the first few remainders in the improved and primitive PRS.}}
\end{center}
It is possible to guess the essential part of  at lower orders and then use Theorem~\ref{mainthm} to get the primitive remainders, but like in the direct computation of the primitive PRS, GCD computations in the base ring would be necessary after each division step.
\end{example}


 As another consequence of Theorem~\ref{mainthm}, we can give a new bound for the coefficient degrees of the primitive PRS in terms of the essential parts of the left ideal generated by  and . 
 \begin{theorem}
  Let  be the primitive PRS of  and~. Fix  and let  be such that  and . If  denotes the essential part of  at order , then
  
 \end{theorem}
\begin{proof}
 The bound follows directly from Hadamard's inequality, the subresultant block structure and Corollary~\ref{cor2}. \qed
\end{proof}

\section*{Acknowledgements}
I would like to thank Ziming Li and Manuel Kauers for their helpful comments and support during our personal communication. Also, I thank the reviewers for their careful reading of this article and for pointing out mistakes and shortcomings in the draft.

\bibliographystyle{model2-names}
\bibliography{main}
\end{document}
