\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage[]{booktabs}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[numbers, sort]{natbib}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Lifting Transformer for 3D Human Pose Estimation in Video} 

\author{
   Wenhao Li\textsuperscript{1}, 
   Hong Liu\textsuperscript{1},
   Runwei Ding\textsuperscript{1},
   Mengyuan Liu\textsuperscript{2}, 
   Pichao Wang\textsuperscript{3} \\
   \textsuperscript{1}Key Laboratory of Machine Perception, Shenzhen Graduate School, Peking University, China \\
   \textsuperscript{2}School of Intelligent Systems Engineering, Sun Yat-sen University, China \\
   \textsuperscript{3}Alibaba Group \\
   {\tt\small \{wenhaoli,hongliu,dingrunwei\}@pku.edu.cn, nkliuyifang@gmail.com, pichao.wang@alibaba-inc.com}
}

\maketitle

\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}   
Despite great progress in video-based 3D human pose estimation, it is still challenging to learn a discriminative single-pose representation from redundant sequences. To this end, we propose a novel Transformer-based architecture, called Lifting Transformer, for 3D human pose estimation to lift a sequence of 2D joint locations to a 3D pose. Specifically, a vanilla Transformer encoder (VTE) is adopted to model long-range dependencies of 2D pose sequences. To reduce redundancy of the sequence and aggregate information from local context, fully-connected layers in the feed-forward network of VTE are replaced with strided convolutions to progressively reduce the sequence length. The modified VTE is termed as strided Transformer encoder (STE) and it is built upon the outputs of VTE. STE not only significantly reduces the computation cost but also effectively aggregates information to a single-vector representation in a global and local fashion. Moreover, a full-to-single supervision scheme is employed at both the full sequence scale and single target frame scale, applying to the outputs of VTE and STE, respectively. This scheme imposes extra temporal smoothness constraints in conjunction with the single target frame supervision. The proposed architecture is evaluated on two challenging benchmark datasets, namely, Human3.6M and HumanEva-I, and achieves state-of-the-art results with much fewer parameters. 
\end{abstract}

\section{Introduction}
3D human pose estimation aims to estimate 3D joint locations of a human body from images or videos, which is a regression problem. 
This task has drawn tremendous attention in the past decades \cite{radwan2013monocular,li20143d,zhou2017towards,dabral2018learning}, with wide applications in computer animation~\cite{pullen2002motion}, action understanding~\cite{wang2018rgb,liu2017enhanced,liu2018recognizing}, and human-robot interactions~\cite{garcia2019human,gui2018teaching}. 
It is a challenging problem due to the depth ambiguity and self-occlusions. 

Many state-of-the-art approaches adopt a two-stage pipeline \cite{martinez2017simple,pavllo20193d,wang2020motion}, which first estimates 2D keypoints and then lifts them to the 3D space. 
Although the 2D-3D lifting methods benefit from the reliable performance of 2D pose detectors, it is still an ill-posed problem due to the depth ambiguity, where multiple 3D interpretations can map to the same 2D keypoints from monocular images.
To resolve this ambiguity, some methods \cite{lee2018propagating,rayat2018exploiting,cai2019exploiting} exploit temporal information by leveraging past and future data in the sequence to predict the 3D pose of the target frame. 
For instance, Cai \emph{et al.} \cite{cai2019exploiting} presented a local-to-global graph convolutional network to exploit spatial-temporal relations to estimate 3D poses from a sequence of skeletons. 
However, these approaches cannot explicitly and effectively model long-term dependencies result in small temporal receptive fields. 

\begin{figure}[t]
	\centering
	\includegraphics[width=1.00 \linewidth]{figure/moti.pdf}
	\caption
	{
      Our strided Transformer encoder (STE) takes the outputs of vanilla Transformer encoder (VTE) as input (yellow) and generates a 3D pose for the target frame as output (top). 
      The self-attention mechanism (blue) concentrates on global context and strided convolution (green) aggregates information from local context. 
	}
	\label{fig:moti}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98 \linewidth]{figure/pipline.pdf}
	\caption
	{
      Overview of our proposed Lifting Transformer for predicting the 3D joint locations of the target frame from the estimated 2D pose sequences. 
      The network first models long-range information via a vanilla Transformer encoder (VTE), 
      and then aggregates the information to one target pose representation from the proposed strided Transformer encoder (STE). 
      The model is trained end-to-end at both the full sequence scale and single target frame scale. 
	}
	\label{fig:pipline}
\end{figure*}

Vanilla Transformer \cite{Attention} is developed for exploiting long-range dependencies and achieves tremendous success in natural language processing~\cite{tay2020efficient,zihang2020funnel-transformer} and computer vision~\cite{han2020survey,he2021transreid,li2021trear,li2021transformer,han2020exploiting}. 
It consists of self-attention module and point-wise feed-forward network (FFN). 
The self-attention module computes pairwise dot-product between all input elements to capture global-context information and the FFN acts as pattern detectors over the input across all layers~\cite{geva2020transformer}. 
Such design is a good choice for video-based 3D human pose estimation to capture long-range dependencies. 
However, the full-length representation in the vanilla Transformer encoder (VTE) \cite{Attention} actually contains significant redundancy for video-based pose estimation, as nearby poses are quite similar. 
Based on the aforementioned observations, we propose to gradually merge nearby poses to reduce the sequence length till to one target pose representation. 
An alternative is to perform the pooling operation after the FFN~\cite{zihang2020funnel-transformer}. 
However, lots of valuable information may be lost if using pooling operation and the local information can not be well exploited. 
Inspired by previous methods \cite{pavllo20193d,liu2020attention} that take temporal convolutions to handle sequences with different input lengths, we propose to replace fully-connected layers in FFN with strided convolutions to progressively reduce the sequence length. 
The modified Transformer is dubbed strided Transformer encoder (STE), and it trades off the computation in FFN for constructing a deeper model and aggregates information in a global and local fashion to boost the model capacity, as shown in Figure \ref{fig:moti}. 
In addition, based on the outputs of VTE and STE, a novel full-to-single supervision scheme is proposed at both the full and single scales. 
More precisely, the full sequence scale can enforce temporal smoothness and single target frame scale helps learn a specific representation for the target frame. 

The proposed architecture is called Lifting Transformer, as shown in Figure~\ref{fig:pipline}. 
Extensive experiments are conducted on two standard 3D human pose estimation datasets, i.e., Human3.6M \cite{ionescu2013human3} and HumanEva-I \cite{sigal2010humaneva}. 
Experimental results show that our model achieves state-of-the-art performance. 

We summarize our contributions as follows:
\begin{itemize}
   \item We propose the Lifting Transformer, a Transformer-based architecture for 3D human pose estimation, which is simple and efficient to lift 2D joint locations to 3D poses. 
   \item To reduce the sequence redundancy and computation cost, STE is introduced to progressively reduce the temporal dimensionality and aggregate information to a single-vector representation of pose sequences in a global and local fashion.
   \item A full-to-single supervision scheme is designed to impose extra temporal smoothness constraints during training at the full sequence scale and further refine the estimation at the single target frame scale. 

   \item State-of-the-art results are achieved with fewer parameters on two commonly used benchmark datasets, making it a strong baseline for Transformer-based 3D pose estimation. 
\end{itemize}

\section{Related Work}
\textbf{One-stage pose estimation. }
At the early stage of applying deep neural networks on 3D pose estimation task, many methods \cite{pavlakos2017coarse,sun2018integral,zhao2019semantic,liu2019feature} learned the direct mapping from RGB images to 3D poses, termed as one-stage pose estimation.
However, these methods require sophisticating architectures, which are impractical in realistic applications.

\textbf{Two-stage pose estimation. }
Two-stage methods formulate the problem of 3D human pose estimation as 2D keypoint detection followed by 2D-3D lifting estimation \cite{martinez2017simple,fang2018learning,wandt2019repnet}. 
For example, Martinez \emph{et al.} \cite{martinez2017simple} lifted 2D joint locations to 3D space via a fully-connected residual network. 
Fang \emph{et al.} \cite{fang2018learning} proposed a pose grammar model to encode the human body configuration of human poses from 2D space to 3D space. 
We follow this two-stage pipeline because pre-trained 2D pose estimators \cite{chen2018cascaded} are mature enough to be deployed elsewhere. 

\textbf{Video pose estimation. }
Since past and future frames are beneficial for 3D human pose estimation when the pose of a person is ambiguous or the body is partially occluded in one frame, many approaches tried to exploit temporal information \cite{rayat2018exploiting,pavllo20193d,cai2019exploiting,wang2020motion}. 
To predict temporally consistent 3D poses, Hossain \emph{et al.} \cite{rayat2018exploiting} designed a sequence-to-sequence network with LSTM. 
Pavllo \emph{et al.} \cite{pavllo20193d} introduced a fully convolutional model based on dilated temporal convolution. 
Cai \emph{et al.} \cite{cai2019exploiting} directly chose the 3D pose of the target frame from outputs of the proposed graph-based method and then fed it to a refinement model. 
To produce smoother 3D sequences, Wang \emph{et al.} \cite{wang2020motion} designed an U-shaped graph convolutional network and involved motion modeling into learning. 
However, this architecture is limit to embed fixed-length sequences. 
Different from most existing works that employed LSTM-based, graph-based, or fully convolutional architectures to exploit temporal information, we propose to leverage a Transformer-based architecture to capture long-range dependencies from input 2D pose sequences. 
Furthermore, compared to previous methods \cite{cai2019exploiting,wang2020motion} that either utilizing a refinement model or using a motion loss, we design a full-to-single supervision scheme to produce predictions at both the full sequence scale and single target frame scale rather than using a single component with a single output. 

\textbf{Transformer network. }
Transformer architecture was first proposed by \cite{Attention} and commonly used in various language tasks. 
Recently, Transformer has shown promising performance in computer vision task, such as object detection \cite{carion2020end,zhu2020deformable} and image classification \cite{dosovitskiy2020image,yuan2021tokens}. 
Unlike DETR \cite{carion2020end} and ViT \cite{dosovitskiy2020image} that directly applied Transformer to images, we use Transformer to map 2D keypoints to 3D poses. 
Additionally, local context is incorporated into the standard Transformer to deal with the redundancy of sequences for the video-based 3D pose estimation task. 

\section{Lifting Transformer}
Our proposed Lifting Transformer is illustrated in Figure \ref{fig:pipline}. 
Given a sequence of the estimated 2D poses
 from videos, we aim to reconstruct 3D joint locations  for a target frame, where  denotes the 2D joint locations at frame ,  is the number of video frames, and  is the number of joints. 
The network contains a vanilla Transformer encoder (VTE) followed by a strided Transformer encoder (STE) and is trained in a full-to-single prediction scheme at both the full sequence scale and single target frame scale. 
Specifically, VTE is first used to model long-range information and is supervised by the full sequence scale to enforce temporal smoothness. 
Then, the information is aggregated in a global and local fashion to one target pose representation from the proposed STE.  

\subsection{Transformer for 3D Pose Estimation}
Motivated by the substantial performance gains achieved by the Transformer architecture in NLP, we propose to employ the Transformer encoder to solve the 3D human pose estimation. 
Each layer of the encoder has two sub-modules in \cite{Attention}: a multi-head self-attention and a position-wise feed-forward network (FFN). 

\textbf{Multi-head attention.}
The core mechanism of Transformer is the multi-head self-attention. 
Specifically, suppose there are a set of queries () and keys () of dimension , and values () of dimension , then a multi-head attention \cite{Attention} can be computed as:


where  and , and  are parameter matrices. 
The hyperparameters  is the number of multi-attention heads,  is the dimension of model, and  in our implementation. 

\textbf{Feed-forward network. }
In the existing FFN (formulation (\ref{equ:ffn})) from VTE, it always maintains a full-length sequence of hidden representations across all layers. 
It contains significant redundancy for video-based pose estimation, as nearby poses are quite similar. 
However, to reconstruct more accurate 3D body joints of the target frame, crucial information should be extracted from the entire pose sequences. 
Therefore, it requires selectively aggregating the useful information. 

To tackle this issue, inspired from the previous works \cite{pavllo20193d,liu2020attention} that employ temporal convolutions to handle varying length sequences, we make modifications to the generic FFN. 
Given the input feature vector  with  sequences and  channels to generate an output of  features, the operation performed by FFN can be computed as: 


If 1D convolution is considered with kernel size  and strided factor , a convolution feed-forward network can be computed as:


\textbf{Strided Transformer Encoder. }
In this way, fully-connected layers in FFN of VTE are replaced with strided convolutions. 
The modified VTE is termed as strided Transformer encoder (STE), which consists of a multi-head self-attention and a convolution feed-forward network, as presented in Figure \ref{fig:network} (right). 
STE is a global and local architecture,
where self-attention mechanism models global context and strided convolution helps capture local context. 
It gradually reduces the temporal dimensionality from layer to layers and merges the nearby poses to a short sequence length representation. 
More importantly, the redundancy of all frames is reduced and hence boosts the model capacity.  

\begin{figure}[t]
	\centering
	\includegraphics[width=0.98 \linewidth]{figure/network.pdf}
	\caption
	{
      The network architecture of our proposed Lifting Transformer, where the left is VTE and the right is STE with strided convolutions that reconstructs the target 3D body joints by progressively reducing the sequence length. 
      Here,  and  denotes the layers of the two modules, respectively. 
      The hyperparameters , ,  and  are the kernel size, the strided factor, the dimensions, and the number of hidden units. 
      We slice the residuals to match the temporal dimensions of subsequent tensors. 
	}
	\label{fig:network}
\end{figure}

\subsection{Transformer-based Full-to-single Prediction}
The iterative refinement scheme aimed at producing predictions in multiple processing stages is effective for 3D pose estimation \cite{pavlakos2017coarse,cai2019exploiting}. 
Inspired by the success of such iterative processing, we also consider a refinement scheme. 
Furthermore, we note that directly supervised the model at the single target frame scale always ignores temporal smoothness between video frames, while only supervised at a full sequence scale cannot explicitly learn a specific representation for the target frame. 

To incorporate both scale constraints into the framework, a full-to-single scheme is proposed, which further refines the intermediate predictions to produce more accurate estimations rather than using a single component with a single output. 
The first step is to supervise with full sequence scale by imposing extra temporal smoothness constraints during training from the outputs of VTE. 
A sequence loss  is adopted to improve upon single frame predictions for temporal consistency over a sequence. 
This loss ensures that the estimated 3D poses from VTE coincide with the ground-truth 3D joint sequences: 

where  and  represents the estimated 3D poses and ground truth 3D joint locations of joint  at frame , respectively. 

In the second step, the supervision is upon the output of STE, which is a progressive reduction architecture to reduce the temporal dimensionality from layer to layer. 
The output is a prediction of the 3D poses for all frames in the input sequences using both past and future data. 
A single-frame loss  is used to minimize the distance between the estimated 3D poses  from STE and the target ground-truth 3D joint annotations : 


In our implementation, the model is supervised at both the full sequence scale and single target frame scale. 
We train the entire network in an end-to-end manner with the combined loss:


where  and  are weighting factors. 

\begin{table*}[t]
	\centering
\resizebox{\textwidth}{!}{
		\begin{tabular}{@{}l|ccccccccccccccc|c@{}}
         \toprule[1pt]
			\textbf{Protocol \#1} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
         \midrule[0.5pt]
			
         Martinez \emph{et al.} (ICCV'17)\cite{martinez2017simple} &51.8 &56.2 &58.1 &59.0 &69.5 &78.4 &55.2 &58.1 &74.0 &94.6 &62.3 &59.1 &65.1 &49.5 &  52.4 &62.9\\

         Fang \emph{et al.} (AAAI'18) \cite{fang2018learning} & 50.1& 54.3& 57.0& 57.1& 66.6& 73.3& 53.4& 55.7& 72.8& 88.6& 60.3& 57.7& 62.7& 47.5& 50.6& 60.4 \\

         Zhao \emph{et al.} (CVPR'19) \cite{zhao2019semantic}&47.3 &60.7 &51.4 &60.5 &61.1 &{49.9} &47.3 &68.1 &86.2 &55.0 &67.8 &61.0 &42.1 &60.6 &45.3 &57.6 \\



         Lee \emph{et al.} (ECCV'18) \cite{lee2018propagating} &{40.2} &49.2 &47.8 &52.6 &50.1 &75.0 &50.2 &43.0 &55.8 &73.9 &54.1 &55.6 &58.2 &43.3 &43.3 &52.8 \\



			Cai \emph{et al.} (ICCV'19) \cite{cai2019exploiting} &44.6 &47.4 &45.6 &48.8 &50.8 &59.0 &47.2 &43.9&57.9 &{61.9} &49.7 &46.6 &51.3 &37.1 &39.4 &48.8 \\
			
         Pavllo \emph{et al.} (CVPR'19) \cite{pavllo20193d} & 45.2 & 46.7 & 43.3 & 45.6 & 48.1 & 55.1 & 44.6 & 44.3 & 57.3 & 65.8 & 47.1 & 44.0 & 49.0 & 32.8 & 33.9 & 46.8 \\

         Xu \emph{et al.} (CVPR'20) \cite{xu2020deep}& {37.4}& {43.5}& 42.7& {42.7}& {46.6}& 59.7& {41.3}& 45.1& {52.7}& {60.2}& 45.8& {43.1}& 47.7& 33.7& 37.1& 45.6 \\
      
         Liu \emph{et al.} (CVPR'20) \cite{liu2020attention} &41.8 &44.8 &{41.1} &44.9 &47.4 &54.1 &{43.4} &{42.2} &56.2 &63.6 &{45.3} &43.5 &{45.3} &{31.3} &{32.2} &{45.1} \\

Zeng~et al. (ECCV'20) \cite{zeng2020srnet}& 46.6& 47.1& 43.9& 41.6& 45.8& 49.6& 46.5& 40.0& 53.4& 61.1& 46.1& 42.6& 43.1& 31.5& 32.6& 44.8 \\

Wang~et al. (ECCV'20) \cite{wang2020motion} &40.2 &42.5 &42.6 &41.1 &46.7 &56.7 &41.4 &42.3 &56.2 &60.4 &46.3 &42.2 &46.2 &31.7 &31.0 &44.5 \\
         
			\midrule[0.5pt]
         Ours (T=243 CPN) &41.2 &{44.0} &{40.2} &{42.9} &{45.6} &{52.0} &43.1 &{40.8} &{56.2} &62.4 &{45.6} &{43.5} &{44.5} &{30.5} &31.6 &\textbf{44.3} \\
         
         \toprule[1pt]
         \textbf{Protocol \#2} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
			\midrule[0.5pt]

         Martinez \emph{et al.} (ICCV'17) \cite{martinez2017simple} &39.5 &43.2 &46.4 &47.0 &51.0 &56.0 &41.4 &40.6 &56.5 &69.4 &49.2 &45.0 &49.5 &38.0 &43.1 &47.7\\

         Fang \emph{et al.} (AAAI'18) \cite{fang2018learning} &38.2 &41.7 &43.7 &44.9 &48.5 &55.3 &40.2 &38.2 &54.5 &64.4 &47.2 &44.3 &47.3 &36.7 &41.7 &45.7 \\

         Pavlakos \emph{et al.} (CVPR'18) \cite{pavlakos2018ordinal}&34.7 &39.8 &41.8 &38.6 &42.5 &47.5 &38.0 &36.6 &50.7 &56.8 &42.6 &39.6 &43.9 &32.1 &36.5 &41.8 \\

			Liu \emph{et al.} (ECCV'20) \cite{liu2020comprehensive}&35.9 &40.0 &38.0 &41.5 &42.5 &51.4 &37.8 &36.0 &48.6 &56.6 &41.8 &38.3 &42.7 &31.7 &36.2 &41.2 \\

         Cai \emph{et al.} (ICCV'19) \cite{cai2019exploiting}&35.7 &37.8 &36.9 &40.7 &39.6 &45.2 &37.4 &34.5 &46.9 &{50.1} &40.5 &36.1 &41.0 &29.6 &33.2 &39.0 \\

         Pavllo \emph{et al.} (CVPR'19) \cite{pavllo20193d} &34.1 &36.1 &{34.4} &37.2 &36.4 &{42.2} &{34.4} &33.6 &45.0 &52.5 &37.4 &{33.8} &{37.8} &{25.6} &{27.3} &36.5\\

         Xu \emph{et al.} (CVPR'20) \cite{xu2020deep}&31.0 &34.8 &34.7 &34.4 &36.2 &43.9 &31.6 &33.5 &42.3 &49.0 &37.1 &33.0 &39.1 &26.9 &31.9 &36.2 \\

			\midrule[0.5pt]
         Ours (T=243 CPN) &33.8 &35.7 &33.6 &{35.8} &35.7 &41.4 &34.5 &32.1 &44.9 &51.4 &37.3 &34.1 &35.7 &24.3 &26.4 &\textbf{35.8} \\
         
         \toprule[1pt]
		\end{tabular}
	}
	\caption
	{
      Quantitative comparisons with the state-of-the-art methods on Human3.6M under protocol \#1 and protocol \#2,
      where  denotes the number of input frames used. 
      (CPN) - Cascaded Pyramid Network.
      Best in bold. 
}
	\label{table:h36m}
\end{table*}

\begin{table}[t]
   \centering  
   \small
   \setlength{\tabcolsep}{2.45mm} 
 
   \begin{tabular}{ccc}
     \toprule [1pt]
     Model &Parameters &MPJPE (mm)\\
      \midrule [0.5pt]  

      Pavllo \emph{et al.} \cite{pavllo20193d}  &8.56M &48.8 \\
      Pavllo \emph{et al.} \cite{pavllo20193d}  &12.75M &47.7 \\
      Pavllo \emph{et al.} \cite{pavllo20193d}  &16.95M &46.8 \\
      \midrule[0.5pt] 

      Liu \emph{et al.} \cite{liu2020attention}  &5.69M &48.5 \\
      Liu \emph{et al.} \cite{liu2020attention}  &8.46M &46.3 \\
      Liu \emph{et al.} \cite{liu2020attention}  &11.25M &45.1 \\
      \midrule[0.5pt] 

      Ours  &3.22M &\textbf{46.9} \\
      Ours  &3.28M &\textbf{45.7} \\
      Ours  &3.44M &\textbf{44.3} \\
      \toprule [1pt]
   \end{tabular}
     \caption
     {
         Quantitative comparisons with state-of-the-art methods in different receptive fields on Human3.6M.
         The MPJPE metric and number of parameters are reported. 
         Best in bold. 
     }
   \label{table:compare}
 \end{table}

\section{Experiments and Discussions}
\subsection{Datasets and Evaluation}
The proposed method is evaluated on two challenging benchmark datasets, i.e., Human3.6M \cite{ionescu2013human3} and HumanEva-I \cite{sigal2010humaneva}. 

The Human3.6M dataset is the largest publicly available dataset for 3D human pose estimation, which consists of 3.6 million images captured from 4 synchronized 50Hz cameras. 
There are 7 professional subjects performing 15 everyday activities such as “Waiting”, “Smoking”, and “Posing”.
Following the standard protocol in prior works \cite{chen2019weakly,tome2018rethinking}, 5 subjects (S1, S5, S6, S7, S8) are used for training and 2 subjects (S9 and S11) are used for evaluation. 
The frames from all views are trained by a single model for all actions.
HumanEva-I is a much smaller dataset with fewer subjects and actions compared to Human3.6M. 
Following \cite{pavllo20193d,lee2018propagating}, a single model is trained for all subjects (S1, S2, S3) and all actions (Walk, Jog, Box). 

Two common evaluation protocols are used in the experiments. 
The mean per joint position error (MPJPE) is the average Euclidean distance between the ground-truth and predicted positions of the joints, which is referred to as protocol \#1 in many works \cite{fang2018learning,kocabas2019self}. 
Procrustes analysis MPJPE (P-MPJPE) is adopted, where the estimated 3D pose is aligned to the ground truth in translation, rotation, and scale, which is referred to as protocol \#2 \cite{martinez2017simple,rayat2018exploiting}. 

\subsection{Implementation Details}
In our experiments, the input layer takes the concatenated  coordinates of the  joints for each frame and extracts sequence features using a linear layer. 
VTE \cite{Attention} is adopted as the basic architecture. 
We choose encoder layers , multi-attention heads , dimensions , and hidden units  for both VTE and STE. 
The kernel size and strided factor  are set to 1 in all STE layers.
The strided factor  is set to  for the receptive field of 27 frames,  for 81, and  for 243. 
The learnable position embeddings are used for the first layer of VTE and every layer of STE due to the different sequence lengths. 
Finally, we reproject the outputs via a linear layer. 
Note that we only use the outputs of STE as the final predictions for the full-to-single prediction scheme. 

\begin{figure} \centering
   {
   \includegraphics[width=0.484\columnwidth]{figure/frames.pdf} }
   {
   \includegraphics[width=0.484 \columnwidth]{figure/2D_detections.pdf}
   }
   \caption{
      Left: ablation studies on different sequence lengths of our method on Human3.6M with the MPJPE metric. 
      Right: the impact of 2D detections on Human3.6M. 
      Here,  represents the Gaussian noise with mean zero and  is the standard deviation. 
      (CPN) - Cascaded Pyramid Network; (SH) Stack Hourglass; (GT) - 2D ground truth. 
   }
	\label{fig:frames and 2D}
\end{figure}

\begin{table*}[t]
	\centering
\resizebox{\textwidth}{!}{
		\begin{tabular}{@{}l|ccccccccccccccc|c@{}}
         \toprule[1pt]
			\textbf{Protocol \#1} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
         \midrule[0.5pt]

         Martinez \emph{et al.} (ICCV'17)\cite{martinez2017simple} &37.7 &44.4 &40.3 &42.1 &48.2 &54.9 &44.4 &42.1 &54.6 &58.0 &45.1 &46.4 &47.6 &36.4 &40.4 &45.5 \\

         Lee \emph{et al.} (ECCV'18) \cite{lee2018propagating} &{32.1} &{36.6} &34.3 &37.8 &44.5 &49.9 &40.9 &36.2 &44.1 &45.6 &35.3 &35.9 &{30.3} &37.6 &35.5 &38.4 \\



         Cai \emph{et al.} (ICCV'19) \cite{cai2019exploiting} &32.9 &38.7 &32.9 &37.0 &37.3 &44.8 &{38.7} &36.1 &41.0 &45.6 &36.8 &37.7 &37.7 &29.5 &31.6 &37.2 \\

         Liu \emph{et al.} (CVPR'20) \cite{liu2020attention} &34.5 &37.1 &33.6 &{34.2} &{32.9} &{37.1} &39.6 &{35.8} &{40.7} &{41.4} &{33.0} &{33.8} &33.0 &{26.6} &{26.9} &34.7 \\

Chen \emph{et al.} (TCSVT'21) \cite{chen2021anatomy}&- &- &- &- &- &- &- &- &- &- &- &- &- &- &- &32.3 \\



Zeng~et al. (ECCV'20) \cite{zeng2020srnet}&34.8 &32.1 &28.5 &30.7 &31.4 &36.9 &35.6 &30.5 &38.9 &40.5 &32.5 &31.0 &29.9 &22.5 &24.5 &32.0 \\

         \midrule[0.5pt]
         Ours (T=243 GT) &{26.9} &{30.6} &{27.2} &{27.4} &{29.7} &{33.6} &{31.9} &{26.3} &{37.8} &{38.6} &{29.0} &{29.2} &{27.5} &{19.8} &{20.3} &\textbf{29.1} \\
         \toprule[1pt]
		\end{tabular}
	}
	\caption
	{
      Quantitative comparisons of MPJPE in millimeter on Human3.6M under protocol \#1, using ground truth 2D joint locations as input. 
Best in bold. 
	}
	\label{table:h36m_gt}
\end{table*}

 \begin{table}
   \centering
   \resizebox{\linewidth}{!}
   {
     \begin{tabular}{@{}l|ccc|ccc|ccc|c@{}}
       \toprule[1pt]
       & \multicolumn{3}{c}{Walk} & \multicolumn{3}{c}{Jog} &
       \multicolumn{3}{c}{Box} \\
     & S1 & S2 & S3 & S1 & S2 & S3 & S1 & S2 & S3 & Avg. \\
     \midrule[0.5pt]

     Martinez \emph{et al.} \cite{martinez2017simple} &19.7 &17.4 &46.8 &26.9 &18.2 &18.6 &- &- &- &-\\

     Pavlakos \emph{et al.} \cite{pavlakos2017coarse} &22.3 &19.5 &\textbf{29.7} &28.9 &21.9 &23.8 &- &- &- &-\\

     Lee \emph{et al.} \cite{lee2018propagating} &18.6 &19.9 &\underline{30.5} &25.7 &16.8 &17.7 &42.8 &48.1 &53.4 &30.3 \\

     Pavllo \emph{et al.} \cite{pavllo20193d} &\textbf{13.9} &\underline{10.2} &46.6 &\underline{20.9} &\textbf{13.1} &\textbf{13.8} &\underline{23.8} &\underline{33.7} &\underline{32.0} &\underline{23.1} \\

     \midrule[0.5pt]
     Ours (T=27 MRCNN) &\underline{14.0} &\textbf{10.0} &32.8 &\textbf{19.5} &\underline{13.6} &\underline{14.2} &\textbf{22.4} &\textbf{21.6} &\textbf{22.5} &\textbf{18.9} \\
     \midrule[0.5pt]

     Ours (T=27 GT) &9.7 &7.6 &15.8 &12.3 &9.4 &11.2 &14.8 &12.9 &16.5 &12.2 \\
     \toprule[1pt]
     \end{tabular}
   }
   \caption
   {
      Quantitative results on HumanEva-I dataset under protocol \#2. 
      Best in bold, second-best underlined.
      (MRCNN) - Mask-RCNN; 
      (GT) - 2D ground truth. 
   }
   \label{table:humaneva_eval}
 \end{table} 

The 2D poses can be obtained by performing any classic 2D pose detections or directly using the 2D ground truth. 
Following \cite{pavllo20193d}, the cascaded pyramid network (CPN) \cite{chen2018cascaded} is used for Human3.6M and Mask R-CNN \cite{he2017mask} is adopted for HumanEva-I to obtain 2D poses for a fair comparison. 

In this work, all experiments are conducted on the PyTorch framework with one GeForce GTX 3090 GPU.
The network is trained using Adam optimizer with a mini-batch size of 256 for Human3.6M, 64 for HumanEva-I. 
An initial learning rate of 0.001 is used with a shrink factor of 0.95  applied after each epoch and 0.5 after every 5 epochs. 
Note that we only adopt horizontal flip augmentation during training and test stages and only compute MPJPE loss for training. 

\subsection{Comparison with the State-of-the-art}
We compare our method with the previous state-of-the-art approaches on Human3.6M dataset. 
As shown in Table \ref{table:h36m}, the performance of our 243-frame model with CPN input is presented. 
Our method outperforms the state-of-the-art methods on Human3.6 under all metrics (44.3 mm on protocol \#1 and 35.8 mm on protocol \#2). 

Table \ref{table:compare} compares the MPJPE metric and number of parameters with several state-of-the-art methods in different receptive fields () on Human3.6M. 
It can be seen that our method has the fewest parameters but achieves the best performance from overall receptive fields. 
This shows that our proposed Transformer-based network is more efficient than fully convolutional architectures at the same level of accuracy for video 3D pose estimation. 
Besides, our model is lightweight and parameters hardly increase with the increased receptive fields, which is practical for real-time applications. 
Figure \ref{fig:results} shows the visualized qualitative results from the 243-frame models of TCN \cite{pavllo20193d}, ATTN-TCN \cite{liu2020attention}, and our model. 

Additionally, we report the results when using ground truth 2D poses to explore the upper bound of our method. 
As illustrated in Table \ref{table:h36m_gt}, it can be seen that our method achieves the best result (29.1 mm in MPJPE) outperforming all other methods. 
Moreover, it has a larger performance gain than CPN input compared with state-of-the-art methods, which indicates that our method has a stronger capacity in capturing temporal dependencies from more accurate 2D representations. 

To evaluate the generalizability of our model to smaller datasets, experiments are conducted on HumanEva-I based on Mask R-CNN 2D detections and 2D ground truth. 
The results are given in Table \ref{table:humaneva_eval}, which demonstrates that our method achieves promising results in each action. 

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98 \linewidth]{figure/results.pdf}
	\caption
	{
      Qualitative comparisons with the previous state-of-the-art methods, 243-frame model of TCN \cite{pavllo20193d} and ATTN-TCN \cite{liu2020attention} on Human3.6M dataset. 
	}
	\label{fig:results}
\end{figure*}

\subsection{Ablation Studies}
\textbf{Input sequence length. }
The MPJPE results of our model with different sequence lengths on Human3.6M are shown in Figure \ref{fig:frames and 2D} (left). 
It can be seen that with more input frames used for predictions, our proposed method obtains larger gains when using ground truth 2D poses. 
This is expected since direct lifting 3D poses from disjointed 2D poses leads to temporally incoherent outputs \cite{dabral2018learning}. 
Particularly the best results are obtained with  (44.3 mm) by using CPN. 
However, the performance decreases when  (45.1 mm) and  (45.5 mm), which is the same phenomenon as \cite{liu2020attention}.  
The reason is that the 2D detector CPN is limited to provide useful information for our model to distinguish the discriminative representation from redundant long sequences. 
Next, we choose  on Human3.6M in the following ablation experiments as a compromise between the accuracy and the computational complexity. 

\textbf{2D detections. }
For the 2D-3D lifting task, the accuracy of the 2D detections directly influences the results of 3D pose estimation \cite{martinez2017simple}. 
To show the effectiveness of our method on different 2D pose detectors, we carry out experiments with the detections from Stack Hourglass (SH) \cite{newell2016stacked}, Detectron \cite{pavllo20193d}, and CPN \cite{chen2018cascaded}. 
Moreover, to test the tolerance of our method to different levels of noise, we also train our network by 2D ground truth (GT) with different levels of additive Gaussian noises. 
The results are shown in Figure \ref{fig:frames and 2D} (right). 
It can be observed that the MPJPE of 3D poses increases linearly with the two-norm errors of 2D poses.
Note that 2D pose detector methods achieve worse estimation performance than 2D ground truth with noises, which is due to the self-occlusions problem for 2D pose estimation. 

\begin{table}[t]
   \centering
   \small
   \setlength{\tabcolsep}{1.6mm} 
   \begin{tabular}{ccccccc}
     \toprule  [1pt]
      & & & &Parameters &FLOPs &MPJPE (mm)\\
      \midrule  [0.5pt]
      2 &- &512 &2048 &6.36M &171.08M  &47.9 \\
      3 &- &512 &2048 &9.51M &256.02M  &47.8 \\
      4 &- &512 &2048 &12.66M &340.95M &48.1 \\
      5 &- &512 &2048 &15.82M &425.89M &48.4 \\
      6 &- &512 &2048 &18.97M &510.82M &48.6 \\
      \midrule [0.5pt]  

      2 &- &256 &512 &1.08M &28.92M &47.8 \\
      3 &- &256 &512 &1.61M &43.07M &47.5 \\
      4 &- &256 &512 &2.13M &57.23M &47.9 \\
      5 &- &256 &512 &2.66M &71.38M &47.8 \\
      6 &- &256 &512 &3.19M &85.54M &47.9 \\
      \midrule [0.5pt]  

      - &3 &256 &512 &1.61M &17.68M &48.3 \\
      2 &3 &256 &512 &2.69M &45.99M &47.4 \\
      3 &3 &256 &512 &3.22M &60.15M &\textbf{46.9} \\
      2 &3 &512 &2048 &15.89M &266.62M &47.7 \\
      3 &3 &512 &2048 &19.04M &351.55M &48.1 \\
      \toprule  [1.0pt] 

   \end{tabular}
     \caption
     {
      Ablation study on the hyperparameters of our model on Human3.6M under protocol \#1. 
       and  is the number of VTE and STE layers, respectively. 
       and  are the dimensions and the number of hidden units. 
     }
   \label{table:hyperparameters}
 \end{table}

\textbf{Model hyperparameters. }
As shown in Table \ref{table:hyperparameters}, we first analyze the effect of the number of VTE layers. 
Empirically, it can be found that the performance cannot be improved when naively stacking multiple standard Transformer encoder layers. 
However, our model that introduces STE is more accurate at the same level of the number of Transformer encoder layers and model parameters. 
For example, our method ( and ) has better performance and fewer FLOPs than  at the same  and  hidden units (46.9 mm vs. 47.9 mm, 60.15M vs. 85.54M). 
Meanwhile, our STE (, 17.68M) also have fewer FLOPs than standard Transformer encoder (, 43.07M) with similar parameters, which achieves  less computation. 
It verifies the effectiveness of our proposed STE that merges the nearby poses to reduce the redundancy of the sequence in a simple form. 
Then, we investigate the hyperparameters 
on the performance and parameters of both modules. 
It can be observed that using 3 encoder layers, 256 dimensions, and 512 hidden units achieves the best performance. 

\begin{table}[t]
   \centering  
   \small
   \setlength{\tabcolsep}{4.36mm} 
   \begin{tabular}{cccc}
      \toprule  [1pt]
      Layers &Strided factor &MPJPE (mm) &\\
      \midrule  [0.5pt]  
      3 & &\textbf{46.9} &- \\
      3 & &47.3 &0.4 \\
      3 & &47.4 &0.5 \\
      2 & &47.4 &0.5 \\
      2 & &47.5 &0.6 \\
      1 & &47.6 &0.7 \\
      \toprule [1pt]
   \end{tabular}
   \caption
   {
    Ablation study on the strided factor of STE with the receptive field . 
    The evaluation is performed on Human3.6M under protocol \#1.
   }
 \label{table:Strided}
\end{table}

\begin{table}[t]
   \centering
   \small
   \setlength{\tabcolsep}{3.75mm} 
 
   \begin{tabular}{lcc}
     \toprule [1.0pt] 
       Method& MPJPE (mm) &  \\
       \midrule [0.5pt] 
       Ours, proposed &\textbf{46.9} &- \\
       Ours, intermediate predictions &48.8 &1.9 \\
       Ours, Pooling Transformer &47.3 &0.4 \\
      \midrule [0.5pt]

       w/o sequence loss &48.9 &2.0 \\
       w/o VTE &48.3 &1.4 \\
       w/o STE &47.5 &0.6 \\
       \toprule [1.0pt] 
   \end{tabular}
     \caption
     {
         Ablation study on each component of our network architecture on Human3.6M under protocol \#1. 
     }
   \label{table:ablation_method}
 \end{table}

 \begin{figure*} \centering
   { 
   \includegraphics[width=0.95 \columnwidth]{figure/243_3_no_1.png}
   }
   {
   \includegraphics[width=0.95 \columnwidth]{figure/243_3_no_11.png}
   }
   \caption{
      Multi-head attention maps () from VTE (left) and STE (right) of our 243-frame model. 
      It illustrates the self-attention mechanism systematically assigns a weight distribution to frames, all of which might contribute to the inference. 
      The brighter color indicates stronger attention across frames. 
   }
   \label{fig:attention}
\end{figure*}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.95 \linewidth]{figure/wild.pdf}
	\caption
	{
      Qualitative results on challenging wild videos. 
      The number is the frame index of input videos.
	}
	\label{fig:wild}
\end{figure}

\textbf{Strided factor. }
We also explore the design choice of strided factor for STE when fixing VTE with 3 encoder layers and 8 multi-attention heads, 256 dimensions, and 512 hidden units for both VTE and STE. 
The experimental results have been depicted in Table \ref{table:Strided}, it shows that using a strided factor  has the best performance. 
It demonstrates the benefit of leveraging a progressive reduction architecture to gradually reduce the temporal dimensionality from layer to layer with a small strided factor. 

\textbf{Model components. }
An ablation study is performed to assess the effectiveness of different components of our method. 
As shown in Table \ref{table:ablation_method}, we select the center frames of the intermediate predictions from VTE as the final results, the MPJPE decreases by 1.9 mm (from 46.9 mm to 48.8 mm). 
Additionally, the sequence loss  leads to a significant performance gain (2.0 mm). 
The empirical results indicate that the intermediate supervision allows for a stronger capacity in producing smooth 3D sequences. 
Following \cite{zihang2020funnel-transformer}, we perform pooling operation after FFN of VTE and then replace STE of our proposed model with it, the new architecture is termed as Pooing Transformer. 
The error increases by 0.4 mm, which highlights that our STE can preserve more valuable information. 
Removing VTE (only trained with single-frame loss) leads to 1.4 mm increase in MPJPE error. 
Meanwhile, removing STE (only trained with sequence loss), the performance decreases. 
These results validate the effectiveness of our proposed full-to-single mechanism by using further processing to produce more accurate predictions. 

\subsection{Qualitative Results}
\textbf{Attention visualization. }
Our method is easily interpretable through visualizing the attention score across frames to explain what temporal dependencies the target frame relies on. 
Visualization results of the multi-head attention maps of the first attention layers from VTE and STE (243-frame model) are shown in Figure \ref{fig:attention}. 
The left map shows it selectively identifies important sequences close to the input frames \cite{Wu2020LiteTransformer,jiang2020convbert} and the right map mainly pays strong attention to the target frame across all the sequences. 
This is expected since the proposed full-to-single strategy enables the VTE and STE modules to learn different representations, where VTE models long-range dependencies and enforces temporal consistency across frames, and STE learns a specific representation to reach an optimal inference for the target frame. 
Note that few attention head maps are sparse due to the different temporal patterns or semantics. 

\textbf{3D reconstruction visualization. }
We further evaluate our method on wild videos from YouTube, as shown in Figure \ref{fig:wild}. 
Despite the challenging samples with huge movements and hard actions, our model can produce realistic and structurally plausible 3D predictions from complicated pose articulation. 
This demonstrates that our method is robust to partial occlusions and tolerant to depth ambiguity. 
More results can be seen in the appendix. 

\section{Conclusion} 
We present Lifting Transformer network with strided Transformer encoder (STE) and full-to-single supervision scheme for lifting a sequence of 2D joint locations to a 3D pose. 
Compared with standard Transformer encoder, the proposed STE can aggregate long-range information to a single-vector pose in a global and local fashion. Meanwhile, the computation cost can be reduced by a large margin. 
The proposed full-to-single supervision scheme enforces temporal smoothness and further refines the estimation. 
Comprehensive experiments on two benchmark datasets demonstrate that our method achieves superior performance compared to state-of-the-art methods. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref.bib}
}

\section{Appendix}
\begin{figure*}[hb]
	\centering
	\includegraphics[width=0.66 \linewidth]{figure/dataset.pdf}
	\caption
	{
    Visual results of our proposed method on Human3.6M dataset (first 3 rows) and HumanEva-I dataset (last 2 rows). 
	}
	\label{fig:dataset}
\end{figure*}

\begin{figure*}[hb]
	\centering
	\includegraphics[width=0.65 \linewidth]{figure/wild_compare.pdf}
	\caption
	{
    Qualitative comparisons on challenging in-the-wild videos with previous state-of-the-art methods, ATTN-TCN \cite{liu2020attention}, TCN \cite{pavllo20193d}, and GCN \cite{cai2019exploiting}. 
    The last row shows the failure case, where the 2D detector has failed badly. 
	}
	\label{fig:wild_compare}
\end{figure*}


\end{document}
