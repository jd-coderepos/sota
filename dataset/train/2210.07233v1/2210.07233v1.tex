\documentclass{bmvc2k}

\title{Shape Preserving Facial Landmarks with Graph Attention Networks}

\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{amsfonts}

\addauthor{Andr\'es Prados-Torreblanca}{a.prados@upm.es}{1,2}
\addauthor{Jos\'e M. Buenaposada}{josemiguel.buenaposada@urjc.es}{1}
\addauthor{Luis Baumela}{lbaumela@fi.upm.es}{2}

\addinstitution{
 ETSII\\
 Universidad Rey Juan Carlos\\
 M\'ostoles, Spain
}
\addinstitution{
 Departamento de Inteligencia Artificial.\\
 Universidad Polit\'ecnica de Madrid,\\
 Boadilla del Monte, Spain
}

\runninghead{Prados, Buenaposada, Baumela}{Shape preserving facial landmarks}

\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

\def\mat#1{\mathchoice{\mbox{\boldmath }}
{\mbox{\boldmath}}
{\mbox{\boldmath}}
{\mbox{\boldmath}}}

\def\vect#1{\mathchoice{\mbox{\boldmath }}
{\mbox{\boldmath  }}
{\mbox{\boldmath  }}
{\mbox{\boldmath  }}}

\def\textphd#1{\texttt{\textsc{#1}}}


\def\inv#1{{#1}^{-1}}

\def\dt{{\delta t}}
\def\dtau{{\delta \tau}}

\def\dif{{\mbox{d}}}

\def\v0{{\vect 0}}
\def\vone{{\vect 1}}

\def\va{{\vect a}}
\def\vb{{\vect b}}
\def\vc{{\vect c}}
\def\vd{{\vect d}}
\def\ve{{\vect e}}
\def\vf{{\vect f}}
\def\vg{{\vect g}}
\def\vh{{\vect h}}
\def\vi{{\vect i}}
\def\vj{{\vect j}}
\def\vl{{\vect l}}
\def\vm{{\vect m}}
\def\vn{{\vect n}}
\def\vo{{\vect o}}
\def\vp{{\vect p}}
\def\vq{{\vect q}}
\def\vr{{\vect r}}
\def\vs{{\vect s}}
\def\vt{{\vect t}}
\def\vu{{\vect u}}
\def\vv{{\vect v}}
\def\vw{{\vect w}}
\def\vx{{\vect x}}
\def\vy{{\vect y}}
\def\vz{{\vect z}}


\def\vA{{\vect A}}
\def\vB{{\vect B}}
\def\vE{{\vect E}}
\def\vC{{\vect C}}
\def\vD{{\vect D}}
\def\vF{{\vect F}}
\def\vH{{\vect H}}
\def\vI{{\vect I}}
\def\vJ{{\vect J}}
\def\vL{{\vect L}}
\def\vS{{\vect S}}
\def\vO{{\vect O}}
\def\vP{{\vect P}}
\def\vM{{\vect M}}
\def\vN{{\vect N}}
\def\vT{{\vect T}}
\def\vW{{\vect W}} 
\def\vX{{\vect X}}
\def\vY{{\vect Y}}
\def\vZ{{\vect Z}}

\def\vepsilon{{\vect\epsilon}}

\def\dvt{{\delta\vt}}

\def\vmu{{\vect\mu}}
\def\dvmu{{\delta\vmu}}
\def\dvc{{\delta\vc}}
\def\vSigma{{\vec\Sigma}}
\def\vomega{{\boldsymbol\omega}}
\def\vOmega{{\boldsymbol\Omega}}
\def\vtheta{{\vect\theta}}
\def\vphi{{\vect\phi}}
\def\vpsi{{\vect\psi}}
\def\vPi{{\vect\pi}}
\def\vpsi{{\vect\psi}}
\def\vPsi{{\vect\Psi}}
\def\vepsilon{{\vect\epsilon}}

\def\v0{{\vect 0}}
\def\vzero{{\vect 0}}
\def\vone{{\vect 1}}

\def\bvb{{\bar{\vb}}}
\def\bvu{{\bar{\vu}}}
\def\bvx{{\bar{\vx}}}
\def\bvI{{\bar{\vI}}}

\def\mDelta{{\mat\mDelta}}
\def\mSigma{{\mat\Sigma}}
\def\mGamma{{\mat\Gamma}}
\def\mLambda{{\mat\Lambda}}
\def\mOmega{{\mat\Omega}}
\def\mTau{{\mat\tau}}


\def\mZero{{\mat 0}}

\def\mm{{\mat m}}
\def\ms{{\mat s}}
\def\mA{{\mat A}}
\def\mB{{\mat B}}
\def\mC{{\mat C}}
\def\mD{{\mat D}}
\def\mE{{\mat E}}
\def\mF{{\mat F}}
\def\mh{{\mat h}}
\def\mH{{\mat H}}
\def\mI{{\mat I}}
\def\mJ{{\mat J}}
\def\mK{{\mat K}}
\def\mL{{\mat L}}
\def\mM{{\mat M}}
\def\mN{{\mat N}}
\def\mP{{\mat P}}
\def\mQ{{\mat Q}}
\def\mR{{\mat R}}
\def\mS{{\mat S}}
\def\mT{{\mat T}}
\def\mU{{\mat U}}
\def\mV{{\mat V}}
\def\mW{{\mat W}}
\def\mx{{\mat x}}
\def\mX{{\mat X}}
\def\mY{{\mat Y}}
\def\mZ{{\mat Z}}

\def\dmH{\delta{\mat H}}
\def\dmR{\delta{\mat R}}
\def\ddmR{\dot{\mR}}

\def\mZero{{\mat 0}}
\def\m1{{\mat 1}}


\def\cA{\mathcal{A}}
\def\cB{\mathcal{B}}
\def\cC{\mathcal{C}}
\def\cl{\ell}
\def\cD{\mathcal{D}}
\def\cF{\mathcal{F}}
\def\cE{\mathcal{E}}
\def\cF{\mathcal{F}}
\def\cG{\mathcal{G}}
\def\cH{\mathcal{H}}
\def\cI{\mathcal{I}}
\def\cJ{\mathcal{J}}
\def\cK{\mathcal{K}}
\def\cL{\mathcal{L}}
\def\cN{\mathcal{N}}
\def\cM{\mathcal{M}}
\def\cP{\mathcal{P}}
\def\cQ{\mathcal{Q}}
\def\cR{\mathcal{R}}
\def\cS{\mathcal{S}}
\def\cU{\mathcal{U}}
\def\cV{\mathcal{V}}
\def\cW{\mathcal{W}}
\def\cX{\mathcal{X}}
\def\cY{\mathcal{Y}}
\def\cZ{\mathcal{Z}}
\def\cT{\mathcal{T}}
\def\cUT{\mathcal{UT}}

\def\vcl{\vect \ell}
\def\vcL{\vect \cL}

\def\sfE{\mbox{\textsf{E}}}

\def\argmin{\mbox{arg}\min}

\def\Matrices{\mathbb{M}}
\def\Reales{\mathbb{R}}
\def\Pspace{\mathbb{P}}
\def\RigidBody{\mathbb{SE}}
\def\Rotations{\mathbb{SO}}

\def\hF{\hat{F}}
\def\hY{\hat{Y}}
\def\hvc{\hat{\vc}}
\def\hvf{\hat{\vf}}
\def\hvp{\hat{\vp}}
\def\hvr{\hat{\vr}}
\def\hvt{\hat{\vt}}
\def\hvu{\hat{\vu}}
\def\hvx{\hat{\vx}}
\def\hvy{\hat{\vy}}
\def\hvz{\hat{\vz}}
\def\hvX{\hat{\vX}}
\def\hvY{\hat{\vY}}
\def\hmR{\hat{\mR}}
\def\halpha{\hat{\alpha}}
\def\hbeta{\hat{\beta}}
\def\hgamma{\hat{\gamma}}
\def\hpsi{\hat{\psi}}
\def\hvpsi{\hat{\vpsi}}
\def\homega{\hat{\omega}}
\def\hvmu{\hat{\vmu}}

\def\tve{\mathbf{\tilde{e}}}
\def\tvx{\mathbf{\tilde{x}}}
\def\tvu{\mathbf{\tilde{u}}}
\def\tmH{\tilde{\mH}}

\newcommand{\todo}[1]{
  \addcontentsline{tdo}{todo}{\protect{#1}}
  \begin{tikzpicture}[remember picture, baseline=-0.75ex]
    \node [coordinate] (inText) {};
  \end{tikzpicture}
   
\marginpar{
    \begin{tikzpicture}[remember picture]
      \definecolor{orange}{rgb}{1,0.5,0}
      \draw node[draw=black, fill=orange, text width = 2.1cm] (inNote)
      {#1};
    \end{tikzpicture}
  } 
  \begin{tikzpicture}[remember picture, overlay]
  \draw[draw = orange, thick]
    ([yshift=-0.2cm] inText)
    -| ([xshift=-0.2cm] inNote.west)
    -| (inNote.west);
  \end{tikzpicture}

} 

 
\newcommand{\first}[1]{{\color{blue} \textbf{#1}}}
\newcommand{\second}[1]{{\color{green} #1}}
\newcommand{\third}[1]{{\color{red} #1}}

\renewcommand{\floatpagefraction}{.9}

\begin{document}

\maketitle

\begin{abstract}
Top-performing landmark estimation algorithms are based on exploiting the excellent ability of large convolutional neural networks (CNNs) to represent local appearance. However, it is well known that they can only learn weak spatial relationships. To address this problem, we propose a model based on the combination of a CNN with a cascade of Graph Attention Network regressors. To this end, we introduce an encoding that jointly represents the appearance and location of facial landmarks and an attention mechanism to weigh the information according to its reliability. This is combined with a multi-task approach to initialize the location of graph nodes and a coarse-to-fine landmark description scheme. Our experiments confirm that the proposed model learns a global representation of the structure of the face, achieving top performance in popular benchmarks on head pose and landmark estimation. The improvement provided by our model is most significant in situations involving large changes in the local appearance of landmarks. The code is publicly available at \href{https://github.com/andresprados/SPIGA}{\color{bmv@sectioncolor}{https://github.com/andresprados/SPIGA}}

\end{abstract}

\section{Introduction}
\label{sec:intro}

Landmarks (or keypoints) are a widely used representation to address high-level vision tasks such as image retrieval~\cite{MoskvyakWACV21}, facial expression recognition~\cite{Sun19}, face reenactment~\cite{Zhang20Freenet}, etc.
The performance of computer vision algorithms on the final task depends, to a great extent, on the accuracy and robustness of this intermediate representation. Thus, although many algorithms with excellent performance have recently emerged, research is still very intense in this area.

Top facial landmark estimation methods may be broadly grouped into coordinate and heatmap regression approaches. \textit{Coordinate regression approaches} directly estimate the landmark position by projecting the representation estimated by a CNN encoder onto a set of 2D coordinates~\cite{Feng18wing,Kowalski17,Feng20rwing,Trigeorgis16,LinTIP21}. They are the most efficient since they only require an encoder architecture to compute the facial representation. 
The \textit{heatmap regression approach} is based on appending multiple encoder-decoder modules to estimate a 2D data structure modeling the landmark position likelihood, the heatmap~\cite{Honari16,Wu18lab,Wang19Awing,Huang20propnet,Kumar20luvli,Huang21ADnet}. The landmark coordinates are typically estimated at the maximum of each heatmap. This architecture provides an increase in accuracy at the expense of a considerable boost in computational and memory requirements. A fundamental limitation of both approaches is their degradation when there is ambiguity or noise contaminating the local landmark appearance. This typically happens at the presence of occlusions, heavy make-up, blur and extreme illuminations or poses. This is because of the known fact that CNNs cannot learn simple spatial relationships~\cite{Santoro17} and, in the case of facial landmarks, are unable to learn a global representation of the face structure. However, a human face is a highly structured object with a prominent landmark configuration. Therefore, an effective way of representing the local appearance of each landmark and its geometric relationship to the other landmarks is needed.

This problem has been partially addressed in the literature with a local attention module combining landmarks with facial boundaries~\cite{Wu18lab,Huang20propnet,Huang21ADnet}. This is a solution that learns short-distance geometrical relationships. An alternative solution combines the advantages of a CNN description with traditional Ensemble of Regression Trees (ERT)~\cite{Valle18,Valle193dde}. Although this solution is able to learn long-distance geometrical dependencies, it is not fully satisfactory because of the limited learning capabilities of ERTs and the impossibility of end-to-end training.
Other approaches use a Graph Convolutional Network (GCN) to learn the facial geometrical structure~\cite{Li20sld,LinTIP21}. This is achieved by combining the landmark local description, extracted from the CNN representation, with geometrical information represented by the relative landmark locations. However, poor initialization and the lack of an advanced attention mechanism reduce the performance of these models.
More recent approaches use transformers~\cite{Li22casctransf, Xia22slpt} in a cascade shape regressor, obtaining very good results due to the built-in attention mechanisms.

In this paper, we present the SPIGA (\emph{Shape Preserving wIth GAts}) model for the estimation of human face landmarks. We follow the traditional regressor cascade approach~\cite{Cao14} and present an algorithm that combines a multi-stage heatmap backbone with a cascade of Graph Attention Network (GAT) regressors~\cite{Velickovic18gats}. The backbone provides a top-performing facial appearance representation. The cascaded GAT regressor is endowed with a positional encoding and attention mechanism that learn the geometrical relationship among landmarks. Another element of our proposal that improves the convergence of the GAT cascade is a coarse-to-fine feature extraction procedure and a good initialization. To do this, we train our backbone with a multi-task approach that also estimates the head pose, using its projection to establish the initial landmark locations. We evaluate the performance of our proposal in 300W, COFW-68, MERL-RAV and WFLW datasets. It achieves top performance on both head pose and face landmarks estimation. The improvement is most significant in situations involving large appearance changes, such as occlusions, heavy make-up, blur and extreme illuminations. 
We make the following contributions:
1) A GAT cascade with an attention mechanism to weigh the information provided by each landmark according to its reliability;
2) A positional encoding to jointly represent relative landmark locations and local appearance;
3) A multi-task approach to initialize the location of graph nodes;
4) A coarse-to-fine landmark description scheme.



\section{Shape Regressor Model}
\label{sec:our_method}

We propose a coarse-to-fine cascade of landmark regressors~\cite{Dollar10,Cao14} that iteratively refines the landmarks coordinates while preserving the face shape. Our approach involves three critical components: 1) the initialization, 2) the features used for regression, and 3) the regressors that estimate the face shape deformation at each step of the cascade. 

In our proposal, we use a multi-task CNN backbone to provide both, the initialization and the local appearance representation. We set the initial shape of the face, ,  by projecting  landmarks from a generic 3D rigid face mesh oriented using the head pose backbone prediction. At each cascade step , a GAT-based~\cite{Velickovic18gats} regressor computes a displacement vector, , to update the landmarks location, . After  steps, the final face shape is . We denote the 2D location of -th landmark at step  as . In Fig.~\ref{fig:cascade_regressor_full} we show the regressor with a two-step cascade configuration.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/bmvc22_cnn_gatregressor_wdiv.pdf}
    \caption{Regressor architecture with a two-step cascade.}
    \label{fig:cascade_regressor_full}
\end{figure}


\subsection{Initialization by Head Pose Estimation}
\label{sec:init_3dpose}

Our multi-task backbone, termed \emph{Multi Task Network} (\emph{MTN}), is a cascade of  encoder-decoder Hourglass (HG) 
modules.
Each HG module in MTN is composed of a shared encoder with two task branches: 
1) a 3D head pose estimation branch and 2) a landmark estimation decoder to the end of which we attach the next HG module. Defining and balancing the depth of the three components is a critical factor to boost the head pose estimation accuracy. We supervise the -th module pose 
head by comparing its estimation, , with the ground truth, , using the L2 loss, . Our annotations for pose, , are obtained from the ground truth landmarks using a rigid head model (see Fig.~\ref{fig:cascade_regressor_full}). In the landmarks task we optimize a coordinate smooth L1 loss () enhanced by a local attention mechanism () on the heatmaps, like~\cite{Wang19Awing,Huang21ADnet}. The final landmark loss is defined as , where 's are scalars empirically optimized. For further details, please see the supplementary material.

To obtain a top-performing head pose estimation model (see Table~\ref{tab:pose})
we pre-train the network only with the landmark task, , and fine-tune with both tasks, landmarks and pose, like~\cite{Valle21}. For multi-task fine-tuning we use the loss  , where  is a hyperparameter. Although we use intermediate supervision at every HG module, the prediction of  to estimate , as well as the visual features, are extracted from the last module. Let  be the 3D coordinates on the 3D head model that correspond to the  2D landmarks. If the pose estimated by the backbone is given by , then the \emph{initial shape}, , is computed by projecting the 3D model, , where  is the 3D2D projection function. 



\subsection{Geometric and Visual Feature Extraction}
\label{sec:features}


For each step in the cascaded regressor, the input features are a combination of local appearance at each landmark (i.e. visual features) and global representation of the facial structure (i.e. geometric features). 
How visual and positional information is extracted and combined has a direct impact on the performance of the regressor (see Table \ref{tab:ablation_wflw_npe90}). 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/bmvc22_step_regressor.pdf}   
    \caption{Appearance and shape feature extraction for the -th step regressor.}
    \label{fig:cascade_regressor_step}
\end{figure}


Let  be the output feature map of the last stacked HG module in the MTN. We extract local appearance information from a square window, , of size , centered at each landmark location, , in . We use a fixed affine transform with a grid generator and sampler~\cite{Jaderberg15spatial} to crop and re-sample  at a fixed size, regardless of . Then, using convolutional layers, we extract the visual features, , corresponding to the -th landmark at step . We iteratively reduce  at each step , in a coarse-to-fine approach. 

Positional information is crucial to maintain the shape of the face when local appearance alone is not sufficient (e.g. in presence of occlusions, blur, make-up, etc.). Relative distances between landmarks provide enhanced geometrical features compared to their absolute locations since they explicitly represent the facial shape. This relative positional information can be defined from displacement vectors between landmarks~\cite{Li20sld}. Let  be the displacement vector corresponding to -th landmark in the -th step. In contrast to~\cite{Li20sld}, we learn a high dimensional embedding from  using a Multi layer Perceptron (MLP), , that facilitates the aggregation of the visual local appearance and the facial shape information. In the experiments, we show that this way of encoding relative positional information in  improves the shape-preserving ability of the network (see section \ref{sec:ablation}).

Let  be the feature vector used to compute .
At each step  of the cascade (see Fig.~\ref{fig:cascade_regressor_step}), and for each landmark , we add the visual features extracted from the backbone network, , with the relative positional features, , computed from the current shape, , to produce the encoded features, . 


\subsection{Cascade Shape Regressor Using GATs}
\label{sec:gnn_regressor}
The step regressor architecture (Fig.~\ref{fig:cascade_regressor_step}) is composed of stacked GAT layers inspired by the ones in the Attentional Graph Neural Net~\cite{Sarlin20superglue}. We consider the facial shape as a single densely connected graph where nodes are the landmark locations, . To weigh the shared information across nodes, we compute a dynamic adjacency matrix per GAT layer , .  We learn these matrices as an attention from a given landmark to every other in the graph.

The input to the first GAT layer at step  are the encoded features, . Let  be the features of the -th landmark produced by the (-1)-th GAT layer, that are also the input to -th layer (). From now on, we drop the step-index  to simplify the notation.
The updated feature vector after the -th layer is defined as 
where  is the concatenation operator,  is the information aggregated, or message, of the nodes neighboring . Focusing on the message generation procedure, a query vector , is assigned to landmark  and key , and value vectors , to every other landmark . The attention weight of landmark  to landmark  is the \texttt{SoftMax} over the key-query similarities  SoftMax, being  the elements of the adjacency matrix  and the transmitted message  the weighted average of the value vectors:
,
where
, 
 and 
. 
Matrices  and bias vectors  are learned.

Finally, the last GAT layer output  is processed by a decoder, an MLP, to obtain the corresponding displacement, . We constraint the values in , applying an \texttt{ArcTan} activation and scaling the result, to be in the interval . In practice, this constraint makes the single-step regressor search problem simpler, boosting training convergence. Given a trained MTN backbone, we train the cascade with the  loss, where  are the ground truth landmark coordinates.


\section{Experiments}
\label{sec:experiments}

To train and evaluate our method, we conduct different experiments in four complementary datasets which have been acquired in-the-wild and bear different levels of difficulty:

\textbf{300W}~\cite{Sagonas16} provides 68 manually annotated landmarks.
We employ the 300W private extension, which uses 3837 images as training set and adds 600 test images divided into indoor and outdoor subgroups.

\textbf{COFW-68} is a re-annotated version of COFW~\cite{Burgos13} with 68 landmarks. It is conceived for testing landmark detectors with occlusions in a cross-dataset approach. The testing set in COFW-68 is made of 507 images. The annotations include the landmark positions and the visibility labels for the same 68 points as in 300W.

\textbf{WFLW}~\cite{Wu18lab} is composed of challenging in-the-wild images and provides 98 manually annotated landmarks. The dataset has 7500 training and 2500 testing faces. It is divided into 6 subgroups: pose, expression, illumination, make-up, occlusion and blur.

\textbf{MERL-RAV}~\cite{Kumar20luvli} is a re-annotated version of 19,000 AFLW images with 68 landmarks, like 300W. It provides 15,449 training and 3,865 test faces divided into 3 orientation subsets: frontal, half-profile and profile. This recent dataset includes externally occluded visibility and self-occluded labels. 


\subsection{Evaluation Metrics}
In order to quantify the head pose estimation error, we use the Mean Absolute Error (MAE) metric,
, 
where  is the number of testing images,  is the ground truth and  represents a single predicted pose parameter. 

Focusing on the landmark estimation task, Normalized Mean Error (NME) is the standard metric, 
.
Where  and  denote, respectively, the ground-truth and predicted coordinates of the -th landmark and  is a normalization value which varies depending on the dataset: inter-ocular (int-ocul), distance between outer eye corners; inter-pupils, distance between pupil/eye centers; and box, computed as the geometric mean of the landmarks ground truth bounding box (). 

We also use Failure Rate (FR) and Area Under the Curve (AUC). FR evaluates the robustness of algorithms in terms of NME, indicating the percentage of images with an NME above a given threshold. AUC is calculated by computing the area under the Cumulative Error Distribution (CED) curve from 0 to the FR threshold. We introduce the Normalized mean Percentile Error 90 () which represents the NME for the image at the 90\% of the dataset, sorted by NME. This metric is particularly convenient for small data subsets where the FR is not representative. 

In all our tables results ranked \first{first}, \second{second} and \third{third} are shown respectively in blue, green and red colors.


\subsection{3D Pose Estimation Results}

First, we evaluate the MTN performance in 3D pose estimation. In Table~\ref{tab:pose}, we compare our pose estimation in 300W and WFLW with previous works in the literature. Our model shows a significant improvement. We reduce the mean MAE of the previous top performer, MNN~\cite{Valle21}, by 17\% and 27\% respectively in 300W and WFLW. The main reason behind this improvement is a better network architecture, stacked HGs vs. a single encoder-decoder in~\cite{Valle21} and the use of an attention mechanism.
Having such a precise head pose estimation is a critical factor in our proposal, since the cascade shape regressor initialization relies on this prediction. 

\begin{table}
\footnotesize
\begin{center}
\setlength{\tabcolsep}{4pt} 
\begin{tabular}{l|ccc|c|ccc|c|ccc|c}
\hline
       & \multicolumn{4}{c}{300W}   & \multicolumn{4}{c}{WFLW} & \multicolumn{4}{c}{MERL-RAV} \\
       & \multicolumn{4}{c}{Angular error }   & \multicolumn{4}{c}{Angular error } & \multicolumn{4}{c}{Angular error } \\
Method & yaw & pitch & roll & mean  & yaw & pitch & roll & mean & yaw & pitch & roll & mean \\
\hline
Yang~\cite{Yang15}         & 4.2 & 5.1 & 2.4 & 3.9 & - & - & - & - & - & - & - & - \\
JFA~\cite{Xu17JFA}         & 2.5 & 3.0 & 2.6 & 2.7 & - & - & - & - & - & - & - & - \\
ASMNet~\cite{Fard21AMSNet} & 1.62  & 1.80 & 1.24 & 1.55 & 2.97  & 2.93 & 2.21 & 2.70 & - & - & - & - \\
MNN~\cite{Valle21}         &  -  & - &  - &  1.56 & - & - & - & 2.08 & - & - & - & - \\
\hline
\textbf{SPIGA (Ours)}      &  \first{1.41}  & \first{1.70} &  \first{0.77} &  \first{1.29}
                           & \first{1.78} & \first{1.86} & \first{0.93} & \first{1.52} 
                           & \first{3.23} & \first{2.24} & \first{1.71} & \first{2.39}\\
\hline
\end{tabular}
\end{center}
\caption{Head pose MAE, in degrees, for 300W public, WFLW and MERL-RAV datasets.}
\label{tab:pose}
\end{table}

\subsection{Landmark Detection Results}
\label{landmars_sota}

WFLW is the most popular benchmark to evaluate the performance of facial landmark detection. Recent methods that adopt this dataset use the bounding boxes provided by HRnet~\cite{Wang21hrnet}, that were obtained from the ground truth landmark annotations.
By doing so, they achieve better performance (see  Table~\ref{tab:wflw}, AWing results improve from 4.36 to 4.21 NME). In Table~\ref{tab:wflw}, we clearly distinguish the bounding boxes used in the evaluation. Another important aspect to perform a fair comparison is the use of additional training data. 
In our discussion we do not consider methods that train with images or annotations other than those provided by WFLW. 

In Table~\ref{tab:wflw}, we show that our model outperforms current state-of-the-art (SOTA) in most of the WFLW subsets, as well as in the full set metrics. When it is compared with other GraphNets-based methods, our approach is 4\% and 32\% better in terms of NME and FR than  SLD~\cite{Li20sld}, and 7\% and 23\% better than SDFL~\cite{LinTIP21}. These results show that our relative positional encoding and the per layer graph attention mechanism have a strong impact on the performance of GraphNets. Further, our proposal is also more accurate than recent approaches based on transformers, when these models are trained only with WFLW data,  DTLD-s~\cite{Li22casctransf} and SPLT~\cite{Xia22slpt}, both with 4.14 NME in the full set. 
If we analyze the performance on some of the subsets, our method is 35\%, 25\%, 23\% and 39\% better than the previous SOTA, ADNet~\cite{Huang21ADnet}, in the illumination, make-up, occlusion and blur subsets. This proves the importance of learning a global representation of the facial structure, that CNNs alone do not provide.
Additionally, the low FR across the different subsets and better AUC values reaffirm that our model achieves a balanced trade-off between robustness and precision,  taking advantage of the complementary benefits from the CNN and GAT architectures.

\begin{table}
\scriptsize
\begin{center}
\setlength{\tabcolsep}{4pt} 
\begin{tabular}{c|c|ccccccc}
\hline 
Metric & Method & Testset & Pose & Expression & Illumination & Make-up & Occlusion & Blur \\
\hline
\hline 
&\multicolumn{8}{c}{Bounding boxes from WFLW benchmark} \\
\cline{2-9} 
\multirow{16}{*}{ (\%)()}  
& 3DDE~\cite{Valle193dde}                   & 4.68 & 8.62  & 5.21 & 4.65 & 4.60 & 5.77 & 5.41 \\
& DeCaFA~\cite{Dapogny19decafa}             & 4.62 & 8.11  & 4.65 & 4.41 & 4.63 & 5.74 & 5.38 \\
& AVS+SAN~\cite{Qian19Avs}                  & 4.39 & 8.42  & 4.68 & 4.24 & 4.37 & 5.60 & 4.86 \\
& AWing~\cite{Wang19Awing}                  & 4.36 & 7.38  & 4.58 & 4.32 & 4.27 & 5.19 & 4.96 \\
\cline{2-9}
&\multicolumn{8}{c}{Bounding boxes from GT landmarks (HRnet~\cite{Wang21hrnet} annotations)} \\
\cline{2-9}
& GlomFace~\cite{Zhu22glomface}             & 4.81 & 8.17  & - & - & - & 5.14 & - \\
& LUVLI~\cite{Kumar20luvli}                 & 4.37 & 7.56  & 4.77 & 4.30 & 4.33 & 5.29 & 4.94\\
& SDFL~\cite{LinTIP21}                 & 4.35 & 7.42  & 4.63 & 4.29 & 4.22 & 5.19 & 5.08\\
& AWing~\cite{Wang19Awing}              & 4.21 & \third{7.21}  & 4.46 & 4.23 & 4.02 & \third{4.99} & 4.82 \\ 
& SLD~\cite{Li20sld}                        & 4.21 & 7.36  & 4.49 & 4.12 & 4.05 & \second{4.98} & 4.82 \\
& HIHc\tablefootnote{Use RetinaFace detections.}~\cite{Lan21hih} & \third{4.18} & 7.20  & \first{4.19} & 4.45 & \second{3.97} & 5.00 & \third{4.81} \\
& ADNet~\cite{Huang21ADnet}                 & \second{4.14} & \first{6.96} & \second{4.38} & \third{4.09} & 4.05 & 5.06 & \second{4.79} \\
& DTLD-s~\cite{Li22casctransf}             & \second{4.14} & - & - & - & - & - & -\\
& SPLT~\cite{Xia22slpt}                 & \second{4.14} & \first{6.96} & \third{4.45} & \second{4.05} & \third{4.00} & 5.06 & \second{4.79}\\
\cline{2-9}
& \textbf{SPIGA (Ours)}       & \first{4.06} & \second{7.14} & \textbf{4.46} & \first{4.00} & \first{3.81} & \first{4.95} & \first{4.65}\\
\hline
\hline 
\multirow{7}{*}{ (\%)()} 
& GlomFace~\cite{Zhu22glomface}             & 3.77 & 17.48  & - & - & - & 6.73 & - \\
& DTLD-s~\cite{Li22casctransf}             & 3.44 & - & - & - & - & - & -\\
& LUVLI~\cite{Kumar20luvli}                 & 3.12 & 15.95 & 3.18 & 2.15 & 3.40 & 6.39 & \third{3.23} \\
& SDFL~\cite{LinTIP21}               & 2.72 & 12.88  & \second{1.59} & 2.58 & 2.43 & 5.71 & 3.62\\
& AWing~\cite{Wang19Awing}                  & \first{2.04} & \first{9.20} & \first{1.27} & \third{2.01} & \first{0.97} & \first{4.21} & \second{2.72} \\ 
& SLD~\cite{Li20sld}                        & 3.04 & 15.95 & 2.86 & 2.72 & \second{1.46} & \third{5.29} & 4.01 \\
& HIHc~\cite{Lan21hih}                      & 2.96 & 15.03 & \second{1.59} & 2.58 & \second{1.46} & 6.11 & 3.49 \\
& ADNet~\cite{Huang21ADnet}         & \third{2.72} & 12.72 & \third{2.15} & 2.44 & \third{1.94} & 5.79 & 3.54 \\
& SPLT~\cite{Xia22slpt}             & 2.76 & \third{12.27} & 2.23 & \second{1.86} & 3.40 & 5.98 & 3.88\\
\cline{2-9}
& \textbf{SPIGA (ours)}             & \second{2.08} & \second{11.66} & \textbf{2.23} & \first{1.58} & \second{1.46} & \second{4.48} & \first{2.20}\\
\hline
\hline 
\multirow{4}{*}{ (\%)()} 
& AWing~\cite{Wang19Awing}                  & 58.95 & 33.37 & 57.18 & 59.58 & 60.17 & \third{52.75} & 53.93 \\ 
& SLD~\cite{Li20sld}                        & 58.93 & 31.50 & 56.63 & 59.53 & 60.38 & 52.35 & 53.29 \\
& HIHc~\cite{Lan21hih}                      & \third{59.70} & 34.20 & \first{59.00} & \second{60.60} & \third{60.40} & 52.70 & \second{54.90} \\
& ADNet~\cite{Huang21ADnet}                 & \second{60.22} & \third{34.41} & 52.34 & 58.05 & 60.07 & \second{52.95} & \third{54.80} \\
& SPLT~\cite{Xia22slpt}                 & 59.50 & \second{34.80} & \third{57.40} & \third{60.10} & \second{60.50} & 51.50 & 53.50\\
\cline{2-9}
& \textbf{SPIGA (Ours)}                             & \first{60.56} & \first{35.31} & \second{57.97} & \first{61.31} & \first{62.24} & \first{53.31} & \first{55.31}\\
\hline
\end{tabular}
\end{center}
\caption{Evaluation of landmark detection on WFLW.}
\label{tab:wflw}
\end{table}


On the other hand, results of subsets where our approach is not competitive also bear some relevant insights. First, further research is needed in the expression subset, where our performance is not as good as the rest. This is due to the fact that the 3D facial model used to initialize the cascade is rigid (see Fig.~\ref{fig:expresion_pose_wflw_qualitative}).
Second, seemingly, in the pose subset, we are not the top performers. However, as we can see in Fig.~\ref{fig:expresion_pose_wflw_qualitative}, faces with extreme poses are not well annotated and self-occlusions are not marked. So, the evaluation on this subset of WFLW is questionable.

\begin{figure}
    \centering
    \includegraphics[width=0.15\textwidth]{figures/figure_qualitative/expression1.png}
    \includegraphics[width=0.15\textwidth]{figures/figure_qualitative/expression3.png}
    \includegraphics[width=0.15\textwidth]{figures/pose_fail/militar.png}
    \includegraphics[width=0.15\textwidth]{figures/pose_fail/smoking_guy.png}
    \includegraphics[width=0.15\textwidth]{figures/figure_qualitative/occlusion7.png}
    \includegraphics[width=0.15\textwidth]{figures/figure_qualitative/occlusion9.png}
    \caption{WFLW results on expressions (first 2 cols.) and pose examples (last 4 cols.). Shown in blue the ground truth and in green estimated landmarks.}
    \label{fig:expresion_pose_wflw_qualitative}
\end{figure}

MERL-RAV is one of the newest datasets, created to evaluate 2D facial alignment in-the-wild. It improves landmark annotations at half-profile and profile images by labeling the self-occlusion of landmarks.  
Hence, this dataset allows to correctly measure the performance of landmark detectors on samples with extreme poses. 
As we can see in Table~\ref{tab:merl_rav}, in terms of NME, our model is 6\% better than LUVLI's~\cite{Kumar20luvli} baseline, performing the best in all pose subsets. 

\begin{table}
\scriptsize
\begin{center}
\setlength{\tabcolsep}{4pt} 
\begin{tabular}{c|cccc|cccc}
  & \multicolumn{4}{c}{NME(\%)()} & \multicolumn{4}{|c}{AUC(\%)()} \\ 
  Method & All & Frontal & Half-Prof. & Profile & All & Frontal & Half-Prof.& Profile \\ 
\hline
DU-Net & 1.99 & 1.89 & 2.50 & 1.92 & 71.80 & 73.25 & 64.78 & 72.79\\ 
LUVLI~\cite{Kumar20luvli} & 1.61 & 1.74 & 1.79 & 1.25 & 77.08 & 75.33 & 74.69 & 82.10 \\
\hline
\textbf{SPIGA (Ours)} & \first{1.51} & \first{1.62} & \first{1.68} & \first{1.19} & \first{78.47} & \first{76.96} & \first{75.64} & \first{83.00} \\ 
\hline
\end{tabular}
\end{center}
\caption{Evaluation of landmark detection on MERL-RAV.}
\label{tab:merl_rav}
\end{table}

Finally, to verify the generalization and performance against occlusions, we conduct a cross-dataset experiment training with the 300W public split and testing with COFW-68 and 300W private. Results are summarized in Table~\ref{tab:cofw68_300w_private}. They prove the importance of the graph attention mechanism, which dynamically weighs landmark relationships according to the local image appearance and relative position, versus a learned static relationship approach, such as SLD~\cite{Li20sld}, ( of 3.93 vs 4.22 in COFW-68). Further, SPIGA trained on the 300W public dataset beats LUVLI~\cite{Kumar20luvli} ( of 2.52 vs 2.75 in COFW-68) with a backbone that has half the number of HG modules. 
It also obtains comparable results to a recent transformer-based method trained from scratch, DTLD-s~\cite{Li22casctransf}. It is marginally better than DTLD-s in 300W private and worse in COFW-68.
These results prove that a general architecture using GATs can complement and enhance CNN-based models, reaching better results in situations where ambiguity or noise is contaminating the local landmark appearance, where preserving structural landmarks consistency contributes to the final solution.

\begin{table}
\scriptsize
\setlength{\tabcolsep}{4pt} 
\begin{center}
\begin{tabular}{l|cc|cc|c}
\hline
       & \multicolumn{2}{c|}{ (\%)()}  
       & \multicolumn{2}{c|}{ (\%)()} 
       & \multicolumn{1}{c}{ (\%)()}  \\ 
       & 300W priv. & COFW-68 & 300W priv. & COFW-68  & COFW-68 \\ 
\hline
HRNetV2-W18~\cite{Wang21hrnet}    & -    & -    & -    & -    & 5.06  \\ 
HG1+SAAT \cite{Zhu21adversarial} & -    & -    & -    & -  & 4.61  \\ 
LUVLI(8)~\cite{Kumar20luvli}      & 2.24 & 2.75 & 68.3 & 60.8 & - \\ 
GlomFace~\cite{Zhu22glomface}     & -    & 2.69~\tablefootnote{Result comes from a personal communication with authors of~\cite{Zhu22glomface}, 2.09 mistakenly in the paper.} & -    & -    & 4.21 \\ 
SLD~\cite{Li20sld}                & -    & -    & -    & -    & 4.22  \\
SDFL~\cite{LinTIP21}              & -    & -    & -    & -   & 4.18  \\
SPLT~\cite{Xia22slpt}           & -    & -    & -    & -    & 4.10 \\
DTLD-s~\cite{Li22casctransf}      & 2.05 & \first{2.47} & 70.9 & \first{65.0} & - \\ 
\hline
\textbf{SPIGA(4) (ours)}               & \first{2.03} & 2.52 & \first{71.0} & 64.1 & \first{3.93} \\
\hline
\end{tabular}
\end{center}
\caption{Landmark detection results on 300W private and COFW-68. In (Â·) we show the number of HG modules.}
\label{tab:cofw68_300w_private}
\end{table}


\subsection{Ablation Study}
\label{sec:ablation}

We conduct our ablation study on WFLW to understand how SPIGA components impact specific subset metrics. 
Table~\ref{tab:ablation_wflw_npe90} shows that the addition of the cascade shape regressor outperforms the bare MTN backbone (using \texttt{SoftArgMax}). 
Our new relative positional encoding is better than stacking the vector  with the visual features,
and much better than using no positional information. The estimation of an attention per layer with the GAT improves with respect to use of a common attention matrix (GCN). An extended view of the effect of the learned adjacency matrix is shown in Fig.~\ref{fig:attention}. Occlusion images show how the attention mechanism relies on visible landmarks regardless of the layer. The regressor "looks" at distant and unoccluded landmarks at the first GAT layer and then at closer ones in the last layers. The contribution of the proposed coarse-to-fine scheme w.r.t. a constant size window () or a single pixel window () is also clear in Table~\ref{tab:ablation_wflw_npe90}. The improvement provided by SPIGA can be seen across all metrics. However, it is more prominent with the hard cases, as demonstrated by the results for the subsets  Makeup, Occlusion, and Blur, and the  of the full set.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/attention.png}
    \caption{Left pupil attention mechanism at first and last layer, respectively, of the first regressor step.}
    \label{fig:attention}
\end{figure}

\begin{table*}
\scriptsize
\begin{center}
\setlength\tabcolsep{2pt}
\begin{tabular}{c|c|cc|cc|cc|cc}
\hline 
\multicolumn{2}{c|}{Changed from SPIGA model:} & \multicolumn{2}{c}{Full} & \multicolumn{2}{c}{Make-up}  & \multicolumn{2}{c}{Occlusion} & \multicolumn{2}{c}{Blur}  \\
Changed       & From  To   &  &  &  &  &  &  &  &  \\
\hline
Shape model & SPIGA  MTN backbone 
& 4.13 & 6.93  & 4.06 & 7.43 & 5.10 & 8.58 & 4.81 & 7.70  \\
\hline
\multirow{2}{*}{Positional encoding} & SPIGA  w/o pos. encod. & 4.17 & 7.07  & 4.01 & 6.71 & 5.03 & 8.33 & 4.72 & 7.52 \\
 & SPIGA  stacking & 4.09 & 6.87  & \third{3.83} & \second{6.47} & \second{4.97} & 8.15 & \third{4.68} & \second{7.37} \\
\hline
Attention & GAT  GCN & \second{4.08} & \second{6.79}  & 3.84 & 6.54 & \third{4.98} & \first{8.05} & \third{4.68} & \second{7.37} \\
\hline
\multirow{2}{*}{Coarse-to-Fine} &    & 4.12 & 6.95  & 3.88 & 6.76 & 4.99 & 8.19 & 4.71 & 7.44  \\
 &    & \second{4.08} & \third{6.84}  & \second{3.82} & \third{6.53} & \third{4.98} & \third{8.13} & \second{4.67} & 7.43  \\
\hline
- & \textbf{Best SPIGA model}  & \first{4.06} & \first{6.76}  & \first{3.81} & \first{6.32} & \first{4.95} & \second{8.09} & \first{4.65} & \first{7.31}  \\
\hline
\end{tabular}
\end{center}
\caption{
Contribution of the SPIGA components to the () and () in WFLW.} 
\label{tab:ablation_wflw_npe90}
\end{table*}


In each row of Table~\ref{tab:ablation_step}, we display respectively the performance of three SPIGA models configured with one, two and three steps cascade. In each column, we show the NME obtained at each step. The final NME is reduced gradually as we increase the number of steps. Further, shorter cascades tend to have a better NME at the first step (4.17 vs 4.22). However, given also the larger FR they achieve (2.60 vs 2.44), we can conclude that longer cascades focus their first steps on improving their robustness.


\begin{table}
\scriptsize
\begin{center}
\begin{tabular}{c|ccc|ccc|ccc}
\hline 
 & \multicolumn{3}{c}{Step 1} & \multicolumn{3}{|c}{Step 2} & \multicolumn{3}{|c}{Step 3}  \\
Method &  &  &  &  &  &  &  &  &  \\
&  &  &  &  &  &  &  &  &  \\
\hline
SPIGA(1) & 4.17 & 59.53 & 2.60 & - & - & - & - & - & - \\
SPIGA(2) & 4.17 & 59.55 & 2.44 & 4.07 & 60.45 & 2.20 & - & - & - \\
SPIGA(3) & 4.22 & 59.10 & 2.44 & 4.08 & 60.41 & 2.12 & 4.06 & 60.56 & 2.08 \\
\hline
\end{tabular}
\end{center}
\caption{SPIGA results for cascades with different number of steps, shown in ().}
\label{tab:ablation_step}
\end{table}

In Fig.~\ref{fig:regressor_steps} we show the initialization and the landmark locations estimated at each step of the regressor cascade. When the face displays a neutral expression (top row), the initialization is reasonably good and the model converges to a solution within one regression step. Since SPIGA initializes landmarks with a 3D model featuring a neutral expression, when the face displays any other configuration, the initialization is much worse (lower row). However, even in this situation, the model is able to estimate the correct landmark locations in three regression steps. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/figure_gatstep/man1.png}
    \includegraphics[width=0.9\columnwidth]{figures/figure_gatstep/woman2.png}
    \caption{Estimated landmark locations: from 2D projection of the rigid 3D model (left) to the final result after the 3 regressor steps (right).}
    \label{fig:regressor_steps}
\end{figure}


\section{Conclusions}
\label{sec:conclussions}

We presented SPIGA, a face landmark regressor that combines a CNN with a cascade of Graph Attention Networks (GATs). The CNN provides the local appearance representation. The GAT regressor is endowed with a positional encoding and attention mechanism that learn the geometrical relationship among landmarks and encourage the model to produce plausible face shapes. It establishes a new SOTA in the WLFW, COFW-68 and MERL-RAV datasets. In our experimentation we verify that the positional encoding is the component that contributes most to the final result and the first steps of the cascade focus on improving the robustness. 
In addition, at each step, the regressor "looks" at distant and reliable landmarks in the first GAT layer and progressively focuses its attention on closer landmarks in the following ones.  These insights from our ablation analysis confirm that SPIGA is learning a global representation and explains why its improvement is most significative in challenging situations involving occlusions, heavy make-up, blur and extreme illumination.


\section*{Acknowledgements}

The following funding is gratefully acknowledged. Andr{\'e}s Prados was funded by the Comunidad de Madrid, Ayudantes de Investigaci{\'o}n grant PEJ-2019-AI/TIC-15032. Jos{\'e} M. Buenaposada is funded by the Comunidad de Madrid project RoboCity2030-DIH-CM (S2018 /NMT-4331).

\bibliography{faces}
\end{document}
