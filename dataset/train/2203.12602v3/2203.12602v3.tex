\documentclass{article}













\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[final]{neurips_2022}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{tabularx}
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage{subcaption}
\usepackage{tabulary,multirow,overpic,xcolor}
\definecolor{mygray}{gray}{0.92}
\definecolor{baselinecolor}{gray}{.9}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{tabulary}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{braket}
\usepackage{array}

\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{bbding}
\usepackage{listings}
\usepackage{tabu}

\usepackage{pifont} 
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\ncdot}{{\mkern 0mu\cdot\mkern 0mu}}
\def\x{}

\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcolumntype{z}[1]{>{\raggedleft\arraybackslash}p{#1pt}}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
		\global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcommand{\blocks}[3]{\multirow{3}{*}{-.1em] \text{\tcolor{1}\xycolor{3}, \wcolor{#2}}\#3}
}
\newcommand{\blockatt}[3]{\multirow{2}{*}{-.1em] \text{MLP(\wcolor{#2})}\end{array}\right]\times\times^\text{2}\times^\text{2}\times^{1, 2}^{2}^{2}^{1,3\dagger}^{1}^{2}^{3}90\%95\%I \in \mathcal{R}^{3 \times H \times W}  \Phi_{enc} \Phi_{dec}p\OmegaI\hat{I}tVTH\times W\times 3 \tau2\times16\times16 \frac{T}{2}\times \frac{H}{16}\times \frac{W}{16}D\mathbb{I}[p_{x, y, \cdot} \in \Omega] \sim \mathrm{Bernoulli}(\rho_\mathrm{mask})t*^*T \times \tau T\tau T\tau\emph{center}T\frac{\tau}{2}T\frac{\tau}{2}T\tauT\tauT\tau2T\frac{\tau}{2}\rho\frac{\tau}{2}\tau2TT\times\rightarrow\rightarrow\rightarrow\bullet\bulletT \times \tau\rho_{\rho=3}\rho_{\rho=3}224^2T \times \tau_{En}_{\times 2}_{En}_{\times 2}_{En}_{\times 2}_{En}\times\times224^2^2\rho\rho\rho\{ \}\emph{lr}=\emph{base learning rate}\times \emph{batch\  size\ /\ 256}\beta_1, \beta_2{=}0.9, 0.95\beta_1, \beta_2{=}0.9, 0.999\beta_1, \beta_2{=}0.9, 0.999\emph{no flip augmentation}\times\geq^2^2^2^2 \times _{f}\rho_{\rho=2}\rho_{\rho=2}\rho_{\rho=2}\rho_{\rho=2}\rho_{\rho=4}$~\cite{large}  & SlowOnly-R50 & Kinetics-400 & 8  & 32M & V & 94.2 & 72.1  \\
	\hline 
	\textcolor{gray}{MIL-NCE~\cite{milnce}}  & \textcolor{gray}{S3D} & \textcolor{gray}{HowTo100M} & \textcolor{gray}{32} & \textcolor{gray}{9M}   & \textcolor{gray}{V+T} & \textcolor{gray}{91.3} & \textcolor{gray}{61.0} \\
	\textcolor{gray}{MMV~\cite{mmv}}& \textcolor{gray}{S3D-G} & \textcolor{gray}{AS+HTM}  & \textcolor{gray}{32}   & \textcolor{gray}{9M}   & {\textcolor{gray}{ V+A+T}} & \textcolor{gray}{92.5} & \textcolor{gray}{69.6} \\
	\textcolor{gray}{CPD~\cite{cpd}}  & \textcolor{gray}{ResNet50}& \textcolor{gray}{IG300k} & \textcolor{gray}{16} & \textcolor{gray}{N/A} & \textcolor{gray}{V+T} & \textcolor{gray}{92.8} & \textcolor{gray}{63.8}  \\
    \textcolor{gray}{ELO~\cite{elo}}  & \textcolor{gray}{R(2+1)D} & \textcolor{gray}{Youtube8M-2} & \textcolor{gray}{N/A} &  \textcolor{gray}{N/A}  & \textcolor{gray}{V+A}  & \textcolor{gray}{93.8} & \textcolor{gray}{67.4}   \\
	\textcolor{gray}{XDC~\cite{xdc}}  & \textcolor{gray}{R(2+1)D} & \textcolor{gray}{Kinetics-400}  & \textcolor{gray}{32}  & \textcolor{gray}{15M} &  \textcolor{gray}{V+A} & \textcolor{gray}{84.2}  & \textcolor{gray}{47.1} \\ 
	\textcolor{gray}{XDC~\cite{xdc}}  & \textcolor{gray}{R(2+1)D} & \textcolor{gray}{IG65M}  & \textcolor{gray}{32}  & \textcolor{gray}{15M} &  \textcolor{gray}{V+A} & \textcolor{gray}{94.2}  & \textcolor{gray}{67.1} \\ 
	\textcolor{gray}{GDT~\cite{gdt}}  & \textcolor{gray}{R(2+1)D}& \textcolor{gray}{Kinetics-400} & \textcolor{gray}{32} & \textcolor{gray}{15M} & \textcolor{gray}{V+A} & \textcolor{gray}{89.3} & \textcolor{gray}{60.0}  \\
	\textcolor{gray}{GDT~\cite{gdt}}  & \textcolor{gray}{R(2+1)D}& \textcolor{gray}{IG65M} & \textcolor{gray}{32} & \textcolor{gray}{15M} & \textcolor{gray}{V+A} & \textcolor{gray}{95.2} & \textcolor{gray}{72.8}  \\
	
	\shline
	\textbf{VideoMAE} & ViT-B & Kinetics-400 & 16  & 87M & V & \textbf{96.1} & \textbf{73.3} \\
\end{tabular}
	\caption{\textbf{Comparison with the state-of-the-art methods on UCF101 and HMDB51.} Our VideoMAE reconstructs normalized cube pixels and is pre-trained with a masking ratio of 75\% for 3200 epochs on UCF101 and 4800 epochs on HMDB51, respectively. We report fine-tuning accuracy for evaluation. `V' refers to visual only, `A' is audio, `T' is text narration. ``N/A'' indicates the numbers are not available for us.} \label{tab:ucf_hmdb_supp}
	\vspace{-0.5em}
\end{table*} 


\section{Model result analysis}\label{sec:analysis}
In this section, we add the analysis of model results. As shown in Figure~\ref{fig:res:ImageMAE} and Figure~\ref{fig:res:IN21k}, our VideoMAE bring significant gain for most categories on SSV2, which implies that our VideoMAE can capture more spatiotemporal structure representations than ImageMAE and ImageNet-21k supervised pre-trained model. On the other hand, we also notice that our VideoMAE performs slightly worse than other two models on some categories. To better understand how the model works, we select several examples from validation set. The examples are shown in Figure~\ref{fig:sample}. For the example in the 1st row, we find our VideoMAE might not capture the motion information from very small object. We suspect that tokens containing the small motion might all be masked due to our extremely high masking ratio, so our VideoMAE could hardly reconstruct the masked small motion pattern. For the example in the 2nd row, we find our VideoMAE could capture the deformation of objects and movement from the squeeze of the hand, while this cannot be discriminated by image pre-training. We leave more detailed analysis of our VideoMAE for future work.


\begin{figure*}[t]
\centering
\includegraphics[width=.98\textwidth]{figures/image_videomae_top1.pdf} \\
(a) Categories that VideoMAE outperforms ImageMAE. We only show those gain larger than 10\%. \\
\includegraphics[width=.58\textwidth]{figures/image_videomae_top1_minus.pdf} \\
(b) Categories that ImageMAE outperforms VideoMAE. \\
\caption{ ImageMAE (64.8\%) vs. VideoMAE (69.6\%) on Something-Something V2.}
\label{fig:res:ImageMAE}
\end{figure*}

\begin{figure*}[t]\centering
\includegraphics[width=.98\textwidth]{figures/image21k_videomae_top1.pdf} \\
(a) Categories that VideoMAE outperforms ImageNet-21k supervised pre-trained model. We only show those gain larger than 15\%. \\
\includegraphics[width=.58\textwidth]{figures/image21k_videomae_top1_minus.pdf} \\
(b) Categories that ImageNet-21k supervised pre-trained model outperforms VideoMAE. \\
\caption{ImageNet-21k supervised pre-trained model (61.8\%) vs. VideoMAE (69.6\%) on Something-Something V2.}
\label{fig:res:IN21k}
\end{figure*}

\begin{figure*}[t]\centering
\includegraphics[width=.98\textwidth]{figures/sample.pdf} \\
\caption{Prediction examples of different models on Something-Something V2. For each example drawn from the validation dataset, the predictions with blue text indicating a correct prediction and red indicating an incorrect one. ``GT'' indicates the ground truth of the example.}
\label{fig:sample}
\end{figure*}


\section{Visualization}\label{sec:vis}
We show several examples of reconstruction in Figure~\ref{fig:vis_k400_sup} and Figure~\ref{fig:vis_ssv2_sup}. Videos are all randomly chosen from the validation set. We can see that even under an extremely high masking ratio, VideoMAE can produce satisfying reconstructed results. These examples imply that our VideoMAE is able to learn more representative features that capture the holistic spatiotemporal structure in videos.

\section{License of Data}\label{sec:license}
All the datasets we used are commonly used datasets for academic purpose. The license of the Something-Something V2\footnote{URL: \url{https://developer.qualcomm.com/software/ai-datasets/something-something}} and UCF101\footnote{URL: \url{https://www.crcv.ucf.edu/data/UCF101.php}} datasets is custom. The license of the Kinetics-400\footnote{URL: \url{https://www.deepmind.com/open-source/kinetics}}, HMDB51\footnote{URL: \url{https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database}} and AVA\footnote{URL: \url{https://research.google.com/ava/index.html}} datasets is CC BY-NC 4.0\footnote{URL: \url{https://creativecommons.org/licenses/by/4.0}}.
\begin{figure*}[t]\centering
\includegraphics[width=.98\textwidth]{figures/vis_k400_supp.pdf}\\
\caption{ \textbf{Uncurated random videos} on Kinetics-400 \emph{validation} set. We show the original video squence and reconstructions with different masking ratios. 
Reconstructions of videos are predicted by our VideoMAE pre-trained with a masking ratio of 90\%. 
}
\label{fig:vis_k400_sup}
\end{figure*}


\begin{figure*}[t]\centering
\includegraphics[width=.98\textwidth]{figures/vis_ssv2_supp.pdf}\\
\caption{\textbf{Uncurated random videos} on Something-Something V2 \emph{validation} set. We show the original video squence and reconstructions with different masking ratios. 
Reconstructions of videos are all predicted by our VideoMAE pre-trained with a masking ratio of 90\%. 
}
\label{fig:vis_ssv2_sup}
\end{figure*}


\end{document}