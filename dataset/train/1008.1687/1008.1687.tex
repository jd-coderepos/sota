\documentclass[11pt]{article}

\usepackage{amssymb,amsmath,amsthm}




\def\R{\mathbb{R}}
\def\eps{\varepsilon}
\def\ra{\rightarrow}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]


\def\eric#1{\marginpar{\fbox{E}}\footnote{~{\sf #1 --Eric}}}
\def\santosh#1{\marginpar{\fbox{L}}\footnote{~{\sf #1 --Santosh}}}
\def\daniel#1{\marginpar{\fbox{D}}\footnote{~{\sf #1 --Daniel}}}

\begin{document}

\title{A Deterministic Polynomial-time Approximation Scheme for Counting Knapsack Solutions}

\author{
Daniel Stefankovic\thanks{
Department of Computer Science, University of Rochester,
Rochester, NY 14627.  Email: stefanko@cs.rochester.edu.
Research supported in part by NSF grant CCF-0910415.
}
\and
Santosh Vempala\thanks{School of Computer Science, Georgia
Institute of Technology, Atlanta GA 30332.
Email: \{vempala,vigoda\}@cc.gatech.edu.
Research supported in part by NSF grant CCF-0830298 and CCF-0910584.}
\and
Eric Vigoda}



\maketitle


\begin{abstract}
Given  elements with nonnegative integer weights  and an integer capacity , we consider the counting version
of the classic knapsack problem: find the number of distinct
subsets whose weights add up to at most the given capacity. We
give a deterministic algorithm that estimates the number of
solutions to within relative error  in time polynomial
in  and  (fully polynomial approximation scheme). More
precisely, our algorithm takes time .
Our algorithm is based on dynamic programming.  Previously,
randomized polynomial time approximation schemes were known first
by Morris and Sinclair via Markov chain Monte Carlo techniques,
and subsequently by Dyer via dynamic programming and rejection
sampling.
\end{abstract}

\section{Introduction}
Randomized algorithms are usually simpler and faster than their
deterministic counterparts. In spite of this, it is widely
believed that P=BPP (see, e.\,g., \cite{AroraBarak}), i.e., at
least up to polynomial complexity, randomness is not essential.
This conjecture is supported by the fact that there are relatively
few problems for which exact randomized polynomial-time algorithms
exist but deterministic ones are not known. Notable among them is
the problem of testing whether a polynomial is identically zero (a
special case of this, primality testing was open for decades but a
deterministic algorithm is now known,~\cite{AKS}).

However, when one moves to approximation algorithms, there are
many more such examples. The entire field of approximate counting
is based on Markov chain Monte Carlo (MCMC) sampling
\cite{JerrumSinclair:survey}, a technique that is inherently
randomized, and has had remarkable success. The problems of
counting matchings
\cite{JerrumSinclair:matchings,JerrumSinclairVigoda}, colorings
\cite{Jerrum:colorings}, various tilings, partitions and
arrangements \cite{LubyRandallSinclair}, estimating partition
functions \cite{JerrumSinclair:Ising,StefankovicVempalaVigoda}, or
volumes \cite{DyerFriezeKannan,LovaszVempala} are all solved by
first designing a random sampling method and then reducing
counting to repeated sampling.
In all these cases, when
the input is presented explicitly, it is conceivable that
deterministic polynomial-time algorithms exist.\footnote{Volume
computation has an exponential lower bound for deterministic
algorithms, but that is due to the more general oracle model in
which the input is presented.}

The one notable example of a deterministic approximate counting
algorithm is Weitz's algorithm \cite{Weitz} for counting independent
sets weighted by an activity  for graphs of maximum degree 
when  is constant and
 where  is
the uniqueness threshold for the -regular tree.
This was later extended to counting all matchings of bounded degree
graphs \cite{BGKNT}.
An alternative deterministic approach of Bandyopadhyay and
Gamarnik \cite{BG} for colorings and independent sets of bounded degree
graphs only approximates the logarithm of the size of the feasible set.
The results of \cite{Weitz,BGKNT}
are the only two examples of an FPAS (fully polynomial approximation
scheme) for a \#P-complete problem that we are aware of.
One limitation of both of these results is that the running time is quite
large, in particular, the exponent depends on .
In contrast, our algorithm has a small polynomial running time.


Here we consider one of the most basic counting problems, namely
approximately counting the number of  knapsack solutions.
More precisely, we are given a list of nonnegative integer weights
 and an integer capacity ,
\footnote{Our results extend to real-valued inputs, but we do not
consider that here to avoid the issue of the model of computation.}
and wish to count the number of subsets of
the weights that add up to at most . This decision version of
this problem is NP-hard, but has a well-known pseudo-polynomial
algorithm based on dynamic programming. For any , we
give a deterministic algorithm that estimates the number of
solutions to within relative error  in time polynomial in
 and~.

Our result follows a line of work in the literature. Dyer et al.
\cite{DFKKPV} gave a randomized subexponential time algorithm for
this problem, based on near-uniform sampling of feasible solutions
by a random walk. Morris and Sinclair \cite{MorrisSinclair}
improved this, showing a rapidly mixing Markov chain, and obtained
an FPRAS (fully polynomial randomized approximation scheme). The
proof of convergence of the Markov chain is based on the technique
of canonical paths and a notion of balanced permutations
introduced in their analysis. In a surprising development, Dyer
\cite{Dyer}, gave a completely different approach, combining
dynamic programming with simple rejection sampling to also obtain
an FPRAS. Although much simpler, randomization still appears to be
essential in his approach---without the sampling part, his
algorithm only gives a factor  approximation.

Our algorithm is also based on dynamic programming, and similar to
Dyer, is inspired by the pseudo-polynomial algorithm for the
decision/optimization version of the knapsack problem. The
complexity of the latter algorithm is , where  is the
capacity bound. A similar complexity can be achieved for the
counting problem as well using the following recurrence:

with appropriate initial conditions. Here  is the number
of knapsack solutions that use a subset of the items  and their weights sum to at most .

Roughly speaking, since we are only interested in approximate
counting, Dyer's idea was the following: scale down the capacity
to a polynomial in , scale down the weights by the same factor
and round down the new weights, and then
count the solutions to the new problem efficiently using the
pseudo-polynomial time dynamic programming algorithm. The
new problem could have more solutions (since we rounded down)
but Dyer showed it has at
most a factor of  more for a suitable choice of scaling.
Further, given the exact counting algorithm for the new problem,
one gets an efficient sampler, then uses rejection sampling to
only sample solutions to the original problem. The sampler leads
to a counting algorithm using standard techniques.
Dyer's algorithm has running time  using
the above approach, and 
using a more
sophisticated approach that also utilizes randomized rounding.

To remove the use of randomness, one might attempt to use a more
coarse-grained dynamic program, namely rather than consider all
integer capacities , what if we only consider
weights that go up in some geometric series? This would allow us
to reduce the table size to  rather than . The
problem is that varying the capacity even by an exponentially
small factor  can change the number of solutions by a
constant factor!

Instead, we index the table by the prefix of items
allowed and {\em the number of solutions}, with the entry in the
table being the minimum capacity that allows these indices to be
feasible. We can now consider approximate numbers of solutions and
obtain a small table.

Our main result is the following:

\begin{theorem}\label{thm:main}
Let  and  be an instance of a knapsack problem. Let  be the number
of solutions of the knapsack problem. There is a deterministic algorithm which
for any  outputs  such that .
The algorithm runs in time .
\end{theorem}

The running time of our algorithm is competitive with that of
Dyer. One interesting improvement is the dependence on .
Our algorithm has a linear dependence on  (ignoring
the logarithm term), whereas Monte Carlo approaches, including
Dyer's algorithm \cite{Dyer} and earlier algorithms for this
problem \cite{MorrisSinclair,DFKKPV}, have running time which
depends on .

\section{Algorithm}
\label{sec:algorithm}

In this section we present our dynamic programming algorithm.
Fix an knapsack instance and fix an ordering on the elements and their weights.

We begin by defining the function

where  is the smallest
 such that there exist at least  solutions to the knapsack problem with weights
 and capacity .
We can not compute the function  efficiently since the second argument
ranges over all real numbers.  It will be used in the analysis and it is useful
for motivating the definition of our algorithm.

Note that, by definition,  is monotone in
, that is,

The value of  is easy to compute for :

Note that the number of knapsack solutions satisfies:


We will show that  satisfies the following recurrence.
\begin{lemma}
\label{lem:recur}
For any  and any  we have

\end{lemma}

We defer the proof of the above lemma to Section \ref{sec:proofs}.

Now we move to an approximation of  that we can compute efficiently.
We define a function  which only considers a small set of values 
for the second argument in the function , these values will form a
geometric progression.

Let

and let

The function  is defined
using the recurrence~\eqref{recur} that the function  satisfies.
Namely,  is defined by the following recurrence:


More precisely,  is defined by the following algorithm {\bf CountKnapsack}.

\begin{center}
\fbox{\parbox{5in}{
\begin{minipage}{4.5in}
\begin{tt}
{\bf CountKnapsack\\}

Input: Integers  and .

\begin{enumerate}
\item Set  and  for .
\item Set  and .
\item For , for , set

where, by convention,  for .
\item
Let

\item Output .
\end{enumerate}
\end{tt}
\end{minipage}
}}
\end{center}


The minimum in the recurrence \eqref{upd:alg}, although formally over the entire interval , only needs to be evaluated at the discrete subset where the second argument goes to the next integer.  Hence we will be able to compute  efficiently.

The key fact is that  approximates  in the following sense.

\begin{lemma}
\label{lem:approx}
Let . Assume that for all  we have that  satisfy~\eqref{appr}.
Then for all  we have that  computed using~\eqref{upd:alg}
satisfies:

\end{lemma}

We defer the proof of Lemma \ref{lem:approx} to Section \ref{sec:proofs}.

We can now prove that the output  of the
algorithm {\bf CountKnapsack} is a
 multiplicative
approximation of .

Note that  is never an underestimate of , since,

that is, there are at most  solutions. We also have
 that is, there are at least  solutions.
Hence


This proves that the output  of the algorithm {\bf CountKnapsack}
satisfies the conclusion of Theorem \ref{thm:main}.
It remains to show that the algorithm can be modified to achieve the
claimed running time.

\subsection{Running Time}

As noted earlier, the minimum in the
recurrence \eqref{upd:alg} only needs to be evaluated at the discrete subset 
where the second argument goes to the next integer.
For ,
the set  is  where:

Thus,  can be computed in  time.  Since
there are  entries of the table and  the
algorithm {\bf CountKnapsack} can be implemented in  time.

To improve the running time, recall that  is a non-decreasing function in .
Similarly, it is easy to see by induction that  is a non-decreasing
function in .  Hence, in \eqref{upd:alg}, the first argument in the
maximum (namely, )
is non-decreasing in .
Similarly, the second argument in the maximum is a non-increasing function in .
Hence
the minimum of the maximum of the two arguments occurs either at
the boundary (that is, for ) or for  where the derivative changes from negative to positive,
that is  such that for 

and for 

Therefore, if we had the set  in sorted order, we can find the
 that achieves the minimum in \eqref{upd:alg} using binary
search in  time.  We do not have  in sorted order,
but we do have  and  in sorted order. We can instead do
binary search over  to find the  that achieves
the minimum over that set, and then over , and finally
compare the two values. Therefore, step 3 of the algorithm {\bf
CountKnapsack} to compute  can be implemented in
 time, and the entire algorithm then takes
 time. This completes the proof of
the running time claimed in Theorem \ref{thm:main}.


\section{Proofs of Lemmas}
\label{sec:proofs}

Here we present the proofs of the earlier lemmas.

We begin with the proof of Lemma \ref{lem:recur} which presents the
recurrence for the function .

\begin{proof}[Proof of Lemma \ref{lem:recur}]
Fix any . Let . There exist at least
 solutions with weights  and capacity
. There exist at least 
solutions with weights  and capacity . Hence there exist at least 
solutions with weights  and capacity  and thus
. To see that we did not double count, note
that the first type of solutions (of which
there are at least ) has  and the second type of
solutions (of which there are at least ) has .

We established


Consider the solution of the knapsack problem with weights  and capacity 
that has at least  solutions. Let  be the fraction of the solutions that do
not include item . Then , , and
hence

We established

Equations~\eqref{elo} and~\eqref{eup} yield~\eqref{recur}.
\end{proof}

We now prove Lemma \ref{lem:approx} that the function  approximates .
\begin{proof}[Proof of Lemma \ref{lem:approx}]
By the assumption of the lemma and~\eqref{monot} we have

and

Combining~\eqref{h1} and~\eqref{h2} with min and max operators we obtain

establishing that  computed using~\eqref{upd:alg} satisfy the lower bound in~\eqref{appr}.

By the assumption of the lemma and~\eqref{monot} we have

and

Combining~\eqref{h3} and~\eqref{h4} with min and max operators we obtain

establishing that  computed using~\eqref{upd:alg} satisfy the upper bound in~\eqref{appr}.
\end{proof}









\bibliographystyle{abbrv}
\bibliography{knapsack}


\end{document}
