\begin{table*}[t]
\centering
\Huge
\resizebox{\textwidth}{!}{


\begin{tabular}{lcccccccccccccccccc} 
\rowcolor[HTML]{CBCEFB}
                                    & Avg.                & Avg.              & Train time             &  Inf. Speed          & Close & Drag & Insert & Meat off & Open   & Place & Place \\
\rowcolor[HTML]{CBCEFB}
Models                              & Success   & Rank  & (in days)  &  (in fps)  & Jar   & Stick & Peg   & Grill    & Drawer & Cups  & Wine  \\
\toprule
\bczcnn~\cite{jang2021bcz,peract2022arxiv}      & ~~1.3  & 3.7 & -  & - & 0 & 0 & 0 & 0 & 4 & 0 & 0 \\
\rowcolor[HTML]{EFEFEF}
\bczvit~\cite{jang2021bcz,peract2022arxiv}      & ~~1.3  & 3.8 & -  & - & 0 & 0 & 0 & 0 & 0 & 0 & 0   \\
\unet~\cite{c2farm,peract2022arxiv}             & 20.1 & 3.1 & -  & -  & 24 & 24 & 4 & 20 & 20 & 0 & 8 \\
\rowcolor[HTML]{EFEFEF}
\peract~\cite{peract2022arxiv}      & 49.4      & 1.9      & 16.0       & ~~4.9     & \tb{55.2} \rpmh 4.7 & 89.6      \rpmh 4.1 & ~~5.6     \rpmh 4.1 & 70.4      \rpmh 2.0 & \tb{88.0} \rpmh 5.7 & 2.4      \rpmh 3.2 & 44.8      \rpmh 7.8 \\
\method (ours)                      & \tb{62.9} & \tb{1.1} & ~~\tb{1.0} & \tb{11.6} & 52.0      \rpmh 2.5 & \tb{99.2} \rpmh 1.6 & \tb{11.2} \rpmh 3.0 & \tb{88.0} \rpmh 2.5 & 71.2      \rpmh 6.9 & \tb{4.0} \rpmh 2.5 & \tb{91.0} \rpmh 5.2 \\
\bottomrule
\rowcolor[HTML]{CBCEFB}
                                    & Push & Put in &  Put in & Put in & Screw & Slide & Sort & Stack & Stack & Sweep to & Turn \\
\rowcolor[HTML]{CBCEFB}
Models                              &  Buttons & Cupboard & Drawer & Safe & Bulb & Block & Shape & Blocks & Cups & Dustpan & Tap \\
\toprule
\bczcnn~\cite{jang2021bcz,peract2022arxiv}      &  0 & 0 & 8 & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 8 \\
\rowcolor[HTML]{EFEFEF}
\bczvit~\cite{jang2021bcz,peract2022arxiv}      &  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 16 \\
\unet~\cite{c2farm,peract2022arxiv} & 72 & 0 & 4 & 12 & 8 & 16 & 8 & 0 & 0 & 0 & 68 \\
\rowcolor[HTML]{EFEFEF}
\peract~\cite{peract2022arxiv}      & ~~92.8     \rpmh 3.0 & 28.0      \rpmh 4.4 & 51.2      \rpmh 4.7 & 84.0      \rpmh 3.6 & 17.6      \rpmh 2.0 & 74.0      \rpmh 13.0  & 16.8      \rpmh 4.7 & 26.4      \rpmh 3.2 & ~~2.4     \rpmh 2.0 & 52.0      \rpmh 0.0 & 88.0      \rpmh 4.4 \\
\method (ours)                      & \tb{100.0} \rpmh 0.0 & \tb{49.6} \rpmh 3.2 & \tb{88.0} \rpmh 5.7 & \tb{91.2} \rpmh 3.0 & \tb{48.0} \rpmh 5.7 & \tb{81.6} \rpmh ~~5.4 & \tb{36.0} \rpmh 2.5 & \tb{28.8} \rpmh 3.9 & \tb{26.4} \rpmh 8.2 & \tb{72.0} \rpmh 0.0 & \tb{93.6} \rpmh 4.1 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Multi-Task Performance on RLBench.} \method outperforms state-of-the-art methods while being faster to train and execute. \method has the best success rate and rank when averaged across all tasks. Performance for Image-BC (CNN), Image-BC (ViT) and C2F-ARM-BC are as reported by Shridhar et al. in \cite{peract2022arxiv}. We re-evalaute PerAct using the released final model and estimate mean and variance. \method is 2.3X faster on execution speed than PerAct and outpeforms it on 16/18 tasks. The training time and inference speed of PerAct and \method are measured on the same GPU model.}
\vspace{-2mm}
\label{table:rlbench}
\end{table*}

 \subsection{Simulation Experiments} 
\label{sec:exp_sim}

\smallsec{Simulation Setup}
We follow the simulation setup in PerAct \cite{peract2022arxiv}, where CoppelaSim \cite{rohmer2013v} is applied to simulate various RLBench \cite{james2020rlbench} tasks. A Franka Panda robot with a parallel gripper is controlled to complete the tasks. We test on the same  tasks as PerAct, including picking and placing, tool use, drawer opening, and high-accuracy peg insertions (see the appendix for a detailed specification of each task). Each task includes several variations specified by the associated language description. Such a wide range of tasks and intra-task variations requires the model to not just specialize in one specific skill but rather learn different skill categories. The visual observations are captured from four noiseless RGB-D cameras positioned at the front, left shoulder, right shoulder, and wrist with a resolution of . To achieve the target gripper pose, we generate joint space actions by using the same sampling-based motion planner \cite{sanchez2001single, sucan2012open} as in \cite{c2farm, peract2022arxiv}.

\smallsec{Baselines}
We compare against the following three baselines: (1) \textit{Image-BC} \cite{jang2021bcz} is an image-to-action behavior cloning agent that predicts action based on the image observations from the sensor camera views. We compare with two variants with CNN and ViT vision encoders respectively. (2) \textit{C2F-ARM-BC} \cite{c2farm} is a behavior cloning agent that converts the RGB-D images into multi-resolution voxels and predicts the next key-frame action using a coarse-to-fine scheme. (3) \textit{PerAct} \cite{peract2022arxiv} is the state-of-the-art multi-task behavior cloning agent that encodes the RGB-D images into voxel grid patches and predicts discretized next key-frame action using the perceiver~\cite{jaegle2021perceiver} transformer.

\smallsec{Training and Evaluation Details}
Just like the baselines, we use the RLBench training dataset with  expert demonstrations per task ( demonstrations over all tasks). Similar to PerAct, we apply translation and rotation data augmentations. For translation, we randomly perturb the point clouds in the range . For rotation, we randomly rotate the point cloud around the -axis (vertical) in the range of . We train \method for 100k steps, using the LAMB~\cite{you2019large} optimizer as PerAct. We use a batch size of 24 and an initial learning rate of . We use cosine learning rate decay with warm-start for 2K steps.

For Image-BC and C2F-ARM-BC, we adopt the evaluation results from \cite{peract2022arxiv} since their trained models have not been released. These results overestimate the performance of Image-BC and C2F-ARM-BC, as they select the best model for each of the 18 tasks independently based on the performance on validation sets. Hence, the reported performance does not reflect a single multi-task model. Nevertheless, these baselines still underperform both PerAct and \method (see Tab.~\ref{table:rlbench}). For PerAct, we evaluate the final model released by \citet{peract2022arxiv}.  We test our models (including the models in the ablation study, Tab.~\ref{table:rlbench_ablation} (left)) and PerAct on the same  variations for each task. Due to the randomness of the sampling-based motion planner, we run each model five times on the same  variations for each task and report the average success rate and standard deviation in Tab. \ref{table:rlbench}.

To fairly compare the training efficiency against PerAct, we train both PerAct and our model with the same GPU type (NVIDIA Tesla V100) and number of GPUs (8), as reported by \citet{peract2022arxiv}. We report the total training time for both models in Tab.~\ref{table:rlbench} (``Training time''). We also evaluate the inference speed of PerAct and \method models by running the prediction inferences for the same input data on the same GPU (NVIDIA RTX 3090). 

\smallsec{Multi-Task Performance}
Tab. \ref{table:rlbench} compares the performance between \method and the baselines. We find that PerAct and \method perform significantly better than the rest.
Overall, \method outperforms all baselines with the best rank and success rate when averaged across all tasks. It outperforms prior state-of-the-art methods, C2F-ARM, by 42 percentage points (213\% relative improvement); and PerAct by 13 percentage points (26\% relative improvement). \method outperforms PerAct on  (16/18) of the tasks. More remarkably, \method trains 36X faster than PerAct for achieving the same performance (see Fig.~\ref{fig:rvt_vs_peract}). We also observe that at inference time, \method is 2.3X faster than PerAct. 
These results demonstrate that \method is both more accurate and scalable when compared to existing state-of-the-art voxel-based methods. More visualizations of the task setups and the model performance are also provided.

\smallsec{Ablation Study}
We conduct ablation experiments to analyze different design choices of \method: (a) the resolution of the rendered images (``Im. Res.'' column in Tab.~\ref{table:rlbench_ablation} (left)); (b) whether to include the correspondence information across rendered images (``View Corr.''); (c) whether to include the depth channel (``Dep. Ch.''); (d) whether to separately process the tokens of each image before jointly processing all tokens (``Sep. Proc.''); (e) the projection type for rendering---perspective or orthographic (``Proj. Type''); (f) whether to use rotation augmentation (``Rot. Aug.''); (g) the number of views and camera locations for re-rendering (``\# of View'' and ``Cam. Loc.''); and (h) the benefit of using re-rendered images versus using real sensor camera images (``Real'' for ``Cam. Loc.'').
\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{figures/views_3.pdf}
\caption{We evaluate \method with various camera locations for re-rendering (a-d) and find that locations in (a) perform best. We also test various projection options (e-f) for rendering images and find that \method works better with orthographic images.}
\vspace{-3mm}
\label{fig:view}
\end{figure*}

Tab.~\ref{table:rlbench_ablation} (left) summarizes the ablation experiment results. The same table along with the mean and standard deviation for each task can be found in the appendix Tab.~\ref{table:rlbench_ablation_std}. Below we discuss the findings: 

(a) As expected, virtual images rendered at higher resolution help as \method with virtual image resolution 220 outperforms the one with 100.

(b) Adding correspondence information for points across different views helps (see Sec.~\ref{sec:method}). This is likely because the network need not learn to solve the correspondence problem and can predict more consistent heatmaps across views. Note that the view correspondence channel is not present in sensor images but is rendered along with RGB(D) images in \method.
\vspace{1mm}
\\
(c) Adding the depth channel along with RGB channels helps, likely because it aids 3D reasoning.

(d) Independently processing the tokens from a single image, before merging all the image tokens, helps. It is likely because this design expects the network to extract meaningful features for each image before reasoning over them jointly.
\begin{table}[!t]
 \centering
 \scriptsize
 \setlength\tabcolsep{4pt}
 \begin{tabular}{ccccccccccccccccc}
\rowcolor[HTML]{CBCEFB}
Im.   & View  & Dep. & Sep.  & Proj.  & Rot. & Cam     & \# of & Avg.          \\
\rowcolor[HTML]{CBCEFB}
Res.  & Corr. & Ch.  & Proc. & Type   & Aug. & Loc.    & View  & Succ.         \\
\toprule
220   & \yes  & \yes & \yes  & Orth.  & \yes & Cube    & 5     & \textbf{62.9} \\
\rowcolor[HTML]{EFEFEF}
100   & \yes  & \yes & \yes  & Orth.  & \yes & Cube    & 5     & 51.1          \\
220   & \no   & \yes & \yes  & Orth.  & \yes & Cube    & 5     & 59.7          \\
\rowcolor[HTML]{EFEFEF}
220   & \yes  & \no  & \yes  & Orth.  & \yes & Cube    & 5     & 60.3          \\
220   & \yes  & \yes & \no   & Orth.  & \yes & Cube    & 5     & 58.4          \\
\rowcolor[HTML]{EFEFEF}
220   & \yes  & \yes & \yes  & Pers.  & \yes & Cube    & 5     & 40.2          \\
220   & \yes  & \yes & \yes  & Orth.  & \no  & Cube    & 5     & 60.4          \\
\rowcolor[HTML]{EFEFEF}
220   & \yes  & \yes & \yes  & Orth.  & \yes & Cube    & 3     & 60.2          \\
220   & \yes  & \yes & \yes  & Orth.  & \yes & Front   & 1     & 35.8          \\
\rowcolor[HTML]{EFEFEF}
220   & \yes  & \yes & \yes  & Orth.  & \yes & Rot. 15 & 5     & 59.9          \\
220   & \yes  & \yes & \yes  & Pers.  & \no  & Real    & 4     & 10.4          \\
\rowcolor[HTML]{EFEFEF}
220   & \yes  & \yes & \yes  & Orth. & \no  & Real    & 4     & 22.9          \\
\bottomrule
 \end{tabular}
 ~~~~~
 \begin{tabular}{lccccc}
\rowcolor[HTML]{CBCEFB}
                & \# of                 & \# of                 & \# of                & Succ.                    & Succ.                    \\
\rowcolor[HTML]{CBCEFB}
Task            & vari.                 & train                 & test                 & (+ mark.)                & (- mark.)               \\
\toprule
Stack           & \multirow{2}{*}{~~3}  & \multirow{2}{*}{14}   & \multirow{2}{*}{10}  & \multirow{2}{*}{100\%}   & \multirow{2}{*}{100\%}   \\
blocks          &                       &                       &                      &                          &                          \\
\rowcolor[HTML]{EFEFEF}
Press           &                       &                       &                      &                          &                          \\
\rowcolor[HTML]{EFEFEF}
sanitizer       & \multirow{-2}{*}{~~1} & \multirow{-2}{*}{~~7} & \multirow{-2}{*}{10} & \multirow{-2}{*}{~~80\%} & \multirow{-2}{*}{~~80\%} \\
Put marker      & \multirow{2}{*}{~~4}  & \multirow{2}{*}{12}   & \multirow{2}{*}{10}  & \multirow{2}{*}{~~~~0\%} & \multirow{2}{*}{--}      \\
in mug/bowl     &                       &                       &                      &                          &                          \\
\rowcolor[HTML]{EFEFEF}
Put object      &                       &                       &                      &                          &                          \\
\rowcolor[HTML]{EFEFEF}
in drawer       & \multirow{-2}{*}{~~3} & \multirow{-2}{*}{10}  & \multirow{-2}{*}{10} & \multirow{-2}{*}{~~50\%} & \multirow{-2}{*}{100\%}  \\
Put object      & \multirow{2}{*}{~~2}  & \multirow{2}{*}{~~8}  & \multirow{2}{*}{10}  & \multirow{2}{*}{~~50\%}  & \multirow{2}{*}{~~50\%}  \\
in shelf        &                       &                       &                      &                          &                          \\
\midrule
All tasks       & 13                    & 51                    & 50                   & ~~56\%                   & 82.5\%                   \\
\bottomrule
 \end{tabular}
 \vspace{1mm}
 \caption{\textbf{Left:} Ablations on RLBench. A larger res., adding view correspondence, adding depth channel, separating initial attention layers, orthographic projection, using rotation aug., and re-rendered views around cube improve the performance. \textbf{Right:} Results of the real-world experiments. A single \method model can perform well on most tasks with only a few demonstrations.}
 \label{table:rlbench_ablation}
 \vspace{-7mm}
\end{table}


 (e) Rendering images with orthographic projection performs better than rendering with perspective projection, for both the cube and real camera locations. We hypothesize that it is because orthographic projection preserves the shape and size of an object regardless of its distance from the camera (see Fig.~\ref{fig:view} (e-f)). It also highlights the advantage of re-rendering, as real sensors generally render with perspective projections.

(f) As expected, using 3D rotation augmentation in the point cloud before rendering helps. To take advantage of 3D augmentations, the re-rendering process is necessary. 

(g) The model with  views around a cube (Fig.~\ref{fig:view} (a)) performs the best followed by the one with  views (front, top, left) around a cube (Fig.~\ref{fig:view} (b)). The single view model, where we predict the third coordinate as an offset like TransporterNet~\cite{transporter2021}, performs substantially worse, calling for the need for multiple views for 3D manipulation. It also highlights the advantage of re-rendering as with re-rendering we can leverage multiple views even with a single sensor camera. We also empirically find that rotating the location of the cameras by  (see Fig.~\ref{fig:view}) with respect to the table (and robot) decreases performance. This could be likely because views aligned with the table and robot might be easier to reason with (e.g., overhead top view, aligned front view).

(h) \method performs better with re-rendered images as compared to using sensor camera images (Tab.~\ref{table:rlbench_ablation} (left), second last row). The sensor camera images are rendered with perspective projection (physical rendering process) and are not straightforward to apply 3D augmentations (e.g., rotation) without re-rendering. Also, the location of sensor cameras may be sub-optimal for 3D reasoning, e.g., the views are not axially aligned with the table or robot (see Fig.~\ref{fig:view} (d)). All these factors contribute to \method performing better with re-rendered images than with sensor camera images. 

Notably, one might consider rearranging the sensor cameras to match the re-rendering views in order to bypass re-rendering. However, this will void the gains from using orthographic projections, 3D augmentation, and adding correspondences. This also strictly requires a multi-camera setup (Fig.~\ref{fig:view} (a)), which is more costly and less portable in the real world than using one sensor camera. Finally, we have briefly explored view selection and found an option that works well across tasks. Further optimization of views, including the sensor and re-rendered ones, is an interesting future direction.

\subsection{Real-World}
\label{sec:exp_real}
\ifarxiv
We study the performance of \method on real visual sensory data by training and testing the model on a real-world setup. See the attached videos\footnote{\label{footnote:video}Videos are provided at \href{https://robotic-view-transformer.github.io/}{https://robotic-view-transformer.github.io/}.}
for more details about the setup and model performance.
\else
We study the performance of \method on real visual sensory data by training and testing the model on a real-world setup. See the attached videos\footnote{\label{footnote:video}Videos are provided at \href{https://corlrvt.github.io/}{https://corlrvt.github.io/}.}for more details about the setup and model performance.
\fi

\smallsec{Real World Setup}
We experiment on a table-top setup using a statically mounted Franka Panda arm. The scene is perceived via an Azure Kinect (RGB-D) camera statically mounted in a third-person view. We calibrate the robot-camera extrinsics and transform the perceived point clouds to the robot base frame before passing into \method. Given a target gripper pose from \method, we use FrankaPy~\cite{zhang:arxiv2020} to move the robot to the target with trajectory generation and feedback control.

\smallsec{Tasks}
We adopt a total of 5 tasks similar to the ones in PerAct~\cite{peract2022arxiv} (see Tab.~\ref{table:rlbench_ablation} (right)): \textit{stack blocks}, \textit{press sanitizer}, \textit{put marker in mug/bowl}, \textit{put object in drawer}, \textit{put object in shelf}. Each task can be instantiated with different \textit{variations} defined by the language description. For example, for \textit{stack blocks}, some variations could be ``put yellow block on blue block'' and ``put blue block on red block''. Given a task and variation, we sample a scene by placing the task-related objects and a set of distractor objects on the table in a random configuration. 

\smallsec{Data Collection}
We first collect a dataset for training \method through human demonstration. Given a sampled task and scene configuration, we ask the human demonstrator to specify a sequence of gripper target poses by kinesthetically moving the robot arm around. Once we have the target pose sequence, we reset the robot to the start pose, and then control it to sequentially move to each target pose following the specified order. We simultaneously record the RGB-D stream from the camera during the robot's motion to the targets. This provides us with a dataset of RGB-D frames paired with target pose annotations. In total, we collected 51 demonstration sequences over all 5 tasks.

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/real_quantitative_2.pdf}
\caption{\textbf{Examples of \method in the real world.} A single \method model can perform multiple tasks (5 tasks, 13 variations) in the real world with just 10 demonstrations per task.}
\vspace{-3mm}
\label{fig:real}
\end{figure*}

\smallsec{Results}
We train on real-world data for 10K steps, with the same optimizer, batch size, and learning rate schedule as the simulation data. We report the results in Tab.~\ref{table:rlbench_ablation} (right). Overall, \method achieves high success rates for the \textit{stack block} task (100\%) and the \textit{press sanitizer} task (80\%). Even on longer horizon tasks such as putting objects in drawers and shelves (e.g., the robot has to first open the drawer/shelf and then pick up the object), our model achieves 50\% success rates (see Fig.~\ref{fig:real}). We found \method struggled with marker-related tasks, which is likely due to sparse and noisily sensed point clouds. We further divide the results into two sets: ``+ markers'' (full set) and ``- markers''. Our model overall achieves an 82.5\% success rate on non-marker tasks. The marker issue can potentially be addressed by attaching the camera to the gripper to capture point clouds at higher quality. Another possibility is to use zoom-in views similar to C2F-ARM~\cite{c2farm}.