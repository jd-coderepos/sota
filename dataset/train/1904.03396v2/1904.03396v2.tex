

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[figuresleft]{rotating}

\usepackage{caption} 
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}
\usepackage{url}
\usepackage{todonotes}
\usepackage{ctable}
\usepackage{stfloats}

\usepackage{makecell}
\usepackage{tikz}
\usepackage{appendix}


\usepackage{tkz-graph}
\usetikzlibrary{fit}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{orange},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\usepackage{fdsymbol}

\newcommand{\cevb}[1]{\reflectbox{\scalebox{2}[1.5]{\ensuremath{\vec{\scalebox{0.5}[0.66]{\reflectbox{\ensuremath{#1}}}}}}}}
\newcommand{\vecb}[1]{\reflectbox{\reflectbox{\scalebox{2}[1.5]{\ensuremath{\vec{\scalebox{0.5}[0.66]{\ensuremath{#1}}}}}}}}


\aclfinalcopy \def\aclpaperid{1620} 



\newcommand\BibTeX{B{\sc ib}\TeX}

\newcommand\yg[1]{\textcolor{blue}{[YG: #1]}} \newcommand\ygr[1]{\textcolor{pink}{[YG: #1]}} 

\newcommand\amr[1]{\textcolor{red}{[AM: #1]}} 

\newcommand\ido[1]{\textcolor{orange}{[ID: #1]}} \newcommand\idor[1]{\textcolor{yellow}{[ID: #1]}} 

\newcommand\am[1]{\textcolor{blue}{[AM: #1]}} 

\newcommand\uc[1]{\textcolor{purple}{#1}} 


\newcommand\ourplans[0]{BestPlan}
\newcommand\ourbaseline[0]{StrongNeural}
\newcommand\ourrandom[1]{RandomPlan-#1}


\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Accepted as a long paper in NAACL-2019}
\rhead{}
\setlength{\headheight}{11pt}
\setlength{\headsep}{10pt}

\title{
    Step-by-Step: Separating Planning from Realization \\ 
    in Neural Data-to-Text Generation\thanks{\hspace{0.17cm}This research was supported in part by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1) and by a grant from Theo Hoffenberg and Reverso.}
}

\author{Amit Moryossef \;\; Yoav Goldberg \;\; Ido Dagan \\
\texttt{amitmoryossef@gmail.com, \{yogo,dagan\}@cs.biu.ac.il} \\
\\ Bar Ilan University, Ramat Gan, Israel \\
Allen Institute for Artificial Intelligence
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. 
For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system's reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure.
\end{abstract}

\thispagestyle{fancy}

\section{Introduction}
Consider the task of data to text generation, as exemplified in the WebNLG corpus \cite{colin2016webnlg}. The system is given a set of RDF triplets describing facts (entities and relations between them) and has to produce a fluent text that is faithful to the facts. An example of such triplets is:\0.5em]
With a possible output:\0.5em]
Other outputs are also possible:\0.5em]
These variations result from different ways of structuring the information: choosing which fact to mention first, and in which direction to express each fact. Another choice is to split the text into two different sentences, e.g., \0.5em]
Overall, the choice of fact ordering, entity ordering, and sentence splits for these facts give rise to 12 different structures, each of them putting the focus on somewhat different aspect of the information. Realistic inputs include more than two facts, greatly increasing the number of possibilities.

Another axis of variation is in how to verbalize the information for a given structure. For example, (2) can also be verbalized as\0.5em] and (5) as:\0.5em]
We refer to the first set of choices (how to structure the information) as \emph{text planning} and to the second (how to verbalize a plan) as \emph{plan realization}.\footnote{Note that the variation from 5 to 5a includes the introduction of a pronoun. This is traditionally referred to as \emph{referring expression generation} (REG), and falls between the planning and realization stages. We do not treat REG in this work, but our approach allows natural integration REG systems' outputs.} 

The distinction between planning and realization is at the core of classic natural language generation (NLG) works \cite{reiter2000building,DBLP:journals/corr/GattK17}. However, a recent wave of \emph{neural NLG systems} ignores this distinction and treat the problem as a single end-to-end task of learning to map facts from the input to the output text \cite{gardent2017webnlg,duvsek2018findings}. These neural systems encode the input facts into an intermediary vector-based representation, which is then decoded into text. While not stated in these terms, the neural system designers hope for the network to take care of both the planning and realization aspect of text generation. A notable exception is the work of \citet{puduppully2018data}, who introduce a neural content-planning module in the end-to-end architecture.


While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts \cite{wiseman2017challenges}, struggle to produce a coherent order of facts, and are often not faithful to the input facts, either omitting, repeating, hallucinating or changing facts (the NLG community refers to such errors as errors in \emph{adequacy} or \emph{correctness} of the generated text).
When compared to template-based methods, the neural systems win in fluency but fall short regarding content selection and faithfulness to the input \cite{puzikov2018e2e}. Also, they do not allow control over the output's structure.
We speculate that this is due to demanding too much of the network: while the neural system excels at capturing the language details required for fluent realization, they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner.

\paragraph{Proposal} we propose an explicit, symbolic, text planning stage, whose output is fed into a neural generation system. The text planner determines the information structure and expresses it unambiguously---in our case as a sequence of ordered trees. This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts. Once the plan is determined,\footnote{The exact plan can be determined based on a data-driven scoring function that ranks possible suggestions, as in this work, or by other user provided heuristics or a trained ML model. The plans' symbolic nature and precise relation to the input structures allow verification of their correctness.} a neural generation system is used to transform it into fluent, natural language text. By being able to follow the plan structure closely, the network is alleviated from the need to determine higher-level structural decisions and can track what was already covered more easily. This allows the network to perform the task it excels in, producing fluent, natural language outputs.

We demonstrate our approach on the WebNLG corpus and show it results in outputs which are as fluent as neural systems, but more faithful to the input facts. 
The method also allows explicit control of the output structure and the generation of diverse outputs (some diversity examples are available in the Appendix).
We release our code and the corpus extended with matching plans in \url{https://github.com/AmitMY/chimera}.

\section{Overview of the Approach}\label{sec:preliminaries}
\paragraph{Task Description}
Our method is concerned with the task of generating texts from inputs in the form of RDF sets. Each input can be considered as a graph, where the entities are nodes, and the RDF relations are directed labeled edges. 
Each input is paired with one or more reference texts describing these triplets. The reference can be either a single sentence or a sequence of sentences. 
Formally, each input  consists of a set of triplets of the form , where  (``subject'' and ``object'') correspond to entities from DBPedia, and  is a labeled DBPedia relation ( and  are the sets of entities and relations, respectively). 
For example, Figure \ref{fig:graph-original} shows a triplet set  and Figure \ref{fig:graph-text} shows a reference text.
We consider the data set as a set of input-output pairs , where the same  may appear in several pairs, each time with a different reference.

\paragraph{Method Overview}
We split the generation process into two parts: text planning and sentence realization. Given an input , we first generate a text plan  specifying the division of facts to sentences, the order in which the facts are expressed in each sentence, and the ordering of the sentences. This data-to-plan step is non-neural (Section \ref{sec:plan-structure}).
Then, we generate each sentence according to the plan. This plan-to-sentence step is achieved through an NMT system (Section \ref{sec:plan-neural}). 

\begin{figure*}[t]
    \centering
    \begin{subfigure}{.49\textwidth}
        \caption{Example input RDF}

        \tikz[remember picture] \node[inner sep=0] (a) {
            \begin{footnotesize}
            \makecell[l]{
                AIP\_Advances \textbar\ editor \textbar\ A.T.\_Charlie\_Johnson\\
                A.T.\_Charlie\_Johnson \textbar\ almaMater \textbar\ Harvard\_University\\
                AIP\_Advances \textbar\ ISSN\_number \textbar\ "2158-3226"\\
                A.T.\_Charlie\_Johnson \textbar\ residence \textbar\ United\_States
            }
            \end{footnotesize}
        };
        \vspace{1cm}
        \label{fig:graph-original}
    \end{subfigure}\begin{subfigure}{.49\textwidth}
        \centering
        \caption{Possible corresponding text plan}
        \tikz[remember picture] \node[inner sep=0] (b) {
        \resizebox{\linewidth}{!}{
            \begin{tikzpicture}
              \GraphInit[vstyle=Normal]
              \SetGraphUnit{1.5}
              \tikzset{VertexStyle/.append style={rectangle}}
            
              \Vertex[x=0,y=3,L=AT Charlie Johnson]{AT1}
              \Vertex[x=4.5,y=3]{AIP Advances}
              \Vertex[x=10,y=3]{2158-3226}
              
              \tikzset{VertexStyle/.append style={circle}}
              \Vertex[x=-2.1,y=3]{1}
              
              \Edge[style={->,>=triangle 45},label={}](AT1)(AIP Advances)
              \Edge[style={->,>=triangle 45},label=](AIP Advances)(2158-3226)
              \draw[draw=black] (-2.5,2.5) rectangle ++(13.5,1);
            
              \tikzset{VertexStyle/.append style={rectangle}}
            
              \Vertex[x=0,y=1,L=AT Charlie Johnson]{AT2}
              \Vertex[x=7,y=2]{United States}
              \Vertex[x=7,y=0]{Harvard University}
              
              \tikzset{VertexStyle/.append style={circle}}
              \Vertex[x=-2.1,y=1,style={circle}]{2}
            
              \Edge[style={->,>=triangle 45},label=](AT2)(United States)
              \Edge[style={->,>=triangle 45},label=](AT2)(Harvard University)
              \draw[draw=black] (-2.5,-0.5) rectangle ++(13.5,3);
            \end{tikzpicture}
        }
        };
        \label{fig:graph-plan}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \caption{Linearization of the text plan\hspace{3cm}\phantom1}
        \tikz[remember picture] \node[inner sep=0] (c) {
            \makecell[l]{
                \textbf{A.T.\_Charlie\_Johnson}  editor [ \textbf{AIP\_Advances}  issn number [ \textbf{2158-3226} ] ] . \\
                \textbf{A.T.\_Charlie\_Johnson}  residence [ \textbf{United\_States} ]  alma mater [ \textbf{Harvard\_University} ]
            }
        };
        \label{fig:graph-linearized}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \vspace{0.3cm}
        \caption{Possible output sentence\hspace{3cm}\phantom1}
        \tikz[remember picture] \node[inner sep=0] (d) {
            \makecell[l]{
                \textbf{A.T. Charlie Johnson} is the editor of \textbf{AIP Advances} which has the ISSN number \textbf{2158-3226}.\\
                \textbf{He} lives in the \textbf{United States}, and graduated from \textbf{Harvard University}.
            }
        };
        \label{fig:graph-text}
    \end{subfigure}
    
    \caption{Summary of our proposed generation process: the planner takes the input RDF triplets in (a), and generates the explicit plan in (b). The plan is then linearized (c) and passed to a neural generation system, producing the output (d).}
    \label{fig:sentence-plan}
    
    \begin{tikzpicture}[overlay, remember picture]
\draw[-stealth, line width=5pt, gray] ([shift={(-5mm, -2mm)}]a.east)--([shift={(-1mm, 0.5mm)}]b.west);
        \draw[-stealth, line width=5pt, gray] ([shift={(-30mm, -2mm)}]b.south)--([shift={(17mm, 0mm)}]c.north);
        \draw[-stealth, line width=5pt, gray] ([shift={(17mm, 0mm)}]c.south)--([shift={(16.7mm, 0mm)}]d.north);
    \end{tikzpicture}
\end{figure*} Figure \ref{fig:sentence-plan} demonstrates the entire process.

To facilitate our plan-based architecture, we devise a method to annotate  pairs with the corresponding plans (Section \ref{sec:plan-train}), and use it to construct a dataset which is used to train the plan-to-text translation. The same dataset is also used to devise a plan selection method (Section \ref{sec:plan-ranking}).

\paragraph{General Applicability}
It is worth considering the dataset-specific vs. general applicability aspects of  our method. On the low-level details, this work is very much dataset dependent. We show how to represent plans for specific datasets, and, importantly for this work, how to automatically construct plans for this dataset given inputs and expected natural language outputs. The method of plan construction will likely not generalize ``as is'' to other datasets, and the plan structure itself may also be found to be lacking for more demanding generation tasks. However, on a higher level, our proposal is very general: intermediary plan structures can be helpful, and one should consider ways of obtaining them, and of using them. In the short term, this will likely take the form of ad-hoc explorations of plan structures for specific tasks, as we do here, to establish their utility. In the longer term, research may evolve to looking into how general-purpose plan are structured. Our main message is that the separation of planning from realization, even in the context of neural generation, is a useful one to be considered.


\section{Text Planning} \label{sec:plan-structure}

\paragraph{Plan structure}
Our text plans capture the division of facts to sentences and the ordering of the sentences. Additionally, for each sentence, the plan captures
(1) the ordering of facts within the sentence; 
(2) The ordering of entities within a fact, which we call the \emph{direction} of the relation. For example, the \texttt{\{A, location, B\}} relation can be expressed as either \emph{A is located in B} or \emph{B is the location of A}; 
(3) the structure between facts that share an entity, namely chains and sibling structures as described below.

\begin{figure}[ht]
    \begin{subfigure}{\linewidth}
        \resizebox{\linewidth}{!}{
        \begin{tikzpicture}
          \GraphInit[vstyle=Normal]
          \SetGraphUnit{1.5}
          \tikzset{VertexStyle/.append style={rectangle}}
        
          \Vertex[x=0,y=0]{John}
          \Vertex[x=4.5,y=0]{London}
          \Vertex[x=9,y=0]{England}
          \Edge[style={->,>=triangle 45},label=](John)(London)
          \Edge[style={->,>=triangle 45},label=](London)(England)
        \end{tikzpicture}
        }
        \caption{Chain: John lives in London, the capital of England.}
        \label{fig:structure-chain}
    \end{subfigure}\\
    \begin{subfigure}{\linewidth}
        \resizebox{0.58\linewidth}{!}{
        \begin{tikzpicture}
          \GraphInit[vstyle=Normal]
          \SetGraphUnit{1.5}
          \tikzset{VertexStyle/.append style={rectangle}}
        
          \Vertex[x=0,y=1]{John}
          \Vertex[x=4.5,y=2]{London}
          \Vertex[x=4.5,y=0]{Bartender}
          \Edge[style={->,>=triangle 45},label=](John)(London)
          \Edge[style={->,>=triangle 45},label=](John)(Bartender)
        \end{tikzpicture}
        }        
        \caption{Sibling: John lives in London and works as a bartender.}
        \label{fig:structure-siblings}
    \end{subfigure}\\
    \begin{subfigure}{\linewidth}
        \resizebox{\linewidth}{!}{
        \begin{tikzpicture}
          \GraphInit[vstyle=Normal]
          \SetGraphUnit{1.5}
          \tikzset{VertexStyle/.append style={rectangle}}
        
          \Vertex[x=0,y=1]{John}
          \Vertex[x=4.5,y=2]{London}
          \Vertex[x=4.5,y=0]{Bartender}
          \Vertex[x=9,y=2]{England}
          \Edge[style={->,>=triangle 45},label=](John)(London)
          \Edge[style={->,>=triangle 45},label=](John)(Bartender)
          \Edge[style={->,>=triangle 45},label=](London)(England)
        \end{tikzpicture}
        }        
        \caption{Combination: John lives in London, the capital of England, and works as a bartender.}
        \label{fig:structure-combined}
    \end{subfigure}

    \caption{Fact construction structure.}
    \label{fig:structure-plan}
\end{figure} 

A text plan is modeled as a sequence of sentence plans, to be realized in order. Each sentence plan is modeled as an ordered tree, specifying the structure in which the information should be realized. Structuring each sentence as a tree enables a clear succession between different facts through shared entities. Our text-plan design assumes that each entity is mentioned only once in a sentence, which holds in the WebNLG corpus.
The ordering of the entities and relations within a sentence is determined by a pre-order traversal of the tree.

Figure \ref{fig:sentence-plan}b shows an example of a text plan. 
Formally, given the input , a text plan  is a sequences of sentence plans . A sentence plan  is a labeled, ordered tree, with arcs of the form , where  are head and modifier nodes, each corresponding to an input entity, and  is the relation between nodes, where  is the RDF relation, and  denotes the direction in which the relation is expressed:  if , and  if .
A text plan  is said to match an input  iff every triplet  in  is expressed in  exactly once, either as an edge  or as an edge .

Chains  represent a succession of facts that share a middle entity (Figure \ref{fig:structure-chain}), while siblings --- nodes with the same parent ---  represents a succession of facts about the same entity (Figure \ref{fig:structure-siblings}). Sibling and chain structures can be combined (Figure \ref{fig:structure-combined}).
An example of an input we addressed in the WebNLG corpus, and matching text plan is given in Figure \ref{fig:graph-plan}.\\
\textbf{Exhaustive generation } For small-ish input graphs ---such as those in the WebNLG task we consider here---it is trivial to generate all possible plans by first considering all the ways of grouping the input into sets, then from each set generating all possible trees by arranging it as an undirected graph and performing several DFS traversals starting from each node, where each DFS traversal follows a different order of children.\footnote{If a graph includes a cycle (0.4\% of the graphs in the WebNLG corpus contain cycles) we skip it, as it is guaranteed that a different split will result in cycle-free graphs.}


\subsection{Adding Plans to Training Data}\label{sec:plan-train}
While the input RDFs and references are present in the training dataset, the plans are not. We devise a method to recover the latent plans for most of the input-reference pairs in the training set, constructing a new dataset of  triplets of inputs, reference texts, and corresponding plans.

We define the reference , and the text-plan  to be consistent with each other iff (a) they exhibit the same splitting into sentences---the facts in every sentence in  are grouped as a sentence plan in , and (b) for each corresponding sentence and sentence-plan, the order of the entities is identical.

The matching of plans to references is based on the observations that (a) it is relatively easy to identify entities in the reference texts, and a pair of entities in an input is unique to a fact; (b) it is relatively easy to identify sentence splits; (c) a reference text and its matching plan must share the same entities in the same order, and with the same sentence splits.\\
\textbf{Sentence split consistency }
We define a set of triplets to be \emph{potentially consistent} with a sentence
iff each triplet contains at least one entity from the sentence (either its subject or object appear in the sentence), and each entity in the sentence is covered by at least one triplet.
Given a reference text, we split it into sentences using NLTK \cite{bird2004nltk}, and look for divisions of  into disjoint sets such that each set is consistent with a corresponding sentence. For each such division, we consider the exhaustive set of all induced plans.\\
\textbf{Facts order consistency } 
A natural criterion would be to consider a reference sentence and a sentence-plan originating from the corresponding RDF as matching iff the sets of entities in the sentence and the plan are identical, and all entities appear in the  same order.\footnote{An additional constraint is that no two triplets in the RDFs set share the same entities. This is to ensure that if two entities appeared in a structure, only one relation could have been expressed there. This almost always holds in the WebNLG corpus, failing on only 15 out of 6,940 input sets.}
Based on this, we could represent each sentence and each plan as a sequence of entities, and verify the sequences match.

However, using this criterion is complicated by the fact that it is not trivial to map between the entities in the plan (that originate from the RDF triplets) and the entities in the text. In particular, due to language variability, the same plan entity may appear in several forms in the textual sentences. Some of these variations (i.e. ``A.F.C Fylde'' vs.  ``AFC Fylde'') can be recognized heuristically, while others require external knowledge (``UK conservative party'' vs. ``the Tories''), and some are ambiguous and require full-fledged co-reference resolution (``them'', ``he'', ``the former''). Hence, we relax our matching criterion to allow for possible unrecognized entities in the text.

Concretely, we represent each sentence plan as a sequence of its entities , and each sentence as the sequence of its entities which we managed to recognize and to match with an input entity .\footnote{We match plan entities to sentence entities using greedy string matching with Levenshtein distance   \cite{levenshtein1966binary} for each token and a manually tuned threshold for a match. 
While this approach results in occasional false positives, most cases are detected correctly. We match dates by using the 
\hyperlink{https://github.com/wanasit/chrono-python}{chrono-python} package that parses dates from natural language texts.}

We then consider a sentence and a sentence-plan to be \emph{consistent} if the following two conditions hold:
(1) The sentence entities  are a proper sub-sequence of the plan entities ; and (2) each of the remaining entities in the plan already appeared previously in the plan. The second condition accounts for the fact that most un-identified entities are due to pronouns and similar non-lexicalized referring expressions, and that these only appear after a previous occurrence of the same entity in the text.\footnote{A sensible alternative would be to use a coreference resolution system at this stage. In our case it turned out to not help, and even performed somewhat worse.}

\subsection{Test-time Plan Selection}\label{sec:plan-ranking}

To select the plan to be realized, we propose a mechanism for ranking the possible plans. Our plan scoring method is a product-of-experts model, where each expert is a conditional probability estimate for some property of the plan. The conditional probabilities are MLE estimates based on the plans in the training set constructed in section \ref{sec:plan-train}. Estimates involving relation names are smoothed using Lidstone smoothing to account for unseen relations.
We use the following experts:

\noindent\textbf{Relation direction} 
For every relation , we compute its probability to be expressed in the plan in its original order ( or in the reverse order (): .
This captures the tendency of certain relations to be realized in the reversed order to how they are defined in the knowledge base.
For example, in the WebNLG corpus the relation ``manager'' is expressed as a variation of ``is managed by'' instead of one of ``is the manager of'' in 68\% of its occurrences ().

\noindent\textbf{Global direction} 
We find that while the probability of each relation to be realized in a reversed order is usually below 0.5, still in most plans of longer texts there are one or two relations that appear in the reversed order. 
We capture this tendency using an expert that considers the conditional probability  of observing  reversed edges in an input with  triplets.

\noindent\textbf{Splitting tendencies}
For each input size, we keep track of the possible ways in which the set of facts can be split to subsets of particular sizes. 
That is, we keep track of probabilities such as  of realizing an input of 7 RDF triplets as three sentences, each realizing the corresponding number of facts.

\noindent\textbf{Relation transitions}
We consider each sentence plan as a sequence of the relation types expressed in it  followed by an EOS symbol, and compute the markov transition probabilities over this sequence: . The expert is the product of the transition probabilities of the individual sentence plans in the text plan.
This captures the tendencies of relations to follow each other and in particular, the tendencies of related relations such as birth-place and birth-date to group, allowing their aggregation in the generated text (\emph{John was born in London on Dec 12th, 1980}).


Each of the possible plans are then scored based on the product of the above quantities.\footnote{We note that for an input of  triplets, there are  possible plans,
making this method prohibitive for even moderately sized input graphs. However, it is sufficient for the WebNLG dataset in which . For larger graphs, better plan scoring and more efficient search algorithms should be devised. We leave this for future work.}

The scores work well for separating good from lousy text plans, 
and we observe a threshold above which most generated plans result in adequate texts. 
We demonstrate in Section \ref{sec:results} that realizing highly-ranked plans manages to obtain good automatic realization scores.
We note that the plan in Figure \ref{fig:graph-plan} is the one our ranking algorithm ranked first for the input in Figure  \ref{fig:graph-original}.

\begin{figure*}[ht]
    \begin{subfigure}{\linewidth}
        (a) 
        \resizebox{\linewidth}{!}{
            \begin{minipage}{46em}
                \underline{\textbf{Dessert}  course [ \textbf{Bionico}  country [ \textbf{Mexico} ]  ingredient [ \textbf{Granola} ]  region [ \textbf{Jalisco} ] ] \hspace{2cm}}\\
                \emph{The \textbf{Dessert} \textbf{Bionico} requires \textbf{Granola} as one of its ingredients and originates from the \textbf{Jalisco} region of \textbf{Mexico} .}
            \end{minipage}
        }

\end{subfigure}
    
    \vspace{1em}
    
    \begin{subfigure}{0.5\linewidth}
        (b)
        \resizebox{.95\linewidth}{!}{
            \begin{minipage}{23em}
                \textbf{Bionico}  country [ \textbf{Mexico} ]  region [ \textbf{Jalisco} ] . \\
                \underline{\textbf{Dessert}  course [ \textbf{Bionico}  ingredient [ \textbf{Granola} ] ]} 
                \emph{\textbf{Bionico} is a food found in the \textbf{Mexico} region \textbf{Jalisco}. \\
                The \textbf{Dessert} \textbf{Bionico} requires \textbf{Granola} as an ingredient.}
            \end{minipage}
        }
\end{subfigure}
    \begin{subfigure}{0.5\linewidth}
         (c)
        \resizebox{.95\linewidth}{!}{
            \begin{minipage}{23em}
                \textbf{Bionico}  ingredient [ \textbf{Granola} ]  course [ \textbf{Dessert} ] . \\
                \underline{\textbf{Bionico}  region [ \textbf{Jalisco} ]  country [ \textbf{Mexico} ]\hspace{.8cm}}
                \emph{\textbf{Bionico} contains \textbf{Granola} and is served as a \textbf{Dessert}.
                \textbf{Bionico} is a food found in the region of \textbf{Jalisco}, \textbf{Mexico}}
            \end{minipage}
        }
\end{subfigure}
    
    \caption{Three random linearized plans for the same input graph, and their text realizations. All taken from the top 10\% scoring plans. (a) structures the output as a single sentence, while (b) and (c) as two sentences. The second sentence in (b) puts emphasis on Bionico being a dessert, while in (c) the emphasis is on the ingredients.}
    \label{fig:random-linearized}
\end{figure*}
 
\paragraph{Possible Alternatives} In addition to the single plan selection, the explicit planning stage opens up additional possibilities. Instead of choosing and realizing a single plan, we can realize \textbf{a diverse set} of high-scoring plans, or realizing a random high-scoring plan, resulting in a diverse and less templatic set of texts across runs. 
This relies on the combination of two factors: the ability of the scoring component to select plans that correspond to plausible human-authored texts, and the ability of the neural realizer to faithfully realize the plan into fluent text. While it is challenging to directly evaluate the plans adequacy, we later show an evaluation of the plan realization component. Figure \ref{fig:random-linearized} shows three random plans for the same graph and their realizations. Further examples of the diversity of generation are given in the appendix. 

The explicit and symbolic planning stage also allows for \textbf{user control} over the generated text, either by supplying constraints on the possible plans (e.g., number of sentences, entities to focus on, the order of entities/relations, or others) or by supplying complete plans. We leave these options for future work.


\section{Plan Realization}\label{sec:plan-neural} 

For plan realization, we use an off-the-shelf vanilla neural machine translation (NMT) system to translate plans to texts.
The explicit division to sentences in the text plan allows us to realize each sentence plan individually which allows the realizer to follow the plan structure within each (rather short) sentence, reducing the amount of information that the model needs to remember. As a result, we expect a significant reduction in over- and under-generation of facts, which are common when generating longer texts. Currently, this comes at the expense of not modeling discourse structure (i.e., referring expressions). This deficiency may be handled by integrating the discourse into the text plan, or as a post-processing step.\footnote{Minimally, each entity occurrence can keep track of the number of times it was already mentioned in the plan. Other alternatives include using a full-fledged referring expression generation system such as NeuralREG \cite{ferreira2018neuralreg}}. We leave this for future work.

To use text plans as inputs to the NMT, we linearize each sentence plan by performing a pre-order traversal of the tree, while indicating the tree structure with brackets (Figure \ref{fig:graph-linearized}). The directed relations  are expressed as a sequence of two or more tokens, the first indicating the direction and the rest expressing the relation.\footnote{We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase.}  
Entities that are identified in the reference text are replaced with single, entity-unique tokens. This allows the NMT system to copy such entities from the input rather than generating them.
Figure \ref{fig:graph-text} is an example of possible text resulting from such linearization.

\paragraph{Training details} 
We use a standard NMT setup with a copy-attention mechanism \cite{gulcehre2016pointing}\footnote{Concretely, we use the OpenNMT toolkit \cite{klein2017opennmt} with the \texttt{copy\_attn} flag. Exact parameter values are detailed in the appendix.}
and the pre-trained GloVe.6B word embeddings\footnote{\url{nlp.stanford.edu/data/glove.6B.zip}} \cite{pennington2014glove}.  The pre-trained embeddings are used to initialize the relation tokens in the plans, as well as the tokens in the reference texts. 

\paragraph{Generation details} 
We translate each sentence plan individually. Once the text is generated, we replace the entity tokens with the full entity string as it appears in the input graph, and lexicalize all dates as \emph{Month DAY+ordinal, YEAR} (i.e., \emph{July 4th, 1776}) and for numbers with units (i.e., \emph{``5''(minutes)}) we remove the parenthesis and quotation marks (\emph{5 minutes}). 




\section{Experimental Setup}
The WebNLG challenge \cite{colin2016webnlg} consists of mapping sets of RDF triplets to text including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation. It contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: \emph{seen}, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and \emph{unseen}, containing inputs extracted for entities and relations belonging to 5 unseen categories. While the unseen category is conceptually appealing, we view the seen category as the more relevant setup: generating fluent, adequate and diverse text for a mix of known relation types is enough of a challenge also without requiring the system to invent verbalizations for unknown relation types. Any realistic generation system could afford to provide at least a few verbalizations for each relation of interest. We thus focus our attention mostly on the seen case (though our system does also perform well on the unseen case).

Following Section \ref{sec:plan-train}, we manage to match a consistent plan for  of the reference texts and use these plan-text pairs to train the plan realization NMT component. Overall, the WebNLG training set contains  RDF-text pairs while our plan-enhanced corpus contains  plan-text pairs.\footnote{Note that this only affects the training stage. At test time, we do not require gold plans, and evaluate on all sentences.}

\paragraph{Compared Systems}
We compare to the best submissions in the WebNLG challenge \cite{gardent2017webnlg}: Melbourne, an end-to-end system that scored best on all categories in the automatic evaluation, and UPF-FORGe \cite{mille2017forge}, a classic grammar-based NLG system that scored best in the human evaluation.

Additionally, we developed an end-to-end neural baseline which outperforms the WebNLG neural systems. It uses a set encoder, an LSTM \cite{hochreiter1997long} decoder with attention \cite{attention}, a copy-attention mechanism \cite{gulcehre2016pointing} and a neural checklist model \cite{kiddon2016globally}, as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. 
We refer to this system as \textbf{\ourbaseline}.

\section{Experiments and Results}\label{sec:results}

\subsection{Automatic Metrics}
We begin by comparing our plan-based system (\textbf{\ourplans}) to the state-of-the-art using the common automatic metrics:
 BLEU \cite{papineni2002bleu}, Meteor \cite{banerjee2005meteor}, ROUGE \cite{lin2004rouge} and CIDEr \cite{vedantam2015cider}, using the \hyperlink{https://github.com/Maluuba/nlg-eval}{nlg-eval}\footnote{\url{https://github.com/Maluuba/nlg-eval}} tool \cite{sharma2017nlgeval} on the entire test set and on each part separately (seen and unseen).

In the original challenge, the best performing system in automatic metric was based on end-to-end NMT (\textbf{Melbourne}). Both the \textbf{\ourbaseline} and \textbf{\ourplans} systems outperform all the WebNLG participating systems on all automatic metrics (Table \ref{table:scores-all}).
\textbf{\ourplans} is competitive with \textbf{\ourbaseline} in all metrics, with small differences either way per metric.\footnote{At least part of the stronger results for \textbf{StrongNeural} can be attributed to its ability to generate referring expressions, which we currently do not support.}


\begin{table}[h]
        \centering
        \scalebox{0.7}{\begin{tabular}{|l|c|c|c|c|}
        \hline& \textbf{BLEU} & \textbf{METEOR} & \textbf{ROUGE\textsubscript{L}} & \textbf{CIDEr} \\ \hline
{\color{blue} UPF-FORGe} & 38.5 & 0.390 & 60.9 & 2.500\\ \hline
{\color{red} Melbourne} & 45.0 & 0.376 & 63.5 & 2.814\\ \specialrule{.1em}{.05em}{.05em} 
{\color{purple} \ourrandom{1}} & 43.3 & 0.384 & 57.6 & 2.342\\ \hline
{\color{purple} \ourrandom{2}} & 43.5 & 0.384 & 57.4 & 2.332\\ \hline
{\color{purple} \ourrandom{3}} & 43.5 & 0.384 & 57.4 & 2.303\\ \hline
{\color{red} \ourbaseline} & 46.5 & \textbf{0.392} & \textbf{65.4} & \textbf{2.866}\\ \hline
{\color{purple} \ourplans} & \textbf{47.4} & 0.391 & 63.1 & 2.692\\ \hline
\end{tabular}}
        \caption{Results for all categories. Team color indicates the type of system used 
        ({\color{red}NMT}, {\color{blue}Rule-Based}, {\color{purple}Rule-Based + NMT}). }
                \label{table:scores-all}

        \end{table} 
\subsection{Manual Evaluation}
Next, we turn to manually evaluate our system's performance regarding faithfulness to the input on the one hand and fluency on the other. 
We describe here the main points of the manual evaluation setup, with finer details in the appendix.

\begin{figure*}[t]
	\centering
	\begin{subfigure}{.9\textwidth}
    	\fbox{\begin{subfigure}{.5\textwidth}
    		\begin{footnotesize}
            1. William\_Anders \textbar\ dateOfRetirement \textbar\ "1969-09-01" \\
            2. William\_Anders \textbar\ was selected by NASA \textbar\ 1963 \\
            3. William\_Anders \textbar\ timeInSpace \textbar\ "8820.0"(minutes) \\
            4. William\_Anders \textbar\ birthDate \textbar\ "1933-10-17"
    		\end{footnotesize}
    	\end{subfigure}
    	\begin{subfigure}{.5\textwidth}
    		\begin{footnotesize}
            5. William\_Anders \textbar\ occupation \textbar\ Fighter\_pilot \\
            6. William\_Anders \textbar\ birthPlace \textbar\ British\_Hong\_Kong \\
            7. William\_Anders \textbar\ was a crew member of \textbar\ Apollo\_8
    		\end{footnotesize}
    	\end{subfigure}}
		\caption{The last RDF in the seen test-set}
		\label{fig:compare-rdf}
	\end{subfigure}
	\begin{subfigure}{.9\linewidth}
	    \vspace{0.3cm}
	    \fbox{\begin{subfigure}{\textwidth}
    		\begin{footnotesize}
            	\textbf{William Anders} was born on \textbf{October 17th, 1933} in \textbf{British Hong Kong}.\\
                \textbf{He} was selected by nasa in \textbf{1963} and became a crew member on the \textbf{Apollo 8} flight mission.\\
                \textbf{He} retired on \textbf{September 1st, 1969}.
    		\end{footnotesize}
    	\end{subfigure}}
		\caption{Output from \ourbaseline}
		\label{fig:compare-baseline}
	\end{subfigure}
	\begin{subfigure}{.9\linewidth}
	    \vspace{0.3cm}
		\fbox{\begin{subfigure}{\textwidth}
    		\begin{footnotesize}
            	\textbf{William Anders} was a \textbf{fighter pilot} who joined nasa in \textbf{1963} and served as a crew member of \textbf{Apollo 8}.\\
                \textbf{William Anders} retired on \textbf{September 1st, 1969} and spent \textbf{8820.0 minutes} in space.\\
                \textbf{William Anders} was born in \textbf{British Hong Kong} on october \textbf{October 17th, 1933}.
    		\end{footnotesize}
    	\end{subfigure}}
		\caption{Output from \ourplans}
		\label{fig:compare-plan}
	\end{subfigure}\caption{Comparing end-to-end neural generation with our plan based system.}
	\label{fig:compare}
\end{figure*} 
\paragraph{Faithfulness} 
As explained in Section \ref{sec:plan-structure}, the first benefit we expect of our plan-based architecture is to make the neural systemâ€™s task simpler, helping it to remain faithful to the semantics expressed in the plan which in turn is guaranteed to be faithful to the original RDF input (by faithfulness, we mean expressing all facts in the graph and only facts from the graph: not dropping, repeating or hallucinating facts).  We conduct a manual evaluation over the seen portion of the WebNLG human evaluated test set (139 input sets). We compare \textbf{\ourplans} and \textbf{\ourbaseline}.\footnote{We do not evaluate \textbf{UPF-FORGe} as it is a verifiable grammar-based system that is fully faithful by design.} For each output text, we manually mark which relations are expressed in it, which are omitted, and which relations exist with the wrong lexicalization. We also count the number of relations the system over generated, either repeating facts or inventing new facts.\footnote{This evaluation was conducted by the first author, on a set of shuffled examples from the \textbf{\ourplans} and \textbf{\ourbaseline} systems, without knowing which outputs belongs to which system. We further note that evaluating for faithfulness requires careful attention to detail (making it less suitable for crowd-workers), but has a precise task definition which does not involve subjective judgment, making it possible to annotate without annotator biases influencing the results. We release our judgments for this stage together with the code.}


Table \ref{table:expert} shows the results. \textbf{\ourplans} reduces all error types compared to \textbf{\ourbaseline}, by ,  and  respectively.
While on-par regarding automatic metrics, \textbf{\ourplans} substantially outperforms the new state-of-the-art end-to-end neural system in semantic faithfulness. 

For example, Figure \ref{fig:compare} compares the output of \textbf{\ourbaseline} (\ref{fig:compare-baseline}) and \textbf{\ourplans} (\ref{fig:compare-plan}) on the last input in the seen test set (\ref{fig:compare-baseline}).
While both systems chose three sentences split and aggregated details about birth in one sentence and details about the occupation in another, \textbf{\ourbaseline} also expressed the information in chronological order. However,  \textbf{\ourbaseline} failed to generate facts 3 and 5. \textbf{\ourplans} made a lexicalization mistake in the third sentence by expressing ``October'' before the actual date, which is probably caused by faulty entity matching for one of the references, and (by design) did not generate any referring expression, which we leave for future work.
\begin{table}[h]
\resizebox{\linewidth}{!}
{
\begin{tabular}{|l|l|l|}
\hline
                     & \textbf{\ourplans} & \textbf{\ourbaseline} \\ \hline
Expressed            & 417                & 360                   \\ \specialrule{.1em}{.05em}{.05em} 
Omitted              & 6                  & 41                    \\ \hline
Wrong-lexicalization & 17                 & 39                    \\ \hline
Over-generation      & 3                  & 29                    \\ \hline
\end{tabular}
}
\caption{Semantic faithfulness of each system regarding 440 RDF triplets from 139 input sets in the seen part of the manually evaluated test set.}

\label{table:expert}
\end{table} 
\paragraph{Fluency} 
\begin{table}[t]
\resizebox{\linewidth}{!}
{
\begin{tabular}{|l|l|l|l|}
\hline
            & \textbf{\ourbaseline} & \textbf{Reference} & \textbf{UPF-FORGe} \\ \hline
\textbf{\ourplans}   & -0.6\%      & -5.4\% &  +5.1\%   \\ \hline
\end{tabular}
}
\caption{MTurk average worker score for \emph{\ourplans} compared to each system. 
It is a worse than the reference texts, on-par with the neural end-to-end system, and a better than the previous state-of-the-art.}
\label{table:mturk}
\end{table} Next, we assess whether our systems succeed at maintaining the high-quality fluency of the neural systems.
We perform pairwise evaluation via Amazon Mechanical Turk wherein each task the worker is presented with an RDF set (both in a graph form, and textually), and two texts in random order, one from \textbf{\ourplans}, the other from a competing system. 
We compare \textbf{\ourplans} against a strong end-to-end neural system (\textbf{\ourbaseline}), a grammar-based system which is the state-of-the-art in human evaluation (\textbf{UPF-FORGe}), and the human-supplied WebNLG references (\textbf{Reference}).
The workers were presented with three possible answers: \textbf{\ourplans} text is better (scored as 1), the other text is better (scored as -1), and both texts are equally fluent (scored as 0).
Table \ref{table:mturk} shows the average worker score given to each pair divided by the number of texts compared. 
\textbf{\ourplans} performed on-par with \textbf{\ourbaseline}, and surpassed the previous state-of-the-art \textbf{UPF-FORGe}. It, however, scored worse than the reference texts, which is expected given that it does not produce referring expressions.
Our approach manages to keep the same fluency level typical to end-to-end neural systems, thanks to the NMT realization component.


\subsection{Plan Realization Consistency}
We test the extent to which the realizer generates texts that are consistent with the plans. For several subsets of ranked plans (best plan, top , and top ) for the seen and unseen test sets separately, 
we realize up to 100 randomly selected text-plans per input.
We realize each sentence plan and evaluate using two criteria: (1) Do all entities from the plan appear in the realization; (2) Like the consistency we defined above, do all entities appear in the same order in the plan and the realization.

Table \ref{table:coverage} indicates that for decreasingly probable plans our realizer does worse in the first criterion. However, for both parts of the test set,
if the realizer managed to express all of the entities, it expressed them in the requested order, meaning the outputs are consistent with plans.
This opens up a potential for user control and diverse outputs, by choosing different plans for realization.

\begin{table}[!t]
\centering
\resizebox{\linewidth}{!}
{\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
       & \multicolumn{2}{c|}{\textbf{Best Plan}} & \multicolumn{2}{c|}{\textbf{Top 1\% Plans}} & \multicolumn{2}{c|}{\textbf{Top 10\% Plans}}     \\ \hline
       & \textbf{Entities} & \textbf{Order} & \textbf{Entities} & \textbf{Order} & \textbf{Entities} & \textbf{Order} \\ \hline
Seen   & 98.9\%                  & 100\%                & 95.9\%                  & 99.9\%                & 93.6\%                  & 100\%                \\ \hline
Unseen & 66.7\%                  & 100\%                & 45.3\%                  & 100\%                & 41.3\%                  & 100\%                \\ \hline

\end{tabular}}
\centering

\caption{Surface realizer performance. \emph{Entities:} Percent of sentence plans that were realized with all the requested entities. \emph{Order}: of the sentences that were realized with all requested entities, percentage of realizations that followed the requested entity order.}
\label{table:coverage}
\end{table}





%
 
Finally, we verify that the realization of potentially diverse plans is not only consistent with each given plan but also preserves output quality.
For each input, we realize a random plan from the top . We repeat this process three times with different random seeds to generate different outputs, and mark these systems as \textbf{\ourrandom{1/2/3}}. 
Table \ref{table:scores-all} shows that these random plans maintain decent quality on the automatic metrics, with a limited performance drop, and the automatic score is stable across random seeds.\footnote{While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure \ref{fig:random-linearized}.}  

\section{Related Work}\label{sec:related}
Text planning is a major component in classic NLG. For example, \citet{stent2004trainable} shows a method of producing coherent sentence plans by exhaustively generating as many as 20 sentence plan trees for each document plan, manually tagging them, and learning to rank them using the RankBoost algorithm \cite{schapire1999brief}. Our planning approach is similar, but we only have a set of ``good'' reference plans without internal ranks. While the sentence planning decides on the aggregation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. \citet{lapata2003probabilistic} devised a probabilistic model for sentence ordering which correlated well with human ordering. Our plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works \cite{barzilay2006aggregation, konstas2012unsupervised, konstas2013inducing}.

Many generation systems \cite{gardent2017webnlg, duvsek2018findings} are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process.

Generation from structured data often requires referring to a knowledge base \cite{mei2015talk, kiddon2016globally, wen2015semantically}.
This led to input-coverage tracking neural components such as the checklist model \cite{kiddon2016globally} and copy-mechanism \cite{gulcehre2016pointing}. Such methods are effective for ensuring coverage and reducing the number of over-generated facts and are in some ways orthogonal to our approach. While our explicit planning stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model.


More complex tasks, like RotoWire \cite{wiseman2017challenges} require modeling also document-level planning. \citet{puduppully2018data} explored a method to explicitly model document planning using the attention mechanism.

The neural text generation community has also recently been interested in ``controllable'' text generation \cite{hu2017toward}, where various aspects of the text (often sentiment) are manipulated \cite{ficler2017controlling} or transferred \cite{shen2017style, zhao2018adversarially, li2018delete}. In contrast, like in \cite{wiseman2018learning}, here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation.


\section{Conclusion}
We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation, it substantially outperforms the end-to-end system regarding faithfulness to the input. Additionally, the planning stage allows explicit user-control and generating diverse sentences, to be pursued in future work.


\clearpage

\appendix
\appendixpage
\addappheadtotoc
\section{Diverse Outputs}

We demonstrate the ability of the model to produce diverse outputs by showing examples of generation from graphs with 4, 5 or 6 edges. For each graph, we show every th plan, where  is chosen so that our 25 examples cover the top 10\% of the plans, and order them by the scores assigned to them by the scoring model (the score is shown to the right of each plan, as well as the rank in the list). Higher scoring plans correspond to more natural plans, according to our model, but all of them are viable options. Then, for each plan we show the corresponding text generated by the NMT model. This provides a glimpse of: (1) the quality of the scoring model; (2) the diversity of the plans; (3) the naturalness of the generation.

For the plans, color boxes indicate entities, and gray boxes around them indicate bracketing. Vertical bars indicate sentence splits.
For the generated text, each entity is underlines with the color corresponding to its box.







\begin{figure*}[!hb]
    \centering
    \includegraphics[width=\linewidth]{inc/diversity/size-4/graph.jpg}
    \caption{Example of a graph with 4 edges}
    \label{fig:diversity:size-4:graph}
\end{figure*}



\subsection{Example: Graph with 4 Edges}
Figure \ref{fig:diversity:size-4:graph} shows a random 4-edge graph from the seen part of the test set. Figure \ref{fig:diversity:size-4:plans} shows the plans and Figure \ref{fig:diversity:size-4:output} the corresponding texts.






\begin{sidewaysfigure*}
    \centering
    \includegraphics[width=\linewidth]{inc/diversity/size-4/plans.jpg}
    \caption{25 random linearized plans (out of 1,295 possible plans) for the graph in Figure \ref{fig:diversity:size-4:graph}, and their ranks and model scores.}
    \label{fig:diversity:size-4:plans}
\end{sidewaysfigure*}

\begin{sidewaysfigure*}
    \centering
    \includegraphics[width=0.95\linewidth]{inc/diversity/size-4/output.jpg}
    \caption{Realizations of the plans from Figure \ref{fig:diversity:size-4:plans} as produced by the NMT realizer.}
    \label{fig:diversity:size-4:output}
\end{sidewaysfigure*}
 \clearpage
\begin{figure*}[!hb]
    \centering
    \includegraphics[width=\linewidth]{inc/diversity/size-5/graph.jpg}
    \caption{Example of a graph with 5 edges}
    \label{fig:diversity:size-5:graph}
\end{figure*}


\subsection{Example: Graph with 5 Edges}
Figure \ref{fig:diversity:size-5:graph} shows a random 5-edge graph from the seen part of the test set. Figure \ref{fig:diversity:size-5:plans} shows the plans and Figure \ref{fig:diversity:size-5:output} the corresponding texts.








\begin{sidewaysfigure*}
    \centering
    \includegraphics[width=0.87\linewidth]{inc/diversity/size-5/plans.jpg}
    \caption{25 random linearized plans (out of 9,460 possible plans) for the graph in Figure \ref{fig:diversity:size-5:graph}, and their ranks and model scores.}
    \label{fig:diversity:size-5:plans}
\end{sidewaysfigure*}

\begin{sidewaysfigure*}
    \centering
    \includegraphics[width=\linewidth]{inc/diversity/size-5/output.jpg}
    \caption{Realizations of the plans from Figure \ref{fig:diversity:size-5:plans} as produced by the NMT realizer.}
    \label{fig:diversity:size-5:output}
\end{sidewaysfigure*}
 \clearpage


\begin{figure*}[!hb]
    \centering
    \includegraphics[width=\linewidth]{inc/diversity/size-6/graph.jpg}
    \caption{Example of a graph with 6 edges}
    \label{fig:diversity:size-6:graph}
\end{figure*}

\subsection{Example: Graph with 6 Edges}
Figure \ref{fig:diversity:size-6:graph} shows a random 6-edge graph from the seen part of the test set. Figure \ref{fig:diversity:size-6:plans} shows the plans and Figure \ref{fig:diversity:size-6:output} the corresponding texts.






\begin{sidewaysfigure*}
    \centering
    \includegraphics[width=\linewidth]{inc/diversity/size-6/plans.png}
    \caption{25 random linearized plans (out of 171,024 possible plans) for the graph in Figure \ref{fig:diversity:size-6:graph}, and their ranks and model scores.}
    \label{fig:diversity:size-6:plans}
\end{sidewaysfigure*}

\begin{sidewaysfigure*}
    \centering
    \includegraphics[width=\linewidth]{inc/diversity/size-6/output.png}
    \caption{Realizations of the plans from Figure \ref{fig:diversity:size-6:plans} as produced by the NMT realizer.}
    \label{fig:diversity:size-6:output}
\end{sidewaysfigure*}
 \clearpage

\section{Manual Evaluation Setup}\label{app:manual}
When performing pairwise system comparisons, we show the user, for each set of RDFs, the two texts produced by the compared systems in random order, along with the RDF triplets in textual and image forms as a reference. For consistency, both texts are normalized by lower-casing and splitting tokens on punctuation.
The same interface is used for turkers (for the fluency task) and local annotators (for the faithfulness task).

\subsection{Fluency Evaluation by Crowd}\label{app:manual:turk}
We evaluate on the RDF sets in the original WebNLG manual evaluation setup. The task is performed by mechanical-turk workers. The workers are presented with the question:

\textbf{``Which text reads more fluently?''} \\
which can be answered by either \emph{Text 1}, \emph{Text 2} or \emph{Both are equally good or bad}.

We paid $ per hit, employing three workers on each. For qualification, workers were required to have over 98\% hit approval rate, and over 1000 approved hits. 

\subsection{Faithfulness Evaluation by Expert}\label{app:manual:expert}
To obtain reliable fine-grained evaluation of semantic faithfulness, the first author annotated the system outputs of \textbf{\ourbaseline} and \textbf{\ourplans}.
    
For each text, we present all the RDF input triplets, and ask the annotator to choose for each triplet one of three options: (1) This triplet is \textit{expressed} in the text; (2) This triplet is not expressed in the text (\textit{ommitted}); (3) The text expresses a relation between the two entities that is different than the one specified for them in the RDF triplet (\textit{wrong lexicalization}). Also, for each text, we ask the annotator to count the number of facts that were wrongly \textit{over generated}, counting both repeated facts and hallucinated ones.

\section{Training Parameters}
For the realization model we use the OpenNMT toolkit \cite{klein2017opennmt} with pre-trained GloVe.6B word embeddings \cite{pennington2014glove}, downloaded from \url{http://nlp.stanford.edu/data/glove.6B.zip}. We used the default parameters (except for the \texttt{-copy\_attn} flag). This corresponds to the following values:
\begin{itemize}
    \item train\_steps = 40000
    \item save\_checkpoint\_steps = 2000
    \item batch\_size = 16
    \item word\_vec\_size = 300
    \item layers = 3
    \item copy\_attn
    \item position\_encoding
\end{itemize}

\clearpage

\bibliography{naaclhlt2019}
\bibliographystyle{acl_natbib}

\end{document}
