

\documentclass{article}
\usepackage{enumitem}
\usepackage{spconf,amsmath,graphicx,amssymb}
\usepackage{color}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\ms}{\mathbf{S}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\ie}{\textit{i.e.}, }

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\hypersetup{colorlinks=true}

\title{Generalized End-to-End Loss for Speaker Verification}
\name{Li Wan \qquad Quan Wang \qquad Alan Papir \qquad Ignacio Lopez Moreno\thanks{More information of this work can be found at: \url{https://google.github.io/speaker-id/publications/GE2E}}}

\address{Google Inc., USA\
  \label{eqn:centroid}
  \vc_k=\E_m [ \ve_{km} ]=\frac{1}{M}\sum_{m=1}^M \ve_{km}.

\label{eqn:similarity_old}
  s=w\cdot \cos(\ve_{j\sim}, \vc_{k})+b,

  L_{\mathrm{T}}(\ve_{j\sim}, \vc_k)=\delta(j,k)\Big(1- \sigma(s)\Big)+ \Big( 1-\delta(j,k) \Big) \sigma(s).

\label{eqn:lstm}
  \ve_{ji}=\frac{f(\vx_{ji};\vw)}{||f(\vx_{ji};\vw)||_2}.

\label{eqn:similarity}
  \ms_{ji,k}=w\cdot \cos(\ve_{ji},\vc_k) + b ,

		\label{eqn:softmax}
		L(\ve_{ji})=-\ms_{ji,j}+\log\sum_{k=1}^N\exp(\ms_{ji,k}).
	
		\label{eqn:contrast}
		L(\ve_{ji})=1-\sigma (\ms_{ji,j}) + \max_{\substack{1\leq k\leq N\\k\neq j}} \sigma (\ms_{ji,k}),
	
  \vc_{j}^{(-i)}&=&\frac{1}{M-1}\sum_{\substack{m=1\\m\neq i}}^{M}\ve_{jm},\label{eqn:centroid2} \\
  \ms_{ji,k} &=&
  \begin{cases}
    w\cdot \cos(\ve_{ji}, \vc_j^{(-i)})+b & \text{if} \quad k=j; \\
    w\cdot \cos(\ve_{ji}, \vc_k)+b & \text{otherwise}.
  \end{cases} \label{eqn:similarity2}

\label{eqn:ge2e}
  L_{G}(\vx;\vw)=L_{G}(\ms)=\sum_{j,i} L(\ve_{ji}).

  \label{eqn:num_updates}
  2\times \max\Big(\binom{M}{P}, (N-1)\binom{M}{P}\Big)\geq 2(N-1) .

  L(D_1,D_2;\vw)=\E_{x\in D_1}[L(\vx;\vw)] + \alpha \E_{x\in D_2}[L(\vx;\vw)] .

This is similar to the regularization technique:
in normal regularization, we use  to regularize the model. But
here, we use  for regularization.
When dataset  does not have sufficient data,
training the network on  can lead to overfitting. Requiring the network
to also perform reasonably well on  helps to regularize the network.

This can be generalized to combine  different, possibly extremely unbalanced,
data sources: . We assign a weight  to each data
source, indicating the importance of that data source.
During training, in each step we fetch one batch/tuple of
utterances from each data source, and compute the combined loss as:
,
where each  is the loss defined in Equation~\ref{eqn:ge2e}.


\section{Experiments}
\label{sec:exp}

In our experiments, the feature extraction process is the same as~\cite{prabhavalkar2015automatic}.
The audio signals are first transformed into frames of width 25ms and step 10ms.
Then we extract 40-dimension log-mel-filterbank energies as the features for
each frame. For TD-SV applications, the same features are used for both keyword
detection and speaker verification. The keyword detection system will only pass
the frames containing the keyword into the speaker verification system. These
frames form a fixed-length (usually 800ms) segment. For TI-SV applications, we
usually extract random fixed-length segments after Voice Activity Detection
(VAD), and use a sliding window approach for inference (discussed in Sec.~\ref{sec:tisid}) .

Our production system uses a 3-layer LSTM with projection~\cite{sak2014long}. The embedding vector
(d-vector) size is the same as the LSTM projection size. For TD-SV, we use 
hidden nodes and the projection size is . For TI-SV, we use  hidden
nodes with projection size . When training the GE2E model, each batch contains 
speakers and  utterances per speaker.
We train the network with SGD using initial learning rate , and decrease it by half every 30M steps.
The L2-norm of gradient is clipped at ~\cite{Pascanu2012}, and the gradient scale for projection
node in LSTM is set to . Regarding the scaling factor  in loss function,
we also observed that a good initial value is , and
the smaller gradient scale of  on them helped to smooth convergence.

\subsection{Text-Dependent Speaker Verification}
\label{sec:tdsid}
Though existing voice assistants usually only support a single keyword,
studies show that users prefer
that multiple keywords are supported at the same time. For multi-user on Google
Home, two keywords are supported simultaneously: ``OK Google" and ``Hey Google".

Enabling speaker verification on multiple keywords falls between TD-SV and TI-SV,
since the transcript is neither constrained to a single phrase, nor completely
unconstrained. We solve this problem using the MultiReader technique (Sec.~\ref{sec:multireader}).
MultiReader has a great advantage compared to simpler approaches,
e.g. directly mixing multiple data sources together:
It handles the case when different data sources are unbalanced in size.
In our case, we have two data sources for training:
1) An ``OK Google" training set from anonymized user queries with ~M utterances and ~K speakers;
2) A mixed ``OK/Hey Google" training set that is manually collected with ~M utterances and ~K speakers.
The first dataset is larger than the second by a factor of 125 in the number of utterances and 35 in the number of speakers.



For evaluation, we report the Equal Error Rate (EER) on four cases: enroll with either
keyword, and verify on either keyword. All evaluation datasets are manually collected from 665 speakers with an average of  enrollment utterances and  evaluation utterances per speaker.
The results are shown in Table \ref{tab:multireader}.
As we can see, MultiReader brings around 30\% relative improvement on all four
cases.

\begin{table}[t]
  \caption{MultiReader vs. directly mixing multiple data sources.}
  \label{tab:multireader}
  \begin{center}
  \scalebox{0.9}{
  \begin{tabular}{ l c c }
    \hline
    Test data & Mixed data & MultiReader \\
    (Enroll  Verify) & EER (\%) & EER (\%) \\ \hline \hline
    OK Google  OK Google & 1.16 & 0.82 \\
    OK Google  Hey Google & 4.47 & 2.99 \\
    Hey Google  OK Google & 3.30 & 2.30 \\
    Hey Google  Hey Google & 1.69 & 1.15 \\
    \hline
  \end{tabular}
  }
  \end{center}
\end{table}

We also performed more comprehensive evaluations in a larger dataset collected from ~K different speakers and environmental conditions, from both anonymized logs and manual collections. We use an average of  enrollment utterances and  evaluation utterances per speaker.
Table~\ref{tab:tdsv} summarizes average EER for different
loss functions trained with and without MultiReader setup. The baseline model is
a single layer LSTM with  nodes and an embedding vector size of
~\cite{heigold2016end}.
The second and third rows' model architecture is 3-layer LSTM.
Comparing the 2nd and 3rd rows, we see that GE2E is about  better than TE2E.
Similar to Table \ref{tab:multireader}, here we also see that the model performs
significantly better with MultiReader. While not shown in the table, it is also
worth noting that the GE2E model took about  less training time than TE2E.

\begin{table}[t]
  \caption{Text-dependent speaker verification EER.}
  \label{tab:tdsv}
  \begin{center}
  \scalebox{0.9}{
  \begin{tabular}{ l l l  c c}
    \hline
    Model         & Embed& Loss & Multi & Average \\
    Architecture  & Size &      & Reader & EER (\%) \\
    \hline \hline
     \cite{heigold2016end} &  & TE2E & No  & 3.30 \\
                                  &        &           & Yes & 2.78 \\
    \hline
     &  & TE2E & No  & 3.55 \\
                       &     &  & Yes & 2.67 \\
    \hline
     &  & GE2E & No  & 3.10\\
                       &     &  & Yes & 2.38 \\
    \hline
  \end{tabular}
  }
  \end{center}
\end{table}

\subsection{Text-Independent Speaker Verification}
\label{sec:tisid}

For TI-SV training, we divide training utterances into smaller segments, which we
refer to as partial utterances. While we don't require all partial
utterances to be of the same length, all partial utterances in the same batch
must be of the same length. Thus, for each batch of data,
we randomly choose a time length  within  frames,
and enforce that all partial utterances in that batch are of length 
(as shown in Figure~\ref{fig:TI_train}).

During inference time, for every utterance we apply a sliding window of fixed size
 frames with  overlap. We compute the d-vector for each window.
The final utterance-wise d-vector is generated by L2 normalizing the window-wise d-vectors,
then taking the element-wise averge (as shown in Figure~\ref{fig:TI_inference}).

Our TI-SV models are trained on around 36M utterances from 18K speakers, which are extracted from anonymized logs. For evaluation, we use an additional 1000 speakers with in average 6.3 enrollment utterances and 7.2 evaluation utterances per speaker.
Table~\ref{tab:tisv} shows the performance comparison between different training
loss functions.
The first column is a softmax that predicts the speaker label for all speakers in the training data.
The second column is a model trained with TE2E loss.
The third column is a model trained with GE2E loss.
As shown in the table, GE2E performs better than both softmax and TE2E.
The EER performance improvement is larger than  .
In addition, we also observed
that GE2E training was about  faster than the other loss functions.

\begin{figure}[t]
  \centering
    \includegraphics[width=0.4\textwidth]{figures/ti_train}
  \caption{Batch construction process for training TI-SV models.}
  \label{fig:TI_train}
\end{figure}
\begin{figure}[t]
  \centering
    \includegraphics[width=0.45\textwidth]{figures/TI_inference}
  \caption{Sliding window used for TI-SV.}
  \label{fig:TI_inference}
\end{figure}

\begin{table}[th]
  \caption{
    Text-independent speaker verification EER (\%).
  }
  \label{tab:tisv}
  \begin{center}
  \scalebox{0.9}{
  \begin{tabular}{ l c c c}
    \hline
    Softmax & TE2E~\cite{heigold2016end} & GE2E \\
    \hline \hline
    4.06 & 4.13 & 3.55 \\
    \hline
  \end{tabular}
  }
  \end{center}
\end{table}


\section{Conclusions}
\label{sec:conclusions}
In this paper, we proposed the generalized end-to-end (GE2E) loss function to train
speaker verification models more efficiently. Both theoretical and experimental results
verified the advantage of this novel loss function.
We also introduced the MultiReader technique to combine different data sources, enabling
our models to support multiple keywords and multiple languages. By combining
these two techniques, we produced more accurate speaker verification models.




\newpage
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}