\section{Experiments}
\subsection{Experimental setup}
\paragraph{Model notation} For the backbone of each view, we consider four ViT variants, ``Tiny'', ``Small'', ``Base'', and ``Large''. Their settings strictly follow the ones defined in BERT~\cite{devlin_naacl_2019} and ViT~\cite{dosovitskiy2020image}, \ie number of transformer layers, number of attention heads, hidden dimensions.
For convenience, each model variant is denoted with the following abbreviations indicating the backbone size, tubelet length, and input modality.
For example, B/2:R+S/4:S+Ti/8:F denotes a three-view model, where a ``Base'', ``Small'', and ``Tiny'' encoders are used to processes tokens from RGB tubelets of sizes , spectrogram tubelets of sizes , and optical flow tubelets of sizes , respectively.
Note that we omit 16 in our model abbreviations because all our models use  as the spatial tubelet size following ViT~\cite{dosovitskiy2020image}. If we omit the modality in the notation, we assume all views use RGB frames as the modality.
All model variants use the same global encoder which follows the ``Base'' architecture, except that the number of heads is set to 8 instead of 12. The reason is that the hidden dimension of the tokens should be divisible by the number of heads for multi-head attention, and the number of hidden dimensions across all standard transformer architectures (from ``Tiny'' to ``Large''~\cite{steiner2021augreg, dosovitskiy2020image}) is divisible by 8.

\paragraph{Optical flow and spectrogram extraction} 
We compute optical flow using the FlowNet~\cite{dosovitskiy2015flownet} algorithm. Audio spectrograms are extracted in a similar manner to~\cite{hershey2017cnn}. All audio is converted to monochannel and resampled to 16kHz. Spectrograms are then extracted using short-term Fourier transforms with a Hann window of 25ms with 15ms hop. The resulting spectrogram is integrated into
64 mel-spaced frequency bins (lower cutoff 125 Hz and upper corner frequency 7500 Hz) and the squared magnitude is extracted. This gives us mel spectrograms of 96  64 bins for 0.96 seconds of audio. For the entire clip, we run the above procedure in a sliding window fashion with a temporal hop of 40ms to align with RGB frame rate (25FPS). Spectrograms are normalized to [-1, 1] before feeding into the model.

\paragraph{Initialization} We trained two RGB-only models B/2+S/4+Ti/8 and L/2+B/4+S/8+Ti/16 on WTS~\cite{stroud2020learning} and use them to initialize multimodal models. Optical flow images have two input channels and spectrogram images only have one so the initial tubelet embedding layer has a different shape than the pretrained RGB models.
To address this issue, we simply average the kernel of the embedding layer along the input channel axis and perform tiling.

\paragraph{Training and inference}
\begin{table} 
	\centering
	\scriptsize{
\begin{tabular}{  l  c }
    \toprule
    \multicolumn{2}{l}{\textit{Data augmentation}}  \\
    Random crop probability & 1.0 \\
    Random flip probability & 0.5\\
    Scale jitter probability	& 1.0 \\
    Maximum scale			  & 1.33 \\
    Minimum scale			  & 0.9 \\
    Colour jitter probability  & 0.8 \\
    Rand augment number of layers~\cite{cubuk_arxiv_2019}		 & 3 \\
    Rand augment magnitude~\cite{cubuk_arxiv_2019} & 10  \\
    \midrule
    \multicolumn{2}{l}{\textit{Regularisation}} \\
    Stochastic droplayer rate~\cite{huang_stochasticdepth_eccv_2016} & 0.1 \\
    Label smoothing~\cite{szegedy_cvpr_2016} & 0.1 \\
    \bottomrule
   \end{tabular}
   }
	\caption{Data augmentation and regularization parameters.}
\label{tab:hparams}
\end{table}
 All models are trained on 64 frames with a temporal stride of 1. In Epic-Kitchens, each video is labeled with a ``verb'' and a ``noun''. We predict both categories using a single network with two ``heads''.
We train all our models for 50 epochs with a global batch size of 128 using synchronous SGD with momentum of 0.9 following a cosine learning rate schedule with a linear warm up. The initial learning rates for all models are set to 0.4. We follow~\cite{arnab2021vivit,yan2022multiview,dehghani2021scenic} and apply the same data augmentation and regularization schemes~\cite{huang_stochasticdepth_eccv_2016,cubuk_arxiv_2019,szegedy_cvpr_2016}, which were used by~\cite{touvron2021training} to train vision transformers more effectively. For spectrograms we use SpecAugment~\cite{park2019specaugment} with a max time mask length of 96 frames and max frequency mask length of 16 bins following MBT~\cite{nagrani2021attention}. \ARSHA{Xuehan check these hyperparams}\XXMAN{Changed 48 to 16 and 192 to 96.} See Table~\ref{tab:hparams} for detailed settings.
During single-model inference, we adopt the standard evaluation protocol by averaging over four temporal crops. To produce the final predictions from the model ensemble, we simply average the logits produced by each model.

\subsection{Ablation study} 
\label{sec:ablations}
We use a RGB-only model B/2+S/4+Ti/8 for the studies in Table~\ref{tab:pretraining} and ~\ref{tab:resolution}. We report Top-1 accuracies on Action, Noun, and Verb classes obtained from averaging predictions across four temporal crops. All numbers reported in this section are from the validation set.

\paragraph{Effects of pretraining}
Table~\ref{tab:pretraining} presents the finetuning results from models pretrained on Kinetics 400~\cite{kay_arxiv_2017}, Kinetics 700~\cite{kay_arxiv_2017}, and WTS~\cite{stroud2020learning} datasets. Kinetics 400 and 700 consist of 230,000 and 530,000 10s video clips focusing on human actions with each clip labeled with one of the 400 and 700 classes, respectively. WTS contains 60M videos with only video-level labels. All three pretraining datasets are from a different domain than Epic-Kitchens that is composed of egocentric videos. Table~\ref{tab:pretraining} shows that it is more beneficial to pretrain on a large-scale weakly supervised dataset than on a smaller set of trimmed video clips.
\begin{table} 
	\centering
	\scriptsize{
\begin{tabular}{  c  c  c  c }
    \toprule
    Pretraining datasets & Top-1 Action & Top-1 Noun & Top-1 Verb \\
    \midrule
    K400 & 46.7 & 60.5 & 67.8 \\
    K700 & 48.0 & 61.2 & 69.1 \\
    WTS & 49.3 & 63.0 & 69.4 \\
    \bottomrule
   \end{tabular}
   }
	\caption{Effects of different pretraining datasets. All models are trained and evaluated on  crops.}
\label{tab:pretraining}
\end{table}
 
\paragraph{Effects of input resolution}
As Table~\ref{tab:resolution} shown, as spatial resolution increases so does top-1 accuracy for nouns. Accuracies for verbs are also improved and this is likely due to the increased number of tokens that help the model better understand motion in the scene.

\begin{table} 
	\centering
	\scriptsize{
\begin{tabular}{  c  c  c  c }
    \toprule
    Spatial resolution & Top-1 Action & Top-1 Noun & Top-1 Verb \\
    \midrule
    224p & 49.3 & 63.0 & 69.4 \\
    280p & 50.5 & 63.9 & 69.9 \\
    432p & 52.7 & 66.1 & 71.2 \\
    \bottomrule
   \end{tabular}
   }
	\caption{Effects of increasing spatial resolution. All models are finetuned from a WTS-pretrained checkpoint.}
\label{tab:resolution}
\end{table}
 
\paragraph{Effects of combining different modalities}
\begin{table} 
	\centering
	\scriptsize{
\begin{tabular}{  l  c  c  c }
    \toprule
    Models & Top-1 Action & Top-1 Noun & Top-1 Verb \\
    \midrule
    B/2:R+S/4:R+Ti/8:R & 52.7 & 66.1 & 71.2 \\
    B/2:F+S/4:F+Ti/8:F & 40.5 & 50.1 & 68.1 \\
    \midrule
    B/2:R+S/4:F+Ti/8:R & 53.4 & \bf{66.5} & 71.9 \\
    B/2:R+S/4:S+Ti/8:R & 53.2 & 66.3 & \bf{72.0} \\
    \midrule
    B/2:R+S/4:S+Ti/8:F & \bf{53.6} & 66.3 & \bf{72.0} \\
    \bottomrule
   \end{tabular}
   }
	\caption{Effects of combining different modalities. All models are trained and evaluated on  crops.
	As an example of our naming convention, B/2:R+S/4:S+Ti/8:F denotes a three-view model, where a ``Base'', ``Small'', and ``Tiny'' encoders are used to processes tokens from RGB tubelets of sizes , spectrogram tubelets of sizes , and optical flow tubelets of sizes , respectively.
}
\label{tab:modalities}
\end{table}
 The first two rows in Table~\ref{tab:modalities} present the Top-1 accuracies of the RGB-only and the Flow-only models. Changing input modality of the ``Small'' encoder from RGB to flow and to spectrogram improves Top-1 accuracy on action from 52.7 to 53.4 and 53.2, respectively. Combining all three modalities gives the best performance on action with a score of 53.6. All models share similar FLOPs with the only difference being the initial embedding layers. RGB is the most informative modality for predicting ``nouns'', there is little gain by adding flow and audio. 
However, optical flow and audio provide complimentary information to RGB for predicting ``verbs''.
 
\subsection{Comparison to the state-of-the-art}
\label{sec:sota}
\begin{table} 
	\centering
	\scriptsize{
\begin{tabular}{c  l  c  c  c }
    \toprule
    Data split & Models & Top-1 Action & Top-1 Noun & Top-1 Verb \\
    \midrule
    \multirow{4}{*}{validation} & MoViNet~\cite{kondratyuk2021movinets} & 47.7 & 57.3 & \bf{72.2} \\
    & MeMViT~\cite{wu2022memvit} & 48.4 & 60.3 & 71.4 \\
    & Omnivore~\cite{girdhar2022omnivore} & 49.9 & 61.7 & 69.5 \\
    & \bf{M\&M-B} & \bf{53.6} & \bf{66.3} & 72.0 \\
    \midrule
    \multirow{2}{*}{test} & \gray{2021 winner \cite{Damen2021CHALLENGES}} & \gray{48.7} & \gray{59.2} & \gray{\bf{70.6}} \\
    & \bf{M\&M-B} & \bf{49.6} & \bf{63.7} & 68.0 \\
    \bottomrule
   \end{tabular}
   }
	\caption{Comparisons to state-of-the-art. M\&M-B refers to our three-view multimodal MTV model, B/2:R+S/4:S+Ti/8:F (no ensembling). The gray row is the winning entry from last year's challenge, which uses a 6-model ensemble. All other rows are from a single-model evaluation.}
\label{tab:sota}
\end{table}
 Table~\ref{tab:sota} compares our best single model to the previous state-of-the-art on the Epic-Kitchens dataset and last year's winning entry of the challenge. Our M\&M-B model improves over the previous state-of-the-art~\cite{girdhar2022omnivore} by a margin of 3.7\% in Top-1 action accuracy and also outperforms last year's winning method~\cite{Damen2021CHALLENGES}, which uses a 6-model ensemble.

\subsection{Model ensemble}
\label{sec:ensemble}
\begin{table*} 
	\centering
	\scriptsize{
\begin{tabular}{  l l  l c c  c c }
    \toprule
    Model indices & Model variants & Pretraining datasets & Resolution & Top-1 Action & Top-1 Noun & Top-1 Verb \\
    \midrule
    0 & B/2:R+S/4:R+Ti/8:F & WTS  K700 & 432p & 53.4 & 66.4 & 71.8 \\
    1 & B/2:R+S/4:F+Ti/8:R & WTS  K700 & 432p & 53.4 & 66.5 & 71.9 \\
    2 & L/2:R+B/4:F+S/8:F+Ti/16:R & WTS  K700 & 320p & 53.0 & 66.7 & 71.1 \\
    3 & L/2:R+B/4:R+S/8:R+Ti/16:R & WTS & 352p & 52.6 & 67.2 & 69.8 \\
    4 & B/2:F+S/4:F+Ti/8:F & WTS  K700 & 432p & 40.5 & 50.1 & 68.1 \\
    5 & B/2:R+S/4:R+Ti/8:R (1281) & WTS & 304p & 52.4 & 65.6 & 71.3 \\
    6 & L/2:F+B/4:F+S/8:F+Ti/16:F & WTS  K700 & 352p & 40.9 & 50.6 & 67.2 \\
    7 & L/2:R+B/4:F+S/8:S+Ti/16:R & WTS & 320p & 53.6 & 67.0 & 71.7 \\
    8 & B/2:R+S/4:S+Ti/8:F & WTS & 432p & 53.6 & 66.3 & 72.0 \\
    9 & B/2:R+S/4:S+Ti/8:R & WTS & 432p & 53.2 & 66.3 & 72.0 \\
    10 & B/2:R+S/4:R+Ti/8:S & WTS & 432p & 53.4 & 66.6 & 72.0 \\
    \bottomrule
   \end{tabular}
   }
	\caption{All model variants used in our final ensemble and their respective performance on the validation set. WTSK700 denotes a pretraining strategy where we first pretrain the model on WTS and then finetune on Kinetics 700. Model 5 is trained and evaluated on 128 frames instead of 64 for all other models.}
\label{tab:all_models}
\end{table*}
 \begin{table} 
	\centering
	\scriptsize{
\begin{tabular}{  l  c  c  c }
    \toprule
    Model indices & Top-1 Action (val/test) & Top-1 Noun & Top-1 Verb \\
    \midrule
    0,1,2,3,5,6,7,8,9,10 & \multirow{2}{*}{56.9/52.8} & 69.2/66.2 &  \\
    4,5,6,7,8,9,10 & &  & 75.0/70.9 \\
    \bottomrule
   \end{tabular}
   }
	\caption{Results from our final model ensemble on both validation/test sets. Different sets of models are used for predicting nouns and verbs.}
\label{tab:ensemble}
\end{table}
 To create the final submission, we generated two model ensembles one for predicting the verbs and the other for nouns. Table~\ref{tab:all_models} lists all individual models used in this challenge and their corresponding performance on the validation set. Table~\ref{tab:ensemble} shows which models we used for verbs and nouns. Using this model ensembling strategy, we improve the Top-1 action accuracy from 53.6 (from our single best model) to 56.9 on the validation set. Our final submission scored 52.8 on Epic-Kitchens test set, which is 4.1\% higher than last year's winning entry.

\ANURAG{Would our single model have beaten last year's entry, or this year's second place?}\XXMAN{Yes, I mentioned that in the SOTA section.}