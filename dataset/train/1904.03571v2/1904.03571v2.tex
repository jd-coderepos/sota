\section{EXPERIMENTS}
\label{sec:exp}


In this section, we apply our proposed DINet for saliency prediction and report its experimental results on several public saliency benchmark datasets. The effectiveness and efficiency of our model is validated qualitatively and quantitatively. 

\subsection{Saliency Benchmark Datasets} 
For evaluating the saliency prediction model, we adopt three popular saliency benchmark datasets with different image contents and experimental settings.

\subsubsection{SALICON \cite{jiang2015salicon}}
It contains 10,000 training images, 5,000 validation images, and 5,000 testing images, taken from the Microsoft COCO dataset \cite{lin2014microsoft}. The spatial resolution of each image in this dataset is $480 \times 640$. At present, it is the largest public dataset for visual saliency prediction. The ground-truths of training and validation datasets are available while the ground-truths of test dataset are held out. For evaluation on its test dataset, researchers need to submit their results on the SALICON challenge website\footnote{https://competitions.codalab.org/competitions/3791}. Besides, the evaluation protocols and codes are available in the website\footnote{https://github.com/NUS-VIP/salicon-evaluation}.


\subsubsection{MIT1003 \cite{judd2009learning}}
It contains 1,003 images collected from Flickr and LabelMe. The ground-truths for this dataset are created from eye-tracking data of 15 users. The evaluation codes for this dataset are available in the MIT Saliency Benchmark website\footnotemark.

\subsubsection{MIT300 \cite{Judd_2012}}
It contains 300 images, including both indoor and outdoor scenarios. The ground-truths for this entire dataset are held out. Researchers can only submit the results of their models to the MIT Saliency Benchmark website\footnotemark[\value{footnote}] for evaluation. Currently, the MIT1003 dataset is usually used as the training and validation sets for this dataset.

\footnotetext{http://saliency.mit.edu/}





\subsection{Evaluation Metrics for Saliency Prediction} 


There exists a large variety of metrics to measure the agreement between model predictions and human eye fixations.  Following existing works \cite{salMetrics_Bylinskii,riche2013saliency}, we conduct our quantitative experiments by adopting four widely used saliency evaluation metrics, including AUC, shuffled AUC (sAUC), Normalized Scanpath Saliency (NSS), and Linear Correlation Coefficient (CC). For the sake of simplification, we denote the predicted saliency map as P, the ground-truth saliency map as G, and the ground-truth fixation map as Q. The saliency evaluation metrics are listed in Table \ref{tab:evaluation} according to their characteristics.


\begin{table}[]
	\centering
	\scriptsize
	\caption{Saliency evaluation metrics}
	\label{tab:evaluation}
	\begin{tabular}{|l|c|c|}
		\hline
		Metrics                             & Category           & Ground-truth  \\ \hline \hline
		AUC (area under the ROC curve)      & Location-based     & Fixation Map (Q)\\  \hline
		sAUC (shuffled AUC)                 & Location-based     & Fixation Map (Q)\\ \hline
		Normalized Scanpath Saliency (NSS)  & Value-based        & Fixation Map (Q)\\ \hline
		Linear Correlation Coefficient (CC) & Distribution-based & Saliency Map (G) \\ \hline
	\end{tabular}
	\vspace{-4mm}
\end{table}

\subsubsection{AUC and sAUC}
AUC means the Area Under the ROC curve. This metric evaluates the binary classification performance of the predicted saliency map P, where fixation and non-fixation points in its corresponding Q are divided into the positive set and negative set, respectively. By using a threshold, P can be binary classified into the salient and non-salient regions. ROC curve will be obtained by varying this threshold from 0 to 1. Finally, the AUC metric can be calculated by using this ROC curve. Shuffled AUC (sAUC) is introduced to alleviate the influence of center-bias. Differ in AUC, the fixation points of other images in this dataset is used as the negative set in computing sAUC values. However, these two AUC-based metrics have the limitation in penalizing false positives, as reported in \cite{kruthiventi2017deepfix,liu2016deep,cornia2016predicting}.


\subsubsection{NSS}
Normalized Scanpath Saliency (NSS) is a specific value-based saliency evaluation metric. This metric is computed by taking the mean of $\bar{P}$ at the human eye fixations $Q$:

\begin{equation}
NSS = \frac{1}{N}\sum_{i=1}^N \bar{P}(i)\times Q(i),
\end{equation}
where $N$ is the total number of human eye fixations,  $\bar{P}$ is the unit normalized saliency map $P$. 

\subsubsection{CC}
The Linear Correlation Coefficient (CC) is a statistical metric for measuring the linear correlation between two random variables. For saliency prediction evaluation, the predicted saliency maps (P) and ground-truth density maps (G) are treated as two random variables. Then, CC is calculated by the following equation:
\begin{equation}
\label{eq:loss2}
CC = \frac{cov(P,G)}{\sigma(P)\times\sigma(G)},
\end{equation}
where $cov(\cdot,\cdot) $ and $\sigma(\cdot)$ refer to the covariance and standard deviation, respectively. 







\subsection{Implementation Details} 
Our proposed DINet is implemented by Keras with TensorFlow backend \cite{chollet2015keras,abadi2016tensorflow}. During training, the weights in Dilated ResNet-50 Network (DRN) are initialized from the ResNet-50 Network (without the last fully connected layer) which is pre-trained on the ImageNet. The weights of remaining layers are initialized by the default setting of Keras. The whole model is trained with widely used Adam optimizer \cite{kingma2014adam}. A mini-batch of 10 images is used in each iteration. The learning rate is set at $10^{-4}$ and scaled down by a factor of 0.1 after every two epochs. 


We train our model on the training set of SALICON \cite{jiang2015salicon} with 10,000 training images and use its validation datasets (5,000 validation images) to validate the model. For the MIT1003 dataset \cite{judd2009learning}, we directly use the model trained on the SALICON dataset to evaluate the generalization performance of our model on this dataset. For testing on the MIT300 dataset \cite{Judd_2012}, we fine-tune our model in the MIT1003 dataset with the same evaluation protocol in \cite{cornia2016predicting,liu2016deep}. The fine-tuned results of the MIT1003 dataset are also presented. For the latter two datasets, the input images are all resized to $320 \times 480$ with zero padding to keep the original content aspect ratio. This input image size is decided by our validation experiments on SALICON dataset.

It is worth mentioning that our model can achieve processing speed as little as 0.02s and 0.03s for one input image of size $240 \times 320$ and $320 \times 480$, respectively, by using one single GTX 1080 Ti GPU. 

\subsection{Loss Function Analysis} 
\label{lossfa}
We compare the performance of our baseline models trained by our proposed probability distribution distance metrics with linear normalization to those trained on standard regression loss functions and existing softmax normalization based statistical distances. 

Table \ref{table:losscompare} presents the experimental results for each loss function, as measured by the overall performance with respect to four aforementioned evaluation metrics on SALICON validation dataset. These results support that: (i) generally, the loss functions based on probability distribution distance metrics perform better than standard regression loss functions, such as BCE, $\ell1$-norm, and $\ell2$-norm in our experiments; (ii) for a specific statistical distance based loss function, our proposed linear normalization method is more compatible than the softmax normalization as it can measure the distance between the predicted probability distribution and its target in a more proper way; (iii) Using NSS loss function alone can obtain an extremely high NSS score while this loss function is not very good at other three evaluation metrics. 

The first two conclusions have been discussed in section \ref{sec:Loss}. The reason for (iii) can be illustrated by Table \ref{tab:evaluation}. NSS is a value-based saliency evaluation metric since it is computed by the average of the normalized saliency values at eye fixation locations. In other words, a saliency map with a higher NSS score is more like a fixation map which is not similar to the fixation density map, \ie saliency map. Conversely, another three evaluation metrics (CC, AUC, sAUC) prefer the latter one. Therefore, it is difficult to use one single loss function to train the DCNN model for obtaining a promising result on both NSS and other evaluation metrics. 





\begin{table}[]
	\centering
	\scriptsize
	\caption{Performance comparison of the baseline models with different loss functions on SALICON validation dataset \cite{jiang2015salicon}.}
	\label{table:losscompare}
	\begin{tabular}{|l|c c c c|}
		\hline
		Loss function                      & CC    & sAUC  & AUC   & NSS   \\ \hline \hline
		Total Variation distance (linear)  & \textbf{0.843} & 0.788 & 0.885 & 3.077 \\ \hline
		Total Variation distance (softmax) & 0.826 & 0.786 & \textbf{0.888} & 2.906 \\ \hline
		$\ell1$-norm                & 0.810  & 0.783 & 0.874 & 2.960  \\ \hline \hline
		Bhattacharyya distance (linear)    & 0.839 & 0.786 & 0.881 & 3.077 \\ \hline
		Bhattacharyya distance (softmax)   & 0.828 & 0.785 & 0.884 & 2.992 \\ \hline \hline
		KLD (linear)             & 0.842 & 0.788 & 0.886 & 3.070  \\ \hline
		KLD (softmax)            & 0.827 & 0.785 & 0.884 & 2.968 \\ \hline\hline
		$\chi^2$ divergence (linear)             & 0.826 & \textbf{0.790}  & 0.886 & 2.994 \\ \hline
		$\chi^2$ divergence (softmax)            & 0.826 & 0.786 & 0.883 & 2.968 \\ \hline \hline
		Cosine distance (linear)           & 0.835 & 0.789 & 0.885 & 3.048 \\ \hline
		Cosine distance (softmax)          & 0.828 & 0.786 & 0.887 & 2.999 \\ \hline \hline
		BCE               & 0.826 & 0.785 & 0.885 & 2.963 \\ \hline
		Euclidean ($\ell2$-norm)                          & 0.824 & 0.781 & 0.884 & 2.958 \\ \hline\hline
		NSS                                & 0.733 & 0.782 & 0.860  & \textbf{3.411} \\ \hline
	\end{tabular}
\end{table} 





\subsection{Model Visualization}
 We verify the effectiveness of DINet by individually visualizing the responses of each dilated convolutional branch. This visualization experiment is realized by adding an additional decoder network without non-linear activation at the end of our DIM. Both of this additional decoder and the original decoder are jointly trained with the same loss and the same inputs from the DIM. Since the additional decoder is a linear operator applied to input feature maps, the joint decoded output in this decoder can be decoupled into a linear combination of the outputs coming from individual branches. Moreover, the input dimension of our decoder is the same as the output dimension of every branch in our DIM (all are equal to 256). The responses of each branch can be easily obtained by feeding this additional decoder with the features learned in this specific branch. By visualizing both joint and individual saliency prediction results, we can analyze the contribution of these dilated convolutional branches in our DIM.








	Fig. \ref{fig:inc2} demonstrates the saliency prediction results of five validation images. The first three columns show the saliency maps independently predicted by branch-$\alpha$, -$\beta$ and -$\gamma$, and the fourth column shows the final saliency maps by sum-fusing the outputs produced by mentioned branches. All of these predicted saliency maps are generally consistent with the ground-truth. As demonstrated in the second and the third rows, branches with different receptive fields learn to focus on different parts of an input image. Specifically, the branch $\gamma$, \ie $b_\gamma$, with the largest dilation rate, learns the center-bias implicitly without any additional supervision. These learned center-bias patterns compensate the negligence on the center salient regions from other two branches, $b_\alpha$ and $b_\beta$, and produce a more accurate saliency prediction result. On the other hand, $b_\gamma$ sometimes generates false alarms in the center regions with low confidence. In this case, as shown in the last two rows of Fig. \ref{fig:inc2}, the previous two branches $b_\alpha$ and $b_\beta$ can help in reducing this unwilling side-effect on the final fusion results. These three branches in our DIM work in a collaborative manner. The results by using the features from a single branch are no need to be perfect for all possible cases. These incomplete predictions will be ensembled by the sum-fusion to become more comprehensive and reliable final results, which can be also supported by our ablation analysis in Table \ref{table:dimaa1}.






\begin{figure}[h]
	\centering
	\scriptsize
	\includegraphics[page=6,trim = 5mm 3mm 5mm 7mm, clip,width=1.0\linewidth]{newdrawing.pdf}
	\caption{
The influence of each dilated convolutional branch in the DIM to visual saliency. In each col, images are the saliency prediction results by using the features captured from the above indicated branch. GT: Ground Truth.
	}
	\label{fig:inc2}
\end{figure}


\subsection{Model Ablation Analysis} 
In this section, we conduct ablation analysis for our DINet on the SALICON validation dataset. The complete ablation results are presented in Table \ref{table:maa}. It should be noted that all of models in this table are trained by the proposed linear normalization-based total variation distance loss function.

\begin{table}[]
	\centering
	\scriptsize
	\caption{
		Model ablation analysis on SALICON validation dataset \cite{jiang2015salicon}.
		}
	\label{table:maa}
	\begin{tabular}{|l|cccc|}
		\hline
		Model                                & CC     & sAUC  & AUC   & NSS   \\ \hline\hline
		\multicolumn{5}{|c|}{Influence of backbone network}               \\ \hline
		ResNet + Decoder               & 0.776  & 0.762  & 0.879 & 2.456 \\ \hline
		Baseline (DRN + Decoder)       & 0.843  & 0.788 & 0.885 & 3.077 \\ \hline\hline
		\multicolumn{5}{|c|}{Influence of decoder network}               \\ \hline
		DRN + Decoder(1 conv layer)               & 0.838  & 0.785  & 0.883 & 3.052 \\ \hline
		DRN + Decoder(2 conv layers)               & 0.841  & 0.787 & 0.884 & 3.067\\ \hline
		DRN + Decoder(4 conv layers)               & 0.843  & 0.788  & 0.885 & 3.072 \\ \hline
		DRN + Decoder(1 deconv + 1 conv layers)       &0.841   & 0.787 & 0.884  & 3.064 \\ \hline			
		DRN + Decoder(2 deconv + 1 conv layers)       & 0.841  & 0.788 & 0.884 & 3.067 \\ \hline
		DRN + Decoder(3 deconv + 1 conv layers)       & 0.841  & 0.787  & 0.885 & 3.061 \\ \hline\hline						
		\multicolumn{5}{|c|}{Effectiveness of multi-scale features} \\ \hline
		ResNet + Skip-layer + Decoder & 0.841  & 0.786  & 0.885 & 3.053 \\ \hline 
		Baseline + IPN                       & 0.849   & 0.787 & 0.885 & 3.086 \\ \hline
		Baseline + Skip-layer                      & 0.847  & 0.788  & 0.886 & 3.084 \\ \hline
		Baseline + Inception(a)            & 0.850   & 0.788 & 0.886 & 3.094 \\ \hline
		Baseline + Inception(a) - 1$\times $1 branch      &0.849	&0.789	&0.886	&3.091 \\ \hline
		Baseline + Inception(b)             &0.852	&0.790	&0.886	&3.107 \\ \hline		
		Baseline + Inception(c)             & 0.852	&0.790	&0.886	&3.111 \\ \hline		
		Baseline + Inception(d)             & 0.854	&0.790	&0.887	&3.114 \\ \hline
		DINet (Baseline + Inception(e))  & 0.853  & 0.789 & 0.887 & 3.117 \\ \hline
		DRN + ASPP-S             & 0.853&	0.789&	0.887	&3.112 \\ \hline		
		DRN + ASPP-L            & 0.852&	0.789	&0.887	&3.102 \\ \hline \hline			
		\multicolumn{5}{|c|}{Influence of training image size}               \\ \hline
		DINet ($240 \times 320$)                      & 0.853  & 0.789 & 0.887 & 3.117 \\ \hline
		DINet ($320 \times 480$)                      & 0.858  & 0.790  & 0.887 & 3.143 \\ \hline
		DINet ($480 \times 640$)                      & 0.854  & 0.789 & 0.886 & 3.128 \\ \hline
		DINet    (ensemble)                  & \textbf{0.867}  & \textbf{0.792}  & \textbf{0.889} & \textbf{3.168} \\ \hline
	\end{tabular}
\vspace{-4mm}
\end{table}

\subsubsection{Influence of the backbone network} 
Our baseline model is built on DRN where the \emph{output\_stride} is equal to 8. As mentioned in Section \ref{sec:related}, the \emph{output\_stride} of original ResNet is 32 which means that less spatial information are included in the output of this backbone network and thus leads to the unsatisfactory performance. To verify this statement, we compare our baseline model (DRN + decoder) with a more basic model (ResNet + decoder). From the first part of Table \ref{table:maa}, we can conclude that \emph{output\_stride} is one of the key elements for the dense prediction tasks. There is a significant performance gain by replacing the original ResNet with DRN. 


\subsubsection{Influence of the decoder network} 
\label{sec:dec}
In our baseline model, our designed decoder network is just three convolutional layers plus sigmoid activation in the end. The reason for using three layers is determined by the experiments. We have tried to use different number of convolutional or deconvolutional layers before the prediction layer (one convolutional layer followed by a sigmoid activation) to form other decoder networks. Their results are reported in the second part of Table \ref{table:maa}. As we can see that the models with these decoders cannot get good results as our original decoder, \ie Decoder(3 conv layers).




\subsubsection{Effectiveness of multi-scale features} 

DINet uses the proposed DIM to capture multi-scale contextual features. To support the conclusions in \cite{huang2015salicon,kruthiventi2017deepfix,liu2016deep} that integrating multi-scale features can further improve saliency detection performance, we incorporate existing alternative multi-scale feature extraction modules, including IPN, skip-layer, inception and ASPP, into our baseline or backbone network. From the third part of Table \ref{table:maa}, we can observe that the saliency prediction performance indeed boosted by incorporating the multi-scale features. Especially, when the backbone network is not DRN, the multi-scale features can compensate the performance drop significantly, by comparing two models with the plain ResNet backbone network. In all these multi-scale saliency prediction framework, our proposed inception(d) and (e) obtain the optimal results among them. For the reason that inception(e) is more efficient in terms of \#parameters and inference time, as illustrated by Table \ref{inception1}, we pick this dilated inception module to form our DINet.


\subsubsection{Ablation analysis on DIM} 
 We further verify the effectiveness of our DIM by conducting two quantitative experiments. In the first experiment, we evaluate the performance of a trained DINet with two decoders mentioned in the visualization experiment to investigate the contribution of each dilated convolutional branch in our DIM respectively. In the second experiment, we make a comparison among a set of variants of DINet to explore the impact of the number of parallel dilated convolutional layers.





\begin{table}[]
	\centering
	\scriptsize
	\caption{
Dilated inception module ablation analysis within a trained DINet with two decoders on SALICON validation dataset \cite{jiang2015salicon}.
	}
	\label{table:dimaa1}
	\begin{tabular}{|l|ccc|cccc|}
		\hline
		Type                          &$b_\alpha$ & $b_\beta$ &$b_\gamma$ & CC    & sAUC  & AUC   & NSS   \\ \hline \hline
		\multicolumn{8}{|c|}{The results on additional decoder network}               \\ \hline
		0 branch                 &    &    &    &  0.752  & 0.794 & 0.858 & 2.729 \\ \hline
		\multirow{3}{*}{1 branch}     & \checkmark  &    &    & 0.833 & 0.799 & 0.882 & 3.012 \\ \cline{2-8} 
		&    & \checkmark  &    & 0.811  & 0.793 & 0.864 & 3.025 \\ \cline{2-8} 
		&    &    & \checkmark  & 0.801 & 0.759  & 0.873 & 2.967  \\ \hline
		\multirow{3}{*}{2 branches-sum} & \checkmark  & \checkmark  &    & 0.831 & 0.799 & 0.879  & 3.035 \\ \cline{2-8} 
		& \checkmark  &    & \checkmark  & 0.804 &0.799 & 0.869 & 3.036 \\ \cline{2-8} 
		&    & \checkmark  & \checkmark  & 0.813 &\textbf{0.800} & 0.872 & 3.032 \\ \hline
		3 branches-sum                 & \checkmark  & \checkmark  & \checkmark  & \textbf{0.853} & 0.789 & 0.886 & 3.098 \\ \hline \hline
		\multicolumn{8}{|c|}{The results on original decoder network}               \\ \hline
		3 branches-sum                 & \checkmark  & \checkmark  & \checkmark  & \textbf{0.853} & 0.789 & \textbf{0.887} & \textbf{3.107} \\ \hline		
	\end{tabular}
\vspace{-4mm}
\end{table}

\begin{table}[]
	\centering
	\scriptsize
	
	\caption{
Dilated inception module ablation analysis with individual trained variants of DINet on SALICON validation dataset \cite{jiang2015salicon}.
	} 
	\label{table:dimaa2}
	\begin{tabular}{|l|ccc|cccc|}
		\hline
		Type                            & $b_\alpha$ & $b_\beta$ &$b_\gamma$ & CC    & sAUC  & AUC   & NSS   \\ \hline
		0 branch                &    &    &    &  0.843  & 0.788 & 0.885 & 3.077 \\ \hline
		
		\multirow{3}{*}{1 branch}       & \checkmark   &    &    & 0.847 & \textbf{0.790}  & 0.886 & 3.080  \\ \cline{2-8} 
		&    & \checkmark   &    & 0.849 & 0.788 & \textbf{0.887} & 3.086 \\ \cline{2-8} 
		&    &    & \checkmark   & 0.851 & 0.788 & \textbf{0.887} & 3.095 \\ \hline
		\multirow{3}{*}{2 branches-sum} & \checkmark   & \checkmark   &    & 0.852 & 0.788 & \textbf{0.887} & 3.099 \\ \cline{2-8} 
		& \checkmark   &    & \checkmark   & 0.853 & 0.788 & 0.886 & 3.098 \\ \cline{2-8} 
		&    & \checkmark   & \checkmark   & 0.852 & 0.788 & \textbf{0.887} & 3.103 \\ \hline
		3 branches-sum                  & \checkmark   & \checkmark   & \checkmark   & 0.853 & 0.789 & \textbf{0.887} & \textbf{3.117} \\ \hline
		3 branches-concat               & \checkmark   & \checkmark   & \checkmark   & \textbf{0.854} & 0.789 & \textbf{0.887} & 3.116 \\ \hline 
\end{tabular}
\vspace{-4mm}
\end{table}



Table \ref{table:dimaa1} shows the results of the first experiment. Each row in this table represents the evaluation results by using the outputs from the indicated branch(es) as the input to a trained decoder. As we can see that, 1 branch type of DIM will learn different bias under its specific receptive fields to help in predicting visual saliency. Specifically, $b_\alpha$ prefers the results with higher sAUC score, while $b_\beta$ is more interested in the NSS metric. By comparing the results between the row of 3 branches-sum and the rows in 2 branches-sum type on the first part of this table, we can observe that the performance drop dramatically with the absence of any one branch, which means every branch in our DIM has its irreplaceable impact on the final results. These three branches in our DIM work in a collaborative manner. Even if the performance by using any individual branch is not comparable to the performance of our baseline model, their fused results can deal with the diverse images with different patterns of salient regions. Moreover, the results on the last row show that the features used in the additional decoder can still be decoded by our original decoder with only a little bit performance drop in the NSS metric. It can guarantee the generality of the above conclusions.


Table \ref{table:dimaa2} compares the performance of several variants of DINet.
 Each row in this table means the evaluation results by testing the individual trained variant which has the indicated branch(es). Especially, the model in 3 branches-sum type is the proposed DINet, while the model in 0 branch type is our baseline model. This table shows that using more branches (from 0 to 3), which means using more comprehensive features, will lead to a higher performance on evaluation metrics. Besides, in the 1 branch type of DINet, using dilated convolution with larger dilation rate before the decoder network can achieve a better performance than using a smaller one. It can be credited to the larger size of receptive fields which represent the longer range of dependencies in captured features. Moreover, using concatenation to replace our element-wise addition has a limited impact on the final results, as presented in the last two rows in this table. Mathematically, element-wise addition followed by a convolution layer is a special case of concatenation followed by another convolution layer \cite{chen2017dual}, which can be used to explain this limited difference on evaluation results. In summary, both of these two experiments can verify that the performance gain of our DIM is realized by the corporation of these three parallel dilated convolutional branches.







\subsubsection{Influence of training image size} 
The previous experimental results on SALICON validation dataset are all obtained from $240 \times 320$ images, whose size is the half resolution of the original SALICON images. Here we want to see the performance of our DINet models which are trained by images with different spatial resolution. From Table \ref{table:maa}, we find that the DINet trained by input images of size $320 \times 480$ can obtain the best performance among these three models. This model will be directly fine-tuned in the MIT1003 dataset for the evaluation of the MIT300 dataset. Note that these evaluation results are the average scores, there are some validation images which perform better in other DINets ($240 \times 320$ or $480 \times 640$). In order to characterize this phenomenon, we adopt a simple ensemble learning metric, i.e. average voting, to further improve the performance of our model. By using the average results from these three different models, this ensemble model obtain the best scores in our model ablation analysis.

\subsubsection{Ensemble learning for improving NSS} 
However, our best model, which is trained by a single total variation distance loss function, still cannot beat two state-of-the-art models \cite{cornia2016predicting,liu2016deep} in NSS metrics, as shown in Table \ref{table:lossen}. These two models use the NSS itself as one of the loss functions for training. To further improve our performance on NSS metrics, we use the same ensemble learning method as above to combine the results of two DINet models which are trained by using two different loss function (total variation distance with linear normalization and NSS) separately. The last ensemble model in this table is our final submission to the SALICON test dataset which results in a good comprise between NSS and another three evaluation metrics.





\begin{table}[]
	\centering
	\scriptsize
	\caption{Performance comparison of our DINet models with different loss functions on SALICON validation dataset \cite{jiang2015salicon}.}
	\label{table:lossen}
	\begin{tabular}{|l|cccc|}
		\hline
		Model                                & CC    & sAUC  & AUC   & NSS   \\ \hline 
		DINet (TV distance)                  & \textbf{0.867} & \textbf{0.792} & \textbf{0.889} & 3.168 \\ \hline 
		DSCLSTM     \cite{liu2016deep}                         & 0.835 & 0.788 & 0.887 & 3.221 \\ \hline
		SAM-ResNet  \cite{cornia2016predicting}                         & 0.844 & 0.787 & 0.886 & 3.260  \\ \hline 
		DINet (NSS)                           & 0.724 & 0.782 & 0.861 & \textbf{3.600}   \\ \hline 
		DINet (ensemble NSS and TV distance)) & 0.862 & \textbf{0.792} & 0.886 & 3.310  \\ \hline 
	\end{tabular}
\vspace{-4mm}
\end{table}


\subsection{Comparison with state-of-the-arts}

To demonstrate the effectiveness of our proposed DINet model in predicting visual saliency, we quantitatively compare our method with state-of-the-art models on SALICON, MIT1003, and MIT300 datasets. 



\begin{table}[]
	\scriptsize
	\centering
	\caption{Comparison results on the SALICON test dataset \cite{jiang2015salicon}.}

\label{salicontest}
	\begin{tabular}{|l|c c c c|}
		\hline
		Models & CC & sAUC & AUC & NSS \\ \hline  
		\textbf{DINet (Ours)} & \textbf{0.860} & 0.782 & \textbf{0.884} & \textbf{3.249} \\ \hline
		SAM-ResNet \cite{cornia2016predicting} & 0.842 & 0.779 & 0.883 & 3.204\\ \hline
		DSCLRCN \cite{liu2016deep} & 0.831 & 0.776 & \textbf{0.884} & 3.157 \\ \hline
		SAM-VGG \cite{cornia2016predicting} & 0.825 & 0.774 & 0.881 & 3.143 \\ \hline
		SalGAN \cite{pan2017salgan} & 0.781 & 0.772 & 0.781 & 2.459 \\ \hline
		SU \cite{kruthiventi2016saliency} & 0.780 & 0.760 & 0.880 & 2.610 \\ \hline
		PDP \cite{jetley2016end}& 0.765 & 0.781 & 0.882 & - \\ \hline
		ML-Net \cite{cornia2016deep} & 0.743 & 0.768 & 0.866 & 2.789 \\ \hline
		MxSalNet \cite{dodge2017visual} & 0.730 & 0.771 & 0.861 & 2.767 \\ \hline
		Deep Convnet \cite{pan2016shallow} & 0.622 & 0.724 & 0.858 & 1.859 \\ \hline
		Shallow Convnet \cite{pan2016shallow} & 0.562 & 0.658 & 0.821 & 1.663 \\ \hline
		DeepGazeII \cite{kummerer2016deepgaze} & 0.479 & \textbf{0.787} & 0.867 & 1.271 \\ \hline		
	\end{tabular}
	\vspace{-4mm}
\end{table}


Table \ref{salicontest} shows the evaluation results on the SALICON dataset. The results of other models come from their papers or the leaderboard of this dataset. In this table, the results in bold indicate the best performance method on each evaluation metric. As it can be observed, our DINet outperforms all competitors on CC, AUC, and NSS three metrics. The DeepGazeII \cite{kummerer2016deepgaze} model get the best sAUC score and relatively lower scores on other metrics. The saliency maps generated by this model actually are very blurred/hazy and visually different from the ground-truth, as shown in the left part of Fig.\ref{fig:salicon}. This is because AUC-based metrics mainly relied on true positives without significantly penalizing false positives \cite{kruthiventi2017deepfix,cornia2016predicting}. 







\begin{figure*}[h]


	\centering
\includegraphics[page=5,trim = 5mm 4mm 5mm 5mm, clip, width=1.0\linewidth]{newdrawing.pdf}
	\caption{Qualitative comparison results on two datasets. Left images are from SALICON validation dataset \cite{jiang2015salicon}, while right images are from MIT1003 dataset \cite{judd2009learning}. GT: Ground Truth.}
	\label{fig:salicon}
\end{figure*} 

 








\begin{table}[]
\centering
	\caption{Comparison results on the MIT1003 dataset \cite{judd2009learning}.}
	\label{MIT1003}
	\begin{tabular}{|l|c c c c|}
		\hline
		Model                                     & CC   & sAUC & AUC  & NSS  \\ \hline
		\textbf{DINet (w/o finetune)}                      & \textbf{0.67} & 0.70  &\textbf{0.88} &\textbf{2.40}  \\ \hline
		DVA \cite{wang2017deep}                                      & 0.64 & \textbf{0.77} & 0.87 & 2.38  \\ \hline
		GBVS    \cite{harel2007graph}                                  & 0.42 & 0.66 & 0.83 & 1.38  \\ \hline
		eDN    \cite{vig2014large}                                   & 0.41 & 0.66 & 0.85 & 1.29   \\ \hline		
		Mr-CNN \cite{liu2016learning}                                   & 0.38 & 0.73 & 0.80  & 1.36  \\ \hline
		BMS    \cite{zhang2013saliency}                                    & 0.36 & 0.69 & 0.79 & 1.25  \\ \hline
		ITTI   \cite{itti1998model}                                    & 0.33 & 0.66 & 0.77 & 1.10  \\ \hline
	\end{tabular}
	\vspace{-4mm}
\end{table}


\begin{table}[]
\centering
	\caption{Comparison results on the MIT1003 validation dataset \cite{judd2009learning}. }
	\label{MIT10031}
	\begin{tabular}{|l|c c c c|}
		\hline
		Model               & CC   & sAUC & AUC  & NSS   \\ \hline \hline
		\textbf{DINet (w finetune)}   & \textbf{0.87} & \textbf{0.77} & \textbf{0.91} & \textbf{3.27} \\ \hline
		SAM-ResNet  \cite{cornia2016predicting}               & 0.77 & 0.62 & \textbf{0.91} & 2.89   \\ \hline
		SAM-VGG     \cite{cornia2016predicting}           & 0.76 & 0.61 & \textbf{0.91} & 2.85   \\ \hline		
		DeepFix     \cite{kruthiventi2017deepfix}        & 0.72 & 0.74 & 0.90  & 2.58  \\ \hline	\hline	
		\textbf{DINet (w/o finetune)} & 0.67 & 0.72 & 0.89 & 2.50  \\ \hline
	\end{tabular}
	\vspace{-4mm}
\end{table}

\begin{table}[]
	\centering
	\caption{Comparison results on the MIT300 dataset \cite{Judd_2012}. }
	\label{mit300}
	\begin{tabular}{|l|c c c c |}
		\hline
		Model      & CC   & sAUC & AUC  & NSS   \\ \hline
		DSCLRCN \cite{liu2016deep}   & \textbf{0.80}  & 0.72 & 0.87 & \textbf{2.35}  \\ \hline
		\textbf{DINet (Ours)}      & 0.79 & 0.71 & 0.86 & 2.33  \\ \hline
		SAM-ResNet \cite{cornia2016predicting}   & 0.78 & 0.70  & 0.87 & 2.34  \\ \hline
		DeepFix \cite{kruthiventi2017deepfix}       & 0.78 & 0.71 & 0.87 & 2.26  \\ \hline
		SAM-VGG \cite{cornia2016predicting}      & 0.77 & 0.71 & 0.87 & 2.30  \\ \hline
		SALICON \cite{huang2015salicon}    & 0.74 & \textbf{0.74} & 0.87 & 2.12  \\ \hline
		SalGAN \cite{pan2017salgan}     & 0.73 & 0.72 & 0.86 & 2.04 \\ \hline
		PDP   \cite{jetley2016end}      & 0.70  & 0.73 & 0.85 & 2.05   \\ \hline
		DVA   \cite{wang2017deep}           & 0.68 & 0.71 & 0.85 & 1.98  \\ \hline
		ML-Net  \cite{cornia2016deep}    & 0.67 & 0.70  & 0.85 & 2.05  \\ \hline
				SalNet  \cite{pan2016shallow}    & 0.58 & 0.69 & 0.83 & 1.51 \\ \hline
		BMS     \cite{zhang2013saliency}    & 0.55 & 0.65 & 0.83 & 1.41 \\ \hline
		DeepGazeII  \cite{kummerer2016deepgaze} & 0.52 & 0.72 & \textbf{0.88} & 1.29 \\ \hline
		GBVS     \cite{harel2007graph}      & 0.48 & 0.63 & 0.81 & 1.24 \\ \hline
		Mr-CNN  \cite{liu2016learning}       & 0.48 & 0.69 & 0.79 & 1.37 \\ \hline
		eDN     \cite{vig2014large}     & 0.45 & 0.62 & 0.82 & 1.14 \\ \hline
		ITTI   \cite{itti1998model}      & 0.37 & 0.63 & 0.75 & 0.97  \\ \hline
	\end{tabular}
	\vspace{-4mm}
\end{table}

 \begin{table*}[]
	\centering
	\scriptsize
	\begin{threeparttable}
	\caption{
		comprehensive comparison with the state-of-the-arts.
		}
	\label{stoa}
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline
		Model                  & Backbone network                  & \#parameters              & key module                                                                 & Input image size  & inference time (per image) \\ \hline
		\multirow{3}{*}{DINet (Ours)} & \multirow{3}{*}{Dilated ResNet50} & \multirow{3}{*}{27.04M} & \multirowcell{3}{Dilated inception module for multi-scale} & $240 \times 320$           & 0.02s          \\ \cline{5-6} 
		&                                   &                         &                                                                             & $320 \times 480$        & 0.03s          \\ \cline{5-6} 
		&                                   &                         &                                                                             & $480 \times 640$           & 0.06s          \\ \hline
		\multirow{2}{*}{SAM$^a$  \cite{cornia2016predicting}}   & Dilated ResNet50                  & 70.09M                  & \multirow{2}{*}{Conv-LSTMs \cite{xingjian2015convolutional} for iterative refinement}                        & \multirow{2}{*}{$240 \times 320$}           & 0.09s        \\ \cline{2-3} \cline{6-6} 
		& Dilated VGG16                     & 51.84M                  &                                                                             &            & 0.07s         \\ \hline
		DSCLRCN$^b$   \cite{liu2016deep}              & Dilated ResNet50 + Places-CNN     & \textgreater{}33.71M    & Spatial LSTMs \cite{visin2015renet} for context incorporation                                        & $480 \times 640+227 \times 227$  & 0.27s         \\ \hline
		DVA$^c$   \cite{wang2017deep}                   & VGG16                             & 25.07M                  & Skip-layers for multi-scale                               & $224 \times 224$         & 0.02s          \\ \hline
	\end{tabular}
    \begin{tablenotes}
	\small
	\item 	$a$: The codes for these models are from the authors' github website. We test its inference time in our experimental environment.
	\item  $b$: This model has many customized operations which is hard to count their trainable parameters completely and reimplement in the Keras framework. 0.27s is adopted from their paper. It is certain that this model need more parameters and longer inference time than our method.
	\item  $c$: This model is reimplemented by ours and tested in our experimental environment. Its deconvolutions-based decoder network slow down the whole model and thus it has similar inference time as our model. 
\end{tablenotes}
\end{threeparttable}
	\vspace{-4mm}

\end{table*}

The results on MIT1003 are reported in Table \ref{MIT1003}. We directly use the DINet trained on the SALICON dataset to evaluate the generalization performance of our model on the whole MIT1003 dataset, as the DVA model \cite{wang2017deep}. Our model also achieves promising results on this dataset which verifies its robustness and generality. Qualitative comparison results of our model with other state-of-the-art saliency models on SALICON validation and MIT1003 datasets can be found in Fig.\ref{fig:salicon}. This figure can also support that our results match the ground-truth saliency maps best among all the compared models in both two datasets.

In order to evaluate the MIT300 dataset, we fine-tune our DINet on the MIT1003 dataset. The fine-tuned results are shown in Table \ref{MIT10031}, As we can see that, the performance of our model improves significantly after fine-tuning which can also outperform other existing fine-tuned models. The results on MIT300 dataset are presented in Table \ref{mit300}. Different in the previous two datasets, our DINet can not outperform the DSCLRCN model \cite{liu2016deep}. Our model may over-fitted on the MIT1003 dataset which leads to lower generalization performance on MIT300 dataset. Both DSCLRCN model and our DINet use multi-scale features to further improve saliency prediction performance. Besides, DSCLRCN model incorporates the global context and scene context by using spatial LSTM \cite{visin2015renet} method and additional Places-CNN \cite{zhou2014learning} backbone network to achieve this performance. Consequently, their model is more complex and much slower than our method. When testing one image with size $480 \times 640$, the DSCLRCN model needs 0.27s while our DINet needs only 0.06s.   


A comprehensive summary of our model and other three state-of-the-art competitors, \ie SAM \cite{cornia2016predicting}, DSCLRCN \cite{liu2016deep}, and DVA \cite{wang2017deep}, are listed in Table \ref{stoa}. Apart from achieving superior performance on the SALICON and MIT1003 datasets, our model also has the obvious advantages in terms of both \#parameters and inference time compared to the SAM and DSCLRCN models. 

However, despite the good results, there are still a small number of failure cases, as shown in Fig. \ref{fig:fc1}. These bad cases are caused by the fact that so many objects are cumulated in a single image. Within them, the relative importance of these objects cannot be fully learned by simply utilizing the multi-scale contextual features without higher level visual understanding. Therefore, some non-salient regions are highlighted (like the first row) or some salient regions are missed, as shown in the second row. Note that SAM and DSCLRCN models suffer from the same problem as ours. It can be concluded that even the state-of-the-art saliency models still cannot fully understand the relative importance of image regions in such semantically rich scenes. To further approach human-level performance, saliency models will need to discover increasingly higher-level concepts in images for determining an appropriate amount of visual attention on a certain image region.




\begin{figure}[h]
	\centering
	\scriptsize
	\includegraphics[page=7,trim = 5mm 3mm 5mm 7mm, clip,width=1.0\linewidth]{newdrawing.pdf}
	\caption{ Some failure cases of our DINet and two state-of-the-arts. Images are from SALICON validation dataset \cite{jiang2015salicon}.
	}
	\label{fig:fc1}
	\vspace{-4mm}
\end{figure}















