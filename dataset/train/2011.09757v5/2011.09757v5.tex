

\documentclass{article}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2021}
\usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\algorithmicensure}{ \textbf{Output:}} 
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}
\usepackage{hyperref}
\hypersetup{
     colorlinks=true,
     linkcolor=blue,
     filecolor=blue,
     citecolor = blue,      
     urlcolor=cyan,
 }
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{xcolor}



\icmltitlerunning{KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation}

\begin{document}

\twocolumn[
\icmltitle{KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hao-zhe Feng}{ZJU}
\icmlauthor{Zhaoyang You}{ZJU}
\icmlauthor{Minghao Chen}{ZJU}
\icmlauthor{Tianye Zhang}{ZJU}
\icmlauthor{Minfeng Zhu}{ZJU}\\
\icmlauthor{Fei Wu}{ZJU}
\icmlauthor{Chao Wu}{ZJU}
\icmlauthor{Wei Chen}{ZJU}
\end{icmlauthorlist}

\icmlaffiliation{ZJU}{Zhejiang University}
\icmlcorrespondingauthor{Wei Chen}{chenvis@zju.edu.cn}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}
Conventional unsupervised multi-source domain adaptation (UMDA) methods assume all source domains can be accessed directly. However, this assumption neglects the privacy-preserving policy, where all the data and computations must be kept decentralized. There exist three challenges in this scenario: (1) Minimizing the domain distance requires the pairwise calculation of the data from source and target domains, while the data on the source domain is not available. (2) The communication cost and privacy security limit the application of existing UMDA methods, such as the domain adversarial training. (3) Since users cannot govern the data quality, the irrelevant or malicious source domains are more likely to appear, which causes negative transfer. To address the above problems, we propose a privacy-preserving UMDA paradigm named \textbf{K}nowledge \textbf{D}istillation based \textbf{D}ecentralized \textbf{D}omain \textbf{A}daptation (KD3A), which performs domain adaptation through the knowledge distillation on models from different source domains. The extensive experiments show that KD3A significantly outperforms state-of-the-art UMDA approaches. Moreover, the KD3A is robust to the negative transfer and brings a 100 reduction of communication cost compared with other decentralized UMDA methods. 
%
 \end{abstract}

\section{Introduction}\label{sec:introduction}
Most deep learning models are trained with large-scale datasets via supervised learning. Since it is often costly to get sufficient data, we usually use other similar datasets to train the model. However, due to the domain shift, naively combining different datasets often results in unsatisfying performance. \textbf{U}nsupervised \textbf{M}ulti-source \textbf{D}omain \textbf{A}daptation (UMDA) \citep{DBLP:conf/aaai/ZhangGS15} addresses such problems by establishing transferable features from multiple source domains to an unlabeled target domain.

Recent advanced UMDA methods \cite{DBLP:conf/cvpr/ChangYSKH19,DBLP:conf/aaai/ZhaoWZGLS0HCK20} perform the knowledge transfer within two steps: (1) Combining data from source and target domains to construct \textbf{S}ource-\textbf{T}arget pairs. (2) Establishing transferable features by minimizing the -divergence. This prevailing paradigm works well when all source domains are available. However, in terms of the privacy-preserving policy, we cannot access the sensitive data such as the patient data from different hospitals and the client profiles from different companies. In these cases, all the data and computations on source domains must be kept decentralized.

Most conventional UMDA methods are not applicable under this privacy-preserving policy due to three problems: (1) Minimizing the -divergence in UMDA requires the pairwise calculation of the data from source and target domains, while the data on source domain is not available. (2) The communication cost and privacy security limit the application of advanced UMDA methods. For example, the domain adversarial training is able to optimize the -divergence without accessing data \citep{DBLP:conf/iclr/PengHZS20}. However, it requires each source domain to synchronize model with target domain after every single batch, which results in huge communication cost and causes the privacy leakage \citep{DBLP:conf/nips/ZhuLH19}. (3) The negative transfer problem \citep{5288526}. Since it is difficult to govern the data quality, there can exist some irrelevant source domains that are very different from the target domain or even some malicious source domains which perform the poisoning attack \cite{DBLP:conf/aistats/BagdasaryanVHES20}. With these bad domains, the negative transfer occurs.

In this study, we propose a solution to the above problems, \textbf{K}nowledge \textbf{D}istillation based \textbf{D}ecentralized \textbf{D}omain \textbf{A}daptation (KD3A), which aims to perform decentralized domain adaptation through the knowledge distillation on  models from different source domains. Our KD3A approach consists of three components used in tandem. First, we propose a multi-source knowledge distillation method named \textit{Knowledge Vote} to obtain high-quality domain consensus. Based on the consensus quality of different source domains, we devise a dynamic weighting strategy named \textit{Consensus Focus} to identify the malicious and irrelevant source domains. Finally, we derive a decentralized optimization strategy of -divergence named \textit{BatchNorm MMD}. Moreover, we analyze the decentralized generalization bound for KD3A from a theoretical perspective. The extensive experiments show our KD3A has the following advantages:
\begin{itemize}
    \item The KD3A brings a 100 reduction of communication cost compared with other decentralized UMDA methods and is robust to the privacy leakage attack.
    \item The KD3A assigns low weights to those malicious or irrelevant domains. Therefore, it is robust to negative transfer.
    \item The KD3A significantly outperforms the \textit{state-of-the-art} UMDA approaches with  accuracy on the large-scale DomainNet dataset. 
\end{itemize}
In addition, our KD3A is easy to implement and we create an \href{https://github.com/ICML2021-13/KD3A}{open-source framework} to conduct KD3A on different benchmarks. \section{Related work}
\subsection{Unsupervised multi-source domain adaptation}
Unsupervised Multi-source Domain Adaptation (UMDA) establish the transferable features by reducing the -divergence between source domain  and target domain . There are two prevailing paradigms that provide the optimization strategy of -divergence, i.e. maximum mean discrepancy (MMD) and the adversarial training. In addition, knowledge distillation is also used to perform model-level knowledge transfer.

\textbf{MMD based methods} \cite{DBLP:journals/corr/TzengHZSD14} construct a reproducing kernel Hilbert space (RKHS)  with the kernel , and optimize the -divergence by minimizing the MMD distance  on . Using the kernel trick, MMD can be computed as 

Recent works propose the variations of MMD, e.g., multi-kernel MMD \citep{DBLP:conf/icml/LongC0J15}, class-weighted MMD\citep{DBLP:conf/cvpr/YanDLWXZ17} and domain-crossing MMD \citep{DBLP:conf/iccv/PengBXHSW19}. However, all these methods require the pairwise calculation of the data from source and target domains, which is not allowed under the decentralization constraints.

\textbf{The adversarial training strategy} \cite{DBLP:conf/cvpr/SaitoWUH18,DBLP:conf/iclr/0002ZWCMG18} apply adversarial training in feature space to optimize -divergence. It is proved that with the adversarial training strategy, the UMDA model can work under the privacy-preserving policy \citep{DBLP:conf/iclr/PengHZS20}. However, the adversarial training requires each source domain to exchange and update model parameters
with the target domain after every single batch, which consumes huge communication resources.

\textbf{Knowledge distillation in domain adaptation.} 
Knowledge distillation (KD) \cite{DBLP:journals/corr/HintonVD15} is an efficient way of transferring knowledge between different models. Recent works \cite{DBLP:conf/icassp/MengLGJ18,DBLP:journals/corr/abs-2003-07325} extend the knowledge distillation into domain adaptation with a teacher-student training strategy: training multiple teacher models on source domains and ensembling them on target domain to train a student model. This strategy outperforms other UMDA method in practice. However, due to the
irrelevant and malicious source domains, the conventional
KD strategies may fail to obtain proper knowledge.
\subsection{Federated learning}
Federated learning \citep{DBLP:journals/corr/KonecnyMYRSB16} is a distributed machine learning approach, it can train a global model by aggregating the updates of local models from multiple decentralized datasets. Recent works \cite{DBLP:conf/aistats/McMahanMRHA17} find a trade-off between model performance and communication efficiency, that is, to make the global model achieve better performance, we need to conduct more communication rounds, which raises the communication costs. Besides, the frequent communication will also cause privacy leakage \cite{DBLP:conf/infocom/WangSZSWQ19}, making the training process insecure.

\textbf{Federated domain adaptation}. There are few works discussing the decentralized UMDA methods. FADA \citep{DBLP:conf/iclr/PengHZS20} first raises the concept of federated domain adaptation. It applies the adversarial training to optimize the -divergence without accessing data. However, FADA consumes high communication costs and is vulnerable to the privacy leakage attack. SHOT \cite{DBLP:conf/icml/LiangHF20} provides a self-supervised method to solve the single
source decentralized domain adaptation. However, it is vulnerable to the negative transfer in multi-source situations.
 \section{KD3A: decentralized domain adaptation via knowledge distillation}
Let  and  denote the source domain and target domain. In UMDA, we have  \textit{source} domains  where each domain contains  labeled examples as  and a \textit{target} domain  with  unlabeled examples as . The goal of UMDA is to learn a model  which can minimize the task risk  in , i.e. . Without loss of generality, we consider -way classification task and assume the target domain shares the same tasks with the source domains. In a common UMDA, we combine  source domains with different domain weights as , and perform domain adaptation by minimizing the following generalization bound \citep{DBLP:journals/ml/Ben-DavidBCKPV10,DBLP:conf/iclr/0002ZWCMG18} with the multiple source domains as:

\textbf{Theorem 1} \textit{Let  be the model space,  and  be the task risks of source domains  and the target domain , and  be the domain weights. Then for all  we have:}

where  is a constant according to the task risk of the optimal model on the source domains and target domain. 

In decentralized UMDA, we apply knowledge distillation to perform domain adaptation without accessing the data.
\subsection{Extending source domains with consensus knowledge}
Knowledge distillation can perform knowledge transfer through different models. Suppose we have  fully-trained models from  source domains denoted by . we use  to denote the confidence for each class and use the class with the maximum confidence as label, i.e. . As shown in Figure \ref{fig:kdkv}(a), the knowledge distillation in UMDA consists of two steps. First, for each target domain data , we obtain the inferences of the source domain models. Then, we use the ensemble method to get the consensus knowledge of the source models, e.g., . In order to utilize the consensus knowledge for domain adaptation, we define an extended source domain  with the consensus knowledge  for each target domain data  as

We also define the related task risk for  as 

With this new source domain, we can train the source model  through the knowledge distillation loss as

In decentralized UMDA, we get the target model as the aggregation of the source models, i.e. . A common question is, how does the new model  improve the UMDA performance? It is easy to find that minimizing KD loss (\ref{eq:kdloss}) leads to the optimization of  (proof in Appendix A). With this insight, we can derive the generalization bound for knowledge distillation as follows (proof in Appendix B):

\textbf{Proposition 1} (\textit{The generalization bound for knowledge distillation). Let  be the model space and  be the task risk of the new source domain  based on knowledge distillation. Then for all , we have:}

where  is a constant for the task risk of the optimal model.
\begin{figure}[t]
\centering
\subfigure[Knowledge distillation process in UMDA.]{
    \includegraphics[width=3in]{content/images/kd.PNG}
}
\quad    
\subfigure[Knowledge vote ensemble.]{
\includegraphics[width=3in]{content/images/kv.PNG}
}
\caption{(a) Knowledge distillation in UMDA consists of two steps: obtaining the inferences from source domain models and performing knowledge ensemble to get the consensus knowledge. (b) Our knowledge vote extracts strong consensus knowledge with 3 steps: confidence gate, consensus class vote and mean ensemble. `\textcolor{red}{}' means the eliminated model in each step.}
\label{fig:kdkv}
\vspace{-0.2cm}
\end{figure}

\subsection{Knowledge vote: producing good consensus}
Proposition 1 shows the new source domain  will improve the generalization bound if the consensus knowledge is good enough to represent the ground-truth label, i.e. . However, due to the irrelevant and malicious source domains, the conventional ensemble strategies (e.g., maximum and mean ensemble) may fail to obtain proper consensus. Therefore, we propose the \textit{Knowledge Vote} to provide high-quality consensus. 

The main idea of knowledge vote is that if a certain consensus knowledge is supported by more source domains with high confidence (e.g., ), then it will be more likely to be the true label. As shown in Figure \ref{fig:kdkv}(b), it takes three steps to perform \textit{Knowledge Vote}:

\begin{enumerate}
    \item \textbf{Confidence gate}. For each , we firstly use a high-level confidence gate to filter the predictions  of teacher models and eliminate the unconfident models.
    \item \textbf{Consensus class vote}. For the models remained, the predictions are added up to find the consensus class which has the maximum value. Then we drop the models that are inconsistent with the consensus class.
    \item \textbf{Mean ensemble}. After the class vote, we obtain a set of models that all support the consensus class. Finally, we get the consensus knowledge  by conducting the mean ensemble on these supporting models. We also record the number of domains that support , denoted by . For those  with all teacher models eliminated by the confidence gate, we simply use the mean ensemble to get  and assign a relatively low weight to them as . 
\end{enumerate}
After \textit{Knowledge Vote}, we obtain the new source domain . We use the  to re-weight the knowledge distillation loss as  


Compared with other ensemble strategies, our \textit{Knowledge Vote} makes model learn high-quality consensus knowledge since we assign high weights to those items with high confidence and many support domains. 
\subsection{Consensus focus: against negative transfer}
Domain weights  determine the contribution of each source domain. \citet{DBLP:journals/ml/Ben-DavidBCKPV10} proves the optimal  should be proportional to the amount of data when all source domains are 
equally important. However, this condition is hard to satisfy in KD3A since some source domains are usually very different from the target domain, or even malicious domains with corrupted labels. These bad domains lead to negative transfer. One common solution \citep{DBLP:conf/aaai/ZhaoWZGLS0HCK20} is to re-weight each source domain with the -divergence as 

However, calculating -divergence requires to access the source domain data. Besides, -divergence only measures the domain similarity on the input space, which does not utilize the label information and fails to identify the malicious domain. Reasonably, we propose \textit{Consensus Focus} to identify those irrelevant and malicious domains. As mentioned in \textit{Knowledge Vote}, the UMDA performance is related to the quality of consensus knowledge. With this motivation, the main idea of \textit{Consensus Focus} is to assign high weights to those domains which provide high-quality consensus and penalize those domains which provide bad consensus. To perform \textit{Consensus Focus}, we first derive the definition of consensus quality and then calculate the contribution to the consensus quality for each source domain.\\
\begin{algorithm}[t] 
\caption{KD3A training process with epoch t.} 
\label{alg:KD3A} 
\begin{algorithmic}[1] 
\REQUIRE ~~\\ Source domains . Target domain ;\\
Target model  with parameters ;\\
Confidence gate ;\\
\ENSURE ~~\\ Target model  with parameters .\\
\STATE // Locally training on source domains:
\FOR{ in } 
    \STATE Model initialize: .
    \STATE Train  with classification loss on .
\ENDFOR
\STATE Upload  to the target domain.
\STATE // \textit{Knowledge Vote:}
\STATE .
\STATE Train  with  loss (\ref{eq:kvloss}) on .
\STATE // \textit{Consensus Focus:}
\STATE .
\STATE // \textit{Model Aggregation:}
\STATE .
\STATE // \textit{BatchNorm MMD:}
\STATE Obtain  from 
\STATE Train  with BatchNorm MMD on .
\STATE Return .
\end{algorithmic}
\end{algorithm}
\textbf{The definition of consensus quality.} Suppose we have a set of source domains denoted by . For each coalition of source domains , we want to estimate the quality of the knowledge consensus obtained from . Generally speaking, if one consensus class is supported by more source domains with higher confidence, then it will be more likely to represent the true label, which means the consensus quality gets better. Therefore, for each  with the consensus knowledge  obtained from , We define the related consensus quality as  and the total consensus quality  is

With the consensus quality defined in (\ref{eq:cq}), we derive the consensus focus (CF) value to quantify the contribution of each source domain as 

 describes the marginal contribution of the single source domain  to the consensus quality of all source domains . If one source domain is a bad domain, then removing it will not decrease the total quality , which leads to a low consensus focus value. With the CF value, we can assign proper weights to different source domains. Since we introduce a new source domain  in \textit{Knowledge Vote}, we compute the domain weights with two steps. First, we obtain  for  based on the amount of data. Then we use the CF value to re-weight each original source domain as


Compared with the re-weighting strategy in (\ref{eq:H-reweight}), our \textit{Consensus Focus} has two advantages. First, the calculation of  does not need to access the original data. Second,  obtained through \textit{Consensus Focus} is based on the quality of consensus, which utilize both data and label information and can identify malicious domains.

\subsection{BatchNorm MMD: decentralized optimization strategy of divergence}
To get a better UMDA performance, we need to minimize the -divergence between source domains and target domain, where the kernel-based MMD distance is widely used. Existing works \cite{DBLP:conf/icml/LongC0J15,DBLP:conf/iccv/PengBXHSW19} use the feature  extracted by the fully-connected (fc) layers to build kernel as  and the related optimization target is

However, these methods is not applicable in decentralized UMDA since the source domain data is unavailable. Besides, only using the high-level features from fc-layers may lose the detailed 2-D information. Therefore, we propose the \textit{BatchNorm MMD}, which utilizes the mean and variance parameters in each BatchNorm layer to optimize the divergence without accessing data.

BatchNorm (BN) \citep{DBLP:conf/icml/IoffeS15} is a widely-used normalization technique. For the feature , BatchNorm is expressed as , where  are estimated in training process\footnote{Implemented with \textit{running-mean} and \textit{running-var} in Pytorch.}. Supposing the model contains  BatchNorm layers, we consider the quadratic kernel for the feature  of the -th BN-layer, i.e. . The MMD distance based on this kernel is 


Compared with other works using the quadratic kernel \cite{DBLP:conf/iccv/PengBXHSW19}, we can obtain all required parameters in (\ref{eq:BN-MMD}) through the parameters  of BN-layers in source domain models without accessing data\footnote{Notice }. Based on this advantage, BatchNorm MMD can perform the decentralized optimization strategy of divergence with two steps. First, we obtain   from the models on different source domains. Then, for every mini-batch , we train the model  to optimize the domain adaptation target (\ref{eq:MMD-target}) with the following loss

where  are the features of target model  from BatchNorm layers corresponding to the input . In training process, We use the mean value  of every mini-batch to estimate the expectation .
\subsection{The algorithm of KD3A}
In the above sections, we have proposed three essential components that work well in KD3A, and the complete algorithm of KD3A can be obtained by using these components in tandem: First, we obtain an extra source domain  and train the source model  through  \textit{Knowledge Vote}. Then, we get the target model by aggregating  source models through  \textit{Consensus Focus}, i.e. . Finally, we minimize the divergence of the target model through  \textit{Batchnorm MMD}. The decentralized training process of KD3A is shown in Algorithm \ref{alg:KD3A}. Confidence gate is the only hyper-parameter in KD3A, and should be treated carefully. If the confidence gate is too large, almost all data in target domain would be eliminated and the knowledge vote loss would not work. If too small, then the consensus quality would be reduced. Therefore, we gradually increase it from low (e.g., ) to high (e.g., ) in training. 










%
 \section{Generalization bound for KD3A}
We further derive the generalization bound for KD3A by combining the original bound (\ref{eq:original-bound}) and the knowledge distillation bound (\ref{eq:kb}). The related generalization bound is:

\textbf{Theorem 2} \textit{(The decentralized generalization bound for KD3A). Let  be the target model of KD3A,  be the extended source domains through Knowledge Vote and  be the domain weights through Consensus Focus. Then we have:}


The generalization performance of KD3A bound (\ref{eq:kvb}) depends on the quality of the consensus knowledge, as the following proposition shows (see Appendix C for proof):

\textbf{Proposition 2} \textit{The KD3A bound (\ref{eq:kvb}) is a tighter bound than the original bound (\ref{eq:original-bound}), if the task risk gap between the knowledge distillation domain  and the target domain  is smaller than the following upper-bound for all source domain , that is,  should satisfy:}


Proposition 2 points out two tighter bound conditions: (1) For those good source domains with small divergence and low optimal task risk , the model should take their advantages to provide better consensus knowledge, i.e. the task risk  gets close enough to . (2) For those irrelevant and malicious source domains with high divergence and , the model should filter out their knowledge, i.e. the task risk  stays away from that for bad domains. 

The KD3A has heuristically achieved the above two conditions through the \textit{Knowledge Vote} and \textit{Consensus Focus}. We also conduct sufficient experiments to show our KD3A achieves tighter bound with better performance than other UMDA approaches. \begin{figure}[t]
\centering
\includegraphics[width=2.95in]{content/images/domainnet.PNG}
\caption{The large-scale dataset DomainNet. \textit{Real} is a domain of high quality containing real-world images, while \textit{Quickdraw} is an irrelevant source domain and may cause the negative transfer.}
\label{fig:DomainNet}
\vspace{-0.2cm}
\end{figure}
\begin{table*}[t]
\setlength\extrarowheight{4.5pt}
\centering
\begin{tabular}{c|c|cccccc|c}
Standards& Methods & \textit{Clipart} & \textit{Infograph} & \textit{Painting} & \textit{Quickdraw} & \textit{Real} & \textit{Sketch} & Avg \\ 
\hline
\multirow{2}{*}{W/o DA} 
& Oracle &         &           &          &      &       &        &     \\
\cline{2-9}
& Source-only       &         &           &          &           &      &        &     \\ \hline
\multirow{2}{*}{divergence}                     
& MDAN    &  &       &          &          &      &        &     \\
\cline{2-9}
&  &         &           &           &           &      &        &     \\ 
\hline
\makecell[c]{Knowledge\\ Ensemble}
& DAEL  &         &           &   &           &       &       &     \\
\hline
\makecell[c]{Source Selection}& CMSS  &      &        &        &        &    &      &     \\ 
\hline
Others & DSB       &         &           &          &           &      &        &     \\ 
\hline
\multirow{4}{*}{\makecell[c]{Decentralized\\ UMDA}}     
& SHO &  &  &  &  & & & \\
\cline{2-9}
   & FAD   &      &           &          &           &      &        &     \\
\cline{2-9}
       & FADA   &      &           &          &           &      &        &     \\
\cline{2-9}
 & KD3A   &        &       &         &            &       &       &     \\ 
 \hline
\end{tabular}
\caption{UMDA accuracy  on the DomainNet dataset. Our model KD3A achieves  accuracy, significantly outperforming all other baselines. Moreover, KD3A achieves the oracle performance on two domains: clipart and sketch. *: The best results recorded in our re-implementation.}
\label{table:dataset}
\end{table*}
\section{Experiments}
\subsection{Domain adaptation performance}
We perform experiments on 3 benchmark datasets: (1) \textbf{Digit-5}, which is a digit recognition dataset including 5 domains. (2) \textbf{Office-Caltech10} \citep{DBLP:conf/cvpr/GongSSG12}, which contains 10 object categories from four domains. (3) \textbf{DomainNet} \citep{DBLP:conf/iccv/PengBXHSW19}, which is a recently introduced benchmark for large-scale multi-source domain adaptation with 345 classes and six domains, i.e. \textit{Clipart (clp)}, \textit{Infograph (inf)}, \textit{Painting (pnt)}, \textit{Quickdraw (qdr)}, \textit{Real (rel)} and \textit{Sketch (skt)}, as shown in Figure \ref{fig:DomainNet}. We follow the protocol used in prevailing works, selecting each domain in turn as the target domain and using the rest domains as source domains. Due to space limitations, we present results on DomainNet; more results on Digit-5 and Office-Caltech10 are provided in Appendix.

\textbf{Baselines.} We conduct extensive comparison experiments with the current best UMDA approaches from 4 categories: (1) -divergence based methods, i.e. the multi-domain adversarial network (MDAN) \cite{DBLP:conf/iclr/0002ZWCMG18} and moment matching () \citep{DBLP:conf/iccv/PengBXHSW19}. (2) Knowledge ensemble based methods, i.e. the domain adaptive ensemble learning (DAEL) \cite{DBLP:journals/corr/abs-2003-07325}. (3) Source selection based methods, i.e. the curriculum manager (CMSS) \cite{DBLP:conf/eccv/YangBLS20}. (4) Decentralized UMDA, i.e. SHOT \citep{DBLP:conf/icml/LiangHF20} and FADA \citep{DBLP:conf/iclr/PengHZS20}. The DSBN proposes a domain-specific BatchNorm, which is similar to Batchnorm MMD, so we also take it into comparison. In addition, We report two baselines without domain adaptation, i.e. oracle and source-only. Oracle directly performs supervised learning on target domains and source-only naively combines source domains to train a single model. 

\textbf{Implementation details.} Following the settings in previous UMDA works \cite{DBLP:conf/iccv/PengBXHSW19,DBLP:conf/eccv/YangBLS20}, we use a 3-layer CNN as backbone for Digit-5, and use the ResNet101 pre-trained on ImageNet for Office-Caltech10 and DomainNet. The settings of communication rounds  is important in decentralized training. Since the models on different source domains have different convergence rates, we need to aggregate models  times per epoch. To perform the -round aggregation, we uniformly divide one epoch into  stages and aggregate model after each stage. The KD3A Algorithm \ref{alg:KD3A} is a decentralized training strategy with  and we use this setting in all experiments. For model optimization, We use the SGD with 0.9 momentum as the optimizer and take the cosine schedule to decay learning rate from high (i.e. 0.05 for Digit5 and 0.005 for Office-Caltech10 and DomainNet) to zero. We conduct each experiment five times and report the results with the form . Since SHOT and DSBN do not report the results on DomainNet, we re-implement them with the official code and report the best testing results.
\begin{figure}[t]
\centering
\includegraphics[width=3in]{content/images/ab.PNG}
\caption{The ablation study of KD3A. Results show that \textit{Knowledge Vote}, \textit{Consensus Focus} and \textit{BatchNorm MMD} all contribute to the UMDA performance in all target domains.}
\label{fig:DFDA-ablation}
\end{figure}

\textbf{DomainNet.} The results on DomainNet are presented in Table \ref{table:dataset}. In general, our KD3A outperforms all the baselines by a large margin. Moreover, KD3A achieves the oracle performance on clipart and sketch. Table \ref{table:dataset} also shows the UMDA performance can benefit from the knowledge ensemble (DAEL) and source domain selection (CMSS). Compared with DAEL, the KD3A provides better consensus knowledge on the high-quality domains such as \textit{Clipart} and \textit{Real}, while it also identifies the bad domains such as \textit{Quickdraw}. CMSS select domains by checking the quality of each data with an independent network. Compared with CMSS, the KD3A does not introduce additional modules and can perform source selection in privacy-preserving scenarios. Moreover, our KD3A outperforms other decentralized models (e.g., SHOT and FADA) through the advantages in knowledge ensemble and source selection. 

\textbf{Ablation study.} To evaluate the contributions of each component, We perform ablation study for KD3A , as shown in Figure \ref{fig:DFDA-ablation}. \textit{Knowledge Vote}, \textit{Consensus Focus} and \textit{Batchnorm MMD} are all able to improve the accuracy, while most contributions are from \textit{Knowledge Vote}, which indicates our KD3A can also perform well on those tasks that cannot use Batchnorm MMD.
\subsection{Robustness to negative transfer}
We construct irrelevant and malicious source domains on DomainNet and conduct synthesized experiments to show that with \textit{Consensus Focus}, our KD3A is robust to negative transfer. 

Since \textit{\textit{Quickdraw}} is very different from other domains, and all models perform bad on it, we take \textit{Quickdraw} as the irrelevant domain, denoted by \textbf{IR-qdr}. To construct malicious domains, we perform poisoning attack \citep{DBLP:conf/aistats/BagdasaryanVHES20} on the high-quality domain \textit{Real} with  wrong labels, denoted by \textbf{MA-m}. For the irrelevant domain IR-qdr, we select the remaining five domains in turn as target domains and train KD3A with the rest source domains. In training process, we plot the curve of the mean weight  assigned to IR-qdr by \textit{Consensus Focus}. We also report the average UMDA accuracy across all target domains. For the malicious domain MA-m, we conduct the same process on the remained four domains except for \textit{Quickdraw}. We report the same experiment results as IR-qdr. 

We consider two advanced weighting strategies 
for comparison: the -divergence re-weighting in equation (\ref{eq:H-reweight}) and the \textit{Info Gain} in FADA \citep{DBLP:conf/iclr/PengHZS20}. In addition, we also report the average UMDA accuracy of KD3A model with the bad domain dropped. According to the results provided in Table \ref{table:robust} and Figure \ref{fig:irma}, we can get the following insights: (1) For IR-qdr and MA-(30,50), the negative transfer occurs since the domain-drop outperforms the others. (2) The three weighting strategies are robust to the irrelevant domain since they all assign low weights to IR-qdr. (3) \textit{Consensus Focus} outperforms other strategies in malicious domains since it assigns extremely low weights to the bad domain (i.e.  for MA-30), while other strategies can not identify the malicious domain. Moreover, our KD3A can use the correct information of less malicious domains (i.e. MA-(15,30)) and achieves better performance than the domain-drop.
\begin{table}[t]
\setlength\extrarowheight{4.5pt}
\begin{tabular}{c|ccc|c}
      &\makecell[c]{-divergence} & \makecell[c]{Info\\gain}
      & \makecell[c]{Consensus\\ focus} &  \makecell[c]{Domain\\ drop} \\ \hline
IR-qdr &  &   &      &   \\\cline{1-5} 
MA-15  &  &  &      & \multirow{3}{*}{}  \\
MA-30  &  &  &      &  \\
MA-50  &  &   &       &  \\
\hline
\end{tabular}
\caption{Average UMDA accuracy () with irrelevant and malicious domains. IR-qdr means to use the \textit{Quickdraw} as the irrelevant source domain, while MA-m means to construct a malicious source domain with  mislabeled data. With consensus focus, our KD3A is robust to negative transfer.}
\label{table:robust}
\end{table}
\begin{figure}[t]
\centering
\subfigure[IR-qdr.]{
    \includegraphics[height=1.25in]{content/images/ir.PNG}
}
\subfigure[MA-30.]{
\includegraphics[height=1.25in]{content/images/ma.PNG}
}
\caption{Weights assigned to the irrelevant and malicious domains in the training process. Our consensus focus can identify these bad domains with the low weights.}
\label{fig:irma}
\vspace{-0.2cm}
\end{figure}

\subsection{Communication efficiency and privacy security}
To evaluate the communication efficiency, We train the KD3A with different communication rounds  and report the average UMDA accuracy on DomainNet. We take the FADA method as a comparison. The results in Table \ref{table:robust-to-cr} show the following properties: (1) Due to the adversarial training strategy, FADA works under large communication rounds (i.e.  = ). (2) Our KD3A works under the low communication cost with  = , leading to a 100  communication reduction. (3) KD3A is robust to communication rounds. For example, the accuracy only drops  when  decreases from  to . Moreover, we consider two extreme cases where we synchronize models every 2 and 5 epochs, i.e.  =  and . In these cases, FADA performs worse than the source-only baseline while our KD3A can still achieve \text{state-of-the-art} results. 

In decentralized training process, the frequent communication will cause privacy leakage \cite{DBLP:conf/infocom/WangSZSWQ19}, making the training process insecure. To verify the privacy protection capabilities, we perform the advanced gradient leakage attack \citep{DBLP:conf/nips/ZhuLH19} on KD3A and FADA. As shown in Figure \ref{fig:gl}, the source images used in FADA are recovered under the attack, which causes privacy leakage. However, due to the low communication cost, our KD3A is robust to this attack, which demonstrates high privacy security.
\begin{table}[t]
\setlength\extrarowheight{4.5pt}
\begin{tabular}{c|cccccc}
      & & & & & &  \\ \hline
FADA & && &   &      &   \\ \hline
KD3A  &  &  &  &  &      &   \\
\hline
\end{tabular}
\caption{Average UMDA accuracy () with different communication rounds  for our KD3A and FADA. KD3A achieves good performance with low communication cost (e.g., ). }
\label{table:robust-to-cr}
\end{table}
\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{content/images/gl.PNG}
\caption{The gradient leakage attack \citep{DBLP:conf/nips/ZhuLH19} on decentralized training strategy. KD3A is robust to this attack while FADA causes the privacy leakage.}
\label{fig:gl}
\vspace{-0.2cm}
\end{figure}
%
 \section{Conclusions}
We propose an effective approach KD3A to address the problems in decentralized UMDA. The main idea of KD3A is to perform domain adaptation through the knowledge distillation without accessing the source domain data. Extensive experiments on the large-scale DomainNet demonstrate that our KD3A outperforms other \textit{state-of-the-art} UMDA approaches and is robust to negative transfer. Moreover, KD3A has a great advantage in communication efficiency and is robust to the privacy leakage attack. 


\bibliography{main}
\bibliographystyle{icml2021}
\newpage
\setcounter{equation}{0}
\section{Appendix}
\subsection{Appendix A}
\textbf{Claim} \textit{For the extended source domain , training the related source model  with the knowledge distillation loss  equals to optimizing the task risk .}

\textbf{Proof:}

First, we prove that ,

The widely used \textbf{Pinsker's inequality} states that, if  and  are two probability distributions on a measurable space , then 

where 

In our situation, we choose the event  as the probability of classifying the input  into class , and the related probability under  is  and . With \textbf{Pinsker's inequality}, it is easy to prove .  Since the inequality  holds for all class , minimizing the knowledge distillation loss will make , that is, .
\subsection{Appendix B}
\textbf{Proposition 1} (\textit{The generalization bound for knowledge distillation). Let  be the model space and  be the task risk of the new source domain  based on knowledge distillation. Then for all , we have:}

where  is a constant for the task risk of the optimal model.

\textbf{Proof:}

Following the Theorem 2 in \citet{DBLP:journals/ml/Ben-DavidBCKPV10}, for the source domain  and the target domain , for all , we have

where  is constant of the optimal model on the source domain and the target domain as . 

In addition, the following inequality also holds for all :

where  is the upper bound of the task risk gap between the target domain  and the extended domain . Notice  shares the same input space with  since they all use  as inputs. Therefore, we have

Substituting  into , we have 

Combining  and , we get the \textbf{Proposition 1}.

\textbf{The learning bound with empirical risk error.} Proposition 1 shows how to relate the extended source domain  and the target domain . Since we use the finite samples to empirically estimate the  and  at the training time, We now proceed to give a learning
bound for empirical risk minimization using  sampled training data.

Following the learning bound \textbf{Lemma 1,5} in \citet{DBLP:journals/ml/Ben-DavidBCKPV10}, for all , with probability at least , we have:

where  is the VC-dimension of model space . 

Combining  and , we get the generalization bound for knowledge distillation with the empirical learning error as follows:

where  is a constant as 

\begin{table*}[ht]
\centering
\begin{tabular}{cc}
\hline
\multicolumn{1}{c|}{Layer} & Configuration                                                       \\ \hline
\multicolumn{1}{c|}{1}     & 2D Convolution with kernel size 5*5 and output feature channels 64  \\ \hline
\multicolumn{1}{c|}{2}     & BatchNorm, ReLU, MaxPool                                            \\ \hline
\multicolumn{1}{c|}{3}     & 2D Convolution with kernel size 5*5 and output feature channels 64  \\ \hline
\multicolumn{1}{c|}{4}     & BatchNorm, ReLU, MaxPool                                            \\ \hline
\multicolumn{1}{c|}{5}     & 2D Convolution with kernel size 5*5 and output feature channels 128 \\ \hline
\multicolumn{1}{c|}{6}     & BatchNorm, ReLU                                                     \\ \hline
\multicolumn{1}{c|}{7}     & Fully connection layer with output channels 10                      \\ \hline
\multicolumn{1}{c|}{8}     & Softmax                                                             \\ \hline
\end{tabular}
\caption{The 3-layers CNN backbone for \textbf{Digit-5}.}
\label{table:backbone}
\end{table*}
\begin{table*}[htbp]
\centering
\begin{tabular}{c|c|c|c}
Parameters             & \multicolumn{3}{c}{Benchmark Datasets}                                                            \\ \hline
                       & \textbf{Digit-5}                              & \textbf{Office-Caltech10}              & \textbf{DomainNet}                   \\ \hline
Data Augmentation      & \multicolumn{3}{c}{Mixup }                                                          \\ \hline
Backbone               & 3-layers CNN (pretrained = False)    & \multicolumn{2}{c}{Resnet101 (pretrained = True)}          \\ \hline
Optimizer              & \multicolumn{3}{c}{SGD with momentum = 0.9}                                                       \\ \hline
Learning rate schedule & From 0.05 to 0.001 with cosine decay & \multicolumn{2}{c}{From 0.005 to 0.0001 with cosine decay} \\ \hline
Batchsize              & 100                                  & 32                            & 50                          \\ \hline
Total epochs           & \multicolumn{3}{c}{40}                                                                            \\ \hline
Communication rounds   & \multicolumn{3}{c}{r=1}                                                                           \\ \hline
Confidence gate        & \multicolumn{2}{c|}{From 0.9 to 0.95}                                & From 0.8 to 0.95            \\ \hline
\end{tabular}
\caption{Implementation details of our KD3A on three benchmark datasets: Digit-5, Office-Caltech10 and DomainNet.}
\label{table:implement}
\end{table*}
\subsection{Appendix C}
\textbf{Proposition 2} \textit{The KD3A bound is a tighter bound than the original bound, if the task risk gap between the knowledge distillation domain  and the target domain  is smaller than the following upper-bound for all source domain , that is,  should satisfy:}


\textbf{Proof:}

Following the Theorem 2 in \citet{DBLP:journals/ml/Ben-DavidBCKPV10}, for each source domain  and for all , we have

where  is the optimal task risk of  and .

The original bound states that for all , we have

where  and we have the following relations between  and :


With , the original bound  can be considered as the weighted combination of the source domains. In addition, the KD3A bound is also the combination of the original bound  and the knowledge distillation bound . Then we get that the KD3A bound is a tighter bound than the original bound if the knowledge distillation bound  is tighter than the single source bound  for each source domain , that is, for all source domain  and all , the knowledge distillation bound should satisfy:

Since  and  is a constant, the task risk gap  should satisfy the following condition for all , that is:

Since condition  holds for all , we have the tighter bound condition as 

\subsection{Appendix D}
\subsection{Implementation details.} 
We perform UMDA on those datasets with multiple domains. During experiments, we choose one domain as the target domain, and use the remained domains as source domains. Finally, we report the average UMDA results among all domains. The code, with which the most important results can be reproduced, is available at Github\footnote{\underline{github.com/ICML2021-13/KD3A}}. We also provide the code as well as the detailed documentation
in “SourceCode.zip”. In this section, we discuss the implementation details. Following previous settings \citep{DBLP:conf/iccv/PengBXHSW19}, we use a 3-layer CNN as backbone for Digit-5, as shown in Table \ref{table:backbone}, and use the pretrained ResNet101 for Office-Caltech10 and DomainNet. The details of hyper-parameters are provided in Table \ref{table:implement} and the backbones and training epochs are set to same in all method comparison experiments. In training process, We use the SGD as optimizer and take the cosine schedule to decay learning rate from high (i.e.  for Digit5 and  for Office-Caltech10 and DomainNet) to zero. 
\begin{table}[t]
\begin{tabular}{c|ccc|c}
     & Clipart                    & Infograph                  & Painting                   & Avg             \\ \hline
KD3A &           &           &           &           \\ \hline
KD3A &  &  &  &  \\ \hline
     & Quickdraw                  & Real                       & Sketch                     &                 \\ \hline
KD3A &           &           &           &           \\ \hline
KD3A &  &  &  &  \\ \hline
\end{tabular}
\caption{The ablation study for data-augmentation strategies on DomainNet.\dag: Methods trained without data-augmentation.}
\label{table:data-aug}
\end{table}
\begin{table*}[h!]
\setlength\extrarowheight{4.5pt}
\centering
\begin{tabular}{c|ccccc|c}
Methods & mt & mm & sv & syn & usps & Avg \\ 
\hline
Oracle &         &           &          &      &  &     \\
Source-only       &         &           &          &       &    &      \\ \hline
 MDAN    &  &       &          &     &     &     \\
  &         &           &           &           &       &     \\ 
\hline
 CMSS  &      &        &        &        &    &     \\ 
\hline 
 DSBN       &         &           &          &           &      &     \\ 
\hline
FADA  &      &           &          &           &       &     \\
\cline{2-7}
FADA    &      &           &          &           &       &     \\
\cline{2-7}
SHOT &  &  &  &  &  & \\
\cline{2-7}
KD3A   &        &       &         &            &     &   \\
\cline{2-7}
KD3A   &        &       &         &            &     &     \\ 
 \hline
\end{tabular}
\caption{UMDA accuracy  on the \textbf{Digit-5}. *: The best results recorded in our re-implementation. \dag: Methods trained without data-augmentation. Our model KD3A achieves  accuracy and outperforms all other baselines.}
\label{table:digit5}
\end{table*}
\begin{table*}[h!]
\setlength\extrarowheight{4.5pt}
\centering
\begin{tabular}{c|cccc|c}
Methods & A & C & D & W  & Avg \\ 
\hline
Oracle &         &           &          &      &     \\
Source-only       &         &           &          &       &      \\ \hline
 MDAN    &  &       &          &      &     \\
  &         &           &           &          &     \\ 
\hline
CMSS  &         &           &   &    &    \\
\hline
 DSBN        &         &           &          &       &     \\ 
\hline
FADA   &      &           &          &        &     \\
\cline{2-6}
SHOT &  &  &  &  & \\
\cline{2-6}
KD3A &        &       &            &     &     \\ 
\cline{2-6}
KD3A   &        &       &            &     &     \\ 
 \hline
\end{tabular}
\caption{UMDA accuracy  on the Office-Caltech10. *: The best results recorded in our re-implementation. \dag: Methods trained without data-augmentation.}
\label{table:office}
\end{table*}
\textbf{Data augmentations.} Data augmentations are important in deep network training process. Since different datasets require different augmentation strategies (e.g. rotate, scale, and crop), which introduces extra hyper-parameters, we use mixup \citep{DBLP:conf/iclr/ZhangCDL18} as a unified augmentation strategy and simply set the mix-parameter  in all experiments. For fair comparison, we report the results on both conditions, i.e. with/without data-augmentations. The results are shown in Table 3-5. The ablation study in data augmentations indicates that mixup strategy can unify different augmentation strategies on different doman adaptation datasets with only one hyper-parameter. Moreover, KD3A can achieve good results even without data-augmentation. 
\subsection{Results on Digit-5 and Office-caltech10.}

In this section, we report the experiment results on \textbf{Digit-5} and \textbf{Office-Caltech10}. Digit-5 is a digit classification dataset including MNIST (mt), MNISTM(mm), SVHN (sv), Synthetic (syn), and USPS (up). Office-Caltech10 contains 10 object categories from four domains, i.e. Amazon (A), Caltech (C), DSLR (D). and Webcam (W). \textbf{Note that results are directly cited from published papers if we follow the same setting.} The results on Table \ref{table:digit5},\ref{table:office} show that our KD3A outperforms other UMDA methods and advanced decentralized UMDA methods. Moreover, our KD3A provides better consensus knowledge on the hard domains such as the \textit{MNISTM} domain on the \textbf{Digit-5}, which outperforms other methods by a large margin. 



\end{document}
