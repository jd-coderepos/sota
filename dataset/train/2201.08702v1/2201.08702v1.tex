\section{Experiments}

\subsection{Datasets}

We conduct our experiments on following five benchmark text classification datasets. SST-2 \cite{socher2013parsing_sst2} is a sentiment classification dataset of movie reviews. SUBJ \cite{pang2004sentimental} is a review dataset with sentence labelled as subjective or objective. TREC \cite{li2002learning_trec} contains questions from six different domains, including description, entity, abbreviation, human, location and numeric. PC \cite{ganapathibhotla2008mining_procon} is a binary sentiment classification dataset that includes Pros and Cons data. CR \cite{ding2008holistic_cr} is a customer review data set and each sample is labelled as positive or negative. Table~\ref{tab:datasets} summarizes the statistics of the datasets.

\begin{table}[t]
	\centering
	\resizebox{.85\columnwidth}{!}{
	\begin{tabular}{cccccc}
		\toprule
		Dataset & \#Class & AvgLen & \#Train & \#Test & $|\text{V}|$ \\
		\midrule
		SST-2 & 2 & 17 & 7,447 & 1,821 & 15,300 \\
		\midrule
		SUBJ & 2 & 21 & 9,000 & 1,000 & 20,874 \\
		\midrule
		TREC & 6 & 9 & 5,452 & 500 & 8,751 \\
		\midrule
		PC & 2 & 7 & 32,097 & 13,759 & 9,982 \\
		\midrule
		CR & 2 & 18 & 3,394 & 376 & 5,542 \\
  		\bottomrule
	\end{tabular}}
	\caption{Statistics for the five text classification datasets.}
	\label{tab:datasets}
\end{table}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        \includegraphics[width=.95\textwidth]{figs/scatter_baseline.pdf}
        \caption{Cross-Entropy}
    \end{subfigure}
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        \includegraphics[width=.95\textwidth]{figs/scatter_ours_wo_dual.pdf}
        \caption{DualCL w/o $\cL_{\rm Dual}$}
    \end{subfigure}
    \begin{subfigure}[t]{.33\textwidth}
        \centering
        \includegraphics[width=.95\textwidth]{figs/scatter_ours.pdf}
        \caption{DualCL}
    \end{subfigure}
    \caption{The tSNE plots of the learned representations on the SST-2 dataset.}
    \label{fig:scatter}
\end{figure*}

\subsection{Implementation Details}

To adapt the input format to the BERT-family pretrained language models, for each input sentence, we list the names of all the labels as a token sequence and insert it before the input sentence, and split them with a special [SEP] token. We also insert a special [CLS] token in the front of the input sequence and append a [SEP] token at the end of the input sequence.

Both BERT and RoBERTa employ the position embeddings to make use of the order of the tokens in the input sequence, thus the class labels will be associated with the position embeddings if they are listed in a fixed order. In order to mitigate the influence of label orders, we randomly change the order of the labels before forming the input sequence in the training phase. During the test phase, the label order remains unchanged.

We use the AdamW \cite{loshchilov2018decoupled} optimizer to finetune the pretrained BERT-base-uncased and RoBERTa-base model \cite{wolf2019huggingface} with a $0.01$ weight decay. We train the model for $30$ epochs and use a linear learning rate decay from $2\times10^{-5}$ to $10^{-5}$. We set the dropout rate to $0.1$ for all layers and the batch size to 64 for all datasets. For the hyperparameters, we adopt a grid search strategy to choose the best $\lambda$ in $\{0.01, 0.05, 0.1\}$. The temperature factor $\tau$ is chosen as $0.1$. Our PyTorch implementation is available at: \url{https://github.com/hiyouga/Dual-Contrastive-Learning}.

\subsection{Experimental Results}

We compare DualCL with three supervised learning baselines: the model trained with cross-entropy loss (CE), with both the cross-entropy loss and the standard supervised contrastive loss (CE+SCL) \cite{gunel2021supervised_contrastive_language}, with both the cross-entropy loss and the self-supervised contrastive loss (CE+CL) \cite{gao2021simcse}. The results are shown in Table~\ref{tab:main_results}. 

From the results, it can be seen that DualCL with both BERT and RoBERTa encoders achieves the best classification performance in almost all settings, except on the TREC dataset where RoBERTa is employed. Compared to CE+CL with full training data, the average improvement of DualCL is 0.46\% and 0.39\% on BERT and RoBERTa, respectively. Furthermore, we observe that with 10\% training data, DualCL outperforms the CE+CL method by a larger margin, which is 0.74\% and 0.51\% higher on BERT and RoBERTa, respectively. Meanwhile, CE and CE+SCL cannot surpass the performance of DualCL. This is because the CE method neglects the relation between the samples and the CE+SCL method cannot directly learn a classifier for the classification tasks.

In addition, we find that the dual contrastive loss term helps the model to achieve better performance on all five datasets. It shows that leveraging the relations between samples helps the model to learn better representations in contrastive learning.

\subsection{Visualization}

To investigate how dual contrastive learning improves the quality of representations, we draw the tSNE plots of learned representations on the SST-2 test set. We use RoBERTa as the encoder and finetune the encoder with 25 training samples per class. We show the results of CE, DualCL without $\mathcal{L}_{\rm Dual}$ and DualCL in Figure~\ref{fig:scatter}.

In Figure~\ref{fig:scatter}, we can find that DualCL learns representations both for the input samples and the classifier associating to each sample. Comparing Figure~\ref{fig:scatter}(b) with Figure~\ref{fig:scatter}(c), we observe that the dual contrastive loss helps the model to learn more discriminative and robust representations for the input features and the classifiers, by exploiting the relation between training samples and imposing additional constraints to the model.

\subsection{Effects in Low-Resource Scenarios}

\begin{figure}[t]
    \centering
    \includegraphics[width=.495\columnwidth]{figs/SST2_few_samples.pdf}
    \includegraphics[width=.495\columnwidth]{figs/SUBJ_few_samples.pdf}
    \caption{Test accuracy on the SST2 (left) and SUBJ (right) datasets in low-resource scenarios of DualCL with or without $\cL_{\rm Dual}$ compared with cross-entropy using BERT. The batch size here is set to $4$.}
    \label{fig:few_data}
\end{figure}

In DualCL, it uses the label-aware input representation as another view of the input samples. Thus we conjecture that the label-aware data augmentation is supposed to serve for the low-resource scenarios. To validate this, we perform experiments on the SST-2 and SUBJ datasets in low-resource scenarios. We evaluate the model performance using $N$ training samples per class, where $N\in\{5,10,20,30,40,50,60,70,80,90,100\}$. We sketch the results of BERT trained with CE, DualCL without $\mathcal{L}_{\rm Dual}$ and DualCL in Figure~\ref{fig:few_data}.

In Figure~\ref{fig:few_data}, we can see that DualCL significantly surpasses the CE method on the reduced datasets. Specifically, the improvement is up to 8.5\% on the SST2 dataset and 5.4\% on the SUBJ dataset with only 5 training samples per class. Even without the dual contrastive loss, the label-aware data augmentation can consistently improve the model's performance on the reduced dataset.

\subsection{Case Study}

To validate whether DualCL can capture informative features, we compute the attention score between the feature of [CLS] token and each word in the sentence. We firstly finetune the RoBERTa encoder on the full training set. Then we compute the $\ell_2$ distance between the features and visualize the attention map in Figure~\ref{fig:case_study}. It shows that when classifying sentiments, the captured features are different. The above example comes from the SST-2 dataset, we can see that our model pays higher attention to  ``predictably heart warming'' for the sentence expressing a ``positive'' sentiment. The below example comes from the CR dataset, we can see that our model pays higher attention to ``small'' for the sentence expressing a ``negative'' sentiment. On the contrary, the CE method fails to concentrate on these discriminative features. The results suggest that our DualCL can successfully attend to the informative keywords in the sentence.

\begin{figure}[t]
    \centering
    \includegraphics[width=.99\columnwidth]{figs/case_study_1.pdf}
    \includegraphics[width=.99\columnwidth]{figs/case_study_2.pdf}
    \caption{The visualization of attention map for CE and DualCL. The darker blue refers to higher attention scores.}
    \label{fig:case_study}
\end{figure}
