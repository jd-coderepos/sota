\documentclass[sigconf]{acmart}
\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
    
    
    
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{arydshln}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{graphicx}

\usepackage{pifont}
\usepackage{newunicodechar}
\usepackage{color}
\newunicodechar{✓}{\ding{51}}
\newunicodechar{✗}{\ding{55}}
\definecolor{greencheck}{RGB}{0,176,80}
\definecolor{redcross}{RGB}{255,0,0}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\usepackage{xcolor}
\usepackage{bm}

\usepackage{algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\let\oldnl\nl \newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}
\newcommand\mycommfont[1]{\footnotesize\textit{\textcolor{gray}{#1}}}
\SetCommentSty{mycommfont}
\SetNlSty{emph}{}{}


\copyrightyear{2021} 
\acmYear{2021} 
\setcopyright{acmcopyright}\acmConference[SIGIR '21]{Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval}{July 11--15, 2021}{Virtual Event, Canada}
\acmBooktitle{Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '21), July 11--15, 2021, Virtual Event, Canada}
\acmPrice{15.00}
\acmDOI{10.1145/3404835.3462853}
\acmISBN{978-1-4503-8037-9/21/07}

\settopmatter{printacmref=true}






\begin{document}


\fancyhead{}

\title{Answering Any-hop Open-domain Questions \\ with Iterative Document Reranking}

\author{Ping Nie}
\authornote{Both authors contributed equally to this work.}
\affiliation{\institution{Peking University}\country{}}
\email{ping.nie@pku.edu.cn}

\author{Yuyu Zhang}
\authornotemark[1]
\affiliation{\institution{Georgia Institute of Technology}\country{}}
\email{yuyu@gatech.edu}

\author{Arun Ramamurthy}
\affiliation{\institution{Siemens Corporate Technology\country{}}}
\email{arun.ramamurthy@siemens.com}

\author{Le Song}
\affiliation{\institution{Georgia Institute of Technology\country{}}}
\email{lsong@cc.gatech.edu}






















\begin{abstract}
Existing approaches for open-domain question answering (QA) are typically designed for questions that require either single-hop or multi-hop reasoning, which make strong assumptions of the complexity of questions to be answered. Also, multi-step document retrieval often incurs higher number of relevant but non-supporting documents, which dampens the downstream noise-sensitive reader module for answer extraction. To address these challenges, we propose a unified QA framework to answer any-hop open-domain questions, which iteratively retrieves, reranks and filters documents, and adaptively determines when to stop the retrieval process. To improve the retrieval accuracy, we propose a graph-based reranking model that perform multi-document interaction as the core of our iterative reranking framework. Our method consistently achieves performance comparable to or better than the state-of-the-art on both single-hop and multi-hop open-domain QA datasets, including Natural Questions Open, SQuAD Open, and HotpotQA.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003348</concept_id>
       <concept_desc>Information systems~Question answering</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Question answering}

\keywords{open-domain question answering; iterative document reranking; multi-document interaction}


\maketitle



\section{Introduction}

\setlength{\abovedisplayskip}{3pt}
\setlength{\abovedisplayshortskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\setlength{\belowdisplayshortskip}{3pt}
\setlength{\jot}{2pt}
\setlength{\floatsep}{1ex}
\setlength{\textfloatsep}{1ex}
\setlength{\intextsep}{1ex}





Open-domain question answering (QA) requires a system to answer factoid questions using a large text corpus (e.g., Wikipedia or the Web) without any pre-defined knowledge schema. Most state-of-the-art approaches for open-domain QA follow the retrieve-and-read pipeline initiated by \citet{chen-etal-2017-reading}, using a retriever module to retrieve relevant documents, and then a reader module to extract answer from the retrieved documents. These approaches achieve prominent results on single-hop QA datasets such as SQuAD~\cite{rajpurkar-etal-2016-squad}, whose questions can be answered using a single supporting document. However, they are inherently limited to answering simple questions and not able to handle multi-hop questions, which require the system to retrieve and reason over evidence scattered among multiple documents. In the task of open-domain multi-hop QA~\cite{yang-etal-2018-hotpotqa}, the documents with the answer can have little lexical overlap with the question and thus are not directly retrievable. Take the question in Figure~\ref{figure:example_of_qa_dev} as an example, the last paragraph contains the correct answer but cannot be directly retrieved using TF-IDF. In this example, the single-hop TF-IDF retriever is not able to retrieve the last supporting paragraph since it has no lexical overlap with the question, but this paragraph contains the answer and plays a critical role in the reasoning chain.


\begin{figure}[t]
\center
\includegraphics[width=0.48\textwidth]{figures/example_ddr_case-crop.pdf}
\center
\caption{An example open-domain multi-hop question from the HotpotQA dev set, where the question has only partial clues to retrieve supporting documents. The first and fourth paragraphs are gold supporting documents, and the remaining two paragraphs are relevant but non-supporting documents. ABR stands for ALBERT-base reranker, which serves as a reference of the retrieval performance of existing multi-hop QA methods that independently consider the relevance of each document to the question. The \textbf{\color{greencheck}✓} and \textbf{\color{redcross}✗} symbols mark whether the retriever correctly identifies the document as a supporting / non-supporting one. Below each symbol, we annotate the output of the corresponding retriever with regard to the document, where ``positive'' (``negative'') means that the retriever classifies it as a supporting (non-supporting) document.}
\label{figure:example_of_qa_dev}
\end{figure}

Recent studies on multi-hop QA attempt to perform iterative retrievals to improve the answer recall of the retrieved documents. However, several challenges are not solved yet by existing multi-hop QA methods: 1) The iterative retrieval rapidly increases the total number of retrieved documents and introduces much noise to the downstream reader module for answer extraction. Typically, the downstream reader module is noise-sensitive, which works poorly when taking noisy documents as input or missing critical supporting documents with the answer~\cite{nie-etal-2019-revealing}. This requires the QA system to reduce relevant but non-supporting documents fed into the reader module. However, to answer open-domain multi-hop questions, it is necessary to iteratively retrieve documents to increase the overall recall of supporting documents. This dilemma poses a challenge for the retrieval phase of open-domain QA systems; 2) Existing multi-hop QA methods such as MUPPET~\cite{feldman-el-yaniv-2019-multi} and Multi-step Reasoner~\cite{das2018multistep} perform a fixed number of retrieval steps, which make strong assumptions on the complexity of open-domain questions and perform fixed number of retrieval steps. In real-world scenarios, open-domain questions may require different number of reasoning steps; 3) The relevance of each retrieved document to the question is independently considered. As exemplified in Figure~\ref{figure:example_of_qa_dev}, ABR stands for ALBERT-base reranker, which serves as a reference of the retrieval performance of existing multi-hop QA methods that independently consider the relevance of each document to the question. Without considering multiple retrieved documents as a whole, these methods can be easily biased to the lexical overlap between each document and the question, and incorrectly classify non-supporting documents as supporting evidence (such as the middle two non-supporting paragraphs in Figure~\ref{figure:example_of_qa_dev}, which have decent lexical overlap with the question) and vice versa (such as the bottom paragraph in Figure~\ref{figure:example_of_qa_dev}, which has no lexical overlap with the question but is a critical supporting document that contains the answer).


To address the challenges above, we introduce a unified QA framework for answering any-hop open-domain questions named Iterative Document Reranking (IDR). Our framework learns to iteratively retrieve documents with updated question, rerank and filter documents, and adaptively determine when to stop the retrieval process. In this way, our method can significantly reduce the noise introduced by multi-round retrievals and handle open-domain questions that require different number of reasoning steps. To avoid the bias of lexical overlap in identifying supporting documents, our framework considers the question and retrieved documents as a whole and models the multi-document interactions to improve the accuracy of classifying supporting documents.

As illustrated in Figure~\ref{figure:pipeline}, our method constructs a document graph linked by shared entities to propagate information using a graph attention network (GAT). By leveraging the multi-document information, our reranking model has more knowledge to differentiate supporting documents from irrelevant documents. After initial retrieval, our method updates the question at every retrieval step with a text span extracted from the retrieved documents, and then use the updated question as query to retrieve complementary documents, which are added to the document graph for a new round of interaction. The reranking model is reused to score the documents again and filter the most irrelevant ones. The maintained high-quality shortlist of remaining documents are then fed into the Reader Module to determine whether the answer exists in them. If so, the retrieval process ends and the QA system delivers the answer span extracted by the Reader Module as the predicted answer. Otherwise, the retrieval process continues to the next hop.

Our contributions are summarized as follows:
\begin{itemize}[leftmargin=*]
    \item \textit{Noise control for iterative retrieval}: We propose a novel QA method to iteratively retrieve, rerank and filter documents, and adaptively determine when to stop the retrieval process. Our method maintains a high-quality shortlist of remaining documents, which significantly reduces the noise introduced to the downstream reader module for answer extraction. Thus, the downstream reader module can extract the answer span with higher accuracy.
    \item \textit{Unified framework for any-hop open-domain QA}: We propose a unified framework that does not require to pre-determine the complexity of input questions. Different from existing QA methods that are specifically designed for either single-hop or fixed-hop questions, our method can adaptively determine the termination of retrieval and answer any-hop open-domain questions.
    \item \textit{Multi-document interaction}: We construct entity-linked document graph and employ graph attention network for multi-document interaction, which boosts up the reranking performance. To the best of our knowledge, we are the first to propose graph-based document reranking method for open-domain multi-hop QA.
    


\end{itemize}



















\begin{figure*}
\center
\includegraphics[width=\textwidth]{figures/framework-crop.pdf}
\centering
\caption{An overview of the IDRQA system, which consists of an Iterative Document Reranking (IDR) phase and a question answering phase. Given an open-domain question, IDR iteratively retrieves, reranks and filters documents, and adaptively determines when to stop the retrieval process. After the initial retrieval, IDR updates the question with an extracted text span as a new query to retrieve more documents at every iteration. Once the retrieval is done, the final highest-scoring documents are fed into the downstream reader module for answer extraction.
}
\label{figure:pipeline}
\end{figure*}








\section{Overview}



\subsection{Problem Definition}
Given a factoid question, the task of open-domain question answering (QA) is to answer it using a large corpus which can have millions of documents (e.g., Wikipedia) or even billions (e.g., the Web). Let the corpus  consist of  documents as the basic retrieval units\footnote{We use the natural paragraphs as the basic retrieval units.}. Each document  can be viewed as a sequence of tokens . Formally, given a question , the task is to find a text span  from one of the documents  that can answer the question\footnote{In this work, we focus on the extractive or span-based QA setting, but the problem definition and our proposed method can be generalized to other QA settings as well.}. For open-domain multi-hop QA, the final documents with the answer are typically multiple hops away from the question, i.e., the system is required to find seed documents and subsequent supporting documents in order of a chain or directed graph to locate the final documents. The retrieved documents are usually connected via shared entities or semantic similarities, and the formed chain or directed graph of documents can be viewed as the reasoning process for answering the question.

Note that the task of \textit{open-domain multi-hop QA} that we describe above is much different from the \textit{few-document setting of multi-hop QA}~\cite{qi-etal-2019-answering}, where the QA system is provided with a tiny set of documents that consists of all the gold supporting documents together with several irrelevant ``distractor'' documents. The \textit{few-document setting} is designed to test the system's capability of multi-hop reasoning given all of the gold supporting documents, but this is far from being realistic. A real-world open-domain QA system has to locate the necessary supporting documents from a large corpus on its own, which is especially challenging for multi-hop questions since the indirect supporting documents are not easily retrievable given the question itself.






The nature of multi-hop questions poses significant challenge for retrieving supporting documents, which is crucial to the downstream QA performance. To address this challenge, we argue that it is necessary to iteratively retrieve, rerank and filter documents, so that we can maintain a high-quality shortlist of documents. To this end, we propose the Iterative Document Reranking (IDR) method, which is introduced in the next section.





















\subsection{System Overview}





\begin{algorithm}[t]
\caption{Iterative Document Reranking (inference)}
\label{alg:algorithm}
\DontPrintSemicolon
\SetNoFillComment
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{A textual question ; the maximum number of retrieval hops ; the number of retrieved documents at each hop ; the number of remaining documents after each reranking process ; the graph-based reranking model ; the reader module ; the question updater model }
\Output{Predicted answer }
 \;
 \;
\While{}
{
     \;
     Retrieve top  documents according to  \;
     \;
     Rerank and get top  documents using  \;
     Predict the answer using  \;
    \If(\tcc*[f]{No more hop needed})
    { is not None}
    {
        \Return a \;
    }
    \Else(\tcc*[f]{More hop with question updater})
    {
         \;
    }
}
\Return a
\end{algorithm}








We illustrate the overview of our IDRQA system in Figure~\ref{figure:pipeline}. The IDRQA system first uses a given question as query to retrieve top  documents using TF-IDF. To construct the document graph,
IDR extracts the entities from the question and retrieved documents using an off-the-shelf Named Entity Recognition (NER) system and connects two documents if they have shared entities.
The graph-based reranking model takes the document graph as input to score each document, filter the lowest-scoring documents, and adaptively determines whether to continue the retrieval process. At every future retrieval step, IDR updates the question with an extracted text span from the retrieved documents as a new query to retrieve more documents.
Once the retrieval is done, the final highest-scoring documents are concatenated to feed into the downstream reader module \citep{devlin-etal-2019-bert,Lan2020ALBERT:} for answer extraction.


To describe the pipeline of our IDRQA system more precisely, we provide a concise algorithm that summarizes the inference procedure of the iterative document reranking (IDR) phase, as in Algorithm~\ref{alg:algorithm}. We maintain a set of retrieved documents . For each retrieval step, we retrieve the top  documents according to the question  and then append all the newly retrieved documents to the current set of retrieved documents. Then we use the graph-based reranking model  to rerank those documents, get top  of them, and filter the other ones. This shortlist of retrieved documents together with the question  are then fed in to the downstream reader module  for answer extraction. Note that the reader module has the option to predict that there is no answer to be extracted from the provided documents. If the reader module does predict no answer, we use the question updater model  to update the question with an extracted span and move to the next hop of retrieval. The while loop continues until a valid answer is extracted by the reader module or the maximum number of retrieval hops  is reached.




\begin{figure*}[ht]
\center
\includegraphics[width=\textwidth]{figures/ddr-crop.pdf}
\center
\caption{As the core of our IDR framework, the Graph-based Reranking Model first encodes the question and each retrieved document with pre-trained language model to generate contextual representations, and uses the shared entities to propagate information using a Graph Attention Network (GAT). After the entity-entity interaction across multiple documents, the updated entity representations with the original contextual encodings are fed into the fusion layer for further interaction. Finally, the reranking model takes pooled document representations to score each document and filter low-scoring documents. The maintained high-quality shortlist of remaining documents are then fed into the Reader Module to determine whether the answer exists in them. If so, the retrieval process ends and the QA system delivers the answer span extracted by the Reader Module as the predicted answer. Otherwise, the retrieval process continues to the next hop.}
\label{figure:IDR}
\end{figure*}


\section{IDRQA System} \label{sec:ddr}
















\subsection{Graph-based Reranking Model} \label{subsec:graph-ranking-model}

The graph-based reranking model (Figure~\ref{figure:IDR}) is designed to precisely identify the supporting documents in the document graph. We present the components of this reranking model as follows.



\BlankLine
\noindent \textbf{Contextual Encoding.}
Given a question  and  documents retrieved by TF-IDF, we concatenate the tokens of the question and each document to feed into the pre-trained language model as:

where  and  denote the number of tokens in the question  and the document , respectively.  and  are special tokens used in pre-trained language models such as BERT~\cite{devlin-etal-2019-bert} and ALBERT~\cite{Lan2020ALBERT:}. Thus we independently encode each document  along with the question  to obtain the contextual representation vector , where  is the maximum length of the input tokens , and  is the embedding size.
For efficient batch computation, we pad or truncate the input tokens to the length of .
We then concatenate all documents' contextual representation vectors as .


\BlankLine
\noindent \textbf{Graph Attention.}
After we obtain the question-dependent encoding of each document, we employ a Graph Attention Network (GAT; \citet{velickovic2018graph}) to propagate information on the document graph, where two documents are connected if they have shared entities. To be more specific, for each shared entity  in the document graph, we perform pooling over its token embeddings from  to produce the entity embedding as  where  is the embedding of the -th token in , and  is the number of tokens in . We use both mean- and max-pooling, thus we have . Inspired by \citet{qiu-etal-2019-dynamically}, we apply a dynamic soft mask on the entities, serving as the information ``gatekeeper'' which assigns more weights to entities pertaining to the question. The soft mask applied on each entity  is computed as

where  is the concatenated mean- and max-pooling of the question token embeddings, and  is a linear projection matrix,  is the sigmoid function, and  is the masked entity embedding.
We then use GAT to disseminate information between entities. Starting from , GAT iteratively updates the embedding of each entity with the information from its neighbors as

where  denotes the hidden states of  on the -th GAT layer,  is a linear projection matrix,  is a bias term,  is the set of neighbor entities of , and the entity-entity attention  is computed as follows:

where  is a linear projection matrix. 
We finally obtain the GAT updated entity embeddings , where  is the number of GAT layers.


\BlankLine
\noindent \textbf{Multi-document Fusion.}
To further propagate the information to non-entity tokens, we firs use the embedding of each entity token as

where  is a linear projection matrix. Then we replace the corresponding vectors in  with  to obtain . Finally,  is fed into a Transformer \citep{vaswani2017attention} layer for multi-document fusion, which updates the representations of all the tokens and outputs the fused representation vectors .









\BlankLine
\noindent \textbf{Document Filter.}
For each document, we use the  token embedding from  as the document representation, which is fed into a binary classifier to score the document's supporting level. In each retrieval hop, the top  documents with the highest scores are selected and the rest are filtered.





\subsection{Question Updater}


In open-domain multi-hop QA, the question seldom contains all the retrievable clues and one has to identify the missing information to proceed with further reasoning~\cite{yang-etal-2018-hotpotqa}. To increase the recall of indirect supporting documents, we integrate the query generator of GoldEn Retriever~\citep{qi-etal-2019-answering} into our system, which serves as the question updater at every retrieval step after the initial one.

Question Updater comes after Reader module if no answer was found in top  documents. It aims to generate the \textit{clue span} other than the answer span from the reranked top  documents given current question . Thus conceptually, this process is similar to the Reader Module which extracts answer span from retrieved documents. More specifically, following GoldEn Retriever~\citep{qi-etal-2019-answering}, we use the same method for data construction to train a span extractor model, which extracts the \textit{clue span} from retrieved documents by predicting its start and end tokens. Formally, given the question  and the reranked top  documents , the Question Updater generates the \textit{clue span}  and concatenate  with the original question  as

where  is a special separator token. In order to train the Question Updater, we construct each training sample as a triple of , where  is the question,  is the set of top  retrieved documents reranked by our graph-based reranking model.


\subsection{Reader Module}
In this work, we mainly focus on the retrieval phase, and use a standard span-based reader module as in BERT~\citep{devlin-etal-2019-bert} and ALBERT~\citep{Lan2020ALBERT:}. We concatenate the tokens of the question  and the final top  reranked documents to feed into the reader module. At inference time, the reader finds the best candidate answer span by

where ,  denote the probability that the -th and -th tokens are the start and end positions in the concatenated text, respectively, of the answer span. During inference, there is no guarantee that the answer span exists in the reader's input text. To handle the no-answer cases, the reader predicts  as the probability of having no answer span, and compares  with  to determine the output between a special no-answer prediction and the best candidate answer span.









\section{Experiments}



\subsection{Experimental Settings} \label{graph-base reranking setting}
We conduct all the experiments on a GPU-enabled (Nvidia RTX 6000) Linux machine powered by Intel Xeon Gold 5125 CPU at 2.50GHz with 384 GB RAM. For the graph-based reranking model, we set the maximum token sequence length , the number of retrieved documents , the maximum number of entities . The embedding size  is  and  for the ALBERT-base and ALBERT-xxlarge model, respectively. The graph attention module has  GAT layers. The maximum number of retrieval hops . The top  reranked paragraphs are sent into the downstream reader module.

We implement our system based on HuggingFace's Transformers~\citep{Wolf2019HuggingFacesTS}. Following the previous state-of-the-art method~\cite{Fang2019HierarchicalGN}, we use ALBERT-xxlarge~\citep{Lan2020ALBERT:} as the pre-trained language model. We use AdamW \citep{Wolf2019HuggingFacesTS} as the optimizer and tune the initial learning rate between  and .


\subsection{Datasets} \label{appdx:data}














\setlength\tabcolsep{16pt}
\begin{table*}[]
\centering
\caption{Performance on the HotpotQA full wiki dev set. }
\label{table:overall qa performance on dev}
\begin{tabular}{@{}lccccc@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Answer} & \multicolumn{2}{c}{Supporting Fact} & Paragraph \\ \cmidrule(l){2-6} 
 & EM & F & EM & F & EM \\ \midrule
Cognitive Graph \citep{ding-etal-2019-cognitive} & 37.6 & 49.4 & 23.1 & 58.5 & 57.8 \\
DecompRC \citep{min-etal-2019-multi} & -- & 43.3 & -- & -- & -- \\
MUPPET \citep{feldman-el-yaniv-2019-multi} & 31.1 & 40.4 & 17.0 & 47.7 &  \\
GoldEn Retriever \citep{qi-etal-2019-answering} & -- & 49.7 & -- & -- & -- \\
DrKIT \citep{Dhingra2020Differentiable} & 35.7 & 46.6 & -- & -- & -- \\
Semantic Retrieval MRS \citep{nie-etal-2019-revealing} & 46.5 & 58.8 & 39.9 & 71.5 & 63.9 \\
Transformer-XH \citep{zhaotransxh2020} & 50.2 & 62.4 & 42.2 & 71.6 & -- \\
Recurrent Retriever \citep{asai2020learning} & 60.5 & 73.3 & 49.3 & 76.1 & 75.7 \\
HopRetriever \citep{li2020hopretriever} & 62.1 & 75.2 & \textbf{52.5} & 78.9 & \textbf{82.5} \\
\midrule
IDRQA (ours) & \textbf{62.9} & \textbf{75.9} & 51.3 & \textbf{79.1} & 79.8 \\ \bottomrule
\end{tabular}
\end{table*}







\setlength\tabcolsep{14pt}
\begin{table*}[]
\centering
\caption{Performance on the HotpotQA full wiki test set.}
\label{table:overall QA performance on test}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Answer} & \multicolumn{2}{c}{Supporting Fact} & \multicolumn{2}{c}{Joint} \\ \cmidrule(l){2-7} 
 & EM & F & EM & F & EM & F \\ \midrule
DecompRC \citep{min-etal-2019-multi} & 30.0 & 40.6 & -- & -- & -- & -- \\
QFE \citep{nishida-etal-2019-answering} & 28.6 & 38.0 & 14.2 & 44.3 & 8.6 & 23.1 \\
Cognitive Graph \citep{ding-etal-2019-cognitive} & 37.1 & 48.8 & 22.8 & 57.6 & 12.4 & 34.9 \\
MUPPET \citep{feldman-el-yaniv-2019-multi} & 30.6 & 40.2 & 16.6 & 47.3 & 10.8 & 27.0 \\
GoldEn Retriever \citep{qi-etal-2019-answering} & 37.9 & 48.5 & 30.6 & 64.2 & 18.0 & 39.1 \\
DrKIT \citep{Dhingra2020Differentiable} & 42.1 & 51.7 & 37.0 & 59.8 & 24.6 & 42.8 \\
Semantic Retrieval MRS \citep{nie-etal-2019-revealing} & 45.3 & 57.3 & 38.6 & 70.8 & 25.1 & 47.6 \\
Transformer-XH \citep{zhaotransxh2020} & 51.6 & 64.0 & 40.9 & 71.4 & 26.1 & 51.2 \\
Recurrent Retriever \citep{asai2020learning} & 60.0 & 73.0 & 49.0 & 76.4 & 35.3 & 61.1 \\
Hierarchical Graph Network \citep{Fang2019HierarchicalGN} & 59.7 & 71.4 & 51.0 & 77.4 & 37.9 & 62.2 \\
HopRetriever \citep{li2020hopretriever} & 60.8 & 73.9 & 53.1 & 79.3 & 38.0 & 63.9 \\
Multi-hop Dense Retrieval \citep{xiong2021answering} & 62.3 & 75.3 & \textbf{57.5} & \textbf{80.9} & \textbf{41.8} & \textbf{66.6} \\
\midrule
IDRQA (ours) & \textbf{62.5} & \textbf{75.9} & 51.0 & 78.9 & 36.0 & 63.9 \\ \bottomrule
\end{tabular}
\end{table*}












\setlength\tabcolsep{14pt}
\begin{table}[ht]
\centering
\caption{Answer EM scores on the test set of Natural Questions Open and SQuAD Open.}
\label{table:single hop QA performance on test}
\begin{tabular}{@{}lcc@{}}
\toprule
Method & NQ & SQuAD \\ \midrule
DrQA \citep{chen-etal-2017-reading} & -- & 29.8 \\
R \citep{wang2018r} & -- & 29.1 \\
Multi-step Reasoner \citep{das2018multistep} & -- & 31.9 \\
BERTserini \citep{yang-etal-2019-end-end-open} & -- & 38.6 \\
MUPPET \citep{feldman-el-yaniv-2019-multi} & -- & 39.3 \\
Multi-passage BERT \citep{wang-etal-2019-multi} & -- & 53.0 \\
ORQA \citep{lee-etal-2019-latent} & 33.3 & 20.2 \\
Recurrent Retriever \citep{asai2020learning} & 31.7 & \textbf{56.5} \\
Dense Passage Retriever \citep{karpukhin2020dense} & 41.5 & 36.7 \\
\midrule
IDRQA (ours) & \textbf{45.5} & \textbf{56.6}  \\ \bottomrule
\end{tabular}                                                                  
\end{table}




We evaluate our method on three open-domain QA benchmark datasets: HotpotQA~\cite{yang-etal-2018-hotpotqa}, Natural Questions Open~\cite{lee-etal-2019-latent} and SQuAD Open~\cite{chen-etal-2017-reading}. On all the three datasets, we focus on the \textit{full wiki} open-domain QA setting, which requires the system to retrieve evidence paragraphs from the entire Wikipedia and extract the answer span from the retrieved paragraphs.

Following the train / dev / test splits of Natural Questions Open and SQuAD Open in previous works~\cite{lee-etal-2019-latent,karpukhin2020dense}, we use the original validation set as our test set and keep 10\% training set as our dev set. Natural Questions Open and SQuAD open consist of single-hop questions, while HotpotQA consists of 113K crowd-sourced multi-hop questions that require Wikipedia introduction paragraphs to answer.
In the train and dev splits of HotpotQA, each question for training comes with two gold supporting paragraphs annotated by the crowd workers. Thus we can evaluate the retrieval performance on the dev set of HotpotQA dataset.






\subsection{Data Preprocessing}

Here we describe the details of data preprocessing methods that we develop for the HotpotQA dataset.

\BlankLine
\noindent \textbf{Training data construction for the graph-based reranking model.}
Graph-based reranking model aims to precisely score the retrieved documents by considering multiple documents as a whole instead of independent instances. In order to make our model reusable and robust to new test cases, we carefully design the training data construction method. To keep the distribution of training and test data consistent, we add negative samples to the training data for our graph-based reranking model. Formally, We pair each question  with a set of documents  to form a training sample. We design each training sample with the following strategies: 1) For each training sample which contains all supporting documents, we pair the question  with  where  includes all  supporting documents necessary for multi-hop QA and  noisy documents.  is a set of documents in training.  are the number of noisy documents and the number of supporting documents; 2) For each training sample which contains partial supporting documents, we paired question  with  where  contains  supporting documents and  noisy documents. supporting documents are randomly sampled from all supporting documents; 3) For noisy documents, we sample them according to their score given by TF-IDF; 4) For multi-step reranking, we also randomly sample 30\% questions and use Question Updater to generate \textit{the clue span} and update questions. These new questions and their original paired documents are also used as training samples; 5) We concatenate all documents in  in random order, rather than the reranked order. We use  and  in our experiments.


\BlankLine
\noindent \textbf{Training data construction for the reader module.}
We designed the training data for the Reader Module carefully since the Reader module is not only used to predict the final answer but also to tell that \textit{no answer} found in in current context. The training sample is a triple of  where  is question,  is 4 documents feed into QA model and  is the final answer. For each question, we construct 5 types of training sample: 1) We concatenate two supporting passages (ordered as originally in the dataset) and two highest TF-IDF scored negative passages (ordered from higher to lower reranking score) as training samples; 2) We construct a random shuffled version of the 1st type in passage level; 3) We randomly replace one of the supporting passages of the 1st type by a negative passage; 4) One passage which has the final answer and is not one of the supporting passage and three negative passages; 5) We construct four negative passages with high TF-IDF score which are not supporting passages and do not contain the final answer.


\subsection{Evaluation Metrics}

We evaluate both the paragraph reranking performance and the overall QA performance. Following the existing studies~\cite{asai2020learning,nie-etal-2019-revealing,nishida-etal-2019-answering}, we evaluate the paragraph-level retrieval accuracy using the Paragraph Exact Match (EM) metric, which compares the top 2 paragraphs with the gold supporting paragraphs. For the QA performance, we report standard answer Exact Match (EM) and F scores to measure the overlap between the gold answer and the extracted answer span.



\subsection{Overall Results}

We evaluate our method on both single-hop and multi-hop open-domain QA datasets. Note that our method is hop-agnostic, i.e., we consistently use the same setting of  as the maximum number of retrieval hops for all the three datasets.

For the multi-hop QA task, we report performance on both the dev and test sets of the HotpotQA dataset.
As reported in Table~\ref{table:overall qa performance on dev}, we compare the performance of our system IDRQA with various existing methods on the HotpotQA full wiki dev set. Since the golden supporting paragraphs are only labeled on the train / dev splits of the HotpotQA dataset, we can only report the paragraph EM metric on the dev set.
In Table~\ref{table:overall QA performance on test}, we compare our system with various existing methods on HotpotQA full wiki test set. IDRQA outperforms all published and previous unpublished methods on the HotpotQA dev set and the hidden test set on the official leaderboard (on May 21, 2020). 
Note that we focus on the QA task in this paper, and for the supporting fact prediction task we simply concatenate each pair of question and supporting fact to train a binary classification model. The joint EM and F scores combine the evaluation of answer spans and supporting facts as detailed in \cite{yang-etal-2018-hotpotqa}. Thus we are behind state-of-the-art performance on supporting fact and joint scores.

For the single-hop QA task, we evaluate our method on Natural Questions (NQ) Open and SQuAD Open datasets. We summarize the performance in Table~\ref{table:single hop QA performance on test}. For NQ Open, we follow the previous work DPR~\cite{karpukhin2020dense} to use dense retriever instead of TF-IDF for document retrieval. Our method also achieves QA performance comparable to or better than the state-of-the-art methods on both datasets, which shows the robustness of our method across different open-domain QA datasets.


\begin{figure*}[ht]
\center
\includegraphics[width=.95\textwidth]{figures/case_study-crop.pdf}
\center
\caption{Case study of example questions with supporting paragraphs from HotpotQA dev set.}
\label{figure:case study}
\end{figure*}






\setlength\tabcolsep{12pt}
\begin{table*}
\centering
\caption{Ablation study of our system in different settings on HotpotQA full wiki dev set.}
\label{table:ablation comparison and bridge}
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multirow{3}{*}{Ablation Setting} & \multicolumn{2}{c}{Bridge (79.9\%)} & \multicolumn{2}{c}{Comparison (20.1\%)} & \multicolumn{3}{c}{Full Dev (100\%)} \\ \cmidrule(l){2-8} 
 & \multicolumn{2}{c}{Answer} & \multicolumn{2}{c}{Answer} & \multicolumn{2}{c}{Answer} & Paragraph \\ \cmidrule(l){2-8} 
 & EM & F & EM & F & EM & F & EM \\ \midrule
IDRQA & 58.1 & 73.2 & 71.4 & 77.9 & 60.7 & 74.2 & 73.8 \\
\ \ \ w/o Graph-based Reranking & 47.4 & 59.5 & 70.1 & 76.4 & 52.1 & 62.9 & 62.6 \\
\ \ \ w/o Iterative Reranking & 49.8 & 61.7 & 70.5 & 77.3 & 54.3 & 64.5 & 65.2 \\
\ \ \ w/o Question Updater & 56.6 & 71.4 & 71.3 & 77.8 & 59.8 & 72.9 & 70.3 \\ \bottomrule
\end{tabular}
\end{table*}






\subsection{Detailed Analysis}


\BlankLine
\noindent \textbf{Ablation study.}
To investigate the effectiveness of each module in IDRQA, we compare the performance of several variants of our system on HotpotQA full wiki dev set.
As shown in Table \ref{table:ablation comparison and bridge}, once we disable the \emph{Iterative Reranking}, \emph{Graph-based Reranking} and \emph{Question Updater} module, both the paragraph reranking and QA performance drop significantly. Notably, there is a 11 points drop in Paragraph EM decrease and a 12 points drop in Answer F when \emph{Graph-based Reranking} is removed from IDRQA. This shows the importance of the graph-based reranking model in our system.
To further study the impact of these modules, we decompose the QA performance into question categories \textit{Bridge} and \textit{Comparison}. We find that the QA performance on the bridge questions drops much more significantly than that on the comparison questions. This is because the comparison questions require to compare two entities mentioned in the question~\cite{yang-etal-2018-hotpotqa}, thus iterative retrieval and multi-hop reasoning may not be necessary. In contrast, to answer the bridge questions which often have missing entities, our iterative graph-based document reranking method is of crucial importance.























\BlankLine
\noindent \textbf{Impact of retrieval steps.}
IDRQA aggregates the document scores to check whether the collected evidence is enough to answer the question, and adaptively determines when to stop the retrieval process. We investigate the number of retrieval steps selected by IDRQA, and report its distribution with breakdown performance in Table~\ref{table:stats_dynamic}. Over 60\% questions are answered with 2-step retrieval. About 20\% questions are answered with 1-step retrieval, which is close to the ratio of the comparison questions that may not need iterative retrieval. For questions that IDRQA selects to perform over 2-step retrieval, a significant drop on both reranking and QA performance is observed, showing that these questions are the hardest ones in HotpotQA.













\setlength\tabcolsep{6pt}
\begin{table}
\centering
\caption{Distribution of the selected retrieval steps on HotpotQA dev set, which is adaptively determined by IDRQA.}
\label{table:stats_dynamic}
\begin{tabular}{@{}lcccc@{}}
\toprule
\multirow{2}{*}{Retrieval} & \multirow{2}{*}{\% of Questions} & Paragraph & \multicolumn{2}{c}{Answer} \\ \cmidrule(l){3-5} 
 &  & EM & EM & F \\ \midrule
1-step & 21.8 & 88.4 & 70.3 & 81.9 \\
2-step & 63.2 & 76.9 & 62.0 & 75.9 \\
3-step & 5.3 & 39.6 & 45.2 & 59.8 \\
4-step (max) & 9.7 & 22.0 & 37.7 & 50.4 \\ \bottomrule
\end{tabular}
\end{table}





\BlankLine
\noindent \textbf{Case study and limitations.}
We showcase several example questions with answers from IDRQA and the baseline ALBERT-base reranker (ABR) in Figure~\ref{figure:case study}. The first case is a hard bridge question where ABR extracts the wrong answer from a relevant but non-supporting paragraph, showing the advantage of iterative reranking in our system.
The second question is correctly answered by both IDRQA and ABR, since it provides sufficient clues to retrieve both paragraphs.
The final case is a comparison question that requires numerical reasoning, which is not correctly answered by both IDRQA and ABR. This shows the limitation of our system, and we plan to explore the combination of multi-hop and numerical reasoning in future work.







\section{Related Work}



\noindent \textbf{Open-domain QA.}  For text-based QA, the QA system is provided with semi-structured or unstructured text corpora as the knowledge source to find the answer. Text-based QA dates back to the QA track evaluations of the Text REtrieval Conference (TREC)~\cite{voorhees2000building}. Traditional approaches for text-based QA typically use a pipeline of question parsing, answer type classification, document retrieval, answer candidate generation, and answer reranking, such as the famous IBM Watson system which beat human players on the \textit{Jeopardy!} quiz show~\cite{ferrucci2010building}. However, such pipeline-based QA systems require heavy engineering efforts and only work on specific domains.
The open-domain QA task was originally proposed and formalized in \citet{chen-etal-2017-reading}, which builds a simple pipeline with a TF-IDF retriever module and a RNN-based reader module to produce answers from the top 5 retrieved documents. Different from the machine reading comprehension task (MRC) that provides a single paragraph or document as the evidence~\cite{rajpurkar-etal-2016-squad}, open-domain QA is more challenging since the retrieved documents are inevitably noisy. Recent works on open-domain QA largely follow the retrieve-and-read approach, and have made prominent improvement on both the retriever module~\cite{lee-etal-2019-latent,wang2018r,nie-etal-2019-revealing} and the reader module~\cite{wang2018evidence,wang2019sigir,ni-etal-2019-learning}. These approaches simply perform one-shot document retrieval to handle single-hop questions. However, for complicated questions that require multi-hop reasoning, these approaches are not applicable since they fail to collect necessary evidence scattered among multiple documents.

\BlankLine
\noindent \textbf{Open-domain multi-hop QA.} HotpotQA~\cite{yang-etal-2018-hotpotqa} is crowd-sourced over Wikipedia as the largest free-form dataset for open-domain multi-hop QA to date. Recently, a variety of approaches have been proposed to address the multi-hop challenge.
DecompRC~\cite{min-etal-2019-multi} decomposes a multi-hop question into simpler sub-questions and leverages single-hop QA models to answer it, which still uses one-shot TF-IDF retrieval to collect relevant documents. BERT Reranker~\cite{das-etal-2019-multi}, DrKIT~\cite{Dhingra2020Differentiable} and Transformer-XH~\cite{zhaotransxh2020} employ the entities in the question and the retrieved documents to link additional documents, which expands the one-shot retrieval results and improves the evidence coverage. GoldEn Retriever~\cite{qi-etal-2019-answering} adopts iterative TF-IDF retrieval by generating a new query for each retrieval step. Graph Retriever~\cite{min2019knowledge} employs entity linking to iteratively retrieve and construct a graph of documents. These iterative retrieval methods mitigate the recall problem of one-shot retrieval, however, without iterative reranking and filtering process, the expanded documents inevitably introduce noise to the downstream QA model. Multi-step Reasoner~\cite{das2018multistep} and MUPPET~\cite{feldman-el-yaniv-2019-multi} read retrieved documents to reformulate the query in latent space for iterative retrieval. However, these embedding-based retrieval methods have difficulties to capture the lexical information in entities due to the compression of information into embedding space. Moreover, these methods perform a fixed number of retrieval steps, which are not able to handle questions that require arbitrary hops of reasoning. Recurrent Retriever~\cite{asai2020learning} supports adaptive retrieval steps, but can only select one document at each step and has no interactions among documents outside of the retrieval chain. In contrast, our method leverages document graph instead of chain to propagate information, reranks and filters documents at each hop of retrieval, and terminates the retrieval process according to the number of positive retrieved documents in the graph.


\BlankLine
\noindent \textbf{Graph Neural Networks for QA.} Encouraged by the success of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), graph neural networks (GNNs) are proposed to generalize both of them and organize the connections of neurons according to the structure of the input data~\cite{battaglia2018relational,wu2020comprehensive}. The main idea is to generate a node's representation by aggregating its own features and neighbors' features. Similar to CNNs, in GNNs, multiple graph convolutional layers can be stacked to produce high-level node representations. Many popular variants of GNNs are proposed, such as GraphSage~\cite{hamilton2017inductive}, Graph Convolutional Network (GCN)~\cite{Kipf:2016tc}, Graph Attention Network (GAT)~\cite{velickovic2018graph}, etc. GNNs have achieved great success on graph-related tasks such as node classification, node regression and graph classification.
Recently, Graph neural networks (GNNs) have been shown effective on knowledge-based QA tasks by reasoning over graphs~\cite{de-cao-etal-2019-question,sorokin-gurevych-2018-modeling,zhang2018variational}. Recent studies on text-based QA also leverage GNNs for multi-hop reasoning. HDE-Graph~\cite{tu-etal-2019-multi} constructs the graph with entity and document nodes to enable rich information interaction. CogQA~\cite{ding-etal-2019-cognitive} extracts candidate answer spans and next-hop entities to build the cognitive graph for reasoning. HGN~\cite{HGN2019} employs a hierarchical graph that consists of paragraph, sentence and entity nodes for reasoning on different granularities. DFGN~\cite{qiu-etal-2019-dynamically} introduces a fusion layer on top of the entity graph with a mask prediction module. Graph Retriever~\cite{min2019knowledge} uses graph convolution network to fuse information on the entity-linked graph of passages. Multi-grained MRC~\cite{zheng-etal-2020-document} utilizes GATs to obtain different levels of representations of documents for machine reading comprehension. These GNN-based methods serve as the \textit{reader} module to extract answers from a few documents. In contrast, our work employs graph attention network, a popular GNN variant, in the \textit{retriever} module. To the best of our knowledge, we are the first to propose GNN-based document reranking method for open-domain multi-hop QA.





\section{Conclusion}

We present a QA framework that can answer any-hop open-domain questions, which iteratively retrieves, reranks and filters documents with a graph-based reranking model, and adaptively decides how many steps of retrieval and reranking are needed for a multi-hop question. Our method consistently achieves promising performance on both single- and multi-hop open-domain QA datasets.


\tiny
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}


\end{document}
