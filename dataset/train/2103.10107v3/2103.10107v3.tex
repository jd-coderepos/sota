\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}
\usepackage{fontawesome}
\usepackage{enumitem} 



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\iccvfinalcopy 

\def\iccvPaperID{10103} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Danish Fungi 2020 - Not Just Another Image Recognition Dataset}


\author{
Luk\'{a}\v{s} Picek \\ University of West Bohemia \\ {\tt\small picekl@kky.zcu.cz} 
\and Milan \v{S}ulc, Ji\v{r}\'{i} Matas \\ CTU in Prague \\ {\tt\small {{sulcmila,matas}@fel.cvut.cz}}
\and Thomas S. Jeppesen \\ GBIF \\ {\tt\small tsjeppesen@gbif.org}
\and Jacob Heilmann-Clausen, Thomas Læssøe, Tobias Frøslev \\ University of Copenhagen \\ {\tt\small jheilmann-clausen@snm.ku.dk, thomasl@bio.ku.dk, tobiasgf@sund.ku.dk}





}







\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}

We introduce a novel fine-grained dataset and benchmark, the Danish Fungi 2020 (DF20). The dataset, constructed from observations submitted to the Danish Fungal Atlas, is unique in its taxonomy-accurate class labels, small number of errors, highly unbalanced long-tailed class distribution, rich observation metadata, and well-defined class hierarchy. DF20 has zero overlap with ImageNet, allowing unbiased comparison of models fine-tuned from publicly available ImageNet checkpoints. The proposed evaluation protocol enables testing the ability to improve classification using metadata -- e.g. precise geographic location, habitat, and substrate, facilitates classifier calibration testing, and finally allows to study the impact of the device settings on the classification performance. 
Experiments using Convolutional Neural Networks (CNN) and the recent Vision Transformers (ViT) show that DF20 presents a challenging task. Interestingly, ViT achieves results superior to CNN baselines with 81.25\% accuracy, reducing the CNN error by 13\%. A baseline procedure for including metadata into the decision process improves the classification accuracy by more than 3.5 percentage points, reducing the error rate by 20\%. The source code for all methods and experiments is available at \url{https://sites.google.com/view/danish-fungi-dataset}.

\end{abstract}


\section{Introduction}
Publicly available datasets and benchmarks accelerate machine learning research and allow for quantitative comparison of novel methods. In the area of deep learning and computer vision, the rapid progress over the past decade was, to a great extent, facilitated by the publication of large-scale image datasets. In the case of image recognition, the formation of the ImageNet\,\cite{imagenet} database and its usage in the ILSVRC\footnote{\,The ImageNet Large Scale Visual Recognition Challenge.}\,challenge\,\cite{ILSVRC15}, together with PASCAL\,VOC\,\cite{voc} and Caltech-256\,\cite{griffin2007caltech} among others, helped start the CNN revolution. The same holds for the problem of fine-grained visual categorization\,(FGVC), where datasets and challenges like PlantCLEF\,\cite{plantclef2016, plantclef2017, plantclef2015}, iNaturalist\,\cite{inaturalist2017}, CUB\,\cite{dataset-CUBS}, and Oxford Flowers \,\cite{dataset-flower}, have helped to develop and evaluate novel approaches to fine-grained domain adaptation\,\cite{domain_adap}, domain specific transfer learning\,\cite{transfer_learning}, image retrieval\,\cite{ft_imagenet_3, sohn2016improved, zhai2019classification}, unsupervised visual representation, few-shot learning \,\cite{wertheimer2019few}, transfer learning\,\cite{transfer_learning}, prior-shift\,\cite{sulc2018improving} and many others.

\begin{figure}[!t]
  \begin{center}
  \renewcommand{\arraystretch}{0.4}
  \begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}}
\includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2237851963-3.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2986451309-147236.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2237853733-428.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2237852385-148534.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2237859161-224212.jpg}\hspace{2px}
  \end{tabular}
  \begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}}
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Hard/2238584635-187064.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/10252_Amanita_muscaria.JPG}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2237855636-223478.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2237856898-1169.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2238403840-163381.jpg}\hspace{2px}
  \end{tabular}
  \begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}}
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2238491204-95537.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2238584641-261203.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2986409373-73320.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/14337_Glyphium_elatum_JAH2016-9176815_Byea3EqpT.JPG}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2986451320-221552.jpg}\hspace{2px}
  \end{tabular}
  \begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}}
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Hard/2237855204-75346.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Hard/2237930428-3527.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Hard/2238559322-331261.jpg}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/10257_Amanita_pantherina_BWP2013PIC77498444.JPG}\hspace{2px} &
    \includegraphics[width=0.195\linewidth, height=0.195\linewidth]{fgvcx_images/Perfect/2868488323-65739.jpg}\hspace{2px}
  \end{tabular}
  \end{center}
  \caption{Selected images from the \textbf{DF20 dataset}. For visualisation, max-center-squares were extracted from the images.   
}
  \label{fig:short}

\end{figure}

While the datasets have been extremely useful for the image recognition community, there are issues that limit their relevance to real-world applications. We mention several such problems. Flat class priors, common in research datasets, are rare in practice. Often, class prior distributions are the same in the training and test splits. This is a standard machine learning assumption that, nevertheless, is not valid if the collection of training data differs from the deployment of the trained system, which is not rare. Non-negligible percentage of noisy-labels restricts quality assessment\,\cite{done_with_imagenet}, and, despite CNN's surprising robustness to label noise\,\cite{unreas_effec}, may influence the perceived relative merit of learning algorithms. Some commonly used datasets\,\cite{imagenet, dataset-Cars, dataset-flower} are saturated in accuracy or close to the point, leaving limited space for improvement in future research\,\cite{done_with_imagenet}. 
Extremely large dataset sizes might discourage researchers that do not have access to massive computational resources as experiments have become time and hardware demanding. \\

With these observations in mind, we introduce {\bf the DF20 dataset} with a number of unique characteristics. Its class labels are exceptionally accurate, annotated by domain experts. Estimates on a random subset show about 0.2\% error; for the DNA sequenced private test set the labelling error is 0\%. The minimal error levels allow highly accurate performance evaluation. 
With its zero overlap with ImageNet, it allows an unbiased comparison of models fine-tuned from publicly available ImageNet checkpoints.

The class frequencies in DF20 follow the natural species distribution, which is long-tailed. The frequencies change significantly within the calendar year, making the data suitable for testing the response of the classifier to differing long tailed distributions and changing class priors. 
The continuous data flow of collection over a long period starting from 1874 provide a ground for modelling and exploiting the temporal phenomena on different scales, e.g., month, season, year.

The visual data is accompanied with metadata for more than 99\% of the image observations. The rich metadata include information about 20 attributes related to the environment, place, time and full taxonomy labels.
The metadata enable to test the ability to improve classification accuracy using different metadata types -- time, precise location, habitat, and substrate type, to perform hierarchical classification, evaluate fine-grained classification on different levels of granularity (taxonomic ranks), to test classifier calibration, and to model intra-metadata and metadata-visual appearance relationships.
Moreover, EXIF metadata is available for many observations, which is useful, e.g., for studying the impact of the device settings on classification performance.


\textbf{The DF20 Benchmark.}
To allow evaluation at any time, we have prepared a web-based public automatic benchmark\footnote{\,\url{https://aicrowd.com/challenges/danish-fungi}} for different scenarios, including visual-based, metadata focused or classifier-calibration related research.
Besides the full benchmark, we introduce DF20\,-\,mini, a small subset with roughly 1/10 of the data and species, for fast, low-energy friendly prototyping. DF20\,-\,mini includes six well-known genera of fungi forming fruit-bodies of the toadstool type, and offers, surprisingly, an even more challenging problem then the full benchmark, while having a compact size. 

We prepared a baseline performance evaluation, including the quantitative and qualitative analysis of the results for a number of well-known CNN and recent ViT architectures\,\cite{vit}. The recent ViT achieves excellent results in fine-grained classification outperforming the state-of-the-art CNN classifiers. We show that ViT performs way better on the FGVC domain, where attention to detail is needed, than in a common object recognition. 
We show that both the DF20 and DF20\,-\,Mini benchmarks are far from saturated as the best performing model - ViT-Large/16-384 - achieved 81.25\% and 75.32\% accuracy on DF20 and DF20\,-\,Mini, respectively.  
We propose a baseline solution for processing the habitat, substrate and time (month) metadata, showing that -- even with the baseline approach -- utilizing the metadata increases the classification performance significantly. To support and accelerate future research on the DF20 we open sourced the code through the public GitHub repository\footnote{\,\url{https://github.com/picekl/DanishFungiDataset}}.

\section{Related Work}

This section overviews existing fine-grained image datasets. Fine-grained datasets, unlike datasets with common object classes such as - dog, plant, aircraft, or vehicle\,\cite{imagenet, voc, cifar}, are characterized by classes with small-interclass differences and huge intra-class similarity. Currently, there exists a number of FGVC dataset with a focus on plants\,\cite{plantclef2017, vegfru_dataset, plantclef2015, leafsnap, dataset-flower}, animals\,\cite{dataset-dataset-DOGS, nabirds_dataset, dataset-CUBS}, cars\,\cite{dataset-Cars,vmmrdb} or airplanes\,\cite{aircafts}. They feature object classes within one meta-category,i.e., dog breeds, vehicle models and plant or animal species. The dataset statistics are compared in Table\,\ref{table:fgvc_datasets}. Most of the datasets are artificially constructed to have a flat class distribution. Many datasets use web scraped data that may contain out-of-domain images and wrong labels.

\begin{table}[!t]
\begin{center}
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{0.4em}
\begin{tabular}{|l|r|r|r|}
\hline
Name & \# Classes & \# Training & \# Test \\
\hline
FGVC-Aircafts\,\cite{aircafts}              &  102   &  6,732  &  3,468   \\
Standford Cars\,\cite{dataset-Cars}          &  196   &  8,144  &  8,041   \\
VMMRdv\,\cite{vmmrdb}                        &  9,170    &  291,752  &     \\
\hline
Oxford Flowers\,\cite{dataset-flower}              &    102 &   1,020 &   7,169  \\
Stanford Dogs\,\cite{dogsnap_dataset}              &    120 &  12,000 &   8,580  \\
DogSnap\,\cite{dogsnap_dataset}                    &    133 &   4,776 &   3,575  \\
LeafSnap\,\cite{leafsnap}                          &    185 &  30,866 &  \\
CUB 200-2011\,\cite{dataset-CUBS}                  &    200 &   5,994 &    5,784 \\
VegFru\,\cite{vegfru_dataset}                      &    292 &  29,200 & 116,931  \\
NABirds\,\cite{nabirds_dataset}                    &    555 &  48,562 &  \\
PlantCLEF 2015\,\cite{plantclef2015}               &  1,000 &  91,758 &  21,446  \\
iNaturalist 2017\,\cite{inaturalist2017}           &  5,089 & 579,184 &  95,986  \\
PlantCLEF 2017{}\,\cite{plantclef2017} & 10,000 & 230,658 &  25,629  \\
\hline \hline
\textbf{DF20 - Mini}        &    182 &  32,753 &   3,640  \\
\textbf{DF20}               &  1,604 & 248,466 &  27,608  \\
\hline
\end{tabular}
\end{center}
\caption{Overview of publicly available FGVC datasets, both nature-related (middle section) and other (top), and the number of images and categories. {}\,Including only images with clean labels.}
\label{table:fgvc_datasets}
\end{table}

\textbf{Labels.} As species-level labels are essential for usage in real-world applications, the tedious labelling procedure often rely on domain experts. With just a small number of experts and their limited time, the labelling process is frequently delegated to crowd-sourced annotation platforms such as the Amazon Mechanical Turk\,\cite{imagenet, dataset-dataset-DOGS, dataset-CUBS}. The main drawback of this approach is related to poor domain knowledge of the annotators that results in a high number of noisy labels\,\cite{nabirds_dataset} -- 4.4\% in CUB\,200-2011  and approximately 4.0\% for fine-grained classes in ImageNet. To address this issue, more recent datasets use citizen-science platforms and their users - citizen scientists\footnote{\,Domain specific nonprofessional enthusiasts - experts.} - to label data with high-quality annotations\,\cite{nabirds_dataset, inaturalist2017}.

\textbf{ImageNet Overlap.} Using different data for training and testing is a standard in the field of image recognition. Nevertheless, the data overlap, in fine-tuning, where ImageNet weights are primarily used, is commonly overlooked even though there is substantial overlap for multiple datasets\,\cite{vegfru_dataset, dogsnap_dataset, dataset-CUBS}. Interestingly, a number of publications with high impact used ImageNet weights and performed the fine-tuning and testing with the CUB\,200-2011\,\cite{dataset-CUBS} dataset that overlaps with the ImageNet\,\cite{ft_imagenet_7, ft_imagenet_2, ft_imagenet_4, ft_imagenet_3, ft_imagenet_5, ft_imagenet_6, ft_imagenet_8, ft_imagenet_9, ft_imagenet_1}.

\textbf{Metadata.} Besides images and class labels, image classification datasets often provide additional metadata, such as higher taxon labels\,\cite{plantclef2015, inaturalist2017}, label hierarchy\,\cite{imagenet, vegfru_dataset, aircafts}, object parts and attribute annotations\,\cite{plantclef2015, nabirds_dataset, dataset-CUBS}, masks\,\cite{dataset-CUBS}, GPS location\,\cite{plantclef2015}, and time of observation\,\cite{plantclef2015}. The existence of such metadata enables the usage of these datasets in machine learning research beyond image classification. For example\,\cite{Aodha_2019_ICCV, bargoti2016image, ellen2019improving} use location context, and \,\cite{goo2016taxonomy} use taxonomy labels.

\section{Danish Fungal Atlas}

The Danish Fungal Atlas (Svampeatlas)\,\cite{svampe_databasen, svampeatlas_data, heilmann2019citizen} involves more than 3,000 volunteers who have contributed with more than 900,000 quality-checked observations of fungi (many including images) since 2009. Data from previous years has also been included.

The project has resulted in a vastly improved knowledge of Denmark's fungi\,\cite{heilmann2019citizen}. More than 180 species belonging to \textit{Basidiomycota}\,\footnote{\,a group of fungi that produces their sexual spores (basidiospores) on a club-shaped spore-producing structure (basidium).} have been added to the list of known Danish species, and several species that were considered extinct have been re-discovered. Simultaneously, several search and assistance functions have been developed that present features relating to the individual species, making it much easier to include an understanding of endangered species in nature management and decision-making.

All validated records are published to the Global Biodiversity Information Facility (GBIF) every week, since 2017. The database includes more than 372,154 observation with images till this day.

\section{Annotation Process}

The Danish Fungal Atlas uses an interactive labelling method for all submitted observations. When a user submits a fungal sighting (record) at species level, a "reliability score" (1--100) is calculated based on following factors:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=0.5cm]
    \item Species rarity, \ie its relative frequency in the Atlas.
    \item The geographical distribution of the species.
    \item Phenology of the species, its seasonality.
    \item User's historical species-level proposal precision.
    \item As above, within the proposal's higher taxon rank.
\end{itemize}
Subsequently, other users may agree with the proposed species identity, increasing the identification score following the same principles, or proposing alternative identification for non-committal suggestions.  Once the submission reaches a score of 80, the label (identification) is internally approved. Simultaneously, a small group of taxonomic experts (validators) monitor most of the observation on their own. These have the power to approve or reject species identifications regardless of the score in the interactive validation. This can be relevant for discoveries of new species, rare species, and records of species where experience or sequencing of genetic material (DNA) is required for safe identification. Since 2019, the Danish Fungal Atlas' observation identification is simplified thanks to an image recognition system\,\cite{fungiUsecase}.

\section{Dataset Description}

The \textbf{Danish Fungi 2020} (DF20) dataset contains image observations from the Danish Fungal Atlas belonging to species with more than 30 images. 
The dataset represents real observations dated from 1874 to the end of 2020, coming from more than 30 countries, and  including samples from all seasons. It consists of 295,938 images belonging to 1,604 species from the \textit{Fungi}, \textit{Protozoa} and \textit{Chromista} kingdoms. While the DF20 dataset uses approximately 80\% of the Svampeatlas database images, it represent just 27\% of the species. For a quantitative summary of the data selection, see Table \ref{table:dataset_stats}.
The most frequent species -- \textit{Trametes versicolor} -- is represented by 1,913 images and the least present with 31.

Additionally, we hand-picked a subset of 36,393 images belonging to 182 species from 6 genera with a similar visual appearance. This compact dataset, \textbf{DF20\,-\,Mini}, introduces a challenging fine-grained recognition task, while allowing to decrease the necessary training times and hardware requirements.
As species in the same genus are most likely to be confused, we used six well-known genera of fungi forming fruit-bodies of the toadstool type (\textit{Russula}, \textit{Boletus}, \textit{Amanita}, \textit{Clitocybe}, \textit{Agaricus} and \textit{Mycena}) for the construction of the DF20\,-\,Mini. The most frequent species in the DF20\,-\,Mini dataset -- \textit{Mycena galericulata} -- has 1,221 images, the minimum is again 31 images per class. 

The DF20 and DF20\,-\,Mini datasets were randomly split into the provided training and (public) test sets, where the training set contains  of images of each species.

\begin{table}[t]
\setlength{\tabcolsep}{0.325em}
\renewcommand{\arraystretch}{1.1}
\begin{center}
\begin{tabular}{|l|r|r|r|r|}
\cline{2-5}
\multicolumn{1}{l|}{ } & \multicolumn{1}{c|}{\# Images} &  \multicolumn{1}{c|}{\# Species} & \multicolumn{1}{c|}{\# Genera} & \multicolumn{1}{c|}{\# Family} \\
\hline
Svampeatlas   & 372,154 & 5,923 & 1,417 & 368 \\
DF20           & 295,938 & 1,604 &   566 & 190 \\
DF20 - Mini    &  36,393 &   182 &     6 &   6 \\
\hline
\end{tabular}
\end{center}
\caption{Number of images, species, genera and families in the Danish Fungal Atlas and their subsets DF20 and DF20 - Mini.}
\label{table:dataset_stats}
\end{table}


\begin{table}[tbh]
\begin{center}
\setlength{\tabcolsep}{0.28em} \renewcommand{\arraystretch}{1.2}
\begin{tabular}{| r  p{6.0cm}@{\hskip 5pt}  |}
    \hline
    \textbf{Attribute} & \textbf{Description} \\
    \hline
        \small{\textbf{EventDate}} & \small{Date of observation.} \\
        \small{\textbf{IdentifiedBy}} & \small{Name of the user that identified the specimen.} \\
        \small{\textbf{EXIF}} & \small{Camera device attributes extracted from the image, e.g., metering mode, color space, device type, exposure time, and shutter speed.} \\
        \hline
        \small{\textbf{Habitat}} & \small{The environment where the specimen was observed. Selected from 32 values such as Mixed woodland, Deciduous woodland etc.}\\
        \small{\textbf{Substrate}} & \small{The natural substance on which the specimen lives. Selected from 32 values such as Bark of a living tree, Soil, Stone,  etc.} \\\hline
        \small\textbf{{Scientific}} & \small{Lowest taxonomic rank including specific Epithets. 1,604 unique values present.} \\
        \small{\textbf{Species}} & \small1th taxon rank. 1,578 unique values present. \\ \small{\textbf{Genus}} & \small{2nd taxon rank. 566 unique values present.} \\
        \small{\textbf{Family}} & \small{3rd taxon rank. 190 unique values present.} \\
        \small{\textbf{Order}} & \small{4th taxon rank. 66 unique values present.} \\
        \small{\textbf{Class}} & \small{5th taxon rank. 23 unique values present.} \\
        \small{\textbf{Phylum}} & \small{6th taxon rank. 5 unique values present.} \\
        \small{\textbf{Kingdom}} & \small{7th taxon rank. 3 unique values present.} \\  
\hline
        \small{\textbf{CountryCode}} & \small{ISO 3166-1 alpha-2 code (DK, AT, etc.) of the observation. The dataset covers 30 countries.} \\ \small{\textbf{Locality}} & \small{More precise location information. Mostly smaller than a district, e.g. part of a city or a specific forest. 9003 values present.}  \\
\small{\textbf{Level1Gid}} & \small{ID of a Country region related to the specimen observation, 115 regions are listed.} \\
\small{\textbf{Level2Gid}} & \small{ID of a district region related to the specimen observation, 317 districts are listed.} \\
        \small{\textbf{Latitude}} & \small{A decimal GPS coordinate.} \\
        \small{\textbf{Longitude}} & \small{A decimal GPS coordinate.} \\
        \small{\textbf{GPSUncert}} & \small{GPS coordinates uncertainty in meters.} \\
\hline
\end{tabular}
\end{center}
\caption{Description of the provided metadata (observation attributes). For almost all images, a detailed information about taxonomy, location, time, habitat and substrate type is included.}
\label{table:metadata}
\end{table}


\subsection{Metadata}
Unlike most computer vision datasets, DF20 and DF20\,-\,Mini include rich metadata acquired by citizen-scientists in the field while recording the observations. For clarity, we decided to only publish the frequently filled-in and relevant 21 categories with the dataset, although the Danish Fungal Atlas contains up to 255 data-points for each observation. We see a promising research direction in combining visual data with metadata like timestamp, location at multiple scales, substrate, habitat, full taxonomy labels and camera device settings. For detailed description see Table\,\ref{table:metadata}. 

\textbf{Substrate.} Substrates on which fungi live and fruit are an important source of information that helps differentiate similarly looking species. Each species or genus has its preferable substrate, and it is rare to find it on other substrates. For example, \textit{Trametes} occurs only on wood and \textit{Russula} on soil. As such metadata can improve the final categorization capability, we provide one of 32 substrate types for more than 99\% of images. We differentiate wood of living trees, dead wood, soil, bark, stone, fruits and others. 

\textbf{Habitat.} While substrate denotes the spots, the habitat indicates the more overall environment where fungi grow and hence is vital for fungal recognition. It is well known that some species occur in deciduous forests rather than in conifer forests or plantations, while others grow in farmland. For a deeper understanding of such relation, we include the information about the habitat for approximately 99.5\% of observations.

\textbf{Location.} Fungi are highly location dependent with different species distributions across continents, states, regions or even districts. To support studies on better understanding where Fungi species lives, we include multi-level location information. Starting from GPS coordinates with included uncertainty, we further extracted information about the country, region and district. 


\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1.0\linewidth]{species_dist_months_12.png}
\caption{Monthly distribution of observations in the DF20 dataset for genera Mycena, Boletus, and Agaricus. The differences imply that the class prior distribution varies significantly over time.}
\label{fig:genus_occurence}
\end{center}
\end{figure*}

\textbf{Time-Stamp.} Time of observation is essential for fungi classification in the wild as fruitbodies' presence depends on  seasonality or even (but rarely) the time in a day. Considering the existence of such dependency, integrating information about time into the classification should also improve fungal recognition. In Figure\,\ref{fig:genus_occurence} we show the probability of three genera being observed in different months of the calendar year. Brief inspection shows that there is almost zero probability to spot a \textit{Boletus} in January but still a small chance to find a \textit{Mycena}. In contrast to \textit{Boletus}, \textit{Exidia} occurs mostly during the cold months.


\textbf{EXIF data.} Since the camera device and its settings affect the resulting image, the image classification models may be biased towards certain (e.g. more common) device attributes. To allow a deeper study of such phenomena, we include the EXIF data for approximately 84\% of images, where the EXIF information was available in the Danish Fungal Atlas. The included attributes, the number of unique values in the dataset and the proportion of images with the attributes present are summarized in Table \ref{table:exif_values}.

\begin{table}[b!]
\begin{center}
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{0.4em}
\begin{tabular}{|l|c|r|}
\hline
Attribute & Coverage\,[\%] & \# Values \\
\hline
White Balance                 & 79.99 &     2 \\
Color Space                   & 84.38 &     3 \\
Metering Mode                 & 78.23 &    10 \\
Metering Mode                 & 81.04 &    13 \\
Compressed Bits Per Pixel\,\, & 37.75 &    88 \\
Aperture Value                & 46.63 &   297 \\
Device                        & 80.31 &   688 \\
Focal Length                  & 80.12 & 1,580 \\
Exposure Time                 & 80.12 & 4,594 \\
Shutter Speed Value           & 46.78 & 7,079 \\
\hline
\end{tabular}
\end{center}
\caption{Device settings extracted from the original images' EXIF in the Danish Fungal Atlas, with the proportion of images where the attributes are present (Coverage), and the number of unique values of the attribute in the dataset.}
\label{table:exif_values}
\end{table}

\section{Experiments}
\label{experiments}

To establish a baseline performance on the DF20 and DF20\,-\,Mini datasets, we performed multiple experiments. First, we train a wide variety of well known CNN architectures such as ResNets\,\cite{resnets}, EfficientNets\,\cite{tan2019efficientnet}, MobileNet\,\cite{sandler2018mobilenetv2}, Inception networks\,\cite{inceptions} and \mbox{SE-ResNeXt-101-32x4d} that extends the ResNet-101 by cardinality\,\cite{next} and Squeeze and Excite blocks\,\cite{SE_Net}. Second, the EfficientNet-B0, EfficientNet-B3, and \mbox{SE-ResNeXt-101-32x4d} are compared with Vision Transformer architectures  ViT-Large/16 and ViT-Base/16\,\cite{vit}. Finally, the impact of different metadata and their combinations on both the CNN and the ViT final prediction performance is evaluated.


\subsection{Setup}
\label{setup}

In this section, we describe the full training and evaluation procedure, including the training strategy and image augmentations.

\textbf{Training Strategy.} All architectures were initialized from publicly available ImageNet-1k pre-trained checkpoints and further fine-tuned with the same strategy for 100 epochs with the PyTorch\,framework\,\cite{PyTorch} within the 20.12 NGC deep learning framework Docker container. All neural networks were optimized by Stochastic Gradient Descent with momentum set to 0.9. The start Learning Rate (LR) was set to 0.01 and was further decreased with a specific adaptive learning rate schedule strategy - if the validation loss is not reduced from one epoch to another, reduce LR by 10\%. To have the same effective mini-batch size of 64 for all architectures, we accumulated gradients from smaller mini-batches accordingly, where needed.

\textbf{Augmentations.} For training, we utilized several augmentation techniques from the Albumentations\,library\,\cite{albumentation}. More specifically, we used: random horizontal flip with 50\% probability, random vertical flip with 50\% probability, random resized crop with a scale of 0.8 - 1.0, random brightness\,/\,contrast adjustments with 20\% probability, and mean and std. dev. normalization. All images were resized to the required network input size: For the CNN performance experiment, inputs of size 299299 were used. In the case of the CNN vs ViT experiment, we used two different resolutions, 224224 and 384384, to match the input resolutions of the pre-trained models.

\textbf{Test-time.} While testing, we avoided any extensive techniques such as ensembles, centre-cropping, prior weighting, etc. Only the resize and normalization operations were used to pre-process the data. The impact of test-time augmentation methods on the final performance can be studied in the future.

\subsection{Metadata Use}
\label{metadata_usage}
We propose a simple baseline for the use of metadata.
For a given type of metadata, e.g. month of year, substrate type or habitat, we adopt the following assumption:

i.e. that the visual appearance of a species does not depend on the metadata.
This does not mean that the posterior probability of a species given an image is independent of metadata . The dependence under the assumption is via the conditional probability .
Note that this is the only possible approach not requiring to retrain or fine-tine the model using the metadata;\footnote{\,More precisely "not requiring modelling the dependence of visual appearance and the metadata". The modelling need not necessarily mean training or fine-tuning the species classifier.}
the model trained without metadata has no information about visual appearance changes of a species as a function of . Moreover, this assumption is applicable for situations where the classifier has to be treated as a black box when the user has no access to the code or has no possibility to retrain the model.

A few lines of algebraic manipulation prove  
 that under assumption Eq. \eqref{eq:assumption}, the class posterior given the image  and metadata  is easily obtained:

where  is the class prior in the training set.
The conditional probability  is estimated as the relative frequency of species  with metadata  in the training set.

To utilize several types of metadata at once, e.g. month and habitat, we combine the posteriors assuming statistical independence:

This is a simple, baseline assumption. Direct estimation of , e.g. as relative frequencies, is another possibility. The D20 benchmark has thus the potential to be a fertile ground for evaluation of intra-metadata, as well as visual-metadata, dependencies.

The approach of Eq.~(\ref{eq:posterior-given-meta}) needs
the output of the species classifier to be turned into an estimator of . The process is called {\it calibration} in the literature\,\cite{guo2017calibration,vaicenavicius2019evaluating}.
CNNs typically need re-calibration as their estimates of  are overconfident.
The proposed benchmark may be used, in the context of exploiting metadata, to evaluate and compare classifier calibration techniques.

\subsection{Metrics}
\label{metrics}

Besides commonly used metrics, Top1 and Top3 Accuracy, we measured the macro-averaged  score, , which is not biased by class frequencies and is more suitable for the long-tailed class distributions observed in the nature and FGVC datasets. Interestingly, the  metric is not used as a validation metric in any dataset mentioned in Table\,\ref{table:fgvc_datasets}. The  is defined as the mean of class-wise  scores:



where\, represents the number of classes and  is the species index. Than the  score for each class is calculated as a harmonic mean of the class precision  and recall :





In multi-class classification, the True Positive\,(\textit{TP}) represents the number of correct Top1 predictions, False Positive (\textit{FP}) how many times was a specific class predicted instead of the\,\textit{TP}, and False Negative\,(\textit{FN}).
Differently from the mean Accuracy, , through precision and recall, it allows to easily assign a cost value to both types of error\,(\textit{FP/FN}) for each label and to measure more task-relevant performance. For example, in fungi recognition, mistaking a poisonous mushroom for the edible one is a more significant problem than the opposite. Interestingly, even though the performance across the whole taxonomy in nature-related FGVC datasets is highly demanded, most existing datasets are only using accuracy as the score measure. Considering that the datasets are highly imbalanced with so-called long-tail distribution, CNNs may ignore the least present species.




\subsection{Results}
\label{results}

In this section, we compare the performance of the well known CNN based models and ViT models in terms of Top1 and Top3 accuracy, and the newly included  metric. Additionally, we discuss the impact of the metadata on the classification performance.


\begin{table*}[t]
\begin{center}
\setlength{\tabcolsep}{0.75em} \renewcommand{\arraystretch}{1.1}
\begin{tabular}{| l | c | c | c | c | c | c |}
\cline{2-7}
    \multicolumn{1}{c|}{ } & \textbf{Top1}\,[\%] & \textbf{Top3}\,[\%] & \textbf{} &  \textbf{Top1}\,[\%] & \textbf{Top3}\,[\%] & \textbf{}  \\
    \hline
    MobileNet-V2        & 65.74 & 83.65 & \,\,\,0.546\,\,\, & 69.51 & 84.55 & \,\,\,0.602\,\,\, \\
    \hline 
	ResNet-18           & 63.24 & 82.23 & 0.526 & 67.21 & 82.71 & 0.580 \\
	ResNet-34           & 63.60 & 81.68 & 0.522 & 69.92 & 84.72 & 0.605 \\
	ResNet-50           & 69.26 & 85.03 & 0.590 & 73.15 & 87.03 & 0.643 \\
	\hline
	EfficientNet-B0     & 69.12 & 85.66 & 0.579 & 73.63 & 87.51 & 0.652 \\
	EfficientNet-B1     & 69.23 & 85.38 & 0.592 & 74.11 & 87.62 & 0.658 \\
	EfficientNet-B3     & 70.05 & 85.27 & 0.595 & 74.73 & 88.01 & 0.662 \\
	EfficientNet-B5     & 66.87 & 84.04 & 0.560 & 73.07 & 86.91 & 0.636 \\
	\hline 
	Inception-V3        & 65.30 & 82.83 & 0.530 & 71.45 & 85.64 & 0.622 \\
	InceptionResnet-V2  & 67.42 & 83.60 & 0.559 & 72.68 & 86.37 & 0.629 \\
	Inception-V4        & 67.50 & 83.63 & 0.572 & 74.19 & 87.63 & 0.655 \\
	\hline
	SE-ResNeXt-101-32x4d\,\,\,\,\,\, & \textbf{72.39} & \textbf{86.57} & \textbf{0.635} & \textbf{76.73} & \textbf{89.09} & \textbf{0.691} \\
	\hline	
	\noalign{\vskip 0.5mm}
\cline{2-7}
\multicolumn{1}{c}{ } & \multicolumn{3}{|c|}{\textbf{Danish Fungi 2020 - Mini}} & \multicolumn{3}{c|}{\textbf{Danish Fungi 2020}} \\
 \cline{2-7}
\end{tabular}
\end{center}
\caption{Classification performance of selected CNN architectures on DF20 and DF20\,-\,Mini. All networks share the settings described in Section \ref{setup} and were trained on 299299 images. The top results  -- , see Eq. (\ref{eq:F1_macro}), equal to 0.635 / 0.691 and Top1 to 72.39\% / 76.73\% -- are far from saturated. The datasets are challenging for the state-of-the-art CNN based classifiers.}
\label{table:results}
\end{table*}


\textbf{Convolutional Neural Networks.} 
Comparing well known CNN architectures on DF20 and DF20\,-\,Mini, we can see a similar behaviour as on other datasets\,\cite{imagenet, inaturalist2017, dataset-CUBS}. The EfficientNet models achieve excellent results. However, the high dependence of EfficientNet-B5 on the input size makes it worst then EfficientNet-B0 in the scenario with the 299299 image resolution. Similarly, the compact MobileNet-V2 performed better than ResNet-18, ResNet-34, and even better than Inception-V3. The best performing model on both datasets was SE-ResNeXt-101 with 0.635  score on DF20\,-\,Mini and 0.691  score on DF20. A more detailed comparison of the achieved scores (Top1, Top3, and ) for each model are summarized in Table\,\ref{table:results}. 


\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{0.35em} \renewcommand{\arraystretch}{1.1}
\begin{tabular}{| c | l | c | c | c | c |}
\cline{3-5}
    \multicolumn{1}{c}{ } & \multicolumn{1}{c|}{ } & \textbf{Top1} [\%] & \textbf{Top3} [\%] & \textbf{} &  \multicolumn{1}{c}{ }\\
    \hline
    \multirow{6}{*}{\rotatebox[origin=c]{90}{DF20 - Mini}} & EfficientNet-B0     & 65.66 & 83.35 & \,\,0.531\,\, & \multirow{7}{*}{\rotatebox[origin=c]{90}{}} \\
	& EfficientNet-B3     & 66.90 & 83.49 & 0.537 & \\
	& SE-ResNeXt-101      & 69.48 & 85.58 & 0.593 & \\
	& ViT-Base/16         & 69.37 & \textbf{86.54} & 0.589 & \\
	& ViT-Large/16 & \textbf{70.71} & 86.51 & \textbf{0.599} & \\
	\cline{1-5}
    \multirow{3}{*}{\rotatebox[origin=c]{90}{DF20}} & EfficientNet-B0  & 70.38 & 85.18 & 0.613 & \\ 
    & ViT-Base/16         & 73.45 & 87.15 & 0.658 & \\
	& ViT-Large/16        & \textbf{75.32} & \textbf{88.12} & \textbf{0.679}  & \\
	\hline
	\hline
	\multirow{6}{*}{\rotatebox[origin=c]{90}{DF20 - Mini}} & EfficientNet-B0  & 70.22 & 85.69 & 0.596 & \multirow{7}{*}{\rotatebox[origin=c]{90}{}} \\
	& EfficientNet-B3  & 72.09 & 87.17 & 0.624 & \\ 
	& SE-ResNeXt-101   & 72.34 & 87.53 & 0.631 & \\
	& ViT-Base/16      & 74.84 & 88.74 & 0.655 & \\
	& ViT-Large/16 & \textbf{75.96} & \textbf{89.37} & \textbf{0.664} & \\
	\cline{1-5}
	\multirow{3}{*}{\rotatebox[origin=c]{90}{DF20}} & EfficientNet-B0  & 75.27 & 88.65 & 0.670 & \\ 
    & ViT-Base/16 & 79.40 & 90.93 & 0.724 & \\
	& ViT-Large/16        & \textbf{81.25} & \textbf{91.93} & \textbf{0.747} & \\ \hline
	\noalign{\vskip 0.5mm}
\end{tabular}
\end{center}
\caption{Classification results of selected CNN and ViT architectures on DF20 and DF20\,-\,Mini dataset for two input resolutions.}
\label{table:results_ViT}
\end{table}


\textbf{Vision Transformers.}
The recently introduced ViT\,\cite{vit} showed excellent performance in common object classification compared to state-of-the-art convolutional networks. Appart from the CNNs, the ViT is not using convolutions but interprets an image as a sequence of patches and process it by a standard Transformer encoder as used in  natural language processing\,\cite{vaswani2017attention}.
To evaluate its performance for transfer-learning in the FGVC domain, we compare two ViT architectures - ViT-Base/16 and ViT-Large/16 - against the well performing CNN models - EfficientNet-B0, EfficientNet-B3 and SE-ResNeXt-101. As ImageNet pre-trained ViT models were available just for input sizes of 224224 and 384384, we trained all networks on these resolutions while following the training setup fully described in subsection \ref{setup}. Differently from the performance validation of Dosovitskiy et al.\,\cite{vit} on ImageNet, in our evaluation on DF20, ViTs ourperform state-of-the-art CNNs by a large margin. The best performing ViT model achieved an impressive 0.664  score while outperforming the SE-ResNeXt-101 by a significant margin of 0.033 in , and 3.62\% of Top1 Accuracy on the images with 384384 input size. In the case of the  224224, we see a smaller margin of 1.29\% in Top1 Accuracy and 0.006 in the   score. All the results are shown in Table\,\ref{table:results_ViT}.


\textbf{Importance of the metadata.} Inspired by the common practice in mycology, we set up an experiment to show the importance of metadata for \textit{Fungus} species identification. Using the approach described in Section\,\ref{metadata_usage}, we improved performance in all measured metrics by a significant margin. We measured the performance improvement with all metadata types and their combinations. Overall, habitat was most efficient in improving the performance. More precisely, we improved the ViT-Base/16 model's performance on DF20 by 3.58\%, 3.05\% and 0.062 in Top1, Top3 and , respectively. Similarly to DF20, we measured the relative performance gain of 2.88\%, 1.65\% and 0.047 in the Top1, Top3 and  metrics respectively on DF20\,-\,Mini. Detailed evaluation of the performance gain using different observation metadata and their combinations is shown in Table\,\ref{table:results_metadata}.

\textbf{DF20 vs DF20\,-\,Mini}. The performance evaluation with selected CNN and ViT architectures showed that even with a smaller number of classes and one-tenth of the data, DF20\,-\,Mini as a compact subset of DF20 offers an even more challenging problem for state-of-the-art architectures while being less time and hardware demanding.



\begin{table}[b]
\begin{center}
\setlength{\tabcolsep}{0.45em} \renewcommand{\arraystretch}{1.1}
\begin{tabular}{| c | c | c | c | c | c | c | }
\cline{2-7}
    \multicolumn{1}{c|}{ } & \textbf{H} & \textbf{M} & \textbf{S} & \textbf{Top1}\,[\%] & \textbf{Top3}\,[\%] & \textbf{} \\
    \hline
     \multirow{8}{*}{\rotatebox[origin=c]{90}{Danish Fungi 2020}} &    &    &    & 73.45 & 87.15 & 0.658 \\
    \cline{2-7}
     & \faCheck{} &    &    & +2.00 & +1.42 & \,\,+0.036\,\, \\
     &    & \faCheck{} &    & +1.37 & +1.23 & +0.024 \\
     &    &    & \faCheck{} & +0.98 & +0.96 & +0.016 \\
     &    & \faCheck{} & \faCheck{} & +2.30 & +2.10 & +0.039 \\
     & \faCheck{} &    & \faCheck{} & +2.92 & +2.41 & +0.051 \\
     & \faCheck{} & \faCheck{} &    & +3.16 & +2.50 & +0.056 \\
     & \,\faCheck{}\, & \,\faCheck{}\, & \,\faCheck{}\, & +\textbf{3.58} & +\textbf{3.05} & +\textbf{0.062} \\
\hline
     \multirow{8}{*}{\rotatebox[origin=c]{90}{Danish Fungi 20 Mini}} &    &    &    & 69.37 & 86.54 & 0.589 \\
    \cline{2-7}
     & \faCheck{} &    &    & +1.70 & +1.10 & +0.029  \\
     &    & \faCheck{} &    & +0.77 & +0.19 & +0.011   \\
     &    &    & \faCheck{} & +0.85 & +0.69 & +0.014  \\
     &    & \faCheck{} & \faCheck{} & +1.29 & +0.80 & +0.020   \\
     & \faCheck{} &    & \faCheck{} & +2.75 & +2.01 & +0.043   \\
     & \faCheck{} & \faCheck{} &    & +2.20 & +1.24 & +0.037   \\
     & \faCheck{} & \faCheck{} & \faCheck{} & +\textbf{2.88} & +\textbf{1.65} & +\textbf{0.047}  \\
	\hline
\end{tabular}
\end{center}
\caption{Performance gains from \textit{Fungus} observation metadata: H - Habitat, S - Substrate, M - Month,
and their combinations, on DF20 and DF20\,-\,Mini. ViT-Base/16 with image size .\ }
\label{table:results_metadata}
\end{table}

\section{Conclusion}

This paper introduced a novel fine-grained dataset and classification benchmark, the Danish Fungi 2020, and its subset, Danish Fungi 2020 - Mini. 
The dataset was constructed from data submitted to the Danish Fungal Atlas and includes 295,938 photographs of 1,604 species - mainly from the Fungi kingdom - together with full taxonomic labels, rich metadata, compact size and severe difficulty, and the same training and test set species distribution.

The quantitative and qualitative analysis of CNNs and ViTs shows superior performance of the ViT in fine-grained classification. We present the baselines for processing the habitat, substrate and time (month) metadata. We show that – even with the baseline approach – utilizing the metadata increases the classification performance significantly. We provide the code and trained model check-points to all our baselines. 
A publicly available benchmark allows for an on-line comparison of state-of-the-art results for both image-only and image+metadata submissions. With the precise annotation and rich metadata, we would like to encourage further research in other areas of computer vision and machine learning, beyond fine-grained visual categorization. The benchmark may help research in classifier calibration, loss functions, validation metrics, taxonomy/hierarchical learning, device dependency or time series based species prediction. For example, the standard loss function focusing on recognition accuracy ignores the practically important cost of predicting a species with high toxicity. 







{\small
\bibliographystyle{ieee_fullname}
\bibliography{paper}
}

\end{document}