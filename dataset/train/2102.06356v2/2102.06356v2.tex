
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{siunitx} 

\usepackage{amsmath} \usepackage{hyperref} \usepackage{listings} \usepackage{xcolor} \usepackage{hhline} 


\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{arxiv}

\arxivtitlerunning{A Large Batch Optimizer Reality Check}


\begin{document}

\twocolumn[
\arxivtitle{A Large Batch Optimizer Reality Check:\\Traditional, Generic Optimizers Suffice Across Batch Sizes}



\arxivsetsymbol{equal}{*}

\begin{arxivauthorlist}
\arxivauthor{Zachary Nado}{equal,goo}
\arxivauthor{Justin M. Gilmer}{equal,goo}
\arxivauthor{Christopher J. Shallue}{har}
\arxivauthor{Rohan Anil}{goo}
\arxivauthor{George E. Dahl}{goo}
\end{arxivauthorlist}

\arxivaffiliation{har}{Center for Astrophysics  Harvard \& Smithsonian, Cambridge, MA, USA}
\arxivaffiliation{goo}{Google Research, Brain Team, Mountain View, California, USA}

\arxivcorrespondingauthor{Zachary Nado}{znado@google.com}
\arxivcorrespondingauthor{Justin Gilmer}{gilmer@google.com}
\arxivcorrespondingauthor{Christopher Shallue}{cshallue@cfa.harvard.edu}
\arxivcorrespondingauthor{Rohan Anil}{rohananil@google.com}
\arxivcorrespondingauthor{George Dahl}{gdahl@google.com}

\arxivkeywords{Machine Learning, arxiv}

\vskip 0.3in
]



\printAffiliationsAndNotice{\arxivEqualContribution} 

\begin{abstract}
Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally. 
\end{abstract}


\newcommand{\srange}[2] {\lbrack\num{#1}, \num{#2}\rbrack}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}


\section{Introduction}\label{sec:intro}

In recent years, hardware systems employing GPUs and TPUs have enabled neural network training programs to process dramatically more data in parallel than ever before.
The most popular way to exploit these systems is to increase the batch size in the optimization algorithm (i.e. the number of training examples processed per training step).
On many workloads, modern systems can scale to larger batch sizes without significantly increasing the time per step \citep{jouppi2017datacenter,wang2019benchmarking}, thus proportionally increasing the number of training examples processed per second.
If researchers can use this increased throughput to reduce the time required to train each neural network, then they should achieve better results by training larger models, using larger datasets, and by exploring new ideas more rapidly.



As the capacity for data parallelism continues to increase, practitioners can take their existing, well-tuned training configurations and re-train with larger batch sizes, hoping to achieve the same performance in less training time \citep[e.g.][]{ying2018image}.
On an idealized data-parallel system with negligible overhead from increasing the batch size, they might hope to achieve \textit{perfect scaling}, a proportional reduction in training time as the batch size increases.

However, achieving perfect scaling is not always straightforward.
Changing the batch size changes the training dynamics, requiring the training hyperparameters (e.g. learning rate) to be carefully re-tuned in order to maintain the same level of validation performance.\footnote{Although there are heuristics for adjusting the learning rate as the batch size changes, these heuristics inevitably break down sufficiently far from the initial batch size and it is also not clear how to apply them to other training hyperparameters (e.g. momentum).}
In addition, smaller batch sizes provide implicit regularization from gradient noise that may need to be replaced by other forms of regularization when the batch size is increased.
Finally, even with perfect tuning, increasing the batch size eventually produces diminishing returns.
After a critical batch size, the number of training steps cannot be decreased in proportion to the batch size -- the number of epochs must increase to match the validation performance of the smaller batch size. See \citealt{shallue2019measuring} for a survey of the effects of data parallelism on neural network training. 
Once these effects are taken into account, there is no strong evidence that increasing the batch size degrades the maximum achievable performance on any workload.
At the same time, the ever-increasing capacity for data parallelism presents opportunities for new regularization techniques that can replace the gradient noise of smaller batch sizes and new optimization algorithms that can extend perfect scaling to larger batch sizes by using more sophisticated gradient information \citep{zhang2019algorithmic}.

\citet{you2017lars} proposed the LARS optimization algorithm in the hope of speeding up neural network training by exploiting larger batch sizes. LARS is a variant of stochastic gradient descent (SGD) with momentum \citep{polyak1964some} that applies layer-wise normalization before applying each gradient update.
Although it is difficult to draw strong conclusions from the results presented in the LARS paper, the MLPerf\footnote{MLPerf is a trademark of MLCommons.org.} Training benchmark\footnote{\url{https://mlperf.org/training-overview}} adopted LARS as one of two allowed algorithms in the closed division for ResNet-50 on ImageNet and it became the \textit{de facto} standard algorithm for that benchmark task.
With MLPerf entrants competing to find the fastest-training hyperparameters for LARS, the first place submissions in the two most recent MLPerf Training competitions used LARS to achieve record training speeds with batch sizes of 32,678 and 65,536, respectively.
No publications or competitive submissions to MLPerf have attempted to match these results with a standard optimizer (e.g. Momentum or Adam).
However, MLPerf entrants do not have a strong incentive (nor are necessarily permitted by the rules) to explore other algorithms because MLPerf Training is a systems benchmark that requires algorithmic equivalence between submissions to make fair comparisons.
Thus, it has remained an open question whether LARS was necessary to achieve these training speeds instead of a traditional, generic optimizer. Moreover, since the main justification for LARS is its excellent performance on ResNet-50 at large batch sizes, more work is needed to quantify any benefit of LARS over standard algorithms at any batch size.

\citet{you2019lamb} later proposed the LAMB optimizer to speed up pre-training for BERT \citep{devlin2018bert} using larger batch sizes after concluding that LARS was not effective across workloads.
LAMB is a variant of Adam \citep{kingma2014adam} that adds a similar layer-wise normalization step to LARS.
\citet{you2019lamb} used LAMB for BERT pre-training with batch sizes up to 65,536 and claimed that Adam cannot match the performance of LAMB beyond batch size 16,384.

In this paper, we demonstrate that standard optimizers, without any layer-wise normalization techniques, can match or improve upon the large batch size results used to justify LARS and LAMB. In Section~\ref{sec:resnet50}, we show that Nesterov momentum \citep{nesterov1983method} matches the performance of LARS on the ResNet-50 benchmark with batch size 32,768. We are the first to match this result with a standard optimizer.
In Section~\ref{sec:bert}, contradicting the claims in \citet{you2019lamb}, we show that Adam obtains better BERT pre-training results than LAMB at the largest batch sizes, resulting in better downstream performance metrics after fine-tuning.
In addition, we establish a new state-of-the-art for BERT pretraining speed, reaching an F1 score of 90.46 in 7,818 steps using Adam at batch size 65,536 (we report training speed in steps because our focus is algorithmic efficiency, but since we compare LARS and LAMB to simpler optimizers, fewer training steps corresponds to faster wall-time in an optimized implementation -- our BERT result with Adam also improves upon the wall-time record of LAMB reported in \citealt{you2019lamb}).
Taken together, our results establish stronger training speed baselines for these tasks and batch sizes, which we hope will assist future work aiming to accelerate training using larger batch sizes.


In addition to the contributions mentioned above, we demonstrate several key effects that are often overlooked by studies aiming to establish the superiority of new optimization algorithms. We show that future work must carefully disentangle regularization and optimization effects when comparing a new optimizer to baselines.
We also report several under-documented details used to generate the best LARS and LAMB results, a reminder that future comparisons should document any novel tricks and include them in baselines.
Finally, our results add to existing evidence in the literature on the difficulty of performing independently rigorous hyperparameter tuning for optimizers and baselines. In particular, we show that the optimal shape of the learning rate schedule is optimizer-dependent (in addition to the scale), and that differences in the schedule can dominate optimizer comparisons at smaller step budgets and become less important at larger step budgets.



\subsection{Related work}\label{sec:related-work}

\citet{shallue2019measuring} and \citet{zhang2019algorithmic} explored the effects of data parallelism on neural network training for different optimizers, finding no evidence that larger batch sizes degrade performance and demonstrating that different optimizers can achieve perfect scaling up to different critical batch sizes. \citet{you2017lars,you2019lamb} developed the LARS and LAMB optimizers  in the hope of speeding up training by achieving perfect scaling beyond standard optimizers. Many other recent papers have proposed new optimization algorithms for generic batch sizes or larger batch sizes \citep[see][]{schmidt2020descending}. \citet{choi2019empirical} and \citet{schmidt2020descending} demonstrated the difficulties with fairly comparing optimizers, showing that the hyperparameter tuning protocol is a key determinant of optimizer rankings. The MLPerf Training benchmark \citep{mattson2019mlperf} provides a competitive ranking of neural network training systems, but does not shed much light on the relative performance of optimizers because entrants are limited in the algorithms they can use and the hyperparameters they can tune. We are unaware of any prior studies aiming to establish stronger baselines for standard optimizers at the batch sizes considered in this paper. Optimizer baselines are typically provided by the authors of new algorithms, who have limited incentives to spend significant effort and computational resources producing the strongest possible baselines.


 \clearpage \section{Matching LARS on ImageNet}\label{sec:resnet50}



The MLPerf training benchmark for ResNet-50 v1.5 on ImageNet \citep{mattson2019mlperf} aims to reach 75.9\% validation accuracy in the shortest possible wall-clock time. In the closed division of the competition, entrants must choose between two optimizers, SGD with momentum or LARS, and are only allowed to tune a specified subset of the optimization hyperparameters, with the remaining hyperparameter values set by the competition rules.\footnote{\url{https://git.io/JtknD}}
The winning entries in the two most recent competitions used LARS with batch size 32,768 for 72 training epochs\footnote{\url{https://mlperf.org/training-results-0-6}} and LARS with batch size 65,536 for 88 training epochs,\footnote{\url{https://mlperf.org/training-results-0-7}} respectively.
\citet{kumar2019scale} later improved the training time for batch size 32,768 by reaching the target accuracy in 64 epochs.
These are currently the fastest published results on the ResNet-50 benchmark.
However, it has been unclear whether LARS was necessary to achieve these training speeds since no recent published results or competitive MLPerf submissions have used another optimizer.
In this section, we describe how we matched the 64 epoch, 32,768 batch size result of LARS using standard Nesterov momentum.\footnote{The 88 epoch, 65,536 batch size result is faster in terms of wall-clock time but requires more training epochs, indicating that it is beyond LARS's perfect scaling regime. Although LARS obtains diminishing returns when increasing the batch size from 32,768 to 65,536, future work could investigate whether Nesterov momentum drops off more or less rapidly than LARS.}

A fair benchmark of training algorithms or hardware systems must account for stochasticity in individual training runs. In the MLPerf competition, the benchmark metric is the mean wall-clock time of 5 trials after the fastest and slowest trials are excluded. Only 4 out of the 5 trials need to reach the target accuracy and there is no explicit limit on the number of times an entrant can try a different set of 5 trials.
Since our goal is to compare algorithms, rather than systems, we aim to match the LARS result in terms of training steps instead (but since Nesterov momentum is computationally simpler than LARS, this would also correspond to faster wall-clock time on an optimized system).
Specifically, we measure the median validation accuracy over 50 training runs with a fixed budget of 2,512 training steps\footnote{Corresponding to 64 training epochs in \citet{kumar2019scale}.} at a batch size of 32,768.
When we ran the published LARS training pipeline,\footnote{\url{https://git.io/JtsLQ}} LARS achieved a median accuracy of 75.97\% and reached the target in 35 out of 50 trials. We consider the LARS result to be matched by another optimizer if the median over 50 trials exceeds the target of 75.9\%.

\subsection{Nesterov momentum at batch size 32k}

This section describes how we used the standard Nesterov momentum optimizer to train the ResNet-50 v1.5 on ImageNet to 75.9\% validation accuracy in 2,512 update steps at a batch size of 32,768, matching the best published LARS result at this batch size. Although we implemented our own training program, the only logical changes we made to the published LARS pipeline were to the optimizer and the optimization hyperparameters. Our model implementation and data pre-processing pipeline were identical to those required under the MLPerf closed division rules (see Appendix~\ref{appendix:experiment-details}).

We present two Nesterov momentum hyperparameter configurations that achieve comparable performance to LARS. Configuration A achieved a median accuracy of 75.97\% (the same as LARS) and reached the target accuracy in 34 out of 50 trials. Configuration B is a modified version of Configuration A designed to make as few changes as possible to the LARS hyperparameters; it achieved a median accuracy of 75.92\% and reached the target in 29 out of 50 trials. 
See Appendix~\ref{appendix:nesterov_config} for the complete hyperparameter configurations.

To achieve these results, we tuned the hyperparameters of the training pipeline from scratch using Nesterov momentum. We ran a series of experiments, each of which searched over a hand-designed hyperparameter search space using quasi-random search \citep{bousquet2017critical}. Between each experiment, we modified the previous search space and/or tweaked the training program to include optimization tricks and non-default hyperparameter values we discovered in the state-of-the-art LARS pipeline. The full sequence of experiments we ran, including the number of trials, hyperparameters tuned, and search space ranges, are provided in Appendix~\ref{appendix:historical_search_spaces_nesterov_resnet50}. Once we had matched the LARS result with Configuration A, we tried setting each hyperparameter to its value in the LARS pipeline in order to find the minimal set of changes that still achieved the target result, producing Configuration B. The remainder of this section describes the hyperparameters we tuned and the techniques we applied on the journey to these results.

\subsubsection{Nesterov Momentum Optimizer}\label{sec:nesterov-momentum}

Nesterov momentum is a variant of classical or ``heavy-ball'' momentum defined by the update rule
\newcommand{\loss}{\ell}
\newcommand{\grad}{\nabla}

where ,  is the vector of model parameters after  steps,  is the gradient of the loss function  averaged over a batch of training examples,  is the momentum, and  is the learning rate for step . We prefer Nesterov momentum over classical momentum because it tolerates larger values of its momentum parameter \citep{sutskever2013importance} and sometimes outperforms classical momentum, although the two algorithms perform similarly on many tasks \citep{shallue2019measuring,choi2019empirical}.
We tuned the Nesterov momentum  in Configurations A and B. We discuss the learning rate schedule  separately in Section~\ref{sec:learning-rate}.

\subsubsection{Batch normalization}\label{sec:batch-norm}

The ResNet-50 v1.5 model uses batch normalization \citep{ioffe2015batch}, defined as

where  is a vector of pre-normalization outputs from layer ,  and  denote the element-wise sample mean and variance across the batch of training examples,\footnote{In a distributed training environment the mean and variance are commonly computed over a subset of the full batch. The LARS pipeline uses a ``virtual batch size'' of 64, which we also use to avoid changing the training objective \citep{hoffer2017train}.} and  and  are trainable model parameters.

Batch normalization introduces the following tuneable hyperparameters: , the small constant added to the sample variance; the initial values of  and ; and , which governs the exponential moving averages of the scaling factors used in evaluation. The LARS pipeline uses  and . It sets the initial value of  to 0.0 everywhere, but the initial value of  depends on the layer: it sets  to 0.0 in the final batch normalization layer of each residual block, and to 1.0 everywhere else. In Configuration A, we tuned , , and , the initial value of  in the final batch normalization layer of each residual block. In Configuration B, we used the same values as LARS for  and , but we found that choosing  between 0.0 and 1.0 was important for matching the LARS result with Nesterov momentum.


\subsubsection{Regularization}\label{sec:regularization}

In Configuration A, we tuned both the L2 regularization coefficient  and label smoothing coefficient  \citep{szegedy2016rethinking}. The LARS pipeline uses  and .
Crucially, the LARS pipeline does not apply L2 regularization to the bias variables of the ResNet model nor the batch normalization parameters  and  (indeed, the published LARS pipeline does not even apply LARS to these parameters -- it uses Heavy-ball momentum). This detail is extremely important for both LARS and Nesterov momentum to achieve the fastest training speed. Configuration B used the same  and  as Configuration A.

\subsubsection{Learning rate schedule}\label{sec:learning-rate}

The LARS pipeline uses a piecewise polynomial schedule

with , , , , , and  steps. In Configuration A, we re-tuned all of these hyperparameters with Nesterov momentum. In Configuration B, we set , , and  to the same values as LARS, changing only  from 1 to 2 and re-scaling  and .

\subsubsection{Comparing Nesterov momentum and LARS}\label{sec:nesterov-vs-lars}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Nesterov & LARS \\ \hline
 & 2 & 1 \\ \hline
 & 7.05 & 29.0 \\ \hline
 &  &  \\ \hline
 & 0.02397 & 0.071 \\ \hline
 &  &  \\ \hline
 & 0.15 & 0.10 \\ \hline
 & 0.4138 & 0.0 \\ \hline
\end{tabular}
\caption{The hyperparameters of Configuration B that differ from state-of-the-art LARS at batch size 32,768 \citep{kumar2019scale}.}\label{table:best_nesterov_lars_hparams_resnet50}
\end{table}
Table~\ref{table:best_nesterov_lars_hparams_resnet50} shows the hyperparameter values for Configuration B that differ from the state-of-the-art LARS pipeline. Aside from re-tuning the momentum, learning rate scale, and regularization hyperparameters (whose optimal values are all expected to change with the optimizer), the only changes are setting  to 2 instead of 1 and re-tuning .

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/mlperf_lr_schedules.pdf}
    \caption{The learning rate schedules of LARS and Nesterov momentum Configuration B. Aside from re-scaling, the only difference is setting the warmup polynomial power to 2 instead of 1.}
    \label{fig:mlperf_lr_schedules}
\end{figure}
\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Nesterov & LARS \\ \hline
1 &  75.79\% & 75.97\% \\ \hline
2 & 75.92\% & 75.69\% \\ \hline
\end{tabular}
\caption{The best warmup schedule differs for Nesterov momentum and LARS. Values are medians over 50 training runs after setting  without retuning other hyperparameters.}
\label{table:swap_lr_schedule}
\end{table}
Figure~\ref{fig:mlperf_lr_schedules} shows the LARS learning rate schedule compared to the Nesterov momentum schedule. Even though these schedules are similar, we found that each optimizer had a different optimal value of the warmup polynomial power.
As Table~\ref{table:swap_lr_schedule} shows, Nesterov momentum performs better with  instead of 1, while the opposite is true with LARS.
As discussed in \citet{agarwal2020disentangling}, optimizers can induce implicit step size schedules that strongly influence their training dynamics and solution quality, and it appears from Table~\ref{table:swap_lr_schedule} that the implicit step sizes of Nesterov momentum and LARS may evolve differently, causing the shapes of their optimal learning rate schedules to differ.

\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
Optimizer & Train Acc & Test Acc \\ \hline
Nesterov &  &  \\ \hline
LARS &  &  \\ \hline
\end{tabular}
\caption{Median train and test accuracies over 50 training runs for Nesterov momentum Configuration B and LARS.}\label{table:generalization}
\end{table}
Although the main concern of a practitioner is validation performance, the primary task of an optimization algorithm is to minimize training loss. Table~\ref{table:generalization} shows that Nesterov momentum achieves higher training accuracy than LARS, despite similar validation performance. Thus, it may be more appropriate to consider the layerwise normalization of LARS to be a regularization technique, rather than an optimization technique.


Spending even more effort tuning LARS or Nesterov momentum would likely further improve the current state-of-the-art for that optimizer. Meaningful optimizer comparisons are only possible with independent and equally intensive tuning efforts, and we do not claim that either optimizer outperforms the other on this benchmark.
That said, if the main evidence for LARS's utility as a ``large-batch optimizer'' is its performance on this particular benchmark, then more evidence is needed to quantify any benefit it has over traditional, generic optimizers like Nesterov momentum. 

\subsection{Lessons learned}

In hindsight, it was only necessary to make a few changes to the LARS pipeline to match its performance at batch size 32,768 with Nesterov momentum. However, Table~\ref{fig:mlperf_lr_schedules} does not accurately represent the effort required when attempting to match a highly tuned training-speed benchmark.

Firstly, as described in Sections~\ref{sec:batch-norm} and ~\ref{sec:regularization}, the strong results of LARS depend partly on a few subtle optimization tricks and non-default values of uncommonly-tuned hyperparameters. Fortunately, in this case we could discover these tricks by examining the open-source code required for MLPerf submissions, but machine learning research papers do not always report these important details. Researchers can easily waste a lot of experiments and produce misleading results before getting all of these details right.
We demonstrate the importance of adding these tricks to our Nesterov momentum pipeline in Appendix~\ref{appendix:nesterov-ablations}; without these tricks (or some new tricks), we likely would not have been able to match the LARS performance.

Secondly, the learning rate schedule really matters when trying to maximize performance with a relatively small step budget. Both LARS and Nesterov momentum are sensitive to small deviations from the optimized learning rate schedules in Figure~\ref{fig:mlperf_lr_schedules}, and neither schedule works as well for the other optimizer. Although relatively minor changes were sufficient to match LARS with Nesterov momentum, there is no way to know \textit{a priori} how the optimal schedule will look for a new optimizer \cite{wu2018understanding}. Even in toy settings where the optimal learning rate schedule can be derived, it does not fit into commonly used schedule families and depends strongly on the optimizer \cite{zhang2019algorithmic}. 
Indeed, this problem applies to the other optimization hyperparameters as well: it is extremely difficult to know which are worth considering ahead of time.
Finally, even when we narrowed down our hyperparemeter search spaces around the optimal point, the volume of our search spaces corresponding to near-peak performance was small, likely due to the small step budget \citep{shallue2019measuring}. We investigate how these effects change with a less stringent step budget in Section~\ref{sec:step_budget}.
 
\section{Stronger BERT pretraining speed baselines}\label{sec:bert}


\citet{you2019lamb} developed the LAMB optimizer in the hope of speeding up training for BERT-Large \citep[Bidirectional Encoder Representations from Transformers,][]{devlin2018bert}.
BERT training consists of two phases. The ``pretraining'' phase has two objectives: (1) predicting masked tokens based on the rest of the sequence (a masked language model), and (2) predicting whether two given sentences follow one from another. Finally, the ``fine-tuning'' phase refines the model for a downstream task of interest.  BERT pretraining takes a considerable amount of time (up to 3 days on 16 Cloud TPU-v3 chips \cite{jouppi2017datacenter}), whereas the fine-tuning phase is typically much faster. Model quality is typically assessed on the downstream metrics, not on pretraining loss, making BERT training a somewhat awkward benchmark for optimization research.

\citet{you2019lamb} used LAMB for BERT pretraining with batch sizes up to 65,536 and claimed that LAMB outperforms Adam batch size 16,384 and beyond. The LAMB optimizer has since appeared in several NLP toolkits, including as Microsoft DeepSpeed and NVIDIA Multi-node BERT training, and as a benchmark task in MLPerf v0.7.\footnote{We do not consider the MLPerf task in this paper since it is a warm-start, partial training task.} 

\begin{table}[t]
\centering
 \begin{tabular}{|c|c|c|c|}
\hline
Batch size & Step budget & LAMB & Adam \\
\hline
32k & 15,625 & 91.48 & \textbf{91.58} \\
\hline
65k/32k & 8,599 & 90.58 & \textbf{91.04} \\
\hline
65k & 7,818 & -- & \textbf{90.46} \\
\hline
\end{tabular}
\caption{Using Adam for pretraining exceeds the reported performance of LAMB in \citet{you2019lamb} in terms of F1 score on the downstream SQuaD v1.1 task.}
\label{table:bert-results}
\end{table}
As shown in Table~\ref{table:bert-results}, we trained Adam baselines that achieve better results than both the LAMB and Adam results reported in \citet{you2019lamb}. Our new Adam baselines obtain better F1 scores on the development set of the SQuaD v1.1 task in the same number of training steps as LAMB for both batch size 32,768 and the hybrid 65,536-then-32,768 batch size training regime in \citet{you2019lamb}. We also ran Adam at batch size 65,536 to reach nearly the same F1 score as the hybrid batch size LAMB result, but in much fewer training steps. We believe 7,818 steps is a new state-of-the-art for BERT pretraining speed \citep[in our experiments, it also improves upon the 76-minute record claimed in][]{you2019lamb}. Additionally, at batch size  32,768 our Adam baseline got a better pretraining loss of 1.277 compared to LAMB's 1.342.

We used the same experimental setup as \citet{you2019lamb}, including two pretraining phases with max sequence lengths of 128 and then 512.
In order to match \citet{you2019lamb}, we reported the F1 score on the downstream SQuaD v1.1 task as the target metric, although this metric introduces potential confounds: optimization efficiency should be measured on the training task using training and held-out data sets. Fortunately, in this case better pretraining performance correlated a with higher F1 score after fine-tuning. See Appendix~\ref{appendix:bert_experiment_details} for additional experiment details. We tuned Adam hyperparameters independently for each pretraining phase, specifically learning rate , , , the polynomial power for the learning rate warmup , and weight decay , using quasi-random search \citep{bousquet2017critical}. See Appendix~\ref{appendix:adam_bert_hparams} for the search spaces.


In addition to hyperparmeter tuning, our improved Adam results at these batch sizes are also likely due to two implementation differences. First, the Adam implementation in \citet{you2019lamb} comes from the BERT open source code base, in which Adam is missing the standard bias correction.\footnote{\url{https://git.io/JtY8d}} The Adam bias correction acts as an additional step size warm-up, thereby potentially improving the stability in the initial steps of training. Second, the BERT learning rate schedule had a discontinuity at the start of the decay phase due to the learning rate decay being incorrectly applied during warm-up \footnote{See \url{https://git.io/JtnQW} and \url{https://git.io/JtnQ8}.} (see Figure~\ref{fig:mlperf_bert_schedules} in Appendix~\ref{appendix:experiment-details}). This peculiarity is part of the official BERT release and is present in 3000+ copies of the BERT Training code on GitHub.





 \section{Investigating a less stringent step budget}\label{sec:step_budget}
Part of what makes comparing optimizers so difficult is that the hyperparameter tuning tends to dominate the comparisons \citep{choi2019empirical}. Moreover, tuning becomes especially difficult when we demand a fixed epoch budget even when dramatically increasing the batch size \citep{shallue2019measuring}.
Fixing the epoch budget as the batch size increases is equivalent to demanding perfect scaling (i.e. that the number of training steps decreases by the same factor that the batch size is increased).
We can view the role of hyperparameter tuning for large batch training as resisting the inevitable end of perfect scaling.
For example, it might be possible to extend perfect scaling using delicately tuned learning rate schedules, but comparing optimizers under these conditions can make the learning rate schedule dominate the comparison by favoring some algorithms over others.
Therefore, in order to better understand the behavior of LARS and LAMB compared to Nesterov Momentum and Adam, we ran additional ResNet-50 experiments with a more generous 6,000 step budget (vs 2,512 in Section~\ref{sec:resnet50}) and a more simplistic cosine learning rate schedule. At batch size 32,768, this budget should let us reach better validation accuracy than the MLPerf target of 75.9\%.



Although not mentioned in \citet{you2017lars}, the state-of-the-art MLPerf pipeline for ``LARS'' actually uses both LARS and Heavy-ball Momentum, with Momentum applied to the batch normalization and ResNet bias parameters and LARS applied to the other parameters. \citet{you2019lamb} does not mention whether LAMB was only applied to some parameters and not others.
If layerwise normalization can be harmful for some model parameters, this is critical information for practitioners using LARS or LAMB, since it might not be obvious which optimizer to apply to which parameters.
To investigate this, we trained both pure LARS and LAMB configurations, as well as configurations that did not apply layerwise normalization to the batch normalization and ResNet bias parameters.
Moreover, LAMB's underlying Adam implementation defaults to , rather than the typical  or . In some cases,  can be a critical hyperparameter for Adam \citep{choi2019empirical}, so we included Adam configurations with both  and .



\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
\specialcell{Weights\\Optimizer} & \specialcell{Bias/BN\\Optimizer} & Accuracy \\ \hline
Nesterov & Nesterov & 76.7 \% \\ \hline LARS & Momentum & 76.9 \% \\ \hline LARS & LARS & 76.9 \% \\ \hline Adam () & Adam () & 76.2 \% \\ \hline Adam () & Adam () & 76.4 \% \\ \hline LAMB & LAMB & 27.3 \% \\ \hline LAMB & Adam () & 76.3 \% \\ \hline LAMB & Adam () & 76.3 \% \\ \hline \end{tabular}
\caption{Validation accuracy of ResNet-50 on ImageNet trained for 6,000 steps instead of 2,512. The second column is the optimizer that was applied to the batch normalization and ResNet bias variables. We report the median over 5 random seeds of the best hyperparameter setting in a refined search space. See Appendix~\ref{appendix:cosine_details} for more details.}
\label{table:cosine_schedule}
\end{table}

Table~\ref{table:cosine_schedule} shows the validation accuracy of these different configurations after training for 6,000 steps with batch size 32,768. In every case, we used a simple cosine decay learning rate schedule and tuned the initial learning rate and weight decay using quasi-random search. We used momentum parameters of 0.98 for Nesterov momentum and 0.929 for LARS, respectively, based on the tuned values from Section~\ref{sec:resnet50}. We used default hyperparameters for Adam and LAMB except where specified. We set all other hyperparameters to the same values as the state-of-the-art LARS pipeline, except we set . See Appendix~\ref{appendix:cosine_details} for more details. As expected, highly tuned learning rate schedules and optimizer hyperparameters are no longer necessary with a less stringent step budget. Multiple optimizer configurations in Table~\ref{table:cosine_schedule} exceed the MLPerf target accuracy of 75.9\% at batch size 32,768 with minimal tuning. Training with larger batch sizes is \emph{not} fundamentally unstable: stringent step budgets make hyperparameter tuning trickier.


In Table~\ref{table:cosine_schedule}, ``pure LAMB'' performs extremely poorly: LAMB only obtains reasonable results when it is \textit{not} used on the batch normalization and ResNet bias parameters, suggesting that layerwise normalization can indeed be harmful on some parameters. ``Pure LARS'' and Nesterov momentum perform roughly the same at this step budget, but the MLPerf LARS pipeline, which is tuned for a more stringent step budget, does not use LARS on all parameters, at least suggesting that the optimal choice could be budget-dependent.


Many new optimizers for neural networks, including LAMB, are introduced alongside claims that the new optimizer does not require any---or at least not very much---tuning. Unfortunately, these claims require a lot of work to support, since they require trying the optimizer on new problems without using those problems during the development of the algorithm. Although our experiments here are not sufficient to determine which optimizers are easiest to tune, experiments like these that operate outside the regime of highly tuned learning rate schedules can serve as a starting point.
In this experiment, LARS and LAMB do not appear to have an advantage in how easy they are to tune even on a dataset and model that were used in the development of both of those algorithms. LAMB is a variant of Adam and performs about the same as Adam with the same value of ; LARS is more analogous to Momentum and indeed Nesterov momentum and LARS have similar performance.
 \section{Discussion}\label{sec:discussion}

Our results show that standard, generic optimizers suffice for achieving strong results across batch sizes.
Therefore, any research program to create new optimizers for training at larger batch sizes must start from the fact that Momentum, Adam, and likely other standard methods work fine at batch sizes as large as those considered in this paper.
The LARS and LAMB update rules have no more to do with the batch size (or ``large" batches) than the Momentum or Adam update rules. Whether layer-wise normalization can be useful for optimization or regularization remains an open question. However, if LARS and LAMB have any advantage over standard techniques, it is not that they work dramatically better on the tasks and batch sizes in \citet{you2017lars,you2019lamb}.
It should not surprise us that standard techniques continue to work as we increase the batch size -- increasing the batch size should make optimization easier, not harder, as the stochastic estimate of the full batch gradient becomes more accurate.\footnote{Of course, if the number of epochs is kept fixed as the batch size increases then performance may degrade due to using fewer updates.} This is not to suggest that there is nothing interesting about studying neural network optimization at larger batch sizes. For example, as gradient noise decreases, there may be opportunities to harness curvature information and extend the region of perfect scaling \citep{zhang2019algorithmic}. However, there is currently no evidence that LARS and LAMB scale better than Momentum and Adam.


Our primary concern in this paper has been matching the state of the art---and establishing new baselines---for \emph{training speed} measurements of the sort used to justify new techniques and algorithms for training with larger batch sizes. In contrast, many practitioners are more concerned with obtaining the best possible validation error with a somewhat flexible training time budget. Part of the reason why matching LARS at batch size 32,768 was non-trivial is because getting state of the art training speed requires several tricks and implementation details that are not often discussed. It was not obvious to us \emph{a priori} which ones would prove crucial. These details do not involve changes to the optimizer, but they interact with the optimizer in a regime where all hyperparameters need to be well tuned to stay competitive, making it necessary to re-tune everything for a new optimizer. 

In neural network optimization research, training loss is rarely discussed in detail and evaluation centers on validation/test performance since that is what practitioners care most about. However, although we shouldn't \emph{only} consider training loss, it is counter-intuitive and counter-productive to elide a careful investigation of the actual objective of the optimizer. If a new optimizer achieves better test performance, but shows no speedup on training loss, then perhaps it is \emph{not} a better optimizer so much as an indirect regularizer.\footnote{Deep learning folk wisdom is that ``any method to make training less effective can serve as a regularizer," whether it is a bug in gradients or a clever algorithm.} Indeed, in our experiments we found that Nesterov momentum achieves noticeably better training accuracy on ResNet-50 than the LARS configuration we used, despite reaching roughly the same validation accuracy. Properly disentangling possible regularization benefits from optimization speed-ups is crucial if we are to understand neural network training, especially at larger batch sizes where we lose some of the regularization effect of gradient noise. Hypothetically, if the primary benefit of a training procedure is regularization, then it would be better to compare the method with other regularization baselines than other optimizers.




Ultimately, we only care about batch size to the extent that higher degrees of data parallelism lead to faster training. Training with a larger batch size is a means, not the end goal.
New optimizers---whether designed for generic batch sizes or larger batch sizes---have the potential to dramatically improve algorithmic efficiency across multiple workloads, but our results show that standard optimizers can match the performance of newer alternatives on the workloads we considered.
Indeed, despite the legion of new update rule variants being proposed in the literature, standard Adam and Momentum remain the workhorses of practitioners and researchers alike, while independent empirical comparisons consistently find no clear winner when optimizers are compared across a variety of workloads \citep{schmidt2020descending}. Meanwhile, as \citet{choi2019empirical} and our results underscore, comparisons between optimizers crucially depend on the effort spent tuning hyperparameters for each optimizer. Given these facts, we should regard with extreme caution studies claiming to show the superiority of one particular optimizer over others. Part of the issue stems from current incentives in the research community; we overvalue the novelty of new methods and undervalue establishing strong baselines to measure progress against. This is particularly problematic in the study of optimizers, where the learning rate schedule is arguably more important than the choice of the optimizer update rule itself! As our results show, the best learning rate schedule is tightly coupled with the optimizer, meaning that tuning the learning rate schedule for a new optimizer will generally favor the new optimizer over a baseline unless the schedule of the baseline is afforded the same tuning effort. Unfortunately, these kinds of subtleties are extremely difficult to account for and must be kept in mind when interpreting empirical comparisons of new optimizers to self-reported baselines.

 \vspace{-1em} \section{Conclusion}\label{sec:conclusion}

In this work, we demonstrated that standard optimizers, without any layer-wise normalization techniques, can match or exceed the large batch size results used to justify LARS and LAMB. 
Our results did not require specialized ``large batch optimizers’’ or any new techniques whatsoever, only hyperparameter tuning and replicating all of the essential implementation details.

Future work attempting to argue that a new algorithm is useful by comparing to baseline methods or results, including those established in this paper, faces a key challenge in showing that the gains are due to the new method and not merely due to better tuning or changes to the training pipeline (e.g. regularization tricks).
Although gains from tuning will eventually saturate, we can, in principle, always invest more effort in tuning and potentially get better results for any optimizer. However, our goal should be developing optimizers that work better across many different workloads when taking into account the amount of additional tuning they require.

Moving forward, if we are to reliably make progress we need to rethink how we compare and evaluate new optimizers for neural network training. Given how sensitive optimizer performance is to the hyperparameter tuning protocol and how difficult it is to quantify hyperparameter tuning effort, we can't expect experiments with self-reported baselines to always lead to fair comparisons. Ideally, new training methods would be evaluated in a standardized competitive benchmark, where submitters of new optimizers do not have full knowledge of the evaluation workloads. Some efforts in this direction have started, for instance the MLCommons Algorithmic Efficiency Working Group\footnote{\url{https://mlcommons.org/en/groups/research-algorithms/}}, but more work needs to be done to produce incentives for the community to publish well-tuned baselines and to reward researchers that conduct the most rigorous empirical comparisons.











 \section*{Acknowledgements}
We would like to thank Roy Frostig for helpful discussions and valuable feedback on the manuscript. We would also like to thank James Bradbury for encouraging us to start this project and for help with the JAX MLPerf code.
Finally, we would like to thank Dehao Chen and Tao Wang for their assistance with BERT training using TensorFlow and for helpful discussions.


\bibliography{references}
\bibliographystyle{arxiv}








\clearpage


\appendix
\section{Additional experiment details}\label{appendix:experiment-details}

\subsection{ResNet-50 training benchmark}

All experiments were run on Google TPUs \citep{jouppi2017datacenter}. 
The ResNet-50 experiments used Jax \citep{jax2018github} using the Flax library, with code to be released soon. The BERT experiments were run using TensorFlow \citep{abadi2016tensorflow}  version 1.15.
We used the standard train/validation split from the previous literature and MLPerf competition.

For ImageNet, we used the following sequence of TensorFlow functions for pre-processing:\footnote{Full code available at \url{https://git.io/JtgtE}}
\begin{lstlisting}
tf.image.sample_distorted_bounding_box
tf.image.decode_and_crop_jpeg
tf.image.resize
tf.image.random_flip_left_right
tf.image.convert_image_dtype
\end{lstlisting}

\subsection{BERT pre-training}\label{appendix:bert_experiment_details}

We used the same experimental setup as the official BERT codebase\footnote{\url{https://github.com/google-research/bert}} and the standard train/test split from the previous literature. This matches the experimental setup of \citet{you2019lamb}.

We trained the two pretraining objectives on the combined Wikipedia and Books corpus \citep{ZhuEtAl2015bookcorpus} datasets (2.5B and 800M words, respectively). We used sequence lengths of 128 and 512, respectively, for the pretraining tasks. We ran the fine-tuning phase on the SQuaD v1.1 question answering task. In order to match \citet{you2019lamb}, we report the F1-score on the dev set as the target metric. We followed the fine-tuning protocol described in the LAMB optimizer setup and did not perform any additional tuning for fine-tuning.

We tuned Adam hyperparameters using quasi-random search \citep{bousquet2017critical} in a simple search space. Hyperparameters included learning rate , , , the polynomial power for the learning rate warmup , and weight decay . We fixed the  in Adam to  for all BERT experiments. See Appendix~\ref{appendix:adam_bert_hparams} for the search spaces. We selected the best trial using the masked language model accuracy over 10k examples from the training set. The number of training steps for each of the phases, as well as the warmup steps are identical to \citet{you2019lamb} and are listed in Appendix~\ref{appendix:adam_bert_hparams}.
Each phase of pretraining used completely independent Adam hyperparameters. We found the final hyperparameters within 30 trials of random search for each of the phases, except for the second phase of 65,536 batch size which used 130 trials. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/mlperf_bert_schedules.pdf}
    \caption{An illustration of the sudden drop in the BERT learning rate schedule in the official codebase.}
    \label{fig:mlperf_bert_schedules}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/bert_box_plots.pdf}
    \caption{6 finetuning runs starting from the same pretraining checkpoint to show the stability of our results, at each of the 32,768, mixed 65,536-32,768, and 65,536 batch size settings.}
\end{figure}



\section{Nesterov ablations}\label{appendix:nesterov-ablations}
To explore the sensitivity of our best Nesterov momentum configuration (Configuration A), we ablated several elements of the experiment pipeline, one at a time, and tested their impact on performance. Figure~\ref{fig:one_off_ablations_nesterov} shows the results of these experiments. ``Base'' refers to Nesterov momentum Configuration A (Table~\ref{table:nesterov_config}). ``ResNet version'' is the same point as ``Base'' but with ResNet version 1.0 instead of version 1.5. ``BN init'' is the same point as ``Base'' but with  instead of 0.4138. ``Virtual BN'' is the same point as ``Base'' but with a virtual batch size of 256 instead of 64, which is the largest that fits in a single TPUv3 core. ``BN \& LR tuning'' is Configuration B (Table~\ref{table:nesterov_config}), the same point as ``Base'' but with  set to their values in the LARS pipeline. Finally, ``L2 variables'' is the same point as ``Base'' but where the L2 regularization is applied to all variables.
The only ablation whose median over 50 seeds continues to beat the target 75.9\% accuracy (noted by the dotted red line) is ``BN \& LR tuning'', with the rest having between 0.1\%-0.3\% drops in median accuracy.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/one_off_ablations_nesterov.pdf}
    \caption{Distributions over 50 training runs for each ablation study around our best Nesterov momentum configuration (Configuration A). The dotted red line is at the target accuracy of 75.9\%, and the boxes show the min, max, and quartiles of the distribution of accuracies over the 50 training runs.}
    \label{fig:one_off_ablations_nesterov}
\end{figure}

\section{Hyperparameter tuning}

\subsection{Nesterov momentum training speed on ResNet-50}
\label{appendix:nesterov_config}

We considered two configurations of Nesterov hyperparameters: Configuration A, where we tuned a wide set of hyperparameters in the experiment pipeline, and Configuration B, where we reverted the less impactful hyperparameters to the same values as the LARS baseline (or in the case of , a simpler value). We included Configuration B in order to demonstrate the minimal set of changes to the baseline necessary to still reach the target accuracy. The hyperparameter values for these configurations can be found in Table~\ref{table:nesterov_config}. 

\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|c|}
\hline
 & Configuration A & Configuration B & LARS \\ \hline
 & 638 & 706 & 706 \\ \hline
 & 2.497 & 2.0 & 1.0 \\ \hline
 & 1.955 & 2.0 & 2.0 \\ \hline
 & 0.94 & 0.9 & 0.9\\ \hline
 &  &  &   \\ \hline
 & 7.05 & 7.05 & 29.0 \\ \hline
 &  &  &  \\ \hline
 & 0.02397 & 0.02397 & 0.071 \\ \hline
 &  &  & \num{e-4} \\ \hline
 & 0.15 & 0.15 & 0.10 \\ \hline
 & 0.4138 & 0.4138 & 0.0 \\ \hline
\end{tabular}
\caption{Nesterov momentum Configurations A and B.}\label{table:nesterov_config}
\end{table}

\subsection{Adam on BERT}
\label{appendix:adam_bert_hparams}
The search space used to tune Adam on BERT for all phases of the pipeline can be found in Table~\ref{table:bert_adam_tuning_space}, which yielded our best Adam results on BERT in Table~\ref{table:bert_results_params}.

\begin{table}[t]
\label{table:search_space_bert}
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
Hyperparameter & Range & Scaling \\ \hline
 &  & Discrete \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
\end{tabular}
\caption{The search space used to tune Adam on BERT for all phases of the pipeline.  refers to weight decay and  refers to the polynomial power in the learning rate schedule for both the warmup and decay phases.}
\label{table:bert_adam_tuning_space}
\end{table}

\begin{table*}[t!]
\centering
 \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Batch size & Phase & Seq len & Warmup & Train & Learning &  &  &  &  \\
& & & steps & steps & rate & & & & \\
\hline
32,768 &  1 & 128 & 3,125 & 14,063 & \num{5.9415e-4} & 0.934271 & 0.989295  & 0.31466 & 1 \\
\hline
32,768 &  2 & 512 & 781 & 1,562 & \num{2.8464e-4} & 0.963567 & 0.952647  & 0.31466 & 1 \\
\hhline{|=|=|=|=|=|=|=|=|=|=|}
65,536 &  1 & 128 & 2,000 & 7,037 & \num{1.3653e-3} & 0.952378 & 0.86471  & 0.19891 & 2 \\
\hline
32,768 &  2 & 512 & 781 & 1,562 & \num{2.8464e-4} & 0.952647 & 0.963567  & 0.19891 & 2\\
\hline
65,536 &  2 & 512 & 390 & 781 & \num{6.1951e-5} & 0.65322 & 0.82451  & 0.19891 & 2 \\
\hline
\end{tabular}
\caption{Best hyperparameters from tuning Adam on BERT-Large pretraining.  refers to weight decay and  refers to the polynomial power in the learning rate schedule for both the warmup and decay phases. All trials used .}
\label{table:bert_results_params}
\end{table*}

\begin{table*}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Weights Optimizer & Bias/BN Optimizer & Name & Initial Range & Final Range & Best \\ \hline
Nesterov & Nesterov &  & np.logspace(-.5, .5, 10) &  & 1.173 \\ \hline
Nesterov & Nesterov &  & np.logspace(-4, -3, 10) &  &  \\ \hhline{|=|=|=|=|=|=|}
LARS & \specialcell{Heavy-ball\\momentum} &  & np.logspace(0, 2, 10) &  & 14.49 \\ \hline
LARS & \specialcell{Heavy-ball\\momentum} &  & np.logspace(-5, -2, 10) &  &  \\ \hhline{|=|=|=|=|=|=|}
LARS & LARS &  &  &  & 14.18 \\ \hline
LARS & LARS &  &  &  &  \\ \hhline{|=|=|=|=|=|=|}
Adam () & Adam () &  &  &  & 0.004596 \\ \hline
Adam () & Adam () &  &  &  & 0.6182 \\ \hhline{|=|=|=|=|=|=|}
Adam () & Adam () &  & np.logspace(-3, 0, 10) &  &  \\ \hline
Adam () & Adam () &  & np.logspace(-2, 0.5, 6) &  & 1.055 \\ \hhline{|=|=|=|=|=|=|}
LAMB & LAMB &  & np.logspace(-4, 0, 30) &  & 0.01134 \\ \hline
LAMB & LAMB &  & np.logspace(-5, -2, 4) &  & 0.02657 \\ \hhline{|=|=|=|=|=|=|}
LAMB & Adam () &  &  &  & 0.02569 \\ \hline
LAMB & Adam () &  &  &  & 2.500 \\ \hhline{|=|=|=|=|=|=|}
LAMB & Adam () &  & np.logspace(-3, 0, 10) &  & 0.03378 \\ \hline
LAMB & Adam () &  & np.logspace(-2, 0.5, 6) &  & 4.197 \\ \hline
\end{tabular}
\caption{Search spaces used for the 6,000 step, cosine learning rate schedule experiments. All hyperparameters were tuned on a logarithmic scale, except for those which define a discrete sequence of points to evaluate such as ``np.logspace''.}
\label{table:cosine_hparam_tunings}
\end{table*}

\subsection{Less stringent step budget on ResNet-50}
\label{appendix:cosine_details}
All trials used a cosine decay learning rate schedule and tuned the initial learning rate  and L2 regularization or weight decay parameter\footnote{As suggested in \citet{you2019lamb}, we used L2 regularization for LARS and weight decay for LAMB. For consistency, we used L2 regularization for Nesterov momentum (which is more analogous to LARS) and weight decay for Adam (which is more analogous to LAMB).}  according to Table~\ref{table:cosine_hparam_tunings}. We used 50 or more trials to search in the ``Initial Range'' and then 25 trials to search in the refined ``Final Range.'' Finally, we ran the best point from the latter for 5 random seeds. When LARS or LAMB were used alongside a different optimizer for the batch normalization and ResNet-50 bias parameters, we set  on the batch normalization and ResNet-50 bias parameters. When LAMB was used all parameters, the majority of trials diverged during training -- it took \textbf{67 trials} to get 25 trials that did not NaN during training. Our trial budgets refer to the number of feasible trials, i.e. trials that do not diverge during training.





\subsection{Nesterov ResNet50 search space chronology}
\label{appendix:historical_search_spaces_nesterov_resnet50}

Below we list the sequence of search spaces we used to arrive at our final values in Table~\ref{table:nesterov_config}. Given that the final results reported in papers are rarely found in a single iteration of experiments, we believe that it is important to document the full journey to arriving at our results. 

Note that although we tuned a wide range of hyperparameters to match the LARS result with Nesterov momentum, we later realized that many of these hyperparameters could be reverted to the values from the LARS pipeline (see Table~\ref{table:nesterov_config}).
We started tuning with a training budget of 2,815 steps, which is the number of steps in the MLPerf 0.6 submission. We sometimes would decrease this to 2,658 steps to test how decreasing the training budget would affect tuning performance, before eventually moving to the 2,512 steps used to generate the results in the main text.


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Discrete \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{First search space of the Nesterov tuning journey. The search spaces were mostly by informed guesses by the authors.  refers to weight decay, which is applied to all variables. Tuned for 251 trials. Trained for 2,815 steps (``72 epochs'' as defined by MLPerf epoch calculations). We used a linear learning rate decay schedule that decays for all training steps, starting from  and ending at . Virtual batch size 128.}
\label{table:imagenet_mlperf_nesterov_vizier_2815}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Discrete \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{Same as Table~\ref{table:imagenet_mlperf_nesterov_vizier_2815} but trained for 2,658 steps (``68 epochs'' as defined by MLPerf epoch calculations) for 50 trials.}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Discrete \\ \hline
 &  & Linear \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is now not applied to the bias and batch normalization variables. 50 trials. Trained for 2,658 steps. Linear learning rate decay schedule that decays for  steps, starting from  and ending at . Virtual batch size 128.}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Discrete \\ \hline
 &  & Linear \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,658 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step , and then is constant at the final learning rate . Virtual batch size 128. We increased the max learning rate based off the larger learning rates used by LARS. \textbf{We also ran two additional studies which were the same except with 250 and 977 warmup steps.}}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step , and then is constant at the final learning rate . Virtual batch size 128.}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step , and then is constant at the final learning rate . Virtual batch size 128.}
\label{table:imagenet_mlperf_nesterov_viz2_500w_2815}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{The same as Table~\ref{table:imagenet_mlperf_nesterov_viz2_500w_2815} except with virtual batch size 64.}
\label{table:imagenet_mlperf_nesterov_viz_vbn64_2815}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 & \specialcell{ \\ } & Discrete \\ \hline
 &  & -- \\ \hline
 & 2250 & -- \\ \hline
 & 0.02397 & -- \\ \hline
 & 0.009992 & -- \\ \hline
 & 0.07786 & -- \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters from Table~\ref{table:imagenet_mlperf_nesterov_viz_vbn64_2815}, we swept over the peak learning rate in a discrete set of ten values per order of magnitude, \textbf{each for three random seeds}, to find the max stable learning rate.}
\label{table:imagenet_mlperf_nesterov_lrtune_2815}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &4.118 & -- \\ \hline
 &  & -- \\ \hline
 & 2250 & -- \\ \hline
 & 0.02397 & -- \\ \hline
 & \specialcell{ \\ } & Discrete \\ \hline
 & 0.07786 & -- \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters from Table~\ref{table:imagenet_mlperf_nesterov_viz_vbn64_2815}, we swept over the weight decay in a discrete set of twenty values per order of magnitude, to test how high the regularization has to be in this region of hyperparameter space.}
\label{table:imagenet_mlperf_nesterov_wdtune_2815}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 & 4.118 & -- \\ \hline
 &  & -- \\ \hline
 & 2250 & -- \\ \hline
 & 0.02397 & -- \\ \hline
 & 0.009992 & -- \\ \hline
 & 0.07786 & -- \\ \hline
 & \specialcell{ \\ } & Discrete \\ \hline
 & \specialcell{ \\ } & Discrete \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is not applied to the bias and batch normalization variables. Trained for 2,815 steps. Virtual batch size 64. Using the best hyperparameters from Table~\ref{table:imagenet_mlperf_nesterov_viz_vbn64_2815}, we swept over batch normalization hyperparameters.}
\end{table}


\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
 &  & Linear \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Linear \\ \hline
 &  & Linear \\ \hline
 &  & Linear \\ \hline
\end{tabular}
\caption{ refers to weight decay, which is not applied to the bias and batch normalization variables. 50 trials. Trained for 2,815 steps. Linear warmup for 500 steps followed by a quadratic decay, which decays until step , and then is constant at the final learning rate . Virtual batch size 64. Peak learning rate range was consolidated based off the results of Table~\ref{table:imagenet_mlperf_nesterov_lrtune_2815}. The weight decay range was consolidated based off the results of Table~\ref{table:imagenet_mlperf_nesterov_wdtune_2815}.}
\end{table}

\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 &  & Linear \\ \hline
 &  & Linear \\ \hline
 & 1.8 & -- \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 &  & Log \\ \hline
 & 0.02397 & -- \\ \hline
 &  & -- \\ \hline
 & 0.15 & -- \\ \hline
 &  & Linear \\ \hline
 & 0.94 & -- \\ \hline
 &  & -- \\ \hline
\end{tabular}
\caption{Here we switched  to refer to L2 regularization. We also began training for 2,512 steps, which is the final ``64 epochs'' used in the Nesterov results reported in the main text. Because of this more stringent step budget, we focused on the learning rate schedule.  was set to all remaining steps after the warmup was finished. Tuned for 229 trials. Virtual batch size 64.}
\label{table:momentum_vizier4_random}
\end{table}

\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 & 638 & -- \\ \hline
 &  & Linear \\ \hline
 &  & Linear \\ \hline
 & 0.12 & -- \\ \hline
 & 7.05 & -- \\ \hline
 &  & Log \\ \hline
 &  0.02397 & -- \\ \hline
 &  & Log \\ \hline
 & 0.15 & -- \\ \hline
 &  & Linear \\ \hline
 & 0.94 & -- \\ \hline
 &  & -- \\ \hline
\end{tabular}
\caption{Here we began focusing more on the shape of the learning rate schedule, as well as retuning the L2 regularization.  refers to L2. Several values were picked from the best trial of Table~\ref{table:momentum_vizier4_random}. Trained for 2,512 steps steps. Tuned for 15 trials. Virtual batch size 64.}
\label{table:momentum_vizier5_random}
\end{table}



\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 & 638 & -- \\ \hline
 &  & Linear \\ \hline
 &  & Linear \\ \hline
 & 0.12 & -- \\ \hline
 & 7.05 & -- \\ \hline
 &  & Log \\ \hline
 &  0.02397 & -- \\ \hline
 &  & Log \\ \hline
 & 0.15 & -- \\ \hline
 &  & Linear \\ \hline
 & 0.94 & -- \\ \hline
 &  & -- \\ \hline
\end{tabular}
\caption{Here we focus in more on tuning the L2 regularization.  refers to L2. Trained for 2,512 steps steps. Tuned for 37 trials. Virtual batch size 64.}
\end{table}

\begin{table}[t]
\centering
\setlength{\extrarowheight}{3.5pt}
\begin{tabular}{|c|c|c|}
\hline
 & Range & Scaling \\ \hline
 & 638 & -- \\ \hline
 &  & Linear \\ \hline
 &  & Linear \\ \hline
 & 0.12 & -- \\ \hline
 & 7.05 & -- \\ \hline
 &  & Log \\ \hline
 &  0.02397 & -- \\ \hline
 &  & Linear \\ \hline
 & 0.15 & -- \\ \hline
 &  & Linear \\ \hline
 & 0.94 & -- \\ \hline
 &  & -- \\ \hline
\end{tabular}
\caption{Again we dial in more on a tighter tuning range for the L2 regularization.  refers to L2. Trained for 2,512 steps steps. Tuned for 37 trials. Virtual batch size 64.}
\end{table}
 

\end{document}
