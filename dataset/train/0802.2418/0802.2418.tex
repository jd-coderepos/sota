\secput{indep}{Independent Jobs}

This section describes an -approximation algorithm for
\suui, the \suu problem with independent jobs.  We first give an
oblivious -approximation algorithm for \suui, based on
scheduling an (approximation of an) integer linear program.  We then
modify this algorithm into a semioblivious solution consisting of
 nearly optimal phases.  

\subheading{An oblivious -approximation}

We now describe an -approximation for \suui, called
\ALGobl. 
\punt{which represents an improvement over the oblivious
-approximation given in~\cite{LinRa07}.}
Our approach constructs a schedule of length
, based on an integer linear program, such that each job
has no more than a constant probability of failure upon completion.
This finite oblivious schedule is repeated until all jobs have completed. 
Using Chernoff bounds, we conclude that the expected number of repetitions is 
,  yielding an -approximation.

We use the following integer linear program for \ALGobl.  Let 
denote the number of steps during which machine  is assigned to
job~.  Recall  is the log failure of
job~ on machine~.  Let  be a fixed positive real,
representing a target log mass for each job, and let 
be a subset of jobs that need to achieve that log mass.  For now,
think of  as being fixed at , and .  We assign
different values to  later for the semioblivious -approximation.

Here \eqref{lp1aggfailure} enforces that every
job in  has a failure probability no greater than
, and 
\punt{
\footnote{Any positive constant works here.  A constant
  in the constraint strictly less than  (corresponding to a failure
  probability strictly greater than ) simplifies the proof of
  \lemref{lplower}.}
  }
\eqref{lp1int} guarantees that all jobs
are scheduled for an integral number of steps on each machine.
We use \lpref{lp1} to refer to this integer linear program
generically, and  to refer to it with particular 
values of  and .  We denote the optimal value for 
by .



A solution for  naturally generalizes to a finite
oblivious schedule, denoted by , with
length  as follows.  Consider a machine~,
and consider each job~ in arbitrary order.  Assign machine~ to
job~ for  timesteps. To finish our description of this schedule, 
we first claim that  approximates  
(the proof appears in \appref{app}).
Then we show how to approximate \lpref{lp1} in polynomial time.

\setthmcount{lplower}
\newcommand{\lplowerthm}{
\begin{lemma}\lemlabel{lplower}
  
\end{lemma}}
\lplowerthm
\newcommand{\lplowerproof}{
\begin{proof}
Let  be the optimum solution to .  Consider
  any subset , and its complement .  Then
  ,
  since we can construct a solution to  by adding
  a solution to  and a solution to
  .

  Now recall our view of the problem in terms of \suureform: there is
  an  chosen uniformly at random from  for each job 
  such that job  completes only if .  For any sample from the event
  space, let  be the set of jobs  for which , and let
   be the complement set of jobs  for which  (note  with probability 0 so can be ignored).  By
  definition, each job is in  independently with probability .
  Next observe that however OPT generates its schedule, it must
  allocate at least  unit of ``work'' to each job in ; in
  other words, \eqref{lp1aggfailure} of \lpref{lp1} must hold for
  every .  Thus, the optimum schedule contains a feasible
  solution to .

  Now observe that by construction,  is a uniformly random subset
  of , meaning all subsets are equally likely.  Thus, 

Where the second line of this derivation follows from the first
paragraph of this proof.
\punt{
Viewing the problem in terms of \suureform, there is an  chosen
  uniformly at random from  for each job  such that job 
  completes only if .  

  To prove the claim, we first show that with constant probability, at
  least half the jobs require a constant log mass.  We then show that
  with constant probability, a random selection of  jobs yields
  an \lpref{lp1} instance with optimal solution
  .  We combine these two steps to
  complete the proof.
  
  Let  be a Bernoulli random variable with value  if , or equivalently .  Said
  differently,  when an execution of 
  would complete job~.  When , any execution (and in
  particular an execution of ) must give job  a
  log mass exceeding .  Our goal is thus to show that a large
  fraction of jobs have .  Let .  Then .  Applying Markov's inequality gives that .  Thus,
  with constant probability at least , any schedule
  (and in particular ) must assign machines to jobs
  such that at least half of them have log mass exceeding~ (and
  hence satisfy \eqref{lp1aggfailure}).
  
  Let , and consider any uniformly
  random selection  of  jobs.\footnote{For
    simplicity here, without loss of generality, we assume  is a
    multiple of .}  Then either 
  or , since solving the linear
  program for  then  solves it for .  We observe that
  since , both  and  are
  equalLy likely to be selected as the random subset.  Thus, with
  probability at least , we have 
  
  We thus have that with probability at least ,  must schedule the jobs such that at least half of them
  satisfy \eqref{lp1aggfailure}, and such that \lpref{lp1} applied to
  those jobs has length at least .  We therefore conclude that
  , and hence .
}
\end{proof}
}

The following lemma states that, in polynomial time, we can find an integral
assignment that approximates \lpref{lp1} to within a constant factor.
Some aspects of the proof are similar to~\cite[Theorem 4.1]{LinRa07},
\punt{which solves a slightly different problem useful for precedence
constraints that form disjoint chains,} but we add several steps \punt{(made
possible by our formulation involving log failure)} that improve the
approximation ratio.  
\punt{
Our tighter approximation \emph{does} apply to
the more general disjoint-chains variant, which we revisit in
\secref{constr}.
}

\begin{lemma}\lemlabel{lp1rounding}
There exists a polynomial-time algorithm that computes a feasible
  solution to  having value
  .
\end{lemma}
\begin{proof}
  We relax our integer linear program to a linear program, and then
  show that the relaxed LP can be rounded to yield an integral
   solution with value
  .
  
  First, let .  Then we replace
  each  in \eqref{lp1aggfailure} with ,
  yielding the constraint .  Note that since assignments are restricted to be
  integral, this change has no effect on either the feasibility or the
  value of an assignment.  Next we remove \eqref{lp1int} and solve the
  relaxed linear program.\punt{ \footnote{As far as the relaxed
      linear program is concerned, the change from  to
       may have a big effect on the feasibility of
      solutions.  In particular, suppose there exists a machine~
      such that  for all .  Then an LP based on
       may assign all jobs to machine~ for  time
      steps.}  }  Letting  be an optimal
  solution, we note that , because
  integral solutions are feasible.
  
  Our goal now is to round the LP solution to an integral solution,
  while not increasing its value by very much.  We 
  proceed in three steps.  First, we group machines having
  similar  for a job~, yielding a single assignment for the
  whole group.  Then, we round those assignments to integers.
  Finally, we show that the rounded assignments satisfy
   \lpref{lp1}, using an integral flow network.
  
   For each job , we group machines having  values
   within a factor of 2, and determine the total assignment to that
   group.  More formally, for each  and integer , we let
   \vspace{-.2em} be the total assignment of machines with
    to job~.\punt{ \footnote{Note
       that there are at most  values of  for which .}  }  It should be clear that , for all
   .
  
  We next round the value of  up to .  We
  claim that .
  Note that since , the maximum value of 
  having nonzero  is .  Thus, the claim
  follows because .  In other words,
  rounding the group assignments down to integers can only cause us to
  lose a log mass of at most .  We thus need an assignment giving
   log mass to the job.  
  \punt{The constant  appears because we lose
  a factor of  in our groupings.}
  
  To complete the integral assignment, we construct a network-flow
  instance as follows.  For each job~ and integer , we have a
  node .  For each machine~, we have a node .  We also
  add a source-node~ and a sink-node~.  For each , we
  add a directed edge  with capacity .
  For each , we add a directed edge  with capacity
  .  Finally, we add a directed edge  with
  infinite capacity, for any  such that .  Note that for a given~ and~, there is
  exactly one  such that  exists.  We refer to this
  edge as edge .
   
   Note that if we make the capacity of edges  be
   instead, then a flow of
  demand  exists in this network.
  \punt{
  , and it can be constructed by
  setting the flow along edge  to be  (and flow
  along edge  to be ).
  }  
  Thus, a flow of capacity  exists 
  when we lower the capacity of edge  to .
  
  Ford-Fulkerson's theorem~\cite{FordFu62,CormenLeRi01} states that an
  integral max flow exists whenever the capacities are integral, as
  they are here.  We therefore take the flow across the edges 
  as our integral assignments .  Moreover, by
  construction,  satisfy  for all , and  for all .  We thus have an integral feasible
  solution .  Noting that  completes the proof.
\end{proof}

Recall that \lemref{lplower} shows that .  Then \lemreftwo{lplower}{lp1rounding} in concert state
that in polynomial time, we can find a schedule  of length
, such that every job has at most a constant
probability of failure. Repeating  until all jobs complete
gives our oblivious schedule \ALGobl.  The proof appears in \appref{app}.
\setthmcount{indbound}
\newcommand{\indboundthm}{
\begin{theorem}\thmlabel{indbound}
  Let  denote the random variable corresponding to the
  amount of time it takes for an execution of \ALGobl to complete all
  jobs.  Then .
\end{theorem}
}
\indboundthm
\newcommand{\indboundproof}{
\begin{proof}
  From \lemreftwo{lplower}{lp1rounding}, we have a schedule 
  of length  that gives each job a constant
  probability of success.  Applying a Chernoff bound gives us that a
  particular job completes in  repetitions of ,
  with probability at least , where the constant
  exponent appears as a constant factor in the number of repetitions.
  Taking a union bound over all jobs gives that with probability at
  least , \emph{all} jobs complete in 
  repetitions.  Since this probability drops off dramatically as the
  number of repetitions increases, we have .
\end{proof}
}\vspace{-.5em}
\subheading{A semioblivious -approximation}

We construct our semioblivious schedule \ALGsem as follows.  The
schedule is divided into ``rounds.''  The first round corresponds to
an execution of the schedule suggested by the (rounded) solution
to .  In each following round, \lpref{lp1} is
applied to all remaining jobs with doubling targets.  If  
 are the set of jobs left at
the start of round ,  then for that round we find an approximate solution 
to , and schedule obliviously according to it.

\ALGsem runs at most  of these
rounds. If uncompleted jobs remain after the th round,
one of two things is done.  If , \ALGsem runs each job one at a time
on all machines, until all jobs are completed. If , \ALGsem
simply repeats the schedule  given
by the th round until all jobs complete.

The following theorem states that \ALGsem achieves an -approximation.  The key aspect of our analysis is viewing
\ALGsem as an ``online algorithm'' to solve the \suureform problem
over the hidden input .
\punt{
Essentially, \ALGsem ``discovers'' the values of  as jobs
complete.
}
  We compare the length of \ALGsem's schedule against that of
an optimal offline algorithm, called OFF, that knows the values
.  In particular, we show that if OFF takes total time 
on input , then each round of \ALGsem takes time  on
the same input.  This part of our proof is essentially a competitive
analysis~\cite{SleatorTa85}.


\setthmcount{indepm}
\newcommand{\indepmthm}{
\begin{theorem} \thmlabel{indepalg}
  Let  and let
   denote the random variable corresponding to
  the amount of time it takes for an execution of \ALGsem to complete
  all jobs. Then . 
\end{theorem}
}
\indepmthm
\begin{proof}
  First, we show that for any fixed set of random values ,
  each round of \ALGsem takes time proportional to the optimal
  strategy~OFF.  Then, we show that the expected time to complete 
  the first  rounds is .  Finally, we analyze the
  cases when \ALGsem does not complete in  rounds.
  
  Consider an optimal offline strategy OFF that knows the random
  values of , and let  denote OFF's
  makespan given the values  (i.e.,  is
  the minimum over all strategies for a fixed ).
  For each , let  be the
  subset of jobs such that .  Thus, OFF must
  assign machines to job~ such that
  . Thus,  we have
  .
  

  Now consider an execution of \ALGsem for the same .  We
  note that if a job  remains uncompleted at the start of the th
  round, for , then .
  This inequality follows from the fact that in the th round,
  we give every job a log mass that exceeds .  Hence,
  the jobs executed in the th round are a subset of those
  defined by  above.  By \lemref{lp1rounding}, the th
  round takes time .  Observing that
  ,
  we conclude that \ALGsem's th round takes time
  .
  
  We thus have that for any particular , if \ALGsem
  completes in  rounds, then it takes total time
  .  The  term
  comes from the time it takes to execute the first round (as
  in \lemref{lplower}).  Since , where  is
  the time taken by OFF on a randomly selected , we
  conclude that the expected time for \ALGsem's first 
  rounds is .
  
  We now consider the case when \ALGsem has not completed after 
  rounds.  Let  be Bernoulli r.v. that is 1 if all jobs have
  completed after the th round and 0 otherwise. Clearly,
  .  To
  complete the proof, we must show that .  We consider the remainder of
  the proof in two cases, depending on whether  or .
  The case of  appears in \appref{app}.
  
  Suppose  and .  Recall that after the th
  round, \ALGsem runs jobs one after the other.  Recall also that
  remaining jobs must have .  This event occurs
  when , which, by the union bound, happens with
  probability no more than .  Running jobs one at a time is trivially
   an -approximation, so we conclude
  that .
\end{proof}  

\newcommand{\indepmproof}{
\begin{proof}
  This proof resumes where the same proof from \secref{indep} leaves off.
  In particular, we show here that \ALGsem produces an -approximation by showing that repeating the rd round until completion takes time  .
  
  Suppose that  and .  Let  be the schedule computed at the
  end of the th phase. Here, \ALGsem repeats  until all
  jobs complete. Define the load  of a finite schedule to be the
  maximum number of timesteps during which any machine is assigned to
  an uncompleted job (i.e., ); a schedule
  can be compressed to run in exactly  time-steps.  We will analyze
  how long it takes for the load of our instance to drop from its
  initial expected value  (at the end of the th
  phase) to 0 (when all jobs have completed).


  Define  as a random variable denoting the number of timesteps
  until the compressed load of  drops to 0, and let  be
  the (random variable) denoting the length of .  Then , where  is the random variable representing the
  number of repetitions of  necessary to drop its load from
   to .  By construction, a single execution
  of  ensures that each job remains with probability at most
  , and thus the expected load of each machine shrinks by at
  least a (multiplicative) factor of .  Markov's inequality gives
  us that each machines load decreases by a a factor of . with
  probability at least .  Taking a union bound over all machines
  gives us that the load of \emph{all} machines decreases by a factor
  of . with probability at least~.
  
  For , we now have that each repetition of  decreases
  the remaining load by a factor of 2 with probability at least ,
  and hence  for all~---requiring only an
  expected constant number of repetitions to reduce the load by a
  constant factor.  We now have that .
  Since each  and  are independent, it follows that
  .  Having 
  completes the proof.
\end{proof}
}
