\secput{indep}{Independent Jobs}

This section describes an $O(\log\log{n})$-approximation algorithm for
\suui, the \suu problem with independent jobs.  We first give an
oblivious $O(\log{n})$-approximation algorithm for \suui, based on
scheduling an (approximation of an) integer linear program.  We then
modify this algorithm into a semioblivious solution consisting of
$O(\log\log n)$ nearly optimal phases.  

\subheading{An oblivious $O(\log{n})$-approximation}

We now describe an $O(\log{n})$-approximation for \suui, called
\ALGobl. 
\punt{which represents an improvement over the oblivious
$O(\log{n}\log(\min\set{m,n}))$-approximation given in~\cite{LinRa07}.}
Our approach constructs a schedule of length
$O(\expect{\Topt})$, based on an integer linear program, such that each job
has no more than a constant probability of failure upon completion.
This finite oblivious schedule is repeated until all jobs have completed. 
Using Chernoff bounds, we conclude that the expected number of repetitions is 
$O(\log n)$,  yielding an $O(\log n)$-approximation.

We use the following integer linear program for \ALGobl.  Let $x_{ij}$
denote the number of steps during which machine $i$ is assigned to
job~$j$.  Recall $\ell_{ij} = -\log q_{ij}$ is the log failure of
job~$j$ on machine~$i$.  Let $L$ be a fixed positive real,
representing a target log mass for each job, and let $J' \subseteq J$
be a subset of jobs that need to achieve that log mass.  For now,
think of $L$ as being fixed at $L=1/2$, and $J'=J$.  We assign
different values to $L$ later for the semioblivious $O(\log\log
n)$-approximation.
\begin{eqnarray}
  \lplabel{lp1}\hspace{1cm} \min t \nonumber \\ 
  \mbox{s.t. $\sum_{i\in M} \ell_{ij}x_{ij}$} & \geq & L \;\;\; \forall j \in J' \label{eq:lp1aggfailure}\\
  \mbox{$\sum_{j\in J} x_{ij}$} & \leq & t \;\;\; \forall i \in M
  \label{eq:lp1load} \\
  x_{ij} & \in & \naturals\cup\set{0} \;\;\; \forall i\in M, j\in J \label{eq:lp1int}\ .
\end{eqnarray}
Here \eqref{lp1aggfailure} enforces that every
job in $J$ has a failure probability no greater than
$1/\sqrt{2}$, and 
\punt{
\footnote{Any positive constant works here.  A constant
  in the constraint strictly less than $1$ (corresponding to a failure
  probability strictly greater than $1/2$) simplifies the proof of
  \lemref{lplower}.}
  }
\eqref{lp1int} guarantees that all jobs
are scheduled for an integral number of steps on each machine.
We use \lpref{lp1} to refer to this integer linear program
generically, and $\lpfunc{lp1}(J',L)$ to refer to it with particular 
values of $J'$ and $L$.  We denote the optimal value for $\lpfunc{lp1}(J',L)$
by $t_{\lpfunc{lp1}(J',L)}$.



A solution for $\lpfunc{lp1}(J',L)$ naturally generalizes to a finite
oblivious schedule, denoted by $\Sigma_{\lpfunc{lp1}(J',L)}$, with
length $t_{\lpfunc{lp1}(J',L)}$ as follows.  Consider a machine~$i$,
and consider each job~$j$ in arbitrary order.  Assign machine~$i$ to
job~$j$ for $x_{ij}$ timesteps. To finish our description of this schedule, 
we first claim that $t_{\lpfunc{lp1}(J,1/2)}$ approximates $\expect{\Topt}$ 
(the proof appears in \appref{app}).
Then we show how to approximate \lpref{lp1} in polynomial time.

\setthmcount{lplower}
\newcommand{\lplowerthm}{
\begin{lemma}\lemlabel{lplower}
  $t_{\lpfunc{lp1}(J,1/2)} = O(\expect{\Topt})$
\end{lemma}}
\lplowerthm
\newcommand{\lplowerproof}{
\begin{proof}
Let $t$ be the optimum solution to $\lpfunc{lp1}(J,1/2)$.  Consider
  any subset $U \subseteq J$, and its complement $\overline U$.  Then
  $\lpfunc{lp1}(U,1/2) + \lpfunc{lp1}(\overline U,1/2) \ge t$,
  since we can construct a solution to $\lpfunc{lp1}(J,1/2)$ by adding
  a solution to $\lpfunc{lp1}(U,1/2)$ and a solution to
  $\lpfunc{lp1}(\overline U,1/2)$.

  Now recall our view of the problem in terms of \suureform: there is
  an $r_j$ chosen uniformly at random from $(0,1)$ for each job $j$
  such that job $j$ completes only if $\sum_{i \in
  M} \ell_{ij}x_{ij} \geq -\log r_j$.  For any sample from the event
  space, let $U$ be the set of jobs $j$ for which $r_j>1/2$, and let
  $\overline U$ be the complement set of jobs $j$ for which $r_j <
  1/2$ (note $r_j=1/2$ with probability 0 so can be ignored).  By
  definition, each job is in $U$ independently with probability $1/2$.
  Next observe that however OPT generates its schedule, it must
  allocate at least $1/2$ unit of ``work'' to each job in $U$; in
  other words, \eqref{lp1aggfailure} of \lpref{lp1} must hold for
  every $j\in U$.  Thus, the optimum schedule contains a feasible
  solution to $\lpfunc{lp1}(U,1/2)$.

  Now observe that by construction, $U$ is a uniformly random subset
  of $J$, meaning all subsets are equally likely.  Thus, 
\begin{eqnarray*}
  E[T_{\rm OPT}] &= &2^{-n} \sum_U E[T_{\rm OPT} \mid U]\\
  &= &2^{-n}\cdot \frac12 ( \sum_U E[T_{\rm OPT} \mid U] + \sum_U
  E[T_{\rm OPT} \mid \overline U])\\
  & \ge &2^{-n}\frac12 \sum_U (\lpfunc{lp1}(U,1/2) + \lpfunc{lp1}(\overline
  U,1/2))\\
  & \ge & 2^{-n}\frac12 \sum_U \lpfunc{lp1}(J,1/2)\\
  &= &\frac12 \lpfunc{lp1}(J,1/2)
\end{eqnarray*}
Where the second line of this derivation follows from the first
paragraph of this proof.
\punt{
Viewing the problem in terms of \suureform, there is an $r_j$ chosen
  uniformly at random from $(0,1)$ for each job $j$ such that job $j$
  completes only if $\sum_{i \in M} \ell_{ij}x_{ij} \geq -\log r_j$.  

  To prove the claim, we first show that with constant probability, at
  least half the jobs require a constant log mass.  We then show that
  with constant probability, a random selection of $n/2$ jobs yields
  an \lpref{lp1} instance with optimal solution
  $\Omega(T_{\lpfunc{lp1}(J,1/2)})$.  We combine these two steps to
  complete the proof.
  
  Let $C_j$ be a Bernoulli random variable with value $1$ if $1/2 \geq
  -\log r_j$, or equivalently $r_j \geq 1/\sqrt{2}$.  Said
  differently, $C_j=1$ when an execution of $\lpfunc{lp1}(J,1/2)$
  would complete job~$j$.  When $C_j=0$, any execution (and in
  particular an execution of $\Sigma_{\rm OPT}$) must give job $j$ a
  log mass exceeding $1/2$.  Our goal is thus to show that a large
  fraction of jobs have $C_j=0$.  Let $C = \sum_j C_j$.  Then $\expect{C} =
  n(1-1/\sqrt{2})$.  Applying Markov's inequality gives that $\prob{C
    \geq n/2} \leq n(1-1/\sqrt{2})/(n/2) = 2-\sqrt{2} < 1$.  Thus,
  with constant probability at least $c=1-(2-\sqrt{2})$, any schedule
  (and in particular $\Sigma_{\rm OPT}$) must assign machines to jobs
  such that at least half of them have log mass exceeding~$1/2$ (and
  hence satisfy \eqref{lp1aggfailure}).
  
  Let $t^* = t_{\lpfunc{lp1}(J,1/2)}$, and consider any uniformly
  random selection $J_r\subset J$ of $n/2$ jobs.\footnote{For
    simplicity here, without loss of generality, we assume $n$ is a
    multiple of $2$.}  Then either $t_{\lpfunc{lp1}(J_r,1/2)} > t^*/2$
  or $t_{\lpfunc{lp1}(J-J_r,1/2)} > t^*/2$, since solving the linear
  program for $J_r$ then $J-J_r$ solves it for $J$.  We observe that
  since $\card{J_r} = \card{J-J_r} = n/2$, both $J_r$ and $J-J_r$ are
  equalLy likely to be selected as the random subset.  Thus, with
  probability at least $1/2$, we have $t_{\lpfunc{lp1}(J_r,1/2)} > t^*/2$
  
  We thus have that with probability at least $c/2$, $\Sigma_{\rm
    OPT}$ must schedule the jobs such that at least half of them
  satisfy \eqref{lp1aggfailure}, and such that \lpref{lp1} applied to
  those jobs has length at least $t^*/2$.  We therefore conclude that
  $\expect{\Topt} \geq (c/2) t^*/2$, and hence $t_{\lpref{lp1}}(J,1/2) =
  O(\expect{\Topt})$.
}
\end{proof}
}

The following lemma states that, in polynomial time, we can find an integral
assignment that approximates \lpref{lp1} to within a constant factor.
Some aspects of the proof are similar to~\cite[Theorem 4.1]{LinRa07},
\punt{which solves a slightly different problem useful for precedence
constraints that form disjoint chains,} but we add several steps \punt{(made
possible by our formulation involving log failure)} that improve the
approximation ratio.  
\punt{
Our tighter approximation \emph{does} apply to
the more general disjoint-chains variant, which we revisit in
\secref{constr}.
}

\begin{lemma}\lemlabel{lp1rounding}
There exists a polynomial-time algorithm that computes a feasible
  solution to $\lpfunc{lp1}(J',L)$ having value
  $O(t_{\lpfunc{lp1}(J',L)})$.
\end{lemma}
\begin{proof}
  We relax our integer linear program to a linear program, and then
  show that the relaxed LP can be rounded to yield an integral
  $\set{\widehat{x_{ij}}}$ solution with value
  $O(T_{\lpfunc{lp1}(J',L)})$.
  
  First, let $\ell'_{ij} = \min\set{\ell_{ij},L}$.  Then we replace
  each $\ell_{ij}$ in \eqref{lp1aggfailure} with $\ell'_{ij}$,
  yielding the constraint $\sum_{i\in M} \ell'_{ij} x_{ij} \geq L,
  \forall j\in J'$.  Note that since assignments are restricted to be
  integral, this change has no effect on either the feasibility or the
  value of an assignment.  Next we remove \eqref{lp1int} and solve the
  relaxed linear program.\punt{ \footnote{As far as the relaxed
      linear program is concerned, the change from $\ell_{ij}$ to
      $\ell'_{ij}$ may have a big effect on the feasibility of
      solutions.  In particular, suppose there exists a machine~$i$
      such that $\ell_{ij}=Ln$ for all $j$.  Then an LP based on
      $\ell_{ij}$ may assign all jobs to machine~$i$ for $1/n$ time
      steps.}  }  Letting $\set{x^*_{ij}, t^*}$ be an optimal
  solution, we note that $t^* \leq t_{\lpfunc{lp1}(J',L)}$, because
  integral solutions are feasible.
  
  Our goal now is to round the LP solution to an integral solution,
  while not increasing its value by very much.  We 
  proceed in three steps.  First, we group machines having
  similar $\ell'_{ij}$ for a job~$j$, yielding a single assignment for the
  whole group.  Then, we round those assignments to integers.
  Finally, we show that the rounded assignments satisfy
   \lpref{lp1}, using an integral flow network.
  
   For each job $j$, we group machines having $\ell'_{ij}$ values
   within a factor of 2, and determine the total assignment to that
   group.  More formally, for each $j$ and integer $k$, we let
   $D^*_{jk} = \sum_{i:\floor{\log \ell'_{ij}} = k}
   x^*_{ij}$\vspace{-.2em} be the total assignment of machines with
   $\ell'_{ij} \in [2^k,2^{k+1})$ to job~$j$.\punt{ \footnote{Note
       that there are at most $mn$ values of $k$ for which $D^*_{jk}
       \neq 0$.}  }  It should be clear that $\sum_{i \in M}
   \ell'_{ij} x^*_{ij} \geq \sum_k D^*_{jk} 2^k \geq L/2$, for all
   $j\in J'$.
  
  We next round the value of $D^*_{jk}$ up to $\floor{6D^*_{jk}}$.  We
  claim that $\sum_k \floor{6D^*_{jk}} 2^k \geq L, \forall j\in J'$.
  Note that since $\ell'_{ij} \not> L$, the maximum value of $k$
  having nonzero $D^*_{jk}$ is $\floor{\log L}$.  Thus, the claim
  follows because $\sum_k \floor{6D^*_{jk}} 2^k \geq 3 (2\sum_k
  D^*_{jk} 2^k) - (\sum_{k\leq \log{L}} 2^k) \geq 3 (L)
  - (\sum_{k=0}^{\infty} L / 2^k) \geq 3L - 2L = L$.  In other words,
  rounding the group assignments down to integers can only cause us to
  lose a log mass of at most $2L$.  We thus need an assignment giving
  $3L$ log mass to the job.  
  \punt{The constant $6$ appears because we lose
  a factor of $2$ in our groupings.}
  
  To complete the integral assignment, we construct a network-flow
  instance as follows.  For each job~$j$ and integer $k$, we have a
  node $u_{jk}$.  For each machine~$i$, we have a node $v_i$.  We also
  add a source-node~$s$ and a sink-node~$w$.  For each $u_{jk}$, we
  add a directed edge $(s,u_{jk})$ with capacity $\floor{6D^*_{jk}}$.
  For each $v_i$, we add a directed edge $(v_i,w)$ with capacity
  $\ceil{6t^*}$.  Finally, we add a directed edge $(u_{jk},v_i)$ with
  infinite capacity, for any $j,k,i$ such that $\floor{\log
    \ell'_{ij}} = k$.  Note that for a given~$j$ and~$i$, there is
  exactly one $k$ such that $(u_{jk},v_i)$ exists.  We refer to this
  edge as edge $(j,i)$.
   
   Note that if we make the capacity of edges $(s,u_{jk})$ be
  $6D^*_{jk}$ instead, then a flow of
  demand $\sum_{jk} 6D^*_{jk}$ exists in this network.
  \punt{
  , and it can be constructed by
  setting the flow along edge $(j,i)$ to be $6x^*_{ij}$ (and flow
  along edge $(v_i,w)$ to be $6\sum_{j\in J} x^*_{ij}$).
  }  
  Thus, a flow of capacity $\sum_{jk} \floor{6D^*_{jk}}$ exists 
  when we lower the capacity of edge $(s,u_{jk})$ to $\floor{6D^*_{jk}}$.
  
  Ford-Fulkerson's theorem~\cite{FordFu62,CormenLeRi01} states that an
  integral max flow exists whenever the capacities are integral, as
  they are here.  We therefore take the flow across the edges $(j,i)$
  as our integral assignments $\widehat{x_{ij}}$.  Moreover, by
  construction, $\widehat{x_{ij}}$ satisfy $\sum_{j\in J}
  \widehat{x_{ij}} \leq \ceil{6t^*}$ for all $i\in M$, and $\sum_{i
    \in M} \ell'_{ij}\widehat{x_{ij}} \geq \sum_k \floor{6D^*_{jk}}
  2^k \geq L$ for all $j \in J$.  We thus have an integral feasible
  solution $\set{\widehat{x_{ij}},6t^*}$.  Noting that $6t^* \leq
  6T_{\lpfunc{lp1}(J',L)}$ completes the proof.
\end{proof}

Recall that \lemref{lplower} shows that $t_{\lpfunc{lp1}(J,1/2)} =
O(\expect{\Topt})$.  Then \lemreftwo{lplower}{lp1rounding} in concert state
that in polynomial time, we can find a schedule $\Sigma$ of length
$O(\expect{ \Topt })$, such that every job has at most a constant
probability of failure. Repeating $\Sigma$ until all jobs complete
gives our oblivious schedule \ALGobl.  The proof appears in \appref{app}.
\setthmcount{indbound}
\newcommand{\indboundthm}{
\begin{theorem}\thmlabel{indbound}
  Let $T_{\ALGobl}$ denote the random variable corresponding to the
  amount of time it takes for an execution of \ALGobl to complete all
  jobs.  Then $\expect{T_{\ALGobl}} = O(\expect{\Topt} \log n)$.
\end{theorem}
}
\indboundthm
\newcommand{\indboundproof}{
\begin{proof}
  From \lemreftwo{lplower}{lp1rounding}, we have a schedule $\Sigma$
  of length $O(\expect{ \Topt } )$ that gives each job a constant
  probability of success.  Applying a Chernoff bound gives us that a
  particular job completes in $O(\log n)$ repetitions of $\Sigma$,
  with probability at least $1-1/n^{\Theta(1)}$, where the constant
  exponent appears as a constant factor in the number of repetitions.
  Taking a union bound over all jobs gives that with probability at
  least $1-1/n^{\Theta(1)}$, \emph{all} jobs complete in $O(\log n)$
  repetitions.  Since this probability drops off dramatically as the
  number of repetitions increases, we have $\expect{T_{\ALGobl}} =
  O(\expect{\Topt}\log n)$.
\end{proof}
}\vspace{-.5em}
\subheading{A semioblivious $O(\log\log(\min\set{m,n}))$-approximation}

We construct our semioblivious schedule \ALGsem as follows.  The
schedule is divided into ``rounds.''  The first round corresponds to
an execution of the schedule suggested by the (rounded) solution
to $\lpfunc{lp1}(J,1/2)$.  In each following round, \lpref{lp1} is
applied to all remaining jobs with doubling targets.  If  
$J_k \subseteq J_{k-1}\subseteq J$ are the set of jobs left at
the start of round $k$,  then for that round we find an approximate solution 
to $\lpfunc{lp1}(J_k,2^{k-2})$, and schedule obliviously according to it.

\ALGsem runs at most $K = \ceil{\log\log \min \set{m,n}}+3$ of these
rounds. If uncompleted jobs remain after the $K$th round,
one of two things is done.  If $n \leq m$, \ALGsem runs each job one at a time
on all machines, until all jobs are completed. If $m < n$, \ALGsem
simply repeats the schedule $\Sigma_{\lpfunc{lp1}(J_k,2^{k-2})}$ given
by the $K$th round until all jobs complete.

The following theorem states that \ALGsem achieves an $O(\log\log(\min
\set{m,n}))$-approximation.  The key aspect of our analysis is viewing
\ALGsem as an ``online algorithm'' to solve the \suureform problem
over the hidden input $\set{r_j}$.
\punt{
Essentially, \ALGsem ``discovers'' the values of $r_j$ as jobs
complete.
}
  We compare the length of \ALGsem's schedule against that of
an optimal offline algorithm, called OFF, that knows the values
$\set{r_j}$.  In particular, we show that if OFF takes total time $t$
on input $\set{r_j}$, then each round of \ALGsem takes time $O(t)$ on
the same input.  This part of our proof is essentially a competitive
analysis~\cite{SleatorTa85}.


\setthmcount{indepm}
\newcommand{\indepmthm}{
\begin{theorem} \thmlabel{indepalg}
  Let $K = \ceil{\log\log \min \set{m,n}}+3$ and let
  $T_{\ALGsem}$ denote the random variable corresponding to
  the amount of time it takes for an execution of \ALGsem to complete
  all jobs. Then $\expect{T_{\ALGsem}} = O(\expect{\Topt}\cdot K)$. 
\end{theorem}
}
\indepmthm
\begin{proof}
  First, we show that for any fixed set of random values $\set{r_j}$,
  each round of \ALGsem takes time proportional to the optimal
  strategy~OFF.  Then, we show that the expected time to complete 
  the first $K$ rounds is $O(\expect{\Topt} \cdot K)$.  Finally, we analyze the
  cases when \ALGsem does not complete in $K$ rounds.
  
  Consider an optimal offline strategy OFF that knows the random
  values of $\set{r_j}$, and let $\Toff(\set{r_j})$ denote OFF's
  makespan given the values $\set{r_j}$ (i.e., $\Toff(\set{r_j})$ is
  the minimum over all strategies for a fixed $\set{r_j}$).
  For each $k\in\set{2,3,\ldots,K}$, let $J_k \subseteq J$ be the
  subset of jobs such that $-\log r_j > 2^{k-3}$.  Thus, OFF must
  assign machines to job~$j\in J_k$ such that
  $\sum_{i\in M} x_{ij} \ell_{ij} \geq 2^{k-3}$. Thus,  we have
  $\Toff(\set{r_j}) \geq T_{\lpfunc{lp1}(J_k,2^{k-3})}$.
  

  Now consider an execution of \ALGsem for the same $\set{r_j}$.  We
  note that if a job $j$ remains uncompleted at the start of the $k$th
  round, for $k\in\set{2,3,\ldots K}$, then $-\log r_j > 2^{k-3}$.
  This inequality follows from the fact that in the $(k-1)$th round,
  we give every job a log mass that exceeds $2^{k-3}$.  Hence,
  the jobs executed in the $k$th round are a subset of those
  defined by $J_k$ above.  By \lemref{lp1rounding}, the $k$th
  round takes time $O(T_{\lpfunc{lp1}(J_k,2^{k-2})})$.  Observing that
  $T_{\lpfunc{lp1}(J_k,2^{k-2})} \leq 2T_{\lpfunc{lp1}(J_k,2^{k-3})}$,
  we conclude that \ALGsem's $k$th round takes time
  $O(\Toff(\set{r_j}))$.
  
  We thus have that for any particular $\set{r_j}$, if \ALGsem
  completes in $d+1\leq K$ rounds, then it takes total time
  $O(\expect{\Topt} + d\Toff(\set{r_j}))$.  The $\expect{\Topt}$ term
  comes from the time it takes to execute the first round (as
  in \lemref{lplower}).  Since $\Topt \geq \expect{\Toff}$, where $\Toff$ is
  the time taken by OFF on a randomly selected $\set{r_j}$, we
  conclude that the expected time for \ALGsem's first $d+1 \leq K$
  rounds is $O(\expect{\Topt}\cdot d) = O(\expect{\Topt}\cdot K)$.
  
  We now consider the case when \ALGsem has not completed after $K$
  rounds.  Let $F_K$ be Bernoulli r.v. that is 1 if all jobs have
  completed after the $K$th round and 0 otherwise. Clearly,
  $\expect{T_{\ALGsem} | F_K = 1} = O(\expect{\Topt} \cdot K)$.  To
  complete the proof, we must show that $\prob{F_K = 0} \expect{T_{\ALGsem}
  | F_K = 0 } \leq O(\expect{\Topt} \cdot K)$.  We consider the remainder of
  the proof in two cases, depending on whether $n \leq m$ or $m < n$.
  The case of $m<n$ appears in \appref{app}.
  
  Suppose $n \leq m$ and $F_K = 0$.  Recall that after the $K$th
  round, \ALGsem runs jobs one after the other.  Recall also that
  remaining jobs must have $- \log r_j \geq 2^{K-2} \geq 2^{\log\log n +
  1} \geq 2\log n$.  This event occurs
  when $r_j < 1/n^2$, which, by the union bound, happens with
  probability no more than $1/n$.  Running jobs one at a time is trivially
   an $O(n)$-approximation, so we conclude
  that $\prob{F_K = 0}\expect{T_{\ALGsem} | F_K = 0 \land n \leq m}
  \leq O(\expect{\Topt} K)$.
\end{proof}  

\newcommand{\indepmproof}{
\begin{proof}
  This proof resumes where the same proof from \secref{indep} leaves off.
  In particular, we show here that \ALGsem produces an $O(\log\log
  m)$-approximation by showing that repeating the $(\ceil{\log\log m}
  + 3)$rd round until completion takes time $O(\expect{\Topt})$ .
  
  Suppose that $m < n$ and $F_K = 0$.  Let $\Sigma_K =
  \Sigma_{\lpfunc{lp1}(J_K,2^{k-2})}$ be the schedule computed at the
  end of the $K$th phase. Here, \ALGsem repeats $\Sigma_K$ until all
  jobs complete. Define the load $H$ of a finite schedule to be the
  maximum number of timesteps during which any machine is assigned to
  an uncompleted job (i.e., $H = \max_i \sum_j x_{ij}$); a schedule
  can be compressed to run in exactly $H$ time-steps.  We will analyze
  how long it takes for the load of our instance to drop from its
  initial expected value $O(\expect{\Topt})$ (at the end of the $K$th
  phase) to 0 (when all jobs have completed).


  Define $X$ as a random variable denoting the number of timesteps
  until the compressed load of $\Sigma_K$ drops to 0, and let $T_K$ be
  the (random variable) denoting the length of $\Sigma_K$.  Then $X =
  \sum_{i = 0}^{\log T_K} X_i T_K/2^{i} = T_K \sum_{i=0}^{\log T_K}
  X_i / 2^{i}$, where $X_i$ is the random variable representing the
  number of repetitions of $\Sigma_K$ necessary to drop its load from
  $T_K/2^{i}$ to $T_K/2^{i+1}$.  By construction, a single execution
  of $\Sigma_K$ ensures that each job remains with probability at most
  $1/m^2$, and thus the expected load of each machine shrinks by at
  least a (multiplicative) factor of $m^2$.  Markov's inequality gives
  us that each machines load decreases by a a factor of $m/2$. with
  probability at least $1/2m$.  Taking a union bound over all machines
  gives us that the load of \emph{all} machines decreases by a factor
  of $m/2$. with probability at least~$1/2$.
  
  For $m >4$, we now have that each repetition of $\Sigma_K$ decreases
  the remaining load by a factor of 2 with probability at least $1/2$,
  and hence $\expect{X_i} \leq 2$ for all~$i$---requiring only an
  expected constant number of repetitions to reduce the load by a
  constant factor.  We now have that $E\left[\sum_{i=0}^{\log T_K} X_i
    / 2^i\right] \leq \sum_{i=0}^{\infty} \expect{X_i}/2^i \leq O(1)$.
  Since each $X_i$ and $T_K$ are independent, it follows that
  $\expect{X} \leq \expect{T_K} \sum_{i=0}^{\infty} \expect{X_i}/2^i =
  O(\expect{T_K})$.  Having $\expect{T_K} \leq O(\expect{\Topt})$
  completes the proof.
\end{proof}
}
