\section{Experiment}
In this section, we perform the evaluation experiments for our approach on the KBQA task. 

\subsection{Datasets}

We adopt three benchmark datasets for the multi-hop KBQA task:


\textbf{MetaQA}~\citep{MetaQA-AAAI-2018} contains more than 400k single and multi-hop (up to 3-hop) questions in the domain of movie, containing three 
datasets, namely MetaQA-1hop, MetaQA-2hop and MetaQA-3hop.





\ignore{
\paratitle{MetaQA}~\citep{MetaQA-AAAI-2018} contains more than 400k single and multi-hop (up to 3-hop) questions in the domain of movies. The questions were constructed using the knowledge base provided with the WikiMovies~\citep{KVMem-EMNLP-2016} dataset. We use the “vanilla” version of the queries\footnote{For this version, the 1-hop questions in MetaQA are exactly the same as WikiMovies.}. We use the KB supplied with the WikiMovies dataset, and use exact match on surface forms to perform entity linking. The KB used here is relatively small, with about 43k entities and 135k triples. MetaQA datasets contain 3 datasets, namely MetaQA-1hop, MetaQA-2hop and MetaQA-3hop.
}

\textbf{WebQuestionsSP (webqsp)}~\citep{webqsp-ACL-2015} contains 4737 natural language questions that are answerable using Freebase as the knowledge base. The questions require up to 2-hop reasoning from knowledge base. We use the same train/dev/test splits as GraftNet~\citep{GraftNet-EMNLP-2018}.


\textbf{Complex WebQuestions 1.1 (CWQ)}~\citep{CWQ-NAACL-2018} is generated from WebQuestionsSP by extending the question entities or adding constraints to answers. There are four types of question: composition (45$\%$), conjunction (45$\%$), comparative (5$\%$), and superlative (5$\%$). The questions require up to 4-hops of reasoning on the KB. \ignore{
\begin{table}[!h]
	\centering
	\caption{Statistics of all datasets. $\#$entity denotes average number of entities in subgraph, and coverage denotes the ratio of at least one answer in subgraph.}
	\label{tab:datasets}\begin{footnotesize}
	\begin{tabular}{l | r r r | r r r}
\hline
			Datasets		&	Train&	Dev&	Test&	$\#$entity&	recall& coverage\\
		\hline
		MetaQA-1hop	&	96,106&	9,992&	 9,947&	487.6&	99.9$\%$&	100$\%$\\
		MetaQA-2hop	&	118,980&	14,872&	 14,872&	469.8&	99.9$\%$&	100$\%$\\
		MetaQA-3hop	&	114,196&	14,274&	 14,274&	497.9&	83.1$\%$&	99.0$\%$\\
		webqsp	&	 2,848&	 250&	  1,639&	1429.8&	93.5$\%$&	94.9$\%$\\
		CWQ	&	27,639&	3,519&	  3,531&	1305.8&	77.5$\%$&	79.3$\%$\\
		\hline
	\end{tabular}\end{footnotesize}
\end{table}}
\begin{table}[!h]
	\centering
	\caption{Statistics of all datasets. ``$\#$entity'' denotes average number of entities in subgraph, and ``coverage'' denotes the ratio of at least one answer in subgraph.}
	\label{tab:datasets}\begin{tabular}{l | r r r | r r}
\hline
			Datasets		&	Train&	Dev&	Test&	$\#$entity& coverage\\
		\hline
		MetaQA-1hop	&	96,106&	9,992&	 9,947&	487.6&	100$\%$\\
		MetaQA-2hop	&	118,980&	14,872&	 14,872&	469.8&	100$\%$\\
		MetaQA-3hop	&	114,196&	14,274&	 14,274&	497.9&	99.0$\%$\\
		webqsp	&	 2,848&	 250&	  1,639&	1,429.8&	94.9$\%$\\
		CWQ	&	27,639&	3,519&	  3,531&	1,305.8&	79.3$\%$\\
		\hline
	\end{tabular}\end{table}Following~\citep{GraftNet-EMNLP-2018,PullNet-EMNLP-2019}, we use the topic entities labeled in original datasets and adopt PageRank-Nibble algorithm~(PRN) \citep{PPR-Andersen-2006} to find KB entities close to them.
With these entities, we can obtain a relatively small subgraph that is likely to contain the answer entity.
For CWQ and webqsp datasets, we first obtain the neighborhood graph within two hops of topic entities and then run PRN algorithm on it.  We further expand one hop for CVT entities in Freebase to obtain the neighborhood subgraph. As shown in  Table~\ref{tab:datasets},  2-hop graphs are sufficient to cover most of the answer entities.  While on MetaQA datasets, we run PRN algorithm on the entire KB. 
Specifically, we use the PRN algorithm~\citep{PPR-Andersen-2006} with $\epsilon=1e^{-6}$ and then select the $m$ top-scoring entities. We set $m = 500$ for the smaller MetaQA KB and $m = 2000$ for larger Freebase. 
For the reserved triples, both their head and tail entities are obtained from the top $m$ entities identified by PRN algorithm.
We summarize the statistics of the three datasets in Table~\ref{tab:datasets}.
\ignore{
Following~\citep{GraftNet-EMNLP-2018,PullNet-EMNLP-2019}, we used the topic entities labeled in original datasets and adopted PageRank-Nibble algorithm~(PRN) \citep{PPR-Andersen-2006} to find KB entities close to them, with which a relative small subgraph that is likely to contain the answer entity can be extracted from large scale KB.
For CWQ and webqsp datasets, we first obtain the neighborhood graph within two hops of topic entities and then run PRN algorithm on it.  We further expand one hop for CVT entities in Freebase to obtain the neighborhood subgraph. As shown in  Table~\ref{tab:datasets},  2-hop graphs are sufficient to cover most of the answer entities.  While on MetaQA datasets, we run PRN algorithm on the entire KB. 
Specifically, we used the PRN algorithm~\citep{PPR-Andersen-2006} with $\epsilon=1e^{-6}$ and then selected the $m$ top-scoring entities. We set $m = 500$ for the smaller MetaQA KB and $m = 2000$ for larger Freebase. 
For the reserved triples, both their head and tail entities are obtained from the top $m$ entities identified by PRN algorithm.
We summarize the statistics of the three datasets in Table~\ref{tab:datasets}.
}

\ignore{
It is not obvious how to perform KB retrieval: we would like to retrieve as many facts as possible to maximize the recall of
answers, but it is infeasible to take all facts that are within k-hops of question entities since the number grows exponentially. Based on prior work in the database community on sampling from graphs (Leskovec and Faloutsos, 2006) and local partitioning (Andersen et al., 2006a), we ran an approximate version of personalized PageRank (aka random walk with reset) to find the KB entities closest to the entities in the question—specifically, we used the PageRank-Nibble algorithm (Andersen et al., 2006b) with  and the picked the m top-scoring entities.7 We then eliminate all topm entities that are more than k-hops of the question entities, and finally, retrieve all facts from the KB connecting retrieved entities.
The results for several tasks are shown in Table 2, which gives the answer recall, and the average number of entities retrieved. For the smaller MetaQA KB with m = 500, the retrieval method finds high-coverage graphs when the KB is complete. For ComplexWebQuestions, even with m =2000, the recall is $64\%$—which is expected, since retrieving relevant entities for a multi-hop question from a KB with millions of entities is difficult.	
}

\subsection{Experimental Setting}


\subsubsection{Evaluation Protocol.} 
We follow~\cite{GraftNet-EMNLP-2018,PullNet-EMNLP-2019} to cast the multi-hop KBQA task as a ranking task for evaluation.
For each test question in a dataset, a list of answers are returned by a model according to their predictive probabilities. We adopt two evaluation metrics widely used in previous works, namely \emph{Hits@1} and \emph{F1}.  Specifically, Hits@1 refers to whether the top answer is correct.
For all the methods, we learn them using the training set, and optimize the parameters using the validation set and compare their performance on the test set.

\subsubsection{Methods to Compare.} We consider the following methods for performance comparison:


    $\bullet$ \textbf{KV-Mem}~\cite{KVMem-EMNLP-2016} maintains a memory table for retrieval, which stores KB facts encoded into key-value pairs.
	


	$\bullet$ \textbf{GraftNet}~\cite{GraftNet-EMNLP-2018} adopts a variant of graph convolution network to perform multi-hop reasoning on heterogeneous graph.
	
	$\bullet$ \textbf{PullNet}~\cite{PullNet-EMNLP-2019} utilizes the shortest path as supervision to train graph retrieval module and conduct multi-hop reasoning with GraftNet on the retrieved sub-graph.


	$\bullet$ \textbf{SRN}~\cite{SRN-WSDM-2020} is a multi-hop reasoning model under the RL setting, which solves multi-hop question answering through extending inference paths on knowledge base.


	$\bullet$ \textbf{EmbedKGQA}~\cite{Saxena-ACL-2020} conducts multi-hop reasoning through matching pretrained entity embedings with question embedding obtained from RoBERTa~\cite{RoBERTa-Liu-2019}.
	
$\bullet$ \textbf{NSM}, \textbf{NSM}$_{+p}$ and \textbf{NSM}$_{+h}$ are three variants of our model, which (1) do not use the teacher network, (2) use the teacher network with parallel reasoning, and (3) use the teacher network with hybrid reasoning, respectively.


\subsubsection{Implementation Details}
Before training the student network, we pre-train the teacher network on multi-hop KBQA task. To avoid overfitting, we adopt early-stopping by evaluating Hits@1 on the validation set every 5 epochs. We optimize all models with Adam optimizer, where the batch size is set to 40. The learning rate is tuned amongst \{0.01, 0.005, 0.001, 0.0005, 0.0001\}. The reasoning steps is set to $4$ for CWQ dataset, while $3$ for other datasets. 
The coefficient $\lambda$ (in Eq.~\ref{eq-loss-student}) and $\lambda_b, \lambda_c$(in Eq.~\ref{eq-loss-teahcer}) are tuned amongst \{0.01, 0.05, 0.1, 0.5, 1.0\}.

\subsection{Results}

The results of different methods for KBQA are presented in Table~\ref{tab:res}. It can be observed that:

(1) Among the baselines, KV-Mem performs the worst.
This is probably because it does not explicitly consider the complex 
reasoning steps.
Most methods perform very well on the MetaQA-1hop and MetaQA-2hop datasets, which require only up to 2 hops of reasoning.
On the other hand, the other datasets seem to be more difficult, especially the webqsp and CWQ datasets. 
Overall, EmbedKGQA and PullNet are better than the other baselines. 
PullNet trains an effective subgraph retrieval module based
on the shortest path between topic entities and answer entities. 
Such a module is specially useful to reduce the subgraph size and produce high-quality candidate entities.


\ignore{
 While on CWQ and MetaQA-3hop datasets, PullNet achieve better performance than other methods by a large margin. 
EmbedKGQA is the only baseline incorporate knowledge graph embedding to improve multi-hop KBQA, which show good performance on MetaQA datasets. As EmbedKGQA obtain multi-hop reasoning through simple matching question embedding with entity embeddings, it indicates the effectiveness of knowledge graph embedding to help KBQA. 
Overall, PullNet are the best baseline methods.
}


\ignore{
(2) EmbedKGQA obtain multi-hop reasoning through simple matching question embedding with entity embeddings, which show good performance on MetaQA datasets. It indicates the usefulness of knowledge graph embedding. 
However, EmbedKGQA achieve only compatible results on webqsp and MetaQA-2hop, and cannot perform better than the competitive baseline PullNet. 
}




\ignore{
PullNet train a subgraph retrieval module based on å paths reasoned between topic entities and answers. This retrieval module can construct a question-specific subgraph that contains information relevant to the question. With the help of such retrieval module, PullNet can be viewed as supervised by shorttest paths reasoned between topic entities and answers. As a result, it achieves much better performance compared with GraftNet.
}

(2) Our base model (\ie the single student network) NSM performs better than the competitive baselines in most cases. It is developed based on a graph neural network with two novel extensions for this task (Sec.~\ref{sec-nsm}).  
The gains of teacher-student framework show variance on different datasets. 
Specifically, on the two most difficult datasets, namely Webqsp and CWQ, the variants of NSM$_{+p}$ and NSM$_{+h}$ are substantially better than NSM and other baselines. 
These results have shown the effectiveness of the teacher network in our approach, which largely improves the student network. 
Different from SRN and PullNet, our approach designs a novel bidirectional reasoning mechanism to learn more reliable intermediate supervision signals.
Comparing NSM$_{+p}$ and NSM$_{+h}$, we find that their results are similar. On Webqsp and CWQ datasets, the hybrid reasoning is slightly better to improve the student network than parallel reasoning.

\begin{table}[htbp]
	\centering
	\caption{Performance comparison of different methods for KBQA (Hits@1 in percent). We copy the results for KV-Mem, GraftNet and PullNet from~\cite{PullNet-EMNLP-2019}, and copy the results for SRN and EmbedKGQA from \cite{SRN-WSDM-2020,Saxena-ACL-2020}. Bold and underline fonts denote the best and the second best methods.}
	\label{tab:res}\begin{small}
		\begin{tabular}{m{0.07\textwidth} c c c c c c}
\hline
Models& Webqsp& MetaQA-1& MetaQA-2 & MetaQA-3 & CWQ\\
			\cline{2-6}
\hline
			KV-Mem&	46.7& 96.2& 82.7& 48.9& 21.1\\
			GraftNet &	66.4& 97.0& 94.8& 77.7& 32.8\\
			PullNet &	68.1& 97.0& \textbf{99.9}& 91.4& 45.9\\
			SRN &	-& 97.0& 95.1& 75.2& -\\
			EmbedKGQA&	66.6& \textbf{97.5}& \underline{98.8}& \underline{94.8}& -\\
			\hline
			NSM	&	68.7& 97.1& \textbf{99.9}& \textbf{98.9}&	47.6\\
			\hline
			NSM$_{+p}$&	\underline{73.9}& \underline{97.3}& \textbf{99.9}& \textbf{98.9}& \underline{48.3}	\\
			NSM$_{+h}$&	\textbf{74.3}& 97.2& \textbf{99.9}& \textbf{98.9}& \textbf{48.8}\\
			\hline
		\end{tabular}\end{small}
\end{table}

\ignore{
(2) The base model (\ie the single student network) NSM performs better than the competitive baselines in most cases. It is developed based on a graph neural network with two novel extensions for this task (Sec.~\ref{sec-nsm}).  
Especially, on the two most difficult Webqsp and CWQ datasets, the variants of NSM$_{+p}$ and NSM$_{+h}$ are significantly better
than NSM and other baselines. These results have shown the effectiveness of the teacher network in our approach, which largely improves over the student network. 
Different from SRN and PullNet, our approach designs a novel bidirectional reasoning mechanism to learn more reliable intermediate supervision signals. 
It seems that the hybrid reasoning is more effective to improve the student network than parallel reasoning. 
\yscomment{I suggest to observe the results from the following aspects. (1) Our baseline NSM model could achieve competitive or better results compared with the strong baselines. The teacher-student framework can even boost our baseline model. (2) The gains of teacher-student framework show variance on different datasets. We benefit from them more on the difficult datasets (e.g., Webqsp and CWQ). (3) Comparing hybrid reasoning model and parallel reasoning model, we find that their performances are similar. On Webqsp and CWQ, the hybrid reasoning model slightly outperforms parallel reasoning model.}
\yscomment{People may ask a question: for MetaQA, why the gain of teacher-student framework does not increase with the hop number. We could explain this is because the MetaQA datasets are relatively simple, so the room of improvement is relatively small.}
}

\ignore{
 three variants of our model achieve the best performance in most cases, especially on the two most difficult Webqsp and CWQ datasets. 
The base model (\ie the single student network) NSM performs better than the competitive baselines.   
}

\ignore{
On MetaQA-1hop and MetaQA-2hop datasets, our approach achieve compatible performance compared with all baselines. On webqsp, CWQ and MetaQA-3hop datasets, it is clear to see that our approach is consistently better than these baselines by a large margin.
Different from the above baselines, we don't take the performance of final prediction as the only objective. 
Especially, we design an elaborative approach under teacher-student framework to labeling and utilizing intermediate entities for the multi-hop KBQA task.
We organize both forward and backward reasoning in the teacher, while the constraint of their correspondence is supposed to help us find the relevant entities reasoned in intermediate steps. The student obtain supervision during the intermediate reasoning steps according to the intermediate entity distribution labeled by teacher.
}

\ignore{
\begin{table*}[!h]
	\centering
	\caption{ Performance comparison of different methods for KBQA task. Bold and underline fonts are used to denote the best and second best performance in each metric respectively. The values reported are Hits@1.}
	\label{tab:res}\begin{tabular}{c | c c c c c| c c c}
\hline
Models& KV-Mem& GraftNet& PullNet& SRN& EmbedKGQA& NSM& NSM$_{+p}$& NSM$_{+h}$\\
\hline
			\hline
			Webqsp  & 46.7& 66.4& 68.1& -& 66.6& 68.7& \underline{73.9}& \textbf{74.3}\\
			MetaQA-1& 96.2& 97.0& 97.0& 97.0& \textbf{97.5}& 97.1& \underline{97.3}& 97.2\\
			MetaQA-2& 82.7& 94.8& \textbf{99.9}& 95.1& \underline{98.8}& \textbf{99.9}& \textbf{99.9}& \textbf{99.9}\\
			MetaQA-3& 48.9& 77.7& 91.4& 75.2& -& \underline{94.8}& \textbf{98.9}& \textbf{98.9}\\
			CWQ     & 21.1& 32.8& 45.9& -& -& 47.6& \underline{48.3}& \textbf{48.8}\\
			\hline
		\end{tabular}\end{table*}}

\subsection{Detailed Performance Analysis}
Table~\ref{tab:res} has shown that our approach overall has a better performance.
Next, we perform a series of detailed analysis experiments. For clarity, we only incorporate the results of NSM as the reference, since it performs generally well among all the baselines.

\ignore{
, our basic model NSM and our proposed approach shows a better overall performance than the baselines. Here, we zoom into the results and check whether our approach is indeed better than baselines in specific cases. For ease of comprison, we only incorporate the results of NSM as the reference, since it perform generally well among all the baselines.
}




\subsubsection{Ablation Study}
Previous experiments have indicated that the major improvement is from the contribution of the teacher network. Here, we compare the effect of different implementations of the teacher network. 
The compared variants include: (1) \underline{$\emph{NSM}_{+f}$} using only the forward reasoning (unidirectional); (2)  \underline{$\emph{NSM}_{+b}$} using only the backward reasoning (unidirectional); (3) \underline{$\emph{NSM}_{+p}$} using the parallel reasoning (bidirectional); (4) \underline{$\emph{NSM}_{+h}$} using the hybrid reasoning (bidirectional); (5) \underline{$\emph{NSM}_{+p,-c}$} removing the correspondence loss (Eq.~\ref{eq-loss-corr}) from $\emph{NSM}_{+p}$; and (6) \underline{$\emph{NSM}_{+h,-c}$} removing the correspondence loss (Eq.~\ref{eq-loss-corr}) from $\emph{NSM}_{+h}$.  
In Table~\ref{tab:ablation}, we can see that  unidirectional reasoning is consistently worse than bidirectional reasoning: the variants of $\text{NSM}_{+f}$ and $\text{NSM}_{+b}$ have a lower performance than the other variants. 
Such an observation verifies our assumption that bidirectional reasoning can improve the learning of intermediate supervision signals. 
Besides, by removing the correspondence loss from the teacher network, the performance substantially drops, which indicates that forward and backward reasoning can mutually  enhance each other. 

\begin{table}[htbp]
	\centering
	\caption{Ablation study of the teacher network (in percent).}
	\label{tab:ablation}\begin{tabular}{c c c | c c}
\hline
			\multirow{2}{*}{Models}&\multicolumn{2}{c}{Webqsp}&\multicolumn{2}{c}{CWQ}\\
			\cline{2-5}
			&Hits&F1&Hits&F1\\
			\hline
			NSM	&	68.7& 62.8&47.6&	42.4\\
			\hline
			$\text{NSM}_{+f}$&	70.7& 64.7& 47.2& 41.5\\
			$\text{NSM}_{+b}$&	71.1& 65.4& 47.1& 42.7\\
			$\text{NSM}_{+p,-c}$&	72.5&	66.5& 47.7& 42.7\\
			$\text{NSM}_{+h,-c}$&	73.0& 	66.9& 47.5& 42.1\\
			$\text{NSM}_{+p}$&	73.9& 66.2	&48.3&\textbf{44.0}\\
$\text{NSM}_{+h}$&	\textbf{74.3}& \textbf{67.4}& \textbf{48.8} & \textbf{44.0}\\
			\hline
		\end{tabular}\end{table}

\ignore{
To accurately label the intermediate entities and improve multi-hop KBQA task, our approach has made several technical extensions.
Here, we examine how each of them affects the final performance. 
We consider the following variants of our approach for comparison:
}


	\ignore{
	$\bullet$ \emph{NSM}: the variant without teacher.

	$\bullet$ \emph{teacher-f}: the variant adopt teacher with forward reasoning.
	
	$\bullet$ \emph{teacher-b}: the variant adopt teacher with backward reasoning.

	$\bullet$ \emph{teacher-p w/o corr}: the variant adopt teacher with parallel structure, but drop the correspondence constraint.

	$\bullet$ \emph{teacher-c w/o corr}: the variant adopt teacher with circle structure, but drop the correspondence constraint.
}










\begin{figure}[htbp]
 \centering
 \subfigure[Varying $\lambda$ on webqsp dataset.]{\label{fig-varing-webqsp}
  \centering
  \includegraphics[width=0.225\textwidth, height=0.2\textwidth]{webqsp_1.pdf}
 }
 \subfigure[Varying $\lambda$ on CWQ dataset.]{\label{fig-varing-CWQ}
  \centering
  \includegraphics[width=0.225\textwidth, height=0.2\textwidth]{CWQ_1.pdf}
 }
 \centering
 \caption{Performance tuning of our approach.}
 \label{fig-parameter-tuning}
\end{figure}

\begin{figure*}[htbp]
 \centering
  \subfigure[The student network before improvement.]{\label{fig-case-nsm}
  \centering
  \includegraphics[width=0.32\textwidth]{case-spurious-new.pdf}
 }
  \subfigure[The teacher network with hybrid reasoning.]{\label{fig-case-teacher}
  \centering
  \includegraphics[width=0.32\textwidth]{case-good-new.pdf}
 }
 \subfigure[The student network after improvement.]{\label{fig-case-student}
  \centering
  \includegraphics[width=0.32\textwidth]{case-good-student-new.pdf}
 }
 \centering
 \caption{A case from the MetaQA-3hop dataset. We use green, red, yellow and grey circles to denote the topic entity, correct answer, intermediate entities and irrelevant entities respectively. The red colored edges denote the actual  reasoning paths for different methods. The color darkness indicates the relevance degree of an entity by a method. For simplicity, we only visualize the entities with a probability equal to or above $0.01$.
}
 \label{fig-case-study}
\end{figure*}

\subsubsection{Parameter Tuning} 
In our approach, we have several combination coefficients to tune, including $\lambda$ in Eq.~\ref{eq-loss-student}, and $\lambda_b$ and $\lambda_c$ in Eq.~\ref{eq-loss-teahcer}. We first tune $\lambda$ amongst \{0.01, 0.05, 0.1, 0.5, 1.0\}, which controls the influence of the teacher network on the student network. 
As shown in Fig.~\ref{fig-parameter-tuning},  hybrid reasoning seems to work
well with small $\lambda$ (\eg 0.05), while parallel reasoning works better with relatively large $\lambda$ (\eg 1.0). 
Similarly, we can tune the parameters of $\lambda_b$ and $\lambda_c$.
Overall, we find that  $\lambda_c=0.01$  and $\lambda_b=0.1$ are good choices for our approach. Another parameter to tune is the embedding dimension $d$ (which is set to 100), and we do not observe significant improvement when $d>100$. 
The reasoning steps $n$ should be adjusted for different datasets. We observe that our approach achieves the best performance on CWQ dataset with $n=4$, while $n=3$ for the other datasets with exhaustive search. Due to space limit, we omit these tuning results. 





\ignore{
Besides the effect of intermediate labeling, we also examine the effect of two parameters, namely the constraint term $\lambda_c$ and backward term $\lambda_b$ for training teacher. Overall, we find that it yields a good performance when $\lambda_c=0.01$  and $\lambda_b=0.1$, where the other values in the set  \{0.01, 0.05, 0.1, 0.5, 1.0\} give worse results. 
While, for other parameters like embedding dimensions $d$, it doesn't show further improvement after $d > 100$. Due to space limit, we omit the results here.
}


\ignore{As Fig.~\ref{fig-parameter-tuning} show, our approach of circle structure perform best with relatively small $\lambda$, such as  $\lambda = 0.05$, and show performance degrade with higher value. Our approach of parallel structure keep growing with increasing $\lambda$. 
So $0.05$ may be a good hyperparameter to train our approach of circle, while $1.0$ may be a good hyperparameter to train our approach of parallel structure.
}





\subsubsection{Evaluating Intermediate Entities}
A major assumption we made is that our teacher network can obtain more reliable intermediate entities than the student network. 
Here, we compare the performance of the two networks in finding intermediate entities. Since the MetaQA-3hop dataset is created using pre-defined templates, we can recover the ground-truth entities at intermediate hops. We consider it a retrieval task and adopt the standard \emph{Precision}, \emph{Recall} and \emph{F1} as evaluation metrics.  
From Table~\ref{tab:acc-int}, we can see that the teacher network is much better than the student network in finding intermediate entities, but has slightly worse performance at the second hop. 
Note that the results of the third hop have been omitted, since it is the last hop. 
Since the student network only utilizes forward reasoning, the results of the first hop are more important than those of subsequent hops. These results also explain why our teacher-student approach is better than the single student model. 

\begin{table}[htbp]
	\centering
	\caption{Performance comparison \emph{w.r.t.} different hops on MetaQA-3hop dataset (in percent).}
	\label{tab:acc-int}\begin{tabular}{c c c c| c c c}
\hline
			\multirow{2}{*}{Models}&\multicolumn{3}{c}{Hop 1}&\multicolumn{3}{c}{Hop 2}\\
			\cline{2-7}
			& Pre& Rec& F1& Pre& Rec& F1\\
			\hline
			Student&	61.0& 60.6& 60.4& \textbf{99.9}& 70.2& \textbf{80.8}\\
Teacher$_{+p}$&	80.0& 59.0& 66.3& 95.0& 68.9& 78.8\\
			Teacher$_{+h}$&	\textbf{99.9}& 56.0& \textbf{70.9}& 99.7& 63.0& 75.4\\
			\hline
		\end{tabular}\end{table}

\ignore{
On MetaQA datasets, we can obtain accurate intermediate entities from gold reasoning path. Here, we verify whether our approach can obtain better intermediate accuracy on MetaQA-3hop dataset. Just as results shown in Table~\ref{tab:acc-int}, our approach show much higher accuracy (precision) to predict intermediate entities at the first step of reasoning. 
While on the second step, the forward reasoning achive best accuracy to infer intermediate entities. Our teacher combine forward reasoning with backward reasoning to obtain labeling, and get slightly worse results. Overall, our designed teacher architecture can address the spurious reasoning to some extent, and label entities in intermediate steps with much higher accuracy.
}


\ignore{
\subsubsection{Improvement Analysis w.r.t. Question Type}
On CWQ dataset, all questions are categorized into 4 groups. Here, we try to address how our approach improve multi-hop KBQA w.r.t question type.
As results  shown in Table~\ref{tab:res-cwq}, the improvement mainly come from question of composition and conjunction type. In these questions, multi-hop reasoning play a key role. 
While for question of superlative and comparative type, the cases achieve Hits is close. It may be caused by that our approach don't take numerical operation into consideration.
\begin{table}[htbp]
	\centering
	\caption{Results on CWQ dataset w.r.t question type. The number indicate number of cases achieve Hits.}
	\label{tab:res-cwq}\begin{tabular}{c | c c c c}
\hline
Type&NSM&teacher-p&Total\\
		\hline
		\hline
		composition&	627&	656&	1546\\
		conjunction&	884&	919&	1575\\
		superlative	&	58&	55&	197\\
		comparative	&	85&	87&	213\\
		Total &	1654&	1717&	3531\\
\hline
	\end{tabular}\end{table}}





\subsubsection{One-Shot Evaluation}
In Table~\ref{tab:res}, we have found that the improvement of our approach over the basic NSM model is very small on the MetaQA datasets. We suspect that this is because the amount of training data for MetaQA is more than sufficient:  $100K$ training cases for 
no more than 300 templates in each dataset. 
To examine this, we randomly sample a single training case for every question template from the original training set, which forms a  one-shot training dataset. 
We evaluate the performance of our approach trained with this new training dataset. The results are shown in Table~\ref{tab:res-1shot}. As we can see, our approach still works very well, and the improvement over the basic NSM becomes more substantial.


\ignore{
As MetaQA datasets only contain very limited question template: no more than 300 templates for every dataset, but around 10k training cases. 
Our basic model may be good enough to conduct accurate multi-hop reasoning under so many training cases.
To verify the effectiveness of our approach on MetaQA dataset, we randomly sample only 1 training case for every question template in training set to replace original training set, and keep the validation and testing set same as original. The results are shown in Table~\ref{tab:res-1shot}. These results indicate that our approach are useful to improve the performance under extremely sparse setting.
}

\begin{table}[htbp]
	\centering
	\caption{Results under one-shot setting (in percent).}
	\label{tab:res-1shot}\begin{tabular}{c c c |c c |c c }
\hline
			\multirow{2}{*}{Models}&\multicolumn{2}{c}{MetaQA-1}&\multicolumn{2}{c}{MetaQA-2}&\multicolumn{2}{c}{MetaQA-3}\\
			\cline{2-7}
			&Hits&F1&Hits&F1&Hits&F1\\
			\hline
			NSM	&	 93.3& 92.6 &  97.7& 96.0& 90.6& 74.5\\
			\hline
			NSM$_{+p}$&	 \textbf{94.3}&  \textbf{93.9}&  \textbf{98.7}& \textbf{96.4}& \textbf{97.0}& 79.8\\
NSM$_{+h}$&	 93.9&  93.7&  98.4& 95.8& 95.6& \textbf{81.6}\\
			\hline
		\end{tabular}\end{table}

\subsection{Case Study}




The major novelty of our approach lies in the teacher network.   Next, we present a case study for illustrating how it helps the student network.

Given the question ``\emph{what types are the movies written by the screenwriter of the music lovers}'', the correct reasoning path is 
 ``\emph{The Music Lovers}''~(movie) $\rightarrow_{\textit{written by}}$  ``\emph{Melvyn Bragg}''~(screenwriter) $\rightarrow_{\textit{write}}$ ``\emph{Play Dirty}''~(movie) $\rightarrow_{\textit{has genre}}$ ``\emph{War}''~(genre). Note that ``\emph{Isadora}'' is also qualified at the second step.
However, its genre is missing in the KB.
Fig.~\ref{fig-case-study} presents a comparison between the learned results of the student before improvement (\ie without the teacher network), the teacher network and the student network after improvement. 





As shown in Fig.~\ref{fig-case-nsm}, the original student network has selected a wrong path leading to an irrelevant entity. At the first hop, NSM mainly focuses on the two  entities ``\emph{Ken Russell}'' and ``\emph{Melvyn Bragg}''  with probabilities of $0.48$ and $0.51$ respectively. Since it mistakenly includes ``\emph{Ken Russell}'' (director of ``\emph{The Music Lovers}'') at the first reasoning step, it finally ranks ``\emph{Drama}'' as the top entity and chooses an irrelevant entity as the answer.
In comparison, the teacher network (Fig.~\ref{fig-case-teacher}) is able to combine forward and backward reasoning to enhance the intermediate entity distributions. As we can see, our teacher assigns a very high  probability of $0.99$ to the entity ``\emph{Melvyn Bragg}'' at the first step.
When the supervision signals of the teacher are incorporated into the student, it correctly finds the answer entity ``\emph{War}'' with a high probability of 0.99 (Fig.~\ref{fig-case-student}).

This example has shown that our teacher network indeed provides very useful supervision signals at intermediate steps to improve the student network. 

\ignore{
To address such spurious reasoning, we propose to label entities in intermediate reasoning steps. Here, we adopt teacher with circle structure to label intermediate entities. 
As Fig.~\ref{fig-case-teacher} show, our teacher label ``\emph{Melvyn Bragg}'' with probability $0.99$ at the first reasoning step. At the second reasoning step, our teacher label qualified entities ``\emph{Play Dirty}'' and ``\emph{Isadora}'' with probability $0.74$ and $0.24$ respectively. 
While the teacher can rank the correct answer ``\emph{War}'' as top one at the last forward reasoning step, its prediction about answers is still not perfect. If we follow such predicted distribution to sample entities as answer, wrong entities may be predicted with high probability ($0.23$ for both ``\emph{Musical}'' and ``\emph{Comedy}''). When the supervision signals of the teacher network has been incorporated into the student network, it  correctly finds the answer entity ``\emph{War}'' with a high .
}
\ignore{
NSM model trained without intermediate feedback may have multiple reasoning paths, while most of them are irrelevant with the question. 
At the first reasoning step, NSM pay attention to both ``\emph{Ken Russell}'' and ``\emph{Melvyn Bragg}'', with probability of $0.48$ and $0.51$ respectively. While at the second reasoning step, NSM pay more attention to entities related with ``\emph{Ken Russell}'', such as ``\emph{The Boy Friend}'' with probability $0.1$ and ``\emph{Whore}'' $0.15$. And finally, NSM reason ``\emph{Drama}'', ``\emph{War}'', ``\emph{Musical}'', ``\emph{Comedy}'' as possible answers and rank wrong answer ``\emph{Drama}'' at top. Overall, NSM mistakely include ``\emph{Ken Russell}'' (director of ``\emph{the music lovers}'') at the first reasoning step and finally reason multiple entities related with it as answers. 
This may be caused by spurious reasoning for similar question, which start to reason directors rather than screenwriters and incidentally reach answers .
}

\ignore{
To address such spurious reasoning, we propose to label entities in intermediate reasoning steps. Here, we adopt teacher with circle structure to label intermediate entities. 
As Fig.~\ref{fig-case-teacher} show, our teacher label ``\emph{Melvyn Bragg}'' with probability $0.99$ at the first reasoning step. At the second reasoning step, our teacher label qualified entities ``\emph{Play Dirty}'' and ``\emph{Isadora}'' with probability $0.74$ and $0.24$ respectively. 
While the teacher can rank the correct answer ``\emph{War}'' as top one at the last forward reasoning step, its prediction about answers is still not perfect. If we follow such predicted distribution to sample entities as answer, wrong entities may be predicted with high probability ($0.23$ for both ``\emph{Musical}'' and ``\emph{Comedy}'').
}
\ignore{
After obtaining intermediate labeling (teacher labeling about the entity distribution of first and second reasoning steps) from our teacher, we train a new student model. As Fig.~\ref{fig-case-student} show, our student model follow the expected reasoning path to reason correct answer ``\emph{War}' out. In the view of both answers and reasoning process, it achieve gold reasoning for this question. This case demonstrates the effectiveness of our approach under teacher-student framework to conduct multi-hop reasoning for KBQA task.
}


%
