

\documentclass{article}

\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure} 


\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage{hyperref}
\usepackage{bm}
\usepackage{url}
\usepackage{xspace}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\cfnade}{CF-NADE\xspace}
\newcommand{\cfnadein}{CF-NADE-V1\xspace}
\newcommand{\cfnadeout}{CF-NADE-V2\xspace}
\newcommand{\cfnadeboth}{CF-NADE-V3\xspace}
\newcommand{\sigm}{\mathrm{sigm}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\cost}{\mathcal{C}}


\newboolean{for_submission}
\setboolean{for_submission}{false}
\newcommand{\says}[3]{\ifthenelse{\boolean{for_submission}}{}{{\color{#3}#1 says: \emph{#2}\color{black}}\xspace}}

\newcommand{\ericsays}[1]{\says{Eric}{#1}{blue}}
\newcommand{\yinsays}[1]{\says{Yin}{#1}{red}}
\newcommand{\bangshengsays}[1]{\says{Bangsheng}{#1}{cyan}}
\newcommand{\wenkuisays}[1]{\says{Wenkui}{#1}{magenta}}

\usepackage[accepted]{icml2016}


\icmltitlerunning{A Neural Autoregressive Approach to Collaborative Filtering}

\begin{document} 
\twocolumn[
\icmltitle{A Neural Autoregressive Approach to Collaborative Filtering}

\icmlauthor{Yin Zheng}{yin.zheng@hulu.com}
\icmlauthor{Bangsheng Tang}{bangsheng@hulu.com}
\icmlauthor{Wenkui Ding}{wenkui.ding@hulu.com}
\icmlauthor{Hanning Zhou}{eric.zhou@hulu.com}
\icmladdress{Hulu LLC., Beijing, 100084}
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]



\begin{abstract} 



This paper proposes \cfnade, a neural autoregressive
  architecture for collaborative filtering (CF) tasks, which is
  inspired by the Restricted Boltzmann Machine (RBM) based CF model and
  the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic \cfnade model for CF tasks. 
  Then we propose to improve the model by 
  sharing parameters between different ratings. A factored version of
  \cfnade is also proposed for better scalability. Furthermore,
  we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize \cfnade, which shows superior performance. 
  Finally, \cfnade can be extended to a deep model, with only
  moderately increased computational complexity. Experimental results
  show that \cfnade with a single hidden layer beats all previous
  state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix
  datasets, and adding more hidden layers can further improve the performance.

\end{abstract} 

\section{Introduction}
\label{sec:intro}  




Collaborative filtering (CF) is a class of methods for predicting a
user's \emph{preference} or \emph{rating} of an item, based on his/her
previous preferences or ratings and decisions made by \emph{similar}
users. CF lies at the core of most recommender systems and has
attracted increasing attention along with the recent boom of
e-commerce and social network systems. The premise of CF is that a
person's preference does not change much over time. A good CF
algorithm helps a user discover products or services that suit his/her
taste efficiently.



Generally speaking, there is a dichotomy of CF methods: Memory-based CF and Model-based CF. Memory-based CF usually computes the similarities between users or items directly from the rating data, which are then used for recommendation.  The explainatbility of the recommended results as well as the easy-to-implement nature of memory-based CF ensured its popularity in early recommender
  systems~\citep{resnick1994grouplens}. However, memory-based CF has
  faded out due to its poor performance on real-life large-scale and sparse data. 


Distinct from memory-based CF, model-based CF learns
  a model from historical data and then uses the model to predict
  preferences of users. The models are usually developed with machine
  learning algorithms, such as Bayesian networks, clustering models
  and latent semantic models. Complex preference patterns can be
  recognized by these models, allowing model-based CF to perform better for
   preference prediction tasks.  Among all these models,
  matrix factorization is most popular and successful, c.f.
  \citep{koren2009matrix,salakhutdinov2008bayesian,mackey2011divide,gopalan2013scalable}.



  With the recent development of deep
  learning~\citep{krizhevsky2012imagenet,szegedy2014going,he2015deep},
  neural network based CF, a subclass of model-based CF, has gained
  enormous attention. A prominent example is RBM-based CF
  (RBM-CF)~\citep{salakhutdinov2007restricted}. RBM-CF is a two-layer
  undirected generative graph model which generalizes Restricted
  Boltzmann Machine (RBM) to modeling the distribution of tabular
  data, such as user's ratings of movies. RBM-CF has shown its power
  in Netflix prize challenge. However, RBM-CF suffers from inaccuracy
  and impractically long training time, since training RBM-CF is
  intractable and one has to rely on variational approximation
  or MCMC sampling.



  Recently, a good alternative to RBM has been proposed by
  \citet{larochelle2011neural}. The so-called Neural Autoregressive
  Distribution Estimator (NADE) is a tractable distribution estimator
  for high dimensional binary vectors. NADE computes the conditional
  probabilities of each element given the other elements to its left
  in the binary vector, where all conditionals share the same
  parameters. The probability of the binary vector can then be
  obtained by taking the product of these conditionals. Unlike RBM,
  NADE does not incorporate any latent variable where expensive
  inference is required, in constrast it can be optimized efficiently
  by backpropagation. NADE together with its variants achieved
  competitive results on many machine learning
  tasks~\citep{larochelle2012neural,uria2013rnade,zheng14sup,Uria2013b,zheng2014neural,zheng15deep}.





  In this paper, we propose a novel model-based CF approach named
  \cfnade, inspired by RBM-CF and NADE models. Specifically, we will
  show how to adapt NADE to CF tasks and describe how to improve the
  performance of \cfnade by encouraging the model to share parameters
  between different ratings. We also propose a factored version of
  \cfnade to deal with large-scale dataset efficiently. As
  \citet{phung2009ordinal} observed, preference usually has the
  \emph{ordinal nature}: if the true rating of an item by a user is 3
  stars in a 5-star scale, then predicting 4 stars is preferred to
  predicting 5 stars. We take this ordinal nature of preferences into
  consideration and propose an ordinal cost to optimize
  \cfnade. Moreover, we propose a deep version of \cfnade, which can
  be optimized efficiently. The performance of \cfnade is tested on
   real world benchmarks: MovieLens 1M, MovieLens 10M and Netflix
  dataset. Experimental results show that \cfnade outperforms all previous
  state-of-the-art methods.







\section{Related Work}
\label{sec:related_work}

As mentioned previously, some of the most successful model-based CF
methods are based on matrix factorization (MF) techniques, where a
prevalent assumption is that the partially observed matrix is of low
rank. In general, MF characterizes both users and items by
vectors of latent factors, where the number of factors is much smaller
than the number of users or items, and the correlation between user
and item factor vectors are used for recommendation
tasks. Specifically, \citet{billsus1998learning} proposed to apply
Singular Value Decomposition (SVD) to CF tasks, which is an early work
on MF-based CF. Bias MF \cite{koren2009matrix} is proposed to improve
the performance of SVD by introducing systematic biases associated
with users and items. \citet{mnih2007probabilistic} extended MF to a
probabilistic linear model with Gaussian noise referred to as
Probabilistic Matrix Factorization (PMF), and showed that PMF
performed better than SVD. \citet{salakhutdinov2008bayesian} proposed
a Bayesian treatment of PMF, which can be trained efficiently by MCMC
methods. Along this line, \citet{lawrence2009non} proposed a non-linear
PMF using Gaussian process latent variable models. There are other
MF-based CF methods such as
\citep{rennie2005fast,mackey2011divide}. Recently, Poisson Matrix
Factorization~\citep{gopalan2014content,gopalan2014bayesian,gopalan2013scalable}
was proposed, replacing Gaussian assumption of PMF by Poisson
distribution. \citet{lee2013local} extended the low-rank assumption by
embedding locality into MF models and proposed Local Low-Rank Matrix
Approximation (LLORMA) method, which achieved impressive performance on several public benchmarks.




Another line of model-based CF is based on neural networks. With the
tremendous success of deep
learning~\citep{krizhevsky2012imagenet,szegedy2014going,he2015deep},
neural networks have found profound applications in CF
tasks. \citet{salakhutdinov2007restricted} proposed a variant of
Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully
applied in Netflix prize
challenge~\citep{bennett2007netflix}. Recently,
\citet{sedhain2015autorec} proposed AutoRec, an autoencoder-based CF
model, which achieved the state-of-the-art performance on some
benchmarks. RBM-CF~\citep{salakhutdinov2007restricted} and
AutoRec~\citep{sedhain2015autorec} are common in that both of them
build different models for different users, where all these models
share the parameters.  \citet{phung2009ordinal} proposed to apply
Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by
integrating the correlation between users and between
items. \citet{phung2009ordinal} also extended the standard BM model so
as to exploit the ordinal nature of ratings. Recently,
\citet{dziugaite2015neural} proposed Neural Network Matrix
Factorization (NNMF), where the inner product between the vectors of
users and items in MF is replaced by a feed-forward neural
network. However, NNMF does not produce convincing results on
benchmarks.

Our proposed method \cfnade, can be generally categorized as a neural
network based CF method. \cfnade bears some similarities with
NADE~\citep{larochelle2011neural} in that both model vectors with
neural autoregressive architectures. The crucial difference between
\cfnade and NADE is that \cfnade is designed to deal with vectors of
variable length, while NADE can only deal with binary vectors of fixed
length. Though DocNADE~\citep{larochelle2012neural} does take inputs
with various lengths, it is designed to model unordered sets of words
where each element of the input corresponds to a word, while \cfnade
models user rating vectors, where each element corresponds to the
rating to a specific item.




\section{NADE for Collaborative Filtering}
\label{sec:cfnade}



This section devotes to \cfnade, a NADE-based model for CF
tasks. Specifically, we describe the basic model of \cfnade in
Section~\ref{sec:model}, and propose to improve \cfnade by
sharing parameters between different ratings in
Section~\ref{sec:accu}. At last, a factored version of \cfnade is
described in Section~\ref{sec:factored} to deal with large-scale
datasets.



\subsection{The Model} 
\label{sec:model}


Suppose that there are  items,  users, and the ratings are
integers from  to  (-star scale). One practical and prevalent
assumption in CF literature is that a user usually rated 
items\footnote{ might vary between different users}, where
. To tackle sparsity, similar to
RBM-CF~\citep{salakhutdinov2007restricted}, we use a different \cfnade
model for each user and all these models share the same
parameters. Specifically, all models have the same number of hidden
units, but a user-specific model only has  visible units if the user
only rated  items. Thus, each \cfnade has only one single training
case, which is a vector of ratings that a user gave to his/her viewed
items, but all the weights and biases of these \cfnade 's are tied.

In this paper, we denote the training case for user  as
,
where  is a -tuple in the set of permutations of
 which serves as an ordering of the rated items
, and 
denotes the rating that the user gave to item . For
simplicity, we will omit the index  of , and focus on a
single user-specific \cfnade in the rest of the paper.





\cfnade models the probability of the rating vector  by the chain rule as:

where  denotes the first  elements of  indexed by .

Similar to NADE~\citep{larochelle2011neural}, \cfnade models the
conditionals in Equation~\ref{eqn:chain_rule} with neural networks. To
compute the conditionals in Equation~\ref{eqn:chain_rule}, \cfnade
first computes the hidden representation of dimension  given
 as follows:

where  is the activation function, such as ,  is the connection
matrix associated with rating ,  is the \textsuperscript{th} column of   and  is an interaction
parameter between the \textsuperscript{th} hidden unit and item
 with rating ,   is the bias term.

Then the conditionals in Equation~\ref{eqn:chain_rule} could be modeled as:

where  is the score indicating the preference that the user gave rating  for item  given the previous ratings , and  is computed as,

where  and   are the connection matrix and the bias term associated with rating , respectively. 

\cfnade is optimized for minimum negative log-likelihood of
 (Equation~\eqref{eqn:cfnade_softmax}),

averaged over all the training cases. As in NADE, the ordering  in
\cfnade must be predefined and fixed during training for each
user. Ideally, the ordering should follow the timestamps when the user
gave the ratings. In practice, we find that a ordering that is
randomly drawn from the set of permutations of  for
each user  yields good results. As \citet{Uria2013b} observed, we can think of the models trained with different orderings as different instantiations of \cfnade for the same user.
Section~\ref{sec:deepcfnade} will discuss how to (virtually) train a factorial number of \cfnade 's with different orderings simultaneously, which is the key to extend \cfnade to a deep model efficiently.





Once the model is trained, given a user's past behavior , the user's rating of
a new item  can be predicted as 

where conditional  are computed by Equation~\ref{eqn:cfnade_softmax} along with the hidden representation  and score  computed as 







\subsection{Sharing Parameters Between Different Ratings}
\label{sec:accu}

In Equations~\ref{eqn:cfnade_hidden} and \ref{eqn:score}, the
connection matrices ,  and the bias 
are different for different ratings 's. In other words, \cfnade
uses different parameters for different ratings. In practice, for a
specific item, some ratings can be much more often observed than
others.  As a result, parameters associated with a rare rating might
not be sufficiently optimized. To alleviate this problem, we propose
to share parameters between different ratings of the same item.

Particularly, we propose to compute the hidden representation  as follows:

Note that, given an item  rated  by the user,
 depends on all the weights
, . Thus,
Equation~\ref{eqn:cfnade_hidden_shared} encourages a solution that
 is utilized by all the ratings , .





Similarly, the score  in
Equation~\ref{eqn:cfnade_softmax} is adjusted as

where  and  are shared by the rating , where .

Sharing parameters between different ratings can again be understood
as a kind of regularization, which encourages the model to use as many
parameters as possible to explain the data. Experimental results in
Section~\ref{sec:variants} confirm the advantage of this
regularization.




\subsection{Dealing with Large-Scale Datasets}
\label{sec:factored}
One disadvantage of \cfnade we have described so far is that the
parameterization of  and
, where  ranges from  to ,
will result in too many free parameters, especially when dealing with
massive datasets. For example, for the Netflix
dataset~\citep{bennett2007netflix}, when , the number of free
parameters by  and  would be around 
million\footnote{Netflix dataset contains  movies ()
  and the ratings are -star scale (). Thus, the number of free
  parameters from  and  is
  .}. Although severe
overfitting can be avoided by proper weight-decay or
dropout~\citep{srivastava2014dropout}, learning such a huge network
would still be problematic.


Inspired by RBM-CF~\citep{salakhutdinov2007restricted} and
FixationNADE~\citep{zheng2014neural}, we propose to address this
problem by factorizing  and  into products of
two lower-rank matrices. Particularly,

where ,
,  and
 are lower-rank matrices with 
and . For example, by setting , the number of free
parameters for  and  decreases from  million to
about  million. In our experiments, this factored version of \cfnade
will be applied on large-scale datasets.

\section{Traing \cfnade with Ordinal Cost}
\label{sec:cost}



\cfnade can be trained by minimizing the negative log-likelihood based
on conditionals defined by Equation~\ref{eqn:cfnade_softmax}. To go
one step further, following \citet{phung2009ordinal}, we take the
\emph{ordinal nature} of a user's preference into consideration. That
is, if a user rated an item , the preference of the user
to the ratings from  to  should increase monotonically and the
preference to the ratings from  to  should decrease
monotonically. The basic \cfnade treats different ratings as separate
labels, leaving the ordinal information not captured. Here we describe
how to equip \cfnade with an ordinal cost.

Formally, suppose , the ranking of preferences over all
the possible ratings under the ordinal assumption can be expressed as:

where  denotes the preference of rating  over ,
\footnote{Equation~\ref{eqn:ordinal_down} is
  omitted if the true rating is ; likewise,
  Equation~\ref{eqn:ordinal_up} is omitted if the true rating is .}.
Two rankings of ratings,  and
, can be induced by
Equation~\ref{eqn:ordinal_down} and \ref{eqn:ordinal_up}:


Note that maximizing the conditional  in Equation~\ref{eqn:cfnade_softmax} only ensures that the probability of rating  is the largest among all possible ratings. To capture the ordinal nature induced by Equations~\ref{eqn:ordinal_down} and \ref{eqn:ordinal_up}, we propose to compute the conditional  as

where  is a shorthand for the score  introduced in Section~\ref{sec:model}, which indicates the preference to rating  of item  given the previous context . 

Both two products in Equation~\ref{eqn:ordinal_conditional} can be
interpreted as the \emph{likelihood loss} introduced
in~\citep{xia2008listwise} in the context of \emph{Listwise Learning
  To Rank} problem. Actually, from the perspective of
learning-to-rank, \cfnade acts as a ranking function which produces
rankings of ratings based on previous ratings, where
 corresponds to the score
function in~\citep{xia2008listwise} and the rankings,
 and , corresponds to
true rankings that we would like \cfnade to fit. Thus, the conditional
computed by Equation~\ref{eqn:ordinal_conditional} is actually the
conditional distribution of the rankings  and
 given previous  ratings. Put differently, the ranking loss in Equation~\ref{eqn:ordinal_conditional}
is defined on the ratings, while other learning-to-rank based CF methods, such as \citep{shi2010list}, are on items, which is the crucial difference.

For the rest of the paper, we denote the negative log-likelihood based
on the conditionals computed by Equation~\ref{eqn:ordinal_conditional}
as \textbf{ordinal cost} , and denote the
negative log-likelihood based on Equation~\ref{eqn:cfnade_softmax} as
\textbf{regular cost} . The final cost to
optimize the model is then defined as

where  is the hyperparameter to determine the weight of
. The impact of the hyperparameter  on
the performance of \cfnade is discussed in Section~\ref{sec:variants}.




 





\section{Extending \cfnade to a Deep Model}
\label{sec:deepcfnade}
So far we have described \cfnade with single hidden layer. As suggested
by the recent and impressive success of deep neural
networks~\citep{krizhevsky2012imagenet,szegedy2014going,he2015deep},
extending \cfnade to a deep, multiple hidden layers architecture could
allow us to have better performance. Recently, \citet{Uria2013b}
proposed an efficient deep extension to original
NADE~\citep{larochelle2011neural} for binary vector observations,
which inspires other related deep model~\citep{zheng15deep}. Following
\citep{Uria2013b}, we propose a deep variant of \cfnade.


As mentioned in Section~\ref{sec:model}, a different \cfnade model is
used for each user and the ordering  in  is stochastically
sampled from the set of permutations of . Training
\cfnade on stochastically sampled orderings corresponds, in
expectation, to minimizing the cost in Equation~\ref{eqn:hybrid_cost}
over all possible orderings for each user. As noticed by
\citet{Uria2013b} and \citet{zheng15deep}, training over all possible
orderings for \cfnade implies that for any given context
, the model performs equally well at
predicting all the remaining items in ,
since for each item there is an ordering such that it appears at
position . This is the key
observation to extend \cfnade to a deep model. Specifically, instead
of sampling a complete ordering over all the  items, we instead
sample a context  and perform an update of
the conditionals using that context.

The procedure is done as follows. Given a user who has rated 
items, an ordering  is first sampled randomly from the set of
permutations of  for each update and a vector
 is
generated according to the ordering . Then a split point  is
randomly drawn from  for each update. The split point
 divides  into two parts:  and
. According to the analysis above, in the
new training procedure,  is considered as the
input of \cfnade and the training objective is to maximize
conditionals  for each element in
. The cost function with this procedure is

By Equation~\ref{eqn:deep_cost}, the model predicts the ratings of
each items after the splitting position  in the randomly drawn
ordering  as if it were actually at position . The factors in
front of the sum come from the fact that the total number of elements
in the sum is  and that we are averaging over  possible
choices for the item at position , similar to \citep{Uria2013b} and
\citep{zheng15deep}. Derivation of Equation~\ref{eqn:deep_cost} can be
found in the supplementary materials.


In this procedure, a training update relies only on a single hidden
representation , more hidden
layers can be added into \cfnade with the
computational complexity increased moderately. Particularly, suppose
 is the hidden
representation computed in Equation~\ref{eqn:cfnade_hidden}. Then new hidden layers can be
added as in a regular deep feed-forward neural network:

for , where  is the total number of hidden
layers. Then the conditionals
, either in
Equation~\ref{eqn:cfnade_softmax} or
Equation~\ref{eqn:ordinal_conditional}, can be computed from
. To this end, the number of
operations a \cfnade takes for one input is , as
in regular multiple layers neural networks, where  is the
average number of ratings for a user and  is the number of hidden
units for each layer.  We denote  and
 as the cost functions of
Equation~\ref{eqn:deep_cost} associated with conditionals computed by
Equation~\ref{eqn:cfnade_softmax} and
Equation~\ref{eqn:ordinal_conditional}, respectively.

Finally, similar to Equation~\ref{eqn:hybrid_cost}, we can also define a hybrid cost  as 


Note that \cfnade with a single hidden layer can also be trained by
Equation~\ref{eqn:deep_cost}. In practice,
Equation~\ref{eqn:deep_cost} can be implemented efficiently on GPUs,
hence we use it throughout our experiments for either one hidden layer
or multiple-layers architecture.







\section{Experiments}
\label{sec:experiments}

In this section, we test the performance of \cfnade on 3 real-world
benchmarks: MovieLens 1M, MovieLens 10M~\citep{harper2015movielens}
and Netflix dataset~\citep{bennett2007netflix}, which contain ,
 and  ratings, respectively. Following
LLORMA~\citep{lee2013local} and AutoRec~\citep{sedhain2015autorec},
 of the ratings in each of these datasets are randomly selected
as the test set, leaving the remaining  of the ratings as the
training set. Among the ratings in the training set,  are used as
validation set. We use a default rating of  for items without
training observations. Prediction error is measured by Root Mean
Squared Error (RMSE), 
where  is the \textsuperscript{th} true rating and
 is the predicted rating by the model,  is the total
number of ratings in the testset. We report the average RMSE on test
set over  different splits and compare \cfnade with some strong
baselines including LLORMA, AutoRec, and other competitive
methods. Experimental results show that \cfnade outperforms the
state-of-the-art performance on these benchmarks.

\subsection{Datasets Description}
\label{sec:datasets}
MovieLens 1M dataset contains around  million anonymous ratings of
approximately  movies by  users, where each user rated at
least  items. The ratings in MovieLens 1M dataset are made on a
-star scale, with -star increments. MovieLen 10M dataset
contains about  million ratings of  movies by 
users. The users of MovieLens 10M dataset are randomly chosen and each
user rated at least  movies. Unlike MovieLens 1M dataset, the
ratings in MovieLens 10M are on a -star scale with
\textit{half}-star increments. Thus, the number of rating scales of
MovieLens 10M is actually . In our experiments, we rescale the
ratings in MovieLens 10M to -star scale with -star
increments. Netflix dataset comes from the Netflix challenge
prize\footnote{The test set of Netflix prize challenge dataset is not
  available now. Following \citet{lee2013local} and
  \citet{sedhain2015autorec}, we split the available trainset of
  Netflix dataset into train, valid and test sets.}. It
is massive compared to the previous two, which contains more than
 million ratings of  movies by  users. The ratings
of Netflix dataset are on -star scale, with -star increments.



\subsection{Experiments on MovieLen 1M Dataset}
\label{sec:movielens1m}

In this section, we test the performance of \cfnade on MovieLen 1M
dataset. We first evaluate the performance of the ordinal cost
described in Section~\ref{sec:cost} with/without sharing parameters
between different ratings as described in Section~\ref{sec:accu}. Then
we compare several variants of \cfnade with some strong baselines.

\subsubsection{The performance of the ordinal cost}
\label{sec:variants}

In this section, we evaluate the impact of the ordinal weight  in
Equation~\ref{eqn:deep_hybrid} on the performance of \cfnade. As
\citet{sedhain2015autorec} mentioned, item-based CF outperforms
user-based CF, therefore we use item-based \cfnade (I-\cfnade) in this
section. Distinct from user-based \cfnade (U-\cfnade), which builds a
different model for each user as we described previously, I-\cfnade
model builds a different \cfnade model for each item. In other words,
the only difference between U-\cfnade and I-\cfnade is that the roles
of users and items are switched. Comparison between U-\cfnade and
I-\cfnade can be found in Section~\ref{sec:1mbaselines}.

The configuration of the experiments is as follows. We use a single
hidden layer architecture and the number of hidden units is set to
, same as AutoRec~\citep{sedhain2015autorec} and
LLORMA~\citep{lee2013local}. Adam~\citep{kingma2014adam} with default
parameters (, and ) are utilized
to optimize the cost function in Equation~\ref{eqn:deep_cost}. The
learning rate is set to  , the weight decay is set to
 and we use the tanh activation function. 

Let {\cfnade}-S denote the variant of \cfnade model where parameters are
shared between different ratings, as described in
Section~\ref{sec:accu}. Figure~\ref{fig:exp1} shows the superior
performance of \cfnade and {\cfnade}-S w.r.t different values of
. Effectiveness of parameter sharing and ordinal cost can be
justified by observing that: 1) {\cfnade}-S always outperforms regular
\cfnade; 2) as the ordinal weight  increases, test RMSE of
both \cfnade and {\cfnade}-S decrease monotonically. Based on these
observations, we will use {\cfnade}-S and fix  throughout
the rest of the experiments.

 \begin{figure}[h]
 \begin{center}
 \centerline{\includegraphics[width=\columnwidth]{exp1.pdf}}
 \caption{The performance of \cfnade and {\cfnade}-S w.r.t ordinal weight  on MovieLens 1M dataset.}
 \label{fig:exp1}
 \end{center}
 \end{figure}
 
 \subsubsection{Comparing with strong baselines on MovieLens 1M}
 \label{sec:1mbaselines}
 


 In this comparison, we compare \cfnade with other baselines on MoiveLens 1M dataset. During the comparison, the learning rate is chosen on the validation set by
 cross-validation among , and the weight decay is
 chosen among .  According to
 Section~\ref{sec:variants}, the weight  of ordinal cost is
 fixed to  and {\cfnade}-S is adopted. The model is trained with
 Adam optimizer and tanh as activation function.
 
 \begin{figure}[h]
 \begin{center}
 \centerline{\includegraphics[width=\columnwidth]{1m_itembased.pdf}}
 \caption{The performance of I-\cfnade w.r.t the number of hidden units on MovieLens 1M dataset.}
 \label{fig:exp2_a}
 \end{center}
 \end{figure}
 
 Table~\ref{tab:movielens1m} shows the performance of {\cfnade}-S and
 baselines. The number of hidden units of \cfnade is , same as
 AutoRec~\citep{sedhain2015autorec} for a fair comparison. One can
 observe that I-{\cfnade}-S outperforms U-{\cfnade}-S by a
 large margin. I-{\cfnade}-S with a single hidden
 layer achieves RMSE of , which is comparable with any strong
 baseline. Moreover, I-{\cfnade}-S with 2 hidden layers achieves RMSE of .

 Figure~\ref{fig:exp2_a} illustrates the performance of I-{\cfnade}-S
 w.r.t the number of hidden units. Increasing the number of hidden
 units is beneficial, but the return is
 diminishing. It can also be observed from Figure~\ref{fig:exp2_a} that deep \cfnade models
 achieve better performance than the shallow ones, as expected.


\begin{table}[h]
 \caption{Test RMSE of different models on MovieLens 1M.}
 \label{tab:movielens1m}
 \begin{center}
 \begin{small}
 \begin{sc}
 \begin{tabular}{lc}
 \hline
 \abovespace\belowspace
 Method & Test RMSE  \\
 \hline
 \abovespace
 PMF    & 0.883 \\
 U-RBM & 0.881\\
 U-AutoRec~\citep{sedhain2015autorec} & 0.874\\
 LLORMA-Global~\citep{lee2013local} & 0.865\\
 I-RBM & 0.854\\
 BiasMF  & 0.845\\
 NNMF~\citep{dziugaite2015neural} &0.843\\
 LLORMA-Local~\citep{lee2013local} & 0.833\\
 \belowspace
 I-AutoRec~\citep{sedhain2015autorec} & 0.831\\
 U-{\cfnade}-S (single layer)  & 0.850 \\
 U-{\cfnade}-S (2 layers )  & 0.845 \\
 I-{\cfnade}-S (single layer) & 0.830\\
 I-{\cfnade}-S (2 layers) & {\bf 0.829}\\
 \hline
 \end{tabular}
 \end{sc}
 \end{small}
 \begin{minipage}{0.48\textwidth}
{\small
: Taken from~\citep{dziugaite2015neural}. \\
: Taken from~\citep{sedhain2015autorec}.
}
\end{minipage}
 \end{center}
 \end{table}
 

 
\subsection{Experiments on MovieLens 10M Dataset}
\label{sec:movielens10m}
As mentioned in Section~\ref{sec:datasets}, the MovieLens 10M 
is much bigger than the MovieLens 1M, so we opt to use the factored
version of \cfnade described in Section~\ref{sec:factored} and set
. Even in this setting, I-\cfnade with  users and 
rating scales will still bring about as many as  million free
parameters,
Hence, we only report the performance of
U-\cfnade in this experiment. Same as in Section~\ref{sec:variants},
we train the model with Adam optimizer and using  as activation
function.  Other configurations are as follows: The number of hidden
units to , the weight decay is  and  is set to
 and parameters are shared between ratings following
Section~\ref{sec:variants}. The base learning rate is , and we
double it for the parameters of the first layer.

Table~\ref{tab:movielens10m} shows the comparison between \cfnade and
other baselines on MovieLens 10M dataset. U-{\cfnade}-S with a single
hidden layer has already outperformed the baselines, which achieves
RMSE of . The performance of U-{\cfnade}-S can be slightly
improved by adding another hidden layer. Noticeably, the test RMSE of
U-AutoRec is much worse than I-AutoRec, whereas U-{\cfnade}-S
outperforms I-AutoRec.

\begin{table}[h]
 \caption{Test RMSE of different models on MovieLens 10M.}
 \label{tab:movielens10m}
 \begin{center}
 \begin{small}
 \begin{sc}
 \begin{tabular}{lc}
 \hline
 \abovespace\belowspace
 Method & Test RMSE  \\
 \hline
 \abovespace
 U-AutoRec~\citep{sedhain2015autorec} & 0.867\\
 I-RBM & 0.825\\
 U-RBM & 0.823\\
 LLORMA-Global~\citep{lee2013local} & 0.822\\
 BiasMF & 0.803 \\
 LLORMA-Local~\citep{lee2013local} & 0.782\\
 \belowspace
 I-AutoRec~\citep{sedhain2015autorec} & 0.782\\
 U-{\cfnade}-S (single layer) & 0.772\\
 U-{\cfnade}-S (2 layers) & {\bf 0.771}\\
 \hline
 \end{tabular}
 \end{sc}
 \end{small}
 \begin{minipage}{0.48\textwidth}
{\small
: Taken from~\citep{sedhain2015autorec}.
}
\end{minipage}
 \end{center}
 \end{table}
 
 \subsection{Experiments on Netflix Dataset}
 \label{sec:netflix}
 Our final set of experiments are on the massive Netflix dataset,
 which contains  ratings. Similar to
 Section~\ref{sec:movielens10m}, we use the factored version of
 U-\cfnade with . The Netflix dataset is so big that we need not
 add a strong regularization to avoid overfitting and therefore set
 the weight decay to . Other configurations are the same as in
 Section~\ref{sec:movielens10m}.
 
 Table~\ref{tab:netflix} compares the performance of U-\cfnade with
 other baselines. We can see that U-{\cfnade}-S with a single
 hidden layer achieves RMSE of , outperforming all
 baselines. Another observation from Table~\ref{tab:netflix} is that
 using a deep \cfnade architecture achieves a slight improvement over
 the shallow one, with a test RMSE of .
 
 \begin{table}[h]
 \caption{Test RMSE of different models on Netflix dataset.}
 \label{tab:netflix}
 \begin{center}
 \begin{small}
 \begin{sc}
 \begin{tabular}{lc}
 \hline
 \abovespace\belowspace
 Methods & Test RMSE  \\
 \hline
 \abovespace
 LLORMA-Global~\citep{lee2013local} & 0.874\\
 U-RBM & 0.845\\
 BiasMF & 0.844 \\
 LLORMA-Local~\citep{lee2013local} & 0.834\\
 \belowspace
 I-AutoRec~\citep{sedhain2015autorec} & 0.823\\
 U-{\cfnade}-S (single layer) & 0.804\\
 U-{\cfnade}-S (2 layers) & {\bf 0.803}\\
 \hline
 \end{tabular}
 \end{sc}
 \end{small}
 \begin{minipage}{0.48\textwidth}
{\small
: Taken from~\citep{sedhain2015autorec}.
}
\end{minipage}
 \end{center}
 \end{table}
 
 \begin{table}[h]
\begin{small}
\centering
\caption{Complexity of \cfnade on different benchmarks}
\label{tab:complexity}
\begin{tabular}{@{}lccccc@{}}
\toprule
                Dataset & Layers & Params & Train Time & Test Time\\ 
                &&(million)&(second)&(second)\\ \midrule
\multirow{2}{*}{ML 1M} &     &  &   &   \\
                  &  &  &  &  \\
\multirow{2}{*}{ML 10M} &  &  &  &  \\
                  &  &  &  &  \\
\multirow{2}{*}{Netflix} &  &   &  &  \\
                  &  &  &  &  \\ \bottomrule
\end{tabular}
\end{small}
\end{table}



\subsection{The Complexity and Running Time of CF-NADE}
\label{exp:complexity}




We implement \cfnade using Theano~\cite{Bastien-Theano-2012} and
Blocks~\citep{van2015blocks}, and the code is available at
\url{https://github.com/Ian09/CF-NADE}. Table~\ref{tab:complexity} shows
the running time of one epoch\footnote{Experiments are conducted on a single NVIDIA Titan X Card.}  as
well as the number of parameters used by \cfnade. For MovieLens 1M dataset, we used the
item-based \cfnade and did not use the factorization method introduced
by Sec~\ref{sec:factored}, hence the number of parameters for
MovieLens 1M is bigger than the other two. Running times in
Table~\ref{tab:complexity} include overheads such as transferring data
from and to GPU memory for each update. Note that
there is still room for faster implementations\footnote{In our implementation, samples are
  represented as  binary matrices, where  is the number
  of items and  is the number of rating scales. An entry  is
  assigned  only if the user gave a -star to item . Thus, we
  could use the {\it tensordot} operator in Theano and feed \cfnade
  with a batch of samples. In the experiments, mini-batch size is set
  to . One disadvantage of this implementation is that some
  amount of computational time is spent on unrated items, which can be
  enormous especially when the data is sparse.
}.




\section{Conclusions}
\label{sec:conclusion}

In this paper, we propose \cfnade, a feed-forward, autoregressive
architecture for collaborative filtering tasks. \cfnade is inspired by
the seminal work of RBM-CF and the
recent advancements of NADE. We 
propose to share parameters between different ratings to improve the
performance. We also describe a factored version of
\cfnade, which reduces the number of parameters by factorizing a large matrix by a product of two lower-rank matrices,
for better scalability. Moreover, we take the ordinal nature of
preference into consideration and propose an ordinal cost to optimize \cfnade. Finally,
following recent advancements of deep learning, we extend \cfnade to a
deep model with moderate increase of computational
complexity. Experimental results on three real-world benchmark datasets show
that \cfnade outperforms the state-of-the-art methods on
collaborative filtering tasks. all results of this work rely on explicit feedback, namely, ratings explicitly given by users. however, explicit feedback is not always available or as common as implicit feedback (watch, search, browse behaviors) in real-world recommender systems~\cite{hu2008collaborative}. Developing a version of \cfnade tailored for implicit feedback is left for future work.

\section*{Acknowledgements} 
We thank Hugo Larochelle and the reviewers for many helpful discussions.

\bibliography{example_paper}
\bibliographystyle{icml2016}

\end{document} 
